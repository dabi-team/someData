Comparing Two Samples Through Stochastic
Dominance: A Graphical Approach

Etor Arza
BCAM - Basque Center for Applied Mathematics
and
Josu Ceberio
University of the Basque Country UPV/EHU
and
Ekhiñe Irurozki
Télécom Paris
and
Aritz Pérez
BCAM - Basque Center for Applied Mathematics

August 31, 2022

Abstract

Non-deterministic measurements are common in real-world scenarios: the performance
of a stochastic optimization algorithm or the total reward of a reinforcement learning agent
in a chaotic environment are just two examples in which unpredictable outcomes are com-
mon. These measures can be modeled as random variables and compared among each other
via their expected values or more sophisticated tools such as null hypothesis statistical
tests. In this paper, we propose an alternative framework to visually compare two sam-
ples according to their estimated cumulative distribution functions. First, we introduce a
dominance measure for two random variables that quantiﬁes the proportion in which the
cumulative distribution function of one of the random variables stochastically dominates
the other one. Then, we present a graphical method that decomposes in quantiles i) the
proposed dominance measure and ii) the probability that one of the random variables takes
lower values than the other. With illustrative purposes, we re-evaluate the experimentation
of an already published work with the proposed methodology and we show that additional
conclusions—missed by the rest of the methods—can be inferred. Additionally, the software
package RVCompare was created as a convenient way of applying and experimenting with
the proposed framework.

Keywords: Data visualization, Random variables, Cumulative distribution function, First-order
stochastic dominance

2
2
0
2

g
u
A
0
3

]
L
M

.
t
a
t
s
[

4
v
9
8
8
7
0
.
3
0
2
2
:
v
i
X
r
a

1

 
 
 
 
 
 
Figure 1: Density estimates of the error rates produced by the optimizers adam and RMSProp in
the MNIST dataset. The sci-kit learnPedregosa et al. (2011) package was used in the estimation.

1

Introduction

The objective value obtained by an optimization algorithm may be non-deterministic. For ex-
ample, in stochastic algorithms, the objective value measured depends on the seed used in the
random number generator. In these kinds of scenarios, we can think that these non-deterministic
measurements are observations of random variables with unknown distributions. Based on these
measurements, we sometimes need to choose the random variable that takes the lowest (or
largest) values. The expected values of the random variables—usually estimated as an average
of several repeated observations—can be used for this purpose. However, many statisticians
have claimed that summarizing data with simple statistics such as the average or the standard
deviation is misleading, as very diﬀerent data can still have the same statistics Matejka and
Fitzmaurice (2017); Chatterjee and Firat (2007).

Motivating example 1) A real-world motivation for this work is as follows. Suppose we
need to choose the best option between two stochastic gradient-based methods for optimizing
the parameters of a neural network. A neural network classiﬁer trained with a gradient-based
method will produce diﬀerent error rates Goodfellow et al. (2016) each time it is trained-tested,
even if the same train-test dataset is used in each repeated measurement. One of the reasons is
that the learned classiﬁer depends on the initialization of its weights (before applying a gradient-
based optimizer), which are often initialized randomly Glorot and Bengio (2010).

To illustrate the previous scenario, we trained and tested a neural network1 in the MNIST
dataset, and we compared two gradient-based optimizers in this data set: adam and RM-
SProp Goodfellow et al. (2016). The error rate in the test set depends on the seed used to
train the neural network, and therefore, we can model the error rate of each of the algorithms
in this problem as a random variable. An observation of each of the two random variables (the
error rate of each gradient-based optimizer is modeled as a random variable) involves training
the neural network in the training set and measuring its error rate in the test set: the training
and test sets are the same for each trained neural network. Figure 1 shows the kernel density
estimations of these random variables using the uniform kernel. As we see in the ﬁgure, the error
rate is not the same in each measurement and ranges between 0.022 and 0.04. This shows that, in
this context, it makes sense to model the error rate as a random variable rather than a constant:
a unique value cannot represent the error rate without a signiﬁcant amount of information loss.

Motivating example 2)
In the following, we present another example with synthetic data.
Let us consider the two random variables XA and XB shown in Figure 2. XB has a lower
expected value than XA, E[XB] < E[XA]. If we use the expected value as the only criterion,
then XB takes lower values than XA. However, notice that with a low but nonzero probability,

1We follow an example in the Keras Chollet et al. (2015) library, and train the neural network for one epoch.

2

0.02250.02500.02750.03000.03250.03500.03750.0400Error rate in the test set0100200300Probability densityXA: adam optimizerXB: RMSProp optimizerFigure 2: The probability density of two random variables XA and XB, with probability density
functions gA = 0.925·gN (0.210325,0.002) +0.075·gN (0.010325,0.025) and gB = 0.975·gN (0.01875,0.002) +
0.025 · gN (0.06875,0.001) where N (ρ, σ) is the normal distribution with mean ρ and standard devi-
ation σ. Their expected values are E[XA] = 0.0205 and E[XB] = 0.02 respectively.

XB will take very large values that are undesirable in the context of minimization. Without loss
of generality, in this paper, we assume that lower values are preferred.

An error with low variance is very important in an environment where reliability is key, even if
it means a slightly worse expected value. Some examples include breast cancer detection Cruz-
Roa et al. (2017), or some reinforcement learning tasks François-Lavet et al. (2018); Mnih et al.
(2013) like self-driving cars Badue et al. (2021).

In other circumstances, obtaining the lowest possible error can be more important than reliability.
One could argue that reliability is less important in sentiment analysis Zhang et al. (2018), or
in certain real-world optimization problems Regnier-Coudert et al. (2016), where obtaining the
best possible solution is key. When obtaining the best possible score is more important than
reliability, it may even be worth running an optimization algorithm several times and choosing
the best solution out of all the runs. In that case, XA would also be preferred to XB, as XA has
a higher probability of taking a value lower than 0.01 (see Figure 2).

Related work In these two examples, we have seen that summarizing and comparing random
variables with only the expected value can leave important information out (such as which of the
random variables can take lower values), especially when neither random variable clearly takes
lower values than the other one. Many works in the literature use null hypothesis tests Mann and
Whitney (1947); Conover and Conover (1980); Wilcoxon (1945) to analyze observed samples and
choose one of the random variables accordingly. Nonetheless, as claimed in Benavoli et al. Be-
navoli et al. (2017), null hypothesis tests have their limitations too: when the null hypothesis
is not rejected—this will happen often when the random variables being compared take similar
values—, we get no information. Not only that but even when the null hypothesis is rejected, it
does not quantify the amount of evidence in favor of the alternative hypothesis Benavoli et al.
(2017).

Contribution In this paper, we propose a graphical framework that compares two random
variables using their associated cumulative distribution functions, in the context of choosing the
one that takes lower values. The proposed methodology can compare the scores of two stochastic
optimization algorithms or the error rates of two classiﬁers, among other applications. To achieve
this, the performances of the optimization algorithms (or the error rates of the classiﬁers) are
modeled as random variables, and then, we compare them by measuring the dominance.

3

0200XAExpected value0.000.020.040.06x0200XBSpeciﬁcally, we ﬁrst propose 8 desirable properties for dominance measures:
functions that
compare two random variables in this context. From the measures in the literature, we ﬁnd
that the probability that one of the random variables takes a lower value than the other random
variable satisﬁes most of these properties. In addition, we propose a new dominance measure,
the dominance rate, that also satisﬁes most of the properties and is related to the ﬁrst-order
stochastic dominance Quirk and Saposnik (1962). Then, we propose a graphical method that
involves visually comparing the random variables through these two dominance measures. The
graphical method, named cumulative diﬀerence-plot, can also be used to compare the quantiles
of the random variables, and it models the uncertainty associated with the estimate. By re-
evaluating the experimentation of a recently published paper with the proposed methodology,
we demonstrate that this new plot can be useful to compare two random variables, especially in
the case when the random variables take similar values.

Finally, an R package named RVCompare, available in CRAN, is distributed alongside this paper.
With this package, the cumulative diﬀerence-plot can be conveniently computed. The source code
of the package and the supplementary material for the paper are available at github.com/EtorArza2.

The rest of the paper is organized as follows:
in the next section, we propose eight desirable
properties for dominance measures. Then, in Section 3, we study two dominance measures that
satisfy most of these properties. Section 4 introduces a graphical method to compare random
variables. In Section 5, we discuss related methods in the literature and compare them to the
proposed approach. Section 6, evaluates the proposed graphical method and other alternatives
in an already published experimentation. In Section 7, we state the assumptions and limitations
of the proposed cumulative diﬀerence-plot. Finally, Section 8 concludes the paper.

2 Desirable properties for dominance measures

2.1 Background

When we have two random variables and we need to choose the one that takes the lowest values,
we usually take i) the random variable with the lowest expected value or ii) the random variable
with the lowest median. The median Conover and Conover (1980) of a continuous random
variable XA, denoted as mA, is the value that satisﬁes P(XA < mA) = P(XA > mA). In other
words, if mA is the median of XA, a sample of XA is as likely to be lower than mA as it is to be
higher.

Interestingly enough, the median and the expected value have their strengths and weaknesses
when it comes to choosing the random variable that takes the lowest values. In the following, we
elaborate on this point with two particular cases of study. The ﬁrst case is shown in Figure 3,
with two random variables XA and XB. Each of the random variables is a mixture of two
Gaussian distributions with the same shape and similar weight in the mixture. It is clear that
XA tends to take values lower than XB, as the Gaussian distributions of XA are centered in
0.05 and 0.07, while the Gaussian distributions of XB are centered in 0.06 and 0.08. While
the expected values of XA and XB are aligned with this intuition, the medians are not; as
E[XA] < E[XB] and mA > mB. However, the expected value does a poor job of summarizing
the bimodal shape of XA or XB: both of these random variables usually take much higher or
much lower values than their expected values.

The second case is shown in Figure 4. With a very high probability, XA takes lower values
than XB, even though XB will rarely take really low values, which might prove useful in some
particular applications. In this case, mA < mB and E[XA] > E[XB], hence, the comparison
of the medians are aligned with the intuition that XA takes lower values than XB, while the

2The source of the package RVCompare can be found at github.com/EtorArza/RVCompare. The code to

reproduce every ﬁgure in the paper is available at github.com/EtorArza/SupplementaryPaperRVCompare.

4

Figure 3: Case 1. The probability density functions of XA and XB: gA = 0.489·gN (0.05,0.00125) +
0.511 · gN (0.07,0.00125) and gB = 0.511 · gN (0.06,0.00125) + 0.489 · gN (0.08,0.00125) where gN (ρ,σ) is
the density function of the normal distribution with mean ρ and standard deviation σ.

Figure 4: Case 2. The probability density functions of XA and XB: gA = gN (0.211325,0.002) and
gB = 0.925 · gN (0.21875,0.002) + 0.075 · gN (0.04875,0.002) respectively.

expected values are not. In the presence of outliers Carreño et al. (2020), the median is considered
more robust than the expected value Rousseeuw and Hubert (2011).

Notice that, in the second case, it is not trivial to choose between XA and XB, as XB can
take lower values, but XA is more likely to be lower than XB. So, when can we claim that
one of them clearly takes lower values than the other? When the cumulative distribution of
XA is higher than the cumulative distribution of XB in the entire domain of deﬁnition:
in
that case, XA has a higher probability than XB of taking values lower than x, for all x in
the domain of deﬁnition. This is known Mann and Whitney (1947) as XA being stochastically
smaller than XB. Depending on the ﬁeld of study, it can also be referred to Schmid and Trede
(1996); Bennet (2013); Quirk and Saposnik (1962) as “XA stochastically dominates XB” 3. The
stochastic dominance can be further relaxed, obtaining what is known as ﬁrst-order stochastic
dominance in the literature Schmid and Trede (1996); Quirk and Saposnik (1962), although, for
the sake of brevity, we will call it stochastic dominance throughout the paper.

Deﬁnition 1. (Stochastic dominance) Let XA and XB be two continuous random variables
deﬁned in a connected subset N ⊆ R. We say that XA stochastically dominates XB, denoted as
XA (cid:31) XB, when

i) GA(x) ≥ GB(x) for all x ∈ N
and

3Without loss of generality, minimization is assumed in this paper.

5

0100XAExpected valueMedian0.050.060.070.08x0100XB0100200XAExpected valueMedian0.050.100.150.20x0100200XB(a) Case 1

(b) Case 2

Figure 5: The cumulative distributions of the two cases shown in Figures 3 and 4.

ii) There exists an x ∈ N such that GA(x) > GB(x).

where GA and GB are the cumulative distributions of XA and XB respectively.

For XA not to stochastically dominate XB (denoted as XA (cid:7) XB
4), either condition i) or ii)
must be violated. The special case that XA (cid:7) XB and XB (cid:7) XA at the same time is deﬁned, it
is said that XA and XB cross Bennet (2013), and we denote it as XA ≶ XB. In the non trivial
(XA (cid:54)= XB) case that XA ≶ XB, there exists two points x1, x2 ∈ N such that GA(x1) < GB(x1)
and GA(x2) > GB(x2): we cannot say, for all x ∈ N , that one of the random variables has a
higher probability of taking values lower than x.

Let us now see how the cumulative distributions can be used to compare random variables in an
example. In Figure 5a, the cumulative distributions of the random variables described in Figure 3
are shown. We can see that GA(x) > GB(x) for almost all x ∈ N . But there is at least a point
x ∈ (0.06, 0.07) where GA(x) < GB(x), hence, XA ≶ XB. The same happens in the second
case (Figure 5b). As in the previous case, XA ≶ XB, because even though GA(x) > GB(x) for
almost all x ∈ N (in which gA(x) (cid:54)= 0 and gB(x) (cid:54)= 0), for all x ∈ (0.05, 0.2), GA(x) < GB(x).

In the following, we will study how to quantify the diﬀerence between two random variables,
emphasizing the degree to which one of the random variables stochastically dominates the other.

2.2 Desirable properties

There are many ways to compare two random variables, each with a diﬀerent point of view:
some aim to ﬁnd how dissimilar two random variables are (disregarding which of them takes
lower values), while other methods try to guess if one of the random variables stochastically
dominates the other one. In the context of this paper, we are interested in measures that, given

4Note that XA (cid:7) XB is not equivalent to XB (cid:31) XA.

6

0.050.060.070.08x0.00.51.0GAGB0.050.100.150.20x0.00.51.0two random variables, quantify through the stochastic dominance how much one of the random
variables tends to take lower values than the other. We use the term dominance measure to refer
to functions that quantify the diﬀerence between two random variables following this intuition.
In this section, we deﬁne eight desirable properties for these dominance measures, and we study
the suitability of several measures from the literature.

Deﬁnition 2. Let XA and XB be two continuous random variables. We deﬁne a dominance
measure between two random variables as a function C that maps two random variables into a
real value C(XA, XB).

It is desirable that C(XA, XB) quantiﬁes the stochastic dominance. Hence, we want C(XA, XB)
to be proportional to the portion of the support of XA and XB in which GA(x) < GB(x).
Formally, this intuitive idea can be represented as:

Property 1. C is deﬁned in the [0, 1] interval, where:

i)

ii)

iii)

C(XA, XB) = 1 ⇐⇒ XA (cid:31) XB

C(XA, XB) = 0 ⇐⇒ XB (cid:31) XA

C(XA, XB) ∈ (0, 1) ⇐⇒ XB ≶ XA

Proposition 1. If a dominance measure C satisﬁes Property 1 i) and ii), then it also satisﬁes
Property 1 iii).

Proof. By deﬁnition, XB ≶ XA iﬀ XA (cid:7) XB and XB (cid:7) XA. Property 1 i) and ii) implies
that XA (cid:7) XB and XB (cid:7) XA iﬀ C(XA, XB) (cid:54)= 1 and C(XA, XB) (cid:54)= 0. From Property 1 i) also
C(XA, XB) ∈ [0, 1], thus XB ≶ XA iﬀ C(XA, XB) ∈ (0, 1).

Property 2. (Antisymmetry) C(XA, XB) and C(XB, XA) add up to 1.

C(XA, XB) = 1 − C(XB, XA)

It is noteworthy that Property 1 ii) can be inferred from Property 1 i) and Property 2.

Property 3. The inversion (under the sum) of the operands of C equals the inversion of C:

Property 4. When XA and XB are equal, C is symmetric.

C(−1 · XA, −1 · XB) = 1 − C(XA, XB)

XA = XB =⇒ C(XA, XB) = C(XB, XA)

Assuming Property 2 holds, we can rewrite the previous property as:

XA = XB =⇒ C(XA, XB) = 0.5.

Note that the opposite is not true:

C(XA, XB) = C(XB, XA)

(cid:54)=⇒ XA = XB

Property 5. (Invariance to translation) Moving the domain of deﬁnition of XA and XB by the
same amount does not change C5.

for all λ ∈ R,

C(XA + λ, XB + λ) = C(XA, XB)

5We deﬁne XA + λ as the random variable that is sampled in two steps: ﬁrst obtain an observation from XA

and then add λ to this observation. We deﬁne λ · XA in a similar way.

7

Figure 6: Case 3. The probability density functions of XA and XB: gA = gU (0.2,0.21) and gB =
0.925 · gU (0.19,0.2) + 0.075 · gU (0.04,0.05) respectively, where U(0.2, 0.21) is the uniform distribution
in the interval (0.2, 0.21).

Property 6. (Invariance to scaling) Scaling both XA and XB by the same positive amount does
not change C.

for all λ > 0,

C(λ · XA, λ · XB) = C(XA, XB)

In the following lines, we give an intuition for Property 7. In Case 2, shown in Figure 4, we saw
that for all x ∈ (0.075, 0.2), GA(x) < GB(x). However, notice that most of the mass of XA and
XB is in the interval (0.2, 0.23), where GA(x) > GB(x). This means that most of the observed
points of XA and XB will be in that interval. Therefore, it makes sense that GA(x) > GB(x) has
a higher weight than GA(x) < GB(x) in the computation C(XA, XB). In other words, the small
mass of XB centered in 0.05 can only account for a small part of C(XA, XB). In what follows,
this is formalized as XB being a mixture of two distributions, where one of the distributions
represents this small mass with a small weight in the mixture. Property 7 states that the change
in the computation of C produced by the distribution of small weight in the mixture can be, at
most, its weight in the mixture.

Property 7. Let XB = M[1−τ,τ ](XB1, XB2) be the mixture6 distribution of XB1 and XB2 with
weights 1 − τ and τ respectively and let XA be another random variable. Then,

|C(XA, XB) − C(XA, XB1)| ≤ τ

Property 8 explains that, under certain circumstances, C(XA, XB) is invariant to the transla-
tion/dilatation of one of the random variables. Speciﬁcally, it states that the distribution of one
of the random variables (XB) can change without aﬀecting the value of C(XA, XB) as long as
the changed part does not overlap with the support of the other random variable (XA). Let
us assume that the random variable XB is deﬁned as mixture distribution M[1−ρ,ρ](XB1, XB2)
where the supports of XB2 and XA do not overlap, with ρ ∈ (0, 1). Property 8 states that a
translation and/or dilatation can be applied to XB1, as long as: i) this transformation does not
cause an overlap of the supports of XA and XB2, and ii) partial transformations will also not
cause an overlap (hence the need for ξ1 and ξ2 in Property 8). In the following, we formalize
this property:

Property 8. Let XB = M[1−ρ,ρ](XB1, XB2) be the mixture distribution of XB1 and XB2 with
weights 1 − ρ, and ρ, respectively and let XA be another random variable with ρ ∈ (0, 1). Suppose
that supp(XB2) ∩ supp(XA) = ∅. Let λ1 ∈ R+, λ2 ∈ R be two numbers such that for all ξ1, ξ2 ∈

6The probability density function of M[1−τ,τ ](XB1, XB2) is deﬁned as (1 − τ ) · gB1(x) + τ · gB2(x). Note that

τ ∈ [0, 1].

8

050100XAExpected valueMedian0.0500.0750.1000.1250.1500.1750.200x050100XB[0, 1], supp((1 + (λ1 − 1)ξ1) · XB2 + ξ2λ2) ∩ supp(XA) = ∅. Then,

C(XA, XB) = C(XA, M[1−ρ,ρ](XB1, λ1 · XB2 + λ2)

This property can be applied to the distributions in Case 3 shown in Figure 6. For example,
the probability mass in the interval (0.04, 0.05) could have been centered in 0.1 or 0.15 instead
of 0.045, without any changes to C(XA, XB). In addition to the position, the shape of the mass
can also be altered as long as its weight in the mixture stays the same and does not overlap with
XA.

Unfortunately, it is impossible that a dominance measure satisﬁes Properties 1 and 7 at the same
time. Intuitively, the problem is that, given the distributions XA and XB = M[1−τ,τ ](XB1, XB2),
it is possible that XA (cid:31) XB1 and at the same time XB (cid:31) XA with τ < 0.57. We formalize and
prove this claim in the following proposition:

Proposition 2. Let C be a dominance measure.

i) If C satisﬁes Property 1, then it fails to satisfy Property 7.

ii) If C satisﬁes Property 7, then it fails to satisfy Property 1.

Proof. A dominance measure only satisﬁes a property when that property is true for every
possible random variable. Consequently, to prove this proposition, it is enough to ﬁnd four
random variables XA, XB, XB1 and XB2 where

i) XB = M[0.1,0.9](XB1, XB2),
ii) XA (cid:31) XB1,
iii) XB (cid:31) XA.

If four random variables can be found that satisfy these three statements, then with Property 1
we obtain that C(XA, XB1) = 1 and C(XA, XB) = 0. This contradicts Property 7, because
|C(XA, XB) − C(XA, XB1)| (cid:2) 0.1. The same is true the other way around, Property 7 states
that |C(XA, XB) − C(XA, XB1)| ≤ 0.1 and this contradicts Property 1, with C(XA, XB1) < 1 or
C(XA, XB) > 0.

A simple example in which this happens is for the random variables

XA = U(0, 1),

XB = M[0.9,0.1](U(0.1, 1), U(−0.5, 0)),

XB1 = U(0.1, 1),

XB2 = U(−0.5, 0).

The cumulative distribution functions of XA, XB and XB1 are shown in Figure 7, where it is
clear that XB (cid:31) XA and XA (cid:31) XB1.

In the following, we will brieﬂy review several measures in the literature and, speciﬁcally, which of
the proposed properties they satisfy. Many measures describe the diﬀerence between XA and XB,
disregarding whether the diﬀerence in cumulative density is positive or negative. Consequently,
they cannot satisfy Property 1 (see Appendix 9 for details). This is the case for f-divergences—
including Kullback-Leibler, Jensen-Shannon, the Hellinger distance and the total variation—and
for the Wasserstein distance. These measures also fail to satisfy several other properties (see a
summary in Table 1).

7See https://etorarza.github.io/pages/2021-interactive-comparing-RV.html for an interactive example that il-

lustrates the above point.

9

Figure 7: The cumulative distribution functions of XA, XB and XB1.

Table 1: Which of the properties in Section 2.2 does each measure satisfy?

1

2

3

Kullback-Leibler divergence
Jensen-Shannon divergence
Total-Variation
Hellinger distance
Wasserstein distance
CP : Probabitlity of XA < XB
CD: Dominance rate of XA over XB (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)

(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
(cid:88)

7

8
4
6
5
(cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)

A checkmark (cid:88) indicates that the measure satisﬁes the property.

3 Dominance measures

Most of the measures in the literature fail to satisfy the eight properties introduced in Section 2.2.
However, there is a dominance measure in the literature that overcomes this limitation: the
probability that XA < XB Conover and Conover (1980).

3.1 CP: the probability of XA < XB

We can compare XA and XB with the probability that a value sampled from XA is smaller than
a value sampled from XB. When the random variables are exactly the same, this probability
is 0.5. Formally, given two continuous random variables XA and XB deﬁned in a connected set
N ⊆ R, the probability that XA < XB is deﬁned as:

P(XA < XB) =

(cid:90)

N

gB(x)GA(x)dx.

(1)

When we consider P(XA < XB) as a dominance measure, we will denote it as CP (XA, XB).

One of the advantages of CP is its easy interpretation. In addition, CP is a well behaved domi-
nance measure, as it satisﬁes Properties 2, 3, 4, 5, 6, 7 and 8. It also satisﬁes a weak version of
Property 1:

CP (XA, XB) = 1 =⇒ XA (cid:31) XB =⇒

CP (XA, XB) ∈ (0.5, 1]

and

CP (XA, XB) = 0 =⇒ XB (cid:31) XA =⇒

CP (XA, XB) ∈ [0, 0.5).

10

0.51.0xCumulative distribution0.000.501.00−0.50.0Note that, when XA (cid:31) XB, CP (XA, XB) (cid:54)= 1 is still possible, and this is why it does not
satisfy Property 1 entirely. For instance, when the probability densities of XA and XB are two
Gaussian distributions with the same variance and the mean of XA is lower, then XA (cid:31) XB but
CP (XA, XB) < 1.

So far, we have seen that CP satisﬁes most of the properties. Unfortunately, since it does not
satisfy Property 1, not all cases of XA (cid:31) XB can be identiﬁed by CP . We now propose a
dominance measure that satisﬁes Property 1 and, thus, can be used to identify cases in which
XA (cid:31) XB.

3.2 CD: dominance rate

Intuitively, the dominance rate is a dominance measure that quantiﬁes the extent to which
XA has a lower cumulative distribution function than XB, normalized by the portion of the
probability densities with diﬀerent cumulative distributions.

Deﬁnition 3. (Dominance density function) Let XA and XB be two continuous random vari-
ables deﬁned in a connected set N ⊆ R. We deﬁne the dominance density function as follows:

DXA,XB (x) =






gA(x) · kA
−gB(x) · kB

0

if GA(x) > GB(x)
if GA(x) < GB(x)
otherwise.

(cid:16)(cid:82)

where kA =
likewise.

{x∈N | GA(x)(cid:54)=GB (x)} gA(t)dt

(cid:17)−1

is the normalization constant and kB is deﬁned

Note that the dominance density function is not correctly deﬁned when (cid:82)

N |gA(x)−gB(x)|dx = 0.

Deﬁnition 4. (Dominance rate) Let XA and XB be two continuous random variables deﬁned
in a connected set N ⊆ R. The dominance rate of XA over XB is deﬁned as

CD(XA, XB) =

(cid:40)

0.5,
0.5 (cid:82)

if (cid:82)

N |gA(x) − gB(x)|dx = 0

N DXA,XB (t)dt + 0.5,

otherwise.

Basically, we are measuring the amount of mass of XA in which GA(x) > GB(x) minus the
amount of mass of XB in which GA(x) < GB(x). This value is then normalized so that all
sections in which GA(x) = GB(x) are ignored, i.e. (cid:82)

N DXA,XB (t)dt =

EA[I[GA(x) > GB(x)]]
EA[I[GA(x) (cid:54)= GB(x)]]

−

EB[I[GA(x) < GB(x)]]
EB[I[GA(x) (cid:54)= GB(x)]]

Finally, we apply the linear transformation l(x) = 0.5x − 0.5 ensuring the dominance rate is
deﬁned in the interval [0, 1] (instead of [−1, 1]), required to comply with Property 1.

From

i) CD(XA, XB) = 1 ⇐⇒ XA (cid:31) XB and

ii) CD(XA, XB) = 0 ⇐⇒ XB (cid:31) XA,

we deduce that the dominance rate satisﬁes Property 1. Note that the previous deduction
is only possible when gA and gB are bounded, as this implies that GA and GB are continu-
ous. Speciﬁcally, it is enough to ﬁnd a point in N in which GA(x) > GB(x) to satisfy that

11

(cid:82)

x∈{t∈N | GA(t)>GB (t)} gA(x)dx > 0, and this point is guaranteed to exist when XA (cid:31) XB be-
cause of the deﬁnition of the dominance. The dominance rate is also a well behaved dominance
measure, as it satisﬁes Properties 1, 2, 3, 4, 5, 6 and 8.

We have seen that the dominance measures CP and CD satisfy most of the properties listed in
Section 2.2. As we will see in the next section, their values are related.

3.3 The relationship between CP and CD

In Section 2.1 we stated that CP = 1 is a stronger condition than CD = 1, because CP (XA, XB) =
1 implies that for all x in N that GA(x) < 1, GB(x) = 0. On the other hand, CD = 1 implies
that XA (cid:31) XB (the two conditions in Deﬁnition 1), which is weaker. In the diagram below,
we show the values of CP and CD that imply other values of CP and CD. Each arrow can be
i.g. CD(XA, XB) = 1 implies
interpreted as an implication. The implications are transitive:
CP (XA, XB) > 0.5.

CP (XA, XB) = 1

CD(XA, XB) = 1

CP (XA, XB) > 0.5

CD(XA, XB) > 0.5

Figure 8: Implications between the values of CP and CD.

3.4 Estimating CP and CD

In the previous sections, we have assumed that the random variables XA and XB are known,
but usually, we only have a few observed values from each random variable. Therefore, it may
be interesting to estimate CP and CD from the observed samples. With this purpose, we propose
the following empirical estimates of CP and CD.

Deﬁnition 5. (estimation of CP )
Let XA and XB be two continuous random variables and An = {a1, ..., an} and Bn = {b1, ..., bn}
their n observations respectively. We deﬁne the estimation of the probability that XA < XB as

(cid:102)CP (An, Bn) =

(cid:88)

i,k=1...n

sign(bk − ai)
2n2

+

1
2

.

This estimator is well known in the literature because it is the U statistic of the Mann-Whitney
test Mann and Whitney (1947).

Deﬁnition 6. (estimation of CD)
Let XA and XB be two continuous random variables and An = {a1, ..., an} and Bn = {b1, ..., bn}
their n observations respectively. Let C2n = {cj}2n
j=1 be the list of all the sorted observations of
An and Bn where c1 is the smallest observation and c2n the largest. Suppose that ai (cid:54)= bk for all
i, k = 1, ..., n. We deﬁne the estimation of the dominance rate as

(cid:102)CD(An, Bn) =

2n
(cid:88)

j=1

I( ˆGA(cj) > ˆGB(cj) ∧ cj ∈ An)
2n

−

12

2n
(cid:88)

j=1

I( ˆGA(cj) < ˆGB(cj) ∧ cj ∈ Bn)
2n

+

1
2

.

where I is the indicator function and ˆGA(x) and ˆGB(x) are the empirical distributions estimated
from An and Bn respectively.

For simplicity, this estimator of the dominance rate assumes there are no repeated samples.
However, it can be extended to take into account repeated values (see Appendix 11).

13

Figure 9: An example of the probability density functions of YA and YB given the observed
samples An ∪ Bn.

4 Cumulative diﬀerence-plot

In this section, we propose a graphical method called cumulative diﬀerence-plot that shows
the estimations of CP and CD decomposed by quantiles: CP and CD can be visually estimated
from the diﬀerence plot. In addition, the proposed plot allows a comparison of quantiles of the
two random variables. The proposed approach also models the uncertainty associated with the
estimation of the cumulative diﬀerence-plot from the data.

4.1 Quantile random variables

From a practical point of view, it is unlikely that the probability densities of the compared random
variables XA and XB are known. Usually, we only have n observations An = {a1, ..., an} and
Bn = {b1, ..., bn} from each random variable. The proposed cumulative diﬀerence-plot is based
on two random variables YA and YB that are deﬁned with these observations. Speciﬁcally, we
deﬁne the densities of the two quantile random variables YA and YB as a mixture of several
uniform distributions in the interval [0, 1].

2n

The uniform distributions in the quantile random variables are placed according to their rank
in An ∪ Bn. Assuming no repetitions, for each value k in An ∪ Bn, its corresponding kernel is
centered in rank(k)+0.5
where rank(k) is the rank of k in An ∪ Bn. The kernels have a bandwidth
of 1/4n, ensuring that the sum of the densities of YA and YB is constant. If there are repeated
values in An ∪ Bn, their corresponding kernel is placed at the middle of the previous and the
next rank, and the width of the kernel is increased proportionally with respect to the number
of repetitions. See Figure 9 for an example. In Appendix 10.1 we show how to compute the
probability densities of YA and YB step by step.

A more simple approach would be to estimate and deﬁne the quantile random variables through
the empirical cumulative distribution functions of the observed samples of XA and XB. However,
the quantile random variables deﬁned through uniform kernels have some interesting properties:
they have the same CP and CD as the kernel density estimations of XA and XB (shown in
Appendix 10.2). In addition, gYA(x) + gYB (x) = 2 for all x ∈ [0, 1]. As we will later see, these
properties are essential for the interpretation of the cumulative diﬀerence-plot.

14

Figure 10: The conﬁdence bands of the cumulative distributions of the quantile random variables
YA and YB corresponding to the distributions XA and XB in Case 2 (shown in Figure 4).

4.2 Conﬁdence bands

The cumulative diﬀerence-plot is based on the cumulative distribution functions of YA and YB,
which are estimated from the observed samples. This means that we need to model the uncer-
tainty associated with the estimations. Conﬁdence bands are a suitable choice in this scenario:
a conﬁdence band is a region in which the cumulative distribution is expected to be with a
certain conﬁdence. The size of the band is determined by the number of samples and the desired
level of conﬁdence: a high number of samples or a low level of conﬁdence are associated with
a small band size. There is an extensive literature Cheng and Iles (1983); Steck (1971); Cheng
and Lies (1988); Wang et al. (2013); Faraway and Myoungshic Jhun (1990); Bickel and A. M.
Krieger (1989); Hall and Horowitz (2013) on how to estimate the conﬁdence bands of cumulative
distributions, and, in this work, we use a simple bootstrap approach8.

To illustrate how to interpret the conﬁdence bands of the cumulative distributions of YA and
YB, we will assume that we have observed n = 400 samples from each random variable XA
and XB from Case 2 (see Figure 4 in Section 2.1). We show the 95% conﬁdence bands of
the cumulative distribution functions of YA and YB in Figure 10. The estimated cumulative
distribution functions of YA and YB resemble the cumulative distribution functions of XA and
XB from Figure 5b. However, there are several relevant diﬀerences. In Figure 10, we observe
that YA and YB are deﬁned in the interval [0, 1], while the cumulative distribution functions of
XA and XB are deﬁned in the sample space. Each of the values in this interval can be used
to deduce the distribution with the lowest quantile: at x = 0.5, the cumulative distribution
function of YA is larger than the cumulative distribution function of YB, hence, the median of
XA is lower than the median of XB. In addition, the sum of the density function of YA and YB
is constant. As a result, unlike XA and XB, the probability density functions of YA and YB do
not have large areas where the probability density is zero.

8The bootstrapping Efron and Tibshirani (1993) method involves considering the observed values as a pop-
ulation from which random samples with replacement are drawn. These samples are then used to estimate the
upper and lower pointwise conﬁdence intervals of the cumulative distribution of YA and YB. Since a pointwise
estimation of the conﬁdence interval is used, we can expect that a portion proportional to α will fall outside the
conﬁdence band.
Note that we are interested in having an overall conﬁdence of 1 − α, thus, we want that the cumulative distribu-
tions of YA and YB are inside their conﬁdence bands at the same time with this level of conﬁdence Goeman and
Solari (2014); Bauer (1991). This means that we have to use a higher conﬁdence level for each band: (cid:112)(1 − α).

15

4.3 The cumulative diﬀerence-plot

In this section, we introduce a new graphical method designed to visually analyze the dominance
of XA and XB. Without loss of generality, a minimization9 setting is assumed: lower values in
XA and XB are preferred to higher values. It builds upon the diﬀerence function deﬁned as

diﬀ : [0, 1] −→ [−1, 1]

x (cid:55)−→ GYA(x) − GYB (x),

(2)

where GYA (x) and GYB (x) are the cumulative distribution functions of YA and YB, respectively.

The cumulative diﬀerence-plot is the plot of the diﬀerence function (the diﬀerence between the
cumulative distributions of YA and YB), including a conﬁdence band. A positive value in the
cumulative diﬀerence-plot can be interpreted as a quantile in which the cumulative distribution
function of XA is larger than the cumulative distribution function of XB. Hence, if the diﬀerence
is positive at 0.5, the median of XA is lower than the median of XB (assuming minimization).
In this sense, the best values obtained by both random variables are compared on the left side,
and the worst values are compared on the right side.

4.3.1 CP and CD in the cumulative diﬀerence-plot

CP and CD can be directly obtained from the proposed plot. The integral of the diﬀerence between
YA and YB is CP −0.5 (we prove this in Appendix 11). Formally, CP = 0.5+(cid:82) 1
0 diﬀ(x)dx. However,
in practice, CP can be visually estimated by adding 0.5 to the diﬀerence in the areas over and
under 0. For the example shown in Figure 11, CP = 0.5 − Area1 + Area2. The diﬀerence can
only be in the area highlighted in blue in the cumulative diﬀerence-plot. When the probability
that XA < XB is 1, the diﬀerence is at its maximum: in the cumulative diﬀerence-plot we see a
line from (x, f (x)) = (0, 0) to (0.5, 1) and from (0.5, 1) to (1, 0). Similarly, when the probability
that XA < XB is 0, the diﬀerence between YA and YB is equal to the lowest possible values
inside the light blue area.

By contrast, CD is represented in the plot as the total length in which the diﬀerence is positive
minus the total length in which the diﬀerence is negative. Speciﬁcally,

(cid:82) 1
0 I[diﬀ(x) > 0] − I[diﬀ(x) < 0]dx
2

(cid:82) 1
0 I[diﬀ(x) (cid:54)= 0]dx

+ 1
2

,

CD =

(3)

where I is the indicator function (we prove this in Appendix 11). As an example, CD is propor-
tional to Length2 − Length1 in Figure 11: it is higher than 0.5, because Length2 > Length1. In
this example, there is no need to divide by the total length in which the diﬀerence is nonzero
because the diﬀerence is zero in only a limited number of points. In such cases, CD can also be
estimated as the total length in which diﬀ(x) > 0. In the example in Figure 11, the estimation
is CD = Length2 ≈ 0.75. Note that Equation (3) is not correctly deﬁned when YA and YB are
equal, but this is an easy case to identify, as the diﬀerence is constantly 0.

9Note that if the random variables being compared take values in a maximization setting (higher values are
preferred), then the random variables need to be redeﬁned as the inverse with respect to the sum (this simply
means the sampled values are multiplied by −1) before generating the cumulative diﬀerence-plot. With this
change, the interpretation of the cumulative diﬀerence-plot is consistent and intuitive: for either minimization
or maximization, on the left side of the cumulative diﬀerence-plot, the most desirable values that the random
variables take are compared. If the diﬀerence is positive on the left side of the cumulative diﬀerence-plot, then
the best values that XA takes are better than the best values that XB takes. Similarly, the worst values are
compared on the right side of the cumulative diﬀerence-plot: if the diﬀerence is positive on this side, then the
worst values of XA are better than the worst values of XB.

16

Figure 11: The areas and lengths in the cumulative diﬀerence-plot that can be used to deduce
CP and CD.

Figure 12: The cumulative diﬀerence-plot for Case 2: the diﬀerence between the cumulative
distribution functions of YA minus YB corresponding to the distributions XA and XB in Case 2
(shown in Figure 4).

17

Area1Area2Length1Length2−1.0−0.50.00.51.00.000.250.500.751.00xdiff(x)−1.0−0.50.00.51.00.000.250.500.751.00xdiff(x)4.3.2

Illustrative example

Figure 12 shows the cumulative diﬀerence-plot for the random variables XA and XB from Case
2 (their densities were shown in Figure 4). First, we see that the diﬀerence is both negative and
positive, hence, neither random variable dominates the other. The diﬀerence is negative when
x = 0.05 or lower. This can be interpreted as XB having a smaller 5% quantile than XA. The
diﬀerence is positive otherwise, thus we deduce from the cumulative diﬀerence-plot that the 25%,
the 50% (the median), the 75% and 95% quantiles are smaller in XA than in XB. In other words,
the random variable XB can take really low values with a small probability, but apart from these
really low values, XA takes lower values than XB. This is also reﬂected by CD(XA, XB) > 0.75,
as deduced from diﬀ(x) > 0 for all x ∈ (0.25, 1). The diﬀerence is also near its maximum value,
implying that its integral is high and thus CP (the probability that XA < XB) is also near 1.

5 Related work

Statistical assessment of experimental results is a very studied research topic. In this section,
we locate our proposal in the ﬁeld and focus on similarities and diﬀerences with respect to other
random variable comparison methods.

5.0.1 Visualizing densities

As mentioned in the introduction, it makes sense to model the performance of stochastic op-
timization algorithms as random variables. Therefore, statistical tools that compare random
variables have become an increasingly important part of the analysis of experimental data.
Among these tools, visualization techniques such as histograms or box-plots are usually applied
before the rest of the methods. The advantage of these methods is their simplicity. If one of the
random variables clearly takes lower values than the other, then these two methods eﬀectively
convey this message simply and naturally. Unfortunately, when both random variables have
similar probability densities, these two methods might fail to represent the random variables in
a way that makes it easy to compare them (example shown in Section 6.1).

The simplicity of these methods is also a drawback: for example, they give no information about
the uncertainty associated with the estimates. The histogram suﬀers from the bin positioning
bias Thas (2010); scikit-learn developers (2021). A kernel density estimation with the uniform
kernel—considered to be the moving window equivalent of the histogram—overcomes this lim-
itation Thas (2010), at the cost of using a more complex model. Similarly, the box-plot has a
“non-injectivity” problem: very diﬀerent data can still have the same box-plot Matejka and Fitz-
maurice (2017); Chatterjee and Firat (2007). The violin-plot is an extension of the box-plot that
overcomes the above limitation by combining the kernel density estimate of the random variables
with the traditional box-plot Hintze and Nelson (1998). The proposed cumulative diﬀerence-plot
improves on these methods because it represents the data clearly, even when the two random
variables being compared are similar.

5.0.2 Null hypothesis statistical testing

Null hypothesis tests can be used to compare random variables without having to visually rep-
resent them. In a very general way, carrying out a null hypothesis test involves the following:
ﬁrst, a null hypothesis is proposed. Under certain assumptions, the null hypothesis implies that
a given statistic obtained from the data follows a known distribution. Then, assuming the null

18

hypothesis is true, the probability of obtaining data with a more extreme statistic value10 than
the observed Conover and Conover (1980) is computed. When the probability under the null
hypothesis of the observed statistic is lower than a predeﬁned threshold, the null hypothesis is
rejected and the alternative hypothesis is accepted Greenland et al. (2016). Usually, this thresh-
old is set at an arbitrary but well established Wasserstein and Lazar (2016) p = 0.05, although
recently, further reducing the threshold to p = 0.005 has been proposed Benjamin et al. (2018);
Ioannidis (2018).

In the context of comparing two random variables XA and XB, in general, we cannot assume
that a statistic obtained from the data follows a known distribution under the null hypothesis. In
this case, a non-parametric test Conover and Conover (1980) is a suitable choice. Speciﬁcally, the
Mann-Whitney Mann and Whitney (1947) test is a good choice, as the samples observed from the
random variables are i.i.d for each random variable 11. With this test, the null hypothesis is that
P(XA > XB) = P(XB > XA), and a possible alternative hypothesis is that XA (cid:31) XB Mann
and Whitney (1947).

Null hypothesis tests have some limitations: for example, the p-value does not separate between
the eﬀect size and the sample size Benavoli et al. (2017); Calvo et al. (2019). In addition, rejecting
the null hypothesis does not always mean that there is evidence in favor of the alternative
hypothesis: it just means that the observed statistic (or a more extreme statistic) is very unlikely
when the null hypothesis is true.

To show this, we generate 400 samples of the distributions XA and XB from Case 2 (density
functions shown in Figure 4) and we apply the Mann-Whitney test, rejecting the null hypothesis
when p < 0.005. If we repeat this experiment 104 times (with diﬀerent samples each time), the
null hypothesis is rejected every time12. However, XA (cid:7) XB and XB (cid:7) XA, implying that the
alternative hypothesis is not true. Note that the proposed cumulative diﬀerence-plot (shown
in Figure 12) avoids this problem because it correctly points out that neither random variable
dominates the other one, for the same case and with the same number of samples.

5.0.3 Bayesian analysis

As an alternative Benavoli et al. (2017); Calvo et al. (2019) to the limitations of null hypothesis
test, Bayesian analysis has been proposed. Bayesian analysis Gelman (2014); Bernardo and
Smith (2009) estimates the probability that a hypothesis is true, conditioned to the observed
data. This estimation requires the prior probabilities of the hypotheses and the data, but usually,
they are assumed to follow a distribution that gives equal probability to all hypotheses and data.
Recently a Bayesian version of the Wilcoxon signed-rank test Benavoli et al. (2014, 2017) has
been proposed. In this paper, we will consider the simplex-plot of its posterior distribution. For
convenience, in the rest of the paper, we will call it simplex-plot.

Once the posterior distribution is known, the probability that the diﬀerence between a sample
from XA and a sample from XB is in the interval (−∞, −r), [−r, r] or (r, +∞) can be computed.
These probabilities can be interpreted as the probability that XA > XB, XA = XB and XB >
XA, where two samples xa and xb are considered equal when |xa−xb| ≤ r. Note that the simplex-
plot is just a convenient representation of the posterior distribution, where ‘rope’ or range of
practical equivalence denotes hypothesis XA = XB (when the diﬀerence is in the interval [−r, r]).

10The deﬁnition of what data with a more extreme statistic value is not the same for every null hypothesis test,

and it depends on the test being used.

11For paired data, the Wilcoxon signed-rank test Wilcoxon (1945) or the sign test Conover and Conover (1980)
should be used. However, in the context of this paper, the samples observed from the random variables are not
paired. In this paper, we consider the Mann-Withney test as it is probably the most well known non-parametric
test for unpaired data, although take into account that more modern alternatives have been proposed Ledwina
and Wyłupek (2012); Baumgartner et al. (1998); Biswas and Ghosh (2014).

12The source code to replicate this experiment is available in the ﬁle mann_whitney_counter_example.R in

our Github repository.

19

Figure 13: The simplex-plot computed with the package scmamp Calvo and Santafé Rodrigo
(2016) of the posterior distribution for Case 2.

We computed the simplex-plot (Figure 13) with the 400 samples of XA and XB from Case 2
obtained in Section 4.2. Two samples were considered equal when their diﬀerence is lower than
r = 10−3, and we used the prior proposed by Benavoli et al. (2017). We can deduce from this
ﬁgure that the hypothesis XB > XA is much more likely than XA = XB or XB > XA.

The simplex plot summarizes the data through the probabilities of XB > XA, XA = XB
or XB > XA, but does not oﬀer any additional information: we cannot deduce from these
probabilities in which intervals the values of a random variable are lower than the other. In this
sense, the cumulative diﬀerence-plot is a more detailed comparative visualization. Speciﬁcally,
the observation that the 1% lowest values of XA are lower than the 1% lowest values of XB
cannot be deduced from the simplex-plot, while it is easy to see in the cumulative diﬀerence-
plot. Also, the cumulative diﬀerence-plot shows a comparison of the cumulative distributions
through the dominance rate, while the simplex-plot does not.

5.0.4 Other plots in the interval [0, 1]

The probability-probability plot is deﬁned as

P P : [0, 1] → [0, 1]2 : p → (p, GA(G−1

B (p))).

As proposed by Schmid et al. Schmid and Trede (1996), it can be interpreted via the integral of
the non-negative part, which represents the amount of violation against the hypothesis that XA
dominates XB.

The quantile-quantile plot Thas (2010); Wilk and Gnanadesikan (1968) is deﬁned as

QQ : [0, 1] → N 2 : p → (G−1

A (p), G−1

B (p)),

and it is a natural way to visualize the diﬀerences in quantiles of XA and XB in N (the domain
of deﬁnition of the random variables).

The quantile-quantile plot also allows a comparison between quantiles, just like the cumulative
diﬀerence-plot. However, the cumulative diﬀerence-plot proposed in this paper is distinct from
the two plots above in three aspects:
i) the proposed cumulative diﬀerence-plot is deﬁned di-
rectly from the observed samples. Because of its deﬁnition, it has a conﬁdence band built-in,
which allows the uncertainty associated with the estimation to be directly interpreted within
the plot. ii) The proposed cumulative diﬀerence-plot contains several statistics simultaneously.
Speciﬁcally, the estimated CD, CP and the comparison of the quantiles can be visually inter-
iii) The proposed plot is just the diﬀerence of two cumulative distributions (GYA and
preted.

20

GYB ), and thus, unlike in the pp-plot and qq-plot mentioned above, it can be deﬁned without
the need of the inverse function. The random variables YA and YB have the same CD, CP as
the kernel density estimations of the original distributions, and therefore, we can think of the
cumulative diﬀerence-plot as the diﬀerence between the cumulative distribution function of two
simpler versions of the original random variables.

6 Experimentation with the cumulative diﬀerence-plot

To illustrate the applicability of the proposed methodology, in the following, we re-evaluate the
experimentation of a recently published work. In a recent paper, Santucci et al. (2020) introduced
a gradient-based optimizer for solving problems deﬁned in the space of permutations (from
now on PL-GS ). In their experimentation, they compared it with an estimation of distribution
algorithm Larrañaga and Lozano (2001) (from now on PL-EDA). These two algorithms were
tested in a set of 50 problem instances of the linear ordering problem Schiavinotto and Stützle
(2004). The performance of each algorithm in each instance was estimated with the median
relative deviation from the best-known objective value, with n = 20 repetitions. From now on,
we call score to the relative deviation from the best-known objective value and note that a low
score is better than a high score, as it means that the objective value found is closer to the
best-known.

In the work by Santucci et al. (2020), when the score of one of the algorithms was at least 10−4
higher than the other, it was considered that one of the algorithms performed better than the
other in that instance. Santucci et al. (2020) concluded that both algorithms performed equally
in the instance N-t70n11xx, as the median scores were exactly the same for both algorithms in
this instance.

In the following, we take a closer look at the performance of PL-EDA and PL-GS in this problem
instance by comparing n = 103 measurements of the score from each algorithm. We increase
the sample size from n = 20 to n = 103 because the diﬀerence between the performance of
the algorithms is small. With a sample size of n = 20, the uncertainty is too high to come
to any meaningful conclusion (regardless of the statistical methodology considered). With this
increased sample size, we obtained more accurate estimates of the median scores—PL-GS =
0.00407, EDA = 0.00433, lower is better—and PL-GS obtains a better value by a diﬀerence
higher than 10−4.

6.1 Step 1: Visualization

Figure 14 shows the histogram of the scores. It can be deduced from the ﬁgure that neither
algorithm clearly produces better scores. In particular, neither algorithm dominates the other:
PL-EDA has a longer tail both to the right and to the left. Also, notice that the score of the
algorithms is not normally distributed: PL-GS has a bimodal shape, and PL-EDA has a very
long tail to the right (while the tail to the left is shorter).

Figure 15 shows the box-plot and the violin-plot of the data. Both algorithms have a similar
median, but due to the high number of outliers Carreño et al. (2020), it is diﬃcult to compare
the scores of the algorithms with the box-plot. The same happens with the violin-plot.

6.2 Step 2: Comparing PL-GS with PL-EDA

Sometimes, visualization is enough to compare the performance of two algorithms: if one of the
algorithms always performs better than the other, there is no need for further analysis. However,

21

Figure 14: Histogram of the scores obtained in the instance N-t70n11xx. Lower is better.

Figure 15: Box-plot and violin-plot of the scores obtained in the instance N-t70n11xx. Lower is
better.

in this case, the three visualization methods considered (histogram, box-plot, and violin-plot)
have not been able to summarize the scores obtained with the algorithms in a way that enables an
easy comparison. In the following, we further study the scores of the algorithms with statistical
tests, the simplex-plot, and the cumulative diﬀerence-plot.

6.2.1 Mann-Whitney test

Applying the Mann-Whitney test we obtain a p-value of p = 0.035, lower than the usually used
0.05 threshold. With p < 0.05, we reject the null hypothesis and accept the alternate hypothesis:
the random variable associated with the score of PL-GS dominates PL-EDA. Note that neither
rejecting the null hypothesis nor a small p-value reﬂect the magnitude of the diﬀerence in score
of the algorithms. In addition, as stated when we studied the histogram, we known that it is
unlikely that PL-GS dominates PL-EDA.

6.2.2 Simplex-plot

We show the simplex-plot Benavoli et al. (2017) of the scores in Figure 16. Following the criterion
by Santucci et al. (2020), we considered that two scores are equal when they diﬀer by less than
r = 10−4. Unlike in the statistical test, one can deduce the probability that one of the algorithms
has a better score than the other from simplex-plot: it is more likely that PL-GS takes a lower
value than PL-EDA. A closer position in the plot to PL-EDA indicates a higher probability of
measuring a higher score in PL-EDA than in PL-GS. Speciﬁcally, from the simplex-plot shown
in Figure 16, we can deduce that given two samples xgs and xeda of the scores of PL-GS and
PL-EDA respectively,

P(xeda < xgs) < P(xgs < xeda).

22

01002003004008.158774e−041.347146e−022.612705e−02scorecountPL−EDAPL−GS0.000.010.02PL−EDAPL−GS scoreFigure 16: Simplex-plot of PL-GS and PL-EDA in the instance N-t70n11xx. A closer position
in the plot to PL-EDA indicates a higher probability of measuring a higher score in PL-EDA
than in PL-GS. A low score is preferred to a high score.

However, the diﬀerence in these probabilities is small. Also, the probability that P(xgs = xeda)
is low (no data points near ‘rope’).

6.2.3 Cumulative diﬀerence-plot

We show the 95% conﬁdence cumulative diﬀerence-plot in Figure 17. From this plot, we can
deduce the following:

1. P(xeda < xgs) and P(xgs < xeda) have similar probabilities, as CP (PL-EDA, PL-GS) ≈ 0.5.
However, The area under diﬀ(x) = 0 is a little larger than the area over diﬀ(x) = 0, hence
P(xeda < xgs) is a little smaller than P(xgs < xeda).

2. Neither algorithm dominates the other one, and what is more, CD(PL-EDA, PL-GS) ≈ 0.5.

3. The diﬀerence is positive when x < 0.3, and therefore, if we only consider the best 30%

values of both algorithms, PL-EDA dominates PL-GS.

4. The diﬀerence is negative when x > 0.98. In this case, we conclude that if we only consider

the worst 2% values of PL-EDA and PL-GS, then PL-GS dominates PL-EDA.

5. These “worst” 2% values are much less likely than the “best” 30% values mentioned in 3),
as the estimated probability of these “best” and “worst” values is 0.3 and 0.02 respectively.

6. The diﬀerence is negative at x = 0.5 and at x = 0.75. This can be interpreted as PL-GS

having a better median and a better 75% quantile.

Summarizing the above points, we conclude that the performance of the algorithms is quite
similar, and PL-EDA takes both better and worse scores than PL-GS. The probability that PL-
EDA takes these better values is much higher than the probability that it takes worse values.
Therefore, if we are in a setting in which repeating the execution of the algorithms is reasonable,
PL-EDA is a much better algorithm. On the other hand, if it is critical to avoid really bad values,
then PL-GS would be preferred. With an increased number of samples, it might be possible to
better compare the algorithms (it would reduce the uncertainty associated with the size of the
conﬁdence band).

23

Figure 17: The cumulative diﬀerence-plot of 95% conﬁdence of the objective values obtained
by PL-EDA and PL-GS in the instance N-t70n11xx.

7 Assumptions and limitations

In the following, we brieﬂy summarize the assumptions that the cumulative diﬀerence-plot re-
quires and comment on a few caveats.

7.1 Assumptions

Correctly using the proposed cumulative diﬀerence-plot requires that the following three as-
sumptions are satisﬁed. The ﬁrst assumption is that all samples of both XA and XB are i.i.d,
consequently, it should not be used with paired data. This is also an assumption made by the
Mann-Whitney test.

The second assumption is that the values of the random variables represent a minimization
setting: lower values are preferred to higher values. To apply the proposed method in a maxi-
mization setting, it is enough to redeﬁne the objective function by multiplying it by −1.

The third assumption is that XA and XB are continuous random variables deﬁned in a connected
subset of R. This also implies that the cumulative distribution functions of XA and XB are
continuous and that their probability density functions are bounded. Although having a bounded
density means that there should never be two identical samples—the probability of observing two
independent equal samples is 0 with a bounded density—, in reality, the proposed cumulative
diﬀerence-plot can deal with repeated samples. To do so, when deﬁning the kernel density
estimations of YA and YB in Section 4, repeated samples were assigned the same rank. Then,
the size of the uniform distributions was adjusted (with the γ function) ensuring that the sum of
the estimated densities of YA and YB remains constant even in the case of repeated observations.

7.2 Limitations and future work

Just like with other methods, the number of samples determines in part the stability of the
results. With a small sample size, the conﬁdence band of the cumulative diﬀerence-plot will be
larger. There are three reasons why a larger sample size increases the stability of the plot: i) we
are doing a kernel density estimation, and a higher sample size Danica and power (2009) implies
that the estimation is closer to the real distribution, ii) the bootstrap method also requires

24

PL−EDAPL−GS−1.0−0.50.00.51.00.000.250.500.751.00xdiff(x)several samples to be meaningful Chernick (2011); Hall (2013) and iii) the sample size needs to
be reasonable with respect to the quantiles being estimated. For example, it would not make
sense to use 10 samples to estimate a 1% quantile. In all of these cases, however, determining
what is a too small sample size is a highly debated question, and is beyond the scope of this
paper. To be on the safe side, we recommend using a sample size of at least n = 100. It is worth
noting that this was arbitrarily chosen, and a suitable sample size should be chosen depending
on the desired conclusions (for example, comparing small and big quantiles requires more data).
With n = 100 we ensure that the comparison of 1% quantiles in the cumulative diﬀerence-plot
is meaningful.

The most obvious limitation of the proposed approach is in its applicability: it should only be
used in case of doubt between two random variables, and when none of the random variables
dominates the other one. Otherwise, there are more suitable alternatives such as Bayesian
analysis Benavoli et al. (2014); Calvo et al. (2019), or directly comparing box-plots. For instance,
if we take 103 samples of XA and XB and all samples of XA are lower than all samples of XB,
then there is no need for further statistical comparison, as the results speak for themselves.

The proposed approach assumes XA and XB are continuous random variables and that all
samples of both XA and XB are i.i.d, and consequently, it cannot be used with paired data. As
future work, the proposed methodology could be extended for paired data and ordinal random
variables. Also, the bootstrap method is the slowest part of the cumulative diﬀerence-plot,
especially as the number of samples increases. To increase the computation speed, this slow part
was written in C++ (the rest of the package was written in R R Core Team (2020)). However,
its speed can probably be further improved with a better implementation.

8 Conclusion

In this paper, we approached the problem of choosing between two random variables in terms of
which of them takes lower values. We proposed eight desirable properties for dominance mea-
sures: functions that compare random variables in the context of quantifying the dominance.
Among the measures in the literature, we found out that the probability that one of the ran-
dom variables takes lower values than the other was the one that satisﬁes the most properties.
However, it fails to satisfy Property 1, hence it cannot be used to determine when one of the
random variables stochastically dominates the other. To overcome this limitation, we introduced
a new dominance measure: the dominance rate, which quantiﬁes how much higher one of the
cumulative distribution function is than the other.

Based on the above, we proposed a cumulative diﬀerence-plot that allows two random variables
to be compared in terms of which of them takes lower values. This cumulative diﬀerence-plot
contains a comparison of the quantiles, in addition to allowing a graphical estimation of the
dominance rate and the probability that one of the random variables takes lower values than
the others. It also models the uncertainty associated with the estimate through a conﬁdence
band. Finally, in Section 6 we showed that the proposed methodology is suitable to compare
two random variables, especially when they take similar values and other methods fail to give
detailed and clear answers.

Supplementary Material

RVCompare: With this R package, users can compute the CP and CD of two distributions,
given their probability density functions. Furthermore, it can be used to produce the

25

proposed cumulative diﬀerence-plot, given the observed data. (The package can be directly
installed from CRAN and is also available in the GitHub repo https://github.com/
EtorArza/RVCompare)

Reproducibility: Alongside the paper, we provide the code to generate the ﬁgures in this pa-
per and replicate the experimentation. For instructions on how to install the dependencies
and replicate the results, refer to the README.md ﬁle in the repository. (GitHub repo
https://github.com/EtorArza/SupplementaryPaperRVCompare)

Appendices: To keep the length of the paper at a reasonable size, the appendices have been
moved to another document. (The appendices are available for download at https://doi.
org/10.5281/zenodo.6528669).

Acknowledgments

This work was funded in part by the Spanish Ministry of Science, Innovation and Universities
through PID2019-106453GA-I00/AEI/10.13039/501100011033 and the BCAM Severo Ochoa ex-
cellence accreditation SEV-2017-0718; by the Basque Government through the Research Groups
2019-2021 IT1244-19, ELKARTEK Program (project code KK-2020/00049) and BERC 2018-
2021 program.

Disclosure statement

The authors report that there are no competing interests to declare.

References

Badue, C., R. Guidolini, R. V. Carneiro, P. Azevedo, V. B. Cardoso, A. Forechi, L. Jesus,
R. Berriel, T. M. Paixão, F. Mutz, L. de Paula Veronese, T. Oliveira-Santos, and A. F.
De Souza (2021, March). Self-driving cars: A survey. Expert Systems with Applications 165,
113816.

Bauer, P. (1991, June). Multiple testing in clinical trials. Statistics in Medicine 10 (6), 871–890.

Baumgartner, W., P. WeiB, and H. Schindler (1998, September). A Nonparametric Test for the

General Two-Sample Problem. Biometrics 54 (3), 1129.

Benavoli, A., G. Corani, J. Demšar, and M. Zaﬀalon (2017). Time for a change: A tutorial for
comparing multiple classiﬁers through Bayesian analysis. The Journal of Machine Learning
Research 18 (1), 2653–2688.

Benavoli, A., G. Corani, F. Mangili, M. Zaﬀalon, and F. Ruggeri (2014). A Bayesian Wilcoxon
In International Conference on Machine

signed-rank test based on the Dirichlet process.
Learning, pp. 1026–1034. PMLR.

Benjamin, D. J., J. O. Berger, M. Johannesson, B. A. Nosek, E.-J. Wagenmakers, R. Berk,
K. A. Bollen, B. Brembs, L. Brown, C. Camerer, D. Cesarini, C. D. Chambers, M. Clyde,
T. D. Cook, P. De Boeck, Z. Dienes, A. Dreber, K. Easwaran, C. Eﬀerson, E. Fehr, F. Fidler,
A. P. Field, M. Forster, E. I. George, R. Gonzalez, S. Goodman, E. Green, D. P. Green, A. G.
Greenwald, J. D. Hadﬁeld, L. V. Hedges, L. Held, T. Hua Ho, H. Hoijtink, D. J. Hruschka,
K. Imai, G. Imbens, J. P. A. Ioannidis, M. Jeon, J. H. Jones, M. Kirchler, D. Laibson, J. List,
R. Little, A. Lupia, E. Machery, S. E. Maxwell, M. McCarthy, D. A. Moore, S. L. Morgan,
M. Munafó, S. Nakagawa, B. Nyhan, T. H. Parker, L. Pericchi, M. Perugini, J. Rouder,
J. Rousseau, V. Savalei, F. D. Schönbrodt, T. Sellke, B. Sinclair, D. Tingley, T. Van Zandt,
S. Vazire, D. J. Watts, C. Winship, R. L. Wolpert, Y. Xie, C. Young, J. Zinman, and V. E.

26

Johnson (2018, January). Redeﬁne statistical signiﬁcance. Nature Human Behaviour 2 (1),
6–10.

Bennet, C. J. (2013). Inference for Dominance Relations. International Economic Review 54 (4),

1309–1328.

Bernardo, J. M. and A. F. Smith (2009). Bayesian Theory, Volume 405. John Wiley & Sons.

Bhattacharyya, A. (1943). On a measure of divergence between two statistical populations
deﬁned by their probability distributions. Bulletin of the Calcutta Mathematical Society 35,
99–109.

Bickel, P. J. and A. M. Krieger (1989). Conﬁdence bands for a distribution function using the

bootstrap. Journal of the American Statistical Association 84 (405), 95–100.

Biswas, M. and A. K. Ghosh (2014, January). A nonparametric two-sample test applicable to

high dimensional data. Journal of Multivariate Analysis 123, 160–171.

Calvo, B. and G. Santafé Rodrigo (2016). Scmamp: Statistical comparison of multiple algorithms

in multiple problems. The R Journal, Vol. 8/1, Aug. 2016 .

Calvo, B., O. M. Shir, J. Ceberio, C. Doerr, H. Wang, T. Bäck, and J. A. Lozano (2019).
Bayesian performance analysis for black-box optimization benchmarking. In Proceedings of
the Genetic and Evolutionary Computation Conference Companion on - GECCO ’19, Prague,
Czech Republic, pp. 1789–1797. ACM Press.

Carreño, A., I. Inza, and J. A. Lozano (2020, June). Analyzing rare event, anomaly, novelty and
outlier detection terms under the supervised classiﬁcation framework. Artiﬁcial Intelligence
Review 53 (5), 3575–3594.

Chatterjee, S. and A. Firat (2007). Generating Data with Identical Statistics but Dissimilar
Graphics: A Follow up to the Anscombe Dataset. The American Statistician 61 (3), 248–254.

Cheng, R. C. and T. Lies (1988). One-sided conﬁdence bands for cumulative distribution func-

tions. Technometrics 30 (2), 155–159.

Cheng, R. C. H. and T. C. Iles (1983, February). Conﬁdence Bands for Cumulative Distribution

Functions of Continuous Random Variables. Technometrics 25 (1), 77–86.

Chernick, M. R. (2011). Bootstrap Methods: A Guide for Practitioners and Researchers, Volume

619. John Wiley & Sons.

Chollet, F. et al. (2015). Keras.

Conover, W. J. and W. J. Conover (1980). Practical nonparametric statistics.

Cruz-Roa, A., H. Gilmore, A. Basavanhally, M. Feldman, S. Ganesan, N. N. Shih,
J. Tomaszewski, F. A. González, and A. Madabhushi (2017, June). Accurate and repro-
ducible invasive breast cancer detection in whole-slide images: A Deep Learning approach for
quantifying tumor extent. Scientiﬁc Reports 7 (1), 46450.

Danica and power (2009). What is the minimum number of data points required for kernel

density estimation?

Devroye, L., A. Mehrabian, and T. Reddad (2020, May). The total variation distance between

high-dimensional Gaussians. arXiv:1810.08693 [math, stat] .

Efron, B. and R. Tibshirani (1993). An Introduction to the Bootstrap. Number 57 in Monographs

on Statistics and Applied Probability. New York: Chapman & Hall.

Faraway, J. J. and Myoungshic Jhun (1990). Bootstrap choice of bandwidth for density estima-

tion. Journal of the American Statistical Association 85 (412), 1119–1122.

27

François-Lavet, V., P. Henderson, R. Islam, M. G. Bellemare, and J. Pineau (2018). An Introduc-
tion to Deep Reinforcement Learning. Foundations and Trends in Machine Learning 11 (3-4),
219–354.

Gelman, A. (2014). Bayesian Data Analysis (Third edition ed.). Chapman & Hall/CRC Texts

in Statistical Science. Boca Raton: CRC Press.

Glorot, X. and Y. Bengio (2010, May). Understanding the diﬃculty of training deep feedforward
neural networks.
In Y. W. Teh and M. Titterington (Eds.), Proceedings of the Thirteenth
International Conference on Artiﬁcial Intelligence and Statistics, Volume 9 of Proceedings of
Machine Learning Research, Chia Laguna Resort, Sardinia, Italy, pp. 249–256. JMLR Work-
shop and Conference Proceedings.

Goeman, J. J. and A. Solari (2014, May). Multiple hypothesis testing in genomics. Statistics in

Medicine 33 (11), 1946–1978.

Goodfellow, I., Y. Bengio, and A. Courville (2016). Deep Learning. MIT press.

Greenland, S., S. J. Senn, K. J. Rothman, J. B. Carlin, C. Poole, S. N. Goodman, and D. G.
Altman (2016, April). Statistical tests, P values, conﬁdence intervals, and power: A guide to
misinterpretations. European Journal of Epidemiology 31 (4), 337–350.

Hall, P. (2013). The Bootstrap and Edgeworth Expansion. Springer Science & Business Media.

Hall, P. and J. Horowitz (2013, August). A simple bootstrap method for constructing nonpara-

metric conﬁdence bands for functions. The Annals of Statistics 41 (4).

Hintze, J. L. and R. D. Nelson (1998). Violin plots: A box plot-density trace synergism. The

American Statistician 52 (2), 181–184.

Ioannidis, J. P. (2018). The proposal to lower P value thresholds to. 005. Jama 319 (14),

1429–1430.

Kailath, T. (1967, February). The Divergence and Bhattacharyya Distance Measures in Signal

Selection. IEEE Transactions on Communications 15 (1), 52–60.

Kullback, S. and R. A. Leibler (1951, March). On Information and Suﬃciency. The Annals of

Mathematical Statistics 22 (1), 79–86.

Larrañaga, P. and J. A. Lozano (2001). Estimation of Distribution Algorithms: A New Tool for

Evolutionary Computation, Volume 2. Springer Science & Business Media.

Ledwina, T. and G. Wyłupek (2012, December). Nonparametric tests for stochastic ordering.

TEST 21 (4), 730–756.

Liese, F. and I. Vajda (2006, October). On Divergences and Informations in Statistics and

Information Theory. IEEE Transactions on Information Theory 52 (10), 4394–4412.

Mann, H. B. and D. R. Whitney (1947). On a test of whether one of two random variables is
stochastically larger than the other. The Annals of Mathematical Statistics 18 (1), 50–60.

Matejka, J. and G. Fitzmaurice (2017). Same stats, diﬀerent graphs: Generating datasets with
varied appearance and identical statistics through simulated annealing. In Proceedings of the
2017 CHI Conference on Human Factors in Computing Systems, pp. 1290–1294.

Mnih, V., K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller
(2013, December). Playing Atari with Deep Reinforcement Learning. arXiv:1312.5602 [cs] .

Panaretos, V. M. and Y. Zemel (2019). Statistical aspects of wasserstein distances. Annual

Review of Statistics and Its Application 6 (1), 405–431.

Papadopoulos, A. (2017). Interpretation of the Kullback-Leibler divergence.

28

Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pret-
tenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Per-
rot, and E. Duchesnay (2011). Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research 12, 2825–2830.

Polyanskiy, Y. and Y. Wu (2012). Lecture notes on information theory. MIT (6.441), UIUC

(ECE 563), Yale (STAT 664).

Quirk, J. P. and R. Saposnik (1962, February). Admissibility and measurable utility functions.

The Review of Economic Studies 29 (2), 140–146.

R Core Team (2020). R: A Language and Environment for Statistical Computing. Vienna,

Austria: R Foundation for Statistical Computing.

Regnier-Coudert, O., J. McCall, M. Ayodele, and S. Anderson (2016). Truck and trailer schedul-
ing in a real world, dynamic and heterogeneous context. Transportation research part E:
logistics and transportation review 93, 389–408.

Rényi, A. et al. (1961). On measures of entropy and information. In Proceedings of the Fourth
Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to
the Theory of Statistics. The Regents of the University of California.

Rousseeuw, P. J. and M. Hubert (2011, January). Robust statistics for outlier detection. WIREs

Data Mining and Knowledge Discovery 1 (1), 73–79.

Santucci, V., J. Ceberio, and M. Baioletti (2020, July). Gradient search in the space of permuta-
tions: An application for the linear ordering problem. In Proceedings of the 2020 Genetic and
Evolutionary Computation Conference Companion, Cancún Mexico, pp. 1704–1711. ACM.

Schiavinotto, T. and T. Stützle (2004). The Linear Ordering Problem: Instances, Search Space
Analysis and Algorithms. Journal of Mathematical Modelling and Algorithms 3 (4), 367–402.

Schmid, F. and M. Trede (1996). Testing for First-Order Stochastic Dominance: A New

Distribution-Free Test. The Statistician 45 (3), 371.

Schuhmacher, D. (2021). Compute The Wasserstein Distance Between Two Univariate Samples.

scikit-learn developers (2021). Density Estimation.

Steck, G. P. (1971). Rectangle probabilities for uniform order statistics and the probability that
the empirical distribution function lies between two distribution functions. The Annals of
Mathematical Statistics 42 (1), 1–11.

Thas, O. (2010). Comparing Distributions. Springer.

Tsybakov, A. B. (2009). Introduction to Nonparametric Estimation. Springer Series in Statistics.

New York ; London: Springer.

Vapnik, V. N. (1998). Statistical Learning Theory. Adaptive and Learning Systems for Signal

Processing, Communications, and Control. New York: Wiley.

Wang, J., F. Cheng, and L. Yang (2013, June). Smooth simultaneous conﬁdence bands for

cumulative distribution functions. Journal of Nonparametric Statistics 25 (2), 395–407.

Wasserstein, R. L. and N. A. Lazar (2016, April). The ASA Statement on p-Values: Context,

Process, and Purpose. The American Statistician 70 (2), 129–133.

Wilcoxon, F. (1945, December).

Individual Comparisons by Ranking Methods. Biometrics

Bulletin 1 (6), 80.

Wilk, M. B. and R. Gnanadesikan (1968, March). Probability Plotting Methods for the Analysis

of Data. Biometrika 55 (1), 1.

29

Xi, a. (2017). Diﬀerences between Bhattacharyya distance and KL divergence.

Zhang, L., S. Wang, and B. Liu (2018, July). Deep learning for sentiment analysis: A survey.

Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8 (4).

30

Appendices

9 A literature review of measures

9.1 f -divergences

The f -divergence is a family of functions that can be used to measure the diﬀerence between
two random variables. Given a strictly convex13 function f : (0, +∞) → R with f (1) = 0, and
two continuous random variables XA and XB, the f -divergence Liese and Vajda (2006); Rényi
et al. (1961) is deﬁned as

Df (XA, XB) =

(cid:90)

R

gB(x)f

(cid:19)

(cid:18) gA(x)
gB(x)

dx

(4)

where gA and gB are the probability density functions of the random variables XA and XB
respectively. Since gB(x) can be 0, we assume Polyanskiy and Wu (2012) that 0 · f (0/0) = 0
and 0 · f (a/0) = limx→0+ x · f (a/x). Notice that if gA and gB are the same probability density
functions, then Df (XA, XB) = 0.

Kullback–Leibler divergence: The Kullback–Leibler divergence Kullback and Leibler (1951) is a
particular case of the f -divergence, for f (x) = x · ln(x). Given two random variables XA and
XB, DKL(XA, XB) can be interpreted Papadopoulos (2017) as the amount of entropy increased
by using gB to model data that follows the probability density function gA.

The Kullback–Leibler divergence is non-negative, and non symmetric DKL(XA, XB) (cid:54)= DKL(XB, XA),
and therefore, it is not actually a distance Goodfellow et al. (2016). It will not satisfy Prop-
erty (2), as it is not antisymmetric either. This also makes the interpretation less intuitive.
The Kullback–Leibler divergence is often used to measure the diﬀerence between two random
variables Goodfellow et al. (2016), but since DKL(XA, XB) (cid:54)= DKL(XB, XA), it may be better
to interpret the Kullback–Leibler divergence as stated above Papadopoulos (2017).

In Figure 18, we show the probability density functions and cumulative distribution func-
tions of four random variables XA, XB, XC and XD. Looking at their cumulative distribu-
tions (Figure 18b), one can clearly see that XA (cid:31) XB, XB ≶ XC and XB (cid:31) XD. How-
ever, as shown in Table 9.1, DKL(XB, XA) = DKL(XB, XC) = DKL(XB, XD) = 15.4 and
DKL(XA, XB) = DKL(XC, XB) = DKL(XC, XD) = 6.2. This means that, given any two ran-
dom variables XA and XB, the Kullback-Leibler is not able to distinguish if XA (cid:31) XB, XB (cid:31) XA
or XA ≶ XB. We can interpret this as the Kullback–Leibler divergence only caring about the
diﬀerence between two random variables, and not if this diﬀerence is related to one of the ran-
dom variables taking lower values than the other. Hence, it cannot satisfy Property 1, even if
we try to transform it to be deﬁned in the [0, 1] interval. We conclude that the Kullback–Leibler
divergence is not suitable to gain information regarding which of the random variables takes
lower values.

Jensen-Shannon divergence: The Jensen-Shannon divergence Polyanskiy and Wu (2012) is very
similar to the Kullback-Leibler divergence, and is another the particular case of the f -divergence
for f (x) = x · ln( 2x
It is also known as the symmetrized version of the Kull-
back–Leibler divergence Polyanskiy and Wu (2012), because

x+1 ) + ln( 2

x+1 ).

DJS(XA, XB) = DKL (XA, XM) + DKL (XB, XM)

13A function f : (0, +∞) → R is strictly convex if for all t ∈ [0, 1], for all x1, x2 ∈ (0, +∞), f (tx1 + (1 − t)x1) <

tf (x1) + (1 − t)f (x2)

31

(a) Probability density.

(b) Cumulative distribution function.

Figure 18: The probability density function and cumulative distribution of the four random
variables. The distances between these random variables are listed in Table 9.1.

(a) Probability density.

(b) Cumulative distribution function.

Figure 19: The probability density function and cumulative distribution of four other random
variables. The Wasserstein distance between XB and each of the other random variables is 0.017.

32

0.150.200.250.300.35x204060Probabilitydensity0.150.200.250.300.35x0.20.40.60.81.0Cumulative density function0.160.180.200.220.240.26x10203040Probabilitydensity0.160.180.200.220.240.26x0.20.40.60.81.0Cumulative density functionJensen-Shannon
RV2
XA XB XC XD
1.4
1.2
0.8
0.0

1.2
0.0
1.2
1.2

1.4
1.2
0.0
0.8

XA 0.0
XB 1.2
XC 1.4
XD 1.4

Hellinger
RV2
XA XB XC XD
XA 0.00 1.28 1.41 1.41
XB 1.28 0.00 1.28 1.28
XC 1.41 1.28 0.00 0.99
XD 1.41 1.28 0.99 0.00

CP
RV2
XA XB XC XD
XA 0.50 0.95 0.92 0.99
XB 0.05 0.50 0.54 0.95
XC 0.08 0.46 0.50 0.91
XD 0.01 0.05 0.09 0.50

Kullback–Leibler
RV2
XA XB XC XD
88.8
0.0
XA
15.4
XB 15.4
2.6
XC 29.4
0.0
XD 88.8

28.6
15.4
0.0
2.6

6.2
0.0
6.2
6.2

Total variation
RV2
XA XB XC XD
XA 0.000 0.934 0.999 1.000
XB 0.934 0.000 0.934 0.934
XC 0.999 0.934 0.000 0.818
XD 1.000 0.934 0.818 0.000

Wasserstein
RV2
XA XB XC XD
0.12
0.03
0.04
0.06
0.000 0.083
0.083 0.000

0.06
0.00
0.04
0.06

XA 0.000
XB 0.06
XC 0.03
XD 0.12

CD
RV2
XA XB XC XD
1.00
1.00
1.00
0.50

1.00
0.50
0.41
0.00

1.00
0.59
0.50
0.00

XA 0.50
XB 0.00
XC 0.00
XD 0.00

1
V
R

1
V
R

1
V
R

1
V
R

1
V
R

1
V
R

1
V
R

Table 2: C(RV1, RV2) for the random variables XA, XB, XC and XD shown in Figure 18.

where the probability density function of XM is gM(x) = 0.5(gA(x) + gB(x)). Thus, we can
interpret this divergence as the sum of the Kullback–Leibler divergences of gA and gB with
respect to the average probability density function gM. The Jensen-Shannon divergence also
fails to identify (see Table 9.1) the dominance relationships between XB and the rest of the
random variables in Figure 18, thus, it cannot satisfy Property 1.
In addition, the Jensen-
Shannon divergence also fails to satisfy Properties 2 and 3. See Table 1 for a detailed list of the
properties that each measure satisﬁes.

Total variation: The total variation Polyanskiy and Wu (2012) is also a particular f -divergence,
for f (x) = 1
2 |x − 1|. Unlike the Kullback–Leibler divergence, the total variation is symmetric. In
fact, it is a properly deﬁned distance Tsybakov (2009); Polyanskiy and Wu (2012). In addition,
it is deﬁned between 0 and 1.

Given two random variables XA, XB, the total variation can also be deﬁned as:

T V (XA, XB) = supC⊆R |PA(C) − PB(C)|,

where PA and PB are the probability distributions14 of XA and XB respectively. Since the
subset C that takes the supremum is C = {x ∈ R | gA(x) > gB(x)}Devroye et al. (2020), we can
interpret the total variation as the “size” of the diﬀerence in the density functions in all points
where gA is more likely than gB. Following this intuition, when T V (XA, XB) = 1, gA and gB
have disjoint supports Polyanskiy and Wu (2012), and thus XA and XB are at their maximum
diﬀerence with respect to this metric. On the other hand, when T V (XA, XB) = 0 the random
variables are identical.

14Given the random variable XA deﬁned in R, its probability distribution, noted as PA, is a mapping that,

for all U ⊆ R that is measurable, A(U ) = P(XA ∈ U ) Vapnik (1998).

33

The Total-Variance also fails to identify (see Table 9.1) the dominance relationships between XB
and the rest of the random variables in Figure 18.

√

Hellinger distance and the Bhattacharyya distance: The Hellinger distance is the square root of
x)2 Polyanskiy and Wu (2012). It is related to the Bhat-
the f -divergence for f (x) = (1 −
tacharya coeﬃcient, since DH (XA, XB) = 2(1 − BhattCoef(XA, XB)) Xi (2017); Polyanskiy and
Wu (2012), where BhattCoef(XA, XB) is the Bhattacharyya coeﬃcient Kailath (1967); Bhat-
(cid:112)gA(x)gB(x)dx, and
tacharyya (1943). This coeﬃcient is deﬁned as BhattCoef(XA, XB) = (cid:82)
has proven useful on signal processing Kailath (1967). Given two probability density functions
gA and gB, the Bhattacharyya coeﬃcient can be interpreted as the integral of the geometric
mean of the probability density functions. The Bhattacharyya coeﬃcient is also related to the
Bhattacharyya distance, as DBhatt(XA, XB) = − ln(BhattCoef(XA, XB)).

R

The Hellinger distance and the Bhattacharyya distance also fail to identify (see Table 9.1) the
dominance relationships between XB and the rest of the random variables in Figure 18.

9.2 Wasserstein distance

The Wasserstein distance is another type of distance between probability random variables.
Given two continuous random variables XA, XB, the Wasserstein distance (of order 1) is deﬁned
as Schuhmacher (2021); Panaretos and Zemel (2019)

DW (XA, XB) =

(cid:90)

R

|GA(x) − GB(x)|dx

In Figure 19, we show a diﬀerent set of four random variables XA, XB, XC and XD. In this case,
it is also clear that XA (cid:31) XB, XB ≶ XC and XB (cid:31) XD (Figure 19b), but DW (XB, XA) =
DW (XB, XC) = DW (XB, XD) = 0.017. Therefore, in this case, the Wasserstein distance does
not give any insights about the dominance between XB and the rest of the random variables, thus,
it cannot satisfy Property 1 even with a transformation. It also does not satisfy Properties 2, 3,
6, 7, 8.

However, with a small change, the Wasserstein distance can comply with Properties 2 and 3. This
change also improves its correlation with the dominance, even though it still does not comply
with Property 1. We remove the absolute value, such that the signed Wasserstein distance is
deﬁned as

DSW (XA, XB) =

(cid:90)

R

GA(x) − GB(x)dx.

For the random variables in Figure 19, the signed Wasserstein distance has diﬀerent values:
DSW (XB, XA) = 0.17, DSW (XB, XC) = 0 and DSW (XB, XD) = −0.017. Notice that

XA (cid:31) XB =⇒ DSW (XB, XA) > 0 and XB (cid:31) XA =⇒ DSW (XB, XA) < 0,

but unfortunately, when XA ≶ XB, DSW (XB, XA) could be positive or negative. This implies
that DSW (XB, XA) still can not determine if XA (cid:31) XB, XB (cid:31) XA, or XA ≶ XB.

9.3 Heuristic derivation of the ﬁrst-order stochastic dominance

A measure similar to the Wasserstein distance has been proposed in the literature Schmid and
Trede (1996) in the context of comparing random variables. Speciﬁcally, this measure is part

34

of the heuristic derivation of a distribution-free statistical test for ﬁrst-order stochastic domi-
nance Schmid and Trede (1996). Given two random variables XA, XB, this measure is deﬁned
as

CI (XA, XB) =

(cid:90)

R

max(0, GA(x) − GB(x))dGB(x).

Note that the values of I range between 0 and 0.5. When CI (XA, XB) = 0.5, we know that
XA (cid:31) XB. Unfortunately, when CI (XA, XB) ∈ (0, 0.5), it could be that XA (cid:31) XB or XA (cid:7) XB.
Consequently, CI (XA, XB) cannot satisfy Property 1.

10 Quantile random variables

10.1 Computing the probability density functions of YA and YB

In Section 4.1 we introduced the quantile random variables YA and YB. We now describe how
to compute the probability density functions of gYA and gYB step by step, with the pseudocode
shown in Algorithm 1. We deﬁne a function r that returns the position of an observation
according to its rank in the sorted list of the observation An ∪ Bn (lines 1–4). The ranks go
from 0 (for the smallest observation) to rmax (for the largest), where rmax is the number of
unique observation in An ∪ Bn minus 1. Repeated observations are assigned the same rank,
and no ranks are skipped: there is at least a value in a ∪ b corresponding to each rank from
0 to rmax. For each observation in {a1, ..., an}, a uniform distribution deﬁned in the interval
( r(ai)+γ(r(ai)−1)
) is added to the mixture (lines 10–19), where γ(k) (lines 7–9)
2n
counts the number of ranks in An ∪ Bn that are lower than or equal to k (since the lowest
rank is 0, γ(−1) = 0) . The kernel density estimation for YB is deﬁned similarly, but with the
observations {b1, ..., bn} instead.

, r(ai)+γ(r(ai))
2n

10.2 The quantile random variables have the same CP and CD as the

kernel density estimates of XA and XB.

In Section 4.1, we claimed that when a “small enough” uniform scikit-learn developers (2021)
kernel is used in the kernel density estimations of XA and XB, these estimations will have the
same CP and CD as the quantile random variables YA and YB. Speciﬁcally, the size of the
2|ai − bj|, where An = {a1, ..., an} and
uniform kernels needs to be smaller than

min
i,j∈{1...n}|ai(cid:54)=bj

Bn = {b1, ..., bn} are the n observed samples of XA and XB respectively. As a result, the CP
and CD of the kernel density estimations will not change when the size of the kernels is reduced
below its initial size. This can be deduced from Property 8 in Section 2.2, which both CP and
CD satisfy.

The quantile random variables YA and YB can also be obtained by applying a sequence of
transformations to the kernel density estimations (with small uniform kernels) of XA and XB.
Three consecutive transformations are required, none of which modify the CD and CP due to
Property 8. The ﬁrst transformation involves further reducing the size of the kernels to 1/(4n).
Secondly, each kernel k is moved into the position r(k)/(2n) + (4n)−1, where r(k) is the rank
of the sample in k in An ∪ Bn. In the case of ties, r assigns the same rank to all kernels and
this same rank is the average of the previous and the next rank. Since each of the possible
positions are at distance 1/(2n) from each other, this transformation will not change the CD and
CP . Finally, the length of the kernels is increased to mult/(4n), where mult is the number of

35

Algorithm 1: Kernel density estimation of YA and YB

Input:
An = {a1, ..., an}: The n observed samples of XA.
Bn = {b1, ..., bn}: The n observed samples of XB.

Output:
gYA : The probability density of YA.
gYB : The probability density of YB.

/* Compute the ranks of An ∪ Bn. The lowest value has rank 0.

Assign the same rank to

ties without skipping any rank.

1 for i = 1,...,n do
2

r(ai) ← rank of ai in An ∪ Bn
r(bi) ← rank of bi in An ∪ Bn

3
4 end
5 R ← {r(a1), ..., r(an), r(b1), ..., r(bn)}
6 rmax ← max(R)
7 for k = -1,0,1,...,rmax do
8
9 end

γ(k) ← the number of items in R lower than or equal to k

/* The probability density function of gYA is represented as a mixture of n uniform
2n , s+1
2n ).

gYA [s] is the probability density of YA in the interval [ s

distributions.

10 gYA ← array of zeros of length 2n
11 gYB ← array of zeros of length 2n
12 for xi = a1, ..., an, b1, ..., bn do
13

Amult ← number of times that xi is in An
Bmult ← number of times that xi is in Bn
for mult = 1,...,(Amult + Bmult) do

gYA [γ(r(ai) − 1) + mult − 1] ← (n · Amult)−1
gYB [γ(r(bi) − 1) + mult − 1] ← (n · Bmult)−1

14

15

16

17

*/

*/

end

18
19 end
20 return gYA , gYB

36

times that the sample deﬁning the kernel is repeated in An ∪ Bn. Note that this increase in the
length will in no case cause an overlap of kernels.

11

CP and CD in the cumulative diﬀerence-plot

In this section, we mathematically prove and experimentally verify that the cumulative diﬀerence-
plot can be used to deduce CD and CP . First, we describe which estimators are used when these
dominance measures are visually estimated from the cumulative diﬀerence-plot. Then, we show
that these estimators converge to CP and CD as the number of samples increases.

11.1 Estimating CP and CD from the cumulative diﬀerence-plot

Deﬁnition 7. (observations of random variables)
Let XA be a continuous random variable. We deﬁne n observations of XA as the realizations of
the i.i.d random variables {X i

i=1 that are distributed as XA, denoted as An = {ai}n

i=1.

A}n

Deﬁnition 8. (estimation of CP )
Let XA and XB be two continuous random variables and An and Bn their n observations
respectively. We deﬁne the estimation of the probability that XA < XB as

(cid:102)CP (An, Bn) =

(cid:88)

i,k=1...n

sign(bk − ai)
2n2

+

1
2

.

Deﬁnition 9. (estimation of CD)
Let XA and XB be two continuous random variables and An and Bn their n observations
respectively. Let {cj}2n
j=1 the sorted list of all the observations of An and Bn where c1 is the
smallest observation and c2n the largest. Let {cd}dmax
be the sorted list of unique values in
d=1
{cj}2n

j=1. We deﬁne the estimation of the dominance rate as

(cid:102)CD(An, Bn) =

(cid:80)2n
j=1

ψ(cj)
2n
2

+ 1

· k−1
c

kc =

(cid:80)2n

j=1 I[ψ(cj )(cid:54)=0]
2n

is the normalization constant and ψj is deﬁned as

37

ψ(cd) =






0

1

1

−1

−1

1 − 2γ(cd)

2γ(cd) − 1

if ˆGA(cd−1) = ˆGB(cd−1)
and ˆGA(cd) = ˆGB(cd)

if ˆGA(cd−1) ≥ ˆGB(cd−1)
and ˆGA(cd) > ˆGB(cd)

if ˆGA(cd−1) > ˆGB(cd−1)
and ˆGA(cd) ≥ ˆGB(cd)

if ˆGB(cd−1) ≥ ˆGA(cd−1)
and ˆGB(cd) > ˆGA(cd)

if ˆGB(cd−1) > ˆGA(cd−1)
and ˆGB(cd) ≥ ˆGA(cd)

if ˆGB(cd−1) > ˆGA(cd−1)
and ˆGA(cd) > ˆGB(cd)

if ˆGA(cd−1) > ˆGB(cd−1)
and ˆGB(cd) > ˆGA(cd)

ˆGB(cd−1) − ˆGA(cd−1)
[Bn = cd] − [An = cd]

. Note that [An = cd] counts the number of items in An
with γ(cd) =
equal to cd and ˆGA is the empirical distribution Steck (1971) estimated from An. To improve
the readability, we abuse the notation and assume that ˆGA(c0) = 0.

We now show that these estimates can be directly computed from the cumulative diﬀerence
plot. First, we show that the estimation of CP from the cumulative diﬀerence-plot is equivalent
to the estimation in Deﬁnition 8. As mentioned in Section 4.3, the CP estimated from the
cumulative diﬀerence-plot is 0.5 + (cid:82) 1
0 diﬀ(x)dx where diﬀ is the diﬀerence function introduced
in Equation (2). Speciﬁcally, the diﬀerence function was deﬁned as diﬀ(x) = GYA (x) − GYB (x).

Lemma 1. Let XA and XB be two continuous random variables and An and Bn their n obser-
vations respectively. Then,

(cid:90) 1

0

diﬀ(x)dx =

2n
(cid:88)

j=1

GYA ( j

2n ) − GYB ( j

2n )

2n

Proof. Considering that the density functions of YA and YB are constant in each interval [ j
for j = 0, ..., (2n − 1), we get that

2n , j+1
2n )

(cid:82) j+1

2n
j
2n

diﬀ(x)dx =

diﬀ( j

2n ) + diﬀ( j+1
2n )

4n

=

GYA ( j

2n ) − GYB ( j

2n ) + GYA ( j+1
4n

2n ) − GYB ( j+1
2n )

Taking into account that GYA (0) = GYB (0) = 0 and GYA (1) = GYB (1) = 1,

(cid:82) 1
0 diﬀ(x)dx = (cid:80)2n−1

j=0

(cid:82) j+1

2n
j
2n

diﬀ(x)dx =

GYA ( 0

2n ) − GYB ( 0

2n ) + GYA ( 2n

2n ) − GYB ( 2n
2n )

4n

+

38

(cid:80)2n−1
j=1

(cid:80)2n−1
j=1

2 · GYA ( j

2n ) − 2 · GYB ( j

2n )

4n

GYA( j

2n ) − GYB ( j

2n )

2n

=

Finally, since GYA (1) = GYB (1) = 1, we have that

(cid:80)2n−1
j=1

GYA( j

2n ) − GYB ( j

2n )

2n

(cid:80)2n
j=1

GYA( j

2n ) − GYB ( j

2n )

2n

=

39

Proposition 3. (CP estimated from the cumulative diﬀerence-plot)
s Let XA and XB be two random variables and An and Bn their n observations respectively. Let
diﬀ be the diﬀerence function obtained from the samples An and Bn as deﬁned in Equation (2).
Then,

(cid:102)CD(An, Bn) =

(cid:90) 1

0

diﬀ(x)dx +

1
2

Proof. Given the observations An and Bn, we need to prove that

(cid:80)

i,k=1...n

sign(bk − ai)
2n2

+ 1

2 = (cid:82) 1

0 diﬀ(x)dx + 1

2

With Lemma 1, it is enough to prove that

(cid:80)

i,k=1...n

sign(bk − ai)
2n2

= (cid:80)2n
j=1

GYA( j

2n ) − GYB ( j

2n )

2n

Let C2n = {cj}2n
observation and c2n the largest. Then, we have that

j=1 be the list of all the sorted observations of An and Bn where c1 is the smallest

GYA (

j
2n

) =

[An < cj] +

[An = cj][k ≤ j|ck = cj]
[C2n = cj]

n

and

GYB (

j
2n

) =

[Bn < cj] +

[Bn = cj][k ≤ j|ck = cj]
[C2n = cj]

n

where [An < cj] counts the number of items in An lower than cj, and [k ≤ j|ck = cj] counts the
number of items in C2n equal to cj but with a lower or equal position in C2n. Therefore, we
have that

2n
(cid:88)

j=1

GYA ( j

2n ) − GYB ( j

2n )

2n

=

[An < cj] +

[An = cj][k ≤ j|ck = cj]
[C2n = cj]

− [Bn < cj] −

[Bn = cj][k ≤ j|ck = cj]
[C2n = cj]

2n2

2n
(cid:88)

j=1

[An < cj] − [Bn < cj] +

([An = cj] − [Bn = cj])[k ≤ j|ck = cj]
[C2n = cj]

2n2

2n
(cid:88)

j=1

(5)

Now we group the terms in Equation (5) into dmax groups such that each group contains all the
terms with the same cj, and each group d contains [C2n = cd] terms, with cj = cd.

2n
(cid:88)

j=1

[An < cj] − [Bn < cj]
2n2

+

dmax(cid:88)

(cid:88)

d=1

cj

([An = cj] − [Bn = cj])[k ≤ j|ck = cj]
[C2n = cj]
2n2

=

40

2n
(cid:88)

j=1

[An < cj] − [Bn < cj]
2n2

+

dmax(cid:88)

d=1

([An = cd] − [Bn = cd])(([C2n = cd] + 1) · [C2n = cd]/2)
[C2n = cd]
2n2

=

2n
(cid:88)

j=1

[An < cj] − [Bn < cj]
2n2

+

dmax(cid:88)

d=1

([An = cd] − [Bn = cd])([C2n = cd] + 1)/2
2n2

=

2n
(cid:88)

j=1

[An < cj] − [Bn < cj]
2n2

+

dmax(cid:88)

d=1

([An = cd] − [Bn = cd])([C2n = cd])/2 + ([An = cd] − [Bn = cd])/2
2n2

=

2n
(cid:88)

j=1

[An < cj] − [Bn < cj]
2n2

+

2n
(cid:88)

j=1

([An = cj] − [Bn = cj])/2
2n2

+

dmax(cid:88)

d=1
(cid:124)

([An = cd] − [Bn = cd])/2
2n2

=

(cid:123)(cid:122)
=0

(cid:125)

2n
(cid:88)

j=1
(cid:124)

[An < cj] − [Bn < cj]
2n2

+

(cid:123)(cid:122)
ﬁrst sum

(cid:125)

2n
(cid:88)

j=1
(cid:124)

([An = cj] − [Bn = cj])/2
2n2

(cid:123)(cid:122)
second sum

(cid:125)

Focusing on the ﬁrst sum, we have that

2n
(cid:88)

j=1

[An < cj] − [Bn < cj]
2n2

=

(cid:80)2n

j=1[An < cj] − (cid:80)2n

j=1[Bn < cj]

2n2

=

(cid:80)2n
j=1

(cid:80)n

k=1

(cid:80)n

k=1

(cid:80)n

i=1[{ai} < cj] − (cid:80)2n

j=1

2n2

(cid:80)n

i=1[{ai} < ak] + (cid:80)n

k=1

2n2

(cid:80)n

i=1[{bi} < ak] + (cid:80)n

k=1

2n2

(cid:80)n

i=1[{bi} < cj]

(cid:80)n

i=1[{ai} < bk]

(cid:80)n

i=1[{bi} < bk]

=

−

=

(cid:80)n

k=1

(cid:80)n

k=1

(cid:80)n

i=1[{ai} < ak] + [{ai} < bk] − [{bi} < ak] − [{bi} < bk]

2n2

(cid:80)n

i=1[{ai} < bk] − [{bi} < ak] + [{ai} < ak] − [{bi} < bk]

2n2

=

=

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai) + [{ai} < ak] − [{bi} < bk]

2n2

=

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

+

(cid:80)n

k=1[An < ak] − [Bn < bk]
2n2

From the second sum, we obtain

2n
(cid:88)

j=1

([An = cj] − [Bn = cj])/2
2n2

=

n
(cid:88)

k=1

([An = ak] − [Bn = ak] + [An = bk] − [Bn = bk])/2
2n2

Combining these summations,

41

2n
(cid:88)

j=1

[An < cj] − [Bn < cj]
2n2

+

2n
(cid:88)

j=1

([An = cj] − [Bn = cj])/2
2n2

=

=

=

(cid:80)n

k=1[An < ak] − [Bn < bk]
2n2

+

(cid:80)n

k=1([An = ak] − [Bn = ak] + [An = bk] − [Bn = bk])/2
2n2

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

2n2

+

+

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

(cid:80)n

k=1[An ≤ ak] − [Bn ≤ bk]
2n2

+

(cid:80)n

k=1(−[An = ak] − [Bn = ak] + [An = bk] + [Bn = bk])/2
2n2

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

+

(cid:80)n

k=1[An ≤ ak] − [Bn ≤ bk]
2n2

+

(cid:80)n

k=1(−[C2n = ak] + [C2n = bk])
4n2

=

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

n(n + 1)/2 + (cid:80)dmax
d=1
2n2

+

[An=cd]2−[An=cd]
2

−

n(n + 1)/2 + (cid:80)dmax
2n2

d=1

[Bn=cd]2−[Bn=cd]
2

+

(cid:80)n

k=1(−[C2n = ak] + [C2n = bk])
4n2

=

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

+

(cid:80)dmax
d=1

[An=cd]2−[An=cd]
2

− (cid:80)dmax
2n2
k=1(−[C2n = ak] + [C2n = bk])
4n2

d=1

=

(cid:80)n

[Bn=cd]2−[Bn=cd]
2

+

considering that (cid:80)dmax

d=1

[Bn=cd]−[An=cd]
2

= 0, we simplify the previous equation to

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

+

(cid:80)dmax
d=1

[An=cd]2−[Bn=cd]2
2

2n2

+

(cid:80)n

k=1(−[C2n = ak] + [C2n = bk])
4n2

=

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

+

(cid:80)dmax

d=1 [An = cd]2 − [Bn = cd]2
4n2

+

(cid:80)n

k=1(−[C2n = ak] + [C2n = bk])
4n2

=

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

+

(cid:80)dmax

d=1 [An = cd]2 − [Bn = cd]2
4n2

+

(cid:80)dmax

d=1 (−[C2n = cd][An = cd] + [C2n = cd][Bn = cd])
4n2

=

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

(cid:80)dmax

d=1 [An = cd]2 − [Bn = cd]2
4n2

+

We expand the third sum,

(cid:80)dmax

+

(cid:124)

d=1 [C2n = cd]([Bn = cd] − [An = cd])
4n2
(cid:123)(cid:122)
third sum

(cid:125)

=

(cid:80)dmax

d=1 [C2n = cd]([Bn = cd] − [An = cd])
4n2

=

(cid:80)dmax

d=1 ([Bn = cd] + [An = cd])([Bn = cd] − [An = cd])
4n2

=

(cid:80)dmax

d=1 ([Bn = cd]2 − [An = cd]2)
4n2

42

Finally,

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

(cid:80)dmax

d=1 [An = cd]2 − [Bn = cd]2
4n2

+

+

(cid:80)dmax

d=1 ([Bn = cd]2 − [An = cd]2)
4n2

=

(cid:80)n

k=1

(cid:80)n

i=1 sign(bk − ai)

2n2

Proposition 4. Let XA and XB be two random variables and An and Bn their n observations
respectively. The CD estimated from the cumulative diﬀerence-plot is (cid:102)CD.

Proof. In Section 4.3, we deﬁned the CD estimated from the cumulative diﬀerence-plot as

(cid:82) 1
0 I[diﬀ(x) > 0] − I[diﬀ(x) < 0]dx
2

(cid:82) 1
0 I[diﬀ(x) (cid:54)= 0]dx

+ 1
2

,

CD =

where I is the indicator function. This proposition claims that

(cid:82) 1
0 I[diﬀ(x) > 0] − I[diﬀ(x) < 0]dx
2

(cid:82) 1
0 I[diﬀ(x) (cid:54)= 0]dx
ψ(cj)
2n
2

(cid:80)2n
j=1

+ 1

· k−1
c

.

+ 1
2

=

To prove it, we show that

i) (cid:82) 1

0 I[diﬀ(x) > 0] − I[diﬀ(x) < 0]dx = (cid:80)2n

j=1

ψ(cj)
2n

and

ii) (cid:82) 1

0 I[diﬀ(x) (cid:54)= 0]dx = kc.

Let us focus our attention in i). We split the integral into 2n parts:

(cid:90) 1

0

I[diﬀ(x) > 0] − I[diﬀ(x) < 0]dx =

2n
(cid:88)

(cid:90) j

2n

j=1

j−1
2n

I[diﬀ(x) > 0] − I[diﬀ(x) < 0]dx

(6)

Let C2n = {cj}2n
j=1 be the list of all the sorted observations of An and Bn where c1 is the smallest
observation and c2n the largest and let {cd}dmax
d=1 be the sorted list of unique values in C2n. We
group the terms in the sum of Equation (6) into dmax groups such that for every j in a group,
cj = cd .

43

dmax(cid:88)

(cid:88)

(cid:90) j

2n

d=1

j

j−1
2n

I[diﬀ(x) > 0] − I[diﬀ(x) < 0]dx

Now we join the integrals for every j in each group, such that the j of the integral goes from
jd↓ − 1 to jd↑ (if the sample cd is unique in C2n, then jd↓ = jd↑ = j).

dmax(cid:88)

(cid:90)

jd↑
2n

d=1

jd↓−1
2n

I[diﬀ(x) > 0] − I[diﬀ(x) < 0]dx

(7)

In the interval ( jd↓−1

2n , jd↑

2n ), diﬀ evaluates to one of these four possibilities:

1. diﬀ(x) = 0 for all x ∈ ( jd↓−1

2. diﬀ(x) > 0 for all x ∈ ( jd↓−1

3. diﬀ(x) < 0 for all x ∈ ( jd↓−1

2n , jd↑
2n )
2n , jd↑
2n )
2n , jd↑
2n )

4. diﬀ(x) = 0 in one point in the interval ( jd↓−1

2n , jd↑
2n ) and diﬀ(x) > 0 or diﬀ(x) < 0 for every
other x in the interval. However, we can safely ignore this point as the value of the integral
is invariant to the value of the function in sets of zero measure.

By looking at the empirical distributions ˆGA(x) and ˆGB(x) estimated from An and Bn respec-
tively, we can guess which of these possibilities corresponds to each interval.






1)

2)

2)

3)

3)

4)

4)

if ˆGA(cd−1) = ˆGB(cd−1)
and ˆGA(cd) = ˆGB(cd)

if ˆGA(cd−1) ≥ ˆGB(cd−1)
and ˆGA(cd) > ˆGB(cd)

if ˆGA(cd−1) > ˆGB(cd−1)
and ˆGA(cd) ≥ ˆGB(cd)

if ˆGB(cd−1) ≥ ˆGA(cd−1)
and ˆGB(cd) > ˆGA(cd)

if ˆGB(cd−1) > ˆGA(cd−1)
and ˆGB(cd) ≥ ˆGA(cd)

if ˆGB(cd−1) > ˆGA(cd−1)
and ˆGA(cd) > ˆGB(cd)

if ˆGA(cd−1) > ˆGB(cd−1)
and ˆGB(cd) > ˆGA(cd)

The value of the integral in Equation (7) corresponding to these possibilities are the following:

1. 0

2. [C2n = cd] · 1
2n

44

3. −[C2n = cd] · 1
2n

4. [C2n = cd] · (2 · ld − 1) · 1
2n

where [C2n = cd] counts the number of items in C2n equal to cd and ld is the proportion in which
diﬀ(x) > 0 in the interval ( jd↓−1
2n ). For example, ld = 0.75 would represent that diﬀ(x) > 0
in 75% of the total length of the interval, and diﬀ(x) < 0 in the other 25%.

2n , jd↑

With this, we can rewrite Equation (7) as

dmax(cid:88)

d=1

[C2n = cd] · ψ(cd) ·

1
2n

=

2n
(cid:88)

j=1

ψ(cj)
2n

,

where ψ is the function introduced in Deﬁnition 9.

Now, we only need to prove ii). Speciﬁcally, we need to show that

(cid:90) 1

0

I[diﬀ(x) (cid:54)= 0]dx = kc.

We have that

and

(cid:90) 1

0

I[diﬀ(x) (cid:54)= 0]dx =

dmax(cid:88)

(cid:90)

jd↑
2n

d=1

jd↓−1
2n

I[diﬀ(x) (cid:54)= 0]dx,

kc =

(cid:80)2n

j=1 I[ψ(cj) (cid:54)= 0]
2n

=

dmax(cid:88)

d=1

[C2n = cd]

I[ψ(cd) (cid:54)= 0]
2n

.

Finally, it is easy to see that

(cid:90)

jd↑
2n

jd↓−1
2n

I[diﬀ(x) (cid:54)= 0]dx = [C2n = cd]

I[ψ(cd) (cid:54)= 0]
2n

,

because diﬀ(x) = 0 in the interval ( jd↓−1

2n , jd↑

2n ) if and only if ψ(cd) = 0.

11.2 Convergence of the estimators

Proposition 5. Let XA and XB be two continuous random variables and {ai}i∈N and {bi}i∈N
be two inﬁnite sequences of their observations respectively. Let An and Bn be the two ﬁnite
subsequences that contain the ﬁrst n elements of {ai}i∈N and {bi}i∈N respectively. Then,

CP (XA, XB) = lim
n→∞

(cid:102)CP (An, Bn)

45

Proof. Let {Ps}s∈N be a sequence of estimators with every estimator is determined randomly
with the following procedure:

1) generate two random permutations σs and τs of size n.

2) deﬁne each estimation as

Ps(An, Bn) =

n
(cid:88)

i=1

sign(bσs(i) − aτs(i))
2n

+

1
2

.

It is easy to see that each Ps is an estimator of P(XA < XB) (since XA, XB are continuous,

we know that P(XA = XB) = 0). Now observe that the sequence

(cid:26) (cid:80)s

t=1 Pt(An, Bn)
s

(cid:27)

con-

n∈N

+ 1

2 , which means that (cid:102)CP (An, Bn) is also an

verges to (cid:102)CP (An, Bn) = (cid:80)
estimator of P(XA < XB).

i,k=1...n

sign(bk − ai)
2n2

Unfortunately, the estimator (cid:102)CD will not always converge: CD fails to satisfy Property 7, and
this means that a a few points can still have a big impact in the estimation of CD. Specif-
ically, given the continuous random variables XA and XB deﬁned in N , (cid:102)CD will converge iﬀ
(cid:82)
N I[GA(x) = GB(x)] · (gA + gB)dx = 0.

Luckily, this lack of convergence is not a problem when the estimation of CD is carried out
visually in the cumulative diﬀerence-plot. Since the visual representation of the cumulative
diﬀerence-plot involves rendering the plot with pixels, there exists an small δ > 0 such that
when | diﬀ(x)| < δ, the diﬀerence is displayed as 0.

In practice, we do not even need to account for the case that diﬀ(x) = 0. The cumulative
diﬀerence-plot models the uncertainty with a conﬁdence band, and when diﬀ(x) = 0 is inside
the conﬁdence band, then so are diﬀ(x) > 0 and diﬀ(x) < 0. If we assume that the diﬀerence
is positive, negative or zero every time that diﬀ(x) = 0 is inside the conﬁdence band, we obtain
the estimations (cid:102)CD
, the estimation
of CD with the highest part of the conﬁdence band is an upper bound of CD. The same is true
for the estimation with the lowest part of the conﬁdence band: it is a lower bound of CD.

respectively. Now since (cid:102)CD

and (cid:102)CD

> (cid:102)CD

> (cid:102)CD

, (cid:102)CD

−

−

+

+

=

0

Although (cid:102)CD does not converge to CD, for any (cid:15) > 0 we can ﬁnd a δ small enough such that the
and CD is smaller than (cid:15). We formalize this claim in Conjecture 1, and
diﬀerence between (cid:102)CD
we leave the proof for future work.

δ

Deﬁnition 10. (δ-estimation of CD)
Let XA and XB be two continuous random variables and An and Bn their n observations
respectively. Let {cj}2n
j=1 the sorted list of all the observations of An and Bn where c1 is the
smallest observation and c2n the largest. Let {cd}dmax
be the sorted list of unique values in
d=1
{cj}2n

j=1.

We deﬁne the δ-estimation of CD, denoted as (cid:102)CD
, as the same estimation as (cid:102)CD, but assuming
that the empirical distributions computed from An and Bn are equal when | ˆGA(x) − ˆGB(x)| < δ.

δ

The previous deﬁnition can also be based in the δ-diﬀerence, deﬁned as diﬀ δ(x) = diﬀ(x) when
diﬀ(x) ≥ δ, and diﬀ δ(x) = 0 otherwise.

Conjecture 1. Let XA and XB be two continuous random variables and {ai}i∈N and {bi}i∈N
be two inﬁnite sequences of their observations respectively. Let An and Bn be the two ﬁnite

46

subsequences that contain the ﬁrst n elements of {ai}i∈N and {bi}i∈N respectively. Then, for all
(cid:15) > 0, there exists a δ > 0 such that

(cid:12)
(cid:12)
(cid:12)CD(XA, XB) − lim

n→∞

δ

(cid:12)
(cid:12)
(cid:12) < (cid:15)
(An, Bn)

(cid:102)CD

11.3 Experimental veriﬁcation

In the following, we experimentally verify that the cumulative diﬀerence-plot can be used to
deduce CD and CP . To do so, we deﬁne six pairs of example random variables and measure
the CP and CD with three diﬀerent methods: the deﬁnition of CD and CP (Equation (1) and
Deﬁnition 4), the estimators in Deﬁnitions 8 and 9 and from the cumulative diﬀerence-plot.
The cumulative diﬀerence-plot has a conﬁdence band in addition to the estimation, and this
conﬁdence band allows the lower and upper bounds of CD and CP to be computed.

The probability density functions of the six examples are shown in Figures 20 through 25.
The probability density of these random variables is a mix of normal distributions, the beta
distribution, and the log-normal distribution.

The diﬀerence plot and the estimations were carried out with 5000 samples from each random
variable. The CP and CD values computed are shown in Figures 26 and 27 respectively. In every
case, the estimations with the three methods match, except for CD in Example 4 (Figure 23). This
is a deceptive example because, in most of the probability mass of XA and XB, the cumulative
distribution functions are equal. Consequently, in this example, the estimator of CD introduced in
Deﬁnition 9 is unstable: it is very likely that the estimated empirical distributions are diﬀerent
even though the cumulative distribution functions are identical. Overcoming this limitation
involves choosing a small δ > 0, such that when the diﬀerence between the empirical distributions
is smaller than δ, they are considered equal.

We conclude that, in most cases, the three estimation methods (from densities, using the es-
timators and with the cumulative diﬀerence-plot) yield a similar result, which validates the
statements in the previous section.

Figure 20: Probability density functions of Example 1

47

0501001500.040.050.060.070.080.09xprobability densityX_AX_BFigure 21: Probability density functions of Example 2

Figure 22: Probability density functions of Example 3

Figure 23: Probability density functions of Example 4

48

0501001502000.00.10.20.3xprobability densityX_AX_B024680.00.20.40.6xprobability densityX_AX_B0510150.10.20.30.4xprobability densityX_AX_BFigure 24: Probability density functions of Example 5

Figure 25: Probability density functions of Example 6

Figure 26: The CP values obtained in the six examples with the three methods.

49

012340.000.250.500.751.00xprobability densityX_AX_B0.00.51.01.52.001234xprobability densityX_AX_B0.000.250.500.751.00123456ExampleFrom densityFrom estimatorFrom the estimationof the diﬀerenceFrom the conﬁdence band of the diﬀerenceFigure 27: The CD values obtained in the six examples with the three methods.

50

0.000.250.500.751.00123456ExampleFrom densityFrom estimatorFrom the estimationof the diﬀerenceFrom the conﬁdence band of the diﬀerence