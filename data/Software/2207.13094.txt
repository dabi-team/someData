MD-Bench: A generic proxy-app toolbox for
state-of-the-art molecular dynamics algorithms

Rafael R. L. Machado, Jan Eitzinger, Harald Köstler, and Gerhard Wellein

Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)
Erlangen National High Performance Computing Center (NHR@FAU)

Abstract. Proxy-apps, or mini-apps, are simple self-contained bench-
mark codes with performance-relevant kernels extracted from real appli-
cations. Initially used to facilitate software-hardware co-design, they are
a crucial ingredient for serious performance engineering, especially when
dealing with large-scale production codes. MD-Bench is a new proxy-app
in the area of classical short-range molecular dynamics. In contrast to
existing proxy-apps in MD (e.g. miniMD and coMD) it does not resemble
a single application code, but implements state-of-the art algorithms from
multiple applications (currently LAMMPS and GROMACS). The MD-
Bench source code is understandable, extensible and suited for teaching,
benchmarking and researching MD algorithms. Primary design goals are
transparency and simplicity, a developer is able to tinker with the source
code down to the assembly level. This paper introduces MD-Bench, ex-
plains its design and structure, covers implemented optimization variants,
and illustrates its usage on three examples.

Keywords: proxy app, molecular dynamics, performance engineering

1

Introduction and Motivation

Molecular dynamics (MD) simulations are used in countless research efforts to
assist the investigation and experimentation of systems at atomic level. Both their
system size and timescale are crucially limited by computing power, therefore they
must be designed with performance in mind. Several strategies to speedup such
simulations exist, examples are Linked Cells, Verlet List and MxN kernels from
GROMACS [11, 12]. These improve the performance by exploiting either domain-
knowledge or hardware features like SIMD capabilities and GPU accelerators. MD
application codes can achieve a large fraction of the theoretical peak floating point
performance and are therefore among the few application classes that can make
use of the available compute power of modern processor architectures. Also, cases
scientists are interested in are frequently strong scaling throughput problems, a
single run only exhibits limited parallelism but thousands of similar jobs need
to be executed. This fact combined with arithmetic compute power limitation
makes an optimal hardware-aware implementation a critical requirement. MD
is used in many areas of scientific computing like material science, engineering,
natural science and life sciences.

2
2
0
2

l
u
J

6
2

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

1
v
4
9
0
3
1
.
7
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Rafael Ravedutti Lucio Machado et al.

Proxy-apps are stripped down versions of real applications, ideally they are
self-contained, easy to build and bundled with a validated test case, which can
be a single stubbed performance-critical kernel or a full self-contained small
version of an application. Historically proxy-apps assisted in porting efforts or
hardware-software co-design studies. Today they are a common ingredient of any
serious performance engineering effort of large-scale application codes, especially
with multiple parties involved. Typically proxy-apps resemble a single application
code, e.g., miniMD [3] mimics the performance of LAMMPS [2, 10]. A proxy-app
can be used for teaching purposes and as a starting point for performance oriented
research in a specific application domain.

To investigate the performance of MD applications, we developed MD-Bench
— a standalone proxy-app toolbox implemented in C99 that comprises the most
essential MD steps to calculate trajectories in an atomic-scale system. MD-Bench
contributes clean reference implementations of state-of-the-art MD optimization
schemes. As a result, and in contrast to existing MD proxy-apps, MD-Bench
is not limited to represent one MD application but aims to cover all relevant
contributions. MD-Bench is intended to facilitate and encourage performance
related research for classical MD algorithms. Its applications are low-level code
analysis and performance investigation via fine-grained profiling of hardware
resources utilized by MD runtime-intensive kernels.

This paper is structured as follows: In section 2 we present related work
on proxy-apps for MD simulations, pointing out the differences compared to
MD-Bench. In section 3 we explain the basic theory for MD simulations. In
section 4 we list and discuss current features offered in MD-Bench, besides its
employment on benchmarking, performance analysis and teaching activities. In
section 5 we present cases with analysis studies and results to illustrate how
MD-Bench can be used. Finally section 6 presents the conclusion and outlook,
and a discussion of future work.

2 Related Work

There already exist multiple proxy-apps to investigate performance and portabil-
ity for MD applications. One of the better known examples is Mantevo miniMD,
which contains C++ code extracted from LAMMPS and provides a homoge-
neous copper lattice test case in which the short-range forces can be calculated
with Lennard-Jones (LJ) or embedded atom method (EAM) potentials. It was
used to investigate the performance of single instruction, multiple data (SIMD)
vectorization on the most internal loops of the neighbor-lists building and force
calculation steps [9], as well as to evaluate the portability of MD kernels through
the Kokkos framework, thus executing most of the code on GPU instead of only
the pair force and neighbor-lists. Outcomes from miniMD in LAMMPS include
better SIMD parallelism usage on code generated by compilers and most efficient
use of GPU by avoiding data transfers at all time-steps of the simulation.

ExMatEx coMD is another proxy-app from the material science domain
that focuses on co-design to evaluate the performance of new architectures and

MD-Bench proxy-app toolbox

3

programming models. Besides allowing users to extend and/or re-implement the
code as required, the co-design principle also permits to evaluate the performance
when switching strategies, for example using Linked-Cells for force calculations
instead of Verlet Lists.

ExaMiniMD is an improved and extended version of miniMD with enhanced
modularity that also uses Kokkos for portability. Its main components such
as force calculation, communication and neighbor-list construction are derived
classes that access their functionality through virtual functions.

In previous work we developed tinyMD [8], a proxy-app (also based on
miniMD) created to evaluate the portability of MD applications with the AnyDSL
framework. tinyMD uses higher-order functions to abstract device iteration loops,
data layouts and communication strategies, providing a domain-specific library
to implement pair-wise interaction kernels that execute efficiently on multi-CPU
and multi-GPU targets. Its scope is beyond MD applications, since it can be
used to simulate any kind of particle simulation that relies on short-range force
calculation such as the discrete element method (DEM).

Beyond proxy-apps, performance-engineering of MD can be achieved via auto-
tuning by either running simulations as a black-box for finding optimal system
and hardware specific simulation parameters [4] or by providing programming
interfaces that dynamically tune the application at run-time by selecting the best
optimization strategies and data layouts [5].

MD-Bench differs from available offerings because it was primarily developed
to enable an in-depth analysis of software-hardware interaction. With that in
mind, MD-Bench provides a stubbed variant to evaluate the force calculation
at different levels of the memory hierarchy, as well as a gather benchmark that
mimics the memory operations used in those kernels, thus allowing investigation of
the memory transfers without side-effects from arithmetic operations. In contrast
to other proxy-apps MD-Bench contains optimized algorithms from multiple
MD applications and allows to compare those using the same test cases. Apart
from standard Verlet Lists algorithms it contains state-of-the-art optimization
strategies as, e.g., the GROMACS MxN kernels, which attain higher data level
parallelism in modern architectures by using a more SIMD-friendly data layout
and up to now are not yet available as part of a simple proxy-app. Although
the significant majority of MD-Bench code is implemented in C, it also relies on
SIMD intrinsics and assembly code kernels allowing low-level tweaking of the
code without interference from a compiler.

3 Background and Theory

Fundamentally, atom trajectories in classical MD systems are computed by
integrating Newton’s second law equation (Equation 1) after computing the
forces, which are described by the negative gradient of the potential of interest.
Equation 2 shows how to compute each interaction for the LJ potential, with
xij being the distance vector between atoms i and j, ϵ being the width of the

4

Rafael Ravedutti Lucio Machado et al.

potential well, and σ specifying at which distance the potential is 0.

F = m ˙v = ma

F LJ
2

(xi, xj) = 24ϵ

(cid:19)6 "
2

(cid:18) σ
xij

(cid:19)6

(cid:18) σ
xij

#

− 1

xij
|xij|2

(1)

(2)

To optimize the computation of short-range potentials, only atom pairs within
a cutoff radius may be considered because contributions become negligible at
long-range interactions. Hence, a Verlet List (see Fig. 1(a)) can be regularly
created for each atom to track neighbor candidates within a specific radius r,
which is the cutoff radius plus a small value (verlet buffer).

(a) Verlet List

(b) GROMACS MxN

(c) Stubbed Patterns

Fig. 1. (a, b) Pair list creation for the red atom (cluster) in Verlet List (GROMACS
MxN), blue atoms (clusters) are evaluated and the ones within the green circle with
radius r are added to the pair list. Cell size s must be greater or equal than r. (c)
4-length neighbor-lists for atoms 0 (purple) and 1 (orange) in Stubbed Case patterns.

4 MD-Bench Features

Figure 2 depicts MD-Bench1,2 features. To facilitate experimentation with a
range of settings that influence performance, a robust build system with various
configurations from the compiler and flags to whether atom types should be
explicitly stored and loaded from memory is available. Due to its modularity, the
build system permits to replace kernels at the assembly level. Particularly, we
maintain simplified C versions of each kernel with an eye toward low-level code
analysis and tweaking, which is hardly achievable on production MD applica-
tions due to the massive code base size and extensive employment of advanced

1 https://github.com/RRZE-HPC/MD-Bench
2 MD-Bench is open source and available under LGPL3 License.

rs, s ≥ rs, s ≥ rrrrrrsequential12342345i: vary, j: contiguousﬁxedi: same, j: contiguousrandomi: vary, j:scattered0123012325690378000111MD-Bench proxy-app toolbox

5

Fig. 2. Overview about MD-Bench Features

programming language techniques, which brings in substantial complexity and
makes an analysis more difficult. Apart from that, kernels are instrumented with
LIKWID [13] markers to allow fine-grained profiling of kernels using hardware
performance monitoring (HPM) counters.

4.1 Optimization schemes

Verlet neighbor-lists The most common optimization scheme used in MD
applications to compute short-range forces is arguably the Verlet List algorithm.
It consists of building a neighbor-list for each atom in the simulation, where
the elements are other atoms that lie within a cutoff radius that is higher or
equal than the force cutoff radius. Thus, forces are computed for each atom
by traversing their neighbor-list and accumulating forces for neighbors which
distance is smaller than the cutoff (see Algorithm 1).

Algorithm 1 Force calculation

for i ← 1 to N local do

f ← 0
for k ← 1 to N neighs[i] do
j ← neighbors[i, k]
d ← calculate_distance(i, j)
if d ≤ cutof f _radius then

f ← f + calculate_f orce(d)

end if

end for
f orce[i] ← f orce[i] + f

end for

▷ Number of local atoms
▷ Partial forces (required for parallelism)
▷ Number of neighbors for atom i
▷ k-th neighbor of atom i

▷ Force cutoff check
▷ Depends on the potential used

▷ Accumulate forces

The algorithm contains two major drawbacks with respect to SIMD parallelism:
(a) irregular access pattern, since neighbor atoms are scattered across memory and
(b) no reuse of neighbor data (positions) across atom iterations. The consequence
for them is the requirement of a gather operation to load the neighbor atoms
data into the vector registers for each atom traversed in the outermost loop.

6

Rafael Ravedutti Lucio Machado et al.

There are two strategies to gather data, namely hardware or software gathers.
Hardware gathers on x86-64 processors use a vgather instruction to perform the
gathering at hardware level, where software gathers emulate the gather operation
using separate instructions to load, shuffle and permutate elements within vector
registers. When available in the target processor, hardware gathers clearly use
less instructions, but cannot take advantage of spatial locality in the array of
structures (AoS) layout, thus requiring at least three data transfers instead of
one when keeping elements aligned to the cache line size. Due to this trade-off
between instruction execution and memory transfer, it is not straightforward to
determine the best strategy, as it will depend on the target processor.

To avoid the costs of gathering data and enhance data reuse, a different
data layout is necessary. Therefore, we also introduce the GROMACS MxN
optimization scheme in MD-Bench, which is currently not present in any of the
existing MD proxy-apps. Despite its SIMD-friendly data layout, it introduces
extra computations because interactions are evaluated per clusters instead of per
atom, thus the trade-off between extra computations and overhead for SIMD
parallelism must also be assessed.

For all kernel variants available, MD-Bench contains their half neighbor-lists
counterpart, which take advantage of Newton’s Third Law to compute only half
of the pair interactions, thus decreasing the amount of partial forces to calculate.
Despite the clear benefit of having less operations, this strategy harms parallelism
because it introduces race conditions for the neighbor atoms because forces are
stored back to memory in the innermost loop. Also, gather and scatter operations
are needed for the forces of the neighbor atoms, which not only takes effect in
the instruction execution cost but also increases memory traffic.

MxN cluster algorithm To address the lack of data reuse and necessity of costly
gather operations, the MxN algorithm introduced in GROMACS clusters atoms in
groups of max(M, N ) elements (see Figure 1(b) for M = 4, N = 4 example). Thus,
tailored kernels with SIMD intrinsics are implemented to compute interactions
of MxN atoms. Positions for atoms in the same cluster are contiguously stored in
an array of structures of arrays (AoSoA) fashion, and can be loaded without a
gather operation. In this algorithm, M parametrizes the reusability of data as
atoms in the same i-cluster contains the same pair lists, and therefore a single
load of a j-cluster of size N is enough to compute the interaction among all pairs
of atoms. Since two kernel variants for the force calculation are present (namely
4xN and 2xNN), then N is optimally chosen as either the SIMD width of the
target processor or half of it. Since kernels must be kept simple and bidirectional
mapping between i-clusters and j-clusters is needed, M is either N
2 , N or 2N .
Note that when a cluster size is less than max(M, N ), it is filled in with atoms
placed in the infinity to fail the cutoff checking. Also, different atoms in paired
clusters may not be within the cutoff radius. Hence, choosing large values for
M and N can significantly grow the amount of pairs interactions to compute,
wasting resources and injuring performance.

MD-Bench proxy-app toolbox

7

4.2 Benchmark test cases

Short-range force kernels for LJ and EAM potentials are available, with Copper
face-centered cubic (FCC) lattice and Noble gases (pure argon) setups to embrace
both material modeling and bio-sciences simulation fields. Setups are provided
by providing atoms positions (and velocities in some cases) in a Protein Data
Bank (PDB), Gromos87 or LAMMPS dump file.

4.3 Tools

Detailed statistics mode To collect extended runtime information for the
simulation, a detailed statistics mode can be enabled. It introduces various
statistics counters in the force kernels in order to display relevant performance
metrics such as cycles per SIMD iterations (processor frequency must be fixed),
average cutoff conditions that fail/succeed and useful read data volumes.

Memory traces and gather-bench The gather benchmark is a standalone
benchmark code for x86-64 CPUs (currently) that mimics the data movement
(both operations and transfers) from MD kernels in order to evaluate the “cost
of gather” from distinct architectures. It currently gathers data in the following
patterns: (a) simple 1D arrays with fixed stride, to evaluate single gather instruc-
tion in the target CPU, (b) array of 3D vectors with fixed stride, to evaluate MD
gathers with regular data accesses and (c) array of 3D vectors using trace files
from MD-Bench INDEX_TRACER option, to evaluate MD gathers with irregular
data accesses. The benchmark exploits the memory hierarchy by adjusting the
data volume to fit into a different cache level in successive executions, and yields
a performance metric based on the number of cache lines touched. There are
options to determine the floating point precision and include padding to assure
alignment to the cache line size.

Stubbed force calculation To execute MD kernels in a steady-state and
understand their performance characteristics, we established a synthetical stubbed
case where the number of neighbors per atom remain fixed and the data access
pattern can be predicted. Fig. 1(c) depicts examples for data access patterns
available (namely sequential, fixed and random), indicating whether neighbor-lists
vary or are the same across different atoms in the outermost loops (i) and if
atoms in the lists (j) are contiguous or scattered over memory. It addresses both
irregular data accesses and variations in the inner-most loop size, thus providing a
stable benchmark that can isolate effects caused by distinct reasons like memory
latency and overhead for filling in the CPU instruction pipeline.

5 Examples

For this work experiments, we made use of the following processor architectures:
Intel Cascade Lake: Intel(R) Xeon(R) Gold 6248 CPU at 2.50GHz, with two
20-cores sockets, two threads per core (hyper-threading enabled). Individual
32KB L1 and 1024KB L2 caches for each core, and 28MB of shared L3 cache
for each socket, with one memory domain per socket.

8

Rafael Ravedutti Lucio Machado et al.

Intel Ice Lake: Intel(R) Xeon(R) Platinum 8360Y CPU at 2.40GHz, with
36-cores per chip. Individual 80KB L1 per core (32 instructions + 48 data),
512KB L2 caches for each core, and 54MB of shared L3 cache per chip.

We used LIKWID (V5.2) to fix the CPU frequency, pin tasks to specific cores,
enable/disable prefetchers and make use of HPM counters.

5.1 Assembly analysis

This example illustrates how MD-Bench allows to evaluate the optimizations
performed by compilers at the instruction code level to pinpoint possible im-
provements in the compiler generated code. In the AVX512 generated code from
the Intel compiler, we notice lea and mov instructions before gathering the data
(see Listing 1.1). Removing such instructions does not change the semantics of
the code because the destination registers of the mov operations are not read
afterwards, so we can exclude them to improve performance. For the Copper
FCC lattice case with 200 time-steps, the runtime on Cascade Lake for the force
calculation is about 4.43 seconds without these instructions and about 4.81 with
them, leading to a 8% speedup.

1
2
3
4
5
6
7
8
9
10
11
12

<-

neighs [ k ] * 3

; ymm3
vmovdqu ymm3, [r13+rbx∗4]
vpaddd ymm4, ymm3, ymm3
vpaddd ymm3, ymm3, ymm4
mov r10d, [r13+rbx*4] ;
mov r9d, [4+r13+rbx*4] ;
lea r10d, [r10+r10*2] ;
lea r9d, [r9+r9*2] ;
. . . ; Same
vgatherdpd zmm4{k1}, [16+ rdi +ymm3∗8]
vgatherdpd zmm17{k2}, [8+rdi+ymm3∗8]
vgatherdpd zmm18{k3}, [ rdi +ymm3∗8]

neighs [ k ] * 3
neighs [ k +1] * 3

for k +2, k +3, . . . k +7

neighs [ k +1]

neighs [ k ]

1
2
3
4
5
6
7
8
9
10
11
12

edi , k2

k2 , zmm25, zmm14, 1

vrcp14pd zmm24, zmm25
vcmppd
vfpclasspd k0, zmm24, 30
kmovw
knotw k1, k0
vmovaps zmm17, zmm25
r10d , edi
and
vfnmadd213pd zmm17, zmm24, . . .
kmovw
k3 , r10d
vmulpd zmm18, zmm17, zmm17
vfmadd213pd zmm24{k1}, zmm17, zmm24
vfmadd213pd zmm24{k1}, zmm18, zmm24

Listing 1.1. LEA and MOV instructions.

Listing 1.2. Correction instructions.

Further instructions are also included to perform corrections after computing
the reciprocal (see Listing 1.2), which are not present in kernels with explicit
SIMD intrinsics (like MxN kernels). Their usefulness is arguable because other
factors can change the results at this precision, like the order for partial forces
calculation that varies significantly across different optimization and paralleliza-
tion strategies. Table 1 shows performance, temperature and pressure for the
Verlet Lists algorithm with and without such instructions, as well as for the
MxN algorithm. Without them, we can perceive a performance improvement of
about 11% in the force calculation runtime. In terms of accuracy, not only the
differences for temperature and pressure are small, but even smaller than when
comparing to the MxN algorithm.

5.2 Investigate memory latency contributions

An important assumption for performance engineering of streaming kernels is their
non-significant latency contribution due to regular data access pattern, which
is trivially foreseeable by cache prefetchers. On a first thought, MD simulations
are expected to have significant latency impact due to their memory access

MD-Bench proxy-app toolbox

9

Algorithm Temperature
Verlet+C 7.961495 × 10−1 6.721043 × 10−1
Verlet-C 7.961635 × 10−1 6.721161 × 10−1
MxN 7.961966 × 10−1 6.721441 × 10−1

Pressure

Time(s)
4.78
4.27
3.13

Table 1. Temperatures, pressures and runtimes for Verlet with/without corrections
(+C/-C) and MxN. Quantities are unitless and reflect lj style from LAMMPS.

characteristics, and MD-Bench can assist on measuring such impact via its stubbed
sequential case. Nonetheless, when executing them against standard copper lattice
and melting cases, we observed that such impact is minor. Furthermore, we also
compare measurements in Cascade Lake from our stubbed version with kernel
throughput predictions under ideal conditions from IACA [6] (for Skylake-X
micro-architecture), OSACA [7] and uiCA [1] static code analyzers.

Fig. 3 depicts cycles per SIMD iteration for mentioned cases with distinct
prefetcher settings, together with IACA, OSACA and uiCA predictions. For the
stubbed case, the impact for disabling prefetchers is negligible as expected, and
two versions with different number of neighbors per atom (76 and 1024) are
shown to evaluate the overhead contribution from control flow divergence. With
unsteadier memory accesses, the average cycles grows by 2.1 (4%) in standard
case and 1.3 (3%) in melting case with all prefetchers, a similar behavior with only
the hardware prefetcher enabled, which makes it the most effective one. From 76
to 1024 neighbors per atom, the number of cycles decreases by about 5.8 (15%),
hence control flow divergence contribution is higher than latency contribution.
Predictions from OSACA and uiCA are too optimistic with only 55% and 68% of
best execution, respectively, where IACA prediction matches 92%. IACA reports
stalled backend allocation in the CPU due to frontend bubbles, but a frontend
model is not present in OSACA and uiCA. Frontend stalls therefore contribute
significantly to kernel throughput, and based on IACA results our stubbed case
is close to optimal execution.

Fig. 3. Cycles per SIMD iterations on Standard, Melting and Stubbed setups with
different prefetchers enabled, together with predictions from static analyzers.

10

Rafael Ravedutti Lucio Machado et al.

5.3 Compiler code quality study

(a) Runtime

(b) HN Performance Profile (c) FN Performance Profile

Fig. 4. Runtime (subfigure (a)), and HPM counter profiling results (Half neighbor-list
HN subfigure (b) and full neighbor-list FN subfigure (c)) for the Lennard-Jones copper
lattice testcase. Results are shown using compiler flags to enforce no SIMD vectorization,
SSE (16b), AVX2 (32b), and AVX512 (64b) SIMD vectorization. In subfigures (b) and
(c) the stacked bars show total and arithmetic instruction counts on the left y-axis, the
black lines cycles per instruction (CPI) on the right y-axis.

In this example the Verlet List algorithm with half neighbor-lists (HN) and
full neighbor-lists (FN) is benchmarked and profiled to analyze how well these
are suited for vectorization and to explain the observed runtimes in more detail.
AoS data layout was used with double precision floating point arithmetic. Please
note that HN in MD-Bench currently does not implement the Ghost Newton
optimization, hence its benefits are not accounted for. This study was performed
on the Intel Ice Lake node using the Intel compiler (ICC) 2021.4.0. The force field
kernel was compiled for several target SIMD instruction sets and without vector-
ization using the -no-vec option. The compiler requires a #pragma omp simd
to vectorize the HN variant. The binaries were benchmarked with Turbo mode
enabled, all executions were performed in the same cluster node with the same
frequency, which was endorsed via HPM measurements.

Fig. 4(a) shows the runtime for the standard Copper lattice test case. Without
vectorization, the HN variant is as expected faster by almost a factor of two. When
using wider SIMD units, FN shows almost linear speed-up and is faster than
HN for all SIMD widths. HN gets slower for SSE, stagnates for AVX2, and then
improves by a large step with AVX512 but still being 23% slower than FN. For
instruction throughput bound codes, the best case uses least instructions combined
with optimal pipelined and superscalar execution, improving instruction-level
parallelism (ILP) in the processor hardware used. Both aspects can be directly
measured using HPM counters. Fig. 4(b) and Fig. 4(c) show instruction counts
and cycles per instruction (CPI) measurements for all HN and FN variants. For
HN with SSE it can be seen that the arithmetic instruction count is almost half
due to using the SSE 16b registers. Still, the compiler does not manage to reduce
the overall instruction count. 29.6% more instructions are required to get the

MD-Bench proxy-app toolbox

11

operands into the SIMD registers. The additional instruction work is partially
compensated by an improved CPI resulting in only 3.7% worse runtime. An
explanation for this improved CPI is that the register/register SIMD instructions
on Intel processors are executed on different scheduler ports than the arithmetic
instructions and therefore can be executed out-of-order. The compiler refused to
employ 32b arithmetic SIMD instructions in the AVX2 variant, the instruction
count was still decreased and the runtime slightly improved. The enhanced
capabilities of the AVX512 instruction set extension enables the compiler to
generate a version with just 25% of the instruction count of the no-vec variant.
The runtime advantage is smaller because this instruction mix is executed with a
significantly worse CPI of 1.01. It is still impressive that a code that was impossible
to vectorize efficiently with the previous SIMD instruction set extensions now
shows an instruction count reduction of a factor of four (out of the optimal eight).
For FN the compiler manages to reduce the arithmetic instruction count
with every wider SIMD unit. The overall instruction count increased slighly for
SSE, but then is just 45% for AVX2 and 18% for AVX512 compared to the
no-vec variant. This underscores that the FN version is very well suited for
SIMD vectorization. This kind of study gives interesting insights and is easy
to perform. Apart from comparing algorithmic variants it can be applied to
different processor architectures focusing on the CPI metric or different compilers
focusing on instruction counts. While this type of study can also be done on
other proxy-apps or application it is especially easy in MD-Bench, because all
kernels are already instrumented for LIKWID.

6 Conclusion and Outlook

This paper introduced MD-Bench, a proxy-app toolbox for performance research
of MD algorithms. It facilitates and encourages performance related research and
provides clean implementations of state-of-the-art MD optimization schemes such
as Verlet List and GROMACS MxN. We list and describe the most important
MD-Bench features and its differences to other offerings, highlighting its usage
on low-level code analysis and investigation of performance implications through
profiling with HPM. Further, we support our statements on the applicability of
MD-Bench by providing three use case examples that expose interesting insights
concerning the performance of short-range classical MD kernels.

MD-Bench is mature and usable, but there are still multiple open points.
We want to consider more MD applications as, e.g., NAMD. MD-Bench is cur-
rently a CPU-only application with a few OpenMP parallel loops. A competitive
distributed memory parallelization based on MPI is one of the next work pack-
ages. Another sorely missing part are implementations and specific optimization
schemes for GPU accelerators. Work on supporting GPUs has already started but
is in an early stage. We hope to encourage others with this paper to participate
and contribute to the development of MD-Bench. A project like MD-Bench is
an ongoing effort keeping track with recent developments and supporting novel
hardware architectures.

12

Rafael Ravedutti Lucio Machado et al.

Acknowledgments The authors gratefully acknowledge the scientific support
and HPC resources provided by the Erlangen National High Performance Com-
puting Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-
Nürnberg (FAU). NHR funding is provided by federal and Bavarian state au-
thorities. NHR@FAU hardware is partially funded by the German Research
Foundation (DFG) – 440719683.

References

1. Abel, A., Reineke, J.: A parametric microarchitecture model for accurate basic block
throughput prediction on recent intel CPUs. In: ICS 2022. pp. 1–12 (June 2022)
2. Brown, W.M., Kohlmeyer, A., Plimpton, S.J., Tharrington, A.N.: Implementing
molecular dynamics on hybrid high performance computers – particle–particle
particle-mesh. Computer Physics Communications 183(3), 449 – 459 (2012)
3. Edwards, H.C., Trott, C.R.: Kokkos: Enabling performance portability across manycore
architectures. In: 2013 Extreme Scaling Workshop (xsw 2013). pp. 18–24 (Aug 2013)
4. Gecht, M., Siggel, M., Linke, M., Hummer, G., Köfinger, J.: Mdbenchmark: A toolkit
to optimize the performance of molecular dynamics simulations. The Journal of
Chemical Physics 153(14), 144105 (2020), https://doi.org/10.1063/5.0019045
5. Gratl, F.A., Seckler, S., Tchipev, N., Bungartz, H.J., Neumann, P.: Autopas:
Auto-tuning for particle simulations. In: 2019 IEEE IPDPSW. pp. 748–757 (2019)
6. Intel: Intel architecture code analyzer (Aug 2019), https://www.intel.com/content/
www/us/en/developer/articles/tool/architecture-code-analyzer.html

7. Laukemann, J., Hammer, J., Hofmann, J., Hager, G., Wellein, G.: Automated
instruction stream throughput prediction for intel and amd microarchitectures. In:
2018 IEEE/ACM PMBS. pp. 121–131 (2018)

8. Machado, R.R.L., Schmitt, J., Eibl, S., Eitzinger, J., Leißa, R., Hack, S., Pérard-Gayot,
A., Membarth, R., Köstler, H.: tinymd: Mapping molecular dynamics simulations to
heterogeneous hardware using partial evaluation. Journal of Computational Science
54, 101425 (2021)

9. Pennycook, S.J., Hughes, C.J., Smelyanskiy, M., Jarvis, S.: Exploring simd for
molecular dynamics, using intel® xeon® processors and intel® xeon phi coprocessors.
In: 2013 IEEE 27th IPDPS. pp. 1085–1097 (2013)

10. Plimpton, S.: Fast parallel algorithms for short-range molecular dynamics. Journal of

Computational Physics 117(1), 1 – 19 (1995)

11. Páll, S., Hess, B.: A flexible algorithm for calculating pair interactions on simd
architectures. Computer Physics Communications 184(12), 2641 – 2650 (2013)
12. van der Spoel, D., Lindahl, E., Hess, B., Groenhof, G., Mark, A.E., Berendsen, H.J.C.:
GROMACS: fast, flexible, and free. Journal of Computational Chemistry 26(16),
1701–1718 (2005)

13. Treibig, J., Hager, G., Wellein, G.: Likwid: A lightweight performance-oriented
tool suite for x86 multicore environments. In: Proceedings of PSTI2010, the First
International Workshop on Parallel Software Tools and Tool Infrastructures. San
Diego CA (2010)

All links were last followed on October 5, 2020.

