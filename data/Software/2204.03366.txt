2
2
0
2

p
e
S
2
2

]
E
S
.
s
c
[

2
v
6
6
3
3
0
.
4
0
2
2
:
v
i
X
r
a

1

Impact of Software Engineering Research in
Practice: A Patent and Author Survey Analysis

Zoe Kotti, Georgios Gousios, and Diomidis Spinellis, Senior Member, IEEE

Abstract—Existing work on the practical impact of software engineering (SE) research examines industrial relevance rather than
adoption of study results, hence the question of how results have been practically applied remains open. To answer this and investigate
the outcomes of impactful research, we performed a quantitative and qualitative analysis of 4 354 SE patents citing 1 690 SE papers
published in four leading SE venues between 1975–2017. Moreover, we conducted a survey on 475 authors of 593 top-cited and
awarded publications, achieving 26% response rate. Overall, researchers have equipped practitioners with various tools, processes,
and methods, and improved many existing products. SE practice values knowledge-seeking research and is impacted by diverse
cross-disciplinary SE areas. Practitioner-oriented publication venues appear more impactful than researcher-oriented ones, while
industry-related tracks in conferences could enhance their impact. Some research works did not reach a wide footprint due to limited
funding resources or unfavorable cost-beneﬁt trade-off of the proposed solutions. The need for higher SE research funding could be
corroborated through a dedicated empirical study. In general, the assessment of impact is subject to its deﬁnition. Therefore, academia
and industry could jointly agree on a formal description to set a common ground for subsequent research on the topic.

Index Terms—software engineering, practical impact, empirical study, survey, patent citations

(cid:70)

1 INTRODUCTION

I N 2018, the ﬁeld of software engineering (SE) marked

the 50th anniversary of its ﬁrst two-year conference
series—the 1968–69 NATO Conferences on Software Engi-
neering [1], [2]. Despite its relatively short period of exis-
tence, a lot of research has been performed in SE during
these 50 years, composing a large body of information. In
the meantime, numerous software and technology-related
companies have emerged, partially as a result of hardware
advancement and cloud computing [3], forming a multi-
trillion dollar industry [4]. This growth both in terms of
knowledge and market share raises the question of how
these two relate, and to what extent research may have
impacted industry. In this context, we deﬁne as impact the
direct or indirect incorporation of a software engineering
study’s output in an industrial setting, for example, in an
industrial software development tool, process, marketable
product, or service.

In the scope of this study, we consider software engi-
neering the discipline that systematically employs computer
science knowledge and principles to develop new methods
and tools to improve software development. The discipline’s
areas include software requirements, design, construction,
testing, maintenance, conﬁguration management, quality,
SE management, SE models and methods, and SE pro-
cess [5]. The application process is based on systematic, dis-
ciplined, and quantiﬁable SE approaches, and is inﬂuenced
by cross-disciplinary areas, namely, mathematics, general
management, project management, and systems engineer-

• D. Spinellis and G. Gousios are with the Department of Software Technol-

ogy, Delft University of Technology, The Netherlands.
E-mail: {D.Spinellis,G.Gousios}@tudelft.nl

• D. Spinellis and Z. Kotti are with the Department of Management Science
and Technology, Athens University of Economics and Business, Greece.
E-mail: {dds,zoekotti}@aueb.gr

ing [5]. Note that our deﬁnition distinguishes foundational
computer science research (e.g., devising a new static analy-
sis method, a test prioritization algorithm, or a requirements
deﬁnition language) from that performed in SE contexts. For
the described examples to be considered SE research, we
expect them to be accompanied with empirical evaluation
through, for example, repository mining, a developer sur-
vey, or a case study.

Existing work on the practical impact of SE research
examines industrial relevance rather than adoption of study
results. A variety of interviews and literature reviews have
been conducted, mainly in domain-speciﬁc contexts such as
the ACM SIGSOFT Impact Project [6], to assess the relation
of research to industrial needs, highlight gaps between the
two, and suggest best practices for collaborative projects.
However, the question of how research results have been
practically applied remains open.

To tackle this question and investigate the outcomes of
impactful SE research, we performed a quantitative and
qualitative analysis of SE patents citing SE research from
four leading SE venues. Patents are by deﬁnition practical
applications of technology, and are frequently employed
as an estimator of the academic research impact (e.g., in
the works by Narin et al. [7], Estublier et al. [8], and the
National Academy of Engineering [9]). Software patents
have increased rapidly in number, comprising 15% of all
patents [10]. Most of them are acquired by large manufac-
turing ﬁrms from the computers, electronics, and machinery
industries [10]. Furthermore, we conducted a survey on
authors of highly recognized SE publications to examine im-
pactful types, areas, methods, and outcomes of SE research
as well as their footprint on information technology, society,
and industry.

Our ﬁndings suggest that SE researchers have equipped
practitioners with various tools, processes, and methods,

© 2022 EU Copyright. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any
copyrighted component of this work in other works. DOI: 10.1109/TSE.2022.3208210

 
 
 
 
 
 
and improved many existing products. SE practice seems
to value knowledge-seeking research and is impacted by
diverse cross-disciplinary SE areas. Practitioner-oriented
publication venues appear more impactful than researcher-
oriented ones, while industry-related tracks in conferences
could enhance their impact. Two main obstacles to research
adoption seem to be insufﬁcient funding and the unfavor-
able cost-beneﬁt trade-off of the produced solutions. The
study’s contributions are:
• the systematic collection of top-rated SE research,
• the systematic collection of SE patents citing SE research,
• the categorization of research according to its type, meth-

ods, SE area, and industrial application domain, and

• the synthesis of the preceding results into a taxonomy of

the main practical impacts of SE research.

In the following Section we present an overview of
existing work on the practical impact of SE research. We
then describe the research questions and study methods
in Section 3, and present the research results in Section 4.
An extensive discussion of the study ﬁndings is included
in Section 5. The study is complemented by the associated
limitations in Section 6, followed by our conclusions in Sec-
tion 7. Based on published guidelines [11], the code,1 survey
questionnaire, anonymized responses, and produced data2
associated with this study are publicly available online, and
can be used for replication or further empirical research.

2 RELATED WORK
We analyze and synthesize related work on the practical
impact of SE research according to the study objective (i.e.,
practical impact), and the two employed method axes (i.e.,
patent and author survey analyses). For this, we employ the
classiﬁcation scheme by Lo et al. [12], and classify related
work into three areas: research related to the ACM SIG-
SOFT Impact Project; literature reviews and surveys in SE
evaluating the relationship between academia and industry;
and ranking studies assessing the impact of SE researchers,
institutions, or publication venues. To cover the patent axis,
we extend this classiﬁcation scheme with studies evaluating
the impact of SE research based on patent metadata.

2.1 ACM SIGSOFT Impact Project

In the early 2000s, the Impact Project [6] was established,
in an attempt “to help both the research and practitioner
community to understand each other better”, in order to
strengthen their cooperation, and also avail funding agen-
cies to maximize their return on investment in SE research.
In general, the project aims to study the impact that SE
research has had upon software development practice.

The project uncovers state-of-the-art software technolo-
gies in speciﬁc areas, and examines their inﬂuence by former
research work, through literature searches and personal in-
terviews [13], [14]. Speciﬁc areas include software conﬁgura-
tion management [8], modern programming languages [15],
software testing and analysis [16], middleware technol-
ogy [17], inspections, reviews, and walkthroughs [18], and
software resource estimation [19].

1. https://doi.org/10.5281/zenodo.6780414
2. https://doi.org/10.5281/zenodo.7090818

2

According to the area-speciﬁc studies, academic research
tools and services have been adopted by major industrial
projects and have inﬂuenced various ﬁelds. Although some
original ideas require a long time (up to 15–20 years [17]),
deep reworking, and re-engineering to apply in industrial
practice, the constant ﬂow of researchers between industry
and academia can expedite adoption. By employing aca-
demic research techniques, laboratories have reported up
to 95% increase in defect detection before testing, 50% cost
reduction for newly developed source code lines, and up to
50% shortening of delivery times.

Conversely, companies have contributed to academia in
estimation and mathematical approaches, advanced project
planning, and ﬂexible and realistic models [19]. In esti-
mation and mathematics we distinguish the rise of anal-
ogy, expert judgment, hybrid, and Bayesian approximation
methods. Motivated by corporate projects, academic ones
have started balancing their deadlines and activities bet-
ter, and prioritizing features according to schedule, cost,
and quality requirements. Finally, SE models have become
more adaptable to the diverse programming languages, new
standards, and techniques such as rapid application devel-
opment, and more realistic on the basis of ten evaluation
criteria: deﬁnition, accuracy, scope, objectivity, constructiveness,
detail, stability, ease of use, prospectiveness, and parsimony [19].

2.2 Literature Reviews and Surveys

In 2013, Misirli et al. [20] conducted in-depth interviews
with twelve practitioners who were actively collaborating
with them at that time in three industrial software analytics
projects. These projects involved defect, effort, and quality
prediction. Their aim was to explore practitioners’ expec-
tations, and ways to employ software analytics solutions in
policy making. Respondents suggested enhancing the exam-
ined solutions with defect-severity or defect-type prediction,
defect location, and phase– or requirement-level effort esti-
mation. Furthermore, they stressed the need for collecting
accurate and complete data through the provided solutions,
and integrating these solutions into existing systems (e.g.,
by combining defect prediction results with test interfaces
to determine which interfaces to test ﬁrst).

Around the same time, Beecham et al. [21] conducted
interviews with practitioners from ten companies of various
sizes to assess the impact of Global Software Engineering
(GSE) research papers in practice. GSE research is regarded
as useful, and participants argue that studying the topic
might improve performance. Still, none was found to have
actually consulted the GSE literature. Practitioners mostly
refer to books, blogs, forums, short reports, and their past
experience to resolve problems in GSE. Some respondents
associated GSE with general project management. The au-
thors argue that GSE research should be relevant (i.e., reﬂect-
ing the needs of practice), documented in short, evidence-
based, and readable papers with validated ﬁndings, dis-
seminated more widely as grey literature, and advertised
through social media.

Through another survey on 512 Microsoft practitioners,
Lo et al. [12] examined the relevance of SE research to prac-
tice. Participants rated the relevance of 571 ICSE, ESEC/FSE,
and FSE papers published between 2009–2014: 71% of all

ratings were essential or worthwhile, while no correlation
was observed between citation counts and relevance scores.
Reasons behind research ideas rated as “unwise” include:
unneeded tools; non-actionable empirical studies; general-
izability issues; cost outweighing beneﬁt; questionable as-
sumptions; disbelief in the proposed solution; better alter-
natives or more crucial problems to handle; and side effects
of the suggested solution. Nonetheless, practitioners seem
generally positive to studies performed by the SE research
community.

In 2017, Ivanov et al. [22] investigated the gaps between
research and practice by surveying software engineers and
comparing their answers to research topics covered by re-
cent ICSE and FSE publications. Inconsistencies were de-
tected between practitioners’ needs and actions: while de-
velopment productivity was deemed more important than
software quality, the majority of the examined publications
involve software veriﬁcation and validation. In addition,
practitioners struggle to ﬁnd improved effort estimation
methods.

To facilitate continuous and collaborative technology
transfer, Mikkonen et al. [23] proposed a model for large
consortia of companies and research institutes working on
a common research topic. The model was applied in two
national Finnish software research programs. To evaluate
and reﬁne it the authors conducted interviews with four
participating companies. According to the ﬁndings, com-
panies perform substantial SE research to create new busi-
ness opportunities, and appear willing to provide data to
academia for performing empirical SE research when they
identify a company beneﬁt. To improve industry-academia
collaboration, technology transfer models should foster co-
creation and co-learning rather than a linear one-way prod-
uct transfer from academia to industry. Research institutes
can improve their mindset by grasping long– and short-
term company needs as well as emerging SE research trends,
solutions, and results.

To address the relatively limited joint industry-academia
collaborations in SE, Garousi et al. [24] provided a set of
challenges and best practices for planning and conducting
collaborative projects. The authors denote as challenges: dif-
ﬁculty in understanding the industry problems; differences
in objectives, reward systems, and useful attributes; and dif-
ﬁculty in managing intellectual property rights. Among best
practices we discern: organizing regular workshops and
seminars with the industry; assuring continuous learning
from industry and academia sides; ensuring management
engagement; grounding research on real-world problems;
demonstrating explicit beneﬁts to the industry partners; and
maintaining agility during collaboration.

In a more recent work, Garousi et al. [25] explore and
characterize the state of industry and academia collabora-
tions in SE through an opinion survey among researchers
and practitioners. Around a hundred collaborative projects
from 21 countries were analyzed, revealing that the most
frequent topics are testing, quality, process, and project man-
agement. The vast majority of collaborative projects result
in more than one publication, while more than half have
a positive impact on the industry parties, usually through
a new approach, method, technique, or tool. To improve
industry-academia collaborations, the authors recommend

a set of good practices, including performing pilot tests
in laboratory settings before industrial releases, cultivating
trustful relationships with practitioners, investing in regular
meetings to promote the team spirit, and, again, adopting
iterative approaches such as agile methods.

3

2.3 Ranking Studies

To examine the health of SE conferences, Vasilescu et al. [26]
used a metrics suite to measure the stability, openness,
representativeness, availability, and scientiﬁc prestige of
eleven conferences in a ten-year window, between 1993–
2004. Although SE conferences are generally healthy and
display high author turnover, there are considerable dif-
ferences between wide– and narrow-scoped conferences
with regard to the aforementioned measures. For instance,
narrow-scoped SE conferences tend to be more introvert
than wide-scoped, while maintaining more representative3
program committees and lower author turnover.

In a 13-part study series spanning the years 1993–2008,
Glass et al. [27], [28] assessed scholars and institutions
based on the number of their publications in systems and
SE. In general, top-ranked academic institutions outnumber
industrial research centers. Although the USA was ﬁrst in
number of top-ranked institutions up to 2002, it has been
surpassed by the Asia-Paciﬁc institutions since 2003.

Similarly, Ren and Taylor [29] ranked individuals and
organizations according to their publications in the ICSE,
FSE, TSE, and TOSEM venues between 2000–2004. The
majority of top-ranked scholars and institutions come from
the USA, while a signiﬁcant number originates from Europe.
The authors argue that “rankings based on publications can
supply useful data in a comprehensive assessment process”,
but Parnas [30] warns that “measuring productivity by
counting the number of published papers slows scientiﬁc
progress”. Some organizations may be more conservative
with publications. Consider, for example, Apple’s limited
publishing activity compared to its innovation [31].

A more recent assessment of top-cited SE researchers
was conducted by Petersen and Ali [32] by analyzing a
multi-ﬁeld dataset of author citations provided by Ioannidis
et al. [33]. The authors report that 37% of top researchers
of the dataset were mistakenly assorted in the SE ﬁeld,
while Barry Boehm is the leading SE author. The majority
of top SE researchers come from the USA, Canada, or
the UK, and are afﬁliated with Microsoft. Along with SE,
researchers are frequently involved in artiﬁcial intelligence,
image processing, and human factors.

2.4 Studies on Patent Metadata

Shortly after 2000, Agrawal and Henderson [34] evaluated
the contribution of patents to knowledge transfer from
universities to the industry by focusing on the MIT De-
partments of Mechanical Engineering, and Electrical En-
gineering and Computer Science. Patents account for less
than 10% of knowledge transfer from the aforementioned
Departments, while the majority of the faculty never patents

3. Representativeness of a program committee is measured through
the ratio of its members that have never co-authored a paper in the
conference’s past editions [26].

at all. Although patent volume does not predict publication
volume, it is positively correlated with paper citations, offer-
ing insight into the impact of university research. In general,
patenting is a complementary rather than substitutional
activity for fundamental research.

Through a systematic analysis of citation linkages be-
tween US patents and research papers, Narin et al. [7]
assessed the contribution of public science to industrial
technology. To collect all papers cited by patents, the authors
employed a similar approach to ours (see Section 3.3) by
extracting all non-patent references from around 400 000 US
patents. In total, 73% of papers cited by patents originate
from academic, governmental, and other public institutions,
while only 27% were authored by industrial scientists.
Patent-to-science linkage has a strong national component
with US patents heavily citing US papers, while the link-
age is subject-speciﬁc (e.g., chemical patents cite chemistry
papers). Furthermore, patent-cited engineering and technol-
ogy papers are mainly published in electrical engineering
journals, and IEEE is the top publisher.

3 METHODS

We framed our investigation on the impact of SE research in
practice in terms of the following research questions.
RQ1 What types, areas, and methods of SE research are impact-
ful? To answer this, we ﬁrst collected a set of SE research pa-
pers published in leading venues and complemented them
with their assigned topics, academic and SE patent citation
counts, and awards. Through a survey on authors of most-
cited and awarded publications, we identiﬁed impactful SE
research types, areas, and methods. The set of impactful
areas was enriched by extracting the topics of the papers
cited by SE patents.
RQ2 What are the outcomes of
impactful SE research? For
this we retrieved the most-cited SE research by SE patents,
and evaluated the citing patents based on their associated
litigation cases and maintenance fee events. Furthermore,
we examined the correlation between patent and academic
citations, and patent citations and academic awards.
RQ3 What
types of SE research outcomes are impactful?
Through a survey on authors of top-notch SE publications
we identiﬁed the practical footprint of SE research and how
its results have been exploited by the industry. In addition,
we detected potential obstacles in the practical adoption of
SE research.
RQ4 What are the main practical impacts of SE research on
information technology, society, and industry? Through the
aforementioned survey we further investigated how pre-
mier SE research changed the state of practice.
RQ5 Which SE venues are more likely to publish papers that
impact practice? We assessed the practical impact associated
with top SE venues through their patent-based impact fac-
tors, which we obtained by dividing their publication counts
with their patent citations.
RQ6 Is SE research funding sufﬁcient for obtaining results that
are relevant in practice? We approximated SE research expen-
ditures (and thus funding) by examining the total number
of existing SE publications and PhD dissertations, and com-
pared them to those of the main engineering branches to
evaluate sufﬁciency.

4

Methods’ Overview An overview of the methods we
employed to answer these six research questions is pre-
sented through a UML information ﬂow diagram in Fig. 1.
Their extended descriptions are introduced in the subse-
quent Sections. From the collected premier SE research (Sec-
tion 3.1) we retrieved the academic citations and awards,
and SE patent citations (Section 3.3). The authors of top-
cited and awarded SE publications formed the sample of our
survey (Section 3.6). The set of SE patents citing the premier
SE research (Section 3.2) was evaluated in terms of litigation
and maintenance fee events (Section 3.4). A correlation
analysis was also conducted using the academic citations
and awards, and SE patent citations (Section 3.5). From the
patent-cited SE papers we extracted their research areas
and top-cited papers. Through the correlation analysis, SE
patent evaluation, and top-cited SE research by SE patents
we answered RQ2.

Furthermore, the research areas of the patent-cited SE
papers, and the research areas, methods, types, applied re-
search outcomes, practical impacts, and adoption obstacles
of the surveyed SE papers provided us insights for RQ1,
RQ3, and RQ4. For RQ5 we identiﬁed the top SE venues,
retrieved their publication and SE patent citation counts,
and divided them to compute the venues’ patent-based
impact factors (Section 3.7). Finally, for RQ6 we extracted
the publication and PhD dissertation counts of SE and the
main engineering branches (i.e., chemical, civil, electrical,
and mechanical [35]), and compared them to assess the
sufﬁciency of provided SE research funding (Section 3.8).

3.1 Premier SE Research

We created a dataset of research papers published in four
top-notch SE venues, namely the International Conference on
Software Engineering (ICSE), the IEEE Transactions on Software
Engineering (TSE), the ACM Transactions on Software Engi-
neering and Methodology (TOSEM), and the Empirical Soft-
ware Engineering (EMSE) journal. From these we retrieved
11 419 papers by downloading the complete DBLP computer
science biography database (version April 4, 2017),4 and
ﬁltering its XML records to retain those whose inproceed-
ings key tag contained conf/icse, and those whose article
key tag contained journals/tse, journals/tosem, or journals/ese.
We excluded records with undeﬁned title or author tags,
and obtained 6 950 ICSE, 3 417 TSE, 428 TOSEM, and 624
EMSE publications spanning the years 1975–2017. For each
selected study, we listed its publication year, ﬁrst author,
title, and digital object identiﬁer (DOI).

We then obtained the topics of the selected studies as
follows. For the TSE and the IEEE-published ICSE pa-
pers, we used the DBLP-extracted DOIs to download the
corresponding IEEE Xplore HTML pages. From these we
extracted the INSPEC controlled terms of each paper. In
total, we retrieved 536 distinct topics of 1 106 TSE papers,
and 547 distinct topics of 1 348 IEEE-published ICSE papers.
(The overall distinct topics were 730.)

For the TOSEM and the ACM-published ICSE papers,
we obtained the ACM DL Abstracts and Titles for Research
Purposes database covering the period until 2017 from the

4. https://dblp.org/

5

Fig. 1. Information ﬂow of study methods.

ACM Publications Operations Manager (C. Rodkin, per-
sonal communication, October 21, 2020). We ﬁltered its
XML metadata ﬁles to retain those residing in directories
starting with TRANS-TSEM or PROC-ICSE. We normalized
the ICSE analysis to include only the regular main track,
excluding Companion and Future proceedings as well as
those of the following often co-published tracks: New Ideas
and Emerging Results; Software Engineering Education and
Training; Software Engineering in Practice; and Software
Engineering in Society. We removed these tracks to preserve
homogeneity of our ICSE dataset, which would otherwise
be affected by the tracks’ varying start years and substantial
missing data in the ACM database. For each paper, we
extracted its assigned 2012 ACM Computing Classiﬁcation
System (CCS) [36] concepts from the concept id and con-
cept desc tags. Overall, we obtained 287 distinct topics of
429 TOSEM papers, and 439 distinct topics of 1 648 ACM-
published ICSE papers.

For the EMSE papers, we did not use Springer’s key-
words because they do not have a deﬁned structure. Instead,
we retrieved their assigned 2012 ACM CCS concepts. EMSE
is not included in the aforementioned ACM database, thus
we web-scrapped the CCS concepts of the papers from their
associated ACM Digital Library web pages by developing a
Python script using the Beautiful Soup package [37]. In this
way, we collected 216 distinct topics of 239 EMSE papers.
(The overall distinct topics for the TOSEM, ACM-published
ICSE, and EMSE papers were 545.)

SE research papers. Since the DBLP data did not contain
citation counts, we obtained them from Elsevier’s Scopus
database5 by querying the ﬁeld Source title using as input
each of the four venue names combined with the publisher
(e.g., ACM Transactions on Software Engineering and Method-
ology). In this way, we retrieved 13 862 ICSE, 421 TOSEM,
3 464 TSE, and 694 EMSE records, again spanning the years
1975–2017. For the awards, we manually searched in the
ICSE proceedings for all distinguished and most inﬂuential
papers up to 2016, identifying a set of 74 and 29 papers,
correspondingly.

3.2 SE Patents

To identify SE-related patents, we adopted one of the
two approaches recommended by Griliches [38]: employ-
ing a patent classiﬁcation system developed by a patent
ofﬁce. The alternative involves reading and manually clas-
sifying individual patents (e.g., the work by Bessen and
Hunt [10])—this would restrict the research scope due to
our small number of human raters. Instead, we used the
Cooperative Patent Classiﬁcation (CPC) system (version
2020.08),6 which has been jointly developed by the Euro-
pean Patent Ofﬁce (EPO) and the US Patent and Trademark
Ofﬁce (USPTO) [39], and “is a further step towards a more
general harmonization of the world’s patent classiﬁcation
systems” [40].

The CPC system is divided into nine sections, which in
turn are subdivided into classes, subclasses, groups, and

We also retrieved the academic citations (i.e., references
by other research papers) and conference awards of the

5. https://scopus.com/
6. https://www.cooperativepatentclassiﬁcation.org/

Premier SEresearch (3.1) SE patentcitations of SEresearch AcademiccitationsAcademic awardsSE patents (3.2) Maintenance feeevents Litigation casesTop-cited SEresearch by academicresearchDistinguished/ most influentialSE researchAuthor survey (3.6) Correlationanalysis (3.5)SE patentevaluation (3.4)Top-cited SEresearch bySE patentsRQ2: ImpactfulSE researchoutcomes ResearchmethodsResearchtypesAppliedresearchoutcomesResearchareasResearchadoptionobstaclesRQ1: Impactful SEresearch types, areas,methodsRQ3: Impactfultypes of SEresearchoutcomesPracticalresearchimpactsRQ4: Practical SEresearch impacts oninformation technology,society, industry SE research citedby SE patents«flow»Patent-basedimpact factors (3.7)SE RQ5: SEvenues withpracticeimpact Research publicationsMain engineering branches (chemical, civil, electrical, mechanical) PhDdissertationsRQ6:Sufficiency ofSE researchfunding Researchfundingevaluation (3.8)SE patent citations ofpremier SE research (3.3) SEpublicationsTop SEvenuessubgroups—we manually looked for SE categories in all
levels of the hierarchy. To ensure consistency of this manual
process, guidelines recommended in the work by Brereton
et al. [41] were followed: the ﬁrst author of this paper per-
formed the lookup, and the last author validated the identi-
ﬁed SE categories. Speciﬁcally, the ﬁrst author identiﬁed as
relevant all subgroups under the group G06F8/00 Arrange-
ments for software engineering, along with any subgroups
mentioned in them that belonged to other groups. For in-
stance, G06F8/451 Code distribution references G06F9/5083
load rebalancing, and G06F9/5083 contains G06F9/5088 in-
volving task migration—the latter was also included in this
case. In the end, 173 SE-related categories were identiﬁed
by the ﬁrst author.

The last author veriﬁed these categories taking into
account the lower-level contents of the ACM CCS Software
and its engineering concept. Categories associated with the
following irrelevant CCS concepts were removed: Hard-
ware; Distributed computing methodologies; Concurrent comput-
ing methodologies; Security and privacy; Operations research.
Furthermore, the entire CPC subgroup G06Q10/06 admin-
istrative, planning or organization aspects of software project
management mentioned by G06F8/00 was excluded, because
although it seems relevant to SE, it effectively applies to any
management context. The resulting set of SE-related CPC
categories has 117 members.

To retrieve patents belonging to the 117 CPC cate-
gories, we queried the Google Patents Public Data (GPPD)
dataset [42] on BigQuery. GPPD is a worldwide bibli-
ographic and US full-text dataset of patent publications
provided by IFI CLAIMS Patent Services and Google, and
updated on a quarterly basis—for this study, the April 2020
version was used. From Table patents.publications 202004 we
extracted 304 368 distinct patents associated with at least one
of the aforementioned CPC categories.

3.3 SE Research Cited by SE Patents

Scientiﬁc literature cited by patents (i.e., science linkages)
can provide insights into the impact of science on indus-
try [43]. Science linkages are usually considered the state
of the art and help in evaluating an invention’s novelty
and patentability. Companies whose patents contain many
science linkages are regarded closer to science, basing their
technology on scientiﬁc progress [43]. Moreover, science
linkages may be used as predictors of a company’s ﬁnancial
performance: high-tech companies typically surpass their
competitors in science linkages [44].

Motivated by this, we assessed the practical impact of
the SE papers in terms of their citations by SE patents.
First, we collected all non-patent literature cited by the
patents, which is stored in plain text format in Table
patents.publications 202004 (ﬁeld citation.npl text). Querying
this Table we extracted 830 379 text references associated
with 92 772 distinct patents. To identify any SE papers in the
references, we followed two approaches: DOI crosschecking,
and title and author mapping. For the ﬁrst, we looked up
the available 6 017 ICSE, 3 405 TSE, 426 TOSEM, and 509
EMSE DOIs in the references, and found 43 ICSE, 52 TSE, 12
TOSEM, and 3 EMSE papers cited by 121 patents. To include
in the process papers and references without available DOIs,

6

we also searched in the references all SE titles, combined
with the last names of the ﬁrst authors. In case both a title
and a name were found in a reference, we considered this
a match. From this process, 895 ICSE, 630 TSE, 104 TOSEM,
and 20 EMSE papers (that were not identiﬁed through DOI
crosschecking) were found cited by 4 248 patents. In total,
1 690 (912 ICSE, 649 TSE, 107 TOSEM, 22 EMSE) distinct
papers are referenced in 4 354 distinct patents.

To evaluate our method, we estimated the accuracy us-
ing a random sample of the collected references. The sample
size of a total of 6 469 references was calculated at around
363 using Cochran’s sample size and correction formula
for the proportion [45] (95% conﬁdence, 5% precision). We
manually veriﬁed the sample references and marked 19 (5%)
of them as false positives. These fall into three categories:
different paper version (mostly earlier)7 cited by patent
(63%); inherent dataset issue (i.e., wrong paper title or DOI
documented in dataset—32%); method insufﬁciency (i.e., the
searched DOI is a subset of the one referenced in patent—
5%).

3.4 Evaluation of Citing SE Patents

As explained in Section 3, we estimated the value of the
SE patents that cite SE research based on two indicators:
patent maintenance fee events and litigation cases. These
are recommended measures of patent quality due to the
associated substantial monetary expenses [46].

We extracted the litigation cases associated with the
citing patents from the USPTO Patent Litigation Docket
Reports Data, which contain detailed patent litigation infor-
mation on 81 350 unique district court cases ﬁled during the
period 1963–2016 [47], [48]. Speciﬁcally, we joined the ﬁles
cases and patents (version 2016),8 modifying patent publi-
cation numbers to follow the convention used in the GPPD
dataset, and retrieved the number of cases as well as the cor-
responding aggregated demanded monetary damages per
patent. For each SE paper cited by patents (Section 3.3), we
listed the total number and damages of the corresponding
litigation cases.

Maintenance fee events of patents granted since 1981
are provided on a weekly basis by USPTO;9 we used the
February 8, 2021 release which comprises 18 523 706 unique
events, again adapting patent publication numbers to the
GPPD standard. For each event, a fee code is listed, but
not the actual monetary value; we manually extracted the
related fee values from the USPTO Fee Schedule10 (effective
since January 2, 2021) as follows. A set of 157 codes are
reported in the documentation ﬁle of the dataset. From these
we excluded 37 codes that are irrelevant to payments or
refunds, and four codes subject to the 37 Code of Federal
Regulations, Paragraph 1.28, concerning debts occurring
from errors in the small entity status—these are not ﬁxed
values. The remaining codes were mapped to their values
based on the USPTO Fee Schedule, while deprecated codes

7. In patents, the established practice is to cite the earliest version of

equally important documents [43].

8. https://bulkdata.uspto.gov/data/patent/litigation/2016/
9. https://bulkdata.uspto.gov/data/patent/maintenancefee/
10. https://www.uspto.gov/learning-and-resources/fees-and-

payment/uspto-fee-schedule

were ﬁrst associated with the replacing ones through the Fee
Schedule Crosswalk, FY2002–2003.11 Finally, a total of 112
codes regarding payments of maintenance fees, surcharges,
and refunds were mapped to the current fee rates.

Similar to litigations, for each SE paper cited by patents,
we extracted the number and aggregated monetary value of
the corresponding patent maintenance fee events. To do this,
we computed the monetary value of each distinct patent by
summing its fee payments and surcharges, and subtracting
any refunds. Refund cases were also excluded from the total
number of maintenance fee events of each patent.

3.5 Correlation Analysis

We investigated whether patent citations are correlated with
academic citations and academic awards. For the 1 690 pa-
pers cited by patents we joined their patent citations with
the academic ones, and also marked the awarded papers.
To select the appropriate correlation coefﬁcient, we tested
the two citation distributions for normality with D’Agostino
and Pearson’s omnibus test of normality [49], and found
that they do not follow a normal distribution. Therefore,
we used Spearman’s rank correlation coefﬁcient (ρ) [50],
which summarizes the monotonic relationship between two
variables that do not follow a normal distribution. Patent
citations constituted the dependent variable.

3.6 Survey on SE Research Authors

We conducted a survey on authors of exceptional SE pub-
lications following the set of ten activities recommended in
Kitchenham and Pﬂeeger’s six-part series of survey research
principles [51], [52].

Survey Design We adopted a cross-sectional, case-
control, observational study design, which means that can-
didates were surveyed about their past experiences at a
ﬁxed point in time [53]. The goal of the survey was to ex-
amine how landmark research has affected SE practice, therefore
we framed the objectives of the survey in terms of RQ1,
RQ3, and RQ4 introduced in Section 3.

Survey Sample The sample was composed of the ﬁrst
authors of the most-cited studies published in ICSE, TSE,
TOSEM, and EMSE as well as studies that received distin-
guished and most inﬂuential paper awards. We consider
that ﬁrst authors are the ones who have contributed the
most to the associated research [32], and are therefore more
familiar and knowledgeable about the investigated topics,
and also the most appropriate to provide the required
feedback. For each venue and year up to 2016, we selected
the ﬁve most-cited publications (Section 3.1), leading to a
set of 613 studies. We complemented this with the 103
awarded distinguished and most inﬂuential ICSE papers.
After removing duplicate studies (i.e., ICSE publications
that were subsequently extended in TSE, TOSEM, or EMSE)
keeping the latest occurrences, our ﬁnal set included 677
distinct papers, which were associated with 566 distinct ﬁrst
authors. From these, we managed to contact via e-mail 475
ﬁrst authors of 593 papers (204 ICSE, 176 TSE, 121 TOSEM,

11. https://www.uspto.gov/learning-and-resources/fees-and-

payment/fee-schedule/fee-schedule-crosswalk-fy2002-2003

7

92 EMSE). These constituted the survey sample. Contact fail-
ures involved missing or defunct e-mail addresses, deceased
or unavailable authors, and rejected e-mail deliveries.

A total of 50 (out of 475—10%) surveyed authors asso-
ciated with 58 (out of 593—10%) papers were practitioners
when their work was published. To compute this we re-
trieved the ﬁrst author afﬁliations from Elsevier Scopus’s
API through the pybliometrics Python interface [54] using the
publication DOIs as input. We excluded afﬁliations contain-
ing the (case-insensitive) string “univ”, which would most
likely involve universities, and manually looked in the re-
maining records for companies. In this way we identiﬁed 50
authors afﬁliated with 36 companies, including IBM, Nokia,
Microsoft, Sun Microsystems, AT&T, General Electric, Intel,
and Robert Bosch.

Survey Instrument Participants were provided with per-
sonalized questionnaires that mentioned at the beginning
the author’s name, the examined paper, the venue and year
of publication, and the reason it was selected (i.e., top-cited,
distinguished, or most inﬂuential). (First authors of multiple
papers were provided with multiple such questionnaires.)
They could also include and review additional publications
of theirs not included in the list, which they considered to
have made signiﬁcant impact on SE practice. A total of ﬁve
responses were collected from this option from four distinct
participants.

The questionnaire was composed of mandatory and
optional open-ended, multiple choice, and Likert scale ques-
tions, accompanied by neutral and free-text options. Partici-
pants were initially requested to rate on a three-level Likert
scale the extent of practical impact of their publication, and
specify through a multiple choice question in what products
or processes their work has been incorporated. In an open-
ended question, they were subsequently invited to expand
on the practical impact, along with ways in which their
work changed the state of practice. In case of absence of
practical impact, they were asked to comment on the reasons
behind this. Furthermore, respondents were asked to select
the research areas of their work from a list with the ﬁrst–
and second-level entries to the Software and its engineering
concept of the ACM CCS [36]. Another question involved
specifying the research types and methods employed in
the publication from a list adapted from the work by
Easterbrook et al. [55]. Finally, participants could list other
impactful papers (of which they were not ﬁrst authors),
leave their e-mail address to receive a report with the survey
results, and comment on the survey and its topic.

Survey Evaluation Two pilot studies were conducted
on candidates of the survey sample to evaluate and reﬁne
the questionnaire. The ﬁrst pilot was internal and was com-
pleted by three members of the laboratory associated with
four publications. The second pilot was external and was
distributed to a random subset of eleven candidates linked
to 16 publications. This trial was held from September 5th to
30th, 2017, and we received six responses from six distinct
participants (55% response rate in terms of authors, 37% in
terms of papers).

Survey Operation The ﬁnal survey ran from October
1st to November 5th, 2017, and from May 18th to 31st,
2022. The pilot and the ﬁrst run of the ﬁnal survey were

hosted on the SurveyGizmo online survey platform,12 while
the second run was provided as a Google form. Both runs
were distributed to the candidate participants through an
invitational mail. E-mail addresses were manually fetched
from the candidates’ personal websites. The mailing process
was automated but retained personalization, as explained
before. Candidates were informed about the average time
required for the questionnaire completion—around three
minutes, and the survey objective. The ﬁnal survey received
in total 165 responses from 125 distinct authors (26% re-
sponse rate in terms of authors, 28% in terms of papers). The
anonymized responses are included in the provided dataset.
Some answers were redacted upon respondents’ request.

Survey Analysis We applied manual coding [56] to
summarize the answers to the four open-ended questions
and the free-text option of a multiple choice question. For
each question, the second and third authors of this paper
split the answers in two sets, and each individually applied
codes to a half (in a shared online spreadsheet). At least one
and up to six codes were applied to each answer. Next, the
ﬁrst author grouped together conceptually-related codes by
generalizing or specializing them, following the Qualitative
Content Analysis approach [57].

3.7 SE Venues

We assessed the practical
impact of top SE venues by
computing their patent-based impact factors as follows. We
retrieved the Google Scholar Metrics list of top publications
under Categories>Engineering & Computer Science>Software
Systems (July 2021 index),13 excluding the following entries
that center on programming languages and algorithms:
PLDI, POPL, Proceedings of the ACM on Programming
Languages, TACAS, PPOPP. For the remaining venues we
extracted their publication counts from Elsevier Scopus for
a ten-year window between 2009–2019. This window was
selected to approximate publications of all years, avoiding
bias due to different venue start dates, while the particular
range was chosen to align with the GPPD version. There-
fore, we queried all venue names combined with the year
range, restricting document types to reviews and articles for
journals, and papers for conferences, similar to Clarivate’s
impact factor calculation [58]. To retrieve the patent citation
counts of the SE venues, we searched their full names, abbre-
viations, and acronyms in the 830 379 non-patent references
of patents (Section 3.3). Finally, citation counts were divided
with publication counts to calculate the patent-based impact
factors.

3.8 SE Research Funding

We also approximated the sufﬁciency of provided SE re-
search funding by comparing it to that of the main engineer-
ing branches. We initially searched for existing empirical
analyses in the literature, but did not obtain any fruitful
results. As a workaround, we approximated funding based
on the number of existing publications and PhD disserta-
tions, given that both activities are usually grant-aided. We

8

compared SE results to those of the four main engineering
branches: civil, mechanical, electrical, and chemical [35].
For publications, we extracted from Scopus all English
papers between 2010–2020 that belong to the subject areas
Engineering, Computer Science, or Chemical Engineering, and
contain the above engineering ﬁelds in the keyword list.
For dissertations, we queried the Open Access Theses and
Dissertations (OATD) database,14 and retrieved all English
PhD dissertations from the same period, whose subject and
discipline are the corresponding engineering ﬁelds (except
for SE, where we speciﬁed computer science as discipline).
We used OATD for the following reasons. The database
contains more than six million electronic theses and dis-
sertations (ETDs) published between 1973–2022 (retrieved
April 20, 2022)15 from about 1 100 universities, colleges, and
research institutes [59]. OATD is considered one “of the
ﬁnest resources to access ETDs worldwide” [59], and is a
recommended international thesis resource for researchers
by the Networked Digital Library of Theses and Disserta-
tions (NDLTD) [60]. In a recent evaluation of four prominent
search engines on retrieving ETDs through various search
techniques including title, keyword, and author search,
OATD was ranked second in overall performance (88.5%),
closely following Google (89%), and surpassing Yahoo
(78%), and Google Scholar (76%) [61]. Furthermore, OATD
has been used in various research studies. These mainly
evaluate the quality of the database and its repositories [59],
[61], [62], investigate user behavior [63], assess and contrast
cultural heritage [64], [65], and identify the beneﬁts and
impact of open access PhD e-theses [66].

4 RESULTS

In this section we present the study ﬁndings from the patent
and survey analysis, in respect to the research questions
described in Section 3. Survey percentages are calculated
on the basis of responses (165) rather than distinct authors
(125). Codes derived from the manual coding process of the
open-ended answers are set in bold.

4.1 RQ1: Impactful Research Types, Methods, Areas

Among survey participants, 40% (66 participants) consider
that the work described in their paper resulted in some
practical impact and 29% (47) in wide practical impact, as
opposed to 21% (35) who do not believe that their work had
any practical impact, followed by 10% (17) who are unaware
of any footprint. The variety in the responses could be ex-
plained by the entailed subjectivity, the fact that awards and
citations are, by deﬁnition, not exclusively tied to practical
impact, or the authors’ different impact expectations. For
instance, authors may be more ambitious than award-givers
and researchers citing their work.

Table 1 displays the practical impact of SE research types
declared in the collected responses. Types are sorted in de-
scending order of appearance frequency. Empirical research
(e.g., investigating the adoption of engineering methods, de-
veloping new tools) appears the most common type among

12. https://www.surveygizmo.com/
13. https://web.archive.org/web/20211118180610/https:

//scholar.google.gr/citations?view op=top venues&hl=en&vq=
eng softwaresystems

14. https://oatd.org/
15. https://web.archive.org/web/20220420032348/https:

//oatd.org/oatd/search?q=*%3A*&sort=date

TABLE 1
Practical Impact of SE Research Types
(according to survey respondents’ self-evaluation)

Type

Papers

Impactful %

Empirical
Design
Theoretical

119
72
34

85
53
26

71
74
76

TABLE 2
Practical Impact of SE Research Methods

Method

Case study

Controlled/Natural experiment

Exploratory research

Action research

Ethnography

Simulation

Other

Papers

Impactful %

85

48

31

18

15

10

39

63

32

25

16

10

8

21

74

67

81

89

67

80

54

TABLE 3
Practical Impact of SE Research Areas

9

domains (e.g., operating systems) and Compilers with the
fewest publications. The most frequent subareas are Soft-
ware veriﬁcation and validation and Extra-functional properties
with moderate impact. Again, both areas and subareas can
overlap.

We further retrieved the most frequent IEEE INSPEC
controlled terms and ACM CCS concepts of the SE papers
that are cited by SE patents (Section 3.1). We retrieved the
topics of 277 ACM– and 292 IEEE-published ICSE papers,
107 TOSEM, 227 TSE, and 14 EMSE papers. The ten most
common INSPEC terms are program testing (24%—124),
software maintenance (16%—81), object-oriented programming
(13%—69), program debugging (13%—68), program diagnostics
(12%—65), software engineering (11%—57), Java (11%—56),
formal speciﬁcation (10%—54), software tools (9%—47), and
software quality (8%—42). Similarly, the ten most frequent
CCS concepts are Software testing and debugging (29%—116),
Software development process management (19%—77), Soft-
ware management (14%—55), Designing software (12%—49),
Software maintenance (12%—49), Formal software veriﬁcation
(11%—43), Program veriﬁcation (11%—43), Software design
techniques (%—42), Development frameworks and environments
(10%—40), and Software creation and management (10%—39).

ACM CCS SE Area [36]

Papers

Impactful %

4.2 RQ2: Outcomes of Impactful SE Research

Software creation and management

Software organization and properties

Software notations and tools

142

106

93

104

77

71

73

73

76

the examined publications, followed by design research (e.g.,
developing new methods) and theoretical research (e.g.,
proving properties of systems axiomatically). Papers that
were characterized by some or wide impact were grouped
together as impactful. All types proved considerably impact-
ful.

Table 2 presents the practical impact of the sample’s
employed research methods in descending frequency order.
Action research (e.g., being embedded in the development
team) and exploratory research (e.g., describing character-
istics of a population or phenomenon under investigation)
are the most impactful ones. Case study (e.g., applying a
new technique on existing systems) is the most frequent
method but less impactful. In addition to our survey’s
predeﬁned set, the following methods were also reported
for the examined publications: survey, secondary research
(e.g., systematic review, literature review, meta-analysis),
replication study, content analysis, econometric analysis,
data collection and analysis, formal theory, and design and
evaluation. Both research types and methods can overlap,
because papers may employ many of them.

The impact of SE research areas and subareas based on
the survey ﬁndings can be deduced from Tables 3 and 4,
correspondingly. Areas and subareas correspond to the ﬁrst–
and second-level entries to the Software and its engineering
concept of the ACM CCS, respectively. From Table 3, the
overall most impactful area is Software notations and tools,
but the most frequent one is Software creation and manage-
ment. The most impactful subarea (Table 4) is Development
frameworks and environments, followed by Contextual software

A total of 1 690 SE papers have been cited by 4 354 SE
patents; the 20 most-cited ones are summarized in Table 5.
The majority were published in ICSE between 1984–2013.
Through a correlation analysis between patent and aca-
demic citations, and patent citations and academic awards
(Section 3.5), Spearman’s ρ was calculated at 0.25 and 0.07,
respectively, suggesting a weak positive correlation in both
cases.

From the patents’ evaluation in terms of litigations (Sec-
tion 3.4), twelve patents citing 15 papers are associated
with 20 litigation cases concerning patent infringements,
and ﬁve of these papers are included in the 20 most-cited
ones (Table 5). No damages are documented for the 20 cases
in the USPTO Patent Litigation Docket Reports Data, but
we manually looked them up online and retrieved results
for 14 of them: six cases’ damages are undisclosed, while
the remaining eight cases’ range from six million to eight
billion dollars. Looking at the involved parties, the majority
are/were big corporations, including Apple, Kodak, Erics-
son, Facebook, Google, Lenovo, Microsoft, Oracle, Yahoo!,
Sun Microsystems, and Radware.

Regarding renewals, 1 953 (45%) patents citing 1 159
(69%) papers are linked with maintenance fee events. Eight
papers of the top ten in fee expenses are also among the
20 most-cited. The number of maintenance fee events of
patents citing papers with an equivalent number of patent
citations is higher for older ones, which is reasonable as
the related patents may have been renewed more times.
Although, in general, the number of maintenance fee events
seems to be associated with the aggregated fee value, there
are substantial differences in some cases. For example, al-
though the number of maintenance fee events is equivalent
for the works by Zhang and Cheng [69], and Moher [82],
the aggregated fees of the latter are almost double that of
the former. Such discrepancies could be explained by the

TABLE 4
Practical Impact of SE Research Subareas

10

ACM CCS SE Area [36]

Subarea

Papers

Impactful %

Software creation and management

Software veriﬁcation and validation

Software development techniques

Designing software

Software development process management

Software post-development issues

Collaboration in software development

Search-based software engineering

Software organization and properties

Extra-functional properties

Software functional properties

Software system structures

Contextual software domains

Software notations and tools

Software maintenance tools

Development frameworks and environments

System description languages

Software conﬁguration management and version control systems

General programming languages

Formal language deﬁnitions

Software libraries and repositories

Context speciﬁc languages

Compilers

TABLE 5
Most-cited SE Papers by SE Patents

62

58

48

38

32

20

11

61

49

38

6

44

38

29

20

19

14

14

11

6

47

44

37

29

26

14

9

45

38

26

5

36

36

23

15

12

9

10

9

5

76

76

77

76

81

70

82

74

78

68

83

82

95

79

75

63

64

71

82

83

#

1

2

12

13

14

15

16

17

18

19

Title

Authors

Venue

Year

Tracking down Software Bugs Using Automatic Anomaly
Detection

EDMAS: A Locally Distributed Mail System

Hangal and Lam [67]

Almes et al. [68]

3 Model-based Development of Dynamically Adaptive Software

Zhang and Cheng [69]

4

Software Deployment, Past, Present and Future

5 Aspect-oriented Programming

6

7

8

Program Slicing

The Eden System: A Technical Review

Software Engineering Economics

9 Distribution and Abstract Types in Emerald

A Cooperative Approach to Support Software Deployment
Using the Software Dock

10

11 Automated Software Test Data Generation

Dearle [70]

Kiczales [71]

Weiser [72]

Almes et al. [73]

Boehm [74]

Black et al. [75]

Hall et al. [76]

Korel [77]

Safe Software Updates via Multi-version Execution

Hosek and Cadar [78]

Predicting Source Code Changes by Mining Change History

Ying et al. [79]

Call Path Proﬁling

Hipikat: Recommending Pertinent Software Development
Artifacts

PROVIDE: A Process Visualization and Debugging
Environment

Refactoring

Hall [80]
ˇCubrani´c and
Murphy [81]

Moher [82]

Fowler [83]

ICSE

ICSE

ICSE

ICSE

ICSE

TSE

TSE

TSE

TSE

ICSE

TSE

ICSE

TSE

ICSE

2002

1984

2006

2007

2005

1984

1985

1984

1987

1999

1990

2013

2003

1992

ICSE

2003

TSE

ICSE

1988

2002

The Pan Language-Based Editing System

Ballance et al. [84]

TOSEM 1992

Recovering Traceability Links in Software Artifact
Management Systems Using Information Retrieval Methods

20 An Intrusion-Detection Model

Lucia et al. [85]

TOSEM 2007

Denning [86]

TSE

1987

Papers with bold fees are among the top ten with the highest fees.

Patent
Lit.
Cases

Patent
Maint.
Fee
Events

Patent
Maint.
Fees ($)

SE Patent
Citations

45

43

43

40

39

35

34

33

33

32

32

31

29

27

27

27

27

27

26

25

-

3

-

-

-

-

2

-

2

1

-

-

-

-

-

-

-

-

-

1

34

67

37

21

42

30

79

25

77

44

35

14

14

54

22

38

18

46

8

29

64 830

260 360

75 760

43 760

114 290

62 680

312 580

70 340

306 820

150 360

130 470

26 500

31 780

154 180

46 020

134 230

50 500

164 000

16 000

70 140

11

Along with product attributes, we also distinguish the
advancement of software processes, which provide support
throughout a software product life cycle [5]. Maintenance
was improved by “increasing the visibility of software refac-
toring research” [R52], “providing candidate patches to defects”
[R153], and by warning developers about test smell issues
[R317]. Development collaboration was enhanced through
the advancement of code review tools [R222] and the assess-
ment of global software team conﬁgurations [R76]. Analy-
sis-wise, a study’s empirical results “motivated many teams to
adopt static analysis” [R170], while other efforts found appli-
cation in metaprogramming, process and software analysis
(e.g., [R1,R49,R218,R223,R251]). To estimate cost, risk, and
effort, researchers published effective models and metrics
[R133,R211], and enhanced analogy-based reasoning tools
[R173]. Meanwhile, the objective of some studies was to
demonstrate value or raise awareness about a topic or ex-
isting work. For instance, [R112] aimed “to alert practitioners
for the need for assessing complexity”, [R115] performed an
empirical validation on quality measures, which “are now
incorporated into static analysis tools, quality assurance practices,
and as quality level agreements in software contracts”, and
[R136] “raised awareness of software development and evolution
as an economic activity”.

Signiﬁcant

impact was made on existing or new
tools. The most notable contributions
concern tools
for static program analysis [R49],
logic model check-
ing [R90], software fault and defect prediction (e.g.,
[R111,R148,R301,R302,R306]), symbolic simulation [R154],
and clone detection [R164]. Some of these tools under-
went industrial and commercial adoption from startups
[R51,R106,R156,R207] to established corporations, such as
Microsoft [R222], ABB Corporation Research [R196], Black-
Berry [R211], Huawei [R188], Rational Software [R224], and
IBM [R251]. Acquisition and licensing of tools, techniques,
and even startups, decreased the acquirers’ software devel-
opment costs, and increased substantially the inventors’ rev-
enue. As [R224] declares, they “eventually had almost one bil-
lion in annual revenue”. Moreover, the acquired work is now
used globally by numerous people (e.g., [R106]’s startup was
acquired by Facebook, leading to an approximate monthly
impact on “more than two billion people worldwide”).

Some efforts directly impacted a particular domain. To
assist domain-speciﬁc language (DSL) and IDE develop-
ment, researchers produced a visual language for software
modeling and speciﬁcation [R7], “encouraged more research
on preprocessors”, and experienced adoption of their work
by open source frameworks for feature-oriented software
development [R215]. Exploiting formal methods, ad hoc
pragmatic reuse tasks were simpliﬁed [R228], and bounded
model checking of multi-threaded software was improved,
amplifying the analysis of larger problems and “reducing the
veriﬁcation time over state-of-the-art techniques” [R97].

In the ﬁeld of systems development, we distinguish con-
tributions to version control [R57], concurrent [R90], hybrid
[R154], service-oriented [R250], and embedded software sys-
tems [R206]. A frequently occurring method is model-based
systems engineering (MBSE). Contributions to this involve
component models [R68,R242], consolidated process models
“offering the ability to streamline process analysis and redesign
work” [R223], and modeling language semantics of hybrid

Fig. 2. Practical impact of SE research.

different entity status of the citing patents: fees for large-
entity patents are twice that of small-entity, and quadruple
that of micro-entity.16

4.3 RQ3: Impactful Types of SE Research Outcomes

The results of 44% (72) of the examined publications of the
survey were incorporated in software industry processes,
practices, or methods, 41% (68) in software development
tools, 21% (35) in marketable products, 13% (22) in mar-
ketable services, and 19% (31) in other areas. In Fig. 2
we present an overview of the practical impact of SE re-
search grouped by type of outcome, as expressed in the
responses to the complementary open-ended question. For
each category we show the number of involved responses as
weights. An extensive review is included below, enhanced
with example quotes to support our interpretation of the
responses [87]. Example quotes are marked with an [RX]
notation, where X refers to the respondent’s identiﬁcation
number.

From the open-ended responses we discern the im-
provement of software product attributes, particularly de-
sign, testing, and quality. In their studies, researchers in-
troduced methods and tools for resolving conﬂicting re-
quirements [R190], and documenting and understanding a
system’s design evolution through ﬂexible solutions (e.g.,
[R48,R195,R400]), supported assertion checking [R178], test
approach and testing technique selection [R244,R246,R320],
model-based testing and prioritization [R214], and applied
techniques such as combinatorial testing [R157] and sym-
bolic execution [R154]. Quality was enhanced through fault
injection and clone detection tools (e.g., [R164,R193]), and
by analyzing software faults and failures. In this way,
researchers proposed reﬁned experiment and testing tech-
niques (e.g., [R179,R199]), proved that combinatorial testing
“can provide assurance effectively equivalent” to exhaustive
testing [R157], and also accelerated fault localization [R100].

16. https://www.uspto.gov/learning-and-resources/fees-and-

payment/uspto-fee-schedule

4338 504724Practical impactDemonstratevalue or raiseawarenessProvide novelmethodsImprovesoftwaremaintenanceLead toacquisition andlicensingImprovesoftwarequalityImprovesoftwaretestingImprovesoftwareanalysisImprovesoftwaredesignFormgroundworkfor furtherresearchCoverinformationretrieval needsProvidesoftwaremetadataImprove cost,risk, effortestimationImprovedevelopercollaborationLead to industrial andcommercialadoptionComplementexisting oroffer new toolsExploit formalmethodsEnhanceMBSEAssist DSLand IDEdevelopmentFacilitatesystemsdevelopment24234Software tools815759Software processes5192016Softwareproductattributes817622SE research47162SE domainsImprove faultand failureinspectionsystems [R154].

Various studies affected subsequent research. Some
formed the groundwork for further research, obtaining
multiple academic references [R5,R6,R7,R197]. As [R116]
states, their work on software process models “blew up the
foundation of clean top-down models and forced consideration
of real world issues”. Some novel methods concern qualita-
tive research methods aiming to “help researchers immerse
themselves more fully in practice” [R163], goal orientation
in requirements engineering [R189], experience base au-
tomation [R243], and standardized methods (e.g., drawing
binary trees [R70], collecting data [R234], performing man-
ual inspections [R316]). By providing software metadata,
researchers managed to “reduce the cost of widely used soft-
ware practices” [R54] and to increase analysts’ efﬁciency
[R1,R227,R309]. To cover information retrieval needs, re-
searchers proposed new research agendas [R166], inﬂuenced
source code search solutions [R196], and identiﬁed sub-
stantial mismatches between IDE designs and information
seeking [R102,R103].

Lack of practical impact: Respondents who reported no
practical impact of their work were subsequently encour-
aged to determine the reasons for this inefﬁciency. Some
efforts that could have been impactful did not succeed due
to an immature phase, inefﬁciencies, or additional required
effort and resources. In some cases, a longer time horizon
was needed to cause an impact [R213,R216]. As [R213]
explains, their paper “is at the intersection of programming
language (PL) and SE research, and PL research has a longer
time horizon than typical SE research”. Concerning inefﬁ-
ciencies, some papers contained wrong assumptions [R47],
tooling issues [R64], undocumented practical uses [R149],
incomplete resolution of the addressed problem [R226],
and rarely employed research methods [R118]. Furthermore,
some research outcomes have not been used by practice
due to high implementation and maintenance costs [R226],
risk aversion for technology commercialization [R81], and
paucity of maintenance by their creators [R64].

Additional causes involve undetected needs and lack of
support by SE practice. In [R225]’s words, “most projects feel
they are doing ‘good enough’ with their existing processes”, and
“a project will not invest the time and effort [into integrating a
process such as triage recommendation] unless there is a perceived
signiﬁcant beneﬁt”. Regarding support, some participants
expressed their concerns about SE practice neglecting re-
search, for example, in requirements engineering [R141] and
defect density estimation [R321], while others recognized
high barriers of work adoption in programming languages
[R213], and tool integration challenges in the current SE
community [R144]. It comes as no surprise, therefore, that
trending software development tools in worldwide practice
have industrial provenance [88], [89]. The academic and
industrial research lab foundations of a few popular version
control and programming language technologies are by now
many decades old [90], [91], [92], [93].

Although some were unaware of any incorporation of
their results into development tools, products, or services
[R120,R304], the majority of responses referred to an indi-
rect research impact. This includes fundamental research
contributions [R79,R80,R94], for instance, to distributed sys-
tems [R167], surveys [R91,R96,R304], research methods for

12

conducting computational experiments [R75,R172], guide-
lines for empirical research [R161,R300,R314], design con-
cepts [R212], as well as contributions that “do not work on
real systems” [R185,R186,R315]. As [R216] mentions, having
no practical impact is not necessarily “a bad thing”, especially
when a work inﬂuences other researchers, or facilitates
the exploration of different ﬁelds and the development of
techniques.

4.4 RQ4: Practical Impacts of SE Research on Informa-
tion Technology, Society, and Industry

The authors’ responses to the related open-ended question
revealed various changes of SE research in the state of
practice. Through their prototypes, practitioners advanced
production technology and facilitated the open source
community. Speciﬁcally, they improved practices involving
mining design patterns from source code [R48], code re-
viewing [R222], clone detection [R169], reverse engineering
and program understanding [R171], systems development
[R90], quality and robustness [R175,R193], testing and anal-
ysis [R157,R209,R210], model checking [R159], remodular-
ization optimization [R62], and analogy-based reasoning
[R173,R174]. Various prototypes were quickly adopted by
open source projects [R164,R195]. Software development
companies incorporated researchers’ approaches into their
processes [R173,R174,R209] and product lines (e.g., [R210]).
We also observe adoption by competitors. Although this is
“not exactly desirable from a company perspective, at least the
research had an impact” [R170]. Collaborations with compa-
nies beneﬁted both parties, regardless; companies improved
their solutions, while individuals promoted their open
source tools gaining numerous downloads [R173,R174].

Several attempts were made towards extending the
functionality of existing open source and proprietary prod-
ucts. Researchers provided solutions to code recommen-
dation projects [R229], algorithms to graph layout tools
[R303], support for free text search to code search engines
[R166,R196], conﬂict modeling techniques to commercial
tool sets [R190], bidirectional streaming to HTTP/2 and rel-
evant frameworks [R207], multi-threaded software veriﬁca-
tion methods to state-of-the-art tools [R97], and online tools
for visualizing map representations of GitHub code clones
[R164]. Version control systems were incorporated into Unix
system distributions, eliciting their widespread adoption
by universities and technology-leading corporations—“the
trend setters” [R57]. Moreover, the identiﬁcation of perfor-
mance bug patterns and the development of static perfor-
mance checkers allowed Android developers to “generate
real-time warnings when they are writing code” [R127], while
the construction of rules matrices helped companies “to
differentiate under which legal conditions a dataset can be used
for analysis” [R156].

Along with functionality, practitioners improved prod-
ucts’ design and modeling, quality, and performance. We
observe new architecture concepts [R206], software mod-
eling languages [R152], hybrid system modeling plug-ins
[R154], and project management frameworks [R76]. Support
was provided for identifying countermeasure requirements
by modeling and analyzing threats at the application level
[R187], and for encompassing visual formalisms, includ-
ing statecharts, in modeling processes [R220]. Quality-wise,

some products upgraded the robustness of cloud manage-
ment platforms [R193], the measurement and decrease of
attack surfaces in software systems [R162], the long-term
evaluation of regression testing techniques [R155], and the
independent veriﬁcation and validation of proprietary tools
[R227]. Performance-wise, researchers accelerated testing
[R210] and root cause analysis [R251], and suggested solu-
tions for extending the lifetime of products while reducing
their maintenance costs [R197].

Some studies aimed to advance development processes,
release new methods, or educate researchers and improve
their professional practices. For instance, factor-covering
array generation tools impacted statistical testing and anal-
ysis [R157], model checking techniques affected code anal-
ysis [R159], object-oriented design metrics improved soft-
ware development feedback [R239], data analysis was en-
hanced by linking code reviews to commits [R222], analogy-
based reasoning improved prediction and estimation [R173],
and goal-oriented risk analysis impacted requirements en-
gineering [R188,R189,R190]. In short, new methods en-
tail approaches for developing and reﬁning software de-
velopment processes [R116,R246,R250,R301,R302,R306] and
domain-speciﬁc languages [R218], model-based require-
ments [R184], model checking [R159], formalism-oriented
abstraction levels [R138], conﬂict modeling [R190], syntactic
preprocessors for implementing variability [R215], conﬁg-
uration management for component models [R152], fault
injection [R193], and accident models for safer systems
[R6]. Proposed practices involve abandoning “information
retrieval-based traceability link recovery approaches due to low
performance” [R69] and copy-pasting [R51], favoring “mini-
mization of coupling” [R126], carefully testing code segments
that are predicted to contain faults [R111], using software
metrics to predict programmers’ performance [R115], being
careful with attack surfaces of software systems [R162],
preferring alternative metrics to test coverage [R221], and
detecting and mitigating “architectural mismatch” [R99].

Although some studies may not have directly changed
the state of practice through prototypes and product add-
ons, they still inﬂuenced follow-up work and had a re-
search impact. Second-order mutation algorithms affected
mutation testing tools [R142], a work “served as a guideline
for reusing components” [R242], various doctoral dissertations
were inspired by research on reverse engineering and pro-
gram comprehension [R171], invalid techniques deployed in
safety-critical systems were revealed [R183], and the effect of
design decisions on inspection performance was evaluated
[R316]. Experiments were used as a baseline for subsequent
research in aspect-oriented programming [R124], speciﬁca-
tion models led to further academic research and funding
[R214], and some works were “instrumental in ushering in the
current era of data-driven thinking in SE” [R54]. Finally, some
publications led to author promotion [R116].

4.5 RQ5: SE Venues with Practice Impact

The patent-based impact factors of the top SE venues are
presented in Table 6 in descending order, complemented
with the corresponding number of publications and SE
patent citations. Journals are denoted with J and conferences
with C. The most impactful venue appears to be the IEEE

TABLE 6
Practical Impact of SE Venues

13

Type Name

Publisher

Papers

SE Patent
Citations

Impact
Factor

J

C

C

J

C

J

C

C

C

C

J

J

C

J

J

IEEE Trans. Softw. Eng.

IEEE

MSR

ISSTA

IEEE/ACM

ACM SIGSOFT

Softw. Pract. Exp.

Wiley

ICSE

ACM/IEEE

IEEE Software

IEEE

FSE

ICSME

ASE

SANER

J. Syst. Softw.

Inf. Softw. Technol.

RE

Softw. Syst. Model.

Empir. Softw. Eng.

ACM SIGSOFT

IEEE

IEEE/ACM

IEEE

Elsevier

Elsevier

IEEE

Springer

Springer

681

469

435

666

3 367

1 136

1 061

940

1 120

746

1 979

1 136

683

549

579

1 988

556

473

709

2 497

833

614

511

596

355

364

207

73

53

25

2.919

1.185

1.087

1.064

0.742

0.733

0.579

0.544

0.532

0.476

0.184

0.182

0.107

0.096

0.043

TSE, followed by the International Conference on Mining
Software Repositories (MSR) and the ACM SIGSOFT In-
ternational Symposium on Software Testing and Analysis
(ISSTA).

4.6 RQ6: Sufﬁciency of SE Research Funding

For each engineering branch, we obtained the total number
of publications and PhD dissertations through the process
described in Section 3.8. In this way we retrieved a total
of 56 679 software, 28 514 civil, 23 969 mechanical, 22 851
electrical, and 3 138 chemical engineering publications. With
regard to dissertations, we identiﬁed 55 SE, 302 civil, 2 018
mechanical, 1 861 electrical, and 1 147 chemical engineering
records. Overall, SE appears ﬁrst in publications but last in
dissertations.

5 DISCUSSION

More than ﬁfty years after the launch of the ﬁrst SE con-
ference series, the SE research discipline can be proud
of numerous tangible contributions to practice. From the
survey analysis it appears that researchers have equipped
their industrial partners with a swarm of new open source
software tools, novel development processes and methods,
and advanced professional practices. In addition, they man-
aged to expand the quality and scope of existing proprietary
products, showcasing the importance of maintaining strong
academia-industry ties. However, the quantitative results
of the patent analysis contradict the self-assessed survey
results, since only a limited number of the collected patents
cite any SE paper of the examined venues. Still, almost
half (45%) of the patents citing 69% of the SE papers have
been renewed, paying out substantial corresponding fees.
(The maintenance fees in the US alone range from $500 to
$7 700.17) These facts demonstrate the inﬂuence of SE papers
on valuable patents.

Introspective SE Research Looking at the practical im-
pact of SE research types, areas, and methods (Section 4.1),

17. https://www.uspto.gov/learning-and-resources/fees-and-

payment/uspto-fee-schedule#Patent%20Maintenance%20Fee

we deduce that in a narrow sense the SE ﬁeld is introspec-
tive in its nature. Researchers mostly generate knowledge,
tools, and reviews about SE practices and methods to ad-
dress their own needs [94]. This, by deﬁnition, limits the po-
tential footprint of SE research only within its own bound-
aries, which is typically not the case for other ﬁelds such as
artiﬁcial intelligence and machine learning [95]. Consider,
for example, the application of the latter in the petroleum
industry [96], biotechnology [97], Internet of Things [98],
public administration [99], healthcare [100], and earthquake
engineering [95]. As a result, we would expect SE research
to reach a narrower industrial audience compared to other
science and engineering disciplines.

Yet,

in a wider sense SE, by taming the complex-
ity of software development, has allowed the exponential
growth [101] of the sophisticated software intensive systems
that underpin the modern economy, science, and way of
living. This still leaves open the question of the role of SE
research in this progress.

Cross-disciplinary SE Areas SE practice is impacted by
cross-disciplinary SE areas. This is reasonable considering
that, according to the Guide to the Software Engineer-
ing Body of Knowledge (SWEBOK) [5], SE intersects with
diverse computer science areas and other disciplines. A
considerable portion of impactful SE research pertains to
areas related to programming languages, compilers, and
management (Section 4.1). Furthermore, looking at the CPC
categories of SE patents citing SE research (Section 3.2) we
observe that some of them (G06F8/31, G06F8/37, G06F8/41,
G06F8/53) are associated with the aforementioned areas.
Previous work has also demonstrated that SE researchers
are very interested in human factors [32]. Consequently,
it might be worth investigating in isolation the impact of
cross-disciplinary and specialized SE areas to SE practice by
studying more specialized venues, such as the Conference
on Programming Language Design and Implementation, the
International Conference on Object-Oriented Programming
Systems, Languages, and Applications, the International
Conference on Functional Programming, and the Sympo-
sium on Principles of Programming Languages.

SE Venues With regard to SE venues, one might wonder
whether practitioner-oriented ones are more impactful. We
assessed the top SE venues by computing their impact
factors on the basis of SE patent citations (Table 6). We
observe that practitioner-oriented venues, such as Software:
Practice and Experience and IEEE Software, are higher in the
ranking compared to researcher-oriented ones, such as Em-
pirical Software Engineering. Moreover, we see a large number
of conferences ranked considerably high. This could be an
effect of the practitioner-oriented tracks of these conferences
including ICSE’s Software Engineering in Practice, MSR’s Min-
ing Challenge, ISSTA, FSE and ICSME’s Tool Demonstrations,
and FSE and ICSME’s Industry Showcase [102]. To increase
their industrial appeal, less practitioner-oriented journals
and conferences might want to consider including dedicated
industrial topics, calls, and tracks for contributions, similar
to the aforementioned examples.

Correlation Analysis From the patent analysis we ob-
tained no concrete evidence of relationship between patent
citations and impact of SE research in practice. Although
previous work has demonstrated positive correlation be-

14

tween patent and academic citations [34], our correlation
analysis (Section 4.2) did not show a sufﬁcient associa-
tion between the two. In addition, looking at the most-
cited SE papers by SE patents (Table 5) we infer that
many (e.g., [70], [74]) are knowledge– rather than solution-
seeking [103]. Their main use as citations in patent docu-
ments is most likely the provision of background informa-
tion. Solution-seeking studies usually produce algorithms,
models, and tools to cope with practical problems [104],
whereas knowledge-seeking ones employ cross-disciplinary
research methods (e.g., case studies, surveys) to explain
knowledge problems [104] of SE practice (e.g., to evaluate and
compare different tools, or to study developers’ collabora-
tion) [105].

Knowledge-seeking SE Research The strong appear-
ance of knowledge-seeking studies in patent citations could
be justiﬁed by the fact that SE (beyond the study scope—
Section 1) is a complex discipline consisting of various
dimensions (also knowledge areas in SWEBOK [5]). Apart
from product-related areas (e.g., Software Construction and
Testing), there are also areas associated with the processes,
methods, and models employed in the software construction
(SE Process, SE Models and Methods), the management
and cooperation of the development teams (SE Professional
Practice), and the project management of the software de-
velopment (SE Economics). These topics are highly relevant
to the industry [88], and this could be the reason for their
high citation numbers. Therefore, the interpretation of the
association between patent citations and practical impact
depends on the impact deﬁnition.

Impact Deﬁnition The dependence of our analysis on
the impact deﬁnition, along with the dissent of some survey
respondents from our perceived sense of impact, constitutes
a need for a formal term description. From the answers to
RQ3 and RQ4 (Sections 4.3, 4.4) we infer that the inﬂuence
of subsequent research was rated both as practical and non-
practical impact by respondents. To allow future studies
building on our work to quantify and assess the impact
of SE research in practice, academia and industry need to
jointly agree on what constitutes impact based on empiri-
cally validated research. In this regard, we propose as term
description our impact deﬁnition in Section 1.

SE Research Funding One prominent hindrance to
the development and commercialization of academic SE
research products appears to be insufﬁcient funding (Sec-
tion 4.3). Although some research ideas may be promising
for the industry, they seem to struggle to evolve due to a
lack of ﬁnancial resources. These deﬁciencies concern both
project-based and long-term funding, which is required for
a software product’s maintenance after the project comple-
tion [106]. In addition, trending industrial research topics,
such as the metaverse, self-driving cars, space, robotics, and
quantum computing [31], are often ﬁnancially unbearable
for academia. In 2021, the R&D budget of America’s top
ﬁve tech companies (Amazon, Alphabet, Meta, Apple, Mi-
crosoft) added up to 149 billion dollars, almost a quarter
of America’s 2020 public and private R&D investment (713
billion) [31]. To this end, SE researchers have repeatedly
stressed the need for higher ﬁnancial support by society to
be able to perform more realistic studies for the industry
[107], [108], [109]). Particularly, Sjøberg et al. [109]
(e.g.,

argued that, given the recognized value of software in
business [110], the discipline should not fall back in funding
compared to other ﬁelds, including natural sciences and
medicine. But there is also a matter of priorities resulting
from the ﬁnite amount of available resources: research is the
steering wheel of innovation, but it also needs to cater to
ﬁnancial and societal needs [111].

Due to the lack of empirical data supporting the afore-
mentioned remarks, further analysis is needed to assess the
sufﬁciency of SE funding. To set the path, we approximated
funding based on the number of existing SE publications
and PhD dissertations, and compared them to those of
the main engineering branches (Section 4.6). The resulting
data are inconclusive: SE seems to outnumber the other
branches in publications, while trailing in dissertations. This
can be an effect of the employed search method, leading to
false positives or missed records. Or it could be the case
that SE researchers can get away with publishing studies
of low-hanging fruit, with carefully developed methods
and impeccable text, but having limited practical impact—a
practice that may not be widely tolerated in more mature
engineering disciplines. A comprehensive study of SE re-
search funding could shed light on this issue, and provide
well-grounded insights regarding the funding’s adequacy.

Tech Transfer Challenges Beyond funding, a set of
technology transfer challenges arise from the related work
(Section 2) and the survey answers to RQ3 (Section 4.3).
These can be categorized into two broad areas: industrial
obstacles (i.e., adoption barriers by practice) and research
challenges (i.e., limitations of the proposed work). Con-
cerning the ﬁrst, we observe disregard and questioning
of research progress by practice. In a recent (non peer-
reviewed) study Koziolek reports that practitioners appear
willing to participate in case studies and experiment with
new methods from academia, regardless of the return-on-
investment [112]. However, according to two recent large-
scale developer surveys by JetBrains [88] and Stack Over-
ﬂow [89], the majority of most-used software development
tools in practice are developed in the industry or by practi-
tioners by a vast margin, and any research results they ap-
pear to incorporate were developed many decades ago [90],
[91], [92], [93]. The reasons for the industrial provenance of
tools used by practitioners could be the products’ technol-
ogy readiness [113], stability, usability, trustworthiness, fea-
ture customization, exclusive customer support and train-
ing, and collaboration ties between companies and software
vendors [114], [115]. Furthermore, companies developing
their own software products may be reluctant to work with
open source software coming outside their organization—
also known as the not invented here syndrome [116]. These
factors hinder the adoption of academic technology by
practice.

Regarding research challenges, we notice an unfavorable
cost-beneﬁt trade-off of some proposed solutions, despite
their high citation numbers and awards. We summarize
the following cases: useful ideas that are technology imma-
ture [113], offer conceptual software design solutions [117],
and require a long time span and re-engineering to apply in
the industry [8]; redundant ideas that do not meet industrial
needs or quality standards [12]; research methods suffering
from conﬁrmation bias and unreported implications [112];

15

and promising ideas documented in long, inscrutable pa-
pers that require hours of reading [21]. Reasons leading
to these deﬁciencies could be the publish or perish cul-
ture [118] “forcing scientists to produce publishable results
at all costs” [119], excessive competition over collaboration
among researchers [120], and reductionist thinking hinder-
ing scaling of complex software systems [55], [121].

Best Practices To address the outlined challenges, sev-
eral best practices have been proposed in the related lit-
erature, aiming to bridge the gap between academia and
industry (e.g.,
[6], [24], [25]). Notable recommendations
from Section 2.2 include: grounding research on real-world
problems and reducing generalizability issues; publishing
more accessible content (e.g., through blogs or videos) and
advertising it through social media; producing more action-
able empirical studies, for example, studies on development
productivity and effort estimation techniques; ensuring
open-access availability of research studies; conducting pilot
laboratory tests before industrial releases; and organizing
regular workshops with the industry. In addition, adopting
the open science and ACM Artifact Badging approaches can
facilitate the adoption and application of research products
by independent, unguided users [112]. Collaboration be-
tween academia and software vendors, who already have
an established business model, could also ease the integra-
tion of research products into existing tools through plug-
ins [112]. From a company perspective, practitioners should
systematically evaluate and improve methods they have not
produced themselves [112]. It might be worth investigating
empirically the extent to which such recommended practices
have been adopted by industry and academia as well as any
factors impeding their application.

6 LIMITATIONS

Here we present the risks resulting from the patent and
survey analyses. The survey was designed with the stated
goal of examining how SE research has impacted practice.
For this purpose, we followed recommended guidelines for
survey research [51], [52]. Response options of two out of
three multiple choice questions were adapted from estab-
lished literature [36], [55], and the complete questionnaire
was validated through two pilot runs. Although the survey
was mainly characterized as interesting and potentially
impactful, its results are limited by the reasons detailed
below.

Internal Validity Some risks to the internal validity of
the study stem from the manual processes involving sub-
jective judgment. These include the identiﬁcation of the SE-
related CPC categories (Section 3.2), the mapping of patent
maintenance fee codes to their fee values (Section 3.4), and
the manual coding of the open-ended survey responses.
Although biases related to human judgment cannot be
completely eradicated [122], we aimed to reduce this threat
by employing established methods [41], [56]. Manual coding
also entails loss of accuracy of responses, due to the extreme
level of their categorization; we addressed this by assigning
multiple (rather than only one) codes to each answer.

Risks also rise from some automated processes we em-
ployed. The identiﬁcation of SE papers cited by SE patents
through DOI crosschecking, and title and author mapping

is not completely accurate, according to our evaluation
(Section 3.3). The identiﬁcation of surveyed practitioners
(Section 3.6) does not account for 118 (20%) papers lacking
DOIs. Similarly, the identiﬁcation of SE venues in patent
citations based on their full names, abbreviations, and
acronyms (Section 3.7) may have led to some false positives
in Table 6. In the same Table, the number of MSR papers
does not include records for the years 2010, 2011, and
2018, because these are not tracked by Scopus. Publication
and PhD dissertation counts for the engineering disciplines
may also include false positives, as conﬁrmed in Section 5.
Also, the research funding evaluation (Section 4.6) does
not make a distinction between academic and industrial
research funding. One might expect the former to be less
impactful than the latter. Further analysis is required to
assess this difference.

A few limitations are associated with the analysis of
SE patent citations of SE papers (Section 3.3). The analysis
does not account for instances where an author of a patent
may also be involved in the cited publication either as a
direct author or as a collaborator of the research team. There
may also be cases where authors employ both publication
means (scientiﬁc publication and patent) and one cites the
other to increase the publication’s value. Moreover, patent
citations were not screened to determine the actual use
of SE research. Consequently, citations that may reference
a study as an example, comparison, statement source, or
related work [123] may have skewed the associated quanti-
tative results in Sections 4.1, 4.2, and discussion comments
(Section 5). Although we evaluated patents based on their
litigation and maintenance fee events (Section 4.2), this
does not guarantee that all patented products are impactful.
These cases require special treatment to verify the practical
impact.

Further concerns may result from the patent-to-science
citation linkages regarding novelty. Novelty can be a deal
breaker when preparing a patent application: turning an
already published idea into a patent is usually very hard,
in many jurisdictions a showstopper [124]. However, to
cope with this challenge, it is a recommended practice to
adopt the patent ﬁrst, publish later approach [124], according
to which researchers ﬁle a patent application before they
publish it in a research paper. This approach would result
in patent-publication chains, where, for example, patent A
is cited by related-publication A, publication A is cited by
improvement-patent B, patent B is cited by related-publication
B, and so forth. The publication-to-patent part of this chain
(i.e., publication A is cited by patent B) is the center of our
patent citation analysis (Section 4.2). As a result, the cited
publication and the citing patent can refer to different—
yet possibly similar—research ideas. Still, there may also be
cases of citations diverging from this approach.

The survey questionnaire is associated with two inherent
biases. Social desirability bias [125] (i.e., a respondent’s
potential tendency to appear in a positive light, for example,
by showing they are fair or rational) is a risk associated
with the survey responses to RQ3 and RQ4. For instance,
one should not over-interpret that the majority (69%) of
respondents consider their surveyed work to have impacted
either partially or widely SE practice. To mitigate this is-
sue, participants were informed that their responses would

16

be published anonymized. Furthermore, question-order ef-
fect [126] (e.g., one question may have provided context
for the subsequent one) may have inﬂuenced respondents’
answers. Although this effect could have been reduced, for
example, by shufﬂing questions, we opted to order them in a
rational sequence for participants to recall and comprehend
the context of the asked questions.

The collected survey feedback also revealed the follow-
ing limitations. The deﬁnition of practical impact that was
provided to candidates was an earlier version of the one
introduced in Section 1. This did not successfully clarify that
software development tools impacted by research should
only be industrial, hence answers to RQ3 and RQ4 may also
concern non-industrial tools. In addition, the subjective and
limited deﬁnition of the term along with its quantiﬁcation
process roused concerns to some respondents who empha-
sized that impact “can almost only be assessed by people working
in closely related research areas” [R80]. The classiﬁcation pro-
cess using the 2012 ACM CCS was deemed difﬁcult and ir-
relevant by some participants. Others advised against using
institutional e-mail accounts in surveys, because these are
often overloaded with unsolicited messages, and may thus
limit response rate. To address this, in our manual search we
looked for active e-mail addresses in researchers’ personal
web pages (Section 3.6). Finally, the author selection process
was not sufﬁciently documented in the invitational mail
causing confusion to some respondents.

External Validity Generalizability concerns arise from
the survey sample selection process (Section 3.6), which is
limited to the ﬁrst authors and the venues of ICSE, TSE,
TOSEM, and EMSE, only includes the most-cited, awarded
distinguished, and most inﬂuential works, and involves a
small number of practitioners (10%). Still, these practitioners
are afﬁliated with 36 diverse companies. To counterbalance
this shortcoming, we aimed to cover all publication years,
motivated survey candidates to include and review addi-
tional impactful publications of theirs, and also invited them
to list any notable papers of other researchers that may have
come to their attention. Although these concerns prevent us
from generalizing the survey ﬁndings, meaningful insights
emerged, which could be ampliﬁed through study replica-
tion in other research outlets and author sets. Furthermore,
the computation of the patent-based impact factors of the
top SE venues (Section 3.7) is affected by the restricted
year range (2009–2019). As explained, the rationale was to
approximate publication counts of all years, preventing any
risk that could occur from different venue start dates.

7 CONCLUDING REMARKS

We investigated the impact of SE research in practice
through a systematic analysis of science linkages between
SE research and SE patents, and a survey on authors of
top-notch publications. Speciﬁcally, we identiﬁed impactful
types, areas, and methods of SE research, the outcomes
of impactful research, and its main practical impacts on
information technology, society, and industry. We further
assessed the impact of SE venues and the sufﬁciency of SE
research funding by comparing it to the main engineering
branches’. To address these, we collected 11 419 papers
from ICSE, TSE, TOSEM, and EMSE between 1975–2017,

and complemented them with their assigned topics, citation
counts, and awards. We also retrieved a set of 304 368 SE
patents, and found 1 690 papers cited by 4 354 of them. To
assess patents’ value, we analyzed their litigation cases and
maintenance fee events. Through a survey on 475 authors
of 593 top-cited and awarded papers (26% response rate),
we complemented our study results with quantitative and
qualitative insights. For the venues’ impact we computed
their patent-based impact factors, while for the adequacy of
SE research funding we retrieved the research publication
and dissertation counts of SE and the main engineering
branches. The study’s key ﬁndings are summarized below.
• SE researchers have equipped practitioners with various
tools, processes, and methods, and improved many ex-
isting solutions. Moreover, practitioners seem to value
knowledge-seeking studies.

• SE practice is impacted by cross-disciplinary SE areas,
hence it could be of value to assess this inﬂuence by
studying in more depth some specialized venues.

• Practitioner-oriented tracks in conferences may enhance
their impact. A dedicated study of these tracks could
provide more insights as well as useful recommendations
to organization and program committees.

• Academia and industry could jointly agree on a formal
impact term description based on empirically validated
research and backed by key performance indicators to set
a common ground for subsequent research on the topic.
• There is a claim for higher funding in SE research, which
we cannot corroborate through our analysis on engineer-
ing dissertations and publications, or literature search. A
comprehensive empirical study could shed light on the
matter.

Research Agenda There are various directions for ex-
tending the investigation of the SE research impact in
practice—some are listed in the preceding paragraphs.
Other ideas include: studying the patenting behavior by
SE research area, and investigating the practical impact
of less patent-focused areas (e.g., agile/lean methods, test
automation); conducting in-depth systematic analyses of SE
patent citations to delve into the use of academic research
by patents; and comparing reports of industry impact (e.g.,
company statements, software development metrics) with
venue, funding, and citation metrics. With these ﬁnal re-
marks we aim to steer academia’s attention towards some
research topics requiring further investigation, and begin
a discussion on how we, the researchers, can increase our
footprint on practice.

ACKNOWLEDGMENTS
We would like to thank ACM for providing us with the
ACM DL Abstracts and Titles for Research Purposes database.
This work has received funding from the European Union’s
Horizon 2020 research and innovation programme under
grant agreement No. 825328 (FASTEN project).

REFERENCES

[1]

P. Naur and B. Randell, Software Engineering: Report of a
Conference Sponsored by the NATO Science Committee, Garmisch,
Germany, 7–11 Oct. 1968. Brussels, Scientiﬁc Affairs Division,
[Online]. Available: http:
NATO, 1969, accessed June 2022.
//homepages.cs.ncl.ac.uk/brian.randell/NATO/nato1968.PDF

17

[6]

[5]

[3]

[2]

[4]

Industries:

J. N. Buxton and B. Randell, Software Engineering Techniques:
Report of a Conference Sponsored by the NATO Science Committee,
Rome, Italy, 27–31 Oct. 1969. Brussels, Scientiﬁc Affairs Division,
NATO, 1970, accessed June 2022.
[Online]. Available: http:
//homepages.cs.ncl.ac.uk/brian.randell/NATO/nato1969.PDF
P. Mell and T. Grance, “The NIST deﬁnition of cloud computing,”
NIST Special Publication 800-145, 2011.
Statista:
Technology & Telecommunications,
“Statistics and market data on software,” 2018, accessed
[Online]. Available: https://www.statista.com/
June 2022.
markets/418/topic/484/software/
P. Bourque and R. E. Fairley, Eds., Guide to the Software
Engineering Body of Knowledge, Version 3.0.
IEEE Computer
Society, 2014. [Online]. Available: www.swebok.org
L. J. Osterweil, L. A. Clarke, M. Evangelist, J. Kramer, D. Rom-
bach, and A. L. Wolf, “The Impact Project (panel session): De-
termining the impact of software engineering research upon
practice,” SIGSOFT Softw. Eng. Notes, vol. 25, no. 6, pp. 108–109,
2000.
F. Narin, K. S. Hamilton, and D. Olivastro, “The increasing
linkage between U.S. technology and public science,” Res. Policy,
vol. 26, no. 3, pp. 317–330, 1997.
J. Estublier, D. Leblang, A. v. d. Hoek, R. Conradi, G. Clemm,
W. Tichy, and D. Wiborg-Weber, “Impact of software engineering
research on the practice of software conﬁguration management,”
ACM Trans. Softw. Eng. Methodol., vol. 14, no. 4, pp. 383–430, 2005.
[9] National Academy of Engineering, The Impact of Academic Re-
search on Industrial Performance. The National Academies Press,
2003.
J. Bessen and R. M. Hunt, “An empirical
patents,” J. Econ. Manag. Strategy, vol. 16, no. 1, 2007.

look at software

[10]

[8]

[7]

[11] D. C. Ince, L. Hatton, and J. Graham-Cumming, “The case for
open computer programs,” Nature, vol. 482, no. 7386, pp. 485–
488, 2012.

[12] D. Lo, N. Nagappan, and T. Zimmermann, “How practitioners
perceive the relevance of software engineering research,” in
Proceedings of the 10th Joint Meeting on Foundations of Software
Engineering. ACM, 2015, pp. 415–425.

[13] L. Osterweil, C. Ghezzi, J. Kramer, and A. Wolf, “Editorial,” ACM
Trans. Softw. Eng. Methodol., vol. 14, no. 4, pp. 381–382, 2005.
[14] ——, “Determining the impact of software engineering research

on practice,” Computer, vol. 41, no. 3, pp. 39–49, 2008.

[15] B. G. Ryder, M. L. Soffa, and M. Burnett, “The impact of software
engineering research on modern progamming languages,” ACM
Trans. Softw. Eng. Methodol., vol. 14, no. 4, pp. 431–477, 2005.
[16] L. A. Clarke and D. S. Rosenblum, “A historical perspective on
runtime assertion checking in software development,” SIGSOFT
Softw. Eng. Notes, vol. 31, no. 3, pp. 25–37, 2006.

[17] W. Emmerich, M. Aoyama, and J. Sventek, “The impact of
research on the development of middleware technology,” ACM
Trans. Softw. Eng. Methodol., vol. 17, no. 4, 2008.

[18] D. Rombach, M. Ciolkowski, R. Jeffery, O. Laitenberger, F. Mc-
Garry, and F. Shull, “Impact of research on practice in the ﬁeld of
inspections, reviews and walkthroughs: Learning from successful
industrial uses,” SIGSOFT Softw. Eng. Notes, vol. 33, no. 6, pp. 26–
35, 2008.

[19] B. Boehm and R. Valerdi, “Impact of software resource estimation
research on practice: A preliminary report on achievements,
synergies, and challenges,” in Proceedings of the 33rd International
Conference on Software Engineering. ACM, 2011, pp. 1057–1065.

[20] A. T. Misirli, B. Caglayan, A. Bener, and B. Turhan, “A retro-
spective study of software analytics projects: In-depth interviews
with practitioners,” IEEE Software, vol. 30, no. 5, pp. 54–61, 2013.
S. Beecham, P. OLeary, I. Richardson, S. Baker, and J. Noll,
“Who are we doing global software engineering research for?”
in Proceedings of the 8th International Conference on Global Software
Engineering.

IEEE, 2013.

[21]

[22] V. Ivanov, A. Rogers, G. Succi, J. Yi, and V. Zorin, “What do soft-
ware engineers care about? Gaps between research and practice,”
in Proceedings of the 11th Joint Meeting on Foundations of Software
Engineering. ACM, 2017, pp. 890–895.

[23] T. Mikkonen, C. Lassenius, T. M¨annist ¨o, M. Oivo, and J. J¨arvinen,
“Continuous and collaborative technology transfer: Software en-
gineering research with real-time industry impact,” Inf. Softw.
Technol., vol. 95, pp. 34–45, 2018.

[24] V. Garousi, K. Petersen, and B. Ozkan, “Challenges and best
practices in industry-academia collaborations in software en-

gineering: A systematic literature review,” Inf. Softw. Technol.,
vol. 79, pp. 106–127, 2016.

[51]

[25] V. Garousi, D. Pfahl, J. M. Fernandes, M. Felderer, M. V. M¨antyl¨a,
D. Shepherd, A. Arcuri, A. Cos¸kunc¸ay, and B. Tekinerdogan,
“Characterizing industry-academia collaborations in software
engineering: Evidence from 101 projects,” Empir. Softw. Eng.,
vol. 24, no. 4, pp. 2540–2602, 2019.

[26] B. Vasilescu, A. Serebrenik, T. Mens, M. G. van den Brand, and
E. Pek, “How healthy are software engineering conferences?” Sci.
Comput. Program., vol. 89, pp. 251–272, 2014.

[27] R. L. Glass, “An assessment of systems and software engineering
scholars and institutions,” J. Syst. Softw., vol. 27, no. 1, pp. 63–67,
1994.

[28] W. E. Wong, T. Tse, R. L. Glass, V. R. Basili, and T. Y. Chen, “An
assessment of systems and software engineering scholars and
institutions (2003–2007 and 2004–2008),” J. Syst. Softw., vol. 84,
no. 1, pp. 162–168, 2011.
J. Ren and R. N. Taylor, “Automatic and versatile publications
ranking for research institutions and scholars,” Commun. ACM,
vol. 50, no. 6, pp. 81–85, 2007.

[29]

[30] D. Parnas, “Stop the numbers game,” Commun. ACM, vol. 50, pp.

[31]

19–21, 2007.
“What America’s largest technology ﬁrms are investing in: Their
focus is on the metaverse, cars and health care,” The Economist,
2022.

[32] K. Petersen and N. B. Ali, “An analysis of top author citations
in software engineering and a comparison with other ﬁelds,”
Scientometrics, vol. 126, no. 11, pp. 9147–9183, 2021.
J. P. A. Ioannidis, K. W. Boyack, and J. Baas, “Updated science-
wide author databases of standardized citation indicators,” PLOS
Biol., vol. 18, no. 10, pp. 1–3, 2020.

[33]

[34] A. Agrawal and R. Henderson, “Putting patents in context:
Exploring knowledge transfer from MIT,” Manag. Sci., vol. 48,
no. 1, pp. 44–60, 2002.
J. Hamilton, “The engineering profession,” UK Engineering Coun-
cil, 2000.

[35]

[36] B. Rous, “Major update to ACM’s Computing Classiﬁcation
System,” Commun. ACM, vol. 55, no. 11, pp. 12–12, 2012.
[37] C. Zheng, G. He, and Z. Peng, “A study of web information ex-
traction technology based on Beautiful Soup,” J. Comput., vol. 10,
no. 6, pp. 381–387, 2015.

[38] Z. Griliches, “Patent statistics as economic indicators: A survey,”

J. Econ. Lit., vol. 28, no. 4, pp. 1661–1707, 1990.

[39] M. Blackman, “News from the USPTO,” World Patent Information,

vol. 34, no. 4, pp. 333–335, 2012.

[40] European Patent Ofﬁce, “The Cooperative Patent Classiﬁcation:
Introduction to the CPC,” 2013, accessed June 2022.
[On-
line]. Available: https://e-courses.epo.org/wbts/cpc general/
index.html

[42]

[41] P. Brereton, B. A. Kitchenham, D. Budgen, M. Turner, and
M. Khalil, “Lessons from applying the systematic literature re-
view process within the software engineering domain,” J. Syst.
Softw., vol. 80, no. 4, pp. 571–583, 2007.
IFI CLAIMS Patent Services and Google, “Google Patents
Public Data,” 2017, accessed June 2022.
[Online]. Avail-
able: https://console.cloud.google.com/marketplace/product/
google patents public datasets/google-patents-public-data
[43] Organisation for Economic Co-operation and Development,

[44]

OECD Patent Statistics Manual. OECD, 2009, ch. 6.
S. Nagaoka, “Assessing the R&D management of a ﬁrm in terms
of speed and science linkage: Evidence from the US patents,” J.
Econ. Manag. Strategy, vol. 16, no. 1, pp. 129–156, 2007.

[45] W. G. Cochran, Sampling Techniques, 3rd ed. Wiley, 1977.
[46]

J. Bessen, “The value of U.S. patents by owner and patent
characteristics,” Res. Policy, vol. 37, no. 5, pp. 932–945, 2008.
[47] A. C. Marco, A. Tesfayesus, and A. A. Toole, “Patent litigation
data from US district court electronic records (1963–2015),” SSRN
Electronic Journal, 2017.

[48] D. L. Schwartz, T. M. Sichelman, and R. Miller, “USPTO patent
number and case code ﬁle dataset documentation,” SSRN Elec-
tronic Journal, 2019.

[49] R. D’Agostino and E. S. Pearson, “Tests for departure from
normality. Empirical results for the distributions of b2 and
b1,”
Biometrika, vol. 60, no. 3, pp. 613–622, 1973.

√

[50] C. Spearman, “The proof and measurement of association be-
tween two things,” Am. J. Psychol., vol. 15, no. 1, pp. 72–101,
1904.

S. L. Pﬂeeger and B. A. Kitchenham, “Principles of survey re-
search: Part 1: Turning lemons into lemonade,” SIGSOFT Softw.
Eng. Notes, vol. 26, no. 6, pp. 16–18, 2001.

18

[52] B. Kitchenham and S. L. Pﬂeeger, “Principles of survey research:
Part 6: Data analysis,” SIGSOFT Softw. Eng. Notes, vol. 28, no. 2,
pp. 24–27, 2003.

[53] B. A. Kitchenham and S. L. Pﬂeeger, “Principles of survey re-
search: Part 2: Designing a survey,” SIGSOFT Softw. Eng. Notes,
vol. 27, no. 1, pp. 18–20, 2002.

[55]

[57]

[56]

[54] M. E. Rose and J. R. Kitchin, “pybliometrics: Scriptable biblio-
metrics using a Python interface to Scopus,” SoftwareX, vol. 10, p.
100263, 2019.
S. Easterbrook, J. Singer, M.-A. Storey, and D. Damian, Selecting
Empirical Methods for Software Engineering Research.
Springer,
2008, pp. 285–311.
J. M. Corbin and A. Strauss, “Grounded theory research: Proce-
dures, canons, and evaluative criteria,” Qual. Sociol., vol. 13, no. 1,
pp. 3–21, 1990.
J. Lin˚aker, S. M. Sulaman, R. Maiani de Mello, and M. H ¨ost,
Guidelines for Conducting Surveys in Software Engineering. De-
partment of Computer Science, Lund University, 2015.
citation

reports: Document

[58] Clarivate,

types
calculation,” Sci. Acad.
[Online]. Available: https:

included in the
Res., 2018, accessed June 2022.
//support.clarivate.com/ScientiﬁcandAcademicResearch/s/
article/Journal-Citation-Reports-Document-Types-Included-in-
the-Impact-Factor-Calculation
S. Bains, Electronic Theses and Dissertations (ETDs) Resources. BFC
Publications, 2022, pp. 213–222.

“Journal

impact

factor

[59]

[60] Networked Digital Library of Theses and Dissertations (NDLTD),
“Thesis resources: Find ETDs,” accessed June 2022. [Online].
Available: https://ndltd.org/thesis-resources/ﬁnd-etds/
[61] F. A. Loan, U. Y. Parray, and A. M. Khan, “Performance of search
engines in harvesting grey literature,” Libr. Hi Tech News, vol. 39,
no. 2, pp. 21–23, 2022.
J. A. Wani, “Open access electronic thesis and dissertation repos-
itories: An assessment,” Libr. Philos. Pract. (e-journal), no. 2528,
2019.

[62]

[63] M. Coates, “Electronic theses and dissertations: Differences in
behavior for local and non-local users,” Libr. Hi Tech, vol. 32, no. 2,
pp. 285–299, 2014.

[64] C. J. Greenberg, “Opening cultural heritage in the age of OAI-
PMH: ﬁnding Armenia in the OATD discovery service,” Libr.
Manag., vol. 35, no. 4/5, pp. 320–328, 2014.

[65] A. Ali and S. Jan, “Contribution of UK and USA towards Open
Access Theses and Dissertations (OATD),” in Proceedings of the
International Conference on Knowledge Management in Higher Edu-
cation Institutions. Manipal University Jaipur, 2021, pp. 309–316.
[66] T. Ferreras-Fern´andez, F. Garc´ıa-Pe ˜nalvo, J. A. Merlo-Vega, and
H. Mart´ın-Rodero, “Providing open access to PhD theses: visi-
bility and citation beneﬁts,” Program, vol. 50, no. 4, pp. 399–416,
2016.
S. Hangal and M. S. Lam, “Tracking down software bugs using
automatic anomaly detection,” in Proceedings of the 24th Interna-
tional Conference on Software Engineering. ACM, 2002, pp. 291–
301.

[67]

[68] G. Almes, A. Black, C. Bunje, and D. Wiebe, “EDMAS: A locally
distributed mail system,” in Proceedings of the 7th International
Conference on Software Engineering.
J. Zhang and B. H. C. Cheng, “Model-based development of
dynamically adaptive software,” in Proceedings of the 28th Inter-
national Conference on Software Engineering. ACM, 2006, pp. 371–
380.

IEEE, 1984, pp. 56–66.

[69]

[70] A. Dearle, “Software deployment, past, present and future,” in
Proceedings of the 29th International Conference on Software Engi-
neering.

IEEE, 2007, pp. 269–284.

[71] G. Kiczales, “Aspect-oriented programming,” in Proceedings of the
IEEE, 2005,

27th International Conference on Software Engineering.
pp. 730–730.

[72] M. Weiser, “Program slicing,” IEEE Trans. Softw. Eng., vol. SE-10,

no. 4, pp. 352–357, 1984.

[73] G. T. Almes, A. P. Black, E. D. Lazowska, and J. D. Noe, “The
Eden system: A technical review,” IEEE Trans. Softw. Eng., vol. 11,
no. 1, pp. 43–59, 1985.

[74] B. W. Boehm, “Software engineering economics,” IEEE Trans.

Softw. Eng., vol. 10, no. 1, pp. 4–21, 1984.

[75] A. Black, N. Hutchinson, E. Jul, H. Levy, and L. Carter, “Distribu-
tion and abstract types in Emerald,” IEEE Trans. Softw. Eng., vol.
SE-13, no. 1, pp. 65–76, 1987.

[76] R. S. Hall, D. Heimbigner, and A. L. Wolf, “A cooperative
approach to support software deployment using the software
dock,” in Proceedings of the 21st International Conference on Software
Engineering. ACM, 1999, pp. 174–183.

[77] B. Korel, “Automated software test data generation,” IEEE Trans.

Softw. Eng., vol. 16, no. 8, pp. 870–879, 1990.

[78] P. Hosek and C. Cadar, “Safe software updates via multi-version
execution,” in Proceedings of the 35th International Conference on
Software Engineering.

IEEE, 2013, pp. 612–621.

[79] A. T. T. Ying, G. C. Murphy, R. Ng, and M. C. Chu-Carroll,
“Predicting source code changes by mining change history,” IEEE
Trans. Softw. Eng., vol. 30, no. 9, pp. 574–586, 2004.

[80] R. J. Hall, “Call path proﬁling,” in Proceedings of the 14th Interna-
tional Conference on Software Engineering. ACM, 1992, pp. 296–
306.

[81] D. ˇCubrani´c and G. C. Murphy, “Hipikat: Recommending per-
tinent software development artifacts,” in Proceedings of the 25th
International Conference on Software Engineering.
IEEE, 2003, pp.
408–418.

[82] T. G. Moher, “PROVIDE: A process visualization and debugging
environment,” IEEE Trans. Softw. Eng., vol. 14, no. 6, pp. 849–857,
1988.

[83] M. Fowler, “Refactoring,” in Proceedings of the 24th International

Conference on Software Engineering. ACM, 2002, pp. 701–701.

[84] R. A. Ballance, S. L. Graham, and M. L. Van De Vanter, “The
pan language-based editing system,” ACM Trans. Softw. Eng.
Methodol., vol. 1, no. 1, pp. 95–127, 1992.

[85] A. D. Lucia, F. Fasano, R. Oliveto, and G. Tortora, “Recovering
traceability links in software artifact management systems using
information retrieval methods,” ACM Trans. Softw. Eng. Methodol.,
vol. 16, no. 4, pp. 13–es, 2007.

[86] D. E. Denning, “An intrusion-detection model,” IEEE Trans.

Softw. Eng., vol. 13, no. 2, pp. 222–232, 1987.

[87] K. Krippendorff, Content Analysis: An Introduction to Its Methodol-

[88]

[89]

ogy, 3rd ed. SAGE Publications, 2012.
JetBrains, “The state of developer ecosystem 2021: Team
tools,” accessed June 2022.
[Online]. Available: https://
www.jetbrains.com/lp/devecosystem-2021/team-tools/
Stack Overﬂow,
2021,”
//insights.stackoverﬂow.com/survey/2021/

“Stack Overﬂow developer

[Online]. Available:

accessed June

survey
https:

2022.

[90] M. J. Rochkind, “The source code control system,” IEEE Trans.

Softw. Eng., vol. SE-1, no. 4, pp. 255–265, 1975.

[91] W. F. Tichy, “Design, implementation, and evaluation of a re-
vision control system,” in Proceedings of the 6th International
Conference on Software Engineering.

IEEE, 1982.

[92] D. M. Ritchie, S. C. Johnson, M. E. Lesk, and B. W. Kernighan,
“The C programming language,” Bell Syst. Tech. J., vol. 57, no. 6,
1978.

[93] B. Stroustrup, “Adding classes to the C language: An exercise in
language evolution,” Softw. Pract. Exp., vol. 13, no. 2, pp. 139–161,
1983.

[94] E. Schwitzgebel, “Introspection,” in Stanford Encycl. of Philos.,
E. N. Zalta, Ed. Metaphysics Research Lab, Stanford
University, 2019, accessed June 2022. [Online]. Available: https:
//plato.stanford.edu/archives/win2019/entries/introspection/
[95] Y. Xie, M. E. Sichani, J. E. Padgett, and R. DesRoches, “The
promise of implementing machine learning in earthquake en-
gineering: A state-of-the-art review,” Earthquake Spectra, vol. 36,
no. 4, pp. 1769–1801, 2020.

[96] H. Rahmanifard and T. Plaksina, “Application of artiﬁcial intel-
ligence techniques in the petroleum industry: A review,” Artif.
Intell. Rev., vol. 52, no. 4, pp. 2295–2318, 2018.

[97] A. L. Oliveira, “Biotechnology, big data and artiﬁcial intelli-

gence,” Biotechnol. J., vol. 14, no. 8, p. 1800613, 2019.

[98] F. Al-Turjman, Ed., Artiﬁcial Intelligence in IoT, ser. Trans. Comput.

Sc. Comput. Intell. Springer, 2019.

[99] G. N. Kouziokas, “The application of artiﬁcial intelligence in pub-
lic administration for forecasting high crime risk transportation
areas in urban environment,” Transp. Res. Procedia, vol. 24, pp.
467–473, 2017.

[100] K.-H. Yu, A. L. Beam, and I. S. Kohane, “Artiﬁcial intelligence in

healthcare,” Nat. Biomed. Eng., vol. 2, no. 10, pp. 719–731, 2018.

19

[101] L. Hatton, D. Spinellis, and M. van Genuchten, “The long-term
growth rate of evolving software: Empirical results and implica-
tions,” J. Softw.: Evol. Process, vol. 29, no. 5, 2017.

[102] V. Vyatkin, “Software engineering in industrial automation: State-
of-the-art review,” IEEE Trans. Industr. Inform., vol. 9, no. 3, pp.
1234–1249, 2013.

[103] K.-J. Stol, M. Goedicke, and I. Jacobson, “Introduction to the
special section—General Theories of Software Engineering: New
advances and implications for research,” Inf. Softw. Technol.,
vol. 70, no. C, pp. 176–180, 2016.

[104] R. Wieringa, “Design science as nested problem solving,” in
Proceedings of the 4th International Conference on Design Science
Research in Information Systems and Technology. ACM, 2009.
[105] K.-J. Stol and B. Fitzgerald, “The ABC of software engineering

research,” ACM Trans. Softw. Eng. Methodol., vol. 27, no. 3, 2018.

[106] C. Haupt, T. Schlauch, and M. Meinel, “The software engineering
initiative of DLR: Overcome the obstacles and develop sustain-
able software,” in Proceedings of the 13th International Workshop on
Software Engineering for Science. ACM, 2018, pp. 16–19.

[107] D. I. K. Sjøberg, B. Anda, E. Arisholm, T. Dyba, M. Jorgensen,
A. Karahasanovic, E. Koren, and M. Vokac, “Conducting realistic
experiments in software engineering,” in Proceedings of the 2002
International Symposium on Empirical Software Engineering.
IEEE,
2002, pp. 17–26.

[108] B. Kitchenham, T. Dyba, and M. Jorgensen, “Evidence-based
software engineering,” in Proceedings of the 26th International
Conference on Software Engineering.

IEEE, 2004, pp. 273–281.

[109] D. I. K. Sjøberg, T. Dyba, and M. Jorgensen, “The future of empir-
ical methods in software engineering research,” in Proceedings of
the 2007 Future of Software Engineering.
IEEE, 2007, pp. 358–378.
[110] G. Booch, “Developing the future,” Commun. ACM, vol. 44, no. 3,

pp. 118–121, 2001.

[111] C. Bloch and M. P. Sørensen, “The size of research funding:
Trends and implications,” Sci. Public Policy, vol. 42, no. 1, pp.
30–43, 2014.

[112] H. Koziolek, “Tracing the practical

software
impact of
architecture research,” 2022, accessed June 2022.
[Online].
Available: https://medium.com/@heiko.koziolek/tracing-the-
practical-impact-of-software-architecture-research-a2b91136455
[113] M. H´eder, “From NASA to EU: The evolution of the TRL scale in
Public Sector Innovation,” The Innov. J., vol. 22, no. 2, pp. 1–23,
2017.

[114] A. Singh, R. Bansal, and N. Jha, “Open source software vs
proprietary software,” Int. J. Comput. Appl., vol. 114, no. 18, pp.
26–31, 2015.

[115] S. Dhir and S. Dhir, “Adoption of open-source software versus
proprietary software: An exploratory study,” Strateg. Change,
vol. 26, no. 4, pp. 363–371, 2017.

[116] H. Piezunka and L. Dahlander, “Distant search, narrow attention:
How crowding alters organizations’ ﬁltering of suggestions in
crowdsourcing,” Acad. Manag. J., vol. 58, no. 3, pp. 856–880, 2015.
[117] European Union, “Directive 2009/24/EC of the European Parlia-
ment and of the Council of 23 April 2009 on the legal protection
of computer programs,” OJEU L 111/16.

[118] “Publish or perish,” Nature, vol. 467, no. 7313, pp. 252–252, 2010.
[119] D. Fanelli, “Do pressures to publish increase scientists’ bias? An
empirical support from us states data,” PLOS ONE, vol. 5, no. 4,
pp. 1–7, 2010.

[120] P. van den Besselaar, S. Hemlin, and I. van der Weijden, “Collab-
oration and competition in research,” High. Educ. Policy, vol. 25,
no. 3, pp. 263–266, 2012.

[121] J. Rikkila, P. Abrahamsson, and X. Wang, “The implications of
a complexity perspective for software engineering practice and
research,” J. Comput. Eng. Inf. Technol., vol. 1, no. 1, 2012.
[122] K. Petersen, S. Vakkalanka, and L. Kuzniarz, “Guidelines for
conducting systematic mapping studies in software engineering:
An update,” Inf. Softw. Technol., vol. 64, pp. 1–18, 2015.

[123] Z. Kotti, K. Kravvaritis, K. Dritsa, and D. Spinellis, “Standing on
shoulders or feet? An extended study on the usage of the MSR
data papers,” Empir. Softw. Eng., vol. 25, no. 5, pp. 3288–3322,
2020.

[124] V. Mohan-Ram, “Patent ﬁrst, publish later: How not to ruin your

chances of winning a patent,” Science, 2001.

[125] A. Furnham, “Response bias, social desirability and dissimula-
tion,” Pers. Individ. Differ., vol. 7, no. 3, pp. 385–400, 1986.
[126] L. Sigelaman, “Question-order effects on presidential popular-

ity,” Public Opin. Q., vol. 45, no. 2, pp. 199–207, 1981.

