CLUSTERBUILDER - A DSL TO DEPLOY A PARALLEL
APPLICATION OVER A WORKSTATION CLUSTER

2
2
0
2

n
u
J

9

]

C
D
.
s
c
[

1
v
9
2
4
4
0
.
6
0
2
2
:
v
i
X
r
a

Jon M. Kerridge
School of Computing
Edinburgh Napier University
Merchiston Campus
Edinburgh EH10 5DT
j.kerridge@napier.ac.uk

ABSTRACT

Many organisations have a large network of connected computers, which at times may be idle. These
could be used to run larger data processing problems were it not for the difﬁculty of organising and
managing the deployment of such applications. ClusterBuilder is designed to make this task much
simpler. ClusterBuilder uses its own Domain Speciﬁc Language (DSL) to describe the processing
required that removes the need for a deep understanding of parallel programming techniques. The
application uses extant sequential data objects which are then invoked in a parallel manner. Cluster-
Builder uses robust software components and the created architecture is proved to be correct and free
from deadlock and livelock. The performance of the system is demonstrated using the Mandelbrot set,
which is executed on both a single multi-core processor and a cluster of workstations. It is shown that
the cluster-based system has better performance characteristics than a multi-core processor solution.

Keywords Domain Speciﬁc Language · Parallel Applications · Workstation Cluster Formal Proof

1

Introduction

The goal for ClusterBuilder was to create a system that enables the deployment of a parallel solution to a problem with
few changes to an existing sequential Java code. The solution should exploit parallel techniques and not just run the
same sequential application many times on the available workstations. The aim was to produce a Domain Speciﬁc
Language (DSL)[Wikipedia contributors, 2021] that captured the essence of the required solution and left the detail
to a builder application that created the required parallel coding. The user does not need to be aware of the detailed
architectural, network structure and parallel processing requirements.

1.1 Requirements

The requirements that guided the design of the DSL and the subsequent deployment of the application were:

1. use commodity workstations and operating systems connected using an ethernet
2. use extant techniques to transfer ﬁles around the network and invoke an application
3. minimise the amount of code that needs to be distributed by the user
4. deﬁne and build application network interconnections with no user intervention
5. produce a solution that is provably free from deadlock and livelock
6. require no detailed knowledge of the workstation interconnection network
7. collect detailed timing data of node usage including load and run-time separately

Requirement 1 was predicated by wanting to use an organisation’s existing network of workstations without requiring
the adoption of new software tools. Requirement 2 was simply a reﬂection that Windows provides the Remote

 
 
 
 
 
 
Cluster Builder DSL

JON KERRIDGE

Desktop Connection[Microsoft, 2021] tool and Unix based systems have several readily available tools, for example
PuTTY[Tatham, 2021], that achieve the same effect. Requirement 3 recognises that some code will have to be available
on each node that can load and invoke the application, but this should be as simple and easy as possible, and possibly
independent of the application. Requirement 4 means the application can be built and deployed on the network using
different workstations and is not restricted to using a speciﬁc set of workstations. Requirement 5 ensures the user can
be conﬁdent the solution will work correctly. Requirement 6 allows the user to build and deploy the application on
different occasions using different host workstations and must only know the host’s IP-address. Requirement 7 is not
strictly necessary for correct functioning but does demonstrate the application is expected to terminate in an orderly
manner.

2 Background and Previous Work

ClusterBuilder uses the JCSP [Welch et al., 2020] package, based on the occam [Inmos, 1984] model that implements
Hoare’s Communicating Sequential Processes [Hoare, 1978]. The JCSP library provides the processes and channels
required to build concurrent systems using Java. The JCSP library has a formal proof of correctness for its opera-
tion[Welch and Martin, 2000a].The JCSP library includes a package JCSP.net2 that enables parallel processing over a
TCP/IP network[Chalmers, 2015]. The great advantage of this approach is the process deﬁnitions are transparent as to
whether communication is over internal or net channels. The basic JCSP library has been further enhanced using several
Groovy [Groovy, 2019] [Dierk et al., 2015] classes that make programming systems even simpler, using GroovyJCSP
[Kerridge, 2018]. A previous library, Groovy Parallel Patterns[Kerridge, 2016][Kerridge, 2021], has been created that
uses a similar DSL technology to build parallel applications on a single multi-core processor. ClusterBuilder is an
extension of that system to enable creation of network based parallel applications. The Groovy Parallel Patterns Library
has a formal proof of correctness using CSPm[Scattergood and Armstrong, 2011] and FDR [Gibson-Robinson et al.,
2014] and this approach has also been adopted for ClusterBuilder.

The concept of using workstation clusters is well known[Stone and Ercal, 2001]. The most common form of easily
available cluster computing is known as a Beowulf cluster[Pereira, 2013] that describes the steps needed to create a
simple Beowulf cluster using Ubuntu Linux. This involves setting up speciﬁc ﬁles to hold node information, deﬁning
users, setting up a network ﬁle system to enable software distribution, setting up connections between the nodes using
SSH and a process manager, typically Hydra[Wikipedia contributors, 2020a]. It assumes the user is familiar with
MPI[Blaise Barney, Lawrence, 2020a][Blaise Barney, Lawrence, 2020b] and understands its intricacies. This is a lot of
technology to become familiar with and then conﬁdent in its use. Conversely, workload management systems such
as Condor[Wikipedia contributors, 2020b][eTutorials.org, 2021] provides a means of controlling the resources of a
workstation cluster using a batch control system. The Condor system is able to use currently idle resource, on an as
required basis, to satisfy the needs of jobs in its job queue. Condor provides a management system that requires detailed
knowledge of the requirements of a job, that new, or users with limited knowledge of parallel processing, may not have.

3 The DSL Speciﬁcation

Listing 1 shows the basic structure of an application speciﬁcation with the cluster speciﬁc annotations {1:2,4,6}.
Initially, any constants used in the speciﬁcation are deﬁned.

Listing 1: Outline DSL Speciﬁcation for Cluster Builder

0 1 .
0 2 .
0 3 .
0 4 .
0 5 .
0 6 .
0 7 .

c o n s t a n t s u s e d i n d e f i n i t i o n

e m i t p r o c e s s d e f i n i t i o n

. . .
/ / @emit h o s t − i p
. . .
/ / @ c l u s t e r N c l u s t e r s
. . .
/ / @ c o l l e c t
. . .

c o l l e c t p r o c e s s d e f i n i t i o n

c l u s t e r p r o c e s s d e f i n i t i o n

The //@emit annotation speciﬁes the IP-address of the host machine. The processes that make up the object emit phase
of the application are then deﬁned. The //@cluster annotation speciﬁes the number of workstations in the cluster and is
followed by the process deﬁnitions to be created for each cluster. The IP addresses for the cluster nodes are not required
at this time. Finally, the //@collect annotation introduces the processes that make up the result collection phase of the
application. It is required that the emit and collect processes reside on the same host node. In many applications having
the emit and collect processes on the same node will not cause a reduction in performance as, by deﬁnition, most of the
run-time will be consumed by the nodes undertaking the application processing.

2

Cluster Builder DSL

JON KERRIDGE

4 Network Utilisation During Application Loading

The network is structured and used in two completely different ways. Initially, during application load time, the network
is used in a speciﬁc manner over which the user has no control. Only once the application has been loaded onto the
cluster nodes does the network provide the interconnect required to undertake application processing. The DSL creates
a speciﬁc Host Node Loading application and the same Node Loading application on each node. The DSL also creates a
Host Process, comprising the Emit and Collect processes and a speciﬁc Node Process for each node. The Node Process
is created by the Host Node Loading application.

Figure 1shows the network architecture used during application loading. The Host Node Loading (HNL) application
creates a many-to-one input channel to read messages from the nodes. The user then must run an identical Node
Loading (NL) application on each node, by transferring and running the same executable ﬁle from the host to each of
the nodes, using a remote desktop connection. The Node Loading application must be executed on the same number of
nodes as speciﬁed in Nclusters {1:4}. The NL then determines its IP-address and creates a one-to-one input channel
that the HNL will use to output messages to the node. The NL then sends its IP-address to the HNL. Once the HNL
has received the correct number of node IP-addresses, it creates the output channel to each of the nodes. The HNL
can now create the speciﬁc process required for each of the nodes that is obtained from running the ClusterBuilder
application on the speciﬁcation DSL. The HNL can now send to each node the Node Process (NP) it is to execute,
which contains enough information to be able to create the required application communication network. The HNL
then creates the Host Process (HP) which it loads into the host node. Only when the HP is running can the application
network be instantiated. The JCSP.net2 package requires that an input channel end is created before the corresponding
output end. Thus, the initial part of the NPs and HP ensure this sequence is undertaken by sending synchronisation
messages on the Application Loading Network shown in Figure 1. The HP acts as the co-ordinating process. Once
all the application communication channels have been created the execution of the application can commence. On
termination of the application, the nodes return the timings for both loading and running the application. The HP then
combines these times with its equivalent times and displays the information. Once this has completed all the nodes and
the host processor are idle and all resources will have been reclaimed by the workstations’ operating systems.

4.1 Class Loading

The JCSP.net2 package overcomes the challenge of loading class ﬁles by having the capability of accessing and
dynamically loading them at run time. This is achieved using code-loading channels which transform the class ﬁle into
a serializable object for transmission and subsequently to de-serialize it when it is input into a reading process. Thus,
all the channels from the Host node in Figure 1 to the Nodes are code-loading channels. The code-loading channel
mechanism creates an additional channel from each Node back to the Host so that a Node can make a request for a class
ﬁle if the required one is not available. This means only the Host must have an instance of each class ﬁle, which it can
then send to each Node after a request from the Node. Class ﬁles can be loaded once the application has started running
because the JCSP.net2 package ensures that only classes required by a Node are loaded. The search for class ﬁles is not
a generalised search of the complete network. The search can only retrace through communication channels that have
been declared as code-loading channels. The search can pass through several nodes meaning there does not have to be a
direct connection between a requesting node and the node that holds the class ﬁle. Once the application terminates, the
class ﬁles that have been loaded onto the Nodes are removed automatically, thereby ensuring ﬁle space is not consumed.

5 An Example - The Mandelbrot Set

The Mandelbrot Set[Wikipedia contributors, 2019] is a well-known problem that can be solved using parallel techniques
because it is a so-called embarrassingly parallel problem. Listing 2 shows the DSL speciﬁcation for a cluster-based
solution and will produce the process network shown in Figure 2. It uses a single Host Node comprising the Emit and
Collect processing and two numerical processing Nodes.

Listing 2: Mandelbrot DSL Speciﬁcation for 2 Nodes and a Host Workstation

0 1 .
0 2 .
0 3 .
0 4 .
0 5 .
0 6 .
0 7 .
0 8 .

c o r e s = 4
c l u s t e r s = 2

i n t
i n t
i n t m a x I t e r a t i o n s = 1000
i n t w i d t h

= 5600

/ / number o f w o r k e r s on e a c h node
/ / number o f
c l u s t e r s
/ /
/ / d o u b l e f o r more p o i n t s

e s c a p e v a l u e

/ / @emit 1 9 2 . 1 6 8 . 1 . 1 7 6
d e f

e m i t D e t a i l s = new D a t a D e t a i l s (

dName : Mdata . getName ( ) ,

3

Cluster Builder DSL

JON KERRIDGE

d I n i t M e t h o d : Mdata . i n i t i a l i s e C l a s s ,
d I n i t D a t a :
d C r e a t e M e t h o d : Mdata . c r e a t e I n s t a n c e

[ w i d t h , m a x I t e r a t i o n s ] ,

)

d e f e m i t = new Emit
d e f o n r l = new O n e N o d e R e q u e s t e d L i s t ( )

e D e t a i l s :

(

e m i t D e t a i l s

)

/ / @ c l u s t e r
d e f n r f a = new N od e Re qu es t i n gF a nA n y ( d e s t i n a t i o n s :
d e f g r o u p = new AnyGroupAny (

c l u s t e r s

c o r e s

)

d e f a f o c = new AnyFanOne (

c o r e s ,
w o r k e r s :
f u n c t i o n : Mdata . c a l c u l a t e
c o r e s

s o u r c e s :

)

)

/ / @ c o l l e c t
d e f

r e s u l t D e t a i l s = new R e s u l t D e t a i l s (

d e f a f o = new AnyFanOne (
d e f

c o l l e c t o r = new C o l l e c t (

rName : M c o l l e c t . getName ( ) ,
r I n i t M e t h o d : M c o l l e c t . i n i t ,
r C o l l e c t M e t h o d : M c o l l e c t . c o l l e c t o r ,
r F i n a l i s e M e t h o d : M c o l l e c t . f i n a l i s e
s o u r c e s :

c l u s t e r s

)

)

r D e t a i l s :

r e s u l t D e t a i l s

)

0 9 .
1 0 .
1 1 .
1 2 .
1 3 .
1 4 .
1 5 .
1 6 .
1 7 .
1 8 .
1 9 .
2 0 .
2 1 .
2 2 .
2 3 .
2 4 .
2 5 .
2 6 .
2 7 .
2 8 .
2 9 .

The Mandelbrot space in the range x = -2.5 to 1.0 and y = -1.0 to 1.0 is subdivided into 3200 lines each of 5600
{2:04} points. The emit process {2:12} sends one complete line for processing by one of the Worker processes in
either of the Nodes contained in the group process {2:17-19}. Finally, a processed line of data points is sent to the
collector process {2:29} where the image is reconstructed. The class Mdata used in emitDetails contains {2:7-11}
the deﬁnition of a line for a speciﬁed y-value, together with the methods used to initialise the class (initialiseClass)
and then create object instances one per line (createInstance). Mdata also contains a method, called calculate, that
undertakes the determination of whether the point is in the Mandelbrot set (see Appendix B – Object Deﬁnitions). This
method is invoked from one of the worker processes created by group {2:17-19}. The calculate method uses an extant
algorithm[Wikipedia contributors, 2019], which uses maxIterations {2:03} as the escape value. As the Mdata object is
copied from the Host Node to the other nodes it must implement the Serializable interface and the user must ensure that
the data deﬁnition complies with serialization restrictions (no static or ﬁnal properties can be serialized). The remaining
processes in the speciﬁcation provide the communication between the processes and will be described shortly.

Figure 2shows the communication channels in more detail, with internal channels shown in solid lines. ClusterBuilder
creates all the required channels automatically. It also creates the coding for the required HNL and NL applications and
the HP and NP processes. The emit process writes Mdata objects to the onrl process. The onrl and nrfa processes form
a client-server combination, with onrl acting as the server. The onrl process reads an input from emit. It then waits for a
request signal from any of the nrfa client processes. On reading such a signal, onrl then responds by writing the input
object to the requesting nrfa process. The nrfa process then writes the object to any of the worker processes that is
currently idle. Provided the onrl server process responds to a client request in ﬁnite time and there is no loop in the
client-server network then such a network is guaranteed to be deadlock and livelock free[Welch et al., 1993]. If all the
worker processes at a node are busy, then nrfa acts as a one-place buffer until one of the worker processes writes its
processed object to afoc and thus can read another input object for processing. The nrfa process cannot make a request
for a new input object until it has written an object to a worker process. This ensures the onrl process can never be
blocked from writing an object to a node that has an idle worker process. The afoc process can read an object from any
of the worker processes which it then writes, using a net channel to the afo process that forms part of results processing.
Process afo simply inputs any input object and writes it to the collect process, where the results are collated.

6 ClusterBuilder - Internals

Previously, in Section 3, reference was made to the creation of Host and Node Loading application (HNL, NL) and
Host and Node Processes (HP, NP). The ClusterBuilder contains proforma texts for each of these processes which are
then modiﬁed according to the DSL speciﬁcation. The ClusterBuilder is an extension of the gppBuilder created for the
Groovy Parallel Patterns (GPP) library[Kerridge, 2019]. The internal channels between processes are created by existing
gppBuilder coding. The ClusterBuilder is solely responsible for creating the net channels, together with the loading
applications and node processes. Some of the processes, for example in Figure 2, afoc and afo are specialisations of

4

Cluster Builder DSL

JON KERRIDGE

existing processes from the GPP Library which have a net output and input channel respectively which are created by
ClusterBuilder. ClusterBuilder is also responsible for net channel speciﬁc processes such as onrl and nrfa.

Net Channels have a speciﬁc address structure comprising node IP-address, port and channel number. A computer in
the cluster can support more than one logical node if they have different port numbers. Thus, the load network, shown
in Figure 1 uses port 2000 on all nodes and use channel number 1 for all interactions. A net channel is solely deﬁned
by its net input channel address, thus the address of the input channel to the host is given by 192.168.1.176:2000/1
to which each of the nodes will output. Each of the nodes will have a corresponding net channel input of the form
192.168.1.xxx:2000/1, where xxx is speciﬁc to the node. Each of the nodes can determine their own IP-address and
can send it to the host node because all they need is the host IP-address which is speciﬁed in the DSL speciﬁcation
{2:6}, provided the HNL is started before any of the NL processes. It is a requirement of the JCSP.net2 package that
the input end of a net channel is created before the output end is deﬁned. Such net channels are inherently a many to
one connection and the underlying implementation ensures that inputs are processed in the order they are received.
A communication cannot start until a previous one is fully completed which includes sending an acknowledgement
signal back to the sending process to ensure that all communications, either internal or net, are fully synchronised
between sender and receiver. To ensure that actions are undertaken in the correct order it is sometimes necessary to send
synchronising messages between the nodes and the host to ensure that all nodes have completed a setup task before the
host commences the next.

The DSL speciﬁcation provides sufﬁcient information for the HNL, together with the known structure of the processes
that interact with net channels, to be able to construct node speciﬁc versions of the node process that take account of the
net channel addresses that need to be used by each node. In comparison, the NL process initially executed by each node
is totally application independent such that once a node has an instance of the NL executable it can be used to load and
execute any application created using the DSL. The ﬁrst part of an application node process is in fact concerned with
setting up the application net channels. Only when this is done can the HNL load and invoke the application emit and
collect processes.

6.1

IDE Integration

The DSL speciﬁcation is contained in a ﬁle with a .cgpp sufﬁx. Within the IDE the ﬁle association cgpp is linked to
Groovy so that syntax checking of the speciﬁcation can be undertaken. The ClusterBuilder application is applied to
the ﬁle which creates four output ﬁles with the .groovy sufﬁx. These take the form of the DSL ﬁle name followed by
HostLoader, HostProcess, NodeLoader and NodeProcess. Of these only the app_name_NodeLoader.groovy ﬁle needs
to be made into an executable. In most IDE this can be automated by the production of a jar type artefact that can be
invoked using the java -jar ﬁle command line. This jar-ﬁle needs to be copied to all the other computers in the cluster
upon which the application is to run, most easily by using a remote desktop connection. The HostLoader groovy ﬁle
can be executed directly from the IDE, after which the jar ﬁle at each node can be invoked. The loading of the processes
then follows automatically using the Node and Host Process ﬁles. All class ﬁles are loaded automatically from the
host computer to the nodes without any intervention from the user. On termination the nodes send some timing data to
the host before terminating and recovering all resources. The host prints out any required results and all the timing
data before itself ﬁnishing. In some situations, it may be beneﬁcial to run the host from the command line. This can be
achieved by creating a jar of the HostLoader process. This will contain all the required class ﬁles to run the application.
Care must be taken to ensure the host IP-address is correctly speciﬁed and the number of worker nodes is correct as the
ClusterBuilder encodes these into the Host Loader process. The NodeLoader will also need to match the created host as
it also encodes the host IP-address. The operation and testing of a system can be conducted on a single host node before
using multiple nodes. The number of clusters is set to 1, the host loader process is then run followed by node loader
process directly from the IDE. The application will be loaded and executed directly using the IDE thereby enabling
conﬁdence building. This is possible because the application network uses a different port from the process loading
phase. Obviously, the performance will be not as fast as using multiple nodes, but it is now known the application will
load correctly.

7 Formal Veriﬁcation of the Cluster Architecture

The processes included in the GPP Library have been formally shown to be correct [Kerridge, 2021] using
CSPm[Scattergood and Armstrong, 2011] speciﬁcations and the FDR[Gibson-Robinson et al., 2014] checking tool.
The client-server protocol described earlier, which is based on an original idea of Brinch Hansen[Hansen, 1973], also
has a formal proof[Welch and Martin, 2000b]. The CSPm speciﬁcation shown in Listing 3 contains the deﬁnition of all
the datatypes, objects, channels and processes used in the architecture.

5

Cluster Builder DSL

JON KERRIDGE

Listing 3: CSPm Speciﬁcation of the Cluster Architecture

c l u s t e r n o d e s

c o n n e c t i n g t h e p r o c e s s e s

c h a n n e l s
i n d i c a t i n g t h e o b j e c t
form t h e

e v e n t s on t h e c h a n n e l

t y p e s

t h a t

/ / u s e d d u r i n g s y s t e m r e f i n e m e n t

a s s o c i a t e d w i t h e a c h c h a n n e l

s i g n a l = S

/ / number o f
/ /
/ /
/ /

d a t a t y p e o b j e c t s = A | B | C | D | E | UT
d a t a t y p e
N = 2
c h a n n e l a : o b j e c t s
c h a n n e l b : { 0 . . N− 1 } . s i g n a l
c h a n n e l c : { 0 . . N− 1 } . o b j e c t s
c h a n n e l d : { 0 . . N− 1 } . o b j e c t s
c h a n n e l e : { 0 . . N− 1 } . o b j e c t s
f : o b j e c t s
c h a n n e l
f i n i s h e d : Bool
c h a n n e l
a_A = { | a | }
/ /
a_B = { | b | }
a_C = { | c | }
a_D = { | d | }
a_E = { | e | }
a_F = { | f | }
c r e a t e (A) = B
c r e a t e (B) = C
c r e a t e (C) = D
c r e a t e (D) = E
c r e a t e ( E ) = UT
Emit ( o ) = a ! o −> i f o == UT t h e n SKIP

/ /
/ /
/ / Emit p r o c e s s

( e v e n t s )

a l p h a b e t

t h e
a r e

f i n a l

t h e

t h e

/ /

s p e c i f i c a t i o n o f o b j e c t s
c r e a t e d and e m i t t e d from t h e

a s

t h e y

t e r m i n a t i n g U n i v e r s a l T e r m i n a t o r

S e r v e r ( ) = a ? o −> i f o == UT t h e n S e r v e r _ E n d ( 0 )

e l s e Emit ( c r e a t e ( o ) )

S e r v i c e ( i , o ) = b ? i . S −> c ! i . o −> S e r v e r ( )
S e r v e r _ C h o i c e ( o ) = [ ] x : { 0 . . N−1} @ S e r v i c e ( x , o )
S e r v e r _ E n d ( y ) = b ? y . S −> c ! y . UT −> i f y == N t h e n SKIP

e l s e

S e r v e r _ C h o i c e ( o )

C l i e n t ( i ) = b ! i . S −> c ? i . o −> i f o == UT t h e n ( d ! i . UT −> SKIP )

e l s e

( d ! i . o −> C l i e n t ( i ) )

e l s e S e r v e r _ E n d ( y + 1)

a _ C l i e n t ( x ) = { | b . x , c . x , d . x | }
C l i e n t s ( ) = | | x : { 0 . . N−1} @ [ a _ C l i e n t ( x ) ] C l i e n t ( x )
a_CS = { | b , c | }
Worker ( i ) = d ? i . o −> i f o == UT t h e n ( e ! i . UT −> SKIP )

a l p h a b e t

c l i e n t

/ /

/ /

c l i e n t − s e r v e r combined a l p h a b e t

/ /

p a r a l l e l

a_W ( x ) =
Workers ( ) =
Reduce ( i ) = e ? i . o −> i f o == UT t h e n ( Reduce_End ( i ,

| | x : { 0 . . N−1} @ [ a_W ( x ) ] Worker ( x )

( e ! i . o −> Worker ( i ) )
a l p h a b e t u s e d by e a c h Worker ( x )
p a r a l l e l
)

{ | d . x , e . x | }

( i +1)%N)

e l s e

t h e

/ /

/ /

Reduce_End ( s , n ) = i f

e l s e

(

f
! UT −> SKIP

! o −> Reduce ( i ) )

s ==n t h e n f
e l s e e ? n . o −>

i f o == UT t h e n (

f ! o −> Reduce_End ( s , n )

)

e l s e Reduce_End ( s ,

( n+1)%N )

R e d u c e r ( ) = [ ] x : { 0 . . N−1} @ Reduce ( x )
C o l l e c t ( ) = f ? o −> i f o == UT t h e n C o l l e c t _ E n d ( )

/ /

r e p l i c a t e d c h o i c e

C o l l e c t _ E n d ( ) = f i n i s h e d ! T r u e −> C o l l e c t _ E n d ( )

e l s e C o l l e c t ( )

System = ( ( ( ( Emit (A)

[ | a_A | ] S e r v e r ( ) )

[ | a_CS | ] C l i e n t s ( ) )

[ | a_D | ] Workers ( )

)

[ | a_E | ] R e d u c e r ( )

)

[ |

a_F | ] C o l l e c t ( )

T e s t S y s t e m = f i n i s h e d ! T r u e −> T e s t S y s t e m
( System \ { | a , b , c , d , e ,
a s s e r t
( System \ { | a , b , c , d , e ,
a s s e r t
( System \ { | a , b , c , d , e ,
a s s e r t
a s s e r t System : [ d e a d l o c k f r e e ]
a s s e r t System : [ d i v e r g e n c e
a s s e r t System : [ d e t e r m i n i s t i c ]

f | } )
f | } )
f | } )

f r e e ]

[ T= T e s t S y s t e m
[ F= T e s t S y s t e m
[ FD= T e s t S y s t e m

6

0 1 .
0 2 .
0 3 .
0 4 .
0 5 .
0 6 .
0 7 .
0 8 .
0 9 .
1 0 .
1 1 .
1 2 .
1 3 .
1 4 .
1 5 .
1 6 .
1 7 .
1 8 .
1 9 .
2 0 .
2 1 .
2 2 .
2 3 .
2 4 .
2 5 .
2 6 .
2 7 .
2 8 .
2 9 .
3 0 .
3 1 .
3 2 .
3 3 .
3 4 .
3 5 .
3 6 .
3 7 .
3 8 .
3 9 .
4 0 .
4 1 .
4 2 .
4 3 .
4 4 .
4 5 .
4 6 .
4 7 .
4 8 .
4 9 .
5 0 .
5 1 .
5 2 .
5 3 .
5 4 .
5 5 .
5 6 .
5 7 .
5 8 .

Cluster Builder DSL

JON KERRIDGE

Each of the processes in Listing 2 have their own model in the speciﬁcation. The state space that must be investigated is
reduced by emitting only ﬁve objects {3:1} into the network together with UT, a termination object. The signal object
{3:2} is used only by the Client process to indicate that it needs a new input from the Server. The channel and process
structure used by the speciﬁcation is shown in Figure 3.

In terms of the speciﬁcation, onrl is modelled by Server, nrfa by Client, the group of workers by Worker and Reducer
combines the effect of the afoc and afo processes. Each of the channels, a to f {3:4-10} is deﬁned and the object
types they communicate, and where necessary indexed by the number of clusters N {3:3}. A channel follows the CSP
deﬁnitionHoare [1978] which is unidirectional, synchronised and unbuffered means of writing (!) from one process to a
single reading (?) process. The channel ﬁnished is used in the model (TestSystem {3:52}) against which the System
is compared to check the deadlock and livelock status. The basis of the assertion checking {3:53:58} is that System
{3:50-51} can only behave the same as TestSystem provided the model shown in Figure 3 and Listing 2 works correctly.

The alphabets {3:11-16} specify the set of events that each channel can recognise. The create functions {3:17-21} are
used by the Emit {3:22-23} process to write the sequence of objects A to E into the network. It then writes UT into the
network and then behaves as SKIP. The process SKIP indicates a process has terminated correctly. The object UT will
be passed through all the processes, in turn, causing them to successfully terminate, except Collect {3:46-48}, which
repeatedly outputs True on the ﬁnished channel, the same as the TestSystem. Once FDR detects this situation it can
undertake the required assertion evaluation. The deﬁnition of the Server {3:24-19} is the most complex comprising four
functions. Server() reads an object from channel a and determines if it is UT, in which case Server_End(0) is called.
This will cause UT to be written to each of the Client processes in order. If the object is one of A to E then a replicated
non-deterministic choice is invoked by Server_Choice(o). The Service(i.o) function undertakes a signal input from one
of the Client b channels and then writes a data object on the c channel with the same index value.

The Clients() processes are a parallel replication of Client() processes {3:30-34}. A Client writes a signal to the Server
using its b channel which will only be read once the Server has read an input from Emit. It then reads an input on its c
channel and then writes the value to its d channel taking account of the UT object appropriately.

The Workers process is also a replicated parallel of Worker processes {3:35:38}, which input from their d channel and
output the object on the e channel also taking account of the UT object correctly. The Worker in the model undertakes
no function but this has no effect on the modelling of the communication structures. The Reducer process {3:39-45}
uses a non-deterministic replicated choice to read objects from any of the e channels. It detects the input of a UT object
on one of the e channels and ensures that any non-UT objects are processed before reading UT objects from the other e
channels and then ﬁnally outputs a single UT object to the f channel. The Collect process {3:46-48} inputs objects
from the f channel until a UT is read at which point the process outputs True to the ﬁnished channel.

FDR takes the speciﬁcation and assesses the assertions which determine the correctness of the speciﬁcation. A user can
also use the :probe System at the FDR prompt and can step through the speciﬁcation choosing available events. For
even such a small speciﬁcation it soon becomes obvious the large number of possible event orderings, known as Traces,
that can occur.

8 Performance Evaluation

The performance of the Mandelbrot application is evaluated in two ways. First on a single processor with 16 cores
to evaluate the effect of using a single powerful machine and then on a cluster of much less powerful machines to
evaluate the performance improvement that can be achieved using a workstation cluster. In both evaluations the same
data objects were used, without alteration (see Appendix B). The number of points per line was 5600 on the 3200
lines, giving a total of 17.92 million points of which just over 14 million were white. The total number of iterations
undertaken in doing all the calculations was 3,962 million using an escape value of 1000. The colour of each point was
stored in an integer array and the co-ordinates of each point were stored in a two-dimensional array of doubles. The
results were identical regardless of the processing resource used.

8.1 Single Processor Performance

An X-series Intel i9-7960X overclocked at 4.40Ghz with a 22MB cache and 64GB RAM was used for the evaluation.
The processor also has hyper-threading to a maximum of 32 hyper-threads. The multi-core parallel architecture uses
the same Emit and Collect and Group of worker processes, varying the number of workers between 1 and 32. Table 1
shows the performance achieved with varying number of Worker processes.

The Standard Deviation of the Time was less than 7% of the mean in all cases bar, 1 Worker - 8.6% and 32 Workers -
9.2%. Speedup is a measure of the reduction in time compared to the single Worker version (T1/Tn n = 2 to 32). The

7

Cluster Builder DSL

JON KERRIDGE

Workers Time msecs

Speedup Efﬁciency %

1
2
4
8
12
16
20
28
32

882963
447175
221139
115890
89970
90173
87215
94418
100232

1.97
3.99
7.62
9.81
9.79
10.12
9.35
8.81

98.73
99.82
95.24
81.78
61.20
50.62
33.40
27.53

Table 1: Single Processor Performance

Nodes Cores

0
1
2
3
4
5

4
4
8
12
16
20

Time*
243425
230771
120912
82237
84301
75122

Speedup

Efﬁciency

1.054837
2.013251
2.960049
2.887584
3.240418

105.5%
100.7%
98.7%
72.2%
64.8%

Table 2: Cluster Performance EvaluationTime in milliseconds averaged over 10 runs

Efﬁciency (Speedup / Workers) measures, as a percentage, how effective the use of the multiple Worker processes was.
Ideally, the Speedup should be close to the number of Workers, which for the cases 2, 4 and 8 is excellent with an
Efﬁciency of better than 95%. In the case of 16 Workers, the number of available cores, Speedup is only just better
than with 8 cores and the Efﬁciency is correspondingly worse. The network comprises Emit, Collect and the Worker
processes plus two further processes that distribute the work packets and then combine them together before ﬁnal
collection. This is more than the available processor resource, but the additional processes will be doing very little
work in comparison to the Worker processes. The version with 28 Workers was run to determine the effect of using all
32 hyper-threads. As can be seen it is better than using 32 hyper-threads but still worse than using 20 Workers. Thus,
even though the problem is embarrassingly parallel the processor conﬁguration means that performance reduces as the
number of workers increases. This can possibly be accounted for by the interaction and contention between each of the
cores accessing the single processor cache memory.

8.2 Cluster Performance

This experiment was carried out on a network of Intel i7-8700 computers running at 3.2GHz with a 12MB cache. Each
processor has 6 cores and a further 6 hyper-threads and 16GB RAM. The Nodes were organised as 4 workers plus the
nrfa and afoc processes (see Figure 2), thereby using all the available cores. One workstation acted as the Host Node
with additional nodes being added, thereby enabling comparison with the single multi-core processor. Based on the
previous experiments, it was decided not to make use of hyper-threads. The nodes were connected by a 1GB ethernet
and the results are shown in Table 2.

The zero nodes time refers to the base case where both the Host and the single node processes were running on the
same machine. For the cases using 1 and 2 additional nodes the speedup compared to the base case is super-linear and
thereafter there is a drop off in speedup. However, the comparative efﬁciency for the same number of cores as seen in
Table 1 is better in the Cluster version.

Table 3 shows the mean elapsed time for the multi-core and the cluster versions using the number of worker cores as the
basis for comparison. The cluster machines processor frequency was 37.5% slower than the multi-core machine.

For 4 and 8 worker cores the multi-core machine (16 cores) was faster by a small percentage but once the application
was loaded onto a cluster of smaller (6 cores) and slower machines the cluster gave the better performance in terms
of total run time. In all cases the application load time, once all the nodes had sent their IP-address to the host, was
less than 1% of the total application run time, more signiﬁcantly, the increase in load time was linear in the number of
nodes, 132.5 +/- 2.5 milliseconds, for this application.

8

Cluster Builder DSL

JON KERRIDGE

Worker Cores Multi-core Tm Cluster Tc Difference (Tc-Tm)/Tc

4
8
12
16
20

221139
115890
89970
90173
87215

230771
120912
82237
84301
75122

4.2%
4.2%
-9.4%
-7.0%
-16.1%

Table 3: Performance Comparison Between Multi-core and Cluster Solutions
All Times in milliseconds and averaged over 10 runs

9 Conclusions and Future Work

The experiments have demonstrated that a parallelisable application can be run on a cluster of small multi-core machines
with better scalability than running on a single faster larger multi-core machine. The problem then is one of allocating
and loading code over the cluster. The ClusterBuilder application has satisﬁed the requirements outlined in Section 1.1
and achieves the goal of taking a sequential solution and parallelising it both on a single and a cluster of machines. The
basic sequential solution must be augmented by a small number of relatively simple methods (see Appendix B) to enable
interaction with the supporting parallel library. The solution is fully integrated with existing IDEs, Intellij[JetBrains,
2021], in this case. Furthermore, the generated solution has a formal proof of correctness.

The additional user knowledge required is very limited, Listing 2 and the class deﬁnitions given in Appendix B, present
the total programming required to achieve a cluster based parallel solution. The major step is appreciating the route
to parallelisation of an algorithm is splitting up the data in such a way as to allow secure parallel access. In this
case, splitting the Mandelbrot space into lines and processing each line by itself, rather than in a sequential solution
processing the whole space as a single entity.

Future work will concentrate on two aspects. First, the ability to create a sequence of different algorithms, each on their
own cluster and secondly, ensuring that certain nodes in the network can be ﬁxed so that node speciﬁc access, say to
data storage or by means of a network ﬁle system, is enabled.

References

Wikipedia contributors. Domain-speciﬁc language, 2021. https://en.wikipedia.org/wiki/Domain-specific_

language accessed 6-05-2021.

Microsoft.

How to use remote desktop, 2021.

https://support.microsoft.com/en-us/windows/

how-to-use-remote-desktop-5fe128d5-8fb1-7a23-3b8a-41e636865e8c accessed 6-05-2021.

Simon Tatham. Download putty, 2021. https://www.putty.org/ accessed 6-05-2021.

Peter Welch, Kevin Chalmers, and Jon Kerridge. Csp for java, 2020. URL https://github.com/CSPforJAVA/jcsp.

https://github.com/CSPforJAVA/jcsp accessed 23-Jan-2020.

Inmos. Occam Programming Manual. Prentice Hall, 1984.
C. A. R. Hoare. Communicating sequential processes. Communications of the ACM, 21:666–677, 1978.
Peter H Welch and Jeremy M R Martin. Formal analysis of concurrent java systems. Communicating Process

Architectures 2000, page 275–301, 2000a.

Kevin Chalmers. Communicating Process Architectures in the Light of Parallel Design Patterns and Skeletons. IOS

Press, 2015.

Apache Groovy.

org/operators.html#method-pointer-operator.
method-pointer-operator.

Groovy programming language: Operators, 2019.

URL http://groovy-lang.
http://groovy-lang.org/operators.html#

Konig Dierk, Paul King, Guillaume Laforge, Hamlet D’arcy, Cedric Champeau, Erik Pragt, and Jon Skeet. Groovy in

action. Manning, 2 edition, 2015.

Jon Kerridge. groovyjcsp, 2018. URL https://github.com/JonKerridge/groovyJCSP. https://github.com/

JonKerridge/groovyJCSP accessed 6th May 2021.

Jon Kerridge. Groovy Parallel Patterns: A Library to support parallelization.

2016. URL https://
www.youtube.com/watch?v=YtB8V37IS6k&feature=youtu.be&a=. https://www.youtube.com/watch?
v=YtB8V37IS6k&feature=youtu.be&a=.

9

Cluster Builder DSL

JON KERRIDGE

Jon M. Kerridge. Parallel Programming Made Simple, Using Groovy Parallel Patterns. Bookboon, 2021. https:

//bookboon.com/en/parallel-programming-made-simple-ebookaccessed8/03/2022.

Brian Scattergood and Philip Armstrong. Cspm: A reference manual, 2011. http://www.cs.ox.ac.uk/ucs/cspm.

pdf accessed 7-05-2021.

Thomas Gibson-Robinson, Philip Armstrong, Alexandre Boulgakov, and Andrew W. Roscoe. Fdr3 — a modern
reﬁnement checker for csp. Tools and Algorithms for the Construction and Analysis of Systems Lecture Notes in
Computer Science, page 187–201, 2014. doi:10.1007/978-3-642-54862-8_13.

John Stone and Fikret Ercal. "workstation clusters for parallel computing". IEEE Potentials, pages 31–33, April-May

2001.

Serrano Pereira. Building a simple beowulf cluster with ubuntu, 2013. https://www-users.cs.york.ac.uk/~mjf/

pi_cluster/src/Building_a_simple_Beowulf_cluster.html accessed 6-05-2021.

Wikipedia contributors. Hydra oms, 2020a. https://en.wikipedia.org/wiki/Hydra_OMS accessed 6-05-2021.

Blaise Barney, Lawrence.

"message passing interface(mpi)", 2020a. URL https://computing.llnl.gov/

tutorials/mpi/. https://computing.llnl.gov/tutorials/mpi/ accessed 9-Oct-2020.

Blaise Barney, Lawrence. "openmp", 2020b. URL https://computing.llnl.gov/tutorials/openMP/. https:

//computing.llnl.gov/tutorials/openMP/ accessed 9-Oct-2020.

Wikipedia contributors. Htcondor, 2020b. https://en.wikipedia.org/wiki/HTCondor accessed 6-05-2021.

eTutorials.org. Introduction to condor, 2021. http://etutorials.org/Linux+systems/cluster+computing+
with+linux/Part+III+Managing+Clusters/Chapter+15+Condor+A+Distributed+Job+Scheduler/15.
1+Introduction+to+Condor/ accessed 6-05-2021.

Wikipedia contributors. Mandelbrot, 2019. URL https://en.wikipedia.org/wiki/Mandelbrot_set#Escape_
https://en.wikipedia.org/wiki/Mandelbrot_set#Escape_time_algorithm ac-

time_algorithm.
cessed 20-Jan-2019.

P Welch, G Justo, and C Wilcock. High-level paradigms for deadlock-free high-performance systems. IOS Press, 1993.

Jon Kerridge. Gpplibrary, 2019. URL https://github.com/JonKerridge/gppLibrary. https://github.com/

JonKerridge/gppLibrary.

Brinch Hansen. Operating Systems Principles. Prentice Hall, 1973.

Welch and Martin. A csp model for java multithreading. Proceedings International Symposium on Software Engineering

for Parallel and Distributed Systems PDSE-99, 2000b. doi:10.1109/pdse.2000.847856.

JetBrains. Intellij idea, 2021. https://www.jetbrains.com/idea/ accessed 7-05-2021.

A Software Availability

the demonstration software used in this paper is https://github.com/
The primary download for all
JonKerridge/ClusterDemos.
Its build ﬁle will download all the other libraries used by the library including
the groovy_parallel_patterns library and its associated gppClusterBuilder program. Once a reader has decided they
wish to delve further they may want to look at all the libraries used in the demonstration system. The README
in the repository gives the location of the repositories used and the actual dependency of the library. The library
software also requires the JavaFX capability, used by a visualisation capability, but this is downloaded as part of the
build ﬁle for the Groovy Parallel Patterns library. The version used is version 11 and thus an environment using the
groovy_parallel_patterns library must use Java JDK 11 and Groovy.3.

To download Packages from the Github Package Repository a user must provide a personal Access Token, see https:
//docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token This
token must be made available in a ﬁle called gradle.properties. This ﬁle should not be saved in a repository as the
Personal Access Token acts in the same way as a password. The build.gradle ﬁle contains a mechanism that accesses
the gradle.properties ﬁle from a local folder (C:/Github/gradle.properties) that can be accessed by the Gradle build
mechanism. It should contain the two lines.

gpr.user=userName
gpr.key=userPersonalAccessToken

10

Cluster Builder DSL

JON KERRIDGE

B Object Deﬁnitions

01.
02.
03.
04.
05.
06.
07.
08.
09.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
46.
47.
48.
49.
50.
51.
52.
53.
54.
55.
56.

class Mdata extends gr o ov y Pa r a lle lP at t e rn s . DataClass {

Listing 4: Mdata and Mcollect deﬁnitions

// array of colour values for this line
// array of [x , y ] values for this line
// y value for this line
// Mandelbrot maximum iterations before escape

int [] colour
double [][] line
double ly
int escapeValue
long totalIterations // total iterations per line
int WHITE = 1
int BLACK = 0
double minX = -2.5
double minY = 1.0
double rangeX = 3.5
double rangeY = 2.0
static String initialiseClass = " initClass "
static String createInstance = " createInstance "
static String calculate = " calculateColour "
static int lineY = 0
static int heightPoints , widthPoints , maxIterations
static double delta
int initClass ( List d ){

// values used by createInstance

// range over which calcualtions undertaken

widthPoints = ( int ) d [0]
maxIterations = ( int ) d [1]
delta = rangeX / (( double ) widthPoints )
heightPoints = ( int ) ( rangeY / delta )
return completedOK

}
int createInstance ( List d ) {

if ( lineY == heightPoints ) return normalTermination
colour = new int [ widthPoints ]
line = new double [ widthPoints ][2]
escapeValue = maxIterations
totalIterations = 0
ly = lineY * delta // y value for this line
0. upto ( widthPoints -1){ int w ->

// instance variables

line [ w ][0] = minX + ( w * delta )
line [ w ][1] = minY - ly

}
lineY = lineY + 1
return normalContinuati on

}

// based on algorithm at https :// en . wikipedia . org / wiki / Mandelbrot_set

int calculateColour ( List d ) {
int width = colour . size ()
0. upto ( width -1){ int w - >

double xl = 0.0 , yl = 0.0 , xtemp = 0.0
int iterations = 0
while ((( xl * xl )+( yl * yl ) < 4) && iterations < escapeValue ) {

xtemp = ( xl * xl ) - ( yl * yl ) + line [ w ][0]
yl = (2 * xl * yl ) + line [ w ][1]
xl = xtemp
iterations = iterations + 1

}
totalIterations += iterations
colour [ w ] = ( iterations < escapeValue ) ? WHITE : BLACK

}
return completedOK

}

11

57.
58.
59.
60.
61.
62.
63.
64.
65.
66.
67.
68.
69.
70.
71.
72.
73.
74.
75.
76.
77.
78.
79.
80.
81.
82.
83.
84.

Cluster Builder DSL

JON KERRIDGE

}

class Mcollect extends gr o o vy Pa ral le lPa tt ern s . DataClass {

// counts for points and black and white

int blackCount = 0
int whiteCount = 0
int points = 0
long totalIters = 0
static String init = " initClass "
static String collector = " collector "
static String finalise = " finalise "
int initClass ( List d ){

// sums total iterations over all lines

return completedOK

}
int finalise ( List d ){

println " $points , $whiteCount , $blackCount , $totalIters "
return completedOK

}
int collector ( Mdata ml ){

int width = ml . colour . size ()
0. upto ( width -1){ int w - >
points = points + 1
if ( ml . colour [ w ] == ml . WHITE ) whiteCount = whiteCount + 1
else blackCount = blackCount + 1

}
totalIters += ml . totalIterations
return completedOK

}

}

12

Cluster Builder DSL

JON KERRIDGE

13

Figure 1: Application Loading Network Architecture

 Figure 1 Node A NL Host Node HNL Node B NL Node C NL Cluster Builder DSL

JON KERRIDGE

14

Figure 2: Resultant Mandelbrot Process Network (Net Channels shown Dashed

      Figure 2 emit onrl nrfa      group worker afoc afo collect worker worker worker nrfa      group worker afoc worker worker worker Node-0Node-1 e-0 Host Node Cluster Builder DSL

JON KERRIDGE

15

Figure 3: Channel and Process Structure Used in Formal Proof

      Figure 3 c.1 c.0 b.1 b.0 e.1 e.0 f d.0 a Emit Server Client Worker Reducer Collect d.1 Client Worker 