2
2
0
2

r
p
A
7

]

R
A
.
s
c
[

1
v
0
9
2
3
0
.
4
0
2
2
:
v
i
X
r
a

Memory Performance of AMD EPYC Rome and Intel Cascade
Lake SP Server Processors
Markus Velten
Robert Sch√∂ne
Thomas Ilsche
Daniel Hackenberg
{givenname.lastname}@tu-dresden.de
Technische Universit√§t Dresden, Center for Information Services and High Performance Computing (ZIH)
Dresden, Germany

ABSTRACT
Modern processors, in particular within the server segment, inte-
grate more cores with each generation. This increases their com-
plexity in general, and that of the memory hierarchy in particular.
Software executed on such processors can suffer from performance
degradation when data is distributed disadvantageously over the
available resources. To optimize data placement and access patterns,
an in-depth analysis of the processor design and its implications for
performance is necessary. This paper describes and experimentally
evaluates the memory hierarchy of AMD EPYC Rome and Intel
Xeon Cascade Lake SP server processors in detail. Their distinct
microarchitectures cause different performance patterns for mem-
ory latencies, in particular for remote cache accesses. Our findings
illustrate the complex NUMA properties and how data placement
and cache coherence states impact access latencies to local and
remote locations. This paper also compares theoretical and effec-
tive bandwidths for accessing data at the different memory levels
and main memory bandwidth saturation at reduced core counts.
The presented insight is a foundation for modeling performance of
the given microarchitectures, which enables practical performance
engineering of complex applications. Moreover, security research
on side-channel attacks can also leverage the presented findings.

CCS CONCEPTS
‚Ä¢ Computer systems organization ‚Üí Multicore architectures;
Single instruction, multiple data; Interconnection architectures.

KEYWORDS
AMD Zen 2; AMD EPYC Rome; Intel Xeon Cascade Lake; Intel Xeon
Skylake; cache coherence; memory hierarchy

ACM Reference Format:
Markus Velten, Robert Sch√∂ne, Thomas Ilsche, and Daniel Hackenberg. 2022.
Memory Performance of AMD EPYC Rome and Intel Cascade Lake SP Server
Processors. In Proceedings of the 2022 ACM/SPEC International Conference
on Performance Engineering (ICPE ‚Äô22), April 9‚Äì13, 2022, Bejing, China. ACM,
New York, NY, USA, 11 pages. https://doi.org/10.1145/3489525.3511689

ICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.
¬© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
This is the author‚Äôs version of the work. It is posted here for your personal use. Not
for redistribution. The definitive Version of Record was published in Proceedings of the
2022 ACM/SPEC International Conference on Performance Engineering (ICPE ‚Äô22), April
9‚Äì13, 2022, Bejing, China, https://doi.org/10.1145/3489525.3511689.

Figure 1: Stacked line chart for Intel and AMD microarchi-
tectures found in Top500 systems [30]

1 INTRODUCTION
x86 processors dominate the HPC market, with 483 systems in the
November 2021 Top500 list [30]. While most of these systems (408)
are Intel-based, AMD continuously increases its market share in re-
cent years (Figure 1). Processors of these two vendors mostly share
the same instruction set architecture (ISA), but feature different
internal architectures. The differences have strong implications on
the performance of executed codes. Some criteria such as frequen-
cies, SIMD widths, cache sizes, and number of cores are featured
in high level specification descriptions. Other features are much
less prominently documented. This includes the internal network,
which connects cores, DRAM, and I/O, but also the implementa-
tion of the cache coherence protocol, which directly influences the
memory-bandwidth and latencies under different conditions. This
paper reveals data on these aspects for recent processors: AMD
EPYC Rome (Rome, implementing the Zen 2 microarchitecture) and
Intel Cascade Lake SP (CLX, implementing the Cascade Lake mi-
croarchitecture). This information is crucial for various fields, e.g.,
security [33] and optimization of parallel software [27].

The remainder of this paper is structured as follows: we intro-
duce related research in Section 2 and discuss the Rome and CLX
architectures in Section 3 and Section 4, respectively. Section 5 de-
scribes the measurement setup and Section 6 presents evaluations of
local memory accesses. This is followed by a performance analysis
for accesses to remote cache and memory locations within a socket
in Section 7 and to a remote socket in Section 8. We summarize our
findings and conclude this paper in Section 9.

18-1119-0619-1120-0620-1121-0621-11Top500 List [YY-MM]0250500Number of SystemsAMD Zen 3 (Milan)AMD Zen 2 (Rome)AMD Zen (Naples)Intel Ice LakeIntel Cascade LakeIntel SkylakeIntel (Others) 
 
 
 
 
 
ICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.

Markus Velten, Robert Sch√∂ne, Thomas Ilsche, and Daniel Hackenberg

2 RELATED WORK
Manuals from the processor vendors provide high level overviews,
e.g., AMD‚Äôs Processor Programming Reference [1] and the Intel
Specification Updates [13, 14]. This information is complemented
by the respective software optimization guidelines [2, 15] and addi-
tional articles, e.g., on Intel Skylake SP (SKX) [24]. Further details
are presented by processor designers in peer-reviewed articles,
which also describe physical implementations and control loops.
Suggs et al. present the Zen 2 architecture in [7, 8]. Naffziger et
al. describe how multiple dies form a chiplet in [25, 26]. Tam et
al. [31] and Arafa et al. [4] detail the SKX and CLX architecture,
respectively.

Researchers investigate finer details and independently validate
specific details of x86 processors. The memory subsystem is the
main focus of several publications. Molka et al. cover cache coher-
ence and memory performances of older architectures in [21‚Äì23].
We continue and extend their work in this paper and compare the
more recent architectures Rome and CLX. Alappat et al. investi-
gated the CLX architecture in [6]. We extend the research with a
more in-depth analysis of the memory latencies, validate the band-
width results with different benchmarks and compare latency and
bandwidth results with Rome. While cache and memory perfor-
mance is influenced by power saving mechanisms [10, 28, 29], the
focus of this paper is on the performance at constant processor
frequencies.

3 THE AMD EPYC ROME ARCHITECTURE
3.1 General Concept
Rome processors use two different types of dies that are combined
on one package [8, 26]. Up to eight Core Complex Dies (CCD) are
connected to the I/O-die via AMD‚Äôs Infinity Fabric (IF). The I/O-die
(Figure 2) and its Infinity Fabric (IF) grid connect the CCDs among
each other and to external components, including the 128 PCIe
Gen 4 lanes [1, Figure 21], [25] and main memory. IF-switches are
used to route data through the I/O-die, which induce a latency of
at least 2 Fabric Clock (FCLK) cycles at the nominal IF frequency
of 1467 MHz [25]. Additionally, IF-repeaters are used, which cause
a 1 FCLK cycle latency [25].

Each CCD includes two Core Complexes (CCX), which consist of
up to four Zen 2 cores each (see Figure 3), resulting in up to 64 cores
per processor. While L1 and L2 caches are per core (see Figure 4), all
cores within a CCX share a common 16 MiB L3 cache. The Infinity
Fabric on Package (IFOP) interface on each CCD connects the two
CCX to the I/O-die, but not to each other [1, 26].

3.2 Memory Architecture Details
Burd et al. published in [5] that Zen uses a MDOEFSI (Modified,
Dirty, Owned, Exclusive, Forward, Shared, Invalid) protocol, without
naming details. They also state that Zen‚Äôs Infinity Fabric is based
on an enhanced coherent HyperTransport protocol which has been
used by AMD before (compare [20]). It can be assumed that these
protocols are used in Zen 2 as well.

The layout of a Rome processors indicates the presence of four
non-uniform memory access (NUMA) nodes. BIOS settings can be
used to expose NUMA nodes to the operating system (OS), one, two

Figure 2: Layout of an AMD Rome processor with its cen-
tral I/O-die, which connects CCDs, DRAM, and I/O via an In-
finity Fabric (IF) network, Global Memory Interfaces (GMI),
and Unified Memory Controllers (UMCs). [1, 25]

Figure 3: Layout of an AMD Rome Core Complex Die (CCD)
hosting two Core Complexes (CCX) and the Infinity Fabric
On-Package (IFOP). [8, 26]

Figure 4: Layout of an AMD Zen 2 core (see [2, 8]).

Memory Performance of AMD EPYC Rome and Intel Cascade Lake SP Server Processors

ICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.

and four nodes can be configured [19, Section 2.5] where ‚Äú[m]emory
is interleaved across the [...] memory channels in each NUMA domain‚Äù.
Also, ‚Äúeach server can be configured [...] with an additional option to
configure L3 cache as NUMA nodes‚Äù, enabling up to 16 NUMA nodes
per processor. Due to the positions of the socket-to-socket Global
Memory Interconnect (xGMI) interfaces, remote socket access la-
tencies will depend on the relative position of the communication
partners. The architecture supports up to two sockets.

Each of the cores of a CCX hold one slice of the victim L3
cache [2]. The L3 contains evicted cache lines from the L2 caches
and valid copies of cache lines shared by multiple cores. Also, data
transfers and cache coherency between L2 caches on the CCX are
managed by the shadow tags in the L3. The L3 cache slices use a
common L3 frequency, which is generally defined by the highest
core frequency of all cores within the CCX [29].

AMD Zen 2 cores use three Address Generation Units (AGU)
in the integer execution unit to perform up to two 256-bit loads
and one 256-bit store per cycle [2, 8]. Compared to first generation
Zen processors, these have been widened to match the increased
width of SIMD execution. Zen 2 floating point units are able to
process 256-bit Advanced Vector Extensions (AVX) instructions in
one cycle [2, 8], even though this could limit core frequencies in
some cases [29]. Load and store queues attached to the AGUs have
44 and 48 entries, respectively, and load data from a 32 KiB L1d
cache [2, 8]. Instructions are loaded at 32 B/cycle from a 32 KiB L1i
cache and buffered in a 4096 ùëíùëõùë°ùëüùë¶ op cache [8]. The 512 KiB L2
cache is inclusive of both L1 caches [2, 8]. AMD uses hardware
prefetchers for the L1d, L1i, and the L2 cache to avoid stalls due to
cache misses [2, Section 2.1]. L1 and L2 prefetchers can be deacti-
vated via the BIOS, or, according to our findings, by disabling bit
0 of MSR 0xc001102b and enabling bit 16 of MSR 0xc0011022 for
each core.

4 THE INTEL CASCADE LAKE SP

ARCHITECTURE
4.1 General Concept
The CLX processor architecture succeeds the SKX architecture.
Both are using a 14 nm process and a monolithic design [4] and
are available with up to 28 cores per processor. As described in [4]
and [6], changes between these two generations are minimal, in-
cluding higher core frequencies, support for Optane DC, faster
DRAM, and additional instructions. This is confirmed by Alappat et
al., who note in [6, Section 2] that SKX and CLX behave identically
in memory and floating point benchmarks.

A grid of horizontal and vertical fabric lanes connects all parts
of the processor [4, 28, 31] (see Figure 5). Transfers within the
2D-mesh are always routed along the vertical axis, followed by
the horizontal direction [31]. All uncore components operate on a
common frequency which is managed by a hardware control loop
from a range of frequencies [28, 31]. The number of PCIe ports, and
thus lanes, depends on the number of cores, since entire columns
or single cores of the grid may be deactivated/removed [11, 17].
This is a significant difference to AMD EPYC processors, where I/O
interfaces are identical for all core counts.

4.2 Memory Architecture Details
Intel uses a unified execution unit design for CLX, unlike AMD
(Section 3.2) [9, 15]. Four AGUs are available: two for 512-bit loads,
one for 512-bit stores and one for address generation only [9, Table
11.1]. Port 6 is reserved for integer/logic-only instructions. The
read buffers have 72 entries, 56 entries are available in the write
buffers [9, Section 11.9]. Both, integer and SIMD floating point
instructions, can be executed by ports 0, 1 and 5. Ports 0 and 1
can be fused for the execution of AVX-512 vector instructions. On
some processors, port 5 has as a second AVX-512 unit, Intel lists the
number of available AVX-512 units in [15, Page 10]. When executing
AVX(-512) instructions, cores use dedicated frequencies in order to
prevent thermal damages to the processor. These depend on the
type of instruction and the number of cores that are running that
instruction [14]. Sch√∂ne et al. describe side effects for transitions
between normal and AVX-512 frequencies for SKX in [28].

CLX processors have a L1i and L1d cache of 32 KiB (Figure 6),
equal to Zen 2. Both caches load data at 32 B/cycle. A 1536 entry op
cache buffers instructions [9, Table 11.12]. The L2 cache is inclusive
of the L1 caches and at 1 MiB twice as large as the Zen 2 L2 [31].
The L1 and L2 caches utilize hardware prefetchers to reduce the
impact of cache misses [15, Section E.3].

Figure 5: Layout of an Intel Cascade Lake SP processor,
where a 2D-mesh connects core tiles, memory controllers
and I/O [17, 28].

Figure 6: Layout of an Intel Cascade Lake SP core

2 x UPIPCIe x16PCIe x16PCIe x16CoreL3/CHA Memory ControllerMemory Controller DDR4DDR4CoreL3/CHACoreL3/CHACoreL3/CHACoreL3/CHACoreL3/CHACoreL3/CHACoreL3/CHACoreL3/CHACoreL3/CHACoreL3/CHACoreL3/CHACoreL3/CHACoreL3/CHA..............................ICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.

Markus Velten, Robert Sch√∂ne, Thomas Ilsche, and Daniel Hackenberg

As indicated in Figure 5, a core-tile holds not only a core, but
also a slice of 1.375 MiB L3 cache and the Caching and Home Agent
(CHA) [31]. The CHA maps accessed addresses outside its own
caches and provides the necessary routing information through the
mesh [24]. CLX has a non-inclusive L3 cache, which ‚Äúmay appear as
a victim cache‚Äù, ‚Äúdepending on the access pattern, size of the code and
[accessed] data, and sharing behavior between cores‚Äù [15, Section
2.2.1.2] [3]. Generations prior to SKX used smaller L2 and larger per-
core L3 caches, the latter being inclusive. According to [24, Section
‚ÄúCache Hierarchy Changes‚Äù], the new cache layout was chosen
to reduce overall latencies, as more requests may be served from
the larger L2. The non-inclusive L3 leads to a higher effective total
cache size, since it does not necessarily hold data that is present in
other cache levels. Although the per-core L3 cache size is higher
for Zen 2 (4 MiB over 1.375 MiB), Intel‚Äôs 2D mesh interconnect
allows all cores to access all L3 slices with a worst-case overhead
of 18 uncore cycles1 [24]. However, Intel implements a suboptimal
slice hash mechanism, which can lead to an imbalance in using the
available L3-slices [16].

SKX and CLX processors have two memory controllers with
three memory channels each [28]. They can be split into two NUMA
nodes, referred to as Sub-NUMA Clusters (SNC), by assigning each
half of the cores to one of the two memory controllers as well
keeping the L3 cache slices local to the core‚Äôs respective SNC [24].
SKX processors implement the MESIF coherency protocol with
the states Modified, Exclusive, Shared, Invalid, Forwarded [12, Section
2.2.4]. The directory protocol names two additional states: Any and
L2 [12, Table 2-179], which are not documented and for this reason
not investigated in this paper. We assume that CLX uses the same
protocol.

5 TEST SYSTEM & BENCHMARKS
We performed our measurements on two dual socket servers with
AMD EPYC 7702 and Intel Xeon Gold 6248 processors, respectively.
Table 1 lists relevant details and configurations. Unless stated oth-
erwise, we use nominal core frequencies. The uncore frequency of
the Intel system was set to its nominal frequency of 2400 MHz with
the likwid-setFrequencies3 tool [32]. While the uncore clock
can be lowered if the Thermal Design Power (TDP) is reached [28],
we designed our experiments in a way that such a behavior is not
triggered.

We use BenchIT 4 [18], with the x86-membench extension [20] for
latency and bandwidth analyses. These benchmarks can be config-
ured for a wide range of objectives by changing the configuration of
the respective PARAMETERS file. To analyze the impact of the cache
coherence protocol, we use the benchmark memory_latency.

As described in [20, Section 3.5.1], the memory_latency bench-
mark uses pointer-chasing to determine latencies. The size of the
allocated buffer determines the memory level(s) to be analyzed.
The accessed addresses are created by an additional thread using a
random number generator to minimize prefetcher-effects. A coher-
ence state control routine ensures that accessed cache lines have
the requested coherence state on the requested core by running

19-hop distance for a 28 core configuration
2as defined in description, or P as defined in unit mask name
3https://github.com/RRZE-HPC/likwid/wiki/likwid-setFrequencies
4https://tu-dresden.de/zih/forschung/projekte/benchit/

Table 1: Overview on hardware and software of used test sys-
tems

Processor
Cores

2 √ó AMD EPYC 7702 2 √ó Intel Xeon Gold 6248
2 √ó 20
2 √ó 64
Avail. freq.s 1.2, 1.5, 2.0 GHz, Turbo
1.2‚Äì2.5 GHz, Turbo
40 √ó (32 KiB + 32 KiB)
L1 cache 128 √ó (32 KiB + 32 KiB)
L2 cache 128 √ó 512 KiB = 64 MiB
40 √ó 1024 KiB = 40 MiB
L3 cache 32 √ó 16 MiB = 512 MiB 2 √ó 24.75 MiB = 49.5 MiB
HPE
ProLiant DL360 Gen10
HPE P03052-091

Server Model
/ Mainboard
Memory

GIGABYTE
MZ62-HD0-00
Micron
18ASF4G72PDZ-3G2B2
16 √ó 32 GiB = 512 GiB
1600 MHz
OS CentOS 7.7.1908 (Core)
3.10.0 x86_64
2 √ó 4 = 8 (NPS4)

‚Äî BenchIT ‚Äî
GCC 10.2.0

‚Äî STREAM ‚Äî
Intel 19.0.1

12 √ó 32 GiB = 384 GiB
1467 MHz
Ubuntu 18.04
4.15 x86_64
2 √ó 2 = 4 (SNC)

GCC 7.5

Intel 19.0.5

-DSTATIC -DSTREAM_ARRAY_SIZE=800000000
-qopenmp -Ofast

-mcmodel=large
-shared-intel

-qopt-streaming-
stores always
-march=cascadelake
-xCORE-AVX512
-qopt-zmm-usage=high

Kernel
NUMA-Setup

Compiler

Compiler

Comp.
flags

an additional thread on this core before the actual measurement.
In [20, Section 3.3], Molka describes how the different coherence
states are created. After measuring the start time (rdtsc serialized
with mfence and lfence calls), the measurement thread accesses
the memory addresses (loop with unrolled mov (%rbx), %rbx),
and measuring the end time (serialized rdtsc). The duration is
then adjusted by the measurement overhead, which is determined
with the same routine without the memory accesses. Algorithm 1
provides a high-level overview of the steps to measure the memory
accesses.

Listing 1: memory_latency overview. Thread M is only
required for
coherence-states Shared, Forward, and
Owned. [20, Section 3]
p i n t h r e a d s 0 ,N ( ,M) v i a
t h r e a d 0 ,N ( ,M ) :
t h r e a d 0 : warm‚àíup o f TLB by t o u c h i n g memory
t h r e a d 0 :
t h r e a d N ( ,M ) :
t h r e a d 0 : w a i t
t h r e a d 0 :

t h r e a d 1 t o p r e p a r e d a t a
t o u c h d a t a c o r r e c t l y [ 2 0 , S e c .

a l l o c a t e memory v i a numa_set_membind ( )

t h r e a d N , measure l a t e n c y

s c h e d _ s e t a f f i n i t y ( )

a c c e s s memory o f

t h r e a d N ( ,M)

s i g n a l

3 . 3 ]

f o r

1
2
3
4
5
6
7

In order to achieve reproducible results, we flushed all cache lev-
els before each run with the BENCHIT_KERNEL_FLUSH_{L1|L2|L3}=1
flag in the PARAMETERS file of the benchmark. We use the de-
fault 512 B alignment to avoid the re-use of cache lines, set with
BENCHIT_KERNEL_ALIGNMENT=512, unless otherwise noted.

Memory Performance of AMD EPYC Rome and Intel Cascade Lake SP Server Processors

ICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.

Cache and main memory bandwidth values are determined with
the throughput benchmark where we use different Streaming SIMD
Extensions (SSE) and AVX instructions. To ensure that our band-
width results are truly limited by the achievable bandwidth, we con-
figure the benchmark with the BENCHIT_KERNEL_BURST_LENGTH
parameter to use eight 64 bit xmm, 16 256 bit ymm or 32 512 bit
zmm vector registers for SSE, AVX, and AVX-512 kernels, respec-
tively. In order to use the 32 zmm vector registers, additional com-
piler flags (-mavx512f in our case) have to be used. We extended
the benchmark with a range of AVX-512 kernels. Transparent huge
pages are used for both benchmarks to clearly distinguish memory
levels, as advised in [20]. Hardware prefetchers are enabled for
all measurements on both systems, except for the latency bench-
mark on CLX. We flush all cache levels before each run in order to
achieve reproducible results and to avoid the impact of prefetch-
ers. We set /sys/kernel/mm/ transparent_hugepage/enabled
to always to clearly distinguish memory levels.

We configure BenchIT to use four dataset sizes per memory level.
For each of these values, BenchIT reports the minimum for latencies
or maximum for bandwidths of three internal measurements to filter
out outliers from external influences. We repeat each experiment
ten times resulting in a total of 120 values (10√ó4√ó3) per reported
data point, combined using minimum and maximum. As latencies
for remote L1 accesses were noisy, we report the medians for these
measurements, which coincide with the modes of the samples.

Additional measurements are performed with STREAM, as it pro-
vides more realistic memory access patterns with loads and stores.
We use non-temporal stores in order to achieve a higher bandwidth
closer to theoretical limits. STREAM repeats measurements ten
times internally and reports the highest bandwidth. In addition, we
run it ten times and use the maximum of the reported values.

6 LOCAL MEMORY ACCESS
6.1 Latencies
In the simplest case, processor cores access data in their own mem-
ory hierarchy without influences from additional cores and NUMA
domains. We compare the local latencies of the two architectures,
using the BenchIT memory_latency benchmark, in Figure 7. To
focus on the efficiency of the architecture rather than the applied
core frequency, we mainly present results in cycles and not in ns.
The L1d caches can be accessed with 4 cycles on both architectures
(2 and 1.6 ns for the Rome and CLX architecture, respectively) .
L2 cache read latencies for Rome and CLX are 12 cycles (6 ns) and
14 cycles (5.6 ns), respectively. However, the Intel CLX platform
provides twice as much L2 cache per core. This reduces the proba-
bility for L2 cache misses and even less efficient L3 cache accesses.
For the locally accessible L3 cache5, Intel provides more capacity
available for a single core, but at higher access latencies (54 cycles
(21.6 ns) vs 39 cycles (19.5 ns)). While single-threaded applications
will benefit from larger local L2 and L3 caches on Intel platforms,
the total size of L2 and L3 caches is higher for AMD processors,
which is beneficial for parallel applications. Moreover, for equally
clocked processors, the AMD system provides lower latencies to
caches, which further reduces memory stall cycles.

5We set BENCHIT_KERNEL_ALIGNMENT=64 for L3 latency tests on the Rome system to
use all L3 slices equally.

Figure 7: Local memory access latencies for AMD EPYC 7702
(Rome) and Intel Xeon Gold 6248 (CLX), error bars indicate
maxima.

Main memory latencies cannot be easily compared between pro-
cessors, since the specifications of the installed memory DIMMs also
contribute to their access time. Moreover, adapting I/O p-states [29]
and uncore frequencies [28] lead to more uncertainty. We measure
a latency of about 220 cycles (110 ns) for the Rome system, and
200 cycles (80 ns) for CLX. The higher RAM latencies for Rome
may be attributed to the data path routing via the I/O-die, whereas
the memory controllers of the CLX processor are integrated into
the fabric mesh.

6.2 Bandwidth
HPC applications often access main memory with a predictable
pattern, which allows the compiler (via instruction re-ordering), the
out-of-order engine, and hardware prefetchers to hide occurring
latencies and thereby improve the effective memory bandwidth.
However, bandwidth is also limited by the width of datapaths and
concurrent access to shared resources, i.e., L3 cache and DRAM.

We use the BenchIT throughput kernel for our analysis, which
only performs loads. This kernel relies on a streaming access to
memory using SIMD extensions to benefit from wider load instruc-
tions. We extended the kernel so that 512-bit wide accesses can be
measured. Intel SKX and CLX processors have dedicated AVX(-512)
frequency ranges with dedicated nominal frequencies, which di-
rectly influence these measurements. To avoid an unwanted and
unpredictable change of frequencies, we pinned core frequencies
to 1.6 GHz ‚Äì the nominal core frequency for AVX-512 for the Intel
Xeon Gold 6248 processor [14, Figure 3]. Since AMD did not openly
publish dedicated AVX frequencies, we use the nominal frequency
(2.0 GHz) for the Rome system, even though highly demanding
workloads could lead to throttling [29, Section V.E].

Figure 8 shows bandwidth results for memory reads on the AMD
Rome system. The AMD cores reach the theoretical L1 bandwidth
of 128 GB/s (64 B/cycle) if two 256-bit floating point pipes are uti-
lized. The bandwidth for the L2 cache falls slightly short of their
theoretical maximum, with measured 63.7 GB/s (31.4 B/cycle) in-
stead of 64 GB/s. A single core can read from the L3 with up to
46 GB/s (23 B/cycle) when using AVX. According to our measure-
ments, bandwidth for the L3 does not scale linearly for an entire
CCX. With four cores the achieved bandwidth is only 151 GB/s
(18.9 B/cycle/core). Also, RAM bandwidth for one CCD is saturated
by using 3 cores of a single CCX in these measurements. While

L1L2L3RAMMemory Level050100150200Latency [cycles]4 (2.0 ns)4 (1.6 ns)12 (6.0 ns)14 (5.6 ns)39 (19.5 ns)54 (21.6 ns)221 (110.5 ns)200 (80.0 ns)RomeCLXICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.

Markus Velten, Robert Sch√∂ne, Thomas Ilsche, and Daniel Hackenberg

adding accesses from the second CCX on one CCD (cores 0-7) does
not help in gaining more performance, using the second CCD of a
NUMA node (cores 0-15) increases the measured RAM bandwidth
slightly. The results for SSE (add_pd instruction) and AVX do not
differ significantly, except for L1.

Since an increase of used cores beyond three per CCX does not in-
crease bandwidth, we evaluate further bandwidth saturation points:
Figure 9 shows bandwidth measurements on a single NUMA node
with two CCDs hosting two CCX each. We use STREAM for this
analysis to achieve higher bandwidths for low core counts (due to
non-temporal stores). We scaled the number of cores per CCX and
thus per CCD of a single NUMA node. The highest bandwidth of
a single NUMA node (42.9 GB/s) was measured in configurations
with two participating CCDs and two cores per CCD. Since each
NUMA node has its own set of memory channels, this bandwidth
scales to ~171 GB/s for the entire socket, as dedicated measurements
show (not depicted). Even with only one core per CCD, the mea-
sured bandwidth is only slightly lower at 42.3 GB/s. These numbers
indicate that a small number of cores per CCX should be used for
bandwidth-limited workloads. Our measurements imply that users
with a demand for a high bandwidth may benefit from processors
with two CCDs per NUMA node, even if the higher core counts
might not be necessary with respect to compute performance.

Figure 10 shows bandwidth results for reading memory accesses
on the Intel Cascade Lake system. The results do not reach the theo-
retical bandwidth of the L1d caches of 128 B/cycle when two 512-bit
floating point units are used. Instead, we measure 116.25 B/cycle
for AVX-512 instructions. This is lower than the sustained band-
widths listed in [15, Table 2-6], but in line with measurements in [6,
Figure 3]. Notably, the L2 cache bandwidth varies for different
instructions despite being consistently lower than the respective
L1 bandwidth. The L3 bandwidth results are similar for all SIMD
widths, achieving 11.3 B/cycle. Again, this value is lower than the
sustained bandwidth of 15 B/cycle described in [15, Table 2-6]. The
RAM bandwidth is barely affected by SIMD width. On a SNC, the
RAM bandwidth can be saturated with eight cores.

The prerequisites for our measurements on the Rome and CLX
system are significantly different. This needs to be taken into ac-
count when comparing the observed cache bandwidth of the two
architectures. Running the AMD processor at its base clock speed
may be a best case scenario, whereas the AVX-512 frequency we
chose for CLX is more of a worst case, in particular for SSE and AVX.

Figure 8: AMD EPYC 7702 ‚Äì bandwidth at reference fre-
quency (2 GHz) for one NUMA node.

Figure 9: AMD EPYC 7702 ‚Äì STREAM Triad: bandwidth mea-
sured on a single NUMA node with two CCDs and two CCX
per CCD, error bars indicate minima.

Figure 10: Intel Xeon Gold 6248 ‚Äì bandwidth at nominal
AVX-512 frequency (1.6 GHz) for one SNC

In real world workloads, both processors might be able to use turbo
frequencies. As demonstrated in [29], the Rome processor might
throttle to a lower, not publicly disclosed frequency. Ultimately,
the used frequencies, and thus achieved bandwidths, depend on
the workload and its distribution across cores and the thermal con-
ditions the systems operate in. Nevertheless, a few conclusions
may be drawn. Notably, the measured values for the CLX proces-
sor are lower than the maximum bandwidth and in many cases
even the sustained bandwidth that are defined in [15, Table 2-6,
Section 2.2.1.3]. Moreover, the L2 bandwidths depend on the used
instruction. In an ideal situation for this processor, the SSE and
AVX bandwidths would be close to the maximum throughput of
the execution units for both the L1 and L2 caches, as the bandwidth
requirement for these instructions is well within the maximum
cache bandwidths.

This ideal case can be observed for Rome processors. Here, the L2
bandwidth for SSE instructions is very close to the L1 bandwidth as
well as to the L2 bandwidth for AVX instructions and the theoretical
maximum.

The main memory bandwidth of a single NUMA node of a CLX
processor is higher than on Rome, as CLX has three memory chan-
nels per NUMA node, Rome two. However, the bandwidth of a
NUMA node can be saturated with fewer cores on Rome than on
CLX. Additionally, the overall bandwidth for an entire socket for
Rome is higher than for CLX, which is caused by Rome‚Äôs eight
memory channels compared to CLX‚Äô six.

7 INTRA-SOCKET MEMORY LATENCIES
While local memory accesses as described in the previous section
are most common, thread migration, wrong data placements, and
communication via shared memory lead to situations where a core

add_pdavx_add_pdInstruction00-10-20-30-70-1500-10-20-30-70-15CoresL1L2L3RAMMemory Level63.862.544.817.712812589.232.419118813236.825525015236.651050130435.7102099860442.712863.746.021.025512791.434.938319113736.851025415136.4102050930435.82039101460242.80203951010201530Bandwidth [GB/s]1 CCD,1CCX1 CCD,2 CCX2 CCD,1CCX/CCD2 CCD,2 CCX/CCD123424682468481216Number of Cores used02040Bandwidth [GB/s]31.437.636.735.938.336.335.434.942.342.942.141.242.941.440.139.0add_pdavx_add_pdavx512_add_pdInstruction00-10-30-70-900-10-30-70-900-10-30-70-9CoresL1L2L3RAMMemory Level46.535.317.510.793.168.534.821.918614172.043.837228218560.246535223759.593.050.618.211.418698.136.122.937220077.844.474640021860.193250029360.018687.318.211.537215236.122.874532876.643.3149262922760.4186484729560.7018644669321398Bandwidth [GB/s]Memory Performance of AMD EPYC Rome and Intel Cascade Lake SP Server Processors

ICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.

~3‚Äì4 FCLK cycles per switch and repeater combination. A ~28 cycle
(14 ns) penalty is incurred for accesses to the second NUMA node,
and 35 cycles (17.5 ns) to the third NUMA node. This implies that
each IF-switch (Figure 2) in a path to another NUMA node incurs a
~2‚Äì2.5 ns latency per direction in our workload.

7.2 AMD EPYC Rome - Intra-CCX
Figure 12 shows latencies for all possible communication patterns
within a CCX when Modified cache lines are accessed. L3 latencies
seem uniform. Accesses to other cores show a clear correlation
between latencies and the relative position of the communicating
cores within the CCX. It is possible that these latencies are an effect
of the 512 B alignment, which we used for the L1 and L2 cache
measurements. With this alignment, the slice hash mechanism,
which selects the L3 slice for a given cache line based on its address,
will only use the L3-slice of core 0. This gives us insight into the
internal layout of a CCX: If only the L3 slice of core 0 is accessed, the
latency penalties for accesses will be caused by the distance between
the used cores. This can be observed when core 0 requests memory
from other cores. Cache lines held by cores 1 or 2 can be accessed
within the same time. Requests to core 3 take additional time, i.e.,
~5‚Äì7 cycles for the L1 and L2 caches. A possible explanation would
be that a ring interconnect is used between the cores/caches, where
core 3 has a greater distance to core 0 than core 1 and 2.

7.3 AMD EPYC Rome - Complex Request Flow
Figure 13 shows more complex cache request flows when access-
ing Modified cache lines. For example, a program is started on a
core attached to NUMA node 2 and allocates its data there (home
node). Later, this data is used and changed by a core at NUMA
node 3 (forwarding node) and holds the most recent copy of the
memory in its cache. Finally, the data is accessed by a core located
at NUMA node 0 (requesting node). The data request would go from
the requesting core to the memory controller of the home node and
being forwarded to the core on the forwarding node, which replies
to the request. Please note that while three cores are involved in
this scenario, the access pattern can be realized by a single thread,
which is migrated by the operating system. A core on another CCX
is chosen for cases in which NUMA node 0 is also the forwarding
node, as cache request would be fulfilled within the CCX otherwise.

Figure 12: AMD EPYC 7702 ‚Äì CCX cache read latencies (Mod-
ified): latencies for all communication patterns within a
CCX.

Figure 11: Memory read latencies for an AMD EPYC 7702 on
one socket, measured from NUMA node 0 ‚Äì NUMA charac-
teristics can be clearly observed. Owned and Shared as well
as Modified and Exclusive states result in identical latencies.
Refer Table 2 for precise values.

accesses memory, which is not placed in its local memory hierarchy.
To measure latencies for such accesses, we again use the BenchIT
memory_latency benchmark. Threads are placed using the environ-
ment variables provided by x86-membench. We distinguish accesses
to cache lines placed in the coherency states Modified, Owned (on
Rome) and Exclusive, Shared, Invalid, Forward (on CLX). We omit
invalid cache lines from further analyses. Since they are fetched
from RAM at all times, the latencies are equal to those reported in
Section 6.

7.1 AMD EPYC Rome
For Rome, we present Owned and Shared as well as Modified and
Exclusive cache lines in combination as the observed performance
pattern for each of these pairs is equal. We performed measurements
at nominal frequencies of 2 GHz.

Figure 11 summarizes the measurements for the Rome system:
When a core loads data from a different L1 cache within the same
CCX, Owned and Shared cache lines can be served from the L2
cache within 72‚Äì74 cycles (36‚Äì37ns). This reflects the nature of
L2 being inclusive of the L1 cache. Modified and Exclusive data
carry a slight penalty when accessed on a different L1 cache in the
same CCX increasing latency to 78 cycles (39 ns). Data stored in
the shared L3 cache of a CCX can be read within 39 cycles (19.5 ns),
which is about half the latency when accessing data in other L1
or L2 of other cores of a CXX. According to [2], the load-to-use
latency of the L3 cache is 39 cycles.

Accesses to another CCX within the same NUMA node traverse
through the I/O-die, since CCXs within a CCD are not directly
interconnected. At least a 200 cycles (100 ns) latency is incurred
when performing accesses to non-local L3 caches.

As shown in Section 6, reading from local RAM takes 220 cycles
(110 ns). When reading memory from another NUMA node, the
properties of the I/O-die can be observed. Accessing the RAM of
the closest NUMA node requires additional ~10 cycles (5 ns). This
translates to two IF-switch and IF-repeater hops. Assuming the IF
operated at its nominal frequency of 1467 MHz, this translates to

0255075100125150Latency [ns]L1 (O/S)L1 (M/E)L2 (O/S)L2 (M/E)L3 (O/S)L3 (M/E)RAMMemory Level (Cacheline State)050100150200250300Latency [cycles]NUMA 3NUMA 2NUMA 1Neighbor CCXNeighbor CCDLocal CCXLocalL1L2L3Memory Level012301230123Requesting Core3210Owning Core80.075.075.04.081.080.04.075.083.04.080.076.04.086.084.080.077.070.072.012.079.075.012.073.081.012.078.072.012.079.082.076.039.039.039.039.040.040.038.039.039.039.039.039.038.039.039.039.00.0028.6757.3386.00Latency in [Cycles]ICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.

Markus Velten, Robert Sch√∂ne, Thomas Ilsche, and Daniel Hackenberg

tiles. We used the *_RING_BL_IN_USE performance counters [12,
Section 2.2.3] to find the mapping of CPUs (as reported by the OS)
to core tiles in our system, which we present in Figure 14. While
analyzing the layout, we observed that the third UPI link is not
present and/or not used on our system. This behavior was also
observed by McCalpin.

Figure 15 shows that latencies of the CLX system depend on
the relative positions of the cores in the mesh, as seen in the case
of latencies from core 0 to its closest and distant neighbor within
the SNC. The differences between the lowest and highest L1 and
L2 latencies within an SNC can be roughly assumed to be 2 ns
or 5 cycles, highlighting the effectiveness of Intel‚Äôs mesh design.
Increased latencies can also be observed for accesses to memory
on the second SNC. While L2 and L3 cache latencies for the states
Modified and Exclusive are identical and have therefore been con-
solidated, L1 cache latencies from Modified cache lines are higher
than those for Exclusive cache lines: Reading Exclusive cache lines
from L1 seems to be identical to L2 accesses, indicating that they
can be fetched from the inclusive L2 cache. Shared and Forwarded
cache lines are fetched from the L3 cache, if they are not present in
local caches, as described in [3].

When accessing the second SNC on the same socket, an addi-
tional latency of ~15 cycles (~6 ns) is incurred for all cache levels.
In contrast, accesses to other CCX on Rome add an additional 130‚Äì
200 cycles. These results indicate that CLX processors may be better
suited for shared memory workloads where more than four cores
work on the same data. RAM latencies for local and remote SNC
are much lower than those on Rome, as it was to be expected due
to the less complex design of CLX processors.

Our results highlight the benefit of having the memory con-
trollers integrated into the fabric mesh as it is the case with the
Intel processor. Whilst L3 latencies on CLX are ~15 cycles higher
than on Rome, all CLX cores can access the L3 at similar latencies.
This is a potential benefit compared to Rome, where only four cores
within a CCX share a L3 cache, in particular for workloads that rely
on many cores that share cache lines. If, however, a large per-core
L3 is required, Rome processors may be better suited due to their
larger per-core L3 slices and lower local L3 latencies.

8 INTER-SOCKET MAIN MEMORY

LATENCIES

While previous measurements only showed memory characteristics
for accesses within a single socket, adding another processor will
further increase complexity. Again, we use BenchIT‚Äôs latency bench-
mark to measure main memory latencies at nominal frequencies.
We schedule threads to the first core within each NUMA domain
and analyze RAM latencies for all inter-socket node-pairings.

We present results for the Rome system in Figure 16(a). Here,
a clear split along one axis of the processor can be observed. The
latency patterns indicate that the xGMI interfaces are not cross-
connected but connected as shown in Figure 16(b). Threads sched-
uled on NUMA nodes that are closer to the xGMI interfaces commu-
nicate with lower latencies with the remote socket. Communication
between nodes 0 and 6 or 2 and 4, respectively, takes the least time,
406‚Äì408 cycles (203‚Äì204 ns). Apparently, in those cases only one
IF-switch hop is required per socket in addition to the mandatory

Figure 13: AMD EPYC 7702 ‚Äì L2 read latencies for complex
cache accesses for Modified cache lines: NUMA node 0 re-
quests memory which is allocated at the Home Node, but
present in a Modified state in the L2 cache of a core on the
Forwarding Node. All potential scenarios for such accesses
with node 0 as requesting node are evaluated. The diagonal
from (0,0) to (3,3) indicates standard L2 latencies for direct
access from node 0 with forwarding node being the home
node.

The highest latency we measured in this scenario, 330 cycles
(165 ns), is higher than any latency for a native access in which
home node and forwarding node are identical. If the home as well
as the forwarding node are different from each other and from the
requesting node 0, the resulting latency is determined by the home
node. For instance, if NUMA node 1 is the home node, a latency
of 322‚Äì324 cycles (161‚Äì162 ns) is measured if either NUMA node
2 or 3 are forwarding nodes. The same is true for other cases. If,
however, NUMA node 0 is both the requesting as well as the home
node, the latency is determined by the choice of forwarding node.
Similarly, if NUMA node 0 is requesting and forwarding, the home
node determines the latency.

Interestingly, the latencies are not symmetrical along the identity
axis. Instead, if NUMA node 0 is requesting and home node, the
penalty of using a different forwarding node is higher than in cases
where NUMA node 0 is requesting and forwarding node. This is
notable since in the first case the memory is already present at
NUMA node 0 and must not necessarily be transferred to this node.
In cases where the forwarding and home node are not identical and
not at NUMA node 0, the missing symmetry can be observed as
well. If NUMA node 1 is home node and NUMA nodes 2 or 3 are
forwarding nodes, the latencies are almost identical. However, if
the roles of forwarding and home node are reversed, the latencies
decrease by 12‚Äì14 cycles (6‚Äì7 ns) when switching from NUMA
node 2 to 3 as home node.

These results highlight the importance of a careful consideration
of the NUMA properties of Rome processors. If the processor is
set up to report only as a single NUMA domain to the OS, thread
pinning should be used to avoid these latency penalties.

7.4 Intel Xeon Cascade Lake SP
We performed latency measurements on CLX at its nominal core
frequency of 2.5 GHz and 2.4 GHz uncore frequency. Latencies for
cache lines in state Forward and Shared are reported as one value as
they are identical on CLX. As described by McCalpin in [17], proces-
sors with a reduced core count can have different deactivated core

0123Home Node3210Forwarding Node165155135127161162132134156146156148153153154155127165136146155Latency [ns]Memory Performance of AMD EPYC Rome and Intel Cascade Lake SP Server Processors

ICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.

Figure 14: Mapping of CPUs as listed by the OS to core tiles on CLX test system, deactivated core tiles are depicted in gray.

hop when accessing the I/O-die. The highest latencies occur for
‚Äúdiagonal‚Äù accesses, e.g., from node 1 on socket 0 to node 5 on socket
1. For those cases, ~436 cycles (218 ns) are required, 30 cycles (15 ns)
more than the fastest case.

Main memory accesses to the second socket on the CLX system
traverse through the UPI interfaces. In our case, only the interface
tile with two UPI links is being used (see Figure 14), as performance
counter events proved6. As we show in Figure 17, the lowest la-
tencies can be measured when core 0 requests memory from the
first SNC on the second socket (SNC 2), independently of the core
that allocated memory in this NUMA node. At 138 ns, this value
is ~60 ns higher than a local main memory access. Accessing SNC
3 takes an additional 10 ns. Based on the location of the accessing
core on the first socket, additional latencies can be observed.

Our measurements show that transferring data to/from a remote
socket comes at a high additional latency penalty. If communication
between to sockets is necessary, it is beneficial to schedule com-
municating threads carefully in order to avoid additional overhead.

6We measured {R|T}xL_FLITS_ALL_DATA events for all UPI ports for various inter-
socket accesses.

Figure 15: Memory read latencies for an Intel Xeon Gold
6248 on one socket, measured from SNC 0 ‚Äì Forwarded and
Shared states result in identical latencies, Modified and Ex-
clusive latencies only for L2 and L3. Refer Table 3 for precise
values.

9 SUMMARY, CONCLUSION, AND FUTURE

WORK

In this paper, we provided in-depth descriptions of the architectures
of current AMD Rome and Intel CLX server processors. The modu-
lar chiplet design of the Rome processor allows AMD to assemble
up to 64 cores in one package and ensures that all SKUs can profit
from the same number of I/O interfaces and memory controllers.
This design results in accentuated NUMA properties and an L3
cache that is only shared among four cores at a time. In contrast,
Intel uses a monolithic design without inherent NUMA properties.
It scales up to 28 cores and shares a common L3 cache among all
cores.

We updated the existing x86-membench code of the BenchIT
suite and improved support for the cache design of these proces-
sors. The cache flush routines are now able to reflect the usage of
inclusive L2 and exclusive L3 caches by both AMD and Intel in their
current designs. The throughput benchmark can now use AVX-512
operations with up to 32 512-bit zmm vector registers.

We also evaluated the cache and main memory latencies of the
two processors. Our measurements revealed similar latencies for
local cache accesses to the L1 and L2 for both processors. The
L3 access latencies to the CCX-local L3 slices take fewer cycles
on AMD processors. However, as only four cores share the CCX-
local L3, there is no commonly shared cache with low latencies
for all cores on AMD Rome processors. Therefore, shared memory
workloads which rely heavily on data sharing among cores may
profit from Intel‚Äôs monolithic design, as all cores share a common
L3 with similar latencies.

We demonstrated the complex NUMA properties of AMD‚Äôs In-
finity Fabric grid, both for intra-socket latencies, and inter-socket
latencies. The CLX architecture only exhibits small variances for
remote cache access latencies, making these processors potentially
better suited for shared memory workloads.

More in-depth analyses of latencies for the Rome processor stress
the importance of well thought out data placement when using
these processors. This is caused by the I/O-die and the latencies it
adds to data transfers that pass through it.

The L1 and L2 cache bandwidth of the AMD processor is close
to its theoretical maximum for each vector length we evaluated.
This is not the case for the CLX processor. Instead, we observed

2 x UPICPU 0CHA 0 Memory ControllerCPU 5CHA 1CPU 3CHA 2CPU 8CHA 3CPU 6CHA 5CPU 1CHA 4CPU 4CHA 6CPU 2CHA 8CPU 7CHA 9CPU 9CHA 7CPU 12CHA 10CPU 10CHA 12CPU 13CHA 14CPU 11CHA 16CPU 15CHA 13CPU 16CHA 17CPU 14CHA 18CPU 19CHA 19CPU 17CHA 11CPU 18CHA 15 Memory Controller2 x UPICPU 20CHA 0 Memory ControllerCPU 25CHA 1CPU 23CHA 2CPU 21CHA 4CPU 28CHA 3CPU 26CHA 5CPU 22CHA 8CPU 27CHA 9CPU 24CHA 6CPU 32CHA 10CPU 30CHA 12CPU 33CHA 14CPU 31CHA 16CPU 35CHA 13CPU 36CHA 17CPU 34CHA 18CPU 39CHA 39CPU 37CHA 11CPU 38CHA 15 Memory ControllerCPU 29CHA 7020406080Latency [ns]L1 (F/S)L1 (M)L1 (E)L2 (O/F/S)L2 (M/E)L3 (F/S)L3 (M/E)RAMMemory Level (Cacheline State)050100150200Latency [cycles]SNC 1, Distant Core SNC 1, Closest CoreDistant NeighborClosest NeighborLocalICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.

Markus Velten, Robert Sch√∂ne, Thomas Ilsche, and Daniel Hackenberg

(a) Socket-socket RAM-latencies for several
NUMA node pairings.

(b) Two AMD EPYC Rome processors with interconnect paths according to measurements.

Figure 16: AMD EPYC Rome 7702: socket-socket memory latencies. The xGMI interfaces to a second processor on the same
mainboard cannot be accessed by all cores in the same time due to their asymmetric placement in the I/O-die.

ACKNOWLEDGMENTS
The authors gratefully acknowledge the compute resources and
support provided by NHR@FAU for the Caskade Lake SP system.
We used the Romeo partition by NEC of the Bull Cluster TAU-
RUS at the Center for Information Services and High Performance
Computing (ZIH) at TU Dresden for measurements on AMD Rome.

REFERENCES
[1] Advanced Micro Devices, Inc. 2020. Preliminary Processor Programming Reference
(PPR) for AMD Family 17hModel 31h, Revision B0Processors. https://developer.
amd.com/wp-content/resources/55803_B0_PUB_0_91.pdf

[2] Advanced Micro Devices, Inc. 2020. Software Optimization Guide for AMD Family
17h Models 30h and Greater Processors. https://developer.amd.com/wp-content/
resources/56305.zip

[3] Akhilesh Kumar, Don Soltis, Irma Esmer, Adi Yoaz, and Sailesh Kottapalli. 2017.
The New Intel¬Æ Xeon¬Æ Processor Scalable Family (Formerly Skylake-SP). https:
//old.hotchips.org/wp-content/uploads/hc_archives/hc29/HC29.22-Tuesday-
Pub/HC29.22.90-Server-Pub/HC29.22.930-Xeon-Skylake-sp-Kumar-Intel.pdf
Hot Chips 29.

[4] M. Arafa, B. Fahim, S. Kottapalli, A. Kumar, L. P. Looi, S. Mandava, A. Rudoff,
I. M. Steiner, B. Valentine, G. Vedaraman, and S. Vora. 2019. Cascade Lake: Next
Generation Intel Xeon Scalable Processor. IEEE Micro (2019). https://doi.org/10.
1109/MM.2019.2899330

[5] T. Burd, N. Beck, S. White, M. Paraschou, N. Kalyanasundharam, G. Donley,
A. Smith, L. Hewitt, and S. Naffziger. 2019. ‚ÄúZeppelin‚Äù: An SoC for Multichip
Architectures. IEEE Journal of Solid-State Circuits (2019). https://doi.org/10.1109/
JSSC.2018.2873584

[6] C. L. Alappat, J. Hofmann, G. Hager, H. Fehske, A. R. Bishop, and G. Wellein. 2020.
Understanding HPC Benchmark Performance on Intel Broadwell and Cascade
Lake Processors. In International Conference on High Performance Computing.
https://doi.org/10.1007/978-3-030-50743-5_21

[7] D. Suggs, D. Bouvier, M. Clark, K. Lepak, and M. Subramony. 2019. AMD ‚ÄúZEN
2‚Äù. In IEEE Hot Chips 31 Symposium (HCS). https://doi.org/10.1109/HOTCHIPS.
2019.8875673

[8] D. Suggs, M. Subramony, and D. Bouvier. 2020. The AMD ‚ÄúZen 2‚Äù Processor.

IEEE Micro (2020). https://doi.org/10.1109/MM.2020.2974217

[9] A. Fog. 2020. 3. The microarchitecture of Intel, AMD and VIA CPUs: An op-
timization guide for assembly programmers and compiler makers.
https:
//www.agner.org/optimize/microarchitecture.pdf Technical University of Den-
mark.

[10] Daniel Hackenberg, Robert Sch√∂ne, Thomas Ilsche, Daniel Molka, Joseph
Schuchart, and Robin Geyer. 2015. An Energy Efficiency Feature Survey of
the Intel Haswell Processor. In International Parallel and Distributed Processing
Symposium Workshop. https://doi.org/10.1109/ipdpsw.2015.70

[11] Intel Corporation. [n.d.].

Product brief

-

form ‚Äì Second Generation - Intel Xeon Scalable Processors.
//www.intel.com/content/dam/www/public/us/en/documents/product-
briefs/2nd-gen-xeon-scalable-processors-brief-Feb-2020-2.pdf

[12] Intel Corporation. 2017.

Intel Xeon Processor Scalable Memory Family Uncore
Performance Monitoring. https://www.intel.com/content/www/us/en/processors/

Intel Xeon Scalable Plat-
https:

Figure 17: Socket-Socket main memory read latencies for an
Intel Xeon Gold 6248. Only the UPI-tile with two UPI links
is being used.

L1 bandwidths that were below the theoretical maximum for each
vector length and below the sustainable bandwidths Intel defined
in [15, Table 2-6, Section 2.2.1.3]. A similar picture emerges for the
L2 cache bandwidth. Here, we expected bandwidths similar to the
L1 bandwidth for AVX and SSE instructions, as the L1 bandwidth
was well within the specified L2 bandwidth. Instead, we observed L2
bandwidths well below the results for the L1. Our cache bandwidth
results are in line with other research on these processors [6].

We observed a 41 % lower RAM bandwidth for an entire socket
for the CLX processors compared to Rome in our throughput bench-
mark. This is a result of the lower number of memory channels
of CLX processors. Although the Intel design allows one core to
access three memory channels when using the SNC configuration,
a lower single core bandwidth was observed compared to AMD. We
therefore conclude that Rome processors may be better suited for
memory bandwidth bound workloads than their CLX counterparts,
as long as data sharing among cores is avoided as much as possible.
Future research could focus on the new architectures by Intel
and AMD, Ice Lake SP and EPYC Milan. Furthermore, the impact
of our findings on real world applications should be considered in
future work.

1023Requesting Node on S07645Home Node on S1212210212217206204209211210207203205218213210212203218206210214Latency [ns]0 SNC 07 SNC 010 SNC 112 SNC 1Requesting Core on S020 SNC 227 SNC 232 SNC 339 SNC 3Core ID on S1138138148147140140150150141141150151139139148149138151141144147Latency [ns]Memory Performance of AMD EPYC Rome and Intel Cascade Lake SP Server Processors

ICPE ‚Äô22, April 9‚Äì13, 2022, Bejing, China.

xeon/scalable/xeon-scalable-uncore-performance-monitoring-manual.html
[13] Intel Corporation. 2020. Intel¬Æ Xeon¬Æ Processor Scalable Family ‚Äì Specification
Update. https://www.intel.com/content/dam/www/public/us/en/documents/
specification-updates/xeon-scalable-spec-update.pdf

[14] Intel Corporation. 2020. Second Generation Intel¬Æ Xeon¬Æ Scalable Processors ‚Äì
Specification Update. https://www.intel.com/content/www/au/en/products/docs/
processors/xeon/2nd-gen-xeon-scalable-spec-update.html

[15] Intel Corporation. 2021. Intel¬Æ 64 and IA-32 Architectures Optimization Refer-
ence Manual. https://software.intel.com/content/dam/develop/external/us/en/
documents-tps/64-ia-32-architectures-optimization-manual.pdf

[16] John McCalpin. 2018. Address Hashing in Intel Processors). https://doi.org/10.

26153/tsw/13161

[17] John McCalpin. 2021. Mapping Core and L3 Slice Numbering to Die Location in

Intel Xeon Scalable Processors). https://doi.org/10.26153/tsw/13119

[18] G. Juckeland, S. B√∂rner, M. Kluge, S. K√∂lling, W.E. Nagel, S. Pfl√ºger, H. R√∂ding,
S. Seidl, T. William, and R. Wloch. 2004. BenchIT ‚Äî Performance measurement
and comparison for scientific applications. In Parallel Computing. https://doi.
org/10.1016/S0927-5452(04)80064-9

[19] A. Kashyap. 2020. High Performance Computing: Tuning Guide for AMD EPYC‚Ñ¢
7002 Series Processors. Advanced Micro Devices, Inc. https://developer.amd.com/
wp-content/resources/56827-1-0.pdf

[20] Daniel Molka. 2017. Performance Analysis of Complex Shared Memory Systems.
https://nbn-

Ph.D. Dissertation. Technische Universit√§t Dresden, Dresden.
resolving.org/urn:nbn:de:bsz:14-qucosa-221729

[21] Daniel Molka, Daniel Hackenberg, and Robert Sch√∂ne. 2014. Main Memory and
Cache Performance of Intel Sandy Bridge and AMD Bulldozer. In Workshop on
Memory Systems Performance and Correctness (MSPC). https://doi.org/10.1145/
2618128.2618129

[22] D. Molka, D. Hackenberg, R. Sch√∂ne, and M. S. M√ºller. 2009. Memory Performance
and Cache Coherency Effects on an Intel Nehalem Multiprocessor System. In
18th International Conference on Parallel Architectures and Compilation Techniques
(PACT). https://doi.org/10.1109/PACT.2009.22

[23] Daniel Molka, Daniel Hackenberg, Robert Sch√∂ne, and Wolfgang E. Nagel. 2015.
Cache Coherence Protocol and Memory Performance of the Intel Haswell-EP
Architecture. In 44th International Conference on Parallel Processing (ICPP). https:
//doi.org/10.1109/ICPP.2015.83

[24] D. Mulnix. 2017.

Intel¬Æ Xeon¬Æ Processor Scalable Family Technical
https://software.intel.com/en-us/articles/intel-xeon-processor-

Overview.
scalable-family-technical-overview

[25] Samuel Naffziger, Noah Beck, Thomas Burd, Kevin Lepak, Gabriel H. Loh, Mahesh
Subramony, and Sean White. 2021. Pioneering Chiplet Technology and Design
for the AMD EPYC‚Ñ¢ and Ryzen‚Ñ¢ Processor Families : Industrial Product. In 2021
ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA).
57‚Äì70. https://doi.org/10.1109/ISCA52012.2021.00014

[26] S. Naffziger, K. Lepak, M. Paraschou, and M. Subramony. 2020. 2.2 AMD Chiplet
Architecture for High-Performance Server and Desktop Products. In International
Solid- State Circuits Conference (ISSCC). https://doi.org/10.1109/ISSCC19947.2020.
9063103

[27] Sabela Ramos and Torsten Hoefler. 2013. Modeling Communication in Cache-
Coherent SMP Systems: A Case-Study with Xeon Phi. In 22nd International
Symposium on High-Performance Parallel and Distributed Computing (HPDC).
https://doi.org/10.1145/2462902.2462916

[28] Robert Sch√∂ne, Thomas Ilsche, Mario Bielert, Andreas Gocht, and Daniel Hacken-
berg. 2019. Energy Efficiency Features of the Intel Skylake-SP Processor and Their
Impact on Performance. In International Conference on High Performance Com-
puting & Simulation (HPCS). https://doi.org/10.1109/HPCS48598.2019.9188239
[29] Robert Sch√∂ne, Thomas Ilsche, Mario Bielert, Markus Velten, Markus Schmidl,
and Daniel Hackenberg. 2021. Energy Efficiency Aspects of the AMD Zen 2 Ar-
chitecture. In 2021 IEEE International Conference on Cluster Computing (CLUSTER).
562‚Äì571. https://doi.org/10.1109/Cluster48925.2021.00087

[30] Erich Strohmaier, Jack Dongarra, Horst Simon, Martin Meuer, and Hans Meuer.

2021. TOP500. https://top500.org (accessed 2022-01-07).

[31] S. M. Tam, H. Muljono, M. Huang, S. Iyer, K. Royneogi, N. Satti, R. Qureshi, W.
Chen, T. Wang, H. Hsieh, S. Vora, and E. Wang. 2018. SkyLake-SP: A 14nm 28-
Core Xeon¬Æ processor. In International Solid - State Circuits Conference (ISSCC).
https://doi.org/10.1109/ISSCC.2018.8310170

[32] J. Treibig, G. Hager, and G. Wellein. 2010. LIKWID: A lightweight performance-
oriented tool suite for x86 multicore environments. In First International Workshop
on Parallel Software Tools and Tool Infrastructures (PSTI). https://doi.org/10.1109/

ICPPW.2010.38

[33] Yuval Yarom and Katrina Falkner. 2014. FLUSH+ RELOAD: A high resolu-
tion, low noise, L3 cache side-channel attack. In 23rd USENIX Security Sympo-
sium. https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-
paper-yarom.pdf

A CACHE AND MAIN MEMORY LATENCIES -

SUMMARY

Table 2: AMD EPYC 7702 ‚Äì memory read latencies: memory
accesses on one socket. Shared cachelines fetched from an-
other CCX are read from RAM, therefore they are listed for
local CCX reads only. Invalid cache lines are fetched from
RAM at all times.

Latency in [ns] ([cycles])

Source
Local
Same
CCX
Same
CCD

NUMA 1

NUMA 2

NUMA 3

State
M/O/E/S
M/E
O/S
M/E
O/S
M/E
O/S
M/E
O/S
M/E
O/S

L3

19.5 (39)

L1
2 (4)
39 (78)
36 (72)

L2
6 (12)
37 (74)
37 (74)
123.5 (247) 126.5 (253) 120.5 (241)
102.5 (205) 108 (216)
108 (216)
129 (258) 131.5 (263) 125.5 (251)
115 (230)
114 (228)
107 (214)
142 (284) 145.5 (291) 140 (280)
115.5 (231) 122.5 (245) 143 (246)
148.5 (297) 152 (304)
146 (292)
126 (252) 151.5 (253)
120 (240)

RAM

110 (220)

115 (230)

144 (248)

127.5 (255)

Table 3: Intel Xeon Gold 6248 ‚Äì memory read latencies: mem-
ory accesses on one socket. Modified and Exclusive cache
lines have the same latencies, except for remote L1 accesses.
Shared and Forwarded cache lines can be read with identical
latencies. Invalid cache lines are fetched from RAM at all
times.

Latency in [ns] ([cycles])

Source State
Local M/E/S/F 1.6 (4)

L1

50.8‚Äì52.8
(127‚Äì132)
48‚Äì50
(120‚Äì125)

Same
SNC

Other
SNC

M

E

S/F

M

E

S/F

L2
5.6 (14)

48.8‚Äì50.4
(122‚Äì126)

L3

RAM

21.6 (54) 80 (200)

20 (50)

56.4‚Äì58
(141‚Äì145)
53.6‚Äì55.2
(134‚Äì138)

54.4‚Äì56
(136‚Äì140)
54‚Äì56
(135‚Äì140)

25.6 (64)

25.6 (64) 86.4 (216)

