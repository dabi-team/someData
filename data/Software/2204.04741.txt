Is GitHub’s Copilot as Bad As Humans at Introducing
Vulnerabilities in Code?∗
Meiyappan Nagappan
University of Waterloo
Waterloo, Canada
mei.nagappan@uwaterloo.ca

Owura Asare
University of Waterloo
Waterloo, Canada
oasare@uwaterloo.ca

N. Asokan
University of Waterloo
Waterloo, Canada
asokan@acm.org

2
2
0
2

r
p
A
0
1

]
E
S
.
s
c
[

1
v
1
4
7
4
0
.
4
0
2
2
:
v
i
X
r
a

ABSTRACT
Background/Context: Several advances in deep learning have been
successfully applied to the software development process. Of re-
cent interest is the use of neural language models to build tools
that can assist in writing code. There is a growing body of work
to evaluate these tools and their underlying language models. We
aim to contribute to this line of research via a comparative empir-
ical analysis of these tools and language models from a security
perspective. For the rest of this paper, we use CGT (Code Genera-
tion Tool) to refer to language models as well as other tools, such
as Copilot, that are built with language models.

Objective/Aim: The aim of this study is to compare the perfor-
mance of one CGT, Copilot, with the performance of human devel-
opers. Speciﬁcally, we investigate whether Copilot is just as likely
to introduce the same software vulnerabilities as human develop-
ers.

Method: We will use the Big-Vul dataset proposed by Fan et
al. [8] - a dataset of vulnerabilities introduced by human develop-
ers. For each entry in the dataset, we will recreate the scenario
before the bug was introduced and allow Copilot to generate a
completion. The completions are manually inspected by three in-
dependent coders in order to be classiﬁed as 1. containing the same
vulnerability (introduced by the human), 2. containing a ﬁx for the
vulnerability or 3. other. The other category is used as a catchall
for scenarios that are out of scope for this project.

1 INTRODUCTION
Deep learning (DL) research is advancing at a quick pace and its
techniques are being applied in a number of other research areas.
One such area is Natural Language Processing (NLP) where deep
learning architectures have been used to create language models
capable of various tasks such as language translation, question an-
swering, and text summarization.

Of the many tasks that language models can be used for, a grow-
ing area of interest is their use in the software development pro-
cess. Ultimately, these language models and their ﬁne-tuned de-
scendants will be used by developers to generate code. Naturally,
researchers are concerned about the security of these language
models and the extent to which they can be relied upon; Some have
shown that code generation tools like GitHub’s Copilot have a ten-
dency to suggest code that is not completely secure [13], while
others have demonstrated ways that language models can be re-
purposed for the task of bug ﬁxing and security improvement [12]
[14][11]. For the rest of this paper, we will use CGT (Code Gener-
ation Tool) to refer to language models as wells as tools, such as
Copilot, that are built with language models.

∗This report was accepted at the MSR 2022 Registered Reports Track

While we know that CGTs (speciﬁcally Copilot) have a propen-
sity to generate insecure code [13], we do not know if they are
as bad as humans at generating insecure code. If CGTs are not
as bad as humans, there may be a role that they can play in soft-
ware development. If CGTs are as bad as humans, practitioners and
researchers can postpone their adoption until there is a demon-
strated improvement in their performance.

After mining software repositories that contain vulnerabilities
introduced by human developers, we recreate scenarios to be com-
pleted by Copilot in order to determine whether it generates code
fragments containing the same vulnerabilities as the original. This
way, we can more accurately judge whether Copilot is better than
or as bad as human developers.

We believe the outcome of this study can assist in making a
stronger argument either for or against the incorporation of as-
sistive code generation tools in the software development process.
The results from this study can also augment future research at-
tempts to improve the security and performance of CGTs like Copi-
lot.

2 BACKGROUND
2.1 Language Models
At a high level, statistical language models provide probability dis-
tributions of some set of words in a vocabulary. Although earlier
language models-like the n-gram model-were based on non-neural
methods, there was a shift towards using neural networks to build
language models [3]. This shift entailed using Recurrent Neural
Networks (RNNs) as the main component of language models that
accepted input sequences and generated a corresponding output
probability distribution for the next token(s) in a sequence. RNN-
based language models serve many purposes including machine
translation, question answering and part-of-speech (POS) tagging.
Despite the improved performance provided by RNNs, they had a
short-term memory which had an impact on their performance es-
pecially in situations that involve long input sequences. Long Short
Term Memory (LSTM) and Gated Recurrent Units (GRU) were pre-
sented as improvements to RNNs that solved the short-term mem-
ory issue [15][5].

While the performance of RNNs was improved by the introduc-
tion of LSTM and GRU, there are still some shortcomings when it
comes to parallelizing training. This is a result of the sequential na-
ture of RNN processing and can incur high costs as the lengths of
input sequences increases (as is likely in real world scenarios). The
work done by Vaswani et al. [17] introduced the Transformer ar-
chitecture currently used by high performing language models and
ushered in the transition from a recurrent model to one that relies
mainly on the concept of attention. Simply put, attention provides

 
 
 
 
 
 
a mechanism for a model to focus on relevant features/aspects of
the input data regardless of its length. More formally, “it computes
a weight distribution on the input sequence, assigning higher val-
ues to more relevant elements” [9]. The transformer architecture
is what powers several popular language models today such as
BERT (Bidirectional Encoder Representations from Transformers)
and GPT-3 (Generative Pre-trained Transformer).

2.2 Code Generation
Research has been conducted that shows that the software devel-
opment process is amenable to the techniques of machine learning
[11] [7]. Advancements in deep learning and natural language pro-
cessing mean that machine learning tools and techniques can be
used in the process of creating code.

Code completion (generation) tools available either through in-
tegrated development environments (IDEs) or as extensions to text
editors are already widely used by developers. These code comple-
tion tools continue to evolve in complexity as advances in NLP/DL
are made [16]. GitHub’s Copilot is an example of an evolved code
completion tool. Copilot is described as an “AI pair programmer
. . . trained on billions of lines of public code” [1]. Currently avail-
able as an extension for the VSCode text editor, Copilot takes into
account the surrounding context of a program and generates pos-
sible code completions for the developer. IntelliCode is another ex-
ample of such a tool that generates recommendations “based on
thousands of open-source projects on GitHub” [2].

Beneath the surface, tools like Copilot and IntelliCode are a
composition of diﬀerent language models trained and ﬁne tuned
towards the speciﬁc task of generating code. The language mod-
els themselves consist of diﬀerent neural architectures that are
grounded in the RNN model. However, improvements upon the
basic RNN have been made resulting in more complex models like
LSTMs and the Transformer-based encoder-decoder models. Copi-
lot is based on OpenAI’s Codex [4] which is itself a ﬁnely tuned ver-
sion of GPT-3. Similarly, IntelliCode uses a generative transformer
model (GPT-C) which is a variant of GPT-2 [16].

These code generation tools are eﬀective because researchers
have uncovered ways to take the underlying syntax of the target
programming language into consideration instead of approaching
it as a purely language or sequence generation task [19]. However,
as stated by Pearce et al. [13], these tools that inherit from lan-
guage models do not necessarily produce the best (or most secure)
code but generate the most likely completion (for a given prompt)
based on the encountered samples during training. This necessi-
tates a rigorous security evaluation of such tools so that any glar-
ing ﬂaws are identiﬁed before widespread use by the larger devel-
opment community.

3 RESEARCH OVERVIEW
3.1 Hypothesis
We hypothesize that Copilot is equally likely to introduce the same
bugs that have previously been introduced by human developers
when placed in similar contexts. The rationale for this hypothesis
is that Copilot is trained on codebases generated by humans and
as a result is likely to reproduce the same mistakes/errors resulting
in the same kinds of bugs.

2

Research Question: Is Copilot equally likely to generate the same
vulnerabilities as human developers? The answer to this question
can inform software developers about the potential risks in adopt-
ing assistive CGTs (like Copilot) into the software development
process.

3.2 Variables
Code Generation Tool. The study consists of an evaluation of Copi-
lot which is a CGT. While not as mature/reﬁned as Copilot, there
are other examples of potential CGTs such as the Codex language
model and its two variants (the davinci model and the cushman
model) and the AI21 Jurassic models namely j1-jumbo and j1-large.
Our evaluation focuses on Copilot because it is the most special-
ized towards the task of code generation and it is the only one we
currently have access to.

Programming Language. The dataset consists of vulnerabilities
from C and C++ projects. While these languages are fairly popular
and will likely have featured in the training data for the models,
prior research has shown that less popular languages can aﬀect
the Copilot’s ability to generate better results. [13].

Prompt. The prompts function as the input to Copilot. They are
created by removing portions of the original ﬁle that contain the
vulnerable code such that they contain code from the start of the
ﬁle up to the line directly before the vulnerability is introduced.

Copilot Output. Copilot is given various prompts as input to
which it returns a suggested completions. This output code is the
variable we measure/analyze in this study. We determine whether
the completions contain the same vulnerabilities introduced by hu-
man developers.

Analysis Methodology. We use manual inspection to analyze out-
puts from Copilot. Through the use of three independent coders,
we compare the output from Copilot with the original (human-
generated) ﬁle to determine whether the same vulnerability is present.

3.3 Dataset
We use the Big-Vul dataset provided by Fan et al. [8]. It was col-
lected by crawling the Common Vulnerabilities and Exposures (CVE)
website and collecting certain information about vulnerabilities
such as CVE ID, CVE summary, and conﬁdentiality impact. From
the CVE page, they select entries that have links to publicly avail-
able Git repositories where they can collect commit related infor-
mation such as commits before and after a certain vulnerability is
ﬁxed.

The dataset contains vulnerabilities in C and C++ from open
source projects (mainly on GitHub) from 2002 to 2019. Our prelim-
inary evaluation of this dataset-obtained from the GitHub reposi-
tory associated with the work in [8] indicates that it currently con-
tains 4,432 commits used to ﬁx 3,754 vulnerabilities. The dataset
contains 21 features for each of its entries. The features of the
dataset are provided in Table 1.

We select this dataset for this study for several reasons. In ad-
dition to being an accepted dataset by the community (by virtue
of its publication), the presence of the “reference link” feature is
invaluable for the methodology of this study. Since the intention
is to evaluate Copilot in situations where real world vulnerabilities
have already been introduced by human developers, we believe we

can make good use of links to vulnerability ﬁxing commits and any
associated CVE and CWE (Common Weakness Enumeration) data
which this dataset provides.

4 EXECUTION PLAN
Dataset acquisition and preprocessing
The study begins with mining repositories that contain real world
vulnerabilities and their associated ﬁxes. This process is facilitated
by the ref_link column in the BigVul dataset which contains 4,432
examples (rows). Considering the manual component of this study,
we select a subset of this data of at least a 100 entries for our evalu-
ation. The preprocessing step involves selecting samples that only
involve single ﬁle bug ﬁxes. This is as a result of the nature of gener-
ating outputs from language models; Copilot cannot generate code
for multiple ﬁles at the same time. As a result we consider multi-ﬁle
ﬁxes as out of scope for the purposes of this study. We start from
vulnerabilities published in 2019 and work our way backwards un-
til our desired sample size is obtained (≥ 100).

Scenario Re-creation
Once ﬁltering is complete and the resulting dataset of at least 100
vulnerabilities is obtained, we proceed to the manual component
of the study. For each entry in the dataset, we locate the corre-
sponding project repository and identify the speciﬁc vulnerability-
ﬁxing commit. We obtain a diﬀ that depicts speciﬁc lines of code
that were added and removed in order to ﬁx the vulnerability. We
save the version of the ﬁle before the ﬁx was introduced, i.e, the
version of the ﬁle that still contains the vulnerable code. We cu-
rate prompts (inputs to Copilot) by removing the vulnerable lines
of code (as speciﬁed by the commit diﬀ) as well as all subsequent
lines from the ﬁle. For example, given a ﬁle that contains 100 lines
of code with the vulnerable code located at lines 60-68, we save the
contents of the ﬁle from line 1 to line 59 and delete the remaining
41 lines (lines 60 - 100). The prompt is saved in a diﬀerent ﬁle so as
to retain the original vulnerable ﬁle for comparison and analysis.

Output Generation
Once prompts have been created for each entry in the dataset, we
proceed to response generation from the Copilot. As mentioned
earlier, we focus on Copilot because it is the most specialized tool
towards the task of code generation and it is the only one we cur-
rently have access to. Inputs (prompts) are fed to the Copilot via an
extension in the VSCode text editor. Copilot is prompted to gener-
ate a response simply by placing the cursor in the desired location.
We save responses from the CGTs in a response ﬁle for subsequent
analysis.

Analysis
We plan to use manual inspection to analyze responses from Copi-
lot. The manual inspection will be conducted by three independent
coders selected from a pool of computer science graduate students
who are ﬂuent in C/C++. Since we are primarily concerned with
whether the code generated by Copilot contains the same vulner-
ability introduced by the human developer, our analysis approach

3

is to manually inspect the output and compare it with the original
ﬁle. We classify Copilot’s outputs into one of three categories:

• Class A - same as the vulnerable code
• Class B - same as the ﬁxed code
• Class C - Other

For each evaluated sample, the coders will pick the best category
for the generated output. If Copilot’s output implements a diﬀerent
feature, it will fall under category C. For outputs in categories A
and B, the coders will also decide (based on their expertise and the
assistance of the text editor/linting tools) if the code is compilable
and/or runnable. In the predictably rare case where all the three
coders completely disagree on the categorization of an output, we
categorize it as “Other” by default.

When manual analysis determines that the output from Copi-
lot does not contain the original vulnerability (i.e not a Class A
output), we look to see if the generated code matches the ﬁx from
humans (Class B) or is some other code completely (Class C). If it
is class C, then we do not know if the completion contains a vul-
nerability. We could use a static analysis tool like CodeQL [10], but
as static analysis tools only output warnings at best, we would not
know if the generated code in Class C is actually vulnerable or not.
If all vulnerabilities in the sample (introduced by humans) are
also introduced by Copilot (i.e. all evaluated samples are Class A),
then we conclude that Copilot is equally likely to create the same
vulnerabilities as humans. However, if Copilot introduces a ﬁx for
any non-empty subset of the dataset (i.e. number of Class B sam-
ples > 0), then we can claim that it is less likely to introduce vulner-
abilities as compared to humans. In the event where there are no
Class B samples and the number of Class C samples > 0, we draw
no deﬁnite conclusions about the performance of Copilot relative
to human developers.

It is worth noting that the parameters of this study do not al-
low us to detect whether Copilot is worse than human developers
in terms of introducing vulnerabilities. We focus on whether Copi-
lot is as bad as humans or better in terms of introducing vulner-
abilities. The question of whether Copilot is worse than humans
requires a diﬀerent type of study. This could be a human study
where both humans and humans assisted with Copilot are given
the same prompt and asked to generate completions. Even in this
setup, there are some key considerations to be made about the se-
lection of prompts (to prevent a biased sample) and the analysis
approach for checking for vulnerabilities. We consider this an av-
enue for future work and limit the scope of this study to the task
of determining whether Copilot is just as bad as human developers
or better.

5 IMPACT
As is typical with most technological advancements, AI-based code
generation tools can simultaneously augment human capabilities
in some way but may have negative impacts on an existing ecosys-
tem. This calls for due diligence in analyzing the potential negative
impacts of adapting new technological advancements.

Our study is a step in this direction: by answering the ques-
tion of whether CGTs like Copilot are likely to introduce the same
vulnerabilities that a human developer would, we can determine

Features
Access Complexity

Column Name in CSV
access_complexity

Authentication
quired
Availability Impact

Re-

authentication_required

availability_impact

Commit ID

commit_id

Commit Message
Conﬁdentiality Impact

commit_message
conﬁdentiality_impact

CWE ID
CVE ID

CVE Page
CVE Summary
CVSS Score

cwe_id
cve_id

cve_page
summary
score

Files Changed

ﬁles_changed

Integrity Impact

integrity_impact

Description
Reﬂects the complexity of the attack re-
quired to exploit the software feature
misuse vulnerability
If authentication is required to exploit
the vulnerability
Measures the potential impact to avail-
ability of a successfully exploited mis-
use vulnerability
Commit ID in code repository, indicat-
ing a mini-version
Commit message from developer
Measures the potential impact on con-
ﬁdentiality of a successfully exploited
misuse vulnerability
Common Weakness Enumeration ID
Common Vulnerabilities and Expo-
sures ID
CVE Details web page link for that CVE
CVE summary information
The relative severity of software ﬂaw
vulnerabilities
All the changed ﬁles and correspond-
ing patches
Measures the potential impact to in-
tegrity of a successfully exploited mis-
use vulnerability
Mini-version ID after the ﬁx
Mini-version ID before the ﬁx

Lan-

Mini-version After Fix
Mini-version
Before
Fix
Programming
guage
Project
Publish Date
Reference Link
Update Date
Vulnerability Classiﬁ-
cation

version_after_ﬁx
version_before_ﬁx

lang

Project programming language

project
publish_date
ref_link
update_date
vulnerability_classiﬁcation

Project name
Publish date of the CVE
Reference link in the CVE page
Update date of the CVE
Vulnerability type

Table 1: An overview of the features of the Big-Vul Dataset provided by Fan et al. [8]

whether these tools are as bad as or better than their human coun-
terparts. The results of our study can inform software developers
and other industry professionals about the potential advantages
or disadvantages in adapting these tools to assist in the software
development process.

REFERENCES
[1] GitHub Copilot · Your AI pair programmer. https://copilot.github.com/.
[2] Visual Studio IntelliCode. https://visualstudio.microsoft.com/services/intellicode/.
[3] Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhuvana Ramabhadran.
Deep Neural Network Language Models. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Lan-
guage Modeling for HLT, pages 20–28, Montréal, Canada, June 2012. Association
for Computational Linguistics.

[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,

4

Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy
Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens
Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plap-
pert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir
Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr,
Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew
Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew,
Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evalu-
ating Large Language Models Trained on Code. arXiv:2107.03374 [cs], July 2021.
arXiv: 2107.03374.

[5] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Em-
pirical evaluation of gated recurrent neural networks on sequence modeling. In
NIPS 2014 Workshop on Deep Learning, December 2014, 2014.

[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-
training of deep bidirectional transformers for language understanding. In Jill
Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational

Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186. Associ-
ation for Computational Linguistics, 2019.

[7] Du Zhang and J.J.P. Tsai. Machine learning and software engineering. In 14th
IEEE International Conference on Tools with Artiﬁcial Intelligence, 2002. (ICTAI
2002). Proceedings., pages 22–29, Washington, DC, USA, 2002. IEEE Comput. Soc.
[8] Jiahao Fan, Yi Li, Shaohua Wang, and Tien N. Nguyen. A C/C++ Code Vulnera-
bility Dataset with Code Changes and CVE Summaries. In Proceedings of the 17th
International Conference on Mining Software Repositories, pages 508–512, Seoul
Republic of Korea, June 2020. ACM.

[9] Andrea Galassi, Marco Lippi, and Paolo Torroni. Attention in natural lan-
guage processing. IEEE Transactions on Neural Networks and Learning Systems,
32(10):4291–4308, 2021.

[13] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and
Ramesh Karri. Asleep at the Keyboard? Assessing the Security of GitHub Copi-
lot’s Contributions. In 2022 IEEE Symposium on Security and Privacy (SP), 2022.
[14] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan
Dolan-Gavitt. Can OpenAI Codex and Other Large Language Models Help Us
Fix Security Bugs? arXiv:2112.02125 [cs], December 2021. arXiv: 2112.02125.
[15] Hasim Sak, Andrew W. Senior, and Françoise Beaufays. Long short-term mem-
ory based recurrent neural network architectures for large vocabulary speech
recognition. CoRR, abs/1402.1128, 2014.

[16] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. Intel-
liCode compose: code generation using transformer. In Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, pages 1433–1443, Virtual Event USA,
November 2020. ACM.

[10] GitHub. About CodeQL. https://codeql.github.com/docs/codeql-overview/about-codeql/,

2022.

[11] Nan Jiang, Thibaud Lutellier, and Lin Tan. CURE: Code-Aware Neural Machine
Translation for Automatic Program Repair. In 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE), pages 1161–1173, May 2021.
ISSN:
1558-1225.

[12] E. Mashhadi and H. Hemmati. Applying codebert for automated program repair
of java simple bugs. In 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pages 505–509, Los Alamitos, CA, USA, may 2021.
IEEE Computer Society.

[17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need.
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett, editors, Advances in Neural Information Processing Systems, vol-
ume 30. Curran Associates, Inc., 2017.

[18] Wei Xu and Alex Rudnicky. Can artiﬁcial neural networks learn language mod-
els? In Sixth International Conference on Spoken Language Processing, ICSLP 2000
/ INTERSPEECH 2000, Beijing, China, October 16-20, 2000, pages 202–205. ISCA,
2000.

[19] Pengcheng Yin and Graham Neubig. A syntactic neural model for general-

purpose code generation. CoRR, abs/1704.01696, 2017.

5

