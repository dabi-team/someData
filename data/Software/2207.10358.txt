Domain Decomposition Learning Methods for Solving Elliptic Problems

Qi Sun∗ , Xuejun Xu∗ ,† , and Haotian Yi∗

Abstract. With the aid of hardware and software developments, there has been a surge of interests in solving
partial diﬀerential equations by deep learning techniques, and the integration with domain decom-
position strategies has recently attracted considerable attention due to its enhanced representation
and parallelization capacity of the network solution. While there are already several works that
substitute the numerical solver of overlapping Schwarz methods with the deep learning approach,
the non-overlapping counterpart has not been thoroughly studied yet because of the inevitable inter-
face overﬁtting problem that would propagate the errors to neighbouring subdomains and eventually
hamper the convergence of outer iteration. In this work, a novel learning approach, i.e., the compen-
sated deep Ritz method, is proposed to enable the ﬂux transmission across subregion interfaces with
guaranteed accuracy, thereby allowing us to construct eﬀective learning algorithms for realizing the
more general non-overlapping domain decomposition methods in the presence of overﬁtted interface
conditions. Numerical experiments on a series of elliptic boundary value problems including the
regular and irregular interfaces, low and high dimensions, smooth and high-contrast coeﬃcients on
multidomains are carried out to validate the eﬀectiveness of our proposed domain decomposition
learning algorithms.

Key words. neural networks, domain decomposition methods, elliptic partial diﬀerential equations, boundary

overﬁtting, compensated deep Ritz method.

AMS subject classiﬁcations. 65M55, 65Nxx, 92B20, 49S05.

1. Introduction. Many problems of interested in science and engineering are modelled by
partial diﬀerential equations, which help us understand and control complex systems across
a wide variety of real-world applications [10]. Unfortunately, it is often diﬃcult or impossible
to obtain their analytical solutions, therefore various numerical techniques including, but not
limited to the ﬁnite diﬀerence, ﬁnite volume, and ﬁnite element methods [27, 28, 5] have been
developed to obtain their approximate solutions. Based on a discretization of the solution
space by dividing the computational domain into a polygon mesh, these numerical methods
are highly accurate and eﬃcient for low-dimensional problems on regular domains. However,
there are still many challenging issues to be addressed, e.g., mesh generation remains complex
when the boundary is geometrically complicated or dynamically changing, computation of
high-dimensional problems is often infeasible due to the so-called curse of dimensionality, and
many others. As the traditional methods are continuously being improved, it also raises the
need for new methods and tools in order to tackle the diﬃculties mentioned above [23].

With the signiﬁcant enhancement of hardware and software developments, the deep learn-
ing methods [26] have recently emerged as an attractive alternative for solving diﬀerent types
of equations in both the forward and inverse problems [43, 13, 6], and have achieved remark-
able success due to the universal approximation [40] of neural networks as an ansatz to the

∗School of Mathematical Sciences, Tongji University, Shanghai 200092, China (qsun irl@tongji.edu.cn,

2111166@tongji.edu.cn).

†Institute of Computational Mathematics, AMSS, Chinese Academy of Sciences, Beijing 100190, China

(xxj@lsec.cc.ac.cn).

1

2
2
0
2

l
u
J

1
2

]

A
N
.
h
t
a
m

[

1
v
8
5
3
0
1
.
7
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

solution function or the operator mapping [23]. For instance, one of the most representative
work is the physics-informed neural network (PINN) [39, 25, 24], where the diﬀerential op-
erators can be exactly calculated via automatic diﬀerentiation [37] and the residual of the
governing equations is incorporated into the training loss function. Another pioneering work
is the deep Ritz method [47], which resorts to the variational principle and performs better for
problems with low-regularity solutions [7]. It is also possible to design the training task that
is corresponded to the weak formulation of the underlying equations [48], but the training
process is often hard to converge due to the imbalance between generative and adversarial
networks. Furthermore, to make the trained model satisfy the boundary conditions as precise
as possible, various techniques including, but not limited to the deep Nitsche method [32], the
augmented Lagrangian relaxation [20], and the auxiliary network with distance function [34, 3]
have been developed. When compared with the traditional numerical solvers [27, 5], these
deep learning methods enjoy advantages of ﬂexible and meshless implementation, strong abil-
ity to tackle non-linearity and to break the curse of dimensionality, and others [23]. However,
they may still exhibit poor performance in handling problems with multi-scale phenomena
[46, 21], and the large training cost is also a major drawback that limits their application
in the ﬁeld of large-scale scientiﬁc computing. To further enhance the representation and
parallelization capacity of the network solution, it is natural to integrate the state-of-the-art
deep learning techniques with the conventional domain decomposition strategies [17], aiming
at paving the wary for truly enabling the large-scale computation using neural networks.

One way is to incorporate the distributed training techniques [2], e.g., the data and mod-
ule parallelization, into the PINN approach [21, 22, 19], where the training task is split into
multiple subproblems according to the non-overlapping partition of domain and simple con-
tinuity conditions are enforced across the subregion interfaces. Although this combination is
quite general and parallelizable, no speciﬁc knowledge of the underlying problem is utilized
during the training process, which substantially diﬀers from the conventional methodology of
splitting a partial diﬀerential equation [45, 38]. On the other hand, as the classical domain
decomposition methods [45] can be formulated at the continuous or the weak level, various
works have been devoted to employing the learning approaches for solving the decomposed
subproblems and therefore beneﬁt from the mesh free nature of deep learning solvers. Under
such circumstances, the machine learning analogue of the overlapping domain decomposition
methods have emerged recently and have successfully handled many problems [32, 29, 35, 42],
however, the more general non-overlapping counterpart has not been thoroughly studied yet.
A major diﬃculty is that the network solutions of local problems are prone to overﬁtting at
and near the interface [9, 1] since the interface conditions are enforced as soft penalty func-
tions during training and the size of training data on interface is smaller than that of interior
domains, which eventually propagates the errors to neighboring subdomains and hampers the
convergence of outer iteration. In other words, the issue of boundary overﬁtting is a key threat
to the integration of deep learning techniques and domain decomposition methods, especially
for those based on a direct ﬂux exchange across the subdomain interfaces. What’s worse, it
will always occur to a greater or lesser extent in practical implementations and has not been
fully addressed or studied in the existing literature.

In this work, we consider the benchmark Poisson equation that serves as a necessary
prerequisite to validate the eﬀectiveness of the domain decomposition learning approaches

DOMAIN DECOMPOSITION LEARNING METHODS

3

interface exchange
solution

overlapping

Alternating/Jacobi-
Schwarz Algorithm

non-overlapping

Robin-Robin
Learning Algorithm

Domain Decomposition
Learning Methods

interface exchange
solution and ﬂux

non-overlapping

Dirichlet-Neumann
Learning Algorithm

Neumann-Neumann
Learning Algorithm

Dirichlet-Dirichlet
Learning Algorithm

Robin-Robin
Learning Algorithm

Dirichlet
subproblems

Solution-Oriented
Learning Methods

Dirichlet
subproblems

Solution-Oriented
Learning Methods

Robin
subproblems

Solution-Oriented
Learning Methods

Robin
subproblems

Solution-Oriented
Learning Methods

Dirichlet
subproblems

Solution-Oriented
Learning Methods

Neumann
subproblems

Compensated
Deep Ritz Method

Dirichlet
subproblems

Solution-Oriented
Learning Methods

Neumann
subproblems

Compensated
Deep Ritz Method

Neumann
subproblems

Compensated
Deep Ritz Method

Dirichlet
subproblems

Solution-Oriented
Learning Methods

Robin
subproblems

Solution-Oriented
Learning Methods

Robin
subproblems

Compensated
Deep Ritz Method

Notation:

em

data exchange between subregions

em

domain decomposition strategy

em / em

learning methods with accurate treatment of interface solution/ﬂux

em transmission of the local solution explicitly across the subdomain interfaces

em transmission of the local ﬂux data implicitly between adjacent subdomains

Figure 1: Our proposed framework of domain decomposition learning methods for solving the
elliptic boundary value problems.

[29, 30, 35], namely,

(1.1)

−∆u(x) = f (x)
u(x) = 0

in Ω,
on ∂Ω,

4

where d ∈ N+ is the dimension, Ω ⊂ Rd a bounded domain, and f (x) ∈ L2(D) a given func-
tion. Note that for the case of inhomogeneous boundary conditions, it is equivalent to solving
(1.1) after employing an auxiliary network with distance function for boundary ﬁtting [34, 3].
In contrast to the standard classiﬁcation of domain decomposition methods that are based on
the partition strategy of the computational domain, our proposed framework (see Figure 1) is
established on the information exchange across the subdomain interfaces. More speciﬁcally,
with the set of interface points being small compared to the entire training dataset, the trained
models are often found to satisfy the governing equations but for overﬁtted interface condi-
tions [9]. As a direct result, though the solution value of local problem is often found to be
of satisfactory accuracy in both the interior and boundary points, the ﬂux prediction through
trained models is typically of unacceptable low accuracy along the interface and therefore
hampers the convergence of direct ﬂux exchange schemes. For simplicity, we refer to these
deep learning solvers as the solution-oriented learning methods [39, 47] in what follows, since
the issue of boundary overﬁtting will always occur to a greater or lesser extent in practice. To
deal with the overﬁtted interface conditions, we propose a novel learning approach, i.e., the
compensated deep Ritz method, that allows the accurate ﬂux transmission between neighbour-
ing subdomains but without explicitly computing the solution’s derivatives on the interface.
Thanks to the proposed method, we are now able to construct eﬀective learning approaches
for realizing the classical Dirichlet-Neumann, Neumann-Neumann, Dirichlet-Dirichlet, and
Robin-Robin algorithms in the non-overlapping regime [45] (see Figure 1), and experimental
results conducted on a series of elliptic boundary value problems validate our statements. It is
noteworthy that although the Robin-Robin algorithm only requires the exchange of solution
value across the interfaces [8], two additional parameters in the Robin boundary conditions
need to be determined and may lead to interface overﬁtting if not chosen appropriately. For-
tunately, our compensated deep Ritz method can also help to alleviate this problem.

The remainder of this paper is organized as follows. In section 2, we present a brief review
of the classical domain decomposition methods and the solution-oriented learning methods for
solving elliptic boundary value problems. Then, according to our framework (see Figure 1), the
most straightforward machine learning analogue of the Robin-Robin algorithm is introduced
in section 3, followed by the detailed illustration of our compensated deep Ritz method in
section 4. Numerical experiments on a series of problems including the regular and irregular
interfaces, two and four subdomains, low and high dimensions are reported in section 5, as
well as the more challenging elliptic interface problem with high-contrast coeﬃcients. Section
6 will conclude this paper and present some directions for future work.

2. Preliminaries. This section is devoted to brieﬂy reviewing the classical domain de-
composition methods [45, 38] and the widely-used deep learning methods for solving the
second-order elliptic boundary value problems.

2.1. Domain Decomposition Methods. The idea of domain decomposition for solving the
Poisson equation has a long history dating back to the 18th century [41], and there is a vast
literature on domain decomposition methods (we refer to [45, 38, 33] and the references cited
therein). For the ease of illustration, the computational domain Ω ⊂ Rd is ﬁrst assumed to be
partitioned into two subdomains {Ωi}2
i=1 (see Figure 2 for example), while the case of multiple
subdomains can be obtained in a similar way. Depending on the partition strategies being

DOMAIN DECOMPOSITION LEARNING METHODS

5

Γ2

Γ1

Γ

Γ

Ω1

2
Ω
∩
1
Ω

Ω2

Ω1

Ω2

Ω1

Ω2

ΩR

ΩB

ΩB

ΩR

Figure 2: Decomposition of a bounded domain Ω ⊂ R2 in two dimension. Left: Overlapping
partition with two subdomains. Middle: Non-overlapping partition with two subdomains,
which is separated by curved regular or irregular interfaces. Right: Red-Black partition of the
non-overlapping multidomains into two sets, where the intersection between two subregions
in the same class is marked in bold.

employed, the domain decomposition methods are typically categorized into the overlapping
and non-overlapping groups, which poses no problem if the ﬁnite diﬀerence or ﬁnite element
methods are utilized as the numerical solvers for local problems [45, 38].

However, when the neural network is adopted as the solution ansatz, the trained model is
often found to satisﬁes the governing equations but for overﬁtted boundary conditions [9, 1],
which greatly diﬀers from the traditional methods [45, 38]. To this end, the classiﬁcation of
domain decomposition methods adopted in this paper is based on the information exchange
between neighbouring subregions (see also Figure 1). More speciﬁcally, we summarize in what
follows some representative decomposition-based approaches in the literature [45]. Here, we
refer to the Schwarz alternating method and the Robin-Robin algorithm as SAM and RRA
in algorithm 2.1, while the Dirichlet-Neumann, Neumann-Neumann, and Dirichlet-Dirichlet
algorithms are abbreviated as DNA, NNA, and DDA in algorithm 2.2, respectively. In addi-
tion, the relaxation parameter ρ shall lay between (0, ρmax) in order to achieve convergence
[11]. Notably, although the overlapping methods with small overlap are cheap and easy to
implement, it comes at the price of slower convergence. Besides, the non-overlapping methods
are more eﬃcient in handling elliptic problems with large jumps in the coeﬃcients.

2.2. Deep Learning Solvers. As can be concluded from the previous discussion, the de-

composed problem on each subregion takes on the following form

(2.1)

−∆ui(x) = f (x)
ui(x) = 0
Biui(x) = hi(x)

in Ωi,
on ∂Ωi \ Γ,
on Γ,

where Bi is a boundary operator on interface that may represent the Dirichlet, Neumann, or
Robin boundary condition, namely,

Dirichlet condition: Biu(x) = u(x),
Neumann condition: Biu(x) = ∇u(x) · ni,

Robin condition: Biu(x) = ∇u(x) · ni + κiu(x),

and the associated boundary conditions hi(x) are iteratively determined so as to ensure the
transmission conditions across subdomain interfaces [45].

6

Algorithm 2.1 Domain Decomposition Methods Based on Solution Exchange

Start with the initial guess h[0]
for k ← 0 to K (maximum number of outer iterations) do

1 and h[0]

2 of each subsolution along the interface;

while stopping criteria are not satisﬁed do

% Local Problem-Solving





1 = f

−∆u[k]
u[k]
1 = 0
1 = h[k]
B1u[k]

1

in Ω1
on ∂Ω1 \ Γ1
on Γ1

(cid:40)

where B1u[k]

1 =

% Solution Exchange Between Subdomains

u[k]
1
1 · n1 + κ1u[k]

1

∇u[k]

(cid:40)

h[k]
2 =

% Local Problem-Solving

u[k]
1
1 + (κ1 + κ2)u[k]

1

−h[k]

on Γ2
on Γ2

(SAM)

(RRA)






2 = f

−∆u[k]
u[k]
2 = 0
2 = h[k]
B2u[k]

2

in Ω2
on ∂Ω2 \ Γ2
on Γ2

(cid:40)

where B2u[k]

2 =

u[k]
2
2 · n2 + κ2u[k]

2

∇u[k]

(SAM)

(RRA)

(SAM)

(RRA)

% Solution Exchange Between Subdomains
(cid:40)

h[k+1]
1

=

u[k]
2
2 + (κ1 + κ2)u[k]
ρ + (1 − ρ)(−h[k]
2 )

on Γ1
on Γ1

(SAM)

(RRA)

end while

end for

Remark: RRA is deﬁned in the non-overlapping regime, i.e., Γ1 = Γ2 during iteration.

When the deep learning techniques are employed for solving the boundary value problem
(2.1), the hypothesis space of local solution is ﬁrst built by neural networks. Here, we adopt
the standard fully-connected neural network of depth L ∈ N+ [18], in which the (cid:96)-th hidden
layer receives an input x(cid:96)−1 ∈ Rn(cid:96)−1 from its previous layer and transforms it to

T (cid:96)(x(cid:96)−1) = W (cid:96)x(cid:96)−1 + b(cid:96),

where W (cid:96) ∈ Rn(cid:96)×n(cid:96)−1 and b(cid:96) ∈ Rn(cid:96) are the weights and biases to be learned. By choosing an
appropriate activation function σ(·) for each hidden layer, the network solution can then be
deﬁned as

ˆui(x; θ) = (cid:0)T L ◦ σ ◦ T L−1 · · · ◦ σ ◦ T 1(cid:1)(x),

where ◦ represents the composition operator and θ = {W (cid:96), b(cid:96)}L
(cid:96)=1 denotes the collection of
the trainable parameters. One can also employ other network architectures, e.g., the residual

DOMAIN DECOMPOSITION LEARNING METHODS

7

Algorithm 2.2 Domain Decomposition Methods Based on Solution and Flux Exchange

Start with the initial guess h[0]
for k ← 0 to K (maximum number of outer iterations) do

1 and h[0]

2 of each subsolution along the interface;

while stopping criteria are not satisﬁed do

% Local Problem-Solving





i = f

−∆u[k]
u[k]
i = 0
i = h[k]
Biu[k]

i

in Ωi
on ∂Ωi \ Γ
on Γ

where Biu[k]

i =





u[k]
i
u[k]
i
∇u[k]
i

· ni

for i = 1,

(DNA)

for i = 1, 2,

(NNA)

for i = 1, 2,

(DDA)

% Solution or Flux Exchange Between Subdomains

h[k]
i =





∇u[k]

−∇u[k]
i−1 · ni−1
1 · n1 + ∇u[k]
1 − u[k]
u[k]

2

on Γ for i = 2,

(DNA)

2 · n2 on Γ for i = 1, 2,
on Γ for i = 1, 2,

(NNA)

(DDA)

% Local Problem-Solving






i = f

−∆u[k]
u[k]
i = 0
i = h[k]
Biu[k]

i

in Ωi
on ∂Ωi \ Γ
on Γ

where Biu[k]

i =






· ni
· ni

∇u[k]
i
∇u[k]
i
u[k]
i

for i = 2,

(DNA)

for i = 1, 2,

(NNA)

for i = 1, 2,

(DDA)

% Solution or Flux Exchange Between Subdomains
i+1 + (1 − ρ)u[k]
ρu[k]
i
1 + u[k]
i − ρ(u[k]
h[k]
2 )
1 · n1 + ∇u[k]
i − ρ(∇u[k]
h[k]





h[k+1]
i

=

on Γ for i = 1,

(DNA)

on Γ for i = 1, 2,
2 · n2) on Γ for i = 1, 2,

(NNA)

(DDA)

end while

end for

network and its variants [15, 13], for the parametrization of local solutions, which is omitted
here and left for future investigation.

To update the trainable parameters through the celebrated backpropagation algorithm
[16], various training loss functions (before employing the numerical integration) have been
proposed, e.g., the PINN approach [39] that is based on the strong form of (2.1), namely,

ˆui(x; θ) = arg min

θ

(cid:90)

Ωi

| − ∆ˆui − f |2dx + β

(cid:32)(cid:90)

∂Ωi\Γ

|ˆui|2ds +

(cid:90)

Γ

(cid:33)

|Bi(ˆui) − hi|2ds

,

or the deep Ritz method [47] that is based on the variational form of (2.1), namely,

ˆui(x; θ) = arg min

θ

(cid:90)

Ωi

(cid:16) 1
2

|∇ˆui|2 − f ˆui

(cid:17)

dx + β

(cid:90)

|ˆui|2ds + LΓ(ˆui(x; θ))

∂Ωi\Γ

8

where the last term depends on the boundary condition being imposed at the interface

LΓ(ˆui(x; θ)) =






(cid:90)

Γ
(cid:90)

|ˆui − hi|2ds

(Dirichlet condition),

hi ˆui ds

(Neumann condition),

Γ
(cid:16) κi
2

|ˆui|2 − hi ˆui

(cid:17)

ds

(Robin condition),

β

−

(cid:90)

Γ

and β > 0 is a user-deﬁned penalty coeﬃcient.

Apart from these two widely-used methods, the weak adversarial network [48] is based
on the weak form of (2.1), while another series of neural network methods is designed to use
separate networks to ﬁt the interior and boundary equations respectively [34, 3]. We refer
the readers to [23, 42, 17] for a more detailed review of the existing deep learning solvers.
Notably, with the interface conditions being included as soft constraints in the training loss
function and the set of interface points being small compare to that of interior domains, the
trained network is highly prone to overﬁtting at the interface [9, 1], which is a key threat
to the integration of the direct ﬂux exchange schemes and the deep learning techniques but
rarely studied or addressed in the literature.

3. Robin-Robin Algorithm via Solution-Oriented Learning Methods. Note that in ad-
dition to the deep learning analogue of overlapping Schwarz methods [29, 35, 30, 42, 44], the
non-overlapping Robin-Robin algorithm [45, 38] is also based on a direct exchange of solution
value between neighbouring subdomains (see Figure 1 or algorithm 2.1). Moreover, as the
decomposition leads to simpler and smoother functions to be learned on each subregion, it
thus enables us to employ the standard solution-oriented learning methods [39, 47, 48, 43]
for the numerical solution of local problems. As such, the PINN approach [39] is generally
preferred since it is known to empirically work better for problems with smooth solutions [7].
However, a major drawback is the determination of two additional parameters in the Robin
boundary conditions, which may cause diﬃculties for network training or require more outer
iterations to converge.

For the ease of illustration, we consider the case of two non-overlapping subregions (see
Figure 2) in what follows, where the interface conditions are invariably of the Robin type [45,
38, 8]. Note that the detailed iterative process in terms of diﬀerential operators is presented
in algorithm 2.1, which is not restated here for simplicity. It can be observed that the update
of interface conditions only requires the Dirichlet data of local solutions, which seems simpler
and more straightforward than those based on a direct ﬂux exchange [45, 38] when combining
with the deep learning models.

To realize the Robin-Robin algorithm using the PINN approach, the local problem is ﬁrst
rewritten as an equivalent optimization problem by minimizing the residual of the governing
equation, namely, for i = 1, 2,

(3.1) u[k]

i = arg min

ui∈Vi

(cid:90)

Ωi

| − ∆ui − f |2 dx + β

(cid:18)(cid:90)

Γ

(cid:12)
(cid:12)
(cid:12)κiui +

∂ui
∂ni

− h[k]
i

(cid:12)
2
(cid:12)
(cid:12)

(cid:90)

ds +

|ui|2 ds

(cid:19)

,

∂Ω∩∂Ωi

where the boundary and interface conditions are included as soft penalty terms in the training

DOMAIN DECOMPOSITION LEARNING METHODS

9

Algorithm 3.1 Deep Learning Analogue of Robin-Robin Algorithm for Two Subdomains

% Initialization
– divide domain Ω ⊂ Rd into two non-overlapping subregions Ω1 and Ω2;
– specify network structures ˆu1(x; θ1) and ˆu2(x; θ2) for each subproblem;
– generate Monte Carlo training samples XΓ, XΩi, and XDi for i = 1, 2;
% Outer Iteration Loop
Start with the initial guess h[0] along the interface Γ;
for k ← 0 to K (maximum number of outer iterations) do

while stopping criteria are not satisﬁed do

% Robin Subproblem-Solving via Solution-Oriented Learning Method
for (cid:96) ← 0 to L (maximum number of training epochs) do

for m ← 0 to M (maximum number of training mini-batches) do

– draw mini-batch data uniformly at random from XΓ, XΩ1, and XD1;
– network training by employing solution-oriented learning method (3.2), i.e.,

θ[k]
1 = arg min

θ1

LΩ1(ˆu1) + β

(cid:16)

(cid:17)
LD1(ˆu1) + LR(ˆu1, h[k]
1 )

end for

end for
% Update of boundary values at interface Γ
for n ← 1 to NΓ do

h[k]
2 (xΓ

n) = −h[k]

1 (xΓ

n) + (κ1 + κ2)ˆu1(xΓ

n; θ[k]
1 )

end for
% Robin Subproblem-Solving via Solution-Oriented Learning Method
for (cid:96) ← 0 to L (maximum number of training epochs) do

for m ← 0 to M (maximum number of training mini-batches) do

– draw mini-batch data uniformly at random from XΩ1, XΩ2, XD1, and XD2;
– network training by employing solution-oriented learning method (3.2), i.e.,

θ[k]
2 = arg min

θ2

LΩ2(ˆu2) + β

(cid:16)

LD2(ˆu2) + LR(ˆu2, h[k]
2 )

(cid:17)

end for

end for
% Update of boundary values at interface Γ
for n ← 1 to NΓ do

(xΓ

n) = ρh[k]

1 (xΓ

n) + (1 − ρ)(−h[k]

2 (xΓ

n) + (κ1 + κ2)u[k]

2 (xΓ

n))

h[k+1]
1

end for
end while

end for

loss function. Then, by parametrizing the trail functions as neural networks

1 (x) ≈ ˆu[k]
u[k]

1 (x) := ˆu1(x; θ[k]
1 )

and u[k]

2 (x) ≈ ˆu[k]

2 (x) := ˆu2(x; θ[k]
2 ),

10

and generating the training sample points inside each subregion and at its boundary, i.e.,

XΩi = (cid:8)xΩi

n

(cid:9)NΩi
n=1, XDi = (cid:8)xDi

n

(cid:9)NDi
n=1,

and XΓ = (cid:8)xΓ

n

(cid:9)NΓ
n=1,

the powerful stochastic optimization tools [4] can be applied for fulﬁlling the learning tasks
associated with (3.1), that is, for i = 1, 2 and the k-th outer iteration,

(3.2)

θ[k]
i = arg min

θi

LΩi(ˆui(x; θi)) + β

(cid:16)

LDi(ˆui(x; θi)) + LR(ˆui(x; θi), h[k]

(cid:17)
i (x))

,

where the loss functions (not relabelled) are deﬁned as

LΩi(ˆui(x; θi)) =

|Ωi|
NΩi

LDi(ˆui(x; θi)) =

NΩi(cid:88)

(cid:12)
(cid:12) − ∆ˆui(xΩi

n )(cid:12)
2,
n ; θi) − f (xΩi
(cid:12)

n=1
|∂Ωi \Γ|
NDi

NDi(cid:88)

n=1

|ˆui(xDi

n ; θi)|2,

LR(ˆui(x; θi), h[k]

i (x)) =

|Γ|
NΓ

NΓ(cid:88)

n=1

(cid:12)
(cid:12)κi ˆui(xΩi

n ; θi) +

∂ ˆui
∂ni

(xΩi

n ; θi) − h[k]

n )(cid:12)
2.
i (xΩi
(cid:12)

Here, and in what follows, |Ωi|, |∂Ωi \ Γ|, and |Γ| denote the Jacobians of the transformations
that map the random variables generated from a standard uniform distribution to the input
sample points XΩi, XDi, and XΓ, respectively, where i = 1, 2.

To sum up, by employing the standard PINN approach as the numerical solver of local
problems, the deep learning analogue of the classical Robin-Robin algorithm is presented in
Algorithm 3.1, where κ1, κ2 > 0 are two additional user-deﬁned parameters. We can assume,
without loss of generality, that κ1 = 1 and leave the other parameter to be tuned. In fact,
as the size of the training data on interfaces is typically much smaller than that of interior
domains, too large (or small) value of κ2 > 0 may cause weights imbalance in the training loss
function and hence make the training process susceptible to overﬁtting on the interface, while
a moderate value can guarantee convergence but at the cost of more outer iterations. This
greatly diﬀers from the the conventional ﬁnite element setting [8] since the neural network is
adopted as the solution ansatz, and is further demonstrated through numerical experiments in
section 5. Fortunately, the issue of weights imbalance can be tackled by using our compensated
deep Ritz method, which is theoretically and numerically studied in the following sections.

4. Compensated Deep Ritz Method. In this section, we begin by focusing on the non-
overlapping domain decomposition methods that are based on the direct ﬂux exchange between
neighbouring subregions (see Figure 1). When solving the decomposed problems using neural
networks, it is known that although the overall training loss tends to be decreased as the
iteration proceeds, the trained model often converges to a local minimizer that adequately
satisﬁes the constrained equations but for overﬁtted boundary conditions [9, 1].
In other
words, the ﬂux approximation using trained networks is typically of unacceptable low accuracy
at and near the interface, preventing us from deploying the solution-oriented learning methods
[39, 47] for solving the decomposed subproblems.

DOMAIN DECOMPOSITION LEARNING METHODS

11

To overcome such an inevitable drawbacks in practical situations, the compensated deep
Ritz method is proposed to enable the accurate transmission of numerical ﬂux in the pres-
ence of overﬁtted interface conditions, which is of key importance for developing the eﬀective
Dirichlet-Neumann, Neumann-Neumann, and Dirichlet-Dirichlet learning algorithms. More-
over, it can also help to alleviate the issue of interface overﬁtting when realizing the Robin-
Robin algorithm through deep learning techniques. Consequently, we will be able to fully
leverage the beneﬁts of deep learning solvers to deal with the complicated geometry domains,
high-dimensional problems, and many others [23].

4.1. Dirichlet-Neumann Learning Algorithm. We begin by considering the Dirichlet-
Neumann algorithm [45, 38], where the detailed iterative process in terms of diﬀerential op-
erators is presented in Algorithm 2.2. To avoid the explicit computation and transmission of
interface ﬂux, the variational formulation of the multidomain problem is taken into consider-
ation. More precisely, by integrating by parts in Ω ⊂ Rd, the weak formulation of the Poisson
equation (1.1) reads: ﬁnd u ∈ H 1

0 (Ω) such that

(4.1)

a(u, v) = (f, v)

for any v ∈ H 1

0 (Ω),

where the bilinear forms are deﬁned as

a(u, v) =

(cid:90)

Ω

∇u · ∇v dx and (f, v) =

(cid:90)

Ω

f v dx.

For the ease of illustration, we consider a two subdomain decomposition of (4.1) in what
follows, and similar results can be achieved for multidomain problems using a red-black par-
tition [45]. Under such a circumstance, let Ω1, Ω2 denote a non-overlapping decomposition
of domain Ω ⊂ Rd, with interface Γ = ∂Ω1 ∩ ∂Ω2 separating the subregions (see Figure 2 for
example). By deﬁning the bilinear terms

ai(ui, vi) =

(cid:90)

Ωi

∇ui · ∇vi dx and (f, vi)i =

(cid:90)

Ωi

f vi dx

and setting

Vi = (cid:8)vi ∈ H 1(Ωi) (cid:12)

(cid:12) vi|∂Ω∩∂Ωi = 0(cid:9) and V 0

i = H 1

0 (Ωi)

for i = 1, 2, the Green’s formula then implies that (4.1) can be equivalently reformulated as:
ﬁnd u1 ∈ V1 and u2 ∈ V2 such that

a1(u1, v1) = (f, v1)1
u1 = u2
a2(u2, v2) = (f, v2)2 + (f, R1γ0v2)1 − a1(u1, R1γ0v2)

for any v1 ∈ V 0
1 ,
on Γ,
for any v2 ∈ V2,

where γ0v = v|Γ indicates the restriction of v ∈ H 1(Ωi) on interface Γ, and Ri : H
00(Γ) → Vi
any diﬀerentiable extension operator [38, 45]. Based on the minimum total potential energy

1
2

12

principle [10], we immediately obtain its equivalent variational form, that is,

arg min
u1∈V1, u1|Γ=u2

1
2

a1(u1, u1) − (f, u1)1,

a2(u2, u2) − (f, u2)2 − (f, R1γ0u2)1 + a1(u1, R1γ0u2),

arg min
u2∈V2

1
2

and therefore the variational formulation of the iterative Dirichlet-Neumann algorithm [45, 38]:
given the initial guess h[0] ∈ H
1
2

00(Γ), then solve for k ≥ 0,

a1(u1, u1) − (f, u1)1,

1) u[k]

1 =

1
2

arg min
u1∈V1, u1|Γ=h[k]

2) u[k]

2 = arg min
u2∈V2
3) h[k+1] = ρu[k]

2 + (1 − ρ)h[k]

on Γ,

1
2

a2(u2, u2) − (f, u2)2 − (f, R1γ0u2)1 + a1(u[k]

1 , R1γ0u2),

with ρ ∈ (0, ρmax) being the acceleration parameter [11]. Notably, the continuity of ﬂux across
the interface is guaranteed without explicitly calculating and exchanging local ﬂuxes.

The variational formulation also makes it possible to integrate with the machine learning

methods [47]. More precisely, the unknown solutions are parametrized by neural networks

(4.2)

1 (x) ≈ ˆu[k]
u[k]

1 (x) := ˆu1(x; θ[k]
1 )

and u[k]

2 (x) ≈ ˆu[k]

2 (x) := ˆu2(x; θ[k]
2 )

i ) indicates the approximate solution with trained network parameters θ[k]

where ˆui(x; θ[k]
for
i = 1, 2, and the network structure employed here could be a fully-connected neural network
[12] or a residual neural network with several building blocks [15]. We note that in contrast
to the ﬁnite element methods [45] where the extension is mesh-dependent and locally deﬁned,
the mesh-free neural network parametrization (4.2) can be regarded as a global function and
thus provides a natural extension operator, i.e., the neural network extension operator,

i

R1γ0 ˆu2(x, θ2) = ˆu2(x, θ2)

which extends the restriction of ˆu2(x, θ2) on interface Γ to the subregion Ω1 with zero boundary
value on ∂Ω1 ∩ ∂Ω. Here, the requirement of homogeneous boundary condition is dealt with
a soft manner by introducing the penalty function

(cid:90)

∂Ω1∩∂Ω

|ˆu2|2 ds

into the learning task (4.5) of the mixed Neumann-Dirichlet problem. Notably, as the ex-
tension operator is required to be diﬀerentiable, the hyperbolic tangent or sigmoid activation
function should be used rather than the ReLU function.

Accordingly, by introducing the penalty term for enforcing essential boundary conditions,

DOMAIN DECOMPOSITION LEARNING METHODS

13

the Dirichlet problem on Ω1 can be formulated as∗

(4.3)

θ[k]
1 = arg min

θ1

(cid:90)

Ω1

(cid:16) 1
2

|∇ˆu1|2 − f ˆu1

(cid:18)(cid:90)

(cid:17)

dx + β

∂Ω1∩∂Ω

|ˆu1|2 ds +

|ˆu1 − h[k]|2 ds

(cid:19)

,

(cid:90)

Γ

where β > 0 is a user-deﬁned penalty coeﬃcient.
In fact, as the decomposition of equa-
tion leads to simpler functions to be learned on each subregion and hence the second-order
derivatives can be explicitly involved during the training process, the residual form

(4.4)

θ[k]
1 = arg min

θ1

(cid:90)

Ω1

| − ∆ˆu1 − f |2dx + β

(cid:18)(cid:90)

∂Ω1∩∂Ω

|ˆu1|2 ds +

|ˆu1 − h[k]|2 ds

(cid:19)

(cid:90)

Γ

is preferred to the variational form (4.3) since the former is empirically found to be capable of
oﬀering more accurate estimates of the solution’s gradient inside the computational domain
[7]. On the other hand, the mixed Neumann-Dirichlet problem on Ω2 can be written as

(4.5)

θ[k]
2 = arg min

θ2

(cid:90)

Ω2

(cid:16) 1
2

|∇ˆu2|2 − f ˆu2

(cid:17)

dx +

(cid:90)

(cid:16)

Ω1

∇ˆu[k]

1 · ∇ˆu2 − f ˆu2

(cid:17)

dx + β

(cid:90)

|ˆu2|2 ds,

∂Ω

which relies on the gradient value ∇ˆu[k]

1 and therefore beneﬁts from the residual form (4.4).
Now we are ready to discretize the functional integrals (4.3, 4.4) and (4.5), where the Monte
Carlo method is adopted so as to overcome the curse of dimensionality [36]. Speciﬁcally, by
generating the training sample points inside each subdomain and at its boundary, i.e.,

XΩi = (cid:8)xΩi

n

n=1, XDi = (cid:8)xDi
(cid:9)NΩi

n

(cid:9)NDi
n=1,

and XΓ = (cid:8)xΓ

n

(cid:9)NΓ
n=1,

where Di = ∂Ωi ∩ ∂Ω, and the total number of points in the training datasets XΩi, XDi, and
XΓ are denoted by NΩi, NDi, and NΓ, respectively. Consequently, by deﬁning the following
loss functions

LΩi(ˆui(x; θi)) =






|Ωi|
NΩi

|Ωi|
NΩi

LDi(ˆuj(x; θj)) =

|∂Ωi \Γ|
NDi

NΩi(cid:88)

n=1

NΩi(cid:88)

n=1

NDi(cid:88)

n=1

LN (ˆu2(x; θ2), ˆu1(x; θ[k]

1 )) =

|Ω1|
NΩ1

NΩ1(cid:88)

(cid:16)

n=1

(cid:12)
(cid:12) − ∆ˆui(xΩi

n )(cid:12)
n ; θi) − f (xΩi
(cid:12)

2

(residual form),

|∇ˆui(xΩi

n ; θi)|2 − f (xΩi

n )ˆui(xΩi

n ; θi)

(cid:19)

(cid:18) 1
2

(variational form),

|ˆuj(xDi

n ; θj)|2, LΓ(ˆu1(x; θ1)) =

|Γ|
NΓ

NΓ(cid:88)

n=1

|ˆu1(xΓ

n; θ1)−h[k](xΓ

n)|2,

∇ˆu1(xΩ1

n ; θ[k]

1 ) · ∇ˆu2(xΩ1

n ; θ2) − f (xΩ1

n )ˆu2(xΩ1

(cid:17)
n ; θ2)

,

the learning task associated with (4.3, 4.4) is deﬁned as

(4.6)

θ[k]
1 = arg min

θ1

LΩ1(ˆu1(x; θ1)) + β

(cid:16)

LD1(ˆu1(x; θ1)) + LΓ(ˆu1(x; θ1))

(cid:17)

,

∗For notational simplicity, ˆu1(x, θ1) and ˆu2(x, θ2) are abbreviated as ˆu1 and ˆu2 if not speciﬁed otherwise.

14

while that of the functional integral (4.5) is given by
(4.7)
θ[k]
2 = arg min

LΩ2(ˆu2(x; θ2)) + LN (ˆu2(x; θ2), ˆu1(x; θ[k]

θ2

1 )) + β

(cid:16)

LD1(ˆu2(x; θ2)) + LD2(ˆu2(x; θ2))

(cid:17)

.

Clearly, even though the network solution of the Dirichlet subproblem on Ω1 is prone to
overﬁtting on the interface [9, 1], it can be observed from (4.7) that the mixed Neumann-
Dirichlet problem on Ω2 can be solved without explicitly computing and enforcing the ﬂux
transmission condition across subdomain interfaces. Moreover, as the second-order derivatives
are explicitly involved during the training process of Dirichlet subproblems (4.4), the network
approximation to the solution’s gradient is rather accurate inside each subregion, which is
highly desirable for solving the mixed Neumann-Dirichlet problem (4.5).

To sum up, by using our compensated deep Ritz method, the proposed Dirichelt-Neumann
learning algorithm is presented in algorithm 4.1, where the mini-batch data computed during
training are not relabelled for notational simplicity, and the stopping criteria can be construc-
ted by measuring the diﬀerence between two consecutive iterations [29]. We also note that,
though the Dirichlet-Neumann learning algorithm 4.1 has sequential steps that inherited from
the original Dirichlet-Neumann approach [45, 38], various techniques have been developed to
solve subproblems in parallel (see [33] and references cited therein), which is left for future
investigation.

Remark 4.1. Our compensated deep Ritz method (or the Dirichlet-Neumann learning al-
gorithm 4.1) can also be easily extended to solve the more challenging elliptic interface problem
with high-contrast coeﬃcients [31, 14]

−∇ · (c(x)∇u(x)) = f (x)
u(x) = 0

in Ω,
on ∂Ω,

where Γ = ∂Ω1 ∩ ∂Ω2 is an immersed interface (see Figure 2), the coeﬃcient function c(x) is
piecewise constant with respect to the decomposition of domain

(cid:40)

c(x) =

c1 > 0
c2 > 0

in Ω1,
in Ω2,

and the natural jump conditions [31] are deﬁned as

[u] = 0 and

(cid:21)

(cid:20)

c

∂u
∂n

= q

on Γ.

Applying Green’s formula in each subdomain and adding them together, we obtain the weak
formulation for the high-contrast problem, i.e., ﬁnd u1 ∈ V1 and u2 ∈ V2 such that

b1(u1, v1) = (f, v1)1
u1 = u2
b2(u2, v2) = (f, v2)2 + (f, R1γ0v2)1 − b1(u1, R1γ0v2) − (q, v2)Γ,

for any v1 ∈ V 0
1 ,
on Γ,
for any v2 ∈ V2,

DOMAIN DECOMPOSITION LEARNING METHODS

15

Algorithm 4.1 Dirichlet-Neumann Learning Algorithm for Two Subdomains

% Initialization
– divide domain Ω ⊂ Rd into two non-overlapping subregions Ω1 and Ω2;
– specify network structures ˆu1(x; θ1) and ˆu2(x; θ2) for each subproblem;
– generate Monte Carlo training samples XΓ, XΩi, and XDi for i = 1, 2;
% Outer Iteration Loop
Start with the initial guess h[0] along the interface Γ;
for k ← 0 to K (maximum number of outer iterations) do

while stopping criteria are not satisﬁed do

% Dirichlet Subproblem-Solving via Solution-Oriented Learning Method
for (cid:96) ← 0 to L (maximum number of training epochs) do

for m ← 0 to M (maximum number of training mini-batches) do

– draw mini-batch data uniformly at random from XΓ, XΩ1, and XD1;
– network training by employing the physics-informed neural network (4.6), i.e.,

θ[k]
1 = arg min

θ1

LΩ1(ˆu1) + β

(cid:16)

LD1(ˆu1) + LΓ(ˆu1)

(cid:17)

end for

end for
% Neumann Subproblem-Solving via Compensated Deep Ritz Method
for (cid:96) ← 0 to L (maximum number of training epochs) do

for m ← 0 to M (maximum number of training mini-batches) do

– draw mini-batch data uniformly at random from XΩ1, XΩ2, XD1, and XD2;
– network training by employing the compensated deep Ritz method (4.7), i.e.,

θ[k]
2 = arg min

θ2

LΩ2(ˆu2) + LN (ˆu2, ˆu[k]

1 ) + β

(cid:16)

LD1(ˆu2) + LD2(ˆu2)

(cid:17)

end for

end for
% Update of boundary values at interface Γ for Dirichlet subproblem
for n ← 1 to NΓ do

h[k+1](xΓ

n) = ρˆu2(xΓ

n; θ[k]

2 ) + (1 − ρ)h[k](xΓ
n)

end for
end while

end for

where the bilinear forms are deﬁned as

bi(ui, vi) =

(cid:90)

Ωi

ci∇ui · ∇vi dx,

(f, vi)i =

(cid:90)

Ωi

f vi dx,

and (q, v2)Γ =

(cid:90)

Γ

qv ds,

for i = 1, 2. By resorting to the variational form [10, 5] and parametrizing the trail functions
as neural networks, i.e., ui(x) ≈ ˆui(x; θi) for i = 1, 2, the learning task associated with the

16

Dirichlet problem† on Ω1 gives

(4.8)

θ[k]
1 = arg min

θ1

(cid:90)

Ω1

| − ∇ · (c1∇ˆu1) − f |2dx + β

(cid:18)(cid:90)

∂Ω1∩∂Ω

|ˆu1|2 ds +

(cid:19)

|ˆu1 − ˆu2|2 ds

,

(cid:90)

Γ

and that of the mixed Neumann-Dirichlet problem on Ω2 takes on the form
(4.9)
θ[k]
2 = arg min

|∇ˆu2|2−f ˆu2

1 ·∇ˆu2−f ˆu2

c1∇ˆu[k]

dx+

dx+

(cid:17)

(cid:17)

(cid:16)

(cid:90)

(cid:90)

(cid:90)

(cid:16) c2
2

θ2

Ω2

Ω1

qˆu2 ds+β

Γ

(cid:90)

∂Ω

|ˆu2|2 ds.

Therefore, an iterative learning approach for solving the elliptic interface problem with high-
contrast coeﬃcients can be immediately constructed from (4.8) and (4.9).

4.2. Neumann-Neumann Learning Algorithm. Similar in spirit, the compensated deep
Ritz method can be applied to construct the Neumann-Neumann learning algorithm (see
Figure 1). Using the same notations as before, the iterative Neumann-Neumann scheme
(see Algorithm 2.2) can be written in an equivalent variational form: given the initial guess
h[0] ∈ H

00(Γ), then solve for k ≥ 0 and i = 1, 2,

1
2

1) u[k]

i =

arg min
ui∈Vi, ui|Γ=h[k]

1
2

ai(ui, ui) − (f, ui)i,

2) ψ[k]

i = arg min

ψi∈Vi

1
2

ai(ψi, ψi)+(f, ψi)i +(f, R3−iγ0ψi)3−i −ai(u[k]
i

, ψi)−a3−i(u[k]

3−i, R3−iγ0ψi),

3) h[k+1] = h[k] − ρ(ψ[k]
with ρ ∈ (0, ρmax) being the acceleration parameter. Next, by parametrizing the trail functions
as neural networks, that is,

1 + ψ[k]
2 )

on Γ,

i (x) ≈ ˆu[k]
u[k]

i (x) := ˆui(x; θ[k]
i )

and ψ[k]

i (x) ≈ ˆψ[k]

i (x) := ˆψi(x; η[k]
i )

for i = 1, 2, and by employing the neural network extension operators

R1γ0 ˆψ2(x, η2) = ˆψ2(x, η2)

and R2γ0 ˆψ1(x, η1) = ˆψ1(x, η1),

the learning tasks associated with the Neumann-Neumann algorithm can be formulated as

θ[k]
i = arg min

θi

(cid:90)

Ωi

| − ∆ˆui − f |2dx + β

(cid:18)(cid:90)

|ˆui|2 ds +

(cid:19)

|ˆui − h[k]|2 ds

,

(cid:90)

Γ

∂Ωi∩∂Ω
(cid:90)

(cid:16)

Ω3−i

(cid:90)
η[k]
i = arg min

ηi

(cid:16) 1
2

Ωi

|∇ ˆψi|2 +f ˆψi −∇ˆu[k]
i

·∇ ˆψi

(cid:17)

dx+

f ˆψi −∇ˆu[k]

3−i ·∇ ˆψi

(cid:90)

(cid:17)

dx+β

| ˆψi|2ds,

∂Ω

where β > 0 is a user-deﬁned penalty coeﬃcient, i = 1, 2, and the training tasks associated
with Dirichlet subproblems are deﬁned in a residual form as before. Therefore, the iterative
learning approach can be constructed by using numerical integration formulas.

†Here, the residual form is used instead since the solution on each subregion is rather smooth, resulting in

more accurate approximation of the solution’s gradient inside the subdomain.

DOMAIN DECOMPOSITION LEARNING METHODS

17

00(Γ) and ψ[0]




:

∇u[k]
i



4.3. Dirichlet-Dirichlet Learning Algorithm. To build the Dirichlet-Dirichlet learning
algorithm in Figure 1, we ﬁrst rewrite the iterative process (see Algorithm 2.2) as: given the
initial guess h[0] ∈ H

2 = 0, then solve for k ≥ 0,

1 = ψ[0]

1
2

1) solve on Ωi for u[k]
i

i = f

−∆u[k]
u[k]
i = 0
· ni = h[k] − θ(∇ψ[k]

1 · n1 + ∇ψ[k]

2 · n2)

in Ωi,
on ∂Ω ∩ ∂Ωi,
on Γ,

2) solve on Ωi for ψ[k+1]

:

i

= 0
1 − u[k]
where i = 1, 2. Using the same notations as before, the Green’s theorem indicates that the
the variational formulation of the mixed Neumann-Dirichlet problem reads: for i = 1, 2,

= u[k]



2

= 0

−∆ψ[k+1]
i
ψ[k+1]
i
ψ[k+1]
i

in Ωi,
on ∂Ω ∩ ∂Ωi,
on Γ,

u[k]
i = arg min

ui∈Vi

1
2

ai(ui, ui) − (f, ui)i − (h[k], ui)Γ + ρ(cid:0)ai(ψ[k]

i

, ui) + a3−i(ψ[k]

3−i, R3−iγ0ui)(cid:1)

where (·, ·)Γ denotes the L2 inner product on Γ, ρ ∈ (0, ρmax), and the Dirichlet problem is
reformulated as

ψ[k]
i =

arg min
ψi∈Vi, ψi|Γ=u[k]

1 −u[k]

2

1
2

ai(ψi, ψi)

for i = 1, 2. Consequently, by parametrizing the trail functions as neural networks, i.e.,

i (x) ≈ ˆu[k]
u[k]

i (x) := ˆui(x; θ[k]
i )

and ψ[k]

i (x) ≈ ˆψ[k]

i (x) := ˆψi(x; η[k]
i )

for i = 1, 2, and by employing the neural network extension operators

R1γ0 ˆu2(x, θ2) = ˆu2(x, θ2)

and R2γ0 ˆu1(x, θ1) = ˆu1(x, θ1),

the learning tasks associated with the Dirichlet-Dirichlet algorithm can be formulated as

θ[k]
i = arg min

θi

(cid:90)

Ωi

(cid:16) 1
2

|∇ˆui|2 − f ˆui + ρ∇ ˆψ[k]
i

· ∇ˆui

(cid:90)

(cid:17)

dx + ρ

Ω3−i

∇ ˆψ[k]

3−i · ∇ˆui dx

h[k] ˆui ds + β

(cid:90)

|ˆui|2 ds,

(cid:90)

−

Γ

η[k]
i = arg min

ηi

(cid:90)

Ωi

|∆ ˆψi|2 dx + β

∂Ωi∩∂Ω

(cid:18)(cid:90)

∂Ωi∩∂Ω

| ˆψi|2 ds +

(cid:90)

Γ

| ˆψi − ˆu[k]

1 + ˆu[k]

2 |2 ds

(cid:19)

,

where β > 0 is a user-deﬁned penalty coeﬃcient, i = 1, 2, and the training tasks associated
with Dirichlet subproblems are deﬁned in a residual form as before.

18

4.4. Robin-Robin Learning Algorithm. As mentioned before, the Robin-Robin algorithm
only requires solution exchange between neighbouring subdomains, however, it may still suﬀer
from the problem of overﬁtted interface conditions. More speciﬁcally, let κ1 = 1 in what
follows, then a relatively large value of κ2 > 0 is typically required in order to reduce the
total number of outer iterations [8]. When the Robin boundary condition is incorporated as
the soft penalty term during training, it may cause weights imbalance between the solution
and its gradient on the interface, thereby making the trained model prone to overﬁtting at
and near the interfaces. To alleviate the issue of weights imbalance, the compensated deep
Ritz method is a promising alternative for realizing the deep learning analogue of the classical
Robin-Robin algorithm.

Note that in terms of diﬀerential operator, the second subproblem with κ2 (cid:29) κ1 = 1 in

the Robin-Robin algorithm [38] can be rewritten as

2 = f





−∆u[k]
u[k]
2 = 0
κ2u[k]

2 + ∇u[k]

2 · n2 = κ2u[k]

1 − ∇u[k]

1 · n1

in Ω2,
on ∂Ω ∩ ∂Ω2,
on Γ.

Using the same notations as before, it is equivalent to ﬁnd the weak solution u[k]
that

2 ∈ V2 such

(4.10)

a2(u[k]

2 , v2) = (f, v2)2 + (κ2(u[k]

1 − u[k]

2 ) − ∇u[k]

1 · n1, v2)Γ

for any v2 ∈ V2.

Next, by using the Green’s formula, we arrive at another form of (4.10), that is,

a2(u[k]

2 , v2) = (f, v2)2 + κ2(u[k]

2 , v2)Γ − a1(u[k]
and therefore the variational formulation of (4.10) due to the symmetry of bilinear forms

1 , R1γ0v2) + (f, R1γ0v2)1

1 − u[k]

for any v2 ∈ V2

u[k]
2 = arg min
u2∈V2

1
2

a2(u2, u2) − (f, u2)2 − κ2(u[k]

1 −

1
2

u2, u2)Γ + a1(u[k]

1 , R1γ0u2) − (f, R1γ0u2)1

which completely diﬀers from the original PINN approach (3.1).

As such, by parametrizing the trail functions as neural networks, i.e.,

1 (x) ≈ ˆu[k]
u[k]

1 (x) := ˆu1(x, θ[k]
1 )

and u[k]

2 (x) ≈ ˆu[k]

2 (x) := ˆu2(x, θ[k]
2 ),

and by employing the neural network extension operator

the learning task associated with the second Robin problem takes on the form:

R1γ0 ˆu2(x, θ2) = ˆu2(x, θ2),

θ[k]
2 = arg min

θ2

(cid:90)

Ω2

(cid:16) 1
2

|∇ˆu2|2 − f ˆu2

(cid:17)

dx − κ2

(cid:90)

(cid:16)

Γ

ˆu[k]
1 ˆu2 −

|ˆu2|2(cid:17)

ds

1
2

+

(cid:90)

(cid:16)

Ω1

∇ˆu[k]

1 · ∇ˆu2 − f ˆu2

(cid:17)

dx + β

(cid:90)

|ˆu2|2 ds,

∂Ω

DOMAIN DECOMPOSITION LEARNING METHODS

19

which obviously removes the issue of weights imbalance between the solution value and its
normal derivatives on the interface.

5. Numerical Experiments. To validate the eﬀectiveness of our proposed domain de-
composition learning algorithms, we conduct experiments using the Dirichlet-Neumann and
Robin-Robin learning algorithms on a wide range of elliptic boundary value problems in this
section, while the Neumann-Neumann and Dirichlet-Dirichlet learning algorithms are omit-
ted due to the limitation of pages. In what follows, our proposed Dirichlet-Neumann learning
method, i.e., algorithm 4.1, is abbreviated as DNLM for simplicity, with bracket indicating the
numerical solver adopted for solving the Dirichlet subproblems. In contrast to our proposed
methods, the existing learning approach [30] for realizing the classical Dirichlet-Neumann al-
gorithm is based on a direct substitution of the numerical solvers with the PINN approach,
which is referred to as DN-PINNs and is used for comparison. On the other hand, although
the update of interface conditions in the Robin-Robin algorithm does not directly depends
on the ﬂux exchange, it may still suﬀer from the weights imbalance and therefore the inter-
face overﬁtting. To further investigate the overﬁtting eﬀects, the Robin-Robin algorithm is
realized using the standard PINN approach and our compensated deep Ritz method after the
empirical study of DNLM, which is referred to as RR-PINNs, RRLM (PINN), and RRLM
(deep Ritz) in a similar fashion.

As is common for practical implementation [30, 23], the network architecture deployed
for each subregion is a fully connected network with 3 hidden layers of 50 neurons each [12],
while the hyperbolic tangent activation function is employed due to the smoothness of local
solutions and the diﬀerentiability of extension operators. During the training mode and for
i = 1, 2, we randomly sample NΩi = 20000 points from the interior domain Ωi, NΓ = 5000
points from the interface Γ, and ND = 5000 points from the boundary ∂Ωi \ Γ of length Γ
each. Then the trained models are evaluated the test dataset, i.e., NΩ = 10000 points that
are uniformly distributed over the entire computational domain, and compared with the true
solution to test their performance. The penalty coeﬃcient is set to be β = 400 and the number
of mini-batches is chosen as 5 for all training datasets. When executing the learning task on
each subdomain, the initial learning rate of Adam optimizer is set to be 0.1, which is divided
by 10 at the 600 and 800 epochs. The training process is terminated after 1000 epochs for
each decomposed subproblem, and the model with minimum training loss is chosen to execute
the subsequent operations. All the experiments are implemented using PyTorch and trained
on the NVIDIA GeForce RTX 2060.

5.1. Dirichlet-Neumann Learning Algorithm. As a representative benchmark, we con-
sider the learning approaches for realizing the classical Dirichlet-Neumann algorithm. More
precisely, a comparison study between DN-PINNs, DNLM (PINN), and DNLM (deep Ritz)
is presented in this subsection, where experiments on a wide variety of elliptic problems are
conducted to demonstrate the eﬀectiveness and ﬂexibility of our proposed methods.

5.1.1. Poisson’s Equation with Simple Interface. To begin with, we consider a bench-

mark Poisson problem in the two-dimensional case, that is,

(5.1)

−∆u(x, y) = 4π2 sin(2πx)(2 cos(2πy) − 1)
u(x, y) = 0

in Ω = (0, 1)2,
on ∂Ω,

20

where the exact solution is given by u(x, y) = sin(2πx)(cos(2πy) − 1), and the interface
Γ = ∂Ω1 ∩ ∂Ω2 is a straight line segment from (0.5, 0) to (0.5, 1) as depicted in Figure 3. It is
noteworthy that although the exact solution on each subregion is rather smooth, its normal
derivative on the interface is non-trivial that may bring diﬃculties for network training [1],
this diﬀers from the widely-used non-overlapping example that has trivial gradients on the
interface [30].

Figure 3: From left to right: decomposition of domain into two subregions, exact solution
u(x, y), and its partial derivatives ∂xu(x, y) for the numerical example (5.1).

Based on the conventional Dirichlet-Neumann algorithm [45, 38] that is deﬁned in terms
of diﬀerential operators, we ﬁrst conduct experiments using the most commonly used PINNs
[39, 23] as the numerical solver of all decomposed subproblems. We show in Figure 4a the
iterative numerical solutions over the entire computational domain in a typical simulation,
and in Figure 4b the corresponding pointwise absolute errors. Here, the initial guess for the
Dirichlet interface condition is set to be

h[0](x, y) = sin(2πx)(cos(2πy) − 1) − 50xy(x − 1)(y − 1)

on Γ,

which remains unchanged for the methods tested below. Unfortunately, DN-PINNs fails to
converge to the correct solution of (5.1) shown in Figure 4, since the trained networks are prone
to overﬁtting on the interface [9]. In other words, with the interface conditions being included
as extra soft constraints in the loss function and the size of training data on interface being
smaller than that of interior domains, the trained model using standard PINN approach [39]
suﬀers from the issue of boundary overﬁtting [9, 1], and therefore fails to be accurate enough
for the prediction of local ﬂuxes even if the training loss is very small. As a result, it would
hamper the convergence of outer iteration but is perhaps inevitable in practice for problems
with complex interface conditions. In fact, a straightforward replacement of the numerical
solvers by other learning strategies, e.g., the deep Ritz method [47], also has the same issue.
In contrast, although the predicted ﬂux through the network solution of Dirichlet subprob-
lem is usually of unacceptable low accuracy, our proposed method doesn’t need to explicitly
enforce the ﬂux continuity condition along the interface, thereby enabling the eﬀectiveness of
outer iteration in the presence of overﬁtted interface (see Figure 7). To validate our statements,
we show in Figure 5 and Figure 6 the computational results using our Dirichlet-Neumann
learning algorithm 4.1, where the PINN [39] and the deep Ritz method [47] are employed for

0101-1010101-10-50510DOMAIN DECOMPOSITION LEARNING METHODS

21

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 4: Numerical results of example (5.1) using the DN-PINNs on the test dataset.

solving the Dirichlet subproblem, respectively.

As can be observed from Figure 5 and Figure 6, the predicted solution using our learning
methods is in agreement with the true solution over the entire computational domain, while
the corresponding ﬁrst-order derivatives shown in Figure 7 indicate that the network solution
of Dirichlet subproblem rather learns to overﬁt at the interface. More quantitatively, we run
the simulations for 5 times to calculate the relative L2 errors, and the results (mean value
± standard deviation) are reported in Table 1 and Figure 8. By employing our proposed
compensated deep Ritz method for solving the mixed Neumann-Dirichlet subproblem, it is
obvious that our learning algorithms converge to the exact solution very well, while the DN-
PINNs is typically divergent due to the lack of accurate ﬂux estimation along the interface.
Moreover, as the solution of (5.1) is rather smooth on each subregion, it can be found in Table 1
and Figure 8 that the DNLM (PINN) performs better than the DNLM (deep Ritz). This is
because that the second-order derivatives are explicitly involved during training, leading to
better estimates of the solution’s gradient inside the computational domain (see Figure 7).

5.1.2. Poisson’s Equation with Zigzag Interface. To demonstrate the advantage of our
mesh-free approach over the traditional mesh-based numerical methods [45], we consider the
previous example but with a more complex interface geometry, namely,

(5.2)

−∆u(x, y) = 4π2 sin(2πy)(2 cos(2πx) − 1)
u(x, y) = 0

in Ω = (0, 1)2,
on ∂Ω,

22

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 5: Numerical results of example (5.1) using our DNLM (PINN) on the test dataset.

Table 1: Relative L2 errors of the predicted solution along the outer iteration k for example
(5.1), with mean value (± standard deviation) being reported over 5 runs.

Relative Errors

(cid:107)ˆu[k] − u(cid:107)L2(Ω)
(cid:107)u(cid:107)L2(Ω)

Outer Iterations

1

3

5

7

9

DN-PINNs

DNLM (PINN)

DNLM (Deep Ritz)

24.16
(± 48.04)
9.16
(± 2.34)
9.50
(± 2.46)

6.70
(± 12.10)
2.81
(± 0.56)
1.83
(± 0.86)

2.28
(± 2.70)
1.17
(± 0.71)
0.41
(± 0.35)

0.91
(± 0.52)
0.08
(± 0.07)
0.62
(± 0.55)

1.15
(± 0.49)
0.10
(± 0.04)
0.40
(± 0.49)

where the exact solution is deﬁned as u(x, y) = sin(2πy)(cos(2πx) − 1) and the interface is a
curved zigzag line as depicted in Figure 9. More precisely, the zigzag function reads

x = c(a(20y − ﬂoor(20y)) + b) + 0.5

where coeﬃcients a = 0.05(−1 + 2 × mod(ﬂoor(20y), 2)), b = −0.05 × mod(ﬂoor(20x), 2) and
c = −2 × mod(ﬂoor(10x), 2) + 1, thereby enabling the sample generation process inside each
subdomain and along its boundaries. Our proposed learning algorithm 4.1 can easily handle

DOMAIN DECOMPOSITION LEARNING METHODS

23

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 6: Numerical results of example (5.1) using our DNLM (deep Ritz) on the test dataset.

such irregular boundary shapes (see Figure 11 and Figure 12), while the traditional ﬁnite
diﬀerence or ﬁnite element method [5] calls for a computationally expensive mesh generation
procedure.

We ﬁrst conduct experiments using the DN-PINNs scheme [29] for solving (5.2), and the
numerical results in a typical simulation are depicted in Figure 10. Here, the initial guess is

h[0](x, y) = sin(2πx)(cos(2πy) − 1) − 1000 sin(2πx)2 sin(2πy)

on Γ,

which remains unchanged for other methods tested below. Similar as before, the DN-PINNs
scheme fails to converge to the exact solution shown in Figure 9, since the network solution of
Dirichlet subproblem learns to satisfy the constrained equations but for an overﬁtted interface
condition.

On the other hand, by replacing the numerical solver of Neumann subproblem with our
compensated deep Ritz method, the numerical results depicted in Figure 11‡ indicate that
our DNLM (PINN) approach can obtain a satisfactory approximation to the exact solution
of (5.2), which circumvents the meshing procedure that remains challenging for problems
with complex interfaces. Moreover, our learning approaches remain eﬀective in the presence
of interface overﬁtting (see Figure 13), and therefore is highly desirable in practice since
overﬁtting always occurs to a greater or lesser extent.

‡The ﬁrst ﬁgure in Figure 11a means that the current solution is still far away from the real one, which is

24

(a) Network solution ∂x ˆu[9]

1 (x, y) and its error |∂x ˆu[9]

1 (x, y) − ∂xu1(x, y)| using DNLM (PINN).

(b) Network solution ∂x ˆu[9]

1 (x, y) and its error |∂x ˆu[9]

1 (x, y) − ∂xu1(x, y)| using DNLM (deep Ritz).

Figure 7: Overﬁtting phenomenon in solving the Dirichlet subproblem of (5.1) on testdata.

Figure 8: Relative L2 errors on testdata along outer iterations for example (5.1).

Note that when the deep Ritz method [47] is adopted for solving the Dirichlet subproblem,
the accuracy of approximate gradients inside the computational domain is no longer compa-
rable to that of PINN approach [7]. The situation is susceptible to become even worse for
irregular domains, and therefore the DNLM (deep Ritz) may fail to be accurate enough as

not restated in what follows.

DOMAIN DECOMPOSITION LEARNING METHODS

25

Figure 9: From left to right: decomposition of domain into two subregions, exact solution
u(x, y), and its partial derivatives ∂xu(x, y), ∂yu(x, y) for the numerical example (5.2).

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 10: Numerical results of example (5.1) using the DN-PINNs on the test dataset.

shown in Figure 12. To further validate our statements, we show in Table 2 and Figure 14 the
quantitative results over 5 runs, where DNLM (PINN) outperforms DN-PINNs and DNLM
(deep Ritz) in terms of accuracy.

5.1.3. Poisson’s Equation with Four Subdomains. Next, we consider the Poisson prob-

lem divided into four subproblems in two-dimension, that is,

(5.3)

−∆u(x, y) = f (x, y)
u(x, y) = 0

in Ω = (0, 1)2,
on ∂Ω,

0101-1010101-10-505100101-50526

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.§

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 11: Numerical results of example (5.2) using our DNLM (PINN) on the test dataset.

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 12: Numerical results of example (5.2) using our DNLM (deep Ritz) on the test dataset.

DOMAIN DECOMPOSITION LEARNING METHODS

27

(a) Network solutions ∂x ˆu[4]

1 , ∂y ˆu[4]

1 and errors |∂x ˆu[4]

1 − ∂xu1|, |∂y ˆu[4]

1 − ∂yu1| using DNLM (PINN).

(b) Network solutions ∂x ˆu[9]

1 , ∂y ˆu[9]

1 and errors |∂x ˆu[9]

1 −∂xu1|, |∂y ˆu[9]

1 − ∂yu1| using DNLM (deep Ritz).

Figure 13: Overﬁtting phenomenon in solving the Dirichlet subproblem of (5.2) on testdata.

Table 2: Relative L2 errors of the predicted solution along the outer iteration k for example
(5.2), with mean value (± standard deviation) being reported over 5 runs.

Relative Errors

(cid:107)ˆu[k] − u(cid:107)L2(Ω)
(cid:107)u(cid:107)L2(Ω)

Outer Iterations

1

2

3

4

5

DN-PINNs

DNLM (PINN)

DNLM (Deep Ritz)

3.12
(± 1.18)
1.49
(± 0.43)
1.64
(± 0.45)

1.92
(± 0.95)
0.95
(± 0.39)
1.37
(± 0.14)

1.69
(± 0.58)
0.30
(± 0.28)
1.39
(± 0.08)

1.77
(± 0.32)
0.27
(± 0.26)
1.40
(± 0.02)

1.70
(± 0.19)
0.38
(± 0.36)
1.40
(± 0.01)

where true solution u(x, y) = sin(2πx)(cos(2πy) − 1) + 100xy(x − 1)2(y − 1)2, and f (x, y) =
4π2 sin(2πx)(2 cos(2πy) − 1) − 200x(x − 1)2(3y − 2) − 200y(y − 1)2(3x − 2). Here the domain is
decomposed using the red-black partition [45], and the multidomains are categorized into two
sets [45] as depicted in Figure 15. Then, the learning algorithms of interest can be deployed,
and the initial guess of Dirichlet data along the interface is chosen as

h[0](x, y) = u(x, y) − 50x(x − 1)y

on Γ.

28

Figure 14: Relative L2 errors on testdata along outer iterations for example (5.2).

Figure 15: From left to right: decomposition of domain into two subregions, exact solution
u(x, y) and its partial derivatives ∂xu(x, y), ∂yu(x, y) for numerical example (5.3).

For the more general multidomain problem (5.3) with non-trivial ﬂux functions, the nu-
merical results using DN-PINNs are depicted in Figure 16, which is not guaranteed to converge
to the true solution due to the issue of interface overﬁtting.

Note that the overﬁtting phenomenon on subdomain interfaces remains unsettled when
using our methods (see Figure 19). However, thanks to the compensated deep Ritz method, it
can be observed from Figure 17 and Figure 18 that the outer iterations using DNLM (PINN)
and DNLM (deep Ritz) have converged, which validates the eﬀectiveness of our proposed
methods in dealing with the inevitable overﬁtting problem in practice. Moreover, we execute
the simulation for 5 runs and report the statistical results in Table 3 and Figure 20 to further
demonstrate that DNLM (PINN) can outperform other methods in terms of accuracy.

5.1.4. Poisson’s Equation in High Dimension. As is well known, another key and desir-
able advantage of using deep learning solvers is that it can tackle diﬃculties induced by the

01010120101-10-5050101-50510DOMAIN DECOMPOSITION LEARNING METHODS

29

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 16: Numerical results of example (5.3) using the DN-PINNs on the test dataset.

Table 3: Relative L2 errors of the predicted solution along the outer iteration k for example
(5.3), with mean value (± standard deviation) being reported over 5 runs.

Relative Errors

(cid:107)ˆu[k] − u(cid:107)L2(Ω)
(cid:107)u(cid:107)L2(Ω)

Outer Iterations

1

2

3

4

6

DN-PINNs

DNLM (PINN)

DNLM (Deep Ritz)

2.40
(± 0.13)
1.68
(± 0.16)
1.97
(± 0.47)

1.49
(± 0.22)
1.30
(± 0.24)
1.38
(± 0.29)

0.82
(± 0.13)
0.88
(± 0.26)
0.90
(± 0.29)

0.91
(± 0.52)
0.35
(± 0.11)
0.95
(± 0.63)

0.58
(± 0.18)
0.29
(± 0.28)
0.42
(± 0.26)

curse of dimensionality. To this end, we consider a Poisson problem in ﬁve dimension, i.e.,

(5.4)

−∆u(x1, · · · , x5) = 4π2

5
(cid:88)

i=1

sin(xi)

in Ω = (0, 1)5,

u(x1, · · · , x5) = 0

on ∂Ω,

30

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 17: Numerical results of example (5.3) using our DNLM (PINN) on the test dataset.

where the exact solution is given by u(x1, · · · , x5) =

into two subregions

5
(cid:80)
i=1

sin(xi), and the domain is decomposed

Ω1 = (cid:8)(x1, · · · , x5) ∈ Ω (cid:12)

(cid:12) x1 < 0.5(cid:9) and Ω2 = (cid:8)(x1, · · · , x5) ∈ Ω (cid:12)

(cid:12) x1 > 0.5(cid:9).

Here, the initial guess of the Dirichlet data at interface is chosen as

h[0](x) = −5000

5
(cid:88)

5
(cid:89)

i=1

i=1

xi(xi − 1),

and the fully-connected neural network employed here has 4 hidden layers of 50 neurons
each. The computational results using DN-PINNs, DNLM (PINN), and DNLM (Deep Ritz)
approaches are shown in Table 4 and Figure 21, which implies that our proposed learning
algorithms can achieve comparable performance to the existing learning methods.

5.1.5. High-Contrast Elliptic Equation. Note that as mentioned in remark 4.1, our pro-
posed Dirichlet-Neumann learning algorithm 4.1 can also be easily extended to solving the
more complicated interface problems with high-contrast coeﬃcients. As such, we consider the

DOMAIN DECOMPOSITION LEARNING METHODS

31

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 18: Numerical results of example (5.3) using our DNLM (deep Ritz) on the test dataset.

(a) Network solutions ∂x ˆu[6]

R , ∂y ˆu[6]

R and errors |∂x ˆu[6]

R − ∂xuR|, |∂y ˆu[6]

R − ∂yuR| using DNLM (PINN).

(b) Network solutions ∂x ˆu[7]

R , ∂y ˆu[7]

R and errors |∂x ˆu[7]

R −∂xuR|, |∂y ˆu[7]

R −∂yuR| using DNLM (deep Ritz).

Figure 19: Overﬁtting phenomenon in solving the Dirichlet subproblem of (5.3) on testdata.

32

Figure 20: Relative L2 errors on testdata along outer iterations for example (5.3).

Table 4: Relative L2 errors of the predicted solution along the outer iteration k for example
(5.4), with mean value (± standard deviation) being reported over 5 runs.

Relative Errors

(cid:107)ˆu[k] − u(cid:107)L2(Ω)
(cid:107)u(cid:107)L2(Ω)

Outer Iterations

1

3

5

7

DN-PINNs

DNLM (PINN)

DNLM (Deep Ritz)

0.77
(± 0.38)
0.48
(± 0.03)
0.54
(± 0.80)

0.08
(± 0.12)
0.04
(± 0.01)
0.11
(± 0.11)

0.01
(± 0.06)
0.05
(± 0.02)
0.05
(± 0.02)

0.02
(± 0.03)
0.05
(± 0.02)
0.04
(± 0.02)

Figure 21: Relative L2 errors on testdata along outer iterations for example (5.4).

DOMAIN DECOMPOSITION LEARNING METHODS

33

an elliptic interface problem in two dimension, that is,

(5.5)

−∇ · (c(x, y)∇u(x, y)) = 32π2 sin(4πx) cos(4πy)
u(x, y) = 0

in Ω = (0, 1)2,
on ∂Ω,

where the domain is decomposed into four subregions using the red-black partition (see Fig-
ure 22), the exact solution is given by u(x, y) = sin(4πx) sin(4πy)/c(x, y), and the coeﬃcient
c(x, y) is piecewise constant with respect to the partition of domain

c(x, y) =

(cid:26) 1
100

in ΩR,
in ΩB.

Figure 22: From left to right: decomposition of domain into two subregions, exact solution
u(x, y) and its partial derivatives ∂xu(x, y), ∂yu(x, y) for numerical example (5.5).

Here, we choose h[0] = 100 cos(100πx) cos(100πy)+100xy(x−1)3(y−1)3 as the initial guess
on the interface, and the computational results using DN-PINNs, DNLM (PINN), and DNLM
(deep Ritz) are depicted in Figure 23, Figure 24, and Figure 25, respectively. Clearly, our
proposed learning methods can facilitate the convergence of outer iteration with guaranteed
accuracy than the straightforward DN-PINNs approach, which shows the potential to solve
more complicated interface problems in the presence of interface overﬁtting (see Figure 26).
Moreover, the statistical results over 5 runs are reported in Table 5 and Figure 27, which
further validate the eﬀectiveness of our proposed learning methods for solving the elliptic
interface problems.

5.2. Robin-Robin Learning method. To demonstrate the eﬀectiveness and eﬃciency of
our compensated deep Ritz method for realizing the classical Robin-Robin algorithm, we
consider the following Poisson equation in two-dimension

(5.6)

−∆u(x, y) = 4π2 sin(2πx)(2 cos(2πy) − 1)
u(x, y) = 0

in Ω = (0, 1)2,
on ∂Ω,

where the exact solution u(x, y) = sin(2πx)(cos(2πy) − 1), and the interface Γ = ∂Ω1 ∩ ∂Ω2
is a straight line segment from (0.5, 0) to (0.5, 1) as depicted in Figure 3.

By choosing (κ1, κ2) = (1, 0.01), the computational results using RR-PINNs, i.e., Al-
gorithm 3.1, in a typical simulation is depicted Figure 28, which can converge to the true

0101-0.500.50101-10-505100101-10-5051034

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 23: Numerical results of example (5.5) using the DN-PINNs on the test dataset.

Table 5: Relative L2 errors of the predicted solution along the outer iteration k for example
(5.5), with mean value (± standard deviation) being reported over 5 runs.

Relative Errors

(cid:107)ˆu[k] − u(cid:107)L2(Ω)
(cid:107)u(cid:107)L2(Ω)

Outer Iterations

1

3

5

6

7

DN-PINNs

DNLM (PINN)

DNLM (Deep Ritz)

1.90
(± 1.03)
62.46
(± 56.64)
81.56
(± 48.50)

1.17
(± 0.34)
0.92
(± 0.31)
0.98
(± 0.14)

13.75
(± 22.70)
0.65
(± 0.49)
2.94
(± 4.41)

20.19
(± 33.04)
0.83
(± 0.40)
4.72
(± 8.80)

2.47
(± 2.45)
0.51
(± 0.54)
0.82
(± 0.42)

solution but requires more outer iterations when compared to the DNLM (PINN) or DNLM
(deep Ritz) approach (see Figure 5 or Figure 6).

To further enhance the speed of outer convergence, these two additional parameters are set
to be (κ1, κ2) = (1, 1000) in what follows. Unfortunately, due to the weights imbalance, the
RR-PINNs approach could not work under such a circumstance (see Figure 30). On the other
hand, the integration with our compensated deep Ritz method (see Figure 29 and Figure 31)
is convergent in the presence of interface overﬁtting, which only requires the substitution of

DOMAIN DECOMPOSITION LEARNING METHODS

35

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 24: Numerical results of example (5.5) using our DNLM (PINN) on the test dataset.

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 25: Numerical results of example (5.5) using our DNLM (deep Ritz) on the test dataset.

36

(a) Network solutions ∂x ˆu[7]

R , ∂y ˆu[7]

R and errors |∂x ˆu[7]

R − ∂xuR|, |∂y ˆu[7]

R − ∂yuR| using DNLM (PINN).

(b) Network solutions ∂x ˆu[7]

R , ∂y ˆu[7]

R and errors |∂x ˆu[7]

R −∂xuR|, |∂y ˆu[7]

R −∂yuR| using DNLM (deep Ritz).

Figure 26: Overﬁtting phenomenon in solving the Dirichlet subproblem of (5.5) on testdata.

Figure 27: Relative L2 errors on testdata along outer iterations for example (5.4).

second subproblem with our proposed learning approach.

6. Conclusion. In this paper, a general framework is proposed for realizing the classical
domain decomposition methods through deep learning approaches, which is based on the infor-
mation exchange between neighbouring subregions rather than the domain partition strategies.
For the methods that are based on a direct ﬂux exchange, a key diﬃculty of deploying the deep

DOMAIN DECOMPOSITION LEARNING METHODS

37

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 28: Numerical results of example (5.6) using the RR-PINNs on the test dataset.

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 29: Numerical results of example (5.6) using the RR-PINNs on the test dataset.

38

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 30: Numerical results of example (5.6) using our RRLM (PINN) on the test dataset.

(a) The numerical solutions ˆu[k](x, y) along the outer iterations.

(b) The pointwise absolute errors |ˆu[k](x, y) − u(x, y)| along the outer iterations.

Figure 31: Numerical results of example (5.6) using our RRLM (deep Ritz) on the test dataset.

DOMAIN DECOMPOSITION LEARNING METHODS

39

learning solvers is the issue of interface overﬁtting that will always occur to a greater or lesser
extent in practice. To deal with the overﬁtted interface conditions, we develop a novel learning
approach, i.e., the compensated deep Ritz method, that enables the ﬂux transmission across
subdomain interfaces with guaranteed accuracy. As a result, it allows us to construct eﬀective
learning approaches for realizing the classical Dirichlet-Neumann, Neumann-Neumann, and
Dirichlet-Dirichlet algorithms, and therefore fully leverage the advantages of deep learning
solvers to deal with the complicated geometry domains and high-dimensional problems. On
the other hand, the Robin-Robin algorithm, which does not require the ﬂux exchange but
may suﬀer from the issue of weights imbalance, can also beneﬁt from our compensated deep
Ritz method. Finally, we conduct numerical experiments on a series of elliptic boundary value
problems to demonstrate the eﬀectiveness of our proposed learning algorithms. Possible future
explorations would involve the coarse space acceleration [35], the adaptive sampling technique
[14], and the improvement of network architecture that could potentially further accelerate
the convergence at a reduced cost.

40

REFERENCES

[1] C. Bajaj, L. McLennan, T. Andeen, and A. Roy, Robust learning of physics informed neural networks,

arXiv preprint arXiv:2110.13330, (2021).

[2] T. Ben-Nun and T. Hoefler, Demystifying parallel and distributed deep learning: An in-depth concur-

rency analysis, ACM Computing Surveys (CSUR), 52 (2019), pp. 1–43.

[3] J. Berg and K. Nystr¨om, A uniﬁed deep artiﬁcial neural network approach to partial diﬀerential

equations in complex geometries, Neurocomputing, 317 (2018), pp. 28–41.

[4] L. Bottou, F. E. Curtis, and J. Nocedal, Optimization methods for large-scale machine learning,

SIAM Review, 60 (2018), pp. 223–311.

[5] S. C. Brenner, L. R. Scott, and L. R. Scott, The Mathematical Theory of Finite Element Methods,

vol. 3, Springer, 2008.

[6] S. Brunton, B. Noack, and P. Koumoutsakos, Machine learning for ﬂuid mechanics, arXiv preprint

arXiv:1905.11075, (2019).

[7] J. Chen, R. Du, and K. Wu, A comparison study of deep Galerkin method and deep Ritz method for
elliptic problems with diﬀerent boundary conditions, arXiv preprint arXiv:2005.04554, (2020).
[8] W. Chen, X. Xu, and S. Zhang, On the optimal convergence rate of a Robin-Robin domain decompo-

sition method, Journal of Computational Mathematics, (2014), pp. 456–475.

[9] T. Dockhorn, A discussion on solving partial diﬀerential equations using neural networks, arXiv preprint

arXiv:1904.07200, (2019).

[10] L. C. Evans, Partial Diﬀerential Equations, vol. 19, American Mathematical Society, 2010.
[11] D. Funaro, A. Quarteroni, and P. Zanolli, An iterative procedure with interface relaxation for

domain decomposition methods, SIAM Journal on Numerical Analysis, 25 (1988), pp. 1213–1236.

[12] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning, MIT Press, 2016.
[13] J. Han, A. Jentzen, et al., Deep learning-based numerical methods for high-dimensional parabolic par-
tial diﬀerential equations and backward stochastic diﬀerential equations, Communications in Mathe-
matics and Statistics, 5 (2017), pp. 349–380.

[14] C. He, X. Hu, and L. Mu, A mesh-free method using piecewise deep neural network for elliptic interface

problems, Journal of Computational and Applied Mathematics, 412 (2022), pp. 114–358.

[15] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of

the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.

[16] R. Hecht-Nielsen, Theory of the backpropagation neural network, in Neural networks for perception,

Elsevier, 1992, pp. 65–93.

[17] A. Heinlein, A. Klawonn, M. Lanser, and J. Weber, Combining machine learning and domain de-
composition methods for the solution of partial diﬀerential equations—A review, GAMM-Mitteilungen,
44 (2021), p. e202100001.

[18] K. Hornik, M. Stinchcombe, and H. White, Multilayer feedforward networks are universal approxi-

mators, Neural Networks, 2 (1989), pp. 359–366.

[19] Z. Hu, A. D. Jagtap, G. E. Karniadakis, and K. Kawaguchi, When do extended physics-informed
neural networks (XPINNs) improve generalization?, arXiv preprint arXiv:2109.09444, (2021).
[20] J. Huang, H. Wang, and T. Zhou, An augmented Lagrangian deep learning method for variational

problems with essential boundary conditions, arXiv preprint arXiv:2106.14348, (2021).

[21] A. D. Jagtap and G. E. Karniadakis, Extended physics-informed neural networks (XPINNs): A
generalized space-time domain decomposition based deep learning framework for nonlinear partial dif-
ferential equations, Communications in Computational Physics, 28 (2020), pp. 2002–2041.
[22] A. D. Jagtap, E. Kharazmi, and G. E. Karniadakis, Conservative physics-informed neural networks
on discrete domains for conservation laws: Applications to forward and inverse problems, Computer
Methods in Applied Mechanics and Engineering, 365 (2020), pp. 113–028.

[23] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang, Physics-

informed machine learning, Nature Reviews Physics, 3 (2021), pp. 422–440.

[24] I. E. Lagaris, A. Likas, and D. I. Fotiadis, Artiﬁcial neural networks for solving ordinary and partial

diﬀerential equations, IEEE Transactions on Neural Networks, 9 (1998), pp. 987–1000.

[25] I. E. Lagaris, A. C. Likas, and D. G. Papageorgiou, Neural-network methods for boundary value
problems with irregular boundaries, IEEE Transactions on Neural Networks, 11 (2000), pp. 1041–1049.

DOMAIN DECOMPOSITION LEARNING METHODS

41

[26] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature, 521 (2015), pp. 436–444.
[27] R. J. LeVeque, Finite Diﬀerence Methods for Ordinary and Partial Diﬀerential Equations: Steady-State

and Time-Dependent Problems, SIAM, 2007.

[28] R. J. LeVeque et al., Finite Volume Methods for Hyperbolic Problems, vol. 31, Cambridge University

Press, 2002.

[29] K. Li, K. Tang, T. Wu, and Q. Liao, D3M: A deep domain decomposition method for partial diﬀerential

equations, IEEE Access, 8 (2019), pp. 5283–5294.

[30] W. Li, X. Xiang, and Y. Xu, Deep domain decomposition method: Elliptic problems, in Mathematical

and Scientiﬁc Machine Learning, PMLR, 2020, pp. 269–286.

[31] Z. Li and K. Ito, The Immersed Interface Method: Numerical Solutions of PDEs Involving Interfaces

and Irregular Domains, SIAM, 2006.

[32] Y. Liao and P. Ming, Deep Nitsche method: Deep Ritz method with essential boundary conditions, arXiv

preprint arXiv:1912.01309, (2019).

[33] T. Mathew, Domain Decomposition Methods for the Numerical Solution of Partial Diﬀerential Equa-

tions, vol. 61, Springer Science & Business Media, 2008.

[34] K. S. McFall and J. R. Mahan, Artiﬁcial neural network method for solution of boundary value prob-
lems with exact satisfaction of arbitrary boundary conditions, IEEE Transactions on Neural Networks,
20 (2009), pp. 1221–1233.

[35] V. Mercier, S. Gratton, and P. Boudier, A coarse space acceleration of deep-DDM, arXiv preprint

arXiv:2112.03732, (2021).

[36] N. Metropolis and S. Ulam, The Monte Carlo method, Journal of the American Statistical Association,

44 (1949), pp. 335–341.

[37] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison,

L. Antiga, and A. Lerer, Automatic diﬀerentiation in PyTorch, (2017).

[38] A. Quarteroni and A. Valli, Domain Decomposition Methods for Partial Diﬀerential Equations,

no. BOOK, Oxford University Press, 1999.

[39] M. Raissi, P. Perdikaris, and G. E. Karniadakis, Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial diﬀerential equations,
Journal of Computational physics, 378 (2019), pp. 686–707.

[40] F. Scarselli and A. C. Tsoi, Universal approximation using feedforward neural networks: A survey of

some existing methods, and some new results, Neural Networks, 11 (1998), pp. 15–37.

[41] H. Schwarz, Gesammelte mathematische abhandlungen. Vierteljahrss-chrift der naturforschenden ges-

selschaft in Zurich, republished by Springer-Verlag, 1870.

[42] H. Sheng and C. Yang, PFNN-2: A domain decomposed penalty-free neural network method for solving

partial diﬀerential equations, arXiv preprint arXiv:2205.00593, (2022).

[43] J. Sirignano and K. Spiliopoulos, DGM: A deep learning algorithm for solving partial diﬀerential

equations, Journal of Computational Physics, 375 (2018), pp. 1339–1364.

[44] A. Taghibakhshi, N. Nytko, T. Zaman, S. MacLachlan, L. Olson, and M. West, Learning

interface conditions in domain decomposition solvers, arXiv preprint arXiv:2205.09833, (2022).

[45] A. Toselli and O. Widlund, Domain Decomposition Methods: Algorithms and Theory, vol. 34, Springer

Science & Business Media, 2004.

[46] C. L. Wight and J. Zhao, Solving Allen-Cahn and Cahn-Hilliard equations using the adaptive physics

informed neural networks, arXiv preprint arXiv:2007.04542, (2020).

[47] B. Yu et al., The deep Ritz method: a deep learning-based numerical algorithm for solving variational

problems, Communications in Mathematics and Statistics, 6 (2018), pp. 1–12.

[48] Y. Zang, G. Bao, X. Ye, and H. Zhou, Weak adversarial networks for high-dimensional partial dif-

ferential equations, Journal of Computational Physics, 411 (2020), pp. 109–409.

