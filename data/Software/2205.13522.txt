JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Dynamically Relative Position Encoding-Based
Transformer for Automatic Code Edit

Shiyi Qi, Yaoxian Li, Cuiyun Gao, Xiaohong Su, Shuzheng Gao, Zibin Zheng, and Chuanyi Liu

2
2
0
2

l
u
J

1
3

]
E
S
.
s
c
[

3
v
2
2
5
3
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—Adapting Deep Learning (DL) techniques to auto-
mate non-trivial coding activities, such as code documentation
and defect detection, has been intensively studied recently. Learn-
ing to predict code changes is one of the popular and essential
investigations. Prior studies have shown that DL techniques such
as Neural Machine Translation (NMT) can beneﬁt meaningful
code changes, including bug ﬁxing and code refactoring. However,
NMT models may encounter bottleneck when modeling long
sequences, thus are limited in accurately predicting code changes.
In this work, we design a Transformer-based approach, consid-
ering that Transformer has proven effective in capturing long-
term dependencies. Speciﬁcally, we propose a novel model named
DTrans. For better incorporating the local structure of code, i.e.,
statement-level information in this paper, DTrans is designed
with dynamically relative position encoding in the multi-head
attention of Transformer. Experiments on benchmark datasets
demonstrate that DTrans can more accurately generate patches
than the state-of-the-art methods, increasing the performance
by at least 5.45%-46.57% in terms of the exact match metric
on different datasets. Moreover, DTrans can locate the lines to
change with 1.75%-24.21% higher accuracy than the existing
methods.

Index Terms—Code edit, Transformer, position encoding

I. INTRODUCTION

Deep Learning (DL) techniques have been adapted to solve
many traditional software engineering problems and tasks
localization [3]–[5], automatic
recently [1], [2], e.g., fault
program repair [6]–[8], code summarization [9]–[11], code
prediction [12], [13], and defect prediction [14]–[16]. Among
these ﬁelds, learning from code for code change prediction
draws more and more research investigations [17], [18]. Pre-
cisely editing code can signiﬁcantly facilitate the software
maintenance process for developers [19], [20].

In the process of program development and maintenance,
developers usually need to modify the source code for various
reasons, including program repair [21], [22], code refactor-
ing [23]–[25]and API-related changes [26], [27], etc. Such
behavior is known as “code edit” or “code change” [19],
[20]. Prior research
[19], [20], [28], [29] discovers that
code edits generally follow repetitive edit patterns and can

S.Qi, Y.Li, C. Gao, X. Su, S. Gao, C. Liu were with Harbin
syqi981125@163.com,
sxh@hit.edu.cn,

Institute
of
yaoxian0803@icloud.com,
szgao98@gmail.com, liuchuanyi@hit.edu.cn).

China
gaocuiyun@hit.edu.cn,

Technology,

(e-mail:

C. Gao and C. Liu were also with Peng Cheng Laboratory and Guangdong
Provincial Key Laboratory of Novel Security Intelligence Technologies,
China.

Z. Zheng was with Sun Yat-sen University, China

(email:

zib-

inzheng2@yeah.net).

C. Gao and C. Liu are the corresponding authors.
Manuscript received April 19, 2005; revised August 26, 2015.

be employed to automatically generate targeted code based on
original code. Figure 1 shows two examples for illustrating
the code edit task. In the original code of Figure 1 (a), the
method testEmpty needs to return the object’s ID and name.
However, the functions id() and name() do not exist for
the object, which leads to a program bug. The correct code
edit operation is to generate a correct patch for ﬁxing the bug,
i.e, changing to the corresponding correct functions getId()
and getName(), respectively. For the example in Figure 1(b),
the parameter name is changed from type to method for
enhancing the readability of the code. The code edit task aims
at generating the edited code given the original code [19]. Due
to the complex code edit patterns, automatically identifying the
lines for editing and producing accurate edits are challenging.
In recent years, deep learning has made great progress and
been applied to many code-related tasks [30], [31]. The large
software engineering datasets, such as GitHub which includes
over 100 million repositories with over 200 million merged
pull requests (PRs) and 2 billion commits [19], provide us
with sufﬁcient source code for training DL models. Prior
studies [19], [29], [32] have shown that DL techniques such
as Neural Machine Translation (NMT) [33] can automatically
apply developers’ pull request code to generate meaningful
code changes. NMT models treat pull request code as a
series of tokens or use the parsed tree structure as input,
then creating an intermediate representation with an encoder
network and decoding the representation into target sequence
with a decoder network [9], [34]. This mechanism makes
NMT models can learn complex code change patterns between
input and output sequences [20]. However, NMT models
have proven ineffective in modeling long sequences [35], thus
may be limited in accurately editting code. Considering that
Transformer [36]–[38] has shown more effective than NMT in
modeling long sequences, it is more applicable for the task.
But directly adopting Transformer still cannot well capture
the structural dependencies between tokens [9], [39]. Thus, to
mitigate the issue of NMT models and better capture the code
dependencies, we propose a novel Transformer-based model,
named as DTrans.

Prior research [20] extracts the AST of original code for
capturing the structural information. In this work, to alleviate
the complexity caused by the AST extraction, we focus on
exploiting the local structure, i.e., the statement-level informa-
tion, which can be easily obtained without parsing. Besides,
for the code editing task, the changes generally happen within
several statements, indicating the importance of local structure
[20], [29]. Speciﬁcally, we propose a dynamically relative po-
sition encoding strategy to integrate the variational statement-

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

(a) Example 1.

(b) Example 2.

Fig. 1: Examples for illustrating the code edit task. The grey blocks highlight the changed parts in the code.

level information. Different from the Transformer [36] and
Transformer with relative position [40], which represent po-
sition embedding by absolute position and relative position
respectively, DTrans conducts positional encoding guided by
statement information.

To evaluate the performance of our proposed DTrans model,
we choose three pull request repositories utilized by the
work [19] as benchmark datasets, including Android [41],
Google Source [42], and Ovirt [43]. Besides, we also involve
the 2.3 million 121,895 pair-wise code changes from GitHub
open-source projects [32]. During evaluation, we group project
datasets to two levels,
i.e., small and medium levels, ac-
cording to the token numbers of original code following
prior studies [29], [32], [44]. Experiments demonstrate that
DTrans accurately predicts more code edits in both small-level
and medium-level projects, increasing the performance of the
best baseline [29] by at least 5.45% and 25.76% in terms
of the exact match metric, respectively. Moreover, DTrans
successfully locates the lines to change with 1.75%-24.21%
higher accuracy than the best baseline.

Overall, we make the following contributions:
• A novel Transformer-based approach is proposed to in-
corporate dynamically relative position encoding strategy
in the multi-head attention of Transformer, which explic-
itly incorporates the statement-level syntactic information
for better capturing the local structure of code.

• We evaluate our approach on benchmark datasets, and
the results demonstrate the effectiveness of DTrans in
predicting accurate code changes.

Paper structure. We introduce the background in Sec-
tion II. The proposed approach is illustrated in Section III.
Experimental setup and results are depicted in Section IV and
Section V, respectively. We show some cases in Section VI.
The threats to validity and related work are introduced in
Section VII and Section VIII, respectively. We conclude our

work in Section IX.

II. BACKGROUND

In this section, we ﬁrst formulate the code change prediction

task and then introduce the basic approach - Transformer.

A. Deep learning (DL) in Code Change Prediction

DL-based techniques aim at learning the mapping relations
between the original code and the target code by training, and
generating edited code for facilitating software development
[29], [45], [46]. Programming languages can be treated as
sequences of code tokens. Therefore, the problem of code
change prediction can be tackled as a neural machine transla-
tion problem [20], [32], that is, to “translate” from a sequence
of code tokens (the original code) to another sequence of code
tokens (the target code).

We take a a sequence of original code O as an example,

and let

O = (o1, o2, ..., oi, ...om),

where each oi
sequence O corresponds to a target code C, denoted as:

is the i-th token in the code. Each input

C = (c1, c2, ..., cn),

where m and n indicate the lengths of the original and target
sequences, respectively. Our goal is to learn the conditional
distribution and generate changed code sequence by maximiz-
ing the conditional likelihood:

C = arg max

P (C|O).

C

Finally, we achieve an optimized target sequence as the
predicted code change.

Public void testEmpty() {   org.ovirt.engine.api.types.V4Vm object = objectFromJson("{}");   org.junit.Assert.assertNotNull(object);   org.junit.Assert.assertNull(object.getId());   org.junit.Assert.assertNull(object.getName()); }Public void testEmpty(){   org.ovirt.engine.api.types.V4Vm object = objectFromJson("{}");   org.junit.Assert.assertNotNull(object);   org.junit.Assert.assertNull(object.id());   org.junit.Assert.assertNull(object.name());}    Original codeTargeted codepublic void endTrace(@Nonnull JMethod method) {composedStatus.pop();for (TracerBrush config : brushes) {config.endTrace(method);}}public void endTrace(@Nonnull JMethod type) {composedStatus.pop();for (TracerBrush config : brushes) {config.endTrace(type);}} Original codeTargeted codeJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Fig. 2: Framework of the proposed DTrans. The statement mask matrix takes the code snippet shown in Figure 5 as example.

B. Transformer

Transformer employs the typical encoder-decoder struc-
ture [44], and is composed of stacked Transformer blocks.
Each block contains a multi-head self-attention sub-layer
followed by a fully connected positional-wise feed-forward
network sub-layer. The sub-layers are connected by residual
connections [47] and layer normalization [48]. In addition,
Transformer augments the input features by adding a positional
embedding since the self-attention mechanism lacks a natural
way to encode the word position information. Transformer also
applies pad masking to resolve the problem of variable input
lengths and its decoder uses sequence masking in its self-
attention to ensure that the predictions for the i-th position
can only depend on the known outputs at positions less
than i. We introduce the major components of Transformer,
including multi-head self-attention, position-wise feed-forward
networks, and basic blocks of Transformer in the following.
1) Multi-Head Self-Attention: Multi-head self-attention in-
volves multiple attention heads and performs self-attention
mechanism on every head. One attention head obtains one
representation space for the same text, and multi-head attention
obtains multiple different representation spaces. The self-
attention mechanism can be described as mapping a query
and a set of key-value pairs to an output, where the query, key,
value, and output are all d-dimensional vectors. The output of
each head is concatenated and results in the ﬁnal output vector
once again projected.

Scaled Dot-Product Attention. The self attention used in
Transformer is also known as scaled dot-product attention.

Scaled dot-product attention aims to pay more attention to the
important information of input sequence [36]. It transposes
the sequence of input vectors X = (x1, x2, ..., xn) into the
sequence of output vectors Z = (z1, z2, ..., zn), where xi,
zi ∈ Rdmodel . When doing self attention, Transformer ﬁrst
projects the input vector X into three vectors: the query Q, key
K and value V by trainable parameters W Q, W K, W V . The
attention weight is calculated using dot product and softmax
function. The output vector is the weighted sum of the value
vector:

eij =

(xiW Q)(xjW K)T
√
d

,

αij =

exp eij
k=1 exp eij

(cid:80)n

,

zi =

n
(cid:88)

j=1

αij(xjW V ),

(1)

(2)

(3)

where d is the dimension of each vector, and is used to scale
the dot product.

Multi-Head Attention. Multi-head attention captures dif-
ferent context with multiple individual self-attention functions.
This mechanism allows Transformer to jointly attend to infor-
mation from different representation sub-spaces. Multi-head
attention is computed after scaled dot-product attention:

M ultiHead(X) = Concat(head1, ..., headh)W O,

(4)

Multi-HeadAttentionAdd & NormInputEmbeddingOutputEmbeddingFeedForwardAdd & NormMaskedMulti-HeadAttentionAdd & NormMulti-HeadAttentionAdd & NormFeedForwardAdd & NormLinearSoftmaxPositional EncodingPositional EncodingOriginal CodeTargeted CodeExtract Syntactic Characteristic111000000111000000111000000000111000000111000000111000000000111000000111000000111…..1Statement Mask Matrix Dynamically Relative PositionRelative PositionS1S2S3S8…..…..Targeted CodeEncode BlockDecode BlockJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

headi = Attention(XW Q

i , XW K
i

, XW V

i ),

(5)

where W O indicates the learnable parameters and the param-
eters W Q

are independent in each head.

i , W K
i

, W V
i

2) Position-wise Feed-Forward Networks:

In addition to
multi-head self-attention sub-layers, each block in encoder and
decoder also contains a fully connected feed-forward network
(FFN) sub-layer. FFN transforms the current feature space into
another space through non-linear mapping, aiming at learning
a better representation of the input. The parameters of each
position are shared. This FFN can be computed by two linear
transformations and a ReLU activation function between them.

F F N (x) = max(0, xW1 + b1)W2 + b2,

(6)

Fig. 3: Example of code abstraction

where W1, W2, b1, and b2 are learnable parameters.

3) Basic Blocks of Transformer: Transformer is composed
of stacked encoder-decoder blocks. Every block in the Trans-
former has a multi-head self-attention sub-layer and an FFN
sub-layer. These sub-layers are connected by the residual
connections (He et al. [47]) and layer normalization (Ba
et al. [48]). Different from the encoder block, the decoder
block has another attention sub-layers that use the key and
value matrices from the encoder instead of calculating them
from the projection of input (DTrans is a Transformer-based
architecture, so the structure of encoder and decoder block also
can refer to Fig. 2). Besides, the number of encoder-decoder
blocks will affect the performance of Transformer, and more
encoder-decoder block will increase the model size and require
more time to train

III. METHODOLOGY

In this section, we introduce the Transformer-based model
DTrans for automatic code change prediction. The overall
architecture of DTrans is shown in Figure 2, following the
general Transformer framework (as introduced in Section II).
In order to mitigate the out-of-vocabulary (OOV) problem, we
ﬁrst perform code abstraction following the prior work [32],
[49]. Also, different from the vanilla Transformer, we propose
a novel position encoding strategy, named dynamically relative
position encoding,
to incorporate statement-level syntactic
information into Transformer for better capturing the local
structure of code. We elaborate on the code abstraction pro-
cess, and the proposed dynamically relative position encoding
in more details in the following.

A. Code Abstraction

Different from natural language, tokens in programming
language are more diverse since developers can deﬁne variable
names and function names in variant ways. The diversity
of identiﬁes and literals in the code leads to more serious
OOV problem during program comprehension. Thus, follow-
ing Ahmed et al. [49] and Tufano et al. [32]’s good practice,
we adopt code abstraction to reduce vocabulary size and
mitigate the OOV problem.

An example of code abstraction is shown in Figure 3.
Speciﬁcally, we use src2abs provided by [19], [32] to

abstract source code. It feeds sequence of source code to a Java
parser [50] which can recognize identiﬁers and literals, and
then generate and substitute a unique ID for each identiﬁer and
literal. If the identiﬁer or literal appears multiple times in the
same source code, it will be replaced with the same ID. Since
some identiﬁers and literals appear frequently in the source
code, they can be treated as keywords of the dataset [32]. The
frequent identiﬁers and literals should not be abstracted but
regarded as idioms that src2abs has provided for us.

B. Dynamically Relative Position Representations

Relation-aware Self-Attention. Using different position
embeddings for different positions helps Transformer capture
the position information of input words. However, absolute
positional encoding in the vanilla Transformer is ineffective to
capture the relative word orders [40]. To encode the pairwise
positional relationships between input elements, Shaw et al.
[40] propose the relative position encoding which models the
relation of two elements through their distance in the input
sequence. Formally, the relative position embedding between
input element xi and xj is represented as aV

ij ∈ Rd.

ij,aK

In this way, the self attention calculated in Equ. (1) and

Equ. (3) can be rewritten as:

eij =

(xiW Q)(xjW K + aK
√

ij )T

dz

zi =

n
(cid:88)

j=1

αij(xjW V + aV

ij).

,

(7)

(8)

Shaw et al. [40] also clip the maximum relative position
to a maximum absolute value of k since they hypothesize that
precise relative position information is not useful beyond a
certain distance. Clipping the maximum distance also enables
the model to generalize to sequence lengths unseen during
training:

i,j = wK
aK

clip(j−i,k),

i,j = wV
aV

clip(j−i,k),

(9)

(10)

source codeabstracted source codeJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

clip(x, k) = max(−k, min(k, x)).

(11)

i.e., wK = (wK
wK

Hence, we learn 2k + 1 relative position representations,
k ) and wV = (wv
k ) where

−k, ..., wK

−k, ..., wV

i ∈ Rdmodel .

i ,wV
Dynamically relation position. The relative position encod-
ing [40] captures the pairwise positional relationships between
the input elements by simply ﬁxing the maximum relative
distance at k. To involve the local structure information of
code, we propose to incorporate the statement-level informa-
tion into the position encoding. Different from pre-deﬁning
a maximum clipping distance k, we propose to dynamically
the distance based on the length of the statement
adjust
during the relative position encoding, named as dynamically
relation position encoding. The difference between relative
position embedding and the proposed strategy is illustrated
in Figure 4, and the clipping distance k is deﬁned as 3. For
the token VAR_1, the relative position encoding enhances the
relationship among the tokens before and behind the token
VAR_1, which is indicated with dotted lines in the relative
position encoding method of Figure 4 (a). We hypothesize
that tokens in one statement have stronger relations with the
tokens in other statements, e.g., the token VAR_1 tends to
be weakly relevant to the token METHOD_1 compared with
the tokens in the same statement (e.g., token METHOD_2).
To incorporate statement-level syntactic information into the
position embedding, we propose a dynamically relation po-
sition encoding strategy. The proposed position encoding can
help Transformer pay more attention to the tokens in the same
statement (denoted as the solid lines in Figure 4 (b)) and the
tokens with a relative distance smaller than k (denoted as the
dotted lines in Figure 4 (b)). In addition, the two kinds of
attention can be superimposed in our strategy. For example,
the token METHOD_2 receives the two kinds of attention,
while the last two tokens “)” and “;” do not receive the
relative position attention because their relative distance to
VAR_1 is bigger than the clipping distance k (k = 3 here).
In decoder, the current token cannot see the token behind, so
it is impossible to get the statement mask matrix. Therefore,
we only use the dynamically relative position in encoder.

Similar to padding mask and sequence mask, we propose a
statement mask operation to divide the code into a sequence
of statements. For the code example shown in Figure 5, we
illustrate the statement mask matrix for its statements s1, s2
and s3 in Figure 2. Speciﬁcally, the statement mask matrix
W L is a n × n matrix which records the statement-aware
information of the source code, where n is the length of code
tokens. For the tokens xi, xj in the same statement, the value
W L
ij is set
as 0.

ij between them is set as 1; otherwise the value W L

We compute the dynamically relative position embeddings

as below:

where W L ∈ Rn×n is the statement mask matrix, and aV
∈
Rdmodel is a learnable parameter vector. We then recalculate
the attention weight to incorporate the dynamically relative
position embeddings:

(cid:48)

eij =






xiW Q(xj W K +ak

ij +aK

xiW Q(xj W K +ak
√

ij )T

√

dz

dz

(cid:48)

)T

W L

ij = 1
ij = 0

W L

(13)

(cid:48)

where aK

∈ Rdmodel is a learnable parameter vector.

IV. EXPERIMENTAL SETUP

In this section, we will introduce the benchmark datasets,
implementation details, evaluation metrics and comparison
models for experimentation.

A. Benchmark Datasets

We conduct evaluation on two benchmark datasets12 fol-
lowing the previous work [19], [28], Gerrit3 code reviews
repository and open-source projects in GitHub (namely Git-
Projs). Gerrit includes Android [41], Google Source [42], and
Ovirt [43], while GitProjs contains 121,895 PRs commits from
GitHub open-source projects. We classify all projects in the
datasets into two levels, i.e., small level Msmall and medium
level Mmedium, according to the tokens numbers of original
code. Msmall and Mmedium contain 0-50 tokens and 500-100
tokens in each piece of original code, respectively. The two
benchmark datasets are partitioned into training set (80%),
validation set (10%) and test set (10%) following the prior
studies, with detailed statistics shown in Table I.

TABLE I: Statistics of the two benchmark datasets.

Dataset

Gerrit

Google
Android
Ovirt
All

GitProjs

Msmall Mmedium
2,286
3,617
5,088
10,991
65,545

2,165
4,162
4,456
10,783
58,350

B. Implementation and Supporting Tools/Platform

Data Preparation. We ﬁrst abstract the source code according
to Section III-A. Then, we compute the statement mask ma-
trices for the two benchmark datasets, respectively. However,
computing the matrices for a large amount of code is time-
consuming and inefﬁcient, so we convert the computation
into a series of matrix operations to fully use the computing
resources of GPU and improve the efﬁciency (Section IV-C
shows details). We test the time cost of the matrix computation
before and after the acceleration, respectively. The results
on the training set of GitProjs show that
it reduces the
computation time from 10 minutes to 7 seconds, indicating
the efﬁciency of the acceleration operation.
Hyper-parameters Setting. DTrans is composed of 6 hidden
layers and 8 heads. The hidden layer size of the model and

zi =

(cid:40)(cid:80)n
(cid:80)n

ij + aV
j=1 αij(xjW V + aV
j=1 αij(xjW V + aV
ij)

(cid:48)

) W L
W L

ij = 1
ij = 0

(12)

1https://sites.google.com/view/learning-codechanges
2https://sites.google.com/view/learning-ﬁxes
3https://www.gerritcodereview.com

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

Fig. 4: Illustration of the token relations for relative positions (a) and dynamically relative positions (b). For each relative
position, the clipping distance k is assumed as 3. Only the second and third statements of the source code are illustrated here
for simplicity. Solid lines and dotted lines indicate different token relations. The dotted line represents the relative distance is
smaller than k and the solid line represents the tokens in the same statement with VAR_1.

of im ∈ I is 1; otherwise, it is 0 (Line 2). We can get
W A ∈ Rn by multiplying I and the lower triangular matrix
W M (W M ∈ Rn×n) (Line 3-4). Next we will repeat W A to
get W B ∈ Rn×n (Line 5) and can ﬁnd that if i, j are in
the same statement, W B
j,i, and vice versa. So ﬁnally,
if W B
j,i = 1; otherwise it is
W B
j,i = 0. In this step (Line 6-10), we also use the
matrix operations completely instead of nested loops, so this
step is also efﬁcient.

j,i, we let W B

i,j = W B

i,j = W B

i,j = W B

i,j = W B

Algorithm 1 Computation of the statement mask matrix
Input: X = (x1, x2, ..., xn) ∈ Rn, which is a sequence of

source code tokens.

Output: the statement mask matrix, W L ∈ Rn×n
1: function COMPUTESMM(X)
2:

f rom X

triangular matrix

identif iers
I ← f ind the
// W M is lower triangular matrix, W M ∈ Rn×n
W M ← lower
W A ← I(W M )
// repeat W A n times to get W B ∈ Rn×n
W B ← repeat(W A)
W BT ← (W B)T
W S1 = |W B − W BT − 1| − |W B − W BT |
W S2 = |W BT − W B − 1| − |W B − W BT |
W S = (W S1 + W S2)/2
return W S

3:
4:

5:
6:
7:
8:
9:

10:
11: end function

D. Evaluation Metrics

We evaluate the performance of DTrans in code editting
using three popular metrics, including Exact Match [19], [32],
BLEU-4 [53] and ROUGE-L [54].

Fig. 5: Example of source code to represent statement mask
matrix. Note that we only take statements s1, s2, and s3 as
example for illustrating the statement mask matrix in Figure 2.

the size of every head are deﬁned as 512 and 64, respectively.
We train DTrans using Adam optimizer [51] with an initial
learning rate of 1.0 and use warm-up [52] to optimize the
learning rate. We set the mini-batch size as 32 and the dropout
as 0.1 during training. DTrans is trained for a maximum
of 20,000 steps and performed early stops if the validation
performance does not improve during 2,000 steps. We also
use beam search during inference and set the beam size as 10.
Platform. Our experiments are conducted on a single Tesla
p100 GPU for about 10 hours for Mmedium datasets and
5 hours for Msmall datasets for both benchmark datasets,
respectively.

C. GPU Acceleration

Algorithm 1 shows how we use matrix operations to replace
inefﬁcient nested loops during computing the statement mask
matrix.

The input X = x1, x2, ..., xn ∈ Rn is the sequence of
source code token. We ﬁrst compute I = i1, i2, ...in ∈ Rn,
which is a vector consisting of 0 and 1, from X. The rule
for generating I is : if xm ∈ X is an identiﬁer, the value

protected void METHOD_1 ( ) {super . METHOD_1 ( ) ;VAR_1 . METHOD_2 ( ) ;VAR_2 . METHOD_3 ( ) ;}(a)super.METHOD_1();VAR_1.METHOD_2();(b)super.METHOD_1();VAR_1.METHOD_2();public static long METHOD_1( ) {long a ;long b ;long c ;a = INT_1;b = INT_2 ;c = a + b ;return c ;}S0S1S2S3S4S5S6S7S8JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

Exact Match computes the number and percentage of
predicted code changes that exactly match the changed code
in the test sets.

BLEU-4 is a widely-used metric in natural language pro-
cessing and software engineering ﬁelds to evaluate the quality
of generated texts, e.g., machine translation, code comment
generation, and code commit message generation [53], [55],
[56]. It computes the frequencies of the co-occurrence of n-
grams between the ground truth ˆy and the generated sequence
y to judge the similarity:

BLEU-N = b(y, ˆy) · exp

(cid:33)

βn log pn(y, ˆy)

,

(cid:32) N
(cid:88)

n=1

where b(y, ˆy) indicates the brevity penalty, and pn(y, ˆy) and
βn represent the geometric average of the modiﬁed n-gram
precision and the weighting parameter, respectively. We use
corpus-level BLEU-4, i.e., N = 4 for evaluation since it is
demonstrated to be more correlated with human judegments
than other evaluation metrics [57].

ROUGE-L is commonly used in natural language transla-
tion [54], and is a F-measure based on the Longest Common
Subsequence (LCS) between candidate and target sequences,
where the LCS is a set of words appearing in the two
sequences in the same order.

ROU GE-L =

(cid:0)1 + β2(cid:1) RlcsPlcs
Rlcs + β2Plcs

,

len(Y )

and Plcs = LCS(X,Y )

where Rlcs = LCS(X,Y )
. X and Y de-
note candidate sequence and reference sequence, respectively.
LCS(X, Y ) represents the length of the longest common sub-
sequence between X and Y .

len(X)

E. Comparison Model

We compare DTrans with three baseline models, including
Tufano el al. (an NMT-based model) [19], [32], SequenceR
[29] and CODIT [20]. Tufano el al. [19], [32] employ a
typical encoder-decoder model LSTM to edit method-level
code, where the input is a sequence of code tokens. SequenceR
[29] is also LSTM-based encoder-decoder model, but it uses
copy mechanism to copy code tokens from the source code
during decoding. The input of SequenceR is also code token
sequence. CODIT [20] is a tree-based model, which uses the
ASTs of source code as input and predicts code edit at the
AST level.

V. EXPERIMENT RESULTS

In this section, we aim at verifying the effectiveness of the
proposed approach, speciﬁcally by answering the following
research questions:

RQ1: What is the performance of the proposed approach

compared with the baseline models?

RQ2: What

is the impact of the proposed dynamically
relative position encoding on the model performance?
RQ3: What is the effectiveness of DTrans in generating

multi-lines code change prediction?

RQ4: Whether DTrans can accurately locate the lines to

edit for code change prediction?

RQ5: What is the impact of different parameters on the

model performance?

RQ6: What is the performance of DTrans in cross-project

setting?

Speciﬁcally, RQ1 is to evaluate the performance of the
proposed model compared with baselines, including token-
based models and tree-based models. To verify the advantage
of the proposed dynamically relative position embedding,
we compare DTrans with Transformer [36] and Transformer
with relative position embedding (namely Transformerrelative)
[40] in RQ2. For RQ3, since we ﬁnd that more than 30%
of the code samples in the datasets need multi-line code
changes, the research question is to evaluate the capacity of
DTrans for generating multiple-line code changes. RQ4 is to
validate the ability of locating lines to edit. Finally, since
the hyper-parameters can impact the performance of DTrans,
RQ5 discusses the hyper-parameter conﬁgurations. RQ6 is to
evaluate the performance of DTrans in cross-project setting.

A. Answer to RQ1: Performance of the proposed DTrans

1) Comparison with token-based models: Table II presents
the experimental results of our proposed model and the token-
based baselines on the benchmark datasets. From the table,
we can observe that DTrans performs better than the token-
based baselines in predicting exact-matched code changes for
all the datasets. For example, DTrans successfully generates
489 exact-matched code changes in Gerrit for Msmall and
409 for Mmedium, while Tufano et al. only generates 388 and
334 exact-matched code changes, respectively, and SequenceR
only generates 405 and 284 exact-matched code changes for
Msmall and Mmedium. Compared with Tufano et al. , DTrans
outperforms 26.04% and 22.45% for Msmall and Mmedium,
respectively. Compared with SequenceR, DTrans outperforms
20.74% and 44.01% for Msmall and Mmedium respectively.
For GitProjs, SequenceR outputs 2,255 code changes that
are consistent with the ground truth for Msmall and 1,214
for Mmedium, while DTrans successfully produces 2,573 and
1,625 for the two types of datasets, respectively. Besides, the
ground truth is human-writing code [19], [28], so the higher
scores of BLEU-4 and ROUGE-L represent that the results
generated by DTrans are semantically similar to the human-
writing code. For example, DTrans increases the performance
of SequenceR by 2.59% and 0.66% in Google Mmedium with
respect to the BLEU-4 and ROUGE-L metrics, respectively.
The results demonstrate the effectiveness of the proposed
DTrans over the token-based models.

2) Comparison with tree-based models: Because CODIT
does not provide the source code for data processing, and
the data processing process of CODIT is very complex, we
directly compare it on the code change dataset used by
CODIT. Table IV shows the experimental results of DTrans
and CODIT. In the abstracted code change dataset provided
by CODIT, the result of CODIT is not good. Compared with
SequenceR, CODIT is lower than SequenceR by 17.80%,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

TABLE II: Comparison results of DTrans and token-based baselines. The bold indicates the best results.

Dataset

Approach

Google

Android

Ovirt

Overall

Gerrit

GitProjs

Tufano et al.
SequenceR
DTrans
NMT-based
Tufano et al.
DTrans
Tufano et al.
SequenceR
DTrans
Tufano et al.
SequenceR
DTrans
Tufano et al.
SequenceR
DTrans

Msmall

Mmedium

Exact Match
20/216(9.25%)
55/216(25.46%)
58/216(26.85%)
79/416(18.99%)
157/416(37.74%)
174/416(41.82%)
113/445(25.39%)
173/445(38.87%)
204/445(45.84%)
388/1,077(36.02%)
405/1,077(37.60%)
489/1,077(45.40%)
2,119/5,835(36.31%)
2,255/5,835(38.64%)
2,573/5,835(44.09%)

BLEU-4
55.29%
76.87%
78.09%
64.29%
83.86%
84.63%
73.60%
85.10%
86.81%
82.47%
85.82%
86.82%
85.84%
86.72%
87.14%

ROUGE-L
83.81%
93.13%
93.30%
88.16%
95.64%
95.64%
91.14%
95.79%
95.81%
94.57%
95.69%
96.10%
96.06%
96.33%
96.55%

Exact Match
17/228(7.45%)
43/228(18.85%)
63/228(27.63%)
76/361(21.05%)
83/361(22.99%)
112/361(31.02%)
102/509(20.03%)
167/509(32.80%)
210/509(41.25%)
334/1,098(30.41%)
284/1,098(25.86%)
409/1,098(37.24%)
1,166/6,545(17.82%)
1,214/6,545(18.54%)
1,625/6,545(24.82%)

BLEU-4
75.12%
89.35%
91.67%
87.33%
90.67%
91.80%
82.66%
91.69%
92.82%
91.57%
92.04%
92.79%
90.97%
91.03%
91.56%

ROUGE-L
91.46%
96.85%
97.49%
96.14%
97.48%
97.81%
94.07%
97.52%
97.60%
97.53%
97.57%
97.85%
97.58%
97.62%
97.81%

TABLE III: Comparison results of DTrans, Transformer and Transformerrelative. The bold indicates the best results. “*” denotes
statistical signiﬁcance in comparison to the baselines(i.e.,two-sided t-test with p-value<0.05)

Dataset

Approach

Gerrit

Google

Andriod

Ovirt

Overall

GitProjs

Transformer
Transformerrelative
DTrans
Transformer
Transformerrelative
DTrans
Transformer
Transformerrelative
DTrans
Transformer
Transformerrelative
DTrans
Transformer
Transformerrelative
DTrans

Msmall

Mmedium

Exact Match
40/216(18.52%)∗
58/216(26.85%)
58/216(26.85%)
146/416(35.09%)∗
173/416(41.58%)
174/416(41.82%)
182/445(40.89%)∗
189/445(42.47%)∗
204/445(45.84%)
428/1,077(39.74%)∗
472/1,077(43.82%)
489/1,077(45.40%)
2,503/5,835(42.89%)∗
2,540/5,835(43.53%)
2,573/5,835(44.09%)

BLEU-4
73.20%∗
78.06%
78.09%
83.00%∗
84.37%
84.63%
83.73%∗
84.82%∗
86.81%
84.37%∗
86.18%
86.82%
86.49%∗
87.01%
87.14%

ROUGE-L
91.89%∗
93.04%
93.30%
95.29%
95.58%
95.64%
94.84%∗
95.29%∗
95.81%
95.35%∗
95.95%
96.10%
96.40%∗
96.47%
96.55%

Exact Match
38/228(16.66%)∗
61/228(26.75%)
63/228(27.63%)
97/361(26.86%)∗
99/361(27.42%)∗
112/361(31.02%)
172/509(33.79%)∗
188/509(36.93%)∗
210/509(41.25%)
355/1,098(32.33%)∗
388/1,098(35.33%)
409/1,098(37.24%)
1,509/6,545(23.05%)∗
1,574/6,545(24.04%)∗
1,625/6,545(24.82%)

BLEU-4
88.49%∗
91.08%
91.67%
91.42%
91.66%
91.80%
92.16%∗
92.56%
92.82%
92.19%∗
92.29%∗
92.79%
91.29%∗
91.56%
91.56%

ROUGE-L
97.11%∗
97.38%
97.49%
97.77%
97.62%
97.81%
97.42%∗
97.55%
97.60%
97.70%∗
97.72%∗
97.85%
97.73%∗
97.79%
97.81%

4.59%, and 3.41% with respect to the Exact Match, BLEU-
4 and ROUGE-L metrics, respectively. DTrans improves the
performance of SequenceR by 16.10%, 2.38% and 0.83%
regarding the three metrics respectively.

the original Transformer [36] and Transformer with relative
position (Transformerrelative) [40]. We reproduce their experi-
ments under the same hyper-parameter settings as DTrans for
fair comparison.

TABLE IV: Comparison results of DTrans and tree-based
baseline CODIT. The bold indicates
results.
“*” denotes statistical signiﬁcance in comparison to the
baselines(i.e.,two-sided t-test with p-value<0.05)

the best

Method
Tufano et al.
SequenceR
CODIT
Transformer
Transformerrelative
DTrans

Exact Match
1,898/5,143(36.90%)∗
2,130/5,143(41.41%)∗
1,808/5,143(35.15%)∗
2,293/5,143(44.58%)∗
2,426/5,143(47.17%)∗
2,473/5,143(48.08%)

BLEU-4
75.54%∗
78.02%∗
74.59%∗
77.87%∗
79.31%∗
79.88%

ROUGE-L
93.03%∗
93.87%∗
90.77%∗
93.98%∗
94.56%
94.65%

Answer to RQ1: In summary, DTrans can more accurately
predict code changes, and the generated code changes are
more semantically relevant to the ground truth.

B. Answer to RQ2: Impact of the proposed dynamically rela-
tive position encoding on the model performance

To evaluate the effectiveness of the proposed dynamically
relative position encoding strategy, we compare DTrans with

Table III shows the experimental results, we ﬁnd that
Transformer performs better than Tufano et al.. In more details,
Transformer improves the performance of Tufano et al. by
6.3%-123.62%, 0.35%-32.39%, 0.15%-9.64% regarding the
three metrics on the two benchmark datasets, respectively.
Besides, Transformer can generate 4,795 code changes that
exactly match the ground truth on the two benchmark datasets,
which outperforms 15.32% than SequenceR. The experimental
results suggest
that Transformer-based models can predict
more effective code edits than token-based models. Moreover,
Transformerrelative performs better than the vanilla Transformer
in most cases, which indicates that the relative position en-
coding in Transformer is more effective in capturing the code
edit patterns. Finally, DTrans achieves better performance than
Transformerrelative, with increase rates at 1.28% and 3.24% in
terms of the exact match metric on the Msmall and Mmedium
datasets, respectively. The results indicate the efﬁcacy of the
proposed dynamically relative position encoding strategy.

Answer to RQ2: In summary, the Transformer-based mod-
els outperform baselines. The statement-level syntactic infor-

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

mation for position encoding facilitates more accurate code
change prediction.

TABLE VII: Comparison results in locating lines to edit of
DTrans with other techniques. The bold fonts indicate the best
results.

C. Answer to RQ3: Effectiveness of code change prediction
for multiple lines

According to the statistics [29], [49], most edits are ac-
complished through a single-line code change. However, some
edits still need multi-line code changes in practice. We analyze
the multi-line code changes in the Gerrit dataset [58] in
Table V, and observe that nearly 35% edits involve changes
of more than one line of code.

TABLE V: Statistics of the edits that need multi-line code
changes.

Dataset

Gerrit

Google
Android
Ovirt
Overall

Msmall
74/216(34.26%)
177/416(42.55%)
130/445(29.21%)
381/1,077(35.38%)

Mmedium
103/228(45.18%)
162/361(44.88%)
171/509(33.60%)
436/1,098(39.71%)

We then investigate the effectiveness of DTrans in producing
multi-line code changes, with the evaluation results shown in
Figure 6. As illustrated in the table, DTrans achieves the best
performance among all the baselines in multi-line code change
prediction. For example, DTrans overall produces 42.25%
exact-matched code changes for Msmall projects and 33.02%
for Mmedium projects, while Tufano et al. only outputs 30.97%
and 23.16% for the two types of datasets, respectively. The
results indicate the usefulness of DTrans in multi-line code
prediction. We can also observe that compared with Tufano
et al., the improvement of DTrans on the Mmedium projects
(42.57%) is more signiﬁcant than that on the Msmall projects
(36.42%). We then analyze the average lines of code in the
test sets of the Msmall and Mmedium projects, with the results
shown in Table VI. We can ﬁnd that the code in the Mmedium
projects are longer than that in the Msmall projects on average.
Therefore, we suppose the signiﬁcant performance of DTrans
on the Mmedium projects may be attributed to that DTrans is
more effective for predicting the changes of long code snippets
than baseline models.

TABLE VI: Statistics of the average line of code in test sets
of the Msmall and Mmedium projects.

Dataset

Msmall Mmedium

Gerrit

Google
Android
Ovirt
Overall

4.47
4.89
4.41
4.60

9.42
10.31
9.01
9.52

Answer to RQ3: In summary. DTrans demonstrates the
superior ability of accurately generating multiple-line code
changes, and has a great improvement over the baselines.

D. Answer to RQ4: Accuracy of DTrans in locating lines to
edit for predicting code change

Locating correct lines to edit is the premise of the accurate
code changes in the subsequent step. So in this research

Dataset

Google

Android

Overit

Overall

Approach
Tufano et al.
Sequencer
Transformer
Transformerrelative
DTrans
Tufano et al.
Sequencer
Transformer
Transformerrelative
DTrans
Tufano et al.
Sequencer
Transformer
Transformerrelative
DTrans
Tufano et al.
Sequencer
Transformer
Transformerrelative
DTrans

Msmall
63/216(29.16%)
114/216(52.77%)
91/216(42.12%)
110/216(50.92%)
116/216(53.70%)
164/416(39.42%)
255/416(61.29%)
235/416(56.49%)
255/416(61.29%)
258/416(62.01%)
212/445(47.60%)
303/445(68.08%)
287/445(64.49%)
302/445(67.86%)
315/445(70.78%)
666/1,077(61.83%)
718/1,077(66.66%)
690/1,077(64.06%)
727/1,077(67.50%)
757/1,077(70.28%)

Mmedium
45/228(19.73%)
95/228(41.66%)
84/228(36.84%)
118/228(51.75%)
118/228(51.75%)
138/361(38.22%)
163/361(45.15%)
170/361(47.09%)
179/361(49.58%)
189/361(52.35%)
189/509(37.13%)
311/509(61.10%)
299/509(58.74%)
307/509(60.31%)
328/509(64.44%)
587/1,098(53.46%)
598/1,098(54.46%)
601/1,098(54.73%)
630/1,098(57.37%)
648/1,098(59.01%)

question, we analyze whether the proposed approach can
accurately predict which lines to edit.

Table VII shows the experimental results of locating the
lines for editing. We can observe that DTrans performs better
than other techniques on all projects. For example, SequenceR
can only locate 66.66% correct lines for Msmall and 54.46%
correct lines for Mmedium, while DTrans can locate 70.28%
and 59.01%, respectively. This observation demonstrates that
DTrans can obtain more contextual information than other
techniques.

Answer to RQ4: In summary, DTrans can greatly outper-
form the baselines in locating the lines to change (e.g.,
achieving 8.35% higher accuracy than the best baseline).

E. Answer to RQ5: Impact of the model parameters

In this section, we extend our experiments with different
parameters to investigate the inﬂuence of internal factors of
DTrans.

Figure 7 (a) presents the impact of the clipping distance
(k) on the effectiveness of DTrans using other default conﬁg-
urations (Deﬁned in Section.III-B). In this ﬁgure, the x axis
presents various clipping distances, while the y axis presents
the values of different evaluation metrics. We can ﬁnd that the
clipping distance does not impact the DTrans effectiveness
much. For example, the largest performance difference among
different clipping distances is within 2% for all evaluation
metrics. Since DTrans achieves good performance on the
datasets when the clipping distance is 32, we choose the
parameter as 32 during experimentation.

Figure 7 (b) presents the impact of the number of encoder-
decoder block (l) on the effectiveness of DTrans using other
default conﬁgurations (Deﬁned in Section.II-B). In this ﬁgure,
the x axis presents different number of encoder-decoder block,
while the y axis presents the values of different evaluation
metrics.From the ﬁgure, we observe that
the number of
encoder-decoder block has a signiﬁcant impact on the model.
For example, in GitProjs, the Exact Match of 2-blocks DTrans

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

(a) Msmall projects

Fig. 6: Comparison results of generating multi-lines patch on the Msmall (a) and Mmedium (b) projects. “*” denotes statistical
signiﬁcance in comparison to the baselines (i.e., two-sided t-test with p-value<0.05).

(b) Mmedium projects

(a) Clipping distance k.

(b) Number of encoder-decoder block l of Transformer.

Fig. 7: Impact of key parameters on the model performance.

is only 42.26%, while the Exact Match of 6-blocks DTrans
is 44.09%. Besides, more encoder-decoder blocks do not
mean better performance. For example, 6-blocks DTrans is
better than the 8-blocks DTrans in Gerrit-All. Moreover, more
encoder-decoder block will increase the model size and require
more time to train. In terms of overall considerations, DTrans
with 6 encoder-decoder blocks is a good option.

Answer to RQ5: In summary, the experimental results can
the
be inﬂuenced by parameter conﬁguration. Moreover,
clipping distance has little inﬂuence on DTrans, but the

number of layers has much inﬂuence on DTrans.

F. Answer to RQ6: erformance of DTrans in cross-project
setting

In this section, we train the models in one project and test
them in another project to simulate a more practical setting. We
use the Gerrit dataset which contains three different projects
for the evaluation. We adopt the best Transformer-based base-
lines for comparison. The results are shown in Table VIII.
We can observe that DTrans consistently performs better than

Benchmark DatasetExactMatch9.46%*14.12%*20.77%*30.97%*22.97%31.63%*30.76%33.33%*16.22%*31.64%*29.23%*38.58%*21.62%34.46%35.38%41.46%24.32%38.41%35.38%42.25%0%5%10%15%20%25%30%35%40%45%GoogleAndroidOvirtAllTufano et al.SequenceRTransformerTransformerreltiveDTransrelativeBenchmark DatasetExactMatch8.73%*17.28%*9.94%*23.16%*20.38%*18.51%*23.97%*17.88%*16.50%*24.07%*25.73%27.52%*30.10%25.31%26.90%31.42%32.04%27.77%29.23%33.02%0%5%10%15%20%25%30%35%GoogleAndroidOvirtAllTufano et al.SequenceRTransformerTransformerreltiveDTransrelative43.0%44.0%45.0%46.0%4816326442.0%43.0%44.0%45.0%4816326485.0%86.0%87.0%4816326486.8%87.0%87.2%87.4%87.6%4816326495.4%95.6%95.8%96.0%96.2%4816326496.4%96.5%96.6%96.7%48163264Gerrit-AllGitProjsROUGE-LBLEU-4Exact Match38.0%40.0%42.0%44.0%46.0%246841.0%42.0%43.0%44.0%45.0%246882.0%84.0%86.0%88.0%246886.0%86.5%87.0%87.5%88.0%246895.0%95.5%96.0%96.5%246896.3%96.4%96.5%96.6%96.7%2468Exact MatchBLEU-4ROUGE-LGerrit-AllGitProjsJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

Transformer and Transformerrelative in the cross-project setting.
For example, when we train the models in Google Msmall
and then test them in Android Msmall, DTrans can gener-
ate 17 exact-matched code changes, while Transformer only
generates 11 exact-matched code changes. Overall, DTrans in-
creases the performance of the Transformer-based baselines by
0∼200%,0.03∼5.66%,0.09∼1.30% with respect to the Exact
Match, BLEU-4, ROUGE-L metrics, respectively. We can also
ﬁnd that despite the good performance of DTrans in cross-
project setting, it presents obvious decline compared with the
in-project performance, e.g., the exact match score drops by
more than 80%. The Transformer-based baselines show the
similar trend. The phenomenon is reasonable since the edit
patterns of different projects may be greatly different.

Answer to RQ6: In summary, DTrans performs better than
the baseline models in the cross-project setting. However, the
performance of all the models drops greatly comparing with
the in-project setting, indicating that cross-project evaluation
is a more challenging setting for the code edit task.

VI. CASE STUDY

To evaluate the performance of DTrans in predicting accu-
rate code edits, we select three cases from benchmark datasets
as shown in Figure 8.

Figure 8 (a) presents an example of code edit operation
prediction. The method addSlices lacks an object to con-
duct the method function slices.addAll(slices), so
the correct edit operation is to add an object this. However,
Tufano et al. mistakenly predicts the operation is to change the
return from true to false. Similarly, SequenceR incorrectly
returns the variable slices. DTrans successfully predicts the
correct edit operation and adds this to point to the variable
inside the class.

In Figure 8 (b),

the original code needs to remove
grade from curve, but
it does not check whether the
variable grade is null. The correct edit operation is
to check variable grade before executing remove. Tu-
fano et al. does not check variable grade and just refac-
tors original code. SequenceR predicts the correct operation
method but the incorrect operation object. It checks null
for curve.remove(grade) rather than variable grade.
DTrans successfully predicts both the correct operation method
and operation object. It checks whether the variable grade
is null before executing curve.remove(grade).

In addition, DTrans does not always predict accurate
code changes. In Figure 8 (c),
the original code needs a
return statement because the modiﬁer void does not
appear in the method deﬁnition. Tufano et al. successfully
adds return before getCFlags(). Sequencer mistakenly
thinks that the original code should be a static function, so
it inserts the modiﬁer static. For DTrans, it successfully
adds the return token, but incorrectly changes the API from
java.lang.Iterable to java.lang.Set, which is a
non-existent interface. This example motivates us to create
an API knowledge base to facilitate the code edit process in
future.

VII. THREAT TO VALIDITY
Internal validity is mainly about the hyper-parameter con-
ﬁguration we adopted in our DTrans model. To reduce this
threat, we conduct an experiment
to study the impact of
conﬁguration, and we explain in Section V-E about how hyper-
parameters inﬂuence our model’s performance.

Construct validity is mainly the suitability of our evalu-
ation metrics. To reduce this risk, we additionally introduce
BLEU-4 (Bilingual evaluation understudy in 4-gram) [53] and
ROUGE-L (Recall-Oriented Understudy for Gisting Evalua-
tion in Longest Common Sub-sequence) [54] to evaluate the
effectiveness of our approaches, which can well simulate the
non-trivial coding activities in evaluating generated code.

External validity is mainly concerned with whether the
performance of our DTrans techniques can still effective in
other datasets. To reduce these threats, we additional select
2.3 million pair-wise code changes generated from GitHub
open-source projects [32] to evaluate the effectiveness of our
approach. And experimental results demonstrate the effective-
ness of our approach (in Section V-A). To further reduce the
threats, we are planning to collect more open-source projects
to evaluate our approach. Besides, the quality of the datasets
may be another threat. In this paper, we simply follow the
previous work [19], [20] by directly adopting the benchmark
datasets without further cleaning. As illustrated in Sun et
al. [59], high-quality datasets are very important for DL
models. We will study the quality of the datasets in future
work.

VIII. RELATED WORK

Related works focus on two key aspects: position represen-

tations of Transformer and automatic code edit.

A. Position Representations of Transformer

Unlike RNN [60], which incorporates inductive bias by
successively loading the input
tokens, Transformer is less
position-sensitive [36]. It is critical to incorporate position
encoding into the Transformer.

Absolute Position Representations. Vaswani et al. [36]
proposed Transformer and trigonometric function to calculate
positional information for each token, but the positional infor-
mation cannot change, while Devlin et al. [61] and Liu et al
[62] use parameter matrix to calculate positional information.
Liu et al. [63] proposed FLOATER, which models position
encoding as a continuous dynamical system and admit the
standard sinusoidal position encoding as a special case, making
more ﬂexible in theory. Dehghani et al. [64]and Lan et al. [65]
found that injecting the position information into layers can
improve performance of Transformer in some tasks.

Relative Position Representations. Relative position Rep-
resentations take the relative distance into calculating attention
rather than absolute position, which performs more effective
and ﬂexible. Shaw et al. [40] ﬁrst proposed the concept of
relative position embedding and its application scope. Yang
et al. [66] and Dai et al. [67] improved the relative position
embedding to boost the effectiveness. Raffel et al. [68] and
Ke et al. [69] evaluated the effective of “input-position” and

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

12

Fig. 8: Qualitative analysis of our DTrans model and the baseline models on three examples.

JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST201514TABLEVII:QualitativeanalysisofDTransmodelandthebaselinemodelsontwoexamples.Example(a):OriginalCode:publicbooleanaddSlices(java.util.Collection<it.polito.atlas.alea2.Slice>slices){slices.addAll(slices);returntrue;}Tufanoetal.:publicbooleanaddSlices(java.util.Collection<it.polito.atlas.alea2.Slice>slices){slices.addAll(slices);returnfalse;}SequenceR:publicbooleanaddSlices(java.util.Collection<it.polito.atlas.alea2.Slice>slices){slices.addAll(slices);returnslices;}Dtrans:publicbooleanaddSlices(java.util.Collection<it.polito.atlas.alea2.Slice>slices){this.slices.addAll(slices);returntrue;}Example(b):OriginalCode:publicbooleanremove(model.curve.Gradegrade){returncurve.remove(grade);}Tufanoetal.:publicbooleanremove(model.curve.Gradegrade){booleanresult=curve.remove(grade);returnresult;}SequenceR:publicbooleanremove(model.surve.Gradegrade){return!(curve.remove(grade).isEmpty());}Dtrans:publicbooleanremove(model.surve.Gradegrade){if(grade!=null){returncurve.remove(grade);}else{returnfalse;}}Example(c):OriginalCode:publicjava.lang.Iterable<java.lang.String>getCppFlags(){getCFlags();}Tufanoetal.:publicjava.lang.Iterable<java.lang.String>getCppFlags(){returngetCFlags();}SequenceR:publicstaticjava.lang.Iterable<java.lang.String>getCppFlags(){getCFlags();}Dtrans:publicjava.lang.Set<java.lang.String>getCppFlags(){returngetCFlags();}JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

13

TABLE VIII: Cross-project comparison results of DTrans and Transformer-based baselines. The bold indicates the best results.

Dataset

Training

Test

Google

Android

Ovirt

Android

Ovirt

Google

Ovirt

Google

Android

Approach

Transformer
Transformerrelative
DTrans
Transformer
Transformerrelative
DTrans
Transformer
Transformerrelative
DTrans
Transformer
Transformerrelative
DTrans
Transformer
Transformerrelative
DTrans
Transformer
Transformerrelative
DTrans

Exact Match
11/416(2.64%)
16/416(3.84%)
17/416(4.08%)
13/445(2.92%)
20/445(4.49%)
22/445(4.94%)
6/216(2.77%)
8/216(3.70%)
10/216(4.62%)
21/445(4.71%)
22/445(4.94%)
30/445(6.74%)
6/216(2.77%)
10/216(4.62%)
10/216(4.62%)
25/416(6.01%)
23/416(5.52%)
25/416(6.01%)

Msmall

BLEU-4
58.38%
62.33%
63.92%
54.67%
59.49%
61.10%
59.66%
62.37%
63.28%
61.20%
64.84%
64.86%
56.78%
57.99%
59.06%
60.94%
62.60%
63.75%

ROUGE-L
86.06%
87.52%
87.74%
81.53%
82.44%
82.93%
83.69%
83.82%
84.09%
82.86%
83.61%
83.70%
82.29%
82.78%
82.86%
86.70%
87.02%
87.50%

Exact Match
0/361(0%)
4/361(1.1%)
7/361(1.93%)
0/509(0%)
0/509(0%)
1/509(0.20%)
2/228(0.87%)
2/228(0.87%)
2/228(0.87%)
0/509(0%)
1/509(0.19%)
3/509(0.58%)
0/228(0%)
1/228(0.43%)
1/228(0.43%)
1/361(0.27%)
4/361(1.10%)
5/361(1.38%)

Mmedium
BLEU-4
71.58%
80.19%
80.23%
64.23%
71.61%
71.90%
73.35%
75.30%
76.15%
68.23%
71.42%
72.05%
62.90%
65.13%
68.82%
72.24%
73.38%
76.64%

ROUGE-L
90.11%
91.82%
91.92%
83.72%
84.94%
85.06%
87.81%
87.83%
88.26%
84.61%
85.02%
85.27%
81.34%
82.54%
83.60%
88.60%
88.91%
90.07%

“position-input” and remove them from Transformer. He et
al. [70] evaluated the absolute and relative position embed-
ding and proved the usability of relation position embedding.
Since these approaches are developed for natural language
they are unable to capture the statement-level
processing,
information included in code; while our proposed dynamically
relative position encoding strategy is speciﬁcally designed for
involving the statement-level syntax information of source
code.

B. Automatic Code Edit

Code edit throughout the program development and main-
tenance relates to various behaviors, e.g., automatic program
repair [71]–[75], API-related update [26], and code refactoring
[25], [76]. In recent years more and more proposed works
adapted Deep Learning (DL) techniques in automatic code
edit [20], [24], [75], [77], aiming at automatically predicting
code changes using a data-driven approach. Tufano et al.
[19] applied Neural Machine Translation (NMT) techniques
to generate target code at the method level. They treated code
as natural language, converting it into tokens and using code
abstraction to overcome the issue of out-of-vocabulary. Chen
et al. [29] presented the SequenceR, an NMT-based approach,
which uses the attention mechanism and outperforms Tufano et
al. [19]. Chakraborty et al. [20] presented CODIT, a tree-based
NMT model for predicting concrete source code changes and
learning code change patterns in the wild, and it is the state-
of-the-art NMT-based model in code edit. Above approaches
ignore the statement-level information, so we propose DTrans,
a novel Transformer-based approach, which explicitly incor-
porates the statement-level syntactic information for better
capturing the local structure of code, to predict code changes.

IX. CONCLUSION AND FUTURE WORK
In this paper, we introduced DTrans, a Transformer-based
technique that can predict code changes from merged pull re-
quests codes from developers. To better capture the statement-
level information of code, DTrans is designed with dynam-
ically relative position encoding in multi-head attention of

Transformer. Compared with other DL-based techniques such
as neural machine translation (NMT), DTrans can capture
the syntactic information, which makes the generated code
changes higher-quality. The experimental results show that
DTrans can more accurately generate program changes in
automatic code edit.

Our experiments also demonstrate the difﬁculties in the
cross-project code edit task. In the future, we plan to inves-
tigate the cross-project challenges and incorporate more se-
mantic information(e.g., Control-Flow Graph, Abstract Syntax
Tree) to increase our capacity in code editing for the cross-
project task.

X. ACKNOWLEDGMENTS

This

research was

by National Natural
supported
Science Foundation of China Grant under project No.
62002084, 61872110, 61672191, Stable support plan for
colleges and universities in Shenzhen under project No.
GXWD2020
the
Major Key Project of PCL (Grant No. PCL2022A03,
PCL2021A02, PCL2021A09), Guangdong Provincial Key
Laboratory of Novel Security Intelligence Technologies
(2022B1212010005), the Science and Technology Program
of Guangzhou, China (202103050004).

1230155427003-20200730101839009,

REFERENCES

[1] F. Ferreira, L. L. Silva, and M. T. Valente, “Software engineering meets
deep learning: a mapping study,” in Proceedings of the 36th Annual
ACM Symposium on Applied Computing, 2021, pp. 1542–1549.

[2] Y. Yang, X. Xia, D. Lo, and J. Grundy, “A survey on deep learning for

software engineering,” arXiv preprint arXiv:2011.14597, 2020.

[3] H. F. Eniser, S. Gerasimou, and A. Sen, “Deepfault: Fault localization
for deep neural networks,” in International Conference on Fundamental
Approaches to Software Engineering. Springer, 2019, pp. 171–191.
[4] X. Li, W. Li, Y. Zhang, and L. Zhang, “Deepﬂ: Integrating multiple fault
diagnosis dimensions for deep fault localization,” in Proceedings of the
28th ACM SIGSOFT International Symposium on Software Testing and
Analysis, 2019, pp. 169–180.

[5] M. Wardat, W. Le, and H. Rajan, “Deeplocalize: Fault localization
for deep neural networks,” in 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE).
IEEE, 2021, pp. 251–
262.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

14

[6] T. Lutellier, L. Pang, V. H. Pham, M. Wei, and L. Tan, “Encore:
Ensemble learning using convolution neural machine translation for
automatic program repair,” arXiv preprint arXiv:1906.08691, 2019.
[7] Y. Li, S. Wang, and T. N. Nguyen, “Dlﬁx: Context-based code trans-
formation learning for automated program repair,” in Proceedings of
the ACM/IEEE 42nd International Conference on Software Engineering,
2020, pp. 602–614.

[8] R. Gupta, S. Pal, A. Kanade, and S. Shevade, “Deepﬁx: Fixing com-
mon c language errors by deep learning,” in Proceedings of the aaai
conference on artiﬁcial intelligence, vol. 31, no. 1, 2017.

[9] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “A
transformer-based approach for source code summarization,” arXiv
preprint arXiv:2005.00653, 2020.

[10] A. LeClair, S. Haque, L. Wu, and C. McMillan, “Improved code
summarization via a graph neural network,” in Proceedings of the 28th
International Conference on Program Comprehension, 2020, pp. 184–
195.

[11] Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu,
“Improving automatic source code summarization via deep reinforce-
ment learning,” in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, 2018, pp. 397–407.

[12] S. Kim, J. Zhao, Y. Tian, and S. Chandra, “Code prediction by feeding
trees to transformers,” in 2021 IEEE/ACM 43rd International Conference
on Software Engineering (ICSE).

IEEE, 2021, pp. 150–162.

[13] Y. Huang, X. Hu, N. Jia, X. Chen, Z. Zheng, and X. Luo, “Commtpst:
Deep learning source code for commenting positions prediction,” Jour-
nal of Systems and Software, vol. 170, p. 110754, 2020.

[14] A. Hasanpour, P. Farzi, A. Tehrani, and R. Akbari, “Software defect
prediction based on deep learning models: Performance study,” arXiv
preprint arXiv:2004.02589, 2020.

[15] J. Chen, K. Hu, Y. Yu, Z. Chen, Q. Xuan, Y. Liu, and V. Filkov,
“Software visualization and deep transfer learning for effective software
defect prediction,” in Proceedings of the ACM/IEEE 42nd International
Conference on Software Engineering, 2020, pp. 578–589.

[16] S. Wang, T. Liu, J. Nam, and L. Tan, “Deep semantic feature learning for
software defect prediction,” IEEE Transactions on Software Engineering,
vol. 46, no. 12, pp. 1267–1293, 2018.

[17] M. J. Islam, G. Nguyen, R. Pan, and H. Rajan, “A comprehensive study
on deep learning bug characteristics,” in Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, 2019, pp. 510–
520.

[18] M. Wen, R. Wu, and S.-C. Cheung, “How well do change sequences
predict defects? sequence learning from software changes,” IEEE Trans-
actions on Software Engineering, vol. 46, no. 11, pp. 1155–1175, 2018.
[19] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshyvanyk,
“On learning meaningful code changes via neural machine translation,”
in 2019 IEEE/ACM 41st International Conference on Software Engi-
neering (ICSE).

IEEE, 2019, pp. 25–36.

[20] S. Chakraborty, Y. Ding, M. Allamanis, and B. Ray, “Codit: Code
editing with tree-based neural models,” IEEE Transactions on Software
Engineering, 2020.

[21] T. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei, and L. Tan, “Coconut:
combining context-aware neural translation models using ensemble for
program repair,” in Proceedings of the 29th ACM SIGSOFT international
symposium on software testing and analysis, 2020, pp. 101–114.
[22] N. Jiang, T. Lutellier, and L. Tan, “Cure: Code-aware neural machine
translation for automatic program repair,” in 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE).
IEEE, 2021,
pp. 1161–1173.

[23] W. Tansey and E. Tilevich, “Annotation refactoring: inferring upgrade
transformations for legacy applications,” in Proceedings of the 23rd
ACM SIGPLAN conference on Object-oriented programming systems
languages and applications, 2008, pp. 295–312.

[24] N. Meng, L. Hua, M. Kim, and K. S. McKinley, “Does automated
refactoring obviate systematic editing?” in 2015 IEEE/ACM 37th IEEE
International Conference on Software Engineering, vol. 1.
IEEE, 2015,
pp. 392–402.

[25] X. Ge, Q. L. DuBose, and E. Murphy-Hill, “Reconciling manual
and automatic refactoring,” in 2012 34th International Conference on
Software Engineering (ICSE).

IEEE, 2012, pp. 211–221.

[26] H. A. Nguyen, T. T. Nguyen, G. Wilson Jr, A. T. Nguyen, M. Kim, and
T. N. Nguyen, “A graph-based approach to api usage adaptation,” ACM
Sigplan Notices, vol. 45, no. 10, pp. 302–321, 2010.

[27] A. T. Nguyen, M. Hilton, M. Codoban, H. A. Nguyen, L. Mast,
E. Rademacher, T. N. Nguyen, and D. Dig, “Api code recommendation
using statistical learning from ﬁne-grained changes,” in Proceedings of

the 2016 24th ACM SIGSOFT International Symposium on Foundations
of Software Engineering, 2016, pp. 511–522.

[28] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and
D. Poshyvanyk, “An empirical investigation into learning bug-ﬁxing
patches in the wild via neural machine translation,” in Proceedings of
the 33rd ACM/IEEE International Conference on Automated Software
Engineering, 2018, pp. 832–837.

[29] Z. Chen, S. J. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshyvanyk,
and M. Monperrus, “Sequencer: Sequence-to-sequence learning for end-
to-end program repair,” IEEE Transactions on Software Engineering,
2019.

[30] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,
A. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng, C. B. Clement,
D. Drain, N. Sundaresan, J. Yin, D. Jiang, and M. Zhou, “Graph-
codebert: Pre-training code representations with data ﬂow,” CoRR, vol.
abs/2009.08366, 2020.

[31] H. Tian, K. Liu, A. K. Kabor´e, A. Koyuncu, L. Li, J. Klein, and
T. F. Bissyand´e, “Evaluating representation learning of code changes
for predicting patch correctness in program repair,” in 35th IEEE/ACM
International Conference on Automated Software Engineering, ASE
2020, Melbourne, Australia, September 21-25, 2020.
IEEE, 2020, pp.
981–992.

[32] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and
D. Poshyvanyk, “An empirical study on learning bug-ﬁxing patches in
the wild via neural machine translation,” ACM Transactions on Software
Engineering and Methodology (TOSEM), vol. 28, no. 4, pp. 1–29, 2019.
[33] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” arXiv preprint arXiv:1409.0473,
2014.

[34] U. Alon, S. Brody, O. Levy, and E. Yahav, “code2seq: Generating
sequences from structured representations of code,” arXiv preprint
arXiv:1808.01400, 2018.

[35] A. N. Le, A. Martinez, A. Yoshimoto, and Y. Matsumoto, “Improving
sequence to sequence neural machine translation by utilizing syntactic
dependency information,” in Proceedings of the Eighth International
Joint Conference on Natural Language Processing, IJCNLP 2017,
Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long
Papers, G. Kondrak and T. Watanabe, Eds. Asian Federation of Natural
Language Processing, 2017, pp. 21–29.

[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” arXiv preprint
arXiv:1706.03762, 2017.

[37] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap,
“Compressive transformers for long-range sequence modelling,” in 8th
International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
[38] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,
S. Onta˜n´on, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed,
“Big bird: Transformers for longer sequences,” in Advances in Neural
Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
H. Lin, Eds., 2020.

[39] Y. Wang and H. Li, “Code completion by modeling ﬂattened abstract
syntax trees as graphs,” Proceedings of AAAIConference on Artiﬁcial
Intellegence, 2021.

[40] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative
position representations,” arXiv preprint arXiv:1803.02155, 2018.

[41] “Gerrit - android.” https://android-review.googlesource.com/.
[42] “Gerrit - goggle source.” https://gerrit-review.googlesource.com/.
[43] “Gerrit - ovirt.” https://gerrit.ovirt.org/.
[44] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[45] S. Bhatia, P. Kohli, and R. Singh, “Neuro-symbolic program corrector
for introductory programming assignments,” in 2018 IEEE/ACM 40th
International Conference on Software Engineering (ICSE).
IEEE, 2018,
pp. 60–70.

[46] E. Dinella, H. Dai, Z. Li, M. Naik, L. Song, and K. Wang, “Hoppity:
Learning graph transformations to detect and ﬁx bugs in programs,” in
International Conference on Learning Representations (ICLR), 2020.

[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[48] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv

preprint arXiv:1607.06450, 2016.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

15

[75] H. Tian, K. Liu, A. K. Kabor´e, A. Koyuncu, L. Li, J. Klein, and
T. F. Bissyand´e, “Evaluating representation learning of code changes for
predicting patch correctness in program repair,” in 2020 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE).
IEEE, 2020, pp. 981–992.

[76] V. Raychev, M. Sch¨afer, M. Sridharan, and M. Vechev, “Refactoring
with synthesis,” ACM SIGPLAN Notices, vol. 48, no. 10, pp. 339–354,
2013.

[77] M. Boshernitsan, S. L. Graham, and M. A. Hearst, “Aligning develop-
ment tools with the way programmers think about code changes,” in
Proceedings of the SIGCHI conference on Human factors in computing
systems, 2007, pp. 567–576.

[49] U. Z. Ahmed, P. Kumar, A. Karkare, P. Kar, and S. Gulwani, “Compila-
tion error repair: for the student programs, from the student programs,” in
Proceedings of the 40th International Conference on Software Engineer-
ing: Software Engineering Education and Training, 2018, pp. 78–87.

[50] D. van Bruggen, “Javaparser,” https://javaparser.org/about.html.
[51] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[52] P. Goyal, P. Doll´ar, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
A. Tulloch, Y. Jia, and K. He, “Accurate, large minibatch sgd: Training
imagenet in 1 hour,” arXiv preprint arXiv:1706.02677, 2017.

[53] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in Proceedings of the 40th
annual meeting of the Association for Computational Linguistics, 2002,
pp. 311–318.

[54] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,”

in Text summarization branches out, 2004, pp. 74–81.

[55] S. Liu, C. Gao, S. Chen, L. Y. Nie, and Y. Liu, “ATOM: commit
message generation based on abstract syntax tree and hybrid ranking,”
Transactions on Software Engineering, 2020.

[56] L. Y. Nie, C. Gao, Z. Zhong, W. Lam, Y. Liu, and Z. Xu, “Contex-
tualized code representation learning for commit message generation,”
Neurocomputing, 2021.

[57] C. Gao, W. Zhou, X. Xia, D. Lo, Q. Xie, and M. R. Lyu, “Automating
app review response generation based on contextual knowledge,” ACM
Transactions on Software Engineering and Methodology, 2020.

[58] “Gerrit .” https://www.gerritcodereview.com.
[59] Z. Sun, L. Li, Y. Liu, and X. Du, “On the importance of building
high-quality training datasets for neural code search,” arXiv preprint
arXiv:2202.06649, 2022.

[60] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural net-
works,” IEEE transactions on Signal Processing, vol. 45, no. 11, pp.
2673–2681, 1997.

[61] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.

[62] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.

[63] X. Liu, H.-F. Yu, I. Dhillon, and C.-J. Hsieh, “Learning to encode posi-
tion for transformer with continuous dynamical model,” in International
Conference on Machine Learning. PMLR, 2020, pp. 6327–6335.
[64] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser,
“Universal transformers,” arXiv preprint arXiv:1807.03819, 2018.
[65] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,
“Albert: A lite bert for self-supervised learning of language representa-
tions,” arXiv preprint arXiv:1909.11942, 2019.

[66] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le,
“Xlnet: Generalized autoregressive pretraining for language understand-
ing,” arXiv preprint arXiv:1906.08237, 2019.

[67] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdi-
nov, “Transformer-xl: Attentive language models beyond a ﬁxed-length
context,” arXiv preprint arXiv:1901.02860, 2019.

[68] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of
trans-
transformer,” arXiv preprint
fer learning with a uniﬁed text-to-text
arXiv:1910.10683, 2019.

[69] G. Ke, D. He, and T.-Y. Liu, “Rethinking the positional encoding in
language pre-training,” arXiv preprint arXiv:2006.15595, 2020.
[70] P. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert

with disentangled attention,” arXiv preprint arXiv:2006.03654, 2020.

[71] K. Liu, L. Li, A. Koyuncu, D. Kim, Z. Liu, J. Klein, and T. F.
Bissyand´e, “A critical review on the evaluation of automated program
repair systems,” Journal of Systems and Software, vol. 171, p. 110817,
2021.

[72] K. Liu, D. Kim, A. Koyuncu, L. Li, T. F. Bissyand´e, and Y. Le Traon,
“A closer look at real-world patches,” in 2018 IEEE International
Conference on Software Maintenance and Evolution (ICSME).
IEEE,
2018, pp. 275–286.

[73] S. Wang, K. Liu, B. Lin, L. Li, J. Klein, X. Mao, and T. F. Bissyand´e,
“Beep: Fine-grained ﬁx localization by learning to predict buggy code
elements,” arXiv preprint arXiv:2111.07739, 2021.

[74] X. Wang, Y. Wang, F. Mi, P. Zhou, Y. Wan, X. Liu, L. Li, H. Wu,
J. Liu, and X. Jiang, “Syncobert: Syntax-guided multi-modal contrastive
pre-training for code representation,” arXiv preprint arXiv:2108.04556,
2021.

