Predicting Issue Types with seBERT

Alexander Trautsch
alexander.trautsch@cs.uni-goettingen.de
University of Goettingen
Göttingen, Germany

Steﬀen Herbold
steﬀen.herbold@tu-clausthal.de
TU Clausthal
Clausthal-Zellerfeld, Germany

2
2
0
2

y
a
M
3

]
E
S
.
s
c
[

1
v
5
3
3
1
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Pre-trained transformer models are the current state-of-the-art for
natural language models processing. seBERT is such a model, that
was developed based on the BERT architecture, but trained from
scratch with software engineering data. We ﬁne-tuned this model
for the NLBSE challenge for the task of issue type prediction. Our
model dominates the baseline fastText for all three issue types in
both recall and precision to achieve an overall F1-score of 85.7%,
which is an increase of 4.1% over the baseline.

CCS CONCEPTS
• Software and its engineering → Software notations and
tools.

KEYWORDS
natural language processing, seBERT, BERT, issue type prediction

ACM Reference Format:
Alexander Trautsch and Steﬀen Herbold. 2022. Predicting Issue Types with
seBERT. In The 1st Intl. Workshop on Natural Language-based Software En-
gineering (NLBSE’22), May 21, 2022, Pittsburgh, PA, USA. ACM, New York,
NY, USA, 3 pages. https://doi.org/10.1145/3528588.3528661

1 INTRODUCTION
Accurate issue type prediction can support developers and researchers [4].
Within this paper, we show how the pre-trained transformer model
seBERT [11] can be used for this purpose as part of the NLBSE 2022
tool competition [6]. The seBERT model is a Natural Language
Processing (NLP) model that was trained from scratch for the use
within the software engineering domain, based on data from Stack
Overﬂow, Jira and GitHub. seBERT uses the BERTLARGE architec-
ture [2]. In comparison, the original BERT was trained for the gen-
eral domain and has sometimes problems with understanding words
within the software engineering context [11]. We already showed
on a diﬀerent data set that seBERT outperforms BERT and fast-
Text [3] for the identiﬁcation of bug issues [11]. We now trans-
fer these results to the larger context of issue type prediction as a
multi-class problem for the diﬀerentiation between the classes bug,
enhancement, and question.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
the author(s) must be honored. Abstracting with credit is permitted. To copy other-
wise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee. Request permissions from permissions@acm.org.
NLBSE’22, May 21, 2022, Pittsburgh, PA, USA
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9343-0/22/05. . . $15.00
https://doi.org/10.1145/3528588.3528661

Table 1: Overview of the amount of data for each class in the
training and test data.

Class
bug
enhancement
question
Total

#Development
361,238
299,287
62,373
722,899

#Test
40,152
33,290
7,076
80,518

2 ISSUE TYPE PREDICTION WITH SEBERT
We now describe the data we used, our model architecture and
training, the performance criteria we use, and the results we achieve.
Our code for the experiments is available online.1 We also provide
a live version of the model.2

2.1 Data and Preprocessing
The data we use consists of 722,899 issues collected from GitHub
as development data, and 80,518 issues for testing [6–8]. We have
the following information for each issue:3

• The label, which is one of the three classes we want to pre-

dict, i.e., bug, enhancement, and question.

• The title of the issue.
• The description of the issue, also referred to as body.

We concatenate the title and body of the issue. Then, we replace
line breaks with whitespaces and remove multiple subsequent whites-
paces. Moreover, we encode the label as an integer, such that bug is
zero, enhancement is one, and question is two. Moreover, we split
the development data into 80% training data and 20% validation
data.

2.2 Model Architecture and Fine-tuning
We use a transfer learning approach based on the ﬁne-tuning of
a pre-trained model. This means that we re-use a neural network
that was trained for a diﬀerent purpose. Figure 1 shows an overview
of our model which is based on the BertForSequenceClassification
implementation of HuggingFace.4

We use seBERT [11] as foundation for our approach, a pre-trained
BERT [2] model which is based on the transformer architecture [10].
In comparison to the BERT model of Google, seBERT was trained
from scratch based on data from the Software Engineering (SE) do-
main. Moreover, seBERT only has an input length of 128 tokens,5

1https://github.com/atrautsch/nlbse2022_replication_kit
2https://user.informatik.uni-goettingen.de/ trautsch2/nlbse2022/
3The data also contains the URL of the issue and repository, the time of creation, and
the author, but we do not use these ﬁelds.
4https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassiﬁcation
5Words are tokenized based on the vocabulary. Words within the vocabulary are used
as is, unknown words are represented by subword tokens.

 
 
 
 
 
 
NLBSE’22, May 21, 2022, Pittsburgh, PA, USA

Alexander Trautsch and Steﬀen Herbold

Result:

p(bug) p(enhancement) p(question)

Table 2: Performance of seBERT in comparison to the base-
line fastText.

Fully Connected (1024)

seBERT Output:

𝐶

𝑇1

...

𝑇𝑛

seBERT

Tokens:

Input:

[CLS]

𝑡1

...

𝑡𝑛

I think I found a bug!

Figure 1: Overview of the architecture of our model.

in comparison to the 512 tokens from the original BERT models.
Thus, our seBERT model only reasons about the ﬁrst 128 tokens,
i.e., only the title and the beginning of the issue text. BERT models
use the [CLS] token and the output 𝐶 as placeholder for a classiﬁ-
cation task. 𝐶 is a 768 dimensional vector, that we use as input for
one fully connected layer with 1024 neurons. The fully connected
layer learns a logistic regression over 𝐶 and computes the proba-
bilities of each class. We achieve this by using the cross-entropy
loss [1]. For more details regarding seBERT and BERT we refer the
to literature [2, 11].

We initialized our architecture with the weights of the pre-trained

seBERT model. We then trained the model for ﬁve epochs on the
training data. We did not freeze any weights, i.e., we did not just
learn the weights of the fully connected layer, but also allowed
updating the internal weights of seBERT. As optimizer, we used
AdamW [9] with a learning rate of 𝑙𝑟 = 5 · 10−5, 𝛽1 = 0.9 for the
moving average of the gradients, 𝛽2 = 0.999 for the moving aver-
age of the squared gradients, and no weight decay. We validated
each epoch on the validation set and use the model with the high-
est validation accuracy, which was already achieved after the ﬁrst
epoch. No further hyperparameter tuning was applied.

2.3 Performance Criteria
The performance criteria we use are based on 𝑡𝑝𝑐, 𝑓 𝑛𝑐 , and 𝑓 𝑝𝑐 ,
which are deﬁned as follows.

• 𝑡𝑝𝑐 are the true positive predictions of the class 𝑐 ∈ 𝐶 =
{bug, enhancement, question}. This means that the class 𝑐 is
predicted correctly.

• 𝑓 𝑛𝑐 are the false negative predictions of the class 𝑐. This
means that the class 𝑐 is missed and is instead predicted as
a diﬀerent class.

• 𝑓 𝑝𝑐 are the false positive predictions of class 𝑐. This means

that a diﬀerent class is wrongly predicted as class 𝑐.

Based on these values, we deﬁne our performance criteria for

each class recall𝑐, precision𝑐 , and F-Score𝑐 :

l bug

n bug

enhancement
question

enhancement
question

e bug

l
a
c
e
r

o
i
s
i
c
e
r
p

r
o
c
s
-
F

enhancement
question
micro average

fastText
81.6%
84.5%
35.0%
83.1%
81.6%
65.2%
83.1%
83.1%
45.6%
81.6%

seBERT Diﬀerence

90.6%
87.7%
48.7%
86.6%
86.4%
73.1%
88.6%
87.1%
58.4%
85.7%

+9.0%
+3.2%
+13.7%
+3.5%
+4.8%
+7.9%
+5.5%
+4.0%
+12.8%
+4.1%

recall𝑐 =

𝑡𝑝𝑐
𝑡𝑝𝑐 + 𝑓 𝑛𝑐

precision𝑐

F-Score𝑐 = 2 ·

=

𝑡𝑝𝑐
𝑡𝑝𝑐 + 𝑓 𝑝𝑐
recall𝑐 · precision𝑐
recall𝑐 + precision𝑐

(1)

(2)

(3)

The above criteria measure the performance for the individual
classes. We use the micro average6 of these scores for our overall
comparison of the performance:

micro average =

Í𝑐 ∈𝐶 𝑡𝑝𝑐
Í 𝑐 ∈ 𝐶𝑡𝑝𝑐 + 𝑓 𝑛𝑐

(4)

2.4 Results
Table 2 reports the performance of seBERT on the test data in com-
parison to the baseline model fastText. Overall, we achieve a micro
average score of 85.7%, an improvement of 4.1% over fastText. The
results show that seBERT is superior to the baseline in all crite-
ria, i.e., we are both better at correctly identifying issues of each
class (recall), as well as reducing the noise among these predictions
(precision). This means that the improvement is not at the cost of
predictive power for one class, but beneﬁts all classes. The largest
improvement of seBERT is for the class question, with an increase
of 12.8% in F-score. However, we note that seBERT still misses over
50% of the questions, i.e., there is still room for improvement. For
the classes bug and enhancement, the performance of seBERT is
much stronger, with F-scores of 88.6%, respectively 87.1%. However,
the diﬀerence to fastText is smaller, i.e., the improvement is only
by 5.5% for bug, respectively 4.0% for enhancement.

3 THREATS TO VALIDITY
There is one notable threat to the internal validity of our results,
due to the way seBERT was created. The corpus used for pre-training
included the Github Issues (including comments) from 2015 to 2019 [11].
The pre-training was self-supervised based on Masked-Language
Modelling (MLM) and Next Sentence Prediction (NSP), i.e., did not

6Please note that we use the micro average of the recall directly, since the micro aver-
ages of recall, precision, and F-score are equal. This is because Í𝑐 ∈𝐶 𝑓 𝑝𝑐 = Í𝑐 ∈𝐶 𝑓 𝑛𝑐 ,
which leads to the equality of the micro average of recall and precision. Since the F-
Score is their harmonic mean, it follows that is is also equal.

Predicting Issue Types with seBERT

NLBSE’22, May 21, 2022, Pittsburgh, PA, USA

include any information about the labels of the issues. However,
we cannot rule out the possibility that the issue type prediction
works better with data seen during the pre-training, because it is
more likely that the language structure is already known by the
model. Still, we believe that this threat is low: having seen the data
before also increases the risk of memorization instead of general-
ization, which would actually decrease the performance on the test
data.

4 CONCLUSION
We demonstrated the strong performance of the seBERT model for
the task of issue type prediction over smaller models like fastText
that were not pre-trained on a large corpus of data. While the per-
formance is still not perfect, especially with respect to the detec-
tion of questions, results from earlier work demonstrate that hu-
mans are also not perfect at this task and there is likely some label
noise [4, 5]. Still, there is room for improvement with pre-trained
models. seBERT uses only a short input size of 128 tokens, which
should be suﬃcient for most issues [11], but may lead to missing
vital information later in the issue for longer issues. Thus, models
that are able to consider longer inputs may yield further improve-
ments. For example, the recent Big Bird model is able to consider
inputs with up to 4096 tokens [12]. Moreover, we did not conduct
extensive hyperparameter tuning to ﬁnd the best possible training
conﬁguration (e.g., best learning rate) for seBERT, which means
we possibly underestimate the capabilities of seBERT.

REFERENCES
[1] Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein.
2005. A tutorial on the cross-entropy method. Annals of operations research 134,
1 (2005), 19–67.

[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
In Proceedings of the 2019 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers). Association for Computational Linguistics, Minneapolis,
Minnesota, 4171–4186. https://doi.org/10.18653/v1/N19-1423

[3] Facebook AI Research. 2019.

[4] Steﬀen Herbold, Alexander Trautsch,

fastText - Library for eﬃcient text classiﬁcation
and representation learning. https://fasttext.cc/. [accessed 14-November-2019].
2020.
On the feasibility of automated prediction of bug and non-bug is-
sues.
Empirical Software Engineering 25, 6 (Sept. 2020), 5333–5369.
https://doi.org/10.1007/s10664-020-09885-w

Fabian Trautsch.

and

[5] Kim Herzig, Sascha Just, and Andreas Zeller. 2013.

It’s Not a Bug, It’s a Fea-
ture: How Misclassiﬁcation Impacts Bug Prediction. In Proceedings of the 2013
International Conference on Software Engineering (ICSE). IEEE.

[6] Rafael Kallis, Oscar Chaparro, Andrea Di Sorbo, and Sebastiano Panichella. 2022.
NLBSE’22 Tool Competition. In Proceedings of The 1st International Workshop on
Natural Language-based Software Engineering (NLBSE’22).

[7] Rafael Kallis, Andrea Di Sorbo, Gerardo Canfora, and Sebastiano Panichella.
2019. Ticket Tagger: Machine Learning Driven Issue Classiﬁcation. In 2019 IEEE
International Conference on Software Maintenance and Evolution (ICSME). 406–
409. https://doi.org/10.1109/ICSME.2019.00070

[8] Rafael Kallis, Andrea Di Sorbo, Gerardo Canfora, and Sebastiano Panichella.
2021. Predicting issue types on GitHub. Science of Computer Programming 205
(2021), 102598. https://doi.org/10.1016/j.scico.2020.102598

[9] Ilya Loshchilov and Frank Hutter. 2019.

Decoupled Weight Decay
In International Conference on Learning Representations.

Regularization.
https://openreview.net/forum?id=Bkg6RiCqY7

[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Atten-
arXiv:1706.03762

Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
tion Is All You Need.
http://arxiv.org/abs/1706.03762

CoRR abs/1706.03762 (2017).

[11] Julian von der Mosel, Alexander Trautsch, and Steﬀen Herbold. 2021. On the
validity of pre-trained transformers for natural language processing in the soft-
ware engineering domain. arXiv:2109.04738 [cs.SE]

[12] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie,
Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qi-
Big Bird: Transform-
fan Wang, Li Yang, and Amr Ahmed. 2020.
ers
Information Pro-
In Advances
cessing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (Eds.), Vol. 33. Curran Associates,
Inc., 17283–17297.
https://proceedings.neurips.cc/paper/2020/ﬁle/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf

for Longer Sequences.

in Neural

