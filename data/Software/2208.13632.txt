Neuroevolution-Based Generation of Tests and Oracles for Games

Patric Feldmeier
University of Passau
Germany

Gordon Fraser
University of Passau
Germany

2
2
0
2

g
u
A
9
2

]
E
S
.
s
c
[

1
v
2
3
6
3
1
.
8
0
2
2
:
v
i
X
r
a

ABSTRACT
Game-like programs have become increasingly popular in many
software engineering domains such as mobile apps, web appli-
cations, or programming education. However, creating tests for
programs that have the purpose of challenging human players is a
daunting task for automatic test generators. Even if test generation
succeeds in finding a relevant sequence of events to exercise a pro-
gram, the randomized nature of games means that it may neither
be possible to reproduce the exact program behavior underlying
this sequence, nor to create test assertions checking if observed
randomized game behavior is correct. To overcome these problems,
we propose Neatest, a novel test generator based on the Neu-
roEvolution of Augmenting Topologies (NEAT) algorithm. Neatest
systematically explores a program’s statements, and creates neural
networks that operate the program in order to reliably reach each
statement—that is, Neatest learns to play the game in a way to
reliably cover different parts of the code. As the networks learn the
actual game behavior, they can also serve as test oracles by evaluat-
ing how surprising the observed behavior of a program under test
is compared to a supposedly correct version of the program. We
evaluate this approach in the context of Scratch, an educational
programming environment. Our empirical study on 25 non-trivial
Scratch games demonstrates that our approach can successfully
train neural networks that are not only far more resilient to ran-
dom influences than traditional test suites consisting of static input
sequences, but are also highly effective with an average mutation
score of more than 65%.

CCS CONCEPTS
• Software and its engineering → Software testing and debug-
ging; • Computing methodologies → Reinforcement learning.

KEYWORDS
Neuroevolution, Game Testing, Automated Testing, Scratch

ACM Reference Format:
Patric Feldmeier and Gordon Fraser. 2022. Neuroevolution-Based Genera-
tion of Tests and Oracles for Games. In 37th IEEE/ACM International Con-
ference on Automated Software Engineering (ASE ’22), October 10–14, 2022,
Rochester, MI, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.
1145/3551349.3556939

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASE 2022, 10-14 October, 2022, Ann Arbor
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9475-8/22/10. . . $15.00
https://doi.org/10.1145/3551349.3556939

(a) FruitCatching example game.

(b) Example of an optimized network with excluded regression head.
The width and brightness of connections represent their impor-
tance.

Figure 1: FruitCatching game and corresponding neural net-
work that is capable of winning the game.

1 INTRODUCTION
Game-like programs allow players to interact with them using spe-
cific input actions, and challenge them to solve dedicated tasks
while enforcing a set of rules. Once a player successfully completes
the task of a game, the game usually notifies the player of his/her
victory, leading to an advanced program state (winning state). Nowa-
days, game-like programs can be found in many software engineer-
ing domains. For example, the most common category of mobile
apps in Google’s Playstore1 as well as in Apple’s App Store2 are
games by far, and games are equally common in web and desk-
top applications. Programming education is also heavily reliant on
learners building game-like programs in order to engage them with
the challenges of programming. Fig. 1a shows the FruitCatching
game, a typical program created by young learners of the Scratch
programming language [39]: The player has to control a bowl at

1[August 2022] https://www.statista.com/statistics/279286/google-play-android-app-
categories/
2[August 2022] https://www.businessofapps.com/data/app-stores/

PunkteH18RightLeftBH21WaitAppleXH20AppleYAppleDirAppleSizeAppleBowlDistXAppleBowlDistYBowlXH19BowlYBowlDirBowlSize 
 
 
 
 
 
ASE 2022, 10-14 October, 2022, Ann Arbor

Patric Feldmeier and Gordon Fraser

the bottom of the screen using the cursor keys to catch as much
dropping fruit with the bowl as possible within a given time limit.
Games are intentionally designed to be challenging for human
players in order to keep them hooked and entertained. Unfortu-
nately, precisely this aspect also makes game-like programs very
difficult to test, and classical approaches to automated test genera-
tion will struggle to interact with games in a successful way [11]. In
Fig. 1a, a test generator would need to successfully catch all apples
for 30 seconds without dropping a single one in order to win the
game and reach the winning state.

While winning the game is out of question for a traditional auto-
mated test generator, it may nevertheless succeed in producing tests
for partial aspects of a game, such as catching an individual banana
or apple for the game shown in Fig. 1a. Such tests typically consist
of static, timed input sequences that are sent to the program under
test to reproduce a certain program execution path. This causes a
problem: Games are inherently randomized, and playing the same
game twice in a row may lead to entirely different scenarios. It may
be possible to instrument the test execution environment such that
the underlying random number generator produces a deterministic
sequence [38]. The Whisker [58] test framework for Scratch, for
example, allows setting the random number generator to a fixed
seed, such that the fruit in Fig. 1a always appears at the same loca-
tion in the same program. However, even the slightest change to the
program may affect the order in which the random number genera-
tor is queried, thus potentially invalidating the test. For example, if
the banana sprite is removed from the game in Fig. 1a, even though
the code of the apple sprite is unchanged the position at which the
apple spawns changes, as the order in which the random numbers
are consumed in the program is different, and as a result a test
intended to catch the apple sprite would likely drop the apple. Even
if by chance the test would succeed in catching the apple despite
the randomness, the exact program state would likely be different,
and an assertion on the exact position of the apple or bowl in Fig. 1a
would fail, rendering the use of classical test assertions impossible.
To tackle the test generation and oracle problem for game-like
programs, we propose the use of neural networks as dynamic tests.
For instance, Fig. 1b shows a network structure adapted to reaching
code related to winning the FruitCatching game (Fig. 1a). It uses
aspects of the program state such as sprite positions as input fea-
tures, and suggests one of the possible ways to interact with the
program. The architecture of this example network puts strong
emphasis on the horizontal distance between the bowl and the
apple which reflects that catching apples rather than bananas is
necessary to win the game, and indeed the network is able to win
the FruitCatching game by catching all apples for 30s. In contrast to
conventional test suites consisting of static input sequences, such
dynamic tests are capable of reacting to previously unseen program
states, which is particularly useful for game-like programs that are
heavily randomized. The same dynamic tests can also serve as test
oracles in a regression testing scenario by comparing node activa-
tions of a supposedly correct program version against a modified
one. For instance, Fig. 2 shows the activation distribution of node
H20 from Fig. 1b at time-step 50 when executed 100 times on a
correct version of FruitCatching. Since the most influential input
of node H20 is the horizontal distance between the bowl and the
apple, the network’s effort to catch the fruit can clearly be observed

Figure 2: Activation distribution of node H20 from Fig. 1b at
time-step 50 after 100 FruitCatching execution. Orange dots
correspond to activations from two faulty program versions.

from the high density around the node activation of zero. The two
example incorrect program versions (mutants) depicted as orange
dots disallow the player to move to the left or right, respectively,
resulting in suspicious activation values that will be reported as
invalid program behavior.

In this paper we introduce Neatest, an approach that imple-
ments this idea using neuroevolution [18] to automatically gener-
ate artificial neural networks that are able to test games reliably,
such as the network shown in Fig. 1b. Neatest targets games
created in Scratch [39], a popular block-based programming en-
vironment with over 90 million registered users and even more
shared projects at the time of this writing3 and is implemented on
top of the Whisker [58] testing framework for Scratch. Given a
Scratch program, Neatest iteratively targets statements based
on the control dependence graph of the program, and evolves net-
works (i.e., dynamic tests) to reach those statements guided by an
adapted reachability-related fitness function. Notably, by including
statements related to the winning state of a game, the networks
implicitly learn to meaningfully play and eventually win those
games without requiring any domain-specific knowledge about the
games. Using the dynamic tests it produces, Neatest introduces
a novel test oracle approach based on Surprise Adequacy [30] to
detect incorrect program behavior by analyzing the networks’ node
activations during the execution of a program under test.
In detail, the contributions of this paper are as follows:

• We propose Neatest, a novel test generation approach that
uses the NeuroEvolution of Augmenting Topologies (NEAT) [61]
technique to evolve artificial neural networks capable of test-
ing game-like programs reliably.

• Our testing tool implements this approach in the context
of the Scratch educational programming environment and
the Whisker test generator for Scratch.

• We empirically demonstrate that the proposed framework
is capable of producing tests for 25 Scratch games that are
robust against randomized program behavior.

• We empirically show through mutation analysis on 25 Scratch
games that the behavior of neural networks can serve as a
test oracle and detect erroneous program behavior.

3[August 2022] https://scratch.mit.edu/statistics

0.80.60.40.20.00.20.40.60.8Node Activation Values0123DensityIncorrect ProgramNeuroevolution-Based Generation of Tests and Oracles for Games

ASE 2022, 10-14 October, 2022, Ann Arbor

2 BACKGROUND
In this paper we combine the ideas of search-based software testing
with neuroevolution, considering the application domain of games
implemented in the Scratch programming language.

2.1 Search-based Software Testing
Search-based software testing (SBST) describes the process of ap-
plying meta-heuristic search algorithms, such as evolutionary al-
gorithms, to the task of generating program inputs. The search
algorithm is guided by a fitness function, which typically estimates
how close a test is towards reaching an individual coverage objec-
tive, or achieving 100% code coverage. The most common fitness
function derives this estimate as a combination of the distance be-
tween the execution path followed by an execution of the test and
the target statement in the control dependence graph (approach
level [66]), and the distance towards evaluating the last missed con-
trol dependency to the correct outcome (branch distance [34]). The
approach is applicable to different testing problems by varying the
representation used by the search algorithm. For example, SBST has
been used to evolve parameters for function calls [36], sequences
of method calls [5, 19, 62], sequences of GUI interactions [22, 41],
or calls to REST services [3]. The search only derives test inputs
to exercise a program under test, and test oracles are typically
added separately as regression assertions, which capture and assert
the behavior observed on the program from which the tests are
derived [20, 69]. Since the tests derived this way cannot adapt to
different program behavior, we call them static tests.

2.2 Neuroevolution
Artificial neural networks (ANNs) are computational models com-
posed of several interconnected processing units, so-called neurons.
An ANN can be interpreted as a directed graph where each node
corresponds to a neuron model, which usually implements a non-
linear static transfer function [14]. The edges of the graph, which
connect nodes with each other, are represented by weights that de-
termine the connection strength between two neurons. Altogether,
the number and type of neurons and their interconnections form
an artificial neural network’s topology or architecture.

Before an ANN can solve specific problems, it has to be trained
in the given problem domain. During training, learning algorithms
are used to find suitable network parameters, such as the weights
of the connections within a network. Learning algorithms com-
monly apply backpropagation [51], which updates the weights by
calculating the gradient for each connection individually. However,
backpropagation on its own is only capable of tuning the weights of
a neural network. Developing an appropriate network architecture
remains a tedious task for the developer. A promising approach
to eliminate these time-consuming tasks is to apply evolutionary
algorithms that can optimize both, the weights of networks and
their architecture.

Instead of using a conventional learning algorithm, neuroevolu-
tion encodes ANNs in genomes and gathers them in a population.
In each iteration, a subset of the current population is selected and
evolved using mutation and mating procedures inspired by the prin-
ciples of Darwinian evolution. Then, each genome’s performance is
evaluated using a fitness function, which reflects how close a given

genome is to solving the problem at hand. Finally, following the
paradigm of survival of the fittest, a new generation of the popula-
tion is formed by selecting the fittest genomes of the population. By
exploring the search space through many generations of mutating
and mating these genomes, it is expected that eventually a neural
network with optimal topology and weights for the given problem
will be found. With this learning approach the weights, the number
of neuron layers, and the interconnections between them, can be
genetically encoded and co-evolved at the same time.

The popular NeuroEvolution of Augmenting Topologies [61] (NEAT)
algorithm solves several challenges of Topology and Weight Evolv-
ing Artificial Neural Networks, such as the competing conventions
problem [49]. NEAT solves this problem by introducing innova-
tion numbers that enable the comparison of genes based on their
historical origin. Another crucial aspect of NEAT is the use of
speciation [21] to protect innovative networks. This is necessary
since innovative topologies tend to initially decrease the fitness
of a network, leading to architectural innovations having a hard
time surviving even a single generation. Speciation ensures that
neural networks with similar structures have to primarily compete
against other networks in the same species, which gives them time
to optimize their weights. Two networks are assigned to the same
species if the number of mismatching genes and the average con-
nection weight difference falls below a threshold, which is adjusted
in each generation of the algorithm based on the number of species
that are present in the current generation: if there are fewer species
than desired, the threshold is reduced (e.g., by 0.3) and vice versa.
NEAT’s mutation operators can be divided into two groups: struc-
tural mutations and non-structural mutations. The former modify
a network’s topological structure, while the latter operate on the
attributes of connection genes. The crossover operator randomly
selects two parents and aligns their connection genes using the
assigned innovation numbers. Then, both connection gene lists
are sequentially traversed, and certain rules are applied to each
gene pair: Similar genes are either inherited randomly from one
parent or combined by averaging the connection gene weights of
both parents. On the other hand, genes with differing innovation
numbers are always inherited from the more fit parent.

2.3 Testing Scratch Programs
The block-based programming language Scratch [39] provides pre-
defined command blocks encoding regular programming language
statements as well as domain-specific programming instructions
that make it easy to create games. Scratch programs are created by
visually arranging these command blocks according to the intended
program behavior. Scratch programs consist of a stage and a col-
lection of sprites. The former represents the application window
and displays a background image chosen by the user. Sprites, on
the other hand, are rendered as images on top of the stage and can
be manipulated by the user. Both the stage and the sprites contain
scripts formed by combining command blocks to define the func-
tional behavior of the program. Besides control-flow structures or
variables, the blocks represent high-level commands to manipulate
sprites. Scratch programs are controlled via user inputs such as
key presses, text, or mouse movement. To engage learners, the
Scratch programs they create are usually interactive games.

ASE 2022, 10-14 October, 2022, Ann Arbor

Patric Feldmeier and Gordon Fraser

Testing and dynamic analyses are important for Scratch pro-
grams: Even though the Scratch blocks ensure syntactic correct-
ness, functional errors are still feasible, raising the need for tests to
validate the desired behavior of programs. In particular, tests are a
prerequisite for effective feedback [6, 23, 28, 70] and hints [29] to
learners, as well as for automated assessment [1, 25]. Whisker [58]
is a framework that automates testing of Scratch programs by
sending inputs to Scratch programs and comparing the system’s
observed behavior against a predefined specification.

In order to discover as many system states as possible and thereby
enhance the chances of finding violated properties, it is crucial to
have a widespread set of test inputs. The Scratch inputs used to
stimulate the given program under test are assembled in the so-
called test harness. Whisker offers two types of test harnesses: a
manually written test harness and an automatically generated test
harness using search-based test generation approaches [11].

Conventional search-based test generation is often not powerful
enough to cover advanced program states, especially in game-like
projects [11]. In order to increase the effectiveness of automatic
test generation, we introduce a novel test generation approach by
extending the Whisker testing tool. The proposed test generator
uses neuroevolution to produce neural networks that are capable
of extensively exercising game-like programs.

3 TESTING WITH NEUROEVOLUTION
Traditional tests of static input sequences cannot react to deviating
program behavior properly and therefore fail to test randomized
programs reliably. As a robust alternative to static test suites, we
introduce Neatest, an extension of the Whisker test generation
framework [58] that is based on the NEAT [61] algorithm. Neatest
generates dynamic test suites consisting of neural networks that
are able to withstand and check randomized program behavior.

Algorithm 1 shows the test generation approach of Neatest,
which starts in line 5 with the selection of a target statement 𝑠𝑡
based on the CDG and a list of yet uncovered program statements
(Section 3.1). Next, a new population of networks is created depend-
ing on the search progress: If no statements have been covered
yet, the first population consists of networks where each sprite’s
feature group (Section 3.2.1) is fully connected to a single hidden
node that is connected to all output nodes. Otherwise, a new popu-
lation is produced by cloning and mutating networks that proved
to be viable solutions for previously targeted statements. By se-
lecting prior solutions, we focus on networks already capable of
reaching interesting program states, which may be prerequisites for
reaching more advanced states. To avoid unnecessary complex net-
work structures, a certain share (e.g., 10%) of every new population
consists of freshly generated networks. We require hidden layer
neurons to determine the correctness of a program (Section 3.4).

The following loop over all networks in line 9 starts with the play
loop in which a network simulates a human player by repeatedly
sending input events to a given Scratch program (Section 3.2.2).
The inputs for the networks are extracted from the current state
of the program, while the possible outputs are determined by the
actions a player can take in the game (Section 3.2.1).

After a network has finished its playthrough, lines 11 to 16 evalu-
ate how reliably the network can cover the current target statement

Algorithm 1: Neatest

input : control dependence graph 𝐶𝐷𝐺
input : list of uncovered program statements 𝑆
input : desired robustness count 𝑟𝑑
output : dynamic test suite 𝐷
1 function Neatest (𝐶𝐷𝐺, 𝑆, 𝑟𝑑 ):
2

𝑟𝑒𝑞𝑢𝑖𝑟𝑒𝑁 𝑒𝑥𝑡𝑆𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡 ← 𝑡𝑟𝑢𝑒;
while stopping condition not reached do
if 𝑟𝑒𝑞𝑢𝑖𝑟𝑒𝑁 𝑒𝑥𝑡𝑆𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡 then

𝑠𝑡 ← selectTargetStatement(𝐶𝐷𝐺, 𝑆);
𝑃 ← generatePopulation(𝐷);
𝑟𝑒𝑞𝑢𝑖𝑟𝑒𝑁 𝑒𝑥𝑡𝑆𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡 ← 𝑓 𝑎𝑙𝑠𝑒;

end
foreach network 𝑛 ∈ 𝑃 do
initiatePlayLoop(𝑛);
𝑓 ← calculateFitness(𝑛);
if 𝑓 == 1 then

// Statement was covered

𝑟𝑐 ← robustnessCheck(𝑛, 𝑟𝑑 );
𝑓 ← 𝑓 + 𝑟𝑐

end
𝑛𝑒𝑡𝑤𝑜𝑟𝑘.𝑓 𝑖𝑡𝑛𝑒𝑠𝑠 ← 𝑓 ;
if 𝑓 == 𝑟𝑑 then
𝐷 ← 𝐷 + 𝑛;
𝑆𝑢 ← 𝑆𝑢 − 𝑠𝑡 ;
𝑟𝑒𝑞𝑢𝑖𝑟𝑒𝑁 𝑒𝑥𝑡𝑆𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡 ← 𝑡𝑟𝑢𝑒

end

end
if !𝑟𝑒𝑞𝑢𝑖𝑟𝑒𝑁 𝑒𝑥𝑡𝑆𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡 then

𝑃 ← NEAT(𝑃);

end

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

end
return 𝐷

27
28 end

𝑠𝑡 (Section 3.3). If no network manages to cover 𝑠𝑡 with sufficient
robustness (line 17), the algorithm executes line 24 and evolves a
new generation of networks using the NEAT algorithm (Section 2.2).
This new generation is then the starting point for the next iteration
of the algorithm. Once a network has been optimized to cover 𝑠𝑡
reliably for a user-defined number of times 𝑟𝑑 , the network is added
as a test to the dynamic test suite, and the next target statement
is selected (lines 18–20). This process is repeated until the search
budget is exhausted or all statements have been covered, and the dy-
namic test suite is returned to the user. The dynamic tests serve not
only to exercise the programs, but also as test oracles for validating
the functionality in a regression testing scenario (Section 3.4).

3.1 Explorative Target Statement Selection
Neatest iteratively selects increasingly more difficult target state-
ments to guide the creation of incrementally more complex net-
works. In order to achieve this, targets are selected from the test
program’s control dependence graph (CDG) [9], a directed graph
that shows which program statements depend on which control

Neuroevolution-Based Generation of Tests and Oracles for Games

ASE 2022, 10-14 October, 2022, Ann Arbor

stuck trying to cover hard-to-reach statements while other, perhaps
easier-to-reach states are still present, Neatest changes its cur-
rently selected target after a user-defined number of consecutive
generations without improvement of fitness values.

3.2 Play Loop
Within the Play Loop, the networks govern the execution of a pro-
gram by repeatedly sending input events until the execution halts,
a pre-defined timeout has been reached, or the targeted statement
𝑠𝑡 was covered. To make a decision on which user event to select
at a given moment, we provide the networks with input features
extracted from the current program state. The selected event is then
sent to the Scratch program to advance the program execution.

3.2.1
Feature Extraction. Input features are fed to the input neu-
rons of networks and play a crucial role during a playthrough by
serving as the neural networks’ eyes. The features are collected by
filtering attributes of visible sprites by their relevance to the given
Scratch program, as determined by analyzing the source code:

• Position of a sprite on the Scratch canvas is always added
and defined through its x- and y-coordinates in the range
𝑥 ∈ [−240, 240] and 𝑦 ∈ [−180, 180].

• Size of a sprite is always added. The size bounds depend on
the currently selected costume (i.e., visual representation).
• Heading direction iff the rotation style is set to all around.
Defined through an angle 𝛼 in the range 𝛼 ∈ [−180, 180].
• Selected costume iff the sprite contains code to change its
costume. Defined as 𝑖 ∈ N, indexing the current costume.
• Distance to sensed sprite iff the sprite contains code that
checks for touching another sprite. Defined through the x
and y distance to the sensed sprite in the range [−600, 600].
• Distance to sensed color iff the sprite contains code that
checks for touching a given color. Colors are measured by
four rangefinders, which measure the distance on the canvas
to the sensed color in the directions [0, 90, 180, −90], with
0 representing the current heading direction of the sprite.
Admissible values are again restricted to [−600, 600].

In addition to relevant sprite attributes, we also add all program
variables, for example, the current score of a game, to the set of
input features. We add input nodes for all extracted features, and
group them by the sprite they belong to. Since we only include
relevant features for which corresponding code exists, the input
features may not only differ between games but also within the
same game, for example whenever a sprite creates a clone that has
its own attributes. However, neuroevolution allows the framework
to adapt to these input feature changes dynamically by adding
input nodes to the networks whenever a new behavioral pattern
is encountered. Finally, to ensure all extracted input features have
the same weight, they are normalized to the range [−1, 1].

Output features of a network resemble the set of user events
a human player can send to the Scratch application at a given
point in time. The framework simulates human inputs using the
following Scratch events:

• KeyPress: Presses a specific key on the keyboard for a pa-

rameterized amount of time.

• ClickSprite: Clicks on a specific sprite.

(a) Two Scratch scripts that implement the movement of the host-
ing sprite and a check if the hosting sprite touches a bear sprite.

(b) CDG for the depicted script with squared green and oval red
nodes representing covered and uncovered Scratch statements.

Figure 3: Example of a Scratch script and the correspond-
ing CDG.

locations. Since Scratch programs typically contain many small
scripts that communicate via events and messages, we combine the
scripts of a Scratch program into an interprocedural CDG that
covers the control flow of the entire program. For example, Fig. 3a
shows a Scratch program consisting of two scripts, which are
combined into one interprocedural CDG (Fig. 3b) by introducing
an artificial Entry node.

Target statements 𝑠𝑡 are selected from children of already cov-
ered program statements, as advanced states can only be reached if
previous control locations can be passed appropriately. If no state-
ments have been covered yet, the root node of the CDG (Entry)
is selected. In case more than one viable child exists, Neatest fa-
vors program statements that have accidentally been reached while
focusing on other program states but have not yet passed the ro-
bustness check. Considering the example of Fig. 3b and assuming
that the search was able to trigger the right key-press once without
covering the corresponding event handler reliably, the next target
statement would be set to the keyPressed:right statement. Other-
wise, if none of the three statements that have a directly covered
parent (say, stop, keyPressed:right) had been reached once, 𝑠𝑡 would
be randomly set to one of these three statements. To avoid getting

whenright arrowkeypressedmove10stepswhenclickedgotox-130y0foreveriftouchingBearthensayHello Bear!stopallEntryflagClickedkeyPressed:rightforevergoToxyif:touchingsaystopmoveStepsASE 2022, 10-14 October, 2022, Ann Arbor

Patric Feldmeier and Gordon Fraser

• ClickStage: Clicks on the stage.
• TypeText: Types text using the keyboard.
• MouseMove: Moves the mouse to a parameterized position.
• MouseMoveTo: Moves the mouse to a position inferred by

static analysis (e.g., location of another sprite).

• MouseDown: Presses the left mouse button for a parame-

terized amount of time.

• Sound: Sends simulated sound to the program.
• Wait: Waits for a parameterized amount of time.

Neatest analyzes the source code of all actively running scripts for
a given program state in order to select the subset of event types
for which active event handlers exist. After minimizing the set of
event types, we create one output node for each remaining event.
In case there are parameterized events, such as MouseMove events
requiring coordinates where the mouse pointer should be moved to,
the classification problem is further extended to a combination of a
classification and regression problem. These parameters are pro-
duced via a multi-headed network model [27] in which regression
nodes are mapped to parameterized events. Similar to input fea-
tures, the set of feasible output features and the number of required
output nodes may change throughout the course of a game.

3.2.2 Network Activation & Event Selection. The problem of de-
ciding which action to take at the current state of a game is cast
to a multi-class single-label classification problem. Classification
steps start by loading the input features of the current program
state into the corresponding input nodes. Then, the neural net-
work is activated in order to produce an output based on the fed
input features. During network activation, values residing in the
input nodes flow through the network in several timesteps. In ev-
ery timestep, each node adds up all the activation values from the
previous timestep and calculates the result of its own activation
using the tanh function [26] on the summed-up values.

To not perturb values gathered during feature extraction, all input
nodes send the normalized input features directly into the network
without activation functions. In a single timestep, activation only
flows from one neuron to the next, thus several network activations
are required for the input nodes’ signals to reach the output nodes.
After the input signal has reached the output nodes, a probability
distribution is calculated over the set of available events using the
traditional softmax function [15]. The classification task is extended
into a regression problem whenever a network selects a parame-
terized event. Parameters for the selected event are gathered by
querying the responsible nodes in the network’s regression head.
Selected events are passed on to the Scratch VM, where they
may trigger the activation of previously inactive scripts. Upon
receiving an input event, the Scratch VM executes a Scratch step
by sequentially executing blocks of currently active scripts until
either a block that halts the program execution or the end of the
script are reached. The Scratch step finishes by updating the state
of the VM to match the effects of the executed blocks. The playing
loop then starts anew by extracting the input and output features
of the updated VM.

3.3 Network Evaluation
Given a selected target statement 𝑠𝑡 , the evolution of networks is
guided by a fitness function 𝑓 which estimates how close a network

is to reaching 𝑠𝑡 and, once reached, how reliably it is executed. We
first calculate the objective function 𝑓𝑠𝑡 [11] that guides the search
towards reaching 𝑠𝑡 using a linear combination of:

• Approach level (AL): Integer defining the number of con-
trol dependencies between the immediate control depen-
dency of the target and the closest covered statement.

• Branch distance (BD): Indicates how close a network is to
satisfying the first missed control dependency. Zero if the
control location has been passed towards the target state-
ment. Scratch often allows for a precise definition of the
branch distance. For example, in Fig. 3b, the branch distance
for the if:touching statement is defined as the normalized
distance between the corresponding sprites.

• Control flow distance (CFD): As execution may stop in
the middle of a sequence of blocks (e.g., because of timed
statements), the CFG counts the number of blocks between
the targeted statement and the nearest covered statement.
To avoid deceiving fitness landscapes, we normalize the branch
distance and the control flow distance in the range [0, 1] using
the normalization function 𝛼 (𝑥) = 𝑥/(1 + 𝑥) [2], and multiply the
approach level with a factor of two:

𝑓𝑠𝑡 = 2 × 𝐴𝐿 + 𝛼 (𝐵𝐷) + 𝛼 (𝐶𝐹 𝐷)
In case 𝑓𝑠𝑡 > 0, the target statement was not covered and we set
the fitness to 𝑓 = 1/𝑓 𝑠𝑡 to create a maximization objective towards
covering 𝑠𝑡 . Otherwise (𝑓𝑠𝑡 = 0), the network managed to cover
the target once and we evaluate whether it can also cover the
same target in randomized program scenarios. For this purpose, we
generate up to 𝑟𝑑 − 1 random seeds, count for how many seeded
executions 𝑟𝑐 the network is able to reach the target statement again,
and finally set the fitness function to 𝑓 = 1 + 𝑟𝑐 . We decrement the
desired robustness count 𝑟𝑑 and increment the fitness value 𝑓 by
one to account for the first time 𝑠𝑡 was covered due to 𝑓𝑠𝑡 = 0.

The fitness function ensures that networks that are closer or
more robust in covering a specific statement are prioritized during
the evolution. If a network eventually passes the robustness check
(𝑓 = 𝑟𝑑 ), the network is added to the dynamic test suite as test
for target statement 𝑠𝑡 and the next target is selected by querying
the CDG as described in Section 3.1. Since covering one statement
usually leads to reaching other related statements as well, we si-
multaneously evaluate whether the network may also satisfy the
robustness check for other yet uncovered program statements.

3.4 Neural Networks as Test Oracles
In order to determine whether a test subject is erroneous, we ob-
serve how surprised a network behaves when confronted with the
program under test and regard highly surprising observations as
suspicious. As a metric, we use the Likelihood-based Surprise Ad-
equacy (LSA) [30], which uses kernel density estimation [65] to
determine how surprising the activations of selected neurons are
when the network is presented with novel inputs. In the application
scenario of regression testing, node activations originating from
a supposedly correct program version serve as the ground truth
and will be compared to activations occurring during the execu-
tion of the test subject. To incorporate many randomized program
scenarios, we collect the ground truth activations by executing the
networks on the sample program using a series of different random

Neuroevolution-Based Generation of Tests and Oracles for Games

ASE 2022, 10-14 October, 2022, Ann Arbor

seeds. Defective programs are then detected if the LSA value sur-
passes a pre-defined threshold, or for the special case that all ground
truth traces have a constant value, if the surprise value is above
zero since in this scenario already tiny deviations are suspicious.

Neatest only evaluates activation values occurring within hid-
den nodes because these nodes strike a nice balance between being
close to the raw inputs that represent the program state while al-
ready offering an abstraction layer by incorporating a network’s
behavior. This allows differentiating between suspicious and novel
program behavior, an essential characteristic for testing random-
ized programs that is missing in traditional static test assertions. For
example, in the FruitCatching game (Fig. 1a), the apple and bananas
may spawn at entirely new x-coordinates, but the program state is
still correct as long as they spawn at a certain height. Successfully
trained networks can differentiate between novel and suspicious
states because they are optimized to catch falling fruit by minimiz-
ing the distance between the bowl and the fruit, as shown by the
zero-centered activation distribution in Fig. 2. Since both example
defective programs, represented as orange dots, are far off the dis-
tribution center, they are rather surprising, with LSA values of 50
and 102. Therefore, given a threshold below these values, Neatest
correctly classifies these program versions as faulty.

In contrast to the original definition of LSA [30], we do not com-
pare groups of nodes that may reside in the same layer but compare
the activation values of hidden nodes directly. Furthermore, we
classify activation values by the Scratch step in which they oc-
curred. This precise approach of comparing node activations offers
two major advantages: First, we can detect tiny but nonetheless
suspicious program deviations that may only occur within a single
node by calculating the LSA value for each node separately. Sec-
ond, if a suspicious activation within a node was detected, we can
identify the exact point in time during the execution and the rough
reason for the suspicious observation by following the suspicious
node’s incoming connections back to the input nodes.

Changes to the structure of a network, which may occur during
an execution, can also provide strong evidence of suspicious pro-
gram behavior. For instance, output nodes may be added if a game
can process different user events, and input nodes may be added
for new sprite clones (cf. Section 3.2). Therefore, we can deduce
from a change in a network’s structure that the test subject exposes
diverging and maybe even erroneous program behavior.

4 EVALUATION
In order to evaluate the effectiveness of the proposed testing frame-
work we aim to answer the following three research questions:

• RQ1: Can Neatest optimize networks to exercise Scratch

games reliably?

• RQ2: Are dynamic test suites robust against randomized

program behavior?

• RQ3: Can neural networks serve as test oracles?

Neatest is integrated and available as part of the Whisker test-
ing framework4; the experiment dataset, parameter configurations,
raw results of the experiments, and scripts for reproducing the
experiments are publicly available on GitHub5.

4[August 2022] https://github.com/se2p/whisker
5[August 2022] https://github.com/FeldiPat/ASE22-Neatest-Artifact

Table 1: Evaluation games.

s
t
n
e
m
e
t
a
t
S

#

s
e
t
i
r
p
S

s
t
p
i
r
c
S

#

#

Project

4
3
4
3
2
10
10
4
4
6
8
4
13

10
18
10
7
3
12
27
14
10
33
29
4
48

FlappyParrot
Frogger
FruitCatching

69
76
82
68 HackAttack
25
LineUp
97 OceanCleanup
212
Pong
87 RioShootout
Snake
78
SnowballFight
381
163
SpaceOdyssey
91 WhackAMole
286 Mean

s
t
n
e
m
e
t
a
t
S
#

37
105
55
93
50
156
15
125
60
39
116
391
118.3

s
e
t
i
r
p
S
#

2
8
3
6
2
11
2
8
3
3
4
10
5.5

s
t
p
i
r
c
S

#

7
22
4
19
4
22
2
26
14
6
13
49
16.6

Project

BirdShooter
BrainGame
CatchTheDots
CatchTheGifts
CatchingApples
CityDefender
DessertRally
DieZauberlehrlinge
Dodgeball
Dragons
EndlessRunner
FallingStars
FinalFight

4.1 Dataset
To establish a diverse dataset for the evaluation, we collected 187
programs that represent the educational application domain of
Scratch from an introductory Scratch book6, prior work [58],
and four tutorial websites for children (Code Club7, Linz Coder
Dojo8, learnlearn9, junilearning10). Three selection criteria were
used to determine if a project included in this set of 187 programs
should be considered for evaluation: First, a given project has to re-
semble a game that processes user inputs supported by the Whisker
framework and challenges the player to achieve a specific goal while
enforcing a set of rules. Second, the game must include some form
of randomized behavior. Finally, we exclude games that do not have
a challenging winning state and can thus be easily covered just by
sending arbitrary inputs to the program. Applying all three selec-
tion criteria to the mentioned sources and skipping programs that
are similar to already collected ones in terms of player interaction
and the overall goal of a game resulted in a total of 25 programs
(including the FruitCatching game shown in Fig. 1a); statistics of
these games are shown in Table 1. Overall, each of the 25 games
offers unique challenges and ways of interacting with the program.

4.2 Methodology
All experiments were conducted on a dedicated computing cluster
containing nine nodes, with each cluster node representing one
AMD EPYC 7443P CPU running on 2.85 GHz. The Whisker testing
framework11 allows users to accelerate test executions using a cus-
tomizable acceleration factor. In order to speed up the experiments,
all conducted experiments used an acceleration factor of ten.

RQ1: We evaluate whether Neatest can train networks to cover
program states reliably by comparing the proposed approach against
a random test generation baseline. While Neatest produces dy-
namic tests in the form of neural networks, the random testing

6[August 2022] https://nostarch.com/catalog/scratch
7[August 2022] https://projects.raspberrypi.org/en/codeclub
8[August 2022] https://coderdojo-linz.github.io/uebungsanleitungen/programmieren/scratch/
9[August 2022] https://learnlearn.uk/scratch/
10[August 2022] https://junilearning.com/blog/coding-projects/
11[August 2022] https://github.com/se2p/whisker

ASE 2022, 10-14 October, 2022, Ann Arbor

Patric Feldmeier and Gordon Fraser

approach randomly selects input events from a subset of process-
able events and saves the chosen events as static test cases. Neatest
maintains a population of 300 networks, and both methods generate
tests until a search budget of 23 hours has been exhausted.

The combination of 300 networks per population and the search
budget of 23 hours allows the framework to conduct a reasonable
amount of evolutionary operations in order to sufficiently explore
the search space of the most challenging test subjects in the dataset.
However, for some test subjects, satisfying results may already be
found using a population size of 100 and a search budget of only
one hour. In general, the easier a winning state can be reached and
the fewer control dependencies a Scratch program has, the fewer
resources are required.

We set the number of non-improving generations after which
a target is switched to five because this value provides the evolu-
tion enough time to explore the search space while also avoiding
wasting too much time on difficult program states. To distribute
the 300 networks of a population across the species such that each
species can meaningfully evolve the weights of its networks, we
set the targeted species number to ten. For both test generators,
the maximum time for a single playthrough was set to ten sec-
onds, which translates to a play duration of up to 100 seconds due
to the acceleration factor of ten. The remaining neuroevolution
hyperparameters are set according to the original NEAT study [61].
As a metric of comparison, we consider coverage values across
30 experiment repetitions with the Vargha & Delaney ( ˆ𝐴12) effect
size [64] and the Mann-Whitney-U-test [40] with 𝛼 = 0.05 to de-
termine statistical significance. To evaluate whether Neatest can
train networks to reach challenging program states, we manually
identified statements representing a winning state for each pro-
gram and report how often this winning state was reached within
the 30 experiment repetitions. Since we are interested in tests that
are robust against randomized program behavior, we treat a state-
ment only as covered if a given test passes the robustness check
(Section 3.3) for ten randomly seeded program executions.

RQ2. In order to explore if the generated dynamic test suites are
truly robust against diverging program behavior, we randomly
extract for each program ten dynamic and static test suites that were
produced during RQ1. For the purpose of eliminating differences
originating from the test generation process, both test suite types
are extracted from the Neatest approach. The static test suites
correspond to the input sequences successful networks produced
when they entered the robustness check. All extracted test suites are
then executed on the corresponding programs using seeds different
from those during the test generation phase.

We evaluate the robustness of both test suite types by reporting
the difference in coverage between the test generation and test
execution phase and investigate the effectiveness of dynamic test
suites against static test suites by comparing the achieved coverage
and ˆ𝐴12 values. Similar to RQ1, we use the Mann-Whitney-U-test
to report statistically significant results. To match the application
scenario of test suites, we do not use robustness checks and treat a
statement as covered if it has been reached at least once. Within RQ2,
we compensate for random influences by applying every extracted
test suite to ten new random seeds, which results in 100 test runs
for each test suite type on every dataset project.

Table 2: Mutation operators.

Operator

Description

Replaces a block’s key listener.
Removes a single block.
Deletes all blocks of a given script.

Key Replacement Mutation (KRM)
Single Block Deletion (SBD)
Script Deletion Mutation (SDM)
Arithmetic Operator Replacement (AOR) Replaces an arithmetic operator.
Logical Operator Replacement (LOR)
Relational Operator Replacement (ROR)
Negate Conditional Mutation (NCM)
Variable Replacement Mutation (VRM)

Replaces a logical operator.
Replaces a relational operator.
Negates boolean blocks.
Replaces a variable.

RQ3. To answer the question if networks can serve as test oracles,
we extended Whisker with a mutation analysis feature implement-
ing the eight different mutation operators shown in Table 2, and
applied mutation analysis to all 25 projects. These mutation opera-
tors were selected based on the traditional set of sufficient mutation
operators [44]. After generating mutant programs, we execute the
dynamic test suites of RQ2 on the respective mutants and measure
how surprised the networks are by the modified programs. The
ground truth activation traces required for calculating the LSA are
generated by executing each test suite 100 times on the unmodified
program using randomly generated seeds. A program is marked
as mutant if the LSA value of a single node activation surpasses
an experimentally defined threshold of 30. Besides calculating the
LSA value, mutants are also killed if the network structure changes
while being executed on the mutant.

We answer RQ3 by reporting the distribution of killed mutants
across the eight applied operators. Furthermore, we verify that
the networks do not simply mark every program as a mutant by
calculating the false-positive rate on unmodified programs. Similar
to RQ2, we account for randomness within the mutant generation
and network execution process by repeating the experiment for
all extracted test suites ten times using varying seeds. To avoid an
explosion of program variations, each experiment repetition was
restricted to 50 randomly selected mutants per operator.

4.3 Threats to Validity
External Validity: The dataset of 25 projects covers a broad range
of game types and program complexities. Even though high prior-
ity was given to establish a dataset of games with many different
objectives and ways to interact with them, we cannot guarantee
that the results generalize well to other Scratch applications or
games developed in different programming languages.

Internal Validity: Randomized factors within the experiments
pose another threat to validity since repeated executions of the same
experiments will, by definition, lead to slightly different outcomes.
However, we expect 30 experiment repetitions for RQ1 and 100 test
results for each project in RQ2 and RQ3 to suffice for assuming
robust experiment results. The threshold that determines at which
surprise value a program is marked as a mutant was determined
experimentally and may not work well for other programs.

Construct Validity: For evaluating the effectiveness of Neatest
in generating robust test suites, we used block coverage which is
similar to statement coverage in text-based programming languages.
However, coverage is not always a good indicator because Scratch
programs may often already be covered by sending arbitrary inputs

Neuroevolution-Based Generation of Tests and Oracles for Games

ASE 2022, 10-14 October, 2022, Ann Arbor

Table 3: Reliable mean coverage, coverage effect size (A12),
and number of reached winning states (Wins) of the ran-
dom test generator (R) and Neatest (N) during test gener-
ation. Boldface indicates strong statistical significance with
𝑝 < 0.05.

Project

BirdShooter
BrainGame
CatchTheDots
CatchTheGifts
CatchingApples
CityDefender
DessertRally
DieZauberlehrlinge
Dodgeball
Dragons
EndlessRunner
FallingStars
FinalFight
FlappyParrot
Frogger
FruitCatching
HackAttack
LineUp
OceanCleanup
Pong
RioShootout
Snake
SnowballFight
SpaceOdyssey
WhackAMole

R

98.60
89.34
97.64
89.80
98.53
74.30
93.58
75.50
94.87
88.30
73.95
97.80
97.50
92.61
88.69
69.09
86.02
96.00
73.65
94.22
94.40
95.00
94.87
92.70
79.64

Coverage
N A12

100
100
100
100
100
70.62
93.90
92.17
96.15
89.01
90.52
100
99.65
100
95.74
100
97.38
100
78.61
100
94.40
95.00
96.32
95.17
80.44

0.98
0.95
0.82
0.67
0.68
0.28
0.58
1.00
1.00
0.61
0.90
1.00
1.00
1.00
1.00
1.00
1.00
1.00
0.69
0.72
0.50
0.50
0.78
0.67
0.54

𝑝

< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
0.02
< 0.01
< 0.01
0.17
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
0.01
< 0.01
1.00
1.00
< 0.01
< 0.01
0.62

Mean

89.07

94.60

0.80

-

Wins
R N

1
3
15
20
19
0
0
0
0
11
5
0
0
4
0
0
0
0
14
17
30
0
0
0
0

5

30
30
30
30
30
3
0
28
0
12
11
30
30
30
15
30
20
30
27
30
30
0
17
9
19

20

to the program. For exactly this reason, we focused on selecting
games that have a winning state and therefore pose a true challenge
to test generators. We answer whether networks can serve as test
oracles by reporting the ratio of killed mutants. However, not every
mutant must necessarily represent erroneous behavior and may
still be considered a valid program alternative by a human.

4.4 RQ1: Can Neatest Optimize Networks to
Exercise Scratch Games Reliably?

Our first research question evaluates whether neural networks
can be trained to exercise randomized Scratch programs reliably.
Table 3 summarizes the experiment results and shows that the ran-
dom tester and Neatest reach generally high coverage values of
89.07% and 94.60%. This is because the programs tend to be small in
terms of their number of statements. However, Scratch programs
consist of many domain-specific blocks specifically designed for
game-behavior, such that it is possible to implement fully functional
game-behavior with very few blocks, which in other programming
languages might require hundreds of lines of code. Blindly sending
inputs, as random testing does, leads to the execution of a large
share of these blocks without, however, actually playing the game.

Figure 4: Coverage over time across all experiment repeti-
tions and dataset programs with ▼ representing an achieved
total coverage value of 90%.

This can be seen when considering specifically how often an actual
winning state was reached. For example, in FallingStars both ap-
proaches achieve almost the same average coverage (97.80% and
100%), although only Neatest is able to cover statements related
to the challenging winning state. Table 3 shows that Neatest wins
the game, on average, 20 times, while the random test generator
has trouble meaningfully exercising the games and reaches the
winning state only five times per game. For three games, Neatest
fails to reach the winning state because either the allotted play
duration is too short (DessertRally), the game is too complex to
evolve sufficiently optimized networks using the experiment pa-
rameters (Snake), or more sophisticated evolution strategies are
required to solve advanced search-problems, such as the maze prob-
lem [52] (Dodgeball).

CityDefender is the only game for which Neatest achieves less
coverage than the random tester. The statements responsible for
these results are linked to relatively easy program states that the
random tester can easily reach through randomly firing inputs at
the program under test. While Neatest also sometimes performs
the required actions, the networks must be specifically trained to
send those inputs regardless of which randomized program state
they are confronted with. However, the relatively long duration of
a single playthrough in this game sometimes leads to experiment
repetitions in which the respective statements never get targeted.
The three reached winning states in CityDefender that correspond to
experiment repetitions in which a winning statement was actually
targeted demonstrates that using a more sophisticated statement
selection technique would allow Neatest to train robust networks
that reliably perform the required actions.

Having execution times of 23 hours may be recognized as a lim-
iting factor of the proposed approach. However, we selected this
relatively high search budget in order to sufficiently explore those
programs in the dataset that have exceptionally long fitness evalua-
tion durations. Fig. 4 illustrates the achieved reliable coverage over
time for both approaches across all evaluation subjects. The results
show that Neatest is more effective than random testing by cov-
ering more program statements at any stage of the search process.
Furthermore, the proposed approach covers 90% of all program

0200400600800100012001400Time in Minutes0.00.20.40.60.81.0Coverage in %RandomNeatestASE 2022, 10-14 October, 2022, Ann Arbor

Patric Feldmeier and Gordon Fraser

Table 4: Mean coverage and effect size (A12) during the test
execution phase of dynamic (D) and static (S) test suites. The
column Difference shows coverage differences between the
generation and execution phase. Values in boldface indicate
strong statistical significance with 𝑝 < 0.05.

Project

BirdShooter
BrainGame
CatchTheDots
CatchTheGifts
CatchingApples
CityDefender
DessertRally
DieZauberlehrlinge
Dodgeball
Dragons
EndlessRunner
FallingStars
FinalFight
FlappyParrot
Frogger
FruitCatching
HackAttack
LineUp
OceanCleanup
Pong
RioShootout
Snake

S

94.76
23.53
99.40
99.47
96.17
72.91
93.65
78.66
95.17
90.95
84.65
98.57
96.07
93.59
93.95
77.69
83.84
71.03
92.38
89.93
85.10
93.15

Coverage
D A12

99.59
95.76
100
100
100
81.02
93.97
94.92
95.83
91.02
94.92
99.69
99.69
96.41
95.72
96.70
92.06
99.46
98.51
98.00
96.67
95.26

0.91
1.00
0.75
0.56
0.67
0.87
0.62
1.00
0.64
0.53
0.97
0.85
0.99
0.79
0.78
0.93
0.75
1.00
0.85
0.79
0.96
0.79

𝑝

< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01

Difference

S

D

-5.24
-76.47
-0.60
-0.53
-3.83
2.29
-0.25
-13.51
-0.98
1.94
-5.87
-1.43
-3.58
-6.41
-1.79
-22.31
-13.54
-28.97
13.77
-10.07
-9.30
-1.85

-0.41
-4.24
0.00
0.00
0.00
10.40
0.07
2.75
-0.32
2.01
4.40
-0.31
0.04
-3.59
-0.02
-3.30
-5.32
-0.54
19.90
-2.00
2.27
0.26

Mean

86.43

95.83

0.82

-

-8.17

+1.22

statements in less than three hours (156 min), while the random
tester is not able to reach the same level of coverage at all. Moreover,
Neatest fully covers 5/25 projects in all 30 repetitions within the
first hour, indicating that the required search time is highly depen-
dent on the complexity of the game. In order to reduce the required
search time for the more challenging programs, advanced search
strategies, such as combining Neatest with backpropagation [7],
could be interesting avenues for future work.

4.5 RQ2: Are Dynamic Test Suites Robust

Against Randomized Program Behavior?
In order to evaluate the importance of training networks to reliably
cover statements, RQ2 extracts static event sequences from the
dynamic tests generated for RQ1, and compares them in terms
of the coverage variation observed on executions with different
random seeds. Each static test is the result of executing a dynamic
test on the program under test once. Hence, for the particular seed
of the random number generator on which the static test is derived,
the coverage between both test types is by construction identical.
Table 4 summarizes the effects of re-running the resulting tests
for ten different random seeds on the respective programs. The
results show that dynamic tests are robust against randomized

programs—coverage even increases slightly by 1.22% compared to
the test generation phase. In contrast, static tests have a drop of
8.17% in coverage, which is substantial considering the generally
high coverage observed in RQ1.

The slight increase in coverage for the dynamic test suites may
seem surprising, but can be explained by the way coverage is mea-
sured: In contrast to RQ1, which measured how many statements
were reliably reached by a test generator, here we count how many
statements are covered on average over the 100 test executions.
Thus, also statements that are only covered in some of the runs
contribute to the overall coverage. The drop of 8.17% coverage for
static tests demonstrates the need for robust test suites when test-
ing randomized programs. This clear advantage of dynamic tests
becomes even more apparent when looking at the ˆ𝐴12 values (on
average 0.82), which indicate that for every program in the dataset,
networks perform significantly better than static test sequences.

4.6 RQ3: Can Networks Serve as Test Oracles?
The randomized behavior of games impedes the generation of reli-
able test oracles. Hence, RQ3 aims to investigate if the networks
of dynamic tests can serve as test oracles that can distinguish ran-
domized program scenarios from faulty behavior. We generated
243835 mutants using eight different mutation operators across all
games in our dataset and checked whether the networks were able
to identify them. Table 5 demonstrates that the networks managed
to achieve an average mutation score of 65.62 % per project, while
maintaining a relatively low false-positive rate of 19.70 %.

Figure 5 indicates that the average false-positive rate is deceiving
as there are two extreme outliers that have a strong influence on
the mean, while the median is considerably lower with a value of
only 10%. In those two programs, namely Dragons and FinalFight,
the networks are confronted with many sprites having attributes
that are distinct in almost every program execution, resulting in
activation traces that are entirely different from previously observed
ones, and thus seem suspicious. This problem could be solved by
either increasing the number of recorded ground truth traces or
through a more sophisticated abstraction layer by restricting the
set of observed hidden nodes to nodes that reside in hidden layers
that are further away from the input layer.

Neatest detects the least mutants for CityDefender, which is a
direct consequence of the poorly trained networks (see Section 4.4).
The lowest mutation kill rate can be observed in Fig. 5 for the Single
Block Deletion mutation operator, because Scratch programs often
contain statements that do not change the program behavior but
instead offer purely visual effects. Depending on the scope of what
is recognized as faulty behavior, these visual deviations could be
detected by collecting more fine-grained input features, such as the
visual effects applied to the individual sprites. The highest mutation
kill rate was achieved for the Key Replacement Mutation operator
(99.18 %), in which an event handler’s observed key gets replaced
with a randomly selected one. In many of these cases, the mutant

ASE2022,10-14October,2022,AnnArborPatricFeldmeierandGordonFraserTable4:Meancoverageandeffectsize(A12)duringthetestexecutionphaseofdynamic(D)andstatic(S)testsuites.ThecolumnDifferenceshowscoveragedifferencesbetweenthegenerationandexecutionphase.Valuesinboldfaceindicatestrongstatisticalsignificancewith𝑝<0.05.CoverageDifferenceProjectSDA12𝑝SDBirdShooter94.7699.590.91<0.01-5.24-0.41BrainGame23.5395.761.00<0.01-76.47-4.24CatchTheDots99.401000.75<0.01-0.600.00CatchTheGifts99.471000.56<0.01-0.530.00CatchingApples96.171000.67<0.01-3.830.00CityDefender72.9181.020.87<0.012.2910.40DessertRally93.6593.970.62<0.01-0.250.07DieZauberlehrlinge78.6694.921.00<0.01-13.512.75Dodgeball95.1795.830.64<0.01-0.98-0.32Dragons90.9591.020.53<0.011.942.01EndlessRunner84.6594.920.97<0.01-5.874.40FallingStars98.5799.690.85<0.01-1.43-0.31FinalFight96.0799.690.99<0.01-3.580.04FlappyParrot93.5996.410.79<0.01-6.41-3.59Frogger93.9595.720.78<0.01-1.79-0.02FruitCatching77.6996.700.93<0.01-22.31-3.30HackAttack83.8492.060.75<0.01-13.54-5.32LineUp71.0399.461.00<0.01-28.97-0.54OceanCleanup92.3898.510.85<0.0113.7719.90Pong89.9398.000.79<0.01-10.07-2.00RioShootout85.1096.670.96<0.01-9.302.27Snake93.1595.260.79<0.01-1.850.26Mean86.4395.830.82--8.17+1.22testerisnotabletoreachthesamelevelofcoverageatall.Moreover,Neatestfullycovers5/25projectsinall30repetitionswithinthefirsthour,indicatingthattherequiredsearchtimeishighlydepen-dentonthecomplexityofthegame.Inordertoreducetherequiredsearchtimeforthemorechallengingprograms,advancedsearchstrategies,suchascombiningNeatestwithbackpropagation[7],couldbeinterestingavenuesforfuturework.SummaryRQ1:Neatestsucceedsinmasteringgameswithoutanydomainspecificknowledgebywinninggames20/30timesonaverage,achieving94.60%blockcoverage.4.5RQ2:AreDynamicTestSuitesRobustAgainstRandomizedProgramBehavior?Inordertoevaluatetheimportanceoftrainingnetworkstoreliablycoverstatements,RQ2extractsstaticeventsequencesfromthedynamictestsgeneratedforRQ1,andcomparesthemintermsofthecoveragevariationobservedonexecutionswithdifferentrandomseeds.Eachstatictestistheresultofexecutingadynamictestontheprogramundertestonce.Hence,fortheparticularseedoftherandomnumbergeneratoronwhichthestatictestisderived,thecoveragebetweenbothtesttypesisbyconstructionidentical.Table4summarizestheeffectsofre-runningtheresultingtestsfortendifferentrandomseedsontherespectiveprograms.Theresultsshowthatdynamictestsarerobustagainstrandomizedprograms—coverageevenincreasesslightlyby1.22%comparedtothetestgenerationphase.Incontrast,statictestshaveadropof8.17%incoverage,whichissubstantialconsideringthegenerallyhighcoverageobservedinRQ1.Theslightincreaseincoverageforthedynamictestsuitesmayseemsurprising,butcanbeexplainedbythewaycoverageismea-sured:IncontrasttoRQ1,whichmeasuredhowmanystatementswerereliablyreachedbyatestgenerator,herewecounthowmanystatementsarecoveredonaverageoverthe100testexecutions.Thus,alsostatementsthatareonlycoveredinsomeoftherunscontributetotheoverallcoverage.Thedropof8.17%coverageforstatictestsdemonstratestheneedforrobusttestsuiteswhentest-ingrandomizedprograms.Thisclearadvantageofdynamictestsbecomesevenmoreapparentwhenlookingattheˆ𝐴12values(onaverage0.82),whichindicatethatforeveryprograminthedataset,networksperformsignificantlybetterthanstatictestsequences.SummaryRQ2:Statictestsuiteshaveasignificantdropof8.17%incoverageonre-execution,whiledynamictestsuitesarerobustandlosenocoverage.4.6RQ3:CanNetworksServeasTestOracles?Therandomizedbehaviorofgamesimpedesthegenerationofreli-abletestoracles.Hence,RQ3aimstoinvestigateifthenetworksofdynamictestscanserveastestoraclesthatcandistinguishran-domizedprogramscenariosfromfaultybehavior.Wegenerated243835mutantsusingeightdifferentmutationoperatorsacrossallgamesinourdatasetandcheckedwhetherthenetworkswereabletoidentifythem.Table5demonstratesthatthenetworksmanagedtoachieveanaveragemutationscoreof65.62%perproject,whilemaintainingarelativelylowfalse-positiverateof19.70%.Figure5indicatesthattheaveragefalse-positiverateisdeceivingastherearetwoextremeoutliersthathaveastronginfluenceonthemean,whilethemedianisconsiderablylowerwithavalueofonly10%.Inthosetwoprograms,namelyDragonsandFinalFight,thenetworksareconfrontedwithmanyspriteshavingattributesthataredistinctinalmosteveryprogramexecution,resultinginactivationtracesthatareentirelydifferentfrompreviouslyobservedones,andthusseemsuspicious.Thisproblemcouldbesolvedbyeitherincreasingthenumberofrecordedgroundtruthtracesorthroughamoresophisticatedabstractionlayerbyrestrictingthesetofobservedhiddennodestonodesthatresideinhiddenlayersthatarefurtherawayfromtheinputlayer.NeatestdetectstheleastmutantsforCityDefender,whichisadirectconsequenceofthepoorlytrainednetworks(seeSection4.4).ThelowestmutationkillratecanbeobservedinFig.5fortheSingleBlockDeletionmutationoperator,becauseScratchprogramsoftencontainstatementsthatdonotchangetheprogrambehaviorbutinsteadofferpurelyvisualeffects.Dependingonthescopeofwhatisrecognizedasfaultybehavior,thesevisualdeviationscouldbedetectedbycollectingmorefine-grainedinputfeatures,suchasthevisualeffectsappliedtotheindividualsprites.ThehighestmutationkillratewasachievedfortheKeyReplacementMutationoperator(99.18%),inwhichaneventhandler’sobservedkeygetsreplacedwitharandomlyselectedone.Inmanyofthesecases,themutantASE2022,10-14October,2022,AnnArborPatricFeldmeierandGordonFraserTable4:Meancoverageandeffectsize(A12)duringthetestexecutionphaseofdynamic(D)andstatic(S)testsuites.ThecolumnDifferenceshowscoveragedifferencesbetweenthegenerationandexecutionphase.Valuesinboldfaceindicatestrongstatisticalsignificancewith𝑝<0.05.CoverageDifferenceProjectSDA12𝑝SDBirdShooter94.7699.590.91<0.01-5.24-0.41BrainGame23.5395.761.00<0.01-76.47-4.24CatchTheDots99.401000.75<0.01-0.600.00CatchTheGifts99.471000.56<0.01-0.530.00CatchingApples96.171000.67<0.01-3.830.00CityDefender72.9181.020.87<0.012.2910.40DessertRally93.6593.970.62<0.01-0.250.07DieZauberlehrlinge78.6694.921.00<0.01-13.512.75Dodgeball95.1795.830.64<0.01-0.98-0.32Dragons90.9591.020.53<0.011.942.01EndlessRunner84.6594.920.97<0.01-5.874.40FallingStars98.5799.690.85<0.01-1.43-0.31FinalFight96.0799.690.99<0.01-3.580.04FlappyParrot93.5996.410.79<0.01-6.41-3.59Frogger93.9595.720.78<0.01-1.79-0.02FruitCatching77.6996.700.93<0.01-22.31-3.30HackAttack83.8492.060.75<0.01-13.54-5.32LineUp71.0399.461.00<0.01-28.97-0.54OceanCleanup92.3898.510.85<0.0113.7719.90Pong89.9398.000.79<0.01-10.07-2.00RioShootout85.1096.670.96<0.01-9.302.27Snake93.1595.260.79<0.01-1.850.26Mean86.4395.830.82--8.17+1.22testerisnotabletoreachthesamelevelofcoverageatall.Moreover,Neatestfullycovers5/25projectsinall30repetitionswithinthefirsthour,indicatingthattherequiredsearchtimeishighlydepen-dentonthecomplexityofthegame.Inordertoreducetherequiredsearchtimeforthemorechallengingprograms,advancedsearchstrategies,suchascombiningNeatestwithbackpropagation[7],couldbeinterestingavenuesforfuturework.SummaryRQ1:Neatestsucceedsinmasteringgameswithoutanydomainspecificknowledgebywinninggames20/30timesonaverage,achieving94.60%blockcoverage.4.5RQ2:AreDynamicTestSuitesRobustAgainstRandomizedProgramBehavior?Inordertoevaluatetheimportanceoftrainingnetworkstoreliablycoverstatements,RQ2extractsstaticeventsequencesfromthedynamictestsgeneratedforRQ1,andcomparesthemintermsofthecoveragevariationobservedonexecutionswithdifferentrandomseeds.Eachstatictestistheresultofexecutingadynamictestontheprogramundertestonce.Hence,fortheparticularseedoftherandomnumbergeneratoronwhichthestatictestisderived,thecoveragebetweenbothtesttypesisbyconstructionidentical.Table4summarizestheeffectsofre-runningtheresultingtestsfortendifferentrandomseedsontherespectiveprograms.Theresultsshowthatdynamictestsarerobustagainstrandomizedprograms—coverageevenincreasesslightlyby1.22%comparedtothetestgenerationphase.Incontrast,statictestshaveadropof8.17%incoverage,whichissubstantialconsideringthegenerallyhighcoverageobservedinRQ1.Theslightincreaseincoverageforthedynamictestsuitesmayseemsurprising,butcanbeexplainedbythewaycoverageismea-sured:IncontrasttoRQ1,whichmeasuredhowmanystatementswerereliablyreachedbyatestgenerator,herewecounthowmanystatementsarecoveredonaverageoverthe100testexecutions.Thus,alsostatementsthatareonlycoveredinsomeoftherunscontributetotheoverallcoverage.Thedropof8.17%coverageforstatictestsdemonstratestheneedforrobusttestsuiteswhentest-ingrandomizedprograms.Thisclearadvantageofdynamictestsbecomesevenmoreapparentwhenlookingattheˆ𝐴12values(onaverage0.82),whichindicatethatforeveryprograminthedataset,networksperformsignificantlybetterthanstatictestsequences.SummaryRQ2:Statictestsuiteshaveasignificantdropof8.17%incoverageonre-execution,whiledynamictestsuitesarerobustandlosenocoverage.4.6RQ3:CanNetworksServeasTestOracles?Therandomizedbehaviorofgamesimpedesthegenerationofreli-abletestoracles.Hence,RQ3aimstoinvestigateifthenetworksofdynamictestscanserveastestoraclesthatcandistinguishran-domizedprogramscenariosfromfaultybehavior.Wegenerated243835mutantsusingeightdifferentmutationoperatorsacrossallgamesinourdatasetandcheckedwhetherthenetworkswereabletoidentifythem.Table5demonstratesthatthenetworksmanagedtoachieveanaveragemutationscoreof65.62%perproject,whilemaintainingarelativelylowfalse-positiverateof19.70%.Figure5indicatesthattheaveragefalse-positiverateisdeceivingastherearetwoextremeoutliersthathaveastronginfluenceonthemean,whilethemedianisconsiderablylowerwithavalueofonly10%.Inthosetwoprograms,namelyDragonsandFinalFight,thenetworksareconfrontedwithmanyspriteshavingattributesthataredistinctinalmosteveryprogramexecution,resultinginactivationtracesthatareentirelydifferentfrompreviouslyobservedones,andthusseemsuspicious.Thisproblemcouldbesolvedbyeitherincreasingthenumberofrecordedgroundtruthtracesorthroughamoresophisticatedabstractionlayerbyrestrictingthesetofobservedhiddennodestonodesthatresideinhiddenlayersthatarefurtherawayfromtheinputlayer.NeatestdetectstheleastmutantsforCityDefender,whichisadirectconsequenceofthepoorlytrainednetworks(seeSection4.4).ThelowestmutationkillratecanbeobservedinFig.5fortheSingleBlockDeletionmutationoperator,becauseScratchprogramsoftencontainstatementsthatdonotchangetheprogrambehaviorbutinsteadofferpurelyvisualeffects.Dependingonthescopeofwhatisrecognizedasfaultybehavior,thesevisualdeviationscouldbedetectedbycollectingmorefine-grainedinputfeatures,suchasthevisualeffectsappliedtotheindividualsprites.ThehighestmutationkillratewasachievedfortheKeyReplacementMutationoperator(99.18%),inwhichaneventhandler’sobservedkeygetsreplacedwitharandomlyselectedone.Inmanyofthesecases,themutantNeuroevolution-Based Generation of Tests and Oracles for Games

ASE 2022, 10-14 October, 2022, Ann Arbor

Table 5: Generated (capped at 50 mutants per operator and
game), killed and false-positive (FP) marked mutants.

d
e
t
a
r
e
n
e
G
#

7200
7800
9000
8000
2500
8900
14569
12397
7821
23838
13199
13162
20650

%
d
e
l
l
i

K

96.68
40.40
68.79
81.53
76.92
11.79
99.65
29.57
84.22
98.27
46.21
87.41
90.41

%
P
F

Project

FlappyParrot
Frogger
FruitCatching

22.00
4.00
0.00
0.00 HackAttack
LineUp
1.00
0.00 OceanCleanup
33.00
Pong
18.00 RioShootout
Snake
26.26
SnowballFight
99.00
SpaceOdyssey
5.00
13.13 WhackAMole
97.00 Mean

d
e
t
a
r
e
n
e
G
#

%
d
e
l
l
i

K

%
P
F

3800
14600
6279
8200
5600
10600
1300
10900
7000
4596
7500
14424
9753.40

38.58
92.97
66.22
40.44
98.61
63.17
34.00
54.43
49.21
70.39
42.37
78.31
65.62

0.00
2.00
13.19
3.00
10.00
44.00
10.00
16.00
3.00
5.00
26.00
42.00
19.70

Project

BirdShooter
BrainGame
CatchTheDots
CatchTheGifts
CatchingApples
CityDefender
DessertRally
DieZauberlehrlinge
Dodgeball
Dragons
EndlessRunner
FallingStars
FinalFight

Figure 5: Mutant kill rates across all mutation operators.

is detected due to the network’s dynamic adaption to changes in
the input and output space (cf. Section 3.2.1).

5 RELATED WORK
Due to the increasing popularity of NEAT, many variations have
been proposed [32, 50, 60, 67, 68] and applied to master games in
many different gaming environments, such as Atari [24] and real-
time based games [45, 59]. Since, to the best of our knowledge, no
previous work exists that uses neuroevolution for block-based pro-
gramming environments like Scratch, we applied the established
variant of NEAT, and will evaluate improvements in future work.
Many reinforcement learning techniques have been shown to be
capable of learning to play different game genres such as strategy
games [55, 56], arcade games [63], first-person shooter games [37,
42], and casual games [33]. We use neuroevolution rather than alter-
native reinforcement learning techniques since NEAT allows easy
adaptation to changes in the input space. Furthermore, prior rein-
forcement learning-based approaches reward an agent for achieving
game-specific objectives. In contrast, Neatest explores the source
code and automatically derives code-based objectives to cover spe-
cific target statements, thus implicitly learning to play the games.

The Likelihood-based Surprise Adequacy metric, which we use as
a test oracle, has previously been used for prioritizing test inputs
for image classification tasks [30]. Kim et al. [31] reduced the cost
of calculating the Surprise Adequacy by replacing the kernel density
estimation with the Mahalanobis distance [10]. In future research,
we will explore other metrics, such as the certainty of networks in
their predictions [16], and their usability as test oracles.

Nowadays, games are either tested manually [13, 17, 48] or
with semi-automated approaches [8, 53, 57]. Furthermore, many
approaches do not aim to detect bugs but rather to evaluate the
usability of games [12, 35, 54]. While recently automated game test-
ing has been explored [4, 46, 47, 71], overall the research area is still
young, and to the best of our knowledge no automated approaches
exist that generate tests using neuroevolution.

6 CONCLUSIONS
Games pose a considerable challenge for automated test generation
because they require the test generator to master a given game.
Furthermore, games tend to be heavily randomized to keep players
engaged, which prevents traditional static test suites from testing
such programs reliably, and static test assertions from differenti-
ating between novel and erroneous program states. In an attempt
to solve these challenges, this work combines search-based soft-
ware testing with neuroevolution to produce a novel type of test
suites: Dynamic test suites consist of neural networks trained via
neuroevolution to reach program states such as the winning state
reliably, regardless of the encountered program scenarios. By ob-
serving the network structure and measuring how surprised the
networks are by a given program state, the networks serve as test
oracles that detect incorrect program behavior. Our experiments
confirm that Neatest manages to master and test Scratch games.
The Neatest approach provides an initial foundation for com-
bining automated testing with neuroevolution. The Scratch games
we considered in our prototype are often similar to classic arcade
games, a common target for neuroevolution research. In principle,
Neatest is also applicable to other domains, such as Android or
iOS games, but adaptation will require engineering effort for fea-
ture extraction and implementing the play loop. Integrating recent
advances in neuroevolution, such as deep learning extensions of
NEAT [43], will be of interest in order to generalize further beyond
arcade games. Furthermore, it may be worth exploring if Neatest
provides a way to overcome the problems caused by randomized
program behavior outside the domain of games (i.e., flaky tests). Fi-
nally, the Surprise Adequacy metric originates from a highly active
area of research, and further research with new and alternative met-
rics may further improve the detection rates for erroneous program
behavior. Neatest and the hosting Whisker testing framework
are publicly available at: https://github.com/se2p/whisker

ACKNOWLEDGMENTS
This work is supported by DFG project FR2955/3-1 “TENDER-
BLOCK: Testing, Debugging, and Repairing Blocks-based Programs”
and the Federal Ministry of Education and Research through project
01JA2021 (primary::programming) as part of the “Qualitätsoffensive
Lehrerbildung”, a joint initiative of the Federal Government and the
Länder. The authors are responsible for this publication’s content.

OriginalKRMSBDSDMAORLORRORNCMVRM0.00.20.40.60.81.0Kill Rate in %0.100.030.261.000.660.370.820.720.540.940.970.710.870.410.980.690.350.900.770.580.910.710.480.91Neuroevolution-BasedGenerationofTestsandOraclesforGamesASE2022,10-14October,2022,AnnArborTable5:Generated(cappedat50mutantsperoperatorandgame),killedandfalse-positive(FP)markedmutants.Project#GeneratedKilled%FP%Project#GeneratedKilled%FP%BirdShooter720096.6822.00FlappyParrot380038.580.00BrainGame780040.404.00Frogger1460092.972.00CatchTheDots900068.790.00FruitCatching627966.2213.19CatchTheGifts800081.530.00HackAttack820040.443.00CatchingApples250076.921.00LineUp560098.6110.00CityDefender890011.790.00OceanCleanup1060063.1744.00DessertRally1456999.6533.00Pong130034.0010.00DieZauberlehrlinge1239729.5718.00RioShootout1090054.4316.00Dodgeball782184.2226.26Snake700049.213.00Dragons2383898.2799.00SnowballFight459670.395.00EndlessRunner1319946.215.00SpaceOdyssey750042.3726.00FallingStars1316287.4113.13WhackAMole1442478.3142.00FinalFight2065090.4197.00Mean9753.4065.6219.70OriginalKRMSBDSDMAORLORRORNCMVRM0.00.20.40.60.81.0Kill Rate in %0.100.030.261.000.660.370.820.720.540.940.970.710.870.410.980.690.350.900.770.580.910.710.480.91Figure5:Mutantkillratesacrossallmutationoperators.isdetectedduetothenetwork’sdynamicadaptiontochangesintheinputandoutputspace(cf.Section3.2.1).SummaryRQ3:Ahighmutationscoreof65.62%andalowfalse-positivemedianof10%showthatnetworkscandistinguishbetweennovelanderroneousbehaviorandmayserveassuitabletestoraclesforrandomizedprograms.5RELATEDWORKDuetotheincreasingpopularityofNEAT,manyvariationshavebeenproposed[32,50,60,67,68]andappliedtomastergamesinmanydifferentgamingenvironments,suchasAtari[24]andreal-timebasedgames[45,59].Since,tothebestofourknowledge,nopreviousworkexiststhatusesneuroevolutionforblock-basedpro-grammingenvironmentslikeScratch,weappliedtheestablishedvariantofNEAT,andwillevaluateimprovementsinfuturework.Manyreinforcementlearningtechniqueshavebeenshowntobecapableoflearningtoplaydifferentgamegenressuchasstrategygames[55,56],arcadegames[63],first-personshootergames[37,42],andcasualgames[33].Weuseneuroevolutionratherthanalter-nativereinforcementlearningtechniquessinceNEATallowseasyadaptationtochangesintheinputspace.Furthermore,priorrein-forcementlearning-basedapproachesrewardanagentforachievinggame-specificobjectives.Incontrast,Neatestexploresthesourcecodeandautomaticallyderivescode-basedobjectivestocoverspe-cifictargetstatements,thusimplicitlylearningtoplaythegames.TheLikelihood-basedSurpriseAdequacymetric,whichweuseasatestoracle,haspreviouslybeenusedforprioritizingtestinputsforimageclassificationtasks[30].Kimetal.[31]reducedthecostofcalculatingtheSurpriseAdequacybyreplacingthekerneldensityestimationwiththeMahalanobisdistance[10].Infutureresearch,wewillexploreothermetrics,suchasthecertaintyofnetworksintheirpredictions[16],andtheirusabilityastestoracles.Nowadays,gamesareeithertestedmanually[13,17,48]orwithsemi-automatedapproaches[8,53,57].Furthermore,manyapproachesdonotaimtodetectbugsbutrathertoevaluatetheusabilityofgames[12,35,54].Whilerecentlyautomatedgametest-inghasbeenexplored[4,46,47,71],overalltheresearchareaisstillyoung,andtothebestofourknowledgenoautomatedapproachesexistthatgeneratetestsusingneuroevolution.6CONCLUSIONSGamesposeaconsiderablechallengeforautomatedtestgenerationbecausetheyrequirethetestgeneratortomasteragivengame.Furthermore,gamestendtobeheavilyrandomizedtokeepplayersengaged,whichpreventstraditionalstatictestsuitesfromtestingsuchprogramsreliably,andstatictestassertionsfromdifferenti-atingbetweennovelanderroneousprogramstates.Inanattempttosolvethesechallenges,thisworkcombinessearch-basedsoft-waretestingwithneuroevolutiontoproduceanoveltypeoftestsuites:Dynamictestsuitesconsistofneuralnetworkstrainedvianeuroevolutiontoreachprogramstatessuchasthewinningstatereliably,regardlessoftheencounteredprogramscenarios.Byob-servingthenetworkstructureandmeasuringhowsurprisedthenetworksarebyagivenprogramstate,thenetworksserveastestoraclesthatdetectincorrectprogrambehavior.OurexperimentsconfirmthatNeatestmanagestomasterandtestScratchgames.TheNeatestapproachprovidesaninitialfoundationforcom-biningautomatedtestingwithneuroevolution.TheScratchgamesweconsideredinourprototypeareoftensimilartoclassicarcadegames,acommontargetforneuroevolutionresearch.Inprinciple,Neatestisalsoapplicabletootherdomains,suchasAndroidoriOSgames,butadaptationwillrequireengineeringeffortforfea-tureextractionandimplementingtheplayloop.Integratingrecentadvancesinneuroevolution,suchasdeeplearningextensionsofNEAT[43],willbeofinterestinordertogeneralizefurtherbeyondarcadegames.Furthermore,itmaybeworthexploringifNeatestprovidesawaytoovercometheproblemscausedbyrandomizedprogrambehavioroutsidethedomainofgames(i.e.,flakytests).Fi-nally,theSurpriseAdequacymetricoriginatesfromahighlyactiveareaofresearch,andfurtherresearchwithnewandalternativemet-ricsmayfurtherimprovethedetectionratesforerroneousprogrambehavior.NeatestandthehostingWhiskertestingframeworkarepubliclyavailableat:https://github.com/se2p/whiskerACKNOWLEDGMENTSThisworkissupportedbyDFGprojectFR2955/3-1“TENDER-BLOCK:Testing,Debugging,andRepairingBlocks-basedPrograms”andtheFederalMinistryofEducationandResearchthroughproject01JA2021(primary::programming)aspartofthe“QualitätsoffensiveLehrerbildung”,ajointinitiativeoftheFederalGovernmentandtheLänder.Theauthorsareresponsibleforthispublication’scontent.ASE 2022, 10-14 October, 2022, Ann Arbor

Patric Feldmeier and Gordon Fraser

REFERENCES
[1] Kirsti M Ala-Mutka. 2005. A survey of automated assessment approaches for
programming assignments. Computer science education 15, 2 (2005), 83–102.
[2] Andrea Arcuri. 2013. It really does matter how you normalize the branch distance
in search-based software testing. Software Testing, Verification and Reliability 23,
2 (2013), 119–147.

[3] Andrea Arcuri. 2019. RESTful API automated test case generation with EvoMaster.
ACM Transactions on Software Engineering and Methodology (TOSEM) 28, 1 (2019),
1–37.

[4] Sinan Ariyurek, Aysu Betin-Can, and Elif Surer. 2019. Automated video game
testing using synthetic and humanlike agents. IEEE Transactions on Games 13, 1
(2019), 50–67.

[5] Luciano Baresi, Pier Luca Lanzi, and Matteo Miraz. 2010. Testful: an evolutionary
test approach for java. In 2010 Third International Conference on Software Testing,
Verification and Validation. IEEE, 185–194.

[6] Kevin Buffardi and Stephen H Edwards. 2015. Reconsidering Automated Feedback:
A Test-Driven Approach. In Proceedings of the 46th ACM Technical Symposium
on Computer Science Education. 416–420.

[7] Lin Chen and Damminda Alahakoon. 2006. Neuroevolution of augmenting
topologies with learning for data classification. In 2006 International Conference
on Information and Automation. IEEE, 367–371.

[8] Chang-Sik Cho, Kang-Min Sohn, Chang-Jun Park, and Ji-Hoon Kang. 2010. Online
game testing using scenario-based control of massive virtual users. In 2010 The
12th International Conference on Advanced Communication Technology (ICACT),
Vol. 2. IEEE, 1676–1680.

[9] Ron Cytron, Jeanne Ferrante, Barry K Rosen, Mark N Wegman, and F Kenneth
Zadeck. 1991. Efficiently computing static single assignment form and the control
dependence graph. ACM Transactions on Programming Languages and Systems
(TOPLAS) 13, 4 (1991), 451–490.

[10] Roy De Maesschalck, Delphine Jouan-Rimbaud, and Désiré L Massart. 2000. The
mahalanobis distance. Chemometrics and intelligent laboratory systems 50, 1
(2000), 1–18.

[11] Adina Deiner, Patric Feldmeier, Gordon Fraser, Sebastian Schweikl, and Wengran
Wang. 2022. Automated Test Generation for Scratch Programs. arXiv preprint
arXiv:2202.06274 (2022).

[12] Heather Desurvire, Martin Caplan, and Jozsef A Toth. 2004. Using heuristics to
evaluate the playability of games. In CHI’04 extended abstracts on Human factors
in computing systems. 1509–1512.

[13] Norizan Mat Diah, Marina Ismail, Suzana Ahmad, and Mohd Khairulnizam Md
Dahari. 2010. Usability testing for educational computer game using observation
method. In 2010 international conference on information retrieval & knowledge
management (CAMP). IEEE, 157–161.

[14] Włodzisław Duch and Norbert Jankowski. 1999. Survey of Neural Transfer

Functions. Neural computing surveys 2, 1 (1999), 163–212.

[15] Rob A Dunne and Norm A Campbell. 1997. On the Pairing of the Softmax Acti-
vation and Cross-Entropy Penalty Functions and the Derivation of the Softmax
Activation Function. In Proc. 8th Aust. Conf. on the Neural Networks, Melbourne,
Vol. 181. Citeseer, 185.

[16] Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and Zhenyu
Chen. 2020. Deepgini: prioritizing massive tests to enhance the robustness of
deep neural networks. In Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis. 177–188.

[17] Xavier Ferre, Angelica de Antonio, Ricardo Imbert, and Nelson Medinilla. 2009.
Playability testing of web-based sport games with older children and teenagers.
In International Conference on Human-Computer Interaction. Springer, 315–324.
[18] Dario Floreano, Peter Dürr, and Claudio Mattiussi. 2008. Neuroevolution: From

Architectures to Learning. Evolutionary intelligence 1, 1 (2008), 47–62.

[19] Gordon Fraser and Andrea Arcuri. 2011. Evosuite: automatic test suite generation
for object-oriented software. In Proceedings of the 19th ACM SIGSOFT symposium
and the 13th European conference on Foundations of software engineering. 416–419.
[20] Gordon Fraser and Andreas Zeller. 2011. Mutation-driven generation of unit tests

and oracles. IEEE Transactions on Software Engineering 38, 2 (2011), 278–292.

[21] NN Glibovets and NM Gulayeva. 2013. A Review of Niching Genetic Algorithms
for Multimodal Function Optimization. Cybernetics and Systems Analysis 49, 6
(2013), 815–820.

[22] Florian Gross, Gordon Fraser, and Andreas Zeller. 2012. Search-based system
testing: high coverage, no false alarms. In Proceedings of the 2012 International
Symposium on Software Testing and Analysis. 67–77.

[23] Sumit Gulwani, Ivan Radiček, and Florian Zuleger. 2018. Automated Clustering
and Program Repair for Introductory Programming Assignments. ACM SIGPLAN
Notices 53, 4 (2018), 465–480.

[24] Matthew Hausknecht, Joel Lehman, Risto Miikkulainen, and Peter Stone. 2014.
A Neuroevolution Approach to General Atari Game Playing. IEEE Transactions
on Computational Intelligence and AI in Games 6, 4 (2014), 355–366.

[25] Petri Ihantola, Tuukka Ahoniemi, Ville Karavirta, and Otto Seppälä. 2010. Review
of recent systems for automatic assessment of programming assignments. In
Proceedings of the 10th Koli calling international conference on computing education
research. 86–93.

[26] Bekir Karlik and A Vehbi Olgac. 2011. Performance Analysis of Various Activation
Functions in Generalized MLP Architectures of Neural Networks. International
Journal of Artificial Intelligence and Expert Systems 1, 4 (2011), 111–122.

[27] Shruti Kaushik, Abhinav Choudhury, Nataraj Dasgupta, Sayee Natarajan, Larry A
Pickett, and Varun Dutt. 2020. Ensemble of Multi-headed Machine Learning
Architectures for Time-Series. Applications of Machine Learning (2020), 199.
[28] Hieke Keuning, Johan Jeuring, and Bastiaan Heeren. 2016. Towards a Systematic
Review of Automated Feedback Generation for Programming Exercises. In Pro-
ceedings of the 2016 ACM Conference on Innovation and Technology in Computer
Science Education. 41–46.

[29] Dohyeong Kim, Yonghwi Kwon, Peng Liu, I Luk Kim, David Mitchel Perry,
Xiangyu Zhang, and Gustavo Rodriguez-Rivera. 2016. Apex: Automatic Pro-
gramming Assignment Error Explanation. ACM SIGPLAN Notices 51, 10 (2016),
311–327.

[30] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testing using surprise adequacy. In 2019 IEEE/ACM 41st International Conference
on Software Engineering (ICSE). IEEE, 1039–1049.

[31] Jinhan Kim, Jeongil Ju, Robert Feldt, and Shin Yoo. 2020. Reducing dnn labelling
cost using surprise adequacy: An industrial case study for autonomous driving.
In Proceedings of the 28th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. 1466–
1476.

[32] Nate Kohl and Risto Miikkulainen. 2012. An Integrated Neuroevolutionary
IEEE Transactions on

Approach to Reactive Control and High-level Strategy.
Evolutionary Computation 16, 4 (2012), 472–488.

[33] Naoki Kondo and Kiminori Matsuzaki. 2019. Playing Game 2048 with Deep
Convolutional Neural Networks Trained by Supervised Learning. Journal of
Information Processing 27 (2019), 340–347.

[34] Bogdan Korel. 1990. Automated software test data generation. IEEE Transactions

on software engineering 16, 8 (1990), 870–879.

[35] Hannu Korhonen and Elina MI Koivisto. 2006. Playability heuristics for mobile
games. In Proceedings of the 8th conference on Human-computer interaction with
mobile devices and services. 9–16.

[36] Kiran Lakhotia, Mark Harman, and Hamilton Gross. 2010. AUSTIN: A tool for
search based software testing for the C language and its evaluation on deployed
automotive systems. In 2nd International symposium on search based software
engineering. IEEE, 101–110.

[37] Guillaume Lample and Devendra Singh Chaplot. 2017. Playing FPS Games
with Deep Reinforcement Learning. In Thirty-First AAAI Conference on Artificial
Intelligence.

[38] Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, and Darko Marinov. 2014. An
Empirical Analysis of Flaky Tests. In Proceedings of the 22nd ACM SIGSOFT
International Symposium on Foundations of Software Engineering. 643–653.
[39] John Maloney, Mitchel Resnick, Natalie Rusk, Brian Silverman, and Evelyn East-
mond. 2010. The Scratch Programming Language and Environment. ACM
Transactions on Computing Education (TOCE) 10, 4 (2010), 1–15.

[40] Henry B Mann and Donald R Whitney. 1947. On a Test of Whether One of
Two Random Variables is Stochastically Larger than the Other. The annals of
mathematical statistics (1947), 50–60.

[41] Ke Mao, Mark Harman, and Yue Jia. 2016. Sapienz: Multi-objective automated test-
ing for android applications. In Proceedings of the 25th International Symposium
on Software Testing and Analysis. 94–105.

[42] Michelle McPartland and Marcus Gallagher. 2010. Reinforcement Learning in
First Person Shooter Games. IEEE Transactions on Computational Intelligence and
AI in Games 3, 1 (2010), 43–56.

[43] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink,
Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy,
et al. 2019. Evolving deep neural networks. In Artificial intelligence in the age of
neural networks and brain computing. Elsevier, 293–312.

[44] A Jefferson Offutt, Ammei Lee, Gregg Rothermel, Roland H Untch, and Christian
Zapf. 1996. An experimental determination of sufficient mutant operators. ACM
Transactions on Software Engineering and Methodology (TOSEM) 5, 2 (1996), 99–
118.

[45] Jacob Kaae Olesen, Georgios N Yannakakis, and John Hallam. 2008. Real-Time
challenge balance in an RTS Game using rtNEAT. In 2008 IEEE Symposium On
Computational Intelligence and Games. IEEE, 87–94.

[46] Johannes Pfau, Jan David Smeddinck, and Rainer Malaka. 2017. Automated
Game Testing with Icarus: Intelligent Completion of Adventure Riddles via
Unsupervised Solving. In Extended abstracts publication of the annual symposium
on computer-human interaction in play. 153–164.

[47] Cristiano Politowski, Yann-Gaël Guéhéneuc, and Fabio Petrillo. 2022. To-
wards Automated Video Game Testing: Still a Long Way to Go. arXiv preprint
arXiv:2202.12777 (2022).

[48] Cristiano Politowski, Fabio Petrillo, and Yann-Gaël Guéhéneuc. 2021. A survey
of video game testing. In 2021 IEEE/ACM International Conference on Automation
of Software Test (AST). IEEE, 90–99.

[49] Nicholas J Radcliffe. 1993. Genetic Set Recombination and its Application to
Neural Network Topology Optimisation. Neural Computing & Applications 1, 1

Neuroevolution-Based Generation of Tests and Oracles for Games

ASE 2022, 10-14 October, 2022, Ann Arbor

(1993), 67–90.

(2009), 185–212.

[50] Aditya Rawal and Risto Miikkulainen. 2016. Evolving Deep LSTM-based Memory
Networks using an Information Maximization Objective. In Proceedings of the
Genetic and Evolutionary Computation Conference 2016. 501–508.

[51] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning
Representations by Back-Propagating Errors. nature 323, 6088 (1986), 533–536.
[52] Stefano Sarti, Jason Adair, and Gabriela Ochoa. 2022. Recombination and Novelty

in Neuroevolution: A Visual Analysis. SN Computer Science 3, 3 (2022), 1–15.

[53] Christopher Schaefer, Hyunsook Do, and Brian M Slator. 2013. Crushinator: A
framework towards game-independent testing. In 2013 28th IEEE/ACM Interna-
tional Conference on Automated Software Engineering (ASE). IEEE, 726–729.
[54] Yuchul Shin, Jaewon Kim, Kyohoon Jin, and Young Bin Kim. 2020. Playtesting in
Match 3 Game using Strategic Plays via Reinforcement Learning. IEEE Access 8
(2020), 51593–51600.

[55] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George
Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, et al. 2016. Mastering the Game of Go with Deep Neural
Networks and Tree Search. nature 529, 7587 (2016), 484–489.

[56] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel,
et al. 2018. A General Reinforcement Learning Algorithm that Masters Chess,
Shogi, and Go through Self-play. Science 362, 6419 (2018), 1140–1144.

[57] Adam Smith, Mark Nelson, and Michael Mateas. 2009. Computational support
for play testing game sketches. In Proceedings of the AAAI Conference on Artificial
Intelligence and Interactive Digital Entertainment, Vol. 5. 167–172.

[58] Andreas Stahlbauer, Marvin Kreis, and Gordon Fraser. 2019. Testing Scratch
Programs Automatically. Proceedings of the 2019 27th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering (2019), 165–175.

[59] Kenneth O Stanley, Bobby D Bryant, and Risto Miikkulainen. 2005. Real-time
Neuroevolution in the NERO Video Game. IEEE transactions on evolutionary
computation 9, 6 (2005), 653–668.

[60] Kenneth O Stanley, David B D’Ambrosio, and Jason Gauci. 2009. A Hypercube-
Based Encoding for Evolving Large-Scale Neural Networks. Artificial life 15, 2

[61] Kenneth O Stanley and Risto Miikkulainen. 2002. Evolving Neural Networks
through Augmenting Topologies. Evolutionary computation 10, 2 (2002), 99–127.
[62] Paolo Tonella. 2004. Evolutionary testing of classes. ACM SIGSOFT Software

Engineering Notes 29, 4 (2004), 119–128.

[63] Nikolaos Tziortziotis, Konstantinos Tziortziotis, and Konstantinos Blekas. 2014.
Play Ms. Pac-man Using an Advanced Reinforcement Learning Agent. In Hellenic
Conference on Artificial Intelligence. Springer, 71–83.

[64] András Vargha and Harold D Delaney. 2000. A Critique and Improvement of the
CL Common Language Effect Size Statistics of McGraw and Wong. Journal of
Educational and Behavioral Statistics 25, 2 (2000), 101–132.

[65] Matt P Wand and M Chris Jones. 1994. Kernel smoothing. CRC press.
[66] Joachim Wegener, André Baresel, and Harmen Sthamer. 2001. Evolutionary test
environment for automatic structural testing. Information and software technology
43, 14 (2001), 841–854.

[67] Shimon Whiteson and Peter Stone. 2006. On-line Evolutionary Computation for
Reinforcement Learning in Stochastic Domains. In Proceedings of the 8th annual
conference on Genetic and evolutionary computation. 1577–1584.

[68] Shimon Whiteson, Peter Stone, Kenneth O Stanley, Risto Miikkulainen, and Nate
Kohl. 2005. Automatic Feature Selection in Neuroevolution. In Proceedings of the
7th annual conference on Genetic and evolutionary computation. 1225–1232.
[69] Tao Xie. 2006. Augmenting automatically generated unit-test suites with regres-
sion oracle checking. In European Conference on Object-Oriented Programming.
Springer, 380–403.

[70] Jooyong Yi, Umair Z Ahmed, Amey Karkare, Shin Hwei Tan, and Abhik Roy-
choudhury. 2017. A Feasibility Study of Using Automated Program Repair for
Introductory Programming Assignments. In Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering. 740–751.

[71] Yan Zheng, Xiaofei Xie, Ting Su, Lei Ma, Jianye Hao, Zhaopeng Meng, Yang
Liu, Ruimin Shen, Yingfeng Chen, and Changjie Fan. 2019. Wuji: Automatic
online combat game testing using evolutionary deep reinforcement learning. In
2019 34th IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE, 772–784.

