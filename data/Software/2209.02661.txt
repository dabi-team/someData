Hardware Software Co-design of Statistical and
Deep Learning Frameworks for Wideband Sensing
on Zynq System on Chip

Rohith Rajesh, Sumit J. Darak, Akshay Jain, Shivam Chandhok, and Animesh Sharma

1

2
2
0
2

p
e
S
6

]
P
S
.
s
s
e
e
[

1
v
1
6
6
2
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—With the introduction of spectrum sharing and het-
erogeneous services in next-generation networks, the base stations
need to sense the wideband spectrum and identify the spectrum
resources to meet the quality-of-service, bandwidth, and latency
constraints. Sub-Nyquist sampling (SNS) enables digitization
for sparse wideband spectrum without needing Nyquist speed
analog-to-digital converters. However, SNS demands additional
signal processing algorithms for spectrum reconstruction, such
as the well-known orthogonal matching pursuit (OMP) algo-
rithm. OMP is also widely used in other compressed sensing
applications. The ﬁrst contribution of this work is eﬃciently
mapping the OMP algorithm on the Zynq system-on-chip (ZSoC)
consisting of an ARM processor and FPGA. Experimental
analysis shows a signiﬁcant degradation in OMP performance for
sparse spectrum. Also, OMP needs prior knowledge of spectrum
sparsity. We address these challenges via deep-learning-based
architectures and eﬃciently map them on the ZSoC platform as
second contribution. Via hardware-software co-design, diﬀerent
versions of the proposed architecture obtained by partitioning
between software (ARM processor) and hardware (FPGA) are
considered. The resource, power, and execution time comparisons
for given memory constraints and a wide range of word lengths
are presented for these architectures.

Index Terms—Deep learning, Hardware software co-design,
Convolutional Neural Network, Zynq System-on-chip, Sub-
Nyquist Sampling, Orthogonal Matching Pursuit.

I. Introduction
Wideband spectrum sensing (WSS) involves digitizing the
wideband spectrum and identifying available spectrum for
resource allocation in wireless networks [1]–[3]. WSS has
gained signiﬁcant importance in 5G and next-generation wire-
less networks due to the introduction of spectrum sharing
policies replacing the conventional static spectrum allocation
policies [4]. Spectrum sharing allows the deployment of the
wireless networks in licensed, shared, and unlicensed spectrum
[5]. The advantages include signiﬁcant cost savings for Tele-
com service operators since they can reduce licensed spectrum
requirement and corresponding exorbitant spectrum license
fee, which is usually in billions of dollars [6]. A broader
spectrum enables the deployment of upcoming heterogeneous
services that demand a wide range of bandwidth, quality-of-
service, and latency constraints. Though the millimeter wave
spectrum (above 6 GHz) is being explored due to the avail-
ability of large bandwidth, various studies have conﬁrmed that

This work is supported by the funding received from core research grant

(CRG) awarded to Dr. Sumit J. Darak from DST-SERB, GoI.

Rohith Rajesh, Sumit J. Darak and Animesh Sharma are with Elec-
tronics and Communications Department, IIIT-Delhi, India-110020 (e-mail:
{rohith18182,sumit,animesh20317}@iiitd.ac.in

Akshay Jain is associated with AMD, Hyderabad, India and Electronics and

Communications Department, IIIT-Delhi, India. email: akshayj@amd.com
Shivam Chandhok is with INRIA, Universite Grenoble Alpes, France

it is not a feasible alternative to the sub-6 GHz spectrum for
reliable outdoor communication and comprehensive network
coverage.

Numerous spectrum measurement and utilization studies
have shown that overall utilization of the sub-6 GHz spectrum
is poor even though most of the spectrum is licensed [7]–
[9]. For digitization of such a sparse wideband spectrum,
sub-Nyquist sampling (SNS) based WSS is an eﬃcient and
feasible alternative to Nyquist sampling based WSS. This
is because SNS needs low-speed analog-to-digital converters
(ADCs) compared to Nyquist rate ADCs in the latter [2], [10].
However, SNS-based WSS needs additional digital signal pro-
cessing to recover the SNS sampled spectrum so that it closely
resembles the original spectrum [11], [12]. Such recovery must
be done accurately and should meet the stringent area, power,
and latency constraints.

Various signal recovery techniques for SNS have been re-
viewed in [13]. The greedy-approach-based orthogonal match-
ing pursuit (OMP) framework is popular due to its lower
complexity and faster execution time. Other applications of
OMP includes image processing, and radar systems [14]–
[17]. Recently, advances in deep learning (DL) have been
explored to improve the performance of the OMP [18]–[20].
Most of these approaches augment the OMP by replacing
the matched ﬁltering task with the DL. The OMP and its
variants suﬀer from a signiﬁcant degradation in reconstruction
performance, especially when the spectrum sparsity is high.
Also, they need prior knowledge of spectrum sparsity which
may not be available in a dynamic environment. From an
architecture perspective, in 5G and next-generation wireless
networks, distributed base station approach where radio unit
(RU), distributed unit (DU), and central unit (CU) may not be
co-located is being explored [21], [22]. Such deployment de-
mands the mapping of SNS algorithms on hardware platforms
such as system on chip (SoC) under the limited computing
resource constraints due to remote locations of the RU and
DU.

The ﬁrst contribution of this work is to eﬃciently map
statistical signal processing based OMP framework for SNS
spectrum recovery on Zynq system-on-chip (ZSoC). The ZSoC
is the heterogeneous SoC comprising of software (SW), i.e.,
dual-core Cortex ARM A9 processor, and hardware (HW), i.e.,
7-series ﬁeld programmable gate array (FPGA). To address
the drawbacks of OMP and make the spectrum reconstruction
agnostic to sparsity, we replace OMP with a convolutional
neural network (CNN) based deep learning (DL) architecture
and eﬃciently map it on the ZSoC platform as the second
contribution. Via hardware-software co-design, we explore
diﬀerent versions of the proposed DL architecture obtained by

 
 
 
 
 
 
partitioning between SW (ARM processor) and HW (FPGA).
Our study oﬀers interesting insights which may not be visible
in conventional theoretical and simulation-based analysis. For
these architectures, we present the resource utilization, power
consumption, and execution time comparisons for given mem-
ory constraints and a wide range of word lengths (WL) at
the parameters and computation levels. We develop an end-to-
end application with a live graphical user interface (GUI) to
demonstrate the real-time WSS on the ZSoC platform. Please
refer to [23] for source codes, hardware IPs, datasets and
detailed tutorial used for generating the results presented in
this paper.

The rest of the paper is organized as follows. We discuss
the OMP architecture and experimental results in Section II.
The DLWSS algorithm is presented in Section III and cor-
responding architecture in Section IV. Section V presents
the performance analysis results and comparison of DLWSS
with OMP followed by complexity analysis in Section VI.
Section VII concludes the paper.

II. Spectrum Recovery via OMP for SNS based WSS
This section discusses the design and implementation of the
OMP on the ZSoC. We demonstrate the various drawbacks of
OMP via experimental analysis and results.

A. Orthogonal Matching Pursuit (OMP) on ZSoC

OMP [24]–[26] is one of the most widely used sparse
recovery algorithm for SNS based WSS. It follows an iterative
formulation where an occupied band that is highly correlated
with the residual matrix is identiﬁed. Then its contribution
is removed from the residual matrix to identify the next
highly correlated occupied band. This process is repeated for
all occupied bands when we have prior knowledge of the
spectrum sparsity. Otherwise, we need to have a stopping
criterion based on estimated sparsity. The OMP pseudo-code is
given in Algorithm 1. Here, K represents the number of ADCs
used for SNS, Q is the number of snapshots produced by the
ADC, and N is the number of frequency bands in the wideband
spectrum. Further, A is the sensing matrix of dimension K × N,
Y are the received SNS samples of dimension N × Q, and
† represents a matrix pseudo-inverse. It comprises four main
steps, i.e., 1) Matching (Line 6-8); 2) Identiﬁcation (Line 9-
13); 3) Least squares (Line 14) and 4) Approximation (Line
15).

We have realized the OMP in Algorithm 1 on ZSoC ZC706
comprising of Dual ARM Cortex A9 processor and 7-series
FPGA with 1090 units of 18kB Block RAMs, 80 DSP48E
units, 218600 units of 6-input
look-up-tables (LUTs) and
437200 ﬂip-ﬂops (FFs). Table I shows the execution time,
resource utilization, and power consumption comparison for
the two best possible realizations of the OMP algorithm: 1)
Only software (ARM), and 2) Software and hardware co-
design (ARM+FPGA). Note that we have optimized the code
for software implementation and carefully chosen the word
length on hardware to minimize the execution time, power
consumption, and resource utilization without compromising
the functional accuracy. Interestingly, FPGA realization is
slower than ARM processors due to the sequential nature
of the OMP algorithm and the need for variable size matrix
inversion operations. On FPGA, we need to implement single

2

Algorithm 1 OMP algorithm for WSS
Require: Sensing matrix A[K][N], aliased

sub-Nyquist samples Y [K] [Q], Sparsity S

#Normalize A
#Initialize Residual

#No. of occupied bands

Z[j] ← norm(Anorm[ : , j ]T × Res)

for j in columns(Anorm) do

1: occupied bands ← []
2: Anorm ← Column normalized A
3: Res ← Y
4: iter ← 1
5: while iter ≤ S do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end while
16: return occupied bands

end for
Res ← Res − As × (As† × Res)
iter ← iter + 1

end for
Append argmax(Z) to occupied bands
for j in occupied bands do

Append column Anorm[ : , j ] to As

TABLE I: Comparison of execution time, resource and power
statistics of OMP realization on ZC706 Platform

Conﬁguration

SW
SW+HW

Execution Time
(Seconds)
0.103
0.189

Resource Utilization
{BRAM,DSP,FF,LUT}
–
{44,258,17160,13070}

Total Power (W)

1.57
3.664

matrix inversion architecture for the largest possible matrix
size and use the same for other matrices. Thus, the execution
time of matrix inversion operations in all stages is the same on
FPGA, irrespective of matrix size. In contrast, matrix inversion
time in SW increases with the increase in matrix size. Even
though the OMP is realized on FPGA in the second case,
we still need an ARM processor for running the operating
system and control operations in remote edge deployment.
Thus, the execution of the second realization has a signiﬁcant
contribution from the data communication overhead between
ARM and FPGA. Also, the relatively small dimension of
matrices means that
the ARM processor can execute the
arithmetic operations faster due to eﬃcient caching and higher
clock frequency than FPGA.

B. Functional Performance Analysis of OMP on ZSoC

In this section, we analyze the functional correctness of the
OMP algorithm realized on ZSoC. To begin with, we discuss
the performance metrics used for such analysis.

d ) [27], [28]. The metric, PAB

1) Performance Metrics: We consider two well-know per-
formance metrics: Detection accuracy of all bands in percent-
age (PAB
d ) and detection accuracy of occupied bands in per-
centage (POB
d , corresponds to the
fraction of frequency bands whose status is correctly detected
by the algorithm. The second metric, POB
d , corresponds to the
fraction of occupied bands whose status is correctly detected
by the algorithm. Mathematically, PAB
d are formulated
as:

d and POB

PAB
d

= 100 ∗

(cid:80)N

n=1(yn
(cid:80)N

== yn

true)

true)

pred
n=1(yn
==1 & yn
true
==1)

true

==1)

(cid:80)N

n=1(yn
pred
(cid:80)N
n=1(yn

POB
d

= 100 ∗

is worth noting that POB

Here, N is the total number of frequency bands in the digitized
spectrum, ypred and ytrue are the predicted and ground-truth
band status, respectively (i.e.,, 0 for vacant and 1 for occupied
band). It
d is the preferred metric
for a sparse spectrum consisting of fewer occupied bands
since PAB
d may give a high value even when the algorithm
erroneously detects all bands as vacant. Similarly, PAB
d is the
preferred metric for spectrum with lower sparsity comprising
the higher number of occupied bands. For such spectrum,
POB
d may be high even if the algorithm erroneously detects
all bands as occupied. We do not consider the spectrum
recovery error as a performance metric since it does not oﬀer
additional insights for WSS. There are challenges in capturing
a large number of samples from hardware for accurate error
calculation of spectrum recovery error in real-time.

2) Dataset: Since SNS-based WSS is based on the un-
derlining assumption of the sparse spectrum, we have gen-
erated various datasets with sparsity ranging from 50%-100%.
We consider the SNS with eight analog-to-digital converters
(ADCs), which allow the recovery of the spectrum when the
number of occupied bands is less than or equal to 8. We
consider two groups of datasets:

• Extremely Sparse Spectrum (ESS): This dataset contains
the spectrum with the number of occupied bands between
1 and 3.

• Highly Sparse Spectrum (HSS): This dataset contains the
spectrum with the number of occupied bands between 4
and 7.

As discussed before, we use POB

d as performance
metrics for ESS and HSS datasets, respectively. Interested
readers can refer to [23] for datasets and source code used
for the dataset generation.

d and PAB

3) OMP without Sparsity Knowledge: As discussed in
Algorithm 1,
the number of iterations in OMP algorithm
depends on the sparsity, S (Line 5) and this requirement
limits its usefulness in realistic applications. We consider the
variation of OMP where sparsity knowledge is not known by
exploring the stopping criteria for residual, such as ||Res||< (cid:15)
where (cid:15) is the convergence constant. The convergence con-
stant, (cid:15), depends on both the sparsity and SNR of the digitized
spectrum. To ﬁx (cid:15), we assume prior knowledge of the SNR,
which is common in wireless systems since wireless receivers
can easily measure the SNR of the digitized spectrum. Next,
via empirical analysis, we study the correlation between SNR
and residual, ||Res||, to get the desired performance. We use
this value of ||Res|| as an appropriate estimate for (cid:15) for a given
SNR. In Fig. 1, we compare the desired values of (cid:15) for wide
range of spectrum sparsity and SNRs. Note that we can not
have ﬁxed (cid:15) for all SNRs. However, variation in (cid:15) for diﬀerent
spectrum sparsity is insigniﬁcant, and we can use the average
value of (cid:15) for all spectrums with varying sparsity. We refer to
this algorithm as OMP-(cid:15).

In a wireless environment, the wireless channel may vary
depending on the deployment scenario. In Fig. 2, we study the
eﬀect of wireless channel on the average value of (cid:15) obtained
over wide range of SNRs, (cid:15)S NR. As expected, the eﬀect of the
channel on the WSS is limited since we do not need to recover
the spectrum. Thus, we can ﬁx a single value of (cid:15)S NR for all
types of wireless channels.

4) Performance Analysis and Drawbacks: In Fig. 3, we
compare the performance of the OMP and OMP-(cid:15) for ESS

3

Fig. 1: Variation in (cid:15) estimates for wide range of spectrum
sparsity and SNRs.

Fig. 2: Average (cid:15)S NR estimates for various types of wireless
channels.

and HSS datasets for a wide range of SNRs. As expected,
the performance improves with the increase in SNR. In the
case of ESS with POB
d as a performance metric, both OMPs
achieve 100% accuracy at high SNR due to accurate estimation
of (cid:15). However, the diﬀerence between OMP and OMP-(cid:15) is
around 10-40% at low SNR. In the case of HSS with PAB
d as
a performance metric, overall accuracy is lower, and OMP-
(cid:15) can not meet the accuracy of OMP even at high SNR.
This is because the accurate selection of (cid:15)S NR guarantees
correct detection of occupied bands but does not guarantee
the correct detection of the status of remaining bands. These
results indicate that the state-of-the-art OMP algorithm does
not oﬀer reliable performance for HSS even at high SNR.

OMP suﬀers from multiple drawbacks: 1) Prior knowledge
of sparsity is needed. If such knowledge is unavailable, it
suﬀers from signiﬁcant performance degradation even with
prior knowledge of SNR, 2) Poor performance in low SNR
for both types of spectrum, and 3) Poor performance even at
high SNR for HSS. These shortcomings motivates the search
for potential alternatives for SNS based WSS.

III. DLWSS: Spectrum Recovery via Deep Learning for SNS
based WSS
Previous eﬀorts that aim to tackle SNS-based WSS using
deep networks can be broadly classiﬁed into two categories:

-20-15-10-50510SNR (dB)0246810Average S=92.8%S=85.7%S=78.5%S=64.3%S=50%-20-15-10-50510SNR(dB)0246Average SNRAWGNRayleighRician4

TABLE II: CNN based deep network prediction Architecture

Layers
CV
CV
CV
Flatten
FC

Filters Kernel size

Input Shape Output Shape

256
128
64
-
-

1x150
1x100
1x51
-
-

14x299x2
14x150x256
14x51x128
14x1x64
896x1

14x150x256
14x51x128
14x1x64
896x1
14

task. Further, parameter sharing allows them to operate with
fewer parameters, enabling the network to be memory eﬃcient
and a good candidate for hardware realization. Table II shows
the architecture of the DL model, which comprises three
Convolutional (CV) layers and a single Fully Connected (FC)
layer. All intermediate activations are Rectiﬁed Linear Units
(ReLU), and the activation at the output layer is Sigmoid.

Similar to the deployment of any DL algorithm, DLWSS
design has two phases, training and inference. The training
phase minimizes a loss function, which measures the diﬀer-
ence between the predicted and actual labels. Since more than
one frequency band can be occupied in a wideband spectrum,
we formulate the problem as a multi-label binary classiﬁcation
with binary cross-entropy as the training loss function on ﬁnal
sigmoid outputs. After the training mode, the model weights
are frozen, and the CNN model is used in inference mode to
ﬁnd the occupancy status of an unknown signal in real-time.
Similar to [29], we train the architecture for a dataset using a
machine learning framework (PyTorch in our case) and GPUs.
After training, model architecture, weights and parameters are
extracted for realization on the ZSoC, followed by inference
on the new dataset.

IV. Realization of DLWSS on ZSoC
This section presents the complete architecture and mapping
of DLWSS on ZSoC, and real-time demonstration via Linux-
based graphical user interface (GUI) deployed on ZSoC. In
Fig. 4, various building blocks of the DLWSS such as Pre-
processing, CNN and FC layers, scheduler, DMA, interrupt,
and GUI controller are shown. For illustration, we have shown
the Pre-processing, CNN, and FC layer processing on FPGA.
To enable this, we have developed AXI-stream compatible
hardware IPs for Pre-processing, CNN, and FC blocks and
interconnected them with PS via direct-memory access (DMA)
in the scatter-gather mode for eﬃcient data transfers. Later in
Section VI, we considered various architectures via hardware-
software co-design by moving the blocks between ARM Pro-
cessor and FPGA. We have deployed Linux based operating
system on PS, which takes care of various scheduling and
controlling operations. It also enables GUI development for
real-time demonstration.

As discussed in Algorithm 2, Pre-processing stage involves
large-size matrix multiplication and inversion operations. We
have modiﬁed Xilinx’s existing matrix multiplication and
matrix inversion reference examples to support the complex
number arithmetic since the baseband wireless spectrum is
represented using complex samples. The well-known lower-
upper (LU) decomposition method is selected for matrix
inversion. We have included data buﬀers using block memory
to minimize the repeated data communication between SW
and HW and enable matrix operations of diﬀerent sizes. While
the overall algorithm is sequential, we parallelize individual

Fig. 3: Performance comparisons of the OMP and OMP-(cid:15) for
(a) ESS and (b) HSS datasets for a wide range of SNRs.

1) Iterative approaches [18]–[20] that follow the OMP formu-
lation and augment the frequency band status detection with a
deep network, and 2) Non-iterative approaches which utilize a
deep network for end-to-end spectrum recovery and frequency
band status detection [29]. Since the iterative approaches
follow the algorithmic formulation of OMP, they suﬀer from
the same limitations as that of OMP. In this paper, we focus on
a non-iterative approach that can handle the signals of varying
sparsity and does not require sparsity information or any
manually adjusted convergence constant [29]. The proposed
DLWSS algorithm aims to learn an end-to-end model for WSS.
DLWSS pipeline comprises two stages: 1) Pre-processing and
2) Deep Network Prediction.

1) Pre-Processing: Algorithm 2 shows the steps involved
in the Pre-processing stage of the DLWSS. The pre-processing
step receives the signal captured by the antenna and dig-
itized using the SNS-based analog front-end. It processes
and normalizes the digitized signal so the DL architecture
can handle it. Speciﬁcally, this involves computation of the
pseudo-recovered spectrum ˜X using the sensing matrix and
sub-Nyquist samples. Since the complex input signal, ˜X can
not be fed directly to the DL architecture; it is converted to a
high dimensional real-valued signal as shown in Algorithm 2.
In the end, the signal is normalized for faster convergence of
the training process.

Algorithm 2 Pre-Processing
Require: Sensing matrix A[K][N], aliased

sub-Nyquist samples Y[K][Q]

=U−1 × D−1 × L−1 × P−1

1: Asq= A∗ × A
2: P × Asq = L × D × U
3: A−1
sq
4: A†=A−1
5:
6:
7: Xn ← Normalize( ˜Xd)
8: return Xn

sq × A∗
˜X ← A† × Y
˜Xd ← Concatenate( ˜Xreal, ˜Ximag)

#Square Matrix Asq
#LU Factorization

#Pseudo-inverse of A

2) Deep Network Prediction: The deep network block
of the DLWSS receives the processed input samples from
Pre-processing block and predicts the status (vacant or oc-
cupied) of each frequency band of the digitized spectrum.
The DLWSS architecture is based on a convolutional neural
network (CNN) due to its ability to capture spatial correlation
in input signals which is integral for the spectrum sensing

-20-10010SNR (dB)20406080100PdOBESS DatasetOMPOMP--20-10010SNR (dB)406080100PdABHSS DatasetOMPOMP-(b)(a)5

Fig. 4: Proposed DLWSS Architecture on Zynq SoC.

operations like Matrix Conjugate Transpose and Matrix Mul-
tiplication on FPGA. Every element in the matrix is parallelly
processed to compute transpose, and every row column dot
product in matrix multiplication is performed in parallel to
speed up the computation. In addition, multiple instances of
these IPs are integrated to get
the desired Pre-processing
functionality, as shown in Fig. 4. In the end, a normalization
block is included to meet the input requirements of a deep
neural network.

Next, we focus on mapping each CNN layer on the FPGA.
As shown in Fig. 4, CNN involves many multiply and ac-
cumulation operations on non-contiguous data, i.e., frequent
reading and writing from memory is needed. Depending on
the CNN layer dimensions, it may not be possible to store all
weights and input data in on-chip memory such as block RAM
(BRAM) on FPGA due to limited size and fewer read/write
ports. For instance, if we store all the inputs and weights of
the CNN model considered in Table II in the BRAM with
single-precision ﬂoating (SPFL) number representation, we
need a total of 116.4 mega Byte (Mb) of BRAM, assuming the
CNN output is written directly in the external DDR memory.
Such a large amount of on-chip memory is expensive and may
not be available in most SoC. Using external DDR memory
leads to frequent data communication overhead resulting in
high latency. Thus, mapping the CNN layer on FPGA requires
careful sharing of data between external and on-chip memory
to get the desired latency. Underlining architecture can be
layer-speciﬁc depending on the various parameters of the CNN
layer, resource, and latency constraints.

For the CNN model in Table II, we have explored the

Fig. 5: Illustrative example demonstrating the memory tiling
approach for realization of the CNN on FPGA.

memory tiling approach in which small tiles or blocks of
weights and inputs are loaded into the on-chip memory,
and care has been taken to maximize the utilization of data
currently present in on-chip memory. We deﬁne 4 parameters,
< To, Ti, Tr, Tc > i.e., the tiling factors of output channels,
input channels, output rows and output columns, respectively.
For easier understanding, we present illustrative example with
tiling parameters < 1, 2, 2, 2 > in Fig. 5. The input is of size
4 × 4 × 2, i.e, 2 channels and it is convoluted with 2 ﬁlters

LinuxARM (PS)A X IXAI DDR MemoryFPGA (PL)D M A AXI LiteAXI MM  ConjugateMatrix MultiplicationMatrix InversionMatrix MultiplicationMatrix MultiplicationAXI StreamMultiplyAccumulateand ReLuUnits  DMAAXI StreamPre-ProcessingNormalizationAXI StreamDDR Controller UART SD Card Ethernet UARTSD CardDesktopDLWSS SchedulerPre-ProcessingSchedulerDMA ControllerPerformance AnalysisInterrupt ControllerGUI ControllerAXI3 AZBRAM2BRAMNBRAM1BRAM2BRAMNBRAM1B R A M C T R LCNN Controller CNN Layer 1B R A M C T R LB R A M C T R LAXI LiteAXI MM   DMAAXI LiteAXI MM   DMAAXI LiteAXI MM   DMAAXI LiteAXI MM  CNN Layer 2CNN Layer 3FC LayerCNN Layer 3B R A M C T R LAXI LiteDDR MemoryOn-chip Block RAM++XXXXXXXX+RELUCNN Computationsof dimension 2 × 2 × 2. Initially, we initialize an output tile
and load the input tile and weight tile required to compute
the output tile into the on-chip Block RAM. For instance, to
compute a 2 × 2 × 1 output tile, we need a 3 × 3 × 2 input
tile (Itile) and one ﬁlter of dimension 2 × 2 × 2 (Wtile). After
loading, the tiled convolution is performed between Itile and
Wtile using a set of parallel multipliers and adders, referred
to as CNN Computations in Fig. 5. Once all elements of the
output tile are computed, the tile is sent back to the DDR
memory, and a similar process is repeated for computing the
next output tile. Here, we have shown the computation of one
element of the output tile. Depending on resource availability,
latency constraints, and memory ports, we can have CNN
computations of all elements of a tile or even multiple tiles in
a parallel or serial-parallel fashion.

The tiling approach signiﬁcantly reduces the on-chip mem-
ory requirement for the CNN model in Table II. For instance,
tiling with parameters < 20, 16, 20, 20 > needs only 3.35Mb
of BRAM (0.24Mb for output tile,1.464Mb for weight tile and
1.650Mb for input tile) compared to 116.4 Mb in non-tiling
based architecture. This in turn allows eﬃcient optimization
of hardware IP cores via pipelining and unrolling, resulting
in signiﬁcant improvement in performance. We have explored
a wide range of tiling parameters and implemented these
architectures on the ZSoC. Please refer to Section VI-C for
more details. We have explored a similar tiling approach for
FC layers as well. However, experimental analysis shows that
FC layers do not need tiling due to the smaller dimensions of
inputs and weights.

The DLWSS model in Table II contains two types of non-
linear activations; ReLU and Sigmoid. As shown in Fig. 6,
the realization of the ReLU on the FPGA is simple as it needs
only one comparator and multiplexer. The realization of the
Sigmoid on the FPGA can be done either using LUT based
approach or by polynomial approximation. The LUT-based
approach is memory intensive, while the polynomial-based
approach involves many arithmetic and logical operations. We
have realized ReLU on FPGA and Sigmoid on ARM processor
based on the experimental analysis.

Fig. 6: ReLU and Sigmoid activations

An end-to-end application with a live graphical user inter-
face (GUI) is developed to demonstrate the real-time WSS
on the ZSoC using the proposed architectures. The applica-
tion running on a Petalinux-based operating system deployed
on the ARM processor accelerates the computation of pre-
processing algorithm and the DL model on the FPGA. The
real-time predictions are obtained and stored on the SD-card
by the application. The GUI application, shown in Fig. 7,
is deployed on the remote server and reads the contents of
SD-card at regular intervals. The GUI is developed using the

6

Fig. 7: GUI for real-time visualisation of SNS based WSS on
ZSoC platform.

Fig. 8: Performance comparison of DLWSS and OMP on (a)
ESS and (b) HSS spectrum for diﬀerent channel conditions.
Sparsity level is known.

Tkinter [30] framework and provides visualization in real-time
as and when the architecture predicts the status of frequency
bands in the received digitized spectrum.

V. Performance Analysis and Comparison with OMP

In this section, we compare the functionality of the OMP
and DLWSS architectures for diﬀerent wireless channels, a
wide range of SNRs, and sparsity levels. We consider the
eﬀect of prior knowledge of sparsity on performance. As dis-
cussed in Section II-B, we use POB
d as the performance
metrics for ESS and HSS spectrums, respectively. For all the
experimental results presented in this section, we consider the
ﬂoating-point arithmetic based architecture realized on ZSoC
platform.

d and PAB

In Fig. 8, we compare the performance of OMP and DLWSS
for three diﬀerent channels (AWGN, Rayleigh, and Rician).
We assume the prior knowledge of the spectrum sparsity in the
case of OMP. For the ESS spectrum (Fig. 8 (a)), the DLWSS
performs better than OMP with an average performance gain
of around 12 %. We also observed that DLWSS and OMP are
robust to changes in channel conditions, which is expected
since channel conditions’ impact on the spectrum sensing is
fairly limited. For the HSS spectrum (Fig. 8(b)), the DLWSS
signiﬁcantly outperforms the OMP algorithm with an average
performance improvement of around 24%.

-10-50510x00.20.40.60.81sigmoid(x)sigmoid(x)=(1+exp(-x))-1-10-50510x0246810relu(x)relu(x)=max(0,x)-20-10010SNR (dB)406080100PdOBESS DatasetOMPAWGNOMPRicianOMPRayleighDLWSSAWGNDLWSSRicianDLWSSRayleigh-20-10010SNR (dB)60708090100PdABHSS Dataset(b)(a)7

VI. Complexity Analysis
In this section, we analyze the complexity of the DLWSS
algorithms for diﬀerent architectures realized via hardware-
software co-design, various word lengths, and memory tiling
approaches.

A. Hardware Software Co-Design (HSCD)

A heterogeneous SoC such as Zynq SoC from Xilinx
contains ARM Cores as the processing system (SW) and
FPGA as the programmable logic (HW). An integral aspect
of developing an eﬃcient mapping of the DLWSS on the
ZSoC is to design an optimal HSCD strategy to facilitate
functionally accurate architecture for the desired latency and
resource constraints. Speciﬁcally, we need to decide how to
partition the algorithm between HW and SW and minimize
the data communication overhead between them. The HSCD
is important since sequential operations, scheduling tasks,
and GUI are preferred on SW. At
the same time, FPGA
can eﬃciently handle the task, which can be accelerated via
parallel processing. However, in certain situations, serial tasks
are preferred on HW, while parallel tasks are preferred on SW
to avoid data communication overhead between SW and HW.
Furthermore, some operations may oﬀer speed up on HW.
Still, such speed up may not be signiﬁcant compared to the
algorithm’s overall execution time; hence, realizing such tasks
on SW can reduce FPGA size, cost, and power consumption.
Such trade-oﬀs demand a detailed study of various HSCD
architectures to design an architecture that meets the cost,
latency, and power constraints.

Table III shows the results of our HSCD study for the DL-
WSS architecture comprising pre-processing (P), convolution
(CV), fully connected (FC), and activations (A) blocks. As
shown in Row 1, mapping the entire architecture on SW results
in a high execution time of 30.8 seconds (s). As we shift more
blocks to HW, the execution time is reduced while resource
and power consumption increase (Rows 2-6). The CV layer
is the most computational complex unit in the DLWSS, and
realizing it on the HW results in a signiﬁcant reduction in the
execution time from 30.8s to 2.87s.

Since the DLWSS model has fewer FC layers as compared
to the CV layers, shifting the FC layer to HW does not oﬀer
substantial improvement in execution time compared to 2.7%,
0.6%, 9.4% and 7.8% increase in BRAM, DSP, FF and LUT
utilization, respectively. Similarly, shifting the pre-processing
algorithm (P) on HW leads to improvement in execution time
of around 7 ms compared to increase in the BRAM, DSP and
LUT usage by 7%, 11% and 8%, respectively with respect to
previous architecture in Row 3. As we move FC block to HW
(Row 5), we notice that the impact of FC block is the same
as seen in Row 2, which is expected as adding the FC block

Fig. 9: Performance comparison of DLWSS and OMP on (a)
ESS and (b) HSS spectrum for diﬀerent channel conditions.
Sparsity level is not known.

Fig. 10: Impact of sparsity level on the performance of the
DLWSS and OMP algorithms.

In practical deployments, spectrum occupancy changes dy-
namically with time and hence, knowledge of spectrum spar-
sity is not available. In Fig. 9, we analyze the performance of
the DLWSS and OMP algorithms when spectrum sparsity is
not known. It is observed that the proposed DLWSS signiﬁ-
cantly outperforms the OMP algorithm at all range of SNRs
with an average improvement of around 36.4% and 31.8% for
for ESS and HSS dataset, respectively.

In Fig 10, we compare the impact of sparsity on the
performance of DLWSS and OMP algorithms when spectrum
sparsity is unknown. We consider two SNRs: 0 dB and 10 dB.
We trained the DLWSS model using the dataset comprising
an equal number of samples from each sparsity level. It can
be observed that the DLWSS oﬀers superior performance and
signiﬁcantly outperforms OMP as the sparsity level increases.
Thus, results in Fig. 8, Fig. 9 and Fig. 10 conﬁrms the
superiority of the DLWSS over the OMP based approach.

TABLE III: HSCD Study of DLWSS Architecture on ZSoC

No.
1
2
3
4
5
6

Blocks on HW Blocks on SW Execution Time (s)

CV
CV+FC
P+CV
P+CV+FC
P+CV+FC+A

P+CV+FC+A
P+FC+A
P+A
FC+A
A
-

30.8
2.87
2.869
2.863
2.863
2.864

{BRAM, DSP, FF, LUT}
{0, 0, 0, 0}
{220, 719, 126741, 100250}
{249, 724, 167676, 117382}
{296, 818, 139857, 117738}
{325, 823, 180792, 134870}
{325, 853, 183173, 138230}

{Total Power, Dynamic Power}(Watts)
{1.6, 1.2}
{3.205, 2.205}
{3.373, 2.369}
{3.391, 2.387}
{3.540, 2.533}
{3.567, 2.559}

-20-10010SNR (dB)20406080100PdOBESS DatasetOMPDLWSS-20-10010SNR (dB)5060708090100PdABHSS DatasetOMPDLWSS(b)(a)1234567Sparsity Level707580859095100PdOBESS DatasetOMP0dBOMP10dBDLWSS0dBDLWSS10dB1234567Sparsity Level708090100PdABHSS Dataset(b)(a)adds a constant time and constant resource utilization for all
cases. In Row 6, we move Sigmoid based activation block to
HW which results in increase in the DSP and LUT utilization
by 3% and 1.5%, respectively, with around 1 ms improvement
in execution time. Note that as we gradually shift more and
more blocks from SW to HW, the overall power consumption
increases from 3.205W (Row 2) to 3.567W (Row 6). Thus,
from HSCD perspective, it is better to keep all blocks except
CV layers on the SW given high resource penalty in HW for
small gain in execution time.

B. Word Length Optimization

Conventionally, HW realization of the algorithm in ﬂoating-
point arithmetic oﬀers good functional accuracy but incurs
high resource utilization, power consumption, and execution
time. Since the extremely-large dynamic range oﬀered by
ﬂoating-point arithmetic may not be needed for all the sub-
blocks of the algorithm, ﬁxed-point arithmetic can potentially
oﬀer a signiﬁcant reduction in resource, power, and execution
time without compromising on the functional accuracy. In
wireless applications such as DLWSS, the dynamic range of
inputs, weights, and activation is limited due to analog-to-
digital converters (ADCs); hence, ﬁxed-point architectures are
preferred. In this section, we discuss the selection of appropri-
ate word length for the part of the algorithm realized on the
HW, i.e., FPGA, and its impact on the functional accuracy,
resource utilization, execution time, and power consumption.
We use W bits to represent each number in ﬁxed point
quantization. Out of W bits, we use I bits to represent the
integer part and (W − I) bits to represent the fractional part.
For example, the ﬁxed-point representation of a number, 3.25,
needs only 6 bits with I = 3 compared to 64 bits in double-
precision ﬂoating point (DPFP), 32 bits in single-precision
ﬂoating point (SPFP), and 16 bits in half-precision ﬂoating-
point (HPFP). Thus, depending on the dynamic range of the
given variable, the appropriate selection of W and I can avoid
loss in functional accuracy. To identify appropriate values
of W and I for the DLWSS architecture, we analyzed the
dynamic range of inputs, outputs, and intermediate outputs of
various sub-blocks. For instance, Table IV shows the analysis
to determine the optimal integer width for hardware realization
of CV and FC layers of the DLWSS, where we infer the model
on samples of the dataset to estimate the ranges of intermediate
activations and the model weights. It can be observed that
the minimum value of I is 9 and 2 bits for activation and
weights, respectively. The value of W depends on the number
of bits for accurate fractional number representation to get
the desired functional performance, and we select them via
heuristic experiments.

TABLE IV: Integer Word Length Selection for DLWSS

Type
Activation
Activation
Weight
Weight

Layer Minimum Value Maximum Value

CV
FC
CV
FC

-77.061
-86.594
-0.4812
-0.0661

199.309
158.975
0.9561
0.0219

Imin
9
9
2
2

8

In Table V, we compare 10 diﬀerent DLWSS architectures
with ﬁxed integer WL of 9 for activation and 2 for weights.
Here, we have ﬁxed the WL of weights to <16,2> to identify
the WL for activation. A similar process is done to identify the
WL of weights by ﬁxing the WL of activation. Corresponding
details are omitted to avoid repetition of discussion. All these
results are evaluated for a ﬁxed tile size of <20,16,20,20>.
Please refer to Section VI-C for more details about impact of
tile size on resource utilization.

To begin with, we consider DLWSS with SPFL WL in Row
1 of Table V. As expected, it oﬀers excellent functional accu-
racy in terms of the chosen performance metrics. In Row 2,
we consider DLWSS with HPFP WL. It oﬀers nearly identical
functional performance as that of the SPFP architecture with
savings of around 7% in BRAM, 17% in DSP, and 46 in %
LUT and power savings of 0.47W. Also, there is a signiﬁcant
reduction of 0.17s in execution time.

Next, we have explored eight diﬀerent DLWSS architectures
via ﬁxed-point quantization, and corresponding results are
there is a
given in Rows 3-10 of Table V. As expected,
slight degradation in functional accuracy as WL decreases.
The DLWSS architecture with the WL of <23,9> or below in
Rows 9-10 suﬀers from signiﬁcant degradation in performance
and should be avoided. The DLWSS architecture in Row 7
with a WL of 25 oﬀers functional performance same as that
of ﬂoating-point architectures with more than 50% savings
in DSP, FFs, and LUTs over HPFP architecture in Row 2
and over 60% savings in DSP, FFs, and LUTs over SPFP
architecture in Row 1. These savings can be further improved
using the DLWSS architecture with WL of <24,9> in Row
8 with minor degradation in performance. Note that we have
assumed that the inputs and outputs are in SPFP format; hence,
additional WL conversion inside the architecture is needed. We
can reduce the execution time further if the input and output
WLs are the same as the rest of the architectures.

In Table V, we have ﬁxed the SNR to 10 dB. To analyze
the architecture’s performance at diﬀerent SNRs, we have
compared the POB
d for HSS for SNRs ranging
from -20dB to 10dB in Fig. 11. It can be observed that the
performance degrades at higher SNR due to insuﬃcient WL.

d for ESS and PAB

Fig. 11: Impact of the WL on the functional accuracy of the
DLWSS architecture.

We have designed and implemented DLWSS architectures
of various WLs on ZSoC and analyzed their performance to
identify appropriate WL for fractional number representation.

C. Impact of Memory Tiling Approach

As discussed in Section IV, memory tiling is essential to
enable an eﬃcient implementation of the memory-intensive

-20-10010SNR (dB)5060708090100PdOBESS DatasetSPFPHPFP<25,9> <23,9>-20-10010SNR (dB)60708090100PdABHSS Dataset(b)(a)9

TABLE V: Functionality and Complexity Analysis of DLWSS Architectures for Diﬀerent Word Lengths at 10 dB SNR

No.

1
2
3
4
5
6
7
8
9
10

Activation
WL: <Wa, Ia>
SPFP
HPFP
<29,9>
<28,9>
<27,9>
<26,9>
<25,9>
<24,9>
<23,9>
<22,9>

Weights
WL: <Ww, Iw>
SPFP
HPFP
<16,2>
<16,2>
<16,2>
<16,2>
<16,2>
<16,2>
<16,2>
<16,2>

Execution Time (s)

ESS: POB

d

HSS: PAB

d

{BRAM, DSP, FF, LUT}

{Tot. Power, Dyn. Power}

2.28
2.11
2.28
2.28
2.30
2.30
2.30
2.30
2.28
2.28

100
100
100
100
100
100
100
100
87.38
70.96

100
99.97
100
100
99.98
99.94
99.94
98.78
79.45
57.46

{256,879,150107,117023}
{213,719,98614,74952}
{248,399,75935,81119}
{248,399,71907,74381}
{243,399,59641,50259}
{240,399,59641,50259}
{240,399,53926,41443}
{232,399,53926,41443}
{232,399,53926,41443}
{232,399,53926,41443}

{4.218,3.944}
{2.688,2.448}
{2.523,2.285}
{2.484,2.247}
{2.353,2.119}
{2.351,2.114}
{2.299,2.065}
{2.294,2.061}
{2.294,2.061}
{2.294,2.061}

convolution layer operations on HW. In this section, we discuss
the impact of tiling size on the performance of the DLWSS
architecture realized on the SW and HW. In Fig. 12, we
compare the acceleration factor, i.e. the ratio of execution time
on SW and HW, for diﬀerent tiling sizes on SW and HW
realizations. It can be observed that the acceleration factor
increases with the increase in tiling size of the HW architecture
for a given tiling size of SW realization. Such behavior can
be attributed to the following:

1) Higher tile size on HW means fewer accesses to external
DDR memory since more data is buﬀered in on-chip
block RAM.

2) Higher tile size allows more opportunities for intra-
tile parallelization i.e. dot products within a tile can be
computed in parallel on HW.

Interestingly, an increase in the tiling size of the SW
realization results in degradation in the execution time of the
DLWSS on SW. For instance, acceleration factor is higher for
SW tiling size of < 20, 16, 20, 20 > compared to SW tiling size
of < 2, 2, 2, 2 >. This behavior is exactly reverse compared to
that of HW, and one possible reason is data caching in SW
realization. Smaller tile size allows tiles to be cached in the
local data cache memory of ARM cores, thereby limiting the
number of accesses to external DDR memory. Since cache size
is signiﬁcantly smaller than on-chip BRAM on HW, larger tile
size results in frequent cache ﬂush requirements, resulting in
execution time degradation.

Though the tiling size does not impact functional accuracy, a
larger tiling size leads to higher resource utilization, as shown
in Table VI. This is because a larger tile size needs a higher
amount of on-chip memory and enables parallel computations
due to the availability of more data on the chip. The use of
ﬁxed-point arithmetic can help to reduce resource utilization
signiﬁcantly. Thus, combining hardware-software co-design,
WL, and tiling parameters are vital to meet the given resource
and execution time constraints.

TABLE VI: Eﬀect of Tiling and WL on resource utilization

Resources Word-Length

BRAM

DSP

LUT

Floating Point
Fixed Point
Floating Point
Fixed Point
Floating Point
Fixed Point

Memory Tiling Parameters

<2,2,2,2>
14.9
14.5
82
81
35107
35652

<8,8,8,8>
67.9
63.5
239
143
49097
37656

<16,16,16,16>
219.96
203.5
719
335
100249
46824

<16,16,20,20>
219.96
203.5
719
335
100687
47630

<20,16,20,20>
255.72
239.5
879
399
117016
59259

Fig. 12: Impact of tile size on the execution time acceleration
factor.

VII. Conclusions

In this paper, we designed and implemented statistical
orthogonal matching pursuit (OMP), and deep learning (DL)
based algorithms on Zynq System-on-chip (SoC) for wideband
sensing applications. We have provided in-depth experimental
results and complexity comparisons among various architec-
tures obtained via hardware-software co-design, word-length
optimization, and memory tiling. Speciﬁcally, we demon-
strated the drawbacks of conventional OMP algorithms, such
as poor performance at a low signal-to-noise ratio (SNR) and
the need for prior knowledge of sparsity. These drawbacks
are addressed via a novel DL-based approach. However, the
DL architecture’s high resource utilization and execution time
is a concern. In the future, we also plan to explore Neural
architecture search (NAS) along with quantized model training
to reduce the complexity of DL architecture. We also plan to
integrate the proposed architecture with analog-front-end for
experiments with real-radio signals.

References

[1] A. Ali and W. Hamouda, “Advances on spectrum sensing for cogni-
tive radio networks: Theory and applications,” IEEE Communications
Surveys Tutorials, vol. 19, no. 2, pp. 1277–1304, 2017.

[2] C.-P. Yen, Y. Tsai, and X. Wang, “Wideband spectrum sensing based on
sub-nyquist sampling,” IEEE Transactions on Signal Processing, vol. 61,
no. 12, pp. 3028–3040, 2013.

2,2,2,24,4,4,48,8,8,816,16,16,1616,16,20,2020,16,20,20HW Tile Size051015Acceleration FactorSW Tile Size: <2,2,2,2>SW Tile Size: <20,16,20,20>10

[26] Y. Pati, R. Rezaiifar, and P. Krishnaprasad, “Orthogonal matching
pursuit: recursive function approximation with applications to wavelet
decomposition,” in Proceedings of 27th Asilomar Conference on Signals,
Systems and Computers, 1993, pp. 40–44 vol.1.

[27] Z. Quan, S. Cui, A. H. Sayed, and H. V. Poor, “Wideband spectrum sens-
ing in cognitive radio networks,” in 2008 IEEE International Conference
on Communications, 2008, pp. 901–906.

[28] Y. L. Polo, Y. Wang, A. Pandharipande, and G. Leus, “Compressive
wide-band spectrum sensing,” in 2009 IEEE International Conference
on Acoustics, Speech and Signal Processing, 2009, pp. 2337–2340.
[29] S. Chandhok, H. Joshi, A. V. Subramanyam, and S. J. Darak, “Novel
deep learning framework for wideband spectrum characterization at sub-
nyquist rate,” 2020.

[30] F. Lundh, “An introduction to tkinter,” URL: www. pythonware.

com/library/tkinter/introduction/index. htm, 1999.

[3] M. Labib, V. Marojevic, J. H. Reed, and A. I. Zaghloul, “Extending
lte into the unlicensed spectrum: Technical analysis of the proposed
variants,” IEEE Communications Standards Magazine, vol. 1, no. 4, pp.
31–39, 2017.

[4] W. S. H. M. W. Ahmad, N. A. M. Radzi, F. S. Samidi, A. Ismail,
F. Abdullah, M. Z. Jamaludin, and M. N. Zakaria, “5g technology:
Towards dynamic spectrum sharing using cognitive radio networks,”
IEEE Access, vol. 8, pp. 14 460–14 488, 2020.

[5] L. Zhang, M. Xiao, G. Wu, M. Alam, Y.-C. Liang, and S. Li, “A survey
of advanced techniques for spectrum sharing in 5g networks,” IEEE
Wireless Communications, vol. 24, no. 5, pp. 44–51, 2017.

[6] M. Matinmikko-Blue, S. Yrj¨ol¨a, V. Sepp¨anen, P. Ahokangas,
H. H¨amm¨ainen, and M. Latva-Aho, “Analysis of spectrum valuation
elements for local 5g networks: Case study of 3.5-ghz band,” IEEE
Transactions on Cognitive Communications and Networking, vol. 5,
no. 3, pp. 741–753, 2019.

[7] V. Valenta, R. Marˇs´alek, G. Baudoin, M. Villegas, M. Suarez,
and F. Robert, “Survey on spectrum utilization in europe: Mea-
surements, analyses and observations,” in 2010 Proceedings of the
Fifth International Conference on Cognitive Radio Oriented Wireless
Networks and Communications, 2010, pp. 1–5.

[8] Y. Chen and H.-S. Oh, “A survey of measurement-based spectrum oc-
cupancy modeling for cognitive radios,” IEEE Communications Surveys
Tutorials, vol. 18, no. 1, pp. 848–859, 2016.

[9] M. L´opez-Ben´ıtez and F. Casadevall, “Space-dimension models of
spectrum usage for cognitive radio networks,” IEEE Transactions on
Vehicular Technology, vol. 66, no. 1, pp. 306–320, 2017.

[10] S. Stein Ioushua, O. Yair, D. Cohen, and Y. C. Eldar, “Cascade:
Compressed carrier and doa estimation,” IEEE Transactions on Signal
Processing, vol. 65, no. 10, pp. 2645–2658, 2017.

[11] E. Crespo Marques, N. Maciel, L. Naviner, H. Cai, and J. Yang, “A
review of sparse recovery algorithms,” IEEE Access, vol. 7, pp. 1300–
1322, 2019.

[12] T. T. Cai and L. Wang, “Orthogonal matching pursuit for sparse signal
recovery with noise,” IEEE Transactions on Information Theory, vol. 57,
no. 7, pp. 4680–4688, 2011.

[13] E. Crespo Marques, N. Maciel, L. Naviner, H. Cai, and J. Yang, “A
review of sparse recovery algorithms,” IEEE Access, vol. 7, pp. 1300–
1322, 2019.

[14] M. Yang and F. de Hoog, “Orthogonal matching pursuit with threshold-
ing and its application in compressive sensing,” IEEE Transactions on
Signal Processing, vol. 63, no. 20, pp. 5479–5486, 2015.

[15] S. Zhang, J. Wu, D. Chen, S. Li, B. Yu, and J. Qu, “Fast frequency-
domain compressed sensing analysis for high-density super-resolution
imaging using orthogonal matching pursuit,” IEEE Photonics Journal,
vol. 11, no. 1, pp. 1–8, 2019.

[16] J. A. Becerra, M. J. Madero-Ayora, J. Reina-Tosina, C. Crespo-Cadenas,
J. Garc´ıa-Fr´ıas, and G. Arce, “A doubly orthogonal matching pursuit
algorithm for sparse predistortion of power ampliﬁers,” IEEE Microwave
and Wireless Components Letters, vol. 28, no. 8, pp. 726–728, 2018.

[17] B. Mamandipoor, D. Ramasamy, and U. Madhow, “Newtonized orthog-
onal matching pursuit: Frequency estimation over the continuum,” IEEE
Transactions on Signal Processing, vol. 64, no. 19, pp. 5066–5081, 2016.
[18] H. Palangi, R. K. Ward, and L. Deng, “Convolutional deep stacking
networks for distributed compressive sensing,” Signal Process., vol. 131,
pp. 181–189, 2017.

[19] H. Palangi, R. Ward, and L. Deng, “Using deep stacking network
to improve structured compressed sensing with multiple measurement
vectors,” in 2013 IEEE International Conference on Acoustics, Speech
and Signal Processing, 2013, pp. 3337–3341.

[20] ——, “Distributed compressive sensing: A deep learning approach,”
IEEE Transactions on Signal Processing, vol. 64, no. 17, pp. 4504–4518,
2016.

[21] Y. Nakayama, D. Hisano, and K. Maruta, “Adaptive c-ran architecture
with moving nodes toward beyond the 5g era,” IEEE Network, vol. 34,
no. 4, pp. 249–255, 2020.

[22] F. D. L. Coutinho, J. D. Domingues, P. M. C. Marques, S. S. Pereira,
H. S. Silva, and A. S. R. Oliveira, “Towards the ﬂexible and eﬃcient
implementation of the 5g-nr ran physical layer,” in 2021 IEEE Radio
and Wireless Symposium (RWS), 2021, pp. 130–132.

[23] R. Rajesh, “Source code and datasets,” https://drive.google.com/drive/
folders/1NjVnkbUNwwITOi77UVTNq2v50sQ9 z7C?usp=sharing,
2022, [Online; accessed 5-August-2022].

[24] T. T. Cai and L. Wang, “Orthogonal matching pursuit for sparse signal
recovery with noise,” IEEE Transactions on Information Theory, vol. 57,
no. 7, pp. 4680–4688, 2011.

[25] E. Crespo Marques, N. Maciel, L. Naviner, H. Cai, and J. Yang, “A
review of sparse recovery algorithms,” IEEE Access, vol. 7, pp. 1300–
1322, 2019.

