2
2
0
2

r
p
A
9

]

G
L
.
s
c
[

1
v
7
0
5
4
0
.
4
0
2
2
:
v
i
X
r
a

MR-iNet Gym: Framework for Edge Deployment of Deep
Reinforcement Learning on Embedded Software Defined Radio

Jithin Jagannath1, Kian Hamedani1, Collin Farquhar1, Keyvan Ramezanpour1, Anu Jagannath1
1Marconi-Rosenblatt AI/ML Innovation Lab, ANDRO Computational Solutions LLC
Rome, NY, USA
{jjagannath,khamedani,cfarquhar,kramezanpour,ajagannath}@androcs.com

ABSTRACT
Dynamic resource allocation plays a critical role in the next gen-
eration of intelligent wireless communication systems. Machine
learning has been leveraged as a powerful tool to make strides in
this domain. In most cases, the progress has been limited to simu-
lations due to the challenging nature of hardware deployment of
these solutions. In this paper, for the first time, we design and deploy
deep reinforcement learning (DRL)-based power control agents on
the GPU embedded software defined radios (SDRs). To this end, we
propose an end-to-end framework (MR-iNet Gym) where the simu-
lation suite and the embedded SDR development work cohesively to
overcome real-world implementation hurdles. To prove feasibility,
we consider the problem of distributed power control for code-
division multiple access (DS-CDMA)-based LPI/D transceivers. We
first build a DS-CDMA ns3 module that interacts with the OpenAI
Gym environment. Next, we train the power control DRL agents in
this ns3-gym simulation environment in a scenario that replicates
our hardware testbed. Next, for edge (embedded on-device) deploy-
ment, the trained models are optimized for real-time operation
without loss of performance. Hardware-based evaluation verifies
the efficiency of DRL agents over traditional distributed constrained
power control (DCPC) algorithm. More significantly, as the primary
goal, this is the first work that has established the feasibility of de-
ploying DRL to provide optimized distributed resource allocation
for next-generation of GPU-embedded radios.

CCS CONCEPTS
â€¢ Networks â†’ Network protocols; â€¢ Computing methodolo-
gies â†’ Machine learning.

KEYWORDS
Deep reinforcement learning, software defined radio, GPU, power
allocation, machine learning

ACM Reference Format:
Jithin Jagannath1, Kian Hamedani1, Collin Farquhar1, Keyvan Ramezanpour1,
Anu Jagannath1. 2022. MR-iNet Gym: Framework for Edge Deployment of
Deep Reinforcement Learning on Embedded Software Defined Radio. In
Proceedings of WiSeML â€™22: 15th ACM Conference on Security and Privacy

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
WiSeML â€™22, May 16â€“19, 2022, San Antonio, Texas, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/3522783.3529530

in Wireless and Mobile Networks (WiSeML â€™22). ACM, New York, NY, USA,
6 pages. https://doi.org/10.1145/3522783.3529530

1 INTRODUCTION
Wireless communication has been evolving rapidly to keep up with
the requirement of the consumers both in the commercial and tacti-
cal domain. As compared to wired networks, the challenges posed
by dynamic and rapidly changing channel is well known. In addi-
tion, the resources available for wireless devices are limited and
do not follow the exponential growth trajectory of the number of
wireless devices. These factors dictate the need for intelligent deci-
sion engines that can learn from the environment to dynamically
use the scarce resources while meeting the requirements of the end
customer.

The rapid growth of computational resources and availability of
digital data has revived machine learning from its second winter.
Machine learning has been revolutionizing several domains when
applied judiciously. The wireless domain is no exception to these
influences. There have been significant efforts made to effectively
leverage various strengths of machine learning to solve crucial
hurdles encountered in the wireless domain [7, 9, 12, 14]. For exam-
ple, supervised learning is being widely deployed to perform blind
signal characterization and classification; several approaches are
being leveraged to detect intrusions, anomalies, and enhance the
security of wireless systems including the Internet-of-Things (IoT)
; and reinforcement learning is being used in several applications
for dynamic resource allocation. Even with the recent advances,
there are several limitations and challenges that have held back the
real-world deployment of machine learning based decision engines
in the wireless domain. In this paper, we propose an end-to-end
framework and related methodology for deployment of deep rein-
forcement learning (DRL) for a distributed wireless network. To
accomplish this, we choose the problem of distributed power con-
trol in low probability of intercept and detection (LPI/D) networks
which play a crucial role in tactical communication. Direct sequence
code-division multiple access (DS-CDMA) is especially popular for
implementing a robust LPI/D physical layer due to its unique advan-
tages, such as easy frequency management (no frequency planning)
in multi-user scenarios, low peak-to-average power ratio (PAPR),
and less stringent synchronization requirements as necessary for
orthogonal frequency division multiplexing (OFDM). We would
like to emphasize the fact that this is just one example of an opti-
mization problem that can be solved using the proposed framework
which is the focus of this work.

Deploying machine learning at the edge (on embedded platform)
has always been a formidable challenge in the wireless domain. To
the best of our knowledge, this is the first work that puts forth an

 
 
 
 
 
 
end-to-end framework to demonstrate the feasibility of designing,
training, and deploying DRL-based decision engines for next gener-
ation of embedded software defined radio (SDR) hardware. The key
contributions and impact of this work are as follows,
â€¢ We develop the first known DS-CDMA network architecture in

ns3-gym to ensure rapid training of the DRL module.

â€¢ Next, we perform optimization of the trained model to ensure
lightweight deployment on embedded SDR to minimize the in-
ference time and ensure real-time decision making.

â€¢ The results of our over-the-air experiments indicate that DRL
consumes significantly less power while maintaining high re-
liability by taking into account higher dimensional input yet
making a real-time decision.

â€¢ This is the first work that establishes the feasibility of leveraging
DRL for distributed decision making in the wireless network
using GPU-embedded transceiver. We believe edge deployment
of machine learning solutions will be a key enabler for future
networks including but not limited to 6G.

â€¢ Overall, the progress and impact of this work can arguably be
abstracted from the specific optimization (resource allocation)
problem discussed here.

2 RELATED WORK
Machine learning has been emerging as a tool suited to solve several
challenging problems in the domain of wireless communication
[7, 9, 10, 12]. Yet, there are several hurdles that need to be overcome
to transition some of these solutions from concept to simulations
to actual hardware implementation. The majority of the advances
have been limited to simulations due to the challenges associated
with demonstrating the effectiveness of machine learning-based
approaches on actual radio hardware. It is also important to point
out that in many cases (especially in the case of supervised learning)
hardware is used to collect relevant data or execute the decision of
a machine learning engine but the actual computation still happens
on large host PCs with ample computing resources [5, 8]. Here we
argue that in several cases for machine learning to truly impact the
wireless domain, techniques and solutions will have to be deployed
on the devices themselves using the embedded computing resources
that could include GPP, GPU, or FPGA. This argument can also be
seen in the light of the importance and every increasing attention
edge computing has been witnessing to meet the QoS constraints
(including and not limited to latency).

In[4, 11], distributed constrained power control (DCPC) algo-
rithm was introduced where the power is adjusted iteratively by
DCPC given the signal-to-interference-plus-noise ratio (SINR). In
this approach, i.e., DCPC, a minimum required SINR threshold is
considered and the optimization process continues as long as the
minimum required quality of service (QoS) is achieved. Later, sev-
eral other algorithms [2, 6, 17, 19] were introduced which were
the modified version of [4]. In[6], a power and admission control
algorithm with beamforming is introduced where the coexistence
of the primary users (PUs) and secondary users (SUs) in a shared
spectrum is guaranteed while the total transmission power is mini-
mized. The hard SINR constraint issue in distributed power control
is addressed in[19] where a utility based power control (UBPC) is
presented. In this approach[19], the problem is reformulated us-
ing a softened SINR constraint (utility) and a penalty for power

consumption (cost) where the goal is to maximize the net utility
which is equal to utility without the cost. In[2], a twofold mul-
tiuser interference management approach for wireless ad hoc net-
works is introduced where the objectives are to increase single-hop
throughput and reduce the power consumption. In this approach[2],
distributed power control is performed to achieve the admissible
power vector which can be used by the scheduled transmitters to
satisfy their single-hop requirements. A distributed power alloca-
tion algorithm is presented in[17] where the PUs communicate
under a minimum QoS requirement, whereas the SUs opportunis-
tically utilize the primary band. Fractional programming (FP)[15]
and weighted minimum mean square error (WMMSE)[16] are the
two iterative centralized power control algorithms which are model
driven [13]. In the two aforementioned centralized power control
algorithms, i.e., FP and WMMSE, the delay caused by the feedback
exchange mechanism between a central controller and the users
[13] could deteriorate performance in a distributed LPI/D network.
On the other hand, the existing power control algorithms require
perfect CSI knowledge and are model driven. The data driven meth-
ods, e.g., DRL do not require perfect CSI knowledge, hence, they
are more promising for realistic wireless environments. To tackle
the challenges of conventional power control algorithms, in this
paper for the first time, we train and deploy the DRL agents on
GPU-embedded SDRs. In particular, we train and deploy actor-critic
(AC) networks with deep deterministic policy gradient (DDPG) for
power control in a distributed network of GPU-embedded AIR-T
SDRs. Additionally, all the recent works discussed above have been
limited to only simulations.

3 PROBLEM FORMULATION
3.1 An overview of Reinforcement Learning
In a general Markov decision process (MDP), a single or multiple
agents interact with their surrounding environment. For each action
taken by the agents, a feedback reward and the new state is returned
by the environment. At each given time step, ğ‘¡, the environmentâ€™s
essential features or state space (observation space), ğ‘  (ğ‘¡ ) âˆˆ S, is
observed by the agent where S is the set of all possible states. The
agent picks an action ğ‘ (ğ‘¡ ) from the set of all possible actions, A,
based on a policy which could be either deterministic and stochastic.
The deterministic and stochastic policies are denoted by ğœ‡, and ğœ‹,
respectively, where ğ‘ (ğ‘¡ ) = ğœ‡ (ğ‘  (ğ‘¡ ) ) or ğ‘ (ğ‘¡ ) = ğœ‹ (ğ‘  (ğ‘¡ ) ). After the ğ‘ (ğ‘¡ )
is taken by the agent, the environmentâ€™s state moves to a new state,
namely, ğ‘  (ğ‘¡ +1) . The transition probability from state-action pair, i.e.,
ğ‘ (ğ‘¡ )
< ğ‘  (ğ‘¡ ), ğ‘ (ğ‘¡ ) > to the new state ğ‘  (ğ‘¡ +1) is denoted by ğ‘ƒ
ğ‘  (ğ‘¡ ) âˆ’â†’ğ‘  (ğ‘¡ +1) =
ğ‘ƒğ‘Ÿ (ğ‘  (ğ‘¡ +1) |ğ‘  (ğ‘¡ ), ğ‘ (ğ‘¡ ) ). The reward signal, ğ‘Ÿ (ğ‘¡ +1) , which is returned
from the environment, determines how good or bad the action is.
The set of described interactions is called as an experience at ğ‘¡ + 1
and is denoted as ğ‘’ (ğ‘¡ +1) = (ğ‘  (ğ‘¡ ), ğ‘ (ğ‘¡ ), ğ‘  (ğ‘¡ +1), ğ‘Ÿ (ğ‘¡ +1) ).

In model free reinforcement learning, these interactions are
learned without any prior knowledge about the transition proba-
bilities and the optimal policy is achieved through maximizing the
long-term accumulative discounted return of agent at time ğ‘¡,

ğº (ğ‘¡ ) =

âˆ
âˆ‘ï¸

ğ‘–=0

ğ›¾ğ‘–ğ‘Ÿ (ğ‘¡ +ğ‘–+1),

(1)

where ğ›¾ < 1 denotes the discount factor. DDPG is a well known
actor-critic based technique which supports continuous action
spaces. In a DDPG network, a critic network is trained iteratively
which learns the action value function. The optimal deterministic
policy is determined by another network, namely, actor.

3.2 Power Control Algorithm
We assume ğ¾ nodes (wireless transceivers) with spreading sequence
matrix S âˆˆ Cğ¿Ã—ğ¾ forming ğ¾/2 transmit-receive pairs operating
simultaneously in the same frequency and ğ¿ corresponds to the
length of the spreading codes.

To maintain distributed operation and minimize overhead, we
use only local information and minimal information gathered from
immediate neighbors. The state of a node ğ‘– at time slot ğœ can be
expressed as

ğ‘– , ğ¼ğœ

ğ‘– , ğµğœ

ğ‘– = {ğ‘‘ğœ

ğ‘– = {Dğœ
Sğœ

ğ‘– , ğ‘†ğœ
ğ‘– }
(2)
where Dğœ
ğ‘– ğ‘— | ğ‘— = 1, 2, Â· Â· Â· , ğ¾ } is the set of distances to neigh-
boring receivers, ğµğœ
ğ‘– is the number of packets in the buffer, ğ¼ğœ
is the
ğ‘–
interference caused by a transmission from node ğ‘– (can be estimated
by node ğ‘–), and ğ‘†ğœ
ğ‘– is the interference sensed by the receiver of node
ğ‘–. The value for interference sensed at the receiver is communicated
to node ğ‘– through an ACK. If the transmission from node ğ‘– is not
received, then it will not receive an ACK message and the value for
ğ‘†ğœ
ğ‘– is set to -1. The action of node ğ‘– at time slot ğœ is given by

Ağœ

ğ‘– = {ğ‘ğ‘– |ğ‘ğ‘– âˆˆ A}
where the action space is the set of available power levels A =
[0.1mW, 5mW]. This is a hardware specific choice and can be cus-
tomized when the target hardware capabilities change.

We implement a reward function which aims to maximize SINR
on successful transmission and penalize based on interference
caused for unsuccessful transmission (i.e., transmission does not
complete with an acknowledgement). The SINR of a transmission
from node ğ‘– to node ğ‘— at a given slot ğœ is represented as Î“ğœ
ğ‘– ğ‘— . Let us
define the interference caused by a transmission from agent ğ‘– to
ğ‘ğ‘˜
agent ğ‘— be ğ¼ğ‘– ğ‘— = (cid:205)ğ‘˜â‰ ğ‘—â‰ ğ‘–
ğ‘‘ğ›¼ where the summation index ğ‘˜ goes over
all neighboring nodes where ğ‘‘ is the distance and ğ›¼ is the path loss
coefficient.

The reward for taking an action Ağ‘– is given below.

ğ‘…ğ‘– =

(cid:40) ËœÎ“ğ‘– ğ‘— if transmission successful
âˆ’ Ëœğ¼ğ‘– ğ‘— , if transmission unsuccessful

(3)

Where ËœÎ“ğ‘– ğ‘— and Ëœğ¼ğ‘– ğ‘— are, respectively, the normalized SINR and
normalized interference caused from a transmission from agent ğ‘– to
agent ğ‘—. We normalize these quantities by experimentally finding
the largest value likely to be observed and then using this value as a
divisor. We emphasize that this is only one formulation of a resource
allocation problem that can leverage the proposed framework. It
can be easily seen how this could be applied to a plethora of wireless
network optimization problems.

4 MR-INET GYM FRAMEWORK
In this work, we present the Marconi-Rosenblatt framework for
intelligent networks (MR-iNet Gym) shown in Fig. 1. The MR-iNet
Gym facilitates the designing, training, and deployment of the DRL

decision engine on embedded SDR. To accomplish the objective
of deploying DRL on embedded transceiver hardware, we identify
three main components, (i) A high fidelity simulator that facili-
tates design and setup of network scenarios while allowing the
transceivers (agents) to run DRL decision engines with ease, (ii)
methodology to optimize (from computation standpoint) the trained
model for embedded deployment on wireless transceivers, and (iii)
a flexible embedded software defined radio hardware that can house
the AI-enabled communication protocol stack.

4.1 Simulation Suite for AI-enabled Networks
For our simulation environment, to ensure rapid development, we
leverage several open source software packages for C++ and Python.
We begin by building the first known custom DS-CDMA module for
ns-3 to simulate a distributed LPI/D wireless network. Next, we use
OpenAI Gym [1] to handle the reinforcement learning training loop.
This enables us to rapidly implement several DRL algorithms to
compare and fine-tune the best-performing model. In this work, we
focus on DDPG with continuous action space as it maps naturally
onto the power control problem. Finally, we use ns3-gym [3] to
serve as a gateway between the simulation environment provided
by ns-3 with reinforcement learning structure provided by Gym.
Inside of Gym training loop our reinforcement learning agent then
receives state information, uses this information to select an action,
and then passes the selected action back to the environment. The
information flow between ns-3 and Gym that ns3-gym facilitates is
shown in Fig. 2.

4.2 Target Embedded Software Defined Radio
In the conventional embedded SDRs, General Purpose Processing
(GPPs) or Field Programmable Gate Arrays (FPGAs) are commonly
used to provide the processing capabilities. Neither of these is ideal
for deploying DRL decision engines and often leads to undesired la-
tency. Therefore, deploying the deep learning (DL)-based solutions
on the SDRs will even further reduce their speed which makes the
adoption of the conventional SDRs in the real-world applications
impractical in the fifth generation (5G) and beyond communication
systems. Therefore, it is required to utilize the parallel processing
computational power of the graphical processing units (GPUs) in
the embedded SDRs for: 1) offloading the processing of GPP/CPU
onto GPU to accelerate the signal processing; 2) making it practical
to deploy the DL-based solutions while not affecting the speed of
the signal processing algorithms through the parallel processing
capabilities of the embedded GPU. Due to the aforementioned rea-
sons, in this work, we implement and deploy our signal processing
operations and AC power control engines on the Deepwave Digitalâ€™s
Artificial Intelligence Radio Transceiver (AIR-T) which is the worldâ€™s
first SDR equipped with embedded GPUs. The AIR-Ts (shown in Fig.
7) are designed and developed to operate in the DL-based radio
frequency (RF) applications and they are equipped with embedded
NVIDIA GPU, an FPGA, and dual embedded CPUs.

4.3 Bridging the Reality Gap
Another key roadblock to overcome is the reality gap between simu-
lations and hardware. Even with reliable simulations, it is extremely
difficult to capture the unpredictable behaviors of the wireless chan-
nels and its instantaneous effects on the physical layer of modern

Figure 1: MR-iNet Gym Framework for Intelligent Networks

communication systems. While certain abstractions at the physical
layer are appropriate and highly beneficial in reducing the complex-
ity of the simulator, it is important those abstractions happen in an
informed manner trying to represent hardware-specific nuances
and model the interaction with the expected channel conditions as
closely as possible. In other words, it is important to feedback the
finding regarding generalized hardware-specific operation charac-
teristics from hardware evaluation back into the simulation suite
to minimize the degradation that can be caused by these reality
gaps. This interaction is represented by the interaction between
simulation and hardware deployment in Fig. 1.

5 SIMULATION AND TESTBED

EXPERIMENTS

Our testbed shown in Fig. 3 consists of six AIR-T SDRs which in
total establish three CDMA-based communication pairs. Due to
the space restriction, we do not elaborate on the physical layer
implementation but the solution can be extended to any CDMA
based tactical or commercial network. We implement AI-enabling
protocol stack on SDR to ensure communication and interfaces
between layers so that information can flow from different layers to
the DRL engine. The DRL agents are deployed on each transmitter
and the power control decisions are made by the agents after they
receive feedback from their corresponding receiver.

Figure 3: 6-node GPU-embedded SDR testbed.

5.1 Training in NS3-GYM
We train our AC power control agents with DDPG algorithm in ns3-
gym environment. The AC networks constitute an actor network
and a critic network. After trying different configurations, the final
actor network is a fully connected DNN with one hidden layer
where there are 10 neurons in the hidden layer. We use rectified
linear unit (ReLU) used as the activation function in all the layers.
The critic network is a fully connected DNN with four hidden layers
where there are 16, 32, 32, and 256 neurons in each hidden layer,
respectively. Similarly, ReLU function is adopted as the activation

Figure 2: ns3-gym information flow: Dotted lines
indicate the transfer of state information. Solid
lines indicate the communication of the action se-
lected by the RL agent.

function in all layers. The training is performed using the reward
function given in Eq 3 and the reward plot of the AC agents trained
by DDPG algorithm in ns3-gym environment is plotted in Fig. 4. As
it can be seen in Fig.4, the reward plot of the AC agents respectively
converging within 2000 steps. However, as it can be seen, there
is a high variance between different runs for the agents while
the environment has not changed. We will address this issue in
section 5.2 by introducing an approach to improve the stability of
DDPG actors.
5.2 Stabilizing DDPG With Model Aggregation
In a DDPG network, the policy is determined by a non-linear neural
network which is very sensitive to its parameters, namely, ğœƒ . This
implies that a small variation in ğœƒ could result in a very large
difference in the policy that is represented by the actor network. In
Figure 4, the reward plots of three agents on three different runs
on the same environment have been demonstrated. As it can be
seen, the variance of the rewards among different runs for the same
agent is very high. In other words, although our environment has
not changed, the DDPG agents are converging to different points
amongst different runs. This behavior implies that the DDPG agents
are not demonstrating a good stability. This is mainly caused due
to the non-convex optimization problem, where a small change
in the gradient steps can lead to a different policy which could be
arbitrarily a bad solution[18]. In general, policy-based DRL methods
such as DDPG perform poorly when compared against tabular
methods in terms of stability.

Therefore, in this paper, we introduce an approach which im-
proves the stability of policy-based DRL methods, in particular
DDPG network. We apply our approach on DDPG network and
verify its effectiveness through extensive simulations. However,
without loss of generality, this approach can be applied on any
policy-based DRL method. The approach that we propose to stabi-
lize the DDPG network is based on model aggregation. Combining
different models together is a well know approach in machine
learning to improve the robustness and decrease the variance of the
modelsâ€™ outputs. In this way, the sensitivity of the models to initial
parameters and noise is decreased. Algorithm 1 is the pseudo code
representing our approach for model aggregation and stabilizing
of DDPG agents.

We perform the model aggregation of the DDPG actors every 100
steps. As it can be seen in Fig 5, after performing the model aggrega-
tion, the actors show much less variance during the training among
several runs compared to the case where no model aggregation has
been performed, i.e., Fig 4.

CDMA ModuleCHNLPHYMACNETAPPNetworking Generating ScenariosBuilding & Testing Various RL ModelsSIMULATION[NS3-GYM]LPI/D PHY-MAC Design AI-Capable Protocol StackPHYMACMACNETTRANAPPDeploy Inference Engine on SDREMBEDDEDHARDWARE DEVELOPMENT [AIR-T SDR]Evaluate & FeedbackGYMGYMModel Optimization (TensorRT)C++Pythonns-3ns3-GymGatewayns3-GymProxyOpenAIGymRL AgentStateActionLegendTX 2RX 2RX 1TX 1RX 3TX 3Figure 4: Agentsâ€™ rewards w/o model aggregation

Figure 5: Agentsâ€™ rewards with model aggregation.

paper, for the first time, the performance of a SDR-based power al-
location algorithm which relies on TensorRT and CUDA to optimize
the DNN inference engine is presented. In Table 1 a comparison
between the original model(.h5 format), ONNX format and PLAN
(optimized by TensorRT) is demonstrated with respect to the aver-
age inference time of 30000 runs. The inference time of the original
model, and ONNX model are measured on GPP. However, the in-
ference time of the PLAN file (optimized by TensorRT) is measured
on the GPU (cannot run on GPP). As the results show, the total
inference time of the optimized model is significantly lower than
the original, and ONNX models, respectively, while the precision
of model has not been compromised.

Format

Original

ONNX

PLAN

Inference time
2266 ğœ‡s
14.6 ğœ‡s
0.23 ğœ‡s

Figure
SDR

7: AIR-T

Table 1: Inference time of a DNN
model in different formats

5.4 DRL Over-the-air Results
To deploy the models on the AIR-T SDRs, the PLAN files are re-
quired to be linked to the DS-CDMA physical layer to perform
inference. Our DS-CDMA physical layer is developed purely in
C++, however, the PLAN inference engines are CUDA and GPU
friendly. To manage the complexity in a large program like ours, it is
essential to break down the large program into smaller components.
In fact, the CUDA-based PLAN inference engines and C++ codes are
compiled separately and are linked together using the linking tools
that have been made available since CUDA 5.0. The experiments
were conducted in the 2.45 GHz ISM band. The power of the first
packet is set by the user (can be set to the max value to ensure
reliability). Upon the successful delivery of the first packet at the
receiver, an ACK (acknowledgment) message is transmitted from
the receiver to the transmitter which contains two key elements
(i) SINR, and (ii) the interference sensed at the receiver. The infor-
mation required for the DRL which included SINR, interference,
buffer length, and the distance of each transmitter from its intended
receiver and other receivers as well, are fed to the DRL agents to
choose optimal action (optimal power between 0 to 5 mW) for the
transmission of the next packet. For each experiment 1000 pack-
ets were transmitted and in total 16 experiments were performed
where each experiment corresponded to a different combination of

5.3 Optimizing the DRL Model for Hardware
To ensure real-time operation, it is essential to accelerate the de-
cision making process of the DRL-based inference engines on the
SDRs. Our preliminary results indicate that adopting the DNN
framework directly on the SDRs, can cause high latency in the
delivery of the packets, hence it is not an optimal approach for a
delay-sensitive application. In our SDR hardware setup, the Nvidia
Jetson TX2 module is the accelerating hardware module and the
TensorRT is the software framework for accelerating the DNN infer-
ence engines. TensorRT is comprised of an inference optimizer and
a runtime which together deliver low-latency and high-throughput
for resource allocation on SDRs. In Fig. 6, the flowchart of convert-
ing the DNN models to the optimized TensorRT format is depicted.

Figure 6: Flowchart of TensorRT model optimization.

Firstly, the Tensorflow based DNN model of the DRL network
which is trained in the ns3-gym environment is saved. Conse-
quently, this model is converted to the ONNX format. Then, the
inference engine is built by TensorRT which can be serialized to
PLAN file. Our SDR-based resource allocation method utilizes Ten-
sorRT to control the GPU-based DNN inference engine. In this

Algorithm1ModelAggregatedDDPG(MA-DDPG)N=NumberofactorsL=NumberoflayersineachactorÎ¸i=actoriÏ‰li=weightoflayerlofactoriÏ‰lagg=weightsoflayerlaftermodelaggregationl=1,2,...,NÎ¸agg=aggregatedactormodelS=NumberoftrainingstepsinitializeÎ¸ii=1,2,...,Nforsteps=1,2,...,Sdofori=1,2,...,NdoRunpolicyÏ€Î¸iinenvironmentfor100timestepsandupdateÎ¸isteps%100=0forl=1,2,...,LdoÏ‰lagg=average([Ï‰li])i=1,2,...,NendforÎ¸agg=clone([Ï‰lagg])l=1,2,...,LÎ¸i(i=1,2,...,N)=Î¸aggendforendfor1bytesNetwork definition EngineBuilderRuntimeOutput tensorsOptimization ParametersONNXTf/kerasPLANInput TensorDeserializeSerializeFigure 8: Estimated power for DRL and DCPC.

Figure 9: The average PDR for DRL and DCPC.

Format

Power

PDR

DRL

0.19mW .922

DCPC

0.59mW .955

Table 2: Performance Com-
parison of DRL agents and
DCPC Algorithm

the transmitter and receiver gains to represent different operation
constraints. To evaluate the performance of the DRL agents, we
compare their performance against DCPC[4, 11], in terms of PDR
and total power consumption.

Fig. 8 depicts the estimated average power of 1000 packet trans-
mission for different experiments. As it was mentioned, each ex-
periment corresponds to a different combination of the transmitter
and receiver gains. Fig. 9 shows the average PDR of DCPC and
DRL. The power consumption of DCPC is significantly higher than
of DRL without a significant gain in PDR. In Table II, the average
performance metrics of DCPC and DRL are presented. The results
indicate that the power consumption of DCPC is â‰ˆ 3 times higher.
6 CONCLUSION AND FUTURE WORK
Machine learning has been rapidly emerging as a tool to accelerate
the strides towards realizing the next-generation of radios. While
there have been several advancements in this domain, the major-
ity of them have been limited to simulations or relied on host-PC
with ample computational resources. Due to the inherent need for
mobility, energy-efficiency, and other strict Quality-of-Service re-
quirements, it is inevitable that machine learning models will have
to live on embedded devices (at the edge) to ensure AI-enabled
radios can make real-world impacts. In this work, for the first time
in literature, we have used GPU-embedded SDR to demonstrate
real-time (<ğœ‡s) distributed decision making for a resource allocation
problem in distributed wireless networks. To accomplish this, we
proposed the MR-iNet Gym framework that orchestrates a cohesive
development process. This leverages ns3-gym simulation for train-
ing, optimizing the trained model for hardware deployment, and
achieving superior performance on GPU-embedded SDR hardware
in our over-the-air experiments.

In the future, we hope to publish an extended version discussing
the physical layer and AI-enabling protocol stack implementation
in detail. In 2022, we also hope to release the ns3-gym software
since it is the first one developed for the CDMA network. We are
actively utilizing MR-iNet Gym to address a wide variety of network
optimization problems which we hope to discuss in our future work.
Finally, we hope the contribution of this paper has a broader impact
that motivates the community to adopt the design ethos of MR-iNet
Gym to solve open research challenges in the wireless domain.

ACKNOWLEDGMENT AND DISCLAIMER
This material is based upon work supported by the US Army Con-
tract No. W15P7T-20-C-0006. Any opinions, findings, and conclu-
sions or recommendation expressed in this material are those of the
author(s) and do not necessarily reflect the views of the US Army.

REFERENCES
[1] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John
OpenAI Gym.

Jie Tang, and Wojciech Zaremba. 2016.

Schulman,
arXiv:1606.01540 [cs.LG]

[2] Tamer ElBatt and Anthony Ephremides. 2004. Joint scheduling and power control
for wireless ad hoc networks. IEEE Transactions on Wireless communications 3, 1
(2004), 74â€“85.

[3] Piotr GawÅ‚owicz and Anatolij Zubow. 2019. ns-3 meets OpenAI Gym: The
Playground for Machine Learning in Networking Research. In ACM Intl. Conf. on
Modeling, Analysis and Sim. of Wireless and Mobile Systems (Miami Beach, USA).
[4] Sudheer A Grandhi, Jens Zander, and Roy Yates. 1994. Constrained power control.

Wireless Personal Communications 1, 4 (1994), 257â€“270.

[5] Ade Hermawan, Rizki Ginanjar, Dong-Seong Kim, and Jae-Min Lee. 2020. CNN-
Based Automatic Modulation Classification for Beyond 5G Communications.
IEEE Communications Letters 24, 5 (2020), 1038â€“1041.

[6] Md Habibul Islam, Ying-Chang Liang, and Anh Tuan Hoang. 2007. Distributed
power and admission control for cognitive radio networks using antenna arrays.
In 2007 2nd IEEE International Symposium on New Frontiers in Dynamic Spectrum
Access Networks. IEEE, 250â€“253.

[7] Anu Jagannath, Jithin Jagannath, and Tommaso Melodia. 2021. Redefining Wire-
less Communication for 6G: Signal Processing Meets Deep Learning with Deep
Unfolding. IEEE Transaction on Artificial Intelligence (2021).

[8] Jithin Jagannath, Nicholas Polosky, Dan Connor, Lakshmi Theagarajan, Brendan
Sheaffer, Svetlana Foulke, and Pramod Varshney. 2018. Artificial Neural Network
based Automatic Modulation Classifier for Software Defined Radios. In Proc. of
IEEE Intl, Conf. on Communications (ICC). Kansas City, USA.

[9] Jithin Jagannath, Nicholas Polosky, Anu Jagannath, Francesco Restuccia, and
Tommaso Melodia. 2019. Machine Learning for Wireless Communications in the
Internet of Things: A Comprehensive Survey. Ad Hoc Networks (Elsevier) (2019).
[10] Jithin Jagannath, Nicholas Polosky, Anu Jagannath, Francesco Restuccia, and
Tommaso Melodia. 2020. Neural Networks for Signal Intelligence: Theory and
Practice,. In Machine Learning for Future Wireless Communications, F.L. Luo (Ed.).
John Wiley & Sons, Limited.

[11] Xingjian Li, Jun Fang, Wen Cheng, Huiping Duan, Zhi Chen, and Hongbin Li.
2018. Intelligent power control for spectrum sharing in cognitive radios: A deep
reinforcement learning approach. IEEE access 6 (2018).

[12] Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang,
Ying-Chang Liang, and Dong In Kim. 2019. Applications of Deep Reinforcement
Learning in Communications and Networking: A Survey. IEEE Communications
Surveys Tutorials 21, 4 (2019), 3133â€“3174.

[13] Yasar Sinan Nasir and Dongning Guo. 2020. Deep Actor-Critic Learning for
Distributed Power Control in Wireless Mobile Networks. In 2020 54th Asilomar
Conference on Signals, Systems, and Computers. IEEE, 398â€“402.

[14] Keyvan Ramezanpour and Jithin Jagannath. 2022. Intelligent Zero Trust Archi-
tecture for 5G/6G Networks: Principles, Challenges, and the Role of Machine
Learning in the context of O-RAN. arXiv preprint arXiv:2105.01478 (2022).
[15] Kaiming Shen and Wei Yu. 2018. Fractional programming for communication
systemsâ€”Part I: Power control and beamforming. IEEE Transactions on Signal
Processing 66, 10 (2018), 2616â€“2630.

[16] Haoran Sun, Xiangyi Chen, Qingjiang Shi, Mingyi Hong, Xiao Fu, and Nicholas D
Sidiropoulos. 2018. Learning to optimize: Training deep neural networks for
interference management. IEEE Transactions on Signal Processing (2018).
[17] John Tadrous, Ahmed Sultan, Mohammed Nafie, and Amr El-Keyi. 2010. Power
control for constrained throughput maximization in spectrum shared networks.
In IEEE Global Communications Conference (GLOBECOM).

[18] Chen Tessler, Nadav Merlis, and Shie Mannor. 2019. Stabilizing off-policy rein-

forcement learning with conservative policy gradients. (2019).

[19] Mingbo Xiao, Ness B Shroff, and Edwin KP Chong. 2003. A utility-based power-
control scheme in wireless cellular systems. IEEE/ACM Transactions On Network-
ing 11, 2 (2003), 210â€“221.

# of Experiment246810121416Estimated Power00.20.40.60.81DCPC Average Power of 3 PairsDRL Average Power of 3 Pairs# of Experiment246810121416Packet Delivery Ratio0.70.750.80.850.90.951DCPC Average of 3 PairsDRL Average of 3 Pairs