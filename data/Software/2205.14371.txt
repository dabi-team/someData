GLITCH: Automated Polyglot Security Smell Detection in
Infrastructure as Code

Nuno Saavedra
nuno.saavedra@tecnico.ulisboa.pt
INESC-ID and IST, University of Lisbon
Lisbon, Portugal

João F. Ferreira
joao@joaoff.com
INESC-ID and IST, University of Lisbon
Lisbon, Portugal

2
2
0
2

p
e
S
7

]

R
C
.
s
c
[

3
v
1
7
3
4
1
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Infrastructure as Code (IaC) is the process of managing IT in-
frastructure via programmable configuration files (also called IaC
scripts). Like other software artifacts, IaC scripts may contain se-
curity smells, which are coding patterns that can result in security
weaknesses. Automated analysis tools to detect security smells in
IaC scripts exist, but they focus on specific technologies such as
Puppet, Ansible, or Chef. This means that when the detection of a
new smell is implemented in one of the tools, it is not immediately
available for the technologies supported by the other tools — the
only option is to duplicate the effort.

This paper presents an approach that enables consistent security
smell detection across different IaC technologies. We conduct a
large-scale empirical study that analyzes security smells on three
large datasets containing 196,755 IaC scripts and 12,281,251 LOC.
We show that all categories of security smells are identified across
all datasets and we identify some smells that might affect many
IaC projects. To conduct this study, we developed GLITCH, a new
technology-agnostic framework that enables automated polyglot
smell detection by transforming IaC scripts into an intermediate
representation, on which different security smell detectors can be
defined. GLITCH currently supports the detection of nine different
security smells in scripts written in Ansible, Chef, or Puppet. We
compare GLITCH with state-of-the-art security smell detectors. The
results obtained not only show that GLITCH can reduce the effort
of writing security smell analyses for multiple IaC technologies,
but also that it has higher precision and recall than the current
state-of-the-art tools.

CCS CONCEPTS
• Software and its engineering → Software configuration man-
agement and version control systems; Software maintenance
tools; • Security and privacy → Software and application se-
curity.

KEYWORDS
devops, infrastructure as code, security smells, Ansible, Chef, Pup-
pet, intermediate model, static analysis

ASE ’22, October 10–14, 2022, Rochester, MI, USA
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
This is the author’s version of the work. It is posted here for your personal use. Not
for redistribution. The definitive Version of Record was published in 37th IEEE/ACM
International Conference on Automated Software Engineering (ASE ’22), October 10–14,
2022, Rochester, MI, USA, https://doi.org/10.1145/3551349.3556945.

ACM Reference Format:
Nuno Saavedra and João F. Ferreira. 2022. GLITCH: Automated Polyglot
Security Smell Detection in Infrastructure as Code. In 37th IEEE/ACM In-
ternational Conference on Automated Software Engineering (ASE ’22), Octo-
ber 10–14, 2022, Rochester, MI, USA. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3551349.3556945

1 INTRODUCTION
Infrastructure as Code (IaC) is a process which has been progres-
sively gaining more adoption in the DevOps world since it facilitates
the provision of scalable and reproducible environments. In current
practice, the use of IaC scripts is essential to efficiently maintain
servers and development environments. For example, according
to Rahman et al. [18], Intercontinental Exchange (ICE), a Fortune
500 company, maintains 75% of its 20,000 servers using IaC scripts.
The use of IaC scripts has helped ICE decrease the time needed to
provision development environments from 1–2 days to 21 minutes.
Despite its many benefits, IaC scripts may contain defects that
can have serious implications. For instance, due to bugs in their
IaC scripts, GitHub experienced an outage of their DNS infrastruc-
ture [2] and Amazon Web Services lost around 150 million USD after
issues with their S3 billing system [5]. To address this, there has
been an effort by the research community to categorize and iden-
tify defects, and in particular, so-called security smells, which are
coding patterns that can result in security weaknesses [15, 18–20].
Even when a security smell does not lead to a security breach, it
deserves attention and inspection. It is thus important to develop
automated methods that can assist developers identifying security
smells in their IaC scripts. Two influential automated tools devel-
oped by the research community are SLIC [18], which supports
seven types of security smells in Puppet1 scripts, and SLAC [19],
which supports nine types of security smells in Chef2 scripts and
six types in Ansible3 scripts. These tools are very valuable, since
they cover a wide range of security smells and three of the major
IaC technologies. However, their implementations are separate and
involve substantial duplication. If one wishes to implement the
detection of a new smell, one has to develop a different implemen-
tation for each of the IaC technologies supported. Consequently, it
is often the case that the detection of security smells is inconsistent
for different IaC technologies. Figure 1a presents part of a Chef
script with no security smells taken from the project Vagrant Chef
for CakePHP.4 For this example, SLAC reports a false positive: a
non-existent security smell of type Hard-coded secret. On the other

1https://puppet.com
2https://www.chef.io
3https://www.ansible.com
4https://github.com/FriendsOfCake/vagrant-chef/blob/
288336e506a5009ed93c06a784fa93e30a27040c/cookbooks/percona/recipes/server.rb#
L28

 
 
 
 
 
 
ASE ’22, October 10–14, 2022, Rochester, MI, USA

Saavedra and Ferreira

hand, if we consider the same script in Puppet (Figure 1b), SLIC will
not report any security smell. Surprisingly, inconsistencies exist
even when considering the same tool: SLAC will not report any
security smell when considering the same script in Ansible (this
happens because SLAC uses separate code for Ansible and Chef).
These inconsistencies would not occur if we had polyglot defect
prediction and debugging environments for IaC, a direction recently
proposed by Alnafessah et al. [1]. Also, a problem that has been
observed by Guerriero et al., after interviews with IaC experts,
is that the IaC technology ecosystem is currently very scattered
and heterogeneous [3]. Guerriero et al. also identified the need to
develop more IaC development tools, such as static analysis tools
and security-related tools. It is thus clear that it would be beneficial
to develop unifying methods that can reduce inconsistencies.

This paper presents an approach that enables consistent security
smell detection across different IaC technologies. We conduct a
large-scale empirical study that analyzes security smells on three
large datasets containing Ansible, Chef, and Puppet scripts. We
show that all categories of security smells are identified across
all datasets and we identify some smells that might affect many
IaC projects. To conduct this study, we developed GLITCH, a new
technology-agnostic framework that enables automated polyglot
smell detection by transforming IaC scripts into an intermediate
representation, on which different security smell detectors can be
defined. GLITCH currently supports the detection of nine different
security smells and it can analyze scripts written in Puppet, Ansible,
or Chef. We compare GLITCH with the state-of-the-art security
smell detectors SLIC [18] and SLAC [19]. The results obtained not
only show that GLITCH can reduce the effort of writing security
smell analyses for multiple IaC technologies, but also that it has
higher precision and recall than the current state-of-the-art tools.
Contributions. Our main contributions are: (1) A new inter-
mediate representation that can be used to model IaC scripts and
on which security smell detection rules can be defined. (2) The
implementation of a framework called GLITCH that is able to trans-
form IaC scripts written in Ansible, Chef, or Puppet into the new
intermediate representation, and that supports the detection of
nine security smells. We show that the average precision and recall
values of GLITCH are substantially better than the average pre-
cision and recall values of state-of-the-art tools. (3) An empirical
study that investigates how frequently security smells occur in
IaC scripts. We consider Ansible, Chef, and Puppet scripts. We use
three large datasets containing 196,755 IaC scripts and 12,281,251
LOC. We show that all categories of security smells are identified
across all datasets and we identify some smells that might affect
many IaC projects. (4) A replication package containing all the
datasets used in this work, including three oracle datasets that
were manually annotated. We tried to use replication packages
from other authors, but they were lacking data. As a result, to the
best of our knowledge, our replication package is the first to be
complete and available. It is available as a Docker container at
https://doi.org/10.6084/m9.figshare.19726603.v2

GLITCH is open source and available from GitHub:

https://github.com/sr-lab/GLITCH

Characteristic

Ansible

Conf. Setup

Add. Agent

Syntax

Push

No

YAML

Exec. Order

Procedural

Atomic Unit

Task

Code
Structure

Roles
- Playbooks
- - Plays
- - - Tasks

Chef

Pull

Yes

Ruby

Procedural

Resource

Cookbooks
- Recipes
- - Resources

Puppet

Pull

Yes

Puppet DSL

Declarative

Resource

Modules
- Manifests
- - Classes
- - - Resources

Table 1: Summary of Ansible, Chef and Puppet’s characteristics.

2 BACKGROUND AND RELATED WORK
We focus on IaC tools for configuration management of services.
The main reason is that the ecosystem around this category of
tools is heterogeneous with several technologies widely adopted.
Guerriero et al. [3] listed four technologies in this category that are
adopted by industry experts: Ansible, Chef, Puppet, and Saltstack.
Out of these four, three of them had an adoption rate greater than
29%: Puppet with 29.5%, Chef with 36.3%, and Ansible with 52.2%
(note that industry experts can adopt more than one technology
simultaneously). To maximize impact of our work, we focus on
these three technologies.

2.1 Ansible, Chef, and Puppet Scripts
We provide a brief background on Ansible, Chef, and Puppet scripts.
Table 1 summarizes and compares some relevant characteristics of
these technologies. There are two types of configuration setups for
IaC technologies: push and pull. In a push configuration setup, the
sysadmin commands a centralized server, able to connect to every
node, to provide the configuration to a set of nodes. In a pull config-
uration setup, each node periodically contacts the server to retrieve
the latest configuration for that particular machine. Technologies
may require an additional agent to be installed in every node. The
agent is a program that runs as a background service and is capa-
ble of doing the necessary operations in the nodes (e.g., updates).
Regarding, syntax, IaC technologies use different programming
languages. Ansible uses YAML, Chef uses Ruby, and Puppet uses a
domain-specific language (DSL). Using a programming language
like Ruby allows complex programs to be written. However, it may
be more difficult to abstract the concepts being represented. Ansible
and Chef encourage a procedural style, which means that scripts
follow, in order, a sequence of instructions specified by practition-
ers. On the other hand, Puppet uses a declarative style, in which
practitioners specify the desired state and it is up to the Puppet
tool to decide how the state is achieved. Regarding atomic units
and code structure, while Ansible considers the notion of Task as
the atomic unit, both Chef and Puppet use the notion of Resource.
In Ansible, configurations are managed using Playbooks, which are
decomposed into Plays that define Tasks. In the case of Chef, con-
figurations are defined as Cookbooks, which are decomposed into
Recipes specifying Resources. Puppet structures the configurations
using Modules that contain configuration files called Manifests.
Resources are specified in Classes, which are named blocks used to
configure larger chunks of functionality.

GLITCH: Automated Polyglot Security Smell Detection in Infrastructure as Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

server_root_password = node['mysql']['server_root_password']
execute 'set-mysql-root' do

command <<-EOH

mysqladmin -u root password #{server_root_password}
mysql -uroot -p#{server_root_password} -e (...)

EOH
only_if "/usr/bin/mysql -u root -e 'show databases;'"

end

$server_root_password = $facts['mysql']['server_root_password']
exec { 'set-mysql-root':

command => @("COMMAND"/L)

mysqladmin -u root password ${server_root_password}
mysql -uroot -p${server_root_password} -e (...)

| COMMAND,
only_if => "/usr/bin/mysql -u root -e 'show databases;'"

}

(a) Part of a Chef script (from Vagrant Chef for CakePHP)

(b) Same part of a Chef script rewritten in Puppet

Figure 1: Inconsistencies in state-of-the-art tools: SLAC reports false positive “Hard-coded secret” for script (a); SLIC does not report any
security smell for script (b).

2.2 Security Smells in IaC Scripts
Several catalogs and categories of code smells for IaC scripts have
been proposed. Sharma et al. [26] created a configuration smells
catalog for Puppet scripts with 13 implementation and 11 design
configuration smells. Schwarz et al. [24] extended the research
done by Sharma et al. by applying the detection of IaC smells to
Chef scripts. Rahman and Williams [20] characterized defective IaC
scripts by extracting text features from faulty scripts. Rahman and
Williams [21] identified 10 source code properties that correlate
with defective IaC scripts. Rahman et al. [15] proposed a defect
taxonomy for IaC scripts that includes eight categories. In another
work, Rahman et al. [16] identified five development anti-patterns
for IaC scripts. Focusing on security smells, Rahman et al. [17]
concluded, after a systematic mapping study with 32 IaC-related
publications, that there is a need for more research studies focused
on defects and security flaws for IaC. Rahman et al. [18] identi-
fied seven types of security smells that are indicative of security
weaknesses in Puppet scripts. Rahman et al. [19] later replicated
this study for Ansible and Chef scripts, identifying two additional
security smells.

Like Rahman et al. [18, 19], we also focus on security smells.
However, to the best of our knowledge, we are the first to provide
a method for enabling consistent security smell detection across
different IaC technologies. We consider the following nine security
smells (we adapt the descriptions by Rahman et al. [19]): Admin
by default (CWE-250 [11]): This smell is the recurring pattern of
specifying default users. The smell can violate the “principle of least
privilege” property [14]. Empty password (CWE-258 [11]): The
smell is the recurring pattern of using a string of length zero for a
password. Hard-coded secret (CWE-259, CWE-798 [11]): This
smell is the recurring pattern of revealing sensitive information,
such as user name and passwords in IaC scripts. Unrestricted
IP Address (CWE-284 [11]): This smell is the recurring pattern
of assigning the address 0.0.0.0 for a database server or a cloud
service/instance. Binding to the address 0.0.0.0 may cause security
concerns as this address can allow connections from every possi-
ble network [13]. Suspicious comment (CWE-546 [11]): This
smell is the recurring pattern of putting information in comments
about the presence of defects, missing functionality, or weakness of
the system (e.g., “TODO” and “FIXME”). Use of HTTP without
SSL/TLS (CWE-319 [11]): : This smell is the recurring pattern of
using HTTP without the Transport Layer Security (TLS) or Secure
Sockets Layer (SSL). Such use makes the communication between
two entities less secure [22]. No integrity check (CWE-353 [11]):
This smell is the recurring pattern of downloading content from

the Internet and not checking the downloaded content using check-
sums or gpg signatures. Use of weak cryptography algorithms
(CWE-326, CWE-327 [11]): This smell is the recurring pattern
of using weak cryptography algorithms, namely, MD5 and SHA-1,
for encryption purposes. Missing Default in Case Statement
(CWE-478 [11]): This smell is the recurring pattern of not han-
dling all input combinations when implementing a case conditional
logic.

2.3 Related Work
Several studies have been published on code quality and security
coding practices for IaC scripts. For example, Jiang and Adams [7]
conducted an empirical study on the co-evolution of IaC scripts and
other software artifacts. They found that the IaC scripts are coupled
tightly with the other files in a project. Hanappi et al. [4] intro-
duced a conceptual framework for asserting reliable convergence
in configuration management. Van der Bent et al. [28] proposed a
code quality model for Puppet and validated it with experts.

In terms of analysis tools for IaC scripts, Hanappi et al. [4] pro-
pose a tool that detects idempotence and convergence related issues
in a set of existing Puppet scripts. Schwarz et al. [24] picked smells
from the catalog proposed by Sharma et al. [26] and convert them
into detection rules for Foodcritic, a static code analysis tool de-
signed for Chef. Sotiropoulos et al. [27] propose a tool for detecting
faults regarding ordering violations and notifiers in Puppet scripts.
Lepillet et al. [10] propose Häyä, a tool that uses dataflow graph
analysis to detect intra-update sniping vulnerabilities in CloudFor-
mation templates. More relevant for our work are the tools SLIC
and SLAC, which are focused on security smells. SLIC, developed
by Rahman et al. [18], detects seven types of security smells in
Puppet scripts and SLAC, developed by Rahman et al. [19], detects
nine types of security smells in Chef scripts and six types in An-
sible scripts. Our tool extends the state-of-the-art by providing
the first IaC-technology-agnostic framework that can be used to
unify tools such as SLIC and SLAC, facilitating the detection of
security smells in different IaC technologies. When compared with
the security smells supported by SLIC and SLAC, we also identify
two additional types of smell in Puppet scripts (Missing default
case statement and No integrity check) and two additional types in
Ansible scripts (Admin by default and Use of weak cryptographic
algorithms).

Finally, some analysis tools for IaC use intermediate represen-
tations [6, 25, 27] to describe file-system manipulations done by
IaC scripts. Shambaugh et al. [25] translated IaC scripts to the in-
termediate representation by mapping types of resources to their

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Saavedra and Ferreira

Table 2: Correspondence between the abstract concepts and the con-
cepts in each IaC technology.

Abstract Concepts

Ansible

Chef

Puppet

Modules
Unit Blocks
Atomic Units

Roles
Playbooks
Tasks

Cookbooks
Recipes
Resources

Modules
Manifests, Classes
Resources

filesystem operations. Sotiropoulos et al. [27] used system calls
executed by each resource in a IaC script to automatically map the
resources to the filesystem operations, which were represented in
the intermediate language. To the best of our knowledge, our work
is the first that translates scripts of different IaC technologies into
an intermediate representation.

3 INTERMEDIATE REPRESENTATION
In order to achieve a technology-agnostic framework, we use an
intermediate representation. Our representation is able to capture
similar concepts from different IaC technologies, while assuring
it is expressive enough to apply analyses that identify security
smells. Figure 2 describes the abstract syntax of our intermedi-
ate representation. We follow an object-oriented approach with
a hierarchical structure. As the top-level structure, the intermedi-
ate representation can model a Project, a Module, or a Unit block.
Projects represent a generic folder that may contain several mod-
ules and unit blocks. This structure allows us to represent the
high-level code structures described in Table 1 from Ansible, Chef
and Puppet. Table 2 shows the relation between the high-level code
structures in each IaC technology and the abstract concepts in our
intermediate representation. As the table shows, it is possible to
find similar structures in the different technologies. Modules are
the top component from each structure and they agglomerate the
scripts necessary to execute a specific functionality. Modules are
file system folders, usually with a specific organization (e.g. a role
in Ansible usually has a tasks and a vars folder where, respectively,
the tasks and variables for the role are defined). Unit Blocks corre-
spond to the IaC scripts themselves or to a group of atomic units.
For instance, in Puppet, we can agglomerate resources in classes.
Finally, Atomic Units are the building block of IaC scripts. Atomic
units define the system components we want to change and the
actions we want to perform on them. As shown in Figure 2, unit
blocks can have attribute definitions, variable definitions, and con-
ditions. Atomic units have attribute definitions. When values in
attribute and variable definitions use variable references, the field
has_variable is true. For instance, in Figure 1b, the definition of
the variable $server_root_password has as value a reference to
the variable $facts['mysql']['server_root_password'], set-
ting the field has_variable in our intermediate representation to true.
Figure 3 shows a graph-based visualization of how our intermediate
representation models the scripts in Figure 1a and Figure 1b.

4 SECURITY SMELL DETECTION
In Table 3, we define the rules used by GLITCH to detect security
smells. The formalism used to define rules is similar to the one used
by SLIC [18] and SLAC [19]. The functions isAttribute, isVariable,
isComment, isAtomicUnit, and isConditionStatement verify the type
of instance being analyzed (e.g., if the node 𝑥 is an Attribute node,

<S> ::= <project>
| <module>
| <unitblock>

<project> ::=
Project {

name: <str>,
modules: <module>*
blocks: <unitblock>*

}

<module> ::=
Module {

name: <str>,
blocks: <unitblock>*

}

<condition> ::=

ConditionStatement {
type: IF | SWITCH
condition: <str>,
else_statement: <condition>,
is_default: <bool>

}

<comment> ::=
Comment {

content: <str>

}

<unitblock> ::=
UnitBlock {

name: <str>,
atomic_units: <atomicunit>*,
variables: <variable>*,
attributes: <attributes>*
comments: <comment>*,
conditions: <condition>*,
unit_blocks: <unitblock>*

}

<atomicunit> ::=
AtomicUnit {

name: <str>,
type: <id>,
attributes: <attribute>*

}

<attribute> ::=
Attribute {

name: <id>,
value: <value>,
has_variable: <bool>

}

<variable> ::=

Variable {

name: <id>,
value: <value>,
has_variable: <bool>

}

-

-

-

-

-

-

-

-

-

-

-
-
-
<value> ::= <str> | <number> | <bool> | <value>* | <id>
<id> ::= ;sequence of alphanumerics which starts with a letter
<str> ::= "<character>*"
<bool> ::= True | False

<number> ::= ;integer or double

-

-

-

-

-

-

-

-

-

-

-

Figure 2: Abstract syntax of our intermediate representation.

Figure 3: Graph-based representation of the scripts in Figure 1 us-
ing our intermediate representation. In black and red: the represen-
tation of the Chef script from Figure 1a. In black and orange: the
representation of the Puppet script from Figure 1b.

isAttribute(x) is true). Each node in our representation is referred
by the variable x. We traverse the nodes using a depth-first search
(DFS). We start in the initial node (a Project, a Module or a Unit
Block) and then we execute the DFS considering each collection
inside the node as its children. Each node may have more than
one security smell, and so every rule is applied, even if a smell
was already identified for that node. Previous nodes have no in-
fluence in the analyses of other nodes. The function hasDownload
goes through a list of attributes and verifies if for at least one of
them isDownload(x.value) is true. The same goes for the function
hasChecksum but instead of using isDownload, it uses isChecksum.
The function isDefault is a recursive function that returns true if a
default branch is found in the case statement, and false otherwise.
The remaining functions are defined in Table 4. These functions
verify if any of the string patterns described are present in the
values they receive.

Unit Blockname =Variablename = server_root_passwordvalue =has_variable = TrueAtomic Unitname = set-mysql-roottype =Attributename = commandvalue = "mysqladmin -u (...)"has_variable = FalseAttributename = only_ifvalue = "/usr/bin/mysql (...)"has_variable = Falseserver.ppexec$facts['mysql']['server_root_password']server.rbexecutenode['mysql']['server_root_password']GLITCH: Automated Polyglot Security Smell Detection in Infrastructure as Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

The GLITCH framework allows the definition of different con-
figurations to identify security smells. These configurations change
the keywords in the (disjunctive) string patterns for each function
defined in Table 4. In the table, we describe the configuration used
by the improved version of GLITCH to which we will refer in Sec-
tion 5. Configurations allow users to tweak the tool to best suit
the needs of the IaC developers and to better adapt to each IaC
technology. GLITCH is implemented in Python and it currently
supports the analysis of Ansible, Chef, and Puppet scripts. Our im-
plementation transforms the original scripts into our intermediate
representation and then attempts to detect security smells as de-
scribed above. To parse the Ansible scripts we used the ruamel.yaml
package5 for Python. The Chef scripts were parsed using Ripper,6
a script parser for Ruby. We developed a parser for Ripper’s output
using a package called ply.7 Finally, for Puppet scripts, we devel-
oped our own parser8 using the same ply package. We decided to
develop our parser since we did not find any other good options to
parse Puppet DSL in Python.

5 EVALUATION
This section describes the evaluation of GLITCH.

5.1 Research Questions
RQ1. [Abstraction] Can our intermediate representation model
IaC scripts and support automated detection of security
smells?

RQ2. [Accuracy and Performance] How does GLITCH com-
pare with existing state-of-art tools for detecting security
smells in terms of accuracy and performance?

RQ3. [Frequency] How frequently do security smells occur in

IaC scripts?

5.2 Datasets
This section describes how we constructed the datasets used for
our evaluation. Since we consider Ansible, Chef, and Puppet scripts,
our first step was to attempt to obtain the same datasets as used in
the studies involving SLIC and SLAC [18, 19]. We got hold of the
publicly available datasets9 and Docker image10, and we observed
that only the oracle for Ansible was available. We thus contacted
the first author of the studies mentioned above, who very kindly
shared with us a Puppet dataset almost identical to the one used in
the empirical study using SLIC (there were small differences in the
number of Puppet scripts contained in the dataset). We constructed
oracle datasets for Chef and Puppet as these oracle datasets were
not available as part of Rahman et al.’s replication packages. We
further contacted the first author about the availability of the oracle
datasets and learned that these datasets reside in computing clusters
to which the first author no longer has access to. Given this, we
decided to reuse their oracle for Ansible and the Puppet dataset,
and to construct new oracles for Chef and Puppet, and new IaC
datasets for Ansible and Chef.

5https://pypi.org/project/ruamel.yaml/
6https://github.com/ruby/ruby/tree/master/ext/ripper
7https://github.com/dabeaz/ply
8https://github.com/Nfsaavedra/puppetparser
9https://doi.org/10.6084/m9.figshare.8085755
10https://hub.docker.com/repository/docker/akondrahman/slic_ansible

IaC datasets. To perform an empirical study of security
5.2.1
smells in Ansible, Chef, and Puppet scripts, we require three datasets
of IaC scripts, one for each technology. As mentioned above, we
reused Rahman et al.’s Puppet dataset [18], which is composed of
four different sub-datasets. Three datasets are constructed using
repositories collected from three organizations: Mozilla (MOZ),
Openstack (OST), and Wikimedia (WIK). The fourth dataset is con-
structed from repositories hosted on GitHub (GH).

For Ansible and Chef, we created two new datasets by select-
ing OSS repositories from GitHub. As described in previous re-
search [12], OSS repositories need to be curated. We apply the same
criteria that Rahman et al. [18] used to construct their Puppet sub-
datasets extracted from GitHub (except that we consider all the
available repositories created between 2012 and 2022):

Criterion 1: At least 11% of the files belonging to the repository
must be IaC scripts. This follows from a Jiang and Adams’s study [7],
where it was observed that in OSS repositories, a median of 11% of
the files are IaC scripts. The rationale is to collect repositories that
contain sufficient amount of IaC scripts for analysis. Criterion 2:
The repository is not a clone. Criterion 3: The repository must
have at least two commits per month. This is based on Munaiah et
al. [12], who used the threshold of at least two commits per month
to determine which repositories have enough software development
activity. Criterion 4: The repository has at least 10 contributors.
Similar to Rahman et al. [18], we assume that this criterion may
help us to filter out irrelevant repositories.

Table 5 presents the number of repositories, the number of IaC
scripts, and the number of LOC in the three IaC datasets. The An-
sible dataset was constructed from 681 repositories and contains
108,509 Ansible scripts (5,180,747 LOC). The Chef dataset was con-
structed from 439 repositories and contains 70,939 Chef scripts
(6,071,035 LOC). The Puppet dataset was constructed from 293
repositories and contains 17,307 Puppet scripts (1,029,469 LOC).
When considering the three IaC datasets as a whole, there are 1413
repositories with 196,755 IaC scripts. In total, there are 12,281,251
LOC.

5.2.2 Oracles. To determine the accuracy of GLITCH and to com-
pare it with other tools, we require three oracle datasets, one for
each IaC technology considered. In what follows, we describe how
we selected the IaC scripts included in each oracle and how we
annotated the datasets.

File collection. For the Ansible oracle, we reused Rahman et al.’s
oracle [19], which contains 81 IaC scripts. We constructed new
oracle datasets for Chef and Puppet. To ensure that the size of the
three oracles was similar, based on the size of the Ansible oracle
dataset, we decided to create oracles with exactly 80 IaC scripts.
To select the files, we wrote a Python script that kept selecting a
random file from the respective IaC dataset described in the previous
subsection while the desired size was not achieved. For each file,
we ran GLITCH and either SLAC (if the file was a Chef script) or
SLIC (if the file was a Puppet script). We kept track of the number
of security smells reported and their respective categories. If, after
analyzing a file, the file contained a smell of a category that up to
that point had less than 5 reports, then the file was included in the
oracle dataset. Table 6 presents the number of IaC scripts and the
number of LOC in the three oracle datasets.

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Saavedra and Ferreira

Table 3: Rules to detect security smells used by GLITCH.

Smell Name
Admin by default
Empty password
Hard-coded secret
Invalid IP address binding
Suspicious comment
Use of HTTP without TLS
No integrity check

Use of weak crypto alg.
Missing default case statement

Rule
(𝑖𝑠𝐴𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 (𝑥) ∨ 𝑖𝑠𝑉 𝑎𝑟𝑖𝑎𝑏𝑙𝑒 (𝑥)) ∧ (𝑖𝑠𝑈 𝑠𝑒𝑟 (𝑥 .𝑛𝑎𝑚𝑒) ∨ 𝑖𝑠𝑅𝑜𝑙𝑒 (𝑥 .𝑛𝑎𝑚𝑒)) ∧ 𝑖𝑠𝐴𝑑𝑚𝑖𝑛 (𝑥 .𝑣𝑎𝑙𝑢𝑒) ∧ ¬𝑥 .ℎ𝑎𝑠_𝑣𝑎𝑟𝑖𝑎𝑏𝑙𝑒
(𝑖𝑠𝐴𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 (𝑥) ∨ 𝑖𝑠𝑉 𝑎𝑟𝑖𝑎𝑏𝑙𝑒 (𝑥)) ∧ 𝑖𝑠𝑃𝑎𝑠𝑠𝑤𝑜𝑟𝑑 (𝑥 .𝑛𝑎𝑚𝑒) ∧ 𝑙𝑒𝑛𝑔𝑡ℎ (𝑥 .𝑣𝑎𝑙𝑢𝑒) == 0
(𝑖𝑠𝐴𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 (𝑥) ∨ 𝑖𝑠𝑉 𝑎𝑟𝑖𝑎𝑏𝑙𝑒 (𝑥)) ∧ (𝑖𝑠𝑃𝑎𝑠𝑠𝑤𝑜𝑟𝑑 (𝑥 .𝑛𝑎𝑚𝑒) ∨ 𝑖𝑠𝑆𝑒𝑐𝑟𝑒𝑡 (𝑥 .𝑛𝑎𝑚𝑒) ∨ 𝑖𝑠𝑈 𝑠𝑒𝑟 (𝑥 .𝑛𝑎𝑚𝑒)) ∧ ¬𝑥 .ℎ𝑎𝑠_𝑣𝑎𝑟𝑖𝑎𝑏𝑙𝑒
(𝑖𝑠𝐴𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 (𝑥) ∨ 𝑖𝑠𝑉 𝑎𝑟𝑖𝑎𝑏𝑙𝑒 (𝑥)) ∧ 𝑖𝑠𝐼𝑛𝑣𝑎𝑙𝑖𝑑𝐵𝑖𝑛𝑑 (𝑥 .𝑣𝑎𝑙𝑢𝑒)
𝑖𝑠𝐶𝑜𝑚𝑚𝑒𝑛𝑡 (𝑥) ∧ ℎ𝑎𝑠𝑊 𝑟𝑜𝑛𝑔𝑊 𝑜𝑟𝑑𝑠 (𝑥 .𝑐𝑜𝑛𝑡𝑒𝑛𝑡 )
(𝑖𝑠𝐴𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 (𝑥) ∨ 𝑖𝑠𝑉 𝑎𝑟𝑖𝑎𝑏𝑙𝑒 (𝑥)) ∧ 𝑖𝑠𝑈 𝑅𝐿 (𝑥 .𝑣𝑎𝑙𝑢𝑒) ∧ ℎ𝑎𝑠𝐻𝑇𝑇 𝑃 (𝑥 .𝑣𝑎𝑙𝑢𝑒) ∧ ¬ℎ𝑎𝑠𝐻𝑇𝑇 𝑃𝑊 ℎ𝑖𝑡𝑒𝐿𝑖𝑠𝑡 (𝑥 .𝑣𝑎𝑙𝑢𝑒)
(𝑖𝑠𝐴𝑡𝑜𝑚𝑖𝑐𝑈 𝑛𝑖𝑡 (𝑥) ∧ℎ𝑎𝑠𝐷𝑜𝑤𝑛𝑙𝑜𝑎𝑑 (𝑥 .𝑎𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒𝑠) ∧¬ℎ𝑎𝑠𝐶ℎ𝑒𝑐𝑘𝑠𝑢𝑚 (𝑥 .𝑎𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒𝑠)) ∨ ( (𝑖𝑠𝐴𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 (𝑥) ∨𝑖𝑠𝑉 𝑎𝑟𝑖𝑎𝑏𝑙𝑒 (𝑥)) ∧𝑖𝑠𝐶ℎ𝑒𝑐𝑘𝑆𝑢𝑚 (𝑥 .𝑛𝑎𝑚𝑒) ∧
(𝑥 .𝑣𝑎𝑙𝑢𝑒 == ”𝑛𝑜” ∨ 𝑥 .𝑣𝑎𝑙𝑢𝑒 == ”𝑓 𝑎𝑙𝑠𝑒”))
(𝑖𝑠𝐴𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒 (𝑥) ∨ 𝑖𝑠𝑉 𝑎𝑟𝑖𝑎𝑏𝑙𝑒 (𝑥)) ∧ 𝑖𝑠𝑊 𝑒𝑎𝑘𝐶𝑟 𝑦𝑝𝑡 (𝑥 .𝑣𝑎𝑙𝑢𝑒) ∧ ¬ℎ𝑎𝑠𝑊 𝑒𝑎𝑘𝐶𝑟 𝑦𝑝𝑡𝑊 ℎ𝑖𝑡𝑒𝐿𝑖𝑠𝑡 (𝑥 .𝑛𝑎𝑚𝑒) ∧ ¬ℎ𝑎𝑠𝑊 𝑒𝑎𝑘𝐶𝑟 𝑦𝑝𝑡𝑊 ℎ𝑖𝑡𝑒𝐿𝑖𝑠𝑡 (𝑥 .𝑣𝑎𝑙𝑢𝑒)
𝑖𝑠𝐶𝑜𝑛𝑑𝑖𝑡𝑖𝑜𝑛𝑆𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡 (𝑥) ∧ 𝑥 .𝑖𝑠_𝑑𝑒 𝑓 𝑎𝑢𝑙𝑡 == 𝐹𝑎𝑙𝑠𝑒 ∧ ¬𝑖𝑠𝐷𝑒 𝑓 𝑎𝑢𝑙𝑡 (𝑥 .𝑒𝑙𝑠𝑒_𝑠𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡 )

Table 4: String patterns used in the GLITCH’s rules. These are con-
figurable. The configuration shown is the one used by the improved
version of GLITCH.

Function
isUser()
isRole()
isAdmin()
isPassword()
isSecret()
isInvalidBind()
hasWrongWords()
hasHTTP()
hasHTTPWhiteList()
isDownload()
isCheckSum()
isWeakCrypt()
hasWeakCryptWhiteList()

String Pattern
"user", "uname", "username", "login", "userid", "loginid" (...)
(the config is empty for this function)
"admin", "root"
"pass", "pwd", "password", "passwd", "passno", "pass-no" (...)
"auth_token", "authetication_token", "secret", "ssh_key" (...)
"0.0.0.0"
"bug", "debug", "todo", "hack", "solve", "fixme" (...)
"http"
"localhost", "127.0.0.1"
"(http|https|www).*iso$", "(http|https|www).*tar.gz$" (...)
"gpg", "checksum"
"md5", "sha1", "arcfour"
"checksum"

Annotating the oracle datasets. After collecting the scripts that
make the oracle datasets, we manually annotated them, identifying
security smells. Despite the use of analysis tools in the file selection
process described above, we guaranteed that the location of the
security smells was not disclosed. In other words, at the annotation
stage we only had access to the files, but not the reports. We did this
to reduce bias in the annotation process. The Ansible oracle dataset
was already annotated, but since the numbers of smell occurrences
did not match the numbers reported in Rahman et al.’s study [19], we
decided to reannotate the dataset. To annotate the oracle datasets,
we used closed coding [23], where three raters identified security
smells and their agreement was checked. In total, there were seven
raters involved. One of the raters was the first author. For each of
the three IaC technologies, we recruited two postgraduate students
who had experience with IaC and/or cybersecurity. They were given
access to: the 80 files in the oracle datasets, a general description of
the IaC technology, and a description of the nine security smells
considered. For each report, raters identified the name of the file,
the category of the security smell, and the line where it occurs; they
collated this information in a CSV file.

We then manually inspected the three CSV files produced for
each oracle dataset, and we decided to keep only the classifications
where at least two raters agreed. Table 7 shows the agreement distri-
bution for each dataset. We only consider the lines of code where at
least one rater identified a smell. The percentage values shown are
for the cases where there was no agreement, two raters agreed, or
all the raters agreed. When a rater did not identify a smell identified
by other rater, we considered the label “none” to be attributed. The
results on the table demonstrate that at least two raters agreed on
the great majority of subjects: 99.1% in Ansible, 93.9% in Chef, and
95.1% in Puppet. We calculated the agreement distribution instead
of other statistics, such as Cohen’s Kappa or Krippendorff’s alpha,
since these statistics consider the probability of chance agreement.

We argue that, since our annotation task includes finding the smells
in the scripts, the likelihood of chance agreement is significantly
reduced. After this process, we obtained: an oracle of 44 Ansible
security smells categorized as shown in Table 8 and with 69 files
with no smells; an oracle of 105 Chef security smells categorized as
shown in Table 9 and with 43 files with no smells; and an oracle
of 65 Puppet security smells categorized as shown in Table 10 and
with 52 files with no smells.

5.3 Accuracy of GLITCH
To determine the accuracy of GLITCH, we ran it for the oracle
datasets. We also ran SLIC for the Puppet oracle dataset and SLAC
for the other two oracle datasets. We measured precision and recall
of each tool. Since it is easy to configure GLITCH (see Section 4), we
used two versions of GLITCH for each oracle dataset: one version
was configured to behave similarly to SLIC (or SLAC), and the other
was an improved version. As described in Section 4, the difference
between the two versions is on the keywords for each function in
Table 4: one uses the keywords used by SLIC (or SLAC) and the other
configuration was tweaked by us. In the tables below, we use the
headers GLITCH (SLIC) and GLITCH (SLAC) to refer to GLITCH
configured to behave similarly to SLIC and SLAC, respectively. The
header GLITCH refers to the improved version of GLITCH that
uses the rules shown in Table 4.

Tables 8, 9, and 10 report the accuracy results for Ansible, Chef,
and Puppet, respectively. We use N/I to denote that the detection
of a certain smell is not implemented (e.g., SLAC does not detect
the smell Admin by default for Ansible scripts); N/A to denote that
a certain smell cannot occur (e.g., Ansible does not have switch
statements, so the smell Missing default case statement does not
apply); and N/D to denote that the tool does not report any security
smell or to denote that there are no occurrences of a given smell
(see, for example, the recall value of GLITCH for the Use of weak
crypto algorithm in Table 8). To facilitate comparison between tools
and IaC technologies, we decided to keep all the rows in these tables,
even when there are no smell occurrences or when its detection is
not implemented.

5.3.1 Accuracy results for the Ansible oracle dataset. As shown
in Table 8, GLITCH configured to behave similarly to SLAC has
the same precision and recall as SLAC (same average). There is a
small discrepancy in the recall values for No Smell. This happens
because SLAC detects one No integrity check smell in an Ansible
script where no smells should be detected. The difference between
both tools is that GLITCH enforces detection of No integrity check
smells only on Atomic Unit nodes, while SLAC ignores the type of

GLITCH: Automated Polyglot Security Smell Detection in Infrastructure as Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Table 5: Attributes of IaC Datasets.

Table 6: Attributes of Oracle Datasets.

Attribute

Ansible

Chef

GH MOZ

OST

WIK

Repository count
Total IaC scripts
Total LOC (IaC scripts)

681
108,509
5,180,747

439
70,939
6,071,035

219
10,009
610,122

2
1613
66,367

61
2,840
217,843

11
2,845
135,137

Total IaC scripts
Total LOC (IaC scripts)

81
4,185

80
4,630

80
4,367

Puppet

Attribute

Ansible Chef

Puppet

Table 7: Agreement distribution for the oracle datasets (%).

Ansible Chef

Puppet

No agreement
2 raters agreed
3 raters agreed

0.9
86.4
12.7

6.1
73.6
20.3

4.9
78.6
16.5

node, which leads SLAC to detect this type of smell in the definition
of a variable.

Regarding the improved version of GLITCH, the average preci-
sion improves from 67% to 77% and recall improves from 79% to
87%. There are also improvements regarding files with no smells.
We can also see that it supports the smell Admin by default with
perfect precision and recall. GLITCH keeps the values of precision
and recall when they were already 100%. It also improves the pre-
cision for Hard-coded secret by 10 percentage points (from 32% to
42%); for Suspicious comment by 8 percentage points (from 67% to
75%); and for Use of HTTP without TLS by 24 percentage points
(from 71% to 95%). Recall for Suspicious comment improved from
67% to 100%. The only case where improvements do not occur is
for the smell No integrity check, where the single occurrence is not
detected (note that SLAC did not detect it either). This happens
because the occurrence of this smell is regarding a URL referring
to an YAML file, which GLITCH does not consider (i.e., the string
pattern isDownload() shown in Table 4 does not contain URLs that
end with .yml). Finally, the worst precision value is for the smell
Hard-coded secret (42%). This happens mainly because the string
patterns isSecret(), isPassword(), and isUser() are the ones with more
possibilities, thus increasing the probability of having false posi-
tives. Some of the possibilities are keywords such as “user”, which
result in a higher number of false positives.

5.3.2 Accuracy results for the Chef oracle dataset. Table 9 shows
that when GLITCH is configured to behave similarly to SLAC, it
actually obtains better results than SLAC: the average precision
improves 28 percentage points (from 49% to 77%) and the average
recall improves 16 percentage points (from 60% to 76%). There are
also improvements regarding files with no smells. Contributing
to these improvements is the substantial increase in precision for
the smells Empty password and No integrity check. Regarding the
first smell, this is because SLAC wrongly treats variables as empty
values; regarding the second, GLITCH searches for links in the
values of variables and attributes, while SLAC is searching for links
on a line-by-line basis.

When compared to GLITCH configured to behave similarly to
SLAC, the improved version maintains the average precision and
increases the average recall by 10 percentage points (76% to 86%).

When compared to SLAC, the results for all smells improve, except
for Invalid IP address binding and Use of HTTP without TLS, where
the results are the same, and for Suspicious comment, where the pre-
cision decreases. This decrease in precision is because GLITCH uses
a larger set of keywords (this is similar to what caused the low pre-
cision for the smell Hard-coded secret when analyzing the Ansible
oracle dataset). This is also why the worst precision value is for the
smell Hard-coded secret. The worst recall value is for the smell Ad-
min by default (41%). This happens because there are some scripts in
the dataset that configure the execution of MySQL commands. The
commands executed as root, such as the following, were considered
by the raters as a security smell: cmd = "mysql -uroot ...". How-
ever, for this smell, GLITCH only considers the value of attributes
or variables that define users (e.g. user: root).

5.3.3 Accuracy results for the Puppet oracle dataset. Similar to what
was described above, Table 10 shows that when GLITCH is config-
ured to behave similarly to SLIC, it also obtains better results than
SLIC: the average precision improves 8 percentage points (from 60%
to 68%) and the average recall improves 10 percentage points (from
72% to 82%). Contributing to this is the fact that GLITCH detects
smells of type Missing default case statement with a high preci-
sion. Also, the precision for the smell Empty password is noticeable
higher (GLITCH reports no false positives). This is because GLITCH
seems to deal better with variables. There are also improvements
regarding files with no smells.

When compared to GLITCH configured to behave similarly to
SLIC, the improved version maintains the average precision and
improves the average recall by 3 percentage points (82% to 85%).
The precision and recall for No Smell decreased 1 and 6 percentage
points, respectively. We can see that for the smell Admin by default
many more true positives are identified, but there are some false
positives. There were no reports for the smell No integrity check.
Precision and recall improved or remained the same for all the
smells, except for Suspicious comment. Similar to what happened
with the Chef oracle dataset, the precision values for the smells
Hard-coded secret and Suspicious comment are low due to the use of
more keywords.

5.4 Security Smells Frequency
Using GLITCH, we performed an empirical study to quantify the
prevalence of security smells in Ansible, Chef, and Puppet. Similar
studies were performed by Rahman et al. [18] (for Puppet scripts
using SLIC) and Rahman et al. [19] (for Ansible and Chef scripts
using SLAC). Here, the goal is to use GLITCH and investigate
whether there are any noticeable differences. The IaC datasets used
are described in Section 5.2 and their attributes shown in Table 5.
This means that, when considering the three IaC datasets as a whole,

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Saavedra and Ferreira

Table 8: GLITCH vs SLAC: Accuracy for the Ansible Oracle Datasets (N/I - Not implemented, N/A - Not applicable, N/D - No data)

Original Oracle

SLAC

GLITCH (SLAC)

GLITCH

Smell Name

Occurr.

Precision Recall

Precision Recall

Precision Recall

Admin by default
Empty password
Hard-coded secret
Invalid IP address binding
Suspicious comment
Use of HTTP without TLS
No integrity check
Use of weak crypto alg.
Missing default case statement

No smell

Average

7
1
8
1
6
20
1
0
0

69

N/I
1.00
0.32
1.00
0.67
0.71
0.00
N/I
N/A

0.98

0.67

N/I
1.00
1.00
1.00
0.67
1.00
0.00
N/I
N/A

0.87

0.79

N/I
1.00
0.32
1.00
0.67
0.71
0.00
N/I
N/A

0.98

0.67

N/I
1.00
1.00
1.00
0.67
1.00
0.00
N/I
N/A

0.88

0.79

1.00
1.00
0.42
1.00
0.75
0.95
0.00
N/D
N/A

1.00

0.77

1.00
1.00
1.00
1.00
1.00
1.00
0.00
N/D
N/A

0.94

0.87

Table 9: GLITCH vs SLAC: Accuracy for the Chef Oracle Datasets (N/I - Not implemented, N/A - Not applicable, N/D - No data)

Original Oracle

SLAC

GLITCH (SLAC)

GLITCH

Smell Name

Occurr.

Precision Recall

Precision Recall

Precision Recall

Admin by default
Empty password
Hard-coded secret
Invalid IP address binding
Suspicious comment
Use of HTTP without TLS
No integrity check
Use of weak crypto alg.
Missing default case statement

No smell

Average

37
4
13
7
4
13
6
1
20

43

0.00
0.00
0.13
1.00
0.80
0.71
0.20
0.25
1.00

0.85

0.49

0.00
0.00
0.54
1.00
1.00
0.92
0.33
1.00
0.45

0.79

0.60

N/D
1.00
0.20
1.00
0.80
0.71
1.00
0.25
1.00

0.95

0.77

0.00
0.75
0.69
1.00
1.00
0.92
0.33
1.00
0.95

0.93

0.76

0.94
1.00
0.20
1.00
0.40
0.71
1.00
0.50
1.00

0.95

0.77

0.41
0.75
0.69
1.00
1.00
0.92
1.00
1.00
0.95

0.88

0.86

Table 10: GLITCH vs SLIC: Accuracy for the Puppet Oracle Datasets (N/I - Not implemented, N/A - Not applicable, N/D - No data)

Original Oracle

SLIC

GLITCH (SLIC)

GLITCH

Smell Name

Occurr.

Precision Recall

Precision Recall

Precision Recall

Admin by default
Empty password
Hard-coded secret
Invalid IP address binding
Suspicious comment
Use of HTTP without TLS
No integrity check
Use of weak crypto alg.
Missing default case statement

No smell

Average

14
5
11
6
9
5
1
4
10

52

N/D
0.60
0.10
1.00
0.75
0.38
N/I
0.43
N/I

0.95

0.60

0.00
0.60
0.73
1.00
1.00
1.00
N/I
0.75
N/I

0.71

0.72

N/D
1.00
0.14
1.00
0.60
0.42
N/I
0.50
0.83

0.98

0.68

0.00
1.00
0.82
1.00
1.00
1.00
N/I
0.75
1.00

0.77

0.82

0.81
1.00
0.14
1.00
0.39
0.45
N/D
0.57
0.83

0.97

0.68

0.93
1.00
0.82
1.00
1.00
1.00
0.00
1.00
1.00

0.71

0.85

this empirical study considers 1413 repositories with 196,755 IaC
scripts. In total, we analyze 12,281,251 LOC.

Similar to previous studies, the first step was to determine the oc-
currences of security smells for each IaC script. We then calculated
the two following metrics:
• Smell density: frequency of a given security smell for every

1,000 LOC [9, 19]. For a given smell 𝑥,

𝑆𝑚𝑒𝑙𝑙𝐷𝑒𝑛𝑠𝑖𝑡𝑦 (𝑥) =

Total occurrences of 𝑥
Total line count for all scripts/1000

• Proportion of scripts (Script%): percentage of scripts that con-

tain at least one occurrence of smell 𝑥.

5.4.1 Occurrences. Looking at Table 11, we observe that all cate-
gories of security smells are identified across all datasets. Overall,
GLITCH detects 76,015 security smells for Ansible, 19,455 for Chef,
and 18,151 for Puppet. GLITCH identifies fewer security smells in
the Chef dataset than SLIC. On the other hand, GLITCH identifies
more security smells than SLIC in the Ansible and Puppet datasets
(76,015 vs 67,078 and 18,151 vs 12,884). When using GLITCH for

GLITCH: Automated Polyglot Security Smell Detection in Infrastructure as Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Table 11: Smell Occurrences. (N/I - Not implemented, N/A - Not applicable, N/D - No data)

Ansible

Chef

GH

MOZ

OST

WIK

SLAC

GLITCH

SLAC

GLITCH

SLIC

GLITCH

SLIC

GLITCH

SLIC

GLITCH

SLIC

GLITCH

Puppet

Admin by default
Empty password
Hard-coded secret
Invalid IP address binding
Suspicious comment
Use of HTTP without TLS
No integrity check
Use of weak crypto alg.
Missing default case statement

Combined

N/I
1,973
47,735
914
10,498
4,812
1,146
N/I
N/A

67,078

10,222
1,432
45,325
2,033
10,749
3,393
1,359
1,502
N/A

248
115
15,100
499
2,267
2,507
1,662
76
702

1,821
303
7,763
603
4,343
2,281
304
147
1,890

34
131
5,608
179
868
934
N/I
227
N/I

1,201
348
6,236
96
1,802
703
44
109
527

76,015

23,176

19,455

7,981

11,066

4
20
394
20
202
52
N/I
48
N/I

740

30
20
592
26
285
31
0
28
210

35
24
1,751
90
309
453
N/I
27
N/I

172
127
2,172
45
965
163
4
18
36

6
36
858
41
343
164
N/I
26
N/I

1,222

2,689

3,702

1,474

136
66
1,114
18
609
111
3
21
83

2,161

Table 12: Smell density (per KLOC). (N/I - Not implemented, N/A - Not applicable, N/D - No data)

Ansible

Chef

GH

MOZ

OST

WIK

SLAC

GLITCH

SLAC

GLITCH

SLIC

GLITCH

SLIC

GLITCH

SLIC

GLITCH

SLIC

GLITCH

Puppet

Admin by default
Empty password
Hard-coded secret
Invalid IP address binding
Suspicious comment
Use of HTTP without TLS
No integrity check
Use of weak crypto alg.
Missing default case statement

N/I
0.38
9.21
0.18
2.03
0.93
0.22
N/I
N/A

1.97
0.28
8.75
0.39
2.07
0.65
0.26
0.29
N/A

Combined

12.95

14.66

0.04
0.02
2.49
0.08
0.37
0.41
0.27
0.01
0.12

3.81

0.30
0.05
1.28
0.10
0.72
0.38
0.05
0.02
0.31

3.21

0.06
0.21
9.19
0.29
1.42
1.53
N/I
0.37
N/I

1,97
0.57
10.22
0.16
2.95
1.15
0.07
0.18
0.86

0.06
0.30
5.94
0.30
3.04
0.78
N/I
0.72
N/I

0.45
0.30
8.92
0.39
4.29
0.47
0.00
0.42
3.16

0.16
0.11
8.04
0.41
1.42
2.08
N/I
0.12
N/I

0.79
0.58
9.97
0.21
4.43
0.75
0.02
0.08
0.17

0.04
0.27
6.35
0.30
2.54
1.21
N/I
0.19
N/I

1.00
0.49
8.24
0.13
4.51
0.82
0.02
0.16
0.61

13.07

18.13

11.14

18.40

12.34

17.00

10.90

15.98

Table 13: Proportion of Scripts (Script%) with at Least One Smell. (N/I - Not implemented, N/A - Not applicable, N/D - No data)

Ansible

Chef

GH

MOZ

OST

WIK

SLAC

GLITCH

SLAC

GLITCH

SLIC

GLITCH

SLIC

GLITCH

SLIC

GLITCH

SLIC

GLITCH

Puppet

Admin by default
Empty password
Hard-coded secret
Invalid IP address binding
Suspicious comment
Use of HTTP without TLS
No integrity check
Use of weak crypto alg.
Missing default case statement

Combined

N/I
0.8
18.3
0.7
5.4
2.3
0.8
N/I
N/A

23.8

5.7
0.4
13.9
0.7
5.4
1.6
0.9
0.6
N/A

19.6

0.2
0.2
7.7
0.5
2.6
1.8
1.4
0.1
0.9

1.7
0.3
5.2
0.4
3.8
1.8
0.4
0.2
2.1

11.4

10.4

0.3
1.1
18.2
1.4
5.3
5.1
N/I
1.6
N/I

25.5

3.6
2.5
20.2
0.7
8.9
3.7
0.4
0.8
2.9

29.6

0.2
0.6
9.9
0.7
8.6
1.5
N/I
1.4
N/I

18.0

1.5
0.9
12.3
0.6
11.0
0.9
0.0
0.4
9.9

27.5

1.1
0.7
24.6
2.8
7.0
8.2
N/I
0.8
N/I

32.5

5.2
3.5
31.3
1.4
13.5
3.1
0.1
0.5
1.0

40.1

0.2
0.4
17.0
1.4
9.1
3.8
N/I
0.5
N/I

26.8

4.0
1.1
19.1
0.6
13.7
2.5
0.1
0.4
1.8

31.5

Ansible and Puppet, the three most dominant security smells are
Hard-coded secret, Admin by default, and Suspicious comment. For
Chef, the three most dominant security smells are Hard-coded secret,
Suspicious comment, and Use of HTTP without TLS.

Smell density. Table 12 shows the smell density for the three
5.4.2
datasets. Overall, GLITCH detects 14.66 security smells per 1,000
LOC in Ansible scripts, 3.21 in Chef scripts, and an average of
17.38 in Puppet scripts. For all datasets, the dominant security smell
is Hard-coded secret, followed by Suspicious comment. Given that
the precision values for these smells tend to be the lowest (see
Section 5.3), this suggests that many of these are false positives.
The third most dominant security smell differs across the three

datasets: for Ansible, it is Admin by default (1.97); for Chef, it is Use
of HTTP without TLS (0.38); and for Puppet, it is Admin by default
when considering the GitHub dataset (1.97), Missing default case
statement when considering the Mozilla dataset (3.16), and Admin
by default when considering the Openstack and Wikimedia datasets
(0.79 and 1.00, respectively).

5.4.3 Proportion of Scripts (Script%). Table 13 shows, for the three
datasets, the proportion of scripts with at least one occurrence
of a smell. For Ansible, GLITCH detects at least one of the eight
identified security smells in 19.6% of the total scripts. For SLIC, the
percentage is 23.8%, but note that SLIC only supports six security

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Saavedra and Ferreira

smells. This is not very different from the values obtained by Rah-
man et al. [19], where the percentages obtained with SLIC were
25.3% and 29.6% for their GitHub and Openstack datasets, respec-
tively. For Chef, GLITCH detects at least one of the nine identified
security smells in 10.4% of the total scripts. For SLIC, the percentage
is slightly higher at 11.4%. Here, we note a more noticeable discrep-
ancy with Rahman et al.’s study [19]: the percentages obtained with
SLIC were 20.5% and 30.4% for their GitHub and Openstack datasets,
respectively. For Puppet, in the GitHub, Mozilla, OpenStack, and
Wikimedia datasets, GLITCH detects at least one of the nine identi-
fied security smells in, respectively, 29.6%, 27.5%, 40.1%, and 31.5%
of the total scripts. These percentages are slightly higher than those
obtained for SLIC.

For all datasets, the dominant security smell is Hard-coded secret,
followed by Suspicious comment. Given that the precision values for
these smells tend to be the lowest (see Section 5.3), this suggests that
many of these are false positives. However, there is an exception:
for Ansible, the second most dominant smell is Admin by default
(5.7%); since the accuracy of GLITCH for this smell is high, this
suggests that there is a substantial number of Ansible scripts that
are affected by this problem. The third most dominant security smell
differs across the three datasets: for Ansible, it is Suspicious comment
(5.4%); for Chef, it is Missing default case statement (2.1%); and for
Puppet, it is Use of HTTP without TLS when considering the GitHub
dataset (3.7%), Missing default case statement when considering
the Mozilla dataset (9.9%), Admin by default when considering the
Openstack and Wikimedia datasets (5.2% and 4.0%, respectively).
We note that the high accuracy of GLITCH for the smell Missing
default case statement, suggests that a substantial number of scripts
in the Mozilla dataset are affected by this problem.

5.4.4 Execution times. The execution times of GLITCH, SLIC, and
SLAC for the three datasets are shown in Table 14 (in seconds).
These times were obtained in a server machine running Debian 10,
with 4 Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz, 64GB RAM,
and with a Toshiba MG03ACA100 hard drive. We executed 5 runs
for each pair tool/dataset and averaged the obtained execution
times. Each run was executed in its own Docker container created
from the Docker image we provide in the replication package. Runs
from the same set of 5 runs were executed simultaneously. GLITCH
is much quicker than SLIC and SLAC when running on Chef or Pup-
pet scripts (speedups vary from 9.14× to 32.07×). SLIC and SLAC
respectively call puppet-lint11 and foodcritic12 to analyze each Pup-
pet or Chef script. The overhead of creating a new system process
for each script analyzed and other non-related analyses performed
by puppet-lint and foodcritic are the main reason for the slower
execution times. However, when compared to SLAC, GLITCH takes
more than double the time to run on the Ansible dataset. This
happens because we parse Ansible scripts using ruamel.yaml, a
Python package slower than the popular yaml package, but with
the advantage of saving comments in the AST.

11https://github.com/rodjek/puppet-lint
12http://www.foodcritic.io/

Table 14: The average execution times between 5 runs (seconds).

Puppet

Tool

Ansible

Chef

GH MOZ

OST WIK

SLIC/SLAC
GLITCH

Speedup

797
1,668

0.48

76,153
8,335

2,615
86

380
14

915
35

866
27

9.14

30.41

27.14

26.14

32.07

6 DISCUSSION
In this section, we answer the research questions listed in Sec-
tion 5.1, we discuss the practical implications of our findings, and
we outline potential threats to the validity of our work.

6.1 Answers to Research Questions
Given the findings reported in the previous section, we answer the
research questions posed in Section 5.1 as follows:

Answer to RQ1 [Abstraction]. Can our intermediate represen-
tation model IaC scripts and support automated detection of security
smells? Yes. We demonstrate that our intermediate representation
can model scripts written in different IaC technologies, with our
current implementation supporting Ansible, Chef, and Puppet. We
also define and implement nine rules that operate on the interme-
diate representation and that can be used to detect security smells.
New rules can be easily created and existing rules can be easily
changed. We evaluate our implementation with three large datasets
containing 196,755 IaC scripts and 12,281,251 LOC. This strongly
suggests that the intermediate representation is robust enough to
support a large variety of IaC scripts.

Answer to RQ2 [Accuracy and Performance]. How does
GLITCH compare with existing state-of-art tools for detecting se-
curity smells in terms of accuracy and performance? As shown in Ta-
bles 8, 9, and 10, the average precision and recall values of GLITCH
are substantially better than the average precision and recall values
of SLIC and SLAC. For Puppet, average precision and average recall
improved by 8 and 13 percentage points, respectively. For Ansible,
average precision improved 10 percentage points and average recall
improved 8 percentage points. For Chef, the improvement was more
expressive: the average precision and average recall improved by 28
and 26 percentage points, respectively. In terms of performance, as
Table 14 shows, GLITCH is much faster analyzing Chef and Puppet
scripts than tools such as SLIC or SLAC (speedups vary from 9.14×
to 32.07×). For Ansible, GLITCH takes more than twice as long than
SLAC, but it can still analyze IaC scripts in an acceptable amount
of time (e.g., it took us around 28 minutes to analyze more than 5M
LOC).

Answer to RQ3 [Frequency]. How frequently do security smells
occur in IaC scripts? All categories of security smells are identified
across all datasets considered in this work. For Ansible, GLITCH
detects at least one of the eight identified security smells in 19.6%
of the total scripts. For Chef, it detects at least one of the nine iden-
tified security smells in 10.4% of the total scripts. For Puppet, in
the GitHub, Mozilla, OpenStack, and Wikimedia datasets, GLITCH
detects at least one of the nine identified security smells in, respec-
tively, 29.6%, 27.5%, 40.1%, and 31.5% of the total scripts.

GLITCH: Automated Polyglot Security Smell Detection in Infrastructure as Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

In general, the most dominant security smell is Hard-coded secret,
followed by Suspicious comment. Given that the precision values
for these smells tend to be the lowest (see Section 5.3), this suggests
that many of these are false positives. For Ansible, the second most
dominant smell is Admin by default (5.7%). For Chef and for the
Mozilla dataset of Puppet scripts, the third most dominant smell is
Missing default case statement (2.1% and 9.9%). Since the accuracy
of GLITCH for these smells is high, this suggests that there is a
substantial number of Ansible and Chef scripts that are affected by
these problems.

6.2 Practical Implications and Challenges
The main practical implication of this work is that it is now possible
to implement new rules to detect code smells that can be imme-
diately applied to a variety of IaC technologies. Some of the rules
currently implemented have very high precision and recall, and
have been used to identify a considerable number of smells in our
study. This suggests that IaC practitioners can benefit if they focus
first on smells of those specific categories (e.g., Admin by default
and Missing default case statement). Also, during the development
of this work it became clear that there are no open replication
packages that IaC researchers and practitioners can use. Therefore,
we constructed a new open-source replication package that can be
used by the community.

We identify three main challenges: (1) Quality. This challenge
is about increasing the precision and recall of GLITCH. For ex-
ample, the definitions of some rules (e.g., those that use many
keywords) still report a considerable number of false positives (e.g.
Hard-coded secret). Future work should be invested in improving
the quality of the rules that GLITCH implements. Addressing this
challenge is perhaps an important step toward real-life adoption of
GLITCH. (2) Scope. This challenge is about extending GLITCH to
support more IaC technologies and to detect more vulnerabilities.
For example, it would be interesting to extend GLITCH to support
Terraform and to support the detection of faults regarding order-
ing violations [27] or intra-update sniping vulnerabilities [10]. By
addressing this challenge, we will be in a better position to provide
a more precise characterization of the expressiveness of the smell
detection engine. (3) Development process. This challenge is
about integrating these tools into the development process, thus
contributing to real-life adoption. The following could bring added
value: integration with continuous integration (CI) processes (e.g.,
GitHub actions), integration with popular IDEs, interactive reports
(e.g., highlight vulnerable code), and explainable warnings. Since
GLITCH is much faster than other state-of-the-art tools for analyz-
ing Chef and Puppet scripts, it becomes more appealing to integrate
GLITCH as part of a CI workflow [8].

6.3 Threats to Validity
A threat to conclusion validity is that the identification of security
smells in the oracle datasets are susceptible to the subjectivity of the
raters. We mitigated this by using three raters, with two of them not
being authors of the paper and with experience in IaC technologies
and/or cybersecurity. Also, we only kept the classifications where
at least two raters agreed.

A threat to internal validity is that, due to the complexity and
generality of GLITCH, there may exist implementation bugs in
the codebase. We extensively tested the tool to mitigate this risk.
Furthermore, all our code and datasets are publicly available for
other researchers and potential users to check the validity of the
results. Finally, the detection accuracy of GLITCH depends on the
rules that we have provided in Table 3. These rules are heuristic-
driven and can result in false positives and false negatives.

A threat to external validity is that, since we focus on Ansible,
Chef, and Puppet scripts, our findings may not be generalizable
to other IaC technologies. Moreover, in its current form, our inter-
nal representation might not be rich enough to detect other cate-
gories of security smells not considered in this paper. We mitigated
this risk by ensuring that the concepts modeled by the intermedi-
ate representation are as general as possible and by choosing to
demonstrate its validity using three different IaC technologies that,
as shown in Table 1, have different characteristics (procedural vs
declarative, different configuration setup, etc.). Also, the classifica-
tion of security smells used is subject to practitioner interpretation
and their relevance may vary from one practitioner to another. To
mitigate this, we followed classifications established by previous
work [18, 19]. Finally, all the datasets used in our work are from
open-source projects and not from proprietary sources.

7 CONCLUSION
This paper shows that it is possible and beneficial to consistently de-
tect security smells across different IaC technologies. We conducted
a large-scale empirical study where we consider nine security smells
documented in the literature. We found that all categories of se-
curity smells are identified across all datasets. We identified some
smells that might affect many IaC projects.

An outcome of this work is GLITCH, a new technology-agnostic
framework that allows polyglot security smell detection in IaC
scripts, by transforming them into a new intermediate represen-
tation on which different security smell detectors can be defined.
GLITCH currently supports the detection of nine different security
smells in Puppet, Ansible, or Chef scripts. Our evaluation not only
shows that GLITCH can reduce the effort of writing security smell
analyses for multiple IaC technologies, but also that it has higher
precision and recall than the current state-of-the-art tools.

All our code and datasets are publicly available. We argue that
GLITCH and the datasets that we created and made available in our
replication package are very valuable assets for driving reproducible
research in the analysis of IaC scripts.

ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers, whose
comments and corrections have led to significant improvements. We
would also like to thank Akond Rahman, who very kindly provided
access to datasets used in the evaluation of the tools SLIC and SLAC.
The first author is funded by the Advanced Computing/EuroCC
MSc Fellows Programme, which is funded by EuroHPC under grant
agreement No 951732. This project was supported by national funds
through FCT under project UIDB/50021/2020, and by project ANI
045917 funded by FEDER and FCT.

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Saavedra and Ferreira

International Conference on Software Engineering. 26–37.

[28] Eduard Van der Bent, Jurriaan Hage, Joost Visser, and Georgios Gousios. 2018.
How good is your puppet? an empirically defined and validated quality model for
puppet. In 2018 IEEE 25th international conference on software analysis, evolution
and reengineering (SANER). IEEE, 164–174.

REFERENCES
[1] Ahmad Alnafessah, Alim Ul Gias, Runan Wang, Lulai Zhu, Giuliano Casale, and
Antonio Filieri. 2021. Quality-Aware DevOps Research: Where Do We Stand?
IEEE Access 9 (2021), 44476–44489.

[2] James Fryman. 2014. DNS outage post mortem. https://github.blog/2014-01-18-

dns-outage-post-mortem/ Accessed: 3 May 2022.

[3] Michele Guerriero, Martin Garriga, Damian A Tamburri, and Fabio Palomba.
2019. Adoption, support, and challenges of infrastructure-as-code: Insights from
industry. In 2019 IEEE International Conference on Software Maintenance and
Evolution (ICSME). IEEE, 580–589.

[4] Oliver Hanappi, Waldemar Hummer, and Schahram Dustdar. 2016. Asserting
reliable convergence for configuration management scripts. In Proceedings of the
2016 ACM SIGPLAN International Conference on Object-Oriented Programming,
Systems, Languages, and Applications. 328–343.

[5] Rebecca Hersher. 2017. Amazon and the $150 Million typo.

https:
//www.npr.org/sections/thetwo-way/2017/03/03/518322734/amazon-and-the-
150-million-typo?t=1651588365675 Accessed: 3 May 2022.

[6] Katsuhiko Ikeshita, Fuyuki Ishikawa, and Shinichi Honiden. 2017. Test suite
reduction in idempotence testing of infrastructure as code. In International Con-
ference on Tests and Proofs. Springer, 98–115.

[7] Yujuan Jiang and Bram Adams. 2015. Co-evolution of infrastructure and source
code-an empirical study. In 2015 IEEE/ACM 12th Working Conference on Mining
Software Repositories. IEEE, 45–55.

[8] Xianhao Jin and Francisco Servant. 2021. What helped, and what did not? An
Evaluation of the Strategies to Improve Continuous Integration. In 2021 IEEE/ACM
43rd International Conference on Software Engineering (ICSE). IEEE, 213–225.
[9] John C Kelly, Joseph S Sherif, and Jonathan Hops. 1992. An analysis of defect
densities found during software inspections. Journal of Systems and Software 17,
2 (1992), 111–117.

[10] Julien Lepiller, Ruzica Piskac, Martin Schäf, and Mark Santolucito. 2021. Analyz-
ing Infrastructure as Code to Prevent Intra-update Sniping Vulnerabilities.. In
TACAS (2). 105–123.

[11] MITRE. 2022. CWE-Common Weakness Enumeration. https://cwe.mitre.org/

index.html.

[12] Nuthan Munaiah, Steven Kroh, Craig Cabrey, and Meiyappan Nagappan. 2017.
Curating github for engineered software projects. Empirical Software Engineering
22, 6 (2017), 3219–3253.

[13] Pars Mutaf. 1999. Defending against a Denial-of-Service Attack on TCP.. In

Recent Advances in Intrusion Detection.

[14] National

Institute of Standards and Technology. 2014.

Security and
Information Systems and Organizations.

Privacy Controls for Federal
https://www.nist.gov/publications/security-and-privacy-controls-federal-
information-systems-and-organizations-including-0.

[15] Akond Rahman, Effat Farhana, Chris Parnin, and Laurie Williams. 2020. Gang of
eight: A defect taxonomy for infrastructure as code scripts. In 2020 IEEE/ACM
42nd International Conference on Software Engineering (ICSE). IEEE, 752–764.
[16] Akond Rahman, Effat Farhana, and Laurie Williams. 2020. The ‘as code’activities:
development anti-patterns for infrastructure as code. Empirical Software Engi-
neering 25, 5 (2020), 3430–3467.

[17] Akond Rahman, Rezvan Mahdavi-Hezaveh, and Laurie Williams. 2019. A system-
atic mapping study of infrastructure as code research. Information and Software
Technology 108 (2019), 65–77.

[18] Akond Rahman, Chris Parnin, and Laurie Williams. 2019. The seven sins: Security
smells in infrastructure as code scripts. In 2019 IEEE/ACM 41st International
Conference on Software Engineering (ICSE). IEEE, 164–175.

[19] Akond Rahman, Md Rayhanur Rahman, Chris Parnin, and Laurie Williams. 2021.
Security smells in ansible and chef scripts: A replication study. ACM Transactions
on Software Engineering and Methodology (TOSEM) 30, 1 (2021), 1–31.

[20] Akond Rahman and Laurie Williams. 2018. Characterizing defective configuration
scripts used for continuous deployment. In 2018 IEEE 11th International conference
on software testing, verification and validation (ICST). IEEE, 34–45.

[21] Akond Rahman and Laurie Williams. 2019. Source code properties of defective
infrastructure as code scripts. Information and Software Technology 112 (2019),
148–163.

[22] Eric Rescorla et al. 2000. HTTP over TLS. RFC 2818, May.
[23] Johnny Saldaña. 2021. The coding manual for qualitative researchers. sage.
[24] Julian Schwarz, Andreas Steffens, and Horst Lichter. 2018. Code smells in in-
frastructure as code. In 2018 11th International Conference on the Quality of
Information and Communications Technology (QUATIC). IEEE, 220–228.

[25] Rian Shambaugh, Aaron Weiss, and Arjun Guha. 2016. Rehearsal: A configuration
verification tool for puppet. In Proceedings of the 37th ACM SIGPLAN Conference
on Programming Language Design and Implementation. 416–430.

[26] Tushar Sharma, Marios Fragkoulis, and Diomidis Spinellis. 2016. Does your
configuration code smell?. In 2016 IEEE/ACM 13th Working Conference on Mining
Software Repositories (MSR). IEEE, 189–200.

[27] Thodoris Sotiropoulos, Dimitris Mitropoulos, and Diomidis Spinellis. 2020. Prac-
tical fault detection in Puppet programs. In Proceedings of the ACM/IEEE 42nd

