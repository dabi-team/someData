2
2
0
2

n
u
J

0
3

]
T
S
.
h
t
a
m

[

1
v
8
7
1
5
1
.
6
0
2
2
:
v
i
X
r
a

Likelihood Asymptotics in Nonregular Settings: A Review with
Emphasis on the Likelihood Ratio

Alessandra R. Brazzale∗ and Valentina Mameli†

June 21, 2022

Abstract

This paper reviews the most common situations
where one or more regularity conditions which un-
derlie classical likelihood-based parametric infer-
ence fail. We identify three main classes of prob-
lems: boundary problems, indeterminate param-
eter problems—which include non-identiﬁable pa-
rameters and singular information matrices—and
change-point problems. The review focuses on
the large-sample properties of the likelihood ra-
tio statistic, though other approaches to hypoth-
esis testing and connections to estimation may be
mentioned in passing. We emphasize analytical so-
lutions and acknowledge software implementations
where available. Some summary insight about the
possible tools to derivate the key results is given.

Key words and phrases: boundary point, change-
point, ﬁnite mixture, ﬁrst order theory, identiﬁabil-
ity, large-sample inference, singular information.

1

Introduction

It is commonly believed that under the null hy-
pothesis the three classical tests of likelihood-based
inference—that is, those based on the Wald, score
and likelihood ratio statistics—are asymptotically
equivalent and, to the ﬁrst order of approximation,

∗Alessandra R. Brazzale is Associate Professor of Statis-
tics, Department of Statistical Sciences, University of
Padova, Via Cesare Battisti 241, 35121 Padova,
Italy,
alessandra.brazzale@unipd.it

†Valentina Mameli is tenure track Assistant Professor
at the Department of Economics and Statistics, Univer-
sity of Udine, Via Tomadini 30/A, 33100 Udine, Italy,
valentina.mameli@uniud.it.

follow a chi-squared distribution. However, in or-
der to hold true this statement requires a number of
regularity conditions. These conditions, which are
typically of Cram´er type (Cram´er, 1946, §33.3), re-
quire, among others, diﬀerentiability with respect
to the parameters of the underlying joint probabil-
ity or density function up to a suitable order and
ﬁniteness of the Fisher information matrix. Models
which satisfy these requirements are said to be ‘reg-
ular’ and cover a wide range of applications. How-
ever, there are many important cases where one or
more conditions break down. A classical example,
which is traditionally used to demonstrate the fail-
ure of parametric likelihood theory, is Neyman and
Scott’s (1948) paradox.

Example 1.1 (Growing number of parameters).
Let (X1, Y1), . . . , (Xn, Yn) denote n independent
pairs of mutually independent and normally dis-
tributed random variables such that for each i =
1, . . . , n, Xi and Yi have mean µi and common vari-
ance σ2. The maximum likelihood estimator of σ2
is

ˆσ2
n =

1
2n

n
(cid:88)

i=1

{(Xi − ˆµi)2 + (Yi − ˆµi)2},

with ˆµi = (Xi + Yi)/2. Straightforward calculation
shows that, for n → ∞, ˆσ2
n converges in probability
to σ2/2 instead of the true value σ2. The reason
is that only a ﬁnite number of observations, in fact
two, is available for estimating the unknown sam-
ple means µi. This violates a major requirement
which underlies the consistency of the maximum
likelihood estimator, namely that the uncertainty of
all parameter estimates goes to zero.

Example 1.1 is an early formulation of an inci-

1

 
 
 
 
 
 
left panel of Figure 1 shows the χ2
1 quantile plot
of the likelihood ratio statistic observed in 10,000
exponential samples of size n = 50 generated with
rate equal to 1 and translated by θ0 = 3. The ﬁnite-
sample distribution of W (3) is visibly far from the
theoretical χ2
1 approximation represented by the dot-
ted diagonal line. The right panel reports the em-
pirical distribution of the likelihood ratio statistics
with superimposed the χ2

2 density (solid line).

These situations are not mere mathematical ar-
tifacts, but include many models of practical inter-
est, such as mixture distributions and change-point
problems, in genetics, reliability, econometrics, and
many other ﬁelds. Especially practitioners may be
less familiar with the resulting limiting distribu-
tions. As will be shown in Section 3, the distribu-
tion of the likelihood ratio statistic may converge
to a mixture of chi-squared distributions, such as
when the true value of the parameter belongs to
the boundary of its parameter space, with mixing
proportions which are awkward to determine. Or,
its asymptotic behaviour may be characterised by
the distribution of the supremum of a squared trun-
cated Gaussian process, which is the common case
for the ﬁnite mixture models reviewed in Section 5.
Asymptotic theory is an essential part of statis-
tical methodology. It provides ﬁrst thing approxi-
mate answers where exact ones are unavailable. Be-
yond this, it serves to check if a proposed inferen-
tial solution provides a sensible answer when the
amount of information in the data increases with-
out limit. Given the tremendous advances in com-
puter age statistical inference (Efron and Hastie,
2016) one could be tempted to by-pass the often
rather demanding algebraic derivations of asymp-
totic approximation. Gaining insight in what hap-
pens to the limiting distribution of likelihood-based
test statistics when one or more regularity condi-
tions fail is a central issue to decide whether and
to which extent to rely upon simulation. The fol-
lowing simple example tries and makes the point.

Example 1.3 (Testing for homogeneity in a von
Mises mixture). Suppose we observe a random
sample y1, . . . , yn from the mixture model

(1 − p)f (yi; 0, κ) + pf (yi; µ, κ),

(1.1)

where 0 ≤ p ≤ 1 is the mixing proportion. Fur-
thermore, f (yi; µ, κ) denotes the von Mises distri-

Figure 1:
Example 1.2: Translated exponen-
tial distribution. Values of the likelihood ratio
W (3) observed in 10,000 exponential samples of
size n = 50 generated with rate equal to 1 and
translated by θ0 = 3. Left: χ2
1 quantile plot. The
diagonal dotted line is the theoretical χ2
1 approx-
imation. Right: histogram and superimposed χ2
2
density (solid line).

dental parameters problem. Other examples of this
type are reviewed in Lancaster (2000), who also dis-
cusses the relevance of the Neyman–Scott paradox
in statistics and economics. A recent contribution
is Feng et al. (2012). Non-regularity may also arise
when the parameter value under the null hypothe-
sis is not an interior point of the parameter space,
or when some of the parameters disappear under
the null hypothesis. The following simple example
shows what may happen when the support of the
distribution depends on the parameter θ.

Example 1.2 (Translated exponential distribu-
tion). Let X1, . . . , Xn be an independent and iden-
tically distributed sample from an exponential dis-
tribution with rate equal to 1. Consider the trans-
lation Yi = Xi + θ, with θ > 0 unknown. Given
the minimum observed value Y(1), the likelihood ra-
tio statistic for testing the hypothesis that θ = θ0
is W (θ0) = 2n(Y(1) − θ0). Straightforward calcu-
lation proves that under the null hypothesis W (θ0)
2 distribution, not the classical χ2
has a χ2
1 limiting
distribution. Furthermore, the maximum likelihood
estimator of θ is no longer asymptotically normal.
Indeed, it is easy to show that Y(1) − θ follows ex-
actly an exponential distribution with rate n. The

2

02468100246810Quantiles of c12Likelihood Ratio Statistic02468100.00.10.20.30.4wDensitybution with mean direction |µ| ≤ π and concentra-
tion parameter κ ≥ 0. Fu et al. (2008) prove that
the asymptotic null distribution of the likelihood ra-
tio statistic for testing the hypothesis p = 0 is the
squared supremum of a truncated Gaussian process.
The quantiles of the process can in principle be ap-
proximated to desirable precision by simulation, this
way overcoming the algebraic diﬃculties of the ex-
act solution. However, the same authors also show
that if a suitable penalisation term is used, the dis-
tribution of the corresponding modiﬁed likelihood
ratio statistic converges to the simple χ2
1 distribu-
tion for n → ∞. This is wholly diﬀerent from what
happens in the Gaussian case.
If the component
densities f (yi; µ, κ) in (1.1) represent normal dis-
tributions with unknown mean µ ∈ R and variance
κ > 0, then the distribution of the likelihood ra-
tio statistic for testing model homogeneity diverges
to inﬁnity unless suitable constraints are imposed
(Chen and Chen, 2003). This is because normal
mixtures with unknown variance are not identiﬁ-
able unlike the von Mises mixture model (1.1); see
Section 5.3. Trying and simulating the limiting dis-
tribution in this case would lead to totally mislead-
ing results as the likelihood ratio tends to inﬁnity
with probability one. The ﬁnite-sample distribu-
tion of the likelihood ratio statistic, however, can
be approximated using the parametric bootstrap as
in McLachlan (1987).

The purpose of this paper is to present the most
common situations where one or more regularity
conditions fail. A highly cited review of nonregular
problems is Smith (1989); see also the discussion
paper by Cheng and Traylor (1995). Further ex-
amples can be found in Barndorﬀ-Nielsen and Cox
(1994, §3.8), Davison (2003, §4.6) and Cox (2006,
Chapter 7). The majority of existing results con-
sider the failure of one condition at a time, but
failure of two assumptions simultaneously has also
received attention.
Indeed, there is a rich litera-
ture on this topic. Since it is nearly impossible
to cover all aspects of the subject, here, we will
focus on the large-sample properties of likelihood-
based parametric test statistics derived under non-
standard conditions, that is, when the likelihood
function is nonregular. Special attention will be
paid to the likelihood ratio and its limiting dis-
tribution, although analogies with alternative test
statistics and/or nonparametric and semiparamet-

ric models may be mentioned in passing. This is
justiﬁed by the widespread use of Wilks’ statistic,
and its chi-squared limiting distribution, for hy-
pothesis testing, model selection and other related
uses. We furthermore restrict our attention to the
key results and the corresponding prototype deriva-
tions; further contributions are mentioned in the
annotated bibliography available as Supplementary
Material.

The paper is organised as follows. First order
parametric inference based on the likelihood func-
tion of a regular model is reviewed in Section 2, to-
gether with the conditions upon which it is based.
However, when these are not fulﬁlled, deriving the
ﬁnite and/or asymptotic properties of the likeli-
hood ratio statistic can be very challenging.
In
the absence of a unifying theory, most of the in-
dividual problems have been treated on their own.
After careful consideration, we decided to group
them into three broad classes. The ﬁrst consid-
ers the case where the parameter space is bounded
and embraces, in particular, testing for a value of
the parameter which lies on its boundary; see Sec-
tion 3. Section 4 concerns models where one part of
the parameter vanishes when the remaining one is
set to a particular value. The best-studied indeter-
minate parameter problems are ﬁnite mixture mod-
els. Given their widespread use in statistical prac-
tice, and their closeness to boundary problems, we
will consider them separately in Section 5. Change-
point problems are the third broad class of nonreg-
ular models, which we review in Section 6. Most ar-
ticles investigate the consequences of the failure of
one regularity condition at a time. Mixture distri-
butions and change-point problems deserve special
attention as they represent situations where two
conditions fail simultaneously. Section 7 reviews
cases which do not ﬁt into the above three broad
model classes, but still fall under the big umbrella
of nonregular problems. These include, among oth-
ers, shape constrained inference, a genre of non-
parametric problem which leads to highly nonreg-
ular models.

Despite the many remarkable theoretical devel-
opments in likelihood-based asymptotic theory for
nonregular parametric models, one may wonder
why the corresponding results are little known es-
pecially among practitioners. We believe there are
at least two reasons. The ﬁrst is that the results are
highly scattered, in time and scope, which makes

3

it diﬃcult to get the general picture. The second
reason is that the limiting distributions are often
fairly complex in their derivation and implementa-
tion. Section 8 reviews the few software implemen-
tation we are aware of. The paper then closes with
the short summary discussion of Section 9.

2 Likelihood Asymptotics

2.1 First order theory

2.1.1 General notation.

Consider a parametric statistical model F =
{f (y; θ), Θ, Y}, where y = (y1, . . . , yn) are n ob-
servations from the d-dimensional random variable
Y = (Y1, . . . , Yn), d ≥ 1, with probability density
or mass function f (y; θ) whose support is Y ⊆ Rd.
Furthermore, the p-dimensional parameter θ takes
values in a subset Θ ⊆ Rp, p ≥ 1. Throughout
the paper we will consider y an independent and
identically distributed random sample unless stated
diﬀerently. Furthermore, there may be situations
where the model is speciﬁed by a function diﬀer-
ent from f (y; θ) such as the cumulative distribu-
tion function F (y; θ). Let L(θ) = L(θ; y) ∝ f (y; θ)
and l(θ) = log L(θ) denote the likelihood and the
log-likelihood functions, respectively. The large in-
terest in this inferential tool is motivated by the
idea that L(θ) will be larger for values of θ near
the true value θ0 of the distribution which gener-
ated the data. The maximum likelihood estimate
(MLE) ˆθ of θ is the value of θ which maximises L(θ)
or equivalently l(θ). That is, it is the value to which
the data assign maximum evidence. Under mild
regularity conditions on the log-likelihood function
to be discussed in Section 2.2, ˆθ solves the score
equation u(θ) = 0, where u(θ) = ∂l(θ)/∂θ is the
score function. We furthermore deﬁne the observed
information function j(θ) = −∂2l(θ)/∂θ∂θ(cid:62), where
θ(cid:62) denotes transposition of θ, and the expected or
Fisher information i(θ) = E [j(θ; Y )].

2.1.2 No nuisance parameter.

The three classical likelihood-based statistics for
testing θ = θ0 are the

standardized MLE,

score statistic,

(ˆθ − θ0)(cid:62)j(ˆθ)(ˆθ − θ0),
u(θ0)(cid:62)j(ˆθ)−1u(θ0),

likelihood ratio

W (θ0) = 2{l(ˆθ)−l(θ0)},
where the observed information j(ˆθ) is at times re-
placed by the Fisher information i(θ). These statis-
tics are also known under the names of Wald’s,
Rao’s and Wilks’ tests, respectively.
If the para-
metric model is regular, the ﬁnite-sample null dis-
tribution of the above three statistics converges
p distribution to the order O(n−1) as n →
to a χ2
inference may be based on
For θ scalar,
∞.
the corresponding signed versions, that is, on the
signed Wald statistic, (ˆθ − θ0)j(ˆθ)1/2, score statis-
tic, u(θ0)j(θ0)−1/2, and likelihood root,

r(θ0) = sign(ˆθ − θ0)[2{l(ˆθ) − l(θ0)}]1/2,

whose ﬁnite-sample distributions
to
the standard normal distribution to the order
O(n−1/2).

converge

2.1.3 Nuisance parameters.

Suppose now that the parameter θ = (ψ, λ) ∈ Ψ×Λ
is partitioned into a p0-dimensional parameter of
interest, ψ ∈ Ψ ⊆ Rp0, and a vector of nuisance
parameters λ ∈ Λ ⊆ Rp−p0 of dimension p − p0.
Large-sample inference for ψ is commonly based
on the proﬁle log-likelihood function

lp(ψ) = sup
λ∈Λ

l(ψ, λ),

which maximises the log-likelihood l(ψ, λ) with re-
spect to λ for ﬁxed ψ. The proﬁle likelihood ratio
statistic for testing ψ ∈ Ψ0 is

Wp(ψ0) = 2{ sup
ψ∈Ψ

lp(ψ) − sup
ψ∈Ψ0

lp(ψ)},

where Ψ0 ⊂ Ψ is the parameter space speciﬁed
under the null hypothesis.
If the null hypothe-
sis is ψ = ψ0, the ﬁnite-sample distribution of
Wp(ψ0) converges to the χ2
distribution to the
p0
order O(n−1) for n → ∞.

If there exists a closed form expression for the
constrained maximum likelihood estimate ˆλψ of λ
for given ψ, the proﬁle log-likelihood function may
be written as

lp(ψ) = sup
λ∈Λ

l(ψ, λ) = l(ψ, ˆλψ).

(2.1)

A typical situation where ˆλψ is not available in
closed form is when the nuisance parameter λ van-
ishes under the null hypothesis, as will be addressed

4

If (2.1) holds, we may deﬁne the
in Section 4.1.
proﬁle Wald, score and likelihood ratio statistics
for testing ψ = ψ0 as in Section 2.1.2, but now
in terms of the proﬁle log-likelihood lp(ψ), with
up(ψ) = ∂lp(ψ)/∂ψ and jp(ψ) = ∂lp(ψ)/∂ψ∂ψ(cid:62)
being the proﬁle score and proﬁle observed infor-
mation functions. The asymptotic null distribution
of these statistics is a χ2
distribution up to the or-
p0
der O(n−1). If ψ is scalar, the distributions of the
corresponding signed versions, ( ˆψ − ψ0)jp( ˆψ)1/2,
up(ψ0)jp(ψ0)−1/2, and

rp(ψ0) = sign( ˆψ − ψ0)[2{lp( ˆψ) − lp(ψ0)}]1/2, (2.2)

may be approximated by standard normal distri-
butions up to the order O(n−1/2).

2.2 Regularity conditions

The ﬁrst step in the derivation of the large-sample
approximations and statistics of Sections 2.1 is typ-
ically Taylor series expansion of the log-likelihood
function l(θ), or quantities derived thereof, in ˆθ
around θ. We illustrate this by considering the
derivation of the asymptotic distribution of the like-
lihood ratio statistic W (θ) = 2{l(ˆθ) − l(θ)} for the
scalar parameter case.

Example 2.1 (Asymptotic distribution of the like-
lihood ratio). Let p = 1 and lm = lm(θ) =
dml(θ)/dθm be the derivative of order m = 2, 3, . . .
of l(θ), the log-likelihood function for θ in a reg-
ular parametric model. Recall that −l2(θ; y) =
j(θ) represents the observed information, while
E[−l2(θ; Y )] = i(θ) is the expected Fisher infor-
mation. Taylor series expansion of l(θ) around ˆθ
yields

l(θ) = l(ˆθ) −

j(ˆθ)(ˆθ − θ)2 −

1
2

1
6

(ˆθ − θ)3l3(˜θ),

where ˜θ is such that |˜θ − ˆθ| < |θ − ˆθ|. Suitable
rearrangement of the terms, leads to

W (θ) = j(ˆθ)(ˆθ − θ)2 +

1
3

l3(˜θ)(ˆθ − θ)3

=

j(ˆθ)
i(θ)

i(θ)(ˆθ − θ)2 +

1
3

l3(˜θ)(ˆθ − θ)3.(2.3)

1980, Statement (i) of Theorem on p. 145),

j(θ)
i(θ)

p
→ 1 and

i(θ)1/2(ˆθ − θ) d→ Z,

where Z has the standard normal distribution (Ser-
ﬂing, 1980, Lemma B and Lemma A(ii)). Further-
p
more, by the law of large numbers l3(θ)
→ c < +∞.
Advocating Slutzky’s lemma, the leading term in
(2.3) hence converges asymptotically to the χ2
1 dis-
tribution, while the second addend is of order op(1).
This leads to the well known result for Wilks’ statis-
tic.

The derivation of Example 2.1 requires that the
model under consideration is regular. This im-
plies ﬁrst of all that the log-likelihood function
can be diﬀerentiated at least to the third order,
but also that the expected values of log-likelihood
derivatives are ﬁnite and that their asymptotic
order is proportional to the sample size. Wald
(1949)—who is generally acknowledged for having
provided the earliest proof of consistency of the
maximum likelihood estimator which is mathemat-
ically correct—furthermore emphasized the impor-
tance of the compactness of the parameter space
Θ and that the maximum likelihood estimator be
unique. Indeed, the former condition was missing
in Cram´er’s (1946) and Huzurbazar’s (1948) proofs.
In this paper, by the term “regularity conditions”
we mean the assumptions on the parametric sta-
tistical model F that ensure the validity of classi-
cal asymptotic theory. These may be formulated
in several ways; see e.g. Cox and Hinkley (1974,
p. 281), Barndorﬀ-Nielsen and Cox (1994, §3.8),
Azzalini (1996, §3.2.3), Severini (2000, §4.7), van
der Vaart (2000, Chap. 5), Davison (2003, §4.6),
Hogg, McKean and Craig (2019, §6.1, §6.2 and
A.1). We will assume that the following ﬁve condi-
tions on F = {f (y; θ), Θ, Y} and related likelihood
quantities hold.

Condition 1 All components of θ are identiﬁable.
That is, two probability density or mass func-
tions f (y; θ1) and f (y; θ2) deﬁned by any two
diﬀerent values θ1 (cid:54)= θ2 of θ are distinct almost
surely.

Now, under suitable regularity conditions on l(θ)
p
and of its ﬁrst three derivatives, ˆθ
→ θ (Serﬂing,

Condition 2 The support Y of f (y; θ) does not de-

pend on θ.

5

Condition 3 The parameter space Θ is a compact
subset of Rp, for a ﬁxed positive integer p, and
the true value θ0 of θ is an interior point of Θ.

Condition 4 The partial derivatives of the log-
likelihood function l(θ; y) with respect to θ up
to the order three exist in a neighbourhood of
the true parameter value θ0 almost surely. Fur-
thermore, in such a neighbourhood, n−1 times
the absolute value of the log-likelihood deriva-
tives of order three are bounded above by a
function of Y whose expectation is ﬁnite.

Condition 5 The ﬁrst two Bartlett identities hold,

which imply that

E[u(θ; Y )] = 0,

i(θ) = Var[u(θ; Y )],

in addition to 0 < Var[u(θ; Y )] < ∞.

interest, and can fail

Conditions 1–5 are relevant in many important
models of practical
in as
many ways. For instance, from the perspective
of signiﬁcance testing, Condition 1 fails when un-
der the null hypothesis parameters deﬁned for the
whole model become undeﬁned and therefore ines-
timable. We already mentioned this situation when
introducing the proﬁle log-likelihood function; non-
identiﬁability of the parameters will be addressed
in Section 4.1. Further examples are treated in Sec-
tions 4.2 and 5. Failure of Condition 2 is addressed
in Hirano and Porter (2003) and Severini (2004).
Failure of Condition 3 characterises the ﬁrst and
most extensively explored nonregular setting, that
is, boundary problems; see Section 3. Furthermore,
they include the, to our knowledge, only contribu-
tion which explores the higher order properties of
likelihood-based test statistics in a nonregular set-
ting (del Castillo and Lopez-Ratera, 2006). The
compactness condition, in particular, can be omit-
ted, provided it is replaced by some other require-
ments; see, for instance, Pfanzagl (2017, Page 119).
This will be also the case for a number of the large-
sample results derived for nonregular models; see,
for instance, Section 5. Condition 4 typically does
not hold in change-point problems, which will be
treated in Section 6. A further prominent exam-
ple where Condition 4 is not satisﬁed, is the double
exponential, or Laplace, distribution, which arises
in quantile regression. For a book-length review
of this topic we refer the reader to Koenker et al.

(2017). Condition 5 is guaranteed if standard re-
sults on the interchanging of integration and dif-
ferentiation hold, Condition 2 is satisﬁed, and the
log-likelihood derivatives are continuous functions
of θ. A typical situation where this condition fails
is when the data under analysis are derived from
a probability density which does not belong to the
model f (y; θ), a topic of much investigation in ro-
bustness (Huber and Ronchetti, 2009). A remedy is
provided by Godambe’s theory of estimating equa-
tions (Godambe, 1991).

Conditions 4 and 5, as used by Cram´er (1946),
Wald (1949) and others, require the existence of at
least three derivatives of the log-likelihood function
together with some uniform integrability restric-
tions. Condition 4, in particular, embraces both,
the existence of the partial derivatives of l(θ) and
their asymptotic order. An example for which this
latter condition does not hold is the Pearson Type
III (or translated Gamma) distribution (Blischke et
al., 1969), which generalizes Example 1.2. In this
latter case, |dl(θ; y)/dθ| = n is not dominated by
an integrable function on (θ, +∞). Condition 4 en-
sures the consistency of the maximum likelihood es-
timator ˆθ, and the existence of a quadratic approx-
imation to the likelihood ratio function l(ˆθ) − l(θ)
in the Euclidean n−1/2-neighbourhood of θ of the
form

l(ˆθ) − l(θ) = (ˆθ − θ)(cid:62)u(θ; y)

−

1
2

(ˆθ − θ)(cid:62)i(θ)(ˆθ − θ) + op(1),(2.4)

which involves the score function u(θ; y) and
the expected Fisher information i(θ). However,
these conditions do not have by themselves any
direct statistical
LeCam (1970)
interpretation.
presents a diﬀerent type of regularity assumption—
diﬀerentiability in quadratic mean—which requires
only diﬀerentiation of order one and may be justi-
ﬁed from a statistical point of view.

2.3 Local asymptotics

To give a glimpse of LeCam’s ideas, assume, with-
out loss of generality, θ ∈ R scalar. A statistical
model F = {f (yi; θ), θ ∈ Θ} is said to be diﬀeren-
tiable in quadratic mean (DQM) at θ if there exists

6

a random function ˙u(θ; Yi) such that, as h → 0,

whose deﬁnition can be linked to the notion of dif-
ferentiability in quadratic mean as

Eθ

(cid:104)(cid:110)(cid:112)f (Yi; θ + h) − (cid:112)f (Yi; θ)
(cid:27)2(cid:35)

˙u(θ; Yi)(cid:112)f (Yi; θ)

= o(||h2||).

−

h
2

(2.5)

H 2(θ) =

=

h2
8
h2
8

(cid:104)

{ ˙u(θ; Yi)}2(cid:105)

Eθ

+ o(||h||2)

¨ı(θ) + o(||h||2)

The expectation is taken with respect to the distri-
bution indexed by θ and the function ˙u(θ; Yi) is said
to be the quadratic mean derivative of the square
root (cid:112)f (Yi; θ) of the probability density function
(Lehmann and Romano, 2005, Chapter 12). As
shown in LeCam’s (1970) paper, the regularity con-
ditions of Cram´er type imply diﬀerentiability in
quadratic mean.
Indeed, for every diﬀerentiable
f (yi; θ)

(cid:112)f (yi; θ) =

∂
∂θ

1
2

u(θ; yi)(cid:112)f (yi; θ),

(2.6)

where ˙u(θ; yi) = u(θ; yi) is the score function as
deﬁned in Section 2.1. The opposite does not
hold true, a prominent counterexample being the
Laplace distribution. Diﬀerentiability in quadratic
mean hence generalizes Condition 5 in a natural
way as it is possible to show that Eθ[ ˙u(θ; Yi)] = 0
and that the equivalent of the unit expected Fisher
information ¨ı1(θ) = Eθ[{ ˙u(θ; Yi)}2] is ﬁnite.

√

Using diﬀerentiability in quadratic mean, LeCam
gives rise to a radically diﬀerent type of asymptotic
inference called local asymptotics. The word ‘local’
is meant to indicate that one looks at a sequence of
alternative hypotheses of the form θn = θ + (cid:15)/
n,
where (cid:15) is any given real number. The properties
of the likelihood-based procedures are hence stud-
ied in a small neighbourhood θ ± (cid:15)/
n of the ﬁxed
parameter θ deﬁned by (cid:15), where ‘small’ means of
size O(1/
n). The motivation for studying a local
approximation is that, usually, asymptotically, the
‘true’ parameter value can be known with unlimited
precision. The real diﬃculty is therefore to distin-
guish between values which are ‘close’. Closeness
in this case is measured in terms of the Hellinger
distance

√

√

H 2(θ) =

1
2

(cid:20)(cid:110)(cid:112)λ(θ; Yi) − 1

(cid:111)2(cid:21)

,

Eθ

with

λ(θ; Yi) =

f (Yi; θ + h)
f (Yi; θ)

,

7

if the model satisﬁes the DQM condition (2.5).
in this case it is possible to show that
Indeed,
the likelihood ratio function of a random sample,
y = (y1, . . . , yn), of size n

lr(θ; y) =

n
(cid:88)

i=1

log λ(θ; yi)

√

with h = (cid:15)/
in that

n, is locally asymptotically quadratic

lr(θ; y) =

(cid:15)
√
n

˙u(θ; y) −

1
2

(cid:15)2
n

¨ı(θ) + op(1).

Note how this expression mimics the quadratic
likelihood-based
approximation (2.4) of classical
asymptotics, where ˆθn − θ = Op(1/
n) and the
score and the expected Fisher information func-
tions are replaced by ˙u(θ; Y ) and ¨ı(θ) = n¨ı1(θ). For
large n we can locally approximate the likelihood
ratio function by the normal distribution

√

(cid:18)

N

−

1
2

(cid:15)2¨ı1(θ), (cid:15)2¨ı1(θ)

(cid:19)

,

which then serves as the basis for the derivation
of the limiting distributions of estimators and test
statistics. For further details see the two mono-
graphs by van der Vaart (2000, Chapter 7) and
LeCam and Yang (1970).

In the remainder of the paper, we review the
most common situations where one or some of Con-
ditions 1–5 fail. We will also provide some sum-
mary insight into the main prototype derivations
of the corresponding ﬁnite-sample or asymptotic
results. The vast majority of the proofs require
conditions of Cram´er type; in some occasions, as
for instance in Section 4.1, LeCam’s local asymp-
totic theory will be used.

3 Boundary Problems

A boundary problem arises when the value θ0 spec-
iﬁed by the null hypothesis, or parts of it, is not an
interior point of the parameter space.
In general
terms, the “boundary” of the parameter space Θ
is the set of values θ such that every neighbour-
hood of θ contains at least one interior point of Θ
and at least one point which is not in Θ.
Infor-
mally, the methodological diﬃculties in likelihood-
based inference occur because the maximum like-
lihood estimate can only fall ‘on the side’ of θ0
that belongs to the parameter space Θ. This im-
plies that if the maximum occurs on the boundary,
the score function need not be zero and the distri-
butions of the related likelihood statistics will not
converge to the typical normal or chi-squared dis-
tributions. Because of the diﬃculties inherent the
derivation of the limiting distribution of the likeli-
hood ratio statistic, especially practitioners tend to
ignore the boundary problem and to proceed as if
all parameters where interior points of Θ. This is
commonly called the na¨ıve approach. An alterna-
tive approach is to suitably enlarge the parameter
space so as to guarantee that the likelihood ratio
maintains the common limiting distribution; see,
for instance, Feng and McCulloch (1992). However,
this idea works only as long as the null hypothesis
is uniquely identiﬁed. A counterexample for ﬁnite
mixtures is given by B¨ohning and Dietz (1995) in
their Discussion to Cheng and Traylor (1995). The
literature on boundary problems is very rich and in-
cludes, among others, solutions for random eﬀects
and frailty models, and for times series analysis.
The following example gives a ﬂavour of the statis-
tical issues.

Example 3.1 (Bivariate normal). Consider a sin-
gle observation y = (y1, y2) from the bivariate nor-
mal random variable Y = (Y1, Y2) ∼ N2(θ, I2),
where θ = (θ1, θ2), with θ1 ≥ 0 and θ2 ≥ 0, and
I2 is the 2 × 2 identity matrix. Straightforward cal-
culation shows that the null distribution of the like-
lihood ratio statistic for θ0 = (0, 0) versus the alter-
native hypothesis that at least one equality does not
hold, converges to a mixture of a point mass χ2
0 at 0
and two chi-squared distributions, χ2
2 (Das-
Gupta, 2008, Example 21.3). Figure 2 provides a
graphical representation of the problem. Because
of the limitedness of the parameter space, we have
that ˆθ1 = max(y1, 0) and ˆθ2 = max(y2, 0). The grey
shaded area is the parameter space into which the

1 and χ2

Figure 2: Example 3.1: Bivariate normal. The
grey shaded area represents the parameter space Θ.
Under the null hypothesis θ0 = (0, 0), the parame-
ter space collapses with the origin. The asymptotic
distribution of the corresponding likelihood ratio
statistics is a mixture of χ2
2 distribu-
tions with weights (0.25, 0.5, 0.25).

1 and χ2

0, χ2

MLE is bound to fall. However, the random obser-
vation Y = (Y1, Y2) can fall into any of the 4 quad-
rants of R2 with equal probability 1/4. When Y falls
into the ﬁrst quadrant, that is, when y1, y2 > 0, the
likelihood ratio statistic is W (θ0) = Y 2
2 and
follows the common χ2
2 distribution. However, if
y1 > 0 and y2 < 0 or when y1 < 0 and y2 > 0, we
have that W (θ0) = Y 2
2 ∼ χ2
1,
respectively. Lastly, when Y lies in the third quad-
rant, W (θ0) = 0 and its distribution is a point mass
in 0. Summing up, we can informally write

1 and W (θ0) = Y 2

1 + Y 2

1 ∼ χ2

W (θ0) ∼

1
4

χ2

0 +

1
2

χ2

1 +

1
4

χ2
2.

(3.1)

Distribution (3.1) is a special case of the so-called
chi-bar squared distribution (Kudˆo, 1963), denoted
by ¯χ2(ω, N ), with cumulative distribution function

Pr( ¯χ2 ≤ c) =

N
(cid:88)

ν=0

ωνPr(χ2

ν ≤ c),

which corresponds to a mixture of chi-squared dis-
tributions with degrees of freedom ν from 0 to N .
In some cases, explicit and computationally fea-
sible formulae are available for the weights ω =
(ω0, . . . , ωN ). Extensive discussion on their compu-
tation and use, with special emphasis on inequal-
ity constrained testing, is given in Robertson et al.
(1988, Chapters 2 and 3), Wolak (1987), Shapiro
(1985, 1988) and Sun (1988).

8

W(θ0)=Y22~χ12W(θ0)=Y12~χ12W(θ0)=0~χ02θ2θ1W(θ0)=Y12+Y22~χ22θ0=(0,0)3.1 General results

the distribution of

The research on boundary problems was initiated
by Chernoﬀ (1954) who derives the asymptotic
null distribution of the likelihood ratio statistic for
testing whether θ lies on one or the other side
of a smooth (p − 1)-dimensional surface in a p-
dimensional space when the true parameter value
lies on the surface. Using a geometrical argu-
ment, Chernoﬀ established that this distribution
is equivalent to the distribution of the likelihood
ratio statistic for testing suitable restrictions on
the mean of a multivariate normal distribution
with covariance matrix given by the inverse of the
Fisher information matrix using a single observa-
tion. In particular, Chernoﬀ proved that the lim-
iting distribution is a ¯χ2(ω, 1) distribution, with
ω = (0.5, 0.5), that is, a mixture of a point mass
at zero and a χ2
1, with equal weights. This general-
izes Wilks (1938) result when the parameter space
under the null hypothesis is not a hyperplane.

In Chernoﬀ (1954), the parameter spaces Θ0 and
Θ1, speciﬁed by the null and the alternative hy-
potheses, are assumed to have the same dimen-
sion. Furthermore, the true parameter value falls
on the boundary of both, Θ0 and Θ1, while it is
still an interior point of the global parameter space
Θ = Θ0 ∪ Θ1. The no doubt cornerstone contribu-
tion which inspired many researchers and fuelled
an enormous literature, is the highly-cited article
by Self and Liang (1987). Using geometrical ar-
guments similar to those of Chernoﬀ (1954), Self
and Liang (1987) study the asymptotic null distri-
bution of the likelihood ratio statistic for testing
the null hypothesis θ ∈ Θ0 against the alternative
θ ∈ Θ1 = Θ \ Θ0. This time, the true parame-
ter value θ0 no longer needs be an interior point,
but can fall onto the boundary of Θ. The two sets
Θ and Θ0 must be regular enough to be approxi-
mated by two cones, CΘ and CΘ0, with vertex at θ0
(Chernoﬀ, 1954, Deﬁnition 2). Under this scenario
and provided their Assumptions 1–4 hold—which
translate into our Conditions 1–2 and 4–5, with
likelihood derivatives taken from the appropriate
side—Self and Liang (1987, Theorem 3) show that
the distribution of the likelihood ratio converges to

(cid:110)
−( ˜Z − θ)(cid:62)i1(θ0)( ˜Z − θ)

(cid:111)

−

(3.2)

sup
θ∈CΘ−θ0

(cid:111)
(cid:110)
−( ˜Z − θ)(cid:62)i1(θ0)( ˜Z − θ)

.

sup
θ∈CΘ0−θ0

Here, CΘ−θ0 and CΘ0−θ0 are the translations of the
cones CΘ and CΘ0, such that their vertices are at
the origin, and ˜Z is a multivariate Gaussian vari-
able with mean 0 and covariance matrix given by
i1(θ0)−1, which is the Fisher information matrix for
a single observation. If we transform the random
variable ˜Z so that it follows a multivariate standard
Gaussian distribution Z, we can re-express Equa-
tion (3.2) as

||Z − θ||2 − inf
θ∈ ˜C

||Z − θ||2 =

inf
θ∈ ˜C0
||Z − P ˜C0

(Z)||2 − ||Z − P ˜C(Z)||2,

(3.3)

where ˜C and ˜C0 are the corresponding transforma-
tions of the cones CΘ−θ0 and CΘ0−θ0 and ||·|| is the
Euclidean norm. Finding the null distribution re-
quires to work out the two projections P ˜C(Z) and
(Z) of Z onto the cones ˜C and ˜C0. This must
P ˜C0
be done on a case by case basis as shown by the
following revisitation of Example 3.1.

Example 3.2 (Bivariate normal revisited). In Ex-
ample 3.1 we faced a typical non-standard situation
where both components of the parameter θ are of
interest and both lie on the boundary of the param-
eter space. Here, the Fisher information matrix
is the identity matrix which is why ˜Z = Z = Y
and the original two set Θ and Θ0 agree with the
approximating cones. That is, the grey shaded re-
gion [0, ∞) × [0, ∞) in Figure 2 represents the sets
Θ = CΘ = CΘ−θ0 = ˜C, while the origin {0} corre-
sponds to the sets Θ0 = CΘ0 = CΘ0−θ0 = ˜C0. The
derivation of the second term of (3.3) depends on
the projection of Z onto ˜C, which is

P ˜C(Z) =


Z = (Z1, Z2)

Z2
0

Z1

if Z1, Z2 > 0
if Z1 < 0, Z2 > 0
if Z1, Z2 < 0
if Z1 > 0, Z2 < 0,

while P ˜C0
(Z) = 0. As shown in Example 3.1,
P ˜C(Z) takes on the four possible values with equal

9

probability 1/4. By simple algebra, we can prove
that the distribution of the likelihood ratio statistics
is given by the mixture of Equation (3.1).

A sketch of the derivation of Equation (3.2) is
given in Appendix A.1. The proof consists of two
steps. We ﬁrst consider a quadratic Taylor series
expansion of the log-likelihood l(θ) around θ0, the
true value of the parameter. The asymptotic dis-
tribution of the likelihood ratio statistic is then de-
rived as in Chernoﬀ (1954) by approximating the
sets Θ and Θ0 using the cones CΘ and CΘ0. Self
and Liang (1987) present a number of special cases
in which the representations (3.2) and (3.3) are
used to derive the asymptotic null distribution of
the likelihood ratio statistic.
In most cases, the
limiting distribution is a chi-bar squared distribu-
tion whose weights depend, at times in a rather
tricky way, on the partition of the parameter space
induced by the geometry of the cones.

A further major step forward in likelihood
asymptotics for boundary problems was marked by
Kopylev and Sinha (2011) and Sinha et al. (2012).
Now, the null distribution of the likelihood ratio
statistic is derived by using algebraic arguments.
From the technical point of view, the derivation of a
closed form expression for the limiting distribution
of the likelihood ratio becomes the more diﬃcult
the more nuisance parameter lie on the boundary
of the parameter space. In particular, the deriva-
tion of the limiting distribution becomes awkward
when there are more than four boundary points
and/or the Fisher information matrix is not diag-
onal. Sinha et al. (2012) furthermore show that
when one or more nuisance parameters are on the
boundary, following the na¨ıve approach can result
in inferences which are anticonservative.
In gen-
eral, the asymptotic distribution turns out to be
a chi-bar squared distribution with weights that
depend on the number of parameters of interest
and of nuisance parameters, and on where these lie
in Θ. However, limiting distributions other than
the ¯χ2 distribution are found as well; see, for in-
stance, Theorem 2.1 of Sinha et al. (2012). A con-
cise review of the cases considered in Self and Liang
(1987), Kopylev and Sinha (2011) and Sinha et al.
(2012), with some interesting examples and an ac-
count of the areas of interest in genetics and biol-
ogy, is given by Kopylev (2012). The following two
sections treat two special cases, namely testing for a

zero variance component and constrained one-sided
tests. We will mention the mainstream contribu-
tions, while further related work can be found in the
annotated bibliography available as Supplementary
Material. This includes, for instance, alternatives
which avoid the calculation of the mixing weights
of the ¯χ2 distribution and/or lead to the classical
χ2 limiting distribution.

3.2 Null variance components

In linear and generalized linear mixed models a
boundary problem arises as soon as we want to as-
sess the signiﬁcance of one or more variance com-
ponents. The two reference papers are Crainiceanu
and Ruppert (2004) and Stram and Lee (1994).
Both consider a linear mixed eﬀects model and test
for a zero scalar variance component. However,
Stram and Lee (1994) assume that the data vec-
tor can be partitioned into a large number of in-
dependent and identically distributed sub-vectors,
which needs not hold for Crainiceanu and Ruppert
(2004). The limiting distributions are derived from
the spectral decomposition of the likelihood ratio
statistic.

More precisely, assume the following model

holds,

Y = Xβ + Zb + ε,

where Y is a vector of observations of dimension n,
X is a n × p ﬁxed eﬀects design matrix and β is a
p-dimensional vector of ﬁxed eﬀects. In addition,
Z is a n × k random eﬀects design matrix and b
is a k -dimensional vector of random eﬀects which
are assumed to follow a multivariate Gaussian dis-
tribution with mean 0 and covariance matrix σ2
b Σ
of order k × k. The error term ε is assumed to be
independent of b and distributed as a normal ran-
dom vector with zero mean and covariance matrix
σ2
ε In, where In is the identity matrix. Suppose we
are interested in testing

H0 : βp+1−q = β0

p+1−q, . . . , βp = β0
p,

σ2
b = 0

against

H1 : βp+1−q (cid:54)= β0

p+1−q, . . . , βp (cid:54)= β0
p,

or σ2

b > 0

for some positive value of q ∈ {1, . . . , p}. Non-
regularity arises as under the null hypothesis σ2
b =
0 falls on the boundary of the parameter space.

10

Furthermore, the alternative hypothesis that σ2
b >
0 induces dependence among the observations Y .
Crainiceanu and Ruppert (2004, Theorem 1) show
that the ﬁnite-sample distribution of the likelihood
ratio statistic agrees with the distribution of

(cid:32)

n

1 +

(cid:80)q

s=1 u2
s
s=1 w2
s

(cid:80)n−p

(cid:33)

+ sup
λ≥0

fn(λ),

(3.4)

where us for s = 1, . . . , k and ws for s = 1, . . . , n−p
are independent standard normal variables, λ =
b /σ2
σ2

ε , and

Ruppert’s (2004) results include testing for level-
or subject-speciﬁc eﬀects in a balanced one-way
ANOVA, testing for polynomial regression versus a
general alternative described by P-splines and test-
ing for a ﬁxed smoothing parameter in a P-spline
regression.

3.3 Constrained one-sided tests

Multistage dose-response models are a further ex-
ample of boundary problem. A K-stage model is
characterised by a dose-response function of the
form

log (1 + λξs,n),

ψ(d; β) = ψ(β0 + β1d + β2d2 + · · · + βKdK),

(cid:26)

fn(λ) = n log

1 +

(cid:27)

−

Nn(λ)
Dn(λ)

k
(cid:88)

s=1

where

and

Nn(λ) =

k
(cid:88)

s=1

λµs,n
1 + λµs,n

w2
s,

Dn(λ) =

k
(cid:88)

s=1

w2
s
1 + λµs,n

+

n−p
(cid:88)

s=k+1

w2
s.

2 Z T ZΣ 1

2 and Σ 1

2 Z T P0ZΣ 1

Here, µs,n and ξs,n are the k eigenvalues of the ma-
trices Σ 1
2 , respectively.
The matrix P0 = In −X(X T X)−1X T is the matrix
which projects onto the orthogonal complement to
the subspace spanned by the columns of the design
matrix X. Theorem 2 of Crainiceanu and Rup-
pert (2004) shows that the asymptotic null distribu-
tion of the likelihood ratio statistic depends on the
asymptotic behaviour of the eigenvalues µs,n and
ξs,n. The limiting distribution, in general, diﬀers
from the chi-bar squared distribution which often
holds for independent and identically distributed
data.

Formula (3.4) represents the spectral decompo-
sition of the likelihood ratio statistic. A similar
result is also derived for the restricted likelihood
ratio (Crainiceanu and Ruppert, 2004, Formula 9).
The unquestioned advantage of these two results
is that they allow us to simulate the ﬁnite-sample
null distribution of the two test statistics once the
eigenvalues are calculated. Furthermore, this simu-
lation is more eﬃcient than bootstrap resampling,
as the speed of the algorithm only depends on the
number of random eﬀects k, and not on the number
of observations n. Applications of Crainiceanu and

11

where d is the tested dose and ψ(·) is a function
of interest such as, for instance, the probability
of developing a disease. The coeﬃcients βk ≥ 0,
for k = 1, . . . , K, are often constrained to be non-
negative so that the dose-response function will be
non-decreasing. There is no limit on the number of
stages K, though in practice this is usually speciﬁed
to be no larger than the number of non-zero doses.
Testing whether βk = 0 results in a boundary prob-
lem and requires the application of a so-called con-
strained one-sided test. Apart from clinical trials,
constrained one-sided tests are common in a num-
ber of other areas, where the constraints on the
parameter space are often natural such as testing
for over-dispersion, for the presence of clusters and
for homogeneity in stratiﬁed analyses. All these in-
stances amount to having the parameter value lying
on the boundary of the parameter space under the
null hypothesis. Despite their importance in sta-
tistical practice, few contributions are available on
the asymptotic behaviour of the most commonly
used test statistics, and of the likelihood ratio in
particular.

A ﬁrst contribution which evaluates the asymp-
totic properties of constrained one-sided tests is
Andrews (2001), who establishes the limiting dis-
tributions of the Wald, score, quasi-likelihood and
rescaled quasi-likelihood ratio statistics under the
null and the alternative hypotheses. The results
are used to test for no conditional heteroscedas-
ticity in a GARCH(1,1) regression model and zero
variances in random coeﬃcient models. Sen and
Silvapulle (2002) review reﬁnements of likelihood-
based inferential procedures for a number of para-
metric, semiparametric, and nonparametric models

when the parameters are subject to inequality con-
straints. Special emphasis is placed on their appli-
cability, validity, computational ﬂexibility and ef-
ﬁciency. Again, the chi-bar squared distribution
plays a central role in characterising the limiting
null distribution of the test statistics, while the cor-
responding proof requires tools of convex analysis,
such projections onto cones. See Silvapulle and Sen
(2005) for a book-length account of constrained sta-
tistical inference.

4

Indeterminate
problems

parameter

An “indeterminate parameter” problem occurs
when setting one of the components of the param-
eter θ = (θ1, θ2) to a particular value, say θ1 = θ10,
leads to the disappearance of some or all compo-
nents of θ2. The model is no longer identiﬁable,
as all probability density or mass functions f (y; θ)
with θ1 = θ10 and arbitrary θ2 identify the same
distribution. The following simple example illus-
trates this point.

Example 4.1 (Loss of identiﬁability in jump re-
gression). Consider the model

Y = θ11 + θ121(X > θ2) + ε,

ε ∼ f (ε),

where Y is a continuous response, X a correspond-
ing covariate and 1(X > θ2) represents the indi-
cator function which assumes value 1 if X > θ2
and zero otherwise. Furthermore, θ1 = (θ11, θ12) is
a real valued vector of regression coeﬃcients, while
θ2 ∈ R deﬁnes the point at which the jump occurs.
Assume that ε is a zero-mean error term with den-
sity function f (ε). The mean of the variable Y is
θ11 for values of X less or equal to θ2 and is equal
to θ11 + θ12 for values of X larger than θ2. Un-
der the null hypothesis of no jump, θ11 is arbitrary,
but the parameters θ12 = 0 and θ2 disappear; the
model is no longer identiﬁable. Arbitrary values of
θ2 identify the same distribution for the variable Y .

Loss of identiﬁability occurs in areas as diverse as
econometrics, reliability theory and survival anal-
ysis (Prakasa Rao, 1992), and has been the sub-
ject of intensive research. Rothenberg (1971) stud-
ied the conditions under which a general stochastic

model whose probability law is determined by a ﬁ-
nite number of parameters is identiﬁable. Paulino
and Pereira (1994) present a systematic and uniﬁed
description of the aspects of the theory of identiﬁ-
ability.

As illustrated in Example 2.1, the classical theory
of asymptotic inference heavily relies on quadratic
approximation of the log-likelihood function.
In-
deed, if the value θ0 speciﬁed by the null hypothe-
sis is unique, we can use (2.4) to approximate twice
the likelihood ratio function by

2{l(θ) − l(θ0)} = 2

√

n(θ − θ0)(cid:62)νn(u(θ0; Yi))

− n(θ − θ0)(cid:62)i1(θ0)(θ − θ0)
+ op(1),

(4.1)

for θ belonging to an n−1/2-neighbourhood of θ0.
Here, νn(u) = n−1/2 (cid:80)n
i=1{u(θ; Yi) − Eθ0[u(θ; Yi)]}
is a random process deﬁned for any integrable score
function u(θ; Yi). The asymptotic null distribu-
tion of W (θ0) can be obtained by maximizing this
quadratic form in θ. When the parameter which
indexes the true distribution is not unique, various
diﬃculties may arise. For instance, the maximum
likelihood estimator may not converge to any point
in the parameter space speciﬁed by the null hy-
pothesis. Or, the Fisher information matrix degen-
erates. Typically, the limiting distribution of the
likelihood ratio statistics will not be chi-squared.

In the remainder of the section we will con-
sider two special cases: non-identiﬁable parameters
and singular information matrix. We will report
the main research strains; related contributions are
listed in the annotated bibliography available as
Supplementary Material.

4.1 Non-identiﬁable parameters

The general framework for deriving the asymp-
totic null distribution of the likelihood ratio statis-
tic when some of the parameters are not identiﬁ-
able under the null hypothesis was developed by
Liu and Shao (2003). They address the common
hypothesis testing problem H0 : θ ∈ Θ0 against
H1 : θ ∈ Θ \ Θ0, where Θ0 = {θ ∈ Θ : Fθ = F 0}
with Fθ the distribution function indexed by θ and
F 0 the true distribution. The true distribution is
hence unique and H0 is a simple null hypothesis.
However, the set Θ0 may contain more than one
value. When the true parameter value θ0 is not

12

unique, the classical quadratic approximation of
the likelihood ratio function in an Euclidean neigh-
bourhood of θ0 no longer holds. Liu and Shao
(2003) by-pass this problem by establishing a gen-
eral quadratic approximation of the likelihood ratio
function, this time in the so-called Hellinger neigh-
bourhood of F 0

Θ(cid:15) = {θ ∈ Θ | 0 < H(θ) ≤ (cid:15)},

where H 2(θ) is the squared Hellinger distance be-
tween Fθ and F 0. The rationale is that, instead of
using the Euclidean distance between two param-
eter values, θ and θ0, closeness between the two
models deﬁned by Fθ and F 0 is now measured in
terms of a distance, which is valid with or without
loss of identiﬁability of the true distribution F 0.
This is closely related to, and indeed generalizes,
LeCam’s (1970) local asymptotic theory which we
brieﬂy discussed in Section 2.3. The proof is de-
tailed in Appendix A.2. Here, we sketch the main
steps in its derivation.

As in Liu and Shao (2003), we express the like-
lihood ratio function based on a random sample of
n observations y = (y1, . . . , yn)

lr(θ) =

n
(cid:88)

i=1

log {λi(θ)} ,

in terms of the Radon-Nikodym derivative, λi(θ) =
λ(θ; yi) = dFθ/dF 0, evaluated at yi,
for i =
1, . . . , n, and recall the deﬁnition

H 2(θ) =

1
2

EF 0

(cid:20)(cid:110)(cid:112)λi(θ) − 1

(cid:111)2(cid:21)

of the square Hellinger distance. As lr(θ) may di-
verge to −∞ for some θ ∈ Θ(cid:15), it can be diﬃcult
to ﬁnd a quadratic approximation of the likelihood
ratio function with a uniform residual term op(1)
in Θ(cid:15). This is why we rewrite the likelihood ratio
statistic as

W (H0) = 2 sup

{lr(θ) ∨ 0},

(4.2)

θ∈Θ\Θ0

EF 0 [Ri(θ)] = 0, satisﬁes
hi(θ) = (cid:112)λi(θ) − 1

= H(θ)Si(θ) − H 2(θ) + H 2(θ)Ri(θ),

for all θ ∈ Θ(cid:15), in addition to the generalized dif-
ferentiable in quadratic mean (GDQM) condition
(Liu and Shao, 2003, Deﬁnition 2.3). We may then
approximate twice the likelihood ratio function by

√

2 lr(θ) = 2

nH(θ)νn(Si(θ))
− nH 2(θ) (cid:8)2 + EFn [S2

i (θ)](cid:9) + op(1),(4.3)

where νn(Si) = n−1/2 (cid:80)n
i=1 {Si(θ) − EF 0 [Si(θ)]}
now is deﬁned in terms of the expectations taken
with respect to the empirical distribution func-
tion Fn(·) and the true distribution F 0. Expan-
sion (4.3) is then used to prove that the distribu-
tion of the likelihood ratio statistic (4.2) converges
to the distribution of the supremum of a squared
left-truncated centered Gaussian process with uni-
formly continuous sample paths. Though Si(θ) and
Ri(θ) may not be unique, they yield the same limit-
ing distribution of the likelihood ratio statistic un-
der suitable conditions. In principle, the distribu-
tion of the Gaussian process can be approximated
by simulation, since its covariance kernel is known.
The most crucial aspect, however, is the derivation
of the set which contains the L2 limits of the gen-
eralized score function

Si(θ)
(cid:112)1 + EF 0[S2

i (θ)]/2

over which the supremum is to be taken. This needs
be worked out on a case by case basis.

The GDQM expansion always exists and reduces

to LeCam’s DQM expansion

hi(θ) = (θ − θ0)(cid:62)u(θ0; yi)

if θ0 is unique. Furthermore, Liu and Shao (2003)
show how (4.3) is equivalent to (4.1) by rewriting
the latter as

√

2 lr(θ) = 2

nD(θ0)νn(S(cid:48)
− nD2(θ0) + op(1),

i(θ0))

where {a ∨ b} = max(a, b), and maximize lr(θ) ∨
0, which generally has a quadratic approxima-
Liu and Shao (2003) show that such
tion.
an approximation exists if,
for some (cid:15) > 0,
the trio {Si(θ), H(θ), Ri(θ)}, with EF 0 [Si(θ)] =

in terms of the squared Pearson-type L2 distance

D2(θ) = Eθ0

{λi(θ) − 1}2(cid:105)
(cid:104)
= (θ − θ0)(cid:62)i1(θ0)(θ − θ0) + op(1),

13

if ||θ − θ0|| = O(n−1/2), and where

S(cid:48)

i(θ) = {λi(θ) − 1} /D(θ),

for θ ∈ Θ \ Θ0, deﬁnes the generalized score func-
tion.

4.2 Singular information matrix

A further case of indeterminate parameter problem
is when the expected Fisher information matrix is
singular at the true value θ0 of the parameter.

Example 4.2 (Singular information). Consider a
sample of size n from a normal random variable
Y with mean θq, for a given odd integer q > 0,
and variance 1. The information function i(θ) =
nq2θ2(q−1) is non singular in an open neighbour-
hood of θ0, but vanishes for θ0 = 0, which vi-
olates Condition 5. For scalar θ, zero informa-
tion implies a null score statistic with probability
1. The left panel of Figure 3 plots the score func-
tions of three diﬀerent samples of size n = 10 for
θ0 = 0 and q = 3. The right panel shows the corre-
sponding normalised log-likelihood functions. The
score function vanishes at the origin and at the
maximum likelihoood estimate ˆθ = ¯y1/q. The log-
likelihood function hence admits a global maximum
in the neighbourhood of the true parameter value
and an inﬂection at θ0 = 0. Standard techniques to
prove consistency of the maximum likelihood esti-
mator and to derive the limiting distribution of the
likelihood ratio statistics, such as Expansion (4.1),
won’t apply as both u(θ0; y) = i(θ0) = 0 at θ0 = 0.

Generally speaking, the singularity of the Fisher
information matrix prevents the use of the usual
second-order expansions of the log-likelihood func-
tion. The, to our knowledge, earliest contribution
who addresses this type of problem is Silvey (1959).
The author proposes to modify the curvature of
the quadratic approximation of the likelihood ratio
by replacing the inverse of the Fisher information
matrix with a generalized inverse matrix obtained
by imposing suitable constraints on the model pa-
rameters. The however cornerstone contribution
to the development of the theory of singular in-
formation matrices is Rotnitzky et al. (2000) who
derive the asymptotic null distribution of the like-
lihood ratio statistic for testing the null hypothesis

14

Figure 3:
Example 4.2: Singular information.
The three solid curves represent the score functions
(left panel) and normalised log-likelihood functions
(right panel) for three samples of size n = 10 for
θ0 = 0 and q = 3.

H0 : θ = θ0 versus H1 : θ (cid:54)= θ0, when θ is a p-
dimensional parameter of an identiﬁable paramet-
ric model and the information matrix is singular at
Indeed, Rotnitzky et al.
θ0 and has rank p − 1.
(2000) derive a suitable approximation for the like-
lihood ration function l(θ) − l(θ0) from a higher-
order Taylor expansion. The theory is developed
only for independent and identically distributed
random variables, though the authors point out
that the same theory may straightforwardly be ex-
tended to non-identically distributed observations.
When θ is scalar, the asymptotic properties of the
maximum likelihood estimator and of the likelihood
ratio statistic depend on the integer m0, which rep-
resents the order of the ﬁrst derivative of the log-
likelihood function which does not vanish at θ = θ0;
see Theorems 1 and 2 of Rotnitzky et al. (2000). If
m0 is odd, the distribution of the likelihood ratio
converges under the null hypothesis to a χ2
1 distri-
bution, while for even m0 it converges to a ¯χ2(ω, 1)
with ω = (0.5, 0.5). As far as Example 4.2 goes,
m0 = 3 and l3(0; y) = 6n¯y (cid:54)= 0. Indeed, the likeli-
hood ratio statistic W (0) = n ¯Y 2 distributes exactly
as a chi-squared distribution with one degree of
freedom. Extensions of these results when the pa-
rameter θ is p-dimensional are also provided. These
are generally based on suitable re-parametrizations
of the model which remove the speciﬁc causes of
the singularity, but are diﬃcult to generalize as
they are ad-hoc solutions. Further contributions
who propose to penalise the likelihood function so
as to guarantee the consistency and normality of

-1.0-0.50.00.51.0-4-2024θu(θ)-1.0-0.50.00.51.0-1.0-0.8-0.6-0.4-0.20.0θl(θ)−l(θ^)the maximum likelihood estimator are mentioned
in the annotated bibliography available as Supple-
mentary Material.

5 Finite mixture models

Finite mixture models deserve special attention,
because of their widespread use in statistical prac-
tice, but also because of the methodological chal-
lenges posed by the derivation of their asymp-
totic properties. They probably represent the best-
studied indeterminate parameter problem, though
In-
we may also treat them as a boundary case.
deed, testing a hypothesis such as model homo-
geneity against the alternative that the model is
a ﬁnite mixture of two or more components will
most likely lead to the failure of two regularity con-
ditions. As we shall see in Section 5.1, this occurs
because while under the null hypothesis the mixing
proportions fall on the boundary of their param-
eter space, some of the parameters of the corre-
sponding component distributions become indeter-
minate. Under this scenario, the asymptotic dis-
tribution of the likelihood ratio statistic does not
follow the commonly believed chi-squared distribu-
tion, and its limiting distribution has for long been
unknown.

The remainder of the section outlines the many
mainstream contributions for this class of models,
with special emphasis on hypothesis testing using
the likelihood ratio. Further related work is listed
in the annotated bibliography available as Supple-
mentary Material. General reference for mixture
distributions are Lindsay (1995) and McLachlan
and Peel (2000).

5.1 Testing for homogeneity

Consider the two-component mixture model

(1 − π)f1(y; θ1) + πf2(y; θ2),

(5.1)

where the probability density or mass functions
f1(y; θ1) and f2(y; θ2), with θ1 ∈ Θ1 ⊆ Rp1 and
θ2 ∈ Θ2 ⊆ Rp2, represent the mixture components
and 0 ≤ π ≤ 1 is the mixing probability. The null
hypothesis of homogeneity can be written in diﬀer-
ent ways. We may set π = 0, which corresponds
to H0 : f 0 = f1(y; θ1), where f 0 represents the
true unknown distribution, or alternatively, π = 1

and H0 : f 0 = f2(y; θ2).
If the two components,
f1(y; θ1) and f2(y; θ2), are known, then the limiting
distribution is a ¯χ2(ω, 1) with ω = (0.5, 0.5) (Lind-
say, 1995, p. 75). Otherwise, for f1(y; θ) = f2(y; θ)
in this case homogene-
a third possibility arises:
ity assumes that H0 : θ1 = θ2. Whatever choice
is made, some model parameters, that is, θ2 and
θ1, respectively, in the ﬁrst two cases and π in the
third, vanish under the null hypothesis. This con-
tradicts classical likelihood theory, where the pa-
rameter which characterises the true distribution is
typically assumed to be a unique point θ0 in the
open subset Θ ⊆ Rp. As we have seen in Section 3,
the failure of Condition 3 generally implies that the
limiting distribution is truncated on its left to ac-
count for the fact that the maximum likelihood esti-
mate can only fall on one side of the true parameter
value. The failure of Condition 1 in addition im-
plies that there is no value to which the maximum
likelihood estimator of the indeterminate parame-
ters can converge.

5.1.1 General results

The ﬁrst discussion of asymptotic theory for test-
ing homogeneity of model (5.1) when all parame-
ters are unknown was provided by Ghosh and Sen
(1985). As the two authors point out, there is
an additional major diﬃculty in dealing with ﬁ-
nite mixture models: though the mixture itself may
be identiﬁable, the parameters π, θ1 and θ2 may
not be. For instance, for the simple mixture where
f1(y; θ) = f2(y; θ) = f (y; θ), the equality

(1 − π)f (y; θ1) + πf (y; θ2)
= (1 − π(cid:48))f (y; θ(cid:48)

1) + π(cid:48)f (y; θ(cid:48)
2)

1, θ2 = θ(cid:48)

holds for π = π(cid:48), θ1 = θ(cid:48)
2, but also for
2, θ2 = θ(cid:48)
1 − π = π(cid:48), θ1 = θ(cid:48)
1. That is, if the al-
ternative hypothesis is true, there is a second set
of parameters which gives rise to the same distri-
bution. Furthermore, under the null hypothesis of
homogeneity the model is represented by the three
curves π = 1, π = 0 and θ1 = θ2. As illustrated
by Ghosh and Sen (1985), choosing an identiﬁable
parametrisation doesn’t bring any improvement as
the density is then no longer diﬀerentiable.

The ﬁrst result derived by Ghosh and Sen (1985)
characterises the limiting distribution of the likeli-
hood ratio statistic for strongly identiﬁable contin-
uous mixtures. Write f (y; θ) = (1 − π)f1(y; θ1) +

15

1 and θ2 = θ(cid:48)

πf2(y; θ2) with the convention that θ = (π, θ1, θ2).
Strong identiﬁability holds if f (y; θ) = f (y; θ(cid:48)) im-
plies that π = π(cid:48), θ1 = θ(cid:48)
2. Ghosh and
Sen (1985) furthermore assume that Θ2 is a closed
bounded interval of R, while Θ1 ⊆ Rp1, p1 ≥ 1. The
distribution of the likelihood ratio statistic for test-
ing H0 : π = 0 then converges to the distribution
of T 2I{T >0}, where T = supθ2{Z(θ2)} and Z(θ2)
is a zero-mean Gaussian process on Θ2 whose co-
variance function depends on the true value of the
parameters under the null hypothesis (Ghosh and
Sen, 1985, Theorem 2.1). This results from pro-
ceeding in two steps. We ﬁrst approximate the log-
likelihood function by a quadratic expansion with
respect to π and θ1 which, under the null hypoth-
esis, converges to the square of a Gaussian random
process indexed by the non-identiﬁable parameter
θ2. The supremum of this process with respect to
θ2 is then taken. The sketch of this proof is given
in Appendix A.3

A similar result holds if the ﬁnite mixture is
not strongly identiﬁable, such as when f1(y; θ) =
f2(y; θ) in (5.1). In this case, a separation condition
between θ1 and θ2 of the form ||θ1 − θ2|| ≥ (cid:15) for a
ﬁxed quantity (cid:15) > 0 needs be imposed, so that H0 is
described by either π = 0 or π = 1 (Ghosh and Sen,
1985, §5). The two authors furthermore restrict the
parameter space of π to [0, 0.5] and impose again
that Θ1 be an open set containing the true value θ0
1
and Θ2 be a closed set such that Θ1∩Θ2 = ∅. These
additional conditions guarantee that the maximum
likelihood estimate (ˆπ, ˆθ1) will fall with high proba-
bility into the n−1/2–neighbourhood of (0, θ0
1). The
proof outlined in Appendix A.3 still applies with
the exception that now the non-identiﬁable param-
eter θ2 varies in a subset of Θ2 which depends on
the given (cid:15). Ghosh and Sen (1985, §4) also discuss
the link to Bayesian testing and develop asymptot-
ically locally minimax tests for some special cases.

5.1.2 Assessing the number of components

Consider now the general K-component mixture
model

K
(cid:88)

k=1

πkfk(y; θk), K ≥ 2,

where fk(y; θk) are probability density or mass
functions indexed by θk ∈ Θk ⊆ Rpk and 0 ≤ πk ≤
1, k = 1, . . . , K, with (cid:80)K
k=1 πk = 1. Developing a

formal test for the null hypothesis H0 : K = K0
against the alternative that the mixture includes
K > K0 components is a diﬃcult task. Many
routes have been taken, including Wald-type statis-
tics derived from moment or alternative estimators,
adaptation of model selection techniques and the
use of simulation. Oliveira-Brochado and Martins
(2005) give a partial review of these techniques. A
comprehensive survey of the large-sample proper-
ties of some classical test statistics and recently
developed modiﬁed likelihood ratio and EM-test
statistics for assessing the order of a ﬁnite mixture
model is given in Chen (2017b). Further contribu-
tions are mentioned in the annotated bibliography
available as Supplementary Material.

5.2 Alternative approaches

Removing the separation condition of Section 5.1 is
challenging. Several authors have addressed this is-
sue. As we will see in Section 5.2.1, some succeeded
by reparametrizing the probability density or mass
function. Others add a penalty to the likelihood
to ensure identiﬁability or rely on simulation (Sec-
tions 5.2.2 and 5.2.3). Reparametrization does not
change the model, and the limiting distribution is
still the supremum of a Gaussian process. Con-
versely, regularising the likelihood function does
change the problem at hand, so do the limiting
distributions typically involve a chi-squared or a
mixture of chi-squared distributions.

5.2.1 Reparametrization

The ﬁrst contribution which, to our knowledge,
uses ad hoc reparametrization in place of a separa-
tion condition between the parameters θ1 and θ2 to
derive the limiting distribution of the likelihood ra-
tio statistic for testing model homogeneity, is Cher-
noﬀ and Lander (1995). The two authors study sev-
eral versions of the two-component binomial mix-
ture model, which is typically used in linkage analy-
sis. They heuristically prove that the ﬁnite-sample
null distribution of the likelihood ratio statistic
again converges to the supremum of the square of
a left-truncated zero-mean unit-variance Gaussian
process with well-behaved covariance function. The
formal proof is given in Lemdani and Pons (1997)
for several classical models. Later, Lemdani and
Pons (1999) study the limiting distribution of the

16

likelihood ratio statistic to test whether a known
density f (y; θ0) is contaminated by another den-
sity of the same parametric family.
In particu-
lar, the null hypothesis corresponds to assuming
f 0 = f (y; θ0) while under the alternative hypothe-
sis the model becomes (1−π)f (y; θ0)+πf (y; θ). By
reparametrizing to µ = π||θ − θ0||, they express the
null hypothesis as H0 : µ = 0, that is, as a function
of the single parameter µ, and avoid any separation
condition on the parameters θ0 and θ. The likeli-
hood ratio statistic is again shown to converge to
the distribution of the supremum of a squared left-
truncated Gaussian process. The result is extended
to the case where a mixture of K0 known densities
is contaminated by additional K1 ones of the same
family.

Testing for homogeneity of the two-component
mixture model (5.1) is furthermore considered in
Ciuperca (2002) who assumes that f1(y; θ) is an
exponential density and f2(y; θ, τ ) = f1(y − τ ; θ) is
a translation of the same by an unknown amount τ
belonging to a compact set. Here, the limiting dis-
tribution of the likelihood ratio statistic is shown to
converge to a ﬁfty-ﬁfty mixture of a point mass at
zero and of a distribution which diverges in prob-
ability to +∞, and this despite the fact that all
parameters are assumed to belong to a compact
set. This shows that Condition 3 of Section 2.2
is necessary, but not suﬃcient. The unbounded-
ness behaviour of the likelihood ratio of Ciuperca
(2002) can be explained by means of the theory
of “locally conic” reparametrizations proposed by
Dacunha-Castelle and Gassiat (1997, 1999) whose
treatment goes beyond the scope of this paper.

5.2.2 Penalisation

A rather diﬀerent route is taken in Chen et
al. (2001). To overcome the two diﬃculties of
asymptotic theory for mixture models—the bound-
ary problem and non-identiﬁability under the
null hypothesis—they suggest to penalise the log-
likelihood function

l(π, θ; y) + c log{4π(1 − π)},

(5.2)

where the degree of penalisation is controlled by the
constant term c. As the authors point out, the pe-
nalisation term can be justiﬁed from the Bayesian
perspective, as a prior on the mixing proportion π.

It furthermore guarantees that the maximum like-
lihood estimate of the mixing proportion 0 < ˆπ < 1
will not fall on the boundary of the parameter space
and that the maximum likelihood estimators of all
parameters are consistent under the null hypoth-
esis π = 0. Provided Conditions 1–5 of their pa-
per hold, the distribution of the modiﬁed likeli-
hood ratio statistic derived from (5.2) converges
to a ¯χ2(ω, 1) distribution with ω = (0.5, 0.5) in-
stead of the unquestioned supremum of a squared
truncated Gaussian random process. Numerical as-
sessment for Poisson and Gaussian mixtures reveals
that their proposal competes well with alternative
solutions especially with respect to power.

Chen et al. (2008) derive the asymptotic null dis-
tribution of the modiﬁed likelihood ratio test intro-
duced in Chen et al. (2001) and of a further modi-
ﬁcation, called the iterative modiﬁed likelihood ra-
tio test, for testing model homogeneity against the
alternative that the model is a two-component von
Mises mixture with unknown mean directions with-
out and with nuisance parameters. A further ex-
ample of penalisation for von Mises mixtures is Fu
et al. (2008); see Example 1.3. Both papers out-
line how to improve the accuracy of the asymptotic
approximation in ﬁnite samples.

5.2.3 Simulation

A third route to investigate the asymptotic null
distribution of the likelihood ratio statistic for ﬁ-
nite mixture models is by simulation. Thode et
al. (1988) consider testing the hypothesis that the
sample comes from a normal random variable with
unknown mean and unknown variance against the
alternative that the sample comes from a two-
component Gaussian mixture with unequal means
and common variances. All model parameters are
assumed to be unknown. Their extensive numeri-
cal investigation shows that the distribution of the
likelihood ratio statistic converges very slowly to a
limiting distribution, if any exists, and is rather un-
stable even for sample sizes as large as n = 1, 000.
For very large sample sizes, the empirical distribu-
tions rather closely agree with the commonly as-
sumed χ2
2, though this may be too liberal for small
to moderate n. This gives little support to Harti-
gan’s (1977) conjecture that the asymptotic distri-
bution may lie between a χ2
2. An exam-
ple of application to a study of population genet-

1 and a χ2

17

ics is given, motivated by the fact that these stud-
ies are typically of small to moderate sample sizes,
which justiﬁes the use of empirical approximations.
The distribution of the likelihood ratio under the
alternative hypothesis of a two-component Gaus-
sian mixture with unequal means and common vari-
ances is investigated numerically in Mendell et al.
(1991) for a wide range of mixing proportions π.
The authors conjecture that the limiting distribu-
tion is a non-central χ2

2 distribution.

B¨ohning et al. (1994) investigate numerically the
asymptotic properties of the likelihood ratio statis-
tic for testing homogeneity in the two-component
mixture model (5.1) when the component distri-
butions fk(y; θk), k = 1, 2 are binomial, Poisson,
exponential or Gaussian with known common vari-
ance. They establish that, for suﬃciently large
sample sizes, the null distribution is well approx-
imated by a ¯χ2(ω, 1) which remains stable across
the possible range of values for the parameters θ1
and θ2, but is model-speciﬁc in the sense that the
weights ω depend on the model under considera-
tion.
In case of the exponential distribution, the
distribution of the likelihood ratio statistic con-
verges to a proper distribution, which may seem
to contradict Ciuperca (2002). Remember that
B¨ohning et al. (1994) consider a mixture of two
exponentials, while in Ciuperca (2002) the second
distribution is a translation of the ﬁrst one. This
leads to the additional failure of Condition 2, since
the support depends on the unknown translation
parameter τ . Chen and Chen (2001b) consider the
same setting than B¨ohning et al. (1994), though the
component distributions are allowed to belong to a
generic parametric family. They show that under
suitable conditions which guarantee identiﬁability
of the mixture and regularity of the component dis-
tributions fk(y; θk), the limiting distribution of the
likelihood ratio is the distribution of the squared
supremum of a left-truncated standard Gaussian
process, whose autocorrelation function is explicitly
presented; see Sections 2 and 3 of their paper. Chen
and Chen (2001b) recommend using resampling to
calculate the desired tail probabilities. The proce-
dure is illustrated for normal, binomial and Poisson
models. Lo (2008) shows that the commonly used
χ2 approximation for testing the null hypothesis of
a homoscedastic normal mixture against the alter-
native that the data arise from a heteroscedastic
model is reasonable only for samples as large as

n = 2, 000 and component distributions that are
well separated under the alternative. Furthermore,
the restrictions of Hathaway (1985) need be im-
posed to ensure that the likelihood is bounded and
to rule out spurious maxima under the alternative.
Otherwise, the author suggests use of parametric
resampling. Very recently, Cong and Yao (2021)
study the behaviour of the likelihood ratio statis-
tics for multivariate normal mixtures. They rec-
ommend using parametric boostrap resampling as,
similarly to Lo (2008), the ﬁnite-sample distribu-
tion is well approximated by the commonly used χ2
distribution only for suﬃciently large sample sizes,
say n > 2, 000.

5.3 Gaussian mixtures

Theoretical results are particularly generous if the
two-component model is a normal mixture. This
is justiﬁed not only by the widespread use of the
normal distribution in a wide variety of situations,
but also by the technical challenges posed by some
undesirable properties of this type of models, which
add to the two common failures of regularity of ﬁ-
nite mixtures, that is, parameter non-identiﬁability
and testing on the boundary. As discussed in Chen
and Li (2009), the expected Fisher information for
the mixing proportion is not ﬁnite unless a bound
is placed on the variance of the corresponding com-
ponent density. Furthermore, the derivatives of
the log-density become linearly dependent since
∂2φ(x; µ, σ)/∂µ2 = 2∂φ(x; µ, σ)/∂σ2. Last, but not
least, the log-likelihood function is unbounded in
case of heterogeneous components, and the maxi-
mum likelihood estimator may not exist (Hartigan,
1985). These deﬁciencies generally invalidate the
standard quadratic approximation of the likelihood
ratio function, which needs be expanded further.
This is why most contributions on normal mix-
tures assume homogeneous variances. Several solu-
tions have been proposed to overcome these short-
comings; see among others Chen and Li (2009);
Kasahara and Shimotsu (2015); Wichitchan et al.
(2019). Here, we review the results available for the
likelihood ratio statistic assuming homogeneity of
variances. Goﬃnet et al. (1992) consider an i.i.d.
sample from a d-dimensional random variable with
density function

(1 − π)φd(y; µ1, Σ) + πφd(y; µ2, Σ),

18

with 0 ≤ π ≤ 1 and φd(y; µ, Σ) the d-dimensional
normal density with mean µ ∈ Rd and covariance
matrix Σ. They derive the asymptotic distribution
of the likelihood ratio statistic for testing the null
hypothesis of homogeneity of the means, that is,
H0 : µ1 = µ2, with known mixing proportion π.
Their Theorem 1 treats the univariate case, while
its bivariate extension is given in their Theorem 2.
For d = 1 the null distribution of the likelihood
ratio converges to a χ2
1 distribution if Σ is unknown
and π (cid:54)= 0.5.
In all other scenarios, it converges
to a ¯χ2(ω, 1) distribution with ω = (0.5, 0.5). The
convergence rate depends on the mixing proportion
π and is particularly slow if π is close to 0.5.

If d = 2 the limiting distribution of the likelihood

ratio for known Σ is the distribution of

{sup (0, T )}2,

T = Z +

√

W ,

1
2

where Z is the standard normal and W is an inde-
pendent χ2
2 random variable. This corresponds to
a ﬁfty-ﬁfty mixture of a point mass at zero and the
squared sum of a standard normal plus the square
root of an independent χ2
2. No result is given for
d = 2 and Σ unknown.

Chen and Chen (2001a) consider the slightly dif-

ferent univariate setting

(1 − π)φ(y; µ1, 1) + πφ(y; µ2, 1),

(5.3)

where φ(y; µ, 1) is the univariate normal density
with unit variance and mean µ ∈ R. The mixing
proportion π is unknown and the two means lie in
an interval [−M, M ] for M ﬁnite. Chen and Chen
(2001a) consider two cases: where only µ2 is un-
known and µ1 = 0, or where both location param-
eters are unknown. In both cases the asymptotic
null distribution of the likelihood ratio statistics for
testing homogeneity involves the distribution of the
supremum of a squared Gaussian random process.
If both means are unknown, π ≤ 0.5 to ensure iden-
tiﬁability and we want to test µ1 = µ2 = 0, the lim-
iting distribution agrees with the distribution of

(cid:8) sup

Z(t)(cid:9)2

+ W,

|t|≤M

(5.4)

where Z(t), t ∈ [−M, M ], is a Gaussian process and
W is an independent chi-squared random variable
with one degree of freedom. The Gaussian process

Z(t) has zero mean and covariance function (Chen
and Chen, 2001a, Theorem 3)

Cov{Z(s), Z(t)} =

est − 1 − st
(cid:112)(es2 − 1 − s2)(et2 − 1 − t2)

,

for st (cid:54)= 0, and Cov{Z(s), Z(t)} = 0 when st = 0.
If instead we want to test the composite hypothesis
µ1 = µ2 or the simple hypothesis µ2 = 0 with the
assumption that µ1 = 0, (5.4) stil holds but the
chi-squared term is absent and the expression of
the covariance is slightly diﬀerent; see Chen and
Chen (2001a, Theorems 2 and 4).

As mentioned in Section 2.2, the compactness
of the parameter space is a necessary condition
to avoid that the distribution of the likelihood ra-
tio statistic diverges to inﬁnity. This was already
proved by Hartigan (1985) and is an immediate im-
plication of Theorem 2 by Chen and Chen (2001a)
as {sup|t|≤M Z(t)}2 tends in probability to inﬁnity
if M → ∞. For the latter proof, see Chernoﬀ and
Lander (1995, Section 5.6 and Appendix D).

The generalization to the two-component mix-

ture model

(1 − π)φ(y; µ1, σ2) + πφ(y; µ2, σ2),

(5.5)

which now includes an unknown variance parame-
ter σ2 > 0, can be found in Chen and Chen (2003).
They prove that the asymptotic distribution of the
likelihood ratio for testing model homogeneity is
the distribution of the sum of a χ2
2 variable and
the supremum of the square of a left-truncated
Gaussian process with zero mean and unit variance.
Again, the correlation structure of the process in-
volved in the limiting distribution is presented ex-
plicitly; see their Theorem 2.

The proofs of the Theorems in Chen and Chen
(2001a, 2003) essentially are suitable adaptations
of the prototype derivation for ﬁnite mixture mod-
els reported in Appendix A.3. All passages are de-
tailed in the original contributions to which we refer
the interested reader. As in most cases the asymp-
totic distribution of the likelihood ratio is related to
a Gaussian random ﬁeld, the computation of per-
centile points becomes tricky or impossible. That
is why other tests or methods have been proposed.
Reviewing all these would go beyond the scope of
the paper. Let us mention, here, the most fruit-
ful research strained initiated by Li et al. (2009)
who propose an EM-test for homogeneity, which

19

Chen and Li (2009) decline in the case of a two-
component Gaussian mixture. A most recent treat-
ment is Chauveau et al. (2018).

6 Change-point problems

A change-point problem arises when we seek to
identify a possible change in the probability distri-
bution of a univariate or multivariate random se-
quence, in a series of time-dependent observations
or in a sample of responses whose regime may sud-
denly change. A modiﬁcation in the data generat-
ing process generally implies that the log-likelihood
function is no longer diﬀerentiable with respect to
some values of the parameter. This typically leads
to the failure of Condition 4 of Section 2.2. Further-
more, setting one of the components of the model
to a particular value, can make other components,
or parts of it, disappear, as in Example 4.1, which
links change-point problems to indeterminate pa-
rameter problems.

The, to our knowledge, ﬁrst treatment of a
change-point problem is Page (1955). Since then,
change-point problems have been the subject of
intensive research owing to their widespread use
whenever the constancy over time of random events
is questioned. The theory has evolved over the past
ﬁve decades to the extent that summarizing all con-
tributions would ﬁll in book-length accounts. A
ﬁrst annotated bibliography of change-point prob-
lems is Shaban (1980). Krishnaiah and Miao (1988)
give an overview of change-point estimation up to
their time of writing; Cs¨org¨o and Horv´ath (1997)
focus their review monograph on limit theorems
for change-point analysis. Khodadadi and Asghar-
ian (2008) is a more than 200 pages length anno-
tated bibliography of change-point problems in re-
gression. Lee (2010) summarizes the most recent
literature and gives a comprehensive bibliography
for the ﬁve types of change-point problems char-
acterised by a shift in the mean, a change in the
variance, a switch in the regression slope, a change
in the hazard rate or a change in the distribution.
A book-length account of change-point problems
with examples from medicine, genetics and ﬁnance
is Chen and Gupta (2012). The discussion papers
of Horv´ath and Rice (2014a,b) mention, in addition
to classical methods, also modern lines of research
in functional data and high dimensions. Niu et

al. (2016) provide a selected overview of multiple
change-point detection. The proposed inferential
solutions range from parametric to nonparamet-
ric techniques and include frequentist and Bayesian
approaches. Most recently, Sofronov et al. (2020)
edited a special issue of the journal Statistical Pa-
pers dedicated to change-point detection.

Generally speaking, two questions are of interest
in change-point analysis: identifying the potential
number of changes, and, once identiﬁed, estimating
where these occur, together with further quantities
of interest such as the size of the change. In the re-
mainder of this section we will focus our attention
on the ﬁrst problem, that is, the identiﬁcation of a
change by means of the likelihood ratio statistic. As
highlighted by Chen and Gupta (2012), the major-
ity of reference models which have been proposed
for change-point detection assume normality of the
observations. These will be treated extensively in
Sections 6.2 and 6.3 with special emphasis on re-
gression type problems. In particular, Section 6.2
addresses the issue of detecting possible shifts in the
location and/or the scale of the distribution. Sec-
tions 6.3 extends the treatment to linear regression
and piecewise linear models. Section 6.1 discusses
the most basic problem of change-point detection
in a random sequence of discrete or continuous ob-
servations. Given the breadth of the available solu-
tions, each section contains a selection of contribu-
tions which illustrate the main currents of research.
Further related work is listed in the annotated bib-
liography available as Supplementary Material.

6.1 Changes in random sequences

The most basic change-point problem tries and
identiﬁes patterns in a random sequence. Several
authors applied the likelihood ratio statistic to this
type of problem. Among the earliest contributions
is Page (1957). Given n independent observations
y1, . . . , yn, listed in the order they occurred, Page
(1957) considers the problem of verifying whether
these were generated by a random variable with
distribution function F (y; θ) against the alterna-
tive that only the ﬁrst τ , 0 ≤ τ < n, observa-
tions are generated from F (y; θ) while the remain-
ing n − τ come from F (y; θ(cid:48)) with θ (cid:54)= θ(cid:48) and τ
unknown. Most results are for continuous random
variables of exponential family type, though some
results are available for the binomial and Poisson

20

cases. The asymptotic distribution of the likelihood
ratio statistic is generally found, as we will see in
Section 6.2, by splitting the observations before and
after the change-point τ . The remainder of the sec-
tion illustrates some revealing examples where the
test statistics can also be unbounded. Further re-
lated work is listed in the annotated bibliography
available as Supplementary Material.

Worsley (1983) derives the exact null and alter-
native distributions of the likelihood ratio statistic
and of the cumulative sum (cumsum) statistic to
detect a possible change in the probability of se-
quence of independent binomial random variables.
These distributions are obtained by conditioning on
the total number of successes and using an iterative
procedure similar to the one developed by Hawkins
(1977), which we will see in Section 6.2. Numeri-
cal investigation indicates that the likelihood ratio
test is more powerful than the cumsum test if the
change occurs at the beginning or towards the end
of the sequence, while it is slightly less powerful if
the change occurs in the middle of the same. How-
ever, the likelihood ratio statistic is not bounded in
probability.

Worsley (1986) extends his previous results to
test for a change in the mean value of indepen-
dent observations from an exponential family, with
particular emphasis on the exponential distribu-
tion. The exact null and alternative distributions of
the likelihood-based statistics are found, and their
power is compared with a test based on a linear
trend statistic. The likelihood ratio is a function of
both, of the sample mean ¯Y = (cid:80)n
i=1 Yi/n and of
the partial means

¯Yτ =

1
τ

τ
(cid:88)

i=1

Yi,

¯Yn−τ =

1
(n − τ )

n
(cid:88)

i=τ +1

Yi.

(6.1)

These represent the suﬃcient statistics for the nat-
ural parameter θ which indexes the exponential
family under the null hypothesis of no change and
of the the natural parameters θ1 and θ2, which in-
dex the two distributions under the hypothesis that
a change occurred at τ . An exact conﬁdence region
for the change-point τ is also derived.

Gombay and Horv´ath (1994b) derive the limiting
distribution of the likelihood ratio type statistic for
testing whether there is a change in the parameter
θ which indexes a general distribution f (y; θ); this
can be seen as the continuation of Page (1957).

21

Given f (y; θ), since the change location τ is un-
known, the likelihood ratio statistic

W = max

Wτ

1≤τ ≤n−1

is deﬁned as a maximum over τ of

Wτ = −2 log






sup
θ∈Θ

(cid:81)n

sup
θ∈Θ0

i=1 f (yi; θ)
(cid:81)n

(cid:81)τ

i=1 f (yi; θ) sup
θ∈Θ

i=τ +1 f (yi; θ)






.

Using results of extreme value theory, Gombay and
Horv´ath (1994b) prove that if suitably centered and
rescaled, W converges to a Gumbel distribution un-
der the null hypothesis of no change. Gombay and
Horv´ath (1996) generalise to the case where a ﬁxed
nuisance parameter is present.

The detection of a change-point in a series of
dependent observations is typical of longitudinal
data analysis. A review of how to identify struc-
tural changes in time series is Aue and Horv´ath
(2013). Also Robbins et al. (2011, 2016) address
the problem of change-point detection in time se-
ries. The former considers the mean-shift model of
Section 6.2, while the latter assumes the linear re-
gression model of Section 6.3 and the extension to
multi-phase regression, for which we give references
in the annotated bibliography.

6.2 Shifts in location and scale

The reference model for testing a change in the
mean value of a random variable can generally be
written as

yi = ηi + εi,

i = 1, . . . , n,

(6.2)

where the εi’s are independent zero-mean random
errors. All observations are considered in the or-
der they appear, an assumption which will hold for
the whole section. The function ηi may change K
times,

0 < i ≤ τ1,
τ1 < i ≤ τ2,

(6.3)

ηi = µ1,
= µ2,
...
= µK+1,

τK < i ≤ n,

where the change-points τk can only assume integer
values. Unless diﬀerently stated, both the K+1 dif-
ferent mean values µk and the K change-points τk

are supposed to be unknown, though the very early
contributions focus on the simpler setting where
one or both pieces of information are given.

where φ(u) is the density of the standard normal,
g1(u) = 1 for u ≥ 0 and gτ (u) is a recursive function
such that

Assuming K = 1, Hawkins (1977) considers test-
ing the null hypothesis of no change in the mean ηi
when the εi ∼ N (0, σ2) are centered normal vari-
ables with constant variance σ2 > 0, that is,

H0 : Yi ∼ N (µ, σ2),

i = 1, . . . , n,

against the alternative that there exists a 0 < τ < n
at which the unknown mean switches from µ to
µ(cid:48) (cid:54)= µ. The variance σ2 is assumed to be known
and we set it to one without loss of generality.
This is a non-standard problem because the change-
point appears only under the alternative hypoth-
esis, but not under the null. The corresponding
likelihood ratio statistic is a function of

Wτ = τ ( ¯Yτ − ¯Y )2 + (n − τ )( ¯Yn−τ − ¯Y )2,

where ¯Yτ and ¯Yn−τ are the partial means, given
at (6.1). Since the change-point τ is unknown, the
likelihood ratio

W = Wτ ∗ = max
1≤τ <n

Wτ

maximises Wτ over all possible values of τ , and
is usually refereed to a “maximally selected likeli-
hood ratio”. To derive its exact null distribution,
Hawkins (1977) re-expresses Wτ as

Wτ = T 2
τ ,

where

Tτ =

(cid:114) n

τ (n − τ )

τ
(cid:88)

i=1

(Yi − ¯Y )

(6.4)

has standard normal distribution. It follows that
the ﬁnite-sample distribution of

(cid:112)

U =

Wτ ∗ = max
1≤τ <n

|Tτ |

(6.5)

agrees with the distribution of the maximum ab-
solute value attained by a Gaussian process in dis-
crete time having zero mean, unit variance and au-
tocorrelation function given by Expression (3.2) of
Hawkins (1977). In particular, the null distribution
of U has density function

gτ (u) = Pr(|Ti| < u, i = 1, . . . , τ − 1 | |Tτ | = u).

(6.7)
The sketch of the proof of (6.6) is given in Ap-
pendix A.4.

To avoid the cumbersome calculation of the ex-
act distribution, Yao and Davis (1986) derive the
asymptotic null distribution of U using results from
the theory of Brownian processes. Equation (6.5)
is rewritten as

U = max
1≤τ <n

(cid:1) (cid:12)
(cid:12)

(cid:12)
(cid:12) τ√
n
(cid:113) τ
n

(cid:0) ¯Yτ − ¯Yn
(cid:1)
(cid:0)1 − τ

n

.

Let {B(t); 0 ≤ t < ∞} be a standard Brownian
motion. Under H0 the process

(cid:26) τ ( ¯Yτ − µ)
√

n

(cid:27)

; 1 ≤ τ ≤ n

distributes like {B (τ /n) ; 1 ≤ τ ≤ n}. We can
hence rewrite U as

U = max

1≤nt<n

(cid:12)B0(t)(cid:12)
(cid:12)
(cid:12)
(cid:112)t(1 − t)

,

(6.8)

where t = τ /n and B0(t) = B(t) − tB(1) is a Brow-
nian bridge. The exact critical values provided by
(6.8) are not easy to calculate. A suitably normal-
ized version of U converges under H0 to the dou-
ble exponential, or Gumbel, distribution (Yao and
Davis, 1986, Theorem 2.1), which provides approx-
imate quantiles. See also Irvine (1986). However,
it is well known from extreme value theory that
the distribution of the maximum of a Gaussian se-
quence converges slowly to the double exponential
distribution (Jaruˇskov´a, 1997; Cs¨org¨o and Horv´ath,
1997).

If the change occurs at the very beginning, or
at the very end, of the sequence, there may not
be enough observations to estimate all parameters
or the maximum likelihood estimates may not be
unique. If so, we may consider a trimmed version
of the test statistic,

max
τ0≤τ ≤n−τ0

Wτ , with τ0 > 1,

fU (u) = 2φ(u)

n−1
(cid:88)

τ =1

gτ (u)gn−τ (u),

(6.6)

as, for example, done by Zou et al. (2007). Cs¨org¨o
and Horv´ath (1997, Theorem A.3.4) provide gen-
eral conditions on τ0, which guarantee that under

22

H0 the likelihood ratio converges asymptotically
to the Gumbel distribution. This way of proceed-
ing, however, has been criticized by some authors
since it entails an arbitrary choice of the parameter
τ0; see for example Jaruˇskov´a (1997) and reference
therein.

The ﬁnite-sample null distribution of the likeli-
hood ratio statistic for unknown σ2 was worked out
by Worsley (1979). The likelihood ratio statistic

W = max
1≤τ <n

(n − 2)

1
2

|Tτ |
Sτ

,

(6.9)

where Sτ is the square root of

is used for change-point detection in simple lin-
ear regression. The early contributions by Quandt
(1958, 1960) derive the likelihood ratio statistic un-
der the null hypothesis of no switch against the
alternative that the model possibly obeys two sep-
arate regimes under the assumption of independent
and zero-mean normal error terms εi. Under the al-
ternative hypothesis, the variance is furthermore al-
1 to σ2
lowed to switch from σ2
2 at instant τ , when the
linear predictor ηi undergoes a structural change.
The likelihood ratio statistic is

W = max

3≤τ ≤n−3

Wτ ,

τ
(cid:88)

(Yi − ¯Yτ )2 +

n
(cid:88)

S2

τ =

(Yi − ¯Yn−τ )2,

i=1

i=τ +1

with

Wτ = −2 log

(cid:32)

1 ˆσ2(n−τ )
ˆσ2τ
2
ˆσ2n

(cid:33)

,

that is, of the within-group sum of squares of the
observations split at τ . Under H0,

(n − 2)

1
2

Tτ
Sτ

distributes like a t distribution with n−2 degrees of
freedom. Tail probabilities for (6.9) are calculated
by numerical integration for sample sizes n ≤ 10
and using simulation if 10 < n ≤ 50. Approxi-
mate critical values for (6.9) are obtained by con-
sidering Bonferroni-type inequalities. For large n,
percentage points can be calculated also by using
Hawkins’s (1977) recursion rule. Further general-
izations, such as to the multivariate case and/or to
account for a possible change in the scale of the
distribution, can be found in the book length ac-
count of Chen and Gupta (2012, §§2.2–2.3 and 3.2–
3.3). See also the selection of references given in the
annotated bibliography available as Supplementary
Material.

6.3 Change-point detection in re-

gression

A further extension of Model (6.3) with respect to
location,

0 < i ≤ τ1,
τ1 < i ≤ τ2,

ηi = α1 + β1xi,
= α2 + β2xi,
...
= αK+1 + βK+1xi,

(6.10)

1 and σ2

is a function of the least squares estimators ˆσ2
1 and
ˆσ2
2 of σ2
2, respectively, computed using the
corresponding subsets of observations, and of the
MLE ˆσ2 of the common variance σ2 = σ2
1 = σ2
2
based upon the entire sample. Quandt (1958) ini-
tially conjectured that the asymptotic distribution
of W may be χ2
4 under the null hypothesis of no
change. However, the numerical investigation he
reported in a later publication for the three sample
sizes n = 20, 40, 60 (Quandt, 1960, Table 3) re-
vealed that the ﬁnite-sample distribution depends
on the number of observations n.

Change-point detection in simple linear regres-
sion using the likelihood ratio is also the subject of
Kim and Siegmund (1989). These authors consider
two situations: where only the intercept is allowed
to change and where both, the intercept and the
slope change. The variance remains constant. Un-
der the ﬁrst scenario, we reject the null hypothesis
of no change for large values of maxt1≤τ ≤t2 |U (τ )|,
with 0 < t1 < t2 < 1, where
(cid:19)1/2 


U (τ ) =

 .



(cid:18) nτ
n − τ

1
ˆσ

¯Yτ − ˆα − ˆβ ¯xτ
1 − nτ
n−τ

(¯xτ −¯x)2
ˆσ2
x

(cid:113)

Here, ˆσ2 is again the maximum likelihood estimator
of the common variance σ2, and ˆσ2
x is the sample
variance of (x1, . . . , xn). Furthermore, (ˆα, ˆβ) are
the maximum likelihood estimators of (α, β). The
statistic U (τ ) is a weighted function of the corre-
sponding likelihood ratio statistic with weight func-

(cid:113)

1 − nτ
n−τ

(¯xτ −¯x)2
ˆσ2
x

, in order

τK < i ≤ n,

tion given by ω(τ ) =

23

to guarantee a valid limiting distribution. A sim-
ilar result is derived for the second scenario. The
null distribution of the likelihood ratio statistics is
shown to depend on the independent variable x.
Again, the Brownian Bridge process is central to
the derivation of the corresponding limiting dis-
tributions as in Yao and Davis (1986). Approxi-
mations for the corresponding tail probabilities are
given by Kim and Siegmund (1989) under reason-
ably general assumptions. The maximum is taken
over the range 3 ≤ τ ≤ n − 3 to improve power,
which, however, requires that the unknown change-
point τ occurs in the middle of the sequence. Gom-
bay and Horv´ath (1994a) consider the same prob-
lem and let τ vary from 1 to n−1 both, in a random
and in non random designs context.

Model (6.10) can be extended to account for
changes in the covariates, which is known as piece-
wise linear regression. This type of models are very
popular in a large number of disciplines, including
among others environmental sciences (Piegorsch
and Bailer, 1997, Section 2.2; Muggeo, 2008a),
medical sciences (Smith and Cook, 1980; Muggeo
et al., 2014), epidemiology (Ulm, 1991) and econo-
metrics (Zeileis, 2006). A review of likelihood ratio
testing for piecewise linear regression up to his time
of writing can be found in Bhattacharya (1994).
See also the annotate bibliography for a selection
of related contributions.

7 Beyond parametric

infer-

ence

This section reviews cases of interest which do not
ﬁt into the previously mentioned three broad model
classes, but still fall under the big umbrella of non-
standard problems. In particular we will focus on
shape constrained inference, a genre of nonpara-
metric problem which leads to highly nonregular
models.

As brought to our attention by an anonymous
Referee, the asymptotic theory of semiparametric
and nonparametric inference has interesting ana-
logues to the classical parametric likelihood the-
Indeed, the parame-
ory reviewed in Section 2.
ter space of a semiparametric model is an inﬁnite-
dimensional metric space. This makes the model
non-standard as we typically consider a real pa-

rameter of interest in the presence of an inﬁnitely
large nuisance parameter. Despite this departure
from regularity, the likelihood ratio statistic still
behaves as we would expect it. Murphy and van
der Vaart (1997, 2000), for instance, show that the
corresponding limiting distribution is chi-squared
also when we proﬁle out the inﬁnite-dimensional
nuisance parameter. The classical approximations
of Section 2 also hold for the asymptotic theory of
empirical likelihood (Owen, 1990, 1991); see Chen
and Van Keilegom (2009) for a review. These re-
sults are quite remarkable given that the underlying
distributional assumptions are much less strict.

An area of research which has received much at-
tention in the last decade is nonparametric infer-
ence under shape constraints (Samworth and Bod-
hisattva, 2018). Shape constraints originate as a
natural modelling assumption and lead to highly
nonregular models. As highlighted by Groeneboom
and Jongbloed (2018), the probability density or
mass functions of many of the widely used paramet-
ric models satisfy shape constraints. For example,
the exponential density is decreasing, the Gaussian
density is unimodal, while the Gamma density can
be both, depending on whether its shape parame-
ter is smaller or larger than one. Estimation under
shape constraints leads to an M-estimation prob-
lem where the parameter vector typically has the
same length as the sample size and is constrained to
lie in a convex cone. Non-regularity arises since the
M-estimator typically falls on the face of the cone.
As for boundary problems, convex geometry is an
essential tool to treat shape constrained problems.
The ﬁeld of shape constraint problems originated
from ‘monotone’ estimation problems, where func-
tions are estimated under the condition that they
are monotone. The maximum likelihood estimator
converges typically at the rate n−1/3 if reasonable
conditions hold, that is, at a slower pace than the
n−1/2 rate attained by regular problems. More-
over, the maximum likelihood estimator has a non-
standard limiting distribution known as Chernoﬀ’s
distribution (Groeneboom and Wellner, 2001). A
considerable body of work has studied the asymp-
totic properties of the nonparametric likelihood
ratio statistic under monotonicity.
In particular,
Banerjee and Wellner (2001) initiated the research
strain of testing whether a monotone function ψ
assumes the particular value ψ(t0) = ψ0 at a ﬁxed
point t0. An extension to regression is given by

24

Banerjee (2007), who assumes that the conditional
distribution p(y, θ(x)), of the response variable Y
given the covariate X = x, belongs to a regular
parametric model, where the parameter θ, or part
of it, is speciﬁed by a monotone function θ(x) ∈ Θ
of x. Other types of shape constraint problems
have emerged in the meantime which entail concav-
ity or convexity and uni-modality of the functions
to be estimated; see the annotated bibliography
available as Supplementary Material. Many high-
dimensional problems fall in this framework, which
opens frontiers for research in nonregular settings;
see for example Bellec (2018). Most recently, Doss
and Wellner (2019) showed that the likelihood ra-
tio statistic is asymptotically pivotal if the density
is log-concave. The class of log-concave densities
has many attractive properties from a statistical
viewpoint; an account of the key aspects is given
in Samworth (2018). Non-standard limiting distri-
butions characterize shape constrained inferential
problems. Generally, the likelihood ratio statistic
converges to a limiting distribution which can be
described by a functional of a standard Brownian
motion plus a quadratic drift. In addition, the lim-
iting distribution is asymptotically pivotal, that is,
it doesn’t depend on the nuisance parameters, as
happens for the common χ2 distribution of regular
parametric problems.

8 Computational aspects and

software

Deriving the asymptotic distribution of the likeli-
hood ratio statistic under non-standard conditions
is generally a cumbersome task.
In some cases
the limiting distribution is well deﬁned and us-
able, as for instance when it boils down to a chi
or chi-bar squared distribution. Quite often, how-
ever, the analytical approximation is intractable,
so as when we have to determine the percentiles
of a Gaussian random ﬁeld. This fact has moti-
vated the development of alternative test statistics
whose null distribution presents itself in a more
manageable form; see, for instance, the contribu-
tions mentioned in Section 5.2.2. Or, we may rely
upon simulation, using Monte Carlo or the boot-
strap, as mentioned in passing in Sections 3.2, 4.1,
5.2.3 and 6.2. Bootstrapping, in particular, allows

us to recover the ﬁnite-sample null distribution of
the test statistic very naturally provided that the
bootstrap resamples from a consistently estimated
density (Titterington, 1990; Feng and McCulloch,
1996). In general terms, this requires that the max-
imum likelihood estimator converges to the possi-
bly non-identiﬁable subset of the parameter space
to which the true parameter belongs to. Bootstrap
likelihood ratio tests for ﬁnite-mixture models are
reviewed in Feng and McCulloch (1996). A most re-
cent application for boundary points is Cavaliere et
al. (2022), while Kirch (2008), Huˇskov´a and Kirch
(2012), and the very recent papers by Chen and
Cabrera (2020) and Yu and Chen (2022) boostrap
the critical values of change-point tests. Permuta-
tion is also used to derive the critical values for tests
statistics in change-point analysis; see for example
Kirch and Steinebach (2006) and reference therein.
A compromise between analytical approximation
and simulation is the hybrid approach described in
Brazzale et al. (2007, Section 7.7) where parts of
the analytical approximation are obtained by sim-
ulation. However, simulation becomes useless if the
limiting distribution diverges to inﬁnity, as already
mentioned in Example 1.3. A non exhaustive list
of examples is provided in Section 6.1 and in para-
graphs 5.2–5.3 of the annotated bibliography in the
Supplementary Material. Substantive applications
in which the approximations have been found use-
ful and details of how to implement the methods in
standard computing packages are generally miss-
ing. Reviewing all software contributions which
implement likelihood ratio based inference for non-
regular problems in a more or less formalized way is
beyond the scope of this paper. In the following we
try and give a selected list of packages for the nu-
merical computing environment R (R Core Team,
2020). We will again group them into the three
broad classes reviewed in the previous Sections 3–
6, that is, boundary problems, mixture models and
change-point problems.

Crainiceanu and Ruppert’s

(2004) proposal,
is
which tests for a null variance component,
implemented in the RLRsim package by Scheipl
et al. (2008). We furthermore mention the
varTestnlme package by Baey and Kuhn (2019)
and the lmeVarComp package by Zhang (2018). The
ﬁrst again tests for null variance components in lin-
ear and non linear mixed eﬀects model, while the
second implements the method proposed by Zhang

25

et al. (2016) for testing additivity in nonparametric
regression models.

An account of some early software implementa-
tions to handle mixture models can be found in
Haughton (1997), in the Appendix of McLachlan
and Peel (2000) and also in the Software section
of the recent review paper by McLachlan et al.
(2019). A most recent implementation for use in
astrostatistics is the TOHM package by Algeri and
van Dyk (2020) which implements a computation-
ally eﬃcient approximation of the likelihood ra-
tio statistic for a multimensional two-component
ﬁnite-mixture model. The package is also avail-
able for the Python programming language. The
code provided by Chauveau et al. (2018) for test-
ing a two-component Gaussian mixture versus the
null hypothesis of homogeneity using the EM test
is available through the MixtureInf package by
Li et al. (2016). Maximum likelihood estimation
in ﬁnite mixture models based on the EM algo-
rithm is furthermore addressed in the mixR pack-
age by Yu (2018), which also considers diﬀerent in-
formation criteria and bootstrap resampling. The
clustBootstrapLRT function of the mclust pack-
age by Scrucca et al. (2016) also implements boot-
strap inference for the likelihood ratio to test the
number of mixture components. A further imple-
mentation of the likelihood ratio test for mixture
models is the mixtools package by Benaglia et al.
(2009). All R packages linked to ﬁnite mixture
models are listed on the CRAN Task View webpage
for Cluster Analysis & Finite Mixture Models1.

The changepoint package by Killick and Eck-
ley (2014) considers a variety of test statistics for
detecting change-points among which the likeli-
hood ratio. The strucchange package by Zeileis et
al. (2002) provides methods for detecting changes
in linear regression models. We may further-
more mention the segmented package by Muggeo
(2008b) for change-point detection in piecewise lin-
ear models, the bcp package by Erdman and Emer-
son (2007) for Bayesian analysis of a single change
in univariate time series and the CPsurv package
by Brazzale et al. (2019) for nonparametric change-
point estimation in survival data.

1http://cran.r-project.org/web/views/Cluster.html

26

9 Discussion

Non-regularity can arise in many diﬀerent ways,
though all entail the failure of one, at times even
two, regularity conditions. Many problems can be
dealt with straightforwardly; other require sophis-
ticated tools including limit theorems and extreme
value theory for random ﬁelds. The wealth of con-
tributions, which has been produced during the last
70 years, testiﬁes that the interest in this type of
problems has not faded since they made their en-
trance back in the early 50’s. Most solutions, how-
ever, are freestanding and scattered in time and
scope. We grouped them into boundary, indeter-
minate parameter and change-point problems, ac-
cording to which conditions fail and the type of
asymptotic arguments used.

The best-studied nonregular case are boundary
problems. Common examples of application are
testing for a zero variance component in mixed ef-
fect models and constrained one-sided tests. The
limiting distribution of the likelihood ratio is gen-
erally a chi-bar squared distribution with a number
of components and mixing weights that depend on
the number of parameters which fall on the bound-
ary. This is also the only type of problem for which
higher order results are available.

Indeterminate parameter problems are far more
heterogenous. Apart from ﬁnite mixtures, the re-
maining cases can be put under the two umbrellas
of non-identiﬁable parameters and singular infor-
mation matrix. The methodological diﬃculties in-
crease as the limiting distributions depend on the
parametric family and on the unknown parame-
ters.
If θ is scalar and we want to test homo-
geneity against a two-component mixture, the dis-
tribution of the likelihood ratio converges to the
distribution of the supremum of a Gaussian pro-
cess. For a larger number of mixture components
and/or multidimensional θ, this becomes the dis-
tribution of the supremum of a Gaussian random
ﬁeld. In these cases, simulation-based approaches
are often needed to obtain the required tail prob-
abilities. Moreover, constraints must be imposed
to guarantee identiﬁability of the mixture param-
eters. As outlined by Garel (2007), these may act
on the parameter space, by bounding it or imposing
suitable separation conditions among the parame-
ters, or on the alternative hypotheses which must
be contiguous. A further possibility is to penalize

the likelihood function so that the limiting distribu-
tion of the corresponding modiﬁed likelihood ratio
statistic is chi-squared or well approximated by a
chi-bar squared distribution.

Change-point problems range from the simple
situation of detecting an alteration in the regime
of a random sequence to identifying a structural
break in multiple linear regression with possibly
correlated errors. Although in the second case the
change-point can assume any value, in the ﬁrst sit-
uation it must lie in a discrete set. Generally, the
limiting distribution of the likelihood ratio statis-
tic for detecting a change either converges to a
Gaussian processes or can be adjusted to converge
to a Gumbel type distribution (Horv´ath and Rice,
2014a). This technique was ﬁrst applied by Darling
and Erd˝os (1956) to derive the limiting distribu-
tion of the maximum of independent random vari-
ables, and has further been extended to depedent
data; see Aue and Horv´ath (2013) for a review. Ap-
proximate critical values of the test statistics can
be obtained from Bonferroni’s inequality, by using
asymptotic arguments or simulation. In some situ-
ations the likelihood ratio statistic for the unknown
change-point is unbounded.

From the more practical point of view, use of
the asymptotic distribution of the likelihood ratio
statistic loses its appeal once it goes beyond the
common χ2 distribution. As a result, simulation-
based tests that circumvent the asymptotic theory
are often used. Indeed, simulation may nowadays
be used to establish the desired empirical distribu-
tions of the estimators and to compute approxima-
tions for p-values obtained from Wald-type statis-
tics. For the most intricate situations, the authors
suggest to use resampling-based techniques, such
as parametric and nonparametric bootstrapping, to
explore the ﬁnite-sample properties of likelihood-
based statistics. Methodological diﬃculties, such
as the possibile divergence of the likelihood ratio
statistic, and prohibitive computational costs limit,
however, this possibility to speciﬁc applications.

The review has focused on frequentist hypothe-
sis testing using the likelihood ratio statistic. Maxi-
mum likelihood estimation for a class of nonregular
cases, which include the three-parameter Weibull,
the gamma, log-gamma and beta distributions, is
considered in Smith (1985). A signiﬁcant litera-
ture has grown since then, parts of which culmi-
nated in the book-length account of techniques for

27

parameter estimation in non-standard settings by
Cheng (2017). Most of the diﬃculties encountered
in nonregular settings vanish if the model is anal-
ysed using Bayes’ rule, though one has always to
be cautious. Bayesian and nonparametric contri-
butions were mentioned in passing throughout the
paper with suitable links to their frequentist coun-
terparts.

A Appendix

A.1 Prototype demonstrations

Proof sketch A.1. Boundary problem (Self and
Liang, 1987, Theorem 3) Let y1, . . . , yn be n inde-
pendent observations on the random variable Y ,
and let l(θ) denote the associated log-likelihood
function, where θ takes values in the parameter
space Θ, a subset of Rp. We want to test whether
the true value of θ lies in the subset of Θ denoted
by Θ0 versus the alternative that it falls in the com-
plement of Θ0 in Θ, denoted by Θ1. Let θ0 be the
true value of θ, which may fall on the boundary of
Θ. First, expand 2 (cid:8)l(θ) − l(θ0)(cid:9) around θ0,

2 (cid:8)l(θ) − l(θ0)(cid:9) = 2(θ − θ0)(cid:62)u(θ0)

− (θ − θ0)(cid:62)i(θ0)(θ − θ0)
+ Op(||θ − θ0||3),

where u(θ) is the score function, i(θ) the Fisher
information matrix and || · || represents the Eu-
clidean norm. Rewrite this expansion as a func-
tion of the variable ˜Zn = n−1i1(θ0)−1u(θ0), where
i(θ0) = ni1(θ0) and i1(θ0) is the Fisher information
matrix associated with a single observation. This
yields

2 (cid:8)l(θ) − l(θ0)(cid:9) =
√
n ˜Zn −

√

− {

n(θ − θ0)}(cid:62)i1(θ0)
√
√
n ˜Zn −
{
+ u(θ0)(cid:62)i(θ0)−1u(θ0)
+ Op(||θ − θ0||3).

n(θ − θ0)}

hi(θ) = H(θ)Si(θ) − H 2(θ) + H 2(θ)Ri(θ),
with hi(θ) = (cid:112)λi(θ) − 1. H(θ) is the Hellinger
distance between Fθ and F 0 deﬁned as
(cid:111)2(cid:21)

(cid:20)(cid:110)(cid:112)λi(θ) − 1

H 2(θ) = EF 0

/2

Consider now the likelihood ratio statistic

(GDQM)

(cid:26)

W = 2

√

(cid:104)

−{

= sup
θ∈Θ

(cid:27)

l(θ)

sup
θ∈Θ
n ˜Zn −

l(θ) − sup
θ∈Θ0
n(θ − θ0)}(cid:62)i1(θ0)

√

√

(cid:105)
n(θ − θ0)}

n ˜Zn −

√

n(θ − θ0)}(cid:62)i1(θ0)

√
{
(cid:104)
−{

n ˜Zn −
√

− sup
θ∈Θ0

√
{

n ˜Zn −
+ Op(||θ − θ0||3).

√

(cid:105)
n(θ − θ0)}

and Si(θ) and Ri(θ) are such that EF 0 [Si(θ)] =
EF 0 [Ri(θ)] = 0. Furthermore assume that

√

Approximate the two sets Θ and Θ0 by the cones
CΘ−θ0 and CΘ0−θ0 centered at θ0, respectively.
n ˜Zn converges in distribution to
Now, given that
a multivariate normal distribution with mean zero
and covariance matrix i1(θ0)−1, for all θ such that
θ − θ0 = Op(n−1/2), the limiting distribution of W
becomes

(cid:8)−(Z − θ)(cid:62)(Z − θ)(cid:9) −

sup
θ∈ ˜C

(cid:8)−(Z − θ)(cid:62)(Z − θ)(cid:9) ,

sup
θ∈ ˜C0

or equivalently as in Expression (3.3), where ˜C and
˜C0 are the corresponding transformations of the
cones CΘ−θ0 and CΘ0−θ0, respectively, and Z is
multivariate standard normal.

Proof sketch A.2. Non-identiﬁable parameter
(Liu and Shao, 2003, Theorem 2.3) Let Y1, . . . , Yn
be n independent and identically distributed ran-
dom observations from the true distribution func-
tion F 0. Suppose that we want to test H0 : θ ∈ Θ0
against H1 : θ ∈ Θ \ Θ0, where Θ0 = {θ ∈ Θ : Fθ =
F 0} with Fθ the distribution indexed by θ. Let
n
(cid:88)

lr(θ) =

log{λi(θ)}

sup
√
θ∈Θc/

n

|νn (Si(θ)) | = Op(1)

and

sup
√
θ∈Θc/

n

|EFn [Ri(θ)] | = op(1),

for all c > 0, where Fn(·) indicates the empirical
distribution function and νn(g) = n−1/2(nEFn −
EF 0 )[g] is a random process deﬁned for any inte-
grable function g. Here, Θ(cid:15) = {θ ∈ Θ | 0 < H(θ) ≤
(cid:15)} deﬁnes the Hellinger neighbourhood of F 0. Now,
using the GDQM expansion and a Taylor series ex-
pansion of 2 log{1 + hi(θ)}, the log-likelihood ratio
function lr(θ) can be expressed as

lr(θ) = 2

n
(cid:88)

log{1 + hi(θ)}

i=1
√

= 2
nH(θ)νn(Si(θ))
− nH 2(θ) (cid:8)2 + EFn

(cid:2)S2

i (θ)(cid:3)(cid:9) + op(1),(A.2)

√

n for all c > 0. Under some general con-
in Θc/
ditions on the trio {Si(θ), H(θ), Ri(θ)} (Liu and
Shao, 2003, Theorem 2.2), the quadratic expan-
sion in (A.2) holds uniformly in θ ∈ Θ(cid:15) for some
small enough (cid:15) > 0. Direct maximization of (A.1)
by
nH(θ) allows us to approximate the likelihood
ratio statistic by the quadratic form

√

i=1

be the log-likelihood ratio function, where λi(θ) =
λ(Yi; θ) denotes the Radon-Nikodym derivative,
λ(θ) = dFθ/dF 0, evaluated at Yi. Deﬁne the likeli-
hood ratio statistic as

W (H0) = 2 sup

{lr(θ) ∨ 0},

(A.1)

θ∈Θ\Θ0

where {a∨b} = max(a, b). Assume that there exists
a trio {Si(θ), H(θ), Ri(θ)} which satisﬁes the gen-
eralized diﬀerentiable in quadratic mean expansion

{νn(Si(θ)) ∨ 0}2
1 + EFn [S2
i (θ)]/2

≈ {νn(S∗

i (θ)) ∨ 0}2.

Let S be the se of all L2 limits of the standardized
score function

S∗

i (θ) =

Si(θ)
(cid:112)1 + EF 0 [S2

i (θ)]/2

as H(θ) → 0. To complete the proof we assume
there exists a centered Gaussian process {GS : S ∈

28

S} on the same probability space of the empirical
process νn with uniformly continuous sample paths
and covariance kernel EF 0 [GS1GS2] = EF 0 [S1S2],
for all S1, S2 belonging to S. Using results from
statistical limit theory, it is possibile to prove the
following two inequalities

W (H0) ≤ sup
S∈S

{GS ∨ 0}2 + op(1),

W (H0) ≥ sup
S∈S

{GS ∨ 0}2 + op(1),

which imply that

lim
n→∞

W (H0) = sup
S∈S

{GS ∨ 0}2.

Proof sketch A.3. Finite mixture model (Ghosh
and Sen, 1985, Theorem 2.1) Let y1, . . . , yn be a
sample of n i.i.d. observations from the strongly
identiﬁable mixture model (5.1) and

l(θ) =

n
(cid:88)

i=1

log {(1 − π)f1(yi; θ1) + πf2(yi; θ2)}

be the corresponding log-likelihood function. Sup-
pose that H0 : π = 0 is true, so the true model
density is f1(y; θ0
1 is the true value of
θ1. Unless diﬀerently stated, all functions and ex-
pectations will be evaluated under this assumption,
that is, for θ0 = (0, θ0
1, θ2), with arbitrary θ2. Let
W (H0) be the likelihood ratio statistic

1), where θ0

W (H0) = 2{ sup
π∈[0,1]
θ1∈Θ1
θ2∈Θ2

l(θ) − sup
π=0
θ1∈Θ1
θ2∈Θ2

l(θ) }

= sup
θ2∈Θ2

2{ sup

π∈[0,1]
θ1∈Θ1

l(θ) − sup
π=0
θ1∈Θ1

l(θ)}.(A.3)

Expand l(θ) with respect to the ﬁrst two compo-
nents of θ = (π, θ1, θ2) around π = 0 and θ1 = θ0
1.
This yields

l(θ) = l1(θ0

1) + An(θ) + op(1),

(A.4)

where l1(θ1) = (cid:80)n

i=1 log f1(yi; θ1) and

An(θ) = πlπ + (θ1 − θ0

1)(cid:62)lθ1 +

(cid:8)π2lππ

1
2

+ 2π(θ1 − θ0
+ (θ1 − θ0

1)(cid:62)lπθ1

1)(cid:62)lθ1θ1(θ1 − θ0

1)(cid:9) .

Here, the two indexes π and θ1 denote diﬀerenti-
ation with respect to the corresponding parameter
components. As shown in Ghosh and Sen (1985), in
virtue of the Kuhn-Tucker-Lagrange theorem, the
unconstrained supremum of An(θ) becomes

An(θ) =

1
2

sup
π∈[0,1]
θ1∈Θ1

(cid:8)u0(θ2), u(cid:62)

1

(cid:9) i(θ2)−1 (cid:8)u0(θ2), u(cid:62)

1

(cid:9)(cid:62)

if Zn(θ2) ≥ 0 and

An(θ) =

1
2

1 i−1
u(cid:62)

11 u1

sup
π∈[0,1]
θ1∈Θ1

if Zn(θ2) < 0, where we deﬁne

Zn(θ2) =

(cid:8)u0(θ2)i00(θ2) + u1(θ2)(cid:62)i01(θ2)(cid:9)
{i00(θ2)}1/2

.

In the previous three expressions, u0(θ2) = lπ(θ0),
u1 = lθ1 (θ0), i represents the expected information
matrix with respect to π and θ1, ijk(θ2) denotes the
(jk)-th component of i, for j = 0, 1 and k = 0, 1,
while ijk(θ2) denotes the (jk)-th component of i−1.
Similarly, the constrained supremum of An(θ) is

An(θ) =

sup
π=0
θ1∈Θ1

1
2

1 i−1
u(cid:62)

11 u1.

Using known results on the inversion of block ma-
trices, the likelihood ratio statistic (A.3) reduces
to

W (H0) = sup
θ2∈Θ2

Z 2

n(θ2) I{Zn≥0} + op(1).

To ensure the convergence of Zn(θ2) to the zero-
mean Gaussian processes Z(θ2), the set Θ2 needs
be bounded and a Lipschitz condition has to hold
for the u0 component of the score vector which,
in turn, implies tightness of u0. These conditions
furthermore guarantee that the remainder term in
expansion (A.4) is op(1) over the two bounded sets
of π and θ1 and uniformly in θ2.

Proof sketch A.4. Shift in location for Gaussian
model (Hawkins, 1977, Theorem 1) Given n inde-
pendent Gaussian observations, we want to test
whether

Yi ∼ N (µ, σ2),

i = 1, . . . , n,

29

against the alternative that there exists a 0 < τ < n
at which the unknown mean µ switches to µ(cid:48) (cid:54)= µ.
The variance σ2 is assumed to be known; we set
it to one without loss of generality. Recall from
Section 6.2 that the likelihood ratio statistic can
be re-expressed as a function of

U = max
1≤τ <n

|Tτ |,

where

Tτ =

(cid:114) n

τ
(cid:88)

τ (n − τ )

i=1

(Yi − ¯Y ).

The null distribution of U is given at (6.6). The
proof considers the following events

Markovian,

{T1, T2, . . . , Tτ −1}

and
is
{Tτ +1, Tτ +2, . . . , Tn−1} are
It
follows that the events Bτ and Cτ are independent
given Tτ = u, that is,

independent.

Pr(Cτ |Aτ ∩ Bτ ) = P(Cτ |Aτ ).

According to the probability symmetry between Bτ
and Cτ (Chen and Gupta, 2012, §2.1.1), similar to
Pr(Bτ |Aτ ), it follows that

Pr(Cτ |Aτ ) = gn−τ (u) + O(du).

(A.6)

Combining (A.5) and (A.6), we obtain

Aτ = {|Tτ | ∈ (u, u + du)},

Pr{U ∈ (u, u + du)} = 2φ(u)

n−1
(cid:88)

τ =1

gτ (u)gn−τ (u)du

Bτ = {|Ti| < |Tτ |, ∀i ∈ (1, . . . , τ − 1)},

+ o(du),

and

which corresponds to Expression (6.6).

Cτ = {|Ti| < |Tτ |, ∀i ∈ (τ + 1, . . . , n)}.

Deﬁne

FU (u + du) − FU (u) = Pr(cid:8)U ∈ (u, u + du)(cid:9)

= Pr

(cid:32)n−1
(cid:91)

τ =1

(cid:20)
{|Tτ | ∈ (u, u + du)}∩

{|Tτ | > |Ti|, i (cid:54)= τ }

(cid:21)(cid:19)

=

=

n−1
(cid:88)

τ =1

n−1
(cid:88)

τ =1

Pr(Aτ ∩ Bτ ∩ Cτ )

Pr(Aτ )Pr(Bτ |Aτ )Pr(Cτ |Aτ ∩ Bτ ).

Since Tτ ∼ N (0, 1), we have that

Pr(Aτ ) = 2φ(u)du + o(du).

Acknowledgements

This manuscript has undergone two major re-
writings. We thank the two Editors, the two
Associate Editors and three anonymous Referees
for their valuable suggestions which greatly helped
us improve many aspects of the paper.
In par-
ticular, they brought to our attention a num-
ber of relevant contributions which we hadn’t in-
cluded in the previous version of the manuscript.
It’s furthermore a pleasure to acknowledge dis-
cussion with Prof. Ruggero Bellio, Prof. An-
thony C. Davison and Prof. Nancy Reid. This
research was supported by University of Padova
grant no. CPDA101912 Large- and small-sample
inference under non-standard conditions (Progetto
di Ricerca di Ateneo 2010).

Moreover,

References

Pr(Bτ |Aτ )
= Pr(|Ti| < |Tτ |, ∀i ∈ (1, . . . , τ − 1) | |Tτ | = u)
= Pr(|Ti| < u, ∀i ∈ (1, . . . , τ − 1) | |Tτ | = u) + O(du)
= gτ (u) + O(du),

Algeri, S. and van Dyk, D. A. (2020). Test-
ing one hypothesis multiple times: the multidi-
mensional case. Journal of Computational and
Graphical Statistics, 29, 358–371.

(A.5)
where g1(u) = 1 for u ≥ 0 and gτ (u) is given
Since the series {T1, T2, . . . , Tn−1}
in (6.7).

Andrews, D. W. K. (2001). Testing when a pa-
rameter is on the boundary of the maintained
hypothesis. Econometrica, 69, 683–734.

30

Aue, A. and Horv´ath, L. (2013). Structural
breaks in time series. Journal of Time Series
Analysis, 34, 1–16.

Azzalini, A. (1996). Statistical Inference Based on

the likelihood. Chapman & Hall, London.

Baey, C. and Kuhn, E. (2019). varTestnlme:
in mixed-
https://github.com/baeyc/

components

testing

variance
eﬀect models.
varTestnlme

Banerjee, M. (2007). Likelihood based inference
for monotone response models. The Annals of
Statistics, 35, 931–956.

Banerjee, M. and Wellner, J. A. (2001). Like-
lihood Ratio Tests for Monotone Functions. The
Annals of Statistics, 29, 1699–1731.

Barndorff-Nielsen, O. E. and Cox, D. R.
(1994). Inference and Asymptotics. Chapman &
Hall, London.

Bellec, P. C. (2018). Sharp oracle inequalities
for Least Squares estimators in shape restricted
regression. The Annals of Statistics, 46, 745–780.

Benaglia, T., Chauveau, D., Hunter, D. R.,
Derek, Y. (2009). mixtools: An R Package
for Analyzing Finite Mixture Models. Journal of
Statistical Software, 32, 1–29.

Bhattacharya, P. K. (1994). Some aspects of
change-point analysis. In Change-point Prob-
lems. IMS Lecture Notes - Monograph Series
(edited by E. Carlstein, H.-G. M¨oller and D. Sieg-
mud), 23, pp. 28–56. IMS, Hayward.

Blischke, W. R., Truelove, A. J. and Mun-
dle, P. B. (1969). On non-regular estimation.
I. Variance bounds for estimators of location pa-
rameters. Journal of the American Statistical As-
sociation, 64, 1056–1072.

B¨ohning, D., Dietz, E.,

Schaub, R.,
Schlattmann, P. and Lindsay, B. G. (1994).
The distribution of the likelihood ratio for mix-
ture of densities from the one-parameter expo-
nential family. The Annals of the Institute of Sta-
tistical Mathematics, 46, 373–388.

B¨ohning, D. and Dietz, E. (1995). Discussion of
the paper by Cheng and Traylor (1995). Journal
of the Royal Statistial Society Series B (Method-
ological), 57, 33–34.

Bowden, R. (1973). The Theory of Parametric
Identiﬁcation. Econometrica, 41, 1069–1074.

Brazzale, A. R., Davison, A. C. and Reid,
N. (2007). Applied Asymptotics: Case Studies
in Small Sample Statistics. Cambridge Univer-
sity Press, Cambridge.

Brazzale, A. R., K¨uchenhoff, H., Kr¨ugel,
S., Schiergens, T. S., Trentzsch, H. and
Hartl, W. (2019). Nonparametric change point
estimation for survival distributions with a par-
tially constant hazard rate. Lifetime Data Anal-
ysis, 25, 301–321.

Cavaliere, G., Nielsen, H. B., Pedersen, R.
S. and Rahbek, A. (2022). Bootstrap inference
on the boundary of the parameter space, with ap-
plication to conditional volatility models. Jour-
nal of Econometrics, 227, 241–263.

(2018).

Chauveau, D., Garel, B. and Mercier,
univariate
Testing
URL:
in

S.
Gaussian
hal.archives-ouvertes.fr/hal-01659771/,
Version 2.

for
practice.

mixture

Chen, H. and Chen, J. (2001a). Large sample
distribution of the likelihood ratio test for nor-
mal mixtures. Statistics & Probability Letters,
52, 125–133.

Chen, H. and Chen, J. (2001b). The likelihood ra-
tio test for homogeneity in ﬁnite mixture models.
The Canadian Journal of Statistics, 29, 201–215.

Chen, H. and Chen, J. (2003). Tests for homo-
geneity in normal mixtures in the presence of a
structural parameter. Statistica Sinica, 13, 351–
365.

Chen, H., Chen, J. and Kalbfleisch, J. D.
(2001). A modiﬁed likelihood ratio test for ho-
mogeneity in ﬁnite mixture models. Journal of
the Royal Statistical Society, Series B (Method-
ological), 63, 19–29.

31

Chen, J. and Gupta, A. K. (2012). Paramet-
ric Statistical Change Point Analysis with Appli-
cation to Genetics, Medicine and Finance (2nd
ed.). Birkh¨auser, Boston.

Chen, J. and Li, P. (2009). Hypothesis test for
normal mixture models: the EM approach. The
Annals of Statistics, 37, 2523–2542.

Chen, J., Li, P. and Fu, Y. (2008). Testing ho-
mogeneity in a mixture of von Mises distributions
with a structural parameter. The Canadian Jour-
nal of Statistics, 36, 129–142.

Chen, S. X. and Van Keilegom, I. (2009). A
review on empirical likelihood methods for re-
gression. TEST, 18, 415–447.

Chen, J. (2017). On ﬁnite mixture models. Statis-

tical Theory and Related Fields, 1, 15–27.

Chen, R. and Cabrera, J. (2020). Bootstrap
conﬁdence intervals using the likelihood ratio test
in changepoint detection. Available on https:
//arxiv.org/abs/2011.03718.

Cheng, R. (2017). Non-Standard Parametric Sta-
tistical Inference. Oxford University Press, New
York.

Cheng, R. C. H. and Traylor, L. (1995). Non-
regular maximum likelihood problems (with Dis-
cussion). Journal of the Royal Statistical Society,
Series B (Methodological), 57, 3–44.

Cox, D. R. (2006). Principles of Statistical Infer-
ence. Cambridge University Press, New York.

Cox, D. R. and Hinkley, D. V. (1974). Theoret-

ical Statistics. Chapman & Hall, London.

Crainiceanu, C. M. and Ruppert, D. (2004).
Likelihood ratio tests in linear mixed models
with one variance component. Journal of the
Royal Statistical Society, Series B (Methodologi-
cal), 66, 165–185.

Cram´er, H. (1946). Mathematical Methods of
Statistics. Princeton University Press, Princeton.

Cs¨org¨o, M. and Horv´ath, L. (1997). Limit
Theorems in Change-point Analysis. Wiley, New
York.

Dacunha-Castelle, D. and Gassiat, ´E. (1997).
Testing in locally conic models, and applica-
tion to mixture models. ESAIM: Probability and
Statistics, 1, 285–317.

Darling, D.A. and Erd˝os, P. (1956). A limit
theorem for the maximum of normalized sums of
independent random variables. Duke Mathemat-
ical Journal, 23, 143–155.

Dacunha-Castelle, D. and Gassiat, ´E. (1999).
Testing the order of a model using locally conic
parametrization: population mixtures and sta-
tionary ARMA processes. The Annals of Statis-
tics, 27, 1178–1209.

Chernoff, H. (1954). On the distribution of the
likelihood ratio. The Annals of Mathematical
Statistics, 54, 573–578.

DasGupta, A. (2008). Asymptotic Theory of
Statistics and Probability. Springer-Verlag, New
York.

Chernoff, H. and Lander, E. (1995). Asymp-
totic distribution of the likelihood ratio test that
a mixture of two binomials is a single binomial.
Journal of Statistical Planning and Inference, 43,
19–40.

Ciuperca, G. (2002). Likelihood ratio statistic for
exponential mixtures. The Annals of the Institute
of Statistical Mathematics, 54, 585–594.

Cong, L., and Yao, W. (2021). A Likelihood Ra-
tio Test of a Homoscedastic Multivariate Normal
Mixture Against a Heteroscedastic Multivariate
Normal Mixture. Econometrics and Statistics,
18, 79–88.

Davison, A. C. (2003). Statistical Models. Cam-

bridge University Press, Cambridge.

del Castillo, J. and

Lopez-Ratera, A.
(2006). Saddlepoint approximation in exponen-
tial models with boundary points. Biometrika,
12, 491–500.

Doss, C. R. and Wellner, J. A. (2019). Infer-
ence for the mode of a log-concave density. The
Annals of Statistics, 47, 2950–2976.

Efron, B. and Hastie, T. (2016). Computer Age
Statistical Inference: Algorithms, Evidence, and
Data Science. Cambridge University Press, USA.

32

Erdman, C. and Emerson, J. W. (2007). bcp:
An R Package for Performing a Bayesian Analy-
sis of Change Point Problems. Journal of Statis-
tical Software, 23, 1–13.

Gombay, E. and Horv´ath, L. (1994b). An ap-
plication of the maximum likelihood test to the
change-point problem. Stochastic Processes and
their Applications, 50, 161–171.

Feng, Z. and McCulloch, C. E. (1992). Sta-
tistical inference using maximum likelihood esti-
mation and the generalized likelihood ratio when
the true parameter is on the boundary of the pa-
rameter space. Statistics & Probability Letters,
13, 325–332.

Feng, Z. and McCulloch, C. E. (1996). Using
bootstrap likelihood ratios in ﬁnite mixture mod-
els. Journal of the Royal Statistical Society, Se-
ries B (Methodological), 58, 609–617.

Feng, C., Wang, H. and Tu, X. M. (2012). The
asymptotic distribution of a likelihood ratio test
statistic for the homogeneity of poisson distribu-
tion. Sankhy¯a A, 74, 263–268.

Fu, Y., Chen, J. and Li, P. (2008). Modiﬁed like-
lihood ratio test for homogeneity in a mixture
of von Mises distributions. Journal of Statistical
Planning and Inference, 138, 667–681.

Garel, B. (2007). Recent asymptotic results in
testing for mixtures. Computational Statistics &
Data analysis, 51, 5295–5304.

Ghosh, J. K. and Sen, K. P. (1985). On the
asymptotic performance of the log likelihood ra-
tio statistic for the mixture model and related
results. In Proceedings of the Berkeley Confer-
ence in Honor of Jerzy Neyman and Jack Kiefer
(edited by L. LeCam, R. A. Olshen and C.-S.
Cheng), Vol. II, pp. 789–806. Wadsworth Ad-
vanced Books & Software, Monterey.

Godambe, V. P. (1991). Estimating Functions.

Oxford University Press, Oxford.

Goffinet, B., Loisel, P. and Laurent, B.
(1992). Testing in normal mixture models when
the proportions are known. Biometrika, 79, 842–
846.

Gombay, E. and Horv´ath, L.(1994a). Limit the-
orems for change in linear regression, Journal of
Multivariate Analysis, 48, 43–69.

Gombay, E. and Horv´ath, L. (1996). On the rate
of approximations for maximum likelihood tests
in change-point models. Journal of Multivariate
Analysis, 56, 120–152.

Gombay, E. and Horv´ath, L. (1999). Change-
points and bootstrap. Environmetrics, 10, 725–
736.

Groeneboom, P. and Jongbloed, G. (2018).
Some Developments in the Theory of Shape Con-
strained Inference. Statistical Science, 33, 473–
492.

Groeneboom, P. and Wellner, J. A. (2001).
Computing Chernoﬀ’s distribution. Journal of
Computational and Graphical Statistics, 10, 388–
400.

Hartigan, J. A. (1977). Distribution problems
in clustering. In Classiﬁcation and Clustering
(edited by J. van Ryzin), pp. 45-72. Academic
Press, New York.

Hartigan, J. A. (1985). A failure of likelihood
asymptotics for normal mixtures. In Proceedings
of the Berkeley Conference in Honor of Jerzy
Neyman and Jack Kiefer (edited by L. LeCam,
R. A. Olshen and C.-S. Cheng), Vol. II, pp. 807–
810. Wadsworth Advanced Books & Software,
Monterey.

Hathaway, R. J. (1985). A constrained formula-
tion of maximum-likelihood estimation for nor-
mal mixture distributions. The Annals of Statis-
tics, 13, 795–800.

Haughton, D. (1997). Packages for estimating ﬁ-
nite mixtures: a review. The American Statisti-
cian, 51, 194–205.

Hawkins, D. M. (1977). Testing a sequence of ob-
servations for a shift in location. Journal of the
American Statistical Association, 72, 180–186.

Hirano, K. and Porter, J. R. (2003). Asymp-
totic eﬃciency in parametric structural models
with parameter-dependent support. Economet-
rica, 71, 1307–1338.

33

Hogg, R. V., McKean, J. W. and Craig, A.
T. (2019). Introduction to Mathematical Statis-
tics (8th ed.). Pearson, Boston.

Horv´ath, L. and Rice, G. (2014a). Extensions of
some classical methods in change point analysis
(with Discussion). TEST, 23, 219–255.

Horv´ath, L. and Rice, G. (2014b). Rejoinder on:
Extensions of some classical methods in change
point analysis. TEST, 23, 287–290.

Huber, P. J. and Ronchetti, E. M. (2009). Ro-
bust Statistics (2nd ed.). John Wiley & Sons,
Hoboken, New Jersey.

Huˇskov´a, M. and Kirch, C. (2012). Bootstrap-
ping sequential change-point tests for linear re-
gression. Metrika, 75, 673–7088.

Huzurbazar, V. S. (1948). The likelihood equa-
tion, consistency and the maxima of the likeli-
hood function. The Annals of Eugenics, 14, 185–
200.

Irvine, J. M. (1986). The asymptotic distribu-
tion of the likelihood ratio test for a change in
the mean. Statistical Research Division Report
Series, CENSUS/SRD/RR-86/10, Bureau of
the Census, Washington, D.C. 20233.

Jaruˇskov´a, D.(1997). Some Problems with appli-
cation of change-point detection methods to en-
vironmental data. Environmetrics, 8, 469–483.

Kasahara, H., and Shimotsu, K. (2015). Test-
ing the number of components in normal mixture
regression models. Journal of the American Sta-
tistical Association, 110, 1632–1645.

Khodadadi, A. and Asgharian, M. (2008).
Change-point problem and regression: an an-
notated bibliography. COBRA Preprint Se-
ries, Working Paper 44. https://biostats.
bepress.com/cobra/art44

Kirch, C. and Steinebach, J. (2006). Permuta-
tion principles for the change analysis of stochas-
tic processes under strong invariance. Journal of
Computational and Applied Mathematics, 186,
64–88.

Kirch, C.

(2008). Bootstrapping sequential
change-point tests. Sequential Analysis, 27, 330–
349.

Koenker, R., Chernozhukov, V., He, X. and
Peng, L. (2017). Handbook of Quantile Regres-
sion. Chapman and Hall/CRC, Boca Raton, FL.

Kopylev, L. (2012). Constrained parameters in
applications: Review of issues and approaches.
International Scholarly Research Network ISRN
Biomathematics.

Kopylev, L. and Sinha, B. (2011). On the
asymptotic distribution of likelihood ratio test
when parameters lie on the boundary. Sankhy¯a
B, 73, 20–41.

Krishnaiah, P. R. and Miao, B. Q. (1988). Re-
view about estimation of change points. In Hand-
book of Statistics (edited by P. R. Krishnaiah and
C. R. Rao), Vol. 7, pp. 375–402. Elsevier, Ams-
terdam.

Kudˆo, A. (1963). A multivariate analogue of the

one-sided test. Biometrika, 50, 404–418.

Lancaster, T. (2000). The incidental parame-
ter problem since 1948. Journal of Econometrics,
95, 391–413.

LeCam, L. (1970). On the assumptions used to
prove asymptotic normality of maximum like-
lihood estimates. The Annals of Mathematical
Statistics, 41, 802–828.

LeCam, L. and Yang, G. L. (1990). Asymptotics
in Statistics: Some Basic Concepts, Springer-
Verlag, New York.

Killick, R. and Eckley, I.A. (2014). change-
point: An R Package for Changepoint Analysis.
Journal of Statistical Software, 58, 1–19.

Lee, T.-S. (2010). Change-point problems: bibli-
ography and review. Journal of Statistical Theory
and Practice, 4, 643–662.

Kim, H. J. and Siegmund, D. (1989). The likeli-
hood ratio test for a change-point in simple linear
regression. Biometrika, 76, 409–423.

Lehman, E. L. and Romano, J. P. (2005). Test-
ing Statistical Hypotheses (3rd ed.). Springer-
Verlag, New York.

34

Lemdani, M. and Pons, O. (1997). Likelihood ra-
tio tests for genetic linkage. Statistics and Prob-
ability Letters, 33, 15–22.

Lemdani, M. and Pons, O. (1999). Likelihood
ratio tests in contamination models. Bernoulli,
5, 705–719.

Li, S., Chen, J. and Li, P. (2016). MixtureInf:
Inference for Finite Mixture Models. R pack-
age version 1.1. https://CRAN.R-project.org/
package=MixtureInf

Li, P., Chen, J. and Marriot, P. (2009). Non-
ﬁnite Fisher information and homogeneity: an
EM approach. Biometrika, 96, 411–426.

Lindsay, B. G. (1995). Mixture Models: Theory,
Geometry and Applications. Institute of Mathe-
matical Statistics, Hayward.

Liu, X. and Shao, Y. (2003). Asymptotics for like-
lihood ratio tests under loss of identiﬁability. The
Annals of Statistics, 31, 807–832.

Lo, Y. (2008). A likelihood ratio test of a
homoscedastic normal mixture against a het-
eroscedastic normal mixture. Statistics and Com-
puting, 18, 233–240.

McLachlan, G. J. (1987). On bootstrapping the
likelihood ratio test statistic for the number of
components in a normal mixture. Applied Statis-
tics, 36, 318–324.

McLachlan, G., Lee, S. X. and Rathnayake,
S. (2019). Finite mixture models. Annual Review
of Statistics and Its Application, 6, 355–378.

McLachlan, G. and Peel, D. (2000). Finite
Mixture Models. John Wiley & Sons, New York.

Mendell, N. R., Thode, H. C. Jr. and Finch,
S. J. (1991). The likelihood ratio test for the two-
component normal mixture problem. Biometrics,
47, 1143–1148.

Muggeo, V. M. R., Atkins, D. C., Gallop, R.
J. and Dimidjian, S. (2014). Segmented mixed
models with random changepoints: a maximum
likelihood approach with application to treat-
ment for depression study. Statistical Modelling,
14, 293–313.

Murphy, S. and Van der Vaart, A. (1997).
Semiparametric likelihood ratio inference. The
Annals of Statistics, 25, 1471–1509.

Murphy, S. and Van der Vaart, A. (2000). On
Proﬁle Likelihood. Journal of the American Sta-
tistical Association, 95, 449–465.

Neyman, J. and Scott, E. L. (1948). Consistent
estimates based on partially consistent observa-
tions. Econometrica, 16, 11–32.

Niu, Y.S., Hao, N. and Zhang, H. (2016). Multi-
ple change-point detection: A selective overview.
Statistical Science, 31, 611–623.

Oliveira-Brochado, A. and Martins, N.
(2005). Assessing the number of components in
mixture models: a review. FEP Working Pa-
pers, 194, Universidade do Porto, Faculdade de
Economia do Porto.

Owen, A. B. (1990). Empirical likelihood conﬁ-
dence regions. The Annals of Statistics, 18, 90–
120.

Owen, A. B. (1991). Empirical likelihood for lin-
ear models. The Annals of Statistics, 19, 1725–
1747.

Pace, L. and Salvan, A. (1997). Principles of
Statistical Inference: from a Neo-Fisherian Per-
spective. World Scientiﬁc Publishing, Singapore.

Page, E. S. (1955). A test for a change in a param-
eter occurring at an unknown point. Biometrika,
42, 523–527.

Page, E. S. (1957). On problems in which a change
in a parameter occurs at an unknown point.
Biometrika, 44, 248–252.

Muggeo, V. M. R. (2008b). segmented: an R
Package to Fit Regression Models with Broken-
Line Relationships. R News, 8/1, 20–25. https:
//cran.r-project.org/doc/Rnews/

Paulino, C. D. M. and Pereira, C. A. B.
(1994). On identiﬁability of parametric statistical
models. Journal of the Italian Statistical Society,
1, 125–151.

35

Pfanzagl, J. (2017). Mathematical Statistics:
Essays on History and Methodology. Springer-
Verlag, Berlin Heidelberg.

Samworth, R. J. and Bodhisattva, S. (Eds.)
(2018). Special Issue on Nonparametric Inference
Under Shape Constraints. Statistical Science, 33.

Piegorsch, W. W. and Bailer, A. J. (1997).
Statistics for Environmental Biology and Toxi-
cology. Chapman & Hall, London.

Prakasa Rao, B. L. S. (1992). Identiﬁability in
Stochastic Models: Characterization of Probabil-
ity Distributions. Academic Press, London.

Quandt, R. E. (1958). The estimation of the pa-
rameters of a linear regression system obeying
two separate regimes. Journal of the American
Statistical Association, 53, 873–880.

Quandt, R. E. (1960). Tests of the hypothesis
that a linear regression system obeys two sepa-
rate regimes. Journal of the American Statistical
Association, 55, 324–330.

R Core Team (2020). R: A Language and En-
vironment for Statistical Computing. R Founda-
tion for Statistical Computing, Vienna, Austria.
https://www.R-project.org/

Robbins, M. W., Gallagher, C. M. and Lund,
R. B. (2016). A general regression changepoint
test for time series data. Journal of the American
Statistical Association, 111, 670–683.

Robbins, M., Gallagher, C., Lund, R. and
Aue, A. (2011). Mean shift testing in correlated
data. Journal of Time Series Analysis, 32, 498–
511.

Robertson, T., Wright, F. T. and Dykstra,
R. L. (1988). Order Restricted Inference. John
Wiley & Sons, New York.

Rothenberg, T. J. (1971). Identiﬁcation in para-

metric models. Econometrica, 39, 577–591.

Rotnitzky, A., Cox, D. R., Bottai, M. and
Robins, J. (2000). Likelihood-based inference
with singular information matrix. Bernoulli, 6,
243–284.

Samworth, R. J. (2018). Recent progress in log-
concave density estimation. Statistical Science
33, 493–509.

Scheipl, F., Greven, S. and K¨uchenhoff, H.
(2008). Size and power of tests for a zero ran-
dom eﬀect variance or polynomial regression in
additive and linear mixed models. Computational
Statistics & Data Analysis, 52, 3283–3299.

Scrucca, L., Fop, M., Murphy, T. B. and
Raftery, A. E. (2016). mclust 5: clustering,
classiﬁcation and density estimation using Gaus-
sian ﬁnite mixture models. The R Journal, 8,
289–317.

Self, S. G. and Liang, K. (1987). Asymptotic
properties of maximum likelihood estimators and
likelihood ratio tests under non standard condi-
tions. Journal of the American Statistical Asso-
ciation, 82, 605–610.

Sen, P. K. and Silvapulle, M. J. (2002). An
appraisal of some aspects of statistical inference
under inequality constraints. Journal of Statisti-
cal Planning and Inference, 107, 3–43.

Serfling, R. J. (1980). Approximation Theorems
of Mathematical Statistics. John Wiley & Sons,
New York.

Severini, T. (2000). Likelihood Methods in Statis-

tics. Oxford University Press, Oxford.

Severini, T. (2004). A modiﬁed likelihood ratio
statistic for some nonregular models. Biometrika,
91, 603–612.

Shaban, S. A. (1980). Change point problems and
two-phase regression: an annotated bibliography.
International Statistical Review, 48, 83–93.

Shapiro, A. (1985). Asymptotic distribution of
test statistics in the analysis of moment struc-
tures under inequality constraints. Biometrika,
72, 133–144.

Shapiro, A. (1988). Towards a uniﬁed theory
of inequality constrained testing in multivari-
ate analysis. International Statistical Review, 56,
49–62.

36

Silvapulle, M. J. and Sen, P. K. (2005). Con-
strained Statistical Inference. John Wiley & Sons,
New York.

Titterington, D. M. (1990). Some recent re-
search in the analysis of mixture distributions.
Statistics, 21, 619–641.

Silvey, S. D. (1959). The Lagragian multiplier
test. The Annals of Mathematical Statistics, 30,
382–407.

Ulm, K. W. (1991). A statistical method for as-
sessing a threshold in epidemiological studies.
Statistics in Medicine, 10, 341–349.

Sinha, B., Kopylev, L. and Fox, J. (2012).
Some new aspects of dose-response multi-stage
models with applications. Pakistan Journal of
Statistics and Operation Research, 8, 441–478.

Smith, R. L. (1985). Maximum likelihood estima-
tion in a class of non regular cases. Biometrika,
72, 67–92.

Smith, R. L. (1989). A survey of non-regular prob-
lems. Proceedings of the 47th Session of the In-
ternational Statistical Institute, Paris, August 29
– September 6, 1989, pp. 353–372.

Smith, A. M. F. and Cook, D. G. (1980).
Straight lines with a change point: A Bayesian
analysis of some renal transplant data. Applied
Statistics, 29, 180–189.

Sørensen, H. (2008). Small sample distribution
of the likelihood ratio eﬀects in the random ef-
fects model. Journal of Statistical Planning and
Inference, 138, 1605–1614.

Stram, D. O. and Lee, J. W. (1994). Variance
components testing in the longitudinal mixed rf-
fects model. Biometrics, 50, 1171–1177.

Sun, H.-J. (1988). A FORTRAN subroutine for
computing normal orthant probabilities of di-
mensions up to nine. Communication in Statis-
tics – Computation and Simulation, 17, 1097–
1111.

Sofronov, G., Wendler, M. and Liebscher,
V. (Eds.) (2020). Part 1: Special issue on change
point detection (ﬁrst 10 articles). Statistical Pa-
pers, 61, 1347–1588.

Thode, H. C. Jr., Finch, S. J. and Mendell,
N. R. (1988). Simulated percentage points for
the null distribution of the likelihood ratio test
for a mixture of two normals. Biometrics, 44,
1195–1201.

van der Vaart, A. W. (2000). Asymtptotic
Statistics. Cambridge University Press, New
York.

Wald, A. (1949). Note on the consistency of the
maximum likelihood estimate. Annals of Mathe-
matical Statistics, 20, 595–601.

Wichitchan, S., Yao, W., and Yang, G. (2019).
Hypothesis testing for ﬁnite mixture models.
Computational Statistics & Data Analysis, 132,
180–189.

Wilks, S. S. (1938). The large sample distribution
of the likelihood ratio for testing composite hy-
potheses. The Annals of Mathematical Statistics,
1, 60–62.

Wolak, F. A. (1987). An exact test for multiple
inequality and equality constraints in the linear
regression model. Journal of the American Sta-
tistical Association, 82, 782–793.

Worsley, K. J. (1979). On the likelihood ratio
test for a shift in location of normal populations.
Journal of the American Statistical Association,
74, 365–367.

Worsley, K. J. (1983). The power of likelihood
ratio and cumulative sum tests for a change in a
binomial probability. Biometrika, 70, 455–464.

Worsley, K. J. (1986). Conﬁdence regions and
test for a change point in a sequence of expo-
nential family random variables. Biometrika, 73,
91–104.

Yao, Y.-C. and Davis, R.A. (1986). The asymp-
totic behavior of the likelihood ratio statistic for
testing a shift in mean in a sequence of indepen-
dent normal variates. Sankhy¯a A, 48, 339–353.

Yu, Y. (2018). mixR: Finite Mixture Modeling for
Raw and Binned Data. R package version 0.1.1.
https://CRAN.R-project.org/package=mixR

37

Yu, M. and Chen, X. (2022). A robust bootstrap
change point test for high-dimensional location
parameter. Electronic Journal of Statistics, 16,
1096–1152.

Zeileis, A. (2006). Implementing a class of struc-
tural change tests:
an econometric comput-
ing approach. Computational Statistics & Data
Analysis, 50, 2987–3008

Zeileis, A., Leisch, F., Hornik, K. and
Kleiber, C. (2002). strucchange: An R package
for testing for structural change in linear regres-
sion models. Journal of Statistical Software, 7,
1–38.

Zhang, Y., Staicu, A.-M., and Maity, A.
(2016). Testing for additivity in non-parametric
regression. Canadian Journal of Statistics, 44,
445–462.

Zhang, Y. (2018). lmeVarComp: Testing for a
Subset of Variance Components in Linear Mixed
Models. R package version 1.1. https://CRAN.
R-project.org/package=lmeVarComp

Zou, C., Y. Liu, Y., Qin, P. and Wang,
Z.(2007). Empirical likelihood ratio test for the
change-point problem. Statistics and Probability
Letters, 77, 374–382.

38

