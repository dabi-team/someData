2
2
0
2

y
a
M
4

]
E
S
.
s
c
[

1
v
8
3
9
1
0
.
5
0
2
2
:
v
i
X
r
a

DeepFD: Automated Fault Diagnosis and Localization for Deep
Learning Programs

Jialun Cao
The Hong Kong University of Science
and Technology, and Guangzhou
HKUST Fok Ying Tung Research
Institute
Hong Kong, China
jcaoap@cse.ust.hk

Ming Wen*
Huazhong University of Science and
Technology
Wuhan, China
mwenaa@hust.edu.cn

Meiziniu Li
The Hong Kong University of Science
and Technology
Hong Kong, China
mlick@cse.ust.hk

Xiao Chen
Huazhong University of Science and
Technology
Wuhan, China
xchencr@hust.edu.cn

Bo Wu
MIT-IBM Watson AI Lab
Cambridge, MA, U.S.
bo.wu@ibm.com

Yongqiang Tian
University of Waterloo, Canada, and
The Hong Kong University of Science
and Technology, China
Waterloo, Canada
yongqiang.tian@uwaterloo.ca

Shing-Chi Cheung*
The Hong Kong University of Science
and Technology, and Guangzhou
HKUST Fok Ying Tung Research
Institute
Hong Kong, China
scc@cse.ust.hk

ABSTRACT
As Deep Learning (DL) systems are widely deployed for mission-
critical applications, debugging such systems becomes essential.
Most existing works identify and repair suspicious neurons on the
trained Deep Neural Network (DNN), which, unfortunately, might
be a detour. Specifically, several existing studies have reported that
many unsatisfactory behaviors are actually originated from the
faults residing in DL programs. Besides, locating faulty neurons is
not actionable for developers, while locating the faulty statements
in DL programs can provide developers with more useful informa-
tion for debugging. Though a few recent studies were proposed
to pinpoint the faulty statements in DL programs or the training
settings (e.g. too large learning rate), they were mainly designed
based on predefined rules, leading to many false alarms or false
negatives, especially when the faults are beyond their capabilities.
In view of these limitations, in this paper, we proposed DeepFD,
a learning-based fault diagnosis and localization framework which
maps the fault localization task to a learning problem. In particu-
lar, it infers the suspicious fault types via monitoring the runtime
features extracted during DNN model training, and then locates

* Corresponding author.

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
This is the author’s version of the work. It is posted here for your personal use. Not
for redistribution. The definitive Version of Record was published in 44th International
Conference on Software Engineering (ICSE ’22), May 21–29, 2022, Pittsburgh, PA, USA,
https://doi.org/10.1145/3510003.3510099.

the diagnosed faults in DL programs. It overcomes the limitations
by identifying the root causes of faults in DL programs instead of
neurons, and diagnosing the faults by a learning approach instead
of a set of hard-coded rules. The evaluation exhibits the potential of
DeepFD. It correctly diagnoses 52% faulty DL programs, compared
with around half (27%) achieved by the best state-of-the-art works.
Besides, for fault localization, DeepFD also outperforms the exist-
ing works, correctly locating 42% faulty programs, which almost
doubles the best result (23%) achieved by the existing works.

CCS CONCEPTS
• Software and its engineering → Software testing and de-
bugging; • Computing methodologies → Neural networks.

KEYWORDS
Neural Networks, Fault Diagnosis, Fault Localization, Debugging

ACM Reference Format:
Jialun Cao, Meiziniu Li, Xiao Chen, Ming Wen*, Yongqiang Tian, Bo Wu,
and Shing-Chi Cheung*. 2022. DeepFD: Automated Fault Diagnosis and
Localization for Deep Learning Programs. In 44th International Conference on
Software Engineering (ICSE ’22), May 21–29, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3510003.3510099

1 INTRODUCTION
Deep learning (DL) software has been actively deployed for appli-
cations such as fraud detection, medical diagnosis, face recognition,
and autonomous driving [27, 30, 31]. Such software comprises DL

 
 
 
 
 
 
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

CAO, et al.

programs, which encode the structure of a Deep Neural Network
(DNN) model and the process by which the model is trained. Various
studies [8, 10, 26, 39, 40, 58, 61] have been conducted to understand
and detect DL program faults.1

Yet, debugging DL programs is still challenging [20, 31, 40, 58, 71].
Unlike conventional software programs, the decision logic based on
which a DNN model makes predictions is not explicitly encoded by
the DL program’s control flow [20]. Instead, predictions are made by
propagating inputs against the tuned parameters through a trained
DNN model. The program plays the crucial role by defining, guiding
and monitoring the model training process (e.g., defining network
structures, training strategies and hyperparameters) based on a
training set to indirectly tune the parameters of the DNN model.
Due to the stochastic nature of DL computation, a DNN model is
unable to make every prediction correctly, and an incorrect predic-
tion result does not necessarily indicate a fault in the underlying
program. Therefore, it is hard to debug DL computation faults using
conventional fault localization techniques [6, 11, 57, 59, 61, 63, 65,
73] based on individual correct and incorrect prediction results.
Techniques [20, 40, 66] have been proposed to debug such faults
by identifying and locating suspicious neurons or layers in the
trained DNN model. Specifically, they draw an analogy between a
faulty neuron (or layer) in a DNN model and a faulty statement (or
branch) in a conventional program. With the analogy, they adapt
fault localization techniques such as the spectrum-based ones to
detect suspicious neurons or layers based on statistical analysis.
However, directly tuning the weights of neurons after training can-
not facilitate pinpointing the flaws in DL programs. Even worse,
after tuning, developer may ignore the faults in the program, thus
leaving the real root causes (e.g., inappropriate loss function and
optimizer setting) of the unsatisfactory behavior uncovered.

Recent works addressed the problem (i.e. debugging DL systems)
in a more practical way. For instance, UMLAUT [54] was proposed
to detect program faults using predefined rules, and provide advice
on how to fix them. AutoTrainer [70] was proposed to detect five
predefined problems that might occur during training DNN models,
with solutions to eradicate the detected problems. These rule-based
approaches do pinpoint more accurate root causes of the faults (i.e.
lacking of data normalization), or provide more actionable advice on
how to fix the DL program or tuning the hyper-parameters (i.e. set
the learning rate between 10−7 to 0.1). However, such hard-coded
rules may induce false alarms and are incapable of revealing faults
that go beyond the capability of those predefined rules. For example,
UMLAUT reports a fault whenever the output layer is not activated
by softmax, while in fact, the activator of the output layer varies
case by case. Besides, the pre-defined rules can over-simplify fault
detection to limited symptoms exhibited by several types of faults
(e.g., the example presented in Section 2).

In view of the above-mentioned limitations, we look for an al-
ternative. Specifically, we propose a learning-based fault diagnosis
and localization framework, DeepFD, which maps a fault local-
ization task to a learning problem. In particular, it diagnoses and
locates faults in a DL program by inferring the suspicious fault
types using the features extracted in the model training process.

1The existing related works may use other terms like ‘’bug”, “defect” or “issue”. This pa-
per uses “fault” as a representative of all such related terms to refer to any human-made
mistake in the DL program that leads to functionally insufficient performance [27].

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

# Model construction
model = Sequential()
model.add(Dense(input_dim=2,

output_dim=4,
init="glorot_uniform"))

model.add(Activation("sigmoid"))
model.add(Dense(input_dim=4,

output_dim=1,
init="glorot_uniform"))

model.add(Activation("sigmoid"))

# Training Configuration
sgd = SGD(lr=0.05,

l2=0.0, decay=1e-6,
momentum=0.11, nesterov=True)

# Bug 1

model.compile(loss='mean_absolute_error', optimizer=sgd)
model.fit(train_data, label, nb_epoch=1000, batch_size=4)

# Bug 2
# Bug 3

Listing 1: A Faulty Code Snippet Extracted from StackOver-
flow #31556268.

The intuition behind is that the trend or distribution of certain
external or internal variables (e.g., loss or gradients) in a program’s
training process can suggest the likelihood of faults and their types.
Such an intuition is also echoed by our observation that the value
of variables in a DL program can follow certain patterns during the
model training process, and these patterns exhibit strong correla-
tions with certain types of faults. For instance, if the loss variable’s
value oscillates wildly, the training is likely using an inappropriate
learning rate [70]. However, it is difficult to set a specific threshold
for such oscillations and identify the existence of learning rates fault
accordingly. We, therefore, resort to a learning-based approach.

However, the design of DeepFD needs to address two main chal-
lenges. First, there is no existing off-the-shelf dataset (i.e., faulty DL
programs with the ground-truth of faults) available that is sufficient
to enable the learning of fault-related features. Though one may
seed faults into correct DL programs to construct such a training
set, how to seed diverse faults effectively into the programs, and
further determine whether the DNN model is indeed faulty after
fault seeding remains unknown. Second, there are few references
of deploying learning algorithms to locate DL program faults while
the effectiveness of a learning-based technique often requires a set
of high-quality features. However, since there are enormous param-
eters and outputs during DNN training, it is infeasible to store all
the values during the continuous training iterations, and use such
values as features. Therefore, how to select qualified features that
can be utilized for effective localization of DNN faults remains to
be challenging.

To address the first challenge, we collected a benchmark with 58
real faults in the wild with patches and analyzed the prevalent fault
types made by developers. As a result, five common fault types
are observed. We seeded faults of these types to hundreds of DL
programs from a recent benchmark [70] to generate a training set
with thousands of faulty DL programs, serving for the learning part
of DeepFD. Furthermore, we adopted the generalised linear model
(GLM) [46] and Cohen’s effect size d [17] to determine whether a
fault-seeded DL program is statistically different from and worse
than the original program, thus determining the oracle of the seeded
programs. To address the second challenge, inspired by the runtime
information used in previous studies [50, 54, 58, 70], we designed
and traced the runtime information such as loss, gradients, and

DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

the number of times loss increases. Then, we applied statistical
analysis on each trace of runtime information, resulting in a list of
extracted runtime features, which can represent the process of this
DNN model training. Thereby, with the extracted features as inputs,
and the seeded faults as ground-truth labels, the fault diagnosis
problem can, therefore, be translated into a classification problem.
In summary, our work makes the following major contributions:

• Originality: We identify a set of features that can be used to
classify major types of faults in DL programs, which are extracted
from the DNN training process. Leveraging these features, we
propose a learning-based fault localization framework, DeepFD,
for DL programs. It is able to diagnose multiple faults and identify
their individual fault types. For each of these faults, it can further
locate the faulty code snippet in the concerned DL program.
• Benchmark: We build a benchmark containing 58 faulty DL
programs collected from Stack Overflow and GitHub. Each of
them include faults, patches and line numbers of the faulty state-
ments. This benchmark can serve for future researches on DNN
debugging and repair tools.

• Usefulness: We encapsulate the fault seeding, faulty program
checking, feature extraction, fault diagnosis and localization into
the DeepFD framework, and open-sourced it [15]. The tool is
extensible for seeding diverse faults and mutation testing inde-
pendently. It can also be adapted to extract more runtime features
for other learning-based works.

• Evaluation: The evaluation exhibits the potential of DeepFD.
It can correctly diagnose 52% cases, compared with half (27%)
achieved by the best state-of-the-art works. Besides, for fault lo-
calization, DeepFD also outperforms the existing works, reaching
42%, while the existing works range from 10% to 23%.

2 MOTIVATION
Listing 1 shows a buggy DL program extracted from StackOver-
flow.2 The program constructs a DNN model for the XOR problem,
but the model’s accuracy gets stuck at around 50%. The faults in
this DNN program include: (1) an inappropriate learning rate (line
13), (2) unsuitable loss function (line 16) and (3) insufficient training
epoch (line 17). It is challenging for developers to diagnose and
localize these faults all at once.

We applied three state-of-the-art techniques, i.e., AutoTrainer [70],
UMLUAT [54] and DeepLocalize [58] to this buggy program to ex-
amine whether they can help diagnose and locate the faults. Note
that we do not compare with automated machine learning (AutoML)
algorithms [29] since these works are not for debugging a faulty
program, but selecting machine learning algorithms according to
the user-provided data [29]. However, our goal is to debug a faulty
program and identify the root causes. To reduce the randomness of
training process, we ran each work 10 times then report the result.
The results are shown in Table 1. Specifically, AutoTrainer cannot
detect any faults over the 10 runs, let alone diagnosing the faults.
The faults are escaped from the AutoTrainer’s detection because
this buggy program does not exhibit the abnormal symptoms that
are predefined by AutoTrainer such as gradient vanish [25, 44, 55]
and dying ReLU [9]. On the other hand, though UMLUAT and

Table 1: Fault Diagnosis and Localization Results.

Method
AutoTrainer [70]

UMLUAT [54]

DeepLocalize [58]

DeepFD

Fault Diagnosis
No training problem
Critical: Missing Softmax layer before loss
Critical: Missing activation functions
Warning: Last model layer has nonlinear activation
Warning: Learning Rate is high
Warning: Possible overfitting
layer-3 Error in Output Gradient
Stop at 1 epoch
Fault 1: [Loss]
Fault 2: [lr]
Fault 3: [Epoch]
Fault 4: [Act]

Fault Localization
–

–

Layer 3

Lines: [16]
Lines: [13]
Lines: [17]
Lines: [6, 10]

DeepLocalize are able to detect the existence of faults, the diag-
nosed faults are inaccurate. Specifically, UMLUAT falsely alarmed
that the last layer has two faults (i.e. missing Softmax layer before
loss, and missing activation functions). In fact, these two alarms
will be fired as long as the output layer is not activated by Softmax.
The only root cause that has been correctly diagnosed by UMLUAT
is the inappropriate learning rate, but it is marked as a warning. On
the other hand, DeepLocalize reports a fault at the output gradient
of layer 3 (i.e., line 10) due to numerical errors (e.g. NaN) occurred
when propagating to this layer. Though it does locate where the
symptom happens, it fails to correctly pinpoint any of the faults.

Taking a closer look, some existing works prescribe the symp-
toms and map them only to the existence of potential faults, but
cannot identify the specific types of faults [58]. Other works pre-
defined rules relying on various predefined thresholds, yet it is
infeasible to try out all the combinations of various thresholds to
find out the optimal one [54, 70]. As such, our learning-based frame-
work, DeepFD, is able to overcome these limitations by learning the
correlations between symptoms and specific types of faults, and
learning the optimal thresholds automatically.

The secret of DeepFD is that some runtime information exhibits
significant correlations with certain types of faults. Specifically,
Figure 1 shows the distributions of several features (see Section 3.1
for more details) when the learning rate and loss function are faulty
(in red) or not (in blue). We can observe that there are obvious
statistical differences when the learning rate or loss function are
set appropriately or inappropriately. The observation enables us to
perform fault diagnosis and localization as a learning problem via
leveraging the relevant stochastic runtime information of a buggy
DL program. Finally, for the example as shown in Listing 1, our
approach, DeepFD, can report the four types of suspicious faults
in the program: the loss function (line 16), learning rate (line 13),
training epoch (line 17) and activation function (line 6 and line
10). After repairing these faults accordingly, the DNN model can
achieve 100% accuracy.3

3 APPROACH
3.1 Workflow of DeepFD
Figure 2 shows the workflow of DeepFD. It comprises three steps:
diagnostic feature extraction, fault diagnosis and fault localization.

2https://stackoverflow.com/questions/31556268/how-to-use-keras-for-xor.

3Since it is a XOR problem, there is no need to consider over-fitting.

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

CAO, et al.

Figure 1: Correlation Between Types of Faults and Features. The first row illustrates correlations between certain features
with unsuitable learning rate, the second row with unsuitable loss function. The naming convention of features starts with
‘ft’ (feature), followed by the name of runtime data and the applied statistical operators.

Figure 2: Overview of DeepFD.

Step 1: Diagnostic Feature Extraction. Given a program, DeepFD
constructs a DNN architecture and collects runtime data such as the
loss and neurons’ information by training the architecture. How-
ever, storing all the weights and gradients for each neuron at each
training iteration is expensive. Referencing recent works on dy-
namic trace collection [58, 70] and runtime monitoring [50, 54],
DeepFD collects 20 runtime data (see Table 2 for more details), in-
cluding loss and accuracy, the number of times the loss increases,
the mean or standard derivation of weights, etc. The data are repeat-
edly collected at certain intervals (e.g., every 256 batches or every
epoch). Finally, 160 diagnostic features are extracted by applying
statistical analyses (e.g., calculating the variance and skewness) to
the collected data.

Table 2 lists the 20 runtime data collected by DeepFD along with
the monitored variables and detailed description. The collection
of these runtime data are inspired by various existing works [1, 9,
44, 55, 55, 58, 62, 70] and processed in a way following the settings
used by existing studies [58, 70]. The runtime data are collected at
predefined intervals during the training stage, resulting in 20 data

sequences. The data sequences are then processed using the eight
statistical operators in Table 3 to extract diagnostic features. For
example, the skewness of a data sequence serves as a diagnostic
feature that quantifies the asymmetry of the probability distribu-
tion of the sequence with respect to its mean. After applying the
statistical operators, 160 (20 * 8 = 160) diagnostic features are ex-
tracted to characterize the training process of the given DL program.

Step 2: Fault Diagnosis. After obtaining the diagnostic features,
we then infer the possible types of faults that exist in the DL pro-
gram according to the features. We regard it as a multi-label clas-
sification problem, which maps the obtained features into one or
more possible labels. Each label corresponds to a fault type. The
classification relies on the predictions made by a set of pretrained
diagnosis models (see Section 3.2 for details). The diagnosis result
is given by the union of the diagnosed faults predicted by each
diagnosis model to maximize the number of faults diagnosed. Also,
for each DL program under test, we run them ten times to address
the randomness of DNN model training [70].

FaultLocalizationFaultDiagnosisDiagnosticFeatureCollectionDNN TrainingFaultyLinesDLProgramUnderTestRuntimeInfo•Gradient•Loss•…DiagnosedTypesofFaultsFault(s)Found?NYPretrainedDiagnosisModelsDiagnosticFeaturesDNNModelTrainedDNNModelInformationExtractionandProcessProgramAnalysisLabeledbySeeded FaultsLabelingFault SeedingTraining Set PreparationDiagnosisModelTrainingMutantFaulty?YNLabeledasCorrectRuntime Info•Gradient•Loss•…DNNTrainingInformationExtractionandProcessDiagnosticFeaturesDNNModelTrainedDNNModelTrainingSetDiagnosisModelsFaults to be SeededOriginalProgramConfigurations•Loss•Activation•Optimizer•…Seed FaultsDeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

Table 2: Runtime Data Collected by DeepFD During DNN Model Training

Monitored Variables
Loss
Accuracy
Validation loss
Validation Aacuracy
Loss
Accuracy
Weight
Gradient
Weight
Trace of accuracy
Trace of loss

Runtime Data
loss
acc
loss_val
acc_val
nan_loss
nan_accuracy
nan_weight
nan_gradient
large_weight
decrease_acc
increase_loss
cons_mean_weight Trace of mean of weight
cons_std_weight
gap_train_test
test_turn_bad
slow_converge
oscillating_loss

Description
Loss on the training set.
Accuracy on the training set.
Loss on the validation set.
Accuracy on the validation set.
The number of times loss is not a number (i.e., NaN)
Whether there are NaN in accuracy
Whether there are NaN in weight
Whether there are NaN in gradient
The number of times the maximum weight is larger than a certain threshold.
The number of times the accuracy is smaller than the last recorded accuracy.
The number of times the loss is larger than the last recorded loss.
The number of times the mean of weight remains the same as the last record.
Trace of standard deviation of weight Whether the standard deviation of weight remains the same as the last record.
Whether the gap between training accuracy and validating accuracy is too big.
Accuracy, Validation accuracy
Whether the loss on the training set decreases while on validation set increases.
Loss, Validation loss
Whether the accuracy of trained models is growing slowly.
Trace of accuracy
Whether the loss is oscillating.
Trace of Loss, Accuracy
There has been a set of neurons whose gradients have been 0 in the recent a few iterations and this set
is large forms a large portion of the whole DNN while the accuracy of the neuron network is still low.
Whether the gradient vanish problem occurs.
Whether the gradient explode problem occurs.

gradient_vanish
Gradient, Accuracy
gradient_explosion Gradient, Accuracy

Traces of Gradient, Accuracy

dying_relu

Currently, the diagnosis models are trained to classify five major
types of faults, including unsuitable loss function, optimizer, acti-
vation function, insufficient iteration and inappropriate learning
rate, which account for the majority (73.55%) types of faults in our
benchmark (see Section 4 for more details). We use these common
fault types to show the promising results of how learning-based
fault localization framework works, and make it extensible for sup-
porting more types of faults.

Step 3: Fault Localization. After acquiring the diagnosed types
of faults, DeepFD performs fault localization at the program level.
Specifically, the program is first parsed into its abstract syntax
tree (AST), and DeepFD goes through the nodes of the parse tree,
traverses assignment statement as well as expressions, and then
identifies the places (i.e. lines) where the diagnosed types of faults
are defined. For example, to localize the optimizer in the source
code, DeepFD looks for invocations to specific model training APIs
(i.e., fit), and parses the argument and keywords of this node, and
finally returns the line number where the optimizer is assigned.
However, this process is not always as easy as keyword identifica-
tion. For example, the learning rate, as a hyperparameter of the
optimizer, is usually omitted in the program (i.e. the default learn-
ing rate will be used), making keyword identification infeasible. In
this case, DeepFD locates to the line where the optimizer is defined,
while reporting the fault is in the type of learning rate, which can
provide a more accurate debugging information for the developers.
Since a fault may involve multiple lines, DeepFD reports a set of
suspicious lines of code for each fault diagnosed.

3.2 Diagnosis Model Construction
Two decisions are to be made in constructing diagnosis models.
First, we need to choose which machine learning algorithms to
construct the models. We choose K-Nearest Neighbors [7, 22, 34,
48], Decision Tree [45, 52, 53] and Random Forest [14, 23, 24])
to construct three diagnosis models. These three algorithms are

Table 3: Statistical Operators for Runtime Data Aggregation.

Operators
max
min
median
mean
var
std
skew
sem

Description
The maximum value in a feature trace.
The maximum value in a feature trace.
The median value of a feature trace.
The mean value of a feature trace.
The variance of a feature trace.
The standard deviation of a feature trace.
The skewness of a feature trace.
The standard error of mean of a feature trace.

chosen because of their wide adoption, explainability and simplicity.
Second, we need to decide how to aggregate their results. We choose
to union their results to maximize the number of faults diagnosed.
We will evaluate the individual effectiveness of the three diagnosis
models in Section 5.

The three diagnosis models are used in the second step (i.e. fault
diagnosis) of DeepFD, serving as the classifiers mapping diagnostic
features into different types of faults that might exist. The diagnosis
models are constructed in three steps as shown in Figure 3.

Step 1: Fault Seeding. This step is to prepare sufficient training
samples for the diagnosis models. Since there is no off-the-shelf
training set for this purpose, we are inspired by the idea of fault
seeding in mutation testing [33] to seed faults into normal pro-
grams. However, fault seeding for DL programs needs to address
multiple challenges. First, what types of faults should we seed?
Though there are several existing empirical studies [27, 30, 31, 71],
most of the collected buggy programs are not reproducible because
of incomplete or missing code snippets, unavailable training sets,
or program crashes. Without reproduction, inappropriate plausible
fixes may be included, inducing noise to our study. To address this
challenge, we constructed a benchmark of 58 buggy DL programs
by reproducing the failures of the DL program faults collected by

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

CAO, et al.

Figure 3: The Workflow of Diagnosis Models Construction.

recent empirical studies [27, 30, 31, 58] from Github and Stack-
Overflow. For each reproduced faulty program, we investigated
its characteristics, including the number of faults, the fault types,
as well as the code differences between the faulty version and its
fixed version. Furthermore, we found that a faulty DL program
often contain faults of more than one type. Since most statements
are involved in a DL program execution, the effects of multiple
faults in a faulty execution can easily cascade. To mimic the situa-
tion, we randomly seed up to five types of faults in each program
mutant. Second, how to seed concrete faults for a specific type of
fault? Adapting DeepCrime [28], we designed seven fault-seeding
mutation operators in Table 4 for the five fault types.

We explain the design of these operators and their differences
from existing works. The first two mutation operators target at
loss functions. Loss functions measure the difference between the
ground truth and the predicted values, which can be further divided
into probabilistic loss functions and regression ones, suiting for
classification and regression tasks, respectively. When mutating
the loss functions, DeepCrime [28] randomly picks one from all
the other available loss functions regardless of which category the
loss function is. On the contrary, we first find out the category of
loss used by the given DL program, and then randomly select one
from another category. It is because losses from another category
are more likely to be unsuitable for the original task, and thus the
generated mutant is more likely to be faulty. Next, to seed faults
in the learning rate setting, we increase or decrease the learning
rate, deliberately setting them to an extremely large or small value.
While DeepCrime only consider setting learning rates to extremely
small values. Furthermore, for the training epochs, we assign the
epoch to a small value by randomly dividing the current number of
epoch with a random number within 10 to 50, aiming at generating
a small enough epoch. While the existing work only generate a
random number, which may be even larger than the current number
of epochs. Finally, for the activation function and optimization
function, we randomly choose another function from the available
function sets apart from the original one. Note that though existing
works [28, 32] can seed more faults, their works are designed for
mutation testing, which serves a different purpose from that of
DeepFD. The fault seeding step in DeepFD serves as a preparation
for the diagnosis model training to perform the final fault diagnosis
and localization. It is also extensible to support seeding more types
of faults based on our framework.

Step 2: Training Set Preparation. In the previous step, a set of
mutants (i.e. fault-seeded programs) are generated. However, not
all the mutants are necessarily faulty. Due to the randomness of
DNN training, slight variations of DNNs’ performance 4 are natural.
Simply considering a DNN with slightly varied performance as
faulty ignores the nature of randomness, and may induce many
false alarms. We present the criteria of faulty performance and
the procedure of seeding multiple faults. To determine whether
a mutant is faulty, we first check whether there is a significant
statistical difference between the distribution of the original DNN’s
accuracy (i.e. 𝐴DO ) and that of its mutant 𝐴DM as adopted the
following equation [28, 32]:

𝑡𝑟𝑢𝑒

𝑓 𝑎𝑙𝑠𝑒





𝑖𝑠𝐾𝑖𝑙𝑙 (N, M, 𝑋 ) =

if effectSize(𝐴DO (𝑋 ), 𝐴DM (𝑋 )) ≥ 𝛽
& pValue(𝐴DO (𝑋 ), 𝐴DM (𝑋 )) < 𝛼



(1)
where 𝛼 and 𝛽 are thresholds that control the statistical significance
and effect size, and 𝑋 represents the testing set. Specifically, to
quantify the statistical significance and the effect size, we adopted
the generalised linear model (GLM) [46] and Cohen’s d [17]. If
the distribution of the mutant’s accuracy is statistically different
from that of the original model, then we further check whether the
average accuracy of the mutant is worse than its original one. If the
above two requirements are satisfied at the same time, the mutant
is considered as faulty, and is labeled as the types of faults that has
been seeded. If not, the mutant is regarded as non-buggy.

Furthermore, for mutants with more than one fault, another
challenge is how to determine whether all these seeded faults are
the root causes leading to the deteriorated performance? We ad-
dress this challenge by seeding faults one by one. Specifically, after
obtaining a mutant with one type of fault, we then try to seed a
second fault that is independent of the first one, and use Equation 1
to check whether the second fault is successfully seeded. We repeat
the above process to inject the third fault and so on. The process
iterates until up to five types of faults are seeded in one original
program. Finally, we run all these generated mutants together with
the original programs to collect the diagnostic features as described
in the Step 1 of Section 3.1. These diagnostic features and their
labels are then used to train the diagnosis models of DeepFD.

4In this paper, we use “performance” to refer the accuracy of loss of a DNN model, as
used in the existing work[67]

FaultLocalizationFaultDiagnosisDiagnosticFeatureCollectionDNN TrainingFaultyLinesDLProgramUnderTestRuntimeInfo•Gradient•Loss•…DiagnosedTypesofFaultsFault(s)Found?NYPretrainedDiagnosisModelsDiagnosticFeaturesDNNModelTrainedDNNModelInformationExtractionandProcessProgramAnalysisLabeledbySeeded FaultsLabelingFault SeedingTraining Set PreparationDiagnosisModelTrainingMutantFaulty?YNLabeledasCorrectRuntime Info•Gradient•Loss•…DNNTrainingInformationExtractionandProcessDiagnosticFeaturesDNNModelTrainedDNNModelTrainingSetDiagnosisModelsFaults to be SeededOriginalProgramConfigurations•Loss•Activation•Optimizer•…Seed FaultsDeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

Table 4: Seeded Faults and the Corresponding Fix Patterns

Mutation Operator

Target

Search Range

Change loss function to those for
classification problems

Change loss function to those for
regression-based problems

Loss Function

Enumerate

Loss Function

Enumerate

Change activation function in layers Activation Function

Enumerate

Parameter
“categorical_crossentropy”,
“sparse_categorical_crossentropy”,
“binary_crossentropy”
“mean_absolute_error”,
“mean_absolute_percentage_error”,
“mean_squared_error”
“relu”,“sigmoid”,“softmax”,“softplus”,
“softsign”,“tanh”,“selu”,“elu”,“linear”

Activation

Decrease number of epoch

Epoch

Range

originEpoch / random(10,50)

Iterations

Change optimization function

Optimization Function

Enumerate

“SGD”, “RMSprop”, “Adam”,
“Adadelta”, “Adagrad”

Decrease learning rate to an extreme
small value

Increase learning rate to an extreme
large value

Learning Rate

Range

[1e-16, 1e-10]

Learning Rate

Range

[1, 10]

Optimizer

Change neural
architecture

Change neural
architecture

Fix Patterns

Description

Loss Function

This group of fixes adjusts the loss function which helps
the training process to identify the deviation from the
learned and actual examples.

This group of fixes changes the activation function
used in a layer to better match the problem.
This group of fixes adjusts the number
of times the training process will be run.
This group of fixes modifies the optimization algorithms
used by the DNN model.
This group of fixes overhauls the design of
the DNN’s architecture including a new set
of layers and hyperparameters.
This group of fixes overhauls the design of
the DNN’s architecture including a new set
of layers and hyperparameters.

Step 3: Diagnosis Model Training. We treat the fault diagnosis
as a multi-label classification problem mapping a faulty program
to the multiple types of faults that it contains. We construct three
diagnosis models using the three widely-used and effective ma-
chine learning algorithms (i.e., K-Nearest Neighbors [7, 22, 34, 48],
Decision Tree [45, 52, 53] and Random Forest [14, 23, 24]) to learn
the correlations between diagnostic features and types of faults.
Specifically, the K-Nearest Neighbor algorithm assumes that sim-
ilar samples exist in close proximity. It clusters samples into K
groups according to the distance between samples. The decision
tree algorithm predicts the value of a target variable by learning
simple decision rules inferred from the features. The random for-
est algorithm is an ensemble learning method, which operates by
constructing a multitude of decision trees during training and out-
putting the result that is favored by most of the decision trees. Since
a sample can have multiple labels, we adopt the multi-label version
of these algorithms [56, 60, 68], which classifies a given sample into
a set of target labels. Finally, DeepFD trains three diagnosis models
against the constructed training set using these algorithms.

4 BENCHMARK CONSTRUCTION
To investigate the characteristics of faults in real faulty programs,
and to evaluate the effectiveness of DeepFD, we build a benchmark
with 58 buggy DL programs from StackOverflow and GitHub. The
benchmark includes the artifacts required to reproduce the bugs.
In this section, we elaborate how we construct the benchmark.

Subject Collection and Selection. We collect the benchmark
in two steps. First, we revisit all the benchmarks collected by previ-
ous studies [27, 30, 31, 58], e.g., 3,003 posts from StackOverflow and
2,328 commits from GitHub in total. Besides, in order to cover the
recently posted issues that are not included by the previous studies,
we also search StackOverflow for recent issues following the same
selection criteria as specified in [31]. Specifically, we collect the
posts from StackOverflow with accepted answers with the score
greater or equal to 5, and the posted time ranges from March, 2019
till April, 2021. Then, we select the subject implemented by Keras
and the symptom of the program is poor performance (i.e., the
program exhibits poor accuracy, loss during the training process).

We select Keras because 46.4% of the posts and commits concern
DL programs implemented on Keras. We select programs with poor
performance since it is the most common symptom apart from crash
(i.e., the program raises an exception or crash) [71]. We exclude the
crashed programs because they are uncommon for those written
by experienced DL developers [27]. For posts in StackOverflow,
we exclude those without accepted answers and without source
code or the training dataset since we cannot reproduce without the
concerned issues. For a similar reason, we exclude those GitHub
projects that do not have training sets available.

Reproduction and Repair. We reproduce and repair the col-
lected subjects with the following procedures. (1) We run the source
code to see whether it is executable. If the source code crashes due
to API upgrade, versioning or typo issues, we fix these issues; oth-
erwise we skip this subject. (2) We examine whether the output
of the source code exhibits the same symptom as described in the
post. For GitHub commits, if a commit message does not describe
the symptom, we capture the symptom by running the program,
comparing the results before and after applying the commit. If the
performance increases after applying the commit, then we take this
commit into account. (3) To fix the program, we adopt the patches
as suggested by the accepted answers in StackOverflow, as well as
other patches recommended in other replies. For GitHub commits,
we repair the program according to the committed changes. Finally,
we obtain a benchmark with 58 faulty DNN programs, along with
(1) the patches, usually more than one; (2) the types of faults and (3)
line numbers where the faults are introduced. If a fault (e.g., adding
more layers) is not localized down to a specific piece of code, we
recorded the line numbers whether the patch should be added. Our
benchmark is made publicly available [15].

Statistics of the Benchmark. The statistics of our benchmark
are given in Figure 4. In the left part of the Figure, we present the
number of each type of faults in the benchmark. In particular, the
taxonomy of faults follows the existing study [27], including the ac-
tivation function, optimizer, loss function, hyperparameters (includ-
ing the suboptimal learning rate, the suboptimal number of epochs,
and the suboptimal batch size), training data quality, model type
and properties, layer properties, missing/redundant/wrong layer,

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

CAO, et al.

Figure 4: The Statistics of Benchmark.

missing preprocess, and wrong preprocessing. The top five types of
faults (i.e., activation function, suboptimal number of epoch, subop-
timal learning rate, loss function and optimizer) are highlighted in
yellow, accounting for the majority (73.55%) types of faults in our
benchmark. In addition, we also realized that one program usually
contains more than one types of faults, so we further present the
statistics of the number of fault types (in blue) and number of faulty
lines (in red) in one program. Note that if one type of fault occurs
in multiple places in the program (e.g., the activation function in all
the layers are inappropriately set), we only count one since these
faults belong to the same type. Similarly, one faulty line of code
is counted regardless of how many faults contained in this line.
For the cases with missing layers or missing data preprocessing,
we regard their number of faulty lines as one. From the second
diagram of Figure 4, we can see that over half of the programs in
the benchmark contain more than one fault type, and most of them
involve more than one line.

5 EVALUATION
We study three research questions to evaluate the relevancy of
diagnostic features and the effectivness of DeepFD in this section.
• RQ1. Are the extracted features relevant to fault diagno-
sis? To investigate whether the diagnostic features have high
correlations with certain types of faults, we apply these features
to perform fault diagnosis on the generated training sets to eval-
uate the relevancy of such extracted features based on the results.
The higher relevancy, the more accurate the diagnosis will be.
• RQ2. How effective is DeepFD in fault diagnosis? To evalu-
ate the effectiveness of DeepFD on fault diagnosis, we compared
DeepFD with existing works on the benchmark in terms of the
number of identified and correctly diagnosed faulty cases. We
also showed the diagnosis information reported by each work.
• RQ3. How effective is DeepFD for fault localization? Accu-
rate fault localization is an important step towards automated
program repair. So we further evaluate the effectiveness of fault
localization on the benchmark and compare it with the existing
works.

5.1 Experiment Setup
We implemented DeepFD in Python, and conducted experiments
on a machine with Intel i7-8700K CPU and Nvidia GeForce Titan V
12GB VRAM. For mutation generation, we run each DNN model 20
times to obtain a reliable statistical results. To address the random-
ness in DNN training, we run each experiment 10 times. For the

thresholds in DeepFD, we set 𝛼 and 𝛽 to be 0.2 and 0.05, respectively,
following the settings in [73]. We used their default parameters
in the diagnosis models. We did not fine-tune the parameters for
better performance to facilitate reproducibility. In addition, param-
eter tuning is not the theme of this work. We collected runtime
data using the same default interval as in prior work [70]. The
experimental results are made available for validation [15].

Datasets and Original DL Programs. We performed our eval-
uation on six popular datasets: Circle [4], Blob [3], MNIST [36],
CIFAR-10 [35], IMDB [41] and Reuters [5]. Circle and Blob are
two datasets from Sklearn [2] for classification tasks. MNIST is a
gray-scale image dataset used for handwritten digit recognition.
CIFAR10 is a colored image dataset used for object recognition.
IMDB is a movie review dataset for sentiment analysis. Reuters is a
newswire dataset for document classification. For the DL programs,
we use the programs published by a recent work [70] as the original
programs for mutant generation. Specifically, the work [70] pro-
vided 495 DNN models and their training scripts of various DNN
model structures (convolutional neural network, recurrent neural
network and fully connected layers only) for these six datasets.
Among them, we were able to reproduce the training of 233 DNN
models. Therefore, we used these 233 DNN models as the original
models to generate mutants, following the process described in
Section 3.2. The statistics of generated mutants is shown in Table 5
(Entry ‘Mutant’). For RQ1, the training set and validation set were
split in a proportion of 7:3. The evaluation was conducted on the
validation set.

Baselines. In the experiment, we compared DeepFD with three
baselines: UMLAUT [54], AutoTrainer [70] and DeepLocalize [58].
Specifically, the first baseline, UMLAUT [54], is a heuristic-based
framework providing an interface to debug faults in data prepara-
tion, model architecture and parameter tuning. In particular, UM-
LAUT allows users to check the structure of deep learning programs,
model behavior during training. After detecting faults, it provides
a set of human-readable error messages and repair advice. The
second baseline, AutoTrainer, is designed for detecting five types of
training problems (i.e., Gradient Vanish, Gradient Explode, Dying
ReLU, Oscillating Loss and Slow Convergence). We use its default
parameters in our evaluation. The third baseline, DeepLocalize, is
able to identify the faulty layers when numerical error occurs. Both
approaches monitor the runtime information (e.g., weight and gra-
dient) during training, and report training problems or faulty layers
once detected. Besides, DeepLocalize performs the fault localiza-
tion using two methods: translating the code into an imperative

2518181711975434ActivationEpochLearning RateLossOptimizerMissing prepMissing/Wrong LayersLayer PropertiesBatch_sizeWrong prepOtherNumber of Types of Faults241498201916115240510152025123455+NumberofProgramsNumber ofTypesofFaults(Faultylines)Faulty LinesTypes of Faults25181817119754322ActivationEpochLearning RateLossOptimizerMissing prepM/R/W LayersLayer PropertiesBatch_sizeWrong prepData qualityModel T&PDeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

representation manually or inserting customized callback functions.
We used their second method for comparison because the second
one achieves better performance than the first method according to
their paper. Although these baselines do not explicitly point out the
fault types they addressed, the fault types can still be mapped from
the patches or faulty layers they provided to the fault types. After
the mapping, the fault types appearing in the evaluation buggy
programs can be covered by the baselines.

The three baselines were not originally designed for fault local-
ization. They do not locate faulty lines down to program code. To
measure their effectiveness for fault localization, we adapt their
diagnosis results as follows. For UMLAUT, we manually identify the
lines of code relating to the error messages. For AutoTrainer, though
it does not explicitly locate the faults, it tries to repair when training
problems happens by applying predefined solutions. Therefore, we
regard the location of the solutions adopted by AutoTrainer as the
fault it located, and manually identify the lines of code relating to
the repaired artifact (e.g. learning rate, layers’ initialization) to the
corresponding lines. For DeepLocalize, we map the reported layer
to the corresponding lines in the program. And because of the man-
ual work that is involved in both fault diagnosis and localization
stages, we do not compare the overall runtime of each stage.

Evaluation Criteria. We adopt the following criteria in decid-
ing whether a fault is successfully detected, diagnosed and localized.
A fault is successfully detected if the detection result corresponds
to the existence of a real fault, regardless of whether its root causes
(i.e. types of faults) has been identified. A fault is successfully di-
agnosed if its root cause has been identified. If there are multiple
types of faults, a successful fault diagnosis refers to those that can
pinpoint at least one of them. For fault localization, we examine
whether any of the faulty lines is correctly located after a correct
diagnosis.

5.2 RQ1. Relevancy of Diagnostic Features
We evaluate the relevancy of the extracted diagnostic features by
showing the accuracy of the diagnosis models trained with these
features. If the models perform well when using these features, then
we regard these diagnostic features as relevant to fault diagnosis.
The experimental results are shown in Table 5. Before analyzing the
prediction result, we first demonstrate the statistics of the mutants
generated in the first step of the Bootstrap stage. Specifically, for
each dataset, we list the number of normal DNN models (Origin), the
number of generated mutants (Mutant), the detailed distribution of
how many mutation operators are applied to the generated mutants
(column “Mutation Operator”) and the average time for training
each mutant in seconds (Time).

On top of this training set, we further trained diagnosis mod-
els with three underlying algorithms (i.e., KNN, DT and RF are
abbreviations of K-Nearest Neighbors, decision tree and random
forest), and demonstrated the accuracy of these models. In the
implementation, we normalized the features to better fit the K-
Nearest-Neighbors algorithm. As shown in the entry of “Accuracy
and Average Runtime of Diagnosis Models” in Table 5, the average
accuracy of these diagnosis models range from 70.68% to 79.90%
over different datasets. Besides, the accuracy obtained varies cross
datasets, ranging from 45.2% to 93.3%. Among all these datasets,

the best performance of three underlying algorithms is achieved
on IMDB dataset, with the accuracy varying from 88.8% to 93.3%.
On the other hand, the accuracy on Circle and CIFAR-10 tend to
be worse. The result may be caused by the unbalanced mutants
with different numbers of faulty cases. The various performance
of different underlying algorithms on datasets also suggest us to
aggregate the diagnosis decisions by taking advantage of different
diagnosis models.

Answer to RQ1: The extracted diagnostic features have strong
correlations with our targeted five fault types, which is reflected
by the accuracy of diagnosis models, with an average accuracy
varying from 70.68% to 79.90%, and up to 93.30% for certain
cases.

5.3 RQ2. Effectiveness of Fault Diagnosis.
To answer RQ2, we evaluate the effectiveness of DeepFD in terms
of fault diagnosis on the 52 buggy programs 5 from the benchmark.
Note that the subjects used for RQ2 and RQ3 do not overlap with
those used in RQ1. Due to space limitation, Table 6 shows partially
our evaluation results, 6 including the diagnosis information pro-
vided by each method (column ‘Diagnosis Information of Different
Methods’), whether they detect the existence of faults (column
‘Fault Detection’) and whether the fault diagnosis is correct or not
(column ‘Fault Diagnosis’). UMLAUT enumerates error messages
with three severity levels (i.e., Error, Critical and Warning). Au-
toTrainer reports the training problem. DeepLocalize reports the
place (e.g., layer) and the batch at which the predefined symptom
occurs. DeepFD reports the diagnosed fault types.

The ‘Summary’ and ‘Overall Ratio’ of Table 6 summarize the
results of each work on the benchmark. UMLAUT, DeepLocalize
and DeepFD can detect 69% to 96% faulty cases. Although UMLAUT
detects the most number (50) of faulty cases, only 14 of them are
correctly diagnosed. In contrast, DeepFD is able to detect 47 faulty
cases and correctly diagnosed 27 of them.

There are different reasons for inaccurate diagnosis. UMLAUT
is designed for classification problems. It assumes that the output
layer is always activated by Softmax. If this is not the case, it re-
ports “Missing Softmax layer before loss”. However, this assumption
does not necessarily hold. Activation functions like Linear and
Sigmoid are also frequently used to activate the output layer for
classification and regression problems. Indeed, most of the faults
detected by UMLAUT are attributed to the violation of this as-
sumption, causing many false alarms. AutoTrainer only detects five
training problems (e.g. dying ReLU), leaving most cases without ap-
parent symptoms escaped from its detection. DeepLocalize detects
faults that cause numerical anomalies such as NaN, and reports the
place where the anomalies happen. Yet, anomalies rarely occur at
the point where the faults reside, making the fault diagnosis by
DeepLocalize inaccurate. On the other hand, the effectiveness of
DeepFD is limited by the number of fault types that can be classified
by the diagnosis models. With more types of faults seeded, more

5The rest six programs were omitted because they either were unable to be adapted to
launch the methods, or crashed when the methods are applied.
6We listed the cases that can be correctly diagnosed by at least three methods among
UMLAUT, AutoTrainer, DeepLocalize and DeepFD. The complete experimental results
are online available [15]

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

CAO, et al.

Table 5: Effectiveness of Diagnosis Models of DeepFD on Labeled DNN Set. KNN, DT and RF stand for the underlying algorithms
(K-Nearest Neighbors, Decision Tree and Random Forest) of diagnosis models.

Dataset
MNIST
CIFAR-10
Circle
Blob
Reuters
IMDB
Total

Origin Mutant

78
35
36
39
32
13
233

1768
786
936
685
175
110
4,460

1
1027
651
580
335
138
76
2,807

Statistics
# Types of Faults
4
134
0
44
50
12
0
240

3
295
33
133
137
3
9
610

2
302
102
174
158
22
25
783

Time

5
10
0
5
5
0
0
20 Average

0.60
1.56
8.87
0.13
36.30
53.72

RF

KNN

Accuracy and Average Runtime of Diagnosis Models
DT
Acc (%) Time (s) Acc (%) Time (s) Acc(%) Time (s)
69.29
52.63
45.20
83.95
79.69
93.30
70.68

81.58
64.47
61.36
87.65
81.25
93.30
78.27

63.15
53.94
55.50
79.01
79.69
88.80
79.90

0.10
0.05
0.23
0.06
0.05
0.04
0.09

0.01
0.02
0.03
0.01
0.01
0.03
0.02

0.12
0.12
0.22
0.10
0.09
0.09
0.12

faults can be detected and diagnosed, and thus the effectiveness of
DeepFD can be improved.

Answer to RQ2: DeepFD outperforms the existing works on
fault diagnosis, with a fault diagnosis rate of 52%, which almost
doubles that of the best baseline (i.e., 27%).

5.4 RQ3. Effectiveness of Fault Localization
The ‘Fault Diagnosis’ column in Table 6 gives the evaluation re-
sult of fault localization. DeepFD outperforms other methods by
correctly locating 42% of faults, which favorably compares to the
23% by UMLAUT, the best baseline performer. The performance of
AutoTrainer and DeepLocalize are relatively unsatisfactory, ranging
from 10% to 15%.

Indeed, the performance on fault localization are not satisfactory,
ranging from 10% to 42%. The reasons behind are mainly two folds.
First, the ratio of correctly diagnosed cases are not high, which is at
most 52%. Second, considering the complicated types of faults and
possible multiple patches, an effective localization strategy is to be
explored. For example, suppose we know the root cause is the use
of inappropriate activation, yet there are multiple activation func-
tions in several layers. Consequently, locating faults to a specific
activation function at a specific layer is an outstanding challenge.

Answer to RQ3: The results show that DeepFD significantly
outperforms the three baselines by correctly locating 42% of the
cases. In comparison, only 10% to 23% cases can be located by
the baselines.

6 RELATED WORK
6.1 Debugging Deep Learning Systems
Recently, a branch of techniques have been proposed to facilitate
the debugging process of deep learning software systems. Several
works focus on the DNN models, locating suspicious neurons and
correcting them by prioritizing or augmenting the training data.
For instance, Ma et al. [40] leveraged the state differential analysis
to identify the features that are responsible for the incorrect classifi-
cation results, and then generate the inputs based on these features
to correct the behaviours of the DNN models. Eniser et al. [20] pro-
posed DeepFault, a framework to identify the suspicious neurons

in a DNN and then fix these errors by generating samples for model
retraining. Apricot [66] is a weight-adaption approach to fix DNN
models. Their intuition is that the limitation of the complete models
may be addressed via adapting the weight of the compact models.
These previous studies concentrate on the the DNN models and
they are not able to locate the faulty lines in DNN programs.

Besides, there are several studies that are closer to ours. For
instance, AutoTrainer [70] proposed an automatic approach to de-
tect and fix the training problems in DNN programs at runtime.
It particularly focuses on detecting and repairing five common
training problems: gradient vanish, gradient explode, dying ReLU,
oscillating loss, and slow convergence. It encapsulates and auto-
mates the detecting and repairing process by dynamic monitoring.
However, it relies on predefined rules to perform the bug detection.
Wardat et al. [58] proposed DeepLocalize, the first fault localization
approach for DNN models (such as incorrect learning rate or inap-
propriate loss function). Yet, it locates to layers where the symptom
happens instead of where the fault’s root cause resides. Amazon
SageMaker Debugger [50] and UMLAUT [54] both provide a set of
built-in heuristics to debug faults in DNN models during training.
The major difference between our work from these methods is that
DeepFD does not rely on predefined rules, making it more flexible
and adaptive for various types of faults.

6.2 Automated Machine Learning
Automated Machine Learning (AutoML) provides methods and
processes to make machine learning available for non-Machine
Learning experts. The user simply provides data, and the AutoML
system automatically determines the approach that performs the
best for a particular application [29]. In particular, there are three
main problems in AutoML, including Hyperparameter Optimiza-
tion (HPO), Neural Architecture Search (NAS), and meta-learning.
HPO [49, 64, 69] search for methods to set optimal hyperparam-
eters in ML algorithms automatically, thus reducing the human
efforts necessary for applying ML. Yet, it is not always clear which
hyperparameters of an algorithm need to be optimized, and in
which ranges [29]. In contrast, DeepFD differs from HPO in nature.
DeepFD diagnoses and locates faults in a given model, while the
HPO methods search an optimal model from scratch. NAS meth-
ods [19, 37, 38, 43, 47, 51, 72] are designed to automatically design
more complex neural architectures. Meta-Learning [12, 13, 16, 18,

DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

Table 6: Comparison on Diagnosis Information, Fault Detection, Diagnosis and Localization Between UMLAUT (UT), Auto-
Trainer (AT), DeepLocalize (DLoc) and DeepFD (DFD).

Post #

UMLAUT

Diagnosis Information of Different Methods
DeepLocalize

AutoTrainer

48385830

(1) Critical: Missing Softmax layer before loss
(2) Warning: Possible overfitting

explode

Layer-1 Error in forward
Stop at epoch 1, batch 2
Accuracy: 0.13, loss: inf

DeepFD

Fault 1: [act] (Lines: [-])
Fault 2: [loss] (Lines: [57])

Fault Detection

Fault Diagnosis
UT AT DLoc DFD UT AT DLoc DFD UT AT DLoc DFD

Fault Localization

✔ ✔

✔

✔ ✔ ✔

✔

✔ ✔ ✔

✕

✔

(1) Error: Input data exceeds typical limits
(2) Warning: Possible overfitting
(3) Warning: Check validation accuracy
(4) Critical: Missing Softmax layer before loss
(5) Critical: Missing activation functions
(6) Warning: Last model layer has nonlinear activation
(1) Critical: Missing Softmax layer before loss
(2) Warning: Last model layer has nonlinear activation
(1) Critical: Missing Softmax layer before loss
(2) Critical: Missing activation functions
(3) Warning: Last model layer has nonlinear activation

55328966

34311586

50079585

explode

–

Fault 1: [opt] (Lines: [49])

✔ ✔

✕

✔ ✔ ✔

✕

✔ ✔ ✔

✕

✔

–

Batch 0 layer 2: Error in delta
Weights, terminating training

Fault 1: [lr] (Lines: [27])

✔ ✕

✔

✔ ✔ ✕

✔

✔ ✔ ✕

✔

✔

unstable

–

Fault 1: [lr] (Lines: [44])
Fault 2: [epoch] (Lines: [15])

✔ ✔

✕

✔ ✔ ✔

✕

✔ ✔ ✔

✕

✔

47352366

(1) Critical: Missing Softmax layer before loss
(2) Warning: Last model layer has nonlinear activation

explode

Layer-12 Error in delta weights
Stop at epoch 1, batch 24
accuracy: 0.31, loss: 6.16

Fault 1: [opt] (Lines: [40])

✔ ✔

✔

✔ ✔ ✔

✔

✕ ✔ ✔

✔

✕

59282996

37624102

41600519

(1) Error: Input data exceeds typical limits
(2) Warning: Check validation accuracy
(3) Critical: Missing Softmax layer before loss
(1) Critical: Missing Softmax layer before loss
(2) Critical: Missing activation functions
(3) Warning: Last model layer has nonlinear activation
(4) Error: Image data may have incorrect shape
(5) Warning: Learning Rate is high
(6) Warning: Check validation accuracy
(1) Error: Input data exceeds typical limits
(2) Critical: Missing Softmax layer before loss
(3) Warning: Last model layer has nonlinear activation

unstable

–

Fault 1: [epoch] (Lines: [309])

✔ ✔

✕

✔ ✔ ✔

✕

✔ ✔ ✔

✕

✔

unstable

Batch 0 layer 9: Error in Output
Gradient, terminating training

Fault 1: [lr] (Lines: [66])
Fault 2: [Act] (Lines: [54, 56, 61, 64])

✔ ✔

✔

✔ ✔ ✔

✕

✔ ✕ ✔

✕

✔

unstable

Batch 0 layer 6: Error in forward,
terminating training

Fault 1: [loss] (Lines: [32])

✔ ✔

✔

✔ ✕ ✔

✔

✔ ✕ ✔

✔

✔

......

Summary

12
Overall Ratio 0.96 0.23

50

36
0.69

47

10
14
0.90 0.27 0.19

7

27

8
0.13 0.52 0.23 0.15

12

5

22
0.10 0.42

21, 42] aims to improve learning across different tasks or datasets.
It can significantly improve the efficiency of HPO and NAS with
the help of the transferred knowledge between tasks. Note that
though these methods are not designed for debugging an existing
program, it is potential to apply these methods, especially the HPO
and NAS ones for repairing after faults are diagnosed and localized.
The diagnosed information can serve as a guidance, narrowing
down the parameters that need to be tuned, and the search space
for NAS to explore.

7 THREATS TO VALIDITY
In this section, we discuss three threats that may affect the validity
of our work. First, the construction of benchmark (e.g. the repro-
duction, root cause analysis and repair) involved manual inspection
of the source code, which may be subjective. To reduce this threat,
each subject was examined by three authors separately and the
results were cross-validated. Decisions were made only if the three
authors reached an agreement. Second, to prepare the training set
for diagnosis models, we assume the original programs (i.e. pro-
grams before seeding faults) are fault-free, yet it may not always be
the truth. Though they are released [70] and guaranteed to be free
from five training problems (e.g. gradient vanish and dying ReLU),
it is still possible there are hidden faults in the program. Third, to
reproduce the results of existing works, we adopt the default values
for the parameters, which may affect their performance and effi-
ciency. Besides, some works need to manually adapt the programs
in order to launch the debugging process, which may introduce
unexpected variance from the original program. Also, with the man-
ual work involved, the time cost by each work is hard to evaluate,

leaving the efficiency of each work incomparable. Finally, since
existing works cannot locate faults to the program, we carefully
investigate their diagnosis information, and manually locate the
faulty lines for fair comparisons, which may also slightly affect the
comparison results.

8 CONCLUSION
In this paper, we proposed DeepFD, a learning-based fault diagnosis
and localization framework which maps the fault localization task
to a learning problem. In particular, DeepFD diagnosis faults by
classifying runtime features into possible types of faults (e.g. inap-
propriate optimizer), then locates faulty lines to the program. The
evaluation shows the potential of DeepFD. Specifically, it correctly
diagnoses 52% of the cases, compared with half (27%) achieved
by the best state-of-the-art works. Besides, for fault localization,
DeepFD also outperforms the existing works, correctly locating
42% faulty cases on the benchmark, which almost doubles the result
(23%) achieved by the best state-of-the-art work.

ACKNOWLEDGMENT
This work was supported by the National Key Research and Devel-
opment Program of China, No. 2019YFE0198100, National Natural
Science Foundation of China (Grant Nos. 61932021, 62002125), the
Hong Kong RGC/GRF (Grant No. 16207120), Hong Kong ITF (Grant
No. MHP/055/19), Huawei PhD Fellowship, and MSRA Collabo-
rative Research Grant. The authors would also like to thank the
anonymous reviewers for their comments and suggestions.

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

CAO, et al.

REFERENCES
[1] 2020. Convolutional neural networks for visual recognition. https://cs231n.

github.io/neural-networks-3/

[2] 2020. scikit-learn, machine learning in python. https://scikit-learn.org/stable/
https://scikit-learn.org/stable/modules/
[3] 2020. Sklearn, make blobs dataset.

generated/sklearn.datasets.make_blobs.html

[4] 2020. Sklearn, make circles dataset. https://scikit-learn.org/stable/modules/

generated/sklearn.datasets.make_circles.html

[5] 2021. Reuters-21578 Text Categorization Collection Data Set. https://archive.ics.

uci.edu/ml/datasets/reuters-21578+text+categorization+collection

[6] Rui Abreu, Peter Zoeteweij, Rob Golsteijn, and Arjan J. C. van Gemund. 2009.
A practical evaluation of spectrum-based fault localization. J. Syst. Softw. 82, 11
(2009), 1780–1792.

[7] Naomi S Altman. 1992. An introduction to kernel and nearest-neighbor nonpara-

metric regression. The American Statistician 46, 3 (1992), 175–185.

[8] Paul Ammann and Jeff Offutt. 2016. Introduction to software testing. Cambridge

University Press.

[9] Isac Arnekvist, J. Frederico Carvalho, Danica Kragic, and Johannes A. Stork.
2020. The effect of Target Normalization and Momentum on Dying ReLU. CoRR
abs/2005.06195 (2020).

[10] Anders Arpteg, Björn Brinne, Luka Crnkovic-Friis, and Jan Bosch. 2018. Software
Engineering Challenges of Deep Learning. In 44th Euromicro Conference on Soft-
ware Engineering and Advanced Applications, SEAA 2018, Prague, Czech Republic,
August 29-31, 2018, Tomás Bures and Lefteris Angelis (Eds.). IEEE Computer
Society, 50–59.

[11] Shay Artzi, Julian Dolby, Frank Tip, and Marco Pistoia. 2010. Practical fault
localization for dynamic web applications. In Proceedings of the 32nd ACM/IEEE
International Conference on Software Engineering - Volume 1, ICSE 2010, Cape Town,
South Africa, 1-8 May 2010, Jeff Kramer, Judith Bishop, Premkumar T. Devanbu,
and Sebastián Uchitel (Eds.). ACM, 265–274. https://doi.org/10.1145/1806799.
1806840

[12] Besim Bilalli, Alberto Abelló, and Tomàs Aluja-Banet. 2017. On the predictive
power of meta-features in OpenML. Int. J. Appl. Math. Comput. Sci. 27, 4 (2017),
697–712. https://doi.org/10.1515/amcs-2017-0048

[13] Pavel Brazdil, Christophe G. Giraud-Carrier, Carlos Soares, and Ricardo Vilalta.
2009. Metalearning - Applications to Data Mining. Springer. https://doi.org/10.
1007/978-3-540-73263-1

[14] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5–32.
[15] Jialun Cao, Meiziniu Li, Xiao Chen, Ming Wen, Yongqiang Tian, et al. 2022.

DeepFD. https://github.com/ArabelaTso/DeepFD

[16] Ciro Castiello, Giovanna Castellano, and Anna Maria Fanelli. 2005. Meta-data:
Characterization of Input Features for Meta-learning. In Modeling Decisions for
Artificial Intelligence, Second International Conference, MDAI 2005, Tsukuba, Japan,
July 25-27, 2005, Proceedings (Lecture Notes in Computer Science), Vicenç Torra,
Yasuo Narukawa, and Sadaaki Miyamoto (Eds.), Vol. 3558. Springer, 457–468.
https://doi.org/10.1007/11526018_45

[17] Jacob Cohen. 1992. A power primer. Psychological bulletin 112, 1 (1992), 155.
[18] Iddo Drori, Yamuna Krishnamurthy, Remi Rampin, Raoni Lourenço, Jorge One,
et al. 2018. AlphaD3M: Machine learning pipeline synthesis. In AutoML Workshop
at ICML.

[19] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019. Neural Architecture
Search: A Survey. J. Mach. Learn. Res. 20 (2019), 55:1–55:21. http://jmlr.org/
papers/v20/18-598.html

[20] Hasan Ferit Eniser, Simos Gerasimou, and Alper Sen. 2019. DeepFault: Fault
Localization for Deep Neural Networks. In Fundamental Approaches to Software
Engineering - 22nd International Conference, FASE 2019, Held as Part of the European
Joint Conferences on Theory and Practice of Software, ETAPS 2019, Prague, Czech
Republic, April 6-11, 2019, Proceedings (Lecture Notes in Computer Science), Reiner
Hähnle and Wil M. P. van der Aalst (Eds.), Vol. 11424. Springer, 171–191.

[21] Matthias Feurer, Benjamin Letham, and Eytan Bakshy. 2018.

Scalable
CoRR abs/1802.02219 (2018).

Meta-Learning for Bayesian Optimization.
arXiv:1802.02219 http://arxiv.org/abs/1802.02219

[22] Evelyn Fix and Joseph L Hodges Jr. 1952. Discriminatory analysis-nonparametric
discrimination: Small sample performance. Technical Report. CALIFORNIA UNIV
BERKELEY.

[23] Tin Kam Ho. 1995. Random decision forests. In Proceedings of 3rd international

conference on document analysis and recognition, Vol. 1. IEEE, 278–282.

[24] Tin Kam Ho. 1998. The random subspace method for constructing decision
forests. IEEE transactions on pattern analysis and machine intelligence 20, 8 (1998),
832–844.

[25] Sepp Hochreiter. 1991. Untersuchungen zu dynamischen neuronalen Netzen.

Diploma, Technische Universität München 91, 1 (1991).

[26] Qiang Hu, Lei Ma, Xiaofei Xie, Bing Yu, Yang Liu, et al. 2019. DeepMutation++:
A Mutation Testing Framework for Deep Learning Systems. In 34th IEEE/ACM
International Conference on Automated Software Engineering, ASE 2019, San Diego,
CA, USA, November 11-15, 2019. IEEE, 1158–1161.

[27] Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, Andrea
Stocco, et al. 2020. Taxonomy of real faults in deep learning systems. In ICSE

’20: 42nd International Conference on Software Engineering, Gregg Rothermel and
Doo-Hwan Bae (Eds.). ACM, 1110–1121.

[28] Nargiz Humbatova, Gunel Jahangirova, and Paolo Tonella. 2021. DeepCrime:
mutation testing of deep learning systems based on real faults. In ISSTA ’21: 30th
ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual
Event, Denmark, July 11-17, 2021, Cristian Cadar and Xiangyu Zhang (Eds.). ACM,
67–78. https://doi.org/10.1145/3460319.3464825

[29] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (Eds.). 2019. Automated

Machine Learning - Methods, Systems, Challenges. Springer.

[30] Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A com-
prehensive study on deep learning bug characteristics. In Proceedings of the ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia,
August 26-30, 2019, Marlon Dumas, Dietmar Pfahl, Sven Apel, and Alessandra
Russo (Eds.). ACM, 510–520.

[31] Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repairing
deep neural networks: fix patterns and challenges. In ICSE ’20: 42nd International
Conference on Software Engineering, Gregg Rothermel and Doo-Hwan Bae (Eds.).
ACM, 1135–1146.

[32] Gunel Jahangirova and Paolo Tonella. 2020. An Empirical Evaluation of Mutation
Operators for Deep Learning Systems. In 13th IEEE International Conference on
Software Testing, Validation and Verification, ICST 2020, Porto, Portugal, October
24-28, 2020. IEEE, 74–84.

[33] Yue Jia and Mark Harman. 2011. An Analysis and Survey of the Development of

Mutation Testing. IEEE Trans. Software Eng. 37, 5 (2011), 649–678.

[34] Liangxiao Jiang, Zhihua Cai, Dianhong Wang, and Siwei Jiang. 2007. Survey of
improving k-nearest-neighbor for classification. In Fourth international conference
on fuzzy systems and knowledge discovery (FSKD 2007), Vol. 1. IEEE, 679–683.

[35] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features

from tiny images. (2009).

[36] Yann LeCun and Corinna Cortes. 2010. MNIST handwritten digit database.
http://yann.lecun.com/exdb/mnist/. (2010). http://yann.lecun.com/exdb/mnist/
[37] Jason Zhi Liang, Elliot Meyerson, Babak Hodjat, Daniel Fink, Karl Mutch, et al.
2019. Evolutionary neural AutoML for deep learning. In Proceedings of the
Genetic and Evolutionary Computation Conference, GECCO 2019, Prague, Czech
Republic, July 13-17, 2019, Anne Auger and Thomas Stützle (Eds.). ACM, 401–409.
https://doi.org/10.1145/3321707.3321721

[38] Peiye Liu, Bo Wu, Huadong Ma, and Mingoo Seok. 2020. MemNAS: Memory-
Efficient Neural Architecture Search With Grow-Trim Learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
[39] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, et al. 2018. DeepMutation:
Mutation Testing of Deep Learning Systems. In 29th IEEE International Symposium
on Software Reliability Engineering, ISSRE 2018, Memphis, TN, USA, October 15-18,
2018, Sudipto Ghosh, Roberto Natella, Bojan Cukic, Robin S. Poston, and Nuno
Laranjeiro (Eds.). IEEE Computer Society, 100–111.

[40] Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and Ananth Grama.
2018. MODE: automated neural network model debugging via state differential
analysis and input selection. In Proceedings of the 2018 ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, No-
vember 04-09, 2018, Gary T. Leavens, Alessandro Garcia, and Corina S. Pasareanu
(Eds.). ACM, 175–186.

[41] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,
et al. 2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of
the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies. Association for Computational Linguistics, Portland,
Oregon, USA, 142–150. http://www.aclweb.org/anthology/P11-1015

[42] Elan Sopher Markowitz, Keshav Balasubramanian, Mehrnoosh Mirtaheri, Sami
Abu-El-Haija, Bryan Perozzi, et al. 2021. Graph Traversal with Tensor Func-
tionals: A Meta-Algorithm for Scalable Learning. In 9th International Conference
on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net. https://openreview.net/forum?id=6DOZ8XNNfGN

[43] Risto Miikkulainen. 2021. Evolution of neural networks. In GECCO ’21: Genetic
and Evolutionary Computation Conference, Companion Volume, Lille, France, July
10-14, 2021, Krzysztof Krawiec (Ed.). ACM, 426–442. https://doi.org/10.1145/
3449726.3461432

[44] John Miller and Moritz Hardt. 2019. Stable Recurrent Models. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019. OpenReview.net.

[45] Sreerama K Murthy. 1998. Automatic construction of decision trees from data:
A multi-disciplinary survey. Data mining and knowledge discovery 2, 4 (1998),
345–389.

[46] J. A. Nelder and R. W. M. Wedderburn. 1972. Generalized Linear Models. Journal
of the Royal Statistical Society. Series A (General) 135, 3 (1972), 370–384. http:
//www.jstor.org/stable/2344614

[47] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. 2018. Efficient
neural architecture search via parameters sharing. In International Conference on
Machine Learning. PMLR, 4095–4104.

DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs

ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA

[48] Thair Nu Phyu. 2009. Survey of classification techniques in data mining. In
Proceedings of the international multiconference of engineers and computer scientists,
Vol. 1.

[49] Basheer Qolomany, Majdi Maabreh, Ala I. Al-Fuqaha, Ajay Gupta, and Driss
Benhaddou. 2017. Parameters optimization of deep learning models using Particle
swarm optimization. In 13th International Wireless Communications and Mobile
Computing Conference, IWCMC 2017, Valencia, Spain, June 26-30, 2017. IEEE,
1285–1290. https://doi.org/10.1109/IWCMC.2017.7986470

[50] Nathalie Rauschmayr, Vikas Kumar, Rahul Huilgol, Andrea Olgiati, Satadal Bhat-
tacharjee, et al. 2021. Amazon SageMaker Debugger: A System for Real-Time
Insights into Machine Learning Model Training. Proceedings of Machine Learning
and Systems 3 (2021).

[51] Raanan Y. Yehezkel Rohekar, Shami Nisimov, Yaniv Gurwicz, Guy Koren, and Gal
Novik. 2018. Constructing Deep Neural Networks by Bayesian Network Structure
Learning. In Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8,
2018, Montréal, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kris-
ten Grauman, Nicolò Cesa-Bianchi, et al. (Eds.). 3051–3062. https://proceedings.
neurips.cc/paper/2018/hash/95d309f0b035d97f69902e7972c2b2e6-Abstract.html
In Data mining and

[52] Lior Rokach and Oded Maimon. 2005. Decision trees.
knowledge discovery handbook. Springer, 165–192.

[53] S Rasoul Safavian and David Landgrebe. 1991. A survey of decision tree classifier
methodology. IEEE transactions on systems, man, and cybernetics 21, 3 (1991),
660–674.

[54] Eldon Schoop, Forrest Huang, and Bjoern Hartmann. 2021. UMLAUT: Debugging
Deep Learning Programs using Program Structure and Model Behavior. In CHI
’21: CHI Conference on Human Factors in Computing Systems, Virtual Event /
Yokohama, Japan, May 8-13, 2021, Yoshifumi Kitamura, Aaron Quigley, Katherine
Isbister, Takeo Igarashi, Pernille Bjørn, et al. (Eds.). ACM, 310:1–310:16. https:
//doi.org/10.1145/3411764.3445538

[55] David Sussillo and LF Abbott. 2014. Random walk initialization for training very

deep feedforward networks. arXiv preprint arXiv:1412.6558 (2014).

[56] Celine Vens, Jan Struyf, Leander Schietgat, Sašo Džeroski, and Hendrik Blockeel.
2008. Decision trees for hierarchical multi-label classification. Machine learning
73, 2 (2008), 185.

[57] Xinming Wang, Shing-Chi Cheung, Wing Kwong Chan, and Zhenyu Zhang. 2009.
Taming coincidental correctness: Coverage refinement with context patterns to
improve fault localization. In 31st International Conference on Software Engineering,
ICSE 2009, May 16-24, 2009, Vancouver, Canada, Proceedings. IEEE, 45–55.
[58] Mohammad Wardat, Wei Le, and Hridesh Rajan. 2021. DeepLocalize: Fault Local-
ization for Deep Neural Networks. In ICSE’21: The 43nd International Conference
on Software Engineering.

[59] Ming Wen, Rongxin Wu, and Shing-Chi Cheung. 2016. Locus: locating bugs from
software changes. In Proceedings of the 31st IEEE/ACM International Conference
on Automated Software Engineering, ASE 2016, Singapore, September 3-7, 2016,
David Lo, Sven Apel, and Sarfraz Khurshid (Eds.). ACM, 262–273. https://doi.
org/10.1145/2970276.2970359

[60] Qingyao Wu, Mingkui Tan, Hengjie Song, Jian Chen, and Michael K Ng. 2016.
ML-FOREST: A multi-label tree ensemble method for multi-label classification.

IEEE transactions on knowledge and data engineering 28, 10 (2016), 2665–2680.

[61] Rongxin Wu, Hongyu Zhang, Shing-Chi Cheung, and Sunghun Kim. 2014.
CrashLocator: locating crashing faults based on crash stacks. In International
Symposium on Software Testing and Analysis, ISSTA ’14, San Jose, CA, USA - July
21 - 26, 2014, Corina S. Pasareanu and Darko Marinov (Eds.). ACM, 204–214.
[62] Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. 2018. A walk

with sgd. arXiv preprint arXiv:1802.08770 (2018).

[63] Shin Yoo, Mark Harman, and David Clark. 2013. Fault localization prioritization:
Comparing information-theoretic and coverage-based approaches. ACM Trans.
Softw. Eng. Methodol. 22, 3 (2013), 19:1–19:29. https://doi.org/10.1145/2491509.
2491513

[64] Steven R. Young, Derek C. Rose, Thomas P. Karnowski, Seung-Hwan Lim, and
Robert M. Patton. 2015. Optimizing deep learning hyper-parameters through
an evolutionary algorithm. In Proceedings of the Workshop on Machine Learning
in High-Performance Computing Environments, MLHPC 2015, Austin, Texas, USA,
November 15, 2015. ACM, 4:1–4:5. https://doi.org/10.1145/2834892.2834896
[65] Andreas Zeller. 2009. Why Programs Fail - A Guide to Systematic Debugging, 2nd

Edition. Academic Press.

[66] Hao Zhang and W. K. Chan. 2019. Apricot: A Weight-Adaptation Approach to
Fixing Deep Learning Models. In 34th IEEE/ACM International Conference on
Automated Software Engineering, ASE 2019, San Diego, CA, USA, November 11-15,
2019. IEEE, 376–387.

[67] Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020. Machine learning testing:
IEEE Transactions on Software Engineering

Survey, landscapes and horizons.
(2020).

[68] Min-Ling Zhang and Zhi-Hua Zhou. 2005. A k-nearest neighbor based algorithm
for multi-label classification. In 2005 IEEE international conference on granular
computing, Vol. 2. IEEE, 718–721.

[69] Xiang Zhang, Xiaocong Chen, Lina Yao, Chang Ge, and Manqing Dong. 2019.
Deep Neural Network Hyperparameter Optimization with Orthogonal Array
Tuning. In Neural Information Processing - 26th International Conference, ICONIP
2019, Sydney, NSW, Australia, December 12-15, 2019, Proceedings, Part IV (Com-
munications in Computer and Information Science), Tom Gedeon, Kok Wai Wong,
and Minho Lee (Eds.), Vol. 1142. Springer, 287–295. https://doi.org/10.1007/978-
3-030-36808-1_31

[70] Xiaoyu Zhang, Juan Zhai, Shiqing Ma, and Chao Shen. 2021. AUTOTRAINER:
An Automatic DNN Training Problem Detection and Repair System. In ICSE’21:
The 43nd International Conference on Software Engineering.

[71] Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, and Lu Zhang. 2018.
An empirical study on TensorFlow program bugs. In Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2018,
Frank Tip and Eric Bodden (Eds.). ACM, 129–140.

[72] Barret Zoph and Quoc V. Le. 2017. Neural Architecture Search with Reinforcement
Learning. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.
https://openreview.net/forum?id=r1Ue8Hcxg

[73] Daming Zou, Jingjing Liang, Yingfei Xiong, Michael D. Ernst, and Lu Zhang.
2021. An Empirical Study of Fault Localization Families and Their Combinations.
IEEE Trans. Software Eng. 47, 2 (2021), 332–347. https://doi.org/10.1109/TSE.2019.
2892102

