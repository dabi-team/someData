2
2
0
2

p
e
S
7

]
E
S
.
s
c
[

3
v
1
3
2
3
1
.
3
0
2
2
:
v
i
X
r
a

A Broad Comparative Evaluation of x86-64 Binary Rewriters

Eric Schulte
schulte.eric@gmail.com

Michael D. Brown
Trail of Bits
New York, NY, USA
michael.brown@trailofbits.com

Vlad Folts
Grammatech, Inc.
Ithaca, NY, USA
vfolts@grammatech.com

ABSTRACT
Binary rewriting is a rapidly-maturing technique for modifying
software for instrumentation, customization, optimization, and hard-
ening without access to source code. Unfortunately, the practical
applications of binary rewriting tools are often unclear to users
because their limitations are glossed over in the literature. This,
among other challenges, has prohibited the widespread adoption
of these tools. To address this shortcoming, we collect ten popular
binary rewriters and assess their generality across a broad range
of input binary classes and the functional reliability of the result-
ing rewritten binaries. Additionally, we evaluate the performance
of the rewriters themselves as well as the rewritten binaries they
produce.

The goal of this broad evaluation is to establish a shared con-
text for future research and development of binary rewriting tools
by providing a state of the practice for their capabilities. To sup-
port potential binary rewriter users, we also identify input binary
features that are predictive of tool success and show that a simple
decision tree model can accurately predict whether a particular
tool can rewrite a target binary. The binary rewriters, our corpus
of 3344 sample binaries, and the evaluation infrastructure itself are
all freely available as open-source software.

CCS CONCEPTS
• Security and privacy → Software reverse engineering.

KEYWORDS
Binary Rewriting, Binary Analysis, Binary Recompilation

1 INTRODUCTION
Binary rewriting (also referred to as recompilation) is an emerging
research area that has been enabled by recent advances in binary
analysis. Binary rewriting tools have the potential to address long-
standing problems in cyber security by enabling binary analysis,
patching, and security hardening for programs where source code
is not available (e.g., legacy or closed-source software). For binary
rewriting tools to be viable, they must generalize to the full variety
of programs available on heterogeneous computing platforms and
reliably produce functional rewritten binaries.

A surfeit of research into binary rewriting applications includ-
ing instrumentation, optimization, conﬁguration, debloating, and
hardening reveals a wide and largely under-emphasized variance
in the generality and reliability of binary rewriters [6, 8, 9, 16, 24,
25, 27, 29, 30, 34, 37, 42, 45, 46]. Frequently, papers presenting these
tools only brieﬂy mention large gaps in generality such as sup-
port limited to binaries with relocation information and/or sym-
bols – neither are typical of commercial oﬀ-the-shelf (COTS) soft-
ware [8, 42].

While there has been signiﬁcant research to date seeking to sys-
tematize knowledge of general binary rewriting techniques [38]
and evaluate the quality of binary lifters and disassemblers [2, 20],
no systematic comparative evaluation of binary rewriters has yet
been conducted. In this work, we address this important knowl-
edge gap by conducting such an evaluation of 10 binary rewriters
across 3344 variant binaries sourced from 34 benchmark programs
and 3 production compilers. The tools evaluated in this work are
static binary rewriting tools; we exclude dynamic binary rewrit-
ing tools (e.g., PIN [19]) whose runtime harnesses and overhead
often make them impractical for the applications considered by
this work.

Our work diﬀers from previous surveys in two key ways. First,
prior work systematizing knowledge on binary rewriters [38] fo-
cused primarily on their underlying techniques and algorithms and
as such did not evaluate their artifacts empirically. In contrast, our
evaluation focuses on measuring and comparing the generality and
reliability of a broad collection of publicly available binary lifter
and rewriter tools.1 Second, prior works performing comparative
evaluation of binary disassmblers and lifters [2, 20] focus on depth
achieving near complete measurement of binary analysis accuracy
across a small pool of binaries. The diﬃculties implicit in a truly
thorough analysis limits the breadth of these works to small num-
bers of binaries or to speciﬁc classes of binaries. Our evaluation fo-
cuses on input program breadth to directly address tool generality
and rewritten binary functionality to directly address reliability.

Summary of Contributions. In this paper, we ﬁrst review re-
lated work evaluating binary transformation tools in Section 2. We
then describe our experimental methodology to assess the gener-
ality and reliability of existing tools in Section 3. Next, we present
our experimental results and predictive models derived from them
in Section 4. Finally, we discuss the state of practice in binary rewrit-
ing tools and options for potential users in Section 5.

2 BACKGROUND
2.1 Types of Binary Rewriters
Direct Rewriting. Zipr [14] lifts a binary into an Intermediate
Representation Data Base (IRDB [18]) upon which transformations
may be applied. The IRDB can then be written directly to a binary
executable. Similarly, Egalito [42] lifts a binary into a low-level In-
termediate Representation (IR), provides an API for transforming
this IR, and supports lowering this IR back to a modiﬁed binary.

Reassemblable Disassemblers. Uroboros [36] – superseded
by the Phoenix reassemblable decompiler [23] – popularized the
idea of reassemblable disassembly (i.e., disassembly to assembly
code that can be readily reassembled). Retrowrite [8] also emits re-
assemblable assembly code. DDisasm [10] lifts to GrammaTech’s

1We are not aware of any comparable closed source binary rewriters.

 
 
 
 
 
 
Intermediate Representation for Binaries (GTIRB) [26], and can
also directly emit assembly code that can be reassembled.

LLVM Rewriting. The now defunct SecondWrite [29] was the
ﬁrst tool to lift binaries to LLVM IR. More recently, McSema [7]
and rev.ng [6] have become the predominant binary lifters to tar-
get LLVM IR. Although LLVM has a large user community and
provides many analysis, optimization, and hardening passes, there
are two properties of its IR that make it diﬃcult for lifters to target.
First, it requires type information to represent data. This requires
binary type analysis, which is prohibitively diﬃcult at the scale
required to rewrite large programs. Instead, many tools explicitly
emulate the stack, stack maintenance around function calls, and
memory as a large array of bytes. The lack of true types and stack-
/memory emulation limits the utility of most existing LLVM passes
and results in baroque and ineﬃcient rewritten binaries [15]. Sec-
ond, LLVM represents code in static single assignment form result-
ing in a loss of speciﬁcity with respect to the original instructions.
Trampoline Rewriting. Trampoline rewriters such as e9patch
minimally disturb the memory image of the original binary [4, 9].
New code is placed outside of the original image and the only
changes within the image are jumps to this new code, which itself
jumps back to the original code. Trampoline rewriting can be very
robust as it requires minimal binary analysis (e.g., symbolization is
not necessary), however it is only well-suited to additive transfor-
mation. The original code cannot be modiﬁed, moved, optimized,
or removed easily or eﬀectively.

2.2 Related Work
Wenzl et. al. [38] survey over 50 years of binary rewriter research
with the primary objective of categorizing binary rewriting ap-
proaches by end use-case, analysis techniques, code transforma-
tion methods, and code generation methods. However, this survey
does not include a comparative evaluation of the tools presented
in the literature. Our work aims to complement and extend their
survey by providing an empirical evaluation of binary rewriters.

Several works performing extensive evaluations of binary dis-
assemblers have been published in recent years [2, 17, 20, 22]. Col-
lectively, these works thoroughly document the approaches, chal-
lenges, trade-oﬀs, and shortcomings of disassemblers. Further, they
establish that modern disassemblers achieve high accuracy (close
to 100%) even in the presence of challenging (although rare) code
constructs due to advances and specialization in code discovery
and control-ﬂow reconstruction algorithms. These evaluations are
similar in scale to our work, however we evaluate tools with cus-
tom disassembly routines not covered in these works (all except
uroboros). Further, our evaluation focuses on binary rewriting as
opposed to disassembly, which is a prerequisite step to rewriting.

Additionally, several evaluation frameworks and benchmark suites

for evaluating binary analysis tools related to binary rewriting have
been recently proposed. Mishegos [43] is a novel diﬀerential fuzzer
that can be used to evaluate x86-64 instruction decoders. ConFIRM [44]
is an evaluation framework and benchmark suite for assessing soft-
ware hardening transformations, namely control-ﬂow integrity im-
plementations. Finally, Dasgupta et. al. [5] recently presented a
scalable technique for validating and detecting bugs in binary lifters
such as McSema [7].

Schulte, et al.

3 METHODOLOGY
3.1 Tool Selection
We selected ten binary rewriters for our evaluation, listed in Table
1. While our corpus of tools is not exhaustive, it provides excellent
coverage of tools that are mature, robust, and scale via automa-
tion. We exclude two notable binary rewriting tools, McSema[7]
and Ramblr[35], from our evaluation.

In McSema’s case, the tool can be automated to lift a wide va-
riety of binaries to LLVM IR. In our initial evaluation, McSema
successfully lifted 57% of the 3344 program variants we tested it
against. However, McSema’s rewriting workﬂow currently requires
manual steps by a knowledgeable operator. As a result, rewriting
binaries with McSema does not satisfy the requirements of our
evaluation. While still maintained, McSema’s shortcomings in this
area are largely due to age (its last major release was in 2018) and
the recent modernization of its dependent libraries, Anvill [31] and
Remill [33], to support newer lifting tools such as Rellic [32].

In Ramblr’s case, it is implemented as a supplementary anal-
ysis for the Angr binary analysis framework [28]. In its current
from, Ramblr is capable of reassembling x86 and x86_64 binaries
disassembled by Angr; however, it does not expose an interface
for transforming disassembled binaries. As such, Ramblr cannot be
truly used as a binary rewriter (i.e., it cannot perform our second
evaluation task) "out of the box".

Table 1: Tools selected for this evaluation

Rewriter Type
Reassemblable Disassembler
LLVM Trampoline
Direct Rewriter
LLVM Rewriter
Direct Rewriter
LLVM Rewriter
LLVM Rewriter

Tool
ddisasm [10]
e9patch [9]
Egalito [42]
mctoll [21]
multiverse [3]
ReOpt [11]
revng [6]
Retrowrite [8] Reassemblable Disassembler
Reassemblable Disassembler
Uroboros [36]
Direct Rewriter
Zipr [14]

3.2 Evaluation Variant Generation
In order to obtain realistic evaluation results, we combined bench-
mark lists compiled by two program managers from the United
States Department of Defense to arrive at a diverse list of 34 real-
world benchmark programs, shown in Table 2. To this corpus we
also added a trivial “Hello World!” program to our corpus to pro-
vide a low-water mark for program complexity. For each bench-
mark and the hello-world program, we compiled an x86-64 vari-
ant of the program using one permutation of the compilers, op-
timization levels, code layout, symbol options, and operating sys-
tems listed in Table 3. In total, we generated 3344 distinct evalua-
tion variants.

3.3 Evaluation Tasks
We evaluate our selected binary rewriters based on their ability to
successfully perform two rewriting tasks and record their progress

A Broad Comparative Evaluation of x86-64 Binary Rewriters

Table 2: Benchmark programs used in this evaluation

Program
anope
asterisk
bind
bitcoind
dnsmasq
ﬁlezilla
gnome-calculator
leafnode
Libreoﬃce
libzmq
lighttpd
memcached
monerod
mosh
mysql
nginx
ssh

SLOC Description
65,441
IRC Services
771,247 Communication Framework
376,147 DNS System
229,928 Bitcoin Client
34,671 Network Services
176,324

FTP Client and Server

301 Calculator
12,945 NNTP Proxy

5,090,852 Oﬃce Suite

In-memory Object Cache

62,442 Messaging Library
89,668 Web Server
33,533
394,783 Blockchain Daemon
12,890 Mobile Shell
SQL Server
170,602 Web Server
127,363

SSH Client and Server

3,331,683

Program
openvpn
pidgin
pks
poppler
postﬁx
proftpd
qmail
redis
samba
sendmail
sipwitch
snort
sqlite
squid
unrealircd
vi/vim
zip

SLOC Description
89,312 VPN Client
259,398 Chat Client
Public Key Server
40,788
188,156
PDF Reader
134,957 Mail Server
FTP Server
544,178
14,685 Message Transfer Agent
14,685

In-memory Data Store

1,863,980 Windows Interoperability

104,450 Mail Server
17,134 VoIP Server
Intrusion Prevention
344,877
292,398
SQL Server
212,848 Caching Web Proxy
IRC Server
90,988
394,056 Text Editor
54,390 Compression Utility

Table 3: Variant conﬁguration options

Compiler

Flags

clang
gcc
icx

OLLVM

O0
O1
O2
O3
Os
Ofast
ﬂa
sub
bcf2

Relocation
(Position-)
Independent
Dependent

Symbols

Present
Stripped

Operating
Systems
Ubuntu 16.041
Ubuntu 20.04

Independent
Dependent

Present
Stripped

Ubuntu 20.04

1Some binaries could not be built on this OS due to unavailable dependencies.
2Probability variable set to always insert (100%)

at multiple checkpoints. In the interest of breadth we use a proxy
for successful (i.e., correct) rewriting. Speciﬁcally, we consider a
rewrite to be successful if the output executable passes a very sim-
ple functional test (described in subsection 3.4). In practice many
binary rewriting tools fail fast when problems arise, meaning they
either completely fail to produce a new executable or they produce
a executable that is unable to start execution. For a small subset of
our benchmark with readily executable test suites with high cov-
erage we also tested programs against the full test suite.
Tasks. The two tasks we use to evaluate our tools are:

NOP This task is a minimal NOP (i.e., No Operation) trans-
form that simply lifts the binary and then rewrites without
modiﬁcation. The NOP transform tests the ability of a bi-
nary rewriter to successfully process the input binary, pop-
ulate its internal or external intermediate representation of
the binary, and then produce a new rewritten executable.
Despite its name this transform is decidedly non-trivial for

most rewriters, evidenced by the fact that rewritten binaries
typically look very diﬀerent from the original.

AFL This task is a more complex transform characteristic of
the needs of instrumentation, e.g. to support gray-box fuzz
testing. It evaluates our tools’ abilities to extensively trans-
form a binary with instrumentation to support AFL++ [1]2.
This task is important to include as many rewriters cover
up analysis errors by incorporating reasonable defaults (e.g.,
linking code from the original binary on lifting failure, or
preserving unidentiﬁed symbols which continue to resolve
correctly if code is left undisturbed in memory).

Checkpoints. For every attempted rewrite operation, we col-

lect some of the following artifacts to checkpoint the process:

IR For every binary rewriting tool that leverages some form
of external IR, we collect that IR. Speciﬁcally, we collect the
ASM ﬁles generated by tools that emit reassemblable dis-
assembly and the LLVM IR for tools targeting LLVM. Zipr,
Egalito, and multiverse use a internal IRs that are not eas-
ily serialized to disk. E9patch does not present the original
code for rewriting. As such, we do not track successful IR
generation for these tools.

EXE We next check if the rewriter successfully creates a new
executable. In some cases rewritten executables are trivially
recreated and are not an indicator of success (e.g., Egalito
almost always generates a new executable even if most of
them are non-functional). However, in most cases the abil-
ity to re-assemble and re-link a new executable indicates
that the rewriting tool both successfully disassembled rea-
sonable assembly instructions and generated the required
combinations of symbols and included libraries.

2Each tool we selected except multiverse claims to support AFL++’s instrumentation.

3.4 Evaluation Metrics
Functional Metrics. To measure rewriter correctness, we ﬁrst ob-
serve the success rate for each tool across all variants for both tasks
(i.e., NOP and AFL) at both checkpoints (i.e., IR and EXE). Next, we
perform a simple invocation of the NOP rewritten programs (e.g.,
running --help) to ensure the rewrite did not obviously corrupt
the program. We refer to this as the Null Function test. Finally, we
execute the AFL rewritten programs with the driver provided by
AFL++ to ensure instrumenting the program via binary rewriting
did not corrupt the program and that instrumentation was success-
fully incorporated. We refer to this as the AFL Function test.

Non-Functional Metrics. To measure rewriter runtime per-
formance we observe the total required runtime and the memory
high-water mark used by tools during rewriting. Available mem-
ory is often the limiting factor for rewriting because many under-
lying analyses scale super-linearly in the input binary size.

To determine the performance impacts of rewriting on binaries,
we ﬁrst measure ﬁle size impacts using Bloaty [12]. Size is an im-
portant metric as it measures the degree to which a rewriting tool
has inserted dynamic emulation or runtime supports – with their
associated increased complexity and runtime costs. Finally, for suc-
cessfully rewritten program variants with publicly available test
suites we measure the impact of rewriting on performance. Specif-
ically, we measure pass rate for all tests in the suite, runtime of the
test suite, and the memory consumption high-water mark during
execution of the full test suite as compared to the original.

4 EXPERIMENTAL RESULTS
We present binary rewriter success both in aggregate across our
entire benchmark set and broken out into cohorts. Each cohort
of binaries has like characteristics that highlight the comparative
strengths and weaknesses of the evaluated tools.

4.1 Aggregate Rewriting Success Rates
Our aggregate success results are presented in Table 4. Overall, we
observed a very broad range of success rates (and by extension
levels of support) achieved by our selected binary rewriting tools.
For the NOP transform, the tools fall into four distinct categories
characterized by the fraction of the universe of potential binaries
they can handle:

(1) Tools that work only on a tiny fraction (≤5%) of binaries.

This group includes mctoll and uroboros.

(2) Tools which work on a few (∼10%) binaries. This group in-

cludes multiverse and retrowrite.

(3) Tools that work on some (∼33%) binaries. This group in-

cludes egalito, reopt, and revng.

(4) Tools that work on most (≥80%) binaries. This group in-

cludes ddisasm, e9patch, and zipr.

In category (1) we ﬁnd tools that handle a very limited set of
binaries (e.g., mctoll only successfully transformed hello-world
binaries). The tools in category (2) support a wider range of bi-
naries but in many cases make hard and fast assumptions (e.g.,
multiverse can only successfully rewrite binaries compiled by old
versions of clang and gcc available on Ubuntu 16.04). These tools
still do not handle binaries that make use of fairly common code
structures (e.g., C++ exceptions). The tools in category (3) largely

Schulte, et al.

only work with relocation and debug information,3 but are able
to handle a wide range of the binaries meeting these restrictions.
Finally, category (4) tools do not require relocation or debug infor-
mation and support a wide range of both complex code structures
and compiler-speciﬁc binary generation behaviors such as multi-
ple forms of jump tables, and data intermixed with code.

Given that so many tools require relocation and debug informa-
tion we present a second view of our results limited to binaries that
include this information (i.e., non-stripped, position-independent
variants) in the right of Table 4. Although position-independent bi-
naries are increasingly common as ASLR becomes the norm, it is
still uncommon for COTS binaries to include debug information.
These results show the increase in rewriting success rate that a
developer might expect if they compile their binaries with reloca-
tion and debug information to support binary rewriting. However,
such binaries are not characteristic of the stripped COTS binaries
likely to be received from third parties or found in the wild.

As shown in Table 4, the AFL transform provides a much better
proxy for actual binary rewriter performance than the NOP trans-
form. This is true for at least two reasons. First, when applying the
NOP transform many relative and absolute locations in a rewritten
binary will continue to match their locations in the original binary
because no attempt is made to modify the lifted code. This provides
a great deal of grace for rewriters that missed code or symbols in
the original binary because symbols treated as literals or as data in
NOP transformed binaries remains sound surprisingly often. Sec-
ond, the AFL test is stricter because the rewritten binary must suc-
cessfully interact with the AFL++ harness to record a successful
execution.

Every rewriter included in this evaluation except multiverse pro-
vides some out-of-the-box support for AFL++ instrumentation. ddis-
asm, retrowrite, and uroboros all produce assembly-level IR that an
AFL++ provided tool can directly instrument. Similarly mctoll, re-
opt, and revng produce LLVM IR that an AFL++ provided LLVM
pass can directly instrument. E9patch, egalito, and zipr ship with
an AFL++ instrumentation pass compatible with their frameworks.
Despite this broad support, only ddisasm, mctoll, retrowrite, and
zipr successfully transform any of our test binaries for use with
AFL++. In Egalito’s case, the included AFL++ transform requires
a patched afl-fuzz program [39–41]. Reopt and revng appear to
produce LLVM IR that is not suitable for transformation. Further,
it appears reopt may only perform well on the NOP transform be-
cause it falls back to directly re-linking sections of the original bi-
nary when rewriting fails. Uroboros appears to fail to produce any
functional AFL transformed binaries not due to any uniform sys-
tematic reason but simply because the rewritten assembly code is
very brittle due to incorrect analyses during lifting.

4.2 Rewriting Success Rates by Compiler
In this section we present the success rates of our binary rewrit-
ers broken out by the compiler used to generate variants. Binary

3The exception is rev.ng which can work without relocation or debug information.

A Broad Comparative Evaluation of x86-64 Binary Rewriters

Table 4: Number and percentage of x86-64 Linux binaries for which the rewriter successfully produces IR, produces a NOP-
transformed executable (“EXE”), passes the Null Function test, produces an AFL++ instrumented executable (“AFL EXE”), and
passes the AFL Function test. Data is aggregated across the the full suite of binaries (3344) in the ﬁrst set of columns and across
position-independent, non-stripped binaries (1672) in the second set.

Full Suite

Position-Independent, Symbols Present only

Tool

ddisasm
%
e9patch
%
egalito
%
mctoll
%
multiverse
%
reopt
%
retrowrite
%
revng
%
uroboros
%
zipr
%

IR

EXE

Null
Func.
2873
2972
3282
98.14% 88.87% 85.91%
2620
3344
78.34%
100%
3294
983
98.50% 29.39%

NA
NA
NA
NA
30
.89%
NA
26.31% 10.82%
NA
1134
2556
3007
89.92% 76.43% 33.91%
334
9.98%
885

309
9.24%
786

30
.89%
362

30
.89%
880

26.46% 23.50%

216
6.45%
3344
100%

96
2.87%
3344
100%

817
24.43%
NAa
NA𝑎
364
10.88%
NA
NA

AFL EXE

3020
90.31%
3344
100%
2493
74.55%
30
.89%
0
0
0
0
330
9.86%
0
0
210
6.27%
2708
80.98%

AFL
Func.
2346
70.15%
1212
36.24%
0
0
30
.89%
0
0
0
0
254
7.59%
0
0
0
0
1609
48.11%

IR

EXE

30
1.78%
437

30
1.78%
181

Null
Func.
1385
1428
1638
97.96% 85.40% 82.82%
1310
1672
78.34%
100%
1661
492
99.34% 29.42%

NA
NA
NA
NA
30
1.78%
NA
26.12% 10.82%
NA
1504
1425
89.94% 85.22% 40.90%
334
48.86% 19.96% 18.48%
NA𝑎
439
NA𝑎
183
5.47%
NA
NA

108
6.44%
1672
100%

48
2.86%
1672
100%

26.26% 23.32%

817

390

309

684

AFL EXE

1456
87.08%
1672
100%
1261
75.40%
30
1.78%
0
0
0
0
330
19.72%
0
0
105
6.26%
1560
93.30%

AFL
Func.
1125
67.28%
607
36.30%
0
0
30
1.78%
0
0
0
0
254
15.18%
0
0
0
0
1079
64.52%

aAlthough rev.ng can produce IR, its normal usage is to output rewritten binaries directly which is how it was run in this evaluation.

rewriting success is often dependent on the compiler used to pro-
duce the input binary as many of the heuristics baked into rewrit-
ing tools target binary code generation logic or optimizations spe-
ciﬁc to certain compilers. For example, the Intel compiler (icx) in-
lines data into the code section on Linux whereas Clang and GCC
do not. As a result of this behavior and other ICX-speciﬁc optimiza-
tions, some binary rewriters have a signiﬁcantly lower success rate
against ICX-produced binaries.

The results restricted to GCC-compiled variants in Table 5 are
most similar to the aggregate results. This is unsurprising as GCC
is the prototypical compiler for Linux systems and represents a
middle ground in optimization aggressiveness between the rela-
tively conservative Clang and the very aggressive Intel ICX. In-
terestingly, the success rate for Clang-compiled variants (Table 5)
across all tools is slightly higher than GCC’s success rate. This
could be due to a number of factors including GCC-only optimiza-
tions that prove diﬃcult for binary rewriting tools or GCC leverag-
ing non-standard ELF ﬁle format extensions that are not produced
by Clang.

Our ICX-compiled results are shown in Table 6. They vary widely
from both GCC and Clang and also across our evaluated tools.
DDisasm performs better on ICX binaries generating an IR in 99%
of cases and generating functional AFL rewrites 2% more frequently
with these binaries than in aggregate. By contrast, Egalito’s NOP
transform success rate drops for ICX-produced variants, mctoll,
Multiverse, and Uroboros are unable to process any ICX binaries,

and Retrowrite and Zipr both perform signiﬁcantly worse on ICX
binaries although they are still able to successfully generate func-
tional AFL++ instrumented binaries in some cases.

Given the obfuscation techniques employed by ollvm are meant
to hinder binary analysis and reverse engineering, we expected the
tools to perform worst against ollvm compiled binaries (Table 6).
However, we were surprised to ﬁnd that in most cases the rewrit-
ing success rate increased for these binaries. It is not clear if this is
because those programs which could be compiled with ollvm rep-
resent the simpler end of our benchmark set, or if there is some-
thing about the ollvm transformations that are amenable to binary
rewriting if not to traditional reverse engineering.

4.3 Analysis of Binary Rewriter Success
In this section, we investigate binary formatting options to deter-
mine if they are correlates for binary rewriter success. We col-
lected three readily-identiﬁable formatting features using readelf
from GNU binutils: (1) whether or not the binary is position-
independent, (2) whether or not the binary is stripped, and (3) the
sections included in the binary4.

We collated the success and failure rate across these features for
each tool against our corpus of variants considering both rewrit-
ing tasks (i.e., NOP and AFL). Then we identiﬁed the four most

4We exclude sections which appear in all binaries (e.g., .text) and sections unique to
speciﬁc program (e.g., .gresource.gnome_calculator).

Table 5: Number and percentage of 1280 GCC-compiled and 1176 Clang-compiled x86-64 Linux binaries for which the rewriter
successfully reaches task checkpoints and passes functional tests.

Schulte, et al.

IR

EXE

994
77.65%
235

GCC
Null
Func.
1237
1106
1146
96.64% 89.53% 86.40%
1280
100%
1257
98.20% 18.36%

NA
NA
NA
NA
16
1.25%
NA
NA
1168
874
91.25% 68.28% 30.23%
129

16
1.25%
176

16
1.25%
438

34.22% 13.75%

387

308

24.06% 10.08%

124
9.69%
354

394

NA
NA
150
11.72%
NA
NA

30.78% 27.66%

92
7.19%
1280
100%

16
1.25%
1280
100%

AFL EXE

1157
90.39%
1280
100%
1113
86.95%
16
1.25%
0
0
0
0
126
9.84%
0
0
92
7.19%
1033
80.70%

AFL
Func.
871
68.04%
453
35.39%
0
0
16
1.25%
0
0
0
0
99
7.73%
0
0
0
0
674
52.66%

Tool

ddisasm
%
e9patch
%
egalito
%
mctoll
%
multiverse
%
reopt
%
retrowrite
%
revng
%
uroboros
%
zipr
%

Clang
Null
Func.
990

IR

EXE

494

952

1034
87.93% 84.18%
1174
99.83% 80.95%
1161
98.72% 42.01%

1167
100%
NA
NA
NA
NA
11
.94%
NA
NA
1057
901
89.88% 76.62% 39.37%
147
24.49% 12.50% 11.56%
379

37.59% 15.82%

11
.94%
442

11
.94%
186

288

463

340

136

NA
NA
144
12.24%
NA
NA

32.23% 28.91%

56
92
4.76%
7.82%
1174
1174
99.83% 99.83%

AFL EXE

1057
89.88%
1174
99.83%
1052
89.46%
11
.94%
0
0
0
0
146
12.41%
0
0
86
7.31%
1010
85.88%

AFL
Func.
799
67.94%
442
37.59%
0
0
11
.94%
0
0
0
0
108
9.18%
0
0
0
0
671
57.06%

Table 6: Number and percentage of 646 ICX-compiled and 244 Ollvm compiled and obfuscated x86-64 Linux binaries for which
the rewriter successfully reaches task checkpoints and passes functional tests.

ICX
Null
Func.
552

Tool

IR

EXE

ddisasm
%
e9patch
%
egalito
%
mctoll
%
multiverse
%
reopt
%
retrowrite
%
revng
%
uroboros
%
zipr
%

NA
NA
NA
NA
0
0
NA
NA
575

641

561
99.23% 86.84% 85.44%
646
100%
636

466
72.14%
136

98.45% 21.05%

0
0
0
0
173

0
0
0
0
571
89.01% 88.39% 26.78%
24
3.71%
24
3.71%
0
0
646
100%

157
24.30%
NA
NA
4
.62%
NA
NA

23
3.56%
16
2.47%
0
0
646
100%

AFL EXE

574
88.85%
646
100%
119
18.42%
0
0
0
0
0
0
24
3.71%
0
0
0
0
479
74.15%

AFL
Func.
468
72.45%
214
33.13%
0
0
0
0
0
0
0
0
18
2.79%
0
0
0
0
154
28.84%

OLLVM
Null
Func.
225

IR

EXE

NA
NA
NA
NA
3
1.23%
NA
NA
207

237

231
97.13% 94.67% 92.21%
244
100%
240

208
85.24%
118

98.36% 48.36%

3
1.23%
0
0
111

3
1.23%
0
0
210
84.84% 86.07% 45.49%
34
26.23% 13.93% 10.66%
88

64

26

76

36.07% 31.15%

32

27.05% 13.11%

NA
NA

244
100%

24
9.84%
244
100%

NA
NA
66

AFL EXE

232
95.08%
244
100%
209
85.66%
3
1.23%
0
0
0
0
34
13.93%
0
0
32
13.11%
186
76.23%

AFL
Func.
208
85.25%
103
42.21%
0
0
3
1.23%
0
0
0
0
29
11.89%
0
0
0
0
110
45.08%

predictive features for rewriting success or failure of the AFL trans-
form for each tool. These features are presented in Table 7. In many

cases these features are expected and match the advertised capabil-
ities of each tool. For example, retrowrite only supports relocatable

A Broad Comparative Evaluation of x86-64 Binary Rewriters

(i.e., pi) and non-stripped (i.e., strip) binaries which are its two
most predictive features for success.

Next we train a decision tree based on this feature collection
to predict the likelihood of success of each rewriter against an ex-
ample binary when using the AFL transform. Before training we
use linear support vector classiﬁcation to select the most discrim-
inating features for that rewriter. The resulting decision trees are
printed as Python code in Appendix B. We evaluate the resulting
decision tree using 70% of our binaries for training and reserv-
ing 30% for testing. The accuracy of the resulting tree is shown
in Table 7.

As shown in Table 7 the resulting decision trees, despite their
reliance on very simple binary features were very accurate in pre-
dicting the chances of tool success. We anticipate two beneﬁts from
this analysis. First, tool developers will have insight into properties
of binaries that cause their rewriting tools to fail. Second, users can
nearly instantaneously run a combination of readelf and our de-
cision tree to see what tools, if any, will reliably transform a given
target binary. This is useful when many binary rewriting tools can
run for minutes and even hours on a single binary. The success of
this simple machine learning model trained on simple inputs in-
dicates promising new directions for the practical application of
binary rewriting technology discussed in Section 5. The decision
trees and the code used to build and train them are included in our
publicly available artifact repository.

4.4 Size of Rewritten Binaries
Changes in binary size (shown in column 3 of Table 8) reﬂect a
tool’s design decisions and can impact the utility, eﬃciency, and
potential use cases for the rewritten binary. On average, ddisasm
and retrowrite’s rewritten binaries are slightly smaller, likely due
to symbol and debug information dropped during their rewriting
processes. For Egalito, mctoll, uroboros, and zipr the size of the bi-
nary increases by a non-trivial amount. Revng and Multiverse are
outliers with rewritten binaries that are nearly 16 and 9 times the
size of the original, respectively. For Revng, the recompiled bina-
ries exhibit very large increases in unmapped areas of the binary
(223 times on average!), potentially indicating ﬂaws in the recom-
pilation stage that can be rectiﬁed. For Multiverse the increase is
due to a deﬁning design decision: it produces rewritten binaries
that contain all possible disassemblies of the original binary.

It is important to note that size increases are calculated for bina-
ries the tool can successfully rewrite, which varies for each tool. As
a result, a direct comparison of tools using Table 8 is not possible.
As such, we also conducted a comparative evaluation of binary size
increases between pairs of tools where each successfully rewrote
one or more of the same programs (data shown in Appendix A).
The best performing tools were retrowrite and ddisasm which pro-
duced binaries that were on average 62% and 65% the size of those
produced by the other tools, respectively. The tools that produced
the largest binaries were revng and multiverse, which produced
binaries that were approximately 13 and 7 times the size of those
produced by the other tools, respectively.

cases bloaty [12], the tool we use to collect section size, is unable
to determine sizes for that section in corresponding binaries. In
nearly every rewriting tool the largest increase in size of the elf
ﬁle is in unmapped bytes or bytes that are not accounted for by
the section table. This is likely due to at least the following two
factors. First, because any extra non-standard runtime harnesses
or extra rewriting-speciﬁc supports are not properly entered into
the section table of the rewritten binary. Second, binary rewriting
tools are not penalized for dropping sections or breaking parts of
the section header table that are not required for execution.

4.5 Binary Rewriter Performance
To accurately and successfully rewrite a binary executable requires
signiﬁcant static analysis. These analyses often scale super-linearly
with the size of the program being rewritten. We summarize the
average run time of each tool in Table 8.

As with rewritten program size, the reported averages are skewed
because they are calculated across the set of binaries successfully
rewritten by each tool. Thus, rewriters that successfully rewrite
larger and more complicated binaries have an average that skews
higher. To account for this we also present the comparative aver-
age in Table 9. In each cell, the comparative average tool runtime
across successfully rewritten binaries by both tools is expressed
as a percentage of the row tool to the column tool. For example,
uroboros runtime is roughly 14.49% of the runtime of ddisasm. A
signiﬁcant trend observable in Table 9 is that tools with higher suc-
cess rates tend to run longer than tools with low rewriting success
rates. This is not surprising as successful tools perform more anal-
yses and more detailed analyses. They also explicitly handle more
portions of the ELF ﬁle and more edge cases.

4.6 Binary Rewriter Memory High-Water Mark
Like runtime, a tool’s memory requirements may make rewriting
impractical. For large binaries, memory requirements will frequently
outstrip the memory available on a server class machine. We present
the average memory high-water mark during binary rewriting in
column 2 of Table 8 and for the same reasons described in subsection 4.4
we also conducted a comparative evaluation of memory high-water
marks (data shown in Appendix A). As with rewriter performance,
we generally ﬁnd that successful tools require more resources dur-
ing rewriting.

4.7 Functionality Against Full Test Suite
For three benchmark programs with readily available test suites we
check the degree to which successful execution of the Null Func-
tion test predicts successful execution of the complete test suite5.
Our results are presented in Table 10. In each cell we report the
number of binaries that completely pass the full test suite and the
number of binaries that pass the Null Function test as “full/null”.
We report this for each binary rewriter as well as for the original in-
put binaries ﬁles. There are up to 60 binaries for each program due
to the multiple build options (e.g., compiler, optimization level, pie,
stripped, etc.). We do not include ICX-compiled binaries due to the

To investigate these size changes further, we analyzed size changes

per section per rewriting tool (data shown in Appendix A). Note
that in many cases rewriting tools break elf section tables. In these

5We report full results for only three benchmark programs due to the dearth of high-
quality test suites for real-world programs and the high level of eﬀort required for
properly conﬁguring them.

Table 7: Decision tree accuracy predicting binary rewriting success based on simple binary features

Schulte, et al.

NOP AFL

Most Predictive Features (AFL)

Rewriter
ddisasm 90.03% 81.47% note.abi-tag, interp, gcc_except_table debug_str
e9patch
egalito
mctoll
multiverse
reopt
retrowrite
revng
uroboros
zipr

80.57% 86.06% pi, got.plt, data.rel.ro, plt.got
87.15%
98.80% 98.80% strip, data.rel.ro, symtab, strtab
97.80%
67.82%
94.32% 93.02% pi, strip, symtab, strtab
78.78%
96.31%
86.65% 79.98% strip, note.gnu.build-id, symtab, strtab

Table 8: Average tool runtime memory high-water mark,
and relative program size change across successful rewrites

Tool
ddisasm
e9patch
egalito
mctoll
multiverse
reopt
retrowrite
revng
uroboros
zipr

Runtime Memory high-
(seconds) water (kbytes)

72.81
2.74
454.40
0.00
1195.72
169.89
114.57
703.74
19.17
233.61

509215.92
105132.97
10433233.07
1405.30
687609.83
4061695.00
1967393.19
2244106.85
93575.58
1015891.15

Relative program
size change
91.90%
114.45%
169.17%
128.22%
870.71%
99.61%
83.78%
1581.95%
148.97%
140.05%

extra runtime dependencies they require that impose signiﬁcant
extra burden when running full test suites in the test environment.
Overall we ﬁnd a weak correlation with only 258 rewritten pro-
grams passing their full test suite of the 395 rewritten programs
that passed their Null Function tests. However, it is worth noting
that some original binaries (i.e., inputs to the binary rewriter) that
pass the Null Function test do not pass the full test suite (e.g., redis
when compiled with -Ofast).

Note that we disable one test in redis because it looks for a
speciﬁc symbol in the stack trace. This is suﬃciently internal that
we believe it does not compromise program soundness for binary
rewriters to change this behavior.

4.8 Performance Against Full Test Suite
The performance of rewritten binaries is critical to many use cases
for static binary rewriting. If performance degradation exceeds that
of dynamic binary rewriting then dynamic rewriting is often a
better alternative as it is able to leverage runtime information to
more reliably transform program behavior. We report the change
in runtime and memory requirements for successfully rewritten
programs running against their full test suite in Table 11. Only
those rewriters which produced binaries capable of passing all tests
are included. With the exception of Reopt-rewritten binaries which
had resource consumption at least an order of magnitude over the

original, runtime and memory consumption of the rewritten bina-
ries is close to that of the original binary. This is especially true of
the memory high-water mark.

5 DISCUSSION
We identify several trends with respect to binary rewriter IR from
our results. First, rewriting via LLVM IR appears to be infeasible
given the current state of binary type analysis. Only one binary
(the trivial hello-world) was successfully instrumented for AFL++
using an LLVM rewriter (mctoll). Additionally, non-trivial NOP-
transformed binaries successfully produced with reopt had dramat-
ically increased runtime and memory consumption as compared
to the original. Second, direct rewriting as performed by Egalito
and Zipr successfully produced executables even in the presence of
analysis errors; however their output binaries also demonstrated a
higher functional failure rate. Conversely, reassemblable disassem-
blers were more likely to raise errors during re-assembling and re-
linking and thus fail to create an executable. Trampoline rewriters
such as e9patch are very reliable across a wide range of binaries if
only additive instrumentation and no modiﬁcation of existing code
is required.

During our evaluation, we communicated with the developers
of our evaluated tools to share our partial results and our bench-
mark set.6 Unsurprisingly, the best-performing tools in our eval-
uation, ddisasm and zipr, had suﬃcient development resources to
respond to speciﬁc failures encountered in our work. Thus, their
performance against this evaluation set likely outperforms their ex-
pected performance in general. This is indicative of a deﬁning char-
acteristic of binary rewriting at this point in time; binary rewriting
is eminently practical in many particular cases that have been ad-
dressed and considered by tool developers, but impossible in the
general case as the universe of binary formats and features is sim-
ply too large with too many edge cases to handle.

Our evaluation indicates that practical applications of binary
rewriting should be preceded by a scoping stage. In this stage, the
target binary is classiﬁed as either “in scope” or “out of scope” for
the binary rewriting tool(s) of interest. While scoping can be ac-
complished via traditional binary analysis, the high success rate

6We did not communicate with the rev.ng developers until after our evaluation. As
a result our evaluation did not include the recommended but non-default arguments
“-O2 -i” which are expected to result in reduced rewritten binary code size and run-
time but are not expected to impact rewritten binary functionality

A Broad Comparative Evaluation of x86-64 Binary Rewriters

Table 9: Comparative runtime of binary rewriting tools

Tool
ddisasm
e9patch
egalito
mctoll
multiverse
reopt
retrowrite
revng
uroboros
zipr

ddisasm
100%
3.76%
626.77%
0.00%

e9patch
2656.96%
100%
16963.74%
0.01%

egalito
15.95%
0.59%
100%
0.00%

mctoll
35288840.58%
1328163.77%
219758856.52%
100%

2270.67% 51759.60% 546.47% 441563025.00%
82335688.41%
233.32%
6199.21%
4485.01%
370.73%
32450926.47%
25678.91% 154.87% 341058002.90%
966.48%
7081250.00%
645.59%
14.49%
104211334.78%
8349.20%
296.89%

36.91%
19.85%

4.71%
48.67%

Table 10: Number of binaries passing their full test suite ver-
sus passing the Null Function test

Tool
original
ddisasm
e9patch
egalito
multiverse
reopt
retrowrite
revng
uroboros
zipr

lighttpd
30/30
0/30
30/30
18/18
0/0
2/19
0/9
0/0
0/0
28/30

nginx
60/60
60/60
60/60
18/18
0/0
4/60
16/26
0/0
0/0
58/58

redis
26/30
26/30
26/30
10/10
0/0
2/8
0/0
0/0
0/0
22/30

Table 11: Average performance of rewritten binaries when
run against the full test suite

Tool
ddisasm
e9patch
egalito
reopt
retrowrite
zipr

Runtime Memory High-water
Mark (kbytes)
(seconds)
100.21%
109.43%
99.06%
119.53%
99.85%
104.45%
51937.17%
1324.66%
100.17%
103.84%
102.38%
102.50%

demonstrated by our simple predictive model with trivially col-
lected features shows that accurate scoping can be conducted with
little eﬀort. Further, the relative simplicity of our model implies
that more reliable and accurate predictive models are likely eas-
ily within reach. With such a model, users of binary rewriting
tools may quickly ensure their target binaries meet the expecta-
tions of the available rewriting tools before initiating expensive
binary rewriting tasks. For binaries that are likely to fail during
static rewriting, the user could either conserve their resources by
forgoing binary rewriting or spend them employing more expen-
sive techniques such as dynamic binary rewriting.

multiverse
4.40%
0.19%
18.30%
0.00%
100%
13.34%
5.04%
44.53%
0.49%
6.45%

reopt
42.86%
1.61%
270.91%
0.00%
749.41%
100%
60.22%
414.23%
12.33%
133.56%

retrowrite
26.97%
2.23%
503.69%
0.00%
1983.34%
166.07%
100%
517.54%
298.95%
196.23%

zipr
33.68%
1.20%
205.48%
0.00%

uroboros
revng
689.90%
10.35%
15.49%
0.39%
2123.71%
64.57%
0.00%
0.00%
224.58% 20301.47% 1550.43%
74.87%
811.10%
24.14%
50.96%
33.45%
19.32%
300.14%
4073.82%
100%
10.68%
100%
2.45%
100%
936.69%
33.32%

6 CONCLUSION
In this work, we evaluated and compared ten binary rewriting
tools on two rewriting tasks across a corpus of 3344 variant bina-
ries produced using three compilers and 34 benchmark programs.
Our evaluation measured the performance of the tools themselves
as well as the performance and soundness of the rewritten bina-
ries they produce. In general, our evaluation indicates that binary
rewriters that lift to high-level machine-independent IRs (e.g., LLVM
IR) were much less successful in terms of generality and reliability.
Additionally, we identiﬁed binary features that are predictive of
rewriting success and showed that a simple decision tree model
trained on these features can accurately predict whether a particu-
lar tool can rewrite a target binary. The ﬁndings and artifacts con-
tributed by this evaluation have been made publicly available and
are intended to support users and developers of binary rewriting
tools and drive rewriter adoption and maturation.

ARTIFACT AVAILABILITY
We have made the full set of artifacts generated in this work includ-
ing our evaluation infrastructure, corpus of test binaries, predictive
models, and the evaluated tools publicly available at:

https://gitlab.com/GrammaTech/lifter-eval [13]

ACKNOWLEDGMENTS
This material is based upon work supported by the Oﬃce of Naval
Research (ONR) under Contract No. N00014-21-C-1032. Any opin-
ions, ﬁndings and conclusions or recommendations expressed in
this material are those of the author(s) and do not necessarily re-
ﬂect the views of the ONR.

REFERENCES
[1] AFL++. 2022. AFL++. https://aﬂplus.plus/.
[2] Dennis Andriesse, Xi Chen, Victor

Slowin-
An In-Depth Analysis of Disassem-
ska, and Herbert Bos. 2016.
bly on Full-Scale x86/x64 Binaries.
In 25th USENIX Security Sympo-
sium (USENIX Security 16). USENIX Association, Austin, TX, 583–600.
https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/andriesse

van der Veen, Asia

[3] Erick Bauman, Zhiqiang Lin, and Kevin W. Hamlen. 2018.

Superset Dis-
assembly: Statically Rewriting x86 Binaries Without Heuristics. In NDSS.
https://doi.org/10.14722/ndss.2018.23304

[4] Andrew R. Bernat and Barton P. Miller. 2011. Anywhere, Any-time Binary In-
strumentation. In Proceedings of the 10th ACM SIGPLAN-SIGSOFT Workshop on

Schulte, et al.

[29] Matthew Smithson, Khaled ElWazeer, Kapil Anand, Aparna Kotha, and Rajeev
Barua. 2013. Static binary rewriting without supplemental information: Over-
coming the tradeoﬀ between coverage and correctness. In Reverse Engineering
(WCRE), 2013 20th Working Conference on. IEEE, 52–61.

[30] Eli Tilevich and Yannis Smaragdakis. 2005. Binary refactoring: Improving code
behind the scenes. In Proceedings of the 27th international conference on Software
engineering. ACM, 264–273.

[31] Inc. Trail of Bits. 2022. Anvill. https://github.com/lifting-bits/anvill.
[32] Inc. Trail of Bits. 2022. Rellic. https://github.com/lifting-bits/rellic.
[33] Inc. Trail of Bits. 2022. Remill. https://github.com/lifting-bits/remill.
[34] Ludo Van Put, Dominique Chanet, Bruno De Bus, Bjorn De Sutter, and Koen
De Bosschere. 2005. Diablo: a reliable, retargetable and extensible link-time
rewriting framework. In Proceedings of the Fifth IEEE International Symposium
on Signal Processing and Information Technology, 2005. IEEE, 7–12.

[35] Ruoyu Wang, Yan Shoshitaishvili, Antonio Bianchi, Aravind Machiry, John
Grosen, Paul Grosen, Christopher Kruegel, and Giovanni Vigna. 2017. Ramblr:
Making Reassembly Great Again. In NDSS.

[36] Shuai Wang, Pei Wang, and Dinghao Wu. 2015.

Disassembling.
curity
https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/wang-shuai

Association, Washington,

24th USENIX Security

In
USENIX

15).

Reassembleable
Symposium (USENIX Se-
627–642.
D.C.,

[37] Richard Wartell, Vishwath Mohan, Kevin W Hamlen, and Zhiqiang Lin. 2012.
Securing untrusted code via compiler-agnostic binary rewriting. In Proceedings
of the 28th Annual Computer Security Applications Conference. ACM, 299–308.

[38] Matthias Wenzl, Georg Merzdovnik, Johanna Ullrich, and Edgar Weippl. 2019.
From hack to elaborate technique—a survey on binary rewriting. ACM Comput-
ing Surveys (CSUR) 52, 3 (2019), 1–37.

[39] David

Williams-King.

2022.

AFL

Setup.

https://github.com/columbia/egalito-artefact/blob/master/aﬂ-support/setup.sh#L25.

[40] David

Williams-King.

2022.

AFL

Support.

https://github.com/columbia/egalito-artefact/blob/master/aﬂ-support/test-readelf.sh#L10.

[41] David Williams-King. 2022. README. https://github.com/columbia/egalito-artefact/blob/master/README-aﬂ.txt.
[42] David Williams-King, Hidenori Kobayashi, Kent Williams-King, Graham Patter-
son, Frank Spano, Yu Jian Wu, Junfeng Yang, and Vasileios P Kemerlis. 2020. Egal-
ito: Layout-Agnostic Binary Recompilation. In Proceedings of the Twenty-Fifth In-
ternational Conference on Architectural Support for Programming Languages and
Operating Systems. 133–147.

[43] William Woodruﬀ, Niki Carroll, and Sebastiaan Peters. 2021. Diﬀerential anal-
ysis of x86-64 instruction decoders. In Proceedings of the Seventh Language-
Theoretic Security Workshop (LangSec) at the IEEE Symposium on Security and
Privacy.

[44] Xiaoyang Xu, Masoud Ghaﬀarinia, Wenhao Wang, Kevin W Hamlen, and
{CONFIRM}: Evaluating compatibility and relevance of
Zhiqiang Lin. 2019.
control-ﬂow integrity protections for modern software. In 28th USENIX Security
Symposium (USENIX Security 19). 1805–1821.

[45] Mingwei Zhang, Rui Qiao, Niranjan Hasabnis, and R Sekar. 2014. A platform
for secure static binary instrumentation. In Proceedings of the 10th ACM SIG-
PLAN/SIGOPS international conference on Virtual execution environments. ACM,
129–140.

[46] Mingwei Zhang and R Sekar. 2013. Control Flow Integrity for COTS Binaries..

In USENIX Security. 337–352.

Program Analysis for Software Tools (Szeged, Hungary) (PASTE ’11). ACM, New
York, NY, USA, 9–16. https://doi.org/10.1145/2024569.2024572

[5] Sandeep Dasgupta, Sushant Dinesh, Deepan Venkatesh, Vikram S. Adve, and
Christopher W. Fletcher. 2020. Scalable Validation of Binary Lifters. In Proceed-
ings of the 41st ACM SIGPLAN Conference on Programming Language Design and
Implementation (London, UK) (PLDI 2020). Association for Computing Machin-
ery, New York, NY, USA, 655–671. https://doi.org/10.1145/3385412.3385964
[6] Alessandro Di Federico, Mathias Payer, and Giovanni Agosta. 2017. rev.ng: a
uniﬁed binary analysis framework to recover CFGs and function boundaries. In
Proceedings of the 26th International Conference on Compiler Construction. 131–
141.

[7] Artem Dinaburg and Andrew Ruef. 2014. McSema: Static translation of x86

instructions to llvm. In ReCon 2014 Conference, Montreal, Canada.

[8] Sushant Dinesh. 2019. RetroWrite: Statically Instrumenting COTS Binaries for

Fuzzing and Sanitization. Ph. D. Dissertation. ﬁgshare.

[9] Gregory J Duck, Xiang Gao, and Abhik Roychoudhury. 2020. Binary rewriting
without control ﬂow recovery. In Proceedings of the 41st ACM SIGPLAN Confer-
ence on Programming Language Design and Implementation. 151–163.
and

[10] Antonio
alog
(USENIX
USENIX
https://www.usenix.org/conference/usenixsecurity20/presentation/ﬂores-montoya

Flores-Montoya
In
20).

2020.
Security
Association,

Dat-
Symposium
1075–1092.

Disassembly.
Security

Schulte.

USENIX

29th

Eric

[11] Inc. Galois. 2021. ReOpt. https://github.com/GaloisInc/reopt.
proﬁler
[12] Google.

Bloaty:

2022.

size

a

for

binaries.

https://github.com/google/bloaty.
Grammatech.

2022.

[13] Inc.

Lifter

Evaluation.

https://GitLab.com/GrammaTech/lifter-eval.

[14] Jason D Hiser, Anh Nguyen-Tuong, Michele Co, Benjamin Rodes, Matthew Hall,
Clark L Coleman, John C Knight, and Jack W Davidson. 2014. A Framework
for Creating Binary Rewriting Tools (Short Paper). In Dependable Computing
Conference (EDCC), 2014 Tenth European. IEEE, 142–145.

[15] Pantea Kiaei, Cees-Bart Breunesse, Mohsen Ahmadi, Patrick Schaumont, and
Jasper van Woudenberg. 2020. Rewrite to Reinforce: Rewriting the Binary to
Apply Countermeasures against Fault Injection. arXiv preprint arXiv:2011.14067
(2020).

[16] M. A. Laurenzano, M. M. Tikir, L. Carrington, and A. Snavely. 2010.

PE-
BIL: Eﬃcient static binary instrumentation for Linux. In 2010 IEEE Interna-
tional Symposium on Performance Analysis of Systems Software (ISPASS). 175–183.
https://doi.org/10.1109/ISPASS.2010.5452024

[17] Kaiyuan Li, Maverick Woo, and Limin Jia. 2020. On the Generation of Disas-
sembly Ground Truth and the Evaluation of Disassemblers. In Proceedings of the
2020 ACM Workshop on Forming an Ecosystem Around Software Transformation.
9–14.

[18] Zephyr

Cookbook
2022.
https://git.zephyr-software.com/opensrc/irdb-cookbook-examples.

Software

IRDB

LLC.

Examples.

[19] Chi-Keung Luk, Robert Cohn, Robert Muth, Harish Patil, Artur Klauser, Geoﬀ
Lowney, Steven Wallace, Vijay Janapa Reddi, and Kim Hazelwood. 2005. Pin:
building customized program analysis tools with dynamic instrumentation. Acm
sigplan notices 40, 6 (2005), 190–200.

[20] Xiaozhu Meng and Barton P. Miller. 2016. Binary Code is Not Easy. In Pro-
ceedings of the 25th International Symposium on Software Testing and Analysis
(Saarbr&#252;cken, Germany) (ISSTA 2016). ACM, New York, NY, USA, 24–35.
https://doi.org/10.1145/2931037.2931047

[21] Microsoft. 2022. mctoll. https://github.com/microsoft/llvm-mctoll.
[22] Chengbin Pang, Ruotong Yu, Yaohui Chen, Eric Koskinen, Georgios Portokalidis,
Bing Mao, and Jun Xu. 2021. Sok: All you ever wanted to know about x86/x64
binary disassembly but were afraid to ask. In 2021 IEEE Symposium on Security
and Privacy (SP). IEEE, 833–851.

[23] Phoenix. 2021. Phoenix. https://github.com/s3team/phoenix.
[24] Chenxiong Qian, Hong Hu, Mansour Alharthi, Pak Ho Chung, Taesoo Kim, and
Wenke Lee. 2019. {RAZOR}: A framework for post-deployment software de-
bloating. In 28th {USENIX} Security Symposium ({USENIX} Security 19). 1733–
1750.

[25] Ted Romer, Geoﬀ Voelker, Dennis Lee, Alec Wolman, Wayne Wong, Hank Levy,
Brian Bershad, and Brad Chen. 1997.
Instrumentation and optimization of
Win32/Intel executables using Etch. In Proceedings of the USENIX Windows NT
Workshop, Vol. 1997. 1–8.

[26] Eric M. Schulte, Jonathan Dorn, Antonio Flores-Montoya, Aaron Ballman, and
Tom Johnson. 2019. GTIRB: Intermediate Representation for Binaries. ArXiv
abs/1907.02859 (2019).

[27] Benjamin Schwarz, Saumya Debray, Gregory Andrews, and Matthew Legendre.
2001. Plto: A link-time optimizer for the Intel IA-32 architecture. In Proc. 2001
Workshop on Binary Translation (WBT-2001).

[28] Yan Shoshitaishvili, Ruoyu Wang, Christopher Salls, Nick Stephens, Mario
Polino, Audrey Dutcher, John Grosen, Siji Feng, Christophe Hauser, Christopher
Kruegel, and Giovanni Vigna. 2016. SoK: (State of) The Art of War: Oﬀensive
Techniques in Binary Analysis. (2016).

A Broad Comparative Evaluation of x86-64 Binary Rewriters

A SUPPLEMENTARY PERFORMANCE TABLES

Table 12: Comparative size of rewritten binaries in the intersection of those programs which are successfully rewritten by
both tools. The percentage of the successfully rewritten binary sizes by both tools are calculated as a ratio of the row tool to
the column tool. For example, multiverse rewritten binaries are just over 9 times bigger on average than ddisasm rewritten
binaries. An entry of “NA” indicates that no binaries were successfully rewritten by both tools.

Tool
ddisasm
e9patch
egalito
mctoll
multiverse
reopt
retrowrite
revng
uroboros
zipr

egalito
51.45%
67.71%
100%
52.69%
463.33%
59.55%
60.71%

ddisasm e9patch
79.65%
100%
147.69%
101.94%
765.59%
86.38%
70.60%

mctoll
100.52%
100%
98.09%
125.54%
189.77%
194.36%
100%
99.48%
369.23%
937.59%
28.42%
108.00%
93.99%
101.12%
1546.44% 1318.30% 949.20% 2823.92%
155.01%
77.42%
118.20%
156.68%

104.63%
82.56%

118.68%
122.37%

multiverse
10.67%
13.06%
21.58%
27.08%
100%
11.71%
11.02%
193.97%
17.63%
15.96%

reopt
92.59%
115.77%
167.93%
351.83%
854.06%
100%
87.01%
1696.73%
177.89%
141.60%

retrowrite
106.39%
141.65%
164.72%
98.89%
907.58%
114.93%
100%
1509.82%
NA
148.11%

revng
6.47%
7.59%
10.54%
3.54%
51.55%
5.89%
6.62%
100%
6.83%
10.18%

uroboros
64.51%
84.26%
95.58%
129.17%
567.15%
56.22%
NA

zipr
63.82%
81.72%
121.13%
84.60%
626.59%
70.62%
67.52%
1463.48% 982.19%
109.41%
100%

100%
91.40%

Table 13: Average size change by section for each rewriting tool

Section
.got.plt
.data
.dynamic
.rela.dyn
.strtab
.dynsym
.dynstr
.symtab
.eh_frame_hdr
.plt
.rela.plt
.eh_frame
[ELF Program Headers]
[ELF Section Headers]
.rodata
.text
[Unmapped]

mctoll multiverse
e9patch
egalito
ddisasm
89.60%
100.00% 100.67%
100.08%
NA
100.00% 100.24%
102.94%
97.70%
100.00%
66.53%
97.48%
54.35%
114.67% 823.91%
87.27%
95.27%
100.00%
77.54%
104.90%
75.24%
100.00% 101.86%
75.44%
73.18%
100.00% 101.42%
72.78%
93.93%
100.00%
84.73%
118.87%
NA 110.43%
100.00%
103.76%
89.60%
100.00% 100.45%
100.05%
NA
100.00%
99.35%
99.93%
NA 101.88%
100.00%
109.75%
94.29% 105.53%
100.00%
96.37%
94.93%
100.00%
71.70%
97.66%
100.00% 100.43%
NA
100.10%
146.85%
100.00% 161.64% 100.89%
128.93% 13162.97% 670.47% 350.40%

reopt
100.00% 100.29%
99.99% 103.29%
100.00%
99.82%
100.00% 102.65%
99.26%
76.69%
100.00% 110.20%
99.99% 104.62%
100.00% 107.44%
100.00% 106.36%
100.00% 100.28%
100.00% 100.30%
100.00% 125.52%
94.00%
144.44%
116.13% 103.62%
100.00%
99.96%
100.00% 240.13%
2285.56% 181.98%

zipr
uroboros
revng
retrowrite
NA
99.24%
NA
100.00%
NA
101.87%
1014.94%
100.42%
NA
99.91%
NA
101.83%
NA
91.19%
355.43%
97.30%
NA
485.39%
407.41%
105.45%
NA
99.43%
821.27%
77.96%
NA
1075.44%
99.75%
74.56%
NA
270.77% 1675.22%
95.09%
NA
108.05%
25.33%
93.36%
NA
99.75%
990.99%
99.81%
NA
100.00%
757.03%
99.81%
111.34%
520.42%
NA
15.14%
97.77% 119.81%
NA
104.79%
98.55%
97.57%
148.37%
92.86%
NA
100.03%
480.80%
100.06%
NA
110.17%
120.45%
3817.02%
10.40%
165.31%
225.89% 22384.87%

Schulte, et al.

Table 14: Comparative memory high-water mark in kilobytes between rewriting tools. The comparative memory high-water
mark across successfully rewritten binaries by both tools is expressed as a percentage of the row tool to the column tool. For
example, uroboros’ maximum memory consumption is roughly 15.14% of the maximum memory consumption of ddisasm.

Tool
ddisasm
e9patch
egalito
mctoll
multiverse
reopt
retrowrite
revng
uroboros
zipr

ddisasm
100%
20.65%
2133.00% 10124.69%

39patch
484.35%
100%

0.28%
130.07%
797.64%
404.93%
440.70%
15.14%
186.28%

1.34%
761.64%
3863.39%
1856.78%
2134.54%
83.40%
945.96%

mctoll
egalito
36235.17%
4.69%
7481.13%
0.99%
741944.13%
100%
100%
0.01%
6.91%
43096.38%
38.51% 289025.13%
20.02% 112051.06%
21.19% 159687.83%
6083.90%
0.87%
70240.87%
9.53%

multiverse
76.88%
13.13%
1446.63%
0.23%
100%
542.85%
151.14%
262.22%
12.40%
89.89%

reopt
12.54%
2.59%
259.70%
0.03%
18.42%
100%
48.46%
55.25%
2.31%
24.56%

retrowrite
24.70%
5.39%
499.62%
0.09%
66.16%
206.36%
100%
105.97%
39.76%
50.54%

zipr
uroboros
revng
53.68%
660.63%
22.69%
4.68%
10.57%
119.91%
471.95% 11511.47% 1048.77%
1.64%
0.06%
806.39%
38.14%
4336.39%
180.99%
251.50%
94.37%
2492.68%
100%
100%
4.01%
886.33%
43.66%

0.14%
111.25%
407.09%
197.87%
229.02%
11.28%
100%

B PREDICTIVE MODELS

Figure 1: Decision tree to predict the success of AFL instrumentation with ddisasm. Accuracy of 81.47%.

def d d i s a s m _ t r e e ( n o t e . a b i _ t a g ,

i n t e r p ,

s t r i p ,

r e l a . p l t , p i ) :

i f not n o t e . a b i _ t a g :

i f not
i f

s t r i p :

i n t e r p :

i f

i n t e r p :

e l s e : # n o t

return { ' FAIL ' :
i n t e r p
return { ' FAIL ' :

5 0 . 0 ,

' PASS ' :

1 1 2 . 0 }

3 7 . 0 ,

' PASS ' :

3 3 . 0 }

e l s e : # n o t

s t r i p
return { ' FAIL ' :

1 2 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # i n t e r p
r e l a . p l t :
i n t e r p :

i f

i f

e l s e : # n o t

return { ' FAIL ' :
i n t e r p
return { ' FAIL ' :
r e l a . p l t

e l s e : # n o t

4 7 . 0 ,

' PASS ' :

9 1 0 . 0 }

9 2 . 0 ,

' PASS ' :

3 6 8 . 0 }

return { ' FAIL ' :
e l s e : # n o t e . a b i _ t a g

1 0 . 0 ,

' PASS ' :

0 . 0 }

i f not

s t r i p :
return { ' FAIL ' :

e l s e : #

s t r i p

5 3 . 0 ,

' PASS ' :

0 . 0 }

i f not
i f

i n t e r p :

i n t e r p :

e l s e : # n o t

return { ' FAIL ' :
i n t e r p
return { ' FAIL ' :

6 4 . 0 ,

' PASS ' :

1 1 . 0 }

2 2 . 0 ,

' PASS ' :

3 . 0 }

e l s e : # i n t e r p
i f not p i :

i f

i n t e r p :

e l s e : # n o t

return { ' FAIL ' :
i n t e r p
return { ' FAIL ' :

e l s e : # p i

2 1 5 . 0 ,

' PASS ' :

1 6 8 . 0 }

8 2 . 0 ,

' PASS ' :

3 8 . 0 }

return { ' FAIL ' :

0 . 0 ,

' PASS ' :

1 5 . 0 }

A Broad Comparative Evaluation of x86-64 Binary Rewriters

Figure 2: Decision tree to predict the success of AFL instrumentation with e9patch. Accuracy of 86.06%.

def e 9 p a t c h _ t r e e ( pi , n o t e . gnu . b u i l d _ i d , g o t . p l t ,

i n t e r p ,

s t r i p , n o t e . a b i _ t a g ,

r e l a . p l t ) :

i f not p i :

i f n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

7 2 3 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # n o t n o t e . gnu . b u i l d _ i d

i f g o t . p l t :

i f not n o t e . gnu . b u i l d _ i d :

i f

i n t e r p :

e l s e : # n o t

return { ' FAIL ' :
i n t e r p
return { ' FAIL ' :

3 9 . 0 ,
e l s e : # n o t e . gnu . b u i l d _ i d

3 . 0 ,

' PASS ' :

0 . 0 }

' PASS ' :

6 . 0 }

return { ' FAIL ' :
e l s e : # n o t g o t . p l t

1 3 . 0 ,

' PASS ' :

0 . 0 }

i f not n o t e . gnu . b u i l d _ i d :

i f

i n t e r p :

e l s e : # n o t

return { ' FAIL ' :
i n t e r p
return { ' FAIL ' :

4 6 . 0 ,

' PASS ' :

0 . 0 }

1 6 0 . 0 ,

' PASS ' :

7 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :

5 8 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # p i
i f not
i f

i n t e r p :

i n t e r p :

i f

s t r i p :

i f g o t . p l t :

i f not n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

2 . 0 ,

' PASS ' :

3 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :
e l s e : # n o t g o t . p l t

9 . 0 ,

' PASS ' :

1 . 0 }

i f not n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

3 1 . 0 ,

' PASS ' :

3 2 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :

2 2 . 0 ,

' PASS ' :

2 2 . 0 }

e l s e : # n o t

e l s e : # n o t

s t r i p
return { ' FAIL ' :
i n t e r p
i f n o t e . a b i _ t a g :
i f g o t . p l t :

0 . 0 ,

' PASS ' :

1 5 . 0 }

return { ' FAIL ' :
e l s e : # n o t g o t . p l t

2 3 . 0 ,

' PASS ' :

1 . 0 }

i f not n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

9 6 . 0 ,

' PASS ' :

2 1 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :

6 . 0 ,

' PASS ' :

2 . 0 }

e l s e : # n o t n o t e . a b i _ t a g

return { ' FAIL ' :

5 3 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # i n t e r p
i f g o t . p l t :
i f not

r e l a . p l t :
return { ' FAIL ' :
e l s e : # r e l a . p l t
i n t e r p :

i f

1 2 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # n o t

return { ' FAIL ' :
i n t e r p
return { ' FAIL ' :

1 7 . 0 ,

' PASS ' :

1 5 . 0 }

5 1 . 0 ,

' PASS ' :

4 8 . 0 }

e l s e : # n o t g o t . p l t
i f n o t e . a b i _ t a g :

i f not n o t e . gnu . b u i l d _ i d :

i f

i n t e r p :

e l s e : # n o t

return { ' FAIL ' :
i n t e r p
return { ' FAIL ' :

8 0 . 0 ,
e l s e : # n o t e . gnu . b u i l d _ i d

0 . 0 ,

' PASS ' :

4 7 . 0 }

' PASS ' :

5 0 1 . 0 }

return { ' FAIL ' :
e l s e : # n o t n o t e . a b i _ t a g

3 5 . 0 ,

' PASS ' :

1 3 2 . 0 }

return { ' FAIL ' :

1 0 . 0 ,

' PASS ' :

0 . 0 }

Figure 3: Decision tree to predict the success of AFL instrumentation with mctoll. Accuracy of 98.80%.

def m c t o l l _ t r e e ( n o t e . a b i _ t a g ,

s t r i p , pi , g o t . p l t ,

d a t a . r e l . ro ,

symtab , n o t e . gnu . b u i l d _ i d ) :

i f n o t e . a b i _ t a g :
return { ' FAIL ' :

1 6 7 2 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # n o t n o t e . a b i _ t a g

Schulte, et al.

i f

s t r i p :

i f p i :

i f not g o t . p l t :

i f d a t a . r e l . r o :

i f not symtab :

return { ' FAIL ' :

5 . 0 ,

' PASS ' :

6 . 0 }

e l s e : # sym ta b

return { ' FAIL ' :
e l s e : # n o t d a t a . r e l . r o

3 . 0 ,

' PASS ' :

0 . 0 }

i f

symtab :
return { ' FAIL ' :

e l s e : # n o t

sym ta b

2 1 . 0 ,

' PASS ' :

4 . 0 }

return { ' FAIL ' :

3 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # g o t . p l t

return { ' FAIL ' :

1 7 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # n o t p i

i f

symtab :

i f g o t . p l t :

return { ' FAIL ' :
e l s e : # n o t g o t . p l t

2 1 . 0 ,

' PASS ' :

0 . 0 }

i f not n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

9 8 . 0 ,

' PASS ' :

6 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :

8 0 . 0 ,

' PASS ' :

3 . 0 }

e l s e : # n o t

sym ta b

return { ' FAIL ' :

6 9 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # n o t

s t r i p
return { ' FAIL ' :

3 3 4 . 0 ,

' PASS ' :

0 . 0 }

A Broad Comparative Evaluation of x86-64 Binary Rewriters

Figure 4: Decision tree to predict the success of AFL instrumentation with retrowrite. Accuracy of 93.02%.

def

r e t r o w r i t e _ t r e e ( n o t e . gnu . b u i l d _ i d , pi , g o t . p l t ,

n o t e . a b i _ t a g ,
d a t a . r e l . ro , i n t e r p ) :

r e l a . p l t ,

i f n o t e . gnu . b u i l d _ i d :

i f not p i :

return { ' FAIL ' :

5 3 1 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # p i

i f g o t . p l t :

return { ' FAIL ' :
e l s e : # n o t g o t . p l t
i f n o t e . a b i _ t a g :

1 6 9 . 0 ,

i f not n o t e . a b i _ t a g :
r e l a . p l t :

i f

i f d a t a . r e l . r o :

' PASS ' :

0 . 0 }

return { ' FAIL ' :

3 6 . 0 ,
e l s e : # n o t d a t a . r e l . r o
7 8 . 0 ,

return { ' FAIL ' :
r e l a . p l t

e l s e : # n o t

' PASS ' :

5 0 . 0 }

' PASS ' :

6 4 . 0 }

return { ' FAIL ' :
e l s e : # n o t e . a b i _ t a g

8 . 0 ,

' PASS ' :

0 . 0 }

i f not d a t a . r e l . r o :
return { ' FAIL ' :
e l s e : # d a t a . r e l . r o
return { ' FAIL ' :

6 4 . 0 ,

1 1 . 0 ,

' PASS ' :

3 2 . 0 }

' PASS ' :

0 . 0 }

e l s e : # n o t n o t e . a b i _ t a g

i f

i n t e r p :

e l s e : # n o t
i f not

return { ' FAIL ' :
i n t e r p
r e l a . p l t :
return { ' FAIL ' :
e l s e : # r e l a . p l t
return { ' FAIL ' :

1 1 . 0 ,

' PASS ' :

0 . 0 }

8 2 . 0 ,

' PASS ' :

3 6 . 0 }

4 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # n o t n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :

1 1 6 6 . 0 ,

' PASS ' :

0 . 0 }

Figure 5: Decision tree to predict the success of AFL instrumentation with zipr. Accuracy of 79.98%.

Schulte, et al.

def

z i p r _ t r e e ( g o t . p l t ,

i n t e r p , pi ,

r e l a . p l t ,

n o t e . gnu . b u i l d _ i d , n o t e . a b i _ t a g ,
s t r i p ) :

i f not g o t . p l t :
i f g o t . p l t :

i f

i n t e r p :

i f not

i n t e r p :

return { ' FAIL ' :

1 9 . 0 ,

' PASS ' :

1 1 4 . 0 }

e l s e : # i n t e r p

return { ' FAIL ' :
i n t e r p

e l s e : # n o t

i f not p i :

1 7 . 0 ,

' PASS ' :

1 1 3 . 0 }

return { ' FAIL ' :

3 0 . 0 ,

' PASS ' :

1 0 3 . 0 }

e l s e : # p i

return { ' FAIL ' :

2 6 . 0 ,

' PASS ' :

1 0 8 . 0 }

e l s e : # n o t g o t . p l t

i f

i n t e r p :
i f not p i :

return { ' FAIL ' :

1 0 . 0 ,

' PASS ' :

2 6 . 0 }

e l s e : # p i

7 . 0 ,

' PASS ' :

2 4 . 0 }

return { ' FAIL ' :
i n t e r p

e l s e : # n o t
i f p i :
i f

r e l a . p l t :

return { ' FAIL ' :

1 4 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # n o t

r e l a . p l t

i f not n o t e . gnu . b u i l d _ i d :

i f not p i :

return { ' FAIL ' :

2 9 . 0 ,

' PASS ' :

5 . 0 }

e l s e : # p i

return { ' FAIL ' :

2 1 . 0 ,
e l s e : # n o t e . gnu . b u i l d _ i d

' PASS ' :

1 0 . 0 }

return { ' FAIL ' :

1 3 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # n o t p i

return { ' FAIL ' :

0 . 0 ,

' PASS ' :

1 5 . 0 }

e l s e : # g o t . p l t
i f not p i :

i f

i n t e r p :

i f g o t . p l t :

i f not n o t e . a b i _ t a g :

i f not n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

4 . 0 ,

' PASS ' :

1 0 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :
e l s e : # n o t e . a b i _ t a g

1 1 . 0 ,

' PASS ' :

7 6 . 0 }

i f not n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

0 . 0 ,

' PASS ' :

1 4 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :

7 . 0 ,

' PASS ' :

2 9 . 0 }

e l s e : # n o t g o t . p l t
return { ' FAIL ' :
i n t e r p

e l s e : # n o t

0 . 0 ,

' PASS ' :

1 6 . 0 }

i f not n o t e . a b i _ t a g :

i f not

s t r i p :
i f g o t . p l t :

i f not n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

4 1 . 0 ,

' PASS ' :

1 3 3 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :
e l s e : # n o t g o t . p l t

1 0 . 0 ,

' PASS ' :

4 5 . 0 }

i f not n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

2 3 . 0 ,

' PASS ' :

4 3 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :

4 1 . 0 ,

' PASS ' :

3 4 . 0 }

e l s e : #

s t r i p

return { ' FAIL ' :
e l s e : # n o t e . a b i _ t a g

i f g o t . p l t :
i f not

s t r i p :

2 1 . 0 ,

' PASS ' :

0 . 0 }

i f not n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

6 3 . 0 ,

' PASS ' :

5 5 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :

3 1 . 0 ,

' PASS ' :

2 7 . 0 }

e l s e : # s t r i p

return { ' FAIL ' :
e l s e : # n o t g o t . p l t
r e l a . p l t :

i f

4 . 0 ,

' PASS ' :

0 . 0 }

return { ' FAIL ' :

6 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # n o t

r e l a . p l t

i f not n o t e . gnu . b u i l d _ i d :

return { ' FAIL ' :

3 2 . 0 ,

' PASS ' :

9 . 0 }

e l s e : # n o t e . gnu . b u i l d _ i d

return { ' FAIL ' :

3 0 . 0 ,

' PASS ' :

6 . 0 }

e l s e : # p i

i f not n o t e . gnu . b u i l d _ i d :

i f n o t e . a b i _ t a g :
return { ' FAIL ' :

3 0 . 0 ,

' PASS ' :

0 . 0 }

e l s e : # n o t n o t e . a b i _ t a g

i f g o t . p l t :

i f not n o t e . a b i _ t a g :

i f

i n t e r p :

e l s e : # n o t

return { ' FAIL ' :
i n t e r p
return { ' FAIL ' :
e l s e : # n o t e . a b i _ t a g

i f

i n t e r p :

e l s e : # n o t

return { ' FAIL ' :
i n t e r p
return { ' FAIL ' :

1 3 . 0 ,

' PASS ' :

1 . 0 }

1 3 4 . 0 ,

' PASS ' :

3 9 . 0 }

8 . 0 ,

' PASS ' :

3 . 0 }

9 8 . 0 ,

' PASS ' :

2 1 . 0 }

e l s e : # n o t g o t . p l t

i f

i n t e r p :

return { ' FAIL ' :
i n t e r p

e l s e : # n o t

0 . 0 ,

' PASS ' :

9 . 0 }

i f not n o t e . a b i _ t a g :
return { ' FAIL ' :
e l s e : # n o t e . a b i _ t a g
return { ' FAIL ' :
e l s e : # n o t e . gnu . b u i l d _ i d

4 0 . 0 ,

3 7 . 0 ,

' PASS ' :

2 2 . 0 }

' PASS ' :

7 . 0 }

return { ' FAIL ' :

3 5 5 . 0 ,

' PASS ' :

0 . 0 }

