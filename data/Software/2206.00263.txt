PiDRAM: An FPGA-based Framework
for End-to-end Evaluation of Processing-in-DRAM Techniques

Ataberk Olgun§

Juan Gomez Luna§

Konstantinos Kanellopoulos§

Behzad Salami§

Hasan Hassan§

Oğuz Ergin†

Onur Mutlu§

§ETH Zürich

†TOBB University of Economics and Technology

2
2
0
2

n
u
J

1

]

R
A
.
s
c
[

1
v
3
6
2
0
0
.
6
0
2
2
:
v
i
X
r
a

1. Background
DRAM-based main memory is used in nearly all computing
systems as a major component. Modern memory-intensive
workloads have increasing memory bandwidth, latency, and ca-
pacity requirements. However, DRAM vendors often prioritize
memory capacity scaling over latency and bandwidth [1–4]. As
a result, main memory is an increasingly worsening bottleneck
in computing systems [3, 5–9].

One way of overcoming the main memory bottleneck is
to move computation near memory, a paradigm known as
processing-in-memory (PiM) [6, 7, 10–150]. PiM reduces mem-
ory latency between the memory units and the compute units,
enables the compute units to exploit the large internal band-
width within memory devices, and reduces the overall power
consumption of the system by eliminating the need for trans-
ferring data over power-hungry off-chip interfaces [3, 5, 9].

PiM techniques provide significant performance benefits and
energy savings by exploiting the high internal bit-level paral-
lelism of DRAM for different arithmetic [50, 52, 71, 73–78, 151]
and data movement operations [49,51,68,69]. Recent works [72,
75, 76, 151, 152] show that some of these PiM techniques can
already be supported in contemporary, off-the-shelf DRAM
chips with modifications only to the memory controller. Given
that DRAM is a dominant memory technology, such commod-
ity DRAM based PiM techniques provide a promising way to
improve the performance and energy efficiency of existing and
future systems at no additional DRAM hardware cost.

2. Motivation
Integration of PiM techniques in a real system imposes non-
trivial challenges that require further research to find appro-
priate solutions. For example, in-DRAM copy and initializa-
tion techniques [51, 72] that copy data in bulk (e.g., one or
multiple memory pages) require modifications related to mem-
ory management that affect various different parts of the sys-
tem. First, in-DRAM copy and initialization techniques have
specific memory allocation and alignment requirements (e.g.,
page-granularity source and destination operands should be al-
located and aligned at row granularity within the same DRAM
subarray). These requirements are not satisfied by existing
system-level memory allocation primitives (e.g., malloc [153],
posix_memalign [154]). Second, the source operands (usually
one or more pages) of a copy operation must be up to date in
DRAM (i.e., it should not have dirty copies in CPU caches).
Existing cache coherence management operations (e.g., the

1

x86 CLFLUSH instruction [155]) cannot efficiently evict page-
granularity source operands from the caches [85,156], as cache
coherence management operations need to query the coher-
ence states of all cache blocks of the source operands (i.e., even
the coherence states of the cache blocks that are not present
in CPU caches are queried) [156].

2.1. Limitations of the State-of-the-Art
System integration challenges of PiM techniques can be effi-
ciently studied in existing general-purpose computing systems
(e.g., personal computers, cloud computers, embedded sys-
tems), special-purpose testing platforms (e.g., SoftMC [157]), or
system simulators (e.g., gem5 [158, 159], Ramulator [160, 161],
Ramulator-PIM [162], zsim [163]). First, many commodity
DRAM based PiM techniques1 rely on non-standard DDRx oper-
ation, where manufacturer-recommended timing parameters
for DDRx commands are violated [72, 75, 76, 151, 152] (or oth-
erwise new DRAM commands are added, which requires new
chip designs and interfaces).2 Existing general-purpose com-
puting systems do not permit dynamically changing DDRx
timing parameters [1, 76, 157, 164, 165], which is required to
integrate these PiM techniques into real systems. Second, prior
works [72,75,76] show that the reliability of commodity DRAM
based PiM techniques is highly dependent on environmental
conditions such as temperature and voltage fluctuations. These
effects are exacerbated by the non-standard behavior of PiM
techniques in real DRAM chips. Although special purpose test-
ing platforms (e.g., SoftMC [157]) can be used to conduct relia-
bility studies, these platforms do not model an end-to-end com-
puting system, where system integration of PiM techniques
can be studied. Third, system simulators (e.g., gem5 [158, 159],
Ramulator [160, 161], Ramulator-PIM [162], zsim [163]) can
model end-to-end computing systems. However, they (i) do not
model DRAM operation beyond manufacturer-recommended
timing parameters, (ii) do not have a way of interfacing with
real DRAM chips that embody undisclosed and unique char-
acteristics that have implications on how PiM techniques are
integrated into real systems (e.g., proprietary and chip-specific
DRAM internal address mapping [166, 167]), and (iii) cannot
support studies on the reliability of PiM techniques since sys-
tem simulators do not model environmental conditions.

1Commodity DRAM based PiM techniques are PiM techniques that can
already be supported in existing off-the-shelf DRAM chips with hardware
modifications only to the memory controller.

2We are especially interested in PiM techniques that do not require any

modification to the DRAM chips or the DRAM interface.

 
 
 
 
 
 
3. PiDRAM Framework
Our goal is to design and implement a flexible framework that
can be used to solve system integration challenges and ana-
lyze trade-offs of end-to-end implementations of commodity
DRAM based PiM techniques. To this end, we develop the
PiDRAM (Processing-in-DRAM) framework, the first flexible,
end-to-end, and open source framework that enables system
integration studies and evaluation of real PiM techniques using
real unmodified DRAM chips.

PiDRAM facilitates system integration studies of commod-
ity DRAM based PiM techniques by providing a set of four
customizable hardware and software components that can be
used as a common basis to enable system support for such
techniques in real systems. Figure 1 presents an overview of
PiDRAM’s components.

3.1. Hardware Components
PiDRAM comprises two key hardware components. Both of
these components are designed with the goal to provide a flex-
ible and easy to use framework for evaluating PiM techniques.
(cid:182) PiM Operations Controller (POC). POC decodes and exe-
cutes PiDRAM instructions (e.g., RowClone-Copy [51]) that are
used by the programmer to perform PiM operations. POC com-
municates with the rest of the system over two well-defined
interfaces. First, it communicates with the CPU over a memory-
mapped interface, where the CPU can send data to or receive
data from POC using memory store and load instructions. The
CPU accesses the memory-mapped registers (instruction, data,
and flag registers) in POC to execute in-DRAM operations.
This way, we improve the portability of the framework and
facilitate porting it to systems that employ different instruc-
tion set architectures. Second, POC communicates with the
memory controller to perform PiM operations in the DRAM
chip over a simple hardware interface. To do so, POC (i) re-
quests the memory controller to perform a PiM operation, (ii)
waits until the memory controller performs the operation, and
(iii) receives the result of the PiM operation from the memory
controller. The CPU can read the result of the operation by
executing load instructions that target the data register in POC.
(cid:183) PiDRAM Memory Controller. PiDRAM’s memory con-
troller provides an easy-to-extend basis for commodity DRAM
based PiM techniques that require issuing DRAM commands

with violated timing parameters [72, 75, 76, 151, 152]. The
memory controller is designed modularly and requires easy-
to-make modifications to its scheduler to implement new PiM
techniques. For instance, our modular design enables sup-
porting RowClone [51] operations using only 60 additional
lines of Verilog code on top of the baseline custom memory
controller’s scheduler that implements conventional DRAM
operations [2, 167–176] (e.g., activate, precharge, read, write).

3.2. Software Components
PiDRAM comprises two key software components that comple-
ment and control PiDRAM’s hardware components to provide
a flexible and easy to use end-to-end PiM framework.
(cid:184) Extensible Software Library (pimolib). The extensible
library (PiM operations library) allows system designers to
implement software support for PiM techniques. Pimolib con-
tains customizable functions that interface with the POC to
perform PiM operations in real unmodified DRAM chips. A
typical function in pimolib performs a PiM operation in four
steps: It (i) writes a PiDRAM instruction to the POC’s instruc-
tion register, (ii) sets the Start flag in the POC’s flag register,
(iii) waits for the POC to set the Ack flag in the POC’s flag
register, and (iv) reads the result of the PiM operation from
the POC’s data register (e.g., the true random number after
performing an in-DRAM true random number generation op-
eration, Section 4.2).
(cid:185) Custom Supervisor Software. The supervisor software
contains the necessary OS primitives (e.g., memory manage-
ment, allocation, and alignment specialized for RowClone [51]).
This facilitates developing end-to-end integration of PiM tech-
niques in the system as these techniques require modifications
across the software stack. For example, integrating RowClone
end-to-end in the full system requires a new memory allo-
cation mechanism (Section 4.2) that can satisfy the memory
allocation constraints of RowClone [51, 177].

3.3. General PiDRAM Execution Workflow
We describe the general workflow for a PiDRAM operation (e.g.,
RowClone-Copy [51], random number generation using D-
RaNGe [76]) in Figure 2 over an example copy() function that
is called by the user to perform a RowClone-Copy operation
in DRAM.

Figure 1: PiDRAM overview. Modified hardware (in green) and software (in blue) components. Unmodified components are in
gray. A pimolib function executes load and store instructions in the CPU to perform PiM operations (in red). We use yellow to
highlight the key hardware structures that are controlled by the user to perform PiM operations.

2

User ApplicationCustom Supervisor SoftwareRocket ChipPiDRAM  Memory ControllerDRAM ModulePiM Ops. Controller (POC)RISC-V CPU CoreInstruction RegisterFlag RegisterPhysical InterfaceDDR3 InterfacepimolibSystem CallsFunction Callspimolib functionSTORE InstructionLOAD InstructionData RegisterCommand SchedulerMemory Bus4312Figure 2: Workflow for a PiDRAM RowClone-Copy operation.

The user makes a system call to the custom supervisor soft-
ware (cid:172) that in turn calls the copy(source, destination)
function in the pimolib (cid:173). The function executes two store
instructions in the RISC-V core (cid:174). The first store instruction
updates the instruction register with the copy instruction (i.e.,
the instruction that performs a RowClone-Copy operation in
DRAM) (cid:175) and the second store instruction sets the Start flag
in the flag register to logic-1 (cid:176) in the POC. When the Start
flag is set, the POC instructs the PiDRAM memory controller
to perform a RowClone-Copy operation using violated timing
parameters (cid:177). The POC waits until the memory controller
starts executing the operation, after which it sets the Start
flag to logic-0 and the Ack flag to logic-1 (cid:178), indicating that
it started the execution of the PiM operation. The PiDRAM
memory controller performs the RowClone-Copy operation by
issuing a set of DRAM commands with violated timing param-
eters (cid:179). When the last DRAM command is issued, the memory
controller sets the Finish flag (denoted as Fin.
in Figure 2)
in the flag register to logic-1 (cid:180), indicating the end of execu-
tion for the last PiM operation that the memory controller
acknowledged. The copy function periodically checks either
the Ack or the Finish flag in the flag register (depending on a
user-supplied argument) by executing load instructions that
target the flag register (cid:181). When the periodically checked flag
is set, the copy function returns. This way, the copy function
optionally blocks until the start (i.e., the Ack flag is set) or
the end (i.e., the Finish flag is set) of the execution of the PiM
operation (in this example, RowClone-Copy).3

4. FPGA Prototype & Case Studies
PiDRAM allows (i) interfacing with and performing in-DRAM
operations on real unmodified DRAM chips, (ii) observing the
effects of environmental conditions on PiM techniques, and (iii)
conducting end-to-end system-level studies on PiM techniques
using real DRAM chips. We demonstrate the versatility and
ease-of-use of PiDRAM by prototyping it on an FPGA board
and conducting two case studies of PIM computation.

3The data register is not used in RowClone-Copy [51] operations because
the result of the RowClone-Copy operation is stored in memory (i.e., the source
memory row is copied to the destination memory row). The data register
is used in D-RaNGe [76] operations, as described in [177]. When used, the
command scheduler stores the random numbers generated by the D-RaNGe
operation in the data register. To read the generated random numbers, we
implement a pimolib function called rand_dram() that executes load instruc-
tions in the RISC-V core to retrieve the random numbers from the data register
in the POC.

4.1. FPGA Prototype
We demonstrate a prototype of PiDRAM on an FPGA-based
platform (Xilinx ZC706 [178]) that implements an open-source
RISC-V system (Rocket Chip [179]). Our custom supervisor
software extends the RISC-V PK [180] to support the necessary
OS primitives on PiDRAM’s prototype. We perform in-DRAM
operations in real unmodified DDR3 DRAM chips that are
connected to the FPGA-based platform. Figure 3 shows our
FPGA prototype.

Figure 3: PiDRAM’s FPGA prototype.

4.2. Case Studies
To demonstrate the flexibility and ease of use of PiDRAM,
we implement two PiM techniques on unmodified DRAM
(1) RowClone [51],
chips by violating timing parameters:
an in-DRAM copy and initialization mechanism (using com-
mand sequences proposed by ComputeDRAM [72]), and (2)
D-RaNGe [76], an in-DRAM true random number generator
based on DRAM activation-latency failures induced by viola-
tion of manufacturer-recommended DRAM timing parameters.
To support RowClone, we (i) customize PiDRAM’s memory
controller to issue carefully-engineered sequences of DRAM
commands that perform copy operations in DRAM, and (ii) ex-
tend the custom supervisor software to implement a new mem-
ory management mechanism that satisfies the memory alloca-
tion and alignment requirements of RowClone [51, 177]. The
key idea of the memory management mechanism is to allocate
physical addresses in the same DRAM subarray [49–51, 167]
for the source and destination operands (i.e., rows) of the copy
operation. We develop a new methodology that checks if Row-
Clone operations on randomly-generated physical addresses
succeed or fail, to find physical row addresses that are in the
same DRAM subarray. We explain this methodology and our
techniques in more detail in our arXiv paper [177].

3

12User ApplicationCustom Supervisor SoftwareRocket ChipPiDRAM  Memory ControllerDRAM ModulePOCRISC-V CPU CorePhysical InterfaceDDR3 InterfacepimolibSystem Callscopy(S, D) S: source D: destinationcopy (S, D)copy (S, D)STORE InstructionLOAD Instruction410567Data RegisterCommand SchedulerMemory BusStart (S)100Ack (A)Fin. (F)101983Host MachineFPGA BoardRISC-V SystemPiM-Enabled DIMMTo support D-RaNGe, we (i) make simple modifications to
the PiDRAM memory controller to implement a new storage
structure that buffers random numbers (i.e., a random number
buffer) and (ii) extend the software library with functions that
retrieve data from the random number buffer and supply it to
the user program.

implementation provides a solid foundation for future work
on system integration of DRAM-based PiM security primi-
tives (e.g., PUFs [75,152], TRNGs [76,151,152]), implemented
using real unmodified DRAM chips.

• PiDRAM is open sourced and freely available on Github:

https://github.com/CMU-SAFARI/PiDRAM.

5. Key Results & Contributions
Our evaluation of RowClone shows that an end-to-end imple-
mentation of RowClone achieves (i) 118.5× speedup for copy
and 88.7× speedup for initialization operations, assuming data
in DRAM is up to date and the source operand is not cached
in CPU caches (i.e., no cache coherence management opera-
tions are required) and (ii) 14.6× speedup for copy and 12.6×
initialization operations assuming data in DRAM is up to date
but the source operand has cached copies in CPU caches (i.e.,
a cache coherence management operation must be performed
for every cache block of the source operand) over CPU-based
copy (i.e., conventional memcpy [181]) and initialization (i.e.,
conventional calloc [182]) operations.

We make two observations from our evaluation of D-RaNGe.
First, end-to-end integration of D-RaNGe into a real system
provides true random numbers at low latency. Our implemen-
tation can generate a 4-bit random number in 220 ns. Second,
an end-to-end integration of D-RaNGe into a real system gener-
ates true random numbers at high throughput. Our implemen-
tation achieves 8.30 Mb/s sustained throughput. PiDRAM’s
D-RaNGe implementation can be optimized to generate ran-
dom numbers more frequently (i.e., at higher throughput). We
leave such optimizations to PiDRAM’s D-RaNGe implementa-
tion for future work.

Over the Verilog and C++ codebase provided by PiDRAM,
integrating RowClone end-to-end in the full system takes 198
lines of Verilog and 565 lines of C++ code and integrating D-
RaNGe end-to-end in the full system takes 190 lines of Verilog
and 78 lines of C++ code.

Our contributions are as follows:

• We develop PiDRAM, the first flexible framework that en-
ables end-to-end integration and evaluation of PiM tech-
niques using real unmodified DRAM chips.

• We develop a prototype of PiDRAM on an FPGA-based plat-
form. To demonstrate the ease-of-use and evaluation benefits
of PiDRAM, we implement two state-of-the-art DRAM-based
PiM techniques, RowClone and D-RaNGe, and evaluate them
on PiDRAM’s prototype using unmodified DDR3 DRAM
chips.

• We devise a new memory management mechanism that sat-
isfies the memory allocation and alignment requirements
of RowClone [51]. We demonstrate that our mechanism en-
ables RowClone end-to-end in the full system, and provides
significant performance improvements over traditional CPU-
based copy and initialization operations (memcpy [181] and
calloc [182]) as demonstrated on our PiDRAM prototype.
• We implement and evaluate a state-of-the-art DRAM true
random number generation technique (D-RaNGe [76]). Our

6. Summary
We introduce the first flexible, end-to-end, and open source
in-DRAM computation framework, called PiDRAM. Unlike ex-
isting evaluation frameworks and prior work, PiDRAM allows
us to conduct end-to-end full system studies of commodity
DRAM based PiM techniques that use real unmodified DRAM
chips. We demonstrate PiDRAM’s versatility and ease of use by
prototyping it on an FPGA board and conducting case studies
of two state-of-the-art PiM techniques for in-DRAM data copy
& initialization (RowClone [51]) and in-DRAM true random
number generation (D-RaNGe [76]). We show that an end-to-
end implementation of (i) RowClone greatly improves data
copy and initialization performance at the full system level,
and (ii) D-RaNGe provides true random numbers to applica-
tions at high throughput. We hope and believe that researchers
and industry will benefit from and build on PiDRAM to better
evaluate PiM techniques and their benefits in real systems.

Acknowledgements
We thank the SAFARI Research Group members for valuable
feedback and the stimulating intellectual environment they
provide. We acknowledge the generous gifts provided by our
industrial partners, including Google, Huawei, Intel, Microsoft,
and VMware. We acknowledge support from the Semicon-
ductor Research Corporation and the ETH Future Computing
Laboratory.

This invited extended abstract is a summary version of our
prior work [177]. A presentation that describes this work can
be found at [183].

References
[1] K. K. Chang et al., “Understanding Latency Variation in Modern DRAM Chips: Ex-
perimental Characterization, Analysis, and Optimization,” in SIGMETRICS, 2016.
[2] D. Lee et al., “Tiered-Latency DRAM: A Low Latency and Low Cost DRAM Archi-

tecture,” in HPCA, 2013.

[3] O. Mutlu et al., “A Modern Primer on Processing in Memory,” Emerging Computing:

From Devices to Systems - Looking Beyond Moore and Von Neumann, 2021.

“A Case for Transparent Reliability in DRAM Systems,”

[4] M. Patel et al.,

arXiv:2204.10378, 2022.

[5] S. Ghose et al., “Processing-in-Memory: A Workload-driven Perspective,” IBM JRD,

2019.

[6] O. Mutlu, “Memory Scaling: A Systems Architecture Perspective,” IMW, 2013.
[7] O. Mutlu et al., “Research Problems and Opportunities in Memory Systems,” SU-

PERFRI, 2014.

[8] O. Mutlu, “Main Memory Scaling: Challenges and Solution Directions,” 2015, In-
vited Book Chapter in More than Moore Technologies for Next Generation Com-
puter Design, pp. 127-153, Springer.

[9] O. Mutlu et al., “Processing Data Where It Makes Sense: Enabling In-Memory Com-

[10] I. Fernandez et al., “NATSA: A Near-Data Processing Accelerator for Time Series

putation,” MicPro, 2019.

Analysis,” in ICCD, 2020.

[11] D. S. Cali et al., “GenASM: A High-Performance, Low-Power Approximate String
Matching Acceleration Framework for Genome Sequence Analysis,” in MICRO,
2020.

[12] J. S. Kim et al., “GRIM-Filter: Fast Seed Location Filtering in DNA Read Mapping

Using Processing-in-Memory Technologies,” BMC Genomics, 2018.

4

cessing,” in ISCA, 2015.

Accelerators,” in ISCA, 2019.

Processing-in-Memory,” CAL, 2016.

[13] J. Ahn et al., “PIM-Enabled Instructions: A Low-Overhead, Locality-Aware

[57] Y. Levy et al., “Logic Operations in Memory Using a Memristive Akers Array,” Mi-

Processing-in-Memory Architecture,” in ISCA, 2015.

croelectronics Journal, 2014.

[14] J. Ahn et al., “A Scalable Processing-in-Memory Accelerator for Parallel Graph Pro-

[58] S. Kvatinsky et al., “MAGIC—Memristor-Aided Logic,” IEEE TCAS II: Express Briefs,

[15] A. Boroumand et al., “Google Workloads for Consumer Devices: Mitigating Data

[59] A. Shafiee et al., “ISAAC: A Convolutional Neural Network Accelerator with In-Situ

Movement Bottlenecks,” in ASPLOS, 2018.

Analog Arithmetic in Crossbars,” in ISCA, 2016.

[16] A. Boroumand et al., “CoNDA: Efficient Cache Coherence Support for near-Data

[60] S. Kvatinsky et al., “Memristor-Based IMPLY Logic Design Procedure,” in ICCD,

2014.

2011.

[17] A. Boroumand et al., “LazyPIM: An Efficient Cache Coherence Mechanism for

[61] S. Kvatinsky et al., “Memristor-Based Material Implication (IMPLY) Logic: Design

Principles and Methodologies,” TVLSI, 2014.

[18] G. Singh et al., “NAPEL: Near-memory Computing Application Performance Pre-

[62] P.-E. Gaillardon et al., “The Programmable Logic-in-Memory (PLiM) Computer,” in

diction via Ensemble Learning,” in DAC, 2019.

DATE, 2016.

[19] H. Asghari-Moghaddam et al., “Chameleon: Versatile and Practical Near-DRAM

[63] D. Bhattacharjee et al., “ReVAMP: ReRAM based VLIW Architecture for In-Memory

Acceleration Architecture for Large Memory Systems,” in MICRO, 2016.

Computing,” in DATE, 2017.

[20] S. L. Xi et al., “Beyond the Wall: Near-Data Processing for Databases,” in DaMoN,

[64] S. Hamdioui et al., “Memristor Based Computation-in-Memory Architecture for

2015.

[21] A. Farmahini-Farahani et al., “NDA: Near-DRAM acceleration architecture leverag-
ing commodity DRAM devices and standard memory modules,” in HPCA, 2015.
[22] M. Gao et al., “Practical Near-Data Processing for In-Memory Analytics Frame-

[23] M. Gao et al., “HRL: Efficient and Flexible Reconfigurable Logic for Near-Data Pro-

Highly-Banked Memories,” CAL, 2020.

[24] B. Gu et al., “Biscuit: A Framework for Near-Data Processing of Big Data Work-

DRAM Data Relocation and Caching,” in MICRO, 2020.

[25] M. Hashemi et al., “Accelerating Dependent Cache Misses with an Enhanced Mem-

namic Random-Access Memory (DRAM),” IBM JRD, 2002.

works,” in PACT, 2015.

cessing,” in HPCA, 2016.

loads,” in ISCA, 2016.

ory Controller,” in ISCA, 2016.

Data-intensive Applications,” in DATE, 2015.

[65] L. Xie et al., “Fast Boolean Logic Mapped on Memristor Crossbar,” in ICCD, 2015.
[66] S. Hamdioui et al., “Memristor for Computing: Myth or Reality?” in DATE, 2017.
[67] J. Yu et al., “Memristive Devices for Computation-in-Memory,” in DATE, 2018.
[68] S. H. S. Rezaei et al., “NoM: Network-on-Memory for Inter-Bank Data Transfer in

[69] Y. Wang et al., “FIGARO: Improving System Performance via Fine-Grained In-

[70] J. A. Mandelman et al., “Challenges and Future Directions for the Scaling of Dy-

[71] X. Xin et al., “ELP2IM: Efficient and Low Power Bitwise Operation Processing in

[26] M. Hashemi et al., “Continuous Runahead: Transparent Hardware Acceleration for

DRAM,” in HPCA, 2020.

Memory Intensive Workloads,” in MICRO, 2016.

[27] K. Hsieh et al.,

“Transparent Offloading and Mapping (TOM): Enabling

Programmer-Transparent Near-Data Processing in GPU Systems,” in ISCA, 2016.

[28] D. Kim et al., “Neurocube: A Programmable Digital Neuromorphic Architecture

with High-Density 3D Memory,” in ISCA, 2016.

[72] F. Gao et al., “ComputeDRAM: In-Memory Compute Using Off-the-Shelf DRAMs,”

[73] S. Li et al., “DRISA: A DRAM-Based Reconfigurable In-Situ Accelerator,” in MICRO,

in MICRO, 2019.

2017.

[74] Q. Deng et al., “DrAcc: a DRAM Based Accelerator for Accurate CNN Inference,”

[29] G. Kim et al., “Toward Standardized Near-Data Processing with Unrestricted Data

in DAC, 2018.

Placement for GPUs,” in SC, 2017.

[30] Z. Liu et al., “Concurrent Data Structures for Near-Memory Computing,” in SPAA,

2017.

[31] A. Morad et al., “GP-SIMD Processing-in-Memory,” ACM TACO, 2015.
[32] L. Nai et al., “GraphPIM: Enabling Instruction-Level PIM Offloading in Graph Com-

[33] A. Pattnaik et al., “Scheduling Techniques for GPU Architectures with Processing-

DRAM,” in ASPLOS, 2021.

puting Frameworks,” in HPCA, 2017.

in-Memory Capabilities,” in PACT, 2016.

[75] J. Kim et al., “The DRAM Latency PUF: Quickly Evaluating Physical Unclonable
Functions by Exploiting the Latency–Reliability Tradeoff in Modern DRAM De-
vices,” in HPCA, 2018.

[76] J. Kim et al., “D-RaNGe: Using Commodity DRAM Devices to Generate True Ran-

dom Numbers with Low Latency and High Throughput,” in HPCA, 2019.

[77] N. Hajinazar et al., “SIMDRAM: A Framework for Bit-Serial SIMD Processing Using

[78] M. F. Ali et al., “In-Memory Low-Cost Bit-Serial Addition Using Commodity DRAM

[34] S. H. Pugsley et al., “NDC: Analyzing the Impact of 3D-Stacked Memory+Logic

Technology,” in TCAS-I, 2019.

Devices on MapReduce Workloads,” in ISPASS, 2014.

[79] R. Ronen et al., “The Bitlet Model: A Parameterized Analytical Model to Compare

[35] D. P. Zhang et al., “TOP-PIM: Throughput-Oriented Programmable Processing in

PIM and CPU Systems,” J. Emerg. Technol. Comput. Syst., 2022.

Memory,” in HPDC, 2014.

[36] Q. Zhu et al., “Accelerating Sparse Matrix-Matrix Multiplication with 3D-Stacked

Logic-in-Memory Hardware,” in HPEC, 2013.

[37] B. Akin et al., “Data Reorganization in Memory Using 3D-Stacked DRAM,” in ISCA,

[80] Y. Zha et al., “Liquid Silicon: A Nonvolatile Fully Programmable Processing-In-
Memory Processor with Monolithically Integrated ReRAM for Big Data/Machine
Learning Applications,” in Symposium on VLSI Circuits, 2019.

“Inversion Optimization in Majority-Inverter Graphs,”

in

[81] E. Testa et al.,

NANOARCH, 2016.

terial Implication,” Nature, 2010.

[83] Intel, “Taking Neuromorphic Computing to the Next Level with Loihi 2,” Technol-

2015.

Memory,” in ASPLOS, 2017.

[38] M. Gao et al., “Tetris: Scalable and Efficient Neural Network Acceleration with 3D

[82] J. Borghetti et al., “Memristive Switches Enable Stateful Logic Operations via Ma-

[39] M. P. Drumond Lages de Oliveira et al., “The Mondrian Data Engine,” in ISCA, 2017.
[40] G. Dai et al., “GraphH: A Processing-in-Memory Architecture for Large-scale

ogy Brief, 2022.

Graph Processing,” IEEE TCAD, 2018.

[84] G. W. Burr et al., “Neuromorphic Computing Using Non-volatile Memory,” Ad-

[41] M. Zhang et al., “GraphP: Reducing Communication for PIM-based Graph Process-

vances in Physics: X, 2017.

ing with Efficient Data Partition,” in HPCA, 2018.

[42] Y. Huang et al., “A Heterogeneous PIM Hardware-Software Co-Design for Energy-

Efficient Graph Processing,” in IPDPS, 2020.

[43] Y. Zhuo et al., “GraphQ: Scalable PIM-based Graph Processing,” in MICRO, 2019.
[44] C. Giannoula et al., “SynCron: Efficient Synchronization Support for Near-Data-

Processing Architectures,” in HPCA, 2021.
[45] S. Aga et al., “Compute Caches,” in HPCA, 2017.
[46] C. Eckert et al., “Neural Cache: Bit-serial In-cache Acceleration of Deep Neural

Networks,” in ISCA, 2018.

[47] D. Fujiki et al., “Duality Cache for Data Parallel Acceleration,” in ISCA, 2019.
[48] M. Kang et al., “An Energy-Efficient VLSI Architecture for Pattern Recognition via

Deep Embedding of Computation in SRAM,” in ICASSP, 2014.

[49] K. K. Chang et al., “Low-Cost Inter-Linked Subarrays (LISA): Enabling Fast Inter-

Subarray Data Movement in DRAM,” in HPCA, 2016.

[50] V. Seshadri et al., “Ambit: In-Memory Accelerator for Bulk Bitwise Operations Us-

ing Commodity DRAM Technology,” in MICRO, 2017.

[85] V. Seshadri, “Simple DRAM and Virtual Memory Abstractions to Enable Highly
Efficient Memory Systems,” Ph.D. dissertation, Carnegie Mellon University, 2016.

[86] V. Seshadri et al., “Fast Bulk Bitwise AND and OR in DRAM,” CAL, 2015.
[87] V. Seshadri et al., “Buddy-RAM: Improving the Performance and Efficiency of Bulk

Bitwise Operations Using DRAM,” arXiv:1611.09988, 2016.

[88] V. Seshadri et al., “Simple Operations in Memory to Reduce Data Movement,” in

Advances in Computers, Volume 106, 2017.

[89] V. Seshadri et al., “The Processing Using Memory Paradigm: In-DRAM Bulk Copy,

Initialization, Bitwise AND and OR,” arXiv:1610.09603, 2016.

[90] Q. Guo et al., “3D-Stacked Memory-Side Acceleration: Accelerator and System De-

sign,” in WoNDP, 2014.

[91] S. Cho et al., “McDRAM v2: In-Dynamic Random Access Memory Systolic Array
Accelerator to Address the Large Model Problem in Deep Neural Networks on the
Edge,” IEEE Access, 2020.

[92] J. D. Ferreira et al., “pLUTo: In-DRAM Lookup Tables to Enable Massively Parallel

General-Purpose Computation,” arXiv:2104.07699, 2021.

[51] V. Seshadri et al., “RowClone: Fast and Energy-Efficient In-DRAM Bulk Data Copy

[93] Y. Kang et al., “FlexRAM: Toward an Advanced Intelligent Memory System,” in

and Initialization,” in MICRO, 2013.

computing,” in GLSVLSI, 2019.

[52] S. Angizi et al., “Graphide: A Graph Processing Accelerator Leveraging In-dram-

[94] C. Giannoula et al., “Towards Efficient Sparse Matrix Vector Multiplication on Real

[53] S. Li et al., “Pinatubo: A Processing-in-Memory Architecture for Bulk Bitwise Op-

erations in Emerging Non-Volatile Memories,” in DAC, 2016.

[54] S. Angizi et al., “PIMA-Logic: A Novel Processing-in-Memory Architecture for

Highly Flexible and Energy-efficient Logic Computation,” in DAC, 2018.

Processing-in-Memory Architectures,” in SIGMETRICS, 2022.

[95] R. C. Murphy et al., “The Characterization of Data Intensive Memory Workloads

on Distributed PIM Systems,” in Intelligent Memory Systems.

Springer, 2001.

[96] J. Landgraf et al., “Combining Emulation and Simulation to Evaluate a Near Mem-

ory Key/Value Lookup Accelerator,” arXiv:2015.06594, 2021.

[55] S. Angizi et al., “CMP-PIM: An Energy-Efficient Comparator-Based Processing-in-

[97] H. Ahmed et al., “A Compiler for Automatic Selection of Suitable Processing-in-

ICCD, 1999.

[56] S. Angizi et al., “AlignS: A Processing-in-Memory Accelerator for DNA Short Read

[98] G. Singh et al., “FPGA-based Near-Memory Acceleration of Modern Data-Intensive

Memory Instructions,” in DATE, 2019.

Memory Neural Network Accelerator,” in DAC, 2018.

Alignment Leveraging SOT-MRAM,” in DAC, 2019.

5

[110] Y. Zha et al., “Hyper-AP: Enhancing Associative Processing Through A Full-Stack

True Random Numbers,” in ICCE, 2019.

Applications,” IEEE Micro, 2021.

ware Issues,” in MEMSYS, 2019.

[99] A. Rodrigues et al., “Towards a Scatter-Gather Architecture: Hardware and Soft-

[143] G. Singh et al., “Accelerating Weather Prediction using Near-Memory Reconfig-

[100] N. M. Ghiasi et al., “GenStore: A High-Performance and Energy-Efficient In-Storage

[144] H. Shin et al., “McDRAM: Low latency and energy-efficient matrix computations

2002.

urable Fabric,” ACM TRETS, 2021.

in DRAM,” IEEE TCADICS, 2018.

Computing System for Genome Sequence Analysis,” in ASPLOS, 2022.

[101] G. F. Oliveira et al., “DAMOV: A New Methodology and Benchmark Suite for Eval-

uating Data Movement Bottlenecks,” IEEE Access, 2021.

[102] S. Jain et al., “Computing-in-Memory with Spintronics,” in DATE, 2018.
[103] A. Boroumand et al., “Google Neural Network Models for Edge Devices: Analyzing
and Mitigating Machine Learning Inference Bottlenecks,” arXiv:2109.14320, 2021.
[104] V. Zois et al., “Massively Parallel Skyline Computation for Processing-in-Memory

Architectures,” in PACT, 2018.

[105] A. Boroumand et al., “Google Neural Network Models for Edge Devices: Analyzing

and Mitigating Machine Learning Inference Bottlenecks,” in PACT, 2021.

[106] W. H. Kautz, “Cellular Logic-in-Memory Arrays,” IEEE TC, 1969.
[107] R. Nair, “Evolution of Memory Architecture,” Proceedings of the IEEE, 2015.
[108] S. Lloyd et al., “Near Memory Key/Value Lookup Acceleration,” in MEMSYS, 2017.
[109] P. C. Santos et al., “Operand Size Reconfiguration for Big Data Processing in Mem-

ory,” in DATE, 2017.

Optimization,” in ISCA, 2020.

[111] P. Gu et al., “iPIM: Programmable In-Memory Image Processing Accelerator using

Near-Bank Architecture,” in ISCA, 2020.

[112] B. Asgari et al., “FAFNIR: Accelerating Sparse Gathering by Using Efficient Near-

Memory Intelligent Reduction,” in HPCA, 2021.

[113] C. Giannoula et al., “SparseP: Towards Efficient Sparse Matrix Vector Multiplication

on Real Processing-In-Memory Systems,” arXiv:2201.05072, 2022.

[114] A. Boroumand et al., “Polynesia: Enabling Effective Hybrid Transactional Analyti-
cal Databases with Specialized Hardware Software Co-Design,” in ICDE, 2022.
[115] Y. Xi et al., “In-Memory Learning With Analog Resistive Switching Memory: A

Review and Perspective,” Proceedings of the IEEE, 2020.

[116] R. Balasubramonian et al., “Near-Data Processing: Insights from a MICRO-46 Work-

shop,” IEEE Micro, 2014.

[117] J. M. Herruzo et al., “Enabling Fast and Energy-Efficient FM-Index Exact Matching

Using Processing-Near-Memory,” The Journal of Supercomputing, 2021.

[118] A. C. Jacob et al., “Compiling for the Active Memory Cube,” Tech. rep. RC25644

(WAT1612-008). IBM Research Division, Tech. Rep., 2016.

[119] D. Fujiki et al., “In-Memory Data Parallel Processor,” in ASPLOS, 2018.
[120] M. Gokhale et al., “Processing in Memory: The Terasys Massively Parallel PIM

[126] P. M. Kogge, “EXECUBE - A New Architecture for Scaleable MPPs,” in ICPP, 1994.
[127] D. E. Shaw et al., “The NON-VON Database Machine: A Brief Overview,” IEEE

tecture,” in BIBM, 2020.

Database Eng. Bull., 1981.

2000.

[129] M. Besta et al., “SISA: Set-Centric Instruction Set Architecture for Graph Mining

on Processing-in-Memory Systems,” in MICRO, 2021.

[130] D. Patterson et al., “A Case for Intelligent RAM,” IEEE Micro, 1997.
[131] L. Yavits et al., “GIRAF: General Purpose In-Storage Resistive Associative Frame-

work,” IEEE TPDS, 2021.
[132] A. Boroumand et al.,

Enabling Effective Hybrid Transac-
tional/Analytical Databases with Specialized Hardware/Software Co-Design,”
arXiv:2103.00798, 2021.

“Polynesia:

[133] K. Hsieh et al., “Accelerating Pointer Chasing in 3D-Stacked Memory: Challenges,

Mechanisms, Evaluation,” in ICCD, 2016.

[134] H. S. Stone, “A Logic-in-Memory Computer,” IEEE TC, 1970.
[135] A. Denzler et al., “Casper: Accelerating stencil computation using near-cache pro-

cessing,” arXiv:2112.14216, 2021.

[136] S. Lloyd et al., “In-memory Data Rearrangement for Irregular, Data-intensive Com-

[137] Z. Sura et al., “Data Access Optimization in a Processing-in-Memory System,” in

[138] W.-M. Hwu et al., “Rebooting the Data Access Hierarchy of Computing Systems,”

[139] P. Chi et al., “PRIME: A Novel Processing-In-Memory Architecture for Neural Net-

work Computation In ReRAM-Based Main Memory,” in ISCA, 2016.

[140] G. F. Oliveira et al., “DAMOV: A New Methodology and Benchmark Suite for Eval-

uating Data Movement Bottlenecks,” arXiv:2105.03725, 2021.

[141] S. Diab et al., “High-throughput Pairwise Alignment with the Wavefront Algorithm

using Processing-in-Memory,” in HICOMB, 2022.

[142] J. Draper et al., “The Architecture of the DIVA Processing-in-Memory Chip,” in SC,

puting,” Computer, 2015.

CF, 2015.

in ICRC, 2017.

[145] M. Gokhale et al., “Near Memory Data Structure Rearrangement,” in MEMSYS, 2015.
[146] M. Oskin et al., “Active Pages: A Computation Model for Intelligent Memory,” in

ISCA, 1998.

[147] L. Zheng et al., “RRAM-based TCAMs for pattern search,” in ISCAS, 2016.
[148] A. Boroumand, “Practical Mechanisms for Reducing Processor-Memory Data
Movement in Modern Workloads,” Ph.D. dissertation, Carnegie Mellon University,
2020.

[149] O. O. Babarinsa et al., “JAFAR: Near-Data Processing for Databases,” in SIGMOD,

2015.

[150] S. Diab et al., “High-throughput Pairwise Alignment with the Wavefront Algorithm

using Processing-in-Memory,” arXiv:2204.02085, 2022.

[151] A. Olgun et al., “QUAC-TRNG: High-Throughput True Random Number Genera-
tion Using Quadruple Row Activation in Commodity DRAM Chips,” in ISCA, 2021.
[152] B. M. S. Bahar Talukder et al., “Exploiting DRAM Latency Variations for Generating

[153] Linux man-pages Project, “malloc(3) — Linux manual page,” https://man7.org/

linux/man-pages/man3/malloc.3.html, 2022.

[154] Linux man-pages Project, “posix_memalign(3) — Linux manual page,” https://man7.

org/linux/man-pages/man3/posix_memalign.3.html, 2022.

[155] Intel,

“Intel 64 and IA-32 Architectures Software Developer Manuals,”
2011. [Online]. Available: http://www.intel.com/content/www/us/en/processors/
architectures-software-developer-manuals.html
[156] V. Seshadri et al., “The Dirty-Block Index,” in ISCA, 2014.
[157] H. Hassan et al., “SoftMC: A Flexible and Practical Open-Source Infrastructure for

Enabling Experimental DRAM Studies,” in HPCA, 2017.

[158] J. Power et al., “gem5-gpu: A Heterogeneous CPU-GPU Simulator,” CAL, Jan 2015.
[159] N. Binkert et al., “The gem5 Simulator,” CAN, 2011.
[160] SAFARI Research Group, “Ramulator: A DRAM Simulator – GitHub Repository,”

https://github.com/CMU-SAFARI/ramulator/, 2015.

[161] Y. Kim et al., “Ramulator: A Fast and Extensible DRAM Simulator,” CAL, 2015.
[162] S. R. Group, “Ramulator-PIM: A Processing-in-Memory Simulation Framework –
GitHub Repository,” https://github.com/CMU-SAFARI/ramulator-pim, 2021.
[163] D. Sanchez et al., “ZSim: Fast and Accurate Microarchitectural Simulation of

Thousand-Core Systems,” in ISCA, 2013.

[164] D. Lee et al., “Adaptive-Latency DRAM: Optimizing DRAM Timing for the

[165] J. Kim et al., “Solar-DRAM: Reducing DRAM Access Latency by Exploiting the Vari-

[166] L. Cojocar et al., “Are We Susceptible to Rowhammer? An End-to-End Methodol-

[167] Y. Kim et al., “A Case for Exploiting Subarray-Level Parallelism (SALP) in DRAM,”

[169] K. K. Chang et al., “Understanding Reduced-Voltage Operation in Modern DRAM
Devices: Experimental Characterization, Analysis, and Mechanisms,” in SIGMET-
RICS, 2017.

[170] S. Ghose et al., “What Your DRAM Power Models Are Not Telling You: Lessons

[171] M. Patel et al., “The Reach Profiler (REAPER): Enabling the Mitigation of DRAM

Retention Failures via Profiling at Aggressive Conditions,” in ISCA, 2017.

[172] H. Luo et al., “CLR-DRAM: A Low-Cost DRAM Architecture Enabling Dynamic

Capacity-Latency Trade-Off,” in ISCA, 2020.

[173] S. Ghose et al., “Demystifying Complex Workload-DRAM Interactions: An Experi-

mental Study,” in SIGMETRICS, 2019.

[174] K. K. Chang, “Understanding and Improving the Latency of DRAM-Based Memory

Systems,” Ph.D. dissertation, Carnegie Mellon University, 2017.

[175] Y. Kim, “Architectural Techniques to Enhance DRAM Scaling,” Ph.D. dissertation,

Carnegie Mellon University, 2015.

[176] D. Lee, “Reducing DRAM Latency at Low Cost by Exploiting Heterogeneity,” Ph.D.

dissertation, Carnegie Mellon University, 2016.

[177] A. Olgun et al., “PiDRAM: A Holistic End-to-end FPGA-based Framework for

Processing-in-DRAM,” arXiv:2111.00082, 2021.

[178] Xilinx, “Xilinx Zynq-7000 SoC ZC706 Evaluation Kit,” 2021. [Online]. Available:

https://www.xilinx.com/products/boards-and-kits/ek-z7-zc706-g.html

[179] K. Asanović et al., “The Rocket Chip Generator,” Technical Report No. UCB/EECS-

2016-17, 2016.

[180] RISC-V, “RISC-V Proxy Kernel,” https://github.com/riscv-software-src/riscv-pk,

2022.

[181] Linux man-pages Project, “memcpy(3) — Linux manual page,” https://man7.org/

linux/man-pages/man3/memcpy.3.html, 2022.

[182] Linux man-pages Project, “calloc(3p) — Linux manual page,” https://man7.org/

linux/man-pages/man3/calloc.3p.html, 2022.

[183] A. Olgun, “Projects and Seminars on PIM, Invited Talk, End-to-end Framework for

Processing-using-Memory,” https://youtu.be/s_z_S6FYpC8, 2021.

6

[121] D. G. Elliott et al., “Computational RAM: Implementing Processors in Memory,”

Common-Case,” in HPCA, 2015.

[122] R. Nair et al., “Active Memory Cube: A Processing-in-Memory Architecture for

ation in Local Bitlines,” in ICCD, 2018.

[123] S. Lloyd et al., “Design Space Exploration of Near Memory Accelerators,” in MEM-

ogy for Cloud Providers,” in S&P, 2020.

Array,” IEEE Computer, 1995.

IEEE Design & Test of Computers, 1999.

Exascale Systems,” IBM JRD, 2015.

SYS, 2018.

[124] G. Singh et al., “NERO: A Near High-Bandwidth Memory Stencil Accelerator for

in ISCA, 2012.

Weather Prediction Modeling,” in FPL, 2020.

[168] D. Lee et al., “Decoupled Direct Memory Access: Isolating CPU and IO Traffic by

[125] D. Lavenier et al., “Variant Calling Parallelization on Processor-in-Memory Archi-

Leveraging a Dual-Data-Port DRAM,” in PACT, 2015.

[128] K. Mai et al., “Smart Memories: A Modular Reconfigurable Architecture,” in ISCA,

from a Detailed Experimental Study,” in SIGMETRICS, 2018.

