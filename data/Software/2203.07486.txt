Constrained Precision Tuning

Dorra Ben Khalifa1 and Matthieu Martel1,2

2
2
0
2

r
a

M
4
1

]
E
S
.
s
c
[

1
v
6
8
4
7
0
.
3
0
2
2
:
v
i
X
r
a

Abstract— Precision tuning or customized precision number
representations is emerging, in these recent years, as one of
the most promising techniques that has a positive impact on
the footprint of programs concerning energy consumption,
bandwidth usage and computation time of numerical programs.
In contrast to the uniform precision, mixed precision tuning
assigns different ﬁnite-precision types to each variable and
arithmetic operation of a program and offers many additional
optimization opportunities. However, this technique introduces
new challenge related to the cost of operations or type conver-
sions which can overload the program execution after tuning.
In this article, we extend our tool POP (Precision OPtimizer),
with efﬁcient ways to limit the number of drawbacks of mixed
precision and to achieve best compromise between performance
and memory consumption. On a popular set of tests from the
FPBench suite, we discuss the results obtained by POP.

Index Terms— Floating-point arithmetic, numerical accuracy,

static analysis, code optimization.

I. INTRODUCTION

In recent years, precision tuning to improve the perfor-
mance metrics is emerging as a new trend to save the
resources on the available processors, especially when new
error-tolerant applications are considered [1]. By way of
illustration, many applications can tolerate some loss in qual-
ity during computation, as in the case of media processing
(audio, video and image), data mining, machine learning,
etc. In addition, as almost all numerical computations are
performed using ﬂoating-point operations to represent real
numbers [2], the precision of the related data types should
be adapted in order to guarantee the desired overall rounding
error and to strengthen the performance of programs. For
instance, using FP32 single precision formats is often at least
twice as fast as the FP64 double precision ones on most
modern processors [3]. Consequently, the natural question
that arises is how to obtain the best precision/performance
trade-off by allocating some program variables in low preci-
sion (e.g. FP16 and FP32) and by using high precision (e.g.
FP64 and FP128) selectively. This process is also called,
mixed-precision tuning.

Let us precise that precision tuning is not a simple task
limited to changing the data type in the source code with
the Find-and-Replace button of any text editor. It is a more
complex technique which analyzes the semantics of the
programs and presents several challenges both architectural
and algorithmic. For this reason, various tools [4], [5], [6],

1

are with

The
university
66100,
matthieu.martel@univ-perp.fr

the
Perpignan,
dorra.ben-khalifa@univ-perp.fr

the LAMPS Laboratory

authors
of
France

Paul Alduy,

52 Avenue

Perpignan,

of

2 Matthieu Martel is also with the Numalis company, 265 Avenue des

´Etats du Languedoc, Montpellier, 34000, France.

[7], [8], [9] have been proposed to help developers select
the most appropriate data representations. Such tools may
integrate different approaches but their common goal is still
to automatically or semi-automatically adapt an original code
given in higher precision to the selected lower precision type.
However, the common point to all the existing techniques is
that they follow a trial-and-error strategy: they change the
data types of some variables of the program and evaluate
the accuracy of the result and depending on what is obtained
they change more or less data types and repeat the process.
At this level, we present an important difference relating the
terms precision and accuracy that are often confused, even
though they have signiﬁcantly different meanings. Here, we
call precision a property of a number format that refers to the
amount of information used to represent a number. Better or
higher precision means more numbers can be represented,
and also means a better resolution. Otherwise,
the term
accuracy denotes how close a ﬂoating-point computation
comes to the real value [10]: a bound on the absolute error
|x − (cid:98)x| between the represented (cid:98)x value and the exact value
x that we would have in the exact arithmetic.

The POP tool [11], [12] proposes a novel static technique
based on a semantic modelling of the propagation of the
numerical errors throughout the code formulated with two
methods. The ﬁrst method consists of generating an Integer
Linear Problem (ILP) from the program. Basically,
this
is done by reasoning on the most signiﬁcant bit and the
number of signiﬁcant bits of the values which are integer
quantities. The integer solution to this problem, computed in
polynomial-time by a classical linear programming solver,
gives the optimal data types at bit-level. The second method
proposes a ﬁner set of semantic equations which does not
reduce directly to an ILP problem. So, we use the policy
iteration (PI) technique to ﬁnd a solution. Let us note that
the originality of both methods is to ﬁnd directly the minimal
number of bits needed, known as bit-level precision tuning,
at each control point of the variables of the program.

In this article, we focus on improving the efﬁciency of
POP by experimenting several optimization criteria to our
system of constraints in order to achieve the best compromise
between performance and memory saving. The ﬁrst criteria
is related to minimize the number of formats in the tuned
programs. The second criteria correponds to minimize the
number of bits needed for each operation performed in
programs. The third optimization function aims to avoid type
conversions of the same variables in the same program.

The remainder of this article is as follows. Section II
describes the static approach inside the POP tool. Section III
point out the new optimization criteria related to the type

 
 
 
 
 
 
II. POP: PRECISION OPTIMIZER

ξ (z)(x, y) =

Fig. 1. Schematic representation of ufp, nsb and ulp for values and errors.

conversions of tuning. Experimental results are presented in
Section IV. Section V discusses the state-of-the-art tools for
precision tuning before concluding in Section VI.

In this section, we start by describing the necessary
background to understand the technique behind our tool POP
(Section II-A). Next, we highlight the main architecture of
POP in Section II-B. Section II-C presents a motivating
example of tuning with POP.

A. Background

POP manipulates numbers for which we know their unit
in the ﬁrst place denoted by ufp (Deﬁnition 1), their number
of signiﬁcant bits, denoted by nsb (Deﬁnition 2) and their
unit in the last place denoted by ulp (Deﬁnition 3). We
also assume that the constants occurring in the source codes
are exact and we bound the errors introduced by the ﬁnite
precision computations. These functions are deﬁned hereafter
and more intuitive presentation is given in Figure 1.
Deﬁnition 1 (Unit in the First Place): The unit

in the
ﬁrst place of a real number x, denoted by ufp(x), and possibly
encoded up to some rounding mode by a ﬂoating-point or a
ﬁxed-point number is given in Equation (1). This function is
independent of the representation of x:

ufp(x) =

(cid:26) min{i ∈ Z : 2i+1 > |x|} = (cid:98)log2(|x|)(cid:99)

0

if x (cid:54)= 0,
if x = 0.

(1)

Deﬁnition 2 (Number of Signiﬁcant Bits): Intuitively,

nsb(x) is the number of signiﬁcant bits of x. Let
ˆx the
approximation of x in ﬁnite precision and let ε(x) = |x − ˆx|
be the absolute error. Following Parker [13], if nsb(x) = k,
for x (cid:54)= 0, then

ε(x) ≤ 2ufp(x)−k+1 .
(2)
Deﬁnition 3 (Unit in the Last Place): The unit in the last
place of a number x denoted by ulp(x) is deﬁned below in
Equation (3). It depends on the unit in the ﬁrst place ufp(x)
and the number of signiﬁcant bits nsb(x):

For example,

ulp(x) = ufp(x) − nsb(x) + 1 .

(3)
if x = 2.75 then, following equations 1
to 3, we have ufp(x) = 1, nsb(x) = 4 and consequently
ulp(x) = −2. Note that we deﬁne in the same manner ufpe(x)
and nsbe(x) respectively the ufp and nsb of the error on a
number x (see Figure 1) which are used to describe the error
propagation through the computations and to compute either
a carry bit can be propagated through a computation or not.
In practice, POP implements an optimized carry bit function,
denoted by ξ , that adds an extra bit in some arithmetic

operation only if the errors of the operands can overlap.
Deﬁnition 4 highlights this function.

x

and y

Deﬁnition 4 (Carry Bit Function): Let

two
operands of some operation which result is z. The optimized
ξ function is given as shown in Equation (4): if the ulpe of
one of the two operands error ε(x) or ε(y) is greater than
the ufpe of the other one (or conversely) then the numbers
x and y are not aligned and consequently ξ = 0 (otherwise
ξ = 1). The optimized ξ function is given by






0 ulpe(x) ≥ ufpe(y),
0 ulpe(y) ≥ ufpe(x),
1 otherwise.

(4)

Let us state that the carry bit function has an effect on the
linearity of the constraints that we will highlight in the next
section.

B. POP Outline

From a grammar, POP parses the input source codes
using the ANTLR framework [14]. Next, it performs a range
determination consisting in launching the execution of the
program a certain number of times in order to determine
dynamically the range of variables. More precisely, this is
done by reasoning on the weight of the most signiﬁcant bit
already deﬁned in Equation (1).

The new features of POP, in contrast to its former in-
troduction in [15], [16], [17], [18],
is to solve an ILP
problem generated from the program source code. Next,
the optimal solution computed by a classical LP solver (we
use GLPK1 in practice) gives the optimized data types that
satisfy the user accuracy requirement in a polynomial-time
and with respect to some optimization function. We remind
the reader that the goal of this article is to introduce several
optimization functions and to evaluate the performance of
tuning in terms of mixed-precision and quality of analysis.
However, we must precise that we have over-approximated
the carry bit propagation throughout the computations in
the ILP formulation because of the min and max operators
that arise when solving Equation (4). For this purpose, POP
implements an optimization of the previous ILP method by
introducing a second set of semantic equations. These new
equations make it possible to tune even more the precision
by being less pessimistic on the propagation of carries in
arithmetic operations. So, we use the policy iteration (PI)
method [19] to ﬁnd efﬁciently a solution. We refer the reader
to [12] for a detailed explanation about our method.

Concerning the complexity of the analysis performed by
POP, in practice, the number of variables and constraints
is linear in the size of the program and consequently the
complexity to analyze a program of size n is equivalent to
that of solving a system of n constraints in our language
of constraints. In addition, POP is able to analyze large
programs (≈ KLOC) but the only limitation is related to
the size of the problem accepted by the solver.

C. Motivating Example

1https://www.gnu.org/software/glpk/

ufpufpensbeulpeulpnsbd t (cid:96)2 = 0 . 5 (cid:96)0 ;
kp(cid:96)8 = 9 . 4 5 1 4 (cid:96)6 ; k i (cid:96)11 = 0 . 6 9 0 0 6 (cid:96)9 ;
kd(cid:96)14 = 2 . 8 4 5 4 (cid:96)12 ; c (cid:96)17 = 5 . 0 (cid:96)15 ;

i n v d t (cid:96)5 = 0 . 5 (cid:96)3 ;

p(cid:96)26 = 0 . 0 (cid:96)24 ;
d(cid:96)32 = 0 . 0 (cid:96)30 ;

1
2
3
4 m(cid:96)20 = 8 . 0 (cid:96)18 ; e (cid:96)23 = 0 . 0 (cid:96)21 ;
i (cid:96)29 = 0 . 0 (cid:96)27 ;
5
r (cid:96)35 = 0 . 0 (cid:96)33 ;
6
e o l d (cid:96)42 = 0 . 0 (cid:96)40 ;
7 m1(cid:96)39 = m(cid:96)37 ;
8
9 w h i l e ( t <100.0) {
10
11
12
13

e (cid:96)56 = c (cid:96)51 −(cid:96)54 m1(cid:96)53 ;
p(cid:96)63 = kp(cid:96)58 *
i (cid:96)76 = i (cid:96)65 +(cid:96)74 k i (cid:96)67 *
d(cid:96)89 = kd(cid:96)78 *
;

t (cid:96)45 = 0 . 0 (cid:96)43 ;

i n v d t (cid:96)80 *

(cid:96)61 e (cid:96)60 ;

(cid:96)81

(cid:96)70 d t (cid:96)69 *

(cid:96)73 e (cid:96)72 ;
(cid:96)87 e (cid:96)83 −(cid:96)86 e o l d (cid:96)85

14
15
16
17
18
19

i (cid:96)93 +(cid:96)97 d(cid:96)96 ;
r (cid:96)99 = p(cid:96)91 +(cid:96)94
m1(cid:96)108 = m1(cid:96)101 +(cid:96)106 0 . 0 1 (cid:96)102 *
e o l d (cid:96)112 = e (cid:96)110 ;
t (cid:96)119 = t (cid:96)114 +(cid:96)117 d t (cid:96)116 ;

;

}(cid:96)120
r e q u i r e n s b ( m1 , 1 2 ) (cid:96)122 ;

(cid:96)105

r (cid:96)104 ;

1
2
3
4
5
6
7
8
9
10
11
12

13

14
15
16
17
18
19

k i | 1 2 | = 0 . 6 9 0 0 6 | 1 2 | ;

i n v d t | 1 2 | = 0 . 5 | 1 2 | ;

d t | 1 2 | } = 0 . 5 | 1 2 | ;
kp | 1 2 | = 9 . 4 5 1 4 | 1 2 | ;
kd | 1 2 | = 2 . 8 4 5 4 | 1 2 | ; c | 1 4 | = 5 . 0 | 1 4 | ;
m| 1 5 | = 8 . 0 | 1 5 | ; e | 0 | = 0 . 0 | 0 | ;
i | 1 3 | = 0 . 0 | 1 3 | ;
p | 0 | = 0 . 0 | 0 | ;
r | 0 | = 0 . 0 | 0 | ;
d | 0 | = 0 . 0 | 0 | ;
m1 | 1 5 | = m| 1 5 | ;
t | 1 3 | = 0 . 0 | 1 3 | ;
w h i l e ( t <100.0) {

e o l d | 1 2 | = 0 . 0 | 1 2 | ;

e | 1 2 | = c | 1 4 | − | 1 2 | m1 | 1 5 | ;
p | 1 3 | = kp | 1 2 | * | 1 3 | e | 1 2 | ;
i | 1 2 | = i | 1 3 | + | 1 2 | k i | 9 | * | 1 0 | d t | 9 | * | 1 1 |

e | 1 0 | ;

i n v d t | 1 0 | * | 1 2 | e | 1 2 |

d | 1 2 | = kd | 1 0 | * | 1 1 |
− | 1 1 | e o l d | 1 2 | ;
r | 1 2 | = p | 1 3 | + | 1 2 |
m1 | 1 2 | = m1 | 1 4 | + | 1 2 | 0 . 0 1 | 8 | * | 9 |
e o l d | 1 2 | = e | 1 2 | ;
t | 1 2 | = t | 1 3 | + | 1 2 | d t | 6 | ;

i | 1 1 | + | 1 2 | d | 1 0 | ;

r | 8 | ;

} ;
r e q u i r e n s b ( m1 , 1 2 ) ;

Fig. 2.

Top left: PID source program annotated with labels. Top right: PID program tuned with POP.

To better explain the ILP nature of the precision tuning
problem in POP, we consider the PID controller program [20]
of Figure 2. This algorithm is widely used in embedded
and critical systems e.g. aeronautic and avionic systems. The
main feature of this program is to keep a physical parameter
m at a speciﬁc value known as the setpoint . In other words,
it tries to correct a measure by maintaining it at a deﬁned
value. The original PID program is depicted in the left hand
side of Figure 2 whereas the optmized program by POP is
given in the right hand side corner.

Some points can be highlighted about this example. For
the variables dt and invdt are both initialized
instance,
respectively to the value 0.5, annotated with their control
points thanks to the following annotations dt(cid:96)2 = 0.5(cid:96)0 and
invdt(cid:96)5 = 0.5(cid:96)3 in the left hand side of Figure 2. Also, the
statement require nsb(m1, 12)(cid:96)122 at Line 19 informs POP
that the user wants to get 12 signiﬁcant bits on variable m1
and so, nsb(m1) = 12. Next, POP reduces the problem to an
ILP formulation by generating a set of constraints on all the
labels of programs. Finally, the optimal solution computed
by a the GLPK solver [21] gives the optimized data types
that satisfy the user accuracy requirement in a polynomial-
time as shown in the right hand side of Figure 2. By way
of illustration, the results obtained on Line 10 of the PID
program says that for nsb(m1) = 12 bits, the number of
signiﬁcant bits needed for variable e is 12 bits, the variable c
is computed with 14 bits whereas the operator − is computed
with 12 bits, etc.

III. GUIDING TUNING WITH CONSTRAINTS

In this section, we introduce the different cost functions
used by POP to obtain the best compromise between per-
formance and memory saving. Recall that POP generates a
set of constraints whose solution gives the tuning (See Sec-
tion II-B.) Our new optimization criteria are then expressed
as cost functions that the solver has to optimize. Below we

propose three optimization criteria related to the largest data
type, the number of bits needed for each operation and the
prohibition of type conversions.

Let us remark that one may combine the cost functions
introduced in this article and try and optimze the accuracies
at any control point globally. While encompassing the cost
functions presented in previous work [12] in terms of tuning,
this method suffers from several drawbacks. First, it increases
the number of constraints and variables of the system. Sec-
ond, it over-constraints the system which makes the solver
fail more often and ﬁnally it slows down the tuning time.

Let us also remark that, compared to other approaches
such as the ones based on delta-debugging (see Section
VI), POP approach which is based on solving a system of
constraints allows one to deﬁne easily many optimization
criteria such as the ones introduced below without modifying
signiﬁcantly the tool and without incereasing the combina-
tory of the problem.

A. Minimize the Number of the Largest Data Type

The purpose of this cost function is to answer the fol-
lowing question: What is the minimal number of bits of the
greatest format needed in the program in order to ensure
some accuracy on the result? For example, by this technique
one may answer to questions such as is it possible to obtain a
result with 18 signiﬁcant bits using only, e.g. single precision
numbers (FP32)?

More formally, let Lab denotes the set of labels of the
program and let T : Lab → N be a tuning assigning to each
control point (cid:96) ∈ Lab an integer precision. A tuning is correct
if it satisﬁes the system of constraints generated by POP [12]
including the accuracy requirement on the result and we
denote T the set of correct tunings. The cost function for
maximal precision MP that we aim to compute is given as

shown in Equation (5).

(cid:26)

(cid:27)

MP = min
T ∈T

max
(cid:96)∈Lab

T ((cid:96))

.

(5)

Minimizing the largest format may enable one to use a
processor with limited formats (e.g. only single precision).
In former work, our cost function was to minimize the sum
of the precision of the assigned variables in the program, i.e.
∑(cid:96)∈Lab T ((cid:96)). However, this may lead to cases where some
variables have large formats and others small ones (e.g. from
FP16 half precision to FP64 double) which makes difﬁcult
hardware optimizations [15].

B. Minimize of the Operations Number of Bits

Our second cost function MOp focuses on the operators
instead of the variables of the program. We aim at minimiz-
ing only the number of bits used in the arithmetic operations,
without considering what is used for variables. The interest
is to minimize the hardware needed to run the programs.
Also, this optimization is particularly relevant for circuit
implementations, e.g. using FPGAs [22].

Formally, let Op ⊆ Lab be the subset of labels attached to
operators (additions, multiplications, elementary functions,
etc.) Here, we aim at computing

MOp = min
T ∈T

(cid:40)

(cid:41)

T ((cid:96))

.

∑
(cid:96)∈Op

(6)

In the present work, we assign the same weight to each
operation (i.e. its number of bits). However, it would be in-
teresting to assign different weights, for instance to take into
account that a multiplication is more costly than an addition
at the hardware level (same for elementary functions.)

C. Avoid Type Conversions

Mixed precision tuning, as done by POP, offers the advan-
tage of optimizing the precision of a variable at each of its
occurences. However, from a performance point of view, this
introduces type conversions which may slow down the pro-
grams. Let V : Var → ℘(Lab) be a function mapping each
variable x of a program to the set of labels corresponding to
the occurrences of x and let Dom(V ) denote the deﬁnition
domain of V . We add a mode in POP which enforces it to
produce an uniform tuning by adding the constraints

∀x ∈ Dom(V ), ∀(cid:96)1, (cid:96)2 ∈ V (x), T ((cid:96)1) = T ((cid:96)2) .

(7)

Let us remark that, in this mode, POP still achieve bit-level
precision tuning. However this tuning is uniform and only
one precision is returned for each variable which avoids type
conversions. As previously mentionned, let us also underline
the fact that since POP is based on a system of constraints,
assigning to it new optimization objectives can be done
easily, without a deep refactoring of the tool.

Prog.

Require 8 bits

Require 12 bits

Acc.
Odo.
Pend.
PID
RK
Trap.

Acc.
Odo.
Pend.
PID
RK
Trap.

MP
9
13
17
12
10
14

MP
17
21
25
20
18
22

FP16
18
22
12
17
22
5

FP32
0
7
1
2
0
10

FP64
0
0
0
0
0
0

MP
13
17
21
16
14
18

FP16
3
0
0
0
0
0

FP32
15
29
13
19
22
15

Require 16 bits

Require 32 bits

FP16
0
0
0
0
0
0

FP32
18
29
12
19
22
15

FP 64 MP
25
37
33
28
26
30

0
0
1
0
0
0

FP16
0
0
0
0
0
0

FP32
15
0
10
10
18
3

FP64
0
0
0
0
0
0

FP 64
3
29
3
9
4
12

TABLE I
MINIMIZATION OF THE WORST PRECISION IN FUNCTION OF THE

ACCURACY REQUIREMENT. MAXIMAL PRECISION (MP) COMPUTED BY
POP FOLLOWED BY THE NUMBER OF FP16, FP32 AND FP64
VARIABLES.

IV. EXPERIMENTAL EVALUATION

We evaluate the performance of POP on a standard bench-
mark set for ﬂoating-point analysis (Section IV-A), and com-
pare the results of precision tuning using the different cost
functions already deﬁned in Section III. Note that POP and
all the data and results presented in this article (Section IV-
B), are publicly available under GPL-3.0 License2. Also, the
experiments are ran on an Intel Core i5-8350U 1.7GHz Linux
machine with 8 GB RAM.

2(cid:135)https://github.com/benkhelifadorra/POP-v2.0

Fig. 3.
Percentage of reduction after tuning by POP of the total number
of bits for the operators occurring in our test programs in function of the
accuracy requirement on the result.

	0	20	40	60	80	100AccelerometerOdometryPendulumPIDRunge	KuttaTrapeze8163248Prog.

Acc.
Odo.
Pend.
PID
RK
Trap.

8 bits

16 bits

24 bits

32 bits

85 %
86 %
85 %
86 %
85 %
80 %

85 %
81 %
84 %
85 %
85 %
78 %

71 %
76 %
72 %
73 %
71 %
67 %

71 %
73 %
71 %
73 %
71 %
65 %

57 %
65 %
59 %
61 %
58 %
53 %

57 %
62 %
57 %
60 %
57 %
52 %

42 %
54 %
46 %
48 %
44 %
40 %

42%
51%
44%
48%
44%
38%

TABLE II
MEMORY SAVINGS (IN PERCENTAGE) ON THE NUMBER OF BITS NEEDED
TO STORE VARIABLES, IN MIXED OR UNIFORM PRECISION, IN FUNCTION
OF THE ACCURACY REQUIREMENT, FOR OUR TEST PROGRAMS.

A. Benchmarks

Our cost functions are experimented on several FPBench3
benchmarks. FPBench develops standards for describing
ﬂoating-point benchmarks and for measuring their accu-
racy [23]. We have selected programs from the embedded
systems, IoT and numerical analysis ﬁelds.

• Accelerometer [16]: this program comes from the IoT
ﬁeld and measures the angle of inclination of an object.
• Odometry [24]: an example taken from robotics which
concerns the computation of the position(x,y) of a two
wheeled robot by odometry.

• Pendulum [12]: models the movement of a simple

pendulum without damping.

• PID Controller [20]: is a widely used algorithm in em-
bedded and critical systems e.g. aeronautic and avionic
systems. This program was highlithed in Section II-C.
• Runge Kutta method [25]: is an effective and widely
used method for solving the initial-value problems of
differential equations.

• The trapezoidal rule [25]: a well known algorithm
in numerical analysis which approximates the deﬁnite

integral

f (x) dx.

(cid:90) b

a

B. Evaluation

In this section, we present the results obtained with POP
on the sample programs described in Section IV-A for the
cost functions introduced in Section III.

Table I presents the results of the cost function concerning
the minimization of the largest data type already presented in
Section III-A. In this experiment, we consider four accuracy
requirements for the results of our test programs: 8, 12, 16
and 32 signiﬁcant bits. For each program and requirement,
we display the maximal precision MP as deﬁned in Equa-
tion (5), as well as the number of variable in half, single and
double precision (FP16, FP32 and FP64 respectively). For
example, for the accelerometer program with an accuracy
requirement of 12 bits, we ﬁnd that MP = 13, meaning
in half precision and that
that 3 variables may be set
the remaining 15 variables may be set in single precision.
A ﬁrst observation is that the MPs are not intuitive and
should be difﬁcult to obtain without POP, either by hand

3https://fpbench.org/

or by tuning tools based on delta-debugging (because of the
theoretical complexity which makes the bit-level precision
tuning untracktable for this class of tools, see related work
in Section V). A second observation is that POP often ﬁnds
MPs close to the accuracy requirement of the result, which
means that the optimization is important. Not surprisingly,
the MP increases as the required number of signiﬁcant bits
on the result increases.

Our second experiment concerns the cost function of
Section III-B related to the size of the arithmetic operators.
Our results are summarized in Figure 3. The histogram
gives percentages of optimization for the total number of
bits needed for the operations. The percentage is computed
with respect to an initial number of bits corresponding to all
the operations done in FP64 double precision (which corre-
sponds to 100%.) The ﬁrst four bars are for the accelerometer
program, the next four bars are for the odometry program,
etc. The four bars dedicated to a program correspond to
the gains obtained for the accuracy requirements set on
the results: 8, 16, 32 and 48 bits. For example, for the
accelerometer program,
the total number of bits for the
operations is reduced by 85% and 70% for requirements of
8 and 16 bits respectively.

A ﬁrst observation is that POP is able to reduce very
signiﬁcantly the number of bits needed when small nsb are
required on the outputs (around 80% and 70% of reduction
for nsb = 8 or nsb = 16 bits). Also, we observe that when
large nsb quantities are required (e.g. 48 bits is close to the
53 bits of the double precision), POP is able to reduce the
size of the operators by around 10%.

Finally, our last experiment, summed up in Table II
is for uniform precision as deﬁned in Section III-C. We
consider four accuracy requirements for the results of our
test programs: 8, 16, 24 and 32 bits. For each of these
requirements, we run POP in mixed-precision mode and
then in uniform precision mode. We display the percentages
of optimization on the total number of bits used by the
variables. Again, 100% corresponds to all the variables in
double precision. In Table II, two consecutive values are
for the same requirement, in mixed and uniform precision
respectively. For example, for the odometry program and
for a requirement of 8 bits, the savings in number of bits
obtained by POP are of 86% in mixed precision and of 81%
in uniform precision. Similarly, for the same program and
for a requirement of 32 bits, the saving are of 54% and 51%
respectively. From Table II, we may observe that POP still
optimizes well the programs in uniform precision mode. This
is mainly due to the fact that our sample programs do not use
the same variables many times (see, for example the code of
the PID program in Figure 2.) A second remark is that bit-
level precision tuning makes it possible to obtain important
memory savings, even in uniform mode.

V. RELATED WORK

The last few years have seen a wealth of precision tuning
tools. In this section, we discuss the strengths and shortcom-
ings of each tool. Besides, we classify these approaches into

two categories: static methods that extract additional knowl-
edge from the program source code without executing it with
input data and dynamic methods that involve the proﬁling
of the target application to extract pieces of information by
running the original version of the program.

A. Static Analysis Tools

Rosa [26] is a source-to-source compiler which takes as
input a real-valued program with error speciﬁcations and syn-
thesizes code over an appropriate ﬂoating-point (FP32, FP64,
FP128, and an extended format with 256 bit width) or ﬁxed-
point data type (8, 16, 32 bit) which fulﬁlls the speciﬁcation.
Unlike POP which is able to ensure mixed precision tuning
on programs containing expressions, loops, conditionals and
even arrays, Rosa handles conditional statements soundly
and assigns only uniform preicision to the variables of their
programs. In addition, FPTuner [4] exposes a user-deﬁned
threshold for the amount of type casts that the tool may
insert into the code. Let us state that the approach deployed
by FPTuner is close the static technique of POP, especially
in the constraint generation step. However, it relies on a
local optimization procedure by solving quadratic problems
for a given set of candidate data types. Contrarily to POP ,
FPTuner is limited to straight-line programs. Moreover, the
TAFFO tool [27] is a LLVM-based tool-chain. Its strategy
is to collect statically annotations from the source code and
it converts them into LLVM-IR metadata with the goal to
replace ﬂoating-point operations with ﬁxed-point operations
to the extent possible. This analysis is used to project on the
output the error introduced by each ﬁxed-point instruction.
In contrast to TAFFO, POP is able to return solutions at
bit-level suitable for the IEEE754 ﬂoating-point arithmetic,
the ﬁxed-point arithmetic and the MPFR library for non-
standard precision. Nevertheless, the static tool Daisy [5] is
able to provide a mixed precision solution that considers
both ﬂoating-point and ﬁxed-point data making it generally
applicable to both scientiﬁc com- puting and embedded
applications. However, Daisy does not address conditional-
based programs.

B. Dynamic Analysis Tools

Precimonious [9] is a dynamic automated search-based
tool that leverages the LLVM framework to tweak variable
declarations to build and prototype mixed-precision conﬁgu-
rations within a given error threshold. It is based on the delta-
debugging algorithm search [28] which guarantees to ﬁnd a
local 1-minimum if one exists. A conﬁguration is said to be
1-minimal if lowering any additional variable (or function
call) leads to a conﬁguration that produces an inaccurate
result, or is not faster than the original program. Unlike POP
which optimizes all the variables of the program, Precimo-
nious optimizes only the precision of declared variables. It
uses external description ﬁles (JSON or XML) to declare
which variables in the source code should be explored
and which data types have to be investigated. Moreover,
it estimates round-off errors by dynamically evaluating the
the
program on several random inputs. For this reason,

Blame Analysis technique [29] aims at reducing the space of
variables of Precimonious. It performs shadow execution to
identify variables that are numerically insensitive and which
can consequently be excluded from the search space before
tuning. The analysis ﬁnds a set of variables that can be
in single precision, while the rest of the variables are in
double precision. However, the output conﬁgurations may
or may not improve performance, so to use the analysis in
practice one must perform runs of the program to determine
which conﬁgurations actually improve performance. Another
dynamic tool sharing some methodologies of Precimonious
is called PROMISE [30]. It modiﬁes automatically the preci-
sion of variables taking into account an accuracy requirement
on the computed result. Based on the delta-debugging search
algorithm which reduces the search space of the possible
variables to be converted, it provides a subset of the program
variables which can be converted from FP64 to FP32 only.
Meanwhile, PROMISE is able to tune programs only in FP32
single precision and it remains a time-intensive tool. HiFP-
Tuner [31] is another extension of Precimonious which uses
a hierarchical search approach. It combines a static analysis
to create the hierarchical structure in order to minimize the
number of type cast operations whereas the dynamic proﬁling
highlights the hottest dependencies. A major limitation is
that HiFPtuner’s conﬁgurations are dependent on the tuning
inputs, and no accuracy guarantee is provided for untested
inputs.

In addition, the CRAFT tool [32] is a framework that
performs an automated search of a program’s instruction
space, determining the level of precision necessary in the
result of each instruction to pass a user-provided veriﬁcation
routine assuming all other operations are done in high
precision such as FP64 double precision. However, it can be
very time consuming even for very small programs. Let us
note that there are other tools oriented to GPU applications
[33], [34], [35] which combine static analysis for casting-
aware performance modeling with dynamic analysis for
enforcing precision constraints. For an in-depth description
and a theoretical description between these tools, we refer
the reader to our survey in [36].

VI. CONCLUSION AND FUTURE WORK

In this article, we have extended our tool POP relying on a
modeling of the propagation of the errors throughout the code
with new optimization criteria in order to obtain a trade-off
between, precision, analysis time and memory consumption.
To our knowledge, this is the ﬁrst work interested in optimiz-
ing the results obtained after the precision tuning phase. The
results discussed show that our tool succeeded in limiting
the number of formats, the number of bits of operations and
the number of type conversions between the variables, for the
majority of our benchmarks and with respect to the accuracy
requirements given by the user. We shed the light that these
results are helpful in the hardware level, especially for some
processors that are limited to speciﬁc formats.

In furture work, we aim to adapt our precision tuning tool
to generate code in the ﬁxed-point arithmetic. In practice,

the information provided by POP may be used to generate
computations in the ﬁxed-point arithmetic with an accuracy
guaranty on the results. Also, we are interested in combining
POP with other tools performing error analysis [37], [38]
and code transformation [20], [39] tasks in order to better
improve the accuracy of the programs. Our technique is
also generalizeable to Deep Neural Networks for which
it is important to save memory usage and computational
resources.

REFERENCES

[1] S. Cherubin and G. Agosta, “Tools for reduced precision computation:

A survey,” ACM Comput. Surv., vol. 53, no. 2, 2020.

[2] IEEE Standard for Binary Floating-point Arithmetic, Std 754-

2008 ed., ANSI/IEEE, 2008.

[3] M. Baboulin, A. Buttari, J. J. Dongarra, J. Kurzak, J. Langou, J. Lan-
gou, P. Luszczek, and S. Tomov, “Accelerating scientiﬁc computations
with mixed precision algorithms,” Comput. Phys. Commun., vol. 180,
no. 12, pp. 2526–2533, 2009.

[4] W. Chiang, M. Baranowski, I. Briggs, A. Solovyev, G. Gopalakrishnan,
and Z. Rakamaric, “Rigorous ﬂoating-point mixed-precision tuning,”
in Proceedings of the 44th ACM SIGPLAN Symposium on Principles of
Programming Languages, POPL 2017, G. Castagna and A. D. Gordon,
Eds. ACM, 2017, pp. 300–315.

[5] E. Darulova, E. Horn, and S. Sharma, “Sound mixed-precision op-
timization with rewriting,” in Proceedings of
the 9th ACM/IEEE
International Conference on Cyber-Physical Systems, ICCPS 2018,
IEEE Computer
C. Gill, B. Sinopoli, X. Liu, and P. Tabuada, Eds.
Society / ACM, 2018, pp. 208–219.

[6] H. Guo and C. Rubio-Gonz´alez, “Exploiting community structure
for ﬂoating-point precision tuning,” in Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
ISSTA 2018. ACM, 2018, pp. 333–343.

[7] P. V. Kotipalli, R. Singh, P. Wood, I. Laguna, and S. Bagchi, “AMPT-
GA: automatic mixed precision ﬂoating point tuning for GPU ap-
plications,” in Proceedings of the ACM International Conference on
Supercomputing, ICS. ACM, 2019, pp. 160–170.

[8] M. O. Lam, J. K. Hollingsworth, B. R. de Supinski, and M. P. LeGen-
dre, “Automatically adapting programs for mixed-precision ﬂoating-
point computation,” in International Conference on Supercomputing,
ICS’13. ACM, 2013, pp. 369–378.

[9] C. Rubio-Gonz´alez, C. Nguyen, H. D. Nguyen, J. Demmel, W. Kahan,
K. Sen, D. H. Bailey, C. Iancu, and D. Hough, “Precimonious: tuning
assistant for ﬂoating-point precision,” in International Conference for
High Performance Computing, Networking, Storage and Analysis,
SC’13. ACM, 2013, pp. 27:1–27:12.

[10] M. Martel, “Floating-point format inference in mixed-precision,” in
NASA Formal Methods - 9th International Symposium, NFM, ser.
Lecture Notes in Computer Science, vol. 10227, 2017, pp. 230–246.
[11] D. Ben Khalifa and M. Martel, “A study of the ﬂoating-point tuning
behaviour on the n-body problem,” in Computational Science and Its
Applications - ICCSA 2021 - 21st International Conference, Cagliari,
Italy, September 13-16, 2021, Proceedings, Part V, ser. Lecture Notes
in Computer Science, O. Gervasi, B. Murgante, S. Misra, C. Garau,
I. Blecic, D. Taniar, B. O. Apduhan, A. M. A. C. Rocha, E. Tarantino,
and C. M. Torre, Eds., vol. 12953. Springer, 2021, pp. 176–190.

[12] A. Adj´e, D. Ben Khalifa, and M. Martel, “Fast and efﬁcient bit-level
precision tuning,” in Static Analysis - 28th International Symposium,
SAS 2021, ser. Lecture Notes in Computer Science. Springer, 2021.
[13] D. S. Parker, “Monte carlo arithmetic: exploiting randomness in
ﬂoating-point arithmetic,” University of California (Los Angeles),
Tech. Rep. CSD-970002, 1997.

[14] T. Parr, The Deﬁnitive ANTLR 4 Reference, 2nd ed.

Pragmatic

Bookshelf, 2013.

[15] D. Ben Khalifa, M. Martel, and A. Adj´e, “POP: A tuning assistant for
mixed-precision ﬂoating-point computations,” in Formal Techniques
for Safety-Critical Systems - 7th International Workshop, FTSCS 2019,
ser. Communications in Computer and Information Science, vol. 1165.
Springer, 2019, pp. 77–94.

[16] D. Ben Khalifa and M. Martel, “Precision tuning and internet of
things,” in International Conference on Internet of Things, Embedded
Systems and Communications, IINTEC 2019.
IEEE, 2019, pp. 80–85.

[17] D. Ben Khalifa and M. Martel, “Precision tuning of an accelerometer-
based pedometer algorithm for iot devices,” in International Confer-
ence on Internet of Things and Intelligence System, IOTAIS 2020.
IEEE, 2020, pp. 113–119.

[18] D. Ben Khalifa and M. Martel, “An evaluation of POP performance
for tuning numerical programs in ﬂoating-point arithmetic,” in 4th
International Conference on Information and Computer Technologies,
ICICT 2021, Kahului, HI, USA, March 11-14, 2021.
IEEE, 2021,
pp. 69–78.

[19] A. Costan, S. Gaubert, E. Goubault, M. Martel, and S. Putot, “A
policy iteration algorithm for computing ﬁxed points in static analysis
of programs,” in Computer Aided Veriﬁcation, 17th International
Conference, CAV 2005, ser. Lecture Notes in Computer Science, vol.
3576. Springer, 2005, pp. 462–475.

[20] N. Damouche, M. Martel, and A. Chapoutot, “Transformation of a PID
controller for numerical accuracy,” Electron. Notes Theor. Comput.
Sci., vol. 317, pp. 47–54, 2015.

[21] A. O. Makhorin, “Glpk (gnu linear programming kit),”

Available at http://www.gnu.org/software/glpk/glpk.html.

[22] X. Gao and G. A. Constantinides, “Numerical program optimization
for high-level synthesis,” in Proceedings of the 2015 ACM/SIGDA
International Symposium on Field-Programmable Gate Arrays, G. A.
Constantinides and D. Chen, Eds. ACM, 2015, pp. 210–213.
[23] N. Damouche, M. Martel, P. Panchekha, J. Qiu, A. Sanchez-Stern,
and Z. Tatlock, “Toward a standard benchmark format and suite for
ﬂoating-point analysis,” 2016.

[24] N. Damouche, M. Martel, and A. Chapoutot, “Improving the numerical
accuracy of programs by automatic transformation,” Int. J. Softw. Tools
Technol. Transf., vol. 19, no. 4, pp. 427–448, 2017.

[25] K. E. Atkinson, “An introduction to numerical analysis. 1989,” New

York, vol. 528, p. 38, 1991.

[26] E. Darulova and V. Kuncak, “Towards a compiler for reals,” ACM
Trans. Program. Lang. Syst., vol. 39, no. 2, pp. 8:1–8:28, 2017.
[27] S. Cherubin, D. Cattaneo, M. Chiari, A. D. Bello, and G. Agosta,
“TAFFO: tuning assistant for ﬂoating to ﬁxed point optimization,”
IEEE Embed. Syst. Lett., vol. 12, no. 1, pp. 5–8, 2020.

[28] A. Zeller and R. Hildebrandt, “Simplifying and isolating failure-
inducing input,” IEEE Trans. Softw. Eng., vol. 28, no. 2, p. 183–200,
2002.

[29] C. Rubio-Gonz´alez, C. Nguyen, B. Mehne, K. Sen, J. Demmel,
W. Kahan, C. Iancu, W. Lavrijsen, D. H. Bailey, and D. Hough,
“Floating-point precision tuning using blame analysis,” in Proceedings
of the 38th International Conference on Software Engineering, ICSE
2016, L. K. Dillon, W. Visser, and L. A. Williams, Eds. ACM, 2016,
pp. 1074–1085.

[30] S. Graillat, F. J´ez´equel, R. Picot, F. F´evotte, and B. Lathuili`ere, “Auto-
tuning for ﬂoating-point precision with discrete stochastic arithmetic,”
J. Comput. Sci., vol. 36, 2019.

[31] H. Guo and C. Rubio-Gonz´alez, “Exploiting community structure
for ﬂoating-point precision tuning,” in Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
ISSTA 2018, F. Tip and E. Bodden, Eds. ACM, 2018, pp. 333–343.
[32] M. O. Lam, J. K. Hollingsworth, B. R. de Supinski, and M. P. LeGen-
dre, “Automatically adapting programs for mixed-precision ﬂoating-
point computation,” in International Conference on Supercomputing,
ICS’13, A. D. Malony, M. Nemirovsky, and S. P. Midkiff, Eds. ACM,
2013, pp. 369–378.

[33] A. Angerd, E. Sintorn, and P. Stenstr¨om, “A framework for automated
and controlled ﬂoating-point accuracy reduction in graphics applica-
tions on gpus,” ACM Trans. Archit. Code Optim., vol. 14, no. 4, pp.
46:1–46:25, 2017.

[34] I. Laguna, P. C. Wood, R. Singh, and S. Bagchi, “Gpumixer:
Performance-driven ﬂoating-point tuning for GPU scientiﬁc applica-
tions,” in High Performance Computing - 34th International Confer-
ence, ISC High Performance 2019, ser. Lecture Notes in Computer
Science, M. Weiland, G. Juckeland, C. Trinitis, and P. Sadayappan,
Eds., vol. 11501. Springer, 2019, pp. 227–246.

[35] P. V. Kotipalli, R. Singh, P. Wood, I. Laguna, and S. Bagchi, “AMPT-
GA: automatic mixed precision ﬂoating point tuning for GPU ap-
plications,” in Proceedings of the ACM International Conference on
Supercomputing, ICS 2019, R. Eigenmann, C. Ding, and S. A. McKee,
Eds. ACM, 2019, pp. 160–170.

[36] D. Ben Khalifa, “Fast and efﬁcient bit-level precision tuning,”
Theses, Universit´e de Perpignan, Nov. 2021. [Online]. Available:
https://tel.archives-ouvertes.fr/tel-03509266

[37] A. Das, S. Krishnamoorthy,

I. Briggs, G. Gopalakrishnan, and
R. Tipireddy, “Fpdetect: Efﬁcient reasoning about stencil programs
using selective direct evaluation,” ACM Trans. Archit. Code Optim.,
vol. 17, no. 3, pp. 19:1–19:27, 2020.

[38] A. Das,

I. Briggs, G. Gopalakrishnan, S. Krishnamoorthy, and
P. Panchekha, “Scalable yet rigorous ﬂoating-point error analysis,” in
Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, SC 2020, C. Cuicchi,
I. Qualters, and W. T. Kramer, Eds.

IEEE/ACM, 2020, p. 51.

[39] B. Saiki, O. Flatt, C. Nandi, P. Panchekha, and Z. Tatlock, “Combining
precision tuning and rewriting,” in 28th IEEE Symposium on Computer
Arithmetic, ARITH 2021.

IEEE, 2021, pp. 1–8.

