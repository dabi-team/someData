2
2
0
2

y
a
M
3
1

]
L
C
.
s
c
[

1
v
9
0
4
6
0
.
5
0
2
2
:
v
i
X
r
a

ENPH 455 Final Report

Design and Implementation of a Quantum

Kernel for Natural Language Processing

by

Matt Wright

Supervisor: Dr. Stephen Hughes

A thesis submitted to the

Department of Physics, Engineering Physics, and Astronomy

in conformity with the requirements for

the degree of Bachelor of Applied Science

Queen’s University

Kingston, Ontario, Canada

April 2022

Copyright © Matt Wright, 2022

 
 
 
 
 
 
Abstract

Natural language processing (NLP) is the ﬁeld of computer science that attempts to make
human language accessible to computers, and it relies on developing a mathematical model
to express the meaning of symbolic language. One such model, referred to as DisCoCat,
deﬁnes how to express both the meaning of individual words as well as the compositional
nature of the sentence speciﬁed by grammar. This category-theoretic model is costly to
implement on a classical computer but has an equivalent quantum mechanical formulation,
which has lead to its implementation on quantum computers and the creation of the ﬁeld
quantum NLP (QNLP). This recent experimental work used quantum machine learning
techniques to learn a mapping from text to class label using the expectation value of
the quantum encoded sentence. Some theoretical work has been done on computing the
similarity of sentences using this quantum model but they rely on an unrealized quantum
memory store. The main goal of this thesis is to leverage the DisCoCat model to design a
quantum-based kernel function that can be used by a support vector machine (SVM) for
NLP tasks. The kernel function must compute the similarity of two given sentences with
high accuracy, eﬃciency, and resilience to noise. Two similarity measures were studied:
(i) the transition amplitude approach and (ii) the SWAP test. A simple NLP meaning
classiﬁcation task from previous work was used to train the word embeddings and evaluate
the performance of both models. The Python module lambeq and its related software stack
was used for implementation. The explicit model from previous work was used to train
word embeddings and achieved a testing accuracy of 93.09 ± 0.01%. It was shown that
both the SVM variants achieved a higher testing accuracy of 95.72 ± 0.01% for approach
(i) and 97.14 ± 0.01% for (ii). The SWAP test achieved a slightly higher accuracy and
yielded a more appropriate similarity matrix. The SWAP test was then simulated under
a noise model deﬁned by the real quantum device, ibmq_guadalupe. The explicit model
achieved an accuracy of 91.94 ± 0.01% while the SWAP test SVM achieved 96.7% on the
testing dataset, suggesting that the kernelized classiﬁers are resilient to noise. These are
encouraging results and motivate further investigations of our proposed kernelized QNLP
paradigm.

Acknowledgements

I would like to thank my supervisor, Dr. Stephen Hughes, for his insight and guidance
throughout this project. This ﬁeld was new to both of us so I appreciate him for wondering
into the unknown with me.

As this is the culmination of my undergraduate degree, I would also like to thank my
family, friends, peers, and the Queen’s Faculty that have all supported me and played a
part in the development of my personal and professional self.

i

Contents

1 Introduction and Motivation

2 Background Theory

2.1 Grammatical Model of Meaning: DisCoCat . . . . . . . . . . . . . . . . . .

2.2 Quantum Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Quantum Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4 Quantum Natural Language Processing . . . . . . . . . . . . . . . . . . . .

3 Problem Deﬁnition

4 Methodology

5 Models and Simulation

5.1

Initial Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2 Transition Amplitude . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3 SWAP Test

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4 Noise Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Conclusions

A Statement of work and contributions

1

2

2

3

4

9

10

11

12

12

13

16

18

20

26

ii

List of Figures

1

2

3

4

5

6

Diagrammatic representation of two DisCoCat encoded example sentences.

General parameterized quantum circuit architecture.

. . . . . . . . . . . .

Transition amplitude similarity measure in quantum circuit form.

. . . . .

SWAP test circuit diagram.

. . . . . . . . . . . . . . . . . . . . . . . . . .

QNLP example using DisCoCat. . . . . . . . . . . . . . . . . . . . . . . . .

Diagrammatic bra-ket inner-product of two sentences using DisCoCat. . . .

7 Word embedding training convergence.

. . . . . . . . . . . . . . . . . . . .

8

9

Gram matrices of the transition amplitude approach.

. . . . . . . . . . . .

Gram matrices of the SWAP test similarity measure.

. . . . . . . . . . . .

10 Gram matrices of the SWAP test similarity measure simulated under noise.

3

5

7

8

9

10

14

15

17

19

iii

List of Tables

1

2

3

Summary of similarity matrix regions generated from the transition ampli-

tude approach.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Summary of similarity matrix regions generated from the SWAP test.

. . .

15

17

Summary of similarity matrix regions generated from the SWAP test under

the presence of noise. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

iv

1 Introduction and Motivation

Machine learning (ML) has applications in almost all computational tasks and one ﬁeld

under active research is natural language processing (NLP), which attempts to make hu-

man language accessible to computers [1]. NLP research is motivated in part by the

prevalence of natural language data in the form of emails, tweets, books, and more – ef-

fectively leveraging this data would be incredibly valuable. Applications of NLP include

behavioural analysis, spam ﬁltering, virtual assistants, and more. To understand natural

language, one must understand the ambiguities, context, and the large set of complex

linguistic rules that deﬁne the meaning of language. Consequently, developing a mathe-

matical model for language that computers can eﬃciently implement is a diﬃcult problem

and is at the core of NLP research. Most common approaches to NLP are limited to ana-

lyzing either the meaning of the individual words and phrases based on context or feeding

the text into large black-box neural networks. Alternatively, some approaches leverage

the grammatical structure of the text. Unifying these two approaches is an active area of

research [2] and is the focus of this project as it often leads to an exponential increase in

the size of the vector space required to describe a given sentence [3]. The tensor network

structure, (more precisely, the category-theoretic nature [4]) of some of these grammar

models, namely DisCoCat, has motivated the recent application of quantum computing

to NLP [3], [5]–[7], creating a ﬁeld dubbed quantum natural language processing (QNPL).

Quantum information science (QIS) has become a rich and vibrant area of research

because of the fundamental challenges quantum information theory poses to the classical

computational paradigm, together with experimentally realizable systems [8]. The interest

in quantum computing is motivated by the proven exponential algorithmic complexity

speed-ups in addition to the high-dimensional computational vector spaces that these

computers can access. This has lead to the application of quantum computing to areas

like cryptography, chemistry, machine learning, and NLP.

Quantum hardware today is in the so-called noisy intermediate-scale quantum (NISQ)

device era. As a result, many of the famed quantum algorithms that promise exponential

1

speed-ups are not attainable at large scales yet, which has lead to creation of a class of

hybrid classical-quantum algorithms that reduce the size of the quantum circuits. This is

the ﬁeld of quantum machine learning. These algorithms have been shown to be equiv-

alent to kernel methods [9], [10], which have two distinct manifestations: explicit and

implicit models [11]. Explicit models use a variational circuit to learn how to classify the

data while implicit models compute the similarity of two data points for use by a kernel-

ized classiﬁer such as a support vector machine (SVM). Thus far, QNLP work has been

restricted to building explicit models with the exception of some theoretical work using

nearest neighbour classiﬁers but this is outdated and can be ineﬃcient.

The goal of this thesis is to design a high accuracy, robust implicit quantum machine

learning model that leverages the DisCoCat grammar model to encode natural language

data. This will require the design and analysis of diﬀerent algorithms to measure the

ﬁdelity of these quantum states to determine the similarity of two sentences.

2 Background Theory

2.1 Grammatical Model of Meaning: DisCoCat

One of the main complexities that the ﬁeld of NLP continues to deal with is building a

mathematical representation of natural language. Historically, the approach to this repre-

sentation has been based on the paradigm famously summarized by Firth that, “you shall

know a word by the company it keeps” [12]. This states that the meaning of words can

be learned by examining their context in a corpus, which is a distributional approach to

NLP most commonly employed by bag-of-words models [2], [13]. Modern ML approaches

have moved away from this paradigm but still are limited in their ability to incorpo-

rate the grammatical and linguistic structure of sentences [2] – this is what DisCoCat

aims to solve. DisCoCat is a grammar model that expresses both the meaning of words

(distributional ) and how words interact (compositional ) to form a sentence, and is based

on a category-theoretic model. For a deeper examination of the sophisticated mathemat-

2

(a) Simple noun, transitive verb, noun
sentence, from [3].

(b) More complex sentence, from [3].
Figure 1: The two subﬁgures above show the diagrammatic representation of two sentences
using DisCoCat. Mathematically, the boxes (like Alice) are tensors and the strings con-
necting the boxes are reduction operations that are derived from the grammatical structure
of the sentence. We will see shortly that in QNLP, the boxes are quantum states and the
wires are quantum operations that typically involve entangling.

ics underpinning this model, we refer the reader to the topics of applied category theory,

categorical quantum mechanics, and ZX-calculus [3], [14], [15].

Words in DisCoCat are categorized as types speciﬁed by a pregroup grammar [16],

which deﬁnes the order of tensor used to represent them and the reduction rules for

combining them [7]. The pregroups used here are nouns, n, and sentences, s. Together,

the tensors and reduction rules have the eﬀect of modelling the ﬂow of meaning through

the sentence [3]. This model has a natural diagrammatic representation as shown in Fig. 1.

To combine multiple words, we must combine the individual vector spaces of each word

using tensor products.

2.2 Quantum Computing

Quantum computing is a branch of QIS that leverages the phenomenons of superposition

and entanglement to perform computation. The basic unit of information is a quantum

bit or qubit, which can be represented as an abstract mathematical object that lives in a

two-dimensional complex Hilbert space [8]. We assign this two-level quantum system the

orthonormal basis vectors |0(cid:105) and |1(cid:105), which correspond to the measurement outcomes “0”

3

and “1”, respectively. One can represent the state of the system as a linear combination of

these basis vectors as,

|ψ(cid:105) = α |0(cid:105) + β |1(cid:105) ,

(1)

where α and β are complex coeﬃcients that satisfy |α|2 + |β|2 = 1. The linear combination

of states is referred to as superposition and is one of the oddities of quantum theory

because the system (the qubit in our case) can exist in all possible states at once. This

leads to the probabilistic nature of quantum mechanics that arises from measuring states

in superposition but only observing one of the outcomes 0 or 1. The Born rule is the

postulate of quantum mechanics that speciﬁes that the probability of measuring some

outcome, i (where i = 0, 1 for a qubit), is given by the projection of the system’s state

onto the subspace spanned by the eigenvectors with eigenvalues i: P [i] = | (cid:104)i|ψ(cid:105) |2. It can

then be seen that the probabilities associated with the outcomes speciﬁed by the general

qubit state in Eq. (1) are P [0] = |α|2 and P [1] = |β|2. The state of a system is evolved by

the operation of a unitary transformation, (cid:98)U , which is represented as |ψ1(cid:105) = (cid:98)U |ψ0(cid:105).

To represent the composite space of multiple qubits, one must use tensor products

to combine the individual Hilbert spaces. That is, the joint state of two uncorrelated

qubits is represented as |ψ(cid:105) = |ψ1(cid:105) ⊗ |ψ2(cid:105). Thus, an n-qubit system has a dimension of

2n. Representing a general state vector in this system on a classical computer requires

an exponential amount of memory, which quickly becomes intractable. One represents

a quantum circuit by combining n qubits and applying single- or multi-qubit operations

(gates) to the system.

For a more exhaustive and in-depth introduction to the ﬁeld of QIS, we refer the reader

to Ref. [8].

2.3 Quantum Machine Learning

While some near-term quantum devices have shown increasingly promising results, they

are not fault-tolerant and suﬀer from decoherence due to noise. Therefore, so-called vari-

4

ational quantum algorithms have become a vibrant area of research because they use a

hybrid quantum-classical approach wherein parameterized quantum circuits are executed

on quantum computers and the parameters are optimized using classical optimization

techniques. This keeps the classically intractable portion of the computation task on the

quantum computer while reducing the depth of the circuit. This class of hybrid quantum-

classical algorithms is closely related to the ﬁeld of quantum machine learning (QML)

since many techniques of classical machine learning are applied here and the quantum cir-

cuit is, in essence, learning a function. QML algorithms can have three main components

shown in Fig. 2: (i) the data encoding/embedding layer, (ii) the processing layer, and (iii)

the measurement. The encoding layer ingests the data and converts it to a quantum state

representation through a feature map. The processing layer is a parameterized operation

typically composed of rotating and entangling gates and is subject to training. The mea-

surement speciﬁes which qubits to measure and in what basis, and the outcome of which

is interpreted as the prediction. This type of architecture will be referred to as an explicit

QML model [11] as the circuit directly learns how to transform and separate data.

Figure 2: Parameterized quantum circuit architecture for a data point, x, and set of
parameters, θ. Encoding uses a feature map to transform classical data into a quantum
state and the parameterized “processing” operations can be optimized to learn a desired
mapping – this is an explicit model. One can see the similarity between such a circuit and
a classical machine learning model. Taken from [10].

A recent argument showed that QML algorithms can be equivalently described as

kernel methods [10], [11], which are pervasive in ML. Many classical ML algorithms use

a (classical) feature map to transform data to a more rich feature space in which the

solution is easier to learn. Kernel methods are a diﬀerent class of algorithms that use

a speciﬁed kernel function to compute the similarity of a pair of data points. Kernel

5

functions are desirable as they can implicitly map data to the rich feature space without

explicitly representing the vectors in this space, allowing for the algorithms to compute

similarity in, say, inﬁnite-dimensional feature spaces [17]. Similarly, quantum kernels use

quantum feature maps to transform data to quantum states and compute similarity by

estimating the inner-product, | (cid:104)ψ|φ(cid:105) |2. The use of such kernels is motivated by their

ability to eﬃciently compute inner-products in exponentially high dimensional Hilbert

spaces. For example, consider a pair of data points (cid:126)x1 and (cid:126)x2, and a classical feature map
(cid:126)φ((cid:126)x). Classically, the kernel could be given by,

k((cid:126)x1, (cid:126)x2) = (cid:126)φ((cid:126)x2)T (cid:126)φ((cid:126)x1).

(2)

In the quantum realm, the feature map uses an operator parameterized by a given data

point: (cid:98)Uφ((cid:126)x)|0⊗n(cid:105) = |φ((cid:126)x)(cid:105), and the kernel computes the inner-product of the resulting
states,

k((cid:126)x1, (cid:126)x2) = | (cid:104)φ((cid:126)x2)|φ((cid:126)x1)(cid:105) |2.

(3)

With this kernel function one can the build a classiﬁcation model using, such as a

support vector machine (SVM), which is trained on the Gram matrix of the dataset,

Ki,j = k((cid:126)xi, (cid:126)xj). This type of QML model is an implicit model since the quantum computer

does not map directly to the model output [11]. An SVM is a sparse maximum-margin

machine learning model that ﬁnds a hyper-plane deﬁned by a subset of data points, called

support vectors, to separate training data [17]. An SVM is sparse because it only requires

the support vectors for prediction as they alone deﬁne the decision boundary. The model

is referred to as maximum-margin because the objective of training is to ﬁnd the hyper-

plane that maximizes the distance between the decision boundary and the training data

points that are closest to it. Since dot products (more generally, vector similarities) are

fundamental to SVMs, kernels are often used to implicitly enhance data and have become

a promising framework for QML [9], [18]. In quantum SVMs, the quantum computer is

6

responsible for only the similarity computation which is used by the classical computer to

determine the model output.

...

...

|0(cid:105)⊗n

(cid:98)Uφ((cid:126)xi)

(cid:98)U †
φ((cid:126)xj )

|0(cid:105)⊗n

Figure 3: Circuit diagram for a transition amplitude similarity measure. The circuit is
generalized for n qubits that are encoded with the quantum feature map (cid:98)Uφ((cid:126)x). The
probability of measuring all zeros is used to determine the ﬁdelity of the two quantum
states corresponding to (cid:126)xi and (cid:126)xj via Eq. (4).

To implement a quantum kernel, there are two main approaches we discuss for estimat-

ing the ﬁdelity of two quantum states. One could use the transition amplitude approach

wherein the quantum feature map, (cid:98)Uφ((cid:126)xi), encoding (cid:126)xi is applied to the |0⊗n(cid:105) state, followed
by the adjoint of the operator, (cid:98)U †
, for data point xj. This circuit is then executed for
many “shots” to estimate the probability of measuring |0⊗n(cid:105), which estimates,

φ((cid:126)xj )

| (cid:104)φ((cid:126)xj)|φ((cid:126)xi)(cid:105) |2 = | (cid:104)0⊗n| (cid:98)U †

φ((cid:126)xj ) (cid:98)Uφ((cid:126)xi)|0⊗n(cid:105) |2,

(4)

which resembles the familiar quantum mechanical transition amplitude of the operator.

The quantum circuit diagram for such a circuit is given in Fig. 3.

For a geometric argument of this approach, one must recall that quantum gates can be

represented as rotations of the system’s state along the surface of the Bloch sphere [8]. So

one can think of (cid:98)Uφ((cid:126)xi) as a rotation by an amount speciﬁed by (cid:126)xi and, conversely, (cid:98)U †
a rotation in the opposite direction by (cid:126)xj. Thus, the similarity function deﬁned in Eq. (4)

φ((cid:126)xj )

is

is measuring how close the system is to its initial state after being rotated in the forward

direction by xi and then rotated in the reverse direction by xj.

Alternatively, one could use the SWAP test as shown in Fig. 4 to estimate the ﬁdelity

7

|Ψ(cid:105)

H

|0(cid:105)

H

|φ((cid:126)xi)(cid:105)

|φ((cid:126)xj)(cid:105)

SWAP

Figure 4: Circuit diagram for a SWAP test similarity measure. The H refers to the
Hadamard gate and the multi-qubit gate is the controlled-SWAP gate. The top qubit is
the “ancillary” qubit and the other qubits are prepared in the states corresponding to two
data points. The similarity of the two arbitrary states is estimated with this circuit using
Eq. (6). The state of Ψ is given in Eq. (5).

of the quantum states using an extra “ancillary” qubit [9], [19]. The SWAP test uses

a controlled-SWAP gate exchange two states (i.e., |φ(cid:105) |ψ(cid:105) → |ψ(cid:105) |φ(cid:105)) controlled on the

ancillary qubit, which is put in and then taken out of superposition using the Hadamard

gate. Here, we use the quantum encoded data points as the two states.

Before measurement, the ﬁnal state of the circuit shown in Fig. 4 is given by [19],

|Ψ(cid:105) =

1
2

|0(cid:105) (|φ((cid:126)xi)(cid:105) |φ((cid:126)xj)(cid:105) + |φ((cid:126)xj)(cid:105) |φ((cid:126)xi)(cid:105)) +

1
2

|1(cid:105) (|φ((cid:126)xi)(cid:105) |φ((cid:126)xj)(cid:105) − |φ((cid:126)xj)(cid:105) |φ((cid:126)xi)(cid:105)). (5)

It can then be seen (using the Born rule) that measuring 0 for the ancillary qubit

has the probability P r[0] = 1

2| (cid:104)φ((cid:126)xj)|φ((cid:126)xi)(cid:105) |2. Thus, repeating this experiment many
times allows us to obtain an estimate for P r[0], which can then be used to compute the

2 + 1

similarity of the data points via,

| (cid:104)φ((cid:126)xj)|φ((cid:126)xi)(cid:105) |2 = 2P r[0] − 1.

(6)

Since the Hadamard gate, H, is its own inverse (i.e., (cid:98)H (cid:98)H = (cid:98)I), if there was no
controlled-SWAP gate then the ancillary qubit would always be 0. Intuitively, one can

imagine that the similarity of the two states in the circuit above is measured based on the

amount of disturbance that swapping the two states has on the ancillary qubit’s super-

position. This can easily be generalized to multi-qubit states using additional ancillary

8

qubits or just additional controlled-SWAP gates targeted on diﬀerent qubit pairs.

2.4 Quantum Natural Language Processing

The tensor network nature of DisCoCat (as described in Section 2.1) means that it requires

an intractable amount of resources to model modest sized NLP problems on a classical

computer; however, it is naturally and eﬃciently realizable on a quantum computer. In

fact, the model’s origin has roots in both linguistics and categorical quantum mechanics

[4], [14]. In this formulation, the “boxes” that represent word tensors are quantum states

of qubits and the “strings” connecting boxes are implemented as quantum entangling

operations as seen in Fig. 5. This has lead to the creation of a new ﬁeld named quantum

natural language processing (QNLP).

(a) DisCoCat string diagram representation of a sentence with pregroup type labelled.

(b) Quantum circuit representation of the sentence in (a). Boxes (iii), (i), (ii), (v) correspond to
“person”, “prepares”, “tasty”, “dinner”, respectively. (iv) is the quantum operation (Bell measure-
ment) that corresponds to the “cup” (reduction operation, ∪) joining “prepares” and “tasty”. The
triangles with 0 at the bottom of the circuit indicate post-selecting for 0 measurements.

Figure 5: Both ﬁgures taken from [7], exemplify how a sentence is converted from DisCoCat
string diagram (a) to a quantum circuit (b). Note that qn = 1 = qs.

Existing experimental QNLP work [6], [7] use DisCoCat to represent sentences as

parameterized quantum circuits. This allows the practitioner to deﬁne how words are

9

represented by specifying the number of qubits per n and s pregroups (qn and qs, respec-

tively) and the circuit structure used to encode meaning (i.e., the ansatz ). These circuits

are trained on a dataset to learn the quantum state representation of each word (i.e., the

word embeddings) using a hybrid quantum-classical optimization approach. Model pre-

diction is done using the explicit approach of measuring the output qubit(s) of a given

circuit. These qubits correspond to the quantum representation of the sentences (see the

free thick black wires at the bottom of Fig. 1 or the s string in Fig. 5). This variational

quantum encoding requires post-selection (shown in Fig. 5(b)) wherein only the circuit ex-

ecution results that yield 0 for certain qubits are valid representations of the state. Other

theoretical work [5], [20] propose computing the similarity of sentences by evaluating the

inner-product of two quantum-encoded sentences. Diagrammatically, this can be seen in

Fig. 6. This work relies on the use of a quantum random access memory (qRAM) [21]

that is not physically realizable yet and is applied to a nearest-neighbour classiﬁer, which

is ineﬃcient for prediction.

Figure 6: Diagrammatic bra-ket inner-product of two sentences using DisCoCat string
diagrams. The noun pregroup type corresponds to thin strings and the bold strings corre-
spond to the sentence group. Also note that some simpliﬁcations were made by removing
the words “does” and “not” for eﬃciency.

3 Problem Deﬁnition

Linguistic structure is a critical aspect of language comprehension but is often overlooked

in classical NLP techniques. Recent experimental work in QNLP is limited to explicit

quantum algorithms that compute single-sentence expectation values to obtain a binary

10

label [6], [7]. Other theoretical models propose leveraging sentence similarity in a nearest

neighbour classiﬁer, but this can be ineﬃcient because prediction requires that the sim-

ilarity between a given data point and all training data samples be computed [5], [20].

Further, the design of an executable algorithm and experimental analysis of such a model

has not been done.

This thesis project aims to design an algorithm that extends the predominant QNLP

grammar model, DisCoCat, by designing a kernel function that leverages the model to

allow for more robust and eﬃcient classiﬁcation algorithms, e.g., SVMs. The main mea-

sure that will be used to validate the results of this project is accuracy on a given NLP

classiﬁcation task compared to existing algorithms. The design is constrained primarily by

the level of noise in current quantum devices as well as the number of qubits such devices

have (say <50) so the number of gates (depth) and qubits (width) are the main technical

constraints for real world application.

4 Methodology

When considering the software tools available today for designing quantum algorithms,

there are many diﬀerent options but the two classes of tool kits discussed here are general-

purpose established libraries like IBM’s qiskit1 and the niche software stack designed

speciﬁcally for QNLP, namely lambeq [22]. lambeq is a Python library (released in October

2021) built for QLNP experiments and is the ﬁrst of its kind. As such, this option requires

a signiﬁcant time investment in learning about the packages from source code, accepting

the risk of running into some bugs, and a relatively limited number of features compared to

qiskit. The alternative option would be to build custom tools from scratch using qiskit.

This alternative would allow for maximal control over the design but it would require a

signiﬁcant amount of additional work in building the tools already provided by lambeq and

other libraries (e.g., discopy [23]). Moreover, lambeq is naturally integrated with tket

1https://qiskit.org/

11

[24], a tool used for cross-platform quantum circuit compilation and optimization so the

quantum circuits built with lambeq can be directly executed on IBM simulators/hardware

or converted to qiskit objects for further modiﬁcation. Therefore, we decided to use

lambeq so that initial design and experimentation could be done immediately rather than

spending signiﬁcant time on replication.

To build a quantum kernel, we must deﬁne an algorithm to compute the ﬁdelity of two

quantum states. The two similarity measures considered in this project are the transition

amplitude (Fig. 3 and Eq. (4)) and SWAP test (Fig. 4 and Eq. (6)) approaches. The design

that is canonically optimal with respect to noise and accuracy is not immediately clear.

One might opt for the transition amplitude approach because the states are not arbitrary so

the similarity can be estimated directly from the feature maps of each data point, according

to Eq. (4). Further, this approach requires at least two less qubits to implement (one for

the ancillary qubit and one for the shared s qubit(s)), meaning that it minimizes the width

of the circuit. Since this approach requires an inversion and composition of the two given

circuits it has an added technical challenge of processing the results and determining how

to compose the circuits when there are multiple qubits per pregroup type, qs > 1. This

trade-oﬀ between the two ﬁdelity measures in the context of QNLP is investigated further

in this work, but since the the transition amplitude approach minimizes width and it

computes the overlap in a more direct manner, it will be initially preferred.

5 Models and Simulation

5.1

Initial Training

We must ﬁrst determine how to load sentences into their appropriate quantum state.

Past work [5], [20] has relied on qRAM, but since this is infeasible at the moment we

adopt the paradigm that uses the DisCoCat model as an envelope to encode our sentences

using QML techniques as demonstrated in recent work [6], [7]. This is analogous to

the classical ML technique of learning word embeddings but now has the advantage of

12

incorporating linguistic knowledge as conveyed by DisCoCat. This was done by replicating

the meaning classiﬁcation experiments done in Ref. [7]. We used their same dataset

and model to learn the quantum word embeddings and benchmark our proposed models.

The classiﬁcation task consists of determining if a given sentence’s topic is “food” or

“technology” related. The dataset is built from a vocabulary of size 17 and has 100

sentences which split according to a 70/30 train/test split. Some data points include:

“skillful man prepares tasty meal”,
“skillful person debugs program”,
“woman prepares useful application”,

and other grammatically correct combinations of the vocabulary. While this task is small

and relatively simple compared to state-of-the-art NLP work, it is nontrivial since some

words are shared between the classes, making it an illustrative task for this proof-of-concept

design. Following the work in Ref. [7], we use an Instantaneous Quantum Polynomial

(IQP) ansatz for the quantum circuit with one layer and one qubit to express the sentence,

s, and noun, n, grammar types – that is qs = 1 = qn. An example of this ansatz is given

in Fig. 5. We then trained the model using appropriate post-selection and the expectation

value of the sentence qubit as the label. This explicit model achieved a training accuracy

of 95.31 ± 0.01% and testing accuracy of 93.09 ± 0.01%, estimated over seven trials. The

training progress is given in Fig. 7.

It should be noted that due to the inherent probabilistic nature of these models in

addition to the limited size of the dataset, the accuracies are subject to ﬂuctuations.

We attempted to correct for this by executing circuits for 8192 shots and estimating our

accuracies over seven trials using diﬀerent random number seeds to report both the mean

accuracy and its standard error.

5.2 Transition Amplitude

Armed with a classiﬁcation task and appropriate word embedding parameters, we could

experiment with the various state ﬁdelity measures available. As mentioned in Section 4,

the transition amplitude approach uses less qubits and allows us to directly implement

13

Figure 7: Training convergence for cross-entropy loss (left) and accuracy (right) of the
explicit model used to learn word embeddings over 100 epochs. A one layer IQP ansatz
with one qubit per sentence and noun types (qs = 1 = qn) was used. Circuit was executed
on IBM’s Aer quantum simulator with no noise for 8192 shots.

| (cid:104)φ((cid:126)xj)|φ((cid:126)xi)(cid:105) |2 using the quantum feature map, (cid:98)Uφ((cid:126)xi), and its adjoint. This suggests
that this approach may be more suitable for this task. To implement this, we initially

tried the simplistic approach of taking the adjoint of the second data point, composing

the two quantum circuits, and executing with post-selecting for 0 on all qubits. However,

this performed very poorly, resulting in roughly a 50% accuracy. Even the similarity of a

point with itself was not near a value of one as we would expect.

The ﬂaw with this design is that it is in eﬀect ﬁnding the transition amplitude of

the entire circuit, but we only want the transition amplitude of the s qubit(s) (i.e., the

bold string in Fig. 6). This requires removing post-selection from the s qubit(s) and then

ﬁnding the probability the this reduced state is 0 after post-selecting the other qubits.

This is a more technically challenging post-processing task, especially since it is diﬃcult

to keep track of the desired s qubit(s) with the chosen software stack. Thus, this kernel

function was only implemented for the case where a single qubit expresses a sentence type,

qs = 1. Re-normalizing the probabilities after post-selection, we obtain a value between 0

and 1, which directly corresponds to similarity.

This kernel function was used to create the Gram matrices, Ki,j = | (cid:104)φ((cid:126)xj)|φ((cid:126)xi)(cid:105) |2,

14

Figure 8: Gram matrices of the transition amplitude approach for training and testing.
These matrices hold elements, Ki,j = | (cid:104)φ((cid:126)xj)|φ((cid:126)xi)(cid:105) |2, which were computed using the
approach in Eq. (4). An SVM ﬁt to these matrices achieves a training accuracy of 91.4%
and testing accuracy of 96.7%. Data points are ordered by class label for visualization.

Table 1: Summary of similarity values obtained from the transition amplitude approach
for diﬀerent regions of the Gram matrix. This table reports the mean and standard
deviation, σ. The regions studied are the corners in Fig. 8 that correspond to data from
Class 0 compared with data from Class 0, similarly for Class 1, and then Class 0 data
being compared to Class 1 data (mixed).

Region Mean Similarity Similarity σ
Class 0
Class 1
Mixed

0.86
0.49
0.22

0.15
0.29
0.21

for both training and testing data, shown in Fig. 8 and numerically summarized in Tab. 1.

Note the symmetry of the kernel function, i.e., Ki,j = Kj,i, allows us to reduce the number

of circuit executions for training. A SVM classiﬁer from scipy2 was then ﬁt and tested

on these precomputed matrices. This resulted in a training accuracy of 89.59 ± 0.01%

and testing accuracy of 95.72 ± 0.01%, which is an increase in accuracy compared to the

explicit model that was used to train the word embeddings in Section 5.1.

While accuracy was high, the similarity matrices in Fig. 8 are not as we expected. The

data points are ordered by class so one would expect to see a four tile chess board plot.

Interestingly, the upper left corner (class 0 compared to class 0) of the matrices are more

2https://scipy.org/

15

0204060xi0204060xjTraining Gram matrix0204060xi020xjTesting Gram matrix0.00.20.40.60.81.00.00.51.0uniformly dark, which indicates high similarity throughout while the bottom right corner

(class 1 compared to class 1) is less solid and dark. This is quantiﬁed in Tab. 1 where

we can see that class 0 has a high mean similarity and low variance compared to class

1. We suspect that the reason for this imbalance is that adjoint of the sentence feature

map circuit is not as “powerful” of a reversing operation as we expected. That is, class 0

data points were trained to map the s qubit to |0(cid:105) so these states coming into the adjoint

feature map are already close to |0(cid:105) but the opposite is true for the class 1 data – the states

are close to |1(cid:105) so the adjoint has to do a more diﬃcult task to rotate the state back to |0(cid:105).

This creates a larger chance of error for class 1 data. Further (and most concerning), the

diagonal of the training Gram matrix is not all ones as would be expected for the similarity

of a point with itself. This suggests that there is a ﬂaw with the approach but this was

not investigated further because of time constraints and the encouraging accuracy.

Achieving an acceptable accuracy on a ﬂawed set of Gram matrices, suggests a certain

degree of robustness in the kernelized model paradigm. As a result, we decided not to

spend additional time trying to generalize the algorithm to handle higher dimensional

sentence tensors but rather experiment with an alternative similarity measure that may

be more reliable. This single qubit simulation serves as a valid proof-of-concept test for

the approach and increasing qs would only expand the dimension of the space. This is not

likely to address the core problem that we suspect lies in the inversion of the circuit but

would raise more theoretical consideration regarding the order to compose circuits.

5.3 SWAP Test

The SWAP test similarity measure requires more qubits to implement (speciﬁcally, 1 + qs

additional qubits) then the transition amplitude approach but has the advantage of being

able to measure the ﬁdelity of two arbitrary quantum states. The implementation of this

approach is less complicated and can easily be generalized to higher dimensional sentence

tensors by either performing multiple controlled-SWAP gates on the single ancillary qubit

or creating additional ancillary qubits, each with their own test. The question still remains

16

of what qubit pairs we target for the similarity with qs > 1 though.

Implementation was done by preparing the two given data points’ quantum circuits in

parallel, applying a SWAP test (see Fig. 4), and post-selecting the appropriate encoding

qubits to get the probability distribution of ancillary qubit. Since lambeq does not have

a supported controlled-SWAP gate, we converted to tket objects once the individual

circuits were constructed. Using Eq. (6), we can then estimate | (cid:104)φ((cid:126)xj)|φ((cid:126)xi)(cid:105) |2 and build

a quantum kernel function.

Figure 9: Gram matrices of the SWAP test similarity measure. These matrices hold
elements, Ki,j = | (cid:104)φ((cid:126)xj)|φ((cid:126)xi)(cid:105) |2, which were computed using the approach in Eq. (6).
An SVM ﬁt to these matrices achieves a training accuracy of 98.6% and testing accuracy
of 100%. Data points are ordered by class label.

Table 2: Summary of similarity values obtained from the SWAP test approach for diﬀerent
regions of the Gram matrix. This table reports the mean and standard deviation, σ. The
regions studied are the corners in Fig. 9 that correspond to data from Class 0 comparing
with data from Class 0, same for Class 1, and then Class 0 data being compared to Class
1 data (mixed).

Region Mean Similarity Similarity σ
Class 0
Class 1
Mixed

0.89
0.68
0.24

0.12
0.27
0.25

This kernel function was used to compute the Gram matrices shown in Fig. 9 and

numerically summarized in Tab. 2. The SVM classiﬁer was then ﬁt and tested on these

precomputed matrices. This resulted in a training accuracy of 96.32 ± 0.01% and a testing

17

0204060xi0204060xjTraining Gram matrix0204060xi020xjTesting Gram matrix0.00.20.40.60.81.00.00.51.0accuracy of 97.14 ± 0.01%. This outperformed both the explicit model as well as the

implicit model built with the transition amplitude approach.

The Gram matrices in Fig. 9 are closer to what we expect for this task: a more uniform

four tile chess board plot with a dark line of all ones along the diagonal. Interestingly, this

plot also shows diﬀerent values of similarity between the two classes, with class 1 again

having a slightly lower mean similarity score within itself, relative class 0. This is likely

due to the imbalance of the dataset or that the qubits start in |0(cid:105) but the class 1 mean

similarity for the SWAP test is signiﬁcantly larger than the other approach, scoring 0.68

compared to the transition amplitude’s mean score of 0.49.

5.4 Noise Simulation

Thus far, we have simulated these quantum similarity measures on an ideal backend from

qiskit. Since current quantum hardware is in the NISQ era and it is not clear when

there will be fault-tolerant quantum computers, noise tolerance is a critical factor to

consider when dealing with real world applications. Therefore, we simulated our proposed

algorithm in the presence of noise to understand the algorithm’s robustness. Recall that

the goal of this thesis is the proof-of-concept design of the algorithm so fully dealing with

the complexities of decoherence and error was deemed out of the scope of this work.

Using the mock module of qiskit we could access simulators of real IBM Quantum

hardware. We decided to simulate the ibmq_guadalupe device because it has the least

number of qubits required for our task and it has the same basis gates our ansatz was

designed for [7]. The explicit model achieved training and testing accuracies of 92.86 ±

0.01% and 91.94 ± 0.01% under the noise simulation. This model performs well in the

presence of noise with only a few percentage point drop in accuracy.

The resulting Gram matrices for the SWAP test simulated under these conditions are

shown in Fig. 10 and summarized in Tab. 3. This gave accuracies of 80% and 96.7% on

training and testing data. Note that this simulation was only run once because of its high

computational demand so we have no estimate of the error on this accuracy. However,

18

Figure 10: Gram matrices of the SWAP test similarity measure simulated under noise. The
noise model was based on the ibmq_guadalupe device. Similarity values were computed
using Eq. (6). An SVM ﬁt to these matrices achieves a training accuracy of 80% and
testing accuracy of 96%.

Table 3: Summary of similarity values for diﬀerent regions of the Gram matrix obtained
from the SWAP test approach under the presence of noise. This table reports the mean
and standard deviation, σ. The regions studied are the corners in Fig. 10 that correspond
to data from Class 0 comparing with data from Class 0, same for Class 1, and then Class
0 data being compared to Class 1 data (mixed)

Region Mean Similarity Similarity σ
Class 0
Class 1
Mixed

0.71
0.55
0.36

0.09
0.16
0.16

seeding the noiseless simulator with the same value yields respective accuracies of 98.6%

and 100% and the the similarity matrices in Fig. 9 for comparison. The mean similarity

scores in each region of the Gram matrices approached a more central value near 0.5 in

the presence of noise, which is to be expected. Still, the SVM is able to ﬁnd the optimal

support vectors and classify the testing data with high accuracy.

Unfortunately, the transition amplitude approach could not be tested on the noise

simulator. It has not been determined why this is but simulation kills the Python kernel

almost immediately so it is likely a memory issue.

19

0204060xi0204060xjTraining Gram matrix0204060xi020xjTesting Gram matrix0.00.20.40.60.81.00.00.51.06 Conclusions

The goal of this thesis was to design an implicit quantum kernelized classiﬁer, namely the

SVM, that leverages the DisCoCat grammar model. This required designing a function

that can measure the similarity of the quantum states of two sentences after the required

quantum circuit encoding. We explored two common similarity measures: the transition

amplitude approach and the the SWAP test. The implementation and testing of these

techniques were done using Python libraries like lambeq [22], tket [24], and dicopy [23]

aided by a qiskit quantum simulator backend.

It was shown that the SVM models

consistently performed better on the testing dataset than the explicit model that was used

to train the embeddings. Between the two similarity measures proposed, the SWAP test

approach performed slightly better on both training and testing data, achieving respective

accuracies of 96.32 ± 0.01% and 97.14 ± 0.01% compared to the 89.59 ± 0.01% and 95.72 ±

0.01% achieved by the transition amplitude approach. Further, the transition amplitude

approach yields a ﬂawed Gram matrix but has the beneﬁt of using less qubits. However,

we can consider the additional qubits required by the SWAP test a negligible overhead

cost because (i) the number of qubits on real hardware has been steadily growing with

no sign of stopping, (ii) the number of qubits required for QNLP tasks is bounded by

the eﬀective upper limit on sentence length, and (iii) the additional qubits required by

the SWAP test scales as a constant additive factor. Moreover, the SWAP test requires

a smaller percentage of post-selection because of the additional qubits but since this will

likely introduce additional noise in real experiments this is not a compelling advantage of

the SWAP test but is worth noting. The SWAP test was also shown to be robust to noise,

achieving a comparable testing accuracy of 96.7% when simulated under noise, as speciﬁed

by the ibmq_guadalupe device. Therefore, the increased reliability and performance of

the SWAP test and its comparable eﬃciency, we recommend the SWAP test over the

transition amplitude for most applications.

Moving forward, the obvious next step in the analysis would be executing the kernel

function on real quantum hardware (ideally a variety of diﬀerent hardware types) but this

20

requires special access. We also recommend experimenting with more qubits assigned to

both noun and sentence grammar types. This will encode the sentence’s state over multiple

qubits so the correct method for computing similarity between two general sentences is

not immediately obvious (i.e., which qubit is compared to which). It will depend on the

method used to train the word embeddings, opening up an interesting avenue for further

research. Further work could be done to improve the the word embedding scheme. The

technique used here is in some sense a variational qRAM that must be trained for each

task, vocabulary, and ansatz deﬁnition. Finally, an interesting extension to this work

would be incorporating the kernel function with the extended model, DisCoCirc [25], that

expresses meaning of an entire text.

Word Count

≈ 5,600

21

References

[1] J. Eisenstein, Introduction to Natural Language Processing, ser. Adaptive Computa-

tion and Machine Learning Series. MIT Press, 2019, isbn: 978-0-262-35457-8. [On-

line]. Available: https://books.google.ca/books?id=XGquDwAAQBAJ.

[2] A. Lenci, “Distributional Models of Word Meaning,” Annual review of linguistics,

vol. 4, no. 1, pp. 151–171, 2018, issn: 2333-9683.

[3] B. Coecke, G. de Felice, K. Meichanetzidis, and A. Toumi. (Dec. 7, 2020). “Founda-

tions for Near-Term Quantum Natural Language Processing.” arXiv: 2012.03755,

[Online]. Available: http://arxiv.org/abs/2012.03755.

[4] B. Coecke, M. Sadrzadeh, and S. Clark. (Mar. 23, 2010). “Mathematical Foundations

for a Compositional Distributional Model of Meaning.” arXiv: 1003.4394, [Online].

Available: http://arxiv.org/abs/1003.4394.

[5] W. Zeng and B. Coecke, “Quantum Algorithms for Compositional Natural Lan-

guage Processing,” Electronic Proceedings in Theoretical Computer Science, vol. 221,

pp. 67–75, Aug. 2, 2016, issn: 2075-2180. doi: 10 . 4204 / EPTCS . 221 . 8. arXiv:

1608.01406. [Online]. Available: http://arxiv.org/abs/1608.01406.

[6] K. Meichanetzidis, A. Toumi, G. de Felice, and B. Coecke. (Dec. 7, 2020). “Grammar-

Aware Question-Answering on Quantum Computers.” arXiv: 2012.03756, [Online].

Available: http://arxiv.org/abs/2012.03756.

[7] R. Lorenz, A. Pearson, K. Meichanetzidis, D. Kartsaklis, and B. Coecke. (Feb. 25,

2021). “QNLP in Practice: Running Compositional Models of Meaning on a Quan-

tum Computer.” arXiv: 2102.12846, [Online]. Available: http://arxiv.org/abs/

2102.12846.

[8] M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Informa-

tion: 10th Anniversary Edition, 10th. USA: Cambridge University Press, 2011, isbn:

1107002176.

22

[9] V. Havlíček, A. D. Córcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow,

and J. M. Gambetta, “Supervised learning with quantum-enhanced feature spaces,”

Nature, vol. 567, no. 7747, pp. 209–212, 7747 Mar. 2019, issn: 1476-4687. doi:

10.1038/s41586- 019- 0980- 2. [Online]. Available: https://www.nature.com/

articles/s41586-019-0980-2.

[10] M. Schuld. (Apr. 17, 2021). “Supervised quantum machine learning models are kernel

methods.” arXiv: 2101.11020, [Online]. Available: http://arxiv.org/abs/2101.

11020.

[11] M. Schuld and N. Killoran, “Quantum machine learning in feature hilbert spaces,”

Phys. Rev. Lett., vol. 122, p. 040 504, 4 Feb. 2019. doi: 10.1103/PhysRevLett.122.

040504. [Online]. Available: https://link.aps.org/doi/10.1103/PhysRevLett.

122.040504.

[12] J. R. Firth, “A synopsis of linguistic theory, 1930-1955,” Studies in linguistic analysis,

pp. 1–32, 1957, Reprinted in F. Palmer (ed.)(1968). Studies in Linguistic Analysis

1930-1955. Selected Papers of J.R. Firth., Harlow: Longman.

[13] Z. S. Harris, “Distributional structure,” WORD, vol. 10, no. 2-3, pp. 146–162, 1954.

doi: 10.1080/00437956.1954.11659520. [Online]. Available: https://doi.org/

10.1080/00437956.1954.11659520.

[14] B. Coecke, M. Sadrzadeh, and S. Clark, “Mathematical foundations for a compo-

sitional distributional model of meaning,” Lambek Festschrift Linguistic Analysis,

vol. 36, Mar. 2010.

[15] B. Coecke and A. Kissinger, Picturing Quantum Processes: A First Course in Quan-

tum Theory and Diagrammatic Reasoning. Cambridge University Press, 2017. doi:

10.1017/9781316219317.

[16] J. Lmabek, From word to sentence : A computational algebraic approach to grammar.

Polimetrica, 2008. doi: 10.1017/9781316219317.

23

[17] C. M. Bishop, Pattern Recognition and Machine Learning (Information Science and

Statistics). Berlin, Heidelberg: Springer-Verlag, 2006, isbn: 0387310738.

[18] P. Rebentrost, M. Mohseni, and S. Lloyd, “Quantum support vector machine for big

data classiﬁcation,” Physical Review Letters, vol. 113, no. 13, p. 130 503, Sep. 25,

2014, issn: 0031-9007, 1079-7114. doi: 10.1103/PhysRevLett.113.130503. arXiv:

1307.0471. [Online]. Available: http://arxiv.org/abs/1307.0471.

[19] H. Buhrman, R. Cleve, J. Watrous, and R. de Wolf, “Quantum ﬁngerprinting,” Phys.

Rev. Lett., vol. 87, p. 167 902, 16 Sep. 2001. doi: 10.1103/PhysRevLett.87.167902.

[Online]. Available: https : / / link . aps . org / doi / 10 . 1103 / PhysRevLett . 87 .

167902.

[20] L. J. O’Riordan, M. Doyle, F. Baruﬀa, and V. Kannan, “A hybrid classical-quantum

workﬂow for natural language processing,” Machine Learning: Science and Technol-

ogy, vol. 2, no. 1, p. 015 011, Dec. 2020. doi: 10.1088/2632-2153/abbd2e. [Online].

Available: https://doi.org/10.1088/2632-2153/abbd2e.

[21] V. Giovannetti, S. Lloyd, and L. Maccone, “Quantum random access memory,” Phys-

ical Review Letters, vol. 100, no. 16, Apr. 2008. doi: 10.1103/physrevlett.100.

160501. [Online]. Available: https : / / doi . org / 10 . 1103 % 2Fphysrevlett . 100 .

160501.

[22] D. Kartsaklis, I. Fan, R. Yeung, A. Pearson, R. Lorenz, A. Toumi, G. de Felice, K.

Meichanetzidis, S. Clark, and B. Coecke. (Oct. 8, 2021). “Lambeq: An Eﬃcient High-

Level Python Library for Quantum NLP.” arXiv: 2110.04236, [Online]. Available:

http://arxiv.org/abs/2110.04236.

[23] G. de Felice, A. Toumi, and B. Coecke, “DisCoPy: Monoidal categories in python,”

Electronic Proceedings in Theoretical Computer Science, vol. 333, pp. 183–197, Feb.

2021. doi: 10.4204/eptcs.333.13. [Online]. Available: https://arxiv.org/pdf/

2005.02975.pdf.

24

[24] S. Sivarajah, S. Dilkes, A. Cowtan, W. Simmons, A. Edgington, and R. Duncan,

“Tket: A retargetable compiler for NISQ devices,” Quantum Science and Technology,

vol. 6, no. 1, p. 014 003, Nov. 2020. doi: 10.1088/2058- 9565/ab8e92. [Online].

Available: https://doi.org/10.1088/2058-9565/ab8e92.

[25] B. Coecke. (Feb. 28, 2020). “The Mathematics of Text Structure.” arXiv: 1904 .

03478, [Online]. Available: http://arxiv.org/abs/1904.03478.

25

A Statement of work and contributions

I was aware of the QNLP ﬁeld before September 2021 and wanted to design my thesis

around the topic so I familiarized myself with the relevant literature in the days lead-

ing up to the start of the academic year. Beyond this preliminary reading and ideation,

I completed no work relating to this project before the school year. The ﬁrst semester

was spent deﬁning the speciﬁc project goal, conducting literature review, and replicating

existing results as presented in Section 5.1. The second semester was dedicated to im-

plementation and analysis of the proposed design. All code is available on this project’s

GitHub: https://github.com/mattwright99/thesis

26

