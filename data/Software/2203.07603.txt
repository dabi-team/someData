2
2
0
2

r
a

M
5
1

]

R
C
.
s
c
[

1
v
3
0
6
7
0
.
3
0
2
2
:
v
i
X
r
a

SmartValidator: A Framework for Automatic Identiﬁcation and
Classiﬁcation of Cyber Threat Data

Chadni Islama,b, M. Ali Babara,b, Roland Crofta,b and Helge Janickeb

aCREST – Centre for Research on Engineering Software Technologies, University of Adelaide, Australia
bCyber Security Cooperative Research Centre, Australia

A R T I C L E I N F O

Keywords:
Security Operation Centre
Cyber Threat Information
Threat Intelligence
Alert Validator
Cyber Security
Threat Data
Security Automation
Machine Learning
Artiﬁcial Intelligence
Natural Language Processing

A B S T R A C T

A wide variety of Cyber Threat Information (CTI) is used by Security Operation Centres (SOCs) to
perform validation of security incidents and alerts. Security experts manually deﬁne diﬀerent types
of rules and scripts based on CTI to perform validation tasks. These rules and scripts need to be
updated continuously due to evolving threats, changing SOCs’ requirements and dynamic nature of
CTI. The manual process of updating rules and scripts delays the response to attacks. To reduce
the burden of human experts and accelerate response, we propose a novel Artiﬁcial Intelligence (AI)
based framework, SmartValidator. SmartValidator leverages Machine Learning (ML) techniques to
enable automated validation of alerts. It consists of three layers to perform the tasks of data collection,
model building and alert validation. It projects the validation task as a classiﬁcation problem. Instead
of building and saving models for all possible requirements, we propose to automatically construct the
validation models based on SOC’s requirements and CTI. We built a Proof of Concept (PoC) system
with eight ML algorithms, two feature engineering techniques and 18 requirements to investigate the
eﬀectiveness and eﬃciency of SmartValidator. The evaluation results showed that when prediction
models were built automatically for classifying cyber threat data, the F1-score of 75% of the models
were above 0.8, which indicates adequate performance of the PoC for use in a real-world organization.
The results further showed that dynamic construction of prediction models required 99% less models
to be built than pre-building models for all possible requirements. Thus, SmartValidator is much
more eﬃcient to use when SOCs’ requirements and threat behaviour are constantly evolving. The
framework can be followed by various industries to accelerate and automate the validation of alerts
and incidents based on their CTI and SOC’s preferences.

1. Introduction

Identifying and analyzing Cyber Threat Information (CTI)

is an important part of validating security alerts and inci-
dents [25, 27, 31, 33]. Any piece of information that helps
organizations identify, assess, and monitor cyber threats is
known as CTI [26]. To help a Security Operation Centre
(SOC) in using CTI, existing approaches, such as a unify-
ing threat intelligence platform [25, 27, 31, 51], aim to au-
tomatically gather and unify CTI relevant to security alerts
and incidents. However, gathering CTI is not enough to
perform validation tasks, as security teams need to analyze
and understand CTI for deﬁning response actions. Secu-
rity teams write scripts and deﬁne rules to extract neces-
sary information from CTI, and map alerts and incidents to
CTI [3, 16, 38, 46]. Whilst techniques such as deﬁning rules
and scripts can be automated, they do not help in identify-
ing evolving threats and alerts [41, 52, 56], because rules
can only be deﬁned for behavior of known threats. Thus,
human understanding and resolution are required to identify,
deﬁne and update CTI, rules and scripts for emerging threats
to adapt changing contexts.

The vast volume of CTI makes it time-consuming for
a human to analyze. Thus, to address the shortcoming of

This preprint is accepted for publication in Journal of Network and

Computer Applications, 2022.

chadni.islam@adelaide.edu.au (C. Islam);

ali.babar@adelaide.edu.au (M.A. Babar); roland.croft@adelaide.edu.au
(R. Croft); helge.janicke@cybersecuritycrc.org.au (H. Janicke)

ORCID(s):

deﬁning rules and scripts to use CTI, we present a novel
framework, SmartValidator. SmartValidator identiﬁes CTI
and validates security alerts and incidents by leveraging Ar-
tiﬁcial Intelligence (AI) based automation techniques [18,
36, 41]. SmartValidator follows a systematic and structured
approach for reducing the human cognitive burden to con-
tinuously monitor for changes (e.g., change in attack pat-
terns and CTI) and deﬁne the automation strategies when-
ever changes occur. We focus on two aspects of automa-
tion: (i) automatic identiﬁcation of CTI for diﬀerent alerts
and (ii) automatic validation of alerts using identiﬁed CTI.
By automatic identiﬁcation of CTI, we mean identifying
CTI from a wide variety of sources. The increasing pres-
ence and amount of CTI over the internet demands eﬀective
techniques to automate the identiﬁcation of the required CTI
for validation tasks [18, 31, 34, 36]. The sources of CTI
vary with diﬀerences in alerts and incidents [17, 32, 38].
Examples of CTI include Indicators of Compromise (IoC)
(system artifacts or observables associated with an attack),
Tactics Techniques Procedures (TTP) and threat intelligence
reports [26].

By automatic validation of alerts and incidents, we refer
to validating (i.e., prioritizing, and assessing the relevance
or impact of) diﬀerent types of alerts and incidents gener-
ated and identiﬁed by diﬀerent detectors. In this context, by
detector we mean any tools or systems used for detection of
malicious activities. An organization deploys or develops
diﬀerent types of detectors that generate alerts upon detec-
tion of malicious activities. Examples of such detectors in-

Islam et al.: Preprint submitted to Elsevier

Page 1 of 24

 
 
 
 
 
 
SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

clude Intrusion Detection Systems (IDS), vulnerability scan-
ners and spam detectors. Validation of diﬀerent types of
security alerts and incidents requires extracting information
from relevant CTI [3, 16, 38, 46, 53]. For example, a network
administrator or threat hunter writes scripts to search for CTI
(e.g., information about the suspicious incident) and deﬁnes
rules to validate an alert. There are always cases for which
automatic validation would not be suitable. For example, in
our scenario, automated validation is not applicable for alerts
and incidents that do not have associated CTIs; hence, such
scenario would require a security team to perform manual
analysis.

The massive volume and variations of CTI opens the
door for automatic identiﬁcation of patterns and gathering
insights about CTI using Natural Language Processing (NLP)
and Machine Learning (ML) techniques. For instance, Son-
icwall has reported 9.9 billion malware attacks in its 2020 cy-
ber threat report [45]. The threat research team of Sonicwall
has come across more than 1,200 new malware variants each
day. Existing studies [24, 28, 34, 39, 41, 50, 54, 56] have
highlighted the power of AI to monitor, gather and analyze
security intelligence. Recent advances have also been no-
ticed in the use of NLP and ML techniques to extract patterns
from threat data and gain insight about attacks and threats.
The focus of these studies are application-speciﬁc, for ex-
ample, detecting anomalies [56] or automating vulnerabil-
ity assessment [28], which need to be updated with chang-
ing CTI and organizational needs. These studies required
knowledge of NLP and ML to build a model for performing
the assessment or detection task. Most existing SOCs are
managed SOCs (SOC as a service), which are subscription
based [9, 24]. They do not have dedicated data science or
ML experts to design and update the AI based system based
on their need. Considering this scenario “Can we design an
eﬃcient system to automate and assist the validation of secu-
rity incidents and alerts with changing threat data and user
needs”?.

Evolving threat landscapes and changing needs of secu-
rity teams demand a dynamic AI/ML-based validation sys-
tem which can be adapted at runtime. For instance, if a secu-
rity expert expresses interest to validate the maliciousness of
a domain "URL", a prediction model is built by a data scien-
tist team that classiﬁes a URL as malicious or non-malicious.
In an ML context, this task is known as a prediction or clas-
siﬁcation task. We propose three diﬀerent layers to diﬀeren-
tiate the tasks of threat data collection, validation and pre-
diction model building. The purpose is to hide implemen-
tation complexity of data processing, prediction models and
validators from security teams. Each layer is controlled and
developed by experts with dedicated capabilities. Changing
threat landscapes require SOCs to request new CTI and pre-
diction models. One possible solution to this is to build and
save prediction models for all possible attributes sets. How-
ever, building all possible models whenever changes occur
will incur signiﬁcant resource consumption (e.g., computa-
tion time). Most SOCs have limited resources; hence, in-
stead of pre-building all possible combinations of prediction

.

models, SmartValidator is designed to build the model based
on a SOC’s demands.

We have implemented a Proof of Concept (PoC) sys-
tem to evaluate the eﬀectiveness and eﬃciency of our pro-
posed framework with two feature engineering approaches
and eight ML algorithms. We have used an IoC form of
CTI [26] and collected open source threat intelligence (OS-
INT) from public websites and a CTI platform, MISP [32,
38]. The input of the developed system is a set of attributes
from a security team. For example, a security team may want
to investigate the “domain name” and “URL” to identify the
“maliciousness” of an incident. The developed system takes
these three attributes as input where “domain name” and
) and “malicious-
“URL” are the observed attribute (𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
). The prediction mod-
ness” is the unknown attribute (𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
based on 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
els are built for classifying/ predicting 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
To capture changing contexts (i.e., security team require-
ments), we have considered ﬁve 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
: (i) attack, (ii) threat
type, (iii) name, (iv) threat level and (v) event. Eighteen
diﬀerent sets of 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
(shown in Table 3) are provided to
validate these ﬁve attributes to demonstrate the performance
of the PoC with changing requirements. We have designed
the PoC to select the suitable feature engineering approaches
and ML algorithms at run time. Seven 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
are selected
by the PoC to predict attack and 11 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
sets are used to
predict the remaining four attributes. Hence, the PoC pro-
vided a total 51 optimal prediction models for predicting
. The results
based on the preferred 18 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
ﬁve 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
show that approximately 84% of the models have F1-scores
above 0.72 and 75% of the models have F1-scores above 0.8.
These results imply that SmartValidator is able to assist the
automatic validation of threat alerts with a high level of con-
ﬁdence. Most of the models that were built with data gath-
ered from the MISP platform can eﬀectively predict 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
based on 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
with a higher F1-score than the models that
were built with CTI gathered from public websites. This
demonstrates that trusted threat intelligence is more eﬀec-
tive in validating alerts.

The results also demonstrate the eﬃciency of SmartVal-
idator with dynamic changes in the preferred set of attributes.
We pre-built all possible models, which required us to run 814
experiments. Given a maximum time limit of 48 hours and
a memory limit of 100GB to build each prediction model,
20% of the models failed to complete within the time limit
and given memory. Hence, it shows the diﬃculties a se-
curity team would encounter in manually constructing each
model. Results further reveal that building prediction mod-
els is a time-consuming process and requires expertise that
can be automated through orchestrating diﬀerent tasks. Sav-
ing the feature engineering approaches and ML algorithms
helps SmartValidator to use them for predicting new attributes
based on changing CTI and SOC requirements. Thus, con-
structing prediction models at run time based on a security
team’s preferred attributes sets reduces the overhead and re-
source consumption. The key contributions of this work are:

• A novel AI-based framework, SmartValidator, that con-
sists of three layers to eﬀectively and eﬃciently iden-

Islam et al.: Preprint submitted to Elsevier

Page 2 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

(a)

(b)

Figure 1: Motivation scenario illustrated (a) detection of malicious activities and validation of alerts with multiple detectors and
sources of CTI respectively, (b) validation of same alerts for diﬀerent set of preferences required two diﬀerent validators and
diﬀerent CTI

tify and classify CTI for validating security alerts with
changing CTI and security team requirements.

• A PoC system that automatically built 51 models to
predict ﬁve diﬀerent unknown attributes with 18 ob-
served attributes sets using two sources of OSINT.

• We demonstrated that SmartValidator can eﬀectively
select optimal prediction models to classify CTI where
approximately 75% of optimal models have an F1-score
of above 0.8.

• We showed the eﬃciency of SmartValidator by build-
ing prediction models based on security team demands
which requires approximately 99% less models to build,
thus less resources and time consumption.

Paper organization: Section 2 presents a motivation sce-
nario that highlights the need for SmartValidator. Section 3
discusses the background knowledge about CTI. Section 4
introduces the proposed framework, SmartValidator. Sec-
tion 5 describes the large-scale experiment that is carried
out for the evaluation of SmartValidator. Section 6 demon-
strates the eﬀectiveness and eﬃciency of the proposed ap-
proach. Section 7 discusses related work. Finally, section 8
concludes the paper with future works.

2. Motivation Scenario

In this section, we motivate an AI-based solution for alert
validation through an example scenario. Figure 1 shows a
scenario where a SOC of an organization has deployed dif-
ferent types of detectors, validators and CTI to monitor and
validate malicious behaviour in its network and business data.
Figure 1a shows that three detectors (intrusion, phishing email,
and vulnerability detectors) are deployed to detect suspicious
and malicious activities of an organization. The informa-
tion used by detectors varies with attack types. For exam-
ple, the information that detectors use to identify an intrusion
is diﬀerent from identifying a phishing email1 (Figure 1a).
These detectors continuously monitor an organization’s net-
work and business data2 (e.g., emails, network traﬃc and
business reports).

Most detectors produce alerts upon detecting malicious
activity that require a security team to act on it. These alerts
require validation before analysing them for decision mak-
ing3. In this paper, we consider a validator performs a task
related to prioritizing and identifying the relevance or im-

1https://github.com/counteractive/incident-response-plan-

template/blob/master/playbooks/playbook-phishing.md

2https://github.com/rosenbet/demisto/tree/master/Playbooks
3https://www.incidentresponse.com/playbooks/

Islam et al.: Preprint submitted to Elsevier

Page 3 of 24

CTI data foralerts validationThreat DetectorAlert validatorNetwork andbusiness dataSecurity tool andsecurity teamValidateMalicious IPNetwork TrafﬁcEmailBusiness ReportStep 1: Send datato detectorStep 2: Generate alertsfor malicious activity IntrusionDetectorPhishing EmaildetectorVulnerabilityDetectorValidatePhishing URLIdentify severityof VulnerabilityStep 3: Validate alertsusing CTISuspiciousURL ListVulnerabilityDatabaseSecurityTeamStep 4 Send validated datato security tool and teamSIEM toolBlack list &white list IPEDR toolStep 3.1 Extract attributes from alertsStep 3.2 Identify CTIswith extracted attributes Step 3.4 Validateobserve attributesValidate IPmaliciousnessStep 3.3 Extract datafrom identified CTIExtractattributesExtractattributesCTI1Identify CTIAlertsIdentify CTIExtract datafrom CTI1Extract datafrom CTI2Categorise URLbased on threatlevelIPmaliciousnessURL threatlevelOutput CTI2Validator V1Validator V2AttributesCTI2CTI1SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

pact of alerts. Let us assume that an intrusion detector has
detected a list of malicious IP addresses. A SOC has an alert
validator to validate the maliciousness of the IPs [42]. Fig-
ure 1a shows that validation of diﬀerent types of alerts re-
quires diﬀerent forms of CTI. To validate IP maliciousness,
blacklist and whitelist IP addresses are used. CTI further
varies from organization to organization. Each security team
has its own set of requirements (diﬀerent attributes) to val-
idate an alert.
In this scenario, the security team rely on
three types of CTI for alert validation. Considering diﬀerent
types of alerts have diﬀerent attributes and require diﬀerent
sources of CTI, a SOC needs three validators to validate the
alerts produced by three detectors. Figure 1b illustrates the
use of CTI to validate alerts.

(𝐴𝑖

We assume, an alert of type 𝐴𝑖 ∈ 𝐴 (𝐴 is the set of alerts)
is produced by detector 𝐷1
(e.g., IDS). Each alert is repre-
sented with diﬀerent attributes (or features). The function
𝐹𝑎𝑡𝑡𝑟𝑖𝑏

) provides attributes list of 𝐴𝑖

.
𝐹𝑎𝑡𝑡𝑟𝑖𝑏(𝐴𝑖) →< 𝑓1, 𝑓2, 𝑓3, 𝑓4, 𝑓5 >

where 𝑓1 = IP, 𝑓2 = 𝑑𝑜𝑚𝑎𝑖𝑛, 𝑓3 = URL, 𝑓4 = 𝑎𝑡𝑡𝑎𝑐𝑘 𝑡𝑦𝑝𝑒,
and 𝑓5 = 𝑡ℎ𝑟𝑒𝑎𝑡 𝑙𝑒𝑣𝑒𝑙. Considering two diﬀerent security
roles of a SOC have diﬀerent preferences and used diﬀerent
are built
attributes to validate 𝐴𝑖
(Figure 1b). For validator 𝑉1
, a security team prefers to val-
idate attack type based on IP, domain and URL, thus

, two validators 𝑉1

and 𝑉2

𝑜𝑏1
𝑢𝑛1

𝑎𝑡𝑡𝑟𝑖𝑏 =< IP, 𝑑𝑜𝑚𝑎𝑖𝑛, URL > and
𝑎𝑡𝑡𝑟𝑖𝑏 =< IP 𝑚𝑎𝑙𝑖𝑐𝑖𝑜𝑢𝑠𝑛𝑒𝑠𝑠 >.

For validator 𝑉2
, a security team’s preference is to validate
threat level using attributes domain, URL and attack type,
thus,

1

2

from 𝐴𝑖

and 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏

. On the other hand, CTI

and if available 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏 =< IP, 𝑑𝑜𝑚𝑎𝑖𝑛, URL, 𝑎𝑡𝑡𝑎𝑐𝑘 𝑡𝑦𝑝𝑒 > and
𝑎𝑡𝑡𝑟𝑖𝑏 =< URL 𝑡ℎ𝑟𝑒𝑎𝑡 𝑙𝑒𝑣𝑒𝑙 >.

𝑜𝑏2
𝑢𝑛2
For both cases, to perform validation, validators ﬁrst ex-
tract 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
and then identify
CTI with these attributes. In most cases, a security team pro-
vides CTI sources to a validator. In the next step, CTI that
are identiﬁed. As shown in Figure
have 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
extracts three attributes to validate IP mali-
1b, validator 𝑉1
ciousness and validator 𝑉2
extracts four attributes to validate
has the attributes that are required
URL threat level. CTI
has the attributes
by validator 𝑉1
. Therefore,
required to investigate URL threat level by 𝑉2
threat data is extracted from CTI
respectively for
1
further investigation.
and 𝑉2
Though 𝑉1

have investigated two diﬀerent sets of
attributes, the key steps (step 3.1 to step 3.4), as shown in
Figure 1b, are the same. The tasks 𝑉1
can be formu-
lated as an ML classiﬁcation problem, where two diﬀerent
prediction models are required to be built. Building a pre-
diction model involves pre-processing of data (e.g., 𝑜𝑏1
),
feature engineering, training and selecting a model, and then
predicting a output (𝑢𝑛1
of
Cyber Threat Information (CTI) (e.g., domain, ﬁlename, de-
scription and comment) are textual features. Traditional cat-
egorical feature engineering or transformation approaches
are not suitable to encode the textural features, hence re-
quire the application of NLP technique. To perform valida-

). Many of the possible 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏

and CTI
2

and 𝑉2

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

and 𝑜𝑏2

and 𝑢𝑛2

using 𝑜𝑏1

tion of 𝑢𝑛1
, predic-
tion models are required to be built where input of prediction
models are security team preferences. Here, we consider the
observed attributes and unknown attributes as SOCs’ pref-
erences/ requirements. We assume, 𝐴𝑆 as a set of a SOC’s
requirements, where

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

, 𝑢𝑛1

= <𝑜𝑏1
, 𝑢𝑛2

𝐴𝑆 =< 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏, 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏 >
This mean for validator 𝑉1, 𝐴𝑆1
= <𝑜𝑏2
𝐴𝑆1 ∈ 𝐴𝑆. For 𝑉2, 𝐴𝑆2

> &
𝑎𝑡𝑡𝑟𝑖𝑏
> & 𝐴𝑆2 ∈ 𝐴𝑆
Considering emerging threat patterns, a SOC may de-
ploy new detectors and update the existing rules of intrusion
detectors to detect evolving anomalies. To validate the alerts
of a new detector, new validators may be required. Thus,
several changes can arise in the scenario of Figure 1. Fol-
lowing, we present the three scenarios that we consider in
this work.

𝑎𝑡𝑡𝑟𝑖𝑏

• Change in Alert: With changes in alerts types, vari-
ation can be seen in the attributes of alerts. For ex-
may have a diﬀerent set of
ample, an alert of type 𝐴2
attributes from 𝐴1
such as timestamps, date, IP, orga-
nization, tools and comments. Depending on types of
attack and detector, an alert attribute changes. Build-
ing prediction models with changing alerts might re-
quire incorporation of various types of pre-processing
and features engineering approaches.

• Change in CTI: CTI are continuously changing with
In most cases,
changing requirements from SOCs.
SOCs buy CTI from third parties where attributes pro-
vided by diﬀerent vendors vary, or they build their
own CTI platform. The validation of alerts relies on
the attributes available in CTI.

• Changes in Preferred Attributes Set: Change in pre-
ferred attributes (𝐴𝑆) requires re-designing and build-
ing of prediction models. A SOC does not always have
dedicated data scientists or experts to design and build
prediction models. Even though the steps of model
building are repetitive (e.g., pre-processing, feature en-
gineering, model building and selection), few changes
may be required for adaptation of the variations as ex-
isting solutions are not designed to automatically work
with changing attributes.

To address these changes, we propose SmartValidator to sup-
port the ﬂexible design of a validator following a systematic
and structured approach. The proposed framework can auto-
matically construct prediction models to validate alerts with
changing requirements.

3. Preliminaries

This section provides background information about CTI

and MISP (an Open Source Threat Intelligence Platform).

Indicator of Compromise:

IOCs provide character-
istics of cyberattacks and threats. Based on IOCs, a secu-
rity team decides whether a system is aﬀected by a particu-
lar malware or not [3, 16, 46]. Examples of IOCs include
domain name, IP, ﬁle names and md5 ﬁle hashes. Three

Islam et al.: Preprint submitted to Elsevier

Page 4 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Table 1
Description of each CTI sources with the Indicator of Compromise (IOCs) they contain

Source

Description

C&C Tracker [47]

Feodo Tracker [48]
Malware Domain List [14]

Ransomware Tracker [49]

WHOIS data [13]

Zeus Tracker [55]
OpenPhish [35]

Contains a list of C&C IPs (command and control botnets), date, and a link to a manual which contains
text description and false positive risk value.
Tracks the Feodo Trojan. Contains IP, port, and date.
Contains a searchable list of malicious domains, IP, date, domain, reverse lookups
and lists registrants. Mostly focused on phishing, Trojans, and exploit kits.
Provides overview of infrastructures used by Ransomware, status of URLs, IP address and
domain names associated with Ransomware and various block list of malicious traﬃc.
Provides a database of registered users and assignees of internet resources, which is widely used
for lookup of domain names.
Tracks domains of Zeus Command & Control servers.
A list of phishing URLs and their targeted brand.

Table 2
An example of a list of observed malware domains with corresponding Indicator of Com-
promise (IOCs) from the website malwaredomainlist.com [14]

Date

Domain

IP

Reserve Lookup

Description

2017/12/04 18:50
2017/10/26 13:48

2017/06/02 08:38

2017/05/01 16:22

2017/03/20 10:13

textspeier.de
photoscape.ch/
Setup.exe
sarahdaniella.com/swift
/SWIFT%20$.pdf.ace
amazon-sicherheit.kunden
-ueberpruefung.xyz
alegroup.info/ntnrrhst

104.27.163.228
31.148.219.11

-
knigazdorovya.com

phishing/ fraud
trojan

63.247.140.224

63.247.140.224

coriandertest.
hmdnsgroup.com
hosted-by.blazingfast.io

trojan

phishing

ASN

13335
14576

19271

49349

185.61.138.74

mccfortwayne.org

Ransom, Fake
.PCN, Malspam

197695

common categories of IOCs are network indicators, host-
based indicators and email indicators. IP addresses, URLs
and domain names are the most popular types of network in-
dicators. The malicious ﬁle hash or signature, registry keys,
malware name and dynamic link libraries are widely used
host-based indicators. An email indicator may consist of a
source email address, message objects, attachments, links
and source IP addresses. The source of IOCs ranges from
crowd-sourcing to government-sponsored sources. Just hav-
ing threat data is not enough to fully understand the context
or patterns of a cyberattack. For example, threat data may
contain an IP address that is used only once to attack a net-
work. Conversely, an associated URL in threat data might
have been used many times. Therefore, threat intelligence
must be extracted from the threat data with possible IOCs
and their contextual information [3, 16, 52, 46]. Table 1
shows examples of some of these websites that provide threat
feeds and are utilized for gathering OSINT. Table 2 shows
examples of CTI publicly available in the malware domain
website [14].

Threat Intelligence: Threat Intelligence, also known
as security threat intelligence, is an essential piece of CTI
for a cybersecurity team. According to Recorded Future,
“threat intelligence is the output of the analysis based on de-
tection, identiﬁcation, classiﬁcation, collection and enrich-
ment of relevant data and information” [38].Threat intelli-
gence helps a security team understand what causes an at-
tack and what needs to be done to defend against it by gath-

ering contextual information about an attack. For example,
security teams use threat intelligence to validate security in-
cidents or alerts and enrich threat data to get more insights
about a particular security incident [3, 16, 46, 38, 53]. The
gathered data is organized in a human-readable and struc-
tured form for further analysis [3, 16, 18, 20, 53]. Open
Source Intelligence (OSINT) is gathered from various web-
sites (e.g., Zeus Tracker and Ransomware Tracker) that pro-
vide information about malware or blacklist domain/IPs.

Cyber Threat Intelligence Platform: Threat intelligence
platforms allow security communities to share and collab-
orate to learn more about the existing malware or threats.
Using threat intelligence platforms, companies can improve
their countermeasures against cyber-attacks and prepare de-
tection and prevention mechanisms. In recent years, the cy-
bersecurity communities have emphasized building common
threat intelligence platforms to share threat information in a
uniﬁed and structured way, and make CTI actionable [18, 22,
31, 33, 46, 52]. Various speciﬁcations and protocols such as
STIX, TAXII, Cybox, CWE, CAPEC and CVE are widely
used to describe and share threat information through com-
mon platforms [5, 6, 12, 37]. Trusted Automated Exchange
of Indicator Information (TAXII) [12] is developed as a pro-
tocol for exchanging threat intelligence represented in STIX
format. Both STIX and TAXII are open source and have
collaborative forums [5, 12].

MISP: Malware Information Sharing Platform (MISP)
is one of the most popular trusted threat intelligence plat-

Islam et al.: Preprint submitted to Elsevier

Page 5 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

dation. SOC requirements are interpreted by an interpreter.
Based on a SOC’s requirements, attributes are extracted from
alerts. The extracted attributes are used to choose suitable
prediction model from the candidate list of saved models for
predicting the unknown attributes (𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
) and perform val-
idation of alerts based on observed attributes (𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏

The following sections elaborate the core components

).

Figure 2: An example of MISP showing evoluation of a multi
task Botnet

forms used by diﬀerent industries to store and share CTI [32,
51]. MISP is a framework for sharing and storing threat data
in a structured way [51, 4]. MISP enables an organization
to store both technical and non-technical information about
attacks, threats and malware in a structured way. The re-
lationship between malware and their IOCs are also avail-
able in MISP. Rules for network Intrusion Detection Sys-
tems (IDS) can also be generated from MISP, which can be
imported into an IDS system, and hence improve the detec-
tion of anomalies and malicious behavior. A security team
queries MISP for relevant data, and it shows the details of
the attack. For example, Figure 2 shows the details of the
evolution of a multitask Botnet. Table 11 and Table 12 of
Appendix A show the key attributes gathered from MISP
for this study and and the percentage of each attribute.

4. Proposed Framework

Figure 3 provides an overview of our proposed frame-
work, SmartValidator, that automates the identiﬁcation and
classiﬁcation of CTI for validation of alerts. It comprises of
three layers: (i) threat data collection layer, (ii) threat data
prediction model building layer and (iii) threat data valida-
tion layer. We consider each layer to have a separation of
concerns so that while updating components of one layer, a
security team does not need to worry about other layers.

Threat Data Collection layer (section 4.1): The threat
data collection layer consists of a collector that automates the
identiﬁcation of CTI information from various sources (Fig-
ure 4). It further transforms the gathered CTI into a uniﬁed
form (if required) and passes it to the threat data prediction
model building layer.

Threat Data Prediction Model Building layer(section 4.2):

This layer has a model builder that builds models for val-
,
idation of alerts based on a SOC’s requirements (𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
) using the gathered CTI (Figure 5). Pre-processing of
𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
data, feature engineering, training and evaluation of predic-
tion models are performed in this layer to generate candidate
validation models. The candidate models and corresponding
feature sets are saved with a SOC’s preferences for use by the
threat data validation layer at runtime to validate alert.

Threat Data Validation layer (section 4.3): The threat
data validation layer takes alerts and a SOC’s requirements
as input to choose suitable prediction models for alert vali-

and functionalities of each layer of SmartValidator.

4.1. Threat Data Collection Layer

Validation of a security alert requires the identiﬁcation
of relevant CTI for building a threat data prediction model.
The purpose of the prediction model is to learn the pattern
of CTI for automatic validation of alerts. Here, we have for-
mulated the validation tasks as a classiﬁcation task. For in-
stance, to validate an IP maliciousness, a system needs to
classify IPs as malicious or non malicious, which can be
achieved through a prediction model. Similar to existing
studies [18, 46, 51], the data collection layer gathers CTI
from multiple sources and combines them into a uniﬁed for-
mat. The data collection layer employs a collector to gather
CTI data from an organization’s preferred sources. Consid-
ering several types of CTI sources are used by an organi-
zation, deployment of various plugins, APIs and crawlers
are required to collect CTI from these sources. Figure 4 has
shown the processing of three types of CTI data - (i) inter-
net data, (ii) business data and (iii) external data. These are
most commonly used CTI. Other forms of CTI can also be
integrated by following standard data processing strategies.

For processing internet data, the collector has web crawlers,

scrapers and parsers to gather and process CTI data from
web pages (Figure 4). A web crawler searches and identi-
ﬁes reliable sites that contain threat information and IOCs of
various malware. Considering threat intelligence team pro-
vides the relevant list of websites or keywords of their inter-
est to gather CTI [43], a crawler crawls through the internet
to search the relevant information. We propose a scraper as
part of the collector for ﬁltering out unnecessary information
from crawled data. Crawling and scraping can be done on a
variety of sources, such as RSS feeds, blogs and social me-
dia, but require diﬀerent types of processing. A parser uti-
lizes various information processing techniques to extract in-
formation from the output of a scraper and organise the data
into a structured and language-agnostic format (e.g., markup
language such as ontology). For forums or blog posts written
in natural language, a parser is required to extract threat in-
formation from sentences. NLP tools and techniques (e.g.,
Spacy and NLTK) are used to build a parser based on the
structure of a document and information required by secu-
rity team.

To get threat feeds from databases and CTI platforms, we
design API calls and queries that are parts of the collector.
Threat feeds can be gathered from both an organization’s in-
ternal business data and external data (Figure 4). In this pa-
per, we only gather external threat feeds. The collector can
also query external data sources to ﬁnd out missing informa-
tion about available threat data. For example, after receiving
an IP address, a query can be made to WHOIS query website

Islam et al.: Preprint submitted to Elsevier

Page 6 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Figure 3: An overview of SmartValidator for automated identiﬁcation and validation (i.e., classiﬁcation) of cyber threat data

gence team.

4.2. Threat Data Prediction Model Building Layer
The threat data prediction model building layer is de-
signed to build ML-based classiﬁcation models using CTI
and SOC’s preferences. For example, if a security team wants
to validate the maliciousness of an IP considering the IP, do-
main and URL, an ML classiﬁcation model is built that takes
IP, domain and URL and predicts IP maliciousness. We con-
sider these attributes (IP, domain, URL and IP malicious-
ness) as SOC’s preference where 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏 = {IP, 𝑑𝑜𝑚𝑎𝑖𝑛, URL}
and 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏 = {IP 𝑚𝑎𝑙𝑖𝑐𝑖𝑜𝑢𝑛𝑒𝑠𝑠}. SOC’s preferences drive
from organizational security requirements and alerts.

Figure 5 shows the core components and workﬂow of the
threat data prediction model building layer. It comprises of
a pre-processor that pre-processes CTI (step 1, Figure 5) for
extracting features from it. The pre-processing techniques
depend on the types of attributes (e.g., categorical or text-
based). For example, IP maliciousness can be either mali-
cious or non-malicious, which is categorical. On the other
hand, domain name is text based, as shown in Table 2. Pre-
processed data is passed to a feature engineering module
where data is transformed into features (numeric form) (step
2, Figure 5), which are used as input for the ML algorithms.
The reason behind this is that ML algorithms can only work
with numerical data [2, 39, 40, 50]. Depending on the type,
size and diversity of CTI, the data science team chooses a
feature engineering approach. The ﬁrst two steps of Fig-
ure 5 leverage simple NLP techniques for pre-processing and
feature engineering. Categorical values can be directly trans-
formed into features using label encoding or one-hot encod-
ing and text values are transformed into features using count
vectorizer and TFIDF (Term Frequency-Inverse Document
Frequency) techniques. The associated text cleaning and
pre-processing steps for each text attribute are discussed in
Appendix B1. Common pre-processing techniques such as

Figure 4: Threat data collection layer - IOCs are extracted and
processed from three types of data and then combined into a
uniﬁed form

to search for domain name. In this way, a collector gathers
diﬀerent sets of data, for example, blacklist and white list IP
addresses, list of phishing websites and so on, from diﬀerent
types sources. The collected data is further combined into a
uniﬁed form (e.g., dataset P, Figure 4). To combine the data
into a uniﬁed form, we ﬁrst normalised the data, removed
redundant information and then combined them. Examples
of normalisation techniques include 1NF, 2NF and so forth.
Depending on validation tasks (e.g., validation of IP mali-
ciousness or validation of domain threat level), CTI is ex-
tracted and sent to the threat data prediction model building
layer to build a validation model.

We consider organizations (e.g., government or ﬁnan-
cial) may have a dedicated threat intelligence teams or may
use third party services to gather CTI. Any update related
to the collection of CTI, such as adding or modifying CTI
sources, deploying APIs or parsers to gather and extract in-
formation from these CTI, and inclusion or deletion of new
data collection and normalisation techniques, are performed
in the threat data collection layer by dedicated threat intelli-

Islam et al.: Preprint submitted to Elsevier

Page 7 of 24

DataPredictedOutputInterpreterinterpret user requirementsSecurity Operation CentreCTI dataCollectorCollect CTI dataNoYesCheck model availabilityFeaturesModel BuilderBuild model using CTI data based on user requirementsPredictorPredictunknownattributes Saved modelsYesCTI sourcesNoOrchestratorCoordinate the task of model building and selectionData ProcessorTransform data to provide into the modelSaved feature setsThreat Intelligence TeamData Science TeamOptimal features and modelsModelsThreat data collection layerThreat data prediction model building layerThreat data validation layerSaved ModelsSet of requirementsCheck data availability123Data NormaliserData Set PInternal Business IntelligenceBusiness Data ProcessingParserExtract IOC from structured dataExtract IOC from unstructured dataData Set BExternal Threat Intelligence DatabaseGather  threat intelligence using APIExternal Data ProcessingExtract IOC from Threat intelligenceData Set CInternetWeb CrawlerScraperParserInternet Data ProcessingData Set ASmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

tokenization, stop words removal and lemmatization are per-
formed before transforming text data into features. Hence,
based on the alert attributes types, data science teams per-
form data pre-processing and select a feature engineering
approach.

Figure 5: Threat data prediction layer - prediction models
are built to predict unknown attributes based on observed at-
tributes

The transformed data is split into training and testing
datasets to build, select and evaluate an prediction model
(step 3, Figure 5). ML algorithms are applied to train mod-
els based on CTI that connects and learns patterns in the
data to derive a working model. Depending on the nature
of the training data, diﬀerent models are built by a data sci-
ence team. Traditionally a set of ML algorithms are applied
to ﬁnd an algorithm suitable for a speciﬁc dataset and user
requirements [1, 2, 40]. To investigate the eﬀectiveness of
prediction models for the validation task, we considered a
set of ML algorithms (e.g., Decision Tree, Naïve Bayes, K-
Nearest Neighbours and Random Forest). The details of the
PoC is discussed in section 5. As most ML algorithms have
a list of hyperparameters, validation techniques (e.g., k-fold
cross-validation, random cross-validation and Bayesian op-
timisation) are incorporated to select hyperparameters4 set-
ting and feature engineering approach for a speciﬁc ML al-
gorithm (step 4, Figure 5).

The built model’s performance is evaluated using the
testing dataset (step 5, Figure 5). Diﬀerent types of per-
formance metrics (e.g., precision, recall, accuracy and F1-
score) are used to choose a model (also known as the optimal
model) that provides the best performance (details in sec-
tion 5.6). In this work, we mainly consider F1-score for eval-
uation, which is a score between 0 and 1. Higher F1-score
indicates better performance of a model. Figure 5 shows
how the pre-processing techniques, feature engineering ap-
proaches and ML algorithms that are used to build threat data

4Hyperparameters are user-deﬁned values that determine details about
the ML classiﬁer before training. For example, a decision tree requires tun-
ing the value of variable depth, and k-nearest neighbours has a variable
number of neighbours.

prediction models are stored for future model building pro-
cess. Once an prediction model is found to have performed
best, that model can be rebuilt using both training and test-
ing datasets. The best models are saved with the evaluation
score (step 6, Figure 5) for using it in the threat data valida-
tion layer.

4.3. Threat Data Validation Layer

We design the threat data validation layer to (i) collect a
SOC’s needs, (ii) automatically orchestrate and request for
CTI and prediction models and (iii) validate alerts. Figure 3
shows the validation layer comprises of an interpreter, or-
chestrator, data processor and predictor. In this layer, secu-
rity teams of SOCs provide their preferences, 𝐴𝑆, as a set
of requirements, where 𝐴𝑆 = <𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏, 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏 >. Security
teams may also provide a minimum threshold for F1-score,
which we refer to as the conﬁdence score of prediction mod-
els. The reason behind gathering conﬁdence score is that
the performance of prediction models will diﬀer with varia-
tion in CTI, alerts and attributes sets. A security team might
need a higher F1-score while dealing with safety critical data
and sensitive information. For example, to identify IP mali-
ciousness, a security team may request for a validation model
with a minimum conﬁdence score of 0.9. While categorizing
comments or text messages as spam, model performance of
0.8 or above may be approved. Selection of F1-score values
vary from application to application. Thus, instead of setting
a ﬁxed value, we consider providing security teams the ﬂex-
ibility to set the conﬁdence score based on their application
needs.

We design Algorithm 1 describing the key steps of the
threat data validation layer. These steps are coordinated and
orchestrated by the orchestrator. SOC preferences (𝐴𝑆 and
conﬁdence score) are the input of Algorithm 1. The inter-
preter receives the SOC requirements and extracts observed
and conﬁdence
, unknown attributes 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
attributes 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
scores from that (line 3). The orchestrator checks model
availability with F1-score above the conﬁdence score for pre-
dicting 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
(line 4). If a model is avail-
able, the attributes are passed to the data processor, where
it pre-processes and transforms the data based on the saved
pre-processing and feature engineering approaches (lines 5-
7). Finaly, the pre-processed and transformed data is sent
to the predictor, which uses the available model to predict
𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏

based on 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏

based on 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏

If a model is unavailable for the requested attributes set,
e.g., for predicting 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
, then the orches-
trator requests the data collector module to gather the rele-
vant CTI data for the preferred attribute sets (line 9). Af-
ter identifying the relevant information, the data collector
module sends the collected CTI data to the model builder to
build an prediction model (lines 12-13). Using the CTI data
from the data collection layer, the model builder follows the
model building process as discussed in section 4.2 (lines 14-
15). After building a model, it sends the model availabil-
ity notiﬁcation to the orchestrator. Then the same process
of data processing and prediction is performed. If the re-

(line 8).

Islam et al.: Preprint submitted to Elsevier

Page 8 of 24

Perform data pre-processingPre-processeddataPerform feature extraction and data transformationSplit DataTransformed DataTrainingDataBuild ModelTraining, tuning hyperparameter and validating Evaluate ModelOptimal ModelSet of Feature Engineering TechniquesSet of Pre-processing TechniquesSet of ML algorithms234TestingDataSet of saved modelSave Model5Evaluated Model6AttributesListUser Preference1Raw DataList of CTI Data SetSmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Algorithm 1 Model building with orchestrator in threat data
validation layer

• RQ1. How eﬀective is machine learning in classifying

CTI for SmartValidator?

model, featureEng = getModel(AS,confidence score)
processedData = transformData(featureEng, AS)
predictedData = predictOutput(model,processedData)

>, confidence score

IsData = CheckData(AS)
if IsData true then

1: Input: AS <𝑜𝑏𝑎𝑡𝑟𝑟𝑖𝑏, 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
2: Output: predictedData
3: Interpret (AS, confidence score)
4: IsModels= CheckModel(AS, confidence score)
5: if IsModels true then
6:
7:
8:
9: else
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: return predictedData

RequestData(AS)
return NotApplicable

go to step 19

else

else

CTIData = RetrieveData(CTI, AS)
model = buildModel(CTIData, AS, confidence score)
if model is built then
go to step 6

quested data is unavailable, a notiﬁcation is sent to a threat
intelligence team and a SOC team to gather the required CTI
and manually analyze the alerts, respectively (lines 18-19).
To ensure that alerts are not ignored when models or CTIs
are unavailable, SOC teams must keep informed that manual
analysis is required.

Our proposed framework, SmartValidator, streamlines
gathering, identiﬁcation and classiﬁcation of CTI. Smart-
Validator allows a security team to swiftly make a response
about incoming alerts. As most information is generated
in a structured way, it can be easily pre-processed to share
through a CTI platform such as MISP or Collective Intel-
ligence Framework (CIF) to beneﬁt diverse security teams.
SmartValidator can be integrated with the existing security
orchestration and automation process to validate alerts and
thus work together with the existing security tools, such as
Security Information and Event Management (SIEM) and
Endpoint Detection and Response (EDR). Microsoft Azure
Sentinel5 and Splunk6 are examples of SIEM where Limachar-
lie7 and Google Chronicle8 are considered as EDR.

5. Experiment Design and Setup

We designed and implemented a Proof of Concept (PoC)
system to evaluate SmartValidator. We expected to demon-
strate eﬀectiveness of prediction models in validating secu-
rity threat data and eﬃciency of building prediction models
based on a SOC’s requirements. The goal of the PoC is to
identify the relevant CTI and build prediction models based
on a list of SOC’s requirements. Hence, we evaluated the
PoC system based on the following two Research Questions
(RQ).

5https://azure.microsoft.com/en-au/services/azure-sentinel/
6https://www.splunk.com/
7https://www.limacharlie.io/
8https://cloud.google.com/blog/products/identity-
security/introducing-chronicle-detect-from-google-cloud

• RQ2. How eﬃcient is SmartValidator in selecting and
building prediction models at runtime over pre-building
all possible prediction models?

To build the PoC system for SmartValidator, we implemented
the core components of Figure 3: the data collection layer
presented in Figure 4, the prediction layer presented in Fig-
ure 5 and an orchestrator for the validation layer described
in section 4.3.

5.1. SOC’s Requirement

We deﬁned a set of attributes (validated attributes and
observed attributes) as SOC’s requirements to carry on the
experiment. These attributes were mainly given by a team
who were diﬀerent than the one that implemented the pre-
diction models. Thus, here we considered the team who pro-
vided the requirement as part of the SOC and the other team
as part of the data science team. This setting gave us the op-
tion to evaluate a variety of diﬀerent SOC’s requirements, to
appropriately assess the PoC system. In a practical scenario,
these requirements would usually be deﬁned by a security
team. Among the various attributes, commonly validated
attributes (𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
) are attack, threat type and threat level.
We considered them as the desired unknown attributes. Be-
side them, we also considered two others attributes name
and event as the desired unknown attributes. Example of
these attributes are shown in Table 11 in appendix A. As
shown in Table 11, an example of an event title (or event)
is “OSINT Leviathan: Espionage actor spear phishes mar-
itime and defence targets”.

As observed attributes (𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏

) vary from security team
to security team, we gathered 18 diﬀerent sets of observed
attributes from security team to validate the aforementioned
. As shown in Table 3, IP information (i.e., ASN, IP
𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
owner, country, and domain), organization, comments about
attributes, comments about attacks, event data, timestamp
and category are the attributes that we considered in the set
of observed attributes. We selected these attributes from
alert data that are also commonly used to validate alerts gen-
erated by diﬀerent IDS. We also used the metadata of at-
tributes such as URL, domain and ﬁlename.

5.2. Collecting CTI

We gathered CTI from two types of sources – publicly
available internet data and data from an OSINT platform,
MISP. CTI gathered from these two sources is considered as
dataset 1 (𝐷𝑆1

) and dataset 2 (𝐷𝑆2

), respectively.

Gathering CTI from websites: We obtained a list of pub-
licly available websites from a GitHub CTI repository [43]
which are shown in Table 1. We selected these websites
because they provided malware RSS feeds and their access
were not restricted (e.g., API limit). We built web crawlers
and scrapers to gather and extract the key pieces of informa-
tion from the selected websites. A parser was built to parse
the information and stored it in a structured format (i.e., a
CSV ﬁle). The gathered data had consistent tagging and

Islam et al.: Preprint submitted to Elsevier

Page 9 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Table 3
List of the observed attribute sets

#

List of attributes

𝑎𝑡𝑡𝑟𝑖𝑏 Date
𝑎𝑡𝑡𝑟𝑖𝑏 Domain
𝑎𝑡𝑡𝑟𝑖𝑏
𝑎𝑡𝑡𝑟𝑖𝑏 Date, Domain
𝑎𝑡𝑡𝑟𝑖𝑏

IP, ASN, Owner, Country

𝑜𝑏1
𝑜𝑏2
𝑜𝑏3
𝑜𝑏4
𝑜𝑏5
𝑜𝑏6
𝑜𝑏7
𝑜𝑏8

𝑜𝑏9

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

IP, ASN, Owner, Country, Domain
IP, ASN, Owner, Country, Date
IP, ASN, Owner, Country, Domain, Date
IP destination, Port, IP source, ASN, Owner,
Country, Domain, File hash, Filename
IP destination, Port, IP source, ASN, Owner,
Country, Domain, Description, Comment, File
hash, Filename
IP destination, Port, IP source, ASN, Owner,
Country, Domain, Description, Comment
IP destination, Port, IP source, ASN, Owner,
Country, Domain, Date, Timestamp, File hash ,
Filename
IP destination, Port, IP source, ASN, Owner,
Country, Domain, Date, Timestamp, Description,
Comment, File hash, Filename
IP destination, Port, IP source, ASN, Owner,
Country, Domain, Date, Timestamp, Description,
Comment
IP destination, Port, IP source, ASN, Owner,
Country, Domain, Date, Timestamp
𝑎𝑡𝑡𝑟𝑖𝑏 Description, Comment, File hash, Filename
𝑎𝑡𝑡𝑟𝑖𝑏 Date, Timestamp, File hash, Filename
𝑎𝑡𝑡𝑟𝑖𝑏 Date, Timestamp, Description, Comment,

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏10

𝑜𝑏11

𝑜𝑏12

𝑜𝑏13

𝑜𝑏14

𝑜𝑏15
𝑜𝑏16
𝑜𝑏17

File hash, ﬁlename

𝑜𝑏18

𝑎𝑡𝑡𝑟𝑖𝑏 Date, Timestamp, Description, Comment

was labelled with the malware used in the attack, for exam-
ple, Zeus, Citadel or Ice IX. 𝐷𝑆1
contained 4060 events and
represented the data available through public CTI feeds from
websites.

Gathering CTI from MISP: We selected the MISP plat-
form as a threat intelligence platform due to its popularity
amongst businesses and the abundance of labelled data. We
ﬁrst gathered the MISP default feeds that were written in
JSON format and then built a parser to extract the key at-
tributes from that. 𝐷𝑆2
contained 213,736 events and repre-
sented the data available to an organisation from a dedicated
threat intelligence platform.

and 𝐷𝑆2

Gathering additional attributes: External information

was gathered utilizing the parsed attributes of both 𝐷𝑆1
For example, the common features amongst each source were
IP, domain and date. Additional attributes were gathered
from WhoIS data (e.g., a database query of the RFC 3912
protocol) for each IP. We used the Python cymruwhois9 mod-
ule to search each IP in the WhoIS database, which returned
the IPs ASN (i.e, a unique global identiﬁer), owner, and
country location. Besides, AlienVault forum was chosen as
an external information source. We scraped the AlienVault

9https://pypi.org/project/cymruwhois/

forum updates using the Python module BeatifulSoup10. The
AlienVault data, which were natural language text descrip-
tions, was searched and extracted for the associated event
and threat.

5.3. Building Data Processor

We built a data processor to clean, pre-process and trans-

form the collected data for building ML-based validators.

Cleaning and pre-processing: We used the Python sci-
kit learn libraries to pre-process the attribute values [10, 29].
We ﬁrst cleaned the data by removing null values and remov-
ing events with missing information. We observed missing
information to be relatively infrequent, resulting in minimal
information loss and a more robust model. For text values,
we found two types of natural language features from the
MISP data (i) text attributes which are short paragraphs that
describe an event in natural language and are often taken
from blogs, and (ii) comments.

We ﬁrst analyzed the text and comments attribute to ﬁnd
a suitable processing and encoding technique. Thus, sim-
ple processing techniques were undertaken to decrease the
dimensionality and remove any uninformative words (e.g.,
articles and prepositions). Each piece of text was stripped of
all non-alphabetical characters, as numbers and special char-
acters can rapidly increase dimensionality and rarely contain
valuable information. The text was then stripped of any non-
noun or non-proper noun tokens, as nouns are the most in-
formative part of the text (e.g., attack names, attack types,
and organisation). Finally, each word was lemmatized (i.e.,
changed to the base form of the word), so that similar words
can be recognized. One of the key steps we followed was
to tokenize the string values of attributes (i.e., domain, ﬁle-
name, hostname, URL) where patterns exit. We removed
punctuation and special characters within a string to clean
the data. We further split the text into small tokens based
on a regular expression that tokenized a string using a given
character. This separated each word within a value (e.g.,
value of domain or URL) and allowed a string to be tok-
enized. For example, we split the value of the URL in terms
of “//” and “.”. The tokenized data were then encoded as
integers to create a numeric form of a feature vector.

.

Feature engineering: We encoded the categorical vari-
ables using one-hot encoding and label encoding. One-hot
encoding considers each categorical value separately and rep-
resents each categorical variable as a column. Label encod-
ing represents each categorical value as a unique integer. Ta-
ble 4 shows an example of one-hot encoding in the ﬁrst table
and label encoding in the second, for three types of attacks:
phishing, DDoS and SQL injection. We used the labelEn-
coder() method of sci-kit learn to convert the string data into
numerical values. An inbuilt function from the sci-kit-learn
library, standardScaler(), was used to standardize the data.
The function transformed data into a normalized distribu-
tion to remove outliers from the data, allowing for building
more accurate prediction models. The text variables (i.e.,
unstructured and structured natural language) did not con-

10https://pypi.org/project/beautifulsoup4/

Islam et al.: Preprint submitted to Elsevier

Page 10 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Table 4
An example of one-hot encoding and label encoding for three
types of attack

Phishing DDoS

0
1
0
1
1
0

1
0
1
0
0
0

SQL
injection
0
0
0
0
0
1

Attack

Phishing

1 DDoS
2
3 DDoS
4
5
6

Phishing
Phishing
SQL
injection

Encoded
attack
2
1
2
1
1
3

form to traditional one-hot or label encoding, as one-hot or
label encoding interprets the text as a whole. Hence, we used
two techniques: count vectorization and TFIDF as our fea-
ture engineering approach to encode text into numerical val-
ues. Count vectorization techniques stored each tokenized
word as a column with its value being the number of times it
appeared in each respective document. Table 5 shows the ex-
amples of count vectorization of two sentences. The TFIDF
vectorizer11 worked similar to the count vectorization, ex-
cept rather than storing counts it stored the TFIDF value of
each word. TFIDF provided a metric for how ‘important’ a
word is within a part of the text by comparing the term’s fre-
quency in a single document to the inverse of its frequency
amongst all documents.

For the one-hot encoding setup, we used a simple count
vectorizer, and for the label encoding setup we used a TFIDF
vectorizer. It should be noted that whilst the dataset included
text variables, the vast majority did not follow a natural lan-
guage convention (e.g., domain or ﬁlename). Hence, more
advanced NLP techniques, such as word embedding, can-
not be accurately applied. The feature engineering schemes
were saved for runtime use with the model building and pre-
diction phase. Appendix B summarizes the pre-processing
techniques that we followed for diﬀerent attributes.

5.4. Building the Validation Model

).

We built prediction models following the traditional ML
pipeline (i.e., selecting ML algorithms, building prediction
models, performing hyperparameter tuning and evaluating
the built model). We designed a model builder to build pre-
diction models for various attribute sets 𝐴𝑆 (𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
and 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
We selected eight commonly used classiﬁcation algorithms [8]:
Decision Tree (DT), Random Forest (RF), K-Nearest Neigh-
bors (KNN), Support Vector Machine (SVM), Multi-Layer
Perceptron (MLP), Ridge Classiﬁer (RID), Naïve Bayes (BAY)
and eXtreme Gradient Boost (XGB) [11] to cover a wide
range of classiﬁer types. Appendix B summarizes the ML al-
gorithsm that we considered to build the PoC system. Bayesian
Optimization was used to automatically tune each model [44].
We used a straightforward train test split for evaluation, with 30%
of the dataset hold out for testing. In a real world, setting
the training data would be selected by the threat intelligence

11https://scikit-learn.org/stable/modules/generated/sklearn.feature_

extraction.text.TﬁdfVectorizer.html

team, to ensure data quality. The built model was optimised
by performing hyperparameter tuning. The Python module
sci-kit learn was used to build the prediction models, as it
is one of the popular and widely used libraries for building
prediction models [10, 29, 40].

5.5. Developing the Orchestrator

.

We designed and implemented a Python script to coordi-
nate the data collector and model builder. The script worked
as an orchestrator that automated the process from gathering
SOC’s requirements to predicting the outputs that is validat-
ing alerts. For example, we took the SOC’s requirements
as an attribute set, < 𝑜𝑏𝑎𝑡𝑟𝑟𝑖𝑏, 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏 > and the conﬁdence
score (a value between 0 and 1). The output of the script
was the value of 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
and F1-score. In this process, the
script ﬁrst checked whether a model was available to predict
. If a model was available, it then called
𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
the data processor to process < 𝑜𝑏𝑎𝑡𝑟𝑟𝑖𝑏, 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏 > and pre-
dict the value of 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏

with 𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏

If a model was not found, it checked the availability of
CTI with attributes < 𝑜𝑏𝑎𝑡𝑟𝑟𝑖𝑏, 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏 >. If CTI was avail-
able, the model builder was invoked and models were built
following the process of model building discussed in pre-
vious section. Here, the orchestrator used the saved feature
engineering approaches and algorithm to train the model and
then selected the model with the best F1-score as the opti-
mal model. The scripts then checked the value of the F1-
score. If the F1-score was lower than the conﬁdence score,
it requested the data science team for building the model
and returned that there is no model available to the security
team. Otherwise, it notiﬁed the orchestrator about model
availability and the next steps of data processing and pre-
dicting 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
was
returned to the security team.

was followed and the value of 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏

If required CTI data was not available, the orchestrator
notiﬁed the security team about CTI unavailability. For ex-
ample, the two CTI data we used did not have any vulnera-
bility description and values. Now if we provided the vul-
nerability description as input and requested that we wanted
to predict the severity, the orchestrator would return no data
available.

5.6. Evaluation Metrics

Evaluation metrics are needed to measure the success of
a prediction model in validating security alerts and building
prediction models on run time. This will determine the ef-
fectiveness and eﬃciency of SmartValidator. Accuracy, pre-
cision, recall and F1-score are the four commonly accepted
evaluation metrics for evaluating a prediction models per-
formance [10, 40]. The correct and incorrect predictions
are further calculated using number of (i)True Positive (TP)
(refers to correct prediction of an attributes label), (ii) False
Positive (FP) (indicates incorrect prediction of an attributes
label), (iii) True Negative (TN) (refers to correct prediction
that a threat does not have a particular label) and (iv) False
Negative (FN) (indicates incorrect prediction that a threat
does not have particular label). For example, if a model clas-
siﬁes a malicious IP address as non-malicious, it is calcu-

Islam et al.: Preprint submitted to Elsevier

Page 11 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Table 5
Count vectors for two sentences, S1: “Fireball is malware” and S2: “Malware is any program
that is harmful”

ﬁreball
1
0

is malware
1
2

1
1

any
0
1

S1
S2

program that

0
1

0
1

harmful
0
1

lated as a false positive. If it refers a malicious IP address as
malicious it is calculated as a true positive. A true negative is
when non-malicious IPs are not classiﬁed as malicious IPs.
A false negative is if a malicious IP is not classiﬁed as such.
Equation 1 to equation 4 provides details of how accuracy,
recall, precision and F1-score are calculated using TP, FP,
TN and FN.

Accuracy =

𝑇 𝑃 + 𝑇 𝑁
𝑇 𝑃 + 𝑇 𝑁 + 𝐹 𝑃 + 𝐹 𝑁

Recall =

𝑇 𝑃
𝑇 𝑃 + 𝐹 𝑁

Precision =

𝑇 𝑃
𝑇 𝑃 + 𝐹 𝑃

F1-score =

2 × (𝑅𝑒𝑐𝑎𝑙𝑙 × 𝑃 𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛)
𝑅𝑒𝑐𝑎𝑙𝑙 + 𝑃 𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛

(1)

(2)

(3)

(4)

We assessed the eﬀectiveness of the prediction models
in validating security alerts with F1-score because accuracy
(equation 1) is not always a useful metric on its own. It does
not capture the bias of the data. The recall (equation 2) is
a measure of robustness; it displays if a model is failing to
predict the relevant samples, e.g., failing to classify IP ma-
liciousness correctly. It is important for the PoC system to
have high recall to ensure that no malicious events are mis-
interpreted or ignored. Precision is the model’s ability to ac-
curately predict the positive class (malicious events), shown
in equation 3. A low value for precision indicates a high
amount of false positives. Thus, it is important to achieve
high precision, as low precision would introduce the need for
human validation of the output of SmartValidator. The F1-
score can be considered the best metric for an overall eval-
uation, as it considers both precision and recall (equation 4)
together and evaluates each class separately. F1-score does
not have any unit as this is the harmonic mean of precision
and recall which do not have any unit as well.

We deﬁned a conﬁdence score between 0-1 to be used
by the security team as a threshold value for prediction mod-
els. In our PoC, we compared the conﬁdence score with the
F1-score of the prediction model. If a model had a lower
F1-score than the conﬁdence score, the PoC discarded that
model.

We deﬁned computation time, as shown in equation 5, to
evaluate the eﬃciency of building prediction models based
on SOC’s need. Computation time is the summation of the
).
) and prediction time (𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑡𝑖𝑚𝑒
training time (𝑡𝑟𝑎𝑖𝑛𝑡𝑖𝑚𝑒

Training time is the time required to build a model and pre-
diction time is the time required to predict unknown attributes
using an optimal model.

𝑐𝑜𝑚𝑝𝑢𝑡𝑎𝑡𝑖𝑜𝑛𝑡𝑖𝑚𝑒 = 𝑡𝑟𝑎𝑖𝑛𝑡𝑖𝑚𝑒 + 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑡𝑖𝑚𝑒

(5)

6. Evaluation and Results

In this section, we present the results of the developed
PoC of SmartValidator to show the eﬀectiveness and eﬃ-
ciency of a dynamic ML-based validator to automate and
assist the validation of security alerts with changing threat
data and SOC’s needs.

6.1. Evaluation of Eﬀectiveness

and 𝐷𝑆2

We evaluated the eﬀectiveness of prediction models to
answer RQ1 which is “How eﬀective are prediction mod-
els in classifying CTI?”. Speciﬁcally, we used two datasets
, as described in Section 5.2, for predicting
𝐷𝑆1
ﬁve observed attributes (𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
). We collected the datasets
in a way so that each dataset was conﬁrmed to have at least
one observed value. Finally, all experiments were conducted
on the collected datasets and diﬀerent combinations of at-
tributes sets. Based on self-deﬁned SOC requirements, CTI
datasets were selected and models were built. Optimal mod-
els were selected based on their eﬀectiveness for a particular
attribute set. Eﬀectiveness is measured using the metrics de-
scribed in Section 5.6. We investigated the performance of
the optimal models for classifying the threat data.

.

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

to 𝑜𝑏7

We found that 51 optimal models were returned by the
PoC system based on the given requirements. Among them,
seven of the models were built using 𝐷𝑆1
to predict attack
and the other 44 models were
that have used 𝑜𝑏1
𝑎𝑡𝑡𝑟𝑖𝑏
to predict the four other unknown attributes
build using 𝐷𝑆2
to 𝑜𝑏18
based on 𝑜𝑏8
𝑎𝑡𝑡𝑟𝑖𝑏

The performance of diﬀerent ML algorithms and encod-
ing methods are summarized in Table 6 for the two observed
attributes sets – 𝑜𝑏7
and 𝑜𝑏14
. The results shows that
XGBoost (XGB) with Label Encoding (LE) achieved a near
perfect F1 score while using 𝐷𝑆2
. We further observed that
while using One Hot Encoding (OHE), K-Nearest Neigh-
bors (KNN) performed better than XGB. However, consid-
ering the time and memory constraints, XGB failed to train
a model to predict “event” when LE was used as an encod-
ing method. Some of the model building processes failed as
they could not ﬁnish within the allocated memory and time
limits that were 24 hours and 10GB for 𝐷𝑆1
and 48 hours
and 100GB for 𝐷𝑆2
. These limits were set and tested to in-
vestigate and simulate computational resource limits.

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

Figure 6 shows the evaluation score (F1-score) for dif-
ferent datasets, labels and encoding methods. Figure 6a and

Islam et al.: Preprint submitted to Elsevier

Page 12 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Table 6
Performance (F1-score) of diﬀerent models for prediction of attack based on 𝑜𝑏7
𝐷𝑆1 and prediction of threat type, threat level, name and event based on 𝑜𝑏14
𝐷𝑆2

𝑎𝑡𝑡𝑟𝑖𝑏 using
𝑎𝑡𝑡𝑟𝑖𝑏 using

Model

DT+LE
RF+LE
KNN+LE
GBAY+LE
RID+LE
SVM+LE
MLP+LE
XGB+LE
DT+OHE
RF+OHE
KNN+OHE
GBAY+OHE
RID+OHE
SVM+OHE
MLP+OHE
XGB+OHE

𝑎𝑡𝑡𝑟𝑖𝑏

𝐷𝑆1 𝑜𝑏7
attack
0.719
0.274
0.594
0.312
0.627
0.401
0.546
0.787
0.587
0.501
0.351
0.382
0.763
0.322
0.762
0.784

threat type
0.941
0.727
0.912
0.15
0.055
0.135
0.762
0.998
0.917
0.916
0.998
0.677
0.121
0.051
0.061
-

𝐷𝑆2𝑜𝑏14
𝑎𝑡𝑡𝑟𝑖𝑏
threat level
0.998
0.76
0.986
0.343
0.082
0.654
-
0.999
0.988
0.948
0.998
-
-
0.126
-
-

name
0.938
0.689
0.917
0.102
0.105
0.295
0.878
0.997
0.905
0.912
0.999
0.465
0.007
0.001
0.079
0.996

event
0.995
0.362
0.902
0.077
0.001
0.283
-
-
0.926
0.87
0.996
0.241
-
-
-
-

and 𝐷𝑆2

Figure 6b shows the comparison of the diﬀerent classiﬁers
when trained on 𝐷𝑆1
, respectively. We ﬁrst ob-
serve that ML algorithms generally performed better using
data, with the exception of the Ridge classiﬁer. This
𝐷𝑆2
ﬁnding demonstrates the importance of CTI data and infor-
mation quality. prediction models require a large number of
training examples to properly learn trends and patterns. We
recommend utilizing data from CTI platforms such as MISP,
as these platforms aggregate a large quantity of veriﬁed in-
formation from a variety of sources.

and 𝐷𝑆2

Figure 6c shows the comparative classiﬁer performance
across both 𝐷𝑆1
. We observe a large range in
classiﬁer performance. The variance in best classiﬁcation
algorithms further motivates the need for automated model
building and selection. Some ML algorithms were not as ef-
fective for this classiﬁcation task. Hence, if a human were to
repetitively use one algorithm to build models for diﬀerent
set of attributes, the results would not be good (i.e., eﬀec-
tive) for all models. It would also be time consuming to val-
idate every model. The results demonstrate on average, the
XGB classiﬁer performed extremely well, but KNN, as well
as other tree-based classiﬁers (DT and RF) also performed
well. MLP classiﬁers also appear to perform well. As MLP
utilized a simplistic artiﬁcial neural network, this potentially
motivates the investigation of more sophisticated deep learn-
ing methods for future work [19].

We further performed comparative analysis for predict-
ing the ﬁve observed attributes in Figure 6d. Models gener-
ally performed well for all prediction tasks, but performed
best when classifying threat_level. This is potentially be-
cause threat_level has the lowest dimensionality of the ob-
served attributes, and hence ample training data for each
class. Correspondingly, the event attribute had lower per-
formance due to its high dimensionality. 𝐷𝑆1
data was used
to predict attack. Performance is noticeably worse for this

Table 7
Optimal classiﬁer and encoding method with evaluation score
(F1-score) for observed attributes 𝑜𝑏1
𝑎𝑡𝑡𝑟𝑖𝑏 to predict
attack using 𝐷𝑆1

𝑎𝑡𝑡𝑟𝑖𝑏 to 𝑜𝑏7

Model

𝑜𝑏1

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏2

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏3

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏4

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏5

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏6

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏7

𝑎𝑡𝑡𝑟𝑖𝑏

attack

Optimal model
KNN + LE

F1-score
0.572

RID + OHE

SVM + OHE

XGB + OHE

RID + OHE

RID + OHE

XGB + LE

0.537

0.623

0.728

0.637

0.772

0.787

attribute, with mean evaluation score of approximately 0.5.
This further emphasizes the importance of CTI quality.

The relative eﬀectiveness of the two encoding strategies
is analyzed and shown in Figure 6e. These two encoding
strategies also provide a comparison of the two NLP tech-
niques that we considered, which are count vectorization and
TFIDF vectorization. One-hot encoding appeared to typi-
cally outperform label encoding, but the results are relatively
similar. This refers to the similar performance of count vec-
torization and TFIDF, where the count vectorizer performed
slightly better, as seen under the one-hot encoding results.
Similar to the previous observation, we found that depend-
ing on the type of attributes and algorithms, the performance
of count vectorizer and TFIDF varies. However, the perfor-
, irrespective of the encod-
mance of classiﬁcation with 𝐷𝑆2
ing, built eﬀective machine learning models [2, 10, 25, 40].
Table 7 shows the optimal model for predicting attack
using 𝑜𝑏1
. Table 8
shows optimal model for predicting threat type, threat level,

that were built using 𝐷𝑆1

to 𝑜𝑏7

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

Islam et al.: Preprint submitted to Elsevier

Page 13 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

(a)

(b)

(c)

(d)

(e)

(f)

Figure 6: Comparative analysis of diﬀerent classiﬁers performance (F1-score) while (a) using dataset 1, 𝐷𝑆1, (b) using dataset
2, 𝐷𝑆2, and (c) using both datasets, (d) predicting diﬀerent labels and (e) using diﬀerent encoding methods. (f) number of
optimal models for diﬀerent evaluation score (F1-score)

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

to 𝑜𝑏14

that were built
name and event based on 𝑜𝑏8
using 𝐷𝑆2
. Table 7 and Table 8 demonstrate that for diﬀer-
ent attribute sets diﬀerent classiﬁers were seen to perform
better. Thus, we cannot rely on a single algorithm. Table
7 shows that the IP extracted features that were in 𝑜𝑏3
𝑎𝑡𝑡𝑟𝑖𝑏
(i.e., IP, ASN, owner, country) performed the best singu-
larly, out of the three available features. 𝑜𝑏5
which had
date and IP features was also actually relatively similar in
terms of predictive capability. However, by itself the IP fea-
tures that is 𝑜𝑏2
can only achieve a F1-Score of 0.623.
Using more available features increased the eﬀectiveness of
the prediction model, with the largest feature set achieving
the best F1-score of 0.787. The addition of the domain fea-
ture to the IP feature set which formed 𝑜𝑏4
did not sig-
niﬁcantly increase the eﬀectiveness of the prediction model.
This is likely due to the existing correlation between the IP
and domain features. However, the date did appear to notice-
ably improve the F1-score. The large diﬀerence between the
best and worst models F1-scores highlights the importance
of proper feature engineering and model selection.

𝑎𝑡𝑡𝑟𝑖𝑏

𝑎𝑡𝑡𝑟𝑖𝑏

Analysing the results of Table 8, we found that the op-
timal models generally performed very well, obtaining ex-
tremely good evaluation scores (F1-score). Several of the
optimal models achieved a near perfect score on the test-
ing dataset. The models also performed noticeably better
than those trained on 𝐷𝑆1
, further showcasing the need for
good features and a large dataset. The models which used
time-based features performed noticeably better than simi-
lar feature sets which did not. The time-based features likely

had such strong predictive power as only a few MISP events
were recorded close together. However, models without the
temporal features were still able to achieve an F1-score of
over 0.8. The ﬁle-based features also appeared to have weak
predictive power as their inclusion or exclusion appeared to
have very little impact on the evaluation score. The results
of Table 8 further reﬂects that the event label was harder to
predict than other three labels. This was likely because event
was the label with the highest number of classes. Interest-
ingly, tree-based classiﬁers seemed to perform better for the
event label. However, it should be noted that a lot of the
more sophisticated models timed-out for the event label, so
their results were not recorded. It can also be seen that the
threat level was the easiest to predict, likely because it had
the lowest number of classes. Figure 6d displays the huge
variance in F1-scores of trained models for diﬀerent labels.
We further analyzed the classiﬁcation conﬁdence of the
models of SmartValidator on the collected data. The eval-
uation results are visualized in Figure 6f. We consider dif-
ferent conﬁdence scores (0.6 – 0.9) as with the variation in
attributes sets, preference of conﬁdence score also varies.
Figure 6f shows that at runtime, with conﬁdence score of
0.8, 80% of the models that were built based on 𝐷𝑆2
ﬁts
the need of a SOC. These models were built based on the
saved feature engineering and ML algorithms. Figure 6f
provides an overview to the security team whether they can
rely on a dataset where the performance is not up to their re-
quirements. It shows with increase in the conﬁdence score
the number of models above the conﬁdence score decreases.

Islam et al.: Preprint submitted to Elsevier

Page 14 of 24

dtrfsvmknnridgbaymlpxgbClassifier based on dataset 100.10.20.30.40.50.60.70.8Evaluation scoredtrfknngbayridsvmmlpxgbClassifier based on dataset 200.20.40.60.81Evaluation scorexgbsvmridrfmlpknngbaydtClassifier based on dataset 1 and dataset 200.20.40.60.81Evaluation scorethreat_levelnamethreat_typeattackeventPredicted label00.20.40.60.81Evaluation scorelabel_DS2label_DS1onehot_DS1onehot_DS2Encoding method00.20.40.60.81Evaluation score0.60.650.70.750.80.850.9Evaluation score5060708090100Optimal model satisying evaluation scoreDS1DS2SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Table 8
Optimal models and encoding methods with evaluation score (f1-score) for attributes sets
𝑜𝑏8

𝑎𝑡𝑡𝑟𝑖𝑏 to 𝑜𝑏18
threat type

𝑎𝑡𝑡𝑟𝑖𝑏 to predict threat type, threat level, name and event
name
Optimal model

Optimal model

threat level

Observed
attributes

𝑜𝑏8

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏9

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏10

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏11

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏12

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏13

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏14

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏15

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏16

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏17

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏18

𝑎𝑡𝑡𝑟𝑖𝑏

Optimal model

MLP + OHE

SVM + OHE

SVM + OHE

KNN + OHE

KNN + OHE

KNN + OHE

KNN + OHE

SVM + OHE

XGB + LE

XGB + LE

XGB + LE

F1-
score
0.648

0.854

0.859

0.998

0.997

0.998

0.998

0.873

0.998

0.997

0.998

XGB + LE

XGB + LE

SVM + OHE

XGB + LE

KNN + OHE

XGB + LE

XGB + LE

SVM + OHE

F1-
score
0.68

0.84

0.915

0.999

0.999

0.999

0.999

0.862

RID + OHE

SVM + OHE

SVM + OHE

KNN + OHE

KNN + OHE

KNN + OHE

KNN + OHE

SVM + OHE

XGB + LE

1

KNN + OHE

XGB + LE

0.999

KNN + OHE

XGB + LE

1

KNN + OHE

F1-
score
0.593

0.809

0.864

0.999

0.998

0.998

0.999

0.813

0.998

0.999

0.998

event
Optimal model

DT + LE

SVM + OHE

SVM + OHE

DT + LE

DT + LE

KNN + OHE

KNN + OHE

SVM + OHE

DT + LE

RF + OHE

DT + LE

F1-
score
0.275

0.735

0.763

0.898

0.994

0.995

0.999

0.725

0.996

0.865

0.996

. Thus, with 𝐷𝑆2

Obviously, it can be seen that the models on 𝐷𝑆2
classiﬁed
the attributes with a much higher conﬁdence. We found that
one of the key reasons behind this is that 𝐷𝑆1
had compar-
atively less data elements than 𝐷𝑆2
cap-
turing the variation in data and correlating them provided
better results than 𝐷𝑆1
. The results show that approximately
84% of the 51 optimal models had an F1-score (or conﬁdence
score) above 0.72 and 75% of the models had F1-score above
0.8. Most of the models that were built with data gathered
from CTI platforms can eﬀectively predict 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
based on
with a higher F1-score than the models that were built
𝑜𝑏𝑎𝑡𝑡𝑟𝑖𝑏
with CTI gathered from public websites.

In summary, the MISP dataset (i.e., 𝐷𝑆2

) was found to
be a high-quality dataset that worked well with automated
classiﬁcation and thus validation of alerts. Hence, using at-
tributes that are representative of possible threat data, pre-
diction models can be built to eﬀectively validate alerts with
a substantial degree of accuracy, precision and recall. The
reﬂect that ML based validation models can
results of 𝐷𝑆2
be used to eﬀectively validate the alerts with high quality
CTI like 𝐷𝑆2
. Model choice and alternatives are seen to be
important steps to ﬁnd the optimal models, as for diﬀerent
attributes sets the PoC has returned diﬀerent models.

6.2. Evaluation of Eﬃciency

To demonstrate the eﬃciency of SmartValidator, we an-
swer RQ2 that is “How eﬃcient is SmartValidator in se-
lecting and building prediction models on runtime over pre-
building prediction models?”.

In SmartValidator, we propose to build the models at run
time based on SOC’s requirements instead of pre-building
all possible models. We considered the time to build pos-
sible model combinations of the eight aforementioned clas-
siﬁer algorithms as a baseline to compare the eﬃciency of
SmartValidator. We observed that it was infeasible to pre-
build models for every possible combination of features. For
, there were 62 possible feature combinations, and for
𝐷𝑆1

there were 8190. This would increase the number of
𝐷𝑆2
experiments for 𝐷𝑆2
to 524,160. For predicting the ﬁve un-
known attributes with 18 attribute sets we would only re-
quire to run 1440 experiments; only 0.26% of the total ex-
periments. Thus, for calculating the eﬃciency, that is the
computation time, based on SOC’s requirement the PoC ran
a total of 816 experiments; 112 experiments for DS1 (8 ML
algorithms × 7 input attribute sets × 1 output attribute × 2
encoding methods) and 704 experiments for DS2 (8 ML al-
gorithms × 11 input attribute sets × 4 output attributes ×
2 encoding methods) to test all combinations of the 18 ob-
served attribute sets, prediction models, encoding methods
and ﬁve unknown attributes (i.e., classiﬁcation labels).

We considered the total time as a baseline to evaluate
the eﬃciency of building the model at runtime. Here we
attempted to simulate the resource limitations of model con-
struction for a real-world environment. We considered 𝐷𝑆1
as a lightweight dataset and assigned the restrictions of 24
was a
hours runtime and 10GB of memory, whereas 𝐷𝑆2
heavyweight dataset and assigned a 48 hours runtime limit
and 100GB of memory. Any experiment that exceeded this
run time or consumed too much memory would be aborted,
as it was deemed impractical due to organisations’ strict re-
sources and fast response requirements [25, 45]. In this re-
sult section, we reported the eﬃciency in terms of time.

, all 112 experiments completed successfully.
For 𝐷𝑆1
However, for 𝐷𝑆2
, 169 of the 704 experiments failed to ﬁn-
ish whilst enforcing our experimental setup. The most com-
mon classiﬁer model to time out were MLP and XGB, as
these models had a signiﬁcantly larger training time. For
these failed jobs, 149 out of 169 jobs were either for the event
or threat level label, as these datasets had many more valid
entries, and thus also took more time and memory to train.
Similarly, 116 of the failed jobs used one-hot encoding, as
this encoding method was much less eﬃcient than label en-
coding, due to every possible value adding a dimension to

Islam et al.: Preprint submitted to Elsevier

Page 15 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Table 9
Training time and prediction time of optimal models in seconds
for attribute sets 𝑜𝑏1
𝑎𝑡𝑡𝑟𝑖𝑏 to predict attack using 𝐷𝑆1

𝑎𝑡𝑡𝑟𝑖𝑏 to 𝑜𝑏7

Model

𝑜𝑏1

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏2

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏3

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏4

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏5

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏6

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏7

𝑎𝑡𝑡𝑟𝑖𝑏

attack

Optimal model Train time
KNN + LE

758

Predict time
0.676

RID + OHE

SVM + OHE

XGB + OHE

RID + OHE

RID + OHE

0.90

482

2597

5.6

2.2

XGB + LE

1422.9

0.002

0.001

0.196

0.001

0.001

0.09

the encoded input. However, 53 of the label encoding exper-
iments also failed due to the size of the text features. These
features were encoded with very high dimensionality due to
the lack of a natural language convention. To investigate
this issue further in our future work, we plan to investigate
eﬃcient encoding methods through vocabulary size and di-
mensionality reduction.

and 𝐷𝑆2

and 𝐷𝑆2

(0.09 seconds), and 2.9% for 𝐷𝑆2

Table 9 and Table 10 show the training time and pre-
diction time of the optimal models that were built based on
respectively. The experimental results show
𝐷𝑆1
that it is extremely ineﬃcient to pre-build a large number
of prediction models. For 𝐷𝑆1
the total training time was
the total training time
61064 seconds (0.7 days), and for 𝐷𝑆2
was 7010279 seconds (81.1 days). The prediction time of a
model was signiﬁcantly faster than the training time, which
further encouraged the use of validation models. On aver-
age, models made predictions in 0.6% of the training time
(17.29 seconds).
for 𝐷𝑆1
Figure 7 shows the logarithmic distribution of the train-
ing time for diﬀerent datasets, labels and encoding methods.
, 𝐷𝑆2
The logarithmic distribution of training times for 𝐷𝑆1
and both 𝐷𝑆1
is shown in Figure 7a, Figure 7b
and Figure 7c, respectively. It should be noted that Figure
7 did not consider the run time of experiments which were
timed out. As shown in Figure 7, the run times for more in-
tensive models are skewed to the left. Noticeably, the Naïve
Bayes (GBAY) classiﬁers were trained near instantaneously,
as these models did not require heavy ﬁtting to the data.
DT similarly was trained faster for both datasets, due to the
simplicity of this model. Figure 7a shows for 𝐷𝑆1
, SVM,
KNN and MLP required an average of 10-15 minutes to train.
However, the XGB classiﬁer took signiﬁcantly the longest
time to train with a median value of 36 minutes. For 𝐷𝑆2
,
the average overall training time was 217 minutes (shown
in Figure 7b) which was signiﬁcantly larger than the aver-
age overall training time of 9 minutes for 𝐷𝑆1
, due to the
substantial dataset size increase. However, XGB had a sig-
niﬁcantly larger training time of over 8 hours. We observed
that the runtime between RID and SVM was quite diﬀerent
even though both are linear classiﬁers (Figure 7c). The SVM
classiﬁer took an average of 30 minutes to train on 𝐷𝑆2
due
to hyperparameter optimization, whereas the RID classiﬁer

took an average of 10 seconds as they did not require any
signiﬁcant hyperparameters. These observations highlight
the importance of SOC requirements, as we can see a trade-
oﬀ between model performance and training time. XGB is
the best performing model on average, but also exhibits the
largest training time. Hence, SOC analysts would need to
weigh model eﬀectiveness against eﬃciency.

Figure 7d displays the training time for completed ex-
periments for each predicted attribute. Models targeted to-
wards predicting the attack attribute only took an average
training time of 9 minutes, as they were trained using the
dataset in comparison to the other at-
much smaller 𝐷𝑆1
tributes that were trained using 𝐷𝑆2
. Models took an aver-
age of 2 hours to train for the name attribute, in comparison
to threat_level, threat_type and event, which took a mean
time of around 4.5 hours. This could be because the train-
ing set was much smaller for the name attribute, as not as
many CTI entries were assigned such information. Simi-
larly, it should be noted that a large portion of the event and
threat_level experiments timed out, as the training set was
larger for these attributes, due to more valid entries.

Figure 7e reﬂects that the training time did not signiﬁ-
cantly diﬀer between encoding methods. This is because the
major encoding dimensionality came from the domain and
ﬁlename features, which were treated as text attributes and
were thus only one-hot encoded in our experiments. How-
ever, one-hot encoding usually exhibited larger training times,
as it has much higher dimensionality and is thus less eﬃ-
, one-hot experiments took an extra 4.6 min-
cient. For 𝐷𝑆1
utes on average (11.3 minutes vs 6.8 minutes). For 𝐷𝑆2
,
one-hot experiments took an extra 18.9 minutes on average
(227.7 minutes vs 208.8 minutes).

prediction models were built by experts (i.e., data sci-
entists) who have the knowledge of ML technologies and
pipeline to automatically validate the alerts. We observed
that for changing SOC requirements, interaction or collabo-
ration were required among the security team and the data
science team where a security team speciﬁed the require-
ments and requested for the models they need. If the data
required by the data science team were not available, they
needed to request the threat intelligence team who gathered
the requested information and updated the relevant list of in-
formation. Hence, the model required redesigning and fur-
ther actions needed to be performed to achieve the best pre-
diction models. Whilst using the PoC based on SmartVadila-
tor, these interaction could be minimized, by managing the
interaction through the orchestrator. In this way, the orches-
trator requested the model builder to build the required vali-
dation model. Constructing the models automatically based
on a SOC’s needs required less time and was more feasible
than constructing possible model for all combination of at-
tribute sets.

Evaluating the eﬃciency of SmartValidator, we found
that SmartValidator successfully identiﬁed and classiﬁed threat
data required for alert validation. The same framework can
be used to automate the validation of newly listed alerts, with
new data sources. The data science team requires to map the

Islam et al.: Preprint submitted to Elsevier

Page 16 of 24

Observed
attributes

𝑜𝑏8

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏9

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏10

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏11

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏12

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏13

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏14

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏15

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏16

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏17

𝑎𝑡𝑡𝑟𝑖𝑏

𝑜𝑏18

𝑎𝑡𝑡𝑟𝑖𝑏

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Table 10
Training time and prediction time of optimal models in seconds that were built to predict
threat type, threat level, name and event based on attributes 𝑜𝑏8
𝑎𝑡𝑡𝑟𝑖𝑏 using 𝐷𝑆2

𝑎𝑡𝑡𝑟𝑖𝑏 to 𝑜𝑏18

threat type

Optimal
model
MLP+OHE 166623

Train
time

Predict
time
0.18

Optimal
model
XGB+LE

threat level
train
Time
111788

Predict
time
56.86

Optimal
model
RID+OHE

name
train
time
10959

SVM+OHE 1141.1

0.031

XGB+LE

144473

139.2

SVM+OHE 925.37

event

Predict
time
0.036

Optimal
model
DT+LE

Train
time
289.87

Predict
time
0.199

0.016

0.014

SVM+OHE 37846

0.686

SVM+OHE 27004

0.657

SVM+OHE 1199.66

0.024

SVM+OHE 804.92

KNN+OHE 2599.72

23.1

XGB+LE

19190

KNN+OHE 2592.79

22.11

KNN+OHE 16871

KNN+OHE 3177.3

32.04

XGB+LE

KNN+OHE 2535.45

22.09

XGB+LE

27067

16036

SVM+OHE 1081.05

0.015

SVM+OHE 739.58

0.006

3.115

176.6

4.296

2.556

0.004

SVM+OHE 994.43

KNN+OHE 984.94

5.98

DT+LE

371.38

0.204

KNN+OHE 2795.24

14.56

DT+LE

534.94

0.197

KNN+OHE 1178.39

9.311

KNN+OHE 18065

259.95

KNN+OHE 951.54

6.713

KNN+OHE 16731

224.98

SVM+OHE 1019.56

0.007

SVM+OHE 21275

0.369

XGB+LE

10552.3

21.38

XGB+LE

4033.701 1.913

KNN+OHE 899.12

5.679

DT+LE

102.35

0.213

XGB+LE

19602.7

19.32

XGB+LE

13950.9

3.938

KNN+OHE 1005.59

6.64

RF+OHE

7036.7

15.78

XGB+LE

14257.7

18.57

XGB+LE

8569.79

3.175

KNN+OHE 949.95

5.611

DT+LE

142.76

0.22

(a)

(b)

(c)

(d)

(e)

Figure 7: Comparative analysis of time in seconds required to train diﬀerent ML based validation models with (a) dataset 1, (b)
dataset 2, (c) both datasets, (d) diﬀerent labels and (e) encoding methods

suitable algorithm with suitable attributes sets and deﬁne the
required data sources.

6.3. Discussion

We consider the same attributes sets and CTI sources
that are used for security incident and alert validation for
three validation approaches. The validation approaches are
(i) manual validation, (ii) pre-building prediction models and
(iii) automatic construction of prediction models based on
SOC’s requirements

In manual validation, for each attribute or set of attributes,
a security team ﬁrst searched for attribute types and then
looked for the relevant CTI availability. A security team
used their previous experience to select the CTI to perform
the validation. For example, to validate a malicious IP, a se-
curity team collected the blacklisted IP addresses and then
looked for the IPs on the list. They further used WhoIS
database to identify the relevant information about suspected
IPs. The security team needed to manually write queries or
call APIs to ﬁnd and extract information for CTI and re-

Islam et al.: Preprint submitted to Elsevier

Page 17 of 24

dtrfsvmknnridgbaymlpxgbClassifier based on dataset 1-4-202468Log of training timedtrfknngbayridsvmmlpxgbClassifier based on dataset 2-4-2024681012Log of training timexgbsvmridrfmlpknngbaydtClassifier based on dataset 1 and dataset 2-4-2024681012Log of training timethreat_levelnamethreat_typeattackeventPredicted label-4-2024681012Log of training timelabel_DS2label_DS1onehot_DS1onehot_DS2Encoding method-6-4-202468101214Log of training timeSmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

quired knowledge about the underlying CTI sources. For
similar types of alerts or changing context (i.e., change in
CTI, alerts or SOC’s requirement), the same sequence of
actions were repeated, that cost signiﬁcant man-hours and
required knowledge about the underlying plugin, API, CTI
sources and so on.

For changing context while following approach 2 (pre-
building prediction models), the security team needed to re-
quest the data science team to train and build the possible
prediction models for the new context. Section 6.2 reveals
that to build prediction models each time a change occurs
is not feasible. With automatic construction of prediction
models, each time a SOC requested for a validation task, the
models were built automatically. With changing context, the
orchestrator coordinated the data collection and model build-
ing process that also freed the security team from coordinat-
ing and communicating with the data science team and also
reduced the delay incurred due to communication. In this
work, we have experimentally evaluated the performance of
SmartValidator. We did not discuss the amount of time re-
quired by the security team to perform the validation activi-
ties or the time required for communication between the se-
curity team and data science team. This will also include the
time gap between a request being made and the time for the
security team to get the model. In future work, we plan to
evaluate SmartValidator in a real SOC environment to fur-
ther demonstrate how SmartValidator can be beneﬁcial in a
SOC environment. For example, the eﬀort required to man-
age the PoC system versus the cost saving from automation.
Further we want to evaluate the maximum upfront cost to
incorporate the PoC system in a real SOC environment.

For automating construction of ML-based validation mod-

els, the PoC followed three major steps - i) collecting and
processing the data, ii) training the classiﬁer and iii) run-
ning the prediction model. Step 1 required a large amount
of time (in hours) as there were hundreds of thousands of
data points to download and process, step 2 took a reason-
able amount of time (in minutes) as the data were needed
to be encoded and the choice of classiﬁer were needed to
train and optimise its hyperparameters. Step 3 was reason-
ably fast as it only needed time (in seconds) to apply a pre-
trained model. These steps were bundled into an installable
python package which could be made publicly available. We
designed the PoC in a modular fashion so that it can be inte-
grated into other network-enabled services to gain more in-
formation about network security. The system could easily
be built for the future improvements.

To validate alerts coming from an IDS, the developed
PoC system can be extended to ﬁrst receive the IDS alerts
over a network. After parsing the alert attributes (which
would be similar to the attributes sets used in our PoC), the
next steps are to transform the alert attributes into features,
and then look for potential CTI to correlate the alerts infor-
mation into patterns by building prediction models to pre-
dict suspicious behavior. Furthermore, the alerts can be val-
idated using the models and the validated output can display
security context of the network in a graphical user interface

that is easy to understand. The PoC system can be enhanced
to provide API that can be integrated as a part of a SOC’s
existing security system, such as middleware for EDR or
SIEM.

In our experiment, we have selected two types of CTI –
one is gathered from the websites, and another is from an
OSINT platform MISP, which is widely used by industry as
it contains high-quality data with enriched IOCs. We con-
sider the conﬁdence score of users to ensure that the models
with low evaluation scores are not selected. We also merged
multiple data sources to enrich CTI. The experimental re-
sults show that while using MISP, SmartValidator performs
better than when using web data. We assert that this is due to
the high quality of MISP. The lower level of CTIs used in our
experiment can be replaced with higher quality CTIs such
as TTP, where SmartValidator will perform the same steps
from identifying CTIs to building models and performing
the validation. The prediction models can be enriched with
advanced IOC and TTP with more details about the threats.
The key steps identiﬁed to improve the models automat-
ically built through SmartValidator are eﬀective feature en-
coding, hyperparameter optimization, data distribution, fea-
ture extraction, dimensionality reduction and classiﬁer se-
lection. Increasing the size of the dataset and number of fea-
tures increases the F1-score. The dimensionality of the cat-
egorical variables needs to be decreased. It is worth noting
that our investigation is by no means exhaustive; we adopt
basic ML principles and NLP techniques to develop a sim-
plistic PoC. We plan to investigate more ML techniques such
as data balancing, normalization and feature selection with
diverse types of CTI. Similarly, we intend to investigate more
sophisticated NLP techniques, such as word embeddings that
can capture semantic information of the natural language at-
tributes.

We found that CTI datasets contain highly multi-variate
categorical variables. Highly dimensional problems like this
are likely to be linearly separable, as we can separate any
d+1 points in a d-dimensional space with a linear classiﬁer,
regardless of how the points are labelled. We further found
that overall ensemble classiﬁers such as XGB performed bet-
ter than the other selected seven algorithms.

As shown in Figure 3, we consider that dedicated ex-
pertise is required (e.g., a data scientist) to build prediction
models, which in most cases is diﬀerent from the SOC team
using the models for validation task. The prediction mod-
els are built by experts (e.g., data scientists, ML experts, or
developers) who are knowledgeable about ML libraries, fea-
ture engineering and algorithms. Considering a SOC’s se-
curity team capability, the model building process can also
be replaced with Automated Machine Learning (AutoML)12
framework such as Google Cloud AutoML13. AutoML frame-
works are designed to provide ML as a service, where a secu-
rity team is required to provide the pre-processed data and
for some cases the transformed data. It considers multiple
ML algorithms in a pipeline to evaluate the performance,

12https://www.automl.org/automl/
13https://cloud.google.com/automl

Islam et al.: Preprint submitted to Elsevier

Page 18 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

perform hyperparameter tuning and validation in an attempt
to improve the performance. An AutoML framework pro-
vides a list of optimal models. For example, TPOTClas-
siﬁer14 is an automated ML classiﬁer that is developed in
an attempt to automate the ML pipeline in python. It ex-
plores prediction models conﬁgurations that a data analyst or
security analyst may not consider and attempts to ﬁne-tune
the model to achieve the most optimized model. Hence, the
model builder of our proposed SmartValidator can be devel-
oped following the process discussed in Figure 5 or using
an AutoML framework. Thus, depending on an organiza-
tions SOC capabilities they may use an AutoML framework
instead of building the prediction models with assistance of
data scientist.

SOC teams overwhelmed with massive volume of alerts
failed to respond to a security incident even they had the
alerts and corresponding information in their CTIs15. Hence,
we assert that the manual and repetitive validation task can
be automated through SmartValidator, whereas more critical
or unknown alerts and incidents would still require human
involvement. In future work, we plan to extend the PoC to
provide more explainable output so that SOC can make deci-
sions based on the validated output, where prediction model
choice and alternatives would be captured with explanation.

6.4. Limitation of SmartValidator

𝑎𝑡𝑡𝑟𝑖𝑏

The experimental results show the eﬀectiveness of Smart-
Validator. However, we observed several cases in which
SmartValidator is unable to perform the validation. Further-
more using CTI to validate alerts for unknown known, un-
known or zero-day attacks might not always be practical. To
empirically investigate this, we ran an experiment separately
to explore the ability of SmartValidator to detect unknown
alerts. For our experiment, we used the alert data of Snort
and selected IP information that is 𝑜𝑏3
to predict threat
level. To validate such information, we used SmartValida-
tor to build a model trained exclusively with the MISP data
(𝐷𝑆2
), so that the Snort alert data is almost entirely unseen.
The Snort alert data contained 16317 distant IPs, for which
only 78 of the IPs were seen in the our MISP training dataset.
The prediction model achieved an F1-score of 0.307 which
implies SmartValidator has some capability for prediction of
unknown alerts, albeit limited. Even though the alerts and
IPs are unseen, the prediction model was still able to detect
some patterns inferred from the ASN, IP owner and country
attributes. We assert that due to the intelligent nature and the
ability to learn the underlying semantic patterns, the mod-
els built with SmartValidator have the potential to validate
some unseen values of unknown attributes (i.e., unknown
attributes for which the model is built depending on the ob-
served attributes). If the alert data is entirely unseen, i.e., the
IP, ASN, owner and country are all uncontained in the train-
ing data, then SmartValidator will predict an output based
on the most common value in the training data.

To validate any unknown attributes (i.e., 𝑢𝑛𝑎𝑡𝑡𝑟𝑖𝑏
14http://epistasislab.github.io/tpot/api/
15

https://www.trendmicro.com/explore/en_gb_soc-research

) with

speciﬁc values, SmartValidator always needs the observed
attributes as input to build a model. Therefore, even if the
given value of unknown attributes is not seen in the train-
ing data, it is still possible to make a correct prediction by
learning the patterns from the model building phase. For ex-
ample, malicious IPs can have the same domain and threat
actors. Here, an IP that has not been seen before can be
identiﬁed as malicious, observing the threat actors and the
domain. However, it is always possible that a trained model
can fail to correctly predict IPs, which is a limitation of our
proposed approach. We observered there are the cases when
the observed attributes are not representative for capturing
and learning patterns about the unknown attributes. Smart-
Validator will not be applicable to automate the validation
tasks in these scenarios. Quantiﬁable investigation of this
limitation is out of the scope of this work; but it is an excit-
ing area of exploration for future research.

CTI is time-sensitive. Hence, SOC teams will need to
update the model whenever a new CTI is available. The
framework can further be extended to capture the timeliness
of the CTI used for building the models and keep track of
the models with up-to-date CTI. However, all the CTI will
not be updated simultaneously; thus, changing all the models
whenever there is an update in the CTI will not be feasible.
SmartValidator can be extended to handle this situation by
retraining the model when new requests come and retraining
and evaluating the available model built based on old CTI.
There can be bad quality of CTI sources which may af-
fect the performance of SmartValidator. For example, it is
possible that the models infer wrong patterns with bad qual-
ity data and consider legitimate or benign IPs as malicious.
There is a need of empirical studies that focus on ensuring
the quality of CTIs. However, this is not within the scope of
this study.

6.5. Threats to validity

Construct Validity: Our choice of data for our evalua-
tion setup may not be suitable. We have considered CTI data
to also be representative of alert attributes, as CTI is often
generated from existing security alerts from external orga-
nizations. However, the classiﬁers have not yet been tested
thoroughly with real-world internal business data. This eval-
uation will be attempted in future work.

Internal Validity: A potential concern is that our mod-
els are not properly optimised. The hyperparameter tuning
was performed for a speciﬁc set of conﬁgurations, as testing
hyperparameters with all possible combination would take
a large amount of time that may not justify here. Moreover,
knowing all the combination of hyperparameters is quite im-
possible. Similarly, the features that we have selected to train
our models are non-exhaustive. The attribute sets selected
were chosen based on the attributes used for validating alerts
through security orchestration. The purpose was to show
that SmartValidator can automate the construction of predic-
tion models by identifying the CTI and the constructed pre-
diction models can eﬀectively validate alerts based on SOC’s
requirement. The attributes list might not reﬂect a complete

Islam et al.: Preprint submitted to Elsevier

Page 19 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

list of attributes for validating certain alerts, but our system
can be easily extended to several other attack scenarios.

External Validity: Our experiments may not generalize
to other datasets. The built classiﬁers and prediction models
were evaluated based on simulated alerts attributes sets and
publicly available CTIs such as MISP.

7. Related Work

Research trends are seen in the use of Machine Learn-
ing (ML) and Deep Learning (DL) in cybersecurity domain
for detection and classiﬁcation of cyberattacks. Most of the
existing literature focused on using AI techniques such as
NLP, ML and DL tools and techniques to identify and detect
cyber attacks such as malware, network intrusion, data ex-
ploitation and vulnerabilities [1, 2, 19, 21, 23, 40, 30]. ML
algorithms are used to extract knowledge from open source
public repositories which are later used to analyze attacks,
or validate alerts. Although automation has been achieved
in detecting and analysis of attacks, validation of alerts and
incidents still requires SOC’s involvement [25].

CTI is used by security experts of SOCs to analyze and
validate alerts. To ease the use of CTI, researchers have
been trying to come up with a uniﬁed structured for sharing
CTI [31, 46]. STIX [5], TAXII [12], CyBox [6] and UCO
[31] are popular among them. Use of Artiﬁcial Intelligence
(AI) is encouraged for identifying, gathering and extracting
CTI objects [20, 36, 50]. Various AI tools and techniques
are used for knowledge extraction, representation and ana-
lytics of CTI [7, 46]. For example, Zahedi et al. [54] has
applied topic modelling techniques such as LDA to ﬁnd the
security relevant topics from open source repositories such
as GitHub. Another example is a system used by EY [17]
who mined previous threat data and then analyze it to give
information on threats. Using this information, they can re-
spond to attacks and continuously monitor a system. They
place data collectors at points of high movement in a net-
work, like a server, where a system can continuously ana-
lyze data and keep the system safe. Attacks detected can be
used to harvest IOCs and analyzed to discover security is-
sues within the network. Recorded Future (widely known
CTI service providers [38]) also elaborated on the fact that
threat data can be found in a large variety of places such as
Tweets, Facebook posts and emails. They also use AI to rec-
ognize patterns in email so that phishing emails can be found
based on the information of the sender or ﬁle attached.

Recent advances in CTI domain have drawn attention to
the use of the existing knowledge to automate the manual
analysis of human experts and enrichment of quality CTIs
[4, 15, 34, 56]. For example, vulnerability description of
NVD like databases are being used to predict the severity,
conﬁdentiality and availability of threats. Le et al.
[28]
have used NLP and traditional ML algorithms to perform
automated vulnerability assessment using vulnerability de-
scription of open source public repositories. Noor et al. [34]
have used data provided by STIX and Mitre corporation to
identify the documents related to attacks. Azevedo et al.
have proposed a platform Pure to improve quality of CTI in

the form of enriched IoCs by automatically correlating and
aggregating the IoCs [4]. They have evaluated the perfor-
mance of the proposed platform with 34 OSINT feeds gath-
ered from MISP.

One recent study by Recorded Future has laid out four
ways of using AI techniques to extract CTI from a detected
attack [50]. They have deﬁned risk score metrics to identify
malicious network activity. This extends the classiﬁcation
from being just about whether an attack has occurred and
provides more in-depth information on the threat [39, 50].
Recorded Future has used NLP to increase the range of pos-
sible data sources by removing the limit on just structured in-
formation [50]. They utilize extracted text and perform clas-
siﬁcation for the language, topic and company. They have
applied ML and NLP techniques to rank documents to iden-
tify malware data attacks. Their model also considers dif-
ferent classiﬁcations needed like scoring a risk value. They
do not always use ML for scoring a risk value as they often
have a rule-based system for the classiﬁer to follow.

Unlike the above-mentioned work, we propose Smart-
Validator to utilize NLP and ML techniques to assist in au-
tomating validation of security alerts and incident. To the
best of our knowledge, this is the ﬁrst attempt to use CTI,
such as MISP data, to automate the classiﬁcation and vali-
dation of security threat data based on a SOC’s preferences.
Here, we have investigated how eﬀective ML algorithms are
while classifying CTI to assist in alert validation. Unlike the
existing works, where possible prediction models are pre-
built, here we propose to build the models on demand. We
have demonstrated the eﬃciency of constructing prediction
models dynamically.

8. Conclusion

Many organizations are facing diﬃculty to keep pace with
the changing threat landscape as security experts need to
identify and analyze threat data in most circumstances. With-
out the indulgence of automation techniques, it is impos-
sible to reduce the burden of analyzing the CTI to make a
timely decision. In this work, we propose a novel framework
SmartValidator, to build an eﬀective and eﬃcient validation
tool using CTI that automates the validation of the security
alerts and incidents based on SOC’s preferences. Diﬀerent
from the manual approaches, SmartValidator is designed in a
way so that SOCs can add their requirements without worry-
ing about collecting CTI and using CTI to build a validation
model. SmartValidator consists of three layers: threat data
collection, threat data prediction model building and threat
data validation. Diﬀerent teams are responsible for updat-
ing the components of diﬀerent layers, thus freeing secu-
rity teams from learning data processing and model building
techniques. The validation task is designed as a classiﬁca-
tion problem that leverages existing NLP and ML techniques
to extract features from CTI and learn patterns for alert val-
idation. We developed a Proof of Concept (PoC) system to
automatically extract features from CTI and build prediction
models based on the preferences of SOCs. A SOC’s prefer-
ences are collected as a set of attributes sets: observed and

Islam et al.: Preprint submitted to Elsevier

Page 20 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

unknown attributes, where the task of the PoC is to predict
unknown attributes based on observed attributes.

We have demonstrated the eﬀectiveness of SmartValida-
tor by predicting attack, events, threat type, threat level and
name. It collected and processed data from public websites
and MISP. Next, CTI with preferred attributes sets were se-
lected to build prediction models. Eight ML algorithms were
ran to build and select the models with the highest F1-score.
The best model was used to predict the unknown attributes
and thus validate alerts. The developed PoC constructed val-
idation models, and can be used to validate alerts generated
by the threat detection tools and ﬁnd the missing informa-
tion to store the data in a structured format. The results
show prediction models are eﬀective in validating security
alerts. Building prediction models at run time are more ef-
ﬁcient then building prediction models for all possible at-
tributes sets and CTI.

In future work, we plan to extend the PoC system to re-
duce the amount of data that is sent to the SIEM tool, thus
reducing the cost of data analysis. The system can also be
extended to reduce the organizational dependence on hu-
man expertise to take actions against security threats such as
blocking ports or identifying maliciousness of an incident.
The proposed framework can assist an organization’s secu-
rity team to focus on decision making, rather than manually
extracting and validating security alerts and incidents. The
framework can also be leveraged to provide the beneﬁt to
choose CTI suitable for an organization’s application rather
than using generalized CTI.

Acknowledgements
This work is supported by Cyber Security Cooperative

Research Centre (CSCRC), Australia.

References
[1] Ahmed, M., Naser Mahmood, A., Hu, J., 2016. A survey of network
anomaly detection techniques. Journal of Network and Computer Ap-
plications 60, 19 – 31.

[2] Ahmim, A., Ferrag, M.A., Maglaras, L., Derdour, M., Janicke, H.,
Drivas, G., 2020. Taxonomy of supervised machine learning for in-
trusion detection systems, in: Kavoura, A., Kefallonitis, E., Theodor-
idis, P. (Eds.), Strategic Innovative Marketing and Tourism, Springer
International Publishing, Cham. pp. 619–628.

[3] Anstee, D., 2017. The great threat intelligence debate. Computer

Fraud & Security 2017, 14–16.

[4] Azevedo, R., Medeiros, I., Bessani, A., 2019. Pure: Generating qual-
ity threat intelligence by clustering and correlating osint, in: 2019
18th IEEE International Conference On Trust, Security And Privacy
In Computing And Communications/13th IEEE International Confer-
ence On Big Data Science And Engineering (TrustCom/BigDataSE),
IEEE. pp. 483–490.

[5] Barnum, S., 2012. Standardizing cyber threat intelligence informa-
tion with the structured threat information expression (stix). Mitre
Corporation 11, 1–22.

[6] Barnum, S., Martin, R., Worrell, B., Kirillov, I., 2012. The cybox

language speciﬁcation. draft, The MITRE Corporation .

[7] Brazhuk, A., 2019. Semantic model of attacks and vulnerabilities
based on capec and cwe dictionaries. International Journal of Open
Information Technologies 7, 38–41.

[8] Caruana, R., Niculescu-Mizil, A., 2006. An empirical comparison of
supervised learning algorithms, in: Proceedings of the 23rd interna-
tional conference on Machine learning, pp. 161–168.

[9] Cavalancia, N., 2020. What is a managed soc and how does it

work?
managed-soc-explained.

https://cybersecurity.att.com/blogs/security-essentials/

[10] Chen, H., Le, T.H.M., Babar, M.A., 2020. Deep learning for source
code modeling and generation: Models, applications and challenges.
ACM Computing Surveys (CSUR) .

[11] Chen, T., Guestrin, C., 2016. Xgboost: A scalable tree boosting sys-
tem, in: Proceedings of the 22nd acm sigkdd international conference
on knowledge discovery and data mining, pp. 785–794.

[12] Connolly, J., Davidson, M., Schmidt, C., 2014. The trusted automated
exchange of indicator information (taxii). The MITRE Corporation ,
1–20.

[13] Data, W., 2019. Whois data. https://www.whoisxmlapi.com/.
[14] Domain, M., 2021.

Malware domain list.

https://www.

malwaredomainlist.com/mdl.php.

[15] Edwards, M., Larson, R., Green, B., Rashid, A., Baron, A., 2017.
Panning for gold: Automatically analysing online social engineering
attack surfaces. Computers & Security 69, 18–34.

[16] Elmellas, J., 2016. Knowledge is power: the evolution of threat intel-

ligence. Computer Fraud & Security 2016, 5–9.

[17] EY, 2020.

Cybersecurity compromise diagnostic: Hunting for

evidence of cyber attackers.

https://www.ey.com/Publication/

vwLUAssets/ey-cybersecurity-compromise-diagnostic/\protect\T1\
textdollarFile/ey-cyber-compromise-diagnostic.pdf.

[18] Faiella, M., Granadillo, G.G., Medeiros, I., Azevedo, R., Zarzosa,
S.G., 2019. Enriching threat intelligence platforms capabilities., in:
ICETE (2), pp. 37–48.

[19] Ferrag, M.A., Maglaras, L., Moschoyiannis, S., Janicke, H., 2020.
Deep learning for cyber security intrusion detection: Approaches,
datasets, and comparative study. Journal of Information Security and
Applications 50, 102419.

[20] Future, R., 2019. Recorded future. https://www.recordedfuture.com/.
[21] Gamage, S., Samarabandu, J., 2020. Deep learning methods in net-
work intrusion detection: A survey and an objective comparison.
Journal of Network and Computer Applications 169, 102767.
[22] Gao, Y., Li, X., Li, J., Gao, Y., Guo, N., 2018. Graph mining-based
trust evaluation mechanism with multidimensional features for large-
scale heterogeneous threat intelligence, in: 2018 IEEE International
Conference on Big Data (Big Data), IEEE. pp. 1272–1277.

[23] Gibert, D., Mateu, C., Planes, J., 2020. The rise of machine learning
for detection and classiﬁcation of malware: Research developments,
trends and challenges. Journal of Network and Computer Applications
153, 102526.

[24] Ibrahim, A., Thiruvady, D., Schneider, J., Abdelrazek, M., 2020.
The challenges of leveraging threat intelligence to stop data breaches.
Front. Comput. Sci. 2: 36. doi: 10.3389/fcomp .

[25] Islam, C., Babar, M.A., Nepal, S., 2019. A multi-vocal review of
security orchestration. ACM Computing Surveys (CSUR) 52, 1–45.
[26] Johnson, C., Badger, M., Waltermire, D., Snyder, J., Skorupka, C.,
2016. Guide to cyber threat information sharing. Technical Report.
National Institute of Standards and Technology.

[27] Koyama, T., Hu, B., Nagafuchi, Y., Shioji, E., Takahashi, K., 2015.
Security orchestration with a global threat intelligence platform. NTT
Technical Review 13.

[28] Le, T.H.M., Sabir, B., Babar, M.A., 2019. Automated software vul-
nerability assessment with concept drift, in: 2019 IEEE/ACM 16th
International Conference on Mining Software Repositories (MSR),
IEEE. pp. 371–382.

[29] scikit

learn, 2021. Machine learning in python.

https://

scikit-learn.org/stable/.

[30] Lin, G., Wen, S., Han, Q.L., Zhang, J., Xiang, Y., 2020. Software
vulnerability detection using deep neural networks: A survey. Pro-
ceedings of the IEEE 108, 1825–1848.

[31] Menges, F., Sperl, C., Pernul, G., 2019. Unifying cyber threat intel-
ligence, in: International Conference on Trust and Privacy in Digital
Business, Springer. pp. 161–175.

[32] MISP, 2021. Misp - open source threat intelligence platform & open
standards for threat information sharing. https://www.misp-project.

Islam et al.: Preprint submitted to Elsevier

Page 21 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

org/.

[33] Mittal, S., Joshi, A., Finin, T., 2019. Cyber-all-intel: An ai for security
related threat intelligence. arXiv preprint arXiv:1905.02895 .
[34] Noor, U., Anwar, Z., Malik, A.W., Khan, S., Saleem, S., 2019. A
machine learning framework for investigating data breaches based on
semantic analysis of adversary’s attack patterns in threat intelligence
repositories. Future Generation Computer Systems 95, 467–487.

[35] OpenPhish, 2019. Openphish. https://openphish.com.
[36] Qamar, S., Anwar, Z., Rahman, M.A., Al-Shaer, E., Chu, B.T., 2017.
Data-driven analytics for cyber-threat intelligence and information
sharing. Computers & Security 67, 35–58.

[37] Ramsdale, A., Shiaeles, S., Kolokotronis, N., 2020. A comparative
analysis of cyber-threat intelligence sources, formats and languages.
Electronics 9. doi:10.3390/electronics9050824.

[38] RFID, 2021. What is threat intelligence? deﬁnition and examples.

www.recordedfuture.com/threat-intelligence-definition/.
Practical
Machine

[39] RFteam,
tions
machine-learning-cybersecurity-applications/.

cybersecurity.

learning:

2018.

for

https://www.recordedfuture.com/

applica-

[40] Sabir, B., Ullah, F., Babar, M.A., Gaire, R., 2020. Machine learning
for detecting data exﬁltration. arXiv preprint arXiv:2012.09344 .
[41] Serketzis, N., Katos, V., Ilioudis, C., Baltatzis, D., Pangalos, G., 2019.
Actionable threat intelligence for digital forensics readiness. Informa-
tion and Computer Security 27, 273–291.

[42] Siemplify, 2019. Top Security Playbooks for 2019 - SOC Investiga-
tion and Response – Driven by Context . White Paper. Siemplify.
[43] Slatman, H., 2021. Awesome threat intelligence. https://github.com/

hslatman/awesome-threat-intelligence.

[44] Snoek, J., Larochelle, H., Adams, R.P., 2012. Practical bayesian opti-
mization of machine learning algorithms, in: Proceedings of the 25th
International Conference on Neural Information Processing Systems-
Volume 2, pp. 2951–2959.

[45] Sonicwall, 2021. 2020 sonicwall cyber threat report. https://www.

sonicwall.com/resources/2020-cyber-threat-report-pdf/.

[46] Tounsi, W., Rais, H., 2018. A survey on technical threat intelligence
in the age of sophisticated cyber attacks. Computers & security 72,
212–233.

[47] Tracker, C..C., 2019a. Command and control tracker.

https://

ransomwaretracker.abuse.ch/.

[48] Tracker, F., 2019b. F tracker. https://ransomwaretracker.abuse.ch/.
[49] Tracker, R., 2019c. Ransomware tracker. https://ransomwaretracker.

abuse.ch/.

[50] Truve, S., 2017. T4 ways machine learning is powering smarter threat

intelligence.
machine-learning.pdf.

https://go.recordedfuture.com/hubfs/white-papers/

[51] Wagner, C., Dulaunoy, A., Wagener, G., Iklody, A., 2016. Misp:
The design and implementation of a collaborative threat intelligence
sharing platform, in: Proceedings of the 2016 ACM on Workshop
on Information Sharing and Collaborative Security, Association for
Computing Machinery, New York, NY, USA. p. 49–56.

[52] Ward, L., 2017. Building an eﬀective threat intelligence platform that
would make einstein proud. Computer Fraud & Security 2017, 11–12.
[53] Winkler, I., Gomes, A.T., 2017. Chapter 12 - what is threat intel-
ligence?, in: Winkler, I., Gomes, A.T. (Eds.), Advanced Persistent
Security. Syngress, pp. 143 – 150.

[54] Zahedi, M., Ali Babar, M., Treude, C., 2018. An empirical study of
security issues posted in open source projects, in: Proceedings of the
51st Hawaii International Conference on System Sciences, pp. 5504–
5513.

[55] ZeusTracker, 2019. Zeus tracker. https://zeustracker.abuse.ch/.
[56] Zhou, Y., Wang, P., 2019. An ensemble learning approach for xss at-
tack detection with domain knowledge and threat intelligence. Com-
puters & Security 82, 261–269.

A. Appendix: Example of MISP Attribute

Table 11 and Table 12 show the key attributes of MISP

and the percentage of each attribute used in this study.

B. Apppendix: Experiment details
B.1. Data pre-processing

We conducted the pre-processing using Python ‘spaCy’
.
module. Following are the details of pre-processing of 𝐷𝑆2
• The event’s tools/malware and threat actor are extracted
from misp-galaxy event tags. Tags that are labelled
with threat-actor are treated as threat actors. Other-
wise, it is treated as a tool or malware.

• The port is extracted from any IP attribute that con-

tains a port.

• The IP address of any hostname or domain attribute is
searched using the Python socket module and included
as an additional IP attribute if found.

• The attribute timestamp is rounded to the nearest hour

to increase multiplicity.

• The natural language attributes (text, and comment)
are processed. Any non-alpha character is removed,
the text is converted to lowercase, and any non-noun
word is removed using the Python Spacy module.

• The labels of attack type description are slightly cleaned
for the purposes of grouping and readability: each de-
scription is stripped down to a base word so that en-
tries describing the same attack type are grouped.

• Non-reoccurring labels are grouped into ‘other’.

B.2. ML Algorithms

Following we describes the algorithms considered as the

most common and eﬀective ML classiﬁer.

Decision Tree (DT): The decision tree classiﬁer splits
the data into a series of branching nodes that end in leaves.
The nodes represent a rule, e.g., a data point is red, and the
leaves are a classiﬁcation. Trees are fast to learn and thus
applied to a very wide range of problems.

Random Forest (RF): The random forest classiﬁer uses
a multitude of decision trees to create a ‘forest’. Certain trees
can predict certain classes more strongly than others, so the
forest picks which trees to use.

K-Nearest Neighbours (KNN): K-nearest neighbours is
another very simple but eﬀective classiﬁer. The classiﬁca-
tion of the data points is made based on the K most sim-
ilar instances (or neighbours) of the data point. To do that
searching is made on the entire training set to ﬁnd the k most
similar instances.

Support Vector Machine (SVM): Support vector ma-
chine separates n features into an n-dimensional space, and
attempts to identify the optimal hyperplane between them.
The points surrounding the border of a feature are called sup-
port vectors which deﬁne the hyperplane. SVM is consid-
ered one of the most powerful ‘out-of-the-box’ classiﬁers.

Multi-Layer Perceptron (MLP): Multi-Layer Percep-
tron models use a feed-forward Artiﬁcial Neural Network
(ANN). It has three layers of nodes: the input layer, a hidden
layer and an output layer. Each node is considered as a neu-
ron that uses a nonlinear activation function. Even though
MLP is considered as a simplistic deep learning model, the
computation of MLP is quite expensive.

Islam et al.: Preprint submitted to Elsevier

Page 22 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Table 11
MISP attributes and their description used in the PoC

Name
event

Description
The title of the MISP event. A short
natural language description of event.

Extraction
The MISP ‘Event’ feature.

threat level

threat type

name

date
timestamp

ip dst

ip src

A number between 1 to 3 assigned to
indicate the threat level with 3 being the
highest and 0 being undeﬁned.
The type of threat the MISP event is
about. E.g. malware, exploit-kit, tool,
threat-actor.
The name of the threat that the MISP
event is about
The reference date of the MISP event.
The creation time of the individual MISP
attribute. Expressed in Unix time.
The destination IP of the IOC. The vic-
tim’s IP.
The source IP of the IOC. The attacker’s
IP.

port

The port on which IOC was recorded.

domain

The domain of the IOC.

ﬁle hash

The encrypted value of a ﬁle based IOC.

The MISP ‘Threat Level’ feature.

The ‘misp-galaxy’ tag type. E.g.
‘misp-
galaxy:tool=KHRAT’. Otherwise the ‘clas-
siﬁcation’ tag value.
The value of the ‘misp-galaxy’ tag.

The MISP ‘date’ feature.
The MISP ‘timestamp’ feature.

The MISP ‘value’ feature if the MISP ‘type’
feature equals ‘ip-dst’ or ‘ip-port’.
The MISP ‘value’ feature if the MISP ‘type’
feature equals ‘ip-src’. Also taken from IP
address lookup of domain feature (below).
The MISP ‘value’ feature if the MISP ‘type’
feature equals ‘ip-port’.
The MISP ‘value’ feature if the MISP ‘type’
feature equals ‘domain’, ‘hostname’ or ‘url’.
Also extracted from a domain name lookup
of the ip-src feature.
The MISP ‘value’ feature if the MISP ‘type’
feature equals ‘sha1’, ‘sha256’ or ‘md5’.

ﬁlename

The ﬁlename of the IOC.

description

A natural
MISP event.

language description of the

The MISP ‘value’ feature if the MISP ‘type’
feature equals ‘ﬁlename’.
The MISP ‘value’ feature if the MISP ‘type’
feature equals ‘comment’ or ‘text’

comment

A short natural language comment pro-
viding some context to the IOC.

The MISP ‘comment’ feature.

Example
Trojanized Adobe Installer
used to Install DragonOK,
New Custom Backdoor
1

malware

KHRAT

2017-03-29
1490818721

23.229.221.200

208.91.197.46

40

cookie.inter-ctrip.com

ﬀc0ebad7c1888cc4
a3f5cd86a5942014
b9e15a833e57561
4cd01a0bb6f5de2e
Byebye.dll

Since January of this year
... ... ... which pertained to
Cambodia’s country code.
Sample malicious URL
hosting location

Table 12
Most common attributes of the MISP OSINT feed as of Jan.
2019

Attribute
Hostname
md5
Domain
sha256
ip-dst
sha1
link
url
ﬁlename | sha256
ip-src
ﬁle
text
comment

Number Percentage
41135
29085
25382
22525
16582
15726
13908
10036
8840
7417
5324
905
371

19.2
13.6
11.9
10.5
7.8
7.4
6.5
4.7
4.1
3.5
2.5
0.4
0.2

Ridge Classiﬁer (RID): Ridge regression is a regression
model that aims to alleviate the problems of multi-collinearity
and overﬁtting that other regression models may have. Multi-
collinearity is the existence of near-linear relationships amongst
the independent variables that can distort the results.

Naïve Bayes (BAY): Naïve Bayes classiﬁers use the Bayes
theorem to predict the probability of a value to be in a class.

Various distribution and statistical features are used with the
classiﬁer to build the model.

Chadni Islam is a postdoctoral re-
searcher in CREST-Centre for Re-
search on Engineering Software Tech-
nologies, School of Computer Sci-
ence,University of Adelaide.
She
completed her PhD degree in 2020
under the supervision of Professor M.
Ali Babar from the same school. She
was co-supervised by Dr.
Surya Nepal from CSIRO’s
Data61. Her research expertise falls at the intersection of
Cyber security and Software Engineering. Her PhD research
was focused on providing architecture support for security
orchestration and automation systems using advanced soft-
ware engineering technologies. She is interested in leverag-
ing existing software engineering,analytical reasoning, natu-
ral language processing and machine learning tools and tech-
niques to develop an intelligence self-adaptive security or-
chestration and automation platform.

M. Ali Babar is a Professor in the
School of Computer Science, Uni-
versity of Adelaide. He is an hon-

Islam et al.: Preprint submitted to Elsevier

Page 23 of 24

SmartValidator: A Framework for Automatic Identiﬁcation and Classiﬁcation of Cyber Threat Data

Industrial Control System Cyber Security Research (ICS-
CSR) and contributed over 150 peer reviewed articles and
conference papers to the ﬁeld that resulted from his collab-
orative research with industry partners such as Airbus, BT,
Deloitte, Rolls-Royce, QinetiQ, and General-Dynamics.

orary visiting professor at the Soft-
ware Institute, Nanjing University,
China. He has authored/co-authored
more than 230 peer-reviewed papers
in premier Software Technology jour-
nals and conferences. With an H-Index 48, the level of cita-
tions to his publications is among the leading Software En-
gineering researchers in Aus/NZ. At the University of Ade-
laide, Professor Babar has established an interdisciplinary
research centre, CREST-Centre for Research on Engineer-
ing Software Technologies, where he leads the research and
research training of more than 30 members. He has been
involved in attracting several millions of dollar worth of re-
search resources over the last ten years. Prof Babar leads the
University of Adelaide’s participation in the Cyber Security
Cooperative Research Centre (CSCRC), one of the largest
Cyber Security initiative of the Australasian region. Within
the CSCRC, he leads the theme on Platform and Architec-
ture for Cyber Security as a Service. Further details can
be found on:https://researchers.adelaide.edu.au/profile/
ali.babar#my-research.

Roland Croft is currently a PhD stu-
dent at the University of Adelaide,
where he also completed his Bache-
lor’s degree. He graduated with ﬁrst
class honours and was awarded the
University Medal for achieving the
highest academic score in his year.
During his undergraduate study he
completed several research projects
in the ﬁeld of cyber security. His research interests lie in
natural language processing, cyber security, machine learn-
ing and data mining. His research aims to utilize open source
security data and intelligence to create tools and frameworks
for automated vulnerability analytic and prediction.

Helge Janicke is the Research Direc-
tor of the Cyber Security Cooperative
Research Centre, Australia. He is af-
ﬁliated with Edith Cowan University
and holds a visiting Professorship in
Cyber Security at De Montfort Uni-
versity, UK. Prof. Janicke’s research
interests are in the area of cyber security, in particular with
applications in critical infrastructures using cyber-physical
systems, SCADA and Industrial Control Systems. Prof. Jan-
icke’s current research investigates the application of Agile
Techniques to Cyber Incident Response in Critical Infras-
tructure, Managing Human Errors that lead to Cyber Inci-
dents, and research on Cyber warfare & Cyber peacekeep-
ing. Prof. Janicke established DMU’s Cyber Technology
Institute and its Airbus Centre of Excellence in SCADA Cy-
ber Security and Forensics Research. He has been the Head
of School of Computer Science at De Montfort University
UK, before taking up his current position as Research Di-
rector for the Cyber Security Cooperative Research Cen-
tre. Prof. Janicke founded the International Symposium on

Islam et al.: Preprint submitted to Elsevier

Page 24 of 24

