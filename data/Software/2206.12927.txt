Noname manuscript No.
(will be inserted by the editor)

An Empirical Study on Bug Severity Estimation
Using Source Code Metrics and Static Analysis

Ehsan Mashhadi · Shaiful Chowdhury ·
Somayeh Modaberi · Hadi Hemmati ·
Gias Uddin

Received: date / Accepted: date

Abstract In the past couple of decades, signiﬁcant research eﬀorts are de-
voted to the prediction of software bugs (i.e., defects). These works leverage a
diverse set of metrics, tools, and techniques to predict which classes, methods,
lines, or commits are buggy. However, most existing work in this domain treats
all bugs the same, which is not the case in practice. The more severe the bugs
the higher their consequences. Therefore, it is important for a defect predic-
tion method to estimate the severity of the identiﬁed bugs, so that the higher
severity ones get immediate attention. In this paper, we provide a quantitative
and qualitative study on two popular datasets (Defects4J and Bugs.jar), using
10 common source code metrics, and also two popular static analysis tools
(SpotBugs and Infer) for analyzing their capability in predicting defects and
their severity. We studied 3,358 buggy methods with diﬀerent severity labels

Ehsan Mashhadi
Schulich School of Engineering
University of Calgary, Calgary, Canada
E-mail: ehsan.mashhadi@ucalgary.ca

Shaiful Chowdhury
Schulich School of Engineering
University of Calgary, Calgary, Canada
E-mail: shaiful.chowdhury@ucalgary.ca

Somayeh Modaberi
Schulich School of Engineering
University of Calgary, Calgary, Canada
E-mail: somayeh.modaberi@ucalgary.ca

Hadi Hemmati
Schulich School of Engineering
University of Calgary, Calgary, Canada
E-mail: hadi.hemmati@ucalgary.ca

Gias Uddin
Schulich School of Engineering
University of Calgary, Calgary, Canada
E-mail: gias.uddin@ucalgary.ca

2
2
0
2

n
u
J

6
2

]
E
S
.
s
c
[

1
v
7
2
9
2
1
.
6
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Ehsan Mashhadi et al.

from 19 Java open-source projects. Results show that although code metrics
are powerful in predicting the buggy code (Lines of the Code, Maintainable
Index, FanOut, and Eﬀort metrics are the best), they cannot estimate the
severity level of the bugs. In addition, we observed that static analysis tools
have weak performance in both predicting bugs (F1 score range of 3.1%-7.1%)
and their severity label (F1 score under 2%). We also manually studied the
characteristics of the severe bugs to identify possible reasons behind the weak
performance of code metrics and static analysis tools in estimating the sever-
ity. Also, our categorization shows that Security bugs have high severity in
most cases while Edge/Boundary faults have low severity. Finally, we show
that code metrics and static analysis methods can be complementary in terms
of estimating bug severity.

Keywords Bug Severity · Defect Prediction · Code Complexity Metrics ·
Static Analysis Tools · Defects4J · Bugs.jar

1 Introduction

Software maintenance is one of the most challenging and expensive parts in the
software development life cycle [44]. Handling bugs (including detecting, lo-
calizing, ﬁxing, etc.) is the most typical challenge associated with the software
maintenance step [19,17]. Consequently, both practitioners and researchers are
trying to make this tedious task as automated as possible from diﬀerent as-
pects such as defect prediction, test generation, fault-localization, and program
repair [49,89,100,113,59].

While there has been much research in handling bugs using diﬀerent tech-
niques like search-based [53], pattern-based [55] and ML-based techniques [59],
there is little research that focuses explicitly on the severe bugs. In other words,
most of the research implicitly assumes that all bugs have the same importance
[86,59,110,74]. However, the bug severity indicates the intensity of the impact
the bug has on system operation [67]. Critical bugs may cause a system to
crash completely or cause non-recoverable conditions such as data loss. High-
severity bugs aﬀect major system components that prevent users from working
with some parts of the system. Fixing severe bugs are typically more challeng-
ing compared to the medium or low severity bugs, where few components are
aﬀected and there is an easy workaround [103]. Therefore, in practice, bugs
with higher severity tend to be ﬁxed sooner than other less severe bugs [102,
45,82].

In general, software practitioners have several means to detect bugs. These
can range from QA practices such as code review [50, 56, 14], and inspection to
diﬀerent testing approaches, and even development methodologies such as pair
programming [109,66,95], and Test Driven Development (TDD) [18,57,10,78].
When a bug is detected, it is common for the development teams to consider
the most severe/important bugs ﬁrst which helps them to prevent extreme

Title Suppressed Due to Excessive Length

3

consequences. Some issue tracking systems, such as Jira1, have a speciﬁc ﬁeld
named Severity/Priority which is assigned during the bug reporting, and helps
the development teams to consider bug importance during debugging.

It is important to note that bug severity does not have a standard formal
deﬁnition. The deﬁnition depends on the context of the software, the nature of
the bug, the current state of the project, the ratio of aﬀected users, potential
harm to users, and many other factors. The current state of the practice to
identify and record the severity of bugs is through a manual process in issue
tracking systems, where the severity of a bug has its own ﬁeld (with options
such as Blocker, Critical, Major, Minor, and Trivial or sometimes with num-
bers ranging from zero to 20). The ﬁeld is manually populated by the person
who documents the bug, however, it may change by the technical team during
the bug reporting review process. Thus, the whole process is not only quite
subjective but is also an error-prone task.

Therefore, it seems that an approach that can estimate the severity of a
given bug, automatically and systematically (not biased to the opinion of the
bug reporter), can be quite helpful not only for the developers and QA teams
but also as an input for other automated software tools, such as program repair,
to prioritize their process, accordingly. However, the current literature in this
domain is limited to predicting the severity based on the bug report description
using NLP techniques [21,97], which still has the bias issue (subjective to
reporter’s opinion) and ignores the context (underlying source code). Also,
their applicability is limited to projects with rich and informative bug reports.
Therefore, in this paper, we would like to empirically investigate the potential
of source code for estimating the severity of a bug.

To be able to estimate bug severity, we study two well-known categories of
approaches that have a track record of predicting issues in the code: (a) source
code metrics and (b) static analysis.

Code metrics have shown a great deal of success in predicting code smell

[101], maintenance eﬀort [76] and defects [28, 26].

Static analysis tools such as SpotBugs [6], the successor to the popular
FindBugs [2] tool, Facebook Infer [3], and Google Error Prone [1] have also
been successfully used in related research [32, 99]. These tools use diﬀerent
techniques such as AST-based patterns or data-ﬂow analysis to ﬁnd a bug’s
existence and to predict the bug’s type and severity. These tools are updated
regularly to cover modern bug patterns and they require the compiled code to
perform the analysis which takes longer than the source code metric calculation
(e.g, several hours compared to several minutes) which makes it less practical
for big projects.

To the best of our knowledge, there is no study that explores code metrics
and modern static analysis tools’ ability in estimating bug severity. Our ap-
proach focuses on method-level granularity in contrast to class/module-level,
since researchers and developers ﬁnd the class/module level granularity too
coarse-grained for practical usage [88,73,31, 36].

1 https://www.atlassian.com/software/jira

4

Ehsan Mashhadi et al.

We use two popular datasets: Defects4J [43], and Bugs.jar [83] which con-
tain real bugs from diﬀerent open-source Java projects. We studied 19 projects
containing 1,668 bugs (3,358 buggy methods) for our quantitative study. Fur-
thermore, we studied 60 randomly sampled bugs from both datasets for our
qualitative study to ﬁnd the bug characteristic according to their severity la-
bels.

To guide our study, we target answering the following research questions

(RQ):

– RQ1: Are source code metrics good indicators of buggyness and bug sever-
ity? The results show that most of the code metrics (e.g, Lines of Code,
McCabe, McClue, Nested Block Depth, Proxy Indentation, FanOut, Read-
ability, Diﬃculty, and Eﬀort) are good indicators of buggyness, but they
perform quite poorly for predicting bug severity. Line of Code has the best
performance in ﬁnding buggyness, but the least performance in ﬁnding bug
severity. However, the Diﬃculty and Eﬀort metrics are the best indicators
of bug severity.

– RQ2: What is the capability of static analysis tools in ﬁnding bugs and
their severity? The results show that the studied static analysis tools (Spot-
Bugs and Infer) are not yet powerful enough to ﬁnd many bug types and
in many cases, they miss or mislabel the bug severity.

– RQ3: What are the characteristics of bugs with diﬀerent severity values?
Results reveal that the severity of bugs is mostly related to the software’s
speciﬁcation, which is not predictable solely based on method-granularity
level analysis. Also, we found no direct relationship between method com-
plexity and its severity value. Many low severity bugs exist in the quite
complex methods according to the code metrics, but these functions han-
dle trivial functionalities, such as GUI, or they do not lead to a crash or
unauthorized access.

The ﬁndings of this paper help researchers and practitioners to better un-
derstand the characteristics of severe bugs and guide future research/tools on
how to advance the ﬁeld to better predict bug severity.

The remainder of the paper is organized as follows: We provide our dataset,
and experiments setup in Section 2. Experiment motivation, design and results
are discussed in Section 3. Discussions about the results are provided in Section
4. Threats to validity of our work are described in Section 5. Related works
to this paper are described in Section 6. We conclude this paper with possible
future work in Section 7.

2 Study Setup

In this section, we describe our datasets, the preprocessing steps, studied code
metrics, and static analysis tools. The overall approach of our data gathering,
preprocessing, and their relation to each RQ is shown in Fig 1 where Step1
shows the process of extracting labels from diﬀerent issue management systems

Title Suppressed Due to Excessive Length

5

Fig. 1: Overview of our empirical study

for each dataset. Step2 depicts the preprocessing method and Step3 provides
the details of how the data is used for each RQ.

2.1 Dataset and Preprocessing

We use Defect4J [43] and Bugs.jar [83] datasets for our study which contain
bugs from diﬀerent popular Java projects. Defect4J is used because it has been
widely used in diﬀerent automated software engineering research domains such
as Test Generation [86], Program Repair [58], and Fault Localization [74].
Also, this is a generic dataset containing real bugs from diﬀerent projects with

6

Ehsan Mashhadi et al.

diﬀerent domains. Bugs.jar has also been widely used in Program Repair [84]
and contains many real bugs and also there is a corresponding bug report for
each sample in Jira2 containing the bug severity.

Table 1: Unifying the raw severity labels of Defects4J dataset. For example,
the third row means that bugs having raw severity labels of Medium or 5 are
considered as Medium bugs.

Uniﬁed Severity Label (USL) Raw Severity Label (RSL)

Critical
High
Medium
Low

Critical
High, Major, 3
Medium, 5
Low, Trivial, Minor, 7, 8, 9

2.1.1 Defects4J

The latest version (2.0.0) of Defects4J contains 835 bugs from 17 Java projects.
The dataset does not have any information regarding the bug severity itself, so
we extract bug severity values from diﬀerent issue tracking systems. There are
three software hosting services used for these projects: Jira, SourceForge, and
Google Code Archive. After extracting the severity values from these systems,
we concluded with 510 bugs out of 835 bugs containing severity labels.

After extracting the raw severity labels (RSL), we found that there are
many inconsistencies between these raw severity labels of the Defects4J dataset.
Some projects use numerical (smaller values indicate higher severity while
larger values indicate lower severity) and others use categorical values because
of diﬀerent issue tracking systems. Therefore, we uniﬁed similar raw labels
to produce four meaningful uniﬁed severity labels (USL): “Critical”, “High”,
“Medium”, and “Low” severity labels. Table 1 shows the mapping between all
RSL and the USL labels of the Defects4J dataset.

2.1.2 Bugs.jar

The Bugs.jar dataset contains 1,158 bugs from eight large, popular, and di-
verse open-source Java projects. All of these projects use Jira as an issue
management system, and each of the bugs has an assigned severity level. The
severity labels from this dataset are “Blocker”, “Critical”, “Major”, “Minor”,
and “Trivial”. Since the RSLs of this dataset are consistent, there is no need
for a unifying process like what we did for the Defects4J dataset, and we con-
sidered all the RSLs as USLs

2 https://www.atlassian.com/software/jira

Title Suppressed Due to Excessive Length

7

After mentioned preprocessing step we concluded with two datasets con-
taining the USLs (RSLs are discarded and not used anymore). The Defect4J
dataset contains “Critical”, “High”, “Medium”, and “Low”. Bugs.jar dataset
contains “Blocker”, “Critical”, “Major”, “Minor”, and “Trivial”. This is our
extracted and preprocessed dataset, but we will group some of these USLs in
diﬀerent RQs according to their experiment design requirements, which will
be explained in their related sections accordingly.

2.1.3 Buggy vs. Not Buggy

Since we focus on the method-level granularity, we consider a method as a
buggy method if it is modiﬁed/removed by a bug ﬁxing patch. If a method,
however, is introduced with a bug ﬁx patch, we do not label this method as
buggy. We discard the static initialization blocks and constructors since those
are special types of methods that are used for initializing the enclosed class,
mostly.

A bug ﬁxing patch, however, can impact multiple methods, in that case,
we consider all of them as buggy. This has been a common practice in earlier
studies (e.g., [22,73,65]). We discuss the relevant threats in Section 5. Finally,
from the 510 bugs of the Defects4J dataset, we found 742 buggy methods and
from the 1,158 bugs of the Bugs.jar dataset, we found 2616 buggy methods.

Similar to earlier studies (e.g., [22,73, 65]), we considered a method as non-
buggy if it was not modiﬁed in the current bug ﬁxing patch inside the buggy
class. In this way, we are extracting methods that are contributing to the
same functionality as the buggy methods since methods of a class have high
cohesion.

We extracted 20,179 and 57,197 non-buggy methods from the Defects4J
and Bugs.jar, respectively. The list of the buggy and non-buggy methods con-
taining their project name, class name, start line, end line, and bug severity
(corresponding bug severity for non-buggy methods) is available in our pub-
licly shared GitHub repository 3

The severity distributions of the buggy methods for Defects4J and Bugs.jar
datasets are shown in Fig 2. We study these two datasets separately because
of the diﬀerence in their labels. The severity distribution per project is shown
in Fig 3 and Fig 4. The ﬁgures show that some projects do not have enough
samples in all severity categories such as Chart project, containing only a few
bugs in “Medium” and “Low” severity groups, or the Closure project, which
mainly contains Medium severity bugs. Because of not having enough samples
in each project/severity group, we may combine data in diﬀerent RQs, which
we discuss later.

3 https://github.com/EhsanMashhadi/EMSE-BugSeverity

8

Ehsan Mashhadi et al.

Fig. 2: Buggy methods severity distributions of Defects4J and Bugs.jar
datasets with their USL values.

Fig. 3: Buggy methods severity distributions in Defects4J dataset with the
USL values.

2.2 Studied Code Metrics and Static Analysis Tools

In this section, we describe the code metrics and static analysis tools we use
in this study, as well as the reasons for choosing them.

2.2.1 Source Code Metrics

Diﬀerent source code metrics with module-level, class-level, and method-level
granularities have been used to measure software quality and predict bugs
in previous research [28,75]. In this paper, we leverage most of the common
method-level source code metrics that are used in the previous research (men-
tioned in the Related Work section) to see their capability in predicting buggy

Title Suppressed Due to Excessive Length

9

Fig. 4: Buggy methods severity distributions in Bugs.jar dataset with the USL
values.

codes and their severity. Although the list of selected metrics is not exhaus-
tive (given the numerous metrics explored in this ﬁeld and the limitations of
experiment size in one article), but we made sure we have most of the metrics
that have been shown eﬀective in predicting the method-level bugginess, in
the past. These metrics are deﬁned as follows:

Lines of Code (LC): Size, also known as lines of code (LC), is the most
popular, easy to measure, and the most eﬀective code metric for estimating
software maintenance [30,25,22]. The use of LC as a proxy maintenance indi-
cator is so prevalent that there are dedicated studies that completely focus on
LC and its correlation with other quality metrics (e.g.,
[30,52,22]). LC has
been extensively studied for bug prediction, fault localization, and for ﬁnd-
ing vulnerabilities (e.g., [73,11,89,22]). In this paper, we calculate LC as the
source lines of code without comments and blank lines, similar to [52,80,22]
to prevent the code formatting and comments eﬀects, which are out of this
study scope.

McCabe (MA): McCabe [61,52], also known as cyclomatic complexity, is
another very popular metric that indicates the number of independent paths,
and thus the logical complexity of a program. Intuitively, components with
high McCabe values are more bug-prone. McCabe has been studied extensively
to ﬁnd bugs and locate suspicious code [11], to understand its correlation
with code quality [72], and to leverage its value for test generation methods,
such as structured testing (path testing) [107]. McCabe can be calculated as
1 + #predicates [61].

McClure (ML): McClure [62,44] was proposed as an improvement over
McCabe. Unlike McCabe, McClure considers the number of control variables,
and the number of comparisons in a predicate, which is not supported by Mc-

10

Ehsan Mashhadi et al.

Cabe. Intuitively, a predicate with multiple comparisons and multiple control
variables would be more complex, and thus more bug-prone, than a predicate
with only one comparison or only a single control variable.

Nested Block Depth (NBD): McCabe and McClure do not consider
nested depth. According to these metrics, there is no diﬀerence between two
code snippets containing two identical for loops if they are arranged serially
or nested. NBD [46,8,111] has been studied alongside McCabe and McClure
to alleviate this issue.

Proxy Indentation (PI): Since McCabe-like complexity measures re-
quire a language-speciﬁc parser (for ﬁnding the predicates), Hindle et al. [40]
proposed Proxy Indentation metric as a proxy for McCabe-like complexity
metrics. It was shown that, for measuring complexity, indentation measure-
ment can perform very similar to more complex measurements such as Mc-
Cabe, without requiring a language-speciﬁc parser. Indentation measurement
is done for each line, and then an aggregated value is calculated for the whole
program component (e.g., a method). Hindle et al. showed that the standard
deviation as an aggregated value outperforms mean, median or max.

FanOut (FO): This metric calculates the total number of methods called
a given method. This provides an estimate of the coupling—i.e., dependency
of a particular method on other methods. It is observed that code components
that are highly coupled are less maintainable and bug-prone [64,9].

Readability (R): This metric combines diﬀerent code features to calculate
a single value for estimating code readability. We used the readability metric
proposed by Buse et al. [20] which generates a readability score for a given
method. The readability scores range from 0 to 1 for specifying least readable
code to most readable code, respectively. The authors concluded that this
metric has a signiﬁcant level of correlation with defects, code churn, and self-
reported stability.

Halstead Metrics: The Halstead code metrics contain seven measures
based on the number of operators and operands in a component [35]. These
metrics have been used in diﬀerent research such as measuring code complexity
perceived by developers [12], calculating the complexity of software mainte-
nance tasks [23,44], ﬁnding their correlation with indentation measures [40],
and estimating software readability [77]. As all the Halstead metrics are highly
correlated to each other, so in this paper, we consider only two of them: Diﬃ-
culty (D) and Eﬀort (E) which use other Halstead metrics in their formulas.
The Halstead Diﬃculty is calculated as:

D =

n1
2

∗

N 2
n2

Where n1 is the number of distinct operators, n2 is the number of distinct

operands, and N 2 is the total number of operands.

The Halstead Eﬀort is calculated as:

E = D ∗ V

Title Suppressed Due to Excessive Length

11

V = N ∗ log2(n)
N = N 1 + N 2

n = n1 + n2

Where N 1 is the total number of operators.

Maintainability Index (MI): Maintainability Index has been introduced
by Omran and Hagemeister [69] where the authors deﬁned metrics for measur-
ing the maintainability of a software system and combine those metrics into a
single value. This metric has evolved over time and was adopted by popular
tools like Visual Studio [7]. MI can be calculated as:

171 − 5.2 ∗ ln(HalsteadV olume) − 0.23 ∗ (M cCabe) − 16.2 ∗ ln(LOC)

Table 2 provides a brief description for each of our 10 studied metrics with
their abbreviation.

Table 2: List of studied metrics and their brief description.

Metric Description

LC
MA
ML
NBD
PI
FO
R
D
E
MI

Number of source code lines without comments and blank lines
Number of independent paths (logical complexity)
Number of control variables and number of comparisons in a predicate
Counting the depth of most nested block
Counting indentation of source code lines
Counting total number of methods called a given method
Measuring the readability of the code in the range of 0-1 (least to most)
Measure the diﬃculty of the program to write or understand the code
Determining the required eﬀort for developing/maintaining the code
Measuring how much the code is maintainable

2.2.2 Static Analysis Tools

Static analysis tools have been introduced for ﬁnding bugs by using diﬀerent
techniques, such as pattern matching. FindBugs [2] is an open-source tool
that analyzes Java byte code for ﬁnding bugs. SpotBugs [6] is a successor
of FindBugs which is actively under maintenance. SpotBugs has more than
400 bug patterns and their description is available on the tool’s website [6].
Infer [3] is a static analysis tool for diﬀerent programming languages such as
Java, C++, Objective-C, and C, which is also actively maintained. Infer has
more than 100 predeﬁned issue types which are described on its website [3].
Error prone [1] is another static analysis tool from Google which is used
for Java code to catch common programming faults at the compile-time, and

12

Ehsan Mashhadi et al.

it is actively under maintenance. We have selected SpotBugs and Infer tools
for our study because of their extensive usage in the previous studies and in
practice, and we exclude the ErrorProne because its latest versions do not
support older versions of Java projects which exist in our datasets. A more
detailed explanation of our selected conﬁguration for these tools is explained
in Section 3.2.2.

3 Experiment Design and Results

In this section, we provide the motivation, experiment designs, and results for
each RQ. Since each RQ has a few sub-RQs they may have diﬀerent experiment
designs which are explained in the related section.

3.1 Code Metrics & Bug Severity (RQ1)

We aim to understand the capabilities of source code metrics in ﬁnding bugs
and predicting their severity labels.

3.1.1 Motivation

One of the most important attributes in every issue tracking system is bug
severity which is usually determined manually by the developers/QA team.
This process is time-consuming and error-prone since the technical team should
investigate the eﬀects of the reported bug (e.g., number of aﬀected users,
number of crashes, probable consequence on the whole system) based on the
bug description or other available data from analytical systems (e.g., crash
report logging systems).

As we have mentioned in Section 6, there have been various prosperous
research that uses source code metrics for defect-related issues, but the severity
prediction studies mostly use bug description which works in cases where there
is a well-written bug description and also is quite subjective. Therefore, in this
RQ we study source code metrics’ ability in predicting bug severity.

3.1.2 Approach

We examine the code metrics in predicting buggyness and the severity labels
by following RQ and sub-RQs.

RQ1: Are source code metrics good indicators of buggyness and bug severity?
RQ1-1: Do source code metrics identify buggy code?
RQ1-2: Do source code metrics distinguish between diﬀerent bug severity?

Design of RQ1: In this RQ, we ﬁrst assess code metrics’ capabilities in
ﬁnding bugs and then evaluate them further in terms of their potential in

Title Suppressed Due to Excessive Length

13

estimating the bugs’ severity, using a static analysis test. To ﬁnd the answer
to this RQ, we apply a statistical test to see if distributions of methods (e.g.,
buggy vs non-buggy or critical vs low) are statistically diﬀerent according to
our selected code metrics.

After applying the Shapiro-Wilk test [87] using 5% level of signiﬁcance (α
= 0.05), we found out that none of our distributions are normal, so we use the
non-parametric Wilcoxon Rank-sum test for answering RQ1 with the 5% level
of signiﬁcance (α = 0.05) with the following two null-hypotheses for RQ1-1
and RQ1-2:

– RQ1-1: Distributions of buggy methods and non-buggy methods are not

statistically diﬀerent.

– RQ1-2: Distributions of buggy methods with diﬀerent severity values are

not statistically diﬀerent.

For ﬁnding the signiﬁcance of the diﬀerence, we calculated Cliﬀ’s Delta
eﬀect size. We followed the values provided by Hess and Kromrey [39] for
interpreting the result of this value. We considered values smaller than 0.147
to be negligible, values in range [0.147, 0.33) to be small, values in range [0.33,
0.474) to be medium, values greater than 0.474 to be large.

Since we are using the static analysis test for this RQ, it is important to
make sure that each distribution has enough samples, and distributions are
diverse enough (having samples from diﬀerent projects for reducing the bias
problem) to make the results robust. Also, we discard the methods with LC<4
in this RQ since they are mostly boilerplate code such as setters, getters, and
constructors which are generated automatically.

We found that our studied datasets contain imbalanced distributions for
RQ1-2 since some USLs contain only a few samples (e.g., the Critical USL in
the Defects4J contains only 25 samples) or some USLs do not contain enough
bugs from various projects to make that category diverse (e.g., 92% of samples
in the Medium USL of the Defect4J dataset are from Closure project). Be-
cause of the mentioned problem, we merge some USLs (Deﬁned in Section 2.1
one level further for RQ1-2 which is shown in Table 3.

Table 3: Merging diﬀerent USLs to create categories for Defect4J and Bugs.jar
datasets. For example, the ﬁrst row means that bugs having Critical or High
USLs are considered in the Critical category.

Dataset Uniﬁed Severity Label (USL) Merged Category

D4J

Bugs.jar

Critical, High
Low, Medium

Blocker, Critical
Major
Minor, Trivial

High
Low

Critical
Major
Minor

14

Ehsan Mashhadi et al.

As shown in Table 3, for the Defects4J dataset, the Critical USL is merged
into the High, and also the Medium and Low USLs are merged, so we ended
up with two categories of High and Low severity bugs.

In the Bugs.jar dataset, the Blocker and Critical USLs are merged, and also
the Trivial and Minor USLs are merged, so we concluded with three severity
categories of Critical, Major, and Minor. The Blocker and Critical USLs are
merged since the Critical USL does not contain enough samples, the Minor
and Trivial USLs are merged because of not enough sample in Trivial. We keep
the Major USL as an independent category since it has enough samples.

3.1.3 Results

RQ1-1 Answer: Figure 5 compares the distribution of diﬀerent code met-
rics between buggy and non-buggy methods (Figure 5a for the Defect4J, and
Figure 5b for the Bugs.jar datasets). These are aggregated results, as we have
combined data for all the projects in a speciﬁc dataset. Results suggest that
the median values of all code metrics except for the Readability and Main-
tainable Index in the buggy methods are larger than the median or Q3 (75th
percentile) values in the non-buggy methods. Also, for Maintainable Index and
Readability, the median of non-buggy methods are larger than the median or
Q3 (75th percentile) values in the non-buggy methods. Evidently, the buggy
methods are generally larger (LC), more complex (e.g., McCabe, McClure),
and less readable than the non-buggy methods. These ﬁgures imply that buggy
and non-buggy distributions are diﬀerent regarding all code metrics.

After applying the Wilcoxon Rank Sum Test to ﬁnd if there is a statistical
diﬀerence between these two distributions, we found that all of our compar-
isons for code metrics between buggy and non-buggy methods are statistically
signiﬁcant (P ≤ 0.5). Therefore, the null hypothesis is rejected which means
that these metrics are able to distinguish between buggy code and non-buggy
code. We provided the Cliﬀ’s delta size values in Table 4 which shows that
eﬀect size values are small or medium in all cases. In particular, LC, FanOut,
Maintainable Index and, Halstead Eﬀort exhibit better performance than other
metrics, because for them the eﬀect size is medium in both datasets.

Table 4: Cliﬀ’s Delta Eﬀect sizes for comparing code metrics between buggy
and non-buggy methods. S refers to Small and M refers to Medium eﬀect size.

Dataset

LC PI MA NBD ML D MI

FO R E

Defects4J M
M
Bugs.jar

S
S

M
S

S
S

M
S

S M
S M

M
M

S M
S M

With aggregated analysis, however, we can not observe if our results are
true for all of the projects. The aggregated data can be highly inﬂuenced

Title Suppressed Due to Excessive Length

15

(a) Defects4J

(b) Bugs.jar

Fig. 5: Comparing source code metrics between buggy methods (b axis) and
non-buggy methods (nb axis) using aggregated dataset.

by a few large projects. Also, diﬀerent external factors, such as code review
policy [106], developers’ commit patterns [38] and expertise [60], can impact
the distribution of code metrics and bug-proneness of a particular project.
Therefore, we now reproduce the results for each project individually.

We calculated the percent of projects where the distributions of a partic-
ular metric are statistically diﬀerent between buggy and non-buggy methods.
Table 5 shows the results for all the code metrics. The distribution of LC,
Maintainable Index, and FanOut are statistically diﬀerent between buggy and
non-buggy methods in all the studied projects from both datasets (100%).

10305070nbbLC26nbbPI51525nbbMA1357nbbNBD1030nbbML2.57.512.517.5nbbD2060100140180nbbMI1030nbbFO0.20.61nbbR2000600010000nbbE10305070nbbLC1357nbbPI2.57.512.517.5nbbMA1357nbbNBD51525nbbML2610nbbD2060100140180nbbMI5152535nbbFO0.20.61nbbR100030005000nbbE16

Ehsan Mashhadi et al.

Table 5: Wilcoxon Rank-sum test results to show the percentage of projects
where code metrics exhibit statistically diﬀerence between buggy and non-
buggy methods.

Dataset

LC

PI

MA

NBD ML

D

MI

FO

R

E

Defects4J
Bugs.jar

100% 50%
100% 87.5% 100% 100%

80%

70%

80%
100% 100% 100% 100% 100% 100%

100% 100% 90%

70%

80%

Table 6: Cliﬀ’s Delta Eﬀect sizes after comparing buggy vs non-buggy meth-
ods. N refers to Negligible, S refers to Small, M refers to Medium, and L refers
to Large eﬀect size. For example, the ﬁrst column of the ﬁrst row compares
the LC distributions between buggy and non-buggy methods and shows the
percent of diﬀerences with negligible, small, medium, and large eﬀect sizes.

Metrics

LC
PI
MA
NBD
ML
D
MI
FO
R
E

N

0
30
20
20
10
20
0
0
0
10

Defects4J

S M L

N

Bugs.jar
M
S

40
50
40
50
30
40
30
40
50
30

30
10
20
10
40
30
40
40
40
30

30
10
20
20
20
10
30
20
10
30

0.0
37.5
12.5
12.5
0.0
12.5
0.0
0.0
0.0
0.0

25
62.5
37.5
75
62.5
37.5
25
37.5
87.5
37.5

62.5
0.0
50
12.5
37.5
50
62.5
50
12.5
62.5

L

12.5
0.0
0.0
0.0
0.0
0.0
12.5
12.5
0.0
0.0

In the Defect4J dataset, the proxy indentation (PI) metric has the weakest
performance since in 50% of the projects, the result is not signiﬁcantly diﬀer-
ent. Most of the metrics, however, show good performance in distinguishing
the buggy method from the non-buggy method. We also calculated the Cliﬀ’s
eﬀect sizes of the diﬀerences, as presented and described in Table 6. The re-
sults show that Lines of Code, Maintainable Index, Fan Out, and Readability
metrics do not exhibit negligible eﬀect size for both datasets, but Proxy In-
dentation has the largest percentage of negligible eﬀect size in both datasets.

Summary of RQ1.1 Results: Our selected code metrics show high perfor-
mance in identifying the buggyness of methods in both datasets, and on both
the aggregated datasets and the individual projects. Line of Code, Maintain-
able Index, Fan-out, and Eﬀort metrics have the best performance while Proxy
Indentation and Nested Block Depth have the worst performance.

RQ1.2 Answer: After observing the somewhat known power of code met-
rics in distinguishing buggy and non-buggy methods, in this sub-RQ, we show

Title Suppressed Due to Excessive Length

17

their power in estimating the bug’s severity. Although we wanted to perform
the analysis for each project separately, there is not enough data in each
severity category, except for accumulo and jackrabbit-oak projects. The
accumulo project has 44, 100, 87 and jackrabbit-oak project has 71, 439, 80
samples in Critical, Major, and High categories (we described in Section2.1)
respectively. The number of samples for each category in other projects was
much lower, so we did not report their statistical test results separately.

Fig 6 compares the distributions of diﬀerent code metrics after grouping
the bugs into diﬀerent bug severity categories. To provide better insights, we
also provide the Wilcoxon Rank-Sum test and Cliﬀ’s Delta size values for
both datasets in Table 7. While the ﬁrst two major rows (for Defect4J, and
Bugs.jar) show results for aggregated analysis, the last two rows also show
results for the two individual projects: accumulo and jackrabbit-oak.

In contrast to the previous section, in many cases, the results are not sig-
niﬁcantly diﬀerent between diﬀerent bug severity categories. Considering the
aggregated analysis in both datasets, only Halstead diﬃculty, and eﬀort show
desired behavior: methods with higher severity bugs have higher diﬃculty and
eﬀort distribution. Maintainability index (MI) performs the worst: only in two
cases it exhibits statistically diﬀerent distributions. Readability shows desired
behavior for the Bugs.jar dataset when comparison was done between criti-
cal and major, and critical and minor. The statistically diﬀerent distributions
among these groups suggest that, a method with lower readability has higher
chance to have more severe bugs. Although source lines of code (LC) was very
helpful to diﬀerentiate between buggy and non-buggy methods (Figure 5b,
Table 5 and Table 6), its performance is signiﬁcantly worse when it comes to
distinguishing diﬀerent levels of bug severity. This is interesting because nu-
merous previous studies (e.g., [25,30, 91]) has shown that size (source lines of
code) is the most important code metric, and none of the other code metrics
provide any new information if their correlation with size is normalized. Our
result challenges that claim because we ﬁnd the superior performance of other
code metrics (e.g., Halstead diﬃculty, and Halstead eﬀort) than size when
distinguishing methods with diﬀerent bug severity.

In many cases, there is no statistical diﬀerence between these code metrics
when compared against bug severity. It means that there are some cases where
the code is complex which makes it error-prone, but severe bugs are not nec-
essary in the complex code. We may intuitively say that there may be bugs in
a simple method containing a SQL query for reading data from a database. In
such a method, the code complexity metrics suggest a simple method, but if
the developers do not handle input validation, so the code will be vulnerable
to SQL Injection. On the other hand, we may have a very complex and big
method containing nested loops with diﬀerent switch cases and exception han-
dling statements. The code complexity metrics suggest the complex method,
but this method may try to handle the UI/GUI part of the system only, which
is not vulnerable to any critical or high severity bugs. We will explore the real
examples from our datasets regarding our intuition in RQ3.

18

Ehsan Mashhadi et al.

(a) Defects4J

(b) Bugs.jar

Fig. 6: Comparing source code metrics between diﬀerent severity groups for
Defects4J and Bugs.jar datasets.

Summary of RQ1.2 Results: None of the selected code metrics show
promising results in distinguishing bug severity. While Diﬃculty and Eﬀort
metrics have the best performance in the aggregated datasets, the Maintain-
able Index and Proxy indentation metrics have the weakest performance. Al-
though the Line of Code metric has the best performance in distinguishing
buggyness of the methods, it has the worst performance in ﬁnding bug sever-
ity, in both the aggregated datasets and the individual projects.

2060100LowHighLC26LowHighPI51525LowHighMA2610LowHighNBD103050LowHighML51525LowHighD2060100140LowHighMI103050LowHighFO0.050.150.25LowHighR250075001250017500LowHighE10305070CriticalMinorMajorLC1357CriticalMinorMajorPI51525CriticalMinorMajorMA1357CriticalMinorMajorNBD5152535CriticalMinorMajorML261014CriticalMinorMajorD2060100140CriticalMinorMajorMI103050CriticalMinorMajorFO0.10.3CriticalMinorMajorR2000600010000CriticalMinorMajorETitle Suppressed Due to Excessive Length

19

Table 7: Cliﬀ’s Delta Size for bugs with diﬀerent severity values. α was set to
0.05. A red cell indicates that the null hypothesis is rejected (i.e., the distribu-
tions are statistically signiﬁcantly diﬀerent), whereas a blue cell means they
are not statistically diﬀerent. Eﬀect sizes are N∼Negligible, and S∼Small. For
example, the ﬁrst cell shows that in the Defect4J dataset, the distribution of
source lines of code (LC) between methods with high and low severe bugs is
not statistically diﬀerent (blue) and the diﬀerence has a negligible eﬀect size.

Project

Distributions

LC PI MA NBD ML D MI

FO R E

Defect4J (all)

High-Low

N

N

N

N

N

S

N

S

N

N

Bugs.jar (all)

accumulo

jackrabbit-oak

Critical-Major
Critical-Minor
Major-Minor

Critical-Major
Critical-Minor
Major-Minor

Critical-Major
Critical-Minor
Major-Minor

N
N
N

N
S
N

N
N
S

N
N
N

N
S
S

N
S
S

S
N
N

S
S
N

N
N
S

N
N
N

S
S
N

N
S
S

S
N
N

S
S
N

N
S
S

S
S
N

N
S
N

N
S
S

N
N
N

N
S
N

N
N
N

N
N
N

N
S
S

S
N
S

N
N
N

N
N
N

N
S
N

S
S
N

N
S
N

N
N
S

3.2 Static Analysis Tools & Bug Severity (RQ2)

The second goal of our study is to ﬁnd out if static analysis tools can detect
the bugs and their severity labels.

3.2.1 Motivation

Since source code metrics do not show promising results in predicting the bug
severity according to Section 3.1.2, we explore other approaches. To do so, we
study the static analysis tools which are widely used in practice for diﬀerent
purposes (e.g., ﬁnding programming errors, coding standard violations, syntax
violations, security vulnerabilities) by technical teams. This approach is favor-
able because of its higher speed than the human code review and its capability
in working oﬄine with low required resources. Since these tools are already
integrated into the deployment process of many companies, so if we could use
them for predicting the severity we can leverage them for prioritizing the bugs
without requiring any other tools/methods. Therefore, in this RQ, ﬁrst, we
need to ﬁnd out if these tools can detect real-world complex bugs, and then
we study their performance in predicting the detected bug’s severity.

3.2.2 Approach

We pose one RQ with its two sub-RQs to ﬁnd the static analysis tools capa-
bilities in bug/severity detection.

RQ2: What is the capability of static analysis tools in ﬁnding bugs and their

20

Ehsan Mashhadi et al.

severity?
RQ2-1: How eﬀective static analysis tools are in detecting buggy methods?
RQ2-2: Can static analysis tools estimate detected bugs’ severity?

Design of RQ2: To answer this RQ, we selected two static analysis tools
named SpotBugs, [6] and Infer [3]. We selected SpotBugs because it not only
can detect bugs but also report a Rank value which indicates the severity. We
used the popular Infer tool only to detect buggy methods regardless of their
severity values. Since there is no need for a statistical test in this RQ, and
therefore there is no restriction on the number of samples in each group we
used the USL values (mentioned in Section 2.1 directly without a need for any
merging process.

SpotBugs has diﬀerent conﬁgurations which aﬀect it performance in ﬁnding
bugs. It has some speciﬁc detector modules, which focus on speciﬁc bug types,
but we used the standard detectors, which is also the default option. Also, this
tool has a conﬁguration option named effort which adjusts the taken eﬀort
in ﬁnding bugs. We used the effort=max, which is the highest level of eﬀort,
so in this way, we have provided as much as the computation cost this tool
requires to work on our datasets.

There are diﬀerent ways to provide projects’ source code to these tools,
such as providing only the buggy class(es), the package containing the buggy
class(es), or the whole project. By providing only the buggy class, the tool
may not be able to ﬁnd the bugs that are across diﬀerent classes since it only
analyses that speciﬁc class, but when we provide the whole package or the
whole project, it analyses any ﬁle in these modules to ﬁnd the bugs accurately
which also needs more resources and time. We provided the whole project to
be analyzed with tools to make the result more robust. However, this takes 3
and 52 hours for the Defect4J and Bugs.jar dataset, respectively, on a regular
computer with 16GB RAM and 16 CPU cores.

Although we successfully applied these tools on all of the Defects4J projects,
for several projects of the Bugs.jar dataset we faced build errors due to de-
pendency issues. We were successful in building flink, commons-math, and
accumulo projects which consist of 248 buggy methods.

SpotBugs reports the Confidence value for each bug instance, which in-
dicates the level of conﬁdence the tool has in reporting this warning. This
attribute may have diﬀerent values such as 1 to match high-conﬁdence warn-
ings, 2 to match normal-conﬁdence warnings, or 3 to match low-conﬁdence
warnings. We considered all of the reported warnings regardless of their con-
ﬁdence values.

Also, this tool reports the Rank property which indicates the bug rank.
This property is an integer value between 1 and 20, where 1 to 4 are scariest,
5 to 9 scary, 10 to 14 troubling, and 15 to 20 of concern bugs. [6]. The higher
the number, the more important it is. This property has a similar semantic
to the bug severity value, so we used this value to see the tools capability in
ﬁnding the bugs’ severity.

Title Suppressed Due to Excessive Length

21

Table 8: Translation between the SpotBugs reported rank (SRR) value and
the uniﬁed severity label (USL) value.

Uniﬁed Severity Label (USL)

SpotBugs Reported Rank (SRR)

Critical, Blocker
Major, High
Medium
Low, Trivial, Minor

1 to 4
5 to 9
10 to 14
15 to 20

The translation between the range of this value and our USLs is provided
in Table 8. As the table shows, we consider 1 to 4 as Critical and Blocker USL,
5 to 9 as Major and High USL, 10 to 14 as Medium USL, and 15 to 20 as
Minor, Low, and Trivial USL. For example, when the SpotBugs reports any
number in the range of [1,4], and the actual severity value is either Critical or
Blocker we say that it detects the severity correctly.

The Infer tool reports the IssueType attribute with these categories: 1)
Error 2) Warning 3) Info, Advice, Like. These values can be interpreted as the
bug importance, but we found that all of our bugs are mapped to the Error
category, based on their severity values. Thus it is not possible to leverage the
IssueType property to predict the bug severity, and we cannot use this tool
to predict the severity. Therefore, we leverage this tool only to see if it can
detect buggy methods, regardless of their severity values.

Both of these tools report the roots of detected bugs according to their
deﬁned patterns, by locating the start and end lines of the bug. Since we are
considering bugs at the method-level granularity, we consider a method to be
buggy whenever these tools report a bug in that method regardless of the
reported start and end lines.

3.2.3 Results

RQ2-1 Answer: After applying SpotBugs and Infer tools we count the num-
ber of buggy methods that are reported as buggy (TP), the number of non-
buggy methods which are not reported as buggy (TN), the number of buggy
methods that are not detected by tools (FN), and the number of non-buggy
methods which are reported as buggy methods (FP). If any of these tools re-
port more than one bug for each method we only count that method once to
prevent the duplication problem.

We calculated the Accuracy, Precision, Recall, and F1 values which are
shown in Table 9. These values show that these tools have poor performance
in detecting buggy methods of both datasets. The precision value range is
7%-9% which means that they may report many non-buggy methods as buggy
methods, so the technical team may spend extra time on ﬁnding bugs in these
methods, and this increase the cost of the development phase without any
useful result. However, the recall value (predicting buggy methods as non-

22

Ehsan Mashhadi et al.

Table 9: Static analysis tools performance in ﬁnding buggy methods.

Dataset Tool

Accuracy Precision Recall

F1

D4J

SpotBugs
Infer

93%
95%

Bugs.jar

SpotBugs

95%

7.1%
9%

7.6%

7.2%
1.9%

4.3%

7.1%
3.1%

5.4%

buggy methods) is a more important metric in our case which has a smaller
range of 2%-7%. This small value indicates that these tools miss many bugs
which may lead to harmful consequences. The calculated F1 score shows that
SpotBugs have better performance in ﬁnding bugs in the Defect4J dataset than
the Bugs.jar dataset, and Infer tool has worse performance than SpotBugs for
the Defects4J dataset.

Our results are consistent with the previous study [32] in 2018 where the
authors studied the capability of SpotBugs, Infer, and ErrorProne tools in
ﬁnding buggy methods of an older version of the Defects4J dataset. They
concluded that 95.5% of buggy methods are not detected by these tools. Sur-
prisingly, although these tools have been updated several times since four years
ago, their performance in ﬁnding bugs is not improved much.

Since these tools are matching the provided code with their predeﬁned
generic bug patterns, we may intuitively say that in cases where the bug is
complex or related to the software speciﬁcation, they are missed by the tools.
We will perform a qualitative study of some randomly sampled bugs to ﬁnd
the reasons behind this poor performance, in RQ3.

Summary of RQ2.1 Results: Both SpotBugs and Infer tools have signif-
icantly low performance in ﬁnding bugs even with many existing developed
patterns. The precision range for both datasets is 7%-9% and the Recall range
is 2%-7% which shows the high number of missed bugs.

RQ2.2 Answer: Since only the SpotBugs tool can detect the severity of
bugs, in this section we provide the results of only this tool on both Defects4J
and Bugs.jar datasets.

We provided the confusion matrices for SpotBugs tool in Fig 7. These
ﬁgures show that this tool cannot detect bug severity in many cases, and also
mislabeled many bugs with wrong severity labels. For example, it detects two
critical bugs as low-severity bugs and 20 high-severity bugs as low-severity
bugs, in the Defects4J dataset. These ﬁgures show that the SpotBugs did not
detect the severity of any Critical bugs in Defect4J and Bugs.jar datasets.

Table 10 shows diﬀerent descriptive statistics of this tool performance.
Since we have multi-class labels, we used macro average [4] for calculating
these values. In this method, the ﬁnal result is computed by taking the arith-
metic mean of all the per-class descriptive scores.

Title Suppressed Due to Excessive Length

23

Defects4J

Bugs.jar

Fig. 7: Confusion matrices of bug severity prediction by SpotBugs for Defects4J
and Bugs.jar datasets

The overall score F1 score shows that SpotBugs has signiﬁcantly better
performance for the Defect4J dataset, but the scores show that this tool has,
overall, a very low performance. We studied bug characteristics manually in
RQ3 to ﬁnd the reasons for this weak performance.

Table 10: SpotBugs performance in ﬁnding bug severity of Defects4J and
Bugs.jar datasets.

Dataset Accuracy Precision Recall

F1

D4J
Bugs.jar

94.00%
0.03%

12.35%
1.00%

1.25%
0.04%

1.67%
0.08%

Summary of RQ2.2 Results: SpotBugs has signiﬁcantly better performance
in ﬁnding bug severity of the Defects4J dataset than the Bugs.jar dataset.
However, this tool wrongly assigns lower severity values to bugs in many cases.
The overall performance shows that there is still much work to be done for
this tool to accurately estimate bug severity.

3.3 Manual Analysis of Bug Characteristics (RQ3)

Our ﬁnal goal, in RQ3, is to dig more into the results of RQ1 and RQ2 and
study the bug characteristics in diﬀerent important scenarios such as (a) when
there is a contradiction between code metrics and severity, (b) when the static

24

Ehsan Mashhadi et al.

analysis tools miss/mislabel severe bugs, and (c) when code metrics and static
analysis tools can be complementary.

3.3.1 Motivation

To understand why source code metrics cannot indicate the bug severity, we
need to study the cases where there is a contradiction between the bug severity
and source code metrics. Also, it seems important to ﬁnd out if there is any
relationship between the bug’s type and its severity. The answer to this ques-
tion can help researchers to consider bug types as important features when
providing new approaches for bug prediction to have better accuracy, and also
developers may pay more attention to speciﬁc bug types during the manual
severity assignment process.

To ﬁnd out why static analysis tools miss many severe bugs or predict
lower severity than the actual severity, in most cases, we need to study if
this problem exists because of the nature of static analysis tools or if there is
potential for improvement. The answer to this question will help researchers
to improve the developed approaches to enhance the bug severity performance
and also developers can get insights into how conﬁdently they should use these
tools in their projects.

Finally, since real bugs are very complex due to the complexity of the
current software and various new technologies, it would be useful to consider
multiple approaches instead of relying on a single approach to predict the
severity. Therefore, we also study if code metrics and static analysis tools are
complementary in terms of bug severity prediction. This may lead to a tech-
nique that combines these approaches to predict the severity more accurately.

3.3.2 Approach

We provided one RQ with three sub-RQs to study the bug characteristics.

RQ3: What are the characteristics of bugs with diﬀerent severity values?
RQ3-1: What factors may lead to inconsistencies between bug severity and
code metrics?
RQ3-2: Why do static analysis tools miss or mislabel severe bugs?
RQ3-3: Do the results of code metrics and static analysis tools have overlap?

Design of RQ3-1: As we show in the RQ1 results section, there is a
correlation between bug existence and code complexity metrics, but there is
no signiﬁcant correlation between bug severity and code complexity metrics.
Therefore, we manually investigated a random sample of bugs for ﬁnding fac-
tors that lead to contradictions between bug severity values and code metrics.
The goal of this sub-RQ is to study the characteristics of buggy methods with
high severity (e.g., Blocker, Critical, Major) which have low code metrics mea-
surement. Also, to study buggy methods with low severity (e.g., Low, Minor,
Trivial) which have high code metrics measurement.

Title Suppressed Due to Excessive Length

25

To facilitate the manual analysis, we split all bugs into two high-level cat-
egories of severity, which are: severe (including Blocker, Critical, Major, and
High severity USLs from RQ1) and non-severe (including Medium, Low, Mi-
nor, and Trivial USLs from RQ1).

To ﬁnd the two lists of the most and the least complex methods, we calcu-
late a representative sum complexity value from diﬀerent code metrics. Since
our code metric values do not have the same range, we normalize them by us-
ing the Robust Scaler algorithm [5], which is robust against outliers. It follows
a similar algorithm to the MinMax scale, but it uses the interquartile range
rather than the min-max. The formula for Robust Scaler is:

xi − Q1(x)
Q3(x) − Q1(x)

Where Q1 is the 1st quartile, and Q3 is the third quartile.

To ﬁnd the Sum Complexity (SC) value—which indicates the total com-
plexity of the method—we take the sum of diﬀerent code metrics (we multiply
Readability and Maintainable Index by -1 to reverse their value). We provide
the same weight to all the code metrics and the possible impact of this deci-
sion is discussed in the Threats to Validity section. Table 11 shows two sample
bugs with the calculated code metrics, the normalized code metric, and also
the Sum Complexity values.

Table 11: Two sample bugs from Severe and Non-Severe categories with their
code metric values, normalized values, and the Sum Complexity values. The
value in each row and the value of Sum Complexity are rounded to two preci-
sion.

Metrics

Raw

Normalized

Raw

Normalized

Severe

Non-Severe

LC
PI
MA
NBD
ML
D
FO
E
MI
R

13.00
2.89
3.00
1.00
2.00
1.86
4.00
220.14
-103.93
-0.22

Sum Complexity

143.74

-0.35
-0.25
-0.37
-0.33
-0.53
-0.39
-0.35
-0.27
-0.53
-3.66

-7.07

532.00
8.61
194.00
12.00
219.00
18.84
40.00
214131.00
23.86
0

21.27
2.75
23.50
3.33
13.93
2.20
22.14
45.50
4.15
0

215459.26

138.80

After calculating the SC value, to ﬁnd the contradiction, we sorted our sam-
ples based on the SC (normalized version) value in ascending and descending

26

Ehsan Mashhadi et al.

orders separately and keep the ﬁrst 60 samples in both cases for both datasets
which result in 240 samples:

60 ∗ 2 (Severe — Non-Severe) ∗ 2 (Defects4J — Bugs.jar) = 240

For preventing bias, we randomly select 60 bugs out of these 240 bugs
distributed evenly between Severe and Non-Severe bugs (our two categories in
this RQ) and also between Defects4J and Bugs.jar (our datasets). Then, we
provide the buggy code and the corresponding ﬁxed code of these samples to
the ﬁrst and third authors.

We use a grounded theory approach called Card Sorting [63] to label the
bug type manually. In this approach, we asked the ﬁrst and third authors to
label each buggy method with one or more bug types of their choice, based on
their own-deﬁned list of bugs types (e.g., Network, UI/GUI, IndexOutofBound,
Math calculation), by looking at the buggy code and the ﬁxed code only. The
total unique labels created by the two authors in this phase were nine low-level
labels (mentioned in the 3.3.3 section).

Then, the ﬁrst and third authors had a meeting to discuss their assigned
labels and also merge them into the ﬁnal set of high-level labels containing four
labels. During the merging process, while authors agreed on most of the sam-
ples’ labels, several assigned labels had similar meanings but were expressed in
diﬀerent terms. We resolved those conﬂicts by assigning a more generic label
that covers both original labels. For example, there is a sample where the ﬁrst
author assigned “I/O bug”, as the label, but the third author assigned “Net-
work bug”. We merged these two labels in I/O bug label which covers Network
bug too. Also, there are a few cases where the ﬁrst and third authors cannot
make the decision because of the conﬂicts, so we resolved these conﬂicts by
comparing both provided categories and the code. Following this approach, we
ended up with these four labels per bug: Integration, Edge/Boundary/ Secu-
rity, and Speciﬁcation.

Design of RQ3-2: Based on RQ2 results, we found that the SpotBugs
tool missed or mislabeled bug severity in many cases, which may lead to un-
desirable consequences. For example, mislabeling the high-severity bugs with
low severity values will reduce the bug’s importance. Thus the development
team may postpone the ﬁxing process. Therefore, in this sub-RQ, we want to
ﬁnd out the reasons behind why these tools are not able to ﬁnd the bugs or
their severity values.

To do so, we ﬁrst manually analyze 25 randomly chosen severe bugs (Blocker,
Critical, Major, and High severity categories) in both datasets that are missed
by static analysis tools, by comparing the buggy code and ﬁxed code. In this
step, we mainly want to know if the lack of bug patterns in static analysis tools
prevents them from detecting these bugs, or if these bugs’ nature is such that
static analysis tools cannot detect them regardless of what pattern/rule they
implement. We will provide a few samples in the following section including
buggy and ﬁxed code.

Title Suppressed Due to Excessive Length

27

We then study a set of 25 randomly selected samples of severe bugs (Blocker,
Critical, Major, and High severity categories) which are detected by SpotBugs
tools, but the reported severity is lower than the actual severity values, e.g.,
when the actual severity value is Critical, but the tools report the Low severity
value.

Design of RQ3-3: Given the results of RQ1 and RQ2, in this sub-RQ, we
want to see if both severity estimation approaches (based on code complexity
and static analysis tools) are leveraging similar or diﬀerent characteristics of
the code to predict severity or not. The motivation behind this sub-RQ is that
if they are focusing on diﬀerent aspects, combining both approaches will lead
to a better prediction.

To answer this question, we manually investigate all the samples predicted
by the SpotBugs to see if the calculated Sum Complexity (mentioned in the
RQ3-1) values are aligned with the reported bug severity by this tool. We
report samples where these values are contradictory, and one of them predicts
the bug severity, accurately, in the following section. For instance, we look at
a case where there is a high-severity buggy method in the dataset, which has a
large Sum Complexity value, but the SpotBugs reports Low severity. We also
summarize cases where using both of these approaches has better performance
than using either of them in the following section.

3.3.3 Results

RQ3-1 Answer: After applying the Card Sorting approach which is explained
in Section 3.3.2 we found nine low-level bug labels such as UI (User Interface)
bug, multi-threading bug, and math calculation bugs. These labels may have
overlap in many scenarios, and some of them contain only one or two samples.

Table 12: Distribution of randomly sampled bugs with low-level bug labels.

Low-level Label

Number

I/O
UI
Multi-threading
Null checking
Exception handling
Math/String calculation
Wrong/missing objects and parameters
Input/state checking
Completely wrong implementation

2
1
2
4
4
3
21
16
7

Table 12 shows bug types and their distributions. I/O bugs occur in a
method where the code access any I/O such as a ﬁle or network. Multi-
threading bugs are due to not handling threading issues such as deadlock.

28

Ehsan Mashhadi et al.

Null checking bugs happen when the code access the null object or do not
handle special speciﬁcation when the method argument is null. Exception
handling bugs occur when the method does not handle exceptions when a
variable/object has a special value. Math/String calculation bugs occur
when the mathematical calculations or string manipulations are performed
wrongly. Wrong/missing objects and parameters type refers to code
where the developer passes a wrong parameter to a method or use the wrong
object to call. Input/state checking bugs happen when the code does not
validate the function inputs or it does not consider the speciﬁc case of an
object state for having diﬀerent logic. Completely wrong implementation
bug type shows cases where the ﬁxed code is changed completely to implement
new logic compared to the buggy code.

Table 13: Distribution of high-level bug labels where there is contradiction
between severity values and Sum Complexity (SC) value.

High-level Label

Severe Bug & Low SC Non-severe Bug & High SC

Integration
Edge/Boundary
Security
Speciﬁcation

2
2
7
19

0
7
3
20

We provided the distributions of high-level bug labels in case of severe
bugs and low Sum Complexity (SC), and also non-severe bugs and high Sum
Complexity (SC) in Table 13. The number of Security faults is more when the
bugs are labeled as severe, but the code metric values are low which indicates
that security bugs may appear in simple methods but may lead to severe bugs.
Furthermore, Edge/Boundary faults are mostly available in samples with high
Sum Complexity (SC) values where the code is complex by having many dif-
ferent branches and statements where developers tend to miss edge/boundary
cases, but these bugs are not severe necessarily.

In the following, we will provide a deﬁnition for each of these four bug type
categories and give some samples per category. Just a reminder that these cat-
egories have been deﬁned based on our dataset and are the result of merging
the labels from the Card Sorting phase.

Integration faults: This bug label exhibit cases where the bug is related
to another component, module, or class that our buggy method is using, so
it would be impossible to access the buggy method without considering the
called module(s). For example, Fig 8 shows a buggy code and its corresponding
ﬁxed code in the JxPath-14 project. The buggy method seems to be very
simple containing only three statements, so based on the code metric values
this is not a complex code, but the actual severity value is high. The bug is
related to the floor() method which is from another module (math). It seems

Title Suppressed Due to Excessive Length

29

Fig. 8: Sample bug with High severity value in the JxPath-14 project of the
Defects4J dataset.

that this module may not handle NaN and Infinite values or it may handle
them diﬀerently than the studied project’s (JxPath) speciﬁcation. From the
patch, we see that the developer needs to return a diﬀerent value when the
v parameter is NaN or Infinite, so the developer had to handle this case in
the code before calling the Math.floor() function. As another example, the
buggy method in the Cli-2 project uses the NumberUtils.createNumber()
method to convert the variable with String type to the Number type, but the
patch shows that the developer has rewritten the logic of converting String
to Number inside the code without using the NumberUtils anymore. In this
case, the called component may contain a bug or it may be developed with
diﬀerent speciﬁcation which does not satisfy the caller component’s developer’s
requirement, so for detecting the severity value of this kind of bug accurately
it is required to consider both caller and called module code together.

Since we found only two Integration faults, we do not draw a conclusion
about the correlation between the Integration faults and their high severity.
However, our observations show that estimating the severity of faults in this
category by only considering the caller method seems impossible. Thus an
analysis in a coarser granularity (e.g, class/module/package) is suggested.

Edge/Boundary cases: This kind of bug happens when the developers
do not handle edge/boundary cases which usually leads to a software crash.
Several samples in this category contain a severe bug with a simple code ac-
cording to our code metrics, containing only a few statements. For example,
Fig 9 shows the buggy method in the Math-36 project, where the developer
did not handle the edge case (Double.isNaN(result)), but the patch shows
that the studied method should have performed an extra calculation when the
result variable is NaN. This bug may lead to a software crash, unauthorized
access, or undesired behaviour.

Another severe bug exists in the accumulo project where the buggy method
renames a requested table. In this method, the code does not check if the
requested table exists already or whether the new requested name is already

30

Ehsan Mashhadi et al.

Fig. 9: Sample bug with High severity value in the Math-36 project of the
Defects4J dataset.

taken by another table, so this bug probably leads to a software crash or
overwriting the existing table.

We found that most of the Edge/Boundary bugs are not severe in our
datasets, but they appear in complex methods. Measuring the bug conse-
quences in the whole system would be required to ﬁnd the bug impact and
eventually the severity. Edge/Boundary case testing could be a possible solu-
tion to determine the bug impact based on the number of failed tests.

Security faults: Security faults can be exploited to gain unauthorized
access/privileges by intruders. Since it is not easy to ﬁnd if there is a security
fault, solely from the source code, we found that some bugs are highly potential
to be exploited by intruders by considering keywords and the method context.
For example, there is a bug with Major severity in the jackrabbit-oak
project which is shown in Fig 10. This simple (based on the code metric
values) method checks whether the requested path argument has the access
permission to be used. Handling permissions incorrectly may potentially lead
to unauthorized access, and this usually has harmful consequences such as
data loss or breaking the system completely.

After investigating all the samples in this category, we found that Security
faults are severe (Blocker, Critical, Major, High) in most cases. However, these
bugs are available in simple methods (based on our code metrics) that handle
a sensitive task which may lead to harmful consequences and result in severe
bugs.

Although we found most of our Security faults happen in the simple meth-
ods, but more speciﬁc methodologies such as security testing are required to
ﬁnd these bugs’ severity values. If we can ﬁnd the security faults type (e.g.,
Injection, Data Exposure, Broken Access Control, etc) the bug severity can be

Title Suppressed Due to Excessive Length

31

Fig. 10: Sample bug with the Major severity value in the OAK-3324 5f863af6
project of Bugs.jar dataset.

Fig. 11: Sample bug with the Low severity value in the Closure-119 project of
the Defects4J dataset.

found easily, since security fault types typically have a speciﬁc ranking based
on their probable consequences in a few deﬁned standards.

Speciﬁcation faults: This category, which is our largest category, ex-
hibits bugs where developers have not implemented the full speciﬁcation (e.g.,

32

Ehsan Mashhadi et al.

Fig. 12: Sample bug with the Low severity value in the Lang-30 project of the
Defects4J dataset.

the logic related to a partition in the input space has been ignored, i.e., mis-
represented). This bug type is called Speciﬁcation faults since they may not
occur if the developers would follow the speciﬁcation of the software com-
pletely [37]. Patches for these bugs contain changing conditional statements,
math calculations, string manipulation, or a complete rewrite of the code.

For example, there is a bug with the Low severity value in Closure-119
project where the buggy method is too complex (containing 120 lines of code
having many nested switch-case and conditional statements), but the patch
is very simple which handles a small part of input space. Part of this method
and its patch are shown in Fig 11. Long methods such as our mentioned
example are usually bug-prone, and they are detected as buggy by code metrics
correctly, but the bug could have a Low severity value since these long and
complicated methods may handle a not sensitive task such as handling the
GUI part.

Also, Fig 12 shows part of a buggy method from the Lang30 project which
has a Low severity value. In this sample, the patch is not as simple as the
previous example and part of the code is rewritten, but the severity is still
low.

The number of Speciﬁcation faults in both severe and non-severe categories
is almost equal as shown in Table 13, so further information is strongly re-

Title Suppressed Due to Excessive Length

33

quired to detect the bug severity in this case.

Summary of RQ3.1 Results: Most of our studied bugs were placed in the
Speciﬁcation categories, evenly, regardless of their severity value. Our studied
bugs in the Edge/Boundary category are complex (high SC value) but mainly
have a Low severity value. Security bugs have High severity, but they are
mostly not complex (low SC value). It is almost impossible to detect the
Integration fault severity label without considering the called module/class.

RQ3-2 Answer: In this section, we provide the results of our manual
investigation of severe bugs (Blocker, Critical, Major, High) that are not de-
tected by static analysis tools and go through some examples of severe bugs
that are miss-labeled by SpotBugs.

Fig 13 shows a Critical bug in Cli-2 project, which is not detected by
SpotBugs and Infer because it cannot be ﬁtted into any rule. The provided
patch seems to be simple, but the bug is too complex, since it is related to
a speciﬁc case where the developer should use token variable and also the
break statement in the last else branch to handle a special case required by
the speciﬁcation.

Another example is a Critical bug in the Math-11 project which is shown
in Fig 14. The patch is very simple, and the buggy statement looks almost
identical to the ﬁxed statement such that it is even very hard for a human to
ﬁnd the issue without knowing the special values of dim variable that leads to
the bug.

We found other severe bugs that cannot be generalized in the bug patterns
such as a bug in the Math-105 project where the getSumSquaredErrors() in
the buggy code returns the maximum number between the error value and
zero wrongly. The patch shows that it should return the calculated error value
only (there is no need for the max function). Also, Codec-18 project contains a
bug where the equals method check if two provided CharSequence objects are
equal. The patch shows that the developer implemented the equality condition
wrongly by not considering the maximum length of these strings which are
patched in the ﬁxed version.

In all of the mentioned examples, we found that these bugs are so complex
that cannot be mapped to any of the predeﬁned patterns. Detecting these
severe bugs by only leveraging the source code (even at the coarse-level gran-
ularity such as class/package/module) is almost impossible. Therefore, more
sophisticated patterns need to be implemented by static analysis tools. One
may also try using dynamic approaches such as testing to detect these types
of bugs.

Furthermore, we found some samples when there is a speciﬁc rule for the
bug in the tools, but the bug is not detected by any of the tools. For example,
the buggy method in the Cli-9 project copies the values of a mapping variable
(deﬁned at the class level) to the provided argument of the method. The
provided patch shows that there is a speciﬁc condition when the mapping

34

Ehsan Mashhadi et al.

Fig. 13: Critical bug in the Cli-2 project of the Defects4J dataset.

variable is null, so the buggy method should return the provided argument
instead of copying data. Although the SpotBugs tool has some predeﬁned rules
regarding the null handling of variables in the method it cannot report this
method as buggy.

One possibility for this low eﬀectiveness by static analysis tools is that
since these tools are heavily used in production as a part of the development
environment or building process, there is a high chance that all of these projects

Title Suppressed Due to Excessive Length

35

Fig. 14: Critical bug in the Math-11 project of the Defects4J dataset.

were scanned by these tools after each commit/release, so all bugs that can be
detected by these tools have been ﬁxed already.

By investigating the results of the SpotBugs tool to see why this tool
mislabeled the severity of many bugs we found that 70 bugs are detected to
have lower severity values than their actual severity value. After analyzing
these samples, we found two scenarios. The ﬁrst one would be samples that
SpotBugs reported another bug in the buggy method than the actual bug. In
the second case, Spotbugs reports the bug correctly, but it assigns the lower
severity labels.

For example, the reported bug in the Math-91 project has High severity, but
SpotBugs reports Low severity. The actual reported bug is due to the equality
problem of two Fraction objects because of the limited precision 4, but the
SpotBugs reports the CO_COMPARETO_INCORRECT_FLOATING as the issue type
which is in the BAD_PRACTICE category with the following description:

“This method compares double or ﬂoat values using pattern like this:

val1 > val2 ? 1 : val1 < val2 ? − 1 : 0”

Although this statement exists in the buggy method, the actual reported

bug is not happening because of this statement.

Another example is the bug in the Compress-12 project where the ac-
tual reported bug is related to throwing the IllegalArgumentException in-
stead of IOException on corrupted ﬁles 5. However, the SpotBugs reported
this method containing a Low-severity bug with the I18N issue type in the
DM_DEFAULT_ENCODING category with the following description:

“Found a call to a method which will perform a byte to String (or String
to byte) conversion, and will assume that the default platform encoding is
suitable.”

In the mentioned examples we found that SpotBugs reported a diﬀerent

bug with Low severity instead of the actual bug with High severity.

4 https://issues.apache.org/jira/browse/MATH-252
5 https://issues.apache.org/jira/browse/COMPRESS-178

36

Ehsan Mashhadi et al.

Conversely, there is a Major bug in the accumulo-209_397f86f6 project
regarding the multi-byte character encoding 6, and the SpotBugs reported a
DM_DEFAULT_ENCODING as the issue type in I18N category with the following
description:

“Found a call to a method which will perform a byte to String (or String
to byte) conversion, and will assume that the default platform encoding is
suitable.”

It seems that the SpotBugs has detected the bug correctly, but it assigns a
low severity to it. Based on our best knowledge, SpotBugs assign the severity
of the reported bugs based on their implemented patterns, so it always reports
Low severity for this kind of bug. However, in practice, one speciﬁc issue may
lead to a severe or non-severe bug in diﬀerent cases such as the number of
aﬀected users or the project timeline.

Summary of RQ3.2 Results: SpotBugs missed most of the studied bugs
regardless of their severity value because of no matching rule for those bugs.
In speciﬁc cases there are matching (almost the same) rules, however, the bugs
cannot be detected accurately. Bugs are assigned diﬀerent (lower) severity by
SpotBugs since it detected the severity value based on the general pre-deﬁned
bug patterns, but in practice, a speciﬁc bug may have diﬀerent severity values
in diﬀerent projects based on the number of aﬀected users or the project
timeline.

RQ3-3 Answer: By comparing the calculated SC (mentioned in 3.3.2)
and the severity predicted by the SpotBugs tool we found some samples where
using both of these results will be more useful.

For example, we consider the buggy method computeShiftIncrement()
in the Math-87 project has High severity value. The SpotBugs tool reports
the Low severity value, but the SC value shows that this method is a complex
method regarding the code metrics we used. In this case, the code metric values
exhibit the severity better.

Conversely, there is a buggy method named parse() in the Closure-68
project with the Medium severity value. The SC value for this buggy method
is very large which shows the complexity of the method, so we may assume
this is a severe bug, but the SpotBugs reports the Low severity value. In the
mentioned example, the SpotBugs prediction is closer to the actual severity
value than the code metric values.

The above-mentioned examples show that these two approaches focus on
diﬀerent aspects of bugs, so considering both of them can lead to better pre-
diction than using only one of them.

6 https://github.com/bugs-dot-jar/accumulo/tree/bugs-dot-jar_ACCUMULO-209_

397f86f6

Title Suppressed Due to Excessive Length

37

Summary of RQ3.3 Results: Considering both code metrics and static
analysis tools together may lead to better performance since they leverage
diﬀerent characteristics of bugs in some cases.

4 Discussion

Our results from RQ1 indicate that all of the studied code metrics have signif-
icant performance in ﬁnding the buggy methods. However, the Line of Code,
Maintainable Index, FanOut, and Eﬀort metrics have the best performance
which implies the importance of these metrics to be used in the research com-
munity and also in practice to warn developers about their code complexity.
However, we found no signiﬁcant relationship between studied code metrics
and the bug severity which means high-severity bugs may exist in the simple
code or low-severity bugs may exist in the complex code. Surprisingly, the Line
of Code has is the worst indicator of bug severity which indicates there is no
relationship between the size of a buggy method and its severity. Our ﬁndings
from this RQ suggest that developers may still avoid long and complex meth-
ods to reduce the chance of bugs, but they may expect severe bugs even in the
simple method with few line of codes.

Our RQ2 results show that studied static analysis tools (SpotBugs and
Infer) can detect neither the buggy methods nor their severity. However, since
these tools are used extensively in practice, many of the detected bugs may
have been ﬁxed soon during the development process by developers already.
Our results imply that they are not practical in case of real complex problems,
and there is a potential gap for the research community and the developers to
study and provide more complex bug patterns or a combination of diﬀerent
approaches to detect real bugs. Furthermore, the reported severity values by
SpotBugs show that it mislabels many of the severe bugs which reduces the
usability of this tool in ﬁnding the bug severity, so this implies that the imple-
mented approach by this tool, which is assigning the predeﬁned severity value
based on the bug pattern, is not practical and diﬀerent conditions should be
considered in determining the severity of a speciﬁc bug.

The results from RQ3 show that Security bugs have high severity in most
cases while the Edge/Boundary bugs have low severity. We found that the bug
type is a better indication of the bug severity in contrast to the bug complexity.
Also, we found that SpotBugs tool does not have enough bug patterns to
predict many of our studied severe bugs while it predicts lower severity values
in most of the cases. However, we found that source code metrics and static
analysis tools leverage diﬀerent characteristics of the bugs, so leveraging both
of them is more helpful.

Our ﬁndings can be used by the research community and the developers,
so that they leverage other code metrics to see their capability in ﬁnding bug
severity, or perhaps other combinations of several code metrics may be found
more useful. Also, our ﬁndings can be used for improving the quality of static
analysis tools, especially in the case of the studied missed bugs. Developers

38

Ehsan Mashhadi et al.

should insist on writing simple methods to prevent bugs, but they may con-
sider diﬀerent conditions (the potential consequences on the overall system
such as leaking sensitive information or crashing the whole system) during the
development phase in order to reduce the risk of having high-severity bugs.

5 Threats to Validity

Similar to any empirical study there are some threats that may impact our re-
sults, so in this section, we provide possible threats and our actions to mitigate
these threats.

5.1 External Validity

Our selected projects may not be consistent with the closed-source projects or
may not cover various software domains or other programming languages. We
did our best to select two large and popular datasets containing real-world bugs
of a diverse set of open-source projects which have been used in many studies
[58,86,74,84]. The next threat would be the selection of static analysis tools,
and for mitigating this problem we used the most popular tools which are used
in practice and state-of-the-art research. We exclude Google Error Prone from
the list of our tools since we found that it is not possible to handle older Java
versions (exist in our dataset) with the latest versions of this tool. Also, the
selected code metrics may not reveal the potential of all available code metrics
in ﬁnding bug severity and this may impact our result generalizability, but
to mitigate this issue we tried to leverage as many as available method-level
code metrics that are used in previous research for defect-related problems
(e.g, defect prediction).

5.2 Internal Validity

Another threat would be our strategy in selecting the buggy methods since we
selected all methods that are changed/removed during the bug ﬁxing patch
as a buggy method which is a common approach in the state-of-the-art ap-
proaches (referenced in 2.1). The reason behind this is that bug ﬁxing usually
requires changing multiple methods and specifying one single method to be the
source of a bug requires other information such as a bug report description. For
mitigating this threat we consider each of those methods as an independent
sample, so code metrics and static analysis tools can consider them indepen-
dently. The next threat is the method of identifying reported bugs by static
analyzers’ tools. In this study, we consider the tools’ report at the method-level
granularity without considering the exact reported lines by these tools. We did
this because there is no information regarding the exact bug location (line) in
our datasets, and we want to keep the same method-granularity for RQ1 and
RQ2. In order to mitigate this threat whenever any of our tools report a bug

Title Suppressed Due to Excessive Length

39

inside the method, we count it as a correct detection to not underestimate the
tools’ performance.

5.3 Construct Validity

When calculating the sum complexity for answering RQ3-1, we have provided
the same weight to all the diﬀerent code metrics. These can be inaccurate
because some code metrics can be more important than others. Through our
manual observation, however, we found that a more complex method based
on the sum complexity has higher complexity values for all the code metrics
than a less complex method.

5.4 Conclusion Validity

Despite the eﬀorts made though, these results may not be generalized using
other tools and datasets. Our results of static analysis tools may be aﬀected
by using projects that have already applied the studied static analyzer tools
warnings during the software development life cycle. Since it is not easy to
understand if it happened, our results should be considered as an assessment
of those bugs that are not already ﬁxed by tools. Our selection of code metrics
may not be suﬃcient to expose the bug severity, but for mitigating this issue
we considered the 10 popular metrics which have established performance in
ﬁnding bugs in our study.

6 Related Work

There has been much research in the bug-prediction ﬁeld by focusing on dif-
ferent aspects using diverse granularity levels and various techniques [34,41,
104,54]. Previous studies can be categorized from diﬀerent aspects such as
working on the characteristics of source code [48,71, 15, 16,70,92,79] or the
bug description [96,105,47]. From the source code granularity aspect, there
has been research in ﬁle level [114,29], package level [114], class level [90], and
method level [28]. For bug severity prediction task, almost all of the existing
works have leveraged bug report using natural language techniques or diﬀerent
classic and deep neural network models. In the rest of this section, we brieﬂy
discuss the related work on generic bug prediction, bug severity prediction,
and static analysis tools.

6.1 Bug Prediction

Bug prediction studies have used diﬀerent code metrics such as LOC [73,
11, 89, 22], McCabe [11], Halstead metrics [12], C&K metrics [94,42]. Code
metrics have been applied on diﬀerent granularity levels such as package/class

40

Ehsan Mashhadi et al.

level [68,51,114], method level [73,28,27,22,31,65], and line level [108]. One
major problem with the high granularity levels (package/class) is that they are
practically less helpful for the developers [28, 73] since it requires signiﬁcant ef-
fort to locate bugs at the package/class components. Unfortunately, line-level
granularity can suﬀer from too many false positives, because multiple lines
can be similar just by chance [93,85]. Consequently, method level granular-
ity has been the new focus to the community [73,28, 27, 22,31,65], especially
for developing bug prediction models, and several studies show positive and
encouraging results [28,27,65].

6.2 Bug Severity Prediction

Tian et al. [98] leveraged bug reports and their severity labels in the past to
recommend ﬁne-grained severity labels for newly reported bugs by measur-
ing the similarity between bug reports and using nearest neighbors classiﬁ-
cation. The authors improved the f-measure of the state-of-the-art approach
signiﬁcantly. Ramay et al [81] proposed an approach by using a deep learning
model, natural language techniques, and emotion analysis by using bug re-
ports for predicting bug severity. They mentioned that this method improved
the f-measure of state-of-the-art approaches by 7.90% on average. Lamkanﬁ et
al [112] proposed the bug triage and bug severity prediction using Topic Mod-
eling. Their evaluation of 30,000 bug reports extracted from Eclipse, Mozilla,
and Netbeans projects shows the eﬀectiveness of their approach in predicting
severity. Tan et al [97] leveraged question-and-answer pairs from Stack Over-
ﬂow and combine them with related bug reports to make an enhanced version
of the bug reports. They predict the severity of bug reports using diﬀerent
machine learning models which led to improvements by approximately 23% of
the average f-measure.

6.3 Static Analysis Tools

Various research has been done to evaluate the true eﬀectiveness of various
static analysis tools for bug detection tasks. Some researchers leveraged these
tools as an oracle for their own provided techniques. For example, Tomassi [99]
used SpotBugs and ErrorProne to detect bugs in the BugSwarm dataset, and
they found that these tools are not eﬀective in ﬁnding the bugs since their
results show only one successful bug detection by SpotBugs. Ayewah et al. [13]
leveraged FindBugs for ﬁnding bugs in Google’s internal code-base, and they
found that integrating this tool into Google’s Mondrian code review system
would help developers see the potential bug existence in the code. Habib and
Pradel [32] studied SpotBugs, Infer, and Google Error Prone tools to ﬁnd
their capabilities in detecting java real bugs. They concluded that these tools
are mostly complementary to each other, and they miss the large majority of
the bugs. Dura et al. [24] introduced the JavaDL which is a Datalog-based

Title Suppressed Due to Excessive Length

41

declarative speciﬁcation language for bug pattern detection in Java code, and
compare it against the SpotBugs and ErrorProne tools. Authors found that
JavaDL has comparable performance to these tools. Habib and Pradel [33]
proposed a method to consider bug detection as a classiﬁcation problem by
using the neural networks and using Google Error Prone as an oracle.

To the best of our knowledge, none of the previous studies have investigated
the relationship between various popular code metrics and bug severity in
method-level granularity, nor the eﬀectiveness of static analysis tools for bug
severity detection.

7 Conclusion and Future Work

Detecting and ﬁxing bugs are among the main maintenance activities in the
software life cycle. However, among many bugs that potentially exist in the
code the high severity ones are in great interest for the developers since their
consequences are the most. Unfortunately, most existing studies in defect de-
tection take all bugs the same and ignore their severity. In this paper, we stud-
ied 10 source code metrics and two static analysis tools, as common methods
for predicting buggy code, to ﬁnd their capability in estimating the bugs’ sever-
ity levels. Our quantitative and qualitative studies on Defects4J and Bugs.jar
datasets containing 3,358 buggy methods from 19 projects of Java open-source
projects, showed that code metrics are good indicators of buggyness (Line of
Code, Maintainable Index, Fan-out, and Eﬀort are the best metrics), but nei-
ther code metrics nor static analysis tools are signiﬁcant estimators of bugs
severity. We found that there is no relationship between the code complexity
and bug severity, and static analysis tools miss many bugs due to the lack of
complex deﬁned patterns.

Manual inspection of severe bugs reveals that the Security and Edge/Bound-
ary bug types have high and low severity in most cases, respectively. Finally, we
showed that code metrics and static analysis tools leveraged diﬀerent charac-
teristics of the code, so they can be complementary to predict the bug severity.
Potential future directions of this research are studying the power of dy-
namic analysis and testing in estimating bug severity. In addition, one can use
the ﬁndings of this study regarding static analysis tools limitations and try to
enrich their rule sets to better identify severe bugs.

References

1. Errorprone. URL https://errorprone.info/. [Online; accessed 20-April-2022
2. Findbugs. URL http://findbugs.sourceforge.net/. [Online; accessed 20-April-2022]
3. Infer. URL https://fbinfer.com/. [Online; accessed 20-April-2022]
4. Macroaverage.

URL https://scikit-learn.org/stable/modules/generated/

sklearn.metrics.f1_score.html. [Online; accessed 20-April-2022]

5. Robustscaler.

URL https://scikit-learn.org/stable/modules/generated/

sklearn.preprocessing.RobustScaler.html. [Online; accessed 20-April-2022]
6. Spotbugs. URL https://spotbugs.github.io/. [Online; accessed 20-April-2022]

42

Ehsan Mashhadi et al.

7. Visual

studio3.

URL

https://docs.microsoft.com/en-us/visualstudio/

code-quality/code-metrics-maintainability-index-range-and-meaning?view=
vs-2022&viewFallbackFrom=vs-2022%3A. [Online; accessed 20-April-2022]

8. Alenezi, M., Akour, M., Al Sghaier, H.: The impact of co-evolution of code production
and test suites through software releases in open source software systems. International
Journal of Innovative Technology and Exploring Engineering (IJITEE) 9(1), 2737–2739
(2019)

9. AlOmar, E.A., Mkaouer, M.W., Ouni, A., Kessentini, M.: On the impact of refac-
toring on the relationship between quality attributes and design metrics.
In: 2019
ACM/IEEE International Symposium on Empirical Software Engineering and Mea-
surement (ESEM), pp. 1–11. IEEE (2019)

10. Aniche, M.F., Oliva, G.A., Gerosa, M.A.: What do the asserts in a unit test tell us about
code quality? a study on open source and industrial projects. In: 2013 17th European
Conference on Software Maintenance and Reengineering, pp. 111–120. IEEE (2013)
11. Antinyan, V., Staron, M., Meding, W., ¨Osterstr¨om, P., Wikstrom, E., Wranker, J.,
Henriksson, A., Hansson, J.: Identifying risky areas of software code in agile/lean soft-
ware development: An industrial experience report. In: 2014 Software Evolution Week-
IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering
(CSMR-WCRE), pp. 154–163. IEEE (2014)

12. Antinyan, V., Staron, M., Sandberg, A.: Evaluating code complexity triggers, use of
complexity measures and the inﬂuence of code complexity on maintenance time. Em-
pirical Software Engineering 22(6), 3057–3087 (2017)

13. Ayewah, N., Pugh, W., Morgenthaler, J.D., Penix, J., Zhou, Y.: Using ﬁndbugs on pro-
duction software. In: Companion to the 22nd ACM SIGPLAN conference on Object-
oriented programming systems and applications companion, pp. 805–806 (2007)
14. Bacchelli, A., Bird, C.: Expectations, outcomes, and challenges of modern code review.
In: 2013 35th International Conference on Software Engineering (ICSE), pp. 712–721.
IEEE (2013)

15. Bacchelli, A., D’Ambros, M., Lanza, M.: Are popular classes more defect prone? In:
International Conference on Fundamental Approaches to Software Engineering, pp.
59–73. Springer (2010)

16. Bavota, G., De Carluccio, B., De Lucia, A., Di Penta, M., Oliveto, R., Strollo, O.: When
does a refactoring induce bugs? an empirical study. In: 2012 IEEE 12th International
Working Conference on Source Code Analysis and Manipulation, pp. 104–113. IEEE
(2012)

17. Bennett, K.H., Rajlich, V.T.: Software maintenance and evolution: a roadmap.

In:
Proceedings of the Conference on the Future of Software Engineering, pp. 73–87 (2000)
18. Bhat, T., Nagappan, N.: Evaluating the eﬃcacy of test-driven development: industrial
In: Proceedings of the 2006 ACM/IEEE international symposium on

case studies.
Empirical software engineering, pp. 356–363 (2006)

19. B¨orstler, J., Paech, B.: The role of method chains and comments in software readabil-
ity and comprehension—an experiment. IEEE Transactions on Software Engineering
42(9), 886–898 (2016)

20. Buse, R.P., Weimer, W.R.: Learning a metric for code readability. IEEE Transactions

on software engineering 36(4), 546–558 (2009)

21. Chaturvedi, K.K., Singh, V.: Determining bug severity using machine learning tech-
niques. In: 2012 CSI sixth international conference on software engineering (CONSEG),
pp. 1–6. IEEE (2012)

22. Chowdhury, S., Uddin, G., Holmes, R.: An empirical study on maintainable method
size in java. In: 19th International Conference on Mining Software Repositories (2022)
23. Curtis, B., Sheppard, S.B., Milliman, P., Borst, M., Love, T.: Measuring the psycholog-
ical complexity of software maintenance tasks with the halstead and mccabe metrics.
IEEE Transactions on software engineering (2), 96–104 (1979)

24. Dura, A., Reichenbach, C., S¨oderberg, E.: Javadl: automatically incrementalizing
java bug pattern detection. Proceedings of the ACM on Programming Languages
5(OOPSLA), 1–31 (2021)

25. El Emam, K., Benlarbi, S., Goel, N., Rai, S.N.: The confounding eﬀect of class size
on the validity of object-oriented metrics. IEEE Transactions on Software Engineering
27(7), 630–650 (2001)

Title Suppressed Due to Excessive Length

43

26. Ferenc, R., B´an, D., Gr´osz, T., Gyim´othy, T.: Deep learning in static, metric-based

bug prediction. Array 6, 100021 (2020)

27. Ferenc, R., Gyimesi, P., Gyimesi, G., T´oth, Z., Gyim´othy, T.: An automatically created
novel bug dataset and its validation in bug prediction. The Journal of systems and
software 169, 110691 (2020)

28. Giger, E., D’Ambros, M., Pinzger, M., Gall, H.C.: Method-level bug prediction. In:
Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software
Engineering and Measurement, pp. 171–180. IEEE (2012)

29. Giger, E., Pinzger, M., Gall, H.C.: Comparing ﬁne-grained source code changes and
In: Proceedings of the 8th Working Conference on

code churn for bug prediction.
Mining Software Repositories, pp. 83–92 (2011)

30. Gil, Y., Lalouche, G.: On the correlation between size and metric validity. Empirical

Software Engineering 22(5), 2585–2611 (2017)

31. Grund, F., Chowdhury, S., Bradley, N.C., Hall, B., Holmes, R.: Codeshovel: Con-
structing method-level source code histories. In: 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE), pp. 1510–1522. IEEE (2021)

32. Habib, A., Pradel, M.: How many of all bugs do we ﬁnd? a study of static bug de-
tectors. In: 2018 33rd IEEE/ACM International Conference on Automated Software
Engineering (ASE), pp. 317–328. IEEE (2018)

33. Habib, A., Pradel, M.: Neural bug ﬁnding: A study of opportunities and challenges.

arXiv preprint arXiv:1906.00307 (2019)

34. Hall, T., Beecham, S., Bowes, D., Gray, D., Counsell, S.: A systematic literature re-
view on fault prediction performance in software engineering. IEEE Transactions on
Software Engineering 38(6), 1276–1304 (2011)

35. Halstead, M.H.: Elements of Software Science (Operating and programming systems

series). Elsevier Science Inc. (1977)

36. Hata, H., Mizuno, O., Kikuno, T.: Bug prediction based on ﬁne-grained module his-
In: 2012 34th international conference on software engineering (ICSE), pp.

tories.
200–210. IEEE (2012)

37. Hemmati, H.: How eﬀective are code coverage criteria? In: 2015 IEEE International

Conference on Software Quality, Reliability and Security, pp. 151–156. IEEE (2015)

38. Herzig, K., Zeller, A.: The impact of tangled code changes. In: 2013 10th Working

Conference on Mining Software Repositories, pp. 121–130 (2013)

39. Hess, M.R., Kromrey, J.D.: Robust conﬁdence intervals for eﬀect sizes: A comparative
study of cohen’sd and cliﬀ’s delta under non-normality and heterogeneous variances.
In: annual meeting of the American Educational Research Association, vol. 1. Citeseer
(2004)

40. Hindle, A., Godfrey, M.W., Holt, R.C.: Reading beside the lines: Indentation as a
proxy for complexity metric. In: 2008 16th IEEE International Conference on Program
Comprehension, pp. 133–142. IEEE (2008)

41. Hosseini, S., Turhan, B., Gunarathna, D.: A systematic literature review and meta-
analysis on cross project defect prediction. IEEE Transactions on Software Engineering
45(2), 111–147 (2017)

42. Jureczko, M., Spinellis, D.: Using object-oriented design metrics to predict software de-
fects. Models and Methods of System Dependability. Oﬁcyna Wydawnicza Politechniki
Wroc(cid:32)lawskiej pp. 69–81 (2010)

43. Just, R., Jalali, D., Ernst, M.D.: Defects4j: A database of existing faults to enable
controlled testing studies for java programs. In: Proceedings of the 2014 International
Symposium on Software Testing and Analysis, pp. 437–440 (2014)

44. Kafura, D., Reddy, G.R.: The use of software complexity metrics in software mainte-

nance. IEEE Transactions on Software Engineering (3), 335–343 (1987)

45. Kanwal, J., Maqbool, O.: Bug prioritization to facilitate bug report triage. Journal of

Computer Science and Technology 27(2), 397–412 (2012)

46. Kasto, N., Whalley, J.: Measuring the diﬃculty of code comprehension tasks using
software metrics. In: Proceedings of the Fifteenth Australasian Computing Education
Conference-Volume 136, pp. 59–65 (2013)

47. Khatiwada, S., Tushev, M., Mahmoud, A.: Just enough semantics: An information
theoretic approach for ir-based software bug localization. Information and Software
Technology 93, 45–57 (2018)

44

Ehsan Mashhadi et al.

48. Khomh, F., Penta, M.D., Gu´eh´eneuc, Y.G., Antoniol, G.: An exploratory study of
the impact of antipatterns on class change-and fault-proneness. Empirical Software
Engineering 17(3), 243–275 (2012)

49. Kondo, M., German, D.M., Mizuno, O., Choi, E.H.: The impact of context metrics on
just-in-time defect prediction. Empirical Software Engineering 25(1), 890–939 (2020)
50. Kononenko, O., Baysal, O., Godfrey, M.W.: Code review quality: How developers see
it. In: Proceedings of the 38th international conference on software engineering, pp.
1028–1038 (2016)

51. Koru, A., Liu, H.: Building eﬀective defect-prediction models in practice. IEEE Soft-

ware 22(6), 23–29 (2005)

52. Landman, D., Serebrenik, A., Vinju, J.: Empirical analysis of the relationship between
cc and sloc in a large corpus of java methods. In: 2014 IEEE International Conference
on Software Maintenance and Evolution, pp. 221–230. IEEE (2014)

53. Le Goues, C., Nguyen, T., Forrest, S., Weimer, W.: Genprog: A generic method for
Ieee transactions on software engineering 38(1), 54–72

automatic software repair.
(2011)

54. Li, N., Shepperd, M., Guo, Y.: A systematic review of unsupervised learning techniques
Information and Software Technology 122, 106287

for software defect prediction.
(2020)

55. Long, F., Rinard, M.: Automatic patch generation by learning correct code. In: Pro-
ceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages, pp. 298–312 (2016)

56. M¨antyl¨a, M.V., Lassenius, C.: What types of defects are really discovered in code

reviews? IEEE Transactions on Software Engineering 35(3), 430–448 (2008)

57. Martin, R.C.: Professionalism and test-driven development. Ieee Software 24(3), 32–36

(2007)

58. Martinez, M., Durieux, T., Sommerard, R., Xuan, J., Monperrus, M.: Automatic repair
of real bugs in java: A large-scale experiment on the defects4j dataset. Empirical
Software Engineering 22(4), 1936–1964 (2017)

59. Mashhadi, E., Hemmati, H.: Applying codebert for automated program repair of java
simple bugs. In: 2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR), pp. 505–509. IEEE (2021)

60. Matter, D., Kuhn, A., Nierstrasz, O.: Assigning bug reports using a vocabulary-based
expertise model of developers. In: 2009 6th IEEE International Working Conference
on Mining Software Repositories, pp. 131–140 (2009)

61. McCabe, T.J.: A complexity measure. IEEE Transactions on software Engineering (4),

308–320 (1976)

62. McClure, C.L.: A model for program complexity analysis. In: Proceedings of the 3rd

international conference on Software engineering, pp. 149–157 (1978)

63. Miles, M.B., Huberman, A.M.: Qualitative data analysis: An expanded sourcebook.

sage (1994)

64. Mo, R., Cai, Y., Kazman, R., Xiao, L., Feng, Q.: Decoupling level: a new metric
In: 2016 IEEE/ACM 38th International

for architectural maintenance complexity.
Conference on Software Engineering (ICSE), pp. 499–510. IEEE (2016)

65. Mo, R., Wei, S., Feng, Q., Li, Z.: An exploratory study of bug prediction at the method

level. Information and software technology 144, 106794 (2022)

66. Nawrocki, J., Wojciechowski, A.: Experimental evaluation of pair programming. Eu-

ropean Software Control and Metrics (Escom) pp. 99–101 (2001)

67. Neysiani, B.S., Babamir, S.M., Aritsugi, M.: Eﬃcient feature extraction model for
validation performance improvement of duplicate bug report detection in software bug
triage systems. Information and Software Technology 126, 106344 (2020)

68. Okutan, A., Yıldız, O.T.: Software defect prediction using bayesian networks. Empir-

ical software engineering : an international journal 19(1), 154–181 (2014)

69. Oman, P., Hagemeister, J.: Metrics for assessing a software system’s maintainability. In:
Proceedings Conference on Software Maintenance 1992, pp. 337–338. IEEE Computer
Society (1992)

70. Pai, G.J., Dugan, J.B.: Empirical analysis of software fault content and fault proneness
using bayesian methods. IEEE Transactions on software Engineering 33(10), 675–686
(2007)

Title Suppressed Due to Excessive Length

45

71. Palomba, F., Bavota, G., Penta, M.D., Fasano, F., Oliveto, R., Lucia, A.D.: On the
diﬀuseness and the impact on maintainability of code smells: a large scale empirical
investigation. Empirical Software Engineering 23(3), 1188–1221 (2018)

72. Pantiuchina, J., Lanza, M., Bavota, G.: Improving code: The (mis) perception of qual-
In: 2018 IEEE International Conference on Software Maintenance and

ity metrics.
Evolution (ICSME), pp. 80–91. IEEE (2018)

73. Pascarella, L., Palomba, F., Bacchelli, A.: On the performance of method-level bug
prediction: A negative result. Journal of Systems and Software 161, 110493 (2020)
74. Pearson, S., Campos, J., Just, R., Fraser, G., Abreu, R., Ernst, M.D., Pang, D., Keller,
B.: Evaluating and improving fault localization. In: 2017 IEEE/ACM 39th Interna-
tional Conference on Software Engineering (ICSE), pp. 609–620. IEEE (2017)

75. Pecorelli, F., Palomba, F., Di Nucci, D., De Lucia, A.: Comparing heuristic and ma-
chine learning approaches for metric-based code smell detection. In: 2019 IEEE/ACM
27th International Conference on Program Comprehension (ICPC), pp. 93–104. IEEE
(2019)

76. Polo, M., Piattini, M., Ruiz, F.: Using code metrics to predict maintenance of legacy
programs: A case study. In: Proceedings IEEE International Conference on Software
Maintenance. ICSM 2001, pp. 202–208. IEEE (2001)

77. Posnett, D., Hindle, A., Devanbu, P.: A simpler model of software readability.

In:
Proceedings of the 8th working conference on mining software repositories, pp. 73–82
(2011)

78. Raﬁque, Y., Miˇsi´c, V.B.: The eﬀects of test-driven development on external quality
and productivity: A meta-analysis. IEEE Transactions on Software Engineering 39(6),
835–856 (2012)

79. Rahman, A., Williams, L.: Source code properties of defective infrastructure as code

scripts. Information and Software Technology 112, 148–163 (2019)

80. Ralph, P., Tempero, E.: Construct validity in software engineering research and soft-
ware metrics. In: Proceedings of the 22nd International Conference on Evaluation and
Assessment in Software Engineering 2018, pp. 13–23 (2018)

81. Ramay, W.Y., Umer, Q., Yin, X.C., Zhu, C., Illahi, I.: Deep neural network-based

severity prediction of bug reports. IEEE Access 7, 46846–46857 (2019)

82. Saha, R.K., Khurshid, S., Perry, D.E.: An empirical study of long lived bugs. In: 2014
Software Evolution Week-IEEE Conference on Software Maintenance, Reengineering,
and Reverse Engineering (CSMR-WCRE), pp. 144–153. IEEE (2014)

83. Saha, R.K., Lyu, Y., Lam, W., Yoshida, H., Prasad, M.R.: Bugs. jar: a large-scale,
In: Proceedings of the 15th international

diverse dataset of real-world java bugs.
conference on mining software repositories, pp. 10–13 (2018)

84. Saha, R.K., Lyu, Y., Yoshida, H., Prasad, M.R.: Elixir: Eﬀective object-oriented pro-
gram repair. In: 2017 32nd IEEE/ACM International Conference on Automated Soft-
ware Engineering (ASE), pp. 648–659. IEEE (2017)

85. Servant, F., Jones, J.A.: Fuzzy ﬁne-grained code-history analysis. In: Proceedings of
the International Conference on Software Engineering (ICSE), pp. 746–757 (2017)
86. Shamshiri, S., Just, R., Rojas, J.M., Fraser, G., McMinn, P., Arcuri, A.: Do auto-
matically generated unit tests ﬁnd real faults? an empirical study of eﬀectiveness and
challenges (t).
In: 2015 30th IEEE/ACM International Conference on Automated
Software Engineering (ASE), pp. 201–211. IEEE (2015)

87. Shaphiro, S., Wilk, M.: An analysis of variance test for normality. Biometrika 52(3),

591–611 (1965)

88. Shihab, E., Hassan, A.E., Adams, B., Jiang, Z.M.: An industrial study on the risk of
software changes. In: Proceedings of the ACM SIGSOFT 20th International Sympo-
sium on the Foundations of Software Engineering, pp. 1–11 (2012)

89. Shin, Y., Meneely, A., Williams, L., Osborne, J.A.: Evaluating complexity, code churn,
and developer activity metrics as indicators of software vulnerabilities. IEEE transac-
tions on software engineering 37(6), 772–787 (2010)

90. Shivaji, S., Whitehead, E.J., Akella, R., Kim, S.: Reducing features to improve bug
In: 2009 IEEE/ACM International Conference on Automated Software

prediction.
Engineering, pp. 600–604. IEEE (2009)

46

Ehsan Mashhadi et al.

91. Sjøberg, D.I.K., Yamashita, A., Anda, B.C.D., Mockus, A., Dyb˚a, T.: Quantifying the
eﬀect of code smells on maintenance eﬀort. IEEE Transactions on Software Engineering
39(8), 1144–1156 (2013)

92. Spadini, D., Palomba, F., Zaidman, A., Bruntink, M., Bacchelli, A.: On the relation of
test smells to software code quality. In: 2018 IEEE international conference on software
maintenance and evolution (ICSME), pp. 1–12. IEEE (2018)

93. Steidl, D., Hummel, B., Juergens, E.: Incremental origin analysis of source code ﬁles.
In: Proceedings Working Conference on Mining Software Repositories (MSR), pp. 42
– 51 (2014)

94. Subramanyam, R., Krishnan, M.S.: Empirical analysis of ck metrics for object-oriented
design complexity: Implications for software defects. IEEE Transactions on software
engineering 29(4), 297–310 (2003)

95. Sun, W., Marakas, G., Aguirre-Urreta, M.: The eﬀectiveness of pair programming:

Software professionals’ perceptions. IEEE Software 33(4), 72–79 (2015)

96. Sun, X., Zhou, W., Li, B., Ni, Z., Lu, J.: Bug localization for version issues with defect

patterns. IEEE Access 7, 18811–18820 (2019)

97. Tan, Y., Xu, S., Wang, Z., Zhang, T., Xu, Z., Luo, X.: Bug severity prediction using
question-and-answer pairs from stack overﬂow. Journal of Systems and Software 165,
110567 (2020)

98. Tian, Y., Lo, D., Sun, C.: Information retrieval based nearest neighbor classiﬁcation
for ﬁne-grained bug severity prediction. In: 2012 19th Working Conference on Reverse
Engineering, pp. 215–224. IEEE (2012)

99. Tomassi, D.A.: Bugs in the wild: examining the eﬀectiveness of static analyzers at
ﬁnding real-world bugs.
In: Proceedings of the 2018 26th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, pp. 980–982 (2018)

100. Tosun, A., Bener, A., Turhan, B., Menzies, T.: Practical considerations in deploying
statistical methods for defect prediction: A case study within the turkish telecommu-
nications industry. Information and Software Technology 52(11), 1242–1257 (2010)

101. Tufano, M., Palomba, F., Bavota, G., Oliveto, R., Di Penta, M., De Lucia, A., Poshy-
vanyk, D.: When and why your code starts to smell bad. In: 2015 IEEE/ACM 37th
IEEE International Conference on Software Engineering, vol. 1, pp. 403–414. IEEE
(2015)

102. Uddin, J., Ghazali, R., Deris, M.M., Naseem, R., Shah, H.: A survey on bug prioriti-

zation. Artiﬁcial Intelligence Review 47(2), 145–180 (2017)

103. Vucevic, D., Yaddow, W.: Testing the data warehouse practicum: Assuring data con-

tent, data structures and quality. Traﬀord Publishing (2012)

104. Wahono, R.S.: A systematic literature review of software defect prediction. Journal of

software engineering 1(1), 1–16 (2015)

105. Wang, D., Lin, M., Zhang, H., Hu, H.: Detect related bugs from source code using
bug information. In: 2010 IEEE 34th Annual Computer Software and Applications
Conference, pp. 228–237. IEEE (2010)

106. Wang, Q., Xia, X., Lo, D., Li, S.: Why is my code change abandoned? Information

and Software Technology 110, 108 – 120 (2019)

107. Watson, A.H., Wallace, D.R., McCabe, T.J.: Structured testing: A testing methodol-
ogy using the cyclomatic complexity metric, vol. 500. US Department of Commerce,
Technology Administration, National Institute of . . . (1996)

108. Wattanakriengkrai, S., Thongtanunam, P., Tantithamthavorn, C., Hata, H., Mat-
sumoto, K.: Predicting defective lines using a model-agnostic technique. arXiv preprint
arXiv:2009.03612 (2020)

109. Williams, L., Kessler, R.R., Cunningham, W., Jeﬀries, R.: Strengthening the case for

pair programming. IEEE software 17(4), 19–25 (2000)

110. Wong, W.E., Gao, R., Li, Y., Abreu, R., Wotawa, F.: A survey on software fault
localization. IEEE Transactions on Software Engineering 42(8), 707–740 (2016)
111. Zaw, K.K., Hnin, H.W., Kyaw, K.Y., Funabiki, N.: Software quality metrics calcula-
tions for java programming learning assistant system. In: 2020 IEEE Conference on
Computer Applications (ICCA), pp. 1–6. IEEE (2020)

Title Suppressed Due to Excessive Length

47

112. Zhang, T., Chen, J., Yang, G., Lee, B., Luo, X.: Towards more accurate severity pre-
diction and ﬁxer recommendation of software bugs. Journal of Systems and Software
117, 166–184 (2016)

113. Zhou, Y., Xu, B., Leung, H.: On the ability of complexity metrics to predict fault-prone
classes in object-oriented systems. Journal of Systems and Software 83(4), 660–674
(2010)

114. Zimmermann, T., Premraj, R., Zeller, A.: Predicting defects for eclipse.

In: Third
International Workshop on Predictor Models in Software Engineering (PROMISE’07:
ICSE Workshops 2007), pp. 9–9. IEEE (2007)

