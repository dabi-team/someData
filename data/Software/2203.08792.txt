2
2
0
2

r
a

M
6
1

]

V
C
.
s
c
[

1
v
2
9
7
8
0
.
3
0
2
2
:
v
i
X
r
a

PosePipe: Open-Source Human Pose Estimation
Pipeline for Clinical Research

R. James Cotton*,1,2

*rcotton@sralab.org
1Department of Physical Medicine and Rehabilitation, Northwestern University
2Shirley Ryan AbilityLab

Abstract

There has been signiﬁcant progress in machine learning algorithms for human pose estimation
that may provide immense value in rehabilitation and movement sciences. However, there
remain several challenges to routine use of these tools for clinical practice and translational
research, including: 1) high technical barrier to entry, 2) rapidly evolving space of algorithms,
3) challenging algorithmic interdependencies, and 4) complex data management requirements
between these components. To mitigate these barriers, we developed a human pose estimation
pipeline that facilitates running state-of-the-art algorithms on data acquired in clinical context.
Our system allows for running diﬀerent implementations of several classes of algorithms and
handles their interdependencies easily. These algorithm classes include subject identiﬁcation
and tracking, 2D keypoint detection, 3D joint location estimation, and estimating the pose
of body models. The system uses a database to manage videos, intermediate analyses, and
data for computations at each stage. It also provides tools for data visualization, including
generating video overlays that also obscure faces to enhance privacy. Our goal in this work is
not to train new algorithms, but to advance the use of cutting-edge human pose estimation
algorithms for clinical and translation research. We show that this tool facilitates analyzing
large numbers of videos of human movement ranging from gait laboratories analyses, to
clinic and therapy visits, to people in the community. We also highlight limitations of these
algorithms when applied to clinical populations in a rehabilitation setting. Code for PosePipe
can be found at https://github.com/peabody124/PosePipeline/.

Introduction

Accurate tracking of human movement is a critical prerequisite for movement science and rehabilitation
research. The gold standard is a movement analysis lab where optical markers are tracked with high spatial
and temporal precision in order to precisely reconstruct biomechanical movements. While this method is
highly accurate, it requires signiﬁcant expertise, is time consuming and expensive, and can only be performed
in a lab with specialized equipment. Wearable sensors equipped with inertial measurement units can help
track movement outside of the laboratory, but often require signiﬁcant time for setup and calibration and are
typically less accurate, as reconstructing the underlying biomechanics from sensor data is a challenging data
processing problem1–3.

Deep learning-based approaches to human pose estimation (HPE) from video have advanced rapidly in recent
years4 and show promise in enabling easy-to-use and precise movement analysis outside of a specialized labo-
ratory setting. These approaches could be a key enabling technology for movement science and rehabilitation
research. For example, it could enable more frequent and precise measurements of patient’s movement during
recovery and enable longitudinal quantiﬁcation of their movement impairments. This could allow better
understanding of how movement impairments relate to functional abilities (i.e., ability to performing activities
of daily living) and enable more sensitive clinical trials to improve them. A recent consensus paper on upper
extremity rehabilitation after stroke highlighted the need for more routine kinematic measurements for this
express purpose, while also pointing out this is impeded by the lack of easy to use measurement tools5.

 
 
 
 
 
 
PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

Despite recent advancements, there are still numerous barriers that prevent video-based human pose estimation
from fulﬁlling this goal for rehabilitation and movement science6. Among these barriers are: 1) there is a
high technical barrier to using many state-of-the-art algorithms; 2) these algorithms are rapidly evolving; 3)
many of the algorithms have interdependencies; 4) they require a signiﬁcant amount of data management; 5)
the accuracy of many of these algorithms have not been validated on clinical populations7,8; and 6) many
algorithms do not produce clinically pertinent outputs. Here, we will brieﬂy review some of the classes of
algorithms for HPE before demonstrating how our work on PosePipe reduces barriers 1-4 listed above. We
also highlight some issues that occur when these algorithms are tested on clinical populations, which relates
to barrier 5, but we defer quantitative analyses of this and work on barrier 6 to future work.

Types of HPE Algorithms

The ﬁeld of human pose estimation from computer vision is large and growing rapidly, and we refer interested
readers to recent in-depth reviews on the topic for additional details4,9–11. Here, we will provide a brief
high-level overview of some common classes of algorithms and taxonomic categories used in our framework
and how these interact.

HPE algorithms either process all people in a frame (bottom-up approaches), or analyze a single person (top-
down approaches). One bottom-up approach utilized in numerous clinical studies (e.g.12–15) is OpenPose16.
OpenPose locates keypoints in the image (e.g. joint locations, ﬁnger keypoints and some facial keypoints) for
all people in the scene, and groups them by individuals. However, if multiple people are visible in a scene,
then additional work is required to select the subject of interest. In contrast, top-down approaches require the
person of interest to already be localized in each frame. For analyzing movement, it is critical to consistently
and accurately localize that particular person throughout the video. Thus, the ﬁrst step is running a tracking
algorithm, and if there are multiple people, selecting the subject for analysis with subsequent algorithms.
An additional beneﬁt of top-down approaches are that the keypoint accuracy tends to exceed bottom-up
approaches.

Top-down algorithms include a range of approaches with outputs that are either two- or three-dimensional.
Two-dimensional algorithms localize a number of keypoints, such as the location of major joints in the image.
A strength of 2D keypoint detection is that the algorithms are fairly mature and robust. However, to the
best of our knowledge, there are no large-scale systematic evaluations of this robustness in clinical contexts
or on patient populations. Therefore, it remains to be seen how well they will generalize to these situations.

Three-dimensional approaches include those that predict 3D joint locations, of which there are two common
approaches. The ﬁrst approach is “lifting” 2D keypoints into 3D coordinates17,18. Lifting algorithms are
trained on datasets of paired 2D and 3D data and use the implicit prior distribution over body conﬁgurations
learned from training data to resolve the inherent ambiguities involved in going from 2D to 3D. The second
approach estimates the parameters of a body model, such as the Skinned Multi-Person Linear Model (SMPL)
model19, including body shape and pose. Frequently, a neural network is trained to directly predict these
parameters from an image or video20–22. While these approaches provide a rich description of the joint angles
and body shape, 3D accuracy often lags behind lifting approaches (e.g.23). Optimization based approaches
can reﬁne model ﬁtting, but are much slower than regression approaches24,25. The representation of pose
is less sensitive to the perspective the video was recorded from in 3D approaches, which provides a unique
advantage. However, none of these 3D approaches produce the joint angle representations recommended
by the International Standard of Biomechanics26,27, although we have previously shown that the SMPL
parameters for the arm can be converted to this format28.

Overview of our approach

Utilizing the evolving landscape of HPE tools, particularly when managing large numbers of videos, presents
several barriers. The ﬁrst is managing the dependencies between algorithms. For example, the bounding
boxes from tracking algorithms are used in top-down 2D keypoint detection. The 2D keypoints sequences
over time from a person are then used by lifting algorithms to generate 3D keypoint trajectories. There is
no standardization over the formats of these datatypes, so typically additional processing or reformatting
is required to use the output of one algorithm as input to another. A second challenge is managing data
from diﬀerent processing stages and running them through a pipeline when analyzing large sets of videos.
Further, for each class of algorithm, there are numerous diﬀerent versions that have been released with limited
validation in clinical populations, making it hard a priori to determine the optimal combination of algorithms
for a particular question. Thus, it is important to have ﬂexible pipelines that allow analyzing videos using
diﬀerent algorithms, a task which is made additionally diﬃcult due to the lack of consistency in data formats.

2

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

To facilitate HPE use in rehabilitation research and movement science, we have developed an open-source
tool that addresses and minimizes these barriers. Video processing pipelines are created using DataJoint29,
which builds computational pipelines by managing all videos and outputs in a MySQL database, and manages
dependencies between computations. We wrote wrappers for speciﬁc implementations of the algorithm classes
previously described (bounding box tracking, 2d keypoints, lifting, and SMPL) that store data in DataJoint
using a standardized format for each step This standardization makes it easier to create a pipeline using
any particular set of implementations. Using newly developed algorithms on data is also simpliﬁed in our
method, as it only requires implementing a new wrapper that uses the consistent format, rather than creating
a complete pipeline from scratch. This approach also allows mixing algorithms implemented in diﬀerent
frameworks (e.g. Jax, TensorFlow and PyTorch). DataJoint also provides a job management system that
allows parallel computation across multiple GPUs or multiple computers with minimal overhead and no
code changes. Finally, PosePipe makes it easy to visualize the outputs, which is essential when testing HPE
algorithms on clinical populations. We have used this pipeline to analyze 10s of thousands of videos acquired
in clinical settings, something only possible due to PosePipe.

Having HPE results in DataJoint provides additional beneﬁts, as it is a very eﬀective general data analysis
tool with wide adoption in the neuroscience community30,31. By associating videos with experiment-
speciﬁc, DataJoint schemas, subsequent analysis becomes much easier (e.g., plotting longitudinal summary
statistics from subjects can be done with only a few lines). A central database is also beneﬁcial because
multiple researchers on separate computers can easily collaborate and access the same data with a consistent
organization and processing system. Code for PosePipe can be found at https://github.com/peabody124/Po
sePipeline/.

Contributions
In short, our contributions in this work are:

• We describe and release PosePipe, a DataJoint based pipeline for HPE that facilitates large scale

analysis of videos acquired in a clinical context

• We provide wrappers to several state-of-the-art algorithms for HPE including for bounding box
tracking, 2D keypoint estimation, 3D lifting, and estimating SMPL meshes. Implementing wrappers
for newly released algorithms is substantially easier than creating custom pipelines to test algorithms.
• We describe the pros and cons of diﬀerent algorithm types and implementations and give examples

of the types of errors that occur when using these algorithms on clinical populations.

Methods
PosePipe is written in Python and uses DataJoint29 for data and computation management. We brieﬂy
highlight several pertinent details of DataJoint that are important for understanding PosePipe, but refer to
the documentation for details. Under the DataJoint model, Python classes that inherit from DataJoint base
classes have a corresponding table in a database. DataJoint classes can be of several types including: Manual,
Lookup, and Computed. As names suggest, Manual corresponds to rows manually entered into the database
(e.g. by uploading videos), Lookup corresponds to a lookup table and is commonly used to indicate speciﬁc
computation types, and Computed are tables computed based on existing parent data in the database. When
required inputs are available, dependent rows in the database are automatically computed (or populated)
with the populate method on a Computed table, which can also be performed for many entries in parallel
using the job management system built into DataJoint.

The relationship between the classes (i.e., tables) must correspond to a directed acyclic diagram (DAG) and
is described in the Python class deﬁnitions and is enforced in the database through foreign key constraints.
This ensures data integrity; for example, a row can only exist if the required rows in parent tables also exist.
Individual rows are identiﬁed in the database by primary keys, which are a set of ﬁelds that must be unique.
Child tables inherit the primary keys of their parents (with the foreign key constraint) and can extend the
primary key with additional ﬁelds, which allows multiple descendent rows (e.g., when using diﬀerent speciﬁc
algorithms or analyzing multiple subjects of interest in a video).

The core classes (tables) for PosePipe and dependencies are diagramed in Fig. 1, which illustrates the structure
previously described, such as 2D keypoints depending on bounding box calculations and 3D lifting keypoints
depending on 2D keypoints. We also illustrate examples of these outputs.

PosePipe Stages
Video importing Rows in the Video table Fig. 1 correspond to individual videos imported into PosePipe.
PosePipe provides optional scripts to recompress videos with a consistent codec to ensure that downstream

3

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

Figure 1: A) Diagram of PosePipe computational tree, where each node corresponds to a DataJoint class and
table in the database. Green squares correspond to manually entered rows, or rows imported by scripts outside
the pipeline. Grey squares indicate lookup tables enumerating available methods. Circles indicate computed
nodes, based on the information in parent nodes. The lookup tables allow selecting speciﬁc algorithms for
each of these nodes. Arrows show the relationship between the output nodes and example visualizations. B)
Visualization showing bounding box and 2D keypoints. C) Example SMPL mesh output. D) 3D keypoint
skeleton.

analyses can read them. The primary keys for videos are two strings: one for the ﬁlename and one for a
project name. The latter prevents potential name collisions for identically named videos from two projects,
and also makes it easy to restrict analysis to videos from a particular project.

Because storing large ﬁles such as videos in a database can result in poor performance, we use the DataJoint
attach type to store the videos externally on the ﬁlesystem. When retrieving a video from the database, it is
copied to the current working directory and the checksum is validated against the original video checksum
to ensure data integrity. This design enables additional, more granular, access control to the raw videos by
controlling which users have ﬁlesystem permissions to access the external video storage. This is a necessary
and important feature when working with clinical data where wider access might be granted to the extracted
movement trajectories stored in the database compared to the raw videos that contain identiﬁable information,
such as faces. In a situation where multiple computers need to access the raw videos, it does introduce an
additional step of ensuring the same directory is shared and available on all of the computers.

Subject of Interest Identiﬁcation Frequently, multiple people are visible in videos (i.e., patient, physical
therapist, and background individuals), but it is only necessary to perform HPE on one or some of them.
The most common solution to this challenge are algorithms which ﬁrst identify all individuals in a frame by
computing a bounding box that surrounds the individual, followed by grouping the bounding boxes over time
into tracklets. These tracklets often cannot be used immediately. For example, if there are multiple tracklets
(for diﬀerent people), it is ﬁrst necessary to identify which corresponds to the person of interest. In some
cases, the tracklets may also have two types of problems: splitting or swapping.

In the case of splitting, the subject of interest is represented by multiple tracklets at diﬀerent points of times,
sometimes with a gap. This can occur because the subject was brieﬂy occluded and the algorithm failed to
reidentify them when they reappeared. In other cases, it can occur spontaneously due to a failure of the
algorithm to detect people. These separate tracklets can be manually linked to allow consistent tracking
throughtout the video, provided any gaps are not too long. Swapping indicates that a single tracklet contains
two diﬀerent people at diﬀerent time points. In this case the tracklet cannot be used without contaminating
subsequent analyses. If the video contains additional uncontaminated tracklets at diﬀerent time points, these
can be used without issue, but the subject of interest will only be tracked for a subset of the video. The
video can also be reanalyzed with a diﬀerent algorithm, which normally show diﬀerent idiosyncrasies.

In Figure 1, TrackingBboxMethodLookup, TrackingBboxMethod, TrackingBbox, PersonBboxValid, and
interest.
PersonBbox are pipeline classes (and tables) used to track and annotate the subject of

4

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

TrackingBboxMethodLookup is a simple lookup table that enumerates the implemented algorithms.
TrackingBboxMethod are manually entered rows that specify which algorithms to compute on which videos.
TrackingBbox is a computed class that calls the wrapper for the selected algorithm indicated by the
TrackingBboxMethod entries with the speciﬁc video. We use this design pattern throughout PosePipe and it
is shown in Lst 1. Wrappers must produce the bounding boxes tracklets in a standardized format, which
often takes minimal manipulation from output of the released implementations. It is a list with entries for
each frame and each entry is itself a list of dictionaries. Each element in the dictionary contains the tracklet
ID, the bounding box coordinates and dimensions, and the conﬁdence the algorithm assigned to identifying
the person in that frame.

PosePipe provides wrappers for several algorithms. This includes MMTrack32, which provides a consistent
API to several trackers and is under active development. It also includes the released implementations
of DeepSort33, FairMOT34, TraDeS35, TransTrack36. We implemented several tracking algorithms into
our pipeline because the optimal tracking algorithm for rehabilitation subjects is an open question. Some
algorithms seem to generalize poorly to rehabilitation subjects and poor tracking precludes any subsequent
analysis.

Listing 1 TrackingBbox Listing. This shows a standard DataJoint design pattern used throughout PosePipe
to allow selecting speciﬁc algorithm implementations.

class TrackingBbox(dj.Computed):

definition = '''
-> TrackingBboxMethod
---
tracks
num_tracks
'''

: longblob
: int

def make(self, key):

video = Video.get_robust_reader(key, return_cap=False)

if (TrackingBboxMethodLookup & key).fetch1('tracking_method_name') in 'MMTrack_tracktor':

from pose_pipeline.wrappers.mmtrack import mmtrack_bounding_boxes
tracks = mmtrack_bounding_boxes(video, 'tracktor')
key['tracks'] = tracks

elif (TrackingBboxMethodLookup & key).fetch1('tracking_method_name') == 'MMTrack_deepsort':

from pose_pipeline.wrappers.mmtrack import mmtrack_bounding_boxes
tracks = mmtrack_bounding_boxes(video, 'deepsort')
key['tracks'] = tracks

elif (TrackingBboxMethodLookup & key).fetch1('tracking_method_name') == 'MMTrack_bytetrack':

from pose_pipeline.wrappers.mmtrack import mmtrack_bounding_boxes
tracks = mmtrack_bounding_boxes(video, 'bytetrack')
key['tracks'] = tracks

else:

os.remove(video)
raise Exception(f"Unsupported tracking method: {key['tracking_method']}")

track_ids = np.unique([t['track_id'] for track in tracks for t in track])
key['num_tracks'] = len(track_ids)

self.insert1(key)

# remove the downloaded video to avoid clutter
if os.path.exists(video):
os.remove(video)

5

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

After computing the tracklets, the next step is manual annotation of the subject of interest. If there is only
a single tracklet for the whole video, it can be automatically selected. However, when there are multiple
tracklets, experimenter input is needed to properly identify the one containing the subject of interest. We
implemented a simple GUI Fig. 2 that runs in a Jupyter Notebook and shows the video with the tracklets
overlaid (discussed more below). This allows the user to select one of multiple tracklets that reﬂect the subject
of interest. This information is stored as rows in PersonBboxValid with a subject ID. In most cases, we use
a subject ID of 0 to indicate the primary subject of interest or a subject ID of -1 if the video was invalid for
subsequent analysis (i.e., identity swaps or missed detection). However, this ﬁeld could also reﬂect unique
subject IDs if using PosePipe alone to study multiple individuals without additional experiment-speciﬁc
DataJoint schemas, as discussed more below. By having multiple PersonBboxValid entries per video with
diﬀerent subject IDs, it is possible to track multiple subjects in a video, such as for analyzing interaction
between patients and therapists. Based on this information, the PersonBbox rows is populated and contains
the validated tracklets for individuals used for subsequent top-down algorithms.

Figure 2: Screenshot from GUI for annotation. Bounding boxes of tracklets are shown with their identities,
allowing the experimenter to eﬃciently annotate the tracklets corresponding to the subject of interest.

2D Keypoint Detection Computing 2D keypoints using top-down algorithms depends on the PersonBbox
computed in the prior section and follows a similar design pattern Fig. 1. The supported list of 2D keypoint
algorithms are enumerated in the TopDownMethodLookup table and the user selects speciﬁc algorithms to run
by manually inserting the corresponding rows into the TopDownMethod table. Populating TopDownPerson
runs a similar function as Lst 1, where the selected algorithm determines which wrapper is called. In each
case, the wrappers use the video and bounding box for each frame to extract a cropped portion of each
image with the subject of interest centered that will be passed to top down algorithms. The output from the
wrappers are a 3D array of dimension frames × num joints × 3, where the last dimension has the keypoint x
and y coordinates and conﬁdence estimates for each keypoint. For any frame where there is no bounding box
detected for the subject of interest, the returned elements are NaN. For 2D keypoint detection, we used the
MMPose Toolbox37, which provides a wide range of state-of-the-art neural network architectures pretrained
on multiple datasets.

3D Keypoint Lifting The sequence of 2D keypoints can be ‘lifted’ to 3D with a number of algorithms that
have been trained on datasets of paired 2D-3D data in order to resolve the ambiguity of the 2D observations
and to constrain the results to plausible 3-dimensional body conﬁgurations. Again, the same design pattern
is used with LiftingMethodLookup containing the list of supported lifting methods and manually inserted
rows in LiftingMethod indicating which ﬁles and methods the user would like to ﬁll in when populating
LiftingPerson. Wrapper functions take the 2D keypoint array output from the previous step and transform
this into a 3D keypoint sequence with dimension frames × num joints × 3, where the last dimensions are the
x, y and z coordinates (lifting methods typically do not produce a conﬁdence estimate for each joint). We
implemented a wrapper for GAST-Net23, which lifts a sliding windows of 27 frames into 3D joint locations.

6

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

SMPL Fitting The SMPL body model19 is parameterized with 10 parameters to describe the body shape
and the rotation at 23 joints, each of which has 3 degrees of freedom. There are an additional 6 degrees of
freedom to capture the overall body rotation and position. It is worth noting that this is over-parameterized
compared to the typical human body. For example, a real knee and elbow do not have 3 degrees of freedom.
The SMPL-X model25 is an extension of SMPL with more degrees of freedom in the hands and face to more
expressively capture human movement.

PosePipe implements this class of algorithms with the same design pattern used above: the SMPLMethodLookup
table lists the speciﬁc algorithms supported to estimate the parameters, SMPLMethod is manually inserted
into the database to indicate that the user wants to analyze a particular video with a particular algorithm,
and SMPLPerson is populated using the bounding box information and video to estimate the parameters
using the selected algorithms. Wrappers for various algorithms that estimate SMPL or SMPL-X parameters
are implemented, which use a similar approach to the 2D keypoints by passing the cropped frames from
the video to the selected algorithms. The returned values from the wrappers include the model type, body
shape as a function of time, body pose as a function of time, the 3D keypoint positions and 2D keypoints
after reprojecting the 3D keypoints into the image via the camera model, and the camera model used during
video acquisition. Diﬀerent algorithms use inconsistent rotation representation (e.g., rotation vectors passed
through the Rodriguez equation, quaternion rotations, rotation matrices, and 6D representations), which
are standardized by the wrappers to a rotation vector. Additionally, the mathematical representation of
the camera model varies between algorithms from a weak perspective model (e.g., HMR20) to a full camera
model (e.g.,38), which is relevant because this can change the accuracy of the ultimate inference and when
producing the visualizations described below.

We implemented wrappers for several recent state-of-the-art SMPL or SMPL-X parameter inference algorithms.
VIBE22 processes video sequences rather than individual frames to estimate SMPL trajectories. PARE
uses an attentional mechanisms to improve alignment between body segments in the image and the body
model reconstruction, although processes frames independently39. Expose40 and a recent successor PIXIE41
processes individual frames to estimate parameters of an SMPL-X model, which enables tracking ﬁner scale
changes like ﬁnger movements and facial expression. PIXIE uses a similar attentional mechanism with an
SMPL-X model to improve the accuracy of hand tracking over Expose41. ProHMR42 produces a probability
distribution over body poses for each frame which can be ﬁne-tuned based on the alignment to 2D keypoint
estimates, producing accurate ﬁts at the expense of computation time. Finally, HuMoR takes an alternative
approach by optimizing the body model parameter trajectory over time to accurately match the detected 2D
keypoints43. We implemented this many algorithms because the ﬁeld is advancing incredibly rapidly11 and
diﬀerent approach have diﬀerent strengths and might be appropriate depending on the question.

Visualization
Visualizing the output of diﬀerent stages in the pipeline is critical to identify failure modes and ensure these
algorithms generalize reliably to clinical populations. Because PosePipe is also designed for data collected in
clinical settings, a default behavior to preserve privacy – such as obscuring faces – is an important feature.
OpenPose16 is a popular and eﬃcient bottom-up HPE algorithm that detects keypoints, including facial
ones, for all people in a frame. Populating the OpenPose table computes these keypoints for all videos and
allows populating the BlurredVideo table, which places a circle over all faces detected by OpenPose. Like
the Video table, the video ﬁles from all visualizations are stored in external storage. The BlurredVideo is
then used to create overlays from other algorithms such as computing TrackingBboxVideo, which show all
of the tracklets and are used for manual annotation Fig. 2, computing TopDownPersonVideo to show the
bounding box for the person of interest along with the detected 2D keypoints, or SMPLPersonVideo to show
the estimated body mesh shape overlaid on the video. The visualizations are implemented with a consistent
API that only needs to be provided a callback that takes in the individual frame and frame index and then
overlays the desired information. This tool is also useful when producing visualizations with additional
analyses to estimate clinically relevant parameters, for example gait event timing. It is also worth noting in
the PosePipe data schema Fig. 1 that the HPE algorithms results do not depend on the visualizations, so
they can be not computed or deleted if space is a concern.

Organization of algorithms and weights
There are several additional challenges to using multiple released algorithms. Firstly, the code is often
architected primarily for evaluating performance on one or several datasets, and not for being called from
other software like a typical library. This includes the challenge that multiple implementations may use
identical directory names, which can prevent Python from correctly identifying them if all are added to the
same path. Many algorithms also have a large number of parameters that must be conﬁgured and passed into
the script from the command line, which further hinders calling them from external tools. Secondly, deep

7

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

learning algorithms are only partially speciﬁed by their code and also require the weights determined after
training algorithms. These are often not distributed directly with the source code and must be downloaded
separately. In general, both the code and weights cannot be universally distributed as a monolithic bundle as
they may have license requirements that must be respected. For example, one must register and agree to the
license to download the weights for the SMPL/SMPL-X body models.

PosePipe reduces these challenges through two approaches. First, the user conﬁgures PosePipe with a list
of paths where each of the algorithms has been downloaded locally. PosePipe then transiently adds the
speciﬁc algorithm to the path when running it and removes it when complete. This prevents any namespace
collision that would occur if all algorithms were added to the path at once. To handle the weights, the
user must download them to a subdirectory in the PosePipe installation (named 3rdparty). The wrappers
specify the path within the subdirectory and pass them to the initialized models, which avoids having to
follow algorithm-speciﬁc installation instructions for weights. Finally, the wrappers also provide the list of
conﬁguration options compatible with those weights and avoid the need for passing command line parameters
to external scripts.

Running the pipeline
The components described make up stages in a particular pipeline for analyzing videos for HPE. An example
of a common pipeline might look like: 1) import videos, 2) run bounding box detection, 3) annotate subject
of interest, 4) run 2D keypoints detection, 5) run 3D keypoint detection, 6) produce visualization. A speciﬁc
pipeline can be implemented using PosePipe with a short script, as shown in Lst 2. We refer readers to
the DataJoint documentation for details about the syntax. In this example, videos are imported directly
from a source directory. In most of our use of PosePipe, videos to be analyzed have additional information
pertaining to the experimental question and we use additional DataJoint schemas to organize the videos as
part of the video import step. This script can easily be modiﬁed to run in parallel on multiple GPUs by
simply adding reserve=True to the parameter of each populate method.

Results

In our work, we found PosePipe is an eﬀective tool for analyzing tens of thousands of videos acquired in
clinical settings. It greatly reduces the barriers to testing emerging state-of-the-art algorithms for use in
HPE, as this can be done by only writing the wrapper to the novel algorithms rather than creating de novo
pipelines between all the requisite components. By storing the ﬁnal outputs in DataJoint with an associated
experiment-speciﬁc schema, PosePipe makes subsequent analysis for clinical variables of interest signiﬁcantly
easier. Because this manuscript is primarily to introduce and describe PosePipe, we will show some sample
outputs, including errors, and note some qualitative trends. However, we do not attempt a quantitative
analysis comparing speciﬁc algorithms.

Subject tracking In most videos, especially videos with an unobstructed view of the whole person
throughout, we found that bounding box estimation produced a single tracklet that uniquely mapped to the
subject of interest. However, several types of error were also noted Fig. 3. The most problematic type of
error was when the tracklet swaps from tracking the person of interest to another person. This is particularly
common when a subject and therapist are working closely together, and one brieﬂy occludes the other. In
these cases, unless it is a brief tracklet that can simply be discarded, we typically mark the tracking output as
invalid and process the video with a diﬀerent algorithm. As diﬀerent algorithms have diﬀerent idiosyncrasies
this usually produces a usable output. For videos where the subject is brieﬂy occluded, multiple tracklets are
sometimes produced, but the annotation GUI allows stitching these together. In rare cases, people would
fail to be detected despite being clearly visible. This seems to occur most commonly with assistive devices,
especially children with assistive devices, and was more pronounced when using FairMOT34. Related to this,
when people are sitting in wheelchairs, they are less likely to be detected or the bounding box may only
include the upper torso (example shown in Fig. 4). From these error observations, our typical initial tracking
algorithm is DeepSort33 as it quite reliably detects people and tracks them smoothly.

Top Down 2D Keypoint Detection For top down 2D keypoint detection, we use two pretrained
algorithms from the MMPose Toolbox37. One algorithm detects the main body joints and several facial
keypoints, which uses an HRNet architecture44 and a distribution-aware coordinate representation45 trained
on the COCO keypoints (ankles, knees, hips, shoulders, elbows, wrists, eyes, ears and nose)46. The other uses
the same architecture but is trained on the COCO-WholeBody47 dataset, which includes 68 facial keypoints
and 42 on the hands to produce much more ﬁne-grained tracking. We noted that people interacting closely
also can introduce errors at this step. These algorithms are fairly reliable, provided the bounding box was
detected accurately and the joint is not visually obscured. However, errors do occur when limbs appear

8

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

Listing 2 Example pipeline that produces an example from each of the algorithm classes.
# insert videos into database
video_path = '/path/to/files'
videos_files = os.listdir(video_path)
for v in video_files:

insert_local_video(v, os.path.join(video_path, v), video_project='PROJECT_NAME')

# run tracking algorithms
keys = (Video - TrackingBboxMethod).fetch('KEY')
tracking_method = (TrackingBboxMethodLookup & 'tracking_method_name="MMTrack"') \

# find videos without bounding boxes computed

.fetch1('tracking_method')

TrackBboxMethod.insert([k.update({'tracking_method': tracking_method}) for k in keys])
TrackingBbox.populate() # compute all of the tracking boxes

# prepare blurred video for overlays
OpenPose.populate()
BlurredVideo.populate()

# annotate videos
TrackingBboxVideo.populate()

#### run annotation GUI here, or automatically compute if only one person is in videos ###

PersonBbox.populate()

# and compute the final bounding box for subjects of interest

# compute 2D keypoints on videos
keys = (PersonBbox - TopDownMethod).fetch('KEY') # find videos without top down method selected
top_down_method = (TopDownMethodLookup & 'top_down_method_name="MMPose"').fetch1('top_down_method')
TopDownMethod.insert([k.update({'top_down_method': top_down_method}) for k in keys])
TopDownPerson.populate() # analyze videos using selected algorithm

# find videos waiting to be processed with lifting algorithm and run them
keys = (TopDownPerson - LiftingPersonMethod).fetch('KEY')
lifting_method = (LiftingMethodLookup & 'lifting_method_name="GastNet"').fetch1('lifting_method')
LiftingPersonMethod.insert([k.update({'lifting_method': lifting_method}) for k in keys])
LiftingPerson.populate() # analyze videos using selected algorithm

# find videos waiting to be processed with SMPL algorithm
keys = (PersonBbox - TopDownMethod).fetch('KEY') # find videos with no SMPL method selected
smpl_method = (TopDownMethodLookup & 'top_down_method_name="VIBE"').fetch1('top_down_method')
SMPLPersonMethod.insert([k.update({'smpl_method': smpl_method}) for k in keys])
SMPLPerson.populate() # analyze videos using selected algorithm

# produce visualizations
TopDownPersonVideo.populate()
LiftingPersonVideo.populate()
SMPLPeronVideo.populate()

diﬀerent than able bodied adults, such as thin limbs wearing braces or some prosthetic users. In these cases,
a custom model can be trained with DeepLabCut48,49, which allows manually annotating the location of
prosthetic joints. A custom algorithm can be created and entered in TopDownPersonLookup and can then be
used to combine the keypoints from MMPose of the intact joints with the prosthetic joint locations from
DeepLabCut. Examples of these algorithms and examples of errors are shown in Fig. 4.

3D Keypoint Lifting. GAST-Net23 produces realistic appearing 3D keypoint trajectories, provided the
bounding box and 2D keypoints were visible and detected accurately. These trajectories are also quite smooth,
likely due to the combination of information over multiple frames. However, because of the scale ambiguity
from pure 2D keypoints, the joint locations are only relative and do not scale to the individual. Example

9

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

Figure 3: Samples frames analyzed with diﬀerent tracking algorithms. A-C) Show an identity swap occurring
after a brief obstruction. D and E) are processed with FairMOT and shows how in some frames where the
subject of interest is not detected and false positives from the walker wheels. F and G) analyze the same
video without showing those errors.

Fig. 5 shows an example stick ﬁgure reconstruction of a subject walking, clearly showing the concordance
between the frames and body positions.

SMPL Fitting Estimating SMPL parameters produced mixed results. In many cases, the results were
promising, but sometimes contained notable examples of brittleness (i.e. sensitivity to irrelevant features)
Fig. 6. For example, with VIBE22 we noted instances where the presence of assistive device caused signiﬁcant
errors. PARE39 makes estimation of body models more robust to occlusion through an attentional mechanism,
and seemed to reduce this sensitivity. Expose40 and PIXIE41 uses the SMPL-X model and estimates
parameters for the shape of the hand. PIXIE is more recent and produces more consistent results, although
both outputs contain high frequency jitter in the details and particularly struggled when the video contained
motion blur artifacts or additional hands nearby. Another approach that improves the alignment of the mesh
output with limbs in the image is ProHMR42. It infers a probability distribution over poses for each frame
which can be combined with the robustly estimated 2D keypoints to compute optimized SMPL parameters
that are consistent with both the image and keypoints. This has the beneﬁts of capturing nuance that
is not contained in the 2D keypoint location of major body joints (e.g., wrist supination and pronation),
while reducing gratuitous errors. However, the additional optimization step takes several seconds per frame,
making analyzing many videos with this algorithm very time consuming. It also analyzes individual frames
independently, so can contain jitter. HuMoR43 is another algorithm that uses an optimization approach to
2D keypoints and speciﬁcally optimizes a trajectory over time. The outputs from HuMoR are very smooth

10

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

Figure 4: A-C) Examples of top-down keypoint detection including several examples of failures (D and E).
C) also demonstrates WholeBody keypoints including dense annotation of the hand and face. D) shows an
example where legs are missed due to bounding box detection missing the legs. E) shows an example where
the right prosthetic is missed. F) a DeepLabCut model trained on prosthetic joints can resolve this.

and well aligned to the body, although in rare cases it fails to converge to a good solution. It also can take
multiple hours to optimize the pose trajectory for a video.

Experiment-speciﬁc schema The pipeline can be easily used with experiment speciﬁc DataJoint schemas
that help perform and organize subsequent analyses Fig. 7. In this example case shown, the Subject table
contains rows with information for each subject, and the Activity table contains rows for each activity they
performed and is linked to entries in the Video table. SMPL trajectories are computed using the pipeline
we have described and stored in SMPLPerson . The FtnStatistics and RamStatistics classes compute
summary statistics of speciﬁc activities (in this case performing ﬁnger to nose movements or rapid alternating
movements). The link to the Activity table ensures the speciﬁc analysis are only performed on appropriate
activities and makes it easy to retrieve the right data. For example to retrieve the frequency of movement at
diﬀerent time points for an individual, it is as simple as:

timepoints, frequency = (FtnStatistics & 'subject_id=5') \
.fetch('timepoints', 'frequency')

Discussion

We developed PosePipe as a simpliﬁed, easy to use method for HPE analysis on large volumes of videos
acquired in clinical situations. We ﬁnd this system makes it much easier to test and compare diﬀerent
algorithms on videos within a consistent framework. This comes from using DataJoint as the framework for
PosePipe29, which stores the results of each stage of the HPE pipeline in a database while managing all of

11

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

Figure 5: A-C) Several frames of someone walking with bounding box and 2D keypoints that correspond to
the beginning, middle and end of the trajectory shown below. D) The 3D skeleton trajectory lifted using
GAST-Net showing the gait cycle. Note that the skeleton appears accurate. despite being viewed orthogonally
to the camera perspective. Red corresponds to the left side and blue the right.

the computational dependencies and ensuring data integrity. As shown by Lst 2, videos can be analyzed
in a custom pipeline with only a short script. The tables in DataJoint enforce a consistent data format to
represent each class of algorithm (e.g., bounding box tracklet computation) and newly released algorithms
can be supported simply by writing a wrapper to the algorithm compatible with this data format.

The accuracy of the outputs from PosePipe depends upon the accuracy of speciﬁc implementations and
their performance when combined. Our goal with this work is very intentionally not to produce or train a
state-of-the-art algorithm, but to build a tool that advances the use of new HPE algorithms for clinical and
translation research, with a particular focus on rehabilitation. A systematic evaluation of the accuracy of
speciﬁc components applied to clinical populations, while extremely important, is outside the scope of this
work. However, our qualitative results oﬀer several tips and warnings for using various HPE algorithms for
clinical and translation research. Our broadest and most strongly recommended tip is the importance of
rendering and reviewing the visualizations of diﬀerent algorithms, rather than blithely trusting the outputs.
While HPE tools are advancing rapidly, they are still not completely reliable; this is particularly true when
they are applied to clinical populations that may be systematically diﬀerent from the data the algorithms
were trained on.

Subject tracking For tracking algorithms, identity swaps of the tracklets are most common when two
individuals are in close proximity (e.g., patient and therapist) and can render the output unusable in some
cases. Frequently, reprocessing a video with identity swaps with a diﬀerent algorithm produces a better result.
Fragmented tracklets are a lesser problem, but these increase the time needed to manually annotate the
subject of interest in videos and are an obstacle to high throughput, fully automated approaches. Detection
gaps commonly occur when an individual is transiently not detected, either due to occlusion or because of an
algorithm error. These failures to detect seem to occur more frequently when people use an assistive device
and this was particularly notable with FairMOT34. Further, the use of a wheelchair sometimes results in
bounding boxes that miss the subject’s legs. Whether one algorithm is consistently the most reliable for
rehabilitation patients remains an open question. Our current impression is that each algorithm has diﬀerent
idiosyncrasies. However, we are optimistic that the continual advances in tracking for HPE will continue to

12

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

Figure 6: Examples estimating SMPL/SMPL_X meshes. A,B) Two frames processed with VIBE showing
confusion about the direction of the subject. C) In comparison, PARE did not have this error and more
accurately aligned with the limbs. D) PIXIE can capture detail hand gestures, such as a thumbs up. E,F)
Both VIBE and PIXIE do not always capture the leg placement properly, although the later still captures
ﬁne hand movements. G) ProHMR can improve the alignment of the mesh joints with the image. H) HuMoR
shows particularly good tracking of the joints with smooth trajectories.

Figure 7: Examples of an experiment speciﬁc schema. The complete PosePipe diagram from Fig 1 is
abstracted into the blue box and additional nodes indicate the data organization for a particular experiment

13

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

lessen this problem (e.g.50,51), with the caveat that underrepresentation of people with disabilities in the
publicly available training datasets may bound the performance when they are applied for those who might
beneﬁt the most from this technology.

Top down 2D keypoints For 2D keypoint detection, we used the MMPose toolbox37, which oﬀers many
beneﬁts including a standardized API to use multiple diﬀerent cutting-edge architectures trained on a range
of datasets and the fact it is actively maintained, with new algorithms routinely added. In our results, we
have primarily utilized their implementation of a HRNet trained on the COCO body keypoints (ankles,
knees, hips, shoulders, elbows, wrists, eyes, ears and nose)46 and ﬁnd this generally works quite well for most
cases tested. It primarily fails when tested on people with limbs that do not resemble an intact limb of
an able-bodied person, such as amputees using a prosthetic device and particularly for people with more
proximal or bilateral amputations. In this case, the visual diﬀerence between a prosthetic and intact limb
makes it unlikely that any contemporary algorithm trained on able-bodied data alone will succeed in this
task, because the neural networks trained to solve this task are designed to recognize relatively low level
visual properties rather than reason about the functional homology of a prosthetic and intact limb. As such,
we found it was occasionally necessary to use a tool like DeepLabCut48,49, which allows training a custom
detection algorithm after manually annotating prosthetic joints in some videos. We have previously found
this is also required when videos do not include the upper torso of people52. In general, we see 2D keypoint
trajectories an impoverished representation for the true 3D biomechanical movement a subject performs in
the world. This is unfortunate as they are some of the most robust HPE algorithms available. In many
works, these estimates are referred to as a 2D pose, but avoid this nomenclature as the true pose cannot be
recovered directly from these estimates.

Lifted 3D joint locations When the 2D keypoints were accurately detected and the person was fully
visible, the lifted 3D keypoint trajectories appeared accurate. It is worth noting that lifting algorithms are not
calibrated to match the height of an individual, which is particularly relevant when analyzing the movements
of children who will be scaled towards an adult. As mentioned, 3D joint locations cannot be recovered
analytically from 2D keypoints. This is because any 3D location along an epipolar line from the camera
would project to the same point in the image plane, creating a fundamental ambiguity. Lifting methods
address this ambiguity by not just trying to ﬁnd 3D points consistent with these epipolar lines, but also those
consistent with plausible human conﬁgurations. Essentially, they learn a priori over poses from the training
data. This raises the concern that they may exhibit biases when tested on people who move diﬀerently than
the able-bodied population these algorithms were trained on, such as people with range of motion restrictions
or people with dystonic cerebral palsy or other movement disorders.

SMPL Methods Algorithms that estimate parameters of body-models such as SMPL19 or SMPL-X25
are particularly promising as they provide an inference that is closer to the biomechanical understanding
than 3D joint locations by ﬁnding the joint angles or pose that recreates the body conﬁguration in the
image. However, the SMPL/SMPL-X models were developed with a focus on realistic computer graphics
and producing an accurate external body shape rather than with a detailed biomechanical focus, so there
is not a 1:1 mapping from these parameters to the International Standard of Biomechanics recommended
descriptions26,27, although we have previously shown these can be computed from the SMPL parameters for
the arm28.

A limitation of these algorithms is they sometimes have signiﬁcant errors, particularly in the presence of
occlusions. Because of this, we ﬁnd visualizing the outputs prior to using them is a critical step. More
recent methods that include attentional mechanisms, like PARE39, are more robust to occlusions. With the
exception of VIBE22 and HuMoR43, the methods we tested analyze frames independently which also results
in increased jitter. Most also do not typically produce conﬁdence estimates to know when they are accurate
and can be trusted. An exception is ProHMR42 produces a probability distribution over poses that can be
reﬁned using detected keypoints. This produces very promising results for HPE analysis as it reduces these
inconsistencies by optimizing the parameters to align the mesh to the keypoints, but the time to run this is
quite limiting and it tends to have a fair amount of jitter between frames. Expose40 and PIXIE41 are useful
for questions involving hand function with the later being more accurate in our experience, but these also
tend to have some jitter that limits analyzing detailed hand trajectories. If expressive hand movement isn’t
required, HuMoR43 produces excellent results that are very smooth, although the time required to optimize
the trajectories can be prohibitive.

Need for systematic evaluation With the increasing use of deep learning systems in medicine, there is a
concern that they function as a black box and the need for Explainable Artiﬁcial Intelligence (XAI) has been
emphasized53,54. XAI has multiple components and perspectives55, with one notion being that intermediate

14

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

steps produce outputs that can be meaningfully interpreted and reviewed, as well as understanding the
inﬂuence of those intermediate steps on the ﬁnal output. The trajectory of a subject’s movement – regardless
of the speciﬁc representation – is typically an input used to compute clinically pertinent metrics (e.g., gait
cadence and deviation index13, gait temporal parameters52, or Parkinson’s Disease motor symptom severity56).
They also have the beneﬁt of being very interpretable and can be easily checked with visualizations. Thus, to
improve the explainability of these systems, it is critical to know how accurate the pose estimate inputs are
and how that accuracy inﬂuences overall algorithm reliability.

Given the potential beneﬁt of these algorithms for rehabilitation research and outcome measures, there is a
substantial need for large-scale validation studies of these algorithms when applied to clinical populations.
Quantitatively evaluating the accuracy of HPE algorithms on any clinical populations is challenging because
there are few datasets that reﬂect real world use cases with ground truth annotation. These are challenging to
collect because ground truth using optical motion tracking requires bringing subjects into a motion analysis
laboratory. Advances in wearable sensors or pose estimation with multiple cameras could reduce this barrier
while producing suﬃciently accurate annotation.
Additionally, these validation studies are important for understanding how algorithmic fairness57–59 interacts
with people with disabilities60. For example, our observations that the use of assistive devices, such as a
walker, seems to reduce the probability that a person (particularly a child) is detected or that in some cases
prosthetic limbs are poorly tracked. One contribution to this type of bias may be the underrepresentation of
people with disabilities in the datasets used to train the algorithms. It is also important to note that dataset
bias is only one source of algorithmic bias. Because applying these algorithms to clinical populations may
be the situation where they can provide the most societal beneﬁt, it is critical to understand and address
sources of any systematic errors.

Facilitating analysis of HPE outputs
PosePipe makes analysis of the HPE outputs for the primary research questions much easier. DataJoint
provides an eﬃcient mechanism to allow multiple users to easily access the outputs from multiple computers
(if enabled) and beneﬁts from all the development making MySQL highly performant. DataJoint provides a
pythonic API to retrieve the speciﬁc data desired that maps to MySQL queries. For example, retrieving a
SMPL pose trajectory for a speciﬁc video over a remote connection takes a negligible amount of time. This
also avoids recurring challenges associated with using a ﬁle system for data organization. DataJoint allows
granular user access controls, including assigning users read-only access to certain tables, which can prevent
them from accidentally deleting raw data. Controlling access to the external data stores also determines who
can or cannot view the raw videos and/or visualizations, which may help to restrict access to identiﬁable
information.

In many cases, videos are collected with additional metadata and experimental information that is critical
for subsequent analysis of the HPE results from those videos. In these cases, we recommend designing an
experiment-speciﬁc DataJoint schema that organizes the videos accordingly and is used to insert them into
PosePipe (as opposed to Lst 2 which imports them directly from a directory organized only by ﬁlename).
Experiment-speciﬁc schemas provide a tremendous beneﬁt. For example, there may be a table containing
demographic or clinical information about subjects and another table that computes a clinically relevant
feature from the 3D keypoints, such as walking speed. In this case, a one-line query can return the walking
speed of a subject associated with their demographic data and the data of measurements in a Pandas
dataframe61. We defer further speciﬁc examples to subsequent manuscripts using outputs from PosePipe.

Limitations
In addition to limitations on accuracy from the individual components just discussed, PosePipe itself has
several limitations. Setup requires several steps. These include setting up a DataJoint database, which is
fairly straightforward using the provided Docker container. It also includes downloading the code and model
weights for each of the algorithms the user wishes to apply to videos. Detailed installation instructions for all
steps are provided in the PosePipe repository.

The facial blurring is an important default privacy-preserving feature, but is only as reliable as the face
detection algorithm used (currently OpenPose). This makes additional manual review an important step
before releasing any videos requiring anonymization.

The DataJoint model also introduces some friction when interacting with experiment-speciﬁc schemas. For
example, videos may be conceptually organized as children of other tables, such as a table of experimental
sessions which itself is a descendent from a table of clinical subjects with demographic information. However,
when working with multiple experiments it is not possible for some Video rows to have a parent from one

15

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

experimental table and in other cases a diﬀerent parent table. One future option is to adopt DataJoint
Elements62, which allows creating reusable pipelines that can be attached to diﬀerent experimental tables.

Future Directions
Consistent with our goal to facilitate HPE in research, an important future step is to simplify the installation
process. We envision doing so by developing a Docker container with all the required ﬁles for a minimal
pipeline, including the DataJoint database and a minimal set of algorithms that could be distributed consistent
with their licenses. This will likely heavily leverage implementations provided by MMPose37 and MMTrack32,
which provide installable libraries and numerous competitive pretrained algorithms that can be accessed with
a straightforward API.

Further, we hope to support new classes of algorithms that are becoming available. We are particularly
excited about approaches that integrate physics-based biomechanical modeling to constrain estimates to more
physically plausible ones while also inferring joint torques and ground reaction forces66, although these will
also require substantial amounts of validation. We also plan to include action recognition algorithms in future
revisions67.

Additionally, we will use PosePipe to perform systematic evaluations on the performance of speciﬁc algorithms
on videos of rehabilitation subjects, as described above. The lack of appropriate datasets with ground truth
annotation makes this task diﬃcult, but does not preclude having human raters evaluate the quality of
outputs, which would still allow quantitative comparison of diﬀerent algorithms. We are currently developing
tools to enable collection of simultaneous video and wearable sensor data and video data with multiple
cameras, which can provide additional signals for both evaluation and training of algorithms on data acquired
in the clinic28,68.

Most importantly, we will use our PosePipe tool to further our ultimate goal: to develop clinically useful tools
and outcome measures that can be used in wide ranging clinical contexts. High-quality HPE analysis, even
if it produces near-perfect biomechanical understanding, will need additional analysis to produce clinically
useful measures. As an example, when performing gait analysis, the ﬁrst step is detecting the timing of gait
events, such as foot contact and toe oﬀ, and then, after aligning the gait cycle, additional statistics can be
computed and compared to normative datasets69,70. Thus, advancing the clinical utility of HPE will also
require developing similar high-level clinical measures and validating the properties of these measures on
clinical populations.

Conclusion

We introduce PosePipe, an open-source tool based on DataJoint that makes it easy to implement pipelines to
analyze videos of human movement acquired in clinical situations using cutting-edge algorithms for human
pose estimation. It supports several released implementations from diﬀerent classes of algorithms including
bounding box tracklet computation to track an individual in videos, top-down 2D keypoint estimation, lifting
2D keypoints to 3D joint locations, and estimating the parameters of SMPL/SMPL-X body models. We
anticipate that this tool will facilitate the use of these algorithms in clinical and translational research for
movement science and rehabilitation and help enable much needed systematic evaluation of their performance
when tested on clinical populations.

Acknowledgment This work was generously supported by the Research Accelerator Program of the Shirley
Ryan AbilityLab. We would like to thank Tasos Karakostas, Arun Jayaraman, and Anothony Cimorelli for
data used in the development of this pipeline and productive discussions. We would also like to thank Dimitri
Yatsenko, Meghan OConnell and Kyle Embry for constructive feedback on the manuscript and code.

Author Contribution RJC developed PosePipe, analyzed the videos used as examples in this work, and
wrote the manuscript.

References
1.

Poitras, I. et al. Validity and Reliability of Wearable Sensors for Joint Angle Estimation: A Systematic
Review. Sensors (Basel) 19, 1555 (2019).
Cuesta-Vargas, A. I., GalÃ¡n-Mercant, A. & Williams, J. M. The use of inertial sensors system for
human motion analysis. Physical Therapy Reviews 15, 462–473 (2010).
Filippeschi, A. et al. Survey of motion tracking methods based on inertial sensors: A focus on upper
limb human motion. Sensors (Basel) 17, (2017).

2.

3.

16

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

Zheng, C. et al. Deep Learning-Based Human Pose Estimation: A Survey. 663–676 https://github.c
om/zczcwh/DL-HPE (2020).
Kwakkel, G. et al. Standardized measurement of quality of upper limb movement after stroke:
Consensus-based core recommendations from the Second Stroke Recovery and Rehabilitation
Roundtable. International journal of stroke : oﬃcial journal of the International Stroke Society
1747493019873519 (2019) doi:10.1177/1747493019873519.
Seethapathi, N., Wang, S., Saluja, R., Blohm, G. & Kording, K. P. Movement science needs diﬀerent
pose tracking algorithms. arXiv (2019).
Parks, M. T., Wang, Z. & Siu, K.-C. Current Low-Cost Video-Based Motion Analysis Options for
Clinical Rehabilitation: A Systematic Review. Physical therapy 99, 1405–1425 (2019).
Needham, L. et al. Human Movement Science in The Wild: Can Current Deep-Learning Based Pose
Estimation Free Us from The Lab? 2021.04.22.440909 https://www.biorxiv.org/content/10.1101/2021
.04.22.440909v1 (2021) doi:10.1101/2021.04.22.440909.
Liu, W., Bao, Q., Sun, Y. & Mei, T. Recent Advances in Monocular 2D and 3D Human Pose Estimation:
A Deep Learning Perspective. arXiv:2104.11536 [cs] (2021).
Chen, Y., Tian, Y. & He, M. Monocular human pose estimation: A survey of deep learning-based
methods. Computer Vision and Image Understanding 192, 102897 (2020).
Tian, Y., Zhang, H., Liu, Y. & Wang, L. Recovering 3D Human Mesh from Monocular Images: A
Survey. arXiv:2203.01923 [cs] (2022).
Sato, K., Nagashima, Y., Mano, T., Iwata, A. & Toda, T. Quantifying normal and parkinsonian gait
features from home movies: Practical application of a deep learning-based 2D pose estimator. PLoS
One 14, e0223549 (2019).
Kidzinski, L. et al. Deep neural networks enable quantitative movement analysis using single-camera
videos. Nature Communications 11, 1–10 (2020).
Stenum, J., Rossi, C. & Roemmich, R. T. Two-dimensional video-based analysis of human gait using
pose estimation. PLOS Computational Biology 17, e1008935 (2021).
Mehdizadeh, S. et al. Concurrent validity of human pose tracking in video for measuring gait parameters
in older adults: a preliminary analysis with multiple trackers, viewing angles, and walking directions.
Journal of NeuroEngineering and Rehabilitation 18, 139 (2021).
Cao, Z., Simon, T., Wei, S.-E. & Sheikh, Y. Realtime Multi-Person 2D Pose Estimation using Part
Aﬃnity Fields. (2016).
Martinez, J., Hossain, R., Romero, J. & Little, J. J. A Simple Yet Eﬀective Baseline for 3d Human
Pose Estimation. Proceedings of the IEEE International Conference on Computer Vision 2017-Octob,
2659–2668 (2017).
Pavllo, D., Feichtenhofer, C., Grangier, D. & Auli, M. 3D human pose estimation in video with
temporal convolutions and semi-supervised training. Proceedings of the IEEE Computer Society
Conference on Computer Vision and Pattern Recognition 2019-June, 7745–7754 (2018).
Loper, M., Mahmood, N., Romero, J., Pons-Moll, G. & Black, M. J. SMPL. ACM Transactions on
Graphics 34, 1–16 (2015).
Kanazawa, A., Black, M. J., Jacobs, D. W. & Malik, J. End-to-End Recovery of Human Shape and
Pose. in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition 7122–7131 (2018).
doi:10.1109/CVPR.2018.00744.
Kolotouros, N., Pavlakos, G., Black, M. & Daniilidis, K. Learning to Reconstruct 3D Human Pose and
Shape via Model-Fitting in the Loop. in 2019 IEEE/CVF International Conference on Computer
Vision (ICCV) 2252–2261 (2019). doi:10.1109/ICCV.2019.00234.
Kocabas, M., Athanasiou, N. & Black, M. J. VIBE: Video Inference for Human Body Pose and Shape
Estimation. in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
5253–5263 (2020).
Liu, J., Rojas, J., Liang, Z., Li, Y. & Guan, Y. A Graph Attention Spatio-temporal Convolutional
Network for 3D Human Pose Estimation in Video. (2020).

17

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

24.

25.

Bogo, F. et al. Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image.
in ECCV 2016: Computer Vision – ECCV 2016 561–578 (Springer, Cham, 2016). doi:10.1007/978-3-
319-46454-1_34.
Pavlakos, G. et al. Expressive Body Capture: 3D Hands, Face, and Body From a Single Image. in
2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 10967–10977
(IEEE, 2019). doi:10.1109/CVPR.2019.01123.

26. Wu, G. et al. ISB recommendation on deﬁnitions of joint coordinate system of various joints for the
reporting of human joint motion--part I: ankle, hip, and spine. International Society of Biomechanics.
J Biomech 35, 543–548 (2002).

28.

27. Wu, G. et al. ISB recommendation on deﬁnitions of joint coordinate systems of various joints for the
reporting of human joint motion--Part II: shoulder, elbow, wrist and hand. Journal of biomechanics
38, 981–992 (2005).
Cotton, R. J. Kinematic Tracking of Rehabilitation Patients With Markerless Pose Estimation Fused
with Wearable Inertial Sensors. IEEE 15th International Conference on Automatic Face & Gesture
Recognition (2020).
Yatsenko, D. et al. DataJoint: managing big scientiﬁc data using MATLAB or Python. 031658
https://www.biorxiv.org/content/10.1101/031658v1 (2015) doi:10.1101/031658.
Yatsenko, D., Walker, E. Y. & Tolias, A. S. DataJoint: A Simpler Relational Data Model.
arXiv:1807.11104 [cs] (2018).
DataJoint Team. DataJoint Projects. https://www.datajoint.com/projects (2022).

29.

31.

30.

32.

MMTracking Contributors. MMTracking: OpenMMLab. (2020).

33. Wojke, N., Bewley, A. & Paulus, D. Simple Online and Realtime Tracking with a Deep Association

34.

Metric. arXiv:1703.07402 [cs] (2017).
Zhang, Y., Wang, C., Wang, X., Zeng, W. & Liu, W. FairMOT: On the Fairness of Detection and
Re-Identiﬁcation in Multiple Object Tracking. arXiv:2004.01888 [cs] (2020).

35. Wu, J. et al. Track to Detect and Segment: An Online Multi-Object Tracker.

in 2021
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 12347–12356 (2021).
doi:10.1109/CVPR46437.2021.01217.
Sun, P. et al. TransTrack: Multiple-Object Tracking with Transformer. (2020).

MMPose Contributors. OpenMMLab Pose Estimation Toolbox and Benchmark. (2020).

Kocabas, M. et al. SPEC: Seeing People in the Wild with an Estimated Camera. arXiv:2110.00620
[cs] (2021).
Kocabas, M., Huang, C.-H. P., Hilliges, O. & Black, M. J. PARE: Part Attention Regressor for 3D
Human Body Estimation. arXiv:2104.08527 [cs] (2021).
Choutas, V., Pavlakos, G., Bolkart, T., Tzionas, D. & Black, M. J. Monocular Expressive Body
Regression Through Body-Driven Attention. in Computer Vision – ECCV 2020 (eds. Vedaldi, A.,
Bischof, H., Brox, T. & Frahm, J.-M.) vol. 12355 20–40 (Springer International Publishing, 2020).
Feng, Y., Choutas, V., Bolkart, T., Tzionas, D. & Black, M. J. Collaborative Regression of Expressive
Bodies using Moderation. arXiv:2105.05301 [cs] (2021).
Kolotouros, N., Pavlakos, G., Jayaraman, D. & Daniilidis, K. Probabilistic Modeling for Human Mesh
Recovery. arXiv:2108.11944 [cs] (2021).
Rempe, D. et al. HuMoR: 3D Human Motion Model for Robust Pose Estimation. in International
Conference on Computer Vision (ICCV) (2021).
Sun, K., Xiao, B., Liu, D. & Wang, J. Deep High-Resolution Representation Learning for Human Pose
Estimation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern
Recognition 2019-June, 5686–5696 (2019).
Zhang, F., Zhu, X., Dai, H., Ye, M. & Zhu, C. Distribution-Aware Coordinate Representation for
Human Pose Estimation. in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) 7091–7100 (2020). doi:10.1109/CVPR42600.2020.00712.

18

36.

37.

38.

39.

40.

41.

42.

43.

44.

45.

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context. in Computer Vision – ECCV 2014
(eds. Fleet, D., Pajdla, T., Schiele, B. & Tuytelaars, T.) 740–755 (Springer International Publishing,
2014). doi:10.1007/978-3-319-10602-1_48.
Jin, S. et al. Whole-Body Human Pose Estimation in the Wild. in Computer Vision – ECCV 2020
(eds. Vedaldi, A., Bischof, H., Brox, T. & Frahm, J.-M.) 196–214 (Springer International Publishing,
2020). doi:10.1007/978-3-030-58545-7_12.
Mathis, A. et al. DeepLabCut: markerless pose estimation of user-deﬁned body parts with deep
learning. Nature Neuroscience 2018 1 (2018) doi:10.1038/s41593-018-0209-y.
Nath, T. et al. Using DeepLabCut for 3D markerless pose estimation across species and behaviors.
Nature Protocols 1 (2019) doi:10.1038/s41596-019-0176-0.
Rajasegaran, J., Pavlakos, G., Kanazawa, A. & Malik, J. Tracking People with 3D Representations.
arXiv:2111.07868 [cs] (2021).
Yuan, Y., Iqbal, U., Molchanov, P., Kitani, K. & Kautz, J. GLAMR: Global Occlusion-Aware Human
Mesh Recovery with Dynamic Cameras. (2021).
Lonini, L. et al. Video-Based Pose Estimation for Gait Analysis in Stroke Survivors during Clinical
Assessments: A Proof-of-Concept Study. DIB 6, 9–18 (2022).
Holzinger, A., Langs, G., Denk, H., Zatloukal, K. & MÃŒller, H. Causability and explainability of
artiﬁcial intelligence in medicine. Wiley Interdiscip Rev Data Min Knowl Discov 9, e1312 (2019).
Amann, J. et al. Explainability for artiﬁcial intelligence in healthcare: a multidisciplinary perspective.
BMC Medical Informatics and Decision Making 20, 310 (2020).
Vilone, G. & Longo, L. Notions of explainability and evaluation approaches for explainable artiﬁcial
intelligence. Information Fusion 76, 89–106 (2021).
Lu, M. et al. Quantifying Parkinson’s disease motor severity under uncertainty using MDS-UPDRS
videos. Medical Image Analysis 73, 102179 (2021).
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K. & Galstyan, A. A Survey on Bias and Fairness in
Machine Learning. ACM Comput. Surv. 54, 115:1–115:35 (2021).
Pessach, D. & Shmueli, E. Algorithmic Fairness. arXiv:2001.09784 [cs, stat] (2020).

Chouldechova, A. & Roth, A. The Frontiers of Fairness in Machine Learning. (2018).

Trewin, S. et al. Considerations for AI fairness for people with disabilities. AI Matters 5, 40–63 (2019).

team.

Pandas 1.0.3.

(Zenodo, 2020).

pandas-dev/pandas:

The pandas development
doi:10.5281/zenodo.3715232.
Yatsenko, D. et al. DataJoint Elements: Data Workﬂows for Neurophysiology. 2021.03.30.437358
https://www.biorxiv.org/content/10.1101/2021.03.30.437358v2 (2021) doi:10.1101/2021.03.30.437358.
Yuan, Y., Wei, S.-E., Simon, T., Kitani, K. & Saragih, J. SimPoE: Simulated Character Control
for 3D Human Pose Estimation. in 2021 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) 7155–7165 (2021). doi:10.1109/CVPR46437.2021.00708.
Shi, M. et al. MotioNet: 3D Human Motion Reconstruction from Monocular Video with Skeleton
Consistency. ACM Transactions on Graphics 40, (2020).
Shimada, S., Golyanik, V., Xu, W., PÃ©rez, P. & Theobalt, C. Neural Monocular 3D Human Motion
Capture with Physical Awareness. arXiv:2105.01057 [cs] (2021).
Shimada, S., Golyanik, V., Xu, W. & Theobalt, C. PhysCap: Physically Plausible Monocular 3D
Motion Capture in Real Time. arXiv:2008.08880 [cs] (2020).
Gu, C. et al. AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions.
in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition 6047–6056 (2018).
doi:10.1109/CVPR.2018.00633.
Cotton, R. J. & Rogers, J. Wearable Monitoring of Joint Angle and Muscle Activity. in 2019 IEEE
16th International Conference on Rehabilitation Robotics (ICORR) vol. 2019 258–263 (IEEE, 2019).
Richard, J., Levine, D. & Whittle, M. Whittle’s Gait Analysis - 5th Edition. (Elsevier, 2012).

46.

47.

48.

49.

50.

51.

52.

53.

54.

55.

56.

57.

58.

59.

60.

61.

62.

63.

64.

65.

66.

67.

68.

69.

19

PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical Research

70. Whittle, M. W. Clinical gait analysis: A review. Human Movement Science 15, 369–387 (1996).

20

