Review of the Assurance of Machine Learning for use in Autonomous 
Systems (AMLAS) Methodology for Application in Healthcare  

Shakir Laher1,4 *, Carla Brackstone2, Sara Reis3, An Nguyen3, Sean White1, Ibrahim Habli4 

* Correspondence: shakir.laher1@nhs.net  

  sl2318@york.ac.uk 

1. NHS Digital 
2. Kheiron Medical Technologies 
3. Health Navigator 
4. University of York 

Abstract 

In  recent  years,  the  number  of  machine  learning  (ML)  technologies  gaining 
regulatory  approval  for  healthcare  has  increased  significantly  allowing  them  to  be 
placed  on  the  market.  However,  the  regulatory  frameworks  applied  to  them  were 
originally  devised  for  traditional  software,  which has  largely  rule-based  behaviour, 
compared to the data-driven and learnt behaviour of ML. As the frameworks are in 
the process of reformation, there is a need to proactively assure the safety of ML to 
prevent patient safety being compromised. The Assurance of Machine Learning for 
use in Autonomous Systems (AMLAS) methodology was developed by the Assuring 
Autonomy International Programme based on well-established concepts in system 
safety. This review has appraised the methodology by consulting ML manufacturers 
to  understand  if  it  converges  or  diverges  from  their  current  safety  assurance 
practices,  whether  there  are  gaps  and  limitations  in  its  structure  and  if  it  is  fit  for 
purpose when applied to the healthcare domain. Through this work we offer the view 
that there is clear utility for AMLAS as a safety assurance methodology when applied 
to  healthcare machine  learning  technologies,  although  development  of  healthcare 
specific  supplementary  guidance  would  benefit 
the 
methodology.     

implementing 

those 

Keywords - Machine Learning, Safety, Assurance, Regulation, Healthcare 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1. Introduction 

The development of machine learning technologies for healthcare has seen significant growth 
in recent years. This can be witnessed through hundreds of ML-based medical devices gaining 
regulatory approval in the US and Europe between 2015 - 2020 [1]. However, these approvals 
are based on regulatory frameworks originally devised for traditional software, by which we 
mean,  software  that  is  constructed  of  rule-based  algorithms  for  a  specific  task  [2].  In 
comparison, ML software is not explicitly coded but instead developed by constructing a model 
that  is  learned  through  mathematical  algorithms  and  training  datasets  to  identify  patterns 
which  can  be  harnessed  to  make  predictions  on  previously  unseen  data.  This  makes  ML 
intrinsically  data-driven  and  stochastic  [3].  Therefore,  possessing  a  fit  for  purpose  safety 
assurance methodology for these emergent technologies will be a fundamental component in 
managing patient safety. We define safety assurance as “all planned and systematic actions 
necessary  to  afford  adequate  confidence  that  a  product,  a  service,  an  organisation  or  a 
functional system achieves acceptable or tolerable safety” [4]. 

The novelty offered by ML has led to those with policy, safety and regulatory responsibility 
needing  to appraise and  update their  existing  regulatory frameworks  and  safety  assurance 
routes  through  a  series  of  projects  and  initiatives.  Examples  of  such  work  include  the  UK 
Medicines  and  Healthcare  Products  Regulatory  Agency  (MHRA)  changing  its  regulatory 
framework through the Software and AI as a Medical Device Change Programme [5] [6]; in 
England the Care Quality Commission (CQC), as part of their regulatory sandbox project on 
the use of ML as part of a service, highlighted in their findings a need to improve their methods 
to better regulate services which include ML [7] and; the US Food and Drug Administration 
(FDA) released a discussion paper in 2019 [8] from which they now have an action plan [9] to 
begin  addressing  their  challenges.  Similarly,  the  international  standards  community  who 
develop standards that are often harmonised to regulation are embarking on similar projects 
to develop AI specific standards [10] [11]. While these nascent endeavours are being worked 
through a gap exists in how ML safety should be assured. The Assurance of Machine Learning 
for use in Autonomous Systems (AMLAS) [12] may offer a solution towards bridging this gap.   

This  paper  presents  an  appraisal  of  the  AMLAS  methodology  for  its  suitability  as  a  safety 
assurance  methodology  to  be  utilised  by  digital  health  technology  (DHT)  manufacturers. 
Specific objectives were three-fold, (1) appraise how AMLAS converges or diverges from DHT 
manufacturers’  existing  safety  assurance  practices  for  ML;  (2)  to  identify  any  gaps  and 
limitations;  and,  (3)  to  identify  key  themes  towards  healthcare  specific  supplementary 
guidance for AMLAS. It should be noted, any divergence, gaps and limitations are scoped to 
the AMLAS and does not imply manufacturers are not compliant with their existing regulatory 
obligations. 

The  rest  of  the  paper  is  organised  as  follows.  Section  2  describes  the  methods.  Section  3 
describes in summary form the AMLAS methodology. Section 4 presents the results of how 
each  AMLAS  stage  converges  or  diverges  from  existing  manufacturer  safety  assurance 
practices and includes any gaps and limitations. Section 5 presents a discussion according to 
key themes and Section 6 concludes the paper. 

 
 
 
 
 
 
 
 
 
 
2. Methods 

2.1 Manufacturer Recruitment 

Due to the nature of the expertise required to conduct this review, only those manufacturers 
deploying  or  ready  to  deploy  their  ML  technologies  were  approached  to  participate.  The 
researchers  selected  two  manufacturers  who  fulfilled  this  criterion  and  are  referenced 
throughout as Manufacturer A and Manufacturer B. 

The core technology of each manufacturer is summarised below: 

Manufacturer A – Developed a deep learning implementation of ML to assist radiologists with 
the decision to recall a patient if cancer is suspected on a mammogram. The current use case 
focuses on the ML being used as a second reader as currently all mammograms are read by 
two  readers  (i.e.,  a  double  reading  workflow).  It  is  envisaged  the  ML  component  will  be 
integrated into existing breast screening pathways. This will involve them having a need to 
deploy their ML at various healthcare organisations’ screening sites.     

Manufacturer B – Developed a deep learning implementation of ML to identify patients who 
are at greatest risk of requiring non-elective care, with the ultimate goal of preventing these 
non-elective  events 
the 
manufacturer’s employees who work directly with patients, building up their health literacy and 
empowering  them  to  take  an  active  role  in  managing  their  health.  This  manufacturer  both 
develops and deploys their technology and therefore is more analogous to a service provider.  

identified,  patients  are  coached  by 

from  occurring.  Once 

2.2 Review Instrument 

Prior to conducting the review, which was achieved through a series of workshops the lead 
researcher  formulated  a  framework  of  questions  based  on  each  stage (1-6)  of  the  AMLAS 
which  needed  DHT  manufacturer  input  (see  Appendix  A  for  the  entire  framework).  This 
approach ensured discussions focused on those salient points which needed exploration. 

Table  1  illustrates  a  sample  of  the  framework  which  showcases  how  the  questions  were 
presented and corresponding answers captured, all labelled with unique identifiers (e.g., MVA-
1, MVA-1.Q1 etc). Each heading’s purpose was as follows: 

·     Key Discussion Point – A key discussion point was extrapolated from the AMLAS 

directly quoting the text, where possible. 

·         Review Question – A question is presented linked directly to the key discussion point. 

·        Review Answer - A – Manufacturer A response. 

·        Review Answer - B – Manufacturer B response.  

 
 
 
 
 
Key Discussion Point  Review Question  Review Answer - A 

Review Answer - B 

MVA-1:  

MVA-1.Q1:  

MVA-1.Q1_A:  

MVA-1.Q1_B:  

Do you agree with 
the key discussion 
point MVA-1 as a 
sensible approach 
to verification?  

Model verification may 
consist of two sub‐
activities: test‐based 
verification and formal 
verification. For every 
ML safety requirement 
at least one 
verification activity 
shall be undertaken. 

Yes, we currently 
verify our models 
with test-based 
verification 

At each new site we 
engage with we run 
a pass of our current 
ML model. 
Depending on 
performance we will 
calibrate with this 
new site data if 
needed. Additionally 
we have run and 
intend to run formal 
clinical investigations 
on performance in 
both a double 
reading and 
standalone workflow 

Table 1: AMLAS Review Questions Framework  

2.3 Data Collection 

Online workshops consisting of 3 x 1hr were conducted with each manufacturer separately to 
collect  the  data.  Each  workshop  was  attended  by  the  lead  researcher,  research  team 
members  and  representatives  from  the  manufacturer.  Online  document  templates,  as 
described  in  2.2,  were  set  up  for  each  manufacturer.  These  were  populated  during  the 
workshops and offline as some questions required input from the wider organisation.  

2.4 Data Analysis 

On  completion  of  the  data  collection  phase,  the  lead  researcher  combined  the  data  into 
aggregate form to begin analysis. Since the sample size was small, specific coding was not 
utilised.  Instead,  each AMLAS  stage  has  a  convergence,  divergence  and gaps &  limitation 
heading where each manufacturer's responses have been reported thematically. A convention 
adopted for clarity was as follows. If the combined responses reached agreement, they have 
been reported in the results section (4) as “both manufacturers …”. Where there were no clear 
themes  or  agreement  amongst  the  manufacturers,  we  reported  them  specifically  as 
“manufacturer A/B …”  

 
 
 
 
 
 
 
 
 
 
 
3. The Assurance of Machine Learning for use in Autonomous Systems 

3.1 AMLAS Stages 

AMLAS is a safety assurance methodology for autonomous systems which aims to integrate 
safety assurance during the development of ML components. For this reason, it is primarily 
constructed of iterative stages that resemble a typical ML engineering life cycle. There are six 
stages in total; (1) ML safety assurance scoping, (2) ML requirements, (3) Data management, 
(4) Model learning, (5) Model verification and (6) Model deployment. Figure 1 illustrates the 
six  stages.  There  is  a  prerequisite  to  stage  1,  which  is  to  establish  the  system  safety 
requirements  that  are  used  as  an  input  into  AMLAS.  This  is  due to  AMLAS  taking  a  whole 
system safety approach even though the primary focus of the assurance methodology is on 
the ML component. AMLAS considers safety assurance to be meaningful if scoped as part of 
the wider system and operational context. The final box in Figure 1 labelled “Safety Case1 for 
ML  component”  depicts  the  final  artefact  produced  as  part  of  implementing  the  entire 
methodology and should not be considered as a stage.  

Figure 1: Overview of AMLAS 

3.2 Processes & Argument Patterns 

Each  stage  of  AMLAS  comprises  of  a  process  which  takes  in  inputs  that  are  used  by  the 
activities to produce stage outputs. Figure 2 is an example of the stage 1 process. Here, inputs 
A, B, C, D & F are all supplied to the activities 1 & 2, which output E & G. 

1 A safety case can be defined as “a documented body of evidence that provides a convincing and valid argument 
that a system is adequately safe for a given application in a given environment” [13]. The concept of a ML safety 
case  in  AMLAS  harmonises  with  current  healthcare  safety  standards  (DCB0129  [14]  &  DCB0160  [15])  which 
include a requirement to produce a clinical safety case. 

 
 
 
 
Figure 2: ML Assurance Scoping Process (Stage 1) 

Furthermore, each stage as part of its process has an activity (e.g., see Figure 2, Activity 2) 
for instantiating a safety argument pattern2 which references the artefacts of each stage. This 
gives implementers of the methodology clear guidance on how to present a safety argument 
informed by the safety work carried out at each stage. Figure 3 is an example of the safety 
argument pattern for stage 1. All six stages’ arguments contribute to the final ML safety case. 

Figure 3: Argument Pattern for ML Safety Assurance Scoping 

2  A  safety  argument  pattern  explicitly  illustrates  the  relationships  between  a  safety  claim,  the  context  and  the 
evidence required to satisfy the claim. AMLAS employs The Goal Structuring Notation Standard to formulate these 
patterns [16].  

 
 
 
 
 
 
 
 
4. Results  

This section presents the results of the review structured according to each stage of AMLAS.  

4.1 Stage 1: ML Safety Assurance Scoping 

Discussions  at  this  stage  centred  around  understanding  current  manufacturer  practice  of 
safety assurance using a whole system safety approach and the feasibility of addressing the 
inputs  (A,  B,  C,  D  &  F),  activities  (1  &  2)  and  producing  outputs  (E  &  G)  as  prescribed  by 
AMLAS. 

Convergence  

Both  manufacturers  understood  and  accepted  the  concept  of  carrying  out  a  system  safety 
assessment prior to the ML safety assurance scope. 

Manufacturer A explained how the inputs of this stage are already in part addressed through 
harmonised  standards,  clinical  workflow  plans,  risk  assessments  and  deployment 
methodologies. In addition, manufacturer A stated, 

“In  depth  understanding  of  the  breast  screening  system  was  essential  when 
building  the  integrations needed for the  ML  component  to fit  seamlessly into 
the current clinical workflow” 

Manufacturer B, being both a manufacturer and deployer of the ML component understood 
the need to scope the safety of the ML from a system-level. They expressed how they had 
already scoped the safety of the ML according to decisions it made and how that impacted the 
wider clinical pathway. 

Both  manufacturers  were confident  they  would be  able to address  the  inputs  and  activities 
leading to the desired outputs as prescribed by AMLAS at this stage. 

Divergence 

Both  manufacturers  stated  their  ML  component  safety  scope  was  gathered  from  its  binary 
output  and  how  that  decision  affected  the  wider  system.  This  invariably  led  conversations 
towards  performance  metrics.  This  involved  assessing  the  safety  of  the  ML mostly  against 
performance metrics, which if acceptable to the manufacturer, would translate into the ML’s 
contribution  to  the  wider  system  as  being  safe.  However,  if  performance  metrics  are  not 
context specific enough (e.g., different patient types may require specific metrics), they could 
contribute  towards  compromising  safety  of  the  wider  system.  Further  guidance  for 
manufacturers of how to explicitly consider the ML safety assurance scope linked to a wider 
system safety assessment would be beneficial. 

In practice, both manufacturers were producing content similar to the inputs A, B, C, D for their 
regulatory  tasks  and  internal  quality  assurance  routes,  although  not  always  as  distinct 
artefacts as defined per AMLAS at this stage.  

 
 
 
 
 
 
 
 
 
Gaps and Limitations 

Discussions highlighted the importance of including qualified healthcare professionals (HCPs) 
as subject matter experts due to the nature of how integral they are in healthcare pathways. 
AMLAS  allows  for  this  and  provides  some  guidance  through  the  notes  and  examples. 
However, explicit identification of where they should be involved would provide benefit. 

4.2 Stage 2: ML Requirements Assurance 

Discussions  at  this  stage  centred  around  understanding  current  manufacturer  practice  of 
assigning safety requirements to the ML component. 

Convergence 

It was clear from discussions, both manufacturers understood and made use of performance 
and robustness metrics for safe operation of the ML. However, their assignment was with an 
implied system level thinking, e.g., ML false negative equals potential towards patient harm. 

Manufacturer A performance metrics included “recall rate (RR), cancer detection rate (CDR), 
sensitivity, specificity assessed as part of each deployment to ensure performance on a per 
site  basis”.  Robustness  of  the  component  was  addressed  through  “any  cases  that  the  ML 
cannot read  i.e.,  technical  recalls,  not  sufficient  images etc,  are  not  processed through the 
tool”. 

Manufacturer B made use of “no false negatives (FN) associated with extreme events such 
as mortality” and an “area under the curve (AUC) of >0.80%”. Furthermore, measuring the ML 
performs  equally  across  differing  patient  attributes  was a  key performance concern  for this 
manufacturer. Regarding robustness, they augmented missing data by accessing alternative 
data sources which in turn allowed the model to continue with desired outputs. 

Divergence 

This paragraph repeats the corresponding comment in the above stage. Both manufacturers 
were not explicitly cascading the system level safety requirements to a ML safety assurance 
scope and then assigning ML safety requirements, as prescribed per AMLAS. 

Gaps and Limitations 

Interpretability was the only area which was discussed as a potential addition to the types of 
ML safety requirements. AMLAS allows for additional ML safety requirements to be added. 
Manufacturer B raised an interesting point of “… maybe to distinguish between interpretability 
vs  explainability”.  A  way  forward  would  be  to  consider  them  under  the  broader  term 
“transparency”, which expands further to include attributes such as fairness etc. This will need 
to be explored further by the research team. 

 
 
 
 
 
 
 
 
 
 
4.3 Stage 3: Data Management Assurance 

Discussions  at  this  stage  centred  around  understanding  the  concept  of  data  requirements 
which are sufficient to allow for the ML safety requirements to be encoded as features against 
which the data sets to be produced in this stage may be assessed.  

Convergence 

Both manufacturers made use of well-established data spitting methods. Specifically, the data 
set is split into a development (training) set and a verification/validation set. 

 Manufacturer A stated: 

“data set for our large scale Trial 2 was split into 2 sets; trial set vs development 
set. Where the development set was used for machine learning and the trial 
set was used to verify/evaluate the tool against. Reason for this was to evaluate 
on before unseen data” 

 Manufacturer B stated: 

“The model is trained on 75% of the total data, where 25% of the data is held-
out for validation and reporting purposes.” 

Terminology differs amongst manufacturers in a similar fashion to the ML community, 
although the concepts hold. 

Divergence 

Currently,  both manufacturers  obtain their  data  sets  from real-world  settings.  Primarily,  this 
data  is  obtained  from  partnered  healthcare  organisations.  This  does  provide  data  of  the 
intended  target  population  but  comes  with  associated  biases  and  limitations.  For  example, 
real-world data does not necessarily ensure the entire population is present in the data set as 
this is dependent on attendance or having access to data of all types of patients. However, it 
is not always necessary for all patient types to attend or be present in the data set, in order for 
a model to generalise due to the prevailing concept of ML (i.e., to learn patterns from training 
data which can be applied to unseen data). However, this can only be assessed from rigorous 
evaluation.  
The challenge here is for manufacturers to develop explicit safety requirements based on the 
AMLAS data attributes, “relevant”, “complete”, “accurate” and “balance”, to assess the safety 
of the ML component when generalising for previously unseen population.  

Gaps and Limitations 

This point formulated below as a question was explicitly raised by the lead researcher and will 
continue to be explored further as no clear consensus was reached. 

Will  the  AMLAS  data  attributes  suffice  towards  data  safety  requirements  associated  with 
healthcare  data?  For  example,  under  which  attribute  would  data  distribution  drift/shift  be 
included? It should be noted, AMLAS does allow for additional attributes to be added. 

 
 
 
 
  
 
 
4.4 Stage 4: Model Learning Assurance 

This  stage  focuses  on  developing  the  machine  learnt  model  using  the  development  data 
obtained in the previous stage such that the allocated ML safety requirements are satisfied. 
However,  since  this  was  not  explicitly  occurring  as  part  of  current  assurance  practices  the 
discussions  involved  understanding  how  it  could  be  achieved  and  managed  by  the 
manufacturer.    

Convergence 

Regarding the activities as prescribed by AMLAS, both manufacturers create models and test 
them based on performance metrics and robustness requirements in an iterative manner. This 
feedback  loop  informs  optimisation  techniques  which  leads  to  the  candidate  model  for 
deployment. This is similar in nature to the requirements of AMLAS, except they would need 
to align existing model learning to ensure ML safety requirements are met based on the data 
set. 

The practice of discussing trade-offs is followed by both manufacturers as it is a fundamental 
aspect  of  acquiring  the  best  fit  model.  Manufacturer  A  pointed  towards  aspects  of  their 
regulatory requirements as a factor that drove their discussions but were limited in what they 
could state due to Intellectual Property (IP). 

Manufacturer B mentioned having a need to maintain an area under the curve (AUC) >0.80%. 
This informed their decision of how regular the data acquisition cycle should run to maintain 
their  metrics  leading to  trade-offs  between what can  be  practically  achieved  with  partnered 
organisations and internal metrics.  

Divergence 

Both manufacturers are expecting specific metrics to be met in order to provide evidence of 
the viability of their ML component. Hence, it is implied safety of the component will be justified 
from the metrics. However, these metrics will need to be explicitly linked to the wider system 
safety requirements to be meaningful. Manufacturer A exemplifies this thinking by stating: 

“As discussed, in the context of our product it is difficult to separate improved 
performance with improved safety; i.e., if the model is more accurate with its 
binary decision it is safer. However, when looking at safety as a whole it comes 
down to more than just ML performance metrics when implemented into a wider 
system  and  this  is  covered  through  robust  deployment  methodologies  and 
novel workflows using AI”. 

 Manufacturer B concerns were: 

“ML  engineers  want  to  deploy  models  and  safety  could  be  considered  as 
additional work that takes them away from their core role so I believe the form 
of communication should be in the easiest/least time-consuming way to follow.” 

Both  points  of  view  highlight  that  ML  engineering  teams  are  immersed  in  solving  complex 
problems that are intrinsically linked to metrics. The research team will need to address explicit 

 
 
 
 
inclusion in any guidance of the potential pitfalls of linking metrics to safety. To elaborate, more 
thought will need to be given on how specific metrics translate towards safety for a specific 
use case and whether the metrics are applicable to all types of patients and rare conditions. 

The Model Development Log has a pivotal role as per the AMLAS. This was not something 
that  was  currently  used  as  part  of  a  safety  process.  However,  manufacturer  B  provided  a 
sensible approach to how it could be utilised with the time constraints already placed on ML 
engineering teams:  

“The  project/workstream  owner  to  map  out  the  process,  where  the  ML 
engineers  have  to  mark  which  stages  will  have  the  highest  impact  on  the 
performance of the model and all changes made to these key pivots are to be 
logged” 

As mentioned, “… impact on the performance of the model and all changes made to 
these key pivots are to be logged” could be explicitly linked to how it affects the safety 
requirements. Any associated work that needs to be conducted could be recorded in 
the model development log.  

Gaps and Limitations 

A specific inclusion criterion in the argument could be to provide explicit justification as to why 
specific metrics are chosen for optimisation of performance particularly to enhance the safety. 
This is best expressed by manufacturer B as:  

“It won't be superfluous to also describe what it will mean if one of the metrics 
is chosen for optimisation. E.g., if you choose false negatives that means you 
will have higher sensitivity and lower positive predictive value. Again, helping 
to build a common understanding on related but often assumed similar metrics”   

4.5 Stage 5: Model Verification Assurance 

Discussions at this stage were centred on verification of the ML model based on current 
manufacturer practice in comparison to specific AMLAS verification requirements. 

Convergence 

Of the two available options in AMLAS to verify a model, both manufacturers made use of test-
based verification. Manufacturer A conducted their verification by stating:  

“At  each  new  site  we  engage  with  we  run  a  pass  of  our  current  ML  model. 
Depending on performance we will calibrate with this new site data if needed. 
Additionally,  we  have  run  and  intend  to  run  formal  clinical  investigations  on 
performance in both a double reading and standalone workflow” 

Based on AMLAS guidance, the verification data not being made available to the development 
team was seen as a sound concept by the manufacturers, however manufacturer B did state, 

 
 
  
 
 
 
 
 
  
“this is tricky as additional data for verification might be difficult to obtain”. The lack of good 
quality  data available in ample  quantities  is  an  ongoing  challenge  for the  machine  learning 
community.  A  further  practical  challenge  involves  shielding  the  verification  data  from  the 
development team. 

Divergence 

No divergence to report. 

Gaps and Limitations 

One particular area that was discussed and could be included is an explicit requirement to 
verify against bias/fairness of the model for sub-groups of patients. These sub-groups would 
need to be proactively identified as those that are most at risk of being unfairly discriminated 
against, depending on the use case being executed. 

Manufacturer B explains this by stating: 

“… additionally models could also be tested/verified on their bias/fairness for 
particular unprivileged groups. This is particularly relevant in healthcare ….” 

4.6 Stage 6: Model Deployment Assurance 

AMLAS is written from a development (ML engineering) perspective, therefore it is fair to state 
that  within  its  guidance  much  of  the  responsibility  of  deploying  safely  is  placed  on  the 
manufacturer.  However,  if  the  technology  is  to  be  deployed  into  a  third-party  environment, 
responsibility  of  deploying  safely  becomes  more  a  joint  responsibility  between  the 
manufacturer  and  healthcare  organisation.  This  aspect  was  given  emphasis  through 
discussions at this stage. 

Convergence 

Deployment was seen as a multi-stage activity involving integration into the existing hardware 
& software infrastructure and the corresponding clinical pathway to test safety requirements 
satisfied during pre-deployment stages continue to be satisfied. In line with this, manufacturer 
A  is  taking  the  approach  of  deploying  a  model  which  is  frozen  and  then  monitored.  Any 
subsequent changes that are required that affect the wider system safety requirements are 
discussed and approved by a multi-disciplinary team. Routine monitoring and modification of 
the model will be necessary to continue meeting safety requirements. Regarding this matter, 
Manufacturer B stated, we should be asking prior to any modifications or updates “why are we 
changing the model?”. Furthermore, keeping “track of what changed and its impact on output” 
should be logged which aligns with the AMLAS.    

With  regard  to  specific  safety  assurance  process  and  documentation,  Manufacturer  A  are 
complying both with the CE marking process (Medical Device Regulation) [17]  and the Health 
Information  Technology  (HIT)  standard  for  manufacturers,  DCB0129  [14].  The  evidence  is 
being developed as a technical file and safety case, respectively. In addition, Manufacturer A 

 
 
 
 
 
 
 
 
 
has instructed the healthcare organisations to complete the DCB0160 (HIT standard for the 
deployment  and  use  of  HIT  systems)  [15].  Involving  the  healthcare  organisation  and  their 
clinicians in this process is seen as vital as manufacturer A stated, “our phased deployment 
has vast involvement from the deploying organisation, particularly the clinicians”. 

Divergence 

AMLAS places significant responsibility on those who follow the methodology to complete logs 
(development,  error,  etc).  This  is  widely  used  in  engineering,  however  it  was  felt  from  a 
practical  perspective,  completing  logs  should  in the  most  be  automated rather  than  human 
led. This is not a clear divergence from AMLAS, more a clarification of opinion.  

Gaps and Limitations 

Deployment  in  itself  will  provide  safety  assurance  linked  to  temporality,  whereas  routine 
monitoring  and  modification  of  the  ML  will  be  needed  to  continue  satisfaction  of  safety 
requirements.  Therefore,  we  see  a  need  for  routine  monitoring  and  modification  to  be 
expanded into additional stages as they are not covered in sufficient detail at stage 6. Within 
the healthcare domain, on-going monitoring and modification are crucial stages where safety 
assessments continue as part of regulatory compliance and HIT standards. 

 
 
 
 
 
5. Discussion 

Five conceptual themes emerged from this deep review of the AMLAS which are discussed 
below as, (1) ML safety as part of whole system safety; (2) explicit inclusion of HCPs leading 
to  richer  safety  assessments;  (3)  mapping  the  contribution  of  performance  metrics  &  soft 
constraints towards the ML safety profile; (4) data management processes to satisfy safety 
requirements; and, (5) apportioning roles and responsibilities between the manufacturer and 
deploying organisation to maintain safety requirements of the ML in live operation. 

The manufacturers, depending on the nature of how they build and deploy ML, are considering 
the  safety  of  their  product  as  part  of  the  wider  system.  Based  on  the  classification  of  their 
technology,  this  involved  formal  safety  assessments  through  their  work  associated  with 
regulatory  compliance  routes  or  internal  quality  &  safety  approaches.  Manufacturers 
understood  the  concept  of  safety  assurance  from  a  whole  system  approach  which  is 
decomposed to specific ML safety requirements. This concept is in part addressed through 
their existing processes although not from the initiating phases of development as prescribed 
by AMLAS. However, both manufacturers agreed they could comply with ML safety from a 
whole  system  approach.  The  proposed  future  work  (healthcare  specific  supplementary 
guidance  for  AMLAS)  should  include  current  methods  in  place  that  allow  for  derivation  of 
system safety requirements. 

AMLAS  throughout  its  guidance  makes  reference  to  the  inclusion  of  experts  in  the  safety 
assurance process. This was a recurring discussion point in the workshops of where qualified 
HCPs should be included to provide much needed clinical expertise in safety assessments. 
The benefits of their participation is self-evident due to being subject matter experts bringing 
numerous benefits, one of which is to contribute to those areas which need human factors to 
be considered as part of safety assessments, such as automation bias, handover, etc. A key 
finding  from  this  work  is  to  consider  where  HCPs  shall/should  be  included  explicitly  in  the 
AMLAS assurance process and argument patterns. It should be noted, the group recognised 
best-practice to have HCPs included throughout, although the reality of obtaining their time 
can  be  challenging,  therefore  leading  towards  selective  inclusion  or  creation  of  dedicated 
roles. 

Metrics are fundamental to how the manufacturers assess the safety of their ML. Discussions 
identified common metrics used were sensitivity and specificity, expressed as Area Under the 
curve  of  the  Receiver Operating  Characteristic  (AUROC),  and benchmarked  recall  rates  to 
satisfy  internal  target  performance  criteria  which  linked  to  implied  safety.  However,  having 
high-performance  levels  is  only  part  of  the  solution  to  assuring  the  safety  of  the  ML.  “Soft 
constraints”,  such  as  transparency  (e.g.,  interpretability,  explainability,  etc),  will  need  to  be 
considered from the viewpoint of how they impact human factors [18] and should potentially 
be explicit criteria as part of safety assessments. One specific standard, but by no means the 
only one, ISO 62366: Application of Usability Engineering to Medical Devices can assist the 
thinking required to address soft constraints. Furthermore, manufacturers will need to consider 
what trade-offs would need to be made to the performance of the ML to achieve these soft 
constraints. 

 
 
 
  
As expected, manufacturers made use of well-established data management techniques and 
were splitting their data sets as per current methods. However, they were not setting safety 
requirements  of  their  data  to  be  relevant,  complete,  accurate  &  balanced  as  per  AMLAS 
guidance. This does not imply manufacturers are not making use of their own techniques and 
the concepts were accepted as being extremely important and integral to any ML project. A 
question  that  arose  from  this  which  requires  further  research  was,  under  which  attribute/s 
would data distribution drift/shift be included?  
Currently,  both  manufacturers  obtain  their  data  sets  from  real-world  settings  (partnered 
healthcare organisations)  which  come  with  associated  biases  and limitations.  Therefore,  to 
comply with AMLAS the challenge for manufacturers here is more to change their mindset in 
how they currently approach data management and the assignment of safety requirements to 
data which produce models that satisfy ML safety requirements. This is by no means a simple 
task and the hope is future guidance will help towards this goal. 
Finally, any argument for having an appropriate data set should present why it is sufficient to 
produce models that generalise for previously unseen populations (i.e., patient subgroups). 
This is particularly relevant to healthcare as models will eventually be deployed on thousands 
of patients coming from diverse and somewhat fluid populations. 

During  the  engineering  of  a  ML  component  heavy  emphasis  is  placed  on  its  performance 
accuracy and for that to hold when deployed in a real-world live setting. This can often translate 
as a safe product which is not the case as there are other factors, such as transparency, which 
will be just as integral in safe deployment. This responsibility, as per AMLAS, is apportioned 
to  the  manufacturer.  However,  if  deploying  at  a  third-party  site,  the  deploying  organisation 
should be fully involved with the manufacturer in safely integrating the ML into their existing 
hardware/software  infrastructure  and  clinical  pathway.  AMLAS  currently  does  not  explicitly 
include the deploying organisation to be involved in the deployment phase, nevertheless it is 
flexible enough for its inclusion. Furthermore, routine monitoring is crucial to satisfying safety 
requirements  as  this  stage  allows  for  data  to  be  gathered  on  safety  requirements  being 
maintained and justification for when change is required. AMLAS does not include any stages 
beyond deployment and therefore, additional stages or safety argument updates will need to 
be considered by the research team as per their future work.   

5.1. Limitations 

Every effort was made to recruit and work with DHT manufacturers with ML technologies which 
are deployed or in the process. While this was achieved, having two manufacturers does limit 
the  review  to  specific  technologies,  scenarios  and  working  practices.  A  greater  number  of 
manufacturers may have yielded further conceptual themes of interest. 

 
 
 
 
 
 
6. Conclusion 

Assuring the safety of ML-based technologies has never been more pressing with the current 
upward trend of ML technologies gaining regulatory approval through frameworks originally 
devised  for  traditional  software.  As  those  organisations  with  policy,  regulatory  and  safety 
responsibility continue to reform their frameworks, methodologies such as the AMLAS need 
to be appraised and evaluated to assess whether they are fit for purpose as safety assurance 
methodologies alongside regulation. This work has concluded the methodology to be one that 
is appropriate to be applied in the healthcare domain with additional healthcare supplementary 
guidance. 

Acknowledgements      

This work is part of work package 2 of the Safety Assurance FRamework for Machine Learning 
in  the  Healthcare  Domain  (SAFR)  project,  funded  through  the  Assuring  Autonomy 
International  Programme  (AAIP),  which  is  overseen  by  the  University  of  York  and  Lloyd's 
Register Foundation. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Appendix A - AMLAS Review Questions Framework 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
References 

[1]  U. J. Muehlematter et al., 2021. ‘Approval of artificial intelligence and machine learning-
based medical devices in the USA and Europe (2015–20): a comparative analysis’. The 
Lancet Digital Health, vol. 3, no. 3, pp. e195–e203, Mar.  

[2]  D.  Sculley  et  al.,  2015.  ‘Hidden  Technical  Debt  in  Machine  Learning  Systems’,  in 
Advances  in  Neural  Information  Processing  Systems.  vol.  28.  [Online]  Available: 
https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-
Paper.pdf. 

[3]  C. Garbin and O. Marques. 2021. ‘Assessing Methods and Tools to Improve Reporting, 
Increase Transparency, and Reduce Failures in Machine Learning Applications in Health 
Care’. Radiology Artificial Intelligence, vol. 4, no. 2, p. e210127, Mar. 

[4]  The European Commission (EU). 2011. ‘COMMISSION IMPLEMENTING REGULATION 
(EU) No 1035/2011, laying down common requirements for the provision of air navigation 
services and amending Regulations (EC) No 482/2008 and (EU) No 691/2010’.   
eur-lex.europa.eu. EU. 
[Online]Available:https://eurlex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2011:271
:0023:0041:EN:PDF. 

[5]  Medicines and Healthcare products Regulatory Agency (MHRA). 2021. ‘Software and AI 
as  a  Medical  Device  Change  Programme’.  GOV.UK:  MHRA.  [Online]  Available: 
https://www.gov.uk/government/publications/software-and-ai-as-a-medical-device-
change-programme/software-and-ai-as-a-medical-device-change-programme. 

[6]  U. K. Govt. 2021. ‘Medicines and Medical Devices Act 2021’. Parliamentary Bills. [Online] 

Available: https://bills.parliament.uk/bills/2700. 

[7]  Care Quality Commission (CQC). 2020. ‘Using machine learning in diagnostic services A 
report with recommendations from CQC’s regulatory sandbox’. CQC.  [Online] Available: 
https://www.cqc.org.uk/sites/default/files/20200324%20CQC%20sandbox%20report_ma
chine%20learning%20in%20diagnostic%20services.pdf. 

[8]  US  Food  and  Drug  Administration  (FDA).  2019.  ‘Proposed  Regulatory  Framework  for 
Modifications  to  AI/ML-Based  Software  as  a  Medical  Device  (SaMD)’,  FDA.  [Online] 
Available: https://www.fda.gov/media/122535/download. 

[9]  US  Food  and  Drug  Administration  (FDA).  2021.  ‘Artificial  Intelligence  and  Machine 
Learning  (AI/ML)  Software  as  a  Medical  Device  Action  Plan’.  FDA.  [Online]  Available: 
https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-
intelligence-and-machine-learning-software-medical-device. 

[10] International Organization for Standardization (ISO). 2017. ‘ISO/IEC JTC 1/SC 42’. ISO. 

[Online] Available: https://www.iso.org/committee/6794475.html 

[11] Joint Technical Committee (JTC 21). 2022. ‘Artificial Intelligence’. CEN CENELEC.  

[Online]Available: https://www.cencenelec.eu/areas-of-work/cen-cenelec-topics/artificial-
intelligence/ 

[12] R. Hawkins et al., 2021. ‘Guidance on the Assurance of Machine Learning in Autonomous 

Systems (AMLAS)’. University of York. 
[Online] Available: https://www.york.ac.uk/assuring-autonomy/guidance/amlas/ 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
[13] P. Bishop and R. Bloomfield. 2000. ‘A Methodology for Safety Case Development’. Safety 

and Reliability, vol. 20, no. 1, pp. 34–42, Mar. 

[14] National  Health  Service  (NHS).  2018.  ‘DCB0129:  Clinical  Risk  Management:  its 
Application  in  the  Manufacture  of  Health  IT  Systems’.  Specification  NPFIT-FNT-TO-
TOCLNSA-1792.06,  V4.2, 
[Online]  Available: 
https://digital.nhs.uk/data-and-information/information-standards/information-standards-
and-data-collections-including-extractions/publications-and-notifications/standards-and-
collections/dcb0129-clinical-risk-management-its-application-in-the-manufacture-of-
health-it-systems 

02.05.2018.  NHS  Digital. 

[15] National  Health  Service  (NHS).  2018.  ‘DCB0160:  Clinical  Risk  Management:  its 
Application in the Deployment and Use of Health IT Systems’. Specification NPFIT-FNT-
TO-TOCLNSA-1793.05,  V3.2,  02.05.2018.  NHS  Digital. 
[Online]  Available: 
https://digital.nhs.uk/data-and-information/information-standards/information-standards-
and-data-collections-including-extractions/publications-and-notifications/standards-and-
collections/dcb0160-clinical-risk-management-its-application-in-the-deployment-and-
use-of-health-it-systems 

[16] The  Assurance  Case  Working  Group  (ACWG).  2018.  ‘Goal  Structuring  Notation 

Community Standard Version 2’, [Online] Available: https://scsc.uk/r141B:1?t=1 

[17] European Union (EU). 2017. ‘EU Medical Device Regulation 2017/745 (EU MDR)’. 

EUR-Lex. [Online] Available:  
https://eurlex.europa.eu/legalcontent/EN/TXT/?uri=CELEX%3A32017R0745 

[18] M. Sujan et al., 2021. ‘Human Factors and Ergonomics in Healthcare AI’, The Chartered 
[Online]  Available: 

Institute  of  Ergonomics  and  Human  Factors 
https://www.ergonomics.org.uk/common/Uploaded%20files/Publications/CIEHF-AI-in-
Healthcare-White-Paper.pdf 

(CIEHF). 

 
 
 
 
 
 
 
 
 
 
