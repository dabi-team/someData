Accepted at the 18th International Symposium on Applied Reconﬁgurable Computing (ARC) 2022

2
2
0
2

g
u
A
1
2

]

R
A
.
s
c
[

3
v
9
5
3
0
1
.
3
0
2
2
:
v
i
X
r
a

FPGA-extended General Purpose
Computer Architecture

Philippos Papaphilippou1[0000−0002−7452−7150] and Myrtle
Shah2[0000−0002−5410−1646]

1 Department of Computing, Imperial College London, UK(cid:63)(cid:63)
p.papaphilippou17@alumni.imperial.ac.uk
2 ChipFlow Ltd, UK

Abstract. This paper introduces a computer architecture, where part
of the instruction set architecture (ISA) is implemented on small highly-
integrated ﬁeld-programmable gate arrays (FPGAs). Small FPGAs inside
a general-purpose processor (CPU) can be used eﬀectively to imple-
ment custom or standardised instructions. Our proposed architecture
directly address related challenges for high-end CPUs, where such highly-
integrated FPGAs would have the highest impact, such as on main mem-
ory bandwidth. This also enables software-transparent context-switching.
The simulation-based evaluation of a dynamically reconﬁgurable core
shows promising results approaching the performance of an equivalent
core with all enabled instructions. Finally, the feasibility of adopting the
proposed architecture in today’s CPUs is studied through the prototyping
of fast-reconﬁgurable FPGAs and studying the miss behaviour of opcodes.

Keywords: computer architecture · memory hierarchy · reconﬁgurable extensions

1

Introduction

There has been considerable maturity around traditional software on today’s
CPUs. This has led to easier development through high-quality libraries and
debug tools, as well as relatively mature programming models and veriﬁcation
routines. Additionally, a variety of software and hardware abstractions have
enabled portability of code, such as with virtual memory and cache hierarchies,
and enabled more eﬀortless increase in performance, such as through instruction-
level parallelism.

However, general purpose processors leave a lot to be desired in terms of
performance, hence the increase in use of computation oﬄoading to specialised
processors. These include graphics processing units (GPUs), FPGAs and even
purpose-built silicon in the form of application-speciﬁc integrated circuits (ASICs).
One consideration in today’s hardware specialisation technologies is the fact
that they are mostly based on the non-uniform memory-access model (NUMA).
Large oﬀ-chip memories are found in the majority of today’s high-end FPGA

(cid:63)(cid:63) The ﬁrst author is now with Huawei Technologies R&D (UK) Limited.

 
 
 
 
 
 
oﬀerings, resulting in power-hungry and expensive setups, as well as in limitations
in programming models, and complicating deployment and data movement.

While promising techniques like wide single-instruction multiple-data (SIMD)
instructions [17] in CPUs attempt to close the gap between specialised and
general purpose computing [10], this gap is wider than it has ever been. This
is because of the increased need for highly customised architectures in trending
workloads [26], whose functionality cannot be eﬃciently expressed with a ﬁxed
general purpose ISA and architecture.

In this paper, we extend the most common computer architecture in to-
day’s systems (modiﬁed Harvard architecture [15]) to introduce FPGA-based
instruction implementations in general purpose systems. In contrast to current
research, this goes beyond embedded and heterogeneous processors, and intro-
duces multi-processing for operating systems and ﬁne-grain reconﬁguration, as
with standardised instruction extensions. A feasibility study shows promising
performance for supporting reconﬁgurable extensions on-demand, especially when
supporting fast FPGA reconﬁguration. The list of contributions is as follows:

1. The “FPGA-extended modiﬁed Harvard Architecture”, a novel computer
architecture to introduce FPGAs working as custom instructions, enabling
context-switching and other advanced concepts for higher-end applications.
2. A comprehensive evaluation with ﬁne-grain reconﬁguration (at the instruction-
level), providing insights on the impact of the reconﬁguration time and the
operating system’s scheduler properties for multi-processing.

3. Feasibility studies elaborating on the readiness of current SoC technology to

adopt the proposed approach

2 Challenges

The research on FPGAs implementing instructions can be considered an attempt
to overcome a series of challenges in current systems. This work addresses
challenges found in existing research on custom instructions.

Current CPUs and Discrete FPGAs. One challenging design choice that
relates to both hardware and software is the selection of instructions that would
be more beneﬁcial to include as part of the instruction set architecture (ISA).
With a ﬁxed ISA, vendors can select a subset of instructions, such as with the
modularity of RISC-V [3], or design custom instructions. For general purpose
computing it is diﬃcult to predict what the most appropriate instructions will be.
For instance, some applications may be ephemeral, as with some deep learning
models, for which specialised hardware becomes obsolete faster.

Another challenge is hardware complexity. Supporting a high-number of
instructions is expensive, but sometimes this has been unavoidable for widening
the applicability of general purpose processors. For example, Intel’s AVX2 and
AVX-512 include thousands of instructions [17], and RISC-V’s unratiﬁed vector
extension hundreds [1]. The related implementation complexity, such as with AVX-
512, is associated with a decrease in operating frequency and power eﬃciency, and

2

area increase [11, 14]. Additionally, AVX-512 is suboptimal for certain workloads,
where a serial code could surpass them in terms of performance and scalability [11].
Expanding the ISA can also harm the SoC scalability to many-cores, which heavily
relies on core miniaturisation and power eﬃciency.

When using FPGAs as accelerators, one of the most limiting bottlenecks to
performance is the bandwidth to main memory [21]. For example, even with Intel’s
Xeon+FPGA, although the FPGA is directly connected to the memory controller
it only achieves 20 GB/s [7]. The memory hierarchy tends to always favour
CPU performance, hence the presence of expensive oﬀ-chip memories in high-end
FPGA boards. This heterogeneity is considered to impact FPGA development
and increases the cost and deployment of FPGAs in the datacenter [22].

FPGAs Implementing Instructions. The basic limitation of the related
work on FPGA-based instructions is the focus on embedded and/or heteroge-
neous systems, with no notion for multi-processing, context-switching and other
advanced micro-architectural features. The use of embedded FPGAs (eFPGAs)
has many practical applications in embedded systems [5,18], but there is currently
no computer architecture to “hide” reconﬁguration from traditional software.

One challenge in existing methods of introducing FPGAs as custom instruc-
tions is the need for manual intervention for reconﬁguration. Although the
recommended procedures to handle bitstreams can be well documented, deviating
from conventional software development could be detrimental for adoption [5].
By initially focusing on highly-customised instructions and more complex
accelerators, there has been less opportunity for modern processors to gradually
adopt small reconﬁgurable regions as part of their core. It is more complex to
derive conclusions from speciﬁc custom instructions and accelerators, as their
exploration usually shifts the focus to specialisation and optimisation.

3 Solution

The proposed solution is the “FPGA-extended modiﬁed Harvard Architecture”,
which uniﬁes the address space for instructions, data, as well as for FPGA
bitstreams. When compared to the traditional modiﬁed Harvard architecture,
the proposed solution also adds a separate bitstream cache at level 1, to provide
bitstreams for FPGA-instructions after an instruction opcode is ready. The idea
is for a computing core that features reconﬁgurable slots for instructions, to be
able to eﬃciently fetch instruction bitstreams transparently from the software.
Figure 1 (left) introduces the proposed computer architecture.

This architecture assumes that the computing core features fast-programmable
FPGAs that can be used to implement instructions. This can be achieved with
the help of a small cache-like structure, the instruction disambiguator, shown
in ﬁgure 1 (right). On every instruction decode there is a request to this unit
to see if there is an instruction implementation for the requested instruction.
It operates as a fully-associative cache and uses opcodes (plus any additional
ﬁelds for deﬁning functions) as tags to determine the bitstream locations. On

3

Fig. 1. Proposed computer architecture (left) and instruction disambiguator (right)

an opcode miss, it requests the instruction bitstream from the bitstream cache,
while on a hit it multiplexes the operands to the appropriate slot.

The bitstream cache is a separate cache speciﬁcally designed for FPGA bit-
streams that can increase the performance of the reconﬁgurable core. Similarly to
today’s modiﬁed Harvard architecture, the L1 instruction and data caches are still
separated and connected to a uniﬁed cache, allowing easier simultaneous memory
accesses for pipelining the instructions. Since the instruction disambiguator unit
waits for an instruction opcode to be ready, a bitstream fetch phase can be
placed subsequently to the instruction decode pipeline stage in heavily-pipelined
processors. This cache is separated to also allow diﬀerent features than the rest
of the caches, such as with wider blocks to facilitate the increased width to carry
bitstreams, as opposed to instructions (see section 5.2).

This approach enables the applications to be agnostic of the reconﬁguration
aspect. An operating system can provide ISA extensions (or part of them) in
the form of bitstream libraries, while the hardware fetches the corresponding
bitstreams on demand. Sharing the same address space for the bitstreams also
enables keeping bitstreams in software binaries, so that they can provide custom
instruction extensions alongside their data segment for acceleration potential.

4 Evaluation

This evaluation works as a proof of concept, thus any platform limitations are not
handed-down over the proposed computer architecture. As our proposal concerns
a fundamentally diﬀerent computer architecture and targets high-end hardened
processors, future research on a detailed evaluation would involve fabrication.

The framework for evaluating the performance of the proposed architecture3
is based on Simodense [22], an open-source FPGA-optimised RISC-V softcore,
which was heavily modiﬁed to facilitate our study on the system eﬀects of
our proposal. This study extends it with the “F” extension for single-precision
ﬂoating-point support. This resulted in the RV32IMF, where “I” is the base 32-bit
integer and “M” is the integer multiplication/division extension [3]. Most of the “I”
instructions introduce one cycle of latency, while the “M” instructions occupy 4

3 Source code available: https://github.com/pphilippos/fpga-ext-arch

4

InstructionCacheBitstreamCacheDataCacheRest of the Memory HierarchyComputing CoreInstructionDisambiguatorTag (opcode) arrayInstruction slots(fully-associative cache)(fnmsub.s) 0x12(fmadd.s) 0x10(mul) 0x1 0x0C(fsqrt.s) x0B 0x14FPGA 0FPGA 1FPGA 2FPGA 3InstructionDecodeBitstream,Input 1,Input 2,Input 3 Hit location?HaltBitstream,Output 1busy?non-blocking cycles of latency. The “F” extension is pipelined with a latency of 6
cycles, excluding the fused multiply-add instructions that yield a 12-cycle latency.
RISC-V’s “Zicsr” and a set of control status registers (mstatus, mie, mcause,
mepc and mtvec) were also added to support the experiment of section 4.3.

The main addition is the instruction disambiguator. Its functionality here is
to process opcodes (and related ﬁelds) and add artiﬁcial latency when there is an
instruction slot miss (or hit). All required instructions actually pre-exist on the
softcore, emulating the performance overhead of the proposal as observed by the
software. The instruction opcodes are ﬁrst being resolved through the instruction
cache, and the instruction slot disambiguator here works as an L0 instruction
cache that uses opcodes as cache tags and adds latency on opcode misses.

With respect to the size and complexity of the reconﬁgurable instructions,
we explore a compartmentalisation scenario, where instructions are grouped into
single reconﬁgurable regions according to their logic similarity. There are 3 groups
for the “M” extension ({mul, mulh, mulhsu, mulhu}, {div, divu}, {rem, remu}),
and 7 groups for the “F” extension {fadd.s, fsub.s}, {fmul.s}, {fdiv.s}, {fsgnj.s,
fsgnjn.s, fsgnjx.s, fmin.s, fmax.s, ﬂe.s, ﬂt.s, feq.s}, {fsqrt.s}, {fcvt.w.s, fcvt.wu.s,
fcvt.s.w, fcvt.s.wu}, {fmadd.s, fmsub.s, fnmsub.s, fnmadd.s}), totalling 10 groups.
The number of free slots is parameterisable.

This emulates an environment where the CPU has no space for all extensions,
and the workload exhibits competitiveness for a limited number of instruction
slots. This instruction selection and granularity is indicative, thus a more complete
ISA research would be appropriate to decide what fraction of instructions remains
hardened in ﬁnal products. Such an exploration would relate to the features and
performance of the embedded FPGAs, while still allowing custom extensions.

The resulting codebase is synthesisable and also passed benchmark-based test
cases on a Xilinx Zynq UltraScale+ FPGA. However, as the resulting framework
ran relatively fast using Verilator 4.224, we opted to use simulations instead.

4.1 Benchmark Classiﬁcation

The utilised benchmark suite is Embench [23], providing a selection of benchmarks
with diﬀerent attributes of interest. It was ported for use in our infrastructure,
and each benchmark was made to run as a thread instead of a process. This
required some additional modiﬁcation, such adding thread safety for shared local
libraries. Some benchmarks with double-precision ﬂoating point arithmetic were
modiﬁed to use single-precision to make use of the “F” extension.

In order to quantify the impact of the studied extensions on the benchmark
performance, they are ﬁrst seen individually. There are four binaries/runs per
benchmark, one for each of the following ﬁxed speciﬁcation combinations: RV32I,
RV32IF, RV32IM and RV32IMF. When a useful instruction is absent from the
speciﬁcation of the compiler, it is replaced by a sub-optimal pre-deﬁned routine,
as speciﬁed by the application binary interface (ABI). The underlying softcore
supports their superset RV32IMF and can run all 4 binaries per benchmark.

The benchmark classiﬁcation is illustrated in ﬁgure 2. The axes represent
the speedup of using one of “M” or “F” over only the base instruction set RV32I.

5

Fig. 2. Task classiﬁcation based on the speedups of RV32IM and RV32IF over RV32I

As expected, the ﬁve benchmarks that used ﬂoating point all seem beneﬁt from
“F” (minver, wikisort, st, nbody and cubic), while “M” seems a relatively more
popular set amongst the benchmark selection (crc32, qrduino, primecount, ud,
aha-mont64, tarﬁnd, matmult-int and edn). The remaining 9 benchmarks are
classiﬁed as “insensitive”, which exhibit diﬀerent properties such as being control-
heavy. Interestingly, there is no class where an Embench benchmark is only
beneﬁted from “F” and not from “M” here.

4.2 Single-program

For the evaluation of the proposed architecture under single benchmarks, we select
the “improved by both F and M” class from the classiﬁcation of the previous
section. This is done to focus on workloads where there is demand for both
instruction extensions, before introducing multi-processing.

In this experiment with simulated reconﬁgurability, there are six data series for
diﬀerent miss and hit latency combinations for the instruction slot disambiguator.
There are 10-cycle, 50-cycle and 250-cycle miss latencies representing both
reconﬁguration technologies that approach a latency closer to that of CPU
instructions, and slower which could be achievable with more traditional partial
reconﬁguration techniques. For each of the three, there are versions with and
without a hit latency, which is useful to represent potential discrepancies between
the CPU core and the fabric (e.g. frequency drops).

Figure 3 presents these results for 4 available instruction (group) slots. The
y axis shows the slowdowns over when running with a ﬁxed speciﬁcation with
both “M” and “F” (RV32IMF). Note that all series regard slowdowns, but the
term speedup is also kept for consistency. There are also the RV32I and max(IM,
IF) series. The latter represents the maximum performance between the ﬁxed
speciﬁcations RV32IM and RV32IF per individual run.

When selecting the (50,0)-cycle latency conﬁguration, it can approach se-
lecting the best extension per benchmark (max(IM, IF) series) with an average
performance at around 71% of RV32IMF. It also exceeds the max(IM, IF) perfor-
mance in benchmarks like st and wikisort, where the use of “F” instructions is used
more sporadically. Over a ﬁxed baseline, when considering both the benchmarks
classes “improved by both F and M” and “improved by M” (latter not in ﬁgure 3),

6

sglib-combined110110Speedup of F over RV32ISpeedup of M over RV32Iaha-mont64crc32cubicednhuffbenchmatmult-intmd5summinvernbodynettle-aesnettle-sha256nsichneupicojpegprimecountqrduinoslrestatematesttarfindudwikisortImproved by both"F" and "M"Improved by "M"InsensitiveFig. 3. Approaching RV32IMF with reconﬁgurability for single benchmarks

a (50,0)-cycle latency conﬁguration is 2.46x, 1.4x and 3.62x faster than RV32IF,
RV32IM and RV32I respectively.

When comparing the versions for without and with a hit latency (lighter
colours in ﬁgure 3), there is a considerable performance degradation at the
higher values. For instance, for 250-cycle misses and 16-cycle hits, the approach
performs similarly to featuring no instructions from “M” and “F”, at 20% of the
RV32IMF performance. However, targeting a 0-to-few-cycle observable hit latency
in future implementations, such as with fast FPGAs or more pipelining, seems
to provide promising performance. This includes the (50,4)-cycle combination,
which updates the above comparison of (50,0) to a speedup of 1.75x, 1.05x and
2.7x over RV32IF, RV32IM and RV32I respectively.

A general conclusion with regards to the latencies is that there is a sensible
point where the approach is still helpful. For each workload there could be
detailed curves with smaller latency intervals than the indicative values here.
A similar argument can be made for the number of slots and other attributes.
Though, analyzing speciﬁc points would be less signiﬁcant, as this would relate
more directly to the speciﬁcation of the FPGAs and the core.

4.3 Multi-program

The eﬀects of multi-processing are studied with the help of an operating system.
FreeRTOS [8], a real-time operating system, was selected to provide a minimal
framework allowing experimentation with a task scheduler. A single binary is
obtained, containing both FreeRTOS task scheduler and the benchmarks as
threads. This is run as a bare-metal application by the adopted softcore to study
the eﬀects of context switching under our proposal for multi-programming. The
main modiﬁcation to FreeRTOS was the porting of the context-switching routine
to support the “F” extension in our platform.

A periodic interrupt is set by its task scheduler, responsible for context-
switching. The FreeRTOS scheduler enforces a round-robin priority between the
tasks (benchmarks). A pair of benchmarks are run through two independent
inﬁnite loops, and once one of them does a certain number of iterations, the
operating system terminates.

Following the benchmark classiﬁcation of section 4.1, the category that is not
improved by “F” or “M” (“insensitive”) is not considered. The studied pairs are

7

combinations between two of the ﬁve benchmarks that are improved by “F” and
“M” (totalling 10) and combinations between one from the latter category with
one from the eight benchmarks that are only improved by “M” (totalling 40).

Fig. 4. Multi-programming using the reconﬁgurable approach with 2, 4 or 8 slots versus
subsets of RV32IMF, under diﬀerent scheduler timings. All series are sorted individually.

Figure 4 presents the results of this experiment with a 50-cycle miss latency
(no hit latency) from the single-program experiments, as well as with variations
of it for a diﬀerent number of slots (2 and 8). The latter variations are added to
elaborate on the slot interaction with this multi-program case, as the competi-
tiveness between the slots is increased. The y-axes are the average speedups for
each of the paired benchmarks over their corresponding runtimes with RV32IMF.
The left plots in ﬁgure 4 use binaries compiled for a 1000-cycle (1K) timer
interrupt for context-switching, while the right plots present the results for a
20-fold increase in the timer interrupt delay. With the shorter 1K-cycle delay,
all runtimes increase due to the additional instructions coming from the inter-
rupt handler of the operating system. However, due to the diﬀerent instruction
distributions amongst the benchmarks, this also increases the instruction slot
misses, hence the 20K-cycle versions improve the speedups of the reconﬁgurable
approach. For instance, the average speedup of 4-slot series improves from 0.62
to 0.71 (i.e. from 38% to 29% slowdown) for the top selection of pairs, and from
0.82 to 0.9 for the benchmark pairs on the bottom of the ﬁgure.

One observation when combining the benchmarks of the same class (ﬁgure
4 top) is that the reconﬁgurable approach remains at the similar levels of per-
formance degradation as with the last section (single-program). For instance,

8

00.20.40.60.8102468Avg speedup ‒ slowdownover RV32IMFPair of F/M-sensitive benchmarks1K-cycle timer interrupt00.20.40.60.8102468Avg speedup ‒ slowdownover RV32IMFPair of F/M-sensitive benchmarks20K-cycle timer interrupt00.20.40.60.8105101520253035Avg speedup ‒ slowdownover RV32IMFPair of 1 F/M and 1 M-sensitive1K-cycle timer interrupt00.20.40.60.8105101520253035Avg speedup ‒ slowdownover RV32IMFPair of 1 F/M and 1 M-sensitiveRV32IRV32IFRV32IM8-slot4-slot2-slot20K-cycle timer interruptthe average speedup for the 4-slot with 50-cycle reconﬁguration and a 20K-cycle
timer is 0.71, while the last section’s corresponding average was also around 0.71.
From ﬁgure 4 (bottom right) we can see that the potential of reconﬁguration is
relatively higher when combining benchmarks with diﬀerent extension preferences.
The average speedup over RV32IMF for the 2-slot, 4-slot and 8-slot approaches
is 0.62, 0.9 and 0.94 respectively, under 20K-cycle interrupts.

The proposed reconﬁgurable approach is shown to be more well-rounded
than ﬁxed extensions. For example, RV32IF performs signiﬁcantly better than
RV32IM in the pairs of the upper half of ﬁgure 4, but this is reversed for the
pairs of the lower part. When considering all 50 of the aforementioned benchmark
combinations for 20K cycle interrupts, the 4-slot version is 3.39x, 1.48x and 2.04x
faster on average when compared to RV32I, RV32IM and RV32IF respectively,
at an average of 0.82x the performance of RV32IMF. Finally, ﬁne-tuning the
operating system’s scheduler parameters could be a cheap but necessary step to
fully take advantage of the proposed computer architecture.

5 Feasibility

It is also important to comment on the readiness of current technologies to
support such fast reconﬁguration in future SoCs. The main evaluation refrained
from elaborating on this aspect to enable a discussion through system eﬀects.

5.1 Reconﬁguration Latency Representativeness

In order to demonstrate that future CPUs which feature FPGAs as functional
units can be reprogrammed under a latency of the order of magnitude studied in
sections 4.2 and 4.3, we present an example fast-reconﬁgurable FPGA architecture
and prototype it in simulation.

The modelled FPGA is based on a traditional FPGA fabric layout but directly
exposes a wide conﬁguration bus which can be loaded from a wide bitstream
cache. In contrast, typical FPGA architectures such as UltraScale+ constrain the
reconﬁguration port width to 32 bits [29]. The test designs for the FPGA were
based on the RISC-V bit manipulation extension [2], including clmul (carry-less
multiply) and bextdep (bit extract and deposition).

The FPGA is modelled inside nextpnr [24] using the viaduct plugin framework
for architectures, with a Verilog simulation model to conﬁrm that bitstreams can
be loaded in the target latency and function correctly. There are two connections
between the FPGA fabric and the CPU. A wide conﬁguration bus based on the
Pico Co-Processor Interface (PCPI) from PicoRV32 [28] is loaded through an L1
cache. Once the FPGA is conﬁgured, the fabric itself can also receive instruction
operands and source register values; and returns a destination value after some
cycles. This approach also enables partial instruction decoding; so one bitstream
could implement multiple related instructions.

A series of optimisations are applied to the architecture to minimise the
conﬁguration array size (and hence cache size and conﬁguration port width) and

9

reconﬁguration latency. The ﬁrst relates to the removal of features less likely
to be useful for this application, such as block RAM (BRAM) for storing large
states. The inclusion of DSPs is not explored, though this could further reduce
the conﬁguration state by avoiding the use of fabric resources e.g. for multipliers.
An optimisation relates to the type of the look-up tables (LUTs), which are
basic building blocks in FPGAs. 4-LUTs (i.e. with 4 inputs totalling 16 entries)
are used rather than 6-LUTs. In this way the size of the conﬁguration information
is reduced; full instead of one-hot muxes is used for the routing; and the number
of routing resources is generally minimised whilst keeping target designs routable.
LUT permutation and route-throughs in place and route were used to partially
compensate for the latter. The beneﬁt of 4-LUTs in this context is shown with
the experiment of ﬁgure 5, that determines the minimum conﬁguration FPGA
array size necessary to implement the bextdep benchmark. Future work includes
further optimising internal architecture, such as with fracturable LUTs.

6-LUT

5-LUT

4-LUT

Conﬁguration width

0

800 1600 2400 3200 4000

0

50

100

150

200

Bitstream size (K bits)

Fig. 5. LUT type versus bitstream size

Fig. 6. Modiﬁcation of SRAM FPGAs

When conﬁgured for 1680 LUTs, the bitstreams are a total of 91 kbits,
requiring a 1824-bit wide conﬁguration port for a 50-cycle reconﬁguration latency.
This is within the reasonable range of wide datapaths (see section 5.2), and it
could be reduced at the expense of latency. Similarly, a 250-cycle latency (that
still beneﬁted some applications) would only require a 365-bit-wide port.

A necessary architectural change was to keep the entire conﬁguration data
path equal to the number of bitlines, rather than narrowing to an 8-bit or 32-bit
external port or memory mapped conﬁguration interface. This can then be loaded
at full rate, in the target number of cycles, directly from the bitstream cache.

FPGAs generally use static RAM (SRAM) cells to store the conﬁguration
bits; and a word/bit line architecture to conﬁgure them. Architectures typically
have a similar number of word and bitlines to ease routing. However, this would
generally lead to unacceptably high conﬁguration latencies for this application.
Reducing the conﬁguration latency requires more bitlines and fewer wordlines
– the number of wordlines being equal to the latency, all things being equal. A
diagrammatic example of the implication of increasing wordlines to reduce latency
is shown in ﬁgure 6, simpliﬁed to few tiles and word/bitlines (showing only four
conﬁguration bits per tile, rather than a typical value of about a thousand).

10

bitlinesbitlinesClassical SRAM layout,boxes are tilesModified for widerreconfigurationwordlineswordlinesThis prototype uses a chain of shift registers to store the conﬁguration bits.
A conﬁguration word is being shifted through the chain each conﬁguration cycle
(the chain is 50 deep, 1824 wide). This is for brevity, but has not challenged the
routability of the test case. The operating frequency aspect is left as future work,
but does not seem prohibitive at the moment, given the margins for a hit latency
(section 4.2) and reports for instruction-like tasks operating in the GHz range [4].

5.2 Bitstream Cache Dimensions

To better understand the bitstream cache requirements for high-end processors,
a separate study is conducted on a commercial x86 platform with a higher
instruction variety (such as with vector instructions). By using dynamic binary
instrumentation (DBI), this section explores the spatial needs and temporal
localities with respect to the bitstream usage by comparing it to the traditional
instruction and memory usage.

The study of cache size requirements would normally involve measuring the
working set by simulating caches of diﬀerent sizes and levels and pinpointing the
size where the miss rate declines sharply. However, this could use assumptions
relating to data and instructions, such as about the longevity of the working
set (beneﬁting from multiple cache levels) and the access pattern (the notion of
working set implies certain access distributions in space and time).

Through a custom Intel Pin [19] tool, on every dynamic instruction call,
a routine updates a series of data structures for statistics on the opcodes, in-
struction pointers (IPs) and memory locations (where applicable). Each opcode
is perceived as a separate bitstream. This represents the worst case to avoid
specialisation in the observations, since it also includes control ﬂow and data
movement instructions, which are expected to be the most frequent [6, 12], as
well as for similar instructions that could be grouped together. A mask is applied
to ignore the last 6 bits for a 64-byte-granularity in data and instruction blocks,
which is commonly found in today’s x86 systems.

The data structures inside the Pin tool are mainly hashsets that provide
the number of unique opcodes, instruction and data blocks. On every n ∈ N
number of instructions, the 3 corresponding sets are cleared and their cardinality
is saved in lists (implemented as maps of <cardinality, occurrence> pairs to
conserve memory). This provides the distribution of compulsory miss cardinalities
occurring in the speciﬁed periods of time (measured in instructions) for each of
the opcode, instruction and memory cache blocks. For the instructions and data,
the algorithm’s input would represent the stream observed right before the L1
instruction and L1 data caches. Though, the spatiotemporal locality scope of
this experiment extends beyond the L1 caches. For the opcodes, this stream is
considered to be observed from the bitstream disambiguator.

The benchmark suite selection for the single-program experiment is the single-
core part of Geekbench 5. This is a series of 21 compute-intensive benchmarks
ranging from encryption to machine learning one by one. Here, the same instance
of the Intel Pin tool is used for the entirety of all Geekbench benchmarks.

11

These results are illustrated in ﬁgure 7. The x-axis summarises the time period
the hashsets are collecting information for, and are used to observe temporal
locality. The y-axis shows the observed median cardinalities for each hashset,
and represents the compulsory misses for each type of cache block (bitstream
for opcode, instruction for IPs and data for data addresses). The shaded regions
underneath show the lower and upper quartiles of the cardinalities in each
corresponding list of hashsets. As shown, the opcode count starts from below 16
for the shorter time slices, while peaking at below 64 for the longer time slices.

Fig. 7. Reuse behaviour in single-program Fig. 8. Reuse behaviour in multi-program

Another observation is that the opcode series is "ﬂatter" than the other curves,
meaning that there is higher reuse of opcodes than instruction and memory blocks.
It is also longer lasting. This could have been conjectured, but the relationship
between the 3 types of reuse is a result of multiple factors. For instance, each
instruction and data block already covers multiple locations (64-byte granularity),
whose reuse also depends on the memory access pattern. From the opcode’s
perspective, CISC ISAs like x86-64 include a rather high number, as with Intel
Pin’s catalogue of over 8000 entries. The fact that the instruction and data
series have a steeper upwards slope can also justify the need for multi-level cache
hierarchies, as their requirements grow more rapidly with time.

For the multi-program study, the same Pin tool (but with added thread safety
mechanisms) is used on the adapted Embench suite for section 4.3. This time
each benchmark is run as a thread using pthreads in Linux/x86_64 and is pinned
down to the same core for oversubscription. The benchmark suite is compiled as
a single binary, and with the -march=native ﬂag to promote vectorisation. The
idea of oversubscription here is to attempt increasing opcode demand, similar to
the operating system’s frequent migration of hundreds of tasks.

The results of the multi-program experiment is shown in ﬁgure 8, where the
median compulsory misses are measured for diﬀerent amounts of task oversub-
scription to a single core. The time window size is ﬁxed to 32768 instructions.

12

4166425610244096163846553664102416384262144Compulsory misses (median)Time slice length (in instructions)opcodesinstruction blocksdata blocksy=xGeekbench 5 (Single-Core)100200300400500124822Compulsory misses (median)Task oversubscriptionopcodesinstruction blocksEmbenchSuperimposed to the medians are violin plots, which are used to visually provide
more detailed distribution information than percentiles. The data behaviour was
fairly similar to the instruction blocks in this instance, hence its omission for
readability. As expected, there is higher reuse of opcodes than instructions among
the diﬀerent tasks when oversubscribed. For example, when all 22 tasks are run
simultaneously, the median and maximum opcode miss count reaches 59 and 152
respectively, while the respective numbers for instructions are 119 and 672.

These numbers show that the bitstream cache is feasible with today’s tech-
nology on SoCs. Speciﬁcally, a 64-block bitstream cache is shown to be enough
for relatively long periods of time in both the single-program and multi-program
experiments. This totals 768 KB of SRAM when using 12 KB bitstreams, being
inline with the example FPGA architecture of section 5.1. By observing recent
processor trends that feature, for example, up to 256 MB L3 caches [25], the sub-
MB size requirement is in the L2 territory. Additionally, a sub 2048-bit datapath
that is demonstrated in section 5.1 is only needed between this cache and the
instruction disambiguator, as the expected latency proﬁle of the bitstream cache
makes progressively-loaded bitstream blocks meaningful, even with 128/256-bit
datapaths to L2. Given the oversubscription experiment results and the read-only
nature of the bitstreams from the FPGA’s view, future multicores would also
beneﬁt from sharing of the bitstream cache(s).

These conclusions are drawn with the worst case approach in mind, such as by
associating compulsory misses with the desirable cache size, and by not classifying
opcodes into groups. A fraction of the reported desirable bitstream cache could
still beneﬁt future high-end CPUs with FPGAs working as instructions.

6 Related Work

Earlier research focused on using FPGAs as a functional unit. Garp [16] targets
embedded processors without multi-processing support, but it introduces the idea
of combining a bitstream alongside the process binary. It does not make FPGAs
transparent, as it requires conﬁguration instructions. DISC [27] is an earlier work
that elaborates on reconﬁguration in a similar context. Its instruction decoder is
similar to the proposed instruction disambiguator by using caching. It is not a
general-purpose computer architecture, as the processor has a separate ISA from
the host processor. Chimaera [30] provides a reconﬁgurable array to dynamically
load FPGA-based instruction implementations. This is somewhat reminiscent
of the proposed bitstream cache, but only supports specially-compiled software.
Architectures like CCA [13] and RISPP [9] aimed to improve the adaptability of
embedded systems by providing a set of specialised functional units that can be
dynamically selected at run-time. The latter does not involve FPGAs.

FABulous [18] is an open-source framework for integrating FPGAs into an
ASIC. One of its applications is for the implementation of eFPGAs, also for the
purposes of extending hardened cores. A RISC-V SoC with eFPGAs is presented
as a use case. Related research studied the integration of SIMD units [20],
but the insights were platform-related, such as with regards to Xilinx’ partial

13

reconﬁguration. The custom instruction usage is limited to specialised kernels,
and concepts like context-switching are not studied.

7 Conclusions

The FPGA-extended modiﬁed Harvard architecture can be used to transparently
fetch standardised ISA extensions or custom instructions through the computer’s
memory hierarchy. The disambiguator unit works as an L0 cache for the FPGA
slots and requests and multiplexes the bitstreams and instructions to reconﬁg-
urable regions. The evaluation showed promising results, generally surpassing the
performance of a core with a constrained extension subset. The operating system
in such computers is shown to beneﬁt from longer times between context-switches
to compensate for the reconﬁguration time. Finally, a low reconﬁguration latency
is deemed necessary for the eﬃciency of the proposal, and our feasibility study
ﬁnds this possible by mainly using existing FPGA building blocks and a cache
with appropriate dimensions for providing the bitstreams.

Acknowledgement We would like to thank Anuj Vaishnav for his feedback on an
earlier version. The second author mainly contributed with the section 5.1.

References

1. “RISC-V "V" Vector Extension, Version 0.9,” 2020.
2. “RISC-V "B" Bitmanip Extension, Version 0.94-draft,” 2021.
3. A. Waterman and K. Asanovic, “The RISC-V instruction set manual, volume I:
Unprivileged ISA, version 20191214-draft,” RISC-V Foundation, Tech. Rep, 2020.
4. Achronix Semiconductor Corp., “Speedcore architecture (accessed on 10/04/2022).”

[Online]. Available: https://www.achronix.com/speedcore-architecture

5. S. Z. Ahmed, “efpgas: Architectural explorations, system integration & a visionary
industrial survey of programmable technologies,” Ph.D. dissertation, Université
Montpellier II-Sciences et Techniques du Languedoc, 2011.

6. A. Akshintala, B. Jain, C.-C. Tsai, M. Ferdman, and D. E. Porter, “X86-64 in-
struction usage among c/c++ applications,” in Proceedings of the 12th ACM
International Conference on Systems and Storage, 2019, pp. 68–79.

7. G. Alonso, Z. Istvan, K. Kara, M. Owaida, and D. Sidler, “doppiodb 1.0: Machine
learning inside a relational engine.” IEEE Data Eng. Bull., vol. 42, no. 2, pp. 19–31,
2019.

8. R. Barry, FreeRTOS reference manual: API functions and conﬁguration options.

Real Time Engineers Limited, 2009.

9. L. Bauer, M. Shaﬁque, and J. Henkel, “Rispp: A run-time adaptive reconﬁgurable
embedded processor,” in 2009 International Conference on Field Programmable
Logic and Applications.

IEEE, 2009, pp. 725–726.
10. R. Bordawekar, U. Bondhugula, and R. Rao, “Can cpus match gpus on performance
with productivity?: Experiences with optimizing a ﬂop-intensive application on
cpus and gpu,” IBM Research Report, RC25033, Tech. Rep., 2010.

11. J. M. Cebrian, L. Natvig, and M. Jahre, “Scalability analysis of avx-512 extensions,”

The journal of supercomputing, vol. 76, no. 3, pp. 2082–2097, 2020.

14

12. Y.-J. Chang, “Exploiting frequent opcode locality for power eﬃcient instruction
cache,” in Proc. of the 18th ACM Great Lakes sym. on VLSI, 2008, pp. 399–402.
13. N. Clark, J. Blome, M. Chu, S. Mahlke, S. Biles, and K. Flautner, “An architecture
framework for transparent instruction set customization in embedded processors,”
IEEE, 2005, pp. 272–283.
in 32nd Intl Sym. on Computer Architecture (ISCA’05).
14. M. Gottschlag, T. Schmidt, and F. Bellosa, “Avx overhead proﬁling: How much
does your fast code slow you down?” in Proceedings of the 11th ACM SIGOPS
Asia-Paciﬁc Workshop on Systems, ser. APSys ’20. ACM, 2020, p. 59–66.
15. J. H. Grosbach, J. M. Conner, and M. Catherwood, “Modiﬁed harvard architecture
processor having program memory space mapped to data memory space,” Apr. 27
2004, US Patent 6,728,856.

16. J. R. Hauser and J. Wawrzynek, “Garp: A mips processor with a reconﬁgurable copro-
cessor,” in Proceedings. The 5th Annual IEEE Symposium on Field-Programmable
Custom Computing Machines Cat. No. 97TB100186).

IEEE, 1997, pp. 12–21.

17. Intel (R), “Intel intrinsics guide.” [Online]. Available: https://software.intel.com/

sites/landingpage/IntrinsicsGuide/

18. D. Koch, N. Dao, B. Healy, J. Yu, and A. Attwood, “FABulous: an Embedded
FPGA Framework,” in The 2021 ACM/SIGDA International Symposium on Field-
Programmable Gate Arrays, 2021, pp. 45–56.

19. C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney, S. Wallace, V. J.
Reddi, and K. Hazelwood, “Pin: building customized program analysis tools with
dynamic instrumentation,” Acm sigplan notices, vol. 40, no. 6, pp. 190–200, 2005.
20. J. R. G. Ordaz and D. Koch, “A soft dual-processor system with a partially run-time
reconﬁgurable shared 128-bit simd engine,” in 29th Intl Conf. on Application-speciﬁc
Systems, Architectures and Processors (ASAP).

IEEE, 2018, pp. 1–8.

21. P. Papaphilippou and W. Luk, “Accelerating database systems using FPGAs: A
survey,” in 2018 28th International Conference on Field Programmable Logic and
Applications (FPL).

IEEE, 2018, pp. 125–130.

22. P. Papaphilippou, K. Paul H. J., and W. Luk, “Simodense: a risc-v softcore optimised
for exploring custom simd instructions,” in 2021 31st International Conference on
Field-Programmable Logic and Applications (FPL), Aug 2021, pp. 391–397.

23. D. Patterson, J. Bennett, P. Dabbelt, C. Garlati, G. Madhusudan, and T. Mudge,

“Embench: A modern embedded benchmark suite,” 2020.

24. D. Shah, E. Hung, C. Wolf, S. Bazanski, D. Gisselquist, and M. Milanovic,
“Yosys+nextpnr: An open source framework from verilog to bitstream for commer-
cial fpgas,” in IEEE 27th Annual International Symposium on Field-Programmable
Custom Computing Machines (FCCM), 2019.

25. D. Suggs, M. Subramony, and D. Bouvier, “The amd “zen 2” processor,” IEEE

Micro, vol. 40, no. 2, pp. 45–52, 2020.

26. Y. E. Wang, G.-Y. Wei, and D. Brooks, “Benchmarking TPU, GPU, and CPU

platforms for deep learning,” arXiv preprint arXiv:1907.10701, 2019.

27. M. J. Wirthlin and B. L. Hutchings, “Disc: The dynamic instruction set computer,”
in Field Programmable Gate Arrays (FPGAs) for Fast Board Development and
Reconﬁgurable Computing, vol. 2607. SPIE, 1995, pp. 92–103.

28. C. Wolf, “PicoRV32-a size-optimized RISC-V CPU,” 2019.
29. Xilinx Inc., “Vivado Design Suite User Guide, Partial Reconﬁguration - UG909

(v2018.1),” 2018.

30. Z. A. Ye, A. Moshovos, S. Hauck, and P. Banerjee, “Chimaera: A high-performance
architecture with a tightly-coupled reconﬁgurable functional unit,” ACM SIGARCH
computer architecture news, vol. 28, no. 2, pp. 225–235, 2000.

15

