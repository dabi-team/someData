A gentle tutorial on accelerated parameter and conﬁdence interval estimation
for hidden Markov models using Template Model Builder

Timoth´ee Bacri 1, Geir D. Berentsen 2, Jan Bulla* 1,3, Sondre Hølleland 2,4
1 Department of Mathematics, University of Bergen, Postbox 7803, 5007 Bergen, Norway
2 Department of Business and Management Science, Norwegian School of Economics, Helleveien 30, 5045
Bergen, Norway
3 Department of Psychiatry and Psychotherapy, University of Regensburg, Universit¨atsstraße 84, 93053
Regensburg, Germany
4 Department of Pelagic Fish, Institute of Marine Research, Postbox 1870, 5817 Bergen, Norway

A very common way to estimate the parameters of a hidden Markov model (HMM) is the relatively
straightforward computation of maximum likelihood (ML) estimates. For this task, most users rely on
user-friendly implementation of the estimation routines via an interpreted programming language such
as the statistical software environment R (R Core Team, 2021). Such an approach can easily require
time-consuming computations, in particular for longer sequences of observations. In addition, select-
ing a suitable approach for deriving conﬁdence intervals for the estimated parameters is not entirely
obvious (see, e.g., Zucchini et al., 2016; Lystig and Hughes, 2002; Visser et al., 2000), and often the
computationally intensive bootstrap methods have to be applied.

In this tutorial, we illustrate how to speed up the computation of ML estimates signiﬁcantly via the
R package TMB. Moreover, this approach permits simple retrieval of standard errors at the same time.
We illustrate the performance of our routines using different data sets. First, two smaller samples from a
mobile application for tinnitus patients and a well-known data set of fetal lamb movements with 87 and
240 data points, respectively. Second, we rely on larger data sets of simulated data of sizes 2000 and
5000 for further analysis. This tutorial is accompanied by a collection of scripts which are all available
on GitHub. These scripts allow any user with moderate programming experience to beneﬁt quickly from
the computational advantages of TMB.

Key words: Hidden Markov model; TMB; Conﬁdence intervals; Maximum Likelihood Estimation; Tu-
torial

Supporting Information for this article is available from the authors or on the WWW under
https://timothee-bacri.github.io/HMM with TMB

*Corresponding author: e-mail: jan.bulla@uib.no, phone: +47 55 58 28 75

1

2
2
0
2

y
a
M
7

]

O
C

.
t
a
t
s
[

1
v
6
8
6
3
0
.
5
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
1 Introduction

Hidden Markov models (HMMs) are a well-established, versatile type of model employed in many dif-
ferent applications. Since their ﬁrst application in speech recognition (see, e.g., Baum and Petrie, 1966;
Fredkin and Rice, 1992; Gales and Young, 2008), HMMs found wide usage in many applied sciences.
To name only a few, biology and bioinformatics (Schadt et al., 1998; Durbin, 1998; Eddy, 1998), ﬁnance
(Hamilton, 1989), ecology (McClintock et al., 2020), stochastic weather modeling (Lystig and Hughes,
2002; Ailliot et al., 2015), and engineering (Mor et al., 2021). Furthermore, the scientiﬁc literature
on this topic in statistics is rich, as illustrated e.g. by the manuscripts of Zucchini et al. (2016); Capp´e
et al. (2006); Bartolucci et al. (2012). The aforementioned sources contain, among many other aspects,
detailed descriptions of parameter estimation for HMMs by maximization of the (log-)likelihood func-
tion. In short, maximum likelihood (ML) estimation is commonly achieved either by a direct numerical
maximization as introduced by Turner (2008) and later detailed by Zucchini et al. (2016), who also
provided a collection of R (R Core Team, 2021) scripts that is widely used. Alternatively, Expectation
Maximization (EM) type algorithms as ﬁrstly described by Baum et al. (1970) or Dempster et al. (1977)
serve for parameter estimation equally well. These algorithms possess several advantageous properties,
for example, their robustness to poor initial values compared to direct numerical maximization via hill-
climbing algorithms. For more details on the EM algorithm in the context of HMMs and a comparison
of both approaches, see Bulla and Berzel (2008), who also describe a hybrid approach combining both
algorithms.

Evaluating uncertainty and obtaining conﬁdence intervals (CIs) constitutes another essential aspect
when working with HMMs - and it is less straightforward than parameter estimation. Although Capp´e
et al. (2006, Ch. 12) showed that CIs could be obtained based on asymptotic normality of the ML
estimates of the parameters under certain conditions, Fr¨uhwirth-Schnatter (2006, p. 53) points out that in
independent mixture models, “the regularity conditions are often violated”. McLachlan and Peel (2004,
p. 68) adds that “In particular for mixture models, it is well known that the sample size n has to be very
large before the asymptotic theory of maximum likelihood applies.” Lystig and Hughes (2002) shows a
way to compute the exact Hessian, and Zucchini et al. (2016) presents an alternative way to compute the
approximate Hessian and thus conﬁdence intervals, but admits that “the use of the Hessian to compute
standard errors (and thence conﬁdence intervals) is unreliable if some of the parameters are on or near
the boundary of their parameter space”.
In addition, Visser et al. (2000) report that computational
problems arise when deriving CIs from the Hessian for sequences longer than 100 observations.

In this tutorial, we illustrate how to accelerate parameter estimation for HMMs with the help of
Template Model Builder (TMB). As described by Kristensen et al. (2016), TMB is an R package devel-
oped for efﬁciently ﬁtting complex statistical models to data. It provides exact calculations of ﬁrst-
and second-order derivatives of the (log-)likelihood of a model by automatic differentiation, which al-
lows for efﬁcient gradient- and/or Hessian-based optimization of the likelihood on the one hand. On
the other hand, TMB permits to infer CIs for the estimated parameters by means of the Hessian. We
show how to carry out this part for HMMs using a couple of simple examples. Then, we compare the
Hessian-based CIs with CIs resulting from likelihood proﬁling and bootstrapping, which are both more
computationally intensive.

The tutorial is accompanied by a collection of scripts (listed at https://timothee-bacri
.github.io/HMM with TMB/github.html#directory-structure), which guide any
user working with HMMs through the implementation of computationally efﬁcient parameter estima-
tion. The majority of scripts require only knowledge of R, just the computation of the (log-)likelihood
function requires the involvement of C++. Moreover, we illustrate how TMB permits Hessian- or proﬁle
likelihood-based CIs for the estimated parameters at a very low computational cost. Naturally, the accel-
erated parameter estimation procedure may also serve for implementing computationally more efﬁcient
bootstrap CIs, an aspect we also make use of for our analyses.

2

2 Principles of using TMB for maximum likelihood estimation

In order to keep this tutorial at acceptable length, all sections follow the same concept. That is, the
reader is encouraged to consult the respective part of our GitHub repository in parallel to reading each
section; it is available at https://timothee- bacri.github.io/HMM with TMB.
In
particular, we recommend opening the ﬁle Data supplements.Rproj (available in the related repository at
https://github.com/timothee-bacri/HMM with TMB) with R-Studio, which lets the
reader have the correct working path set up automatically. This permits to copy-paste or download all
the scripts presented in this tutorial for each section on the one hand. On the other hand, it allows
for consistent maintenance of the code. Moreover, the repository also contains additional explanations,
comments, and scripts.

2.1 Setup

Execution of our routines requires the installation of the R-package TMB and the software Rtools,
where the latter serves for compiling C++ code. In order to ensure reproducibility of all results involving
the generation of random numbers, the set.seed function requires R version number 3.6.0 or greater.
Our scripts were tested on a workstation with 4 Intel(R) Xeon(R) Gold 6134 processors (3.7 GHz)
each running under the Linux distribution Ubuntu 18.04.6 LTS (Bionic Beaver) with 384 GB RAM and
required about one week of computing time.

In particular for beginners, those parts of scripts involving C++ code can be difﬁcult to debug be-
cause the code operates using a speciﬁc template. Therefore it is helpful to know that TMB provides a
debugging feature, which can be useful to retrieve diagnostic error messages, in RStudio. Enabling this
feature is optional and can be achieved by the command TMB:::setupRStudio() (requires manual
conﬁrmation and re-starting RStudio).

2.2 Linear regression example

We begin by demonstrating the principles of TMB, which we illustrate through the ﬁtting procedure for
a simple linear model. This permits, among other things, to show how to handle parameters subject to
constraints, an aspect particularly relevant for HMMs. A more comprehensive tutorial on TMB present-
ing many technical details in more depths is available at https://kaskr.github.io/adcomp
/ book/Tutorial.html.

Let x and y denote the predictor and response vector, respectively, both of length n. For a simple

linear regression model with intercept a and slope b, the negative log-likelihood equals

− log L(a, b, σ2) = −

n
(cid:88)

i=1

log(φ(yi; a + bxi, σ2)),

where φ(·; µ, σ2) corresponds to the density function of the univariate normal distribution with mean µ
and variance σ2.

The use of TMB requires the (negative) log-likelihood function to be coded in C++ under a spe-
ciﬁc template, which is then loaded into R. The minimization of this function and other post-processing
procedures are all carried out in R. Therefore, we require two ﬁles. The ﬁrst ﬁle, named linreg.cpp, is
written in C++ and deﬁnes the objective function, i.e. the negative log-likelihood (nll) function of the
linear model, as follows.

#include <TMB.hpp> //import the TMB template

3

template<class Type>
Type objective_function<Type>::operator() ()
{

DATA_VECTOR(y); // Data vector y passed from R
DATA_VECTOR(x); // Data vector x passed from R

PARAMETER(a);
PARAMETER(b);
PARAMETER(tsigma);

// Parameter a passed from R
// Parameter b passed from R
// Parameter sigma (transformed, on log-scale)
// passed from R

// Transform tsigma back to natural scale
Type sigma = exp(tsigma);

// Declare negative log-likelihood
Type nll = - sum(dnorm(y,

a + b * x,
sigma,
true));

// Necessary for inference on sigma, not only tsigma
ADREPORT(sigma);

return nll;

}

Note that we deﬁne data inputs x and y using the DATA VECTOR() declaration in the above
code. Furthermore, we declare the nll as a function of the three parameters a, b and log(σ) using the
PARAMETER() declaration. In order to be able to carry out unconstrained optimization procedures in
the following, the nll function is parametrized in terms of log(σ). While the parameter σ is constrained
to be non-negative, log(σ) can be freely estimated. Alternatively, constraint optimization methods could
be carried out, but we do not investigate such procedures. The ADREPORT() function is optional but
useful for parameter inference at the postprocessing stage.

The second ﬁle needed is written in R and serves for compiling the nll function deﬁned above and
carrying out the estimation procedure by numerical optimization of the nll function. The .R ﬁle (shown
below) carries out the compilation of the C++ ﬁle and minimization of the nll function:

# Loading TMB package
library(TMB)
# Compilation. The compiler returns 0 if the compilation of
# the cpp file was successful
TMB::compile("code/linreg.cpp")

## [1] 0

# Dynamic loading of the compiled cpp file
dyn.load(dynlib("code/linreg"))
# Generate the data for our test sample
set.seed(123)
data <- list(y = rnorm(20) + 1:20, x = 1:20)
parameters <- list(a = 0, b = 0, tsigma = 0)
# Instruct TMB to create the likelihood function

4

obj_linreg <- MakeADFun(data, parameters, DLL = "linreg",

silent = TRUE)

# Optimization of the objective function with nlminb
# Arguments are the initial parameters, the objective function to
# minimize, and the gradient and Hessian passed from TMB
mod_linreg <- nlminb(start = obj_linreg$par,

objective = obj_linreg$fn,
gradient = obj_linreg$gr,
hessian = obj_linreg$he)

mod_linreg$par

##
##

tsigma
0.31009251 0.98395536 -0.05814649

a

b

In addition to the core functionality presented above, different types of post-processing of the results
are possible as well. For example, the function sdreport returns the ML estimates and standard errors
of the parameters in terms of which the nll is parametrized:

sdreport(obj_linreg, par.fixed = mod_linreg$par)

## sdreport(.) result
Estimate Std. Error
##
0.31009251 0.43829087
## a
0.98395536 0.03658782
## b
## tsigma -0.05814649 0.15811383
## Maximum gradient component: 1.300261e-10

In principle, the argument par.fixed = mod linreg$par is optional but recommended, be-
cause it ensures that the sdreport function is carried out at the minimum found by nlminb (Gay,
1990). Instead of nlminb, other optimization routines may be used as well. For example Zucchini
et al. (2016) rely on the R function nlm. We selected nlminb because it offers a higher degree of
ﬂexibility, while having a syntax close to nlm. Note that the standard errors above are based on the
Hessian matrix of the nll.

From a practical perspective, it is usually desirable to obtain standard errors for the constrained
variables, in this case σ. To achieve this, one can run the summary function with argument select
= "report":

summary(sdreport(obj_linreg, par.fixed = mod_linreg$par),

select = "report")

Estimate Std. Error
##
## sigma 0.9435117 0.1491823

These standard errors result from the generalized delta method described by Kass and Steffey (1989),
which is implemented within TMB. Note that full functionality of the sdreport function requires call-
ing the function ADREPORT on the additional parameters of interest (i.e. those including transformed
parameters, in our example σ) in the C++ part. The select argument restricts the output to variables
passed by ADREPORT. This feature is particularly useful when the likelihood has been reparametrized
as above, and is especially relevant for HMMs. Following Zucchini et al. (2016), we refer to the original
parameters as natural parameters, and to their transformed version as the working parameters.

5

Last, we display the estimation results from the lm function for comparison.

rbind(

"lm" = lm(y ˜ x, data = data)$coef, # linear regression using R
"TMB" = mod_linreg$par[1:2] # intercept and slope from TMB fit

)

##
## lm
## TMB

(Intercept)

x
0.3100925 0.9839554
0.3100925 0.9839554

3 Parameter estimation techniques for HMMs

In this section we recall basic concepts underlying parameter estimation for HMMs via direct numerical
optimization of the likelihood.
In terms of notation, we stay as close as possible to Zucchini et al.
(2016), where a more detailed presentation is available.

3.1 Basic notation and model setup

A large variety of modeling approaches is possible with HMMs, ranging from rather simple to highly
complex setups. In a basic HMM, one assumes that the data-generating process corresponds to a time-
dependent mixture of conditional distributions. More speciﬁcally, the mixing process is driven by an
unobserved (hidden) homogeneous Markov chain. In this paper we focus on a Poisson HMM, but only
small changes are necessary to adapt our scripts to models with other conditional distributions. For an
example of the application of TMB in a more complex HMM setting, see Berentsen et al. (2022). Let
{Xt : t = 1, . . . , T } and {Ct : t = 1, . . . , T } denote the observed and hidden process, respectively,
where t denotes the (time) index ranging from one to T . For an m-state Poisson HMM, the conditional
distributions with parameter λi are then speciﬁed through

pi(x) = P(Xt = x|Ct = i) =

e−λiλx
i
x!

,

where i = 1, . . . , m. Furthermore, we let Γ = {γij} and δ denote the transition probability matrix
(TPM) of the Markov chain and the corresponding stationary distribution, respectively. It is noteworthy
that Markov chains in the context of HMMs are often assumed irreducible and aperiodic. For exam-
ple, Grimmett and Stirzaker (2001, Lemma 6.3.5 on p. 225 and Theorem 6.4.3 on p. 227) show that
irreducibility ensures the existence of the stationary distribution, and Feller (1968, p. 394) describe that
aperiodicity implies that a unique limiting distribution exists and corresponds to the stationary distri-
bution. These results are, however, of limited relevance for most estimation algorithms, because the
elements of Γ are in general strictly positive. Nevertheless, one should be careful when manually ﬁxing
selected elements of Γ to zero.

3.2 The likelihood function of an HMM

The likelihood function of an HMM requires, in principle, a summation over all possible state sequences.
As shown e.g. by Zucchini et al. (2016, p. 37), a computationally convenient representation as a product
of matrices is possible. Let X (t) = {X1, . . . , Xt} and x(t) = {x1, . . . , xt} denote the ’history’ of
the observed process Xt and the observations xt, respectively, from time one up to time t. Moreover,
let θ denote the vector of model parameters, which consists of the parameters of the TPM and the

6

parameters of the conditional probability density functions. Given these parameters, the likelihood of
the observations {x1, . . . , xT } can then be expressed as

L(θ) = P(X (T ) = x(T )) = δP(x1)ΓP(x2)ΓP(x3) . . . ΓP(xT )1(cid:48),

(1)

where

P(x) =



p1(x)






0

p2(x)

. . .

0








pm(x)

corresponds to a diagonal matrix with the m conditional probability density functions evaluated at x (we
will use the term density despite the discrete support), and 1 denotes a vector of ones. The ﬁrst element
of the likelihood function, the so-called initial distribution, is given by the stationary distribution δ
here. Alternatively, the initial distribution may be estimated freely, which requires minor changes to the
likelihood function discussed in Section 4.1.

Note that the treatment of missing data is comparably straightforward in this setup. If x is a missing
observation, one just has to set pi(x) = 1, thus P(x) reduces to the unity matrix as detailed in Zucchini
et al. (2016, p. 40). Zucchini et al. (2016, p. 41) also explains how to adjust the likelihood when entire
intervals are missing. Furthermore, this representation of the likelihood is quite natural from an intuitive
point of view. From left to right, it can be interpreted as a pass through the observations: one starts with
the initial distribution multiplied by the conditional density of x1 collected in P(x1). This is followed by
iterative multiplications with the TPM modeling the transition to the next observation, and yet another
multiplication with contributions of the following conditional densities.

3.3 Forward algorithm and backward algorithm

The pass through the observations described above actually forms the basis for an efﬁcient evaluation
of the likelihood function. More precisely, the so-called “forward algorithm” allows for a recursive
computation of the likelihood. For setting up this algorithm, we need to deﬁne the vector αt by

αt = δP(x1)ΓP(x2)ΓP(x3) . . . ΓP(xt)

= δP(x1)

t
(cid:89)

s=2

ΓP(xs)

= (αt(1), . . . , αt(m))

for t = 1, 2, . . . , T . The name forward algorithm originates from the way of calculating αt, i.e.

α0 = δP(x1)
αt = αt−1ΓP(xt) for t = 1, 2, . . . , T.

After a pass through all observations, the likelihood results from

L(θ) = αT 1(cid:48).

In a similar way, the “backward algorithm” also permits the recursive computation of the likelihood, but
starting with the last observation. To formulate the backward algorithm, let us deﬁne the vector βt for

7

t = 1, 2, . . . , T so that

β(cid:48)

t = ΓP(xt+1)ΓP(xt+2) . . . ΓP(xT ) . . . 1(cid:48)
(cid:33)

(cid:32) T
(cid:89)

=

ΓP(xs)

1(cid:48)

s=t+1

= (βt(1), . . . , βt(m)) .

The name backward algorithm results from the way of calculating βt, i.e.

βT = 1(cid:48)
βt = ΓP(xt+1)βt+1 for t = T − 1, T − 2, . . . , 1.

Again, the likelihood can be calculated after a pass through all observations by

L(θ) = δβ1.

In general, parameter estimation bases on the forward algorithm. The backward algorithm is, however,
still useful because the quantities αt and βt together serve for a couple of interesting tasks. For example,
they are the basis for deriving a particular type of conditional distributions and for state inference by
local decoding (Zucchini et al., 2016, Ch. 5, pp. 81-93). We present details on local decoding on the
GitHub page.

Last, it is well-known that the execution of the forward (or backward) algorithm may quickly lead
to underﬂow errors, because many elements of the vectors and matrices involved take values between
zero and one. To avoid these difﬁculties, a scaling factor can be introduced. We follow the approach
suggested by Zucchini et al. (2016, p. 48) and implement a scaled version of the forward algorithm,
which directly provides the (negative) log-likelihood as result.

3.4 Reparametrization of the likelihood function

The representation of the likelihood and the algorithms shown above rely on the data and the set of
parameters θ as input. The data are subject to several constraints:

(i) Typically there are various constraints of the parameters in the conditional distribution. For the
Poisson HMM, all elements of the parameter vector λ = (λ1, . . . , λm) must be non-negative.

(ii) In general, the parameters γij of the TPM Γ have to be non-negative, and the rows of Γ must sum

up to one.

The constraints of the TPM can be difﬁcult to deal with using constrained optimization of the likeli-
hood. A common approach is to reparametrize the log-likelihood in terms of unconstrained “working”
parameters {T, η} = g−1(Γ, λ), as follows. A possible reparametrization of Γ is given by

exp(τij)
k(cid:54)=i exp(τik)
where τij are m(m − 1) real-valued, thus unconstrained, elements of an m times m matrix T with no
diagonal elements. The diagonal elements of Γ follows implicitly from (cid:80)
j γij = 1 ∀ i (Zucchini et al.,
2016, p. 51). The corresponding reverse transformation is given by

, for i (cid:54)= j,

1 + (cid:80)

γij =

τij = log

(cid:32)

γij
1 − (cid:80)
k(cid:54)=i γik

(cid:33)

8

= log(γij/γii), for i (cid:54)= j.

For the Poisson HMM the intensities can be reparametrized in terms of λi = exp(ηi), and conse-
quently the unconstrained working parameters are given by ηi = log(λi), i = 1, . . . , m. Estimates of
the “natural” parameters {Γ, λ} can then be obtained by maximizing the reparametrized likelihood with
respect to {T, η} and then transforming the estimated working parameters back to natural parameters
via the above transformations, i.e. {ˆΓ, ˆλ} = g( ˆT, ˆη). Note that in general the function g needs to be
one-to-one for the above procedure to work.

4 Using TMB

In the following we show how ML estimation of the parameters of HMMs can be carried out efﬁciently
via TMB.

4.1 Likelihood function

Similar to the linear regression example presented in Section 2.2, the ﬁrst and essential step is to deﬁne
our nll function to be minimized later in a suitable C++ ﬁle. In our case, this function calculates the neg-
ative log-likelihood presented by Zucchini et al. (2016, p. 48), and our C++ code is analog to the R-code
shown by Zucchini et al. (2016, p. 331 - 333). This function, deﬁned in the ﬁle named poi hmm.cpp,
tackles our setting with conditional Poisson distributions only. An extension to, e.g., Gaussian, binomial
and exponential conditional distributions is straightforward. It requires to modify the density function
in the poi hmm.cpp ﬁle and the related functions for parameter transformation presented in Section 3.4.
We illustrate the implementation of the Gaussian case in the GitHub repository.

However, note that the number of possible modelling setups is very large: e.g., the conditional
distributions may vary from state to state, nested model speciﬁcations, the conditional mean may be
linked to covariates, or the TPM could depend on covariates - to name only a few. Due to the very
large number of possible extensions of the basic HMM, we refrain from implementing an R-package,
but prefer to provide a proper guidance to the reader for building custom models suited to a particular
application. As a small example, we illustrate how to implement a freely estimated initial distribution
in the ﬁle poi hmm.cpp. This modiﬁcation can be achieved by uncommenting a couple of lines only.

The ﬁle poi hmm.cpp is available at https://github.com/timothee-bacri/HMM wit

h TMB/blob/main/code/poi hmm.cpp and contains the following.

#include <TMB.hpp>
#include "../functions/utils.cpp"

// Likelihood for a poisson hidden markov model.
template<class Type>
Type objective_function<Type>::operator() ()
{

// Data
DATA_VECTOR(x);
DATA_INTEGER(m);

// timeseries vector
// Number of states m

// Parameters
PARAMETER_VECTOR(tlambda);
PARAMETER_VECTOR(tgamma);

// conditional log_lambdas's
// m(m-1) working parameters of TPM

// Uncomment only when using a non-stationary distribution
//PARAMETER_VECTOR(tdelta);

// transformed stationary distribution,

9

// Transform working parameters to natural parameters:
vector<Type> lambda = tlambda.exp();
matrix<Type> gamma = gamma_w2n(m, tgamma);

// Construct stationary distribution
vector<Type> delta = stat_dist(m, gamma);
// If using a non-stationary distribution, use this instead
//vector<Type> delta = delta_w2n(m, tdelta);

// Get number of timesteps (n)
int n = x.size();

// Evaluate conditional distribution: Put conditional
// probabilities of observed x in n times m matrix
// (one column for each state, one row for each datapoint):
matrix<Type> emission_probs(n, m);
matrix<Type> row1vec(1, m);
row1vec.setOnes();
for (int i = 0; i < n; i++) {

if (x[i] != x[i]) { // f != f returns true if and only if f is NaN.

// Replace missing values (NA in R, NaN in C++) with 1
emission_probs.row(i) = row1vec;

}
else {

emission_probs.row(i) = dpois(x[i], lambda, false);

}

}

// Corresponds to (Zucchini et al., 2016, p 333)
matrix<Type> foo, P;
Type mllk, sumfoo, lscale;

foo = (delta * vector<Type>(emission_probs.row(0))).matrix();
sumfoo = foo.sum();
lscale = log(sumfoo);
foo.transposeInPlace();
foo /= sumfoo;
for (int i = 2; i <= n; i++) {

P = emission_probs.row(i - 1);
foo = ((foo * gamma).array() * P.array()).matrix();
sumfoo = foo.sum();
lscale += log(sumfoo);
foo /= sumfoo;

}
mllk = -lscale;

// Use adreport on variables for which we want standard errors
ADREPORT(lambda);
ADREPORT(gamma);
ADREPORT(delta);

// Variables we need for local decoding and in a convenient format

10

REPORT(lambda);
REPORT(gamma);
REPORT(delta);
REPORT(n);
REPORT(emission_probs);
REPORT(mllk);

return mllk;

}

4.2 Optimization

With the nll function available in C++, we can carry out the parameter estimation and all pre-/post-
processing in R. In the following we describe the steps to be carried out.

(i) Loading of the necessary packages, compilation of the nll function with TMB and subsequent

loading, and loading of the auxiliary functions for parameter transformation.

# Load TMB and optimization packages
library(TMB)
# Run the C++ file containing the TMB code
TMB::compile("code/poi_hmm.cpp")

## [1] 0

# Load it
dyn.load(dynlib("code/poi_hmm"))
# Load the parameter transformation function
source("functions/utils.R")

(ii) Loading of the observations. The data are part of a large data set collected with the “Track
Your Tinnitus” (TYT) mobile application, a detailed description of which is presented in Pryss
et al. (2015b) and Pryss et al. (2015a). We analyze 87 successive days of the “arousal” variable
recorded for a single individual. This variable is measured on a discrete scale, where higher values
correspond to a higher degree of excitement and lower values to a more calm emotional state (for
details, see Probst et al., 2016, 2017).

Loading the “arousal” variable can be achieved simply with

load("data/tinnitus.RData")

Table 1 presents the raw data, which are also available for download at the GitHub repository.

6 5 3 6 4 3 5 6 6 6 4 6 6 4 6 6 6 6 6 4 6 5 6 7 6 5 5 5 7 6 5 6 5 6 6 6 5 6 7 7 6 7 6 6 6 6 5 7 6 1 6 0 2 1 6 7 6 6
6 5 5 6 6 2 5 0 1 1 1 2 3 1 3 1 3 0 1 1 1 4 1 4 1 2 2 2 0

Table 1: TYT data. Observations collected by the TYT app on 87 successive days (from left to right)
for a single individual.

11

(iii) Initialization of the number of states and starting (or initial) values for the optimization. First,
the number of states needs to be determined. As explained by Pohle et al. (2017b), Pohle et al.
(2017a), and Zucchini et al. (2016, Section 6) (to name only a few), usually one would ﬁrst ﬁt
models with a different number of states. Then, these models are evaluated e.g. by means of
model selection criteria (as carried out by Leroux and Puterman, 1992) or prediction performance
(Celeux and Durand, 2008). The model selection procedure shows that both AIC and BIC prefer
a two-state model over a model with three or four states. Consequently, we focus on the two-state
model in the following.
The list object TMB data contains the data and the number of states.

# Model with 2 states
m <- 2
TMB_data <- list(x = tinn_data, m = m)

Secondly, initial values for the optimization procedure need to be deﬁned. Although we will
apply unconstrained optimization, we initialize the natural parameters, because this is much more
intuitive and practical than handling the working parameters.

# Generate initial set of parameters for optimization
lambda <- c(1, 3)
gamma <- matrix(c(0.8, 0.2,

0.2, 0.8), byrow = TRUE, nrow = m)

(iv) Transformation from natural to working parameters. The previously created initial values are

transformed and stored in the list parameters for the optimization procedure.

# Turn them into working parameters
parameters <- pois.HMM.pn2pw(m, lambda, gamma)

(v) Creation of the TMB negative log-likelihood function with its derivatives. This object, stored as
obj tmb requires the data, the initial values, and the previously created DLL as input. Setting
argument silent = TRUE disables tracing information and is only used here to avoid excessive
output.

obj_tmb <- MakeADFun(TMB_data, parameters,

DLL = "poi_hmm", silent = TRUE)

This object also contains the previously deﬁned initial values as a vector (par) rather than a
list. The negative log-likelihood (fn), its gradient (gr), and Hessian (he) are functions of the
parameters (in vector form) while the data are considered ﬁxed:

obj_tmb$par

tlambda

##
tgamma
## 0.000000 1.098612 -1.386294 -1.386294

tlambda

tgamma

obj_tmb$fn(obj_tmb$par)

12

## [1] 228.3552

obj_tmb$gr(obj_tmb$par)

##
[,4]
## [1,] -3.60306 -146.0336 10.52832 -1.031706

[,3]

[,1]

[,2]

obj_tmb$he(obj_tmb$par)

[,1]

[,3]
##
## [1,] 1.902009 -5.877900 -1.3799682
## [2,] -5.877900 188.088247 -4.8501589
## [3,] -1.379968 -4.850159
## [4,] 2.405402

[,4]
2.4054017
2.3434284
9.6066700 -0.8410438
0.7984216

2.343428 -0.8410438

[,2]

(vi) Execution of the optimization. For this step we rely again on the optimizer implemented in the
initial values for the parameters and the function to be

nlminb function. The arguments, i.e.
optimized, are extracted from the previously created TMB object.

mod_tmb <- nlminb(start = obj_tmb$par, objective = obj_tmb$fn)
# Check that it converged successfully
mod_tmb$convergence == 0

## [1] TRUE

As mentioned previously, various alternatives to nlminb exist and could be used at this step
instead.

(vii) Obtaining the ML estimates of the natural parameters together with their standard errors is pos-
sible by using the previously introduced command sdreport. Recall that this requires the
parameters of interest to be treated by the ADREPORT statement in the C++ part. It should be
noted that the presentation of the set of parameters gamma below results from a column-wise
representation of the TPM.

summary(sdreport(obj_tmb, par.fixed = mod_tmb$par), "report")

##
Estimate Std. Error
## lambda 1.63641070 0.27758294
## lambda 5.53309626 0.31876141
## gamma 0.94980192 0.04374682
## gamma 0.02592209 0.02088689
## gamma 0.05019808 0.04374682
## gamma 0.97407791 0.02088689
## delta 0.34054163 0.23056401
## delta 0.65945837 0.23056401

Note that the table above also contains estimation results for δ and accompanying standard errors,
although δ is not estimated, but derived from Γ. We provide further details on this aspect in
Section 5.1.

13

The value of the nll function in the minimum found by the optimizer can also be extracted directly
from the object mod tmb by accessing the list element objective:

mod_tmb$objective

## [1] 168.5361

(viii) In the optimization above we already beneﬁted from an increased speed due to the evaluation
of the nll in C++ compared to the forward algorithm being executed entirely in R. However, the
use of TMB also permits to introduce the gradient and/or the Hessian computed by TMB into the
optimization procedure. This is in general advisable, because TMB provides an exact value of
both gradient and Hessian up to machine precision, which is superior to approximations used by
optimizing procedure. Similar to the nll, both quantities can be extracted directly from the TMB
object obj tmb:

# The negative log-likelihood is accessed by the objective
# attribute of the optimized object
mod_tmb <- nlminb(start = obj_tmb$par, objective = obj_tmb$fn,
gradient = obj_tmb$gr, hessian = obj_tmb$he)

mod_tmb$objective

## [1] 168.5361

Note that passing the gradient and Hessian provided by TMB to nlminb leads to the same mini-
mum, i.e. value of the nll function, here.

4.3 Basic nested model speciﬁcation

In the context of HMMs (and other statistical models), nested models or models subject to certain param-
eter restrictions are commonly used. For example, it may be necessary to ﬁx some parameters because
of biological or physical constraints. TMB can be instructed to treat selected parameters as constants, or
impose equality constraints on a set of parameters. For the practical implementation, it is noteworthy
that such parameter restrictions should be imposed on the working parameters. However, it is also eas-
ily possible to impose restrictions on a natural parameter (e.g. λ), and then identify the corresponding
restriction on the working parameter (i.e. log(λ)). We illustrate a simple nested model speciﬁcation
by ﬁxing λ1 to one in our two-state Poisson HMM, the other parameter components correspond to the
previous initial values.

# Get the previous values, and fix some
fixed_par_lambda <- lambda
fixed_par_lambda[1] <- 1

We then transform these natural parameters into a set of working parameters.

# Transform them into working parameters
new_parameters <- pois.HMM.pn2pw(m = m,

lambda = fixed_par_lambda,
gamma = gamma)

14

For instructing TMB to treat selected parameters as constants, the map argument of the MakeADFun
has to be speciﬁed in addition to the usual arguments. The map argument is a list consisting factor-
valued vectors which possess the same length as the working parameters and carry their names as well.
The factor levels have to be unique for the regular parameters not subject to speciﬁc restrictions. If a
parameter is ﬁxed the corresponding entry of the map argument is ﬁlled with NA. In our example, this
leads to:

map <- list(tlambda = as.factor(c(NA, 1)),

tgamma = as.factor(c(2, 3)))

fixed_par_obj_tmb <- MakeADFun(TMB_data, new_parameters,

DLL = "poi_hmm",
silent = TRUE,
map = map)

It is noteworthy that more complex constraints are possible as well. For example, to impose equality
constraints (such as γ11 = γ22), the corresponding factor level has to be identical for the concerned
entries. We refer to our GitHub page for details.

Last, estimation of the remaining model parameters and extraction of the results is achieved as

before.

fixed_par_mod_tmb <- nlminb(start = fixed_par_obj_tmb$par,

summary(sdreport(fixed_par_obj_tmb), "report")

objective = fixed_par_obj_tmb$fn,
gradient = fixed_par_obj_tmb$gr,
hessian = fixed_par_obj_tmb$he)

##
Estimate Std. Error
## lambda 1.00000000 0.00000000
## lambda 5.50164872 0.30963641
## gamma 0.94561055 0.04791050
## gamma 0.02655944 0.02133283
## gamma 0.05438945 0.04791050
## gamma 0.97344056 0.02133283
## delta 0.32810136 0.22314460
## delta 0.67189864 0.22314460

Note that the standard error of λ1 equals zero, because it is no longer considered a parameter and

does not enter the optimization procedure.

4.4 State inference and forecasting

After estimating a HMM by the procedures illustrated in Section 4.2, it is possible to carry out a couple
analyses that provide insight into the interpretation of the estimated model. These include, e.g., the
so-called smoothing probabilities, which correspond to the probability of being in state i at time t for
i = 1, . . . , m, t = 1, . . . , n, given all observations. These probabilities can be obtained by

P(Ct = i|X (n) = x(n)) =

αt(i)βt(i)
L(ˆθ)

,

15

where ˆθ denotes the set of ML estimates. The derived smoothing probabilities then serve for determining
the most probable state i∗

t at time t given the observations by

i∗
t = arg max
it∈{1,...,m}

P(Ct = it|X (n) = x(n)).

Furthermore, the Viterbi algorithm determines the overall most probable sequence of states i∗
given the observations. This is achieved by evaluating

1, . . . , i∗
T ,

(i∗

1, . . . , i∗

n) =

arg max
i1,...,in∈{1,...,m}

P(C1 = i1, . . . , Cn = in|X (n) = x(n)).

Other quantities of interest include the forecast distribution or h-step-ahead probabilities, which are
obtained through

P(Xn+h = x|X (n) = x(n)) =

αnΓhP(x)1(cid:48)
αn1(cid:48)

= ϕnΓhP(x)1(cid:48),

where ϕn = αn/αn1(cid:48).

All the quantities shown above and the related algorithms for deriving them are described in detail
in Zucchini et al. (2016, Chapter 5). In order to apply these algorithms, it is only necessary to extract
the quantities required as input from a suitable MakeADFun object. Note that most algorithms rely on
scaled versions of the forward- and backward-algorithm. This is illustrated in detail on GitHub. Figure 1
shows the TYT data together with the conditional mean values linked to the most probable state inferred
by the smoothing probabilities.

Figure 1: Plot of the TYT data. The solid horizontal lines correspond to the conditional mean of the
inferred state at each time. See Table 5 for the values of (cid:98)λi.

16

020406080TimeArousal01234567l1l25 Conﬁdence intervals

A common approach for deriving conﬁdence intervals (CIs) for the estimated parameters of statisti-
cal models bases on ﬁnite-difference approximations of the Hessian. This technique is, however, not
suited for most HMMs due to computational difﬁculties, as already pointed out by Visser et al. (2000).
The same authors suggest likelihood proﬁle CIs or bootstrap-based CIs as potentially better alterna-
tives. Despite the potentially high computational load, bootstrap-based CIs have become an established
method in the context of HMMs (Bulla and Berzel, 2008; Zucchini et al., 2016) and found widespread
application by practitioners.

In this section we illustrate how CIs based on the Hessian, likelihood proﬁling, and the bootstrap can
be efﬁciently implemented by integrating TMB. This permits in particular to obtain Hessian based and
likelihood proﬁle based CIs at very low computational cost. For simplicity, we illustrate our procedures
by means of the parameter λ2 of our two-state Poisson HMM. We will further address the resulting CIs
for Γ and λ and performance-related aspects in Section 6.

5.1 Wald-type conﬁdence intervals based on the Hessian

Since the negative log-likelihood function of HMMs typically depends on the working parameters, eval-
uation of the Hessian in the optimum found by numerical optimization only serves for inference about
the working parameters. From a practical perspective, however, inference about the natural parameters
usually is of interest. As the Hessian ∇2 log L({ ˆT, ˆη}) refers to the working parameters {T, η}, the
delta method is suitable to obtain an estimate of the covariance matrix of {ˆΓ, ˆλ} by

= −∇g( ˆT, ˆη)

(cid:16)

∇2 log L( ˆT, ˆη)

(cid:17)−1

∇g( ˆT, ˆη)(cid:48),

Σ ˆΓ,
ˆλ

(2)

with {ˆΓ, ˆλ} = g( ˆT, ˆη) as deﬁned in Section 3.4. From a user’s perspective, it is highly convenient that
the entire right-hand side of Equation 2 can be directly computed via automatic differentiation in TMB.
Moreover, it is particularly noteworthy that the standard errors of derived parameters can be calculated
by the delta-method similarly. For example, the stationary distribution δ is a function of Γ in our case,
and TMB provides a straightforward way to obtain standard errors of δ. This is achieved by ﬁrst deﬁning
δ inside the C++ ﬁle poi hmm.cpp (or, in our implementation, the related utils.cpp, which gathers
auxiliary functions). Secondly, it is necessary to call ADREPORT on δ within the poi hmm.cpp ﬁle.
To display the resulting estimates and corresponding standard errors in R, one can rely on the command
shown previously in Section 4.2.

Subsequently, Wald-type conﬁdence intervals (Wald, 1943) follow in the usual manner. For example,
the (1 − α)% CI for λ1 is given by λ1 ± z1−α/2 ∗ σλ1 where zx is the x-percentile of the standard
normal distribution, and σλ1 is the standard error of λ1 obtained via the delta method. This part is easily
implemented in R. We illustrate the calculation of these CIs for our two-state Poisson HMM on GitHub.
Finally, note that the reliability of Wald-type CIs may suffer from a singular Fisher information
matrix, which can occur for many different types of statistical models, including HMMs. This also
jeopardizes the validity of AIC and BIC criteria. For further details on this topic, see, e.g., Drton and
Plummer (2017).

5.2 Likelihood proﬁle based conﬁdence intervals

The Hessian based CIs presented above rely on asymptotic normality of the ML estimator. Properties
of the ML estimator may, however, change in small samples. Moreover, symmetric CIs may not be
suitable if the ML estimator lies close to a boundary of the parameter space. This occurs, e.g., when
states are highly persistent, which leads to entries close to one in the TPM. An alternative approach to

17

construct CIs bases on the so-called proﬁle likelihood (see, e.g., Venzon and Moolgavkar, 1988; Meeker
and Escobar, 1995), which has also shown a satisfactory performance in the context of HMMs (Visser
et al., 2000).

In the following, we illustrate the principle of likelihood proﬁle based CIs by the example of the
parameter λ2 in our two-state Poisson HMM. The underlying basic idea is to identify those values of
our parameter of interest λ2 in the neighborhood of ˆλ2 that lead to a signiﬁcant change in the log-
likelihood, whereby the other parameters (i.e. Γ, λ1) are considered nuisance parameters (Meeker and
Escobar, 1995). The term “nuisance parameters” means that these parameters need to be re-estimated
(by maximizing the likelihood) for any ﬁxed value of λ2 different to ˆλ2. That is, the proﬁle likelihood
of λ2 is deﬁned as

Lp(λ2) = max
Γ,λ1
In order to construct proﬁle likelihood-based CIs, let {ˆΓ, ˆλ} denote the ML estimate for our HMM
computed as described in Section 4.2. Evaluation of the log-likelihood function in this point results in
the value log L({ˆΓ, ˆδ}). The deviation of the likelihood of the ML estimate and the proﬁle likelihood
in the point λp

2 is then captured by the following likelihood ratio:

L(Γ, λ).

Rp(λ2) = −2

(cid:104)

(cid:105)
log(Lp(λ2)) − log(L(ˆΓ, ˆλ))

.

(3)

As described above, the log-likelihood log(Lp(λ2)) results from re-estimating the two-state HMM with
ﬁxed parameter λ2. Therefore, this model effectively corresponds to a nested model of the full model
with ML estimate ˆΓ, ˆλ. Consequently, Rp asymptotically follows a χ2 distribution with one degree of
freedom - the difference in degrees of freedom between the two models. Based on this, a CI for λ2 can
be derived by evaluating Rp at many different values of λp
2 and determining when the resulting value of
Rp becomes “too extreme”. That is, for a given α, one needs to calculate the 1 − α quantile of the χ2
1
distribution (e.g., 3.841 for α = 5%). The CI at level 1 − α for the parameter λ2 is then given by

(cid:110)

λ2 : Rp(λ2) < χ2

1,(1−α)

(cid:111)

.

(4)

For simplicity, the principles of likelihood proﬁling shown above rely on the natural parameters. Our
nll function is, however, parametrized in terms of and optimized with respect to the working parameters.
In practice, this aspect is easy to deal with. Once a proﬁle CI for the working parameter (here η2)
has been obtained following the procedure above, the corresponding CI for the natural parameter λ2
results directly from transforming the upper and lower boundary of the CI for η2 by the one-to-one
transformation λ2 = exp(η2). For further details on the invariance of likelihood-based CIs to parameter
transformations, see, e.g., Meeker and Escobar (1995).

TMB provides an easy way to proﬁling through the function tmbprofile, which requires several
inputs. First, the MakeADFun object called obj tmb from our two-state Poisson HMM. Secondly,
the position of the (working) parameter to be proﬁled via the name argument. This position refers to
the position in the parameter vector obj tmb$par. Moreover, here the optional trace argument
indicates how much information on the optimization is displayed. The following commands permit to
proﬁle the second working parameter η2 = log(λ2).

# Parameters and covariates
m <- M_LIST_TINN
if (m == 1) {

gamma <- matrix(1)

} else {

18

gamma <- matrix(0.2 / (m - 1), nrow = m, ncol = m)
diag(gamma) <- 0.8

}
lambda <- seq(from = quantile(tinn_data, 0.1),

to = quantile(tinn_data, 0.9),
length.out = m)

delta <- stat.dist(gamma)

# Parameters & covariates for TMB
working_params <- pois.HMM.pn2pw(m, lambda, gamma)
TMB_data <- list(x = tinn_data, m = m)

# Estimation
tmb_gh <- TMB.estimate(TMB_data = TMB_data,

parameters = working_params,
gradient = TRUE,
hessian = TRUE)

# Profile
profile <- tmbprofile(obj = tmb_gh$obj,

name = 2,
trace = FALSE)

par(mgp = c(2, 0.5, 0), mar = c(3, 3, 2.5, 1),

cex.lab = 1.5)

plot(profile, level = 0.95,

xlab = expression(eta[2]),
ylab = "nll")

Furthermore, Figure 2 obtained via the plot function shows the resulting proﬁle nll as function of
the working parameter η2. The vertical and horizontal lines correspond to the boundaries of the 95%
CI and the critical value of the nll derived from Equation 4, respectively. The CI for η2 can directly be
extracted via the function confint:

# Confidence interval of tlambda
confint(profile, level = 0.95)

##
upper
## tlambda 1.593141 1.820641

lower

The corresponding CI for λ2 the follows from:

# Confidence interval of lambda
exp(confint(profile, level = 0.95))

upper
##
## tlambda 4.919178 6.175815

lower

While simple linear combinations of variables can be proﬁled through the argument lincomb in
the tmbprofile function, this is not possible for more complex functions of the parameters. This
includes the stationary distribution δ, for which CIs cannot be obtained by this method.

19

Figure 2: Proﬁle likelihood plot. This ﬁgure shows the proﬁle nll as function of the working parameter
η2. The vertical and horizontal lines correspond to the boundaries of the conﬁdence interval and the
critical value of the nll, respectively.

Note that the function tmbprofile carries out several optimization procedures internally for cal-
culating proﬁle CIs. If this approach fails, or one prefers a speciﬁc optimization routine, the necessary
steps for proﬁling can also be implemented by the user. To do so, it would be - roughly speaking -
necessary to compute Rp(η2) for a sufﬁcient number of η2 values to achieve the desired precision.

Last, it is noteworthy that calculating proﬁle CIs for the elements of the TPM is not trivial. Since
all elements in each row of the TPM depend on each other, the deﬁnition of the proﬁle log-likelihood
is already problematic. We apply a rather simplistic approach and compute CIs for each of the working
parameters τi,j, i (cid:54)= j. The resulting upper and lower bounds of the proﬁle CI are then transformed to
their natural counterparts. However, other approaches may lead to improved results, see, e.g., Fischer
and Lewis (2021). Note that this approach seems to produce biased results for models with more than
two states due to the dependence between row elements of the TPM. Hence, we advise to treat proﬁle
CIs for the TPM with care, in particular for HMMs with more than two states.

5.3 Bootstrap-based conﬁdence intervals

The last approach for deriving CIs is the bootstrap, which is frequently applied by many practitioner.
Efron and Tibshirani (1993) describe the underlying concepts of the bootstrap in their seminal manuscript.
Many different bootstrap techniques have evolved since then, leading to an extensive treatment of this
subject in the scientiﬁc literature.

A thorough overview of this subject would go beyond the scope of this paper. As pointed out by
H¨ardle et al. (2003), the so-called parametric bootstrap is suitable in the context of time series models.
For further details on the bootstrap for HMMs including the implementation of a parametric bootstrap,
we refer to Zucchini et al. (2016, Ch. 3, pp. 56-60).

Basically all versions of the bootstrap have in common that some kind of re-sampling procedure

20

1.601.651.701.751.80168.5169.0169.5170.0170.5h2nllneeds to be carried out ﬁrst. Secondly, the model of interest is re-estimated for each of the re-sampled
data sets. A natural way to accelerate the second part consists in the use of TMB for the model estimation
by means of the procedures presented in Section 4.2. Our GitHub page contains a detailed example
illustrating the implementation of a parametric percentile bootstrap for our two-state Poisson HMM.

6 Application to different data sets

This section aims to demonstrate the performance of TMB by means of a couple of practical examples
that differ in terms of the number of observations and model complexity. These examples include the
TYT data shown above, a data set of fetal lamb movements, and simulated data sets. For the performance
comparisons, the focus lies on computational speed and the reliability of conﬁdence intervals. The R
scripts necessary for this section may serve interested users for investigating their own HMM setting,
and are all available on GitHub.

6.1 TYT data

We begin by investigating the speed of ﬁve approaches for parameter estimation: one without the usage
of TMB, and four with TMB. In the following, DM denotes direct maximization of the log-likelihood
through the optimization function nlminb without TMB. Furthermore, T M B0, T M BH , T M BG, and
T M BGH denote direct maximization with TMB without using the gradient and Hessian provided by
TMB, with the Hessian, with the gradient, and with both gradient and Hessian, respectively.

As a preliminary reliability check of our IT infrastructure and setup, we timed the ﬁtting of our two-
state HMM to the TYT data with the help of the microbenchmark package (Mersmann, 2021). For
this data set, all ﬁve approaches converged to the same optimum and parameter estimates, apart from
minor variations typical for numerical optimization (see Table 2).

Par.
λ1
λ2
γ11
γ12
γ21
γ22
δ1
δ2
nll

DM
1.636410931
5.533095962
0.949802041
0.050197959
0.025922044
0.974077956
0.340541816
0.659458184
168.536055869

T M B0
1.636410932
5.533095962
0.949802041
0.050197959
0.025922044
0.974077956
0.340541816
0.659458184
168.536055869

T M BG
1.636410933
5.533095957
0.949802042
0.050197958
0.025922044
0.974077956
0.340541819
0.659458181
168.536055869

T M BH
1.636410932
5.533095962
0.949802041
0.050197959
0.025922044
0.974077956
0.340541816
0.659458184
168.536055869

T M BGH
1.636410997
5.533095759
0.949802094
0.050197906
0.025922038
0.974077962
0.340541999
0.659458001
168.536055869

Table 2: Parameter estimates and corresponding nll of the two-state Poisson HMM with and without
using TMB obtained for the TYT data.

Table 3 shows the resulting average time required for the parameter estimation and the number of
iterations needed by each approach, measured over 200 replications. The results show that the use
of TMB signiﬁcantly accelerates parameter estimation in comparison with DM . The most substantial
acceleration is achieved by T M BG, underlining the beneﬁt of using the gradient provided by TMB.
Moreover, T M BGH requires fewer iterations than the other approaches. However, the evaluation of the
Hessian seems to increase the computational burden.

Next, we veriﬁed the reproducibility of the acceleration by TMB in a parametric bootstrap setting.
More speciﬁcally, we simulated 200 bootstrap samples from the model estimated on the TYT data. Then,
we re-estimated the same model by our ﬁve approaches and derived acceleration ratios (with DM as

21

Time (ms)

Iterations

DM
36.5
(36.2, 36.9)
13

T M B0
1.66
(1.6, 1.72)
13

T M BG
0.824
(0.822, 0.826)
13

T M BH
1.64
(1.6, 1.68)
13

T M BGH
2.62
(2.59, 2.65)
7

Table 3: Average duration (in milliseconds) together with 95% CI and number of iterations required for
ﬁtting a two-state Poisson HMM to the TYT data. The CIs are of Wald-type and base on the standard
error of the mean derived from 200 replications.

reference approach) and their corresponding percentile CIs. As shown in Table 4, all acceleration ratios
take values signiﬁcantly larger than one, whether the gradient and Hessian are passed from TMB or
not. In addition, the ﬁndings from the single TYT data set are conﬁrmed, with T M BG providing the
most substantial acceleration and T M BGH reducing the number of iterations. This underlines that two
factors are sources of the acceleration in Table 4: the use of C++ code on the one hand, and computation
of the gradient and/or Hessian by TMB on the other hand.

Acceleration ratio

Iterations

T M B0
21.8
(20.7, 23)
12.6
(9, 18)

T M BG
41.8
(37.4, 46.8)
12.6
(9, 19)

T M BH
21.8
(20.8, 23.3)
12.6
(9, 18)

T M BGH
16.7
(12.9, 22.1)
5.07
(4, 8.03)

Table 4: Acceleration and iterations for the TYT data. The top lines show the acceleration ratios together
with 95% percentile bootstrap CIs when using TMB in a bootstrap setting with 200 bootstrap samples.
The bottom lines display the corresponding values for the number of iterations. The average parameter
estimation duration without TMB is 37 milliseconds.

In order to obtain reliable results, we excluded all those bootstrap samples with a simulated state
sequence not sojourning in each state of the underlying model at least once. This is necessary because
such a constellation almost certainly leads to convergence problems on the one hand. On the other
hand, even if the estimation algorithms converge, the estimated models are usually degenerate because
of the lack of identiﬁability. Furthermore, for some very rare bootstrap samples, one or several of the
estimation algorithms did not converge properly. In such cases, we discarded the results, generated
an additional bootstrap sample, and re-ran the parameter estimation. Convergence problems mainly
occurred due to T M B0 and T M BH failing. Therefore, we recommend passing at least the gradient
when optimizing with TMB for increased stability.

Last, we investigate CIs obtained for the TYT data by the three different methods described in
Section 5, T M BGH served as the sole estimation approach. The columns to the left in Table 5 show the
parameter estimates and the three types of 95% CIs obtained using the Hessian, likelihood proﬁling, and
bootstrapping. For this data set, no major differences between the different CIs are visible. Furthermore,
we assessed the accuracy of the different CIs by computing coverage probabilities, which are shown in
the last three columns of Table 5. For calculating these coverage probabilities, we used a Monte Carlo
setting similar to the one described above. Samples that possessed state sequences not visiting all
states or samples for which the estimation algorithm did not converge were replaced. Moreover, we
also simulated a replacement sequence when the proﬁle likelihood method failed to converge on any
bound to ensure comparability of the results. The results, shown on the right of Table 5 indicate that all
methods provide comparably reliable CI estimates, and neither outperforms the other for all parameters.
For the Wald-type CIs, the coverage probabilities almost reach 100% for γ22, γ21, but lie comparably
low for both the other elements of the TPM and δ1, δ2. However, proﬁle likelihood-based CIs also take

22

values above 95% for all elements of the TPM, and the coverage probabilities for bootstrap CIs are all
above 95%. Altogether, it appears that the reliability of CIs should be investigated carefully when ﬁtting
HMMs to very short sequences of observations.

Par.
λ1
λ2
γ11
γ12
γ21
γ22
δ1
δ2

Est.
1.64
5.53
0.95
0.05
0.03
0.97
0.34
0.66

Wald-type CI
L.
1.09
4.91
0.86
0.00
0.00
0.93
0.00
0.21

U.
2.18
6.16
1.00
0.14
0.07
1.00
0.79
1.00

Proﬁle CI
U.
L.
2.23
1.15
6.18
4.92
1.00
0.82
0.18
0.00
0.09
0.00
1.00
0.91

Bootstrap CI
L.
0.95
4.88
0.54
0.01
0.01
0.86
0.07
0.21

U.
2.93
6.31
0.99
0.46
0.14
0.99
0.79
0.93

Coverage prob. (%)

Wald
92.7
93.5
89.7
89.7
100.0
100.0
86.2
86.2

Proﬁle Bootst.
94.9
93.5
97.3
97.3
95.6
95.6

98.8
98.4
96.5
96.5
95.5
95.5
95.8
95.8

Table 5: CIs for the TYT dataset. From left to right, the columns contain: the parameter name, parameter
estimate, and lower (L.) and upper (U.) bound of the corresponding 95% CI derived via the Hessian
provided by TMB, likelihood proﬁling, and percentile bootstrap. Then follow coverage probabilities
derived for these three methods in a Monte-Carlo study.

6.2 Lamb data

The well-known data set presented in Leroux and Puterman (1992) serves as second example. This
data set consists of the number of movements by a fetal lamb observed through ultrasound during 240
consecutive 5-second intervals, as shown in Figure 3. Since the results reported by Leroux and Puterman
(1992) show that a two-state model is preferred by the BIC, we focus on this model only here - although
other choices would be possible, e.g. the AIC selects a three-state model.

We selected this data set for several reasons. First, the number of observations is larger than for
the TYT data (but still comparably low). Secondly, according to the results of Leroux and Puterman
(1992), the ﬁrst state largely dominates the data generating process, whereas the second state is not very
persistent and linked to only a few observations. Thirdly, the conditional means of the two states are not
very different. The latter two aspects qualify this data as a “non-textbook example”.

Similar to the TYT data, all estimation algorithms converged to the same minimum of the nll, and
provided almost identical parameter estimates on the original data set. A bootstrap experiment similar
to the one described above for the TYT data led to comparable results, as shown in Table 6. The
highest acceleration is achieved by T M BG, whereas T M BGH achieves the lowest acceleration despite
requiring a lower number of iterations than the other approaches. However, all ratios lie above those
obtained for the TYT data, indicating an increased beneﬁt of using TMB with an increasing number of
observations.

Next, Table 7 shows parameter estimates and corresponding CIs in the columns to the left. Our
estimates conﬁrm the results of Leroux and Puterman (1992): the second state is not very persistent,
and the conditional means λ1 and λ2 do not lie very far from each other. Concerning the CIs resulting
from our three approaches, it is noticeable that the bootstrap CIs for the elements of the TPM are larger
than those obtained by the other two approaches. The coverage probabilities presented in the columns
to the right of Table 7 show similar patterns as observed for the TYT data. Wald-type CIs show certain
deviations from 95% for the parameters related to the hidden state sequence, and the same is true for the
proﬁle CIs. Bootstrap CIs seem to be slightly too large for most parameters, leading to values greater
than 95%. As for the TYT data, we recommend to be aware of the limited reliability of CIs for short
sequences of observations.

23

Figure 3: Plot of the lamb data. The solid horizontal lines correspond to the conditional mean of the
inferred state at each time. See Table 7 for the values of (cid:98)λi.

Acceleration ratio

Iterations

T M B0
26.8
(25, 29.1)
13.3
(8.98, 33)

T M BG
49.4
(44.2, 64.8)
13.3
(8.98, 32)

T M BH
26.7
(25.3, 29)
13.3
(8.98, 33)

T M BGH
14.8
(9.17, 20.5)
6.28
(4, 22)

Table 6: Acceleration and iterations for the lamb data. The top lines show the acceleration ratios together
with 95% percentile bootstrap CIs when using TMB in a bootstrap setting with 200 bootstrap samples.
The bottom lines display the corresponding values for the number of iterations. The average parameter
estimation duration without TMB is 76 milliseconds.

On a minor note, some non-negligible differences can be noted when comparing our estimation
results to those reported by Leroux and Puterman (1992). The reasons for this are difﬁcult to determine,
but some likely explanations are given in the following. First, differences in the parameter estimates
may result e.g. from the optimizing algorithms used and related setting (e.g. convergence criterion,
number of steps, optimization routines used in 1992,. . . ). Moreover, Leroux and Puterman (1992) seem
to base their calculations on an altered likelihood, which is reduced by removing the constant term
(cid:80)T
i=1 log(xi!) from the log-likelihood. This modiﬁcation may also possess an impact on the behavior

of the optimization algorithm, as e.g. relative convergence criteria and step size could be affected.

6.3 Simulation study

The two previously analyzed data sets are both of comparably small size. In order to systematically
investigate the performance of TMB in the context of larger samples, we carried out a small simulation
study. For this study, we simulated sequences of observations of length 2000 and 5000 from HMMs

24

050100150200Interval ## of movements01234567l1l2−−−−−−−Coverage prob. (%)

Par.
λ1
λ2
γ11
γ12
γ21
γ22
δ1
δ2

Est.
0.26
3.11
0.99
0.01
0.31
0.69
0.96
0.04

Wald-type CI
L.
0.18
1.11
0.97
0.00
0.00
0.33
0.90
0.00

U.
0.34
5.12
1.00
0.03
0.67
1.00
1.00
0.10

Proﬁle CI
U.
L.
0.33
0.15
4.95
1.27
1.00
0.93
0.07
0.00
0.68
0.04
0.96
0.32

Bootstrap CI
L.
0.08
0.50
0.71
0.00
0.10
0.00
0.61
0.01

U. Wald
95.0
0.33
93.3
5.32
99.9
1.00
99.9
0.29
93.3
1.00
93.3
0.90
98.2
0.99
98.2
0.39

Proﬁle Bootst.
96.2
94.6
96.1
96.1
97.6
97.6

95.7
97.7
97.9
97.9
98.8
98.8
98.9
98.9

Table 7: CIs for the lamb dataset. From left to right, the columns contain: the parameter name, parameter
estimate, and lower (L.) and upper (U.) bound of the corresponding 95% CI derived via the Hessian
provided by TMB, likelihood proﬁling, and percentile bootstrap. Then follow coverage probabilities
derived for these three methods in a Monte-Carlo study.

with two and three states, respectively. The parameters underlying the simulation are
(cid:18)0.95
0.15

, λ = (1, 7),

0.05
0.85

Γ =

(cid:19)

for the two-state HMM and

Γ =





0.95
0.05
0.075

0.025
0.90
0.075

0.025
0.05
0.85



 , λ = (1, 4, 7),

for the HMM with three states. Figure 4 and Figure 5 display the ﬁrst 500 observations of two exemplary
sequences of observations generated for the two- and three-state model, respectively. We also simulated
sequences of observations of length 2000 and 5000 from HMMs with four states. The parameters
underlying both simulations are

Γ =







0.05
0.85
0.85
0.05
0.05
0.10
0.034 0.033

0.05
0.05
0.80
0.033







0.05
0.05
0.05
0.90

, λ = (1, 5, 9, 13).

The bootstrap setting described in the previous sections served for computing acceleration ratios
for the simulated data sets. Table 8 displays the results obtained for the two-state model. Again,
T M BG provides the strongest acceleration compared to DM . Furthermore, T M BGH achieves the
second place, closely followed by the remaining two approaches. This conﬁrms the good performance
of T M BG for large samples as well. Furthermore, it suggests a certain beneﬁt from employing the
Hessian computed by TMB for longer observation sequences, since this method requires a much lower
number of iterations than all other approaches.

The results for the three-state model, as shown in Table 9, basically underline all ﬁndings from the
two-state model. Most importantly, again T M BGH requires a much lower number of iterations than
the other approaches.

Similar to the two previous sections, Table 12 - Table 15 show parameter estimates, CIs, and cov-
erage probabilities. The four exemplary sequences of observations shown in Figure 4 - Figure 7 served
for deriving parameter estimates and CIs. The coverage probabilities result from a Monte-Carlo study
as the ones previously described. For the two-state model, the CIs obtained by our three methods are

25

Figure 4: Plot of the ﬁrst simulated data (of size 2000) generated by a two-state Poisson HMM. The solid
horizontal lines correspond to the conditional mean of the inferred state at each time. For readability,
the graph is truncated to 500 data points. See Table 12 for the values of (cid:98)λi.

very similar for all parameters. The coverage probabilities lie comparably close to the theoretical level
of 95% for all parameters of the two-state model. Moreover, no systematically too small or large CIs
seem to result from any of the three methods. The same holds true for the three-state model, with minor
exceptions for the proﬁle CIs. For this method, the coverage probabilities of the CIs are close to 100%
for diagonal elements of the TPM, while the corresponding probabilities for off-diagonal elements are
less than 95%. This pattern ampliﬁes for the four-state models, as shown in Table 15. Moreover, proﬁle
likelihood CIs of the elements of the TPM are not consistently provided by the tmbprofile function
for the sequences of length 2000 from the four-state model. Therefore, the corresponding entries in
Table 14 remain empty.

Last, we would like to point out that the runtime of our entire simulation study was approximately
one week. Without the acceleration by TMB, several months would have been necessary. This illustrates
not only the saving of time but also of resources (e.g. energy consumption, wear of IT-infrastructure).

26

0100200300400500TimeObs051015l1l2Figure 5: Plot of the second simulated data (of size 5000) generated by a three-state Poisson HMM.
The solid horizontal lines correspond to the conditional mean of the inferred state at each time. For
readability, the graph is truncated to 500 data points. See Table 13 for the values of (cid:98)λi.

Figure 6: Plot of the third simulated data (of size 2000) generated by a four-state Poisson HMM. The
solid horizontal lines correspond to the conditional mean of the inferred state at each time. For readabil-
ity, the graph is truncated to 500 data points. See Table 14 for the values of (cid:98)λi.

27

0100200300400500TimeObs024681012l1l2l30100200300400500TimeObs05101520l1l2l3l4Figure 7: Plot of the fourth simulated data (of size 5000) generated by a four-state Poisson HMM.
The solid horizontal lines correspond to the conditional mean of the inferred state at each time. For
readability, the graph is truncated to 500 data points. See Table 15 for the values of (cid:98)λi.

Acceleration ratio

Iterations

T M B0
22.5
(20.6, 26.3)
22.3
(17, 29)

T M BG
41.3
(33, 50.6)
22.6
(18, 28)

T M BH
22.4
(20.6, 26.6)
22.3
(17, 29)

T M BGH
23.7
(17.7, 31.2)
4.08
(3, 6)

Table 8: Acceleration and iterations for the first simulated data (of size 2000). The top lines show the
acceleration ratios together with 95% percentile bootstrap CIs when using TMB in a bootstrap setting
with 200 bootstrap samples. The bottom lines display the corresponding values for the number of
iterations. The average parameter estimation duration without TMB is 832 milliseconds.

Acceleration ratio

Iterations

T M B0
14.2
(12.7, 15.7)
42.4
(34, 50)

T M BG
54.1
(44.4, 65.4)
42.6
(34, 51.1)

T M BH
14.2
(12.6, 15.9)
42.4
(34, 50)

T M BGH
31.4
(24.4, 39.5)
4.28
(3, 6)

Table 9: Acceleration and iterations for the second simulated data (of size 5000). The top lines show the
acceleration ratios together with 95% percentile bootstrap CIs when using TMB in a bootstrap setting
with 200 bootstrap samples. The bottom lines display the corresponding values for the number of
iterations. The average parameter estimation duration without TMB is 8187 milliseconds.

28

0100200300400500TimeObs0510152025l1l2l3l4Acceleration ratio

Iterations

T M B0
9.95
(9.38, 10.8)
49.5
(38, 66)

T M BG
61.2
(52.8, 69.5)
49.6
(39, 67)

T M BH
9.95
(9.37, 10.7)
49.5
(38, 66)

T M BGH
13.2
(8.12, 18.6)
13
(6, 24)

Table 10: Acceleration and iterations for the third simulated data (of size 2000). The top lines show the
acceleration ratios together with 95% percentile bootstrap CIs when using TMB in a bootstrap setting
with 200 bootstrap samples. The bottom lines display the corresponding values for the number of
iterations. The average parameter estimation duration without TMB is 6777 milliseconds.

Acceleration ratio

Iterations

T M B0
12.2
(10.8, 13.7)
51.7
(44, 58)

T M BG
74.3
(62.9, 87.1)
51.8
(44, 59)

T M BH
12.2
(10.6, 13.6)
51.7
(44, 58)

T M BGH
29.1
(21.5, 35.9)
5.62
(4, 7.05)

Table 11: Acceleration and iterations for the fourth simulated data (of size 5000). The top lines show the
acceleration ratios together with 95% percentile bootstrap CIs when using TMB in a bootstrap setting
with 200 bootstrap samples. The bottom lines display the corresponding values for the number of
iterations. The average parameter estimation duration without TMB is 21358 milliseconds.

Coverage prob. (%)

Par. Value
λ1
1.00
λ2
7.00
γ11
0.95
γ12
0.05
γ21
0.15
γ22
0.85
δ1
0.75
δ2
0.25

Est.
0.99
6.89
0.94
0.06
0.16
0.84
0.73
0.27

Wald-type CI
L.
0.93
6.66
0.93
0.05
0.13
0.80
0.67
0.22

U.
1.04
7.13
0.95
0.07
0.20
0.87
0.78
0.33

Proﬁle CI
U.
L.
1.04
0.93
7.13
6.66
0.95
0.92
0.08
0.05
0.20
0.13
0.87
0.80

Bootstrap CI
L.
0.93
6.64
0.92
0.05
0.13
0.80
0.67
0.22

U. Wald
95.1
1.04
95.4
7.14
95.1
0.95
95.1
0.08
94.1
0.20
94.1
0.87
94.0
0.78
94.0
0.33

Proﬁle Bootst.
95.4
95.6
95.1
95.1
95.1
95.1

94.9
95.7
95.1
95.1
94.5
94.5
93.7
93.7

Table 12: CIs for the first simulated dataset (of size 2000). From left to right, the columns contain: the
parameter name, true parameter value, parameter estimate, and lower (L.) and upper (U.) bound of the
corresponding 95% CI derived via the Hessian provided by TMB, likelihood proﬁling, and percentile
bootstrap. Then follow coverage probabilities derived for these three methods in a Monte-Carlo study.

29

Coverage prob. (%)

Par. Value
λ1
1.000
λ2
4.000
λ3
7.000
γ11
0.950
γ12
0.025
γ13
0.025
γ21
0.050
γ22
0.900
γ23
0.050
γ31
0.075
γ32
0.075
γ33
0.850
δ1
0.545
δ2
0.273
δ3
0.182

Est.
1.016
4.014
7.329
0.950
0.026
0.024
0.050
0.906
0.045
0.077
0.089
0.834
0.541
0.300
0.159

Wald-type CI
L.
0.97
3.86
7.06
0.94
0.02
0.02
0.03
0.89
0.03
0.05
0.06
0.80
0.48
0.25
0.12

U.
1.06
4.17
7.60
0.96
0.03
0.03
0.06
0.93
0.06
0.10
0.12
0.87
0.60
0.35
0.19

Proﬁle CI
U.
L.
1.06
0.97
4.17
3.86
7.60
7.07
0.97
0.93
0.04
0.02
0.03
0.02
0.06
0.04
0.93
0.87
0.06
0.03
0.10
0.06
0.12
0.06
0.88
0.78

Bootstrap CI
L.
0.98
3.86
7.07
0.94
0.02
0.02
0.04
0.88
0.03
0.05
0.06
0.79
0.49
0.25
0.13

U. Wald
95.4
1.06
93.6
4.16
94.3
7.59
95.3
0.96
93.8
0.03
94.5
0.03
95.2
0.07
95.2
0.93
94.1
0.06
95.2
0.10
94.9
0.13
95.0
0.87
94.2
0.60
94.7
0.35
93.9
0.20

Proﬁle Bootst.
95.3
93.7
94.6
100.0
93.4
94.2
93.1
99.5
93.3
92.1
93.5
99.6

95.6
94.2
94.8
95.1
93.7
95.0
95.2
95.0
94.8
95.9
95.2
95.1
94.2
94.7
94.4

Table 13: CIs for the second simulated dataset (of size 5000). From left to right, the columns contain:
the parameter name, true parameter value, parameter estimate, and lower (L.) and upper (U.) bound of
the corresponding 95% CI derived via the Hessian provided by TMB, likelihood proﬁling, and percentile
bootstrap. Then follow coverage probabilities derived for these three methods in a Monte-Carlo study.

30

Coverage prob. (%)

Proﬁle CI
U.
L.

Par. Value
λ1
1.000
λ2
5.000
λ3
9.000
λ4
13.000
γ11
0.850
γ12
0.050
γ13
0.050
γ14
0.050
γ21
0.050
γ22
0.850
γ23
0.050
γ24
0.050
γ31
0.050
γ32
0.100
γ33
0.800
γ34
0.050
γ41
0.034
γ42
0.033
γ43
0.033
γ44
0.900
δ1
0.223
δ2
0.266
δ3
0.177
δ4
0.333

Est.
1.024
5.298
10.359
13.451
0.815
0.061
0.026
0.097
0.062
0.847
0.061
0.031
0.033
0.107
0.827
0.033
0.022
0.023
0.058
0.896
0.173
0.278
0.231
0.318

Wald-type CI
U.
1.15
5.59
11.11
13.92
0.86
0.10
0.07
0.14
0.09
0.89
0.10
0.06
0.06
0.16
0.90
0.07
0.04
0.05
0.10
0.93
0.22
0.34
0.31
0.42

L.
0.90
5.01
9.61
12.98
0.77
0.03
0.00
0.05
0.04
0.81
0.02
0.00
0.00
0.05
0.76
0.00
0.00
0.00
0.01
0.86
0.13
0.22
0.15
0.22

Bootstrap CI
L.
0.90
4.99
9.52
13.01
0.76
0.03
0.00
0.05
0.04
0.80
0.02
0.00
0.00
0.06
0.74
0.00
0.00
0.00
0.02
0.85
0.13
0.21
0.15
0.21

U. Wald
95.7
1.16
93.7
5.61
92.4
11.13
95.1
14.00
94.8
0.86
93.8
0.10
93.7
0.07
93.2
0.15
92.7
0.09
95.4
0.88
90.0
0.11
92.7
0.07
91.9
0.07
93.9
0.17
94.3
0.88
89.5
0.09
93.8
0.04
91.7
0.05
89.4
0.12
95.1
0.93
93.1
0.22
94.1
0.34
93.8
0.32
93.6
0.42

Proﬁle Bootst.
95.7
93.9
93.7
95.3

95.9
94.4
95.9
96.1
94.0
94.7
94.6
94.9
94.0
95.4
94.3
94.1
93.0
94.7
96.0
95.4
94.1
93.4
93.5
94.4
93.2
94.8
96.9
93.8

Table 14: CIs for the third simulated dataset (of size 2000). From left to right, the columns contain:
the number of hidden states, parameter name, true parameter value, parameter estimate, and lower (L.)
and upper (U.) bound of the corresponding 95% CI derived via the Hessian provided by TMB, likelihood
proﬁling, and percentile bootstrap. Then follow coverage probabilities derived for these three methods
in a Monte-Carlo study.

31

Coverage prob. (%)

Par. Value
λ1
1.000
λ2
5.000
λ3
9.000
λ4
13.000
γ11
0.850
γ12
0.050
γ13
0.050
γ14
0.050
γ21
0.050
γ22
0.850
γ23
0.050
γ24
0.050
γ31
0.050
γ32
0.100
γ33
0.800
γ34
0.050
γ41
0.034
γ42
0.033
γ43
0.033
γ44
0.900
δ1
0.223
δ2
0.266
δ3
0.177
δ4
0.333

Est.
0.955
5.192
9.253
12.987
0.840
0.064
0.044
0.052
0.058
0.836
0.049
0.057
0.057
0.098
0.786
0.059
0.029
0.050
0.028
0.892
0.220
0.283
0.154
0.342

Wald-type CI
U.
1.02
5.39
9.86
13.24
0.86
0.08
0.07
0.07
0.08
0.86
0.08
0.08
0.09
0.15
0.84
0.10
0.04
0.07
0.05
0.91
0.25
0.32
0.19
0.40

L.
0.89
5.00
8.65
12.74
0.82
0.04
0.02
0.03
0.04
0.81
0.02
0.04
0.02
0.05
0.73
0.02
0.02
0.03
0.01
0.87
0.19
0.24
0.11
0.29

Proﬁle CI
U.
L.
1.02
0.89
5.38
4.99
9.86
8.66
13.25
12.75
0.89
0.78
0.08
0.05
0.06
0.02
0.07
0.04
0.07
0.04
0.89
0.78
0.08
0.03
0.08
0.04
0.08
0.03
0.14
0.06
0.89
0.68
0.10
0.02
0.04
0.02
0.07
0.03
0.05
0.01
0.94
0.84

Bootstrap CI
L.
0.89
4.99
8.63
12.74
0.82
0.04
0.02
0.03
0.04
0.80
0.02
0.04
0.03
0.05
0.71
0.01
0.02
0.03
0.01
0.87
0.19
0.24
0.11
0.29

U. Wald
96.0
1.03
95.2
5.40
93.4
9.85
95.5
13.27
95.1
0.86
94.4
0.09
93.4
0.07
94.5
0.07
95.0
0.08
94.8
0.86
94.7
0.08
95.0
0.08
96.1
0.09
94.3
0.15
94.4
0.84
97.0
0.12
94.5
0.04
95.8
0.07
95.0
0.06
95.7
0.91
94.7
0.25
94.4
0.32
96.6
0.20
94.1
0.39

Proﬁle Bootst.
95.9
95.1
93.6
95.3
99.9
92.5
90.4
89.6
91.2
100.0
92.2
91.8
90.0
90.3
100.0
94.2
91.2
93.2
94.7
100.0

95.7
95.2
94.6
95.1
95.0
95.2
93.8
94.4
95.3
95.2
95.1
95.8
95.9
94.5
95.2
96.7
94.3
96.1
96.2
95.6
94.8
94.8
96.7
94.1

Table 15: CIs for the fourth simulated dataset (of size 5000). From left to right, the columns contain:
the parameter name, true parameter value, parameter estimate, and lower (L.) and upper (U.) bound of
the corresponding 95% CI derived via the Hessian provided by TMB, likelihood proﬁling, and percentile
bootstrap. Then follow coverage probabilities derived for these three methods in a Monte-Carlo study.

32

7 Discussion

In this tutorial, we provide researchers from all applied ﬁelds with an introduction to parameter estima-
tion for HMMs via TMB using R. Some procedures need to be coded in C++, which represents a certain
requirement on the user and may not be beneﬁcial if, e.g., an HMM needs to be estimated only once.
However, for users working more or less regularly with HMMs, the use of TMB accelerates existing
parameter estimation procedures without having to carry out major changes to R code that is already in
use. Moreover, after ﬁnishing the estimation procedure, TMB obtains standard errors for the estimated
parameters at a very low computational cost.

We examined the performance of TMB in the context of Poisson HMMs through two small real data
sets and in a simulation setting with longer sequences of observations. Overall, it is notable that the
parameter estimation process is strongly kcelerated on the one hand. This applies even to small data
sets, and the highest acceleration is obtained when only the gradient is supplied by TMB (instead of both
gradient and Hessian). On the other hand, the standard errors obtained through TMB are very similar
to the standard errors obtained by proﬁling the likelihood and bootstrapping while being (much) less
computationally intensive. This is novel since Hessian-based CIs did not seem to be reliable in the past,
as illustrated e.g. by Visser et al. (2000).

Along with the tutorial character of this paper comes the shortcoming that we restricted ourselves
to only one comparably simple HMM with Poisson conditional distributions. The extension to other
distributions is, however, not overly complicated. We brieﬂy illustrate the case of Gaussian conditional
distributions on the supporting GitHub page. Moreover, to keep the paper at acceptable length, we were
not able to address a couple of potential extensions and research questions. For example, it would be
interesting to investigate whether supplying derivatives provided by TMB to different optimizers has
a positive impact on convergence properties. This may be of particular interest when considering the
impact of different potentially poor initial values on the optimization results. Following Bulla and Berzel
(2008), one could also consider a hybrid algorithm in this context to beneﬁt from the advantageous
properties of the EM algorithm, such as the resilience to poor initial values. When setting up this
approach, the EM algorithm would most likely also beneﬁt from acceleration by C++. The reliability
of Wald-type CIs provided by TMB for other models and very long sequences with e.g. hundreds of
thousands or millions of observations could also be of interest. In addition, improving the reliability
of CIs for small samples (such as our TYT and lamb data) may be worth investigating. In a similar
direction, it seems rather obvious to beneﬁt from TMB for more complex settings such as panel data
with random effects, where computationally efﬁcient routines play an important role. Furthermore,
in the bootstrap analysis of the TYT and in particular of the lamb data set, we noted that the TMB-
internal function tmbprofile sometimes fails to provide CIs for parameters very close to a boundary.
For these two data sets, this was the case for the elements of the TPM which take values close to
one: approximately 7% (TYT) and 29% (lamb), respectively, of the generated bootstrap samples were
affected. The same problem showed for the four-state model with 2000 observations (here more than
50% of the generated data sets). It remains to clarify whether this problem can be solved by a suitable
modiﬁcation of tmbprofile, or if the underlying difﬁculties require an entirely different approach.
Last, proﬁle likelihood CIs for the elements of the TPM seems to be subject to bias for models with
more than two states. This may be due to the strong dependence of all elements of the TPM in the same
row, which is problematic for a proper deﬁnition of the proﬁle likelihood function Lp(γij) difﬁcult (see,
e.g., Fischer and Lewis, 2021). Therefore, we recommend treating proﬁle CIs for the elements of the
TPM with care, in particular for models with more than two states, and further research in this direction
is needed.

From an application perspective, the use of TMB allows executing estimation procedures at a signif-
icantly reduced cost compared to the execution of plain R. Such a performance gain could be of interest
when repeatedly executing statistical procedures on mobile devices. It seems plausible to enrich the

33

TYT app (or similar apps collecting a sufﬁcient amount of data) by integrating a warning system that
detects when the user enters a new state, inferred through a suitable HMM or another procedure accel-
erated via TMB in real-time. This new state could, e.g., represent an improvement or worsening of a
pre-deﬁned medical condition and recommend the user to contact the consulting physician if the change
persists for a certain period. Provided the agreement of the user, this collected information could also be
pre-processed and transferred automatically to the treating physician and allow to identify personalized
treatment options.

Acknowledgement

The work of J. Bulla was supported by the GENDER-Net Co-Plus Fund (GNP-182). Furthermore, this
work was supported by the Financial Market Fund (Norwegian Research Council project no. 309218).
We thank the University of Regensburg and European School for Interdisciplinary Tinnitus Research
(ESIT) for providing access to the TYT data. Special thanks go to J. Sim˜oes for data preparation, and
W. Schlee and B. Langguth for helpful comments and suggestions. J. Bulla’s profound thanks for moral
support go to the highly skilled, very helpful, and always friendly employees Vestbrygg AS (Org. ID
912105954).

Conﬂict of Interest
The authors have declared no conﬂict of interest.

34

References

Ailliot, P., Allard, D., Monbet, V., and Naveau, P. (2015). Stochastic weather generators: an overview

of weather type models. Journal de la soci´et´e franc¸aise de statistique, 156(1):101–113.

Bartolucci, F., Farcomeni, A., and Pennoni, F. (2012). Latent Markov Models for Longitudinal Data.

Chapman & Hall/CRC Statistics in the Social and Behavioral Sciences. Taylor & Francis.

Baum, L. E. and Petrie, T. (1966). Statistical Inference for Probabilistic Functions of Finite State Markov

Chains. The Annals of Mathematical Statistics, 37(6):1554–1563.

Baum, L. E., Petrie, T., Soules, G., and Weiss, N. (1970). A maximization technique occurring in the

statistical analysis of probabilistic functions of Markov chains. 41:164–171.

Berentsen, G. D., Bulla, J., Maruotti, A., and Støve, B. (2022). Modeling clusters of corporate de-
faults: Regime-switching models signiﬁcantly reduce the contagion source. Journal of the Royal
Statistical Society: Series C, (accepted).

Bulla, J. and Berzel, A. (2008). Computational issues in parameter estimation for stationary hidden

Markov models. Computational Statistics, 23(1):1–18.

Capp´e, O., Moulines, E., and Ryden, T. (2006). Inference in Hidden Markov Models. Springer Science

& Business Media.

Celeux, G. and Durand, J.-B. (2008). Selecting hidden Markov model state number with cross-validated

likelihood. Computational Statistics, 23(4):541–564.

Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum Likelihood from Incomplete Data Via
the EM Algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1–
22.

Drton, M. and Plummer, M. (2017). A Bayesian information criterion for singular models. Journal of

the Royal Statistical Society: Series B (Statistical Methodology), 79(2):323–380.

Durbin, R. (1998). Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.

Cambridge University Press, Cambridge, illustrated edition edition.

Eddy, S. R. (1998). Proﬁle hidden Markov models. Bioinformatics, 14(9):755–763.
Efron, B. and Tibshirani, R. J. (1993). An Introduction to the Bootstrap. Chapman & Hall, New York,

N.Y.; London.

Feller, W. (1968). An Introduction to Probability Theory and Its Applications. Wiley.
Fischer, S. M. and Lewis, M. A. (2021). A robust and efﬁcient algorithm to ﬁnd proﬁle likelihood

conﬁdence intervals. Statistics and Computing, 31(4):38.

Fredkin, D. R. and Rice, J. A. (1992). Bayesian Restoration of Single-Channel Patch Clamp Recordings.

Biometrics, 48(2):427–448.

Fr¨uhwirth-Schnatter, S. (2006). Finite Mixture and Markov Switching Models. Springer Series in Statis-

tics. Springer-Verlag, New York.

Gales, M. and Young, S. (2008). The Application of Hidden Markov Models in Speech Recognition.

Foundations and Trends® in Signal Processing, 1(3):195–304.

Gay, D. M. (1990). Usage summary for selected optimization routines. Computing science technical

report, 153:1–21.

Grimmett, G. R. and Stirzaker, D. R. (2001). Probability and Random Processes. Oxford University

Press, New York, third edition.

Hamilton, J. D. (1989). A New Approach to the Economic Analysis of Nonstationary Time Series and

the Business Cycle. Econometrica, 57(2):357–384.

H¨ardle, W., Horowitz, J., and Kreiss, J.-P. (2003). Bootstrap Methods for Time Series. International

Statistical Review, 71(2):435–459.

35

Kass, R. E. and Steffey, D. (1989). Approximate Bayesian Inference in Conditionally Independent
Hierarchical Models (Parametric Empirical Bayes Models). Journal of the American Statistical
Association, 84(407):717–726.

Kristensen, K., Nielsen, A., Berg, C., Skaug, H., and Bell, B. (2016). TMB: Automatic differentiation

and laplace approximation. Journal of Statistical Software, Articles, 70(5):1–21.

Leroux, B. G. and Puterman, M. L. (1992). Maximum-Penalized-Likelihood Estimation for Independent

and Markov- Dependent Mixture Models. Biometrics, 48(2):545–558.

Lystig, T. C. and Hughes, J. P. (2002). Exact Computation of the Observed Information Matrix for
Hidden Markov Models. Journal of Computational and Graphical Statistics, 11(3):678–689.
McClintock, B. T., Langrock, R., Gimenez, O., Cam, E., Borchers, D. L., Glennie, R., and Patterson,
T. A. (2020). Uncovering ecological state dynamics with hidden Markov models. Ecology Letters,
23(12):1878–1903.

McLachlan, G. J. and Peel, D. (2004). Finite Mixture Models. John Wiley & Sons.
Meeker, W. Q. and Escobar, L. A. (1995). Teaching about Approximate Conﬁdence Regions Based on

Maximum Likelihood Estimation. The American Statistician, 49(1):48–53.

Mersmann, O. (2021). microbenchmark: Accurate Timing Functions. R package version 1.4.9.
Mor, B., Garhwal, S., and Kumar, A. (2021). A Systematic Review of Hidden Markov Models and
Their Applications. Archives of Computational Methods in Engineering, 28(3):1429–1448.
Pohle, J., Langrock, R., van Beest, F., and Schmidt, N. M. (2017a). Selecting the Number of States in
Hidden Markov Models - Pitfalls, Practical Challenges and Pragmatic Solutions. arXiv:1701.08673
[q-bio, stat].

Pohle, J., Langrock, R., van Beest, F. M., and Schmidt, N. M. (2017b). Selecting the Number of States
in Hidden Markov Models: Pragmatic Solutions Illustrated Using Animal Movement. Journal of
Agricultural, Biological and Environmental Statistics, 22(3):270–293.

Probst, T., Pryss, R., Langguth, B., and Schlee, W. (2016). Emotion dynamics and tinnitus: Daily life

data from the “TrackYourTinnitus” application. Scientiﬁc Reports, 6(1):31166.

Probst, T., Pryss, R. C., Langguth, B., Rauschecker, J. P., Schobel, J., Reichert, M., Spiliopoulou, M.,
Schlee, W., and Zimmermann, J. (2017). Does Tinnitus Depend on Time-of-Day? An Ecologi-
cal Momentary Assessment Study with the “TrackYourTinnitus” Application. Frontiers in Aging
Neuroscience, 9:253.

Pryss, R., Reichert, M., Herrmann, J., Langguth, B., and Schlee, W. (2015a). Mobile Crowd Sensing in
Clinical and Psychological Trials – A Case Study. In 2015 IEEE 28th International Symposium on
Computer-Based Medical Systems, pages 23–24.

Pryss, R., Reichert, M., Langguth, B., and Schlee, W. (2015b). Mobile Crowd Sensing Services
In 2015 IEEE International Conference on

for Tinnitus Assessment, Therapy, and Research.
Mobile Services, pages 352–359.

R Core Team (2021). R: A Language and Environment for Statistical Computing. R Foundation for

Statistical Computing, Vienna, Austria.

Schadt, E. E., Sinsheimer, J. S., and Lange, K. (1998). Computational Advances in Maximum Likeli-

hood Methods for Molecular Phylogeny. Genome Research, 8(3):222–233.

Turner, R. (2008). Direct maximization of the likelihood of a hidden Markov model. Computational

Statistics & Data Analysis, 52(9):4147–4160.

Venzon, D. J. and Moolgavkar, S. H. (1988). A Method for Computing Proﬁle-Likelihood-Based Conﬁ-
dence Intervals. Journal of the Royal Statistical Society: Series C (Applied Statistics), 37(1):87–94.
Visser, I., Raijmakers, M. E. J., and Molenaar, P. C. M. (2000). Conﬁdence intervals for hidden Markov
model parameters. British Journal of Mathematical and Statistical Psychology, 53(2):317–327.

36

Wald, A. (1943). Tests of Statistical Hypotheses Concerning Several Parameters When the Number of
Observations is Large. Transactions of the American Mathematical Society, 54(3):426–482.
Zucchini, W., MacDonald, I., and Langrock, R. (2016). Hidden Markov Models for Time Series: An
Introduction Using r, Second Edition. Chapman & Hall/CRC Monographs on Statistics & Applied
Probability. CRC Press.

37

