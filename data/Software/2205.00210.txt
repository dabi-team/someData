Software Testing for Machine Learning

Dusica Marijan, Arnaud Gotlieb
Simula Research Laboratory, Norway
dusica@simula.no, arnaud@simula.no

2
2
0
2

r
p
A
0
3

]
E
S
.
s
c
[

1
v
0
1
2
0
0
.
5
0
2
2
:
v
i
X
r
a

Abstract

Machine learning has become prevalent across a wide va-
riety of applications. Unfortunately, machine learning has
also shown to be susceptible to deception, leading to errors,
and even fatal failures. This circumstance calls into ques-
tion the widespread use of machine learning, especially in
safety-critical applications, unless we are able to assure its
correctness and trustworthiness properties. Software veriﬁ-
cation and testing are established technique for assuring such
properties, for example by detecting errors. However, soft-
ware testing challenges for machine learning are vast and pro-
fuse - yet critical to address. This summary talk discusses the
current state-of-the-art of software testing for machine learn-
ing. More speciﬁcally, it discusses six key challenge areas
for software testing of machine learning systems, examines
current approaches to these challenges and highlights their
limitations. The paper provides a research agenda with elab-
orated directions for making progress toward advancing the
state-of-the-art on testing of machine learning.

Index terms— testing challenges, machine learning, ma-

chine learning testing, testing ML, testing AI

1 Introduction
Applications of machine learning (ML) technology have be-
come vital in many innovative domains. At the same time,
the vulnerability of ML has become evident, sometimes
leading to catastrophic failures1. This entails that compre-
hensive testing of ML needs to be performed, to ensure the
correctness and trustworthiness of ML-enabled systems.

Software testing of ML systems is susceptible to a num-
ber of challenges compared to testing of traditional software
In this paper, by traditional systems we mean
systems.
software systems not integrating ML, and by ML systems
we mean software systems containing ML-trained compo-
nents (e.g self-driving cars, autonomous ships, or space ex-
ploration robots). As an example, one such challenge of
testing ML systems stems from non-determinism intrinsic
to ML. Traditional systems are typically pre-programmed
and execute a set of rules, while ML systems reason in a
probabilistic manner and exhibit non-deterministic behavior.

1Tesla failure, www.theguardian.com/technology/2016/jul/01/tesla-

driver-killed-autopilot-self-driving-car

This means that for constant test inputs and preconditions,
an ML-trained software component can produce different
outputs in consecutive runs. Researchers have tried using
testing techniques from traditional software development
(Hutchison et al. 2018), to deal with some of these chal-
lenges. However, it has been observed that traditional test-
ing approaches in general fail to adequately address funda-
mental challenges of testing ML (Helle and Schamai 2016),
and that these traditional approaches require adaptation to
the new context of ML. The better we understand current
research challenges of testing ML, the more successful we
can be in developing novel techniques that effectively ad-
dress these challenges and advance this scientiﬁc ﬁeld.

In this paper, we: i) identify and discuss the most chal-
lenging areas in software testing for ML, ii) synthesize the
most promising approaches to these challenges, iii) spotlight
their limitations, and iv) make recommendations of further
research efforts on software testing of ML. We note that
the aim of the paper is not to exhaustively list all published
work, but distill the most representative work.

2 Testing ML
As ML technologies become more pervasive enabling au-
tonomous system functionality, it is more and more impor-
tant to assure the quality of autonomous reasoning supported
by ML. Testing is such a quality assurance activity that aims
(in a broad sense) to determine the correctness of the system-
under-test, for example, by checking whether the system re-
sponds correctly to inputs, and to identify faults which may
lead to failures.
Interpreting ”Testing ML”: Two distinct communities
have been studying the concept of testing ML, the ML scien-
tiﬁc community (MLC) and the software testing community
(STC). However, as the two communities study ML algo-
rithms from different perspectives, they interpret the term
testing ML differently, and we think it is worth noting the
distinction. In MLC, testing an ML model is performed to
estimate its prediction accuracy and improve its prediction
performance. Testing happens during model creation, us-
ing validation and test datasets, to evaluate the model ﬁt on
the training dataset. In STC, testing an ML system has a
more general scope aiming to evaluate the system behav-

 
 
 
 
 
 
ior for a range of quality attributes. For example, in case
of integration or system level testing, an ML component is
tested in interaction with other system components for func-
tional and non-functional requirements, such as correctness,
robustness, reliability, or efﬁciency.

Challenges of Testing ML
Challenges of testing ML stem from the innate complex-
ity of the underlying stochastic reasoning. Unlike tradi-
tional systems, for which the code is built deductively,
ML systems are generated inductively. The logic deﬁn-
ing system behavior is inferred from training data. Conse-
quently, a fault could originate not only from a faulty soft-
ware code, but also errors in training data. However, ex-
isting approaches often assume that high quality datasets
are warranted, without applying systematic quality evalua-
tion. Furthermore, ML systems require advanced reason-
ing and learning capabilities that can give answers in con-
ditions where the correct answers are previously unknown
(Murphy, Kaiser, and Arias 2007). Even though this may be
the case for traditional systems, ML systems have inher-
ent non-determinism which makes them constantly change
behavior as more data becomes available, unlike tradi-
tional systems (Marijan, Gotlieb, and Kumar Ahuja 2019).
Furthermore, for a system containing multiple ML mod-
els, the models will affect each other’s training and tun-
ing, potentially causing non-monotonic error propagation
(Amershi, Begel, and Bird 2019).

We elaborate further challenges of testing ML in the fol-
lowing sections. Speciﬁcally, we identify six key challenge
areas and discuss their implications. We synthesize ex-
isting work pertaining to these challenges and provide its
structured presentation corresponding to the identiﬁed chal-
lenges.

3 Missing Test Oracles
Unlike traditional systems which operate pre-programmed
deterministic instructions, ML systems operate based on
stochastic reasoning. Such stochastic or probability-based
reasoning introduces uncertainty in the system response,
which gives rise to non-deterministic behavior, including
unpredictable or underspeciﬁed behavior. Due to non-
determinism, ML systems can change behavior as they learn
over time. The implications for testing are that system out-
puts can change over time for the same test inputs. This fact
largely complicates test case speciﬁcation.

Test cases are typically speciﬁed with speciﬁc inputs to
the system under test and expected outputs for these inputs,
known as test oracles. However, due to stochastic reason-
ing, the output of an ML system cannot be speciﬁed in ad-
vance, rather it is learned and predicted by an ML model.
This means that ML systems do not have deﬁned expected
values against which actual values can be compared in test-
ing. Thus, the correctness of the output in testing ML can-
not be easily determined. While this problem has been
known for traditional systems, called ”non-testable” systems
(Weyuker 1982), ML systems have non-determinism as part
of their design, making the oracle problem even more chal-
lenging.

An approach that has been considered for non-testable
systems are pseudo-oracles (Weyuker 1982).
Pseudo-
oracles are a differential testing technique that consists in
running multiple systems satisfying the same speciﬁcation
as the original system under test, then feeding the same in-
puts to these systems and observing their outputs. Discrep-
ancies in outputs are considered indicative of errors in the
system under test. A limitation of differential testing is that
it can be resource-inefﬁcient as it requires multiple runs of
the system, and error-prone, as the same errors are possi-
ble in multiple implementations of the system under test
(Knight and Leveson 1986).

Metamorphic Testing
Metamorphic testing is another approach to testing of soft-
ware without test oracles. In this approach, a transforma-
tion function is used to modify the existing test case in-
put, and produce a new output.
If the actual output for
the modiﬁed input differs from the expected output, it is
indicative of errors in the software under test. Metamor-
phic testing has been applied to machine learning classiﬁers
(Xie, Ho, and et al. 2011) (Dwarakanath et al. 2018). How-
ever, in testing ML systems with a large input space, writ-
ing metamorphic transformations is laborious, and there is
a great potential for ML to circumvent this difﬁculty by au-
tomating the creation of metamorphic relationships.

Test Data Prioritization
Since automated oracles are typically not available for test-
ing of big and realistic ML models, there is a great effort in-
volved in manual labeling of test data for ML models. Deep-
Gini (Shi et al. 2019) is an initial work on reducing the effort
in labeling test data for DNNs by prioritizing tests that are
likely to cause misclassiﬁcations. The assumption made by
DeepGini is that a test is likely to be misclassiﬁed if a DNN
outputs similar probabilities for each class. The limitation
of this approach is that it requires running all tests ﬁrst, to
obtain the output vectors used to calculate the likelihood of
misclassiﬁcation.

4

Infeasibility of Complete Testing

ML systems are commonly deployed in application ar-
eas dealing with a large amount of data. This creates
large and diverse test input space. Unfortunately, testing
is rarely able to cover all valid inputs and their combi-
nations to examine the correctness of a system-under-test,
and therefore coverage metrics are typically applied to se-
lect an adequate set of inputs from a large input space, to
generate tests, or to assess the completeness of a test set
and improve its quality (Marijan, Gotlieb, and Liaaen 2019;
Marijan and Liaaen 2018).

Test Coverage
The ﬁrst attempts to deﬁne coverage metrics for testing of
neural networks are inspired by the traditional code cover-
age metrics. A metric called neuron coverage was proposed
in DeepXplore (Pei et al. 2017) for testing deep neural net-
works (DNN). DeepXplore measures the amount of unique

neurons activated by a set of inputs out of the total num-
ber of neurons in the DNN. The limitation of this coverage
metric is that a test suite that has full neuron coverage (all
neurons activated) can still miss to detect erroneous behav-
ior if there was an error in all other DNNs that were part
of a differential comparing (Pei et al. 2017) used by neuron
coverage (DeepXplore leverages the concept of differential
testing). Furthermore, it has been shown that neuron cov-
erage can be too coarse a coverage metric, meaning that a
test suite that achieves full neuron coverage can be easily
found, but the network can still be vulnerable to trivial ad-
versarial examples (Sun, Huang, and Kroening 2018). Sun
et al.
therefore proposed DeepCover, a testing methodol-
ogy for DNNs with four test criteria, inspired by the mod-
iﬁed condition/decision coverage (MC/DC) for traditional
software. Their approach includes a test case generation
algorithm that perturbs a given test case using linear pro-
gramming with a goal to encode the test requirement and
a fragment of the DNN. The same author also developed a
test case generation algorithm based on symbolic approach
and the gradient-based heuristic (Sun et al. 2019). The dif-
ference between their coverage approach, based on MC/DC
criterion, and neuron coverage is that the latter only consid-
ers individual activations of neurons, while the former con-
siders causal relations between features at consecutive layers
of the neural network.

Neuron coverage has been further extended in Deep-
Gauge (Ma et al. 2018a), which aims to test DNN by com-
bining the coverage of key function regions as well as cor-
ner case regions of DNN, represented by neuron bound-
ary coverage. Neuron boundary coverage measures how
well the test datasets cover upper and lower boundary val-
ues. DeepRoad (Zhang et al. 2018) is another test genera-
tion approach for DNN-based autonomous driving. Deep-
Road is based on generative adversarial networks and it
generates realistic driving scenes with various weather con-
ditions. DeepCruiser is an initial work towards testing
recurrent-neural-network (RNN)-based stateful deep learn-
ing (Du et al. 2018). DeepCruiser represents RNN as an
abstract state transition system and deﬁnes a set of test
coverage criteria for generating test cases for stateful deep
learning systems. Other approaches were proposed ex-
tending the notion of neuron coverage, such as DeepTest
(Tian et al. 2018) for testing other types of neural networks.
DeepTest applies image transformations such as contrast,
scaling, blurring to generate synthetic test images. How-
ever, such generated images were found to be insufﬁciently
realistic for testing real-world systems.

In summary, a common limitation of techniques based on
neuron coverage is that they can easily lead to combinato-
rial explosion. Ma et al. initiated the work on the adapta-
tion of combinatorial testing techniques for the systematic
sampling of a large space of neuron interactions at differ-
ent layers of DNN (Ma et al. 2018c). This approach can be
promising for taming combinatorial explosion in testing of
DNN based systems, given that its current limitations are
overcome. First, only 2-way interactions of input parame-
ters are supported, while real systems typically have much
higher interaction levels of inputs. Second, the approach has

been found to face scalability problems for large and com-
plex DNNs.

Fuzzing

Since the input space of DNNs is typically large and highly-
dimensional, selecting test data for DNNs can be highly
laborious. One approach to deal with this challenge is
fuzzing, which generates large amounts of random input
data that is checked for failures. TensorFuzz is an initial
work that applies fuzzing to testing of TensorFlow DNNs
(Odena and Goodfellow 2018). TensorFuzz uses a coverage
metric consisting of user-speciﬁed constraints to randomly
mutate inputs. The coverage is measured by a fast approxi-
mate nearest neighbour algorithm. TensorFuzz has showed
to outperform random testing. Another similar approach
is DeepHunter (Xie et al. 2018). This is an initial work on
automated feedback-guided fuzz testing for DNNs. Deep-
Hunter runs metamorphic mutation to generate new seman-
tically preserved tests, and uses multiple coverage criteria as
a feedback to guide test generation from different perspec-
tives. The limitation of this approach is that it uses only
a single coverage criteria at the time, not supporting multi-
criteria test generation. Moreover, the general limitation of
fuzzing is that it cannot ensure that certain test objectives
will be satisﬁed.

Concolic Testing

To provide more effective input selection that increases test
coverage, a concolic testing approach has been proposed in
DeepConcolic (Sun et al. 2018). The approach is parame-
terised with a set of coverage requirements. The require-
ments are used to incrementally generate a set of test inputs
with a goal to improve the coverage of requirements by al-
ternating between concrete execution (testing on particular
inputs) and symbolic execution. For an unsatisﬁed require-
ment, a test input within the existing test suite that is close
to satisfying that requirement is identiﬁed, based on concrete
execution. Later, a new test input that satisﬁes the require-
ment is generated through symbolic execution and added to
the test suite, improving test coverage.

5 Quality of Test Datasets for ML Models
When training ML models, the quality of the training dataset
is important for achieving good performance of the learned
model. The performance is evaluated using a test dataset.

Mutation Testing

To evaluate the quality of test dataset for DNNs, DeepMu-
tation (Ma et al. 2018b) proposes an initial work, inspired
by traditional mutation testing concepts. DeepMutation ﬁrst
designs a set of mutation operators to inject faults into train-
ing data. Then, it retrains models with the mutated training
data to generate mutated models, which means that faults
are injected in the models. After that, mutated models are
tested using a test dataset. Finally, the quality of the test
dataset is evaluated by analysing to what extent the injected
faults are detected. The limitation of this approach is that

it employs basic mutation operators covering limited as-
pects of deep learning systems, so that the injected faults
may not be representative enough of real faults. MuNN
(Shen, Wan, and Chen 2018) is another mutation testing ap-
proach for neural networks, which needs further work for
the application on DNNs. Speciﬁcally, the authors of the
approach showed that neural networks of different depth re-
quire different mutation operators. They also showed the
importance of developing domain-dependent mutation oper-
ators rather than using common mutation operators.

6 Vulnerability to Adversaries
ML classiﬁers are known to be vulnerable to attacks
where small modiﬁcations are added to input data, caus-
ing misclassiﬁcation and leading to failures of ML sys-
tems (Szegedy et al. 2014). Modiﬁcations made to in-
put data, called adversarial examples, are small per-
turbations designed to be very close to the original
data, yet able to cause misclassiﬁcations and to com-
accuracy) of clasifﬁer.
promise the integrity (e.g.
Such attacks have been observed for image recognition
(Xie et al. 2017), text (Sato et al. 2018), and speech recog-
nition tasks (Carlini et al. 2016) (Carlini and Wagner 2018)
(Jia and Liang 2017).
In the latter, it was shown that ad-
versarially inserted sentences in the Stanford Question An-
swering Dataset can decrease reading comprehension of ML
from 75% to 36% of F-measure (harmonic average of the
precision and recall of a test).

Generating Adversarial Examples

Adversarial examples can be generated for the purpose of
attack or defense of an ML classiﬁer. The former often
use heuristic algorithms to ﬁnd adversarial examples that are
very close to correctly classiﬁed examples. The latter aim to
improve the robustness of ML classiﬁers.

Some

approaches

to adversarial

to cause adversarial examples.

example genera-
tion include Fast Gradient Sign Method (FGSM)
(Goodfellow, Shlens, and Szegedy 2015), which showed
linear behavior in high-dimensional spaces is suf-
that
ﬁcient
Later, FGSM
was
shown to be less effective for black-box at-
tacks (Tram`er et al. 2017), and the authors developed
random perturba-
RAND-FGSM method which adds
tions to modify adversarial perturbations.
DeepFool
(Moosavi-Dezfooli, Fawzi, and Frossard 2016)
is another
approach that generates adversarial examples based on
to generate
an iterative linearization of
minimal perturbations that are sufﬁcient to change clas-
The limitation of this approach lies
siﬁcation labels.
is a greedy heuristic, which cannot
in the fact
that
guarantee to ﬁnd optimal adversarial examples.
Fur-
ther, a two-player turn-based stochastic game approach
was developed for generating adversarial
examples
(Wicker, Huang, and Kwiatkowska 2018). The ﬁrst player
tries to minimise the distance to an adversarial example
by manipulating the features, and the second player can
be cooperative, adversarial, or random. The approach has
shown to converge to the optimal strategy, which represents

the classiﬁer

it

a globally minimal adversarial image. The limitation of this
approach is long runtime. Extending the idea of DeepFool,
a universal adversarial attack approach was developed
(Moosavi-Dezfooli et al. 2017). This approach generates
universal perturbations using a smaller set of input data, and
uses DeepFool to obtain a minimal sample perturbation of
input data, which is later modiﬁed into a ﬁnal perturbation.
Adversarial examples can be generated with generative
adversarial networks, such as AdvGAN (Xiao et al. 2018).
This approach aims to generate perturbations for any in-
stance, which can speed up adversarial training. The limi-
tation of the approach is that the resulting adversarial exam-
ples are based on small norm-bounded perturbations. This
challenge is further addressed in (Song et al. 2018) by de-
veloping unrestricted adversarial examples. However, their
approach exploits classiﬁer vulnerability to covariate shift
and is sensitive to different distributions of input data.

Countering Adversarial Examples

To counter adversarial attacks, reactive and proactive defen-
sive methods against adversaries have been proposed. De-
fensive distillation is a proactive approach which aims to
reduce the effectiveness of adversarial perturbations against
DNNs (Papernot et al. 2016). Defensive distillation extracts
additional knowledge about training points as class probabil-
ity vectors produced by a DNN. The probability vectors are
fed back into training, producing DNN-based classiﬁer mod-
els that are more robust to perturbations. However, it has
been shown that such defensive mechanisms are typically
vulnerable to some new attacks (Carlini and Wagner 2017).
Moreover, just like in testing, if a defense cannot ﬁnd any
adversarial examples, it does not mean that such examples
do not exist.

Automated veriﬁcation is a reactive defensive approach
against adversarial perturbations which analyses the ro-
bustness of DNNs to improve their defensive capabili-
ties. Several approaches exist to deal with the robust-
ness challenge. An exhaustive search approach to veri-
fying the correctness of a classiﬁcation made by a DNN
has been proposed (Huang et al. 2017).
This approach
checks the safety of a DNN by exploring the region around
a data point to search for speciﬁc adversarial manipula-
The limitation of the approach is limited scal-
tions.
ability and poor computational performance induced by
state-space-explosion. Reluplex is a constraint-based ap-
proach for verifying the properties of DNNs by providing
counter-examples (Katz et al. 2017), but is currently limited
to small DNNs. An approach that can work with larger
DNNs is global optimization based on adaptive nested op-
timisation (Ruan, Huang, and Kwiatkowska 2018). How-
ever,
in-
the approach is limited in the number of
A common chal-
put dimensions to be perturbed.
lenge for veriﬁcation approaches is their computational
complexity. For both approaches (Katz et al. 2017) and
(Ruan, Huang, and Kwiatkowska 2018), the complexity is
NP-complete. For the former, the complexity depends on
the number of hidden neurons, and for the latter, on input
dimensions.

7 Evaluating the Robustness of ML Models
To reduce the vulnerability of ML classiﬁers to adversaries,
research efforts are made on systematically studying and
evaluating the robustness of ML models, as well as on pro-
viding frameworks for benchmarking the robustness of ML
models.

Robustness Metrics
Lack of robustness in neural networks raises valid concerns
about the safety of systems relying on these networks, es-
pecially in safety-critical domains such as transportation,
robotics, medicine, or warfare. A typical approach to im-
prove the robustness of a neural network would be to iden-
tify adversarial examples that make the network fail, then
augment the training dataset with these examples and train
another neural network. The robustness of the new net-
work is the ratio between the number of adversarial exam-
ples that failed the original network and that were found for
the new network (Goodfellow, Shlens, and Szegedy 2015).
The limitation of this approach is the lack of objec-
tive robustness measure (Bastani et al. 2016). Therefore,
a metrics for measuring the robustness of DNNs us-
ing linear programming (Bastani et al. 2016) was proposed.
Other approaches include deﬁning the upper bound on
the robustness of classiﬁers to adversarial perturbations
(Fawzi, Fawzi, and Frossard 2018). The upper bound is
found to depend on a distinguishability measure between the
classes, and can be established independently of the learn-
ing algorithms. In their work, Fawzi et al. report two ﬁnd-
ings: ﬁrst, non-linear classiﬁers are more robust to adver-
sarial perturbations than linear classiﬁers, and second, the
depth (rather than breath) of a neural network has a key role
for adversarial robustness.

this

alleviate

Benchmarks for Robustness Evaluation
There is a difﬁculty of reproducing some of the methods
developed for improving the robustness of neural net-
works or methods for comparing experimental results,
as different sources of adversarial examples in the train-
ing process can make adversarial training more or less
(Goodfellow, Papernot, and McDaniel 2016).
effective
Cleverhans
To
challenge,
Fool-
and
(Goodfellow, Papernot, and McDaniel 2016)
box (Rauber, Brendel, and Bethge 2017) are adversarial
example libraries for developing and benchmarking adver-
sarial attacks and defenses, so that different benchmarks can
be compared. The limitation of both of these frameworks
is that they lack defensive adversarial generation strategies
(Yuan et al. 2019). Robust Vision Benchmark 2 extends
the idea of Foolbox, by allowing the development of novel
attacks which are used to further strengthen robustness
measurements of ML models. Other initiatives include a
competition organized at NIPS 2017 conference by Google
Brain, where researchers were encouraged to develop new
methods for generating adversarial examples and new
methods for defense against them (Kurakin et al. 2018).

2http://robust.vision/benchmarks/leaderboard

Formal Guarantees over Robustness

it

is of critical

Existing attempts

input perturbations.

For safety-critical domains which need to comply with
safety regulation and certiﬁcation,
im-
portance to provide formal guarantees of performance
of ML under adversarial
Provid-
ing such guarantees is a real challenge of most of
including the approaches discussed
defense approaches,
above.
in this direction include
(Hein and Andriushchenko 2017), by using regularization
in training, and (Sinha, Namkoong, and Duchi 2018), by
updating the training objective to satisfy robustness con-
straints. While these initial approaches are interesting, they
can provably achieve only moderate levels of robustness, i.e.
provide approximate guarantees. As such, further research
advances on providing robustness guarantees for ML models
are needed.

8 Verifying Ethical Machine Reasoning

ML systems can be deployed in environments where their
actions have ethical implications, for example self-driving
cars, and as a consequence, they need to have the capabil-
ities to reason about such implications (Deng 2015). Even
more so, if such systems are to become widely socially
accepted technologies. While multiple approaches have
been proposed for building ethics into ML, the real research
challenge lies in building solutions for verifying such ma-
chine ethics. This research area has remained largly unad-
dressed. Existing efforts are limited and include a theoretical
framework for ethical decision-making of autonomous sys-
tems that can be formally veriﬁed (Dennis et al. 2016). The
framework assumes that system control is separated from
a higher-order decision-making, and uses model checking
to verify the rational agent (model checking is the most
widely used approach to verifying ethical machine reason-
ing). However, as a limitation, the proposed approach re-
quires ethics plans that have been correctly annotated with
ethical consequences, which cannot be guaranteed. Second,
the agent veriﬁcation is demonstrated to be very slow. For
situations where no ethical decision exists, the framework
continuous ethical reasoning, negatively affecting overall
performance. Third, the approach scales poorly to the num-
ber of sensors and sensor values, due to non-deterministic
modelling of sensor inputs. Furthermore, the approach can-
not provide any guarantees that a rational agent will always
operate within certain bounds regardless of the ethics plan.
Regarding the certiﬁcation of autonomous reasoning, a
proof-of-concept approach (Webster et al. 2014) was devel-
oped for the generation of certiﬁcation evidence for au-
tonomous aircraft using formal veriﬁcation and ﬂight simu-
lation. However, the approach relies on a set of assumptions,
such as that the requirements of a system are known, or that
they have been accurately translated into a formal speciﬁca-
tion language, which may not always hold. Finally, ethical
machine reasoning should be transparent to allow for check-
ing of the underlying reasoning. These ﬁndings emphasize
the need for further progress in verifying and certifying eth-
ical machine reasoning.

9 Summary and Future Directions

Software testing of ML faces a range of open research chal-
lenges, and further research work focused on addressing
these challenges is needed. We envision such further work
developing in the following directions.
Automated test oracles. Test oracles are often missing in
testing ML systems, which makes checking the correctness
of their output highly challenging. Metamorphic testing can
help address this challenge, and further work is needed on
using ML to automate the creation of metamorphic relation-
ships.
Coverage metrics for ML models. Existing coverage met-
rics are inadequate in some contexts. Structural cover-
age criteria can be misleading, i.e.
too coarse for adver-
sarial inputs and too ﬁne for misclassiﬁed natural inputs
(Li et al. 2019). High neuron coverage does not mean in-
vulnerability to adversarial examples (Sun et al. 2019). In
addition, neuron coverage can lead to input space explosion.
Adaptation of combinatorial testing techniques is a promis-
ing approach to this challenge, given that progress is made
on improving its scalability for real-word ML models.
Quality of test datasets for ML models. Evaluation of
the quality of datasets for ML models is in its early stages.
Adaptation of mutation testing can alleviate this challenge.
Common mutation operators are insufﬁcient for mutation
testing of DNNs. Instead, domain-speciﬁc operators are re-
quired.
Cost-effectiveness of adversarial examples. Generation
strategies for adversarial examples need further advancing
to reduce computational complexity and improve effective-
ness for different classiﬁers.
Cost-effectiveness of adversarial countermeasures. Cur-
rent techniques are mainly vulnerable to advanced attacks.
Veriﬁcation approaches for DNNs to counter adversarial ex-
amples are computationally complex (especially constraint-
based approaches) and unscalable for real DNNs. More
cost-effective veriﬁcation approaches are required.
Robustness evaluation of ML models. Metrics for robust-
ness evaluation of ML models and effectiveness evaluation
of adversarial attacks need further advancing. Open bench-
marks for developing and evaluating new adversarial attacks
and defense mechanisms can be useful tools to achieve an
improved robustness of defense. Further efforts on under-
standing the existence of adversarial examples is desired
(Yuan et al. 2019).
Certiﬁed guarantees over robustness of ML models.
Such guarantees are required for the deployment of ML in
safety-critical domains. Current approaches provide only
approximate guarantees. Also, further research progress is
needed to overcome high computational complexity of pro-
ducing the guarantees.
Veriﬁcation of machine ethics. Formal veriﬁcation and cer-
tiﬁcation of ethical machine reasoning is uniquely challeng-
ing. Further efforts are needed to enable the scalability of
these approaches for real systems operating in real-time, and
to reach lower computational complexity. In addition, veri-

ﬁcation approaches may leverage different formal methods,
which underlines the open challenge of interoperability be-
tween different methods. Finally, research advances on en-
abling the transparency of ethical decision making process
is required.

In conclusion, with this paper we hope to provide re-
searchers with useful insights into an unaddressed chal-
lenges of testing of ML, along with an agenda for advancing
the state-of-the-art in this research area.

10 Acknowledgments
This work is supported by the Research Council of Norway
through the project T3AS No 287329.

References
[Amershi, Begel, and Bird 2019] Amershi, S.; Begel, A.; and
Bird, Christian, e. a. 2019. Software engineering for machine
learning: A case study. In Int. Conf. on Soft. Eng.: SEIP, 291–
300. IEEE Press.

[Bastani et al. 2016] Bastani, O.; Ioannou, Y.; Lampropoulos,
L.; Vytiniotis, D.; Nori, A.; and Criminisi, A. 2016. Mea-
suring neural net robustness with constraints. In Int. Conf. on
Neural Inf. Processing Systems, 2621–2629.

[Carlini and Wagner 2017] Carlini, N., and Wagner, D. 2017.
IEEE

Towards evaluating the robustness of neural networks.
Symp. on Security and Privacy 39–57.

[Carlini and Wagner 2018] Carlini, N., and Wagner, D. 2018.
Audio adversarial examples: Targeted attacks on speech-to-
text. In IEEE Security and Privacy Worksh., 1–7.

[Carlini et al. 2016] Carlini, N.; Mishra, P.; Vaidya, T.; Zhang,
Y.; Sherr, M.; Shields, C.; Wagner, D.; and Zhou, W. 2016.
Hidden voice commands. In USENIX Conf. on Security Symp.,
513–530.

[Deng 2015] Deng, B. 2015. The robot’s dilemma. Nature

523/7558.

[Dennis et al. 2016] Dennis, L.; Fishera, M.; Slavkovik, M.; and
Webstera, M. 2016. Formal veriﬁcation of ethical choices in
autonomous systems. Robotics and Autonomous Systems 77:1–
14.

[Du et al. 2018] Du, X.; Xie, X.; Li, Y.; Ma, L.; Zhao, J.; and
Liu, Y. 2018. Deepcruiser: Automated guided testing for state-
ful deep learning systems. CoRR abs/1812.05339.

[Dwarakanath et al. 2018] Dwarakanath, A.; Ahuja, M.;
Sikand, S.; and et al. 2018. Identifying implementation bugs in
machine learning based image classiﬁers using metamorphic
testing. In Int. Symp. on Soft. Test. and Anal., 118–128.

[Fawzi, Fawzi, and Frossard 2018] Fawzi, A.; Fawzi, O.; and
Frossard, P. 2018. Analysis of classiﬁers’ robustness to ad-
versarial perturbations. Machine Learning 107(3):481–508.

[Goodfellow, Papernot, and McDaniel 2016] Goodfellow,

I.;
Papernot, N.; and McDaniel, P. 2016. cleverhans v0.1: an
adversarial machine learning library. CoRR abs/1610.00768.

[Goodfellow, Shlens, and Szegedy 2015] Goodfellow,

I.;
Shlens, J.; and Szegedy, C. 2015. Explaining and harness-
Int. Conf. on Learn. Represen.
ing adversarial examples.
abs/1412.6572.

[Hein and Andriushchenko 2017] Hein,

An-
driushchenko, M. 2017. Formal guarantees on the robustness
In Annual
of a classiﬁer against adversarial manipulation.
Conf. on Neural Inf. Processing Systems.

and

M.,

[Helle and Schamai 2016] Helle, P., and Schamai, W. 2016.
Testing of autonomous systems – challenges and current state-
of-the-art. In INCOSE Int. Symposium (IS 2016).

[Huang et al. 2017] Huang, X.; Kwiatkowska, M.; Wang, S.;
and Wu, M. 2017. Safety veriﬁcation of deep neural networks.
In Computer Aided Veriﬁcation, 3–29.

[Hutchison et al. 2018] Hutchison, C.; Zizyte, M.; Lanigan,
P. E.; Guttendorf, D.; Wagner, M.; Le Goues, C.; and Koop-
man, P. 2018. Robustness testing of autonomy software.
In
IEEE/ACM Int. Conf. on Soft. Eng., 276–285.

[Jia and Liang 2017] Jia, R., and Liang, P. 2017. Adversarial
In

examples for evaluating reading comprehension systems.
Conf. on Emp. Methods in Natural Lang. Process.

[Katz et al. 2017] Katz, G.; Barrett, C.; Dill, D.; Julian, K.; and
Kochenderfer, M. 2017. Reluplex: An efﬁcient smt solver for
verifying deep neural networks. In Computer Aided Verif., 97–
117.

[Knight and Leveson 1986] Knight, J. C., and Leveson, N. G.
1986. An experimental evaluation of the assumption of inde-
pendence in multiversion programming. IEEE Trans. on Soft.
Eng. (1):96–109.

[Kurakin et al. 2018] Kurakin, A.; Goodfellow, I.; Bengio, S.;
and et al. 2018. Adversarial attacks and defences competi-
tion. In The NIPS ’17 Competition: Building Intelligent Sys-
tems, 195–231.

[Li et al. 2019] Li, Z.; Ma, X.; Xu, C.; and Cao, C. 2019. Struc-
tural coverage criteria for neural networks could be misleading.
In Int. Conf. on Soft. Eng.: NIER, 89–92.

[Ma et al. 2018a] Ma, L.; Juefei-Xu, F.; Zhang, F.; Sun, J.; and
et al. 2018a. Deepgauge: Multi-granularity testing criteria for
deep learning systems. In Int. Conf. on Aut. Soft. Eng., 120–
131.

[Ma et al. 2018b] Ma, L.; Zhang, F.; Sun, J.; Xue, M.; and et al.
2018b. Deepmutation: Mutation testing of deep learning sys-
tems. IEEE Int. Symp. on Soft. Reliab. Eng. 100–111.

[Ma et al. 2018c] Ma, L.; Zhang, F.; Xue, M.; Li, B.; Liu, Y.;
Zhao, J.; and Wang, Y. 2018c. Combinatorial testing for deep
learning systems. CoRR abs/1806.07723.

[Marijan and Liaaen 2018] Marijan, D., and Liaaen, M. 2018.
Practical selective regression testing with effective redundancy
in interleaved tests. In 2018 IEEE/ACM International Confer-
ence on Software Engineering: Software Engineering in Prac-
tice Track (ICSE-SEIP), 153–162.

[Marijan, Gotlieb, and Kumar Ahuja 2019] Marijan,

D.;
Gotlieb, A.; and Kumar Ahuja, M.
2019. Challenges of
testing machine learning based systems. In 2019 IEEE Inter-
national Conference On Artiﬁcial Intelligence Testing (AITest),
101–102.

[Marijan, Gotlieb, and Liaaen 2019] Marijan, D.; Gotlieb, A.;
and Liaaen, M. 2019. A learning algorithm for optimizing
continuous integration development and testing practice. Soft.
Pract. and Exper. 49:192–213.

[Moosavi-Dezfooli et al. 2017] Moosavi-Dezfooli, S.; Fawzi,

A.; Fawzi, O.; and Frossard, P. 2017. Universal adversarial
perturbations. Conf. on Comp. Vis. and Pattern Recog. 86–94.

[Moosavi-Dezfooli, Fawzi, and Frossard 2016] Moosavi-

Dezfooli, S.; Fawzi, A.; and Frossard, P. 2016. Deepfool:
A simple and accurate method to fool deep neural networks.
IEEE Conf. on Computer Vision and Pattern Recognition.

[Murphy, Kaiser, and Arias 2007] Murphy, C.; Kaiser, G. E.;
and Arias, M. 2007. An approach to software testing of ma-
chine learning applications. In Soft. Eng. and Knowledge Eng.
[Odena and Goodfellow 2018] Odena, A., and Goodfellow, I.
2018. Tensorfuzz: Debugging neural networks with coverage-
guided fuzzing. CoRR abs/1807.10875.

[Papernot et al. 2016] Papernot, N.; McDaniel, P.; Wu, X.; Jha,
S.; and Swami, A. 2016. Distillation as a defense to adversarial
perturbations against deep neural networks. In IEEE Symp. on
Sec. and Privacy, 582–597.

[Pei et al. 2017] Pei, K.; Cao, Y.; Yang, J.; and Jana, S. 2017.
Deepxplore: Automated whitebox testing of deep learning sys-
tems. In Symp. on Oper. Syst. Princip., 1–18.

[Rauber, Brendel, and Bethge 2017] Rauber, J.; Brendel, W.;
and Bethge, M. 2017. Foolbox v0.8.0: A python toolbox to
benchmark the robustness of machine learning models. CoRR
abs/1707.04131.

[Ruan, Huang, and Kwiatkowska 2018] Ruan, W.; Huang, X.;
and Kwiatkowska, M. 2018. Reachability analysis of deep
neural networks with provable guarantees. In Int. J. Conf. on
Artif. Intel., 2651–2659.

[Sato et al. 2018] Sato, M.; Suzuki, J.; Shindo, H.; and Mat-
sumoto, Y. 2018. Interpretable adversarial perturbation in input
embedding space for text. In Int. J. Conf. on Artif. Intel.

[Shen, Wan, and Chen 2018] Shen, W.; Wan, J.; and Chen, Z.
2018. Munn: Mutation analysis of neural networks. Int. Conf.
on Soft. Quality, Reliab. and Secur. Comp. 108–115.

[Shi et al. 2019] Shi, Q.; Wan, J.; Feng, Y.; Fang, C.; and Chen,
Z. 2019. Deepgini: Prioritizing massive tests to reduce labeling
cost. CoRR abs/1903.00661.

[Sinha, Namkoong, and Duchi 2018] Sinha, A.; Namkoong, H.;
and Duchi, J. 2018. Certiﬁable distributional robustness with
principled adversarial training. Int. Conf. on Learning Repre-
sentations abs/1710.10571.

[Song et al. 2018] Song, Y.; Shu, R.; Kushman, N.; and Ermon,
S. 2018. Constructing unrestricted adversarial examples with
generative models. In Adv. in Neural Inf. Proc. Sys.

[Sun et al. 2018] Sun, Y.; Wu, M.; Ruan, W.; Huang, X.;
Kwiatkowska, M.; and Kroening, D. 2018. Concolic testing
for deep neural networks. In Int. Conf. on Autom. Soft. Eng.,
109–119.

[Sun et al. 2019] Sun, Y.; Huang, X.; Kroening, D.; Sharp, J.;
Hill, M.; and Ashmore, R. 2019. Structural test coverage crite-
ria for deep neural networks. In Int. Conf. on Soft. Eng.

[Sun, Huang, and Kroening 2018] Sun, Y.; Huang, X.; and
Kroening, D. 2018. Testing deep neural networks. CoRR
abs/1803.04792.

[Szegedy et al. 2014] Szegedy, C.; Zaremba, W.; Sutskever, I.;
Intriguing properties
Bruna, J.; Erhan, D.; and et al. 2014.
of neural networks. In Int. Conf. on Learning Representations,
volume abs/1312.6199.

[Tian et al. 2018] Tian, Y.; Pei, K.; Jana, S.; and Ray, B. 2018.
Deeptest: Automated testing of deep-neural-network-driven
autonomous cars. In Int. Conf. on Soft. Eng., 303–314.

[Tram`er et al. 2017] Tram`er, F.; Kurakin, A.; Papernot, N.;
Boneh, D.; and McDaniel, P. 2017. Ensemble adversarial train-
ing: Attacks and defenses. In Int. Conf. on Learning Represen-
tations.

[Webster et al. 2014] Webster, M.; Cameron, N.; Fisher, M.;
and Jump, M. 2014. Generating certiﬁcation evidence for au-
tonomous unmanned aircraft using model checking and simu-
lation. J. Aerospace Inf. Sys. 11:258–279.

[Weyuker 1982] Weyuker, E. J. 1982. On testing non-testable

programs. Comput. J. 25:465–470.

[Wicker, Huang, and Kwiatkowska 2018] Wicker, M.; Huang,
X.; and Kwiatkowska, M. 2018. Feature-guided black-box
safety testing of deep neural networks. In Tools and Alg. for the
Construction and Anal. of Systems, 408–426.

[Xiao et al. 2018] Xiao, C.; Li, B.; Zhu, J.; He, W.; Liu, M.; and
Song, D. 2018. Generating adversarial examples with adver-
sarial networks. In Int. Joint Conf. on Artif. Intel., 3905–3911.
[Xie et al. 2017] Xie, C.; Wang, J.; Zhang, Z.; and et al., Y. Z.
2017. Adversarial examples for semantic segmentation and ob-
ject detection. In IEEE Int. Conf. on Computer Vision, 1378–
1387.

[Xie et al. 2018] Xie, X.; Ma, L.; Juefei-Xu, F.; Chen, H.; and
et al. 2018. Coverage-guided fuzzing for deep neural networks.
CoRR abs/1809.01266.

[Xie, Ho, and et al. 2011] Xie, X.; Ho, J.; and et al. 2011. Test-
ing and validating machine learning classiﬁﬁers by metamor-
phic testing. J. of Sys. and Soft. (84):544–558.

[Yuan et al. 2019] Yuan, X.; He, P.; Zhu, Q.; and Li, X. 2019.
Adversarial examples: Attacks and defenses for deep learning.
Tr. on Neural Net. and Learn. Syst. 1–20.

[Zhang et al. 2018] Zhang, M.; Zhang, Y.; Zhang, L.; and et al.
2018. Deeproad: Gan-based metamorphic testing and input
validation framework for autonomous driving systems. In Int.
Conf. on Aut. Soft. Eng., 132–142.

