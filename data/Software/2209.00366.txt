How to Evaluate Explainability? –
A Case for Three Criteria

Timo Speith
Saarland University, Institute of Philosophy, Saarbr¨ucken, Germany
Saarland University, Department of Computer Science, Saarbr¨ucken, Germany
Email: timo.speith@uni-saarland.de

2
2
0
2

p
e
S
1

]
E
S
.
s
c
[

1
v
6
6
3
0
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—The increasing complexity of software systems and
the inﬂuence of software-supported decisions in our society
have sparked the need for software that is safe, reliable, and
fair. Explainability has been identiﬁed as a means to achieve
these qualities. It is recognized as an emerging non-functional
requirement (NFR) that has a signiﬁcant impact on system
quality. However, in order to develop explainable systems, we
need to understand when a system satisﬁes this NFR. To this end,
appropriate evaluation methods are required. However, the ﬁeld
is crowded with evaluation methods, and there is no consensus
on which are the “right” ones. Much less, there is not even
agreement on which criteria should be evaluated. In this vision
paper, we will provide a multidisciplinary motivation for three
such quality criteria concerning the information that systems
should provide: comprehensibility, ﬁdelity, and assessability. Our
aim is to to fuel the discussion regarding these criteria, such that
adequate evaluation methods for them will be conceived.

Index Terms—Explainability, Explainable Artiﬁcial Intelli-

gence, Evaluation, Non-Functional Requirements, NFR, XAI

I. INTRODUCTION

In today’s world, software systems are increasingly impact-
ing sensitive areas of daily life (e.g., healthcare [1] or criminal
justice [2]). However, contemporary systems are complex,
effectively being black boxes for the growing number of stake-
holders involved. The ubiquitous inﬂuence of such “black-box
systems” has induced discussions about the transparency and
ethics of modern systems [3]. Responsible collection and use
of data, privacy, safety, and security are just a few among many
concerns. In light of this, it is becoming increasingly crucial
to understand how to incorporate these concerns into systems
and, thus, how to deal with them during software engineering
(SE) and requirements engineering (RE) [4].

A. The Need for Explainability

In this regard, research regarding explainability has recently
experienced an upsurge [5], [6]. Explainability promises to
alleviate a system’s lack of transparency [7] and to provide
a fruitful way to address ethical concerns about modern
systems [8]. Furthermore, explainability is increasingly seen
as crucial for high software quality, and should be treated
as a non-functional requirement (NFR) [9]. Unfortunately,
“explainability” is a nebulous and elusive concept that is hard
to target. This causes difﬁculties for researchers, especially
when it comes to designing and evaluating explainability in
systems [6], [10]. Regarding the design of explainable systems,
for instance, scholars often seem to rely on intuition [4], [11].

As for the evaluation of a system’s explainability, there is
no consensus on what constitutes a good method [12]–[14].
There are several methods for evaluating explainability, each
of which comes with its own underlying rationale for which
criteria are essential for good system explainability. However,
these evaluation methods all have their limitations. First and
foremost, we are not aware of any theoretical considerations
that motivate certain quality criteria over others.

In this vision paper, we will provide a multidisciplinary
motivation for three quality criteria concerning the information
that systems should provide in order to count as explainable:
comprehensibility, ﬁdelity, and assessability. Our aim is to to
fuel the discussion regarding these criteria, such that useful
and adequate evaluation methods for them will be conceived.

B. The Goals of Explainability

Before we can motivate that these criteria must be evaluated,
we ﬁrst need to be clear about the goals of explainability in
general. Hofmann et al. [15] and Langer et al. [6] propose
similar models that outline how key concepts in explainabil-
ity are related to one another. According to their models,
explainability approaches (i.e., ways to reach explainability)
provide explanatory information with the aim of facilitating
people’s understanding. This understanding, in turn, affects the
satisfaction of so-called desiderata (e.g., transparency, safety;
see Figure 1 for a simpliﬁed version of their models).

Understanding

Explainability
Approach

provides

facilitates

Explanatory
Information

affects

Desiderata
Satisfaction

Fig. 1: A simpliﬁed version of the models of the main concepts
and relations in explainability that Langer et al. [6] and
Hofmann et al. [15] propose. Dotted arrows indicate relations
that are fully mediated by the solid arrows.

To summarize the models, explainability does not necessar-
ily serve as a goal directly, but rather as a means to other goals
(i.e., desiderata), such as transparency, satisfaction, and safety.
In a literature survey of over 100 works, Langer et al. [6] found
29 desiderata that explainability is intended to contribute to.
In an even more comprehensive literature review of over 200
works, Chazette et al. [4] found 57 such desiderata that can
(positively and negatively) be impacted by explainability and
should be considered when it comes to high system quality.

© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.

 
 
 
 
 
 
II. QUALITY CRITERIA FOR EVALUATING EXPLANATORY
INFORMATION
Based on the above conceptualization of the goal(s) of ex-
plainability, it is possible to distill multidisciplinary motivated
quality criteria that the explanatory information provided by
systems should meet to ensure good system explainability.

A. The Three Dimensions of Explainability

In particular, we motivate three criteria by which to eval-
uate the quality of explanatory information produced by an
explainability approach. These criteria are inspired by the three
dimensions of explainability as described by Baum et al. [16]:
• Comprehensibility: Explanatory information must be
conveyed in a way that is comprehensible to humans, for
humans must understand (certain aspects of) the system
based on the information. Obviously, comprehensible
information is best suited for this purpose.

• Fidelity: Explanatory information must be ﬁdelitous with
respect to (the aspect of) the system it is about. For
instance, the information must describe the accurate (i.e.,
correct, real) reasons for a system acting the way it did.
Although some details may be omitted or simpliﬁed for
comprehensibility, outright lies should never be told.
• Assessability:1 Explanatory information must be such
that one can assess the satisfaction of a given desidera-
tum. In other words, the quality of explainability depends
on the desiderata that one aims to satisfy with it.

While these criteria are inspired by previous work (i.e.,
[16]), it is easy to motivate them independently. First, infor-
mation that is incomprehensible or misleading (because it is
not ﬁdelitous) is unlikely to help facilitate a person’s true
understanding of a phenomenon. Considering the fact that
understanding is an essential factor in the ﬁeld of explain-
ability [4], [6], [9], systems that produce incomprehensible or
inﬁdelitous information do not count as explainable.

Let us now turn to assessability. As stated above, the ﬁnal
goal of receiving information about (certain aspects of) a
system is to satisfy certain desiderata [4]–[6]. However, it is
conceptually possible that a given piece of information can be
both comprehensible and ﬁdelitous, but completely irrelevant
to the desiderata of interest. For this reason, we need a criterion
that restricts the set of relevant pieces of information to those
that serve the the targeted goal: satisfying these desiderata.

B. Arguments for Our Criteria

In what follows, we will provide multidisciplinary argu-
ments to further corroborate our criteria. In particular, these
arguments are based on satisfying certain often cited desiderata
(see [4], [6], [8]) that are important for different disciplines.

1In the original source, the third dimension is Permissibility. We deliberately
deviate here, as we are concerned with the desirable properties of explanatory
information. The original source dealt with the properties of computational
systems that are necessary to make them trustworthy. Trustworthy systems
should indeed not only be be able to deliver assessable information but they
should in fact be permissible. That is, they should be positively assessed
concerning certain desiderata (e.g., morality, fairness, safety, reliability).
However, the information must be assessable with respect to these desiderata
to allow somebody to determine whether the system is in fact permissible.

1) Acceptance and Trustworthiness: Our primary argument
is the need for acceptance and trustworthiness. It is often
argued that systems that are unable to explain themselves will
lack acceptance in the long run [10], [17], [18].

From a moral point of view,

is plausibly to assume
that the deployment of some kinds of (autonomous) systems
promises to bring about overall positive effects. Thus, as
long as people do not accept these systems, their presumably
beneﬁcial deployment is threatened [19], [20].

it

Furthermore, from a pragmatical point of view, developers
and companies want the systems they design to be accepted by
the populace. Systems that are developed but not sufﬁciently
deployed will threaten the economical basis of the company
(and, thus, of the developers in it).

For people to accept systems, trust is an essential prerequi-
site [21], [22]. However, not any kind of trust will be beneﬁcial
in the long run. Only justiﬁed trust in a system will bring about
the best consequences concerning it; inadequate trust can lead
to disastrous consequences [23]. Consequently, adequate trust
is desirable from a moral and pragmatical point of view.

So, to signiﬁcantly increase the probability that autonomous
systems will be used on a large scale, they must be justiﬁ-
ably trusted. Justiﬁed trust is based on trustworthiness [23],
[24]. Furthermore, trustworthiness depends on a stakeholder’s
justiﬁcation in believing that a system works properly, and
explanations are one way in giving these justiﬁcations [23].
Thus, explanations are an important factor in calibrating trust,
a connection that is is often argued for [17], [25]–[27].

First,

Of course, not just any kind of explanation will do. To
be trustworthy, software systems must be able to justify their
actions in the right way. A well-motivated means of doing so
is to provide information that at least meets the criteria above.
the information must be comprehensible, because
incomprehensible information is not sufﬁcient to provide any
justiﬁcation. Furthermore, the information must be ﬁdelitous,
since a lying system, even if otherwise functioning properly,
cannot count as trustworthy. Finally, the information must
support the assessability of pertinent desiderata, as this allows
people to judge whether the system is working properly.2

2) Accountability and Autonomy: Our second argument is
the need for responsibility and autonomy in the interaction
of artiﬁcial systems and humans. It is foreseeable that in
the future, humans will increasingly rely on the decisions of
software systems. Currently, the trend is to use such systems to
make recommendations in situations where much is at stake.
From a legal and moral perspective, it is highly desirable
to be able to hold someone accountable when something goes
wrong in such critical situations [28], [29]. However, with
modern systems, it is often difﬁcult to assign responsibility
[30], [31]. In order to still be able to hold someone accountable
in such situations, it is often assumed that there must be a
human in the loop who makes the ﬁnal decision and who is
the most probable bearer of responsibility [30].

2For further discussion of why the quality criteria we set out are good

criteria to judge the trustworthiness of a system, see [16].

Imagine an HR person. They receive a recommendation for
each and every applicant, solely stating whether to keep the
applicant in the running or not. How can the HR person come
to the kind of decision that makes them a bearer of responsi-
bility? If they decide solely in line with the recommendations,
they are just a submissive executor of the decision made by a
system. In this case, one could simply dismiss the human in
the loop altogether. If they decide against the recommendation,
they cannot have good reasons for doing so without knowing
the reasons for the recommendation in the ﬁrst place [30].

Accordingly, to properly bear responsibility for a decision, it
must be made autonomously [32]. However, without being able
to comprehend and assess a recommendation, human beings
lose their autonomy. After all, when someone is to decide
competently and autonomously, they need more than simple
recommendations from a system. A human in the loop needs
reasons for the recommendations to assess their correctness
and potentially challenge them. Explanatory information, thus,
must be at least comprehensible and assessable to properly al-
low for human autonomy, and, with this, human accountability.
3) Fairness: The last argument is the need for fairness. In
many situations, human beings are immediately affected by
the decisions made or supported by software systems [6], [8].
From a societal point of view, a purely statistical checking
of systems is not sufﬁcient to ensure that no individual’s
fundamental rights are violated. Instead, one needs to be able
to assess the individual decisions of such systems. In other
words, one should not only be concerned with whether a
system overall did not discriminate against certain groups of
people, but rather with whether each decision did not do so.
The reason for this restriction is simple: different systems
can arrive at the same result in completely dissimilar ways. To
illustrate, imagine two systems that rank job applicants. Both
systems rank a Black woman last, but for different reasons.
The ﬁrst system does so because she really is the least qualiﬁed
(e.g., she has no prior work experience, bad grades, etc.). The
second system does so just because she is Black and a woman.
While to some this point may seem to be practically
unimportant under the assumption that a statistical bias can
be reasonably excluded, it is essential to make the system
trustworthy and establish public acceptance. In particular, it is
crucial to guarantee that the fundamental values of liberal soci-
eties are considered sufﬁciently. Statistical adherence to norms
and rules is important, but it should not be the only thing
to be considered. Each individual case matters. Explanatory
information, thus, must be at least assessable and ﬁdelitous.

III. EVALUATING CONTEMPORARY APPROACHES

Having motivated and argued for our criteria, we will now
turn to their application. In particular, we will examine some
prominent explainability approaches in the ﬁeld of explainable
artiﬁcial intelligence (XAI) and evaluate the information they
produce based on our criteria. In this regard, it should be noted
that we are mostly raising theoretical points, since, as stated
above, there is little to no agreement on evaluation methods.
Still, this should show that our criteria are worthy of attention.

In particular, we will examine LIME [33], Vanilla Back-
propagation [34], Guided Backpropagation [35], Integrated
Gradients [36], Grad-CAM [37], and TCAV [38]. Figure 2
shows exemplary outputs of some of these approaches.

(a) Vanilla Backpropagation

(b) Integrated Gradients

(c) Grad-CAM

(d) LIME

Fig. 2: Outputs of different explainability approaches.

A. Comprehensibility

Most of the above approaches beneﬁt from the visual
representation of the explanatory information they produce.
One does not have to be an expert to recognize that highlighted
areas of an image correspond to important parts. However,
people without sufﬁcient background in machine learning
(ML) will, in many cases, not comprehend what the high-
lighting means on a deeper (e.g., technical) level [6], [39].

In particular, pixel-oriented saliency masks (produced by,
for instance, Vanilla Backpropagation and Integrated Gradi-
ents) suffer from this. Additionally, these techniques have
further problems. First, they generate explanatory information
that is very difﬁcult to analyze even for experts [6], [39], [40].
For instance, for people without sufﬁcient background, Vanilla
Backpropagation sometimes seems to produce saliency masks
with random highlighting (see Figure 2a). Moreover, these
techniques suffer from computational artifacts, often produc-
ing out-of-context highlighting that hinders comprehensibility.
LIME and TCAV perform better in terms of comprehensi-
bility. When it comes to LIME, its inventors have conﬁrmed
the comprehensibility of the information it generates in several
studies [33]. As for TCAV, it has the advantage that the users
themselves can deﬁne the concepts for which an artiﬁcial neu-
ral network (ANN) is to be tested. On the other hand, TCAV
has the disadvantage that it requires at least some background
knowledge of how ANNs work, since one must specify which
layers of an ANN to test and analyze the results accordingly.
For example, one needs to know that deeper layers of ANNs
are more like to respond to complex concepts (e.g., gender)
than other layers that respond to simpler concepts (e.g., color).

B. Fidelity

Currently, there is not much research that evaluates the
ﬁdelity of explanatory information produced by explainability
approaches. The reason for this shortcoming is, among others,
that it is not yet entirely clear, formally and technically, what it
means for explanatory information to be ﬁdelitous [41]–[43].
Despite this difﬁculty, there are some authors who try to
address the issue. For instance, Adebayo et al. [42] developed
a sanity check for approaches producing saliency maps. Using
this test, they evaluated several well-known approaches. They
found that while some approaches pass their sanity check and
produce ﬁdelitous information (e.g., Vanilla Backpropagation,
Grad-CAM), others do not (e.g., Guided Backpropagation).

Another notable work is that of Amparore et al. [43].
they found that many implementations of
Among others,
LIME do not satisfy the theoretical properties originally
promised by this approach. For instance, these implemen-
tations produce unstable and inﬁdelitous information. The
inﬁdelity of LIME was to be expected, since it is a model-
agnostic approach. In other words, the internals of the model
to be explained are not taken into account in the generated
information: it is based solely on an input-output analysis [44].

C. Assessability

It should be noted that assessability is a difﬁcult criterion
to test because there are so many desiderata that could, in
principle, be of interest. For this reason, we will only give
some general remarks, starting with LIME.

In addition to studies on the comprehensibility of the
information generated by LIME, Ribeiro et al. [33] have also
conducted studies on what participants could do with this
information. Among others, these studies showed that using
LIME enables individuals to identify incorrect classiﬁcations
and even helps them improve the classiﬁer. This suggests that
the provided information is useful for assessing at least some
desiderata (e.g., correctness, trustworthiness, and fairness).

Kim et al. [38] compare different approaches that generate
saliency maps (i.e., Vanilla Backpropagation, Guided Back-
propagation, Integrated Gradients, and Smoothgrad) to check
whether the information they generate enables one to evaluate
whether a given classiﬁcation makes sense.

To this end, they add a visible label, which varies across
trials, to the lower left corner of images. In one trial, the
label is constant across individual classes (e.g., all images
of cabs receive the label “cab”). In another trial, the label
sometimes deviates within a class (e.g., some cabs receive the
label “cucumber”). In yet another trial, the label is completely
random (e.g., each image of a cab receives a different label).
They found that in all trials, the image’s lower left corner was
highlighted by the approaches to a non-negligible extent.

It is important to point out that this does not preclude the ap-
proaches from being ﬁdelitous to the classiﬁcation algorithm,
since the lower left corner might actually be used by it (it
has a prominent label, after all), even if only insigniﬁcantly.
However, it does mean that it is difﬁcult to impossible to use
the generated information for meaningful assessments.

Coming to the next approaches, Rudin [45] argues that many
heatmaps (including those produced by Grad-CAM) do not
really allow for good assessments. The main reason for this
is that the heatmap for the most probable class (e.g., dog)
is often hardly distinguishable from that of a less probable
class (e.g., wolf). Accordingly, it is questionable whether the
obtained information allows for assessing certain desiderata.
Finally, let us talk about TCAV. In our opinion, TCAV
performs best when it comes to assessability. This is the case
because it allows for hand-crafted concepts to be reviewed.
In other words, a person using TCAV is not simply con-
fronted with an unchanging set of information, as with other
approaches, but can inquire after the information that is of
interest (e.g., whether gender played a role in classiﬁcation).

Approach
LIME
Vanilla Backpropagation
Guided Backpropagation
Integrated Gradients
Grad-CAM
TCAV

Comprehens.
+
−−
−
−
+
+

Fidelity
−−
+
−
?
+
+

Assessability
+
−
−
−
−
++

TABLE I: Contemporary explainability approaches evaluated.
+ indicates a positive evaluation of this criterion, − a negative one.

Summarizing the above, Table I offers an overview of
how we believe the explanatory information generated by the
discussed approaches fare in terms of our criteria. Overall, we
think that TCAV fares best, as it is comprehensible, ﬁdelitous,
and allows for a wide range of assessments.

IV. CONCLUSION AND FUTURE WORK
In this vision paper, we have motivated and argued for three
quality criteria of the information that systems should pro-
vide in order to be considered explainable: comprehensibility,
ﬁdelity, and assessability. By applying these criteria to the
outputs of some well-known approaches in the ﬁeld of XAI,
we have substantiated the reasonableness of these criteria.

While there are some evaluation methods for the compre-
hensibility of explanatory information (e.g., some items on the
explanation satisfaction scale by Hoffman et al. [15]), there
is much less literature on evaluation methods for ﬁdelity and
assessability. Thus, if one accepts our quality criteria, there is
much to be done about evaluation methods.

Among others, one should become clear about what ﬁdelity
actually means and formalize this. Moreover, it should be fur-
ther speciﬁed what it means for information to be assessable.
In general, we hope for more research on evaluation methods.
Finally, these are only three criteria among many others
that are reasonable (e.g., conciseness). By giving arguments
for these criteria, we have taken a principled approach and
given them a theoretical motivation and foundation. We hope
that more criteria will be underpinned in this way in the future.

ACKNOWLEDGMENTS
Work on this paper was funded by the Volkswagen Founda-
tion grant AZ 98514 “Explainable Intelligent Systems” (EIS)
and by the DFG grant 389792660 as part of TRR 248.

REFERENCES

[1] A. Holzinger, G. Langs, H. Denk, K. Zatloukal, and H. M¨uller, “Caus-
ability and explainability of artiﬁcial intelligence in medicine,” Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery,
vol. 9, no. 4, pp. 1–13, 2019.

[2] J. Dressel and H. Farid, “The accuracy, fairness, and limits of predicting

recidivism,” Science Advances, vol. 4, no. 1, pp. 1–5, 2018.

[3] A. Adadi and M. Berrada, “Peeking inside the black-box: A survey
on explainable artiﬁcial intelligence (XAI),” IEEE Access, vol. 6, pp.
52 138–52 160, 2018.

[4] L. Chazette, W. Brunotte, and T. Speith, “Exploring explainability:
A deﬁnition, a model, and a knowledge catalogue,” in IEEE 29th
International Requirements Engineering Conference (RE), J. Cleland-
Huang, A. Moreira, K. Schneider, and M. Vierhauser, Eds. Piscataway,
NJ, USA: IEEE, 2021, pp. 197–208.

[5] A. Barredo Arrieta, N. D´ıaz-Rodr´ıguez, J. Del Ser, A. Bennetot, S. Tabik,
A. Barbado, S. Garc´ıa, S. Gil-Lopez, D. Molina, B. Richard, R. Chatila,
and F. Herrera, “Explainable artiﬁcial intelligence (XAI): Concepts,
taxonomies, opportunities and challenges toward responsible AI,” In-
formation Fusion, vol. 58, pp. 82–115, 2020.

[6] M. Langer, D. Oster, T. Speith, H. Hermanns, L. K¨astner, E. Schmidt,
A. Sesing, and K. Baum, “What do we want from explainable artiﬁcial
intelligence (XAI)? – A stakeholder perspective on XAI and a conceptual
model guiding interdisciplinary XAI research,” Articiﬁal Intelligence,
vol. 296, 2021.

[7] L. Chazette and K. Schneider, “Explainability as a non-functional
requirement: challenges and recommendations,” Requirements Engineer-
ing, vol. 25, no. 4, pp. 493–514, 2020.

[8] M. Langer, K. Baum, K. Hartmann, S. Hessel, T. Speith, and J. Wahl,
“Explainability auditing for intelligent systems: A rationale for multi-
disciplinary perspectives,” in 29th IEEE International Requirements
Engineering Conference Workshops (REW), T. Yue and M. Mirakhorli,
Eds. Piscataway, NJ, USA: IEEE, 2021, pp. 164–168.

[9] M. A. K¨ohl, K. Baum, D. Bohlender, M. Langer, D. Oster, and
T. Speith, “Explainability as a non-functional requirement,” in IEEE
27th International Requirements Engineering Conference (RE), D. E.
Damian, A. Perini, and S. Lee, Eds.
Piscataway, NJ, USA: IEEE,
2019, pp. 363–368.

[10] M. Krishnan, “Against interpretability: A critical examination of the
interpretability problem in machine learning,” Philosophy & Technology,
vol. 33, no. 3, pp. 487–502, 2020.

[11] T. Miller, P. Howe, and L. Sonenberg, “Explainable AI: Beware of
inmates running the asylum. or: How I learnt to stop worrying and
love the social and behavioural sciences,” in Proceedings of the IJCAI
2017 Workshop on Explainable Artiﬁcial Intelligence (XAI), D. W. Aha,
T. Darrell, M. Pazzani, D. Reid, C. Sammut, and P. Stone, Eds. Santa
Clara County, CA, USA: IJCAI, 2017, pp. 36–42.

[12] G. Vilone and L. Longo, “Notions of explainability and evaluation
approaches for explainable artiﬁcial intelligence,” Information Fusion,
vol. 76, pp. 89–106, 2021.

[13] J. Zhou, A. H. Gandomi, F. Chen, and A. Holzinger, “Evaluating the
quality of machine learning explanations: A survey on methods and
metrics,” Electronics, vol. 10, no. 5, 2021.

[14] W. Brunotte, L. Chazette, V. Kl¨os, and T. Speith, “Quo vadis, ex-
plainability? – A research roadmap for explainability engineering,” in
Requirements Engineering: Foundation for Software Quality, V. Gervasi
and A. Vogelsang, Eds. Cham, CH: Springer International Publishing,
2022, pp. 26–32.

[15] R. R. Hoffman, S. T. Mueller, G. Klein, and J. Litman, “Metrics for
explainable AI: challenges and prospects,” 2018. [Online]. Available:
http://arxiv.org/abs/1812.04608

[16] K. Baum, M. A. K¨ohl, and E. Schmidt, “Two challenges for CI trustwor-
thiness and how to address them,” in Proceedings of the 1st Workshop on
Explainable Computational Intelligence (XCI 2017). Dundee, United
Kingdom: Association for Computational Linguistics, 2017, pp. 1–5.

[17] M. Bilgic and R. J. Mooney, “Explaining recommendations: Satisfaction
vs. promotion,” in Beyond Personalization Workshop, 2005, pp. 13–18.
[18] W. Pieters, “Explanation and trust: what to tell the user in security and
AI?” Ethics and Information Technology, vol. 13, no. 1, pp. 53–64, 2011.
[19] K. Baum, H. Hermanns, and T. Speith, “From machine ethics to ma-
chine explainability and back,” in International Symposium on Artiﬁcial
Intelligence and Mathematics (ISAIM), 2018, pp. 1–8.

[20] ——, “Towards a framework combining machine ethics and machine
explainability,” in Proceedings 3rd Workshop on formal reasoning about
Causation, Responsibility, and Explanations in Science and Technology,
(CREST), ser. EPTCS, vol. 286, 2018, pp. 34–49.

[21] A. Rosenfeld and A. Richardson, “Explainability in human–agent sys-
tems,” Autonomous Agents and Multi-Agent Systems, vol. 33, no. 6, pp.
673–705, 2019.

[22] A. Glass, D. L. McGuinness, and M. Wolverton, “Toward establishing
the 13th International
trust
Conference on Intelligent User Interfaces (IUI). New York, NY, USA:
ACM, 2008, pp. 227–236.

in adaptive agents,” in Proceedings of

[23] L. K¨astner, M. Langer, V. Lazar, A. Schom¨acker, T. Speith, and S. Sterz,
“On the relation of trust and explainability: Why to engineer for
trustworthiness,” in 29th IEEE International Requirements Engineering
Conference Workshops (REW), T. Yue and M. Mirakhorli, Eds. Piscat-
away, NJ, USA: IEEE, 2021, pp. 169–175.

[24] A. Jacovi, A. Marasovi´c, T. Miller, and Y. Goldberg, “Formalizing
trust in artiﬁcial intelligence: Prerequisites, causes and goals of human
trust in AI,” in Proceedings of the 2021 ACM Conference on Fairness,
Accountability, and Transparency (FAccT).
New York, NY, USA:
Association for Computing Machinery, 2021, pp. 624–635.

[25] J. L. Herlocker, J. A. Konstan, and J. Riedl, “Explaining collaborative
ﬁltering recommendations,” in Proceedings of the 2000 ACM Conference
on Computer Supported Cooperative Work (CSCW). New York, NY,
USA: Association for Computing Machinery, 2000, pp. 241–250.
[26] R. Sinha and K. Swearingen, “The role of transparency in recommender
systems,” in CHI’02 extended abstracts on Human factors in computing
systems. New York, Ny, USA: Association for Computing Machinery,
2002, pp. 830–831.

[27] P. Symeonidis, A. Nanopoulos, and Y. Manolopoulos, “MoviExplain: A
recommender system with explanations,” in Proceedings of the Third
ACM Conference on Recommender Systems (RecSys), L. D. Bergman,
A. Tuzhilin, R. D. Burke, A. Felfernig, and L. Schmidt-Thieme, Eds.
New York, NY, USA: Association for Computing Machinery, 2009, pp.
317–320.

[28] T. Hagendorff, “The ethics of AI ethics: An evaluation of guidelines,”

Minds and Machines, vol. 30, no. 1, pp. 99–120, 2020.

[29] S. Wachter, B. Mittelstadt, and L. Floridi, “Transparent, explainable, and
accountable AI for robotics,” Science Robotics, vol. 2, no. 6, 2017.
[30] K. Baum, S. Mantel, E. Schmidt, and T. Speith, “From responsibility
to reason-giving explainable artiﬁcial intelligence,” Philosophy & Tech-
nology, vol. 35, no. 1, pp. 1–30, 2022.

[31] A. Matthias, “The responsibility gap: Ascribing responsibility for the
actions of learning automata,” Ethics and information technology, vol. 6,
no. 3, pp. 175–183, 2004.

[32] P. B. de Laat, “Algorithmic decision-making based on machine learning
from big data: Can transparency restore accountability?” Philosophy &
Technology, vol. 31, no. 4, pp. 525–541, 2018.

[33] M. T. Ribeiro, S. Singh, and C. Guestrin, ““Why Should I Trust You?”:
Explaining the predictions of any classiﬁer,” in Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD), B. Krishnapuram, M. Shah, A. J. Smola, C. C.
Aggarwal, D. Shen, and R. Rastogi, Eds.
New York, NY, USA:
Association for Computing Machinery, 2016, pp. 1135–1144.

[34] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside convolutional
networks: Visualising image classiﬁcation models and saliency maps,”
2013. [Online]. Available: http://arxiv.org/abs/1312.6034

[35] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Riedmiller,
“Striving for simplicity: The all convolutional net,” in Proceedings of
the 3rd International Conference on Learning Representations (ICLR)
Workshop Track, Y. Bengio and Y. LeCun, Eds., 2015.

[36] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution
for deep networks,” in Proceedings of
the 34th International
Conference on Machine Learning (ICML), ser. Proceedings of Machine
Learning Research, T. Jebara, D. Precup, and Y. W. Teh, Eds.,
vol. 70.
Proceedings of Machine Learning Research Press, 2017,
pp. 3319–3328. [Online]. Available: http://proceedings.mlr.press/v70/
sundararajan17a.html

[37] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, “Grad-CAM: Visual explanations from deep networks via
gradient-based localization,” in Proceedings of the 16th IEEE Interna-
tional Conference on Computer Vision (ICCV), K. Ikeuchi, G. Medioni,
M. Pelillo, R. Cucchiara, Y. Matsushita, N. Sebe, and S. Soatto, Eds.
Piscataway, NJ, USA: IEEE, 2017, pp. 618–626.

[38] B. Kim, M. Wattenberg, J. Gilmer, C. J. Cai, J. Wexler, F. B.
Vi´egas, and R. Sayres, “Interpretability beyond feature attribution:
Quantitative testing with concept activation vectors (TCAV),” in
Proceedings of the 35th International Conference on Machine Learning
(ICML), ser. Proceedings of Machine Learning Research, F. Bach,
J. G. Dy, and A. Krause, Eds., vol. 80.
Proceedings of Machine
Learning Research Press, 2018, pp. 2668–2677. [Online]. Available:
http://proceedings.mlr.press/v80/kim18d.html

[39] L. H. Gilpin, C. Testart, N. Fruchter, and J. Adebayo, “Explaining
to society,” in NIPS Workshop on Ethical, Social
explanations
and Governance Issues in AI, 2018, pp. 1–6. [Online]. Available:
http://arxiv.org/abs/1901.06560

[40] A. Alqaraawi, M. Schuessler, P. Weiß, E. Costanza, and N. Berthouze,
“Evaluating saliency map explanations for convolutional neural net-
works: A user study,” in Proceedings of the 25th International Con-
ference on Intelligent User Interfaces (IUI). New York, NY, USA:
Association for Computing Machinery, 2020, pp. 275–285.

[41] C. Molnar,
for Making
British Columbia, Canada: Leanpub, 2019.
https://christophm.github.io/interpretable-ml-book/

Interpretable Machine
Box Models

Black

Learning

–
Explainable.

A Guide
Victoria,
[Online]. Available:

[42] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and
B. Kim, “Sanity checks for saliency maps,” 2020. [Online]. Available:
http://arxiv.org/abs/1810.03292

[43] E. G. Amparore, A. Perotti, and P. Bajardi, “To trust or not to trust an
explanation: Using LEAF to evaluate local linear XAI methods,” PeerJ
Computer Science, vol. 7, 2021.

[44] T. Speith, “A review of taxonomies of explainable artiﬁcial intelligence
(XAI) methods,” in Proceedings of
the 2022 ACM Conference on
Fairness, Accountability, and Transparency (FAccT). New York, NY,
USA: Association for Computing Machinery, 2022, p. 2239–2250.
[45] C. Rudin, “Stop explaining black box machine learning models for high
stakes decisions and use interpretable models instead,” Nature Machine
Intelligence, vol. 1, no. 5, pp. 206–215, 2019.

