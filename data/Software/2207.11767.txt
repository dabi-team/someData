Snapshot Metrics Are Not Enough: Analyzing
Software Repositories with Longitudinal Metrics

Nicholas Synovic∗, Matt Hyatt∗, Rohan Sethi∗, Sohini Thota∗, Shilpika‡, Allan J. Miller∗,
Wenxin Jiang†, Emmanuel S. Amobi∗, Austin Pinderski∗§, Konstantin L¨aufer∗, Nicholas J. Hayward∗,
Neil Klingensmith∗, James C. Davis†, George K. Thiruvathukal∗
∗ Loyola University Chicago, Computer Science Department
† Purdue University, Electrical and Computer Engineering Department
‡ University of California at Davis
§ Duke University

2
2
0
2

l
u
J

4
2

]
E
S
.
s
c
[

1
v
7
6
7
1
1
.
7
0
2
2
:
v
i
X
r
a

Abstract—Software metrics capture information about soft-
ware development processes and products. These metrics support
decision-making, e.g., in team management or dependency selec-
tion. However, existing metrics tools measure only a snapshot of
a software project. Little attention has been given to enabling
engineers to reason about metric trends over time—longitudinal
metrics that give insight about process, not just product. In this
work, we present PRIME (PRocess MEtrics), a tool for computing
and visualizing process metrics. The currently-supported metrics
include productivity, issue density, issue spoilage, and bus factor.
We illustrate the value of longitudinal data and conclude with
a research agenda. The tool’s demo video can be watched at
https://youtu.be/YigEHy3 JCo. The source code can be found at
github.com/SoftwareSystemsLaboratory/prime.

I. INTRODUCTION

An effective software engineering process is correlated
with high software quality [1]. Measurements of software
into software quality [2].
processes give engineers insight
Software metrics characterize the software engineering process
(e.g., time to ﬁx a defect) and the engineered product (e.g.,
cyclomatic complexity). Using software metrics, engineers and
managers improve products and assess the risks of external
software dependencies.

In our survey of existing tools for software metrics, we
found that typical metrics tools provide aggregate metrics, or
’snapshot metrics,’ rather than longitudinal metrics (§II). They
capture the current state of the software product and perhaps
some process metrics (e.g., age of current issues). While a
snapshot can be useful—for example, it can quickly reveal if
a project has no test suite—it does not provide a full picture of
the longitudinal evolution of a software project. We conjecture
that engineers will make different decisions when presented
with snapshot metrics compared to considering the trends in
a project’s metrics over time (§V).

To evaluate a development process, one needs to measure
the history of the code. The classic reference on software met-
rics [2] establishes that measurement needs to be related to a
time range and scale for a meaningful longitudinal assessment
of software quality [2]. Tools that measure quality need to
calculate both direct measurements and derived calculations at
consistent intervals to evaluate the process properly. Historical
trends can be visualized and utilized to quantify software

engineering decisions by collecting and reporting process
metrics at consistent intervals.

To support our investigation of this research question, we
present PRIME [3] (PRocess MEtrics): an open-source tool
that enables engineers and researchers to analyze software
projects with longitudinal metrics. PRIME uses a modular
Extract-Transform-Load pipeline architecture for ease of adop-
tion and extension. PRIME currently supports the following
metrics: code size, productivity, bus factor, issue count, issue
spoilage, and issue density.

We close by proposing three studies facilitated by PRIME:
(1) exploring engineers’ use of longitudinal metrics when
assessing their products; (2) exploring their use of longitudinal
metrics during dependency selection; and (3) analyzing the
software supply chain to identify potential weak links.

II. BACKGROUND AND RELATED WORK

Process metrics are critical for improving software quality
as agile repositories may eventually become more estab-
lished and require regular maintenance. Although numerous
efforts have focused on mining open-source repositories, the
current support for process metrics—and visualizing them
longitudinally—is mixed. In our survey of related efforts, we
encountered various tool types, including scorecards, frame-
works, dashboards, and platform monitors.

a

assign

Scorecards

source
risk
projects to assess security risks and project health. The
OSSF/SCORECARD [4] However,
they are computed as a
snapshot metric cannot easily express longitudinal effects.

score

open

for

Frameworks simplify the mining software repository (MSR)
development process. These are typically domain system lan-
guages (DSL) and libraries that researchers and engineers
integrate into their tools. ISHEPARD/PYDRILLER [5] and the
BOA [6] DSL meet this criteria. These frameworks allow for
MSR tools to be developed for the analysis of VCS, but they
are not MSR tools.

Dashboards are built into online VCS platforms designed to
provide visualizations of repository and issue tracker trends.
GitHub Insights [7] and GitLab Insights [8] provide longitudi-
nal metrics for hosted projects. However, these tools provide
limited insights when it comes to process metrics.

 
 
 
 
 
 
Platform monitors are third party analysis tools that com-
pute metrics for hosted packages. NPM [9] implements the
NPM Search [10] monitor for JavaScript packages. While the
GoReportCard [11] is a monitor for Go projects hosted on
GitHub. The GoReportCard tracks code metrics, while NPM
Search tracks process metrics regarding issue trackers.

Aside from dashboards, these tools compute process metrics
as snapshots and do not make longitudinal and trends visual-
ization easy for users.

III. ARCHITECTURE
PRIME is architected as an Extract, Transform, Load (ETL)
pipeline that utilizes modular components to compute and
visualize metrics, as shown in Figure1.

Fig. 1: System architecture of PRIME.

The ETL phases of the pipeline are each module or collec-
tion of modules. In addition, the extraction and transformation
stages of the pipeline store data in text-encoded JSON ﬁles. By
storing measurements in a ﬁle rather than in memory during
pipeline execution, PRIME can be integrated with existing
tools and pipelines.

PRIME extracts base measurements from a project’s ver-
sion control system (VCS) and issue tracker during the API
Phase. Here, using CLOC [12] and SLOCCOUNT [13], PRIME
measures each commit of a repository and measures the size
of the repository in lines of code (LOC), thousands of lines of
code (KLOC), and the size difference between each sequential
commit as the delta thousands of lines of code (DKLOC).
PRIME also extracts issue report metadata by utilizing the
REST API of a repository’s host issue tracker.

PRIME transforms the extracted base measurements into
derived metrics during its Metrics Phase. At the moment,
PRIME can compute the following metrics issue spoilage,
issue/defect density, productivity, and bus factor. Each metric
module takes in a text-encoded JSON ﬁle containing both or
either the commits or issue base measurements.

After both the API and Metrics phases, data is loaded
into either text-encoded JSON ﬁles or visualized with Mat-
PlotLib [14] in the Output Phase. PRIME can export the
JSON and visualization ﬁles to integrate with other pipelines.
Additionally, the visualizations can be customized using style
sheets and parameters for the major components,
thereby
allowing engineering teams to implement style standards for
their visualizations.

This architecture allows engineers to download and install
individual PRIME modules to measure speciﬁc attributes of

their software without waiting for measurements of unwanted
attributes. Furthermore, each phase of the pipeline is con-
ﬁgurable, reducing the time engineering teams need to post-
process the data to match their speciﬁc needs.

IV. METRICS IMPLEMENTED

PRIME computes two types of software metrics: (1) Direct
metrics, which are measurements of internal attributes of the
process, and (2) derived metrics, which are computed metrics
from two or more direct metrics.

A. Direct Metrics

Direct metrics are measurements of a particular attribute of
the process involving no other attribute [2]. These measure-
ments are the foundation for the more complex metrics that
PRIME computes.

1. Code Size: PRIME measures the size of a repository in
terms of the number of source lines of code normalized by
1000 reported as KLOC. Changes in the KLOC (DKLOC)
show the growth (or shrinkage) of a repository over time.

2. Developer Count: PRIME measures this metric as the
number of unique developers who contribute code to a repos-
itory within a time interval. By measuring developer count,
engineering teams can determine the amount of developer
support in contributing new code, maintaining existing code,
and resolving bugs.

3. Issue Count: PRIME measures this this metric as the
count of the number of open and closed issues reported in an
issue tracker, including feature requests, tasks, and bug reports,
in addition to potential and conﬁrmed defects. If an online
VCS has an issue tracker, this metric also reports the count of
open and closed pull requests.

B. Derived Metrics

Derived metrics are useful for analyzing the interactions
between direct metrics [2]. PRIME computes derived metrics
to analyze and subsequently visualize changes in the develop-
ment process of a software product.

1. Issue Density: This metric tracks a project’s total number
of issues normalized by project size. Because we are interested
in open-source repositories on GitHub, we use the more
general issue density rather than defect density, which refers
only to the ratio of bug count to repository size. A high
issue density, regardless of conﬁrmed defects, could signify an
unhealthy repository. For example, if there are many feature
requests that are never acted upon, then the development team
is not implementing the features that users want. This would be
a possible warning sign for poor customer support and, even-
tually, would lead to low customer or user satisfaction [15].

2. Issue Spoilage:

Issue spoilage is the weighted average
age of unresolved issues at a given time in the project timeline.
With further analysis, this metric calculates the age of issues
with respect to the project timeline to measure how quickly
a project’s team resolves issues. Issue spoilage can serve as a
gauge of customer support and the efﬁciency of software teams
in resolving issues. For instance, if issue spoilage increases in

Git RepoCommitsAPIIssuesAPIMetricsModulesJSONStorageData VizGitHub IssueTrackerAPI PhaseMetrics PhaseOutput Phase(a)

(c)

(e)

(g)

(b)

(d)

(f)

(h)

Fig. 2: This ﬁgure shows the PRIME tool’s output for each supported longitudinal derived process metric applied to several
sample projects. In the ﬁrst pair 2a-2b, 2a sees a signiﬁcant increase in issue density over its history with some recent
improvement. In contrast, 2b saw a similar trend with dramatic improvement in issue density. The second pair 2c-2d shows
two projects with contrasting trends in resolving issues. The third pair 2e-2f shows two projects with contrasting productivity
trends. The ﬁrst had a high level of productivity earlier in its history, while the second is showcases more recent evidence of
high productivity. The fourth pair 2g-2h shows two projects with contrasting bus factor. The bus factor metric is binned at 30
days to represent the number of core contributors per month.

a time interval, new issues are being created faster than the
team can resolve old ones. On the other hand, if the issue
spoilage drops in a time interval, the team resolves previous
issues faster than new ones are created.

3. Productivity: Productivity measures the rate at which
a development team adds KLOC within a time interval [2].
Healthy repositories will
typically have high productivity.
However, low productivity is not always a sign of a lack
of productiveness, as when efﬁcient development teams are
refactoring code KLOC may not change signiﬁcantly.

4. Bus Factor: Bus factor [16] is the number of developers
on a project team who would have to be hit by a bus to cause
the project to fail. This metric measures the employee turnover

risk of a project. However, as our work focuses on open-source
projects, we propose that this is a metric of the development
community’s interest. As interest in a project ﬂuctuates, we
propose that a longitudinal analysis of this metric is key to
assessing interest.

V. DEMONSTRATION

Figure 2 shows all four process metrics for several reposi-
tories over their entire project history. We chose projects from
the REPOREAPERS/REAPER data set [17] in pairs that showed
contrasting trends in their process metrics to demonstrate
possible insights from longitudinal analysis. We have orga-
nized this ﬁgure to demonstrate the potential for comparative

01000200030004000Days0510Issue Densityredis/redis Issue Density05001000150020002500Days24Issue Densitycoin-or/pulp Issue Density05001000150020002500300035004000Days050100150200Issue Spoilagefaker-js/faker Issue Spoilage05001000150020002500Days02040Issue Spoilageswisstph/openmalaria Issue Spoilage05001000150020002500Days0.00.10.20.30.4Productivitydronekit/dronekit-python Productivity010002000300040005000600070008000Days020406080Productivitycurl/curl Productivity010203040506070Days (Binned Every 30 Days)01234Bus Factorcoin-or/pulp Bus Factor020406080100120140160Days (Binned Every 30 Days)024Bus Factorredis/redis Bus Factorplanned studies by showing their ability to visualize long
and short-term trends via simple and intuitive charts. Fu-
ture development efforts will include expanding PRIME with
support for more process metrics, emphasizing comparative
visualizations, and expanding the number of data sources.
Future studies will build on this foundation to study the usage
of longitudinal metrics in practice, longitudinal metrics in
selecting dependencies, and the software supply chain.

REFERENCES

[1] I. Sommerville, “Software engineering 10th Edition,” ISBN-10, vol.

137035152, p. 18, 2015.

[2] N. Fenton and J. Bieman, Software Metrics: A Rigorous and Practical
Approach, Third Edition, 3rd ed. Boca Raton: CRC Press, Oct. 2014.
http://github.com/

[3] “PRiME.”

Available:

[Online].

SoftwareSystemsLaboratory/ssl-metrics/

[4] “Open Source Security Foundation Scorecard.” [Online]. Available:

https://github.com/ossf/scorecard

[5] D. Spadini, M. Aniche, and A. Bacchelli, “PyDriller: Python framework
for mining software repositories,” in Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. Lake Buena
Vista FL USA: ACM, Oct. 2018, pp. 908–911. [Online]. Available:
https://dl.acm.org/doi/10.1145/3236024.3264598

[6] R. Dyer, H. A. Nguyen, H. Rajan, and T. N. Nguyen, “Boa: A language
and infrastructure for analyzing ultra-large-scale software repositories,”
in 2013 35th International Conference on Software Engineering (ICSE),
May 2013, pp. 422–431, iSSN: 1558-1225.

[7] “GitHub

Insights.”

[Online].

Available:

https://docs.

github.com/en/issues/trying-out-the-new-projects-experience/
using-insights-with-projects

[8] “GitLab Insights.” [Online]. Available: https://docs.gitlab.com/ee/user/

project/insights/

[9] “NPM.” [Online]. Available: https://www.npmjs.com
[10] “NPM Search.” [Online]. Available: https://npms.io/
[11] “GoReportCard.” [Online]. Available: https://goreportcard.com/
[12] AlDanial, S. Snel, jolkdarr, C. Beckmann, MichaelDimmitt, Roman,
G. Chaves, J. Wilk, boB Rudis, asrmchq, A. Gough, J. Tang, J. Dursi,
achary, A. Ali, C. Ebberson, D. L., D. Ulrich, erkmos, L. Brinkhoff,
LoganDark, T. Irl¨ander, W. R¨osler, b1f6c1c4, S. HOUZ ´E, A. Ryan,
A. Shinn, A. Mastrean, A. Molinaro, and A. Turner, “Aldanial/cloc:
1.92,” Dec. 2021. [Online]. Available: https://doi.org/10.5281/zenodo.
5760077

[13] D. A. Wheeler, “Sloccount.” [Online]. Available: https://dwheeler.com/

sloccount/

[14] J. D. Hunter, “Matplotlib: A 2D Graphics Environment,” Computing in
Science Engineering, vol. 9, no. 3, pp. 90–95, May 2007, conference
Name: Computing in Science Engineering.

[15] W. Scherkenbach, The Deming Route to Quality and Productivity.

WWS, Inc., 2011.

[16] V. Cosentino, J. L. C. Izquierdo, and J. Cabot, “Assessing the bus factor
of Git repositories,” in 2015 IEEE 22nd International Conference on
Software Analysis, Evolution, and Reengineering (SANER), Mar. 2015,
pp. 499–503, iSSN: 1534-5351.

[17] N. Munaiah, S. Kroh, C. Cabrey, and M. Nagappan, “Curating GitHub
for engineered software projects,” Empirical Software Engineering,
[Online]. Available:
vol. 22, no. 6, pp. 3219–3253, Dec. 2017.
https://doi.org/10.1007/s10664-017-9512-6

analysis of process effectiveness, even among projects that
have a good score using existing scorecard apps. The addition
of process metrics clearly demonstrates that all of these
otherwise good projects may beneﬁt from further examining
their development process. This examination is especially
prudent when it comes to addressing issues (issue spoilage),
managing development while addressing issues (issue density),
ensuring appropriate resources (bus factor), or managing group
priorities to avoid team burnout (productivity).

VI. PLANNED STUDIES

We intend to expand on the ideas presented in this paper

through a few planned studies incorporating PRIME.

In the ﬁrst study, we pose the research question: How do
engineers use longitudinal process metrics during their devel-
opment process? We hypothesize that basic metrics are used in
many open-source projects today, but the use of longitudinal
metrics, particularly process metrics, is limited. To perform
this study, we will measure the number of process metrics
utilized and survey open-source developers on established
projects about why and how they use these metrics in their
development process.

In a second study, we pose the research question: Do
longitudinal metrics contribute to selecting dependencies in
software composition? Based on our survey of tools, we
hypothesize that engineers take little consideration of derived
longitudinal process metrics but will consider direct longi-
tudinal process metrics as those are more prevalent when
selecting dependencies for software development. To perform
this study, we intend to survey the current state of software
metrics tooling, and survey open-source engineers about their
utilization of longitudinal direct and derived process metrics
for dependency selection.

In our third study, we pose the research question: What
role can longitudinal process metrics play in analyzing depen-
dencies in open-source software? We hypothesize that many
projects are likely to depend on other projects that require
process improvement, e.g., a third-party library with a risky
bus factor (of one). To perform this study, we will examine
the dependencies of well-known projects by using our tools
to analyze each of the dependent projects for process-related
concerns.

VII. ACKNOWLEDGMENTS

Davis acknowledges support

from NSF OAC-2107230.
Thiruvathukal acknowledges support from NSF OAC-2107020
and NSF OAC-1445347. Davis and Thiruvathukal acknowl-
edge support from the Google TensorFlow Model Garden.

VIII. CONCLUSION

PRIME is an ongoing development effort to understand
process effectiveness beyond snapshots of process metrics
and support more longitudinal analysis and visualization. This
paper demonstrates working software to compute four process
metrics, which represent classical and modern/agile metrics.
We argue for the potential of these tools to support future

