2
2
0
2

l
u
J

2
1

]
x
e
-
p
e
h
[

1
v
2
0
6
5
0
.
7
0
2
2
:
v
i
X
r
a

PITT-PACC-2212

Nanosecond machine learning regression with deep
boosted decision trees in FPGA for high energy physics

B.T. Carlsona,b, Q. Bayerb, T.M. Hong∗b, and S.T. Rocheb

aDepartment of Physics, Westmont College
bDepartment of Physics and Astronomy, University of Pittsburgh

July 13, 2022

Abstract

We present a novel application of the machine learning / artiﬁcial intelligence method called
boosted decision trees to estimate physical quantities on ﬁeld programmable gate arrays (FPGA).
The software package fwXmachina features a new architecture called parallel decision paths
that allows for deep decision trees with arbitrary number of input variables. It also features
a new optimization scheme to use diﬀerent numbers of bits for each input variable, which
produces optimal physics results and ultraeﬃcient FPGA resource utilization. Problems in
high energy physics of proton collisions at the Large Hadron Collider (LHC) are considered.
Estimation of missing transverse momentum (𝐸 miss
) at the ﬁrst level trigger system at the High
T
Luminosity LHC (HL-LHC) experiments, with a simpliﬁed detector modeled by Delphes, is
used to benchmark and characterize the ﬁrmware performance. The ﬁrmware implementation
with a maximum depth of up to 10 using eight input variables of 16-bit precision gives a
latency value of O (10) ns, independent of the clock speed, and O (0.1)% of the available FPGA
resources without using digital signal processors.

Keywords: Data processing methods, Data reduction methods, Digital electronic circuits, Trigger
algorithms, and Trigger concepts and systems (hardware and software).

∗Corresponding author, tmhong@pitt.edu

1

 
 
 
 
 
 
Nanosecond ML regression with deep BDT in FPGA for HEP

Contents

1

Introduction

2 ML training

3 Nanosecond Optimization

3.1 Parallel decision path architecture
3.2 Variable number of bits .

. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

2

3

7
7
9

4 Firmware design

10
4.1 Deep Decision Tree Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.2 Checks and comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

5 Results

6 Conclusions

A Variable number of bits

References

1

Introduction

13

21

21

23

Fast, accurate estimation of physical quantities from detector information is indispensable at high
energy physics experiments, especially ones with high data-taking rates. At the Large Hadron
Collider (LHC) [1], for example, bunches of protons collide at 40 MHz that produce approximately
60 TB of data each second. At detectors such as those at the ATLAS [2] and CMS [3] experiments,
an online trigger system saves a tiny fraction of the LHC collision events—often those deemed
interesting—for follow-up analysis oﬄine. The data may include rare known physical processes
sensitive to the eﬀects from undiscovered laws of physics.

The trigger system must be capable of identifying such rare events to be saved oﬄine while
simultaneously rejecting the orders-of-magnitude larger background processes. ATLAS and CMS
experiments employ a two-level trigger system [4–6]. The ﬁrst level systems [7–11], called level-0
or level-1 depending on the context, have a latency requirement of a few microseconds per event to
decide whether to save or reject the event [12–14]. Because of this strict requirement, traditional
algorithms often utilize simpliﬁed estimates, relative to the oﬄine algorithms, at level-0 / level-1.
Variables such as energy and momentum of a ﬁnal state particle or invariant mass and angular
correlations of a group of ﬁnal state particles are typically considered.

2

Nanosecond ML regression with deep BDT in FPGA for HEP

For low latency implementation the ﬁrmware algorithms are used on custom electronic devices
such as those that employ ﬁeld programmable gate arrays (FPGA) and application-speciﬁc integrated
circuits (ASIC). A few thousandths of the incoming events pass the level-0 / level-1 algorithms, and
pass on a rate of O (100) kHz of data to the software-based trigger system called high level trigger
(HLT) or event ﬁlter (EF) depending on the context. The HLT / EF uses a farm of CPUs to further
evaluate events with more advanced algorithms within latency of a fraction of a millisecond.

Machine learning (ML) / artiﬁcial intelligence (AI) methods allow for improved estimates. In
[15], tau energy estimation [16], electron and

particular, regression models have been used for 𝐸 miss
photon energy [17, 18], pileup mitigation [19], and pion energy calibration [20].

T

In the past few years, progress in FPGA ﬁrmware design for signal-background classiﬁcation
have allowed for the use of more advanced algorithms using ML / AI at level-0 / level-1 [21–33],
typically relegated to the HLT / EF or oﬄine analysis. FPGA ﬁrmware design for regression estimates
have been developed for experiments at level-0 / level-1 [34–38].

In this paper, we present an alternative and novel FPGA ﬁrmware implementation of BDT
algorithms that allows for deeper decision trees. We expand on our previous BDT classiﬁcation
design [31] with 10 ns latency and sub-percent-level resource usage and use it as a blueprint for
regression. We utilize the new design to perform regression to estimate physical quantities, such as
𝐸 miss
T

, that may be of interest at the LHC.

The paper is organized as follows. Section 2 describes the regression problem and the machine
learning training and setup. Section 3 describes the nanosecond optimization, which is the pre-
processing step prior to the ﬁnal ﬁrmware design. Section 4 describes the ﬁrmware design. Section
5 presents the results. Section 6 concludes.

2 ML training

We consider the physics problem of the missing transverse momentum (𝐸 miss
trigger
serves as one of the primary means for identifying and saving high momentum particles that are
invisible to the detector, such as particles without strong or electromagnetic interactions. Such
particles include neutrinos as well as those from hypothesized “beyond” the standard model (BSM)
scenarios such as supersymmetry and dark matter (see, e.g., [39, 40] and the references therein).

). The 𝐸 miss

T

T

T

One particularly relevant example at ATLAS and CMS is the invisible decay of the Higgs bosons
that are produced in vector boson fusion (VBF) during proton collisions [41, 42]. In these cases the
distribution of 𝐸 miss
decays relatively quickly above around 70 GeV [43], which makes it critical to
maintain as low a trigger threshold as possible. The experimental challenge is that the calorimeter
noise level due to the high amount of “pileup” (cid:104)𝜇(cid:105), or simultaneous proton-proton collisions per
bunch crossing, drives the trigger thresholds to higher values to maintain a low event rate. The
pileup dependence of the 𝐸 miss
trigger has plagued ATLAS [44] and CMS [45, 46] during previous
runs, as the luminosity has rapidly increased and may continue to be problematic in future runs
without new techniques to reduce pileup. BDT regression is applied to estimate 𝐸 miss
deﬁned below.

T

T

3

Nanosecond ML regression with deep BDT in FPGA for HEP

T

The calculation of 𝐸 miss

at the level-0 / level-1 trigger system at the LHC is challenging because
of the constraints imposed by the collider. Examples of constraints include the strict timing between
proton bunch crossings and the availability of detector information. In 𝑝 𝑝 collisions, the vector
sum of momenta of the decay products in the plane normal to the beam should be zero. In some
collisions, however, momentum appears to not be conserved due to non-interacting decay products,
mismeasurement, or a combination of both, to produce a non-zero value of

MET𝐼 ≡ 𝐸 miss

T,𝐼 ≡

(cid:12)
(cid:12)
(cid:12)

∑︁

𝑖 ∈ 𝐼

(cid:174)𝑝T, 𝑖

,

(cid:12)
(cid:12)
(cid:12)

where the sum is over the decay products. For notational simplicity we also denote 𝐸 miss

T

as MET.

Samples

We created three samples [47], two samples with non-zero distributions (items A1 and A2 below)
and one sample with zero distribution (item B below) of true 𝐸 miss
at the generator level, i.e.,
METtruth. All three samples contain non-zero distributions of reconstructed MET.

T

A1. Sample of Higgs bosons produced by VBF, with the Higgs subsequently forced to decay to
neutrinos via 𝐻 → 𝑍 𝑍 ∗ → 4𝜈, producing non-zero METtruth. The VBF process produces at
least two highly energetic hadronic jets that are widely separated in polar angle with respect to
the collision axis. The Higgs decay to neutrinos ensures that the majority of the signal events
would have a high value of MET at the generator level relative to the B sample below.

A2. Sample of leptonic 𝑡¯𝑡 decays with non-zero METtruth.

B. Sample of the QCD multĳet process with no METtruth.

We note that samples A1 and B are merged into one training sample for the ML. However, for
the evaluation of ROC curves where “signal” and “background” needs to be deﬁned sample A1 is
taken as the signal and sample B as the background. An alternate choice of sample A2 as signal is
considered to validate the training, but we do not train with A2.

Each sample contains 100𝑘 events, produced using MadGraph5_aMC 2.9.5 [48] for the matrix
element calculation, Pythia8 [49] for the hadronization, and Delphes 3.5.0 [50, 51] for the detector
simulation and object reconstruction. The last step uses the ATLAS card with a mean pileup of
(cid:104)𝜇(cid:105) = 40.

For the input variables described next, the following objects are used. Tracks from charged
particles are reconstructed with a 𝑝T threshold of 100 MeV. Tracks have an eﬃciency applied as a
function of their 𝑝T and location in 𝜂. We use the default eﬃciency formulae from the ATLAS card
with pileup in Delphes. Neutral hadrons are summed into projective towers, which are referred to as
neutral hadron towers. Electrons and photons with a 𝑝T of at least 10 GeV are reconstructed with an

4

Nanosecond ML regression with deep BDT in FPGA for HEP

eﬃciency of 95% within |𝜂| < 1.5 and 85% within −2.5 < 𝜂 < − 1.5 and 1.5 < 𝜂 < 2.5. The muons
are the characterized similarly with the external 𝜂 boundary at 2.7 instead of 2.5. Hadronic jets
are reconstructed with the anti-𝑘𝑡 algorithm [52] with a radius parameter 𝑅 = 0.4 and a minimum
𝑝T of 20 GeV. For tracks, the charged hadron subtraction method is used to remove pileup before
jet reconstruction, while the neutral component applies a residual correction to the reconstructed
jet [53, 54]. The hadronic decays of tau leptons are reconstructed as jets. A calorimeter with
electromagnetic (EM) and hadronic (HAD) projective towers are formed, with ﬁxed 𝜂-𝜙 boundaries,
are distributed with a higher granularity in the central region |𝜂| < 2.6 than in the forward region,
inspired by the granularity implemented in ATLAS [55]. Electrons and photons deposit all of their
energy in the EM calorimeter, while the energy from hadrons is split between the EM and HAD
calorimeters. Subsequently, a detector energy resolution term is added separately for EM and HAD
towers, inspired by ref. [56, 57]. For the purpose of computing 𝐸 miss
, the energies from the EM and
HAD towers are summed into a single tower. The plots also show the tower energy distributions for
a typical signal and background event.

T

Variables

We consider several algorithms for computing MET, which are modeled after those in use by ATLAS
and CMS experiments during the Run 1 (2010–2012) and Run 2 (2015–2018) periods [58, 59], as
input variables to the regression BDT. We hypothesized that training a regression BDT on these
inputs, similar to a regression constructed by CMS [15], to target the generator-level MET, also
called METtruth, would yield a better approximate than any one input variable.

The input variables are four ﬂavors of 𝐸 miss

(METreco, METtowers, METtracks, METjets), one ﬂavor
of Σ𝐸T (SETjets), and three energy densities (𝜌fwd-A, 𝜌barrel, 𝜌fwd-B). The “reconstructed” 𝐸 miss
computed using objects with the 𝑝T thresholds described above (electrons, muons, photons, hadronic
jets) is METreco. The calorimeter tower-based 𝐸 miss
is METtowers. The track- and neutral hadron
tower-based 𝐸 miss
is METjets; this is sometimes referred to as MHT
in the literature. The jet-based Σ𝐸T is SETjets. The energy density 𝜌 is computed from calorimeter
towers in the forward regions and the barrel region of the calorimeter.

T
is METtracks. The jet-based 𝐸 miss

T

T

T

T

Table 1 lists the input, output, and target variables for the regression BDT. As mentioned
previously, the training is performed using one merged sample of sample A1 and sample B. The
input variable distributions are shown in ﬁgure 1. The plot on the left shows the various MET
variables whereas the plot on the right shows the SET variable.

ML conﬁguration

The parameters of the regression BDT is determined by the TMVA [60] library using the adaptive
boosting (AdaBoost) method of regression variance separation. It is conﬁgured with 40 trees at a
maximum depth of 5 for the ﬁgures featuring ML results and the physics performance.

5

Nanosecond ML regression with deep BDT in FPGA for HEP

Table 1: List of variables for the 𝐸 miss
the target variable. The output is the result of the regression.

T

estimation. The regression takes eight input variables and optimizes to

How used
Target
Input 1
2
(cid:48)(cid:48)
3

(cid:48)(cid:48)

(cid:48)(cid:48)

(cid:48)(cid:48)

(cid:48)(cid:48)

4
5
6
7
8
(cid:48)(cid:48)
Output

(cid:48)(cid:48)

Variable
METtruth
METreco
METtowers
METtracks

METjets
SETjets
𝜌fwd-A
𝜌barrel
𝜌fwd-B
OBDT

based on generator quantities due to, e.g., neutrinos
based on reconstructed objects, e.g., 𝑒, 𝜇, 𝛾, jets 𝑗
based on calorimeter towers
by Delphes based on charged tracks and neutral hadron

Description
𝐸 miss
T
𝐸 miss
T
𝐸 miss
T
𝐸 miss
T
towers
𝐸 miss
T
Σ𝐸T of reconstructed hadronic jets
Energy density for −4.9 < 𝜂 < −2.5
Energy density for |𝜂| < 2.5
Energy density for 2.5 < 𝜂 < 4.9
𝐸 miss
T

estimation from the regression

based on reconstructed hadronic jets

Figure 1: Input variable distributions. The reconstructed MET distributions (left) are given for background
process for various estimations that all peak around 30 GeV and the signal process that is broader that all
peaks similarly at higher values. The truth MET distributions are also shown for the background process
that peaks at 0 and for the signal process that is similar to the reconstructed values. The reconstructed Σ𝐸T
distributions (right) are shown for the background process and the signal process.

6

050100150200250300MET, see legend (GeV)00.050.10.15Events / 5 GeV (unit norm.) for bkgtruth MET for sigtruth METtracks METtowers METreco METjets METOF0.98backgroundprocesssignalprocess0100200300400500 (GeV)T EΣ00.050.10.15Events / 20 GeV (unit norm.)OFbackgroundprocesssignalprocessNanosecond ML regression with deep BDT in FPGA for HEP

As is common with ML applications in the trigger system, the training procedure is performed
one time to determine the ML parameters. The resulting setup is incorporated in the ﬁrmware design
to evaluate the collision events in real-time. The training step itself typically takes O (1) minute on a
modern oﬀ-the-shelf laptop, even for relatively deep networks.

The training step is performed on half of the events, both VBF Higgs signal and multĳet
background samples together, without pre-selection. The other half is used to test the result of the
training, discussed in the next section.

3 Nanosecond Optimization

Our BDT regression design is built on the framework of fwXmachina [31]. The workﬂow is
identical to the classiﬁcation design with many components (Tree Flattener, Forest Merger,
Score Finder, Score Normalizer, Tree Remover, and Cut Eraser) re-used for regression. The
new parallel decision path architecture is discussed in section 3.1.

The treatment of the number of bits for variables is diﬀerent than our previous design and is

speciﬁc to the regression problem. This is discussed later in 3.2 and in appendix A.

3.1 Parallel decision path architecture

The depth of the decision tree determines the granularity of the partitions of the input hyperspace:
the deeper the tree, the ﬁner the granularity. We present a new non-iterative design of the deep
decision tree with a complexity scaling with the depth and independent of the number of variables.
A comparison to our previous ﬂattened design is given at the end of the section.

Consider a decision tree of maximum depth 𝐷 with 𝑁bin = 𝐵 terminal nodes, or bins. By
construction, 𝐵 is at most 2𝐷. The set of bins are labeled as {𝑏0, 𝑏1, . . . , 𝑏𝐵−1}. Each terminal node
is logically connected to the initial node by a set of comparisons that deﬁne the intermediate nodes.
For example, a terminal node at depth 2 corresponds to a set of 2 comparisons. More generally,
a terminal node 𝑛 at depth 𝑑 corresponds to a set of 𝑑 comparisons, i.e., 𝑄𝑛 = {𝑞𝑛
𝑑−1},
0
where 𝑞𝑛
𝑖 is the result of the comparison at node 𝑖. For a given terminal node 𝑛, the logical and of
the associated set of comparisons is called a parallel decision path (PDP), i.e., PDP𝑛 = ∩ 𝑄𝑛. Figure
2 works out an example of one decision tree with two variables and maximum depth of three.

, . . . 𝑞𝑛

, 𝑞𝑛
1

The advantage of the deep decision tree is that the set of decision paths can be evaluated
simultaneously. A fully populated tree with 2𝐷 terminal notes has 2𝐷 PDP and requires 𝐷 · 2𝐷
comparisons. We ﬁnd that in our use cases, deep trees are often not fully populated, and thus scaling
is often softer than 2𝐷 (ﬁgure 3). A comparison to the 2𝐷 scaling for 𝐷 = 10 can be made with
the vertical axis on the right. We see that it ranges from 10 to 25% of a fully populated tree, with
density decreasing with 𝑁tree.

7

Nanosecond ML regression with deep BDT in FPGA for HEP

Figure 2: Deep decision tree with parallel decision path (PDP) structure. An example is shown in the leftmost
diagram for a decision tree using two variables (𝑥𝑎, 𝑥𝑏) with a depth of 3. The equivalent representation in
the two-dimensional 𝑥𝑎 vs. 𝑥𝑏 space is given in the middle. The PDP perspective is given on the right. The
table at the bottom lists the logical comparisons per PDP.

Figure 3: Average number of bins per tree (cid:104)𝑁𝑏𝑖𝑛(cid:105) vs. maximum tree depth 𝐷. The right vertical axis shows
the (cid:104)𝑁bin(cid:105) fraction with respect to the exponential scaling of 2𝐷 to compare the points at 𝐷 = 10.

8

b11b10b2qii: xb > 23qiii: xa > 40b0Decision tree structureDestination bin Depth iDepth iiDepth iiiDecision pathPath #b0not(qi)not(qii)N/Anot(qi) and not(qii)0b2qiN/AN/Aqi1b10not(qi)qiinot(qiii)not(qi) and qii and not(qiii)2b11not(qi)qiiqiiinot(qi) and qii and qiii3Worked example55xaxb23b0b2b102d plane: xa vs. xbb1140Decision pathsPath 0Path 1Path 2Path 3qi: xa > 550246810D, max. tree depth0100200300 > per treebin< N01020 (%)10Percentage of 2tree N 1 2 4 5 10 20 30 40Nanosecond ML regression with deep BDT in FPGA for HEP

Comparison to ﬂattened trees architecture

We compare the design in this paper with the non-iterative ﬂattened architecture of ref. [31].

One major limitation of the ﬂattened architecture is that the number of bins 𝐵 scales with product
of the number of cuts 𝑐𝑣 for each variable 𝑣 for 𝑉 total variables. For even relatively shallow trees,
this quickly results in a prohibitive number of bins. More formally, one can get an idea of the scaling
by considering the quantity 𝐵 = (cid:206)𝑉
𝑐𝑣 with some assumptions. Each 𝑐𝑣 is bounded by 2𝐷 since
(cid:205)𝑉
𝑐𝑣 = 2𝐷 − 1. So if we suppose that each variable has the same number of cuts then, we have
𝑐𝑣 ≈ 𝑐 where 𝑐 ≡ 2𝐷/𝑉. In this scenario, we have 𝐵 that scales exponentially with 𝐷 and 𝑉. In
comparison, PDP scales at worst exponentially with 𝐷 and independent of the number of variables
𝑉. Moreover, we saw a much softer scaling vs. 𝐷 in ﬁgure 3 in our examples.

𝑣=1

𝑣=1

3.2 Variable number of bits

Number of bits used per variable is an important aspect of resource optimization. There are
approaches that optimize prior to training [31] or during training [61]. We take the former approach
in optimally distributing the total numbers of bits to achieve the variable resolution at hand. Some
examples that are relevant to the problems at the LHC are discussed in appendix A.

In some applications of ML on FPGA, it is beneﬁcial if diﬀerent variables can be represented
with diﬀerent bit integer precision. For instance, consider a sample BDT that uses two variables:
one with many cuts requiring exact precision, and one with few cuts requiring less precision. To
minimize resource usage on the board, it is useful if the variable with fewer cuts can be expressed
with fewer bits. In ref. [31], we introduced the ability to use a diﬀerent precision for the input
variables than the BDT output score. Here, we add support for diﬀerent variable precision for each
input variable. The bit-optimized conﬁguration gives the same timing results, but with ultraeﬃcient
FPGA implementation, with respect to the unoptimized conﬁguration.

We present the following comparison to illustrate the diﬀerence. In the unoptimized conﬁguration
we use a total of 144 bits with 16 bits for all input and output variables. In the optimized conﬁguration
we use a total of 87 bits with either 12 or 5 bits per variable depending on the variable. In terms
of physics results, both conﬁgurations yield the same area under the ROC curve. In terms of
FPGA cost, both conﬁgurations result in the same latency and interval measurements. However,
the resource usage in look up tables (LUT) and ﬂip ﬂops (FF) are reduced by a factor of about 4.
Table 2 summarizes the results.

We expect similar signiﬁcant reduction in resource usage when applied to the classiﬁcation

problems of ref. [31].

There are subtleties and technical challenges for the implementation. The discussion can be

found in appendix A.

9

Nanosecond ML regression with deep BDT in FPGA for HEP

4 Firmware design

The ﬁrmware implementation of regression is built on fwXmachina [31]. The only substantial
diﬀerence of the existing pieces is in the Score Processor for the gradient boost (GradBoost).1 A
new Engine is introduced next for the parallel decision paths. Firmware veriﬁcation and validation
is given in section 4.2.

4.1 Deep Decision Tree Engine

Deep decision tree is implemented with a Deep Decision Tree Engine (DDTE); see ﬁgure 4.
DDTE makes 𝐵 duplicates of the set of input variables and feeds each one to the One Hot Decision
Path (OHDP) to process each parallel decision path. The OHDP results are collected by a look up
table (LUT) that converts the one-hot vector into an regression estimate 𝑂BDT.

Figure 4: The block diagram of the Deep Decision Tree Engine (DDTE). The input x is a vector of 𝑉
variables and the output 𝑂BDT is the regression estimate. For each 𝐵 bin (terminal node) of the decision tree,
there is a corresponding One Hot Decision Path (OHDP).

Each OHDP implements the parallel decision path logic with a pair of comparisons for each
variable, comparing to a user-conﬁgured upper bound and a lower bound; see ﬁgure 5. The output
of each comparison feeds in to one and operator to yield a binary result.

1This component is intended to normalize the score provided by the BDT into a more useful form. The transformation
for AdaBoost is still trivial, so this went unchanged. The GradBoost option now adds a supplied constant to the sum of
the individual scores. The processor for GradBoost previously applied a piecewise approximation of the tanh function
to the sum of the individual scores; however, this normalization method is no longer desirable. If no constant is provided
when using GradBoost, then the constant is set to 0 by default.

10

One Hot Decision PathOHDP0 x...xxDeep Decision Tree Engine (DDTE)OHDP1OHDPB-1OBDTfor b = 0 .. B-1 terminal binsxO0O1OB-1 in0 in1 inB-1...outLUTactive input array → output arraybus tapNanosecond ML regression with deep BDT in FPGA for HEP

Figure 5: The block diagram of the One Hot Decision Path (OHDP). Each variable 𝑥𝑣 is compared to pair of
“low” and “high” constants 𝛼 to check if it is within the range deﬁned by the constants, i.e., 𝛼low < 𝑥𝑣 < 𝛼high.
The collection of pairs of comparisons for each variable deﬁnes a particular parallel decision path (PDP). The
output of OHDP is a boolean and is one-hot encoded.

11

demuxOne Hot Decision Path (OHDP)andx0x1xV-1>αlow<αhighx0x0>βlow<βhighx1x1>γlow<γhighxV-1xV-1... Not  explicitly used,may be usedindirectlyLUT / BRAMOPDPxfor v = 0 .. V-1 input variablesNanosecond ML regression with deep BDT in FPGA for HEP

4.2 Checks and comparisons

We report the results for the benchmark point of 40 trees and a maximum depth of 5 in this section.

Veriﬁcation and validation

For veriﬁcation of the ﬁrmware against physical FPGA, an RTL design is programmed onto the Xilinx
Virtex UltraScale+ FPGA VCU118 Evaluation Kit and the Xilinx Artix-7 FPGA on Zynq-7020
System on Chip (SoC). The Ultrascale+ is run on three clock speeds—320 MHz, 200 MHz, and
100 MHz—while the Artix-7 is run on 100 MHz. In all scenarios, a test vector is evaluated on the
FPGA and the resulting output is compared to the output received from co-simulation. No diﬀerence
is observed.

Our ﬁrmware design produces latency values that are independent of the choice of clock speed,
as was also the case for ref. [31]. The same conﬁguration is executed using the three clock speeds
noted above on the Ultrascale+. The results produced latency values of 6 clock ticks (18.75 ns), 4
clock ticks (20 ns), and 2 clock ticks (20 ns), respectively. They all produce the same latency values
of about 20 ns within the resolution of the clock period. The interval remains at one clock tick for
all clock speeds.

The latency for the Artix-7 was about 4-fold higher, in absolute terms using the same clock speed,
than for the Ultrascale+ as was seen previously for our ﬂattened tree architecture [31]. Resource
cost was generally higher as well on the Artix-7 with over 6-fold the resource usage compared to the
Ultrascale+. As is seen for the Ultrascale+ results, the interval is one clock tick and no DSP is used.
Validation of the ﬁrmware against software simulation is done with 100 000 input test vectors for
around 200 diﬀerent BDT conﬁgurations. Inputs are processed through software simulation as well
as co-simulation. Other than rounding discrepancies, no diﬀerence is observed for the BDT output.

Comparison of estimated and actual FPGA cost

We compare the actual FPGA resource utilization and timing results to the estimated values we
obtain from C synthesis. The actual values can be measured in Xilinx Vivado after the RTL design
is generated and uploaded to the FPGA. Timing is measured using the ILA, while the resource
utilization is directly reported by the software. We refer to ﬁgure 24 in ref. [31] for the illustration
of the deﬁnitions.

For the timing, we see no diﬀerence between the estimated and actual values in all of the

conﬁgurations that we considered in this paper.

For the resources, the resource utilization on the FPGA was consistently lower than C synthesis
values. Figure 6 shows the LUT and the FF comparisons for the conﬁguration using 𝑁tree = 10
and otherwise the same setup as in table 3. It is notable that the actual values are lower than the
estimated values by a factor of about 20 for the LUT and about 5 for the FF. We repeated the exercise
for 𝑁tree = 20 and the scale factors are about 10 for the LUT and about 2.5 for the FF. The actual
values are scaled up by the stated factor, which shows that the scaling vs. tree depth follows the trend

12

Nanosecond ML regression with deep BDT in FPGA for HEP

presented by the estimated curve. For this reason we report the actual FPGA resource utilization,
rather than the estimated version, in section 5.

Figure 6: Comparison of estimated usage and actual usage for LUT (left column) and FF (right column) for
𝑁tree = 10 (top row). and 𝑁tree = 20 (bottom row). Estimated values are obtained with HLS C synthesis and
the actual values are obtained by RTL synthesis and implementation.

5 Results

We present the physics performance followed by the FPGA cost (timing and resource utilization) for
the 𝐸 miss

problem.

T

Physics performance

Physics performance is evaluated with ROC curves, turn-on curves, and MET resolution.

13

0246810D, max. tree depth020000400006000080000LUT usage01234567LUT usage (%)tree N 10 (estimated) 10 (actual) 20)× 10 (actual 0246810D, max. tree depth05000100001500020000FF usage00.20.40.60.8FF usage (%)tree N 10 (estimated) 10 (actual) 5)× 10 (actual 0246810D, max. tree depth020000400006000080000LUT usage01234567LUT usage (%)tree N 20 (estimated) 20 (actual) 10)× 20 (actual 0246810D, max. tree depth05000100001500020000FF usage00.20.40.60.8FF usage (%)tree N 20 (estimated) 20 (actual) 2.5)× 20 (actual Nanosecond ML regression with deep BDT in FPGA for HEP

ROC curves are shown in left plot of ﬁgure 7, showing the background rejection factor vs. signal
acceptance. The former is deﬁned as the inverse of the background eﬃciency 1/𝜀𝐵, where 𝜀𝐵 is the
false positive rate (FPR) or type I error (𝛼). The latter is deﬁned as the signal eﬃciency 𝜀𝑆, also
called the true positive rate (TPR). The eﬃciencies for category 𝑐 = 𝑆, 𝐵 are deﬁned as the ratio
of the number of events category 𝑐 passing the MET threshold with respect to the total number of
events of category 𝑐, i.e., 𝜀𝑐 ≡ 𝑁 pass
. A scan of the MET thresholds correspond to a point in
the (𝜀𝑆, 1/𝜀𝐵) plane; the collection of points deﬁne the ROC curve shown in the ﬁgure.

/𝑁 total
𝑐

𝑐

The right plot of ﬁgure 7 shows the ratio of background rejection factors with respect to METtowers.
Plots in ﬁgure 7 impose a selection of METtruth > 100 GeV for the signal events to better illustrate the
impact of the BDT for larger values of background rejection, closer to a more realistic experimental
threshold.

Figure 7: ROC curves of background rejection factor vs. signal acceptance (left) and the ratio of rejection
factors with respect to METtowers vs. signal acceptance (right). The background and signal are trained and
evaluated using the QCD multĳet and the VBF Higgs to 4𝜈 sample. The scan is done over the full MET range
from 0 to 1.5 TeV. The order of the legend follows the curves. A subset of events for which the signal sample
has the pre-selection METtruth > 100 GeV applied is shown. The BDT values are using 16-bits.

For an operating point on the ROC curve—say, at a background eﬃciency of 𝜀𝐵 = 10−3—the
signal eﬃciency can be read oﬀ of ﬁgure 7. For a signal eﬃciency value of, say 85%, the background
rejection is approximately 150% lower than the same eﬃciency computed using MET formed only
with calorimeter towers. More information can be obtained at that operating point by scanning the
METtruth threshold given an algorithm on the signal process. This produces the so-called turn-on
curve in the left plot of ﬁgure 8.

The turn-on curve is evaluated by identifying a selection of the MET variable (e.g., OBDT >
75 GeV), and for each bin of METtruth evaluate the fraction of events that satisfy the selection. The
BDT outperforms other curves by reaching full eﬃciency the quickest, i.e., the turn-on curve is
“sharper.”

14

00.20.40.60.81)SεSignal efficiency (110210310410)BεBackground rejection (1/BDTOtracksMETtowersMETobjectsMETjetsMHT > 100 GeV only for signaltruthReq. MET Better1−101−10×21−10×31)SεSignal efficiency (0123Ratios of rejection wrt towersBDTOtracksMETtowersMETobjectsMETjetsMHT > 100 GeV only for signaltruthReq. MET Nanosecond ML regression with deep BDT in FPGA for HEP

In order to verify the performance of an alternate sample with non-zero METtruth with a larger
jet multiplicity, including jets in the central region unlike for VBF processes, we consider leptonic 𝑡¯𝑡
decays in the right plot of ﬁgure 8. The same BDT trained on the merged sample of sample A1 and
B is used to evaluate sample A2 for 𝑡¯𝑡. We see that that turn-on curve for the BDT is sharper than
the input MET variables.

Figure 8: Eﬃciency turn-on curves for the VBF Higgs sample (left) and 𝑡¯𝑡 samples (right) as signal on the
vertical axis and QCD multĳet for background on the horizontal axis. The signal eﬃciency of the BDT is
improved for both VBF Higgs and 𝑡¯𝑡 samples as signal, demonstrating that the regression is robust for a
wide range of topologies.The threshold corresponding to the operating point of background eﬃciency of
𝜀𝐵 = 10−3 is chosen. For each histogram a line is drawn between the data points as a visual guide.

Finally, we consider MET resolution. If the algorithm rejects more background events while
retaining a similar amount of signal events, the BDT regression is worth pursuing. We will see
that this is the case. The MET distribution for events without true 𝐸 miss
, i.e., background events,
is shown in the left plot of ﬁgure 9. As the training sample includes background events without
true 𝐸 miss
, these events tend to be reconstructed with low values of MET by the regression model,
as expected. To highlight the improved rejection, subset of events with non-zero reconstructed
MET, taken to be METtowers > 60 GeV are selected. For this subset of events, the BDT estimate
outperforms the input MET variables for accurately estimating the null MET value.

T

T

The MET resolution for events with true 𝐸 miss

, i.e., signal events, is shown in the right plot of
ﬁgure 9. For the subset of events with non-zero reconstructed MET in the range at which a MET cut
might be placed in a trigger system, taken here to be 150 < METtruth < 200 GeV, the BDT estimate
performs comparably to the input MET variables.

T

We now discuss the trade-oﬀ between physics and engineering performance. As can be seen in
ﬁgure 10 the area under the ROC curve (AUC) is plotted vs. maximum tree depth 𝐷 and number of
bits for input variables.2 An AUC of 0.5 corresponds to the worst possible performance, while an

2AUC is deﬁned to be the area under the curve when plotting 𝜀𝑆 vs. 𝜀𝐵.

15

050100150200250300 (GeV)truthMET00.20.40.60.81SεSignal efficiency,  (16 bits)BDT Otracks METtowers METreco METjets MET050100150200250300 (GeV)truthMET00.20.40.60.81SεSignal efficiency,  (16 bits)BDT Otracks METtowers METreco METjets METt+QCD, eval. on tν4Train on VBF H Nanosecond ML regression with deep BDT in FPGA for HEP

Table 2: Comparison of our benchmark conﬁguration (details in table 3) and the bit optimized conﬁguration.
The setup is given in the top; the physics performance in the middle; the FPGA cost in the bottom.

Parameter
Setup: no. of bits

Benchmark (table 3)

Bit optimized

Ratio

MET𝑘, where 𝑘 = reco, towers, tracks, jets
SETjets
𝜌𝑘, where 𝑘 = fwd-A, barrel, fwd-B
OBDT
All variables

16 bits each
16 bits
16 bits each
16 bits
144 bits in total

12 bits each
12 bits
5 bits each
12 bits
87 bits in total

Physics performance

Area under the ROC curve2 (AUC)

0.90

0.90

FPGA cost for 40 trees, 5 depth

Latency
Interval
Look up tables
Flip ﬂops
Block RAM
Ultra RAM
Digital signal processors

6 clock ticks
1 clock tick
1675
1460
0
0
0

6 clock ticks
1 clock tick
374
352
0
0
0

1.3
1.3
3.2
1.3
1.7

1

1
1
4.5
4.1
Same
Same
Same

T

distributions for background events (left) and 𝐸 miss

Figure 9: 𝐸 miss
resolution for signal events (right).
Background events (left) with METtowers > 60 GeV shows how the higher MET values get remapped. Signal
events (right) with 100 < METtruth < 200 GeV shows the resolution with METtruth values of interest. The
input variable distributions are shown using ﬂoating point values. The output estimate OBDT distributions are
shown using 16-bit as is done in ﬁrmware.

T

16

020406080100MET, see legend (GeV)00.10.20.3Events / 5 GeV (unit norm.) (16 bits)BDT Otracks METtowers METreco METjets MET> 60 GeVtowersReq. MET  Std. dev.Mean,      28, 25 GeV     68, 20 GeV     -     48, 28 GeV     49, 28 GeV     OF00.511.52truth / METsee legendRatio of MET00.10.2Events / 0.1 (unit norm.)BDT Otracks METtowers METreco METjets MET<200GeVtruth150<MET /meanσ0.200.200.210.220.22Nanosecond ML regression with deep BDT in FPGA for HEP

AUC of 1.0 corresponds to a perfect discrimination between signal and background. For the former,
the quick rise is followed by a plateau starting around 𝐷 = 7. For the latter, the plateau begins at
around 5.

Figure 10: Physics performance vs. maximum tree depth 𝐷 (left) and vs. number of bits for input variables
(right). The performance is given by area under the ROC curve2 (AUC) for each conﬁguration.

FPGA cost and parameter scanning

FPGA cost denotes the timing results, consisting of latency and interval, and resource usage.

Starting from our benchmark conﬁgurations listed in table 3, BDT parameters are varied, one at
a time, to investigate cost dependencies on tuneable parameters. We focus on the maximum tree
depth 𝐷, and also show dependencies on the number of bits. We note that the maximum BDT
complexity scales with 2𝐷, but, as discussed previously in section 3, ﬁgure 3 showed that the scaling
is much softer, especially at high 𝐷 values.

The resource usage and latency scaling follows a similar pattern. The number of look-up tables
and ﬂip-ﬂops used vs. 𝐷 for several values of 𝑁trees is shown in ﬁgure 11. DSP usage is at 0 for all
conﬁgurations and BRAM is minimal as shown in ﬁgure 12.

The latency dependency on the maximum tree depth shows a similar pattern in ﬁgure 13, Notably,
this ﬁgure also demonstrates that the number of trees does not seem to have a large signiﬁcant
impact on the latency, with conﬁgurations from 1 to 40 trees all remaining within a single clock tick
of each other at each maximum depth. As in our previous ﬁrmware design, the interval is only one
clock tick for all conﬁgurations.

The latency dependency on the number of bits used in the input variable representations is shown
in ﬁgure 14. Less precise variable representations result in lower latency. As is shown previously in
ref. [31], less precise variable representations often result in degraded ML performance. Appendix A
includes a more in-depth discussion of dependency on integer precision and this trade-oﬀ.

17

12345678910D, max. tree depth0.70.80.91Area under ROC curve (AUC)tree N 401234567891020No. of bits for all input variables0.40.60.81Area under ROC curve (AUC)tree N 4010121416AUC for variable bitsNanosecond ML regression with deep BDT in FPGA for HEP

Table 3: Benchmark conﬁguration and the FPGA cost. Three groups of information are given. The top-most
group deﬁnes the FPGA setup. The second group deﬁnes the ML training used for the MET problem and the
Nanosecond Optimization. The third group gives the actual results measured on the FPGA for four tree-depth
combinations of 40-5, 40-6, 20-7, and 10-8.

Parameter
FPGA setup

Chip family
Chip model
Vivado version
Synthesis type
HLS or RTL
Clock speed

Value

Comments

Xilinx Virtex Ultrascale+
xcvu9p-ﬂga2104-2L-e
2019.2
C synthesis
HLS
320 MHz

HLS interface pragma: None
Clock period is 3.125 ns

ML training conﬁguration & Nanosecond Optimization conﬁguration

ML training method
No. of input variables
Bin Engine type
No. of bits for all variables
FPGA cost for 40 trees, 5 depth

Latency
Look up tables
Flip ﬂops

FPGA cost for 40 trees, 6 depth

Latency
Look up tables
Flip ﬂops

FPGA cost for 20 trees, 7 depth

Latency
Look up tables
Flip ﬂops
Block RAM

FPGA cost for 10 trees, 8 depth

Latency
Look up tables
Flip ﬂops
Block RAM

Boosted decision tree
8
Deep Decision Tree Engine (DDTE)
16 bits for each

binary integers

Regression, Adaptive boosting

6 clock ticks
1675 out of 1 182 240
1460 out of 2 364 480

9 clock ticks
4566 out of 1 182 240
2516 out of 2 364 480

15 clock ticks
4568 out of 1 182 240
2697 out of 2 364 480
4.5 out of 4320

21 clock ticks
2556 out of 1 182 240
2299 out of 2 364 480
5 out of 4320

18.75 ns
0.1% of available
< 0.1% of available

28.125 ns
0.4% of available
0.1% of available

46.875 ns
0.4% of available
0.1% of available
0.1% of available

65.625 ns
0.2% of available
0.1% of available
0.1% of available

Common values for the above conﬁgurations

Interval
Block RAM
Ultra RAM
Digital signal processors

1 clock tick
0 out of 4320
0 out of 960
0 out of 6840

3.125 ns
If not listed above
Same for all trees and all depth
Same for all trees and all depth

18

Nanosecond ML regression with deep BDT in FPGA for HEP

Figure 11: Actual LUT usage (left) and actual FF usage (right) as a function of the maximum depth. Absolute
usage is shown on the left axis and percentage of our FPGA resources is shown on the right axis, both using
the setup in table 3.

Figure 12: Actual DSP usage (left) and actual BRAM usage (right) as a function of the maximum depth.
Absolute usage is shown on the left axis and percentage of our FPGA resources is shown on the right axis,
both using the setup in table 3. No DSP usage is seen.

19

0246810D, max. tree depth02000400060008000Actual LUT usage00.10.20.30.40.50.6LUT usage (% of xcvu9p)tree N 1 2 4 5 10 20 30 400246810D, max. tree depth0100020003000400050006000Actual FF usage00.10.2FF usage (% of xcvu9p)tree N 1 2 4 5 10 20 30 400246810D, max. tree depth0246810Actual DSP usage00.050.1DSP usage (% of xcvu9p)tree N 1 2 4 5 10 20 30 400246810D, max. tree depth051015Actual BRAM usage00.10.20.3BRAM usage (% of xcvu9p)tree N 1 2 4 5 10 20 30 40Nanosecond ML regression with deep BDT in FPGA for HEP

Figure 13: Algorithm latency (left) and interval (right) as a function of the maximum depth. Clock ticks are
shown on the left axis and nanoseconds are shown on the right axis, both using 320 MHz clock speed. Data
series for the a given number of trees are connected. The interval is unity for all data points. Eight input
variables of 16-bit precision are used.

Figure 14: Algorithm latency as a function of the number of bits of the input variables. Clock ticks are shown
on the left axis and nanoseconds are shown on the right axis, both using 320 MHz clock speed. For the other
parameters, Eight input variables of 40 trees with a maximum depth of 5 are used.

20

0246810D, max. tree depth0102030Latency (320 MHz clock ticks)020406080100Latency (ns)tree N 1 2 4 5 10 20 30 400246810D, max. tree depth012Interval (320 MHz clock ticks)0123456Interval (ns)tree N 1 2 4 5 10 20 30 40024681012141618B, no. of bits for input variables0246810Latency (320 MHz clock ticks)051015202530Latency (ns)Max. depth D = 5 = 40treeN = 8varNNanosecond ML regression with deep BDT in FPGA for HEP

6 Conclusions

We present a novel implementation of boosted decision trees on FPGA within the fwXmachina
framework [31] that allows for deep decision trees. In this paper, we demonstrate the use case for
the deep tree structure for regression problems. The new ﬁrmware design makes use of parallel
decision paths (PDP) to allow for deeper trees as well as arbitrarily many variables: two limitations
of the ﬂattened decision tree structure of ref. [31]. Finally, support for varying bit integer precision
per variable is implemented, allowing for further resource usage optimization.

The performance is shown for the problem 𝐸 miss

estimation. It is shown that combining several
T
conventional MET calculations with a regression BDT provided a better signal eﬃciency and
background rejection for reasonable operating point for level-0 / level-1 trigger systems at the LHC.
FPGA implementation details are provided for hundreds of conﬁgurations, We ﬁnd that latency
results are O (10) ns. The resource usage is O (0.1)% of those available on our FPGA with the
important exception that no DSP resources are used. Results for various conﬁgurations by scanning
the BDT parameters—such as the number of trees, the maximum tree depth, and the number of
bits—show that our implementation can be adapted for a variety of use cases

Acknowledgments

We thank Stephen Racz for the initial eﬀort of the project. We thank Gracie Jane Gollinger for
computing infrastructure support. We thank Joerg Stelzer for the TMVA discussions. We thank
Pavel Serhiayenka and Kushal Parekh for their assistance with FPGA data collection. We thank
the University of Pittsburgh for the support of this project, especially for Brandon Eubanks and
Emre Ercikti from the Electronics Shop of the Shared Research Support Services of the Dietrich
School of Arts and Sciences. TMH was supported by the US Department of Energy [award no.
DE-SC0007914]. STR was supported by the Emil Sanielevici Scholarship. Patent pending.

A Variable number of bits

One subtle diﬀerence between the classiﬁcation and regression implementation is the eﬀect of bit
integers on output scores. Due to the advantage of pre-computing and pre-normalizing bin values to
their trees’ boost-weights, it is important that the conversion of ﬂoating point target variable outputs
to integers respects addition, i.e., 𝑓 (𝑥1 + 𝑥2) = 𝑓 (𝑥1) + 𝑓 (𝑥2). This requirement only applies to the
target variable, not the input ones. This is discussed in ref. [31].

For our classiﬁcation application in ref. [31], the mapping is relatively straight forward. The
purity values ranging from [0, 1] could be scaled to a 𝐵-bit integer by multiplying by 2𝐵 − 1 to
achieve a range of [0, 2𝐵 − 1]. The same scaling was applied to Yes/No Leaf outputs of [−1, 1] to
achieve the range of [−(2𝐵 − 1), 2𝐵 − 1]. Such simple scaling is closed under addition, allowing for

21

Nanosecond ML regression with deep BDT in FPGA for HEP

the summation of output scores from a forest of trees.

However, in regression we are not promised such neat outputs. In principle, the target variable
can be bounded between any two values or even be unbounded. Luckily in high energy physics,
physical variables are often either conveniently bounded below by zero or be symmetrical. Energy
can range from [0, 𝐸max] to be scaled to [0, 2𝐵 − 1] as before. Momentum can range [−𝑝max, 𝑝max]
to be scaled to [−(2𝐵 − 1), 2𝐵 − 1]. If a given variable are not bounded or symmetric, we adjust the
range accordingly; see table 4 for examples.

Table 4: Bit integer conversion methods used for the target training variable. Four representative examples
are given.

Adjustment method
Positive
Positive
Symmetric
Symmetric

Initial range
[0, 22]
[50, 450]
[−120, 70]
[−50, 70]

Adjusted range
[0, 22]
[0, 450]
[−120, 120]
[−70, 70]

Adjusted bit range
[0, 2𝐵 − 1]
[0, 2𝐵 − 1]
[−(2𝐵 − 1), 2𝐵 − 1]
[−(2𝐵 − 1), 2𝐵 − 1]

In some cases these methods may necessitate a very high precision. For instance, if a variable
ranges between [1 000 000, 1 000 001], after the conversion there will be many excess bit integers
between [0, 1M], and so a very high precision will be necessary to capture the range of interest. A
similar issue will arise with asymmetric ranges such as from [−0.5, 6000]. While we claim that, in
most physics applications, variables and their ranges are well deﬁned so that such problems will not
arise, we recognize that this may not be generally true for every application. Therefore, in some
cases clever unit manipulation or variable deﬁnition may be necessary.

Table 5: Bit integer example for variables used at the LHC. Two scenarios are considered. The ﬁrst set
distributes 24 bits evenly among three variables. The second set distributes 22 bits more optimally considering
that the angular resolution at the ﬁrst level is not < 0.1 and that the energy resolution is higher.

Variable
𝑝T
𝜙 position
𝜂 position

Range
[−10, 1023] GeV
[−3.14, 3.14]
[−4.9, 4.9]

Evenly distributed
Resolution
Bits
4 GeV
8
≈ 0.025
8
≈ 0.04
8

Optimally distributed
Bits
12
5
5

Resolution
250 MeV
≈ 0.20
≈ 0.15

22

Nanosecond ML regression with deep BDT in FPGA for HEP

References

[1] L. Evans and P. Bryant, LHC Machine, JINST 3, S08001 (2008).

[2] ATLAS Collaboration, The ATLAS Experiment at the CERN Large Hadron Collider, JINST 3,

S08003 (2008).

[3] CMS Collaboration, The CMS Experiment at the CERN LHC, JINST 3, S08004 (2008).

[4] ATLAS Collaboration, Performance of the ATLAS Trigger System in 2010, Eur. Phys. J. C 72,

1849 (2012).

[5] ATLAS Collaboration, Performance of the ATLAS Trigger System in 2015, Eur. Phys. J. C 77,

317 (2017).

[6] CMS Collaboration, The CMS trigger system, JINST 12, P01020 (2017).

[7] R. Achenbach et al., The ATLAS level-1 calorimeter trigger, JINST 3, P03001 (2008).

[8] CMS Collaboration, The Phase-2 Upgrade of the CMS Endcap Calorimeter, CERN-LHCC-

2017-023, 2017, http://cds.cern.ch/record/2293646.

[9] CMS Collaboration, Performance of the CMS Level-1 trigger in proton-proton collisions at

√

𝑠 = 13 TeV, JINST 15, P10017 (2020).

[10] ATLAS Collaboration, Performance of the upgraded PreProcessor of the ATLAS Level-1

Calorimeter Trigger, JINST 15, P11016 (2020).

[11] ATLAS Collaboration, Performance of the ATLAS Level-1 topological trigger in Run 2, Eur.

Phys. J. C 82, no. 1, 7 (2022).

[12] CMS Collaboration, The Phase-2 Upgrade of the CMS Level-1 Trigger, CERN-LHCC-2020-

004, CMS-TDR-021, 2020, http://cds.cern.ch/record/2714892.

[13] ATLAS Collaboration, Technical Design Report for the Phase-I Upgrade of the ATLAS
TDAQ System, CERN-LHCC-2013-018, ATLAS-TDR-023, 2013, http://cds.cern.ch/record/
1602235.

[14] ATLAS Collaboration, Technical Design Report for the Phase-II Upgrade of the ATLAS
TDAQ System, CERN-LHCC-2017-020, ATLAS-TDR-029, 2017, http://cds.cern.ch/record/
2285584.

[15] CMS Collaboration, Performance of the CMS missing transverse momentum reconstruction in

√

pp data at

𝑠 = 8 TeV, JINST 10, no. 02, P02006 (2015).

23

Nanosecond ML regression with deep BDT in FPGA for HEP

[16] ATLAS Collaboration, Reconstruction of hadronic decay products of tau leptons with the

ATLAS experiment, Eur. Phys. J. C 76, no. 5, 295 (2016).

[17] ATLAS Collaboration, Electron and photon energy calibration with the ATLAS detector using

2015–2016 LHC proton-proton collision data, JINST 14, no. 03, P03017 (2019)

[18] ATLAS Collaboration, Electron and photon performance measurements with the ATLAS
detector using the 2015–2017 LHC proton-proton collision data, JINST 14, no. 12, P12006
(2019).

[19] ATLAS Collaboration, Convolutional Neural Networks with Event Images for Pileup Mitigation
with the ATLAS Detector, ATL-PHYS-PUB-2019-028, 2019, http://cds.cern.ch/record/
2684070.

[20] ATLAS Collaboration, Deep Learning for Pion Identiﬁcation and Energy Calibration with the
ATLAS Detector, ATL-PHYS-PUB-2020-018, 2020, http://cds.cern.ch/record/2724632.

[21] J. Duarte et al., Fast inference of deep neural networks in FPGAs for particle physics, JINST

13, P07027 (2018).

[22] Y. J. Jwa, G. D. Guglielmo, L. P. Carloni, and G. Karagiorgi, Accelerating Deep Neural
Networks for Real-time Data Selection for High-resolution Imaging Particle Detectors, 2019
New York Scientiﬁc Data Summit (NYSDS), 1 (2019).

[23] S. Summers et al., Fast inference of Boosted Decision Trees in FPGAs for particle physics,

JINST 15, P05026 (2020).

[24] V. Loncar et al., Compressing deep neural networks on FPGAs to binary and ternary precision

with HLS4ML, 2020, [2003.06308].

[25] Y. Iiyama et al., Distance-Weighted Graph Neural Networks on FPGAs for Real-Time Particle

Reconstruction in High Energy Physics, Front. Big Data 3, 598927 (2020).

[26] A. Heintz et al., Accelerated Charged Particle Tracking with Graph Neural Networks on

FPGAs, 2020, [2012.01563].

[27] J. St. John et al., Real-time Artiﬁcial Intelligence for Accelerator Control: A Study at the

Fermilab Booster, 2020, [2011.07371].

[28] C. N. Coelho, A. Kuusela, S. Li, et al., Automatic heterogeneous quantization of deep neural
networks for low-latency inference on the edge for particle detectors, Nat. Mach. Intell. 3,
675–686 (2021).

24

Nanosecond ML regression with deep BDT in FPGA for HEP

[29] T. Aarrestad et al., Fast convolutional neural networks on FPGAs with hls4ml, 2021,

[2101.05108].

[30] M. Migliorini, J. Pazzini, A. Triossi, M. Zanetti, and A. Zucchetta, Muon trigger with fast

Neural Networks on FPGA, a demonstrator, 2021, [2105.04428]

[31] T.M. Hong, B.T. Carlson, B.R. Eubanks, S.T. Racz, S.T. Roche, J. Stelzer, and D.C. Stumpp,
Nanosecond machine learning event classiﬁcation with boosted decision trees in FPGA for
high energy physics, JINST 16, P08016 (2021).

[32] A. Elabd et al., Graph Neural Networks for Charged Particle Tracking on FPGAs, Front. Big

Data 5, 828666 (2022).

[33] E. E. Khoda et al., Ultra-low latency recurrent neural network inference on FPGAs for physics

applications with hls4ml, 2022, [2207.00559].

[34] S. Neuhaus et al., A neural network z-vertex trigger for Belle II, J. Phys. Conf. Ser. 608, 012052

(2015).

[35] D. Acosta et al., on behalf of the CMS Collaboration, Boosted Decision Trees in the Level-1

Muon Endcap Trigger at CMS, J. Phys. Conf. Ser. 1085, 042042 (2018).

[36] G. Aad, AS. Berthold, T. Calvet, et al., Artiﬁcial Neural Networks on FPGAs for Real-Time
Energy Reconstruction of the ATLAS LAr Calorimeters, Comput. Softw. Big Sci. 5, 19 (2021).

[37] R. Ospanov, C. Feng, W. Dong, W. Feng, and S. Yang, Development of FPGA-based neural
network regression models for the ATLAS Phase-II barrel muon trigger upgrade, Eur. Phys. J.
Web of Conf. 251, 04031 (2021).

[38] R. Ospanov, C. Feng, W. Dong, W. Feng, K. Zhang and S. Yang, Development of a resource-
eﬃcient FPGA-based neural network regression model for the ATLAS muon trigger upgrades,
2022, [2201.06288].

[39] ATLAS Collaboration, Searches for electroweak production of supersymmetric particles with
𝑠 = 13 TeV 𝑝 𝑝 collisions with the ATLAS detector, Phys. Rev. D

√

compressed mass spectra in
101, no. 5, 052005 (2020).

[40] ATLAS Collaboration, Search for new phenomena in events with an energetic jet and missing
√
𝑠 =13 TeV with the ATLAS detector, Phys. Rev. D

transverse momentum in 𝑝 𝑝 collisions at
103, no. 11, 112006 (2021).

[41] CMS Collaboration, Search for invisible decays of a Higgs boson produced through vector
√
𝑠 = 13 TeV, Phys. Lett. B 793, 520-551 (2019)

boson fusion in proton-proton collisions at

25

Nanosecond ML regression with deep BDT in FPGA for HEP

[42] ATLAS Collaboration, Search for invisible Higgs boson decays in vector boson fusion at

𝑠 = 13 TeV with the ATLAS detector, Phys. Lett. B 793, 499-519 (2019).

[43] A. Buckley, X. Chen, J. Cruz-Martinez, S. Ferrario Ravasio, T. Gehrmann, E. W. N. Glover,
S. Höche, A. Huss, J. Huston and J. M. Lindert, et al. A comparative study of Higgs boson
production from vector-boson fusion, JHEP 11, 108 (2021)

[44] ATLAS Collaboration, Performance of the missing transverse momentum triggers for the

ATLAS detector during Run-2 data taking, JHEP 08, 080 (2020).

[45] CMS Collaboration, The CMS trigger system, JINST 12, no. 01, P01020 (2017).

[46] CMS Collaboration, Performance of the CMS Level-1 trigger in proton-proton collisions at

𝑠 = 13 TeV, JINST 15, no. 10, P10017 (2020).

√

√

[47] S. T. Roche, B. Carlson, and T. M. Hong, fwXmachina example: Missing transverse energy

regression, Mendeley Data, V1, 2022, http://dx.doi.org/10.17632/d4c94r9254.1.

[48] J. Alwall et al., The automated computation of tree-level and next-to-leading order diﬀerential
cross sections, and their matching to parton shower simulations, JHEP 07, 079 (2014).

[49] T. Sjöstrand et al., An introduction to PYTHIA 8.2, Comput. Phys. Commun. 191, 159-177

(2015).

[50] S. Ovyn, X. Rouby, and V. Lemaitre, DELPHES, a framework for fast simulation of a generic

collider experiment, 2009, [hep-ph/0903.2225].

[51] DELPHES 3 Collaboration, DELPHES 3, A modular framework for fast simulation of a generic

collider experiment, JHEP 02, 057 (2014).

[52] M. Cacciari, G. P. Salam, and G. Soyez, The anti-𝑘𝑡 jet clustering algorithm, JHEP 04, 063

(2008).

[53] M. Cacciari and G. P. Salam, Pileup subtraction using jet areas, Phys. Lett. B 659, 119-126

(2008).

[54] M. Cacciari, G. P. Salam, and G. Soyez, The Catchment Area of Jets, JHEP 04, 005 (2008).

[55] ATLAS Collaboration, Readiness of the ATLAS Liquid Argon Calorimeter for LHC Collisions,

Eur. Phys. J. C 70, 723-753 (2010).

[56] M. Aharrouche et al., Energy linearity and resolution of the ATLAS electromagnetic barrel

calorimeter in an electron test-beam, Nucl. Instrum. Meth. A 568-2, 601 (2006)

26

Nanosecond ML regression with deep BDT in FPGA for HEP

[57] Y. A. Kulchitsky et al., Hadron energy reconstruction for the ATLAS barrel prototype
combined calorimeter in the framework of the nonparametrical method, JINR-E1-2000-73,
2000, [hep-ex/0004009].

[58] ATLAS Collaboration, Performance of missing transverse momentum reconstruction with the
𝑠 = 13 TeV, Eur. Phys. J. C 78, no. 11, 903

√

ATLAS detector using proton-proton collisions at
(2018)

[59] CMS Collaboration, Performance of missing transverse momentum reconstruction in proton-

proton collisions at

𝑠 = 13 TeV using the CMS detector, JINST 14, no. 07, P07004 (2019).

√

[60] A. Hoecker, P. Speckmayer, J. Stelzer, et al., TMVA - Toolkit for Multivariate Data Analysis,

CERN-OPEN-2007-007, 2007, [physics/0703039].

[61] B. Hawks, J. Duarte, N. J. Fraser, A. Pappalardo, N. Tran, and Y. Umuroglu, Ps and Qs:
Quantization-Aware Pruning for Eﬃcient Low Latency Neural Network Inference, Front. Artif.
Intell. 4, 676564 (2021).

27

