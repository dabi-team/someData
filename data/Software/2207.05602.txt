2
2
0
2

l
u
J

2
1

]
x
e
-
p
e
h
[

1
v
2
0
6
5
0
.
7
0
2
2
:
v
i
X
r
a

PITT-PACC-2212

Nanosecond machine learning regression with deep
boosted decision trees in FPGA for high energy physics

B.T. Carlsona,b, Q. Bayerb, T.M. Hongâˆ—b, and S.T. Rocheb

aDepartment of Physics, Westmont College
bDepartment of Physics and Astronomy, University of Pittsburgh

July 13, 2022

Abstract

We present a novel application of the machine learning / artiï¬cial intelligence method called
boosted decision trees to estimate physical quantities on ï¬eld programmable gate arrays (FPGA).
The software package fwXmachina features a new architecture called parallel decision paths
that allows for deep decision trees with arbitrary number of input variables. It also features
a new optimization scheme to use diï¬€erent numbers of bits for each input variable, which
produces optimal physics results and ultraeï¬ƒcient FPGA resource utilization. Problems in
high energy physics of proton collisions at the Large Hadron Collider (LHC) are considered.
Estimation of missing transverse momentum (ğ¸ miss
) at the ï¬rst level trigger system at the High
T
Luminosity LHC (HL-LHC) experiments, with a simpliï¬ed detector modeled by Delphes, is
used to benchmark and characterize the ï¬rmware performance. The ï¬rmware implementation
with a maximum depth of up to 10 using eight input variables of 16-bit precision gives a
latency value of O (10) ns, independent of the clock speed, and O (0.1)% of the available FPGA
resources without using digital signal processors.

Keywords: Data processing methods, Data reduction methods, Digital electronic circuits, Trigger
algorithms, and Trigger concepts and systems (hardware and software).

âˆ—Corresponding author, tmhong@pitt.edu

1

 
 
 
 
 
 
Nanosecond ML regression with deep BDT in FPGA for HEP

Contents

1

Introduction

2 ML training

3 Nanosecond Optimization

3.1 Parallel decision path architecture
3.2 Variable number of bits .

. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

2

3

7
7
9

4 Firmware design

10
4.1 Deep Decision Tree Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.2 Checks and comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

5 Results

6 Conclusions

A Variable number of bits

References

1

Introduction

13

21

21

23

Fast, accurate estimation of physical quantities from detector information is indispensable at high
energy physics experiments, especially ones with high data-taking rates. At the Large Hadron
Collider (LHC) [1], for example, bunches of protons collide at 40 MHz that produce approximately
60 TB of data each second. At detectors such as those at the ATLAS [2] and CMS [3] experiments,
an online trigger system saves a tiny fraction of the LHC collision eventsâ€”often those deemed
interestingâ€”for follow-up analysis oï¬„ine. The data may include rare known physical processes
sensitive to the eï¬€ects from undiscovered laws of physics.

The trigger system must be capable of identifying such rare events to be saved oï¬„ine while
simultaneously rejecting the orders-of-magnitude larger background processes. ATLAS and CMS
experiments employ a two-level trigger system [4â€“6]. The ï¬rst level systems [7â€“11], called level-0
or level-1 depending on the context, have a latency requirement of a few microseconds per event to
decide whether to save or reject the event [12â€“14]. Because of this strict requirement, traditional
algorithms often utilize simpliï¬ed estimates, relative to the oï¬„ine algorithms, at level-0 / level-1.
Variables such as energy and momentum of a ï¬nal state particle or invariant mass and angular
correlations of a group of ï¬nal state particles are typically considered.

2

Nanosecond ML regression with deep BDT in FPGA for HEP

For low latency implementation the ï¬rmware algorithms are used on custom electronic devices
such as those that employ ï¬eld programmable gate arrays (FPGA) and application-speciï¬c integrated
circuits (ASIC). A few thousandths of the incoming events pass the level-0 / level-1 algorithms, and
pass on a rate of O (100) kHz of data to the software-based trigger system called high level trigger
(HLT) or event ï¬lter (EF) depending on the context. The HLT / EF uses a farm of CPUs to further
evaluate events with more advanced algorithms within latency of a fraction of a millisecond.

Machine learning (ML) / artiï¬cial intelligence (AI) methods allow for improved estimates. In
[15], tau energy estimation [16], electron and

particular, regression models have been used for ğ¸ miss
photon energy [17, 18], pileup mitigation [19], and pion energy calibration [20].

T

In the past few years, progress in FPGA ï¬rmware design for signal-background classiï¬cation
have allowed for the use of more advanced algorithms using ML / AI at level-0 / level-1 [21â€“33],
typically relegated to the HLT / EF or oï¬„ine analysis. FPGA ï¬rmware design for regression estimates
have been developed for experiments at level-0 / level-1 [34â€“38].

In this paper, we present an alternative and novel FPGA ï¬rmware implementation of BDT
algorithms that allows for deeper decision trees. We expand on our previous BDT classiï¬cation
design [31] with 10 ns latency and sub-percent-level resource usage and use it as a blueprint for
regression. We utilize the new design to perform regression to estimate physical quantities, such as
ğ¸ miss
T

, that may be of interest at the LHC.

The paper is organized as follows. Section 2 describes the regression problem and the machine
learning training and setup. Section 3 describes the nanosecond optimization, which is the pre-
processing step prior to the ï¬nal ï¬rmware design. Section 4 describes the ï¬rmware design. Section
5 presents the results. Section 6 concludes.

2 ML training

We consider the physics problem of the missing transverse momentum (ğ¸ miss
trigger
serves as one of the primary means for identifying and saving high momentum particles that are
invisible to the detector, such as particles without strong or electromagnetic interactions. Such
particles include neutrinos as well as those from hypothesized â€œbeyondâ€ the standard model (BSM)
scenarios such as supersymmetry and dark matter (see, e.g., [39, 40] and the references therein).

). The ğ¸ miss

T

T

T

One particularly relevant example at ATLAS and CMS is the invisible decay of the Higgs bosons
that are produced in vector boson fusion (VBF) during proton collisions [41, 42]. In these cases the
distribution of ğ¸ miss
decays relatively quickly above around 70 GeV [43], which makes it critical to
maintain as low a trigger threshold as possible. The experimental challenge is that the calorimeter
noise level due to the high amount of â€œpileupâ€ (cid:104)ğœ‡(cid:105), or simultaneous proton-proton collisions per
bunch crossing, drives the trigger thresholds to higher values to maintain a low event rate. The
pileup dependence of the ğ¸ miss
trigger has plagued ATLAS [44] and CMS [45, 46] during previous
runs, as the luminosity has rapidly increased and may continue to be problematic in future runs
without new techniques to reduce pileup. BDT regression is applied to estimate ğ¸ miss
deï¬ned below.

T

T

3

Nanosecond ML regression with deep BDT in FPGA for HEP

T

The calculation of ğ¸ miss

at the level-0 / level-1 trigger system at the LHC is challenging because
of the constraints imposed by the collider. Examples of constraints include the strict timing between
proton bunch crossings and the availability of detector information. In ğ‘ ğ‘ collisions, the vector
sum of momenta of the decay products in the plane normal to the beam should be zero. In some
collisions, however, momentum appears to not be conserved due to non-interacting decay products,
mismeasurement, or a combination of both, to produce a non-zero value of

METğ¼ â‰¡ ğ¸ miss

T,ğ¼ â‰¡

(cid:12)
(cid:12)
(cid:12)

âˆ‘ï¸

ğ‘– âˆˆ ğ¼

(cid:174)ğ‘T, ğ‘–

,

(cid:12)
(cid:12)
(cid:12)

where the sum is over the decay products. For notational simplicity we also denote ğ¸ miss

T

as MET.

Samples

We created three samples [47], two samples with non-zero distributions (items A1 and A2 below)
and one sample with zero distribution (item B below) of true ğ¸ miss
at the generator level, i.e.,
METtruth. All three samples contain non-zero distributions of reconstructed MET.

T

A1. Sample of Higgs bosons produced by VBF, with the Higgs subsequently forced to decay to
neutrinos via ğ» â†’ ğ‘ ğ‘ âˆ— â†’ 4ğœˆ, producing non-zero METtruth. The VBF process produces at
least two highly energetic hadronic jets that are widely separated in polar angle with respect to
the collision axis. The Higgs decay to neutrinos ensures that the majority of the signal events
would have a high value of MET at the generator level relative to the B sample below.

A2. Sample of leptonic ğ‘¡Â¯ğ‘¡ decays with non-zero METtruth.

B. Sample of the QCD multÄ³et process with no METtruth.

We note that samples A1 and B are merged into one training sample for the ML. However, for
the evaluation of ROC curves where â€œsignalâ€ and â€œbackgroundâ€ needs to be deï¬ned sample A1 is
taken as the signal and sample B as the background. An alternate choice of sample A2 as signal is
considered to validate the training, but we do not train with A2.

Each sample contains 100ğ‘˜ events, produced using MadGraph5_aMC 2.9.5 [48] for the matrix
element calculation, Pythia8 [49] for the hadronization, and Delphes 3.5.0 [50, 51] for the detector
simulation and object reconstruction. The last step uses the ATLAS card with a mean pileup of
(cid:104)ğœ‡(cid:105) = 40.

For the input variables described next, the following objects are used. Tracks from charged
particles are reconstructed with a ğ‘T threshold of 100 MeV. Tracks have an eï¬ƒciency applied as a
function of their ğ‘T and location in ğœ‚. We use the default eï¬ƒciency formulae from the ATLAS card
with pileup in Delphes. Neutral hadrons are summed into projective towers, which are referred to as
neutral hadron towers. Electrons and photons with a ğ‘T of at least 10 GeV are reconstructed with an

4

Nanosecond ML regression with deep BDT in FPGA for HEP

eï¬ƒciency of 95% within |ğœ‚| < 1.5 and 85% within âˆ’2.5 < ğœ‚ < âˆ’ 1.5 and 1.5 < ğœ‚ < 2.5. The muons
are the characterized similarly with the external ğœ‚ boundary at 2.7 instead of 2.5. Hadronic jets
are reconstructed with the anti-ğ‘˜ğ‘¡ algorithm [52] with a radius parameter ğ‘… = 0.4 and a minimum
ğ‘T of 20 GeV. For tracks, the charged hadron subtraction method is used to remove pileup before
jet reconstruction, while the neutral component applies a residual correction to the reconstructed
jet [53, 54]. The hadronic decays of tau leptons are reconstructed as jets. A calorimeter with
electromagnetic (EM) and hadronic (HAD) projective towers are formed, with ï¬xed ğœ‚-ğœ™ boundaries,
are distributed with a higher granularity in the central region |ğœ‚| < 2.6 than in the forward region,
inspired by the granularity implemented in ATLAS [55]. Electrons and photons deposit all of their
energy in the EM calorimeter, while the energy from hadrons is split between the EM and HAD
calorimeters. Subsequently, a detector energy resolution term is added separately for EM and HAD
towers, inspired by ref. [56, 57]. For the purpose of computing ğ¸ miss
, the energies from the EM and
HAD towers are summed into a single tower. The plots also show the tower energy distributions for
a typical signal and background event.

T

Variables

We consider several algorithms for computing MET, which are modeled after those in use by ATLAS
and CMS experiments during the Run 1 (2010â€“2012) and Run 2 (2015â€“2018) periods [58, 59], as
input variables to the regression BDT. We hypothesized that training a regression BDT on these
inputs, similar to a regression constructed by CMS [15], to target the generator-level MET, also
called METtruth, would yield a better approximate than any one input variable.

The input variables are four ï¬‚avors of ğ¸ miss

(METreco, METtowers, METtracks, METjets), one ï¬‚avor
of Î£ğ¸T (SETjets), and three energy densities (ğœŒfwd-A, ğœŒbarrel, ğœŒfwd-B). The â€œreconstructedâ€ ğ¸ miss
computed using objects with the ğ‘T thresholds described above (electrons, muons, photons, hadronic
jets) is METreco. The calorimeter tower-based ğ¸ miss
is METtowers. The track- and neutral hadron
tower-based ğ¸ miss
is METjets; this is sometimes referred to as MHT
in the literature. The jet-based Î£ğ¸T is SETjets. The energy density ğœŒ is computed from calorimeter
towers in the forward regions and the barrel region of the calorimeter.

T
is METtracks. The jet-based ğ¸ miss

T

T

T

T

Table 1 lists the input, output, and target variables for the regression BDT. As mentioned
previously, the training is performed using one merged sample of sample A1 and sample B. The
input variable distributions are shown in ï¬gure 1. The plot on the left shows the various MET
variables whereas the plot on the right shows the SET variable.

ML conï¬guration

The parameters of the regression BDT is determined by the TMVA [60] library using the adaptive
boosting (AdaBoost) method of regression variance separation. It is conï¬gured with 40 trees at a
maximum depth of 5 for the ï¬gures featuring ML results and the physics performance.

5

Nanosecond ML regression with deep BDT in FPGA for HEP

Table 1: List of variables for the ğ¸ miss
the target variable. The output is the result of the regression.

T

estimation. The regression takes eight input variables and optimizes to

How used
Target
Input 1
2
(cid:48)(cid:48)
3

(cid:48)(cid:48)

(cid:48)(cid:48)

(cid:48)(cid:48)

(cid:48)(cid:48)

4
5
6
7
8
(cid:48)(cid:48)
Output

(cid:48)(cid:48)

Variable
METtruth
METreco
METtowers
METtracks

METjets
SETjets
ğœŒfwd-A
ğœŒbarrel
ğœŒfwd-B
OBDT

based on generator quantities due to, e.g., neutrinos
based on reconstructed objects, e.g., ğ‘’, ğœ‡, ğ›¾, jets ğ‘—
based on calorimeter towers
by Delphes based on charged tracks and neutral hadron

Description
ğ¸ miss
T
ğ¸ miss
T
ğ¸ miss
T
ğ¸ miss
T
towers
ğ¸ miss
T
Î£ğ¸T of reconstructed hadronic jets
Energy density for âˆ’4.9 < ğœ‚ < âˆ’2.5
Energy density for |ğœ‚| < 2.5
Energy density for 2.5 < ğœ‚ < 4.9
ğ¸ miss
T

estimation from the regression

based on reconstructed hadronic jets

Figure 1: Input variable distributions. The reconstructed MET distributions (left) are given for background
process for various estimations that all peak around 30 GeV and the signal process that is broader that all
peaks similarly at higher values. The truth MET distributions are also shown for the background process
that peaks at 0 and for the signal process that is similar to the reconstructed values. The reconstructed Î£ğ¸T
distributions (right) are shown for the background process and the signal process.

6

050100150200250300MET, see legend (GeV)00.050.10.15Events / 5 GeV (unit norm.) for bkgtruth MET for sigtruth METtracks METtowers METreco METjets METOF0.98backgroundprocesssignalprocess0100200300400500 (GeV)T EÎ£00.050.10.15Events / 20 GeV (unit norm.)OFbackgroundprocesssignalprocessNanosecond ML regression with deep BDT in FPGA for HEP

As is common with ML applications in the trigger system, the training procedure is performed
one time to determine the ML parameters. The resulting setup is incorporated in the ï¬rmware design
to evaluate the collision events in real-time. The training step itself typically takes O (1) minute on a
modern oï¬€-the-shelf laptop, even for relatively deep networks.

The training step is performed on half of the events, both VBF Higgs signal and multÄ³et
background samples together, without pre-selection. The other half is used to test the result of the
training, discussed in the next section.

3 Nanosecond Optimization

Our BDT regression design is built on the framework of fwXmachina [31]. The workï¬‚ow is
identical to the classiï¬cation design with many components (Tree Flattener, Forest Merger,
Score Finder, Score Normalizer, Tree Remover, and Cut Eraser) re-used for regression. The
new parallel decision path architecture is discussed in section 3.1.

The treatment of the number of bits for variables is diï¬€erent than our previous design and is

speciï¬c to the regression problem. This is discussed later in 3.2 and in appendix A.

3.1 Parallel decision path architecture

The depth of the decision tree determines the granularity of the partitions of the input hyperspace:
the deeper the tree, the ï¬ner the granularity. We present a new non-iterative design of the deep
decision tree with a complexity scaling with the depth and independent of the number of variables.
A comparison to our previous ï¬‚attened design is given at the end of the section.

Consider a decision tree of maximum depth ğ· with ğ‘bin = ğµ terminal nodes, or bins. By
construction, ğµ is at most 2ğ·. The set of bins are labeled as {ğ‘0, ğ‘1, . . . , ğ‘ğµâˆ’1}. Each terminal node
is logically connected to the initial node by a set of comparisons that deï¬ne the intermediate nodes.
For example, a terminal node at depth 2 corresponds to a set of 2 comparisons. More generally,
a terminal node ğ‘› at depth ğ‘‘ corresponds to a set of ğ‘‘ comparisons, i.e., ğ‘„ğ‘› = {ğ‘ğ‘›
ğ‘‘âˆ’1},
0
where ğ‘ğ‘›
ğ‘– is the result of the comparison at node ğ‘–. For a given terminal node ğ‘›, the logical and of
the associated set of comparisons is called a parallel decision path (PDP), i.e., PDPğ‘› = âˆ© ğ‘„ğ‘›. Figure
2 works out an example of one decision tree with two variables and maximum depth of three.

, . . . ğ‘ğ‘›

, ğ‘ğ‘›
1

The advantage of the deep decision tree is that the set of decision paths can be evaluated
simultaneously. A fully populated tree with 2ğ· terminal notes has 2ğ· PDP and requires ğ· Â· 2ğ·
comparisons. We ï¬nd that in our use cases, deep trees are often not fully populated, and thus scaling
is often softer than 2ğ· (ï¬gure 3). A comparison to the 2ğ· scaling for ğ· = 10 can be made with
the vertical axis on the right. We see that it ranges from 10 to 25% of a fully populated tree, with
density decreasing with ğ‘tree.

7

Nanosecond ML regression with deep BDT in FPGA for HEP

Figure 2: Deep decision tree with parallel decision path (PDP) structure. An example is shown in the leftmost
diagram for a decision tree using two variables (ğ‘¥ğ‘, ğ‘¥ğ‘) with a depth of 3. The equivalent representation in
the two-dimensional ğ‘¥ğ‘ vs. ğ‘¥ğ‘ space is given in the middle. The PDP perspective is given on the right. The
table at the bottom lists the logical comparisons per PDP.

Figure 3: Average number of bins per tree (cid:104)ğ‘ğ‘ğ‘–ğ‘›(cid:105) vs. maximum tree depth ğ·. The right vertical axis shows
the (cid:104)ğ‘bin(cid:105) fraction with respect to the exponential scaling of 2ğ· to compare the points at ğ· = 10.

8

b11b10b2qii: xb > 23qiii: xa > 40b0Decision tree structureDestination bin Depth iDepth iiDepth iiiDecision pathPath #b0not(qi)not(qii)N/Anot(qi) and not(qii)0b2qiN/AN/Aqi1b10not(qi)qiinot(qiii)not(qi) and qii and not(qiii)2b11not(qi)qiiqiiinot(qi) and qii and qiii3Worked example55xaxb23b0b2b102d plane: xa vs. xbb1140Decision pathsPath 0Path 1Path 2Path 3qi: xa > 550246810D, max. tree depth0100200300 > per treebin< N01020 (%)10Percentage of 2tree N 1 2 4 5 10 20 30 40Nanosecond ML regression with deep BDT in FPGA for HEP

Comparison to ï¬‚attened trees architecture

We compare the design in this paper with the non-iterative ï¬‚attened architecture of ref. [31].

One major limitation of the ï¬‚attened architecture is that the number of bins ğµ scales with product
of the number of cuts ğ‘ğ‘£ for each variable ğ‘£ for ğ‘‰ total variables. For even relatively shallow trees,
this quickly results in a prohibitive number of bins. More formally, one can get an idea of the scaling
by considering the quantity ğµ = (cid:206)ğ‘‰
ğ‘ğ‘£ with some assumptions. Each ğ‘ğ‘£ is bounded by 2ğ· since
(cid:205)ğ‘‰
ğ‘ğ‘£ = 2ğ· âˆ’ 1. So if we suppose that each variable has the same number of cuts then, we have
ğ‘ğ‘£ â‰ˆ ğ‘ where ğ‘ â‰¡ 2ğ·/ğ‘‰. In this scenario, we have ğµ that scales exponentially with ğ· and ğ‘‰. In
comparison, PDP scales at worst exponentially with ğ· and independent of the number of variables
ğ‘‰. Moreover, we saw a much softer scaling vs. ğ· in ï¬gure 3 in our examples.

ğ‘£=1

ğ‘£=1

3.2 Variable number of bits

Number of bits used per variable is an important aspect of resource optimization. There are
approaches that optimize prior to training [31] or during training [61]. We take the former approach
in optimally distributing the total numbers of bits to achieve the variable resolution at hand. Some
examples that are relevant to the problems at the LHC are discussed in appendix A.

In some applications of ML on FPGA, it is beneï¬cial if diï¬€erent variables can be represented
with diï¬€erent bit integer precision. For instance, consider a sample BDT that uses two variables:
one with many cuts requiring exact precision, and one with few cuts requiring less precision. To
minimize resource usage on the board, it is useful if the variable with fewer cuts can be expressed
with fewer bits. In ref. [31], we introduced the ability to use a diï¬€erent precision for the input
variables than the BDT output score. Here, we add support for diï¬€erent variable precision for each
input variable. The bit-optimized conï¬guration gives the same timing results, but with ultraeï¬ƒcient
FPGA implementation, with respect to the unoptimized conï¬guration.

We present the following comparison to illustrate the diï¬€erence. In the unoptimized conï¬guration
we use a total of 144 bits with 16 bits for all input and output variables. In the optimized conï¬guration
we use a total of 87 bits with either 12 or 5 bits per variable depending on the variable. In terms
of physics results, both conï¬gurations yield the same area under the ROC curve. In terms of
FPGA cost, both conï¬gurations result in the same latency and interval measurements. However,
the resource usage in look up tables (LUT) and ï¬‚ip ï¬‚ops (FF) are reduced by a factor of about 4.
Table 2 summarizes the results.

We expect similar signiï¬cant reduction in resource usage when applied to the classiï¬cation

problems of ref. [31].

There are subtleties and technical challenges for the implementation. The discussion can be

found in appendix A.

9

Nanosecond ML regression with deep BDT in FPGA for HEP

4 Firmware design

The ï¬rmware implementation of regression is built on fwXmachina [31]. The only substantial
diï¬€erence of the existing pieces is in the Score Processor for the gradient boost (GradBoost).1 A
new Engine is introduced next for the parallel decision paths. Firmware veriï¬cation and validation
is given in section 4.2.

4.1 Deep Decision Tree Engine

Deep decision tree is implemented with a Deep Decision Tree Engine (DDTE); see ï¬gure 4.
DDTE makes ğµ duplicates of the set of input variables and feeds each one to the One Hot Decision
Path (OHDP) to process each parallel decision path. The OHDP results are collected by a look up
table (LUT) that converts the one-hot vector into an regression estimate ğ‘‚BDT.

Figure 4: The block diagram of the Deep Decision Tree Engine (DDTE). The input x is a vector of ğ‘‰
variables and the output ğ‘‚BDT is the regression estimate. For each ğµ bin (terminal node) of the decision tree,
there is a corresponding One Hot Decision Path (OHDP).

Each OHDP implements the parallel decision path logic with a pair of comparisons for each
variable, comparing to a user-conï¬gured upper bound and a lower bound; see ï¬gure 5. The output
of each comparison feeds in to one and operator to yield a binary result.

1This component is intended to normalize the score provided by the BDT into a more useful form. The transformation
for AdaBoost is still trivial, so this went unchanged. The GradBoost option now adds a supplied constant to the sum of
the individual scores. The processor for GradBoost previously applied a piecewise approximation of the tanh function
to the sum of the individual scores; however, this normalization method is no longer desirable. If no constant is provided
when using GradBoost, then the constant is set to 0 by default.

10

One Hot Decision PathOHDP0 x...xxDeep Decision Tree Engine (DDTE)OHDP1OHDPB-1OBDTfor b = 0 .. B-1 terminal binsxO0O1OB-1 in0 in1 inB-1...outLUTactive input array â†’ output arraybus tapNanosecond ML regression with deep BDT in FPGA for HEP

Figure 5: The block diagram of the One Hot Decision Path (OHDP). Each variable ğ‘¥ğ‘£ is compared to pair of
â€œlowâ€ and â€œhighâ€ constants ğ›¼ to check if it is within the range deï¬ned by the constants, i.e., ğ›¼low < ğ‘¥ğ‘£ < ğ›¼high.
The collection of pairs of comparisons for each variable deï¬nes a particular parallel decision path (PDP). The
output of OHDP is a boolean and is one-hot encoded.

11

demuxOne Hot Decision Path (OHDP)andx0x1xV-1>Î±low<Î±highx0x0>Î²low<Î²highx1x1>Î³low<Î³highxV-1xV-1... Not  explicitly used,may be usedindirectlyLUT / BRAMOPDPxfor v = 0 .. V-1 input variablesNanosecond ML regression with deep BDT in FPGA for HEP

4.2 Checks and comparisons

We report the results for the benchmark point of 40 trees and a maximum depth of 5 in this section.

Veriï¬cation and validation

For veriï¬cation of the ï¬rmware against physical FPGA, an RTL design is programmed onto the Xilinx
Virtex UltraScale+ FPGA VCU118 Evaluation Kit and the Xilinx Artix-7 FPGA on Zynq-7020
System on Chip (SoC). The Ultrascale+ is run on three clock speedsâ€”320 MHz, 200 MHz, and
100 MHzâ€”while the Artix-7 is run on 100 MHz. In all scenarios, a test vector is evaluated on the
FPGA and the resulting output is compared to the output received from co-simulation. No diï¬€erence
is observed.

Our ï¬rmware design produces latency values that are independent of the choice of clock speed,
as was also the case for ref. [31]. The same conï¬guration is executed using the three clock speeds
noted above on the Ultrascale+. The results produced latency values of 6 clock ticks (18.75 ns), 4
clock ticks (20 ns), and 2 clock ticks (20 ns), respectively. They all produce the same latency values
of about 20 ns within the resolution of the clock period. The interval remains at one clock tick for
all clock speeds.

The latency for the Artix-7 was about 4-fold higher, in absolute terms using the same clock speed,
than for the Ultrascale+ as was seen previously for our ï¬‚attened tree architecture [31]. Resource
cost was generally higher as well on the Artix-7 with over 6-fold the resource usage compared to the
Ultrascale+. As is seen for the Ultrascale+ results, the interval is one clock tick and no DSP is used.
Validation of the ï¬rmware against software simulation is done with 100 000 input test vectors for
around 200 diï¬€erent BDT conï¬gurations. Inputs are processed through software simulation as well
as co-simulation. Other than rounding discrepancies, no diï¬€erence is observed for the BDT output.

Comparison of estimated and actual FPGA cost

We compare the actual FPGA resource utilization and timing results to the estimated values we
obtain from C synthesis. The actual values can be measured in Xilinx Vivado after the RTL design
is generated and uploaded to the FPGA. Timing is measured using the ILA, while the resource
utilization is directly reported by the software. We refer to ï¬gure 24 in ref. [31] for the illustration
of the deï¬nitions.

For the timing, we see no diï¬€erence between the estimated and actual values in all of the

conï¬gurations that we considered in this paper.

For the resources, the resource utilization on the FPGA was consistently lower than C synthesis
values. Figure 6 shows the LUT and the FF comparisons for the conï¬guration using ğ‘tree = 10
and otherwise the same setup as in table 3. It is notable that the actual values are lower than the
estimated values by a factor of about 20 for the LUT and about 5 for the FF. We repeated the exercise
for ğ‘tree = 20 and the scale factors are about 10 for the LUT and about 2.5 for the FF. The actual
values are scaled up by the stated factor, which shows that the scaling vs. tree depth follows the trend

12

Nanosecond ML regression with deep BDT in FPGA for HEP

presented by the estimated curve. For this reason we report the actual FPGA resource utilization,
rather than the estimated version, in section 5.

Figure 6: Comparison of estimated usage and actual usage for LUT (left column) and FF (right column) for
ğ‘tree = 10 (top row). and ğ‘tree = 20 (bottom row). Estimated values are obtained with HLS C synthesis and
the actual values are obtained by RTL synthesis and implementation.

5 Results

We present the physics performance followed by the FPGA cost (timing and resource utilization) for
the ğ¸ miss

problem.

T

Physics performance

Physics performance is evaluated with ROC curves, turn-on curves, and MET resolution.

13

0246810D, max. tree depth020000400006000080000LUT usage01234567LUT usage (%)tree N 10 (estimated) 10 (actual) 20)Ã— 10 (actual 0246810D, max. tree depth05000100001500020000FF usage00.20.40.60.8FF usage (%)tree N 10 (estimated) 10 (actual) 5)Ã— 10 (actual 0246810D, max. tree depth020000400006000080000LUT usage01234567LUT usage (%)tree N 20 (estimated) 20 (actual) 10)Ã— 20 (actual 0246810D, max. tree depth05000100001500020000FF usage00.20.40.60.8FF usage (%)tree N 20 (estimated) 20 (actual) 2.5)Ã— 20 (actual Nanosecond ML regression with deep BDT in FPGA for HEP

ROC curves are shown in left plot of ï¬gure 7, showing the background rejection factor vs. signal
acceptance. The former is deï¬ned as the inverse of the background eï¬ƒciency 1/ğœ€ğµ, where ğœ€ğµ is the
false positive rate (FPR) or type I error (ğ›¼). The latter is deï¬ned as the signal eï¬ƒciency ğœ€ğ‘†, also
called the true positive rate (TPR). The eï¬ƒciencies for category ğ‘ = ğ‘†, ğµ are deï¬ned as the ratio
of the number of events category ğ‘ passing the MET threshold with respect to the total number of
events of category ğ‘, i.e., ğœ€ğ‘ â‰¡ ğ‘ pass
. A scan of the MET thresholds correspond to a point in
the (ğœ€ğ‘†, 1/ğœ€ğµ) plane; the collection of points deï¬ne the ROC curve shown in the ï¬gure.

/ğ‘ total
ğ‘

ğ‘

The right plot of ï¬gure 7 shows the ratio of background rejection factors with respect to METtowers.
Plots in ï¬gure 7 impose a selection of METtruth > 100 GeV for the signal events to better illustrate the
impact of the BDT for larger values of background rejection, closer to a more realistic experimental
threshold.

Figure 7: ROC curves of background rejection factor vs. signal acceptance (left) and the ratio of rejection
factors with respect to METtowers vs. signal acceptance (right). The background and signal are trained and
evaluated using the QCD multÄ³et and the VBF Higgs to 4ğœˆ sample. The scan is done over the full MET range
from 0 to 1.5 TeV. The order of the legend follows the curves. A subset of events for which the signal sample
has the pre-selection METtruth > 100 GeV applied is shown. The BDT values are using 16-bits.

For an operating point on the ROC curveâ€”say, at a background eï¬ƒciency of ğœ€ğµ = 10âˆ’3â€”the
signal eï¬ƒciency can be read oï¬€ of ï¬gure 7. For a signal eï¬ƒciency value of, say 85%, the background
rejection is approximately 150% lower than the same eï¬ƒciency computed using MET formed only
with calorimeter towers. More information can be obtained at that operating point by scanning the
METtruth threshold given an algorithm on the signal process. This produces the so-called turn-on
curve in the left plot of ï¬gure 8.

The turn-on curve is evaluated by identifying a selection of the MET variable (e.g., OBDT >
75 GeV), and for each bin of METtruth evaluate the fraction of events that satisfy the selection. The
BDT outperforms other curves by reaching full eï¬ƒciency the quickest, i.e., the turn-on curve is
â€œsharper.â€

14

00.20.40.60.81)SÎµSignal efficiency (110210310410)BÎµBackground rejection (1/BDTOtracksMETtowersMETobjectsMETjetsMHT > 100 GeV only for signaltruthReq. MET Better1âˆ’101âˆ’10Ã—21âˆ’10Ã—31)SÎµSignal efficiency (0123Ratios of rejection wrt towersBDTOtracksMETtowersMETobjectsMETjetsMHT > 100 GeV only for signaltruthReq. MET Nanosecond ML regression with deep BDT in FPGA for HEP

In order to verify the performance of an alternate sample with non-zero METtruth with a larger
jet multiplicity, including jets in the central region unlike for VBF processes, we consider leptonic ğ‘¡Â¯ğ‘¡
decays in the right plot of ï¬gure 8. The same BDT trained on the merged sample of sample A1 and
B is used to evaluate sample A2 for ğ‘¡Â¯ğ‘¡. We see that that turn-on curve for the BDT is sharper than
the input MET variables.

Figure 8: Eï¬ƒciency turn-on curves for the VBF Higgs sample (left) and ğ‘¡Â¯ğ‘¡ samples (right) as signal on the
vertical axis and QCD multÄ³et for background on the horizontal axis. The signal eï¬ƒciency of the BDT is
improved for both VBF Higgs and ğ‘¡Â¯ğ‘¡ samples as signal, demonstrating that the regression is robust for a
wide range of topologies.The threshold corresponding to the operating point of background eï¬ƒciency of
ğœ€ğµ = 10âˆ’3 is chosen. For each histogram a line is drawn between the data points as a visual guide.

Finally, we consider MET resolution. If the algorithm rejects more background events while
retaining a similar amount of signal events, the BDT regression is worth pursuing. We will see
that this is the case. The MET distribution for events without true ğ¸ miss
, i.e., background events,
is shown in the left plot of ï¬gure 9. As the training sample includes background events without
true ğ¸ miss
, these events tend to be reconstructed with low values of MET by the regression model,
as expected. To highlight the improved rejection, subset of events with non-zero reconstructed
MET, taken to be METtowers > 60 GeV are selected. For this subset of events, the BDT estimate
outperforms the input MET variables for accurately estimating the null MET value.

T

T

The MET resolution for events with true ğ¸ miss

, i.e., signal events, is shown in the right plot of
ï¬gure 9. For the subset of events with non-zero reconstructed MET in the range at which a MET cut
might be placed in a trigger system, taken here to be 150 < METtruth < 200 GeV, the BDT estimate
performs comparably to the input MET variables.

T

We now discuss the trade-oï¬€ between physics and engineering performance. As can be seen in
ï¬gure 10 the area under the ROC curve (AUC) is plotted vs. maximum tree depth ğ· and number of
bits for input variables.2 An AUC of 0.5 corresponds to the worst possible performance, while an

2AUC is deï¬ned to be the area under the curve when plotting ğœ€ğ‘† vs. ğœ€ğµ.

15

050100150200250300 (GeV)truthMET00.20.40.60.81SÎµSignal efficiency,  (16 bits)BDT Otracks METtowers METreco METjets MET050100150200250300 (GeV)truthMET00.20.40.60.81SÎµSignal efficiency,  (16 bits)BDT Otracks METtowers METreco METjets METt+QCD, eval. on tÎ½4Train on VBF H Nanosecond ML regression with deep BDT in FPGA for HEP

Table 2: Comparison of our benchmark conï¬guration (details in table 3) and the bit optimized conï¬guration.
The setup is given in the top; the physics performance in the middle; the FPGA cost in the bottom.

Parameter
Setup: no. of bits

Benchmark (table 3)

Bit optimized

Ratio

METğ‘˜, where ğ‘˜ = reco, towers, tracks, jets
SETjets
ğœŒğ‘˜, where ğ‘˜ = fwd-A, barrel, fwd-B
OBDT
All variables

16 bits each
16 bits
16 bits each
16 bits
144 bits in total

12 bits each
12 bits
5 bits each
12 bits
87 bits in total

Physics performance

Area under the ROC curve2 (AUC)

0.90

0.90

FPGA cost for 40 trees, 5 depth

Latency
Interval
Look up tables
Flip ï¬‚ops
Block RAM
Ultra RAM
Digital signal processors

6 clock ticks
1 clock tick
1675
1460
0
0
0

6 clock ticks
1 clock tick
374
352
0
0
0

1.3
1.3
3.2
1.3
1.7

1

1
1
4.5
4.1
Same
Same
Same

T

distributions for background events (left) and ğ¸ miss

Figure 9: ğ¸ miss
resolution for signal events (right).
Background events (left) with METtowers > 60 GeV shows how the higher MET values get remapped. Signal
events (right) with 100 < METtruth < 200 GeV shows the resolution with METtruth values of interest. The
input variable distributions are shown using ï¬‚oating point values. The output estimate OBDT distributions are
shown using 16-bit as is done in ï¬rmware.

T

16

020406080100MET, see legend (GeV)00.10.20.3Events / 5 GeV (unit norm.) (16 bits)BDT Otracks METtowers METreco METjets MET> 60 GeVtowersReq. MET  Std. dev.Mean,      28, 25 GeV     68, 20 GeV     -     48, 28 GeV     49, 28 GeV     OF00.511.52truth / METsee legendRatio of MET00.10.2Events / 0.1 (unit norm.)BDT Otracks METtowers METreco METjets MET<200GeVtruth150<MET /meanÏƒ0.200.200.210.220.22Nanosecond ML regression with deep BDT in FPGA for HEP

AUC of 1.0 corresponds to a perfect discrimination between signal and background. For the former,
the quick rise is followed by a plateau starting around ğ· = 7. For the latter, the plateau begins at
around 5.

Figure 10: Physics performance vs. maximum tree depth ğ· (left) and vs. number of bits for input variables
(right). The performance is given by area under the ROC curve2 (AUC) for each conï¬guration.

FPGA cost and parameter scanning

FPGA cost denotes the timing results, consisting of latency and interval, and resource usage.

Starting from our benchmark conï¬gurations listed in table 3, BDT parameters are varied, one at
a time, to investigate cost dependencies on tuneable parameters. We focus on the maximum tree
depth ğ·, and also show dependencies on the number of bits. We note that the maximum BDT
complexity scales with 2ğ·, but, as discussed previously in section 3, ï¬gure 3 showed that the scaling
is much softer, especially at high ğ· values.

The resource usage and latency scaling follows a similar pattern. The number of look-up tables
and ï¬‚ip-ï¬‚ops used vs. ğ· for several values of ğ‘trees is shown in ï¬gure 11. DSP usage is at 0 for all
conï¬gurations and BRAM is minimal as shown in ï¬gure 12.

The latency dependency on the maximum tree depth shows a similar pattern in ï¬gure 13, Notably,
this ï¬gure also demonstrates that the number of trees does not seem to have a large signiï¬cant
impact on the latency, with conï¬gurations from 1 to 40 trees all remaining within a single clock tick
of each other at each maximum depth. As in our previous ï¬rmware design, the interval is only one
clock tick for all conï¬gurations.

The latency dependency on the number of bits used in the input variable representations is shown
in ï¬gure 14. Less precise variable representations result in lower latency. As is shown previously in
ref. [31], less precise variable representations often result in degraded ML performance. Appendix A
includes a more in-depth discussion of dependency on integer precision and this trade-oï¬€.

17

12345678910D, max. tree depth0.70.80.91Area under ROC curve (AUC)tree N 401234567891020No. of bits for all input variables0.40.60.81Area under ROC curve (AUC)tree N 4010121416AUC for variable bitsNanosecond ML regression with deep BDT in FPGA for HEP

Table 3: Benchmark conï¬guration and the FPGA cost. Three groups of information are given. The top-most
group deï¬nes the FPGA setup. The second group deï¬nes the ML training used for the MET problem and the
Nanosecond Optimization. The third group gives the actual results measured on the FPGA for four tree-depth
combinations of 40-5, 40-6, 20-7, and 10-8.

Parameter
FPGA setup

Chip family
Chip model
Vivado version
Synthesis type
HLS or RTL
Clock speed

Value

Comments

Xilinx Virtex Ultrascale+
xcvu9p-ï¬‚ga2104-2L-e
2019.2
C synthesis
HLS
320 MHz

HLS interface pragma: None
Clock period is 3.125 ns

ML training conï¬guration & Nanosecond Optimization conï¬guration

ML training method
No. of input variables
Bin Engine type
No. of bits for all variables
FPGA cost for 40 trees, 5 depth

Latency
Look up tables
Flip ï¬‚ops

FPGA cost for 40 trees, 6 depth

Latency
Look up tables
Flip ï¬‚ops

FPGA cost for 20 trees, 7 depth

Latency
Look up tables
Flip ï¬‚ops
Block RAM

FPGA cost for 10 trees, 8 depth

Latency
Look up tables
Flip ï¬‚ops
Block RAM

Boosted decision tree
8
Deep Decision Tree Engine (DDTE)
16 bits for each

binary integers

Regression, Adaptive boosting

6 clock ticks
1675 out of 1 182 240
1460 out of 2 364 480

9 clock ticks
4566 out of 1 182 240
2516 out of 2 364 480

15 clock ticks
4568 out of 1 182 240
2697 out of 2 364 480
4.5 out of 4320

21 clock ticks
2556 out of 1 182 240
2299 out of 2 364 480
5 out of 4320

18.75 ns
0.1% of available
< 0.1% of available

28.125 ns
0.4% of available
0.1% of available

46.875 ns
0.4% of available
0.1% of available
0.1% of available

65.625 ns
0.2% of available
0.1% of available
0.1% of available

Common values for the above conï¬gurations

Interval
Block RAM
Ultra RAM
Digital signal processors

1 clock tick
0 out of 4320
0 out of 960
0 out of 6840

3.125 ns
If not listed above
Same for all trees and all depth
Same for all trees and all depth

18

Nanosecond ML regression with deep BDT in FPGA for HEP

Figure 11: Actual LUT usage (left) and actual FF usage (right) as a function of the maximum depth. Absolute
usage is shown on the left axis and percentage of our FPGA resources is shown on the right axis, both using
the setup in table 3.

Figure 12: Actual DSP usage (left) and actual BRAM usage (right) as a function of the maximum depth.
Absolute usage is shown on the left axis and percentage of our FPGA resources is shown on the right axis,
both using the setup in table 3. No DSP usage is seen.

19

0246810D, max. tree depth02000400060008000Actual LUT usage00.10.20.30.40.50.6LUT usage (% of xcvu9p)tree N 1 2 4 5 10 20 30 400246810D, max. tree depth0100020003000400050006000Actual FF usage00.10.2FF usage (% of xcvu9p)tree N 1 2 4 5 10 20 30 400246810D, max. tree depth0246810Actual DSP usage00.050.1DSP usage (% of xcvu9p)tree N 1 2 4 5 10 20 30 400246810D, max. tree depth051015Actual BRAM usage00.10.20.3BRAM usage (% of xcvu9p)tree N 1 2 4 5 10 20 30 40Nanosecond ML regression with deep BDT in FPGA for HEP

Figure 13: Algorithm latency (left) and interval (right) as a function of the maximum depth. Clock ticks are
shown on the left axis and nanoseconds are shown on the right axis, both using 320 MHz clock speed. Data
series for the a given number of trees are connected. The interval is unity for all data points. Eight input
variables of 16-bit precision are used.

Figure 14: Algorithm latency as a function of the number of bits of the input variables. Clock ticks are shown
on the left axis and nanoseconds are shown on the right axis, both using 320 MHz clock speed. For the other
parameters, Eight input variables of 40 trees with a maximum depth of 5 are used.

20

0246810D, max. tree depth0102030Latency (320 MHz clock ticks)020406080100Latency (ns)tree N 1 2 4 5 10 20 30 400246810D, max. tree depth012Interval (320 MHz clock ticks)0123456Interval (ns)tree N 1 2 4 5 10 20 30 40024681012141618B, no. of bits for input variables0246810Latency (320 MHz clock ticks)051015202530Latency (ns)Max. depth D = 5 = 40treeN = 8varNNanosecond ML regression with deep BDT in FPGA for HEP

6 Conclusions

We present a novel implementation of boosted decision trees on FPGA within the fwXmachina
framework [31] that allows for deep decision trees. In this paper, we demonstrate the use case for
the deep tree structure for regression problems. The new ï¬rmware design makes use of parallel
decision paths (PDP) to allow for deeper trees as well as arbitrarily many variables: two limitations
of the ï¬‚attened decision tree structure of ref. [31]. Finally, support for varying bit integer precision
per variable is implemented, allowing for further resource usage optimization.

The performance is shown for the problem ğ¸ miss

estimation. It is shown that combining several
T
conventional MET calculations with a regression BDT provided a better signal eï¬ƒciency and
background rejection for reasonable operating point for level-0 / level-1 trigger systems at the LHC.
FPGA implementation details are provided for hundreds of conï¬gurations, We ï¬nd that latency
results are O (10) ns. The resource usage is O (0.1)% of those available on our FPGA with the
important exception that no DSP resources are used. Results for various conï¬gurations by scanning
the BDT parametersâ€”such as the number of trees, the maximum tree depth, and the number of
bitsâ€”show that our implementation can be adapted for a variety of use cases

Acknowledgments

We thank Stephen Racz for the initial eï¬€ort of the project. We thank Gracie Jane Gollinger for
computing infrastructure support. We thank Joerg Stelzer for the TMVA discussions. We thank
Pavel Serhiayenka and Kushal Parekh for their assistance with FPGA data collection. We thank
the University of Pittsburgh for the support of this project, especially for Brandon Eubanks and
Emre Ercikti from the Electronics Shop of the Shared Research Support Services of the Dietrich
School of Arts and Sciences. TMH was supported by the US Department of Energy [award no.
DE-SC0007914]. STR was supported by the Emil Sanielevici Scholarship. Patent pending.

A Variable number of bits

One subtle diï¬€erence between the classiï¬cation and regression implementation is the eï¬€ect of bit
integers on output scores. Due to the advantage of pre-computing and pre-normalizing bin values to
their treesâ€™ boost-weights, it is important that the conversion of ï¬‚oating point target variable outputs
to integers respects addition, i.e., ğ‘“ (ğ‘¥1 + ğ‘¥2) = ğ‘“ (ğ‘¥1) + ğ‘“ (ğ‘¥2). This requirement only applies to the
target variable, not the input ones. This is discussed in ref. [31].

For our classiï¬cation application in ref. [31], the mapping is relatively straight forward. The
purity values ranging from [0, 1] could be scaled to a ğµ-bit integer by multiplying by 2ğµ âˆ’ 1 to
achieve a range of [0, 2ğµ âˆ’ 1]. The same scaling was applied to Yes/No Leaf outputs of [âˆ’1, 1] to
achieve the range of [âˆ’(2ğµ âˆ’ 1), 2ğµ âˆ’ 1]. Such simple scaling is closed under addition, allowing for

21

Nanosecond ML regression with deep BDT in FPGA for HEP

the summation of output scores from a forest of trees.

However, in regression we are not promised such neat outputs. In principle, the target variable
can be bounded between any two values or even be unbounded. Luckily in high energy physics,
physical variables are often either conveniently bounded below by zero or be symmetrical. Energy
can range from [0, ğ¸max] to be scaled to [0, 2ğµ âˆ’ 1] as before. Momentum can range [âˆ’ğ‘max, ğ‘max]
to be scaled to [âˆ’(2ğµ âˆ’ 1), 2ğµ âˆ’ 1]. If a given variable are not bounded or symmetric, we adjust the
range accordingly; see table 4 for examples.

Table 4: Bit integer conversion methods used for the target training variable. Four representative examples
are given.

Adjustment method
Positive
Positive
Symmetric
Symmetric

Initial range
[0, 22]
[50, 450]
[âˆ’120, 70]
[âˆ’50, 70]

Adjusted range
[0, 22]
[0, 450]
[âˆ’120, 120]
[âˆ’70, 70]

Adjusted bit range
[0, 2ğµ âˆ’ 1]
[0, 2ğµ âˆ’ 1]
[âˆ’(2ğµ âˆ’ 1), 2ğµ âˆ’ 1]
[âˆ’(2ğµ âˆ’ 1), 2ğµ âˆ’ 1]

In some cases these methods may necessitate a very high precision. For instance, if a variable
ranges between [1 000 000, 1 000 001], after the conversion there will be many excess bit integers
between [0, 1M], and so a very high precision will be necessary to capture the range of interest. A
similar issue will arise with asymmetric ranges such as from [âˆ’0.5, 6000]. While we claim that, in
most physics applications, variables and their ranges are well deï¬ned so that such problems will not
arise, we recognize that this may not be generally true for every application. Therefore, in some
cases clever unit manipulation or variable deï¬nition may be necessary.

Table 5: Bit integer example for variables used at the LHC. Two scenarios are considered. The ï¬rst set
distributes 24 bits evenly among three variables. The second set distributes 22 bits more optimally considering
that the angular resolution at the ï¬rst level is not < 0.1 and that the energy resolution is higher.

Variable
ğ‘T
ğœ™ position
ğœ‚ position

Range
[âˆ’10, 1023] GeV
[âˆ’3.14, 3.14]
[âˆ’4.9, 4.9]

Evenly distributed
Resolution
Bits
4 GeV
8
â‰ˆ 0.025
8
â‰ˆ 0.04
8

Optimally distributed
Bits
12
5
5

Resolution
250 MeV
â‰ˆ 0.20
â‰ˆ 0.15

22

Nanosecond ML regression with deep BDT in FPGA for HEP

References

[1] L. Evans and P. Bryant, LHC Machine, JINST 3, S08001 (2008).

[2] ATLAS Collaboration, The ATLAS Experiment at the CERN Large Hadron Collider, JINST 3,

S08003 (2008).

[3] CMS Collaboration, The CMS Experiment at the CERN LHC, JINST 3, S08004 (2008).

[4] ATLAS Collaboration, Performance of the ATLAS Trigger System in 2010, Eur. Phys. J. C 72,

1849 (2012).

[5] ATLAS Collaboration, Performance of the ATLAS Trigger System in 2015, Eur. Phys. J. C 77,

317 (2017).

[6] CMS Collaboration, The CMS trigger system, JINST 12, P01020 (2017).

[7] R. Achenbach et al., The ATLAS level-1 calorimeter trigger, JINST 3, P03001 (2008).

[8] CMS Collaboration, The Phase-2 Upgrade of the CMS Endcap Calorimeter, CERN-LHCC-

2017-023, 2017, http://cds.cern.ch/record/2293646.

[9] CMS Collaboration, Performance of the CMS Level-1 trigger in proton-proton collisions at

âˆš

ğ‘  = 13 TeV, JINST 15, P10017 (2020).

[10] ATLAS Collaboration, Performance of the upgraded PreProcessor of the ATLAS Level-1

Calorimeter Trigger, JINST 15, P11016 (2020).

[11] ATLAS Collaboration, Performance of the ATLAS Level-1 topological trigger in Run 2, Eur.

Phys. J. C 82, no. 1, 7 (2022).

[12] CMS Collaboration, The Phase-2 Upgrade of the CMS Level-1 Trigger, CERN-LHCC-2020-

004, CMS-TDR-021, 2020, http://cds.cern.ch/record/2714892.

[13] ATLAS Collaboration, Technical Design Report for the Phase-I Upgrade of the ATLAS
TDAQ System, CERN-LHCC-2013-018, ATLAS-TDR-023, 2013, http://cds.cern.ch/record/
1602235.

[14] ATLAS Collaboration, Technical Design Report for the Phase-II Upgrade of the ATLAS
TDAQ System, CERN-LHCC-2017-020, ATLAS-TDR-029, 2017, http://cds.cern.ch/record/
2285584.

[15] CMS Collaboration, Performance of the CMS missing transverse momentum reconstruction in

âˆš

pp data at

ğ‘  = 8 TeV, JINST 10, no. 02, P02006 (2015).

23

Nanosecond ML regression with deep BDT in FPGA for HEP

[16] ATLAS Collaboration, Reconstruction of hadronic decay products of tau leptons with the

ATLAS experiment, Eur. Phys. J. C 76, no. 5, 295 (2016).

[17] ATLAS Collaboration, Electron and photon energy calibration with the ATLAS detector using

2015â€“2016 LHC proton-proton collision data, JINST 14, no. 03, P03017 (2019)

[18] ATLAS Collaboration, Electron and photon performance measurements with the ATLAS
detector using the 2015â€“2017 LHC proton-proton collision data, JINST 14, no. 12, P12006
(2019).

[19] ATLAS Collaboration, Convolutional Neural Networks with Event Images for Pileup Mitigation
with the ATLAS Detector, ATL-PHYS-PUB-2019-028, 2019, http://cds.cern.ch/record/
2684070.

[20] ATLAS Collaboration, Deep Learning for Pion Identiï¬cation and Energy Calibration with the
ATLAS Detector, ATL-PHYS-PUB-2020-018, 2020, http://cds.cern.ch/record/2724632.

[21] J. Duarte et al., Fast inference of deep neural networks in FPGAs for particle physics, JINST

13, P07027 (2018).

[22] Y. J. Jwa, G. D. Guglielmo, L. P. Carloni, and G. Karagiorgi, Accelerating Deep Neural
Networks for Real-time Data Selection for High-resolution Imaging Particle Detectors, 2019
New York Scientiï¬c Data Summit (NYSDS), 1 (2019).

[23] S. Summers et al., Fast inference of Boosted Decision Trees in FPGAs for particle physics,

JINST 15, P05026 (2020).

[24] V. Loncar et al., Compressing deep neural networks on FPGAs to binary and ternary precision

with HLS4ML, 2020, [2003.06308].

[25] Y. Iiyama et al., Distance-Weighted Graph Neural Networks on FPGAs for Real-Time Particle

Reconstruction in High Energy Physics, Front. Big Data 3, 598927 (2020).

[26] A. Heintz et al., Accelerated Charged Particle Tracking with Graph Neural Networks on

FPGAs, 2020, [2012.01563].

[27] J. St. John et al., Real-time Artiï¬cial Intelligence for Accelerator Control: A Study at the

Fermilab Booster, 2020, [2011.07371].

[28] C. N. Coelho, A. Kuusela, S. Li, et al., Automatic heterogeneous quantization of deep neural
networks for low-latency inference on the edge for particle detectors, Nat. Mach. Intell. 3,
675â€“686 (2021).

24

Nanosecond ML regression with deep BDT in FPGA for HEP

[29] T. Aarrestad et al., Fast convolutional neural networks on FPGAs with hls4ml, 2021,

[2101.05108].

[30] M. Migliorini, J. Pazzini, A. Triossi, M. Zanetti, and A. Zucchetta, Muon trigger with fast

Neural Networks on FPGA, a demonstrator, 2021, [2105.04428]

[31] T.M. Hong, B.T. Carlson, B.R. Eubanks, S.T. Racz, S.T. Roche, J. Stelzer, and D.C. Stumpp,
Nanosecond machine learning event classiï¬cation with boosted decision trees in FPGA for
high energy physics, JINST 16, P08016 (2021).

[32] A. Elabd et al., Graph Neural Networks for Charged Particle Tracking on FPGAs, Front. Big

Data 5, 828666 (2022).

[33] E. E. Khoda et al., Ultra-low latency recurrent neural network inference on FPGAs for physics

applications with hls4ml, 2022, [2207.00559].

[34] S. Neuhaus et al., A neural network z-vertex trigger for Belle II, J. Phys. Conf. Ser. 608, 012052

(2015).

[35] D. Acosta et al., on behalf of the CMS Collaboration, Boosted Decision Trees in the Level-1

Muon Endcap Trigger at CMS, J. Phys. Conf. Ser. 1085, 042042 (2018).

[36] G. Aad, AS. Berthold, T. Calvet, et al., Artiï¬cial Neural Networks on FPGAs for Real-Time
Energy Reconstruction of the ATLAS LAr Calorimeters, Comput. Softw. Big Sci. 5, 19 (2021).

[37] R. Ospanov, C. Feng, W. Dong, W. Feng, and S. Yang, Development of FPGA-based neural
network regression models for the ATLAS Phase-II barrel muon trigger upgrade, Eur. Phys. J.
Web of Conf. 251, 04031 (2021).

[38] R. Ospanov, C. Feng, W. Dong, W. Feng, K. Zhang and S. Yang, Development of a resource-
eï¬ƒcient FPGA-based neural network regression model for the ATLAS muon trigger upgrades,
2022, [2201.06288].

[39] ATLAS Collaboration, Searches for electroweak production of supersymmetric particles with
ğ‘  = 13 TeV ğ‘ ğ‘ collisions with the ATLAS detector, Phys. Rev. D

âˆš

compressed mass spectra in
101, no. 5, 052005 (2020).

[40] ATLAS Collaboration, Search for new phenomena in events with an energetic jet and missing
âˆš
ğ‘  =13 TeV with the ATLAS detector, Phys. Rev. D

transverse momentum in ğ‘ ğ‘ collisions at
103, no. 11, 112006 (2021).

[41] CMS Collaboration, Search for invisible decays of a Higgs boson produced through vector
âˆš
ğ‘  = 13 TeV, Phys. Lett. B 793, 520-551 (2019)

boson fusion in proton-proton collisions at

25

Nanosecond ML regression with deep BDT in FPGA for HEP

[42] ATLAS Collaboration, Search for invisible Higgs boson decays in vector boson fusion at

ğ‘  = 13 TeV with the ATLAS detector, Phys. Lett. B 793, 499-519 (2019).

[43] A. Buckley, X. Chen, J. Cruz-Martinez, S. Ferrario Ravasio, T. Gehrmann, E. W. N. Glover,
S. HÃ¶che, A. Huss, J. Huston and J. M. Lindert, et al. A comparative study of Higgs boson
production from vector-boson fusion, JHEP 11, 108 (2021)

[44] ATLAS Collaboration, Performance of the missing transverse momentum triggers for the

ATLAS detector during Run-2 data taking, JHEP 08, 080 (2020).

[45] CMS Collaboration, The CMS trigger system, JINST 12, no. 01, P01020 (2017).

[46] CMS Collaboration, Performance of the CMS Level-1 trigger in proton-proton collisions at

ğ‘  = 13 TeV, JINST 15, no. 10, P10017 (2020).

âˆš

âˆš

[47] S. T. Roche, B. Carlson, and T. M. Hong, fwXmachina example: Missing transverse energy

regression, Mendeley Data, V1, 2022, http://dx.doi.org/10.17632/d4c94r9254.1.

[48] J. Alwall et al., The automated computation of tree-level and next-to-leading order diï¬€erential
cross sections, and their matching to parton shower simulations, JHEP 07, 079 (2014).

[49] T. SjÃ¶strand et al., An introduction to PYTHIA 8.2, Comput. Phys. Commun. 191, 159-177

(2015).

[50] S. Ovyn, X. Rouby, and V. Lemaitre, DELPHES, a framework for fast simulation of a generic

collider experiment, 2009, [hep-ph/0903.2225].

[51] DELPHES 3 Collaboration, DELPHES 3, A modular framework for fast simulation of a generic

collider experiment, JHEP 02, 057 (2014).

[52] M. Cacciari, G. P. Salam, and G. Soyez, The anti-ğ‘˜ğ‘¡ jet clustering algorithm, JHEP 04, 063

(2008).

[53] M. Cacciari and G. P. Salam, Pileup subtraction using jet areas, Phys. Lett. B 659, 119-126

(2008).

[54] M. Cacciari, G. P. Salam, and G. Soyez, The Catchment Area of Jets, JHEP 04, 005 (2008).

[55] ATLAS Collaboration, Readiness of the ATLAS Liquid Argon Calorimeter for LHC Collisions,

Eur. Phys. J. C 70, 723-753 (2010).

[56] M. Aharrouche et al., Energy linearity and resolution of the ATLAS electromagnetic barrel

calorimeter in an electron test-beam, Nucl. Instrum. Meth. A 568-2, 601 (2006)

26

Nanosecond ML regression with deep BDT in FPGA for HEP

[57] Y. A. Kulchitsky et al., Hadron energy reconstruction for the ATLAS barrel prototype
combined calorimeter in the framework of the nonparametrical method, JINR-E1-2000-73,
2000, [hep-ex/0004009].

[58] ATLAS Collaboration, Performance of missing transverse momentum reconstruction with the
ğ‘  = 13 TeV, Eur. Phys. J. C 78, no. 11, 903

âˆš

ATLAS detector using proton-proton collisions at
(2018)

[59] CMS Collaboration, Performance of missing transverse momentum reconstruction in proton-

proton collisions at

ğ‘  = 13 TeV using the CMS detector, JINST 14, no. 07, P07004 (2019).

âˆš

[60] A. Hoecker, P. Speckmayer, J. Stelzer, et al., TMVA - Toolkit for Multivariate Data Analysis,

CERN-OPEN-2007-007, 2007, [physics/0703039].

[61] B. Hawks, J. Duarte, N. J. Fraser, A. Pappalardo, N. Tran, and Y. Umuroglu, Ps and Qs:
Quantization-Aware Pruning for Eï¬ƒcient Low Latency Neural Network Inference, Front. Artif.
Intell. 4, 676564 (2021).

27

