DNNShield: Dynamic Randomized Model
Sparsiﬁcation, A Defense Against Adversarial
Machine Learning

Mohammad Hossein Samavatian, Saikat Majumdar, Kristin Barber, Radu Teodorescu
Department of Computer Science and Engineering
The Ohio State University, Columbus, OH, USA
{samavatian.1, majumdar.42, barber.245, teodorescu.1}@osu.edu

2
2
0
2

l
u
J

1
3

]

R
C
.
s
c
[

1
v
8
9
4
0
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—DNNs are known to be vulnerable to so-called
adversarial attacks that manipulate inputs to cause incorrect
results that can be beneﬁcial to an attacker or damaging to the
victim. Recent works have proposed approximate computation as
a defense mechanism against machine learning attacks. We show
that these approaches, while successful for a range of inputs,
are insufﬁcient to address stronger, high-conﬁdence adversarial
attacks. To address this, we propose DNNSHIELD, a hardware-
accelerated defense that adapts the strength of the response to
the conﬁdence of the adversarial input. Our approach relies on
dynamic and random sparsiﬁcation of the DNN model to achieve
inference approximation efﬁciently and with ﬁne-grain control
over the approximation error. DNNSHIELD uses the output distri-
bution characteristics of sparsiﬁed inference compared to a dense
reference to detect adversarial inputs. We show an adversarial
detection rate of 86% when applied to VGG16 and 88% when
applied to ResNet50, which exceeds the detection rate of the
state of the art approaches, with a much lower overhead. We
demonstrate a software/hardware-accelerated FPGA prototype,
which reduces the performance impact of DNNSHIELD relative
to software-only CPU and GPU implementations.

I. INTRODUCTION

Deep neural networks (DNNs) are rapidly becoming indis-
pensable tools for solving an increasingly diverse set of com-
plex problems, including computer vision, natural language
processing, machine translation, and many others. Some of
these application domains, such as medical, self-driving cars,
face recognition, etc. expect high robustness in order to gain
public trust and achieve commercial adoption. Unfortunately,
DNNs are known to be vulnerable to so-called “adversarial
attacks” that purposefully compel classiﬁcation algorithms to
produce erroneous results. For example, in the computer vision
domain, a large number of attacks [6], [37], [38], [18], [7],
[29], [43], [35] have demonstrated the ability to force state-of-
the art classiﬁers such as Inception[53], VGG[52], ResNet[20],
etc. to misclassify inputs that are carefully manipulated by
an attacker. In most attacks, input images are only slightly
altered such that they appear to the casual observer to be un-
changed. However the alterations are made with sophisticated
algorithms which, in spite of the imperceptible changes to the
input, result in reliable misclassiﬁcation.

Several defenses have been proposed to address adversarial
attacks [33], [42], [60], [12], [3], [34]. Most rely on purely
software implementations, with high overheads, limiting their

utility to real-world applications. A recent line of research
has explored hardware-assisted approximate computing to
introduce controlled errors into the inference process, either
through model quantization [15], [41] or approximate compu-
tation [19]. This inference approximation disrupts the effect of
the adversarial modiﬁcations, making the attacks less likely
to succeed. At the same time, if the errors are kept small,
approximate inference tends to have less effect on benign
inputs’ classiﬁcation accuracy.

We investigate the scalability of defensive approximation
approaches to a broader class of attacks. We ﬁnd that, while
approximation methods work well for some inputs, they do
not scale well to strong adversarial attacks that are trained to
have high classiﬁcation conﬁdence. This is because the noise
introduced through approximation is insufﬁcient to reverse
the adversarial effects. We also show that, even if noise is
increased, full recovery of strong adversarials is less likely.
We therefore argue that defensive techniques should focus on
detecting adversarial inputs, which has higher probability of
success, rather than recovery of the original class.

This paper presents DNNSHIELD, a hardware/software co-
designed defense that
takes a different approach to infer-
ence approximation and addresses some of the limitations
of prior approaches. DNNSHIELD is an online adversarial
detection framework that uses the effects of model sparsiﬁ-
cation to discriminate between adversarial and benign inputs.
DNNSHIELD runs both precise and sparse inference passes for
each input and compares their output. It then uses the deviation
in output classiﬁcation that is triggered by the sparsiﬁcation
to classify the inputs as benign or adversarial.

A key observation we make in this work is that tailoring
the approximation error rate to the conﬁdence of the
input classiﬁcation dramatically increases the adversarial
detection rate, while at the same time maintaining a low false
positive rate for benign inputs. DNNSHIELD is the ﬁrst work
to recognize the importance of this correlation for accurate
adversarial detection. Unlike prior work, DNNSHIELD dy-
namically and randomly varies the approximation error and
distribution. Dynamic approximation error is needed to adapt
to the conﬁdence of diverse inputs. Randomness in the error
distribution is crucial in ensuring that adversaries cannot re-
train to account for predictable inference noise.

 
 
 
 
 
 
To achieve these goals DNNSHIELD uses hardware-assisted
dynamic and random model sparsiﬁcation to implement ap-
proximate inference. Model sparsiﬁcation involves dropping
weights from the model, and has been used to improve
performance and energy efﬁciency [10], [17]. DNNSHIELD
controls the sparsiﬁcation rate dynamically to enable ﬂexible
control over the approximation error. Sparsiﬁcation is also
random to make the noise input independent and consequently
training defense-aware attacks difﬁcult.

DNNSHIELD demonstrates robust detection across a broad
set of attacks, with high accuracy and low false positive rate.
We show an adversarial detection rate of 86% when applied to
VGG16 and 88% when applied to ResNet50, which exceeds
the detection rate of the state of the art approaches. We also
show that DNNSHIELD is robust against attacks that are aware
of our defense and attempt to circumvent it.

DNNSHIELD requires multiple inference passes, increasing
inference latency. To mitigate this overhead we propose a hard-
ware/software co-designed accelerator aimed at reducing the
performance overhead. The accelerator design builds explicit
support for dynamic and random model sparsiﬁcation. The
DNNSHIELD accelerator is optimized for efﬁciently executing
sparsiﬁed models in which the sparsiﬁcation rate changes as
a function of the input – which is more challenging compared
to models for which weight sparsity is ﬁxed. We show that
the DNNSHIELD accelerator reduces the performance im-
pact of approximate inference-based adversarial detection to
1.53 × −2× relative to the unprotected baseline, compared to
15×–25× overhead for a software-only GPU implementation.

This paper makes the following contributions:
• First work to use DNN sparsiﬁcation as a dynamic and
random approximation-based defense against machine
learning attacks.

• First adaptive defense that adjusts the approximation error
in correlation with the conﬁdence of the classiﬁcation,
increasing detection accuracy even for strong attacks.
• Presents DNNSHIELD, a software/hardware-accelerated
co-design of the proposed defense and demonstrates its
robustness against defense-aware attacks.

• Evaluates DNNSHIELD through a proof-of-concept im-
plementation in the Xilinx CHaiDNN accelerator, show-
ing < 2.65% area overhead and < 5.1% power overhead.

II. BACKGROUND - ADVERSARIAL ATTACKS

Adversarial attacks were ﬁrst introduced by Szegedy et al.
in [54]. The objective of an adversarial attack is to force the
output classiﬁcation for some maliciously crafted input x(cid:48),
based on a benign input x. Attacks can be targeted, where the
adversary’s goal is for x(cid:48) to be misclassiﬁed as a particular
class t, or untargeted, in which misclassiﬁcation of x(cid:48) to any
other class is sufﬁcient. Targeted attacks were formally solving
the following optimization problem [54]:

min d(x, x+δ)subject to: C(x+δ) = t, x+δ ∈ [0, 1]n, (1)

where δ is the added noise,
label
for the adversarial example produced by x + δ, and d is a

is the desired target

t

metric to measure the distortion from benign example and the
adversarial one. Lp norm is widely used as distortion metric.
In this paper we refer to the L2-norm as the distortion metric
which is L2 = (cid:112)(cid:80)(x(cid:48) − x)2.

In this work we focus on two strong state of the art attacks.
The Carlini-Wagner (CW) attack [6] has been shown to be
very effective at creating adversarial images that looks very
similar to original inputs and is successful at attacking models
protected by defensive methods such as defensive distillation
[42]. The CW attack uses the loss function:

loss(x(cid:48)) = max(max{Z(x(cid:48))i : i (cid:54)= t} − Z(x(cid:48))t, −k)

(2)

where Z is the logit of classiﬁer and k is controlling the
conﬁdence with which the misclassiﬁcation occurs.

The EAD attack [7] generalizes the CW attack by perform-
ing elastic-net regularization, linearly combining the L1 and
L2 penalty functions. The EAD attack attains superior perfor-
mance in transfer attacks which can reduce the effectiveness
of defenses that are based on attack transfer such as [14], [15].
The EAD attack has also defeated several prior defenses [32],
[51], [49], [50],

III. MOTIVATION

Strong attacks such as CW and EAD can be tuned to
produce a class of adversarial inputs that present a signiﬁcant
challenge to approximation-based defenses. Prior work has
shown that adversarial inputs can be constructed to induce
misclassiﬁcation with very high conﬁdence [51], [4], [6]. In
other words, the victim model assigns a very high probability
to the adversarial input belonging to the wrong class. These
so-called “high conﬁdence” adversarials can be constructed
while minimizing the distortion to original input.

A. High-Conﬁdence Attack Variants

Figure 1 shows an example of multiple adversarial samples
levels of classiﬁcation
for a benign image with different
conﬁdence and the corresponding distortion. To measure clas-
siﬁcation conﬁdence we used the Z-score (the number of
standard deviations by which the value of a raw score is above
or below the mean value) of the maximum logit value, which
corresponds to the class with the highest conﬁdence. Adv1 is a
low distortion adversarial of the benign with low classiﬁcation
conﬁdence of 4.18. Adv2 is a high conﬁdence example of
the same input with very high classiﬁcation conﬁdence of 12.
While distortion is also higher, it is still imperceptible to the
naked eye. We will show that existing defenses are ineffective
against
this type of adversarial. Increasing the conﬁdence
beyond 12 increased distortion signiﬁcantly, as Adv3 shows.

B. Classiﬁcation Conﬁdence and Approximation

Approximate computing defense methods introduce noise
into the input and/or the model and often result in the recovery
of the original class. Figure 2 schematically illustrates how ap-
proximation can recover adversarial inputs. Figure 2 illustrates
the decision space of a classiﬁer, with four output classes: C1,
C2, C3 and C4, represented as regions with different colors.

2

to X1,
misclassiﬁed, resulting in false positives.

there is an increased probability that X1 will be

In order to address this limitation, we propose correlating
the approximation error to the conﬁdence of the classiﬁcation.
Figure 2 (c) illustrates this with circles of different radii: small
radius corresponding to lower approximation error for X1 and
Adv1(X1) – which are low conﬁdence classiﬁcations – and
larger radius (approximation error) for Adv1(X1) and a high
conﬁdence benign, X2. We can see that the low error does not
lead to misclassiﬁcation for X1, while recovering Adv1(X1)
with high probability. At the same time, a high error rate
will not lead to misclassiﬁcation for X2, while increasing the
probability of recovering Adv2(X1).

C. Approximation-Based Defenses

Prior work has used input noise and approximate inference
to improve model robustness against adversarial attacks. For
example, [23] and [8] have shown that adding some amount
of random noise to images can help DNNs correctly classify
adversarial inputs.

Recent work has proposed using hardware-based approx-
imation methods as similar defenses. Guesmi et al. [19]
proposed “Defensive Approximation” (DA) which used custom
approximate multipliers, to introduce controlled errors into
a CNN accelerator. Similarly, Fu et al. [15] used hardware-
assisted parameter quantization as the approximation mech-
anism. Model quantization is the process of reducing the
precision of the model parameter representation, and has been
used to improve performance, energy and storage efﬁciency
of DNNs. In [15] a 2-in-1 hardware accelerator dynamically
chooses between 12 quantization levels to use at inference,
introducing approximation into the model. While these ap-
proaches are effective and have low overheads, they use either
ﬁxed approximation error [19] or randomly-selected error from
a limited set of up to 12 precision levels [15]. In addition, both
techniques generate input-dependent noise, which an attacker
could reproduce to circumvent the defense.

In order to study the response of these approximation
techniques to high conﬁdence adversarials, we re-implemented
both the DA and the 2-in-1 defenses for VGG16 and ResNet50
models. The original DA defense uses a single AMA5 ﬂoating-
point based approximate multiplier. In order to explore a
wider range of approximation errors we use a set of seven
Int8 approximate multipliers from the Evoapproxlib library
[39]. We created approximated models using approximate
convolution (AxConv) implementations from [56], [40]. We
also used the quantized model from 2-in-1 [15] and gener-
ated high conﬁdence adversarial samples targeting two ﬁxed
quantization levels of 16 and 4-bit precision. We then tested
the 2-in-1 defense on these samples. We generated several sets
of adversarial samples with different classiﬁcation conﬁdences
by changing the k parameter in the CW and EAD attacks.

Figure 3 (a) shows correction rate (percentage of adversarial
examples that are correctly classiﬁed by the approximate
inference) vs. adversarial conﬁdence for two approximate

Fig. 1. Benign and adversarial examples with various distortion and conﬁ-
dence values.

The darkness of the color represents the conﬁdence of the
classiﬁcation. We consider a benign input X1 classiﬁed with
low in class C1, and an adversarial sample Adv1(X1) that is
misclassiﬁed into class C2 with low conﬁdence. The dotted
circles around each input represent the range of classiﬁcation
outcomes as a result of noise.

Low noise

High noise

Adaptive noise

Fig. 2. Schematic illustration of decision regions of a base classiﬁer. Different
classes are drawn in different colors with darker shades indicating higher
conﬁdence. X1 and X2 are benign inputs, Adv1(X1) and Adv2(X1) are
low conﬁdence and high conﬁdence adversarial inputs generated from X1.
The circles shows range of classiﬁcation deviation under a certain amount of
noise.

Prior work has observed that high-quality adversarial inputs
occur with low probability, which means they reside in small
and low-density pockets of the classiﬁcation space. As a
result, their output class distribution differs from that of their
closest data submanifold [13]. We can see in Figure 2 (a) that
Adv1(X1) resides in a narrow cone of class C2, where benign
images do not generally exist. This means that approximation
can easily change the classiﬁcation of Adv1(X1) and move it
back into its original class, C1.

In this work we show for the ﬁrst time, that high-conﬁdence
adversarials do not respond in the same way to approximation
errors. To illustrate why, let’s consider Adv2(X1), a high-
conﬁdence adversarial of X1. As Figure 2 (a) shows, for the
same error, the classiﬁcation area of Adv2(X1) lands within
region C2, failing to recover the original class C1.

Figure 2 (b) shows that, if we increase the approximation
error, the probability of recovering Adv2(X1) increases. How-
ever, if the same approximation error is applied uniformly

3

Distortion          -Confidence  5.28Benign  0.95 4.18Adv1 11.48 12.18Adv2 29.59 12.88Adv3High conﬁdenceLow conﬁdenceNoise regionStochastic  inferenceC1C2C3C1C2C3C4X1Adv1(X1)Adv2(X1)C1C2C3C4Adv2(X1)X1C1C2C3C4X2Adv2(X1)Adv1(X1)X1Fig. 4. Adversarial detection and benign FPR for ﬁxed versus adaptive
inference errors in VGG16.

detectable with high probability.

Figure 3 (b) shows adversarial detection for the same ad-
versarial inputs and defense methods. We can see that the de-
tection rate is much higher than correction for low-conﬁdence
adversarials, but still drops below 50% for adversarials with
conﬁdence greater than 8. F S performs better than other
methods, but it is still inadequate for high conﬁdence samples.
We will compare DNNSHIELD with FS in our evaluation.

These results show that approximation methods used in
prior work, which use ﬁxed error rates are insufﬁcient to detect
high conﬁdence adversarial examples.

D. The Case for Adaptive Approximation

The solution we propose in this work is to dynamically
adapt the level of error/approximation to the conﬁdence of
the classiﬁcation. To motivate an adaptive approximation over
simply increasing the approximation error, we conduct an
experiment in which errors are introduced directly into the
model, in the activation layers. Figure 4 shows the adversarial
detection rate and benign false positive rate (FPR) for ﬁxed
and adaptive error rates. We can see that ﬁxed errors below
50% have very low detection rates. Increasing the injected
error to 100% results in high detection rate, but at the cost of
an unacceptable 80% false positive rate. The Adaptive error
rate, correlated with the input conﬁdence, achieves both high
adversarial detection and low benign FPR.

The next research question we tackle is how to introduce a
well controlled, variable and randomly distributed approxima-
tion error into the inference process in a performance-efﬁcient
way. Unfortunately, approximate computation using approxi-
mate functional units does not offer sufﬁcient ﬂexibility to tune
the error rate since they are generally not easily tunable. The
same is true for quantization methods, which do not provide
sufﬁcient granularity for the approximation errors. In addition,
both quantization and approximate computation tends to be
deterministic, producing predictable and reproducible error
distributions that can be exploited by an attacker.

Fig. 3.
(a) Adversarial correction and (b) adversarial detection for different
defense methods versus conﬁdence of adversarial attacks on VGG16 and
ResNet50.

multiplier versions DA(125K) and DA(KEM), and the two 2-
in-1 adversarial variants. For reference, we also include two
software-only approximation techniques: Feature Squeezing
(FS) [51] and SAP [12]. We can see that most approximation
techniques perform relatively well with low and medium con-
ﬁdence samples. However, as adversarial conﬁdence increases,
correction rate drops below 20%. The software-only methods
perform the best, but they also have the highest overhead.
FS, which exhibits the highest correction rate, also has a 4×
performance/energy overhead.

Given the low correction rate of defense methods for high
conﬁdence adversarial inputs, we also evaluated an adver-
sarial detection approach. Except F S which has its own
detection methodology, for detection we simply compared
the classiﬁcation outputs of the unprotected and protected
models for the same input. If the outputs do not match, the
input is classiﬁed as adversarial. The intuition behind this
approach is that the classiﬁcation of adversarial examples is
more likely to change during approximate inference, although
the output classiﬁcation may not be the correct one. This
is especially important for very strong adversarial examples
which are far from decision boundaries (e.g. Adv2(X1) in
Figure 2), and approximation is unlikely to recover the correct
classiﬁcation, as observed in [22]. However, as we will show
in this paper, approximate inference is sufﬁcient to change
output distribution of all adversarials in a way that makes them

4

345678Z-scoreAdversarial confidence(a)0%20%40%60%80%100%Adversarial correction345678Z-scoreAdversarial confidence(b)0%20%40%60%80%100%Adversarial detectionDA(125K) [18]DA(KEM) [18]SAP [11]FS [50]2-in-1(16 bit) [14]2-in-1(4 bit) [14]5%25%45%100%AdaptiveApproximation error0%20%40%60%80%100%PercentageAdversarial detectionBenign false positiveIV. DNNSHIELD DEFENSE DESIGN

In order to address the aforementioned challenges we in-
troduce the DNNSHIELD framework, which includes a new
ﬂexible and efﬁcient mechanism to add controlled approxi-
mation into the model inference, a method to correlate the
amount of error introduced into the model to the conﬁdence
of the non-noisy classiﬁcation and a mechanism for using the
approximate inference to detect adversarial inputs.

Fig. 5.

(a) Baseline accelerator, (b) DNNShield accelerator tile.

A. Dynamic Random Sparsiﬁcation

We set a few important criteria for our approximate infer-
ence design: (1) ﬂexibility to tune the error rate dynamically
at runtime – to allow error rate to be correlated to input
conﬁdence, (2) randomness of the error distribution – to make
defense-aware attacks less likely to succeed, and (3) low
overhead.

In order to satisfy these criteria DNNSHIELD introduces
noise into the DNN by randomly ”dropping” (essentially

5

ignoring) weights from the model,
through a process we
call dynamic random sparsiﬁcation. The fraction of dropped
weights, or sparsiﬁcation rate (SR) controls the amount of
error in the model. The sparsiﬁcation rate for each input is
determined based on the classiﬁcation conﬁdence of the non-
noisy run of that input.

The main advantage of dynamic random sparsiﬁcation is
the potential reduction in performance overhead. Prior work on
sparse convolution accelerators [9], [61], [27], [44], [24], [11],
[25], [45], [36] targeted statically weight sparsed models with
the ultimate goal of improving performance/energy efﬁciency.
However, exploiting sparse ﬁlters is more challenging in
the case of DNNSHIELD because the ﬁlter sparsity changes
randomly from run to run. To address these new challenges
we developed a hardware/software co-design that proﬁles the
model ﬁrst and performs scheduling for efﬁcient resource uti-
lization. The hardware accelerator supports dynamic-random
sparse execution of the inference with minimum stalls with
the help of the software scheduler and ﬂexible control ﬂow.

B. DNNShield Accelerator

Figure 5 shows the DNNSHIELD accelerator tile (b) com-
pared to a baseline Dense CNN accelerator (a). The baseline
design consists of N tiles which share k ﬁlters. Each tile shares
the same set of inputs and consists of k × m MAC units
which perform k × m 8-bit multiplications per cycle. After
a total of k × M (M = ﬁlter size) MAC operations, tree-
adders accumulate m partial results per output and generate
k outputs per tile. The baseline accelerator processes all
available weights uniformly, assuming no sparsity. Since the
DNNSHIELD random sparsiﬁcation is a dynamic process,
using conventional sparse accelerators is not practical since
they require deterministic ofﬂine preprocessing of the statically
sparsed model to utilize the resources efﬁciently at runtime.
Our DNNSHIELD accelerator consists of two components: 1)
software scheduler and 2) hardware accelerator.

1) DNNShield Software: The DNNSHIELD software han-
dles two main tasks: 1) one-time ofﬂine proﬁling and 2)
runtime scheduling (Figure 6). In order to reduce the overhead
of online sparsiﬁcation, the DNNSHIELD software parses the
model ofﬂine and generates a table of threshold values for
each ﬁlter, corresponding to different sparsiﬁcation rates 1 .
At runtime, the SR distribution generator generates random
sparsiﬁcation rates for each ﬁlter 2 , extracts the correspond-
ing threshold value for each ﬁlter from the threshold array, and
creates a threshold vector 3 . The threshold values are used
by the ﬁlter sparsiﬁer to drop/ignore weight values below the
thresholds assigned to each ﬁlter 4 . At the same time, a bit
mask of active weights is generated, and will be used by the
hardware to map the correct inputs to the active weights 5 .
Finally, the scheduler groups the ﬁlters with roughly similar
number of active weights and places them in the task queue
increase the efﬁciency of the inference run
6 . This will
since the ﬁlters scheduled in the same group will require
similar numbers of multiply-accumulate operations, reducing
load imbalance and the number of idle cycles. Finally each

Ofmap1......MMMM........Filter1FilterkShare k filters......Ofmapk.......................++Tile.........MMMM........Filter1Filterk......Tile............Input window......Active weightbit mask bufferMUX signallogic 1MUX signallogic k...Input window shifterShare within tiles(a)(b)Weight MemoryActivationMemoryFig. 6. DNNSHIELD software consisting of ofﬂine model proﬁling and runtime ﬁlter sparsiﬁcation and scheduling.

0 and w3

7 shows 4 ﬁlters sharing the input line within the tile. At
cycle 1 I0 is ready to be used by the MAC units sharing the
input, however, only two ﬁlters have corresponding weights
active (w2
0). While the baseline accelerator leaves two
MAC units underutilized, DNNSHIELD fully utilizes the MAC
units by performing 4 multiplication at cycle 1 due to the
ﬂexibility of selecting appropriate input through MUXes on
top of each MAC unit. Therefore w0
1 are consumed
by the MAC unit in the same cycle. Since no ﬁlter needs
I0 or I1 the next two inputs I2 and I3 will be loaded into
the input buffer in cycle 2. The non-deterministic scheduling
of ﬁlter groups at runtime prevents pre-generating the signals
that drive the selection multiplexers shown in Figure 7 (b).
DNNSHIELD instead uses a ”MUX signal logic” unit that
uses the bit mask of active weights produced by DNNSHIELD
scheduler to dynamically generate the control signals.

1 and w1

Fig. 7. DNNSHIELD sparse weights dataﬂow example through dense baseline
(a), DNNSHIELD accelerator (b).

group is sent to the accelerator together with the active weight
bit mask 7 .

2) DNNShield Hardware: The DNNSHIELD accelerator
modiﬁes the baseline dense accelerator to leverage the dynam-
ically sparsiﬁed model. The DNNSHIELD scheduler attempts
to schedule the k ﬁlters with approximately the same number
of active weights. However, DNNSHIELD needs to make sure
that different weights in each kernel can get access to their
corresponding input with minimum stalling. For this purpose
we used a look-ahead mechanism similar to that in [36] to
match inputs and weights. Figure 5-b shows the DNNSHIELD
accelerator tile. The MUXes are added per each MAC unit to
deliver the correct inputs to the active weights in each ﬁlter.
The accelerator uses the active bit mask to generate the MUX
select signals and also identify the offset by which the input
window will be shifted every cycle.

V. DYNAMIC NOISE AND ADVERSARIAL DETECTION

A. Impact of Noise Rate on Classiﬁcation Output

In order to understand the impact of model approximation
on classiﬁcation output, we use the L1-norm metric to measure
the difference between noisy and non-noisy outputs for the
same inputs. The L1-norm (also known as Manhattan Dis-
tance) is the sum of the absolute pair-wise differences between
elements of two vectors. We refer to this difference as the
Classiﬁcation Probability Deviation under Noise (CPDN).

For this analysis we randomly select 1000 benign images
from ImageNet. We further generate a total of 1000 adversarial
images using 10 different attacks (or attack variants). We run
each input 8 times with different noise distributions to generate
8000 sample points for benign as well as adversarial inputs.
We measure the CPDN for each input. Figure 8 shows a 3D
representation of CPDN for varying classiﬁcation conﬁdence
levels and average noise rates, for benign (a) and adversarial
samples (b). Figure 8 shows that across the entire sample
space of 8000 data points, CPDN is consistently higher for
adversarials compared to benigns, conﬁrming lower robustness
to noise for adversarials.

Figure 7 shows an example of how sparse weights and their
corresponding inputs ﬂow through for baseline accelerator
(Figure 7 (a)) and through DNNSHIELD (Figure 7 (b)). Figure

The data also shows that benign inputs, while more robust
to noise than adversarials, are sensitive to high levels of noise
if their classiﬁcation conﬁdence is low. This can be observed

6

......#F filters1%...95%Profiling5 activeweights6 activeweightsModel1.........ThresholdArray2Filter1Filter2...3Filter sparsifyingSparsification thresholds vector4FilterFActiveweight bitmaskgenrator20 activeweights6SchedulingGroup1Group2...Task queue...001001...100010...RuntimeSparsification rate (SR)28 activeweights5DNNSheild AcceleratorFilter3OfflineThreshold indexesvectorSR distributiongenerator7ExecutionMMMMMMMMMActiveMMMMMMMMMIdle(a) Baseline accelerator: 2 Mul/cycle(b) DNNShield accelerator: 3.5 Mul/cycleCycle1Cycle24x2bitin Figure 8 (a) from the high CPDN at high noise and low Z-
score. This suggests that low-conﬁdence benign inputs should
receive lower noise. Figure 8 (b) shows that low-conﬁdence
adversarials exhibit relatively high CPDN even at low noise
levels. If we turn our attention to high conﬁdence adversarial
samples, we see that they require higher noise levels to exhibit
high CPDN. The same high levels of noise, however, when
applied to high conﬁdence benigns exhibit a much lower
CPDN, indicating that they are robust to high noise. This
data suggests that correlating the noise to the conﬁdence of
the classiﬁcation is important to using CPDN in adversarial
detection.

Figure 9 shows the CPDN distributions for benign and
adversarial images when noise is correlated to the conﬁdence
of the non-noisy classiﬁcation – higher noise for higher
conﬁdence. Figure 9 shows a clear separation between the
CPDN distributions of adversarial and benign images, across
the entire range of classiﬁcation conﬁdence that our attacks can
generate. As expected, adversarial inputs exhibit higher CPDN
deviation from non-noisy baseline compared to benign inputs.
DNNSHIELD uses this separation with appropriate thresholds
to detect adversarial inputs.

B. Robustness Analysis

In order to understand why it is important to correlate ap-
proximation noise to classiﬁcation conﬁdence, let us consider
a model that classiﬁes input X in the most probable class C1
with probability P1 and the runner-up class C2 with probability
P2. Cohen et al. [8] showed that the distance between P1 and
P2 has a direct correlation to the amount of the noise around
X that can be tolerated by the classiﬁer.

The L2 radius R around X can be calculated by:

R =

σ
2

(Φ−1(P1) − Φ−1(P2))

(3)

where Φ−1 is the inverse of standard Gaussian CDF and σ is
the standard deviation of the noise. The higher the R value
for an input X, the more noise the classiﬁer can tolerate and
still classify X correctly. According to Equation 3 the radius
R is large when the probability of top class C1 is high and
the probability of the next class is low, which corresponds to
high conﬁdence classiﬁcation. This shows robustness to noise
is correlated to classiﬁcation conﬁdence. We therefore expect
that benign images will not suffer from high false positive rate
regardless of conﬁdence, if the approximation noise is scaled
with the conﬁdence.

C. Adversarial Detection

The DNNSHIELD framework relies on the CPDN metric to
detect adversarial inputs. This process is depicted in Figure
10. Initially, a ﬁrst inference pass through the network is
performed without noise injection to establish a reference.
The output classiﬁcation is recorded as P b. This is followed
by another approximate inference pass. The conﬁdence of the
classiﬁcation P b is used to determine the amount of noise to
be injected.

Fig. 8. Classiﬁcation Probability Deviation under Noise (CPDN) distribution
under variable noise rate and classiﬁcation conﬁdence for (a) benign and (b)
adversarial samples, for VGG16.

Fig. 9. CPDN distributions for benign and adversarial images when noise
is correlated to the conﬁdence of the non-noisy classiﬁcation for VGG16.

7

conf. (z-score)456789Avg noise rate0%25%50%75%100%CPDN (L1 distance)0.00.51.01.52.0Benignconf. (z-score)45678Avg noise rate0%25%50%75%100%CPDN (L1 distance)0.00.51.01.52.0Adversarial468Z-scoreAdversarial confidence0.00.51.01.52.0CPDN (L1 distance)t02t2t1t01BenignAdversarialFig. 10. Adversarial detection using CPDN (L1 distance).

The L1 distance between the output vectors of the noisy
(P N ) and non-noisy (P b) inference passes is computed. The
L1 distance is then compared with different thresholds values.
Depending on the outcome, subsequent inference passes may
be required. Figure 9 shows the 4 thresholds used by the
detection mechanism overlaid on the L1 distance distribution
for VGG16. t1 and t2 represent the L1 distance below/above
which most benign/adversarial images fall, respectively. t(cid:48)
1 and
t(cid:48)
2 represent tighter thresholds below/above which about 80%
of the benign/adversarial images fall.

1 or > t(cid:48)

If the measured L1 distance is either very high or very
low (< t(cid:48)
2) the input image can immediately, and
with high conﬁdence, be classiﬁed as benign or adversarial,
respectively. Most inputs (>≈ 80%) from both our benign
and adversarial test sets fall in this category. In this case the
detection algorithm terminates and the outcome is reported.

Otherwise, DNNSHIELD cannot yet make

a high-
conﬁdence detection, and another noisy inference pass is
required. The average L1 distance over all the previous noisy
runs is computed and compared with the more conservative
thresholds t1 and t2. The images with average L1 < t1 are
classiﬁed as benign and those with average L1 > t2 are
classiﬁed as adversarial. If t1 < L1 < t2, a new noisy pass
is performed and the average L1 distance is recomputed. The
algorithm repeats until a maximum M number of iterations
is reached (M = 4 and M = 8 in our experiments). If a
classiﬁcation is still not possible, the algorithm defaults to
Benign.

Fig. 11. DNNSHIELD hardware design based on the Xilinx CHaiDNN
accelerator.

Fig. 12. DNNSHIELD Runtime integrated with CHaiDNN software.

VI. DNNSHIELD PROTOTYPE IMPLEMENTATION

As a proof-of-concept, we implement DNNSHIELD in a
FPGA-based DNN accelerator, the Xilinx CHaiDNN archi-
tecture [59]. Figure 11 shows a diagram of the design. The
baseline includes dedicated hardware for Convolution Pooling,
and Deconvolution functions. All the compute elements are
connected to a Memory Interface Block which allows access
to the on-chip SRAM as well as the main system DRAM via
a custom AXI Interconnect. DNNSHIELD augments the base-
line accelerator with the following components color coded
blue: (1) modiﬁed convolution supporting noisy sparsiﬁcation
including MUX select generator logic, input window buffers
and priority encoders, (2) custom logic for computing CPDN,
and (3) additional control logic for coordinating partial result
reuse and early termination.

Convolution is the core of noisy sparsiﬁcation with MUXes
distributed through PEs for and MUX signal generators. PEs
within the column share MUX signals. Also, priority encoders
are used to determine the number of inputs that need to be
bypassed regarding the ignored weights in sparsiﬁcation. Then
the address offset is calculated and the next set of inputs will
be loaded in the input buffer.
CPDN Unit is used to compute the CPDN between a noisy
and non-noisy output. It primarily consists of logic to subtract
vector elements and accumulate the absolute value of the
result, shown in Figure 11.
DNNShield Runtime: The CHaiDNN/DNNSHIELD software
stack as shown in Figure 12, includes of a parser for the input
model and a pre-processor for the DNN inputs. DNNSHIELD
proﬁler is added to the ChaiDNN parser to create the thresh-

8

BaseModelNoisy SparsifiedModelPNPbNoise level12DeconvolutionPoolingConvolutionCPDN UnitOn-Chip SRAMAXIAcceleratorDRAMAXICustom Adapterfor AXIInterconnectCustom Adapterfor AXIInterconnectCustom Adapterfor AXIInterconnectCustom Adapterfor AXIInterconnectMemoryInterfaceBaseline CHaiDNNScale and BiasElement-wiseAdditionDilated ConvolutionBatch NormalizationReLUConvolutionSubtractorAccumulatorInput0Input1OutputCHaiDNN+Defense HardwarePEPEPEPEPEPEPEPEDNNShieldEmbeddedARM CPUMask bitarrayMUX select generatorlogicPriority  encoderMACMUXWeight bufferIFmap window  bufferIFmap memory  address offsetCaffe ModelCHaiDNN ParserSchedulerHardwareAcceleratorExecute on Processor(For SoftwareOptimized Layers)Pre-Processingand InitializationPost-ProcessingDefense APIHandlerImagesCPDNcheckBenign/ AdversarialRepeatNoisy passDefense  Handler: Informs thescheduler if the current layer to beexecuted  is a noisy or a non-noisy layer based onits ID and noisy_layer_mask[].CHaiDNN+Defense SoftwareadversarialDetection(L1_threshold,                                    L1_distance):     return Benign/              Adversarial/               Resume_noisyClass customDNNShieldAPI(*args):        get_L1_threshold():                 return L1_threshold[];        get_noisy_layer_mask():                        return noisy_layer_mask[];       noise_regulation(*args):                 return noise_range; float  noise_range bool noisy_layer_mask[]:Boolean Array indicating if a layeris noisy or non-noisyfloat L1_threshold[]: array ofthreshold for detection Create_Threshold_Arrays(Model):             return Threshold_Arrays[];DNNSHIELDProfilerConvolutionTop (input_buffers,weight_buffers, output_buffers,bias_buffer, params, active_weight_bm)Generate_SR (nu_filter, noise_rane):          return threshold_val[nu_filter] Create_Schedule_groups(threshold_val):          return active_weights, bitmask  old array that
later will be used by DNNSHIELD. The
DNNSHIELD Runtime is invoked by the pre-processor fol-
lowing the initial non-noisy run to determine the noise level
to be injected. The DNNSHIELD handler is designed to invoke
the scheduler when the CPDN needs to be computed in the
hardware. It also augments the scheduler with DNNSHIELD
scheduler for grouping the balanced ﬁlter together for more
efﬁcient execution.

VII. METHODOLOGY
We implement, synthesize and deploy DNNSHIELD on
CHaiDNN running on a Xilinx Zynq UltraScale+ FPGA. The
SoC associated with the board is ZU7EV which integrates a
quad-core Arm Cortex-A53 processor. CHaiDNN is an open-
source Deep Neural Network accelerator designed to run on
Xilinx Ultrascale MPSoCs. We compare our FPGA accelerator
with two software implementation of DNNSHIELD on a CPU
and GPU. We implemented the software DNNSHIELD using
TensorFlow2 [1]. We run our software DNNSHIELD on Intel
Core-i7 CPU@3.40GHz and NVIDIA RTX-2060 Turing GPU.

A. DNN Models, Input Dataset, Attacks

We used two networks VGG16 [52] and ResNet50 [20]
trained on ImageNet [28] for running attacks and generating
adversarial images. Targeted attacks, which aim to misclassify
an input into a target class, use two types of targets called
Next and LL. Next corresponds to choosing the target class
t = L + 1 mod #classes where L is the sorted index
of top ground truth classes. For LL the target class t is
chosen as least likely class (t = min(ˆy)) where ˆy is the
prediction vector of an input image. Table I summarizes the
adversarial attacks alongside their detailed parameters, success
rate, average conﬁdence and average distortion with different
metrics per model.

B. Comparison with Existing Defenses

We compared our detection rate of adversarial as well as
True/False Positive rate (TPR/FPR) of benign images with
two state-of-the-art post-training defense mechanisms, detailed
below. Stochastic Activation Pruning (SAP) [12] introduces
randomness into the evaluation of a neural network to de-
fend against adversarial examples. SAP randomly drops some
neurons of each layer to 0 with a probability proportional
to their absolute value. Values which are retained are scaled
up to retain accuracy. Feature squeezing (FS) [60], [31]
is a correction-detection mechanism that relies on reducing
the input space (and attack surface) by ”squeezing” images.
FS requires off-line proﬁling and training to ﬁnd the best
squeezer and corresponding thresholds for each pair of data-
set and attack, making it less practical to deploy in real-world
applications. FS also requires at least 3 squeezers, resulting
in at least 4× performance overhead. For a fair comparison
we retrained FS on our set of benign and Adversarial im-
ages for both VGG16 and ResNet50 separately. Approximate
mul8u KEM: We also compared with an approximate multi-
plier (Approx. mul8u KEM) from [39], in an approach similar
to [19].

VIII. EVALUATION

We evaluate DNNSHIELD adversarial detection rate, ro-
bustness to defense-aware attacks, and performance and area
overheads. We also conduct a number of sensitivity studies for
the main design parameters.

A. Adversarial Detection

We ﬁrst look at DNNSHIELD’s ability to identify adversar-
ial images. We measure the detection rate for adversarial inputs
as well as the false positive rate (FPR) for benign inputs. We
compare DNNSHIELD with Feature Squeezing (FS) and SAP
for multiple conﬁgurations of CW and EAD. Table I lists the
detection rate for all the attack variants we evaluate, for both
VGG and ResNet.

The results show that both DNNSHIELD signiﬁcantly out-
performs both FS and SAP on average. DNNSHIELD shows an
average detection rate of 86% and 88% for VGG and ResNet,
respectively. DNNSHIELD also signiﬁcantly outperforms the
state of the art defense, FS which averages 55% and 79%
for VGG and ResNet, respectively. This is especially true for
high-conﬁdence attack variants, for which FS does not work
as well. For instance, under the EADL1 attack with k = 70
we see 93% detection rate for DNNSHIELD vs. 4% for FS
(VGG16). This shows that DNNSHIELD is resilient to very
strong attacks.

Figure 13 shows detection rate versus adversarial conﬁ-
dence for DNNSHIELD, FS and the Approx. mul8u KEM
as a function of classiﬁcation conﬁdence. Both FS and Ap-
prox. mul8u KEM detection rates fall steeply as conﬁdence
increases while DNNSHIELD detection rate remains high.
These results re-emphasize the importance of adapting the
approximation error to the conﬁdence of the classiﬁcation.

B. DNNShield-Aware Attacks

In order to investigate the robustness of the DNNSHIELD
defense, we construct a set of attacks tailored speciﬁcally
to defeat
it. These attacks assume full knowledge of the
DNNSHIELD design. In theory, DNNSHIELD could be de-
feated by an attack that generates adversarial examples for
which the model’s robustness to approximate inference is
similar to that of benign examples. In order to attempt to
generate such adversarial examples, we used the approach
suggested in [55] to generate adversarial examples that target
the probability vector of an arbitrary benign example from
another class. The idea is to create an adversarial example that
mimics the response of benign images under noise. Hence,
for sample x of class y, we pick a target t (cid:54)= y and create
adversarial example x(cid:48) that minimizes the objective:

minimize

||y(x(cid:48)) − y(xt)||1

(4)

where y(x(cid:48)) and y(xt) are the probability vector of the
adversarial and target inputs respectively. While we try to
minimize the L1 distance between adversarial and the benign

9

TABLE I
ATTACK PARAMETERS FOR MULTIPLE VARIANTS OF CW AND EAD ATTACKS. ORIGINAL ATTACK SUCCESS RATE, CONFIDENCE, AND DISTORTION.
DETECTION RATES FOR 4 DEFENSES: SAP, FS, APPROXIMATE-MUL AND DNNSHIELD. DATASET FROM IMAGENET.

Attacks

Defenses

Distortion

Param k

Mean Conﬁdence

5

30

10

70

140

L0

CWL2

CWL0

Attack

Target

RNet
94.5%
87.9%

RNet
42.0%
42.1%

VGG
42.2%
42.4%

Next
LL
Next
LL
Next
LL
Next
LL
Next
LL
Next
LL
Next
LL
RNet: ResNet50, VGG: VGG16

L1
VGG RNet
VGG RNet VGG
232
92.9%
5
84.8%
382
100.0% 100.0% 100.0% 100.0% 412
100.0% 100.0% 100.0% 100.0% 555
100.0% 100.0% 99.9%
100.0% 100.0% 99.9%
100.0% 100.0% 735
95.3%
94.7%
91.8%
99.9%
93.4%
100.0% 100.0% 52.4%
100.0% 100.0% 54.7%
100.0% 100.0% 78.8%
100.0% 100.0% 78.3%
100.0% 100.0% 47.9%
100.0% 100.0% 48.0%
+FS threshold: VGG16: 1.022, ResNet50: 1.229,

100.0% 1,023
58.3%
59.4%
80.5%
85.5%
52.4%
57.4%

105
185
411
531
2,609
3,267
523
699
205
276
1,734
2,782
421
596

173
269
1,400
1,510
191
252

100.0% 2,015
100.0% 2,176

EADEN

EADL1

CWL∞

140

10

30

30

70

10

5

5

∗Values for SAP are accuracy

L2
VGG RNet
6.26
10.44
8.33
13.65
1.63
1.69
2.07
2.24
9.22
7.43
11.56
8.07
1.6
2.27
2.12
3.05
2.79
2.69
3.47
3.56
12.34
9.92
17.36
10.65
6.73
4.36
8.59
5.9

L∞
VGG RNet
0.88
0.94
0.92
0.96
0.06
0.07
0.06
0.08
0.2
0.24
0.24
0.25
0.01
0.01
0.01
0.01
0.22
0.24
0.26
0.29
0.49
0.54
0.58
0.55
0.73
0.6
0.8
0.72
AVG
FPR

FS+

RNet

SAP∗

RNet
82%

68% 11%

DNNSHIELD

Approximate
mul8u KEM
VGG
VGG RNet VGG
RNet
VGG
100% 100% 34% 59% 67%
42%
35%
100%
43%
29%
84%
45%
45%
100%
46%
36%
6%
21%
0%
9%
17%
0%
91%
47%
42%
100%
45%
45%
78%
52%
27%
100%
41%
34%
4%
16%
0%
4%
18%
0%
63%
40%
17%
98%
12%
26%
16.6% 33.2% 55%
6%
37%
58%

100% 56% 96% 100% 100%
91% 100%
89%
100% 81% 28% 100% 100%
89%
84%
7%
0%
48%
98%
96%
67%
19% 0%
96% 69% 71% 83%
89%
100% 94% 98% 100% 100%
98%
98%
98%
46%
81%
89%
98%
79%
3%

91%
41% 2%
100% 100%
59% 0%
88%
78%
0%
2%
84%
93%
0%
4%
80% 94%
27% 0%
99%
97%
36% 2%
88%
36% 15% 86%
6%
29% 19% 6%

Fig. 14. L1 distance vs. L2 distortion for different β values.

A4, with β = 10−1 the detection rate is lower. However, in
order to generate A4, the L2 distortion has to be increased by
4-5× relative to A3. To understand why, Figure 14 shows the
effect of β on the L1 distance of probability distribution and
L2 distortion. Optimizing for both low L2 distortion and L1
distance are competing objectives. Increasing β will decrease
the L1 distance, making the adversarial harder to detect, but it
also increases L2 distortion. The target benign input, which the
adversarial sample is trying to mimic, is chosen randomly from
1000 images in the adversarial targeted class. While a few of
these targets do lead to lower distortion, the average distortion,
for high β (10−1), is very high. Another popular approach
is using an EoT attack in which noise (transformation) was
applied during adversarial generation. We injected variable
noise correlated to the conﬁdence of the classiﬁcation in
each training iteration, as in DNNShield. The result was that,
because of the variable noise, the attack could not converge
on a successful adversarial. Using ﬁxed noise as in traditional

Fig. 13. Detection rate for different adversarial conﬁdence generated by
CWL2 attack, (a) VGG16 and (b) ResNet50
.

target, we need to also minimize the adversarial perturbation
under the L2 distortion metric. The ﬁnal objective function is:

minimizex

cf (x, t) + β||y(x(cid:48)) − y(xt)||1 + ||x − x0||2
2
such that x ∈ [0, 1]n

(5)

where f (x, t) denotes the loss function and β is the regulariza-
tion parameter for L1 penalty. Increasing β forces a lower L1
distance between the adversarial and target benign and could
evade DNNSHIELD detection.

Table II summarizes the adaptive attack parameters and
detection rates under DNNSHIELD. We can see that for low-β
attacks, DNNSHIELD detection rate is very high (A1-A3). For

10

6.57.07.58.08.5Z-scoreAdversarial confidence(a)050100Detection rateVGG169.510.010.511.0Z-scoreAdversarial confidence(b)050100Detection rateResNet50FSApprox.KEMDNNShieldCWNextCWLL10−410−310−210−1012L1 distanceVGG1610−410−310−210−1012L1 distanceResNet5010−410−310−210−1β02040L2 distortionVGG16ResNet50EoT did not work either because of the adaptive DNNShield
response.

TABLE II
DNNSHIELD AWARE ADAPTIVE ATTACKS

Attack
A1
A2
A3
A4

β

10−4
10−3
10−2
10−1

Success rate
RNet

Mean Conﬁdence
VGG

VGG
RNet
100% 100% 94.3% 95.9%
100% 100% 92.3% 94.0%
100% 100% 96.1% 96.9%
97.7% 97.9%
58%

31%

L2 Distortion
RNet
VGG
2.71
3.91
1.42
2.48
8.45
10.14
46.99
41.58

DNNSHIELD det.
VGG
99%
99%
95%
81%

RNet
100%
100%
84%
39%

Fig. 15. Adversarials generated by DNNSHIELD-aware attacks.

Figure 15 shows two examples of adversarial inputs gener-
ated with different β values. We can see that distortion artifacts
are clearly visible for β = 10−1, and can be detected through
other means.

At high β values, the attack is also less likely to succeed. For
β = 10−1, only 58% (VGG) and 31% (ResNet) of examples
can be converted into adversarials that defeat the unprotected
baseline. DNNSHIELD is still able to detect 81% and 39% of
the VGG and ResNet ones, respectively.

This shows that DNNSHIELD is robust to defense-aware

attacks that optimize for low L1 distance.

C. Performance, Area and Power Overheads

We next examine the performance, area and overheads of
the DNNSHIELD framework. Figure 16 shows the average
normalized run time of DNNSHIELD on the GPU, CPU and
DNNSHIELD accelerator. The runtime overhead of software
DNNSHIELD on GPU is 15× to 25× higher than the baseline.
This high overhead is primarily due to the random number
generation function used by the dynamic sparsiﬁcation algo-
rithm – which does not appear to be optimized on the GPU
– and is called when sparsifying each ﬁlter. This overhead
is highlighted by the ”DNNSHIELD overhead”, shown as a
pattern in Figure 16.

In contrast, the overhead of the DNNSHIELD accelerator
implementation is much lower at 1.53× and 2× for ResNet50
and VGG16, respectively. Unlike the GPU, the DNNSHIELD
accelerator performance overhead is primarily due to re-
execution of the approximate inference. While not trivial, the
DNNSHIELD performance overhead compares favorably with
that of FS which exceeds 4×. For software-DNNSHIELD on
the CPU the overhead ranges from 2.43× to 4.47× which
is again, higher that for DNNSHIELD. In addition, the total
runtime of the models on the CPU is dramatically longer
than the FPGA. Very slow runtime of convolutional and FC
layers on CPU dominate execution time. Hardware support
for dynamic sparsiﬁcation reduces overhead by 15% and 30%
relative to the DNNSHIELD without sparsiﬁcation support.

Fig. 16. DNNSHIELD runtime on (a) GPU, (b) CPU and (c) DNNSHIELD
accelerator for VGG16 and ResNet50.

Table III summarizes the area and power overhead of
the combined DNNSHIELD hardware relative to the baseline
CHaiDNN accelerator. We can see that the total overhead is
low, with FPGA resource utilization increasing by at most
2.56%. Power overhead is higher, but still small at 4.5%
dynamic.

TABLE III
FPGA RESOURCES AND POWER OVERHEAD OF DNNSHIELD OVER
BASELINE CHAIDNN ACCELERATOR.

Resource
BRAM
DSP
FF
LUT
URAM
BUFG
PLL
Power
Static
Dynamic

Baseline
202.5
696
112501
158060
80
3
1
Baseline
0.721W
5.567W

DNNSHIELD
204
696
113630
159381
80
3
1
DNNSHIELD
0.726
5.822W

Overhead%
0.75%
0.0
1.1%
0.8%
0.00
0.00
0.00
Overhead%
0.6
4.5

D. Sensitivity Studies

The DNNSHIELD design spans a broad design space that
affects performance overhead for adversarial detection accu-
racy.

1) Sparsiﬁcation Approaches: We evaluate multiple ap-
proaches for dynamic sparsiﬁcation. The naive approach of
randomly dropping any weight subject to the sparsiﬁcation rate
(SR) results in, as Figure 17 shows, a very high (> 90%) false
positive rate (FPR) for benign inputs, indicating that random

11

Benignβ=10−4β=10−3β=10−2β=10−1VGG16ResNet50(a) GPU software DNNShield01020Performance overhead0.003s0.014s0.073s0.202sVGG16ResNet50(b) CPU software DNNShield024Performance overhead1.919s0.530s4.383s1.586sVGG16ResNet50(c) DNNShield accelerator024Performance overhead0.224s0.098s0.454s0.150s0.528s0.206sFSBaselineDNNShieldDNNShield overheadDNNShield w/o dynamic sparsification supportFig. 17. Adversarial detection and benign FPR with different sparsiﬁcation
approaches.

Fig. 18. Adversarial attack success rate for multiple attacks as a function of
the number of noisy runs in DNNSHIELD.

weight sparsiﬁcation results cannot be used to discriminate
adversarial inputs. This is because random sparsiﬁcation can
result in the dropping of large weight values, with large impact
on classiﬁcation output. To address this issue, in DNNSHIELD
we drop a random number of weights between 0 and SR from
each ﬁlter, in ascending order of their values. This results
in high adversarial detection, with low benign FPR. This is
mostly due to the fact that dropping weights in ascending order
enables more precise control over the approximation error. We
also show that adapting the SR to classiﬁcation conﬁdence
is very important. The High SR and Low SR experiments in
Figure 17 show the effects of weight dropping at ﬁxed rates of
up to 80% and 20% respectively. The Low SR is insufﬁcient
to achieve adversarial detection, while ﬁxed 80% results in
very high benign FPR.

2) Detection Convergence: Figure 18 shows the attack
success rate as a function of the number of runs with inference
approximation. More runs should ensure higher detection
accuracy by generating more samples for the L1 distance aver-
age. We can see that the attack success rate drops rapidly after
1-2 noisy runs, and remains mostly constant after that. This
translates in DNNSHIELD converging rapidly on a detection
decision. A single noisy run is sufﬁcient for >80% of the
benign inputs, and less than 10% require more than 2.

3) Detection Thresholds: Finally, we performed a sensitiv-
ity analysis on the threshold parameters used for adversarial
detection. To study the effect of detection thresholds, we
varied t(cid:48)
1 in the [0.05, t1] range in 0.1 increments. Then, for

1 we varied t(cid:48)

each value of t(cid:48)
2 in the [t2, 1.95] range and
computed the average false positive rate (FPR). Figure 19
shows the average FPR for benign and adversarial inputs for
different values of t(cid:48)
1. ResNet50 exhibits a tighter distribution
for L1 distance under approximation and is therefore not
sensitive to the threshold values. VGG16 on the other hand
is more sensitive due to its wider distribution The threshold
value allows a small tradeoff between FPRs for benigns vs.
adversarials.

IX. RELATED WORK

Several other methods for designing robust neural networks
to adversarial attacks have been proposed in the literature.
These methods typically fall into four broad categories [2]:
Hardening the model, also known as adversarial training.
Recent works certiﬁed robust models by training models under
Gaussian noise injection into the inputs [8] or the model [21],
[30]. While these methods represent a systematic solution to
adversarial attacks, they are limited to certain perturbation
norms (e.g L2) and do not scale for large datasets like
ImageNet. Unlike model and input hardening approaches,
DNNSHIELD does not require any re-training of the model
and sacriﬁces little in model accuracy. DNNSHIELD is de-
signed to detect adversarial examples post-training, during
model inference.
Hardening the test inputs, also known as applying input
transformations, such as ﬁltering or encoding the image. Input
hardening methods require proﬁling to select appropriate pa-
rameters, such as Feature Squeezing [60] and Path Extraction

12

Low SRHigh SRRandomDNNShieldSparsification methods0.00.20.40.60.81.0VGG16Low SRHigh SRRandomDNNShieldSparsification methods0.00.20.40.60.81.0ResNet50Adversarial detectionBenign FPRbaseinference.12345678Number of noisy runs(a)406080100VGG16baseinference.12345678Number of noisy runs(b)020406080100ResNet50Benign TPRAdversarial success rateapproximation error to attack conﬁdence, and showed robust-
ness against defense-aware attacks.

REFERENCES

[1] Tensorﬂow. https://www.tensorﬂow.org/.
[2] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep
learning in computer vision: A survey. IEEE Access, 6:14410–14430,
2018.

[3] Xiaoyu Cao and Neil Zhenqiang Gong. Mitigating evasion attacks to
deep neural networks via region-based classiﬁcation. In Proceedings of
the 33rd Annual Computer Security Applications Conference on, pages
278–287, 2017.

[4] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel,
Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and
Alexey Kurakin. On evaluating adversarial robustness. arXiv preprint
arXiv:1902.06705, 2019.

[5] Nicholas Carlini and David Wagner. Adversarial examples are not easily
detected: Bypassing ten detection methods. In Proceedings of the 10th
ACM workshop on artiﬁcial intelligence and security, pages 3–14, 2017.
[6] Nicholas Carlini and David Wagner. Towards evaluating the robustness
of neural networks. In IEEE Symposium on Security and Privacy (SP),
2017.

[7] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui
Hsieh. Ead: elastic-net attacks to deep neural networks via adversarial
examples. In Thirty-second AAAI conference on artiﬁcial intelligence,
2018.

[8] Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter.
adversarial robustness via randomized smoothing.
arXiv:1902.02918, 2019.

Certiﬁed
arXiv preprint

[9] Alberto Delmas Lascorz, Patrick Judd, Dylan Malone Stuart, Zissis
Poulos, Mostafa Mahmoud, Sayeh Sharify, Milos Nikolic, Kevin Siu,
and Andreas Moshovos. Bit-tactical: A software/hardware approach to
In Proceedings
exploiting value and bit sparsity in neural networks.
of the Twenty-Fourth International Conference on Architectural Support
for Programming Languages and Operating Systems, pages 749–763,
2019.

[10] Chunhua Deng, Yang Sui, Siyu Liao, Xuehai Qian, and Bo Yuan.
Gospa: An energy-efﬁcient high-performance globally optimized sparse
In 2021 ACM/IEEE 48th
convolutional neural network accelerator.
Annual International Symposium on Computer Architecture (ISCA),
pages 1110–1123. IEEE, 2021.

[11] Chunhua Deng, Yang Sui, Siyu Liao, Xuehai Qian, and Bo Yuan.
Gospa: An energy-efﬁcient high-performance globally optimized sparse
In 2021 ACM/IEEE 48th
convolutional neural network accelerator.
Annual International Symposium on Computer Architecture (ISCA),
pages 1110–1123, 2021.

[12] Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean
Kossaiﬁ, Aran Khanna, Zachary C. Lipton, and Animashree Anandku-
mar. Stochastic activation pruning for robust adversarial defense.
In
International Conference on Learning Representations (ICLR), 2018.

[13] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B
Gardner. Detecting adversarial samples from artifacts. arXiv preprint
arXiv:1703.00410, 2017.

[14] Yonggan Fu, Qixuan Yu, Meng Li, Vikas Chandra, and Yingyan Lin.
Double-win quant: Aggressively winning robustness of quantized deep
neural networks via random precision training and inference. In Marina
Meila and Tong Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Ma-
chine Learning Research, pages 3492–3504. PMLR, 18–24 Jul 2021.

[15] Yonggan Fu, Yang Zhao, Qixuan Yu, Chaojian Li, and Yingyan Lin. 2-
in-1 accelerator: Enabling random precision switch for winning both
In MICRO-54: 54th Annual
adversarial robustness and efﬁciency.
IEEE/ACM International Symposium on Microarchitecture, pages 225–
237, 2021.

[16] Y. Gan, Y. Qiu, J. Leng, M. Guo, and Y. Zhu. Ptolemy: Architecture
In 2020 53rd Annual IEEE/ACM
support for robust deep learning.
International Symposium on Microarchitecture (MICRO), pages 241–
255, 2020.

[17] Zhangxiaowen Gong, Houxiang Ji, Christopher W Fletcher, Christo-
pher J Hughes, Sara Baghsorkhi, and Josep Torrellas. Save: Sparsity-
aware vector engine for accelerating dnn training and inference on
In 2020 53rd Annual IEEE/ACM International Symposium on
cpus.
Microarchitecture (MICRO), pages 796–810. IEEE, 2020.

Fig. 19.
VGG16 and right ResNet50.

Benign and adversarial FPR for different threshold values, left

[46], [16]. Xie et al. [58] propose to defend against adversarial
examples by adding a randomization layer before the input
to the classiﬁer. [16] showed that adversarial inputs tend to
activate distinctive paths on neurons from those of benign
inputs. They proposed hardware accelerated adversarial sample
detection, which uses canary paths from ofﬂine proﬁling. In
contrast, DNNSHIELD does not require proﬁling.
Adding a secondary, external network solely responsible for
adversarial detection and with a separate training phase, such
as NIC [33]. DNNGuard [57] proposed an accelerator for such
detection mechanism but has not evaluated a speciﬁc detection
classiﬁer. Secondary network detection-based methods are not
as effective, and can be evaded by adaptive attacks [5].
Noise-based approaches Prior work has similarly explored
ways of discriminating adversarial inputs using noise. How-
ever, prior approaches have either proposed injecting noise
into the input [8], [3] – with lower detection rate – or into
the model during training [47], [21], [30], [48]. However,
the challenge with training-based approaches such as [21]
is that
the noise parameters tend to converge to zero as
training progresses, making the noise injection progressively
less effective over time [26]. While training-based approaches
that certiﬁcation is
have enabled ”certiﬁed robust” inputs,
generally limited to a very narrow set of inputs.

X. CONCLUSION

In conclusion, this paper showed that dynamic and random
sparsiﬁcation of DNN models enables robust > 88% adversar-
ial detection across multiple strong attacks, for different image
classiﬁers. We also demonstrated the importance of correlating

13

0.050.150.250.350.450.550.650.75t01VGG160.0550.0600.0650.070FPRt02[1.25,1.95],t1=0.75,t2=1.250.050.150.250.350.450.550.650.75t01ResNet500.020.030.040.05FPRMean adversarial FPRMean benign FPR[18] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining
In ICLR 2015 : International

and harnessing adversarial examples.
Conference on Learning Representations 2015, 2015.

[19] Amira Guesmi, Ihsen Alouani, Khaled N Khasawneh, Mouna Baklouti,
Tarek Frikha, Mohamed Abid, and Nael Abu-Ghazaleh. Defensive
approximation: securing cnns using approximate computing. In Proceed-
ings of the 26th ACM International Conference on Architectural Support
for Programming Languages and Operating Systems, pages 990–1003,
2021.

[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 770–778, 2016.

[21] Zhezhi He, Adnan Siraj Rakin, and Deliang Fan. Parametric noise injec-
tion: Trainable randomness to improve deep neural network robustness
against adversarial attack. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 588–597, 2019.
[22] Hossein Hosseini, Sreeram Kannan, and Radha Poovendran. Are odds
really odd? bypassing statistical detection of adversarial examples. arXiv
preprint arXiv:1907.12138, 2019.

[23] Shengyuan Hu, Tao Yu, Chuan Guo, Wei-Lun Chao, and Kilian Wein-
berger. A new defense against adversarial images: Turning a weakness
into a strength. In NeurIPS 2019 : Thirty-third Conference on Neural
Information Processing Systems, pages 1633–1644, 2019.

[24] Chao-Tsung Huang.

Ringcnn: Exploiting algebraically-sparse ring
tensors for energy-efﬁcient cnn-based computational imaging. In 2021
ACM/IEEE 48th Annual International Symposium on Computer Archi-
tecture (ISCA), pages 1096–1109, 2021.

[25] Jun-Woo Jang, Sehwan Lee, Dongyoung Kim, Hyunsun Park,
Ali Shaﬁee Ardestani, Yeongjae Choi, Channoh Kim, Yoojin Kim,
Hyeongseok Yu, Hamzah Abdel-Aziz, Jun-Seok Park, Heonsoo Lee,
Dongwoo Lee, Myeong Woo Kim, Hanwoong Jung, Heewoo Nam,
Dongguen Lim, Seungwon Lee, Joon-Ho Song, Suknam Kwon, Joseph
Hassoun, SukHwan Lim, and Changkyu Choi. Sparsity-aware and re-
conﬁgurable npu architecture for samsung ﬂagship mobile soc.
In
2021 ACM/IEEE 48th Annual International Symposium on Computer
Architecture (ISCA), pages 15–28, 2021.

[26] Ahmadreza Jeddi, Mohammad Javad Shaﬁee, Michelle Karg, Christian
Scharfenberger, and Alexander Wong. Learn2perturb: an end-to-end
feature perturbation learning to improve adversarial robustness.
In
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 1241–1250, 2020.

[27] Hyeong-Ju Kang. Accelerator-aware pruning for convolutional neural
IEEE Transactions on Circuits and Systems for Video

networks.
Technology, 30(7):2093–2103, 2019.

[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
classiﬁcation with deep convolutional neural networks.
in neural information processing systems, pages 1097–1105, 2012.
[29] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial
examples in the physical world. In International Conference on Learning
Representations (ICLR), 2017.

Imagenet
In Advances

[30] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu,
and Suman Jana. Certiﬁed robustness to adversarial examples with
differential privacy. In 2019 IEEE Symposium on Security and Privacy
(SP), pages 656–672. IEEE, 2019.

[31] Bin Liang, Hongcheng Li, Miaoqiang Su, Xirong Li, Wenchang Shi, and
Xiaofeng Wang. Detecting adversarial image examples in deep networks
with adaptive noise reduction. arXiv preprint arXiv:1705.08378, 2017.
[32] Pei-Hsuan Lu, Pin-Yu Chen, Kang-Cheng Chen, and Chia-Mu Yu. On
the limitation of magnet defense against l1-based adversarial examples.
In 2018 48th Annual IEEE/IFIP International Conference on Depend-
able Systems and Networks Workshops (DSN-W), pages 200–214. IEEE,
2018.

[33] Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu
Zhang. Nic: Detecting adversarial samples with neural network invariant
checking. In NDSS, 2019.

[34] Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wi-
jewickrema, Michael E. Houle, Grant Schoenebeck, Dawn Song, and
James Bailey. Characterizing adversarial subspaces using local intrinsic
dimensionality. arXiv preprint arXiv:1801.02613, 2018.

[36] Mostafa Mahmoud, Isak Edo, Ali Hadi Zadeh, Omar Mohamed Awad,
Gennady Pekhimenko, Jorge Albericio, and Andreas Moshovos. Ten-
sordash: Exploiting sparsity to accelerate deep neural network training.
In 2020 53rd Annual IEEE/ACM International Symposium on Microar-
chitecture (MICRO), pages 781–795. IEEE, 2020.

[37] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and
In 2017 IEEE
Pascal Frossard. Universal adversarial perturbations.
Conference on Computer Vision and Pattern Recognition (CVPR), pages
86–94, 2017.

[38] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,

and Pascal
Frossard. Deepfool: A simple and accurate method to fool deep
neural networks. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.

[39] V. Mrazek, R. Hrbacek, Z. Vasicek, and L. Sekanina. Evoapprox8b:
Library of approximate adders and multipliers for circuit design and
benchmarking of approximation methods. In Design, Automation Test
in Europe Conference Exhibition (DATE), 2017, pages 258–261, March
2017.

[40] V. Mrazek, Z. Vasicek, L. Sekanina, M. A. Hanif, and M. Shaﬁque.
Alwann: Automatic layer-wise approximation of deep neural network
accelerators without retraining. In 2019 IEEE/ACM International Con-
ference on Computer-Aided Design (ICCAD), pages 1–8, Nov 2019.

[41] Priyadarshini Panda. Quanos: adversarial noise sensitivity driven hybrid
In Proceedings of the ACM/IEEE
quantization of neural networks.
International Symposium on Low Power Electronics and Design, pages
187–192, 2020.

[42] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Anan-
thram Swami. Distillation as a defense to adversarial perturbations
In 2016 IEEE Symposium on Security
against deep neural networks.
and Privacy (SP), pages 582–597, 2016.

[43] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson,
Z. Berkay Celik, and Ananthram Swami. The limitations of deep
learning in adversarial settings. In IEEE European Symposium Security
and Privacy, 2016.

[44] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli,
Rangharajan Venkatesan, Brucek Khailany, Joel Emer, Stephen W Keck-
ler, and William J Dally. Scnn: An accelerator for compressed-sparse
convolutional neural networks. ACM SIGARCH Computer Architecture
News, 45(2):27–40, 2017.

[45] Eric Qin, Ananda Samajdar, Hyoukjun Kwon, Vineet Nadella, Sudarshan
Srinivasan, Dipankar Das, Bharat Kaul, and Tushar Krishna. Sigma: A
sparse and irregular gemm accelerator with ﬂexible interconnects for dnn
training. In 2020 IEEE International Symposium on High Performance
Computer Architecture (HPCA), pages 58–70, 2020.

[46] Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo,
and Yuhao Zhu. Adversarial defense through network proﬁling based
path extraction. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 4777–4786, 2019.

[47] Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd:
A statistical test for detecting adversarial examples. In ICML, volume 97
of Proceedings of Machine Learning Research, pages 5498–5507, 2019.
[48] Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico
Kolter. Denoised smoothing: A provable defense for pretrained clas-
siﬁers. arXiv preprint arXiv:2003.01908, 2020.

[49] Yash Sharma and Pin-Yu Chen. Attacking the madry defense model
with l 1-based adversarial examples. arXiv preprint arXiv:1710.10733,
2017.

[50] Yash Sharma and Pin-Yu Chen. Attacking the madry defense model
with l 1-based adversarial examples. arXiv preprint arXiv:1710.10733,
2017.

[51] Yash Sharma and Pin-Yu Chen. Bypassing feature squeezing by
increasing adversary strength. arXiv preprint arXiv:1803.09868, 2018.
[52] Karen Simonyan and Andrew Zisserman. Very deep convolutional

networks for large-scale image recognition, 2014.

[53] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and
Zbigniew Wojna. Rethinking the inception architecture for computer
vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2818–2826, 2016.

[35] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
Tsipras, and Adrian Vladu. Towards deep learning models resistant
to adversarial attacks. In International Conference on Learning Repre-
sentations (ICLR), 2018.

[54] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties
of neural networks. In International Conference on Learning Represen-
tations 2014, 2014.

14

[55] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander
Madry. On adaptive attacks to adversarial example defenses. arXiv
preprint arXiv:2002.08347, 2020.

[56] F. Vaverka, V. Mrazek, Z. Vasicek, L. Sekanina, M. A. Hanif, and
M. Shaﬁque. Tfapprox: Towards a fast emulation of dnn approximate
hardware accelerators on gpu. In 2020 Design, Automation and Test in
Europe Conference (DATE), page 4, 2020.

[57] Xingbin Wang, Rui Hou, Boyan Zhao, Fengkai Yuan, Jun Zhang, Dan
Meng, and Xuehai Qian. Dnnguard: An elastic heterogeneous dnn
In Proceedings
accelerator architecture against adversarial attacks.
of the Twenty-Fifth International Conference on Architectural Support
for Programming Languages and Operating Systems, ASPLOS ’20,
page 19–34, New York, NY, USA, 2020. Association for Computing
Machinery.

[58] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille.
Mitigating adversarial effects through randomization. In International
Conference on Learning Representations (ICLR), 2018.
[59] Xilinx. CHaiDNN. https://github.com/Xilinx/CHaiDNN.
[60] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting
In Proceedings 2018

adversarial examples in deep neural networks.
Network and Distributed System Security Symposium, 2018.

[61] Shijin Zhang, Zidong Du, Lei Zhang, Huiying Lan, Shaoli Liu, Ling Li,
Qi Guo, Tianshi Chen, and Yunji Chen. Cambricon-x: An accelerator for
sparse neural networks. In 2016 49th Annual IEEE/ACM International
Symposium on Microarchitecture (MICRO), pages 1–12. IEEE, 2016.

15

