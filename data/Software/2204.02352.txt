2
2
0
2

r
p
A
4

]

C
O
.
h
t
a
m

[

1
v
2
5
3
2
0
.
4
0
2
2
:
v
i
X
r
a

Asynchronous Load Balancing and Auto-scaling:
Mean-Field Limit and Optimal Design

J. Anselmi
Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France.
jonatha.anselmi@inria.fr

April 6, 2022

Abstract

We introduce a Markovian framework for load balancing where classical algorithms such as Power-of-d
are combined with asynchronous auto-scaling features. These allow the net service capacity to scale up
or down in response to the current load within the same timescale of job dynamics. This is inspired
by serverless frameworks such as Knative, used among others by Google Cloud Run, where servers are
software functions that can be ﬂexibly instantiated in milliseconds according to user-deﬁned scaling rules.
In this context, load balancing and auto-scaling are employed together to optimize both user-perceived
delay performance and energy consumption. In the literature, these mechanisms are synchronous or rely
on a central queue. The architectural novelty of our work is to consider an asynchronous and decentralized
system, as in Knative, which takes scalability to the next level.

Under a general assumption on the auto-scaling process, we prove a mean-ﬁeld limit theorem that
provides an accurate and tractable approximation for the system dynamics when the mean demand and
nominal service capacity grow large in proportion. We characterize the ﬁxed points of the mean-ﬁeld
limit model and provide a simple condition telling whether or not all the available servers need to be
turned on to handle the incoming demand. Then, we investigate how to design optimal auto-scaling
rules and ﬁnd a general condition able to drive the mean-ﬁeld dynamics to delay and relative energy
optimality, a situation where the user-perceived delay and the relative energy wastage induced by idle
servers vanish. The proposed optimality condition suggests to scale up capacity if and only if the mean
demand exceeds the overall rate at which servers become idle-on, i.e., idle and active. This yields the
deﬁnition of tractable optimization frameworks to trade oﬀ between energy and performance, which we
show as an application of our work.

1

Introduction

Load balancing is the process of distributing work units (jobs) across a set of distributed computational
resources (servers). Exogenous jobs join the system over time through one or more dispatchers, and these
route each of them to one out of N parallel servers for processing. Then, each job leaves the system
upon service completion at its designated server.
In large architectures, each server has its own queue,
as this enhances scalability, and jobs are immediately dispatched upon their arrival. Given the stringent
latency requirements of modern applications, breaches of which can severely impact revenue, load balancing
techniques are usually designed to optimize delay performance, and popular examples are Power-of-d [24]
and Join-the-Idle-Queue (JIQ) [21]. Closely related to load balancing, auto-scaling is a term often used in
cloud computing to refer to the process of adjusting the available service capacity automatically in response
to the current load [29]. In this context, auto-scaling techniques are meant to control capacity (N ) over
time to avoid performance degradation, which yields unacceptably large delays, and overprovisioning of
resources, which yields high infrastructure and energy costs. Google Cloud Run, Amazon Elastic Compute
Cloud (EC2), Microsoft Windows Azure and Oracle Cloud Platform are examples of platforms that oﬀer
auto-scaling and load balancing features. Users of these platforms deploy their applications deﬁning how
the system should scale up resources in front of an increased load. Modern auto-scaling mechanisms are
extremely reactive in the sense that they adapt capacity relying on fresh observations of the system state

1

 
 
 
 
 
 
rather than historical data. This especially holds true in serverless computing platforms, or Function-as-
a-Service, which nowadays provide the convenient solution to deploy any type of application or backend
service [22].

Most of existing performance models for load balancing assume that the available service capacity, N ,
remains constant over time even when designed for cloud computing systems [37], i.e., auto-scaling is not
taken into account. Nonetheless, auto-scaling mechanisms are frequently employed in cloud applications and
aﬀect delay performance. This does not mean that classic load balancing models are inadequate for cloud
systems but that they assume that auto-scaling operates at a much slower timescale than load balancing.
Essentially, this means that jobs do not see any change in the overall available capacity because they move
much faster than servers. This makes sense if servers are interpreted as physical or even virtual machines
because setup times are of the order of minutes if not longer [15] while in typical applications hosted in
cloud networks job service times are about ten milliseconds. The large body of literature on load balancing,
reviewed in Section 5, is undoubtedly the proof that this timescale separation assumption is well accepted
by researchers and practitioners. In the context of serverless computing however, a server is interpreted as
a software function that can be ﬂexibly instantiated in milliseconds [40, 41], i.e., within a time window that
is comparable with the magnitude of job inter-arrival and service times, and with negligible switching costs.
Here, auto-scaling mechanisms are extremely reactive and the decisions of turning servers on or oﬀ are based
on instantaneous observations of the current system state rather than on the long-run equilibrium behavior
or historical data. Therefore, the timescale separation assumption above becomes arguable [22] because it
would mean to assume that job dynamics achieve stochastic equilibrium between consecutive changes of N ,
i.e., in milliseconds.

While there is a large body of literature investigating load balancing and auto-scaling separately, very
little has been done when both are applied jointly within the same timescale. Most of existing works
focused on synchronous and centralized architectures where all servers share a common queue [15, 22],
where synchronous means that scale-up and dispatching decisions are taken simultaneously. In typical cloud
architectures however, no central queue is maintained as it would aﬀect scalability. A notable exception
is [26], where the authors consider a synchronous but decentralized architecture where each server has its
own queue; see also [16]. In particular, they synchronize JIQ [21] with a speciﬁc auto-scaling strategy and
analyze the resulting performance in the mean-ﬁeld limit; see Section 5 for further details. Also our framework
considers a decentralized architecture but the key novelty is that load balancing and auto-scaling operate
asynchronously; the term “asynchronous” is borrowed from the cloud computing community [22]. In other
words, the scale-up and dispatching processes are decoupled. The asynchronous approach takes scalability to
the next level, for instance because it facilitates the deployment of systems with a large number of dispatchers
[21], and is one principle underlying real systems such as Knative [1], a Kubernetes-based platform to deploy
and manage modern serverless workloads that is used among others by Google Cloud Run; see Section 5.1
for further details. In addition, instead of focusing on a speciﬁc load balancing and auto-scaling strategy,
our work will provide a framework to evaluate the impact of a general class of auto-scaling mechanisms that
are up to the platform user to choose or design. This will be one advantage of our framework with respect
to existing works because systems such as Knative leave the platform user with such control.

1.1 Contributions

We propose a Markovian framework for load balancing that includes asynchronous auto-scaling mechanisms.
We refer to this framework as ‘Asynchronous Load Balancing and Auto-scaling’ (ALBA). ALBA is inspired
by and tailored to serverless frameworks such as Knative; more details in Section 2. Two mechanisms drive
dynamics in ALBA:

i) a dispatching rule, or load balancing rule, which deﬁnes how jobs are dispatched among the set of active

servers as they join the system, and

ii) a scaling rule, which deﬁnes how the number of active servers scales up and down over time, possibly

as a function of the current system state.

In agreement with existing implementations of serverless frameworks [22], we assume that dispatching and
scaling rules operate asynchronously. This implies that the capacity scaling decisions are not taken at

2

the moments where jobs arrive and represents a key diﬀerence with respect to the existing approaches
in the literature. The main motivation for us to consider an asynchronous architecture is to provide the
ﬁrst performance model and analysis of existing serverless platforms such as Knative. Their asynchronous
structure enhances scalability as it facilitates operations in large systems with several dispatchers. The
dispatching rules included in our framework are Power-of-d and Join-Below-Threshold-d (JBT-d) because
they are commonly used in serverless frameworks [2]. A typical scale-down rule turns a server oﬀ only if
it remains idle during an expiration window [22, 1]. This rule, also known as “delay-oﬀ” [15], is easy to
implement and will be assumed within the proposed framework. In contrast, we do not impose any particular
structure on scale-up rules as they are often deﬁned by the user of the serverless platform. Since we have
ﬁxed the scale-down rule, in the following the term “scaling rule” will refer to a scale-up rule.

To the best of our knowledge, ALBA is the ﬁrst framework that models the interplay between load
balancing and auto-scaling asynchronously. Our key technical contributions hold under a general assumption
on the structure of scaling rules, see Assumption 2, and are as follows:

• In Theorem 1, we prove a mean-ﬁeld limit theorem that provides an accurate deterministic approxima-
tion of the sample paths of the ﬁnite-dimensional stochastic process underlying ALBA. This enables
analytical tractability and allows one to study dynamics easily. For instance, given an arbitrary scaling
rule, it may be used to estimate the resulting proportion of active servers, and thus the energy costs,
in both the transient and stationary regimes. We prove Theorem 1 following the framework developed
in [34, 8], which allows us to handle discontinuities of the drift function of the underlying Markov chain.

• In Theorem 2, we characterize the ﬁxed points of the mean-ﬁeld limit in terms of a set of non-linear
equations and provide a simple necessary and suﬃcient condition on the scaling rule able to tell whether
or not full service capacity will be needed to handle the incoming demand. Within Power-of-d, roughly
speaking, there always exists a unique ﬁxed point if the scaling rule is “nice”. Within JBT-d, however,
this is not the case: to ensure uniqueness within a certain class of interesting scaling rules, it is necessary
that the scaling rule has access to the number of servers containing exactly one job at any point in
time (see Remark 1).

• When the mean demand exceeds the current service capacity, one may expect that a (strictly) positive
scale-up rate at all times is enough to eventually drive the system to the reversed situation where
capacity exceeds, or matches, demand. In Proposition 1, we provide a counterexample showing that
this is not necessarily the case. In addition, the mean-ﬁeld dynamics may converge to diﬀerent ﬁxed
points, depending on the initial conditions. This poses the challenge of designing scaling rules able to
drive dynamics to a desired ﬁxed point regardless of the initial condition. This is addressed in our next
contribution.

• In Theorem 3, we investigate how to design ﬂuid optimal scaling rules: we provide a simple general
condition ensuring that dynamics of the mean-ﬁeld limit converge to “delay and relative energy op-
timality”. Following [26], this is a situation where the user-perceived delay and the relative energy
wastage induced by idle servers vanish. We show that delay and relative energy optimality can only
be achieved within JIQ, or equivalently JBT-0. The proposed condition suggests to scale up capacity
if and only if the mean demand exceeds the overall rate at which servers become active and idle. From
a practical standpoint, this requires the central controller to estimate the mean job arrival rate, which
can be done via machine learning techniques, and to have access to the amount of both initializing, i.e.,
in set-up mode, and busy servers containing exactly one job. In existing serverless platforms, this infor-
mation is easily accessible and implies only a constant communication overhead per job as in standard
implementations of JIQ (see Remark 5). In addition, we ﬁnd that being able to distinguish between
busy servers with one or more jobs is necessary for ﬂuid optimality (see Remark 3). This exposes a
structural diﬀerence with respect to synchronous architectures. In fact, delay and relative energy opti-
mality can be achieved even when JIQ is synchronized with the ad-hoc scale-up rule proposed in [26],
which turns a server on if all active servers are busy upon job arrivals. In this respect, our contribution
is to show that optimality can be achieved under broader conditions and also within asynchronous
architectures. In existing serverless platforms, we stress that decentralized implementations of load
balancing and auto-scaling mechanisms are indeed asynchronous.

3

While we show that ﬂuid optimality can only hold within JIQ, we observe that this may not be the
convenient choice within architectures with several dispatchers. In this case, an exact implementation of
JIQ implies an expensive communication overhead per job and Power-of-d may be the way to go. A further
advantage of Power-of-d with respect to JIQ or JBT-d is that is stateless, i.e., it does not require the
dispatcher(s) to store information about the server states. This stateless feature is generally required for
load balancing at the network layer [12]. Within these scenarios, the advantage of the proposed framework
is to allow one to be able to design and eﬃciently evaluate the performance of diﬀerent scaling rules. To the
best of our knowledge, our work is the ﬁrst to provide such ability.

As application of our results, we develop a tractable optimization framework allowing the system manager
to trade oﬀ between performance and energy costs. The main purpose of this optimization framework is to
illustrate how the results presented in this paper can be applied.

1.2 Organization

This paper is organized as follows. In Section 2, we build the ALBA framework and deﬁne the structure
of the scaling rules considered in our framework. In Section 3, we present our main results: Theorems 1,
2 and 3.
In Section 4, we show an application example of our results to trade oﬀ between energy and
performance. In Section 5, we review the existing literature, and in Section 6, we draw the conclusions.
Proofs of our main results are deferred to Appendix 7.

2 Asynchronous Load Balancing and Auto-scaling (ALBA)

In this section, we ﬁrst describe the main principles at the basis of Asynchronous Load Balancing and Auto-
scaling (ALBA). Since our aim is to develop a model tailored to serverless computing, we will make several
references to Knative, a popular serverless framework for hosting Function-as-a-Service processing that is
used, among others, by Google Cloud Run. Then, we propose two performance models for ALBA. The ﬁrst
captures the stochastic nature of the underlying dynamics while the second is deterministic and will serve
to approximate the complex dynamics induced by the ﬁrst. The advantage of the deterministic model is its
tractability. Finally, we formalize the structure of the scaling rules investigated in this paper.

2.1 System Description

The proposed framework, ALBA, is composed of a system of N parallel servers, each with its own queue,
that represent the nominal service capacity, i.e., the upper limit on the amount of resources that one user
can have up and running at the same time1. In the cloud computing community, servers are also referred to
as containers, cloud functions, instances or replicas. Public serverless computing platforms usually require
to specify such limit in order to ensure service availability for other users. In the following, the terms servers
and queues will be used interchangeably. A server is said warm if turned on, cold if turned oﬀ and initializing
if making the transition from cold to warm. These are the possible server states [41, 23, 22]. An initializing
server performs basic startup operations such as connecting to database, loading libraries, etc. This is the
time to provision a new function instance. Only warm servers are allowed to receive jobs. When no cold
server exists, we say that the system is saturated; in ALBA, a saturated system is not representative of
the average case. A server is also said idle-on if warm but not processing any job, and busy if warm and
processing some job. Typically, billing policies charge per number of warm and initializing servers used per
time unit.

Jobs join the system from an exogenous source to receive service. Upon arrival, each job is dispatched to
a warm server according to some dispatching rule. After dispatching, each job is processed by the selected
server according to the presumed scheduling discipline at that server. After processing, each job leaves the
system.

Assumption 1. Jobs are dispatched to servers according to either Power-of-d or Join-Below-Threshold-d
(JBT-d).

1In Knative, this upper limit is speciﬁed by the max-scale-limit global key.

4

We recall that Power-of-d sends an incoming job to the shortest among d

1 warm servers selected at
random at the moment of its arrival and JBT-d sends an incoming job to a warm server containing no more
than d
0 jobs if one exists otherwise to a warm server selected at random. In all cases, ties are broken
randomly. If d = 0, JBT-d is also known as Join-the-Idle-Queue (JIQ) [21]. We limit our framework to these
types of schemes because they involve a constant communication overhead per job (in architectures with a
single dispatcher) and because they are commonly used in practice. For instance, Knative uses Power-of-2
if no limit is set on the queue length of each server and JBT-d if such limit is set to d [2].

≥

≥

Alongside with the above job dynamics, the pools of warm/initializing/cold servers change over time in
the background and in an asynchronous manner. Precisely, the platform monitors the system state at some
epochs that we refer to as scaling times. At such times, a cold server is selected, provided that one exists,
and becomes initializing according to the outcome of some scaling rule. After some initialization time, or
coldstart latency, an initializing server becomes idle-on. When a server becomes idle-on, it becomes cold
after a scale down delay, or expiration time, if during such time the server received no job; this scale-down
rule is used in several serverless computing platforms (including Knative) [41, 38] and also in other settings
[15]. Therefore, the decision of activating a warm server is taken by a central controller while the decision
of making a server cold is taken by the server itself. We observe that the number of warm servers ﬂuctuates
from 0 to N over time. While in practice it may be possible to set a lower limit on the number of warm
servers, the scale down to zero (or one) servers conﬁguration is usually the default choice [3].

To a great extent, the scale up rule, the expiration rate and the scaling times are under the control of the
platform user, which may design them in a way to optimize a trade-oﬀ between performance and energy. On
the other hand, several measurements indicate that initialization times are typically one order of magnitude
higher than jobs’ service times in serverless platforms [22, 41].

2.2 Notation

We introduce some notation that will be used throughout the paper. Let B
that will denote the buﬀer size of each server. We use I
and A denotes an interval, 1a
Unless speciﬁed otherwise, (i, j) ranges over the set
{
otherwise. The process of interest will take values in

{a∈A}. We also let (
·
0, . . . , B

, 0
{·
0, 1, 2

)+ := max

A := I

and
}
if B <
}

} × {

k · k
∞

+

∞}

be a constant
R
denotes the L1 norm.
and over Z+ × {
0, 1, 2

∈

}

∈
{A} to denote the indicator function of A. If a

Z+ ∪ {

S

:=

(xi,j

n

R+,

∈

(i, j)) :

∀

xi,j = 1

i,j
X

o

and our analysis holds under the distance function dw induced by the weighted ℓ2 norm
by

. For x

, let yi :=

k≥i xi,2. We also let

|xi,j−x
2i+j

2
w :=

i,j |2

x′

i,j

x

x

′

k · k
:

S1 :=

{

∈ S

k

−

k

∈ S

(1)

w on RZ+ deﬁned
.
i≥1 ixi,2 <

∞}

2.3 Markov Model

P

P

P

We model the dynamics induced by ALBA in terms of a continuous time Markov chain. The exogenous
arrival process of jobs is assumed to be Poisson with rate λN , with 0 < λ < 1. Here, we may consider a
time-varying arrival rate of the form Λ(t)N , where Λ(t) is a bounded positive real-valued function, but we
do not consider this case for simplicity. The processing times, or service times, of jobs are independent and
exponentially distributed random variables with unit mean, and servers process jobs according to any work-
conserving discipline. Upon arrival, each job is assigned to one of the available warm servers as speciﬁed in
Assumption 1. In the extreme case where no warm server exists, we assume that the job is lost. We also
assume that each server can contain at most B > d jobs and a job that is sent to a server with B jobs is
rejected. If not speciﬁed otherwise, B is either ﬁnite or inﬁnite. At each scaling time, a cold server is selected
uniformly at random, provided that one exists, and becomes initializing with some probability g, referred to
as scaling probability (or rule), that will depend on the system state; in the Conclusions, we will discuss how
our work adapts to the case where a random number of cold servers is selected at each scaling time. Given
that jobs arrive with a rate proportional to N and only one server can be added at each scaling time, in our
model we let the scaling frequency increase with N as well. As it occurs in Knative, this implies that the
number of servers created in a time window of constant size is proportional to N if within such window the

5

scaling probability is not zero. We let the inter-scaling, initialization and expiration times be independent
and exponentially distributed with rate αN , β and γ, respectively.

The ﬁve sequences of inter-arrival, service, inter-scaling, initialization and expiration times are also

assumed mutually independent.
1 (t), . . . , QN
1 (t), . . . , SN

Let QN (t) := (QN
N (t)) be the vector of queue lengths at time t, including the jobs in service,
N (t)) be the vector of server states. Speciﬁcally, SN
and let SN (t) := (SN
indicates
whether server k is cold (SN
k (t) = 2) at time t. Under the
above assumptions, the stochastic process (QN (t), SN (t)) is a continuous-time Markov chain on state space
(n, s)

k (t) = 0), initializing (SN

k (t) = 1) or warm (SN

k = 1, . . . , N

0, . . . , B

N : nk > 0

sk = 2,

0, 1, 2

0, 1, 2

k (t)

∈ {

}

N

.

{

⇒
It is convenient to describe dynamics in terms of the process X N (t) := (X N

× {

∈ {

∀

}

}

}

0,0(t), X N

0,1(t), X N

i,2(t) : i =

0, . . . , B) where

X N

i,j(t) :=

1
N

N

k=1
X

I
{QN

k (t)=i,SN

k (t)=j}

(2)

is the proportion of servers in state j with i jobs at time t. The process X N (t) is still a Markov chain with
0, j′ = 0, 1, 2) where δa,b
: i′
values in some set
S
(N ) denote a generic state of X N (t). For conciseness, the
denotes the Kronecker delta and let x := (xi,j )
∈ S
Markov chain X N (t) has the following transitions:

(N ) that is a subset of

. Let ei,j := (δi,i′ δj,j′

0, 1

∈ {

≥

S

}

x

x

x

x

x

7→

7→

7→

7→

7→

x′ := x + 1

x′ := x + 1

ei−1,2) with rate λN fi−1(x)

ei,2) with rate xiN

x′ := x + 1

e0,0, e0,1)

x′ := x + 1

e0,1, e0,2)

with rate αN g

with rate βx0,1N

x′ := x + 1

e0,2)

with rate γx0,2N

N (ei,2 −
N (ei−1,2 −
N (

−

N (
−
N (e0,0 −

(N ). Here, g := g(x) :
for all i = 1, . . . , B, provided that x, x′
[0, 1] is the scaling probability, and
fi(x), which depends on the dispatching rule, represents the probability of assigning an incoming job to a
warm server containing exactly i jobs. If y0 > 0, within Power-of-d we have (assuming that server selections
are with replacement)

S →

∈ S

where yi := yi(x) :=

j≥i xj,2, and within JBT-d we have

fi(x) =

yd
i+1

yd
i −
yd
0

,

i = 0, . . . , B

1

−

P
fi(x) =

xi,2 I

{Pd

k=0 xk,2=0}
y0

+

xi,2 I

k=0 xk,2>0}

{Pd
d
k=0 xk,2

I
{i≤d},

i = 0, . . . , B

1

−

P
where we have taken the convention that 0/0 = 0. If y0 = 0, then fi(x) = 0 as no warm server exists.

2.4 Deterministic Model

(3)

(4)

We introduce the deterministic (or ﬂuid, mean-ﬁeld) model for the dynamics of ALBA.
Deﬁnition 1. A continuous function x(t) : R+ → S
almost all t

[0,

is said to be a ﬂuid model (or ﬂuid solution) if for

∈

)
∞

˙x0,0 = γx0,2 −
˙x0,1 = αgI
˙x0,2 = x1,2 −
˙xi,2 = xi+1,2I

αgI

{x0,0>0} −

γx0,2 I
βx0,1 + γx0,2 I
γx0,2
xi,2 + hi−1(x)

h0(x) + βx0,1 −
{i<B} −

{x0,0>0} −

{x0,0=0, γx0,2≤αg}

{x0,0=0, γx0,2≤αg}

hi(x)I

{i<B},

i = 1, . . . , B

−

(5a)

(5b)

(5c)

(5d)

6

where g := g(x) :

[0, 1], and

S →

if Power-of-d is applied, and

hi(x) =

(

i+1

λ
min

i −yd
yd
yd
0
βx0,1, λ
}

{

if y0 > 0
otherwise

xi,2
k=0 xk,2

I
λ
{i≤d}
Pd
βx0,1 + xd+1,2I
xi,2
xd+1,2 −
(cid:0)
y0
βx0,1, λ
min
}
{

(λ

−

{i=d}

I
{xd+1,2+(d+1)βx0,1≤λ}

(d + 1)βx0,1)+

(cid:1)

if y0 > 0,

if y0 > 0,
if y0 > 0,
otherwise

d
k=0 xk,2 > 0
d
d,
k=0 xk,2 = 0, i
P
d
k=0 xk,2 = 0, i > d,
P
P

≤

hi(x) = 


if JBT-d is applied.

(6)

(7)

As for X N
i,j(t), xi,j(t) is interpreted as the proportion of servers in state j with i jobs at time t.
Let us provide some intuition about the ﬂuid model. First, when a strictly positive ﬂuid mass of warm
server exists, i.e., y0 > 0, the functions hi are interpreted as the rate at which jobs are assigned to servers
with exactly i jobs. When the amount of ﬂuid of cold servers is strictly positive, i.e., x0,0 > 0, to some
extent these equations may be interpreted as the conditional expected change, or drift, from state x of the
I
Markov chain X N (t). In contrast, when x0,0 = 0, there exists a term,
{x0,0=0, γx0,2≤αg}γx0,2 (see (5a) and
(5b)), that still drains the amount of cold servers down. This is due to warm servers that become cold but
immediately turn initializing and it appears if the scaling rule is ‘greedy enough’, i.e., if the rate at which
new initializing servers can be created is greater than or equal to the rate at which warm servers go cold.
This term is due to ﬂuctuations of order 1/N that appear when X N
0,0(t) = 0, which bring discontinuities in
the drift of X N (t), and will come out from the stochastic analysis developed in Section 7.1.

−

Now, let us focus on (6) and (7), and let us assume that y0 > 0. In the case of Power-of-d, hi = λfi and
x(t) evolves following the natural dynamics of Power-of-d as in [24], though normalized on the variable mass
of warm servers y0(t). The case of JBT-d is more delicate because of the discontinuous structure of fi in (4).
If a strictly positive fraction of warm servers with no more than d jobs exist, then hi = λfi and x(t) evolves
following the natural dynamics of JBT-d, though again normalized on a variable number of servers. On the
d
k=0 xk,2 = 0, there is a ﬂow of warm servers with at most d jobs that are created but
other hand, when
immediately used for dispatching jobs. Speciﬁcally, there are two factors that come into play here: the ﬁrst
is due to initializing servers that get warm with exactly i jobs (with rate βx0,1), for all i
d, and the second
is due service completions from servers with exactly d + 1 jobs (with rate xd+1,2). The resulting rate can not
be greater than λ, the rate where jobs are assigned to servers, and this justiﬁes the I
{xd+1,2+(d+1)βx0,1≤λ}
(d + 1)βx0,1)+, is distributed uniformly over servers with
term. Then, the excess of such rate, (λ
i > d jobs. In Theorem 3, we will show that such rate is key for the design of ﬂuid optimal scaling rules.
Finally, assume that no warm server exists, i.e., y0 = 0. Here, initializing servers get idle-on with rate βx0,1
but all of them are immediately ﬁlled by new arrivals if λ
βx0,1, and in this case the mass of idle-on servers
remains zero. Otherwise, x0,2 increases with surplus rate βx0,1 −

xd+1,2 −

P

λ.

−

≤

≥

The existence of a ﬂuid solution started in x(0)

∈ S1 will be direct from Theorem 1.

2.5 Scaling Rules

The scaling rule g gives the probability to activate a new server at each scaling time as a function of the
system state. The following assumption, which will hold throughout the paper, provides the structure of the
scaling rules investigated in this paper.

Assumption 2. The scaling rule g :

S →

[0, 1] is Lipschitz continuous, and g(x) > 0 if x0,0 = 1.

The last technical condition is natural and will rule out the existence of degenerate ﬁxed points. We allow
g(x) to be greater than zero even when no cold server exists, i.e., x0,0 = 0. While this has no impact on the
dynamics of the stochastic model, it does aﬀect the ﬂuid model as there may exist a ﬂow of idle-on servers
that go cold but instantly turn initializing keeping the proportion of cold servers at zero. This situation can
occur if λ is large enough and not only in the transient regime; see Theorem 2.

We propose two scaling rules that satisfy Assumption 2 and that will be used for illustration purposes.

7

Deﬁnition 2. At each scaling time, if the system state is x,

• Blind-θ activates a new server with probability g(x) = θ, θ

• Rate-Idle activates a new server with probability g(x) = 1

∈
λ (λ

(0, 1];

βx0,1 −

−

x1,2)+.

Blind-θ is oblivious of the system state and thus highly scalable. On the other hand, within Rate-Idle, the
auto-scaler needs to know the amount of initializing servers, the amount of busy servers with exactly one job
at any point in time and both the job arrival and server initialization rates, which can be easily estimated.
If λ > βx0,1 + x1,2, then the interpretation is that the scaling probability is proportional to the diﬀerence
between the mean demand, λ, and the rate at which servers become idle-on, βx0,1 + x1,2. Otherwise, the
scale-up process is turned oﬀ. Rate-Idle is interesting because it will induce ﬂuid optimality if combined
with JIQ; see Theorem 3.

3 Main Results

In this section, we present our main results. First, in Theorem 1 we justify the use of the deterministic model
to approximate the behavior of the stochastic. Then, we focus on properties of the deterministic model: we
characterize its ﬁxed points in Theorem 2 and investigate the design of optimal scaling rules in Theorem 3.

3.1 Connection between the Fluid and the Markov models

The following result shows that the ﬂuid model can be seen as a ﬁrst-order approximation of the sample
paths of the stochastic.

Theorem 1. Let T <
0 almost surely. Then, limit
points of the stochastic process (X N (t))t∈[0,T ] exist and almost surely satisfy the conditions that deﬁne a
ﬂuid solution started at x(0).

∈ S1 and assume that

→

∞

−

k

k

w

, x(0)

X N (0)

x(0)

Proof. Given in Appendix 7.

≥

The stochastic and the deterministic models have some non-standard aspects that prevent us to prove
Theorem 1 by directly applying Kurtz’s theorem or similar known results. Speciﬁcally, the technical issues
are that the trajectories of the deterministic model may cross or converge, as time increases, to points of
discontinuity of its drift function and that buﬀer sizes are allowed to be inﬁnite. To handle these issues, we
follow the general framework in [34, 8] developing ad-hoc arguments speciﬁc to the structure of our problem.
In view of Theorem 1 and since typical and default maximum scale limit values are 1000 or more, i.e.,
103, we expect that the ﬂuid model x(t) provides an accurate approximation of the average behavior
N
of X N (t). To support this claim, we now present the results of numerical simulations; see also Section 4.
Figure 1 (left) plots some trajectories of x(t) and X N (t) when N = 103 and B = 102 along the coordinates
of cold (x0,0), initializing (x0,1), idle-on (x0,2) and busy (y1) servers. Also, Figure 1 (right) plots the
average number of jobs per warm server, which in state x is given by Q(x) := 1
i≥1 ixi,2. The ﬂuid
y0
(stochastic) trajectories are always represented by dashed (continuous) lines and each curve is the average
of ten simulations. Each simulation is based on 106 events. We have also set λ = 0.7, α = 0.05, β = 0.1 and
γ = 0.025. If a time unit is 10 milliseconds, these are realistic choices [22, 10]. As scaling rule, we have chosen
Blind-θ where θ = 0.5
; this choice will ensure that a strictly positive proportion of cold servers exists in
α
the long run (see Theorem 2). As dispatching algorithm, we have used Power-of-d with d = 1. At time zero,
λ)N servers
we have assumed that the system is minimally dimensioned for the average demand, i.e., (1
are cold and the remaining ones are idle-on. In both pictures, we observe that the ﬂuid model captures the
dynamics of X N (t) accurately.

1−λ
1
β + 1

P

−

γ

Let us provide some intuition about the dynamics in Figure 1. Initially, the system is close to instability
as capacity exactly matches demand. Here, Q(x(t)) increases rapidly and as soon as a warm server is created,
it is ﬁlled with a job and as a result the proportion of idle-on servers decreases. There is also another factor
that makes the number of idle-on servers decrease: Power-of-d, with d = 1, is not so powerful in discovering
idle-on servers because a fraction of them go cold even in heavy load. This explains why the number of cold

8

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

x

x

x

00

01

02

y

X

1
N
X
00
N
01
N
02
N
Y
1

X

100

200

300

t

9

8

7

6

5

4

3

2

1

0

Q(x)
Q(X N)

0

100

200

300

t

Figure 1: Numerical convergence of the stochastic model X N (t) (continuous lines), N = 103, to the ﬂuid
model x(t) (dashed lines) when combining Power-of-d with d = 1 and Blind-θ.

servers (the blues lines) is increasing at the beginning. Then, more warm servers are created to mitigate the
eﬀect of the “close to instability” window on the accumulated overall number of jobs. Here, the mass of busy
servers (y1) becomes greater than the average demand λ = 0.7 and Q(x(t)) decreases. Finally, dynamics
stabilize and in equilibrium there is a strictly positive fraction of servers that remain cold, initializing and
idle-on. This indicates that there is a ﬂux of idle-on servers that expires continuously even in equilibrium.

3.2 Characterization of Fixed Points

Theorem 1 justiﬁes the use of the ﬂuid model x(t) to approximate the behavior of the stochastic model X N (t).
We now investigate properties of the ﬂuid model and in particular its ﬁxed points when buﬀer sizes are inﬁnite.
The ﬂuid model has the form ˙x = F (x); see Deﬁnition 1. We say that x∗
∈ S1 is a ﬁxed point if F (x∗) = 0.
Now, for x

∈ S1, let us deﬁne the following conditions:

x0,0 + x0,1 + x0,2 + λ = 1
βx0,1 = γx0,2
αg(x),
γx0,2 ≤
γx0,2 = αg(x),

if x0,0 = 0
if x0,0 > 0

if Power-of-d is used: xi,2 = (λ + x0,2)

λ
λ+x0,2

if JBT-d is used: if x0,2 = 0 :

di−1
d−1

−

di+1−1
d−1

λ
λ+x0,2

,

!

1

i

≥

(cid:16)
xi,2 = 0,

(cid:17)
0

≤
xd+i,2 = xd+1,2

(0, λ]

(cid:16)

xd+1,2 ∈
g(x) = 0

(cid:16)

i

≤
1

−

d
xd+1,2
λ

(cid:17)

i−1

(cid:17)

,

i

2

≥

if x0,2 > 0 :

xi,2 =

i

λ
zd + x0,2 (cid:19)

(cid:18)

x0,2 I

{1≤i≤d+1},

1

i

≥

with zd

∈

[0, 1] being the unique solution of

1

zd + x0,2 =

d+1

x0,2

λ
zd+x0,2

(cid:17)
λ
zd+x0,2

−
(cid:16)
1
−

9

(8a)
(8b)

(8c)
(8d)

(8e)

(8f)

(8g)

(8h)
(8i)

(8j)

(9)

 
1 and zd = 0 if d = 0. Here, zd is interpreted as the proportion of busy servers with no more than d

if d
jobs. Let us also introduce the following assumption.

≥

Assumption 3. For any x0,2 ∈

[0, 1

−

λ], (8e)-(8j) uniquely determine xi,2 for all i

1.

≥

Within Power-of-d, this assumption is clearly satisﬁed by (8e). Within JBT-d, it is satisﬁed only if
i,j )

x0,2 > 0, as if x0,2 = 0, then xd+1,2 is only required to belong to (0, λ]. Under Assumption 3, let x◦ = (x◦
0,1 = γ
be the unique point in

β+γ (1
The following result characterizes ﬁxed points and provides a framework to evaluate the equilibrium ﬂuid

S1 such that x◦

0,0 = 0, x◦

0,2 = β

β+γ (1

λ), x◦

λ).

−

−

performance induced by generic scaling rules; see Section 7.2 for a proof.

∈ S1 satisﬁes the conditions in (8)-(9), then it is a ﬁxed point of the ﬂuid model with

. In addition, under Assumption 3

Theorem 2. If x∗
B = +

∞
1. x∗
0,0 > 0 if and only if

αg(x◦) <

1
λ
−
1
β + 1

γ

.

(10)

2. If (10) does not hold, then x◦ is the unique ﬁxed point.

Proof. Given in Appendix 7.

At the ﬂuid scale and in a ﬁxed point, Theorem 2 also provides the boundary scaling probability that
distinguishes between a saturated and a non-saturated system. Speciﬁcally, if g(x◦) does not satisfy (10),
then no cold server exists in a ﬁxed point but we observe that (8a) and (8b) imply that a strictly positive
λ). The interpretation is that there is a mass of idle-on
fraction of servers remain initializing,
servers that go cold but instantly become initializing. This corresponds to a waste of resources because
initializing servers cannot process jobs. In other words, a better performance may be obtained by keeping
the initializing servers warm at all times (no auto-scaling); recall also that billing policies charge warm and
initializing servers. If g(x◦) satisﬁes (10), then in a ﬁxed point there still exists a fraction of idle-on servers
that go cold and instantly become initializing, provided that g(x◦) > 0. However, the pool of cold servers
remains non-empty and this may give a beneﬁt in terms of performance versus operational cost tradeoﬀs
because cold servers are not charged.

γ
β+γ (1

−

Within Blind-θ, g(x) = θ and the conditions (8)-(9) easily identify a unique ﬁxed point, say x∗, with
0,0, x∗
0,2) not depending on the choice of the load balancing algorithm. The following remark says

0,1, x∗

(x∗
that uniqueness is not always guaranteed.

Remark 1 (Multiple Fixed Points). Suppose that g(x) = 0 whenever y1 = λ = 1
x0,0 and that JBT-d
is used. Then, Theorem 2 implies that uncountably many ﬁxed points exist. In fact, while xi,2 = 0 for all
i = 0, . . . , d and xi,2 is uniquely determined for all i
d + 2 once ﬁxed xd+1,2, the conditions (8)-(9) do not
tie xd+1,2 ∈
3.2.1 Blind-θ and Random Dispatching

(0, λ] to a speciﬁc value.

−

≥

For illustration purposes, let us consider Blind-θ with random dispatching (Power-of-1). This combination
does not involve any communication overhead among the auto-scaler, dispatchers and servers, and for this
reason it is well suited for large systems with vast numbers of dispatchers. Here, Theorem 2 identiﬁes a
unique ﬁxed point, x∗. After some algebra, we obtain x∗
and for the mean queue length
per warm server, Q(x) = 1
y0

0,2 = min
i ixi,2, we obtain (using also (8e))

αθ
γ , x◦
0,2

o

n

P

Q(x∗) =

λ
αθ
γ , x◦
0,2

.

min

(11)

As long as a strictly positive fraction of cold servers exists, or equivalently αθ
grows linearly in λ.

γ < x◦

0,2, we remark that Q(x∗)

n

o

10

3.3 Convergence to Multiple Fixed Points

As in the theory of dynamic systems, one is typically interested in whether or not any ﬂuid solution x(t)
converges to x∗, as t
, regardless of the initial condition x(0) and provided that x∗ is the unique ﬁxed
point. In this case, x∗ is called “global attractor”. Unfortunately, we have already shown in Remark 1 that
uniqueness of a ﬁxed point is not always guaranteed. To guarantee uniqueness, the following proposition
shows that it is not enough, as one may expect, to have a strictly positive scaling probability whenever the
current capacity of warm servers is less than the average demand, i.e., g(x) > 0 whenever y0 < λ.

→ ∞

Let

Ssubopt :=

x

{

∈ S

: x0,0 = 1

−

λ, x0,1 = x0,2 = 0, x1,2 < λ and (8g) holds with d = 0

(12)

}

and let Q(x) :=
initializing servers are included in the counting.

i≥1 ixi,2 denote the average number of jobs per server in state x

P

; here, cold and

∈ S

Proposition 1. Let g(x) be any scaling rule such that

g(x) =

1
λ

(x0,0 −

1 + λ)+,

x

∀

∈ S

: y0 < λ.

Let x(t) denote a ﬂuid model induced by such g(x) and JIQ such that

x0,0(0) > 1

−

λ, x0,2(0) = 0, x1,2(0) + βx0,1(0) < λ < Q(x(0)) <

.
∞

Suppose that β < 1, α

= β and B = +

. Then,

∞

g(x(t)) > 0,

≥
Q(x(t)) = Q(x(0)) +

t
∀

0

lim
t→∞

α + β
αβ

(x0,0(0)

−

1 + λ) +

x0,1(0)
β

> λ.

(13)

(14)

(15)

(16)

In addition, y0(t)

λ, and if x1,2(t)

↑

→

Proof. Given in the Appendix.

x1,2(

), then x(t)

∞

x(

) with x(

→

∞

)
∞

∈ Ssubopt.

Thus, while the proportion of warm servers converges to λ, such convergence may occur from below even
if there always exists a strictly positive probability of creating new warm servers. In this case, the average
demand is greater than the current service capacity at any point in time and this makes the mean queue
length converge to a limit that depends on the initial conditions.

↑

Let us comment a little bit further and prepare the setting for our next contribution. To create the
underload situation above where y0(t)
λ, it is not necessary to assume that all warm servers are initially
busy (x0,2(0) = 0), though we have included this condition in (14) to simplify our proof. In contrast, to avoid
this situation, it may be suﬃcient that g(x) is bounded away from zero whenever y0 < λ. By continuity, this
implies that g(x) > 0 as well whenever y0 = λ, but in this case the resulting scaling rule will not possess
the optimality property stated in Theorem 3 below (as this will imply that g(x⋆) > 0). On the other hand,
one may consider a scaling rule that is discontinuous on the set
, a setting that does not satisfy
Assumption 2. Here, beyond revisiting Theorem 1 for justiﬁcation of the ﬂuid model, the problem is that
scale-up decisions would signiﬁcantly depend on small perturbations of the equilibrium system state, severely
impacting robustness from a practical standpoint.

x : y1 = λ
}

{

3.4 Optimal (Stable) Design

Within Blind-θ, Theorem 2 guarantees the existence of a unique ﬁxed point, say x∗, and all of our numerical
simulations, which we omit, indicate that it is a global attractor. Here, necessarily x∗
0,2 > 0, by (8d),
which means that a number of warm servers remain idle-on in equilibrium. Clearly, this is not optimal for
energy consumption because idle-on servers consume energy. While it is mathematically interesting to ﬁnd
conditions on the scaling rule that imply the existence of a global attractor, our goal here is more ambitious:
we are interested in how to drive the limit behavior of any ﬂuid solution x(t) to the optimal point x⋆
,
uniquely deﬁned by x⋆

1,2 = λ, regardless of its initial condition x(0)

λ and x⋆

0,0 = 1

∈ S

∈ S1.

−

11

6
Remark 2 (Fluid Optimality). Following [26], in x⋆ dynamics have achieved “delay and relative energy
optimality” in the sense that both the waiting time of jobs and the relative energy portion consumed by idle-on
and initializing servers vanish in the limit. Here, a possible intuition is that each job is always assigned to a
busy server with exactly one job but at the precise moment where it completes the processing of its previous
job. Therefore, service capacity perfectly matches demand.

A direct consequence of Theorem 2 and (8d) is that it is necessary to impose g(x⋆) = 0 to achieve
ﬂuid optimality. Within Power-of-d, this is impossible as this condition would imply that x0,2 = 0, and
then (8e) would imply xi,2 = 0 for all i, contradicting that
= 1. In fact, Theorem 2 implies that the
x
k
unique candidate is JIQ, though it leaves open the possibility that x(t) may converge to a ﬁxed point in
the sub-optimal set
Ssubopt, see (12). Thus, it remains to understand what additional structure the scaling
rule g(x) should satisfy to make x⋆ a global attractor. Here, we observe that Remark 1 suggests that even
the knowledge of the amount of busy servers is not enough. More precisely, it implies that one needs g(x) > 0
Ssubopt and x⋆,
for all x
we have the following remark.

∈ Ssubopt as otherwise multiple ﬁxed points exist. Therefore, given the structure of

k

Remark 3. A ﬂuid optimal scaling rule needs the access to the amount of busy servers with exactly one job,
i.e., x1,2.

The following result provides a general condition that yields ﬂuid optimality.

Theorem 3 (Optimal Design). Let β < 1 and let x(t), with x(0)
JIQ and any scaling rule g(x) that satisﬁes, beyond Assumption 2,

∈ S1, denote a ﬂuid solution induced by

g(x) = 0 if and only if x1,2 + βx0,1 ≥

λ.

(17)

Then, limt→∞ k

x(t)

−

x⋆

k

w = 0.

The key observation is that x1,2 + βx0,1 is interpreted as the overall rate at which servers become idle-on.
Thus, our requirement for the design of an optimal scaling rule is to ensure to scale up resources whenever
the excess of the mean demand over the rate at which servers become idle-on is positive, as in this case
JIQ is powerful enough to ﬁll them up immediately saturating the surplus service capacity. Otherwise, the
scale-up process is turned oﬀ (g = 0), and in this case the natural dynamics induced by both JIQ and the
scale-down rule are enough to drive the system behavior to the desirable conﬁguration x⋆.

Remark 4. Rate-Idle, see Deﬁnition 2, satisﬁes (17).

As discussed in Section 2.1, the assumption β < 1, i.e., the mean server initialization rate is smaller than
the mean job service rate, is largely accepted in practice [22, 41]. From a mathematical standpoint, it is not
necessary for ﬂuid optimality but simpliﬁes our proof.

Remark 5 (Communication Overhead). A scaling rule satisfying (17) requires the central controller to have
access to the amount of initializing and busy servers containing exactly one job, i.e., x0,1 and x1,2. Since an
initializing server informs the platform as soon as it becomes warm, x0,1 is easily obtained in practice. For
x1,2, the auto-scaler can run a local memory with N slots, where the n-th slot indicates the state of server n,
say ‘Cold’, ‘Init’, ‘Idle-on’, ‘Busy1’ and ‘Busy≥2’, with obvious interpretations. Then, one way to update
the memory is by letting each server send a message to the auto-scaler whenever the transitions ‘Busy≥2’
‘Busy1’ occur. As in standard implementations of JIQ, this
→
involves only a constant number of messages per job to be exchanged between the auto-scaler and the servers.

‘Idle-on’ and ‘Idle-on’

‘Busy1, ‘Busy1’

→

→

Finally, the condition (17) is general enough to provide the platform user with some ﬂexibility when
designing an optimal scaling rule. This opens the way to a new level of optimizations as discussed in the
next section.

4 Energy Optimization with Performance Guarantees

Given that the ﬂuid model provides a tractable ﬁrst-order approximation of the stochastic dynamics within
ALBA and that we have considered mild assumptions on the structure of scaling rules (Assumption 2), it is

12

natural to use the ﬂuid model in an optimization framework to trade oﬀ between performance and energy
costs. As proof of concept, the objective of this section is to show a high-level application of our results
that may serve as a base for future more sophisticated techniques, which may integrate machine learning
mechanisms or consider switching costs.

We limit the focus on JIQ and on sets of scaling rules that satisfy the assumptions in Theorem 3, say
, as they automatically imply ﬂuid optimality (Remark 2) in the stationary regime. Let us deﬁne the
g as the long-run time average of a linear combination between the power consumption
i ixi,2

G
cost function
P (x) = c0,1x0,1 + c0,2x0,2 + c1,2y1, ci,j > 0, and the average queue length per busy server Q(x) = 1
y1
induced by the scaling rule g (as in, e.g., [4]), i.e.,

J

g := lim
T →∞

J

1
T

0
Z

T

(κ1P (x(t)) + κ2Q(x(t))) dt

P

(18)

where κi
≥
implies that

0, i = 1, 2; one can think κ1 in terms of $/watt and κ2 in terms of $/job. Then, Theorem 3

inf
g J

g =

g∗ = κ1P (x⋆) + κ2Q(x⋆) = κ1c1,2λ + κ2,

J

g∗
∀

.

∈ G

(19)

G

yield the same (optimal) cost, their behavior is clearly diﬀerent trajectory-wise.
While all policies in
Depending on the application, a platform user has several options to single out a policy in
that satisﬁes a
further level of optimization. For instance, a substantial portion of the applications hosted in cloud networks
have ultra-low delay requirements, as this may have important consequences on e-commerce sales. On the
other hand, also energy bills are equally important from both ﬁnancial and environmental standpoints. Here,
a system manager may want to look for a scaling rule in

such that

G

G

where q is related to the desired user-perceived performance guarantee; by Little’s law, (20) is equivalent
to a constraint on the mean response time. Towards this purpose, the parameterized subset of scaling rules
taking the form

Q(x(t))

q,

≤

t
∀

≥

0

(20)

g(x) =

1

−

exp(

η
λ (λ
−
1

−

−
exp(

x1,2 −
η)
−

βx0,1)+)

,

η > 0,

(21)

↓

λ

→ ∞

{λ≥x1,2+βx0,1} when η

0, and that g(x) = I

may be considered. Note that these satisfy both Assumption 2 and (17), that Rate-Idle is recovered when
η
. The control parameter η > 0 indicates how aggressive
the scaling rule is. This provides intuition for considering scaling rule of the form in (21). Then, the goal is to
ﬁnd η such that (20) holds true, provided that one exists. This problem can be easily addressed numerically
within the proposed deterministic model. Assume that the system is currently in a light-load condition, say
λ = 0.25, and that, as a result, it is dimensioned accordingly to save energy, say x0,0 = 1
0.05, with
x0,2 = 0.05 and x1,2 = λ; the extra 0.05 is meant to keep a reserve of idle-on servers ready to go. Then,
at time zero, an unexpected workload peak occurs, and λ = 0.5. Here, the platform needs to automatically
adjust the service capacity while ensuring (20). Let us assume q = 2, α = 0.35, β = 0.1 and γ = 0.025. The
dashed lines in Figure 2 represent the dynamics of the ﬂuid queue lengths Q(x(t)) and scaling probabilities
g(x(t)), for η = 1, 103. The corresponding continuous lines represent the average of ten simulations of the
stochastic model X N (t) with N = 1000. First, let us remark that the ﬂuid model approximation accurately
captures the dynamics of X N (t), though it slightly underestimates queue lengths and scaling probabilities.
Now, let us consider η = 1. Initially, queue lengths increase as expected due to the surge of demand and
the scaling probability is large enough to drive the proportion of cold servers to zero. This explains the
non-diﬀerentiability point of the trajectory of the scaling rule because the amount of initializing servers
stops to grow. Then, the system has enough capacity to drain the load and at some point the rate at which
servers become idle-on overﬂows the mean demand, i.e., x1,2 + βx0,1 > λ, so that eventually g(x(t)) = 0.
Finally, queue lengths assess to their asymptotic value Q(x⋆) = 1 We conclude that η = 1 is enough to
make (20) holds true. We also notice that the choice η = 103, which essentially means to scale up resources
at the maximum available rate α whenever x1,2 + βx0,1 < λ, has little impact on performance. Nonetheless,
it should be clear that the larger the value of η, the larger the resulting time-average power consumption.
A deeper analysis is out of the scope of the present work.

−

−

13

2

1.8

1.6

1.4

1.2

1

0.8

0

Q(x(t)), =1
g(x(t)), =1
Q(x(t)), =10 3
g(x(t)), =10 3
q=2
Q(X N(t)), =1
g(X N(t)), =1
Q(X N(t)), =10 3
g(X N(t)), =10 3

2

4

6

8

10

12

t

1

0.8

0.6

0.4

0.2

0

Figure 2: Transient behavior of the queue lengths and scaling probabilities by varying η, see (21), for the
both the ﬂuid (x(t)) and stochastic (X N (t)) models with N = 1000.

5 Literature Review

The existing literature related to load balancing and auto-scaling is huge and our goal here is to provide the
necessary background highlighting the diﬀerence with respect to our work.

The most popular examples of load balancing techniques that work well when servers are homogeneous,
i.e., all servers have the same processing speed, are Random, Round-Robin (RR) [20], Power-of-d [24], Join-
the-Idle-Queue (JIQ) [21], Least-Left-Workload (LLW) and Size Interval Task Allocation (SITA) [19, 18].
Random sends each job to random server, RR sends jobs to servers in a cyclic manner, Power-of-d sends an
incoming job to the least loaded server among d selected uniformly at random. JIQ sends an incoming job
to a random idle server if an idle server exists and to a random one otherwise, LLW sends an incoming jobs
to the queue having the shortest workload, and SITA sends a job to a given server if its size belong to a
given interval. In all cases, ties are broken randomly. In general, it is not possible to identify which of these
algorithms is the best because the general answer depends on the underlying architecture, load conditions,
service time distribution and on the amount of information available to the dispatcher [37]. Recently, a
number of works attempted to understand under which conditions the mean waiting time can be driven
down to zero in the mean ﬁeld limit, i.e., in the limiting regime where the arrival rate grows linearly with the
number of servers while keeping the average load below one. It has been shown that this is possible within
diﬀerent load balancing algorithms and architectures. Examples include JIQ [32], Power-of-d with d
as
the network size grows to inﬁnity [25], Power-of-d with memory [6], SITA combined with RR [5] and the pull-
based policies developed in [13, 36]. To some extent, the fundamental limits of load balancing are described
in [13], where the authors investigate trade-oﬀs between performance, communication overhead and memory
within a certain class of symmetric architectures. There are several variants and generalization of the above
algorithms and these are meant to handle i) heterogeneous servers [33, 9], ii) multiple dispatchers [35, 33],
iii) increased information to the dispatcher [17, 6], or iv) the case where model parameters such as arrival
and service rates are unknown [7].

→ ∞

The dynamics of the above dispatching algorithms have been analyzed under the assumption that jobs are
dispatched among a constant number of parallel servers. To the best of our knowledge, the only exceptions
are [26, 16], which provide the ﬁrst performance model where load balancing and auto-scaling operate jointly
within the same timescale. Here, JIQ is combined with a speciﬁc scaling strategy that turns a server oﬀ if
that server remains idle for a certain amount of time, and turns a server on at the moment of a job arrival
if all active servers are busy upon arrival of that job. In a regime where the traﬃc demand and nominal
service capacity grow large in proportion, this mechanism yields zero user-perceived delay as in the classical

14

version of JIQ with no auto-scaling [32] but also deactivates any surplus idle servers, ensuring that the
relative energy wastage vanishes in the limit as well. In this sense, this strategy induces asymptotic delay
and relative energy optimality. These properties have been strengthened in [27], where the authors relax
some ﬁnite buﬀer assumptions.

Let us comment on the diﬀerences between our work and [26, 16]. First, while in these references the
load balancing and auto-scaling processes are synchronous, in our framework they operate asynchronously;
see below for more details about the synchronous and asynchronous approach. Second, we do not focus on a
speciﬁc load balancing and auto-scaling strategy: our framework allows for the design of general scaling rules
(see Assumption 2) and considers a wide class of load balancing schemes that is not limited to JIQ. In fact,
Power-of-d may be the best option when the underlying architecture is composed of multiple dispatchers
because it still involves a constant communication overhead per job, which is not the case within JIQ. Third,
we show that delay and relative energy optimality can be achieved under broad assumptions (see Theorem 3).
In turn, these yield the deﬁnition of optimization frameworks allowing the system manager to further trade
oﬀ between energy and performance according to his needs.

Another reference that is close to our work is [22], where the authors propose a performance model for
auto-scaling that is tailored to the serverless computing platform AWS Lambda. Here, the auto-scaling
strategy is similar to [26] in the sense that it is synchronized with job arrivals but diﬀerent because an
incoming job that ﬁnds all servers busy is sent anyway, after a coldstart latency, to an idle server that is
turned on at the moment of its arrival. If this is not possible because no further server is available, then the
job is rejected. Within this approach, there is no queueing because jobs are always assigned to idle servers.
We also mention [11], where the authors propose a synchronous and decentralized approach that is an
hybrid between [22] and [26]. Upon arrival, a job visit the active servers in a sequential order until a idle
one is met. This server will process the job if such server exists, otherwise the job queues up in the last
active server and this situation triggers a scale-up decision if the number of jobs in such queue overﬂows
a given threshold. This architecture is meant to operate entirely within the network layer (Layer 4) using
IPv6 Segment Routing. As in [22], their performance analysis assumes a timescale separation between the
load balancing and auto-scaling processes.

There is a line of research works developing performance models for auto-scaling in a diﬀerent vein. One
approach that has been widely considered in the literature is based on the assumption that jobs join a global
queue before being dispatched to one out of a number of parallel servers. This type of approach yields models
that are variations of M/M/K or M/M/K/N queues [15, 43, 28, 4]. In contrast, the focus of this paper is
on decentralized architectures where each server has its own queue, which enhance scalability. A further
class of performance models for auto-scaling in cloud systems assume that job dynamics reach stochastic
equilibrium between consecutive changes of the overall processing capacity [39, 31]; see also Section 8.4
in [29]. This approach is arguable in the context of serverless computing platforms because the auto-scaling
process operates at a fast timescale [40, 41, 22]. Finally, we mention that auto-scaling can be performed at
the level of a single centralized processor. This type of auto-scaling is referred to as “speed scaling” [42, 4].

5.1 Synchronous vs Asynchronous in Serverless Computing

The load balancing and auto-scaling processes of existing implementations of public serverless computing
platforms can be “synchronous” or “asynchronous”; this terminology is borrowed from the cloud computing
community [22].

• In platforms such as AWS Lambda, Azure Functions, IBM Cloud Functions and Apache OpenWhisk,
a new server (instance) is turned on at the arrival time of a job if the job itself ﬁnds all servers busy
[30, 41]. Then, once the server is turned on, the job is executed on that server. The extra time that
an incoming job may incur if it ﬁnds all servers busy (usually referred to as coldstart latency [22]) can
pay oﬀ because of the order of 102 milliseconds [41]. Sporadic excesses of such duration are acceptable
for a substantial number of applications hosted in these systems. Within these technologies, the auto-
scaling process is thus synchronized with job arrival and dispatching decision times, and all the above
implementations require the incoming jobs to wait in a central queue before dispatching. This type of
auto-scaling is also referred to as scale-per-request [22].

• In platforms such as Google Cloud Run and Knative, the load balancing and auto-scaling processes

15

are decoupled in the sense that servers are turned on asynchronously at certain user-deﬁned scaling
times. Speciﬁcally, upon arrival, a job is immediately dispatched to some running server according to
some load balancing algorithm such as Power-of-d [2]. Independently of this, the auto-scaling process
decides whether the current processing capacity should increase as a function of user-deﬁned metrics,
or scaling rules, that may depend on instantaneous observations of the current system state. Because
of this decoupling, jobs may be assigned to busy servers. Asynchronous implementations of serverless
platforms such as Knative are decentralized, i.e., each server has its own queue, and thus highly scalable.
This type of auto-scaling is also referred to as concurrency-value-scaling [22].

Remark 6. Existing implementations of decentralized serverless architectures are asynchronous.
In this
setting, no dynamic performance model is available in the literature, to the best of our knowledge. The main
motivation underlying this work has been to ﬁll this gap.

6 Conclusions

In cloud systems, load balancing and auto-scaling are key mechanisms to optimize both user-perceived delay
performance and energy consumption. The focus of the existing literature has been on architectures where
these mechanisms are synchronous or rely on a central queue. The novelty of our work is to consider an
asynchronous and decentralized architecture. In the limit where the network demand grows in proportion
to the nominal service capacity, our work provides a tractable framework to evaluate the impact of existing
or new auto-scaling algorithms that are up to the platform user to choose or design. In addition, it provides
a general condition under which, with a minimal communication overhead, it is possible to drive dynamics
to the ideal situation where the user-perceived delay and the relative energy wastage induced by idle servers
vanish. While this property also holds within the TABS scheme recently proposed in [26], our contribution
consists in showing that it holds as well in asynchronous architectures provided that resources are scaled
up if and only if the mean demand is greater than the rate at which servers become idle-on (Theorem 3).
We have discussed in Remark 5 how such rate, x0,1 + βx1,2, can be estimated over time but here one may
employ other approaches, for instance based on machine learning techniques.

We discuss some perspectives and generalizations of our work:

• The load-balancing algorithms considered within our framework are Power-of-d and JBT-d. However,
one may consider some of the algorithms mentioned in Section 5 as well to see, for example, whether
ﬂuid optimality can be obtained even without using JIQ (while keeping a comparable communication
overhead).

• We have assumed that only one server at a time can be activated at each scaling time. Our framework
and results generalize trivially to the case where a random number C of cold servers is selected instead
of one, provided that the distribution of C does not depend on the overall number of servers N . Mutatis
mutandis, it is enough to replace α by α E[C].

• Blind-θ, see Deﬁnition 2, is the simplest scaling rule and it may be the best option in systems with a
large number of dispatchers because in this setting the communication overhead induced by algorithms
such as JIQ becomes a limiting factor. While Theorem 2 implies the existence of a unique ﬁxed point,
we leave as future work whether it is a global attractor. This is not trivial as one may check that
natural adaptations of classical Lyapunov functions from queueing theory do not work.

• Since we have proposed a class, instead of a speciﬁc instance, of ﬂuid optimal scaling rules, our work
provides the system manager with some degrees of freedom when designing a suitable scale-up strategy.
This opens the way to a new level of optimization problems such as the one presented in Section 4 as
an application of our results.

References

[1] Knative docs v1.3. https://knative.dev/docs/, 2022. Online; accessed: 2022-04-04.

16

[2] Knative Load balancing. https://knative.dev/docs/serving/load-balancing/, 2022. Online; accessed:

2022-04-04.

[3] Knative scale bounds. https://knative.dev/docs/serving/autoscaling/scale-bounds/, 2022. Online; ac-

cessed: 2022-04-04.

[4] L. L. Andrew, M. Lin, and A. Wierman. Optimality, fairness, and robustness in speed scaling designs.
In Proceedings of the ACM SIGMETRICS International Conference on Measurement and Modeling
of Computer Systems, SIGMETRICS ’10, page 37–48, New York, NY, USA, 2010. Association for
Computing Machinery.

[5] J. Anselmi. Combining size-based load balancing with round-robin for scalable low latency.

IEEE

Transactions on Parallel and Distributed Systems, 31(4):886–896, 2020.

[6] J. Anselmi and F. Dufour. Power-of-d -choices with memory: Fluid limit and optimality. Math. Oper.

Res., 45(3):862–888, 2020.

[7] R. Atar, I. Keslassy, G. Mendelson, A. Orda, and S. Vargaftik. Persistent-idle load-distribution. Stochas-

tic Systems, 10(2):152–169, 2020.

[8] M. Bramson. State space collapse with application to heavy traﬃc limits for multiclass queueing net-

works. Queueing Syst. Theory Appl., 30(1/2):89–148, June 1998.

[9] H. Chen and H.-Q. Ye. Asymptotic optimality of balanced routing. Oper. Res., 60(1):163–179, Jan.

2012.

[10] J. Dean and L. A. Barroso. The tail at scale. Commun. ACM, 56(2):74–80, Feb. 2013.

[11] Y. Desmouceaux, M. Enguehard, and T. H. Clausen. Joint monitorless load-balancing and autoscaling
for zero-wait-time in data centers. IEEE Transactions on Network and Service Management, 18(1):672–
686, 2021.

[12] Y. Desmouceaux, P. Pﬁster, J. Tollet, M. Townsley, and T. Clausen. Srlb: The power of choices in load
balancing with segment routing. In 2017 IEEE 37th International Conference on Distributed Computing
Systems (ICDCS), pages 2011–2016, 2017.

[13] D. Gamarnik, J. N. Tsitsiklis, and M. Zubeldia. Delay, memory, and messaging tradeoﬀs in distributed
service systems. In Proceedings of the 2016 ACM SIGMETRICS International Conference on Measure-
ment and Modeling of Computer Science, SIGMETRICS ’16, pages 1–12, New York, NY, USA, 2016.
ACM.

[14] D. Gamarnik, J. N. Tsitsiklis, and M. Zubeldia. Delay, memory, and messaging tradeoﬀs in distributed

service systems. Stochastic Systems, 8(1):45–74, 2018.

[15] A. Gandhi, S. Doroudi, M. Harchol-Balter, and A. Scheller-Wolf. Exact analysis of the m/m/k/setup
class of markov chains via recursive renewal reward.
the ACM SIGMET-
RICS/International Conference on Measurement and Modeling of Computer Systems, SIGMETRICS
’13, page 153–166, New York, NY, USA, 2013. Association for Computing Machinery.

In Proceedings of

[16] D. Goldsztajn, A. Ferragut, F. Paganini, and M. Jonckheere. Controlling the number of active instances

in a cloud environment. SIGMETRICS Perform. Eval. Rev., 45(3):15–20, Mar. 2018.

[17] V. Gupta and N. Walton. Load balancing in the nondegenerate slowdown regime. Operations Research,

67(1):281–294, 2019.

[18] M. Harchol-Balter, M. E. Crovella, and C. D. Murta. On choosing a task assignment policy for a
distributed server system. Journal of Parallel and Distributed Computing, 59(2):204 – 228, 1999.

[19] M. Harchol-Balter, A. Scheller-Wolf, and A. R. Young. Surprising results on task assignment in server
farms with high-variability workloads. SIGMETRICS ’09, pages 287–298, New York, NY, USA, 2009.
ACM.

17

[20] Z. Liu and R. Righter. Optimal load balancing on distributed homogeneous unreliable processors.

Operations Research, 46(4):563–573, 1998.

[21] Y. Lu, Q. Xie, G. Kliot, A. Geller, J. R. Larus, and A. Greenberg. Join-idle-queue: A novel load
balancing algorithm for dynamically scalable web services. Perform. Eval., 68(11):1056–1071, Nov.
2011.

[22] N. Mahmoudi and H. Khazaei. Performance modeling of serverless computing platforms. IEEE Trans-

actions on Cloud Computing, pages 1–1, 2020.

[23] N. Mahmoudi, C. Lin, H. Khazaei, and M. Litoiu. Optimizing serverless computing: Introducing an
adaptive function placement algorithm. In Proceedings of the 29th Annual International Conference on
Computer Science and Software Engineering, CASCON ’19, page 203–213, USA, 2019. IBM Corp.

[24] M. Mitzenmacher. The power of two choices in randomized load balancing.

IEEE Trans. Parallel

Distrib. Syst., 12(10):1094–1104, Oct. 2001.

[25] D. Mukherjee, S. C. Borst, J. S. H. van Leeuwaarden, and P. A. Whiting. Asymptotic Optimality of

Power-of-d Load Balancing in Large-Scale Systems. ArXiv e-prints, Dec. 2016.

[26] D. Mukherjee, S. Dhara, S. C. Borst, and J. S. van Leeuwaarden. Optimal service elasticity in large-scale

distributed systems. Proc. ACM Meas. Anal. Comput. Syst., 1(1), June 2017.

[27] D. Mukherjee and A. Stolyar. Join idle queue with service elasticity: Large-scale asymptotics of a

nonmonotone system. Stochastic Systems, 9(4):338–358, 2019.

[28] H. Qian, D. Medhi, and K. Trivedi. A hierarchical model to evaluate quality of experience of online
services hosted by cloud computing. In 12th IFIP/IEEE International Symposium on Integrated Network
Management (IM 2011) and Workshops, pages 105–112, 2011.

[29] C. Qu, R. N. Calheiros, and R. Buyya. Auto-scaling web applications in clouds: A taxonomy and

survey. ACM Comput. Surv., 51(4), July 2018.

[30] M. Shahrad, R. Fonseca, I. Goiri, G. Chaudhry, P. Batum, J. Cooke, E. Laureano, C. Tresness, M. Russi-
novich, and R. Bianchini. Serverless in the wild: Characterizing and optimizing the serverless workload
at a large cloud provider. In 2020 USENIX Annual Technical Conference (USENIX ATC 20), pages
205–218. USENIX Association, July 2020.

[31] U. Sharma, P. Shenoy, and D. F. Towsley. Provisioning multi-tier cloud applications using statistical
bounds on sojourn time. In Proceedings of the 9th International Conference on Autonomic Computing,
ICAC ’12, page 43–52, New York, NY, USA, 2012. Association for Computing Machinery.

[32] A. L. Stolyar. Pull-based load distribution in large-scale heterogeneous service systems. Queueing Syst.

Theory Appl., 80(4):341–361, Aug. 2015.

[33] A. L. Stolyar. Pull-based load distribution among heterogeneous parallel servers: The case of multiple

routers. Queueing Syst. Theory Appl., 85(1-2):31–65, Feb. 2017.

[34] J. N. Tsitsiklis and K. Xu. On the power of (even a little) resource pooling. Stoch. Syst., 2(1):1–66,

2012.

[35] M. van der Boor, S. Borst, and J. van Leeuwaarden. Load balancing in large-scale systems with multiple
dispatchers. In IEEE INFOCOM 2017 - IEEE Conference on Computer Communications, pages 1–9,
May 2017.

[36] M. van der Boor, S. C. Borst, and J. van Leeuwaarden. Hyper-scalable JSQ with sparse feedback. Proc.

ACM Meas. Anal. Comput. Syst., 3(1):4:1–4:37, 2019.

[37] M. van der Boor, S. C. Borst, J. S. van Leeuwaarden, and D. Mukherjee. Scalable load balancing in

networked systems: A survey of recent advances. arXiv preprint arXiv:1806.05444, 2018.

18

[38] E. van Eyk, A. Iosup, C. L. Abad, J. Grohmann, and S. Eismann. A spec rg cloud group’s vision on the
performance challenges of faas cloud architectures. In Companion of the 2018 ACM/SPEC International
Conference on Performance Engineering, ICPE ’18, page 21–24, New York, NY, USA, 2018. Association
for Computing Machinery.

[39] D. Villela, P. Pradhan, and D. Rubenstein. Provisioning servers in the application tier for e-commerce
systems. In Twelfth IEEE International Workshop on Quality of Service, 2004. IWQOS 2004., pages
57–66, 2004.

[40] L. Wang, M. Li, Y. Zhang, T. Ristenpart, and M. Swift. Peeking behind the curtains of serverless
platforms. In Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference,
USENIX ATC ’18, page 133–145, USA, 2018. USENIX Association.

[41] L. Wang, M. Li, Y. Zhang, T. Ristenpart, and M. Swift. Peeking behind the curtains of serverless
platforms. In Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference,
USENIX ATC ’18, page 133–145, USA, 2018. USENIX Association.

[42] M. Weiser, B. Welch, A. Demers, and S. Shenker. Scheduling for reduced cpu energy. In Proceedings of
the 1st USENIX Conference on Operating Systems Design and Implementation, OSDI ’94, page 2–es,
USA, 1994. USENIX Association.

[43] B. Yang, F. Tan, Y.-S. Dai, and S. Guo. Performance evaluation of cloud service considering fault
recovery. In Proceedings of the 1st International Conference on Cloud Computing, CloudCom ’09, page
571–576, Berlin, Heidelberg, 2009. Springer-Verlag.

7 Proofs of Theorems 1, 2 and 3

We provide proofs for Theorems 1, 2 and 3.

7.1 Theorem 1: Connection between the Fluid and the Markov models

To prove Theorem 1, we follow two main steps. First, we use a common coupling technique to deﬁne the
Z+, on a common probability space and show that limit trajectories exist
processes (X N (t))t∈[0,T ], for all N
and are Lipschitz continuous with probability one. The arguments used in this step are quite standard [34,
8, 13]. Then, we prove that limit trajectories are ﬂuid solutions, which is the main technical diﬃculty, and
here we develop arguments speciﬁc to our model.

∈

7.1.1 Coupled Construction of Sample Paths

N

c(t) denote a Poisson process of rate c. We construct a probability space where the stochastic processes
N ≥1 are coupled. All the processes of interest can be constructed in terms of the following

Let
(X N (t))t∈[0,T ]}
{
mutually independent primitive processes:

φ(t), a Poisson process of rate φ := λ + 1 + α + β + γ. This process is deﬁned on (ΩE,

•

N
each jump of

φ(t) denotes the occurrence of an event.

N

E, PE) and

A

• (Wn)n, where the random variables Wn are

-valued i.i.d. and such that P(Wn = 0) = λ/φ,
0, 1, 2, 3, 4
P(Wn = 1) = 1/φ, P(Wn = 2) = α/φ P(Wn = 3) = β/φ and P(Wn = 4) = γ/φ. This process is
W , PW ) and will identify the type of the n-th event. Speciﬁcally, Wn = 0 indicates
deﬁned on (ΩW ,
a job arrival, Wn = 1 a potential job departure, Wn = 2 a scaling time, Wn = 3 a potential server
initialization, i.e., a server completed the initialization phase, and Wn = 4 a potential server expiration.

A

}

{

• (Ap

n)n, p = 1, . . . , d, (Dn)n, (In)n, (En)n and (Rn)n, where the random variables Ap

n, Dn, In, En
and Rn, for all n, are all i.i.d. and uniform over the interval [0, 1]. The rvs Ap
n, Dn, In, En will be
respectively used to select a server that i) will process an arriving job, ii) ﬁres a departure, iii) ﬁres
an initialization and iv) ﬁres an expiration. The rv Rn is related to the scaling rule and will decide
whether a new server will be activated. These processes are deﬁned on (ΩS,

S, PS);

A

19

• (X N (0))N , the process of the initial conditions, where each random variable X N (0) takes values in

This process is deﬁned on (Ω0,

A0, P0).

N .

S

Using that

φ(N t) and

a Poisson process produce independent Poisson processes, each process
constructed on the product space, say (Ω,

, P).

N

φN (t) are equal in distribution and the well-known fact that thinnings of
1, can be

, N

(X N (t))t∈[0,T ]}

{

≥

N

Now, let tn be the time of the n-th jump of
i
0, Y N
j=0 X N
then given by
P

−1(t) = 0 and 1x

j,2(t) for all i

≥

A = 1 if x

∈

A

N

φ(N t). Let also X N (t−) := lims↑t X N (s), Y N

i (t) :=
A and 0 otherwise. The coordinates of X N (t) are

X N

0,0(t) =X N

0,0(0) +

X N

0,1(t) =X N

0,1(0) +

X N

0,2(t) =X N

0,2(0) +

X N

i,2(t) =X N

i,2(0) +

1
N

1
N

1
N

1
N

Nφ(N t)

n=1 (cid:18)
X
Nφ(N t)

I
{Wn=2}

n=1 (cid:18)
X
Nφ(N t)

I
{Wn=4}1En

(0,X N

0,2(t

−

n )] −

I
{Wn=2}

I
{X N

0,0(t

−

n )>0} 1Rn

(0,g(X N (t

−
n ))]

I
{X N

0,0(t

−

n )>0} 1Rn

(0,g(X N (t

−

n ))] −

I
{Wn=3}1In

(0,X N

0,1(t

−
n )]

I
{Wn=1}1Dn
[Y N

0 (t

−
n ),Y N

1 (t

−

n )] −

I
{Wn=0}H0(t−
n )

+I

{Wn=3}1In

(0,X N

0,1(t

−

n )] −

I
{Wn=4}1En

(0,X N

0,2(t

−
n )]

n=1 (cid:16)
X

Nφ(N t)

I
{Wn=0}

Hi−1(t−
n )

Hi(t−

n )I

{i<B}

−

n=1 (cid:18)
X

(cid:0)
+I

{Wn=1}

(cid:1)

1Dn
(Y N

i (t

(cid:18)

−
n ),Y N

i+1(t

−

n )] −

1Dn
(Y N

i−1(t

−
n ),Y N

i (t

−
n )]

(22d)

(cid:19)(cid:19)

(22a)

(22b)

(22c)

(cid:19)

(cid:19)

(cid:19)

for all i

≥

1, where within Power-of-d (servers are selected with replacement)

Hi(t−

n ) :=

d

1

p=1
Y

and within JBT-d

Ap
(Y N

n(1−X N
i−1(t

0,0(t
−
n ),1]

−

n )−X N

0,1(t

−
n ))

d

1

−

p=1
Y

Ap
(Y N

n(1−X N
−
n ),1]
i (t

0,0(t

−

n )−X N

0,1(t

−
n ))

0, 1

}

∈ {

(23)

Hi(t−

n ) := 1

−

A1
(Y N

n(1−X N
i−1(t

0,0(t
−
n ),Y N

n )−X N
−
i (t
n )]

0,1(t

−
n ))

I
{Y N

d (t

n )=0} + 1A1

−

−
nY N
d (t
n )
−
n ),Y N
i−1(t

(Y N

i (t

I
{i≤d}

I
{Y N

d (t

−

n )>0} ∈ {

0, 1

.

}

(24)

−
n )]

These expressions follow by uniformization of X N (t). For instance, X N
0,0(t) has an upward jump of size 1/N
at time tn if the event occurring at that time is of type 4 (potential server expiration) and an idle-on server
is actually selected at time t−
0,0(t) decreases by 1/N at time tn
if the event occurring at that time is of type 2, provided that at time t−
n the cold servers pool is not empty
and the scaling rule applies. Similar interpretations hold along the other coordinates of X N (t).

n by the uniformized process. Analogously, X N

7.1.2 Tightness of Sample Paths and Lipschitz Property

We now prove tightness of sample paths. The lemmas in this section are equivalent to the lemmas in [14,
Section 5.2].

Let us introduce the following formulas for quick reference.

Lemma 1. Let T > 0. There exists

Ω such that P(
C

C ⊆

) = 1 and for all ω

:

∈ C

lim
N→∞

sup
t∈[0,T ] |

1
N N

φ(N t, ω)

φt

|

−

= 0,

lim
N→∞

sup
t∈[0,T ]

Nφ(N t,ω)

n=1
X

1
N

(cid:12)
(cid:12)
(cid:12)

I
{Wn(ω)=k} −

P(W1 = k) φ t

= 0,

(cid:12)
(cid:12)
(cid:12)

20

k

∈ {

0, . . . , 4

}

(25)

(26)

lim
N→∞

1
N

N

d

n=1
X

p=1
Y

1cpAp
(ap,bp] =

n

d

bp

p=1
Y

ap

,

−
cp

ap, bp, cp

∀

∈

[0, 1], cp > 0, p = 1, . . . , d.

(27)

Proof. This lemma directly follows by applying the functional strong law of large numbers for the Poisson
process (for (25)), the fact that thinnings of a Poisson process produce independent Poisson processes (for
(26)) and the strong law of the large numbers (for (27)).

We will work on a ﬁxed ω that belongs to
C
Let x0
0 and BN
[0, 1], sequences AN

.

∈

↓

endowed with the uniform metric d(x, y) := supt∈[0,T ] |
x

x(0)

BN ,

E

N (BN , AN , x0) :=
c(x0) :=

E

∈

∈

x

(cid:8)

x0
D[0, T ] :
|
D[0, T ] : x(0) = x0,

−

| ≤

x(a)

|

−

y(t)
|
x(b)

−
x(a)

|
x(b)

−

| ≤

a

φ
|

∈

a

φ
|

∀

| ≤
b

−

,

|

−
a, b

b

|

∈

+ AN ,

[0, T ]

a, b

∀
.

∈

≥
[0, T ]

(cid:9)

↓

0 be given. Let also D[0, T ] denote the Skorokhod space
1, let also

D[0, T ]. For N

, for all x, y

x(t)

(cid:8)

The next lemma says that the sample paths along any coordinate is approximately Lipschitz continuous.
The proof is omitted because follows exactly the same standard arguments used in Lemma 5.2 of [14], which
basically use the fact that the jumps of the Markov chain of interest are of the order of 1/N and that the
evolution of such Markov chain on a given coordinate only depends on the evolution of such Markov chain
on a ﬁnite number of other coordinates.

(cid:9)

0

↓

∈ C

X N (ω, 0)
k
0 such that

0. Then, there exists sequences

, and some x0
B(i,j)
N

Lemma 2. Fix T > 0, ω
˜BN

∈ S1. Suppose that
↓
↓
o
N (B(i,j)
N , AN , x0),
The next proposition shows that any sequence of sample paths X N (ω, t) contains a further subsequence
w, to a coordinate-
−
. The proof is routine and omitted because it is a
wise Lipschitz continuous trajectory x(t), as long as ω
repetition of the argument used in the proof of Proposition 11 in [34] (equivalently, see also Proposition 5.3
in [14]).

that converges in D∞[0, T ], endowed with the metric dZ+(x, y) := supt∈[0,T ] k

˜BN , for some sequence

n
i,j(ω,

y(t)
k

and AN

(i, j),

X N

(28)

x(t)

∈ E

∈ C

N.

x0

≤

−

i,j

∀

∀

k

w

)

·

Proposition 2. Fix T > 0, ω
sequence ˜BN
such that

↓

0. Then, every subsequence of

∈ C

, and some x0

∈ S1. Suppose that

≤
∞
N =1 contains a further subsequence
{

−

k

k

)
}

·

X N (ω, 0)

x0

w

˜BN , for some
X Nk(ω,

∞
k=1

)
}

·

X N (ω,

{

where x(0) = x0 and xi,j

∈ E

Z+ (X Nk , x) = 0
d

lim
k→∞

c(x0), for all i and j.

(29)

Since Lipschitz continuity implies absolute continuity, we have obtained that limit points of X N (t) exist
and are absolutely continuous. Since all sample paths of X N (t) take values in
, these limit points must
is a closed set. Therefore, to conclude the proof of Theorem 1 it remains to
belong as well to
show that the derivative of xi,j(t) is as in Deﬁnition 1 for all i and j, provided that t is a regular time. This
is done in the next subsection and will also prove that a ﬂuid solution started in x(0)

because

S

S

S

∈ S1 exists.

7.1.3 Limit Trajectories are Fluid Solutions

Fix ω

and let

X Nk(ω, t)
}

{

∈ C

∞
k=1 be a subsequence that converges to x (by Proposition 2), i.e.

lim
k→∞

sup
t∈[0,T ] k

X Nk(ω, t)

x(t)
k

−

w = 0.

(30)

In the remainder, we ﬁx such ω
such that (30) holds and for simplicity we drop the dependency on ω.
Since x must be Lipschitz continuous (by Proposition 2), it is also absolutely continuous and to conclude
the proof of Theorem 1, it remains to show that x(t) satisﬁes the conditions on the derivatives given in
Deﬁnition 1 whenever xi,j(t) is diﬀerentiable, for all i, j.

∈ C

We say that t is a point of diﬀerentiability (of x) if xi,j (t) is diﬀerentiable for all i, j.
We will (implicitly) use several times the following elementary lemma, which holds true because x is a

non-negative absolutely continuous function.

21

Lemma 3. If xi,j(t) = 0 and t is a point of diﬀerentiability of xi,j , then ˙xi,j(t) = 0.

Let ǫ > 0. By Lemma 2, there exists a sequence ANk ↓

[t, t + ǫ]. Thus, for all k suﬃciently large, X Nk

0 such that X Nk
i,j (ω, u)

i,j (ω, u)

∈
[xi,j(t)

[xi,j (t)

ǫφ

ANk , xi,j (t)+
2ǫφ, xi,j(t) + 2ǫφ], for

−

−

∈

−

ǫφ + ANk ], for all u
all u

[t, t + ǫ]. Thus, we have

∈

∈

for all k suﬃciently large. In addition, using (31) and that g is Lipschitz, we obtain

X Nk

i,j (u)

|

−

xi,j(t)

| ≤

2φǫ,

u

∀

∈

[t, t + ǫ]

g(X Nk (u))

|

g(x(u))

| ≤

L

k

−

X Nk(u))

i,j
X
where L is the Lipschitz constant of the scaling rule g.

2φǫL

≤

s

w

x(u)
−
k
1
2i+j = 2φǫL√2,

u

∀

∈

[t, t + ǫ]

(31)

(32a)

(32b)

We will refer to the following lemma, which is a straightforward consequence of (31) and of the strong law
of the large numbers. In points where the ﬂuid drift function is continuous, it will provide an expression for
terms related to job departures, server initializations/departures and, in some cases, dispatching decisions.

Lemma 4. Fix ω

∈ C

and let (30) hold. Then,

lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=1}1Dn
(Y N

i−1(t

−
n ),Y N

i (t

−
n )]

lim
ǫ↓0

lim
k→∞

lim
ǫ↓0

lim
k→∞

1
ǫNk

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1
Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=3}1In

(0,X N

0,1(t

−
n )]

I
{Wn=4}1En

(0,X N

0,2(t

−
n )]

= xi,2(t)

= βx0,1(t)

= γx0,2(t).

In addition,

lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

provided that

d
j=0 xj,2 > 0, and

{Wn=0}1A1
I

−
nY N
d (t
n )
−
n ),Y N
i−1(t

(Y N

i (t

−
n )]

I
{i≤d} =

λI

{i≤d}xi(t)
d
j=0 xj,2(t)

P

P

lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=0}Hi(X N (t−

n )) = λhi(x)

provided that Power-of-d is used.

Proof. Given in Section 8.

The next proposition proves the desired condition on the amount of ﬂuid of cold and initializing servers.

Proposition 3. Fix ω

, let (30) hold and assume that t is a point of diﬀerentiability. Then,

∈ C
˙x0,0 = γx0,2(t)
−
˙x0,1 = αg(x(t))I
{x0,0(t)>0} −

αI

{x0,0(t)>0}g(x(t))

γx0,2(t) I
−
βx0,1(t) + γx0,2(t) I

{x0,0(t)=0, γx0,2(t)≤αg(x(t))}
{x0,0(t)=0, γx0,2(t)≤αg(x(t))}.

(33)

(34)

22

Proof. Assume that x0,0(t) > 0 and let ǫ

(0, x0,0(t)

2φ ). Given that

∈

tn

∈

(t, t + ǫ] whenever n

φ(Nkt) + 1, . . . ,

φ(Nk(t + ǫ))
}

N

,

(35)

(31) implies that for all k suﬃciently large,
have shown that

|

x0,0(t)

| ≤

−

2φǫ < x0,0(t) and thus X Nk

0,0 (t−

n ) > 0. We

I
{X

Nk
0,0 (t

−

n )>0} = 1,

n

∀

φ(Nkt) + 1, . . . ,

∈ {N

φ(Nk(t + ǫ))
}

N

(36)

∈ {N
0,0 (t−
n )

X Nk

for all k suﬃciently large. Using (22), Lemma 4 and (36), we have

˙x0,0(t) = lim
ǫ↓0

1
ǫ

= lim
ǫ↓0

lim
k→∞

1
ǫNk

= γx0,2(t)

−

lim
ǫ↓0

lim
k→∞

X Nk

0,0 (t + ǫ)

(cid:16)

Nφ(Nk(t+ǫ))

−

X Nk

0,0 (t)
(cid:17)

I
{Wn=4}1En
(0,X
Xn=Nφ(Nkt)+1 (cid:18)
1
ǫNk

Nφ(Nk(t+ǫ))

lim
k→∞

Xn=Nφ(Nkt)+1

Nk
0,2 (t

−
n )] −

I
{Wn=2}

I
{X

Nk
0,0 (t

−
n )>0}

I
{Wn=2} 1Rn

(0,g(X Nk (t

.

−
n ))]

1Rn
(0,g(X Nk (t

−
n ))]

(cid:19)

(37a)

(37b)

Since t is a point of diﬀerentiability, the double limit in the RHS of (37b) exists. Then, (32) implies that
given ǫ > 0 small enough, g(X Nk (t−
2φǫL√2, g(x(t)) + 2φǫL√2] for all k suﬃciently large.
−
Combining these bounds with Lemma 1 and letting ǫ

0 (as in the proof of Lemma 4), we obtain

[g(x(t))

n ))

∈

↓

lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

Similarly, on coordinates (0,1), we obtain

I
{Wn=2} 1Rn

(0,g(X Nk (t

−
n ))]

= αg(x(t)).

(38)

˙x0,1(t) = lim
ǫ↓0

1
ǫ

lim
k→∞

0,1 (t + ǫ)

X Nk
(cid:16)

−

X Nk

0,1 (t)
(cid:17)

Nφ(Nk(t+ǫ))

= lim
ǫ↓0

lim
k→∞

1
ǫNk

I
{Wn=2}

Xn=Nφ(Nkt)+1 (cid:18)

I
{X

Nk
0,0 (t

−
n )>0}

1Rn
(0,g(X Nk (t

−

n ))] −

I
{Wn=3}1In

(0,X

Nk
0,1 (t

−
n )]

= αg(x)

βx0,1(t).

−

Now, let us assume that x0,0(t) = 0. First, we notice that

˙x0,0(t) = γx0,2(t)

lim
ǫ↓0

lim
k→∞

−

γx0,2(t)

lim
ǫ↓0

lim
k→∞

−

≥

1
ǫNk

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1
Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=2}

I
{X N

0,0(t

−

n )>0} 1Rn

(0,g(X Nk (t

−
n ))]

I
{Wn=2} 1Rn

(0,g(X Nk (t

−
n ))]

= γx0,2(t)

αg(x)

−

(cid:19)

(39)

where the ﬁrst equality follows by (37a) and Lemma 4, and the last equality follows by (38). Thus, if
x0,0(t) = 0 and γx0,2(t) > αg(x), then by the previous inequality ˙x0,0(t) > 0, which is not possible because if
t is a point of diﬀerentiability and x0,0(t) = 0 then necessarily ˙x0,0(t) = 0 as x0,0 is a non-negative absolutely
continuous function. Thus, in a point of diﬀerentiability t where x0,0(t) = 0, we must have γx0,2(t)
αg(x).
and, necessarily, ˙x0,0(t) = 0. In this case, (39) gives

≤

γx0,2(t) = lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=2}

I
{X N

0,0(t

−

n )>0} 1Rn

(0,g(X N (t

−
n ))]

.

(40)

This term is interpreted as the amount of idle-on servers that become cold but instantly turn initializing.
Substituting (40) in the previous equalities within the conditions γx0,2(t)
αg(x) and x0,0(t) = 0, we obtain
(33) and (34).

≤

23

(41)

(42)

(43)

On the coordinates associated to warm servers, it remains to prove that

˙x0,2(t) = x1,2(t)
−
˙xi,2(t) = xi+1,2(t)I

λh0(x(t)) + βx0,1(t)

γx0,2(t)

−

{i<B} −

xi,2(t) + λ(hi−1(x(t))

hi(x(t))I

{i<B}),

−

1,

i

≥

whenever t is a point of diﬀerentiability of x. Let

i(t) := lim
ǫ↓0

lim
k→∞

H

1
ǫNk

Nφ(Nk(t+ǫ))

I
{Wn=0}Hi(t−
n )

0,

≥

Xn=Nφ(Nkt)+1
which is interpreted as the rate at which jobs are assigned to warm servers with exactly i jobs. Using
Lemma 4 and (22), we have

˙x0,2(t) = lim
ǫ↓0

˙xi,2(t) = lim
ǫ↓0

1
ǫ
1
ǫ

lim
k→∞

lim
k→∞

X Nk

0,2 (t + ǫ)

(cid:16)
X Nk

i,2 (t + ǫ)

−

−

X Nk

0,2 (t)
(cid:17)

(cid:16)
In the following, we need to show that
H
the cases of Power-of-d and JBT-d separately.

= x1,2(t)

− H0(t) + βx0,1(t)

−

γx0,2(t)

(44a)

X Nk

= xi+1,2(t)I

i,2 (t)
(cid:17)
i(t) = hi(x(t)) where the hi’s are as in Deﬁnition 1. We treat

{i<B} −

xi,2(t) +

{i<B}.

i−1(t)

(44b)

− H

H

i(t)I

Lemma 5. Assume that Power-of-d is applied. Then, (41) and (42) hold true.

Proof. If x0,0 + x0,1 < 1, then the structure of the Hi’s in (23) and Lemma 4 immediately give (41) and (42).
Now, let us assume that x0,0 + x0,1 = 1. On coordinate (0,2), in a point of diﬀerentiability we necessarily
have ˙x0,2 = 0. Using Lemma 4 and (22), we obtain

˙x0,2(t) = lim
ǫ↓0

1
ǫ

lim
k→∞

0,2 (t + ǫ)

X Nk
(cid:16)

−

X Nk

0,2 (t)
(cid:17)

= βx0,1(t)

− H0(t) = 0.

(45)

Similarly, on coordinate (1, 2), Lemma 4 and (45) imply that in a point of diﬀerentiability we have ˙x1,2(t) =
H0(t) = βx0,1(t). Then, on coordinate (i, 2) by induction we obtain
H0(t)
i(t) =
H

i−1(t) = βx0,1(t). On the other hand, we also have

− H1(t) = 0 and thus

H1(t) =

H

˙x0,2(t) = βx0,1(t)

lim
ǫ↓0

−

lim
k→∞

1
ǫ

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1
where in the last inequality we have just used that H0(t−
n )
and t can not be a point of diﬀerentiability. Substituting
obtain (41) and (42).

≤
H

I
{Wn=0}H0(t−
n )

βx0,1(t)

λ

−

≥

(46)

1. Thus, if βx0,1(t) > λ, we get a contradiction
λ, we
i(t) = βx0,1(t) in (44) when βx0,1(t)

≤

d
j=0 X N

The case of JBT-d is more delicate than Power-of-d because of the discontinuous structure of the Hi’s
when
n ) = 0, see (24). In addition to a more involved argument than the one presented in the
proof of Lemma 5, which we will develop in Lemma 7 below, we need the following lemma, which we will
d
use to determine an expression for
j=0 xj,2(t) = 0.

j,2(t−

i when

P

H

Lemma 6. Assume that x(t) satisﬁes x0,0(t) + x0,1(t) < 1. Then, for all i

P

lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=0}1

A1

(Y

Nk
0,0 (t

n(1−X
−
Nk
n ),Y
i−1(t

−
n )−X
−
Nk
(t
n )]
i

Nk
0,1 (t

−
n ))

I
{Pd

j=0 X

Nk
j,2 (t

−
n )>0}

=

xi,2(t)

x0,0(t)

1

−

−

x0,1(t)

lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=0}

Proof. Given in Section 8.

24

I
{Pd

j=0 X

Nk
j,2 (t

−

n )>0}.

(47)

The following lemma proves the desired property in the case of JBT-d.

Lemma 7. Assume that JBT-d is applied. Then, (41) and (42) hold true.

Proof. We analyze

First, if x0,0 + x0,1 = 1, the argument in the proof of Lemma 5 gives i)

i and the resulting expression will be substituted in (44). This will give (41) and (42).
λ
and ii) t not a point of diﬀerentiability when βx0,1(t) > λ. This gives (41) and (42) (when x0,0 + x0,1 = 1)
and in the remainder we assume that x0,0 + x0,1 < 1.

i(t) = βx0,1(t) when βx0,1(t)

H

H

≤

Let us now assume that
φ(Nkt) + 1, . . . ,
j=0 X Nk

j,2 (tn)

P
N
xj,2(t)

d

d
j=0 xj,2(t) > 0 and let ǫ

(t, t + ǫ] whenever
, (31) and the triangular inequality imply that for all k suﬃciently
d
j,2 (tn) > 0. We have shown

d
j=0 xj,2(t) and thus

j=0 X Nk

). Since tn

(0,

∈

∈

2(d + 1)φǫ <

Pd

j=0 xj,2(t)
2φ(d+1)

φ(Nk(t + ǫ))
}

| ≤

−
I
{Pd

j=0 X

Nk
j,2 (t

−

n )>0} = 1,

P
n

∀

∈ {N

φ(Nkt) + 1, . . . ,

P
N

φ(Nk(t + ǫ))
}

(48)

n
large
that

∈ {N
|

P

for all k suﬃciently large, given ǫ > 0 suﬃciently small. Substituting (48) in (24) and applying Lemma 4,
we obtain (41) and (42) (under the conditions x0,0 + x0,1 < 1 and

d
j=0 xj,2(t) > 0).

It remains to understand the terms

remainder of the proof.

i in the case where

H

d
j=0 xj,2(t) = 0, which we assume in the
P

Suppose that t is a point of diﬀerentiability. Then, by applying Lemma 4 to X N

0,2 (see (22)), we obtain

P

˙x0,2(t) = lim
ǫ↓0

1
ǫ

lim
k→∞

X Nk

0,2 (t + ǫ)

−

and given that necessarily ˙x0,2(t) = 0, we obtain

X Nk

0,2 (t) = x1,2(t)I

{d=0} + βx0,1(t)

H0(t) = x1,2(t)I

{d=0} + βx0,1(t).

Similarly, on coordinate (i, 2), with 0 < i

d, we obtain

≤

˙xi,2(t) = xi+1,2(t)I

{i=d} +

i−1(t)

H

i(t) = 0.

− H

− H0(t),

(49)

(50)

(51)

By induction, this gives

i(t) =

H

H0(t) = βx0,1(t) for all i < d and
i(t) = βx0,1(t) + xd+1,2(t)I
{i=d},

d(t) = βx0,1(t) + xd+1,2(t), that is,

H

d.

i

≤

(52)

H

We have proven (52) under the hypothesis that t was a point of diﬀerentiability but now we show that x(t)
is not diﬀerentiable if λ < xd+1,2 + (d + 1)βx0,1. Towards this purpose, ﬁrst we notice that

i(t) = lim
ǫ↓0

H

lim
k→∞

1
ǫNk

d

i=0
X

Nφ(Nk(t+ǫ))

d

I
{Wn=0}

Hi(t−
n )

Xn=Nφ(Nkt)+1

i=0
X

= lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=0}

I
{Pd

j=0 X N

j,2(t

−

n )>0} ≤

lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=0} = λ.

Here, the ﬁrst equality follows because the limits
that (recall the deﬁnition of

i in (43))

H

i(t) exist and the second inequality follows by the fact

H

Hi(t−

n ) = I

{Pd

j=0 X N

j,2(t

−

n )>0} + 1

A1
(0,Y N

n(1−X N
0,0(t
−
d (t
n )]

−
n )−X N

0,1(t

−
n ))

I
{Pd

j=0 X N

j,2(t

−
n )=0}

d

i=0
X

and by Lemma 4 because

d
j=0 xj,2(t) = 0. Then, using (52), we necessarily have

P

d

i=0
X

i(t) = xd+1,2(t) + (d + 1)βx0,1(t)

H

λ

≤

25

(53)

(54)

and, given that necessarily
does not hold true.

i
H

≥

0, we conclude that t can not be a point of diﬀerentiability whenever (54)

Now, we investigate
diﬀerentiable. We have

i when i > d and assuming that (54) holds as otherwise x(t) would not be

H

i(t) =

H

λxi,2(t)

x0,0(t)

1

−

−

x0,1(t) −

lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=0}1

−

A1
(Y N

n(1−X N
i−1(t

0,0(t
−
n ),Y N

n )−X N
−
i (t
n )]

0,1(t

−
n ))

I
{Pd

j=0 X N

j,2(t

−
n )>0}

λxi,2(t)

xi,2(t)

x0,0(t)

1

−

−

x0,1(t) −

1

x0,0(t)

x0,1(t)

−

−

lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=0}

I
{Pd

j=0 X N

j,2(t

−
n )>0}

=

=

1

−

= xi,2(t)

λ

−

λxi,2(t)

xi,2(t)

x0,0(t)

x0,1(t) −

1

x0,0(t)

x0,1(t)

−
xd+1,2(t)

−

−
(d + 1)βx0,1(t)

−
x0,0(t)

1

−

x0,1(t)

−

.

d

i=0
X

i(t)

H

In the ﬁrst equality, we have used (24) and applied Lemma 4 to the deﬁnition of
we have applied Lemma 6. In the third, we have used (53) and that

i in (43); In the second,

H

0

≤

lim
ǫ↓0

lim
k→∞

1
ǫNk

1

A1
(0,Y N

n(1−X N
0,0(t
−
d (t
n )]

−

n )−X N

0,1(t

−
n ))

I
{Pd

j=0 X N

j,2(t

−

n )=0} ≤

lim
ǫ↓0

lim
k→∞

1
ǫNk

1

A1
(0,Y N

n(1−X N
0,0(t
−
d (t
n )]

−

n )−X N

0,1(t

−
n ))

= 0

with the last inequality following by Lemma 4 as
This concludes the proof.

P

Thus, we have shown that x is a ﬂuid solution.

d
j=0 xj,2(t) = 0; in the fourth, we have substituted (52).

7.2 Proof of Theorem 2: Fixed points

We now prove Theorem 2. By deﬁnition, x

∈ S1 is a ﬁxed point if and only if
{x0,0>0} −

{x0,0=0, γx0,2≤αg}

αgI

{x0,0=0, γx0,2≤αg}

0 = γx0,2 −
0 = αgI
0 = x1,2 −
0 = xi+1,2 −

γx0,2 I
βx0,1 + γx0,2 I
γx0,2

{x0,0>0} −

h0(x) + βx0,1 −
xi,2 + hi−1(x)

hi(x),

i

1.

≥

−

(55a)

(55b)

(55c)

(55d)

Together with

= 1, we now show that these conditions coincide with (8).

x
k

k

If i) x0,0 = 0 and γx0,2 > αg, or if ii) x0,0 + x0,1 = 1, then we easily observe that x cannot be a ﬁxed

point. Therefore, in the following we exclude these conditions. Now, summing (55a) and (55b), we obtain

which gives (8b). Then, (8c) and (8d) directly follow from (55a) and (55b).

Substituting (56) in (55c), the conditions (55c)-(55d) become

βx0,1 = γx0,2

h0(x)

0 = x1,2 −
0 = xi+1,2 −

xi,2 + hi−1(x)

hi(x),

i

1,

≥

−

and taking summations

xi,2 = hi−1(x),

1.

i

≥

(56)

(57a)
(57b)

(58)

The equations in (57) are interpreted as the mean-ﬁeld ﬁxed-point equations associated to Power-of-d and
i≥0 xi,2 is the proportion of
JBT-d when the number of servers is N y0 instead of N ; we recall that yi =
warm servers with at least i jobs. Within Power-of-d, one can directly check that for any given x0,2, (58)
P
i≥1 xi,2 = λ so that (8a)
holds if and only if xi,2 is given by (8e) and that, after a substitution, this gives
must hold true. The following lemma, given in Section 8, handles the more delicate case of JBT-d.

P

26

Lemma 8. Within JBT-d, for any given x0,2, (57) holds if and only if xi,2 satisﬁes (8f)-(8j). In addition,
(8a) holds true.

Therefore, the conditions in (55) are equivalent to (8). This proves the ﬁrst statement of Theorem 2.
Now, under Assumption 3, xi,2 is a function of x0,2, for all i

1, and we write xi,2 as a shorthand

notation for xi,2(x0,2). Using (56), we can then focus only on the following conditions:

≥

(59a)

(59b)
(59c)

(60)

(61)

(62)

x0,0 +

+ 1

γ
β

(cid:18)

λ

x0,2 = 1

−
(cid:19)
αg,
γx0,2 ≤
γx0,2 = αg,

if x0,0 = 0
if x0,0 > 0.

Here, we notice that (x◦

0,0, x◦

0,2) =

0, β

β+γ (1

−

λ)

uniquely solves (59) if

(cid:0)

1
β

+

(cid:1)
1
γ

(cid:19)

αg(x◦)

λ

1

−

≥

(cid:18)
0,0, x◦

where x◦ is uniquely determined by (x◦
[0, 1)2 that solves (59) exists, then necessarily x0,0 > 0 as otherwise x0,2 = x◦
a point (x0,0, x0,2)
(59a)) and (60) would hold, contradicting the hypothesis. This proves the second part of Theorem 2.

0,2). So, let us assume that (60) does not hold true. Then, if
0,2 (by

∈

7.3 Proof of Theorem 3: Fluid Optimality

We give a proof for Theorem 3. The non-linear structure taken by the hi’s when x0,2 = 0, see (7), complicates
the analysis and the identiﬁcation of a Lyapunov function. For this reason, our strategy is based on a divide-
and-conquer approach. This will actually provide insights about the dynamics followed by ﬂuid solutions.
For simplicity, we provide a proof assuming that B <
, which is essentially equivalent to assume that
xi,2(0) = 0 for all i large enough; this is not critical as x⋆

∞
i,2 = 0 for all i
B
i=1 ixi,2, i.e., the overall number of jobs in the system in state x. The following lemma

Let Q(x) :=

2.

≥

gives a property on the time derivative of Q(x(t)).

P

Lemma 9. Let x(t) be a ﬂuid solution induced by JIQ such that x(0)
diﬀerentiability, then

∈ S1 and B <

. If t is a point of

∞

˙Q(x(t)) = λ

y1(t).

−

Proof. First, we notice that

˙Q(x(t)) =

i ˙xi,2(t) =

y1 +

−

i≥1
X

B−1

i=0
X

hi(x(t))

where the second equality follows by applying Deﬁnition 1. Now, we treat the cases x0,2(t) > 0 and
x0,2(t) = 0 separately. Suppose that x0,2(t) > 0. Then, hi(x(t)) = λI
{i=0} (by (7)) and substituting in (62)
we immediately get ˙Q(x(t)) = λ
y1(t) as desired. Thus, suppose in the remainder that x0,2(t) = 0. Now,
assume that y0(t) > 0. Then, using again (7),
˙Q(x(t)) =

y1 + (βx0,1 + x1,2) I

βx0,1)+

(λ

−

−

=

y1 + (βx0,1 + x1,2) I

{x1,2+βx0,1≤λ} +

y1
y0
{x1,2+βx0,1≤λ} + (λ

x1,2 −

−
x1,2 −

βx0,1)+

−
λ. On the other hand, if x1,2(t) + βx0,1(t) > λ,
and the statement follows immediately if x1,2(t) + βx0,1(t)
then, since x0,2(t) = 0 and t is supposed to be a point of diﬀerentiability, we get (by (5c)) the contradiction
that 0 = ˙x0,2(t) = x1,2(t)
γx0,2(t) = x1,2(t) + βx0,1(t) > λ; the ﬁrst equality holds
because x0,2(t) is a non-negative absolutely continuous function. This shows that t cannot be a point of
= 1
diﬀerentiability. Finally, if y0(t) = 0, then the diﬀerentiability at t and the normalizing condition
give 0 = ˙y0(t) =
˙x0.1(t) = βx0,1 and thus x0,0(t) = 1. Assumption 2 requires that g(x) > 0 when
x0,0 = 1, so (5a) implies that ˙x0,0 < 0. This contradicts that t is a point of diﬀerentiability because x0,0(t)
is uniformly bounded by one and absolutely continuous.

h0(x(t)) + βx0,1(t)

˙x0,0(t)

x
k

−

−

−

≤

−

−

k

27

We now prove Theorem 3 by showing that

x(t)

0 in each of the following complete and mutually

exclusive cases. For each case, we show that x(t) follows a unique trajectory that stays in

x⋆

k

−

k →

S1.

Case i). Suppose that x0,2(t) = 0 for all t
all t large enough because (5a) and (5b), together with the normalizing condition
y1(t)
Thus, without loss of generality, let us assume that x0,0(0) > 0. Then, using (5), x(t) satisﬁes

0. This rules out the possibility that x0,0(t) stays on zero for
= 1, would imply that
, and in this case Lemma 9 yields the contradiction that Q(x(t)) is eventually negative.

1 as t

→ ∞

x
k

→

≥

k

αg(x)

−

˙x0,0 =
˙x0,1 = αg(x)
˙x0,2 = 0,

βx0,1
−
x1,2 + βx0,1 ≤

λ.

(63a)

(63b)
(63c)

˙x0,0(t) =

,
→ ∞

Note that limt→∞ x0,0(t) exists, say x0,0(
as t
x1,2(t) + βx0,1(t)
imply that x0,1(t)
condition

αg(x(t))

≤
→

→

= 1 implies that necessarily x0,0(t)

→

−
−
λ for all t, by (63c), we obtain that x1,2(t) + βx0,1(t)
0 and thus x1,2(t)

λ. In turn, (5d) gives xi,2(t)

), because ˙x0,0(t)
0. Given the assumptions on g, (λ

∞

≤

0 and x0,0(t) is uniformly bounded. Thus,
βx1,2(t))+
0 and since
0
2, and the normalizing

λ. Then, (63b) and g(x(t))

x0,1(t)

→

→

−

→
0 for all i
x⋆
0.

k →

→
−

≥

1

−

→

λ. Thus,

k

x(t)

x
k

k

Case ii). Suppose that x0,2(t) > 0 for all t. Then, x(t) satisﬁes the following conditions (using Deﬁnition 1)

αgI

{x0,0>0} −

γx0,2 I
βx0,1 + γx0,2 I
γx0,2

{x0,0>0} −

λ + βx0,1 −
x1,2 + λ

˙x0,0 = γx0,2 −
˙x0,1 = αgI
˙x0,2 = x1,2 −
˙x1,2 = x2,2 −
˙xi,2 = xi+1,2I

{i<B} −

xi,2,

i

2.

≥

{x0,0=0, γx0,2≤αg}

{x0,0=0, γx0,2≤αg}

(64a)

(64b)

(64c)

(64d)
(64e)

The ODE system (64d)-(64e) is an autonomous linear ODE system with constant coeﬃcients and, developing
the matrix-exponential general solution of such ODE system, for all i

1 we obtain

≥

xi,2(t) = λI

{i=1} + e−t

B

tk−i

(k

−

i)!

k=i
X

(xk,2(0)

−

λI

{k=1})

(65)

λI
and thus xi,2(t)
and therefore g(x(t))
obtain x0,2(t)

→

→
0. Since

{i=1} as t

. In turn, limt→∞(λ

→ ∞

0. Since g(x(t))

0, x0,1(t)
= 1, necessarily x0,0(t)

→

x1,2(t)

βx0,1(t))+ = limt→∞(

βx0,1(t))+ = 0
−
0 necessarily by (64b), and using this in (64c) we
→
1
→

λ and we have shown that

k →

x(t)

x⋆

0.

−

−

−

→

−
Case iii). If the conditions in cases i) and ii) are not met, then there exists t0, t1, with t0 ≤
δ > 0 such that

k

k

t1 <

, and

∞

x
k

1. x0,2(t) = 0 for all t

[t0, t1]

∈

2. x0,2(t) > 0 and ˙x0,2(t) < 0 for all t

3. x0,2(t) > 0 and ˙x0,2(t) > 0 for all t

δ, t0), and

[t0 −
(t1, t1 + δ].

∈

∈

On [t0 −
and thus by continuity

δ, t0), h0(x(t)) = λ (by (7)) and using (5c), we obtain ˙x0,2(t) = x1,2(t)

λ + βx0,1(t)

−

−

γx0,2(t) < 0

0

lim
t↑t0

≥

x1,2(t)

−

λ + βx0,1(t)

−

γx0,2(t) = x1,2(t0)

λ + βx0,1(t0).

−

(66)

Since x0,2(t0) = 0 on [t0, t1], (5c) implies that (66) holds as well on [t0, t1]. On (t1, t1 + δ], h0(x(t)) = λ
(by (7)) and using again (5c), we obtain

0 < ˙x0,2(t) = x1,2(t)

h0(x(t)) + βx0,1(t)

γx0,2(t) < x1,2(t)

λ + βx0,1(t)

−

−

−

(67)

28

and therefore g(x(t)) = 0. By continuity of ﬂuid solutions, x1,2(t1)+βx0,1(t1) = λ. In addition, on (t1, t1 +δ],
x(t) is uniquely deﬁned by

˙x0,0 = γx0,2
˙x0,1 =
βx0,1
−
˙x0,2 = x1,2 −
˙x1,2 = x2,2 −
˙xi,2 = xi+1,2I

λ + βx0,1 −
x1,2 + λ

γx0,2

{i<B} −

xi,2,

i

2,

≥

(68a)

(68b)
(68c)

(68d)

(68e)

) the ﬂuid
and we also know that ˙x0,2(t) > 0. As long as a) g(x(t)) = 0 and b) x0,2(t) > 0, on [t1,
solution under investigation x(t) is indeed uniquely given by the trajectory induced by (68) on [t1,
). In
the remainder, we show that both a) and b) hold true for all t. This will conclude the proof because x⋆
is the unique ﬁxed point of (68) and because (68) is a linear ODE system with constant coeﬃcients. For
simplicity of notation, let us shift time and assume that t1 = 0. Now, since x0,1(t) = x0,1(0)e−βt (by (68b))
and since x1,2(t) takes the form given in (65), substituting in (68c) we obtain

∞

∞

˙x0,2(t) = βx0,1(0)e−βt

= βx0,1(0)e−βt

−

−

γx0,2(t) + e−t(x1,2(0)

λ) + e−t

−

B

γx0,2(t)

−

βx0,1(0)e−t + e−t

xk,2(0)

B

tk−1

1)!

−

(k

k=2
X
tk−1

xk,2(0)

(k

−

1)!

k=2
X

βx0,1(0)(e−βt

e−t)

−

−

γx0,2(t).

≥

Thus, x0,2(t)
The solution of this diﬀerential equation is

≥

z(t) where z(t) is uniquely deﬁned by ˙z(t) = βx0,1(0)(e−βt

e−t)

−

−

γz(t) with z(0) = x0,2(0).

z(t) = βx0,1(0)e−γt

1

(cid:18)

−

e−t(β−γ)
γ
β

−

1

e−t(1−γ)
−
γ
1

−

(cid:19)

−

and now we notice that z(t) > 0 if β > 1, for all t. This proves property b). To prove property a), we use
again (65) and x1,2(t1) + βx0,1(t1) = λ to obtain

x1,2(t) + βx0,1(t)

λ = βx0,1(0)

e−βt

e−t

+ e−t

−

−

(cid:0)

(cid:1)

B

tk−1

(k

−

1)!

k=2
X

xk,2(0) > 0,

(69)

where the last inequality follows because β < 1. Given (17), (69) implies g(x(t)) = 0.

8 Proofs of Technical Lemmas

8.1 Proof of Lemma 4

We give a proof for the ﬁrst limit because the argument used for the others is identical.

Since tn
Y Nk
i

∈
(tn)

(t, t + ǫ] whenever n
i
j=0 xj,2(t)

large

φ(Nkt) + 1, . . . ,
∈ {N
Cǫ, for some constant C, i.e.,

φ(Nk(t + ǫ))
}

N

|

−

| ≤

, (31) implies that for all k suﬃciently

1Dn
(Pi

P

j=0 xj,2(t)+Cǫ,Pi

j=0 xj,2(t)−Cǫ] ≤

1Dn
(Y N

i−1(t

−
n ),Y N

i (t

−

n )] ≤

1Dn
(Pi

j=0 xj,2(t)−Cǫ,Pi

j=0 xj,2(t)+Cǫ]

(70)

Let Γ denote the LHS of the ﬁrst equation in Lemma 4. Applying Lemma 1, we obtain

Γ

≤

lim
ǫ↓0

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1

I
{Wn=1}1Dn
(Pi

j=0 xj,2(t)−Cǫ,Pi

j=0 xj,2(t)+Cǫ] = xi,2(t)

and using (70) in the other direction we obtain Γ = xi,2(t) as desired.

29

8.2 Proof of Lemma 6

, where P(
We recall that we have analyzed x along a ﬁxed ω
C
ω and treat quantities x(t) and X N (t) as random variables. Let

∈ C

) = 1. We now explicit the dependence on

(71)

(72)

Wn]. Then,

n

\

(73)

Z N

n := 1

A1
(Y N

n(1−X N
i−1(t

0,0(t
−
n ),Y N

−
n )−X
−
n )]

i (t

Nk
0,1 (t

−
n ))

I
{Pd

j=0 X N

j,2(t

−

n )>0}.

For all n, the random variable Z N

n is

n-measurable where

F

n :=

F

{

X N (tN,λ−
n

), A1

n, Wn

, and

}

E[Z N

n |F

n

\

A1

n] =

1

X N
i,2(t−
n )
0,0(t−
n )

X N

−

X N

0,1(t−
n )

−
n with A1

I
{Pd

j=0 X N

j,2(t

−
n )>0}

where the set
E[∆N

n

n |F

\

F

\
Wn] = 0 and

Wn denotes the set
n | ≤

∆N

|

n

E[Z N
n removed. Now, let ∆N
2, and applying the Azuma–Hoeﬀding inequality, we get

n := Z N

n −

F

n |F

P

N

1
N (cid:12)
(cid:12)
(cid:12)
(cid:12)
N δ2/8
(cid:12)

n=1
X

∆N
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∞

> δ

! ≤

2 exp

−

(cid:18)

(N δ)2
8N

(cid:19)

, an application of the Borel–Cantelli lemma shows that

for any δ > 0. Since
1
N

N
n=1 ∆N

<
0 almost surely. In particular,

N exp

−
(cid:0)

(cid:1)

P

n →

P

lim
N→∞

1
ǫN

Nφ(N (t+ǫ))

1

Xn=Nφ(N t)+1:
Wn=0

A1
(Y N

n(1−X N
i−1(t

0,0(t
−
n ),Y N

−
n )−X N
−
i (t
n )]

0,1(t

−
n ))

(cid:16)

I
{Pd

j=0 X N

j,2(t

−

n )>0} −

E[Z N

n |F

n

\

Wn]

= 0

(74)

(cid:17)

almost surely. We now come back to work on a given trajectory ω. In view of the previous equality, we may
′. Therefore, we ﬁx
redeﬁne
ω

in Lemma 1 to be a subset of
C
and use (31) and (72) to obtain that

′ = 1) and (74) holds for all ω

′ where P(
C

∈ C

C

∈ C

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1:
Wn=0

E[Z Nk

n |F

n

\

Wn]I

{Pd

j=0 X

Nk
j,2 (t

−
n )>0}

lim
k→∞

≤

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1:
Wn=0

xi,2(t) + δ

x0,0(t)

1

−

x0,1(t)

δ

−

−

I
{Pd

j=0 X

Nk
j,2 (t

−
n )>0}

for any δ > 0 suﬃciently small. Replacing δ by
reversed and letting δ

0, we obtain

δ in the last fraction term, the previous inequality can be

−

↓
Nφ(Nk(t+ǫ))

lim
k→∞

1
ǫNk

E[Z Nk

n |F

n

\

Xn=Nφ(Nkt)+1:
Wn=0

Wn]I

{Pd

j=0 X

Nk
j,2 (t

−
n )>0}

=

xi,2(t)

x0,0(t)

1

−

−

x0,1(t)

lim
k→∞

1
ǫNk

Nφ(Nk(t+ǫ))

Xn=Nφ(Nkt)+1:
Wn=0

I
{Pd

j=0 X

Nk
j,2 (t

−
n )>0}

(75)

Finally, (75) and (74) give (47).

8.3 Proof of Lemma 8

Let wd :=
(by (7))

P

d
j=0 xj,2. For now, let us assume that x0,2 > 0. In this case, wd > 0 and (57) boils down to

x1,2 = λ
wd

x0,2

30

(76a)

 
xi+1,2 = xi,2 + λ
wd
xd+2,2 = xd+1,2 −
xi+1,2 = xi,2,

(xi,2 −
xd,2

λ
wd

Since

x
k

k

= 1, (76) holds if and only if xi,2 = 0 for all i

xi,2 =

i

x0,2,

λ
wd

xi−1,2),

i = 1, . . . , d

d + 2.

i

≥
d + 2 and

≥

i = 0, . . . , d + 1.

If d = 0, then wd = x0,2 and x1,2 = λ, and the lemma is proven. Thus, let d
i = 0, . . . , d, we obtain

(cid:16)

(cid:17)

1

wd =

λ
wd

−
(cid:16)
1
−

(cid:17)
λ
wd

d+1

x0,2

(76b)

(76c)

(76d)

(77)

1. Summing (77) over

≥

(78)

d
j=1 xj,2 we obtain (9) as desired and it remains to prove (8a). Using (77), we notice that

and letting zd :=
(9) holds if and only if

P

zd + x0,2 =

xd+1,2
λ
zd+x0,2

x0,2 −
1
−
xd+1,2. Then, (8a) follows by using the normalizing

(79)

and rearranging terms we obtain zd + x0,2 −
d + 2.
condition

= 1 as xi = 0 for all i

λ = x0,2 −

x
k

k

≥

It remains to consider the case where x0,2 = 0. Here, x0,2 = 0 if and only if x0,1 = 0, by (56), which

implies

gI

{x0,0>0} = 0,

(80)

by (55b). In addition, if wd > 0, then (57) boils down again to (76) and x0,2 = 0 would imply that xi,2 = 0
for all i. This is not possible in view of
= 1 and, therefore, we must have wd = 0. Since necessarily
x0,0 < 1, (7) simpliﬁes to

x
k

k

and substituting in (58) we get

hi(x) =

(cid:26)

xd+1,2I
{i=d}
xi,2
(λ
1−x0,0

I
{xd+1,2≤λ}
xd+1,2)+

−

if i
d,
≤
if i > d

d

i

xi,2 = 0,
≤
xd+1,2 = xd+1,2I
xi−1,2
x0,0
1

xi,2 =

−

{xd+1,2≤λ}

xd+1,2)+,

(λ

−

d + 2.

i

≥

(81)

(82a)

(82b)

(82c)

This gives (8f). Now, if xd+1,2 > λ, then (82b) is violated, and if xd+1,2 = 0, then (82c) and
the contradiction that 1 = λ. So, necessarily xd+1,2 ∈
tied to a speciﬁc value. Then, summing (82c) we obtain

= 1 give
(0, λ], i.e., (8h). Here, we notice that xd+1,2 is not

x
k

k

which, using

x
k

k

xi,2 =

i≥d+2
X
= 1 and (82), holds if and only if

i≥d+2
X

xi−1,2
x0,0
1

−

(λ

−

xd+1,2),

1

xd+1,2 −

−

x0,0 =

xd+1,2
x0,0

λ
−
1
−

(1

−

x0,0)

(83)

(84)

i.e., if and only if x0,0 = 1
contradiction that 1 < λ. Since x0,0 > 0, necessarily g = 0 by (80), which gives (8i). Using xd+1,2 ≤
and x0,0 = 1
obtain (8g). This concludes the proof.

= 1 give the
λ
xd+1,2/λ) and applying inductively (82c), we

λ; note that x0,0 = 0 is not possible as otherwise (82c) and

λ in (82c), we obtain xd+2,2 = xd+1,2 (1

x
k

−

−

−

k

31

9 Proof of Proposition 1

The fact that x⋆ is a ﬁxed point is trivial. Suppose that there exists δ > 0 such that x0,2(t) > 0 on (0, δ].
Then, there exists δ′ > 0 such that ˙x0,2(t) > 0 on (0, δ′]. Using (5c), which gives ˙x0,2 = x1,2−
γx0,2,
we obtain

λ+βx0,1 −

x1,2(t) + βx0,1(t) > λ + γx0,2(t),

(0, δ′]

t
∀

∈

(85)

and thus x1,2(0) + βx0,1(0) = limt↓0 x1,2(t) + βx0,1(t)
λ, by continuity of the ﬂuid model. This contradicts
≥
the last condition in (14) and thus x0,2(t) = 0 on a right neighborhood of zero, say [0, δ]. Since g(x) =
λ

1 + x0,0, on [0, δ] we obtain (using (5))

−

1 + x0,0)

−
1 + x0,0)

βx0,1

−

−

α(λ

˙x0,0 =
˙x0,1 = α(λ
˙x0,2 = 0
˙xi,2 = xi+1,2 −

−

xi,2 + hi−1(x)

hi(x),

i

1

≥

−

(86a)
(86b)

(86c)
(86d)

where

hi(x) =

βx0,1 + x1,2
xi,2
(λ
y1

if i = 0,
βx0,1)+ if i > 0.

x1,2 −
We observe that (86a)-(86b) form an autonomous linear ODE system. By continuity of x(t), (14) holds
as well on a right neighborhood of zero. Now, we actually show that (14) holds on [0,
.
), i.e., δ = +
∞
Towards this purpose, let us analyze the system (86a)-(86b) in isolation. After some algebra, we obtain

∞

−

(cid:26)

(87)

x0,1(t) =

Thus,

x0,0(t) = 1

λ + (x0,0 −
1 + λ)
α

−
α(x0,0 −
β
−

1 + λ)e−αt

(e−αt

−

e−βt) + x0,1(0)e−βt.

(88a)

(88b)

i) x0,0(t) monotonically decreases to zero as t

, and

→ ∞

ii) y0(t) = y1(t) < 1 with both y0(t) and y1(t) monotonically increasing to λ because ˙x0,0 + ˙x0,1 is always

non-increasing and x0,2 stays on zero.

To prove that (14) holds on [0,
is true because x1,2 + βx0,1 ≤
on [0,
function of (86) is Lipschitz and therefore it induces a unique ﬂow [14, page 56]. Since x0,0(t)
t

), it remains to show that x1,2(t) + βx0,1(t) < λ for all t
(x0,0(0)

0. This property
1 + λ)e−αt < λ. Thus, x(t) satisﬁes (86)
0 for all t, the drift
λ as

). In addition, since x0,0(0) + x0,1(0) < 1 and ˙x0,0(t) + ˙x0,1(t) =

∞
y1 + x0,1 = 1

, for all t

x0,0 = λ

βx0,1(t)

∞

−

−

≤

−

≥

−

−

↓

0

1

→ ∞

≥

˙Q(x(t)) = λ

−

y1(t) = x0,0(t) + x0,1(t) + λ

1

−

≥

x0,0(t) + λ

1 > 0,

−

(89)

where the ﬁrst equality follows by Lemma 9. In particular, limt→∞ Q(x(t)) exists and must be greater than
λ because λ < Q(x(0)) <

. Combining (88) and (89), we obtain

∞

˙Q(x(t)) = (x0,0(0)

1 + λ)e−αt +

α(x0,0(0)

1 + λ)

(e−αt

−

e−βt) + x0,1(0)e−βt

−
α

1 + λ)

e−αt +

β

−
x0,1(0)

−
β(x0,0(0)

=

−
α

β

−
:=C1

α(x0,0(0)

−

β

:=C2

{z

−
α

−

1 + λ)

e−βt.

(cid:19)

}

(cid:18)

|

Integrating,

|

{z

}

Q(x(t)) = Q(x(0)) +

C1
α

(1

−

e−αt) +

C2
β

(1

−

e−βt)

32

Q(x(0)) +

−−−→t→∞

α + β
αβ

(x0,0(0)

−

1 + λ) +

1
β

x0,1(0)

which proves (16). Finally, suppose that limt→∞ x1,2(t) exists, say x1,2(
because y1(t)
that x1,2(t) is Lipschitz continuous,

λ and limt→∞ Q(x(t)) > λ excludes that x1,2(t)

→

→

). Then, necessarily x1,2(

) < λ
λ. Then, using (86d) when i = 1 and

∞

∞

0 = lim
t→∞

˙x1,2(t) = lim
t→∞

x2,2 + βx0,1 −

1

x2,2 −

x1,2
λ

(λ

−

= lim
t→∞

(cid:16)

x1,2)

=

(cid:17)

−

x1,2
x0,0 −
1
)
∞

(cid:18)

x0,1

−

(λ

−
x1,2(
λ

x1,2 −
)
∞

(cid:19)

−
x1,2(

βx0,1)

+ lim
t→∞

x2,2,

which shows that limt→∞ x2,2 must exists as well and be equal to x1,2(

x1,2(∞)
λ

. By induction,

(cid:17)

1

)
∞
−
(cid:16)
∈ Ssubopt.

limt→∞ xi,2 exists and is equal to xi,2(

)
∞

1

−

(cid:16)

i−1

x1,2(∞)
λ

(cid:17)

. Thus, x(

)
∞

33

