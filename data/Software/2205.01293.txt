2
2
0
2

y
a
M
3

]
E
S
.
s
c
[

1
v
3
9
2
1
0
.
5
0
2
2
:
v
i
X
r
a

A Survey of Deep Learning Models for Structural Code
Understanding

RUOTING WU, Sun Yat-sen University of China
YUXIN ZHANG, Sun Yat-sen University of China
QIBIAO PENG, Sun Yat-sen University of China
LIANG CHEN∗, Sun Yat-sen University of China
ZIBIN ZHENG, Sun Yat-sen University of China

In recent years, the rise of deep learning and automation requirements in the software industry has elevated Intelligent

Software Engineering to new heights. The number of approaches and applications in code understanding is growing,

with deep learning techniques being used in many of them to better capture the information in code data. In this

survey, we present a comprehensive overview of the structures formed from code data. We categorize the models

for understanding code in recent years into two groups: sequence-based and graph-based models, further make a
summary and comparison of them1. We also introduce metrics, datasets and the downstream tasks. Finally, we make
some suggestions for future research in structural code understanding field.

Additional Key Words and Phrases: Code Representation,Intelligent Software Engineering, Graph Neural Networks,

Deep Learning, Code Generation

ACM Reference Format:
Ruoting Wu, Yuxin Zhang, Qibiao Peng, Liang Chen, and Zibin Zheng. 2022. A Survey of Deep Learning Models for

Structural Code Understanding. 1, 1 (May 2022), 48 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

∗Contact Author
1We provide a paper collection about deep learning models and datasets
https://github.com/codingClaire/Structural-Code-Understanding

for

structural code understanding in

Authors’ addresses: Ruoting Wu, wurt8@mail2.sysu.edu.cn, Sun Yat-sen University of China; Yuxin Zhang, Sun Yat-sen University

of China, zhangyx355@mail2.sysu.edu.cn; Qibiao Peng, Sun Yat-sen University of China, pengqb3@mail2.sysu.edu.cn; Liang Chen,

chenliang6@mail.sysu.edu.cn, Sun Yat-sen University of China; Zibin Zheng, zhzibin@mail.sysu.edu.cn, Sun Yat-sen University of

China.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that

copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first

page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To

copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request

permissions from permissions@acm.org.

© 2022 Association for Computing Machinery.

Manuscript submitted to ACM

Manuscript submitted to ACM

1

 
 
 
 
 
 
2

1 INTRODUCTION

Chen et al.

In the last several decades, deep learning has made remarkable achievements in various areas and permeated

every aspect of human lives, especially in the domain of multi-media data processing such as image
recognition, speech recognition, and natural language processing. With the booming development of deep

learning techniques, as well as cooperatively increasing open-source code communities and automation

requirements in the software industry, deep learning techniques began to be applied to more specific tasks

in software engineering in recent years.

Conventionally, source codes are considered as plain text sequences that may be understood using various

existing approaches, such as deep learning approaches in neural language processing(NLP). However,

when applied directly to source code, NLP approaches have the drawback of ignoring the code’s structural

information. When the code is learned merely as a sequence of plain text, syntactic and semantic information

that is crucial to understanding the code, as well as the many relationships between program entities, may be

overlooked. Hence, a surge of works of understanding source code with structural information is proposed in

recent years, which lead by the research of deep learning on sequences and graphs, such as Transformer[19],

Graph Neural Networks[173]. These techniques and their variants are developed to cope with various tasks

in source code understanding including code representation and other downstream tasks. Although these

methods have achieved some improvements, structural code understanding is still facing many challenges,

which are identified and summarized as follows:

• Code Structural Modeling: Since conventional language models feed the sequence of source code
tokens as inputs, the structural information in the code is usually neglected. Therefore as result, a

number of challenges arise about how to use structural information in code successfully, such as how

to model structural information in code effectively and how to select effective structural information
for specific downstream tasks.

• Code Generic Representation Learning: Much of the current research focuses on learning code
representations for specific programming languages, making learning generic code representations

a challenge. It’s about how to learn language-independent code representations that get beyond

programming languages’ constraints.

• Code Task-specific Adaptation: The following adaptations remain a challenge: how to choose
and design specific architectures for downstream applications such as code generation and program

repair, how to process datasets for task specifications, and how to adapt models in few-shots learning,

transfer learning, and cross-linguistic scenarios.

In this survey, we present a comprehensive overview of structural learning techniques for code represen-

tation learning. In summary, our contributions can be listed as below:

• We introduce the structures in code data as well as the generating procedure, then give a summary of

the downstream tasks for structural code understanding.

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

3

• We propose a new taxonomy of deep learning models for structural code understanding based on the

structures, which are categorized into sequence-based models and graph-based models.

• We outline the challenges, the open problems, and future directions for structural code understanding.

The survey is organized as follows. In Section 2, we give some basic introduction of the structures in code

and how they are extracted from code data. In Section 3 and Section 4, we separately introduce the models
based on which structure they generally used, indeed, the sequence-based models and the graph-based

models. We first give an overview of how they change the structures and then categorized them by the

core models. In Section 5, we offer a discussion and comparison between the sequence-based models and

graph-based models. The downstream tasks after conducting code representation is introduced in Section 6.

We then summarize the related metrics and datasets in Section 7. We try to discuss some open research

questions in Section 8. Finally, we draw our conclusion in Section 9.

2 PRELIMINARY

2.1 Structures in Code

2.1.1 Overview.

First, we’ll go through the basic structures that programs can produce. Table 1 summarizes the most

commonly used notations. The following is a piece of code snippet in Python we used to illustrate the

structures in code data.

def add (a ,b):

x=0

if (a>b):

x=a-b;

else :

x=a+b;

return x

res = add (1 ,2)

print ( res )

The general techniques for generating structures from the source code snippet are shown in Fig. 1. The

Lexical Analyzer first converts the code into a token-based sequence. Each token in the sequence has two

attributes, type and value. Lexical Analysis of code is similar to the tokenization stage in natural language.
Inspired by Hindle et al. [64], we refer to these unprocessed code sequences as Natural Code Sequence (NCS)
in this article for the consistence of discussion, which may be called as token sequence or code sequence in

other articles.

The Syntax Analyzer, also known as a parser, takes the tokens and produces an Abstract Syntax Tree(AST)
based on the code snippet’s grammar. The Abstract Syntax Tree is then utilized in the Semantic Analyzer

to verify for semantic consistency, and the Intermediate Code Generator to construct the Intermediate
Manuscript submitted to ACM

4

Chen et al.

Table 1. Summary of notations.

Symbol

Description

Symbol

Description

Symbol

Description

𝑃

𝐴

𝑝𝑎𝑡ℎ

𝐷

Program

Abstract Syntax Tree of a code snippet
or a program

Sequence of nodes extracted
from the AST

Code description in natural language
from the AST

𝑆

𝐺𝑐

𝑛

𝑡

Code Snippet

Control flow graph of a code snippet
or a program

The length (token number) of a code snippet
or a program

single token

𝐹

𝐺𝑑

𝑦

𝐻

function

Data flow graph of a code snippet
or a program

Labels of the code

the intermediate representation
of the code

Representation, which varies depending on the programming language. Control flow and Data flow are both
graph-like Intermediate Representations known as Control Flow Graph(CFG) and Data Flow Graph(DFG),
respectively.

Other structures established in software engineering, such as UML Graph and Program Dependency

Graph, are in addition to the basic structures.

Fig. 1. The basic structures generated from code includes (a) Nature Code Sequence, (b) Abstract Syntax Tree, (c)
Control Flow Graph and (d) Data Flow Graph. The front end of a compiler constitutes the four components (Lexical
Analyzer, Semantic Analyzer, Syntax Analyzer, and Intermediate Code Generator)

2.1.2 Nature Code Sequence.

Given a Program 𝑃 or a Code snippet 𝑆, the Nature Code Sequence(NCS) 𝑆 = {𝑡1, 𝑡2, ..., 𝑡𝑛 } is obtained by
Lexical Analyzer, 𝑡𝑖 refers to the token in the code. Using NCS to represent the code is the simplest and
most common approach. The position of the token in the sequence corresponds to the order in which it

appears in the code snippet.

2.1.3 Abstract Syntax Tree.
Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

5

Given the language grammar, the Abstract Syntax Tree (AST) of code is generated by Syntax Analyzer
and marked as 𝐴, which hierarchically reflects the structural and syntax information of code. The root node
of the syntax tree represents the start symbol. The interior nodes are the nonterminals in the grammar, while

the leaf nodes are the terminals, which are usually the variables and identifiers defined by the programmers
from the NCS. Fig. 2 is the AST of the above code snippet in Python.

Fig. 2. An example of Abstract Syntax Tree

Flow Graph. Flow Graphs are the graphs that cover the semantic information of source code. The
2.1.4
two typical flows in flow graphs are control flow and data flow, which separately represent how the program

executes and how the data flows. Fig.3 shows the example of control flow and data flow graphs for the

python code snippet mentioned.

Fig. 3. An example of Flow Graphs. (a) Control Flow Graph, (b) Data Flow Graph of expression 𝑥 = 𝑎 − 𝑏, (c)Data Flow
Graph of expression 𝑥1 = 𝑎1 + 𝑏1.

Manuscript submitted to ACM

6

Chen et al.

Control Flow Graph. The Control Flow Graph (CFG) of a program marked as 𝐺𝑐 , represents different
execution paths of a program. Each node of CFG is a basic block that represents a behavior without branches.

The edges of the graph represent how these basic blocks flow from one to another. In the example code
snippet, the judgement of 𝑎 > 𝑏 will cause the program into 2 branches, one is 𝑥 = 𝑎 − 𝑏, the other is
𝑥 = 𝑎 +𝑏. As the Fig.3 (a) , the CFG has two edges from the decision node(if-statement) to the two downward
nodes(two basic blocks).

Data Flow Graph. The Data Flow Graph (DFG) of a program marked as 𝐺𝑑 represents the dependency
relation between variables. DFG can represent code snippets without conditionals. In the example code
snippet, we select two statements: 𝑥 = 𝑎 − 𝑏 and 𝑥 = 𝑎 + 𝑏 to draw their DFGs. To eliminate the repeated
assignment to 𝑥, we rename variables in the second assignment, convert them to 𝑥1 = 𝑎1 + 𝑏1. Therefore,
the two DFGs are shown in Fig.3 (b) and (c). Each data flow node represents the operation of variables and

each edge represents how the value flows.

Control/Data Flow Graph. Because the DFG can only represent basic blocks without branches, it can
be used to replace the basic blocks of a CFG, resulting in a Control/Data Flow Graph (CDFG). There are two

types of nodes in a program’s CDFG, the decision node and the data-flow node.

2.1.5 Other Structures.

In addition to the above structures, there are some other code structures that are less common in code

understanding.

Other Intermediate Representation. Intermediate Representation(IR) is a data structure that can be
obtained from a compiler such as LLVM Compiler Infrastructure[85]. The frontend Compiler compiles the

source code and generates an IR for the backend Compiler to optimize and translate. In LLVM infrastructure,

the IR is in Static Single Assignment(SSA) form. The Broad definition of IR includes the flow graphs as well

as other graph structures. Program Dependency Graph(PDG)[53] is one of the intermediate representations

that make explicit both data and control dependencies for each operation in a program. PDG are useful to

perform optimizations through a single walk.

The Unified Modeling Language. The Unified Modeling Language (UML) is a widely-used language
for specifying, visualizing, and documenting the artifacts of a software-intensive system. UML class diagram

is a type of static structure diagram that describes the structure of a system by showing the system classes

and their attributes, operations (or methods), and relationships.

2.2 Deep learning models

In this section, we focus on deep learning models commonly used in code understanding tasks that have
shown to be effective in other domains and are now being used by a growing number of researchers in the

code domain.
Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

7

2.2.1 Recurrent Neural Network.

Recurrent Neural Networks (RNNs)[134] are the neural networks where each unit is connected by

directed cycles. RNNs can use their hidden state to track the long-term information of the sequence data.

Therefore, RNNs are a common choice for sequence modeling. Vanilla RNNs have gradient vanishing and
gradient explosion problems, which can reduce the ability of a model to learn long-term information. Long

Short-Term Memory(LSTM) and Gated Recurrent Units(GRU) are the two most used RNN models that can

avoid the problem and achieve better results.

Long Short-Term Memory(LSTM)[65] has basic model from RNNs. Every unit of LSTM considers the

hidden state, the current input, and the information from its memory cell. LSTM uses 3 gates, input gate,

forget gate and output gate to control the learning and transfer of information.

Gated Recurrent Units(GRU) [38] combine the input gate and forget gate in LSTM into one gate, called

the update gate, and the other gate is the reset gate. The reset gate and the update gate can control the

degree of memory or forgetting of sequence information by the hidden state. Compared with LSTM, GRU

has comparable performance but lower computational cost.

2.2.2 Transformer.

Bahdanau et al.[19] propose the Attention mechanism to solve the problems of excessive information

length and information loss in machine translation tasks. It feeds all of the hidden states from the Encoder

into the Decoder after linear weighting and assigns various attention weights to each input token, indicating
which inputs are more significant to the output.

To enhance computational performance and better describe global relationships in sequences, Vaswani et

al.[153] propose self-attention. It is a special attention mechanism, so that information from any position

in the sequence can directly affect the encoding of the other token. Based on self-attention they propose

a new neural network model called Transformer which consists of multiple attention blocks made up

by self-attention. Transfomer’s encoder uses the self-attention mechanism to associate the tokens in the

input sequence with all other tokens as it learns the representation of the input. Also, the input of the

Transfomer’s decoder is associated with the output of the encoder through self-attention. Transformer and

its variants, such as Bert, GPT, etc., are capable of processing complex data and can process large amounts

of sequence data. Therefore they are often used as pretraining models to capture the rich information from

large amounts of complex data.

2.2.3 Graph Neural Network.

Graph Neural Networks(GNNs) are deep learning models using message passing between nodes to

capture structural information and aggregate semantic information in graphs. GNN can be categorized
into four groups according to the survey by Wu et al.[173]: Recursive GNNs, Convolutional GNNs, Graph

autoencoders, and spatial-temporal GNNs. GNNs are widely used in node classification, edge prediction,

and graph classification tasks.

Manuscript submitted to ACM

8

Chen et al.

The following are the typical models used in code representation-related tasks. Gated graph Neural

Network(GGNN) [100] uses gated recurrent units and unroll the propagation process for a fixed number

of timesteps. The representations of nodes are the final step output. Graph Convolution Network(GCN)

[84] is one of the convolution GNNs, which stack multiple graph convolutional layers to better extract
the information from neighbors. Graph Attention Network(GAT)[154] uses the attention mechanism in

the message-passing step to aggregate neighborhoods’ information with different weights and update the

encoding of the node. The above Graph Neural Networks are typical deep learning models for learning the

representation of graphs or nodes in code data.

2.2.4 Encoder-Decoder Framework.

The Encoder-Decoder framework[38] is presented as a solution to traditional machine translation diffi-

culties. The input language is encoded in the encoder section to obtain the intermediate representation

Context. And then in the decoder part, corresponding outputs are generated one by one based on Context

and related inputs. Sutskever et al.[144] present the seq2seq model based on Encoder-Decoder framework

to overcome the problem of indefinitely long input-output sequences, which aids sequence output with

special markers such as <Eos>. Different encoders and decoders can be selected according to particular

tasks, such as RNN-based models, Transformer-based models, GNN-based models, etc. Encoder-decoder

model architecture has become the mainstream approach to address the code generation issue and other

tasks due to the naturalness and sequence of code. For example, Rabinovich et al.[129] introduce a syntax

network (ASN) that extends the encoder-decoder framework to generate AST.

3 SEQUENCE-BASED MODELS

Sequential models that perform well in sequence-related tasks, such as the Recurrent Neural Network

family[38, 65, 134] and Transformer[153], can be effectively applied to code-related tasks. They can be

used to encoder-decoder architecture to fulfill downstream tasks such as code summarization and code

generation, as well as learn to access code representation for downstream activities.

The frequently used term code sequence refers to natural code sequences created by the code itself, which
is referring to NCS introduced by section 2.1.2. Because the code is highly structured, there are sequences

formed by pre-processing the code structure input such as AST introduced in section 2.1.3, which refer to
the flattened sequence. In this section, we introduce the models for processing serialized code data mentioned
above. We first show the structural transformation of code including the different methods of pre-processing

the structures of code to get the flattened sequence, and then show the models for processing NCS and

flattened sequences respectively.

3.1 Structure Transformation

The AST format is commonly used to express source code structural information. For the sequence model

to make efficient use of the code’s structural information, some strategies for flattening the AST are offered.

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

9

Flattening procedures are divided into four types, as shown in Fig 4, which shows the varied sequences

obtained by different types of flattening approaches.

Type-1: Depth-first traversal. Because the AST represents the code’s structural information as a tree,
traversing the tree structure depth-first, so that the nodes on each subtree are adjacent in the sequence, is

the simplest way to extract the flattened sequence. More models advocate preorder depth-first traversal

because the root node (operator) of each subtree in the AST is frequently the subtree’s center. Type-1 refers

to the Structure Transformation method that uses a depth-first traversal on the AST.

Type-2: AST path. The approach of serializing the AST via paths is recommended because every two
nodes in the AST have a path between them. Methods for extracting paths from ASTs include paths between

arbitrary nodes, paths between terminal nodes, paths from terminal nodes to root nodes, and so on. In code

generation task, paths from terminal nodes to new nodes are also used. Type-2 refers to the approach of

Structure Transformation that uses paths between nodes on the AST.

Type-3: Structure information addition. To better keep structural information in the AST and make
the flattened sequence unique, Type-1/2-based structure information adding methods are proposed. The

Structure-based traversal approach, for example, uses brackets to represent the AST structure, and the

brackets in the created sequence may be used to detect the subtree of a certain node, allowing the generated

sequence to be translated back to the AST. The use of "<" and ">" to enclose the non-terminal node’s subtree,
which corresponds to the code block, is another technique to preserve structural information. As a result, we

refer to the Structure Transformation method as Type-3 because it incorporates code structure information

in the obtained sequence.

Type-4: AST partial retention . AST contains a lot of structural and syntactic information about the
code, but it also contains a lot of useless data. As a result, numerous ways suggest filtering the ASTs,

preserving the nodes of interest, and then flattening the sequence derived from the filtered ASTs. To

generate flattened sequences in code defect detection jobs, only three types of AST nodes are generally

kept, for example, method call and class instance creation nodes, declaration nodes, and control flow nodes.

Type-4 refers to the Structure Transformation approach, which keeps only important nodes.

3.2 Natural code sequence

NCS is a natural way to represent whole code fragments because code is made up of individual tokens. The

approaches and models for comprehending code using NCS are described in this section.

N-gram Models. N-gram models, widely used as early language processing approaches, assume a Markov
property that the probability of the current word is only affected by its prefixed N-1 words, which can
capture the statistical characteristics of a sequence to some extent. In order to take advantage of the statistical

properties of NCS, some early models using N-Gram models to accomplish code representation tasks were
Manuscript submitted to ACM

10

Chen et al.

Fig. 4. Examples of local AST using four types of structure transformation to obtain flattened sequence, (a) using depth
first traversal to convert AST into sequence, (b) showing three paths in the leaf to leaf path set, (c) showing SBT[67]
using parentheses to retain structure information, (d) only retaining the type information of nodes in AST, and the
obtained sequence is the same as (a).

proposed. Hindle et al. [64] firstly adopt N-gram models on NCS and find that the language models are

conducive to extracting local statistics by exploring the naturalness rather than the syntax or semantics. Tu

et al. [150] cooperate N-gram model with a cache component to further capture the localness of source

codes. Karaivanov et al. [80] exploit the N-gram model for phrase-based programming language translation.

The N-gram model takes all of the information from the N-1 tokens before the token to learn its represen-

tation, but it cannot use remote token information (such as the reuse of a variable in the code), and it also

cannot build identical vector representations for tokens with similar meanings. As a result, the N-Gram

model was gradually replaced by the more learning model introduced later in the code representation task.

Convolutional Neural Network. Convolutional neural network (CNN) is first proposed for processing
images to learn image representations by capturing features of local images through convolutional kernels.

Also, It can extract significant n-gram features from input sentences to create a semantic representation of the

potential of sentence information for downstream tasks, while effectively capturing rich structural patterns

in the code, so some work on learning code representations with CNN was proposed. To summarize the code

snippet, Allamanis et al. [7] employ an attentional neural network that uses convolution on the input tokens

to detect local time-invariant and long-range topical attention features in a context-dependent way. For
Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

11

better code searching, CARLCS-CNN[142] first embeds code and query respectively using CNN since CNN

can capture the informative keywords in query and code, then learns interdependent representations for the

embedded code and query by a co-attention mechanism. CNN is unable to model long-range dependencies

in code sequences, there are local limitations on the sensitivity to word order, and since code sequences are
often large, CNN models are less often used in practical applications to learn to understand code.

Recurrent Neural Network. As previously section 2.2.1 stated, RNN and their variants perform excep-
tionally well on sequential tasks, resulting in a huge number of RNN-based models for code-related tasks to

handle NCS. Veselin et al. [132] have shown that the well-trained RNN can outperform N-gram models [64]

when processing NCS. Dam et al. [42] propose to use LSTM to predict the next token in order to address

the inability of n-gram models to capture token dependencies in sequences. CodeNN [72] uses LSTM with

attention to produce sentences that describe code snippets. Liu et al.[106] employ latent attention over

outputs of a Bi-LSTM to better translate natural language descriptions into If-Then program. Bhoopchand

et al.[27] enhance LSTM with a pointer network specialized in referring to predefined classes of identifiers

to well capture long-range dependencies in the code, thus giving better suggestions for the next token input.

CODEnn [57] provides a deep architecture composed of a code embedding network, description embedding

network, and a similarity module to align the embeddings of code-description pairs. The code embedding

network and description embedding network are both use LSTM. Tal et al. [23] utilize RNNs to learn a

language-agnostic intermediate representation that is generated from code syntactical structures. Vasic et

al.[152] present a solution to the general variable-misuse problem in which enumerative search is replaced

by a neural network containing LSTM that jointly localizes and repairs faults. CodeGRU [71] further applies

GRU in code sequence processing to capture contextual dependencies. Compared with CNN, RNN family

can handle arbitrary length input and has more flexible code sequence modeling ability, but vanilla RNN

has the problem of gradient disappearance and gradient explosion. LSTM and GRU, as variants of vanilla

RNN, can learn long-term dependency and solve the problem of gradient explosion and disappearance to a

certain extent, and gradually become the current RNN family Core.

Transformer. Transformer can successfully handle the distant dependence problem, overcome the
limitation that RNNs cannot be computed in parallel, and employ self-attention to generate more explanatory

models, as described in section 2.2.2. Transformer is being used for a growing number of sequence-related
work, and NCS is no exception. Ahmad et al. [2] first employ the Transformer for code summarization

to handle the ubiquitous long-range dependencies in source code from natural code sequence. TFix[26]

works directly on program text and phrases the problem of code fixing as a text-to-text task, so it can

leverage a powerful Transformer based model pre-trained on natural language and fine-tuned to generate

code fixes. CodeBERT [51],CuBert[79], GPT-C[145] and CodeT5[167] use both NCS and related natural

language to pre-train the Transformer architectures for downstream tasks, such as code search, code clone

detection and code summarization. OSCAR[126] and GraphCodeBERT[58] are also pre-trained models

based on transformer using NCS while exploiting the semantic information of flow graph. OSCAR adds
Manuscript submitted to ACM

12

Chen et al.

GCF information to the model training using positional encoding. GraphCodeBERT takes DFG as part of

the input while exploiting the node and edge relationships for Graph-Guided Masked Attention to better

understand the code.

Transformer can effectively learn a big quantity of data, but its memory and processing requirements
are enormous when compared to models like RNN. Following that, work should be measured in terms of

resource consumption and performance improvement, and a suitable model should be chosen by taking

into account the current circumstances.

Others. Other models have been incorporated in related work in addition to CNN, RNN, and other
models that are extensively utilized in NCS related tasks. Sachdev et al.[136] create a continuous vector

embedding of each code fragment at method–level granularity, map the given natural language query to

the same vector space, and use vector distance to simulate relevance of code fragments to a given query.

CCLearner[96] extracts token sequence from known method-level code clones and non-clones to train a

deep Neural Network(classifier) and then uses the classifier to detect clones in a given codebase. SCC[15] is a

classifier that can identify the programming language of code snippets written in 21 different programming

languages, it employs a Multinomial Naive Bayes(MNB) classifier trained using Stack Overflow posts.

Sachdev et al.[136] propose a simple yet effective unsupervised model that combines word2vec[117] and

information retrieval methods for code search. UNIF [31] first uses word2vec to embeds code/quey and then

combines code embedding with attention. These models show better results in specific tasks, and therefore,

subsequent work dealing with NCS should not be limited to the adoption of mainstream models.

3.3 Flattened Sequence

The four structural transformations indicated in 3.1 can be used to obtain flattened sequences. Although the

flattening procedure consumes more resources, the flattened sequences preserve some structural information

about the code, and the applicable models can learn more about the code from the flattened sequences than

NCS. The sequences obtained by different structural transformations may be suitable for different models.

The models for processing the flattened sequence are described in the following and these models also

show good performance in the natural language processing field. We make a finer segmentation of the

RNN family in this section, including Vanilla LSTM, Bi-direction LSTM, and GRU, because of the enormous
variety of applications of RNNs and their variants on flattened sequences.

Word2vec. word2vec is a method of converting an input token into a vector representation, where the
converted vector contains, to some extent, the contextual information of the token. Word2vec contains two

training models, CBOW (Continuous Bag-of-Words Model) and Skip-gram (Continuous Skip-gram Model),

which have strong generality and are therefore used in early work on flattened sequence for learning code
representation. API2Vec[122] traverses the AST using Type-4 to build an annotation sequence according
to the syntactic units related to APIs. These sequences are then used to train CBOW to generate API

embeddings that may be utilized to migrate equivalent API usage from Java to C#. Alon et al.[12] first use
Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

13

Type-3 to obtain flattened sequence by adding up and down momentum information to the paths between
nodes and then use word2vec to complete the prediction of method names. Because Word2vec is unable to

learn the representation of polysemous words and cannot successfully capture long-range dependencies,

further work is being done to combine Word2vec with other models to learn flattened sequences better.

Deep Belief Network. Deep Belief Network(DBN)[25] is a generative model which uses a multi-level
neural network to learn a representation from training data that could reconstruct the semantic and content

of input data with Maximum probability. DBN can be used to identify features, classify data, generate data,
and do other tasks. Wang et al.[160, 161] use Type-4 and then use DBN to complete the defect prediction.
They produce flattened sequences from only three sorts of AST nodes: method invocation and class instance

creation nodes, declaration nodes, and control flow nodes. Because the names of methods, classes, and types

are usually project-specific, there are very few methods with the same name across multiple projects. To get

better detection results, they extract all three classes of AST nodes for cross-project defect prediction(CPDP),

but instead of utilizing their names, it uses their AST node type, such as method declaration and method

invocation, for declaration nodes and control flow nodes. DBN can automatically learn semantic features

from token vectors extracted from ASTs and is one of the first non-convolutional models to be successfully

trained by applying deep architectures. However, DBN has mostly lost favor and is rarely used compared to

other unsupervised or generative learning algorithms nowadays.

Vanilla Long Short-Term Memory. Vanilla LSTM is the LSTM introduced in section 2.2.1, which is
capable of learning long-range dependencies in some extent and is heavily used in work dealing with
flattened sequences. Liu et al.[107] use Type-1 and explore several variants of simple LSTM architecture
for different variants of the code completion problem. Li et al.[93] utilize Type-3, which allows storing two
extra bits of information about whether the AST has children and/or right siblings in the type node. And the

pointer mixture network proposed consists of two main components: a global RNN component(LSTM) and

a local pointer component, which utilizes the pointer network to point to the previous position in the local
context according to the learned position weights to solve the OoV problem. DeepCom [67] uses Type-3 and
design a new structure-based traversal(SBT) method to better preserve structural information in the code.

The SBT traversal method uses parentheses to indicate the structure of the AST, and the parenthesis in the

created sequence may be utilized to determine the subtree of a given node, allowing the generated sequence
to be transformed back to AST. DeepCom employs the seq2seq model which uses LSTM as encoder and
decoder to generate code fragment summaries. code2vec [14] employs Type-2 to represent code fragments
with the set of paths between all terminal node pairs in the code to complete the prediction of method

names of the code and learns the representation of the sequence of internal non-terminal nodes using

LSTM. For better program classification, Compton et al.[39] investigate the effect of obfuscating variable

names during the training of a code2vec model to force it to rely on the structure of the code rather than

specific names and consider a simple approach to creating class-level embeddings by aggregating sets of
method embeddings. Seml[102] uses Type-4 and sends the sequence obtained after filtering the AST to the
Manuscript submitted to ACM

14

Chen et al.

LSTM to complete the defect detection of the code. Type-3 is used by SA-LSTM[110], which surrounds the
sub-tree of each non-leaf node, which corresponds to code blocks, with < and >. SA-LSTM enhances the

LSTM network with a stack to store and recover contextual information based on the code’s structure for

modeling the hierarchical structure of code.

Different works will modify the LSTM to suit their own tasks, but the LSTM has a disadvantage in parallel

processing and cannot fully solve the gradient problem, as well as cannot do anything for a very large order

of magnitude sequences, and Bi-LSTM and GRU introduced later also face the same problem. The latest

effort will use LSTM or other RNN variants as a component of the model and mix it with other models

to accomplish the task, in order to better exploit the advantages of RNN and its variants on sequence

processing.

Bi-direction Long Short-Term Memory. Compared with the traditional LSTM which only retains the
previous information, the bidirectional Long short Memory (Bi-LSTM) can also use the later information,

which can better capture the semantic dependencies in both directions. code2seq [8] employs the same
Type-2 as code2vec to obtain the flattened sequence, and Bi-LSTM was used to learn the representation of
the internal non-terminal nodes sequence. DeepCPDP[34] uses Type-4, using simplified Abstract Syntax
Tree(SimAST) to represent the source code of each extracted program. DeepCPDP uses SimASTToken2Vec,

an unsupervised-based embedding approach, and will classify the code inputted as defective or non-defective
using Bi-LSTM with attention mechanism and Logistic regression. Pythia [146] uses Type-1 to predict
the method names and API calls that the developer wants to use in programming. Pythia aggregates the

initial(obtained by word2vec) and intermediate(learned by Bi-LSTM) vector representations through a fully

connected layer to obtain the final vector for prediction.

Gated Recurrent Units. The GRU introduced in section 2.2.1 is a simplified version of LSTM with a
reduced number of gates and therefore easier to converge with relatively few parameters, so some work
will choose to use GRU to learn the representation of the flattened sequence. ast-attendgru[89] uses Type-3
and proposes SBT-AO which modifies the SBT AST Flastting procedure to simulate the case when only

an AST can be extracted. SBT-AO replaces all words (except official Java API class names) in the code to a

special <OTHER> token, remaining all the code structure in SBT. It uses two GRU with attention mechanism

to process NCS and SBT-AO respectively for getting context vector and then predicts the summary one

word at a time from the context vector, following what is typical in seq2seq models. Hybrid-Deepcom[68]
uses the same Type-4 as Deepcom to obtain the flattened sequence. It also employs the seq2seq model for
code summarization and utilizes two GRU as encoders for NCS and flattened sequence, respectively, to

acquire both lexical and structural information of code fragments. GRU has one less gate and relatively
fewer parameters than LSTM, so it is easier to converge and less computationally expensive, while having

similar results in most tasks, but LSTM performs better with larger data sets, so the choice of LSTM and

GRU in code sequence tasks needs to consider both data set size, training effect and training time.

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

15

Transformer. As previously described, Transformer is a more complex and powerful model compared to
RNN, and more and more work has been done in recent years using transform to learn flattened sequences
to accomplish related tasks. SLM[10] uses Type-2 to represent the code as a path from the root node and
all leaf nodes to the target node. SLM uses LSTM to obtain the representation of all paths separately and
then uses Transformer contextualize the path representation of all leaf nodes to the target node, and at

the same time adds the position index in the parent node to the path representation from the root node

to the target node, and passes the attention mechanism. The final vector is obtained by combining the

path representation to make predictions on the target node. Kim et al.[82] propose three methods based
on Transformer to better predict the token that the developer is about to input: 1. pathTrans, uses Type-2
to serialize the AST using the path from the leaf node to the root node; 2. TravTrans, uses Type-1, and
uses the method of preorder first traversal to obtain sequences from the AST; 3. TravTrans+, uses Type-3,
adding a matrix that saves the unique path between two nodes in TravTrans to enhance the Transformer’s

self Note the block. And the experiment proves that TravTrans+ works better. Liu et al.[109] uses both
Type-1 and Type-2 to complete the prediction of the code. It uses Transformer-XL to encode the sequence
obtained by preorder traversal and uses Bi-LSTM to encode the path from the target node to the root node,
preserving the hierarchical information of the target node. TreeBERT[179] employs Type-2 to represent
the entire code fragment using multiple paths from the root node to the leaf nodes in the AST and then

completes the pre-training using the modified Transformer architecture. The position embedding of each

node is generated from its hierarchical information in the AST and the position information of its parent

node to make better use of the structural information of the code. Since Transformer requires a lot of data

and a lot of computational resources, subsequent work should consider more than just model performance

when using Transformer.

4 GRAPH-BASED MODELS

While the sequential model with serialized structure as input is simple and visible in many works, the

linear order of code snippets is inevitably losing hierarchical syntactic information. Therefore, recent

works pay more attention to capturing the syntactic information of the code. Graph-based models for code

understanding are described and categorized in this section, based on the structures used in the methods

without serialization.

The structures used in graph-based models are usually AST and Flow Graph, including CFG and DFG,

introduced in section 2.1.3 and 2.1.4. We also introduce models that use other structures in section 2.1.5.

Unlike the sequence-based models, these structures of a program are considered as a tree structure or graph

structure. In this section, we first introduce the transformation conducted in these structures and then

categorized different models in AST, flow graphs, the combination of them, and other rarely seen structures

generated from source code.

Manuscript submitted to ACM

16

Chen et al.

4.1 Structure Transformation

Different from the transformation of sequences, the modifications of graph structures tend to add nodes or

edges based on one typical graph structure. We summarize the structure’s transformation of the graph into
three categories, the first two transformations are proposed on the basis of preserving a graph structure for

further learning the representation of the graph, while the last transformation is extracting the information

of graph and constructing a new mechanism or DSL(Domain-specific language) without keeping the

program graph.

Type-1: Adding Edges. In AST-based structure, methods treat the structure from two different perspec-
tives for better using the structural information. One takes the structure as a "tree", which means that

the structure has directed edges with the hierarchy preserved. The other extends the AST structure by

adding various types of edges and eventually makes the edge bidirectional, which makes the original AST

structure a "graph". In two perspectives, adding edges are the most common transformation to preserve

more information of structures. Allamanis et al. [5] use AST as the backbone and add additional edges for

capturing data-flow information. The edges contain types derived from AST(e.g. Child and NextToken)

and from semantic(e.g. LastUse, LastWrite). Wang et al. [165] extend the AST with parent-child edges.

Dinella[45] add SuccToken edges between the leaf nodes.

Type-2: Combination. Some methods use AST-based and Flow-graph-based structures in combination,
such as the structure CDFG, which is a combination of CFG and DFG. Other structures for graph-based

models have been proposed, as well as some new structures based on the three basic structures. The

combination will be discussed and introduced in Section 4.4.

Type-3: Information Extracting. Some methods choose not to preserve the graph structure of AST,
Flow graphs, or others. Instead, they propose new structures, mechanisms, or DSL based on the syntactic

and semantic information extracted from these graph structures. It is not the priority of our introductions

in graph-based models, but still an important kind of structures transformation from graph structures. For

example, Raychev et al. [131] propose a method to build probabilistic models of code and generate DSL
called TGEN, for traversing AST and accumulating a conditioning context. Cvitkovic et al. [40] propose

a Graph-Structured Cache for the out-of-vocabulary problem. An edit DSL called Tocopo[149] is created

for code editing in bug fixing problems, which contains token expressions, copy expressions, and pointer

expressions. Tocopo is a sequence of edit operations that represents the modification of an AST.

4.2 AST-based Structures

As previously mentioned, the following models are considerably varied as a result of the differences between

the perspectives of the AST structure.

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

17

4.2.1 Tree perspective.

Due to the large and deep structure of the tree structure, certain methods tend to use recursion of the

tree structure to reduce the complexity of programs, especially passing the information from sub-trees
to the full tree. Shi et al.[138] propose a method to split and reconstruct the whole AST of a program

into subtrees hierarchically to get the representation of the code snippets. ASTNN [193] uses a preorder

traversal algorithm to split an AST into a set of statement subtrees, recursively encodes them to vectors,

and eventually learns the representation of source code through the captured naturalness by BiGRU and

RvNN. The method can reduce the difficulty in training.

Most methods tend to modify the Recursive Neural Networks or Seq2seq model based on the structure

of AST, such as AST-based LSTM [157] and tree-based Seq2seq model [33]. The Tree-LSTM is one of the

most often modified methods in models of code understanding. Tree-LSTM [147] is a generalization of

the standard LSTM for tree structures that composes the state from an input vector and hidden states of

arbitrarily multiple children units. However, when the tree has ordered nodes, such as AST, the original

Tree-LSTM is unable to manage the circumstance. To address the problem, a Multi-way Tree-LSTM model is

developed, which extends the Tree-LSTM model. The model captures the interaction information between

nodes by adding bidirectional LSTM to each gate before linear transformation to encapsulate the information

of ordered children. In the code summarization task, the Multi-way Tree-LSTM learns the information in

ASTs more effectively than a sequence model.

Convolutional Neural Network is able to train easier and requires less time with the parallel computing

mechanism compared to Recursive Neural Networks. Meanwhile, the adaptations to the vanilla CNN, such

as tree-based CNN, have demonstrated their efficacy on code comprehensive tasks. Mou et al. [118] propose

a Tree-Based Convolutional Neural Network (TBCNN) for the program classification task. The convolution

layer can capture the information from AST. Chen et al. [35] propose a tree-based LSTM over API-enhanced

AST for clone detection. The original AST is modified by adding a new node type to identify the API name.

The model called TBCAA learns the representation of code using tree-based CNN, where each convolution

kernel has a triangle shape.

4.2.2 Graph perspective.

To encode the AST structure, most methods consider the structure as a graph and use a graph neural

network. Two types of GNN are the most used model in the following graph-based models of AST, which

are the Gated Graph Neural Networks(GGNN) and the Convolutional Graph Neural Networks. We also

introduce other graph neural networks such as Graph Attention Networks that are used particularly for the

AST structures.

Gated Graph Neural Network. Gated Graph Neural Network, which is a kind of Recurrent Graph
Neural Networks, is the first developed graph neural network for code-related tasks when Li et al.[98]
Manuscript submitted to ACM

18

Chen et al.

proposed Gated Graph Neural Network to infer formulas for program verification. GGNN updates the state

of a node with a GRU cell, with the information from neighboring nodes and the node state in previous

timestamp. Fernandes et al. [52] modify GGNN to a framework to extend existing sequence encoders

and conduct the experiments on three summarization tasks. Graph-based Grammar Fix (GGF) [172] use
a mixture of GRU and GGNN as an encoder to encode the sub-AST (created by the erroneous code) and

a token replacement mechanism as a decoder to generate the code fixing actions. Graph2diff[149] uses

an encoder-decoder framework to predict the diff, also described as edit operations. The model takes the

source code, bug configures files and compiler diagnostic messages as the graph input. The GGNN is used

in the encoder stage to explicit the structure information of code. Although GGNN has shown its ability to

learn the syntactic information from the AST of the graph, as a kind of Graph Neural network, it can only

aggregate local information rather than global information. Therefore, some works also combine it with

sequential models, such as Graph Sandwiches [62]. Furthermore, since GGNN employs RNN, it requires

more memory to retain the hidden state of all nodes in the graph, despite the fact that it eliminates the

need to constrain parameters to ensure convergence.

Convolutional Graph Neural Network. Convolutional Graph Neural Network(convGNN) learns the
representations of nodes based on the node vector and the neighbors of the node in the graph. The informa-

tion from neighbors is combined through the aggregation process. Compare to Recurrent Graph Neural

Networks such as GGNN, Convolutional Graph Neural Networks can stack multiple graph convolutional

layers to better propagate the information across nodes, which can combine the information from neighbors.

LeClair et al.[88] use convolution GNN to encode the AST nodes and edges for code summarization. The

ConvGNN layer’s input is the AST token embedding and edges. after the ConvGNN layer, the model uses

an attention mechanism to learn the important tokens in the source code and AST. Ji et al.[74] also use GCN

to encode AST for the code clone task. Liu et al.[112] propose a task of code documentation generation for

Jupyter notebooks. When generating documentation, the model HAConvGNN considers the relevant code

cells and code token information. Convolutional GNN layers and a GRU layer are included in the encoder

for code cells’ AST. The output of the GRU layer will be the input of a hierarchical attention mechanism to

better preserve the graph structure.

Other Graph Neural Networks. There are some methods using other Graph Neural Networks. Different
from GCN, Graph Attention Network(GAT) [154] performs self-attention mechanism on message passing

stage to learn the graph representation. Wang et al. [165] models the flattened sequence of a partial AST

as an AST graph. To reduce the information loss, the parent-child relation and the positional information

are recorded. Further, three types of AST Graph Attention Blocks are proposed to capture the structural

information for learning the representation of the graph. Compare to GCN and GGNN, the GAT model

improves the explainability of code completion or code summary tasks due to the utilization of the attention

mechanism. Hoppity[45] uses another type of GNN, which is Graph Isomorphism Network(GIN), as external

memory to encode the AST of a buggy program, further using a central controller implemented by LSTM to
Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

19

predict the sequence of actions to fix bugs. The controller will expand or decline the memory when the

graph structure is changed.

4.3 Flow-Graph-based Structures

As previously introduced, flow graphs are separated into two types: control flow graphs (CFG) and data

flow graphs (DFG). Although a flow graph is more likely to be seen as a graph, there are few works that

treat flow graphs as trees. For example, BAST [103] splits the code of a method according to the blocks

in the dominator tree of CFG and generates the corresponding AST for the split code. The split ASTs’

representations are used in the pre-trained stage by predicting the next split AST in the dominator tree. The

CFG is only used in splitting AST but can make the model more efficient and scalable for large programs.

The works that consider the CFG from a graph perspective and apply deep learning methods to represent

the CFG are described in the following. The attributed Control Flow Graph (ACFG) is a common pre-

processing step in some of the following works, especially in binary code similarity detection, such as

Genius[49], Gemini [178] and BugGraph [75]. There are also some typical modifications for CFG, for

example, lazy-binding CFG [121], inter-procedural CFGs(ICFG) [47].

Convolutional Neural Network. Convolutional neural network (CNN) has translation invariance in
many training data, therefore, it can not only capture the semantic information of code but is also suitable

for order-aware modeling. Nguyen et al. [121] use CNN on the adjacency matrices converted by the lazy-

binding CFG. The CFG will be converted into a pixel image, and later with a CNN model to recognize

whether the target object is appears in the image. The method can be applied for malware detection. Yu et

al. [190] use Bert and CNN to learn CFG graph embedding, which can include semantic, structural, and

order information. During the adjacent node prediction task, the Bert model is used to pre-train tokens and

block embeddings. The order information of CFGs is extracted using CNN models. As indicated previously,

CNN models are employed as a component for learning order information or for downstream tasks rather

than directly for constructing the representation of the graph of code data.

Convolutional Graph Neural Network. DGCNN[195], as one of the ConvGNN, is proposed with a
similar pooling strategy SortPooling. The approaches can allow attributed information to be aggregated

quickly through neighborhood message passing, therefore, it is suitable for embedding structural information

into vectors for further classification. To solve the malware classification challenge, Yan et al.[181] use

DGCNN to embed CFGs. The CFG will first be converted to an attributed CFG, with the code characteristics

defining the attributes. The DGCNN is used to aggregate these attributes with an adaptive max pooling to

concatenate the layer outputs.

Graph Attention Network. Li et al.[91] propose an event-based method CSEM for clone detection. GAT
extracts the context information for each statement, which is modeled by the events that are embedded

to capture execution semantics. BugGraph[75] compares the source-binary code similarity in two steps:
Manuscript submitted to ACM

20

Chen et al.

source binary canonicalization and code similarity computation. In the code similarity computation step,

BugGraph computes the similarity between the target and the comparing binary code. After disassembling

both codes, each function will construct its ACFG and use GAT with the triplet loss as the output of the

model to generate the embedding of each graph.

Other Graph Neural Networks. As previously mentioned, Yu et al. [190] also uses MPNN[56] to compute
the graph embedding of a control-flow graph in order-aware modeling. BugLab [6] uses standard message-

passing graph neural networks to represent the graph of code entities. The graph includes syntactic entities

and relations about the intraprocedural data and control flow, types, and documentation. BugLab trains

two models, a selector model and a detector model to predict the rewrite on code snippet. The select model

introduces buggy code and the detector model detects and repairs the bugs. Wang et al.[166] propose a

new graph neural architecture called Graph Interval Neural Network(GINN) to learn the code embeddings.

GINN takes the program’s control flow graph as input and abstracts it using three operators. The CFG is

partitioned into a series of intervals using the partitioning operator. Messages between intervals passing

are restricted. Then the heightening operator is applied to replace each activate interval with single created
nodes until the sufficient propagation point is reached. The lowering operator will finally recover the

original CFG. GINN model use only looping construct to learn feature representation, and the method

shows improvement across program embeddings.

4.4 Combined Structures

In some cases, the two types of flow graphs do not appear separately as well as AST. The following part

will focus on how these structures are combined and what information can be retrieved from them.

Control Flow and Data Flow. For the combination of two types of graphs, some novel value graph is
presented such as the program’s Interprocedural Value-Flow Graph (IVFG). The IVFG is created using LLVM-

IR, which combines code control-flow and alias-aware data-flows. IVFG is a directed graph with multiple

edges. Flow2Vec[143] is a novel approach for embedding the code with IVFG. Pre-embedding, value-flow

reachability via matrix multiplication, and high order proximity approximation are the three steps in the

method. Brauckmann et al.[28] use AST or control-data flow graphs (CDFGs) as the input of the predictive

model to learn the code representation. The core of the predictive model is the GGNN in the embedding

propagation layer. The model has shown its effectiveness in two complex tasks on OpenCL kernels, the

CPU/GPU mapping, and Thread Coarsening. Deepsim [196] encodes both code control flow graph and

data flow graph into a semantic matrix and uses a deep learning model to measure function similarity. The

semantic matrix contains three features: variable features, basic block features, and relationship features

between variables and basic blocks.

AST and Flow Graphs. Allamanis et al. [5] propose a method based on GGNN to construct graphs for
source code, naming variables, and detecting variable misuse. The program graph combines both AST and
Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

21

the data flow graph, which contains both syntactic and semantic structure information of source code. Some

works combine AST and CFG for multi-modal learning for generating the hybrid representation of code,

such as [155]. Devign [197] utilizes GGNN with a Conv module for graph-level classification. The graph

representation is based on AST structure and added multiple types of edges from CFG, DFG, and NCS. The
Conv module learns the hierarchy information of representation of node features. Multi-modal learning

can reduce the limitation of the approaches only using AST to represent the code. The combination of AST

and Flow Graphs can cover extra semantic information of source code.

4.5 Other Structures

This section will discuss other structures that differ significantly from the AST and flow graphs. Although

the AST and flow-graphs have been employed in most previous works, a program project still contains other

graphs. Some are traditional graphs, such as the Program Dependency Graph and UML Diagrams presented

in 2.1.5, while others are recently proposed structures for better semantic-aware or structure-aware code

understanding.

Program Dependence Graph. Li et al.[99] propose an attention-based neural network to learn code
representation for the bug detection task. The global context is extracted by the Program Dependence

Graph and Data Flow Graph using Node2Vec, while the local context is extracted by previous bug fixes and

AST paths. The global context and local context will be unified as the path vector for further analysis.

UML Diagrams. CoCoSUM[164] model the UML diagrams for code summarization task. The framework
encodes the class names with a transformer-based model as the intra-class context and the UML diagrams

with a Multi-Relational Graph Neural Network (MRGNN) as the inter-class context. The two kinds of

embeddings together with the embeddings of the token and AST will be passed to an attention-based

decoder to generate code summaries.

Code Property Graph. Yamaguchi et al.[180] model three types of code-related structures: ASTs, CFGs,
and Program Dependency Graph [54] as property graphs and combine them into code property graph. The

newly proposed data structure enables characterizing the vulnerability type through graph traversals. Liu

et al.[111] use the code property graph and propose a Hybrid GNN framework in the code summarization

task. The framework fuse the static graph and dynamic graph to capture the global information of graphs.

Program Feedback Graph. Yasunaga et al.[185] introduce a program feedback graph to model the
reasoning process in program repair task. The nodes in the program feedback graph consist of tokens in the

diagnostic arguments, the occurrences in the source code, and the remaining identifiers in the code. The

framework DrRepair uses LSTM to encode the source code initially and GAT to further reason over the

graph.

Manuscript submitted to ACM

22

Chen et al.

5 DISCUSSION AND COMPARISON

We have introduced two types of models in code understanding: sequence-based and graph-based models

in section 3 and Section 4 respectively.

Sequence-based models are models for processing code sequences such as NCS and flatten sequences

obtained from AST by several transformation methods introduced in Section 3.1. Traditional statistical

language models such as N-gram models are used in a large amount of early works. With the development

of deep learning, word2vec and DBN models have also been used for code representation and downstream

tasks. Furthermore, CNN which has revolutionized the computer vision field, can effectively capture rich

structural patterns in sequences and are therefore naturally adopted for code sequence tasks. However, the

most dominant models among sequence-based models are vanilla RNN models with their variants, such as

LSTM, GRU, and Bi-LSTM, which are tailored for sequence modeling tasks. In recent years, transformer-

based models that incorporate self-attention blocks have made major contributions to code sequence-based

models.

Graph-based models are models that use the graph structures generated by codes such as AST and Flow

Graphs to capture structural information. The transformation of graph structures has also been introduced

in Section 4.1. Some of these structures, especially AST can be seen as a tree or a graph, which leads to a

different way to process the structures. From a tree perspective, these models use RNN models designed for

tree structures, such as Tree-LSTM. From the perspective of the graph, CNN is also one of the most used

models, while graph neural networks play an important role in the pipeline to learn the representation of

nodes or graphs, such as GGNN, GCN, GAT, or MPNN with both AST and flow graphs. Above AST and

Flow Graphs, there are also some combination structures and other rarely seen structures, for example,

UML diagrams, Program Dependency Graph and so on used in graph-based models.

The two categories of models have both shown the effectiveness on code representation and downstream

tasks which we will introduce in Section 6. In this section, we emphasize three differences between the two

types of models:

First, sequence-based models and graph-based models view data in a different perspective. Be-
cause sequence-based models treat code as a collection of sequences, they must properly capture the

relationships between sequences that correspond to semantic and syntactic information in the source code.
Graph-based models view code data as a tree or a graph. Since nodes and edges in code graphs are rich

in structural information, these models learn the representation in graphs to better understand the code.

However, the models that use only sequential data are neglecting the syntactic or semantic information

in structures, while the models learn solely on graph structures ignore the sequential information. As

previously introduced, the boundary of the two kinds of models is not clear if we divide them with the

information they used. There are sequences flattened from the AST structure that automatically combine

the structural information in AST. In Graph-based models, some methods also use sequential information

in code.

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

23

Second, sequence-based and graph-based models are both use serial models but with distinct
proposes and scenarios. RNN and Transformer are the most commonly used serial models in deep
learning models in code understanding, but the two types of models use RNN and Transformer with

different proposes. Sequence-based models use RNN and Transformer as the core component of the models
to learn relationships in code sequences. The RNN[23, 132] is used in the sequence-based models to tackle

the problem of information passing in the code sequence. LSTM[42, 72] and GRU[68, 89] which are the

variant of RNN use the gating mechanism to better transfer useful information and solve the problem

of gradient explosion and disappearance in code sequences. Transformer-based models[2, 10] are more

commonly employed to address the issue of global reliance and use positional embedding to maintain the

order information of code sequence, allowing for better learning of the entire code information.

Although RNN-based model designed for serial data, they are widely used in graph-based models as

sequences are the original form of code that contains the context information. Furthermore, although Graph

Neural Network can capture the structural information of structures, the context information cannot be

preserved once the source code is converted to a graph structure. Therefore, approaches in graph-based

models will also use RNN such as LSTM or GRU to gather long-distance information. Some works use RNN

for tree structures such as RvNN[138], or tree-LSTM[155]. Most of the works combined the RNN with GNN as

the whole model frameworks, mainly categorized as the following three usages: 1) Conducting bidirectional

GRU[193] or bidirectional LSTM as the encoder of the models, 2) using GRU to fuse resulting vectors after

GNN components[111], 3) using RNN such as LSTM as decoder[166]. Graph Sandwich Structure[62] is one

of the most typical works that combine GNN with RNN and Transformer structure to combine both local

and global information.

Third, Attention mechanism are used in sequence-based and graph-based models but with

various modifications.

The attention mechanism is widely used in neural machine translation, especially in encoder-decoder

frameworks. The attention mechanism can learn which words are important and further make predictions

according to the importance of words. In code representation-related works, attention mechanisms are

conducted in both sequence-based models and graph-based models.

In sequence-based models, the attention mechanism is primarily used to pay attention to the relationships

between tokens in code sequences, including the relationships between input sequence tokens, between
output sequence tokens and input sequence tokens, or between output sequence tokens, in order to facilitate

more efficient information transmission. The Transformer-based models use self-attention to focus on the

three relationships mentioned above while the other models pay more attention to the relationship between

output sequence tokens and input sequence tokens. Attention mechanisms are added to RNN-based models

by CodeNN[72] and other models, allowing the model to make greater use of crucial token information

while creating code summary or predicting the next token.

Some work in graph-based models conduct attention mechanism before output layer and create a context

vector to predict the next token in the sequence, such as LeClair et al.[88]. Others make modifications on
Manuscript submitted to ACM

24

Chen et al.

vanilla deep learning models with attention mechanism, for example, using attention mechanism in message

propagation process of GCN to hierarchically update the embeddings[74], combining attention mechanism

in GRU layer and Convolutional layer to encode the order of the nodes in a path [99]. Different from

sequence-based models, some works modify the original attention mechanism to better suit the hierarchical
structure. Liu et al[112] propose low-level attention and high-level attention. The former attention module

attends the token in a sequence while the latter attends the code cell in the AST tree.

6 TASKS

We categorized the tasks used in code understanding into the following downstream tasks and summarized

the sequence-based models and graph-based models on each downstream tasks.

6.1 Code Generation

Code Generation can provide varying levels of granularity of code output depending on the input. Code

prediction in IDE that anticipates blank areas of input code snippets, such as method name prediction,

next token prediction, and so on, is a typical code generation task. It’s also a kind of code generation task

to generate snippets of code from natural language. Code generation considerably enhances developers’

efficiency and has been extensively researched in both industry and academia. The relevant approaches are

summarized in Table 2, which is based mostly on the structure used by the model in the code generation

task.

6.1.1 Method name generation. This task generates a summary method name based on the given method
code body, which can help the code be more understandable, maintainable, and invocable. The formalization

of the method name generation task is as follows:

Given a code body of a method with n tokens 𝑆 = 𝑡1, 𝑡2, ...𝑡𝑛, the generative model can output the method

name.

6.1.2 Next token generation. This task predicts the tokens including API calls that will be inputted by the
developer. The models of the next token generation typically provide a list of tokens in order of probability

depending on the developer’s previous input. It can substantially speed up development. The formalization

of the next token generation task is as follows:

Given a partial code snippet with 𝑛 − 1 tokens 𝑆 = 𝑡1, 𝑡2, ...𝑡𝑛−1, the generative model can generate a

token list of 𝑡𝑛 sorted by probability.

6.1.3 Expression generation. Compared to the next token generation, this task is a more granular code
prediction task based on existing code fragments. It generates complete code expressions with certain

functions such as conditional statements, loops, etc. It also includes the task of predicting missing code

statements. The formalization of the next expression generation task is as follows:

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

25

Given a partial code snippet with 𝑛 tokens 𝑆 = 𝑡1, 𝑡2, ..., 𝑡𝑛−1, the generative model can generate a whole
code expression 𝑆𝑚 = 𝑡𝑛, 𝑡𝑛+1..., 𝑡𝑛+𝑚−1 with a specific function. Or given a code with a missing piece
𝑆 = 𝑡1, 𝑡2, ..., 𝑚𝑖𝑠𝑠𝑖𝑛𝑔𝑝𝑖𝑒𝑐𝑒, ..., 𝑡𝑛, the generative model generates the missing piece code.

Function generation. This task can generate corresponding code snippets based on the user’s descrip-
6.1.4
tion of the code’s functionality, which can be seen as a natural language to the code translation process.

Excellent natural language to code translation techniques can enable non-specialists to develop accordingly,

but work in this area is still immature and needs to be further developed. The formalization of the function

generation task is as follows:

Given a natural language 𝐷 for the functional description of the code 𝑆, the generative model can generate

the 𝑆 = 𝑡1, 𝑡2, ..., 𝑡𝑛 according to 𝐷.

6.2 Code Summarization

Code summarization is the work of using natural language to provide a concise explanation of the code’s

functioning. It can help enhance the code’s readability, as well as the developer’s efficiency in understanding

the program. The code summarization task can be thought of as a translation of the input code into natural

language, hence a seq2seq model architecture is commonly used. In the encoder phase, [67, 68, 89] convert

the input NSC or Flattened sequence into a context vector, and then in the decoder phase, they construct

the words in the summarization one by one based on the context vector. To increase the effectiveness of

code summarization, techniques such as the attention mechanism are applied to the task. The relevant

techniques are summarized in Table 3 based on the model’s relevant code structures.

6.3 Code Search

Code search is a information retrieval task. The input of code search is the query which is natural language

description or a code snippet. And the output is the best matching code snippets for input. The goal of Code

Search is to retrieve code snippets from a large code corpus that most closely match a developer’s intent[32].

Being able to explore and reuse existing code that is match to the developer’s intent is a fundamental

productivity tool. Some online sites such as Stack Overflow are popular because of the convenience that

searching for code relevant to a user’s question expressed in natural language.

Some articles refer to this task as semantic code search or code recommendation, these names emphasize
characteristics of code search. It is based on the semantics of the input, and the semantic alignment between

input and code snippets is crucial. And the information retrieval nature of this task is that all outputs are

retrieved from the code corpus as they originally are. The retrieval nature distinguishes code search and

code generation, where the generation task is aiming to synthesize and write codes not only already in the

code corpus.

The code search models are summarized in Table 4.

Manuscript submitted to ACM

26

Chen et al.

Table 2. Code Generation

Type

Reference

Model

Code2seq[8]

BiLSTM

Code2vec[14]

SLM[10]
Pythia[146]

Sequence

[107]

[93]

[82]

[109]

[12]

LSTM

LSTM
LSTM

LSTM

LSTM, Pointer
Network
Transformer
Transformer-
XL

Word2vec

API2Vec[122]

Word2vec

next token

next token

method
name
next token

DEEP3 [131]

Decision tree

next token

Graph

[29]

GRU,GGNN

expression

CCAG[165]

GAT

next token

[5]

GGNN

Graph-
Structured
Cache[40]

MPNN,CharCNN

method
name

method
name

Generation
method
name
method
name
expression
next token

next token

Description
Represents code by a collection of paths between
terminal nodes n the AST

Generates code’s representation by AST path

Generates missing code using all the paths to it
Predicts the methods and API calls by flattened AST
Uses the sequence obtained by traversing the AST
to predict the next possible node

next token

Generates code through LSTM or pointer network

Feeds different paths to Transformer
Feeds different paths to Transformer-XL with multi-
task learning
Feeds different paths with up/down momentum to
Word2vec
Feeds different API paths to Word2vec
Represents code in a DSL called TGEN and build
probabilistic models with decision trees
A generated model on ExprGen task which first
obtains a graph by attribute grammars and later
compute the attribute representations with GGNN
and GRU
Uses AST Graph Attention Block(ASTGab) to model
the flattened sequence of AST to capture different
dependencies
Constructs graphs by adding different types of edges
and use GGNN to learn the representation of the
graph
Introduces a Graph-Structured Cache representing
vocabulary words as additional nodes, uses MPNN
and CharCNN to generate outputs to further address
open vocabulary issue

6.4 Clone Detection

Clone detection task indicates that there are two or multiple similar code snippets in the same software

or system, which is also known as code clones. The code clones can support the modifications by the

developers for better reusing them. Code clones can be described as the following 4 types [22]:

Type-1 clones are identical code fragments, which may contain slight differences in white-space, layouts,

or comments.

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

27

Type

Sequence

Reference
DeepCom
[67]
Hybrid-
Deepcom
[68]
Ast-
attendgru
[89]
[7]

[72]

[192]

[2]

[157]

[52]

Graph

TBCAA[35]

[88]

Flow2Vec
[143]

BASTS
[103]

CoCoSum
[164]

Hy-
bridGNN
[111]

[138]

[112]

Table 3. Code Summarization

Model

LSTM

Description
Employs the seq2seq model which uses LSTM as encoder and decoder
to generate code fragment summaries

LSTM

Utilizes two GRU as encoders for NCS and flattened sequence

Uses two GRU with attention mechanism to process NCS and flattened
sequence respectively for getting context vector

Uses Convolutional Attention Network to summarize the code
Uses LSTM with attention to produce sentences that describe code
snippets
Retrieves flattened sequence on the search engine to obtain syntacti-
cally similar code fragments

Employs transformer for code summarization

Uses a attention layer to fuse two representations, one from the
structure of source code with AST-based LSTM, the other from the
sequence with LSTM
Uses sequential encoder and a GGNN to generate the representations

Uses Tree-based convolution for API-enhanced AST

Encodes the node token embedding with recurrent layer and a Con-
vGNN, later uses an attention layer to learn important tokens
Pre-embeds the interprocedural value-flow graph, considers the reach-
ability via matrix multiplication problem and uses it to approximate
the high-order proximity embedding
Uses Tree-LSTM and Transformer architecture to combine the repre-
sentations of split AST and source code

The global encoder contains an MRGNN to embed the UML class
diagrams and a transformer-based model for embedding the class
names, while the local encoder uses the GRU

GRU

CNN

LSTM

search
engine
Trans-
former
RNN,
Tree-
RNN
GGNN
Tree-
based
LSTM
Con-
vGNN

Flow2Vec

Tree-
LSTM
Trans-
former,
Multi-
Relational
GNN

GNN

A hybrid message passing GNN based which fuse the static and
dynamic graph

RNN,
attention

HACon-
vGNN

A convolutional attention network for extreme summarization of
source code
Hierarchically splits the AST into subtrees, learns the representation
of split AST, and reconstructs them to combine the structural and
semantic information with RvNN

Manuscript submitted to ACM

28

Chen et al.

Table 4. Code Search

Type

Reference

Model

[136]

word2vec

Sequence

CO-
DEnn [57]

RNN

UNIF[32]

word2vec

Description
Combines natural language techniques and information retrieval meth-
ods
Jointly embeds code snippets and natural language descriptions into a
high-dimensional vector space
Uses attention to combine per-token embeddings and produce the code
sentence embedding

CARLCS-
CNN [142]

TBCAA[35]

Graph

MMAN[155]

CNN

Uses CNN to embed code and query respectively

tree-based
LSTM

GGNN,
Tree-LSTM

Tree-based convolution for API-enhanced AST

A multi-modal Attention Network that uses attention mechanism to
capture the information from an LSTM for embedding sequential tokens,
a Tree-LSTM for embedding AST, and a GGNN for representing CFG

Type-2 clones are identical code fragments, which may contain the differences of variable names,

constants, function names, identifiers, literals, types, layouts, white-space, or comments.

Type-3 clones are syntactically similar code fragments with added, deleted, or modified statements.
Type-4 clones are semantic similar code fragments that may use different lexical and syntax to express

the equivalent semantic.

As the similarities decrease in the four types, the difficulty of detecting the clones increases. The ap-

proaches to solve the clone detection is shown in Table 5.

6.5 Safety Analysis

Humans are relying more and more on programs and codes to handle various problems in life as computer

technology advances, and defects and vulnerabilities in codes can result in significant losses. As a result, it is

vital to examine the code’s reliability and security. Defects in code can cause programs to fail to run properly,

and vulnerability can pose a potential threat to the safe operation of computer systems. Furthermore,

Malware is Malicious software which is designed to attack the device. Therefore, in Safety analysis, we

categorized three safety-related tasks: defect prediction, vulnerability prediction, and malware classification

and further summarize the models for these three categories in Table 6.

6.5.1 Defect Prediction. Defect prediction helps developers test more effectively while lowering the cost of
software development by predicting areas of problematic code. The two types of defect prediction tasks

now accessible are within-project defect prediction (WPDP), in which the training and test sets are from the

same project, and cross-project defect prediction (CPDP), in which the test set is different from the training

set. The formalization of the Defect prediction task is as follows:

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

29

Type

Sequence

Graph

Reference
CCLearner
[96]
[170]

DeepSim
[196]

ASTNN
[193]

TBCAA
[35]

DEEPBIN-
DIFF
[47]

[190]

CSEM [91]

OSCAR
[126]

Model

DNN

RNN
Feed-
forward
neural
network
bidirec-
tional
RNN
tree-
based
LSTM
Text-
associated
Deep-
Walk

MPNN,
CNN

Trans-
former,
GAT,
CNN

Trans-
former

HAG [74]

GCN

Table 5. Clone Detection

Description
Extracts tokens from known method-level code clones and non-clones
to train a classifier
Uses RNN modeling sequences of terms in a source code corpus

An approach for measuring code functional similarity that uses two
flow graphs as the basis and encodes them with Feed-forward neural
network

Encodes the statement subtree and uses Bi-GRU to model the natu-
ralness of the statements

Tree-based convolution for API-enhanced AST

Learns basic block embeddings with Text-associated DeepWalk algo-
rithm, and match them with the k-hop greedy matching algorithm

Use BERT to pre-train token and block embeddings on an MLM task,
and fine-tune them on 2 graph-level tasks with MPNN, GRU, and
CNN

Converts source code to intermediate representation, generates Node
vector matrix and inputs it into GAT later and CNN layer to obtain
embedding of code fragment

A hierarchical multi-layer Transformer pre-trained model with a
novel positional encoding, contrastive learning with optimization
techniques
Uses GCN with layer-wise propagation and attention mechanism

Given a code snippet with 𝑛 tokens 𝑆 = {𝑡1, 𝑡2, ...𝑡𝑛 }, the predictive model can output a label 𝑦 which

means the code snippet with defects(Buggy) or without defects(clean).

6.5.2 Vulnerability Detection. The Vulnerability detection task, including vulnerability detection based
on code similarity and code patterns, can prevent code from being attacked and improve the security of

code. Some approaches use deep learning for vulnerability detection ,for example, VulDeePecker [101]. The

formalization of the Vulnerability detection task is as follows:

Given a code snippet with 𝑛 tokens 𝑆 = {𝑡1, 𝑡2, ...𝑡𝑛 }, the predictive model can output a label 𝑦 which

means the code snippet with or without vulnerability.

Manuscript submitted to ACM

30

Chen et al.

Table 6. Safety analysis

Type

Reference

Model Kind

[161]

DBN

Defect

Sequence

Seml[102]

LSTM Defect

Description
Uses DBN to automatically learn the semantic expression of
code
Predicts software defect with the help with LSTM

DeepCPDP[34]

Bi-
LSTM

Defect

Proposes SimAST and SimAST2Vec

[160]

[86]

DBN

Defect

CNN-
BiLSTM

Mal-
ware

[55]

CNN

Mal-
ware

[5]

GGNN Defect

Graph

MAGIC [181]

DGCNN

Defect

Utilizes the DBN to learn the higher-level semantic character-
istics of code AST token
Proposes an approach to enable malware classification by
malware analysis non-experts
Proposes a file agnostic deep learning approach for malware
categorization to efficiently group malicious software into
families based on a set of discriminant patterns extracted
from their visualization as images
Constructs graphs by adding different types of edges and uses
GGNN to learn the representation of the graph
Extends the standard DGCNN on Weighted Vertices layer
and Adaptive Max Pooling to aggregate attributes from graph
structures

[99]

Devign [197]

GRU,
CNN,
Atten-
tion
mecha-
nism
GGNN,
GRU,
CNN

BugGraph [75]

GTN

[28]

GGNN

Defect

Extracts the global (from PDG and DFG) and local (from AST)
context with Attention-Based GRU, CNN.

Vul-
nera-
bility
Vul-
nera-
bility

Mal-
ware

Encodes the code into a joint graph structure and uses GGNN
with the Conv module to learn the embedding.

A Graph Triplet-loss Network on the attributed CFG to learn
similarity ranking.

Uses GGNN for learning predictive compiler tasks on AST
and CDFGs

6.5.3 Malware Classification. Malware classification is one kind of malware detection, which is a binary
classification problem. Malware classification can be formalized as: Given a program P, classify P as a normal

program or malware.

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

31

Table 7. Bug Localization

Type

Reference Model

Sequence

[152]

LSTM

BULNER[21] Word2vec

GGF [172] GGNN

Graph

[45]

GINN
[166]

GAT,
LSTM

GINN

Description
Presents multi-headed pointer networks for training a model that jointly
and directly localizes and repairs variable-misuse bugs
Proposes a method for Bug Localization with word embeddings and
Network Regularization
Uses GRU and GGNN as encoder and a token replacement mechanism
as decoder to encode the token information and generate the fixing
actions
Introduces a program feedback graph and apply GNN to model the
reasoning process.
Proposes Graph Interval Neural Network, which includes the heighten-
ing and lowering operator to learn the representation of CFG

6.6 Bug Localization

Bug localization is a task that localizes the position of bugs in a buggy code snippet. The bug localization

can also be seen as the previous step of program repair. Bugs fall into two categories based on how they are

discovered: static and dynamic. The static bug location is determined by the control and data dependencies,

whereas the dynamic bug location is determined by the program execution. We provide works of bug

localization task in Table 7. The formalization of bug localization task is as follows:

Given a code snippet with 𝑛 tokens 𝑆 = {𝑡1, 𝑡2, ...𝑡𝑛 }, the predictive model will predict the position 𝜆 of
buggy token 𝑡𝜆 such as the misused variables or operators. The place where the correct token is needed to
predict is also called a slot.

6.7 Program Repair

Program Repair, also known as bug fix, code refinement, aims at fixing the localized bugs. Some works

perform the bug localization and program repair tasks jointly, for example, BugLab [6]. After detecting

the bugs, the repair is conducted on the typical line of the program. Some works combine bug detection

and program repair, which predict the location and fix action at the same time with a sequence combining

two of them[152]. Other repair tasks will only fix bugs assuming the bug locations already exist, such as

CODIT[33].

One of the most popular tasks in automated program repair is the VARMISUSE task proposed by

Allamanis[5]. The VARMISUSE task is to automatically detect the variable misuse mistakes in source

code and repair it with the correct variable. In other words, program repair task can be seen as a kind of

code generation in the slot where the mistake is detected. However, due to the difference of input and the

proposal of generating the code, we consider program repair as a new task and summarize the models in

Table 8.

Manuscript submitted to ACM

32

Chen et al.

Table 8. Program Repair

Type

Reference Model

Description

Sequence DeepFix[59]

GRU

Fixes multiple errors by iteratively invoking a trained neural network

TFix[26]

CODIT
[33]

Trans-
former

LSTM

Graph

GGF [172] GGNN

Hoppity
[45]
GINN
[166]

GAT,
LSTM

GINN

Uses T5[130] to accurately synthesize fixes to a wide range of errors

An encoder-decoder model which first learns the structural changes
in AST modifications with tree-to-tree model, then predicts the token
conditioned on the AST
Uses GRU and GGNN as encoder and a token replacement mechanism
as decoder to encode the token information and generate the fixing
actions
Introduces a program feedback graph and applies GNN to model the
reasoning process
Proposes Graph Interval Neural Network, which includes the heighten-
ing and lowering operator to learn the representation of CFG

Because of the diverse datasets, languages, and granularities of the output, different works have distinct

definitions of program repair. For example, the output may be a single word to repair a misused variable or

a full sentence to repair a new code snippet line. The formalization of the repair of a token is similar to the

formalization in 6.6. After detecting the positions, the program repair task will generate the correct token

𝑡 ′
𝜆

. Another of the formalizations of the program repair task is as follows:
Given a broken code snippet 𝑆 = {𝑙1, 𝑙2, ...𝑙𝑘 } (with 𝑘 line) the diagnostic feedback from compiler(the
feedback usually contains line number and error message) 𝐼 , the program repair task is to localize the
erroneous line index 𝑛 and generate a repaired code version 𝑙 ′

𝑛 replacing the wrong code of 𝑙𝑛.

6.8 Pre-training Task

Pre-training is the process of training a model on a large amount of pre-training data to extract as many fea-

tures as possible from the data so that the model can better solve the aiming task after fine-tuning in specific

dataset. With the proliferation of open source corpora, pre-trained models have emerged for large amounts

of code data, such as CodeBERT [51],CuBert[79], GPT-C[145], CodeT5[167]and GraphCodeBERT[58], which

can capture semantic information in code and be quickly and effectively applied to various downstream tasks.

The pre-training models are becoming mainstream models for code-related tasks. A series of pre-training

models have been proposed by large companies such as Microsoft and Facebook. Some models such as
Alphacode2 and Codex3 have been applied in practice.

2https://alphacode.deepmind.com/
3https://openai.com/blog/openai-codex/

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

33

7 METRICS AND DATASETS

We describe the metrics and datasets associated with code-related work in this section to serve as a reference

for future work.

7.1 Metrics

The relevant metrics in the code representation field are divided into two categories: NLP-related metrics

and information retrieval-related metrics.

7.1.1 NLP-related Metrics. Due to the similarity between code and natural language, many metrics employed
in the field of code are generated from the natural language field, particularly the generation tasks such as

code summary.

• Accuracy, Precision, Recall and F1-score One of the most intuitive performance metrics is accu-
racy, which is defined as the ratio of correct predictions to total predictions. Precision refers to the

percentage of correct predictions of true samples among all predictions predicted as true samples
whereas Recall refers to the percentage of correctly predictions of true samples among all true samples.

The weighted average of Precision and Recall is the F1 Score. As a result, this score considers both

false positives and false negatives. It is more useful, especially when the distribution of classes is

uneven. The three metrics are commonly used in classification or prediction tasks.

• BLEU BiLingual Evaluation Understudy(BLEU)[125] is designed for automated evaluation of sta-
tistical machine translation and can be used to measure the performance of code summarization

and generation tasks. The score is computed as Equation 1 and 2, where the former is the Brevity
Penalty(BP) with the length of the candidate translation 𝑐 and the effective reference sequence length
𝑟 . TBP 𝑝𝑛 is the ratio of length n subsequences in the candidate that is also in the reference. And the
𝑁 in Equation 2 is usually set to 4(BLEU-4)[67].

BP =

(cid:40)

1
𝑒 (1−𝑟 /𝑐)

if 𝑐 > 𝑟
if 𝑐 ≤ 𝑟

BLEU = BP · exp

(cid:33)

𝑤𝑛 log 𝑝𝑛

(cid:32) 𝑁
∑︁

𝑛=1

(1)

(2)

• Perplexity (PPL) Perplexity is a great probabilistic measure used to evaluate exactly how confused
the NLP models are. It’s typically used to evaluate language models, as well as the dialog generation
tasks. PPL is defined as Equation 3,where 𝑥𝑖 is the truth label and 𝑃 (𝑥𝑖 ) is the model output. A model
with lower perplexity assigns higher probabilities to the true tokens and is expected to perform better.

(cid:32)

𝑃𝑃𝐿 = exp

−

𝑇
∑︁

𝑖

(cid:33)

𝑃 (𝑥𝑖 ) log 𝑃 (𝑥𝑖 )

, ∀𝑖 ∈ 0 . . . 𝑇 .

(3)

Manuscript submitted to ACM

34

Chen et al.

• ROUGE As opposed to the BLEU score, the Recall-Oriented Understudy for Gisting Evaluation
(ROUGE) evaluation metric[104] measures the recall.It is commonly used in machine translation tasks

to assess the quality of generated text. However, because it assesses recall, it is mostly employed in

summarization tasks, where evaluating the amount of words the model can recall is more significant.
[104] proposes four Rouge methods: 1) ROUGE-N: calculate the recall rate on n-gram, 2) ROUGE-L:
consider the longest common subsequence between the generated sequence 𝐶 and the target sequence
𝑆, 3) ROUGE-W: improve ROUGE-L and calculate the longest common subsequence by weighting
method, and 4) calculate the recall rate on n-gram that allows word skipping. The calculation method
of ROUGE-L is shown in Equation 4,where 𝐹𝐿𝐶𝑆 is ROUGE-L and 𝛽 is a constant.

𝑅𝑒𝑐𝑎𝑙𝑙𝐿𝐶𝑆 =

𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝐿𝐶𝑆 =

𝐹𝐿𝐶𝑆 =

𝐿𝐶𝑆 (𝐶, 𝑆)
len(𝑆)
𝐿𝐶𝑆 (𝐶, 𝑆)
len(𝐶)
(cid:0)1 + 𝛽 2(cid:1) 𝑅𝑒𝑐𝑎𝑙𝑙𝐿𝐶𝑆𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝐿𝐶𝑆
𝑅𝑒𝑐𝑎𝑙𝑙𝐿𝐶𝑆 + 𝛽 2𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝐿𝐶𝑆

(4)

• METEOR The Metric for Evaluation of Translation with Explicit ORdering (METEOR)[20] is a
precision-based metric for the evaluation of machine-translation output. It overcomes some of the

pitfalls of the BLEU score, such as exact word matching whilst calculating precision. The METEOR

score allows synonyms and stemmed words to be matched with a reference word. The most important

thing in meteor is to use WordNet thesaurus to align the generated sequence with the target sequence.
• Word Error Rate (WER) It is important to substitute, delete, or insert some words into the generated
sequence during the generation task in order to keep the generated sequence consistent with the target

sequence. WER, which is defined as Equation 5, can be used to assess the quality of the generated

sequence.

WER = 100 ·

𝑆𝑢𝑏𝑠𝑡𝑖𝑡𝑢𝑡𝑖𝑜𝑛 + 𝐷𝑒𝑙𝑒𝑡𝑖𝑜𝑛 + 𝐼𝑛𝑠𝑒𝑟𝑡𝑖𝑜𝑛
𝑁

%

(5)

• CodeBLEU CodeBLEU[133] is a metric designed for code based on BLEU, which can pay attention
to the keywords, leverage the tree structure and consider the semantic logic. It is defined as the
weighted combination of BLEU, BLEUweight ,Matchast and Matchdf, which is shown in Equation 6.
The first term refers to the standard BLEU. BLEUweight is the weighted n-gram match, obtained by
comparing the hypothesis code and the reference code tokens with different weights. Matchast is the
syntactic AST match, exploring the syntactic information of code. Matchdf is the semantic data-flow
match, considering the semantic similarity between the prediction and the reference. The weighted

n-gram match and the syntactic AST match are used to measure grammatical correctness, whereas

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

the semantic data-flow match is used to calculate logic correctness.

CodeBLEU = 𝛼 · BLEU + 𝛽 · BLEUweightt

+ 𝛾 · Matchast + 𝛿 · Matchdf

35

(6)

Information Retrieval related Metrics. Because some code-related tasks, such as Code Search, are

7.1.2
similar to information retrieval, many code-related metrics are associated to information retrieval field.

• SuccessRate(SR) If the information matching the input is in the top-k of the search information
sorting list, the search is successful. SR@k Calculate the proportion of successful searches in all
searches. The calculation method is shown in Equation 7, where 𝛿 is a constant, 𝐹𝑟𝑎𝑛𝑘𝑘 is the rank
of matched information in the search information sorting list and 𝐹𝑟𝑎𝑛𝑘𝑞 ≤ 𝐾 means successful
retrieval.

SR@K =

1
|𝑄 |

𝑄
∑︁

𝑞=1

𝛿 (cid:0)FRank𝑞 ≤ 𝐾 (cid:1)

(7)

• Mean Reciprocal Rank (MRR) In the retrieval information sorting list, MRR considers the ranking
of the retrieved matching information. The score is 1
𝑁 if the nth information in the list fits the input,
and 0 if there is no matching sentence. The sum of all scores is the final score. Equation 8 shows how
to calculate MRR.

MRR =

|𝑄 |
∑︁

1
|𝑄 |

1
FRank𝑞

(8)

𝑞=1
• Best Hit Rank Best Hit Rank is the highest rank of the hit snippets for the query. A highest best hit

implies lower user effort to inspect the desired hit snippet.

7.2 Datasets

We summarize various datasets in the table 9. Some works collect and generate their own datasets for study,

which may cause the difficulty on comparing different works on the same tasks. Table 9 does not contain

any datasets that are not open-source.

8 OPEN PROBLEMS

It is difficult to fully capture the information of the structures and semantics of codes with the existing

technology. Most deep-learning models are designed for specific tasks and a single language, which are lack

flexibility. The following open problems can be considered as the future work direction.

Information capturing. Many approaches use structural information in code, however the majority
of them just use structure information such as AST to capture syntax information. There are only a few

approaches to learning the whole representation of code that combine structure and semantic information

(such as DFG), and they are only applied to specific tasks, not all tasks. As a result, one of the goals of future

Manuscript submitted to ACM

36

Chen et al.

Table 9. Datasets

Name

Genius Dataset

notebookcdg

py150

js150

HGNN

Study

[50, 178]

[112]

[78, 82, 131]

[131, 165]

[111]

CodeSearchNet

[51, 70, 151]

CONCODE

CodeXCLUE

[3, 73]

[115, 128, 167]

TFix’s Code Patches Data

CoDesc

[26]

[60]

DeepFix

[36, 59, 185]

SPoC

Defects4J

FunCom

CoSQA

CoNaLa

Django

BLANCA

IndoNLG

Neural-Code-Search-
Evaluation-Dataset

Manuscript submitted to ACM

[185]

[33]
[90, 116, 141,
168]

[69, 97]

[188, 189]

[105, 124]

[1]

[30]

[92]

Description
A real-world dataset of 33,045 devices which was
collected from public sources and our system
Has 28,625 code–documentation pairs
Consists of parsed ASTs collected from GitHub
python repositories by removing duplicate files
Consists of 150’000 JavaScript files and their corre-
sponding parsed ASTs
Crawled from diversified large-scale open-source C
projects (total 95k+ unique functions in the dataset)
Consists of 2 million (comment, code) pairs from
open source libraries
Consists over 100,000 examples consisting of Java
classes from online code repositories
Includes a collection of 10 tasks across 14 datasets
Contains more than 100k code patch pairs extracted
from open source projects on GitHub
A large dataset of 4.2m Java source code and parallel
data of their description from code search, and code
summarization studies
Consists of a program repair dataset (fix compiler
errors in C programs)
A program synthesis dataset, containing 18,356 pro-
grams with human-authored pseudocode and test
cases
A large dataset of 32k real code change
A collection of 2.1 million Java methods and their
associated Javadoc comments
Includes 20,604 labels for pairs of natural language
queries and codes, each annotated by at least 3 hu-
man annotators
Consists 2379 training and 500 test examples that
were manually annotated
Comprises of 16000 training, 1000 development and
1805 test annotations
A collection of benchmarks that assess code under-
standing based on tasks
A collection of Natural Language Generation (NLG)
resources for Bahasa Indonesia with 6 kind of down-
stream tasks
An evaluation dataset consisting of natural language
query and code snippet pairs for code search

A Survey of Deep Learning Models for Structural Code Understanding

37

study could be to improve the model’s ability to use the structure and semantic information of codes in

various tasks.

Flexibility. The methods we described above are suitable to a certain task or dataset and lack flexibility.
The model’s flexibility implies it may be utilized in a range of scenarios, including those using datasets

from shorter programs (with small graph structures) or smaller sample sizes, as well as scenarios involving

several downstream jobs or programming languages. As a result, future study could concentrate on how to

train a model that can accommodate a variety of circumstances.

Model Simplicity. Recent models, particularly those based on Transformer and its derivatives, have
improved performance, but they often need more effort and machine capability. Therefore, it is necessary

to propose lightweight model under the premise of ensuring accuracy.

Explainability. Deep learning approaches have always had explainability issues, and the approaches
for code are no exception. While the current usage of attentional mechanisms in code generation tasks

can explain the origins of token generation, there are still no good explanations for other tasks like safety

analysis. Simultaneously, the model’s explainability is very useful in determining structural information

and producing superior metrics in the code. As a result, additional research into the explainability of code

models is still worthwhile.

Robustness. The robustness of code-domain models has not yet been studied, but as model performance
improves, the robustness of code-domain models is sure to become a hot study area in the future. In the

code domain, both sequence-based and graph-based models rely on the representation of code tokens to

some extent, which leads to model performance reduction when test code fragments are not represented in

the same way as training code fragments (e.g., different representations of API calls in the python language).

Furthermore, while code graph-based models can better capture the information of code fragments, graph

structures are sensitive to attacks. There have been numerous techniques to explore the robustness of graph

models, and how to convert them to work on code graphs is an important research field.

Metrics. In the previous section, the metrics for evaluate the effectiveness of the code representation
are based on the the Natural Language Processing area and Information Retrieval area. Although there

are metrics designed specifically for code representation, there is a small amount of number of metrics

proposed to suit the code data and downstream tasks. The following are the potential directions that can be

studied:

1) Measure of information The requirement for appropriate measures for the information used in
deep learning models is growing as the variety of structures employed in learning the representation for

codes increases. Recent metrics that measure how models utilise this information in these structures are

performed after the downstream tasks, however measures directly during the code representation stage

have never been provided.

Manuscript submitted to ACM

38

Chen et al.

2) Measure of Explainability As previous mentioned, generating better metrics for measuring model
efficacy on downstream tasks is vital for the explainability of models, which can better qualify how the

model works.

3) Measure for Bias Problem The code data will inevitably contains repeat and duplication, which
might contribute to a bias problem. As far as we know, there have been few studies and discussions on the

bias problem in code representation. Therefore, it is a new future direction to consider the bias in code data,

while reasonable metrics for measuring the bias of the models in code are required.

9 CONCLUSION

It is critical to understand the structural and semantic meaning of codes when working on intelligent software.

In this survey, We give a comprehensive overview of structure-based methods to code representation learning

in recent years, which we divide into two groups: sequence-based models and graph-based models, then

summarize and compare the methods in each group. The downstream tasks, as well as metrics and datasets,

are also introduced here. It is shown that deep learning models are useful in code understanding, and

further multiple downstream tasks. However, the field of code comprehension is still in its infancy, with
numerous obstacles and unsolved issues. Finally, as future directions for code understanding, we offer four

open questions.

ACKNOWLEDGMENTS

REFERENCES

[1] Ibrahim Abdelaziz, Julian Dolby, Jamie McCusker, and Kavitha Srinivas. 2021. Can Machines Read Coding Manuals Yet? – A

Benchmark for Building Better Language Models for Code Understanding. arXiv: Computation and Language (2021).

[2] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A transformer-based approach for source

code summarization. arXiv preprint arXiv:2005.00653 (2020).

[3] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019
ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software. 143–153.
[4] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big code and

naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 1–37.

[5] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to represent programs with graphs. In
6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings. arXiv:1711.00740 https:
//github.com/Microsoft/gated-graph-neural-network-samples

[6] Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. 2021. Self-Supervised Bug Detection and Repair. Advances in

Neural Information Processing Systems 34 (2021).

[7] Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional Attention Network for Extreme Summarization of

Source Code. arXiv: Learning (2016).

[8] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Generating Sequences from Structured Representations of

Code. arXiv: Learning (2018).

[9] Uri Alon, Omer Levy, Shaked Brody, and Eran Yahav. 2019. Code2Seq: Generating sequences from structured representations of

code. 7th International Conference on Learning Representations, ICLR 2019 1 (2019), 1–22. arXiv:1808.01400

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

39

[10] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural language models of code. In International Conference on

Machine Learning. PMLR, 245–256.

[11] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. A General Path-Based Representation for Predicting Program
Properties. ACM SIGPLAN Notices 53, 4 (mar 2018), 404–419. https://doi.org/10.1145/3192366.3192412 arXiv:1803.09544
[12] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. A general path-based representation for predicting program

properties. In Programming Language Design and Implementation.

[13] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. Code2Vec: Learning Distributed Representations of Code.
Proceedings of the ACM on Programming Languages 3, POPL (2019), 1–29. https://doi.org/10.1145/3290353 arXiv:1803.09473

[14] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: learning distributed representations of code.

[15] Kamel Alreshedy, Dhanush Dharmaretnam, Daniel M. German, Venkatesh Srinivasan, and T. Aaron Gulliver. 2018. SCC:

Automatic Classification of Code Snippets. arXiv: Software Engineering (2018).

[16] Matthew Amodio, Swarat Chaudhuri, and Thomas Reps. 2017. Neural Attribute Machines for Program Generation. arXiv:

Artificial Intelligence (2017).

[17] Syed Arbaaz Qureshi, Sonu Mehta, Ranjita Bhagwan, and Rahul Kumar. [n.d.]. Assessing the Effectiveness of Syntactic Structure

to Learn Code Edit Representations. ([n. d.]). arXiv:2106.06110v1

[18] Youri Arkesteijn and Nikhil Saldanha. [n.d.]. Code Completion using Neural AAention and Byte Pair Encoding.

([n. d.]).

arXiv:2004.06343v1 www.sri.inf.ethz.ch/py150

[19] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and

translate. arXiv preprint arXiv:1409.0473 (2014).

[20] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with

Human Judgments. In Meeting of the Association for Computational Linguistics.

[21] Jacson Rodrigues Barbosa, Ricardo Marcondes Marcacini, Ricardo Britto, Frederico Soares, Solange Oliveira Rezende, Auri

Marcelo Rizzo Vincenzi, and Márcio Eduardo Delamaro. 2019. BULNER: BUg Localization with word embeddings and NEtwork
Regularization. arXiv: Software Engineering (2019).

[22] Stefan Bellon, Rainer Koschke, Giulio Antoniol, Jens Krinke, and Ettore Merlo. 2007. Comparison and Evaluation of Clone
Detection Tools. IEEE Transactions on Software Engineering 33, 9 (2007), 577–591. https://doi.org/10.1109/TSE.2007.70725
[23] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. 2018. Neural Code Comprehension: A Learnable Representation

of Code Semantics. arXiv: Learning (2018).

[24] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. 2018. Neural Code Comprehension: A Learnable Representation
of Code Semantics. Advances in Neural Information Processing Systems 2018-December (jun 2018), 3585–3597. arXiv:1806.07336
https://arxiv.org/abs/1806.07336v3

[25] Yoshua Bengio. 2009. Learning Deep Architectures for AI.

[26] Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. 2021. TFix: Learning to Fix Coding Errors with a Text-to-Text

Transformer. In International Conference on Machine Learning.

[27] Avishkar Bhoopchand, Tim Rocktäschel, Earl T. Barr, and Sebastian Riedel. 2016. Learning Python Code Suggestion with a

Sparse Pointer Network. arXiv: Neural and Evolutionary Computing (2016).

[28] Alexander Brauckmann, Andrés Goens, Sebastian Ertel, and Jeronimo Castrillon. 2020. Compiler-based graph representations
for deep learning models of code. In Proceedings of the 29th International Conference on Compiler Construction. 201–211.
[29] Marc Brockschmidt, Miltiadis Allamanis, Alexander Gaunt, and Oleksandr Polozov. 2019. Generative code modeling with graphs.
In 7th International Conference on Learning Representations, ICLR 2019. International Conference on Learning Representations,
ICLR. arXiv:1805.08490 https://arxiv.org/abs/1805.08490v2

[30] Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder,

Zhi Yuan Lim, Syafri Bahar, Masayu Leylia Khodra, Ayu Purwarianti, and Pascale Fung. 2021. IndoNLG: Benchmark and
Resources for Evaluating Indonesian Natural Language Generation. arXiv: Computation and Language (2021).

Manuscript submitted to ACM

40

Chen et al.

[31] José Pablo Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. 2019. When deep learning met code search.

In FSE.

[32] José Pablo Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. 2019. When deep learning met code search.

In Foundations of Software Engineering.

[33] Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, and Baishakhi Ray. 2020. CODIT: Code Editing with Tree-Based
IEEE Transactions on Software Engineering TBD (sep 2020), 1–1. https://doi.org/10.1109/tse.2020.3020502

Neural Models.

arXiv:1810.00314

[34] Deyu Chen, Xiang Chen, Hao Li, Junfeng Xie, and Yanzhou Mu. 2019. DeepCPDP: Deep Learning Based Cross-Project Defect

Prediction. IEEE Access 7 (2019), 184832–184848.

[35] Long Chen, Wei Ye, and Shikun Zhang. 2019. Capturing source code semantics via tree-based convolution over API-enhanced

AST. In Proceedings of the 16th ACM International Conference on Computing Frontiers. 174–182.

[36] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, and Martin Monperrus. 2021.
SEQUENCER: Sequence-to-Sequence Learning for End-to-End Program Repair. IEEE Transactions on Software Engineering 47
(2021), 1943–1959.

[37] Zimin Chen and Martin Monperrus. 2019. A Literature Study of Embeddings on Source Code. (2019), 1–8. arXiv:1904.03061

http://arxiv.org/abs/1904.03061

[38] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint
arXiv:1406.1078 (2014).

[39] Rhys Compton, Eibe Frank, Panos Patros, and Abigail Koay. 2020. Embedding Java Classes with code2vec: Improvements from

Variable Obfuscation. In Mining Software Repositories.

[40] Milan Cvitkovic, Badal Singh, and Anima Anandkumar. 2019. Open vocabulary learning on source code with a graph-structured
cache. In 36th International Conference on Machine Learning, ICML 2019, Vol. 2019-June. International Machine Learning Society
(IMLS), 2662–2674. arXiv:1810.08305 https://arxiv.org/abs/1810.08305v2

[41] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V. Le. 2020. Funnel-transformer: Filtering out sequential redundancy for
efficient language processing. Advances in Neural Information Processing Systems 2020-Decem (2020), 1–19. arXiv:2006.03236
[42] Hoa Khanh Dam, Truyen Tran, and Trang Pham. 2016. A deep language model for software code. arXiv preprint arXiv:1608.02715

(2016).

[43] Marcelo de Rezende Martins and Marco Aurélio Gerosa. 2020. CoNCRA: A Convolutional Neural Network Code Retrieval

Approach. arXiv: Learning (2020).

[44] Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil Blunsom, and Nando de Freitas. 2014. Modelling‚ Visualising and Sum-

marising Documents with a Single Convolutional Neural Network. arXiv: Computation and Language (2014).

[45] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. 2020. Hoppity: Learning graph transformations

to detect and fix bugs in programs. In International Conference on Learning Representations (ICLR).

[46] Steven H.H. Ding, Benjamin C.M. Fung, and Philippe Charland. 2019. Asm2Vec: Boosting static representation robustness for
binary clone search against code obfuscation and compiler optimization. Proceedings - IEEE Symposium on Security and Privacy
2019-May (2019), 472–489. https://doi.org/10.1109/SP.2019.00003

[47] Yue Duan, Xuezixiang Li, Jinghan Wang, and Heng Yin. 2020. Deepbindiff: Learning program-wide code representations for

binary diffing. In Network and Distributed System Security Symposium.

[48] Sen Fang, You-Shuai Tan, Tao Zhang, and Yepang Liu. 2021. Self-Attention Networks for Code Search. Information & Software

Technology 134 (2021), 106542.

[49] Qian Feng, Rundong Zhou, Chengcheng Xu, Yao Cheng, Brian Testa, and Heng Yin. 2016. Scalable graph-based bug search for
firmware images. In Proceedings of the ACM Conference on Computer and Communications Security, Vol. 24-28-Octo. 480–491.
https://doi.org/10.1145/2976749.2978370

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

41

[50] Qian Feng, Rundong Zhou, Chengcheng Xu, Yao Cheng, Brian Testa, and Heng Yin. 2016. Scalable Graph-based Bug Search for

Firmware Images. In Computer and Communications Security.

[51] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang,
and Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. arXiv: Computation and
Language (2020).

[52] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Structured neural summarization. In 7th International
Conference on Learning Representations, ICLR 2019. International Conference on Learning Representations, ICLR. arXiv:1811.01824
https://arxiv.org/abs/1811.01824v4

[53] Jeanne Ferrante, Karl J Ottenstein, and Joe D Warren. 1987. The program dependence graph and its use in optimization. ACM

Transactions on Programming Languages and Systems (TOPLAS) 9, 3 (1987), 319–349.

[54] Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. 1987. The program dependence graph and its use in optimization. ACM
Transactions on Programming Languages and Systems (TOPLAS) 9, 3 (jul 1987), 319–349. https://doi.org/10.1145/24039.24041
[55] Daniel Gibert, Carles Mateu, Jordi Planes, and Ramon Vicens. 2019. Using convolutional neural networks for classification of

malware represented as images. Journal of Computer Virology and Hacking Techniques 15 (2019), 15–28.

[56] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural message passing for
quantum chemistry. In 34th International Conference on Machine Learning, ICML 2017, Vol. 3. 2053–2070. arXiv:1704.01212
[57] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In International Conference on Software Engineering.
[58] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu
Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366 (2020).
[59] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix: Fixing Common C Language Errors by Deep

Learning. In National Conference on Artificial Intelligence.

[60] Masum Hasan, Tanveer Muttaqueen, Abdullah Al Ishtiaq, Kazi Sajeed Mehrab, Md Haque, Mahim Anjum, Tahmid Hasan,
Wasi Uddin Ahmad, Anindya Iqbal, and Rifat Shahriyar. 2021. CoDesc: A Large Code-Description Parallel Dataset. arXiv preprint
arXiv:2105.14220 (2021).

[61] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum Contrast for Unsupervised Visual
Representation Learning. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (2020),
9726–9735. https://doi.org/10.1109/CVPR42600.2020.00975 arXiv:1911.05722

[62] Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. 2019. Global relational models of

source code. In International conference on learning representations.

[63] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.
2015. Teaching machines to read and comprehend. Advances in neural information processing systems 28 (2015), 1693–1701.

[64] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. 2012. On the naturalness of software. In ICSE.
[65] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997), 1735–1780.

https://doi.org/10.1162/neco.1997.9.8.1735

[66] Gang Hu, Min Peng, Yihan Zhang, Qianqian Xie, and Mengting Yuan. 2020. Neural joint attention code search over structure

embeddings for software Q&A sites. Journal of Systems and Software 170 (2020), 110773.

[67] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. In International Conference on Program

Comprehension.

[68] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment generation with hybrid lexical and syntactical

information. Empirical Software Engineering 25 (2020), 2179–2217.

[69] Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, and Nan Duan. 2021. CoSQA: 20,000+ Web

Queries for Code Search and Question Answering. arXiv: Computation and Language (2021).

[70] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge:

Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).

Manuscript submitted to ACM

42

Chen et al.

[71] Yasir Hussain, Zhiqiu Huang, Yu Zhou, and Senzhang Wang. 2020. CodeGRU: Context-aware deep learning with gated recurrent

unit for source code modeling. Information & Software Technology 125 (2020), 106309.

[72] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention
model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
2073–2083.

[73] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping Language to Code in Programmatic

Context. In Empirical Methods in Natural Language Processing.

[74] Xiujuan Ji, Lei Liu, and Jingwen Zhu. 2021. Code Clone Detection with Hierarchical Attentive Graph Embedding. International

Journal of Software Engineering and Knowledge Engineering 31, 06 (2021), 837–861.

[75] Yuede Ji, Lei Cui, and H. Howie Huang. 2021. BugGraph: Differentiating Source-Binary Code Similarity with Graph Triplet-Loss
Network. ASIA CCS 2021 - Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security 1, c (2021),
702–715. https://doi.org/10.1145/3433210.3437533

[76] Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu. 2021. TreeBERT: A Tree-Based Pre-Trained Model for Programming

Language. (may 2021). arXiv:2105.12485 https://arxiv.org/abs/2105.12485v2http://arxiv.org/abs/2105.12485

[77] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A Convolutional Neural Network for Modelling Sentences. In

ACL.

[78] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2019. Learning and Evaluating Contextual Embedding of

Source Code. (dec 2019). arXiv:2001.00059 http://arxiv.org/abs/2001.00059

[79] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020. Learning and Evaluating Contextual Embedding of

Source Code. In International Conference on Machine Learning.

[80] Svetoslav Karaivanov, Veselin Raychev, and Martin Vechev. 2014. Phrase-Based Statistical Translation of Programming Languages.

In SIGPLAN symposium on New ideas, new paradigms, and reflections on programming and software.

[81] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and Andrea Janes. 2020. Big Code != Big Vocabulary:
Open-Vocabulary Models for Source Code. Proceedings - International Conference on Software Engineering (mar 2020), 1073–1085.
https://doi.org/10.1145/3377811.3380342 arXiv:2003.07914v1

[82] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2020. Code Prediction by Feeding Trees to Transformers. arXiv:

Software Engineering (2020).

[83] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code prediction by feeding trees to transformers. Pro-
ceedings - International Conference on Software Engineering 1 (2021), 150–162. https://doi.org/10.1109/ICSE43902.2021.00026
arXiv:2003.13848

[84] Thomas N. Kipf and Max Welling. 2017.

Semi-Supervised Classification with Graph Convolutional Networks.

arXiv:cs.LG/1609.02907

[85] Chris Lattner and Vikram Adve. 2004. LLVM: A compilation framework for lifelong program analysis & transformation. In

International Symposium on Code Generation and Optimization, 2004. CGO 2004. IEEE, 75–86.

[86] Quan Le, Oisín Boydell, Brian Mac Namee, and Mark Scanlon. 2018. Deep learning at the shallow end: Malware classification

for non-domain experts. Digital Investigation 26 (2018).

[87] Triet H.M. Le, Hao Chen, and Muhammad Ali Babar. 2020. Deep Learning for Source Code Modeling and Generation: Models,

Applications, and Challenges. Comput. Surveys 53, 3 (2020), 1–37. https://doi.org/10.1145/3383458 arXiv:2002.05442

[88] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Improved code summarization via a graph neural
network. In IEEE International Conference on Program Comprehension, Vol. 12. IEEE Computer Society, 184–195.
//doi.org/10.1145/3387904.3389268 arXiv:2004.02843

https:

[89] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for generating natural language summaries of

program subroutines. In International Conference on Software Engineering.

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

43

[90] Alexander LeClair and Collin McMillan. 2019. Recommendations for Datasets for Source Code Summarization. In North American

Chapter of the Association for Computational Linguistics.

[91] Bingzhuo Li, Chunyang Ye, Shouyang Guan, and Hui Zhou. 2020. Semantic Code Clone Detection Via Event Embedding Tree
and GAT Network. Proceedings - 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS 2020 3
(2020), 382–393. https://doi.org/10.1109/QRS51102.2020.00057

[92] Hongyu Li, Seohyun Kim, and Satish Chandra. 2019. Neural Code Search Evaluation Dataset. arXiv: Software Engineering

(2019).

[93] Jian Li, Yue Wang, Irwin King, and Michael R. Lyu. 2017. Code Completion with Neural Attention and Pointer Networks. arXiv:

Computation and Language (2017).

[94] Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2017. Code Completion with Neural Attention and Pointer Networks. IJCAI
International Joint Conference on Artificial Intelligence 2018-July (nov 2017), 4159–4165. https://doi.org/10.24963/ijcai.2018/578
arXiv:1711.09573

[95] Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2017. Code Completion with Neural Attention and Pointer Networks. IJCAI
International Joint Conference on Artificial Intelligence 2018-July (nov 2017), 4159–4165. https://doi.org/10.24963/ijcai.2018/578
arXiv:1711.09573v2

[96] Liuqing Li, He Feng, Wenjie Zhuang, Na Meng, and Barbara Ryder. 2017. Cclearner: A deep learning-based clone detection

approach. In 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 249–260.

[97] Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, and Nan

Duan. 2022. CodeRetriever: Unimodal and Bimodal Contrastive Learning. arXiv preprint arXiv:2201.10866 (2022).

[98] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2015. Gated graph sequence neural networks. arXiv preprint

arXiv:1511.05493 (2015).

[99] Yi Li, Shaohua Wang, Tien N. Nguyen, and Son Van Nguyen. 2019. Improving bug detection via context-based code representation
learning and attention-based neural networks. Proceedings of the ACM on Programming Languages 3, OOPSLA (oct 2019), 30.
https://doi.org/10.1145/3360588

[100] Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow. 2016. Gated graph sequence neural networks. 4th International

Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings 1 (2016), 1–20. arXiv:1511.05493

[101] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, and Yuyi Zhong. 2018. VulDeePecker: A Deep

Learning-Based System for Vulnerability Detection. (jan 2018). https://doi.org/10.14722/ndss.2018.23158 arXiv:1801.01681v1
[102] Hongliang Liang, Yue Yu, Lin Jiang, and Zhuosi Xie. [n.d.]. Seml: A semantic LSTM model for software defect prediction. IEEE

Access 7 ([n. d.]), 83812–83824.

[103] Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen, Hui Li, and Rongxin Wu. 2021. Improving Code Summarization
with Block-wise Abstract Syntax Tree Splitting. In IEEE International Conference on Program Comprehension, Vol. 2021-May.
IEEE Computer Society, 184–195. https://doi.org/10.1109/ICPC52881.2021.00026 arXiv:2103.07845

[104] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Meeting of the Association for Computational

Linguistics.

[105] Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D. Ernst. 2018. NL2Bash: A Corpus and Semantic Parser for

Natural Language Interface to the Linux Operating System.. In Language Resources and Evaluation.

[106] Chang Liu, Xinyun Chen, Eui Chul Richard Shin, Mingcheng Chen, and Dawn Song. 2016. Latent Attention For If-Then Program

Synthesis. In NIPS.

[107] Chang Liu, Xin Wang, Richard Shin, Joseph E. Gonzalez, and Dawn Song. 2017. Neural Code Completion.

[108] Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin. 2020. A self-attentional neural architecture for code completion with
multi-task learning. IEEE International Conference on Program Comprehension (2020), 37–47. https://doi.org/10.1145/3387904.
3389261 arXiv:1909.06983

Manuscript submitted to ACM

44

Chen et al.

[109] Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin. 2020. A Self-Attentional Neural Architecture for Code Completion

with Multi-Task Learning. In International Conference on Program Comprehension.

[110] Fang Liu, Lu Zhang, and Zhi Jin. 2020. Modeling programs hierarchically with stack-augmented LSTM. Journal of Systems and

Software 164 (2020), 110547.

[111] Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, and Yang Liu. 2020. Retrieval-Augmented Generation for Code Summarization

via Hybrid GNN. (jun 2020). arXiv:2006.05405 https://arxiv.org/abs/2006.05405v5http://arxiv.org/abs/2006.05405

[112] Xuye Liu, Dakuo Wang, April Wang, Yufang Hou, and Lingfei Wu. 2021. HAConvGNN: Hierarchical Attention Based

Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks.

(mar 2021).

https:

//doi.org/10.18653/v1/2021.findings-emnlp.381 arXiv:2104.01002

[113] Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin Qiu, and Xun Wang. 2021. Combining Graph Neural Networks
with Expert Knowledge for Smart Contract Vulnerability Detection. IEEE Transactions on Knowledge and Data Engineering 01
(jul 2021), 1–1. https://doi.org/10.1109/TKDE.2021.3095196 arXiv:2107.11598

[114] Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin Qiu, and Xun Wang. 2021. Combining Graph Neural Networks
with Expert Knowledge for Smart Contract Vulnerability Detection. IEEE Transactions on Knowledge and Data Engineering
(2021). https://doi.org/10.1109/TKDE.2021.3095196 arXiv:2107.11598

[115] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin

Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel

Sundaresan, Shao Kun Deng, Fu Shengyu, and Shujie Liu. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for Code
Understanding and Generation. arXiv: Software Engineering (2021).

[116] Junayed Mahmud, Fahim Faisal, Raihan Islam Arnob, Antonios Anastasopoulos, and Kevin Moran. 2021. Code to Comment

Translation: A Comparative Study on Model Effectiveness & Errors.

[117] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases

and their compositionality. Advances in neural information processing systems 26 (2013).

[118] Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN: A tree-based convolutional neural network for programming

language processing. arXiv preprint arXiv:1409.5718 (2014).

[119] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional neural networks over tree structures for programming

language processing. In Thirtieth AAAI Conference on Artificial Intelligence.

[120] Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N. Nguyen. 2014. Migrating code with statistical machine translation. In

International Conference on Software Engineering.

[121] Minh Hai Nguyen, Dung Le Nguyen, Xuan Mao Nguyen, and Tho Thanh Quan. 2018. Auto-detection of sophisticated
malware using lazy-binding control flow graph and deep learning. Computers & Security 76 (jul 2018), 128–155. https:
//doi.org/10.1016/J.COSE.2018.02.006

[122] Trong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and Tien N. Nguyen. 2017. Exploring API embedding for API usages

and applications. In International Conference on Software Engineering.

[123] Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen. 2013. A statistical semantic language model

for source code. In FSE.

[124] Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning

to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T). In Automated Software Engineering.

[125] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine

Translation. In Meeting of the Association for Computational Linguistics.

[126] Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. 2021. How could Neural Networks understand

Programs? (2021). arXiv:2105.04297 http://arxiv.org/abs/2105.04297

[127] Reese T. Prosser. 1959. Applications of Boolean Matrices to the Analysis of Flow Diagrams. In Papers Presented at the December
1-3, 1959, Eastern Joint IRE-AIEE-ACM Computer Conference (Boston, Massachusetts) (IRE-AIEE-ACM ’59 (Eastern)). Association

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

45

for Computing Machinery, New York, NY, USA, 133–138. https://doi.org/10.1145/1460299.1460314

[128] Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir

Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and

Frederick Reiss. 2021. CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks.

[129] Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract Syntax Networks for Code Generation and Semantic Parsing.

In Meeting of the Association for Computational Linguistics.

[130] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.

2019. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv: Learning (2019).

[131] Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic model for code with decision trees. ACM SIGPLAN Notices

51, 10 (2016), 731–747. https://doi.org/10.1145/2983990.2984041

[132] RaychevVeselin, VechevMartin, and YahavEran. 2014. Code completion with statistical language models. Sigplan Notices (2014).
[133] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai

Ma. 2020. CodeBLEU: a Method for Automatic Evaluation of Code Synthesis. arXiv: Software Engineering (2020).

[134] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by back-propagating errors.

nature 323, 6088 (1986), 533–536.

[135] Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A Neural Attention Model for Abstractive Sentence Summarization.

In EMNLP.

[136] Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish Chandra. 2018. Retrieval on source code: a

neural code search.

[137] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. NAACL HLT 2018
- 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -
Proceedings of the Conference 2 (2018), 464–468. https://doi.org/10.18653/v1/n18-2074 arXiv:1803.02155

[138] Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun. 2021. CAST: Enhancing

Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees. (aug 2021). arXiv:2108.12987

http://arxiv.org/abs/2108.12987

[139] Ke Shi, Yang Lu, Jingfei Chang, and Zhen Wei. 2020. PathPair2Vec: An AST path pair-based code representation method for

defect prediction.

[140] Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi Miyamoto, and Tadayuki Matsumura. 2019. Automatic Source
Code Summarization with Extended Tree-LSTM. In Proceedings of the International Joint Conference on Neural Networks, Vol. 2019-
July. Institute of Electrical and Electronics Engineers Inc. https://doi.org/10.1109/IJCNN.2019.8851751 arXiv:1906.08094

[141] Piyush Shrivastava. 2021. Neural Code Summarization. arXiv: Software Engineering (2021).
[142] Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and Yan Lei. 2020. Improving Code Search with Co-Attentive Represen-

tation Learning. In International Conference on Program Comprehension.

[143] Yulei Sui, Xiao Cheng, Guanqin Zhang, and Haoyu Wang. 2020. Flow2Vec: value-flow-based precise code embedding. Proceedings

of the ACM on Programming Languages 4, OOPSLA (nov 2020), 27. https://doi.org/10.1145/3428301

[144] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural

information processing systems. 3104–3112.

[145] Alexey Svyatkovskiy, Shao Kun Deng, Fu Shengyu, and Neel Sundaresan. 2020. IntelliCode compose: code generation using

transformer. In FSE.

[146] Alexey Svyatkovskiy, Ying Zhao, Fu Shengyu, and Neel Sundaresan. 2019. Pythia: AI-assisted Code Completion System. arXiv:

Software Engineering (2019).

[147] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured
long short-Term memory networks. In ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,

Manuscript submitted to ACM

46

Chen et al.

Proceedings of the Conference, Vol. 1. Association for Computational Linguistics (ACL), 1556–1566. https://doi.org/10.3115/v1/p15-
1150 arXiv:1503.00075

[148] Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre Antoine Manzagol, Charles Sutton, and Edward Aftandilian.
2020. Learning to Fix Build Errors with Graph2Diff Neural Networks. In Proceedings - 2020 IEEE/ACM 42nd International
Conference on Software Engineering Workshops, ICSEW 2020. Association for Computing Machinery, Inc, 19–20. https://doi.org/
10.1145/3387940.3392181 arXiv:1911.01205

[149] Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre Antoine Manzagol, Charles Sutton, and Edward Aftandilian.
2020. Learning to Fix Build Errors with Graph2Diff Neural Networks. In Proceedings - 2020 IEEE/ACM 42nd International
Conference on Software Engineering Workshops, ICSEW 2020. 19–20. https://doi.org/10.1145/3387940.3392181 arXiv:1911.01205

[150] Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. 2014. On the localness of software. In FSE.
[151] Daniel Z ¨ Ugner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan G ¨ Unnemann. [n.d.]. Language-agnostic

Representation Learning of Source Code from Structure and Context. ([n. d.]). arXiv:2103.11318v1 www.daml.in.tum.de/code-

transformer,

[152] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. 2019. Neural Program Repair by Jointly Learning

to Localize and Repair. arXiv: Learning (2019).

[153] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

2017. Attention is all you need. In Advances in neural information processing systems. 5998–6008.

[154] Petar Veličković, Arantxa Casanova, Pietro Liò, Guillem Cucurull, Adriana Romero, and Yoshua Bengio. 2018. Graph attention
networks. In 6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings. International
Conference on Learning Representations, ICLR. arXiv:1710.10903 https://arxiv.org/abs/1710.10903v3

[155] Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and Philip Yu. 2019. Multi-modal attention network
learning for semantic source code retrieval. In Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2019. Institute of Electrical and Electronics Engineers Inc., 13–25. https://doi.org/10.1109/ASE.2019.00012
arXiv:1909.13516

[156] Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and Philip S Yu. 2019. Multi-modal attention network

learning for semantic source code retrieval. arXiv preprint arXiv:1909.13516 (2019).

[157] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Improving automatic source
code summarization via deep reinforcement learning. In ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering. Association for Computing Machinery, Inc, 397–407. https://doi.org/10.1145/3238147.3238206
arXiv:1811.07234

[158] D. Wang, Z. Jia, S. Li, Y. Yu, Y. Xiong, W. Dong, and X. Liao. 2021. Bridging Pre-trained Models and Downstream Tasks for

Source Code Understanding. (2021).

[159] Ke Wang. 2019. Learning Scalable and Precise Representation of Program Semantics. (may 2019). arXiv:1905.05251 https:

//arxiv.org/abs/1905.05251v3http://arxiv.org/abs/1905.05251

[160] Song Wang, Taiyue Liu, Jaechang Nam, and Lin Tan. 2020. Deep Semantic Feature Learning for Software Defect Prediction.

IEEE Transactions on Software Engineering 46 (2020), 1267–1293.

[161] Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning semantic features for defect prediction. In International

Conference on Software Engineering.

[162] Wenhan Wang, Sijie Shen, Ge Li, and Zhi Jin. 2021. Towards Full-line Code Completion with Neural Language Models. (2021).

arXiv:2009.08603v1 www.aaai.org

[163] Wenhan Wang, Kechi Zhang, Ge Li, and Zhi Jin. 2020. Learning to Represent Programs with Heterogeneous Graphs. (2020),

1–10. arXiv:2012.04188 http://arxiv.org/abs/2012.04188

[164] Yanlin Wang, Shi Han, Dongmei Zhang, Ensheng Shi, Lun Du, Xiaodi Yang, Yuxuan Hu, Shi Han, and Hongyu Zhang. 2021.
CoCoSum: Contextual Code Summarization with Multi-Relational Graph Neural Network. J. ACM 1, 1 (jul 2021), 24. https:

Manuscript submitted to ACM

A Survey of Deep Learning Models for Structural Code Understanding

47

//doi.org/10.1145/nnnnnnn.nnnnnnn arXiv:2107.01933

[165] Yanlin Wang and Hui Li. 2021. Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs. (2021). arXiv:2103.09499

http://arxiv.org/abs/2103.09499

[166] Yu Wang, Ke Wang, Fengjuan Gao, and Linzhang Wang. 2020. Learning semantic program embeddings with graph interval
neural network. Proceedings of the ACM on Programming Languages 4, OOPSLA (may 2020). https://doi.org/10.1145/3428205
arXiv:2005.09997

[167] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder

Models for Code Understanding and Generation. arXiv: Computation and Language (2021).

[168] Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2019. Retrieve and refine: exemplar-based neural comment generation. In

Automated Software Engineering.

[169] Hui Hui Wei and Ming Li. 2017. Supervised deep features for Software functional clone detection by exploiting lexical and
syntactical information in source code. In IJCAI International Joint Conference on Artificial Intelligence. 3034–3040. https:
//doi.org/10.24963/ijcai.2017/423

[170] Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk. 2016. Deep learning code fragments for code
clone detection. In 2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 87–98.
[171] Martin White, Christopher Vendome, Mario Linares-Vasquez, and Denys Poshyvanyk. 2015. Toward deep learning software

repositories. In Mining Software Repositories.

[172] Liwei Wu, Fei Li, Youhua Wu, and Tao Zheng. 2020. GGF: A graph-based method for programming language syntax error
correction. IEEE International Conference on Program Comprehension (2020), 139–148. https://doi.org/10.1145/3387904.3389252
[173] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. 2019. A Comprehensive Survey

on Graph Neural Networks.

IEEE Transactions on Neural Networks and Learning Systems 32, 1 (jan 2019), 4–24.

https:

//doi.org/10.1109/TNNLS.2020.2978386 arXiv:1901.00596v4

[174] Yan Xiao, Jacky Keung, Qing Mi, and Kwabena Ebo Bennin. 2017. Improving Bug Localization with an Enhanced Convolutional

Neural Network. In Asia-Pacific Software Engineering Conference.

[175] Binbin Xie, Jinsong Su, Yubin Ge, Xiang Li, Jianwei Cui, Junfeng Yao, and Bin Wang. 2021. Improving Tree-Structured Decoder

Training for Code Generation via Mutual Learning. (2021). arXiv:2105.14796v1 www.aaai.org

[176] Ling Xu, Huanhuan Yang, Chao Liu, Jianhang Shuai, Meng Yan, Yan Lei, and Zhou Xu. 2021. Two-Stage Attention-Based Model
for Code Search with Textual and Structural Features. In IEEE International Conference on Software Analysis, Evolution, and
Reengineering.

[177] Sihan Xu, Sen Zhang, Weijing Wang, Xinya Cao, Chenkai Guo, and Jing Xu. 2019. Method name suggestion with hierarchical

attention networks.

[178] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song. 2017. Neural network-based graph embedding for
cross-platform binary code similarity detection. In Proceedings of the ACM Conference on Computer and Communications Security.
Association for Computing Machinery, 363–376. https://doi.org/10.1145/3133956.3134018 arXiv:1708.06525

[179] Jiang Xue, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu. 2021. TreeBERT: A Tree-Based Pre-Trained Model for Programming

Language. In Uncertainty in Artificial Intelligence.

[180] Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad Rieck. 2014. Modeling and discovering vulnerabilities with code property

graphs. In Proceedings - IEEE Symposium on Security and Privacy. 590–604. https://doi.org/10.1109/SP.2014.44

[181] Jiaqi Yan, Guanhua Yan, and Dong Jin. 2019. Classifying Malware Represented as Control Flow Graphs using Deep Graph
Convolutional Neural Network. In Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable Systems and
Networks, DSN 2019. Institute of Electrical and Electronics Engineers Inc., 52–63. https://doi.org/10.1109/DSN.2019.00020
[182] Hao Yang and Li Kuang. [n.d.]. CCMC: Code Completion with a Memory Mechanism and a Copy Mechanism; CCMC: Code

Completion with a Memory Mechanism and a Copy Mechanism. ([n. d.]). https://doi.org/10.1145/3463274.3463332

Manuscript submitted to ACM

48

Chen et al.

[183] Kang Yang, Huiqun Yu, Guisheng Fan, and Xingguang Yang. 2020. Graph embedding code prediction model integrating semantic

features. Computer Science and Information Systems 17, 3 (2020), 907–926. https://doi.org/10.2298/CSIS190908027Y

[184] Ke Yang, MingXing Zhang, Kang Chen, Xiaosong Ma, Yang Bai, and Yong Jiang. 2019. KnightKing: a fast distributed graph

random walk engine. In Proceedings of the 27th ACM Symposium on Operating Systems Principles. 524–537.

[185] Michihiro Yasunaga and Percy Liang. 2020. Graph-based, Self-Supervised Program Repair from Diagnostic Feedback. (2020).

[186] Michihiro Yasunaga and Percy Liang. 2020. Graph-based, Self-Supervised Program Repair from Diagnostic Feedback. In

International Conference on Machine Learning.

[187] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned code and
natural language pairs from stack overflow. In Proceedings - International Conference on Software Engineering. IEEE Computer
Society, 476–486. https://doi.org/10.1145/3196398.3196408 arXiv:1805.08949

[188] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned code and

natural language pairs from stack overflow. In Mining Software Repositories.

[189] Pengcheng Yin and Graham Neubig. 2018. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and

Code Generation. In Empirical Methods in Natural Language Processing.

[190] Zeping Yu, Rui Cao, Qiyi Tang, Sen Nie, Junzhou Huang, and Shi Wu. 2020. Order matters: Semantic-aware neural networks for
binary code similarity detection. In AAAI 2020 - 34th AAAI Conference on Artificial Intelligence, Vol. 34. AAAI press, 1145–1152.
https://doi.org/10.1609/aaai.v34i01.5466

[191] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020. Retrieval-based neural source code summarization.
Proceedings - International Conference on Software Engineering (2020), 1385–1397. https://doi.org/10.1145/3377811.3380383
[192] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020. Retrieval-based neural source code summarization.

In International Conference on Software Engineering.

[193] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. 2019. A Novel Neural Source Code
Representation Based on Abstract Syntax Tree. Proceedings - International Conference on Software Engineering 2019-May (2019),
783–794. https://doi.org/10.1109/ICSE.2019.00086

[194] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. 2019. A Novel Neural Source Code
Representation Based on Abstract Syntax Tree. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE).
783–794. https://doi.org/10.1109/ICSE.2019.00086

[195] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. An end-to-end deep learning architecture for graph

classification. In Thirty-second AAAI conference on artificial intelligence.

[196] Gang Zhao and Jeff Huang. 2018. DeepSim: Deep Learning Code Functional Similarity. (2018). https://doi.org/10.1145/3236024.

3236068

[197] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective Vulnerability Identification by
Learning Comprehensive Program Semantics via Graph Neural Networks. Advances in Neural Information Processing Systems 32
(sep 2019). arXiv:1909.03496 https://arxiv.org/abs/1909.03496v1

[198] Xiaowei Zhu, Wenguang Chen, Weimin Zheng, and Xiaosong Ma. 2016. Gemini: A computation-centric distributed graph
processing system. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16). 301–316.
[199] Fei Zuo, Xiaopeng Li, Patrick Young, Lannan Luo, Qiang Zeng, and Zhexin Zhang. 2019. Neural Machine Translation In-

spired Binary Code Similarity Comparison beyond Function Pairs. Internet Society. https://doi.org/10.14722/ndss.2019.23492

arXiv:1808.04706

Manuscript submitted to ACM

