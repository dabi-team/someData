2
2
0
2

p
e
S
8

]

R
C
.
s
c
[

1
v
0
1
7
3
0
.
9
0
2
2
:
v
i
X
r
a

Strong Optimistic Solving for
Dynamic Symbolic Execution

Darya Parygina∗†, Alexey Vishnyakov∗ and Andrey Fedotov∗
∗Ivannikov Institute for System Programming of the RAS
†Lomonosov Moscow State University
Moscow, Russia
{pa darochek, vishnya, fedotoff}@ispras.ru

Abstract—Dynamic symbolic execution (DSE) is an effective
method for automated program testing and bug detection. It is
increasing the code coverage by the complex branches exploration
during hybrid fuzzing. DSE tools invert the branches along some
execution path and help fuzzer examine previously unavailable
program parts. DSE often faces over- and underconstraint
problems. The ﬁrst one leads to signiﬁcant analysis complication
while the second one causes inaccurate symbolic execution.

We propose strong optimistic solving method that eliminates
irrelevant path predicate constraints for target branch inversion.
We eliminate such symbolic constraints that the target branch
is not control dependent on. Moreover, we separately handle
symbolic branches that have nested control transfer instructions
that pass control beyond the parent branch scope, e.g. return,
goto, break, etc. We implement the proposed method in our
dynamic symbolic execution tool Sydr.

We evaluate the strong optimistic strategy, the optimistic
strategy that contains only the last constraint negation, and their
combination. The results show that the strategies combination
helps increase either the code coverage or the average number of
correctly inverted branches per one minute. It is optimal to apply
both strategies together in contrast with other conﬁgurations.

Index Terms—DSE, symbolic execution, dynamic analysis,
binary analysis, path predicate, overconstraint, underconstraint,
SMT, hybrid fuzzing, computer security, security development
lifecycle, SDL

I. INTRODUCTION

Modern software developers invest a huge amount of efforts
in increasing the programs quality. Companies employ security
development
lifecycle (SDL) [1–3] to ﬁnd bugs in their
products and defend programs from dangerous interventions.
Various automated testing tools provide thorough code explo-
ration. Hybrid fuzzing [4] is the state-of-the-art solution for
ﬁnding bugs. Its efﬁciency comes from fast and lightweight
fuzzing [5, 6], that allows to discover new paths quickly,
and more accurate dynamic symbolic execution [7–12], that
helps reaching difﬁcult program parts by inverting compli-
cated branches along some execution path. Moreover, dynamic

This work was supported by RFBR grant 20-07-00921 A.

symbolic execution (on initially valid paths) enables new
seeds generation that trigger memory and undeﬁned behavior
errors [13].

Dynamic symbolic execution [14, 15] tools execute the
target program and construct the path predicate by symbolic
interpretation of program instructions. The path predicate
contains the branch conditions met during analysis. Symbolic
engines try to invert each branch from the path predicate to
discover new execution paths that are hardly reached with
fuzzing. The predicate for the branch inversion conjuncts
all
the preceding branch constraints (i.e. constraints from
branches executed before the target branch) and the negation of
the target branch constraint. The majority of symbolic engines
frequently face over- and underconstraint problems (similar
to taint analysis [16]) that prevent them from exploring more
program paths. Overconstraint denotes the situation when there
are many redundant constraints in the path predicate that may
cause its complication or even unsatisﬁability. Overconstraint
is increasing the symbolized instructions number during analy-
sis. Underconstraint, on the contrary, means that some variable
is not treated as symbolic, though it should be. Non-trivial
branch conditions or symbolic pointers (that depend on user
data) may cause underconstraint.

Symbolic executors apply various techniques to cope with
these problems (Section II). We propose the strong optimistic
solving method that generates input data for branch inver-
sion even when the constructed predicate is unsatisﬁable.
We eliminate some constraints from the solver query based
on the branches nesting. Thus, we can bypass over- and
underconstraint.

As an illustration, consider the code example below:

if (buf[3] == ’6’ & buf[0] == ’5’) //c_2

else

printf("Success!\n");

1 void func(const char* buf) {
2
3
4
5
6 }
7
8 int main() {

printf("Fail\n");

© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.

 
 
 
 
 
 
char buf[4];
read(0, buf, 4);
if (buf[2] < ’0’)

//c_11

printf("Eliminated by slicing\n");

if (buf[0] == ’3’)

printf("Independent branch\n");

if (buf[1] - buf[3] == 1)

//c_13

//c_15

func(buf);

9
10
11
12
13
14
15
16
17 }

Listing 1. Strong optimistic solving.

Suppose the execution got to line 5 and the branch con-
ditions in lines 11, 13, and 15 appeared to be true, while
the branch constraint in line 2 turned out to be false. Our
goal is inverting the branch in func and reaching line 3.
the corre-
As we want
sponding predicate will have the following form: (buf[2] <
’0’) ∧ (buf[0] = ’3’) ∧ (buf[1] − buf[3] =
1) ∧ (buf[3] = ’6’ ∧ buf[0] = ’5’).

if-statement in line 2 to be true,

This predicate is unsatisﬁable, since buf[0] cannot be
equal to ’3’ and ’5’ simultaneously. The path predicate
is overconstrained, as the branches in lines 11 and 13 do
not actually affect inversion of the branch in line 2. Many
real-world programs face similar situations, and the number
of irrelevant constraints in the path predicates grows along
with the code expansion.

This paper makes the following contributions:

• We propose strong optimistic solving that allows to
discover new program paths via dynamic symbolic execu-
tion. We propose a new strategy for eliminating irrelevant
symbolic constraints from the path predicate, based on
program call stack and branch control dependency anal-
ysis. We propose an improvement to this strategy that
avoids eliminating constraints which may potentially af-
fect the program control ﬂow. We leave the corresponding
symbolic branches in the path predicate, thus increasing
the chance that the generated test case will actually invert
the target branch.

• We evaluate the method on the set of real-world appli-
cations [17], measuring the analysis efﬁciency (i.e. cor-
rectly inverted branches number, accuracy, and speed) and
the code coverage. We also evaluate optimistic solving
strategy [8] and the methods combination. We compare
the results for different conﬁgurations. We show that the
methods combination helps to either increase the code
coverage by inverting more symbolic branches along the
single execution path, or improve symbolic execution
efﬁciency.

The rest of this paper is organized as follows. Section II
contains the overview of techniques, applied in symbolic
executors to cope with over- and underconstraint problems.
Section III presents the strong optimistic solving strategy.
Implementation details are discussed in section IV. The exper-
imental evaluation is shown in Section V. Finally, Section VI
concludes this paper.

II. RELATED WORK

Overconstraint problem leads to signiﬁcant difﬁculties dur-
ing symbolic execution. Excessive constraints complicate the
path predicate and, in some cases, make it unsatisﬁable. Un-
derconstraint is another important obstacle while constructing
the path predicate. The common reason for underconstraint is
related to the symbolic addresses problem, i.e. when load or
store address depends on user input. In this section we sys-
tematically analyze the variety of methods, used in symbolic
engines to cope with over- and underconstraint.

A. Irrelevant Constraint Elimination

that

Symbolic variables and symbolic branches abundance fre-
quently results in great number of redundant constraints in
the path predicate. Irrelevant constraint elimination methods
intend to remove excessive constraints from the solver queries.
In KLEE [18, 19] the constraint independence optimization
exploits the fact
the set of constraints can be often
divided into several subsets of independent constraints. Two
constraints are independent when they are affected by disjoint
sets of symbolic variables. Building independent constraints
subsets is performed by constructing the special graph. Each
pair of connected graph nodes illustrates symbolic variables
that are included in the same symbolic constraint. Each
independent constraints subset is built by adding constraints,
containing symbolic variables from the certain connected com-
ponent of the graph. Such optimization beneﬁts from reducing
solver load by discarding irrelevant constraints when querying
for the certain constraint satisﬁability. Even in the worst case,
when no constraints can be eliminated, the query cost is the
same as without optimization applied (the cost of computing
the independent subsets is small and can be omitted).

The path predicate slicing [9], proposed in our previous
work and implemented in Sydr, is also used for reducing
solver load when inverting the target branch. Sydr eliminates
irrelevant path predicate conjuncts from the queries. We con-
sider some constraint irrelevant if its symbolic variables do
not directly or transitively depend on symbolic variables from
the target branch constraint. Symbolic variable var1 directly
depends on variable var2 if there is some constraint in the
path predicate that contains these variables simultaneously.
Similarly, variable var1 transitively depends on variable var2
if var1 depends directly on some other variable that, in its
turn, depends directly or transitively on var2.

At

the ﬁrst step, Sydr determines the set of symbolic
variables that directly or transitively depend on the target
branch constraint variables. This set contains all the symbolic
data that are responsible for the target branch inversion. After
the new sliced predicate is formed from only those
that,
constraints that contain the selected variables. Thus, solver-
generated model will contain only a subset of all symbolic
data. The values for the missing variables are retrieved from
the initial input data as they are already the solution for the
path predicate. Applying slicing leads to a more powerful
symbolic execution with less time and memory consumption
during queries solving.

B. Optimistic Solving

The hybrid fuzzer QSYM [8] suggests the heuristic method
called optimistic solving. If the whole path predicate is not
satisﬁable, QSYM selects just the last constraint and tries to
get its solution. The last constraint often has a simple form that
facilitates its solving. Additionally, test cases, generated by the
optimistic solving, have a good chance to actually invert the
target branch, since they satisfy at least the last (target branch)
constraint. This heuristic allows QSYM to explore new code
quite efﬁciently due to the fact that the emulation overhead
dominate the overhead for constraint solving in the real-world
applications. Optimistic solving provides much more correct
test cases for inverting symbolic branches at the same time.

C. Function Semantics Modeling

During native symbolic execution, all instructions are instru-
mented and all symbolic branch constraints are added to the
path predicate. Nevertheless, such strategy often leads to the
great time costs for symbolic execution of instrumented code
and signiﬁcant path predicate expansion, especially while pro-
cessing library calls. In order to manage these problems, sym-
bolic engines apply various techniques allowing to decrease
the number of redundant constraints in the path predicate and
reduce the amount of symbolic instructions.

The library code is an essential part of real-world applica-
tions. But at the same time, it is a reason for the great increase
in the symbolic instructions number. The function arguments
symbolization makes the statements inside the function imple-
mentation also symbolized if they are affected by these argu-
ments. The key intuition of the function semantics modeling
methods is that there is no need to symbolically execute every
internal instruction in library functions with standard speciﬁed
semantics. Such approach beneﬁts not only from speeding up
symbolic execution, but also from diminishing symbolic state
overconstrainting. There are ﬁve main directions in function
semantics modeling.
The ﬁrst one,

implemented in KLEE [19], proposes to
replace libc functions with their simpler precompiled LLVM
IR versions. Such approach simpliﬁes the code and avoids
extra components compilation. However, it does not consider
complete function semantics.

The second method, applied in SymCC [10], adds additional
constraints to the path predicate instead of symbolically exe-
cuting the function code. These constraints model the concrete
execution trace.

Another approach is modeling each function with a single
branch that represents different function results. For instance,
strcmp strings may be equal or not. Such method is one of
the three function semantics modeling strategies implemented
in FUZZOLIC [12].

The fourth strategy focuses on construction of symbolically
computed expression for the return value. Under this approach,
in S2E [20], the special wrappers insert the symbolic formulas
in the return values. The similar method is applied in Sydr [9]
where the construction of symbolic formulas is performed for
the set of complicated functions that deal with comparisons

and string to integer conversions; and in angr [21] where some
functions are replaced by the simpliﬁed implementations while
others are fully modeled according to their semantics.

The last but not least method suggests simply skipping
functions that often contain symbolized arguments but do not
affect the branches inversion, e.g. heap operations (malloc,
calloc, realloc, free, etc.), printing functions like
printf. Such functions append lots of inconsequential con-
straints, thus can be skipped without the soundness loss.

D. Index-Based Memory Model

There are two common approaches for handling symbolic
pointers: concretizing symbolic indexes and using a fully sym-
bolic memory model. The ﬁrst one leads to the underconstraint
problem while the second one causes overconstraint and high
performance costs.

Mayhem [22] proposes an index-based memory model as
an approach to handling symbolic addresses according to the
memory index value. Due to the performance issues, Mayhem
uses not the full symbolic memory model, but the partial one
where write addresses are always concretized and reads are
treated as symbolic. The similar approach is implemented in
Sydr [23].

For symbolic reads modeling, Mayhem introduces memory
objects that map indexes to symbolic expressions. Each mem-
ory object bounds are determined via solver with a binary
search. Mayhem applies some optimizations to lighten the bur-
den on the solver during this process. Value set analysis [24]
allows to reduce the set of appropriate values in the memory
object index interval by replacing it with a strided interval.
Each strided interval is then reﬁned using solver queries.
Moreover, Mayhem uses balanced index search trees (IST),
i.e. binary search trees with the index as a key and the memory
object entries as leaf nodes. As an effective approach to
reducing the IST nodes number, Mayhem applies bucketization
with linear functions which key idea is combining multiple
memory object entries into a single index-parametrized leaf:
k ∗ i + b.

E. Basic Block Pruning

The problem of adding constraints, repetitively generated
by the same code, to the path predicate is also crucial for
symbolic execution. It is especially relevant when processing
loops and function calls. Such constraints prevent new code
exploration and can even make solver queries unsatisﬁable
due to their computational complexity. QSYM [8] applies the
method called basic block pruning to cope with this issue. Its
essence is detecting the repetitive basic blocks and pruning
them from symbolic execution. The execution frequency of
each basic block is measured at runtime (each time the block
is executed, its frequency counter is incremented), and QSYM
stops generating further constraints from basic blocks that have
been executed too frequently. Exponential back-off is used to
prune rapid blocks (the blocks whose frequency number is
a power of 2 are the only executed blocks) while heuristic
approaches are applied to avoid excessive pruning. The ﬁrst

heuristic is grouping multiple executions. Thus, several ex-
ecutions make a single increment to the frequency counter.
And the second one is context-sensitivity that distinguishes
frequency counters for the same basic block in different call
contexts.

F. Taint-Assisted Partial Symbolization

As a way to cope with the overconstraint problem, Lean-
Sym [25] proposes taint-assisted partial symbolization. This
approach has similar purposes as the path predicate slicing,
yet exploits the fact that concolic execution involves a concrete
input and uses it to compute the data ﬂow.

LeanSym applies dynamic taint analysis (DTA) to identify
input bytes affecting different symbolic branches. At the be-
ginning, all the input data are tainted and the direct taint prop-
agation is performed. As a result, input bytes affecting each
branch cmp operands are determined. On the next analysis
stage, LeanSym begins inverting symbolic branches along the
execution path. It selects the target branch and symbolizes only
those bytes that affect its cmp operands. These bytes offsets
are taken from the previous DTA run results. Finally, LeanSym
symbolically executes only the statements that operate on the
relevant symbolic data. The path predicate is updated with the
constraints that are necessary for the target branch inversion.
Values for the symbolized input bytes are taken from the
solver-generated model while the rest values are retrieved from
the initial input.

Taint-assisted partial symbolization provides a good speed
increase of symbolic execution and queries solving. But at the
same time, it may lead to enlarging the number of unsatisﬁable
queries due to non-symbolized bytes concretization, leading to
the underconstraint.

III. STRONG OPTIMISTIC STRATEGY

New paths exploration is one of the main symbolic ex-
ecution purposes. A good strategy for increasing the code
coverage is inverting symbolic branches along some execution
trace. But due to the problems of over- and underconstraint,
the solver queries for inverting symbolic branches in a large
number of cases turn out to be unsatisﬁable. The vast majority
of symbolic executors make no attempts to reuse already
collected symbolic constraints for inverting the target branch
or use them in a very simple form. For instance, optimistic
solutions in QSYM [8] do not
tend to satisfy symbolic
constraints needed for reaching the target point in many real-
world applications.

Path predicate slicing, implemented in Sydr [9], is a quite
effective method of reusing collected symbolic constraints to
explore new code. It proposes partial constraint selection based
on data dependencies on the target branch. But despite an in-
tention to minimize the path predicate and save its soundness,
slicing still can provide unsatisﬁable predicates. However,
some constraints can still be removed without accuracy loss.
Hence, as a continuation of the path predicate slicing, we
introduce the strong optimistic strategy that allows to explore
new code by removing excessive symbolic constraints from

the sliced path predicate in attempt to invert the target branch,
thus exploring previously non-invertible symbolic branches.

In our work we base on binary code analysis for x86
architecture. Thus, we handle programs written in compiled
languages such as C, C++, Rust, Go, etc. We analyze the
unmodiﬁed binary code and ignore the program source code.
The proposed method is independent of most compiler op-
timizations, except for gcc with O1 optimization level, as
in this case the if-statement is compiled in unusual way.
For example, consider the binary code generated with gcc
O0/O2/O3 (Listing 2) and O1 (Listing 3) for the fragment
if (x > 0) printf(...);. In Listing 2 printf call
goes right after the conditional branch. However, in Listing 3
the printf call is located in separate basic block that jumps
back to the instruction following the conditional branch.

724:
727:
729:
72b:
732:
737:

71a:
71f:
721:
...
73a:
73b:
742:
747:

mov
test
jle
lea
call
...

cmp
jg
...

ret
lea
call
jmp

eax,DWORD PTR [rbp-0xc]
eax,eax
737
rdi,[rip+0xc5]
5d0 <puts@plt>

Listing 2. gcc -O0/O2/O3 code.

DWORD PTR [rsp+0x4],0x0
73b

rdi,[rip+0xb5]
5d0 <puts@plt>
721

Listing 3. gcc -O1 code.

The key intuition of the strong optimistic strategy is that
symbolic branches, which the target branch is not control
dependent on, do not affect this branch inversion, as they
can be executed independently of it. Each program branch
has source (src addr) and destination (dst addr) addresses,
where the ﬁrst one is the corresponding jcc-instruction ad-
dress and the second one is the jump destination address.
We consider that the ﬁrst branch is control dependent on
the second (or the ﬁrst branch is nested in the second) if
src addr2 < src addr1 < dst addr2. For example, if we
have a symbolic branch, such that its destination address is
this branch is
less than the target branch source address,
independent of the target one. Furthermore, when there is a
function call before the target branch and there are symbolic
branches inside this function, such branches have no impact
on the target branch inversion. So, if we remove symbolic
branches, that the target branch is not control dependent on,
from the sliced predicate, we may have a better chance of
getting a satisﬁable predicate.

Consider the example in Listing 1. The whole path predi-
cate, that contains symbolic constraints from lines 11, 13, 15,
and 2, is unsatisﬁable, as described above. Applying path pred-
icate slicing is not enough in this case: constraints in lines 13,
15, and 2 depend directly on symbolic variables from the target
constraint (buf[3] and buf[0]). So, the sliced predicate
still contains conﬂicting symbolic constraints from lines 2 and
13. On the other hand, the optimistic strategy is not enough

too, since the constraint in line 15 can occur to be false and the
target branch will not be reached. But if we notice that the ﬁrst
constraint in the sliced path predicate (in line 13) corresponds
to the independent branch and remove it from the predicate,
it becomes satisﬁable. The destination address of branch in
line 13 is line 15. Thus, neither branch in line 15, nor branch
in line 2 is control dependent on branch in line 13, so, branch
in line 13 can be eliminated without any consequences for the
target branch inversion. Hence, the only correct solution for the
symbolic data can be found with the strong optimistic strategy:
buf[0] = ’5’, buf[1] = ’7’, buf[3] = ’6’.

We propose the strong optimistic solving for more accurate
symbolic branches inversion and the code coverage increase.
The method ﬂowchart is shown in Figure 1. If the original
path predicate for target branch inversion is satisﬁable, then
no strategy is applied in order to avoid resources waste.
Otherwise, we try to solve only the last constraint, i.e. use
the optimistic strategy. If it appears to be SAT, we apply the
strong optimistic strategy and try to get a strong optimistic
solution. If the optimistic predicate is unsatisﬁable, it means
that it makes no sense to even try applying another strategies,
because the target constraint will always make the modiﬁed
predicate unsatisﬁable.

Sliced
predicate
SAT?

no

Optimistic
predicate
SAT?

yes

yes

Save test case

Strong
optimistic
matches
optimistic?

no

Strong
optimistic
predicate
SAT?

no

Save optimistic

yes

Save optimistic

yes

Save optimistic,
strong optimistic

Fig. 1. Strong optimistic strategy ﬂowchart.

According to the ﬂowchart in Figure 1, we should construct
the strong optimistic predicate only if the sliced predicate is
UNSAT and the optimistic predicate is SAT. So, the goal is
to get something in between these variants, such that it is
more likely to be SAT unlike the ﬁrst one and more likely
to be correct unlike the second one. The key intuition is
that we should eliminate such symbolic branches that the
target branch is not control dependent on. In other words,
if some branch is related to the src_addr → dst_addr
the target branch is control de-
jump,
pendent on it, is equivalent to the condition src_addr ≤
target_branch_addr < dst_addr. It is worth noting
that sometimes strong optimistic predicate may be the same
as optimistic predicate, then we should not query solver with
the same predicate twice.

then the fact,

that

The Algorithm 1 shows how the strong optimistic predicate
is constructed. We analyze the program call stack to deter-

mine the symbolic branches from the sliced path predicate
that the target branch is control dependent on. The global
call_stack structure, passed as input to the algorithm,
contains the mapping from each branch constraint
in the
original predicate Π to the program call stack at this branch
source address. The program call stack in its turn holds call
sites for the functions called before the certain program point.

Algorithm 1 Strong optimistic predicate construction.
Input: call stack – program call stack, point – target branch
address, cbr – target branch constraint, Π – sliced path predicate
for inverting target branch.
Output: Πsopt – strong optimistic predicate for inverting target
branch.

Πsopt ← cbr
cs ← call stack(point)
for all c ∈ reversed(Π) do

⊲ strong optimistic predicate

if call stack(c) is not a preﬁx of cs then

continue

if call stack(c) is preﬁx of cs and call stack(c) 6= cs then

point ← cs[size(call stack(c))]
cs ← call stack(c)

if src(c) ≤ point < dst(c) or ret / far jmp / far jcc

inside (src(c), dst(c)) then

Πsopt ← Πsopt ∧ c

return Πsopt

Before the beginning of the algorithm, we choose the target
branch with the target constraint cbr and apply the path
predicate slicing. At the ﬁrst step, we initialize the resulting
predicate Πsopt with the target constraint cbr and save the
target branch call stack in cs. Then we start iterating over
the sliced predicate constraints in the reversed order. This
way, we move from near to far constraints in the direction of
the program call stack reduction. At each iteration only three
variants are possible: call stack for the analyzed constraint is
not a preﬁx of the current call stack cs, or it is a strict preﬁx,
or they are equal. The ﬁrst option means that the analyzed
branch is located in the function that was called and returned
before the target branch, thus the target branch is not control
dependent on it. The left two variants may potentially cause the
target branch to be control dependent on the analyzed branch,
and we have to make additional checks in that cases. If we get
the second variant, before any other actions we should change
the target point and the current call stack cs as constraint c
is located in different function frame compared to the current
point. For this reason, we reassign point with the call site
of the ﬁrst function distinguishing call stack(c) and cs, and
save the value of call stack(c) in cs. It means that we will
check whether this function call site is control dependent on
the following analyzed branches in order to guarantee that the
function is called.

When the point and analyzed constraint are located in
the same function frame, we start making additional checks.
The main reason for the constraint to be added to the strong
optimistic predicate is the fact that the point is nested in the
corresponding branch. This check is denoted as src(c) ≤
point < dst(c), where src and dst return the respective

jump source and destination addresses. But there is another
important condition notifying that constraint should not be
eliminated: the corresponding branch scope contains at least
one control transfer instruction (CTI). There are two main
instruction types in the CTI category. Firstly, all variants of
ret instruction as they lead to premature function termination.
Secondly, conditional and unconditional jumps with the des-
tination addresses beyond the analyzed branch scope borders.
We process such instructions only if their destination address
is greater than the last instruction address in the scope. On
the contrary, we do not process backward jumps because it
leads to worse symbolic execution accuracy and speed. The
two instruction types (explained above) are used for many
source-code patterns implementation, including loop breaks,
goto, returning from functions, and terminating the program.
Thus, the respective branch constraints are considered relevant
as changing these branches direction may immediately affect
the program control ﬂow. In order to perform this check,
we consequently disassemble all the instructions located in
the branch scope, and examine them until the appropriate
instruction is found or the instructions block ends.

TABLE I
APPLYING STRONG OPTIMISTIC STRATEGY TO LISTING 1

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34

Iter

point

cs

line 2

1 main()
func()
2

c

—

Πsopt

c2

line 16

1 main()

buf[1] - buf[3] == 1

c2 ∧ c15

line 16

1 main()

buf[0] == ’3’

c2 ∧ c15

0

1

2

// cJSON_ParseWithLengthOpts()
{

...
item = cJSON_New_Item(&global_hooks);
if (item == NULL) /* memory fail */
{

goto fail;

}
...
if (!parse_value(item,
buffer_skip_whitespace(skip_utf8_bom(&buffer))))
{

...

}
...

fail: ...
}
...
// cJSON.c:1317
// parse_value()
{

...
if (can_read(input_buffer, 4) &&
(strncmp((constchar*)buffer_at_offset(input_buffer),
"null", 4) == 0))
{

item->type = cJSON_NULL;
input_buffer->offset += 4;
return true;

}
/* false */
...

}

Listing 4. cJSON example with CTI processing.

TABLE II
APPLYING STRONG OPTIMISTIC STRATEGY TO LISTING 4

Iter

point

cs

0

1

line 25

line 11

1
2

1

cJSON ParseWithLengthOpts()
parse value()

cJSON ParseWithLengthOpts()

c

—

c6

Πsopt

c25

c25 ∧ c6

As an illustration, consider the example in Listing 1. Table I
demonstrates the algorithm execution on this example. The
original sliced predicate looks as follows: Π = (buf[0] =
’3’) ∧ (buf[1] − buf[3] = 1) ∧ (buf[3] =
’6’ ∧ buf[0] = ’5’). The target branch is located in
line 2, and the initial cs contains func and main call sites. On
the ﬁrst iteration, we analyze the branch in line 15. Since the
call stack at that point contain only main function call site, we
should update point and cs values. After that, we ﬁnd out that
reaching the point value is control dependent on this branch,
as its src addr corresponds to line 15, and dst addr is related
to the end of main function (line 17). Hence, the constraint
from line 15 is added to the predicate. On the second step,
we consider the branch in line 13. The function frames are
the same but the destination address of that branch is located
before the point – in line 15. The target branch is not control
dependent on the branch in line 13, and there are no CTI
instructions in the latter’s scope, thus constraint from line 13
is not added to the predicate. So, the ﬁnal strong optimistic
predicate has the following form: Πsopt = (buf[3] =
’6’ ∧ buf[0] = ’5’) ∧ (buf[1] − buf[3] = 1).

We suggest the following code to demonstrate the process-

ing of control transfer instructions:

1

// cJSON.c:1100

Suppose our goal is to make the constraint in lines 25–26
false (invert the corresponding branch). Note that a single if-
statement in source code may contain several branches, divided
by logical && and || operators, as in this example. Each
branch is expressed via its own jcc instruction, i.e. it is
the individual branch in binary code. Suppose that the sliced
predicate for inverting the branch turned out to be UNSAT, so,
we try to apply the strong optimistic strategy. According to the
algorithm, we add the target branch to the resulting predicate
and save the call stack in that point (point and cs values are
shown in Table II). Then we start iterating over sliced con-
straints and at some moment get to the branch in line 6. As this
branch is located in another function frame, we need to change
the values of point and cs. In this case call stacks differ only
by parse_value function, and its call site is in lines 11–12
(in function cJSON_ParseWithLengthOpts). Thus, the
current point is in line 11, and the analyzed branch scope is
between lines 6 and 10, so, the point is not control dependent
on the analyzed branch. However, if we explore the scope
instructions, we can see that there is goto command compiled
as an unconditional jump to the label fail beyond the scope.
As a result, the analyzed branch scope has an appropriate CTI

inside, hence, the negation of constraint in line 6 should be
added to the predicate.

The following Listing 5 shows another source code pattern
that may lead to adding the constraint to the strong optimistic
predicate, regardless of whether reaching the point is control
dependent on the corresponding branch:

1
2
3
4
5
6
7
8
9
10

// cbor/internal/builder_callbacks.c:34
// _cbor_builder_append()
{

assert(ctx->stack->top->subitems > 0);
...
if (ctx->stack->top->subitems == 0) {

...

}
...

}

Listing 5.

libcbor example with CTI processing.

In this example, assert is actually a symbolic branch.
When compiling it with clang without any optimizations, the
following assembly is generated:

rax,QWORD PTR [rbp-0x10]
rax,QWORD PTR [rax+0x10]
rax,QWORD PTR [rax]
QWORD PTR [rax+0x10],0x0
4061a5 <_cbor_builder_append+0xb5>

40618a: mov
40618e: mov
406192: mov
406195: cmp
40619a: jbe
4061a0: jmp 4061cd <_cbor_builder_append+0xdd>
4061a5: movabs rdi,0x40eaa8
4061ac:
4061af: movabs rsi,0x40eac6
4061b6:
4061b9: mov
4061be: movabs rcx,0x40eaf5
4061c5:
4061c8: call
4061cd: ...

4010a0 <__assert_fail@plt>

edx,0x22

Suppose we want to invert the branch in line 6 (e.g. make
the condition to be false). Initially, Πsopt will contain the
inverted constraint from line 6. Then at some moment we
start to analyze the assert branch. The function frame is the
same as in line 6, reaching the point (that stands at line 6)
is not control dependent on the branch. If we do not consider
the CTI, we might miss this important branch and make the
strong optimistic solution incorrect. But as the assembly has
the exactly such form as in the listing, we can notice that an
unconditional jump goes beyond the above branch destination
address. Hence, jmp is an appropriate CTI that makes the
above jbe to be added to the predicate.

IV. IMPLEMENTATION

We implement strong optimistic solving in our dynamic
symbolic execution tool Sydr [9]. Sydr consists of two pro-
cesses: Concrete Executor, based on dynamic binary instru-
mentation tool DynamoRIO [26] and Symbolic Executor,
including dynamic symbolic execution framework Triton [27].
Concrete Executor runs the program, handles its instruc-
tions, and provides all the necessary information to the Sym-
bolic Executor via shared memory events. On the concrete
side, we use DynamoRIO to handle symbolic branches scopes
to ﬁnd control transfer instructions (CTI). When we meet a
conditional branch, we get its source and destination addresses,
and sequentially disassemble instructions between them check-
ing their opcodes. For conditional and unconditional jumps

we additionally check if their destination addresses go further
the analyzed scope end. For each symbolic branch we ﬁll the
special ﬁeld in the instruction event structure that signalizes
whether this branch has one of the appropriate control transfer
instructions inside its branch scope.

On the symbolic side, Sydr handles shared memory events.
For the instruction event, it saves the CTI-ﬁeld information and
interprets symbolic instruction. If the instruction is call or
ret, then Symbolic Executor updates the program call stack.
Firstly, it removes inconsistent stack part, i.e. functions with
sp-values greater than the current sp. Secondly, for the call
instructions, it adds a new call stack entry with the function
call site, current sp-value, instruction module address, and
module id.

There are two main threads in Symbolic Executor: the ﬁrst
one uses Triton to build the path predicate, the second one
inverts symbolic branches and communicates with Bitwuzla
solver [28] to generate test cases. We implement optimistic
and strong optimistic strategies in the second thread. We apply
them while inverting symbolic branches in case the sliced path
predicate turns out to be UNSAT. According to the ﬂowchart
in Figure 1, we ask the solver for the optimistic predicate
satisﬁability, and if it occurs to be SAT, we build the strong
optimistic predicate and query the solver again.

V. EVALUATION

We evaluate strong optimistic solving efﬁciency and perfor-
mance on a set of 64-bit Linux programs [17] in the context of
our dynamic symbolic execution tool Sydr [9]. The evaluation
is performed in two steps. Firstly, we run benchmark and
measure the analysis performance. Secondly, we collect the
code coverage information to determine whether the strong
optimistic strategy helps to discover new execution paths.

We use a server with two AMD EPYC 7542 32-Core
processors and 512GB RAM for Sydr benchmarking. For
each target application we launch branch inversion for one
hour. According to the updated Sydr architecture, symbolic
execution is performed within the two main threads: the ﬁrst
one for the path predicate construction, and the second one
for inverting symbolic branches and solving the queries. All
encountered symbolic branches are inverted in the direct order.
The SMT solver is working asynchronously with the job of
the path predicate construction, and there are no time limits
for the target program execution.

Tables III and IV show the benchmarking results for
symbolic execution with various strategies applied. Table III
contains results for the base symbolic execution mode with
solving query timeout of 10 seconds, while Table IV corre-
sponds to the launch with symbolic pointer reasoning enabled
(symptr mode) and solving query timeout of 30 seconds. In
both cases we compared the following runs:

• default symbolic execution without using any strategies,
• symbolic execution with optimistic solutions only,
• symbolic execution with strong optimistic solutions only,
• symbolic execution with both optimistic strategies ap-

plied.

TABLE III
ANALYSIS EFFICIENCY (BASE)

Application

bzip2recover
decompress
faad
foo2lava
hdp
jasper
libxml2
minigzip
muraster
pk2bm
pnmhistmap (ppm)
re2
readelf
sqlite3
yices-smt2
yodl

Correct
2101
992
372
363
4172
19558
1035
3928
2203
190
113
471
648
8412
4407
1130

Default
Accuracy
100.0%
83.36%
64.03%
99.45%
68.38%
98.33%
83.0%
51.9%
99.91%
99.48%
91.13%
100.0%
46.99%
99.98%
79.55%
98.26%

Speed
135.55
16.53
6.2
6.05
69.53
325.97
509.02
434.83
36.72
670.59
10.51
7.85
10.8
3554.37
798.85
4520.0

Strong Optimistic Only
Accuracy
98.53%
65.73%
52.6%
77.62%
65.65%
74.99%
85.91%
49.93%
92.65%
73.76%
8.72%
23.68%
45.62%
99.62%
76.77%
73.01%

Correct
2148
1550
384
770
7435
16921
5708
4110
2282
194
131
745
1057
9500
5170
2316

Speed
93.73
25.83
6.4
12.83
123.92
282.02
2329.8
441.94
38.03
554.29
8.55
12.42
17.62
2740.38
937.16
7720.0

Optimistic Only
Accuracy
100.0%
64.38%
38.66%
54.86%
66.83%
61.59%
89.32%
56.87%
75.24%
53.42%
8.66%
48.82%
59.08%
35.31%
74.1%
70.23%

Correct
2264
4063
409
779
8091
17961
7896
5105
2465
273
558
1964
2661
10432
6002
3078

Speed
141.8
67.72
6.82
12.98
134.85
299.35
3616.49
577.92
41.08
963.53
44.58
32.73
44.35
3243.11
1087.98
10260.0

Optimistic + Strong Optimistic
Speed
Accuracy
Correct
2264
131.76
100.0%
51.52
60.79%
3091
6.78
38.61%
407
11.77
54.48%
706
8379
139.65
70.35%
272.53
61.44%
16352
7936
3283.86
89.77%
5105
539.26
56.87%
41.02
74.73%
2461
275
750.0
53.82%
571
31.58
8.87%
2046
34.1
52.29%
44.08
61.4%
2645
10432
2952.45
35.31%
6006
1085.42
74.15%
3257
9771.0
71.04%

For each conﬁguration we measured the number of correct
branches (column Correct), the analysis accuracy (column
Accuracy), and the average number of correct branches dis-
covered in one minute (column Speed).

To explore new execution paths, Sydr tries to invert sym-
bolic branches encountered during the path predicate construc-
tion. Thus, the solver-generated input is called correct if it
actually inverts the target symbolic branch, i.e. makes the
execution go through the target branch in inverted direction
compared to the original execution. The target program should
reach this branch and pass through it in inverted direction on
the correct test case. However, there may be both optimistic
and strong optimistic solutions for target branch inversion. In
this case we consider the branch to be correct when at least
one of the optimistic predicates for its inversion has correct
solution.

According to the ﬂowchart in Figure 1, if the sliced pred-
icate for inverting a branch turns out to be unsatisﬁable, we
have a chance to get both optimistic and strong optimistic
solutions for this predicate. In this case, we consider them
as one to count exactly the number of branches that have at
least one satisﬁable inversion predicate. And we check that at
least one of them actually inverts the branch. If both queries
are satisﬁable, then the number of SAT queries is increased by
two while we want to increase our counter by one. Similarly, if
both solutions are correct, we get the correct solutions number
instead of the correct branches number. Thus, we recalculate
these numbers by subtraction of one in each case that leads
to the wrong counting. After this operation we obtain the
number of branches with satisﬁable predicates and the number
of correct branches (showed in column Correct). The analysis
accuracy is calculated via dividing the number of correct
branches by the number of SATs (column Accuracy). The
symbolic execution speed is obtained as the result of dividing
the number of correct branches by the symbolic execution time
in minutes (column Speed).

First of all, Tables III and IV show that the optimistic
solving allows to discover more correct branches on all

applications, except for jasper. Such results are obtained
in both default and symptr symbolic execution modes. For
decompress, foo2lava, hdp, libxml2, pnmhistmap
(ppm), re2, readelf, and yodl the number of correct
branches increased in orders of magnitude. The decrease for
jasper is caused by the additional time we spend for each
UNSAT predicate to solve the optimistic predicate. As a
consequence, we are not able to explore as much branches
as in default mode. Moreover, some optimistic solutions may
appear to be incorrect. Along with this, the number of correct
branches discovered per time unit increased in all cases, except
for jasper and sqlite3. Eventually, the optimistic strategy
helps ﬁnd much more correct branches with a signiﬁcant
increase in symbolic execution speed.

In Tables III and IV we can also see that applying the
strong optimistic strategy along with the optimistic one makes
the results even better. On most applications the number
of correct branches increased without signiﬁcant speed loss,
when compared to the optimistic-only conﬁguration. We can
notice that libxml2 and yodl have the proﬁt
in both
runs; decompress, readelf, and sqlite3 – only in
the symptr mode; hdp, re2, and yices-smt2 – only in
the base mode. However, for faad, foo2lava, jasper,
and muraster this value became smaller in both base and
symptr variants. It is a consequence of the method ﬂowchart in
Figure 1. In case the sliced predicate for inverting the branch is
unsatisﬁable, we frequently need to do much more additional
work than querying the solver for just one branch satisﬁability.
Along with this, we construct the strong optimistic predicate
and query the solver for its satisﬁability. Hence, the more
strong optimistics we solve, the more time for one branch
we spend, and the less branches we are able to handle during
one hour interval. So, if we have a limited time which is not
enough for the complete application analysis (that is true for
the programs with worse results) and the symbolic constraints
are complex, the number of processed symbolic branches is
very likely to decrease in comparison with the default symbolic
execution mode.

TABLE IV
ANALYSIS EFFICIENCY (SYMPTR)

Application

decompress
hdp
jasper
libxml2
minigzip
re2
readelf
sqlite3
yices-smt2
yodl

Correct
572
1006
37227
1184
577
332
127
10338
2178
1402

Default
Accuracy
80.56%
64.86%
99.19%
91.22%
59.73%
100.0%
91.37%
99.98%
82.25%
98.66%

Speed
9.53
16.77
620.45
215.93
9.62
5.53
2.12
1797.91
36.3
1682.4

Strong Optimistic Only
Accuracy
66.14%
64.33%
88.43%
88.16%
60.04%
19.66%
76.19%
99.47%
74.74%
75.43%

Correct
793
1311
34845
6152
604
399
160
11502
2326
2625

Speed
13.22
21.85
580.75
932.12
10.07
6.65
2.67
1691.47
38.77
2812.5

Optimistic Only
Accuracy
48.47%
68.52%
80.61%
88.13%
60.02%
49.73%
25.83%
39.44%
74.31%
72.2%

Correct
999
1432
35430
8674
614
1271
179
12462
2734
3387

Speed
16.65
23.87
590.5
1320.91
10.23
21.18
2.98
1917.23
45.57
3628.93

Optimistic + Strong Optimistic
Speed
Accuracy
Correct
1034
17.23
50.27%
23.65
68.39%
1419
549.53
80.62%
32972
8727
1198.22
88.67%
614
10.23
60.2%
21.17
49.76%
1270
207
3.45
30.17%
12463
1776.2
39.45%
42.68
74.38%
2561
3566
3626.44
72.88%

On the contrary, the conﬁguration with strong optimistic
solutions only leads to signiﬁcant decrease in the number
of correct branches and speed. When we stop employing
optimistic solutions, we waste lots of potentially correct inputs
that are generated on average several times faster than the
strong optimistic ones. Besides, the queries for the strong
optimistics are more likely to be unsatisﬁable as they contain
more than one constraint. Thus, we spend time and resources
for solving more complex queries, and do not solve rather
simple ones, what results in analysis efﬁciency decrease. So,
the optimal approach is applying the strong optimistic strategy
as a complement to the optimistic one.

Another observation is that the branch inversion accuracy
decreases when applying any of the strategies, compared to
the default mode, even when the absolute number of unique
correct branches increases. The reason is that
the corpus
size enlarges signiﬁcantly, especially with optimistic solutions
used. Every unsatisﬁable predicate leads to an attempt of
generating a solution for the optimistic predicate that is much
simpler than the original one, thus is more likely to be SAT.
And in case it is, the strong optimistic solution may also be
generated. But not all these solutions are really able to invert
the target branch, so we get an impressive increase in the
number of incorrect branches that were not encountered earlier.
This directly affects the symbolic execution accuracy.

Overall, applying the combination of optimistic and strong
optimistic strategies in most cases allows to discover more cor-
rect symbolic branches during analysis than in the optimistic-
only conﬁguration, and achieve the compatible performance.
Measuring the code coverage is the second evaluation step.
We gather three corpuses containing solver-generated seeds
provided by each conﬁguration to estimate the difference in
coverage between Base conﬁguration, the one with optimistic
solutions (Opt), and the one with both optimistic and strong
optimistic solutions (Sopt). We use aﬂ-showmap [6] utility
in QEMU-mode to get the total number of program edges
achieved on ﬁles from each corpus. The results are shown in
Table V. The columns Base, Opt, and Sopt show the number
of program edges covered by the corpus for each conﬁguration
respectively. The column Opt / Base depicts the coverage
growth provided by Opt (optimistic) conﬁguration compared to
Base. The column Sopt / Opt illustrates the coverage growth

provided by Sopt (optimistic + strong optimistic) conﬁguration
compared to Opt.

As we can see, strong optimistic solutions provide ad-
ditional coverage growth in comparison with the opti-
mistic conﬁguration on the majority of target applications:
cjpeg, decompress, faad, foo2lava, hdp, jasper,
libxml2, minigzip, re2, readelf. The growth range
varies from insigniﬁcant (0.04% – cjpeg and foo2lava)
to quite large (7.28% – re2). However, some programs do
not gain the coverage growth from the strong optimistic solu-
tions (bzip2recover, muraster, pk2bm, pnmhistmap
(ppm), sqlite3, yices-smt2, and yodl). There are
two main reasons for this situation. Firstly, some predicates
might have simple constraints that are not exposed to over-
and underconstraint. In such cases, the optimistic solutions
are enough to invert the corresponding branches. Secondly,
some predicates might contain extremely complex constraints
or be highly over-/underconstrained. In these situations strong
optimistic strategy is still not sufﬁcient to get the correct
inputs. It is worth noting that most of these applications yet
have nice benchmark results (in either base or symptr mode).
Concluding the performed evaluation, we can say that the
strong optimistic strategy in combination with the optimistic
strategy helps either increase the efﬁciency and speed of
symbolic execution, or discover the new coverage. At the same
time, there is no sense in using the strong optimistic strategy
alone, as such variant leads to signiﬁcant performance and
accuracy loss.

VI. CONCLUSION

We propose strong optimistic solving and implement it in
our dynamic symbolic execution tool Sydr [9]. It allows to
eliminate irrelevant constraints from the path predicate based
on the information about symbolic branches nesting. Addi-
tionally, symbolic constraints, which may affect the program
control ﬂow, are not eliminated from the resulting predicate.
The proposed method uses control dependency and call stack
analysis to determine relevant branches.

The strong optimistic strategy allows to discover new exe-
cution paths by inverting more symbolic branches along the
single execution trace. We evaluate the method on the set of
real-world applications [17] measuring the analysis efﬁciency

TABLE V
CODE COVERAGE

Base
Application
281
bzip2recover
2412
cjpeg
4523
decompress
3772
faad
2396
foo2lava
6979
hdp
1556
jasper
8763
libxml2
1675
minigzip
3368
muraster
pk2bm
1572
pnmhistmap (ppm) 1561
4101
re2
4086
readelf
4674
sqlite3
6091
yices-smt2
3156
yodl

Opt
282
2427
4535
4657
2417
7431
1571
8914
1675
3394
1575
1705
4547
4971
4702
6145
3174

Opt / Base
+0.36%
+0.62%
+0.27%
+23.46%
+0.88%
+6.48%
+0.96%
+1.71%
0%
+0.77%
+0.19%
+9.22%
+10.88%
+21.66%
+0.6%
+0.89%
+0.57%

Sopt
282
2428
4679
4689
2418
7472
1574
8928
1696
3394
1575
1705
4878
5076
4702
6145
3174

Sopt / Opt
0%
+0.04%
+3.18%
+0.69%
+0.04%
+0.55%
+0.19%
+0.16%
+1.25%
0%
0%
0%
+7.28%
+2.11%
0%
0%
0%

(i.e. correctly inverted branches number, accuracy, and speed)
and the code coverage. According to the results, applying the
strong optimistic strategy along with the optimistic strategy [8]
leads to either discovering more correct symbolic branches at
the same time without great performance loss, or increasing
the explored code coverage after a complete program ex-
ploration. At the same time, there is no sense in using the
strong optimistic strategy alone, as in this case signiﬁcant
performance and accuracy loss is detected. So, the optimal
way is applying the combination of the solving strategies,
depending on the corresponding queries satisﬁability.

REFERENCES

[2]

[1] M. Howard and S. Lipner, The security development
lifecycle. Microsoft Press Redmond, 2006, vol. 8.
[Online]. Available: http://msdn.microsoft.com/en-us/
library/ms995349.aspx.
ISO/IEC 15408-3:2008: Information technology – Se-
curity techniques – Evaluation criteria for IT security
– Part 3: Security assurance components. ISO Geneva,
Switzerland, 2008. [Online]. Available: https://www.iso.
org/standard/46413.html.
[3] GOST R 56939-2016:

Information protection. Se-
cure software development. General requirements. Na-
tional Standard of Russian Federation, 2016. [On-
line]. Available: http://protect.gost.ru/document.aspx?
control=7&id=203548.

[4] B. S. Pak, “Hybrid fuzz testing: Discovering software
bugs via fuzzing and symbolic execution,” M.S. thesis,
School of Computer Science Carnegie Mellon Univer-
sity, 2012.

[5] K. Serebryany, “Continuous fuzzing with libFuzzer
and AddressSanitizer,” in 2016 IEEE Cybersecurity
Development
IEEE, 2016, p. 157. DOI:
10.1109/SecDev.2016.043.

(SecDev),

[6] A. Fioraldi, D. Maier, H. Eißfeldt, and M. Heuse,
“AFL++: Combining incremental steps of fuzzing re-
search,” in 14th USENIX Workshop on Offensive Tech-
nologies (WOOT 20), 2020. [Online]. Available: https://
www.usenix.org/system/ﬁles/woot20-paper-ﬁoraldi.pdf.
[7] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang,
J. Corbetta, Y. Shoshitaishvili, C. Kruegel, and G.
Vigna, “Driller: Augmenting fuzzing through selective
symbolic execution,” in NDSS, vol. 16, 2016, pp. 1–16.
I. Yun, S. Lee, M. Xu, Y. Jang, and T. Kim, “QSYM:
A practical concolic execution engine tailored for
hybrid fuzzing,” in 27th USENIX Security Symposium,
2018, pp. 745–761. [Online]. Available: https://www.
usenix.org/system/ﬁles/conference/usenixsecurity18/
sec18-yun.pdf.

[8]

[9] A. Vishnyakov, A. Fedotov, D. Kuts, A. Novikov,
D. Parygina, E. Kobrin, V. Logunova, P. Belecky,
and S. Kurmangaleev, “Sydr: Cutting edge dynamic
symbolic execution,” in 2020 Ivannikov ISPRAS Open
Conference (ISPRAS), IEEE, 2020, pp. 46–54. DOI:
10.1109/ISPRAS51486.2020.00014.

[10] S. Poeplau and A. Francillon, “Symbolic execution with
SymCC: Don’t interpret, compile!” In 29th USENIX
Security Symposium (USENIX Security 20), 2020,
pp. 181–198. [Online]. Available: https://www.usenix.
org/system/ﬁles/sec20-poeplau.pdf.

[11] ——, “SymQEMU: Compilation-based symbolic exe-
cution for binaries,” in Proceedings of the 2021 Network
and Distributed System Security Symposium, 2021. DOI:
10.14722/ndss.2021.23118.

[12] L. Borzacchiello, E. Coppa, and C. Demetrescu, “FUZ-
ZOLIC: Mixing fuzzing and concolic execution,” Com-
puters & Security, vol. 108, p. 102 368, 2021. DOI:
10.1016/j.cose.2021.102368.

[13] A. Vishnyakov, V. Logunova, E. Kobrin, D. Kuts, D.
Parygina, and A. Fedotov, “Symbolic security predi-
cates: Hunt program weaknesses,” in 2021 Ivannikov IS-
PRAS Open Conference (ISPRAS), IEEE, 2021, pp. 76–
85. DOI: 10.1109/ISPRAS53967.2021.00016.
J. C. King, “Symbolic execution and program testing,”
Communications of the ACM, vol. 19, no. 7, pp. 385–
394, 1976. DOI: 10.1145/360248.360252.

[14]

[15] R. Baldoni, E. Coppa, D. C. D’Elia, C. Demetrescu,
and I. Finocchi, “A survey of symbolic execution tech-
niques,” ACM Computing Surveys, vol. 51, no. 3, 2018.
DOI: 10.1145/3182657.

[16] M. G. Kang, S. McCamant, P. Poosankam, and D. Song,
“DTA++: Dynamic taint analysis with targeted control-
ﬂow propagation,” in Proceedings of the Network and
Distributed System Security Symposium, ser. NDSS ’11,
2011.

[17] Sydr benchmark. [Online]. Available: https://github.

com/ispras/sydr-benchmark.

[18] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill,
and D. R. Engler, “EXE: Automatically generating
the 13th ACM
inputs of death,” in Proceedings of

Conference on Computer and Communications Secu-
rity, ser. CCS ’06, ACM, 2006, pp. 322–335. DOI:
10.1145/1180405.1180445.

[19] C. Cadar, D. Dunbar, and D. R. Engler, “KLEE: Unas-
sisted and automatic generation of high-coverage tests
for complex systems programs,” in OSDI, vol. 8, 2008,
pp. 209–224. [Online]. Available: https://static.usenix.
org/events/osdi08/tech/full papers/cadar/cadar.pdf.
[20] V. Chipounov, V. Kuznetsov, and G. Candea, “S2E: A
platform for in-vivo multi-path analysis of software sys-
tems,” SIGPLAN Notices, vol. 46, no. 3, pp. 265–278,
2011. DOI: 10.1145/1961296.1950396.

[21] Y. Shoshitaishvili, R. Wang, C. Salls, N. Stephens,
M. Polino, A. Dutcher, J. Grosen, S. Feng, C. Hauser,
C. Kruegel, and G. Vigna, “SOK: (state of) the art of
war: Offensive techniques in binary analysis,” in 2016
IEEE Symposium on Security and Privacy (SP), 2016,
pp. 138–157. DOI: 10.1109/SP.2016.17.

[22] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley,
“Unleashing Mayhem on binary code,” in Proceedings
of the 2012 IEEE Symposium on Security and Privacy,
ser. SP ’12, IEEE Computer Society, 2012, pp. 380–
394. DOI: 10.1109/SP.2012.31.

[23] D. Kuts, “Towards symbolic pointers reasoning in dy-
namic symbolic execution,” in 2021 Ivannikov Memo-
rial Workshop (IVMEM), IEEE, 2021, pp. 42–49. DOI:
10.1109/IVMEM53963.2021.00014.

[24] G. Balakrishnan and T. Reps, “Analyzing memory ac-
cesses in x86 executables,” in International conference
on compiler construction, Springer Berlin Heidelberg,
2004, pp. 5–23. DOI: 10.1007/978-3-540-24723-4 2.

[25] X. Mi, S. Rawat, C. Giuffrida, and H. Bos, “LeanSym:
Efﬁcient hybrid fuzzing through conservative constraint
debloating,” 24th International Symposium on Research
in Attacks, Intrusions and Defenses (RAID ’21), pp. 62–
77, 2021. DOI: 10.1145/3471621.3471852.

[26] D. Bruening, “Efﬁcient, transparent, and comprehensive
runtime code manipulation,” Ph.D. dissertation, Mas-
sachusetts Institute of Technology, Department of Elec-
trical Engineering and Computer Science, 2004. [On-
line]. Available: https://www.burningcutlery.com/derek/
docs/phd.pdf.

[27] F. Saudel and J. Salwan, “Triton: A dynamic symbolic
execution framework,” in Symposium sur la s´ecurit´e
des technologies de l’information et des communica-
tions, ser. SSTIC, 2015, pp. 31–54. [Online]. Avail-
able: https://triton.quarkslab.com/ﬁles/sstic2015 slide
en saudel salwan.pdf.

[28] A. Niemetz and M. Preiner, “Bitwuzla at the SMT-
COMP 2020,” CoRR, vol. abs/2006.01621, 2020. arXiv:
2006.01621. [Online]. Available: https://arxiv.org/abs/
2006.01621.

