Reducing the Cost of Training Security Classifier (via Optimized
Semi-Supervised Learning)

Rui Shu, Tianpei Xia, Huy Tu, Laurie Williams, Tim Menzies
North Carolina State University
Raleigh, North Carolina, USA

2
2
0
2

y
a
M
2

]

R
C
.
s
c
[

1
v
5
6
6
0
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Background: Most of the existing machine learning models for
security tasks, such as spam detection, malware detection, or net-
work intrusion detection, are built on supervised machine learning
algorithms. In such a paradigm, models need a large amount of
labeled data to learn the useful relationships between selected fea-
tures and target class. However, such labeled data can be scarce
and expensive to acquire. Goal: To help security practitioners train
useful security classification models when few labeled training
data and many unlabeled training data are available. Method: We
propose an adaptive framework called Dapper, which optimizes
1) semi-supervised learning algorithms to assign pseudo-labels to
unlabeled data in a propagation paradigm and 2) the machine learn-
ing classifier (i.e., random forest). When the dataset class is highly
imbalanced, Dapper then adaptively integrates and optimizes a data
oversampling method called SMOTE. We use the novel Bayesian Op-
timization to search a large hyperparameter space of these tuning
targets. Result: We evaluate Dapper with three security datasets,
i.e., the Twitter spam dataset, the malware URLs dataset, and the
CIC-IDS-2017 dataset. Experimental results indicate that we can
use as low as 10% of original labeled data but achieve close or even
better classification performance than using 100% labeled data in a
supervised way. Conclusion: Based on those results, we would rec-
ommend using hyperparameter optimization with semi-supervised
learning when dealing with shortages of labeled security data.

ACM Reference Format:
Rui Shu, Tianpei Xia, Huy Tu, Laurie Williams, Tim Menzies. 2022. Reducing
the Cost of Training Security Classifier (via Optimized Semi-Supervised
Learning). In Proceedings of Make sure to enter the correct conference title
from your rights confirmation emai (Conference acronym ’XX). ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
When using machine learning to address security tasks, most exist-
ing models are built on supervised-learning algorithms. For exam-
ple, many existing spam detection techniques [11, 41], malware de-
tection techniques [34] or network intrusion detection systems [26]
train different classifiers to learn inherent relationships that exist
between selected features and associated output class (i.e., label),

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

namely abnormal and benign. Next, those classifiers are tested
on unseen data for classification purposes. Thus, labeled data is
necessary for training a helpful model in a supervised paradigm.
However, there are often cases when labeled security data is
insufficient and expensive to collect, while a large set of unlabeled
security data is available. To make good use of these unlabeled data,
more practitioners resort to ways to annotate data to enlarge the size
of labeled training data. Such process is referred as data annotation
or data labeling. However, the work of data annotation is usually
time-consuming and costly. For example, Tu et al. [37] in other
domain (e.g., SE) task reported that manually reading and labeling
22,500+ GitHub commits requires 175 person-hours (approximately
nine weeks), including cross-checking among labelers. Moreover,
specific domain knowledge (e.g., security) is also required to ensure
the high quality of annotated data.

Semi-supervised learning (SSL) [47, 48] can address the chal-
lenges as mentioned above from the algorithm perspective. SSL is a
method that correlates the features in unlabeled data with labeled
data and further generates pseudo-labels for the unlabelled data
in an intuitive way. The newly labeled dataset (i.e., a mixture of
both labeled and pseudo-labeled samples) is then used to train a
model in a supervised manner. Without involving manual and ex-
pensive labor, SSL significantly reduces the cost of training models
with a large labeled dataset instead of only a tiny portion of them.
Table 1 lists a sample list of prior work that uses semi-supervised
learning in multiple security tasks. Among these work, there are
two representative semi-supervised learning algorithms, i.e., la-
bel propagation [46] and label spreading [45]. Both algorithms use
graph representation, compute data similarity between labeled and
unlabeled data, and further propagate known labels through the
edges of the graph to unlabeled data.

However, we observe that applications in Table 1 rarely apply
hyperparameter optimization on SSL algorithms. We argue that
exploring suitable hyperparameter configurations for semi-supervised
learning algorithms would have an impact on the performance. To
validate this argument, we propose an adaptive framework called
Dapper that adopts hyperparameter optimization to control the
configurations of the SSL algorithms. Beyond that, Dapper also
explores the hyperparameter space of machine learning classifier
(e.g., Random Forest). Furthermore, we also observe that, for some
security datasets, the mixed training dataset (including labeled data
and pseudo-labeled data) still suffers from class imbalance (which
is a common issue in the security dataset). In this case, Dapper
adaptively adds a tunable version of SMOTE [1] to rebalance the
ratio between classes in the training datasets.

We evaluate Dapper with three real-world study cases, i.e. the
Twitter spam dataset [9], malware URLs [22] and CIC-IDS-2017

 
 
 
 
 
 
Conference acronym ’XX, June 03–05, 2018, Woodstock, NY

Rui Shu, Tianpei Xia, Huy Tu, Laurie Williams, Tim Menzies

Table 1: A list of prior work using semi-supervised learning in security tasks.

Publication

Year

Brief Description

[18]
[40]
[43]
[24]
[39]
[2]
[36]
[25]
[23]
[17]

Use label propagation to detect review spam groups.

Use SSL to maximize the effectiveness of limit labeled training data for insider threat detection.

2021
2015 Model and automate the Android policy refinement process with SSL.
2020
2016 Help with classification task of identifying relevant products in darknet/deepnet marketplaces, etc.
2021
2016
2020
2016
2015
2016

Provide a multi-label propagation based method for fraud detection.
Propose a method to estimate the maliciousness of a given file through a semi-supervised label propagation procedure.
Introduce a DL-based semi-supervised approach against label flipping attacks in the malware detection system.
Propose to use label propagation to discover infected Remote Access Trojans packets in large unlabeled data.
Use label propagation for malware detection.
Create a semi-supervised malware classification system that unifies views of static and dynamic malware analysis.

dataset [31]. Our experimental results indicate that Dapper out-
performs default SSL learners and optimized SSL learners. We can
use as low as 10% of the original labeled dataset but achieve close
or even better classification performance (e.g., g-measure and re-
call) than using 100% original labeled data. Based on those results,
we recommend using Dapper framework when dealing with the
shortage of available labeled data for security tasks.

The remainder of this paper is organized as follows. We discuss
background and related work in Section 2 and our methodology
in Section 3. We then report our experiment details in Section 4,
including datasets, evaluation metrics, etc. Section 5 presents our
experiment results. We discuss the threats to validity in Section 6
and then we conclude in Section 7.

2 BACKGROUND AND RELATED WORKS

2.1 Training Security Models Requires Labeled

Data

Diverse machine learning techniques have been widely applied in
the cyber-security field to address wide-ranging problems such as
spam detection, malware detection, and network intrusion detec-
tion. For example, in Twitter spam detection [9], machine learning
algorithms use account-based features (e.g., the number of follow-
ers or friends) or message-based features (e.g., length of a tweet)
to train useful models which are further used to predict other new
spamming activities. In malware detection [22], several prior works
analyze web URLs where malicious URLs are intended for mali-
cious purposes such as stealing user privacy information. Security
practitioners use machine learning techniques to classify malicious
websites with features extracted from URLs such as URL tokens,
length of URLs, etc. Another example is the network intrusion de-
tection [22] which endeavors to identify malicious behaviors in the
network traffic. Machine learning-based techniques have gained
enormous popularity in this field. They learn useful features from
the network traffic and classify the normal and abnormal activities
based on the learned patterns.

Security classification algorithms learn models from data, so
insufficient training data can lead to low-quality models. But in
many cases there are only a small number of labeled instances are
available (compared to a much large amount of unlabeled instances).
For example, in a network intrusion detection scenario, network

traffic of the monitored system is continuously generated with a
large load, but the abnormal traffic, which is few and available only
under malicious attacks [12].

To make good use of unlabeled data, practitioners propose meth-
ods to annotate unlabeled data involving human efforts. However,
the process of data annotation faces several challenges:

• Time-consuming and expensive. Due to the large volume
of unlabeled data, much effort and time or finance is ex-
pected to be involved in the process, and it is not always an
affordable solution. For example, manual labeling would cost
$320K and 39,000 hours to label GitHub issues of 50 projects
as buggy or non-buggy [37].

• Require domain knowledge. A lack of professional exper-
tise is commonly the root cause of poor label quality. In the
context of security, a person who lacks knowledge of secu-
rity vulnerability, intrusion detection, malware, etc., hardly
guarantees the right decision on tagging new data.

Moreover, existing data annotation methods can be mainly cate-

gorized into the following types:

• Manual. Manual data annotation works during the initial
phase of a project when the data size is small and not com-
plicated. This method is not salable for a large collection
and might become overwhelmed and cause degraded label
quality.

• Crowdsourcing. Crowdsource data annotation is a better
choice than manual labeling, which can be scalable with
the help of platforms such as Amazon Mechanical Turk
(MTurk) [15]. However, crowdsourcing can be expensive
and can not guarantee the quality of the label.

• Outsourcing. This method includes employing data label-
ing companies in low-cost markets. With extra QA processes
and other solutions, label quality can be controlled and im-
proved. However, outsourcing data annotation is still a man-
ual process. (albeit with a cheap cost of labor).

• Interactive Learning. With methods such as active learn-
ing, rather than annotating all the data independently and
simultaneously, only a fraction of the total number of data
to be labeled [29]. With active learning, the expert users will
label the most suitable data.

In summary, a drawback with all the above methods is that they
are all human-in-the-loop (HITL) methods. This kind of method
raises the issue of requiring expertise of that human (and they might

Reducing the Cost of Training Security Classifier (via Optimized Semi-Supervised Learning)

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY

make mistakes). The rest of this paper explores fully automated
techniques to avoid this issue.

2.2 Semi-Supervised Learning
Ground truth (i.e., labeled data) is often limited and costly to acquire
when applying machine learning to security tasks. To address this
problem, as shown in Table 1, some prior works in security have
used semi-supervised learning to address such challenge. Specifically,
semi-supervised learning (SSL) [47, 48] is a branch of machine learn-
ing algorithms that lies between supervised learning [13] and unsu-
pervised learning [7]. In supervised learning, the training dataset
comprises only labeled data, which is to learn a function that can
generalize well on the unseen data. Unsupervised learning only
considers unlabeled data, where data points are grouped into clus-
ters with similar properties. Semi-supervised learning combines
both supervised learning and unsupervised learning, which uses
a small amount of labeled data and a large amount of unlabeled
data. The use of semi-supervised learning avoids searching labeled
data or manually annotating unlabeled data. Major semi-supervised
learning algorithms can be mainly categorized into the following
two groups [32]:

Wrapper-Based Methods. Methods in this group use a super-
vised algorithm in an iterative way. During each iteration, a certain
amount of unlabeled data is labeled by the decision function that is
learned and incorporated into the training data. With the labeled
data already available as well as its own prediction, the classifi-
cation model is retrained for the next iteration. Two well-known
representative methods in this group are self-training [28] and co-
training [6]. Self-training is a technique in which initially, a classifier
is trained with the small amount of labeled data and then used to
classify unlabeled data. The high confident unlabeled data as well as
their predicted labels are added to the training dataset. The whole
process is repeated either for a fixed number of iterations or until
there are no high-confidence samples left in the unlabeled data. The
co-training algorithm adopts an iterative learning process similar
to self-training. It combines both labeled and unlabeled data under
two-view setting. Initially, co-training trains two classifiers from
each of the subset separately with limited labeled dataset. Then un-
labeled data which the two classifiers have confident prediction will
enlarge the labeled dataset for further training. This process repeats
until a termination condition is met. The premise of co-training
is that it assumes features can be split into two subsets and both
subsets are conditionally independent given the class and sufficient
to train classifiers by itself. Both method have several drawbacks.
For example, in self-training, mistakes can re-enforce themselves.
Co-training makes several assumptions, and only works well when
conditional independence holds.

Graph-Based Methods. Graph-based methods [38] create a
graph that connects instances in the training dataset and prop-
agates labels from labeled data to unlabeled data, through the edges
of the graph. This process typically involves computing similari-
ties between data instances. Consider the geometry of the dataset,
which can be represented by an empirical graph 𝑔 = (𝑉 , 𝐸), where
nodes 𝑉 = 1, ..., 𝑛 denote the training data and edges 𝐸 represents
the similarities or affinity between adjacent nodes. Labels that as-
signed to the nodes in the graph can propagate along the edges of

the graph to their connected nodes. The assumption behind the
approach is that nodes with strong edges are more likely to share
the same label. Two representative graph-based algorithms are label
propagation [46] and label spreading [45], which we will introduce
in details in Section 3.1 and Section 3.2. Compared with other semi-
supervised learning algorithms, graph-based methods are fast and
easy to use due to its linear time complexity, and therefore explored
in this study.

2.3 SMOTE
Security data commonly suffer from data class imbalance issues.
Many prior works propose methods to rebalance the ratio between
classes to address this concern. SMOTE [8] (i.e., Synthetic Minority
Oversampling Technique) is a widely used oversampling technique
that works by randomly selecting samples from minority classes and
choosing 𝑘 nearest neighbors for each chosen sample. A synthetic
instance is created at a randomly selected point between each pair
of chosen sample and its neighbor. The synthetic samples are added
to the original dataset to balance the ratio between majority and
minority classes. Agrawal et al. proposed an auto-tuning version of
SMOTE, which is called SMOTUNED [1]. SMOTUNED adjusts sev-
eral key parameters of SMOTE such as 𝑘 (the number of neighbors
selected), 𝑚 (the number of synthetic samples to create), and 𝑟 (the
power parameter for the Minkowski distance metric). SMOTUNED
applies an evolutionary algorithm called differential evolution [35]
as the optimizer to explore SMOTE’s parameter space. Our study
uses SMOTUNED as our oversampling technique, but with a more
novel optimizer introduced in the following subsection.

2.4 Hyperparameter Optimization
Hyperparameter is a type of parameter in machine learning models
that can be estimated from data learning and have to be set before
model training [42]. Typically a hyperparameter has a known effect
on a model in the general sense, but it is not clear how to best
set a hyperparameter for a given dataset. In this sense, a range
of possibilities have to be explored. Hyperparameter optimization
or hyperparameter tuning is a technique that explores a range of
hyperparameters and searches for the optimal solution for a task.
Existing hyperparameter optimization methods can be mainly
categorized into the following groups [42]. The first group is decision-
theoretic methods. This kind of methods is based on the concept
of defining a search space and select combinations in the search
space. The most common methods of this type are grid search and
random search. Grid search (GS) [3] defines a search space as a grid
of hyperparameter values and exhaustively searches and evaluates
every position in the grid. Random search (RS) [4] defines a search
space as a bounded domain of hyperparameter values and randomly
samples points in that domain. Decision-theoretic methods are a
good choice when the search space is small and not complicated.
Metaheuristic algorithms such as genetic algorithms and particle
swarm optimization belongs to the second group. Genetic algo-
rithms (GA) [20] detect well-performing hyperparameter combina-
tions during each generation, and pass them to the next generation
until the optimal combination is found. In particle swarm optimiza-
tion (PSO) [21], each particle communicates with other particles
to detect and update the current global optimum in each iteration

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY

Rui Shu, Tianpei Xia, Huy Tu, Laurie Williams, Tim Menzies

until the final optimum is found. Metaheuristic algorithms suffer
from time-consuming problems when search space is large since
they are computationally expensive.

Unlike previous groups of methods, Bayesian optimization [30,
33] is a novel hyperparameter optimization technique that keeps
track of past evaluation results. The principle of Bayesian optimiza-
tion is using those results to build a probability model of objective
function, and maps hyperparameters to a probability of a score
on the objective function, and therefore uses it to select the most
promising hyperparameters to evaluate in the true objective func-
tion. This method is also called Sequential Model-Based Optimiza-
tion (SMBO) [14]. The probability representation of the objective
function is called surrogate function or response surface because it
is a high-dimensional mapping of hyperparameters to the probabil-
ity of a score on the objective function. The surrogate function is
much easier to optimize than the objective function and Bayesian
methods work by finding the next set of hyperparameters to evalu-
ate the actual objective function by selecting hyperparameters that
perform best on the surrogate function. This method continually
updates the surrogate probability model after each evaluation of
the objective function.

3 METHODOLOGY

Based on the above discussion, we were motivated to address our
security problems using a combination of semi-supervised learning
and hyperparameter optimization (and SMOTE when the data is im-
balanced). The basis of our framework about semi-supervised learn-
ing is an idea called pseudo-labeling [19]. Pseudo-labeling works by
iteratively propagating labels from labeled data to unlabeled data,
i.e., relabeling the unlabeled data with algorithms. Our framework
involves two pseudo-labeling approaches, i.e., label propagation
and label spreading, which have been the de facto standard method

of the inference phase in graph-based semi-supervised learning.
We first introduce both algorithms in detail and then present the
proposed framework.

3.1 Label Propagation
Label propagation (LP) algorithm [46] is analogous to the 𝑘-Nearest-
Neighbours algorithms and assumes that data points close to each
other tend to have a similar label. To be specific, LP is an iterative
algorithm that computes soft label assignments by pushing the
estimated label at each node to its neighbouring nodes based on the
edge weights. In other words, the new estimated label at each node
is calculated as the weighted sum of the labels of its neighbours.

More formally, if we consider two set: (𝑥1, 𝑦1), ...(𝑥𝑖, 𝑦𝑖 ) ∈ 𝐿 as la-
beled datasets, and (𝑥𝑖+1, 𝑦𝑖+1, ..., (𝑥𝑛, 𝑦𝑛) ∈ 𝑈 as unlabeled datasets,
where {𝑦1, ..., 𝑦𝑛 } ∈ {0, 1} in a binary classification problem, and
{𝑥1, ..., 𝑥𝑛 } ∈ R. LP constructs a graph 𝐺 = (𝑉 , 𝐸,𝑊 ) where 𝑉 is
the set of vertices representing set 𝐿 and 𝑈 , and the edges in set
𝐸 represents the similarity of two node 𝑖 and 𝑗 with weight 𝑊𝑖 𝑗 .
The weight 𝑊𝑖 𝑗 is computed in a way that two nodes with smaller
distance (i.e., more similar) will have a larger weight. Moreover, a
Laplacian transition matrix on 𝑉 is denoted as

𝑇𝑖 𝑗 =

𝑊𝑖 𝑗
(cid:205)𝑉𝑘 ∈𝑁 (𝑉𝑖 ) 𝑊𝑖𝑘

(1)

is used to propagate the labels.

There are two repeated steps involved in this algorithm until
the label assignment process converges. LP starts with an initial as-
signment, which is random for the unlabeled data points and equal
to the true labels for the labeled data points, then LP propagates
labels from each node to the neighbouring nodes and then reset
the predictions of the labeled data points to the corresponding true
labels. LP finally converges to a harmonic function and this process

Figure 1: An overview of the architecture of Dapper framework.

Reducing the Cost of Training Security Classifier (via Optimized Semi-Supervised Learning)

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY

can also be interpreted as a random walk with the transition matrix
and stops when a labeled node is hit.

3.2 Label Spreading
Label spreading (LS) algorithm [45] is a variant to the label propa-
gation algorithm. LS aims at minimizing a loss function that has
regularization properties which is more robust to noise data. In-
stead of using Laplacian transition matrix for propagation, LS uses
the normalized Laplacian matrix. A second different is the clamping
effect LS has on the label distribution. Clamping allows LS to adjust
the weight of the ground truth labeled data to some degree, rather
than using hard clamping of input labels in LP.

3.3 Dapper Framework
Figure 1 presents the framework of Dapper. We split the original
dataset int three parts, the training dataset, the validation dataset
and the testing dataset with a predefined ratio 𝑟1. The training
dataset is further split into two subsets with another ratio 𝑟2. One
subset is denoted as the labeled training dataset 𝑆𝐿. We remove all
the actual labels of the other subset 𝑆𝑈 and reset with default value
−1. We treat subset 𝑆𝑈 as the unlabeled training dataset.

The optimization module in the Dapper framework has two
sub-modules (as the blue dashed box shows in Figure 1), the semi-
supervised learning algorithm and the machine learning classifier.
Our framework is also adaptive, which means when Dapper detects
that the input training dataset is highly imbalanced (i.e., the percent-
age of minority class samples is lower than a predefined threshold
𝑡), Dapper automatically adds an oversampling sub-module. This
oversampling sub-module is based on a tuned version of SMOTE
which we discuss before in Section 2.3. The optimization process in
the framework proceeds with a fixed number of evaluation trails,
which is shown in Algorithm 1.

During each trail, we repeat the following steps:

(1) With a chosen SSL learner (label propagation algorithm or
label spreading algorithm), as well as a sampled combined set
of hyperparameters, we assign pseudo-labels to unlabeled
dataset with the algorithm.

(2) We concatenate the original labeled dataset and pseudo-

labeled dataset into a new training dataset.

(3) If the percentage of the minority class samples of training
dataset is lower than a threshold, we use SMOTE (with sam-
pled hyperparameters) to balance the class ratio of the new
training dataset. Otherwise, we pass this step.

(4) We train a classifier (with sampled hyperparameters) with
the new training dataset, and evaluate the trained classifier
on the validation dataset.

(5) The loss value of each trail, i.e. the complement of g-measure,

is logged.

After the optimization process, we rank all the evaluated classi-
fiers by the loss value. The one with the smallest loss is selected and
further tested on the testing dataset. Note that we sample the hyper-
parameters of SSL learner, SMOTE, and classifier in a combination
manner, in which way we address multiple optimization problems
simultaneously. Bayesian Optimization directs the whole sampling
process and searches for the next promising set of hyperparameters
after each trail from Table 2.

Algorithm 1: Pseudocode of Dapper’s optimization pro-
cess.
1 Function Dapper (𝐷𝑡𝑟𝑎𝑖𝑛𝑖𝑛𝑔, 𝐷𝑣𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛, 𝑟, 𝑡, 𝜃, 𝐹 );

Input

: Training dataset - 𝐷𝑡𝑟𝑎𝑖𝑛𝑖𝑛𝑔,
Validation dataset - 𝐷𝑣𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛,
SSL ratio - 𝑟 ,
Imbalance threshold - 𝑡 ,
Hyperparameter space - 𝜃 ,
Target function - 𝐹
Output : Optimized model 𝑐𝑙 𝑓𝑜𝑝𝑡𝑖𝑚𝑖𝑧𝑒𝑑

2 Split the training dataset into two subsets with ratio 𝑟 , which

treated as labeled dataset and unlabaled dataset

3 Reset labels of unlabeled dataset to −1
4 for 𝑡𝑟𝑎𝑖𝑙𝑖 ∈ number of Bayesian Optimization trails do
Sample a combined hyperparameter set 𝜃𝑖 ∈ 𝜃
5
Run SSL learner with 𝜃𝑖 and assign pseudo-labels
Concatenate training subsets into 𝐷𝑚𝑖𝑥𝑒𝑑 with actual labels
and pseudo-labels
if minority class percentage < 𝑡 then

6

7

8

9

10

11

12

Rebalance 𝐷𝑚𝑖𝑥𝑒𝑑 with SMOTE with 𝜃𝑖

Train classifier with 𝐷𝑚𝑖𝑥𝑒𝑑 with 𝜃𝑖
Evaluate trained classifier with 𝐷𝑣𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛
Compute loss towards target function 𝐹

13 Rank all optimization trails by loss with smallest on the top
14 return Optimized model 𝑐𝑙 𝑓𝑜𝑝𝑡𝑖𝑚𝑖𝑧𝑒𝑑

Besides, the reasons we design the Dapper framework in an
adaptive manner to address the class imbalance issue are twofold.
Firstly, data class imbalance is common in most security datasets,
and many prior studies hint that oversampling the dataset is more
likely to produce better classification performance. Secondly, we
hypothesize that standard semi-supervised learning algorithms do
not adequately address the class imbalance issue. This means, in the
mixed dataset with original labeled data and pseudo-labeled data,
the problem still remains. Our experimental results further confirm
the hypothesis, which we will discuss in detail in the result section.
In order to endorse the merits of Dapper (i.e., solving multiple
optimization problems), we also compare Dapper with two other
treatments:

(1) No optimization: with default SSL learner (default LP or de-

fault LS);

(2) Single optimization: with optimized SSL learner only (opti-

mized LP or optimized LS).

The first treatment is used to endorse the merit of hyperparameter
optimization, while the second treatment is used to demonstrate the
advantages of Dapper over tuning SSL learners only. Moreover, our
study performs sensitivity experiment, in which we pick different
value of the ratio 𝑟2 and explore the performance under each ratio.

4 EXPERIMENT

4.1 Datasets and Algorithm
Our proposed Dapper framework is evaluated with three security
datasets which cover different security tasks, such as spam detec-
tion, malware detection and network intrusion detection.

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY

Rui Shu, Tianpei Xia, Huy Tu, Laurie Williams, Tim Menzies

Table 2: Hyperparameter space explored in this study. This space covers two semi-supervised learning algorithms, SMOTE,
the Random Forest classifier and the imbalance threshold.

Item

Hyperparameter

Range

Brief Description

Label
Propagation

Label
Spreading

SMOTE

Random
Forest

Imbalance
Threshold

kernel
gamma
n_neighbors
max_iter
kernel
gamma
n_neighbors
alpha
max_iter
k
r
m
n_estimators
min_samples_leaf
min_samples_split
max_leaf_nodes
max_depth
max_features
bootstrap

’knn’, ’rbf’
(10, 30)
(5, 15)
(500, 1500)
’knn’, ’rbf’
(10, 30)
(5, 15)
(0.1, 0.9)
(500, 1500)
[1, 20]
[1, 6]
[50, 500]
[50, 200]
[1, 25]
[2, 25]
[2, 100]
[1, 25]
’auto’, ’sqrt’, ’log2’
’True’, ’False’

Kernel function to use.
Parameter for rbf kernel.
Parameter for knn kernel.
The maximum number of iterations allowed.
Kernel function to use.
Parameter for rbf kernel.
Parameter for knn kernel.
Clamping factor.
The maximum number of iterations allowed.
Number of neighbours.
Minkowski distance metric.
Number of synthetic samples.
The number of trees in the forest.
The minimum number of samples required to be at a leaf node.
The minimum number of samples required to be at an internal node.
Total number of leaf nodes in a tree.
The maximum depth of the tree.
The number of features to consider when looking for the best split.
Whether bootstrap samples are used when building trees.

t

30%

A threshold to control whether SMOTE is used or not.

Twitter Spam [9]. As spam on Twitter becomes a growing prob-
lem, researchers have adopted different machine learning algo-
rithms to detect Twitter spam. This dataset is generated from over
600 millions public tweets, and further labeled around 6.5 million
spam tweets with 12 features extracted. The ground truth is estab-
lished with Trend Micro’s Web Reputation Service, which identify
malicious tweets through URLs. We sample a total size of 5,000
instances from prior work [9] (which has a size of about 100k), with
4,758 non-spam tweets and 242 spam tweets. This dataset has 12
features, such as account age, number of followers of the twitter
user, the number of tweets the twitter user sent, etc.

Malware URLs [22]. The original dataset collects about 114,400
URLs initially, containing benign and malicious URLs in four cat-
egories: spam URLs, phishing URLs, website URLs distributing
malware and defacement URLs where pages belong to the trusted
but compromised sites. This work selects malware URLs as our
experimental target. In the selected dataset, more than 11,500 URLs
related to malware websites were obtained from DNS-BH which is
a project that maintain list of malware sites. There are 7,781 benign
URLs and 6,711 malicious URLs in the dataset, and 79 features such
as ratio of argument and URLs, count of token, and the proportion
of digits in the URL parts, etc.

CIC-IDS-2017 [31]. This dataset consists of labeled network
flows. It is comprised of both normal traffic and simulated abnormal
data caused by intentional attacks on a test network. This dataset
was constructed using the NetFlowMeter Network Traffic Flow
analyzer, which collected multiple network traffic features and
supported Bi-directional flows. We sample a portion of original
dataset, which includes 11,425 normal traffic and 2,714 abnormal
traffic. There are 70 features of the dataset, including average packet
size, mean packet length, total forward packets, etc.

As we show in Table 3, each dataset is split into training set,
validation set and testing set with a ratio of 6.4 : 1.6 : 2 in a stratified
way. To simulate the case of semi-supervised learning, we further
divide the training set into labeled data and unlabeled data with
different ratios in our sensitivity experiment. Another observation
from Table 3 is the imbalance rate, where two datasets suffer from
class imbalance issues.

We select random forest classifier as our machine learning algo-
rithm throughout the whole experiment. Random forest utilizes en-
semble learning which consists of multiple decision trees. The ‘for-
est’ generated by the algorithm is trained through bagging or boot-
strap aggregating. Random forest establishes the outcome based
on the predictions of the decision trees and predicts by taking the
average or mean of the output from various decision trees. There
are two reasons why we select random forest. Firstly, random forest
is commonly used as classifier in previous security tasks such as
intrusion detection [26]. Secondly, the implementation of random
forest in the Scikit-learn machine learning software library [27]
provides multiple hyperparameters that can be tuned (as can be
seen from Table 2).

Table 3: Details of the studied dataset.

Dataset

Twitter
Spam
Malicious
URLs
CIC-IDS-2017

Training
Set

Validation
Set

Testing
Set

Imbalance
Rate

3,200

9,274

9,048

800

1,000

4.84%

2,319

2,263

2,899

2,828

46.3%

19.2%

Reducing the Cost of Training Security Classifier (via Optimized Semi-Supervised Learning)

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY

Furthermore, the implementation of both label propagation algo-
rithm and label spreading algorithm we adopt are publicly available
in Scikit-learn, and the autotuned version of SMOTE is implemented
according to [1].

4.2 Evaluation Metrics
If we let TP, TN, FP, FN to denote true positives, true negatives,
false positives, and false negatives (respectively), we note that recall
(pd), false positive rate (pf), g-measure (g-score), precision (prec),
and f-measure (f1) are defined as follows:

𝑝𝑑 =

𝑝 𝑓 =

𝑇 𝑃
𝑇 𝑃 + 𝐹 𝑁
𝐹 𝑃
𝐹 𝑃 + 𝑇 𝑁

𝑔 − 𝑠𝑐𝑜𝑟𝑒 =

2 ∗ 𝑝𝑑 ∗ (100 − 𝑝 𝑓 )
𝑝𝑑 + (100 − 𝑝 𝑓 )

𝑝𝑟𝑒𝑐 =

𝑇 𝑃
𝑇 𝑃 + 𝐹 𝑃
2 ∗ 𝑝𝑑 ∗ 𝑝𝑟𝑒𝑐
𝑝𝑑 + 𝑝𝑟𝑒𝑐

𝑓 1 =

(2)

(3)

(4)

(5)

(6)

where 1) recall represents the ability of one algorithm to identify
instances of positive class from the given dataset; 2) false positive
rate measures the instances that are falsely classified by an algo-
rithm as positive which are actually negative; 3) g-measure is the
harmonic mean of recall and the complement of false positive rate,
and it is also our optimization goal. We also report AUC-ROC value
(i.e., Area Under the Receiver Operating Characteristics) for the
completeness of results. This metric is an important metric to tell
how much the model is capable to distinguish between classes. The
higher, the better. In the worst case, the value is 0.5, which means
the model has no discrimination ability between positive and nega-
tive class. Note that this study does not focus on other metrics such
as precision or accuracy, as these metrics would fail to demonstrate
the ability of a model under imbalanced classification. We report
f-measure also for the completeness concern.

5 EVALUATION RESULTS

Our study is structured around the following research questions:

RQ1. Can we use less labeled training data with default SSL?

This research question explores whether we can use less labeled
training data than supervised learning with default semi-supervised
learning algorithms. Specifically, we will try to build predictors
using 10, 20, 30...90, 100% of the labelled data.

To answer this question, we first present results from two base-
line treatments: 1) results from prior works which published and
evaluated the original datasets; 2) results from sampled datasets
with 100% labeled training datasets used in training. Table 4 presents
the baseline results and all the results come from the same classi-
fier, i.e., random forest. There are several notes about the results:
1) Some prior works did not report results of some metrics, e.g.
g-measure, which are denoted as N/A in the table; 2) Some prior
work did not present details of data split design, hence we have no

idea the exact size of training dataset to get those results, hence
we denote as a portion of the whole original data size; 3) All the
results are from the same machine learning algorithm, i.e., random
forest, and the summary results indicate that all the datasets are
able to achieve good performance, e.g., about 90% recall, except the
sampled dataset from Twitter spam, which can only achieve about
60% recall with 100% labeled training data.

Table 5, Table 6 and Table 7 present the results of Twitter spam
dataset, malware URLs dataset and CIC-IDS-2017 dataset, respec-
tively. Each table reports the results from evaluation metrics as
defined in Section 4.2. Note that we use different label rate of the
training dataset, and we report the results from each label rate.

There are several observations from these results:

(1) For each datasets, with default label propagation (Default LP)
algorithm, the recall results show a decreasing trend when
the label rate decreases. When the label rate is as low as 10%,
the recall results of label propagation is close to zero.

(2) The results of default label spreading (Default LS) vary among
all datasets. For example, for Twitter spam in Table 5, label
spreading algorithm can achieve as low as half of recall of

Table 4: Summary results of 1) prior work which published
the dataset; 2) 100% training data used in a supervised way;
and 3) 10% of training dataset used with Dapper on label
spreading. All results are based on the random forest clas-
sifier.

(a) Twitter Spam

Results from
Prior Work [9]

100% labeled
data used

Dapper

92.9

7.1

N/A
N/A
56.6
A portion of
over 100k

58.3

0.1

73.6
79.1
72.7

3200

85.4

9.7

87.8
87.8
45.1

320

(b) Malware URLs

Results from
Prior Work [22]

100% labeled
data used

Dapper

99.0

N/A

N/A
N/A
99.0

9,274

99.2

0.4

99.4
99.4
99.3

9,274

94.0

0.6

96.6
96.7
96.6

927

(c) CIC-IDS-2017

Results from
Prior Work [31]

100% labeled
data used

Dapper

97.0

N/A

N/A
N/A
97.0
A portion of
over 2.8m

98.3

0.3

99.0
99.0
98.6

9,048

96.3

6.3

95.0
95.0
86.5

904

Metric

Recall
False Positive
Rate
G-measure
AUC-ROC
F-measure
Size of
Training Data

Metric

Recall
False Positive
Rate
G-measure
AUC-ROC
F-measure
Size of
Training Data

Metric

Recall
False Positive
Rate
G-measure
AUC-ROC
F-measure
Size of
Training Data

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY

Rui Shu, Tianpei Xia, Huy Tu, Laurie Williams, Tim Menzies

Table 5: Results of the Twitter Spam dataset. The best results
of each metric from different treatments in each label rate
are highlighted in blue color. LP,LS= label propagation and
label spreading (described in §3.1 and §3.2).

Table 6: Results of the Malware URLs dataset. The best re-
sults of each metric from different treatments in each label
rate are highlighted in blue color. LP,LS= label propagation
and label spreading (described in §3.1 and §3.2).

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

90%
52.1
52.1
87.5
58.3
58.3
87.5

90%
68.4
68.4
89.1
73.7
73.7
89.1

90%
76.0
76.0
89.1
79.2
79.2
89.1

90%
0.0
0.0
9.2
0.0
0.0
9.2

90%
68.5
68.5
47.2
73.7
73.7
47.2

(a) Recall

70%
50.0
60.4
85.4
56.3
58.3
85.4

60%
27.1
68.8
81.3
68.8
68.8
87.5

50%
10.4
60.4
85.4
66.7
68.8
87.5

(b) G-Measure

70%
66.7
75.3
88.7
72.0
73.7
89.3

60%
42.6
81.4
86.7
81.4
81.4
88.3

50%
18.9
75.3
89.2
79.9
81.4
91.1

(c) AUC-ROC

70%
75.0
80.1
88.8
78.1
79.2
89.5

60%
63.5
84.3
87.1
84.3
84.3
88.3

50%
55.2
80.1
89.4
83.2
84.2
91.2

80%
50.0
60.4
83.3
60.4
62.5
87.5

80%
66.7
75.3
87.3
75.3
76.9
89.1

80%
75.0
80.2
87.5
80.2
81.3
89.1

40%
2.1
52.1
83.3
68.8
68.8
85.4

40%
4.1
68.4
89.4
81.4
81.4
89.1

40%
51.0
75.9
89.9
84.3
84.3
89.3

(d) False Positive Rate

80%
0.0
0.0
8.3
0.0
0.0
9.2

80%
66.7
75.3
47.9
75.3
76.9
47.2

70%
0.0
0.2
7.7
0.1
0.0
6.5

60%
0.0
0.2
7.0
0.2
0.2
10.9

50%
0.0
0.2
6.6
0.3
0.3
5.0

(e) F-Measure

70%
66.7
73.4
50.3
71.1
73.7
54.3

60%
42.6
79.5
50.6
79.5
79.5
43.3

50%
18.9
73.4
53.9
77.1
78.6
60.9

40%
0.0
0.2
3.6
0.2
0.3
6.8

40%
4.1
66.7
65.6
79.5
78.7
53.2

30%
2.1
33.3
83.3
50.0
66.7
85.4

30%
4.1
50.0
86.0
66.6
79.9
88.9

30%
51.0
66.7
86.0
74.9
83.2
89.1

30%
0.0
0.0
11.2
0.1
0.3
7.2

30%
4.1
0.5
41.0
65.8
77.1
51.9

20%
0.0
8.3
83.3
43.8
52.1
85.4

20%
0.0
15.4
83.1
60.8
68.4
85.3

20%
50.0
54.2
83.1
71.7
75.8
85.3

20%
0.0
0.0
17.2
0.3
0.5
14.8

20%
0.0
15.4
31.7
58.3
64.1
35.7

10%
0.0
0.0
72.9
29.2
39.6
85.4

10%
0.0
0.0
81.4
45.2
56.7
87.8

10%
50.0
50.0
82.5
64.5
69.7
87.8

10%
0.0
0.0
8.0
0.1
0.2
9.7

10%
0.0
0.0
44.0
44.4
55.1
45.1

40%
26.5
94.5
97.1
97.2
97.5
98.0

40%
41.8
96.6
97.7
98.3
98.3
98.4

30%
11.5
76.2
79.2
96.3
97.7
97.1

30%
20.6
86.4
88.2
97.8
97.9
98.1

20%
5.7
66.9
69.1
95.2
95.8
96.3

20%
10.7
80.1
81.7
97.2
97.4
97.7

10%
1.8
48.1
49.2
91.2
91.2
94.0

10%
3.5
64.9
65.9
94.9
94.9
96.6

(a) Recall

80%
91.2
98.1
98.2
98.3
98.6
98.8

70%
84.4
98.1
98.1
98.3
98.0
98.5

60%
70.3
97.2
97.5
98.1
97.9
98.4

50%
48.7
97.3
97.1
97.8
97.6
97.8

(b) G-Measure

70%
91.5
98.4
98.5
98.7
98.8
98.9

60%
82.6
97.7
98.3
98.6
98.7
98.7

50%
65.5
97.9
98.0
98.2
98.4
98.6

(c) AUC-ROC

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

90%
96.3
98.3
98.4
98.2
99.0
99.0

90%
97.9
98.1
98.8
98.7
99.2
99.3

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

90%
97.9
98.1
98.8
98.7
99.2
99.3

80%
95.3
98.7
98.7
98.7
99.1
99.2

80%
95.5
98.7
98.7
98.7
99.1
99.2

70%
92.1
98.4
98.5
98.7
98.8
98.9

60%
85.1
97.7
98.3
98.6
98.7
98.7

50%
74.4
97.9
98.0
98.2
98.4
98.6

40%
63.2
96.7
97.8
98.3
98.3
98.4

30%
55.7
88.0
89.4
97.8
97.9
98.1

20%
52.8
83.4
84.5
97.2
97.5
97.7

10%
50.9
74.0
74.5
95.0
95.0
96.7

(d) False Positive Rate

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

90%
0.3
0.6
0.7
0.9
0.5
0.5

90%
97.8
98.0
98.7
98.6
99.2
99.2

80%
0.2
0.6
0.8
0.8
0.4
0.4

80%
95.3
98.7
98.6
98.7
99.0
99.2

70%
0.1
1.1
0.9
0.9
0.4
0.6

60%
0.0
0.7
0.9
1.0
0.5
1.0

50%
0.0
1.4
1.1
1.3
0.7
0.6

(e) F-Measure

70%
91.4
98.4
98.5
98.6
98.8
98.9

60%
82.6
97.6
98.2
98.5
98.6
98.6

50%
65.5
97.8
97.9
98.1
98.3
98.5

40%
0.0
1.1
1.6
0.6
0.8
1.2

40%
41.8
96.5
97.6
98.2
98.2
98.3

30%
0.0
0.2
0.4
0.7
1.8
0.9

30%
20.6
86.4
88.1
97.7
97.8
98.0

20%
0.1
0.2
0.1
0.8
0.8
0.9

20%
10.7
80.1
81.7
97.1
97.3
97.6

10%
0.0
0.1
0.1
1.1
1.2
0.6

10%
3.5
64.9
65.9
94.7
94.7
96.6

results when compared with supervised learning with 100%
labeled training data in Table 4. For the rest two datasets,
the recall results are much better, as we can achieve about
90% of original recall performance even the label rate is low
as 10%.

Answer 1

Default semi-supervised learning algorithm such as label
propagation cannot achieve ideal performance when label
rate is low, while label spreading algorithm is much better
than label propagation, but still cannot compared with
supervised learning results for some datasets.

RQ2. Will hyperparameter optimization on SSL help improve the
results?

We also present the results of all datasets with optimized semi-
supervised learning algorithms. In this treatment, we only apply
Bayesian optimization to seach the hyperparameter space of label
propagation and label spreading, and use default random forest
algorithm as well as not using SMOTE.

The results of using optimized semi-supervised learning algo-

rithms are also several folds:

(1) For the Twitter spam dataset, the optimized label propagation
algorithm (Optimized LP) cannot improve the recall results
when the label rate is as low as 10%, while the optimized
label spreading algorithm (Optimized LS) can improve the
recall from 29.2% to 39.6%, but still cannot achieve 58.3%
recall with 100% label dataset used, and even far below 92.9%
original recall from prior work [9].

(2) For the malware URLs dataset and CIC-IDS-2017 dataset,
the improved performance over default label propagation
(Default LP) is obvious, but still not enough. However, for

Reducing the Cost of Training Security Classifier (via Optimized Semi-Supervised Learning)

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY

Table 7: Results of the CIC-IDS-2017 dataset. The best results
of each metric from different treatments in each label rate
are highlighted in blue color. LP,LS= label propagation and
label spreading (described in §3.1 and §3.2).

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Label Rate
Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

90%
95.9
97.6
99.4
98.1
98.3
98.1

90%
97.9
98.7
98.1
98.9
99.1
97.2

90%
97.9
98.7
98.1
98.9
99.1
97.2

90%
0.1
0.2
3.2
0.2
0.2
3.7

90%
97.7
98.2
93.4
98.7
98.8
91.9

(a) Recall

70%
80.1
97.4
99.6
97.2
97.4
99.2

60%
67.6
95.7
99.1
95.8
96.5
97.6

50%
50.3
94.6
98.9
95.0
96.5
99.4

(b) G-Measure

70%
88.9
98.4
98.5
98.3
98.4
97.2

60%
80.7
97.6
97.9
97.4
97.9
96.5

50%
66.9
96.7
97.9
96.8
97.8
97.5

(c) AUC-ROC

70%
90.0
98.4
98.5
98.3
98.4
97.3

60%
83.8
97.6
97.9
97.5
97.9
96.6

50%
75.1
96.7
97.9
96.8
97.8
97.5

80%
90.1
97.4
99.8
97.2
97.2
99.4

80%
94.7
98.5
98.4
98.4
98.4
96.9

80%
95.0
98.5
98.4
98.4
98.4
97.0

40%
28.9
90.8
97.6
93.0
95.2
98.5

40%
44.8
94.6
98.1
95.8
97.1
97.4

40%
64.4
94.8
98.1
95.8
97.1
97.5

(d) False Positive Rate

80%
0.1
0.4
2.9
0.4
0.4
5.4

80%
94.5
97.9
94.1
97.8
97.8
89.5

70%
0.0
0.5
2.6
0.6
0.6
4.7

60%
0.0
0.5
3.3
0.8
0.5
4.5

50%
0.0
1.2
3.0
1.3
0.8
4.4

(e) F-Measure

70%
88.9
97.6
94.7
97.2
97.4
90.7

60%
80.7
96.7
93.0
96.2
97.1
90.2

50%
66.9
94.7
93.5
94.8
96.5
91.3

40%
0.0
1.2
1.3
1.3
1.0
3.6

40%
44.8
92.7
96.1
93.8
95.5
92.2

30%
17.3
82.1
97.6
91.7
94.1
98.0

30%
29.5
89.5
98.3
95.0
96.4
96.5

30%
58.6
90.3
98.3
95.1
96.4
96.5

30%
0.0
1.6
0.9
1.5
1.2
4.8

30%
29.5
87.0
96.8
92.6
94.5
89.7

20%
8.1
42.3
96.8
90.2
93.2
97.4

20%
15.0
59.5
97.6
93.9
95.7
96.5

20%
54.1
71.0
97.6
94.1
95.7
96.5

20%
0.0
0.3
1.6
2.1
1.7
4.5

20%
15.0
59.1
95.2
90.6
93.0
90.1

10%
3.3
19.3
95.0
87.3
87.8
96.3

10%
6.4
32.4
96.3
92.0
92.5
95.0

10%
51.6
59.6
96.3
92.3
92.8
95.0

10%
0.0
0.2
2.4
2.7
2.2
6.3

10%
6.4
32.2
92.6
87.9
89.2
86.5

the label spreading algorithm, the optimized improvement
is slight, since the default label spreading algorithm (Default
LS) already has achieved a high performance.

(3) Furthermore, by comparing optimized label propagation and
optimized label spreading from all result tables, it is not
hard to say optimized label spreading algorithm outperforms
optimized label propagation, especially when the label rate
is low. For example, for the Twitter spam dataset, the recall
results from optimized label spreading (Optimized LS) is
39.6%, which is far better than 0.0% recall from optimized
label propagation (Optimized LP).

(4) For other metrics such as AUC-ROC and F-measure, the
advantages of optimization is similar to the recall results over
default SSL settings. Besides, optimized label spreading is
also better than optimized label propagation in these metrics.

Answer 2

Compared with default settings, hyperparameter optimiza-
tion on semi-supervised algorithms can alleviate the de-
creasing trend of metrics such as recall and g-measure in
label propagation, but still not good enough. Besides, the
optimized label spreading algorithm show slight improve-
ment over default label spreading.

RQ3. Can we endorse the merits of the Dapper framework?

We now present the results of Dapper as we introduce before
in Section 3.3. Note that we hypothesize standard semi-supervised
learning algorithms do not adequately address the class imbalance
issue. Figure 2 from the Twitter spam dataset validates our hypoth-
esis. As we observe from the figure, the percentage of minority
class in original labeled dataset is about 4.6%. When decreasing
the label rate, the imbalanced issue even get worse. For example,
with only 10% of labeled data, the ratio drops to about 0.5% with
label spreading and 0.3% with label propagation. Several other prior
studies also report similar findings [16, 44]. This might result from
the pseudo-labeling process, which infers the original minority
class even to majority class. This finding also indicates the class
imbalance issue still remains, and should not be ignored, which
motivates us to add SMOTE to resample the mixed dataset in the
adaptive Dapper framework.

Compared with default semi-supervised learning algorithms and
optimized semi-supervised learning algorithms only, we make the
following remarks on Dapper:

(1) In recall, g-measure and AUC-ROC, Dapper is more advan-
tageous over other treatments in different levels. Even with
label propagation, in the Twitter spam dataset and CIC-IDS-
2017 dataset, Dapper can greatly increase the recall perfor-
mance with 10% label rate.

(2) As Dapper with label spreading (Dapper + LS) is slightly
better than with label propagation (Dapper + LP) on Twit-
ter spam and CIC-IDS-2017, but for malware URLs dataset,
Dapper with label spreading is far better than with label
propagation.

(3) What’s more important, the results of Dapper is almost stable

under different label rates.

(4) However, we have to note that, Dapper might bring in an
increment of false positive rate (e.g., in Twitter spam and
CIC-IDS-2017). Since the minority class ratio is lower than
our pre-defined threshold, Dapper adaptively applies opti-
mized SMOTE in the framework, which might cause the
issue. We argue that, considering the improvement of impor-
tant metrics such as recall, the trade-off of such increment
in false positive rate is still acceptable.

Lastly, let’s revisit the results from Table 4. The last column of this
table comes from Dapper, in which label spreading is selected as the
SSL learner, and only 10% of labeled training data is used. Compared
with results from column 3 in which 100% labeled data is used in a
supervised paradigm, Dapper is close or even better in recall (with
an acceptable trade-off in false positive rate). Compared with prior
works which publish the dataset, Dapper shows obvious advantage

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY

Rui Shu, Tianpei Xia, Huy Tu, Laurie Williams, Tim Menzies

Section 2.4, and we believe Bayesian Optimization is good enough
in our study.

Learner Bias. Research into automatic classifiers is a large and
active field. Different machine learning algorithms have been devel-
oped to solve various classification problem tasks. Any data-mining
study, such as this paper, can only use a small subset of the known
classification algorithms. We select the random forest classifier,
commonly used in similar classification tasks, for this work. In the
future, we plan to explore more popular classifiers such as support
vector machines (SVM), XGBoost [10], and so on.

Implementation Bias. The implementation of semi-supervised
learning algorithms and the random forest classifier are from the
Scikit-learn library, the implementation of Bayesian Optimization
is from the hyperopt library [5], and the implementation of the
tunable SMOTE is from scratch by following the idea from [1]
without using existing available libraries. Different implementation
of the above algorithms might have impact on the performance
results, and might even change the conclusions from this work.

Input Bias. Our results come from the space of hyperparameter
optimization listed in Table 2. In theory, other ranges might lead
to other results. That said, our goal here is not to offer the best
optimization but to argue that the optimized algorithms provided
by Dapper can help reduce the ratio of labeled data required to a low
degree, while still achieve promising performance, just by itself. For
those purposes, we would argue that our current hyperparameter
space suffices.

7 CONCLUSION

When labeled data is scarce, it can be hard to build adequately
good prediction models. Prior works in software security have tried
to address this issue with semi-supervised learning using a small
pool of existing labels to infer the labels of unlabeled data. Those
works usually do not explore SSL hyperparameter optimization (or
even data rebalancing with SSL). This paper checks if that was a
deficiency in prior works.

To perform that check, we propose Dapper that explores the
hyperparameter space of existing semi-supervised learning algo-
rithms, i.e., label propagation and label spreading, and machine
learning classifier. When the percentage of minority class is low,
Dapper further adaptively integrates an optimized oversampler
SMOTE into the framework to address the class imbalance issue.
Experimental results with three datasets show that Dapper’s hyper-
parameter optimization and rebalancing combination can efficiently
improve classification performance, even when most labels (90%)
are unavailable. In some datasets, we even observe better results
with Dapper (using 10% of the data) than using 100% of all the la-
bels. Based on those results, we recommend using hyperparameter
optimization when dealing with label shortages for security tasks.

REFERENCES
[1] Amritanshu Agrawal and Tim Menzies. 2018.

Is" Better Data" Better Than"
Better Data Miners"?. In 2018 IEEE/ACM 40th International Conference on Software
Engineering (ICSE). IEEE, 1050–1061.

[2] Ibrahim Alabdulmohsin, YuFei Han, Yun Shen, and Xiangliang Zhang. 2016.
Content-agnostic malware detection in heterogeneous malicious distribution
graph. In Proceedings of the 25th ACM International on Conference on Information
and Knowledge Management. 2395–2400.

Figure 2: The percentage of minority class under different
label rates after using default label propagation and default
label spreading in the Twitter spam dataset.

Table 8: Average runtime (in minutes) of different treat-
ments. We set the number of Bayesian Optimization trails
to 100.

Algorithm

Default LP
Optimized LP
Dapper + LP
Default LS
Optimized LS
Dapper + LS

Dataset

Twitter
Spam

Malicious
URLs

CIC-IDS-2017

< 1
< 3
< 4
< 1
< 2
< 3

< 2
< 10
< 12
< 2
< 6
< 10

< 2
< 10
< 15
< 2
< 6
< 10

in the labeled data size required. In addition, Table 8 shows the
average runtime of different treatments, which also indicates the
Dapper framework is also practical to use. The result also suggests
that Dapper is a promising alternative to reduce the size of labeled
data required to train a useful model.

Answer 3

The adaptive Dapper framework with label spreading pro-
vides a close or even better performance than supervised
learning with 100% labeled training data, but with as low
as 10% of original labeled data required.

6 THREATS TO VALIDITY

Evaluation Bias. In our work, we choose some popular eval-
uation metrics for classification tasks and use g-measure as the
optimization objective. We do not use other metrics because rele-
vant information is not available to us, or we think they are not
suitable enough for this specific task (e.g., precision).

Optimizer Bias. Dapper framework optimizes semi-supervised
learning algorithm, machine learning classifier, or SMOTE with
Bayesian Optimization. We do not claim Bayesian Optimization is
the only best choice, but argue that Bayesian Optimization is fast
to run and also more promising than other methods discussed in

Reducing the Cost of Training Security Classifier (via Optimized Semi-Supervised Learning)

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY

[30] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Fre-
itas. 2015. Taking the human out of the loop: A review of Bayesian optimization.
Proc. IEEE 104, 1 (2015), 148–175.

[31] Iman Sharafaldin, Arash Habibi Lashkari, and Ali A Ghorbani. 2018. Toward gen-
erating a new intrusion detection dataset and intrusion traffic characterization..
In ICISSP. 108–116.

[32] Nadia Felix F Da Silva, Luiz FS Coletta, and Eduardo R Hruschka. 2016. A survey
and comparative study of tweet sentiment analysis via semi-supervised learning.
ACM Computing Surveys (CSUR) 49, 1 (2016), 1–26.

[33] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian
optimization of machine learning algorithms. In Advances in neural information
processing systems. 2951–2959.

[34] Alireza Souri and Rahil Hosseini. 2018. A state-of-the-art survey of malware
detection approaches using data mining techniques. Human-centric Computing
and Information Sciences 8, 1 (2018), 1–22.

[35] Rainer Storn and Kenneth Price. 1997. Differential evolution–a simple and
efficient heuristic for global optimization over continuous spaces. Journal of
global optimization 11, 4 (1997), 341–359.

[36] Rahim Taheri, Reza Javidan, Mohammad Shojafar, Zahra Pooranian, Ali Miri,
and Mauro Conti. 2020. On defending against label flipping attacks on malware
detection systems. Neural Computing and Applications 32, 18 (2020), 14781–14800.
[37] Huy Tu, Zhe Yu, and Tim Menzies. 2020. Better data labelling with emblem (and
how that impacts defect prediction). IEEE Transactions on Software Engineering
(2020).

[38] Jesper E Van Engelen and Holger H Hoos. 2020. A survey on semi-supervised

learning. Machine Learning 109, 2 (2020), 373–440.

[39] Haobo Wang, Zhao Li, Jiaming Huang, Pengrui Hui, Weiwei Liu, Tianlei Hu, and
Gang Chen. 2021. Collaboration based multi-label propagation for fraud detection.
In Proceedings of the Twenty-Ninth International Conference on International Joint
Conferences on Artificial Intelligence. 2477–2483.

[40] Ruowen Wang, William Enck, Douglas Reeves, Xinwen Zhang, Peng Ning, Ding-
bang Xu, Wu Zhou, and Ahmed M Azab. 2015. Easeandroid: Automatic policy
analysis and refinement for security enhanced android via large-scale semi-
supervised learning. In 24th {USENIX} Security Symposium ({USENIX} Security
15). 351–366.

[41] Tingmin Wu, Sheng Wen, Yang Xiang, and Wanlei Zhou. 2018. Twitter spam
detection: Survey of new approaches and comparative study. Computers &
Security 76 (2018), 265–284.

[42] Li Yang and Abdallah Shami. 2020. On hyperparameter optimization of machine
learning algorithms: Theory and practice. Neurocomputing 415 (2020), 295–316.
[43] Fuzhi Zhang, Xiaoyan Hao, Jinbo Chao, and Shuai Yuan. 2020. Label propagation-
based approach for detecting review spammer groups on e-commerce websites.
Knowledge-Based Systems 193 (2020), 105520.

[44] Zhi-Wu Zhang, Xiao-Yuan Jing, and Tie-Jian Wang. 2017. Label propagation based
semi-supervised learning for software defect prediction. Automated Software
Engineering 24, 1 (2017), 47–69.

[45] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard
Schölkopf. 2003. Learning with local and global consistency. Advances in neural
information processing systems 16 (2003).

[46] Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled

data with label propagation. (2002).

[47] Xiaojin Zhu and Andrew B Goldberg. 2009. Introduction to semi-supervised
learning. Synthesis lectures on artificial intelligence and machine learning 3, 1
(2009), 1–130.

[48] Xiaojin Jerry Zhu. 2005. Semi-supervised learning literature survey. (2005).

[3] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms
for hyper-parameter optimization. Advances in neural information processing
systems 24 (2011).

[4] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter

optimization. Journal of machine learning research 13, 2 (2012).

[5] James Bergstra, Dan Yamins, David D Cox, et al. 2013. Hyperopt: A python
library for optimizing the hyperparameters of machine learning algorithms. In
Proceedings of the 12th Python in science conference, Vol. 13. Citeseer, 20.

[6] Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with
co-training. In Proceedings of the eleventh annual conference on Computational
learning theory. 92–100.

[7] M Emre Celebi and Kemal Aydin. 2016. Unsupervised learning algorithms.

Springer.

[8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer.
2002. SMOTE: synthetic minority over-sampling technique. Journal of artificial
intelligence research 16 (2002), 321–357.

[9] Chao Chen, Jun Zhang, Xiao Chen, Yang Xiang, and Wanlei Zhou. 2015. 6 million
spam tweets: A large ground truth for timely Twitter spam detection. In 2015
IEEE international conference on communications (ICC). IEEE, 7065–7070.
[10] Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu
Cho, et al. 2015. Xgboost: extreme gradient boosting. R package version 0.4-2 1, 4
(2015), 1–4.

[11] Michael Crawford, Taghi M Khoshgoftaar, Joseph D Prusa, Aaron N Richter,
and Hamzah Al Najada. 2015. Survey of review spam detection using machine
learning techniques. Journal of Big Data 2, 1 (2015), 1–24.

[12] Amirhossein Gharib, Iman Sharafaldin, Arash Habibi Lashkari, and Ali A Ghor-
bani. 2016. An evaluation framework for intrusion detection dataset. In 2016
International Conference on Information Science and Security (ICISS). IEEE, 1–6.
[13] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. Overview of
supervised learning. In The elements of statistical learning. Springer, 9–41.
[14] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2011. Sequential model-
based optimization for general algorithm configuration. In International confer-
ence on learning and intelligent optimization. Springer, 507–523.

[15] Amazon Mechanical Turk Inc. 2021. Amazon Mechanical Turk. https://www.

mturk.com/

[16] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. 2019. Label
propagation for deep semi-supervised learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 5070–5079.

[17] Bojan Kolosnjaji, Apostolis Zarras, Tamas Lengyel, George Webster, and Claudia
Eckert. 2016. Adaptive semantics-aware malware classification. In International
Conference on Detection of Intrusions and Malware, and Vulnerability Assessment.
Springer, 419–439.

[18] Duc C Le, Nur Zincir-Heywood, and Malcolm Heywood. 2021. Training regime
influences to semi-supervised learning for insider threat detection. In 2021 IEEE
Security and Privacy Workshops (SPW). IEEE, 13–18.

[19] Dong-Hyun Lee et al. 2013. Pseudo-label: The simple and efficient semi-
supervised learning method for deep neural networks. In Workshop on challenges
in representation learning, ICML, Vol. 3. 896.

[20] Stefan Lessmann, Robert Stahlbock, and Sven F Crone. 2005. Optimizing hyper-
parameters of support vector machines by genetic algorithms.. In IC-AI. 74–82.
[21] Pablo Ribalta Lorenzo, Jakub Nalepa, Michal Kawulok, Luciano Sanchez Ramos,
and José Ranilla Pastor. 2017. Particle swarm optimization for hyper-parameter
selection in deep neural networks. In Proceedings of the genetic and evolutionary
computation conference. 481–488.

[22] Mohammad Saiful Islam Mamun, Mohammad Ahmad Rathore, Arash Habibi
Lashkari, Natalia Stakhanova, and Ali A Ghorbani. 2016. Detecting malicious
urls using lexical analysis. In International Conference on Network and System
Security. Springer, 467–482.

[23] Ming Ni, Qianmu Li, Hong Zhang, Tao Li, and Jun Hou. 2015. File relation graph
based malware detection using label propagation. In International Conference on
Web Information Systems Engineering. Springer, 164–176.

[24] Eric Nunes, Ahmad Diab, Andrew Gunn, Ericsson Marin, Vineet Mishra, Vivin
Paliath, John Robertson, Jana Shakarian, Amanda Thart, and Paulo Shakarian.
2016. Darknet and deepnet mining for proactive cybersecurity threat intelligence.
In 2016 IEEE Conference on Intelligence and Security Informatics (ISI). IEEE, 7–12.
[25] Sai C Pallaprolu, Josephine M Namayanja, Vandana P Janeja, and CT Sai Adithya.
2016. Label propagation in big data to detect remote access Trojans. In 2016 IEEE
International Conference on Big Data (Big Data). IEEE, 3539–3547.

[26] Paulo Angelo Alves Resende and André Costa Drummond. 2018. A survey of
random forest based methods for intrusion detection systems. ACM Computing
Surveys (CSUR) 51, 3 (2018), 1–36.

[27] scikit-learn developers. 2021. Scikit-learn. https://scikit-learn.org/0.15/modules/

label_propagation.html

[28] Henry Scudder. 1965. Probability of error of some adaptive pattern-recognition
machines. IEEE Transactions on Information Theory 11, 3 (1965), 363–371.
[29] Burr Settles and Mark Craven. 2008. An analysis of active learning strategies
for sequence labeling tasks. In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing. 1070–1079.

