Survey on Tools and Techniques Detecting
Microservice API Patterns

Alexander Bakhtin
Tampere University
Tampere, Finland
alexander.bakhtin@tuni.ﬁ

Abdullah Al Maruf
Computer Science, Baylor University
Waco, TX, USA
maruf maruf1@baylor.edu

Tomas Cerny
Computer Science, Baylor University
Waco, TX, USA
tomas cerny@baylor.edu

Davide Taibi
Tampere University
Tampere, Finland
davide.taibi@tuni.ﬁ

2
2
0
2

y
a
M
0
2

]
E
S
.
s
c
[

1
v
3
3
1
0
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—It is well recognized that design patterns improve
system development and maintenance in many aspects. While we
commonly recognize these patterns in monolithic systems, many
patterns emerged for cloud computing, speciﬁcally microservices.
Unfortunately, while various patterns have been proposed, avail-
able quality assessment tools often do not recognize many. This
article performs a grey literature review to ﬁnd and catalog
available tools to detect microservice API patterns (MAP). It
reasons about mechanisms that can be used to detect these pat-
terns. Furthermore, the results indicate gaps and opportunities
for improvements for quality assessment tools. Finally, the reader
is provided with a route map to detection techniques that can be
used to mine MAPs.

Index Terms—Microservice API Patterns, Design Patterns,
Best Practices, Pattern Matching, Static Analysis, Dynamic Anal-
ysis, Pattern Mining Tools

I. INTRODUCTION

Design patterns provide a generalized and reusable solution
to common software design problems. They indicate that a
system uses best practices. Patterns are well-recognized to
improve software quality [1]. One of the reasons behind such
impact is better software comprehension related to software
documentation. Detection of patterns is a complex task [2],
[3]. It can constitute static or dynamic system analysis and
serves the purpose of quality assurance and quality indices of
system design.

With the era of cloud systems, we must assume new patterns
emerge that constitute possibly across multiple self-contained
parts of the overall system. In the current context, cloud
computing is fueled by the microservice architecture [4].
Microservices are small and autonomous services deployed
independently, with a single and clearly deﬁned purpose [5],
[6]. We ask what techniques and state-of-the-art tools can
detect microservice patterns and the current gaps in this
context.

Many challenges need to be taken into account when
detecting patterns in cloud-based systems. For instance, the
cloud-native development best practices [4] suggest separat-
ing microservice codebases to enable decentralized evolution.
However, current static analysis tools operate on a single
codebase [7] only. As a result, we cannot detect patterns
that span across the whole system by concatenating analysis
per codebase. This often leads to an alternative direction -
dynamic analysis. Dynamic analysis tools can operate on the

decentralized perspective [8]. However, the dynamic analysis
does not reveal a comprehensive detail of the system (e.g.,
concerning the actual implementation of the service). Quality
engineers, architects, and developers might need to know
underlying code quality details to improve maintenance and
support system comprehension.

a

considers

This paper

catalog of well-established
Microservice-speciﬁc API design Patterns (MAP). It reports
tools that can detect these patterns and mechanisms for the
detection (i.e., static analysis, tracing, log mining). The goal is
to identify current gaps in pattern mining and quality assurance
automation tools.

To approach this, we adopted a Multivocal Literature
Review (MLR) process [9] surveying the systematic gray
literature. The key motivation for conducting an MLR and
therefore including the grey literature is the strong interest of
practitioners on the subject, and grey literature content creates
a foundation for future research.

As a result, this work identiﬁes a list of 46 MAPs and 59
pattern mining tools. Out of the 46 patterns, 34 have been
addressed by found tools. These 34 MAPs can be discovered
by 26 tools out of 59. Most importantly, we identiﬁed gaps in
current tools to support MAP pattern identiﬁcation. We further
provide discussion for the reasons behind such a gap and what
needs to be addressed to overcome the current hurdles.

The remainder of this paper is structured as follows. Sec-
tion II presents the background and related works. Section III
provides detailed information on the gray literature review
process we adopted. Section IV reports the results to our
RQs. Section V discusses implications for practitioners and
researchers. Section VI highlights the threats to validity while
ﬁnally, Section VII draws the conclusions.

II. BACKGROUND AND RELATED WORK

One way to determine software quality is to analyze soft-
ware for patterns and to verify that it doesn’t contain anti-
patterns [2], [3] or to calculate quality metrics such as coupling
and cohesion [10]. Different tools have been proposed to
perform automatic quality reviews using static analysis of code
to operate on pattern detection [11]. Pattern mining has been
broadly researched [2], [12], [13] and it is a well-established
domain, at least for monolithic applications.

 
 
 
 
 
 
Various microservices patterns have also been identiﬁed [4],
[14], [15] for various tasks, such as porting from monoliths
[15], supporting resilience [14], targeting good design prac-
tices [2], [16], among others. Development frameworks apply
these patterns [14] to simplify development.

In this paper, we consider current Microservice API Pat-
terns (MAP further in the text) reported on the API-Pattern
website [3], [17]. The API-Pattern website collects the vast
majority of microservices patterns proposed by its creators in
peer-reviewed literature [3], [18]–[23]. The list of patterns is
provided in Table I while the complete description of each
pattern can be found on the API-pattern website [17]. We
have, however, excluded the pattern “Annotated Parameter
Collection” as it lacked a detailed description.

The major difference between monolith and cloud systems
in regards to pattern mining, however, is the decentralized
codebase, which likely introduces diversity, heterogeneity, and
no obvious connection across codebases. Despite initial code-
base convention efforts, these get easily lost with evolution
and management diversity.

Because of possible diversity across microservices, practi-
tioners often resort to assessing cloud systems through dy-
namic analysis [14], [24]. However, with such direction, we
can recognize endpoints and calls but not internal microservice
details often needed for pattern detection (since many patterns,
including some MAP, have to do with internal implementation
as well, e.g. “Backend Integration”).

When we consider other current analysis approaches [24],
we can notice that static analysis of code or mining software
repositories involves code parsing and conversion to syntax
trees or various graph representations as an intermediate
representation. Then it uses these intermediate representations
to identify patterns. Other approaches consider log analy-
sis. However, these approaches face difﬁculty with the non-
structured format of log messages. While log clustering can be
used, it is very challenging. For this reason, it is common to
integrate event tracing, which adds logging statements to calls
(i.e., via instrumentation) and collects additional information
in log messages, including the originating microservice or the
correlation ID to determine distributed transactions and related
log messages [25].

Another approach worth mentioning is program slicing
[26], which combines log analysis with code analysis. This
is accomplished by locating logging statements in the code
and identifying logging templates in these statements; these
are then matched with log messages found in logs to these
code locations [27]. For instance, in Lprof [27], the authors
used program slicing to proﬁle distributed systems and op-
timize their performance. They matched log statements with
log messages and performed a data-ﬂow analysis of method
parameters to identify if these parameters change across call
paths. Unchanged parameters identiﬁed related log messages
and could be used as a correlation ID similar to event tracing.
Using this, they could recognize distributed transactions and
their frequencies in logs. However, with event tracing (i.e.,
OpenTelemetry [28]), such tasks become much more sim-

pliﬁed and commonly adopted by industry (establishing the
correlation ID). Any of these techniques could be used to help
with detecting MAPs.

III. STUDY DESIGN

This section describes the methods adopted to gather and
classify the different tools to detect microservices best prac-
tices.

Since our goal is to map existing tools recommended and
adopted by practitioners, we performed a systematic review of
the grey literature. A review of peer-review literature would be
biased toward academic opinions and would not clearly enable
us to understand what practitioners can ﬁnd when looking for
such types of tools online.

In order to investigate the aforementioned goal, we formu-

lated our research questions as:
RQ1: Which are the tools available to detect MAPs?

This RQ aims to ﬁnd tools dedicated to mining-speciﬁc pat-
terns described in the previous section. Even though some of
them could be identiﬁed using general-purposes testing tools,
the necessity to write tests for the speciﬁc patterns makes it
unfeasible for the patterns to be adopted in the industry, while
having a set of ready-made tools that could be incorporated
into the CD/CI pipeline would facilitate the adoption of best
practices and improve code and design quality.
RQ2: Which MAPs can be detected automatically with
tools?

In this RQ, we map the tools identiﬁed in RQ1 to the

patterns they detect.
RQ3: Which techniques can be used to detect MAPs?

In this RQ, we aim to understand whether MAPs can be
detected using static or dynamic analysis tools. The possibility
of detecting the pattern from static analysis tools would enable
understanding of the patterns used without running the system.
In contrast, the detection based on the dynamic execution of
the system and the collection of log traces would allow an
understanding of how the system is actually behaving.
A. The Review Process

Grey literature Reviews and Multivocal Literature Reviews
(MLR) proved to be the best choice for the research method
due to the lack of maturity of the subject. MLR includes
both academic and grey literature. However, since we are
aimed at investigating the word of mouth of practitioners, we
will perform a review of the only grey literature. The key
motivation for the inclusion of grey literature is the strong
interest of practitioners in the subject, and grey literature
content creates a foundation for future research.

The process adopted is similar to the MLR, but doesn’t

include the peer-reviewed literature steps.

The process we adopted was based on these steps:
• Selection of keywords and search approach
• Initial search and creation of an initial pool of sources
• Snowballing
• Reading through material
• Application of inclusion / exclusion criteria
• Evaluation of the quality of the grey literature sources
• Creation of the ﬁnal pool of sources

B. Literature Search Process

Since we are interested in ﬁnding tools to detect MAPs
(Table I), we created 53 query search strings. The search
strings were as follows:

• "pattern_name" api pattern detection

tool, where pattern_name was
replaced with
every pattern from table I (46 total searches, excluding
“Annotated Parameter Collection” pattern)

• api security analysis tool
• api parameter analysis tool
• api parameter discovery tool
• api documentation analysis tool
• api specification analysis tool
• semantic versioning identification

tool

• microservice api pattern detection

tool

The latter strings were added because the ﬁrst parametric
search string did not produce many meaningful results, and
at the same time, many patterns can be grouped together, as
represented by other search strings. We understand that not
all groups of patterns are represented by latter search strings,
so partial bias is introduced; however, we could not think of
search strings targeting other groups of patterns and, as stated
before, not using them produced only a handful of results.

We applied the Search strings to the Google Search1 engine,
looking at 10 pages of results per search (excluding ads). The
search was done with Incognito browser mode without logging
into a personal Google account. The decision to use 10 pages
of results was adopted after an informal piloting of the search,
which showed that no relevant results appear on pages 9-10

1www.google.com

TABLE I: The list of Microservice API patterns [17]

Quality
API Key
Rate Limit
Rate Plan
Service Level Agreement
Error Report
Conditional Request
Request Bundle
Wish List
Wish Template
Embedded Entity
Structure
Atomic Parameter
Atomic Parameter List
Parameter Tree
Parameter Forest
Data Element
Id Element
Link Element
Metadata Element
Annotated Parameter Collection
Context Representation

Foundation
Frontend integration
Backend integration
Public API
Community API
Solution Internal API
API Description
Responsibility
Processing Resource
Information Holder Resource
Computation Function
State Creation Operation
Retrieval Operation
State Transition Operation
Operational Data Holder
Master Data Holder
Reference Data Holder
Data Transfer Resource
Link Lookup Resource
Evolution
Version Identiﬁer
Semantic Versioning
Two In Production
Aggressive Obsolescence
Experimental Preview
Limited Lifetime Guarantee
Eternal Lifetime Guarantee

of the search and that for some patterns (search strings) only
a few results (2-3 pages, sometimes not enough even for 1
page) are returned.

Search results consisted of blog posts (including blogs with
lists of tools), websites, research papers, Github2 repositories
of tools, and Github repositories of lists of tools. It is good
to note that StackOverﬂow3 is a popular website for technical
peer questions, and it could be expected to appear in the search
results, but in reality, it did not. It could have been included
in the study separately, but after piloting it, it did not provide
meaningful results, so in this paper, we decided to focus only
on Google Search.

This search was performed between 10th and 21st of

January 2022.

C. Snowballing

We applied a backward snowballing to the retrieved litera-

ture in the following way:

• If the resource extracted from the search is a list of tools
(as opposed to a page of just one tool), such as “Top 10
API security tools in 2021”4, then we checked all of the
referenced tools (and potentially other referenced lists)
• If the resource is a research paper, we checked if the paper
cites other algorithms that ﬁt our criteria and included
them as well

D. Application of inclusion / exclusion criteria

Based on MLR guidelines [9], we deﬁned our inclusion

criteria:

• For pages of tools: the tool description directly contains

one of the MAPs from I

• For research papers: the paper proposes a new algorithm/-
tool to detect a pattern whose description in the abstract
is similar to studied patterns

Moreover, we deﬁned our exclusion criteria as:
• Exclusion criterion 1: Non-English results
• Exclusion criterion 2: Duplicated result
• Exclusion criterion 3: (for papers) The paper proposes a
new algorithm/tool to detect a pattern; however, no source
code for the tool is provided

• Exclusion criterion 4: (for commercial tools) The tool has

no public documentation of functionality available

• Exclusion criterion 5: (for tools) The tool

is an all-
purpose security tool claiming to, e.g., “identify over
1000 different vulnerabilities”, i.e., no particular refer-
ence to one of the MAP is given.

• Exclusion criterion 6: (for tools) The tool cannot automat-
ically perform the analysis but requires the programmers
to conﬁgure it and adapt a certain workﬂow ﬁrst (e.g.,
conﬁgure the tool to detect certain words in commit
messages and then use them in work), so that it cannot be
used to retrospectively analyze the history of an existing
project.

2www.github.com
3www.stackoverﬂow.com
4This is a made-up example

E. Evaluation of the quality and credibility of sources

In order to evaluate the credibility and quality of the selected
grey literature sources and to decide whether to include a
grey literature source or not, we extended and applied the
quality criteria proposed by Garousi et al.
[9], considering
the authority of the producer, the methods applied, objectivity,
date, novelty, impact, and outlet control. We adopted the same
evaluation sheet adopted by Peltonen et al.

[29].

Two authors assessed each source using the aforementioned
criteria, with a binary or 3-point Likert scale, depending on
the criteria themselves. In case of disagreement, we discussed
the evaluation with the third author, which helped to provide
the ﬁnal assessment.

We ﬁnally calculated the average of the scores and rejected
grey literature sources that scored lower than 0.5 on a scale
that ranges from 0 to 1.

F. Creation of ﬁnal pool of sources

Originally, 45 different resources were identiﬁed as relevant.
After performing snowballing on papers and lists and ﬁltering
all papers and tools through the exclusion criteria, we had a
list of 59 tools.

G. Data Extraction

In order to obtain the list of tools to detect MAPs (RQ1)

we extracted the tool names from the selected sources.

To understand which pattern is detected by the tools (RQ2),
ﬁrstly, we read through the tool documentation. There are
two sorts of documents available: a dedicated website and
a README ﬁle. Few tools have both. After reading the
documentation, we investigated whether this tool recognizes
any MAP and whether it is possible to obtain information
about those patterns using this tool. We couldn’t locate any
tool that recognizes MAP on its own because these MAPs
are not commonly well-established yet. However, a few tools
disclose data that can be utilized to develop an insight into
MAPs.

We examined each tool by reviewing its documentation to
see if it exposed any information that may be utilized to detect
any MAP. When we locate a tool that can detect a pattern, we
map that tool to that pattern.

Lastly, to understand which technique can be adapted to
detect MAPs (RQ3), We manually analyzed each pattern,
including those not discovered by tools, and mapped each
pattern to the possible technique. For the techniques, we
considered static and dynamic analysis. For static analysis,
we considered plain code analysis, operation with call graphs,
which might require more advanced algorithms, and mining
software repositories which may include additional informa-
tion. For dynamic analysis, we considered application log
analysis considering rich logging in the system code (an ideal
case) and event logs (i.e., received from instrumentation with
correlation ID [14]). We did not consider program slicing.

The ﬁrst

two authors both independently examined and
mapped each pattern to the various techniques using their own

reasoning. After that, the differing viewpoints were resolved
by consulting with each other as well as the other authors.

IV. RESULTS

This section reports the results we obtained following the

research methodology highlighted in Section 4.

As for the tools available to detect MAPs (RQ1), we
identiﬁed 59 tools from 45 sources. Tables II and III list the
tools retrieved (Open Source and Commercial, respectively),
together with their URL, license (for Open Source tools),
languages supported for analysis, and date of the last update
(for Open Source tools). As for the Licenses adopted by OSS
projects, the license indicated in their repository is stated, such
as MIT, Apache, etc.; ‘OSS’ refers to tools whose source code
is openly available, but no license is added to the repository;
‘N/A (Free)’ refers to tools that are available for free (e.g., as
a web service), but not in Open Source form, and thus might
be subject to a custom license as well. Different tools can
analyze MAPs from the perspective of different programming
languages. Some tools support several languages. Other tools
scan git commits to perform the analysis, and thus applicable
to any language. While other tools either access the APIs
under analysis using the provided endpoints or analyze API
speciﬁcations in OpenAPI format, thus the language of imple-
mentation doesn’t matter. The language reported in Table II
as ‘Any’ refers to tools that parse source code in a language-
agnostic manner and thus apply to any language.

There are 7 Commercial tools and 52 OSS tools to provide

some summary statistics.

When it comes to OSS licenses, 33 of tools (56% of total
tools) are using permissive licenses (Apache, MIT, BSD, etc.),
and 5 more use no license at all, while 7 tools (11%) use
‘copyleft’ licenses (different versions of GPL license).

In terms of languages, 16 tools (27%) are written in Python,
11 tools (19%) are written in JavaScript (some of them in
TypeScript), Java and Go have 6 tools each (10% each); other
represented languages are C++/C#, Kotlin, PHP and shell
scripting (bash).

Supported languages/platforms involve 8 tools analyzing
commits in Git (14%), another 8 tools targeting Python as
the only language (13%), 6 tools (10%) targeting Java, and
another 6 tools (10%) supporting several languages. Also,
19 tools (32%) target REST APIs directly, with 9 analyzing
speciﬁcations and 10 using the endpoints dynamically.
Out of 52 OSS tools, 32 have been updated at

least
once since January 2021. Furthermore, 19 have been updated
already in 2022.

It should also be noted that some tools are research proto-
types (T2, T16, T18, T21), some grew out of research proto-
types (T48), while many are projects done by hobbyists (T19,
T34, T37 to name a few), so their quality and applicability
to up-to-data languages and frameworks could be limited. The
scope of this paper is simply to identify existence of some
tools to address MAPs and see the pattern coverage, assessing
the actual quality and usefulness of the tools is a different,
much more complicated endeavour.

TABLE II: OSS tools to detect Microservice API Patterns (RQ1)

ID
Tool
T1
API security tools audit
T2
apidiff
T3
Arjun
T4
Astra
T5
Brakeman
T6
Coala
T7
code2ﬂow
T8
git-secret
T9
git-semv
T10
GitVersion
T11
go-semrel-gitlab
T12
GraphQL FBC-CLI
T13
Hikaku
T14
jgitver
T15
Kiterunner
LAPD
T16
T17 magento-semver
T18 microservices-antipatterns
T19 modver
T20 Mondrian
T21 MSA-nose
T22
T23
T24
T25
T26
T27
T28
T29
T30
T31
T32
T33
T34
T35
T36
T37
T38
T39
T40
T41
T42
T43
T44
T45
T46
T47
T48
T49
T50
T51
T52

next-ver
NodeJS scan
NoRegrets
OpenAPI diff
OpenAPI spec validator
openapi-lint
openapilint
oval
pact
paramspider
Prometheus
prospector
Public API changes
pycallgraph
Pyramid OpenAPI3
pyramid-swagger
Python Semantic Release
REST API Antip. Inspect.
schaapi
secret-detection
semantic-release
semantic-versioning-anal.
semver-conﬁg
semverbot
Speccy
Spectral
SpotBugs
Standard Version
Vulture
wFuzz
Zally

URL
https://apisecurity.io/tools/audit/
bit.ly/3J0FyZN
bit.ly/3uFZoWv
bit.ly/3ontBp4
bit.ly/3LbQtSo
bit.ly/3AXgKiw
bit.ly/34w2MYM
git-secret.io
bit.ly/34yi1jM
bit.ly/3AVeUi6
bit.ly/3upDINT
bit.ly/333l57q
bit.ly/3Hubkhx
bit.ly/3glahV5
bit.ly/3J77eML
bit.ly/3gl6FlZ
bit.ly/3gnWygi
bit.ly/3GwBwGJ
bit.ly/32WbY8k
bit.ly/3rqHkNZ
bit.ly/3sgODXF
bit.ly/3B5EWzq
bit.ly/3uqxHkm
bit.ly/3JjDAnF
bit.ly/3oncymY
bit.ly/3sogwxc
bit.ly/3oqUJDA
bit.ly/3J45Lqs
bit.ly/3J466te
bit.ly/3gp77Q8
bit.ly/3sf6JZZ
prometheus.io
bit.ly/3oo4Zg7
bit.ly/3gjOKMF
bit.ly/3uqyZfc
bit.ly/3uqbZg5
bit.ly/3sfLKqb
bit.ly/3oo3uyg
bit.ly/3GkLhrG
bit.ly/3J3BcB7
bit.ly/3rqsM0N
bit.ly/3usLkPQ
bit.ly/3HsvgBn
bit.ly/3GnLpGN
bit.ly/3GteoZI
bit.ly/3uknqGe
bit.ly/3gnIeo3
bit.ly/3glbvQj
bit.ly/3uqgx64
bit.ly/3Gq9hd2
bit.ly/3opt5Hg
bit.ly/34lLHkq

Language
supported
REST API
Java
REST API
REST API
Ruby
Any
Many (4)
git
git
git
git
GraphQL
REST-API
Java
REST API
Java
N/A
Python
Go
PHP
Java
git
Node.js
JavaScript
REST API
REST API
REST API
REST API
REST API
REST-API
REST API

License
N/A (Free)
MIT
GPL-3.0
Apache-2.0
MIT
AGPL-3.0
MIT
MIT
MIT
MIT
MIT
OSS
Apache-2.0
Apache-2.0
AGPL-3.0
N/A (Free)
OSL-3.0
Apache-2.0
MIT
OSS
OSS
MIT
GPL-3.0
Apache-2.0
MIT
Apache-2.0
BSD-3
MIT
MIT
MIT
GPL-3.0
Apache-2.0 Many (5)
GPL-2.0
Unlicense
GPL-2.0
MIT
BSD-3
MIT
MIT
MIT
OSS
MIT
MIT
OSS
MPL-2.0
MIT
Apache-2.0
LGPL-2.1
ISC
MIT
GPL-2.0
MIT

Python
C#
Python
Python
Python
Python
REST API
Java
Any
git
.NET
git
git
REST API
REST API
Java
Node.js
Python
REST API
REST API

Updated
N/A (Active)
31.10.2021
29.08.2021
05.04.2019
30.01.2022
11.06.2021
27.12.2021
01.02.2022
17.06.2021
31.01.2022
27.10.2019
06.06.2018
19.08.2021
30.03.2021
10.05.2021
07.09.2013
19.01.2022
17.12.2019
16.01.2022
16.09.2014
12.04.2021
09.02.2018
31.01.2022
02.07.2019
29.08.2017
28.01.2022
12.08.2020
13.05.2019
26.09.2018
03.02.2022
12.09.2021
02.02.2022
01.02.2022
05.11.2017
28.02.2018
07.12.2021
30.03.2020
31.01.2022
31.03.2021
11.02.2019
03.08.2020
18.01.2022
03.11.2021
26.09.2019
03.01.2022
02.10.2019
03.02.2022
29.01.2022
01.01.2022
03.01.2022
28.11.2020
14.01.2022

TABLE III: Commercial tools to detect Microservice API Patterns (RQ1)

Tool
Acunetix
CheckMarx
CodeClimate
Data Theorem API Secure
Dynatrace
SonarQube*
Synopsys

ID
T53
T54
T55
T56
T57
T58
T59
*Dual-licensed, available both as commercial and open source (GPL).

URL
acunetix.com
checkmarx.com
bit.ly/3GrqB12
bit.ly/34ASaYM N/A
dynatrace.com
bit.ly/3GryR15
bit.ly/3uqjhRd

Many
Many (29)
REST API

Supported Languages
Web
Many (20)
Many (11)

As for the MAPs that can be detected automatically with
tools (RQ2), we found 26 tools that expose information about
the tools. These tools are listed as RQ2 column in Table
IV. Our table shows that found tools detect a subset of all
patterns, and a combination of tools is necessary to address
broader coverage. For example, the pattern ‘API Description’
is detected by 9 tools in the Foundation category, while no
tool targets the other 5 patterns in this category. Two tools,
code2ﬂow, and pycallgraph, which are both based on call
graphs, can identify all of the patterns in the Responsibility
category. Hikaku is a tool that can be used to detect all of the
patterns in the Structure category. All of the patterns in this
category, with the exception of ‘Context representation,’ can
be discovered with four or more tools. There are three patterns
in the quality section that no tool can detect. Rate Plan, Service
Level Agreement, and Wish Template are examples of these
patterns. At least one tool detects the rest of the patterns.
Three patterns are not detected by any tools in the Evolution
category, whereas four patterns are detected by at least three
tools.

The most promising techniques to detect MAPs (RQ3)
stem from the static analysis involving source code analysis
as detailed in Table IV. Most of the tools we found use
static analysis. In particular, the static analysis might need
to determine call graphs to detect certain patterns, and also,
the detection process can use mining software repositories.
Nevertheless, given the API level, dynamic analysis can also
determine a large number of patterns. While application log
analysis is one option, it is a challenging option dependent on
the level of logging. It is more convenient to use event logs
resulting from recent cloud-native frameworks and infrastruc-
ture advancements. All events are centralized and aggregated
by the occurrence time, with correlation ID indicating message
dependencies.

V. DISCUSSION

It is interesting to note that the vast majority of MAPs can be
detected by tools, even if there are no tools that analyze them
all. Foundation patterns are the only group of MAPs where
no tools implemented their detection, except for the ”API
description.” However, few other patterns could be technically
detected, and therefore tools could implement them. Other
pattern categories have rather reasonable coverage by tools.

Regarding RQ1, we found 59 tools available for the detec-
tion task. We listed these tools in Tables II and III. These tables
also divided the tools based on open-source availability. We
further categorized these tools based on patterns they identify
in Table IV. However, not all tools had available information
on which patterns they could detect.

With regards to RQ2, almost no tool identiﬁes the patterns
directly. The extracted results must be post-interpreted by users
to identify these patterns from the provided information. For
example, tools discovering the pattern ’Semantic Versioning
Identiﬁer’ do not simply tell if the project has followed the
SemVer speciﬁcation5, but instead tell the correct identiﬁer

5www.semver.org

(increment) based on changes in the source, and it is up to
the developer/researcher to compare it to the one actually
used. This is a missing step to better terminology uniﬁcation,
establishment, and automation in the domain, which some
practitioners could desire. Still, there are notable gaps and
improvement opportunities. Despite 59 tools found, there is
no outstanding tool with respect to the detection coverage
of a number of patterns. Tools must be combined to address
different types of patterns. Table IV outlines identiﬁed gaps
that quality assurance tools should ﬁll to provide better quality
measures through integration into a single solution. Some
conventional API testing tools could be used to detect speciﬁc
patterns, but we excluded such tools as they need explicit
scripting. As an example, in Postman [30], it is possible to
extract request/response headers from HTTP calls, and from
the headers, users can write tests to see if it contains API Key
or Version information, etc.

Related to RQ3, identiﬁed tools are predominantly based
on static code analysis, and more advanced techniques might
be necessary to detect some patterns as depicted by Table IV.
This table also shows that some patterns that could be detected
by the techniques we identiﬁed are not yet recognized by the
pool of tools we found, which opens opportunities. Some of
the patterns currently require manual input to be determined.
However, this opens questions about whether other techniques
could be considered to address these patterns. For instance,
“Rate Plan” and “SLA” are about the legal use of the API, and
perhaps organizational policies need to be taken into account.
Still, these policies are not in a machine-readable format,
and there is no guarantee these are enforced; thus, more
advanced mechanisms would need to be considered to allow
combinations of static or dynamic analysis with organizational
policies.

The results of this work can be useful for researchers
that can investigate different techniques for detecting patterns.
This paper’s outcomes could also be useful to practitioners
who can access the list of tools that automatically detect
patterns and eventually integrate them in continuous quality
control models [31]. Finally, results might be beneﬁcial to
tool providers that might extend their solutions to detect
a large number of patterns or integrate them into DevOps
pipelines [32].

VI. THREATS TO VALIDITY

Systematic reviews and surveys often suffer from several
threats to validity. We discuss the threats considering construct,
internal, external, and conclusions validity.

The construct validity is meant to consider the research
questions within the investigated area. Our queries are mo-
tivated by gaps in related works. The search terms combined
established terms and pattern names commonly recognized
in the community and domain of this work. We addressed a
possible threat of omitting relevant research from our review
by experimenting with several other search strings identifying
related work. The analyzed sample considered grey literature
articles to ensure the up-to-date perspective of practitioners.

TABLE IV: Possible Techniques for detecting microservice patterns

Patterns

RQ2
Detected by tool (ID)

RQ3

Static
Analysis

Frontend integration
Backend integration
Public API
Community API
Solution Internal API
API Description

Processing resource
Information holder resource
Computation function
State recreation operation
Retrieval operation
State transition operation
Operational Data Holder
Master data holder
Reference data holder
Data transfer resource
Link Lookup resource

Atomic parameter
Atomic parameter list
Parameter tree
Parameter forest
Data element
Id element
Link element
Metadata element
Context representation
Pagination

API key
Rate limit
Rate plan
Service level agreement
Error report
Conditional request
Request bundle
Wish list
Wish template
Embedded entity
Linked information holder

Version identiﬁer
Semantic versioning
Two in production
Aggresive obsolesense
Experimental preview
Limited lifetime guarantee
Eternal lifetime guarantee

Foundation

T25, T26, T27, T28, T29, T36, T37, T46, T52
Responsibility
T7, T35
T7, T35
T7, T35
T7, T35
T7, T35
T7, T35
T7, T35
T7, T35
T7, T35
T7, T35
T7, T35
Structure
T3, T6, T13, T31, T36, T37
T3, T6, T13, T31, T36, T37
T3, T6, T13, T31, T36, T37
T3, T6, T13, T31, T36, T37
T6, T13, T31, T36, T37
T3, T6, T13, T31, T36, T37
T6, T13, T31, T36, T37
T13, T31, T36, T37
T13
T13, T31, T36, T37
Quality
T13, T36, T37
T36, T37, T53

T13, T36, T37
T13, T36, T37
T13
T36, T37

T13
T13
Evolution
T6, T10, T13, T45
T2, T9, T13, T19, T38, T40, T42, T43, T44, T45

T2, T13, T40
T2, T13, T40

Dynamic
Analysis
g
o
l

y
r
o
t
i
s
o
p
e
R

g
o
L

t
n
e
v
E

n
o
i
t
a
c
i
l
p
p
A

X X

X X
X X
X X
X X
X X
X X
X X
X X
X X
X X
X X

X X

X X
X X

X X
X X
X X
X X
X X
X X
X X

h
p
a
r
G

l
l
a
C

e
d
o
C

e
c
r
u
o
S

X X
X X

X

X X
X X
X X
X X
X X
X X
X X
X X
X X
X X
X X

X
X
X
X
X
X
X
X
X
X

X
X

X X
X X
X X
X X
X X
X X
X X

X X X
X X X
X X X
X X X X
X X X
X
X

Furthermore, our evidence search was often limited to an
abstract overview of sources, which could miss relevant work.

interpreted results and represents our view on the identiﬁed
literature and domain.

Internal validity involves methods to study and analyze data
(e.g., the types of bias involved). One potential threat is related
to inclusion and exclusion, a process that includes metadata.
Besides, our bias could affect the ﬁltering. Multiple authors
performed this search, with primary authors assigned to par-
ticular patterns and secondary authors spot-checking. Apart
from the ﬁltering process, we performed tool localization by
name, documentation, and repository identiﬁcation if available.
Our approach to taxonomy is a result of our discussions of

External validity is related to knowledge generalization.
This survey interprets and categorizes works we gathered from
established scientiﬁc channels and grey literature along with
our experience related to the ﬁeld. We could have missed
related work on speciﬁc patterns or related tools because of
our selection of search strings, part of which target speciﬁc
patterns or groups of patterns. However, even with a subset
of identiﬁed tools and approaches, we would likely have
identiﬁed common techniques applied for pattern mining and

provided an overview of what has been accomplished and
which gaps remain to be addressed.

The conclusions resulting from several brainstorming ses-
sions were independently settled and agreed on by all au-
thors. To address the validity of the conclusions, we involved
multiple authors in this study with diverse backgrounds, all
discussing the outcomes in the context of extracted and
synthesized information.

VII. CONCLUSION

This paper considered microservice API patterns (MAP)
and their recognition by available quality assurance tools.
It provides a practical road map to what tools and open-
source exits to detect
these patterns and where the gaps
remain. We have identiﬁed 59 tools that address 34 MAPs
out of 46 identiﬁed. We did not ﬁnd a speciﬁc tool that
would surpass others, and thus a combination of tools is
necessary to cover a broader spectrum of MAPs. Yet, not
complete coverage exists considering our search results. This
gap represents an opportunity for the community to combine
and integrate efforts to provide better quality assurance tools
needed in this mainstream ﬁeld. Furthermore, considering that
we identiﬁed many open source projects, their integration is a
logical next step to developing a more advanced microservice
infrastructure for quality assurance.

ACKNOWLEDGMENTS

This material was supported by the ADOMS Grant awarded
by the Ulla Tuominen Foundation (Finland), National Science
Foundation under Grant No. 1854049 and a grant from Red
Hat Research https://research.redhat.com.

REFERENCES

[1] F. Wedyan and S. Abufakher, “Impact of design patterns on software
quality: a systematic literature review,” IET Software, vol. 14, no. 1, pp.
1–17, 2020.

[2] D. Taibi, V. Lenarduzzi, and C. Pahl, “Architectural patterns for mi-
croservices: A systematic mapping study,” in Proceedings of the 8th
International Conference on Cloud Computing and Services Science -
Volume 1: CLOSER,, INSTICC. SciTePress, 2018, pp. 221–232.
[3] O. Zimmermann, M. Stocker, D. L¨ubke, C. Pautasso, and U. Zdun,
“Introduction to microservice api patterns (map),” in International
Conference on Microservices (Microservices 2019), 2019.

[4] A. Wiggins, “The twelve-factor app,” 2017, (Accessed on 10/02/2021).

[Online]. Available: https://12factor.net/

[5] J.

Lewis

“Microservices,”
www.martinfowler.com/articles/microservices.html, Accessed: January
2022, March 2014.

Fowler,

and

M.

[6] S. Newman, Building Microservices. O’Reilly Media, Inc., 2015.
[7] T. Cerny, J. Svacina, D. Das, V. Bushong, M. Bures, P. Tisnovsky,
K. Frajtak, D. Shin, and J. Huang, “On code analysis opportunities and
challenges for enterprise systems and microservices,” IEEE Access, pp.
1–22, 2020.

[8] J. Soldani, G. Muntoni, D. Neri, and A. Brogi, “The µtosca toolchain:
Mining, analyzing, and refactoring microservice-based architectures,”
Software: Practice and Experience, vol. 51, pp. 1591 – 1621, 2021.
[9] V. Garousi, M. Felderer, and M. V. M¨antyl¨a, “Guidelines for including
grey literature and conducting multivocal literature reviews in software
engineering,” Information and Software Technology, vol. 106, pp. 101 –
121, 2019.

[10] S. Panichella, M. R. Imranur, and D. Taibi, “Structural coupling for
microservices,” in 11th International Conference on Cloud Computing
and Services Science, 04 2021.

[11] I. Pigazzini, F. Arcelli Fontana, V. Lenarduzzi, and D. Taibi, “Towards
microservice smells detection,” in Proceedings of the 3rd International
Conference on Technical Debt, ser. TechDebt ’20, 2020, p. 92–97.
[12] A. K. Dwivedi, A. Tirkey, and S. K. Rath, “Software design pattern
mining using classiﬁcation-based techniques,” Frontiers of Computer
Science, vol. 12, no. 5, pp. 908–922, 2018.

[13] M. A. de F. Farias, R. Novais, M. C. J´unior, L. P. da Silva Carvalho,
M. Mendonc¸a, and R. O. Sp´ınola, “A systematic mapping study on
mining software repositories,” in Proceedings of the 31st Annual ACM
Symposium on Applied Computing, ser. SAC ’16, 2016, p. 1472–1479.
Simon

[14] J. Carnell and I. H. S´anchez, Spring microservices in action.

and Schuster, 2021.

[15] A. Balalaie, A. Heydarnoori, P. Jamshidi, D. A. Tamburri, and T. Lynn,
“Microservices migration patterns,” Software: Practice and Experience,
vol. 48, no. 11, pp. 2019–2042, 2018.

[16] D. Taibi, N. El Ioini, P. Claus, and J. R. S. Niederkoﬂer, “Patterns
for serverless functions (function-as-a-service): A multivocal literature
review,” in 10th International Conference on Cloud Computing and
Services Science (CLOSER), 2020, pp. 181–192.

[17] O.

Zimmermann,

“Microservice

api

patterns,”

https://www.microservice-api-patterns.org/, accessed: 2022-02-04.
[18] O. Zimmermann, M. Stocker, D. L¨ubke, C. Pautasso, and U. Zdun,
“Introduction to Microservice API Patterns (MAP),” in Joint Post-
proceedings of the First and Second International Conference on Mi-
croservices (Microservices 2017/2019), 2020, pp. 4:1–4:17.

[19] O. Zimmermann, D. L¨ubke, U. Zdun, C. Pautasso, and M. Stocker,
“Interface responsibility patterns: Processing resources and operation
responsibilities,” in European Conference on Pattern Languages of
Programs 2020, ser. EuroPLoP ’20, 2020.

[20] O. Zimmermann, C. Pautasso, D. L¨ubke, U. Zdun, and M. Stocker,
“Data-oriented interface responsibility patterns: Types of information
holder resources,” ser. EuroPLoP ’20, 2020.

[21] D. L¨ubke, O. Zimmermann, C. Pautasso, U. Zdun, and M. Stocker,
“Interface evolution patterns: Balancing compatibility and extensibility
across service life cycles,” in Proceedings of the 24th European Con-
ference on Pattern Languages of Programs, ser. EuroPLop ’19. New
York, NY, USA: Association for Computing Machinery, 2019.

[22] M. Stocker, O. Zimmermann, D. L¨ubke, U. Zdun, and C. Pautasso,
“Interface quality patterns – communicating and improving the quality of
microservices apis,” in 23rd European Conference on Pattern Languages
of Programs 2018, July 2018.

[23] O. Zimmermann, M. Stocker, D. L¨ubke, and U. Zdun, “Interface
representation patterns - crafting and consuming message-based remote
apis,” in 22nd European Conference on Pattern Languages of Programs
(EuroPLoP 2017), July 2017, pp. 1–36.

[24] V. Bushong, A. S. Abdelfattah, A. A. Maruf, D. Das, A. Lehman,
E. Jaroszewski, M. Coffey, T. Cerny, K. Frajtak, P. Tisnovsky, and
M. Bures, “On microservice analysis and architecture evolution: A
systematic mapping study,” Applied Sciences, vol. 11, no. 17, 2021.
[25] D. Taibi and K. Syst¨a, “From monolithic systems to microservices: A
decomposition framework based on process mining,” in Proceedings
of the 9th International Conference on Cloud Computing and Services
Science (CLOSER), INSTICC. SciTePress, 2019, pp. 153–164.
[26] V. Bushong, R. Sanders, J. Curtis, M. Du, T. Cerny, K. Frajtak,
P. Tisnovsky, and D. Shin, “On log analysis and stack trace use to
improve program slicing,” in Information Science and Applications.
Springer Singapore, December 2021, p. (in print).

[27] X. Zhao, Y. Zhang, D. Lion, M. F. Ullah, Y. Luo, D. Yuan, and
M. Stumm, “lprof: A non-intrusive request ﬂow proﬁler for distributed
systems,” in 11th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 14), Broomﬁeld, CO, 2014, pp. 629–644.
[28] “Open Telemetry,” 2022. [Online]. Available: https://opentelemetry.io
[29] S. Peltonen, L. Mezzalira, and D. Taibi, “Motivations, beneﬁts, and
literature review,”

issues for adopting micro-frontends: A multivocal
Information and Software Technology, vol. 136, p. 106571, 2021.

[30] “Postman

api

platform,”

2022.

[Online].

Available:

https://www.postman.com/

[31] V. Lenarduzzi, A. C. Stan, D. Taibi, D. Tosi, and G. Venters, “A dy-
namical quality model to continuously monitor software maintenance,”
in 11th European Conference on Information Systems Management
(ECISM2017), 2017.

[32] D. Taibi, V. Lenarduzzi, and C. Pahl, “Continuous architecting with
microservices and devops: A systematic mapping study,” in Cloud
Computing and Services Science (CLOSER), 2019, pp. 126–151.

