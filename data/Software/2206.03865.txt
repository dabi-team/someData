2
2
0
2

n
u
J

4

]
L
P
.
s
c
[

1
v
5
6
8
3
0
.
6
0
2
2
:
v
i
X
r
a

Fault-Aware Neural Code Rankers

Jeevana Priya Inala

Chenglong Wang

Mei Yang

Andres Codas

Mark Encarnación

Shuvendu K Lahiri

Madanlal Musuvathi

Jianfeng Gao

Microsoft Research
{jinala,chenwang,meiyang,andres.codas,markenc,
shuvendu,madanm,jfgao}@microsoft.com

Abstract

Large language models (LLMs) have demonstrated an impressive ability to generate
code for various programming tasks. In many instances, LLMs can generate a
correct program for a task when given numerous trials. Consequently, a recent trend
is to do large scale sampling of programs using a model and then ﬁltering/ranking
the programs based on the program execution on a small number of known unit
tests to select one candidate solution. However, these approaches assume that the
unit tests are given and assume the ability to safely execute the generated programs
(which can do arbitrary dangerous operations such as ﬁle manipulations). Both of
the above assumptions are impractical in real-world software development. In this
paper, we propose fault-aware neural code rankers that can predict the correctness
of a sampled program without executing it. The fault-aware rankers are trained
to predict different kinds of execution information such as predicting the exact
compile/runtime error type (e.g., an IndexError or a TypeError). We show that
our fault-aware rankers can signiﬁcantly increase the pass@1 accuracy of various
code generation models (including Codex [11], GPT-Neo, GPT-J) on APPS [25],
HumanEval [11] and MBPP [3] datasets.

1

Introduction

Large transformer-based language models (LLMs) have impressive capabilities [19, 7, 16], including
the ability to generate code [11, 3, 28, 39, 33]. The task here is to take a natural language description or
previous code context as the input and generate an entire program in a general-purpose programming
language such as Python or C++. Generating entire programs is a challenging task as it involves
understanding the task, ﬁguring out how to accomplish it, and writing code without any syntax/runtime
errors. On harder programming problems such as coding competition problems, current models
achieve very low accuracy especially if the inference-time sampling budget is low. For example, on
the APPS dataset [25], a state-of-the-art code generation model, Codex [11], achieves 4% accuracy if
it is allowed to sample only one program per task (called pass@1), but achieves 24% accuracy if it is
allowed 100 samples per task (called pass@100—at least one correct program in 100 samples) (see
Table 7). This observation leads us to an important research problem on exploring approaches to rank
the sampled programs to bridge the gap between the pass@1 and pass@100 performances.

Upon analyzing the sampled programs obtained using an LLM, we ﬁnd that several of them have
syntax errors and runtime errors, and some of them execute to produce undesirable outputs. Therefore,
prior works [11, 28] focused on ranking the programs by executing them on a small set of unit tests
(which are typically assumed to be given as part of the task description). However, there are several
caveats to this approach: First, even if a program passes the given tests, it could still fail on the
unknown unit tests. Second, there is a burden on the user to provide unit tests for every inference

Preprint. Under review.

 
 
 
 
 
 
Task
Write a function name ‘nextPerfectSquare‘ that
returns the ﬁrst perfect square that is
greater than its
example: next_perfect_square(6) == 9
def next_perfect_square(n):
Use Call−Based format

integer argument.

Reference code

def next_perfect_square(n):

return n>=0 and

( int (n∗∗0.5)+1)∗∗2

Unit tests

Inputs :
6 36 0 −5

Outputs:
9 49 1 0

Figure 1: An example code generation task from the APPS dataset (train/3447).

task. Third, to execute the code, all the dependencies has to be installed properly. This is especially
problematic in scenarios where a user wants to get code suggestions in a project with multiple ﬁles and
dependencies (such as using CoPilot in a VS Code environment [1]). Even if these dependencies are
satisﬁed, many realistic programming scenarios involve incomplete code under active development
where execution is just infeasible. Finally, the code generated by a LLM could potentially have
security risks (such as deleting ﬁles on the disk) and hence, executing such a code needs heavy-weight
isolation mechanisms such as a sandbox environment.

To alleviate the above issues with relying on executing code, some of the recent works [18, 36]
proposed to use a neural-network based ranker1 for ranking programs sampled from a LLM. A
ranker model is essentially a classiﬁer that takes as input a task description and a sampled program
and predicts the probability that the program is correct with respect to the task description. When
given multiple programs (obtained by sampling), a ranker re-orders them in the decreasing order of
the predicted probability of the program being correct. The ranker is essentially trying to emulate
executing a program on some unit tests without actually executing the code during inference. The
training data is obtained by executing both correct and wrong programs sampled from the code
generation model itself. Thus, we only need unit tests and the execution ability during the dataset
creation step instead of during inference as in prior approaches.

While previous approaches target math problems to make the learning problem tractable, in this
paper, we target more complex general programming tasks in Python. A sampled Python program
can fail in many different ways. For example, a program when executed on a unit test can result in a
compile/runtime error and can produce a wide variety of outputs such as integers, strings, lists and
dictionaries that can produce a type mismatch. In contrast in the math domain, there is less scope for
compile/runtime errors and the output is usually just a number.

Our approach is based on the idea that a neural ranker trained to distinguish between the various
failure modes can have a better understanding of the program and the task, and hence can do better
at ranking programs. Thus, we design several fault-aware neural rankers and we investigate their
impact on the ranking performance. Each fault-aware ranker is a classiﬁer trained to predict one/two
multi-class labels that are extracted from the rich information obtained by executing the programs.

We used the APPS dataset to ﬁnetune/train the code generation and the ranker models. On this
dataset, we showed that our fault-aware rankers improved the pass@1 performance of Codex (used
in a few-shot manner) from 26% to 39.6% on the validation set and from 3.8% to 4.5% on the test
set. We additionally found our rankers are transferable to a different dataset without any additional
training. On the HumanEval dataset, we improved Codex’s pass@1 from 26% to 32% and on the
MBPP dataset, we improved from 36% to 42%. We found similar performance boosts with other
code generation models such as GPT-J and GPT-Neo. Compared with a naïve binary classiﬁer-based
ranker, our fault aware rankers achieve better ranking performance. Finally, we investigated the effect
of mixing ranker datasets from multiple code generation models which gives us an extra boost in the
performance.

2 Preliminaries

2.1 Code generation

Task: A code generation task G is a prompt, represented as a sequence of tokens, that speciﬁes the
task at hand. G is usually a combination of natural language, input-output examples, and starter

1called a veriﬁer in the previous work

2

Model

# parameters

mode

Codex (Davinci)

Unknown

One-shot

GPT-J
GPT-neo 1.3B
GPT-neo 125M

6B
1.3B
125M

Fine-tuned
Fine-tuned
Fine-tuned

pretrain dataset
GitHub + Common
Crawl, Wiki, etc.
Pile dataset
Pile dataset
Pile dataset

ﬁnetune dataset

N/A

APPS train dataset
APPS train dataset
APPS train dataset

Table 1: Code-generation models, their sizes, the mode of usage (ﬁne-tuned or few-shot), and the datasets used
for pretraining and ﬁnetuning.

code. A solution S to a code generation task is a sequence of tokens that together form a program to
solve the given task. Existing datasets additionally contain a set of input-output pairs that are used
to test the correctness of a generated program. Figure 1 gives an example code generation task, the
corresponding reference solution, and the unit tests taken from the APPS dataset [25].

Models: There are several existing pre-trained LLMs in the literature that are suitable to generating
code either in a few-shot manner or after ﬁnetuning. A code generation model F provides a way
for us to sample programs for a given task G as Si ∼ PF (S | G) where PF denotes the probability
distribution induced by the code generation model F .

In this paper, we study four different code generation models—(i) Codex, (ii) GPT-J (6B), (iii) GPT-
Neo 1.3B and (iv) GPT-Neo 125M (Table 1). Codex is the largest state-of-the-art code generation
model that is publicly available for querying through an API and has shown impressive performance
in code generation [11]. It is built on top of a GPT-3 language model architecture and is trained on
180 GB of GitHub data. GPT-Neo and GPT-J are open-sourced models with the number of parameters
ranging from 125M to 6B. These models are pre-trained on the Pile dataset (800 GB of natural
language corpus with 8% of GitHub data). Since, these models are open-sourced, it is possible to
ﬁnetune these models on a downstream dataset such as the APPS dataset. In addition to the above
models, there are other code-generation models such as AlphaCode [28] and Google’s model [3].
While our approach is applicable to any of these models, in this study, we validate the effectiveness
of our rankers on a set of state-of-the-art code generation models that are publicly available, for
reproducibility.

Metrics: Code generation models are evaluated based on functional correctness rather than exact/-
fuzzy match to a reference program. This is because match-based metrics are unable to account for
the large and complex space of programs functionally equivalent to a reference program. Functional
correctness is estimated by checking whether a sampled program passes a set of unit tests. Prior
approaches evaluate functional correctness using the pass@k metric; given k generated program
samples per task, a task is considered solved if any of the samples passes the unit tests. The pass@k
metric measures the total fraction of tasks solved. Additionally, we deﬁne the exec@k metric which
computes the fraction of the tasks for which there exists at least one program in the k samples that
executes without any compile/runtime errors, i.e., produces a non error value for each of the test
inputs, but may or may not match the desired output.

2.2 Code ranking

Given n sampled programs using a code generation model, S1, · · · , Sn ∼ PF (S | G), the goal of
the code-ranker is to ﬁnd an ordering of the programs So1, · · · , Son such that to compute the ranked
pass@k for k ≤ n, a problem is considered solved if any program in the set {So1 , · · · , Sok } passes
the unit tests. A code ranker model R takes as input a code generation task G and a sampled program
Si and outputs a score si. The ranked ordering of the sampled programs is given by So1, · · · , Son
such that so1 > so2 > · · · > son .

3 Fault-Aware Neural Code Rankers

A code ranker model is a classiﬁer that is trained to classify a pair (cid:104)G, Si(cid:105) as CORRECT or not, where
CORRECT means that Si satisﬁes the task G with respect to its unit tests. The score si is computed as
si = PR(CORRECT|G, Si) where PR is the probability according to a ranker model R.

3

Code generation model
used for generating the
dataset

Codex
GPT-J
GPT-Neo 1.3B
GPT-Neo 125M

Training data

C

W

Validation data
C

W

Test data
W

C

E

I
26K 18K
280K 200K
19K
70K 180K 140K
20K 210K 150K
32K 23K 2.4K 210K 260K
10K 170K 200K 1.5K 28K 27K 0.7K 170K 310K
10K 150K 220K 0.8K 23K 34K 0.2K 140K 340K

16K
3K

E

E

I

I

Table 2: Ranker dataset distribution for the datasets generated using the 4 different code generation models on
the APPS tasks. C: # CORRECT programs, W: # WRONG programs, I: # intent errors, and E: # execution errors.

Generated Program

Full error message

def number_property(n):

if n % 2 == 1: return True
return random.choice(n) % 1 == 0

TypeError("object of type ’int’
has no len()") at Line 2

def gematria( string ):

return ’ ’ . join ( sorted ( string . lower (),

key=lambda item: item.lower (),
reverse =True)).lower()

Expected output is 775, but gen-
erated output is ’vole’

Labels
B: wrong
T: execution error
I : −
E: TypeError
L: Line 2

B: wrong
T: intent error
I : OutputTypeError
E: −
L: −1

Table 3: Examples from the fault-aware ranker dataset showcasing the generated program, the full error message
obtained by executing the program, and the ﬁne-grained labels extracted from this error message. Each entry
contains a binary label (B) that classiﬁes the entry as CORRECT or WRONG, a ternary label (T) that distinguishes
between CORRECT, intent error, and execution error, an intent error label (I) that speciﬁes the type of the intent
error (or -), an execution error label (E) that speciﬁes the type of the execution error (or -), and a line number (L)
that corresponds to the erroneous line in the code (or -1 if there is no execution error).

3.1 Code Ranker Dataset

To train a ranker, we need a dataset that has both CORRECT and WRONG (i.e., not correct) programs.
To collect these programs, we use the code-generation models to sample n = 100 programs for each
task in the training dataset. We then execute the sampled programs on their corresponding unit tests
to generate the classiﬁcation labels. Following the observations from [18], one has to be careful to not
over-train the code generation models to ensure diversity in the sampled programs. Hence, we only
ﬁnetune the code generation models for a maximum of 2 epochs and chose a checkpoint that results
in the lowest validation loss (this does not apply to the Codex model, which is used in a few-shot
manner). Table 2 shows the distribution of the ranker’s datasets obtained by using the 4 different code
generation models for the tasks in the APPS dataset 2. As one can expect, the ranker dataset is highly
imbalanced with about 5X to 40X more WRONG data points than CORRECT data points and this ratio
is higher for smaller models such GPT-Neo models.

Fault-aware ranker dataset: A straightforward ranker is one that is trained to predict a binary label
CORRECT or WRONG. However, a program fails for various reasons and knowing why a program
might fail is crucial to predicting whether a program is CORRECT or not. Therefore, we designed a
fault-aware ranker dataset. When we execute a program on a set of unit tests, the compiler message is
more than just a single bit of information. In fact, when the unit test fails, we know if it failed because
of a compile/runtime error (which we call an execution error) or because the program produced a
wrong output for a particular input (which we call an intent error). Table 2 also shows the distribution
of intent errors and execution errors in the ranker datasets. An interesting observation is that the
percentage of execution errors increases for smaller code generation models.

It is possible to further break down the WRONG datapoints. Within the execution error class, the
compiler message tells us exactly the type of the execution error (such as an IndexError or a TypeError
or a TimeOutError) and the line in the program that caused this error. By parsing the error message
from the Python compiler, we derived 10 most frequent classes of execution errors as shown in
Table 4 and Figure 2. Similarly, for the intent error class, we know how the generated output differs
from the expected output (such as wrong type or wrong length of the array output). We manually

2We used a subset of 600 problems as the validation set, chosen randomly from a subset of the original
training problems for which the best model, Codex, produces at least one correct program in its 100 samples.

4

Class
NameError

ValueError

EOFError

TypeError

IndexError
KeyError
TimeoutException
SyntaxError

Function not found

Misc

Description
Undeﬁned variables
Operation/function received an argument of in-
appropriate value
Raised because of extra input() functions
An operation or function is applied to an object
of inappropriate type
Array index out of bounds
Key in a dictionary is not found
Code is inefﬁcient or doesn’t terminate
Parser encountered an error
The function expected by the unit tests is not
found
Other execution errors

(a) NameError
def bird_code(arr ):

if

’hyphen’ in arr :
return [ arr [ i ][0] + ...

# name ’i’

is not deﬁned in line 2

(b) KeyError
def league_standings(teams):

return {i+1: teams[−i−1] for
in range(len (teams))}

i

# KeyError(−1) at Line 1

(c) TimeoutException
def ﬁbonacci (n):

return n if n in [0, 1] else
ﬁbonacci (n − 1) +
ﬁbonacci (n − 2)

# ineﬃcient

implementation

Table 4: Different classes of execution errors. See https://docs.python.
org/3/library/exceptions.html for more details.

Figure 2: Examples of execution
errors.

designed 9 most frequent classes of intent errors by looking at various expected and generated outputs
from the training examples (see Table 5 and Figure 3). These ﬁne-grained execution-based labels
constitute our fault-aware dataset. Table 3 shows a few data entries with all the labels. Figure 6 in the
Appendix shows the distribution of the various classes of execution errors and intent errors for the
ranker dataset obtained using the Codex model.

3.2 Code Ranker Tasks

We now describe the different classiﬁcation tasks derived from the above dataset. For each classiﬁca-
tion task, the input is a pair of code generation task speciﬁcation G and a generated program Si and
the output is one or multiple labels where each label belongs to pre-determined set of classes. We
describe the different labels/classes of the various tasks below. These tasks are designed to explore the
trade-offs between having abstract classes of failures versus having ﬁne-grained classes of failures.
Binary (B): The output is a single binary label with two classes {CORRECT, WRONG}.
Ternary (T): The ternary task splits the WRONG class into intent error and execution error classes:
this forms a three-class classiﬁcation task with output labels {CORRECT, intent error, execution error}
Intent Error aware (I): The intent error aware task splits the intent error class in the ternary task
into its 9 different sub-classes, thus has a total of 11 classes for the output label.
Execution Error aware (E): The execution error aware task is similar to the intent error aware task
but instead of splitting the intent error class, we now split the execution error class into its 10 different
sub-classes, thus has a total of 12 classes for the output label.
Execution Error + Error Line aware (E+L): This is a multi-class and multi-label classiﬁcation
task which combines two classiﬁcation tasks. The ﬁrst one is the execution error aware task described
above. The second task is to predict the line of the code that corresponds to an execution error. The
labels for the error line number also includes −1 to represent there is no execution error.

3.3 Code Ranker Models

We implemented our fault aware rankers by ﬁnetuning CodeBERT [21].

CodeBERT: It is a state-of-the-art pretrained BERT-style code-understanding model trained on the
CodeSearchNet dataset [26] using a combination of masked language modeling and replaced token
detection objectives [17]. It takes as input a concatenation of two segments with a special separator
token, namely [CLS], w1, w2, ..wn, [SEP], c1, c2, ..., cm, [EOS]. Usually, one segment is a natural
language text, and another is a code. [CLS] is a special token, whose ﬁnal hidden representation can
be treated as the aggregated sequence representation for classiﬁcation or ranking downstream tasks.

Adding a classiﬁcation head: We add a classiﬁcation head on top of a base CodeBERT model by
connecting a linear layer and a softmax layer to the hidden representation of the [CLS] special token.
Let C ∈ RH be the ﬁnal hidden vector corresponding to the [CLS] token and let W ∈ RK×H be the
weights of the newly added classiﬁcation layer where K is the number of classes in the classiﬁcation

5

Class
NoneError

EmptyError

Description
Generated output is None while ex-
pected is not
Generated output is an empty array
while expected is not

OutputTypeError Different types of outputs

LengthError
IntSmallError

IntLargeError

StringSmallError

StringLargeError

Misc

Arrays/Dicts/Sets of different lengths
Integer outputs that are different from
the expected ones with delta ≤ 10
Integer outputs that are different from
the expected ones with delta > 10
String outputs whose length is different
from the expected ones with delta ≤ 3
String outputs whose length is different
from the expected ones with delta > 3
Other intent error

(a) NoneError
def consecutive_sum(num):

sum = 0
n = len( str (num))
for i

in range(n−1):

if num % i == 0 and sum > 0:

return sum

# return statement never gets triggered

(b) LengthError
def diamonds_and_toads(sentence,fairy):

return dict ( zip ( ’Ruby␣Crystal’ ,

’u ’: 2,
# Got {’R’: 0,
# expected {’ruby ’: 3,

(0, 2, 1, 2, 0)))
’b ’: 1,
’ crystal ’: 2}

’y ’: 2},

(c) StringSmallError
def smash(words):

return ’ ’ . join (word for word in words)

# Got ’ helloworld ’,
# expected ’ hello world ’

Table 5: Intent error classes.

Figure 3: Examples of intent errors.

Code gen. model

pass@100

pass@1

Codex
GPT-J
GPT-Neo 1.3B
GPT-Neo 125M

100
48.8
34.6
23.6

26
5.1
2.6
1.4

ranked
pass@1
39.6
11.0
8.0
6.5

pass@5

56.4
15.6
9.1
5.2

ranked
pass@5
63.5
21.7
15.1
11.4

exec@1

69.7
60.4
52.1
41.1

ranked
exec@1
87.0
82.9
85.6
58.9

Table 6: Results on the APPS validation dataset about how our fault-aware rankers can improve the pass@1,
pass@5, and exec@1 performance for various code generation models. We use the best ranker model for each
code generation model for this result. Pass@100 for the Codex model on this validation set is 100% by design.

task. The logits for the classiﬁcation output are computed as softmax(CW (cid:62)) and we use a standard
cross-entropy loss for ﬁnetuning all the weights.

Adding a line prediction head: To predict the line corresponding to an execution error, we introduce
an error line vector S ∈ RH during ﬁne-tuning. The probability of a newline (“\n”) token i being the
erroneous line is computed as a dot product between Ti and S followed by a softmax over all of the
newline tokens in the code, i.e., Pi = eSTi
j eSTj where Ti is the ﬁnal hidden vector corresponding to
the ith newline token. We include a newline token at the beginning and the end of the input to indicate
the case where there is no erroneous line in the code (i.e., code does not result in an execution error)
and to indicate the case where the erroneous line is beyond what is encoded in the input (this occurs
if the task+code context cannot ﬁt within the 512 token limit of CodeBERT), respectively.

(cid:80)

4 Evaluation

We next evaluate fault-aware rankers. We investigate (1) how fault-aware rankers can improve various
code generation models on various code datasets, (2) the impact of the different ranker tasks, and
(3) the effect of mixing ranker datasets generated by different code generation models.

Code gen. model

pass@100

pass@1

Codex
GPT-J
GPT-Neo 1.3B
GPT-Neo 125M

24.1
7.2
3.0
1.5

3.8
0.5
0.14
0.04

ranked
pass@1
4.5
0.8
0.3
0.1

pass@5

9.2
1.6
0.53
0.17

ranked
pass@5
10.2
2.6
1.1
0.5

exec@1

59.6
45.4
35.2
28.5

ranked
exec@1
73.4
63.8
73.7
43.9

Table 7: Results on the APPS test dataset. We use the best checkpoints for the code generation models and the
ranker models based on the results on the validation set.

6

4.1 Experiment Setup

Code generation datasets: We consider three existing code generation datasets for our evaluation:
(1) APPS [25]: a collection of 5000 training and 5000 test tasks collected from coding competitions
and interview problems, (2) HumanEval [11]: a set of 164 test tasks, and (3) MBPP [3]: a set of 974
mostly basic python programming tasks with 474 training problems and 500 test problems.

In our experiments, we only use the APPS dataset for ﬁnetuning the code generation models and the
code ranker models (since it is the largest dataset). But we evaluate these models on all three sets of
test tasks. The APPS dataset does not come with a validation dataset, so we used a set of 600 tasks
from the original training dataset for validation; these are, then, excluded from the training dataset.
Since we are interested in explicitly evaluating the ability of a ranker to distinguish CORRECT code
from WRONG code, we chose our validation set to only include problems for which a Codex model 3
(in a few-shot manner) can generate at least one correct program in its 100 samples. To facilitate the
transfer of GPT-J and GPT-Neo ﬁnetuned models on the HumanEval and the MBPP datasets, we
perform a minor programmatic transformation of the task descriptions to match the APPS style.

Metrics: We use the pass@1, pass@5, exec@1, ranked pass@1, ranked pass@5, and ranked exec@1
metrics (higher values are better). See Section 2 for their deﬁnitions. We also show the pass@100
metric to illustrate the maximum possible value for the pass@k and ranked pass@k metrics. These
metrics are measured using an unbiased estimator from 100 samples as proposed by [11].

Training setup and hyper-parameters: We ﬁnetuned GPT-J and GPT-Neo code generation models
on the APPS training dataset for 2 epochs with a batch size of 256 and a learning rate of 1e-5,
and chose the checkpoint that has the lowest validation loss. For inference, we used temperature
sampling with T = 0.8 for Codex model and T = 0.9 for the GPT-J and GPT-Neo models unless
speciﬁed otherwise. We chose these temperatures to maximize diversity in the 100 samples, but
we also conduct an ablation with lower temperatures in Table 11. For each program, we sample
512 new tokens and truncate the generated program by a special stop sequence that we used in the
few-shot/ﬁnetuning prompts.

We ﬁnetuned the code ranker models for 30 epochs with a batch size of 512 and a learning rate of
1e-4, and chose the checkpoint that results in the best ranked pass@1 metric on the validation dataset.
We used class weights to balance the different classes while training the rankers. All experiments are
conducted on V100-32GB GPUs.
Notation: We use the notation RY
code generation model X from Table 2 and on one of the ﬁve ranker tasks Y from Section 3.2.

to denote a ranker model trained on a dataset obtained using the

DX

4.2 Main Results: rankers improve code generation models

APPS validation dataset: First, we analyze the results on the APPS validation dataset of 600 tasks.
Table 6 shows the performance on various metrics for the 4 different code generation models. These
results use the best ranker model for each code generation model which is shown in Table 10. From
Table 6, we ﬁnd that the fault-aware rankers improve all the metrics for all the code generation models
despite their different sizes. The pass@1 performance increases by 5.1% to 13.6% with the rankers
and the models can solve about 30 to 80 more tasks when it has to select only one program from the
100 samples. Another interesting observation is that a GPT Neo 125M model when combined with a
ranker (another 125M model) beats a GPT-J model (with 50X more parameters). These results show
the effectiveness of the rankers on improving the code generation models.

APPS test dataset: Our results on the APPS test dataset of 5000 tasks is shown in Table 7. The test
problems are harder than the ones on the validation set, which we can see by the smaller pass@100
and pass@1 numbers. Hence, the improvement from the rankers is smaller in scale, but it is still a
signiﬁcant improvement; Codex’s pass@1 increases from 3.8% to 4.7% (35 additional problems).

HumanEval and MBPP datasets: We measure the transfer ability of the rankers on two different
datasets–HumanEval (results in Table 8) and MBPP (results in Table 9). We can again see that the
rankers improve the performance of all code generation models on all metrics for both datasets (one
exception is pass@5 for GPT-J on MBPP). Codex model’s pass@1 performance increases by 6% on

3We chose Codex for this ﬁltering step because it is the best performing model out of the 4 models we

considered.

7

Code gen. model

pass@100

pass@1

Codex
GPT-J
GPT-Neo 1.3B
GPT-Neo 125M

88.4
45.1
19.5
12.8

26.3
9.1
3.2
0.84

ranked
pass@1
32.3
11.6
6.1
3.0

pass@5

50.5
19.1
7.7
3.0

ranked
pass@5
61.6
18.9
8.5
6.1

exec@1

77.0
73.6
66.3
52.3

ranked
exec@1
86.6
89.0
87.8
58.3

Table 8: Results on the HumanEval dataset showing the zero-shot transferability of the rankers trained on APPS.

Code gen. model

pass@100

pass@1

Codex
GPT-J
GPT-Neo 1.3B
GPT-Neo 125M

84.8
60.4
40.0
6.8

36.4
12.1
3.6
0.2

ranked
pass@1
41.8
14.2
5.0
0.8

pass@5

60.9
28.9
11.9
0.9

ranked
pass@5
62.4
28.2
16.2
2.6

exec@1

74.8
73.3
58.3
7.6

ranked
exec@1
93.2
80.8
75.2
14.2

Table 9: Results on the MBPP dataset, showcasing another instance where our fault-aware rankers can be used
in a zero-shot manner on a different dataset.

both datasets. These results show the ability of our ranker models to transfer out-of-distribution and
also showcases that the errors made by the code generation models on different tasks are universal.

Exec@1 vs pass@1:
In all the above results, the improvements in the exec@1 metric are higher
than the improvements in the pass@1 metric; this shows that the rankers are better at identifying
execution errors in code than intent errors, which is expected since executing code is shown to be an
inherently hard task for language models [3, 34].

4.3 Ablations

Effect of sampling temperature: On the HumanEval and the MBPP datasets, we additionally
experiment with different temperatures for sampling the 100 programs (see Table 11). As expected,
we noticed that pass@100 decreases with lower temperatures, but pass@1 increases. We found that
the rankers further increase the pass@1 performance for 3 out of the 4 setups and achieved 42.7%
ranked pass@1 on the HumanEval dataset—the best known result so far on this dataset 4 [16]. On the
MBPP dataset, under a low-temperature sampling setup, we found that the ranker slightly decreases
the pass@1 performance—this we attribute to the small difference between the pass@1 and the
pass@100 metrics, which makes it hard for a learned ranker to beat a random ranking scheme.

DX

Analyzing different ranker tasks: Figure 4 shows 4 training curves; one for each code generation
model, X, showcasing the validation ranked pass@1 curves for the model X when combined with
the 5 different rankers R∗
. Results in a tabular format can be found in the Appendix. From
the curves, we can notice that for larger models such as Codex and GPT-J, the rankers trained on
the ternary classiﬁcation task RT perform the best, and for smaller models such as GPT-Neo 1.3B
and GPT-Neo 125M, the ranker trained on the intent-aware classiﬁcation task RI and the ranker
trained on the execution-aware + error line classiﬁcation task RE+L perform the best, respectively.
We can also notice that the binary rankers RB perform signiﬁcantly worse especially with smaller
models. These results suggest that when training rankers on a dataset that has more WRONG code,
harder classiﬁcation tasks act as better regularizes. Figure 7 in the Appendix shows similar training
curves for the ranked exec@1 metric; here, we can see that RE and RE+L always achieves best
ranked exec@1 because they are trained to identify execution errors and RB and RI have the worst
performance.

Analyzing different ranker datasets: In this experiment, we measure the impact of using the
rankers trained on data from one code generation model on another code generation model and the
effect of mixing these ranker datasets. We have 4 different code generation models in this experiment
and hence, 4 different ranker datasets. We analyze two different mixed datasets—(i) mixed-small
that randomly samples 25% of the above ranker datasets and combines them, and (ii) mixed-large
that combines all of the 4 ranker datasets into one; the former represents a dataset that is roughly the
same size of the individual ranker datsets for a fair comparison, while the latter makes use of all the

4this excludes approaches that use execution during inference

8

Best Ranker

Code gen. Model
Codex
GPT-J
GPT-Neo 1.3B
GPT-Neo 125M RI

RT
RT
RE+L

DCodex

DGPT-J

DGPT Neo 1.3B

DGPT Neo 125M

Setup

pass@100

pass@1

HumanEval, Temp=0.8
HumanEval, Temp=0.2
MBPP, Temp=0.8
MBPP, Temp=0.2

88.4
69.5
84.8
70.0

26.3
35.2
36.4
47.2

ranked
pass@1
32.3
42.7
41.8
46.8

Table 10: Best code ranker model for
each code generation model based on best
ranked pass@1 on the validation dataset.

Table 11: Results with different sampling temperatures for the
Codex model with rankers on HumanEval and MBPP.

Figure 4: Training curves (smoothed) measuring ranked pass@1 over
training steps for various rankers on different code generation models.

Figure 5: Results showing how the
ranked pass@1 changes with differ-
ent ranker datasets.

available data. Figure 5 shows the ranked pass@1 and ranked exec@1 of all the 4x6 combinations in
the form of a heat map. Each column has been normalized such that the values for the same model is
1 (i.e. the diagonal values are 1). From the ﬁgure, among the individual ranker datasets, we notice
that the rankers trained on the same model are the best and that the rankers trained on models with
a large size difference are the worst. Finally, the rankers trained on mixed-small dataset perform
sligthly worse (except for GPT-J code gen. model) than using the same model dataset, but the rankers
trained on mixed-large dataset have the overall best performance. These results suggest that while it is
usually better to use the same model for generating the ranking dataset, we can potentially use smaller
models (less expensive models) to augment the ranker dataset to further improve the performance.

Additional qualitative analysis of the rankers can be found in the Appendix.

5 Related works

Code generation tasks/models: There are several code generation models explored in the literature,
ranging from decoder-only architectures [11, 3, 39, 6, 22] to encoder-decoder architectures [28, 33, 2,
38] of various sizes. Similarly, there are several task datasets from multiple domains, including math-
word-problems [3, 18], Jupyter notebook cell generation [10], common programming tasks [3, 11, 35]
and competition level programming problems [28, 25]. In this paper, we evaluate the ranker’s ability
to improve the performance of four decoder-based code generation models on three programming
datasets. Our approach is agnostic to code generation model architectures as long as they generate
outputs by sampling and our approach can be easily extended to different domains.

Code understanding tasks/models: Besides code generation tasks, there are several code under-
standing tasks including code search [4, 8, 23, 26], clone detection [37, 30], code summarization [26],
code translation [13, 27, 32] and defect detection [9, 5, 15, 40, 5]. Encoder-only code understanding
models [29, 21, 2, 38, 24] are developed for these tasks. Our fault-aware ranker task can be viewed as
a new code understanding task, and our ranker model is ﬁnetuned from CodeBERT [21]. The defect
detection datasets are closely related to ours; the main difference is that prior work focused on ﬁnding
vulnerability in human-written code, while we focus on detecting errors in model-generated code.

9

05000100001500020000Step0.150.200.250.300.35Codex ranked pass@1BTIEE+L05000100001500020000Step0.040.060.080.10GPT-J ranked pass@105000100001500020000Step0.020.040.06GPT-Neo 1.3B ranked pass@105000100001500020000Step0.020.030.040.05GPT-Neo 125M ranked pass@1Filtering/ranking for code generation models: Previous works such as [11, 28] use execution to
prune code completions during inference. While [11] only uses unit tests provided as part of the
task, [28] additionally uses a neural model to generate inputs for the unit tests and uses execution
to ﬁlter out programs that produce the same outputs on those inputs. However, these work requires
executing potentially vulnerable code for every inference task. Our approach bypasses execution
at inference time with the neural ranker. Similar to our work, [18, 36] propose a neural-network
based ranker/veriﬁer for code generation models; the main difference is the domain (general purpose
programming tasks in our case versus math problems in prior work) and we train our neural rankers
to learn why/how a code fails rather than just predicting a binary label. Moreover, [18, 36] use
generative models as their ranker base; in our work, we show beneﬁts of rankers using a simple
encoder-only model with only 125M parameters ( [18] uses models with atleast 3B parameters).

Using execution/static analysis in other contexts: There has been other works on using execution
to guide code generation by conditioning the generation on a representation of the program states
[12, 20]. There are also attempts to replace the execution process with a neural model in these
cases [14, 34]. Another relevant work is [31], which improves the code generation models by using
static analysis to augment the model’s input and has shown to drastically reduce the number of
execution errors made by the generated programs. This paper takes an alternate approach by using a
neural ranker model to prune out wrong programs at inference time.

6 Conclusions

We presented fault-aware rankers that rank programs generated by a code generation model without
explicitly executing the programs. The fault-aware rankers are trained to predict the ﬁne-grained
classes of failure modes and we showed the effectiveness of the fault-aware rankers in improving
pass@k and exec@k metrics for various code generation models and tasks.

One of the main limitations is that the rankers are not sound, i.e., they can classify a correct program
as wrong and vice-versa. Additionally, we incur extra inference time to get better performance,
because we now have to generate n >> k programs to ﬁlter k programs to show to the user. Our
current approach also relies on sampling full programs from the code generation model before using
a ranker. In the future, we want to investigate ranking/classifying partial programs, which can in-turn
reduce the inference time by pruning out wrong programs early. Another future direction for our work
is to investigate other ranker model architectures such as decoder-based architectures and exploring
other ranker tasks such as generating the full error message. Finally, it will be interesting to investigate
the transferability of our fault-aware rankers on detecting bugs on real code and vice-versa.

Acknowledgements: We thank Todd Mytkowicz, Piali Choudhury, Rahee Gosh Peshwaria, Curtis
von Veh, and Xiaodong Liu for helpful discussions on this work.

References

[1] Github copilot: Your ai pair programmer. https://copilot.github.com/, 2021.

[2] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Uniﬁed pre-
training for program understanding and generation. arXiv preprint arXiv:2103.06333, 2021.

[3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David
Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large
language models. arXiv preprint arXiv:2108.07732, 2021.

[4] Antonio Valerio Miceli Barone and Rico Sennrich. A parallel corpus of python functions and
documentation strings for automated code documentation and code generation. arXiv preprint
arXiv:1707.02275, 2017.

[5] Zeki Bilgin, Mehmet Akif Ersoy, Elif Ustundag Soykan, Emrah Tomur, Pinar Çomak, and Leyli
Karaçay. Vulnerability prediction from source code using machine learning. IEEE Access,
8:150672–150684, 2020.

[6] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,
Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source
autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.

10

[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[8] Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. When deep
learning met code search. In Proceedings of the 2019 27th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software Engineering,
pages 964–974, 2019.

[9] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. Deep learning based
vulnerability detection: Are we there yet. IEEE Transactions on Software Engineering, 2021.

[10] Shubham Chandel, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. Training and
evaluating a jupyter notebook data science assistant. arXiv preprint arXiv:2201.12901, 2022.

[11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

[12] Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In

International Conference on Learning Representations, 2018.

[13] Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation.

Advances in neural information processing systems, 31, 2018.

[14] Xinyun Chen, Dawn Song, and Yuandong Tian. Latent execution for neural program synthesis
beyond domain-speciﬁc languages. Advances in Neural Information Processing Systems, 34,
2021.

[15] Xiao Cheng, Haoyu Wang, Jiayi Hua, Guoai Xu, and Yulei Sui. Deepwukong: Statically
detecting software vulnerabilities using deep graph neural network. ACM Transactions on
Software Engineering and Methodology (TOSEM), 30(3):1–33, 2021.

[16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[17] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training
text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.

[18] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christo-
pher Hesse, and John Schulman. Training veriﬁers to solve math word problems. arXiv preprint
arXiv:2110.14168, 2021.

[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[20] Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-
Lezama. Write, execute, assess: Program synthesis with a repl. Advances in Neural Information
Processing Systems, 32, 2019.

[21] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,
Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and
natural languages. arXiv preprint arXiv:2002.08155, 2020.

[22] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,
Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code inﬁlling
and synthesis. arXiv preprint arXiv:2204.05999, 2022.

[23] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. Deep code search. In 2018 IEEE/ACM 40th
International Conference on Software Engineering (ICSE), pages 933–944. IEEE, 2018.

[24] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with
data ﬂow. arXiv preprint arXiv:2009.08366, 2020.

[25] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo,
Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge
competence with apps. arXiv preprint arXiv:2105.09938, 2021.

11

[26] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint
arXiv:1909.09436, 2019.

[27] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and
Percy S Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information
Processing Systems, 32, 2019.

[28] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond,
Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code
generation with alphacode. arXiv preprint arXiv:2203.07814, 2022.

[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.

[30] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural networks over tree
structures for programming language processing. In Thirtieth AAAI conference on artiﬁcial
intelligence, 2016.

[31] Rohan Mukherjee, Yeming Wen, Dipak Chaudhari, Thomas Reps, Swarat Chaudhuri, and
Christopher Jermaine. Neural program generation modulo static analysis. Advances in Neural
Information Processing Systems, 34, 2021.

[32] Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. Divide-and-conquer approach
for multi-phase statistical migration for source code (t). In 2015 30th IEEE/ACM International
Conference on Automated Software Engineering (ASE), pages 585–596. IEEE, 2015.

[33] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,
and Caiming Xiong. A conversational paradigm for program synthesis. arXiv preprint
arXiv:2203.13474, 2022.

[34] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,
David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show
your work: Scratchpads for intermediate computation with language models. arXiv preprint
arXiv:2112.00114, 2021.

[35] Tal Schuster, Ashwin Kalyan, Oleksandr Polozov, and Adam Tauman Kalai. Programming

puzzles. arXiv preprint arXiv:2106.05784, 2021.

[36] Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate
& rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034,
2021.

[37] Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal K Roy, and Mohammad Mamun
In 2014 IEEE
Mia. Towards a big data curated benchmark of inter-project code clones.
International Conference on Software Maintenance and Evolution, pages 476–480. IEEE, 2014.
[38] Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven CH Hoi. Codet5: Identiﬁer-aware uniﬁed
pre-trained encoder-decoder models for code understanding and generation. arXiv preprint
arXiv:2109.00859, 2021.

[39] Frank F Xu, Uri Alon, Graham Neubig, and Vincent J Hellendoorn. A systematic evaluation of

large language models of code. arXiv preprint arXiv:2202.13169, 2022.

[40] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign: Effective
vulnerability identiﬁcation by learning comprehensive program semantics via graph neural
networks. Advances in neural information processing systems, 32, 2019.

12

A Appendix

A.1 More details on the fault-aware ranker datasets

Table 12 and Table 13 show additional examples for each execution error and intent error class.

Figure 6 shows the distribution of the various classes of execution and intent errors for the ranker
dataset generated by the Codex model using the APPS training dataset.

Figure 6: Distributions for various classes of execution errors (left) and intent errors (right) in the training
dataset obtained using the Codex model.

A.2 Additional results

A.2.1 Ablations on different ranker tasks

Table 14 and Table 15 show the various ranked metrics for rankers trained on different fault-aware
tasks when using the Codex and GPT-Neo 125M code generation models respectively. Figure 7
shows 4 training curves, X, showcasing the validation ranked exec@1 curves for the model X when
combined with 5 different rankers R∗

.

DX

Figure 7: Training curves (smoothed) measuring ranked exec@1 on the validation set as training progresses for
various rankers on different code generation models.

13

Class

NameError

ValueError

EOFError

TypeError

IndexError

KeyError

Function not found

TimeoutException

SyntaxError

Misc

Example 1
def bird_code(arr ):

if

’hyphen’ in arr :
return [ arr [ i ][0] + ...

# name ’i’

is not deﬁned in line 2

t=int(input ())
for i

in range(t ):

s ,g,d, t=map(int,input(). split ())
print (d−1)
print (s∗g)

# Too many values to unpack in
# line 2 (expected 4, got 5)

t = int(input ())
for i

in range(t ):

s = input()
a = sum(map(int, input(). split ( ’␣’ )))
print (a)

# EOF when reading a line at Line 3
# caused due to duplicated calls
# to input() within the for loop .

def solomons_quest(arr):

return [abs(a) for a in arr ]

# bad operand type for abs ():
# ’ list ’ at Line 1

for _ in range( int (input ())):

n,m=map(int,input(). split ())
l =[0]∗n
for i

in range(n∗m):

l [ i ]+=1

# List index out of range at Line 4

def deﬁne_suit (card ):

a = {"3C": "clubs", "3D":
"diamonds", "3H": "hearts", "3S":
"spades"}
return a[card ]

# KeyError(’QS’) at Line 4

def seemingly( string ):

...

# The test module is expecting a
# function named "apparently"

def pre_ﬁzz (n):

list = []
while n >= 0:

list .append(n)
n = n//1
return list

# runs into an inﬁnite loop

def game_winners(gryﬃndor,
slytherin ):

if

slytherin == "yes":
return " It ’ s␣a␣draw!".

...

# Syntax error at Line 3 ( extra .)

Example 2
def args_to_string(args ):

return ’\n’ . join ( sorted (s ,

key=lambda x: x. isdigit ())) + ’\n’

# name ’s’ is not deﬁned in line 2

def

capitals_ﬁrst ( text ):

return ’ {:b}{}’.format(text . lower (),
text .upper(), text . capitalize ())
# Unknown format code ’b’ for object
# of type ’ str ’

in line 1

_ = input()
s = input()
ans = 1
for i

in range(1,

len (s )):

if s [ i ] == "?" and s[i − 1] == "?":
ans = (ans ∗ 2) % 1000003

# EOF when reading a line at Line 1

def negation_value(s , val ):
in s : return False

if val
return s [0]

# ’in <string>’ requires
# left operand, not bool at Line 1

string as

def get_last_digit (index ):
digits = [x for x in \
range(1, index + 1) if x < 10]
return digits [ index ]

# List index out of range at Line 3

def league_standings(teams):

return {i+1: teams[−i−1] for
in range(len (teams))}

i

# KeyError(−1) at Line 1

# Empty code

def ﬁbonacci (n):

return n if n in [0, 1] else
ﬁbonacci (n − 1) +
ﬁbonacci (n − 2)

# ineﬃcient

implementation

def process_data(data):

return data [0] ∗ data [1] ∗
data [2]

for i
# Syntax error at Line 1
# ( list comprehension syntax)

in range(len (data))

def duplicate_count( text ):

return sum(count for c, count in
text . items() for count in {’a’ ,

’A’,

’B’})

n = int(input ())
a = a+1
...

for _ in range( int (input ())):

# AttributeError at Line 2
# (str has no attribute items)

# UnboundLocalError at line 2
# local variable ’a’
# before assignment.

is referenced

Table 12: Examples of different classes of execution errors

A.2.2 Ablations on different ranker datasets

Table 16 shows the ablation results when training rankers on datasets collected from code generation
models. These results show the improvements of rankers on the Codex code generation model. We
can see that a ranker trained on completions from the Codex model is better than a ranker trained
on completions from other models. We can also see that a ranker trained on the mixed-large dataset
(which use all completions from all models) beats the ranker trained on Codex dataset by additional
2.7%.

14

Example 1
def consecutive_sum(num):

sum = 0
n = len( str (num))
for i

in range(n−1):

Example 2
def start_smoking(bars ,boxes):
rem = boxes ∗ 18 − bars ∗ 10
if rem != 1:

print ((rem + 4) // 5)

if num % i == 0 and sum > 0:

return sum

else :

print (0)

# return statement never gets triggered

# no return statement

Class

NoneError

EmptyError

OutputTypeError

LengthError

IntSmallError

def grabscrab(word, possible_words ):

if word in possible_words:

return possible_words[word]

else :

return []

# Got [], expected [’ ﬁrst ’]

def diamonds_and_toads(sentence, fairy):

if

fairy == ’good’:
return sum(1 for i
fairy == ’evil ’ :
return sum(1 for i

elif

in sentence . split ())

in sentence . split ())

# Got 3, expected
# [{’ruby ’: 3,

’ crystal ’: 2}])

def rotate ( arr , n):

return list (range(0, len ( arr )+n,n))

# Got [0, 1, 2, 3], expected
’b ’]
# [’c ’,

’a ’,

def is_balanced(source , caps ):

return all (x. startswith (caps) or x == caps

for x in source)
# Got False, expected True

IntLargeError

def missing_angle(h, a, o):

return int ((a∗h+o)/2) if o==0

else int (a∗h+o)
# Got 300, expected 37

StringSmallError

StringLargeError

def smash(words):

return ’ ’ . join (word for word in words)

# Got ’ helloworld ’,
# expected [’ hello world ’]

def jumping_number(number):

number = str(number)
len (number)!= 0:
if
return "Not!!"

else :

return "Jumping!!"

# Got ’Not !!’, expected ’Jumping!!’

def

interleave (args ):

return [elem for elem in zip (args)

if

len (elem) < len(args )]

# Got [], expected
# [1, ’c ’, 2,

’d ’, 3,

’e ’]

class Solution :

def

longestPreﬁx ( self , s : str ) −> str:

s = list (s)
s = [i
for i , j
return s[:−2]

in zip (s , s [::−1][0:])]

# Got [’"’,

’ l ’,

’e ’,

’v ’,

’e ’], expected ’"’

def diamonds_and_toads(sentence,fairy):

return dict ( zip ( ’Ruby␣Crystal’ ,

# Got {’R’: 0,
’u ’: 2,
# expected {’ruby ’: 3,

(0, 2, 1, 2, 0)))
’b ’: 1,
’ crystal ’: 2}

’y ’: 2,

’

’: 0},

T = int(input ())
for _ in range(T):
N = int(input ())
print (bin(N).count(’0’ ))

# Got [[2],

[3]], expected [[1],

[2]]

class Solution :

def ﬁndMedian( self , nums1, nums2):

nums1.sort()
nums2.sort()
return ﬂoat ( ’−inf’ )
# Got −inf, expected 2.0

def solve ( st ):

a = st. ﬁnd ("a")
return a if

len ( st ) == 0
else "". join ( sorted ( list ( st )))[0]

# Got ’a’, expected ’x’

def spacify ( string ):
#your code here
return ’ ’ . join ( sorted ( string . split ()))

# Got ’Pippi ’, expected ’P i p p i ’

Table 13: Examples of different classes of intent errors

Codex + ranker

DCodex

DCodex

RB
RT
RI
RE
DCodex
RE+L
DCodex

DCodex

ranked pass@1
37.6
39.6
36.5
38.6
37.1

ranked pass@5
64.6
63.5
64.7
64.0
61.5

ranked exec@1
83.4
87.0
81.8
87.5
88.3

Table 14: Ablation of the different fault-aware ranker tasks using Codex as the code-generation model (on the
APPS validation dataset).

A.2.3 Qualitative analysis of the rankers

In Table 17, we show various programs generated by the ﬁne-tuned GPT-J model, the actual error
message from the Python compiler upon executing these programs, and the various labels generated
by the different rankers trained on various ranker tasks.

15

GPT-Neo 125M + ranker

DGPT-Neo 125M

DGPT-Neo 125M

DGPT-Neo 125M

RB
RT
RI
RE
RE+L

DGPT-Neo125M

DGPT-Neo 125M

ranked pass@1
4.2
5.7
6.5
5.7
6.5

ranked pass@5
7.7
10.4
11.4
11.0
11.9

ranked exec@1
56.0
78.4
58.9
74.7
87.6

Table 15: Ablation of the different fault-aware ranker tasks using GPT-Neo 125M model as the code-generation
model (on the APPS validation set).

DGPT-J

DCodex

Codex + Ranker
Rbest
Rbest
Rbest
Rbest
Rbest
Rbest

DGPT-Neo 125M

DGPT-Neo 1.3B

Dmixed-small

Dmixed-large

ranked pass@1
39.6
28.3
26.9
27.3
39.1
42.3

ranked exec@1
87.0
83.6
83.6
81.8
88.1
91.0

Table 16: Ablation of the different ranker datasets (using the best ranker task for each dataset) on Codex
completions (on APPS validation set).

16

B: correct
T: intent error
I: IntSmallError
E: intent error
E+L: intent error at Line -1

B: correct
T: intent error
I: StrSmallError
E: correct
E+L: correct at Line -1

B: wrong
T: execution error
I: execution error
E: NameError
E+L: NameError at line 1

B: wrong
T: intent error
I: IntLargeError
E: intent error
E+L: intent error at Line -1

Generated code
def pillow (s ):

for i

in range(len (s )):

if s [ i ] == "n":

if s [ i +1:] == "B":

return True

return False

Actual error message

Predicted

Expected True, but
got False

def camel_case(string ):

return "". join ([ x. capitalize () for

x in string . split ("␣" )])

Passed all tests

def sum_it_up(numbers_with_bases):
return sum(int( str (n [0]) ∗ n [1],
base) for n in numbers_with_bases)

NameError(base
is
not deﬁned) at Line 1

for _ in range( int (input ())):

n = int(input ())
a = n − 2
b = n − 1
c = n
if n % 2 == 0:

print ((a∗((a−1)∗∗2) + (b∗∗2) −

2∗b + 2∗c)%1000000007)

else :

print ((a∗((a−1)∗∗2) + (c∗∗2) −

2∗c + 2∗b)%1000000007)

def bingo( array ):

s = "". join ( array )
return "WIN" if "BINGO" == s

else "LOSE"

T = int(input ())
for _ in range(T):
n = int(input ())
S = sorted( list (map(int,

input (). split ())))

ans = 0
for i

in range(1, n+1):

if S[ i ] > S[i−1]:

ans += 1
print (ans)

Got [[’7’], [’13’]], but
expected [[’1’], [’3’]]

TypeError(sequence
item 0: expected str
instance, int found) at
Line 1

B: correct
T: intent error
I: StringSmallError
E: intent error
E+L: TypeError at Line 1

Passed all tests

B: correct
T: execution error
I: execution error
E: IndexError
E+L: IndexError at Line 6

Table 17: We show various programs sampled from a ﬁne-tuned GPT-J model, the actual error message, and the
labels predicted by various ranker models RX
DGP T J . We use the green color to indicate that label is correct, red
to indicate that the label is incorrect, and blue to indicate the case where the label correctly indicates an error in
the program, but it indicates the wrong error type.

17

