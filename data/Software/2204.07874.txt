Springer Nature 2021 LATEX template

2
2
0
2

p
e
S
5
1

]
E
S
.
s
c
[

2
v
4
7
8
7
0
.
4
0
2
2
:
v
i
X
r
a

Ergo, SMIRK is Safe: A Safety Case for a
Machine Learning Component in a Pedestrian
Automatic Emergency Brake System

Markus Borg1,2*, Jens Henriksson3, Kasper Socha1,2, Olof
Lennartsson4, Elias Sonnsj¨o L¨onegren4, Thanh Bui1, Piotr
Tomaszewski1, Sankar Raman Sathyamoorthy5, Sebastian
Brink6 and Mahshid Helali Moghadam1

1*RISE Research Institutes of Sweden, Lund, Sweden.
2Dept. of Computer Science, Lund University, Lund, Sweden.
3Semcon AB, Gothenburg, Sweden.
4Infotiv AB, Gothenburg, Sweden.
5QRTECH AB, Gothenburg, Sweden.
6Combitech AB, Gothenburg, Sweden.

*Corresponding author(s). E-mail(s): markus.borg@ri.se;
Contributing authors: jens.henriksson@semcon.com;
kasper.socha@ri.se; olof.lennartsson@infotiv.se;
elias.sonnsjo@infotiv.se; thanh.bui@ri.se;
piotr.tomaszewski@ri.se; sankar.sathyamoorthy@qrtech.se;
sebastian.brink@combitech.com; mahshid.helali.moghadam@ri.se;

Abstract

Integration of Machine Learning (ML) components in critical appli-
cations introduces novel challenges for software certiﬁcation and ver-
iﬁcation. New safety standards and technical guidelines are under
development to support the safety of ML-based systems, e.g., ISO 21448
SOTIF for the automotive domain and the Assurance of Machine Learn-
ing for use in Autonomous Systems (AMLAS) framework. SOTIF and
AMLAS provide high-level guidance but the details must be chiseled
out for each speciﬁc case. We initiated a research project with the goal
to demonstrate a complete safety case for an ML component in an

1

 
 
 
 
 
 
Springer Nature 2021 LATEX template

2

Ergo, SMIRK is Safe

open automotive system. This paper reports results from an industry-
academia collaboration on safety assurance of SMIRK, an ML-based
pedestrian automatic emergency braking demonstrator running in an
industry-grade simulator. We demonstrate an application of AMLAS on
SMIRK for a minimalistic operational design domain, i.e., we share a
complete safety case for its integrated ML-based component. Finally,
we report lessons learned and provide both SMIRK and the safety
case under an open-source licence for the research community to reuse.

Keywords: machine learning safety, safety standards, safety case, automotive
demonstrator

1 Introduction

Machine Learning (ML) is increasingly used in critical applications, e.g., super-
vised learning using Deep Neural Networks (DNN) to support automotive
perception. Software systems developed for safety-critical applications must
undergo assessments to demonstrate compliance with functional safety stan-
dards. However, as the conventional safety standards are not fully applicable
for ML-enabled systems (Salay et al, 2018; Tambon et al, 2022), several
domain-speciﬁc initiatives aim to complement them, e.g., organized by the EU
Aviation Safety Agency, the ITU-WHO Focus Group on AI for Health, and
the International Organization for Standardization.

In the automotive industry, several standardization initiatives are ongoing
to allow safe use of ML in road vehicles. It is evident that the established
functional safety as deﬁned in ISO 26262 Functional Safety (FuSa) is no
longer suﬃcient for the next generation of Advanced Driver-Assistance Sys-
tems (ADAS) and Autonomous Driving (AD). One complementary standard
under development is ISO 21448 Safety of the Intended Functionality (SOTIF).
SOTIF aims for absence of unreasonable risk due to hazards resulting from
functional insuﬃciencies, incl. those originating in ML components.

Standards such as SOTIF mandate high-level requirements on what a devel-
opment organization must provide in a safety case for an ML-based system.
However, how to actually collect the evidence – and argue that it is suﬃcient
– is up to the speciﬁc organization. Assurance of Machine Learning for use in
Autonomous Systems (AMLAS) is one framework that supports the develop-
ment of corresponding safety cases (Hawkins et al, 2021). Still, when applying
AMLAS on a speciﬁc case, there are numerous details that must be analyzed,
speciﬁed, and validated. The research community lacks demonstrator systems
that can be used to explore such details. Thus, we embarked on a research
endeavor guided by the following high-level research question:

How to demonstrate and share a complete ML safety case for an open ADAS?

We report results from an industry-academia collaboration on safety assur-
ance of SMIRK, an ML-based Open-Source Software (OSS) ADAS that

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

3

provides Pedestrian Automatic Emergency Braking (PAEB) in an industry-
grade simulator. SMIRK is an “original software publication” (Socha et al,
2022) available on GitHub (RISE Research Institutes of Sweden, 2022). In this
paper, our main contribution is the carefully described application of AMLAS
in conjunction with the development of an ML component. While parts of the
framework have been demonstrated before (Gauerhof et al, 2020), we believe
this is the ﬁrst comprehensive use of AMLAS conducted independently from
its authors. Moreover, we believe this paper constitutes a pioneering safety case
for an ML-based component that is OSS and completely transparent. Thus,
our contribution can be used as a starting point for studies on safety engineer-
ing aspects such as Operational Design Domain (ODD) extension, dynamic
safety cases, and reuse of safety evidence.

Our results show that even an ML component in an ADAS designed for a
minimalistic ODD results in a large safety case. Furthermore, we consider three
lessons learned to be particularly important for the community. First, using
a simulator to create synthetic data sets for ML training particularly limits
the validity of the negative examples. Second, evaluation of object detection
is non-intuitive and necessitates internal training. Third, the ﬁtness function
used for model selection encodes essential tradeoﬀ decisions, thus the project
team must be aligned.

The paper is organized as follows:

• Sections 1–4 contain this introduction, a background section describing
SOTIF, AMLAS, and object detection using YOLO, an overview of related
work, and a section describing the development process in our R&D project.
• Sections 5–10 describe the intertwined development and safety assurance
of SMIRK. We present an overall system description, system requirements,
system architecture, data management strategy, ML-based pedestrian recog-
nition component, test design, and test results, respectively.

• Sections 12–14 present threats to validity, lessons learned, and a conclusion

that outlines future work.

Finally, to ensure a self-contained paper, Appendix A presents the complete

AMLAS safety argumentation for the use of ML in SMIRK.

2 Background

This section brieﬂy presents SOTIF and AMLAS, respectively. Also, we present
details of object detection and recognition using YOLO, which is fundamental
to understand the subsequent safety argumentation.

2.1 Safety of the Intended Functionality (SOTIF)

ISO 21448 Safety of the Intended Functionality (SOTIF) is a candidate stan-
dard under development to complement the established automotive standard
ISO 26262 Functional Safety (FuSa). While FuSa covers hazards caused by
malfunctioning behavior, SOTIF addresses hazardous behavior caused by the

Springer Nature 2021 LATEX template

4

Ergo, SMIRK is Safe

Fig. 1: A simpliﬁed overview of the SOTIF process. Adapted from ISO 21488.

intended functionality. Note that SOTIF covers “reasonably foreseeable mis-
use” but explicitly excludes antagonistic attacks, thus we do not discuss any
security concerns in this paper. A system that meets FuSa can still be haz-
ardous due to insuﬃcient environmental perception or inadequate robustness
within the ODD. The SOTIF process provides guidance on how to systemati-
cally ensure the absence of unreasonable risk due to functional insuﬃciencies.
The goal of the SOTIF process is to perform a risk acceptance evaluation and
then reduce the probability of 1) known and 2) unknown scenarios causing
hazardous behavior.

Figure 1 shows a simpliﬁed version of the SOTIF process. The process
starts in the upper left with A) Requirements speciﬁcation. Based on the
requirements, a B) Risk Analysis is done. For each identiﬁed risk, its potential
Consequences are analyzed. If the risk of harm is reasonable, it is recorded as
an acceptable risk. If not, the activity continues with an analysis of Causes,
i.e., an identiﬁcation and evaluation of triggering conditions. If the expected
system response to triggering conditions is acceptable, the SOTIF process
continues with V&V activities. If not, the remaining risk forces a C) Functional
Modiﬁcation with a corresponding requirements update.

The lower part of Figure 1 shows the V&V activities in the SOTIF pro-
cess, assuming that they are based on various levels of testing. For each risk,
the development organization conducts D) Veriﬁcation to ensure that the sys-
tem satisﬁes the requirements for the known hazardous scenarios. If the F)
Conclusion of Veriﬁcation Tests are satisfactory, the V&V activities continues
with validation. It not, the remaining risk requires a C) Functional Modiﬁca-
tion. In the E) Validation, the development organization explores the presence
of unknown hazardous scenarios – if any are identiﬁed, they turn into known
hazardous scenarios. The H) Conclusion of Validation Tests estimates the like-
lihood of encountering unknown scenarios that lead to hazardous behavior. If

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

5

Fig. 2: An overview of the AMLAS process, adapted from Hawkins et al
(2021). Blue color denotes systems engineering, whereas black color relates
speciﬁcally to the ML component. Numbers refer to the AMLAS stages, not
sections in this paper.

the residual risk is suﬃciently small, it is recorded as an acceptable risk. If
not, the remaining risk again necessitates a C) Functional Modiﬁcation.

2.2 Safety Assurance Using the AMLAS Process

The methodology for the Assurance of Machine Learning for use in
Autonomous Systems (AMLAS) was developed by the Assuring Autonomy
International Programme, University of York (Hawkins et al, 2021). AMLAS
provides a process that results in 34 safety evidence artifacts. Moreover,
AMLAS provides a set of recurring safety case patterns for ML compo-
nents presented using the graphical format Goal Structuring Notation (GSN)
(Assurance Case Working Group, 2021).

Figure 2 shows an overview of the six stages of the AMLAS process. The
upper part stresses that the development of an ML component and its cor-
responding safety case is done in the context of larger systems development,
indicated by the blue arrow. Analogous to SOTIF, AMLAS is an iterative
process as highlighted by the black arrow in the bottom.

Starting from the System Safety Requirements from the left, Stage 1 is ML
Safety Assurance Scoping. This stage operates on a systems engineering
level and deﬁnes the scope of the safety assurance process for the ML compo-
nent as well as the scope of its corresponding safety case – the interplay with
the non-ML safety engineering is fundamental. The next ﬁve stages of AMLAS
all focus on assurance activities for diﬀerent constituents of ML development
and operations. Each of these stages conclude with an assurance argument that
when combined, and complemented by evidence through artifacts, compose
the overall ML safety case.

Springer Nature 2021 LATEX template

6

Ergo, SMIRK is Safe

Stage 2 ML Safety Requirements Assurance. Requirements engineering is used
to elicit, analyze, specify, and validate ML safety requirements in relation
to the software architecture and the ODD.

Stage 3 Data Management Assurance. Requirements engineering is ﬁrst used to
develop data requirements that match the ML safety requirements. Subse-
quently, data sets are generated (development data, internal test data, and
veriﬁcation data) accompanied by quality assurance activities.

Stage 4 Model Learning Assurance. The ML model is trained using the devel-
opment data. The fulﬁlment of the ML safety requirements is assessed using
the internal test data.

Stage 5 Model Veriﬁcation Assurance. Diﬀerent levels of testing or formal ver-
iﬁcation to assure that the ML model meets the ML safety requirements.
Most importantly, the ML model shall be tested on veriﬁcation data that
has not inﬂuenced the training in any way.

Stage 6 Model Deployment Assurance. Integrate the ML model in the overall
system and verify that the system safety requirements are satisﬁed. Conduct
integration testing in the speciﬁed ODD.

The rightmost part of Figure 2 shows the overall safety case for the sys-
tem under development with the argumentation for the ML component as an
essential part, i.e., the target of the AMLAS process.

2.3 Object Detection and Recognition Using YOLO

YOLO is an established real-time object detection and recognition algorithm
that was originally released by Redmon et al (2016). The ﬁrst version of YOLO
introduced a novel object detection process that uses a single DNN to perform
both prediction of bounding boxes around objects and classiﬁcation at once.
YOLO was heavily optimized for fast inference to support real-time applica-
tions. A fundamental concept of YOLO is that the algorithm considers each
image only once, hence its name “You Only Look Once.” Thus, YOLO is
referred to as a single-stage object detector.

Single-stage object detectors consist of three core parts: 1) the model back-
bone, 2) the model neck, and 3) the model head. The model backbone extracts
important features from input images. The model neck generates so called “fea-
ture pyramids” using PANet (Liu et al, 2018) that support generalization to
diﬀerent sizes and scales. The model head performs the detection task, i.e., it
generates the ﬁnal output vectors with bounding boxes and class probabilities.
In a nutshell, YOLO segments input images into smaller images. Each input
image is split into a square grid of individual cells. Each cell predicts bound-
ing boxes capturing potential objects and provides conﬁdence scores for each
box. Furthermore, YOLO does a class prediction for objects in the bounding
boxes. Relying on the Intersection over Union (IoU) method for evaluating
bounding boxes, YOLO eliminates redundant bounding boxes. The ﬁnal out-
put from YOLO consists of unique bounding boxes with class predictions.
There are several versions of YOLO and each version provides diﬀerent model

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

7

architectures that balance the tradeoﬀ between inference speed and accuracy
diﬀerently – additional layers of neurons provide better accuracy at the expense
of computation time.

3 Related Work

Many researchers argue that software and systems engineering practices must
evolve as ML enters the picture. As proposed by Bosch et al (2021), we refer to
this emerging area as AI engineering. AI engineering increases the importance
of several system qualities. Quality characteristics discussed in the area of
“explainable AI” are particularly important to safety argumentation. In our
paper, we adhere to the following deﬁnitions provided by Arrieta et al (2020):
“Given an audience, an explainable AI is one that produces details or reasons
to make its functioning clear or easy to understand.” In the same vein, we refer
to interpretability as “a passive characteristic referring to the level at which
a given model makes sense for a human observer.” In our work on SMIRK,
the human would be an assessor of the safety argumentation. In contrast, we
regard explainability as an active characteristic of a model that clariﬁes its
internal functions. The current ML models in SMIRK do not actively explain
their outputs. For a review of DNN explainability techniques in automotive
perception, e.g., post-hoc saliency methods, counterfactual explanations, and
model translations, we refer to a recent review by Zablocki et al (2022).

In light of AI engineering, the rest of this section presents related work on

safety argumentation and testing of automotive ML, respectively.

3.1 Safety Argumentation for Machine Learning

Many publications address the issue of safety argumentation for systems with
ML-based components and highlight corresponding safety gaps. A solid argu-
mentation is required to enable safety certiﬁcation, for example to demonstrate
compliance with future standards such as SOTIF and ISO 8800 Road Vehicles
– Safety and Artiﬁcial Intelligence. While there are several established safety
patterns for non-AI systems (e.g., simplicity, substitution, sanity check, con-
dition monitoring, comparison, diverse redundancy, replication redundancy,
repair, degradation, voting, override, barrier, and heartbeat (Wu and Kelly,
2004; Preschern et al, 2015)), considerable research is now directed at under-
standing to what extent existing approaches apply to AI engineering. For
example, Picardi et al (2020) presented a set of patterns that later turned into
AMLAS, i.e., the process that guides most of our work in this paper.

Mohseni et al (2020) reviewed safety-related ML research and mapped ML
techniques to to safety engineering strategies. Their mapping identiﬁed three
main categories toward safe ML: 1) inherently safe design, 2) enhanced robust-
ness, a.k.a. “safety margin”, and 3) run-time error detection, a.k.a. “safe fail”.
The authors further split each category into sub-categories. Based on their
proposed categories, the safety argumentation we present for SMIRK used an

Springer Nature 2021 LATEX template

8

Ergo, SMIRK is Safe

inherently safe design as the starting point through a careful “design speciﬁca-
tion” and “implementation transparency.” Moreover, from the third category,
SMIRK relies on “OOD error detection” as will be presented in Section 9.3.

Schwalbe and Schels (2020) presented a survey on speciﬁc considerations
for safety argumentation targeting DNNs, organized into four development
phases. First, they state that requirements engineering must focus on data
representativity and entail scenario coverage, robustness, fault tolerance, and
novel safety-related metrics. Second, development shall seek safety by design
by, e.g., acknowledging uncertainty and enhancing robustness (in line with
recommendations by Mohseni et al (2020)). Third, the authors discuss ver-
iﬁcation primarily through formal model checks using solvers. Fourth, they
consider validation as the task of identifying missing requirements and test
cases, for which they recommend data validation followed by both qualitative
and quantitative analyses.

Tambon et al (2022) conducted a systematic literature review on certi-
ﬁcation of safety-critical ML-based systems. Based on 217 primary studies,
the authors identiﬁed ﬁve overall categories in the literature: 1) robustness, 2)
uncertainty, 3) explainability, 4) veriﬁcation, and 5) safe reinforcment learning.
Moreover, the paper concludes by calling for deeper industry-academia collab-
orations related to certiﬁcation. Our work on SMIRK responds to this call and
explicitly discusses the identiﬁed categories on an operational level (except
reinforcement learning since this type of ML does not apply to SMIRK). By
conducting hands-on development of an ADAS and its corresponding safety
case, we have identiﬁed numerous design decisions that have not been discussed
in prior work. As the devil is in the detail, we recommend other researchers to
transparently report from similar endeavors in the future.

Schwalbe et al (2020) systematically established and broke down safety
requirements to argue the suﬃcient absence of risk arising from SOTIF-style
functional insuﬃciencies. The authors stress the importance of diverse evi-
dence for a safety argument involving DNNs. Moreover, they provide a generic
approach and template to thoroughly report DNN speciﬁcs within a safety
argumentation structure. Finally, the authors show its applicability for an
example use case based on pedestrian detection. In our work, 34 artifacts of dif-
ferent types constitute the safety evidence. Furthermore, pedestrian detection
is a core feature of SMIRK.

Several research projects selected ML-based pedestrian detection systems
to illustrate diﬀerent aspects of safety argumentation. Wozniak et al (2020)
provided a safety case pattern for ML-based systems and showcase its appli-
cability for pedestrian detection. The pattern is integrated within an overall
encompassing approach for safety case generation. On a similar note, Willers
et al (2020) discussed safety concerns for DNN-based automotive perception,
including technical root causes and mitigation strategies. The authors argue
that it remains an open question how to conclude whether a speciﬁc concern is
suﬃciently covered by the safety case – and stress that safety cannot be deter-
mined analytically through ML accuracy metrics. In our work on SMIRK, we

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

9

provide safety evidence that goes beyond the level of the ML model. Using
AMLAS, we also claim that we demonstrate suﬃcient evidence for ML in
SMIRK. Finally, related to pedestrian detection, we ﬁnd that the work by
Gauerhof et al (2020) is the closest to this study, and the reader will ﬁnd that
we repeatedly refer to it in relation to SMIRK requirements in Section 6.

In the current paper, we present a holistic ML safety case building on
previous work for a demonstrator system available under an OSS license. Fur-
thermore, in contrast to a discussion restricted to pedestrian detection, we
discuss an ADAS that subsequently commences emergency braking in a simu-
lated environment. This addition responds to a call from Haq et al (2021a) to
go from oﬄine to online testing, as many safety violations cannot be detected
on the level of the ML model. While online testing is not novel, research papers
on ML testing have largely considered standalone image data sets disconnected
from concerns of running systems.

3.2 Testing of Machine Learning in Automated Vehicles

According to AMLAS, the two primary means to generate safety evidence for
ML-based systems through V&V are testing and veriﬁcation (Hawkins et al,
2021). For SMIRK, we restrict ourselves to testing. In the context of an ML-
based system, this can be split into: 1) data set testing, 2) model testing, 3)
unit testing, 4) integration testing, 5) system testing, and 6) acceptance testing
(Song et al, 2022). The related work focuses on model and system testing.

Model testing shall be performed on the veriﬁcation data set that must not
have been used in the training process. Depending on the test subject and the
test level, the inputs for corresponding ADAS testing might be images such
as in DeepRoad (Zhang et al, 2018) and DeepTest (Tian et al, 2018), or test
scenario conﬁgurations as used by Ebadi et al (2021).

Many research projects investigated eﬃcient design of eﬀective test cases
for ADAS. Thus, several approaches to test case generation in simulated envi-
ronments have been proposed. Pseudo-test oracle diﬀerential testing focuses
on detecting divergence between programs’ behaviors when provided the same
input data. For example, DeepXplore (Pei et al, 2017) changes the test input—
like solving an optimization problem—to ﬁnd the inputs triggering diﬀerent
behaviors between similar autonomous driving DNN models, while trying to
increase the neuron coverage. Metamorphic testing works based on detecting
violations of metamorphic relations to identify erroneous behaviors. For exam-
ple, DeepTest (Tian et al, 2018) applies diﬀerent transformations to a set of
seed images and utilizing metamorphic relations to detect faulty behaviors of
diﬀerent Udacity DNN models for self-driving cars, while aiming for increas-
ing neuron coverage as well. Gradient-based test input generation regards the
test input generation as an optimization problem and generates a high num-
ber of failure-revealing or diﬀerence-inducing test inputs while maximizing the
test adequacy criteria, e.g., neuron coverage. DeepXplore (Pei et al, 2017) uti-
lizes gradient ascent to generate inputs provoking diﬀerent behaviors among
similar DNN models. Generative Adversarial Network (GAN)-based test input

Springer Nature 2021 LATEX template

10

Ergo, SMIRK is Safe

generation is intended to generate realistic test input, which can not be eas-
ily distinguished from original input. DeepRoad (Zhang et al, 2018) utilizes a
GAN-based metamorphic testing approach to generate test images for testing
Udacity DNN models for autonomous driving.

System testing entails ensuring that system safety standards are met fol-
lowing the integration of the model into the system. As for model testing, many
research projects proposed techniques to generate test cases. A commonly pro-
posed approach is to use search-based techniques to identify failure-revealing
or collision-provoking scenarios. Many papers argue that simulated test sce-
narios are eﬀective complements to ﬁeld testing – which tends to be costly and
sometimes dangerous.

Ben Abdessalem et al (2016) proposed the multi-objective search algorithm
NSGA-II along with surrogate models to ﬁnd critical test scenarios with few
simulations for the pedestrian detection system PeVi. PeVi constitutes the ref-
erence architecture for SMIRK, as described in Section 7, for which we obtained
deep insights during a replication study (Borg et al, 2021a). In a subsequent
study, Ben Abdessalem et al (2018b) used MOSA developed by Panichella et al
(2015)—a many-objective optimization search algorithm— along with objec-
tives based on branch coverage and failure-based heuristics to detect undesired
and failure-revealing feature interaction scenarios for integration testing in an
automated vehicle. Furthermore, in a third related study, Ben Abdessalem
et al (2018a) combined NSGA-II and decision tree classiﬁer models—which
they referred to as a learnable evolutionary algorithm—to guide the search
for critical test scenarios. Moreover, the proposed approach characterizes the
failure-prone regions of the test input space. Inspired by previous work, we have
used various search-techniques, including NSGA-II, to generate test scenarios
for pedestrian detection and emergency braking of the autonomous driving
platform Baidu Apollo in the SVL simulator (Ebadi et al, 2021).

The test cases providing safety evidence for SMIRK correspond to sys-
tematic grid searches rather than any metaheuristic search. We think this is
a necessary starting point for a safety argumentation. On the other hand, as
argued in several related papers, we believe that search techniques could be
a useful complement during testing of ML-based systems to pinpoint weak-
nesses and guide functional modiﬁcations – for example as part of the SOTIF
process. In the SMIRK context, we plan to investigate this further as part of
future work.

4 Method: Engineering Research

The overall frame of our work is the engineering research standard as deﬁned
in the community endeavor ACM SIGSOFT Empirical Standards (Ralph et al,
2020). Engineering research is an appropriate standard when evaluating tech-
nological artifacts, e.g., methods, systems, and tools – in our case SMIRK and
its safety case. To support the validity of our research, we consulted the essen-
tial attributes of the corresponding checklist provided in the standard. We

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

11

Fig. 3: An overview of the SMIRK development in the SMILE III project.

provide three clariﬁcations: 1) empirical evaluations of SMIRK are done using
simulation in ESI Pro-SiVIC, 2) empirical evaluation of the safety case has
been done through workshops and peer-review, and 3) we compare the SMIRK
safety case against state-of-the-art implicitly by building on previous work.

Figure 3 shows an overview of the two-year R&D project (SMILE III1)
that resulted in the SMIRK MVP (Minimum Viable Product) and the safety
case for its ML component. Note that collaborations in the preceding SMILE
projects started already in 2016 (Borg et al, 2019), i.e., we report from research
in the context of prolonged industry involvement. Starting from the left, we
relied on A) Prototyping to get an initial understanding of the problem and
solution domain (K¨apyaho and Kauppinen, 2015). As our pre-understanding
during prototyping grew, SOTIF and AMLAS were introduced as fundamental
development processes and we established a ﬁrst System Requirements Speciﬁ-
cation (SRS). The AMLAS process starts in the System Safety Requirements,
which in our case come from following the SOTIF process.

Based on the SRS, we organized a B) Hazard Analysis and Risk Assessment
(HARA) workshop (cf. ISO 262626) with all author aﬃliations represented.
Then, the iterative C) SMIRK development phase commenced, encompass-
ing both software development, ML development, and a substantial amount
of documentation. When meeting our deﬁnition of done, i.e., an MVP imple-
mentation and stable requirements speciﬁcations, we conducted D) Fagan
Inspections as described in Section 4.1. After corresponding updates, we base-
lined the SRS and the Data Management Speciﬁcation (DMS). Note that
due to the Covid-19 pandemic, all group activities were conducted in virtual
settings.

Subsequently, the development project turned to E) V&V and Functional
Modiﬁcations as limitations were identiﬁed. In line with the SOTIF process
(cf. Figure 1), also this phase of the project was iterative. The various V&V
activities generated a signiﬁcant part of the evidence that supports our safety
argumentation. The rightmost part of Figure 3 depicts the safety case for the

1https://tinyurl.com/smileiii

Springer Nature 2021 LATEX template

12

Ergo, SMIRK is Safe

ML component in SMIRK, which is peer-reviewed as part of the submission
process of this paper.

4.1 Safety Evidence from Fagan Inspections

We conducted two formal Fagan inspections (Fagan, 1976) during the
SMILE III project with representatives from the organizations listed as co-
authors of this paper. All reviewers are active in automotive R&D. The
inspections targeted the Software Requirements Speciﬁcation and the Data
Management Speciﬁcation, respectively. The two formal inspections constitute
essential activities in the AMLAS safety assurance and result in ML Safety
Requirements Validation Results [J] and a Data Requirements Justiﬁcation
Report [M]. A Fagan inspection consists of the steps 1) Planning, 2) Overview,
3) Preparation, 4) Inspection meeting, 5) Rework, and 6) Follow-up.

1. Planning: The authors prepared the document and invited the required

reviewers to an inspection meeting.

2. Overview: During one of the regular project meetings, the lead authors
explained the fundamental structure of the document to the reviewers,
and introduced an inspection checklist. Reviewers were assigned particular
inspection perspectives based on their individual expertise. All information
was repeated in an email, as not all reviewers were present at the meeting.
inspection of the

3. Preparation: All reviewers conducted an individual

document, noting any questions, issues, and required improvements.

4. Inspection meeting: Two weeks after the individual inspections were initi-
ated, the lead authors and all reviewers met for a virtual meeting. The entire
document was discussed, and the ﬁndings from the independent inspections
were compared. All issues were compiled in inspection protocols.

5. Rework: The lead authors updated the SRS according to the inspection
protocol. The independent inspection results were used as input to capture-
recapture techniques to estimate the remaining amount of work (Petersson
et al, 2004). All changes are traceable through individual GitHub commits.
6. Follow-up: Reviewers who previously found issues veriﬁed that those had

been correctly resolved.

4.2 Presentation Structure for the Safety Evidence

In the remainder of this paper, the AMLAS stages and the resulting artifacts
act as the backbone in the presentation. Table 1 provides an overview of how
those artifacts relate to the stages of AMLAS and where in this paper they
are described for SMIRK. Throughout the paper, the notation [A]–[HH], in
bold font, refers to the 34 individual artifacts prescribed by AMLAS. Finally,
in Appendix A, the same 34 artifacts are used to present a complete safety
case for the ML component in SMIRK.

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

13

Table 1: SMIRK Safety Assurance Table. Numbers in the Input/Out-
put columns refer to the AMLAS stages in Figure 2. GitHub repos-
anno-
itory: https://github.com/RI-SE/smirk. AI Sweden hosts
tated data set (185 GB) at: https://www.ai.se/en/data-factory/datasets/
data-factory-datasets/smirk-dataset

our

ID

AMLAS Artifact Title

[A]
[B]

System Safety Requirements
Description of Operating Envi-
ronment of System
[C]
System Description
[D] ML Component Description
[E]

Safety Requirements Allocated
to ML Component
ML Assurance Scoping Argu-
ment Pattern

[F]

[G] ML Safety Assurance Scoping

Argument

[H] ML Safety Requirements
[I]

Requirements

ML
Safety
Argument Pattern
ML Safety Requirements Vali-
dation Results

[J]

[K] ML

Safety

Requirements

[L]
[M]

Argument
Data Requirements
Data Requirements Justiﬁca-
tion Report
Development Data
[N]
Internal Test Data
[O]
Veriﬁcation Data
[P]
[Q]
Data Generation Log
[R] ML Data Argument Pattern
ML Data Validation Results
[S]
[T]
ML Data Argument
[U] Model Development Log
[V] ML Model
[W] ML Learning Argument Pat-

tern
Internal Test Results

[X]
[Y] ML Learning Argument
[Z]
ML Veriﬁcation Results
[AA] Veriﬁcation Log
[BB] ML Veriﬁcation Argument

Pattern

[CC] ML Veriﬁcation Argument
[DD]
Erroneous Behaviour Log
[EE] Operational scenarios
[FF]
[GG] ML Deployment Argument

Integration Testing Results

Pattern

[HH] ML Deployment Argument

Input
to
1, 6
1, 6

1, 6
1
2

1

3, 4, 5
2

3

5, 6
4

5

6

6

Output
from

Where?

Sec. 6.1
Sec. 6.4

Sec. 5
Sec. 9
Sec. 6.2

Fig. A1

Sec. A.1

Sec. 6.3
Fig. A2

Sec. 4.1

Sec. A.2

Sec. 8.1
Sec. 4.1

AI Sweden
AI Sweden
AI Sweden
Sec. 8.2
Fig. A3
Sec. 11.1
Sec. A.3
Sec. 9.2
GitHub repo
Fig. A4

Sec. 11.2.1
Sec. A.4
Sec. 11.2.2
Sec. 10.1
Fig. A5

Sec. A.5
Sec. 11.4
Sec. 10.2.1
Sec. 11.3
Fig. A6

Sec. A.6

1

1

2

2

2

3
3

3
3
3
3

3
3
4
4

4
4
5
5

5
6

6

6

Springer Nature 2021 LATEX template

14

Ergo, SMIRK is Safe

5 SMIRK System Description [C]

SMIRK is a PAEB system that relies on ML. As an example of an ADAS,
SMIRK is intended to act as one of several systems supporting the driver
in the dynamic driving task, i.e., all the real-time operational and tactical
functions required to operate a vehicle in on-road traﬃc. SMIRK, including
the accompanying safety case, is developed with full transparency under an
OSS license. We develop SMIRK as a demonstrator in a simulated environment
provided by ESI Pro-SiVIC2.

The SMIRK product goal is to assist the driver on country roads in rural
areas by performing emergency braking in the case of an imminent collision
with a pedestrian. The level of automation oﬀered by SMIRK corresponds
to SAE Level 1: Driver Assistance, i.e., “the driving mode-speciﬁc execution
by a driver assistance system of either steering or acceleration/deceleration”
— in our case only braking. The ﬁrst release of SMIRK is an MVP, i.e., an
implementation limited to a highly restricted ODD, but, future versions might
include steering and thus comply with SAE Level 2.

Sections 5 and 6 presents the core parts of the SMIRK SRS. The SRS, as
well as this section, largely follows the structure proposed in IEEE 830-1998:
IEEE Recommended Practice for Software Requirements Speciﬁcations (IEEE,
1998) and the template provided by Wiegers (2008). This section presents a
SMIRK overview whereas Section 6 speciﬁes the system requirements.

5.1 Product Scope

Figure 4 illustrates the overall function provided by SMIRK. SMIRK sends a
brake signal when a collision with a pedestrian is imminent. Pedestrians are
expected to cross the road at arbitrary angels, including perpendicular move-
ment and moving toward or away from the car. Furthermore, a stationary
pedestrian on the road must also trigger emergency braking, i.e., a scenario
known to be diﬃcult for some pedestrian detection systems. Finally, Figure 4
stresses that SMIRK must be robust against false positives, also know as
“braking for ghosts.” In this work, this refers to the ML-based component rec-
ognizing a pedestrian although another type of object is on collision course
(e.g., an animal or a traﬃc cone) rather than radar noise. Trajectories are
illustrated with blue arrows accompanied by a speed (v) and possibly an angle
(θ). In the superscript, c and p denote car and pedestrian, respectively, and 0
in the subscript indicates initial speed.

Note that the sole purpose of SMIRK is PAEB. The design of SMIRK
assumes deployment in a vehicle with complementary ADAS, e.g., large animal
detection, lane keeping assistance, and various types of collision avoidance. We
also expect that sensors and actuators will be shared between ADAS. For the
SMIRK MVP, however, we do not elaborate further on ADAS co-existence
and we do not adhere to any particular higher-level automotive architecture.

2We stress that SMIRK shall never be used in a real vehicle and the authors take no

responsibility in any such endeavors.

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

15

Fig. 4: Example scenario with a pedestrian crossing the road at an arbitrary
angle. A false positive is also presented, i.e., a ghost.

In the same vein, we do not assume a central perception system that fuses
various types of sensor input. SMIRK uses a standalone ML model trained for
pedestrian detection and recognition. In the SMIRK terminology, to mitigate
confusion, the radar detects objects and the ML-based pedestrian recognition
component identiﬁes potential pedestrians in the camera input.

The SMIRK development necessitated quality trade-oﬀs. The software
product quality model deﬁned in the ISO/IEC 25010 standard consists of
eight characteristics. Furthermore, as recommend in requirements engineering
research (Horkoﬀ, 2019), we add the two novel quality characteristics inter-
pretability and fairness. For each characteristic, we share its importance for
SMIRK by assigning it a low [L], medium [M] or high [H] priority. The pri-
orities inﬂuence architectural decisions in SMIRK and support elicitation of
architecturally signiﬁcant requirements (Chen et al, 2012).
• Functional suitability. No matter how functionally restricted the SMIRK
MVP is, stated and implied needs of a prototype ADAS must be met. [H]
• Performance eﬃciency. SMIRK must be able to process input, do ML
inference, and commence emergency braking in realistic driving scenarios.
Identifying when performance eﬃciency reached excessive levels is vital in
the requirements engineering process. [M]

• Compatibility. SMIRK shall be compatible with other ADAS, but, this is

an ambition beyond the MVP development. [L]

• Usability. SMIRK is an ADAS that operates in the background without a

user interface for direct driver interaction. [L]

• Reliability. A top priority in the SMIRK development that motivates the

application of AMLAS. [H]

• Security. Not included in the SOTIF scope, thus not prioritized. [L]
• Maintainability. Evolvability from the SMIRK MVP is a key concern for

future projects, thus maintainability is important. [M]

Springer Nature 2021 LATEX template

16

Ergo, SMIRK is Safe

• Portability. We plan to port SMIRK to other simulators and physical

demonstrators in the future. Initially, it is not a primary concern. [L]

• Interpretability. While interpretability is vital for any cyber-physical

system, SMIRK’s ML exacerbates the need further. [M]

• Fairness. A vital quality that primarily impacts the data requirements
speciﬁed in the Data Management Speciﬁcation (Borg et al, 2021b). [H]

5.2 Product Functions

SMIRK comprises implementations of four algorithms and uses external vehicle
functions. In line with SOTIF, we organize all constituents into the categories
sensors, algorithms, and actuators.
• Sensors

– Radar detection and tracking of objects in front of the vehicle (see

Section 7.1).

– A forward-facing mono-camera (see Section 7.1).

• Algorithms

– Time-to-collision (TTC) calculation for objects on collision course.
– Pedestrian detection and recognition based on the camera input where

the radar detected an object (see Section 9.1).

– Out-Of-distribution (OOD) detection of never-seen-before input (part of

the safety cage mechanism, see Section 9.3).

– A braking module that commences emergency braking. In the SMIRK

MVP, maximum braking power is always used.

• Actuators

– Brakes (provided by ESI Pro-SiVIC, not elaborated further).

Figure 5 illustrates detection of a pedestrian on a collision course, i.e.,
PAEB shall be commenced. The ML-based functionality of pedestrian detec-
tion and recognition, including the corresponding OOD detection, is embedded
in the Pedestrian Recognition Component (deﬁned in Section 7.1).

6 SMIRK System Requirements

This section speciﬁes the SMIRK system requirements, organized into system
safety requirements and ML safety requirements. ML safety requirements are
further reﬁned into performance requirements and robustness requirements.
The requirements are largely re-purposed from the system for pedestrian detec-
tion at crossings described by Gauerhof et al (2020) to our PAEB ADAS, thus
allowing for comparisons to previous work within the research community.

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

17

Fig. 5: Illustrative example of pedestrian detection that shall trigger emer-
gency braking.

6.1 System Safety Requirements [A]
• SYS-SAF-REQ1 SMIRK shall commence automatic emergency braking if
and only if collision with a pedestrian on collision course is imminent.

Rationale: This is the main purpose of SMIRK. If possible, ego car will
stop and avoid a collision. If a collision is inevitable, ego car will reduce speed
to decrease the impact severity. Hazards introduced from false positives, i.e.,
braking for ghosts, are mitigated under ML Safety Requirements.

6.2 Safety Requirements Allocated to ML Component [E]

Based on the HARA (see Section 4), two categories of hazards were identiﬁed.
First, SMIRK might miss pedestrians and fail to commence emergency braking
– we refer to this as a missed pedestrian. Second, SMIRK might commence
emergency braking when it should not – we refer to this as an instance of ghost
braking.
• Missed pedestrian hazard: The severity of the hazard is very high (high risk

of fatality). Controllability is high since the driver can brake ego car.

• Ghost braking hazard: The severity of the hazard is high (can be fatal). Con-
trollability is very low since the driver would have no chance to counteract
the braking.

We concluded that the two hazards shall be mitigated by ML safety

requirements.

6.3 Machine Learning Safety Requirements [H]

This section reﬁnes SYS-SAF-REQ1 into two separate requirements corre-
sponding to missed pedestrians and ghost braking, respectively.

Springer Nature 2021 LATEX template

18

Ergo, SMIRK is Safe

• SYS-ML-REQ1. The pedestrian recognition component shall

identify
pedestrians in all valid scenarios when the radar tracking component returns
a T T C < 4s for the corresponding object.

• SYS-ML-REQ2 The pedestrian recognition component shall reject false

positive input that does not resemble the training data.

Rationale: SYS-SAF-REQ1 is interpreted in light of missed pedestrians
and ghost braking and then broken down into the separate ML safety require-
ments SYS-ML-REQ1 and SYS-ML-REQ2. The former requirement deals
with the “if” aspect of SYS-SAF-REQ1 whereas its “and only if” aspect is
targeted by SYS-ML-REQ2. SMIRK follows the reference architecture from
Ben Abdessalem et al (2016) and SYS-ML-REQ1 uses the same TTC thresh-
old (4 seconds, conﬁrmed during a research stay in Luxembourg). Moreover,
we have validated that this TTC threshold is valid for SMIRK based on calcu-
lating braking distances for diﬀerent car speeds. SYS-ML-REQ2 motivates
the primary contribution of the SMILE III project, i.e., an OOD detection
mechanism that we refer to as a safety cage.

6.3.1 Performance Requirements

The performance requirements are speciﬁed with a focus on quantitative tar-
gets for the pedestrian recognition component. All requirements below are
restricted to pedestrians on or close to the road.

For objects detected by the radar tracking component with a TTC < 4s,

the following requirements must be fulﬁlled:
• SYS-PER-REQ1 The pedestrian recognition component shall identify

pedestrians with an accuracy of 93% when they are within 80 m.

• SYS-PER-REQ2 The false negative rate of the pedestrian recognition

component shall not exceed 7% within 50 m.

• SYS-PER-REQ3 The false positives per image of the pedestrian recogni-

tion component shall not exceed 0.1% within 80 m.

• SYS-PER-REQ4 In 99% of sequences of 5 consecutive images from a 10
FPS video feed, no pedestrian within 80 m shall be missed in more than
20% of the frames.

• SYS-PER-REQ5 For pedestrians within 80 m, the pedestrian recognition
component shall determine the position of pedestrians within 50 cm of their
actual position.

• SYS-PER-REQ6 The pedestrian recognition component shall allow an

inference speed of at least 10 FPS in the ESI Pro-SiVIC simulation.

Rationale: SMIRK adapts

speciﬁed
by Gauerhof et al (2020) for the SMIRK ODD. SYS-PER-REQ1 reuses
the accuracy threshold from Example 7 in AMLAS, which we empirically
validated for SMIRK – initially in a feasibility analysis, subsequently during
system testing. SYS-PER-REQ2 and SYS-PER-REQ3 are two additional

the performance

requirements

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

19

requirements inspired by Henriksson et al (2019). Note that SYS-PER-
REQ3 relies on the metric false positive per image rather than false positive
rate as true negatives do not exist for object detection (further explained in
Section 10.1 and discussed in Section 12). SYS-PER-REQ6 means that any
further improvements to reaction time have a negligible impact on the total
brake distance.

6.3.2 Robustness Requirements

Robustness requirements are speciﬁed to ensure that SMIRK performs ade-
quately despite expected variations in input. For pedestrians present within
50 m of Ego, captured in the ﬁeld of view of the camera:
• SYS-ROB-REQ1 The pedestrian recognition component shall perform as
required in all situations Ego may encounter within the deﬁned ODD.
• SYS-ROB-REQ2 The pedestrian recognition component shall identify
pedestrians irrespective of their upright pose with respect to the camera.
• SYS-ROB-REQ3 The pedestrian recognition component shall identify

pedestrians irrespective of their size with respect to the camera.

• SYS-ROB-REQ4 The pedestrian recognition component shall identify
pedestrians irrespective of their appearance with respect to the camera.

Rationale: SMIRK reuses robustness requirements for pedestrian detection
from previous work. SYS-ROB-REQ1 is speciﬁed in Gauerhof et al (2020).
SYS-ROB-REQ2 is presented as Example 7 in AMLAS, which has been
limited to upright poses, i.e., SMIRK is not designed to work for pedestrians
sitting or lying on the road. SYS-ROB-REQ3 and SYS-ROB-REQ4 are
additions identiﬁed during the Fagan inspection of the System Requirements
Speciﬁcation (see Section 4.1).

6.4 Operational Design Domain [B]

This section brieﬂy describes the SMIRK ODD. As the complete ODD spec-
iﬁcation, based on the taxonomy developed by NHTSA (Thorn et al, 2018),
is lengthy, we only present the fundamental aspects in this section. We refer
interested readers to the GitHub repository. Note that we deliberately spec-
iﬁed a minimalistic ODD, i.e., ideal conditions, to allow the development a
complete safety case for the SMIRK MVP.
• Physical infrastructure Asphalt single-lane roadways with clear lane
markings and a gravel shoulder. Rural settings with open green ﬁelds.
• Operational constraints Maximum speed of 70 km/h and no surrounding

traﬃc.

• Objects No objects except 0-1 pedestrians, either stationary or moving with

a constant speed (< 15 km/h) and direction.

• Environmental Conditions Clear daytime weather with overhead sun.
Headlights turned oﬀ. No particulate matter leading to dirt on the sensors.

Springer Nature 2021 LATEX template

20

Ergo, SMIRK is Safe

Fig. 6: SMIRK logical view.

7 SMIRK System Architecture

SMIRK is a pedestrian emergency braking ADAS that demonstrates safety-
critical ML-based driving automation on SAE Level 1. The system uses input
from two sensors (camera and radar/LiDAR) and implements a DNN trained
for pedestrian detection and recognition. If the radar detects an imminent col-
lision between the ego car and an object, SMIRK will evaluate if the object is a
pedestrian. If SMIRK is conﬁdent that the object is a pedestrian, it will apply
emergency braking. To minimize hazardous false positives, SMIRK implements
a SMILE safety cage to reject input that is OOD. To ensure industrial rel-
evance, SMIRK builds on the reference architecture from PeVi, an ADAS
studied in previous work by Ben Abdessalem et al (2016).

Explicitly deﬁned architecture viewpoints support eﬀective communication
of certain aspects and layers of a system architecture. The diﬀerent viewpoints
of the identiﬁed stakeholders are covered by the established 4+1 view of archi-
tecture by Kruchten (1995). For SMIRK, we describe the logical view using a
simple illustration with limited embedded semantics complemented by textual
explanations. The process view is presented through a bulleted list, whereas the
interested reader can ﬁnd the remaining parts in the GitHub repository (RISE
Research Institutes of Sweden, 2022). Scenarios are illustrated with ﬁgures and
explanatory text.

7.1 Logical View

The SMIRK logical view is constituted by a description of the entities that
realize the PAEB. Figure 6 provides a graphical description.

SMIRK interacts with three external resources, i.e., hardware sensors and
actuators in ESI Pro-SiVIC: A) Mono Camera (752×480 (WVGA), sensor
dimension 3.13 cm × 2.00 cm, focal length 3.73 cm, angle of view 45 degrees),
B) Radar unit (providing object tracking and relative lateral and longitudinal
speeds), and C) Ego Car (An Audi A4 for which we are mostly concerned with
the brake system). SMIRK consists of the following constituents. We refer to

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

21

E), F), G), I), and J) as the Pedestrian Recognition Component, i.e., the
ML-based component for which this study presents a safety case.
• Software components implemented in Python:

– D) Radar Logic (calculating TTC based on relative speeds)
– E) Perception Orchestrator (the overall perception logic)
– F) Rule Engine (part of the safety cage, implementing heuristics such as

pedestrians do not ﬂy in the air)

– G) Uncertainty Manager (main part of the safety cage, implementing logic

to avoid false positives)

– H) Brake Manager (calculating and sending brake signals to the ego car)

• Trained Machine Learning models:

– I) Pedestrian Detector (a YOLOv5 model trained using PyTorch3
– J) Anomaly Detector (an autoencoder provided by Seldon4)

7.2 Process View

The process view deals with the dynamic aspects of SMIRK including an
overview of the run time behavior of the system. The overall SMIRK ﬂow is
as follows:

1. The Radar detects an object and sends the signature to the Radar Logic

class.

2. The Radar Logic class calculates the TTC. If a collision between the ego
car and the object is imminent, i.e., TTC is less than 4 seconds assuming
constant motion vectors, the Perception Orchestrator is notiﬁed.

3. The Perception Orchestrator forwards the most recent image from the
Camera to the Pedestrian Detector to evaluate if the detected object is a
pedestrian.

4. The Pedestrian Detector performs a pedestrian detection in the image and

returns the verdict (True/False) to the Pedestrian Orchestrator.

5. If there appears to be a pedestrian on a collision course, the Pedestrian
Orchestrator forwards the image and the radar signature to the Uncertainty
Manager in the safety cage.

6. The Uncertainty Manager sends the image to the Anomaly Detector and

requests an analysis of whether the camera input is OOD or not.

7. The Anomaly Detector analyzes the image in the light of the training data

and returns its verdict (True/False).

8. If there indeed appears to be an imminent collision with a pedestrian, the
Uncertainty Manager forwards all available information to the Rule Engine
for a sanity check.

9. The Rule Engine does a sanity check based on heuristics, e.g., in relation

to laws of physics, and returns a verdict (True/False).

3https://pytorch.org/
4https://www.seldon.io/

Springer Nature 2021 LATEX template

22

Ergo, SMIRK is Safe

10. The Uncertainty Manager aggregates all information and, if the conﬁdence
is above a threshold, notiﬁes the Brake Manager that collision with a
pedestrian is imminent.

11. The Brake Manager calculates a safe brake level and sends the signal to

Ego Car to commence PAEB.

8 SMIRK Data Management Speciﬁcation

This section describes the overall approach to data management for SMIRK
and the explicit data requirements. SMIRK is a demonstrator for a simu-
lated environment. Thus, as an alternative to longitudinal traﬃc observations
and consideration of accident statistics, we have analyzed the SMIRK ODD
through the ESI Pro-SiVIC “Object Catalog.” We conclude that the demo-
graphics of pedestrians in the ODD is constituted of the following: adult males
and females in either casual, business casual, or business casual clothes, young
boys wearing jeans and a sweatshirt, and male road workers. As other traﬃc
is not within the ODD (e.g., cars, motorcycles, and bicycles), we consider the
following basic shapes from the object catalog to as examples of OOD objects
(that still can appear in the ODD) for SMIRK to handle in operation: boxes,
cones, pyramids, spheres, and cylinders.

8.1 Data Requirements [L]

This section speciﬁes requirements on the data used to train and test the pedes-
trian recognition component. The data requirements are speciﬁed to comply
with the ML Safety Requirements in the SRS. All data requirements are orga-
nized according to the assurance-related desiderata proposed by Ashmore et al
(2021), i.e., the key assurance requirements that ensure that the data set is
relevant, complete, balanced, and accurate.

Table 2 shows a requirements traceability matrix between ML safety
requirements and data requirements. The matrix presents an overview of
how individual data requirements contribute to the satisfaction of ML Safety
Requirements. Entries in individual cells denote that the ML safety require-
ment is addressed, at least partly, by the corresponding data requirement.
SYS-PER-REQ4 and SYS-PER-REQ6 are not related to the data require-
ments.

8.1.1 Desideratum: Relevant

This desideratum considers the intersection between the data set and the
supported dynamic driving task in the ODD. The SMIRK training data will
not cover operational environments that are outside of the ODD, e.g., images
collected in heavy snowfall.
• DAT-REL-REQ1 All data samples shall represent images of a road from

the perspective of a vehicle.

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

23

Table 2: Requirements-Data traceability matrix.

• DAT-REL-REQ2 The format of each data sample shall be representative

of that which is captured using sensors deployed on the ego car.

• DAT-REL-REQ3 Each data sample shall assume sensor positioning

representative of the positioning used on the ego car.

• DAT-REL-REQ4 All data samples shall represent images of a road

environment that corresponds to the ODD.

• DAT-REL-REQ5 All data samples containing pedestrians shall include

one single pedestrian.

• DAT-REL-REQ6 Pedestrians included in data samples shall be of a type

that may appear in the ODD.

• DAT-REL-REQ7 All data samples representing non-pedestrian OOD

objects shall be of a type that may appear in the ODD.

Rationale: SMIRK adapts the requirements from the Relevant desiderata
speciﬁed by Gauerhof et al (2020) for the SMIRK ODD. DAT-REL-REQ5
is added based on the corresponding fundamental restriction of the ODD of
the SMIRK MVP. DAT-REL-REQ7 restricts data samples providing OOD
examples for testing.

8.1.2 Desideratum: Complete

This desideratum considers the sampling strategy across the input domain and
its subspaces. Suitable distributions and combinations of features are particu-
larly important. Ashmore et al (2021) refer to this as the external perspective
on the data.
• DAT-COM-REQ1 The data samples shall include the complete range of

environmental factors within the scope of the ODD.

Springer Nature 2021 LATEX template

24

Ergo, SMIRK is Safe

• DAT-COM-REQ2 The data samples shall include images representing all

types of pedestrians according to the demographics of the ODD.

• DAT-COM-REQ3 The data samples shall include images representing

pedestrians paces from standing still up to running at 15km/h.

• DAT-COM-REQ4 The data samples shall include images representing all
angles an upright pedestrian can be captured by the given sensors on the
ego car.

• DAT-COM-REQ5 The data samples shall include images representing all
distances to crossing pedestrians from 10 m up to 100 m away from ego car.
• DAT-COM-REQ6 The data samples shall include examples with diﬀerent
levels of occlusion giving partial views of pedestrians crossing the road.
• DAT-COM-REQ7 The data samples shall include a range of examples

reﬂecting the eﬀects of identiﬁed system failure modes.

Rationale: SMIRK adapts the requirements from the Complete desider-
ata speciﬁed by Gauerhof et al (2020) for the SMIRK ODD. We deliberately
replaced the original adjective “suﬃcient” to make the data requirements more
speciﬁc. Furthermore, we add DAT-COM-REQ3 to cover diﬀerent poses
related to the pace of the pedestrian and DAT-COM-REQ4 to cover diﬀerent
observation angles.

8.1.3 Desideratum: Balanced

This desideratum considers the distribution of features in the data set, e.g.,
the balance between the number of samples in each class. Ashmore et al (2021)
refer to this as an internal perspective on the data.
• DAT-BAL-REQ1 The data set shall have a representation of samples
for each relevant class and feature that ensures AI fairness with respect to
gender.

• DAT-BAL-REQ2 The data set shall have a representation of samples for
each relevant class and feature that ensures AI fairness with respect to age.
• DAT-BAL-REQ3 The data set shall contain both positive and negative

examples.

Rationale: SMIRK adapts the requirements from the Balanced desiderata
speciﬁed by Gauerhof et al (2020) for the SMIRK ODD. The concept of AI
fairness is to be interpreted in the light of the Ethics guidelines for trust-
worthy AI published by the European Commission (High-Level Expert Group
on Artiﬁcial Intelligence, 2019). Note that the number of ethical dimensions
that can be explored in through the ESI Pro-SiVIC object catalog is limited
to gender (DAT-BAL-REQ1) and age (DAT-BAL-REQ2). Moreover, the
object catalog does only contain male road workers and all children are boys.
Furthermore, DAT-BAL-REQ3 is primarily included to align with Gauerhof
et al (2020) and to preempt related questions by safety assessors. In practice,
the concept of negative examples when training object detection models are

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

25

typically satisﬁed implicitly as the parts of the images that do not belong to
the annotated class are true negatives (further explained in Section 10.1).

8.1.4 Desideratum: Accurate

This desideratum considers how measurement issues can aﬀect the way that
samples reﬂect the intended ODD, e.g., sensor accuracy and labeling errors.
• DAT-ACC-REQ1: All bounding boxes produced shall include the entirety

of the pedestrian.

• DAT-ACC-REQ2: All bounding boxes produced shall be no more than
10% larger in any dimension than the minimum sized box capable of
including the entirety of the pedestrian.

• DAT-ACC-REQ3: All pedestrians present in the data samples shall be

correctly labeled.

Rationale: SMIRK reuses the requirements from the Accurate desiderata

speciﬁed by Gauerhof et al (2020).

8.2 Data Generation Log [Q]

This section describes how the data used for training the ML model in the
pedestrian recognition component was generated. Based on the data require-
ments, we generate data using ESI Pro-SIVIC. The data are split into three
sets in accordance with AMLAS.
• Development data: Covering both training and validation data used by

developers to create models during ML development.
• Internal test data: Used by developers to test the model.
• Veriﬁcation data: Used in the independent test activity when the model is

ready for release.

The SMIRK data collection campaign focuses on generation of annotated
data in ESI Pro-SiVIC. All data generation is script-based and fully repro-
ducible. Section 8.2.1 describes positive examples [PX], i.e., humans that shall
be classiﬁed as pedestrians. Section 8.2.2 describes examples that represent
OOD shapes [NX], i.e., objects that shall not initiate PAEB in case of an immi-
nent collision. These images, referred to as OOD examples, shall either not be
recognized as a pedestrian or be rejected by the safety cage (see Section 9.3).
In the data collection scripts, ego car is always stationary whereas pedes-
trians and objects move according to speciﬁc conﬁgurations. The parameters
and values were selected to systematically cover the ODD. Finally, images
are sampled from the camera at 10 frames per second with a resolution of
752 × 480 pixels. For each image, we add a separate image ﬁle containing the
ground truth pixel-level annotation of the position of the pedestrian. In total,
we generate data representing 8 × 616 = 4, 928 execution scenarios with pos-
itive examples and 5 × 20 = 100 execution scenarios with OOD examples. In

Springer Nature 2021 LATEX template

26

Ergo, SMIRK is Safe

total, the data collection campaign generates roughly 185 GB of image data,
annotations, and meta-data (including bounding boxes).

8.2.1 Positive Examples

We generate positive examples from humans with eight visual appearances (see
the upper part of Figure 7) available in the ESI Pro-SiVIC object catalog:

P1 Casual female pedestrian
P2 Casual male pedestrian
P3 Business casual female pedestrian
P4 Business casual male pedestrian
P5 Business female pedestrian
P6 Business male pedestrian
P7 Child
P8 Male construction worker

For each of the eight visual appearances, we specify the execution of 616
scenarios in ESI Pro-SiVIC organized into four groups (A-D). The pedestrians
always follow rectilinear motion (a straight line) at a constant speed during
scenario execution. Groups A and B describe pedestrians crossing the road,
either from the left (Group A) or from the right (Group B). There are three
variation points, i.e., 1) the speed of the pedestrian, 2) the angle at which the
pedestrian crosses the road, and 3) the longitudinal distance between ego car
and the pedestrian’s starting point. In all scenarios, the distance between the
starting point of the pedestrian and the edge of the road is 5 m.
• A. Crossing the road from left to right (4 × 7 × 10 = 280 scenarios)

– Speed (m/s): [1, 2, 3, 4]
– Angle (degree): [30, 50, 70, 90, 110, 130, 150]
– Longitudinal distance (m): [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
• B. Crossing the road from right to left (4 × 7 × 10 = 280 scenarios)

– Speed (m/s): [1, 2, 3, 4]
– Angle (degree): [30, 50, 70, 90, 110, 130, 150]
– Longitudinal distance (m): [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

Groups C and D describe pedestrians moving parallel to the road, either
toward ego car (Group C) or away (Group D). There are two variation points,
i.e., 1) the speed of the pedestrian and 2) an oﬀset from the road center. The
pedestrian always moves 90 m, with a longitudinal distance between ego car
and the pedestrian’s starting point of 100 m for Group C (towards) and 10 m
for Group D (away).
• C. Movement parallel to the road toward ego car (4 × 7= 28 scenarios)

– Speed (m/s): [1, 2, 3, 4]
– Lateral oﬀset (m): [-3, -2, -1, 0, 1, 2, 3]

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

27

Fig. 7: Visual appearance of pedestrians (P1–P8) and basic shapes (N1–N5).

• D. Movement parallel to the road away from ego car (4 × 7= 28 scenarios)

– Speed (m/s): [1, 2, 3, 4]
– Lateral oﬀset (m): [-3, -2, -1, 0, 1, 2, 3]

8.2.2 Out-of-Distribution Examples

We generate OOD examples using ﬁve basic shapes (see the lower part of
Figure 7) available in the ESI Pro-SiVIC object catalog:

N1 Sphere
N2 Cube
N3 Cone
N4 Pyramid
N5 Cylinder

For each of the ﬁve basic shapes, we specify the execution of 20 scenarios
in ESI Pro-SiVIC. The scenarios represent a basic shape crossing the road
from the left or right at an angle perpendicular to the road. Since basic shapes
are not animated, we ﬁx the speed at 4 m/s. Moreover, as lateral oﬀsets and
diﬀerent angles make little to no diﬀerence in front of the camera, we disregard
these variation points. In all scenarios, the distance between the starting point
of the basic shape and the edge of the road is 5 m. The only variation points
are the crossing direction and the longitudinal distance between ego car and
the objects’ starting point. As for pedestrians, the objects always follow a
rectilinear motion at a constant speed during scenario execution.
• Crossing direction: [left, right]
• Longitudinal distance (m): [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

Springer Nature 2021 LATEX template

28

Ergo, SMIRK is Safe

8.2.3 Preprocessing and Data Splitting

As the SMIRK data collection campaign relies on data generation in ESI Pro-
SiVIC, the need for pre-processing diﬀers from counterparts using naturalistic
data. The script-based data generation ensures that the crossing pedestrians
and objects appear at the right distance with speciﬁed conditions and with
controlled levels of occlusion. All output images share the same characteristics,
thus no normalization is needed. SMIRK includes a script to generate bounding
boxes for training the object detection model. ESI Pro-SiVIC generates ground
truth image segmentation on a pixel-level. The script is used to convert the
output to the appropriate input format for model training.

The development data contains images with no pedestrians, in line with the
description of “background images” in the YOLOv5 training tips provided by
Ultralytics5. Background images have no objects for the model to detect, and
are added to reduce FPs. Ultralytics recommends 0-10% background images
to help reduce FPs and reports that the fraction of background images in
the well-known COCO data set is 1% (Lin et al, 2014). In our case, we add
background images with cylinders (N5) to the development data. In total,
the SMIRK development data contains 1.98% background images, i.e., 1.75%
images without any objects and 0.23% with a cylinder.

The generated data are used in three sequestered (separated) data sets:

• Development data: P2, P3, P6, and N5
• Internal test data: P1, P4, N1, and N3
• Veriﬁcation data: P5, P7, P8, N2, and N4

Note that we deliberately avoid mixing pedestrian models from the ESI
Pro-SiVIC object catalog in the data sets due to the limited diversity in the
images within the ODD.

9 Machine Learning Component Speciﬁcation

The pedestrian recognition component consists of two ML-based constituents:
a pedestrian detector and an anomaly detector (see Figure 6).

9.1 Pedestrian Detection Using YOLOv5s

SMIRK implements its pedestrian recognition component using the third-
party OSS framework YOLOv5 by Ultralytics. Based on Ultralytics’ publicly
reported experiments on real-time characteristics of diﬀerent YOLOv5 archi-
tectures6, we found that YOLOv5s stroke the best balance between inference
time and accuracy for SMIRK. After validating the feasibility in our experi-
mental setup, we proceeded with this ML architecture selection.

The pedestrian recognition component uses the YOLOv5 architecture with-
out any modiﬁcations. YOLOv5s has 191 layers and ≈7.5 million parameters.

5https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results
6https://github.com/ultralytics/yolov5

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

29

We use the default conﬁgurations proposed in YOLOv5s regarding activa-
tion, optimization, and cost functions. As activation functions, YOLOv5s uses
Leaky ReLU in the hidden layers and the sigmoid function in the ﬁnal layer.
We use the default optimization function in YOLOv5s, i.e., stochastic gradient
descent. The default cost function in YOLOv5s is binary cross-entropy with
logits loss as provided in PyTorch, which we also use. We refer the interested
reader to further details provided by Rajput (2020) and Ultralytics’ GitHub
repository.

9.2 Model Development Log [U]

This section describes how the YOLOv5s model has been trained for the
SMIRK MVP. We followed the general process presented by Ultralytics for
training on custom data.

First, we manually prepared two SMIRK data sets to match the input
format of YOLOv5. In this step, we also divided the development data [N] into
two parts. The ﬁrst part containing approximately 80% of development data,
was used for training. The second part, consisting of the remaining data, was
used for validation. Camera frames from the same video sequence were kept
together in the same partition to avoid having almost identical images in the
training and validation sets. Additionally, we kept the distribution of objects
and scenario types consistent in both partitions. The internal test data [O] was
used as a test set. We then prepared these three data sets, training, validation,
and test, according to Ultralytic’s instructions and trained YOLOv5 for a
single class, i.e., pedestrians. The data sets were already annotated using ESI
Pro-SiVIC, thus we only needed to export the labels to the YOLO format with
one txt-ﬁle per image. Finally, we organize the individual ﬁles (images and
labels) according to the YOLOv5 instructions. More speciﬁcally, each label ﬁle
contains the following information:
• One row per object.
• Each row contains class, x center, y center, width, and height.
• Box coordinates are stored in normalized xywh format (from 0 – 1).
• Class numbers are zero-indexed, i.e., they start from 0.

Second, we trained a YOLO model using the YOLOv5s architecture with
the development data without any pre-trained weights. The model was trained
for 10 epochs with a batch-size of 8. The results from the validation subset
(27,843 images in total) of the development data guide the selection of the
conﬁdence threshold for the ML model. We select a threshold to meet SYS-
PER-REQ3 with a safety margin for the development data, i.e., an FPPI of
0.1%. This yields a conﬁdence threshold for the ML model to classify an object
as a pedestrian that equals 0.448. The ﬁnal pedestrian detection model, i.e.,
the ML model [V], has a size of ≈ 14 MB.

Springer Nature 2021 LATEX template

30

Ergo, SMIRK is Safe

Fig. 8: Overview architecture of an autoencoder. Adapted from Wik-
iUser:EugenioTL (CC BY-SA 4.0)

9.3 OOD Detection for the Safety Cage Architecture

SMIRK detects OOD input images as part of its safety cage architecture. The
OOD detection relies on the OSS third-party library Alibi Detect7 from Seldon.
Alibi Detect is a Python library that provides several algorithms for outlier,
adversarial, and drift detection for various types of data (Klaise et al, 2020).
For SMIRK, we trained Alibi Detect’s autoencoder for outlier detection, with
three convolutional and deconvolutional layers for the encoder and decoder
respectively.

Figure 8 shows an overview of the DNN architecture of an autoencoder.
An encoder and a decoder are trained jointly in two steps to minimize a recon-
struction error. First, the autoencoder receives input data X and encodes it
into a latent space of fewer dimensions. Second, the decoder tries to recon-
struct the original data and produces output X (cid:48). An and Cho (2015) proposed
using the reconstruction error from a autoencoder to identify input that diﬀers
from the training data. Intuitively, if inlier data is processed by the autoen-
coder, the diﬀerence between X and X (cid:48) will be smaller than for outlier data,
i.e., OOD data will stand out. By carefully selecting a tolerance threshold, this
approach can be used for OOD detection.

For SMIRK, we trained Alibi Detect’s autoencoder for OOD detection
on the training data subset of the development data. The encoder part is
designed with three convolutional layers followed by a dense layer resulting in
a bottleneck that compresses the input by 96.66%. The latent dimension is
limited to 1,024 variables to limit requirements on processing VRAM of the
GPU. The reconstruction error from the autoencoder is measured as the mean
squared error between the input and the reconstructed instance. The mean
squared error is used for OOD detection by computing the reconstruction
error and considering an input image as an outlier if the error surpasses a
threshold θ. The threshold used for OOD detection in SMIRK is 0.004, roughly
corresponding to the threshold that rejects a number of samples that equals
the amount of outliers in the validation set. As explained in Section 11.4, the
OOD detection is only active for objects at least 10 m away from ego car

7https://github.com/SeldonIO/alibi-detect

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

31

as the results for close-up images are highly unreliable. Furthermore, as the
constrained SMIRK ODD ensures that only one single object appears in each
scenario, the safety cage architecture applies the policy “once an anomaly,
always an anomaly” – objects that get rejected once will remain anomalous
no matter what subsequent frames might contain.

10 SMIRK System Test Speciﬁcation

This section describes the overall SMIRK test strategy. The ML-based pedes-
trian recognition component is tested on multiple levels. We focus on four
aspects of the ML testing scope facet proposed by Song et al (2022):
• Data set testing: This level refers to automatic checks that verify that spe-
ciﬁc properties of the data set are satisﬁed. As described in the ML Data
Validation Results, the data validation, presented in Section 11.1, includes
automated testing of the Balance desiderata. Since the SMIRK MVP relies
on synthetic data, the distribution of pedestrians is already ensured by
the scripts. However, other distributions such as distances to objects and
bounding box sizes are important targets for data set testing.

• Model testing: Testing that the ML model provides the expected output.
This is the primary focus of academic research on ML testing, and includes
white-box, black-box, and data-box access levels during testing (Riccio et al,
2020). SMIRK model testing is done independently from model development
and results in ML Veriﬁcation Results [Z] as described in Section 11.2.2.
• Unit testing: Conventional unit testing on the level of Python classes. A test
suite developed for the pytest framework is maintained by the developers
and not elaborated further in this paper.

• System testing: System-level testing of SMIRK based on a set of Operational
Scenarios [EE]. All test cases are designed for execution in ESI Pro-SiVIC.
The system testing targets the requirements in the System Requirements
Speciﬁcation. This level of testing results in Integration Testing Results [FF]
presented in Section 11.3.

10.1 ML Model Testing [AA]

This section corresponds to the Veriﬁcation Log [AA] in AMLAS Step 5, i.e.,
Model Veriﬁcation Assurance. Here we explicitly document the ML Model test-
ing strategy, i.e., the range of tests undertaken and bounds and test parameters
motivated by the SMIRK system requirements.

The testing of the ML model is based on assessing the object detection
accuracy for the sequestered veriﬁcation data set. A fundamental aspect of
the veriﬁcation argument is that this data set was never used in any way
during the development of the ML model. To further ensure the independence
of the ML veriﬁcation, engineers from Infotiv, part of the SMILE III research
consortium, led the corresponding veriﬁcation and validation work package
and were not in any way involved in the development of the ML model. As

Springer Nature 2021 LATEX template

32

Ergo, SMIRK is Safe

described in the Machine Learning Component Speciﬁcation (see Section 9),
the ML development was led by Semcon with support from RISE Research
Institutes of Sweden.

The ML model test cases provide results for both 1) the entire veriﬁcation
data set and 2) eight slices of the data set that are deemed particularly impor-
tant. The selection of slices was motivated by either an analysis of the available
technology or ethical considerations, especially from the perspective of AI fair-
ness (Borg et al, 2021b). Consequently, we measure the performance for the
following slices of data. Identiﬁers in parentheses show direct connections to
requirements.

S1 The entire veriﬁcation data set
S2 Pedestrians close to the ego car (longitudinal distance < 50 m) (SYS-PER-

REQ1, SYS-PER-REQ2)

S3 Pedestrians far from the ego car (longitudinal distance ≥ 50 m)
S4 Running pedestrians (speed ≥ 3 m/s) (SYS-ROB-REQ2)
S5 Walking pedestrians (speed > 0 m/s but < 3 m/s) (SYS-ROB-REQ2)
S6 Occluded pedestrians (entering or leaving the ﬁeld of view, deﬁned as
bounding box in contact with any edge of image) (DAT-COM-REQ4)

S7 Male pedestrians (DAT-COM-REQ2)
S8 Female pedestrians (DAT-COM-REQ2)
S9 Children (DAT-COM-REQ2)

Evaluating the output from an object detection model in computer vision is
non-trivial. We rely on the established IoU metric to evaluate the accuracy of
the YOLOv5 model. After discussions in the development team, supported by
visualizations8, we set the target at 0.5. We recognize that there are alternative
measures tailored for pedestrian detection, such as the log-average miss rate
(Dollar et al, 2011) but we ﬁnd such metrics to be unnecessarily complex for
the restricted SMIRK ODD with a single pedestrian. There are also entire
toolboxes that can be used to assess object detection (Bolya et al, 2020).
Whereas more complex metrics could be used, we decide to use IoU in SMIRK’s
safety argumentation. Relying on a simpler metric supports interpretability,
which is a vital to any safety case, especially if ML is involved (Jia et al, 2022).
Even using the standard IoU metric to assess how accurate SMIRK’s ML
model is, the evaluation results are not necessarily intuitive to non-experts.
Each image in the SMIRK data set either has a ground truth bounding box
containing the pedestrian or no bounding box at all. Similarly, when perform-
ing inference on an image, the ML model will either predict a bounding box
containing a potential pedestrian or no bounding box at all. IoU is the inter-
section over the union of the two bounding boxes. An IoU of 1 implies a perfect
overlap. For the ML model in SMIRK, we evaluate pedestrian detection at IoU
= 0.5, which for each image means:

TP True positive: IoU ≥ 0.5
FP False positive: IoU < 0.5

8https://zapirfe.com/docs/visualizing-ml/iou.html

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

33

Fig. 9: Example predictions from the SMIRK ML model. The center image
represents a TP, whereas the left and right examples are FPs with IoU scores
of 0.3 and 0.4, respectively.

FN False negative: There is a ground truth bounding box in the image, but no

predicted bounding box.

TN True negative: All parts of the image with neither a ground truth nor a
predicted bounding box. This output carries no meaning in our case.

Figure 9 shows predictions from the the ML model. The green rectangles
show the ground truth and the red rectangles show ML model’s prediction of
where a pedestrian is present. The left example is a FP since IoU=0.3 with
a predicted box substantially smaller than the ground truth. On the other
hand, the ground truth is questionable, as there probably is only a single pixel
containing the pedestrian below the visible arm that drastically increases the
size of the green box. The center example is a TP with IoU=0.9, i.e., the
overlap between the boxes is very large. The right example is another FP with
IoU=0.4 where the predicted box is much larger than the ground truth. These
examples show that FPs during model testing do not directly translate to FPs
on the system level as discussed in the HARA (Safety Requirements Allocated
to ML Component [E]). If any of the objects within the red bounding boxes
were on a collision course with the ego car, commencing PAEB would indeed
be the right action for SMIRK and thus not violate SYS-SAF-REQ1. This
observation corroborates the position by (Haq et al, 2021b), i.e., system level
testing that goes beyond model testing on single frames is critically needed.

All results from running ML model testing, i.e., ML Veriﬁcation Results

[Z], are documented in the Protocols folder.

10.2 System Level Testing

System-level testing of SMIRK involves integrating the ML model into the
pedestrian recognition component and the complete PAEB ADAS. We do this

Springer Nature 2021 LATEX template

34

Ergo, SMIRK is Safe

by deﬁning a set of Operational Scenarios [EE] for which we assess the satisfac-
tion of the ML Safety Requirements. The results from the system-level testing,
i.e., the Integration Testing Results [FF], are presented in Section 11.3.

10.2.1 Operational Scenarios

SOTIF deﬁnes an operational scenario as “a description of an imagined
sequence of events that includes the interaction of the product or service with
its environment and users, as well as interaction among its product or service
components”. Consequently, the set of operational scenarios used for testing
SMIRK on the system level must represent the diversity of real scenarios that
may be encountered when SMIRK is in operation. Furthermore, for testing
purposes, it is vital that the set of deﬁned scenarios are meaningful with respect
to the veriﬁcation of SMIRK’s safety requirements.

As SMIRK is designed to operate in ESI Pro-SiVIC, the diﬀerence between
deﬁning operational scenarios in text and implementation scripts to execute
the same scenarios in the simulated environment is very small. We will not
deﬁne any operational scenarios that cannot be scripted for execution in ESI
Pro-SiVIC. To identify a meaningful set of operational scenarios, we use equiv-
alence partitioning as proposed by Masuda (2017) as one approach to limit
the number of test scenarios to execute in vehicle simulators. Originating in
the equivalence classes, we use combinatorial testing to reduce the set of oper-
ational scenarios. Using combinatorial testing to create test cases for system
testing of a PAEB testing in a vehicle simulator has previously been reported
by Tao et al (2019). We create operational scenarios that provide complete
pair-wise testing of SMIRK considering the identiﬁed equivalence classes using
the AllPairs test combinations generator9.

Based on an analysis of the ML Safety Requirements and the Data
Requirements, we deﬁne operational scenarios addressing SYS-ML-REQ1
and SYS-ML-REQ2 separately. For each subset of operational scenarios, we
identify key variation dimensions (i.e., parameters in the test scenario genera-
tion) and split dimensions into equivalence classes using explicit ranges. Note
that ESI Pro-SiVIC enables limited conﬁgurability of basic shapes compared
to pedestrians, thus the corresponding number of operational scenarios is
lower.

Operational Scenarios for SYS-ML-REQ1:
• Pedestrian starting point (lateral oﬀset from the road in meters): Left side
of the road (-5 m), Center of the road (0 m), Right side of the road (5 m)
• Longitudinal distance from ego car (oﬀset in meters): Close distance (<

25 m), Medium distance (25–50 m), Far away (> 50 m)

• Pedestrian appearance: Female casual, Male business casual, Male business,

Female business, Child, Male worker

• Pedestrian speed (m/s): Stationary (0 m/s), Slow (1 m/s), Fast (3 m/s)

9https://github.com/thombashi/allpairspy

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

35

• Pedestrian crossing angle (degrees): Toward ego car (0), Diagonal toward

(45), Perpendicular (90), Diagonal away (135), Away from car (180)

• Ego car speed (m/s): Slow (< 10 m/s), Medium (10–15 m/s), Fast (15–

20 m/s)

The dimensions and ranges listed above result in 2,430 possible combina-
tions. Using combinatorial testing, we create a set of 25 operational scenarios
that provides pair-wise coverage of all equivalence classes.

Operational Scenarios for SYS-ML-REQ2:
• Object starting point (lateral oﬀset from the road in meters): Left side of

the road (-5 m), On the road (0 m), Right side of the road (5 m)

• Longitudinal distance from ego car (oﬀset in meters): Close (< 25 m),

Medium distance (25–50 m), Far away (> 50 m)
• Object appearance: Sphere, Cube, Cone, Pyramid
• Object speed (m/s): Stationary (0 m/s), Slow (1 m/s), Fast (3 m/s)
• Ego car speed (m/s): Slow (<10 m/s), Medium (10–15 m/s), Fast (15–

20 m/s)

The dimensions and ranges listed above result in 324 possible combinations.
Using combinatorial testing, we create a set of 13 operational scenarios that
provides pair-wise coverage of all equivalence classes.

For each operational scenario, two test parameters represent ranges of val-
ues, i.e., the longitudinal distance between ego car and the pedestrian and the
speed of ego car. For these two test parameters, we identify a combination
of values that result in a collision unless SMIRK initiates emergency braking.
Table 3 shows an overview of the 38 operational scenarios, whereas all details
are available as executable test scenarios in the GitHub repository.

10.2.2 System Test Cases

The system test cases are split into three categories. First, each operational
scenario identiﬁed in Section 10.2.1 constitutes one system test case, i.e., Test
Cases 1-38. Second, to increase the diversity of the test cases in the simulated
environment, we complement the reproducible Test Cases 1-38 with test case
counterparts adding random jitter to the parameters. For Test Cases 1-38, we
create analogous test cases that randomly add jitter in the range from -10%
to +10% to all numerical values. Partial random testing has been proposed by
Masuda (2017) in the context of test scenarios execution in vehicle simulators.
Note that introducing random jitter to the test input does not lead to the test
oracle problem (Barr et al, 2014), as we can automatically assess whether there
is a collision between ego car and the pedestrian without emergency braking
in ESI Pro-SiVIC or not (TC-RAND-[1-25]). Furthermore, for the test cases
related to provoking ghost braking, we know that emergency braking shall not
commence.

Springer Nature 2021 LATEX template

36

Ergo, SMIRK is Safe

Table 3: Pairwise-testing of equivalence classes constituting 38 operational
scenarios.

ID
TC-OS-1
TC-OS-2
TC-OS-3
TC-OS-4
TC-OS-5
TC-OS-6
TC-OS-7
TC-OS-8
TC-OS-9
TC-OS-10
TC-OS-11
TC-OS-12
TC-OS-13
TC-OS-14
TC-OS-15
TC-OS-16
TC-OS-17
TC-OS-18
TC-OS-19
TC-OS-20
TC-OS-21
TC-OS-22
TC-OS-23
TC-OS-24
TC-OS-25
TC-OS-26
TC-OS-27
TC-OS-28
TC-OS-29
TC-OS-30
TC-OS-31
TC-OS-32
TC-OS-33
TC-OS-34
TC-OS-35
TC-OS-36
TC-OS-37
TC-OS-38

Object
P7
P7
P7
P5
P5
P1
P6
P6
P1
P4
P8
P8
P4
P5
P6
P4
P8
P1
P1
P6
P1
P5
P4
P7
P8
N2
N3
N4
N1
N1
N1
N4
N3
N2
N3
N2
N3
N1

X
close

Y
left

medium center
right
right
center
left
left
center
right
right
left
left
center
left
right
left
right
center
left
left
left
left
left
left
left
left
right
right
left
left
right
left
left
right
left
left
left
left

far
medium
close
far
medium
far
close
left
close
far
far
close
close
medium
close
close
medium
medium
medium
medium
medium
medium
medium
close
medium
far
far
medium
close
close
medium
far
far
medium
close
close

Angle
diagonal away
away
diagonal towards
perpendicular
towards
diagonal away
perpendicular
away
diagonal towards
diagonal away
perpendicular
diagonal towards
perpendicular
diagonal away
diagonal away
diagonal towards
diagonal away
diagonal towards
diagonal towards
diagonal towards
perpendicular
diagonal towards
diagonal away
perpendicular
diagonal towards
perpendicular
perpendicular
perpendicular
perpendicular
perpendicular
perpendicular
perpendicular
perpendicular
perpendicular
perpendicular
perpendicular
perpendicular
perpendicular

Speed
slow
stationary
fast
slow
fast
fast
fast
slow
slow
slow
fast
slow
stationary
slow
slow
slow
slow
stationary
slow
slow
slow
slow
slow
slow
slow
slow
fast
slow
slow
fast
fast
fast
slow
slow
slow
fast
slow
slow

Car Speed
slow
medium
fast
medium
fast
fast
medium
slow
slow
slow
slow
medium
fast
slow
slow
slow
slow
medium
slow
slow
medium
slow
medium
slow
slow
slow
medium
fast
medium
fast
slow
fast
slow
slow
fast
medium
slow
medium

The third category is requirements-based testing (RBT). RBT is used to
gain conﬁdence that the functionality speciﬁed in the ML Safety Require-
ments has been implemented correctly (Hauer et al, 2019). The top-level
safety requirement SYS-SAF-REQ1 will be veriﬁed by testing of all underly-
ing requirements, i.e., its constituent detailed requirements. The test strategy
relies on demonstrating that SYS-ML-REQ1 and SYS-ML-REQ2 are sat-
isﬁed when executing TC-OS-[1-38] and TC-RAND-[1-38]. SYS-PER-REQ1
– SYS-PER-REQ5 and SYS-ROB-REQ1 – SYS-ROB-REQ4 are ver-
iﬁed through the model testing described in Section 10.1. The remaining

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

37

Table 4: SMIRK system test cases. VMC means valid metrics were collected
during execution of the 38 preceding scenarios.
Given
Scenario
[1-25]

SMIRK
PAEB

commences

Then

ID

TC-
OS-[1-
25]
TC-
OS-[26-
38]
TC-
RAND-
[1-25]
TC-
RAND-
[19-38]
TC-
RBT-1

TC-
RBT-2

TC-
RBT-3

Type
Op.
Sce-
nario
Op.
Sce-
nario
Random
Testing

Random
Testing

RBT
(SYS-
ML-
REQ1)
RBT
(SYS-
ML-
REQ2)
RBT
(SYS-
PER-
REQ6)

Scenario
[26-38]

TC-
OS-[1-
25]+jitter
TC-
OS-[19-
38]+jitter
VMC

VMC

VMC

When
Pedestrian crosses the street
and ego car is on collision
course
Object crosses the street and
ego car is on collision course

Pedestrian crosses the street
and ego car is on collision
course
Object crosses the street and
ego car is on collision course

SMIRK does not com-
mence PAEB

SMIRK
PAEB

commences

SMIRK does not com-
mence PAEB

tracking compo-
The radar
nent returns a pedestrian with
TTC < 4 s

The pedestrian recogni-
tion component identiﬁes
the pedestrian

The radar tracking component
returns a basic shape with
TTC < 4 s

The pedestrian recogni-
tion component does not
identify a pedestrian

The radar
tracking compo-
nent returns a pedestrian with
TTC < 4 s within 80 m

The inference speed of
the pedestrian recogni-
tion component is at least
10 FPS

performance requirement SYS-PER-REQ6 is veriﬁed by TC-REQ-3. Table 4
lists all system test cases, of all three categories, using the Given-When-Then
structure as used in behavior-driven development (Tsilionis et al, 2021). For
the test cases TC-RBT-[1-3], the “Given” condition is that all metrics have
been collected during execution of TC-OS-[1-38] and TC-RAND-[1-38]. The
set includes seven metrics:

1. MinDist: Minimum distance between ego car and the pedestrian during a

scenario.

2. TimeTrig: Time when the radar tracking component ﬁrst returned

TTC < 4 s for an object.

3. DistTrig: Distance between ego car and the object when the radar

component ﬁrst returned TTC < 4 s for an object.

4. TimeBrake: Time when emergency braking was commenced.
5. DistBrake: Distance between ego car and the object when emergency

braking commenced.

6. Coll: Whether a scenario involved a collision between ego car and a

pedestrian.

7. CollSpeed: Speed of ego car at the time of collision.

Springer Nature 2021 LATEX template

38

Ergo, SMIRK is Safe

Table 5: Distribution of pedestrian types in Sweden and in the SMIRK data
set.

Pedestrian types
Children & young adults (0-19)
Adult males (20+)
Adult females (20+)

Population Accidents Deadly accidents

23%
39%
38%

27%
31%
42%

12%
57%
31%

SMIRK
12.5%
50%
37.5%

11 SMIRK Test Results

This section presents the most important test results from three levels of ML
testing, i.e., data testing, model testing, and system testing. Complete test
reports are available in the protocols subfolder on GitHub10. Moreover, this
section presents the Erroneous Behaviour Log.

11.1 Results from Data Testing [S]

This section describes the results from testing the SMIRK data set. The data
testing primarily involves a statistical analysis of its distribution and auto-
mated data validation using Great Expectations11. Together with the outcome
of the Fagan inspection of the Data Management Speciﬁcation (described in
Section 4.1), this constitutes the ML Data Validation Results in AMLAS. As
depicted later in Figure A3, the results entail evidence mapping to the four
assurance-related desiderata, i.e., we report a validation of 1) data relevance,
2) data completeness, 3) data balance, and 4) data accuracy. Since we gen-
erate synthetic data using ESI Pro-SiVIC, data relevance has been validated
through code reviews and data accuracy is implicit as the tool’s ground truth
is used. For both the relevance and accuracy desiderata, we have manually
analyzed a sample of the generated data to verify requirements satisfaction.

We validate the ethical dimension of the data balance by analyzing the gen-
der (DAT-BAL-REQ1) and age (DAT-BAL-REQ2) distributions of the
pedestrians in the SMIRK data set. SMIRK evolves as a demonstrator in a
Swedish research project, which provides a frame of reference for this analysis.
Table 5 shows how the SMIRK data set compares to Swedish demographics
from the perspective of age and gender. The demographics originate in a study
on collisions between vehicles and pedestrians by the Swedish Civil Contin-
gencies Agency (Schyllander, 2014). We notice that 1) children are slightly
over-represented in accidents but under-represented in deadly accidents, and
that 2) adult males account for over half of the deadly accidents in Sweden.
The rightmost column shows the distribution of pedestrian types in the entire
SMIRK data set. We designed the SMIRK data generation process to result in
a data set that resembles the deadly accidents in Sweden, but, motivated by AI
fairness, we increased the fraction of female pedestrians to mitigate a poten-
tial gender bias. Finally, as discussed in Section 8.2.3, code reviews conﬁrmed
that the development data contains roughly 2% “background images”.

10https://github.com/RI-SE/smirk/tree/main/docs/protocols
11https://greatexpectations.io/

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

39

Automated data testing is performed by deﬁning conditions that shall be
fulﬁlled by the data set. These conditions are checked against the existing
data and any new data that is added. Some tests are ﬁxed and simple, such
as expecting the dimensions of input images to match the ones produced by
the vehicle’s camera. Similarly, all bounding boxes are expected to be within
the dimensions of the image. Other tests look at the distribution and ranges of
values to assure the completeness, accuracy, and balance of the data set and
catch human errors. This includes validating enough coverage of pedestrians
at diﬀerent positions of the image, coverage of varying range of pedestrian
distances, and bounding box aspect ratios. For values that are hard to deﬁne
rules for, a known good set of inputs can be used as a starting point and
remaining and new inputs can be checked to against these reference inputs.
As an example, this can be used to verify that the color distribution and pixel
intensity are within expected ranges. This can be used to identify images that
are too dark or dissimilar to existing images.

Figure 10 shows a selection of summary plots from the data testing that
support our claims for data validity, in particular from the perspective of
data completeness. Subﬁgure 10a presents the distance distribution between
ego car and pedestrians, verifying that the data set contains pedestrians at
distances 10–100 m (DAT-COM-REQ5). Subﬁgure 10b shows a heatmap
of the bounding boxes’ centers captures by the 752x480 WVGA camera. We
conﬁrm that pedestrians appear from the sides of the ﬁeld of view and a
larger fraction of images contain a pedestrian just in front of ego car. The
position distribution supports our claim that DAT-COM-REQ4 is satisﬁed,
i.e., the data samples represent diﬀerent camera angles. Subﬁgure 10c shows
a heatmap of bounding box dimensions, i.e., diﬀerent aspect ratios. A variety
of aspect ratios indicate that pedestrians move with a diversity of arm and
leg movements – indicating walking and running – and thus support our claim
that DAT-COM-REQ3 is fulﬁlled. Finally, Subﬁgure 10d shows the color
histogram of the data set. In the automated data testing, we use this as a
reference value when adding new images to ensure that they match the ODD.
For example, a sample of nighttime images would have a substantially diﬀerent
color distribution.

11.2 Results from Model Testing

This section is split into results from testing done during development and the
subsequent independent veriﬁcation. Throughout this section, the following
abbreviations are used for a set of standard evaluation metrics: Precision (P),
Recall (R), F1-score (F1), Intersection over Union (IoU), True Positive (TP),
False Positive (FP), FPs Per Image (FPPI), False Negative (FN), and Average
Precision for IoU at 0.5 (AP@0.5), and Conﬁdence (Conf).

Springer Nature 2021 LATEX template

40

Ergo, SMIRK is Safe

(a) Distance distributions between ego
car and pedestrians (m).

(b) Heatmap of center positions of the
bounding boxes (WVGA pixel).

(c) Heatmap of bounding box dimen-
sions (pixels).

(d) The color histogram of the data set.

Fig. 10: Four visualizations from the data testing.

11.2.1 Internal Test Results [X]

In this section, we present the most important results from the internal testing.
These results provide evidence that the ML model satisﬁes the ML safety
requirements (see Section 6.3) on the internal test data. The total number of
images in the internal test data is 139,526 (135,139 pedestrians (96.9%) and
4,387 non-pedestrians (3.1%)). As described in Section 10.1, Figure 11 depicts
four subplots representing IoU = 0.5: A) P vs R, B) F1 vs. Conf, C) P vs. Conf,
and D) R vs. Conf. Subﬁgure 11a shows that the ML model is highly accurate,
i.e., the unavoidable discrimination-eﬃciency tradeoﬀ of object detection (Wu
and Nevatia, 2008) is only visible in the upper right corner. Subﬁgures 11b–11d
show how P, R, and F1 vary with diﬀerent Conf thresholds. Table 6 presents
further details of the accuracy of the ML model for the selected Conf threshold,
organized into 1) all distances from the ego car, 2) within 80 m, and 3) within
50 m, respectively. The table also shows the eﬀect of adding OOD detection
using the autoencoder, i.e., a substantially reduced number of FPs.

Table 7 demonstrates how the ML model satisﬁes the performance require-
ments on the internal test data. First, the TP rate (95.9%) and the FN rate

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

41

(a) P–R curve.

(b) F1 vs. Conf.

(c) P vs. Conf.

(d) R vs. Conf.

Fig. 11: Evaluation of the ML model on the internal test data at IoU=0.5.

Distance
All

Table 6: ML model accuracy on the internal test data at the Conf threshold
0.448. The rows show results for all distances, within 80 m, and within 50
m, respectively. Every second row show results for the ML model followed by
OOD detection using the autoencoder.
FP FN
191
711
212
20
173
444
193
13
173
186
193
13

AP@0.5
0.9942
0.995
0.9948
0.995
0.9944
0.995

TP
134,948
134,927
101,320
101,300
57,877
57,857

F1
0.9967
0.9991
0.997
0.999
0.9969
0.9982

R
0.9986
0.9984
0.9983
0.9981
0.9970
0.9967

P
0.9948
0.9999
0.9956
0.9999
0.9968
0.9998

Total
139,526

105,588

≤ 80 m

≤ 50 m

+OOD

+OOD

+OOD

61,845

Springer Nature 2021 LATEX template

42

Ergo, SMIRK is Safe

Table 7: ML model satisfaction of the performance requirements on the
internal test data at the Conf threshold 0.448. R1–R4 = SYS-PER-REQ1–4.
The rightmost column show results for the YOLOv5 model followed by OOD
detection using the autoencoder.

Req.

R1

R2

R3

R4

Expected
TP rate ≥ 93%
for ≤ 80 m
FN rate ≤ 7%
for ≤ 50 m
FPPI ≤ 0.1%
for ≤ 80 m
≤ 3% of rolling windows
contain ≥ 2 misses in
5 frames for ≤ 80 m

Observed (Model) Observed (Model+OOD)

101,320
105,588 = 96.0%

173
61,845 = 0.28%

444

105,588 = 0.42%

101,300
105,588 = 95.9%

193
61,845 = 0.31%

13

105,588 = 0.012%

216

101,564 = 0.21%

239

101,564 = 0.24%

(0.31%) for the respective distances meet the requirements. The model’s FPPI
(0.42%), on the other hand, is too high to meet SYS-PER-REQ3 as we
observed 444 FPs (cones outnumber spheres by 2:1). This observation rein-
forces the need to use a safety cage architecture, i.e., OOD detection that can
reject input that does not resemble the training data. The rightmost column
in Table 7 shows how the FPPI decreased to 0.012% with the autoencoder. All
basic shapes were rejected, but 13 images with pedestrians led to FPs within
the ODD due to too low IoU scores.

SYS-PER-REQ4 is met as the fraction of rolling windows with more
than a single FN is 0.24%, i.e., ≤ 3%. Figure 12 shows the distribution of
position errors in the object detection for pedestrians within 80 m of ego
car, i.e., the diﬀerence between the object detection position and ESI Pro-
SiVIC ground truth. The median error is 1.0 cm, the 99% percentile is 5.6 cm,
and the largest observed error is 12.7 cm. Thus, we show that SYS-PER-
REQ5 is satisﬁed for the internal test data, i.e., ≤ 50 cm position error for
pedestrian detection within 80 m. Note that satisfaction of SYS-PER-REQ6,
i.e., suﬃcient inference speed, is demonstrated as part of the system testing
reported in Section 11.3. The complete test report is available on GitHub.

Table 8 presents the output of the ML model on the eight slices of internal
test data deﬁned in Section 10.1. Note that we saved the children in the ESI
Pro-SiVIC object catalog for the veriﬁcation data, i.e., S9 does not exist in the
internal test data. Apart from the S6 slice with occlusion, the model accuracy
is consistent across the slices which corroborates satisfaction of the robustness
requirements on the internal test data, e.g., in relation to pose (SYS-ROB-
REQ2), size (SYS-ROB-REQ2), and appearance (SYS-ROB-REQ2).

11.2.2 ML Veriﬁcation Results [Z]

This section reports the key ﬁndings from conducting the independent ML
model testing, i.e., the Veriﬁcation Log in the AMLAS terminology. These
results provide independent evidence that the ML model satisﬁes the ML safety
requirements (see Section 6.3) on the veriﬁcation data. The total number of

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

43

Fig. 12: Distribution of position errors for the internal test data.

Table 8: ML model accuracy on eight slices of the internal test data.
S1=All data, S2=close distance, S3=far distance, S4=running pedestri-
ans, S5=walking pedestrians, S6=occluded pedestrians, S7=males, and
S8=females. Every second rows show results for the ML model followed by
OOD detection using the autoencoder.

Slice
S1

S2

S3

S4

S5

S6

S7

S8

TP
Total
139,526
134,948
+OOD 134,927
57,774
61,333
57,753
+OOD
43,547
43,547
43,547
+OOD
37,804
38,786
37,783
+OOD
97,144
99,740
97,144
+OOD
609
778
593
+OOD
67,470
69,238
67,460
+OOD
67,479
69,288
67,468
+OOD

FP FN
191
711
212
20
172
16
193
13
0
0
0
0
48
9
69
8
143
14
143
12
169
16
185
13
99
14
109
11
91
9
102
9

P
0.9948
0.9999
0.9997
0.9998
1
1
0.9998
0.9998
0.9999
0.9999
0.9744
0.9785
0.9998
0.9998
0.9999
0.9999

R
0.9986
0.9984
0.997
0.9967
1
1
0.9987
0.9982
0.9985
0.9985
0.7823
0.7618
0.9985
0.9984
0.9987
0.9985

F1
0.9967
0.9991
0.9984
0.9982
1
1
0.9992
0.9990
0.9992
0.9992
0.8679
0.8567
0.9992
0.9991
0.9993
0.9992

AP@0.5
0.9942
0.995
0.995
0.995
0.995
0.995
0.995
0.995
0.995
0.995
0.9211
0.8899
0.995
0.995
0.995
0.995

images in the veriﬁcation data is 208,884 (202,712 pedestrians (97.0%) and
6,172 non-pedestrians (3.0%)). Analogous to Section 11.2.1, Figure 13 depicts
four subﬁgures representing IoU = 0.5: P vs R (cf. 13a), F1 vs. Conf (cf. 13b),
P vs. Conf (cf. 13c), and R vs. Conf (cf. 13d). We observe that the appearance
of the four subﬁgures closely resembles the corresponding plots for the internal
test data (cf. Figure 11).

Table 9 shows the output from the ML model using the Conf threshold
0.448 on the veriﬁcation data. The table is organized into 1) all distances from

Springer Nature 2021 LATEX template

44

Ergo, SMIRK is Safe

(a) P–R curve.

(b) F1 vs. Conf.

(c) P vs. Conf.

(d) R vs. Conf.

Fig. 13: Evaluation of the ML model on the veriﬁcation data at IoU=0.5.

the ego car, 2) within 80 m, and 3) within 50 m, respectively. The table also
shows the eﬀect of adding OOD detection using the autoencoder, i.e., the num-
ber of FPs is decreased just as for the internal test data. Table 10 demonstrates
how the ML model satisﬁes the performance requirements on the veriﬁcation
data. Similar to the results for the internal test data, the FPPI (0.21%) is
too high to satisfy SYS-PER-REQ3 without additional OOD detection, i.e.,
we observed 330 FPs (roughly an equal share of pyramids and children). The
rightmost column in Table 10 shows how the FPPI decreased to 0.015% with
the autoencoder. All basic shapes were rejected, instead children at a long dis-
tance with too low IoU scores dominate the FPs. We acknowledge that it is
hard for the YOLOv5 to achieve a high IoU for the few pixels representing

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

45

Table 9: ML model accuracy on the veriﬁcation data at the Conf threshold
0.448. The three rows show results for all distances, within 80 m, and within
50 m, respectively. Every second row show results for the ML model followed
by OOD detection using the autoencoder.

Distance
All

Total
208,884

+OOD

≤ 80 m

158,066

+OOD

≤ 50 m

92,592

+OOD

TP
198,457
195,695
151,976
149,214
86,847
84,085

FP
990
533
330
23
178
21

FN
4,255
7,017
210
2,972
165
2,972

P
0.9950
0.9973
0.9978
0.9998
0.9980
0.9998

R
0.9790
0.9654
0.9986
0.9905
0.9981
0.9805

F1
0.9870
0.9811
0.9982
0.9901
0.9980
0.9901

AP@0.5
0.9942
0.9878
0.9945
0.988
0.9949
0.988

Table 10: ML model satisfaction of the performance requirements on the
veriﬁcation data at the Conf threshold 0.448. R1–R4 = SYS-PER-REQ1–4.
The rightmost column show results for the YOLOv5 model followed by OOD
detection using the autoencoder.

Req.

R1

R2

R3

R4

Expected
TP rate ≥ 93%
for ≤ 80 m
FN rate ≤ 7%
for ≤ 50 m
FPPI ≤ 0.1%
for ≤ 80 m
≤ 3% of rolling windows
contain ≥ 2 misses in
5 frames for ≤ 80 m

Observed (Model) Observed (Model+OOD)

151,976
158,066 = 96.1%

165
92,592 = 0.18%

330

158,066 = 0.21%

149,214
158,066 = 94.4%

2,927
92,592 = 3.2%

23

158,066 = 0.015%

201

152,395 = 0.13%

3,499
152,090 = 2.3%

a child almost 80 m away. However, commencing emergency braking in such
cases is an appropriate action – a child detected with a low IoU is not an exam-
ple of the ghost braking hazard described in Section 6.2. SYS-PER-REQ4
is satisﬁed as the fraction of rolling windows with more than a single FN is
2.3%. Figure 14 shows the distribution of position errors. The median error is
1.0 cm, the 99% percentile is 5.4 cm, and the largest observed error is 12.8 cm.
Consequently, we show that SYS-PER-REQ5 is satisﬁed for the veriﬁcation
data.

Table 12 presents the output of the ML model on the nine slices of the veriﬁ-
cation data deﬁned in Section 10.1. In relation to the robustness requirements,
we notice that there the accuracy is slightly lower for S9 (children). This ﬁnd-
ing is related to the size requirement SYS-ROB-REQ3. Table 11 contains
an in-depth analysis of children at diﬀerent distances with OOD detection.
We conﬁrm that most FPs occur outside of the ODD, i.e., 507 out of 512 FPs
occur for children more than 80 m from ego car. In extension, we show that
the performance requirements are still satisﬁed for the most troublesome slice
of data as follows:
• TP rate children ≤ 80m: 50,402

50,696 = 99.4%

Springer Nature 2021 LATEX template

46

Ergo, SMIRK is Safe

Fig. 14: Distribution of position errors for the veriﬁcation data.

Table 11: Detailed analysis for children. The rows show results results for the
ML model followed by OOD detection using the autoencoder for four distance
ranges. The bottom row in italic font is outside the ODD.

Distance
All
≤ 80 m
≤ 50 m
> 80 m

Total
69,301
50,696
30,731
16,838

TP
63,360
50,402
28,715
12,803

FP
512
5
3
507

FN
4,174
294
249
4,035

P
0.992
0.9999
0.9999
0.9619

R
0.9382
0.9942
0.9914
0.7604

F1
0.9643
0.9971
0.9956
0.8493

AP@0.5
0.9877
0.995
0.995
0.942

• FN rate children ≤ 50m:
• FPPI children ≤ 80m:

249
30,731 = 0.81%
5
52,463 = 0.0099%

The independent veriﬁcation concludes that all requirements are met, based
on the same argumentation as for the internal test results. The complete
veriﬁcation report is available on GitHub.

11.3 Results from System Testing [FF]

This section presents an overview of the results from testing SMIRK in ESI
Pro-SiVIC, which corresponds to the Integration Testing Results in AMLAS.
As explained in Section 10.2.2, we measure seven metrics for each test case
execution, i.e., MinDist, TimeTrig, DistTrig, TimeBrake, DistBrake, Coll, and
CollSpeed.

Table 13 presents the results from executing the test cases representing
operational scenarios with pedestrians, i.e., TC-OS-[1–25]. From the left, the
columns show 1) test case ID, 2) the minimum distance between ego car and
the pedestrian during the scenario, 3) the diﬀerence between TimeTrig and
TimeBrake, 4) the diﬀerence between DistTrig and DistBrake, 5) whether there
was a collision, 6) the speed of ego car at the collision, and 7) the initial speed

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

47

S2

S3

Slice
S1

Table 12: ML model accuracy on nine slices of the veriﬁcation data.
S1=All data, S2=close distance, S3=far distance, S4=running pedestrians,
S5=walking pedestrians, S6=occluded pedestrians, S7=males, S8=females,
and S9=children. Every second rows show results for the ML model followed
by OOD detection using the autoencoder.
TP
Total
208,884
198,457
+OOD 195,695
86,691
92,028
83,929
+OOD
65,285
65,330
65,285
+OOD
56,130
58,267
54,964
+OOD
149,617
142,328
+OOD 140,732
1,031
+OOD
69,292
+OOD
69,291
+OOD
69,301
+OOD

AP@0.5
0.9942
0.9878
0.995
0.9761
0.995
0.995
0.9949
0.9818
0.9949
0.9882
0.9289
0.8783
0.995
0.9741
0.995
0.995
0.9879
0.9871

P
0.995
0.9998
0.9997
0.9997
1
1
0.998
0.998
0.997
0.997
0.9752
0.9746
0.9998
0.9998
0.9999
0.9999
0.992
0.992

R
0.979
0.9616
0.9981
0.9663
0.9993
0.9993
0.9874
0.9669
0.9757
0.9648
0.84
0.7808
0.9992
0.9616
0.9989
0.9987
0.9389
0.9359

F1
0.987
0.9803
0.9989
0.9827
0.9996
0.9996
0.9927
0.9822
0.9863
0.9806
0.9026
0.867
0.9995
0.9803
0.9994
0.9993
0.9647
0.9631

FN
4,255
7,017
165
2,927
45
45
716
1,882
3,538
5,134
165
226
54
2,600
74
87
4,126
4,329

FP
990
533
22
21
2
2
110
110
424
423
22
21
15
14
7
7
512
512

866
805
67,555
65,009
67,495
67,482
63,408
63,205

S4

S8

S7

S5

S6

S9

of ego car. We note that 2) and 3) are 0 for all 25 test cases, showing that the
pedestrian is always detected at the ﬁrst possible frame when TTC ≤ 4s, which
means that SMIRK commenced emergency braking in all cases. Moreover, we
see that SMIRK successfully avoids collisions in all but two test cases. In TC-
OS-5, the pedestrian starts 20 m from ego car and runs towards it while it
drives at 16 m/s – SMIRK brakes but barely reduces the speed. In TC-OS-9,
the pedestrian starts only 15 m from ego car but SMIRK signiﬁcantly reduces
the speed by emergency braking.

The remaining system test cases corresponding to non-pedestrian opera-
tional scenarios (TC-OS-[26–38]) and all test cases with jitter (TC-RAND-[1–
38]) were also executed with successful test verdicts. All scenarios with basic
shapes on collision course were rejected by the safety cage architecture, i.e.,
SMIRK did never commence any ghost braking. In a virtual conclusion of test
meeting, the ﬁrst three authors concluded that TC-RBT-1 and TC-RBT-2
had passed successfully. Finally, Figure 15 shows the distribution of infer-
ence speeds during the system testing. The median inference time is 22.0 ms
and the longest inference time observed is 51.6 ms. Based on these results we
conclude that TC-RBT-3 passed successfully and thus provide evidence that
SYS-PER-REQ6 is satisﬁed. The complete system test report is available
on GitHub.

Springer Nature 2021 LATEX template

48

Ergo, SMIRK is Safe

Fig. 15: Distribution of inference speeds during system testing.

Table 13: Test results and metrics collected during execution of TC-OS-[1-25].

ID

MinDist(m) ∆ Time(s) ∆ Distance(m) Collision Collision Speed(m/s)

Initial Speed(m/s)

TC-OS-1
TC-OS-2
TC-OS-3
TC-OS-4
TC-OS-5
TC-OS-6
TC-OS-7
TC-OS-8
TC-OS-9
TC-OS-10
TC-OS-11
TC-OS-12
TC-OS-13
TC-OS-14
TC-OS-15
TC-OS-16
TC-OS-17
TC-OS-18
TC-OS-19
TC-OS-20
TC-OS-21
TC-OS-22
TC-OS-23
TC-OS-24
TC-OS-25

18
21
12
3
0
18
34
28
0
18
33
27
16
19
1
18
32
17
25
29
28
4
16
30
36

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

False
0.0
False
0.0
False
0.0
0.0
False
0.0 True
False
0.0
False
0.0
0.0
False
0.0 True
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0
False
0.0

-
-
-
-
15.99
-
-
-
3.11
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

3.47
5.78
14.00
18.90
16.00
3.73
5.80
10.00
11.00
3.14
5.66
10.22
18.60
3.68
15.00
3.46
9.00
5.52
4.71
10.30
19.00
6.00
3.46
4.81
12.89

11.4 Erroneous Behaviour Log [DD]

As prescribed by AMLAS, the characteristics of erroneous outputs shall be
predicted and documented. This section presents the key observations from
internal testing of the ML model, independent veriﬁcation activities, and sys-
tem testing in ESI Pro-SiVIC. The ﬁndings can be used to design appropriate
responses by other vehicular systems in the SMIRK context.

Tables 8 and 12 show that the AP@0.5 are lower for occluded pedestrians
(S6). As occlusion is an acknowledged challenge for object detection, which we
previously have studied for automotive pedestrian detection (Henriksson et al,
2021b), this is an expected result. Table 12 also reveals that the number of

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

49

FPs and FNs for the S9 slice (children) is relatively high, resulting in slightly
lower AP@0.5. Table 11 shows that the problem with children is primarily far
away, explained by the few pixels available for the object detection at long
distances. While the SMIRK fulﬁls the robustness requirements within the
ODD, we recognize this perception issue in the erroneous behavior log.

During the iterative SMIRK development (cf. E) in Figure 3), it became
evident that OOD detection using the autoencoder was inadequate at close
range. Figure 16 shows reconstruction errors (on the y-axis) for all objects
in the validation subset of the development data at A) all distances, B) >
10 m, C) > 20 m, and D) > 30 m. The visualization clearly shows that the
autoencoder cannot convincingly distinguish the cylinders from the pedestrians
at all distances (in subplot A), diﬀerent objects appear above the threshold,
but the OOD detection is more accurate when objects at close distance are
excluded (subplot D) displays high accuracy). Based on validation of the four
distances, comparing the consequences of the trade-oﬀ between safety cage
availability and accuracy, the design decision for SMIRK’s autoencoder is to
only perform OOD detection for objects that are at least 10 m away. We
explain the less accurate behaviour at close range by limited training data,
a vast majority of images contain pedestrians at a larger distance – which is
reasonable since the SMIRK ODD is limited to rural country roads.

12 Lessons Learned and Practical Advice

This section shares the most valuable lessons learned during our project. We
organize the section into the perspectives of AI engineering and industry-
academia collaboration.

12.1 AI Engineering in the Safety Context

SOTIF and AMLAS are compatible and complementary. Our experience in
this R&D project is that the processes are feasible to combine during sys-
tems development and safety engineering. We expected this based on reading
process documentation before embarking on this project, and our hands-on
experience conﬁrmed the compatibility in our case under study. As presented
in Figures 1 and 2, both processes are iterative which is key to AI engineer-
ing. In our project, we ﬁrst used SOTIF (including a formal HARA) to iterate
toward an initial SRS. From this point, subsequent development iterations that
adhered to both the cycles of SOTIF and AMLAS followed. Moreover, SOTIF
and AMLAS complement each other as SOTIF maintains a systems perspec-
tive (sensors, ML algorithms, and actuators, cf. Section 5.2) whereas AMLAS
provides an artifact-oriented focus on safety evidence for activities related to
ML. Finally, as stressed by industry partners, we found that we could apply
both SOTIF and AMLAS after initial prototyping (cf. Figure 3) and still har-
ness insights from previous ad hoc work. While safety rarely can be added on
top of a complex system, we found that prototyping accelerated subsequent
safety engineering for SMIRK.

Springer Nature 2021 LATEX template

50

Ergo, SMIRK is Safe

Fig. 16: Reconstruction errors for diﬀerent objects on the validation subset
of the development data at diﬀerent distances from ego car (purple=cylinder,
red=female business casual, blue=male business, green=male casual). The
dashed lines show the threshold for rejecting objects. In SMIRK, we use
alternative B) in the safety cage.

Using a simulator to create data sets limits the validity of the negative
examples. On one hand, our data generation scripts enable substantial free-
dom and cheap access to data. On the other hand, there is barely any variation
in the scenarios (apart from clouds moving on the skydome) as would be the
case for naturalistic data. As anything that is not a pedestrian in our data is
a de facto negative example (see rationale for DAT-BAL-REQ3), and noth-
ing ever appears in our simulated scenarios unless we add it in our scripts,
the diversity of our negative examples is very limited. Our approach to neg-
ative examples in the development data, referred to as “background images”

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

51

in Section 8.2.3, involved including the outlier example Cylinder [N5]. From
experiments on the validation subset of the development data, we found that
adding frames with cylinders representing negative examples was essential to
let the model distinguish between pedestrians and basic shapes. For ML com-
ponents designed for use in the real world, trained on outcomes from real data
collection campaigns, the natural variation of the negative examples would be
completely diﬀerent. When working with synthetic data from simulators, how
to specify data requirements on negative examples remains an open question.
Evaluation of object detection models is non-trivial. We spent substantial
time to align the understanding within the project and we believe other devel-
opment endeavors will need to do the same. In particular, we observed that the
deﬁnition of TP, FP, TN, and FN based on IoU (explained in Section 10.1) is
diﬃcult to grasp for novices. The fact that FPs appear due to low IoU scores
despite parts of a pedestrian indeed is detected is often counter-intuitive, i.e.,
“how can a detected pedestrian ever be a FP?” To align the development team,
organizations should ensure that the true meaning of those KPIs are commu-
nicated as part of internal training. In the same vein, FP rate is not a valid
metric (as TPs do not exist) whereas FN rate is used in SYS-PER-REQ2
– again internal training is important to align the understanding. What intu-
itively is perceived as a FP on the system level is not the same as a FP on the
ML model level. To make the distinction clear, we restrict the use of FPs to
the model level and refer to incorrect braking on the system level as “ghost
braking.”

ML model selection post learning involves fundamental decisions. Model
selection is an essential activity in ML. When training ML models over several
epochs, the best performing model given some criterion shall be kept. Also,
when training alternative models with alternative architectures or hyperpa-
rameter settings, there must be a way to select the best candidate. How to
tailor a ﬁtness function to quantitatively measure what “best” involves is a
delicate engineering eﬀort with inevitable tradeoﬀs. The default ﬁtness func-
tion in YOLOv5 puts 10% of the weight at AP@0.5 and 90% at Mean AP for
a range of ten IoU values between 0.5 to 0.95. It would be possible to further
customize the ﬁtness function to also cover fairness aspects, i.e., to already
during model selection value models that fulﬁll various quality aspects. There
is no upper limit to the possible complexity, as this could encompass gender,
size, ODD aspects etc. For SMIRK, however, we decided to do the opposite,
i.e., to prioritize simplicity to gain interpretability by using a simpler metric.
As explained in Section 10.1, our ﬁtness function solely uses AP@0.5. Future
work could also explore sets of complementary ﬁtness functions and evaluate
approaches for multi-criteria optimization for ML model selection (Koch et al,
2015; Ali et al, 2017).

OOD scores can be measured for diﬀerent parts of an image. What pixels to
send to the autoencoder is another important design decision. Initially, we used
the entire image as input to the autoencoder, which showed promising results
in detecting major changes in the environmental conditions, e.g., leaving the

Springer Nature 2021 LATEX template

52

Ergo, SMIRK is Safe

ODD due to nightfall or heavy fog. However, it quickly became evident that
this input generated too small diﬀerences in the autoencoder’s reconstruction
error between inliers and outliers, i.e., it was not a feasible approach to reject
basic shapes. We ﬁnd this to be in line with how the “curse of dimensionality”
aﬀects unsupervised anomaly detection in general (Zimek et al, 2012) – the
anomalies we try to ﬁnd are dwarfed by the background information. Instead,
we decided to focus on squares (a good shape for the autoencoder) containing
pixels close to the bounding box of the detected object, and tried three solu-
tions: 1) extracting a square centered on the middle pixel, 2) extracting the
entire bounding box and padding with gray pixels to make it a square, and
3) stretching the contents of the bounding box to ﬁt a rectangle matching the
average aspect ratio of pedestrians in the development set. The third approach
was the most successful in our study, and is now used in SMIRK. Future OOD
architectures will likely combine diﬀerent selection of the input images.

The ﬁdelity of the radar signatures in the simulator matters. While it is
easy for a human to tell how realistic the visual appearance of objects are
in ESI Pro-SiVIC, assessing the appropriateness of its radar signature model
requires a sensor expert. In SMIRK, we attached the same radar signature to
all pedestrians, i.e., the one provided for human bodies in the object catalog.
For all basic shapes, on the other hand, we attach the same simplistic spherical
radar signature. Designing customized signatures is beyond the scope of our
project, thus we acknowledge this limitation as a threat to validity. It is possible
that system testing results would have been diﬀerent if more elaborate radar
signatures were used.

12.2 Reﬂections on Industry-Academia Collaboration

Engineering research is ideally done in projects that involve partners from both
industry and academia. While it might sound easy to accomplish, the software
and systems engineering research community acknowledges that it is hard.
Numerous papers address challenges and best practices in industry-academia
collaboration. A systematic review by Garousi et al (2016) identiﬁed 10 chal-
lenge themes (e.g., diﬀerent time horizons, contrasting reward systems, limited
practical relevance, and limited resources) and 17 best practice themes (e.g.,
select real-world problems, work in an agile fashion, organize regular meet-
ings, and identify industry champions). In this section, we share three main
reﬂections related to industry-academia collaboration around safety-critical
ML-based demonstrator development that complement previously reported
perspectives.

Collaborating in a safety-critical context is sensitive. The research relation
between industry and academia in Sweden is recognized in the (empirical)
software engineering community as particularly good. The relations have
developed over decades, and we have successfully conducted several research
projects guided by the best practices from the literature. However, develop-
ing a publicly available demonstrator with an accompanying safety case was
a new experience. We found that industry partners are highly reluctant to

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

53

put their names anywhere in anyway that could suggest any form of liability.
Legal discussions can completely stall research projects. For SMIRK, the only
reasonable way forward was to 1) largely remove the traceability of individual
partner’s contributions, and 2) add explicit disclaimers that SMIRK is only
intended for simulators and that all users assume all responsibility and risk
etc. We accept that this compromise threatens the validity of our work.

Preparing for long-term maintenance of an OSS demonstrator is diﬃcult.
Initiating development of a demonstrator system in a public repository under
an OSS license is no problem. However, preparing for long-term maintenance
of the system is a diﬀerent story. Research funding is typically project-based
and when projects conclude, it might be hard to motivate maintenance eﬀorts
– even if there are active users. Long-term support of OSS tends to depend
on individual champions, but, this is rarely sustainable. We acknowledge the
challenge and support SMIRK’s longevity by 1) hosting the source code in a
GitHub repository managed by the independent non-proﬁt institute RISE, 2)
publishing careful contribution guidelines including a branching model, and
3) explicitly stating the responsible unit in the RISE line organization. Even
more importantly, before we initiated the development, we aligned the goals of
SMIRK with RISE’s long-term research roadmap and project portfolio. Time
will tell whether our eﬀorts are suﬃcient to ensure long-term maintenance.

Finding long-term hosting of large data sets is hard. Fully open ML-based
projects must go beyond sharing its source code by also making corresponding
data sets available. For us, this turned out to be more diﬃcult than expected.
First, GitHub is not a feasible choice for data hosting as they recommend
repositories to remain small, i.e., less than 5 GB is strongly recommended.
Also, GitHub does not support sophisticated data versioning but must be com-
bined with third party solutions such as DVC12. Second, no project partners
volunteered to assure long-term hosting of the 185 GB SMIRK data set. Data
hosting requires appropriate solutions to accommodate access control, backup,
bandwidth etc. Moreover, even with appropriate solutions in place, storing
data is not free – and when the research project is over, someone must keep
paying. Our solution, which involved negotiations with long lead-times, was to
reach out to a national non-proﬁt AI ecosystem. Luckily, AI Sweden agreed to
host the SMIRK data set as part of their Data Factory initiative.

13 Limitations and Threats to Validity

This section discusses the primary limitations of our work and the most impor-
tant threats to validity. Our engineering research is guided by a research
question that necessitates a critical reﬂection of qualitative nature. Did our
eﬀorts result in an open and useful ML safety case? We structure the discussion
according to Maxwell (1992), excluding threats related to theory building.

Descriptive validity refers to the accuracy and objectivity of the infor-
mation gathered. Related threats have been mitigated in the SMILE III project

12https://dvc.org/

Springer Nature 2021 LATEX template

54

Ergo, SMIRK is Safe

through prolonged involvement, i.e., the long-term relations that evolved dur-
ing the study. The ﬁrst SMILE project started already in 2016 and several
partners have worked jointly on safety cage architectures since 2018. What
we report does not represent individual interviews but joint work and regu-
lar project meetings. Moreover, the description of our safety evidence can be
traced on GitHub at the level of commits. Finally, as descriptive validity also
involves issues of omission, we carefully report a complete ML safety case in
Appendix A.

Interpretive validity addresses the researchers’ ability to interpret the
perspectives of the study participants. There is a risk that the perspectives
of the authors inﬂuenced the reported meaning of the results. For case study
research in software engineering, Runeson et al (2012) refer to this as relia-
bility. Our aim was to show a complete example of an ML safety case, and
we conducted engineering research to provide evidence and arguments for the
community to build on. To support the credibility of our work, the SMILE III
project followed existing processes. Section 2 shares our preunderstanding of
SOTIF and AMLAS, which allows others to assess our process interpretations.
We mitigate the obvious researcher bias by carefully presenting our work
in the six AMLAS phases and by opening up all details on GitHub for scrutiny.
With several co-authors, and even more project participants working in the
SMILE projects, we mitigated threats related to individual interpretations.
Member checking, i.e., letting participants of the study validate our ﬁnal
outcome, also supported reliable interpretations.

Evaluation validity refers to the researchers’ ability to look critically
at the results and the research itself as a way of learning and expand-
ing understanding. Maxwell (1992) argues that it is often less critical as
“many researchers make no claims to evaluate what they study.” We, on the
other hand, present a safety case that we evaluated thoroughly. Our primary
approach to mitigate threats to evaluation validity is through transparency, i.e.,
we carefully describe our design decisions and provide complete access to the
details. Thanks to the complete safety case argumentation in Appendix A, we
stress that the traceability from arguments to the design and implementation
supports external assessments.

Generalizabiliy is the condition of extending the ﬁndings to other con-
texts. Runeson et al (2012) call it external validity in case study research, and
additionally consider to what extent the results are of interest to people outside
the investigated case. The rich description of the systems and safety engineer-
ing that lead to the open SMIRK ADAS (Socha et al, 2022) supports analytical
generalization to other contexts. Furthermore, just like AMLAS is applicable
to safety argumentation in diﬀerent domains, we believe that the safety case
provided in Appendix A can inspire similar projects beyond the automotive
sector – especially if computer vision is involved. Still, we acknowledge three
limitations that threaten the generalizability.

First, we developed SMIRK for the simulator ESI Pro-SiVIC. We have not
systematically analyzed how diﬀerent aspects of the safety case generalize to

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

55

an ML-based ADAS intended for deployment in a real-world vehicle. However,
we mitigated this threat through prolonged industry involvement. All SMILE
projects have received an equal share of public and industry funding, which
assured that the R&D activities entailed were considered practically relevant
to industry. We believe that the SMIRK requirements would generalize to real-
world systems, whereas the two AMLAS stages data management and model
deployment would change the most. Future work should investigate this in
detail. However, we argue that our work with synthetic images is relevant for
real-world systems due to the growing interest in combining synthetic and
natural images (Poucin et al, 2021).

Second, SMIRK is a single independent ADAS designed for a minimal-
isic ODD. The main strategy to mitigate threats related to the minimalism
was again prolonged involvement, i.e., our industry partners found our safety
case relevant, thus also others in the community are likely to share this view.
It is less clear how the independence of the SMIRK ADAS inﬂuences the
generalizability of our contributions as a modern car can be considered a
system-of-systems (Pelliccione et al, 2020). We leave it to future work to assess
how the safety case would change if additional ADAS were involved in the
same safety argumentation.

Third, Python is dynamically typed and not an ideal choice for develop-
ment of safety-critical applications. We chose Python to get easy access to
numerous state-of-the-art ML libraries. Also, as it is the dominating language
in the research community others can more easily build on our work. A real-
world in-vehicle implementation would lead to another language choice, e.g.,
adhering to MISRA C (Motor Industry Software Reliability Association et al,
2012), a widely accepted set of software development guidelines for using the
C programming language in safety-critical systems.

14 Conclusion and Future Work

Safe ML is going to be fundamental when increasing the level of vehicle
automation. Several automotive standardization initiatives are ongoing to
allow safety certiﬁcation for ML in road vehicles, e.g., ISO 21448 SOTIF. How-
ever, standards provide high-level requirements that must be operationalized
in each development context. Unfortunately, there is a lack of publicly avail-
able ML-based automotive demonstrator systems that can be used to study
safety case development. We set out to remedy this lack through engineering
research in an industry-academia collaboration in Sweden.

We present safety argumentation for SMIRK, a PAEB designed for oper-
ation in the industry-grade simulator ESI Pro-SiVIC (Socha et al, 2022),
available on GitHub under an OSS license (RISE Research Institutes of
Sweden, 2022). SMIRK uses a radar sensor for object detection and an ML-
based component relying on a DNN for pedestrian recognition. Originating in
SMIRK’s minimalistic ODD, we present a complete safety case for its ML-
based component by following the AMLAS framework (Hawkins et al, 2021).

Springer Nature 2021 LATEX template

56

Ergo, SMIRK is Safe

To the best of our knowledge, this work constitutes the ﬁrst complete applica-
tion of AMLAS independent from its authors. Guided by AMLAS, we argue
that we demonstrate how to provide suﬃcient evidence that ML in SMIRK is
safe given its ODD. We conclude that even for a very restricted ODD, the size
of the ML safety case is considerable, i.e., there are many aspects of the AI
engineering that must be clearly explained.

We report several

lessons learned related to AI engineering in the
SOTIF/AMLAS context. First, using a simulator to create synthetic data
sets for ML training particularly limits the validity of the negative examples.
Second, the complexity of object detection evaluations necessitates internal
training within the project team. Third, composing the ﬁtness function used
for model selection is a delicate engineering activity that forces explicit trade-
oﬀ decisions. Fourth, what parts of an image to send to a autoencoder for
OOD detection is an open question – for SMIRK, we stretch the content
of bounding boxes to a larger square. Finally, we report three reﬂections
related to industry-academia collaboration around safety-critical ML-based
demonstrator development.

Thanks to the complete safety case, SMIRK can be used as a starting point
for several avenues of future research. First, the SMIRK MVP enables stud-
ies on eﬃcient and eﬀective approaches to conduct safety assurance for ODD
extension (Weissensteiner et al, 2021). In this context, SMIRK could be used
as a platform to study dynamic safety cases (Denney et al, 2015), i.e., updat-
ing the safety case as the system evolves, and reuse of safety evidence for new
operational contexts (de la Vara et al, 2019). Second, SMIRK could be used as
a realistic test benchmark for automotive ML testing, including search-based
techniques for test case generation. The testing community has largely worked
on oﬄine testing of single frames, but we know that this is insuﬃcient (Haq
et al, 2021b). Also, we recommend comparative studies involving real-world
testing in controlled environments, as discrepancies do exist between simula-
tions and the physical world (Stocco et al, 2022). Third, we recommend the
community to port SMIRK to other simulators beyond ESI Pro-SiVIC. As we
investigated in previous work, running highly similar test scenarios in diﬀer-
ent simulators can lead to considerably diﬀerent results (Borg et al, 2021a) –
further exploring this phenomenon using SMIRK would be a valuable research
direction. Finally, while SOTIF explicitly excludes antagonistic attacks, there
are good reasons to consider both safety and cybersecurity jointly in auto-
motive systems through co-engineering (Amorim et al, 2017). We plan to use
SMIRK as a starting point in future studies on adversarial ML attacks.

Acknowledgment

Thanks go to ESI Group for supporting us with technical details along the
way, especially Erik Abenius and Fran¸cois-Xavier Jegeden. We also thank
AI Sweden for agreeing to host the SMIRK data set as part of their Data
Factory initiative. This work was carried out within the SMILE III project

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

57

ﬁnanced by Vinnova, FFI, Fordonsstrategisk forskning och innovation under
the grant number 2019-05871 and partially supported by the Wallenberg AI,
Autonomous Systems and Software Program (WASP) funded by Knut and
Alice Wallenberg Foundation.

Conﬂict of Interest Statement

The authors have no conﬂicts of interest to declare.

References

Ali R, Lee S, Chung TC (2017) Accurate multi-criteria decision making
methodology for recommending machine learning algorithm. Expert Systems
with Applications 71:257–278

Amorim T, Martin H, Ma Z, et al (2017) Systematic pattern approach for
safety and security co-engineering in the automotive domain. In: Proc. of
the Int’l. Conf. on Computer Safety, Reliability, and Security, pp 329–342

An J, Cho S (2015) Variational autoencoder based anomaly detection using

reconstruction probability. Special Lecture on IE 2(1):1–18

Arrieta AB, D´ıaz-Rodr´ıguez N, Del Ser J, et al (2020) Explainable artiﬁ-
cial intelligence (XAI): Concepts, taxonomies, opportunities and challenges
toward responsible AI. Information Fusion 58:82–115

Ashmore R, Calinescu R, Paterson C (2021) Assuring the machine learning
lifecycle: Desiderata, methods, and challenges. ACM Computing Surveys
54(5):1–39

Assurance Case Working Group (2021) Goal Structuring Notation Community
Standard (Version 3). Tech. Rep. SCSC-141C, Safety-Critical Systems Club,
UK

Barr ET, Harman M, McMinn P, et al (2014) The oracle problem in software
testing: A survey. IEEE Transactions on Software Engineering 41(5):507–525

Ben Abdessalem R, Nejati S, Briand LC, et al (2016) Testing advanced driver
assistance systems using multi-objective search and neural networks. In:
Proc. of the 31st Int’l. Conf. on Automated Software Engineering, pp 63–74

Ben Abdessalem R, Nejati S, Briand LC, et al (2018a) Testing vision-based
control systems using learnable evolutionary algorithms. In: Proc. of the
40th Int’l. Conf. on Software Engineering, pp 1016–1026

Ben Abdessalem R, Panichella A, Nejati S, et al (2018b) Testing autonomous
cars for feature interaction failures using many-objective search. In: Proc.

Springer Nature 2021 LATEX template

58

Ergo, SMIRK is Safe

of the 33rd Int’l. Conf. on Automated Software Engineering, pp 143–154

Bolya D, Foley S, Hays J, et al (2020) Tide: A general toolbox for identifying
object detection errors. In: Proc. of the European Conf. on Computer Vision,
pp 558–573

Borg M, Englund C, Wnuk K, et al (2019) Safely entering the deep: A review
of veriﬁcation and validation for machine learning and a challenge elicitation
in the automotive industry. Journal of Automotive Software Engineering
1(1):1–19

Borg M, Ben Abdessalem R, Nejati S, et al (2021a) Digital twins are not
monozygotic: Cross-replicating ADAS testing in two industry-grade automo-
tive simulators. In: Proc. of the 14th Conf. on Software Testing, Veriﬁcation
and Validation, pp 383–393

Borg M, Bronson J, Christensson L, et al (2021b) Exploring the assessment list
for trustworthy AI in the context of advanced driver-assistance systems. In:
Prov. of the 2nd Int’l. Workshop on Ethics in Software Engineering Research
and Practice, pp 5–12

Bosch J, Olsson HH, Crnkovic I (2021) Engineering AI systems: A research
agenda. In: Artiﬁcial Intelligence Paradigms for Smart Cyber-Physical
Systems. IGI global, p 1–19

Chen L, Babar MA, Nuseibeh B (2012) Characterizing architecturally signiﬁ-

cant requirements. IEEE Software 30(2):38–45

Denney E, Pai G, Habli I (2015) Dynamic safety cases for through-life safety
assurance. In: Proc. of the 37th Int’l. Conf. on Software Engineering, pp
587–590

Dollar P, Wojek C, Schiele B, et al (2011) Pedestrian detection: An evaluation
of the state of the art. IEEE Transactions on Pattern Analysis and Machine
Intelligence 34(4):743–761

Ebadi H, Moghadam MH, Borg M, et al (2021) Eﬃcient and eﬀective gener-
ation of test cases for pedestrian detection-search-based software testing of
Baidu Apollo in SVL. In: Proc. of the Int’l. Conf. on Artiﬁcial Intelligence
Testing, pp 103–110

Fagan M (1976) Design and code inspections to reduce errors in program

development. IBM Systems Journal 15(3):182–211

Garousi V, Petersen K, Ozkan B (2016) Challenges and best practices in
industry-academia collaborations in software engineering: A systematic
literature review. Information and Software Technology 79:106–127

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

59

Gauerhof L, Hawkins R, Picardi C, et al (2020) Assuring the safety of machine
learning for pedestrian detection at crossings. In: Proc. of the Int’l. Conf.
on Computer Safety, Reliability, and Security, pp 197–212

Haq FU, Shin D, Briand LC, et al (2021a) Automatic test suite generation for
key-points detection dnns using many-objective search (experience paper).
Proc. of the 30th Int’l. Symposium on Software Testing and Analysis, pp
91–102

Haq FU, Shin D, Nejati S, et al (2021b) Can oﬄine testing of deep neu-
ral networks replace their online testing? Empirical Software Engineering
26(5):1–30

Hauer F, Schmidt T, Holzm¨uller B, et al (2019) Did we test all scenarios
for automated and autonomous driving systems? In: Proc. of the IEEE
Intelligent Transportation Systems Conf., pp 2950–2955

Hawkins R, Paterson C, Picardi C, et al (2021) Guidance on the assurance of
machine learning in autonomous systems (amlas). Tech. Rep. Version 1.1,
Assuring Autonomy Int’l. Programme, University of York

Henriksson J, Berger C, Borg M, et al (2019) Towards structured evaluation
of deep neural network supervisors. In: Proc. of the Int’l. Conf. on Artiﬁcial
Intelligence Testing, pp 27–34

Henriksson J, Berger C, Borg M, et al (2021a) Performance analysis of out-of-
distribution detection on trained neural networks. Information and Software
Technology 130:106,409

Henriksson J, Berger C, Ursing S (2021b) Understanding the impact of edge
cases from occluded pedestrians for ML systems. In: Proc. of the 47th
Euromicro Conf. on Software Engineering and Advanced Applications, pp
316–325

High-Level Expert Group on Artiﬁcial Intelligence (2019) Ethics guidelines
for trustworthy AI. Tech. rep., Directorate-General for Communications
Networks, Content and Technology, European Commission

Horkoﬀ J (2019) Non-functional requirements for machine learning: Chal-
lenges and new directions. In: Proc. of the IEEE 27th Int’l. Requirements
Engineering Conf., pp 386–391

IEEE (1998) IEEE recommended practice for software requirements speciﬁ-
cations. Tech. Rep. IEEE 830-1998, Institute of Electrical and Electronics
Engineers

Springer Nature 2021 LATEX template

60

Ergo, SMIRK is Safe

Jia Y, Mcdermid JA, Lawton T, et al (2022) The role of explainability in
assuring safety of machine learning in healthcare. IEEE Transactions on
Emerging Topics in Computing

K¨apyaho M, Kauppinen M (2015) Agile requirements engineering with proto-
typing: A case study. In: Proc. of the 23rd Int’l. requirements engineering
Conf., pp 334–343

Klaise J, Van Looveren A, Cox C, et al (2020) Monitoring and explainability
of models in production. In: Proc. of the ICML Workshop on Challenges in
Deploying and Monitoring Machine Learning Systems

Koch P, Wagner T, Emmerich MT, et al (2015) Eﬃcient multi-criteria opti-
mization on noisy machine learning problems. Applied Soft Computing
29:357–370

Kruchten PB (1995) The 4+1 view model of architecture. IEEE Software

12(6):42–50

Lin TY, Maire M, Belongie S, et al (2014) Microsoft COCO: Common objects

in context. In: European Conf. on Computer Vision, pp 740–755

Liu S, Qi L, Qin H, et al (2018) Path aggregation network for instance seg-
mentation. In: Proc. of the IEEE Conf. on Computer Vision and Pattern
Recognition, pp 8759–8768

Masuda S (2017) Software testing design techniques used in automated vehicle
simulations. In: Proc. of the Int’l. Conf. on Software Testing, Veriﬁcation
and Validation Workshops, pp 300–303

Maxwell J (1992) Understanding and validity in qualitative research. Harvard

educational review 62(3):279–301

Mohseni S, Pitale M, Singh V, et al (2020) Practical solutions for machine
learning safety in autonomous vehicles. In: Proc. of the Artiﬁcial Intelli-
gence Safety (SafeAI) Workshop at AAAI 2020, URL http://ceur-ws.org/
Vol-2560/

Motor Industry Software Reliability Association, et al (2012) MISRA-C

guidelines for the use of the C language in critical systems

Panichella A, Kifetew FM, Tonella P (2015) Reformulating branch coverage
as a many-objective optimization problem. In: Proc. of the 8th Int’l. Conf.
on Software Testing, Veriﬁcation and Validation, pp 1–10

Pei K, Cao Y, Yang J, et al (2017) Deepxplore: Automated whitebox testing
of deep learning systems. In: Proc. of the 26th Symposium on Operating

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

61

Systems Principles, pp 1–18

Pelliccione P, Knauss E, ˚Agren SM, et al (2020) Beyond connected cars:
A systems of systems perspective. Science of Computer Programming
191:102,414

Petersson H, Thelin T, Runeson P, et al (2004) Capture-recapture in soft-
ware inspections after 10 years research: Theory, evaluation and application.
Journal of Systems and Software 72(2):249–264

Picardi C, Paterson C, Hawkins RD, et al (2020) Assurance argument patterns
and processes for machine learning in safety-related systems. In: Proc. of
the Workshop on Artiﬁcial Intelligence Safety, pp 23–30

Poucin F, Kraus A, Simon M (2021) Boosting instance segmentation with
synthetic data: A study to overcome the limits of real world data sets. In:
Proc. of the IEEE/CVF Int’l. Conf. on Computer Vision, pp 945–953

Preschern C, Kajtazovic N, Kreiner C (2015) Building a safety architecture
pattern system. In: Proc. of the 18th European Conf. on Pattern Languages
of Program, pp 1–55

Rajput M (2020) YOLO V5 – Explained and Demystiﬁed. https:

//towardsai.net/p/computer-vision/yolo-v5%E2%80%8A-%E2%80%
8Aexplained-and-demystiﬁed

Ralph P, bin Ali N, Baltes S, et al (2020) Empirical standards for software

engineering research. arXiv preprint arXiv:201003525

Redmon J, Divvala S, Girshick R, et al (2016) You only look once: Uniﬁed,
real-time object detection. In: Proc. of the IEEE Conf. on Computer Vision
and Pattern Recognition, pp 779–788

Riccio V, Jahangirova G, Stocco A, et al (2020) Testing machine learning based
systems: A systematic mapping. Empirical Software Engineering 25(6):5193–
5254

RISE Research Institutes of Sweden (2022) SMIRK GitHub repository. URL

https://github.com/RI-SE/smirk/

Runeson P, H¨ost M, Austen R, et al (2012) Case study research in software

engineering: Guidelines and examples. John Wiley & Sons Inc.

Salay R, Queiroz R, Czarnecki K (2018) An analysis of ISO 26262: Machine

learning and safety in automotive software

Schwalbe G, Schels M (2020) A survey on methods for the safety assurance of
machine learning based systems. In: Proc. of the 10th European Congress

Springer Nature 2021 LATEX template

62

Ergo, SMIRK is Safe

on Embedded Real Time Software and Systems

Schwalbe G, Knie B, S¨amann T, et al (2020) Structuring the safety argumen-
tation for deep neural network based perception in automotive applications.
In: Proc. of the Int’l. Conf. on Computer Safety, Reliability, and Security,
Springer, pp 383–394

Schyllander J (2014) Fotg¨angarolyckor - statistik och analys. Tech. Rep.
MSB744, Swedish Civil Contingencies Agency, URL https://rib.msb.se/
ﬁler/pdf/27438.pdf

Socha K, Borg M, Henriksson J (2022) SMIRK: A machine learning-based
pedestrian automatic emergency braking system with a complete safety case.
Software Impacts 13:100,352

Song Q, Borg M, Engstr¨om E, et al (2022) Exploring ML testing in practice:
Lessons learned from an interactive rapid review with axis communications.
In: Proc. of the 1st Int’l. Conf. on AI Engineering – Software Engineering
for AI

Stocco A, Pulfer B, Tonella P (2022) Mind the gap! A study on the transfer-
ability of virtual vs physical-world testing of autonomous driving systems.
IEEE Transactions on Software Engineering

Tambon F, Laberge G, An L, et al (2022) How to certify machine learning
based safety-critical systems? A systematic literature review. Automated
Software Engineering 29(38)

Tao J, Li Y, Wotawa F, et al (2019) On the industrial application of combina-
torial testing for autonomous driving functions. In: Proc. of the Int’l. Conf.
on Software Testing, Veriﬁcation and Validation Workshops, pp 234–240

Thorn E, Kimmel SC, Chaka M, et al (2018) A framework for automated
driving system testable cases and scenarios. Tech. rep., US Department of
Transportation. National Highway Traﬃc Safety Administration.

Tian Y, Pei K, Jana S, et al (2018) Deeptest: Automated testing of deep-
neural-network-driven autonomous cars. In: Proc. of the 40th Int’l. Conf. on
Software Engineering, pp 303–314

Tsilionis K, Wautelet Y, Faut C, et al (2021) Unifying behavior driven devel-
opment templates. In: Proc. of the 29th Int’l. Requirements Engineering
Conf., pp 454–455

de la Vara JL, Ruiz A, Gallina B, et al (2019) The AMASS approach for
assurance and certiﬁcation of critical systems. In: Embedded World 2019

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

63

Weissensteiner P, Stettinger G, Rumetshofer J, et al (2021) Virtual validation
of an automated lane-keeping system with an extended operational design
domain. Electronics 11(1):72

Wiegers K (2008) Karl Wiegers’

software

template. Tech.

tion (SRS)
//www.modernanalyst.com/Resources/Templates/tabid/146/ID/497/
Karl-Wiegers-Software-Requirements-Speciﬁcation-SRS-Template.aspx

rep., Process

requirements

speciﬁca-
Impact, URL https:

Willers O, Sudholt S, Raafatnia S, et al (2020) Safety concerns and mitigation
approaches regarding the use of deep learning in safety-critical perception
tasks. In: Proc. of the Int’l. Conf. on Computer Safety, Reliability, and
Security, pp 336–350

Wozniak E, Cˆarlan C, Acar-Celik E, et al (2020) A safety case pattern for
systems with machine learning components. In: Proc. of the Int’l. Conf. on
Computer Safety, Reliability, and Security, pp 370–382

Wu B, Nevatia R (2008) Optimizing discrimination-eﬃciency tradeoﬀ in inte-
grating heterogeneous local features for object detection. In: Proc. of the
IEEE Conf. on Computer Vision and Pattern Recognition, pp 1–8

Wu W, Kelly T (2004) Safety tactics for software architecture design. In: Proc.
of the 28th Annual Int’l. Computer Software and Applications Conf., pp
368–375

Zablocki ´E, Ben-Younes H, P´erez P, et al (2022) Explainability of deep vision-
based autonomous driving systems: Review and challenges. Int’l Journal of
Computer Vision (130):2425—-2452

Zhang M, Zhang Y, Zhang L, et al (2018) Deeproad: GAN-based metamorphic
testing and input validation framework for autonomous driving systems.
In: Proc. of the 33rd Int’l. Conf. on Automated Software Engineering, pp
132–142

Zimek A, Schubert E, Kriegel HP (2012) A survey on unsupervised outlier
detection in high-dimensional numerical data. Statistical Analysis and Data
Mining: The ASA Data Science Journal 5(5):363–387

Springer Nature 2021 LATEX template

64

Ergo, SMIRK is Safe

Fig. A1: ML Assurance Scoping Argument Pattern [F].

Appendix A AMLAS Safety Argumentation

for Machine Learning in SMIRK

This section describes the complete SMIRK safety argumentation organized
by the six AMLAS stages (Hawkins et al, 2021). For each step, we present an
argument pattern using GSN notation (Assurance Case Working Group, 2021)
and present the ﬁnal argument in a text box.

A.1 Stage 1: Machine Learning Assurance Scoping

Figure A1 shows the overall ML assurance scoping argument pattern for
SMIRK. The pattern, as well as all subsequent patterns in this paper, follows
the examples provided in AMLAS, but adapts it to the speciﬁc SMIRK case.
Furthermore, we provide evidence that supports our arguments.

The top claim, i.e., the starting point for the safety argument for the
ML-based component, is that the system safety requirements that have been
allocated to the pedestrian recognition component are satisﬁed in the ODD
(G1.1). The safety claim for the pedestrian recognition component is made
within the context of the information that was used to establish the safety
requirements allocation, i.e., the system description ([C]), the ODD ([B]), and
the ML component description ([D]). The allocated system safety requirements
([E]) are provided as context. An explicit assumption is made that the allo-
cated safety requirements have been correctly deﬁned (A1.1), as this is part of
the overall system safety process (FuSa and SOTIF) preceding AMLAS. Our

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

65

claim to the validity of this assumption is presented in relation to the HARA
described in [E]. As stated in AMLAS, “the primary aim of the ML Safety
Assurance Scoping argument is to explain and justify the essential relationship
between, on the one hand, the system-level safety requirements and associated
hazards and risks, and on the other hand, the ML-speciﬁc safety requirements
and associated ML performance and failure conditions.”

The ML safety claim is supported by an argument split into two parts.
First, the development of the ML component is considered with an argument
that starts with the elicitation of the ML safety requirements. Second, the
deployment of the ML component is addressed with a corresponding argument.

ML Safety Assurance Scoping Argument [G]
SMIRK instantiates the ML safety assurance scoping argument through
the artifacts listed in the Table 1. The set of artifacts constitutes the
safety case for SMIRK’s ML-based pedestrian recognition component.

A.2 Stage 2: Machine Learning Requirements Assurance

Figure A2 shows the ML Safety Requirements Argument Pattern [I]. The top
claim is that the system safety requirements that have been allocated to the
ML component are satisﬁed by the model that is developed (G2.1). This is
demonstrated through considering explicit ML safety requirements deﬁned for
the ML model [H]. The argument approach is a reﬁnement strategy translating
the allocated safety requirements into two concrete ML safety requirements
(S2.1) provided as context (C2.1). Justiﬁcation J2.1 explains how we allocated
safety requirements to the ML component as part of the system safety process,
including the HARA.

Strategy S2.1 is reﬁned into two subclaims about the validity of the ML
safety requirements corresponding to missed pedestrians and ghost braking,
respectively. Furthermore, a third subclaim concerns the satisfaction of those
requirements. G2.2 focuses on the ML safety requirement SYS-ML-REQ1,
i.e., that the nominal functionality of the pedestrian recognition component
shall be satisfactory. G2.2 is considered in the context of the ML data (C2.2)
and the ML model (C2.3), which in turn are supported by the ML Data
Argument Pattern [R] and the ML Learning Argument Pattern [W]. The
argumentation strategy (S2.2) builds on two subclaims related to two types of
safety requirements with respect to safety-related outputs, i.e., performance
requirements (G2.5 in context of C2.4) and robustness requirements (G2.6 in
context of C2.5). The satisfaction of both G2.5 and G2.6 are addressed by the
ML Veriﬁcation Argument Pattern [BB].

Subclaim G2.3 focuses on the ML safety requirement SYS-ML-REQ2,
i.e., that the pedestrian recognition component shall reject input that does not
resemble the training data to avoid ghost braking. G2.3 is again considered
in the context of the ML data (C2.2) and the ML model (C2.3). For SMIRK,

Springer Nature 2021 LATEX template

66

Ergo, SMIRK is Safe

Fig. A2: ML Safety Requirements Argument Pattern [I].

the solution is the safety cage architecture (Sn2.1) developed in the SMILE
research program (Henriksson et al, 2021a), described in Section 9.3.

Subclaim G2.4 states that the ML safety requirements are a valid devel-
opment of the allocated system safety requirements. The justiﬁcation (J2.2) is
that the requirements have been validated in cross-organizational workshops
within the SMILE III research project. We provide evidence through ML Safety
Requirements Validation Results [J] originating in a Fagan inspection (Sn2.2).

ML Safety Requirements Argument [K]
SMIRK instantiates the ML safety requirements argument through a
subset of the artifacts listed in Table 1, i.e., ML Safety Requirements
Argument Pattern [I], as well as: Safety Requirements Allocated to ML
Component [E], ML Safety Requirements [H], and ML Safety
Requirements Validation Results [J].

A.3 Stage 3: Data Management Assurance

Figure A3 shows the ML Data Argument Pattern [R]. The top claim is that the
data used during the development and veriﬁcation of the ML model is suﬃcient
(G3.1). This claim is made for all three data sets: development data [N],
internal test data [O], and veriﬁcation data [P]. The argumentation strategy
(S2.1) involves how the suﬃciency of these data sets is demonstrated given the
Data Requirements [L]. The strategy is supported by arguing over subclaims
demonstrating suﬃciency of the Data Requirements (G3.2) and that the Data
Requirements are satisﬁed (G3.3). Claim G3.2 is supported by evidence in the
form of a data requirements justiﬁcation report [M]. As stated in AMLAS,
“It is not possible to claim that the data alone can guarantee that the ML

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

67

Fig. A3: ML Data Argument Pattern [R].

safety requirements will be satisﬁed, however the data used must be suﬃcient
to enable the model that is developed to do so.”

Claim G3.3 states that the generated data satisﬁes the data requirements
in context of the decisions made during data collection. The details of the
data collection, along with rationales, are recorded in the Data Generation
Log [Q]. The argumentation strategy (S2.2) uses reﬁnement mapping to the
assurance-related desiderata of the data requirements. The reﬁnement of the
desiderata into concrete data requirements for the pedestrian recognition com-
ponent of SMIRK, given the ODD, is justiﬁed by an analysis of the expected
traﬃc agents and objects that can appear in ESI Pro-SiVIC. For each subclaim
corresponding to a desideratum, i.e., relevance (G3.4), completeness (G3.5),
accuracy (G3.6), and balance (G3.7), there is evidence in a matching section
in the ML Data Validation Report [S].

Springer Nature 2021 LATEX template

68

Ergo, SMIRK is Safe

ML Data Argument [T]
SMIRK instantiates the ML Data Argument through a subset of the
artifacts listed in Table 1, i.e., the ML Data Argument Pattern [R], as
well as: ML Safety Requirements [H], Data Requirements [L], Data
Requirements Justiﬁcation Report [M], Development Data [N], Internal
Test Data [O], Veriﬁcation Data [P], Data Generation Log [Q], and ML
Data Validation Results [S].

A.4 Stage 4: Model Learning Assurance

Figure A4 shows the ML Learning Argument Pattern [W]. The top claim
(G4.1) is that the development of the learned model [V] is suﬃcient. The
strategy is to argue over the internal testing of the model and that the ML
development was appropriate (S4.1) in context of creating a valid model that
meets practical constraints such as real-time performance and cost (C4.2).
Subclaim (G4.2) is that the ML model satisﬁes the ML safety requirements
when using the internal test data [O]. We justify that the internal test results
indicate that the ML model satisﬁes the ML safety requirements (J3.1) by
presenting evidence from the internal test results [X].

Subclaim G4.3 addresses the approach that was used when developing the
model. This claim is in turn supported by two additional subclaims regarding
the type of model selected and the model parameters selected, respectively.
First, G4.4 claims that the type of model is appropriate for the speciﬁed
ML safety requirements and the other model constraints. ML development
processes, including transfer learning, are highly iterative thus rationales for
development decisions must be recorded. Second, G4.5 claims that the param-
eters of the ML model are appropriately selected to tune performance toward
the object detection task within the speciﬁed ODD. Rationales for all relevant
decisions in G4.4 and G4.5 are recorded in the model development log [U].

ML Learning Argument [T]
SMIRK instantiates the ML Learning Argument through a subset of the
artifacts listed in Table 1, i.e., the ML Learning Argument Pattern [W],
as well as: ML Safety Requirements [H], Development Data [N], and
Internal Test Data [O].

A.5 Stage 5: Model Veriﬁcation Assurance

Figure A5 shows the ML Veriﬁcation Argument Pattern [BB]. The top claim
(G5.1) corresponds to the bottom claim in the safety requirements argument
pattern [I], i.e., that all ML safety requirements are satisﬁed. The argumenta-
tion builds on a subclaim and an argumentation strategy. First, subclaim G5.2
is that the veriﬁcation of the ML model is independent of its development. The
veriﬁcation log [AA] speciﬁes how this has been achieved for SMIRK (Sn5.1).

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

69

Fig. A4: ML Learning Argument Pattern [W].

Second, the strategy S5.1 argues that test-based veriﬁcation is an appropriate
approach to generate evidence that the ML safety requirements are met. The
justiﬁcation (J5.1) is that the SMIRK test strategy follows the proposed orga-
nization in peer-reviewed literature on ML testing, which is a better ﬁt than
using less mature formal methods for ML models as complex as YOLOv5.

Following the test-based veriﬁcation approach, the subclaim G5.3 argues
that the ML model satisﬁes the ML safety requirements when the veriﬁcation
data (C5.1) is applied. The testing claim is supported by three subclaims. First,
G5.4 argues that the test results demonstrate that the ML safety requirements
are satisﬁed, for which Veriﬁcation Test Results [Z] are presented as evidence.
Second, G5.5 argues that the Veriﬁcation Data [P] is suﬃcient to verify the
intent of the ML safety requirements in the ODD. Third, G5.6 argues that the
test platform is representative of the operational platform. Evidence for both
G5.5 and G5.6 is presented in the Veriﬁcation Log [AA].

Springer Nature 2021 LATEX template

70

Ergo, SMIRK is Safe

Fig. A5: ML Veriﬁcation Argument Pattern [BB].

ML Veriﬁcation Argument [CC]
SMIRK instantiates the ML Veriﬁcation Argument through a subset of
the artifacts listed in Table 1, i.e., the ML Veriﬁcation Argument
Pattern [W], as well as: ML Safety Requirements [H], Veriﬁcation Data
[P], and the ML Model [V].

A.6 Stage 6: Model Deployment Assurance

Figure A6 shows the ML Veriﬁcation Argument Pattern [GG]. The top claim
(G6.1) is that the ML safety requirements SYS-ML-REQ1 and SYS-ML-
REQ2 are satisﬁed when deployed to the ego car in which SMIRK operates.
The argumentation strategy S6.1 is two-fold. First, subclaim G6.2 is that
the ML safety requirements are satisﬁed under all deﬁned operating scenarios
when the ML component is integrated into SMIRK in the context (C6.1) of
the speciﬁed operational scenarios [EE]. Justiﬁcation J6.1 explains that the
scenarios were identiﬁed through an analysis of the SMIRK ODD. G6.2 has

Springer Nature 2021 LATEX template

Ergo, SMIRK is Safe

71

Fig. A6: ML Deployment Argument Pattern [GG].

another subclaim (G6.4), arguing that the integration test results [FF] show
that SYS-ML-REQ1 and SYS-ML-REQ2 are satisﬁed.

Second, subclaim G6.3 argues that SYS-ML-REQ1 and SYS-ML-
REQ2 continue to be satisﬁed during the operation of SMIRK. The supporting
argumentation strategy (S6.3) relates to the design of SMIRK and is again
two-fold. First, subclaim G6.6 argues that the operational achievement of
the deployed component satisﬁes the ML safety requirements. Second, sub-
claim G6.5 argues that the design of SMIRK into which the ML component
is integrated ensures that SYS-ML-REQ1 and SYS-ML-REQ2 are satis-
ﬁed throughout operation. The corresponding argumentation strategy (S6.4)
is based on demonstrating that the design is robust by taking into account
identiﬁed erroneous behavior in the context (C5.1) of the Erroneous Behavior

Springer Nature 2021 LATEX template

72

Ergo, SMIRK is Safe

Log [DD]. More speciﬁcally, the argumentation entails that predicted erro-
neous behavior will not result in the violation of the ML safety requirements.
This is supported by two subclaims, i.e., that the system design provides suﬃ-
cient monitoring of erroneous inputs and outputs (G6.7) and that the system
design provides acceptable response to erroneous inputs and outputs (G6.8).
Both G6.7 and G6.8 are addressed by the safety cage architecture that moni-
tors input through OOD detection using an autoencoder that rejects anomalies
accordingly. The acceptable system response is to avoid emergency braking
and instead let the human driver control ego car.

ML Veriﬁcation Argument [HH]
SMIRK instantiates the ML Deployment Argument through a subset of
the artifacts listed in Table 1, i.e., the ML Deployment Argument
Pattern [GG], as well as: System Safety Requirements [A], Environment
Description [B], System Description [C], ML Model [V], Erroneous
Behaviour Log [DD], Operational Scenarios [EE], and Integration
Testing Results [FF].

