JSS

Journal of Statistical Software

MMMMMM YYYY, Volume VV, Issue II.

doi: 10.18637/jss.v000.i00

2
2
0
2

l
u
J

6

]

O
C

.
t
a
t
s
[

1
v
6
8
9
2
0
.
7
0
2
2
:
v
i
X
r
a

fabisearch: A Package for Change Point Detection in
and Visualization of the Network Structure of
Multivariate High-Dimensional Time Series in R

Martin Ondrus
University of Alberta

Ivor Cribben
University of Alberta

Abstract

Change point detection is a commonly used technique in time series analysis, capturing
the dynamic nature in which many real-world processes function. With the ever increasing
troves of multivariate high-dimensional time series data, especially in neuroimaging and
ﬁnance, there is a clear need for scalable and data-driven change point detection meth-
ods. Currently, change point detection methods for multivariate high-dimensional data
are scarce, with even less available in high-level, easily accessible software packages. To
this end, we introduce the R package fabisearch, available on the Comprehensive R Archive
Network (CRAN), which implements the factorized binary search (FaBiSearch) method-
ology. FaBiSearch is a novel statistical method for detecting change points in the network
structure of multivariate high-dimensional time series which employs non-negative ma-
trix factorization (NMF), an unsupervised dimension reduction and clustering technique.
Given the high computational cost of NMF, we implement the method in C++ code and
use parallelization to reduce computation time. Further, we also utilize a new binary
search algorithm to eﬃciently identify multiple change points and provide a new method
for network estimation for data between change points. We show the functionality of
the package and the practicality of the method by applying it to a neuroimaging and a
ﬁnance data set. Lastly, we provide an interactive, 3-dimensional, brain-speciﬁc network
visualization capability in a ﬂexible, stand-alone function. This function can be conve-
niently used with any node coordinate atlas, and nodes can be color coded according to
community membership (if applicable). The output is an elegantly displayed network laid
over a cortical surface, which can be rotated in the 3-dimensional space.

Keywords: change point detection, time series, high-dimensional, NMF, fMRI, network anal-
ysis, R, visualization, bioinformatics.

 
 
 
 
 
 
2

Factorized Change Point Detection in R

1. Introduction: Time Series Change Point Detection in R

Change point detection is a collection of methods in time series analysis that identify if and
when changes occur in the statistical properties of time series data. Consequently, it is a
useful way of studying the evolution of the underlying data generating process. In practice,
changes might correspond to changes in the mean, the variance, or, for multivariate time
series data, changes in the covariance between the marginal time series. Change points
convey useful information and can be applied to a wide variety of problems such as the
active management of ﬁnancial assets (Lai and Xing 2015), anomaly detection in computer
networks for cybersecurity (Tartakovsky, Polunchenko, and Sokolov 2013), or investigating
dynamic brain networks during task performance (Cribben, Haraldsdottir, Atlas, Wager, and
Lindquist 2012).

Given the utility of change point detection, numerous methods have been developed. For a
thorough review on the available methods, see Truong, Oudre, and Vayatis (2020). Many R
packages have also been made available. There are some packages available with a speciﬁc ap-
plication in mind such as genetic/genomics data, for example, DNAcopy (Seshan and Olshen
2020), Rseg (Lamy, Wiuf, Ørntoft, and Andersen 2010), and cumseg (Muggeo 2020). Other
packages exist for more general applications, such as the changepoint package (Killick and
Eckley 2014) which includes a variety of single and multiple change point detection methods.
The strucchange package (Zeileis, Leisch, Hornik, and Kleiber 2002) attempts to ﬁnd struc-
tural breaks in univariate time series. However, it focuses on detecting at most one change
point (AMOC) and is motivated by changes in (linear) regression models. The cpm package
(Ross 2015) provides functionality to detect change points in univariate time series using a
from distribution-free change detection methods for changes
variety of diﬀerent methods:
in the mean, variance, or general distribution to parametric change detection methods for
Gaussian, Bernoulli and Exponential sequences. The mosum package (Meier, Kirch, and Cho
2021) provides functionality for ﬁnding changes in the mean in a univariate time series using
the moving sum statistic. For complex multivariate systems (such as neuroimaging data),
however, these methods are unable to capture changes which occur from the interaction of
multiple univariate components.

Although not as widely available, some R packages for change point detection in multivariate
time series data do exist. The bcp package (Erdman and Emerson 2007) was extended to
multivariate problems through with the work of Wang and Emerson (2015), who extended
the methodology to ﬁnd change points on general connected graphs using Bayesian meth-
ods. Chen and Zhang (2015) proposed a graph based change point detection method in the
gSeg package (Chen, Zhang, Chu, and Song 2020), but it requires a pre-deﬁned similarity
measure. The ecp package (Meier et al. 2021) attempts to ﬁnd multiple change points while
making minimal assumptions about the data generating process, and provides functionality
for non-parametric change point detection for both univariate and multivariate time series
data. Grundy, Killick, and Mihaylov (2020) suggest a novel geometric method to reduce the
change point problem to two dimensions and then apply change point detection methods
to ﬁnd changes in the mean and in the variance. They compare their new method to ecp,
demonstrating its superior performance and making it available as the changepoint.geo pack-
age (Grundy 2020). The onlineCOV (Li and Li 2020b) package implements the work of Li and
Li (2020a) who propose an online change point detection method which can be applied to both
Gaussian and non-Gaussian data. This method, however, requires training in order for the

Journal of Statistical Software

3

time series dependence structure to be learned. Londschien, Kovács, and Bühlmann (2020)
suggest a Gaussian graphical model based method for detecting change points in applications
with missing data. These methods are available in the hdcd package available on GitHub
(https://github.com/mlondschien/hdcd). Xiong and Cribben (2021a) developed the Vine
Copula Change Point (VCCP) method, which uses vine copulas, and thus can detect changes
in dependence structures beyond the linear, Gaussian assumptions that dominate most mod-
els. This methodology is available in the vccp package (Xiong and Cribben 2021b). Lastly,
Anastasiou, Cribben, and Fryzlewicz (2021) developed the Cross-Covariance Isolate Detect
(CCID) method, which considers changes in the cross-covariance structure in a multivariate
(possibly high-dimensional) time series. Using a wavelet based approach, it transforms the
multivariate time series and then uses a CUSUM based statistic to detect change points. This
method is available in the ccid package (Anastasiou, Cribben, and Fryzlewicz 2022).

Neuroimaging data provides an especially unique opportunity to apply change point detection
methods. Functional magnetic resonance imaging (fMRI) is a popular method of indirectly
measuring brain activity using the blood-oxygen-level-dependent (BOLD) signal (Ogawa, Lee,
Kay, and Tank 1990). With increases in neuronal activity, the BOLD signal increases and
shows brain regions “lighting up” in fMRI images. Multiple slices of each subject’s brain
are imaged and divided into voxels, which are small cubes of a few millimeters in dimension.
Subsequently, a time series of voxel activity can be measured by taking multiple fMRI scans
sequentially over time. It is of great interest to understand brain dynamics through these
time series of voxel activity, or, more typically, the activity in a cluster of voxels referred to as
a region of interest (ROI). Functional connectivity (FC) seeks to deﬁne relationships between
the voxel or ROI time series (Biswal, Zerrin Yetkin, Haughton, and Hyde 1995), through the
use of correlation, covariance, precision matrices, or other methods (see Cribben and Fiecas
2016 for a comprehensive review). Using graph theory, we can intuitively understand FC as
a graphical model where nodes represent ROIs (time series) and edges represent temporal
relationships.

Although change point detection has been applied to multivariate fMRI time series data
(Cribben et al. 2012; Barnett and Onnela 2016; Dai, Zhang, and Srivastava 2019; Ofori-
Boateng, Gel, and Cribben 2021), most methods and software lack an ability to consider a
multivariate high-dimensional time series or whole brain dynamics that are a result of ap-
plying high-dimensional cortical parcellations (e.g., the Gordon atlas with p = 333 ROIs,
Gordon, Laumann, Adeyemo, Huckins, Kelley, and Petersen 2016). In addition, many of the
existing R packages require a priori knowledge or assumptions, which makes them less ac-
cessible to a general audience who may not have domain expertise in change point detection.
Furthermore, many of the existing R packages do not provide functionality to both estimate
and visualize (brain-speciﬁc) networks between change points. To this end, we introduce the
R package, fabisearch (Ondrus and Cribben 2021), which implements the factorized binary
search (FaBiSearch) methodology (Ondrus, Olds, and Cribben 2021). FaBiSearch is a pow-
erful technique which employs non-negative matrix factorization (NMF) to detect changes in
the network (or clustering) structure of multivariate high-dimensional time series data. By
integrating NMF as the dependency modelling method in change point detection, FaBiSearch
scales to data sets where the number of variables (or time seres) is in the 100s or 1000s, and
in particular for the case where the number of time series is greater than the number of time
fabisearch also contains functionality to estimate and visualize networks
points (p >> T ).
between change points. In particular, it provides a ﬂexible stand-alone function that has the

4

Factorized Change Point Detection in R

ability to visualize an interactive, 3-dimensional, brain-speciﬁc network. To the best of our
knowledge, the only other R package which includes similar capability is brainGraph, but it
displays only static visualizations at predeﬁned orientations. In contrast, the visualization
function in fabisearch produces an elegant network laid over a cortical surface, which can
be inﬁnitely adjusted by rotating in 3-dimensions over the x, y, and z axes using the cursor.
Accordingly, this provides a level of interaction and exploratory potential that is not possible
with simple, ﬂat visualizations. Conveniently, fabisearch can be used with any node coordinate
atlas such as the Gordon atlas (Gordon et al. 2016) and the Automated Anatomical Labelling
atlas (Tzourio-Mazoyer, Landeau, Papathanassiou, Crivello, Etard, Delcroix, Mazoyer, and
Joliot 2002), and for any manually uploaded coordinate inputs. Nodes can also be colored
based on community membership, which makes it easy to examine complex interactions. In
addition, the scope of the networks can be adjusted by sub-selecting nodes to include in the
visualization, which may allow for easier interpretation. Lastly, the visualization function is
general; it can also be used on data sets without any change points.

The rest of this paper is organized as follows. First, we provide a brief overview of the
FaBiSearch methodology in Section 2. In Section 3 we detail the functionality of fabisearch,
while in Section 4 we implement fabisearch on a resting-state fMRI data set. Next, we
apply fabisearch to a ﬁnancial time series data set to showcase its generalizability to other
data sources in Section 5. Finally, we summarize and conclude our work in Section 6. The
fabisearch package is made freely available on the Comprehensive R Archive Network (CRAN)
at https://cran.r-project.org/package=fabisearch.

2. Methodology

In this section, we provide an overview of the Factorized Binary Search (FaBiSearch) method
for detecting change points in the network (or clustering) structure of multivariate high-
dimensional time series data. We also describe how it estimates networks between each pair
of detected change points.

2.1. Non-negative matrix factorization

Non-negative matrix factorization (NMF) is a matrix factorization technique and an unsu-
pervised dimension reduction method for projecting high-dimensional data sets into lower
dimensional spaces (Lee and Seung 2001). Given an n × p matrix, X, where n is the number
of independent samples and p is the number of features, NMF approximates X, using the
product of a n × r coeﬃcient matrix, W , and a r × p basis matrix, H (Figure 1). The dimen-
sionality of factors W and H is controlled by the factorization rank, r, where r << min(n, p).
Rank is selected to manage the bias-variance tradeoﬀ, and due to the inherent clustering prop-
erty of NMF (Li and Ma 2004; Li, Yang, Xu, Qin, and Zhang 2014), it can be chosen based
on the number of unique clusters in X.
An important step for comparing the quality of NMF models (Lee and Seung 2001) is calculat-
ing the loss between X and the factorization W H. We use the generalized Kullback-Leibler
divergence (or I -divergence: Honkela, Duch, Girolami, and Kaski 2011) based loss,

D(X||W H) = X
ij

(Xij log Xij

(W H)ij

− Xij + (W H)ij),

(1)

Journal of Statistical Software

5

Figure 1: The conceptualization of non-negative matrix factorization (NMF). The input
matrix X is approximated by the product of low dimensional factors W and H, where
r << min(n, p).

where the asymmetric divergence between the input matrix X and the NMF factorization
W H is calculated for each ijth entry. This and many other measures of loss as well as
algorithms for solving NMF are implemented in the R package NMF (Gaujoux and Seoighe
2010). We use the C++ based nmf() function as the basis for implementing NMF in our
fabisearch package. See ?nmf() for more details on methods available in the NMF package,
which correspond to the algtype argument in the fabisearch package. We adjust the NMF
deﬁnition to a time series context, where the input multivariate (possibly high-dimensional)
time series Y ∈ RT ×p
is approximated by the dot product of W and H with r << min(T, p):
≥0

Y T ×p ≈ W T ×rH r×p,

where T, r, and p denote the number of time points, the rank, and the number of time series,
respectively.

In FaBiSearch, we seek to model shared linear dependencies across time series and use the
Kullback-Leibler divergence (1) to evaluate how well W H reconstructs Y . We expect the
NMF reconstruction of the input matrix to have lower loss when Y shows clustering structure
amongst the time series. When Y contains a change point however, NMF suﬀers higher
observed overall loss as it is unable to model these multiple, diﬀerent, clustering structures.

2.2. Initialization and the factorization rank parameter

Given the iterative manner in which the factors, W and H, are estimated, FaBiSearch is
sensitive to the initial values of these factors. To achieve a satisfactory approximation the
factorization must be performed over multiple runs, using random initial values for W and H.
We deﬁne the number of runs as nruns, which is set to balance accuracy (higher nruns) and
computational time (lower nruns). In the fabisearch package, the argument for the number
of runs is denoted as nruns and by default is set to 50, which reasonably balances accuracy
and computational time for most applications (see Section 6 for more details).
The factorization rank, r, is also a key parameter in NMF. For FaBiSearch, it may be speciﬁed
a priori if known. However, if unknown, we provide a method for calculating the optimal rank
that is adapted from Frigyesi and Höglund (2008) with the only salient deviation being that
we choose to permute Y over both rows and columns instead of just over columns, denoted
as Y ∗. Accordingly, we ﬁnd the global optimal rank, ropt, by comparing the change in loss
in response to increasing the rank of the original multivariate time series Y to Y ∗. The ﬁrst
rank where the decrease in loss for Y is less than for Y ∗ is denoted by ropt.

6

Factorized Change Point Detection in R

2.3. Segmentation

FaBiSearch requires a suﬃcient number of data points to compute the factorization, hence,
we deﬁne δ, which is a user speciﬁed variable, as the minimum distance between candidate
change points. Naturally, it also deﬁnes the distance between the beginning (end) of the time
series to the ﬁrst (last) time point to be evaluated. In practice, δ should be large enough to
maintain suﬃcient stability in the NMF estimates, but also small enough such that change
points grouped closely in time are not missed.

Binary segmentation is the most common segmentation technique in change point detection,
In this method, all time
which can be recursively applied to ﬁnd multiple change points.
points from (δ) to (T − δ) are sequentially evaluated to ﬁnd the ﬁrst candidate change point,
which maximizes or minimizes a chosen criterion (Douglas and Peucker 1973; Ramer 1972;
Duda and Hart 1974). For our applications we are interested in multivariate high-dimensional
time series, hence applying NMF to all possible time points (δ) to (T − δ) with a reasonable
nruns is computationally cumbersome. Fortunately, we can intuitively reframe this as an
optimization problem, where we seek to optimize our given criterion (loss).
In particular,
in FaBiSearch, we implement a binary search based method as the optimization technique
for eﬃcient segmentation (for more details see Ondrus et al. 2021). The principle of this
technique is based on the intuition that ﬁtments of NMF on data with changing clustering
structure result in higher loss. To begin, the time series is split into two equal length segments.
From here, each segment is separately ﬁt with NMF and the losses of the two segments are
calculated. The segment with higher loss is the segment with greater clustering heterogeneity,
and therefore greater likelihood of containing a change point. This process is repeated, and at
each step the size of the time segment reduces by approximately half (Figure 2). This continues
until the segment is narrowed down to a single time point, which is then saved as the the ﬁrst
candidate change point, ˆq1. The time series is then separated into two components, (1:ˆq1)
and (ˆq1+1:T), and the binary search algorithm is repeated recursively on each component to
detect multiple change points.

2.4. Reﬁtting segments and statistical inference

FaBiSearch’s strategy is to ﬁrst overestimate the number of change points by recursively ﬁnd-
ing candidate change points, and then pruning the results through a reﬁtting and statistical
inference procedure. To begin, we deﬁne a set ˆQ, which includes all candidate change points
ˆq1, ....ˆqk (detected as depicted in Section 2.3) that have been arranged in ascending order.
Then, we deﬁne a set, W = {1, ˆQ, T }, from ˆQ, and the beginning and the end of the time
series (1 and T , respectively). For each candidate change point, ˆqi, we deﬁne the left and
right boundaries

bL
i

= (wi : wi+1);

bR
i

= (wi+1 + 1 : wi+2).

Next, we deﬁne the data segment for ˆqi as Zi ← {Y t}bL
ZL
i

which capture data before and after the candidate change point ˆqi as

and ZR
i

i ∪bR
i

and corresponding sub-segments

ZL

i ← {Y t}bL

i

; ZR

i ← {Y t}bR

i

.

. The loss (diﬀerence between the
After this, we re-ﬁt NMF to data sub-segments ZL
i
original data and its reconstruction) of these two sub-segments is summed for each ﬁtment
over nreps, generating a distribution of losses, li = {li,1, ..., li,nreps}. To then determine whether

and ZR
i

Journal of Statistical Software

7

Figure 2: A graphic of the ﬁrst two iterations of the binary search procedure utilized in
FaBiSearch to detect the ﬁrst candidate change point. The multivariate time series (T =
200) denoted by a blue line, is recursively divided (by approximately half, at each step) to
ﬁnd the true change point (t∗ = 120) denoted by the red cross.

to keep or remove the candidate change point, we compare this distribution to a reference
distribution. This reference distribution is generated by ﬁrst taking the partition, Zi, and
permuting across time points to disrupt the temporal organization of the data structure. For
each permutation in nreps, we re-ﬁt NMF and sum the loss of the two sub-segments, generating
reference distribution l∗
i,nreps}. By default, statistical inference between these two
i
distributions is performed using a one-sided, two sample t-test assuming unequal variances
(Welch’s t-test). Recall previously the intuition that time series with a change point have
inconsistent clustering structure and thus exhibit higher loss when ﬁtting NMF. Consequently,
we are interested in the following hypothesis:

i,1, ..., l∗

= {l∗

H0 : µ(li) ≥ µ(l∗
)
i
).
Ha : µ(li) < µ(l∗
i

In the fabisearch package, we also provide two non-parametric tests, namely the Wilcoxon
signed-rank test and the Kolmogorov-Smirnov test, to compare the re-ﬁtted distribution, li, to
. The null hypothesis for Wilcoxon signed-rank test is similar to
the reference distribution, l∗
i
the two sample t-test except the test is on the medians of the distribution, and the hypothesis
for the Kolmogorov-Smirnov test is as follows:

H0 : li and l∗
i

are identically distributed

Ha : The cumulative distribution function of li lies above l∗
i

.

Since FaBiSearch may ﬁnd multiple change points in the input time series Y , we adjust the
p-value for each statistical test for multiple comparisons using the Benjamini and Hochberg
(1995) method. For the statistical tests, if we reject H0, we retain the change point ˆqi,
otherwise we remove it.

8

Factorized Change Point Detection in R

2.5. Estimating stationary networks

After the change points have been detected, stable networks between the change points can
be estimated for visualization and interpretation purposes, using various methods such as
correlation or precision matrices. Here, however, we introduce an NMF-based method for
computing the adjacency matrix for data between change points. The procedure proceeds
as follows: the ﬁrst step is to re-apply NMF to each stationary block of data. Then, using
values in the coeﬃcient matrix, H, the cluster membership of each time series (or variable)
is determined. Speciﬁcally, each column in H is assigned to cluster r which has the highest
coeﬃcient value. Then, for each run in nruns, an adjacency matrix,

Aij =

(1,

if i, j are in the same cluster;

0, otherwise

is generated denoting cluster membership of each component in the multivariate time series,
Y . This information is combined into a consensus matrix, which is computed by averaging
the individual adjacency matrices:

Cij = µ(A1
ij

,..., Anrun

ij

), 0 ≤ Cij ≤ 1.

The main advantage of this procedure is that it combines results across nruns, which may
separately have high variance, into a matrix where each entry denotes the probability of two
nodes being in the same cluster. This procedure is similar to stability selection in Meinshausen
and Bühlmann (2010), and the bootstrapping in Zhu and Cribben (2018). Hence, there are
two possible methods for deﬁning the ﬁnal adjacency matrix from this consensus matrix. The
ﬁrst method entails applying hierarchical clustering to classify cluster membership amongst
nodes from the consensus matrix, and cutting the resulting tree at a predetermined number of
clusters. The second method entails deﬁning an adjacency matrix from the consensus matrix
using a prespeciﬁed threshold, λ, which controls the sparsity of the ﬁnal adjacency matrix,
that is,

Aij =

(1,

if Cij > λ;
0, otherwise.

3. Software Overview

In this section, we provide an overview of the functions included in fabisearch, how they
connect to the methodology, and the related arguments for the functions.

3.1. Shared arguments

We ﬁrst delineate four arguments which have the same deﬁnitions and uses in the functions
opt.rank(), detect.cps(), and est.net().

• Y is the input multivariate time series in matrix format, with variables organized in

columns and time points in rows. All entries in Y must be positive.

Journal of Statistical Software

9

• nruns is the number of runs, or random restarts, when computing NMF. This value
should scale appropriately to the size of the input matrix Y; larger data sets require more
random starts to obtain convergence to a stable estimation of the NMF parameters.

• rank is a hyperparameter for NMF and is used to balance the bias-variance tradeoﬀ
when estimating the model parameters. rank can be set to a positive integer value. By
default, rank is set to NULL, and the procedure opt.rank() as described in Section 2.2
is used to ﬁnd the optimal rank for Y.

• algtype denotes the NMF algorithm. The methods available correspond to those avail-
able in Gaujoux and Seoighe (2010)’s NMF package. By default it is set to the "brunet"
algorithm, see ?nmf() for other available methods.

3.2. Finding optimal rank

We provide an automated method for ﬁnding the optimal rank of the multivariate time series,
Y , through the

opt.rank(Y, nruns = 50, algtype = "brunet")

function. All arguments are described in Section 3.1. opt.rank() returns a numeric integer
denoting the optimal rank.

3.3. Change point detection

Change point detection using the FaBiSearch method is carried out by the

detect.cps(Y, mindist = 35, nruns = 50, nreps = 100, alpha = NULL,

rank = NULL, algtype = "brunet", testtype = "t-test")

function. Arguments Y, nruns, rank, and algtype are described in Section 3.1, and the
remaining arguments are characterized as follows:

• mindist is the minimum distance between change points, and also corresponds to the
minimum number of time points to be included in the NMF estimation. It should be
large enough such that adequate precision is attained when estimating parameters, but
small enough to account for possibly multiple change points being grouped closely in
time (see Section 2.3).

• nreps corresponds to the number of repetitions for the statistical inference step. It dic-
tates the number of permutations generated for the statistical inference test on the can-
didate change points (Welch’s t-test, the Wilcoxon signed-rank test, or the Kolmogorov-
Smirnov test) and should be large enough such that adequate statistical power is at-
tained.

• alpha is the signiﬁcance level. It can be set to a positive integer denoting the α value
to use in the Welch’s t-test, the Wilcoxon signed-rank test, or the Kolmogorov-Smirnov
test. By default, it is set to NULL, which returns the p-value of the test.

10

Factorized Change Point Detection in R

• testtype = "t-test" is a character string, which deﬁnes the type of statistical test to
use during the inference procedure. By default it is set to “t-test”. The other options
are “ks” and “wilcox” which correspond to the Kolmogorov-Smirnov and the Wilcoxon
signed-rank tests, respectively.

detect.cps() returns a list with the following three components:

• $rank is the rank used for NMF estimation.

• $change_points are the results of the procedure summarized in a table. Each row in
the matrix corresponds to a candidate change point, where columns T and stat_test
correspond to the time of the change point and the result of the statistical inference test,
respectively. If alpha is a positive real number, this value is used as the signiﬁcance level
for the statistical inference test, and a boolean value is returned based on its statistical
signiﬁcance. Conversely, if the alpha argument is set to the string "p-value", then the
p-value of the statistical test is returned.

• $compute_time is the computational time of the procedure, saved as a difftime object.

3.4. Estimating stationary networks

As described in Section 2.5, methods for estimating stationary networks between change
points are implemented in fabisearch using the

est.net(Y, lambda, nruns = 50, rank = "optimal", algtype = "brunet",
changepoints = NULL)

function. Arguments Y, nruns, rank, and algtype are described in Section 3.1, and the
remaining arguments are characterized as follows:

• lambda has two purposes: ﬁrst, it is used to specify the method for calculating the
adjacency matrix (clustering or thresholding). Second, it is used to denote a value in
the selected process (that is, the number of clusters or the cutoﬀ value, respectively). If
lambda is a positive integer, then the clustering based method is used and the integer
corresponds to the number of clusters. Conversely, if lambda is a positive real number <
1, then the thresholding method is performed and the lambda value is used as the cutoﬀ
for the consensus matrix. lambda may also be a vector of either cluster or threshold
values, in which case the output is a list of results where each component corresponds
to a lambda value in the vector.

• changepoints is a vector of positive integers with default value equal to NULL. It is
used to specify whether change points exist in the input multivariate time series, Y,
and thus whether Y should be split into multiple stationary segments and networks
estimated separately for each. If changepoints, say c(100, 200), are speciﬁed, Y is split
at time points 100 and 200, corresponding to 3 stationary segments. For each stationary
segment an adjacency matrix is estimated sequentially, and returned as a list where each
component corresponds to a stationary segment.

Journal of Statistical Software

11

est.net() returns a square p × p adjacency matrix in the following format:

A =

(1,

if two nodes share an edge;

0, otherwise.

3.5. Brain network visualization

To visualize the stationary networks described in Section 2.5, we create an interactive 3-
dimensional plot for brain-speciﬁc networks through the

net.3dplot(A, ROIs = NULL, colors = NULL, coordROIs = NULL)

function. The input data, A, is a p × p adjacency matrix stored as a numerical matrix in the
following format:

A =

(1,

if two nodes share an edge;

0, otherwise.

It is important to note that A in net.3dplot() can be any adjacency matrix and does not
have to be the result of the change point method discussed in the previous sections, hence
net.3dplot() is ﬂexible and extends beyond the included change point estimation method.
The remaining input arguments are described as follows:

• ROIs is either a vector of character strings specifying the communities to plot, or a
vector of integers specifying which ROIs to plot by their ID. By default it is set to NULL
and all communities and ROIs are plotted. Communities available for the Gordon atlas
include: “Default”, “SMhand”, “SMmouth”, “Visual”, “FrontoParietal”, “Auditory”,
“None”, “CinguloParietal”, “RetrosplenialTemporal”, “CinguloOperc”, “VentralAttn”,
“Salience”, and “DorsalAttn”.

• colors sets the color of nodes, in hex code format, in the ﬁnal plot based on community
membership. If a vector of communities is speciﬁed, then the nth community is assigned
the nth color in the colors vector.

• coordROIs is a dataframe of community tags and Montreal Neurological Institute (MNI)
coordinates for regions of interest (ROIs) to plot, which is by default set to NULL and
uses the Gordon atlas (Gordon et al. 2016). See ?gordon.atlas for an example using
the Gordon atlas. The format of the dataframe is as follows:

Communities
<string>
...

x.mni

y.mni
<double> <double> <double>
...

z.mni

...

...

The ﬁrst column is a string of community labels, and the subsequent three columns are
the x, y, and z coordinates in MNI space, respectively. If communities are not applicable
to the atlas, the ﬁrst column is deﬁned by NA (e.g., in the AALatlas). Using this format,
any atlas/node coordinate system can be used with the net.3dplot() function.

12

Factorized Change Point Detection in R

It is also important to note that the dimension of A must be congruent with the node atlas
deﬁned by coordROIs. For example, the default gordon.atlas has p = 333, and so A must
be a 333 × 333 matrix. net.3dplot() opens and displays brain networks in an interactive
3-dimensional RGL window, which can be interacted with by clicking and dragging the cursor.

4. Resting-state fMRI Example

This resting-state fMRI data set includes 25 participants scanned at New York University over
three visits (http://www.nitrc.org/projects/nyu_trt). For each visit, participants were
asked to relax, remain still, and keep their eyes open. A Siemens Allegra 3.0-Tesla scanner
was used to obtain the resting-state scans for each participant. Each visit consisted of 197
contiguous EPI functional volume scans with time repetition (TR) of 2000ms, time echo (TE)
of 25ms, ﬂip angle (FA) of 90◦, 39 number of slices, matrix of 64 × 64, ﬁeld of view (FOV) of
192mm, and voxel size of 3 × 3 × 3mm3. Software packages AFNI (http://afni.nimh.nih.
gov/afni) and FSL (http://www.fmrib.ox.ac.uk) were used for preprocessing. Motion was
corrected using FSL’s mcflirt (rigid body transform, cost function normalized correlation,
and reference volume the middle volume). Normalization into the Montreal Neurological
Institute (MNI) space was performed using FSL’s flirt (aﬃne transform, cost function,
mutual information). Probabilistic segmentation was conducted to determine white matter
and cerebrospinal ﬂuid (CSF) probabilistic maps and was obtained using FSL’s fast with a
threshold of 0.99. Nuisance signals (the six motion parameters, white matter signals, CSF
signals, and global signals) were removed using AFNI’s 3dDetrend. Volumes were spatially
smoothed using a Gaussian kernel and FWHM of 6mm with FSL’s fslmaths. We used the
work of Gordon et al. (2016) to determine the ROI atlas, which results in the cortical surface
being parcellated into 333 areas of homogenous connectivity patterns, and the time course
for each is determined by averaging the voxels within each region for each subject. Regional
time courses were then detrended and standardized to unit variance. Lastly, a fourth-order
Buttterworth ﬁlter with a 0.01-0.10 Hertz pass band was applied. Hence, in summary, we
have a high-dimensional time series data set with T = 197 and p = 333. For more details on
the data set, see Xu, Reiss, and Cribben (2021).

4.1. Data summary and change point detection

We focus on the second scan of the ﬁrst subject of the resting-state fMRI data set. To begin,
we inspect the data by calculating summary statistics and plotting individual time series for
the ﬁrst 4 ROIs (or nodes):

R> data("gordfmri", package = "fabisearch")
R> print(gordfmri[1:10,1:4])

3

2

1

4
101.38913 99.42839 100.52694 102.54848
97.81678 98.92552 99.19077 101.17978
98.19218 99.78406 97.61275 101.00244
102.66680 99.15784 98.88478 102.29753
102.12487 100.05531 97.08151 99.14587
104.41834 99.33773 97.17439 98.52235
101.86739 99.48253 96.97930 100.00666

1
2
3
4
5
6
7

Journal of Statistical Software

13

Figure 3: Plots of the ﬁrst four ROI (or column, or node) time series of the gordfmri data.
gordfmri is processed from the second scan of the ﬁrst subject from the NYU test-restest
resting-state fMRI data set.

8
9
10

98.79015 99.78261
100.03848 100.08721
99.73903 97.77725

99.77578 95.95149
98.35715 101.50598
98.39061 99.86871

R> summary(gordfmri[,1:4])

1

2

3

4

Min.
: 94.42
1st Qu.: 98.60
Median : 99.74
Mean
:100.00
3rd Qu.:101.39
:106.73
Max.

Min.
: 96.98
1st Qu.: 99.16
Median : 99.90
Mean
:100.00
3rd Qu.:100.96
:103.64
Max.

Min.
: 96.98
1st Qu.: 99.03
Median :100.06
Mean
:100.00
3rd Qu.:100.80
:103.88
Max.

Min.
: 95.54
1st Qu.: 98.78
Median : 99.81
Mean
:100.00
3rd Qu.:101.31
:104.67
Max.

R> par(mfrow=c(4,1))
R> for(i in 1:4){
+
+
+}

plot(gordfmri[,i], type = "l", cex.lab = 1.5, cex.axis = 1.5,

xlab = "Time", ylab = paste("Node", i))

Since we are interested in ﬁnding change points in this data set, we must consider the input

14

Factorized Change Point Detection in R

arguments for detect.cps(). We assume that the number of clusters is unknown a priori,
hence, we keep rank at its default value, optimal. Additionally, we also keep alpha at its
default value, NULL, which returns the p-values for each change point. We use default values
for all other arguments.

R> set.seed(12345)
R> fmrioutput = detect.cps(gordfmri)

As detect.cps() runs, it prints updates in the console. We show these console messages
below:

[1] "Finding optimal rank"
[1] "Optimal rank: 7"
[1] "35 : 162"
[1] "35 : 99"
[1] "35 : 67"
[1] "35 : 51"
[1] "35 : 43"
[1] "35 : 39"
[1] "35 : 37"
[1] "35 : 36"
[1] "Change Point At: 35 , Delta Loss: -26.3978932820244"
[1] "70 : 162"
[1] "70 : 116"
[1] "70 : 93"
[1] "70 : 82"
[1] "70 : 76"
[1] "70 : 73"
[1] "70 : 72"
[1] "70 : 71"
[1] "Change Point At: 70 , Delta Loss: -14.7458419916325"
[1] "105 : 162"
[1] "133 : 162"
[1] "133 : 148"
[1] "133 : 141"
[1] "136 : 141"
[1] "136 : 139"
[1] "136 : 138"
[1] "136 : 137"
[1] "Change Point At: 136 , Delta Loss: -18.0354425518596"
[1] "Refitting split at 35"
[1] "Refitting split at 70"
[1] "Refitting split at 136"
[1] "Permuting split at 35"
[1] "Permuting split at 70"
[1] "Permuting split at 136"

Journal of Statistical Software

15

Since we speciﬁed that the factorization parameter, rank, to be optimally determined, the ﬁrst
set of messages relate to this process. The procedure ﬁnds an optimal rank equal to 7, and then
the function proceeds to apply the FaBiSearch method to the multivariate high-dimensional
time series. Each subsequent message corresponds to the time indices being evaluated at
that moment, where, as described in Section 2.3, time indices are iteratively halved to ﬁnd
candidate change points. Consequently, the time indices being evaluated reduce with each
iteration. Once a candidate change point has been detected (e.g., time point 35), this process
is applied recursively to the child matrices whose boundaries are deﬁned by this new candidate
change point (e.g., time points 70:162). As soon as this search has been exhausted and all
candidate change points have been detected (e.g., time points 35, 70, 136), the reﬁtting,
permutation, and statistical inference procedures are performed (Section 2.4). detect.cps()
saves the output as a list with 3 components:

R> fmrioutput
$rank
[1] 7

$change_points

T

stat_test
35 8.956153e-07
1
2
70 3.987376e-07
3 136 5.344967e-02

$compute_time
Time difference of 12.79541 mins

In the second element of the output list above, each row contains a candidate change point
and its associated p-value. Using a stringent signiﬁcance level of α = 0.001, we can determine
which of the candidates we retain as change points.

< 0.001, 1]

R> finalcpt = fmrioutput$change_points[fmrioutput$change_points[,2]
+
R> finalcpt
[1]

35 70

The change points detected by FaBiSearch occur at time points 35 and 70, which correspond
to changes in the network clustering structure of the particular subject. Even at rest, fMRI
time series data shows evidence of non-stationarity (Delamillieure, Doucet, Mazoyer, Turbelin,
Delcroix, Mellet, Zago, Crivello, Petit, Tzourio-Mazoyer, and Others 2010; Cribben and Yu
2017; Anastasiou et al. 2021 to name just a few) as subjects drift between functional modes
or states of thought and attentiveness which explains the changing network dynamics over
the experiment.

4.2. Estimating stationary networks

For the second scan of the ﬁrst subject of the resting-state fMRI data set in the previous
section, we detected change points at time points t = 35, 70. With two change points, we have

16

Factorized Change Point Detection in R

Figure 4: The adjacency matrices for the three stationary networks in the gordfmri data
set using the clustering based network estimation method. From left to right the adjacency
matrices correspond to stationary networks for time points 1:35, 36:69, and 70:197, respec-
tively. For each adjacency matrix, nodes are numbered 1:333 advancing from left to right in
columns, and from bottom to top in rows. The gordfmri data is processed from the second
scan of the ﬁrst subject from the NYU test-restest resting-state fMRI data set.

three stationary blocks to estimate. We specify these two change points as the changepoints
argument in est.net() so that the data, gordfmri, is segmented appropriately. Given we
identiﬁed an optimal rank of 7 for gordfmri in Section 4.1, we use this as the number of
clusters in each of the stationary blocks. Consequently, we set lambda = 7 and rank = 7 in
est.net(), and save the results in clust.net. We carry over and use finalcpt to specify
the change points for the argument changepoints. From the two methods for estimating
networks in the stationary blocks (Section 2.5), we ﬁrst explore the clustering based method.

R> clust.net = est.net(gordfmri, lambda = rankest, rank = rankest,
+

changepoints = finalcpt)

The output from this is a list with three components, where each component is an adjacency
matrix for each corresponding stationary segment. The components themselves are organized
sequentially based on time indices (e.g., components 1, 2, and 3 correspond to time points
1:35, 36:70, and 71:197, respectively). We visualize the resulting networks in Figure 4 as
adjacency matrices using the heatmap() function, where n is the nth stationary network in
the clust.net list:

R> heatmap(clust.net[[n]], col = grey(c(0.97,0.1)), symm = TRUE,
+

Colv = NA, Rowv = NA, labRow = NA, labCol = NA)

These adjacency matrices show a strong block diagonal structure, since the clustering based
method naturally organizes clusters into separate blocks in each adjacency matrix. Addition-
ally, we can estimate networks from each of these stationary blocks using the thresholding
method (Section 3.4). Again, we use the est.net() function and set rank = 7, however,
we need to determine an appropriate cutoﬀ value, lambda. Typically, this is unknown a pri-
ori and varies based on the estimated consensus matrix. Given the large number of nodes

Journal of Statistical Software

17

(p = 333), there are 55278 possible edges (or upper triangular elements in the adjacency
matrix). We seek to ﬁnd a cutoﬀ which incorporates as much variability as possible (small
lambda), but minimizes the addition of noise and maximizes sparsity for easier interpretability
(large lambda).
One approach is determine an appropriate cutoﬀ value (or the number of edges in the graph),
lambda, is to use a heuristic, such as the elbow method, to ﬁnd the value where increasing
lambda has diminishing marginal returns for improving sparsity. To do so, we ﬁrst deﬁne
lambda as a vector of possible cutoﬀ values, which in this case is a sequence from 0.01 to 0.99
with 0.01 sized steps. Results are saved as a list in thresh.net.

R> lambda = seq(0.01, 0.99, 0.01)
R> thresh.net = est.net(gordfmri, lambda = lambda, rank = rankest,
+

changepoints = finalcpt)

This list however, has two levels. The ﬁrst level has three elements, where each element
corresponds to the stationary network evaluated, just as before. The second level has 99
elements, where each element corresponds to the adjacency matrix at a speciﬁc value of
lambda (e.g., 0.01, 0.02, 0.03,..., 0.99). We then ﬁnd an appropriate cutoﬀ value by examining
a plot of the lambda value against the number of edges (or elements in the upper triangular
part of the adjacency matrix). We save the number of edges in the vector edges as we loop
through diﬀerent values of lambda. For each nth stationary network, we plot the results.

edges = c()
for(i in 1:length(lambda)){

R> par(mfrow = (c(1,3)))
R> for(n in 1:3){
+
+
+
+
+
+
+ }
R> dev.off()

curr.net = thresh.net[[n]][[i]]
edges = c(edges, sum(curr.net[upper.tri(curr.net)]))

}
plot(lambda, edges, cex.lab = 2, cex.axis = 2)

As depicted in Figure 5, the optimal cutoﬀ point is at approximately lambda = 0.4 (element
40 in the second level of the curr.net list), hence we use this for computing the new adjacency
matrix. For the 40th element in the curr.net list, we visualize the adjacency matrices for
the corresponding network (Figure 6). Compared to the clustering method, the thresholding
method provides sparser adjacency matrices. This highlights the main beneﬁt of this approach
compared to the clustering method. It allows for sparsity to be controlled using lambda. Some
block diagonal organization remains, however, compared to the clustering method (Figure 4),
it is less prominent.

R> heatmap(thresh.net[[n]][[40]], col = grey(c(0.97,0.1)), symm = TRUE,
+

Colv = NA, Rowv = NA, labRow = NA, labCol = NA)

18

Factorized Change Point Detection in R

Figure 5: Plots of lambda values against the number of edges (or elements in the upper
triangular part of the adjacency matrices) across stationary networks in the gordfmri data
set. From left to right, the plots correspond to the networks for time points 1:35, 36:70, and
71:197, respectively. The gordfmri data is processed from the second scan of the ﬁrst subject
from the NYU test-restest resting-state fMRI data set.

4.3. 3-dimensional brain network visualization

For visualization, we include a function in the fabisearch package that creates interactive,
brain speciﬁc, 3-dimensional networks (Section 2.5). We use the networks computed in Section
4.2 as an example. We begin by plotting the network of the ﬁrst stationary block of data
(time points 1:35) using the threshold based method (Figure 7, top row). Given the density
in this network, we chose λ = 0.50, which results in a network that is very sparse and has
approximately 200 edges. Since we are using the Gordon et al. (2016) atlas, the rest of the
settings for net.3dplot() are the default values.

R> net.3dplot(thresh.net[[n]][[50]])

Edges between the nodes are added iteratively using a for loop, so depending on the number of
edges to be added, the net.3dplot() function may take some time to incorporate all elements
of the network. In Figure 7 (top row), most connections are concentrated in the “Visual”
and “None” communities, with some dense connections in the “DorsalAttn”, “CinguloOperc”,
and “VentralAttn” communities. Connections to nodes in other communities are also evident,
however, they are not as densely interconnected. We also see the density of connections is
more localized to the anatomical left hemisphere. Furthermore, since most edges appear to
be between the purple (“None”) and yellow (“Visual”) communities, we use net.3dplot to
plot the 3-dimensional networks and allow us to focus on these speciﬁc communities (Figure
7, bottom row). We specify the same node colors to maintain consistency between the plots

Journal of Statistical Software

19

Figure 6: The adjacency matrices for the three stationary networks in the gordfmri data
set using the threshold based network estimation method. From left to right the adjacency
matrices correspond to stationary networks for time points 1:35, 36:70, and 71:197, respec-
tively. For each adjacency matrix, nodes are numbered 1:333 advancing from left to right in
columns, and from bottom to top in rows. The gordfmri is processed from the second scan
of the ﬁrst subject from the NYU test-restest resting-state fMRI data set.

(top and bottom rows of Figure 7), and add ROI labels.

R> communities = c("None", "Visual")
R> colors = c("#FFEB3B", "#673AB7")
R> net.3dplot(thresh.net[[n]][[50]], ROIs = communities,
+

colors = colors)

In Figure 7, we show two angles of the brain network, from a caudal and lateral viewpoint,
respectively. The caudal view allows is to examine inter-hemisphere connections, while the
laternal view allows us to see that most of the connections are in fact within the visual network.
In practice, however, the output of net.3dplot() opens a window for the 3-dimensional plot
using the rgl library (Adler, Murdoch, Suggests, WebGL, OBJ, and OpenGL 2014). This
plot is interactive in that it can be rotated by clicking and dragging the cursor on the plot.
A major advantage of the net.3dplot() function in fabisearch is that we can also use dif-
ferent brain atlases to deﬁne the size and location of the ROIs, and estimate and plot the
corresponding networks. To the best of our knowledge, fabisearch is the ﬁrst R package that
has the ability to visualize an interactive, 3-dimensional, brain speciﬁc network that allows
for various atlases and also for any manually uploaded coordinate inputs. For example, in
addition to the gordatlas, we also include the AALatlas, which is the 90 ROI Automated
Anatomical Labelling (AAL) atlas as deﬁned by Tzourio-Mazoyer et al. (2002) in fabisearch.
In addition, the AALfmri data set in fabisearch is the same as the gordfmri data set, except
it has 90 ROIs instead of 333 ROIs and it uses the AAL atlas parcellation instead of the Gor-
don et al. (2016) atlas parcellation. For the AALfmri data, we use the same detected change
points and re-estimate the network for the ﬁrst stationary time segment, corresponding to
time points 1:35 and save this as AAL.net.

R> data("AALfmri", package = "fabisearch")

20

Factorized Change Point Detection in R

Figure 7: Visualizations of the brain networks of the ﬁrst stationary block of the gordfmri
data, using the net.3dplot() function. Nodes are colored according to community member-
ship, and deﬁned by the Gordon et al. (2016) atlas. The top row includes all the communities
from the Gordon et al. (2016) atlas in the network, whereas the bottom row plots only the
“None” and “Visual” communities in the network. The gordfmri data is processed from the
second scan of the ﬁrst subject from the NYU test-retest resting-state fMRI data set

R> lambda = seq(0.01, 0.99, 0.01)
R> AAL.net = est.net(AALfmri[1:35,], lambda = lambda, rank = 6)

Again, we must ﬁnd an appropriate cutoﬀ for our thresholding of the consensus matrix so we
explore a range of lambda values (Figure 8).

R> edges = c()
R> for(i in 1:length(lambda)){
curr.net = AAL.net[[i]]
+
+
edges = c(edges, sum(curr.net[lower.tri(curr.net)]))
+ }
R> plot(lambda, edges, cex.lab = 1.8, cex.axis = 1.8)

From Figure 8, it is evident that a cutoﬀ value equal to 0.38 for the threshold for the adjacency

Journal of Statistical Software

21

Figure 8: A plot of lambda values against the number of edges (or elements in the upper
triangular part of the adjacency matrices) for the stationary network in the AALfmri data set
for time points 1:35. The AALfmri data is processed from the second scan of the ﬁrst subject
from the NYU test-retest resting-state fMRI data set.

matrix, balances sparsity and interpretability. We plot the adjacency matrix and the 3-
dimensional brain with this cutoﬀ value in Figure 9 (top row) using:

R> heatmap(AAL.net[[38]], col = grey(c(0.97,0.1)), symm = TRUE,
+
R> net.3dplot(AAL.net[[38]], coordROIs = AALatlas)

Colv = NA, Rowv = NA, labRow = NA, labCol = NA)

From Figure 9 (top row), we observe that the network remains relatively dense. Compared
to the ﬁrst stationary network plotted for the gordfmri data set (Figure 7, top row), the
connections between nodes in the AALfmri ﬁrst stationary network appear more diﬀuse. There
is not a clear clustering structure, although it seems that there are more dense connections in
nodes close to the mid-line of the brain. Additionally, the higher density of the left hemisphere
appears to be preserved in this data set as well. We focus in on a subset of nodes by specifying
the node numbers in the net.3dplot() under the ROIs parameter. In this case, we plot the
networks in Figure 9 (bottom row), where we narrow down the relationships to the ﬁrst 30
ROIs (or nodes) using:

R> nodes = 1:30
R> net.3dplot(AAL.net[[38]], coordROIs = AALatlas, ROIs = nodes)

Finally, as mentioned in Section 3.5, net.3dplot() also provides capability for plotting inter-
active brain networks using any cortical atlas (which can be manually uploaded using speciﬁc
coordinate inputs), and hence adjacency matrix, for deﬁning nodes in the network. Lastly,
the visualization function is general; it can also be used on data sets without any change
points.

22

Factorized Change Point Detection in R

Figure 9: Visualizations of the estimated network from the ﬁrst stationary block in the
AALfmri data set, using the threshold based method where lambda = 0.38. The top row,
includes an adjacency matrix, and a 3-dimensional brain plots at diﬀerent angles using the
entire AAL atlas, while the bottom row includes two plots are of the same network but with
only relationships between the ﬁrst 30 ROIs. The AALfmri data is processed from the second
scan of the ﬁrst subject from the NYU test-retest resting-state fMRI data set.

5. Financial Data Example

5.1. Data summary and change point detection

Although the fabisearch package was initially motivated, developed and tested for functional
magnetic resonance imaging (fMRI) time series data, the methodology and package func-
tionality could naturally extend to any multivariate high-dimensional change point detection
problem. We now illustrate such an example.

Another popular area of application for change point detection and network estimation is
ﬁnancial data. Here the objective is to ﬁnd clusters of stocks with strong temporal dependence.
In portfolio management, this is useful in order to spread risk or variability of returns over time
by seeking to minimize dependence between selected components of the portfolio. Naturally,
these dependencies change over time as the general market milieu changes and the companies
themselves adapt. As such, knowing when these changes occur is of high value as it might
signal a time when portfolio allocations should be re-evaluated. To this end, using periodic

Journal of Statistical Software

23

returns from individual stocks over time, we apply the FaBiSearch methodology as described
in Section 2 to the Standard and Poor’s 500 (S&P 500) index (https://www.spglobal.com/
spdji/en/indices/equity/sp-500), which is a market capitalization weighted index of 500
large companies listed on United States stock exchanges. We ﬁrst obtained a list of the 505
ticker symbols (which is greater than the number of companies because some companies have
multiple stock tickers), and then acquired each stock’s publicly available, daily historical price
from Yahoo Finance (https://finance.yahoo.com/) for dates 2018/01/01 to 2021/03/31.
Dates in this range when the stock market was closed were removed from the data set. To
clean the data, we ﬁrst removed any companies which were not publicly listed for the entirety
of our selected time period, thus removing any variables which have missing data. Next, we
standardized the data by calculating the daily log returns over the aforementioned period. To
allow for the use of non-negative matrix factorization (NMF), we added 100 to each entry to
increase the mean of each time series to 100. The fully processed data set has 815 rows/dates,
499 variables/stocks, and is available as the logSP500 data set in the fabisearch R package.
To begin, we inspect the data by calculating summary statistics for, and plotting individual
time series (Figure 10) of four prominent companies included in this index, namely Tesla
Inc (TSLA), Johnson & Johnson (JNJ), JPMorgan Chase & Co. (JPM), and Walt Disney
Company (DIS).

R> data("logSP500", package = "fabisearch")
R> companies = c("TSLA", "JNJ", "JPM", "DIS")
R> columns = colnames(logSP500) %in% companies
R> print(logSP500[1:10, columns])

JPM

JNJ

TSLA

DIS
2018-01-03 101.35288 100.05630 99.35006 100.37610
2018-01-04 99.95933 100.83552 99.44736 99.89045
2018-01-05 101.16552 99.33067 100.16846 99.37655
2018-01-08 100.15435 100.10079 102.87447 98.45363
2018-01-09 102.25866 100.44912 99.45760 99.86150
2018-01-10 99.79798 101.02189 100.02511 99.49626
2018-01-11 100.79597 100.47644 100.32494 101.35214
2018-01-12 100.94112 101.55029 99.60574 101.29637
2018-01-16 101.06352 99.61123 100.42376 98.29807
2018-01-17 100.08850 100.57912 100.88551 101.11678
R> summary(logSP500[, columns])

JNJ

JPM

TSLA

DIS

Min.
: 84.58
1st Qu.: 99.14
Median :100.07
Mean
:100.00
3rd Qu.:100.96
:111.16
Max.

Min.
: 84.19
1st Qu.: 99.15
Median :100.00
Mean
:100.00
3rd Qu.:100.87
:116.07
Max.

Min.
: 88.13
1st Qu.: 98.95
Median : 99.96
Mean
:100.00
3rd Qu.:100.97
:108.86
Max.

Min.
: 85.65
1st Qu.: 99.11
Median : 99.98
Mean
:100.00
3rd Qu.:100.87
:113.76
Max.

R> par(mfrow=c(4,1))
R> for(i in 1:4){
+
+

plot(logSP500[,colnames(logSP500) == companies[i]],

type = "l", cex.lab = 1.5, cex.axis = 1.5, xlab = "Day",

24

+
+ }

Factorized Change Point Detection in R

ylab = companies[i], ylim = c(80, 120))

Figure 10: Plots of the daily adjusted log-returns (y-axis) for companies TSLA, JNJ, JPM,
and DIS in the S&P 500 index for dates 2018/01/01 to 2021/03/31 (x-axis), from the logSP500
data set in the fabisearch R package.

We continue our analysis by applying FaBiSearch to the data in order to detect change points
in the network (or clustering) structure of this multivariate high-dimensional time series. We
utilize the detect.cps() function (as described in Section 4.1), but because of the greater
number of time points in this data set we increase mindist, the minimum distance between
candidate change points, in the detect.cps function, to 100. Additionally, due to the larger
number of nodes, we increase nreps to 150 to increase the power of the statistical inference
test on the candidate change points. Again, we assume the appropriate rank to use for this
data set is unknown a priori, hence we use opt.rank() to determine the appropriate rank.
This is saved as rank, and is then fed into the detect.cps() function. All other arguments for
detect.cps() remain at their default values. The ﬁnal output of the change point detection
procedure are saved as the SP500out object.

R> set.seed(54321)
R> rank = opt.rank(logSP500)
R> SP500out = detect.cps(logSP500, alpha = 0.05, mindist = 100,
+

rank = rank, nreps = 150)

Journal of Statistical Software

25

The corresponding output is, again, a list with three components. Interestingly, the optimal
rank for this data set is lower than that for the fMRI data, indicating a simpler clustering
structure with high dependence amongst components. This is quite plausible given the index
is made up from the same asset class (equities), focuses on the the United States market
exclusively, and is made up of large-capitalization companies only.

R> SP500out
$rank
[1] 4

$change_points

T stat_test
TRUE
FALSE
FALSE
TRUE
TRUE
TRUE

1 115
2 215
3 315
4 460
5 560
6 660

$compute_time
Time difference of 48.32602 mins

Four change points at time points t = 115, 460, 560, 660 were detected and saved in finalcpt.

R> finalcpt = SP500out$change_points$T[SP500out$change_points$stat_test]
R> finalcpt
[1] 115 460 560 660

We now relate these change points to the actual dates, which are stored in logSP500 as the
rownames.

R> rownames(logSP500[finalcpt,])
[1] "2018-06-18" "2019-10-30" "2020-03-25" "2020-08-17"

The four change points correspond to ﬁve unique, macro-level environments which dictate the
underlying clustering and network structure of the stocks in the index. In the ﬁrst segment,
there is relatively steady growth of the index during 2018. The ﬁrst change point charac-
terizes a switch to a higher volatility market environment, in which the S&P 500 dropped
approximately 18% as fear of trade tensions between the US and China began to mount.
The index continued to experience turbulence as the S&P 500 attempted to hit new all time
highs. The point where the index lifted beyond this ceiling corresponds to the second change
point. Afterwards, the recovery was quite strong until in early 2020 when the second major
hit of volatility occurred, coinciding with the beginning of the COVID-19 pandemic. The
end of this volatility is marked by the third change point, after which the index rebounded
sharply, as the index continued to drive higher from the pandemic related lows. During this
time, many tech companies saw accelerated growth from increased web traﬃc and demand

26

Factorized Change Point Detection in R

for technology during stay at home mandates, while the retail, services, and travel indus-
tries remained stagnant. Near the end of 2020 marks the ﬁnal change point, which preceded
a slight change in environment as the market became more selective about valuations and
which stocks could justify their growth. This also coincides with the market testing previous
all time highs, which further added to the general hesitancy about market levels.

5.2. Estimating stationary networks and visualization

We continue our analysis of this data set by estimating the stationary networks for the ﬁve
segments. Given the optimal rank calculated for this data set, we specify rank = 4 in the
est.net() function. We also increase the number of runs to nruns = 100 to improve the
stability of the calculated networks. Again, we specify the changepoints parameter using
the finalcpt vector. We save the output as SP500net.

R> SP500net = est.net(logSP500, rank = rank,
+

lambda = rank, nruns = 50, changepoints = finalcpt)

Given the large number of variables in this data set, we focus on the 15 largest companies
in the S&P 500 index by market capitalization at the time of writing. We deﬁne tickers,
a vector of string values which contains the ticker symbols for these companies, and narrow
down the networks to the relationship between these 15 companies.

R> tickers = c("AAPL", "MSFT", "AMZN", "GOOG", "FB", "TSLA", "BRK-B",
+

"JPM", "JNJ", "NVDA", "UNH", "V", "HD", "PG", "DIS")

The cluster based approach works well for a smaller number of nodes, as the number of edges
is relatively manageable. We use the igraph package to plot the graphs.

G = graph_from_adjacency_matrix(SP500net[[i]][top.15,top.15],

mode = "undirected", diag = FALSE)

R> library(igraph)
R> top.15 = colnames(logSP500) %in% tickers
R> par(mfrow=c(2,2))
R> for(i in 1:length(SP500net)){
+
+
+
+
+
+
+
+
+ }

Sys.sleep(1)

V(G)$color = palette()[components(G)$membership]
plot(G, layout=layout_with_kk, vertex.size=20,

vertex.color="white", vertex.frame.color = "white",
vertex.label.color = V(G)$color, vertex.label.family =
"Helvetica", asp = 0.5)

Figure 11 shows the stationary networks between each pair of change points. There is ev-
idence of a changing clustering and network structure between stationary segments for the
selected 15 companies. At a broad level, the companies became more densely connected
during periods where the index experienced steady, sustainable growth (the ﬁrst and last
stationary segments). For the ﬁrst network (Figure 11, top left), the companies are separated

Journal of Statistical Software

27

Figure 11: The four stationary networks of the top 15 largest S&P 500 companies by market
capitalization from 2018/01/03 to 2021/03/30 estimated using the function est.net(). Each
network is made up of up to 4 clusters, calculated using a rank of 4 for 50 runs, and is labelled
with corresponding date ranges below. Cluster membership is denoted by the vertex label
colors.

into three unique clusters. We see that the big tech companies, outside of Apple (AAPL) are
contained in the black cluster. After the ﬁrst change point however, and as evident in the next
network (Figure 11, top right), the edges re-arrange. The network in general becomes more
sparse, with companies separating into four clusters. The core big tech companies remain
in the black cluster, however Tesla (TSLA) is displaced by AAPL. Disney (DIS), Johnson
& Johnson (JNJ), and United Health Group (UNH) are also separated from the black clus-
ter. After the second change point, the density of edges decreases again, and the individual
associations are greatly changed (Figure 11, middle left). For example, the big tech group
is dissolved, as Microsoft (MSFT), Amazon (AMZN), and AAPL dissociate from the black
cluster. Additionally, MSFT is separate in its own cluster is a behaviour unique only to this
stationary segment. Both of the aforementioned cases are examples where the associations
are out of the norm, or atypical. Furthermore, we see that the general network structure is
much more dispersed, with more smaller clusters dominating the network. This likely due to
the extreme market volatility, wherein companies that typically are not associated with one
another become clustered together in the estimated network. After the third change point,

28

Factorized Change Point Detection in R

the next stationary segment (Figure 11, middle right) corresponds to the sharp rebound in
the index and we see a high concentration of companies into a single cluster. In this segment,
there are four clusters, however the network is particularly dominated by the black cluster
which includes 9 of the 15 companies. The ﬁnal stationary segment (Figure 11, bottom left)
occurs after the fourth and ﬁnal change point, and shows some similarity in structure to the
ﬁrst stationary segment. Here, there are again three clusters albeit the network is heavily
dominated by the black cluster, which includes 11 of the 15 companies.

6. Discussion and Conclusion

The fabisearch R package is a comprehensive software implementation of the FaBiSearch
methodology and can also be used to visualize interactive 3-dimensional brain speciﬁc net-
works. We utilize the NMF package (Gaujoux and Seoighe 2010) as a starting point for
our implementation of non-negative matrix factorization (NMF) in fabisearch, parallelizing
the code to maximize computational eﬃciency. Speciﬁcally, NMF ﬁtments and the reﬁt-
ting procedure in FaBiSearch are parallelized which greatly reduces computational time in
multi-threaded machines.

It is important to note that, in practice, NMF, and consequently FaBiSearch, is sensitive
to the input data set Y . NMF can be considered most simply as an algorithm which seeks
to minimize loss between observed data and the model. A stepwise approach is used to
minimize the objective function, and NMF stops when the diﬀerence in the objective function
between subsequent steps is negligible. Consequently, NMF is sensitive to scale and may
stop prematurely if the absolute diﬀerence between input data points is small. As such, we
recommend re-scaling data to ensure that the standard deviation, σ ≥ 2, if the mean is
approximately equal to 100. Naturally, as µ increases, σ should increase proportionately and
vice versa. If starting with data where µ = 0, it can be scaled by multiplying each entry in Y
by some factor. This is what was performed to scale up σ in the log returns for the logSP500
data set in Section 5. Then, 100 was added to each data entry to ensure that no negative
values remained (new µ = 100) and to make it suitable for use with NMF.
The computational time for functions detect.cps() and est.net() is sensitive to the size
of the input data, and the rank, nruns, nreps, and algtype arguments chosen. Naturally, as
rank, nruns, and nreps increase, so does the computational time. However, for a data set on
the order of 100-200 nodes, and 200-400 time points, most uses of detect.cps() are on the
order of minutes to possibly a couple hours on consumer grade machines, which is reasonable,
given the dimension of the data. Further, the graph estimating function, est.net(), for the
same size of data set executes on the order of seconds to minutes.

NMF, and thus FaBiSearch, is also sensitive to the user speciﬁed input parameters. From the
arguments in detect.cps() and est.net(), inputs rank, nruns, mindist, nreps, and alpha
are the most important. Typically, rank can be accurately assessed using the automated
optimization procedure by specifying rank = "optimal". In general however, we recommend
to overestimating rank if specifying it manually. When specifying nruns it is important to be
mindful of overﬁtting, and for most applications a value between 20-250 is appropriate. The
speciﬁc value which is most appropriate for a data set depends on the number of variables or
columns, but the number of samples or time points is also inﬂuential as well. The value of
mindist should be chosen such that the sample of Y is suﬃciently large to ensure a stable

Journal of Statistical Software

29

NMF ﬁtment. Lastly, nreps and alpha are parameters more well understood in statistics;
general speaking, a larger nreps, and a more stringent and therefore lower value of alpha are
more favourable.
To illustrate how the values of rank and nruns aﬀects the accuracy of FaBiSearch, we provide
a brief sensitivity analysis on a simulated data set. This data set is eﬀectively the same as the
sim2 data set in the fabisearch package, although the number of variables has been increased
to p = 200. More speciﬁcally, data are generated from the multivariate Gaussian distribution
N (0, Σ), with the following structures on Σ:

Σij =






0.75,
1,
0.20,

if i 6= j and i, j are in the same cluster;
if i = j;
otherwise,

Figure 12: A sensitivity analysis on diﬀerent combinations of factorization rank, r and the
number of runs, nruns using a simulated data set. The visualization occurs from two per-
spective plots: true positive (TP) and false positive (FP) rates are shown on the left hand
side and right hand side, respectively. Note the x and y axes are ﬂipped between the two
plots to more clearly see the sharp changes to accuracy before it plateaus. Color scales are
also ﬂipped, however blue is more favourable (higher TP, lower FP) and red is less favourable
(lower TP, higher FP) in both plots.

The length of the time series remains as T = 1 : 200, where the ﬁrst half is characterized Σ.
At t = 100, there is a change point where the node labels are reshuﬄed randomly amongst
the two clusters in Σ. This is then repeated to create a sample of 20 unique instances of this
data set. 50 combinations of rank and nruns were then tested, using values of {2, 3, 4, 5,
6}, and {2, 3, 5, 10, 15, 20, 25, 50, 100, 200}, respectively. For each of these combinations,
true positive (TP) and false positive (FP) rates were calculated for the 20 samples. We deﬁne
a TP as a change point found within 5 time points (t = 100 ± 5), and a FP as a change
point found outside of these bounds (t 6= 100 ± 5). The results are shown in Figure 12 as
two perspective plots for the TP and FP rates. Note the x and y axes are ﬂipped between

30

Factorized Change Point Detection in R

the two plots to more clearly see the sharp changes to accuracy before it plateaus. Color
scales are also ﬂipped, however blue is more favourable (higher TP, lower FP) and red is less
favourable (lower TP, higher FP) in both plots. For both TP and FP rates, there is a steep
improvement by increasing nruns from 2 to 25, and nruns in general plays a key role in the
accuracy of detect.cps(). After this, increasing nruns shows only marginal improvements
and accuracy eﬀectively plateaus. This falls in line with expectations, as only the best and
therefore lowest loss NMF ﬁtment is used when searching for change points. Naturally, as
nruns increases the probability of ﬁnding a good ﬁtment similarly increases. Once enough
random starts have been attempted however, it is unlikely that a better ﬁtment can be found.
Additionally, while rank is also important, it does not contribute as greatly to accuracy in this
data set. Increasing rank at lower values of nruns yields marginally better results. However,
this data set has a relatively simple clustering structure and so for data sets where it is more
complicated rank should play a greater role.
In this paper, we delineate the four main functions included in the fabisearch package, and
apply them to two distinct multivariate high-dimensional time series (a fMRI study, and
the S&P 500 stock index). Further, we show how the functionality of detect.cps() and
est.net(), for change point detection and network estimation respectively, generalizes to any
multivariate time series problem. We also provide speciﬁc functionality to plot interactive
3-dimensional brain networks using the net.3dplot() function, and we emphasize that it
can be used with any cortical atlas (and for any manually uploaded coordinate inputs), and
hence adjacency matrix, for deﬁning nodes in the network. It can also be used on data sets
without any change points. This, in combination with the ability to select sub-networks
and color nodes by community membership, provides great ﬂexibility for displaying elegant
3-dimensional brain networks. Finally, we provide a brief sensitivity analysis on a simulated
data set to characterize how values of rank and nruns aﬀect NMF and change point detection
accuracy using detect.cps().

Computational Details

The results in this paper were obtained using R 4.1.3 with the fabisearch 0.0.4.4 package. R
itself and the fabisearch package are available from the Comprehensive R Archive Network
(CRAN) at https://cran.r-project.org and https://cran.r-project.org/package=
fabisearch, respectively. Computations performed using two Intel Platinum 8160F Sky-
lake at 2.1Ghz (48 cores total) and 187GB of memory.

Acknowledgments

This research was enabled in part by support provided by WestGrid (www.westgrid.ca) and
Compute Canada (www.computecanada.ca). The ﬁrst author was supported by the Natu-
ral Sciences and Engineering Research Council of Canada (NSERC) Alexander Graham Bell
Master’s Scholarship, the Alberta Innovates Graduate Student Scholarships (Alberta Inno-
vates, Alberta Advanced Education), and the Richard B. Stein Studentship (Neuroscience
and Mental Health Institute, University of Alberta). The second author was supported by
the Natural Sciences and Engineering Research Council (Canada) grant RGPIN-2018-06638
and the Xerox Faculty Fellowship (Alberta School of Business).

Journal of Statistical Software

31

References

Adler D, Murdoch MD, Suggests M, WebGL P, OBJ S, OpenGL S (2014). “Package ‘rgl’.”

Anastasiou A, Cribben I, Fryzlewicz P (2021). “Cross-covariance isolate detect: a new change-
point method for estimating dynamic functional connectivity.” Medical Image Analysis, p.
102252. ISSN 1361-8415. doi:https://doi.org/10.1016/j.media.2021.102252. URL
https://www.sciencedirect.com/science/article/pii/S1361841521002978.

Anastasiou A, Cribben I, Fryzlewicz P (2022). “Cross-covariance isolate detect: a new change-
point method for estimating dynamic functional connectivity.” Medical Image Analysis, 75,
102252.

Barnett I, Onnela JP (2016). “Change point detection in correlation networks.” Scientiﬁc

reports, 6, 18893.

Benjamini Y, Hochberg Y (1995). “Controlling the false discovery rate: a practical and
powerful approach to multiple testing.” Journal of the Royal Statistical Society. Series B
(Methodological), pp. 289–300.

Biswal B, Zerrin Yetkin F, Haughton VM, Hyde JS (1995). “Functional connectivity in
the motor cortex of resting human brain using echo-planar mri.” Magnetic Resonance in
Medicine, 34(4), 537–541.

Chen H, Zhang N (2015).

The Annals of
Statistics, 43(1), 139–176. doi:10.1214/14-AOS1269. URL https://doi.org/10.1214/
14-AOS1269.

“Graph-based change-point detection.”

Chen H, Zhang NR, Chu L, Song H (2020). gSeg: Graph-Based Change-Point Detection (g-Segmentation).

URL https://cran.r-project.org/package=gSeg.

Cribben I, Fiecas M (2016). “Functional Connectivity Analyses for fMRI Data.” In WT H Om-
bao MA Lindquist, JAD Aston (eds.), Handbook of Statistical Methods for Brain Signals
and Images. Chapman and Hall - CRC Press.

Cribben I, Haraldsdottir R, Atlas LY, Wager TD, Lindquist MA (2012). “Dynamic connec-
tivity regression: Determining state-related changes in brain connectivity.” NeuroImage,
61(4), 907–920. ISSN 10538119. doi:10.1016/j.neuroimage.2012.03.070. URL http:
//dx.doi.org/10.1016/j.neuroimage.2012.03.070.

Cribben I, Yu Y (2017). “Estimating whole-brain dynamics by using spectral clustering.”
Journal of the Royal Statistical Society: Series C (Applied Statistics), 66(3), 607–627.

Dai M, Zhang Z, Srivastava A (2019). “Discovering common change-point patterns in func-

tional connectivity across subjects.” Medical image analysis, 58, 101532.

Delamillieure P, Doucet G, Mazoyer B, Turbelin MR, Delcroix N, Mellet E, Zago L, Criv-
ello F, Petit L, Tzourio-Mazoyer N, Others (2010). “The resting state questionnaire: an
introspective questionnaire for evaluation of inner experience during the conscious resting
state.” Brain Research Bulletin, 81(6), 565–573.

32

Factorized Change Point Detection in R

Douglas D, Peucker T (1973). “Algorithms for the reduction of the number of points required
to represent a digitized line or its caricature.” Cartographica: The International Journal
for Geographic Information and Geovisualization, 10, 112–122.

Duda RO, Hart PE (1974).
Quarterly, 44(3), 258–259.
620282.

“Pattern Classiﬁcation and Scene Analysis.” The Library
doi:10.1086/620282. URL https://doi.org/10.1086/

Erdman C, Emerson JW (2007). “bcp: An R Package for Performing a Bayesian Analysis
of Change Point Problems.” Journal of Statistical Software, Articles, 23(3), 1–13. ISSN
1548-7660. doi:10.18637/jss.v023.i03. URL https://www.jstatsoft.org/v023/i03.

Frigyesi A, Höglund M (2008). “Non-negative matrix factorization for the analysis of com-
plex gene expression data: Identiﬁcation of clinically relevant tumor subtypes.” Cancer
Informatics, 6(2003), 275–292. ISSN 11769351. doi:10.4137/cin.s606.

Gaujoux R, Seoighe C (2010). “A ﬂexible R package for nonnegative matrix factorization.”

BMC Bioinformatics, 11. ISSN 14712105. doi:10.1186/1471-2105-11-367.

Gordon EM, Laumann TO, Adeyemo B, Huckins JF, Kelley WM, Petersen SE (2016). “Gen-
eration and Evaluation of a Cortical Area Parcellation from Resting-State Correlations.”
Cerebral Cortex, 26(1), 288–303. ISSN 1047-3211. doi:10.1093/cercor/bhu239. URL
http://10.0.4.69/cercor/bhu239https://dx.doi.org/10.1093/cercor/bhu239.

Grundy T (2020). changepoint.geo: Geometrically Inspired Multivariate Changepoint Detection.

URL https://cran.r-project.org/package=changepoint.geo.

Grundy T, Killick R, Mihaylov G (2020).
via a geometrically inspired mapping.”
doi:10.1007/s11222-020-09940-y.
ISSN 0960-3174.
s11222-020-09940-yhttps://dx.doi.org/10.1007/s11222-020-09940-y.

“High-dimensional changepoint detection
Statistics and Computing, 30(4), 1155–1166.
URL http://10.0.3.239/

Honkela T, Duch W, Girolami M, Kaski S (2011). Artiﬁcial Neural Networks and Machine
Learning-ICANN 2011: 21st International Conference on Artiﬁcial Neural Networks, Espoo,
Finland, June 14-17, 2011, Proceedings, Part II, volume 6792. Springer.

Killick R, Eckley IA (2014). “changepoint: An R Package for Changepoint Analysis.” Journal
of Statistical Software, 58(3), 1–19. doi:10.18637/jss.v058.i03. URL https://www.
jstatsoft.org/index.php/jss/article/view/v058i03.

Lai TL, Xing H (2015). Active Risk Management: Financial Models and Statistical Methods.

CRC Press Taylor & Francis Group.

Lamy P, Wiuf C, Ørntoft TF, Andersen CL (2010). “Rseg—an R package to optimize segmen-
tation of SNP array data.” Bioinformatics, 27(3), 419–420. ISSN 1367-4803. doi:10.1093/
bioinformatics/btq668. URL https://doi.org/10.1093/bioinformatics/btq668.

Lee DD, Seung HS (2001).

for Non-negative Matrix Factorization.”
In TK Leen, TG Dietterich, V Tresp (eds.), Advances
Information
Processing Systems 13, pp. 556–562. MIT Press. URL http://papers.nips.cc/paper/
1861-algorithms-for-non-negative-matrix-factorization.pdf.

“Algorithms

in Neural

Journal of Statistical Software

33

Li L, Li J (2020a). “Online Change-Point Detection in High-Dimensional Covariance Structure
with Application to Dynamic Networks.” arXiv pre-print server. doi:None. URL arxiv:
1911.07762https://arxiv.org/abs/1911.07762.

Li L, Li J (2020b). onlineCOV: Online Change Point Detection in High-Dimensional Covariance Structure.

URL https://cran.r-project.org/package=onlineCOV.

Li L, Yang J, Xu Y, Qin Z, Zhang H (2014).

“Documents clustering based on max-
correntropy nonnegative matrix factorization.” Proceedings - International Conference on
ISSN 21601348. doi:10.1109/ICMLC.
Machine Learning and Cybernetics, 2, 850–855.
2014.7009720.

Li T, Ma S (2004). “IFD: Iterative feature and data clustering.” SIAM Proceedings Series,

pp. 472–476. doi:10.1137/1.9781611972740.49.

Londschien M, Kovács S, Bühlmann P (2020). “Change-Point Detection for Graphical Models
in the Presence of Missing Values.” Journal of Computational and Graphical Statistics, pp.
1–12. ISSN 1061-8600. doi:10.1080/10618600.2020.1853549. URL https://doi.org/
10.1080/10618600.2020.1853549.

Meier A, Kirch C, Cho H (2021). “mosum: A Package for Moving Sums in Change-Point
Analysis.” Journal of Statistical Software, Articles, 97(8), 1–42. ISSN 1548-7660. doi:
10.18637/jss.v097.i08. URL https://www.jstatsoft.org/v097/i08.

Meinshausen N, Bühlmann P (2010). “Stability selection.” Journal of the Royal Statistical

Society: Series B (Statistical Methodology), 72(4), 417–473.

Muggeo VMR (2020). cumSeg: Change Point Detection in Genomic Sequences. URL https:

//cran.r-project.org/package=cumSeg.

Ofori-Boateng D, Gel YR, Cribben I (2021). “Nonparametric anomaly detection on time
series of graphs.” Journal of Computational and Graphical Statistics, 30(3), 756–767.

Ogawa S, Lee TM, Kay AR, Tank DW (1990). “Brain magnetic resonance imaging with con-
trast dependent on blood oxygenation.” Proceedings of the National Academy of Sciences,
87(24), 9868–9872.

Ondrus M, Cribben I (2021). fabisearch: Change Point Detection in High-Dimensional Time
Series Networks. R package version 0.0.2.4, URL https://CRAN.R-project.org/package=
fabisearch.

Ondrus M, Olds E, Cribben I (2021). “Factorized Binary Search: change point detection
in the network structure of multivariate high-dimensional time series.” arXiv preprint
arXiv:2103.06347.

Ramer U (1972). “An iterative procedure for the polygonal approximation of plane curves.”
Computer Graphics and Image Processing, 1(3), 244–256. ISSN 0146-664X. doi:https:
//doi.org/10.1016/S0146-664X(72)80017-0. URL http://www.sciencedirect.com/
science/article/pii/S0146664X72800170.

34

Factorized Change Point Detection in R

Ross GJ (2015). “Parametric and Nonparametric Sequential Change Detection in R: The
ISSN 1548-7660.

cpm Package.” Journal of Statistical Software, Articles, 66(3), 1–20.
doi:10.18637/jss.v066.i03. URL https://www.jstatsoft.org/v066/i03.

Seshan VE, Olshen A (2020). DNAcopy: DNA copy number data analysis.

Tartakovsky AG, Polunchenko AS, Sokolov G (2013).

“Eﬃcient Computer Network
Anomaly Detection by Changepoint Detection Methods.”
IEEE Journal of Selected
Topics in Signal Processing, 7(1), 4–11.
ISSN 1932-4553. doi:10.1109/jstsp.2012.
2233713. URL http://10.0.4.85/jstsp.2012.2233713https://dx.doi.org/10.1109/
jstsp.2012.2233713.

Truong C, Oudre L, Vayatis N (2020). “Selective review of oﬄine change point detection

methods.” Signal Processing, 167, 107299.

Tzourio-Mazoyer N, Landeau B, Papathanassiou D, Crivello F, Etard O, Delcroix N, Mazoyer
B, Joliot M (2002). “Automated anatomical labeling of activations in SPM using a macro-
scopic anatomical parcellation of the MNI MRI single-subject brain.” NeuroImage, 15(1),
273–289.

Wang X, Emerson JW (2015). “Bayesian Change Point Analysis of Linear Models on General

Graphs.” Working Paper.

Xiong X, Cribben I (2021a). “Beyond linear dynamic functional connectivity: a vine cop-
ula change point model.” bioRxiv. doi:10.1101/2021.04.25.441254. https://www.
biorxiv.org/content/early/2021/04/26/2021.04.25.441254.full.pdf, URL https:
//www.biorxiv.org/content/early/2021/04/26/2021.04.25.441254.

Xiong X, Cribben I (2021b). vccp: Vine Copula Change Point Detection in Multivariate Time Series.

URL https://cran.r-project.org/package=vccp.

Xu M, Reiss PT, Cribben I (2021). “Generalized reliability based on distances.” Biometrics,

77(1), 258–270.

Zeileis A, Leisch F, Hornik K, Kleiber C (2002). “strucchange: An R Package for Test-
ing for Structural Change in Linear Regression Models.” Journal of Statistical Software,
doi:10.18637/jss.v007.i02. URL https:
Articles, 7(2), 1–38.
//www.jstatsoft.org/v007/i02.

ISSN 1548-7660.

Zhu Y, Cribben I (2018). “Sparse graphical models for functional connectivity networks: best

methods and the autocorrelation issue.” Brain connectivity, 8(3), 139–165.

Journal of Statistical Software

35

Aﬃliation:
Martin Ondrus
Neuroscience and Mental Health Institute
University of Alberta
E-mail: mondrus@ualberta.ca
and
Ivor Cribben
Alberta School of Business, and
Neuroscience and Mental Health Institute
University of Alberta
4-30G Business Building
11203 Saskatchewan Drive NW
Edmonton AB
T6G 2R6
E-mail: cribben@ualberta.ca

Journal of Statistical Software
published by the Foundation for Open Access Statistics

MMMMMM YYYY, Volume VV, Issue II
doi:10.18637/jss.v000.i00

http://www.jstatsoft.org/
http://www.foastat.org/
Submitted: yyyy-mm-dd
Accepted: yyyy-mm-dd

