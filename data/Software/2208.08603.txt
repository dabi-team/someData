An Empirical Evaluation of Competitive
Programming AI: A Case Study of AlphaCode

Sila Lertbanjongngam1, Bodin Chinthanet2, Takashi Ishio2, Raula Gaikovina Kula2,
Pattara Leelaprute1, Bundit Manaskasemsak1, Arnon Rungsawang1, Kenichi Matsumoto2
1Department of Computer Engineering, Faculty of Engineering, Kasetsart University, Bangkok, Thailand
2Graduate School of Science and Technology, Nara Institute of Science and Technology, Nara, Japan
{sila.l, bundit.m, arnon.r}@ku.th, {bodin.ch, ishio, raula-k, matumoto}@is.naist.jp, pattara.l@ku.ac.th

2
2
0
2

g
u
A
6
2

]
E
S
.
s
c
[

2
v
3
0
6
8
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—AlphaCode is a code generation system for assisting
software developers in solving competitive programming prob-
lems using natural language problem descriptions. Despite the
advantages of the code generating system, the open source com-
munity expressed concerns about practicality and data licensing.
However, there is no research investigating generated codes in
terms of code clone and performance. In this paper, we conduct
an empirical study to ﬁnd code similarities and performance dif-
ferences between AlphaCode-generated codes and human codes.
The results show that (i) the generated codes from AlphaCode
are similar to human codes (i.e., the average maximum similarity
score is 0.56) and (ii) the generated code performs on par with
or worse than the human code in terms of execution time and
memory usage. Moreover, AlphaCode tends to generate more
similar codes to humans for low-difﬁculty problems (i.e., four
cases have the exact same codes). It also employs excessive
nested loops and unnecessary variable declarations for high-
difﬁculty problems, which cause low performance regarding our
manual
investigation. The replication package is available at
https:/doi.org/10.5281/zenodo.6820681

Index Terms—code generation, code similarity, code perfor-

mance

I. INTRODUCTION

Over the past few years, Artiﬁcial Intelligence (AI) appli-
cations have become very popular, especially in the software
engineering ﬁeld, as they help software developers to work
faster and more efﬁciently [1]. The examples of tasks that AI
can assist developers in the software development process are
(i) software defect prediction [2, 3], (ii) cost estimation [4, 5],
(iii) task prioritization [6], (iv) expert recommendation [7], (v)
security vulnerability detection [8, 9], and (vi) code generation
[10, 11]. These AI applications rely on massive amounts of
data from software development tools such as version control
systems, issue tracking systems, and continuous integration
and deployment systems (CI/CD).

In the case of the code generation system, AI models are
expected to synthesize new and unseen codes, while they know
existing programs from their training dataset [12]. Recently,
several works focus on evaluating code generation systems
on Codex [10] and GitHub Copilot [13] from security [14]
and correctness aspects [15]. These code generation systems
require a code comment or a short natural language as input
for solving the given task.

AlphaCode, a transformer-based AI code generation sys-
tem developed by DeepMind [11], is currently the state of

the art for competitive programming AI. The advantage of
AlphaCode compared to other code generation systems is that
it can generate source codes from competitive programming
problem descriptions, which usually require an understanding
of algorithms and complex natural languages. However, as
the model behind the AI used human codes for the training
process, concerns about data-related issues were raised in the
open source community, such as data privacy, licensing, and
data extraction attacks [16, 17]. As there is also no research
investigating AlphaCode, it is unclear whether generated codes
are clones of human codes. It is also unclear whether the
performance of the generated codes at which, if deployed,
how many resources (i.e., time and memory) are required to
run the program.

In this paper, we conduct an empirical study to ﬁnd code
similarities and performance differences between AlphaCode-
generated codes and human codes. We deﬁne two following
research questions:

• (RQ1) Are generated codes similar to human codes?
• (RQ2) Can generated codes perform better than human

codes?

We collected 44 samples of valid generated codes in C++ and
Python languages from the AlphaCode ofﬁcial website [18]
that solve 22 problems on Codeforces. We then retrieved
31,736 human codes by using the Codeforces API. For RQ1,
we compare source code similarity between the generated and
human codes using metrics to detect source code reuse [19].
For RQ2, we measure the performance of the codes based on
an existing performance comparison work [20].

The results show that (i) the generated codes from Al-
phaCode are similar to the human codes (i.e., the average
maximum similarity score is 0.56 and 0.50 for C++ and
Python), while the code fragments in the generated code are
comprised of various human codes (i.e., the uniqueness of
3.30% and 8.94% for C++ and Python) and (ii) the generated
code performs on par with or worse than the human code
in terms of execution time and memory usage. Furthermore,
AlphaCode generates code that is more similar to human code
for low-difﬁculty problems (i.e., four cases with the same
code) and employs excessive nested loops and unnecessary
variable declarations (i.e., using long long instead of int,
unused integer list), resulting in poor performance.

 
 
 
 
 
 
TABLE I: Summary information of our dataset.

Summary information of collected generated codes

Detail
C++ language

- # codes
- # line of code
Python language
- # codes
- # lines of code

Mean Median

SD

Total

1
31.772

1
16.318

1
26

1
15.5

1
11.688

1
6.724

22
699

22
359

Summary information of collected human codes

Detail
C++ language

- # codes
- # lines of code
Python language
- # codes
- # lines of code

Mean Median

SD

Total

977
52.117

464
14.274

1000
36.0

461
11.0

102.48
48.740

21,508
1,120,933

352.19
16.990

10,228
145,998

Fig. 1: Example competitive programming problem from
Codeforces (1574A - Regular Bracket Sequences) [21].

The rest of this paper is organized as follows. Section II
describes the background of this research. Section III shows
our analysis method. Section IV describes results of the
analysis. Section V discusses limitations and threats to validity.
Section VI concludes this paper and suggests future work.

II. BACKGROUND

In this section, we brieﬂy explain terms that have been used

throughout the paper.

A. Competitive Programming

Competitive programming is a competition that gives well-
known computer science problems to contestants and asks
them to solve those problems by writing source codes as
quickly as possible [22]. To solve a problem, contestants have
to analyze the description and use their logical, mathematical,
and computer science skills to ﬁnd the optimal solution,
which is usually not directly stated in the description text.
There are several well-known programming competitions that
are annually held and supported by software organizations,
such as Google Code Jam [23], Meta Hacker Cup [24], and
ACM ICPC [25]. The format of competitions can be different
depending on the host; for example, the number of problems,
competition times, and available programming languages. In
this paper, we focus on Codeforces, one of the most popular
competitive programming platforms that has over one million
users and regularly holds a weekly competition [26].

Figure 1 shows the competitive programming problem ex-
ample, called Regular Bracket Sequences, hosted on Code-
forces. The problem description consists of (i) the details of the
problem, and (ii) input and output constraints, which include
the format and the example test case. In this example, the
contestants are asked to generate n different bracket sequences
that can be transformed into a correct expression for each test
case. There are multiple solutions to this problem, but they
have to be executed within limited memory and time.

B. AlphaCode: A Competitive-level AI Code Generation

In February 2022, DeepMind introduced AlphaCode, a
large-scale transformer-based code generation system. The
main target of AlphaCode is to generate code for solving com-
petitive programming problems that require an understanding
of algorithms and complex natural
languages. One of the
highlights of AlphaCode is that it can generate the entire
program from a long natural language description compared
to Codex, a code generation system used in GitHub Copilot
[10, 13], which is capable of generating code for a simple task
with a short solution (e.g., function, API usage).

Several studies focused on the evaluation of the transformer-
based code generation system. Chen et al. [10] evaluated
Codex and found that it has a strong performance for easy
interview problems. Nguyen and Nadi [15] evaluated the
suggested codes of GitHub Copilot with LeetCode and found
that it achieved at most 57% of correctness score. Pearce
et al. [14] found that generated code can introduce the security
vulnerability issue. In the case of AlphaCode, the evaluation
is still limited as only DeepMind provides such information
[11]. They found that AlphaCode achieved an average ranking
within the top 54.3% with over 5,000 human contestants.
Additionally, AlphaCode achieved 20.36% and 7.75% solve
rates (i.e., pass@k metric) in introductory and competition
tasks. However, the understanding of generated codes from
AlphaCode is still unclear especially in terms of the quality of
codes compared to the actual human codes. Hence, this study
empirically evaluates the code similarities and performance
differences between generated codes and human codes, as
described in the following section.

III. EXPERIMENT SETUP

This section presents the dataset and analysis methods to
understand how generated codes are similar to human codes.

A. Dataset

Our dataset consists of (i) the generated codes from Al-
phaCode and (ii) the human codes from Codeforces. From
the AlphaCode ofﬁcial website [18], there are 141 generated

Regular Bracket SequencesExample Input3313Example Output()()()((()))(()())()((()))(())()()(())A bracket sequence is a string containing only characters "(" and ")". A regular bracket sequence is a bracket sequence that can be transformed into a correct arithmetic expression by inserting characters "1" and "+" between the original characters of the sequence. For example, bracket sequences "()()" and "(())" are regular (the resulting expressions are: "(1)+(1)" and "((1+1)+1)"), and ")(", "(" and ")" are not.You are given an integer𝑛. Your goal is to construct and printexactly𝑛different regular bracket sequences of length2𝑛.InputThe first line contains one integer𝑡(1 ≤ 𝑡≤ 50)—the number of test cases.Each test case consists of one line containing one integer𝑛(1 ≤ 𝑛≤ 50).OutputFor each test case, print𝑛lines, each containing a regular bracket sequence of lengthexactly2𝑛. All bracket sequences you output for a testcase should be different (though they may repeat in different test cases). If there are multiple answers, print any of them. It can be shown that it's always possible.codes from 43 different problems written in C++ and Python
languages. However, some of these generated codes do not
successfully solve the problems. Additionally, some problems
have generated code in only one language. In this paper, we
consider only problems that have both C++ and Python gen-
erated codes and those that can solve the problems correctly.
We crawled and collected the 44 generated codes that solve
22 problems written in C++ and Python languages.

To empirically evaluate the generated codes, we retrieved
human codes from Codeforces using a provided API on May
17, 2022. However, we were able to retrieve at most 1,000
submitted codes per language for each problem due to API
limitations. We also collected the difﬁculty score of each
problem. In the end, 21,508 C++ and 10,228 Python human
codes were collected for our experiment. The summary of our
dataset is described in Table I.

B. Analysis for RQ1

TABLE II: An example of similarity value. A bold trigram is
unique to a code fragment.

Code fragment a

Code fragment b

if ( x != y ); x++;

if ( y == x ); y++;

trigrams(a)

trigrams(b)

< ,
, if >, < , if, ( >,
< ,
, if >, < , if, ( >,
<if, (, x >, <(, x, != >,
<if, (, y >, <(, y, == >,
<x, !=, y >, <!=, y, ) >, <y, ==, x >, <==, x, ) >,

<y, ), ; >, <), ;, x >,
<;, x, + >, <x, +, + >,
<+, +, ; >, <+, ;, >,

<x, ), ; >, <), ;, y >,
<;, y, + >, <y, +, + >,
<+, +, ; >, <+, ;, >,

<;,

, >,
21 = 0.238, unique(a, {b}) = 8

<;,

, >,

13 = 0.615

sim(a, b) = 5

To analyze the source code similarity between generated
and human codes, we make a comparison for each problem at
the ﬁle-level granularity. We have employed two source code
similarity metrics that have been used to measure the degree
of source code reuse [19].

sim (f1, f2) =

unique (f, H) =

|trigrams (f1) ∩ trigrams (f2)|
|trigrams (f1) ∪ trigrams (f2)|
(cid:12)
(cid:12)trigrams (f ) \ (cid:83)

h∈H trigrams (h)(cid:12)
(cid:12)

|trigrams (f )|

where trigrams(f ) is a multiset of token trigrams (three
consecutive tokens) extracted from a ﬁle f . Table II shows
example values of the metrics for a pair of code fragments
(i.e., a sample of lines in a ﬁle).

We calculate source code similarity for each pair of gen-
erated and human codes. A higher similarity indicates that a
larger amount of source code could be reused from the human
code. As Juergens et al. [27] reported that different authors
write different source codes, the similarity is expected to be
low.

The uniqueness of a generated code is deﬁned as the ratio of
trigrams that are unique to the generated code. In other words,
it measures the number of trigrams in the generated code

that are never included in human codes. A higher uniqueness
indicates that the generated code includes only its own code.
The uniqueness becomes low if token trigrams in generated
codes are also found in human codes.

C. Analysis for RQ2

To analyze the performance difference between generated
codes and human codes, we implement the system to measure
the execution time and memory usage similar to Leelaprute
et al. [20]. We ﬁrst generate the input data based on the
constraints of Codeforces problems. Due to the limitations of
the Codeforces API, it is impossible to retrieve the largest
input data for each problem. Hence, we use the upper-bond
constraint described in the problem description as the size of
the input in order to simulate the worst-case scenario. We then
compile and execute both generated and human codes by using
our input data. In this case, we use only human codes that are
the most similar to generated codes for each problem. We
repeatedly execute codes 100 times to reduce the threat from
non-deterministic and other factors that affect the execution
time and memory usage. We also set an execution time limit
of ﬁve seconds to avoid the inﬁnity loop case.

In our experiment, we use memory-proﬁler, a Python library
for measuring the amount of memory consumption for each
code execution with the execution timestamp [28]. For com-
piler and interpreter setup, we use the GNU G++20 11.2.0
compiler with -O2 option for C++ code compilation and
CPython 3.9.7 for Python code execution. Our machine uses
an AMD Ryzen Threadripper 3970X CPU and 128 GB of
DDR4 RAM on Ubuntu 20.04 operating system.

To statistically validate the differences between the execu-
tion time and memory usage of generated codes and human
codes, we use the independent sample t-test, which compares
the means between two groups [29]. Our hypothesis is that
“the execution time and memory usage between generated and
human codes are the same or not”. We also measure the effect
size using Cohen’s d, which shows differences based on means
and standard deviations of two groups [30]. The interpretation
is listed as follows: (i) 0 ≤ d < 0.1 as Very small, (ii) 0.1
≤ d < 0.35 as Small, (iii) 0.35 ≤ d < 0.65 as Medium, (iv)
0.65 ≤ d < 0.9 as Large, or (v) d ≥ 0.9 as Very large. In our
experiment, we use NumPy [31], Scipy [32], and researchpy
[33] for the statistical test.

IV. RESULTS

A. (RQ1) Are generated codes similar to human codes?

Similarity between generated and human codes. Figure 2
shows the similarity score between generated codes and human
codes for each problem in C++ and Python. We ﬁnd that,
overall, AlphaCode can generate similar codes to human as
the mean of maximum similarity scores for C++ (Figure 2a)
and Python (Figure 2b) are 0.56 and 0.50 respectively. Only
four cases that generated codes are exactly the same as human
codes for both languages. We also ﬁnd that these four cases
have difﬁculty score at 800, which is one of the lowest score
among all problems in our study.

TABLE IV: Summary statistics and comparison of execution times
(in second) between generated codes and human codes written in
C++. Note that we show |∆| and Cohen’s d if generated codes
perform signiﬁcant difference (p-value < 0.001).

Difﬁculty

AlphaCode
SD

Mean

Human

Mean

SD

|∆|

Cohen’s d

1549A
1549C
1551A
1551B1
1552A
1552B
1553A
1553H
1554A
1554B
1554C
1556A
1557A
1559A
1560A
1561A
1566A
1566D1
1567E
1569A
1573C
1574A

800
1400
800
800
800
1500
800
2900
800
1700
1800
800
800
900
800
800
800
1100
2200
800
1800
800

0.0009
5.821
5.8216
0.0067
0.004
5.8219
0.0019
5.821
5.8199
5.82
5.82
0.0032
0.0914
0.0024
0.001
0.0038
5.8194
0.0271
5.8213
0.0024
0.0291
0.0038

0.0022
0.0069
0.0047
0.0018
0.0006
0.0057
0.0004
0.0038
0.0048
0.0045
0.0052
0.0006
0.0047
0.0027
0.0026
0.0066
0.0041
0.0038
0.0103
0.002
0.0033
0.0017

0.0017
0.0591
0.0133
0.0065
0.0041
0.0463
0.0019
0.0899
0.0589
0.0203
0.009
0.0104
0.0911
0.002
0.0007
0.0034
0.0146
0.0121
1.4461
0.0049
0.0327
0.0021

0.0005
0.0191
0.0027
0.0011
0.0008
0.0017
0.0005
0.0017
0.0025
0.0017
0.0017
0.0016
0.0031
0.0005
0.0005
0.0006
0.0024
0.0018
0.0044
0.0008
0.0036
0.0004

-
5.7619
5.8083
-
-
5.775
-
5.731
5.761
5.799
5.811
0.0072
-
-
-
-
5.8048
0.015
4.3752
0.0025
0.0036
0.0017

-
399.22*
1501.59*
-
-
1377.4*
-
1961.69*
1487.15*
1683.69*
1488.17*
5.74*
-
-
-
-
1722.22*
5.0*
548.34*
1.7*
1.05*
1.33*

TABLE V: Summary statistics and comparison of execution times
(in second) between generated codes and human codes written in
Python. Note that we show |∆| and Cohen’s d if generated codes
perform signiﬁcant difference (p-value < 0.001).

Difﬁculty

AlphaCode
SD

Mean

Human

Mean

SD

|∆|

Cohen’s d

1549A
1549C
1551A
1551B1
1552A
1552B
1553A
1553H
1554A
1554B
1554C
1556A
1557A
1559A
1560A
1561A
1566A
1566D1
1567E
1569A
1573C
1574A

800
1400
800
800
800
1500
800
2900
800
1700
1800
800
800
900
800
800
800
1100
2200
800
1800
800

0.2069
5.8217
0.2271
0.2208
0.211
0.4594
0.2116
5.8183
0.3241
0.2195
5.8167
0.2158
5.8175
0.3236
0.2443
0.3008
0.2296
0.3863
5.8178
0.2228
0.4602
0.2041

0.0067
0.0035
0.0037
0.0052
0.0044
0.0133
0.0067
0.0063
0.0064
0.002
0.0202
0.008
0.0047
0.0113
0.0455
0.0151
0.0078
0.0142
0.0078
0.0044
0.0041
0.0063

0.2082
0.6428
0.2272
0.2203
0.2116
0.3199
0.2092
-
0.327
-
0.2937
0.2297
0.2709
0.2099
0.2472
0.3226
0.2292
0.3776
-
0.214
0.4695
0.205

0.0056
0.0084
0.0084
0.0067
0.004
0.0095
0.0111
-
0.0089
-
0.0363
0.0084
0.0041
0.0053
0.0455
0.0172
0.0075
0.0201
-
0.007
0.0063
0.0101

-
5.1789
-
-
-
0.1395
-
-
-
-
5.523
0.0139
5.5466
0.1137
-
0.0218
-
-
-
0.0088
0.0093
-

-
796.38*
-
-
-
11.99*
-
-
-
-
186.91*
1.7*
1250.18*
12.83*
-
1.34*
-
-
-
1.51*
1.74*
-

our generated input for generated codes and human codes
written in C++ and Python, respectively. From our comparison,
we ﬁnd that 11 out of 22 C++ human codes (50%) and
6 out of 22 (27.27%) for Python signiﬁcantly outperform
generated codes (i.e., less execution times and highlighted in
yellow). We also ﬁnd that 8 out of 22 C++ generated codes
(36.36%) and 5 (22.73%) for Python cannot execute within
5 seconds (i.e., time limit exceeded and highlighted in gray).
On the other hand, only 3 out of 22 generated codes (13.64%)
for both C++ and Python beat human codes with less than
0.01 second difference (i.e., highlighted in green). From our
manual investigation for the time limit exceeded cases, we
ﬁnd that AlphaCode introduced unnecessary nested loops in
the generated codes that are from high-difﬁculty problems.

Comparison of memory usage. Tables VI and VII show the

(a) C++ language

(b) Python language

Fig. 2: Similarity between generated codes and human codes.

TABLE III: Percentage of code fragment uniqueness of gen-
erated codes

Languages Mean Median Min

Max

SD

C++
Python

3.30%
8.94%

2.60%
3.24%

0%
0%

11.36%
3.21%
31.75% 10.66%

Uniqueness of generated code fragments. Table III shows
percentage of uniqueness of generated codes compared to
human codes. We ﬁnd that the code fragments in generated
codes are common in human codes as the mean of uniqueness
for C++ language is 3.30% and 8.93% for Python language.
Considering these two results, it indicates that the AlphaCode
model might not directly clone the training data to solve
the competitive programming problems. However, AlphaCode
generates code that is a mixture of multiple human codes.

Answer to RQ1: Yes, generated codes from Alpha-
Code can be similar to human codes (the average
maximum similarity is 0.56). The code fragments used
by AlphaCode are common across all human codes.

B. (RQ2) Can generated codes perform better than human
codes?

Comparison of execution time. Tables IV and V show
the summary statistics of the execution time when receiving

1549A1549C1551A1551B11552A1552B1553A1553H1554A1554B1554C1556A1557A1559A1560A1561A1566A1566D11567E1569A1573C1574A0.00.20.40.60.81.0Similarity1549A1549C1551A1551B11552A1552B1553A1553H1554A1554B1554C1556A1557A1559A1560A1561A1566A1566D11567E1569A1573C1574A0.00.20.40.60.81.0SimilarityTABLE VI: Summary statistics and comparison of memory usages
(in MB) between generated codes and human codes written in C++.
Note that we show |∆| and Cohen’s d if generated codes perform
signiﬁcant difference (p-value < 0.001).

(i.e., generated list of integer, but did not use it). Similar
to the execution time, these cases usually are high-difﬁculty
problems.

Difﬁculty

Alpha Code

Mean

SD

Human Code
SD
Mean

|∆|

Cohen’s d

1549A
1549C
1551A
1551B1
1552A
1552B
1553A
1553H
1554A
1554B
1554C
1556A
1557A
1559A
1560A
1561A
1566A
1566D1
1567E
1569A
1573C
1574A

800
1400
800
800
800
1500
800
2900
800
1700
1800
800
800
900
800
800
800
1100
2200
800
1800
800

1.5249
14.0974
1.5235
1.5273
1.5762
4.9586
1.5039
5.0258
3.4807
3.5278
8194.9658
1.5289
3.5021
1.5283
1.5284
1.5012
1.5178
1.5177
3.7976
1.5225
13.635
1.5523

0.2347
0.065
0.0386
0.2525
0.3989
0.1488
0.1555
0.055
0.0491
0.075
0.1861
0.241
0.0568
0.257
0.2379
0.1567
0.0381
0.2495
0.0674
0.2495
1.3726
0.3235

1.5765
14.1612
1.5093
1.5041
1.5416
4.2563
1.5303
15.0579
3.7433
3.5279
1.5427
1.5258
3.4982
1.538
1.5052
1.5014
1.5242
1.5085
4.5731
1.507
12.1131
0.5532

0.397
0.0914
0.1562
0.1566
0.3124
0.0446
0.2427
0.0506
0.0508
0.0799
0.1841
0.2391
0.0478
0.2529
0.1563
0.1568
0.2357
0.1564
0.0806
0.1554
1.2187
0.062

-
-
-
-
-
0.7023
-
-
-
-
8193.4231
-
-
-
-
-
-
-
-
-
1.5219
0.9991

-
-
-
-
-
6.36*
-
-
-
-
44044.6*
-
-
-
-
-
-
-
-
-
1.17*
4.27*

TABLE VII: Summary statistics and comparison of memory usages
(in MB) between generated codes and human codes written in Python.
Note that we show |∆| and Cohen’s d if generated codes perform
signiﬁcant difference (p-value < 0.001).

Difﬁculty

AlphaCode

Mean

SD

Human

Mean

SD

|∆|

Cohen’s d

1549A
1549C
1551A
1551B1
1552A
1552B
1553A
1553H
1554A
1554B
1554C
1556A
1557A
1559A
1560A
1561A
1566A
1566D1
1567E
1569A
1573C
1574A

800
1400
800
800
800
1500
800
2900
800
1700
1800
800
800
900
800
800
800
1100
2200
800
1800
800

39.201
83.2158
39.1544
39.1969
39.1747
53.8854
39.1811
110.542
55.1097
50.6909
39.1715
39.4315
50.1959
39.1851
39.1883
39.1639
39.1911
41.4968
62.6948
39.1977
74.2036
39.1903

0.1217
0.1259
0.1226
0.1079
0.122
0.1133
0.1125
0.1253
0.1666
0.1316
0.1258
0.1109
0.1177
0.126
0.1046
0.1255
0.1101
0.1182
0.1079
0.1198
0.1187
0.1124

39.4353
78.5995
39.188
39.1793
39.1938
53.885
39.1672
-
55.1638
-
39.1646
39.1876
58.7845
39.1881
39.1852
39.1803
39.1841
39.1843
-
39.1744
65.5114
39.1848

0.1096
0.1287
0.1253
0.1105
0.1168
0.1344
0.1248
-
0.1779
-
0.124
0.106
0.1173
0.1183
0.1104
0.1248
0.1201
0.1131
-
0.1205
0.1295
0.1237

0.2343
4.6163
-
-
-
-
-
-
-
-
-
0.2439
-
-
-
-
-
2.3125
-
-
8.6922
-

2.01*
36.07*
-
-
-
-
-
-
-
-
-
2.24*
-
-
-
-
-
19.88*
-
-
69.62*
-

summary statistics of the memory usage for generated codes
and human codes written in C++ and Python, respectively. For
the limit exceeded cases (i.e., highlighted in gray), the memory
the total memory to solve each
usages do not represent
problem, but the usages before the process got terminated.
From our comparison, we ﬁnd that 4 out of 22 C++ and
Python human codes (18.18%) signiﬁcantly use a smaller
amount of memory compared to generated codes (highlighted
in yellow). Interestingly,
the C++ generated code for the
problem 1554C uses over 8,000 MB. On the other hand, only
one Python generated code signiﬁcantly beats human codes
with less than 1 MB difference (i.e., highlighted in green). Our
manual investigation for the large memory usage cases shows
that AlphaCode did not optimize the variable type (i.e., use
long long instead of int) and allocated unnecessary variables

Answer to RQ2: No, in terms of execution time and
memory usage, generated codes perform on par with
or worse than human codes. We ﬁnd that AlphaCode
employs excessive nested loops and unnecessary vari-
able declarations to solve the problems.

V. LIMITATIONS AND THREATS TO VALIDITY

Limitations. A key limitation of this work is that we do
not have access to the model of AlphaCode, which limit us
to replicate the entire training and testing process for our
research. We also do not have the access to a complete list of
generated solutions from their ofﬁcial website, which limits
our ability to understand the characteristic of generated codes.
However, the competitive programming dataset for machine
learning is available on GitHub [34], which could be useful
for further investigation of AlphaCode learning process.

Threats to internal validity. The ﬁrst threat is the cor-
rectness of test case for evaluation. Due to the limitations of
Codeforces, we cannot retrieve the edge test cases with the
maximum input size. Instead, we generated those edge test
cases based on the problem descriptions, with a manual check
to minimize a threat. The second threat is the selection of gen-
erated codes. Due to the limited access of AlphaCode model,
we can only collect the randomly selected generated codes
from AlphaCode ofﬁcial website. Even though those codes
were randomly selected, in some cases having performance
issues, the result shows that the similarity to human codes is in
the same trend regardless of their performance and language.
Threats to external validity. The main external threat is the
generality of results to other code generation systems. In this
study, we investigated AlphaCode, which is mainly focused
on generating competitive programming code. As a result,
our ﬁndings may not apply to other types of code generation
systems, such as GitHub Copilot. However, our approaches
can be applied to those systems for further evaluation. Another
threat is the sample size of the analyzed data. We analyzed
only 44 generated codes with 31,736 human codes due to the
dataset limitations. This small sample might not be able to
represent the population. However, we manually checked those
data to ensure the quality and to reduce bias.

VI. CONCLUSIONS AND FUTURE WORKS

This study conducted an empirical analysis to examine the
performance and comparability of the AlphaCode-generated
codes to human codes. Our results show that (i) AlphaCode
generates similar codes to humans, which are comprised of
various human code fragments and (ii) the generated code
performs on par with or worse than the human code in
terms of execution time and memory utilization. These results
indicate that software developers should review the generated

codes as they might contain problematic codes and introduce
performance issues.

In future work, we want to extend the study to a larger
dataset including more languages and problems. While we
have used source code similarity, source code naturalness [35]
might be an interesting metric to understand the generated
codes. We are also interested in other code generation models
such as GitHub Copilot [13].

ACKNOWLEDGMENT

This work has been supported by Japan Society for the
Promotion of Science (JSPS) KAKENHI Grant Numbers
JP20H05706 and JP20K19774.

REFERENCES
[1] C. K. Tantithamthavorn and J. Jiarpakdee, “Explainable ai for
software engineering,” in IEEE/ACM International Conference
on Automated Software Engineering (ASE), 2021, pp. 1–2.
[2] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Mat-
sumoto, “Automated parameter optimization of classiﬁcation
techniques for defect prediction models,” in International
Conference on Software Engineering (ICSE), 2016, pp. 321–
332.

[3] A. Okutan and O. T. Yıldız, “Software defect prediction using
bayesian networks,” Empirical Software Engineering (EMSE),
vol. 19, no. 1, pp. 154–181, 2014.

[4] B. Samson, D. Ellison, and P. Dugard, “Software cost estimation
using an albus perceptron (cmac),” Information and Software
Technology (IST), vol. 39, no. 1, pp. 55–60, 1997.

[5] J. Huang, Y.-F. Li, and M. Xie, “An empirical analysis of data
preprocessing for machine learning-based software cost estima-
tion,” Information and Software Technology (IST), vol. 67, pp.
108–127, 2015.

[6] A. Perini, A. Susi, and P. Avesani, “A machine learn-
ing approach to software requirements prioritization,” IEEE
Transactions on Software Engineering (TSE), vol. 39, no. 4,
pp. 445–461, 2012.

[7] J. Xuan, H. Jiang, Z. Ren, and W. Zou, “Developer prioritization
in bug repositories,” in International Conference on Software
Engineering (ICSE), 2012, pp. 25–35.

[8] Y. Zheng, S. Pujar, B. Lewis, L. Buratti, E. Epstein, B. Yang,
J. Laredo, A. Morari, and Z. Su, “D2a: a dataset built for ai-
based vulnerability detection methods using differential anal-
ysis,” in International Conference on Software Engineering:
Software Engineering in Practice (ICSE-SEIP), 2021, pp. 111–
120.

[9] T. N. Nguyen and R. Choo, “Human-in-the-loop xai-enabled
vulnerability detection,
in
IEEE/ACM International Conference on Automated Software
Engineering (ASE), 2021, pp. 1210–1212.

investigation, and mitigation,”

[10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto,
J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al.,
“Evaluating Large Language Models Trained on Code,” arXiv
preprint arXiv:2107.03374, 2021.

[11] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser,
R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago
et al., “Competition-level code generation with alphacode,”
arXiv preprint arXiv:2203.07814, 2022.

[12] M. Allamanis, “The adverse effects of code duplication
in machine learning models of code,” in ACM SIGPLAN
International Symposium on New Ideas, New Paradigms, and
Reﬂections on Programming and Software (Onward!), 2019, p.
143–153.

[13] “Github copilot - your ai pair programmer,” https://github.com/

features/copilot, (Accessed on 08/04/2022).

[14] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri,
“Asleep at
the keyboard? assessing the security of github
copilot’s code contributions,” in IEEE Symposium on Security
and Privacy (SP), may 2022, pp. 980–994.

[15] N. Nguyen and S. Nadi, “An empirical evaluation of github
copilot’s code suggestions,” in IEEE/ACM Mining Software
Repositories Conference (MSR), 2022, pp. 1–5.

[16] N. Carlini, F. Tram`er, E. Wallace, M. Jagielski, A. Herbert-
Voss, K. Lee, A. Roberts, T. Brown, D. Song, ´U. Erlingsson,
A. Oprea, and C. Raffel, “Extracting training data from large
language models,” in USENIX Security Symposium (USENIX
Security 21), Aug. 2021, pp. 2633–2650.

[17] “Github copilot research recitation — the github blog,” https:

//github.blog/2021-06-30-github-copilot-research-recitation/,
(Accessed on 08/04/2022).

[18] The AlphaCode team, “Alphacode attention visualization,”
on

(Accessed

2022,

2

https://alphacode.deepmind.com/,
07/11/2022).

[19] H. Hata, R. G. Kula, T. Ishio, and C. Treude, “Same File, Differ-
ent Changes: The Potential of Meta-Maintenance on GitHub,”
in International Conference on Software Engineering (ICSE),
May 2021, pp. 773–784.

[20] P. Leelaprute, B. Chinthanet, S. Wattanakriengkrai, R. G. Kula,
P. Jaisri, and T. Ishio, “Does coding in pythonic zen peak per-
formance? preliminary experiments of nine pythonic idioms at
scale,” in International Conference on Program Comprehension
(ICPC), 2022, pp. 575–579.

[21] “Problem - 1574A - Codeforces,” https://codeforces.com/
problemset/problem/1574/A, (Accessed on 08/04/2022).
[22] S. Halim, F. Halim, S. S. Skiena, and M. A. Revilla, Competitive
Lulu Independent Publish Morrisville, NC,

programming 3.
USA, 2013.

[23] “Code

Jam

Competitions,”
https://codingcompetitions.withgoogle.com/codejam, (Accessed
on 08/04/2022).

Google’s

Coding

-

[24] “Meta

Hacker

Cup,”

https://www.facebook.com/

codingcompetitions/hacker-cup, (Accessed on 08/04/2022).
[25] “The ICPC International Collegiate Programming Contest,”

https://icpc.global/, (Accessed on 08/04/2022).

[26] M. Mirzayanov, “Codeforces: Results of 2020 [Annual Re-
port] - Codeforces,” https://codeforces.com/blog/entry/89502,
Apr 2021, (Accessed on 08/04/2022).

[27] E. Juergens, F. Deissenboeck, and B. Hummel, “Code similari-
ties beyond copy & paste,” in European Conference on Software
Maintenance and Reengineering (CSMR), Mar. 2010.

[28] “memory-proﬁler

·

pypi,”

https://pypi.org/project/

memory-proﬁler/, 2022, (Accessed on 08/04/2022).

[29] A. Ross and V. L. Willson, Independent Samples T-Test. Rot-

terdam: SensePublishers, 2017, pp. 13–16.

[30] J. Cohen, Statistical Power Analysis

for

the Behavioral

Sciences. Routledge, 1988.

[31] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers,
P. Virtanen, D. Cournapeau, E. Wieser et al., “Array program-
ming with NumPy,” Nature, vol. 585, no. 7825, pp. 357–362,
Sep. 2020.

[32] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland,
T. Reddy, D. Cournapeau, E. Burovski, P. Peterson et al.,
“SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing
in Python,” Nature Methods, vol. 17, pp. 261–272, 2020.
[33] “researchpy - pypi,” https://pypi.org/project/researchpy/, 2022,

(Accessed on 08/04/2022).

[34] “Deepmind/code contests,” https://github.com/deepmind/code

contests, (Accessed on 08/04/2022).

[35] M. Rahman, D. Palani, and P. C. Rigby, “Natural Software
Revisited,” in International Conference on Software Engineering
(ICSE), May 2019, pp. 37–48.

