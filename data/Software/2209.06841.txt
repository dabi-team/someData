2
2
0
2

p
e
S
4
1

]
h
p
-
t
n
a
u
q
[

1
v
1
4
8
6
0
.
9
0
2
2
:
v
i
X
r
a

The Future of Quantum Computing with Superconducting Qubits

Sergey Bravyi,1 Oliver Dial,1 Jay M. Gambetta,1 Dar´ıo Gil,1 and Zaira Nazario1
IBM Quantum, IBM T.J. Watson Research Center, Yorktown Heights, NY 10598,
USA

(Dated: 16 September 2022)

For the ﬁrst time in history, we are seeing a branching point in computing paradigms with the emergence of
quantum processing units (QPUs). Extracting the full potential of computation and realizing quantum algo-
rithms with a super-polynomial speedup will most likely require major advances in quantum error correction
technology. Meanwhile, achieving a computational advantage in the near term may be possible by combining
multiple QPUs through circuit knitting techniques, improving the quality of solutions through error suppres-
sion and mitigation, and focusing on heuristic versions of quantum algorithms with asymptotic speedups.
For this to happen, the performance of quantum computing hardware needs to improve and software needs
to seamlessly integrate quantum and classical processors together to form a new architecture that we are
calling quantum-centric supercomputing. Long term, we see hardware that exploits qubit connectivity in
higher than 2D topologies to realize more eﬃcient quantum error correcting codes, modular architectures for
scaling QPUs and parallelizing workloads, and software that evolves to make the intricacies of the technology
invisible to the users and realize the goal of ubiquitous, frictionless quantum computing.

I.

INTRODUCTION

The history of computing is that of advances born out
of the need to perform ever more sophisticated calcula-
tions. Increasingly advanced semiconductor manufactur-
ing processes have resulted in faster and more eﬃcient
chips—most recently the 2 nm technology node1—and
special accelerators like the GPU, TPU, and AI proces-
sors2 have allowed more eﬃcient computations on larger
data sets. These advances share the same model of
computation dating back to 1936 with the origins of
the Church-Turing thesis. Now for the ﬁrst time in
history, the ﬁeld of computing has branched with the
emergence of quantum computers, which, when scaled,
promise to implement computations intractable for con-
ventional computers—from modeling quantum mechani-
cal systems3 to linear algebra4, factoring5, search6, and
more.

Unlocking the full potential of quantum processors re-
quires the implementation of computations with a large
number of operations. Since quantum gates are consider-
ably less accurate than their classical counterparts, it is
strongly believed that error correction will be necessary
to realize long computations with millions or billions of
gates. Accordingly, most of quantum computing plat-
forms are designed with the long term goal of realizing
error-corrected quantum circuits. As the noise rate de-
creases below a constant architecture-dependent thresh-
old, an arbitrarily long quantum circuit can be executed
reliably by redundantly encoding each qubit and repeat-
edly measuring parity check operators to detect and cor-
rect errors. However, the number of qubits required to re-
alize error-corrected quantum circuits solving classically
hard problems exceeds the size of systems available today
by several orders of magnitude.

Meanwhile, as the quality and number of qubits in
quantum computers continue to grow, we must be able
to harvest the computational power of quantum circuits

available along the way. For example, a quantum process-
ing unit (QPU) with two-qubit gate ﬁdelity of 99.99%
can implement circuits with a few thousand gates to a
fair degree of reliability without resorting to error correc-
tion. Such circuits are strongly believed to be practically
impossible to simulate classically, even with the help of
modern supercomputers. This suggests the possibility
that the ﬁrst demonstrations of a computational quan-
tum advantage—where a computational task of business
or scientiﬁc relevance can be performed more eﬃciently,
cost-eﬀectively, or accurately using a quantum computer
than with classical computations alone—may be achieved
without or with limited error correction.

Three central questions need to be answered for this
to happen: (1) how to extract useful data from the out-
put of noisy quantum circuits in the weak noise regime,
(2) how to design quantum algorithms based on shal-
low circuits that can potentially solve some classically
hard problems, and (3) how to improve the eﬃciency of
quantum error-correction schemes and use error correc-
tion more sparingly.

These questions and our approach are discussed in de-
tail in Section II. As an illustration, we pick one of the
simplest scientiﬁcally relevant applications of quantum
computers—simulating time evolution of a spin chain
Hamiltonian7. We discuss state-of-the-art quantum algo-
rithms for this problem and highlight the cost of making
time evolution circuits fault-tolerant by encoding each
qubit into the surface code8,9, considered the best ﬁt for
hardware with two-dimensional qubit connectivity. For
problem sizes of practical interest, error correction in-
creases the size of quantum circuits by nearly six orders
of magnitude, making it prohibitively expensive for near-
term QPUs (see Section II A).

Question (1) is approached in Sections II B and II C
through quantum error mitigation10,11 and circuit knit-
ting12–15. These techniques extend the size of quantum
circuits that can be executed reliably on a given QPU
without resorting to error correction. We estimate the

 
 
 
 
 
 
overhead introduced by state-of-the-art error mitigation
methods and discuss recent ideas on how to combine
error correction and mitigation. Circuit knitting tech-
niques exploit structural properties of the simulated sys-
tem, such as geometric locality, to decompose a large
quantum circuit into smaller sub-circuits or combine so-
lutions produced by multiple QPUs.

The classical simulation algorithms used in computa-
tional physics or chemistry are often heuristics and work
well in practice, even though they do not oﬀer rigor-
ous performance guarantees. Thus, it is natural to ask
whether rigorous quantum algorithms designed for simu-
lating time evolution admit less expensive heuristic ver-
sions that are more amenable to near-term QPUs. We
discuss such algorithms in Section II D to address ques-
tion (2).

To approach question (3), we discuss generalizations
of the surface code known as low-density parity check
(LDPC) quantum codes16,17. These codes can pack many
more logical qubits into a given number of physical qubits
such that, as the size of quantum circuits grows, only a
constant fraction of physical qubits is devoted to error
correction (see Section II A for details). These more eﬃ-
cient codes need long-range connections between qubits
embedded in a two-dimensional grid18, but the eﬃciency
beneﬁts are expected to outweigh the long-range connec-
tivity costs.

We then focus on quantum-centric supercomputing,
which is a new architecture for realizing error mitiga-
tion, circuit knitting, and heuristic quantum algorithms
with substantial classical calculations. At the heart of
this architecture is classical and quantum integration and
modularity. We need classical integration at real-time to
enable conditioning quantum circuits on classical compu-
tations (dynamic circuits), at near-time to enable error
mitigation and eventually error correction, and at com-
pile time to enable circuit knitting and advanced com-
piling. We need modulairty to enable scaling and speed-
ing up workﬂows by using parallelization. We ﬁrst start
in Section III by focusing on superconducting computing
hardware and we introduce a series of schemes—which we
denote m, l, c, and t couplers—that give us the amount
of ﬂexibility needed for realizing LDPC codes, scaling
QPUs, and enabling workﬂows that take advantage of lo-
cal operations and classical communication (LOCC) and
In Section IV, we discuss the require-
parallelization.
ments on the quantum stack by deﬁning diﬀerent lay-
ers for integrating classical and quantum computations,
which deﬁne requirements on latency, parallelization, and
the compute instructions. From this, we can deﬁne a
cluster-like architecture that we call quantum-centric su-
percomputer. It consists of many quantum computation
nodes comprised of classical computers, control electron-
ics, and QPUs. A quantum runtime can be executed on
a quantum-centric supercomputer, working in the cloud
or other classical computers to run many quantum run-
times in parallel. Here we propose that a serverless model
should be used so that developers can focus on code and

2

do not have to manage the underlying infrastructure. We
conclude with a high level view from a developer’s/user’s
lens.

This paper oﬀers a perspective of the future of quan-
tum computing focusing on an examination of what it
takes to build and program near-term superconducting
quantum computers and demonstrate their utility. Real-
izing the computational power of these machines requires
the concerted eﬀorts of engineers, physicists, computer
scientists, and software developers. Hardware advances
will raise the bar of quantum computers’ size and ﬁdelity.
Theory and software advances will lower the bar for im-
plementing algorithms and enable new capabilities. As
both bars converge in the next few years, we will start
seeing the ﬁrst practical beneﬁts of quantum computa-
tion.

II. TOWARDS PRACTICALLY USEFUL QUANTUM
CIRCUITS

Although in principle a quantum computer can repro-
duce any calculation performed on conventional classi-
cal hardware, the vast majority of everyday tasks are
not expected to beneﬁt from quantum-mechanical eﬀects.
However, using quantum mechanics to store and process
information can lead to dramatic speedups for certain
carefully selected applications. Of particular interest are
tasks that admit a quantum algorithm with the run-
time scaling as a small constant power of the problem
size n—e.g., as n2 or n3—whereas the best known clas-
sical algorithm solving the problem has runtime grow-
ing faster than any constant power of n—e.g., as 2n or
√
n. We deﬁne runtime as the number of elementary
2
gates in a circuit (or circuits) implementing the algo-
rithm for a given problem instance. As the problem size
n grows, the more favorable scaling of the quantum run-
time quickly compensates for a relatively high cost and
slowness of quantum gates compared with their classical
counterparts. These exponential or, formally speaking,
super-polynomial speedups are fascinating from a purely
theoretical standpoint and provide a compelling practical
reason for advancing quantum technologies.

Known examples of tasks with an exponential quan-
tum speedup include simulation of quantum many-body
systems19, number theoretic problems such as integer
factoring5, solving certain types of linear systems20, es-
timation of Betti numbers used in topological data anal-
ysis21–23, and computing topological invariants of knots
and links24. (We leave aside speedups obtained in the so-
called Quantum RAM model25, for although it appears
to be more powerful than the standard quantum circuit
model, it is unclear whether a Quantum RAM can be
eﬃciently implemented in any real physical system.)

Simulation of quantum many-body systems has re-
ceived the most attention due to its numerous scientiﬁc
and industrial applications, and for being the original
value proposition for quantum computing26. The ground

state and thermal-equilibrium properties of many-body
systems can often be understood, at least qualitatively,
using classical heuristic algorithms such as dynamical
mean-ﬁeld theory (DMFT) or perturbative methods.
However, understanding their behavior far from equi-
librium in the regime governed by coherent dynamics
or performing high-precision ground state simulations
for strongly-interacting electrons—e.g., in the context of
quantum chemistry—is a notoriously hard problem for
classical computers.

As a simple illustration, consider a spin chain com-
posed of n quantum spins (qubits or qudits) with Hamil-
tonian

H =

n−1
(cid:88)

j=1

Hj,j+1,

where Hj,j+1 is a two-spin nearest-neighbor interaction.
The Schr¨odinger equation

i

d|ψ(t)(cid:105)
dt

= H|ψ(t)(cid:105)

governs the coherent time evolution of the system from
some ﬁxed initial state |ψ(0)(cid:105).

Suppose our goal is to compute the expected value of
some local observable on the time-evolved state |ψ(t)(cid:105) =
e−iHt|ψ(0)(cid:105). Such expected values are of great interest
for understanding, among other things, thermalization
mechanisms in closed quantum systems27. Transforming
the time-dependent expected values into the frequency
domain provides valuable information about the excita-
tion spectrum of the system28,29. A slightly modiﬁed ver-
sion of this problem that involves measuring each qubit of
|ψ(t)(cid:105) is known to be BQP-complete30–33, meaning that
it is essentially as hard as simulating a universal quantum
computer.

The known classical algorithms for simulating the co-
herent time evolution of a quantum spin chain have run-
time min (2O(n), 2O(vt)), where v ∼ maxj (cid:107)Hj,j+1(cid:107) is the
Lieb-Robinson velocity, which controls how fast informa-
tion propagates through the system34. For simplicity, we
ignore factors polynomial in n and t. The runtime 2O(n)
can be achieved using a standard state vector simulator
while the runtime 2O(vt) can be achieved by approximat-
ing |ψ(t)(cid:105) with Matrix Product States35,36 or by restrict-
ing the dynamics to a light cone37. In general, the linear
growth of the entanglement entropy with time appears
to be an insurmountable obstacle for classical simulation
algorithms.

Haah, Kothari, et. al. recently found a nearly optimal
quantum algorithm for simulating the time evolution of
spin chain Hamiltonians38 with runtime ˜O(nt), where ˜O
hides factors logarithmic in n, t, and the inverse approx-
imation error. This algorithm works by approximating
the time evolution operator e−iHt by a product of sim-
pler unitaries describing forward and backward time evo-
lution of small blocks of spins of length O(log nt). Assum-
ing that the evolution time t scales as a small constant

3

FIG. 1. Estimated number of CNOT gates required to ap-
proximate the unitary evolution operator e−iHt for the n-
qubit Heisenberg chain with t = n and approximation error
0.001 using randomized k-th order product formulas (k =
1, 4, 6). The presented data is based on empirical estimates of
ref. 39, see Eq. (70) therein, assuming that exponentiating a
single term in the Hamiltonian costs 3 CNOTs.

power of n; e.g., t ∼ n, this constitutes an exponential
quantum speedup.

j=1 (cid:126)σj(cid:126)σj+1 + (cid:80)n

A natural question is what are the minimum quan-
tum resources; i.e., qubit and gate counts, required to
convincingly demonstrate a quantum advantage for sim-
ulating coherent dynamics. Childs, Maslov, et. al. pro-
posed a concrete benchmark problem for this, simulating
the time evolution of the spin-1/2 Heisenberg chain with
n = t = 100 and approximation error 0.0017. The Hamil-
tonian has the form H = (cid:80)n−1
j=1 hjσz
j ,
where hj ∈ [−1, 1] are randomly chosen magnetic ﬁelds.
Fig. 1 shows the gate count estimates for the benchmark
problem obtained by Childs, Ostrander, and Su39, sug-
gesting that about 107 CNOT gates (and a comparable
number of single-qubit gates) are needed. This exceeds
the size of quantum circuits demonstrated experimen-
tally to date by several orders of magnitude. As we move
from simple spin chain models to more practically rel-
evant Hamiltonians, the gate count required to achieve
quantum advantage increases dramatically. For exam-
ple, simulating the active space of molecules involved in
catalysis problems may require about 1011 Toﬀoli gates40.
The only viable path to reliably implementing circuits
with 107 gates or more on noisy quantum hardware is
quantum error correction.

A. Quantum error correction

One reason why conventional classical computers be-
came ubiquitous is their ability to store and process infor-
mation reliably. Small ﬂuctuations of an electric charge
or current in a microchip can be tolerated due to a highly
redundant representation of logical 0 and 1 states by

2030406080100Qubits n106107108CNOT countfirst orderfourth ordersixth ordera collective state of many electrons. Quantum error-
correcting codes provide a similar redundant representa-
tion of quantum states that protects them from certain
types of errors. A single logical qubit can be encoded
into n physical qubits by specifying a pair of orthogonal
n-qubit states |0(cid:105) and |1(cid:105) called logical-0 and logical-1.
A single-qubit state α|0(cid:105) + β|1(cid:105) is encoded by the logical
state α|0(cid:105) + β|1(cid:105). A code has distance d if no operation
aﬀecting fewer than d qubits can distinguish the logical
states |0(cid:105) and |1(cid:105) or map them into each other. More
generally, a code may have k logical qubits encoded into
n physical qubits and the code distance d quantiﬁes how
many physical qubits need to be corrupted before the log-
ical (encoded) state is destroyed. Thus good codes have
a large distance d and a large encoding rate k/n.

Stabilizer-type codes41,42 are by far the most studied
and promising code family. A stabilizer code is deﬁned by
a list of commuting multi-qubit Pauli observables called
stabilizers such that logical states are +1 eigenvectors
of each stabilizer. One can view stabilizers as quantum
analogues of classical parity checks. Syndrome measure-
ments aim to identify stabilizers whose eigenvalue has
ﬂipped due to errors. The eigenvalue of each stabilizer
is repeatedly measured and the result—known as the er-
ror syndrome—is sent to a classical decoding algorithm.
Assuming that the number of faulty qubits and gates is
suﬃciently small, the error syndrome provides enough in-
formation to identify the error (modulo stabilizers). The
decoder can then output the operation that needs to be
applied to recover the original logical state.

Most of the codes designed for quantum computing
are of the LDPC type16,43,44, meaning that each stabi-
lizer acts on only a small number of qubits and each qubit
participates in a small number of stabilizers, where small
means a constant independent of the code size. The main
advantage of quantum LDPC codes is that the syndrome
measurement can be performed with a simple constant-
depth quantum circuit. This ensures that the syndrome
information can be collected frequently enough to cope
with the accumulation of errors. Furthermore, errors in-
troduced by the syndrome measurement circuit itself are
suﬃciently benign since the circuit can propagate errors
only within a “light cone” of constant size.

A code must satisfy several requirements to have appli-
cations in quantum computing. First, it must have a high
enough error threshold—the maximum level of hardware
noise that it can tolerate. If the error rate is below the
threshold, the lifetime of logical qubit(s) can be made ar-
bitrarily long by choosing a large enough code distance.
Otherwise, errors can accumulate faster than the code
can correct them and logical qubits can become even less
reliable than the constituent physical qubits. Second, one
needs a fast decoding algorithm to perform error correc-
tion in real time as the quantum computation proceeds.
This may be challenging since the decoding problem for
general stabilizer codes is known to be NP-hard in the
worst case45–47. Third, one must be able to compute on
the logical qubits without compromising the protection

4

oﬀered by the code.
In the sub-threshold regime, one
must be able to realize arbitrarily precise logical gates
from some universal gate set by choosing a large enough
code distance.

The 2D surface code8,9 has so far been considered
an uncontested leader in terms of the error thresh-
old—close to 1% for the commonly studied depolarizing
noise48–50—yet has two important shortcomings. First,
allocating a roughly d×d patch of physical qubits for each
logical qubit incurs a large overhead. Unfortunately, it
was shown51 that any 2D stabilizer code has encoding
rate k/n = O(1/d2) which vanishes for large code dis-
tance. This means that as one increases the degree of
protection oﬀered by the surface code, quantiﬁed by the
code distance d, its encoding rate approaches zero. That
is, as the size of quantum circuits grows, the vast ma-
jority of physical qubits are devoted to error correction.
This is a known fundamental limitation of all quantum
codes that can be realized locally in the 2D geometry.

√

To make error correction more practical and minimize
qubit overhead, codes with a large encoding rate k/n
are preferable. For example, quantum LDPC codes can
achieve a constant encoding rate independent of the code
size44. In fact, the encoding rate can be arbitrarily close
to one16. A recent breakthrough result52 demonstrated
the existence of so-called good quantum LDPC codes
that combine a constant encoding rate k/n (which can
be close to 1/2) and a linear distance d ≥ cn for some
constant c > 0. For comparison, the 2D surface code
has an asymptotically vanishing encoding rate and has
distance at most
n. Certain LDPC codes have a favor-
able property known as single-shot error correction53,54.
They provide a highly redundant set of low-weight Pauli
observables (known as gauge operators) that can be mea-
sured to obtain the error syndrome more eﬃciently. This
reduces the number of syndrome measurement cycles per
logical gate from O(d) to O(1) and hence enables very
fast logical gates. The syndrome measurement circuit
for a quantum LDPC code requires a qubit connectivity
dictated by the structure of stabilizers, i.e., one must be
able to couple qubits that participate in the same stabi-
lizer. Known examples of LDPC codes with a single-shot
error correction require 3D or 4D geometry53–55.

The second shortcoming of the surface code is the diﬃ-
culty of implementing a computationally universal set of
logical gates56. The surface code and its variations such
as the honeycomb code57 or folded surface code58 oﬀer
a low-overhead implementation of logical Cliﬀord gates
such as CNOT, Hadamard H, and phase shift S. These
gates can be realized by altering the pattern of stabilizers
measured at each time step using the code deformation
method. However, Cliﬀord gates are not computationally
universal on their own. A common strategy for achiev-
ing universality is based on preparation of logical ancil-
√
lary states (|0(cid:105) + eiπ/4|1(cid:105))/
2 known as magic states. A
magic state is equivalent (modulo Cliﬀord operations) to
a single-qubit gate T = diag(1, eiπ/4). The Cliﬀord+T
gate set is universal and has a rich algebraic structure

enabling eﬃcient and nearly-optimal compiling of quan-
tum algorithms59,60. Unfortunately, the overhead for
distilling high-ﬁdelity magic states is prohibitively large.
O’Gorman and Campbell61 performed a careful examina-
tion of available distillation methods and their overhead,
considering the implementation of a logical Cliﬀord+T
circuit of size N with an overall ﬁdelity of 90%. Assum-
ing a physical error rate of 10−3, the following ﬁtting
formulas were found for the space-time volume ((physi-
cal qubits) × (syndrome measurement cycles)) associated
with a single logical gate:

Logical gate Physical space-time volume

CNOT
T -gate

1610 + 45 (log10 N )2.77
3.13 + 3220 (log10 N )3.20

The space-time volume roughly quantiﬁes the number of
physical gates required to implement a single logical gate.
As an example, consider the Heisenberg benchmark
problem described above with 100 logical qubits. The
desired time evolution operator can be approximated us-
ing about 107 CNOTs and single-qubit gates (see ﬁg. 1).
However, each single-qubit gate needs to be compiled us-
ing the logical-level gate set {H, S, T }. In total, this re-
quires roughly 109 T -gates and a comparable number of
Cliﬀord gates7. Accordingly, the physical space-time vol-
umes of a single logical CNOT and T -gate are roughly
2 × 104 and 4 × 106, respectively. (In fact, this underesti-
mates the error correction overhead since the Heisenberg
benchmark problem requires logical circuit ﬁdelity 0.999
rather than 0.9, as considered in ref. 61.)

The large overhead associated with logical non-Cliﬀord
gates may rule out the near-term implementation of
error-corrected quantum circuits, even if fully function-
ing logical qubits based on the surface code become avail-
able soon. There have been several strategies proposed
recently for reducing this overhead, including high-yield
magic state distillation methods62,63, better strategies for
preparing “raw” noisy magic states that reduce the re-
quired number of distillation rounds64, and better sur-
face code implementations of distillation circuits65–68.
A recent breakthrough result by Benjamin Brown69
shows how to realize a logical non-Cliﬀord gate CCZ
(controlled-controlled-Z) in the 2D surface code archi-
tecture without resorting to state distillation. This ap-
proach relies on the observation that a 3D version of the
surface code enables an easy (transversal) implementa-
tion of a logical CCZ and a clever embedding of the 3D
surface code into a 2+1 dimensional space-time. It re-
mains to be seen whether this method is competitive
compared with magic state distillation.

B. Error mitigation

Although error correction is vital for realizing large-
scale quantum algorithms with great computational
power, it may be overkill for small or medium size com-

5

putations. A limited form of correction for shallow quan-
tum circuits can be achieved by combining the out-
comes of multiple noisy quantum experiments in a way
that cancels the contribution of noise to the quantity of
interest10,11. These methods, collectively known as error
mitigation, are well suited for the QPUs available today
because they introduce little to no overhead in terms of
the number of qubits and only a minor overhead in terms
of extra gates. However, error mitigation comes at the
cost of an increased number of circuits (experiments) that
need to be executed.
In general, this will result in an
exponential overhead; however, the base of the exponent
can be made close to one with improvements in hardware
and control methods, and each experiment can be run in
parallel. Furthermore, known error mitigation methods
apply only to a restricted class of quantum algorithms
that use the output state of a quantum circuit to esti-
mate the expected value of observables.

Probabilistic error cancellation (PEC)10,70 aims to ap-
proximate an ideal quantum circuit via a weighted sum of
noisy circuits that can be implemented on a given quan-
tum computer. The weights assigned to each noisy circuit
can be computed analytically if the noise in the system
is suﬃciently well characterized or learned by mitigating
errors on a training set of circuits that can be eﬃciently
simulated classically71. We expect that the adoption of
PEC will grow due to the recent theoretical and exper-
imental advances in quantum noise metrology72–74. For
example, ref. 74 shows how to model the action of noise
associated with a single layer of two-qubit gates by a
Markovian dynamics with correlated Pauli errors. This
model can be described by a collection of single-qubit
and two-qubit Pauli errors P1, . . . , Pm and the associ-
ated error rate parameters λ1, . . . , λm ≥ 0 such that the
combined noise channel acting on a quantum register has
the form Λ(ρ) = exp [L](ρ), where L is a Lindblad gen-
erator, L(ρ) = (cid:80)m
i − ρ). The unknown error
rates λi can be learned to within several digits of preci-
sion by repeating the chosen layer of gates many times
and measuring the decay of suitable observables74. The
error mitigation overhead (as measured by the number of
circuit repetitions) per layer of gates scales as γ2, where

i=1 λi(PiρP †

γ = exp

(cid:33)
.

λi

(cid:32)
2

m
(cid:88)

i=1

For a circuit composed of d > 1 layers, the error rates
λi may be layer-dependent and have to be learned sepa-
rately for each layer. As observed in ref. 74, this model
can approximate the actual hardware noise very well us-
ing only m = O(n) elementary Pauli errors Pi supported
on edges of the qubit connectivity graph, where n is the
total number of qubits in the circuit. In general, the run-
time for getting a noise-free estimate will depend on the
circuit implemented and the noise model used.

A large class of quantum algorithms that can beneﬁt
from PEC is based on the so-called hardware-eﬃcient
circuits75. A depth-d hardware-eﬃcient circuit con-

6

to scale up to 27 qubits and still reconstruct observables.
Whether this method can be combined with PEC, which
gives an unbiased estimation, remains an open question.
More general (non-Markovian) noise can be mitigated
using the virtual distillation technique78,79.
It works
by combining two copies of a noisy output state ρ in a
way that enables measurements of observables on a state
ρ2/Tr(ρ2). Assuming that ρ has most of its weight on
the ideal output state, virtual distillation can quadrati-
cally suppress the contributions of errors. However, this
method introduces at least a factor of two overhead in
the number of qubits and gates. A review of the exist-
ing error mitigation proposals can be found in Endo, et.
al.80.

We anticipate error mitigation to continue to be rele-
vant when error-corrected QPUs with a hundred or more
logical qubits become available. As discussed in Section
II A, the ﬁrst generation of error-corrected quantum chips
based on 2D stabilizer codes may not be able to execute
universal computations. Such QPUs are likely to oﬀer
only high-ﬁdelity Cliﬀord gates such as the Hadamard or
CNOT, which can all be eﬃciently simulated on a classi-
cal computer. Meanwhile, logical non-Cliﬀord gates such
as the T -gate may remain out of reach due to the need
to perform magic state distillation. This leads to the
interesting possibility of combining error correction and
mitigation. A concrete proposal by Piveteau, et al.81
leverages the ability to realize noisy logical T -gates with
ﬁdelity comparable to or exceeding that of physical (un-
encoded) gates. Applying error mitigation protocols at
the logical level to cancel errors introduced by noisy T -
gates enables one to simulate universal logical circuits
without resorting to state distillation. This may consid-
erably reduce the hardware requirements for achieving
a quantum advantage. However, error mitigation comes
at the cost of an increased number of circuit executions.
Assuming a physical gate ﬁdelity of 99.9% and a bud-
get of 1, 000 circuit executions, Piveteau, et al.81 esti-
mate that logical Cliﬀord+T circuits with about 2,000
T -gates can be realized reliably. This is far beyond the
limit of existing classical algorithms that can simulate
Cliﬀord+T circuits with about 50 T -gates82,83. Similar
ideas for combining error correction and mitigation are
discussed in refs. 84 and 85.

C. Circuit Knitting

We can extend the scope of near-term hardware to
compensate for other shortcomings such as a limited
number of qubits or qubit connectivity by using circuit
knitting techniques. This refers to the process of sim-
ulating small quantum circuits on a quantum computer
and stitching their results into an estimation of the out-
come of a larger quantum circuit. As was the case with
error mitigation, known circuit knitting techniques apply
to a restricted class of quantum algorithms that aim to
estimate the expected value of observables.

FIG. 2. Runtime scaling (number of circuit instances) needed
to error mitigate 100 and 1000 Trotter steps in a circuit of 100
qubits and layers of non-overlapping two-qubit gates, each
gate aﬀected by a local depolarizing two-qubit error. The
red dotted line identiﬁes 100 million circuits, the daily limit
assuming a repetition rate of 1 ms. So far the only architec-
tures that have achieved this speed are solid-state based. The
number of circuit instances dramatically decreases with slight
improvements in the error rate of the physical gates.

sists of d layers of two-qubit gates such that all gates
within the same layer are non-overlapping and couple
qubits that are nearest-neighbors in the QPU connec-
tivity graph. Denoting the average error rate per qubit
¯λ = (1/n) (cid:80)m
i=1 λi, averaged over all d layers, the overall
PEC overhead scales as (¯γ)dn, where ¯γ = exp (4¯λ). This
allows a simple formula for estimating the runtime, J,
for a noise-free estimate from a quantum circuit of depth
d and width n to be

J = d(¯γ)dnβ,

(1)

where β is the average time to run a single layer of the
circuit. One can view β as a measure of the “speed” and
¯γ as a hardware-dependent parameter that quantiﬁes the
average “quality” of gates across the entire QPU.

For a spin chain of n = 100 qubits, the size of our
benchmark problem, ﬁg. 2 shows the number of circuit
instances that need to be sampled to perform PEC for
100 and 1000 Trotter steps using the decomposition in ﬁg.
3A of ref. 74. Current hardware runs of up to 108 circuits
daily (red dashed line) and error rates of 10−3 have been
demonstrated. Hence, we anticipate that with a cou-
ple orders of magnitude improvement this becomes pos-
sible. Furthermore, this runtime can be further reduced
with the quantum-centric supercomputing architecture
that allows parallelized execution of quantum circuits.

We can also measure the quantity of interest at several
diﬀerent values of the noise rate and perform an extrapo-
lation to the zero-noise limit10,11,76. This method cancels
the leading-order noise contribution as long as the noise is
weak and Markovian. Unlike PEC, this method is biased
and heuristic but may require fewer circuits for the re-
construction. This method was recently demonstrated77

The most well-known example is circuit cutting12–15.
In this method, a large quantum circuit is approximated
by a weighted sum of circuits consisting of small iso-
lated sub-circuits. Each sub-circuit can be executed sep-
arately on a small QPU. The overhead introduced by
this method (as measured by the number of circuit rep-
etitions) scales exponentially with the number of two-
qubit gates or qubit wires that need to be cut in order
to achieve the desired partition of the circuit. Surpris-
ingly, it was recently shown that the circuit cutting over-
head can be substantially reduced by running the iso-
lated sub-circuits in parallel using non-interacting QPUs
that can only exchange classical data86. This approach
requires hardware capable of implementing dynamic cir-
cuits1, where the control electronics is extended to in-
clude independent QPUs.

A second example is entanglement forging87, where ei-
ther an entangled variational state is decomposed into
a weighted sum of product states or the entanglement
between a pair of qubit registers is converted into time-
like correlations within a single register88. The overhead
of this method typically scales exponentially with the
amount of entanglement across the chosen partition of
the system.

A third example, closely related to circuit knitting,
uses embedding methods to decompose the simulation
of a large quantum many-body system into smaller sub-
systems that can be simulated individually on a QPU.
The interactions between subsystems are accounted for
by introducing an eﬀective bath that could be either a
classical environment or another small quantum system.
The decomposition of the original system and the op-
timization of the bath parameters are performed on a
classical computer that can exchange classical data with
the QPU. Well-known examples of quantum embedding
methods that build on their classical counterparts are
dynamical mean-ﬁeld theory89–91, density-matrix embed-
ding92–94, and density-functional embedding95.

D. Heuristic quantum algorithms

Heuristic quantum algorithms can be employed near-
term to solve classical optimization96, machine learn-
ing97, and quantum simulation98 problems.
These
into two categories—algorithms that use kernel
fall
methods97 and variational quantum algorithms (VQA).
Quantum kernel methods have also been found that lead
to provable speedups99 and expand to a class of kernels
for data with group structure100. For VQA, the basic pro-
posal is appealingly simple: an experimentally-controlled
trial state is used as variational wavefunction to minimize

7

the expected energy of a given quantum Hamiltonian or
a classical cost function encoding the problem of interest.
The trial state is usually deﬁned as the output state of
a shallow quantum circuit. Rotation angles that deﬁne
individual gates serve as variational parameters. These
parameters are adjusted via a classical feedback loop to
optimize the chosen cost function.

At present, there is no mathematical proof that VQA
can outperform classical algorithms in any task. In fact,
it is known that VQA based on suﬃciently shallow (con-
stant depth) variational circuits with 2D or 3D qubit
connectivity can be eﬃciently simulated on a classical
computer101,102. This rules out a quantum advantage.
Meanwhile, the performance of VQA based on deep vari-
ational circuits is severely degraded by noise103. How-
ever, as the error rates of QPUs decrease, we should be
able to execute VQA in the intermediate regime where
quantum circuits are already hard to simulate classically
but the eﬀect of noise can still be mitigated.

As a concrete example, let us discuss possible appli-
cations of VQA to the problem of simulating coherent
time evolution of quantum spin chains. It is commonly
believed38 that approximating the time evolution oper-
ator e−iHt for a Hamiltonian H describing a 1D chain
of n qubits requires a quantum circuit of size scaling at
least linearly with the space-time volume nt. Meanwhile,
the best known rigorous quantum algorithms based on
product formulas39 or Lieb-Robinson bounds38 require
circuits of size O(n2t) or O(nt · polylog(nt)), respectively.
A natural question is whether VQA can reduce the cir-
cuit size to what is believed to be optimal, that is, linear
in nt. If this is indeed the case, a QPU with gate ﬁdelity
around 99.99% may be able to solve the classically hard
problem instances described above with space-time vol-
ume nt ∼ 104 using existing error mitigation techniques.
Variational quantum time evolution (VarQTE) algo-
rithms, pioneered by Li and Benjamin11, could be an
alternative to simulate the time evolution of these classi-
cally hard instances given near-term noisy QPUs. These
algorithms aim to approximate the time-evolved state
|ψ(t)(cid:105) = e−iHt|ψ(0)(cid:105) by a time-dependent variational
ansatz |φ(θ)(cid:105) = U (θ)|0n(cid:105), where U (θ) is a parameter-
ized quantum circuit with a ﬁxed layout of gates and
θ = θ(t) is a vector of variational parameters. The ini-
tial state |ψ(0)(cid:105) is assumed to be suﬃciently simple so
that the variational ansatz for |ψ(0)(cid:105) is easy to ﬁnd. The
goal is to ﬁnd a function θ(t) such that the variational
state |φ(θ(t))(cid:105) approximates the time evolved state |ψ(t)(cid:105)
for all t in the chosen time interval. As shown in ref.
11, the desired function θ(t) can be eﬃciently computed
using the stationary-action principle with a Lagrangian
L(t) = (cid:104)φ(θ(t))|d/dt + iH|φ(θ(t))(cid:105). This yields a ﬁrst-
order diﬀerential equation98 (cid:80)

˙θq = Vp where

q Mp,q

1 Dynamic circuits are computational circuits that combine quan-
tum and classical operations, using the outcome of classical com-
putations to adapt subsequent quantum operations. For more
information, see section IV.

Mp,q = Im((cid:104)∂pφ(θ)|∂qφ(θ)(cid:105)),

Vp = −Re((cid:104)∂pφ(θ)|H|φ(θ)(cid:105)),

and ∂p ≡ ∂
. As shown in [11], the entries of M and
∂θp
V can be eﬃciently estimated on a quantum computer.
A comprehensive review of VarQTE algorithms can be
found in refs. 98 and 104.

The fact that VarQTE algorithms are heuristics and
therefore lack rigorous performance guarantees raises the
question of how to validate them. This becomes par-
ticularly important for large problem sizes where veri-
fying a solution of the problem on a classical computer
becomes impractical. Ref. 105 recently developed a ver-
sion of VarQTE based on McLachlan’s variational princi-
ple that comes with eﬃciently computable bounds on the
distance between the exact time-evolved state |ψ(t)(cid:105) and
the approximate variational state found by the VarQTE
algorithms. Thus, although VarQTE lacks a rigorous jus-
tiﬁcation, one may be able to obtain a posteriori bounds
on its approximation error for some speciﬁc problems of
practical interest.

E. Summary

To summarize, the Heisenberg chain example illus-
trates what we believe are general guidelines for designing
near-term quantum algorithms.

First, our best chance of attaining a quantum advan-
tage is by focusing on problems that admit an exponen-
tial (super-polynomial) quantum speedup. Even though
a quantum algorithm that achieves such speedup with
formal proof may be out of reach for near-term hard-
ware, its mere existence serves as compelling evidence
that quantum-mechanical eﬀects such as interference or
entanglement are beneﬁcial for solving the chosen prob-
lem.

Second, the only known way to realize large-scale quan-
tum algorithms relies on quantum error-correcting codes.
The existing techniques based on the surface code are
not satisfactory due their poor encoding rate and high
cost of logical non-Cliﬀord gates. Addressing these short-
comings may require advances in quantum coding theory
such as developing high-threshold fault-tolerant protocols
based on quantum LDPC codes and improving the qubit
connectivity of QPUs beyond the 2D lattice. Supple-
menting error correction with cheaper alternatives such
as error mitigation and circuit knitting may provide a
more scalable way of implementing high-ﬁdelity quantum
circuits.

Third, near-term quantum advantage should be pos-
sible by exploring less expensive, possibly heuristic ver-
sions of the algorithm considered. Those heuristic quan-
tum algorithms lack rigorous performance guarantees,
but they may be able to certify the quality of a solu-
tion a posteriori and oﬀer a way to tackle problems that
cannot be simulated classically.

We believe these general guidelines deﬁne the future
of quantum computing theory and will guide us to im-
portant demonstrations of its beneﬁts for the solution of
scientiﬁcally important problems in the next few years.

8

III. THE PATH TO LARGE QUANTUM SYSTEMS

The perspective above leads to a challenge in quantum
hardware. We believe there will be near-term advan-
tage using a mixture of error mitigation, circuit knitting
and heuristic algorithms. On a longer time frame, par-
tially error-corrected systems will become critical to run-
ning more advanced applications and further down the
line, fault-tolerant systems running on not-as-yet fully
explored LDPC codes with non-local checks will be key.
The ﬁrst steps for all of these approaches are the same:
we need hardware with more qubits capable of higher ﬁ-
delity operations. We need tight integration of fast clas-
sical computation to handle the high run-rates of circuits
needed for error mitigation and circuit knitting, and the
classical overhead of the error correction algorithm after-
wards. This drives us to identify a hardware path that
starts with the early heuristic small quantum circuits and
grows until reaching an error-corrected computer.

A. Cycles of Learning

The ﬁrst step in this path is to build systems able to
demonstrate near-term advantage with error mitigation
and limited forms of error correction. Just a few years
ago, QPU sizes were limited by control electronics cost
and availability, I/O space, quality of control software,
and a problem referred to as “breaking the plane”106,
i.e., routing microwave control and readout lines to qubits
in the center of dense arrays. Today, solutions to these
direct barriers to scaling have been demonstrated, which
has allowed us to lift qubit counts beyond 100—above the
threshold where quantum systems become intractably
diﬃcult to simulate classically and examples of quantum
advantage become possible. The next major milestones
are (1) increasing the ﬁdelity of QPUs enough to allow
exploration of quantum circuits for near-term quantum
advantage with limited error correction and (2) improv-
ing qubit connectivity beyond 2D—either through modi-
ﬁed gates, sparse connections with non-trivial topologies,
and/or increasing the number of layers for quantum sig-
nals in 3D integration—to enable the longer term explo-
ration of eﬃcient non-2D LDPC error-correction codes.
These developments are both required for our longer term
vision, but can be pursued in parallel.

Work on improving the quality of quantum systems by
improving gate ﬁdelities involves many cycles of learn-
ing, trying coupling schemes, process changes, and in-
novations in controlling coupling and crosstalk. Scal-
ing this work to large QPUs capable of demonstrating
quantum advantage, and ultimately to the extreme sys-
tem scales we anticipate in the distant future, involves
integrating diﬀerent technologies with enough reliability
and skill to make size be limited by cost and need, not
by technological capability. This adds challenges in re-
liability, predictability, and manufacturability of QPUs
while continuing to incorporate improved technologies

9

FIG. 3. An example of a scheme that allows breaking the plane for signal delivery compatible with the integration of hundreds
of qubits. It is composed of technologies adapted from conventional CMOS processing.

into these complex systems. Meanwhile, the increased
development, fabrication, and test times for larger sys-
tems creates a lag in cycles of innovation that must be
overcome.

The manufacturing cycle time increases with QPU so-
phistication. Many simple transmon QPUs require just a
single level of lithography and can be easily fabricated in
a day or two. Even the 5- and 16-qubit QPUs that were
IBM’s initial external cloud quantum systems involved
only two lithography steps and took a week to fabricate.
Compare this to more advanced packaging schemes like
those at MIT Lincoln Laboratory107–109 or IBM’s newer
“Eagle” QPUs (ﬁg. 3), which involve dozens of lithog-
raphy steps and slow process steps, and take months to
build at a research-style facility with one-of-a-kind tools.
This increased cycle time makes it harder to reach the ﬁ-
delities and coherence times needed as well as debug the
manufacturing and assembly for reliable QPU yield.

Reliability in semiconductor manufacturing is not a
new problem. In general, among the unique component
challenges faced in building a scaled machine, the conven-
tional semiconductor technologies integrated on chip are
the most well studied. Incorporating them in supercon-
ducting technologies is more a matter of ensuring that the
associated processes are compatible with each other than
inventing new approaches. However, the rapid growth of
volume we anticipate being needed is a major challenge.
Many failure modes in superconducting quantum sys-
tems are not detectable until the QPUs are cooled to
their operating temperature, sub-100 mK. This is a se-
vere bottleneck that renders in-line test (where a device
sub-component is tested for key metrics before the QPU
build ﬁnishes) and process feed-forward (where future
process steps are modiﬁed to correct for small deviations
in early steps and stabilize total device performance) dif-
ﬁcult or impossible. There are exceptions where it is pos-
sible to tightly correlate an easy measurement at room
for ex-
temperature with ultimate QPU performance:
ample, resistance measurements of Josephson junctions
can accurately predict their critical currents and hence,
the frequency of qubits made with them—a key param-

eter in ﬁxed frequency systems. We can take advan-
tage of these statistical correlations wherever they exist
for rapid progress in parts of our process110 or in post-
process tuning111. However, reliably establishing these
correlations requires measuring hundreds or thousands
of devices, a nontrivial feat.

Absent these correlations, we can use simpliﬁed test ve-
hicles; for example, rather than using the entire compli-
cated signal delivery stack when trying to improve qubit
coherence, we can use a simpliﬁed device designed to ob-
tain good statistics and fast processing112. Still, identi-
fying speciﬁc steps leading to increased coherence is non-
trivial. It is rarely possible to change just one parameter
in materials processing. Changing a metal in a qubit
may also change etch parameters, chemicals compatible
with the metal for subsequent processing, and even al-
lowed temperature ranges113. Once an improved process
is found, it is hard to identify exactly which steps were
critical vs. simply expedient.

We must gather suﬃcient statistics when performing
materials research for the results be meaningful and pro-
vide enough certainty114. We should carefully document
process splits wherever relevant, and we should publish
changes in materials processes that lead to neutral or
even negative results, not just just publish highly suc-
cessful work.

Similar diﬃculties occur in non-material based re-
search on devices. Some gates work well between pairs
of qubits yet exhibit strong couplings that make them
unsuitable for larger QPUs or compromise single-qubit
performance. Three- and four-qubit experiments are no
longer challenging from a technical or budgetary perspec-
tive. To be relevant to larger QPUs, research needs to
move away from two-qubit demos, especially hero exper-
iments between a single pair of qubits in which many
critical defects can be masked by luck.

A mixture of long cycle-time complex devices and short
cycle-time test vehicles for sub-process development and
quantum operations is key to continuing improvements in
the quality of QPUs and provides a recipe for continued
R&D contributions as the largest QPUs begin to exceed

the capabilities of smaller groups and labs. Nonethe-
less, reductions in long cycle times are needed. Some
of this will come naturally—ﬁrst-of-a-kind processes and
QPUs usually take longer as they tend to include ex-
tra steps, inspections, and in-line tests that, while sug-
gested by general best practices, may not be necessary.
While counterproductive from a cost viewpoint, building
the “same” QPU repeatedly to iron out manufacturing
problems and speed up cycles of innovation will likely be
a successful strategy for the largest QPUs with the most
complex fabrication ﬂows.

B. Supporting Hardware

Scaling to larger systems also involves scaling classical
control hardware and the input/output (I/O) chain in
and out of the cryostat. This I/O chain, while still need-
ing substantial customization for the exact QPU being
controlled, consists of high volumes of somewhat more
conventional devices; for example, isolators, ampliﬁers,
scaled signal delivery systems, and more exotic replace-
ments such as non-ferrite isolators and quantum limited
ampliﬁers that may oﬀer performance, cost, or size im-
provements. These components have enormous potential
for being shared between various groups pursuing quan-
tum computing, and in some instances can be purchased
commercially already. However, assembling these sys-
tems at the scale required today, let alone a few years
time, requires a high volume cryogenic test capability
that does not currently exist in the quantum ecosystem,
creating a short-term need for vertically-integrated man-
ufacturing of quantum systems. The challenge here is es-
tablishing a vendor and test ecosystem capable of scaled,
low-cost production—a challenge made diﬃcult by the
fact that the demand is somewhat speculative.

There are also one-oﬀ components per system; for ex-
ample, each quantum computer we deploy only requires
a single dilution refrigerator, or in many cases a frac-
tion thereof. The dilution refrigerator manufacturer ef-
fectively acts as a systems integrator for cryo-coolers,
wiring solutions, pumping systems, and even some ac-
tive electronics. Maintaining the ﬂexibility we need to
change quickly as the systems scale will be most easily
attainable if we can standardize many of these interfaces
so that, for example, moving to a more scalable cooling
technology at 4K doesn’t require redesigning the entire
refrigeration infrastructure.

Currently, each group building large QPUs has their
own bespoke control hardware. Given the radically dif-
ferent control paradigms and requirements115–118, it is
unlikely that the analog front-ends of these systems could
ever be shared. However, there is a common need for
sequencing logic (branching, local and non-local condi-
tionals, looping) at low-cost and low-power for all types
of quantum computers, not just solid-state. These will
likely need to be built into a custom processor—an Appli-
cation Speciﬁc Integrated Circuit or ASIC—as we scale to

10

thousands of qubits and beyond. On top of this, the soft-
ware that translates a quantum circuit into the low-level
representation of this control hardware is becoming in-
creasingly complex and expensive to produce. Reducing
cost favors a common control platform with customized
analog front ends. Open-speciﬁcation control protocols
like OpenQASM3119 are already paving the way for this
transformation.

C. Classical parallelization of quantum processors

Reaching near-term quantum advantage will require
taking advantage of techniques like circuit knitting and
error mitigation that eﬀectively stretch the capabilities of
QPUs—trading oﬀ additional circuit executions to emu-
late more qubits or higher ﬁdelities. These problems can
be pleasingly parallel, where individual circuits can ex-
ecute totally independently on multiple QPUs, or may
beneﬁt from the ability to perform classical communi-
cation between these circuits that span multiple QPUs.
Introduction of control hardware that is able to run mul-
tiple QPUs as if they were a single QPU with shared
classical logic, or split a single QPU into multiple virtual
QPUs to allow classical parallelization of quantum work-
loads is an important near-term technology for stretching
this advantage to the limit. Longer term, these technolo-
gies will play a critical enabling role as we begin to build
quantum systems that span mutliple chips and multiple
cryostats, i.e., modular quantum systems.

D. Modularity

The introduction of modular quantum systems will
be key to bootstrapping ourselves from near-term quan-
tum advantage towards long-term error-corrected quan-
tum systems. These are systems with repeating unit cells
that can be replaced if defective, with quantum links be-
tween the chips to entangle unit cells or perform remote
gates. This approach simpliﬁes QPU design and test, and
allows us to scale quantum systems at will.

In the near term, given limited or no error correction,
the unit cells will require high-bandwidth and high ﬁ-
delity links to connect them—there is not enough time
to use complex protocols such as entanglement distilla-
tion. The simplest proposals to accomplish this extend
quantum busses oﬀ chip, allowing the same gates between
distant chips as on a single processor120,121. This “dense
modularity”, which we denote m, eﬀectively extends the
chip size. This requires linking adjacent chips with ultra
low loss, low cross-talk lines that are short enough to be
eﬀectively single-mode—the distance between chips has
to be of the order of the distance between qubits on a
single chip. Several technologies from classical computa-
tional hardware may be adaptable to this problem but
adding the ﬂexibility to replace individual units will re-
quire other alternatives122.

11

(a)p type modularity for classical parallelization of QPUs

(b)Dense modularity m and on-chip non-local couplers c for LDPC
codes for creating a single QPU from multiple chips

(c)Long-range l type modularity to enable quantum parallelization
of multiple QPUs

(d)l, m, p schemes can be combined to extend the scale of
hardware to thousands of qubits.

(e)t type modularity involves microwave-to-optical transduction to
link QPUs in diﬀerent dilution refrigerators.

FIG. 4. Beyond classical parallelization of QPUs, shown in (a), long-range quantum connections carry a high penalty in gate
speed and ﬁdelity. As shown in (b)-(e), a high ﬁdelity, large quantum system will likely involve three levels of modularity—a
very short-range modularity m that allows breaking a QPU into multiple chips with minimal cost in gate speed and ﬁdelity, a
longer range connection l for use within a single cryogenic environment to both get around I/O bottlenecks and allow non-trivial
topologies or routing, and a very long-range optical “quantum network” t to allow nearby QPUs to work together as a single
quantum computational node (QCN). We will also need on-chip non-local couplers c as shown in (b) for the exploration of
LDPC codes. In this ﬁgure, pink lines represent quantum communication and purple lines represent classical communication.

Type
p
m
l
c
t

Description
Real-time classical communication
Short range, high speed, chip-to-chip
Meter-range, microwave, cryogenic
On-chip non-local couplers
Optical, room-temperature links

Use
Classical parallelization of QPUs
Extend eﬀective size of QPUs
Escape I/O bottlenecks, enabling multi-QPUs
Non-planar error-correcting code
Ad-hoc quantum networking

TABLE I. Types of modularity in a long-term scalable quantum system

12

The high density of qubits in this “dense modular-
ity” creates a spatial bottleneck for classical I/O and
cooling. Proposals to ameliorate this near term include
the development of high-density connectors and cables
to route classical signals on and oﬀ the chip123,124, and
the addition of time- and frequency-domain multiplexing
of controls. A longer term approach to address this is
to improve qubit connectivity through the use of a modi-
ﬁed gate performed over a long conventional cable125–127,
called l modularity. Beyond allowing us to escape con-
trol and cooling bottlenecks, these long-range couplers
enable the realization of non-2D topologies, thereby not
only reducing the average distance between qubits but
also opening the door to the exploration of more eﬃ-
cient non-2D LDPC error correction codes128. Develop-
ing these long-range couplers thus not only allows us to
scale our near-term systems, but begins to form the basis
for how to build quantum systems with mulitple QPUs.
The technologies that enable both dense modularity
and long-range couplers, once developed and optimized,
will ultimately be ported back into the qubit chip to en-
able non-local, non-2D connectivity. These on-chip non-
local c couplers will ultimately allow implementation of
high-rate LDPC codes, bringing our long-term visions to
completion.

Finally, connecting multiple quantum computers in an
ad-hoc way will allow us to create larger systems as
needed. In this “quantum networking” approach, the sig-
nals are typically envisioned to leave the dilution refriger-
ator, enabled by long-term technological advancements in
microwave-to-optical transduction using photonic t links
between diﬀerent fridges.

With these four forms of modularity, we can redeﬁne

“scale” for a quantum system by

n = ([(q m) l]t) p

where n is the number of qubits in the entire modular
and parallelized quantum system. The system is com-
prised of QPUs made from m chips, each QPU having
q × m qubits. The QPUs can be connected with l t quan-
tum channels (quantum parallelization), with l of them
being microwave connections and t optical connections.
Finally, to enable things like circuit cutting and speed-
ing up error mitigation, each of these multi-chip QPUs
can support classical communication, allowing p classical
parallelizations.

A practical quantum computer will likely feature all

ﬁve types of modularity—classical parallelization, dense
chip-to-chip extension of 2D lattices of qubits (m), sparse
connections with non-trivial topology within a dilution
refrigerator (l), non-local on-chip couplings for error cor-
rection (c), and long-range fridge-to-fridge quantum net-
working (t) (Table I). The optimal characteristic size of
each level of modularity is an open question. The individ-
ual “chip-to-chip” modules will still be made as large as
possible, maximizing ﬁdelity and connection bandwidth.
Performing calculations on a system like this with multi-
ple tiers of connectivity is still a matter of research and
development129,130.

Modularity needs to happen not just at the scale of
the QPU, but at all levels of the system. Modular clas-
sical control systems allow for easy subsystem testing,
replacement, and assembly. It’s much easier to build a
test infrastructure for a large number of small modules
each year than a single, re-workable monolith. The same
can be said of refrigeration, with the added beneﬁt that
shipping and deploying monolithic large refrigeration sys-
tems is impractical. A large number of our current fail-
ure points come in I/O and signal delivery, so modular
solutions where sub-assemblies can be swapped out are
essential. The challenge here is moving the replaceable
unit from a single unit (a cable) to a larger unit (a ﬂexible
ribbon cable or other cable assembly).

While the jury is still out on module size and other
hardware details, what is certain is that the utility of
any quantum computer is determined by its ability to
solve useful problems with a quantum advantage while its
adoption relies on the former plus our ability to separate
its use from the intricacies of its hardware and physics-
level operation. Ultimately, the power provided by the
hardware is accessed through software that must enable
ﬂexible, easy, intuitive programming of the machines.

IV. THE QUANTUM STACK

For quantum computing to succeed in changing what it
means to compute, we need to change the architecture of
computing. Quantum computing is not going to replace
classical computing but rather become an essential part
of it. We see the future of computing being a quantum-
centric supercomputer where QPUs, CPUs, and GPUs all
work together to accelerate computations. In integrating
classical and quantum computation, it is important to

13

FIG. 5. Circuits can be represented at various levels. Unitary blocks represent circuits from libraries. These can be decomposed
into parameterized circuits using the universal set of gates. Parameterized physical circuits use the physical gates supported
by the hardware, while scheduled circuits specify timing, calibrations, and pulse shapes.

identify (1) latency, (2) parallelism (both quantum and
classical), and (3) what instructions should be run on
quantum vs. classical processors. These points deﬁne
diﬀerent layers of classical and quantum integration.

Before we go into the stack, we need to redeﬁne a quan-
tum circuit. Here we deﬁne a quantum circuit as follows:

A quantum circuit is a computational routine consisting

of coherent quantum operations on quantum data, such
as qubits, and concurrent (or real-time) classical com-
It is an ordered sequence of quantum gates,
putation.
measurements, and resets, which may be conditioned on
and use data from the real-time classical computation. If
it contains conditioned operations, we refer to it is as a
dynamic circuit. It can be represented at diﬀerent levels
of detail, from deﬁning abstract unitary operations down
to setting the precise timing and scheduling of physical
operations.

14

This

is general enough to represent

the circuit
model131, the measurement model132, and the adiabatic
model133 of computation, and special routines such as
teleportation. Furthermore, it can represent the circuit
at various levels: unitary (unitary block that could rep-
resent circuit libraries such as quantum phase estima-
tion, classical functions, etc.), standard decomposition
(reduced to a universal set of gates or expressing the clas-
sical functions as reversible gates), parameterized phys-
ical circuits (using the physical gates supported by the
hardware, possibly including ancilla qubits not used in
the circuit, or parameters that are easy to update in
real-time), and scheduled circuits (complete timing in-
formation, calibrated gates, or gates with assigned pulse
shape) (see ﬁg. 5). OpenQASM119 is an example inter-
mediate representation for this extended quantum circuit
and can represent each of these various abstractions.

With this extended quantum circuit deﬁnition, it is
possible to deﬁne a software stack. Fig. 6 shows a high
level view of the stack, where we have deﬁned four impor-
tant layers: dynamic circuits, quantum runtime, quan-
tum serverless, and software applications. At the low-
est level, the software needs to focus on executing the
circuit. At this level, the circuit is represented by con-
troller binaries that will be very dependent on the super-
conducting qubit hardware, supported conditional oper-
ations and logic, and the control electronics used. It will
require control hardware that can move data with low
latency between diﬀerent components while maintain-
ing tight synchronization. For superconducting qubits,
real-time classical communication will require a latency
of ∼100 nanoseconds. To achieve this latency, the con-
trollers will be located very close to the QPU. Today, the
controllers are built using FPGAs to provide the ﬂexibil-
ity needed, but as we proceed to larger numbers of qubits
and more advanced conditional logic, we will need ASICs
or even cold CMOS.

We refer to the next level up as the quantum runtime
layer. This is the core quantum computing layer. In the
most general form, we expect a quantum computer to
run quantum circuits and generate non-classical proba-
bility distributions at their outputs. Consequently, much
of the workloads are sampling from or estimating prop-
erties of distributions. The quantum runtime thus needs
to include at least two primitive programs: the sampler
and the estimator. The sampler collects samples from
a quantum circuit to reconstruct a quasi-probability dis-

FIG. 6. The quantum software stack is comprised of four
layers, each targeting the most eﬃcient execution of jobs at
diﬀerent levels of detail. The bottom layer focuses on the ex-
ecution of quantum circuits. Above it, the quantum runtime
eﬃciently integrates classical and quantum computations, ex-
ecutes primitive programs, and implements error mitigation
or correction. The next layer up (quantum serverless) pro-
vides the seamless programming environment that delivers
integrated classical and quantum computations through the
cloud without burdening developers with infrastructure man-
agement. Finally, the top layer allows users to deﬁne work-
ﬂows and develop software applications.

tribution of the output. The estimator allows users to
eﬃciently calculate expectation values of observables.

The circuit sent to the runtime would be a parameter-
ized physical circuit. The software would perform a run-
time compilation and process the results before returning
the corrected outcome. The runtime compilation would
update the parameters, add error suppression techniques
such as dynamical decoupling, perform time-scheduling
and gate/operation parallelization, and generate the con-
troller code. It would also process the results with error
mitigation techniques, and in the future, error correc-
tion. Both today’s error mitigation and tomorrow’s er-
ror correction will place strong demands on the classical
computing needed inside these primitive programs. The
circuit execution time could be as low as 100 microsec-
onds (maybe even 1 microsecond for error correction),
which is not possible over the cloud. It will need to be
installed as part of the quantum computer. Fortunately,
error mitigation is pleasingly parallel, thus using multi-
ple QPUs to run a primitive will allow the execution to
be split and done in parallel.

At the third level, we imagine software that can com-
bine advanced classical calculations with quantum calcu-
lations. As described earlier in this paper, introducing
classical computing can enable ideas such as circuit knit-
ing. Here we need to be able to call quantum primitive
programs as well as perform classical calculations such as
circuit partitions. We call this a workﬂow (ﬁg. 7 shows
examples of workﬂows for circuit knitting). We refer to
quantum serverless as the software architecture and tool-
ing that supports this in a way that allows developers to

15

FIG. 7. Example of a quantum serverless architecture integrating quantum and classical computations. Quantum runtimes
are illustrated by estimator primitives. Cloud computing is illustrated by general classical computing. Specialized classical
computing such as high precision computing (HPC) or graphics processing units (GPUs) could be integrated into the serverless
architecture. In circuit cutting, a larger circuit is split into many smaller circuits using a specialized classical computer. For each
of the smaller circuits, an estimator primitive is executed (E1, · · ·, EN ) and if needed, a classical computing routine could be
used to condition future circuits on the results of previous estimators. The process can be repeated as needed. In entanglement
forging, a 2N-qubit wavefunction is decomposed into a larger number of N-qubit circuits. The entanglement synthesis may need
to be oﬄoaded to specialized classical processors. For each N-qubit circuit, an estimator EN is executed and combined to give
the global outcome. This process could be repeated if used in a variational algorithm. Quantum embedding separates sub-parts
of a problem that can be simulated classically from those computationally most costly and requiring quantum computations.
A specialized classical computer could be used to condition the problem on previous outcomes. The quantum simulations
employ estimators EN running on QPUs. The estimators can condition quantum circuits on previous outcomes with classical
calculations run on the general classical processors. Collectively, this set of tools allows larger systems to be simulated with
higher accuracy.

focus only on code and not on the classical infrastruc-
ture. Along with circuit knitting, this layer will also allow
advanced circuit compiling that could include synthesis,
layout and routing, and optimization—all of which are
parts of the circuit reduction that should happen before
sending the circuit to execute.

Finally, at the highest level of abstraction, the com-
puting platform must allow users to eﬃciently develop
software applications. These applications may need ac-
cess to data and to resources not needed by the quantum
computation itself but needed to provide the user an an-
swer to a more general problem.

Each layer of the software stack we just described
brings diﬀerent classical computing requirements to
quantum computing and deﬁnes a diﬀerent set of needs
for diﬀerent developers. Quantum computing needs to
enable at least three diﬀerent types of developers: ker-
nel, algorithm, and model developers. Each developer
creates the software, tools, and libraries that feed the
layers above, thereby increasing the reach of quantum
computing.

The kernel developer focuses on making quantum cir-
cuits run with high quality and speed on quantum hard-
ware. This includes integrating error suppression, error

mitigation, and eventually, error correction into a run-
time environment that returns a simpliﬁed application
programming interface (API) to the next layer.

The algorithm developer combines quantum runtime
with classical computing,
implements circuit knitting,
and builds heuristic quantum algorithms and circuit li-
braries. The purpose is to enable quantum advantage.
Finally, as we demonstrate examples of quantum advan-
tage, the model developer will be able to build software
applications to ﬁnd useful solutions to complex problems
in their speciﬁc domain, enabling enterprises to get value
from quantum computing. Fig. 8 summarizes the types
of developers addressed by each layer of the software
stack and the time scales involved depending on the type
of job being executed and how close to the hardware each
developer is working.

In putting all of this together and scaling to what
we call a quantum-centric supercomputer, we do not see
quantum computing integrating with classical computing
as a monolithic architecture. Instead, ﬁg. 9 illustrates an
architecture for this integration as a cluster of quantum
computational nodes coupled to classical computing or-
chestration. The darker the color, the closer the classical
and quantum nodes must be located to reduce latency.

16

FIG. 8. The time scales and resources involved in quantum computing depend on the needs of the diﬀerent types of developers
and the level of abstraction at which they work. Quantum researchers and kernel developers work closer to the hardware while
model developers require the highest level of software abstraction.

V. CONCLUSION

In conclusion, we have charted how we believe that
quantum advantage in some scientiﬁcally relevant prob-
lems can be achieved in the next few years. This mile-
stone will be reached through (1) focusing on problems
that admit a super-polynomial quantum speedup and
advancing theory to design algorithms—possibly heuris-
tic—based on intermediate depth circuits that can out-
perform state-of-the-art classical methods, (2) the use of
a suite of error mitigation techniques and improvements
in hardware-aware software to maximize the quality of
the hardware results and extract useful data from the
output of noisy quantum circuits, (3) improvements in
hardware to increase the ﬁdelity of QPUs to 99.99% or
higher, and (4) modular architecture designs that allow
parallelization (with classical communication) of circuit
execution. Error mitigation techniques with mathemati-
cal performance guarantees, like PEC, albeit carrying an
exponential classical processing cost, provide a mean to
quantify both the expected run time and the quality of
processors needed for quantum advantage. This is the
near-term future of quantum computing.

Progress in the quality and speed of quantum systems
will improve the exponential cost of classical processing
required for error mitigation schemes, and a combina-
tion of error mitigation and error correction will drive a
gradual transition toward fault-tolerance. Classical and
quantum computations will be tightly integrated, orches-
trated, and managed through a serverless environment
that allows developers to focus only on code and not in-
frastructure. This is the mid-term future of quantum
computing.

Finally, we have seen how realizing large-scale quan-
tum algorithms with polynomial run times to enable the
full range of practical applications requires quantum er-
ror correction, and how error correction approaches like
the surface code fall short of the long term needs owing to

FIG. 9. Model of a cluster-like architecture integrating classi-
cal processors with QPUs to address latency, parallelization,
and the distribution of instructions among classical and quan-
tum processors. The darker the color, the lower the latency
required.

Threaded runtimes can execute primitives on multiple
controllers. Classical communication in real time be-
tween the controllers can be used to enable things like
circuit cutting. The ﬁgure also shows how future QPUs
with quantum parallelization (l and t couplers) can be
controlled by a single controller. We imagine that there
could be workloads that need near-time classical com-
munication (i.e., calculations based on the outcome of
circuits that must complete in around 100 microseconds)
or to share states between the primitives, enabled by a
data fabric. Finally, the orchestration would be respon-
sible for workﬂows, serverless, nested programs (libraries
of common classical+quantum routines), the circuit knit-
ting toolbox, and circuit compilation.

their ineﬃciency in implementing non-Cliﬀord gates and
poor encoding rate. We outlined a way forward provided
by the development of more eﬃcient LDPC codes with
a high error threshold, and the need for modular hard-
ware with non-2D topologies to allow the investigation
of these codes. This more eﬃcient error correction is the
long-term future of quantum computing.

ACKNOWLEDGMENTS

We thank Kristan Temme, Abhinav Kandala, Ewout
van den Berg, Jerry Chow, Antonio C´orcoles, Ismael
Faro, Blake Johnson, Tushar Mittal, and Matthias Stef-
fen for their assistance reviewing this manuscript.

1“IBM unveils world’s ﬁrst 2 nanometer chip technology, opening
a new frontier for semiconductors,” https://newsroom.ibm.com/
2021-05-06-IBM-Unveils-Worlds-First-2-Nanometer-Chip-
Technology-Opening-a-New-Frontier-for-Semiconductors.
2A. Reuther, P. Michaleas, M. Jones, V. Gadepally, S. Samsi,
and J. Kepner, “Survey of machine learning accelerators,” in
2020 IEEE High Performance Extreme Computing Conference
(HPEC) (IEEE, 2020) pp. 1–12.
3D. S. Abrams and S. Lloyd, “Quantum algorithm providing ex-
ponential speed increase for ﬁnding eigenvalues and eigenvec-
tors,” Physical Review Letters 83, 5162–5165 (1999).
4A. W. Harrow, A. Hassidim, and S. Lloyd, “Quantum algorithm
for linear systems of equations,” Physical Review Letters 103,
150502 (2009).
5P. W. Shor, “Algorithms for quantum computation: Discrete
logarithms and factoring,” in Proceedings 35th annual sympo-
sium on foundations of computer science (Ieee, 1994) pp. 124–
134.
6L. K. Grover, “A fast quantum mechanical algorithm for
database search,” in Proceedings of the Twenty-eighth Annual
ACM Symposium on Theory of Computing, STOC ’96 (ACM,
New York, NY, USA, 1996) pp. 212–219.
7A. M. Childs, D. Maslov, Y. Nam, N. J. Ross, and Y. Su,
“Toward the ﬁrst quantum simulation with quantum speedup,”
Proceedings of the National Academy of Sciences 115, 9456–
9461 (2018).
8A. Y. Kitaev, “Fault-tolerant quantum computation by anyons,”
Annals of Physics 303, 2–30 (2003).
9S. B. Bravyi and A. Y. Kitaev, “Quantum codes on a lattice
with boundary,” arXiv preprint quant-ph/9811052 (1998).
10K. Temme, S. Bravyi, and J. M. Gambetta, “Error mitigation
for short-depth quantum circuits,” Physical Review Letters 119,
180509 (2017).

11Y. Li and S. C. Benjamin, “Eﬃcient variational quantum simu-
lator incorporating active error minimization,” Physical Review
X 7, 021050 (2017).
12S. Bravyi, G. Smith,

and J. A. Smolin, “Trading classical
and quantum computational resources,” Physical Review X 6,
021043 (2016).

13T. Peng, A. W. Harrow, M. Ozols, and X. Wu, “Simulating
large quantum circuits on a small quantum computer,” Physical
Review Letters 125, 150504 (2020).

14W. Tang, T. Tomesh, M. Suchara, J. Larson, and M. Martonosi,
“Cutqc: using small quantum computers for large quantum
circuit evaluations,” in Proceedings of the 26th ACM Interna-
tional Conference on Architectural Support for Programming
Languages and Operating Systems (2021) pp. 473–486.

15K. Mitarai and K. Fujii, “Constructing a virtual two-qubit gate
by sampling single-qubit operations,” New Journal of Physics
23, 023021 (2021).

17

16D. Gottesman, “Fault-tolerant quantum computation with con-

stant overhead,” arXiv preprint arXiv:1310.2984 (2013).

17N. P. Breuckmann and J. N. Eberhardt, “Quantum low-density

parity-check codes,” PRX Quantum 2, 040101 (2021).

18N. Baspin and A. Krishna, “Quantifying nonlocality: How out-
performing local quantum codes is expensive,” arXiv preprint
arXiv:2109.10982 (2021).

19S. Lloyd, “Universal quantum simulators,” Science , 1073–1078

(1996).

20A. W. Harrow, A. Hassidim, and S. Lloyd, “Quantum algorithm
for linear systems of equations,” Physical Review Letters 103,
150502 (2009).

21S. Lloyd, S. Garnerone, and P. Zanardi, “Quantum algorithms
for topological and geometric analysis of data,” Nature commu-
nications 7, 1–7 (2016).
22C. Gyurik, C. Cade,

and V. Dunjko, “Towards quan-
tum advantage via topological data analysis,” arXiv preprint
arXiv:2005.02607 (2020).

23S. Ubaru, I. Y. Akhalwaya, M. S. Squillante, K. L. Clark-
son,
and L. Horesh, “Quantum topological data analysis
with linear depth and exponential speedup,” arXiv preprint
arXiv:2108.02811 (2021).

24D. Aharonov, V. Jones, and Z. Landau, “A polynomial quan-
tum algorithm for approximating the Jones polynomial,” Algo-
rithmica 55, 395–421 (2009).

25V. Giovannetti, S. Lloyd, and L. Maccone, “Quantum random
access memory,” Physical Review Letters 100, 160501 (2008).
26R. Feynman, “Simulating physics with computers,” Interna-

tional Journal of Theoretical Physics 21, 467–488 (1982).

27A. M. Kaufman, M. E. Tai, A. Lukin, M. Rispoli, R. Schit-
tko, P. M. Preiss, and M. Greiner, “Quantum thermalization
through entanglement in an isolated many-body system,” Sci-
ence 353, 794–800 (2016).

28O. Shtanko and R. Movassagh, “Unitary subharmonic response
and ﬂoquet Majorana modes,” Physical Review Letters 125,
086804 (2020).

29I. Aleiner, F. Arute, K. Arya, J. Atalaya, R. Babbush, J. C.
Bardin, R. Barends, A. Bengtsson, S. Boixo, A. Bourassa, et al.,
“Accurately computing electronic properties of materials using
eigenenergies,” arXiv preprint arXiv:2012.00921 (2020).

30K. G. Vollbrecht and J.

I. Cirac, “Quantum simulators,
continuous-time automata, and translationally invariant sys-
tems,” Physical Review Letters 100, 010501 (2008).

31D. Nagaj and P. Wocjan, “Hamiltonian quantum cellular au-
tomata in one dimension,” Physical Review A 78, 032311
(2008).

32A. Kay, “Computational power of symmetric hamiltonians,”

Physical Review A 78, 012346 (2008).

33B. A. Chase and A. J. Landahl, “Universal quantum walks
and adiabatic algorithms by 1d hamiltonians,” arXiv preprint
arXiv:0802.1207 (2008).

34S. Bravyi, M. B. Hastings, and F. Verstraete, “Lieb-robinson
bounds and the generation of correlations and topological quan-
tum order,” Physical Review Letters 97, 050401 (2006).

35T. J. Osborne, “Eﬃcient approximation of the dynamics of one-
dimensional quantum spin systems,” Physical Review Letters
97, 157202 (2006).

36U. Schollw¨ock, “The density-matrix renormalization group in
the age of matrix product states,” Annals of Physics 326, 96–
192 (2011).

37M. Hastings, “Observations outside the light cone: Algorithms
for nonequilibrium and thermal states,” Physical Review B 77,
144302 (2008).

38J. Haah, M. B. Hastings, R. Kothari, and G. H. Low, “Quantum
algorithm for simulating real time evolution of lattice hamilto-
nians,” SIAM Journal on Computing , FOCS18–250 (2021).
39A. M. Childs, A. Ostrander, and Y. Su, “Faster quantum sim-

ulation by randomization,” Quantum 3, 182 (2019).

40D. W. Berry, C. Gidney, M. Motta, J. R. McClean, and R. Bab-
bush, “Qubitization of arbitrary basis quantum chemistry lever-

aging sparsity and low rank factorization,” Quantum 3, 208
(2019).

41D. Gottesman, “Class of quantum error-correcting codes satu-
rating the quantum Hamming bound,” Physical Review A 54,
1862 (1996).

42A. R. Calderbank, E. M. Rains, P. W. Shor, and N. J. Sloane,
“Quantum error correction and orthogonal geometry,” Physical
Review Letters 78, 405 (1997).

43R. Gallager, “Low-density parity-check codes,” IRE Transac-

tions on Information Theory 8, 21–28 (1962).

44J.-P. Tillich and G. Z´emor, “Quantum LDPC codes with posi-
tive rate and minimum distance proportional to the square root
of the blocklength,” IEEE Transactions on Information Theory
60, 1193–1202 (2013).

45M.-H. Hsieh and F. Le Gall, “NP-hardness of decoding quantum
error-correction codes,” Physical Review A 83, 052331 (2011).
46K.-Y. Kuo and C.-C. Lu, “On the hardness of decoding quan-
tum stabilizer codes under the depolarizing channel,” in 2012
International Symposium on Information Theory and its Ap-
plications (IEEE, 2012) pp. 208–211.

47P. Iyer and D. Poulin, “Hardness of decoding quantum stabilizer
codes,” IEEE Transactions on Information Theory 61, 5209–
5223 (2015).

48R. Raussendorf and J. Harrington, “Fault-tolerant quantum
computation with high threshold in two dimensions,” Physical
Review Letters 98, 190504 (2007).

49A. G. Fowler, A. M. Stephens, and P. Groszkowski, “High-
threshold universal quantum computation on the surface code,”
Physical Review A 80, 052312 (2009).

50D. S. Wang, A. G. Fowler, and L. C. Hollenberg, “Surface code
quantum computing with error rates over 1%,” Physical Review
A 83, 020302 (2011).

51S. Bravyi, D. Poulin, and B. Terhal, “Tradeoﬀs for reliable
quantum information storage in 2D systems,” Physical Review
Letters 104, 050503 (2010).

52P. Panteleev and G. Kalachev, “Asymptotically good quantum
and locally testable classical LDPC codes,” STOC 2022: Pro-
ceedings of the 54th Annual ACM SIGACT Symposium on The-
ory of Computing , 375 (2022).

53H. Bomb´ın, “Single-shot fault-tolerant quantum error correc-

tion,” Physical Review X 5, 031043 (2015).

54A. Kubica and M. Vasmer, “Single-shot quantum error correc-
tion with the three-dimensional subsystem toric code,” arXiv
preprint arXiv:2106.02621 (2021).

55E. T. Campbell, “A theory of single-shot error correction for
adversarial noise,” Quantum Science and Technology 4, 025006
(2019).

56S. Bravyi and R. K¨onig, “Classiﬁcation of topologically pro-
tected gates for local stabilizer codes,” Physical Review Letters
110, 170503 (2013).

57M. B. Hastings and J. Haah, “Dynamically generated logical

qubits,” Quantum 5, 564 (2021).

58J. E. Moussa, “Transversal Cliﬀord gates on folded surface

codes,” Physical Review A 94, 042316 (2016).

59V. Kliuchnikov, D. Maslov, and M. Mosca, “Fast and eﬃcient
exact synthesis of single-qubit unitaries generated by Cliﬀord
and T gates,” Quantum Information and Computation 13, 607
(2013).

60N. J. Ross and P. Selinger, “Optimal ancilla-free Cliﬀord+T
approximation of z-rotations,” Quantum Information and Com-
putation 16, 901 (2016).

61J. O’Gorman and E. T. Campbell, “Quantum computation with
realistic magic-state factories,” Physical Review A 95, 032338
(2017).

62J. Haah, M. B. Hastings, D. Poulin, and D. Wecker, “Magic
state distillation with low space overhead and optimal asymp-
totic input count,” Quantum 1, 31 (2017).

63M. B. Hastings and J. Haah, “Distillation with sublogarithmic

overhead,” Physical Review Letters 120, 050504 (2018).

64Y. Li, “A magic state’s ﬁdelity can be superior to the operations

18

that created it,” New Journal of Physics 17, 023037 (2015).
65A. G. Fowler and S. J. Devitt, “A bridge to lower overhead
quantum computation,” arXiv preprint arXiv:1209.0510 (2012).
66D. Litinski, “A game of surface codes: Large-scale quantum

computing with lattice surgery,” Quantum 3, 128 (2019).

67A. Paetznick and A. G. Fowler, “Quantum circuit optimization
by topological compaction in the surface code,” arXiv preprint
arXiv:1304.2807 (2013).

68D. Litinski, “Magic state distillation: Not as costly as you

think,” Quantum 3, 205 (2019).

69B. J. Brown, “A fault-tolerant non-Cliﬀord gate for the surface
code in two dimensions,” Science Advances 6, eaay4929 (2020).
70S. Endo, S. C. Benjamin, and Y. Li, “Practical quantum error
mitigation for near-future applications,” Physical Review X 8,
031027 (2018).

71A. Strikis, D. Qin, Y. Chen, S. C. Benjamin,

and Y. Li,
“Learning-based quantum error mitigation,” PRX Quantum 2,
040330 (2021).

72R. Harper, S. T. Flammia, and J. J. Wallman, “Eﬃcient learn-
ing of quantum noise,” Nature Physics 16, 1184–1188 (2020).
73S. T. Flammia, “Averaged circuit eigenvalue sampling,” arXiv

preprint arXiv:2108.05803 (2021).

74E. van den Berg, Z. Minev, A. Kandala, and K. Temme, “Prob-
abilistic error cancellation with sparse Pauli-Lindblad models
on noisy quantum processors,” arXiv preprint arXiv:2201.09866
(2022).

75A. Kandala, A. Mezzacapo, K. Temme, M. Takita, M. Brink,
J. M. Chow, and J. M. Gambetta, “Hardware-eﬃcient vari-
ational quantum eigensolver for small molecules and quantum
magnets,” Nature 549, 242–246 (2017).

76A. Kandala, K. Temme, A. D. C´orcoles, A. Mezzacapo, J. M.
Chow, and J. M. Gambetta, “Error mitigation extends the com-
putational reach of a noisy quantum processor,” Nature 567,
491–495 (2019).

77Y. Kim, C. J. Wood, T. J. Yoder, S. T. Merkel, J. M. Gambetta,
K. Temme,
and A. Kandala, “Scalable error mitigation for
noisy quantum circuits produces competitive expectation val-
ues,” (2021).

78W. J. Huggins, S. McArdle, T. E. O’Brien, J. Lee, N. C. Ru-
bin, S. Boixo, K. B. Whaley, R. Babbush, and J. R. McClean,
“Virtual distillation for quantum error mitigation,” Phys. Rev.
X 11, 041036 (2021).

79B. Koczor, “Exponential error suppression for near-term quan-

tum devices,” Phys. Rev. X 11, 031057 (2021).

80S. Endo, Z. Cai, S. C. Benjamin,

and X. Yuan, “Hybrid
quantum-classical algorithms and quantum error mitigation,”
Journal of the Physical Society of Japan 90, 032001 (2021).

81C. Piveteau, D. Sutter, S. Bravyi, J. M. Gambetta,

and
K. Temme, “Error mitigation for universal gates on encoded
qubits,” Physical Review Letters 127, 200505 (2021).

82S. Bravyi and D. Gosset, “Improved classical simulation of quan-
tum circuits dominated by Cliﬀord gates,” Physical Review Let-
ters 116, 250501 (2016).

83S. Bravyi, D. Browne, P. Calpin, E. Campbell, D. Gosset, and
M. Howard, “Simulation of quantum circuits by low-rank stabi-
lizer decompositions,” Quantum 3, 181 (2019).

84M. Lostaglio and A. Ciani, “Error mitigation and quantum-
assisted simulation in the error corrected regime,” Phys. Rev.
Lett. 127, 200506 (2021).

85Y. Suzuki, S. Endo, K. Fujii, and Y. Tokunaga, “Quantum
error mitigation for fault-tolerant quantum computing,” PRX
Quantum 3, 010345 (2022).

86C. Piveteau and D. Sutter, “Circuit knitting with classical com-

munication,” arXiv preprint arXiv:2205.00016 (2022).

87A. Eddins, M. Motta, T. P. Gujarati, S. Bravyi, A. Mezzacapo,
C. Hadﬁeld, and S. Sheldon, “Doubling the size of quantum
simulators by entanglement forging,” PRX Quantum 3, 010309
(2022).

88P. Huembeli, G. Carleo, and A. Mezzacapo, “Entanglement
Forging with generative neural network models,” arXiv preprint

arXiv:2205.00933 (2022).

89B. Bauer, D. Wecker, A. J. Millis, M. B. Hastings,

and
M. Troyer, “Hybrid quantum-classical approach to correlated
materials,” Phys. Rev. X 6, 031045 (2016).

90J. M. Kreula, L. Garc´ıa- ´Alvarez, L. Lamata, S. R. Clark,
E. Solano, and D. Jaksch, “Few-qubit quantum-classical sim-
ulation of strongly correlated lattice fermions,” EPJ Quantum
Technology 3, 1–19 (2016).

91S. Bravyi and D. Gosset, “Complexity of quantum impurity
problems,” Communications in Mathematical Physics 356, 451–
500 (2017).

92G. Knizia and G. K.-L. Chan, “Density matrix embedding: A
simple alternative to dynamical mean-ﬁeld theory,” Physical re-
view letters 109, 186404 (2012).

93G. Knizia and G. K.-L. Chan, “Density matrix embedding: A
strong-coupling quantum embedding theory,” Journal of chem-
ical theory and computation 9, 1428–1432 (2013).

94L. Mineh and A. Montanaro, “Solving the Hubbard model using
density matrix embedding theory and the variational quantum
eigensolver,” Physical Review B 105, 125117 (2022).

95H. Ma, M. Govoni, and G. Galli, “Quantum simulations of ma-
terials on near-term quantum computers,” npj Computational
Materials 6, 1–8 (2020).

96E. Farhi, J. Goldstone, and S. Gutmann, “A quantum approx-
imate optimization algorithm applied to a bounded occurrence
constraint problem,” arXiv:1412.6062 (2014).

97V. Havl´ıˇcek, A. D. C´orcoles, K. Temme, A. W. Harrow, A. Kan-
dala, J. M. Chow, and J. M. Gambetta, “Supervised learning
with quantum-enhanced feature spaces,” Nature 567, 209–212
(2019).

98X. Yuan, S. Endo, Q. Zhao, Y. Li, and S. C. Benjamin, “Theory
of variational quantum simulation,” Quantum 3, 191 (2019).
99Y. Liu, S. Arunachalam, and K. Temme, “A rigorous and ro-
bust quantum speed-up in supervised machine learning,” Nature
Physics 17, 1013 (2021).

100J. R. Glick, T. P. Gujarati, A. D. Corcoles, Y. Kim, A. Kandala,
J. M. Gambetta, and K. Temme, “Covariant quantum kernels
for data with group structure,” arXiv:2105.03406 (2021).
101S. Bravyi, D. Gosset, and R. Movassagh, “Classical algorithms
for quantum mean values,” Nature Physics 17, 337–341 (2021).
102N. J. Coble and M. Coudron, “Quasi-polynomial time approx-
imation of output probabilities of geometrically-local, shallow
quantum circuits.” 2021 IEEE 62nd Annual Symposium on
Foundations of Computer Science (FOCS) , 598 (2022).

103D. Stilck Fran¸ca and R. Garc´ıa-Patr´on, “Limitations of opti-
mization algorithms on noisy quantum devices,” Nature Physics
17, 1221 (2021).

104S. Barison, F. Vicentini, and G. Carleo, “An eﬃcient quan-
tum algorithm for the time evolution of parameterized circuits,”
arXiv preprint arXiv:2101.04579 (2021).

105C. Zoufal, D. Sutter,

and S. Woerner, “Error bounds
for variational quantum time evolution,” arXiv preprint
arXiv:2108.00022 (2021).

106J. M. Gambetta, J. M. Chow, and M. Steﬀen, “Building logical
qubits in a superconducting quantum computing system,” npj
Quantum Information 2017 3:1 3, 1–7 (2017).

107D. R. W. Yost, M. E. Schwartz, J. Mallek, D. Rosenberg,
C. Stull, J. L. Yoder, G. Calusine, M. Cook, R. Das, A. L.
Day, E. B. Golden, D. K. Kim, A. Melville, B. M. Niedziel-
ski, W. Woods, A. J. Kerman, and W. D. Oliver, “Solid-state
qubits integrated with superconducting through-silicon vias,”
npj Quantum Information 6, 59 (2020).

108S. K. Tolpygo, V. Bolkhovsky, T. J. Weir, L. M. Johnson, M. A.
Gouker, and W. D. Oliver, “Fabrication process and properties
of fully-planarized seep-submicron Nb/Al–AlOx/Nb Josephson
junctions for VLSI circuits,” IEEE Transactions on Applied Su-
perconductivity 25, 1 (2015).

109D. Rosenberg, S. Weber, D. Conway, D. Yost, J. Mallek,
G. Calusine, R. Das, D. Kim, M. Schwartz, W. Woods, J. L.
Yoder, and W. D. Oliver, “Solid-state qubits: 3D integration

19

and packaging,” IEEE Microwave Magazine 21, 72 (2020).
110J. M. Kreikebaum, K. P. O’Brien, A. Morvan, and I. Siddiqi,
“Improving wafer-scale Josephson junction resistance variation
in superconducting quantum coherent circuits,” Superconductor
Science and Technology 33, 06LT02 (2020).

111J. B. Hertzberg, E. J. Zhang, S. Rosenblatt, E. Magesan, J. A.
Smolin, J.-B. Yau, V. P. Adiga, M. Sandberg, M. Brink, J. M.
Chow, and J. S. Orcutt, “Laser-annealing Josephson junctions
for yielding scaled-up superconducting quantum processors,”
npj Quantum Inf 7, 129 (2021).

112J. M. Gambetta, C. E. Murray, Y.-K.-K. Fung, D. T. Mc-
Clure, O. Dial, W. Shanks, J. W. Sleight,
and M. Steﬀen,
“Investigating surface loss eﬀects in superconducting transmon
qubits,” IEEE Transactions on Applied Superconductivity 27,
1–5 (2017).

113A. P. M. Place, L. V. H. Rodgers, P. Mundada, B. M. Smitham,
M. Fitzpatrick, Z. Leng, A. Premkumar, J. Bryon, A. Vraji-
toarea, S. Sussman, G. Cheng, T. Madhavan, H. K. Babla,
X. H. Le, Y. Gang, B. J¨ack, A. Gyenis, N. Yao, R. J. Cava,
N. P. de Leon, and A. A. Houck, “New material platform for
superconducting transmon qubits with coherence times exceed-
ing 0.3 milliseconds,” Nature Communications 2021 12:1 12, 1–6
(2021).

114C. R. H. McRae, G. M. Stiehl, H. Wang, S. X. Lin, S. A. Cald-
well, D. P. Pappas, J. Mutus, and J. Combes, “Reproducible co-
herence characterization of superconducting quantum devices,”
Applied Physics Letters 119, 100501 (2021).

115L. Geck, A. Kruth, H. Bluhm, S. van Waasen, and S. Heinen,
“Control electronics for semiconductor spin qubits,” Quantum
Science and Technology 5, 015004 (2019).

116X. Xue, B. Patra, J. P. G. van Dijk, N. Samkharadze, S. Sub-
ramanian, A. Corna, B. Paquelet Wuetz, C. Jeon, F. Sheikh,
E. Juarez-Hernandez, B. P. Esparza, H. Rampurawala, B. Carl-
ton, S. Ravikumar, C. Nieva, S. Kim, H.-J. Lee, A. Sammak,
G. Scappucci, M. Veldhorst, F. Sebastiano, M. Babaie, S. Peller-
ano, E. Charbon, and L. M. K. Vandersypen, “CMOS-based
cryogenic control of silicon quantum circuits,” Nature 593, 205–
210 (2021).

117C. A. Ryan, B. R. Johnson, D. Rist`e, B. Donovan, and T. A.
Ohki, “Hardware for dynamic quantum computing,” Review of
Scientiﬁc Instruments 88, 104703 (2017).

118J. M. Pino, J. M. Dreiling, C. Figgatt, J. P. Gaebler, S. A.
Moses, M. S. Allman, C. H. Baldwin, M. Foss-Feig, D. Hayes,
K. Mayer, C. Ryan-Anderson, and B. Neyenhuis, “Demonstra-
tion of the trapped-ion quantum CCD computer architecture,”
Nature 592, 209–213 (2021).

119A. Cross, A. Javadi-Abhari, T. Alexander, N. de Beaudrap, L. S.
Bishop, S. Heidel, C. A. Ryan, J. Smolin, J. M. Gambetta, and
B. R. Johnson, “OpenQASM 3: A broader and deeper quantum
assembly language,” arXiv:2104.14722 (2021).

120A. Gold, J. Paquette, A. Stockklauser, M. J. Reagor, M. S.
Alam, A. Bestwick, N. Didier, A. Nersisyan, F. Oruc, A. Razavi,
B. Scharmann, E. A. Sete, B. Sur, D. Venturelli, C. J. Winkle-
black, F. Wudarski, M. Harburn, and C. Rigetti, “Entangle-
ment across separate silicon dies in a modular superconducting
qubit device,” (2021), arXiv:2102.13293.

121C. Conner, A. Bienfait, H.-S. Chang, M.-H. Chou, ´E. Dumur,
J. Grebel, G. Peairs, R. Povey, H. Yan, Y. Zhong, et al., “Super-
conducting qubits in a ﬂip-chip architecture,” Applied Physics
Letters 118, 232602 (2021).

122M. P. Larsson and S. Lucyszyn, “A micromachined separable
RF connector fabricated using low-resistivity silicon,” Journal
of Micromechanics and Microengineering 16, 2021–2033 (2006).
123D. B. Tuckerman, M. C. Hamilton, D. J. Reilly, R. Bai, G. A.
Hernandez, J. M. Hornibrook, J. A. Sellers, and C. D. Ellis,
“Flexible superconducting Nb transmission lines on thin ﬁlm
polyimide for quantum computing applications,” Superconduc-
tor Science and Technology 29, 084007 (2016).

124D. J. Reilly, “Engineering the quantum-classical interface of
solid-state qubits,” npj Quantum Information 2015 1:1 1, 1–10

(2015).

125Y. Zhong, H.-S. Chang, A. Bienfait, ´E. Dumur, M.-H. Chou,
C. R. Conner, J. Grebel, R. G. Povey, H. Yan, D. I. Schuster,
and A. N. Cleland, “Deterministic multi-qubit entanglement in a
quantum network,” Nature 2021 590:7847 590, 571–575 (2021).
126P. Kurpiers, P. Magnard, T. Walter, B. Royer, M. Pechal,
J. Heinsoo, Y. Salath´e, A. Akin, S. Storz, J.-C. Besse, S. Gas-
parinetti, A. Blais, and A. Wallraﬀ, “Deterministic quantum
state transfer and remote entanglement using microwave pho-
tons,” Nature 558, 264–267 (2018).

127N. Leung, Y. Lu, S. Chakram, R. K. Naik, N. Earnest, R. Ma,
K. Jacobs, A. N. Cleland, and D. I. Schuster, “Deterministic
bidirectional communication and remote entanglement genera-
tion between superconducting qubits,” npj Quantum Informa-
tion 2019 5:1 5, 1–5 (2019).
128E. T. Campbell, B. M. Terhal,

and C. Vuillot, “Roads to-

20

wards fault-tolerant universal quantum computation,” Nature
2017 549:7671 549, 172–179 (2017).

129N. H. Nickerson, Y. Li, and S. C. Benjamin, “Topological quan-
tum computing with a very noisy network and local error rates
approaching one percent,” Nature Communications 2013 4:1 4,
1–5 (2013).

130C. Monroe, R. Raussendorf, A. Ruthven, K. R. Brown,
P. Maunz, L.-M. Duan,
and J. Kim, “Large-scale modular
quantum-computer architecture with atomic memory and pho-
tonic interconnects,” Physical Review A 89, 022317 (2014).

131D.

P.

Science

DiVincenzo,

compu-
tation,”
(1995),
270,
https://www.science.org/doi/pdf/10.1126/science.270.5234.255.
132M. A. Nielsen, “Quantum computation by measurement and
quantum memory,” Physics Letters A 308, 96–100 (2003).
133E. Farhi, J. Goldstone, S. Gutmann, and M. Sipser, “Quantum
computation by adiabatic evolution,” arXiv:0001106 (2000).

“Quantum
255–261

