2
2
0
2

g
u
A
8

]
E
S
.
s
c
[

1
v
2
2
9
3
0
.
8
0
2
2
:
v
i
X
r
a

CSSAM: Code Search via Attention Matching of
Code Semantics and Structures

1st Yi Hu
School of Cyber Science and Engineering
WuHan University
WuHan, China
csehuyi@whu.edu.cn

2nd Bo Cai
School of Cyber Science and Engineering
WuHan University
WuHan, China
caib@whu.edu.cn

3rd Yao Xiangyu
School of Cyber Science and Engineering
WuHan University
WuHan, China
yu.yaoxiang@whu.edu.cn

Abstract—Code search greatly improves developers’ coding
efﬁciency by retrieving reusable code segments from open source
repositories with natural language queries. Despite the continu-
ous efforts in improving both the effectiveness and efﬁciency of
code search, two issues remained unsolved. First, programming
languages have inherent strong structural linkages, and feature
mining of code as text form would omit the structural information
contained inside it. Second, there is a potential semantic rela-
tionship between code and query, it is challenging to align code
and text across sequences so that vectors are spatially consistent
during similarity matching.

To tackle both issues, in this paper, a code search model named
CSSAM (Code Semantics and Structures Attention Matching)
is proposed. By introducing semantic and structural match-
ing mechanisms, CSSAM effectively extracts and fuses multi-
dimensional code features. Speciﬁcally, the cross and residual
layer was developed to facilitate high-latitude spatial alignment
of code and query at the token level. By leveraging the residual
interaction, a matching module is designed to preserve more
code semantics and descriptive features,
that enhances the
adhesion between the code and its corresponding query text.
Besides, to improve the model’s comprehension of the code’s
inherent structure, a code representation structure named CSRG
(Code Semantic Representation Graph) is proposed for jointly
representing abstract syntax tree nodes and the data ﬂow of the
codes. According to the experimental results on two publicly
available datasets containing 540k and 330k code segments,
CSSAM signiﬁcantly outperforms the baselines in terms of
achieving the highest SR@1/5/10, MRR, and NDCG@50 on both
datasets respectively. Moreover, the ablation study is conducted
to quantitatively measure the impact of each key component of
CSSAM on the efﬁciency and effectiveness of code search, which
offers the insights into the improvement of advanced code search
solutions.

Index Terms—code search, graph representation, text match-

ing, attention mechanism, graph attention network

I. INTRODUCTION

With the popularity of open source communities, the amount
of code hosted on platforms like Github and StackOverﬂow
is increasing day by day, providing more possibilities for
code reuse by program developers. In the face of huge code

resources, how to accurately ﬁnd the corresponding code seg-
ment according to user intent has become a popular research
problem. Code search has gone through two stages, the early
ones are based on information retrieval (IR) methods, such
as Portfolio [1] proposed by McMillan et al. and CodeHow
[2] proposed by Fei et al. These IR-based methods only treat
code as a text fragment like natural language, and perform
natural language to code fragment retrieval work with the
help of search engine ideas. However, it is obvious that code
snippets and natural language are heterogeneous and there
are considerable differences between them, and the proposed
deep learning-based code search methods can better solve this
problem. In 2016, CODEnn [3] proposed by Gu et al. was
the ﬁrst method to use deep learning to solve the code search
problem. It embeds natural language and code snippets into a
high-dimensional vector space through a neural network, and
then use the cosine distance to judge the similarity between
them. Compared with information retrieval based models, deep
learning (DL) based models can capture higher dimensional
information, so the results are also much better than IR models.
Like IR models, DL models also have their limitations,
even though existing studies have been able to understand
and analyze natural language by neural networks, they cannot
understand programming language well. In CODEnn [3], the
authors try to use tokens, api sequence, and method name of
code segments to extract semantic features of the code, and
this approach proves to be useful. In the code2vec [4] model,
an attempt is made to use the abstract syntax trees(ASTs) as
a feature of the code for embedding. Similarly there is the
proposed SBT [5] traversal of the abstract syntax tree AST.
All these models process the AST of the code as sequence
features ﬁrst, and then learn its features using RNN or LSTM.
For such topologies as AST, graph neural networks should
be able to embed them better in theory. Wan et al. proposed
MMAN [6] to learn the code attribute graphs of AST, CFG,
DFG, etc. as features with gated neural networks GGNN [7],
which achieved very good results, but the input features are

 
 
 
 
 
 
too scattered.

A. Code Representation

The success of large-scale natural language models has led
to the emergence of pre-training-based code search models,
these encoder models (e.g., CoBert [8], CodeT5 [9]) are
designed to embed the code and natural language descrip-
then the cosine or L2
tions into the same feature space,
this type of
similarity of these vectors is computed, but
encoder models cannot retain the deep code structure and
semantic information, because of its large training size, it is
difﬁcult to make its results converge, besides they could not
learn the dependencies between variables in the code and the
transition process of the program state. Moreover, despite the
fact that the aforementioned models can capture the semantic
information of individual code fragments or query text, they
are hard to investigate the semantic relationships within the
code and between the code and the description text at a ﬁner
granularity.

Based on the existing research, this paper proposes a new
structured code search model CSSAM, which is more effective
than the existing state-of-the-art model, and the contributions
of this paper are as follows:

• A new deep learning-based code search model CSSAM is
proposed, which semantically extracts and matches codes
and descriptions by introducing multiple levels such as
semantic level and structural level matching modules.
• A new code representation structure, the Code Seman-
tic Representation Graph (CSRG) is proposed, which
is based on an abstract syntax tree, aggregates nodes,
and incorporates data ﬂow features so that the structure
contains more semantic information.

• A code-description matching module CRESS is proposed
and used, and a matching module based on residual
interaction is designed in this paper, which enhances the
description ability of words and phrases by cascading
residual information and attention mechanism, thus pre-
serving more textual features of codes and descriptions,
while introducing a weight sharing mechanism to match
codes and descriptions at the semantic level.

• The CSSAM proposed in this paper can be accurately
trained and tested on two publicly available large datasets,
and the experimental results are better than the classical
models.

The remainder of this paper is organized as follows. Section
2 introduces the related work of this paper. Section 3 presents
our proposed model CSSAM. Section 4 describes the exper-
iment setup. Section 5 and Section 6 show the experimental
results and discussion respectively. Section 7 concludes the
work and presents future works.

II. RELATED WORK

In this section, we mainly introduce and discuss the related
work of this paper from three aspects: code representation
based on deep learning, text matching, attention mechanism
and graph neural network.

In early research, code was treated as a natural language,
and it was taken for granted to process code fragments
using natural language models. Later, Gu et al.’s research and
experiments [3] proved that method names and API sequences
in code are also crucial for code feature extraction, and these
methods were used in code search and code annotation genera-
tion. The best representation of programming languages is the
abstract syntax tree AST, and in 2016, the distributed code2vec
model for learning code using AST paths was proposed, which
was used in method name prediction of code fragments with
very good results, and the model was further demonstrated to
contain a large amount of code semantic information in the
AST.

Since code2vec was proposed, people have started to study
the topological representation of code. code2vec uses dis-
tributed representation of words to learn the vector representa-
tion of tokens in code by traversing the AST [10], [11] paths.
Hu et al.’s proposed SBT [5] deﬁnes a rule to traverse the AST
and convert the topological structure into a sequence structure.
Wan et al.’s proposed MMAN [6] converts the topological
structure of code into a sequence structure. MMAN treats the
tree structure of the code (ASTs) representation [12], [13] as
a special graph structure and uses graph neural networks to
learn the embedding of codes.

B. Text Matching

Text matching [14]–[16] is an important fundamental prob-
lem in natural language processing and can be applied to a
large number of natural language processing (NLP) tasks, such
as information retrieval, question and answer systems, para-
phrasing problems, dialogue systems, machine translation, etc.
These NLP tasks have certain similarities with text matching
problems. Similarly, the code search task can be abstracted
as a text matching problem, i.e., determining whether the
code in the code base matches the query statement. With
the successful use of deep learning in the ﬁelds of computer
vision, speech recognition, and recommender systems, there
has been much research in recent years devoted to applying
deep neural network models to natural language processing
tasks to reduce the cost of feature engineering. The baseline
model for deep code search is the classical baseline model
based on text matching.

Traditional

text matching techniques include the vector
space model VSM [17]–[19], commonly known as the “bag
of word model”, which represents words in a dictionary
space generated from text; TF-IDF [20], which adds word
frequency weights to VSM; and the BM25 [21] algorithm,
which calculates the match between web ﬁelds and query
ﬁelds by the degree of coverage of these ﬁelds. The higher
the score, the better the match between the web page and the
query; these algorithms mainly solve the matching problem
at the lexical level, or the similarity problem at the lexical
level. In fact, matching algorithms based on lexical overlap
have great limitations because they ignore grammatical and
structural features.

Fig. 1. The workﬂow of our code search model CSSAM. The ﬁrst thing is to collect code snippets, and then it can be divided into two stages: ofﬂine training
and online searching. The ofﬂine stage uses the processed code-description pairs to train our code search model. After training, it can be used for online code
search task.

Latent Sementic Analysis (LSA) [22], which became pop-
ular in the 1990s, has opened up a new idea. By mapping
utterances to a low-dimensional continuous space of equal
length, similarity computation can be performed on this im-
plicit latent semantic space. Since then, more advanced prob-
abilistic models such as PLSA (Probabilistic Latent Semantic
Analysis) [23], which gives a probabilistic interpretation to
LSA through a generative model, and LDA (Latent Dirichlet
Allocation) [24], which introduces the concept of prior dis-
tribution of parameters based on PLSA’s model, have been
designed. And gradually formed a very hot direction of topic
modeling techniques. These techniques provide a concise and
convenient semantic representation of text, and make up for
the shortcomings of traditional lexical matching methods.

C. Attention Mechanism

Originally used in machine translation [25], AM(Attention
Mechanism) has become an important concept in the ﬁeld of
neural networks. In the ﬁeld of artiﬁcial intelligence, attention
has become an important part of the structure of neural
networks and has a large number of applications in natural
language processing, statistical learning, speech and comput-
ing. Attention mechanism can be explained intuitively using
human visual mechanisms. For example, our visual system
tends to focus on the part of information in an image that
aids in judgment and ignore irrelevant information. Similarly,
in problems involving language or vision [26], some parts of
the input may be more useful for decision making than others
[27]. For example, in translation [25] and summarization [28],
[29] tasks, only certain words in the input sequence may be

relevant for predicting the next word. Similarly, in a code
search problem, only certain words in the input description
may be more relevant to a particular word of the searched
code fragment. The attention mechanism helps to perform the
task at hand by allowing the model to dynamically focus on
certain parts of the input that help to perform the task at hand.

D. Graph Neural Networks

Although traditional deep learning methods have been ap-
plied to extract features from Euclidean space data with great
success, many practical application scenarios in which the data
are generated from non-Euclidean space, the performance of
traditional deep learning methods on processing non-Euclidean
space data is still unsatisfactory. Recent research has focused
on the extension of deep learning methods to graphs. Driven
by the success of multiple factors, researchers have borrowed
ideas from convolutional networks, recurrent networks and
deep autoencoders to deﬁne and design neural network struc-
tures for processing graph data, that is GNN(graph neural
networks) [30], [31].

Code, as a computer instruction, has similarity to natural
language in terms of vocabulary, but is very different from
natural language in terms of syntax and extensibility, and is
more rigorous than natural language. Understanding the se-
mantics of code through words alone often fails to understand
the deeper semantic information expressed in code. Abstract
Syntax Tree (AST), as an intermediate representation of high-
level language and machine instructions, contains the logical
information of the code. Recent researches like SBT [5], Tree-
LSTM [32], etc., all serialize the abstract syntax tree, but

public static void read file by ...open a file and print ...Docstring tokensCode tokensAbstract sytax graphSource code with docstringTokens vectorGraph vectorDocstring vectorCode vectorCosine SimilarityGraph neural networkContext embedding layerContext embedding layerCode vectorDocstring vectora[4]XŶa[1]1a[1]2a[1]3a[1]na[2]1a[2]2a[2]3a[2]na[3]1a[3]2a[3]3a[3]na[4]XŶa[1]1a[1]2a[1]3a[1]na[2]1a[2]2a[2]3a[2]na[3]1a[3]2a[3]3a[3]nSearching……Searching……Searching……Top K Searching results--------------------------------------------------------------------------------------------------Top K Searching results--------------------------------------------------------------------------------------------------Offline TrainingOnline SearchingOpen Source CommunityCode SnippetsTraining CorpusCode BaseSearching QueryRanked ResultTrainingVectorsSimilarity Measure CS ModelFig. 2. The network architecture of our proposed CSSAM model. We ﬁrst extract the < code, description > pairs from training corpus. We then parse the
code snippets into graph modalities, i.e., tokens, CSRG. Then the training samples are fed into the network as input. (a) Embedding layer. We ﬁrst learn the
representation of each token and node via fasttext, graph embeding. (b) Representation layer. We use LSTM and GAT to learn the representation of tokens
and CSRGs. (c) Fusion attention layer. We design an attention layer to assign different weight on different parts for each modality, and then fuse the attended
vector into a single vector. (d) Ranking learning. We map the comment description representation and code representation into an intermediate semantic
common space and design a ranking loss function to learn their similarities.

transforming the topology into serial structure will inevitably
lose a lot of information, and it is reasonable to use GNN,
which specializes in extracting graph structure features, to
extract code structure features.

III. PROPOSED APPROACH

This section ﬁrst introduces the whole workﬂow of code
search, and then presents the details of our proposed model
architecture for CSSAM.

Figure 1 shows the workﬂow of code search [3], [6], [33],
and our model also follows this process. The code search
can be divided into two parts: ofﬂine training and online
search, where the ofﬂine training phase requires us to extract
a large number of < code, decsting > pairs from the open
source code repository as training data, and then train our
model CSSAM with the training data, and after the training
is completed, we get a trained search model. Another phase
is the online search phase, in which we only need to enter a
natural language description and the trained model will search
for the closest code fragment.

The overall structure of the CSSAM model is shown in
Figure 2, which can be divided into two parts: semantic level
matching and structural level matching. In the text semantic
level matching module, this paper designs a matching module
based on residual interaction to improve the accuracy of se-
mantic matching; in the structure level matching module, this
paper designs a new code representation structure of CSRG
(Code Semantic Representation Graph) for structural feature
extraction; in the similarity calculation module, a fusion at-
tention machine layer to balance the matching contribution of
each part.

A. Embedding

The word embedding layer uses unsupervised learning
fasttext [34] model, and uses various feature enhancement
methods including n-gram and subword regularization [35].
Code and Docstring are tokenized and fed into the word
embedding model for word vector training, respectively.

For the graph structure representation of the code, our
model needs to learn the structural and semantic information
between the local to the global within the code, so as to
extract the features embedded in the different structures in
multiple structural representations,
in order to make each
token node able to obtain the local relationships between
the internal nodes, we design the graph embedding layer
by using Deepwalk [36] method, and DeepWalk is divided
into two parts: random walk and generation of representation
vectors. Firstly, some vertex sequences are extracted from the
graph using the Randomwalk algorithm; then, with the help
of natural language processing ideas, the generated ﬁxed-point
sequences are regarded as sentences composed of words, and
all the sequences can be regarded as a large corpus, and ﬁnally,
each vertex is represented as a vector of dimension d using
the natural language processing tool word2vec [17], [37]. The
ﬂow of the DeepWalk is depicted as Algorithm 1.

B. Code Semantic Representation Graph

Intermediate representation structures of codes like AST,
CFG, DFG, etc. can better reﬂect the semantic features of
codes, however, the increase of model complexity and the
improvement of model effectiveness are not proportional when
dealing with these tree structures separately. In order to better
integrate these code property graphs, we propose a code

public static void read file by ...open a file and print ...Docstring tokensCode tokensAbstract sytax graphGraph vectorCode vectorCosine SimilarityGraph Embedding LayerWord Embedding LayerSource code with docstringWord Embedding LayerWord Embedding LayerCosine SimilarityGraph Embedding LayerCode Semantic Representation GraphGraph Attention NetworkWord Embedding LayerWord Embedding LayerDocstring vectorDocstring vectorTokens vectorTokens vectorDocstring vectorCress LayerData PreprocessingWord RepresentationContext EmbeddingAttention FusionModel LearningFusion LayerFusion LayerContext Embedding LayerTokens vectorCSRG vectorAttentionAttentionAttentionAttentionAttentionAttentionAttentionAttentionSemantic FeatureStructure FeatureAlgorithm 1 DeepWalkG, w, d, γ, t
Input: graph G(V, E), window size w, embedding size d,
walks per vertex γ, walk length t
Output: matrix of vertex representations Φ ∈ RV ×
d.

1: Initialization: Sample Φ from U V × d
2: Build a binary Tree T from V
3: for i = 0 to γ do
4: O = Shufﬂe(V )
5:
6:
7:
8:
9: end for

W = RandomW alk(G, vi, t)
SkipGram(Φ, W, w)

for each vi ∈ O do

end for

semantic representation graph based on the abstract syntax
tree.

For snippets with a very large amount of code, there is
the problem of long coding spans when data or variables are
called repeatedly, for example, a function customizes a new
variable in the ﬁrst several lines but does not call it again
until the code’s last line of the function; consequently, when
tokens features are extracted by models such as GNN or RNN,
the long-term relationship of that variable in the code text
would disappear due to the increase in sequence length. Hence,
the data dependency of each statement can be determined by
analyzing the data stream that the statement uses. In order for
the semantic representation model to be able to compensate
for the inherent ﬂaws in the feature extraction algorithm (such
as the model’s inability to capture long-range dependencies,
etc.), the representation model we designed incorporate the
acquisition of contextual information of code statements, by
adding directed edges to variables in the context, the model
could eliminate the problem of disappearing code semantics.
To minimize information loss and maintain the original
AST structure, this paper proposes CSRG (Code Semantic
Representation Graph), a graph structure based on AST that
aggregates node information. CSRG is based on the abstract
syntax tree but more compact, and keeps the data ﬂow infor-
mation in the CSRG structure.

Figure 3 shows the generation process of the code se-
mantic representation graph. We generate the abstract syntax
tree(AST) and data ﬂow graph(DFG) of the code segment by
third-party tools1.

We ﬁrst generate the DFG abstract representation of the
code using the tree-sitt tool, and then use depth ﬁrst search
algorithm to locate the corresponding DFG node in the AST.
We can notice that there are many nodes in the AST and
DFG, Nodes represent variables such as a {i}, b {i} and
x {i}, where i indicates its position in the sequence of tokens
of the code. Nodes in the DFG can be located in the AST.
To obtain CSRG, we ﬁrstly de-duplicate the nodes with the
same attributes, ignore the indicates. Then we get the AST
and DFG with reduced number of nodes. Secondly, ﬁnd the
corresponding DFG node in the AST, and add the node
relationship from the DFG to the AST. The improved AST
now contains two types of edges, one that is present in the
AST itself and another that is added based on the DFG. We
can assign different weights to these two types of edges. As a
result of the rule for generating three-address codes, variables
and attributes are typically located at the leaf node positions
in the AST. After combining the data ﬂow in this structure,
directed edges are added between each leaf node to determine
the relationship between branch statements in the AST, which
further reduces the sequence span during feature extraction.
Finally, the strings in the nodes are replaced based on the
syntax parsing dictionary of the source code in order to obtain
the complete semantic ﬂow graph of the code. And now we
obtained the code semantic representation graph, which has a
smaller number of nodes compared to AST, and also has data
ﬂow feature.

C. Context Representation

For natural language descriptions, current feature extrac-
tion models have achieved very good results in the ﬁeld
of natural language processing. For long texts, Transformer
can be selected for feature extraction, and since the length
of code description statements is generally short, our model
uses LSTM [38] for the extraction of semantic features of
descriptions.

D. Graph Representation

Code Semantic Representation Graph CSRG as a graph
structure, using GNN to encode can dig deeper into its node
features and patterns. GCN(Graph Convolutional Network)
[39] can perform convolutional operations on Graph. However,
GCN has some drawbacks: it relies on Laplacian matrix and
cannot be used directly on directed graphs; model training
relies on the whole graph structure and cannot be used on
dynamic graphs; different weights cannot be assigned to neigh-
boring nodes during convolution processing. Therefore Graph
Attention Network [40] GAT (Graph Attention Network) is
proposed to solve the problems of GCN, and we use GAT to
extract CSRG features of the code.

Fig. 3. Example of code semantic representation graph. There are two types
of edges in CSRG, one from AST and the other from DFG.

1https://tree-sitter.github.io/tree-sitter/

def f(a,b):x=a + breturn xCode SnippetAST EdgesDFG EdgesAST EdgesDFG EdgesAbstract Syntax TreeDataflow ChatCSRG GraphSuppose Graph contains N nodes, each node has a fea-
ture vector of hi and dimension F , denoted as h =
{h1, h2, ..., hN }, hi ∈ RF , a linear transformation of the node
feature vector h.

(cid:48)
i = W hi, W ∈ RF
h

(cid:48)

×F

(1)

interaction of the two sequences. A fusion layer unites the
alignment layer’s inputs and outputs. Finally the output of the
code and query are then sent to the pooling layer, converted
to a vector of ﬁxed length, and used as input for similarity
matching. implemented as shown in Figure 4.

(cid:48)
h(cid:48) = {h

(cid:48)

(cid:48)

1, h

2, ..., h

N }, h

(cid:48)

(cid:48)

i ∈ RF
denotes the dimension,

(2)

(cid:48)

(cid:48)

where h
i is the new eigenvector, F
and W is the linearly transformed matrix.

If node j is a neighbor of node i, the importance of node j
to node i can be calculated using the Attention mechanism,
the AttentionScore.

eij = Attention (W hi, W hj)

αij = Softmaxj (eij) =

(cid:80)

exp (eij)

exp (eik)

k∈Ni

(3)

(4)

(cid:48)

(cid:48)

The speciﬁc Attention approach of GAT is as follows: the
j of nodes i, j are spliced together and the
i, h
feature vectors h
inner product is calculated with a 2F
dimensional vector a.
The activation function uses LeakyReLU with the following
equation:

(cid:48)

αij =

(cid:80)

exp (cid:0) LeakyReLU (cid:0)aT [W hi(cid:107)W hj](cid:1)(cid:1)

exp ( LeakyReLU (aT [W hi(cid:107)W hk]))

k∈Ni

(5)

where (cid:107) indicates a splicing operation.

The feature vector of node i after Attention is as follows:





h(cid:48)
i = σ



(cid:88)

αijW hj



j∈Ni

(6)

where σ represents a nonlinear activation function.

E. CRESS Block(Cross And Residual)

that

Since the lexical and syntactic structures of code and natural
language are different, it is a great challenge to align code and
text across sequences to keep the vectors spatially consistent
during similarity matching. This is necessary for accurate
semantic relationship between code and query. Our task is
essentially to ﬁnd a kind of mapping from code snippet
is to ﬁnd the matching
to corresponding description,
relationship between code snippet and description, and how
to accurately learn the deeper matching relationship between
them is a major difﬁculty in the code search task [41]. At
the lexical level, we can consider code and description as two
different languages, and in order to better match two kinds
of texts just from words and sentences, this paper proposes a
matching module based on residuals and interactions, called
CRESS Block. The CRESS block uses bidirectional version of
residual connections to cross consecutive query text and code
text, it could provide more features for alignment procedures.
Speciﬁcally, the sequence encoder ﬁrst computes the contex-
tual features of the query text and code text sequences within
the CRESS block. The encoder’s inputs and outputs are then
concatenated and fed to the alignment layer, which uses an
enhanced residual concatenation to model the alignment and

Fig. 4. Details of a CRESS block. The operation xl+1 = x0(cid:12)(Wlxl + bl)+
xl is denoted as ⊗.

For a sequence of length l, we denote the input and output
) and o(n) =
) the input of the n’s block is the concat of

of the n’s block as x(n) = (x(n)
(o(n)
the ﬁrst input and the output of previous two blocks.

2 , ..., x(n)

2 , ..., o(n)

1 , x(n)

1 , o(n)

l

l

i = [x(1)
x(n)

i

; o(n−1)
i

+ o(n−2)
i

]

(7)

where [;] denotes the concat operation, which facilitates the
subsequent matching with the addition of residual information.
The cross layer based on the attention mechanism, takes
features from the two sequences as input and computes the
aligned representations as output. Input from the ﬁrst se-
quence of length la is denoted as a = (a1; a2; ...; ala) and
input from the second sequence of length lb is denoted as
b = (b1; b2; ...; blb ). The similarity score eij between ai and
bj is computed as the dot product of the projected vectors:

eij = F (ai)T F (bj)

(8)

F is an identity function or a single-layer feedforward net-
work. The choice is treated as a hyperparameter.The output
vectors a(cid:48) and b(cid:48) are computed by the similarity score eij:

(cid:48)
i =
a

b

(cid:48)

j =

lb(cid:88)

j=1

la(cid:88)

i=1

exp(eij)
k=1exp(eik)

(cid:80)lb

bj

exp(eij)
k=1exp(ekj)

(cid:80)la

ai

(9)

(10)

The residual vector is the output of the previous block. For
the ﬁrst block, its input and residual vectors are the original
embedding vectors in Figure 2(docstring vector, tokens vec-
tor).

Cross layerResidual vectorResidual vectorInput vectorInput vectorF. Attention Fusion

H. Online Code Search

By introducing the above modules, for the code we generate
two vectors, CodeT okensCRESS and CSRGGAT , and for
their corresponding descriptions we generate two vectors,
DocT okenslstm and DocT okensCRESS , and we need to
merge these corresponding vectors are fused into one vector,
and here we use the attention mechanism. In the text sequence
model, the importance of each word to the ﬁnal matching
result is different, so we need to calculate the attention score
for each word and obtain the ﬁnal vector by weighting.

T
(cid:88)

v =

αihi

(11)

i=1
where αi denotes the attention score corresponding to the
hidden state hi of the encoder.

Now that we have the vectors obtained by the different
modules and the attention scores corresponding to each vector,
we can obtain the code and describe the respective vectors.

CodeT okensCRESS = αtokRE2htokRE2

CSRGGAT = αCSRGhCSRG

Docslstm = αtoklstm htoklstm

DocsCRESS = αtokCRESS htokCRESS

xcode = [CodeT okensCRESS; CSRGGAT ]

xdocs = [DocsT okenslstm; DocsT okensCRESS]

(12)

(13)

(14)

(15)

(16)

(17)

where[; ]denotes the concat operation.

G. Loss Function

We have described above how to obtain the vectors cor-
responding to the code and the textual description. And we
already know that semantically more similar statements than
embedding them into the same high-dimensional vector space,
the corresponding vector distances obtained are also more
similar. In other words, given a code fragment x and a
descriptive statement d, if they are corresponding, then the
vector similarity of our model embedding has the highest
degree of similarity, and if they are not in correspondence, then
the vectors embedded in the model should have the smallest
degree of similarity. In the training phase, we use the triple
< x, d+, d− > to train the model, where d+ denotes the
description corresponding to the code segment x, and d−
denotes the description not related to the code segment, and we
want the model to maximize the similarity between < x, d+ >
and minimize the similarity between < x, d− >. In summary,
the model’s loss function is shown as follows:

L(θ) =

(cid:88)

max (cid:0)0, β − sim (cid:0)x, d+(cid:1) + sim (cid:0)x, d−(cid:1)(cid:1)

<x,d+,d−>∈D

(18)
where θ denotes the model parameters, D denotes the
training data set, β is a hyperparameter, sim denotes the
similarity score between two vectors, x, d+, d− denotes the
code fragment x, the description statement d+ corresponding
to the code x,and description statements d− not corresponding
to code fragments with the same dimension after model
embedding.

Given a set χ of code segments to be searched, for an input
query q, the model needs to sort the similarity of all code
segments in the database and select the set x1, x2, ..., xk of k
code segments that are closest to the query q. For the input
query q, the similarity between q and x is calculated by cosine
similarity for each code segment x in the set of code snippets
as follows:

sim(x, q) = cos(x, q) =

xT q
(cid:107)x(cid:107)(cid:107)q(cid:107)

(19)

where x and q denote the vectors of code segments and query
statements, respectively. The larger the value of similarity, the
higher the relevance of the corresponding code segment and
query statement.

IV. EXPERIMENTS

A. General Settings

To train our proposed model, we ﬁrst randomize the training
data and set the mini-batch size to 32. For each batch, the code
is padded with a special token < P AD > to the maximum
length. All tokens in our dataset are converted to lower case.
We set the word embedding size to 300. For LSTM unit, we set
the hidden size to be 256. For CRESS, set 4 blocks of iteration.
The margin β is set to 0.05. We update the parameters via
Adam [42] optimizer with the learning rate 0.0001. To prevent
over-ﬁtting, we use dropout with 0.2. All the models in this
paper are trained for 100 epochs. All the experiments are
implemented using the PyTorch 1.2 framework with Python
3.8, and the experiments were conducted on HPC with four
Nvidia Tesla 100 GPU with 16 GB memory, running CentOS
7.5.

B. Baselines

We have selected the following models to compare with

ours.

• CodeHow [2]: It is a classical code search engine pro-
posed in the previous years. It is based on information re-
trieval code search tool that contains an extended Boolean
model and API matching.

• DeepCS [3]: A code retrieval method based on deep
neural networks. By embedding the source code and
description into the same vector space to perform the
code-description matching search.

• MPCAT [33]: a code search model using hierarchical
traversal method to encode code abstract syntax trees and
incorporating text matching model BiMPM model.

• TabCS [43]: A code search model using two-stage atten-
tion for feature extraction, while using association matrix
for parameter sharing, which achieves good results.

C. Datasets

We train and evaluate the model on two publicly available
datasets. One is the Hu’s [44] dataset2 that contains 480k

2https://github.com/xing-hu/EMSE-DeepCom

code-query pairs. Hu’s dataset was collected from GitHub’s
Java repositories created from 2015 to 2016. To ﬁlter out low-
quality projects, Hu et al. [44] only considered the projects
with more than ten stars. Then, they extracted Java methods
and their corresponding Java doc from these Java projects. The
ﬁrst sentence of the Javadoc is considered as the query. The
other is the Husain’s [45] dataset3. The corpus contains 540k
Java code with queries written in natural language collected
from GitHub repositories. We ﬁlter out
the code snippets
following this criterion: the ﬁrst sentence of its annotation
should be longer than two words. After ﬁltering, we obtain a
train set containing 330k annotation-function pairs and a test
set containing 19k annotation-function pairs. The statistics of
the two datasets are shown in Table 1.

TABLE I
DATASETS DETAILS

Dataset
Train
Test
Avg. tokens in comment
Avg. tokens in code
Max tokens in comment
Max tokens in code

Hu’s dataset
475812
10000
10.25
58.6
32
841

Husain’s dataset
329967
19015
10.2
68.5
440
369

D. Evaluation Metrics

1) MRR(Mean Reciprocal Rank): This is a commonly used
metric to measure the effectiveness of search algorithms, and is
now widely used in problems that allow multiple results to be
returned, or problems that are currently difﬁcult to solve (since
the accuracy or recall would be poor if only top1 results were
returned, multiple results are returned ﬁrst in cases where the
technology is not mature). In such problems, the system gives
a conﬁdence level (score) to each returned result, and then
sorts the results according to the conﬁdence level, returning
the results with high scores ﬁrst.

For a query set Q, the set of returned results is q, the correct
result appears at F Rank, and the score is the reciprocal of
F Rank, then M RR is:

M RR =

1
|Q|

|Q|
(cid:88)

q=1

1
F Rankq

(20)

Higher MRR values indicate better performance of the code
search model.

2) SuccessRate@k(Success Percentage at k): This metric
measures the percentage of the ﬁrst k results for which one or
more correct results may exist, and is calculated as follows:

SuccessRate@k =

1
|Q|

Q
(cid:88)

q=1

δ (F Rank q ≤ k)

(21)

where δ is a function, the output is 1 when the input is true,
otherwise the output should be 0. A good code search engine
should place the correct results as close to the front of the

3https://github.com/github/CodeSearchNet

return value as possible, so that users can easily ﬁnd the results
they need more quickly, and similarly, the higher the R@k
value, the better the performance of the code search model.

3) Normalized Discounted Cumulative Gain(NDCG): The
normalized discounted cumulative gain is used as an evaluation
metric for the sorting results to evaluate the accuracy of the
sorting. Recommender systems usually return a item list for
a user, and assuming the list
the difference
between this sorted list and the user’s real interaction list can
be evaluated with N DCG@K.

length is K,

N DCG@k =

k
(cid:88)

i=1

2r(i) − 1
log2(i + 1)

(22)

where r(i) is the score of the ith result. In the code search task,
only correct or incorrect, the corresponding scores are 1 and
0. In our experiments, N DCG@50 is taken as the evaluation
index considering the dataset size.

E. Implementation Details

1) Tokenization: As a natural language, it is enough to sep-
arate words according to their spacing, but for programming
languages, there are elements such as hump nomenclature and
a lot of symbolic language. For hump nomenclature, such
into three words: “get”,
as “getFileNam”, we can split
“ﬁle” and “name”. For the large number of symbols in the
programming language, in some papers all the symbols are
removed, leaving only the words, but in fact the symbols in
the code also contain a lot of semantic information, so in this
paper, we keep the symbols in the code syntax

it

2) Bias of Code Semantic Representation Graph: To distin-
guish between the two types of edge information in the code
semantic graph. After a variety of weight settings, we found
that the weight setting of the edge in CSRG has little effect
on the accuracy of the search results. We set the weight of the
edge generated by AST to 0.4, the weight of the DFG edges
is set to 0.6, can achieve a relatively better result.

V. RESULTS

To evaluate our proposed approach,

in this section, we

conduct experiments to answer the following questions:

• RQ1. Does our proposed approach improve the perfor-
mance of code retrieval when compared with state-of-
the-art approaches?

• RQ2. What is the effectiveness and the contribution of
each strategy e.g., CRESS layer, CSRG of source code
for the ﬁnal retrieval performance, and what about their
combinations?

• RQ3. What’s better about using CSRG than using AST

or DFG?

• RQ4. What is the performance of our proposed model
when varying the CRESS block number, code CSRG
node number?

• RQ5. Is the search result of CSSAM better than state-of-

the-art approaches?

We ask RQ1 to evaluate our deep learning-based model
compared to some state-of-the-art baselines, which will be

TABLE II
ABLATION STUDY

Model
Base
Base+CSRG
Base+CRESS
Base+CRESS+CSRG
Base+CRESS+CSRG+Attn

SR@1
0.1426
0.1982
0.2286
0.2611
0.3221

SR@5
0.2901
0.3390
0.4642
0.4893
0.6362

SR@10 MRR
0.2313
0.4147
0.2553
0.4865
0.3407
0.5735
0.3854
0.6122
0.4831
0.7051

NDCG@50
0.3065
0.3267
0.4327
0.4797
0.5843

described in the following subsection. We ask RQ2 in order
to evaluate the performance of each module. We ask RQ3 to
analyze the performance of our proposed CSRG. We ask RQ4
to analyze the sensitivity of our proposed model when varying
the CRESS block number, code CSRG node number. We ask
RQ5 to verify the searching results of our proposed CSSAM.

A. RQ1: Does our proposed approach improve the perfor-
mance of code retrieval when compared with state-of-the-art
approaches?

We trained and tested CodeHow, DeepCS, TabCS, MP-
CAT, and our model CSSAM on the same dataset, and the
experimental data are shown in Tables 3 and 4. From the
data, our model outperforms three deep learning-based models
(DeepCS, TabCS, MPCAT) and one information retrieval-
based model codeHow.

For Hu et al.’s dataset, the results in Table 3, our model
achieves an MRR value of 0.483 and 0.322/0.636/0.705
in SR@/1/5/10, CSSAM outperforms CodeHow, DeepCS,
TabCS, and MPCAT 80.39%, 70.04%, 4.82%, 2.33% in MRR,
respectively. For Husain’s dataset, the results in Table 4, our
model achieves an MRR value of 0.394 and 0.259/0.491/0.575
in SR@/1/5/10, CSSAM outperforms CodeHow, DeepCS,
TabCS, and MPCAT in MRR and SR@k.

The experimental results show that our model performs

better than the above baseline model.

B. RQ2: What is the effectiveness and the contribution of each
strategy e.g., CRESS layer, CSRG of source code for the ﬁnal
retrieval performance, and what about their combinations?

We did ﬁve sets of ablation experiments based on the model
to verify the effect of each module on the experimental results.
Table 2 shows the effect of each module on the experimental
results. From the experimental data, we can see that both
the semantic level matching module and the structural level
matching module have positive effects on the experimental
results, and the model performs better after fusing the match-
ing modules of each level than using these modules alone,
which also indicates that the complementary effects between
the matching layers at different levels outweigh the conﬂicts
between them. The experimental results also demonstrate the
positive contribution of fused attention layers to the model
effects by adding and removing Attention layers.

As we can see in table 2, each strategy has a positive
impact on search results. CSRG brings structure information
to the model, and the code features are more complete. CRESS
matches the code and description from the word level, which

TABLE III
EXPERIMENTS ON HU ET AL.’S DATASET.

Model
CodeHow
DeepCS
MPCAT
TabCS
CSSAM

SR@1
0.2341
0.2574
0.2997
0.3152
0.3221

SR@5
0.3585
0.3897
0.4946
0.5744
0.6362

SR@10 MRR
0.2678
0.4944
0.2834
0.5169
0.4721
0.6642
0.4609
0.6831
0.4831
0.7051

NDCG@50
0.3411
0.3690
0.4891
0.5492
0.5843

TABLE IV
EXPERIMENTS ON HUSAIN’S DATASET

Model
CodeHow
DeepCS
MPCAT
TabCS
CSSAM

SR@1
0.1671
0.1460
0.1711
0.2259
0.2585

SR@5
0.3947
0.3562
0.3776
0.4195
0.4912

SR@10 MRR
0.2459
0.4420
0.2243
0.4036
0.2936
0.4414
0.3487
0.4910
0.3941
0.5754

NDCG@50
0.2901
0.2617
0.3496
0.3873
0.4223

increases the performance of the model at the ﬁne-grained
level. ” ”Unity is strength”, which is more obvious under the
blessing of the attention mechanism.

C. RQ3: What’s better about using CSRG than using AST or
DFG?

In order to explore why CSRG is effective, we conducted
performance tests according to different graph structures. The
results are shown in Table 5. We can ﬁnd that using graph
structure information can signiﬁcantly improve the code search
results, but it will also bring the burden of training. Compared
with only using AST, our proposed CSRG signiﬁcantly re-
duces the complexity of input data and improves the training
speed. Compared with just using DFG, our CSRG signiﬁcantly
improves the accuracy of search results.

TABLE V
TRAINING DETAILS

Model
CSSAM-w/o.Graph
CSSAM-w.AST
CSSAM-w.DFG
CSSAM-w.CSRG

Training time MRR Score

16.2 hours
27.1 hours
18.0 hours
24.8 hours

0.292
0.337
0.304
0.348

There is no doubt that structural information is very useful
for code search. After adding AST and DFG information, the
code search MRR is improved. We all know that ast has more
node information than DFG, which also leads to the training
time of AST model is longer than DFG model. And using
CSRG, although the training time is increased compared with
baseline, it is less than that of AST. At the same time, MRR
is also higher than that of AST and DFG models.

D. RQ4: What is the performance of our proposed model
when varying the CRESS block number, code CSRG node
number?

other hand, CSSAM not only matches at the tokens level, but
also takes into account the deep semantic information of the
code, so the search results are more accurate.

In order to explore the robustness of this model, we analyzed
two hyperparameters that may affect the effect of the model,
which are number of CSRG nodes. Figure 5 shows the effects
of each parameter on the model evaluation metrics, and ﬁrstly,
we can see that our model performs relatively stable when
using different hyperparameters, the robustness of our model
can be proved.

Fig. 5. Sensitivity Analysis

We have tried the number of CRESS blocks from 0 to 5. It
can be found that with the increase of the number of layers, all
the indicators of the model show an upward trend, but followed
by a signiﬁcant increase in the consumption of the model. We
ﬁnd that when the number of cress is set to 5, the growth of the
model indicators has slowed down. Taking a comprehensive
consideration, we take 4 as the optimal parameter.

For the number of CSRG nodes, at ﬁrst, as the number of
nodes increases, the indicators are in a stable growth trend, but
when the number of nodes exceeds 80, the indicators appear
to decline, and ﬁnally we tried the case of 300 nodes, the
indicators are still not as good as the optimal point, for this
phenomenon, the reason of occurrence is related to the dataset,
when the number of nodes is close to the average of this
dataset, the model can be made optimal, too few nodes cannot
provide enough useful information, too many nodes means
more padding, which is equivalent to diluting the information
and cannot achieve the optimal performance of the model.

E. RQ5: Is the search result of CSSAM better than state-of-
the-art approaches?

Figure 6 shows the ﬁrst

retrieved results of CSSAM
and other models, for queries “save string into the ﬁle”
and“converts the given hex string into a plain string”. We can
notice the CSSAM returns the correct code snippet.But neither
they just
DeepCS nor TabCS returned the correct results,
matched the correlation between the code and the description
at the tokens level. The fusion of DFG and AST enriches
the semantic and associative information present in the nodes
inside the code snippet, and the CRESS block we proposed
further aligns the textual features of the code and query
semantically,
to learn additional
contextual features based on the sequence structure. On the

thus allowing our model

Fig. 6. First retrieved results for different models

VI. DISCUSSION

A. Strength of CSSAM

We have identiﬁed three advantages of CSSAM, which
may explain its effectiveness in code retrieval: (a) a more
comprehensive representation of source code. CSSAM uses
CSRG, which contains supplementary information in addition
to the code tokens representation. (b) A ﬁne-grained matching
module cress, we have proved the effectiveness of this module
through experiments. And (c) a learning the uniﬁed framework
source code of heterogeneous representation and the interme-
diate semantic space of description. CSSAM is an end-to-end
neural network model, which uses a uniﬁed architecture to
learn the source code to represent the code and description in
the intermediate semantic space.

B. Threats to Validity

Our proposed CSSAM may suffer from two threats to
to validity is on the

validity and limitations. One threat

Query：save string into the fileCode:public static void writeToFile(File file,String contents) throws IOException {FileOutputStream fos=new FileOutputStream(file); fos.write(contents.getBytes()); fos.close();}Query：converts the given hex string into a plain stringCode:public static String hexToStringNoException(final String data){    try {        return ConversionUtils.arrayToString(Hex.decodeHex(data.toCharArray()));        }        catch (  DecoderException e) {            return \"[invalid hex]\";            }}Query：save string into the fileCode:public static void write(File file,CharSequence data,Charset encoding,boolean append) throws IOException {  String str=data == null ? null : data.toString();  writeStringToFile(file,str,encoding,append);}Query：converts the given hex string into a plain stringCode:public static String[] convertStrings(List<String> strings){  String[] ret=new String[strings.size()];  for (int i=0; i < ret.length; i++) {    ret[i]=strings.get(i);  }  return ret;}(a)  TSMCS’s first retrieved result(b)  TabCS’s first retrieved result(c)  TSMCS’s first retrieved result(d)  DeepCS’s first retrieved result(a)  CRESS block number(b)  CSRG node numberQuery：save string into the fileCode:public static void writeToFile(File file,String contents) throws IOException {FileOutputStream fos=new FileOutputStream(file);fos.write(contents.getBytes());fos.close();}Query：converts the given hex string into a plain stringCode:public static String hexToStringNoException(final String data){ try {   return ConversionUtils.arrayToString(Hex.decodeHex(data.toCharArray())); } catch (  DecoderException e) { return \"[invalid hex]\"; }}Query：save string into the fileCode:public static void write(File file,CharSequence data,Charset encoding,boolean append) throws IOException {  String str=data == null ? null : data.toString();  writeStringToFile(file,str,encoding,append);}Query：converts the given hex string into a plain stringCode:public static String[] convertStrings(List<String> strings){  String[] ret=new String[strings.size()];  for (int i=0; i < ret.length; i++) { ret[i]=strings.get(i);  }  return ret;}(a) CSSAM’s first retrieved result(b)TabCS’s first retrieved result(c) CSSAM’s first retrieved result(d)DeepCS’s first retrieved resultevaluation metrics. In practice, there are multiple related code
snippets for a query. However, for automated evaluation, each
query has limited correct code snippets. During the evaluation,
the other results, which are also related, can not be recognized
unless human involves.

Another threat to validity lies in the extensibility of our
proposed approach. Our model has only been trained and
tested on Java data sets. For other programming languages,
the model results are unknown. At the same time, because
CSSAM needs to use CSRG, but this graph structure needs
to be generated by our own tools. At present, it is difﬁcult to
extract CSRG from any language, which is also the focus of
our future work.

VII. CONCLUSION AND OUTLOOK

In this paper, we propose a code search model CSSAM
based on dual-level matching of semantics and structure,
which not only considers the matching relationship between
code and description at the semantic level, but also proposes
CSRG to match code and description at the structural level
based on code AST, and adds an attention mechanism to
balance the matching results at each level, and experiments
prove that our model is effective and achieves state-of-the-art,
which exceeds the existing code search models.

In our

future work, we plan to continue to improve
the model’s effectiveness, and hope to achieve language-
independence, as the current model performs differently on
different language datasets, and the extraction and generation
of CSRG cannot be done for all languages. At the same time,
our work may also have contributions in other areas related
to code such as code generation and code summarization, and
this aspect is also worthy of our research.

REFERENCES

[1] C. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie, and C. Fu,
“Portfolio: ﬁnding relevant functions and their usage,” in Proceedings
of the 33rd International Conference on Software Engineering, ICSE
2011, Waikiki, Honolulu , HI, USA, May 21-28, 2011. ACM, 2011,
pp. 111–120.

[2] L. Fei, H. Zhang, J. G. Lou, S. Wang, and J. Zhao, “Codehow: Effective
code search based on api understanding and extended boolean model
(e),” in 2015 30th IEEE/ACM International Conference on Automated
Software Engineering (ASE), 2015.

[3] X. Gu, H. Zhang, and S. Kim, “Deep code search,” in Proceedings of
the 40th International Conference on Software Engineering, ICSE 2018,
Gothenburg, Sweden, May 27 - June 03, 2018. ACM, 2018, pp. 933–
944.

[4] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “code2vec: learning
distributed representations of code,” Proc. ACM Program. Lang., vol. 3,
no. POPL, pp. 40:1–40:29, 2019.

[5] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment
generation,” in Proceedings of the 26th Conference on Program Com-
prehension, ICPC 2018, Gothenburg, Sweden, May 27-28, 2018. ACM,
2018, pp. 200–210.

[6] Y. Wan, J. Shu, Y. Sui, G. Xu, Z. Zhao, J. Wu, and P. S. Yu, “Multi-
modal attention network learning for semantic source code retrieval,”
in 34th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019.
IEEE, 2019, pp. 13–25.

[7] Y. Li, D. Tarlow, M. Brockschmidt, and R. S. Zemel, “Gated graph
sequence neural networks,” in 4th International Conference on Learning
Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings, 2016.

[8] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou,
B. Qin, T. Liu, D. Jiang, and M. Zhou, “Codebert: A pre-trained
model for programming and natural languages,” in Findings of the
Association for Computational Linguistics: EMNLP 2020, Online
Event, 16-20 November 2020, ser. Findings of ACL, T. Cohn,
Y. He, and Y. Liu, Eds., vol. EMNLP 2020. Association for
Computational Linguistics, 2020, pp. 1536–1547. [Online]. Available:
https://doi.org/10.18653/v1/2020.ﬁndings-emnlp.139

[9] Y. Wang, W. Wang, S. R. Joty, and S. C. H. Hoi, “Codet5:
Identiﬁer-aware uniﬁed pre-trained encoder-decoder models for code
understanding and generation,” in Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021,
M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for
Computational Linguistics, 2021, pp. 8696–8708. [Online]. Available:
https://doi.org/10.18653/v1/2021.emnlp-main.685

[10] L. B¨uch and A. Andrzejak, “Learning-based recursive aggregation of
abstract syntax trees for code clone detection,” in 26th IEEE Interna-
tional Conference on Software Analysis, Evolution and Reengineering,
SANER 2019, Hangzhou, China, February 24-27, 2019.
IEEE, 2019,
pp. 95–104.

[11] S. Luan, D. Yang, C. Barnaby, K. Sen, and S. Chandra, “Aroma: code
recommendation via structural code search,” Proc. ACM Program. Lang.,
vol. 3, no. OOPSLA, pp. 152:1–152:28, 2019.

[12] X. Ling, L. Wu, S. Wang, G. Pan, T. Ma, F. Xu, A. X. Liu, C. Wu, and
S. Ji, “Deep graph matching and searching for semantic code retrieval,”
ACM Trans. Knowl. Discov. Data, vol. 15, no. 5, pp. 88:1–88:21, 2021.
[13] D. Z¨ugner, T. Kirschstein, M. Catasta, J. Leskovec, and S. G¨unnemann,
“Language-agnostic representation learning of source code from struc-
ture and context,” in 9th International Conference on Learning Repre-
sentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Open-
Review.net, 2021.

[14] T. Xia, Y. Wang, Y. Tian, and Y. Chang, “Using prior knowledge to
guide bert’s attention in semantic textual matching tasks,” in WWW ’21:
The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April
19-23, 2021. ACM / IW3C2, 2021, pp. 2466–2475.

[15] K. Swanson, L. Yu, and T. Lei, “Rationalizing text matching: Learning
sparse alignments via optimal transport,” in Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020. Association for Computational Linguis-
tics, 2020, pp. 5609–5626.

[16] S. Wang, Y. Lan, Y. Tay, J. Jiang, and J. Liu, “Multi-level head-
wise match and aggregation in transformer for textual sequence match-
ing,” in The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence,
AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial
Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on
Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020. AAAI Press, 2020, pp. 9209–9216.

[17] Q. V. Le and T. Mikolov, “Distributed representations of sentences and
documents,” in Proceedings of the 31th International Conference on
Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, ser.
JMLR Workshop and Conference Proceedings, vol. 32.
JMLR.org,
2014, pp. 1188–1196.

[18] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, “A neural proba-
bilistic language model,” J. Mach. Learn. Res., vol. 3, pp. 1137–1155,
2003.

[19] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of
word representations in vector space,” in 1st International Conference on
Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May
2-4, 2013, Workshop Track Proceedings, 2013.

[20] G. Salton and C. T. Yu, “On the construction of effective vocabularies
for information retrieval,” in Proceedings of
the 1973 meeting on
Programming languages and information retrieval, SIGPLAN 1973,
Gaithersburg, Maryland, USA, November 4-6, 1973. ACM, 1973, pp.
48–60.

[21] S. E. Robertson and S. Walker, “Some simple effective approximations
to the 2-poisson model for probabilistic weighted retrieval,” in Pro-
ceedings of the 17th Annual International ACM-SIGIR Conference on
Research and Development in Information Retrieval. Dublin, Ireland, 3-
6 July 1994 (Special Issue of the SIGIR Forum). ACM/Springer, 1994,
pp. 232–241.

[22] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A.
Harshman, “Indexing by latent semantic analysis,” J. Am. Soc. Inf. Sci.,
vol. 41, no. 6, pp. 391–407, 1990.

[40] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and
Y. Bengio, “Graph attention networks,” in 6th International Conference
on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018.

[41] R. Wang, R. Shivanna, D. Z. Cheng, S. Jain, D. Lin, L. Hong, and E. Chi,
“DCN V2: improved deep & cross network and practical lessons for
web-scale learning to rank systems,” in WWW ’21: The Web Conference
2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. ACM /
IW3C2, 2021, pp. 1785–1797.

[42] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,” in 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, 2015.

[43] L. Xu, H. Yang, C. Liu, J. Shuai, M. Yan, Y. Lei, and Z. Xu,
“Two-stage attention-based model for code search with textual and
structural features,” in 28th IEEE International Conference on Software
Analysis, Evolution and Reengineering, SANER 2021, Honolulu, HI,
USA, March 9-12, 2021.
IEEE, 2021, pp. 342–353. [Online]. Available:
https://doi.org/10.1109/SANER50967.2021.00039

[44] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment
information,” Empir.
generation with hybrid lexical and syntactical
Softw. Eng., vol. 25, no. 3, pp. 2179–2217, 2020. [Online]. Available:
https://doi.org/10.1007/s10664-019-09730-9

[45] H. Husain, H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
“Codesearchnet challenge: Evaluating the state of semantic code
search,” CoRR, vol. abs/1909.09436, 2019.
[Online]. Available:
http://arxiv.org/abs/1909.09436

[23] T. Hofmann, “Probabilistic latent semantic analysis,” in UAI ’99:
Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial
Intelligence, Stockholm, Sweden, July 30 - August 1, 1999. Morgan
Kaufmann, 1999, pp. 289–296.

[24] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
in Advances in Neural Information Processing Systems 14 [Neural
Information Processing Systems: Natural and Synthetic, NIPS 2001,
December 3-8, 2001, Vancouver, British Columbia, Canada]. MIT
Press, 2001, pp. 601–608.

[25] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” in 3rd International Conference
on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceedings, 2015.

[26] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, “Image captioning with
semantic attention,” in 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016.
IEEE Computer Society, 2016, pp. 4651–4659.

[27] X. Li, B. Zhao, and X. Lu, “MAM-RNN: multi-level attention model
based RNN for video captioning,” in Proceedings of the Twenty-Sixth
International Joint Conference on Artiﬁcial Intelligence, IJCAI 2017,
Melbourne, Australia, August 19-25, 2017.
ijcai.org, 2017, pp. 2208–
2214.

[28] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for
abstractive sentence summarization,” in Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language Processing, EMNLP
2015, Lisbon, Portugal, September 17-21, 2015. The Association for
Computational Linguistics, 2015, pp. 379–389.

[29] Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu,
“Improving automatic source code summarization via deep reinforce-
ment learning,” in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, ASE 2018, Montpellier,
France, September 3-7, 2018. ACM, 2018, pp. 397–407.

[30] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst,
“Geometric deep learning: Going beyond euclidean data,” IEEE Signal
Process. Mag., vol. 34, no. 4, pp. 18–42, 2017.

[31] W. L. Hamilton, R. Ying, and J. Leskovec, “Representation learning on
graphs: Methods and applications,” IEEE Data Eng. Bull., vol. 40, no. 3,
pp. 52–74, 2017.

[32] K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic rep-
resentations from tree-structured long short-term memory networks,”
in Proceedings of
the Association for
the 53rd Annual Meeting of
Computational Linguistics and the 7th International Joint Conference
on Natural Language Processing of the Asian Federation of Natural
Language Processing, ACL 2015, July 26-31, 2015, Beijing, China,
Volume 1: Long Papers.
The Association for Computer Linguistics,
2015, pp. 1556–1566.

[33] R. Haldar, L. Wu, J. Xiong, and J. Hockenmaier, “A multi-perspective
architecture for semantic code search,” in Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, ACL 2020,
Online, July 5-10, 2020. Association for Computational Linguistics,
2020, pp. 8563–8568.

[34] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J´egou, and T. Mikolov,
“Fasttext.zip: Compressing text classiﬁcation models,” arXiv preprint
arXiv:1612.03651, 2016.

[35] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word
vectors with subword information,” arXiv preprint arXiv:1607.04606,
2016.

[36] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: online learning
of social representations,” in The 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD ’14, New
York, NY, USA - August 24 - 27, 2014. ACM, 2014, pp. 701–710.
[37] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of
word representations in vector space,” in 1st International Conference on
Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May
2-4, 2013, Workshop Track Proceedings, 2013.

[38] S. Hochreiter and J. Schmidhuber, “LSTM can solve hard long time lag
problems,” in Advances in Neural Information Processing Systems 9,
NIPS, Denver, CO, USA, December 2-5, 1996. MIT Press, 1996, pp.
473–479.

[39] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph
convolutional networks,” in 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Con-
ference Track Proceedings. OpenReview.net, 2017.

