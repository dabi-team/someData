2
2
0
2

y
a
M
5

]
E
S
.
s
c
[

1
v
4
4
3
3
0
.
5
0
2
2
:
v
i
X
r
a

Shortcomings of Class-level Documentation: A Survey

Jianwei Wu1, Jining Yu2, David Shepherd3, and James Clause4

1,2,4University of Delaware
3Virginia Commonwealth University

To better understand the shortcomings of class-level documentation we conducted a survey of 167 expe-
rienced software developers recruited from from ABB, Inc., the University of Delaware (UDel), and popular,
on-line programming forums (e.g., the programming subreddit1). As part of this survey, we asked partici-
pants two questions: “Q1: How frequently do you read class-level documentation because you want to learn:
infor-
information need ?” and “Q2: How frequently does class-level documentation suﬃciently explain:
mation need ?”. Responses to both of these questions were collected using a 3-point Likert scale: Rarely,
Sometimes, Often. The speciﬁc information needs we asked about were selected based on our experience
and knowledge of class-level documentation as well as investigations into questions developers ask during
software development (e.g., [1–5].)

Figure 1 presents two stacked bar charts that show the distribution of participant responses to our survey
questions. The left-hand chart shows the responses to Q1 and the right-hand chart shows the responses to
Q2. In each chart, the y-axis shows the information needs that were asked about and the stacked bars show
the proportion of participants (x-axis) who answered Rarely (left, green), Sometimes (middle, orange), and
Often (right, purple). For example, the top-most stacked bar in the left-hand chart shows that when asked
“How frequently does class-level documentation suﬃciently explain: The purpose of the class”, ≈18 % of
participants answered Rarely, ≈36 % answered Sometimes, and ≈46 % answered Often.

To help us understand participants’ views on the quality of class-level documentation and to identify
opportunities for improvement, we calculated each participant’s annoyance level with respect to each infor-
mation need. Intuitively, a participant’s annoyance level is a relative approximation of how frequently the
participant is frustrated when attempting to satisfy an information need that takes into account both how
frequently they have the information need (Q1) and how frequently class-level documentation suﬃciently
explains the information need (Q2). For example, if a participant believes that class-level documentation
Often satisﬁes an information need they are unlikely to be frustrated, regardless of how often they want to
learn about the need, and their annoyance level should be negligible. Conversely, if a participant believes
that class-level documentation Rarely or Sometimes satisﬁes an information need, they are likely to be frus-
trated in proportion to how frequently they want to learn about the need, with the highest level of annoyance
occurring when they Often want to learn about a need that is Rarely satisﬁed.

To formalize the intuitive deﬁnition of annoyance described above, we deﬁne a participant’s annoyance

level a with respect to an information need need as follows:

(cid:40)

aneed =

0,
rank(Q1need) − rank(Q2need) + 2, otherwise

if Q2need is Often

where Q1need and Q2need are the participant’s responses to Q1 and Q2 when asked about need, respectively,
and rank returns the rank of the response where Often > Sometimes > Rarely. For example, if, when asked
about need, a participant responded Rarely to Q1 (rank(Q1need) = 1) and Sometimes to Q2 (rank(Q2need) =
2), their annoyance level would be 1 (aneed = 1 − 2 + 2 = 1). Alternatively, if they responded Sometimes

1 https://www.reddit.com/r/programming/

1

 
 
 
 
 
 
Fig. 1: Distribution of participant responses to “Q1: How frequently do you read class-level documentation
because you want to learn:” (left) and “Q2: How frequently does class-level documentation suﬃciently
explain:” (right) for each information need.

Fig. 2: Distribution of participant annoyance levels for each information need sorted by proportion of par-

ticipants with non-zero annoyance levels (a > 0) in decreasing order from top to bottom.

to Q1 (rank(Q1need) = 2) and Rarely to Q2 (rank(Q2need) = 1), their annoyance level would be 3 (aneed =
2 − 1 + 2 = 3). The output of this equation ranges from 0 to 4, with 0 indicating that the participant is likely
not annoyed by the information need (i.e., when a participant believes that class-level documentation Often
satisﬁes an information need, regardless of how frequently they want to know about the need) and numbers
greater than zero indicating increasing levels of annoyance with 4 as the highest level of annoyance (i.e.,
when a participant Often wants to learn about an information need that class-level documentation Rarely
suﬃciently explains).

Figure 2 shows the distribution of annoyance levels for our participants as a stacked bar chart. As in
Figure 1, the y-axis shows the information needs that were asked about and the stacked bars show the
proportion of participants at each annoyance level (x-axis). To simplify interpretation, annoyance levels are
sorted in increasing order from left to right with the least annoyance level colored gray and higher-levels
of annoyance shown in increasingly darker shades of red. For example, the top stacked bar shows that for
Risks/workarounds related to performance, ≈9 % of respondents have an annoyance level of 4, ≈31 % of
respondents have an annoyance level of 3, ≈48 % of respondents have an annoyance level of 2, and ≈6 %
of respondents have an annoyance level of 1, while only ≈6 % of respondents have an annoyance level of 0.
To simplify comparisons between the overall levels of annoyance for each information need, the information
needs are sorted in decreasing order based on the percentage of participants with a non-zero annoyance level.
As a result, the information need that the largest proportion of respondents were annoyed with to some
degree (Risks/workarounds related to performance, a > 0, ≈94 %) is shown at the top of the ﬁgure and
the information need that the smallest proportion of respondents were annoyed with to some degree (What
features the class provides a > 0, ≈50 %) is shown at the bottom.

2

Q1: How frequently do you read class−leveldocumentation because you want to learn:Q2: How frequently does class−leveldocumentation sufficiently explain:0%25%50%75%100%0%25%50%75%100%Risks/workarounds related to deprecationRisks/workarounds related to securityRisks/workarounds related to concurrencyThe class's place in the broader designHow to subclass the classHow to accomplish a specific taskHow to create an instance of the classWhat features the class providesRisks/workarounds related to performanceThe purpose of the classPercentage of participantsInformation needResponseRarelySometimesOftenWhat features the class provides (a>0, 50%)The purpose of the class (a>0, 54%)How to create an instance of the class (a>0, 65%)How to accomplish a specific task (a>0, 81%)Risks/workarounds related to deprecation (a>0, 84%)How to subclass the class (a>0, 90%)The class's place in the broader design (a>0, 90%)Risks/workarounds related to security (a>0, 90%)Risks/workarounds related to concurrency (a>0, 91%)Risks/workarounds related to performance (a>0, 94%)0%25%50%75%100%Percentage of participantsInformation needAnnoyanceLevel (a)01234The predominance of red this ﬁgure clearly shows that developers are annoyed with class-level documen-
tation. For example, there are ﬁve information needs where more than 90 % of participants have a non-zero
level of annoyance. Moreover, even in the case of the least annoying information need (What features the
class provides), 50 % of participants have a non-zero annoyance level. This result strongly motivates our
proposed work by demonstrating (1) the importance of class-level documentation to developers as a poten-
tial resource for satisfying information needs (i.e., developers often consult class-level documentation), and
(2) the inability of class-level documentation to regularly satisfy such information needs.

References

[1] Jonathan Sillito, Gail C Murphy, and Kris De Volder. Questions programmers ask during software

evolution tasks. In Proc. Int’l. Symposium Foundations of Software Eng., pages 23–34, 2006.

[2] Eduardo Cunha Campos and Marcelo de Almeida Maia. Automatic categorization of questions from

Q&A sites. In Proceedings of the Symposium on Applied Computing, pages 641–643, 2014.

[3] Anton Barua, Stephen W. Thomas, and Ahmed E. Hassan. What are developers talking about? An
analysis of topics and trends in stack overﬂow. Empirical Software Engineering, 19(3):619–654, 2014.

[4] Thomas D. LaToza and Brad A. Myers. Hard-to-answer questions about code. In Workshop on Evaluation

and Usability of Programming Languages and Tools, pages 8:1–8:6, 2010.

[5] W. Maalej and M. P. Robillard. Patterns of knowledge in API reference documentation. IEEE Transac-

tions on Software Engineering, 39(9):1264–1282, September 2013.

3

