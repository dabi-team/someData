Automated Feedback Generation for Competition-Level Code
De Li
The MathWorks, Inc.
Natick, Massachusetts, USA

Jialu Zhang
Yale University
New Haven, Connecticut, USA

John C. Kolesar
Yale University
New Haven, Connecticut, USA

Hanyuan Shi
Independent Researcher
Hangzhou, Zhejiang, China

Ruzica Piskac
Yale University
New Haven, Connecticut, USA

2
2
0
2

n
u
J

3

]
E
S
.
s
c
[

1
v
8
4
8
1
0
.
6
0
2
2
:
v
i
X
r
a

ABSTRACT
Competitive programming has become a popular way for program-
mers to test their skills. Large-scale online programming contests
attract millions of experienced programmers to compete against
each other. Competition-level programming problems are challeng-
ing in nature, and participants often fail to solve the problem on
their first attempt. Some online platforms for competitive program-
ming allow programmers to practice on competition-level problems
as well, and the standard feedback for an incorrect practice submis-
sion is the first test case that the submission fails. Often, the failed
test case does not provide programmers with enough information
to resolve the errors in their code, and they abandon the problem
after several more unsuccessful attempts.

We present Clef, the first data-driven tool that can generate
feedback on competition-level code automatically by repairing pro-
grammersâ€™ incorrect submissions. The key development is that Clef
can learn how to generate repairs for incorrect submissions by
examining the repairs that other programmers made to their own
submissions over time. Since the differences between an incorrect
program and a correct program for the same task may be signifi-
cant, we introduce a new data structure, merge trees, to capture the
changes between submissions. Merge trees are versatile: they can
encode both large algorithm-level redesigns and small statement-
level alterations. Clef applies the patterns it learns from a database
of submissions to generate repairs for new submissions outside the
database. We evaluated Clef on six real-world problems from Code-
forces, the worldâ€™s largest platform for competitive programming.
Clef achieves 42.1% accuracy in repairing programmersâ€™ incorrect
submissions. Even when given incorrect submissions from pro-
grammers who never found the solution to a problem on their own,
Clef repairs the usersâ€™ programs 34.1% of the time.

ACM Reference Format:
Jialu Zhang, De Li, John C. Kolesar, Hanyuan Shi, and Ruzica Piskac. 2022.
Automated Feedback Generation for Competition-Level Code. In Proceedings
of ACM Conference (Conferenceâ€™17). ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conferenceâ€™17, July 2017, Washington, DC, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Competitive programming enjoys widespread popularity. The Inter-
national Collegiate Programming Contest (ICPC), one of the most
prestigious programming contests for college students, has been
held annually for more than 50 years. Each year, more than 50,000
students from over 3,000 universities in over 100 countries compete
for medals in the contest [7]. Moreover, competitive programming
has had a significant impact in industry as well. Platforms such as
Codeforces1 and Topcoder2 host large-scale online programming
contests that attract millions of experienced programmers. Soft-
ware companies view the finalists in the competitions as strong
candidates for hiring since the finalists demonstrate solid algorith-
mic problem-solving skills and an outstanding ability to handle
on-the-spot stress. Some software companies, such as Google [5],
Meta [6], Microsoft [3], Yandex [8], and HP [2], even hold their
own online programming contests for recruiting purposes [5].

In a programming competition, participants receive a description
of a problem and a short list of sample tests illustrating how the
program should execute on some example inputs. The participants
develop solutions for the problem and submit them. The solutions
are evaluated automatically on a number of different tests that
are hidden from the participants. If a solution passes every test in
the suite, it is accepted as correct. Competition-level problems are
non-trivial: correct implementations sometimes require hundreds
of lines of code, and the entire program needs to be efficient and
bug-free. Additionally, the platform for the competition evaluates
programmersâ€™ submissions rigorously on multiple criteria. The
automatic tests can involve carefully-designed corner cases, along
with hard limits on execution time and memory usage.

State-of-the-art feedback generation for programmersâ€™ in-
correct submissions. Some competitive programming platforms
allow programmers to practice on problems from past competi-
tions. Currently, the standard response to a programmerâ€™s incorrect
practice submission is simply to expose the first test case that the
submission fails. Even with test case exposure for failures, many
programmers still fail to solve the problem in the end, and they
abandon the problem after several unsuccessful submissions. For
the problems that we surveyed, 52.2% of the incorrect submissions
were never corrected fully by their authors. Forums can serve as a
helpful source of feedback for programmers while they are practic-
ing, but users who post questions on a forum have no guarantee of
a timely response. Furthermore, the feedback from other users on
the forum can be incomplete or incorrect, and the users who post

1https://codeforces.com
2https://www.topcoder.com

 
 
 
 
 
 
Conferenceâ€™17, July 2017, Washington, DC, USA

Jialu Zhang, De Li, John C. Kolesar, Hanyuan Shi, and Ruzica Piskac

the questions might not ask for the right information in the first
place because they misunderstand their own problems. A tool that
repairs programs or provides useful feedback automatically would
be a helpful alternative.

In recent years, researchers in the automated program repair
community have worked on generating feedback automatically for
intro-level programming assignments [17, 19, 37â€“40]. State-of-the-
art feedback generators use data-driven approaches. They all take
advantage of databases of previously-written correct submissions
to learn how to repair new incorrect submissions. Unfortunately,
these tools target only problems from intro-level programming
courses, and their feedback generation techniques do not suffice for
competition-level problems. Two major differences exist between
intro-level and competition-level programming:

â€¢ The difficulty level of the problems. Intro-level program-
ming problems focus primarily on training programmers to
use the features of a language correctly [18]. Consequently,
they usually do not require programmers to use a specific
algorithm or data structure. Typical intro-level problems
include printing a chessboard [40] and computing the deriv-
ative of a function [17]. On the other hand, competition-level
programming problems require programmers to understand
complex natural-language descriptions, to master a wide
range of algorithms and data structures, and to implement
solutions that may involve hundreds of lines of code.

â€¢ The evaluation metrics. Intro-level programming prob-
lems usually do not have rigorous evaluation metrics. Restric-
tions on execution time and memory consumption are rare:
generally, the test suites for intro-level problems only cover
functional correctness. On the other hand, evaluation suites
for competition-level problems perform rigorous checks on
execution time and memory consumption. Any timeout or
excessive memory usage causes the suite to mark a submis-
sion as incorrect even if the program behaves perfectly in
terms of functional correctness.

Furthermore, state-of-the-art intro-level feedback generators suf-
fer from a variety of weaknesses, including the inability to generate
complex repairs [19, 37, 40], the tendency to enlarge programs ex-
cessively with repairs [17], and dependence on manual guidance
from users [38]. Finding an effective automatic feedback generation
method for competition-level code remains an open problem.

Problem 1475A from Codeforces illustrates some of the major
differences between intro-level and competition-level problems.
The input is an integer ğ‘› (2 < ğ‘› < 1014), and the goal is to determine
whether ğ‘› has any odd divisor greater than one.3 The execution time
limit for Problem 1475A is two seconds per test, and the memory
limit is 256 MB per test. One solution for the problem posted on
Stack Overflow [1] performs an exhaustive search by iterating over
every odd number in (1, ğ‘›) and checking whether ğ‘› is divisible by
it:

for (unsigned long long i=3; i<n; i+=2){

if (n%i == 0) return true; //It has an odd divisor

}
return false; // n%i == 0 was never true so it doesn't

have an odd divisor

3https://codeforces.com/contest/1475/problem/A

This submission is syntactically and semantically correct. How-
ever, it fails to pass the evaluation suite: the test suite marks it
as incorrect with â€œTime limit exceeded on test 2â€ as the feedback.
Solving the problem correctly within the time limit requires a more
efficient algorithm, and finding that efficient algorithm requires
an important insight: the odd divisor problem reduces to checking
whether ğ‘› is a power of two. An efficient program for Problem
1475A right-shifts ğ‘› repeatedly to remove trailing zeroes and then
checks at the end that the remaining one is the only one in ğ‘›:

while (!(n&1)) n >>= 1;
if (n==1) return false; else return true;

Problem 1475A presents a challenge for automated feedback gen-
eration tools. A submission that approaches the problem incorrectly
may require a complete algorithm-level redesign, not just a small
local repair. The automatic feedback provided by Codeforces did not
help the programmer who wrote the exhaustive-search implemen-
tation to see that a completely different approach was necessary.
Additionally, state-of-the-art tools [17, 19, 37, 38, 40] cannot make
the repairs that the program needs because they view correctness
only in terms of input-output correspondence, not efficiency.
Our approach: Clef. We introduce CLEF (Competition-Level Ef-
fective Feedback), a new tool that generates feedback automatically
for competition-level problems by repairing errors in programmersâ€™
incorrect submissions. Clef learns the repair patterns that it uses by
analyzing the repairs that other programmers made for the same
problem across their submission histories. Clef applies the patterns
that it learns to target programs outside the database to generate
candidate repaired programs.
Main technical challenges in designing Clef. The main techni-
cal challenge for Clef is having an effective method for learning how
programmers repair errors in their own submissions. The repair pat-
terns that Clef needs to learn range from small statement-level fixes
to algorithm-level control flow redesigns. Other data-driven feed-
back generation tools cannot alter the control flow of an incorrect
program [17, 40], so large-scale algorithm-level changes, precisely
the kind of changes that incorrect submissions for competition-level
problems often require, are impossible for them to make.

Clef employs a number of techniques that no other feedback

generation tool has used previously:

â€¢ We introduce merge trees, a new data structure for encoding
both statement-level and algorithm-level changes between
incorrect programs and corrected versions of the same pro-
grams.

â€¢ We propose a new matching and repairing algorithm that
takes advantage of similarities between the target program
and programs in the database. With the new algorithm, Clef
can repair incorrect submissions even if the errors in the
submission have no exact matches in the database.

Evaluation. To evaluate our tool, we run Clef on thousands of
submissions for six real-world competitive programming problems
obtained from Codeforces. Clef provides feedback successfully for
42.1% of the incorrect solutions overall. Whenever the database
contains both incorrect submissions and a correct submission for
an individual user, we have Clef attempt to fix the incorrect sub-
missions without seeing the correct version, and then we compare

Automated Feedback Generation for Competition-Level Code

Conferenceâ€™17, July 2017, Washington, DC, USA

Clefâ€™s repaired version to the real userâ€™s correct version in terms
of program editing distance. In 40.6% of the cases, Clef generates a
repaired program that is syntactically closer to the original incor-
rect submission than the userâ€™s own corrected version is. For the
cases where a user made incorrect submissions but never made a
correct submission, Clef repairs the userâ€™s incorrect submissions
successfully 34.1% of the time.

In summary, we make the following contributions:

â€¢ We conduct a survey to assess the characteristics and chal-

lenges of competitive programming.

â€¢ We present a data-driven tool, Clef, that generates feed-
back for usersâ€™ incorrect submissions automatically using its
knowledge of how other users repair their own programs.
â€¢ We propose a new data structure for capturing the algorithm-

level design changes in repaired submissions.

â€¢ We evaluate Clef on real-world competitive programming
problems. For the incorrect submissions that were later re-
paired by the same user, Clef provides correct repairs 50.0%
of the time. In 40.6% of the cases, Clef generates repaired
programs that are closer to the original incorrect submission
than the userâ€™s own correct submission. For the incorrect
submissions that were never repaired by their users, Clef
provides correct repairs 34.1% of the time.

2 UNDERSTANDING COMPETITIVE

PROGRAMMING

In this section, we present our empirical study of real-world com-
petitive programming. We illustrate the challenges involved with
solving competitive programming problems through some concrete
examples. We also discuss the implications that drive the design of
Clef.

In a programming competition, a contestant writes a program to
perform a specific task and submits the code to an online evaluation
platform. The platform compiles the program and runs it on a suite
of pre-designed test cases. There are seven possible outcomes for a
submission:

â€¢ Accepted. The submission produces the correct output for
every test and never violates the time and memory limits.
â€¢ Compile-Time Error. The submission has at least one com-
pilation error. Most programs with syntax errors fall into
this category.

â€¢ Runtime Error. The submission encounters an error at run-
time for a test. Common errors include buffer overflow and
invalid array indices.

â€¢ Time Limit Exceeded. The program surpasses the execu-

tion time limit on a test.

â€¢ Memory Limit Exceeded. The program surpasses the mem-

ory usage limit on a test.

â€¢ Wrong Answer. The program returns an incorrect output

for a test.

â€¢ Other. A non-deterministic error, such as a network outage,

occurs.

The major sources of difficulty for competition-level problems
are categorically different from the sources of difficulty for intro-
level problems. The aim of a competition-level problemâ€™s design
is not to teach contestants how to write programs but to push

contestants to the limits of their knowledge. We will highlight
some of the patterns in competition-level problemsâ€™ designs with a
few real-world examples.

Pattern 1: Challenging Problem Descriptions. The first step
in solving a competition-level problem is converting the natural-
language problem description into an idea for an algorithm. Intro-
level programming problems generally have short, straightforward
descriptions, but competition-level problems can have lengthy de-
scriptions designed to mislead contestants. The length of a chal-
lenging problem description comes not from insignificant clutter
but from complicated explanations of problem details meant to test
how well programmers can bridge the gap between an end goal
and an algorithm to accomplish it. For instance, consider Problem
405A from Codeforces:4

A box contains ğ‘› columns of toy cubes arranged
in a line. Initially, gravity pulls all of the cubes
downward, but, after the cubes are settled in place,
gravity switches to pulling them to the right side
of the box instead. The input is the initial config-
uration of the cubes in the box, and the goal is
to print the configuration of the box after grav-
ity changes. The sample case example provided by
Codeforces is shown in the left subfigure in Fig-
ure 1.

The prompt of Problem 405A is designed to test programmersâ€™
ability to reduce a complex problem to a well-known simple al-
gorithm, namely sorting. Attempting to write a brute-force im-
plementation that treats the cubes as distinct entities is a tedious
and error-prone process. A key insight for solving the problem is
the fact that, when gravity changes, the highest columns always
appear at the right end of the box and are of the same height as
the highest columns at the start. The right subfigure in Figure 1
illustrates this. The possibility of reducing the problem to sorting a
one-dimensional array becomes clear after a programmer notices
how the columns behave.

Pattern 2: Challenging Implementation Details. Not every sin-
gle competition-level programming problem is a simple task hidden
behind a complex description. Often, implementing an effective al-
gorithm for the problem is a genuinely difficult task involving minor
details that are easy to mishandle. Problem 579A from Codeforces5
is one such problem:

Start with an empty box. Each morning, you can
put any number of bacteria into the box. Each
night, every bacterium in the box will split into
two bacteria. To get exactly ğ‘¥ (1 â‰¤ ğ‘¥ â‰¤ 109) bacteria
in the box at some moment, what is the minimum
number of bacteria you need to put into the box
across some number of days?

4https://codeforces.com/contest/405/problem/A
5https://codeforces.com/problemset/problem/579/A

Conferenceâ€™17, July 2017, Washington, DC, USA

Jialu Zhang, De Li, John C. Kolesar, Hanyuan Shi, and Ruzica Piskac

(a) Original Sample Illustration

(b) Required New Understanding

Figure 1: Subfigure 1a is the original sample illustration from Codeforces. The initial configuration of the cubes in the box
appears on the left, and the final configuration appears on the right. The cubes whose positions change are highlighted in
orange. The top cube of the first column falls to the top of the last column, the top cube of the second column falls to the top
of the third column, and the middle cube of the first column falls to the top of the second column. Subfigure 1b shows the same
example input but highlights a different detail. The tallest column at the end is of the same height as the tallest column at the
start, but it appears at the right end of the box. The number of columns of a given height is preserved, so the two-dimensional
gravity flip problem reduces to one-dimensional array sorting.

A key insight for solving the problem is the fact that every bac-
terium placed in the box will become 2ğ‘› bacteria after ğ‘› days. What
the problem really requires is an algorithm that can divide one
integer into a sum of powers of two. A natural implementation for
this algorithm is to count the number of ones that appear in the
binary representation of ğ‘¥. Using the provided integer represen-
tation of ğ‘¥ makes this easy, but a program that converts ğ‘¥ into a
binary string instead to count the number of ones can fall victim
to certain errors if implemented carelessly. String operations may
misinterpret the base-2 string as a base-10 string, and this can lead
to incorrect answers or even overflow errors.6 To avoid overflow
errors, a better program for Problem 579A never converts ğ‘¥ into
an alternative format. Instead, it operates directly on the binary
representation of the integer. It right-shifts ğ‘¥ repeatedly one bit at
a time and counts the number of iterations where the right-shifted
version of ğ‘¥ is odd:

while (x > 0){
if (x & 1)
x >>= 1;

}

r += 1;

Pattern 3: Challenging Efficiency Requirements. For other
problems, meeting the evaluation suiteâ€™s efficiency requirements is
the main source of difficulty. Consider the following problem from
GeeksforGeeks:7

Given an array of ğ‘› (1 â‰¤ ğ‘› â‰¤ 107) elements, find
the majority element in the array, if it exists. A
majority element is an element that appears more
than ğ‘›/2 times in the array.

The time complexity limit for the problem is O (ğ‘›), and the space
limit is O (1).8 The time and space requirements rule out a naive

6https://stackoverflow.com/questions/52548304/converting-decimal-to-binary-with-
long-integer-values-c
7https://practice.geeksforgeeks.org/problems/majority-element-1587115620/1
8The stated time complexity requirement is only an indirect indicator of the real
efficiency requirement used by the evaluation since the constant factors are hidden.
In practice, the actual running time of the algorithm is what matters, but the time

brute-force solution because a brute-force search requires two loops
to keep track of the number of occurrences of every element. Binary
search and a hash map-based solution are more efficient alternatives
but are still not efficient enough. The time complexity of a binary
search solution is O (ğ‘› log(ğ‘›)), and a hash map solution has a space
complexity of O (ğ‘›).

One algorithm that can solve the problem within the time and
space constraints is the Boyer-Moore majority voting algorithm [11].
The algorithm starts by treating the first element of the array as
the presumed majority element. It iterates through the array, main-
taining a single integer counter that starts at one. If an element
in the array is equal to the presumed majority element, increase
the counter by one, and decrease the counter by one otherwise. If
the counter ever reaches zero, then reset it to one and make the
element at the current index the new presumed majority element.
When the first traversal finishes, fix the presumed majority element
and compare every element in the array to it. If more than ğ‘›/2 ele-
ments in the array are equal to the presumed majority element, the
presumed majority element is the real majority element. Otherwise,
there is no majority element.

3 MOTIVATING EXAMPLES
Effective feedback generation for competition-level code requires
the ability to apply complex changes to incorrect submissions.
This includes modifying programsâ€™ control flow and making major
statement-level alterations. Along with the ability to perform com-
plex modifications, high repair quality is another priority for Clef:
it returns the smallest repairs that it can find. To illustrate the repair
process that Clef follows, we use a number of real submissions for
Problem 579A from Codeforces as examples. The prompt for the
problem appears in the discussion of Pattern 2 in Section 2.

Incorrect program needs control flow changes. An example of
a control flow modification that Clef applies appears in Figure 2.
The original incorrect submission made by a user for Problem 579A

complexity is the best measurement that we can use here. The same applies for space
complexity.

Automated Feedback Generation for Competition-Level Code

Conferenceâ€™17, July 2017, Washington, DC, USA

if ((x/2)!=0)
{

while ((x/2)!=0)
{

while(x)
{

if ((x%2)==1)

if ((x%2)==1)

if ((X%2)==1)

c++;
x = x/2;

c++;
x = x/2;

c++;
x = x/2;

}
printf("%lld",c+1);

}
printf("%lld",c+1);

}
printf("%lld",c);

(a) Incorrect Program

(b) Clefâ€™s Repair

(c) Userâ€™s Repair

Figure 2: An example repair involving control flow modifica-
tion. The differences between the programs are highlighted
in red. The variable x is the input for the program, represent-
ing the desired number of bacteria to have in the end. The
variable c is the output, the number of bacteria that need to
be inserted.

while (x>0)
{

if (x%2==0)

u++;
x = x/2;

while (x>0)
{

if (x%2)
u++;
x = x/2;

while (x!=0)
{

u += x%2;
x /= 2;

}
printf("%d", u);

}
printf("%d", u);

}
printf("%d",u);

(a) Incorrect Program

(b) Clefâ€™s Repair

(c) Userâ€™s Repair

Figure 3: A example repair involving a statement-level
change. The differences between the programs are high-
lighted in red. The variable x is the input to this program,
and u is the output.

appears in Subfigure 2a. The high-level design of the implementa-
tion is correct, but the control flow needs correction. Computing
the number of ones in the binary representation of the integer x re-
quires a loop rather than a conditional. Other users in the database
repaired their programs by making a similar control flow change
(converting if to while), so Clef applies the same repair in Sub-
figure 2b. In this situation, Clef generates a high-quality repair that
not only passes all of the test cases but also makes minimal changes
to the structure of the original incorrect program. The same userâ€™s
own fix for the problem appears in Subfigure 2c. If we use the
Zhang-Shasha algorithm [46] to measure tree edit distances, the
repair generated by Clef has a distance of one from the original
flawed program, whereas the userâ€™s own repair has a distance of
six from the original program.
Incorrect program needs statement-level changes. In addition
to making algorithm-level control flow changes, Clef is able to
generate repairs that require small statement-level changes. Figure 3
shows an example of a statement-level repair. The control flow in
the original submission is correct, but the guard in the if statement
contains a numerical error. The repair that Clef produces for the
submission appears in Subfigure 3b. The Zhang-Shasha algorithm
gives the new program generated by Clef a tree edit distance of
only two from the original program. Although this repair is not
the smallest possible repair, which would be changing x%2==0 to
x%2==1, Clef still generates a repair that is closer to the userâ€™s
original incorrect submission than the userâ€™s own repaired program

is. The userâ€™s own correction of the program involves three major
changes: changing the guard in the while statement, removing
the if statement inside the while loop, and computing the output
variable u differently by adding the remainder of x mod 2 to it in
each loop iteration.

4 SYSTEM DESCRIPTION
We design and build Clef, a tool that can generate repairs for
competition-level code automatically by learning the ways that
users repair their own programs. Figure 4 gives an overview of
Clefâ€™s architecture. It consists of three main modules: (1) The pre-
processor, described in Section 4.1, takes the database programs as
input, parses them, and generates abstract syntax trees for them. (2)
The pattern learner, described in Section 4.2, uses a new data struc-
ture, merge trees, to represent the algorithm-level and statement-
level changes that users in the database apply to their own programs
over time. (3) The repair generator, described in Section 4.3, applies
program transformation patterns to the incorrect target program
to generate repair candidates, and it also validates the candidates
with the provided test suite for the problem.

4.1 Preprocessor
The preprocessor parses all of the programs in the database into
ASTs offline for later use in repair pattern learning. We use the open-
source complete C99 parser pycparser [4] for the AST conversion.
The preprocessor groups the program ASTs into pairs of the form
(ğ‘–, ğ‘), where ğ‘– and ğ‘ are ASTs for an incorrect program and a correct
program, respectively, written by the same user. If a user made
multiple incorrect or correct submissions, the preprocessor makes
program pairs for all of the possible combinations. It also discards
programs that have syntax errors in this stage.

4.2 Pattern Learner
In the second module, Clef takes the program pairs from the pre-
processor as inputs, and it produces a collection of program trans-
formation patterns based on the changes between the incorrect and
correct programs. The program transformation patterns fall into
three categories of AST changes: additions, deletions, and muta-
tions. Clef uses a new data structure, a merge tree, to represent the
AST changes that occur between the incorrect and correct versions
of a program.
Merge trees. A merge tree encodes the differences between two
abstract syntax trees. The main structure of a merge tree resembles
the unchanged parts of the two ASTs being compared, but the merge
tree also includes special nodes that represent additions, mutations,
and deletions of sub-trees. An important characteristic of merge
trees is their generality: they can match a variety of patterns in
ordinary ASTs rather than only a single pattern. For example, if the
incorrect version of a program contains a statement ğ‘1 where the
correct version contains a different statement ğ‘2, the merge tree for
the transformation can apply to ASTs that contain ğ‘1, ğ‘2, (ğ‘1; ğ‘2),
(ğ‘2; ğ‘1), or neither statement, as long as the surrounding parts of
the AST bear a sufficiently close resemblance to the merge tree. In
contrast, a simpler encoding of the transformation [37] would only
apply to ASTs that contain ğ‘1. The merge treeâ€™s ability to be applied
to any program that contains a combination of the two statements

Conferenceâ€™17, July 2017, Washington, DC, USA

Jialu Zhang, De Li, John C. Kolesar, Hanyuan Shi, and Ruzica Piskac

Figure 4: Clef overview. The green blocks are the input that Clef receives from users and the output that it provides for them.
The yellow blocks are the key modules of Clef.

makes it cover a much larger range of programs than a simpler
encoding does.
Computing program differences. The standard approach for
identifying differences between two programs is to apply the Zhang-
Shasha algorithm directly [46] to compute the edits needed to con-
vert one AST into the other. Multiple state-of-the-art intro-level
feedback generation tools follow this approach [37, 40]. However,
the Zhang-Shasha algorithm on its own is not suitable for comput-
ing program differences in the domain of competition-level code.
First, the Zhang-Shasha algorithm runs in O (ğ‘š2ğ‘›2) time, where
ğ‘š and ğ‘› are the numbers of nodes in the two input trees. This
imposes a sizable overhead on a toolâ€™s operation, hampering its
scalability. During the development of Clef, we found that applying
the Zhang-Shasha algorithm to the full ASTs of usersâ€™ submissions
increased the running time of the tool significantly.

The second and more important reason for not using the Zhang-
Shasha algorithm on full program ASTs is that the algorithm treats
every node in a tree as having equal weight. The Zhang-Shasha
algorithm is a general-purpose algorithm for trees of any kind, not
just program ASTs, so it pays no attention to the semantic signif-
icance of the edits it uses for measuring distance. In the case of
program ASTs, not all nodes deserve equal weight: some node mod-
ifications are more significant than others. For example, changing
an if node to a while node generally qualifies as a major change
because it alters the control flow of a program. A method for mea-
suring the edit distance between two programs should count such
a control flow change as having a higher impact than changing an
x=1 statement to x=0. When we compute tree edit distances, we
assign a higher cost to control flow edits than to other changes.

For Clef, we designed a custom algorithm, shown as Algorithm 1,
that computes the program transformation patterns between the
incorrect and correct versions of a program. To detect algorithm-
level changes between the two versions, Clef uses top-down control
flow alignment to capture changes in the control flow. The nodes
that count as control flow nodes for our purposes are if, while,
and for statements as well as function calls. Two control flow
nodes align if they have the same type (i.e. they are both if, both
while, both for, or both function calls) and they satisfy some
extra type-specific conditions for alignment. Two if statements
need to have matching guards or matching true and false branches.
Two while or for statements need to have matching guards
or matching bodies. Two function calls need to have all of their
arguments match. For two sub-expressions to match in any of these
cases, they need to be mostly the same. Variable names and function
names are not required to be the same, but the values of variables

Algorithm 1 Learning Program Transformations
Input: ğ‘ƒğ‘– : Userâ€™s incorrect program submission (AST).
Input: ğ‘ƒğ‘ : Userâ€™s correct program submission (AST).
Output: patternPool : A set of program transformation patterns
that reflect the changes that users made in repairing their own
programs.

1: procedure learnTransformation(ğ‘ƒğ‘–, ğ‘ƒğ‘ )
2:

patternPool = []
alignedCF, unmatchedCF := ControlFlowAlign(ğ‘ƒğ‘–, ğ‘ƒğ‘ )
for (ğ‘‡ğ‘–,ğ‘‡ğ‘ ) in alignedCF do

ğ‘“ ğ‘™ğ‘ğ‘¡ğ´ğ‘†ğ‘‡ğ‘– , ğ‘“ ğ‘™ğ‘ğ‘¡ğ´ğ‘†ğ‘‡ğ‘ := Flatten(ğ‘‡ğ‘–,ğ‘‡ğ‘ )
edits := Zhang-Shasha(ğ‘“ ğ‘™ğ‘ğ‘¡ğ´ğ‘†ğ‘‡ğ‘– , ğ‘“ ğ‘™ğ‘ğ‘¡ğ´ğ‘†ğ‘‡ğ‘ )
mergeTree += merge(edits, ğ‘“ ğ‘™ğ‘ğ‘¡ğ´ğ‘†ğ‘‡ğ‘– , ğ‘“ ğ‘™ğ‘ğ‘¡ğ´ğ‘†ğ‘‡ğ‘ )

for (ğ‘‘ğ‘’ğ‘™ğ‘’ğ‘¡ğ‘’ğ‘‘ğ¶ğ¹ğ‘– ) in ğ‘¢ğ‘›ğ‘šğ‘ğ‘¡ğ‘â„ğ‘’ğ‘‘ğ¶ğ¹ do

ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‡ğ‘Ÿğ‘’ğ‘’ := augment(ğ‘‘ğ‘’ğ‘™ğ‘’ğ‘¡ğ‘’ğ‘‘ğ¶ğ¹ğ‘–, ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‡ğ‘Ÿğ‘’ğ‘’)

patternPool += mergeTree
return patternPool

3:

4:

5:

6:

7:

8:

9:

10:

and the numbers and types of parameters should be. Clef identifies
all of the pairs of control flow nodes that align with each other and
merges their sub-trees.

A key detail that makes merge tree generation scalable is the fact
that the merge tree for a control flow node does not cover changes
in the sub-trees of the two versionsâ€™ control flow nodes. We leave
the sub-trees for other merge trees to cover. Clef handles program
changes within the current control flow node by flattening all of
the control flow nodes inside its sub-trees and treating the interior
as an empty node. To simplify the process of computing edits, we
run the Zhang-Shasha algorithm only on pairs of these flattened
sub-trees rather than on the full original ASTs of the incorrect and
correct programs. If ğ‘š and ğ‘› are the numbers of nodes of all types
in the two input trees and ğ‘ and ğ‘ are the numbers of control flow
nodes in the two input trees, then this flattening reduces the time
complexity of merge tree generation from O (ğ‘š2ğ‘›2) to O ( ğ‘š2ğ‘›2
ğ‘2ğ‘2 ).
Control flow nodes in the incorrect program with no matches
in the correct program are regarded as deletions. Correspondingly,
control flow nodes in the correct program with no matches in
the incorrect program are regarded as insertions. Clef uses special
nodes as markers in its merge trees to represent these deletions
and insertions. At the end of pattern learning, Clef returns a set of
all the merge trees it generated to use as program transformation
patterns.

Automated Feedback Generation for Competition-Level Code

Conferenceâ€™17, July 2017, Washington, DC, USA

4.3 Repair Generator
The repair generator takes the incorrect target program and the
set of merge trees as input, and it returns a repaired version of
the target program. The repair generatorâ€™s algorithm consists of
three main steps. First, it converts the incorrect target program
into an AST just as the preprocessor described in Section 4.1 does
for the database programs. Second, the repair generator identifies
merge trees that match the target program and produces candidate
repaired programs by applying transformations based on the merge
trees that match the target program. During this step, variable usage
analysis helps with the removal of spurious candidate programs.
Third, the repair generator validates the candidate programs with
the pre-defined test suite for the problem.

Matching the target program with merge trees. Merge trees
represent the changes that programmers made to their own pro-
grams in the database. The goal of the matching process is to apply
similar program edits to repair the target program. Intuitively, the
repair generator takes advantage of the similarities between the
incorrect target program and the merge trees to generate repairs.
To start, the repair generator analyzes the target programâ€™s AST
and identifies several sub-trees that have a control flow node as
their root. Such a sub-tree matches a merge tree if all of the nodes
and edges of the sub-tree are contained within the merge tree. The
names of variables and functions are irrelevant for matching, but the
values of variables and the types and parameters of functions matter.
If the repair generator finds a match between the target program
and a merge tree, it can modify the target program by performing
a repair based on the merge tree. The fact that matching only
requires the merge tree to contain the sub-tree and not the other
way around helps the repair generator to find ways to fix incorrect
submissions in situations where the errors in the submission have
no exact matches in the database. Clef generates a modified version
of the target program by replacing the matched sub-tree with a
new sub-tree based on the merge treeâ€™s transformation.

Replacing a sub-tree using the merge treeâ€™s transformation might
introduce usages of undefined variables. To account for this, the
repair generator tries conservatively fitting different combinations
of defined variable names onto the undefined variables that are
inserted. Then it performs variable usage analysis on modified ver-
sions of the target program to remove candidates that are invalid
simply because of their variable usage. The repair generator dis-
cards candidate programs that still contain undefined variables
after variable alignment or define variables without using them.
This filtering reduces the number of candidate programs to be vali-
dated with the test suite, improving the performance of the repair
generator.

Validation. The repair generator validates candidate programs
simply by running the provided test suite on them. As soon as the
repair generator finds a candidate program that passes all of the
tests, it returns that candidate program as output.

Because small repairs are more beneficial for users, we priori-
tize candidates with small transformations over candidates with
large ones for validation. The repair generator starts by applying
only one merge tree to the target program at a time to generate
candidates. If we fail to find a valid candidate program after trying

every option among the individual merge trees, the repair gen-
erator begins creating candidate programs from combinations of
multiple transformations. The repair generator continues trying
progressively larger repairs until it hits a timeout or the number of
test suite runs reaches a preset limit. For our evaluation, we do not
impose a time limit, but we set a limit of 1,000 on the number of
candidate programs to validate with the test suite.

5 EVALUATION
We answer the following questions with our evaluation of Clef:

â€¢ How effectively can Clef repair incorrect submissions for

competition-level problems?

â€¢ How high is the quality of Clefâ€™s feedback? More specifically,
how closely do the repaired programs that Clef generates
resemble usersâ€™ original programs?

5.1 Implementation and Experimental Setup
Our implementation of Clef uses a mix of Python and open-source
software libraries. As it is now, Clef operates on C programs. We rely
on pycparser [4], a complete C99 parser, to convert C programs into
abstract syntax trees. Also, we use the Zhang-Shasha algorithm [45]
to compute tree edit distances.
Benchmark Setup. For our evaluation suite, we use six problems
from Codeforces,9 the worldâ€™s largest online platform for com-
petitive programming. Codeforces assigns difficulty scores to its
problems, and we group the problems into three categories based
on their difficulty scores. We categorize problems with a difficulty
score of 800 or less as easy, problems with a score between 800 and
1000 as medium, and problems with a score of at least 1000 as hard.
Each of the six problems that we selected received more than 700
submissions written in C. (Multiple submissions made by the same
user count as distinct.) For each submission, we collect not only the
text of the program but also its execution result, running time, and
memory usage.10 Additionally, we have access to the official test
suite used by Codeforces for each of the problems. Table 1 names
the six problems and the specific challenges that each problem
presents.

Table 2 provides a breakdown of the six selected competition-
level programming problems. We can see from analyzing the ex-
ecution results for the database programs that 25.6% (2093/8187)
of the submissions were rejected because of errors rather than
incorrect outputs. Among those, 4.5% (370/8187) of the programs
were classified as incorrect because of runtime errors, time limit
violations, or memory limit violations. Furthermore, of the 5597
incorrect submissions, 2921 (52.2%) come from programmers who
never made a correct submission.

Clef aims to provide effective feedback for programmers as they
practice on competition-level problems. To assess whether Clef
meets this goal, we split the users into two groups for each pro-
gramming problem:

â€¢ Group One. Some users made one or more incorrect sub-
missions but never managed to produce a correct submission.
Generating repairs for these usersâ€™ programs is generally a

9https://codeforces.com
10The founders and owners of Codeforces gave us permission to collect data from the
six problems.

Conferenceâ€™17, July 2017, Washington, DC, USA

Jialu Zhang, De Li, John C. Kolesar, Hanyuan Shi, and Ruzica Piskac

Table 1: Six Representative Competition-Level Programming Problems.

Problem
ID

Difficulty
Level

1312A

1519B

1238A

1295A

579A

1199B

Easy

Medium

Hard

Codeforces Tag

Challenges

Problem Description

Number Theory

Description

Math

Description

Number Theory

Greedy

Bit Mask

Geometry

Algo Design &
Implementation
Description &
Algo Design
Algo Design &
Implementation
Algo Design &
Implementation

Given two integers ğ‘› and ğ‘š, determine whether a convex regular polygon with ğ‘š sides can be
inscribed in a convex regular polygon with ğ‘› sides such that their centers and vertices coincide
Given an ğ‘›-by-ğ‘š grid, with different costs for moving in different directions,
check whether it is possible to reach cell (ğ‘›, ğ‘š) with exactly cost ğ‘˜
Given two integers ğ‘¥ and ğ‘¦, determine whether there is a prime integer ğ‘
such that subtracting ğ‘ from ğ‘¥ any number of times makes ğ‘¥ equal to ğ‘¦
Find the largest integer that can be shown on a seven-segment (alarm clock)
display that requires no more than ğ‘› segments to be turned on in total
Find the minimum number of bacteria that need to be placed into a box over
some number of days in order to have ğ‘¥ bacteria in the box at some moment
Find the depth of a body of water given the distance that a vertical line
segment extending from the bottom can tilt before being submerged

Table 2: Statistics for the six selected competition-level programming problems. The categorizations for submissions here
come from Codeforces. AC: Accepted, WA: Wrong Answer, CE: Compile-Time Error, RE: Runtime Error, TLE: Time Limit
Exceeded, MLE: Memory Limit Exceeded, OT: Other.

Problem ID
1312A
1519B
1238A
1295A
579A
1199B

# Submissions
1160
724
1345
1024
1889
2045

Total

8187

# AC
494
349
303
251
780
413

2590

# WA
327
211
520
349
758
1339

3504

# CE
301
137
358
368
288
269

1721

# RE
15
12
39
13
23
18

120

# TLE
19
14
121
40
36
1

231

# MLE
3
1
0
3
1
0

8

# OT
1
0
4
0
3
5

13

Average LOC
21.1
25.1
27.3
33.3
19.8
11.9

21.4

challenging task: since the users never managed to repair
their own programs, the submissions may contain major
errors.

randomly rather than chronologically allows us to distribute the
users who copied other usersâ€™ submissions more fairly between the
training set and the evaluation set, if there are any such users.

â€¢ Group Two. Other users made one or more incorrect sub-
missions initially but then managed to produce a correct
submission afterward on their own. These usersâ€™ incorrect
submissions are easier to handle in general: the fact that
the users found a solution eventually means that they were
likely close to a right answer with their earlier incorrect
submissions.

For each problem, we perform some cleaning of the database
before we use it as a training set. To clean the database, we discard
all programs with syntax errors and all submissions from users
who solved the problem on their first attempt. Next, we label each
user with a distinct anonymous identifier. After that, we allocate
80% of the users for the training set and 20% for the evaluation
set. Some existing feedback generators use the chronologically
earliest 80% of submissions as the training set and the remaining
20% as the evaluation set [34], but we divide the users at random
instead to avoid skewing the results. The setup of Codeforces makes
problems easier for programmers who submit their programs later:
on Codeforces, users practicing on a specific problem can view every
other userâ€™s submission history for the same problem. Consequently,
there is a risk that programmers who submitted later fixed the
mistakes in their code by copying someone elseâ€™s correct submission
rather than by finding a solution on their own. Grouping the users

5.2 Results

Group One. Table 3 shows Clefâ€™s results for incorrect programs
abandoned by their authors. To this group, Clef has an overall fix
rate of 34.1% across the six problems. Since the programsâ€™ authors
never addressed their mistakes fully on their own, they would ben-
efit from receiving repaired versions of their programs as feedback.
To measure the quality of our repairs for Group One, we introduce
a new metric: the dissimilarity between each target program and
the correct programs in the database. To measure dissimilarity, we
compute the minimum tree edit distance between a target program
and any of the correct programs in the database using the Zhang-
Shasha algorithm, and then we divide this distance by the size of the
target program. This dissimilarity metric quantifies the difficulty
of repairing each program: if a program is not syntactically close
to any correct program in the database, generating a repair for it is
difficult, and any repairs found are likely to be large.

Clef generates high-quality repairs for Group One according to
our standard. We define the relative repair size for a problem as
the tree edit distance between a repaired program and its target
program divided by the size of the target program. For three of
the six problems, Clef has an average relative repair size smaller
than the average dissimilarity between the target programs and

Automated Feedback Generation for Competition-Level Code

Conferenceâ€™17, July 2017, Washington, DC, USA

Table 3: Evaluation of Clef on incorrect programs abandoned by their authors (Group One). The second column shows the
number of incorrect-correct program pairs in the training set for each problem, not the number of individual programs. The
penultimate column shows the average dissimilarity of the target programs that had a successful repair by Clef. The last
column shows the average dissimilarity of the target programs that Clef failed to generate a successful repair.

Problem
ID
1312A
1519B
1238A
1295A
579A
1199B

# Pairs in
Training Set
475
203
1304
277
1654
4027

# Programs
in Test Set
90
31
316
127
362
558

# Programs
Repaired
51
12
119
38
98
82

Accuracy
(Repair Rate)
56.7%
38.7%
37.7%
29.9%
27.1%
14.7%

Avg. Relative
Repair Size
0.24
0.38
0.26
0.86
0.73
0.15

Average
Dissimilarity
0.26
0.43
0.40
0.56
0.44
0.19

Avg. Dissimilarity
for Successes
0.16
0.27
0.27
0.55
0.41
0.06

Avg. Dissimilarity
for Failures
0.39
0.53
0.48
0.57
0.45
0.21

Table 4: Evaluation of Clef on incorrect programs later repaired by their authors (Group Two). The training set for each prob-
lem is the same as it is in Table 3.

Problem
ID
1312A
1519B
1238A
1295A
579A
1199B

# Programs
in Test Set
62
71
55
53
107
176

# Programs
Repaired
44
55
34
22
25
37

Accuracy
(Repair Rate)
71.0%
77.5%
61.8%
41.5%
27.1%
21.0%

High-Quality
Repairs
52.3%
7.3%
58.8%
13.6%
52.0%
59.5%

Avg. Relative
Repair Size
0.19
0.33
0.29
0.54
0.53
0.21

Average
Dissimilarity
0.17
0.19
0.34
0.38
0.40
0.15

Avg. Dissimilarity
for Successes
0.14
0.12
0.24
0.37
0.38
0.08

Avg. Dissimilarity
for Failures
0.21
0.40
0.50
0.39
0.41
0.17

the correct programs. The high dissimilarity values for Group One
make the higher average relative repair sizes for the other three
problems understandable. Another important finding is that the
average dissimilarity across all six problems is 0.44, so a typical
target program needs a large portion of its code to be changed to
become identical to any of the correct database programs.
Group Two. Table 4 shows Clefâ€™s results for incorrect programs
later fixed by their authors. Clef has an overall fix rate of 50.0%
across the six problems, which is better than the result for Group
One. Since we have the authorsâ€™ own repairs for the programs in
Group Two, we use the authorsâ€™ repairs as the ground truth for
assessing the quality of Clefâ€™s feedback. A repair generated by Clef
counts as a high-quality repair if the tree edit distance between
it and the target program is smaller than the tree edit distance
between the userâ€™s own repair and the target program. For four out
of the six problems, more than 50% of the repairs Clef generates are
closer to the target program than the ground truth is, so Clef does
in fact generate high-quality repairs for programs in Group Two.

5.3 Threats to Validity
Internal validity. Clef validates candidate programs by running
the test suite provided by Codeforces on them. Passing every test
within the imposed memory and time limits is not a perfect guaran-
tee of the correctness of a program but only a highly likely indicator
of its correctness. A perfect guarantee would require formal verifi-
cation, which we do not perform. To our knowledge, this limitation
is common to all existing data-driven feedback generation tech-
niques [17, 19, 28, 37, 40].
External validity. Currently, Clef only supports feedback genera-
tion for C programs. However, the general principles behind the

design of Clef are applicable to programs written in any language.
Our method for handling control flow nodes does assume C-like
syntax, but nothing else about the underlying algorithm is tailored
specifically for C.

6 DISCUSSION

State-of-the-art tools. Recent data-driven approaches for feed-
back generation utilize the wisdom of the crowd by selecting donor
programs from their databases [17, 19, 37, 40]. A donor program
is a program that bears a close resemblance to the program to be
repaired. Tools that use donors repair their target programs by
analyzing the differences between a target program and its donors.
State-of-the-art feedback generators choose their donor pro-
grams from a database of programs that are either all correct or
all incorrect. Both options have limitations. Tools that draw their
donors from databases of correct programs [17, 19, 40] operate un-
der the faulty assumption that the target program differs from the
correct database programs only because of the presence of errors
in the program. Tools that draw their donors from databases of
incorrect programs [37] suffer from low success rates because the
mistakes in the donor programs are unlikely to coincide with the
mistakes in the target program.

Clef takes a different approach. Its database includes both cor-
rect programs and incorrect programs, and it draws information
from both sides of the database to produce merge trees that func-
tion like donor programs do for other tools. Merge trees offer a
unique advantage over donor programs: they allow Clef to generate
high-quality repairs in a way that mimics the debugging procedure
that human programmers follow. The structure of a merge tree
represents the changes that a user makes between the different

Conferenceâ€™17, July 2017, Washington, DC, USA

Jialu Zhang, De Li, John C. Kolesar, Hanyuan Shi, and Ruzica Piskac

versions of a program. Clef can learn to imitate the userâ€™s behavior
by observing the differences between an early incorrect version of
the program and the final correct version of the program.

Direct comparison. A number of factors prevent us from perform-
ing a direct comparison between Clef and any existing feedback
generator. Language compatibility is a major issue: the only publicly
available existing feedback generator that operates on C programs
is Clara [17]. We cannot evaluate Clara on the same problems from
Codeforces that we used for Clef because Clara does not support
the full range of Câ€™s syntax. Expressions such as while(tâˆ’âˆ’) or
a[i]++, where a is an array of integers, cause Clara to return an
error. Also, Clara takes only a single C function at a time as input
rather than an entire program. Reformulating every target program
as a single function to circumvent this problem is not an option:
function definitions within the original program would become
nested function definitions, which Clara does not support. Lastly,
the most important reason for not performing a direct comparison
against Clara is that Clara operates under a different definition of
correctness than Clef does. Clara regards all functionally correct
programs as valid, even if they use too much time or memory. A
comparison of the success rates of Clef and Clara on the same set
of programs would not be very informative because the two toolsâ€™
reported success rates mean different things.

7 RELATED WORK

Competitive programming. Researchers have devoted an in-
creasing amount of attention to competitive programming in recent
years because of its growing impact on programming training and
education [10, 22]. Laaksonen [22] provides a systematic guide
to algorithm design strategy in competitive programming. Puri
et al. [35] provide a large database of thousands of competitive
programming problems along with millions of sampled solutions
for the problems. The first tool to generate solutions for program-
ming problems with program synthesis comes from Zavershynskyi
et al. [44]. Unfortunately, the toolâ€™s utility is limited significantly
by the fact that it generates solutions only in a custom-made in-
termediate programming language. Hendrycks et al. [18] are the
first to use large language models to generate solutions for com-
petitive programming problems directly in Python. However, their
approach produces solutions successfully less than 10% of the time.
AlphaCode [25] is a significant improvement over the state of the
art [13, 18, 44]. AlphaCode produces programs based on natural-
language descriptions that it receives as input. In contests with
more than 5,000 participants, AlphaCode places among the top
54.3% of participants on average [25]. In spite of the advances
that researchers have made in the field of competitive program-
ming, no existing tool generates feedback or repairs for incorrect
competition-level programs.

Automated feedback generation. Automatic feedback genera-
tion for programming assignments has been a popular topic in
programming education over the last decade [14, 17, 19, 20, 28, 33,
34, 37â€“41, 43]. The first tools developed for the task [20, 38] rely
on manual guidance from users, in the form of either reference
solutions [20, 38] or an error model [38] that explicitly defines all of
the repairs that the tool can make. Because of their heavy reliance

on input from users, early feedback generation tools do not qualify
as fully automatic.

More recent feedback generators do qualify as fully automatic,
and they rely on data-driven approaches for the task. They learn
how to generate repairs for programs by analyzing programs writ-
ten by other users. Tools such as Clara [17], SARFGEN [40], Refac-
tory [19], FAPR [28], and Cafe [39] use databases of existing correct
solutions for a problem to learn how to repair incorrect programs
written for the same problem. Some of the data-driven tools are
limited by their heavy dependence on syntactic similarities between
the target program and reference solutions from the database. Two
of the tools for imperative languages cannot repair a flawed pro-
gram unless their database contains a correct program with exactly
the same control flow as the flawed program [17, 40]. Similarly, one
of the tools for functional programs requires alignment for function
call sites [39]. Multiple studies have shown that the assumption that
a flawed program will have an exact control-flow match in the data-
base of correct programs is too strong to be reliable [19, 24]. Other
feedback generators suffer from different problems, such as the ten-
dency to enlarge programs excessively with their repairs [17, 19],
the inability to fix errors that require changes to multiple parts of a
program [37], and the inability to take programsâ€™ semantics into
consideration [28, 42].

Furthermore, state-of-the-art feedback generators [17, 39, 40]
cannot generate the complex repairs that flawed competition-level
programs need because the toolsâ€™ creators designed them with
intro-level programming assignments in mind. No existing tool
can repair programs that require an algorithm-level redesign, but
merge trees allow Clef to handle the task. The inspiration behind
our usage of merge trees comes from algorithms for semi-structured
merging [9, 12]. More importantly, no existing feedback generator
attempts to make programs more efficient.

Automated program repair. Researchers have studied automated
program repair techniques extensively for the past sixty years [15,
16, 26, 27]. Automated program repair techniques fall into three
main categories: heuristic-based techniques [21, 23, 36], semantics-
based techniques [29, 30], and learning-based techniques [26, 27].
Heuristic-based approaches use some heuristic, such as genetic
programming [23], randomization [36], or a predefined fitness func-
tion [21], to guide a search procedure to candidate patches for a
program. Semantics-based techniques [29, 30] combine symbolic
execution with SMT solvers to synthesize repairs. Semantics-based
techniques cannot repair competition-level code reliably because
of the limitations of their internal design. Programming compe-
titions make heavy use of floating-point numbers for geometry
problems and lists for string operation problems, both of which
are difficult for SMT solvers to handle effectively. Learning-based
techniques [26, 27] learn code repair patterns from prior patches.
State-of-the-art automated program repair techniques work best
when used to handle a small number of errors in a large code
base that includes millions of lines of code. Consequently, these
techniques are impractical for competition-level code, where errors
appear much more frequently relative to the size of usersâ€™ programs.
Automatic repair for non-functional program properties (i.e. time
and memory usage) has received a small amount of attention from
researchers previously. However, unlike Clef, prior work in the

Automated Feedback Generation for Competition-Level Code

Conferenceâ€™17, July 2017, Washington, DC, USA

area has targeted only specific program patterns [15, 16], such as
unnecessary loop iterations [32] or repeated computations of the
same value [31]. No prior research on the subject has led to the
development of a general-purpose tool for improving the efficiency
of competition-level code automatically.

8 CONCLUSION
We present Clef, a tool that generates feedback automatically for
competition-level code. By observing how other users repair their
own programs over time, Clef learns how to create repairs for its
target programs. The improvement in quality that Clef provides
over the standard feedback that programmers receive when practic-
ing on competition-level problems will make online programming
platforms that utilize Clef more user-friendly.

REFERENCES
[1] 2022. An example of repairing a faulty submission that need an algorithm-level
redesign. https://stackoverflow.com/questions/65896295/finding-odd-divisors-
with-bit-shifting

[2] 2022. HPE CODEWARS. https://hpecodewars.org/.
[3] 2022. Microsoft Imagine Cup. https://imaginecup.com/.
[4] 2022. pycparser: Complete C99 parser in pure Python. https://github.com/eliben/

pycparser.

[5] 2022. The 10 Most Prestigious Programming Contests and Coding Chal-
lenges. https://www.mycplus.com/featured-articles/programming-contests-and-
challenges/.

[6] 2022.

The Facebook Hacker Cup.

https://www.facebook.com/

codingcompetitions/hacker-cup/.

[7] 2022. The International Collegiate Programming Contest. https://icpc.global/.
[8] 2022. The Yandex Algorithm Cup. https://yandex.com/cup/algorithm/.
[9] Sven Apel, JÃ¶rg Liebig, Benjamin Brandl, Christian Lengauer, and Christian
KÃ¤stner. 2011. Semistructured Merge: Rethinking Merge in Revision Control
Systems. In Proceedings of the 19th ACM SIGSOFT Symposium and the 13th Eu-
ropean Conference on Foundations of Software Engineering (Szeged, Hungary)
(ESEC/FSE â€™11). Association for Computing Machinery, New York, NY, USA,
190â€“200. https://doi.org/10.1145/2025113.2025141

[10] Aaron Bloomfield and Borja Sotomayor. 2016. A programming contest strategy
guide. In Proceedings of the 47th ACM technical symposium on computing science
education. 609â€“614.

[11] Robert S. Boyer and J. Strother Moore. 1991. MJRTY: A Fast Majority Vote
Algorithm. In Automated Reasoning: Essays in Honor of Woody Bledsoe (Automated
Reasoning Series), Robert S. Boyer (Ed.). Kluwer Academic Publishers, 105â€“118.
[12] Guilherme Cavalcanti, Paulo Borba, and Paola Accioly. 2017. Evaluating and
Improving Semistructured Merge. Proc. ACM Program. Lang. 1, OOPSLA, Article
59 (oct 2017), 27 pages. https://doi.org/10.1145/3133883

[13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish
Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe
Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis,
Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex
Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam,
Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,
Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam
McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large
Language Models Trained on Code. https://doi.org/10.48550/ARXIV.2107.03374
[14] Loris Dâ€™Antoni, Roopsha Samanta, and Rishabh Singh. 2016. Qlose: Program
Repair with Quantitative Objectives. In Computer Aided Verification - 28th Inter-
national Conference, CAV 2016, Toronto, ON, Canada, July 17-23, 2016, Proceedings,
Part II (Lecture Notes in Computer Science, Vol. 9780), Swarat Chaudhuri and
Azadeh Farzan (Eds.). Springer, 383â€“401. https://doi.org/10.1007/978-3-319-
41540-6_21

[15] Claire Goues, Stephanie Forrest, and Westley Weimer. 2013. Current Challenges
in Automatic Software Repair. Software Quality Journal 21, 3 (sep 2013), 421â€“443.
https://doi.org/10.1007/s11219-013-9208-0

[16] Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Automated
Program Repair. Commun. ACM 62, 12 (nov 2019), 56â€“65. https://doi.org/10.
1145/3318162

[17] Sumit Gulwani, Ivan Radicek, and Florian Zuleger. 2018. Automated Clustering
and Program Repair for Introductory Programming Assignments. SIGPLAN Not.
53, 4 (June 2018), 465â€“480. https://doi.org/10.1145/3296979.3192387

[18] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora,
Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob
Steinhardt. 2021. Measuring Coding Challenge Competence With APPS. CoRR
abs/2105.09938 (2021). arXiv:2105.09938 https://arxiv.org/abs/2105.09938
[19] Y. Hu, U. Z. Ahmed, S. Mechtaev, B. Leong, and A. Roychoudhury. 2019. Re-
Factoring Based Program Repair Applied to Programming Assignments. In 2019
34th IEEE/ACM International Conference on Automated Software Engineering (ASE).
388â€“398.

[20] Shalini Kaleeswaran, Anirudh Santhiar, Aditya Kanade, and Sumit Gulwani. 2016.
Semi-Supervised Verified Feedback Generation. In Proceedings of the 2016 24th
ACM SIGSOFT International Symposium on Foundations of Software Engineering
(Seattle, WA, USA) (FSE 2016). Association for Computing Machinery, New York,
NY, USA, 739â€“750. https://doi.org/10.1145/2950290.2950363

[21] Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic
Patch Generation Learned from Human-Written Patches. In Proceedings of the
2013 International Conference on Software Engineering (San Francisco, CA, USA)
(ICSE â€™13). IEEE Press, 802â€“811.

[22] Antti Laaksonen. 2020. Guide to Competitive Programming - Learning and
https:

Improving Algorithms Through Contests, Second Edition.
//doi.org/10.1007/978-3-030-39357-1

Springer.

[23] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2012.
GenProg: A Generic Method for Automatic Software Repair. IEEE Transactions
on Software Engineering 38, 1 (2012), 54â€“72. https://doi.org/10.1109/TSE.2011.104
[24] Junho Lee, Dowon Song, Sunbeom So, and Hakjoo Oh. 2018. Automatic Diagnosis
and Correction of Logical Errors for Functional Programming Assignments.
Proc. ACM Program. Lang. 2, OOPSLA, Article 158 (oct 2018), 30 pages. https:
//doi.org/10.1145/3276528

[25] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi
Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas
Hubert, Peter Choy, Cyprien de Masson dâ€™Autume, Igor Babuschkin, Xinyun
Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James
Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de
Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-Level Code
Generation with AlphaCode.

[26] Fan Long, Peter Amidon, and Martin Rinard. 2017. Automatic Inference of
Code Transforms for Patch Generation. In Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering (Paderborn, Germany) (ESEC/FSE
2017). Association for Computing Machinery, New York, NY, USA, 727â€“739.
https://doi.org/10.1145/3106237.3106253

[27] Fan Long and Martin Rinard. 2016. Automatic Patch Generation by Learn-
ing Correct Code. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages (St. Petersburg, FL, USA)
(POPL â€™16). Association for Computing Machinery, New York, NY, USA, 298â€“312.
https://doi.org/10.1145/2837614.2837617

[28] Yunlong Lu, Na Meng, and Wenxin Li. 2021. FAPR: Fast and Accurate Program
Repair for Introductory Programming Courses. CoRR abs/2107.06550 (2021).
arXiv:2107.06550 https://arxiv.org/abs/2107.06550

[29] Sergey Mechtaev, Manh-Dung Nguyen, Yannic Noller, Lars Grunske, and Ab-
hik Roychoudhury. 2018. Semantic program repair using a reference imple-
mentation. In Proceedings of the 40th International Conference on Software Engi-
neering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018, Michel Chau-
dron, Ivica Crnkovic, Marsha Chechik, and Mark Harman (Eds.). ACM, 129â€“139.
https://doi.org/10.1145/3180155.3180247

[30] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: Scalable
Multiline Program Patch Synthesis via Symbolic Analysis. In 2016 IEEE/ACM
38th International Conference on Software Engineering (ICSE). 691â€“701. https:
//doi.org/10.1145/2884781.2884807

[31] Khanh Nguyen and Guoqing Xu. 2013. Cachetor: Detecting Cacheable Data
to Remove Bloat. In Proceedings of the 2013 9th Joint Meeting on Foundations of
Software Engineering (Saint Petersburg, Russia) (ESEC/FSE 2013). Association for
Computing Machinery, New York, NY, USA, 268â€“278. https://doi.org/10.1145/
2491411.2491416

[32] Adrian Nistor, Po-Chun Chang, Cosmin Radoi, and Shan Lu. 2015. Caramel:
Detecting and Fixing Performance Problems That Have Non-Intrusive Fixes. In
Proceedings of the 37th International Conference on Software Engineering - Volume
1 (Florence, Italy) (ICSE â€™15). IEEE Press, 902â€“912.

[33] David M. Perry, Dohyeong Kim, Roopsha Samanta, and Xiangyu Zhang. 2019.
SemCluster: Clustering of Imperative Programming Assignments Based on Quan-
titative Semantic Features. In Proceedings of the 40th ACM SIGPLAN Conference
on Programming Language Design and Implementation (Phoenix, AZ, USA) (PLDI
2019). Association for Computing Machinery, New York, NY, USA, 860â€“873.
https://doi.org/10.1145/3314221.3314629

[34] Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, and Regina Barzilay.
2016. Sk_p: A Neural Program Corrector for MOOCs. In Companion Proceedings
of the 2016 ACM SIGPLAN International Conference on Systems, Programming,

Conferenceâ€™17, July 2017, Washington, DC, USA

Jialu Zhang, De Li, John C. Kolesar, Hanyuan Shi, and Ruzica Piskac

Languages and Applications: Software for Humanity (Amsterdam, Netherlands)
(SPLASH Companion 2016). Association for Computing Machinery, New York,
NY, USA, 39â€“40. https://doi.org/10.1145/2984043.2989222

[35] Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi,
Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir R. Choudhury, Lindsey Decker,
Veronika Thost, Luca Buratti, Saurabh Pujar, and Ulrich Finkler. 2021. Project
CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding
Tasks. CoRR abs/2105.12655 (2021). arXiv:2105.12655 https://arxiv.org/abs/2105.
12655

[36] Yuhua Qi, Xiaoguang Mao, Yan Lei, Ziying Dai, and Chengsong Wang. 2014.
The Strength of Random Search on Automated Program Repair. In Proceedings
of the 36th International Conference on Software Engineering (Hyderabad, India)
(ICSE 2014). Association for Computing Machinery, New York, NY, USA, 254â€“265.
https://doi.org/10.1145/2568225.2568254

[37] R. Rolim, G. Soares, L. Dâ€™Antoni, O. Polozov, S. Gulwani, R. Gheyi, R. Suzuki, and
B. Hartmann. 2017. Learning Syntactic Program Transformations from Examples.
In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE).
404â€“415.

[38] Rishabh Singh, Sumit Gulwani, and Armando Solar-Lezama. 2013. Automated
Feedback Generation for Introductory Programming Assignments. In Proceedings
of the 34th ACM SIGPLAN Conference on Programming Language Design and
Implementation (Seattle, Washington, USA) (PLDI 13). Association for Computing
Machinery, New York, NY, USA, 15â€“26. https://doi.org/10.1145/2491956.2462195
[39] Dowon Song, Woosuk Lee, and Hakjoo Oh. 2021. Context-Aware and Data-Driven
Feedback Generation for Programming Assignments. Association for Comput-
ing Machinery, New York, NY, USA, 328â€“340. https://doi.org/10.1145/3468264.

3468598

[40] Ke Wang, Rishabh Singh, and Zhendong Su. 2018. Search, Align, and Repair:
Data-Driven Feedback Generation for Introductory Programming Exercises (PLDI
2018). Association for Computing Machinery, New York, NY, USA, 481â€“495.
https://doi.org/10.1145/3192366.3192384

[41] Ke Wang, Zhendong Su, and Rishabh Singh. 2018. Dynamic Neural Program
Embeddings for Program Repair. In International Conference on Learning Repre-
sentations. https://openreview.net/forum?id=BJuWrGW0Z

[42] Michihiro Yasunaga and Percy Liang. 2021. Break-It-Fix-It: Unsupervised
Learning for Program Repair. CoRR abs/2106.06600 (2021). arXiv:2106.06600
https://arxiv.org/abs/2106.06600

[43] Jooyong Yi, Umair Z. Ahmed, Amey Karkare, Shin Hwei Tan, and Abhik Roy-
choudhury. 2017. A Feasibility Study of Using Automated Program Repair for
Introductory Programming Assignments. In Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering (Paderborn, Germany) (ESEC/FSE
2017). Association for Computing Machinery, New York, NY, USA, 740â€“751.
https://doi.org/10.1145/3106237.3106262

[44] Maksym Zavershynskyi, Alexander Skidanov, and Illia Polosukhin. 2018.
CoRR abs/1807.03168 (2018).

NAPS: Natural Program Synthesis Dataset.
arXiv:1807.03168 http://arxiv.org/abs/1807.03168

[45] Kaizhong Zhang and Dennis Shasha. 1989. Simple Fast Algorithms for the Editing
Distance between Trees and Related Problems. SIAM J. Comput. 18, 6 (1989),
1245â€“1262. https://doi.org/10.1137/0218082

[46] Kaizhong Zhang and Dennis E. Shasha. 1989. Simple Fast Algorithms for the
Editing Distance Between Trees and Related Problems. SIAM J. Comput. 18, 6
(1989), 1245â€“1262. https://doi.org/10.1137/0218082

