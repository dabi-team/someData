T

F

A

R

D

2
2
0
2

n
u
J

8
2

]
E
S
.
s
c
[

1
v
8
6
0
4
1
.
6
0
2
2
:
v
i
X
r
a

Under consideration for publication in Formal Aspects of Computing

FuSeBMC v4 : Improving code
coverage with smart seeds via fuzzing
and static analysis
Kaled M. Alshmrany1 2, Mohannad Aldughaim1, Ahmed Bhayat1, Fedor Shmarov1,

Fatimah Aljaafari1 and Lucas C. Cordeiro1
1University of Manchester, Manchester, UK
2Institute of Public Administration, Jeddah, Saudi Arabia

Abstract. Bounded model checking (BMC) and fuzzing techniques are among the most eﬀective methods
for detecting errors and security vulnerabilities in software. However, there is still a shortcoming in detecting
these errors due to the inability to cover large areas in the target code. Coverage standards and measures
are also an excellent way to ascertain the eﬀectiveness of the test suite. We propose FuSeBMC v4, a test
generator that relies on smart seeds to improve the hybrid fuzzer to achieve high C programs coverage.
First, FuSeBMC analyses and incrementally injects goal labels into the given C program to guide BMC and
Evolutionary Fuzzing engines. Also, It ranks these goal labels according to the given strategy. After that,
the engines are employed to produce smart seeds quickly to use later. Then, FuSeBMC coordinates between
the engines and seed distribution by the Tracer. This Tracer generally manages the tool to record the goals
covered and transfer the information between the engines by providing a shared memory to harness the power
and take advantage of the power of each engine. So that the BMC engine helps give the seed that makes
the fuzzing engine not struggle with complex mathematical guards. Furthermore, Tracer evaluates test cases
dynamically to convert high-impact cases into seeds for subsequent test fuzzing. As a result, we received
three awards for participation in the fourth international competition in software testing (Test-Comp 2022),
outperforming all state-of-the-art tools in every category, including the coverage category.

Keywords: Code Coverage; Coverage Branches; Automated Test Generation; Bounded Model Checking;
Fuzzing; Security.

1. Introduction

Fuzzing is one of the essential techniques for discovering software bugs and is used by major corporations
such as Microsoft [GLM12] and Google [CE]. Fuzzers work by constructing inputs known as seeds and then

Correspondence and oﬀprint requests to: Kaled M. Alshmrany, University of Manchester, Manchester, M13 9QQ, UK e-mail:
kaled.alshmrany@postgrad.manchester.ac.uk

 
 
 
 
 
 
2

K. M. Alshmrany et al.

running the program under test (PUT) on these seeds. The goal is to discover a bug by causing the PUT to
crash. A secondary but essential goal is to cover as many program branches as possible since a bug occurring
on a branch cannot be discovered if the branch is not explored. Broadly, fuzzers can be categorized in three
ways. Firstly, blackbox fuzzers do not analyze the target program when generating seeds. Secondly, whitebox
fuzzers extensively analyze the target programs to guide the seed generation to explore particular branches.
Lastly, greybox fuzzing uses limited program analysis and feedback from the program under test to guide
the input generation. Finally, there are also hybrid fuzzers that combine techniques.

The main disadvantage of blackbox fuzzers is that due to the random manner they generate inputs, they
are often unable to explore program paths with complex guards. Whitebox fuzzers, on the other hand, are
very good at using program information to circumvent guards but are often slow and resource-intensive to
run. Greybox fuzzing techniques such as the American Fuzzing Loop [Zal15] appear to oﬀer a sweet spot in
terms of eﬀort per input. However, they still have some fundamental weaknesses; the most important is that
the straightforward way they generate seeds can lead to the fuzzer becoming stuck in one part of the code and
not exploring other branches. Hybrid fuzzing attempts to circumvent this issue by more signiﬁcant program-
speciﬁc analysis. One common technique is concolic fuzzing, which involves using a theorem prover to solve
path constraints and thereby help the fuzzer explore deeper into the program [BCD+18, MWT+20, SGS+16].
In this paper, we present FuSeBMC, a state-of-the-art hybrid fuzzer that incorporates various innovative
features and techniques. This journal paper is based on several previously published conference papers
[AABC21, AABC22]. We extend those papers by (i) discussing FuSeBMC in greater detail (Section 3) (ii)
providing more examples, and (iii) providing a thorough and up-to-date experimental evaluation of the tool
(Section 4).

One of the primary features of FuSeBMC discussed in this paper is the linking of a greybox fuzzer
with a bounded model checker. A bounded model checker works by treating a program as a state transition
system and then checking whether there exists a transition in this system of length less than some bound k
that violates the property to be veriﬁed [Bie09, CFM12]. In this work, we use ESBMC, an eﬃcient bounded
model checker for C++ and C with support for checking many safety properties fully automatically [CFM12].
ESBMC works by translating the property to check and the bounded transition system into quantiﬁer-free
ﬁrst-order logic. SMT-solvers are then run to ﬁnd a model for
P (the negated property) and C (the
translated transition system). Finally, a counterexample is extracted representing the set of assignments
required to violate the property if a model is found.

¬

Bounded model checkers such as ESBMC are now mature software, used industrially [GMM+18] and
capable of ﬁnding bugs in production software. We leverage this power of model checkers as a method for
smart seed generation. During greybox fuzzing, if a particular branch has not been explored, ESBMC can
be used to provide a model (set of assignments to input variables) that will reach the branch. This model is
then used as a seed for further greybox fuzzing. The technique is implemented in FuSeBMC [AMGC20].

An important FuSeBMC subsystem is the tracer which coordinates the bounded model checker and the
various fuzzing engines. The tracer monitors the test cases produced by the fuzzers. It selects those with the
highest impact (as measured by a couple of metrics discussed in Section 3) to act as seeds for future rounds
of fuzzing. Further, as discussed above, ESBMC produces test cases to cover particular branches. However,
a test case it produces may also cover branches other than the one targeted. In order to ascertain precisely
which branches a test case covers and thereby prevent ESBMC from running multiple times unnecessarily,
the tracer takes a test case produced by ESBMC and runs the PUT on it, recording all goals covered.

Bounded model checking can be slow and resource-intensive. To mitigate against this, FuSeBMC does
not make use of an oﬀ-the-shelf fuzzer for its grey box fuzzing, but instead uses a modiﬁed version of the
popular American Fuzzy Lop tool. One of the features of this modiﬁed fuzzer, is its ability to carry out
lightweight static analysis of a program to recognize input veriﬁcation. It analyzes the code for conditions
on the input variables and ensures that seeds are only selected if they pass these conditions. This reduces
the dependence on the computationally expensive bounded model checker for ﬁnding quality seeds. Another
interesting feature of the modiﬁed fuzzer is that it analyses the PUT and heuristically identiﬁes potentially
inﬁnite loops. It then bounds these loops in an attempt to speed up fuzzing. These bounds are incremented
during the multiple fuzzing rounds.

Together, these features turn FuSeBMC into a leading fuzzer. In the 2022 edition of the Test-Comp
software testing competition, FuSeBMC achieved ﬁrst place in both the main categories, Cover-Error and
Cover-Branches. In the Cover-Branches category, it achieved ﬁrst place in 9 out of the 16 subcategories that
it participated in. In the Cover-Error category, it achieved ﬁrst place, or joint ﬁrst place, in 8 out of the 14
subcategories that it participated in.

FuSeBMC v4 : Improving code coverage with smart seeds via fuzzing and static analysis

3

Contributions. This journal paper explains the latest developments to the FuSeBMC fuzzer. The work
presented here is a substantial extension of our previous published conference papers [AMGC20, AABC21,
AABC22]. FuSeBMC ’s main new features can be summarised as follows:

•

•

•

•

•

•

Reduce the overhead of the fuzzing by using static analysis to manage the mutation process to speed up
the fuzzing process.
Describe a novel seed generation procedure, which is beneﬁcial for programs with complex input val-
idations. FuSeBMC relies on a bounded model checker and various fuzzing engines to create seeds. It
leverages the power of model checkers to provide seeds that can circumvent complex mathematical guards.
Also, we beneﬁt from the lightweight static program analysis to recognize certain restricted forms of input
veriﬁcation, which can help produce quality seeds quickly.
Prioritise exploring longer program paths as they may provide higher code coverage and generating fewer
test cases, as the result.
Set a loop unwinding depth during seed generation and fuzzing. As loop unwinding leads to exponen-
tial path explosion, we restrict the unwinding depth of each loop to a small number, depending on an
approximate estimate of the number of program paths.
Use static analysis to restrict the range of inputs for the fuzzer to explore, which is part of the Clang
project [CLA15], to restrict the range of inputs for the fuzzer to explore.
Provide a detailed analysis of our participation in the international competition on software testing (Test-
Comp 2022), where our tool FuSeBMC was able to achieve three signiﬁcant awards. FuSeBMC earned
ﬁrst place in all the categories by the improvements described in this manuscript.

2. Preliminaries

This section brieﬂy introduces various fuzzing techniques, including general gray box fuzzing, white box
fuzzing, and hybrid fuzzing. In addition, it explains the various code coverage metrics in a simpliﬁed manner.
Lastly, it introduces the technique of bounded model checking.

2.1. Fuzzing

Fuzzing is one of the most eﬀective software testing techniques for ﬁnding security vulnerabilities in software
systems. It automatically generates inputs, i.e., test cases, passes them to a target program, and checks
for abnormal behavior or code coverage. The generated input should be well-formed and accepted by the
software system to be considered a valid test case. The inputs’ structure can be, for example:

1. Very long or completely blank strings.
2. Min/max values of integers, or only zero and negative values.
3. Include unique values or characters likely to trigger bugs.

Figure 1 illustrates the main smart fuzzing approach [LZZ18]. It comprises four main steps, the inputs
generation step, inputs running step, program execution step and monitoring, and analysis of exceptions step.
Generally, it starts with the target program and initial inputs, which can be any ﬁle format (e.g., images,
text, videos, etc.), network communication data, and executable binaries. Generating malformed inputs is
the main challenge for fuzzers [LZZ18]. So, commonly, two kinds of generators are employed in state-of-the-
art fuzzers, generation-based, which generates the inputs from scratch, or mutation-based, which modiﬁes
existing inputs. Inputs are fed to target programs after being generated in the previous step. Then, the
execution state is monitored by the fuzzer during the execution of the PUT to detect crashes or abnormal
behaviors. When the fuzzer detects a violation, it stores the related test cases for later usage and analysis.
Then, the analyzer will attempt to determine the location and the root cause of the detected violation in
the analysis step. Also, in this step, it will collect information about the data ﬂow and program control ﬂow
and use this collected information to enhance the input generation and pass it again to the generator step
as smart input.

As the primary goal of fuzzing is to ﬁnd more crashes with applicable inputs, the most intuitive per-
formance metric is the rate of crashes per time. However, crashes rarely occur, so most existing fuzzers are
designed to maximize code coverage. By maximizing the code coverage, the fuzzer will test more paths in

4

K. M. Alshmrany et al.

Fig. 1. Traditional fuzzing process architecture. It describes the main phases of the traditional fuzzing process.
The process starts with initial inputs used for fuzzing the target program. Then, it monitors and analyzes
the fuzzing process to report any crash.

the program, which increases the probability of causing crashes [KY21]. Many undiscovered crashes occur
in deep code paths, in parts of codes that are not executed frequently. Therefore, one of the critical roles of
fuzzing is ﬁnding inputs that increase the code coverage.

2.2. White Box Fuzzing

Whitebox fuzzing combines fuzz testing with symbolic execution [GKL08]. It symbolically executes the
program under test with initial concrete inputs tracking every conditional statement, producing constraints
over those inputs. The collected constraints capture how the program uses its inputs. The fuzzer then
systematically negates each constraint and solves them using a constraint solver, creating new inputs which
exercise diﬀerent program paths [GLM+08]. For example, for a symbolic variable, i with initial test case
i = 0, followed by branch condition “if i = 10 then”, would generate i
= 10 constraint as a result of the
execution. A constraint solver can solve this formula and obtain a concrete value that would satisfy this new
path by negating this constraint. This process can be repeated for the newly created inputs and can be used
to maximize code coverage.

2.3. Hybrid Fuzzing

In recent years, hybrid fuzzing has been widely researched and discussed in the security ﬁeld [YLX+18].
However, since fuzz testing fails to explore all programs’ state space, hybrid fuzzing employs the eﬃciency
of fuzzing with the eﬀectiveness of concolic execution. Concolic execution is a combination technique of
symbolic execution and concrete execution. The idea is to help fuzzer by using concolic execution to reach
diﬀerent branches in the PUT. Also, it generates new inputs by negating and solving the branches in the
execution path of a speciﬁc input with the help of (SMT) solvers.

Generally, the advantage of hybrid fuzzing is that for complex checks, which are challenging to ﬁnd
by mutation-based fuzzing, concolic execution can help solve those checks and reach unlikely paths (e.g.,
limiting code coverage in breadth). While fuzzing can quickly ﬁnd deep paths with simple checks that will
balance the large resources consumption of concolic execution. Figure 2 illustrates the diﬀerence by showing
the explored path for each technique which consists of the colored nodes.

Existing hybrid fuzzing work [SGS+16, YLX+18] showed better test results and more code coverage than

using fuzzing or symbol execution alone.

2.4. Coverage Code

One of the challenges in software testing is assigning a quality measure to test cases such that higher-quality
test cases detect more bugs than low-quality test cases. These quality measures are called test adequacy
criteria. Code coverage is the commonly used test adequacy criterion [Hem15]. Generally, code coverage is a
measure of identifying the parts of PUT that execute when running the program and the degree to which a
test case covers the PUT [IPJF19]. Code coverage methods can be divided into three standard criteria:

Bugs / CrashesFuzzer(Inputs)Test Generation(Monitor)Program Under TestViolation?NoYes(cid:54)
FuSeBMC v4 : Improving code coverage with smart seeds via fuzzing and static analysis

5

(a) Concolic execution

(b) Fuzzing

Fig. 2. Comparison of Concolic execution and Fuzzing based on path exploration. Concolic execution shows
its ability to explore all possible paths, but when it goes into deep, it cannot be improved practically because
the number of paths quickly becomes exponential. On the other hand, Fuzzing can guarantee to explore
deeper because of its speed, but it is limited in code coverage in width.

1 int a , int b ;
2 int total = a + b ;
3 if ( total >0) {
4
5 else
6
7 }

printf ( " Positive % d " , total ) ;

printf ( " Negative % d " , total ) ;

1 void foo ( int a )
2 {
3 if (a >8)
a = a *2;
4
5 printf ( " a =% d " ,a ) ;
6 }

1 int a ;
2 if (a >5)
3 {
4
5 else :
6
7 }

add ( a ) ;

multiply ( a ) ;

(a)

(b)

(c)

Fig. 3. Some simple programs we use to demonstrate various code coverage concepts.

•

•

•

Statement Coverage: Each program code statement must be executed at least once to reach full state-
ment coverage. To calculate statement coverage, we calculate the total number of executed statements
in the PUT out of the total statements present in the PUT. Figure 3a depicts a simple program. To
illustrate the calculation of statement coverage for this program, consider the case where variable a = 3
and variable b = 7. The statements executed in this case are those on lines 1-4 and 7. So, the number of
executed statements is 5 and the total number of statements in 3a is 7. Thus, the statement coverage for
the program in ﬁgure 3a with inputs a = 3 and b = 7 is 5
Branch coverage: Examines all program branches for each decision point in the code (e.g., if statements,
loops) has been executed at least once during testing. Figure 3b illustrate a simple example of code to
calculate branch coverage. This code has 3 branches, two from the conditional branch from line 3
4 and
5, if a = 10, condition on line 3 will be
from 3
−
true, then in line 4 a = 20 and in line 5 a will be printed. So, the statement coverage 2
3 (100) = 67%. If
a = 3 then line 5 will execute. So, the branch coverage will be 1
Function coverage: Measures the number of functions in PUT that are covered during testing. To
achieve complete function coverage, every function in the PUT must be covered by some input. Consider
ﬁgure 3c. It contains two functions (add and multiply). If a = 10, then only function add will be executed
and thus the function coverage is 1

5 and the third as unconditional branch from line 4

7 (100) = 71%

3 (100) = 33%

−

−

2 (100) = 50%.

FuSeBMCv4:Improvingcodecoveragewithsmartseedsviafuzzingandstaticanalysis51234567891011128(a)Concolicexecution1234567891011128(b)FuzzingFig.2.ComparisonofConcolicexecutionandFuzzingbasedonpathexploration.Concolicexecutionshowsitsabilitytoexploreallpossiblepaths,butwhenitgoesintodeep,itcannotbeimprovedpracticallybecausethenumberofpathsquicklybecomesexponential.Ontheotherhand,Fuzzingcanguaranteetoexploredeeperbecauseofitsspeed,butitislimitedincodecoverageinwidth.1inta,intb;2inttotal=a+b;3if(total>0){4printf("Positive%d",total);5else6printf("Negative%d",total);7}(a)1voidfoo(inta)2{3if(a>8)4a=a*2;5printf("a=%d",a);6}(b)1inta;2if(a>5)3{4add(a);5else:6multiply(a);7}(c)Fig.3.Somesimpleprogramsweusetodemonstratevariouscodecoverageconcepts.•StatementCoverage:Eachprogramcodestatementmustbeexecutedatleastoncetoreachfullstate-mentcoverage.Tocalculatestatementcoverage,wecalculatethetotalnumberofexecutedstatementsinthePUToutofthetotalstatementspresentinthePUT.Figure3adepictsasimpleprogram.Toillustratethecalculationofstatementcoverageforthisprogram,considerthecasewherevariablea=3andvariableb=7.Thestatementsexecutedinthiscasearethoseonlines1-4and7.So,thenumberofexecutedstatementsis5andthetotalnumberofstatementsin3ais7.Thus,thestatementcoveragefortheprograminﬁgure3awithinputsa=3andb=7is57(100)=71%•Branchcoverage:Examinesallprogrambranchesforeachdecisionpointinthecode(e.g.,ifstatements,loops)hasbeenexecutedatleastonceduringtesting.Figure3billustrateasimpleexampleofcodetocalculatebranchcoverage.Thiscodehas3branches,twofromtheconditionalbranchfromline3 4andfrom3 5andthethirdasunconditionalbranchfromline4 5,ifa=10,conditiononline3willbetrue,theninline4a=20andinline5awillbeprinted.So,thestatementcoverage23(100)=67%.Ifa=3thenline5willexecute.So,thebranchcoveragewillbe13(100)=33%•Functioncoverage:MeasuresthenumberoffunctionsinPUTthatarecoveredduringtesting.Toachievecompletefunctioncoverage,everyfunctioninthePUTmustbecoveredbysomeinput.Considerﬁgure3c.Itcontainstwofunctions(addandmultiply).Ifa=10,thenonlyfunctionaddwillbeexecutedandthusthefunctioncoverageis12(100)=50%.FuSeBMCv4:Improvingcodecoveragewithsmartseedsviafuzzingandstaticanalysis51234567891011128(a)Concolicexecution1234567891011128(b)FuzzingFig.2.ComparisonofConcolicexecutionandFuzzingbasedonpathexploration.Concolicexecutionshowsitsabilitytoexploreallpossiblepaths,butwhenitgoesintodeep,itcannotbeimprovedpracticallybecausethenumberofpathsquicklybecomesexponential.Ontheotherhand,Fuzzingcanguaranteetoexploredeeperbecauseofitsspeed,butitislimitedincodecoverageinwidth.1inta,intb;2inttotal=a+b;3if(total>0){4printf("Positive%d",total);5else6printf("Negative%d",total);7}(a)1voidfoo(inta)2{3if(a>8)4a=a*2;5printf("a=%d",a);6}(b)1inta;2if(a>5)3{4add(a);5else:6multiply(a);7}(c)Fig.3.Somesimpleprogramsweusetodemonstratevariouscodecoverageconcepts.•StatementCoverage:Eachprogramcodestatementmustbeexecutedatleastoncetoreachfullstate-mentcoverage.Tocalculatestatementcoverage,wecalculatethetotalnumberofexecutedstatementsinthePUToutofthetotalstatementspresentinthePUT.Figure3adepictsasimpleprogram.Toillustratethecalculationofstatementcoverageforthisprogram,considerthecasewherevariablea=3andvariableb=7.Thestatementsexecutedinthiscasearethoseonlines1-4and7.So,thenumberofexecutedstatementsis5andthetotalnumberofstatementsin3ais7.Thus,thestatementcoveragefortheprograminﬁgure3awithinputsa=3andb=7is57(100)=71%•Branchcoverage:Examinesallprogrambranchesforeachdecisionpointinthecode(e.g.,ifstatements,loops)hasbeenexecutedatleastonceduringtesting.Figure3billustrateasimpleexampleofcodetocalculatebranchcoverage.Thiscodehas3branches,twofromtheconditionalbranchfromline3 4andfrom3 5andthethirdasunconditionalbranchfromline4 5,ifa=10,conditiononline3willbetrue,theninline4a=20andinline5awillbeprinted.So,thestatementcoverage23(100)=67%.Ifa=3thenline5willexecute.So,thebranchcoveragewillbe13(100)=33%•Functioncoverage:MeasuresthenumberoffunctionsinPUTthatarecoveredduringtesting.Toachievecompletefunctioncoverage,everyfunctioninthePUTmustbecoveredbysomeinput.Considerﬁgure3c.Itcontainstwofunctions(addandmultiply).Ifa=10,thenonlyfunctionaddwillbeexecutedandthusthefunctioncoverageis12(100)=50%.6

K. M. Alshmrany et al.

Fig. 4. The Framework of FuSeBMC v4. This ﬁgure illustrates the main components of FuSeBMC. Our tool
starts by instrumenting and analyzing the source code, then performs coverage analysis in two stages: seed
generation and test generation.

2.5. Bounded Model Checking

Bounded model checking unwinds the program until it ﬁnds a property violation or exhausts time or memory
limits. It uses symbolic execution to unroll the program’s loops up to the user-deﬁned positive bound k. It
generates the unrolled program’s static single assignments (SSA) form, which is then automatically translated
into a ﬁrst-order logic formula C. Given a formula P representing a property that needs to be checked in
the veriﬁed program, an SMT solver decides the satisﬁability of C
P . A counterexample to P has been
discovered if the formula is satisﬁable. Otherwise, it can be concluded that P holds in the given program up
to the given context-bound k.

∧ ¬

3. Proposed Approach

FuSeBMC combines dynamic and static veriﬁcation techniques for improving code coverage and bug dis-
covery. It utilises the Clang compiler [CLA15] front-end to perform various code transformations, ESBMC
(Eﬃcient SMT-based Bounded Model Checking) [GMM+20, GMCN19] as a BMC and symbolic execution
engine, and a modiﬁed version of the American Fuzzy Lop (AFL) tool [BPNR17, ame21] as well as a custom
concolic selective fuzzer [AABC21] as fuzzing engines.

FuSeBMC takes a C program as input and produces a set of test cases maximizing code coverage while
also checking for various bugs. Users have full control over what types of bugs they are interested in ﬁnding
(such as array bounds violations, divisions by zero, pointers safety, arithmetic overﬂows, memory leaks, and
other user-deﬁned properties). Figure 4 illustrates its architecture, and Algorithm 1 presents the main stages
of the FuSeBMC workﬂow.

3.1. Overview

FuSeBMC begins by injecting goal labels into the given C program and ranking them according to the given
strategy (i.e., depending on the goal’s origin or depth in the reachability graph). From then on, FuSeBMC’s
workﬂow can be divided into two main stages: seed generation (the preliminary stage) and test generation
(the full coverage analysis stage). During seed generation, FuSeBMC applies the reachability analysis engines
(fuzzers and BMC) to the instrumented code for a short time to produce seeds that are used by the fuzzers
at the test generation stage and test cases that may provide coverage of some “shallow” goals. During test
generation, the above engines are applied with a longer timeout while accompanied by another analysis engine
called the Tracer . It coordinates the execution of the fuzzers and the bounded model checker and handles
the passing of information between them. In particular, the Tracer maintains the program’s execution graph
and records which labels in the graph have been covered by the test cases produced by these engines. This

C CodePropertyTest casesSeed GenerationSeedGenBMCAFLBug reportsUnreached goalsClang tooling infrastructureCode Instrumentation & AnalysisReachability GraphInstrumented C CodeAFLSelective concolic fuzzerBMCTest GenerationGoals covered TracerShared MemoryInput Byte StreamGoals Covered ArrayConsumed Input SizeInitial inputs sizeNew seedsSeedsImport seedsNew seedsFuSeBMC v4 : Improving code coverage with smart seeds via fuzzing and static analysis

7

// Code Instrumentation
// Reachability Graph Analysis
// initializing queues for test cases and bug reports
// Seed Generation

Algorithm 1: FuSeBMC algorithm
1: P (cid:48) := inject goal labels(P );
2: G := get list of sorted goals(P (cid:48));
3: T :=
;
4: S := generate seeds(P (cid:48));
// Test Generation
=

or timeout do

; B :=
∅

∅

∅

5: while G
6:
7:
8:
9:
10:
11:

g := G.pop();
testcases, Gcov
}
{
=
if Gcov
then
testcases;
T
G.remove goals(Gcov);

←

∅

:= run f uzzer(P (cid:48), S, f uzztimeout);

end if
// current goal has been covered, so skip to the next iteration
if g

Gcov then

continue;

∈
end if
// BMC’s output can be a reachability witness or a bug trace
output, res
{
if res = success then
output;
generate testcases(P (cid:48),

:= run bmc(P (cid:48), g, bmctimeout);

);

}

S
←
T
←
Gcov
G.remove goals(Gcov);

←

run tracer(P (cid:48), output);

output
}
{

if output

=

then

generate bug report(P (cid:48), output);

∅

generate testcases(P (cid:48), S);

12:
13:
14:

else

15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28: end while
29: return

B
else
T
←
end if

end if

←

T, B, G
}
{

;

is done to prevent the computationally expensive BMC engine from trying to reach an already covered goal.
FuSeBMC runs reachability analysis engines until all goals are covered or a timeout is reached.

In Figure 5 we introduce a short C program which we use as a running example to demonstrate the main
code transformations throughout this section. The presented program accepts coeﬃcients of a quadratic
polynomial and an integer candidate solution in the range [1,100] as input from the user. It terminates
successfully if the provided candidate solves the equation. However, the program returns an error if the given
equation does not have real solutions or the input candidate value is outside the [1,100] range.

3.2. Code Instrumentation

FuSeBMC uses Clang tooling infrastructure [CLA15] at its front-end to parse the input C program and
traverse the resulting Abstract Syntax Tree (AST), recursively injecting goal labels into every branch. By
default, FuSeBMC inserts labels inside every conditional statement, loop and function and adds declarations
for several intrinsic functions used throughout the code coverage analysis. The resulting instrumented code is
functionally equivalent to the original C program. Figure 5b demonstrates an example of the described code
instrumentation for the program in Figure 5a. Apart from the required code instrumentation, Clang produces
compilation error and warning messages and utilizes its static analyzer to simplify the input program (e.g.,
calculating the sizes of expressions, evaluating static asserts, and performing constants propagation) [LA04].

(cid:54)
(cid:54)
(cid:54)
8

K. M. Alshmrany et al.

return ( a * x * x + b * x + c == 0) ;

int a = _ _ V E R I F I E R _ n o n d e t _ i n t () ;
int b = _ _ V E R I F I E R _ n o n d e t _ i n t () ;
int c = _ _ V E R I F I E R _ n o n d e t _ i n t () ;
if ( b * b >= 4* a * c ) {

1 # include < assert .h >
2 void reach_error () { assert (0) ; }
3
4 bool check ( int a , int b , int c , int x ) {
5
6 }
7
8 int main () {
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24 return 0;
25 }

int x = _ _ V E R I F I E R _ n o n d e t _ i n t () ;
if ( x <= 0 || x > 100)

if ( check (a , b , c , x ) )

reach_error () ;

reach_error () ;

while (1) {

return 0;

}
else

}

(a)

GOAL_1 :;
assert (0) ;

GOAL_2 :;
return ( a * x * x + b * x + c == 0) ;

GOAL_0 :;
int a = _ _ V E R I F I E R _ n o n d e t _ i n t () ;
int b = _ _ V E R I F I E R _ n o n d e t _ i n t () ;
int c = _ _ V E R I F I E R _ n o n d e t _ i n t () ;
if ( b * b >= 4* a * c ) {

1 # include < assert .h >
2 void reach_error () {
3
4
5 }
6
7 bool check ( int a , int b , int c , int x ) {
8
9
10 }
11
12 int main () {
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39 }

GOAL_4 :;
while (1) {
GOAL_6 :;
int x = _ _ V E R I F I E R _ n o n d e t _ i n t () ;
if ( x <= 0 || x > 100) {

}
if ( check (a , b , c , x ) ) {

GOAL_5 :;
reach_error () ;

GOAL_8 :;
reach_error () ;

}
GOAL_3 :;
return 0;

GOAL_9 :;
return 0;

}
GOAL_7 :;

}
else {

}

(b)

Fig. 5. An example of a) a C program, b) the corresponding instrumented code and c) the resulting reacha-
bility graph.

(c)

3.3. Reachability Graph Analysis

FuSeBMC analyzes the instrumented C program and builds the reachability graph attributing each goal
label with its origin information (i.e. if statement, while loop, end of function), its depth in the reachability
graph and some additional metadata. Then FuSeBMC sorts all the goals using one of the two strategies
(line 2 of Algorithm 1): (1) based on their depth (i.e., the deepest goals are analyzed ﬁrst), or (2) based
on their rank scores calculated as a product of a goal’s depth and its power score - value between 1 and 5

04536718921depth = 1depth = 2depth = 3depth = 4depth = 51104920431620252034FuSeBMC v4 : Improving code coverage with smart seeds via fuzzing and static analysis

9

describing the goal’s branching power. Each power score has been decided via the experimental analysis. The
if statement goals are assigned a score of 5 (goals 4, 8 and 9 in Figure 5b), the compound statement (i.e.,
non-main function body) goals - 4 (goals 1 and 2 in Figure 5b), the loop goals - 3 (goal 6 in Figure 5b) and
the else goals - 2 (goal 5 in Figure 5b). All remaining types of goals (i.e., end-of-main (goal 3 in Figure 5b),
empty-else, and after-loop goals (goal 7 in Figure 5b)) are assigned a value of 1.

In general, the goal sorting improves overall FuSeBMC performance. Using the depth strategy, FuSeBMC
attempts to cover deep goals ﬁrst. This is beneﬁcial since all preceding goals on the path to a deep goal can
be ignored during subsequent fuzzing as the same test case covers them. At the same time, the rank scores
allow prioritizing conditional branches as they may lead to multiple goals increasing potential code coverage
and compound goals which allows more eﬃcient function coverage.

Figure 5c features the resulting reachability graph for the instrumented code from Figure 5b. Note that
FuSeBMC builds the graph based on the original Clang AST without analyzing the code for trivially un-
reachable goals. For example, labels GOAL 7 and GOAL 3 can never be reached during the program’s execution.
However, this will not be reﬂected in the reachability graph.

The goal’s depth value is assigned at its highest depth. Therefore, labels GOAL 1 and GOAL 3 are assigned
depth values of 5 and 4, respectively. When the ﬁrst ranking strategy is applied, two goals at the same depth
are ordered in the ascending order of their label names. Using the second ranking strategy, two goals with the
same rank value are processed in the “power score ﬁrst” manner. For example, goal 8 will be placed in front
of goal 1 since it has a higher power score (5 vs 4). Hence, FuSeBMC will process the goal labels in following
orders

using the ﬁrst and second sorting strategies, respectively.

,

9,1,2,3,8,6,7,4,5
}
{

9,8,1,2,4,6,5,3,7
}
{

3.4. Seed Generation

Having ranked the goals, FuSeBMC carries out seed generation (line 4 of Algorithm 1) as a preliminary
step before full coverage analysis (i.e., test generation) begins. In this phase, FuSeBMC simpliﬁes the target
program by limiting loop bounds and assuming a narrow range of values for input variables. FuSeBMC also
carries out a lightweight static analysis that can recognize code applying input veriﬁcation. It does this by
searching through the code for common syntactic veriﬁcation patterns that restrict the range of an input
variable’s values. FuSeBMC then extracts the information about the input ranges and modiﬁes the mutation
engine to produce random input sequences from the determined input ranges. The seed generation consists
of two main stages: SeedGen and ESBMC/AFL. The second stage involves running a pair of tools, ESBMC
and AFL, which are explained in detail in the following section.

SeedGen produces binary seeds (i.e., a stream of bytes) for a speciﬁc executable compiled from the non-
instrumented code. In addition, SeedGen generates random values based on some restrictions. For example,
it sets the range of input between -128 and 128 to check if any goals can be reached using this input range to
avoid consuming more time during this phase. Also, SeedGen takes into account the information generated
during the code instrumentation, such as a restricted range of inputs. Finally, SeedGen can generate one or
more seeds as a starting value for the modiﬁed AFL engine used during the next phase.

ESBMC and AFL are run for a short time to produce test cases that are then converted into seeds and
added to the seed storage (see Seeds in Figure 4). In particular, this step is carried out in the descending
order determined by: (1) the number of unique goal labels they cover,s and (2) the program depth they
reach. Such seeds are called smart due to their powerful eﬀect on code coverage. In this phase, FuSeBMC
tries to achieve synergy between the two engines. The BMC engine uses SMT solvers to produce test cases
that circumvent complex mathematical guards. Such guards (for example, lines 5 and 12 in Figure 5a) pose
a challenge to a fuzzer [SGS+16] as it relies on mutating the given seed randomly and is therefore unlikely
to satisfy the guard condition. Seeds produced by BMC can solve this issue since they can be passed to a
fuzzer which can then advance deeper behind the complex guards into the target program (which is usually
hard for a bounded model checker).

3.5. Test Generation

Following seed generation, FuSeBMC begins the main coverage analysis phase (lines 5 - 29 of Algorithm 1).
FuSeBMC incorporates three engines to carry out this analysis: two fuzzers and a bounded model checker.
The engines run with longer timeouts than during the seed generation stage. The fuzzers making use of
the smart seeds generated by the model checker. The fuzzers generate test cases by randomly mutating the
program’s input and running it to analyze code coverage. The model checker determines the reachability

10

K. M. Alshmrany et al.

of particular goal labels producing a witness in case of success or proof is provided that the goal cannot
be reached. FuSeBMC’s tracer component coordinates the above engines via shared memory (see Figure 4).
Suppose the model checker provides a witness (test case). In that case, the tracer executes the program
using the provided witness as an input to determine if any additional goals can be reached to prevent the
computationally intensive model checker and the fuzzers from analyzing these goals in the future. Also,
FuSeBMC coordinates the fuzzer and the model checker in such a way that the labels that the model checker
cannot reach are assigned to the fuzzer with the incomplete seeding so that it can start test generation
from the point where the model checker stopped. In the following subsections, we discuss the FuSeBMC
components involved in coverage analysis in greater detail.

3.5.1. Bounded Model Checker

A bounded model checker is used to check for the reachability of a given goal label within the instrumented
program (lines 15 - 28 of Algorithm 1). Suppose it concludes that the current goal is reachable. In that case,
the counterexample produced can be turned into a witness – a sequence of inputs that leads the program’s
execution to that goal label – which is then used to generate a test case. Every new test case thus discovered
is added to the Seed repository to be used by the fuzzer. Even if the BMC runs out of time or memory, its
progress in reducing the input ranges is saved and used to generate new seeds for the fuzzer.

FuSeBMC uses ESBMC - a state-of-the-art software veriﬁcation tool based on BMC - as the bounded
model checking engine. It supports several SMT solvers (Boolector [NPB14], Z3 [dMB08], and Yices [Dut14]),
and it can perform program simpliﬁcations to generate small SSA sets, using constant folding and various
arithmetic simpliﬁcations for integer and ﬂoating-point data types.

3.5.2. Fuzzers

FuSeBMC employs two fuzzing engines for coverage analysis: a selective concolic fuzzer [AABC21] and a
modiﬁed version of the American Fuzzy Lop (AFL) tool [ame21].

The modiﬁed AFL generates test cases based on the evolutionary algorithm implemented in AFL [BPNR17].

The standard algorithm implemented in the AFL tool works as follows. Firstly, an initial input stream of
ﬁxed size is generated using the provided seed (a random seed is used if not explicitly speciﬁed). Secondly,
the target program is repeatedly executed with the randomly mutated input. If the target program does not
reach any new states after multiple input mutation rounds, a new byte is added to the input stream, and the
mutation process restarts. The above algorithm continues until an internal timeout is reached or the fuzzer
ﬁnds inputs that fully cover the program. In general, the AFL’s mutation algorithm heavily relies on the
quality of the initial inputs - seeds - for providing higher code coverage. Therefore, generating seeds with
higher coverage potential is crucial.

FuSeBMC modiﬁes the original AFL fuzzer as follows. Firstly, the instrumented program is simpliﬁed
to minimize its execution overhead by limiting the bounds of loops heuristically identiﬁed as potentially
inﬁnite. Note that these bounds can be iteratively changed between the AFL runs. Secondly, we utilize the
seeds produced by BMC as initial inputs to the fuzzer, since these inputs may already cover one of the goals.
Thirdly, the mutation operators are modiﬁed by accepting only inputs from the ranges identiﬁed during the
code analysis. Finally, we use the Consumed Input Size component of the shared memory to set the initial
input size every time the AFL is called so that it does not start generating input from 0 bytes at each
iteration of the loop in Algorithm 1.

3.5.3. Tracer

The tracer is a subsystem that determines the goals covered by test cases produced by the bounded model
checker. Whenever the model checker produces a test case to cover a particular goal, the tracer runs the
PUT on the test to ascertain whether it covers other uncovered goals besides the target goal. This analysis
prevents unnecessary execution of the computationally expensive model checker for those goals. The tracer
works by compiling the instrumented program together with the witness provided by the model checker and
running the resulting executable. It generates information about the input size, the input variables’ type
size, and the visited goals. This information is dynamically updated in the Shared Memory (i.e., the Goals
Covered Array and the Consumed Input Size components in Figure 4). The tracer also analyses the test cases
produced by the other two engines to add the highest impact cases (i.e., the test cases leading to new goals
or reaching the maximum analysis depth) to the Seeds store.

FuSeBMC v4 : Improving code coverage with smart seeds via fuzzing and static analysis

11

In general, FuSeBMC ’s tracer combines the strengths of both types of engines since it allows the fuzzers
to bypass complex guards - a task considered challenging for fuzzers - and explore deeper paths by using
the seeds produced by BMC. In this way, the fuzzers are navigated towards deeper goals which are harder
to analyze by BMC due to the increased state space.

4. Evaluation

4.1. Description of Benchmarks and Setup

We evaluated FuSeBMC on a subset of test generation tasks from Test-Comp 2022 [Bey22]. The vast majority
of Test-COMP benchmarks are taken from SV-COMP [Bey21] – the largest and most diverse open-source
repository of software veriﬁcation tasks. Our evaluation task set yields 4236 test tasks: 776 test tasks in
the Error Coverage category and 3460 test tasks in the Code Coverage category. Both categories contain C
programs with loops, arrays, bit-vectors, ﬂoating-point numbers, dynamic memory allocation and recursive
functions, event-condition-action software, concurrent programs, and BusyBox1 software.

The experiments were conducted on a Test-Comp server featuring an 8-core (4 physical cores) Intel Xeon
E3-1230 v5 CPU @ 3.4 GHz, 33 GB of RAM and running x86-64 Ubuntu 20.04 with Linux kernel 5.4. Each
test suite generation run was limited to 8 CPU cores, 15 GB of RAM, and 15 mins of CPU time, while each
test suite validation run was limited to 2 CPU cores, 7 GB of RAM, and 5 mins of CPU time.

FuSeBMC source code is written in C++ and Python; it is available for downloading on GitHub2, which
includes the latest release of FuSeBMC v4.1.14. FuSeBMC is publicly available under the terms of the MIT
license. Instructions for building FuSeBMC from the source code are given in the ﬁle README.md.

4.2. Objectives

This evaluation’s main goal is to check the performance of FuSeBMC and the system’s suitability for achiev-
ing high code coverage in open-source C programs. In addition, our experimental evaluation aims to answer
four questions:

EG1 (Coverage Capacity) Can FuSeBMC achieve a higher code coverage than other state-of-the-art

software testing tools?

EG2 (Seeds Generation) Can FuSeBMC create novel seeds that assist the engines in achieving

greater coverage?

EG3 (Mutation) Is the mutation strategy we employ in our fuzzer more eﬃcient than that of other

fuzzers?

EG4 (Detection Bugs) Can FuSeBMC ﬁnd more bugs than the other state-of-the-art software testing

tools?

4.3. Results

FuSeBMC participated in the Cover-Branches category at Test-Comp 2022 allowing us to comprehensively
evaluate its approach and benchmark it against other state-of-the-art test generation techniques. The Cover-
Branches category contains 3460 test tasks. Each task features a C program, and the aim of the competing
tools is to achieve maximum branch coverage for this program. Coverage is measured by the TestCov [BL19]
tool which assigns a score between 0 and 1 for each task. If a tool achieves 80% code coverage on a particular
task, it is assigned a score of 0.8 for that task and so forth. Overall scores for categories are calculated by
summing the individual scores for each task in the category and rounding the result. Table 1 shows the
experimental results from this category. FuSeBMC achieved ﬁrst place with an overall score of 2104 out of
3460. Moreover, our recent results in Cover-Branches have improved by
16% on average in comparison to

∼

1 https://busybox.net/
2 https://github.com/kaled-alshmrany/FuSeBMC

12

K. M. Alshmrany et al.

Table 1. Cover-Branches category results at Test-COMP 2022. The best score for each subcategory is high-
lighted in bold.

s
k
s
a
t

#

l
a
t
o
T

z
z
u
F

-

S
E
A
M
C

t
s
e
T
i
r
e
V
o
C

C
M
B
e
S
u
F

r
e
g
i
T
d
i
r
b
y
H

E
E
L
K

n
o
i
g
e
L

Tool
C
C
m
y
S
/
n
o
i
g
e
L

r
e
z
z
u
K
b
L

i

l

t
s
e
T
R
P

c
i
t
o
i
b
m
y
S

X
-
r
e
c
a
r
T

z
z
u
F
i
r
e
V

49
36
6

16
14
2
92
84

33
25
7
19
93

26
5
0
53
23

49
43
10
55
98

49
43
11
113 122
100 104

33
48
45
6
40
41
2
10
3
62
46
103
78 104 49

400 159 257 328 247 104 210 263 323 160 250 243 307
49
48
34
62
44
17
67
39
12
3
29
8
56
72
226
119
143
101 101
81
727 211 574 591 467 380 357 542 575 359 538 544 587
77
77
70
263
41
26
53
40
91
1
103
57
96
119
110
2
338 295 351
671
29
18
19
75
57
56
42
290
0
0
0
1
179 192 202

77
41
79
116
238 401 167 196 179 224 292
24
21
12
60
57
25
0
0
0

6
6
0
231 143 212 213 195 118 168 145 204

69
74
45
27
43
51
102 118 102 114

77
74
56
45
21
39
90
35
58
107 119 102

19
25
0
0
63
0
13
0

79
15
16
0
60

77
43
75

48
11
11

25
59
0

0
56
0

0
47
0

Subcategory

Arrays
BitVectors
ControlFlow
ECA
Floats
Heap
Loops
ProductLines
Recursive
Sequentialized
XCSP
Combinations
BusyBox
DeviceDrivers
SQLite-MemSafety
Termination

Overall score

3460 624 1860 2104 1406 1242 1033 1487 1990 896 1802 1746 2075

the previous FuSeBMC version (see Table 2). Also, ﬁgure 6 presents the performance for each tool. FuSeBMC
participated in 16 subcategories, in 9 of which (i.e. Arrays, BitVectors, Floats, Heap, Loops, ProductLines,
Recursive, Combinations and Termination) it achieved ﬁrst place and in 6 of which it reached second place.

The presented results achieve EG1. FuSeBMC demonstrated its capability in Branch Coverage cat-
egory, by achieving the highest coverage compared with state-of-the-art tools.

The improvements to the seed generation phase are some of the most crucial changes incorporated into
the latest version of FuSeBMC. The impact of these changes can be seen in the Combinations subcategory
and how it changed from a weak side of the tool to its strong side. FuSeBMC achieved eighth place in
our previous participation, while this year it reached ﬁrst place. Table 3 shows that previously FuSeBMC
6.52%), FuSeBMC v4 signiﬁcantly
demonstrated very poor coverage for these programs on average (
improved its performance in this subcategory (up to
90.14% on average), as the result of utilising the smart
∼
seeds generated during the seed generation phase. This shows how important smart seeds are for achieving
high code coverage. Furthermore, we compared the performance of FuSeBMC utilising smart seeds with the
version of FuSeBMC using standard seeds (i.e. all zeros, all ones and all randomly chosen values), and we
found that the novel smart seeds generated by FuSeBMC help detecting bugs more eﬃciently. For example,
Table 4 presents the results of comparison of the two approaches above on the veriﬁcation tasks from the
ECA subcategory featuring a bug (18 tasks overall). FuSeBMC enabled with smart seeds generation was
able to detect 5 more bugs than the version of FuSeBMC using standard seeds which is equivalent to 61%
improvement in this subcategory.

∼

This conﬁrms and answers our objective EG2, which talks about the seeds produced and their eﬀect.

Another strength of FuSeBMC is its mutation strategy. Most of the programs in the ECA category
feature input validation that may include relatively complex equations. This makes such programs more
challenging for many fuzzers to explore successfully, since the random mutation is unlikely to produce values
that can satisfy the input validation conditions. However, FuSeBMC, through its use of static analysis to
recognize input validation code, and its use of ESBMC to ﬁnd test cases that satisfy guards, avoids wasting

FuSeBMC v4 : Improving code coverage with smart seeds via fuzzing and static analysis

13

Table 2. The improvement achieved by FuSeBMC v4 in comparison to FuSeBMC v3 for the Cover-Branches
category.

Subcategory

% coverage

FuSeBMC v4

FuSeBMC v3

Improvement ∆%

Arrays
BitVectors
ControlFlow
ECA
Floats
Heap
Loops
ProductLines
Recursive
Sequentialized
XCSP
Combinations
BusyBox
DeviceDrivers
SQLite-MemSafety
Termination

Average value

82%
80%
64%
37%
54%
73%
81%
29%
85%
87%
90%
61%
34%
20%
4%
92%

61%

71%
60%
22%
17%
46%
62%
71%
0%
68%
76%
82%
7%
1%
12%
0%
87%

45%

11%
20%
42%
20%
8%
11%
10%
0%
18%
11%
8%
53%
32%
8%
4%
5%

16%

Fig. 6. Quantile functions for category Cover-Branches. [Bey22]

time mutating seeds that do not explore new areas of the program. The impact of this can be seen in the
FuSeBMC ’s results in the ECA category, where FuSeBMC signiﬁcantly improved (by
60%) in comparison
to the previous version.

∼

Our management of the mutation process is remarkably eﬀective, which is why we are advancing in
the coverage process with such types of programs, which achieves our third objective EG3.

We evaluated FuSeBMC on the Error Coverage category at Test-Comp 2022 [Bey22]. Each veriﬁcation
task in this category contains a C program with a bug explicitly marked as an error function. A tool earns

05001000150020002500300035000500100015002000Min. number of test tasksCumulative scoreCMA-ES-FuzzCoVeriTestFuSeBMCHybridTigerKLEELegionLegion-SymCCLibKluzzerPRTestSymbioticTracerXVeriFuzz14

K. M. Alshmrany et al.

Table 3. Comparison of code coverage achieved by FuSeBMC v4 and FuSeBMC v3 in a subset of tasks from
the Combinations subcategory.

Task name

% coverage

FuSeBMC v4 FuSeBMC v3 Improvement ∆%

pals lcr.3.1.ufo.BOUNDED-6.pals+Problem12 label01.yml
pals lcr.3.1.ufo.UNBOUNDED.pals+Problem12 label02.yml
pals lcr.4.1.ufo.BOUNDED-8.pals+Problem12 label04.yml
pals lcr.4 overflow.ufo.UNBOUNDED.pals+Problem12 label05.yml
pals lcr.5.1.ufo.UNBOUNDED.pals+Problem12 label05.yml
pals lcr.5 overflow.ufo.UNBOUNDED.pals+Problem12 label09.yml
pals lcr.6.1.ufo.BOUNDED-12.pals+Problem12 label09.yml
pals lcr.7 overflow.ufo.UNBOUNDED.pals+Problem12 label09.yml
pals lcr.8.ufo.UNBOUNDED.pals+Problem12 label08.yml

Average value

94.90%
84.40%
94.10%
94.00%
86.20%
94.00%
92.90%
92.60%
78.20%

90.14%

13.30%
5.19%
4.44%
11.50%
0.78%
4.82%
5.18%
5.31%
8.17%

6.52%

81.60%
79.21%
89.66%
82.50%
85.42%
89.18%
87.72%
87.29%
70.03%

83.62%

Table 4. Comparison of FuSeBMC v4 performance with smart seeds and with standard seeds, where TRUE
shows that the bug has been detected successfully, UNKNOWN means otherwise.

Task name

FuSeBMC v4
Smart Seeds Standard Seeds

eca-rers2012/Problem05 label00.yml
TRUE
eca-rers2012/Problem06 label00.yml
TRUE
eca-rers2012/Problem11 label00.yml
TRUE
eca-rers2012/Problem12 label00.yml
TRUE
eca-rers2012/Problem15 label00.yml
TRUE
eca-rers2012/Problem16 label00.yml
UNKNOWN
eca-rers2012/Problem18 label00.ymll
TRUE
eca-rers2018/Problem10.yml
TRUE
eca-rers2018/Problem11.yml
TRUE
eca-rers2018/Problem12.yml
UNKNOWN
eca-rers2018/Problem13.yml
UNKNOWN
eca-rers2018/Problem14.yml
UNKNOWN
eca-rers2018/Problem15.yml
UNKNOWN
eca-rers2018/Problem16.yml
UNKNOWN UNKNOWN
eca-rers2018/Problem17.yml
UNKNOWN UNKNOWN
eca-rers2018/Problem18.yml
UNKNOWN UNKNOWN
eca-programs/Problem101 label00.yml UNKNOWN UNKNOWN
eca-programs/Problem103 label32.yml UNKNOWN UNKNOWN

TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE

a score of 1 if it can provide a test case that reaches the error function and gets a 0 score otherwise. Table 5
shows the scores obtained by the competing tools in Error Coverage at Test-Comp 2022 while ﬁgure 7
presents the performance of each tool. It can be seen that FuSeBMC achieved ﬁrst place in 9 subcategories
(i.e. Arrays, BitVectors, ControlFlow, Floats, Heap, Loops, ProductLines, Recursive and BusyBox ) reaching
94% success rate). Finally,
the ﬁrst overall place in this category with the result of 730 out of 776 (
FuSeBMC v4 improved its performance in the Cover-Error category by
14% on average in comparison to
the previous FuSeBMC version (see Table 2).

∼

∼

Overall, the results show that FuSeBMC produces test cases that detect more security vulnerabilities
in C programs than state-of-the-art tools, which successfully answer EG4.

FuSeBMC achieved ﬁrst place at Test-Comp 2022 obtaining a score of 3003 out of 4236 with the closest
competitor, VeriFuzz [CMV19], scoring 2971 and signiﬁcantly outperforming several state-of-the-art tools
such as LibKluzzer [Le20a], KLEE [CCDE08], CPAchecker [BK11] and Symbiotic [CMN+21]. Fig. 8 shows

2 https://test-comp.sosy-lab.org/2022/results/results-verified/

FuSeBMC v4 : Improving code coverage with smart seeds via fuzzing and static analysis

15

Table 5. Cover-Error category results at Test-Comp 2022. The best score for each subcategory is highlighted
in bold.

Subcategory

s
k
s
a
t

#

l
a
t
o
T

z
z
u
F

-

S
E
A
M
C

t
s
e
T
i
r
e
V
o
C

C
M
B
e
S
u
F

r
e
g
i
T
d
i
r
b
y
H

E
E
L
K

n
o
i
g
e
L

Tool
C
C
m
y
S
/
n
o
i
g
e
L

r
e
z
z
u
K
b
L

i

l

t
s
e
T
R
P

c
i
t
o
i
b
m
y
S

X
-
r
e
c
a
r
T

z
z
u
F
i
r
e
V

100
Arrays
10
BitVectors
32
ControlFlow
18
ECA
33
Floats
56
Heap
157
Loops
169
ProductLines
Recursive
20
Sequentialized 107
59
XCSP
13
BusyBox
2
DeviceDrivers

Overall score

776

0
0
0
0
0
0
0
0
0
0
0
0
0

0

89
9
27
13
6
52
95

99
67
69
73
10
0
6
8
32
0
16
18
0
1
13
3
33
0
23
25
53
3
49
42
75 146 53
4
160 169 53 169 34
0
5
0
92
0
52
0
0
0
0

19
102
52
2
0

7
61
50
0
0

16
86
37
1
0

0

628 355 500

57

-
-
-
-
-
-
-
-
-
-
-
-
-

-

37
97
10
5
0
27
0
11
3
30
53
13
136 102
169 92
1
17
0
81
0
5
0
0
0
0

74
8
24
14
0
53
81
159
17
79
41
0
0

528 145 463

0
0
0
0
0
0
0
0
0
0
0
0
0

0

99
10
30
15
32
53
142
169
16
104
55
2
0

623

Table 6. The improvement achieved by FuSeBMC v4 in comparison to FuSeBMC v3 for the Cover-Error
category.

Subcategory

% success

FuSeBMC v4

FuSeBMC v3

Improvement ∆%

Arrays
BitVectors
ControlFlow
ECA
Floats
Heap
Loops
ProductLines
Recursive
Sequentialized
XCSP
BusyBox
DeviceDrivers

Average value

99%
100%
100%
72%
100%
95%
93%
100%
95%
95%
88%
15%
0%

81%

93%
100%
25%
44%
97%
80%
83%
0%
95%
94%
90%
0%
0%

67%

6%
0%
75%
28%
3%
14%
10%
100%
0%
1%
-2%
15%
0%

14%

the overall results for the competing tools at Test-Comp 2022 while Table 7 presents the scores obtained by
each tool.

5. Related Work

In this section, we overview related work. Most related techniques fall into one of the following categories:
fuzzing, symbolic execution or the combination of both.

4 https://test-comp.sosy-lab.org/2022/results/results-verified/

16

K. M. Alshmrany et al.

Fig. 7. Quantile functions for category Cover-Error. [Bey22]

Fig. 8. Quantile functions for category Overall. [Bey22].

5.1. Fuzzing

Barton Miller [BHC+90] proposed fuzzing at the University of Wisconsin in the 1990s, and it became
the popular software vulnerabilities detection technique [SGA07]. One of the most common fuzzing tools
is American fuzzy lop (AFL) [BPNR17, ame21]. AFL is a coverage-based fuzzer that was built to ﬁnd
software vulnerabilities. AFL relies on an evolutionary approach to learn mutations based on measuring
code coverage. By employing genetic algorithms with guided fuzzing, AFL yields high code coverage. Another
tool is LibFuzzer [Ser15] which uses code coverage information generated by LLVM’s (SanitizerCoverage)
instrumentation to produce test cases. LibFuzzer is best suited for testing libraries that have small input

01002003004005006007008000100200300400500600700Min. number of test tasksCumulative scoreCMA-ES-FuzzCoVeriTestFuSeBMCHybridTigerKLEELegionLibKluzzerPRTestSymbioticTracerXVeriFuzzAdvancesinAutomaticSoftwareTesting:Test-Comp202211 0 500 1000 1500 2000 2500 3000 3500 4000 0 500 1000 1500 2000 2500 3000Min. number of test tasksCumulative scoreCMA-ES-FuzzCoVeriTestFuSeBMCHybridTigerKLEELegionLibKluzzerPRTestSymbioticTracerXVeriFuzzFig.4:QuantilefunctionsforcategoryOverall.Eachquantilefunctionillustratesthequantile(x-coordinate)ofthescoresobtainedbytest-generationrunsbelowacertainnumberoftest-generationtasks(y-coordinate).Moredetailsweregivenpreviously[7].Thegraphsaredecoratedwithsymbolstomakethembetterdistinguishablewithoutcolor.Table8:Alternativerankings;qualityisgiveninscorepoints(sp),CPUtimeinhours(h),energyinkilo-watt-hours(kWh),theﬁrstrankmeasureinkilo-jouleperscorepoint(kJ/sp),andthesecondrankmeasureinscorepoints(sp);measurementvaluesareroundedto2signiﬁcantdigitsRankTestGeneratorQualityCPUCPURankTimeEnergyMeasure(sp)(h)(kWh)GreenTesting(kJ/sp)1TracerX10691201.44.82Klee?21253103.56.03Symbiotic23675405.99.0worst41‘CPUEnergy’theCPUusageinkilo-watt-hours(kWh),andcolumn‘RankMeasure’reportsthevaluesfortherankmeasure.GreenTesting—LowEnergyConsumption.Sincealargepartofthecostoftestgenerationiscausedbytheenergyconsumption,itmightbeimportanttoalsoconsidertheenergyeﬃciencyinrankings,ascomplementtotheoﬃcialTest-Compranking.Thisalternativerankingcategoryusestheenergyconsump-tionperscorepointasrankmeasure:CPUEnergyQuality,withtheunitkilo-jouleperscorepoint(kJ/sp).TheenergyismeasuredusingCPUEnergyMeter[21],whichweuseaspartofBenchExec[20].NewTestGenerators.ToacknowledgethetestgeneratorsthatparticipatedfortheﬁrsttimeinTest-Comp,welistthetestgeneratorsthatparticipatedfortheﬁrsttime.CMA-ESFuzz?andFuSeBMCparticipatedfortheﬁrsttimeinFuSeBMC v4 : Improving code coverage with smart seeds via fuzzing and static analysis

17

Table 7. Test-Comp 2022 Overall Results.4 The table illustrates the scores obtained by all state-of-art tools
overall, where we identify the best tool in bold.

Cover-Error and Branches

m
u
N
-
k
s
a
T

z
z
u
F

S
E
-
A
M
C

t
s
e
T
i
r
e
V
o
C

C
M
B
e
S
u
F

r
e
g
i
T
d
i
r
b
y
H

E
E
L
K

n
o
i
g
e
L

C
C
m
y
S
/
n
o
i
g
e
L

r
e
z
z
u
l
K
b
i
L

t
s
e
T
R
P

c
i
t
o
i
b
m
y
S

X
-
r
e
c
a
r
T

z
z
u
F
i
r
e
V

OVERALL

4236

382

2293 3003 1830

2125

787

-

2658

945

2367

1069

2971

with a run-time of milliseconds for each input to guarantee not crashing on invalid input in library code5.
Vuzzer [RJK+17] is a fuzzer with an application-aware strategy. The main advantage of this strategy is that it
does not need any knowledge of the application or input format in advance. In order to maximize coverage and
explore deeper paths, the tool leverages control- and data-ﬂow features based on static and dynamic analysis
to infer fundamental properties of the application. This enables a much faster generation of interesting inputs
compared to an application-agnostic approach. Wang et al. [WCWL17] proposed an approach that utilizes
data-driven seed generation. It relies on extracting the knowledge of grammar to process and generate well-
distributed seed inputs for fuzzing programs. However, Skyﬁre is designed for probabilistic context-sensitive
grammar (PCSG) to identify syntax features and semantic rules. AFLFast [BPR17] is an enhanced version
of AFL applying various strategies to exercise a low-frequency path. The tool was shown to achieve a 7x
speedup over AFL [BPR17].

GTFuzz [LLL+20] is a tool that works to prioritize inputs based on extracting syntax tokens that guard
the target place. The backward static analysis technique extracts these tokens. Also, GTFuzz beneﬁts from
this extraction to improve the mutation algorithm. Smart grey-box fuzzing (SGF) [PBS+19] is a fuzzer
that employs high-level structural representation on the original seeds to generate seeds with high impact.
Similarly, AFLSmart [PBS+19] is a structure-aware fuzzing that combines the PEACH fuzzer engine with
the AFL fuzzer. Instrim [HWHH18] is a control ﬂow graph CFG-aware fuzzer. It analyses software to keep
the fuzzing speed in speciﬁc blocks chosen based on the Control Flow Graph CFG. As a result, Instrim
increased speed by a maximum of 1.75x. For network protocols, there is AutoFuzz [GR10], which is a tool
that utilizes fuzzing to verify network protocols. It begins with ﬁnding the protocol speciﬁcations then ﬁnding
vulnerabilities by using fuzzing. Also there is Peach [Edd11]. One of the main features of this tool is being
an advanced and robust fuzzing framework. This framework can produce an XML ﬁle to create a data
model and state model deﬁnition. Furthermore, several fuzzers have been developed, and each fuzzer has its
improvements. For example, DGF [BPNR17] works to search for directed paths, and SYMFUZZ [CWB15]
controls the selection of paths, while Alexandre Rebert’s approach [RCA+14] uses guided seed selection.

One of the weaknesses of pure fuzzing approaches is their inability to ﬁnd test cases that explore program
code occurring beyond complex guards. In addition, because fuzzers essentially work by randomly mutating
seeds, they struggle to ﬁnd inputs required to satisfy complicated guards.

5.2. Symbolic execution

Symbolic execution has shown competence in producing high-coverage test cases and detecting errors in
complex software. One of the popular symbolic execution engines is KLEE [CCDE08]. KLEE is a tool that
explores the search space path-by-path by utilizing LLVM compiler infrastructure and dynamic symbolic ex-
ecution. KLEE has been utilized in many specialized tools for its reliability as a symbolic execution engine.
Furthermore, Tracer [JMNS12] is a veriﬁcation tool that uses solver constraint logic programming (CLP) and
interpolation methods. Another tool based on symbolic execution is DART [GKS05]. It works to conduct
software analysis and applies automatic random testing to ﬁnd software bugs. BAP [BJAS11] is devel-
oped on top of Vine [SBY+08], which relies on symbolic execution. BAP has useful analysis and veriﬁcation
techniques. BAP relies on an intermediate language (IL) in its analysis. Also, SymbexNet [SJCP14] and Sym-
Net [SRK+12] for veriﬁcation of network protocols implementation. Avgerinos, Thanassis, et al. [ARCB14]

5 https://llvm.org/docs/LibFuzzer.html#q-so-what-exactly-this-fuzzer-is-good-for

18

K. M. Alshmrany et al.

presented an approach that enhances symbolic execution with veriﬁcation-based algorithms. It works to in-
crease the eﬀect of dynamic symbolic execution. The approach showed its ability to detect bugs and achieve
higher code coverage than other dynamic symbolic execution approaches. CoVeriTest [BJ19] is a Cooperative
Veriﬁer Test generation that utilizes a hybrid approach for test generation. It applies diﬀerent conditional
model checkers in iterations with many conﬁgurations for value analysis. CoVeriTest also changes the level
of cooperation and assigns the time budget of each veriﬁer. However, symbolic execution suﬀers from the
path explosion problem related to loops and arrays, impacting its practicality.

5.3. Combination

The combination of symbolic execution and BMC with fuzzing has been used recently to combine the
strengths of both techniques. For example, VeriFuzz [CMV19] is a state-of-the-art tool that we have previously
compared to FuSeBMC. The authors describe it as a program-aware fuzz tester that utilizes the combination
of feedback-driven evolutionary fuzz testing with static analysis. It also employs grey-box fuzzing to exploit
lightweight instrumentation for observing the behaviors that occur during test runs. VeriFuzz earned ﬁrst
place in Test-Comp 2020 [Bey20]. Driller [SGS+16] is a hybrid vulnerability excavation tool developed by
Stephens et al. It ﬁnds deeply embedded bugs by leveraging guided fuzzing and concolic execution in a
complementary manner. It employs the concolic execution to analyze the program and trace the inputs.
Also, the concolic execution guides fuzzing on diﬀerent paths by using its constraint-solving engine. Stephens
et al. combine the two techniques’ strengths and mitigate the weaknesses in avoiding the path explosion in
concolic analysis and the incompleteness of fuzzing. First, Driller divides the application, based on checks of
particular values of a speciﬁc input, into compartments. Then by utilizing the proﬁciency of fuzzing, Driller
explores possible values of general input in a compartment. Although Driller showed its eﬃciency in detecting
more vulnerabilities, it may drive to path explosion problem because it requires a lot of computing power.
MaxAFL [KY20] is best described as a gradient-based fuzzer that is built on top of AFL. First, the developer
ﬁnds the Maximum Expectation of Instruction Count (MEIC) by employing lightweight static analysis. Then,
using MEIC, they generate an objective function; then, they apply a gradient-based optimization algorithm
to generate eﬃcient inputs by minimizing the objective function. Hybrid Fuzz Testing [Pak12] is a tool
that generates provably random test cases eﬃciently such that it guarantees the execution of unique paths.
Moreover, It ﬁnds unique execution paths by using symbolic execution to ﬁnd frontier nodes that lead such
paths. The tool also collects all possible frontier nodes depending on resource constraints to employ fuzzing
with provably random input, preconditioned to lead to each frontier node.

He et al. [HBA+19] proposed an approach to learning a fuzzer from symbolic execution. First, it phrases
the learning task in the framework of imitation learning. Then, it employs symbolic execution to generate
quality inputs with high coverage while a fuzzer learns using neural networks to be used to fuzz new pro-
grams. Badger[NKP18] provides a hybrid testing approach for complexity analysis. It generates new input
by using Symbolic PathFinder [PR10] and provides the Kelinci fuzzer with worst-case analysis. Badger aims
to increase coverage and resource-related cost for each path by using fuzz testing. LibKluzzer [Le20a] is
a novel implementation of the combination of Symbolic execution and fuzzing. Its strength resides in the
fusion of coverage-guided fuzzing and white-box fuzzing strengths. LibKluzzer is constructed of LibFuzzer,
and an extension of KLEE called KLUZZER [Le20b]. Munch[OHPP18]is an open-source framework hybrid
tool. It employs fuzzing with seed inputs generated by symbolic execution and targets symbolic execution
when fuzzing saturates. It aims to reduce the number of queries to the SMT solver to focus on the paths
that may reach uncovered functions. The developers created Munch to improve function coverage. SAGE
(Scalable Automated Guided Execution) proposed by Godefroid et al. [GLM12] is a hybrid fuzzer devel-
oped at Microsoft Research. SAGE is used extensively at Microsoft, where it has successfully found many
security-related bugs. It employs generational search to extend dynamic symbolic execution and increase
code coverage by negating and solving the path predicates. Also, SAGE relies on the random test style
used by DART to mutate good inputs using grammar. FairFuzz [LS18] is a grey-box fuzzer that utilizes
guided-mutation. It uses coverage to achieve the guidance by employing a mutation mask for every pair of
seeds and the rare branches to direct the fuzzing to reach each rare branch. SAFL [WLC+18] is an eﬃcient
fuzzer for C/C++ programs. It utilizes symbolic execution (in a lightweight approach) to generate initial
seeds that can get an appropriate fuzzing direction.

Overall, the combination of Fuzzers and Symbolic execution to verify software has been the most success-
ful. Our approach expands on this combination by utilizing our main novelties. Smart seed generation and
the Tracer subsystem diﬀerentiate FuSeBMC from the other tools. Moreover, with the utilization of shared
memory and the analysis of the graph goal to choose a suitable strategy, FuSeBMC was able to rank ﬁrst
at Test-Comp 2022.

FuSeBMC v4 : Improving code coverage with smart seeds via fuzzing and static analysis

19

6. Conclusion

In this paper, we presented FuSeBMC v4, a test generator that relies on smart seed generation to improve
the state-of-the-art in hybrid fuzzing and achieve high coverage for C programs. First, FuSeBMC analyses
and injects goal labels into the given C program. Then, It ranks these goal labels according to the given
strategy. After that, the engines are employed to produce smart seeds for a short time to use them later. Then,
FuSeBMC coordinates between the engines and seed distribution by the Tracer. This Tracer will generally
manage the tool to record the goals covered and deal with the transfer of information between the engines
by providing a shared memory to harness the power and take advantage of the power of each engine. So that
the BMC engine helps give the seed that makes the fuzzing engine not struggle with complex mathematical
guards. Furthermore, Tracer evaluates test cases dynamically to convert high-impact cases into seeds for
subsequent test fuzzing. This approach was evaluated by participating in the fourth international competition
on software testing Test-Comp 2022. Our approach FuSeBMC showed its eﬀectiveness by achieving ﬁrst place
in the Cover-branches category, ﬁrst place in the Cover-Error category and ﬁrst place in the Overall category.
This performance is due to various features of our tool, the most important of which are the following. First
the generation of smart seeds, which help harness the power of the fuzzers and allow them to fuzz deeper.
Second, simplifying the target program by limiting the bounds of potentially inﬁnite loops to avoid the path
explosion problem and produce seeds faster. Third, utilizing static analysis to manage the mutation process
by limiting the range of values input variables can take, speeding up the fuzzing process. In the future, we are
planning on developing the tool to deal with diﬀerent types of programs, such as multi-threaded programs.
Furthermore, we work with the SCorCH project6 to improve our performance in detecting memory safety
bugs by incorporating SoftBoundCETS [NZMZ09] into FuSeBMC.

References

[AABC21] Kaled M Alshmrany, Mohannad Aldughaim, Ahmed Bhayat, and Lucas C Cordeiro. Fusebmc: An energy-eﬃcient
test generator for ﬁnding security vulnerabilities in c programs. In International Conference on Tests and Proofs,
pages 85–105. Springer, 2021.

[AABC22] Kaled M Alshmrany, Mohannad Aldughaim, Ahmed Bhayat, and Lucas C Cordeiro. FuSeBMC v4: Smart seed gen-
eration for hybrid fuzzing. In 25th International Conference on Fundamental Approaches to Software Engineering
(FASE), 13241:336–340, 2022.
American fuzzy lop, https://lcamtuf.coredump.cx/afl/, 2021.

[ame21]
[AMGC20] Kaled M Alshmrany, Rafael S Menezes, Mikhail R Gadelha, and Lucas C Cordeiro. FuSeBMC: A white-box fuzzer
for ﬁnding security vulnerabilities in c programs. In 24th International Conference on Fundamental Approaches
to Software Engineering (FASE), 12649:363–367, 2020.

[ARCB14] Thanassis Avgerinos, Alexandre Rebert, Sang Kil Cha, and David Brumley. Enhancing symbolic execution with
veritesting. In Proceedings of the 36th International Conference on Software Engineering, pages 1083–1094, 2014.
[BCD+18] Roberto Baldoni, Emilio Coppa, Daniele Cono D’elia, Camil Demetrescu, and Irene Finocchi. A survey of symbolic

[Bey20]
[Bey21]

execution techniques. ACM Comput. Surv., 51(3), May 2018.
Dirk Beyer. Second competition on software testing: Test-comp 2020. In FASE, pages 505–519, 2020.
Dirk Beyer. Software veriﬁcation: 10th comparative evaluation (sv-comp 2021). Proc. TACAS (2). LNCS, 12652,
2021.
[Bey22]
D. Beyer. Advances in automatic software testing: Test-Comp 2022. In Proc. FASE, LNCS 13241. Springer, 2022.
[BHC+90] Barton, James H., Edward W. Czeck, Zary Z. Segall, and Daniel P. Siewiorek. Fault injection experiments using

[Bie09]

[BJ19]

ﬁat. IEEE Trans. Comput., 39(4):575–582, 1990.
Armin Biere. Bounded model checking. In Armin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh, editors,
Handbook of Satisﬁability, volume 185 of Frontiers in Artiﬁcial Intelligence and Applications, pages 457–481. IOS
Press, 2009.
Dirk Beyer and Marie-Christine Jakobs. Coveritest: Cooperative veriﬁer-based testing. In FASE, pages 389–408,
2019.

[BJAS11] David Brumley, Ivan Jager, Thanassis Avgerinos, and Edward J Schwartz. Bap: A binary analysis platform. In

[BL19]

[BK11]

International Conference on Computer Aided Veriﬁcation, pages 463–469. Springer, 2011.
Dirk Beyer and M Erkan Keremoglu. Cpachecker: A tool for conﬁgurable software veriﬁcation. In International
Conference on Computer Aided Veriﬁcation, pages 184–190. Springer, 2011.
Dirk Beyer and Thomas Lemberger. Testcov: Robust test-suite execution and coverage measurement. In 2019 34th
IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 1074–1077. IEEE, 2019.
[BPNR17] Marcel B¨ohme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoudhury. Directed greybox fuzzing. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pages 2329–2344,
2017.

6 https://scorch-project.github.io/about/

20

K. M. Alshmrany et al.

[BPR17]

Marcel B¨ohme, Van-Thuan Pham, and Abhik Roychoudhury. Coverage-based greybox fuzzing as markov chain.
IEEE Transactions on Software Engineering, 45(5):489–506, 2017.

[CCDE08] Cadar, Cristian, Daniel Dunbar, and Dawson R. Engler. KLEE: Unassisted and automatic generation of high-

[CE]
[CFM12]

coverage tests for complex systems programs. In OSDI, volume 8, pages 209–224, 2008.
Tavis Ormandy Chris Evans, Matt Moore.
Lucas C. Cordeiro, Bernd Fischer, and Jo˜ao Marques-Silva. Smt-based bounded model checking for embedded
ANSI-C software. IEEE Trans. Software Eng., 38(4):957–974, 2012.
Clang documentation. http://clang.llvm.org/docs/index.html, 2015. [Online; accessed August-2019].

[CLA15]
[CMN+21] Chalupa, Marek, Nov´ak, J, Strejˇcek, and Jan. Symbiotic 8: Parallel and targeted test generation (competition

[dMB08]

[CMV19]

[CWB15]

contribution). In FASE, volume 12649 of LNCS, 2021.
Animesh Basak Chowdhury, Raveendra Kumar Medicherla, and R Venkatesh. VeriFuzz: Program aware fuzzing. In
International Conference on Tools and Algorithms for the Construction and Analysis of Systems, pages 244–249.
Springer, 2019.
Sang Kil Cha, Maverick Woo, and David Brumley. Program-adaptive mutational fuzzing. In 2015 IEEE Symposium
on Security and Privacy, pages 725–741. IEEE, 2015.
Leonardo de Moura and Nikolaj Bjørner. Z3: An eﬃcient smt solver. In C. R. Ramakrishnan and Jakob Rehof,
editors, Tools and Algorithms for the Construction and Analysis of Systems, pages 337–340, Berlin, Heidelberg,
2008. Springer Berlin Heidelberg.
Bruno Dutertre. Yices 2.2.
737–744, Cham, 2014. Springer International Publishing.
Michael Eddington. Peach fuzzing platform. Peach Fuzzer, 34, 2011.
Patrice Godefroid, Adam Kiezun, and Michael Y Levin. Grammar-based whitebox fuzzing. In Proceedings of the
29th ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 206–215, 2008.
Patrice Godefroid, Nils Klarlund, and Koushik Sen. Dart: Directed automated random testing. In Proceedings of
the 2005 ACM SIGPLAN conference on Programming language design and implementation, pages 213–223, 2005.
[GLM+08] Patrice Godefroid, Michael Y Levin, David A Molnar, et al. Automated whitebox fuzz testing. In NDSS, volume 8,

In Armin Biere and Roderick Bloem, editors, Computer Aided Veriﬁcation, pages

[Edd11]
[GKL08]

[GKS05]

[Dut14]

[GLM12]

pages 151–166, 2008.
Patrice Godefroid, Michael Y Levin, and David Molnar. Sage: Whitebox fuzzing for security testing: Sage has had
a remarkable impact at microsoft. Queue, 10(1):20–27, 2012.

[GMCN19] Mikhail R Gadelha, Felipe Monteiro, Lucas Cordeiro, and Denis Nicole. ESBMC v6. 0: Verifying c programs using

k-induction and invariant inference. In International Conference on TACAS, pages 209–213. Springer, 2019.

[GMM+18] Mikhail R. Gadelha, Felipe R. Monteiro, Jeremy Morse, Lucas C. Cordeiro, Bernd Fischer, and Denis A. Nicole.
In 33rd ACM/IEEE Int. Conf. on Automated Software

ESBMC 5.0: An industrial-strength C model checker.
Engineering (ASE’18), pages 888–891, New York, NY, USA, 2018. ACM.

[GMM+20] Mikhail R Gadelha, Rafael Menezes, Felipe R Monteiro, Lucas C Cordeiro, and Denis Nicole. Esbmc: Scalable and
precise test generation based on the ﬂoating-point theory:(competition contribution). Fundamental Approaches to
Software Engineering, 12076:525, 2020.
Serge Gorbunov and Arnold Rosenbloom. Autofuzz: Automated network protocol fuzzing framework. IJCSNS,
10(8):239, 2010.
Jingxuan He, Mislav Balunovi´c, Nodar Ambroladze, Petar Tsankov, and Martin Vechev. Learning to fuzz from
symbolic execution with application to smart contracts. In Proceedings of the 2019 ACM SIGSAC Conference on
Computer and Communications Security, pages 531–548, 2019.
Hadi Hemmati. How eﬀective are code coverage criteria? In 2015 IEEE International Conference on Software
Quality, Reliability and Security, pages 151–156, 2015.

[HBA+19]

[Hem15]

[GR10]

[HWHH18] Chin-Chia Hsu, Che-Yu Wu, Hsu-Chun Hsiao, and Shih-Kun Huang. Instrim: Lightweight instrumentation for
In Symposium on Network and Distributed System Security (NDSS), Workshop on

coverage-guided fuzzing.
Binary Analysis Research, 2018.

[LA04]

[KY21]

[KY20]

[JMNS12]

[IPJF19] Marko Ivankovi´c, Goran Petrovi´c, Ren´e Just, and Gordon Fraser. Code coverage at google. In Proceedings of the
2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering, pages 955–963, 2019.
Joxan Jaﬀar, Vijayaraghavan Murali, Jorge A Navas, and Andrew E Santosa. Tracer: A symbolic execution tool
for veriﬁcation. In International Conference on Computer Aided Veriﬁcation, pages 758–766. Springer, 2012.
Youngjoon Kim and Jiwon Yoon. Maxaﬂ: Maximizing code coverage with a gradient-based optimization technique.
Electronics, 10(1):11, 2020.
Youngjoon Kim and Jiwon Yoon. Maxaﬂ: Maximizing code coverage with a gradient-based optimization technique.
Electronics, 10(1):11, 2021.
Chris Lattner and Vikram Adve. LLVM: A compilation framework for lifelong program analysis and transformation.
pages 75–88, San Jose, CA, USA, Mar 2004.
Hoang M Le. LLVM-based hybrid fuzzing with LibKluzzer (competition contribution). In FASE, pages 535–539,
2020.
Hoang M. Le. LLVM-based hybrid fuzzing with LibKluzzer (competition contribution). In Heike Wehrheim and
Jordi Cabot, editors, Fundamental Approaches to Software Engineering, pages 535–539, Cham, 2020. Springer
International Publishing.

[LLL+20] Rundong Li, HongLiang Liang, Liming Liu, Xutong Ma, Rong Qu, Jun Yan, and Jian Zhang. Gtfuzz: Guard token
directed grey-box fuzzing. In 2020 IEEE 25th Paciﬁc Rim International Symposium on Dependable Computing
(PRDC), pages 160–170. IEEE, 2020.
Caroline Lemieux and Koushik Sen. Fairfuzz: A targeted mutation strategy for increasing greybox fuzz testing
coverage. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,
pages 475–485, 2018.

[Le20b]

[Le20a]

[LS18]

FuSeBMC v4 : Improving code coverage with smart seeds via fuzzing and static analysis

21

[LZZ18]
[MWT+20] Xianya Mi, Baosheng Wang, Yong Tang, Pengfei Wang, and Bo Yu. Shfuzz: Selective hybrid fuzzing with branch

Jun Li, Bodong Zhao, and Chao Zhang. Fuzzing: a survey. Cybersecurity, 1(1):1–13, 2018.

[NKP18]

[NPB14]

scheduling based on binary instrumentation. Applied Sciences, 10(16):5449, 2020.
Yannic Noller, Rody Kersten, and Corina S P˘as˘areanu. Badger: complexity analysis with fuzzing and symbolic
execution. In Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis,
pages 322–332, 2018.
Aina Niemetz, Mathias Preiner, and Armin Biere. Boolector 2.0. J. Satisf. Boolean Model. Comput., 9(1):53–58,
2014.

[NZMZ09] Santosh Nagarakatte, Jianzhou Zhao, Milo MK Martin, and Steve Zdancewic. Softbound: Highly compatible and
complete spatial memory safety for c. In Proceedings of the 30th ACM SIGPLAN Conference on Programming
Language Design and Implementation, pages 245–258, 2009.

[OHPP18] Saahil Ognawala, Thomas Hutzelmann, Eirini Psallida, and Alexander Pretschner. Improving function coverage
with munch: a hybrid fuzzing and directed symbolic execution approach. In Proceedings of the 33rd Annual ACM
Symposium on Applied Computing, pages 1475–1482, 2018.
Brian S Pak. Hybrid fuzz testing: Discovering software bugs via fuzzing and symbolic execution. School of Computer
Science Carnegie Mellon University, 2012.

[PBS+19] Van-Thuan Pham, Marcel B¨ohme, Andrew E Santosa, Alexandru R˘azvan C˘aciulescu, and Abhik Roychoudhury.

[Pak12]

[PR10]

[RJK+17]

Smart greybox fuzzing. IEEE Transactions on Software Engineering, 47(9):1980–1997, 2019.
Corina S P˘as˘areanu and Neha Rungta. Symbolic pathﬁnder: symbolic execution of java bytecode. In Proceedings
of the IEEE/ACM international conference on Automated software engineering, pages 179–180, 2010.
[RCA+14] Alexandre Rebert, Sang Kil Cha, Thanassis Avgerinos, Jonathan Foote, David Warren, Gustavo Grieco, and David
Brumley. Optimizing seed selection for fuzzing. In 23rd USENIX Security Symposium (USENIX Security 14),
pages 861–875, 2014.
Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Cojocar, Cristiano Giuﬀrida, and Herbert Bos. Vuzzer:
Application-aware evolutionary fuzzing. In NDSS, volume 17, pages 1–14, 2017.

[SBY+08] Dawn Song, David Brumley, Heng Yin, Juan Caballero, Ivan Jager, Min Gyung Kang, Zhenkai Liang, James
Newsome, Pongsin Poosankam, and Prateek Saxena. Bitblaze: A new approach to computer security via binary
analysis. In International conference on information systems security, pages 1–25. Springer, 2008.
Kostya Serebryany. libfuzzer–a library for coverage-guided fuzz testing. LLVM project, 2015.
Michael Sutton, Adam Greene, and Pedram Amini. Fuzzing: brute force vulnerability discovery. Pearson Education,
2007.

[Ser15]
[SGA07]

[SGS+16] Nick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang, Jacopo Corbetta, Yan Shoshi-
taishvili, Christopher Kruegel, and Giovanni Vigna. Driller: Augmenting fuzzing through selective symbolic exe-
cution. In NDSS, pages 1–16, 2016.
Song, JaeSeung, Cristian Cadar, and Peter Pietzuch. Symbexnet: testing network protocol implementations with
symbolic execution and rule-based speciﬁcations. IEEE TSE, 40(7):695–709, 2014.
Sasnauskas, Raimondas, Philipp Kaiser, Russ Lucas Juki´c, and Klaus Wehrle.
implementations using symbolic distributed execution. In ICNP, pages 1–6. IEEE, 2012.

Integration testing of protocol

[SRK+12]

[SJCP14]

[WCWL17] Junjie Wang, Bihuan Chen, Lei Wei, and Yang Liu. Skyﬁre: Data-driven seed generation for fuzzing. In 2017 IEEE

Symposium on Security and Privacy (SP), pages 579–594. IEEE, 2017.

[YLX+18]

[WLC+18] Mingzhe Wang, Jie Liang, Yuanliang Chen, Yu Jiang, Xun Jiao, Han Liu, Xibin Zhao, and Jiaguang Sun. Saﬂ:
increasing and accelerating testing coverage with symbolic execution and guided fuzzing. In Proceedings of the
40th International Conference on Software Engineering: Companion Proceeedings, pages 61–64, 2018.
Insu Yun, Sangho Lee, Meng Xu, Yeongjin Jang, and Taesoo Kim. Qsym: A practical concolic execution engine
tailored for hybrid fuzzing. In Proceedings of the 27th USENIX Conference on Security Symposium, SEC’18, page
745–761, USA, 2018. USENIX Association.
Micha(cid:32)l Zalewski. American Fuzzy Lop (AFL) fuzzer, 2015.

[Zal15]

