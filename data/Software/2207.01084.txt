2
2
0
2

l
u
J

3

]
E
S
.
s
c
[

1
v
4
8
0
1
0
.
7
0
2
2
:
v
i
X
r
a

1

Enhancing Automated Software Traceability by
Transfer Learning from Open-World Data

Jinfeng Lin, Amrit Poudel, Wenhao Yu, Qingkai Zeng, Meng Jiang, Jane Cleland-Huang

Abstract—Software requirements traceability is a critical component of the software engineering process, enabling activities such as
requirements validation, compliance veriﬁcation, and safety assurance. However, the cost and effort of manually creating a complete
set of trace links across natural language artifacts such as requirements, design, and test-cases can be prohibitively expensive.
Researchers have therefore proposed automated link-generation solutions primarily based on information-retrieval (IR) techniques;
however, these solutions have failed to deliver the accuracy needed for full adoption in industrial projects. Improvements can be
achieved using deep-learning traceability models; however, their efﬁcacy is impeded by the limited size and availability of project-level
artifacts and links to serve as training data. In this paper, we address this problem by proposing and evaluating several deep-learning
approaches for text-to-text traceability. Our method, named NLTrace, explores three transfer learning strategies that use datasets
mined from open world platforms. Through pretraining Language Models (LMs) and leveraging adjacent tracing tasks, we demonstrate
that NLTrace can signiﬁcantly improve the performance of LM based trace models when training links are available. In such scenarios
NLTrace outperforms the best performing classical IR method with an 188% improvement in F2 score and 94.01% in Mean Average
Precision (MAP). It also outperforms the general LM based trace model by 7% and 23% for F2 and MAP respectively. In addition,
NLTrace can adapt to low-resource tracing scenarios where other LM models can not. The knowledge learned from adjacent tasks
enables NLTrace to outperform VSM models by 28% F2 on generation challenges when presented with a small number of training
examples.

(cid:70)

1 INTRODUCTION

S OFTWARE and systems requirements traceability is de-

ﬁned as ‘the ability to describe and follow the life of
a requirement in both a forwards and backwards direc-
tion (i.e., from its origins, through its development and
speciﬁcation, to its subsequent deployment and use, and
through periods of ongoing reﬁnement and iteration in any
of these phases)’ [1], [2]. Traceability establishes associations
between different levels of requirements and with other
types of artifacts, such as design speciﬁcations, test cases,
models, and process descriptions. It supports numerous
activities such as requirements validation and veriﬁcation,
safety assurance, and impact analysis, and is prescribed
by many regulatory standards as part of the certiﬁcation
process [3], [4], [5], [6], [7], [8], [9], [10].

Given the high cost and effort required to manually
create and maintain trace links during the software develop-
ment process, researchers have proposed various solutions
for generating links automatically [11], [12], [13]. Classical
information retrieval (IR) solutions, such as the Vector Space
Model (VSM) [14], Latent Dirichlet Analysis (LDA) [15],
[16], and Latent Semantic Indexing (LSI) [17] have been ex-
plored in-depth over the past decade, but have met a glass-
ceiling in terms of achievable accuracy, with basic machine
learning (ML) approaches [18], [19], [20], [21] suffering from
similar fates. The primary impedance is caused by their lack
of semantic analysis and of the textual artifacts.

Recent advances in natural language processing intro-
duce potentially more effective approaches for automati-
cally generating accurate trace links. Guo et al. [22] pro-
posed Deep Learning (DL) Trace Models, which leveraged a
Recurrent Neural Network (RNN), to generate trace links
between requirements and design deﬁnitions. However,
their approach required large amounts of previously created

links for training purposes. While Guo et al. demonstrated
the potential for DL techniques to outperform IR ones, they
concluded that much larger training datasets were needed
in order to achieve satisfactory degrees of accuracy. In gen-
eral, NLP-based tracing solutions require huge amounts of
training data in order to perform well; however, in practice,
software projects often lack sufﬁciently large sets of artifacts,
including trace links, to support training of DL models.

More recently NLP researchers have proposed DL meth-
ods based on pre-trained Language Models (LMs). In gen-
eral, these methods use a two-step framework that involves
pre-training an LM, followed by ﬁne-tuning the model. In
the ﬁrst step, the LM conducts a self-supervised training
task to learn general knowledge about natural language
including its vocabulary and grammar. Then in the sec-
ond step, the model is ﬁne-tuned to perform a speciﬁc
task using supervised training [23]. Such approaches can
signiﬁcantly reduce the need for labeled training data dur-
ing ﬁne-tuning because the pre-trained LM can effectively
transfer its knowledge for use in downstream tracing tasks
performed at the project level. For this reason, this two-step
process has been used extensively to address various text-
to-text NLP tasks such as Machine Translation, Question
Answering, Natural Language Inference, and Recognition
of Textual Entailment [24], [25]. However, little work has
investigated the application of such an approach to software
traceability.

The task of generating trace links between natural lan-
guage artifacts represents a domain-speciﬁc text-to-text task
in which the trace model is trained to understand the se-
mantic relatedness between pairs of software artifacts. There
are two main motivations for utilizing a LM in the domain
of software traceability. First, as Allan et al. [26] state, LMs

 
 
 
 
 
 
learn word distributions from a massive corpus of text
data which can be combined with additional information
to accomplish diverse tasks. Trace models supported by
pre-trained LMs, leverage the knowledge they acquire from
the larger training corpus, to understand the semantics of
NL sentences. As such, their level of comprehension is far
better than that of classical machine learning models. In
our recent work, we showed that the use of a pre-trained
LM signiﬁcantly improved the accuracy of generated trace
links when used to trace feature descriptions to source
code in three large OSS [27]. Our DL approach achieved
accuracy of 75% to 99%, measured using Mean Average
Precision (MAP), as compared to 50% to 70% (MAP) when
using VSM. However, we pre-trained an LM model using
millions of documented python methods, previously mined
by Husain et al. [28] from OSS systems, and also leveraged
‘code search’ as an additional ﬁne-tuning task. The result-
ing DL architecture was dependent upon the presence of
source code and not designed to efﬁciently support text-
to-text requirements traceability as addressed in this paper.
This is particularly problematic as much of the traceability
prescribed by standards for developing systems in safety-
critical domains (e.g., [4], [5], [29] [30], [31]) uses text-to-
text traceability to show, for example, that requirements are
satisﬁed by design speciﬁcations, or that hazards have been
fully mitigated by requirements.

In other work [32] we explored bilingual traceability in
OSS projects in which issues and code (including comments
and commit messages) were written in more than one
human language. We leveraged a bilingual LM to mitigate
the semantic and language gap and found that the LM-
based trace model only outperformed translation-enhanced
IR models on larger projects for which training examples
were sufﬁcient to ﬁne-tune the LM. These results suggest
that current LM-based tracing solutions underperform on
small projects or in new projects experiencing the cold
start problem where few or even no training examples are
available [33].

In this paper, we seek to achieve improvements in accu-
racy for text-to-text tracing tasks by leveraging knowledge
acquired from large sized open world data. We start by
utilizing various pre-trained LM models, and then explore
different transfer learning strategies to ﬁne-tune the initial
LM model for use in a speciﬁc software project. We refer
to our approach as NLTrace. Further, we focus on three
speciﬁc tracing tasks – namely link completion in which
links are generated to ﬁll gaps in the set of existing trace
links, link expansion in which links are generated for new
artifacts as they are added to the project, and link generation
in which trace links are generated from scratch without the
beneﬁt of any initial links as training data.

We experimented with three transfer learning strategies
applied within two model architectures, and found that
NLTrace, using task-level transference, achieved the best
performance. It outperformed vanilla BERT-based models
by 7% and 23% with respect to the F2-Measure (i.e., the
harmonic mean of precision and recall) when applied to
link completion and expansion tasks, and also performed
well on the link generation task, where we found that when
even 10 links were provided as training examples (i.e., few-
shot), NLTrace outperformed the best IR model by 28% with

respect to the F2-Measure. Our work therefore makes the
following contributions:

2

• We provide formal deﬁnitions of three unique tracing
tasks related to link completion, link expansion, and
link generation. These tasks have been alluded to previ-
ous papers but without formal deﬁnitions or systematic
evaluations.

• We build a large OSS tracing dataset by mining the
GitHub archive from 2015-2021 to provide an external
data source for supporting transfer learning in DL trac-
ing models. We extract issues, commits, pull requests
and user comments from the REST API dump and mine
their associations using heuristic rules.

• We systematically investigate the use of three different
transfer learning strategies to identify and compare
effective techniques for increasing the accuracy of NL-
NL trace link completion, expansion, and generation
tasks.

• We release a pre-trained LM, targeted at supporting
diverse natural language software engineering tasks,
into the public domain (cf. Open Science at end of the
paper). In this paper, we utilized this LM to pre-train a
LM and enhance the accuracy of trace link generation
in traditional software project environments for three
different tracing tasks.

• We build a framework for searching, retrieving, and
parsing domain-speciﬁc documents, which can be used
to construct a project-speciﬁc corpus, and show how
this corpus can be used to increase tracing accuracy
within a targeted domain.

The remainder of this paper is laid out as follows.
In Section 2, we provide formal deﬁnitions of the tracing
challenges and then in Section 3 describe the research ques-
tions addressed in this study. Section 4 describes the data
collection, transfer strategies and model architectures used
in this study, and Sections 5 and 6 introduce the experiment
setup and discuss the results. Finally, in Sections 7 to 9 we
discuss threats to validity, present related work, and draw
conclusions.

2 PROBLEM DEFINITION

Most existing studies that address traceability automation
focus on the task of ‘trace link generation’; however, in
practice there are three different tracing tasks associated
with trace link generation, trace link completion, and trace
link expansion [22]. Our work explores the effectiveness
of transfer learning solutions upon all three of these tasks,
and therefore we start by providing clear descriptions and
formal deﬁnitions for each of them.

2.1 Trace Link Completion

Trace Link Completion (TLC) refers to scenarios in which
project stakeholders have already established trace links for
a subset of the existing software artifacts. However, given
the non-trivial cost of creating and maintaining trace links,
these links are often incomplete [10], [34]. Link completion
tasks seek to automate the process of generating the missing
links. We formally deﬁne the problem as follows:

′

Deﬁnition 2.1 (TLC). Given a software engineering project
P = (S, T, L
) constituted by source artifacts S, target
′
artifacts T and an incomplete set of trace links L
, an
automated trace model is used to generate the missing true
links ∆L = {⟨si, ti⟩ ∉ L
∣si ∈ S, ti ∈ T } to produce a
completed set of links where project P = (S, T, L

′ ∪ ∆L).

′

2.2 Trace Link Expansion

Trace Link eXpansion (TLX) refers to scenarios in which
a complete set of trace links have been created by project
stakeholders for all existing artifacts; however, new artifacts
are introduced as the project evolves. TLX focuses on con-
structing links between existing and emerging artifacts. We
formally deﬁne this problem as follows:

Deﬁnition 2.2 (TLX). Given a software engineering project
P = (S, T, L) and emerging source artifacts ∆S; the trace
model is used to automatically create a new link set ∆L =
{⟨si, ti⟩ ∣si ∈ ∆S, ti ∈ T } and to update the project as
P

′ = (S ∪ ∆S, T, L ∪ ∆L).

The TLX task is related to, but different from the Trace
Link Evolution (TLE) task, described in our prior work
[35], [36]. Whereas TLX focuses on generating new links in
response to new artifacts, TLE evolves existing trace links
in response to system modiﬁcations. Evaluating NLTrace’s
support for TLE is outside the scope of this paper.

2.3 Trace Link Generation

Trace Link Generation (TLG) is used when no existing trace
links exist – either in a new project or a legacy project with-
out existing links. TLG generates trace links from scratch,
and is formally deﬁned as follows:

Deﬁnition 2.3 (TLG). Given a software engineering project
without existing trace links P = (S, T ), the trace model is
used to automatically create a link set L = {⟨si, ti⟩ ∣si ∈
S, ti ∈ T } in order to produce a ‘link complete’ project P =
(S, T, L)

A special case of TLG leverages sample links that are
explicitly elicited from project stakeholders as training ex-
amples. Our study investigates whether a small number of
example links, can improve the performance of the TLG
task. For experimental purposes we explored the use of 10
training links, and therefore refer to this approach as 10 shot
trace link generation and deﬁne it as follows:

Deﬁnition 2.4 (TLG - 10 shots). Given a software engineer-
ing project with a few trace links P = (S, T, L
∣ =
10, automatically create link set L = {⟨si, ti⟩ ∣si ∈ S, ti ∈ T }
to produce ‘link complete’ project P = (S, T, L

′
) where ∣L

′ ∪ L)

′

The work we present in this paper aims to deliver
signiﬁcant improvements in accuracy for each of these three
tracing tasks (i.e., TLC, TLX, and TLG) when compared to
existing IR and ML tracing solutions.

3 RESEARCH QUESTIONS
Encoder
The
Transformers
proposed by Devlin et al. as a language model

language model was

Representations

Bidirectional

(BERT)

from
initially
for

3

supporting diverse NLP tasks [24]. BERT supports the
transfer learning theory that a model trained on upper-stream
tasks can acquire knowledge to improve its performance
on downstream tasks. This is applicable to the traceability
challenge where TLC, TLX, and TLG represent downstream
tracing tasks.

A recent traceability study used special LMs, pre-trained
using an intermingled corpus of text and code [37], to trace
from NL to code written in various programming languages
(i.e., NL-PL). The results showed that the intermingled
corpus enabled semantic comprehension of PL artifacts and
delivered quite accurate tracing results – at least for the TLC
tasks on which it was evaluated. However, targeted LMs
are not currently available in the area of NL-NL tracing
and to our knowledge no well-trained LMs have been pre-
trained with a software engineering related corpus. Further,
it is unclear whether existing general purpose LMs, that
have been extensively and successfully used to support
other NLP tasks would perform well on the NL-NL tracing
problem. One reason that this might not be the case is due to
the highly technical vocabulary that often characterizes the
domains of software intensive systems projects. Our ﬁrst RQ
therefore addresses the following question in order to estab-
lish a baseline for exploring transfer learning techniques.

RQ1: How well does NLTrace perform without the beneﬁt
of domain-speciﬁc transfer learning, and does it out-
perform classical IR trace models and other previously
described DL tracing models?

Transfer learning has been recognized as an effective
approach for improving model performance [38], [39]. The
underlying notion is that training a model to perform a set
of similar secondary tasks in addition to their primary task
(i.e., generating trace links), enables the model to improve
the performance of its primary task. However, there is an
underlying assumption that the secondary and primary
tasks are sufﬁciently related, so as to allow the DL model
to effectively transfer the knowledge that it learns from
the secondary task(s) to improve its performance on the
primary task.

As our goal is to adapt existing DL models to perform
TLC, TLX, and TLG tracing tasks, we explore different
combinations of model architectures and transfer learning
tasks to determine which solutions can most effectively
apply transfer learning techniques that utilize open world
data sources to facilitate tracing in resource-limited software
project environments. As depicted in Fig. 1, we explored
three transfer strategies and evaluate them through a series
of experiments.

First, we built a LM targeted directly at the general
Software Engineering domain by applying extensive pre-
training strategies to a large SE related corpus. To this end,
we collected more than 372GB of SE artifacts represented
in plain text from millions of OSS projects on GitHub, and
used the text data to pretrain a LM, which we named
SE-BERT. SE-BERT was able to learn the terminology used
in requirements and design artifacts, more effectively than
general LMs.

Second, we adapted the pre-trained LM through addi-
tional pretraining based on domain speciﬁc text. This ap-
proach trained the LM model to better understand project-

speciﬁc vocabulary by utilizing data from two sources that
included (1) the artifacts and glossaries of the targeted
project, and (2) documents retrieved using the Google
Search Engine seeded with search queries extracted from
terms and phrases in the project’s artifacts.

Finally, in our third approach, we explored the effec-
tiveness of task-level knowledge transfer by formulating
an adjacent tracing problem in which the model learned
to recreate a large set of hyperlinks that had previously
been created by OSS project maintainers between GitHub
pages [40]. The updated model was then applied to our
three tracing tasks.

To explore the effectiveness of each of these techniques
we formulated and addressed three additional research
questions. In RQ2 we investigated the performance of the
three proposed transfer-learning techniques used in con-
junction with several different LM models. This part of our
study focused on the two tracing tasks for which training
data was available (i.e., TLC and TLX) and addressed RQ2,
stated as:

RQ2: Which, if any, of the three transfer learning strate-
gies, produce LMs that outperform the original general-
purpose LMs with respect to the TLC and TLX tracing
tasks?

To address the open challenge of the TLG problem, in
which new trace links need to be generated without the
beneﬁt of existing training data we evaluated the beneﬁt of
providing a small number of training examples by asking:

RQ3: Can LM models outperform classical IR methods on
the TLG task when a small number of training examples
are provided?

Finally, we concluded our study by comparing results
across these different methods. While an organization could
use different tracing techniques for different tasks, there is
signiﬁcant beneﬁt and reduced overhead if a single tracing
model can be deployed for all tasks. Therefore we addressed
the ﬁnal research question:

RQ4: What is the overall best method for supporting all
three NL tracing tasks?

4 PROPOSED TRACING MODELS
NLTrace leverages the well-known pretrain-then-ﬁnetune
paradigm. It takes a general purpose BERT model as the
starting point, conducts transfer learning tasks to improve
the LM, and/or performs ﬁne-tuning using the target
project’s data. We evaluated the effectiveness of the three
transfer learning strategies within the context of two differ-
ent model architectures. As shown in Fig. 1, we proposed
and evaluated ﬁve NLTrace variants using the following
techniques and components.

4.1 Mining a Dataset as the LM Knowledge Source

4.1.1 Github Dataset

As of April 2022, GitHub hosted over 73 million develop-
ers and more than 200 million repositories [41]. In order
to build software systems in a collaborative environment,
GitHub developers produce a large amount of textual con-
tent including source code, bug reports, feature requests,

4

Fig. 1: Our experiments evaluated NLTrace with three trans-
fer learning strategies including pretraining a generic soft-
ware engineering language model, adapting the LM to a
speciﬁc project domain, and adaptation to the tracing task.
Three distinct sources of external knowledge were collected
and applied to classiﬁcation and sentence embedding based
architectures, and ultimately ﬁve different NLTrace variants
(labeled 1-5) were evaluated. These models were used for
three different traceability tasks, namely Trace Link Com-
pletion (TLC), Expansion (TLX), and Generation (TLG).

technical discussions, and pull requests. This data serves as
a potentially excellent resource for building LMs targeted
at text-based traceability tasks in the Software Engineering
domain.

Many well-maintained GitHub projects use Autolink
[40] to track and manage complex relationships across dif-
ferent artifacts. Autolink is a GitHub feature which auto-
matically transforms long URLs into a standard abbreviated
form. For example, a developer can conveniently add a
string ‘#1’ in their commit message to refer to Issue No.1
within the same project repository, or could reference a
speciﬁc commit in their issue discussion by adding a 7 digits
SHA hash (e.g. ‘a5c3785’). Since these links have a uniform
format, a regular expression can be used to mine them from
the artifacts. Furthermore, the large quantity of hyperlinks
between textual artifacts, makes them well suited for su-
pervising the training of an NL-NL tracing model, thereby
potentially mitigating TLG’s cold start problem. Throughout
the remainder of this paper we refer to the mined text as the
Git Corpus and the mined autolinks as Git Links. The Git
Corpus was used to build a Generic Software Engineering
LM for use with a classiﬁcation model (cf., Model #1, Fig. 1),
while the Git Links were used for task level adaptation
techniques (cf., Models #4 and #5).

For experimental purposes, we retrieved the Git Corpus
and Git Links archived from 2016 to 2021, using the public
API of the GH Archive project [42] which returns all HTTP
requests sent to the GitHub API during this period, in a stan-
dard JSON format [43]. While data is available from 2011,
we did not use it, because the format was not standardized
until 2016. The six years of downloaded data produced a

2.1TB zip ﬁle, which was used to reconstruct all reposi-
tories by parsing the HTTP requests ordered sequentially
by date and time. We extracted four types of records from
the requests including Comments, Issues, Pull Request and
Commits, but found that Autolinks were primarily present
in Issue-Commit and Issue-Pull requests. A Pull Request is
usually associated with one or more Commits as its purpose
is to deliver a patch that addresses a speciﬁc issue, while
comments are hierarchically associated with Issues and Pull
Requests. Their links can be obtained by parsing the JSON
structure of the request payload. We abstracted the relation-
ships among these four GitHub Artifacts depicted by the
TIM (Traceability Information Model) shown in Fig. 2.

To effectively process such a large amount of data, we de-
ployed a data pipeline on HTCondor, which is a distributed
high throughput computation platform developed by Thain
et al. [44]. We harnessed a machine pool with 300 servers
and distributed the workload evenly across these machines.
We also processed the text to remove non-ascii tokens, code
blocks and stack traces represented in Markdown format.
We removed all artifacts with fewer than ten tokens after
preprocessing, as they tended to be too short to provide
meaningful content, following the cleaning process, and
then veriﬁed that the endpoints of each link existed in the
dataset. Any link with a missing endpoint was removed.
Following this processing step, we obtained a Git dataset
composed of the corpus and links with a size of 372GB.

4.1.2 Domain-Speciﬁc Corpus Construction

In addition to mining a corpus from GitHub, we also used
the Google Search Engine to construct a domain-speciﬁc cor-
pus for each of our target projects. Software artifacts usually
contain technical terminology and jargon rarely found in
vernacular language; therefore, we hypothesized that pro-
viding sentences that included those terms as training exam-
ples for LMs could potentially improve their performance
on downstream tracing tasks. We therefore developed the
pipeline depicted in Fig. 3, which ﬁrst identiﬁes domain-
speciﬁc terms for a speciﬁc project, and then uses the Google
Search Engine to retrieve contextualized examples of their
use. Starting with a project artifact (e.g., a requirement or de-
sign deﬁnition), we used NLTK [45] to perform Noun Phrase
Chunking in order to identify all noun phrases. These noun
phrases included a mix of domain-speciﬁc phrases as well
as more general concepts; however, building a corpus that
includes general concepts could make its use in the transfer
learning process less effective as the model is optimized
for broad, and potentially irrelevant, content. We therefore
generated a black list of general concepts, using the UMBC
webBase corpus provided by Han [46]. This corpus contains
100 million web pages from more than 50,000 websites that
were collected as part of the Stanford WebBase project in
2017, and has a compressed size of 13GB. We applied the
same chunking methods to extract noun phrases from the
UMBC corpus, ranked them by their frequency, and used
the top 1% of the list to create a black list composed of
approximately 30k commonly used phrases. After ﬁltering
each of our domain-speciﬁc concept lists to remove terms
in the black list, we calculated the importance of the re-
maining concepts by computing an IDF (Inverse Document
Frequency) score within the project artifacts. Finally, we

5

Fig. 2: The TIM (Trace Information Model) used in conjunc-
tion with GitHub projects contains four types of artifacts
and ﬁve types of links.

Fig. 3: An automated data pipeline was used to extract con-
cepts from target projects and to retrieve a related corpus.

applied a manually deﬁned threshold to remove additional
concepts and produce a ﬁnal list of domain-speciﬁc concepts
for each project.

We then used the terms from each concept in turn to seed
a query using the public API of the Google Search Engine.
The returned URLs primarily linked to HTML pages, PDF
ﬁles, and/or doc ﬁles, which we converted into plain text
and tokenized into sentences, or long phrases such as those
found in bulleted lists, using the NLTK sentence tokenizer.
Finally, we discarded all sentences that did not have at least
one token that overlapped with the query. The domain-
speciﬁc corpus was used by Models #2 and #3 as shown
in Fig. 1.

4.2 Model Architectures

Traceability tasks are typically formulated as either a classi-
ﬁcation problem or as a distributed representation learning
problem. In the ﬁrst case, binary classiﬁcation models com-
pute a conﬁdence score which is used to predict whether a
candidate trace link is a true link or not; while in the second
case, a model projects the artifacts into a latent semantic
space and then calculates similarity scores as distances
between the source and target artifacts in this space. For
purposes of our experiments, we selected a state-of-the-art
architecture from each of these solution spaces, to serve as a
comparative baseline for evaluating NLTrace.

4.2.1 Classiﬁcation Model
We utilized a Single-BERT architecture from the TBERT
framework proposed by Lin et al. [27] as our classiﬁcation

architecture. This model was selected as it was shown to
perform well on the tracing problem, in comparison to
alternative Twin and Siamese models. It ﬁrst forms pairwise
concatenations for source and target artifacts by adding a
special separator token ‘[SEP]’ between them, and then uses
a single BERT model to encode the merged long sequence.

The BERT model use a self-attention mechanism to ex-
ploit the relevance between the vocabularies and to create a
high dimension hidden-state matrix as output. The classiﬁ-
cation header, which is a 3-layer neural network takes this
matrix as input and produces a score between 0 and 1 to
indicate the relevance of the paired input. It then optimizes
for standard Binary Cross Entropy Loss as the objective
function [47] written as follows:

L = −(ylog(p) + (1 − y)log(1 − p))

(1)

where y is the label for a given training link and p is the
predicted likelihood of it being a true link. We followed
the setup of TBERT to create a balanced training dataset
in which positive examples were over-sampled to achieve
the same number as the negative examples.

4.2.2 Sentence Embedding Model

Distributed representation learning approaches transform
textual documents into vectors in latent space where the
distance between pairs of vectors reﬂects their semantic re-
latedness. Classical IR methods such as VSM, LDA and LSI
all use these types of vectors. For example, the source and
target artifacts are vectorized based on TF (Term Frequency)
and IDF scores, and then a function (e.g., Cosine Similarity)
is applied to obtain a similarity score for the embedded
artifacts. Candidate links are then typically ranked by their
scores and ﬁltered according to a threshold score in order to
identify a set of candidate true links.

We adopted the SimCSE framework [48], which is a
BERT based sentence embedding model that uses con-
trastive learning tasks to calibrate the distance between
artifacts. Contrastive learning is an unsupervised technique
in which a model learns to differentiate between similar and
dissimilar elements. It typically starts with an element (such
as an image or a sentence), modiﬁes it (e.g., via cropping,
transforming, or replacing a word or phrase), and assumes
that the modiﬁed version is similar to the original version,
whereas elements that did not originate from the original
element are dissimilar [49]. Although no previous work has
applied SimCSE to the traceability tasks, it has been used in
similar NLP problems and ﬁts the traceability task well. We
utilized SimCSE to perform both an externally supervised
and a self-supervised task.

For the externally supervised task, the model is trained
to perform a set of small ranking tasks. For example, we
applied it to the traceability task as follows. Given four
source artifacts, four target artifacts, and four previously
deﬁned trace links between any pair of these source and
target artifacts, SimCSE learns to link each source and
target artifact to produce a ranked list of the 16 potential
pairs, such that the top 4 are marked as true links and the
remaining 12 as non-links. The model is then optimized to
reduce the cosine distance between positive links and to
increase the distance for negative examples.

SimCSE incorporates the Cosine Similarity directly into

the objective function as follows:

6

L = −log

esim(hi,h

+
i )/τ

∑N

j=1(esim(hi,h+

i )/τ + esim(hi,h−

i )/τ )

(2)

+
where sim(hi, h
i ) refers to the Cosine Similarity between hi
+
and h
i , which are the hidden state vectors for artifacts in the
−
true links; whilst sim(hi, h
i ) refers to the Cosine Similarity
between vectors of artifacts in negative links.

For the self-supervised task, we leveraged SimCSE’s
ability to perform contrastive learning by creating multiple
vector representations for a single sentence. Within the
SimCSE neural network, the internal semantic representa-
tion, known as its hidden state, was applied to multiple
dropout layers to randomly corrupt a small part of this
representation. These dropout layers produce vectors that
differ from each other by one or few digits. Pairs of vectors
generated from the same original sentence were treated as
links (i.e., positive examples), while all others were treated
as non-links (i.e., negative examples).

SimCSE is a multi-task model. For both the supervised
and self-supervised approaches, it performs the link predic-
tion and a Mask Language Modeling (MLM) task simulta-
neously. MLM objectives are weighted and appended to the
ranking objective formulated in Eq. 2. For this reason, we
did not create a SimCSE based counter-part for our model
#1 in Fig. 1, because model #5 already includes it as a sub-
task.

4.3 Transfer learning for Traceability

Many previous studies have been conducted to improve the
ability of a LM to support downstream tasks. From these we
identiﬁed the following three key strategies.

4.3.1 Pretraining the LM for the SE Domain

Our ﬁrst transfer learning strategy focused on pretraining
a Generic Software Engineering LM to produce SE-BERT-
CLS (Model 1 in Fig. 1). It applied MLM [24] to an otherwise
unorganized text corpus to empower multi-layer transform-
ers to perform the tracing task. The pretraining procedure
leveraged the model’s attention mechanism to learn mutual
relations between the hidden tokens and their surrounding
context.

Other researchers have applied MLM mechanisms to
similar NLP problems. For example, in RoBERTa, Liu et
al. [50] optimized the pretraining process by introducing
a dynamic MLM mechanism whilst leveraging an addi-
tional 16GB of text from BOOKCOR-PUS [51] and English
Wikipedia in the pretraining process. They showed that
RoBERTa outperformed BERT on the majority of general
NLP tasks deﬁned by the GLUE dataset [52]. In addition,
targeted LMs have been created that adapt vanilla BERT
to support speciﬁc domains and their associated tasks, For
example, Finbert [53], BioBERT [54], ClinicalBert [55] and
SciBERT [56] conduct continual pretraining on vanilla BERT
to establish domain dedicated LMs for Finance, Biology,
Medical Care and general Sciences respectively. Gururan-
gan et al. [57] referred to this type of technique as DAPT
(Domain Adaptive Pretraining), in contrast to the TAPT

(Task Adaptive Pretraining) which we describe in Sec 4.3.2.
This extensive body of work shows that DAPT can beneﬁt
diverse domain-speciﬁc downstream tasks, and therefore
potentially be useful for our tracing tasks.

However, DAPT requires a large sized domain-speciﬁc
text corpus. For example, Gururangan et al. utilized around
40GB of text for each of four different experimental do-
mains. While this might be feasible in the Software Engi-
neering domain for large companies with a huge corpus of
their own data, it is likely infeasible for most organizations
to provide such quantities of data. For this reason, we
use data mined from public data sources, as previously
described in Section 4.1, to pretrain SE-BERT. We selected
the ‘bert-base-uncased’ LM [58] as our starting point, and
applied dynamic MLM as the pretraining task for adapting
the BERT model for tracing SE artifacts. We refer to this
pre-trained model as SE-BERT. We decided not to pretrain
SE-BERT from scratch, based on observations made by the
SciBERT team [56], who reported that training from scratch
improved performance by an average of only 0.7%, but took
a signiﬁcant amount of time and computing resources. SE-
BERT is applied on Model #1 in Fig. 1.

4.3.2 Adapting the baseline LM to the Project Domain

Our second transfer learning strategy, which we name
Project Domain Adaptation, focuses on project-level adap-
tation based on vocabulary and concepts of the project itself.
This adopts Task Adaptive Pretraining (TAPT), proposed by
Gururangan et al. [57] as an alternative of DAPT. Instead of
building a large corpus for pretraining a domain-targeted
LM, as in the case of SE-BERT-CLS, they created a much
smaller corpus targeted directly at the speciﬁc NLP task,
which in our case includes the three tracing tasks of TLC,
TLX, and TLG. In sufﬁciently large projects or organizations,
the corpus could be assembled from internal documents and
Wiki pages using product-speciﬁc terminology and jargon to
describe products and processes products; however, in other
projects, external data sources need to be used.

As depicted in Fig. 1, we created two different models
named Proj-CLS (Model #2) and Proj-RANK (Model #3),
based on classiﬁcation and CSE architectures respectively.
In both cases, we used the data pipeline described in Sec-
tion 4.1.2 to automatically mine a project related corpus
from open source datasets. For the Proj-CLS model, we
used the MLM task to adapt the vanilla BERT with project
dedicated vocabulary; while for Proj-RANK, we conducted
a self-supervised ranking task as discussed in Sec. 4.2.2 to
learn the languages in the projects.

4.3.3 Learning From Adjacent Tasks
The ﬁnal approach, which we refer to as Trace Task-Related
Adaptation adapts the LM by training it to perform similar
(adjacent) tasks. As depicted in Fig. 1, we applied adja-
cent task training within the context of the classiﬁcation
(Model #4) and CSE (Model #5) architectures. We previously
showed that adjacent training tasks can be used effectively
to generate trace links between NL and programming lan-
guage (PL) artifacts [59]. In that case we used code search
as an adjacent task, and showed that transfer learning im-
proved MAP scores by more than 20%. We ﬁrst built a BERT-
based trace model to predict the relevance between python

7

function doc strings and function speciﬁcations, and then
ﬁne-tuned it using links between Issue descriptions and the
Code Change Set within Commit messages. Even though
the format and content of artifacts in the code search were
distinctly different from the tracing task, the model was able
to effectively learn a general set of rules that improved the
NL-PL tracing accuracy.

We therefore sought to identify effective adjacent tasks
that could be applied to the NL-NL tracing task prior to
ﬁne-tuning the trace model. As shown in Fig. 2, our git data
contains ﬁve types of links between four types of git ar-
tifacts, of which the Issue-Commit links are most similar to
trace links in SE projects. In traditional software and systems
engineering projects, artifacts such as requirements and de-
sign speciﬁcations, are typically arranged into a hierarchical
structure, in which high-level artifacts are reﬁned into more
detailed lower-level ones. This is similar to, but not the same
as, the relationships between an Issue and its associated
Commits. GitHub users typically create an Issue describing
a problem or new feature request, and articulate their con-
crete implementations through one or more corresponding
Commit messages. While the Pull Request-Commit links
share similar characteristics, users sometimes simply copy
the Commit message as a Pull Request description, making
this type of link less valuable than Issue-Commits as an
adjacent training task. Other types of associations, such as
links from pull requests to issues, are closer to peer-to-peer
relations, and therefore less representative of our targeted
tracing tasks. Similarly we opted not to use links from com-
ments to other artifacts due to the broad range of topics cov-
ered by the comments. Focusing only on the targeted types
of links reduced the pretraining corpus to around 110GB.
Our decision to use issue-commit links was supported by
an informal experiment in which models trained with Issue-
Pull links returned signiﬁcantly fewer improvements than
models based on Issue-Commit links. Given the OSS issue-
commit links as a resource, we investigated various adjacent
tracing tasks and their ability to support knowledge transfer
into more traditional systems projects.

5 TRANSFER-LEARNING TRACING EXPERIMENTS

5.1 Datasets

We evaluated NLTrace against four datasets as shown in
Table 1. In selecting the datasets we established inclusion
criteria that each dataset must include (i) traditional soft-
ware requirement artifacts plus one additional NL artifact
type (e.g., design speciﬁcations or regulations), and (ii) have
an existing trace matrix containing at least 100 manually
vetted links between the two artifact types. In addition
(iii) we sought to select datasets from diverse domains. We
included CM1 as an exception case as it represented a small
project from a niche domain. Our criteria were deﬁned to fo-
cus the research upon requirements traceability solutions in
projects that follow a more traditional requirements-driven
approach, as is common in safety-critical systems domains.
This excluded the use of OSS datasets that use informal
feature requests in lieu of more traditional requirements,
and therefore limited the number of datasets available to us.

TABLE 1: Software Engineering projects in four domains.
The projects were selected because they were non-trivially
sized and provided manually created trace links that were
used for validation purposes.

8

Description
Project
Subway signalling system SRS
PTC
CCHIT
Electronic record system
Dronology Multi-UAV ﬂight coord.
CM1

Scientiﬁc instrument

Regulations
SRS
SRS

Source

Target
SDS
SRS
SDS
SDS

Our ﬁrst dataset was a Positive Train Control (PTC)
system, which supports communication and signaling for
a large underground railway system. The dataset, including
requirements, design speciﬁcations, and an associated trace
matrix, was provided by our industrial collaborators under
a non-disclosure agreement. The Dronology dataset [60] was
developed at the University of Notre Dame for coordinating
emergency response missions of multiple small unmanned
aerial vehicles (UAVs). It includes over 10,000 LoC, and was
developed by a mix of professional developers, post-docs,
and both graduate and supervised undergraduate students.
It has been used by over 20 external research teams to
support research in areas such as product lines, security,
and traceability [61], [62], [63]. For purposes of this paper,
we used a subset of the Dronology dataset including NL
requirements, design deﬁnitions, and associated trace links.
In both PTC and Dronology projects, the trace links were
constructed by the original developers to support activities
such as requirements validation and impact analysis. The
CCHIT dataset was initially derived from two industrial
sources and is available via COEST.org. It includes two sets
of requirements. The ﬁrst set was provided by the Cer-
tiﬁcation Commission for Health Information Technology
(CCHIT) for certifying electronic health records (EHRs) and
the networks they use. The second set of requirements was
provided by the Veteran Administration’s Electronic Health
Record system (WorldVista). Trace links in the CCHIT
dataset are primarily used to support compliance analysis
and were created by researchers for use in a prior publica-
tion [18]. The CCHIT dataset is also available at COEST.org.
The CM1 dataset is provided by NASA. It is an extract from
the artifacts of an interstellar telescope and includes high-
level and low-level design requirements. The links between
the artifacts were manually created by experts in NASA.

5.2 Experiment Setup

As discussed in Sec. 2, we focused on the three tracing
tasks of TLC, TLX and TLG. To validate results from our
experiments, we compared the links generated by our NL-
Trace variants against the manually created trace links (aka
the ‘answer set’), provided with each dataset. We split the
datasets in distinct ways that were appropriate for each
tracing task in order to create a training (train), validation
(valid), and test (test) dataset.

For the TLC task, we followed the ‘split-by-link’ strat-
egy adopted by Guo et al. [22]. We performed a pairwise
mapping between each source and target artifact to create
the complete set of pairs, and then tagged them as ‘true’
or ‘false’ links according to how they were marked in the

Fig. 4: The data organization for experiments related to the
three tracing tasks of trace link completion (TLC), expansion
(TLX) and generation (TLG) are supported though ‘split-by-
link’ and ‘split-by-artifact’ strategies.

answer set. We then randomly split the candidate links
into ten-folds, assigning eight folds for training, one for
validation, and one for test. For this link completion task,
all existing source and target artifacts were visible for each
phase of training, validation, and testing. As CM1 has fewer
links, we split its data into 2/1/1 train, validate, and test
folds to ensure sufﬁcient test links. By applying NLTrace to
predict the links in the test dataset, we simulated the trace
link completion scenario in practice.

For the TLX link expansion task, we adopted a ‘split-
by-artifact’ method in which the source artifacts were ran-
domly divided into ten-folds with 8/1/1 fold(s) assigned
to train/dev/test sets respectively. Target artifacts were not
divided, and all target artifacts were visible to each set of
source artifacts. This simulated the case in which a relatively
complete set of target artifacts are available (e.g., require-
ments) whilst source artifacts (e.g., design speciﬁcations)
are added over time. Within each split, we performed a
pairwise mapping of the partial set of source artifacts to
all target artifacts, and tagged positive and negative links
according to the answer set. In this case, we simulated the
expansion scenario in which 10% of new source artifacts
were added during software development and NLTrace was
used to predict the links between these new source artifacts
and the existing target artifacts. For the smaller CM1 dataset,
we assigned an equal number of source artifacts in each
partition, leading to a test set with 14 links.

For the TLG generation task, we also adopted the ‘split-
by-artifact’ method to create the train/dev/test splits; how-
ever, we hid the trace links that were previously available
as part of the training process. In the case of 0-shot (i.e., no
training examples available), all links were masked; whereas
in our experiment with 10-shot (i.e., 10 examples provided)
we randomly selected 10 links from the training data, and
allowed NLTrace to use these links and their associated
artifacts as examples. All other links were masked. The
details of data splits are shown in Table. 2 and the data split
procedure is illustrated in Fig. 4.

To increase the reliability of our conclusion, we repeated
each experiment ﬁve times using the train/validate/test
folds created with different seeds, and our reported results
reﬂect the average from the ﬁve runs. For the TLC, TLX and
TLG experiments, we ran experiments on machines with
one Quadro RTX 6000 GPU, whilst for the time consuming
SE-Bert pretraining, we deployed our experiments on Azure
servers using 8 Tesla-100V GPUs.

TABLE 2: Train, development and test dataset splits for each project. Split-by-link and Split-by-artifacts were used for link
completion and link expansion tasks respectively. For link generation, we utilized the Split-by-artifacts, but without use of
links in the training set.

9

Completion

Expansion

Generation

Source
Target
True Links
Source
Target
True Links
Source
Target
True Links

CCHIT
valid
419
1816
405
41
1816
318
41
1816
318

train
419
1816
3241
335
1816
3193
335
1816
0

test
419
1816
406
43
1816
541
43
1816
541

train
72
415
470
57
415
468
57
415
0

PTC
valid
72
415
58
7
415
26
7
415
26

test
72
415
59
8
415
93
8
415
93

Drone
valid
94
210
21
9
210
19
9
210
19

train
94
210
167
75
210
166
75
210
0

test
94
210
22
10
210
26
10
210
26

train
22
53
22
7
53
18
7
53
0

CM1
valid
22
53
10
7
53
13
7
53
13

test
22
53
13
8
53
14
8
53
14

5.3 Evaluation Metrics

We used F2 and Mean Average Precision (MAP) as our
evaluation metrics. The F2 score measures the weighted
harmonic mean of precision and recall whilst favoring recall.
We selected F2 over F1 because it is commonly used in
software traceability experiments, where missing a link is
more expensive than including a false link [12].

F 2 = 5 ⋅

precision ⋅ recall
4 ⋅ precision + recall

(3)

MAP measures the overall ranking of the true links
among all generated links. Each source artifact is treated
as a query, and the Average Precision (AP) is calculated
according to the ranking of its true links. In Eq. 4, Ni
refers to the number of true links for source artifact i, and
Precision(j) calculates the precision for linkj by processing
the links ranked above it. It is computed as follows:

APi =

1
Ni

N
∑
j=1

P recision(j)

(4)

MAP is then computed as the mean of all AP values as
shown in Eq. 5, where M refers to the number of source
artifacts.

M AP =

1
M

M
∑
i=1

APi

(5)

6 RESULTS AND DISCUSSION

We now address the four research questions. For each
question we describe the experiments that were conducted,
analyze the results, and summarize the ﬁndings in a series
of nine key observations.

6.1 RQ1: How well does NLTrace perform without the
beneﬁt of domain-speciﬁc transfer learning, and does
it outperform classical IR trace models and other pre-
viously described DL tracing models?

To answer this question we evaluated NLTrace using four
general purpose LMs and three different classical IR trac-
ing models. The general purpose LMs were ‘bert-base-
uncased’, ‘roberta-base’, ‘xlnet-base-cased’ and ‘distilbert-
base-uncased’, while the classical IR models were VSM,
LDA and LSI. In addition, we also evaluated TraceNN [22]
as discussed in Section 8, and DeepMatcher [64].

For TraceNN, we implemented the model according to
the authors’ description. In their study, RNNs with LSTM
and BiGRU architectures are discussed and compared. We
chose the BiGRU version as it achieved better tracing results
in their study and also outperformed LSTM when used in
our own prior work [27] to trace from text to code. We
applied the default conﬁgurations speciﬁed in their paper,
except we adjusted the neural size and training epochs
to adapt the model to our experiment projects’ size. In
addition, we added a dropout, with a rate of 0.2, to the
MLP layers to reduce the chance of overﬁtting. Finally, we
applied the widely used pre-built Glove word embeddings
[65] to initialize the embedding layer for TraceNN.

DeepMatcher is an LM based document encoder which
transforms artifacts into a vector representation and then
uses Cosine similarity to calculate relevance between pairs
of vectors. DeepMatcher uses DistillBert to generate token
embeddings for the tokens in noun phrases, and then takes
the average of those vectors to produce the ﬁnal artifact
representation. We followed the authors’ description to im-
plement this model and used the same libraries mentioned
in their work.

Results are shown in Table 3 for TLC and TLX. TLG
is identical to TLX for techniques that do not have any
inherent training (e.g., VSM, LDA, LSI, and DeepMatcher).
TraceNN explicitly requires training data, and cannot be
executed without it. Bert, RoBerta, XLNet, and DistillBert
all leverage trace links as part of their ﬁne-tuning step,
and cannot be expected to perform well without links. To
demonstrate this, we report the use of Bert as a represen-
tative LM model applied to LTG in the ﬁnal row of Table
3.

The two rightmost columns in Table

3 compare the
performance for each approach averaged across each of the
four projects versus the average performance of VSM. These
results show that VSM outperformed the topic modeling
approaches of LDA and LSI, as reported consistently in prior
work (e.g., [32], [66]). It also outperformed DeepMatcher for
TLC, TLX, and TLG, and outperformed TraceNN for the two
tasks that TraceNN was applied to (i.e., TLX and TLC).

Despite promising previously published results [22], we
observed that TraceNN only outperformed VSM in one out
of eight cases, namely the TLC task in the CCHIT dataset.
The learning curve indicated that TraceNN quickly adapted
to the training data with a rapid drop in training loss,
however the validation loss converged at a relatively high

TABLE 3: Performance of BERT Classiﬁcation Model and baselines on TLC, TLX and TLG without the beneﬁt of the external
knowledge sources and associated transfer learning techniques.

10

VSM
LDA
LSI
TraceNN
DeepMatcher
Bert
RoBerta
XLNet
DistillBert
VSM
LDA
LSI
TraceNN
DeepMatcher
Bert
RoBerta
XLNet
DistillBert
Bert

TLC

TLX

TLG

F2
0.166
0.069
0.033
0.243
0.137
0.599
0.581
0.573
0.568
0.166
0.076
0.071
0.086
0.086
0.284
0.331
0.257
0.224
0.083

(cid:35)
(cid:35)
(cid:35)
(cid:72)(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:72)(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
=Baseline,

CCHIT

PTC

Drone

CM1

MAP
0.314
0.119
0.092
0.333
0.260
0.610
0.599
0.587
0.633

0.162
0.044
0.032
0.093
0.095
0.211
0.223
0.178
0.235
0.058

F2
0.209
0.090
0.077
0.090
0.066
0.404
0.412
0.268
0.345
0.217
0.103
0.081
0.092
0.077
0.488
0.474
0.320
0.365
0.044

MAP
0.510
0.328
0.318
0.315
0.390
0.703
0.640
0.514
0.641
0.277
0.145
0.140
0.097
0.180
0.580
0.588
0.425
0.484
0.080

F2
0.568
0.460
0.348
0.275
0.556
0.677
0.699
0.687
0.651
0.562
0.424
0.408
0.056
0.273
0.541
0.624
0.499
0.525
0.191

MAP
0.807
0.593
0.640
0.397
0.725
0.864
0.925
0.869
0.904
0.698
0.409
0.502
0.027
0.348
0.716
0.729
0.655
0.660
0.198

F2
0.469
0.409
0.290
0.224
0.294
0.503
0.287
0.259
0.264
0.458
0.403
0.295
0.177
0.410
0.318
0.342
0.386
0.255
0.311

MAP
0.637
0.466
0.514
0.224
0.470
0.628
0.367
0.283
0.288
0.531
0.307
0.376
0.124
0.295
0.258
0.366
0.333
0.194
0.173

=Proposed architecture without the beneﬁt of transfer learning

Avg Improve
F2
-

MAP
-

-57.50% -62.96%
-83.32% -65.95%
-12.43% -34.47%
-37.60% -26.31%
186.70% 72.85%
59.16%
170.69%
45.39%
148.25%
62.41%
154.52%
-
-

-35.90% -51.04%
-45.79% -46.69%
-64.33% -70.01%
-43.63% -42.72%
22.65%
40.37%
30.82%
50.94%
4.98%
18.76%
13.04%
12.75%
-57.27% -78.53%

(cid:35)

(cid:72)(cid:35)

value. Guo et al. described this overﬁtting phenomenon as
the ‘glass ceiling’, which is the major impedance of the
RNN based trace model. We also observed that TraceNN
did not converge for the TLX problem, most likely due
to the gap between training and testing data in the TLX
expansion problem, where the model is asked to generate
links for artifacts that have never been seen in training.
DeepMatcher achieved higher MAP scores but lower F2
scores than TraceNN, and also did not outperform VSM on
either the TLC or TLX tracing tasks.

The general LM based models (i.e., Bert, RoBerta, XLNet,
and DistillBert) all achieved signiﬁcantly better performance
across the four projects than the classical IR approaches, Tra-
ceNN, and DeepMatcher. The Bert based model improved
over VSM for both the TLC and TLX tasks by an average
of 186.70% and 72.85% for F2 and MAP respectively in the
TLC task, and by 40.37% (F2) and 22.65% (MAP) for TLX.
The RoBerta model outperformed Bert for the TLX task
achieving an additional 10% (F2) and 8% (MAP). We believe
that the additional corpus used by RoBerta provided the
extra knowledge needed to mitigate the terminology gap
between training and test data in order to generate links for
previously unseen data. This contrasted with the TLC task
in which the training data was able to provide better su-
pervision than the knowledge extracted from the additional
corpus. We hypothesized that the knowledge from RoBerta’s
additional pretraining may have conﬂicted with the ﬁne-
tuning procedure, because the additional corpus used by
RoBerta was not related to our target project domain.

To support our observation that LM based models do
not generally perform well on the TLG tracing task, we also
report the Bert results without the beneﬁt of any training,
and observe that Bert is outperformed by VSM by 57.27%
(F2) and 78.53% (MAP) respectively. This poor performance
is because Bert is not calibrated to the project data and there-
fore tends to produce somewhat haphazard results. These
ﬁndings are summarized through the following observation.

Observation #1: Currently available pre-trained language
models (LMs) do not perform well on tracing tasks without
the beneﬁt of transfer learning.

6.2 RQ2: Which, if any, of the three transfer learning
strategies, produce LMs that outperform the original
general-purpose LMs with respect to the TLC and TLX
tracing tasks?

).

(cid:35)

In this section we address RQ2 through evaluating the
effectiveness of each of the three transfer learning strategies,
as previously described in Section 4.3, applied to the TLC
and TLX tracing tasks. We do not include TLG in this
evaluation, as additional transfer learning strategies cannot
overcome the initial lack of ﬁnetuning discussed in RQ1. For
)
each strategy we compare our novel approaches (labeled
against a set or related baselines (labeled

(cid:32)
Pretraining Transfer Strategies: First, we explored how the
two pretraining based transfer learning strategies, named
‘DomainLM’ (cf. Section 4.3 and ‘Target Project Adaptation’
(cf. Section 4.3.2, impact the trace performance. Results are
reported in Table. 4. As previously explained, SE-BERT-
CLS and Proj-CLS both represent LM based classiﬁcation
trace models that deploy our proposed transfer learning
strategies. We used the Bert based model as a baseline
given its strong performance in the RQ1 experiments. We
also added SciBert [56] for additional comparison purposes.
SciBert is a Bert model pre-trained to perform NLP tasks
in general scientiﬁc areas. Its corpus contains 1.14M papers,
including 18% from the computer science domain and 82%
from the biomedical domain, and as such is a reasonable
match for our four technical domains of health-care, train
controls, UAV, and space exploration (CM1)

The results in this table show that Proj-CLS effectively
improved the trace performance for both the TLC and TLX
tasks and achieved an average improvement of 6.96% (F2)
and 8.18% (MAP) over the Bert baseline for the TLC task,
outperforming the SciBert model. For the TLX task, Proj-CLS

TABLE 4: Accuracy achieved by pretraining the LM on Git Corpus and Project Corpus for the classiﬁcation model. Average
improvement results (right hand side) represent comparisons to Bert (top row).

11

Completion
(TLC)
Bert
Roberta
SciBert
SE-BERT-CLS
Proj-CLS

Expansion
(TLX)
Bert
Roberta
SciBert
SE-BERT-CLS
Proj-CLS

(cid:35)
(cid:35)
(cid:35)
(cid:32)
(cid:32)

(cid:35)
(cid:35)
(cid:35)
(cid:32)
(cid:32)

Arch
Style
CLS
CLS
CLS
CLS
CLS

Arch
Style
CLS
CLS
CLS
CLS
CLS

Trans
Learn
n/a
n/a
PRE
PRE
PDA

Trans
Learn
n/a
n/a
PRE
PRE
PDA

CCHIT

PTC

Drone

CM1

F2
0.599
0.581
0.602
0.612
0.616

MAP
0.610
0.599
0.664
0.654
0.641

F2
0.404
0.412
0.441
0.389
0.412

MAP
0.703
0.640
0.740
0.634
0.692

F2
0.677
0.699
0.683
0.714
0.715

MAP
0.864
0.925
0.904
0.943
0.918

F2
0.503
0.287
0.578
0.375
0.592

MAP
0.628
0.367
0.705
0.564
0.772

CCHIT

PTC

Drone

CM1

F2
0.284
0.331
0.384
0.346
0.371

MAP
0.211
0.223
0.295
0.264
0.253

F2
0.488
0.474
0.493
0.456
0.479

MAP
0.580
0.588
0.634
0.576
0.580

F2
0.541
0.624
0.589
0.628
0.608

MAP
0.716
0.729
0.713
0.789
0.747

F2
0.318
0.342
0.512
0.468
0.479

MAP
0.258
0.366
0.409
0.460
0.394

Avg Improve
F2
-

MAP
-

-10.15% -11.29%
7.74%
6.33%
-0.93%
-5.43%
8.18%
6.96%

Avg Improve
F2
-
9.19%
26.65%
19.68%
23.01%

MAP
-
12.68%
26.81%
28.29%
19.32%

PRE=Pretraining, PDA=Project Adaptation, and ADJ= Adjacent task, ADJ* uses general NLP tasks as adjacent task

=Baseline,

=Proposed transfer learning technique

(cid:35)

(cid:32)

achieved 23.01% (F2) and 19.32% (MAP) improvements,
but did not outperform SciBert. These results suggest the
following:

Observation #2: Pretraining using a project corpus col-
lected through our data pipeline can effectively improve
the accuracy of TLX and TLC tracing tasks in the associated
project.

The performance of SE-BERT-CLS was somewhat mixed.
It achieved its best average MAP on the TLX task, but
performed worse than the Bert baseline on the TLC problem.
Looking at individual projects, we see that for TLC tasks, SE-
BERT-CLS underperformed on the PTC and CM1 projects
but achieved the best and moderate results on Drone and
CCHIT projects respectively. These results can be explained
by the availability, or lack of availability, of relevant projects
in the Git corpus with insufﬁcient coverage for the two un-
derperforming domains. As an OSS community, Github has
fewer repositories related to space instruments (CM1), and
train control (PTC), whilst having 124 repositories 1 related
to EHR (Electronic Health Records), and 33k repositories re-
lated to drones 2. Although SE-BERT did not perform as well
as the SciBERT and Proj-CLS models in this study, it could
potentially be useful to the OSS Engineering community as
a pre-trained LM dedicated to the SE domain. We therefore
publicly release SE-BERT-CLS to support future research in
NL tasks within the OSS domain.
Observation #3: OSS supported pretraining of LMs is only
effective when a sufﬁciently rich corpus of relevant OSS
projects is available for the project domain.

Pretraining Transfer Strategies: Our second experiment fo-
cused on task-level transfer for CLS, with results reported
in Table. 5. In addition to the Bert and SciBert baselines,
we also include models previously developed for general
NL-NL tasks and provided by the GLUE dataset [52]. By
comparing the performance of Task-CLS and these models,
we were able to evaluate whether Commit-Issue trace links
provide a better knowledge source than other NL-NL tasks
that have been shown to perform well for other more

1. EHR repos
2. Drone Repos

general NLP tasks. Descriptions of the general NLP tasks
that we included in our experiment are provided in Table 7.
While many different training tasks have been explored in
prior NLP research, we selected these ﬁve tasks because
they focused on classifying sentence-to-sentence relation-
ships through exploiting token level associations. The tasks,
which include predicting entailments and contradictions,
evaluating paraphrases, determining whether a sentence
provides an answer to a speciﬁc question, and checking for
semantic relatedness between two sentences, were chosen
because they are similar in nature to the tracing tasks and
are therefore more likely to be effective for supporting
transfer learning for TLX, TLC, and TLG related tasks.

Results are reported in Table. 5. Among all the general
NL-NL tasks, MPRC and STS-B achieved the greatest im-
provements over the BERT baseline for the TLC and TLX
tasks, with MRPC improving performance by 3.91% (F2)
and 4.83% (MAP) for TLC, and by a more signiﬁcant amount
of 19.89% (F2) and 21.43% (MAP) for TLX. Similarly, STS-
B improved TLC by 6.55% (F2) and 4.2% (MAP) for TLC,
and by 19.65% (F2) and 22.45% (MAP) for TLX. Intuitively,
the reason that MRPC and STS-B outperformed the other
training tasks, was that their focus on determining similarity
between two sentences, which more closely matched the
objective of software traceability. However, the Task-CLS
model, which applied the classiﬁcation task to the Issue
and Commit Git links instead of applying it to more general
pairs of sentences, performed even better, achieving 10.21%
(F2) and 13.84% (MAP) improvement for TLC, and 27.69%
(F2) and 29.87% (MAP) improvement on TLX. Finally, the
Task-CLS also outperformed the SciBert based trace model.
Observation #4: General NL tasks focused on detecting
similarity between sentence pairs supported transfer learn-
ing for the TLC and TLX tracing tasks; however, the Task-
CLS approach, which applied these tasks to issue-commit
links, performed even better.

Transfer Learning applied to CSE Architectures: Finally, we also
explored transfer learning applied to the CSE architecture.
The authors of SimCSE [48], released their model trained
using task-level knowledge transfer on the GLUE datasets,
and reported that it outperformed its counterpart, the Bert-
RANK model, on several NL-NL NLP tasks. Therefore, we

TABLE 5: Accuracy eciehved through the use of transfer learning from adjacent tasks. Average improvement results (right
hand side) represent comparisons to Bert (top row).

12

Completion

Bert
SciBert
Task-CLS
MNLI
MRPC
QNLI
RTE
STS-B

Expansion

Bert
SciBert
Task-CLS
MNLI
MRPC
QNLI
RTE
STS-B

(cid:35)
(cid:35)
(cid:32)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)

(cid:35)
(cid:35)
(cid:32)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)

Completion

Bert
GLUE-RANK
Task-RANK
Proj-RANK

Expansion

(cid:35)
(cid:35)
(cid:32)
(cid:32)

Arch
Style
CLS
CLS
CLS
CLS
CLS
CLS
CLS
CLS

Arch
Style
CLS
CLS
CLS
CLS
CLS
CLS
CLS
CLS

Trans
Learn
n/a
PRE
ADJ
ADJ*
ADJ*
ADJ*
ADJ*
ADJ*

Trans
Learn
n/a
PRE
ADJ
ADJ*
ADJ*
ADJ*
ADJ*
ADJ*

CCHIT

PTC

Drone

CM1

F2
0.599
0.602
0.610
0.508
0.598
0.591
0.602
0.623

MAP
0.610
0.664
0.679
0.558
0.651
0.643
0.633
0.628

F2
0.404
0.441
0.439
0.439
0.428
0.389
0.403
0.422

MAP
0.703
0.740
0.733
0.657
0.737
0.691
0.660
0.624

F2
0.677
0.683
0.718
0.723
0.699
0.711
0.635
0.714

MAP
0.864
0.904
0.948
0.927
0.869
0.867
0.871
0.917

F2
0.503
0.578
0.626
0.456
0.537
0.318
0.251
0.564

MAP
0.628
0.705
0.817
0.549
0.673
0.362
0.335
0.746

CCHIT

PTC

Drone

CM1

F2
0.284
0.384
0.392
0.333
0.364
0.380
0.379
0.393

MAP
0.211
0.295
0.317
0.246
0.271
0.266
0.260
0.269

F2
0.488
0.493
0.476
0.472
0.486
0.438
0.437
0.450

MAP
0.580
0.634
0.588
0.566
0.595
0.546
0.531
0.552

F2
0.541
0.589
0.611
0.550
0.610
0.557
0.542
0.578

MAP
0.716
0.713
0.743
0.663
0.764
0.670
0.678
0.726

F2
0.318
0.512
0.515
0.371
0.442
0.276
0.294
0.448

MAP
0.258
0.409
0.423
0.310
0.382
0.230
0.202
0.427

Avg Improve
MAP
F2
-
-
7.74%
6.33%
13.84%
10.21%
-5.07%
-2.30%
4.83%
3.91%
-9.22%
-9.54%
-13.98% -12.02%
4.20%
6.55%

Avg Improve
F2
-

MAP
-

26.65% 26.81%
27.69% 29.87%
8.15%
6.74%
19.89% 21.43%
3.37%
0.81%
4.00%
-2.94%
19.65% 22.45%

=Baseline,

=Proposed transfer learning technique

(cid:35)

(cid:32)

TABLE 6: Performance of CSE models with transfer learning

Arch
Style
CLS
CSE
CSE
CSE

Trans
Learn
n/a
ADJ*
ADJ
PDA

CCHIT

PTC

Drone

CM1

F2
0.599
0.334
0.396
0.198

MAP
0.610
0.512
0.586
0.334

F2
0.404
0.283
0.312
0.173

MAP
0.703
0.778
0.820
0.510

F2
0.677
0.701
0.681
0.535

MAP
0.864
0.914
0.954
0.859

F2
0.503
0.518
0.511
0.548

MAP
0.628
0.713
0.713
0.674

Avg Improve
MAP
F2
-
-
-5.49%
-16.95%
-13.65%
0.00%
-34.05% -24.03%

PRE=Pretraining, PDA=Project Adaptation, and ADJ= Adjacent task, ADJ* uses general NLP tasks as adjacent task

CCHIT

PTC

Drone

CM1

Bert
GLUE-RANK
Task-RANK
Proj-RANK
PRE=Pretraining, PDA=Project Adaptation, and ADJ= Adjacent task, ADJ* uses general NLP tasks as adjacent task

(cid:35)
(cid:35)
(cid:32)
(cid:32)

F2
0.488
0.309
0.345
0.163

MAP
0.580
0.506
0.552
0.233

F2
0.541
0.611
0.592
0.510

MAP
0.716
0.812
0.829
0.672

F2
0.318
0.518
0.504
0.520

MAP
0.258
0.473
0.583
0.492

F2
0.284
0.311
0.336
0.228

MAP
0.211
0.237
0.289
0.203

Avg Improve
F2
-
12.23%
14.33%
-7.02%

MAP
-
24.20%
43.55%
5.26%

Arch
Style
CSE
CSE
CSE
CSE

Trans
Learn
n/a
ADJ*
ADJ
PDA

=Baseline,

=Proposed transfer learning technique

(cid:35)

(cid:32)

TABLE 7: Extra text-2-text tasks from GLUE dataset for
improving NLTrace performance at ﬁne-tuning stage

Name

Task
MNLI Multi-Genre Natural
Language Inference
MRPC Microsoft Research
Paraphrase

QNLI Question

Natural
Language Inference
Recognizing Textual
Entailment

RTE

STS-B Sentence
Similarity

Semantic

Description
Predicts whether S1 is entailed, neu-
tral or in contradiction. to S2
Determines whether S2 paraphrases
S1 without changing its meaning.
Determines whether S1 contains an
answer to question S2.
Binary classiﬁcation task predicting
whether S2 entails S1.
Evaluates whether two sentences are
semantically related.

S1 = First sentence; S2 = Second sentence

integrated their approach into our own CSE architecture,
naming it GLUE-RANK and adopting it as our RANK
model baseline. Further, we compared it against our own
Bert-CLS model (described in Section 6.1) as a CLS baseline.
As reported in Table 6, for the TLC task, none of the
CSE based models outperformed Bert-CLS; however, for the
TLX task both Task-RANK and GLUE-RANK signiﬁcantly
outperformed Bert-CLS. Task-RANK achieved a 43.55% im-
provement in MAP score for LTX, which is signiﬁcantly

higher than the 29.87% improvement in MAP achieved by
Task-RANK. These results suggest that the CSE architecture
has a stronger generalization ability than CLS and tends to
perform better in cases where the train and test data have
a larger distribution gap; whilst having a relatively weaker
ability to ﬁt the training data than the CLS architecture.

In summary, we observed that all three types of transfer
learning strategies were beneﬁcial in some way for im-
proving tracing performance. The datasets we collected,
including the Git corpus and Git links, as well as the Project
corpus, were generally more effective as knowledge sources
than their more general NLP counterparts.

Observation #5: Task-related knowledge transfer returned
marked improvements in trace accuracy, especially when
training tasks were performed using sentence matching
tasks trained on the artifacts connected by issue-commit
links.

Among the three strategies that we explored, task-level
transfer achieved the overall best performance evidenced by
the fact that Task-CLS and Task-RANK outperformed other
variants for each of the tasks.

Observation #6: Task-level transfer was more effective
than other transfer strategies that used domain-related
pretraining and adaptation of the LM.

6.3 RQ3: Can LM models outperform classical IR meth-
ods on the TLG task when a small number of training
examples are provided?

Our previous experiments focused on TLC and TLX tasks
for which a training set of links was available; however,
in this research question, we explored the TLG task for
which no training links were available (i.e., 0-shot). We
also investigated the potential improvement of providing
10 example trace links (i.e., 10-shot) for training purposes.
The analysis of more varied numbers of training links are
left for future work.

For models, such as VSM, which tune their parameters
based on text only, we gave access to all source and target
artifacts in the training dataset to conduct indexing and self-
supervised training. Based on the results of RQ2, we focused
on the Task-CLS and Task-RANK variants as they achieved
the best results for TLC and TLX experiments. For com-
parison purposes, we also included VSM, SciBert-CLS and
GLUE-RANK as representative baselines for information
retrieval, classiﬁcation, and contrastive sentence embedding
(CSE) techniques respectively. These were selected because
of their superior performance in our previous experiments.
Results are reported in Table. 8. In the 0-shot experi-
ments, the classical VSM model outperformed both SciBert-
CLS and GLUE-RANK. SciBert-CLS underperformed be-
cause it needs training data to tune its classiﬁcation net-
work. For GLUE-RANK, whilst we allowed it to use the
raw artifacts (without any links) to conduct self-supervised
learning, the self-supervision signal in the ranking task was
unable to compete with VSM’s results.

The Task-CLS however outperformed VSM by 26.59%
with respect to average F2 but exhibited a loss in MAP of
7.08%. While these results are slightly mixed, they suggest
that the issue-commit links played a role in the task-based
transfer by improving trace link accuracy even when no
training links were available. The simultaneous gain in F2
but loss in MAP indicates that more of the targeted links
achieved similarity scores above the prescribed threshold;
but that they were not ranked sufﬁciently high in the or-
dered list to improve MAP. The Task-RANK model, which
uses the Issue-Commit links as a knowledge source, also
outperformed VSM by 4.33% of F2 on average and further
supports our ﬁndings.

Results for the 10-shot showed that both SciBert-CLS and
GLUE-RANK beneﬁted from even a small set of training ex-
amples; however, average results for SciBert-CLS were still
lower than VSM by approximately 32.32% (F2) and 49.22%
(MAP). In contrast, GLUE-RANK outperformed VSM by
11.69% (F2) and 0.98% (MAP). In the case of Proj-RANK,
providing ten examples improved performance on CCHIT,
Drone, and PTC, but not on CM1. In fact, the 10-shot
results for CM1 were worse than the 0-shot results! This was
likely due to the complexity of the domain as well as the
underlying architecture. CSE architectures use a negative
selection mechanism, meaning that when the CSE model
creates in-batch negative examples, it pairs source and target

13

artifacts within the training batch to create a pool of negative
links. However, this approach can incorrectly label positive
links as negative ones, introducing noise into the training
data, and resulting in lowered performance. This effect is
more likely to occur in small projects such as CMI.

In contrast, Task-CLS dramatically beneﬁted from the
ten training examples that were provided. The difference
in F2 scores for Task-CLS versus VSM was approximately
equivalent for 0-shot and 10-shot (i.e., 26.59% vs. 28.58%);
however, with 0-shot, the difference in MAP scores for Task-
CLS versus VSM was negative (i.e., -7.08%), while with 10-
shot they improved over VSM by 11.39%, making it ulti-
mately the overall best performing approach for TLG tasks.
Furthermore, whereas SciBert-CSL and Task-CLS, were both
able to achieve good results in resource rich tracing tasks
such as TLC and TLX tasks, the Task-CLS model performed
better on resource limited cases by leveraging similarities
between the downstream and the and adjacent tasks.

Observation #7: Transfer learning techniques that lever-
aged adjacent tasks returned mixed results when applied
to TLG. Results were best when it was used in conjunction
with the classical CLS architecture, where F2 scores im-
proved but MAP scores reduced in comparison to the VSM
baseline. However, when even 10 training examples were
provided, the combination of task-based transfer and the
classical (CLS) architecture returned improvements over
VSM.

6.4 RQ4: What is the overall best method for support-
ing all three NL tracing tasks?

Finally, based on observations from RQ1, we conclude that
general LM based trace models outperform both conven-
tional IR models and the RNN based model when applied
to the TLC and TLX tracing tasks. Based on results from
RQ2, all three transfer learning approaches evaluated in this
study improved the performance of the general LM based
trace model regardless of whether CLS or CSE architectures
were used. However, the best performance was achieved
when using adjacent tracing tasks within the CLS architec-
ture, where performance for both TLC and TLX improved
by more than 20% for both F2 and MAP scores. In RQ3,
we further evaluated Task-CLS and Task-RANK, as the two
best variants identiﬁed in RQ2, to explore their potential for
supporting TLG in cases where links needed to be generated
from scratch. Our results indicated that the Task-CLS model
was able to effectively use the knowledge learned from
adjacent tasks when few links were available for training
purposes. It outperformed VSM by 26.59% F2 when no
training examples were provided at all. However, when
humans provided even ten links as examples, the model
outperformed the VSM model with an increase of 28.58%
(F2) and 11.39% (MAP). Its performance was better than the
SciBert-CLS baseline, which was effective only in resource
rich scenarios (i.e., TLC, TLX). These results suggest that
Task-CLS is the overall best model in cases where a single
model is desirable to support all three tracing tasks of TLC,
TLE, and TLG.

TABLE 8: Performance of CLS and CSE models on 0 shot and 10 shots link generation problems. Average improvement
results (right hand side) represent comparisons to VSM (top row).

14

Generation-0
(TLG)
VSM
SciBert-CLS
GLUE-RANK
Task-CLS
Task-RANK

(cid:35)
(cid:35)
(cid:35)
(cid:32)
(cid:32)

Arch
Style
CSE
CLS
CSE
CLS
CSE

Trans
Learn
n/a
PRE
ADJ*
ADJ
ADJ

CCHIT

PTC

Drone

CM1

F2
0.166
0.099
0.177
0.335
0.228

MAP
0.162
0.101
0.202
0.229
0.203

F2
0.217
0.034
0.113
0.183
0.163

MAP
0.277
0.065
0.242
0.219
0.233

F2
0.562
0.108
0.324
0.493
0.510

MAP
0.698
0.077
0.502
0.312
0.672

F2
0.458
0.280
0.593
0.605
0.520

MAP
0.531
0.175
0.543
0.564
0.492

Avg Improve
F2
-

MAP
-

-61.00% -67.52%
-3.35%
-13.49%
26.59%
-7.08%
-0.34%
4.33%

Generation-10
SciBert-CLS
GLUE-RANK
Task-CLS
Task-RANK
PRE=Pretraining, PDA=Project Adaptation, and ADJ= Adjacent task, ADJ* uses general NLP tasks as adjacent task

-32.32% -49.22%
0.98%
11.69%
11.39%
28.58%
-1.31%
7.28%

PRE
ADJ*
ADJ
ADJ

0.140
0.332
0.273
0.253

0.075
0.150
0.250
0.209

0.428
0.692
0.603
0.688

0.239
0.488
0.562
0.403

0.065
0.198
0.183
0.149

0.182
0.259
0.337
0.276

0.355
0.491
0.532
0.508

0.310
0.511
0.604
0.475

CLS
CSE
CLS
CSE

(cid:35)
(cid:35)
(cid:32)
(cid:32)

Observation #8: The best overall performer across all
three tracing tasks (i.e., TLC, TLX, TLG) is Task-CLS which
utilizes transfer learning based on adjacent tasks (built
using commit-issue links) within the context of a CLS
architecture.

We also performed preliminary experiments to combine
the strategies into a single solution. we applied transfer
strategies in a sequential order that matched the natural or-
der of our pipeline by ﬁrst preparing the SE-BERT language
model and then conducting transfer learning on adjacent
tasks. However, neither of the three two-way combinations,
nor the three-way combination outperformed individual
strategies in our experiments. We therefore leave further
investigation of this open issue to future work as discussed
in the concluding section of this paper.

Observation #9: Combining the three knowledge transfer
techniques in a pipeline did not outperform the best indi-
vidual approach. Exploring combinations of techniques is
left as an open research question.

7 THREATS TO VALIDITY

Our study is impacted by three primary threats to validity.
First, due to the inclusion and exclusion criteria we estab-
lished for our study, along with the low availability of in-
dustrial project datasets, we evaluated our approach against
only four software projects. However, these were taken from
four different domains with three of them representing in-
dustrial or government projects, and the other representing
a large academic project deployed in the physical world and
developed by a diverse team of academic and professional
developers. Nevertheless, given this limitation we are not
able to more fully generalize our ﬁndings across other
domains or even across a broader set of projects with more
diverse terminology, templates, or styles of requirements
speciﬁcations.

Second, although our experiment datasets are quite large
in comparison to many previously published traceability pa-
pers that focus on traditional software engineering projects,
the project sizes are still relatively small in comparison to
many real-world software projects, and it is well known
that accuracy of tracing results are impacted negatively
by project size due to the ‘needle-in-the-haystack’ phe-
nomenon. This introduces the risk that accuracy might

be negatively impacted as the size of the project grows;
however, based on our observations in this study, and the
general behavior of deep learning techniques, we expect
larger project sizes to actually be beneﬁcial for the perfor-
mance of NLTrace as it will have more training examples
to localize the knowledge obtained during pre-training and
transfer learning.

Third, we relied upon the trace matrices provided by
each of the four project datasets to serve as training data
for TLC and TLC tracing tasks, and as answer sets for
evaluating results. However, any inaccuracies in these ma-
trices introduce noise for training purposes and also could
introduce inaccuracies in the metric results.

Finally, throughout our study we made strategic de-
cisions about which models to use for baselines, how to
build the domain and project corpora, and which adjacent
tasks to evaluate in our transfer learning processes. While
alternate approaches might unearth different ‘winners’, we
observed trends that conﬁrmed our conjecture that adjacent
tasks related to matching sentences and were therefore quite
similar in nature to the tracing task performed better than
other types of tasks. We leave further investigation of tasks
to future work, and release our data to facilitate further
studies.

8 RELATED WORK

The classiﬁcation and sentence embedding models are the
most commonly used architectures for automating the cre-
ation of trace links. Classical approaches built using infor-
mation retrieval techniques belong to the Sentence Embed-
ding category. Tracing models such as VSM [67], LDA [68]
and LSI [69] analyze the common terms that are shared
between source and target artifacts to determine the like-
link; however, these methods are
lihood of a potential
generally unable to produce accurate links on large scale
projects due to the semantic gap between artifacts [22]. To
address this problem, Liu et al. [70] proposed improved
VSM models that leverage concept relations from manually
created knowledge bases or automatically constructed word
embeddings. Researchers also explored machine learning
approaches, which mostly fall into the category of classi-
ﬁcation. For ML methods, feature engineering techniques
have been extensively utilized. Heuristic rules have been
applied to extract semantic relations between artifacts as

semantic features [71], and then ML models such as Random
Forest [72] and MaxEnt [73] models have been applied
to predict the artifact relevance based on these manually
extracted features. Although, these two approaches partially
mitigate the semantic gap, the resulting trace model can not
actually comprehend the meaning of artifacts. To address
this problem, Guo et al. proposed a deep learning tracing
model, referred to as TraceNN [22]. Their model applied
a bidirectional recurrent neural network [74] to encode the
semantic representation of unprocessed NL artifacts. It then
utilized a multi-layer perceptron (MLP) classiﬁcation net-
work [72] to predict the distance between two encoded arti-
facts. By taking the surrounding context around the words
into consideration they created a semantic representation
for a whole artifact. This contrasts with classical IR based
methods that do not consider this context.

Since DL approaches based-on LMs have delivered bet-
ter performance than RNN models on various NLP tasks,
Lin et al. proposed LM-based trace models to trace the
code change set in commits to issue discussions in open
source projects [59]. To allow the LM to understand the
grammar of programming languages they built their model
using the CodeBert LM [37] which was pre-trained on open
source code and documentation. Their results showed that
such LM-based models produced more accurate trace links
than Guo’s TraceNN approach. Our study further investi-
gates the effectiveness of applying LM-based approaches
to address the text-to-text tracing task instead of text-to-
code. Other LM based trace models, such as DeepMatcher
[64], used DistillBert as an alternative encoder of VSM
and RNN; however the output of DistillBert was directly
applied to Cosine Similarity without ﬁne-tuning. We argue
that this method is relatively weak because it only uses the
knowledge from the pretraining stage without the beneﬁt of
calibration with the target project through ﬁne-tuning.

Domain speciﬁc NLP tasks such as Name Entity Recog-
nition (NER) in a technical corpus [75], [76], Reference
Prediction across academic papers [77], and Construc-
tion/Expansion of a domain ontology [78], [79] also require
knowledge about terminology in the document. However,
LMs pre-trained on a general corpus does not include many
project-speciﬁc terms and therefore are not trained on the
full vocabulary of the downstream task. To address this
problem, researchers have created domain-speciﬁc LMs by
collecting a large corpus of data for a target domain, and
pre-trained a LM from scratch to include domain termi-
nology. Models such as SciBERT [56], BioBERT [54], Clini-
calBERT [55] and FinBERT [53] have been created to cover
the general science/academia, biology, clinical, and ﬁnance
domain. Tai et al. also proposed an enhanced pretraining
framework to reduce the training time for constructing a
domain LM [80]. Another branch of research explores the
continual reﬁnement of an existing LM to augment it with
domain knowledge. Rongali et al.
,
[82] have explored methods for augmenting the training
of a generic LM with an additional domain corpus. In
both approaches, a relatively large sized domain corpus
was collected from web scraping or using an additional
publicly accessible domain corpus, such as PubMed [83] or
Wikipedia. However, software projects cover a wide range
of domains and it is challenging to ﬁnd, or construct a sufﬁ-

, [81] and Sun et al.

ciently large corpus for each and every project, especially as
engineers may introduce new concepts for speciﬁc projects.
Our study explores pretraining of a LM by augmenting the
domain corpus with project artifacts.

15

9 CONCLUSION

In this study, we have proposed NLTrace as a technique
for completing, expanding and generating text-to-text trace
links. NLTrace leverages a LM as its underlying knowledge
base and aims to produce trace links with a high degree of
accuracy. Our experimental results show that NLTrace can
generally outperform classical IR trace models when a set
of training links are available that allow it to take advan-
tage of transfer learning techniques. Speciﬁcally, our work
compared three transfer learning strategies using different
knowledge sources. First, we collected data from GitHub
and formulated a corpus that was used to pretrain an LM
dedicated to software projects. Our results showed that this
improved the tracing performance in comparison to general
LMs, and performed particularly well when the project
belonged to a domain with an active GitHub community.
Second, we explored domain adaption of an LM by extend-
ing its pretraining using a project dedicated corpus that was
retrieved in an automated manner using a data pipeline
developed in this work. The corpus was built speciﬁcally
around the concepts found in the targeted project. Building
the corpus required signiﬁcantly less time and computing
resources than training a new LM from scratch, but still im-
proved over the performance of more generic domain LMs,
such as SciBert, that had previously been pre-trained using
tens of thousands of topical papers. Third, we mined the
AutoLinks from GitHub and used them to create a closely
adjacent tracing task which supported task-level transfer.
This strategy achieved the best performance overall, even
for the harder problem of generating links when little or no
training data was available (i.e., the TLG tracing task). In
our experiments, when given even 10 training examples, it
was able to outperform classical IR models.

While the results achieved in this work are not yet per-
fect, they represent signiﬁcant improvements over existing
tracing techniques. To put the results in perspective, based
on our previous experience of traceability in practice, we
estimate that for full industry adoption, we need to achieve
MAP and F2 scores with an accuracy of 0.8 or higher [84].
These results get our closer and additional improvements
are clearly achievable. Furthermore, the ever increasing
size of the training corpus and the subsequent knowledge
provided by baseline LMs, such as GPT-3 [85], suggests
that accuracy will continue to improve at a rapid pace, and
that effective automated traceability solutions are within
reach to support traceability in large and diverse industrial
applications.

This paper has provided an initial exploration of how
deep learning and transfer learning can be used to leverage
these baseline LMs and to address the requirements trace-
ability problem. However, there is still much research to
be performed, and the results and observations from this
study suggest several compelling future research directions.
These include the exploration of combining multiple trans-
fer learning strategies where different knowledge sources

can harmoniously collaborate to improve the downstream
tracing task through multi-task learning.

Another direction focuses on the model architecture.
The CSE architecture has already shown great potential for
software traceability though in our experiments it did not
outperform the CLS architecture. However, in comparison
to CLS, the CSE architecture is more scalable for larger
projects because it does not require a classiﬁcation network,
which creates an efﬁciency bottleneck for CLS by its need
to compute a similarity score using a formula such as the
simple Cosine Similarity. However, the existing SimCSE
framework is designed for sentence semantic alignment
in general NLP and has not been optimized for software
traceability. By improving the self-supervision heuristic and
negative sampling strategies, we hypothesize that such a
framework could potentially reduce the noise that is cur-
rently inherent to training examples and ultimately outper-
form CLS approaches.

as

and source

the
available

Science: All models

generated monitoring
review purposes
for

as
code,
infrastructures,
yet
not
(but
the following google
https://drive.google.com/drive/folders/

Open
well
are
advertised for public use) at
drive
1EPAtwWI8BBZVi-NQj7W5J549SnD69HHy?usp=sharing
We will move them to a permanently archived site upon
acceptance of this paper.

folder:

ACKNOWLEDGMENTS
The work described in this paper was partially funded by
the USA National Science Foundation under grant #CCF-
1901059.

REFERENCES

[1] O. C. Z. Gotel and A. Finkelstein, “An analysis of the requirements
IEEE Computer Society, 1994, pp.

traceability problem,” in ICRE.
94–101.

[2] O. Gotel and A. Finkelstein, “Contribution structures (require-
IEEE Computer Society, 1995, pp. 100–

ments artifacts),” in RE.
107.

[3] RTCA/EUROCAE, “DO-178B/ED-12B: Software considerations

in airborne systems and equipment certiﬁcation,” 2000.

[4] ECSS, “ECSS-E-40C: principles and requirements applicable to

space software engineering,” 2009.

[5] BEL-V, BfS, CSN, ISTec, ONR, SSM, STUK, “IEC 60880:2013: Li-
censing of safety critical software for nuclear reactors (common
position of seven european nuclear regulators and authorised
technical support organisations),” 2013.

[6] E. Bouillon, P. M¨ader, and I. Philippow, “A survey on usage sce-
narios for requirements traceability in practice,” in Requirements
Engineering: Foundation for Software Quality, ser. Lecture Notes
in Computer Science, J. Doerr and A. L. Opdahl, Eds.
Springer,
2013, vol. 7830, pp. 158–173.

[7] P. M¨ader, O. Gotel, and I. Philippow, “Motivation matters
in the traceability trenches,” in Proceedings 17th International
IEEE, 2009, pp. 143–
Conference on Requirements Engineering.
148.

[8] P. Rempel, P. M¨ader, T. Kuschke, and I. Philippow, “Requirements
traceability across organizational boundaries - a survey and tax-
onomy,” in Requirements Engineering: Foundation for Software
Quality, ser. Lecture Notes in Computer Science, J. Doerr and
A. Opdahl, Eds. Springer, 2013, vol. 7830, pp. 125–140.

[9] P. Rempel, P. M¨ader, and T. Kuschke, “An empirical study on
project-speciﬁc traceability strategies,” in Proceedings of the 21st
International Requirements Engineering Conference (RE13), Rio
de Janeiro, Brasil, July 2013, pp. 195–204.

[10] P. M¨ader, P. L. Jones, Y. Zhang, and J. Cleland-Huang, “Strategic
traceability for safety-critical projects,” IEEE Software, vol. 30,
no. 3, pp. 58–66, 2013.

16

[11] O. Gotel,

J. Hayes, A.

J. Cleland-Huang,

Zisman,
A. Egyed, P. Gr ¨unbacher, A. Dekhtyar, G. Antoniol,
J. Maletic, and P. M¨ader, “Traceability fundamentals,” in
Software and Systems Traceability, J. Cleland-Huang, O. Gotel,
and A. Zisman, Eds. Springer London, 2012, pp. 3–22. [Online].
Available: http://dx.doi.org/10.1007/978-1-4471-2239-5 1
[12] J. H. Hayes, A. Dekhtyar, and S. K. Sundaram, “Advancing
candidate link generation for requirements tracing: The study of
methods,” IEEE Trans. Softw. Eng., vol. 32, no. 1, pp. 4–19, 2006.

[13] A. D. Lucia, A. Marcus, R. Oliveto, and D. Poshyvanyk, “Infor-
mation retrieval methods for automated traceability recovery,” in
Software and Systems Traceability. Springer, 2012, pp. 71–98.
[14] J. Huffman Hayes, A. Dekhtyar, and S. K. Sundaram, “Advancing
candidate link generation for requirements tracing: The study of
methods,” IEEE Transactions on Software Engineering, vol. 32,
no. 1, pp. 4–19, 2006.

[15] A. Dekhtyar, J. Huffman Hayes, S. K. Sundaram, E. A. Holbrook,
and O. Dekhtyar, “Technique integration for requirements as-
sessment,” in 15th IEEE International Requirements Engineering
Conference (RE), 2007, pp. 141–150.

[16] H. U. Asuncion, A. Asuncion, and R. N. Taylor, “Software trace-
ability with topic modeling,” in 32nd ACM/IEEE International
Conference on Software Engineering (ICSE), 2010, pp. 95–104.
[17] A. De Lucia, F. Fasano, R. Oliveto, and G. Tortora, “Enhancing an
artefact management system with traceability recovery features,”
in 20th IEEE International Conference on Software Maintenance
(ICSM), 2004, pp. 306–315.

[18] J. Cleland-Huang, A. Czauderna, M. Gibiec, and J. Emenecker, “A
machine learning approach for tracing regulatory codes to product
speciﬁc requirements,” in ICSE (1). ACM, 2010, pp. 155–164.
[19] C. Mills, J. Escobar-Avila, and S. Haiduc, “Automatic traceability
maintenance via machine learning classiﬁcation,” in ICSME. IEEE
Computer Society, 2018, pp. 369–380.

[20] C. Mills and S. Haiduc, “A machine learning approach for de-
termining the validity of traceability links,” in ICSE (Companion
IEEE Computer Society, 2017, pp. 121–123.
Volume).

[21] G. Spanoudakis, A. S. d’Avila Garcez, and A. Zisman, “Revising
rules to capture requirements traceability relations: A machine
learning approach,” in SEKE, 2003, pp. 570–577.

[22] J. Guo,

J. Cheng, and J. Cleland-Huang, “Semantically en-
hanced software traceability using deep learning techniques,”
in 2017 IEEE/ACM 39th International Conference on Software
IEEE, 2017, pp. 3–14.
Engineering (ICSE).

[23] “Supervised learning,” Apr 2021.

[Online]. Available: https:

//en.wikipedia.org/wiki/Supervised learning

[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” arXiv preprint arXiv:1810.04805, 2018.

[25] J. Liu, Y. Lin, Z. Liu, and M. Sun, “Xqa: A cross-lingual open-
domain question answering dataset,” in Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics,
2019, pp. 2358–2368.

[26] J. Allan, J. Aslam, N. Belkin, C. Buckley, J. Callan, B. Croft, S. Du-
mais, N. Fuhr, D. Harman, D. J. Harper et al., “Challenges in in-
formation retrieval and language modeling: report of a workshop
held at the center for intelligent information retrieval, university of
massachusetts amherst, september 2002,” in ACM SIGIR Forum,
vol. 37, no. 1. ACM New York, NY, USA, 2003, pp. 31–47.
[27] J. Lin, Y. Liu, Q. Zeng, M. Jiang, and J. Cleland-Huang, “Traceabil-
ity transformed: Generating more accurate links with pre-trained
bert models,” in Proceedings of the 43rd International Conference
on Software Engineering, ICSE 2021, Spain, 2021.

[28] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis,

and
M. Brockschmidt, “CodeSearchNet challenge: Evaluating the
state of semantic code search,” arXiv preprint arXiv:1909.09436,
2019.

[29] S. C. of RTCA, “DO-178C, software considerations in airborne

systems and equipment certiﬁcation,” 2011.

[30] C. Comar, F. Gasperoni, and J. Ruiz, “Open-do: An open-source
initiative for the development of safety-critical software,” in
Systems Safety 2009. Incorporating the SaRS Annual Conference,
4th IET International Conference on.

IET, 2009, pp. 1–5.

[31] P. Farail, P. Goutillet, A. Canals, C. Le Camus, D. Sciamma,
P. Michel, X. Cr´egut, and M. Pantel, “The topcased project: a toolkit
in open source for critical aeronautic systems design,” Ingenieurs
de l’Automobile, vol. 1, no. 781, pp. 54–59, 2006.

[32] J. Lin, Y. Liu, and J. Cleland-Huang, “Information retrieval versus
deep learning approaches for generating traceability links in bilin-
gual projects,” Empirical Software Engineering, vol. 27, no. 1, pp.
1–33, 2022.

[33] J. Guo, M. Rahimi, J. Cleland-Huang, A. Rasin, J. H. Hayes, and
M. Vierhauser, “Cold-start software analytics,” in Proceedings
of
the 13th International Conference on Mining Software
Repositories, MSR 2016, Austin, TX, USA, May 14-22, 2016,
M. Kim, R. Robbes, and C. Bird, Eds. ACM, 2016, pp. 142–153.
[Online]. Available: https://doi.org/10.1145/2901739.2901740
[34] J. Cleland-Huang, O. Gotel, J. H. Hayes, P. M¨ader, and A. Zisman,
“Software traceability: trends and future directions,” in Proc. of
the on Future of Software Engineering, 2014, pp. 55–69. [Online].
Available: http://doi.acm.org/10.1145/2593882.2593891

[35] M. Rahimi and J. Cleland-Huang, “Evolving software trace
links between requirements and source code,” Empir. Softw.
Eng., vol. 23, no. 4, pp. 2198–2231, 2018. [Online]. Available:
https://doi.org/10.1007/s10664-017-9561-x

[36] M. Rahimi, W. Goss,

and J. Cleland-Huang,

“Evolving
requirements-to-code trace links across versions of a software
system,” in Int’l Conf. on Software Maintenance and Evolution,
2016, pp. 99–109. [Online]. Available: https://doi.org/10.1109/
ICSME.2016.57

[37] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong,
L. Shou, B. Qin, T. Liu, D. Jiang et al., “Codebert: A pre-trained
model for programming and natural languages,” arXiv preprint
arXiv:2002.08155, 2020.

[38] Y. Zhang and Q. Yang, “A survey on multi-task learning,” IEEE

Transactions on Knowledge and Data Engineering, 2021.

[39] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE
Transactions on knowledge and data engineering, vol. 22, no. 10,
pp. 1345–1359, 2009.

[40] “Autolinked

references

[Online].
https://docs.github.com/en/get-started/

urls.”

and

Available:
writing-on-github/working-with-advanced-formatting/
autolinked-references-and-urls

[41] “Github,” Apr 2022. [Online]. Available: https://en.wikipedia.

org/wiki/GitHub#cite note-9

[42] [Online]. Available: https://www.gharchive.org/
[43] “Webhook events and payloads.” [Online]. Available: https:
//docs.github.com/en/developers/webhooks-and-events/
webhooks/webhook-events-and-payloads

[44] D. Thain, T. Tannenbaum, and M. Livny, “Distributed com-
puting in practice: the condor experience,” Concurrency and
computation: practice and experience, vol. 17, no. 2-4, pp. 323–
356, 2005.

[45] S. Bird and E. Loper, “Nltk: the natural language toolkit.” Asso-

ciation for Computational Linguistics, 2004.

[46] L. Han, A. L. Kashyap,

J. M. Tim Finin, and J. Weese,
“UMBC-EBIQUITY-CORE: Semantic Textual Similarity Systems,”
in Proceedings of the Second Joint Conference on Lexical and
Computational Semantics. Association for Computational Lin-
guistics, June 2013.

[47] “Loss functions.” [Online]. Available: https://ml-cheatsheet.

readthedocs.io/en/latest/loss functions.html

[48] T. Gao, X. Yao, and D. Chen, “Simcse: Simple contrastive learning

of sentence embeddings,” arXiv preprint arXiv:2104.08821, 2021.

[49] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple
framework for contrastive learning of visual representations,” in
International conference on machine learning. PMLR, 2020, pp.
1597–1607.

[50] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,
M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A ro-
bustly optimized bert pretraining approach,” arXiv preprint
arXiv:1907.11692, 2019.

[51] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Tor-
ralba, and S. Fidler, “Aligning books and movies: Towards story-
like visual explanations by watching movies and reading books,”
in Proceedings of the IEEE international conference on computer
vision, 2015, pp. 19–27.

[52] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,
“Glue: A multi-task benchmark and analysis platform for natural
language understanding,” arXiv preprint arXiv:1804.07461, 2018.
[53] D. Araci, “Finbert: Financial sentiment analysis with pre-trained

language models,” arXiv preprint arXiv:1908.10063, 2019.

[54] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,
“Biobert: a pre-trained biomedical language representation model

17

for biomedical text mining,” Bioinformatics, vol. 36, no. 4, pp.
1234–1240, 2020.

[55] K. Huang, J. Altosaar, and R. Ranganath, “Clinicalbert: Modeling
clinical notes and predicting hospital readmission,” arXiv preprint
arXiv:1904.05342, 2019.

[56] I. Beltagy, K. Lo, and A. Cohan, “Scibert: A pretrained language

model for scientiﬁc text,” arXiv preprint arXiv:1903.10676, 2019.

[57] S. Gururangan, A. Marasovi´c, S. Swayamdipta, K. Lo, I. Belt-
agy, D. Downey, and N. A. Smith, “Don’t stop pretraining:
adapt language models to domains and tasks,” arXiv preprint
arXiv:2004.10964, 2020.

[58] “Bert-base-uncased · hugging face.” [Online]. Available: https:

//huggingface.co/bert-base-uncased

[59] J. Lin, Y. Liu, Q. Zeng, M. Jiang, and J. Cleland-Huang, “Traceabil-
ity transformed: Generating moreaccurate links with pre-trained
bert models,” arXiv preprint arXiv:2102.04411, 2021.

[60] J. Cleland-Huang, M. Vierhauser, and S. Bayley, “Dronology:
An incubator for cyber-physical system research,” arXiv preprint
arXiv:1804.02423, 2018.

[61] M. Rahimi and J. Cleland-Huang, “Evolving software trace links
between requirements and source code,” Empirical Software
Engineering, vol. 23, no. 4, pp. 2198–2231, 2018.

[62] T. Krismayer, R. Rabiser, and P. Gr ¨unbacher, “A constraint min-
ing approach to support monitoring cyber-physical systems,”
in International Conference on Advanced Information Systems
Engineering. Springer, 2019, pp. 659–674.

[63] T. Krismayer, P. Kronberger, R. Rabiser, and P. Gr ¨unbacher, “Sup-
porting the selection of constraints for requirements monitoring
from automatically mined constraint candidates,” in International
Working Conference on Requirements Engineering: Foundation
for Software Quality. Springer, 2019, pp. 193–208.

[64] M. Haering, C. Stanik, and W. Maalej, “Automatically matching
bug reports with related app reviews,” in 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE).
IEEE,
2021, pp. 970–981.

[65] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global
vectors for word representation,” in Empirical Methods in Natural
Language Processing (EMNLP), 2014, pp. 1532–1543. [Online].
Available: http://www.aclweb.org/anthology/D14-1162

[66] S. Lohar, S. Amornborvornwong, A. Zisman, and J. Cleland-
Huang, “Improving trace accuracy through data-driven conﬁgu-
ration and composition of tracing features,” in Proceedings of the
2013 9th Joint Meeting on Foundations of Software Engineering.
ACM, 2013, pp. 378–388.

[67] G. Salton, A. Wong, and C.-S. Yang, “A vector space model for
automatic indexing,” Communications of the ACM, vol. 18, no. 11,
pp. 613–620, 1975.

[68] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
the Journal of machine Learning research, vol. 3, pp. 993–1022,
2003.

[69] C. H. Papadimitriou, P. Raghavan, H. Tamaki, and S. Vempala,
“Latent semantic indexing: A probabilistic analysis,” Journal of
Computer and System Sciences, vol. 61, no. 2, pp. 217–235, 2000.

[70] Y. Liu, J. Lin, and J. Cleland-Huang, “Traceability support for
the 17th
multi-lingual software projects,” in Proceedings of
International Conference on Mining Software Repositories, 2020,
pp. 443–454.

[71] M. Rath, J. Rendall, J. L. Guo, J. Cleland-Huang, and P. M¨ader,
“Traceability in the wild: automatically augmenting incomplete
trace links,” in Proceedings of the 40th International Conference
on Software Engineering, 2018, pp. 834–845.

[72] M. Riedmiller and A. Lernen, “Multi layer perceptron,” Machine
Learning Lab Special Lecture, University of Freiburg, pp. 7–24,
2014.

[73] A. Berger, S. A. Della Pietra, and V. J. Della Pietra, “A maximum
entropy approach to natural language processing,” Computational
linguistics, vol. 22, no. 1, pp. 39–71, 1996.

[74] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural
networks,” IEEE transactions on Signal Processing, vol. 45, no. 11,
pp. 2673–2681, 1997.

[75] R. I. Do ˘gan, R. Leaman, and Z. Lu, “Ncbi disease corpus: a re-
source for disease name recognition and concept normalization,”
Journal of biomedical informatics, vol. 47, pp. 1–10, 2014.

[76] Y. Luan, L. He, M. Ostendorf, and H. Hajishirzi, “Multi-task
identiﬁcation of entities, relations, and coreference for scientiﬁc
knowledge graph construction,” arXiv preprint arXiv:1808.09602,
2018.

18

[77] S. Bird, R. Dale, B. J. Dorr, B. Gibson, M. T. Joseph, M.-Y. Kan,
D. Lee, B. Powley, D. R. Radev, and Y. F. Tan, “The ACL Anthology
Reference Corpus: A Reference Dataset for Bibliographic Research
in Computational Linguistics,” in Proc. of the 6th International
Conference on Language Resources and Evaluation Conference
(LREC’08), 2008, pp. 1755–1759.

[78] J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii, “Genia corpus—a se-
mantically annotated corpus for bio-textmining,” Bioinformatics,
vol. 19, no. suppl 1, pp. i180–i182, 2003.

[79] A. Cohan, W. Ammar, M. Van Zuylen, and F. Cady, “Structural
scaffolds for citation intent classiﬁcation in scientiﬁc publications,”
arXiv preprint arXiv:1904.01608, 2019.

[80] W. Tai, H. Kung, X. L. Dong, M. Comiter, and C.-F. Kuo, “exbert:
Extending pre-trained models with domain-speciﬁc vocabulary
under constrained training resources,” in Proceedings of the
2020 Conference on Empirical Methods in Natural Language
Processing: Findings, 2020, pp. 1433–1439.

[81] S. Rongali, A. Jagannatha, B. P. S. Rawat, and H. Yu, “Continual

domain-tuning for pretrained language models,” arXiv preprint
arXiv:2004.02288, 2020.

[82] Y. Sun, S. Wang, Y. Li, S. Feng, H. Tian, H. Wu, and H. Wang,
“Ernie 2.0: A continual pre-training framework for language un-
derstanding,” in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, vol. 34, no. 05, 2020, pp. 8968–8975.

[83] K. Canese and S. Weis, “Pubmed: the bibliographic database,” in
The NCBI Handbook [Internet]. 2nd edition. National Center for
Biotechnology Information (US), 2013.

[84] J. Cleland-Huang, B. Berenbach, S. Clark, R. Settimi, and
E. Romanova, “Best practices
for automated traceability,”
Computer, vol. 40, no. 6, pp. 27–35, 2007. [Online]. Available:
https://doi.org/10.1109/MC.2007.195

[85] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari-
wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al.,
“Language models are few-shot learners,” Advances in neural
information processing systems, vol. 33, pp. 1877–1901, 2020.

