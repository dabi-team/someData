RobotCore: An Open Architecture for Hardware Acceleration in ROS 2

V´ıctor Mayoral-Vilches1,2,3, Sabrina M. Neuman4, Brian Plancher4, Vijay Janapa Reddi4

2
2
0
2

y
a
M
8

]

O
R
.
s
c
[

1
v
9
2
9
3
0
.
5
0
2
2
:
v
i
X
r
a

Abstract— Hardware acceleration can revolutionize robotics,
enabling new applications by speeding up robot response
times while remaining power-efﬁcient. However, the diversity of
acceleration options makes it difﬁcult for roboticists to easily
deploy accelerated systems without expertise in each speciﬁc
hardware platform. In this work, we address this challenge with
RobotCore, an architecture to integrate hardware acceleration
in the widely-used ROS 2 robotics software framework. This
architecture is target-agnostic (supports edge, workstation, data
center, or cloud targets) and accelerator-agnostic (supports
both FPGAs and GPUs). It builds on top of the common
ROS 2 build system and tools and is easily portable across
different research and commercial solutions through a new
ﬁrmware layer. We also leverage the Linux Tracing Toolkit
next generation (LTTng) for low-overhead real-time tracing
and benchmarking. To demonstrate the acceleration enabled
by this architecture, we use it to deploy a ROS 2 perception
computational graph on a CPU and FPGA.

We employ our integrated tracing and benchmarking to an-
alyze bottlenecks, uncovering insights that guide us to improve
FPGA communication efﬁciency. In particular, we design an
intra-FPGA ROS 2 node communication queue to enable faster
data ﬂows, and use it in conjunction with FPGA-accelerated
nodes to achieve a 24.42% speedup over a CPU.

I. INTRODUCTION

Recent work has seen an explosion of specialized robotics
acceleration on nontraditional computing platforms such as
GPUs, FPGAs, and ASICs [1], [2], [3], [4], [5], [6], [7], [8],
[9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19],
[20], [21]. This has been sparked by the decline of Moore’s
Law and Dennard Scaling, which limits the performance of
traditional CPU computing, positioning hardware accelera-
tion as an emerging solution to achieve high performance
and power efﬁciency in robotics applications.

However, this increased diversity of computing platforms
leads to a dramatic growth in design space complexity that
makes it difﬁcult for users to easily deploy robotics applica-
tions on hardware accelerators without substantial expertise
in each speciﬁc accelerator platform. The Open Computing

This material is based upon work funded by Xilinx and supported by
the National Science Foundation under Grant 2030859 to the Computing
Research Association for the CIFellows Project. Any opinions, ﬁndings,
conclusions, or recommendations expressed in this material are those of the
authors and may not reﬂect those of the funding organizations.

1V´ıctor Mayoral-Vilches is with Acceleration Robotics, Ecuador 3, 1 I,

Vitoria, ´Alava, Spain victor@accelerationrobotics.com

2V´ıctor Mayoral-Vilches is with the System Security Group, Uni-
versit¨at Klagenfurt, Universit¨atsstr. 65-67 9020 Klagenfurt, Austria
v1mayoralv@edu.aau.at

3V´ıctor Mayoral-Vilches is with Alias Robotics, Venta de la Estrella 6,

pab 130, Vitoria 01006, Spain victor@aliasrobotics.com

4Sabrina M. Neuman, Brian Plancher, and Vijay Janapa Reddi are with
the John A. Paulson School of Engineering and Applied Sciences, Harvard
University, Cambridge, MA, USA. sneuman@seas.harvard.edu,
brian plancher@g.harvard.edu, vj@eecs.harvard.edu

Fig. 1. The open architecture for hardware acceleration in ROS 2 extends
the ROS 2 build system to seamlessly support vendor and platform-agnostic
deployment of robotics applications on accelerator hardware. The integrated
tracing and benchmarking infrastructure enables users to analyze the system
and make strategic design improvements to optimize performance.

Language (OpenCL) [22] is an effort to standardize hardware
acceleration under a common language, but
its adoption
across silicon vendors has been uneven and support for it
varies. As a result, current hardware acceleration usage is
often tied to a particular vendor’s solutions and platforms.
This not only impedes interoperability and reuse of acceler-
ation kernels, but presents yet another layer of complexity
that users must overcome while implementing robot systems.
To address this challenge, in this work we present Robot-
Core, an open architecture for hardware acceleration that
extends the Robot Operating System (ROS) [23], the de facto
standard for robot application development. ROS is widely
used by academia and industry, and early work has demon-
strated its potential for hardware-accelerated robotics appli-
cations [2], [3], [24]. We facilitate this emerging direction
by implementing a vendor and platform-agnostic abstraction
layer for hardware acceleration in robotics (Fig. 1). Starting
with a popular robotics API as the foundation, our ROS 2-
based acceleration architecture provides a common ground
for both academic researchers and silicon vendors alike to
develop specialized robotics acceleration kernels, and deploy
them for easy usage by a large, established user base.

Once roboticists can easily harness hardware accelera-
tion across multiple platforms, the next major challenge is
proﬁling and benchmarking the application. Benchmarking
is needed to determine the best mapping of the robotics
computational graph to the different hardware resources
available to optimize overall robot system performance. This
is a difﬁcult task, however, since every application is different

 
 
 
 
 
 
and deployment scenarios are widespread. Full end-to-end
system analysis is required to understand how different im-
plementation tradeoffs impact overall performance. To enable
this analysis, we demonstrate how to leverage prior work [25]
and benchmark our ROS hardware acceleration framework
with a low-overhead framework for real-time tracing based
on the Linux Tracing Toolkit next generation (LTTng) [26].
We demonstrate analysis of a case study deployment using
CPU and FPGA nodes for a simple perception pipeline.

Using our framework and benchmarking, we diagnose that
substantial latency bottlenecks in this computational graph
come from inter-node interactions across ROS 2 layers in
the CPU. We recognize this as an opportunity for design op-
timization in hardware accelerators, because interaction with
the CPU should not be necessary for dataﬂow between nodes
co-located on the same non-CPU platform (e.g., FPGA).

Based on the benchmarking analysis, we demonstrate
two novel separate paths toward hardware acceleration: (1)
kernel fusion, and (2) improved message passing. Kernel
fusion results in the highest speedup, an average of 26.96%,
but it requires manual redesign of the underlying kernels.
To avoid manual redesign entirely and improve design re-
use and portability, we alternatively develop an intra-FPGA
ROS 2 node communication queue that leverages AXI4-
Stream interfaces [27] and transfers data in a sequential
streaming manner directly between acceleration kernels. This
improves the overall inter-node performance in our compu-
tational graph by 24.42 % on average, while requiring no
change in the accelerated kernels. This design extends to
applications beyond our case study, since this same pattern
is valid for any ROS 2 inter- or intra-process communication,
and aligns with ROS 2 composition capabilities.

In summary, key contributions of this work are that we:
• Create a new open infrastructure to increase the per-
formance of robotics applications by enabling seamless
integration of hardware acceleration into ROS 2 that
is portable across accelerator platforms (e.g., FPGAs,
GPUs) and system deployments (e.g., edge devices,
workstations, data centers, and cloud);

• Expose insights into how to optimize overall system-
wide performance by introducing a low-overhead trac-
ing and benchmarking framework to analyze application
performance across ROS 2 computational graphs, laying
foundation to analyze mixed-platform systems (e.g.,
combinations of CPU and FPGA-based nodes); and
• Increase ROS 2 node-to-node dataﬂow performance
to achieve an average overall accelerator speedup of
24.42% over CPU in our experiments by designing
intra-FPGA ROS 2 node communication queues, based
on insights uncovered using our open acceleration in-
frastructure and low-overhead benchmarking on a case
study analysis of a simple perception graph.

The core components of our architecture are disclosed
under a commercially friendly open-source license and are
available and maintained at the ROS 2 Hardware Accel-
eration Working Group GitHub organization: https://
github.com/ros-acceleration.

II. BACKGROUND AND RELATED WORK

A. ROS and ROS 2

The Robot Operating System (ROS) is an open-source col-
lection of software frameworks and tools designed to provide
a structured communications layer for robotics applications
running on heterogenous computer hardware [28].

ROS applications are designed around event driven graphs
of Nodes which communicate through Messages on various
Topics, Services, and Actions. Each Node can be thought of
as a software process which applies an algorithm to the input
message and then broadcasts the resulting output message.
By managing all inter-Node communications across abstrac-
tion layers (e.g., rclcpp, rcl, rmw), ROS simpliﬁes the
robotic system deployment process and enables roboticists to
quickly develop and test new algorithms. ROS also provides
substantial infrastructure to facilitate the automatic building,
evaluation, and deployment of robotic systems, including
dependency managers, package managers, build systems and
tools, simulators, and visualizers.

ROS 2 is a re-design of ROS that modernizes and updates
all of its components while adhering to its core design prin-
ciples. ROS 2 provides a stronger partitioning of the commu-
nication middleware from the robotics logic, enabling more
ﬂexibility, scalability, and reliability [2]. ROS 2 also provides
an updated build system, ament, and a new universal build
tool, colcon. This provides a single simple interface for
managing the building and deployment of complete robotics
applications. Leveraging these tools, roboticists can write
new algorithms and rely on ROS 2 to handle all lower level
operations and middleware management.

B. Hardware Acceleration for ROS and ROS 2

There has been previous work that has focused on ways
to accelerate robotics applications by developing tools and
methodologies to help roboticists leverage hardware accel-
eration for selected ROS Nodes and to optimize the ROS
computational graph through adaptive computing [29], [30],
[31], [32], [33], [34], [35], [36], [37], [38], [39], [40],
[41], [42], [43], [24]. There has also been some work to
accelerate the scheduling and communication layers used by
ROS and ROS 2 [44], [45], [46], [47], [48], [49], [50], [51].
Unfortunately, the majority of these efforts assume an end-
user has substantial experience with embedded systems and
embedded hardware ﬂows, or is customized to a speciﬁc
hardware acceleration board or deployment scenario.

Our proposed open architecture takes a ROS-centric ap-
proach to integrate the hardware and embedded ﬂows directly
into the core ROS 2 ecosystem. This provides end-users with
a build and deployment experience for hardware accelerators
similar to the standard, non-accelerated ROS 2 experience.

III. AN OPEN ARCHITECTURE FOR HW ACCELERATION

Our open architecture (Fig. 2) extends the core ROS 2
build system and tools to provide platform-agnostic (i.e.,
supports edge, workstation, data center, or cloud targets),
technology-agnostic (i.e., supports FPGAs and GPUs), and

ROS 2 C++ stack

Our open architecture for hardware acceleration

image pipeline

userland

4 tracing

drivers
c

1
ament vitis

1
ament jetpack

...

libraries

e

a

Xilinx
Vitis

e2

e1

cloud

Nvidia
Jetpack

3 acceleration
firmware kv260

3 acceleration
firmware jetson

...

n
o
i
t
a
r
e
l
e
c
c
a
-
s
o
r
/

m
o
c
.
b
u
h
t
i
g

tooling

rclcpp

rcl

rmw

adapter

middleware

1 ament acceleration

2 colcon-acceleration
d

3 acceleration firmware

1 build system (ament)

2 build tools (colcon)

3 ﬁrmware

b

$ colcon build ---merge-install a b c d
$ source install/setup.bash

$ colcon build ---merge-install ---mixin kv260

---install-dir install-kv260
---build-dir build-kv260

e e1 e2

Fig. 2. An open architecture for hardware acceleration in ROS 2.
Our contributions are highlighted in green. The architecture include the following extensions to ROS 2: 1 extensions to the build system
(ament), 2 extensions to the build tools (colcon), 3 adds a new ﬁrmware pillar to workspaces simplifying the production and
deployment of acceleration kernels and 4 introduces a low-overhead real-time tracing and benchmarking approach based on the Linux
Tracing Toolking next generation (LTTng [26]). A walkthrough on the interaction amongst the ROS 2 packages in the open architecture
is also depicted. A ﬁrst invocation to colcon build a will prepare the ROS 2 workspace for hardware acceleration and deploy ﬁles
in the local overlay as illustrated by b , c and d . After that, from the overlay, a second invocation of colcon build with the
--mixin <target-board> ﬂag e will cross-compile e1 and produce the accelerators e2 as needed for the target accelerator. From
this point on, the colcon acceleration package helps further automate additional processes.

portable hardware-accelerated ROS 2 capabilities for roboti-
cists. We: 1 extend the ROS 2 build system, ament; 2 ex-
tend the ROS 2 meta build tool, colcon; and 3 develop
integrated ROS 2 ﬁrmware extensions. We also 4 integrate
a low-overhead tracing and benchmarking framework to
enable the analysis of holistic application performance across
ROS graphs. This section describes these extensions in detail.

A. Extending the ROS 2 Build System - 1

The ﬁrst pillar of our open architecture allows roboticists
to generate acceleration kernels directly from the ROS 2
build system (ament) in the same way they generate CPU
binaries. To do so, the ament acceleration ROS 2
package and its extensions abstract the ROS build system
from vendor-speciﬁc accelerators (e.g. FPGAs or GPUs),
including their frameworks and software platforms. This al-
lows the build system to easily support hardware acceleration
across commercial solutions while using the same syntax,
simplifying the work of ROS 2 package maintainers.

Under

the hood, each hardware-speciﬁc extension of
ament acceleration abstracts away the corresponding
vendor-speciﬁc ﬁrmware. For example, ament vitis1 re-
lies on the proprietary Xilinx Vitis [52] and on the Xilinx
Runtime (XRT) library [53]. This simpliﬁes the creation
of acceleration kernels and separates ﬁrmware concerns

from algorithm development. This way, robotics engineers
can focus on improving their computational graphs with a
ROS-centric development ﬂow. Separately, hardware experts,
potentially sponsored by silicon vendors, can improve accel-
eration kernels for a particular commercial solution. Overall,
these extensions help achieve the objective of simplifying the
creation and integration of acceleration kernels from different
vendors into ROS 2 computational graphs.

Fig. 2 depicts the build system extensions showing how
ament acceleration abstracts the build system from
vendor-speciﬁc solutions. As an example of an alternative
acceleration technology supported, ament jetpack is in-
cluded and illustrates the integration of Nvidia JetPack [54].

B. Extending the ROS 2 Build Tools - 2

The second pillar of our open architecture extends the
colcon ROS 2 meta build tool
to integrate hardware
acceleration ﬂows into the ROS 2 Command Line Interface
(CLI) commands. Examples of these extensions include the
selection of the target accelerator and build-time through
mixins, emulation capabilities to speed-up the development
process and facilitate design without access to the real hard-
ware, raw disk image production tools, and simpliﬁed con-
ﬁguration of hypervisors. These extensions are implemented
by the colcon-acceleration2 ROS 2 package. As

1github.com/ros-acceleration/ament vitis

2github.com/ros-acceleration/colcon-acceleration

in Section III-A, colcon acceleration further enables
roboticists to leverage hardware accelerators while using the
same commands and ﬂows to build their applications.

C. Adding Firmware Extensions - 3

Represented by the abstract acceleration firmware
ROS package and its corresponding specializations (e.g.
acceleration firmware kv2603 for the Xilinx Kria
KV260 board),
the third pillar of our open architecture,
ﬁrmware extensions, are meant to provide ﬁrmware artifacts
for each supported technology solution. This again simpliﬁes
the process for ROS package consumers and maintainers,
and further aligns hardware acceleration workﬂows with
typical ROS development ﬂows. Each ROS 2 workspace
can leverage multiple ﬁrmware packages, but can only use
one at a time. As colcon acceleration supports the
selection of the active ﬁrmware in the ROS workspace, by
separating the ﬁrmware out into their own packages, our
open architecture enables silicon vendors to maintain an
acceleration firmware <solution> ROS package
that automatically integrates into standard ROS 2 workﬂows.

D. Low-Overhead Real-Time Tracing & Benchmarking - 4

In the context of hardware acceleration in robotics, it
is fundamental to be able to inspect performance improve-
ments. To that end, it is important to benchmark and trace the
system. Benchmarking is the process of running a computer
program to assess its relative performance, whereas tracing is
a technique used to understand what is happening in a system
while it is running. Tracing helps determine which pieces of
a Node are consuming more compute cycles or generating
indeterminism, and are thereby good candidates for hard-
ware acceleration. Benchmarking instead helps investigate
the relative performance of an acceleration kernel versus
its CPU scalar computing baseline. Similarly, benchmarking
also helps with comparing acceleration kernels across dif-
ferent hardware acceleration technology solutions (e.g., Kria
KV260 vs. Jetson Nano) and across kernel implementations
within the same hardware acceleration technology solution.
In order to trace and evaluate the relative performance of
both ROS 2 individual Nodes and complete computational
graphs, we leverage Linux Tracing Toolkit next generation
(LTTng [26]) for tracing and benchmarking. Building upon
prior work [25], LTTng provides a collection of ﬂexible
tracing tools and multipurpose instrumentation for ROS 2
that allow collecting runtime execution information in real-
time in distributed systems using low-overhead tracers. In-
strumentation probes introduced in ROS 2 Nodes can be
enabled or disabled at build-time. When enabling all ROS 2
instrumentation, end-to-end message latency overhead is
below 5.5us [25], making it suitable for a wide variety of
hardware acceleration use cases. This infrastructure also lays
a foundation for future integration with platform-speciﬁc
performance counters and tracing tools that can extend
analysis to more ﬁne-grained introspection and proﬁling of
the kernels running onboard an accelerator device.

3github.com/ros-acceleration/acceleration ﬁrmware kv260

Computational graph of our case study perception application,
Fig. 3.
image pipeline, containing two ROS 2 nodes: (1) RectifyNode sub-
scribes to the /camera/image raw and /camera/camera info top-
ics from Gazebo [55] and publishes a rectiﬁed image to (2) ResizeNode,
which publishes the ﬁnal resized image.

IV. ACCELERATING ROS 2 PERCEPTION

For our case study, we trace, benchmark, and accelerate a
subset of image pipeline [56], one of the most popular
packages in the ROS 2 ecosystem, and a core piece of the
ROS perception stack. We compose a simple computational
graph consisting of two nodes, resize and rectify, as
shown in Fig. 3. We then leverage our open architecture
for hardware acceleration (Section III) to benchmark, trace
and accelerate our computational graph, comparing a CPU
to an FPGA implementation. In this section we describe the
methodology of our approach, and analyze our timing results.

A. Methodology

We propose the following steps to analyze a ROS 2
application and design appropriate acceleration: (i) instru-
ment both the core components of ROS 2 and the target
kernels; (ii) trace and benchmark the kernels on the CPU
to establish a baseline; (iii) develop a hardware accelerated
implementation on alternate hardware (e.g., GPU, FPGA);
and (iv) trace, benchmark against the CPU baseline, and
improve the accelerated implementation.

Following this methodology, in our case study we begin
by instrumenting both ROS 2 and our target kernels with
LTTng probes. Reusing past work and probes [25] allows us
to easily get a grasp of the dataﬂow interactions within rmw,
rcl, and rclcpp ROS 2 layers. We then also instrument
the ResizeNode and RectifyNode components of the
image pipeline package used in our case study. The
relevant tracepoints placed in our computational graph across
ROS 2 stack layers are listed in Fig. 4 and 5 (full list in Pull
Request 717 in the image pipeline repository [56]). On
the CPU, these tracepoints enable us to isolate the latency
of computation within a node from the time it takes ROS 2
to package and pass information between nodes.

In the following sections we report timing results from
using a Xilinx Kria® KV260 Vision AI Starter Kit [57],
which has an onboard integrated Quad-core Arm® Cortex®-
A53 CPU and an FPGA containing 256K System Logic
Cells and 1.2K DSP Slices. All benchmark results report
the mean value obtained from a 60 second continuous run of
the computational graph. The FPGA kernels are synthesized,
placed and routed with a 250MHz clock.

Fig. 4. Tracepoints instrumented across ROS 2 abstraction layers on CPU for case study computational graph (Fig. 3). Breakdown summary in Fig. 5.

resize), harnessing our open architecture for implementation.
In Section IV-C.2, we then explore two different FPGA
designs to accelerate the computational graph by optimizing
dataﬂow interactions between FPGA-based nodes, address-
ing the ROS 2 communication infrastructure performance
bottleneck revealed by the CPU baseline in Section IV-B.

1) Accelerating Nodes & Components on an FPGA: We
ﬁrst accelerate the computations at each one of the graph
nodes. The RectifyNode and ResizeNode Components
of Fig. 3 are accelerated using Xilinx’s HLS, XRT, and
OpenCL targeting the Kria KV2604. Each ROS 2 Component
has an associated acceleration kernel5 that
leverages the
Vitis Vision Library, a computer vision library optimized
for Xilinx silicon solutions and based on OpenCV APIs.
These accelerated Components and their kernels seamlessly
integrate with the rest of the ROS meta-package through
our open architecture (Fig. 2), and are openly available to
the public. Building the accelerators is abstracted away from
roboticist end-users, and takes no signiﬁcant additional effort
than the standard build of the image pipeline.

After benchmarking the accelerated Components using the
trace points of Section IV-B, we observe an average 6.22%
speedup in the total computation time of the perception
pipeline when ofﬂoading tasks to the FPGA (see Fig. 6). For
this case study example, it is not surprising that accelerating
the computational nodes and components alone only gives
a modest performance increase because, as we saw in Sec-
tion IV-B, the performance bottleneck in the baseline CPU
system was communication overhead, not computation.

2) Accelerating the Computational Graph on an FPGA:
In our case study application, message-passing overheads
across the ROS 2 abstraction layers far outweigh other
operations, so in this section we focus on optimizing these
dataﬂows. Addressing performance bottlenecks in our system

4github.com/ros-acceleration/image pipeline/blob/ros2/image proc/src/

{rectify,resize} fpga.cpp

5github.com/ros-acceleration/image pipeline/tree/ros2/image proc/src/

image proc

Fig. 5. Breakdown of CPU runtime derived from tracing and benchmarking.
Total computation time of our case study graph is dominated by message
passing overheads, a bottleneck consuming over 73.3% of total runtime.

B. CPU-Only Tracing Results

Fig. 4 demonstrates the results of instrumenting and trac-
ing our target computational graph (Fig. 3) across multiple
ROS 2 stack layers on the CPU, and Fig. 5 summarizes the
breakdown of timing results across operations, establishing
the CPU baseline for our application. The breakdown in
Fig. 5 shows the time taken to do the computations within
each node, as well as the time taken by the ROS 2 lower-
level message-passing system across the various abstraction
layers. We ﬁnd that the message-passing overhead in our
application consumes more than 73.3% of the total time and
is therefore a large bottleneck in the total computation time of
the full graph. We next explore FPGA hardware acceleration
options, comparing performance to the CPU baseline.

C. Accelerating and Benchmarking CPU & FPGA

In this section, we explore hardware acceleration options
for an FPGA for our case study application (Fig. 3). In
Section IV-C.1, we ﬁrst explore hardware acceleration ker-
nels for the core logic of each of the Nodes (rectify and

Fig. 6.
Total runtime of CPU baseline and FPGA, FPGA-Integrated,
and FPGA-Streaming hardware-accelerated implementations of case study
application. Acceleration enables up to 26.96% speedup over CPU.

leads to overall lower computational graph latency, and to
faster robots. To seize this acceleration opportunity in our
case study example, we optimize the dataﬂow within the
computational graph and across ROS 2 Nodes and Com-
ponents through two different design approaches: (a) kernel
fusion, and (b) dedicated streaming queues.

The speedup obtained by integrating both ROS Compo-
nents on the FPGA into a single uniﬁed kernel is shown
in Fig. 6. The beneﬁts of doing this are two-fold. First, we
avoid any message-passing between the Rectify and Resize
Nodes’ Components. Second, we avoid the compute cycles
wasted while memory is mapped back and forth between
the host CPU and the FPGA. This results in an overall
latency speedup of 26.96% over the CPU. In addition to
speeding up the perception stage, another added beneﬁt of
this improvement is that such speedups make room for other
robot tasks in a complete end-to-end system. Note, however,
that this improvement required the construction of an entirely
new ROS Node and uniﬁed acceleration kernel on the FPGA.
The results of a different approach, developing an accel-
erated ROS 2 message passing interface on the FPGA, are
also shown in Fig. 6. This interface is Node and Component-
agnostic and can be leveraged by roboticists to accelerate the
communication channels of any computational graph on an
FPGA. This is done by leveraging an AXI4-Stream interface
to create an intra-FPGA ROS 2 communication queue which
is then used to pass data across Nodes in the FPGA without
sending messages to the CPU. This allows us to completely
bypass the original CPU-centric ROS 2 message-passing
system and optimizes dataﬂow, achieving an overall latency
improvement of 24.42% over the CPU in our application.

Based on these results, for this case study, we show
that implementing FPGA-accelerated versions of key ROS 2
Components is easily feasible, and that addressing the right
bottleneck is key to improving performance. Tracing and
benchmarking the CPU baseline suggested that communi-
cation is the bottleneck in our case study. In fact, indepen-
dent examination of, e.g., a single run of the fused-kernel
accelerator using a device-speciﬁc tool (Fig. 7), conﬁrms
that this is also the case on the FPGA—we note that inte-

Fig. 7. Screenshot thumbnail of single run of case study FPGA-Integrated
kernel accelerator using Xilinx Vitis Analyzer. Data transfer overheads
consume a substantial portion of runtime compared to kernel computation,
conﬁrming the communication bottleneck on the FPGA, similar to CPU.

grating device-speciﬁc proﬁling tools into our foundational
tracing infrastructure in future work can further automate
this type of ﬁne-grained introspection of kernels onboard
accelerator devices. We can achieve overall performance
improvements by either combining Nodes or streamlining
intra-FPGA communication. While combining nodes may
result in slightly higher performance, it is a much more labor-
intensive design effort. By contrast, our accelerated intra-
FPGA-Node communication queues can be easily applied
by any roboticist, to any computational graph.

V. CONCLUSION AND FUTURE WORK

In this work we present a new open infrastructure to
introduce hardware acceleration in ROS 2 in a scalable and
technology-agnostic manner. Our architecture allows us to
increase the performance of robotics applications through
seamless integration of hardware acceleration with ROS 2
APIs and its conventional ﬂows. We do so by extending
ROS 2 in a way that is portable across accelerator platforms
(e.g., FPGAs, GPUs) and system deployments (e.g., edge de-
vices, workstations, data centers, and cloud). We also present
a low-overhead tracing and benchmarking framework to
analyze performance across ROS 2 computational graphs.

We use our open architecture and our tracing and bench-
marking infrastructure to demonstrate a principled design
methodology for ROS 2 hardware acceleration, exposing in-
sights into how to optimize overall system-wide performance
by analyzing a CPU baseline, and comparing accelerator
design iterations to that original baseline. We examine a case
study using the Xilinx Kria KV260 platform to demonstrate
FPGA acceleration of one of the most popular packages in
the ROS perception pipeline: image pipeline. We ﬁrst
demonstrate a modest performance speedup of 6.22% from
ofﬂoading perception tasks to the FPGA, and then increased
speedup by additionally addressing the communication over-
heads that we identiﬁed as bottlenecks by analyzing our
CPU baseline. We achieved a speedup of 26.96% from re-
architecting the graph to combine nodes and avoid inter-
FPGA-node communication delays inﬂicted by interactions
with the CPU, but this approach requires substantial effort
from users to re-architect their graphs. Instead, to avoid this
overhead and stay in alignment with the ROS 2 programming

model, we then design a novel intra-FPGA ROS 2 Node com-
munication queue that allows ROS Nodes and Components
to deliver faster dataﬂows, achieving a 24.42% speedup over
a CPU without excessive manual per-kernel design effort.

We contribute our open architecture to the ROS com-
munity, so that future work can use our infrastructure and
extend to new applications beyond our case study example.
Promising directions for future work include: benchmarking
computational graphs with other hardware solutions (e.g.,
GPUs) to establish consistent cross-accelerator comparisons;
extending our LTTng tracing and benchmarking approach to
include additional tracing information (e.g., proﬁling within
FPGA or GPU devices) for more ﬁne-grained introspection
of kernels running onboard accelerators; and applying our
open architecture and analysis to other ROS 2 packages.

Our code is disclosed under a commercially friendly open-
source license and is available and maintained at the ROS 2
Hardware Acceleration Working Group GitHub organi-
zation: https://github.com/ros-acceleration.
This work is being further integrated into the ROS ecosystem
through a community standardization effort, REP-2008 [58].

REFERENCES

[1] Z. Wan, B. Yu, T. Y. Li, J. Tang, Y. Zhu, Y. Wang, A. Raychowdhury,
and S. Liu, “A survey of fpga-based robotic computing,” IEEE Circuits
and Systems Magazine, vol. 21, no. 2, pp. 48–74, 2021.

[2] V. Mayoral-Vilches and G. Corradi, “Adaptive computing in robotics,
towards ros 2 software-deﬁned hardware,” Xilinx, WP537, 2021.
[3] V. Mayoral-Vilches, “Kria robotics stack, a ros 2-centric approach for

hardware acceleration in robotics,” Xilinx, WP540, 2021.

[4] S. Murray, W. Floyd-Jones, Y. Qi, D. J. Sorin, and G. D. Konidaris,
“Robot motion planning on a chip.” in Robotics: Science and Systems,
2016.

[5] S. Murray, W. Floyd-Jones, Y. Qi, G. Konidaris, and D. J. Sorin, “The
microarchitecture of a real-time robot motion planning accelerator,” in
2016 49th Annual IEEE/ACM International Symposium on Microar-
chitecture (MICRO).
IEEE, 2016, pp. 1–12.

[6] S. Murray, W. Floyd-Jones, G. Konidaris, and D. J. Sorin, “A
programmable architecture for robot motion planning acceleration,”
in 2019 IEEE 30th International Conference on Application-speciﬁc
Systems, Architectures and Processors (ASAP), vol. 2160.
IEEE,
2019, pp. 185–188.

[7] B. Plancher, S. M. Neuman, T. Bourgeat, S. Kuindersma, S. Devadas,
and V. J. Reddi, “Accelerating robot dynamics gradients on a cpu, gpu,
and fpga,” IEEE Robotics and Automation Letters, vol. 6, no. 2, pp.
2335–2342, 2021.

[8] S. M. Neuman, B. Plancher, T. Bourgeat, T. Tambe, S. Devadas,
and V. J. Reddi, “Robomorphic computing: a design methodology for
domain-speciﬁc accelerators parameterized by robot morphology,” in
Proceedings of the 26th ACM International Conference on Architec-
tural Support for Programming Languages and Operating Systems,
2021, pp. 674–686.

[9] B. Plancher, S. M. Neuman, R. Ghosal, S. Kuindersma, and V. J.
Reddi, “Grid: Gpu-accelerated rigid body dynamics with analytical
gradients,” in 2022 IEEE International Conference on Robotics and
Automation (ICRA).

IEEE, 2022.

[10] J. Austin, R. Corrales-Fatou, S. Wyetzner, and H. Lipson, “Ti-
tan: A parallel asynchronous library for multi-agent and soft-body
robotics using nvidia cuda,” in 2020 IEEE International Conference
on Robotics and Automation (ICRA).

IEEE, 2020, pp. 7754–7760.

[11] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and
O. Bachem, “Brax–a differentiable physics engine for large scale rigid
body simulation,” arXiv preprint arXiv:2106.13281, 2021.

[12] A. Suleiman, Z. Zhang, L. Carlone, S. Karaman, and V. Sze, “Navion:
A 2-mw fully integrated real-time visual-inertial odometry accelerator
for autonomous navigation of nano drones,” IEEE Journal of Solid-
State Circuits, vol. 54, no. 4, pp. 1106–1119, 2019.

[13] Y. Liu, C. E. Derman, G. Calderoni, and R. I. Bahar, “Hardware accel-
eration of robot scene perception algorithms,” in 2020 IEEE/ACM In-
ternational Conference On Computer Aided Design (ICCAD).
IEEE,
2020, pp. 1–8.

[14] B. Asgari, R. Hadidi, N. S. Ghaleshahi, and H. Kim, “Pisces: power-
aware implementation of slam by customizing efﬁcient sparse algebra,”
in 2020 57th ACM/IEEE Design Automation Conference (DAC).
IEEE, 2020, pp. 1–6.

[15] W. Liu, B. Yu, Y. Gan, Q. Liu, J. Tang, S. Liu, and Y. Zhu, “Archytas:
A framework for synthesizing and dynamically optimizing accelerators
for robotic localization,” in MICRO-54: 54th Annual IEEE/ACM
International Symposium on Microarchitecture, 2021, pp. 479–493.

[16] V. Mayoral-Vilches, A. Hern´andez, R. Kojcev, I. Muguruza, I. Zamal-
loa, A. Bilbao, and L. Usategi, “The shift in the robotics paradigm: The
hardware robot operating system (h-ros); an infrastructure to create
interoperable robot components,” in 2017 NASA/ESA Conference on
Adaptive Hardware and Systems (AHS), July 2017, pp. 229–236.
[17] G. Williams, A. Aldrich, and E. A. Theodorou, “Model predictive
path integral control: From theory to parallel computation,” Journal
of Guidance, Control, and Dynamics, vol. 40, no. 2, pp. 344–357,
2017.

[18] J. Sacks, D. Mahajan, R. C. Lawson, and H. Esmaeilzadeh, “Robox: an
end-to-end solution to accelerate autonomous control in robotics,” in
2018 ACM/IEEE 45th Annual International Symposium on Computer
Architecture (ISCA).
IEEE, 2018, pp. 479–490.

[19] B. Plancher and S. Kuindersma, “A performance analysis of parallel
differential dynamic programming on a gpu,” in International Work-
shop on the Algorithmic Foundations of Robotics (WAFR). Merida,
Mexico: Springer, Dec. 2018, pp. 656–672.

[20] ——, “Realtime model predictive control using parallel ddp on a gpu,”
in Toward Online Optimal Control of Dynamic Robots Workshop at the
2019 International Conference on Robotics and Automation (ICRA),
Montreal, Canada, May. 2019.

[21] K. Gupta, P. Z. X. Li, S. Karaman, and V. Sze, “Efﬁcient computation
of map-scale continuous mutual information on chip in real time,” in
2021 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS).

IEEE, 2021, pp. 6464–6470.
[22] A. Munshi, “The opencl speciﬁcation,” in 2009 IEEE Hot Chips 21
IEEE, 2009, pp. 1–314.

Symposium (HCS).

[23] M. Quigley, B. Gerkey, K. Conley, J. Faust, T. Foote, J. Leibs,
E. Berger, R. Wheeler, and A. Ng, “Ros: an open-source robot
operating system,” in Proc. of the IEEE Intl. Conf. on Robotics and
Automation (ICRA) Workshop on Open Source Robotics, Kobe, Japan,
May 2009.

[24] NVIDIA,

“Nvidia

isaac

ros,” Accessed

2022,

github.com/

NVIDIA-ISAAC-ROS.

[25] C. B´edard, I. L¨utkebohle, and M. Dagenais, “ros2 tracing: Multipur-
pose low-overhead framework for real-time tracing of ros 2,” Accessed
2022, gitlab.com/ros-tracing/ros2 tracing.

[26] M. Desnoyers and M. R. Dagenais, “The lttng tracer: A low impact
performance and behavior monitor for gnu/linux,” in OLS (Ottawa
Linux Symposium), vol. 2006. Citeseer, 2006, pp. 209–224.

[27] A. AMBA, “Axi4-stream protocol speciﬁcation,” Volume IHI A,

vol. 51, p. 4, 4.

[28] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,
R. Wheeler, and A. Y. Ng, “Ros: an open-source robot operating
system,” in ICRA workshop on open source software, vol. 3, no. 3.2.
Kobe, Japan, 2009, p. 5.

[29] K. Yamashina, T. Ohkawa, K. Ootsu, and T. Yokota, “Proposal of
ros-compliant fpga component for low-power robotic systems: case
study on image processing application,” 2nd International Workshop
on FPGAs for Software Programmers (FSP 2015), 2015.

[30] K. Yamashina, H. Kimura, T. Ohkawa, K. Ootsu, and T. Yokota,
“crecomp: Automated design tool for ros-compliant fpga compo-
nent,” in 2016 IEEE 10th International Symposium on Embedded
Multicore/Many-core Systems-on-Chip (MCSOC).
IEEE, 2016, pp.
138–145.

[31] A. Podlubne and D. G¨ohringer, “Fpga-ros: Methodology to augment
the robot operating system with fpga designs,” in 2019 International
Conference on ReConFigurable Computing and FPGAs (ReConFig).
IEEE, 2019, pp. 1–5.

[32] M. Eisoldt, S. Hinderink, M. Tassemeier, M. Flottmann, J. Vana,
T. Wiemann, J. Gaal, M. Rothmann, and M. Porrmann, “Reconfros:
Running ros on reconﬁgurable socs,” in Proceedings of the 2021

Drone Systems Engineering and Rapid Simulation and Performance
Evaluation: Methods and Tools Proceedings, 2021, pp. 16–21.
[33] C. Lienen, M. Platzner, and B. Rinner, “Reconros: Flexible hardware
acceleration for ros2 applications,” in 2020 International Conference
on Field-Programmable Technology (ICFPT).
IEEE, 2020, pp. 268–
276.

[34] D. P. Leal, M. Sugaya, H. Amano, and T. Ohkawa, “Automated
integration of high-level synthesis fpga modules with ros2 systems,”
in 2020 International Conference on Field-Programmable Technology
(ICFPT), 2020, pp. 292–293.

[35] T. Ohkawa, K. Yamashina, T. Matsumoto, K. Ootsu, and T. Yokota,
“Architecture exploration of
robot system using ros-
compliant fpga component,” in 2016 International Symposium on
Rapid System Prototyping (RSP).

IEEE, 2016, pp. 1–7.

intelligent

[36] S. Panadda, J. Nattha, P. L. Daniel, and O. Takeshi, “Low-power high-
performance intelligent camera framework ros-fpga node,” in Proceed-
ings of Asia Paciﬁc Conference on Robot IoT System Development and
Platform, no. 2020, 2021, pp. 73–74.

[37] J. P. Queralta, F. Yuhong, L. Salomaa, L. Qingqing, T. N. Gia, Z. Zou,
H. Tenhunen, and T. Westerlund, “Fpga-based architecture for a low-
cost 3d lidar design and implementation from multiple rotating 2d
lidars with ros,” in 2019 IEEE SENSORS, 2019, pp. 1–4.

[38] T. K. Maiti, “Ros on arm processor embedded with fpga for im-
provement of robotic computing,” in 2021 International Symposium
on Devices, Circuits and Systems (ISDCS), 2021, pp. 1–4.

[39] T. Ohkawa, K. Yamashina, H. Kimura, K. Ootsu, and T. Yokota,
“Fpga components for integrating fpgas into robot systems,” IEICE
TRANSACTIONS on Information and Systems, vol. 101, no. 2, pp.
363–375, 2018.

[40] D. P. Leal, M. Sugaya, H. Amano, and T. Ohkawa, “Fpga acceler-
ation of ros2-based reinforcement learning agents,” in 2020 Eighth
International Symposium on Computing and Networking Workshops
(CANDARW), 2020, pp. 106–112.

[41] H. Amano, H. Mori, A. Mizutani, T. Ono, Y. Yoshimoto, T. Ohkawa,
and H. Tamukoh, “A dataset generation for object recognition and a
tool for generating ros2 fpga node,” in 2021 International Conference
on Field-Programmable Technology (ICFPT).
IEEE, 2021, pp. 1–4.
[42] Y. Nitta, S. Tamura, and H. Takase, “A study on introducing fpga to ros
based autonomous driving system,” in 2018 International Conference
on Field-Programmable Technology (FPT). IEEE, 2018, pp. 421–424.
[43] K. E. Chen, Y. Liang, N. Jha, J. Ichnowski, M. Danielczuk, J. Gonza-
lez, J. Kubiatowicz, and K. Goldberg, “Fogros: An adaptive framework
for automating fog robotics deployment,” in 2021 IEEE 17th Interna-
tional Conference on Automation Science and Engineering (CASE).
IEEE, 2021, pp. 2035–2042.

[51] C. S. V. Guti´errez, L. U. S. Juan, I. Z. Ugarte, I. M. Goenaga,
L. A. Kirschgens, and V. Mayoral-Vilches, “Time synchronization in
modular collaborative robots,” arXiv preprint arXiv:1809.07295, 2018.

[44] Y. Sugata, T. Ohkawa, K. Ootsu, and T. Yokota, “Acceleration of
publish/subscribe messaging in ros-compliant fpga component,” in
Proceedings of the 8th International Symposium on Highly Efﬁcient
Accelerators and Reconﬁgurable Technologies, 2017, pp. 1–6.
[45] T. Ohkawa, Y. Sugata, H. Watanabe, N. Ogura, K. Ootsu, and
T. Yokota, “High level synthesis of ros protocol interpretation and
communication circuit for fpga,” in 2019 IEEE/ACM 2nd International
Workshop on Robotics Software Engineering (RoSE).
IEEE, 2019,
pp. 33–36.

[46] H. Choi, Y. Xiang, and H. Kim, “Picas: New design of priority-driven
chain-aware scheduling for ros2,” in 2021 IEEE 27th Real-Time and
Embedded Technology and Applications Symposium (RTAS).
IEEE,
2021, pp. 251–263.

[47] Y. Suzuki, T. Azumi, S. Kato, and N. Nishio, “Real-time ros exten-
sion on transparent cpu/gpu coordination mechanism,” in 2018 IEEE
21st International Symposium on Real-Time Distributed Computing
(ISORC).

IEEE, 2018, pp. 184–192.

[48] C. S. V. Guti´errez, L. U. S. Juan, I. Z. Ugarte, and V. Mayoral-
Vilches, “Time-sensitive networking for robotics,” arXiv preprint
arXiv:1804.07643, 2018.

[49] ——, “Real-time linux communications: an evaluation of the linux
communication stack for real-time robotic applications,” arXiv preprint
arXiv:1808.10821, 2018.

[50] ——, “Towards a distributed and real-time framework for robots: Eval-
uation of ros 2.0 communications for real-time robotic applications,”
arXiv preprint arXiv:1809.02595, 2018.

[52] Xilinx,

“Vitis

uniﬁed

software

platform,” Accessed

2022,

xilinx.com/support/download/index.html/content/xilinx/en/
downloadNav/vitis.html.

[53] ——, “Xilinx runtime (xrt),” Accessed 2022, github.com/Xilinx/XRT.
[54] NVIDIA, “Nvidia jetpack sdk,” Accessed 2022, developer.nvidia.com/

embedded/jetpack.

[55] N. Koenig and A. Howard, “Design and use paradigms for gazebo,
an open-source multi-robot simulator,” in Intelligent Robots and Sys-
tems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International
Conference on, vol. 3.

IEEE, 2004, pp. 2149–2154.

[56] P. Mihelich and J. Bowman, “image pipeline ros metapackage,” Ac-

cessed 2022, github.com/ros-perception/image pipeline.

[57] Xilinx, “Kria® kv260 vision ai starter,” Accessed 2022, xilinx.com/

products/som/kria/kv260-vision-starter-kit.html.

[58] ROS 2 Hardware Acceleration Working Group (HAWG), “Ros
enhancement proposal
ros 2 hardware accel-
(rep): Rep-2008 -
eration architecture and conventions,” Accessed 2022, github.com/
ros-infrastructure/rep/pull/324.

