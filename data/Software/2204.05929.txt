2
2
0
2

r
p
A
2
1

]
E
S
.
s
c
[

1
v
9
2
9
5
0
.
4
0
2
2
:
v
i
X
r
a

A Machine Learning Approach to Determine the Semantic
Versioning Type of npm Packages Releases

Rabe Abdalkareema, Md Atique Reza Chowdhuryb and Emad Shihabb

aSchool of Computer Science, Carleton University, Ottawa, Canada
bData-driven Analysis of Software (DAS) Lab, Concordia University, Montreal, Canada

A R T I C L E I N F O

Keywords:
npm Package Releases
Semantic Version
Mining Software Repository
Machine Learning

Abstract

Semantic versioning policy is widely used to indicate the level of changes in a package release.
Unfortunately, there are many cases where developers do not respect the semantic versioning
policy, leading to the breakage of dependent applications. To reduce such cases, we proposed
using machine learning (ML) techniques to eÔ¨Äectively predict the new release type, i.e., patch,
minor, major, in order to properly determine the semantic versioning type. To perform our
prediction, we mined and used a number of features about a release, such as the complexity of the
changed code, change types, and development activities. We then used four ML classiÔ¨Åers. To
evaluate the performance of the proposed ML classiÔ¨Åers, we conducted an empirical study on 31
JavaScript packages containing a total of approximately 6,260 releases. We started by extracting
41 release-level features from historical data of packages‚Äô source code and repositories. Then,
we used four machine learning classiÔ¨Åers, namely XGBoost, Random Forest, Decision Tree, and
Logistic Regression. We found that the XGBoost classiÔ¨Åers performed the best achieving median
ROC-AUC values of 0.78, 0.69, and 0.74 for major, minor, and patch releases, respectively. We
also found that features related to the change types in a release are the best predictors group of
features in determining the semantic versioning type. Finally, we studied the generalizability of
determining the semantic versioning type by applying a cross-package validation. Our results
showed that the general classiÔ¨Åer achieved median ROC-AUC values of 0.76, 0.69, and 0.75 for
major, minor, and patch releases.

1. Introduction

Semantic versioning is a commonly used versioning approach to signal a change‚Äôs compatibility through version
numbers. Prior work showed that properly adapting semantic versioning increases developers‚Äô trust in their dependent
on packages and decreases the chance of facing backward compatibility breakage [58, 11]. Therefore, most language-
speciÔ¨Åc package managers encourage the use of semantic versioning (e.g., npm for JavaScript, Cargo for Rust, Gems for
Ruby, among others) [23, 24]. Likewise, some of the biggest software producers such as Microsoft, NetÔ¨Çix, Facebook,
and Google signiÔ¨Åcantly use semantic versioning to tag their new software releases [43, 54, 29]. In addition, a survey
with two thousand developers shows that developers heavily rely on semantic versioning to determine the version of
their projects‚Äô release type [9].

However, misuse of semantic versioning can cause many problems. Developers may incorrectly identify the
semantic versioning type and may tag a new release as minor or patch even though it introduces breaking changes,
especially for packages that are continuously releasing [11, 4]. One example of such a problem is in the context of the
web browser Firefox and the font selection library fontconÔ¨Åg [4]. At some point, the fontconÔ¨Åg‚Äôs developers decided
to change its implementation so that blank Ô¨Åle names would no longer be permitted. They chose to mark this change
as a minor release. However, this release of fontconÔ¨Åg caused Firefox to fail to render text for any application that used
that minor release. In addition, this issue of release tagging can be particularly problematic for oversized packages or
projects that receive many contributions and perform many changes in one release development duration. Therefor, this
problem can negatively aÔ¨Äect both the developers of the packages and software applications that directly or indirectly
depend on these packages [11, 58].

Due to the increased adoption of semantic versioning, most of the previous work focused on empirically studying
its usage and beneÔ¨Åts (e.g,. [11, 42, 70]). However, very few studies tried to improve the eÔ¨Éciency of applying the

rabe.abdalkareem@carleton.ca (R. Abdalkareem); mwdhu@encs.concordia.ca (M.A.R. Chowdhury);

eshihab@encs.concordia.ca (E. Shihab)

ORCID(s): 0000-0001-9914-5434 (R. Abdalkareem); 0000-0003-1285-9878 (E. Shihab)

Abdalkareem et al.: Preprint submitted to Elsevier

Page 1 of 19

 
 
 
 
 
 
semantic versioning in practice. More importantly, most of the prior studies took reactive approaches and tried to detect
breakage changes of a package after it was released through the use of source code analysis (e,g., [49, 58, 48, 71]).
Thus, we argue that prior approaches have two key limitations. First, they tackled the issue of wrongly tagged releases
after they are out and being integrated by others depending on applications. Second, they heavily relied on source
code analysis, which suÔ¨Äers from high false-positive rates and is incapable of detecting runtime changes, especially
for packages that are written in dynamic type language such as JavaScript [55, 5].

Therefore, the main goal of our work is to automatically determine the type of the new package release, i.e.,
patch, minor, and major. To do so, we proposed the use of machine learning (ML) techniques to predict the semantic
versioning type. We started by analyzing the npm package manager and selected 31 packages with 6,268 releases that
their developers properly use semantic versioning to tag their releases. We then analyzed the source code and mined the
development history of the studied packages, and extracted 41 features that are grouped into six dimensions, namely,
change types, development activities, complexity and code, time, dependency, and text dimensions. Next, we built four
diÔ¨Äerent machine learning classiÔ¨Åers, namely XGBoost, Random Forest, Decision Tree, and Logistic Regression, to
determine the semantic versioning type of the releases. Finally, to evaluate the eÔ¨Äectiveness of using the ML techniques,
we performed an empirical study to answer the following questions:

RQ1: Can we eÔ¨Äectively determine the semantic versioning type of a new package release? We built four diÔ¨Äerent
ML classiÔ¨Åers using 41 features extracted from packages‚Äô repositories and source code. We then compared their
performance to the baseline, which is the ZeroR classiÔ¨Åer. Our results showed that XGBoost classiÔ¨Åers achieved
average ROC-AUC values of 0.77, 0.69, and 0.74 (median = 0.78, 0.69, and 0.74) for major, minor, and patch releases,
respectively. In addition, this improvement equates to an average improvement of 1.58ùëã, 1.38ùëã, and 1.49ùëã by the
built classiÔ¨Åers when they were compared to our baseline for the major, minor, and patch releases.

Then, we examined the most important dimension of features used by the ML classiÔ¨Åers to determine the semantic
versioning type of a new package release in order to provide insights to practitioners as to what features best indicate
the new package release type. This led us to ask the question; RQ2: Which dimension of features are most important
in determining the semantic versioning type of a new package release? We built diÔ¨Äerent classiÔ¨Åers based on each
dimension of features and evaluated and compared their performance. Our results showed that change types (e,g.,
number of JavaScript Ô¨Åles added in a release.) and complexity of the source code of the release are the most important
dimension of features in determining the type of new release.

Lastly, to examine the generalizability of the proposed technique, we investigated the eÔ¨Äectiveness of the ML
techniques in determining the semantic versioning type of a new package release using cross-packages validation. In
particular, we asked the question; RQ3: How eÔ¨Äective are the machine learning techniques when applied on cross-
packages? We built general classiÔ¨Åers and evaluated their performance using cross-package validation. The results
showed that the classiÔ¨Åer achieves average ROC-AUC values of 0.74, 0.68, and 0.75 (median = 0.76, 0.69, and 0.75)
for major, minor, and patch releases. These results also showed that cross-package classiÔ¨Åers‚Äô performances correspond
to an average ROC-AUC improvement of 1.5ùëã, 1.4ùëã, and 1.5ùëã over our baseline.

In general, our work made the following key contributions:

1. We formulated the problem of predicting semantic versioning for JavaScript packages. To the best of our
knowledge, this is the Ô¨Årst work of using ML techniques to determine semantic versioning type for JavaScript
packages. We envision that our approach can be used to predict the releases that are likely to be breakage releases.

2. We proposed features that can be mined from JavaScript package repositories and source code to predict
semantic versioning type of a new package release. We used the proposed features to predict semantic versioning
accurately and studied the features that best indicate the semantic versioning type.

3. We performed an empirical study on 31 open-source JavaScript packages, and our experimental results showed
that the use of ML techniques can achieve an improvement over our baseline approach, which is the ZeroR
classiÔ¨Åer.

Structure of the paper: The remainder of the paper was organized as follows. Section 2 provided a background on
semantic versioning. We described our case study design in Section 3. We presented our case study results in Section 4.
The work related to our study was discussed in Section 5 and the threats to validity of our work is discussed in Section 6.
Finally, Section 7 concluded the paper.

Table 1
The selection steps of the studied JavaScript packages that are published on npm.

Selection Step

# Packages

Most starred packages
Packages without post- and pre- releases
Packages with more than 50 releases
Packages without breakage releases

100
96
77
36

2. Semantic Versioning

Since the primary goal of our work is to determine the semantic versioning type of a new npm package release, it
is essential Ô¨Årst to provide background on the concept of semantic versioning and how it is used to tag new package
releases.

Semantic Versioning is considered the de-facto versioning standard for many software ecosystems, including node
package manager (npm) and Python package index (PyPI), to name a few. Semantic Versioning was introduced by the
co-founder of GitHub, Tom Preston-Werner, in 2011. In our study, we focused on semantic versioning 2.0, which was
released in 2013 [56]. The purpose of semantic versioning is twofold. It Ô¨Årst allows package developers to communicate
the extent of backward-incompatible changes in their new releases to application dependents. Also, it allows for
dependents of a package to specify how restrictive or permissive they want to be in automatically accepting new
versions of the packages.

In general, semantic versioning proposes three dot-separated numbers indicating the major, minor, and patch
versions of a release. Those numbers assist in identifying the type of changes in the newly released package. To explain
how semantic versioning works, we take the release m1.n1.p1 number as an example. The Ô¨Årst part m1 presents the
major type, the number n1 stands for the minor type, and the number p1 stands for the patch type. The semantic
versioning also shows rules for developers to determine how one of the three types number should be incremented
when a new release comes out. In particular, any change to the new release package that is backward-incompatible
(e.g., break the API) requires an update to the major version. Thus, a major release must yield the increment of the
major version type, for example, from m1.n1.p1 to m2.n1.p1. A minor release should be published when some
new backward-compatible change is introduced (e.g., adding or supporting new functionality that does not create
backward incompatibility). A minor release must yield the increment of the minor type of the version number (e.g.,
from m2.n1.p1 to m2.n2.p1). Finally, a patch release should be published when the release represents backward
compatible Ô¨Åxes (e.g., Ô¨Åxing a bug). A patch release must yield the increment of the patch type of the version number,
such as from m2.n2.p1 to m2.n2.p2. In addition, there are some optional tags for example specifying pre-releases
type (e.g., 1.2.3-beta).

Although adopting the semantic version is not mandatory, prior studies showed that mainly packages in npm comply
with this speciÔ¨Åcation (e.g., [23, 37]). The mechanism to resolve a provided version relies on the precedence between
version numbers since npm needs to know if a particular version number is greater than, less than, or equal to another
version number. Similar to decimal numbers, semantic version numbers are compared initially by the magnitude of
their major type, then by their minor and Ô¨Ånally by patch types. For example, version 3.2.1 is lower than versions
4.0.0 (by a major), 3.3.1 (by a minor), and 3.2.2 (by a patch), but greater than versions 2.2.1 (by a major), 3.1.1
(by a minor), and 3.2.0 (by a patch).

While semantic versioning is a promising technique to specify the type of changes in a new package release, and
even though it is recommended by ecosystem maintainers [27], it is not always straightforward to be used in practice.
For example, a package developer can mistakenly Ô¨Çag the new release as a patch release while it is actually a major
release. Therefore, this mistake might lead to many problems, mainly breaking the applications that depend on this
package. In this paper, we formulated the determination of semantic versioning type of a new package release as a
research problem, which aimed to facilitate npm packages developers to Ô¨Ånd the right semantic versioning type for
their new release packages. As a result, this will increase the packages‚Äô trust and reduce the breaking of applications
that depend on those packages.

Table 2
Statistics of the studied JavaScript packages. The Table shows the name, number of commits, releases, analyzed releases,
percentage of major, minor, patch releases of the studied packages.

Package

Commits

Release

Analyzed %Major %Minor %Patch

renovate
turtle.io
sweetalert2
seek-style-guide
oui
react-isomorphic-render
reactive-di
module-deps
express-processimage
sku
bittorrent-dht
nightwatch-cucumber
socketcluster-server
eslint-conÔ¨Åg-canonical
patchbay
penseur
mongo-sql
pacote
octokit/routes
box-ui-elements
rtc-quickconnect
terrestris/react-geo
rtcpeerconnection
speakingurl
license-checker
octokit/Ô¨Åxtures
repofs
jsonrpc-bidirectional
nes
zapier-platform-cli
rtc-signaller
Mean
Median

5,226
1,110
1,924
579
722
977
625
492
595
340
633
634
282
360
2,031
210
511
615
645
1,329
661
2,846
311
429
377
378
574
511
370
1,003
546
898.30
595.00

2293
413
327
280
226
286
133
135
122
122
115
132
111
133
108
95
87
102
99
88
92
73
82
78
70
81
73
97
67
69
79
202.20
102.00

1156
294
266
222
207
176
107
104
102
101
97
97
94
90
87
81
78
77
77
72
72
69
67
66
65
64
63
62
61
61
60
138.50
81.00

0.61
2.38
2.63
10.81
4.35
5.68
6.54
5.77
7.84
5.94
8.25
9.28
12.77
14.44
6.90
8.64
7.69
10.39
15.58
9.72
9.72
11.59
8.96
19.70
35.38
12.50
11.11
11.29
14.75
11.48
10.00
10.09
9.72

23.44
8.16
20.68
39.19
5.31
6.82
8.41
30.77
39.22
31.68
38.14
21.65
27.66
22.22
43.68
50.62
12.82
20.78
29.87
52.78
47.22
46.38
26.87
28.79
18.46
51.56
23.81
40.32
34.43
27.87
41.67
29.72
28.79

75.95
89.46
76.69
50.00
90.34
87.50
85.05
63.46
52.94
62.38
53.61
69.07
59.57
63.33
49.43
40.74
79.49
68.83
54.55
37.50
43.06
42.03
64.18
51.52
46.15
35.94
65.08
48.39
50.82
60.66
48.33
60.20
59.57

3. Case Study Design

The main goal of our study is to automatically determine the semantic versioning type of a new release of a
JavaScript package. To achieve this goal, we proposed the use of machine learning techniques. We begin by selecting
JavaScript packages with a suÔ¨Écient number of releases, and their developers use semantic versioning to identify the
type of the new releases. Next, we used the selected npm packages as a labelled dataset. Then, we mined the source
code and development history of the selected JavaScript packages to extract release-level features and used them as
dependent variables in our machine learning classiÔ¨Åers. In the following subsections, we detail our labelled dataset,
data extraction and processing steps, and the training of our classiÔ¨Åers.

3.1. Test Dataset

To perform our study, we needed to obtain a number of JavaScript packages that follow semantic versioning
guidelines to mark their releases type. To build our labelled dataset, we started by looking at JavaScript packages
that are published on the Node Package Manager (npm). We chose npm package manager as it is the oÔ¨Écial registry
and repository for JavaScript packages.

To collect our dataset, we resorted to the public repository of npm that contains a list of all the published packages
on npm [52]. The npm repository contains metadata about every published package, such as the diÔ¨Äerent releases of a
package, the date of each release, and the release type. Since there are a large numbers of packages published on npm
and some of them did not provide high-quality packages [2], we had to apply Ô¨Åltration steps to select the packages that

Figure 1: Our approach of identifying the period release history on GitHub.

we wanted to study. We used four main criteria to ensure that our dataset contains high-quality packages. The summary
statistics of these steps are shown in Table 1.

The Ô¨Årst criterion in our selection process is to select mature and popular packages. To do so, we chose the top
100 npm packages in our dataset based on the number of stars they received on Github. We chose to use the number
of stars since prior work shows that the number of stars can provide a good proxy for the popularity and maturity of
software applications and packages [12, 22].

Second, we eliminated any packages from the dataset that contain at least one release that is labelled as pre-releases
or post-releases. We chose packages that do not have pre-releases or post-releases since this is a good indicator that the
developers of those packages are somehow familiar with the semantic versioning practices [23]. Also, we eliminated
those packages to simplify our classiÔ¨Åcations process since we would have only the three semantic versioning type as
labels in our dataset.

The third step to select the studied npm packages was to examine packages with a suÔ¨Écient number of releases.
We Ô¨Åltered out from our dataset any package that does not have at least Ô¨Åve releases of each type of the semantic
versioning, and in total, the package must have at least 50 releases. We excluded packages with a small number of
releases since we wanted to use ML techniques to determine the type of semantic versioning. Thus, we wanted to have
a suÔ¨Écient number of labelled releases so that we could build robust ML classiÔ¨Åers.

We Ô¨Ånally excluded packages that have any breakage releases identiÔ¨Åed by developers. It is important to note that
we performed this Ô¨Åltration step to ensure that the developers of our studied packages understand semantic versioning
and use it adequately in practice. Thus, we had a high-quality labelled dataset. To examine this criterion, for every
npm package in our dataset, we searched on Github for the applications that use these packages. Then, we analyzed
the development history of those applications. After that, we examined them to see whether the developers of those
applications that use the package had downgraded a version of that package and indicated that they performed the
downgrade due to a breakage in the release of the package. Mainly, we analyzed the historical data of these applications
and identiÔ¨Åed the commits where the developers rolled back a version of the selected packages. We then manually
examined those commits to determine if developers rolled back a version of the selected packages due to a breaking
release that is not correctly speciÔ¨Åed by the right semantic versioning tag. Finally, we removed any package from our
dataset containing at least one case of such a rollback. At the end of this step, we ended up having 36 packages in our
dataset.

3.2. Dataset Preparation

Once we decided which npm packages we would use in our study, we cloned them locally and collected their
metadata information from the npm registry. Then, we built a semantic versioning parser to analyze every sequence
release of every package to label the release type, whether a release is major, minor, or patch release based on the prior
release. For example, suppose a package has a release in an older date that holds the semantic versioning number as
3.2.6, and the subsequent release based on the date has the semantic versioning number as 3.3.6. In that case, we
considered that release as a minor release for that package (i.e., we labelled it as a minor release type). It is worth
mentioning that following this process, we were able to identify and eliminate any backport releases from our dataset.
In the next step and since we wanted to extract features based on the source code and the development history of
the packages‚Äô releases in our study, we needed to have the source code and the development history of each package in
our dataset. Therefore, for each package in our dataset, we started by collecting their metadata information and source
code from the public repository of npm. To do so, for each npm package in our dataset, we downloaded the appropriate

Changes1.1TimelineReleases1.22.02.12.21.0‚Äòtar‚Äô Ô¨Åle that contains the source code of every release of that package. In addition, we collected the release date for
every release of the packages and the GitHub repository URL of the packages.

Now, we had the source code of each release. Next, we wanted to collect the historical development data from
the GitHub repository of each package. We used the provided URL link to the GitHub repository to access the
development history. Then, we cloned the GitHub repository of each package and analyzed it. However, we could not
clone two package repositories because their GitHub repositories do not exist or are changed to private repositories.
In addition, based on our research experience with the npm registry, we noted that more than one npm packages could
be hosted on the same GitHub repository (i.e., they hosted in monorepo repository). Thus, we manually examined the
selected packages and remove three packages from our dataset that their GitHub repository contains more than one
npm packages.

Once we collected the release information from npm and GitHub repositories, we used a heuristic approach based
on the release date to link each release to its development history on the GitHub repository. Figure 1 shows the overall
approach. First, we analyzed the release date from the npm registry for each package release in our dataset. And then,
we extracted all the commits and their metadata. By analyzing the commits, we extracted the commit date. Based
on the release date, we identiÔ¨Åed the Ô¨Årst commit and the last commit for each release (i.e., we identiÔ¨Åed the release
timeframe). Now we had the source code and the development history of each package release in our dataset, we
analyzed these data to extract a comprehensive set of features. We describe our process for extracting the studied
features for npm packages in our dataset in the next section (Section 3.3).

Table 2 presents various statistics of our studied JavaScript packages from npm. It shows Ô¨Årst the name of the
package and the number of commits. In addition, the Table shows the total number of releases, the number of analyzed
releases of the studied packages, and the percentage of major, minor, and patch releases of the studied packages. In
total, there are 31 packages in our dataset.

3.3. Features for Semantic Versioning ClassiÔ¨Åcation

Since our goal is to perform release-level predictions to determine the semantic versioning type of a new package
release, we resorted to using some of the most commonly used release-level features. Some of these features were
used in prior software engineering tasks to identify post-release defects [63] or used to determine crashing releases of
mobile apps [74]. Therefore, we believed that some of these features can be used to determine the level of complexity
of a new package release, hence, providing useful information as to determine the type of a new release.

To perform our study of determining the semantic versioning type of a new release, we resorted to using release-
level features. In total, we extracted 41 features that are categorized into six dimensions. We distinguished between
these feature categories since; 1) it allowed us to observe the contribution of diÔ¨Äerent types of features, and 2) these
categories let us organize how we created and interpreted features related to determining the semantic versioning type.
In general, we extracted these features from analyzing the source code and the development activities of each new
package release in our dataset. Table 3 presents the names and the deÔ¨Ånition of the extracted features, and the rationale
for examining them. In the following subsections, we presented the detailed process of extracting the studied features
in each of the six dimensions.
Change Type Features: Change type features present the source code elements that may impact the semantic
versioning type of a new package release. To extract change type features, we resorted to using source code analysis to
calculate these features (described in Table 3). Thus, we analyzed the changes made after each release and extracted
Ô¨Åne-grained source code change types. To extract the features from code changes, we used the GumTree code
diÔ¨Äerencing algorithm [30]. GumTree takes as input the pair of revision Ô¨Åles and creates two Abstract Syntax Trees
(ASTs) that are used to compare those diÔ¨Äerent revisions. As a result, GumTree outputs a list of Ô¨Åne-grained source
code changes (e.g., an update in a method invocation or rename). Then, we wrote scripts that extract the Ô¨Åne-grained
source code change types based on the GumTree algorithm.

To extract change types features based on code that happened in each release, we needed to have the complete
version of the JavaScript Ô¨Åles before and after the release. To do so, we ran the diÔ¨Ä command line between two
consecutive releases. Then, we extracted all the JavaScript Ô¨Åles where the Ô¨Åles‚Äô names have a .js extension (i.e.,
JavaScript source Ô¨Åle). Once we had the two revisions of each changed Ô¨Åle in two consecutive releases, we ran the
GumTree tool on them. After that, we analyzed the results of GumTree to extract the change-type features. Since the
GumTree tool‚Äôs output is in a JSON format, we parsed the resulting JSON Ô¨Åles to retrieve the diÔ¨Äerences between the
before and after Ô¨Åles versions. Based on this step‚Äôs results, we counted the number of element changes in every two
revisions of Ô¨Åles and then summed up them to get a change type value for each release.

Dependencies Features: Dependency features present the dependencies change activities that occurred while develop-
ing a new package release. To calculate the dependency-related features, we analyzed the changes that happened to the
package.json Ô¨Åle. First, we analyzed the package.json Ô¨Åle since it is the conÔ¨Åguration Ô¨Åle used in the studied packages
to manage and conÔ¨Ågure dependencies. Then, we calculated the number of commits that touch the package.json Ô¨Åle
and the number of commits that added, deleted, updated packages in the package.json Ô¨Åle. We built a tool that analyzes
the package.json Ô¨Åle at every release and compares it with the previous releases to identify dependencies that were
changed.
Complexity and Code Features: Complexity and code features represent the package‚Äôs source code changes in each
release. To calculate the complexity and code features (e.g., the diÔ¨Äerence average of Cyclomatic and the total line of
code added and deleted) for each examined release in our dataset, we analyzed the release‚Äôs source code and computed
the diÔ¨Ä of the analyzed release with the previous releases. To achieve this, we ran the Understand tool [62] on every
release for the examined packages in our dataset and calculated the diÔ¨Äerence between the current release and the one
before.
Time Feature: The time feature presents the time that a new release takes to be developed and published. We counted
the number of days a new release takes to be published since the previous release date to calculate the time feature.
Development Features: Development features present the development activities performed during the development
of a new release of a package. To calculate the development features, we analyzed the GitHub repository of each
package in our dataset. Then we measured the number of commits, unique developers, open issues, closed pull requests,
and open pull requests that occurred during that release development timeframe.
Textual Features: Text features present extracted information from the commit change logs that the developers have
written during the development of a new release. To extract the text features, we analyzed the commit message and
looked for speciÔ¨Åc keywords, ‚Äúmajor‚Äù, ‚Äúpatch‚Äù, ‚Äúbreak‚Äù, and then counted the number of commits containing these
keywords in each release. As for the identify bug-Ô¨Åxing commits, we used a well-known approach that based on
examining the appearance of a pre-deÔ¨Åned set of keywords that include ‚Äúbug‚Äù, ‚ÄúÔ¨Åx‚Äù, ‚Äúdefect‚Äù, ‚Äúerror‚Äù, ‚Äúissue‚Äù, and
their variants in commit messages [64, 69]. Then, we counted those commits in every studied release.

3.4. ClassiÔ¨Åcation Algorithms

To perform our classiÔ¨Åcation task, we chose four diÔ¨Äerent machine learning algorithms. In particular, we chose to
use XGBoost (XGB), Random Forest (RF), Decision Tree (DT), and Logistic Regression (LR) algorithms to classify
whether a new package release is a major, minor, or patch. We resorted to using these ML algorithms since they 1) have
diÔ¨Äerent assumptions on the examined dataset, 2) show diÔ¨Äerent characteristics in terms of dealing with overÔ¨Åtting and
execution speed [18], and 3) provide an intuitive and straightforward explanation of the classiÔ¨Åcation, which enables
developers to easily understand why a decision to determine the type of package release was made [41]. In addition, they
have been commonly used in the past in other software engineering studies and datasets (e., g. [32, 38, 6, 73, 67, 36, 35]).
We then compared the performances of these diÔ¨Äerent supervised classiÔ¨Åers to determine the type of release. Now, we
brieÔ¨Çy described the four examined machine learning algorithms.
XGBoost (XGB): The XGBoost classiÔ¨Åer is an extended and innovative application of gradient boosting algorithm
proposed by Chen et al. [21]. Gradient boosting is an algorithm in which new models are created that predict the
residuals of prior models and then added together to make the Ô¨Ånal prediction. Models are added recursively until
no noticeable improvements can be detected. This approach supports both regression and classiÔ¨Åcation. XGBoost
has proven to push the limits of computing power for boosted tree algorithms. Furthermore, prior work showed that
applying the XGBoost classiÔ¨Åer on software engineering data produced good performance (e.g., [28, 46])
Random Forest (RF): The Random Forest classiÔ¨Åer is a type of combination approach, which is bagging and random
subsets meta classiÔ¨Åer based on a decision tree classiÔ¨Åer [15]. Random Forest combines multiple decision trees for
prediction. First, each decision tree is built based on the value of an independent set of random vectors. Then, the
Random Forest classiÔ¨Åer adopts the mode of the class labels output by individual trees. Also, prior work showed that
it performs well on software engineering problems (e.g., [59, 75]).
Decision Tree (DT): The decision trees classiÔ¨Åer Ô¨Årst creates a decision tree based on the feature values of the training
data where internal nodes denote the diÔ¨Äerent features [57]. The branches correspond to the value of a particular
feature, and the leaf nodes correspond to the classiÔ¨Åcation of the dependent variable. Then, the decision tree is made
recursively by identifying the feature(s) that discriminate the various instances most clearly, i.e., having the highest
information gain [34]. Once a decision tree is built, the classiÔ¨Åcation for a new instance is performed by checking the
respective features and their values.

Table 3
Features used to determine the semantic versioning type of a new npm package release.

Dim. Name

DeÔ¨Ånition

Rational

The number of JavaScript Ô¨Åles added between two releases.
AJF
The number of JavaScript Ô¨Åles modiÔ¨Åed between two releases.
MJF
The number of JavaScript Ô¨Åles deleted between two releases.
DJF
The number of non-JavaScript Ô¨Åles added between two releases.
ANJF
The number of non-JavaScript Ô¨Åles deleted between two releases.
DNJF
The number of non-JavaScript Ô¨Åles modiÔ¨Åed between two releases.
MNJF
The number of methods that are added between two releases.
ADM
DEM
The number of methods that are deleted between two releases.
MOM The number of methods that are moved between two releases.
MNC

The number of methods whose names are changed between two
releases.
The number of methods whose input parameters are changed
between two releases.
The number of methods whose input parameters are deleted between
two releases.
The number of logics in methods are added between two releases.
The number of logics in methods are moved between two releases.
The number of logics in methods are deleted between two releases.
The number of global variables added in JavaScript Ô¨Åles between
two releases.
The number of global variables deleted in JavaScript Ô¨Åles between
two releases.
The number of total code comments added between two releases.
The number of total code comments deleted between two releases.
The number of total code comments modiÔ¨Åed between two releases.
The number of changes to the package.json Ô¨Åle.
The
added
releases.
The number of used packages deleted between two releases.

packages

between

number

used

two

of

MPC

MPD

MLA
MLM
MLD
GVA

GVD

ICC
DCC
MCC
TCPJ
PA

PD

PU

The number of used packages‚Äô versions changed between two
releases.

ACYCD The diÔ¨Äerence average of Cyclomatic between two consecutive

releases.

CLCJD The diÔ¨Äerence of lines of code between two consecutive releases.

CYCD The diÔ¨Äerence Cyclomatic between two consecutive releases.
The total line of code added between two releases.
LA
The total line of code deleted between two releases.
LD

RDTD The timestamp diÔ¨Äerence between two consecutive releases.

TCM
TAU
POI
PCI
PCPR
POPR
NBF
KWM The total number of commits that have keyword major in commit

The total number of commits between two releases.
The total number of authors made changes between two releases.
The total number of open issue between two releases.
The total number of closed issue between two releases.
The total number of closed pull request between two releases.
The total number of open pull request between two releases.
The total number of bug-Ô¨Åxing commits between two releases.

KWP

KWB

AML

message in the release.
The total number of commits that have keyword patch in commit
message in the release.
The total number of commits that have keyword break in commit
message in the release.
The average commit message length in commits happened in the
release.

e
p
y
t

e
g
n
a
h
C

y
c
n
e
d
n
e
p
e
D

l

y
t
i
x
e
p
m
o
C

e
m
T

i

t
n
e
m
p
o
e
v
e
D

l

l

a
u
t
x
e
T

The releases that modify several JavaScript Ô¨Åles, functions or/and
change the code structure in npm packages tend to be more major
releases than being minor or patch releases. Furthermore, these are
change types that can provide good indications of the semantic ve-
rsioning type of a new npm package release. In other words, the re-
leases that include adding new JavaScript functionalities are not
small releases that are more likely to be major releases. For exam-
ple, if there are several JavaScript Ô¨Åles that are deleted in a new
package release, then that release is not expected to be a patch or
a minor release. Another example, If there are several non-JavaSc-
ript Ô¨Åles are changed (i.e., added, deleted, or modiÔ¨Åed) in a new
package release, then the release is likely to be a patch or a minor
release.

The releases that have more updates to the package dependencies
list are more likely not to be patch releases. For example, adding
more dependencies into the package dependencies list in the new
release can indicate that this release is a major release. Another
example, the changes that delete more dependencies in the new
release can indicate a major release rather than a minor or a patch
release.
We expect that the complexity and code features provide strong
indicators of the semantic versioning type of the new release. If
the complexity and the package size change a lot in the new release,
these changes will likely present the type of semantic versioning
release. For example, a large diÔ¨Ä number of lines between two
releases indicate that the new release introduces more code and is
more likely not to be a patch or a minor release.

A package release development that takes a long time tends to
contains several changes, which is not likely to be patch.
The semantic versioning type of a new package heavily de-
pends on the number of development activities in that rele-
ase. For example, many commits or many numbers of clos-
ed pull requests happened during the releases; this indicat-
es that this release is not a patch release but tends to be a
major or a minor package release.
The change message contains the purpose of this commit.
For example, commits that several messages contain the k-
eyword major changes or breakage changes in a release de-
velopment history provide a high indication that this relea-
se a major release. On the other hand, releases that have co-
mmits messages containing the word min-
or tend to be minor or patch releases.

Logistic Regression (LR): The Logistic Regression is used to estimate the probability of a binary response based on
one or more independent variables (i.e., features). Previous work showed that regression-based classiÔ¨Åers, especially
logistic regression, usually achieve high performance on software engineering classiÔ¨Åcation tasks (e.g., [32, 38]).

Baseline: Finally, to put our ML classiÔ¨Åcation results in perspective, we chose to use a simpler classiÔ¨Åer as a
baseline. In our study, we decided to use the ZeroR (ZR) classiÔ¨Åer, which is a primitive classiÔ¨Åer [13]. It basically
predicts the majority class in the training data for all cases in the test data without considering the independent features.

3.5. Training and Testing ClassiÔ¨Åers

To conduct our experiments and answer our research questions, we constructed an ML pipeline to build three
diÔ¨Äerent groups of classiÔ¨Åers. We Ô¨Årst built within-package classiÔ¨Åers where we used all the six dimensions of features
to train and test data from one package. Second, we built within-package classiÔ¨Åers for each package based on each
feature‚Äôs dimensions (i.e., for each package, we built six classiÔ¨Åers). Finally, we built cross-package classiÔ¨Åers, where
for each package, a cross-package classiÔ¨Åer is trained on data from all packages except one and tested on the remaining
one package.

Since, in our case, we have a multi-classes ML problem (e.g., as a major, minor, patch), we formalized our ML
problem to binary classiÔ¨Åcation problems. In another word, we used a one-versus-the-rest approach [50]. We used
one-versus-the-rest classiÔ¨Åers to ease the interpretation of our classiÔ¨Åers‚Äô outcomes. In our study, we built three one-
versus-the-rest classiÔ¨Åers for each new release type: a major release or not, a minor release or not, and a patch release
or not. Thus, this requires creating three diÔ¨Äerent ML classiÔ¨Åers and training each of them with true positives and true
negatives (e.g., true minor releases and not minor releases). Furthermore, to train and test our classiÔ¨Åers, we used the
5-fold cross-validation technique. In each 5-fold cross-validation, we divided the dataset into Ô¨Åve folds. Then, four
folds are used to train the classiÔ¨Åer, while the remaining one fold is used to evaluate the performance of the built
classiÔ¨Åer. This process is repeated Ô¨Åve times so that each fold is used exactly once as the testing set. We resorted
to using 5-fold cross-validation to reduce the bias due to random training data selection [8]. We Ô¨Ånally reported the
average performance across these test runs. The reported results are the average of 5-fold cross-validation, such that
each sample in the total dataset was included exactly in one test set. We implemented our examined classiÔ¨Åers using
scikit-learn [53]. We also used the default scikit-learn conÔ¨Åguration to set the diÔ¨Äerent parameters of the examined
classiÔ¨Åers.

Furthermore, and as it is shown in Table 2, our dataset has on average 10.09%, 29.72%, and 60.20% for major,
minor, and patch releases, which indicate that our dataset contains imbalances data. Data imbalance occurs when
one class occurs much more than the other in a dataset, which leads to the situation that the trained classiÔ¨Åers will
learn from the features aÔ¨Äecting the majority cases than the minority cases [65]. To deal with the imbalance problem
in our experiments, we applied the synthetic minority oversampling technique (SMOTE). SMOTE is a method for
oversampling and can eÔ¨Äectively boost a classiÔ¨Åer‚Äôs performance in an imbalanced case dataset such as our dataset [20].
We applied the sampling technique to our dataset since it balances the size of the majority class and allows us to report
standard performance and better interpret our results. It is essential to highlight that we only applied this sampling
technique to the training dataset. We did not re-sample the testing dataset since we want to evaluate our classiÔ¨Åer in a
real-life scenario, where the data might be imbalanced.

3.6. Performance Measures

To evaluate the performance of the used four machine learning classiÔ¨Åers and compare their performance to our
baseline, the ZeroR classiÔ¨Åer, we calculated the Area Under the Receiver Operating Characteristic curve (ROC-AUC).
ROC-AUC is a well-known evaluation measurement that is considered statistically consistent. In the ROC curve, the
true positive rate (TPR) is plotted as a function of the false positive rate (FPR) across all thresholds. More importantly,
ROC-AUC is a threshold independent measure [14]. A threshold represents the likelihood threshold for deciding an
instance that is classiÔ¨Åed as positive or negative. Usually, the threshold is set as 0.5, and other performance measures
for a classiÔ¨Åer, such as the F1-score, heavily depend on the threshold‚Äôs determination. However, some cases may need
to change the threshold, such as the class imbalance case. Thus, we used ROC-AUC to avoid the threshold setting
problem since ROC-AUC measures the classiÔ¨Åcation performance across all thresholds (i.e., from 0 to 1). Likewise,
ROC-AUC has the advantage of being robust towards class distributions [44, 51].

The ROC-AUC has a value between 0 and 1, where one indicates perfect classiÔ¨Åcations results and zero indicates
completely wrong classiÔ¨Åcations. It is important to note that prior work shows that achieving a 0.5 ROC-AUC value
indicates that the classiÔ¨Åer performance is as good as random, while the ROC-AUC value equal to or more than 0.7
indicates an acceptable classiÔ¨Åer performance using software engineering datasets [51, 44, 75].

Table 4
The performance of the examined four ML classiÔ¨Åers for determining the release type - major, minor, and patch. The results
are reported for XGBoost (XGB), Random Forest (RF), Decision Tree (DT), and Logistic Regression (LR). In addition, the
Table shows the results of our baseline classiÔ¨Åer, which is the ZeroR (ZR). The best performance values are highlighted in
bold.

Packages

XGB RF

Major
ZR

DT

LR

XGB RF

Minor
ZR

DT

LR

XGB RF

Patch
ZR

DT

LR

sweetalert2
renovate
speakingurl
license-checker
bittorrent-dht
nes
box-ui-elements
sku
mongo-sql
pacote
seek-style-guide
nightwatch-cucumber
zapier-platform-cli
patchbay
module-deps
turtle.io
rtcpeerconnection
react-isomorphic-render
rtc-quickconnect
terrestris/react-geo
eslint-conÔ¨Åg-canonical
repofs
penseur
octokit/routes
socketcluster-server
oui
express-processimage
octokit/Ô¨Åxtures
jsonrpc-bidirectional
reactive-di
rtc-signaller
Average
Median
Relative ROC-AUC

0.92
0.89
0.73
0.64
0.87
0.42
0.89
0.73
0.68
0.90
0.62
0.81
0.85
0.60
0.80
0.88
0.62
0.80
0.85
0.75
0.83
0.91
0.76
0.65
0.80
0.96
0.39
0.71
0.50
0.80
0.85
0.76
0.80

0.85
0.93
0.73
0.62
0.86
0.48
0.84
0.86
0.83
0.93
0.72
0.76
0.87
0.68
0.75
0.77
0.75
0.82
0.78
0.64
0.82
0.80
0.64
0.82
0.78
0.88
0.67
0.75
0.62
0.84
0.81
0.77
0.78
1.58ùëã 1.55ùëã ‚Äì

0.44
0.43
0.50
0.47
0.42
0.44
0.42
0.50
0.48
0.52
0.48
0.48
0.54
0.51
0.57
0.53
0.50
0.55
0.58
0.45
0.50
0.47
0.49
0.49
0.42
0.54
0.46
0.57
0.49
0.43
0.59
0.49
0.49

0.76
0.67
0.73
0.46
0.65
0.49
0.68
0.50
0.50
0.84
0.42
0.46
0.69
0.33
0.64
0.79
0.71
0.59
0.78
0.53
0.56
0.57
0.61
0.65
0.73
0.65
0.47
0.61
0.53
0.69
0.59
0.61
0.61

0.59
0.49
0.60
0.52
0.54
0.56
0.64
0.60
0.70
0.78
0.55
0.56
0.75
0.45
0.51
0.56
0.57
0.54
0.60
0.50
0.75
0.57
0.49
0.68
0.58
0.69
0.48
0.77
0.61
0.66
0.51
0.59
0.57
1.21ùëã 1.25ùëã 1.38ùëã 1.36ùëã ‚Äì

0.71
0.84
0.34
0.50
0.61
0.76
0.60
0.75
0.78
0.81
0.76
0.80
0.75
0.72
0.60
0.76
0.55
0.75
0.66
0.66
0.69
0.84
0.66
0.59
0.45
0.84
0.61
0.70
0.59
0.59
0.64
0.67
0.69

0.73
0.87
0.44
0.59
0.51
0.82
0.68
0.79
0.64
0.82
0.76
0.73
0.78
0.69
0.65
0.80
0.59
0.74
0.72
0.67
0.64
0.72
0.68
0.71
0.45
0.95
0.62
0.74
0.63
0.56
0.63
0.69
0.69

0.49
0.50
0.53
0.49
0.54
0.51
0.49
0.51
0.43
0.46
0.51
0.53
0.54
0.47
0.47
0.49
0.54
0.48
0.51
0.45
0.48
0.49
0.57
0.52
0.46
0.44
0.46
0.52
0.58
0.52
0.57
0.50
0.50

0.59
0.66
0.65
0.39
0.59
0.63
0.61
0.56
0.72
0.77
0.55
0.65
0.65
0.59
0.43
0.64
0.57
0.47
0.58
0.60
0.49
0.42
0.56
0.56
0.46
0.64
0.51
0.65
0.67
0.44
0.57
0.58
0.59

0.56
0.69
0.46
0.52
0.49
0.66
0.61
0.66
0.61
0.61
0.63
0.61
0.57
0.62
0.59
0.58
0.55
0.55
0.66
0.61
0.55
0.58
0.58
0.55
0.49
0.70
0.60
0.70
0.58
0.46
0.61
0.59
0.59
1.18ùëã 1.15ùëã 1.49ùëã 1.48ùëã ‚Äì

0.74
0.86
0.74
0.73
0.67
0.68
0.74
0.78
0.65
0.85
0.75
0.76
0.82
0.68
0.68
0.81
0.62
0.80
0.78
0.71
0.74
0.76
0.75
0.63
0.70
0.91
0.69
0.70
0.57
0.75
0.80
0.74
0.74

0.74
0.81
0.72
0.75
0.74
0.66
0.76
0.70
0.68
0.87
0.73
0.83
0.83
0.73
0.61
0.85
0.44
0.80
0.78
0.66
0.74
0.83
0.75
0.67
0.73
0.94
0.68
0.62
0.62
0.73
0.76
0.73
0.74

0.52
0.51
0.45
0.52
0.48
0.53
0.53
0.44
0.43
0.45
0.49
0.50
0.48
0.48
0.48
0.54
0.51
0.51
0.50
0.58
0.51
0.49
0.45
0.53
0.47
0.55
0.50
0.48
0.48
0.49
0.52
0.50
0.50

0.65
0.67
0.63
0.62
0.53
0.67
0.83
0.64
0.62
0.66
0.61
0.65
0.64
0.57
0.49
0.77
0.63
0.60
0.63
0.62
0.58
0.58
0.73
0.57
0.63
0.75
0.61
0.52
0.60
0.70
0.65
0.63
0.63

0.61
0.71
0.64
0.63
0.60
0.60
0.63
0.67
0.62
0.71
0.67
0.70
0.73
0.60
0.59
0.72
0.55
0.73
0.64
0.63
0.63
0.65
0.71
0.57
0.68
0.83
0.59
0.61
0.51
0.63
0.64
0.65
0.63
1.31ùëã 1.28ùëã

4. Case Study Results

In this section, we presented our case study results for our three research questions. For each research question, we

presented the motivation for the question, the approach to answering the question, and the results.

4.1. RQ1: Can we eÔ¨Äectively determine the semantic versioning type of a new package release?
Motivation: Prior work showed that determining the type of new package release is challenging [11]. Even though
prior work proposed techniques to detect semantic breaking API changes through static analysis for languages such
as Java [71, 58], such techniques require a clear deÔ¨Ånition of the public and private API. Such a distinction does not
explicitly exist in many dynamic languages such as JavaScript. In this question, we wanted to eÔ¨Äectively determine
the semantic versioning type of a new JavaScript package release. Therefore, automatically determining the type of
semantic versioning can help guide package maintainers on deciding the versioning type on a new release. In this RQ,
we aimed to examine the use of machine learning techniques.
Method: For each package in our dataset, we used the extracted 41 release-level features that are presented in Table 3
to train the four classiÔ¨Åers to determine whether a new package release is a major, minor, or patch release. We
had reformulated this classiÔ¨Åcation task into a one-versus-the-rest classiÔ¨Åcation problem since this is a multi-class

Table 5
Mann-Whitney Test (p-value) and CliÔ¨Ä‚Äôs Delta (d ) for the results of the four classiÔ¨Åers vs. the baseline classiÔ¨Åers for the
tree diÔ¨Äerent semantic versioning release types.

ML

XGB
RF
DT
LR

Major

Minor

p-value

d

p-value

d

Patch
d

p-value

7.973e-11
9.392e-09
3.077e-06
4.105e-05

0.96
0.85
0.69
0.61

1.061e-08
1.474e-08
3.382e-07
0.000254

0.85
0.84
0.75
0.54

1.468e-11
2.16e-10
4.802e-11
2.81e-10

0.99
0.94
0.97
0.93

classiÔ¨Åcation problem [50]. We used one-versus-the-rest classiÔ¨Åers since it would help us adequately interpret our
classiÔ¨Åers‚Äô results. We had a one-versus-the-rest classiÔ¨Åer for each new release type: a major release or not, a minor
release or not, and a patch release. Thus, we built three diÔ¨Äerent classiÔ¨Åers for each release type where the true positives
will be the examine release type (e.g., true minor releases and not minor releases).

After that, for each package, we used 5-fold cross validation [8]. First, we divided the dataset for each package
into Ô¨Åve folds. Then, we used four folds (i.e., 80% of the data) to train the four ML classiÔ¨Åers and used the remaining
one fold (i.e., 20% of the data) to evaluate the performance of the classiÔ¨Åers. We ran this process Ô¨Åve times for each
fold (i.e., 1x5-folds). In our study, we used the four ML classiÔ¨Åers described in Section 3.4 that are XGBoost, Random
Forest, Decision Tree, and Logistic Regression.

Finally, to evaluate and compare the performance of the four ML classiÔ¨Åers in determining the semantic versioning
type of a new package release, we computed the Area Under the Receiver Operating Characteristic curve (ROC-AUC).
Then, to come up with one value for the Ô¨Åve runs, we calculated the average of the evaluation measurement for 5-folds
Ô¨Åve times (i.e., 1x5-fold) for every package in our examined dataset.

Since one of the main goals of using machine learning techniques is to help determine the semantic versioning type
of new release, we measured how much better the performance of the four used classiÔ¨Åers is compared to the baseline
for each package. In our case, the baseline classiÔ¨Åer is a classiÔ¨Åer that always reports the class of interest based on the
majority, which is the ZeroR classiÔ¨Åer. In this case, the ZeroR classiÔ¨Åer will achieve 100% recall and precision equal
to the rate of examining release type (i.e., major, minor, patch). We followed the previously described process steps to
train and test the ZeroR classiÔ¨Åer.

Then, we compared the values of ROC-AUC for the four classiÔ¨Åers against the baseline by calculating the relative
ROC-AUC (i. e., ùëÖùëíùëôùëéùë°ùëñùë£ùëí ùëÖùëÇùê∂‚àíùê¥ùëà ùê∂ = ùê∏ùë•ùëéùëöùëñùëõùëíùëë ùê∂ùëôùëéùë†ùë†ùëñùëì ùëñùëíùëü ùëÖùëÇùê∂‚àíùê¥ùëà ùê∂
). Relative ROC-AUC shows how much better
our classiÔ¨Åers perform compared to the baseline. For instance, if a baseline achieves a ROC-AUC of 10%, while the
XGBoost classiÔ¨Åer, for example, achieves a ROC-AUC of 20%, then the relative ROC-AUC is 20
= 2ùëã. In other
10
words, the XGBoost classiÔ¨Åer performs twice as accurately as the baseline classiÔ¨Åer. It is important to note that the
higher the relative ROC-AUC value, the better the classiÔ¨Åer is in determining the semantic versioning type.

ùêµùëéùë†ùëíùëôùëñùëõùëí ùëÖùëÇùê∂‚àíùê¥ùëà ùê∂

Finally, to examine whether the achieved improvement over the baseline classiÔ¨Åer is statistically signiÔ¨Åcant, we
performed a non-parametric Mann-Whitney test [45] to compare the two distributions for each classiÔ¨Åer results in our
dataset and determine if the diÔ¨Äerence is statistically signiÔ¨Åcant, with a ùëù-value < 0.05 [45]. We also used CliÔ¨Ä‚Äôs Delta
(ùëë), a non-parametric eÔ¨Äect size measure to interpret the eÔ¨Äect size between the four classiÔ¨Åer results and our baseline.
We then interpreted the eÔ¨Äect size value to be small for d < 0.33 (for positive or negative values), medium for 0.33
‚â§ ùëë < 0.474 and large for ùëë ‚â• 0.474 [33].
Result: Table 4 presents the ROC-AUC values of the four ML classiÔ¨Åers for determining the release type of major,
minor, and patch releases. Table 4 shows the results for XGBoost (XGB), Random Forest (RF), ZeroR (ZR), Decision
Tree (DT), and Logistic Regression (LR) for the 31 studied npm packages in our dataset. Overall, we observe that for
all three diÔ¨Äerent types of the semantic versioning (i.e., major, minor, and patch), the examined four classiÔ¨Åers achieve
acceptable performance in terms of ROC-AUC values [51, 44].

First, to determine the major release type, Table 4 shows that XGBoost classiÔ¨Åer achieves ROC-AUC values range
between 0.48 and 0.93 with an average ROC-AUC value equal to 0.77 (median=0.78). Also, the Random Forest
achieves a comparable performance in classifying major release types. The Table shows that Random Forest has an
average value of ROC-AUC equal to 0.76. Second, as for the minor releases, we observed that again the XGBoost and
Random Forest classiÔ¨Åers perform better than the Decision Tree and Logistic Regression classiÔ¨Åers. Table 4 shows

(a) Major Releases

(b) Minor Releases

(c) Patch Releases

Figure 2: The distributions of the ROC-AUC values for the diÔ¨Äerent built classiÔ¨Åers.

that XGBoost and Random Forest have average ROC-AUC values equal 0.69 and 0.67. Lastly, the highest ROC-AUC
values for determining the patch release types obtained by the XGBoost classiÔ¨Åer range between 0.57 and 0.91, with
an average of 0.74 (median=0.74). In contrast, the second highest average ROC-AUC for determining the patch release
type is achieved by Random Forest with ROC-AUC values ranging between 0.44 and 0.94 and with an average value
of 0.73 (median = 0.74). In general, the achieved ROC-AUC values indicate that the XGBoost classiÔ¨Åer eÔ¨Äectively
determines the diÔ¨Äerent semantic versioning types compared to the other examined ML classiÔ¨Åers.

Furthermore, Table 4 shows the average relative ROC-AUC values when comparing the performance of the four
classiÔ¨Åers to our baseline. Overall, the computed relative ROC-AUC shows a signiÔ¨Åcant improvement over the baseline.
In particular, for all the 31 packages, the XGBoost outperforms the baseline with average relative ROC-AUC values
of 1.58ùëã, 1.38ùëã, and 1.49ùëã for major, minor, and patch release types, respectively.

Finally, Table 5 presents the adjusted ùëù-values and eÔ¨Äect sizes according to the CliÔ¨Ä‚Äôs delta (ùëë) test. We observed
that the diÔ¨Äerences are statistically signiÔ¨Åcant in the three semantic versioning types and with a large eÔ¨Äect size (ùëë >
0.474).

Our machine learning classiÔ¨Åers achieved a promising performance for determining semantic versioning type of
a new package release. They also outperformed our baseline classiÔ¨Åer in terms of ROC-AUC values. Out of the
four examined ML classiÔ¨Åers, XGBoost tended to achieve the best performance with an average ROC-AUC of
0.77, 0.69, and 0.74 for the major, minor, and patch releases. These results translated to an improvement of 58%,
38%, and 49% compared to our baseline.

4.2. RQ2: Which dimension of features are most important in determining the semantic versioning

type of a new package release?

Motivation: After determining the type of package release with adequate ROC-AUC values and achieving a good
improvement compared to our baseline, we are now interested in understanding what dimensions of features impact
determining the type of new package releases the most. In our study, we have 41 release-level features grouped into
six dimensions. Therefore, being aware of what dimension of features impacts a new release the most can help gain a
deeper understanding of these six dimensions. Also, we aim to provide developers with actionable recommendations
(i.e., determine the type of new package release). More importantly, in our case, developers can know what dimensions
of features they should carefully examine when specifying the new release type.

0.000.250.500.751.00Change TypeComplexityDependencyDevelopmentTextTimePerformance Values (AUC)0.000.250.500.751.00Change TypeComplexityDependencyDevelopmentTextTimePerformance Values (AUC)0.000.250.500.751.00Change TypeComplexityDependencyDevelopmentTextTimePerformance Values (AUC)Method: To identify the dimension of release-level features that are the most important indicators of determining
the semantic versioning type of a new package release, we built several classiÔ¨Åers for each dimension. In particular,
for each package release type (i.e., major, minor, patch release), we built six classiÔ¨Åers (one for each dimension of
features). In total, we built eighteen classiÔ¨Åers. For example, we built a classiÔ¨Åer to determine the major release using
the change type dimension of features. To build and evaluate these classiÔ¨Åers, we follow the same steps described in
Section 3.5. Since we found that the XGBoost classiÔ¨Åer achieves the best performance in our previous question, we
used it as the classiÔ¨Åer in this analysis.

Furthermore, to compare and evaluate the performance of the built classiÔ¨Åers based on the diÔ¨Äerent dimensions of
features, we again used the well-known evaluation measurement, the ROC-AUC. We then used violin plots to compare
the distributions of our results. The vertical curves of violin plots summarize and compare the distributions of diÔ¨Äerent
ROC-AUC results.
Result: Figure 2 shows violin plots of the ROC-AUC values for the built XGBoost classiÔ¨Åer for each dimension of
features for the three semantic versioning release types. Violin plots are an eÔ¨Äective way of presenting the distribution
of data. We also superimposed box plots to highlight the key statistics of our results.

From Figure 2, we observed that all the six dimensions of features in our study appear to be important in determining
the semantic versioning type of a new package release. However, one dimension of features tended to be a strong
indicator of the semantic versioning type of a release, which is the change type dimension. Notably, for the major
release type, Figure 2a shows that the best dimension of features to determine the major release type is the change type
dimension with an average ROC-AUC value equal to 0.72 (median = 0.72).

As for the minor release, the violin plots in Figure 2b show that the built XGBoost classiÔ¨Åers using the change type
dimension outperformed other built classiÔ¨Åers in most of the studied npm packages. Furthermore, our results showed
that the built classiÔ¨Åers based on the complexity and code dimension of features achieved comparable performance to
the change type classiÔ¨Åers with average ROC-AUC values equal to 0.70 and 0.68 for classiÔ¨Åers that were built using
the change type and complexity and code dimension of features.

For determining the patch release type, from Figure 2c, we observed that two built classiÔ¨Åers seemed to have
comparable results, which are the classiÔ¨Åers that were built using change type and complexity dimensions. These two
built classiÔ¨Åers achieved an average ROC-AUC value equal to 0.73 for each. Overall, our built classiÔ¨Åers based on
the six dimensions of features in determining the patch release type tended to achieve better performance in terms of
average ROC-AUC compared to classiÔ¨Åers built to determine the major and minor release.

Interestingly, there is some dimension of features that appeared to be a good determine of release type. For example,
the dependencies related features appeared to identify patch releases with a good performance. However, classiÔ¨Åers
that were built using the dependency dimension of features to determine major and minor releases did not perform as
well.

Our investigation showed that the built XGBoost classiÔ¨Åers using the change type dimension of features tended to
perform the best when used to determine the semantic versioning release type compared to other built classiÔ¨Åers.
However, using all the six dimensions of features still achieved better performance.

4.3. RQ3: How eÔ¨Äective are the machine learning techniques when applied on cross-packages?
Motivation: Building an ML classiÔ¨Åer to determine the semantic versioning release type on package-level requires
having a suÔ¨Écient amount of labelled data to train on. However, many packages do not have enough historical labelled
data to build a classiÔ¨Åer (e.g., newly adopting semantic versioning and/or new packages). Therefore, it would be
impossible to train a machine learning classiÔ¨Åer to determine semantic versioning type of a new release on data from
such packages. In this research question, we investigated to know to what extent and with what performance a semantic
versioning type of a new package release can be automatically determined using a cross-package machine learning
classiÔ¨Åcation. In addition, answering this question allowed us to evaluate the generalizability of the built classiÔ¨Åers
and their applications when applied to other packages.
Method: To better understand the generalizability of the performance achieved by the training classiÔ¨Åer on data from
one package and apply it to another package, we conducted a cross-packages validation. In particular, we experimented
with ùëõ fold cross-packages validation, where ùëõ is the number of packages in our dataset (i.e., in our dataset, we have
31 packages). We conducted an experiment that trains a classiÔ¨Åer on data from thirty packages and uses the built
classiÔ¨Åer to determine the type of semantic versioning in the remaining one package, similar to the method used in

Table 6
Performance of Cross-packages classiÔ¨Åcation. The results are reported for XGBoost (XGB) and ZeroR (ZR) classiÔ¨Åers.

Package

sweetalert2
renovate
speakingurl
license-checker
bittorrent-dht
nes
box-ui-elements
sku
mongo-sql
pacote
seek-style-guide
nightwatch-cucumber
zapier-platform-cli
patchbay
module-deps
turtle.io
rtcpeerconnection
react-isomorphic-render
rtc-quickconnect
terrestris/react-geo
eslint-conÔ¨Åg-canonical
repofs
penseur
octokit/routes
socketcluster-server
oui
express-processimage
octokit/Ô¨Åxtures
jsonrpc-bidirectional
reactive-di
rtc-signaller
Average
Median
Relative ROC-AUC

Major

Minor

Patch

XGB

0.83
0.58
0.71
0.61
0.89
0.59
0.65
0.70
0.76
0.92
0.64
0.78
0.82
0.53
0.82
0.88
0.86
0.66
0.84
0.76
0.70
0.86
0.82
0.61
0.70
0.79
0.69
0.78
0.62
0.80
0.84
0.74
0.76
1.5ùëã

ZR

0.59
0.47
0.61
0.52
0.49
0.49
0.57
0.51
0.40
0.47
0.48
0.53
0.43
0.51
0.62
0.46
0.59
0.62
0.45
0.53
0.56
0.62
0.28
0.44
0.52
0.63
0.45
0.52
0.61
0.47
0.50
0.52
0.51
-

XGB

0.70
0.79
0.56
0.56
0.63
0.75
0.62
0.80
0.55
0.86
0.75
0.80
0.75
0.77
0.53
0.82
0.56
0.59
0.62
0.65
0.68
0.78
0.57
0.70
0.61
0.58
0.69
0.86
0.70
0.60
0.75
0.68
0.69
1.4ùëã

ZR

0.48
0.45
0.62
0.33
0.64
0.49
0.46
0.49
0.54
0.54
0.46
0.58
0.53
0.53
0.50
0.52
0.45
0.57
0.36
0.63
0.41
0.41
0.46
0.64
0.57
0.52
0.56
0.55
0.54
0.49
0.55
0.51
0.52
-

XGB

0.75
0.83
0.68
0.72
0.75
0.75
0.76
0.80
0.60
0.90
0.77
0.82
0.82
0.76
0.61
0.88
0.63
0.63
0.70
0.74
0.78
0.84
0.72
0.63
0.75
0.71
0.72
0.82
0.73
0.74
0.79
0.75
0.75
1.5ùëã

ZR

0.49
0.51
0.39
0.48
0.42
0.56
0.40
0.49
0.59
0.52
0.48
0.53
0.42
0.56
0.49
0.44
0.49
0.44
0.58
0.59
0.42
0.49
0.50
0.55
0.50
0.50
0.53
0.46
0.45
0.48
0.47
0.49
0.49
-

prior work [7, 31, 1]. We repeated this process 31 times, one for each package in our dataset. To build the classiÔ¨Åer, we
trained the XGBoost machine learning classiÔ¨Åers following the same approach described earlier in Section 3.5. Once
again, we employed the well-known evaluation measurement where we computed ROC-AUC values to measure the
performance of the generated classiÔ¨Åers. Finally, to examine the cross-packages classiÔ¨Åer‚Äôs performance with respect
to our baseline, which is the ZeroR classiÔ¨Åer, we computed the relative ROC-AUC values.
Result: Table 6 presents the results of our experiment. It shows the ROC-AUC values for each package for the diÔ¨Äerent
semantic versioning types. In general, we observed that the built cross-packages classiÔ¨Åers achieved good performance.
The built classiÔ¨Åers have average ROC-AUC values of 0.74, 0.68, and 0.75 for the major, minor, and patch releases.
With an average ROC-AUC score equal to 0.74 (median=0.75), the cross-packages classiÔ¨Åer performs signiÔ¨Åcantly
high when it is used to determine the major release type. For example, seventeen packages in our dataset have ROC-
AUC values greater than 0.75, which is an acceptable performance [51, 44, 75]. We also observed similar performance
for determining minor and patch release types.

Moreover, we compared the performance of the cross-packages classiÔ¨Åers to the baseline for all the three semantic
versioning release types (i.e., major, minor, and patch). Our results showed that cross-packages classiÔ¨Åers show an
improvement of 50%, 40%, and 50% on average over the baseline for the major, minor, and patch semantic versioning
release type.

Finally, we investigated whether the achieved improvements by the built classiÔ¨Åers over the baseline classiÔ¨Åers for
the diÔ¨Äerent semantic versioning types are statistically signiÔ¨Åcant. Table 7 shows the p-values and eÔ¨Äect size values.

Table 7
Mann-Whitney Test (p-value) and CliÔ¨Ä‚Äôs Delta (d ) for the results of XGBoost vs. ZeroR classiÔ¨Åers for the tree diÔ¨Äerent
version types.

Version type

p-value

d

Major
Minor
Patch

4.982e-10
1.42e-08
1.353e-11

0.92 (large)
0.84 (large)
1.00 (large)

It shows that for all semantic versioning types, the diÔ¨Äerences are statistically signiÔ¨Åcant, having p-values < 0.05.
Also, the eÔ¨Äect size values are large. These results showed that cross-packages outperform the performance of the
cross-package baseline classiÔ¨Åer with statistically signiÔ¨Åcant results.

Our results indicated that cross-package machine learning classiÔ¨Åers can provide comparable performances to
within-package classiÔ¨Åers for determining the semantic versioning type. For all packages in our dataset, cross-
package classiÔ¨Åers achieved average ROC-AUC values of 0.74, 0.68, and 0.75 with an overall improvement over
the baseline classiÔ¨Åers with relative ROC-AUC equal to 50%, 40%, and 50% for major, minor, and patch release.

5. Related Work

In this paper, we proposed using machine learning techniques to eÔ¨Äectively determine the semantic versioning type
of npm packages. Thus, our work is mainly related to two areas of prior studies; work related to the use of semantic
versioning and work related to identifying breakage changes in third-party packages.

Semantic versioning: Due to the importance of semantic versioning, several studies have examined it. One of the
Ô¨Årst works that looked at the use of semantic versioning is the work by Raemaekers et al. [58]. They investigated the
use of semantic versioning in the dataset of 22K Java packages published on Maven that span for seven years. Their
results showed that breaking changes occur in 30% of the studied releases, including minor releases and patches. Thus,
several packages used strict dependency constraints, and package maintainers avoid upgrading their dependencies. In
addition, Kula et al. [42] found that developers tend not to update their depend on packages even though these updates
are related to the addition of new features and patches to Ô¨Åx vulnerabilities. Interestingly, Raemaekers et al. [58]‚Äôs
approach relies on a tool called tclirr, which detects breaking API changes through static analysis of Java code. While
a similar tool could be developed for other languages, it requires a clear separation between the public and private API.
Such a distinction does not explicitly exist in dynamic languages such as JavaScript, making the accurate detection of
breaking changes much more diÔ¨Écult. Moreover, fundamental diÔ¨Äerences, such as dynamic versus static typing or the
language‚Äôs dynamic nature, between JavaScript and other programming language such as Java make the studies on this
language diÔ¨Écult.

Dietrich, Pearce, Stringer, Tahir and Blincoe [25] also studied large dependencies in seventeen package manager
ecosystems found that many ecosystems support Ô¨Çexible versioning practices and that the adoption of semantic
versioning is increasing. In the same line, Decan and Mens [23] empirically studied semantic versioning compliances
in four ecosystems (Cargo, npm, Packagist, and Rubygems) by analyzing the packages dependency constraints. Their
Ô¨Åndings showed that the proportion of compliant dependency constraints increases over time in all studied ecosystems.
In the same direction, Wittern et al. [70] studied the evolution of a subset of JavaScript packages in npm, analyzing
characteristics such as their dependencies, update frequency, and semantic versioning number. They observed that the
versioning conventions that maintainers use for their packages are not always compatible with semantic versioning.
Also, Bogart et al. [11] conducted a qualitative comparison of npm, CRAN, and Eclipse, to understand the impact
of community values, tools, and policies on breaking changes. They found two main types of mitigation strategies
to reduce the exposure to changes in dependencies: limiting the number of dependencies and depending only on
‚Äútrusted packages‚Äù. In a follow up work, they interviewed more than 2,000 developers about values and practices in
18 ecosystems [10]. Among other Ô¨Åndings, they observed that package maintainers are frequently exposed to breaking
changes and mainly discover them at build time.

Our work is motivated by these prior aforementioned research eÔ¨Äorts. The diÔ¨Äerence is that our work focuses on

proposing a machine learning classiÔ¨Åers to identify the semantic versioning type of a new npm package release.

Identifying breakage changes in third-party packages: Several studies investigated API evolution and stability

and proposed techniques to detect breakage changes [47, 72, 26, 39, 37].

Mujahid et al. [49] proposed the idea of using other‚Äôs tests to identify breaking changes of JavaScript packages.
They examined the accuracy of their proposed approach on ten cases of breaking updates. Their experimental results
showed that their approach identiÔ¨Åed six breaking updates. Similarly, Xavier et al. [72] performed a large-scale analysis
on Java packages. Their results showed that 14.78% of the API changes are incompatible with previous versions. They
also found that packages with a higher frequency of breaking changes are larger, more popular, and more active.
Also, Businge, Serebrenik and van den Brand [16, 17] studied Eclipse interface usage by Eclipse third-party plug-ins
and evaluated the eÔ¨Äect of API changes and non-API changes. Mostafa et al. [48] detected backward compatibility
problems in Java packages by performing regression tests on version pairs and by inspecting bug reports related to
version upgrades. The similarity between our work and these aforementioned work is the idea of identifying the type
of changes in a new package release. However, to the best of our knowledge, our work is the Ô¨Årst work to investigated
the use of ML technique.

6. Threats to Validity

There are few important limitations to our work that need to be considered when interpreting our Ô¨Åndings. In this

section, we described the threats to the validity of our study.

Internal validity: Threats to internal validity concerns with factors that could have inÔ¨Çuenced our study setup.
First, we used the extracted AST diÔ¨Äerence between two source codes to extract the change type features. To do this,
we used GumTree diÔ¨Äerencing algorithm [30]. Thus, we might be limited by the accuracy and correctness of this tool.
However, previous studies used GumTree for calculating diÔ¨Äerences between two source codes for diÔ¨Äerent studies.
It is also mentioned in the documentation of GumTree that the algorithm is prone to some errors in the context of
JavaScript, so it might miss some instances when extracting the diÔ¨Äerence of JavaScript source codes. For parsing the
result of GumTree tool, we developed a parser to extract Ô¨Åne-grained source code changes. This process could result in
some errors. Thus, we manually analyzed randomly selected 300 change types to mitigate this threat, and our manual
examination shows that the implemented parser correctly extracts all the cases.

In addition, to answer our research questions and to extract the complexity and code dimension of features between
two consecutive releases, we used the Understand tool [68]. Therefore, we were limited by the accuracy of the
Understand tool. That said, the Understand tool is a widely used analysis tool in both research and industry [2, 60, 19, 3].
Also, a recent study showed that the Understand tool analyzes JavaScript code with good accuracy [61], which mitigate
such a threat.

Construct validity: Threats to construct validity considers the relationship between theory and observation, in
case the measured variables do not measure the actual factors. The labeled package releases (i.e., patch, minor, or
major) that we examined are releases that are explicitly marked as so by the package developers in our dataset. In some
cases, developers might mislabel the releases. To mitigate this threat, we have applied diÔ¨Äerent Ô¨Åltration criteria (see
Section 3.1) that include selecting mature and popular packages. Also, we Ô¨Åltered out any package that their users
reported it to has at least one breakage release but their developers tagged it a minor or patch release.

Also, to extract the development features, we opted for analyzing the commits in the Git system. Similar to prior
work (e.g., [40, 66]) to identify those commits between two consecutive releases, we consider all commits occurred
in the main trunk of the versioning system based on the release date. It is worth mentioning that these dates could be
approximations, as developers could start working on the release even before it is issued.

External validity: Threats to external validity concern the generalization of our Ô¨Åndings. Our dataset only consists
of JavaScript packages, which are published on the npm package manager. Hence, our Ô¨Åndings might not hold for
packages published on other package managers and written in diÔ¨Äerent programming languages. That said, prior
work (e.g., [24]) showed that npm packages are commonly used, and npm is one of the largest and rapidly growing
package managers, which make it the ideal case to study.

In this study, we performed a combination of feature extraction both from code changes and development history
from JavaScript open-source packages, and the method used to extract the studied features is speciÔ¨Åc to JavaScript,
so our classiÔ¨Åers might not be generalized for other programming languages. Also, diÔ¨Äerent programming languages
might require diÔ¨Äerent feature extraction methods due to their semantic diÔ¨Äerences. However, our data collections and
analysis approaches could be easily generalized to packages written in any language.

In addition, our dataset presented only open-source packages whose source code is hosted on GitHub that might
not reÔ¨Çect close source packages. Also, in our study, we examined a dataset that contains 31 npm JavaScript packages,
which may not represent the whole population of JavaScript packages, and examining a larger number of packages
may show diÔ¨Äerent results.

7. Conclusion

In this paper, our goal is to use ML techniques to determine semantic versioning type of a new package release. We
used 41 release-level features extracted by analyzing the source code and the development activities of the releases of 31
JavaScript packages published on npm. Then, we built four ML classiÔ¨Åers. We found that the XGBoost can eÔ¨Äectively
determine the type of semantic versioning with average ROC-AUC equal to 0.77, 0.69, and 0.74 for major, minor, and
patch releases. It also showed an improvement of 58%, 38%, and 49% over our baseline, which is the ZeroR classiÔ¨Åer.
Regarding the most important features used by the XGBoost classiÔ¨Åers to determine semantic versioning release type,
we found that the change type and complexity and code dimensions of features are the most important indicators of
new release type. Additionally, we investigated the generalizability of determining semantic versioning type when we
used cross-packages validation. Our results showed that the cross-packages validation achieves acceptable performance
compared to within-packages validation.

References
[1] Abdalkareem, R., Mujahid, S., Shihab, E., 2020. A machine learning approach to improve the detection of ci skip commits. IEEE Transactions

on Software Engineering , 1‚Äì1.

[2] Abdalkareem, R., Nourry, O., Wehaibi, S., Mujahid, S., Shihab, E., 2017. Why do developers use trivial packages? an empirical case study on
npm, in: Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, Association for Computing Machinery, New
York, NY, USA. p. 385‚Äì395. URL: https://doi.org/10.1145/3106237.3106267, doi:10.1145/3106237.3106267.

[3] Ahasanuzzaman, M., Hassan, S., Hassan, A.E., 2020. Studying ad library integration strategies of top free-to-download apps.

IEEE

Transactions on Software Engineering .

[4] Alfassa, E., 2013. 857922 - fontconÔ¨Åg change breaks webfonts rendering under linux. https://bugzilla.mozilla.org/show_bug.

cgi?id=857922. (accessed on 02/25/2022).

[5] Andreasen, E., Gong, L., M√∏ller, A., Pradel, M., Selakovic, M., Sen, K., Staicu, C.A., 2017. A survey of dynamic analysis and test generation

for javascript. ACM Comput. Surv. 50.

[6] Bacchelli, A., Dal Sasso, T., D‚ÄôAmbros, M., Lanza, M., 2012. Content classiÔ¨Åcation of development emails, in: Proceedings of the 34th

International Conference on Software Engineering, IEEE Press. pp. 375‚Äì385.

[7] Bacchelli, A., Dal Sasso, T., D‚ÄôAmbros, M., Lanza, M., 2012. Content classiÔ¨Åcation of development emails, in: 2012 34th International

Conference on Software Engineering (ICSE), IEEE. pp. 375‚Äì385.

[8] Bengio, Y., Grandvalet, Y., 2004. No unbiased estimator of the variance of k-fold cross-validation. Journal of machine learning research 5,

1089‚Äì1105.

[9] Bogart, C., Filippova, A., Kastner, C., Herbsleb, J., 2017a. How ecosystem cultures diÔ¨Äer: Results from a survey on values and practices

across 18 software ecosystems. http://breakingapis.org/survey/. (accessed on 11/17/2020).

[10] Bogart, C., Filippova, A., K√§stner, C., Herbsleb, J., 2017b. How ecosystem cultures diÔ¨Äer: Results from a survey on values and practices

across 18 software ecosystems. [Online]. Available: http://breakingapis.org/survey/. (Accessed on 08/10/2020).

[11] Bogart, C., K√§stner, C., Herbsleb, J., Thung, F., 2016. How to break an api: Cost negotiation and community values in three software
ecosystems, in: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, Association
for Computing Machinery, New York, NY, USA. p. 109‚Äì120. URL: https://doi.org/10.1145/2950290.2950325, doi:10.1145/
2950290.2950325.

[12] Borges, H., Valente, M.T., 2018. What‚Äôs in a github star? understanding repository starring practices in a social coding platform. Journal of

Systems and Software 146, 112 ‚Äì 129.

[13] Bouckaert, R.R., Frank, E., Hall, M., Kirkby, R., Reutemann, P., Seewald, A., Scuse, D., 2013. WEKA Manual for Version 3-7-8. (accessed

on 02/28/2021).

[14] Bradley, A.P., 1997. The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern recognition 30,

1145‚Äì1159.

[15] Breiman, L., 2001. Random forests. Machine learning 45, 5‚Äì32.
[16] Businge, J., Serebrenik, A., van den Brand, M.G.J., 2012. Survival of eclipse third-party plug-ins, in: Proceedings of the 28th IEEE
International Conference on Software Maintenance, IEEE, New York, NY, USA. pp. 368‚Äì377. doi:10.1109/ICSM.2012.6405295.
[17] Businge, J., Serebrenik, A., van den Brand, M.G.J., 2015. Eclipse api usage: The good and the bad. Software Quality Journal 23, 107‚Äì141.

doi:10.1007/s11219-013-9221-3.

[18] Caruana, R., Niculescu-Mizil, A., 2006. An empirical comparison of supervised learning algorithms, in: Proceedings of the 23rd International

Conference on Machine Learning, ACM. pp. 161‚Äì168.

[19] Castelluccio, M., An, L., Khomh, F., 2019. An empirical study of patch uplift in rapid release development pipelines. Empirical Software

Engineering 24, 3008‚Äì3044.

[20] Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P., 2002. Smote: synthetic minority over-sampling technique. Journal of artiÔ¨Åcial

intelligence research 16, 321‚Äì357.

[21] Chen, T., Guestrin, C., 2016. Xgboost: A scalable tree boosting system, in: Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, Association for Computing Machinery, New York, NY, USA. p. 785‚Äì794. URL:
https://doi.org/10.1145/2939672.2939785, doi:10.1145/2939672.2939785.

[22] Dabbish, L., Stuart, C., Tsay, J., Herbsleb, J., 2012. Social coding in github: Transparency and collaboration in an open software repository,

in: Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work, ACM. pp. 1277‚Äì1286.

[23] Decan, A., Mens, T., 2019. What do package dependencies tell us about semantic versioning? IEEE Transactions on Software Engineering ,

1‚Äì15.

[24] Decan, A., Mens, T., Grosjean, P., 2019. An empirical comparison of dependency network evolution in seven software packaging ecosystems.

Empirical Software Engineering , 381‚Äì416.

[25] Dietrich, J., Pearce, D., Stringer, J., Tahir, A., Blincoe, K., 2019. Dependency versioning in the wild, in: 2019 IEEE/ACM 16th International

Conference on Mining Software Repositories (MSR), pp. 349‚Äì359. doi:10.1109/MSR.2019.00061.

[26] Dig, D., Johnson, R., 2006. How do apis evolve&quest; a story of refactoring. Journal of Software Maintenance 18, 83‚Äì107. doi:10.1002/

smr.328.

[27] npm documentation, . About semantic versioning | npm docs. https://docs.npmjs.com/about-semantic-versioning. (accessed on

03/08/2022).

[28] Esteves, G., Figueiredo, E., Veloso, A., Viggiato, M., Ziviani, N., 2020. Understanding machine learning software defect predictions.

Automated Software Engineering 27, 369‚Äì392.

[29] FaceBook, 2016. Yarn: A new package manager for javascript - facebook engineering. https://engineering.fb.com/2016/10/11/

web/yarn-a-new-package-manager-for-javascript/. (accessed on 03/13/2021).

[30] Falleri, J., Morandat, F., Blanc, X., Martinez, M., Monperrus, M., 2014. Fine-grained and accurate source code diÔ¨Äerencing, in: ACM/IEEE
International Conference on Automated Software Engineering, ASE ‚Äô14, Vasteras, Sweden - September 15 - 19, 2014, pp. 313‚Äì324. URL:
http://doi.acm.org/10.1145/2642937.2642982, doi:10.1145/2642937.2642982.

[31] Fukushima, T., Kamei, Y., McIntosh, S., Yamashita, K., Ubayashi, N., 2014. An empirical study of just-in-time defect prediction using cross-
project models, in: Proceedings of the 11th Working Conference on Mining Software Repositories, Association for Computing Machinery. p.
172‚Äì181.

[32] Ghotra, B.,

, S., Hassan, A.E., 2015. Revisiting the impact of classiÔ¨Åcation techniques on the performance of defect prediction models, in:

Proceedings of the 37th International Conference on Software Engineering, IEEE Press. pp. 789‚Äì800.

[33] Grissom, R.J., Kim, J.J., 2005. EÔ¨Äect sizes for research: A broad practical approach. Lawrence Erlbaum Associates Publishers.
[34] Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I.H., 2009. The weka data mining software: an update. ACM SIGKDD

explorations newsletter 11, 10‚Äì18.

[35] He, Z., Shu, F., Yang, Y., Li, M., Wang, Q., 2012. An investigation on the feasibility of cross-project defect prediction. Automated Software

Engineering. 19, 167‚Äì199.

[36] Iba, H., 1996. Random tree generation for genetic programming, in: Proceedings of the 4th International Conference on Parallel Problem
Solving from Nature, Springer-Verlag, London, UK, UK. pp. 144‚Äì153. URL: http://dl.acm.org/citation.cfm?id=645823.670546.
[37] Javan Jafari, A., Costa, D.E., Abdalkareem, R., Shihab, E., Tsantalis, N., 2021. Dependency smells in javascript projects. IEEE Transactions

on Software Engineering , 1‚Äì1doi:10.1109/TSE.2021.3106247.

[38] Kamei, Y., Shihab, E., Adams, B., Hassan, A.E., Mockus, A., Sinha, A., Ubayashi, N., 2013. A large-scale empirical study of just-in-time

quality assurance. IEEE Transactions on Software Engineering 39, 757‚Äì773.

[39] Kapur, P., Cossette, B., Walker, R.J., 2010. Refactoring references for library migration. ACM SIGPLAN Notices 45, 726‚Äì738. doi:10.

1145/1932682.1869518.

[40] Khomh, F., Adams, B., Dhaliwal, T., Zou, Y., 2015. Understanding the impact of rapid releases on software quality. Empirical Softw. Engg.

20, 336‚Äì373. URL: https://doi.org/10.1007/s10664-014-9308-x, doi:10.1007/s10664-014-9308-x.

[41] Kotsiantis, S.B., Zaharakis, I.D., Pintelas, P.E., 2006. Machine learning: A review of classiÔ¨Åcation and combining techniques. Artif. Intell.

Rev. 26, 159‚Äì190.

[42] Kula, R.G., German, D.M., Ouni, A., Ishio, T., Inoue, K., 2017. Do developers update their library dependencies?: An empirical study on the

impact of security advisories on library migration. doi:10.1007/s10664-017-9521-5, arXiv:1709.04621.

[43] Lauinger, T., Chaabane, A., Wilson, C., 2018. Thou shalt not depend on me: A look at javascript libraries in the wild. Queue 16, 62‚Äì82.
[44] Lessmann, S., Baesens, B., Mues, C., Pietsch, S., 2008. Benchmarking classiÔ¨Åcation models for software defect prediction: A proposed

framework and novel Ô¨Åndings. IEEE Transactions on Software Engineering 34, 485‚Äì496.

[45] Mann, H.B., Whitney, D.R., 1947. On a test of whether one of two random variables is stochastically larger than the other. The annals of

mathematical statistics , 50‚Äì60.

[46] Mariano, R.V.R., dos Santos, G.E., V. de Almeida, M., Brand√£o, W.C., 2019. Feature changes in source code for commit classiÔ¨Åcation into
maintenance activities, in: 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), IEEE. pp. 515‚Äì518.
[47] Mostafa, S., Rodriguez, R., Wang, X., 2017a. A Study on Behavioral Backward Incompatibility Bugs in Java Software Libraries,
in: Proceedings of the 39th International Conference on Software Engineering Companion, IEEE, New York, NY, USA. pp. 127‚Äì129.
doi:10.1109/ICSE-C.2017.101.

[48] Mostafa, S., Rodriguez, R., Wang, X., 2017b. Experience paper: A study on behavioral backward incompatibilities of java software libraries, in:
Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis, Association for Computing Machinery,
New York, NY, USA. p. 215‚Äì225. URL: https://doi.org/10.1145/3092703.3092721, doi:10.1145/3092703.3092721.

[49] Mujahid, S., Abdalkareem, R., Shihab, E., McIntosh, S., 2020. Using others‚Äô tests to identify breaking updates , 1‚Äì12.
[50] Murphy, K.P., 2012. Machine learning: a probabilistic perspective. MIT press.

[51] Nam, J., Kim, S., 2015. Clami: Defect prediction on unlabeled datasets, in: Proceedings of the 30th IEEE/ACM International Conference on

Automated Software Engineering, IEEE Press. p. 452‚Äì463.

[52] npm, . npm-registry | npm documentation. https://docs.npmjs.com/using-npm/registry.html. (Accessed on 08/13/2020).
[53] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al.,

2011. Scikit-learn: Machine learning in python. the Journal of machine Learning research 12, 2825‚Äì2830.

[54] Potvin, R., Levenberg, J., 2016. Why google stores billions of lines of code in a single repository. Communications of the ACM 59, 78‚Äì87.
[55] Pradel, M., Schuh, P., Sen, K., 2015. Typedevil: Dynamic type inconsistency analysis for javascript, in: 2015 IEEE/ACM 37th IEEE

International Conference on Software Engineering, IEEE. pp. 314‚Äì324.

[56] Preston-Werner, T., 2019. Semantic versioning 2.0. URL: https://semver.org/.
[57] Quinlan, R., 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.
[58] Raemaekers, S., van Deursen, A., Visser, J., 2017. Semantic versioning and impact of breaking changes in the maven repository. Journal of

Systems and Software 129, 140‚Äì158.

[59] Rahman, M.M., Roy, C.K., Kula, R.G., 2017. Predicting usefulness of code review comments using textual features and developer experience,

in: Proceedings of the 14th International Conference on Mining Software Repositories, IEEE Press. pp. 215‚Äì226.

[60] Rahman, M.T., Rigby, P.C., Shihab, E., 2019. The modular and feature toggle architectures of google chrome. Empirical Software Engineering

24, 826‚Äì853.

[61] Reza Chowdhury, M.A., Abdalkareem, R., Shihab, E., Adams, B., 2021. On the untriviality of trivial packages: An empirical study of npm

javascript packages. IEEE Transactions on Software Engineering , 1‚Äì1.

[62] SciTools-Documentation, . Understand static code analysis tool. https://www.scitools.com/. (accessed on 03/08/2022).
[63] Shihab, E., Jiang, Z.M., Ibrahim, W.M., Adams, B., Hassan, A.E., 2010. Understanding the impact of code and process metrics on post-
release defects: A case study on the eclipse project, in: Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software
Engineering and Measurement.

[64] ≈öliwerski, J., Zimmermann, T., Zeller, A., 2005. When do changes induce Ô¨Åxes? ACM sigsoft software engineering notes 30, 1‚Äì5.
[65] Song, Q., Guo, Y., Shepperd, M., 2019. A comprehensive investigation of the role of imbalanced learning for software defect prediction. IEEE

Transactions on Software Engineering 45, 1253‚Äì1269. doi:10.1109/TSE.2018.2836442.

[66] Souza, R., Chavez, C., Bittencourt, R.A., 2014. Do rapid releases aÔ¨Äect bug reopening? a case study of Ô¨Årefox, in: 2014 Brazilian Symposium

on Software Engineering, pp. 31‚Äì40. doi:10.1109/SBES.2014.10.

[67] Thung, F., Lo, D., Jiang, L., Lucia, Rahman, F., Devanbu, P.T., 2012. When would this bug get reported?, in: Proceedings of the 28th IEEE

International Conference on Software Maintenance, IEEE. pp. 420‚Äì429.

[68] Understand, S., . Scitools.com. https://scitools.com/. (Accessed on 08/13/2020).
[69] Williams, C., Spacco, J., 2008. Szz revisited: verifying when changes induce Ô¨Åxes, in: Proceedings of the 2008 workshop on Defects in large

software systems, pp. 32‚Äì36.

[70] Wittern, E., Suter, P., Rajagopalan, S., 2016. A look at the dynamics of the javascript package ecosystem, in: Proceedings of the 13th
International Conference on Mining Software Repositories, Association for Computing Machinery, New York, NY, USA. p. 351‚Äì361. URL:
https://doi.org/10.1145/2901739.2901743, doi:10.1145/2901739.2901743.

[71] Xavier, L., Brito, A., Hora, A., Valente, M.T., 2017. Historical and impact analysis of api breaking changes: A large-scale study, in: 2017

IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER), IEEE. pp. 138‚Äì147.

[72] Xavier, L., Brito, A., Hora, A., Valente, M.T., 2017. Historical and impact analysis of api breaking changes: A large-scale study, in:
Proceedings of the 24th International Conference on Software Analysis, Evolution and Reengineering, IEEE, New York, NY, USA. pp. 138‚Äì
147. doi:10.1109/SANER.2017.7884616.

[73] Xia, X., , E., Kamei, Y., Lo, D., Wang, X., 2016a. Predicting crashing releases of mobile applications, in: Proceedings of the 10th ACM/IEEE

International Symposium on Empirical Software Engineering and Measurement, ACM. pp. 29:1‚Äì29:10.

[74] Xia, X., Shihab, E., Kamei, Y., Lo, D., Wang, X., 2016b. Predicting crashing releases of mobile applications, in: Proceedings of the 10th

ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM‚Äô16).

[75] Yan, M., Xia, X., Shihab, E., Lo, D., Yin, J., Yang, X., 2019. Automating change-level self-admitted technical debt determination. IEEE

Transactions on Software Engineering 45, 1211‚Äì1229.

