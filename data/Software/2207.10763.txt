1

Tactile Gym 2.0: Sim-to-real Deep Reinforcement
Learning for Comparing Low-cost High-Resolution
Robot Touch
Yijiong Lin1, John Lloyd1, Alex Church1, Nathan F. Lepora1
Project Webpage: https://sites.google.com/my.bristol.ac.uk/tactilegym2

2
2
0
2

l
u
J

7
2

]

O
R
.
s
c
[

2
v
3
6
7
0
1
.
7
0
2
2
:
v
i
X
r
a

Abstract—High-resolution optical tactile sensors are increas-
ingly used in robotic learning environments due to their ability
to capture large amounts of data directly relating to agent-
environment interaction. However, there is a high barrier of
entry to research in this area due to the high cost of tactile
robot platforms, specialised simulation software, and sim-to-real
methods that lack generality across different sensors. In this
letter we extend the Tactile Gym simulator to include three new
optical tactile sensors (TacTip, DIGIT and DigiTac) of the two
most popular types, Gelsight-style (image-shading based) and
TacTip-style (marker based). We demonstrate that a single sim-
to-real approach can be used with these three different sensors
to achieve strong real-world performance despite the signiﬁcant
differences between real tactile images. Additionally, we lower the
barrier of entry to the proposed tasks by adapting them to an
inexpensive 4-DoF robot arm, further enabling the dissemination
of this benchmark. We validate the extended environment on
three physically-interactive tasks requiring a sense of touch:
object pushing, edge following and surface following. The results
of our experimental validation highlight some differences between
these sensors, which may help future researchers select and
customize the physical characteristics of
tactile sensors for
different manipulations scenarios.

Index Terms—Force and tactile sensing; Deep Learning; Dex-

terous Manipulation

I. INTRODUCTION

A plausible route to human-like robot dexterity is to com-
bine deep learning with high-resolution tactile sensing, given
the unprecedented recent advances in controlling robots with
deep learning applied to robot vision [2]. Moreover, the use
of deep reinforcement learning (RL) would seem to offer the
potential for learning complex manipulation tasks based on
a reward information, which is both a mechanism for how
humans acquire new skills and has achieved impressive results
in simulated environments such as computer games. However,
there are major challenges preventing tactile deep RL from
being realised: (1) the lack of available and accessible tactile
sensing technologies limits the research capacity available
to develop RL methods for touch; (2) those labs that have

YL was supported by the China Scholarship Council (CSC)/University of
Bristol joint-funded scholarship. AC was supported by an EPSRC CASE
award sponsored by Google DeepMind. NL and JL were supported by a
Leadership Award from the Leverhulme Trust on ‘A biomimetic forebrain for
robot touch’ (RL-2016-39).

1 All authors are with the Department of Engineering Mathematics and
Bristol Robotics Laboratory, University of Bristol, Bristol BS8 1UB, U.K.
(email: {yijiong.lin, j.lloyd, a.church, n.lepora}@bristol.ac.uk)

Code is open-sourced at: https://github.com/ac-93/tactile_gym

Figure 1. Overview of the tactile sim-to-real deep RL robotic system:
(a) Three low-cost high-resolution optical tactile sensors and the raw sensor
images. (b) Training real-to-sim tactile image translation. (c) Policy learning
in the Tactile Gym [1] with simulated tactile images from multiple integrated
sensors. (d) A desktop robot equipped with the DigiTac performing the
surface-following task by translating real tactile images to simulated images
for the RL policy.

expertise in fabricating tactile sensors tend to stay with the
technology where they have expertise; (3) hence, approaches
to tactile deep RL, e.g. [1], [3], [4], stay conﬁned within those
labs, which is inefﬁcient for progressing the ﬁeld and opposite

 (a) Optical Tactile SensorsDIGITTacTipDigiTacRealSimSimSimRealReal  (b)  Real-to-Sim Tactile Image TranslationDiscriminatorGeneratorRealSim   (c) Online RL in Tactile GymRL policyTraining phase in Simulation   (d) Sim-to-Real Policy TransferRL policyGeneratorTesting phase in Reality 
 
 
 
 
 
2

to the open culture that has beneﬁtted AI research.

Meta AI researchers have developed and open-sourced a
tactile robot learning platform called TACTO [5] and tactile
processing libraries PyTouch [6] for GelSight-based tactile
sensors such as the low-cost, open-sourced DIGIT tactile
sensor [7]. Meanwhile, researchers in Bristol Robotics Labo-
ratory have developed a family of high-resolution biomimetic
optical tactile sensors called the TacTip [8], [9], alongside
an open-sourced suite of learning environments called Tactile
Gym that features highly-efﬁcient
tactile image rendering
[1]. In consequence, sim-to-real policy transfer via tactile
image translation enabled zero-shot performance on multiple
exploration and manipulation tasks requiring tactile feedback,
such as delicately tracing the edges and surfaces of complex
objects and pushing objects to goal locations.

The goal of this present research is to bring together
this progress with GelSight-type sensors with the sim-to-real
deep RL methods developed with the TacTip. Whilst it was
previously claimed that the approach used in [1] should be
applicable to “a broad range of other high-resolution optical
tactile sensors including sensors of the Gelsight type”, the
approach was only demonstrated with the TacTip. Here we
extend the approach to the DIGIT sensor (of Gelsight type),
a reduced form-factor TacTip sensor and a DIGIT-TacTip
hybrid sensor referred to as the DigiTac. We demonstrate
strong real-world performance across these sensors despite the
signiﬁcantly different tactile images produced by each.
The main contributions of this work are as follows:

1) We extend the Tactile Gym [1] from one to three low-
cost, high-resolution optical tactile sensors. To the best of our
knowledge, this is the ﬁrst work that integrates two widely-
used yet fundamentally different styles of the optical tactile
sensors: Gelsight-style (DIGIT) and TacTip-style (DigiTac and
TacTip), into one platform. Such an integration validates and
extends earlier results for this platform while making it more
accessible and applicable to the wider research community.
2) With the extended Tactile Gym environments, we success-
fully learn deep RL policies for three physically-interactive
tasks (edge following, surface following and object pushing),
and transfer them into the real world without further tuning.
By comparing task performance, we identify strengths and
weaknesses of the tactile sensors in these different scenarios.
As far as we know, this is the ﬁrst empirical comparison of
optical tactile sensors in a sim-to-real context, which we intend
as a benchmark to aid improvement of this technology.
3) We improve the accessibility of this research by adapting the
tasks to the reduced workspace and degrees of freedom of the
DOBOT MG400, an inexpensive and commercially-available
4-DoF robot arm. Additionally, we commit to openly releasing
all software and hardware developments in this work to aid
dissemination of this benchmark.

II. RELATED WORK

1) Deep Reinforcement Learning in Tactile Robotics: Deep
reinforcement learning (RL) has proved successful in solving
many sequential decision making problems in robotics, par-
ticularly those with high-dimensional observation spaces such

as in computer vision [2]. Some work has explored tactile
RL with low-resolution tactile sensing to perform tasks such
as object stabilisation [10] and learning a forward predictive
model [11]. Work in [12] proposed a deep tactile model-
predictive RL framework for learning how to re-position an
object using a Gelsight-style tactile sensor, and [3] used
Twin Delayed DDPG (TD3) [13] to learn a general tactile-
guided insertion policy in the physical environment from a
sequence of tactile images. A follow-up study simpliﬁed the
observation space containing the raw tactile images and robot
proprioceptive data for the deep RL policy by learning an
extrinsic contact
localization. This
line model for contact
policy was learned in a simulation environment [14].

Recently, a tactile simulation environment for deep RL,
called Tactile Gym [1], has successfully applied the trained
policies to some challenging tasks such as object pushing
and rolling in a real physical environment. The transfer from
simulation to real-world physical environment was facilitated
using a novel real-to-sim tactile image translation technique,
in a zero-shot manner. Tactile-based deep RL has also been
successfully applied to robotic service tasks like learning to
type on a braille keyboard [15] with the TacTip and learning
to play the piano [4] with the DIGIT optical tactile sensor.

2) Tactile Sim-to-real Transfer: The tactile sim-to-real gap
signiﬁcantly hinders the application of learned policy in simu-
lation to reality. Two research directions have been explored to
close this gap: using the Finite Element (FE) method to model
the sensor deformation dynamics [16]–[21], or leveraging
the image rendering method to replicate the sensory data
[1], [5], [22], [23]. For a more thorough review we refer
to [1]. In the present work, we follow the tactile sim-to-
real method described in [1] by using depth image-rendering
and image translation for two popular optical tactile sensor
classes: GelSight-type which is based on image shading [24]
and TacTip type which is based on biomimetic marker-based
transduction [9].

III. METHOD AND EXPERIMENTS

A. Tactile Robot System

In this paper, we use a tactile robot comprising a low-
cost desktop robot arm with high-resolution tactile sensor
mounted as an end effector. This is intended to be a lower-
cost, desktop version of the setup used by Church et al [1] for
investigating sim-to-real tactile deep RL, which used a 6-axis
industrial robot from Universal Robotics. In this paper, we
expand the approach to compare three distinct optical tactile
sensors: the TacTip, DIGIT, and DigiTac. Table I compares
this robot platform to those used in related works, in terms of
the features, cost and the integration of tactile sensors. The
robot platform and the operation of the tactile sensors are
presented in detail in an accompanying second paper, with
the main focus of the present paper on the application of sim-
to-real deep RL on this low-cost desktop robot. Hence, we
summarize just the main aspects of the tactile robot.

1) Desktop robot arm: We use a Dobot MG400 4-axis
arm designed for affordable automation. The base and control
unit has footprint 190 mm×190 mm, payload 750 g, maximum

3

Figure 2. Comparison between the different optical tactile sensors: (a) DIGIT, (b) DigiTac, (c) TacTip. For each sensor we show: top-left, the real sensor
hardware; top-right, simulated contact geometry between sensor skin and a blue edge stimulus; bottom-left, tactile image gathered by the real sensor pressing
onto an edge; and bottom-right, a generated depth image matching those gathered in simulation.

Table I
Comparison of the existing tactile sim-to-real robotic platforms for
TacTip-style (T) and GelSight-style (G) sensors, respectively.

Platform

Robots

Sensors
(if integrated)

Features
Type
Accuracy (mm)
Price ($)
Max. Reach (mm)
Playload (kg)
Weight (kg)
DoF
TacTip (T)
DIGIT (G)
DigiTac (T)

[1]
UR5
0.1
45k
850
5
18.4
6
(cid:88)
(cid:55)
(cid:55)

[21]

Ours

[23]
Sawyer UR5 MG400
0.1
45k
850
5
18.4
6
(cid:55)
(cid:88)
(cid:55)

0.1
26k
1260
4
19
7
(cid:88)
(cid:55)
(cid:55)

0.05
2.7k
440
0.75
8
4
(cid:88)
(cid:88)
(cid:88)

reach 440 mm and repeatability ±0.05 mm. As we describe
later, the accuracy of tactile models trained using this arm is
similar to larger industrial robot arms. The main constraint is
that only the (x, y, z)-position and rotation around the z-axis
of the end effector are actuated.

2) High-resolution optical tactile sensing: Here we con-

sider three distinct optical tactile sensors:
(a) The TacTip, a soft, curved, 3D-printed tactile skin with an
internal array of pins tipped with markers, which are used to
amplify the surface deformation from physical contact against
a stimulus. For more details, we refer to ref. [8], [9].
(b) The DIGIT shares the same principle of the Gelsight tactile
sensor [25], but can be fabricated at low cost and is of a size
suitable for integration of some robotic hands, such as on the
ﬁngertips of the Allegro hand [7].
(c) The DigiTac is an adapted version of the DIGIT and the
TacTip, whereby the 3D-printed skin of a TacTip is customized
to integrated onto the DIGIT housing, while keeping the
camera and lighting system. In other words, this sensor outputs
tactile images of the same dimension as the DIGIT, but with
a soft biomimetic skin like other TacTip sensors.

B. Tactile Sim-to-Real Deep RL Framework

Following Church et al [1] we take a sim-to-real deep
reinforcement learning approach to achieve the desired robot
behaviour on physical hardware. This approach consists of

three distinct phases:
1) An online learning in simulation phase, where deep RL is
applied to images captured by a simulated tactile sensor for the
learning of several distinct tasks (here edge following, surface
following and object pushing).
2) A domain adaption phase where a model is learned for
the translation of images captured by a real tactile sensor to
images captured by the simulated sensor.
3) A zero-shot sim-to-real phase where policies learned in
simulation are transferred to the real hardware using the
networks trained in the previous two steps. An overview of
this approach is shown in Fig. 1 and more details can be found
in the original reference [1].

To adapt the existing approach [1] to our setting, we needed
to make several changes. In this work we chose to use a low-
cost desktop robot arm as described above (Sec.III A). This
arm has only 4 actuated axes, whereas previous work used
a 6-axis industrial robotic arm (UR5, Universal Robotics).
To facilitate the use of this lower degree-of-freedom arm we
have adapted the tasks and data collection procedures while
attempting to meet the challenge of successful performance on
tasks originally developed for a more capable robot arm. In
particular the previous surface following experiments made use
of Roll and Pitch to accurately maintain a normal orientation
to a surface. Instead, we rely on a custom 3D-printed ﬂange so
the sensor is mounted perpendicular to the end effector. In this
way, we can make use of the Yaw DoF of the DOBOT MG400
when maintaining normal orientation to a surface varying in
only one direction, as shown in Fig. 1(d).

Moreover, whilst it was previously claimed that the ap-
proach used in [1] should be applicable to “a broad range of
other high-resolution optical tactile sensors, including sensors
of the Gelsight type”, it was only demonstrated with a hemi-
spherical and ﬂat TacTip. Here we validate that the approach
works with the DIGIT sensor of the Gelsight-type, which has
a distinct tactile sensing principle based on image shading
in place of marker-based transduction. We also validate the
approach with alternative forms of the TacTip sensor, including
a new (more compact) version of TacTip than used in [1] and

(a) DIGIT(b) DigiTac(c) TacTip4

the DigiTac which has a TacTip skin on a DIGIT housing.
When compared with the original work these sensors introduce
differences in camera perspectives, lens dynamics, marker size,
density of markers and have signiﬁcantly different lighting
conditions. To do this, we extended the simulation to include
simulated sensors that matched real hardware using the CAD
ﬁles for 3D-printing those sensors.

C. Deep RL with Different Tactile Dynamics in Simulation

The sim-to-real deep RL framework uses the Tactile
Gym [1] (see above) to simulate the contact dynamics with
rigid-body physics, using tactile information rendered as depth
images relative to the CAD model for the sensor. Thus, the
learnt tactile-feedback policies from [1] apply only the TacTip
originally considered in that study.

Hence, we extend the Tactile Gym [1] with two new virtual
optical tactile sensors: DIGIT (Gelsight-style) and DigiTac
(TacTip-style) (Fig. 2 a,b), based on their open-source CAD
ﬁles [7] used for 3D-printing the DIGIT. We follow the method
described in [1] to efﬁciently capture the depth images as
tactile images by synthetic cameras embedded within those
sensors. Speciﬁcally, we adjust the physics (friction, stiffness,
damping, etc.) of the skin and the parameters of the camera for
each sensor, to achieve realistic performance during learning.
The three distinct tactile observation spaces corresponding
to the three type of sensor (Fig. 2) are then used as input
to train deep RL polices for the tasks, following the methods
detailed in [1]. We report on the training results in Sec. IV.

D. Sim-to-real Transfer for Tactile Images

Here we aim for zero-shot learning so the learned policy in
simulation can be transferred to the real-world task without
further training or tuning. Hence it is essential to have a
model
to bridge the gap between the simulated and real
domains. Progress in Generative Adversarial Network (GAN)
methods has enabled realistic image generation, which we
leverage to learn an image-to-image translation GAN [26]
applied to real-to-sim tactile image translation. The network
takes advantage of a U-net architecture [27] for the image-
conditioned generator to infer better the spatial features during
training, alongside a standard convolutional network [28] with
batch normalization [29] for the discriminator.

Since each of the three optical tactile sensors considered
here have a different design and illumination,
the image-
preprocessing and hyper-parameters need tuning for each
sensor. The arrays of markers in the TacTip and the DigiTac
reﬂect light more clearly, which eases the ﬁne-tuning of their
image processing compared to the DIGIT, where the image
shading is more subtle. However, after tuning we ﬁnd that all
sensors work effectively for sim-to-real transfer, which we will
cover in the results sections later.

E. Sim-to-real Data Collection

The three manipulation tasks considered here require dis-
tinct sim-to-real models across two distinguishing contact
features: an edge for the edge-following task, and a surface

Table II
Sensor pose sampling ranges used during data collection. Sensor poses are
expressed relative to a ﬁxed coordinate frame attached to the training feature
(Rz = axial rotation around z-axis).

Contact Features
Axis

Sensors

Edge

Surface

y (mm) z (mm) Rz (deg)

x (mm) Rz (deg.)

Tactip
DigiTac
DIGIT

[-6,6]
[-5,5]
[-5,5]

[2,5]
[2,4]
[2,3]

[-179,180]
[-179,180]
[-179,180]

[1,4]
[1,3]
[1,2]

[-15,15]
[-15,15]
[-11,11]

for the surface-following and object-pushing tasks. We collect
a training and validation dataset per contact feature (edge and
surface) and per sensor, leading to twelve datasets in total.

Each training dataset comprised 5000 tactile images and
each validation dataset 2000 tactile images, collected by using
the desktop tactile robot (Sec.III A) to randomly contact
data of the appropriate edge or surface feature. For the data
collection, straight edge and ﬂat surface of a 3D-printed
stimulus, labeled with the relative poses between the sensor
and the stimulus under contact. The movement ranges for each
random contact feature are shown in Table II. These datasets
take about 6 hours to collect on the physical robot and less
than 1 minute to collect in simulation.

A further subtlety for the DIGIT and the DigiTac is that
they are no longer symmetric (being broader across one axis)
unlike the original TacTip. This meant we needed to customize
the range of (x, y)-pose data collection depending on the
rotation angle, which was implemented by scaling the y-range
orthogonal to the edge by the tangent of the angle.

F. Tactile Control Tasks

Here we adapt three tactile control tasks proposed in [1]
to the desktop tactile robot: edge-following, surface-following
and object-pushing. Although we expect our platform would
also be viable for the ball-rolling task, we do not implement
it in this work because the DigiTac does not currently have a
ﬂat skin that is suitable for ball-rolling.

1) Object Pushing: This task aims for the robot to push an
object through a sequence of goal positions along a trajectory
on a ﬂat surface. Three trajectories are considered: straight,
curved and sinusoidal. In practise, each trajectory was divided
into 10 equal-length sections with the ﬁnal point on each
section speciﬁed as the goal position; thus, there are ten goals
for each trajectory. The 2D action space comprises the x-
position and rotation angle of the tool center point (TCP)
located at the tip of the tactile sensor. We use three distinct
objects with different shapes and weights (Fig. 3c): a triangular
prism, cube and cylinder varying in weight from 185 g to
363 g. These objects differ from those used in [1], which were
lighter of mass (50 g to 80 g), to better suit the elasticity of
the DIGIT that required a heavier object for an appreciable
tactile deformation. An ArUco marker is place on top of each
object to track its movement for comparison with the ground
truth using the tracking method described in [30].

2) Edge Following: This task aims for the robot to slide
the tactile sensor along a contacted edge while maintaining a
ﬁxed distance between the edge and the TCP located at the

5

Figure 3. Tactile stimuli used in manipulation tasks: (a) square, clover leaf, and teardrop stimuli for the edge following task; (b) arch, ﬂower, and disc for the
(side) surface following task; (c) blue cylinder, yellow triangular prism, and red cube for the object pushing task (ArUco markers for ground truth tracking).

tip of the tactile sensor. The 2D action space comprises the
x and y position of the TCP. To evaluate the robustness and
maneuverability of the tactile robot, we use three stimuli that
contain interesting geometrical features such as straight edges,
positively/negatively curved edges and a right-angled corner
(Fig. 3a). To measure tracking performance, the ground truth
shapes are extracted from the CAD models of these 3D-printed
objects by importing into the Blender CAD software [31] and
outputting the point clouds of their boundaries.

3) Surface Following: This task aims for the robot to slide
the tactile sensor over a contacted surface while maintaining
a ﬁxed contact depth and orientating the TCP representing
the tip of the sensor normal to that surface. The 2D action
space comprises the y-position and rotation angle of the TCP.
To evaluate the robustness and maneuverability of the tactile
robot, we use the side surfaces of three objects comprising
closed-loop 2D surfaces in 3D space (Fig. 3b) that contain
interesting geometrical features such as locally planar, concave
and convex surfaces. As with the edge-following task, the
ground truth shapes and dimensions of the 3D-printed stimuli
are obtained from the CAD models using Blender.

IV. RESULTS

A. RL Performance in the Simulated Environments

The deep RL method PPO [32] is here used to learn
policies for the three simulated optical
tactile sensors on
the three considered tasks (pushing, edge-following, surface-
following). Speciﬁcally, we use the Stable-Baselines-3 [33]
implementation of PPO for training the policies.

The results of training in simulation are given in Fig. 4.
Although there are slight differences in performance during
training between the sensors, all the learned policies achieved
similar accurate ﬁnal performances in all tasks. We attribute
these small differences to the different contact dynamics due to
the different shapes of the tips for the three tactile sensors, but
as we mentioned this does not affect the overall performance.
In particular, we did not ﬁnd it necessary to ﬁne-tune the
training hyperparameters for each tactile sensor. In addition,
we apply the learned policy in each simulated task to the real
robot later without any further ﬁne-tuning.

Table III
Mean SSIM values for image translation GANs trained with edge and
surface features (values closer to 1.0 represent better image translation).

Sensors

Features
Edge
Surface

Tactip DigiTac DIGIT

0.9956
0.9927

0.9953
0.9932

0.9867
0.9818

B. Image Translation for Sim-to-real Transfer

We use the Structural Similarity Index (SSIM) to evaluate
the quality of the translated images for both the edge and
surface validation datasets (Table III). The high SSIM values
close to unity (perfect match) obtained with the trained model
shows that the image-to-image translation produces accurate
tactile images for all sensors. Therefore, the method imple-
mented from [1] is applicable not only for TacTip family of
tactile sensors but also for GelSight-style sensors such as the
DIGIT. Some examples of tactile images and their translated
versions for edge contacts are shown in Fig. 2.

that

As an aside, we comment

the performance of the
DIGIT image translation was slightly lower than the TacTip
and DigiTac. It is possible that a different architecture of neural
network more suited to the GelSight-type sensors could change
this result. We note that this minor difference does not affect
the task overall performance, which we attribute to the deep
RL policies being learnt with domain randomization.

C. RL Performance in the Physical Environments

1) Object Pushing: We consider ﬁrst the object pushing
task, where the tactile robot must move the object along a
desired trajectory through a series of goal points. Successful
task performance corresponds to accurately pushing the test
object (a cube, cylinder or triangular prism) along the trajec-
tories (straight: y = kx, curved: y = 0.001x2, sinuosoidal:
y = 0.02 sin(0.02x), where k ∈ [−0.3, 0.3], x ∈ [0, 200] mm),
using feedback from the tactile sensor to maintain the object
on its desired path.

The tactile robot successfully pushed the object along its
desired trajectory, with a typical mean Euclidean distance
the actual trajectory from its intended trajectory of ∼10 mm

Figure 4. Averaged training performance of learned policies for different sensors in (a) edge-following, (b) surface-following, (c) object-pushing tasks.

(c)(b)(a)(a)(b)((cid:70))(a)Edge-following(b)Surface-following(c)Object-pushing6

Table IV
Mean Euclidean distances of the actual trajectories from the ground-truth trajectories for the object pushing task. The numbers in bold denote the best result
among the three sensors for the same object and trajectory. Failure cases are denoted "N/A". All the experiments are using the objects without additional
weights except the ﬁnal row "DIGIT \ (weighted)" where we increase all the objects’ weights by 150 g.

Trajectories

Obj.

Sensors

Tactip
DigiTac
DIGIT
DIGIT \ (weighted)

Straight

Curve

Sine

Cube

Cylinder

Tri. Prism

Cube

Cylinder

Tri. Prism

Cube

Cylinder

Tri. Prism

10.33 mm 9.21 mm 11.51 mm 12.19 mm 11.29 mm 13.20 mm 11.93 mm 11.30 mm 13.89 mm
11.25 mm 10.24 mm 16.09 mm 13.01 mm 11.20 mm 16.41 mm 12.32 mm 11.48 mm 15.13 mm
11.20 mm 10.13 mm
10.92 mm 11.00 mm 16.65 mm 12.28 mm 11.51 mm 16.58 mm 12.07 mm 11.53 mm 17.06 mm

12.94 mm 12.00 mm

12.41 mm 11.33 mm

N/A

N/A

N/A

(ranging from 50-200 g) using the same deep RL and GAN
models. The results (Table V) show that
the performance
improves with increased object weight upto 150 g after which
there was no beneﬁt. We expect this is because the additional
weight helps the tactile images lie within the distribution of
the GAN training for accurate real-to-sim transfer. Once in
this range, the bottleneck on the performance is the RL policy
instead of the GAN.

2) Edge Following: Next, we consider the edge following
task, where the tactile robot must slide the sensor around the
edge of various planar objects with geometrical features such
as curved edges and a corner. We note that the sim-to-real
image translation was trained only on a straight edge, but as
we see below the method generalizes to more complex shapes.
In all cases, the tactile robot successfully completed the
edge-following trajectory, with typical mean Euclidean posi-
tion errors of 0.6-1.4 mm for the DigiTac and TacTip, and
0.9-1.8 mm for the DIGIT (Table VI). The successful task
performance is shown by the trajectories superimposed on
the ground truth shapes (Fig. 6) and videos are provided in
supplementary material. Again these results are comparable
to those reported in [1] and also for servo control under
supervised learning of the pose [34].

Examining the results more closely, the DIGIT can achieve
accurate performance (mean Euclidean position errors of
1.5 mm overall) when traversing the edge contours despite
having some regions of larger error (Fig. 6 top row, coloured
regions up to 5 mm error). This is because the ﬂatter stiffer
elastomer of the DIGIT sensing surface causes an increased
sensitivity to small deviations in penetration distance while
moving around the object, relative to the softer TacTip and
DigiTac sensing surfaces. For sufﬁcient deformation of the
DIGIT sensor to give good performance, a more forceful
contact needs to be applied, which also increases the frictional
force. To avoid damaging the sensor, we mitigate this friction
by coating the objects with wax to ease the sliding motion.

3) Surface Following: Finally, we consider the surface
following task, where the tactile sensor must slide around
the curved surfaces of various objects with vertical walls,
which have geometrical features such as concave and convex
surfaces. We note that the sim-to-real image translation was
trained only on a planar surface, but as we see below the
method generalizes to more complex shapes.

The tactile robot successfully completed the surface-
following trajectory, with typical mean Euclidean position
errors of 0.6-1.2 mm for the DigiTac and TacTip, and the

Figure 5. The tactile robot executing 3 pushing policies: (a) triangular prism
along a curve trajectory with the DigiTac, (b) cube along a straight line with
the DIGIT, (c) cylinder along a sinusoidal trajectory with the TacTip. The
plots in the most left columns are the objects actual paths recorded by the
tracking system; the right four columns are snapshots taken from the camera.

Table V
Mean Euclidean distances of the trajectories from the ground truth for the
speciﬁc object (triangle prism) pushing task using DIGIT. The number in
bold denotes the best result among the weights.

Traj.

Weights

185g + 0g
185g + 50g
185g + 100g
185g + 150g
185g + 200g

Straight

Curve

Sine

N/A

N/A

N/A
18.05 mm 17.73 mm 17.95 mm
16.93 mm 17.54 mm 18.12 mm
16.65 mm 16.58 mm 17.06 mm
17.17 mm 16.92 mm 17.26 mm

(Table IV), compared to an overall distance travelled of 200-
280 mm (250 steps in total for each episode) and a sensor
tip size of 20-40 mm. The successful task performance is also
indicated by snapshots taken from the trajectories (Fig. 5) and
videos are provided in supplementary material. Overall, the
performance when successfully pushing the objects is similar
to that reported in [1].

Examining the results more closely, the TacTip performs
slightly better than the DigiTac with accuracies of 9-13 mm
compared with 11-16 mm, which we attribute to a stabler
push due to the larger contact surface. The DIGIT has similar
accuracy to the DigiTac, but failed at the pushing task with
the triangular prism for all trajectories. We hypothesise that
the main reason of this failure is that the triangular prism
is the lightest object (185g) and the DIGIT has a relatively
stiff elastomer compared with the TacTip and DigiTac, which
causes the tactile image translation to fail on this object. To
validate this hypothesis, we extend the experiments with the
DIGIT to pushing the triangle prism with additional weights

xposition(mm)yposition(mm)yposition(mm)yposition(mm)10020403050Error(mm)(c)TacTip(b)DIGIT(a)DigiTact=0st=8st=16st=24st=0st=8st=16st=24st=0st=8st=16st=24sxposition(mm)xposition(mm)7

Table VI
Mean Euclidean distances of the trajectories from the ground truth for the
edge following task. The best result from the three sensors is shown in bold.

Sensors
DIGIT
DigiTac
TacTip

Obj.

Square

Clover

Foil

0.88 mm 1.71 mm 1.82 mm
1.04 mm 0.85 mm 0.86 mm
0.67 mm
0.63 mm 1.42 mm

Table VII
Mean Euclidean distances of the actual trajectories from the ground truth
trajectories for the surface following task. The number in bold denotes the
best result from the three sensors. Failure cases are indicated by "N/A".

Sensors
DIGIT
DigiTac
TacTip

Obj.

Arch

Flower

Circle

N/A

0.47 mm
N/A
0.79 mm 1.04 mm 0.58 mm
0.91 mm 1.23 mm 0.59 mm

V. DISCUSSION AND FUTURE WORK

In this paper, we developed a low-cost tactile robot platform
for sim-to-real deep reinforcement learning based on Tactile
Gym [1]. The hardware included a desktop robot (DOBOT
MG400) and three low-cost high-resolution optical
tactile
sensors: the TacTip, DIGIT, and DigiTac. We also integrated
CAD models of the DOBOT MG400 and the considered
sensors into the Tactile Gym and successfully learned policies
for all sensors in several physically-interactive tasks involving
different contact dynamics. To train an effective GAN model
for real-to-sim image translation, we ﬁne-tuned the image
preprocessing parameters and calibrated the internal camera
of each simulated tactile sensor so that the distribution of the
simulated dataset was well-matched with the real dataset.

low-cost

The performance of our

tactile sim-to-real
deep RL robot platform was evaluated in three real-world
physically-interactive tasks: edge-following, surface-following
and object-pushing. The experimental results show that the
developed platform is effective for all tasks with zero-shot
performance on real objects or trajectories unseen in the
simulation learning for all three tactile sensors. The main
differences in performance between the sensors were due to
the physical construction and material properties, rather than
the different sensing mechanisms. For example, the ﬂatter,
stiffer construction the DIGIT with a GelSight-type sensing
surface made it unsuited for following concave surfaces, unlike
the soft domed structure of the TacTip and DigiTac.

Such empirical studies should help other researchers select
and customize the appropriate physical characteristics of tac-
tile sensors for different manipulation scenarios. Overall, we
view the generality of our low-cost platform as opening up the
possibility to apply either TacTip-style or GelSight-style tactile
sensors to learning general sim-to-real deep RL policies for
desired complex behaviors. The tactile robot platform should
also beneﬁt sim-to-real prehensile and dexterous manipulation
tasks, for example by enabling the fundamental methods to
be developed in controlled scenarios before applying them to
more challenging applications with dexterous robot hands.

Acknowledgements: We thank Mike Lambeta and Roberto
Calandra for donating the DIGIT sensors. We thank Di Wu for

Figure 6. The tactile robot executing edge-following policies on 3 distinct
shapes for the (a) DIGIT, (b) DigiTac and (c) TacTip. The ground truth for
each object is shown in green and errors of the traced contour from the ground
truth are colour-coded (side colour bar). The blue arrow denotes the starting
point and direction.

Figure 7. The tactile robot executing 3 surface-following policies on 3 distinct
shapes, corresponding to the (a) DIGIT, (b) DigiTac and (c) TacTip. The
ground truth for each object is shown in green and errors of the traced contour
from the ground truth are colour-coded (side colour bar). The blue arrow
denotes the starting point and direction. The DIGIT failed to follow the arch
and ﬂower objects at points denoted by the red crosses.

DIGIT giving the most accurate trajectory of 0.5 mm error on
the circular wall (compared to 0.6 mm for the other sensors).
The successful task performance is shown by the trajectories
superimposed on the ground truth shapes (Fig. 7) and videos
are provided in supplementary material. Again these results
are comparable to those reported in [1] and also for servo
control under supervised learning of the pose [34].

During the experiments, we observed that the DIGIT tends
to get stuck in the concave-shape surface (shown in the supple-
mentary video). This is again because of the DIGIT’s ﬂatter,
stiffer sensing surface which hinders its sliding movement over
concave surfaces. To avoid breaking the sensor, we decided not
to conduct this task with DIGIT on the ﬂower and the arch.

(a) DIGIT(b) DigiTac(c) TacTip(a) DIGIT(b) DigiTac(c) TacTipN/AN/A8

her preliminary work on the DOBOT hardware development.
We thank Andrew Stinchcombe for helping with the 3D-
printing of the stimuli.

REFERENCES

[1] A. Church, J. Lloyd, R. Hadsell, and N. Lepora. Tactile Sim-to-Real
Policy Transfer via Real-to-Sim Image Translation. In Proceedings of the
5th Conference on Robot Learning, pages 1645–1654. PMLR, October
2021.

[2] Niko Sünderhauf, Oliver Brock, Walter Scheirer, Raia Hadsell, Dieter
Fox, Jürgen Leitner, Ben Upcroft, Pieter Abbeel, Wolfram Burgard,
Michael Milford, et al. The limits and potentials of deep learning for
robotics. The International journal of robotics research, 37(4-5):405–
420, 2018.

[3] Siyuan Dong, Devesh K Jha, Diego Romeres, Sangwoon Kim, Daniel
Nikovski, and Alberto Rodriguez. Tactile-rl for insertion: Generalization
to objects of unknown geometry. In 2021 IEEE International Conference
on Robotics and Automation (ICRA), pages 6437–6443. IEEE, 2021.
[4] Huazhe Xu, Yuping Luo, Shaoxiong Wang, Trevor Darrell, and Roberto
Calandra. Towards learning to play piano with dexterous hands and
touch. arXiv preprint arXiv:2106.02040, 2021.

[5] Shaoxiong Wang, Mike Maroje Lambeta, Po-Wei Chou, and Roberto
Calandra. Tacto: A fast, ﬂexible, and open-source simulator for high-
resolution vision-based tactile sensors. IEEE Robotics and Automation
Letters, 2022.

[6] Mike Lambeta, Huazhe Xu, Jingwei Xu, Po-Wei Chou, Shaoxiong
Wang, Trevor Darrell, and Roberto Calandra. Pytouch: A machine
In 2021 IEEE International
learning library for touch processing.
Conference on Robotics and Automation (ICRA), pages 13208–13214.
IEEE, 2021.

[7] Mike Lambeta, Po-Wei Chou, Stephen Tian, Brian Yang, Benjamin
Maloon, Victoria Rose Most, Dave Stroud, Raymond Santos, Ahmad
Byagowi, Gregg Kammerer, et al. Digit: A novel design for a low-
cost compact high-resolution tactile sensor with application to in-hand
manipulation. IEEE Robotics and Automation Letters, 5(3):3838–3845,
2020.

[8] Benjamin Ward-Cherrier, Nicholas Pestell, Luke Cramphorn, Benjamin
Winstone, Maria Elena Giannaccini, Jonathan Rossiter, and Nathan F
Lepora. The tactip family: Soft optical tactile sensors with 3d-printed
biomimetic morphologies. Soft robotics, 5(2):216–227, 2018.

[19] Carmelo Sferrazza and Raffaello D’Andrea.

Sim-to-real for high-
tactile sensing: From images to 3d contact force

resolution optical
distributions. arXiv preprint arXiv:2012.11295, 2020.

[20] Thomas Bi, Carmelo Sferrazza, and Raffaello D’Andrea. Zero-shot
sim-to-real transfer of tactile control policies for aggressive swing-up
manipulation. IEEE Robotics and Automation Letters, 2021.

[21] Zihan Ding, Nathan F. Lepora, and Edward Johns. Sim-to-Real Transfer

for Optical Tactile Sensing. ICRA, pages 1639–1645, 2020.

[22] Daniel Fernandes Gomes, Paolo Paoletti, and Shan Luo. Generation
IEEE Robotics and

of gelsight tactile images for sim2real learning.
Automation Letters, 6(2):4177–4184, 2021.

[23] Zilin Si and Wenzhen Yuan. Taxim: An example-based simulation model
for gelsight tactile sensors. IEEE Robotics and Automation Letters, 2022.
[24] Alexander C Abad and Anuradha Ranasinghe. Visuotactile sensors
IEEE Sensors Journal,

with emphasis on gelsight sensor: A review.
20(14):7628–7638, 2020.

[25] Wenzhen Yuan, Siyuan Dong, and Edward H Adelson. Gelsight:
High-resolution robot tactile sensors for estimating geometry and force.
Sensors, 17(12):2762, 2017.

[26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-
image translation with conditional adversarial networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition,
pages 1125–1134, 2017.

[27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convo-
lutional networks for biomedical image segmentation. In International
Conference on Medical image computing and computer-assisted inter-
vention, pages 234–241. Springer, 2015.

[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks. Advances in
neural information processing systems, 25, 2012.

[29] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In Interna-
tional conference on machine learning, pages 448–456. PMLR, 2015.
[30] John Lloyd and Nathan F Lepora. Goal-driven robotic pushing using
IEEE Transactions on Robotics,

tactile and proprioceptive feedback.
2021.

[31] Blender Online Community. Blender - a 3D modelling and rendering
package. Blender Foundation, Stichting Blender Foundation, Amster-
dam, 2018.

[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and
Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint
arXiv:1707.06347, 2017.

[9] Nathan F. Lepora. Soft biomimetic optical tactile sensing with the tactip:

[33] Antonin Rafﬁn, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi

A review. IEEE Sensors Journal, 21(19):21131–21143, 2021.

Kanervisto, and Noah Dormann. Stable baselines3, 2019.

[34] Nathan F Lepora and John Lloyd. Pose-based tactile servoing: Con-
IEEE Robotics & Automation

trolled soft touch using deep learning.
Magazine, 28(4):43–55, 2021.

[10] Herke Van Hoof, Nutan Chen, Maximilian Karl, Patrick van der Smagt,
and Jan Peters. Stable reinforcement learning with autoencoders for
tactile and visual data. In 2016 IEEE/RSJ international conference on
intelligent robots and systems (IROS), pages 3928–3934. IEEE, 2016.

[11] Filipe Veiga, Dominik Notz, Thomas Hesse, and Jan Peters. Tactile
based forward modeling for contact location control. In RSS Workshop
on Tactile Sensing for Manipulation, 2017.

[12] Stephen Tian, Frederik Ebert, Dinesh Jayaraman, Mayur Mudigonda,
Chelsea Finn, Roberto Calandra, and Sergey Levine. Manipulation
In 2019
by feel: Touch-based control with deep predictive models.
International Conference on Robotics and Automation (ICRA), pages
818–824. IEEE, 2019.

[13] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function
approximation error in actor-critic methods. In International conference
on machine learning, pages 1587–1596. PMLR, 2018.

[14] Sangwoon Kim and Alberto Rodriguez. Active extrinsic contact
sensing: Application to general peg-in-hole insertion. arXiv preprint
arXiv:2110.03555, 2021.

[15] Alex Church, John Lloyd, Raia Hadsell, and Nathan F Lepora. Deep
reinforcement learning for tactile robotics: Learning to type on a braille
IEEE Robotics and Automation Letters, 5(4):6145–6152,
keyboard.
2020.

[16] Yashraj S Narang, Balakumar Sundaralingam, Karl Van Wyk, Arsalan
Mousavian, and Dieter Fox. Interpreting and predicting tactile signals
for the syntouch biotac. arXiv preprint arXiv:2101.05452, 2021.
[17] Yashraj Narang, Balakumar Sundaralingam, Miles Macklin, Arsalan
Mousavian, and Dieter Fox. Sim-to-real for robotic tactile sensing via
physics-based simulation and learned latent projections. arXiv preprint
arXiv:2103.16747, 2021.

[18] Carmelo Sferrazza, Thomas Bi, and Raffaello D’Andrea. Learning
the sense of touch in simulation: a sim-to-real strategy for vision-
based tactile sensing. In 2020 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), pages 4389–4396. IEEE, 2020.

