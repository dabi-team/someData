2
2
0
2

n
u
J

9

]

G
L
.
s
c
[

1
v
8
8
6
4
0
.
6
0
2
2
:
v
i
X
r
a

NNTrainer: Light-Weight On-Device Training Framework

Ji Joong Moon1 Parichay Kapoor1

Ji Hoon Lee1 Myung Joo Ham1 Hyun Suk Lee1

1Samsung Research, Samsung Electronics
1{jijoong.moon, pk.kapoor, jhoon.it.lee, myungjoo.ham, hs89.lee, eogus.kim}@samsung.com

Abstract
Modern consumer electronic devices have adopted deep
learning-based intelligence services for their key features.
Vendors have recently started to execute intelligence services
on devices to preserve personal data in devices, reduce net-
work and cloud costs. We ﬁnd such a trend as the opportunity
to personalize intelligence services by updating neural net-
works with user data without exposing the data out of devices:
on-device training. For example, we may add a new class, “my
dog, Alpha”, for robotic vacuums, adapt speech recognition
for the user’s accent, let text-to-speech speak as if the user
speaks. However, the resource limitations of target devices
incur signiﬁcant difﬁculties.

We propose NNTrainer, a light-weight on-device training
framework. We describe optimization techniques for neu-
ral networks implemented by NNTrainer, which are evalu-
ated along with the conventional. The evaluations show that
NNTrainer can reduce memory consumption down to 1/28
(saving 96.5%!) without deteriorating accuracy or training
time and effectively personalizes applications on devices.
NNTrainer is cross-platform and practical open source soft-
ware, which is being deployed to millions of devices in the
authors’ afﬁliation.

1 Introduction

We have witnessed the wide and rapid proliferation of in-
telligence services based on deep neural networks in the in-
dustry for a wide range of consumer electronics products.
For example, image classiﬁcation [30, 66], object detection,
and speech recognition [10, 21] are commonly applied to
various key features of consumer electronics such as segmen-
tation [42] for smartphone camera, super-resolution [68] for
TV, ASR [33]/TTS [47] for real-time translation. As a result,
developing better intelligent services has become signiﬁcantly
important for the industry. Therefore, huge amounts of effort
and cost have been invested to build applications using Ma-
chine Learning, where a big section is optimizing various
neural network models.

Recently, after the proliferation of intelligence services in
consumer electronics, the need for meeting the different envi-
ronments and requirements of each individual user, “personal-
ization”, has arisen with additional technical difﬁculties [27].
Because we use general data, not personal data of an individ-
ual user to train a neural network model, the model may look
over-parameterized or its quality of service is low compared
to the size of the model. To satisfy such personalization needs,
we propose to update models on devices with the data avail-
able on devices: “on-device training”, which has the following
advantages.

Personalizing intelligence services by training and updating
neural networks on devices enables various applications. We
may increase the accuracy for a given user with personal data
by ﬁne-tuning the corresponding model. We may also add
classes declared by a user to personalize the corresponding
service. By reducing over-parameterized parts of models and
lightening the models for the given environment of a user, we
may reduce service latency.

Training models on devices instead of remote clouds pro-
vides another advantage. Running intelligence services on
devices, “on-device AI” [17, 39], saves cloud operating costs
(we have hundred of millions of active mobile phones and
even more consumer electronics and IoT devices deployed!).
The more signiﬁcant advantage of on-device AI is that it
protects personal data and privacy by keeping related data
in devices. By training models on devices, we can achieve
the same advantages over conventional remote cloud-based
training. Besides, regulations and consumers’ expectations
on personal data and privacy are becoming more strict. For
example, General Data Protection Regulation (GDPR) [63]
in EU makes it extremely difﬁcult to gather users’ personal
activity records in cloud systems to update a given neural
network model. With a device frequently used (e.g., a mobile
phone), personal data is continuously added and updated, we
can provide continuous personalization with online or con-
tinual learning techniques [11, 52, 59] on device. Besides, in
order to apply federated learning [14, 32], on-device training
mechanism is required, anyway.

 
 
 
 
 
 
Training models for personalization on devices pose chal-
lenges because of the limited resources. The followings are
such challenges and proposed approaches:

Memory: consumer electronic devices usually have a much
smaller size of main memory compared to servers: several
GiBs to tens of MiBs, compared to hundreds of GiBs on
servers. Although the latest high-end mobile phones often
have over 8 GiB of main memory, it is critical to use much
less memory by an application because there are multiple
applications running simultaneously, especially in Android,
which already incurs signiﬁcant issues of insufﬁcient memory
and requires sophisticated techniques to mitigate [7,22,31,50].
Besides, we target to apply on-device training and personal-
ization to budget phones, wearable devices, televisions, and
other IoT devices along with high-end mobile phones, where a
personalization service is expected to consume as little mem-
ory as possible; i.e., ranging from a few MiB to a few 100
MiB. To reduce the memory consumption of neural network
training, we propose general techniques that can be applied
to conventional training mechanisms, save memory signiﬁ-
cantly, without deteriorating the performance or accuracy. The
proposed techniques include intermediate activation manage-
ment, ﬁne-grained memory management, and in-place cal-
culation. Evaluations with synthetic benchmarks and actual
applications for products suggest that such techniques can re-
duce memory consumption down to 1/3x and 1/28x. Note that
even with server-based training, we have witnessed many AI
developers in our afﬁliation challenged by the memory sizes;
thus, such techniques can improve conventional mechanisms
including TensorFlow and PyTorch by allowing larger batch
sizes.

Computation: GPUs are primarily used for training ML
models in servers, but CPU utilization still remains critical
in consumer electronics; a majority of devices either do not
have GPUs capable of general computing or have GPUs dedi-
cated for graphical user interfaces (TVs and mobile phones).
Further, training mechanisms use batched memory chunks
grouping multiple data frames, unlike single data frame in in-
ference. However, if a sufﬁcient batch size cannot be secured
due to large memory requirement, the computation efﬁciency
is low, as these computations are highly sensitive to cache
utilization. In particular, we have to pay more attention to
layers with low compute-to-memory ratio (OP/bytes) such as
Concatenate, Addition, Split, ReLU.

Conﬁguration of models on device: Although we usu-
ally update or adapt pre-trained models on devices, on-device
training mechanisms still need to have the entire training
features, including feed-forward [4, 57] and recurrent net-
work [37, 38]. More importantly, the model itself must be
conﬁgurable within the device which is a critical requirement
for transfer learning [36, 46, 58, 60], and for the users to be
able to reuse and train sections of previously developed mod-
els. Apart from this, it is also important to support custom
extensions for external developers.

Our contributions can be summarized as follows:

• We implement Neural Network Trainer (NNTrainer), a
light-weight on-device neural network training framework,
which is not a research prototype, but a production-ready
implementation. Exposing its features with APIs, it efﬁ-
ciently trains AI models on devices with minimal memory
consumption and latency, which is veriﬁed by evaluations
with conventional state-of-the-art frameworks. It is an
open source software project1 accepting contributions of
the general public and deployed for different software
platforms and devices.

• We identify and implement optimization points that sig-
niﬁcantly reduce memory consumption without deteriorat-
ing performance based on the analysis of neural network
training mechanisms. In an application, peak memory con-
sumption is reduced to 1/28, allowing mobile phones to
adopt on-device training. Moreover, such optimization
points are generic enough to be adapted to conventional
frameworks for machine learning servers.

• We evaluate various neural network models on a Raspber-
ryPI device, showing that NNTrainer is highly effective
and efﬁcient. We also demonstrate that complex AI mod-
els such as Tacotron2 [51], with over 100 length time se-
quence LSTM [16] based encoder-decoder model, can be
personalized with sufﬁcient batch size on mobile phones,
which is scheduled to be deployed for 2022 products.

2 Related Work

There are studies to improve training efﬁciency because it
requires huge computation and memory. For example, it takes
a week to train MobilenetV2 [49] with 8 GPUs for ImageNet
data [12] even with servers having huge computing resources.
Multiple studies [9,26,40,45] have addressed the workload-
balancing problem of training for heterogeneous systems. The
load-balancing problem for heterogeneous systems has been
discussed since the introduction of supercomputing and is now
evolved to optimize the deep neural network model training
instead of numerical calculation. The prior art mainly pro-
poses to increase GPU utilization and reduce communication
costs. Yimin [26] proposes a scheduler considering communi-
cation bandwidth and computation for training. Deepak [40]
addresses fairness between multiple simultaneous users and
tasks and suggests a scheduler as an optimization problem
solver. On the other hand, Kim [29] proposes a scheduling
method for a single-layer workload of on devices for infer-
ences with quantized models.

There are other techniques [15, 35, 67, 69] for special cases.
Yizhi [35] and Chengliang [69] propose an optimization tech-
nique for inferences without training. There are optimization
techniques for specialized networks; Shizhen [67] proposes

1https://github.com/nnstreamer/nntrainer

2

an efﬁcient runtime system for recurrent networks and Jia [15]
proposes optimization for malware detection.

G(X0) = Gn(Gn−1(...G1(G0(X0)))) as a sequence of layers
from the implementation point of view.

There are studies to optimize the computational graphs
of neural networks, in which an individual layer or tensor
is a graph node. Jia [25] proposes a graph optimizer that
automatically generates graph substitutions and Tianqi [8]
proposes a graph optimization compiler.

Neural network model compression [5,34] has been studied
as well. Compressed models may consume less memory and
power, and they usually take less time to run at the cost of
accuracy loss.

The mentioned techniques have unique contributions to
neural network optimization. However, surprisingly, there are
few research efforts on reducing memory consumption of
training. A notable study is TinyTL [6], which proposes to
train a residual network along with a backbone for transfer
learning. Because TinyTL trains a relatively smaller network
than the backbone, it requires a smaller amount of memory;
however, it requires to changing the architecture of the net-
work, making it difﬁcult to generalize. Moreover, TinyTL has
evaluated the memory consumption without actual training
frameworks, but with theoretical analysis.

TensorFlow [1] from Google is a popular open source ma-
chine learning framework, which includes Keras and various
tools for developers. A lightweight variant of TensorFlow,
called TensorFlow-lite, targets on-device AI; however, it does
not train full neural network models, but only supports transfer
learning, and plans to support limited features of training.

PyTorch [44], including Caffe2 [24], is another popular
open source machine learning framework from Facebook,
which includes highly intuitive APIs. It has an experimental
(beta) lightweight variant, PyTorch Mobile, which runs mobile
models converted from PyTorch models. Note that PyTorch
Mobile targets inferences and does not support training.

Both TensorFlow and PyTorch consume an excessive
amount of memory as we show later in Evaluation (§5), which
make it impossible to use in consumer electronics; i.e., the
quality assurance team does not allow in-house applications
consuming memory excessively because the main memory
is supposed to be crowded by a large number of applications
preloaded for the user responsiveness. Note that optimizing
the memory consumption of neural network training is bene-
ﬁcial for conventional server-based machine learning appli-
cations; i.e., developers may increase the batch size further
with more efﬁcient memory usage.

3 Analysis of Deep Neural Network Training

This section elaborates the neural network training process
from the aspect of resource management, and then calcu-
lates the ideal minimum memory requirement of training. In
Figure 1, we show the conventional forward and backward
processes. Let us denote the network T = {Y, G(X0)} and

3

During the forward process, the layer simply takes the Xi
(cid:48) as in
as an input, calculates Gi, and saves the output into Xi
(cid:48) need to be saved for the gradi-
Figure 1 (a). These Xi and Xi
ent calculation during backward operation. Unlike forward,
the backward process starts from the last layer, in which the
loss is calculated and propagated accordingly. First, calculate
gradient ∆Wi for weight update with ∆Di and Xi saved at the
forward process and update Weight Wi using the optimizer.
(cid:48) to propagate to Li−1
Then, calculate the return derivative, ∆Di
with ∆Di and Wi.

Depending on the type of layer Gi(Xi), a layer requires
buffers for weight Wi, gradient ∆Wi, input Xi, and output
Gi(Xi). Usually, the sizes of these depend on the input size
and the depth of the neural network. When the input size
and the network conﬁguration are determined, the amount
of memory required for training can be roughly calculated.
Assume that there is a convolution 2D layer with the same
padding, 1 × 1 stride, and 64 ﬁlters with 3 × 3 size, which
takes an image (32 × 32 × 3) as an input. Then, the size of
the input is 0.39 MiB for 32 batch size (width × height ×
channels (3) × sizeof (ﬂoat) × batch size), and the output
buffer size is 8.3 MiB (width × height × channels (64) ×
sizeof (ﬂoat) × batch size). We also need the same amount
of memory for the backward process (∆Di, ∆Di+1) and addi-
tional buffers for the weight (Wi) and gradient (∆Wi), which
is about 1.3 kiB each. Therefore, we need about 16.6 MiB of
heap memory to train a single layer. Because we usually use
“deep” neural networks, we need multiple times, often over a
hundred times, of such amount of memory depending on the
depth of a neural network model.

Figure 1 (b) shows the memory buffers for neural network
training. It repeatedly reuses the memory buffer conﬁgura-
tion for a single layer of Figure 1 (a), which is depicted as
a dotted rectangle in Figure 1 (b). Conducting forward and
backward process for n layers requires memory space of about
n/2 times of the memory space required to conduct for a sin-
(cid:48), and the input of Li+1,
gle layer. Because the output of Li, Xi
Xi+1, represent the same data and layers do not modify input
data, the two may share the same buffer instance. Besides,
we have also found that we can reduce memory further as
shown in Figure 1 (c). Although the output of each layer,
(cid:48), is stored for the backward process, the derivatives, ∆Di
Xi
and ∆Di+1, are not required after the completion of the layer’s
backward process. Therefore, memory space for all the deriva-
tives of the model can be shared. The same optimization can
also be applied to the gradients because they are not required
once the weight is updated with the gradients. In the mean-
while, fortunately, there are layers that may reuse the buffer
(cid:48) be
of input Xi for its output: e.g., activation layers. Let Xi
the output of a sigmoid activation function, then its deriva-
(cid:48)). During backward process, computing
tive ∆Di
(cid:48), which is the output of forward process, not
∆Di

(cid:48) = Xi
(cid:48) requires Xi

(cid:48)(1 − Xi

Figure 1: Memory buffer usage of forward and backward processes.

on the order of the right and left tensors of a given operator.
However, it is extremely difﬁcult to ﬁnd the order of tensor
operations because they vary according to how each devel-
oper implements their forward process and it is also difﬁcult
to handle Automatic Differentiation [3] simultaneously. On
the other head, layer operation basis frameworks can clearly
identify execution orders (EOs) such as forward, calculate
gradient, and calculate derivative; thus, we can minimize the
memory consumption. Moreover, layer operation basis frame-
works may compute more efﬁciently by fusing several tensor
operations. Figure 2 compares the memory requirement and
EOs of the two bases. NNTrainer uses layer operation basis
as in Figure 2 (b) because the resource utilization efﬁciency
signiﬁcantly matters in embedded devices including mobile
phones and consumer electronics.

4 Design and Implementation

This section introduces the components of NNTrainer and
elaborates its design and implementation. NNTrainer is a mod-
ularized framework written in C++, whose structure is shown
in Figure 3. Most NNTrainer components can be extended by
its users. It is cross-platform; ofﬁcial releases include Ubuntu,
Tizen, and Android to support various types of devices and
developer environments.

The training processes of NNTrainer can be categorized
as Load, Conﬁgure, Compile, Initialize, setData, and Train,
which is the order we elaborate.

Load: the output of Load is an instance of Model, which
has all the information including hyper-parameters and or-
chestrates all the components and processes. NNTrainer users
may describe a neural network model to be instantiated as
Model with an initialization ﬁle [41], C, or C++ APIs. Inter-
preters in Model Loader parse model descriptions and gen-
erate intermediate graph representations. Developers may
extend Interpreters to support conventional frameworks; we
provide an extension for TensorFlow-lite [1] models.

Figure 2: Tensors required to exist in memory at each step.

the input. Therefore, only one intermediate activation is re-
(cid:48); i.e., an in-place computation
quired to store the output Xi
shown as L3 : InplaceOp of Figure 1 (c). This allows freeing
the memory space storing inputs of activation layers. Because
activation layers are applied after most operations, including
convolution and linear layers, this method reduces the mem-
ory requirement of inputs by almost half. This is applied to
batch normalization as well.

We can drop a signiﬁcant part of buffers for inference to
conserve memory space. For an arbitrary number of layers,
(cid:48), is sufﬁcient.
allocating and alternating two buffers, Xi and Xi
The two are temporal buffers for inferences, and we do not
(cid:48) for backward process; thus, we do not need
need to save Xi
to save them. We may also remove the gradient buffer, ∆W ,
and the return derivative buffer, ∆D(cid:48). Besides, we do not need
buffers for batch calculation to further reduce memory con-
sumption.

An important characteristic of a training framework is the
base level of computation. This can be a layer operation basis
or a tensor operation basis, and each has its pros and cons. Ten-
sor operation basis frameworks cost less to develop because
only the forward process is required to be implemented; how-
ever, it is difﬁcult to systematically ﬁnd the minimum memory
requirement. We can reuse a signiﬁcant part of tensors based

4

Role
Identify and create a ﬂatten layer
Identify and create an activation layer

Realizer
Flatten
Activation
BatchNorm Identify and create a batch normalization layer
Multi-Out
Loss
Concat
Recurrent
Slice
Input

Identify and make connection to layers
If loss is cross entropy, remove the activation
Identify inputs and create Concatenate layer
Unroll the graph if there is a loop
Create sub-graph network in the backbone model
Identify and create an input layer

Table 1: Default Realizer subclasses in NNTrainer

The Model’s Graph cannot operate by itself although it is
created via Conﬁgure process. To operate, Realizer of Com-
piler needs to apply lowering operations; i.e., Realizer adds
layers or changes the order based on the analysis of graph
structures. Table 1 shows the default Realizer subclasses of
NNTrainer. Compile process computes connection and order-
ing with the provided Graph Optimizer subclasses, which is
critical for memory efﬁciency. Users may extend both Graph
Optimizer and Realizer by adding subclasses.

Initialize: in Initialize process, a ﬁnalize method of a Layer
subclass is executed while visiting each Layer instance in
Graph and Tensor instance is requested to Tensor Pool. As
speciﬁcation and data of Tensor are managed independently,
NNTrainer manages memory by separating it to Tensor Pool
and Memory Pool. Although a Tensor instance is requested, its
memory space is not yet allocated. After Tensor Pool creates
all requested Tensors instances throughout Model, Memory
Planner plans with Tensor instances given by Tensor Pool,
computes the offset of Memory Pool for each Tensor instance,
and assigns the memory for Tensor data. Memory Planner is
also extendable to support various management algorithms.
setData: To generate batch sized data for training in set-
Data process, NNTrainer provides DataSet, and users can pro-
vide training data using DataProducer, which is extendable.
DataProducer generates data for training and accumulates
the data in the Batch Queue up to the batch size. After the
described processes, we are ﬁnally ready for Train process.
In terms of resources, the uniqueness of NNTrainer over
the conventional is that Tensors in NNTrainer, especially for
intermediate activation, are prioritized based on EO and In-
tegrity. This allows Memory Planner to maximize reusability
of Memory Pool; thus, reducing memory consumption.

4.1 Execution order and Integrity of Tensors

When a neural network is trained, intermediate activation
accounts for more than 90% of the total memory consump-
tion [6]. It is because the result of the forward process must
be saved and used in the backward process. Thus, if we effec-
tively use EOs exploiting orders between processes, we can
use memory more efﬁciently as in Figure 2 (b). To achieve

Figure 3: Abstract architecture of NNTrainer

There are applications running multiple neural network
models simultaneously that need to manage shared parame-
ters. For such usages, NNTrainer provides AppContext, which
allows registering custom layers and optimizers for an appli-
cation shared across its models. After completion of Load
process, layers of the given neural network model are saved as
tuples of [<Layer type>, <Properties (key, value)>].

Conﬁgure process creates layer objects with such tuples,
and a Graph is constructed by such layer objects. Graph in-
stances need to be efﬁciently constructed so that Graph and
Layer instances take less memory until the allocation for
Tensor buffers is actually required. Context provides the in-
formation required to delay allocation. Graph and Layer save
their parameters at their Context.

Compile: before Initialize, Layer Context exists as an init-
Context instance having the information as string values.
Later, initContext becomes runContext by Initialize process,
converting the string values to Tensor objects. For example,
the input and output dimensions are saved as strings in init-
Context, which are converted to input and output Tensor spec-
iﬁcation in runContext. It is also lighter and more efﬁcient to
conduct the Compile process with initContext. Each Layer
subclass provides forward and backward functions that calcu-
late gradients and derivatives, respectively. Each Optimizer
subclass provides a function applying gradients. By adding
such subclasses, users may extend NNTrainer for the needs
of their neural networks.

Developers may add hardware acceleration backends by
supplying subclasses of Delegate on the basis of Tensors; Ten-
sors are the computation basis of NNTrainer. To use memory
efﬁciently, we need to manage data of Tensors independently;
thus, we have separated speciﬁcations (such as dimensions
and status) and data stored in actual buffers of Tensors.

5

Tensor lifespan
Forward (F)
Compute Gradient (CG)
Compute Derivative (CD)
Backward (B)
Iteration (I)
Max (M)

Description
forward
compute gradient
compute derivative
compute gradient & derivative
reset after iteration
always available

Table 2: Tensor lifespan

Sharing mode Tensor create mode

Not sharing

Memory

Tensor

Description
Holding the external memory
Allocate new Tensor
Data changes

Place-Holder (P)
Create (C)
Modify View (MV)
Read-Only View (RV) No data change
Extend (E)

Share everything

Table 3: Tensor sharing and Tensor create mode

such efﬁciency, NNTrainer divides the entire training process
into Forward, Compute Gradient, and Compute Derivatives,
(including Apply Gradient), and conﬁgures EOs to each layer.
It is straightforward to determine the order of layers, but more
information is needed to determine the order of Tensors. Be-
cause some Tensors are required by only speciﬁc training
processes. For example, gradient Tensors are only required by
Compute Gradient. NNTrainer deﬁnes Tensor Lifespan and
Tensor Create Mode in Tensor speciﬁcation so that execution
orders can be determined through them.

Tensor lifespan manages validity of Tensors, whose cate-
gories are Forward (F), Compute Gradient (CG), Compute
Derivative (CD), Backward (B), Iteration (I), and Max (M)
as in Table 2. For example, a Tensor storing derivatives has
backward lifespan, which does not need to be valid during
the forward process. And, a Tensor storing weight has max
lifespan, which is valid and cannot be deleted during the entire
training process.

We need to distinguish data sharing types to utilize memory
efﬁciently. NNTrainer categorizes data sharing into Memory
Sharing and Tensor Sharing as in Table 3. Memory Sharing is
applied if the two Tensors have different speciﬁcations while
the data of Tensors are shared. Tensor Sharing is applied if
the two Tensors have the same speciﬁcations and the data are
shared. NNTrainer allows developers to explicitly conﬁgure
tensor create mode in Table 3 so that developers may mandate
the data sharing modes mentioned above.

In Table 3, Create (C) creates a source Tensor instance.
View (V) creates a memory sharing Tensor. Place holder (P) is
a Tensor holding the data buffer allocated externally and not
managed by NNTrainer. Modify View (MV) creates memory
sharing Tensor in which data is changed, and Read-Only View
(RV) creates memory sharing Tensor which data is guaranteed
not to change by the developer. Extend (E) creates tensor
sharing Tensor, and weight Tensor during time iteration is a

Algorithm 1 Compute Execution Order
Input: Layer List: L = {L0, L1, ..., LN−1}

1: EOmax = N × 3
2: Initialize Tmap
3: for i = 0, 1, . . . , N − 1 do
4:

EOF = i
EOCG = EOmax − (i + 1) × 2
EOCD = EOCG + 1
EOLi = {EOF , EOCG, EOCD}Layer Type
for tr ∈ TLi do

(cid:46) TLi is Tensor list requested in Li

end for

tr = Tmap.get(tr) or Tmap.insert(tr)
Add EO ∈ EOLi to tr w.r.t tensor lifespan of tr

10:
11:
12: end for
13: Sort Tmap by ascending with min(EOs o f Ti ∈ Tmap)
14: for Ti ∈ Tmap do
15:

CMTi = Tensor create mode of Ti
if CMTi == MVTj then

(cid:46) Tj is target Tensor

if min(EOs o f Ti) ≥ max(EOs o f Tj) then

Merge Ti to Tj including EOs

end if

else if CMTi == RVTj or ETj then
Merge Ti to Tj including EOs

5:
6:
7:
8:
9:

16:
17:
18:
19:
20:
21:

end if

22:
23: end for

representative example. Time iterations as used in TTS are
implemented by unrolling internally by the framework. Then,
the weight Tensors between the unrolled layers need to be E
because both the speciﬁcation and data of weights must be
shared with each other.

Algorithm 1 describes how EO is deﬁned using tensor lifes-
pan and tensor create mode. Figure 4 shows how Algorithm 1
works for three layers model. In the ﬁgure, 0, 7(F,CG/P)
denotes that tensor lifespan is F and CG. Its tensor create
mode is P and the EOs of Tensor are 0 and 7. NNTrainer
sequentially assigns EOs starting from L0 according to the
computing process as in line 3 to 7 of Algorithm 1, and the
result is shown in the bottom-right square in the ﬁgure.

In L0, requested Tensors are X0, X1, D1, ∆W0, and W0. Be-
cause the tensor lifespans of X0 are F and CG, their EOs, 0
and 7, are assigned to X0. And set 0 (F of L0) for X1 and set 7
(CG of L0) for D1. This procedure is repeated for each Layer,
Li: line 8 to 11 in Algorithm 1. Note that this covers the ideal
memory requirement if we merge output Tensors, X (cid:48)
n−1 and
Xn, in Figure 1.

In-place layers, such as activation layers, data sharing ap-
pears within the layers as shown in Figure 5. Because the
input, X1, and output, X2, of the layer, L1, can be Memory
Sharing with changing data, X2 can be marked as Modify
View of X1 (MVX1). Here, let’s call X1 a target Tensor and X2
a merged Tensor. When applying tensor create mode, only
the largest order of the target Tensor and the smallest order

6

Figure 4: Execution order and tensor lifespan with tensor
create mode of model A. Refer to Figure 1, Table 2, and
Table 3 for the notations.

Figure 6: Execution order of model C (activation L1 and
ﬂatten L2), where only X0, X1, D3, ∆W0, and W0 are required.

Figure 5: Execution order of model B (activation L1), where
D1 and X2 are not allocated.

of the merged Tensor are compared: line 17 of Algorithm 1.
The largest order of X1, 1, is equal to the smallest order of the
merged Tensor, X2, 1; thus, these Tensors and orders can be
merged. If the largest order of the target Tensor is greater than
the smallest order of the merged Tensor, the integrity of the
target Tensor cannot be guaranteed because the target Tensor
is accessed after the merge; thus, it cannot be combined and
new Tensor needs to be created. Then, we can remove another
intermediate activation as described in Figure 1.

In another case, the integrity of data is kept even if the
Tensor speciﬁcation changes. Flatten layers do not update
data for outputs from inputs; only the dimensions of outputs
are modiﬁed as in Figure 6. Therefore, Read-Only View (RV )
can be given to the corresponding Tensors. X3 in Figure 6 is
Read-Only View of X2 (RVX2). Even though the largest order
of the target Tensor, X2, 6, is greater than the smallest order

7

Figure 7: Memory planning with model A.

of the merged Tensor, X3, 2, it can still be merged because the
integrity of data is guaranteed with RV. This saves additional
memory space by removing another intermediate activation.

4.2 Memory Pool and Memory Planner

After EO is decided, Memory Planner decides how to allocate
memory from Memory Pool with the planner algorithm. The
planner algorithm described in Algorithm 2 is simple sorting-
based; an algorithm minimizing fragmentation for higher
utilization is future work. Figure 7 shows how Algorithm 2
works for model A as an example, where the four iterations
of the for-loop, i = [7...10], are shown, reallocating a few
tensors to share and reuse the memory space.

For each Tensor data, we calculate memory offset and as-
sign the offset value. When we calculate the offset for W1
after the calculation of the offset for W0, because there are no
assigned Tensors whose largest EO is less than the smallest
EO of W1, a new offset is calculated and assigned to W1. It
keeps assigning new offsets until X3 as shown in the ﬁrst
row of Figure 7. As in the second row of the ﬁgure, When
we calculate for D3, the largest EO of X3, 2, is less than the

Algorithm 2 Simple sorting-based Memory Planner
Input: Tensor List: T = {T1, T2, ..., TN}

Sort by descending order based on max(EOs o f T )

1: Sort T by ascending order based on min(EOs o f each Ti)
2: if min(EOs o f Ti) == min(EOs o f Tj) then
3:
4: end if
5: for i = 1, 2, . . . , N do
6:
7:
8:
9:
10:

EOi
last = i
for j = i − 1, . . . , 1 do

max = max (EO o f Tj)
min then

min = min (EO o f Ti)

(cid:46) T is Sorted, now

EO j
if EO j

max < EOi

11:
12:
13:
14:
15:

last = j

end if

end for
if last == i then

Calculate offset for Ti and Assign to Ti data

else

16:
17:
18:
19: end for

end if

Assign data(offset) of Tlast to Ti data(offset)

Figure 8: Memory Planning with model B.

smallest EO of D3, 3. It means that the X3 can be reused and
therefore, the same offset is assigned to data of D3 as in line
17 of Algorithm 2. To assign ∆W2 as the next step, a new
offset is required because there is no Tensor whose largest EO
is smaller than the smallest order of ∆W2. Memory planning
is completed when calculating offsets and assigning Tensors
are completed.

Figure 8 shows the memory planning example with Model
B. Unlike Model A, there is a Tensor that cannot be reused.
For example, D2 causes fragmentation when ∆W0 is assigned
as shown in the last row of Figure 8. Therefore, a planning
algorithm that can minimize or resolve fragmentation is re-
quired; however, the implementation of such an algorithm is
future work for the next releases.

A major advantage of this method is that we can calcu-
late the peak memory consumption beforehand as shown in

8

Figure 7, which is equal to the ideal memory requirement
in Figure 1. Machine learning engineers using servers often
suffer from crashes due to out-of-memory. Because our ap-
proach can prevent such crashes by calculating the memory
requirement before actual execution, engineers can save their
precious time and try to maximize the resource utilization;
e.g., increasing batch sizes as large as possible and reducing
computation time. Besides, with more memory available for a
given task by reducing peak memory consumption, engineers
can further utilize the given resources and try much more
hyper-parameters and model structures with the given time
and resources.

5 Evaluation

We evaluate with components of neural networks (individual
layers or small groups of such layers) and applications in-
cluding popular neural networks. Then, we analyze and com-
pare the performance (latency and memory consumption) of
NNTrainer and conventional frameworks: TensorFlow 2.3.0
and PyTorch 1.5.0. For memory consumption, we also com-
pare with the ideal memory requirement for each case based
on the analysis described in Analysis of Deep Neural Network
Training (§3). We also demonstrate on-device personalization
application on Galaxy Watch and complex text-to-speech ap-
plication consisting of multiple LSTM layers. Except for the
case with Galaxy Watch, we evaluate with Raspberry pi 4,
which has four ARM Cortex-A72 (2.0GHz) CPU cores and 4
GiB RAM running Ubuntu 18.04. NNTrainer in the experi-
ments uses CPU cores as its computation backend. Note that
most NPUs and DSPs of embedded devices do not support
ﬂoating points and GPUs of consumer electronics are often
not suitable or not allowed for training; e.g., GPUs of TV
are supposed to be fully dedicated for video stream and GUI
handling.

5.1 Component Evaluation

Table 4 describes 10 test cases of neural network components
(layers and small sets of layers) with different dimensions
of inputs and labels. The test cases include an LSTM layer
(popular for time series including voice models) and linear
and Conv2D layers (popular for vision models). We evaluate
Conv2D and linear layers of Model A, B, and C introduced in
Figure 4, 5, 6 as well. We have applied Mean Squared Error
(MSE) [2] and Stochastic Gradient Descent (SGD) [28] to
train neural networks of the given test cases. Model D is a
more complex case, which has an input layer, addition, and
linear or Conv2D layer, and a multi-output layer with two acti-
vation layers. We conﬁrm the correctness by comparing every
activation and weight value of models trained by NNTrainer
with the same models trained by TensorFlow with the same
data. The two frameworks result in equivalent neural network
models with errors at 1e−4 level. With automated procedures,

Test Cases
Linear
Conv2D
LSTM
Model A (Linear)
Model A (Conv2D)
Model B (Linear)
Model B (Conv2D)
Model C (Linear)
Model C (Conv2D)
Model D

Input
64:1:1:150528
64:3:224:224
64:1:1:150528
64:1:1:150528
64:3:224:224
64:1:1:150528
64:3:224:224
64:1:1:150528
64:3:224:224
64:1:1:150528

Output (Label)
64:1:1:10
64:3:112:112
64:1:1:10
64:1:1:10
64:3:28:28
64:1:1:10
64:3:56:56
64:1:1:10
64:1:1:37632
64:1:1:10

Ideal Memory (kiB)
49397
65856
84731
188250
51157
112935
54097
49399
65856
162295

Table 4: Conﬁgurations and ideal memory sizes of component
test cases with batch size of 64.

Figure 9: Peak memory consumption with batch size of 64.

we conﬁrm the correctness by training models and examining
their equivalence for every pull request; i.e., if a weight or
activation value has an error over 1e − 4, the corresponding
code commit is automatically rejected.

As explained in Section 3, once the input size and network
conﬁguration are determined, the ideal (minimal) memory re-
quirement can be calculated based on the sizes of intermediate
activation, weight, and gradient as in Table 4.

Figure 9 shows the evaluation results of the baseline and
peak memory consumption. The baseline implies memory
consumed by the framework itself (e.g., codes and libraries),
which does not change for different test cases. The peak im-
plies the memory consumed by training the given model in
addition to the baseline. TensorFlow consumes x27 more
baseline memory (337.8 MiB) than NNTrainer (12.3 MiB)
consumes, and PyTorch consumes x8 more (105.4 MiB). The
major reason for such excessive baseline memory consump-
tion of TensorFlow and PyTorch can be attributed to Python
and a lot of external libraries; NNTrainer is written in C++
with minimal dependencies on external libraries. The peak
memory consumption varies per test case, and, unlike the base-
line, the difference is mostly caused by the design choice (ten-
sor op. vs layer op. basis) and buffer management approaches
affected by the choice, not by the libraries and languages for
the implementation.

In single layer tests of Linear, Conv2D, and LSTM, with

9

Figure 10: Training latency of the test cases from Table 4.

the help of the layer basis computation and the execution
order, NNTrainer consumes signiﬁcantly less amount of mem-
ory. In other cases of Model A, B, C, and D, NNTrainer also
consumes signiﬁcantly less amount of memory.

The proposed resource utilization method allows reusing
memory spaces for Tensors between layers. Note that con-
ventional frameworks use signiﬁcantly more memory than
NNTrainer from x2.19 to x6.47 on average including baseline.
The comparison to the ideal peak memory requirement in
Figure 9 suggests that NNTrainer incurs ignorable overhead
of memory consumption. This implies that the proposed re-
source utilization method effectively maximizes sharing of
tensor and memory with ignorable memory overhead. The
baseline is required to load essential libraries, and additional
heaps attributing to the peak are required for some layers;
e.g., NNTrainer’s Conv2D layer adds “Image to Column”
(im2col) [13] operator for computation efﬁciency, which re-
quires additional memory buffers.

Figure 10 shows the training latency of 1 epoch, 512 dataset
size, and 32 batch size with the test cases in Table 4 to show
that NNTrainer does not sacriﬁce the performance or the accu-
racy to conserve memory. Although consuming signiﬁcantly
less memory, in most cases, NNTrainer is evaluated to be
faster than or equivalent to the conventional frameworks.

Figure 11 compares the memory usage and computation
performance with different batch sizes for Model A with
dataset size of 512 and 1 epoch. The computation perfor-
mance is measured by the time taken to process a constant
amount of training data, which is the product of the number
of iterations and the batch size. In the ﬁgure, the red dotted
line denotes 512 MiB, which is often the main memory size
of embedded devices. Even with a ﬂagship mobile phone hav-
ing over 8 GiB of main memory, it is not acceptable for an
application to consume over 512 MiB, especially for an appli-
cation deployed by the manufacturer. Note that batch size is a
major hyper-parameter that developers want to tune. Larger
batch sizes often allow higher accuracy or performance even
in server and cloud systems; thus, if TensorFlow or PyTorch

5.2 Various Applications and Personalization

Figure 12 shows the memory consumed to train neural net-
works from scratch or partially for personalization with a
batch size of 32 and compares between NNTrainer, Tensor-
Flow, and PyTorch. The ﬁrst three cases of Figure 12 com-
pare the memory consumed to train complete neural net-
work models from scratch: LeNet-5 [53], VGG16 [54], and
Resnet18 [19]. In every case, NNTrainer consumes less mem-
ory: e.g., saving 96.5% with LeNet-5 and 65% with VGG16
and ResNet18. Besides, NNTrainer offers transfer learning for
personalizing neural network models, saving resources further
as in the fourth case of Figure 12. Note that, NNTrainer saves
over 75% of the memory consumption by applying transfer
learning. On the other hand, conventional frameworks can-
not save as much as memory NNTrainer saves with transfer
learning. With transfer learning, we can efﬁciently (for both
memory and computation) personalize pre-trained neural net-
works deployed by vendors on devices with personal data
without exposing such data out of devices.

Transfer learning does not require training or updating back-
bone networks.Thus, memory consumption can be further
reduced. The ideal memory consumption for this case is 44.7
MiB for weights and 23.6 MiB for 3 intermediate activation
values including buffers to save the residual network results
with a size of 32×32×3×4×64. Then, 80.5 MiB is required
including the baseline 12.3 MiB (NNTrainer). NNTrainer is
evaluated to consume 87.8 MiB and it is close to the ideal
consumption, again showing the efﬁciency of the proposed
resource management method.

Product Rating [20] is a neural network model that predicts
a user’s rating value for speciﬁc products by passing three
linear layers after concatenation of the transformed user and
product inputs using embedding layer. It is a neural network
model that is trained and runs on device for recommendation
systems with federated learning. We use Movielens [18] as
the training dataset. Again, NNTrainer saves about 50% of
the memory compared to other frameworks. The saving is
relatively smaller than other cases because the size of the em-
bedding layer input, 49 MiB (193610 × 4 × 64), is dominant
while the intermediate activation of the linear layers, (0.032
MiB - 128 × 4 × 64) is small.

To personalize neural network models on device, a frame-
work should be able to change the neural network conﬁgu-
ration ﬂexibly. NNTrainer supports APIs to conﬁgure and
train such models ﬂexibly. With such APIs, developers can
personalize various neural network based applications includ-
ing not only transfer learning and ﬁne-tuning but also meta-
learning [55, 61, 62] and few-shot learning [48, 56, 64].

HandMoji application is tested on the Samsung Galaxy
watch (Cortex-A53 1.15GHz, 768MiB RAM, and Tizen OS)
with NNTrainer. The application learns the user’s two hand-
drawn symbols, and then, infers mapping to a corresponding
emoji, which is shown in Figure 13. With the given conﬁg-

Figure 11: Performance w.r.t batch size with model A-Linear.

Figure 12: Memory consumption of training applications.

adopts the proposed method, AI researchers may spend less
time for higher accuracy as mentioned in the previous section.

Figure 11 shows that NNTrainer supports a higher batch
size; thus, reducing the latency further with the given re-
sources. In the case of NNTrainer, it does not exceed 512
MiB throughout all the batch sizes, but TensorFlow requires
over 512 MiB from 16 batch size, so that, maximum batch
size to train is 8 and it takes 32.53 seconds. On the other
hand, PyTorch ﬁnished in 12.19 seconds with 64 batch size.
On the other hand, NNTrainer completes training in 6.09 sec-
onds using 128 batch size considering the convergence of the
computation time. NNTrainer uses less memory than other
frameworks despite using 128 batch size due to the proposed
memory management scheme and computation performance
is improved by maximization cache utilization due to the
large batch size. Besides, NNTrainer can be more suitable for
embedded devices as low memory usage is proportional to
low power consumption.

10

Figure 13: HandMoji application on Samsung Galaxy Watch.

ers, and it runs after time iteration is ﬁnished by concatenating
the spectrogram computed with time iterations.

NNTrainer does time-unrolling to perform complex time
iteration, and the entire memory is statically allocated based
on the maximum time iteration declared by the developer.
Weights of the same layers that are time-unrolled incur no
additional memory or computation overhead because of the
Tensor sharing of NNTrainer. Because of time iteration, for-
ward process is performed for all unrolled layers, and gradi-
ents are accumulated in the backward process without updat-
ing weights. The optimizer updates weights only once per
layer, which requires additional memory to store the gradient
during time iterations. Gradient Clipping [43] and Teacher
Forcing [65] are also supported. In Figure 14, peak memory
consumption and latency for one sample training are com-
pared with PyTorch. NNTrainer saves 40% to 56% of memory
consumed by PyTorch, and latency with same memory us-
age was also conﬁrmed to be improved by more than 35%
(PyTorch with 16 batch and NNTrainer with 32 batch). In par-
ticular, when using the 16 batch size, the latency is improved
by 24% and 56% of memory is saved. Through this, we con-
ﬁrm that a neural network model with a complex structure
can also be personalized using NNTrainer efﬁciently.

Figure 14: Performance comparison of Tacotron2

6 Conclusion

uration, NNTrainer provides backbone layers as a feature
extractor using the pre-trained MobileNet-V2 TensorFlow-
lite model and adds one fully connected layer as a classiﬁer.
HandMoji collects 5 images per emoji from the user to train
the classiﬁer with transfer learning. The ability to cache the
results from the feature extractor in the ﬁrst epoch to reuse in
other epochs enables reducing the training time to under 10
seconds. Model description and entire training conﬁguration
is described within 30 lines as shown in Figure 13.

Tacotron2 [51] is a sequence to sequence text to speech
model based on attention which enables end-to-end training
with only a pair dataset of sentence and voice. Developers can
create a personalized text-to-speech (TTS) application by run-
ning Tacotron2 on NNTrainer so that an application may read
books for kids with voices of their parents or translate speech
with the user’s voice. To personalize the voice output of a
TTS application, a user reads 18 sentences to create a dataset
for ﬁne-tuning, which are combined with the pre-installed rep-
resentative dataset. Tacotron2 consists of Encoder, Attention,
and Decoder, where Decoder generates voice outputs. Thus,
we apply training (model updates) on Decoder for the given
pre-trained Tacotron2 model and other parts are kept static
for inference. Decoder consists of Prenet, Decoder LSTMs,
and Postnet. Prenet has 2 linear layers. Decoder LSTMs has 2
LSTM layers including attention and 2 linear layers for gate
prediction and a mel spectrogram. Postnet has 5 Conv1D lay-

11

We propose a light-weight on-device neural network training
framework, NNTrainer. We address the obstacles against on-
device training and propose novel techniques to mitigate them.
We signiﬁcantly reduce the memory consumption so that em-
bedded devices may practically train neural networks without
deterioration of accuracy or modiﬁcation of model conﬁgura-
tion. We evaluate and show how efﬁciently NNTrainer trains
various neural network models with applications. We also
demonstrate a practical on-device neural network personal-
ization with a complicated sequence to sequence model.
NNTrainer is an active project with a future roadmap:
• Proliferate NNTrainer. We are extending NNTrainer to
import and export model ﬁles of other frameworks (e.g.,
TensorFlow-lite). We are also extending interfaces: Java
APIs, .NET APIs, Web APIs, and Yocto/OpenEmbedded.
• We plan to support various computation backends includ-
ing GPUs, DSPs, and NPUs for higher throughput and
energy efﬁciency. Integer-based training and quantization-
aware training [23] are planned for NPUs.

• Support advanced few-shot learning and meta-learning

techniques for adaptation problems.

• Develop techniques for more efﬁcient resource utilization:
i.e., graph optimization and further memory optimization
with dynamic off-loading using secondary memory. Dy-
namic off-loading is expected to be highly efﬁcient be-
cause NNTrainer can predict and decide when a buffer
is accessed; thus, we can swap in and out proactively in
background.

References

[1] ABADI, M., BARHAM, P., CHEN, J., CHEN, Z., DAVIS,
A., DEAN, J., DEVIN, M., GHEMAWAT, S., IRVING, G.,
ISARD, M., KUDLUR, M., LEVENBERG, J., MONGA,
R., MOORE, S., MURRAY, D. G., STEINER, B.,
TUCKER, P., VASUDEVAN, V., WARDEN, P., WICKE,
M., YU, Y., AND ZHENG, X. Tensorﬂow: A system for
large-scale machine learning. In 12th USENIX Sympo-
sium on Operating Systems Design and Implementation
(OSDI 16) (Savannah, GA, Nov. 2016), USENIX Asso-
ciation, pp. 265–283.

[2] ALLEN, D. M. Mean square error of prediction as a
criterion for selecting variables. Technometrics 13, 3
(1971), 469–475.

[3] BAYDIN, A. G., PEARLMUTTER, B. A., RADUL,
A. A., AND SISKIND, J. M. Automatic differentia-
tion in machine learning: a survey. Journal of machine
learning research 18 (2018).

[4] BEBIS, G., AND GEORGIOPOULOS, M. Feed-forward
neural networks. IEEE Potentials 13, 4 (1994), 27–31.

[5] BHARDWAJ, K., LIN, C., SARTOR, A. L., AND MAR-
CULESCU, R. Memory- and communication-aware
model compression for distributed deep learning infer-
ence on iot. ACM Trans. Embed. Comput. Syst. 18, 5s
(2019), 82:1–82:22.

[6] CAI, H., GAN, C., ZHU, L., AND HAN, S. Tinytl:
Reduce memory, not parameters for efﬁcient on-device
learning. arXiv preprint arXiv:2007.11622 (2020).

[7] CERVERA, R., CORTES, T., AND BECERRA, Y. Im-
proving application performance through swap compres-
sion. In 1999 {USENIX} Annual Technical Conference
({USENIX}{ATC} 99) (1999).

[8] CHEN, T., MOREAU, T., JIANG, Z., ZHENG, L., YAN,
E., SHEN, H., COWAN, M., WANG, L., HU, Y., CEZE,
L., GUESTRIN, C., AND KRISHNAMURTHY, A. TVM:
An automated end-to-end optimizing compiler for deep
learning. In 13th USENIX Symposium on Operating Sys-
tems Design and Implementation (OSDI 18) (Carlsbad,
CA, Oct. 2018), USENIX Association, pp. 578–594.

[9] CUI, H., ZHANG, H., GANGER, G. R., GIBBONS,
P. B., AND XING, E. P. Geeps: Scalable deep learning
on distributed gpus with a gpu-specialized parameter
server. In Proceedings of the Eleventh European Con-
ference on Computer Systems (2016), pp. 1–16.

[10] DAHL, G. E., YU, D., DENG, L., AND ACERO, A.
Context-dependent pre-trained deep neural networks for

IEEE Transac-
large-vocabulary speech recognition.
tions on audio, speech, and language processing 20, 1
(2011), 30–42.

[11] DELANGE, M., ALJUNDI, R., MASANA, M., PARISOT,
S., JIA, X., LEONARDIS, A., SLABAUGH, G., AND
TUYTELAARS, T. A continual learning survey: Defying
forgetting in classiﬁcation tasks. IEEE Transactions on
Pattern Analysis and Machine Intelligence (2021).

[12] DENG, J., DONG, W., SOCHER, R., LI, L.-J., LI, K.,
AND FEI-FEI, L. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer
vision and pattern recognition (2009), Ieee, pp. 248–
255.

[13] DUKHAN, M. The indirect convolution algorithm. arXiv

preprint arXiv:1907.02129 (2019).

[14] ESSALMI, F., AYED, L. J. B., JEMNI, M., GRAF, S.,
ET AL. A fully personalization strategy of e-learning
scenarios. Computers in Human Behavior 26, 4 (2010),
581–591.

[15] GONG, L., LI, Z., QIAN, F., ZHANG, Z., CHEN, Q. A.,
QIAN, Z., LIN, H., AND LIU, Y. Experiences of landing
machine learning onto market-scale mobile malware
In Proceedings of the Fifteenth European
detection.
Conference on Computer Systems (2020), pp. 1–14.

[16] GREFF, K., SRIVASTAVA, R. K., KOUTNÍK, J., STE-
UNEBRINK, B. R., AND SCHMIDHUBER, J. Lstm: A
search space odyssey. IEEE transactions on neural net-
works and learning systems 28, 10 (2016), 2222–2232.

[17] HAM, M., MOON, J., LIM, G., JUNG, J., AHN, H.,
SONG, W., WOO, S., KAPOOR, P., CHAE, D., JANG,
G., ET AL. Nnstreamer: Efﬁcient and agile† devel-
opment of on-device ai systems. In 2021 IEEE/ACM
43rd International Conference on Software Engineering:
Software Engineering in Practice (ICSE-SEIP) (2021),
IEEE, pp. 198–207.

[18] HARPER, F. M., AND KONSTAN, J. A. The movielens
datasets: History and context. Acm transactions on
interactive intelligent systems (tiis) 5, 4 (2015), 1–19.

[19] HE, K., ZHANG, X., REN, S., AND SUN, J. Deep
residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern
recognition (2016), pp. 770–778.

[20] HE, X., LIAO, L., ZHANG, H., NIE, L., HU, X., AND
CHUA, T.-S. Neural collaborative ﬁltering. In Proceed-
ings of the 26th international conference on world wide
web (2017), pp. 173–182.

12

[21] HINTON, G., DENG, L., YU, D., DAHL, G. E., MO-
HAMED, A.-R., JAITLY, N., SENIOR, A., VANHOUCKE,
V., NGUYEN, P., SAINATH, T. N., ET AL. Deep neural
networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Signal
processing magazine 29, 6 (2012), 82–97.

[22] HUANG, J., QURESHI, M. K., AND SCHWAN, K. An
evolutionary study of linux memory management for
fun and proﬁt. In 2016 {USENIX} Annual Technical
Conference ({USENIX}{ATC} 16) (2016), pp. 465–478.

[23] JACOB, B., KLIGYS, S., CHEN, B., ZHU, M., TANG,
M., HOWARD, A., ADAM, H., AND KALENICHENKO,
D. Quantization and training of neural networks for efﬁ-
cient integer-arithmetic-only inference. In Proceedings
of the IEEE conference on computer vision and pattern
recognition (2018), pp. 2704–2713.

[24] JIA, Y., SHELHAMER, E., DONAHUE, J., KARAYEV,
S., LONG, J., GIRSHICK, R., GUADARRAMA, S., AND
DARRELL, T. Caffe: Convolutional architecture for
In Proceedings of the 22nd
fast feature embedding.
ACM international conference on Multimedia (2014),
pp. 675–678.

[25] JIA, Z., PADON, O., THOMAS, J., WARSZAWSKI, T.,
ZAHARIA, M., AND AIKEN, A. Taso: Optimizing
deep learning computation with automatic generation
of graph substitutions. In Proceedings of the 27th ACM
Symposium on Operating Systems Principles (New York,
NY, USA, 2019), SOSP ’19, Association for Computing
Machinery, p. 47–62.

[26] JIANG, Y., ZHU, Y., LAN, C., YI, B., CUI, Y., AND
GUO, C. A uniﬁed architecture for accelerating dis-
tributed DNN training in heterogeneous gpu/cpu clus-
ters. In 14th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 20) (Nov. 2020),
USENIX Association, pp. 463–479.

[27] JUSTIN BASILICO.

Netﬂix explains

mendations and personalization.
//scale.com/blog/Netflix-Recommendation-
Personalization-TransformX-Scale-AI-Insights,
2021.

recom-
http:https:

[28] KETKAR, N. Stochastic gradient descent.

In Deep

learning with Python. Springer, 2017, pp. 113–132.

[29] KIM, Y., KIM, J., CHAE, D., KIM, D., AND KIM, J.
µlayer: Low latency on-device inference using cooper-
ative single-layer acceleration and processor-friendly
quantization. In Proceedings of the Fourteenth EuroSys
Conference 2019 (2019), pp. 1–15.

13

[30] KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON,
G. E. Imagenet classiﬁcation with deep convolutional
neural networks. Communications of the ACM 60, 6
(2017), 84–90.

[31] KWON, O., AND KOH, K. Swap space management
technique for portable consumer electronics with nand
ﬂash memory. IEEE Transactions on Consumer Elec-
tronics 56, 3 (2010), 1524–1531.

[32] LI, L., FAN, Y., TSE, M., AND LIN, K.-Y. A review
of applications in federated learning. Computers &
Industrial Engineering (2020), 106854.

[33] LING, Z.-H., KANG, S.-Y., ZEN, H., SENIOR, A.,
SCHUSTER, M., QIAN, X.-J., MENG, H. M., AND
DENG, L. Deep learning for acoustic modeling in para-
metric speech generation: A systematic review of exist-
ing techniques and future trends. IEEE Signal Process-
ing Magazine 32, 3 (2015), 35–52.

[34] LIU, S., LIN, Y., ZHOU, Z., NAN, K., LIU, H., AND
DU, J. On-demand deep model compression for mo-
bile devices: A usage-driven model selection framework.
In Proceedings of the 16th Annual International Con-
ference on Mobile Systems, Applications, and Services
(2018), pp. 389–400.

[35] LIU, Y., WANG, Y., YU, R., LI, M., SHARMA, V.,
AND WANG, Y. Optimizing CNN model inference on
cpus. In 2019 USENIX Annual Technical Conference
(USENIX ATC 19) (Renton, WA, July 2019), USENIX
Association, pp. 1025–1040.

[36] LU, J., BEHBOOD, V., HAO, P., ZUO, H., XUE, S.,
AND ZHANG, G. Transfer learning using computational
intelligence: A survey. Knowledge-Based Systems 80
(2015), 14–23.

[37] MEDSKER, L. R., AND JAIN, L. Recurrent neural net-
works. Design and Applications 5 (2001), 64–67.

[38] MIKOLOV, T., KARAFIÁT, M., BURGET, L., CER-
NOCK `Y, J., AND KHUDANPUR, S. Recurrent neural
network based language model. In Interspeech (2010),
vol. 2, Makuhari, pp. 1045–1048.

[39] MIT TECHNICAL REVIEW.

On-device

ai.

https://www.technologyreview.com/hub/
ubiquitous-on-device-ai/.
2021).

(accessed 14 Dec

[40] NARAYANAN, D., SANTHANAM, K., KAZHAMI-
AKA, F., PHANISHAYEE, A., AND ZAHARIA, M.
Heterogeneity-aware cluster scheduling policies for
deep learning workloads. In 14th USENIX Symposium
on Operating Systems Design and Implementation
(OSDI 20)
2020), USENIX Association,
pp. 481–498.

(Nov.

[41] NDEVILLA.

Iniparser4.
ndevilla/iniparser, 2017.

https://github.com/

[42] PAL, N. R., AND PAL, S. K. A review on image seg-
mentation techniques. Pattern recognition 26, 9 (1993),
1277–1294.

[43] PASCANU, R., MIKOLOV, T., AND BENGIO, Y. On
the difﬁculty of training recurrent neural networks. In
International conference on machine learning (2013),
PMLR, pp. 1310–1318.

[44] PASZKE, A., GROSS, S., CHINTALA, S., CHANAN,
G., YANG, E., DEVITO, Z., LIN, Z., DESMAISON, A.,
ANTIGA, L., AND LERER, A. Automatic differentiation
in pytorch.

[45] PENG, Y., ZHU, Y., CHEN, Y., BAO, Y., YI, B., LAN,
C., WU, C., AND GUO, C. A generic communication
scheduler for distributed dnn training acceleration. In
Proceedings of the 27th ACM Symposium on Operating
Systems Principles (New York, NY, USA, 2019), SOSP
’19, Association for Computing Machinery, p. 16–29.

[46] PRATT, L. Y. Discriminability-based transfer between
neural networks. Advances in neural information pro-
cessing systems 5 (1992), 204–211.

[47] QIAN, Y., FAN, Y., HU, W., AND SOONG, F. K. On
the training aspects of deep neural network (dnn) for
parametric tts synthesis. In 2014 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) (2014), IEEE, pp. 3829–3833.

[48] RAVI, S., AND LAROCHELLE, H. Optimization as a

model for few-shot learning.

[49] SANDLER, M., HOWARD, A., ZHU, M., ZHMOGINOV,
A., AND CHEN, L.-C. Mobilenetv2: Inverted residuals
In Proceedings of the IEEE
and linear bottlenecks.
conference on computer vision and pattern recognition
(2018), pp. 4510–4520.

[50] SAXENA, M., AND SWIFT, M. M. Flashvm: Virtual
In USENIX Annual

memory management on ﬂash.
Technical Conference (2010).

[51] SHEN, J., PANG, R., WEISS, R. J., SCHUSTER, M.,
JAITLY, N., YANG, Z., CHEN, Z., ZHANG, Y., WANG,
Y., SKERRV-RYAN, R., ET AL. Natural tts synthesis by
conditioning wavenet on mel spectrogram predictions.
In 2018 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) (2018), IEEE,
pp. 4779–4783.

[52] SHIN, H., LEE, J. K., KIM, J., AND KIM, J. Continual
learning with deep generative replay. arXiv preprint
arXiv:1705.08690 (2017).

[53] SIMARD, P. Y., STEINKRAUS, D., PLATT, J. C., ET AL.
Best practices for convolutional neural networks applied
to visual document analysis. In Icdar (2003), vol. 3.

[54] SIMONYAN, K., AND ZISSERMAN, A. Very deep con-
volutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 (2014).

[55] SNELL, J., SWERSKY, K., AND ZEMEL, R. Prototyp-
In Advances in
ical networks for few-shot learning.
neural information processing systems (2017), pp. 4077–
4087.

[56] SUNG, F., YANG, Y., ZHANG, L., XIANG, T., TORR,
P. H., AND HOSPEDALES, T. M. Learning to compare:
Relation network for few-shot learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (2018), pp. 1199–1208.

[57] SVOZIL, D., KVASNICKA, V., AND POSPICHAL, J. In-
troduction to multi-layer feed-forward neural networks.
Chemometrics and intelligent laboratory systems 39, 1
(1997), 43–62.

[58] TAN, C., SUN, F., KONG, T., ZHANG, W., YANG, C.,
AND LIU, C. A survey on deep transfer learning. In
International conference on artiﬁcial neural networks
(2018), Springer, pp. 270–279.

[59] THRUN, S., AND MITCHELL, T. M. Lifelong robot
learning. Robotics and autonomous systems 15, 1-2
(1995), 25–46.

[60] TORREY, L., AND SHAVLIK, J. Transfer learning. In
Handbook of research on machine learning applications
and trends: algorithms, methods, and techniques. IGI
global, 2010, pp. 242–264.

[61] VILALTA, R., AND DRISSI, Y. A perspective view and
survey of meta-learning. Artiﬁcial intelligence review
18, 2 (2002), 77–95.

[62] VINYALS, O., BLUNDELL, C., LILLICRAP, T., WIER-
STRA, D., ET AL. Matching networks for one shot
learning. In Advances in neural information processing
systems (2016), pp. 3630–3638.

[63] VOIGT, P., AND VON DEM BUSSCHE, A. The eu gen-
eral data protection regulation (gdpr). A Practical Guide,
1st Ed., Cham: Springer International Publishing 10
(2017), 3152676.

[64] WANG, Y., CHAO, W.-L., WEINBERGER, K. Q., AND
VAN DER MAATEN, L. Simpleshot: Revisiting nearest-
neighbor classiﬁcation for few-shot learning. arXiv
preprint arXiv:1911.04623 (2019).

14

[65] WILLIAMS, R. J., AND ZIPSER, D. A learning al-
gorithm for continually running fully recurrent neural
networks. Neural computation 1, 2 (1989), 270–280.

[66] WU, R., YAN, S., SHAN, Y., DANG, Q., AND SUN,
G. Deep image: Scaling up image recognition. arXiv
preprint arXiv:1501.02876 7, 8 (2015).

[67] XU, S., ZHANG, H., NEUBIG, G., DAI, W., KIM, J. K.,
DENG, Z., HO, Q., YANG, G., AND XING, E. P. Cavs:
An efﬁcient runtime system for dynamic neural net-
works. In 2018 USENIX Annual Technical Conference
(USENIX ATC 18) (Boston, MA, July 2018), USENIX
Association, pp. 937–950.

[68] YANG, W., ZHANG, X., TIAN, Y., WANG, W., XUE,
J.-H., AND LIAO, Q. Deep learning for single image
super-resolution: A brief review. IEEE Transactions on
Multimedia 21, 12 (2019), 3106–3121.

[69] ZHANG, C., YU, M., WANG, W., AND YAN, F. Mark:
Exploiting cloud services for cost-effective, slo-aware
machine learning inference serving. In 2019 USENIX
Annual Technical Conference (USENIX ATC 19) (Ren-
ton, WA, July 2019), USENIX Association, pp. 1049–
1062.

15

