2
2
0
2

l
u
J

2

]
E
S
.
s
c
[

1
v
2
2
6
0
0
.
7
0
2
2
:
v
i
X
r
a

Accelerating System-Level Debug Using Rule
Learning and Subgroup Discovery Techniques

Zurab Khasidashvili

Intel Israel Development Center, Haifa, Israel
zurab.khasidashvili@intel.com

Abstract. We propose a root-causing procedure for accelerating system-
level debug using rule-based techniques. We describe the procedure and
how it provides high quality debug hints for reducing the debug eﬀort.
This includes the heuristics for engineering features from logs of many
tests, and the data analytics techniques for generating powerful debug
hints. As a case study, we used these techniques for root-causing failures
of the Power Management (PM) design feature Package-C8 and showed
their eﬀectiveness. Furthermore, we propose an approach for mining the
root-causing experience and results for reuse, to accelerate future debug
activities and reduce dependency on validation experts. We believe that
these techniques are beneﬁcial also for other validation activities at dif-
ferent levels of abstraction, for complex hardware, software and ﬁrmware
systems, both pre-silicon and post-silicon.

Keywords: System-level Debug · System-level Validation · Validation
Knowledge Mining · Root Cause Analysis · Machine Learning · Rule
Learning · Subgroup Discovery · Feature Range Analysis.

1

Introduction

An estimated 50−60% of validation eﬀort is spent on debugging. Debugging in a
system-level platform represents a great challenge due to the enormous number of
involved software, ﬁrmware and hardware components, domains, ﬂows, protocols,
design features, interfaces etc. The current practice is to follow the traces of
design failures as they cross the system domains and allocate the right experts
to assist in debugging sub-ﬂows associated to these domains, starting from the
sub-ﬂows where the mismatches from the expected behavior is ﬁrst observed.
Another challenge is that there is no established way to learn from root-causing
of a failure and mine the learning in order to reuse it for future root-causing
activities or to automatically root-cause related failure scenarios in the future.
For such distributed debug activities, terms debugging and root-causing might
mean diﬀerent things to diﬀerent validators as well as in diﬀerent validation
contexts: If a bug (or an unexpected behavior or outcome of a test) is originating
from module B, then for a validator responsible for module A root-causing might
mean to understand whether the bug is related to module A, and when there
is a suﬃcient evidence that this is not the case, contact validator of another

 
 
 
 
 
 
2

Z. Khasidashvili

module where the bug might potentially hide. For the validator of module B,
root-causing might mean to understand and conﬁrm that the bug is originating
from the hardware or software code of module B. Furthermore, in case of post-
silicon validation, the bug or unexpected behavior can be caused by physical
phenomena like manufacturing defects or manufacturing process variation, and
in that case the validator of module B might require help from a physical designer
or manufacturing expert to root-cause the unexpected behavior. Root-causing in
such a case might mean to trace down the failure to a physical device that exposes
a rare or a systematic weakness under some operating conditions (temperature,
voltage, power, or other). The failure in that case might not be associated to
a line in hardware, software, or ﬁrmware code, and instead be associated with
phenomena which are currently very diﬃcult or impossible to control.

The proposed solution in this paper is a Machine Learning (ML) based ap-
proach for automating root-causing activity and generating a list of valuable
debug hints based on data that is collected from logs of many tests. It provides
the system-level validator with some level of independency to debug and narrow
down the debug space, and this way reduce the dependency on the experts. On
the other hand, it relieves the experts from the burden of massive debug sup-
port and also helps them with accelerating debugging. The suggested method
also provides a platform for knowledge sharing of previous debug activities. We
demonstrate our method on an important and representative P kgC8 ﬂow in the
Power Management (PM) protocol implemented in Intel processors [14].

There are many ways in which ML can be applied to automate and accelerate
the debugging process, and here we mention some of the important directions
(without aiming at a comprehensive overview).

– Many ML techniques, such as Natural Language Processing, allow to ﬁnd
similarities between manually created bug descriptions, based on occurrences
of key words or advanced semantic similarities. In approaches that use this
technique, before starting to debug, one wants to ﬁnd earlier, resolved bug
records with the same or similar bug descriptions and check whether solu-
tions speciﬁed in these records can be used or adapted to solve the bug at
hand. Text similarity based bug correlation and clustering research is outside
of the scope of this work.

– Trace logs are usually generated by programs and therefore have some or-
ganized form, but not necessarily the form of event records augmented with
attributes and time stamps. Techniques and tools have been developed to
extract from such unstructured logs a structured event, attributes, and time
stamp information [24]. This direction of research is outside of the scope of
this work since the logged event information that we analyze comes from a
simulation, emulation or other validation or test environment with a well-
deﬁned semantics of the log entries, and a Parser and API are available to
extract all relevant information.

– Many research approaches attempt to learn traces of events as appropriate
models such as assertions or automata, each capturing a pattern of event
occurrences observed in the traces [2,26,34]. When information about suc-

Accelerating System-Level Debug Using RL and SD Techniques

3

cessful and failing traces is available, then the models learned from successful
executions can be used to check failing traces against them and identifying
where the execution of failing trace deviates from the expected behavior of
the model [26]. This too is outside the scope of this work as the P kgC8
protocol and other ﬂows of interest in PM protocol are well deﬁned and
implemented as reference models in validation environment that monitor
execution of these ﬂows along traces.

– Unsupervised ML approaches are used when the pass and fail labels are not
available to associate with the successful and failing executions. These ap-
proaches try to detect clusters of related events from log ﬁles [33]. Suspicious
events in the context of root-causing are identiﬁed by selecting the events
that do not belong to any large cluster (also known as outliers) [32]. Outlier
techniques are useful also when pass/fail labels are available, but may lead
to false positives as rare, outlier scenarios do not necessarily lead to incorrect
executions. In this work, information about successful and failing ﬂow exe-
cutions is available to us and we are not using clustering or outlier detection
algorithms though they can be used as complementary techniques.

– When there are many test failures to root-cause on the same design under
test, say failures caught by a regression suite, clustering and prioritization of
failures is useful to reduce the debug eﬀort by identifying similar failures and
debugging a representative [29,30]. This is a wide research area and the most
widely used techniques include supervised or unsupervised clustering algo-
rithms and usage of models that internally perform a kind of clustering, such
as tree based models. In this work, we will present one such technique that
does not use clustering algorithms or ML models though it has similarities
to how tree-based models are built.

– Research has also been done on predicting root-cause of failures [25,12]. The
failure types are predeﬁned and features engineered from failing and passing
test logs are used to train models for predicting these types of failures. In
our work, the root-caused bugs can be labeled and mined along with the
event combinations that explain them, in the form of rules, and these rules
can then be used for building models for predicting failure root-cause for yet
not debugged failures.

In this work, we propose to use F eature Range Analysis (FRA) [16,17] for
root-causing failing ﬂows from trace logs. FRA is a supervised ML technique,
and when the labels are binary, the aim is to isolate subgroups of the positive
samples from the negative ones. In the context of this work, positive samples rep-
resent failing ﬂow executions and negative samples represent passing ones. FRA
is closely related both to Subgroup Discovery (SD ) [18,35,22,1], whose aim is to
identify ”interesting” subgroups of data that can be described as feature-range
combinations; and to Rule Learning (RL) [6,10], which try to build classiﬁcation
models using rules with feature-range pairs (as in SD ) as the antecedent and the
predicted class as the consequent. FRA can be seen as a unifying framework for
both SD and RL approaches: Like SD , it aims at identifying feature-range pairs
for isolating subgroups of the data with respect to quality functions such as Lift

4

Z. Khasidashvili

and WRAcc (see Section 5.1), and in addition it uses F eature Selection [11]
algorithms aiming at selecting both features highly correlated with the class
(response) variable and features giving a good coverage of the variation in the
data [8]. This helps improving the quality of the discovered subgroups of data.
In FRA these high quality feature-range pairs are represented as binary features
and they can be used as antecedents of prediction rules as in RL, as well as be
used as engineered features along with the original features for training classi-
ﬁcation models; such models can outperform both rule-based models as well as
other types of models build from the original features only [17]. In the iterative
procedure of root-causing tree construction proposed in Section 5, FRA is used
in each iteration to identify ﬁner subgroups of failures such that all failures in a
subgroup have the same root-cause and diﬀerent subgroups represent diﬀerent
root-causes. In validation domain, the RL and SD techniques have been used
for improving test coverage and eliminating redundancy [4,15], but to the best
of our knowledge they have not been used for automating root-causing of test
failures.

The paper is organized as follows. In Section 2, we describe the basics of the
PM ﬂow. In Section 3, we deﬁne events, ﬂows, and requirements from traces. In
Section 4, we describe how we generate features from trace logs and the dataset
used for inferring root-causing hints. In Section 5, we will describe our ML
approach. Experimental results are presented in Section 6. In Section 7, we will
explain how our work is diﬀerent from related work on trace based root-causing.
We conclude by discussing future work in Section 8.

2 Power Management Flows

Power-Management (PM) is a classic topic for system-level protocols to verify.
The global PM ﬂows of the Client CP U feature interactions of various internal
and external components controlling the power entities across the system, such
as Voltage-Regulators, Power-Gates, PLLs, Clocks, etc.

The Package C-states (P kgC) are a set of design features for putting the
CP U in various levels of sleep states, while P kgC8 is one of the deeper P kgC
states. The user visible information on PM at Intel processors is described in [14],
Chapter 1.4. The processor supports multiple P kgC states C0, C2, C3, C6 − 10.
State P kgC0 is the normal operation state of the processor – no power saving
requirements are imposed. From P kgC0 state the processor can only transition
to state P kgC2 (which is not a power saving state) from which it then can
transition in any of the power-saving P kgC states C3, C6 − C10: the higher the
number of power saving state the higher is the power saving in that state.

The processor remains in P kgC0 state if at least one of the IA Cores is in
state C0 or if the platform has not yet granted the processor the permission to
go into power saving states. In general, a P kgC state request is determined by
the lowest processor IA Core state. A P kgC state is automatically resolved by
the processor depending on the processor Core idle power states and status of
other platform components.

Accelerating System-Level Debug Using RL and SD Techniques

5

Fig. (1) PkgC8 entry and exit ﬂows.

Figure 1 illustrates the P kgC8 entry and exit ﬂows in a nutshell; it provides
some basic information that may help to understand the examples presented in
this paper. This description of the P kgC states does not apply to any speciﬁc
generation of Intel products and may change in future generations, and therefore
it should be treated as an example. Once all Cores go into sleep-state, the P kgC-
entry ﬂow will be triggered, starting with P kgC2. If the conditions allow to
go deeper, the processor will initiate Last Level Cache (LLC) ﬂush (reaching
P kgC2R). The following stages will be setting the Core, GT and Ring voltage
to 0v, gate part of the Uncore and turn-oﬀ the main CPU clock (Clk). The
P kgC8 wake-up ﬂow will set back the voltages and ungate various components
of the CPU internal domains.

3 Representation of Events, Traces and Flows

We assume a set of observable variables tracked along execution of a test.
Variables originating from hardware are often referred to as signals. In a val-
idation environment, an event is usually referred to as a value change of an
observable variable. In this work, an event is a pair (variable, value range)
for numeric observable variables or a pair (variable, level subset) for categor-
ical observable variables, interpreted as conditions variable ∈ value range and
variable ∈ level subset, respectively. In case of singleton sets value range =
{v} and level subset = {l}, we write these conditions as variable = v and
variable = l. For example, a binary clock signal Clk is treated as an (un-ordered)
categorical variable; two events are associated to it: CLK OF F ≡ Clk = 0 and
CLK ON ≡ Clk = 1, which are ﬁred when Clk changes value from 1 to 0
or from 0 to 1, receptively. These two events cannot occur at the same time
but can occur along the same execution. As another example, one way to en-
code a message M sg with two possible status values {snd, rcv} is to treat it
as a categorical variable with these two levels and associate to it two events
M SG SN D ≡ M sg = snd and M SG RCV ≡ M sg = rcv that are ﬁred when
M sg status changes to sent and received, respectively.

By considering value ranges and level subsets, we allow for an abstraction that
ignores some of the changes in variable values. For ordered categorical variables,
grouping adjacent levels with respect to the order into a single level is a mean-
ingful way of abstraction. For numeric variables, there are many clever ways of
discretization [19] into intervals, both supervised and unsupervised. Because the

6

Z. Khasidashvili

right discretization for root-causing is diﬃcult to quickly estimate upfront, our
suggestion is to use discretization techniques where selected ranges can overlap
and have diﬀerent lengths (including length zero) [16,17]. Then the root-causing
algorithm will have a wider choice to select the most relevant event sets.

We assume that a trace (or trace log) is a collection of event occurrences,

each associated with the following attributes:

– The cycle or a time stamp of the event occurrence. Usually, for the purpose
of validation or root-causing, the absolute values of time stamps are not
relevant and therefore are abstracted away, what is important is that the
time information captures the sequential vs concurrency information along
a trace. In other contexts, the precise timing information can be critical for
detecting anomalous behavior (e.g., [32]).

– The module from which the event is originating. This allows one to zoom-
in into execution of a particular module that contributes to the events of
interest, if debugging so far suggests that the root-cause might be found in
that module.

– An index uniquely determining the instance of the module from which the
event occurrence is originating, if multiple instances of that module can be
executed, including concurrently, along an execution.

Finally, we assume that a ﬂow is succinctly represented as a sequence of
events that must occur along the trace of a successful execution (if that ﬂow
was triggered). In diﬀerent languages, ﬂows are speciﬁed in diﬀerent forms such
as regular expressions of events or as automata. Work [9] introduces a very rich
visual formalism to specify concurrent multi-agent ﬂows, with well deﬁned formal
semantics. In Specman [13] environment that we are using, reference models can
be used to specify ﬂows. The ﬂow formalisms and validation environments deﬁne
and implement the ability to check the traces against ﬂows and conclude a pass or
fail status of the ﬂow execution. In this work we avoid sticking to any particular
formalism for ﬂows or introducing a new one.

Many formalisms for analyzing traces of concurrent executions assume that
the trace logs satisfy the above requirements (see e.g. [9,27], Sections V and II,
respectively). And the logger scripts generating trace logs in validation and test
environments that we have experimented with support the above requirements
(this is common in validation and test environments in general). This allows
one to correctly identify the fragments associated to any ﬂow instance along the
trace and event occurrences associated to that instance of the ﬂow.

It is important to note that the events that we use in root-causing are not
only the ones that occur in the description of ﬂow of interest – let’s call the
latter the ﬂow-events. Most root-causing approaches try to identify anomalous
sub-sequences of a ﬂow, while we want to explain what goes wrong in-between
the expected ﬂow-events.

Accelerating System-Level Debug Using RL and SD Techniques

7

4 Feature Engineering

As a preprocessing step of the trace logs, from each test in the regression suite,
thanks to the trace properties described in Section 3, we can extract sequences
of event occurrences related to each instance of the ﬂow of interest, and associate
a pass/fail label to each instance. Each such instance of the ﬂow will correspond
to a row (a sample) in the dataset that we build as input to ML algorithms.
For each event e, not necessarily a ﬂow-event, we generate two features:

– A binary 1/0 feature Occurs(e) indicating whether event e occurs along the

execution of that ﬂow instance.

– An integer feature Count(e) continuing the number of occurrences of event

e in the ﬂow instance.

In addition, to encode sequential information among the ﬂow-events, for each
pair of distinct ﬂow-events (e1, e2) such that e1 comes before (or optionally,
comes immediately before) e2 in the ﬂow, we can generate:

– A binary feature IN O(e1, e2), assigned 1 in a ﬂow instance if and only if
2 occurs after

2 of e1 and e2 such that e′

1 and e′

there is a pair of occurrences e′
e′
1 in that ﬂow instance.

– A binary feature OOO(e1, e2), assigned 1 in a ﬂow instance if and only if
1 and e′
2 occurs

2 of e1 and e2 such that e′

there is a pair of occurrences e′
before e′

1 in that ﬂow instance.

The event occurrence feature Occurs(e) can be inferred form the event count
feature Count(e), since Count(e) = 0 iﬀ Occurs(e) = 0, but inclusion of oc-
currence features can still help ML algorithms to improve root-causing accu-
racy. In fact, from our experience, occurrence features alone are in most cases
enough for root-causing P kgC8 ﬂow failures and we will use them in illustrat-
ing examples. Recall that we associated events CLK OF F ≡ Clk = 0 and
CLK ON ≡ Clk = 1 to clock signal Clk; now to each of them we associate a
binary occurrence feature which we again denote by CLK OF F and CLK ON ,
respectively.

The count features are more useful for performance analysis, pre- or post-
silicon, and for performance monitoring during in-ﬁeld operation, say for detect-
ing anomalous activities such as security attacks that have speciﬁc performance
proﬁles. There is some similarity in the concept between the count features and
proﬁle features used in [12] in the context of predicting predeﬁned bug types.
Since events capture changes in variable values (with some abstraction) and not
the duration of the asserted values, the count features capture well signal value
switching activities in hardware (which is very important for power analysis),
while proﬁling features are not limited to value switching statistics. On the other
hand, the proﬁling features are limited to only a small fraction of observable vari-
ables for which the count features are deﬁned.

There is some redundancy in the in-order features IN O(e1, e2) and out-of-
order features OOO(e1, e2) when they are used in conjunction with the occur-
rence features. For example, IN O(e1, e2) can be inferred from Occurs(e1) and

8

Z. Khasidashvili

Occurs(e2) when e1 or e2 do not occur in the ﬂow instance, or when they occur
exactly once in the wrong order, which is also captured by OOO(e1, e2).

In addition, any known order between any pair of events can be added as a
feature, but not all these sequential dependencies are known and captured in a
validation environment. The automatically learned ﬂows as in [26] can help here.
Our iterative root-causing procedure introduced in Section 5 also helps discov-
ering potentially unknown sequential event dependencies that can be mined for
future root-causing activities and bug prediction. As a root-cause, we are mainly
interested in non-ﬂow events, since the ﬂow stage where a ﬂow instance devi-
ates from the ﬂow description can be inferred from the validation environment.
Therefore, including features that encode sequential information among the ﬂow
events might or might not make the root-causing procedure more eﬃcient.

Work [3] considers event ordering features for encoding sequential informa-
tion in another application domain (a social study), and besides it also encodes
timing information, such as the age of individuals at the time of occurrence of
age related events. In our use case, this corresponds to associating to each event
the elapsed time from the beginning of the ﬂow, extracted from time stamps at-
tached to event occurrences in the logs. In an experimental evaluation comparing
usefulness of timing, sequential and other information, the authors conclude that
the sequential features are the most useful ones for the accuracy of models in
their study. In general, timing features can be generated for pairs of ﬂow events
(or optionally, for a wider set of pairs of events) for which we have only de-
ﬁned in-order and out-of-order features. We have not used timing features for
root-causing P kgC8 ﬂow, and expect that, in general, they will be useful for
performance analysis as well as for detecting anomalous behaviors in traces.

Useful events are not limited to ones that are associated with variables in the
system’s software and ﬁrmware and with logic signals in the hardware. Sensor
data collecting temperature, frequency and voltage information is often very
important information for root-causing, and this is true for PM in particular.
For root-causing post-silicon test and in-ﬁeld operation monitoring trace data,
the test equipment and environmental eﬀects (such as the temperature in the
environment) can also be considered in feature engineering process.

5 The ML Approach

The aim of our method is to explain diﬀerences between the passing tests and
subgroups of failing tests in terms of the information available in the trace logs.
The identiﬁed subgroups of failures must be such that all failures in a subgroup
have the same root-cause, and failures in diﬀerent subgroups represent diﬀerent
root-causes. Our method does not require full understanding of all legal behaviors
of the system. It closely follows the intuition behind the debugging steps that
the validators follow in root-causing regression failures. We describe how this
method works on root-causing PM P kgC8 ﬂow regression test failures.

Accelerating System-Level Debug Using RL and SD Techniques

9

5.1 Feature Range Analysis

The ML approach that we use for root-causing is F eature Range Analysis, or
Range Analysis (RA), for short [16,17]. Range Analysis is an algorithm resem-
bling Rule Learning (RL) [6,10] and Subgroup Discovery (SD ) [18,35,1]. From
the algorithmic perspective, the main distinguishing feature of RA is that it
heavily employs Feature Selection [11] in two basic building blocks of the algo-
rithm: the ranking and basis procedures. The third basic building block in RA,
the procedure called quality, resembles the technique used in RL and SD , where
the selection of rules or subgroups is done solely based on one or more quality
functions. These three procedures will be explained below.

For a numeric feature F , RA deﬁnes a single range feature as a pair R ≡
(F , value range), interpreted as a binary feature R(s) ≡ F (s) ∈ value range,
for any sample s. And for a categorical feature and level l, the corresponding
single range feature is deﬁned as R(s) ≡ F (s) = l. A range feature is a single
range feature or a conjunction of single range features, also referred to as range
tuples or simply as ranges. A range R covers a sample s iﬀ R(s) evaluates to
true. In the latter case, we will also say that s is in R or is captured by R.

The main purpose of RA is to identify range features that explain positive
samples, or subsets (subgroups) of positive samples with the same root-cause.
The RA algorithm generates range features that are most relevant for explaining
the response, where ‘most relevant’ might mean (a) having a strong correlation
or high mutual information with the response, based on one or more correlation
measures; (b) explaining part of the variability in the response not explained
by the strongest correlating features; or (c) maximizing one or more quality
functions, which are usually designed to prefer ranges that separate subgroups
of positive samples from negate samples; that is, a range is preferred if a majority,
or perhaps a great majority, of positive samples are covered by the range and a
great majority of negative samples fall outside the range. Important examples
of quality functions include the ones deﬁned below.

By treating a range R as a classiﬁer that predicts the positive class for sam-
ples covered by the range and negative class for samples outside the range, one
can apply the well known quality metrics for binary classiﬁcation to characterize
the quality of ranges. Below we list some of the well known and often used qual-
ity metrics for binary classiﬁcation, where T P (R), F P (R), T N (R) and F N (R)
denote true positive, false positive, true negative and false negative, respectively,
n(R) denotes the samples count within range R, P os denotes the positive sam-
ples count, and N denotes the entire samples count. Thus in this context one
can interpret T P (R) as the count of positive samples covered by the range R,
F P (R) as the count of negative samples covered by the range, T N (R) as the
count of negative samples not covered by the range, F N (R) as the count of
positive samples not covered by the range, and n(R) as the count of samples
predicted by R as positive. Then the concepts of quality deﬁned for classiﬁcation
tasks an be deﬁned for ranges R as follows:

– True Positive Rate (or sensitivity, recall, or hit rate), T P R(R) = T P (R)
P os

10

Z. Khasidashvili

– Predictive Positive Value (or precision) P P V (R) =
– Lift Lif t(R) = T P (R)/n(R)
– Normalized Positive Likelihood Ratio N P LR(R) =
– Weighted Relative Accuracy W RAcc(R) = n(R)
– ROC Accuracy associated with Receiver Operating Characteristic (ROC)

T P R(R)
T P R(R)+F P R(R)
n(R) − P os
N )

N · ( T P (R)

= P P V (R)
P os/N

P os/N

T P (R)

T P (R)+F P (R) = T P (R)
n(R)

ROCAcc(R) = T P R(R) − F P R(R)
– F1-score F 1Score(R) = 2·P P V (R)·T P R(R)
P P V (R)+T P R(R)
– Accuracy Acc(R) = T P (R)+T N (R)

N

The RA algorithm works as follows:

1. The RA algorithm ﬁrst ranks features highly correlated to the response; this
can be done using an ensemble Feature Selection [11] procedure, we refer
to it as ranking procedure. In addition, RA uses the mRM Re [7] algorithm
to select a subset of features which both strongly correlate to the response
and provide a good coverage of the entire variability in the response; this
algorithm selects a subset of features according to the principle of Maxi-
mum Relevance and Minimum Redundancy (MRMR) [8], we refer to this
procedure as basis.

2. For non-binary categorical features selected using the ranking and basis pro-
cedures described above, or optionally, for all non-binary categorical features,
for each level a binary range feature is generated through the one-hot en-
coding. In a similar way, a fresh binary feature is generated for each selected
numeric feature and each range constructed through a discretization proce-
dure. These features are called single-range features. Unlike RL and SD , in
RA the candidate ranges can be overlapping, which helps to signiﬁcantly im-
prove the quality of selected ranges. The RA algorithm then applies ranking
and basis procedures to select the most relevant single ranges; in addition,
RA selects single range features that maximize one or more quality functions.
We refer to the quality-function based selection of ranges as quality.

3. For each pair of selected single range features associated with diﬀerent orig-
inal features, RA generates range-pair features which have value 1 on each
sample where both the component single-range features have value 1 and
have value 0 on the remaining samples. RA then applies ranking, basis and
quality procedures to select the most relevant range pairs.

4. Similarly, from the selected single ranges and selected range pairs, the RA
algorithm builds range triplets, and applies ranking, basis and quality pro-
cedures to select most relevant ones. Ranges with higher dimensions can be
generated and selected in a similar way.

RA is implemented in Intel’s auto-ML tool EVA [16,17]. It has been used for
root-causing and design space exploration also in other areas of validation [23].
In this work, in addition to quality function based selection, we suggest to
use quality orders for prioritizing ranges for selection. In particular, we deﬁne

Accelerating System-Level Debug Using RL and SD Techniques

11

Coverage Weighted Precision (CWP), or Weighted Precision for short, as a lexi-
cographic order on pairs (precision, coverage), where coverage denotes the count
of samples covered by the range feature. That is, CWP (R1) > CWP (R2) if and
only if P P V (R1) > P P V (R2) or P P V (R1) = P P V (R2) ∧ n(R1) > n(R2).
For binary responses, which is focus of this work, CWP induces the same or-
dering as Positive Coverage Weighted Precision (PCWP) deﬁned as a lexico-
graphic order on pairs (precision, positive coverage), where positive coverage
is the count of positive samples covered by the range feature. For numeric re-
sponses, there might be multiple suitable ways to deﬁne positive samples (see
Section 6.2 in [17]), thus there will be multiple useful ways to deﬁne PCWP
orderings. We note that W RAcc also prefers selection of ranges with high cover-
age of positive samples, but the ordering induced by W RAcc is not equivalent to
weighted precision. For example, consider dataset with N = 80 samples contain-
ing P os = 20 positive ones, and consider two range tuples R1 and R2, covering
T P (R1) = 10 positive and F P (R1) = 0 negative samples, and T P (R2) = 13
positive and F P (R2) = 7 negative samples, respectively. Then P P V (R1) = 1
is signiﬁcantly higher than P P V (R2) = 0.65 while W RAcc(R1) = 0.09375 is
smaller than W RAcc(R2) = 0.1.

As mentioned earlier, the RL algorithms are designed for building predictive
models that operate through rules, they have been studied mainly for classiﬁca-
tion problems, and the algorithms are symmetric with respect to class labels –
both classes are equally important to predict accurately. The SD algorithms are
designed to ﬁnd subgroups of data that maximize one or more quality functions,
with a bias towards isolating ”interesting” subgroups of positive samples through
rules; their aim is not to build any predictive models. It is well understood that
the techniques used in these two ﬁelds have much in common. RA was proposed
as one way to unify these two ﬁelds: it aims at discovering interesting subgroups,
captures them as binary features (namely, the range features), and proposes to
use the range features along with the original (non-range) features to build ac-
curate models, which are supposed to be more accurate especially on positive
samples (which is also assumed to be a minority class). This goal is achieved
through the way the RA algorithm is composed: it uses quality function based
selection of ranges just like SD algorithms, but it also applies a mixture of fea-
ture selection algorithms that are known as must-have for selecting features that
are most relevant for balding high-quality models. In this work we further en-
hance the RA paradigm by aiming not only to engineer range features that help
to improve binary classiﬁcation accuracy, but also use them for building multi-
class prediction models where along with the negative class we have multiple
new positive class labels learned through an RA-based root-causing procedure
described in Subsection 5.3. Subsection 5.6 gives more detail on how the rules
used in the predictive models are learned.

5.2 Basic Root-Causing Step

We explain the main idea of how RA helps root-causing on an example of Fig-
ure 2a, which displays a binary feature CLK OF F , associated to event Clk = 0,

12

Z. Khasidashvili

(a) Range
highly by RA.

feature

ranked

(b) Root-causing tree of failures of PkgC8 and
wakeup-timer.

Fig. (2)

Iterative root-causing procedure.

ranked highly by RA and plotted for validator’s inspection. The dots in the plot,
known as a Violin plot, correspond to rows in the input dataset, thus each dot
represents a passing or failing instance of P kgC8 ﬂow. Value 1 on the Y axis (the
upper half of the plot) represents the failing tests and value 0 (the lower half)
represents the passing ones. The plot shows that when CLK OF F = 0, that is,
when signal Clk was never set to 0 along the execution of the ﬂow instance, all
tests fail! This is a valuable information that the validator can use as an atomic
step for root-causing. The legend at the top of the plot speciﬁes that there are
actually 115 failing tests and 0 passing tests with CLK OF F = 0, and there are
85 failing tests and 247 passing ones with CLK OF F = 1.

Let’s note that the range feature (CLK OF F, 0) has precision 1, as deﬁned in
Subsection 5.1, since it only covers positive samples (which correspond to failing
tests). To encourage selection of range features with precision 1 in root-causing
runs, in RA we use weighted precision along with WRAcc and Lift, which also
encourage selection of range features with precision 1.

5.3 Root-Causing Strategies

During diﬀerent stages of product development, the nature of validation activi-
ties might vary signiﬁcantly. When there are many failing tests, the root-cause
for failures might be diﬀerent for subgroups of all failed tests. In this case it is
important to be able to discover these subgroups of failing tests so that failures
in each subgroup can be debugged together to save validation eﬀort and avoid
duplication. On the other hand, it is important to be able to eﬃciently debug
individual failures. In our approach, the strategies of identifying subgroups of
failing tests with similar root-cause and putting focus on root-causing individ-
ual failures correspond to breadth-ﬁrst and depth-ﬁrst search strategies in the
root-causing tree that we deﬁne next.

Accelerating System-Level Debug Using RL and SD Techniques

13

Our proposed procedure for root-causing regression failures is an iterative
process and requires a guidance from the validator to decide whether the ranked
root-causing hints are good enough or further reﬁnements are needed. The it-
erative root-causing procedure is illustrated using Figure 2b. More speciﬁcally,
Figure 2b presents a root-causing tree of regression failures of P kgC8 with ran-
dom wakeup-timer, which will issue an interrupt to trigger a wake-up out of
P kgC8 ﬂow. Using the procedure described next, after a few iterations, at the
leaves of the tree we get subgroups of failures characterized by the length of the
wakeup-timer and which actions were achieved. Initially, in the regression we see
200 failing tests as one group of failing tests, and there are 247 passing tests.

(a) [Initialization] Initialize the root-causing tree RCT to a node n, mark it
as open, set the input dataset D to be passed to the iterative root-causing
procedure to the dataset extracted from traces as described in Section 4, and
set the number of iterations i = 0. Go to step (c).

(b) [Iterations] In this step, the procedure receives a four-tuple (RCT, n, S, DS)
where RCT is the current tree, n is a node in it selected for splitting, S is the
subgroup of failing tests associated to node n, and DS is the dataset to be
used for splitting the subgroup S, with the aim to arrive to smaller subgroups
with diﬀerent root-cause or root-causes each. RA is performed on DS and
highly ranked range features are presented to the validator who chooses the
best range feature to split the tree at node n. (Any range feature not identical
to the response implies a split in S.) We add two sons to RCT : one will be
associated with the subgroup of failing tests covered by the selected range
feature, and the other one with the subgroup containing the rest of failing
tests from S; both sons are marked as open. Increment i and go to step (c).

(c) [Decisions]

1. If all leaves of RCT are marked as closed, exit. Otherwise go to (c).2:
2. Choose a leaf node n not marked as closed – this is the leaf targeted for
splitting in order to reﬁne the associated subgroup S of failing tests. if
the validator decides there is no need to split S, that is, all failing tests
in S can be associated to a single root-cause, then mark the leaf n as
closed and go to (d). Otherwise go to (c).3:

3. Deﬁne DS as follows: if i > 1, from the input dataset D subset rows that
correspond to failing tests in this subgroup and rows corresponding to
the passing tests, to obtain dataset DS. Drop from DS all features that
were used for splitting at the parent nodes; in addition, drop the features
that are identical to the dropped features, as well as features that are
identical to the response (the class label), in DS. Otherwise (case i = 0),
set DS = D. Go to (c).4.

4. If there are no features in DS and no new features can be added, mark
the leaf node n as closed and go to step (c). Otherwise, if DS contains all
features relevant to S (subject to validator’s decision), or no additional
features are available, go to step (b) with four-tuple (RCT, n, S, DS).
Otherwise go to step (e) with four-tuple (RCT, n, S, DS).

14

Z. Khasidashvili

(d) [Knowledge Mining] This step performs mining of knowledge learned along
a closed branch of the root-causing tree, and it will be discussed further in
Subsection 5.6.

(e) [Data Reﬁnement ] This step corresponds to the situation when the validator
wants to add more features that can potentially help to split subgroup S
further, and it will be discussed further in Subsection 5.5.

Let’s note that in the root-causing tree of Figure 2b, event C8 ACT ION S is
not a ﬂow-event – it does not occur in P kgC8 ﬂow description. It helps to explain
why some of the ﬂow instances that successfully go through the CLK OF F stage
of the P kgC8 ﬂow fail to complete the ﬂow. Also, the events of the P kgC8 that
should occur between CLK OF F and CORE W AKE T O C0 do not occur in
the root-causing tree. The root-causing tree therefore gives a pretty neat and
succinct explanations and grouping of the failures, and gives deeper insights
compered to just identifying where in the ﬂow the ﬂow instance starts to deviate
from the expected execution. As discussed in Section 4, the information that
event C8 ACT ION S should occur before events notifying exit stages of P kgC8
ﬂow can be mined as out-of-order features, optionally. During building the tree
in Figure 2b, this information was not coded as a feature in the input dataset D.
In each splitting iteration, the range feature selected for splitting can be
multi-dimensional, such a pairs or triplets of feature-range pairs. That is, in
general splitting is performed based on a set of events, not necessarily based
on just one event. This will be discussed in more detail in Subsection 5.4. Each
iteration is zooming into a lower-level detail of the ﬂow execution and smaller
set of modules of the system.

We note that the features identical to the response in dataset DS that are
dropped from analysis in step (c) are very interesting features for root-causing
as they fully separate the current subgroup S and the passing tests.

The iterative root-causing procedure assumes that there are passing tests.
This is a reasonable assumption but does not need to hold at the beginning of a
design project. In such cases, unsupervised rule based or clustering algorithms
can be used, as discussed in the introduction.

Finally, let’s note that at every iteration in the proposed root-causing proce-
dure, we select only one range feature to perform a split. However, multiple range
features can be selected and splitting can be performed in parallel (and then the
root-causing tree will not be binary) or in any order (then the root-causing
tree will stay binary and in each iteration we add more than one generation of
children to the tree). This reduces the number of required iterations.

5.4 Root-Causing Using Range Tuples

Consider the feature signal y 4 in Figure 3a, associated to event signal y =
4, and feature timer 22159 22714 in Figure 3b, associated to event timer ∈
[22159, 22714]. While for a big majority of tests (67 out of 70, in this case) the
value 0 of feature signal y 4 implies test failure, there are a few tests (3 out of 70,
in this case) that are passing. So, the single range feature R1 = (signal y 4, 0)

Accelerating System-Level Debug Using RL and SD Techniques

15

(a) Highly ranked single
range feature signal y can-
not
failing tests
from the passing ones.
Fig. (3) Usefulness of range tuple features in iterative root-causing procedure.

(c) Range pair that com-
bines single ranges (a) and
(b)
failing tests
from the passing ones.

(b) Single range feature
timer 22159 22714 alone
cannot isolate failing tests
from the passing ones.

isolates

isolate

cannot fully isolate the failing tests from passing ones. As can be seen from
Figure 3b, the second single range feature R2 = (timer 22159 22714, 0) cannot
isolate failing tests from the passing tests either – along with 67 failing tests, there
are 106 passing tests when timer does not occur at time range [22159, 22714].
However, the range pair feature P = R1 ∧ R2 (the conjunction of two binary
features R1 and R2, as deﬁned in Subsection 5.1) fully isolates the 67 failing
tests from the passing tests, because the 3 negative samples covered by R1 and
106 negative sample covered by R2 do not intersect. The range pair feature P is
depicted in Figure 3c, where the color encodes the ratio of positive and negative
samples within the colored squares: the red color encodes positive samples and
green encodes negative samples.

Usage of range pairs and range tuples of high dimensions can accelerate de-
bugging as they allow to arrive to a root-cause in fewer debugging iterations.
On the other hand, the splits of subgroups of failures associated to high dimen-
sional range tuples might not match the intuition of the validator controlling
the iterations in the root-causing tree. Thus the time spent by RA algorithm (or
other RL or SD algorithms which can be used instead of RA at each splitting
iteration) will be wasted. This is the point where automating the root-causing
iteration is currently missing. One useful heuristics of choosing ranges for next
splitting iterations is to choose the range features with precision 1 that originate
from the ﬂow-events as a default, without intervention of the validator, if such
range features are selected by RA as candidates for splitting.

To aid the validator in choosing the range features that are associated to well
understood events, to every ranked candidate range features we associate a list
of very highly correlated or identical features, with respect to DS. Thus, instead
of a highly ranked feature the validator can chose another representative with a
clear interpretation. These strongly correlated or identical features are computed
by RA anyway, as a means to reduce the redundancy in the identiﬁed subgroups

16

Z. Khasidashvili

and to speed-up the algorithm [16,17]. In RA, the usage of the M RM R procedure
helps to not miss interesting range features for splitting, when there are many
features in the analysis. Finding diverse, non-redundant and and concise sets
of subgroup is an important direction in the SD research [21,22], and is very
relevant in the context of minimizing the validator’s involvement as the decision
maker in controlling the root-causing iterations.

5.5 Root-Causing in Hierarchical Order

For large systems with many modules, it might not be wise to generate upfront
all features that can be engineered from traces, and include them into the initial
dataset D used in the ﬁrst root-causing iteration. The root-causing iterations
can also be performed in a hierarchical manner, and this is actually a preferred
option which we adopted in our experiments.

In the hierarchical approach, during the initial iterations, one can choose to
only consider features associated with some of the important modules as well
as with observable signals on module interfaces, in order to include in analy-
sis the features associated to all the ﬂow-events in the ﬂow of interest. And if
root-causing hints collected so far indicate that the bug could be in a particular
module or modules, features originating from these modules can be added to
analysis (and optionally, features from some other modules can be dropped).
This ﬂexibility of reﬁning the input data during root-causing iterations helps to
keep the datasets analyzed by ML algorithms smaller and focused, and speeds up
the root-causing iterations. In particular, when the iterative procedure arrives at
step (e), one could consider adding features originating from relevant modules
if they were not present, or consider engineering more features associated to ad-
ditional variables from the relevant modules, when possible, and then continue
with step (b), with tuple (RCT, n, S, DS) received at step (e), but with the up-
dated DS. As an example, in the root-causing tree of Figure 2b, the rightmost
branch was closed with label Reached last stage bef ore C8, because data from
the respective module was not available at the time of initial experiments. In-
terestingly, when the relevant data became available and was added to analysis,
further root-causing revealed that the corresponding regression test failures were
not caused by a bug, instead, the self-checks in these tests were incorrect, and
were ﬁxed as a result of the gained insights during the root-causing iterations.

5.6 Mining Rules

In order to make the root-causing easier for non-expert validators and reduce
their dependency on expert validators, and in order to enable leveraging of knowl-
edge accumulated during root-causing activities both for expert and non-expert
validators, we propose to mine validation knowledge as follows:

– We maintain and improve a table associating their meaning with well un-
derstood events. This table is used to display the meaning of the events
and event combinations associated to range features ranked highly by RA

Accelerating System-Level Debug Using RL and SD Techniques

17

algorithm when the latter are presented to the validator to choose the best
root-causing hints and decide on next root-causing iteration.

– We construct and improve a database or rules as follows: At step (d) of the
root-causing procedure, before returning to step (c), the validator can assign
a label to the respective subgroup of failures, and add a rule to the database
of rules associated to the branch of the tree leading to that subgroup. The
antecedent of this rule is the conjunction of conditions associated with the
ranges along this branch and the consequent is the label of the subgroup S
at the leaf of this branch. As an example, the rule associated to the branch
ending at node Clk Of f without C8 Actions is written as:

CLK OF F = 1 ∧ C8 ACT ION S = 0 −→ Clk Of f without C8 Actions

The learned rules in the rule database can be used to predict root-cause
of test failures in future regressions by building predictive models from rules,
using the algorithms developed under RL or related algorithms such as [21] that
combine RL and SD techniques for building predictive models from rules. An
alternative approach is to train a model predicting the learned failure labels
(failure labels learned during RCT construction) once a rich set of failure labels
and dataset D extracted from passing tests and failing tests with these failure
type are available, like this is done in previous work [25,12] for predeﬁned failure
labels. In addition to the features in D, the range features corresponding to the
left-hand sides of the mined learned rules can be used as derived features when
training the model, as this can improve the quality of the trained model as was
demonstrated in [17].

It is important to point out that the learned failure labels and rules added
to the database are based on the intended behavior of the system and not on its
current implementation. Thus, when the design is updated (say due to bug ﬁxes),
the design intent usually stays the same. Therefore, the failure labels and rules
stay relevant as the design evolves, as long as the design intent is not changed.
That is, the rules are more like the reference models or other types of assertions,
but have the opposite meaning in that they capture cases of incorrect behavior
rather than the intended behavior.

5.7 Backtracking and Termination of Root-Causing Procedure

In our root-causing procedure, we did not discuss backtracking during the RCT
construction. When backtracking is not used, each subgroup of failing tests is
split at most once, in a ﬁnite number of ways (in binary RCT construction
procedure, a subgroup is split in two subgroups or not split at all). Furthermore,
steps (a), (b), (d) transition to step (c) which starts with a termination check,
and step (e) transitions to (b) which then transitions to (c). Therefore, assuming
backtracking is not used and assuming step (b) succeeds by selecting a range that
is used for splitting, it is straightforward to conclude that the RCT construction
will terminate (there will be no inﬁnite loops).

In general, it can be the case that step (b) does not generate valuable root-
causing hints and the validator cannot reﬁne the subgroup at hand further. The

18

Z. Khasidashvili

most likely reason for this is that the used dataset does not contain relevant
features. If that is the case, it is useful to backtrack to the same subgroup S
but upgrade the dataset DS by adding potentially relevant features, by going
through step (e). Otherwise, the problem might be due to incorrect splitting
decisions taken earlier during RCT constriction or poor quality of top ranked
range features returned by RA, and in such cases too backtracking to an already
visited subgroup might be useful. Assuming the number of features available for
root-causing is limited and only a limited number of backtracking to an already
visited subgroup is allowed, RCT construction will always terminate.

6 Results

We explored the Range Analysis algorithm for root-causing Power Management
protocol failures on several regressions, starting with a synthetic regression of
P kgC8 and random wakeup-timer (Figure 2b), and continuing with authentic
random PM regressions on a FullChip emulation platform, on two recent Intel
products. We found this method very eﬀective for classifying the failures and
providing valuable root-causing hints.

The hierarchical approach to root-causing allows to keep the datasets fed to
RA pretty small. While RA can handle input datasets with thousands of columns
and millions of rows, it is important to keep the datasets small to speedup the
iterations. As an example, in the experiments with synthetic regression with ran-
dom wake-up timer, the number of occurrence features range from a few hundred
to a thousand (the count features were not used after initial experiments, and
the number of sequential features depends on the number of ﬂow-events). The
number of rows was from around few hundreds to a few thousands. The largest
analyzed dataset had 944 features and 2826 rows. On such small datasets, each
iteration would typically take up to 2 minutes. Number of iterations range from
just one to 10 or so, with the depth of the tress reaching up to four or ﬁve levels
on average.

Table 1 reports RA results after the ﬁrst RCT iteration on the dataset with
944 features and 2826 rows mentioned above. All features in that dataset are
binary, obtained through one-hot encoding of categorical variables and through
discretization of numeric variables from trace logs, and represent event occur-
rence information (count features and features encoding sequential information
were not used). Each row corresponds to a selected range feature (the names
of features and the selected ranges are not shown), and the ﬁrst column Dim
denotes the dimension of the range features – range features up to triplets were
generated. RA heuristics for generating diverse set of ranked range features were
used, and therefore the top ranked range features – the selected subgroup de-
scriptions – are diverse, though the subgroups of samples that they deﬁne have
overlaps. The remaining columns report the confusion matrix statistics and qual-
ity metrics deﬁned in Section 5.1. The selected ranges were sorted according to
weighted precision deﬁned in Section 5.1. One can see that many ranges with
precision 1 have been selected, which include single ranges and range triplets.

Accelerating System-Level Debug Using RL and SD Techniques

19

Actually, all the selected ranges were of high quality. The validator actually chose
the highest ranked range for splitting, in order to further root-cause the seven
false negatives (value 7 occurs in column F N in the ﬁrst row of Table 1). The
second iteration was RA run on the dataset obtained by dropping all positive
samples except the seven false negatives. With the ranges reported by the second
RA run, it was possible to split the seven test failures into subgroups of four,
two and one failures, respectively, which resulted in full root-causing of all seven
failures.

Our approach allows root-causing regression test failures that are manifested
as incorrect order of events in the execution. For example, an agent might be
attempting to read a register value in a CPU while the latter was shut down
and was awaiting for the V RS SET T O >0V event in order to wake-up (see
Figure 1). Some of the regression failures manifested as wrong order of events
might due to race condition bugs. The sequential and concurrency information
that we encode as part of feature extraction process is currently very basic, and
we expect that smart encoding schemes will be developed in the future to further
facilitate automated root-causing of concurrency bugs using ML techniques.

7 Related Work and Discussion

Work [20] discusses three use cases in other application domains – one medical,
two marketing – with the aim to highlight advantages of SD methodology and the
wide range of algorithms and quality functions developed under SD for the task
of identifying relevant subgroups of a population. While the above application
domains are pretty remote from validation, big part of discussion and learning
from [20] apply to our work as well, and we repeat here some of the most relevant
arguments (see Sections 3 and 8,

[20]).

One can distinguish between objective quality measures and subjective mea-
sures of interestingness [31]. Both the objective and subjective measures need to
be considered in order to solve subgroup discovery tasks. Which of the quality
criteria are most appropriate depends on the application. Obviously, for auto-
mated rule induction it is only the objective quality criteria that apply. However,
for evaluating the quality of induced subgroup descriptions and their usefulness
for decision support, the subjective criteria are more important, but also harder
to evaluate. Below is a list of subjective measures of interestingness:

1. Usefulness. Usefulness is an aspect of rule interestingness which relates a

ﬁnding to the goals of the user [18].

2. Actionability. A rule (pattern) is interesting if the user can do something

with it to his or her advantage [28,31]).

3. Operationality. Operationality is a special case of actionability. Operational
knowledge enables performing an action which can operate on the target
population and and change the rule coverage. It is therefore the most valuable
form of induced knowledge [20].

4. Unexpectedness. A rule (pattern) is interesting if it is surprising to the

user [31].

20

Z. Khasidashvili

Dim FN TN TP FP TPR PPV Lift NPLR WRAcc ROCAcc F1Score Accuracy
0.9975
0.9961
0.9880
0.9876
0.9858
0.9820
0.9795
0.9791
0.9781
0.9774
0.9770
0.9745
0.9742
0.9728
0.9724
0.9611
0.9597
0.9593
0.9575
0.9568
0.9561
0.9473
0.9452
0.1083
0.8301
0.8294
0.8294
0.8294
0.9572
0.9540
0.9565
0.9844
0.9441
0.9420
0.9558
0.9774
0.9628
0.9621
0.9490
0.9540
0.9554
0.9498
0.9452
0.9568
0.9498
0.9335
0.9335
0.9292
0.9360

0 0.9973 1.0000 1.0762 1.0000 0.5328
1
7 200 2619
0 0.9958 1.0000 1.0762 1.0000 0.5327
3
11 200 2615
0 0.9871 1.0000 1.0762 1.0000 0.5325
3
34 200 2592
0 0.9867 1.0000 1.0762 1.0000 0.5324
1
35 200 2591
0 0.9848 1.0000 1.0762 1.0000 0.5324
1
40 200 2586
0 0.9806 1.0000 1.0762 1.0000 0.5322
3
51 200 2575
0 0.9779 1.0000 1.0762 1.0000 0.5322
3
58 200 2568
0 0.9775 1.0000 1.0762 1.0000 0.5321
3
59 200 2567
0 0.9764 1.0000 1.0762 1.0000 0.5321
3
62 200 2564
0 0.9756 1.0000 1.0762 1.0000 0.5321
3
64 200 2562
0 0.9752 1.0000 1.0762 1.0000 0.5321
3
65 200 2561
0 0.9726 1.0000 1.0762 1.0000 0.5320
3
72 200 2554
0 0.9722 1.0000 1.0762 1.0000 0.5320
1
73 200 2553
0 0.9707 1.0000 1.0762 1.0000 0.5319
3
77 200 2549
0 0.9703 1.0000 1.0762 1.0000 0.5319
78 200 2548
3
0 0.9581 1.0000 1.0762 1.0000 0.5315
1 110 200 2516
0 0.9566 1.0000 1.0762 1.0000 0.5315
1 114 200 2512
0 0.9562 1.0000 1.0762 1.0000 0.5314
3 115 200 2511
0 0.9543 1.0000 1.0762 1.0000 0.5314
1 120 200 2506
0 0.9535 1.0000 1.0762 1.0000 0.5314
3 122 200 2504
0 0.9528 1.0000 1.0762 1.0000 0.5313
1 124 200 2502
0 0.9433 1.0000 1.0762 1.0000 0.5310
3 149 200 2477
0 0.9410 1.0000 1.0762 1.0000 0.5309
3 155 200 2471
0 0.0404 1.0000 1.0762 1.0000 0.5013
3 2520 200 106
2 0.8180 0.9991 1.0752 0.9879 0.5266
2 478 198 2148
4 0.8180 0.9981 1.0742 0.9761 0.5262
2 478 196 2148
4 0.8180 0.9981 1.0742 0.9761 0.5262
2 478 196 2148
4 0.8180 0.9981 1.0742 0.9761 0.5262
2 478 196 2148
7 0.9566 0.9972 1.0732 0.9647 0.5303
2 114 193 2512
8 0.9535 0.9968 1.0727 0.9597 0.5300
2 122 192 2504
9 0.9566 0.9964 1.0723 0.9551 0.5300
2 114 191 2512
3
34 190 2592 10 0.9871 0.9962 1.0720 0.9518 0.5308
2 148 190 2478 10 0.9436 0.9960 1.0718 0.9497 0.5294
2 154 190 2472 10 0.9414 0.9960 1.0718 0.9496 0.5293
1 114 189 2512 11 0.9566 0.9956 1.0715 0.9456 0.5296
34 170 2592 30 0.9871 0.9886 1.0638 0.8681 0.5275
2
63 158 2563 42 0.9760 0.9839 1.0588 0.8229 0.5252
2
34 127 2592 73 0.9871 0.9726 1.0467 0.7300 0.5205
2
72 128 2554 72 0.9726 0.9726 1.0467 0.7298 0.5201
2
47 117 2579 83 0.9821 0.9688 1.0426 0.7030 0.5186
3
32 106 2594 94 0.9878 0.9650 1.0385 0.6776 0.5170
1
47 105 2579 95 0.9821 0.9645 1.0379 0.6740 0.5167
2
54 99 2572 101 0.9794 0.9622 1.0355 0.6598 0.5156
2
6 84 2620 116 0.9977 0.9576 1.0305 0.6324 0.5137
2
6 64 2620 136 0.9977 0.9507 1.0231 0.5947 0.5104
1
51 63 2575 137 0.9806 0.9495 1.0218 0.5887 0.5097
2
41 53 2585 147 0.9844 0.9462 1.0183 0.5725 0.5082
1
49 49 2577 151 0.9813 0.9446 1.0166 0.5652 0.5074
1
1
2 21 2624 179 0.9992 0.9361 1.0074 0.5275 0.5034
Table (1) Range Analysis results in ﬁrst iteration of RCT construction

0.9986
0.9979
0.9935
0.9933
0.9923
0.9902
0.9888
0.9886
0.9881
0.9876
0.9874
0.9861
0.9859
0.9851
0.9849
0.9786
0.9778
0.9776
0.9766
0.9762
0.9758
0.9708
0.9696
0.0777
0.8995
0.8991
0.8991
0.8991
0.9765
0.9747
0.9761
0.9916
0.9691
0.9679
0.9757
0.9878
0.9799
0.9798
0.9726
0.9754
0.9763
0.9732
0.9707
0.9772
0.9736
0.9648
0.9649
0.9626
0.9666

0.9987
0.9979
0.9935
0.9933
0.9924
0.9903
0.9890
0.9888
0.9882
0.9878
0.9876
0.9863
0.9861
0.9853
0.9851
0.9791
0.9783
0.9781
0.9772
0.9768
0.9764
0.9716
0.9705
0.5202
0.9040
0.8990
0.8990
0.8990
0.9608
0.9568
0.9558
0.9685
0.9468
0.9457
0.9508
0.9185
0.8830
0.8110
0.8063
0.7836
0.7589
0.7536
0.7372
0.7089
0.6589
0.6478
0.6247
0.6132
0.5521

Accelerating System-Level Debug Using RL and SD Techniques

21

5. Novelty. A ﬁnding is interesting if it deviates from prior knowledge of the

user [18].

6. Redundancy. Redundancy amounts to the similarity of a ﬁnding with respect
to other ﬁndings; it measures to what degree a ﬁnding follows from another
one [18], or to what degree multiple ﬁndings support the same claims.

In the context of pre-silicon validation of microprocessor design, actionable
rules help ﬁxing and optimizing the design, and this way improve the ﬁnal prod-
uct quality and yield, therefore in most cases these rules are also operational. In
case of post-silicon validation, if root-causing identiﬁes say manufacturing equip-
ment or process issues that can be ﬁxed, the rules used for root-causing are still
operational; however, if the root-cause cannot be ﬁxed, one cannot improve man-
ufacturing yield, thus cannot reduce the ratio / count of manufactured defective
dies, and the rules used for root-causing cannot be considered as operational.

The main reason why our root-causing approach is not fully automated is
that we rely on validator’s feedback to decide when splitting of failure subgroups
is not required any further. For example, to decide whether all failures in each
subgroup of failures at the leaves of the RCT constructed so far have the same
root-cause, with actionable insights. Even if the subgroup descriptions – the range
tuples – were generated automatically so that all subgroup descriptions together
cover all the positive samples and only positive ones, and the samples covered
by these subgroup descriptions are disjoint, it is not guaranteed that these sub-
groups are the right ones, and that the descriptions of these subgroups are the
right ones.1 Formal speciﬁcations enabling automating such a decision (to keep
human out of the loop) do not exist, and as discussed in the introduction, such
decisions can also depend on the context and goals of root-causing, and on how
deep root-causing should go. We expect that further automation can be added in
this direction by mining expert validator’s decisions through labeling the identi-
ﬁed intermediate root-causing hints and the rules that can isolate the associated
subgroups of failures, by extending ideas proposed in Subsection 5.6. These rules
can be reused for predicting and classifying failure root-causes in future regres-
sions. In particular, rules and models learned during pre-silicon validation can
be reused for post-silicon validation and later for tracking anomalous behaviors
during in-ﬁeld operation.

Work [20] also compares SD methodology with decision trees and classiﬁca-

tion rule learning. Here are the main relevant points:

1. The usual goal of classiﬁcation rule learning is to generate separate mod-
els, one for each class, inducing class characteristics in terms of properties
(features) occurring in the descriptions of training examples. Therefore, clas-
siﬁcation rule learning results in characteristic descriptions, generated sepa-
rately for each class by repeatedly applying the covering algorithm, meaning

1

There may be many such sets of subgroup descriptions in general, and there might
be none, say in case the dataset has a sample that is assigned two or more diﬀerent
response values; in the latter case, adding more features to the dataset might help
to resolve such conﬂicts in the data and allow for cleaner separation of positive and
negative samples using range tuples or rules.

22

Z. Khasidashvili

that the positive samples covered by already generated rules are dropped
from the dataset when generating a new rule.

2. In decision tree learning, on the other hand, the rules which can be formed
from paths leading from the root node to class labels in the leaves represent
discriminant descriptions, formed from properties that best discriminate be-
tween the classes. As rules formed from decision tree paths form discriminant
descriptions, they are inappropriate for solving subgroup discovery tasks
which aim at describing subgroups by their characteristic properties.

3. Subgroup discovery is seen as classiﬁcation rule learning by treating sub-
group cond as a rule cond −→ class, where usually class is the positive
class. However, the fact that in classiﬁcation rule learning the rules have
been generated by a covering algorithm hinders their usefulness for sub-
group discovery. Only the ﬁrst few rules induced by a covering algorithm
may be of interest as subgroup descriptions with suﬃcient coverage. Subse-
quent rules are induced from smaller and strongly biased example subsets,
excluding the positive examples covered by previously induced rules. This
bias prevents the covering algorithm from inducing descriptions uncovering
signiﬁcant subgroup properties of the entire population.

Our root-causing task is diﬀerent from both SD and RL tasks in that the
dataset that we work with can change during the root-causing procedure: we
might add and/or remove some of the features in the initial input dataset, and
might drop some of the positive samples in the initial dataset, in order to focus
the root-causing procedure to one or more relevant modules and to a subgroup
of positive samples. In addition, we want to learn subgroups of failures with the
same root-cause (which is a task relevant to SD ) as well as to learn rules that can
serve as predictors of previously learned root-causes in the future (which is a task
relevant to RL). Therefore, both SD and RL are relevant for our root-causing
procedure, and so is RA which was designed to serve both above tasks.

Work [5] uses a modiﬁed version of C4.5 decision trees to root-cause failures
in an internet service system, and demonstrates their usefulness. However, due to
the classiﬁcation mindset in searching for root-causes, which requires rules with
high quality discriminant descriptions, their tree splitting and branch selection
heuristics prefer branches that maximize positive samples at the respective leaf
nodes (see the ranking criterion in Section 3.2), and operate under assumption
that there are only a few independent sources or error (see the noise ﬁltering
criterion in Section 3.2). Therefore, unlike the SD algorithms, their search al-
gorithm does not encourage splitting subgroups of failing samples further even
if the latter would result into higher quality subgroups according to relevant
quality functions like WRAcc or Lift. As a consequence, that approach does not
fully adequately address the challenge of root-causing failures caused by a com-
bination of multiple factors, which are usually most diﬃcult to debug. Indeed,
for the root-causing example discussed in Section 6 in some detail, the ﬁnal tree
generated by the decision tree approach in [5] would not contain branches leading
to the three ﬁnal subgroups with full root-causes identiﬁed by our root-causing
procedure, since each of these three subgroups cover fewer positive samples than

Accelerating System-Level Debug Using RL and SD Techniques

23

the subgroup of the seven failures obtained by a ﬁrst split that maximizes the
number of positive samples in one of the branches.

As mentioned in the introduction, work [26] proposed to learn behavior of
the system from successful execution traces and compare failing runs against the
learned models to identify the accepted and the unaccepted event (sub)sequences.
The latter – the suspicious sequences – are presented to validator together with
a representation of the behaviors that are acceptable instead of them, according
to the learned models. While the identiﬁed illegal event sequences localize well
the bug locations within respective modules and serve as valuable input to the
validators, further root-causing might be required since the learned models are
abstract and do not capture all ﬁne-grained legal behaviors (real systems are too
complex to be fully learned as automata, not every single event from the traces
will be part of the learned model). The root-causing techniques proposed in our
work can be considered complementary to the above research goal: we already
have a well deﬁned speciﬁcation for the ﬂow of interest, thus it is straightforward
to determine where the failing trace stops to conform to it, and instead our aim
is to ﬁnd observable event combinations (not necessarily occurring in the ﬂow
speciﬁcation) that jointly explain the root cause.

In [27], the authors address the root-causing problem in a setting close to
ours. They apply clustering to features extracted from traces of message-passing
protocols at SoC level.2 The idea that they follow is as follows: ”Logical bugs
in designs can be considered as triggering corner-case design behavior; which
is infrequent and deviant from normal design behavior. In ML parlance, outlier
detection is a technique to identify infrequent and deviant data points, called
outliers whereas normal data points are called inliers”. As a consequence, their
task is to identify patterns of consecutive messages that cause the bugs, which
they call anomalous message sequences. The infrequency criterion can be cap-
tured through the entropy associated to message sequences and the deviancy is
captured through a distance measure deﬁned on the sequences. The techniques
proposed in our work have been used extensively as part of product development
in the pre-silicon stage to root-cause regression suite failures, and at the early
stages of product development often there are more failing tests than there are
passing ones. And there usually are many reasons for failures. Thus the outlier
detection approach adopted in [27] is more relevant at late stages of the product
development and post-silicon when the bugs are relatively rare.

To summarize, once logs are processed and a dataset is extracted, many ML
and statistical approaches can be applied for the purpose of root-causing failures,
and these techniques are often complementary. We believe that RL and SD based
techniques are among the most suitable techniques for root-causing regression
test failures when there might be multiple causes for failures and each cause
for failure might require analyzing complex interplay between multiple factors
to explain it. We also suggest to try out supervised techniques when pass/fail

2

Their usage of term message is similar to our usage of term event, and messages
might have additional attributes: ”In SoC designs, a message can be viewed as an
assignment of Boolean values to the interface signals of a hardware IP”.

24

Z. Khasidashvili

labels for test runs are available, at least when there are many failures to learn
from and compare with passing executions. Otherwise, and in addition, semi-
supervised or unsupervised techniques like clustering, outlier direction, anomaly
detection novelty detection, change detection and more can be used.

8 Conclusions and Future Work

In this work we have proposed a new method for applying ML techniques on
datasets engineered from a large set of test logs in order to generate powerful
root-causing hints which can be mined and reused. The methodology and tool
proposed in this work has been applied to debug P kgC8 ﬂow of Intel’s Power
Management protocol.

In this work we only considered rules that do not carry any sequential in-
formation of event occurrences. Sequential rules have been well studied in the
context of recommender systems and process monitoring systems in general,
and their application domains are growing fast. In validation domain, learning
sequential rules is somewhat similar to learning assertions from traces in the
form of event sequences or automata, or in a richer formalism that can directly
capture the concurrency information, but now the intention is to learn patterns
of incorrect scenarios rather than the correct ones (where incorrect scenarios
are not necessary rare scenarios). In our iterative root-causing and learned bug
mining approach, the rules can be mined by validation expert also as sequential
rules or in other formats (assuming the expert validator has the knowledge of
the intended behavior). Automating the mining of sequential rules that capture
bug patterns is an important direction for future work.

9 Acknowledgments

We would like to thank Yossef Lampe, Eli Singerman, Yael Abarbanel, Orly Co-
hen, Vladislav Keel and Dana Musmar for their contributions to this work: con-
tributions to discussions, implementation of respective parts of the root-causing
tool, initial experiments, and actual usage in Intel products.

References

1. Atzmueller, M.: Subgroup Discovery – Advanced Review. In: Data Mining and

Knowledge Discovery, 5(1) (2015)

2. Angluin, D.: Learning regular sets from queries and counterexamples. Inf. Comput.,

75(2), 87–106 (1987)

3. Billari, F. C., F¨urnkranz, J., Prskawetz, A.: Timing, sequencing, and quantum of
life course events: A machine learning approach. European Journal of Population,
22(1), 37– 65 (2006)

4. Chen, W., Wang, Li-C., Abadir M.: Simulation Knowledge Extraction and Reuse
in Constrained Random Processor Veriﬁcation. In: 50th ACM/EDAC/IEEE Design
Automation Conference, pp. 1-6 (2013)

Accelerating System-Level Debug Using RL and SD Techniques

25

5. Chen, M., Zheng, A., Lloyd, J. , Jordan, M., Brewer, E.: Failure diagnosis using
decision trees. In: IEEE International Conference on Autonomic Computing (2004)
6. Clark, P., Niblett, T.: The CN2 Induction Algorithm. Machine Learning, 3, 261–

283 (1989)

7. De Jay, N., Papillon-Cavanagh, S., Olsen, C., Bontempi, G., Haibe-Kains, B.:
mRMRe: An R Package for Parallelized mRMR Ensemble Feature Selection (2014)
8. Ding, C., Peng H.: Minimum Redundancy Feature Selection from Microarray Gene
Expression Data. Journal of Bioinformatics and Computational Biology, 32, Impe-
rial College Press, pp. 185–205 (2005)

9. Fraer, R., Keren, D., Khasidashvili, Z., Novakovsky, A., Puder, A., Singerman, E.,
Talmor, E., Vardi, M. Y., Yang, J.: From Visual to Logical Formalisms for SoC
Validation. In: 12th ACM-IEEE International Conference on Formal Methods and
Models for System Design, MEMOCODE’14 (2014)

10. F¨urnkranz, J., Gamberger, D., Lavra˘c, N.: Foundations of Rule Learning. Cognitive

Technologies. Springer (2012)

11. Guyon, I., Elisseeﬀ, A.: An Introduction to Variable and Feature Selection. Journal

of Machine Learning Research 3, 1157–1182 (2003)

12. Hirsch, Th., Hofer, B.: Root

cause prediction based on bug reports.

arXiv:2103.02372 (2021)

13. Hollander, Y., Morley, M., Noy, A.: The e language: a fresh separation of concerns.
In: Technology of Object-Oriented Languages and Systems, TOOLS 38. pp. 41-50
(2001)

14. Intel Corporation: 11th Generation Intel Core TM Processor Desktop, Datatsheet

Volume 1, Revision 002 (2021)

15. Katz; Y., Rimon, M., Ziv, A., Shaked, G.: Learning Micro-architectural Behaviors
To Improve Stimuli Generation Quality. 48th ACM/EDAC/IEEE Design Automa-
tion Conference, pp. 848-853 (2011)

16. Khasidashvili, Z., Norman, A.J.: Range Analysis and Applications to Root Caus-
ing. In: IEEE Intl Conf. Data Science and Advanced Analytics, pp. 298-307 (2019)
17. Khasidashvili, Z., Norman, A.J.: Feature Range Analysis. International Journal of

Data Science and Analytics (JDSA) 11, 195–219 (2021)

18. Kl¨osgen, W.: Explora: A Multipattern and Multistrategy Discovery Assistant. Ad-
vances in Knowledge Discovery and Data Mining, AAAI Press, 249–271 (1996)
19. Kotsiantis, S., Kanellopoulos D.: Discretization Techniques: A recent survey.
GESTS Intl. Transactions on Computer Science and Engineering 32(1), 47–58
(2006)

20. Lavra˘c, N., Cestnik, B., Gamberger, D., Flach P.: Decision Support Through Sub-
group Discovery: Three Case Studies and the Lessons Learned. Machine Learning
57, 115–143 (2004)

21. Lavra˘c, N., Kav˘sek, B., Flach, P., Todorovski, L.: Subgroup discovery with CN2-

SD. Machine Learning Research 5, 153–188 (2004)

22. Van Leeuwen, M., Knobbe A.: Diverse subgroup set discovery. Data Mining and

Knowledge Discovery 25(2), 208–242 (2012)

23. Manukovsky, A., Shlepnev, Y., Khasidashvili, Z.: Machine learning-based design
space exploration and applications for signal integrity analysis of 112Gb SerDes sys-
tems. In: IEEE Electronic Components and Technology Conference, ECTC (2021)
24. Nagappan, M., Vouk, M.A.: Abstracting log lines to log event types for mining soft-
ware system logs. 7th IEEE Working Conference on Mining Software Repositories,
MSR (2010)

26

Z. Khasidashvili

25. Mammo, B., Furia, M., Bertacco, V., Mahlke, S., Khudia, D.S.: BugMD: Automatic
mismatch diagnosis for bug triaging. In: IEEE/ACM International Conference on
Computer-Aided Design (ICCAD), pp. 1–7 (2016)

26. Mariani, L., Pastore, F.: Automated Identiﬁcation of Failure Causes in System
Logs. 19th International Symposium on Software Reliability Engineering, ISSRE,
pp. 117–126 (2008)

27. Pal, D., Vasudevan, S.: Feature Engineering for Scalable Application-Level Post-

Silicon Debugging. arXiv:2102.04554v1 (2021)

28. Piatetsky-Shapiro, G., Matheus, C.: The interestingness of deviation. In: AAAI

Workshop on Knowledge Discovery in Databases (1994)

29. Podgurski, A., Leon, D., Francis, P., Masri, W., Minch, M., Sun, J., Wang. B.:
Automated support for classifying software failure reports. In: ICSE’03. ACM Press,
New York pp. 465-475 (2003)

30. Poulos Z., Veneris, A.: Exemplar-based failure triage for regression design debug-

ging. J Electron Test 32, 125–136 (2016)

31. Silberschatz, A., Tuzhilin, A.: On subjective measures of interestingness in knowl-

edge discovery. In Knowledge Discovery and data mining (1995)

32. Stearley, J.: Towards informatic analysis of syslogs. In: IEEE International Con-

ference on Cluster Computing (2004)

33. Vaarandi, R.: A data clustering algorithm for mining patterns from event logs. In:

3rd IEEE Workshop on IP Operations and Management (2003)

34. Vasudevan, S., Sheridan, D., Patel, S., Tcheng, D., Tuohy, B., Johnson, D.: Gold-
mine: automatic assertion generation using data mining and static analysis. In:
Design, Automation, and Test in Europe, pp. 626–629 (2010)

35. Wrobel, S.: An algorithm for Multi-Relational Discovery of Subgroups. PKDD

(1997)

This figure "clock_off_single_range.PNG" is available in "PNG"(cid:10) format from:

http://arxiv.org/ps/2207.00622v1

This figure "signal_y_4_single_range.PNG" is available in "PNG"(cid:10) format from:

http://arxiv.org/ps/2207.00622v1

This figure "package_c8_flow.PNG" is available in "PNG"(cid:10) format from:

http://arxiv.org/ps/2207.00622v1

This figure "root_causing_tree.PNG" is available in "PNG"(cid:10) format from:

http://arxiv.org/ps/2207.00622v1

This figure "signal_y_timer_range_pair.PNG" is available in "PNG"(cid:10) format from:

http://arxiv.org/ps/2207.00622v1

This figure "timer_interval_single_range.PNG" is available in "PNG"(cid:10) format from:

http://arxiv.org/ps/2207.00622v1

