Towards Efﬁcient Architecture and Algorithms for Sensor
Fusion

2
2
0
2

p
e
S
3
1

]

C
D
.
s
c
[

1
v
2
7
2
6
0
.
9
0
2
2
:
v
i
X
r
a

Zhendong Wang
University of Texas at Dallas
Richardson, Texas, USA

Xiaoming Zeng
University of Texas at Dallas
Richardson, Texas, USA

Shuaiwen Leon Song
University of Sydney, Australia
Sydney, Australia

Yang Hu
University of Texas at Dallas
Richardson, Texas, USA

ABSTRACT
The safety of an automated vehicle hinges crucially upon the
accuracy of perception and decision-making latency. Under
these stringent requirements, future automated cars are usu-
ally equipped with multi-modal sensors such as cameras and
LiDARs. The sensor fusion is adopted to provide a conﬁdent
context of driving scenarios for better decision-making. A
promising sensor fusion technique is middle fusion that com-
bines the feature representations from intermediate layers
that belong to different sensing modalities. However, achiev-
ing both the accuracy and latency efﬁciency is challenging
for middle fusion, which is critical for driving automation
applications.

We present A3Fusion, a software-hardware system special-
ized for an adaptive, agile, and aligned fusion in driving
automation. A3Fusion achieves a high efﬁciency for the mid-
dle fusion of multiple CNN-based modalities by proposing
an adaptive multi-modal learning network architecture and a
latency-aware, agile network architecture optimization algo-
rithm that enhances semantic segmentation accuracy while
taking the inference latency as a key trade-off. In addition,
A3Fusion proposes a FPGA-based accelerator that captures
unique data ﬂow patterns of our middle fusion algorithm
while reducing the overall compute overheads. We enable
these contributions by co-designing the neural network, algo-
rithm, and the accelerator architecture.

1. MOTIVATION

Autonomous vehicles are typically equipped with multiple
different sensors, and thus, multi-sensing modalities can be
fused to exploit their complementary properties to achieve
robust and accurate scene understanding, such as semantic
segmentation [1]. Generally, fusion can be achieved at the
input level, decision level, or intermediately (i.e., middle
fusion). Recently, several methods adopting deep learning
have been proposed to fuse multi-modal sensors to implement
semantic segmentation for autonomous driving [2, 3]. Even
so, it is still an open issue how to achieve the optimal way
to effectively fuse the multiple modalities, especially via the
complicated middle fusion due to the fact that the middle
fusion is highly challenging. Therefore, our work focuses on

RGB-Depth information fusion for semantic segmentation
in autonomous driving and targets at an efﬁcient architecture
and algorithms to achieve middle fusion to enhance scene
perception.

Regarding the middle fusion, the intermediate level fea-
tures from different modalities typically possess misaligned
spatial dimensions (e.g., different channels), making it infea-
sible to simply concatenate or element-wise add the features.
Therefore, effective solutions are required to address the
spatial-feature-misalignment issue.

Secondly, middle fusion provides rich intermediate fea-
tures and offers a large range of choices to combine sensing
modalities, and we cannot easily conclude one fusion im-
plementation is better than the others. Therefore, speciﬁc
strategies are needed to implement middle fusion to ef-
fectively enhance segmentation performance.

Thirdly, most works conducting fusions only consider seg-
mentation accuracy and robustness. However, autonomous
driving poses importance on latency [4, 5]. Therefore, it is
necessary to take latency performance into consideration
in middle fusion besides the accuracy and robustness. Be-
sides, to implement the fusion, the multiple modalities (e.g.,
two DNNs) have to co-execute simultaneously [6], and the
introduced fusion unavoidably incurs extra computation cost
and data dependency between the two modalities, which leads
to signiﬁcant inefﬁciency on existing GPU platforms or ac-
celerators and calls for a more customized hardware design
to amortize computation efﬁciency and data synchronization
issues [7, 8].

2. MAIN ARTIFACTS

We present A3Fusion, a software-hardware system special-
ized for an adaptive, agile, and aligned fusion in driving
automation. A3Fusion achieves a high efﬁciency for the mid-
dle fusion of multiple CNN-based modalities by proposing
an adaptive multi-modal learning network architecture and a
latency-aware, agile network architecture optimization algo-
rithm that enhances semantic segmentation accuracy while
taking the inference latency as a key trade-off. In addition,
A3Fusion proposes a FPGA-based accelerator that captures
unique data ﬂow patterns of our middle fusion algorithm

1

 
 
 
 
 
 
Figure 1: Latency-aware middle fusion.

while reducing the overall compute overheads. We enable
these contributions by co-designing the neural network, algo-
rithm, and the accelerator architecture.

Figure 2: Speciﬁc architecture of Fuse-Multitasking.

2.1

fuseLinks for Accuracy in Middle Fusion
To address the spatial-feature-misalignment issue in intermediate-

level features as well as promote the information integration
across different channels of the fused features, we propose a
fuseLink to implement middle fusion, which includes a con-
volution ﬁlter (i.e., fuseFilter) in each connection. For two
layers with misaligned spatial features, the fuesLink adopts
the dimension of which is cinput * cout put * w * h, to adapt the
channel number of outputted feature of active fusion layer
to exactly match the channel number of inputted feature of
passive fusion layer. Also, as the network goes deeper, the
fuseLink provides an opportunity to fuse the low-level infor-
mation to the high-level features instead of merely summariz-
ing the same level information, beneﬁting the context-critical
semantic segmentation. Besides for two layers with matched
spatial features, the fuseFilter can promote the information
integration across different channels of the spatial features to
improve the segmentation performance.

2.2 How To Effectively Deploy fuseLinks

Even though fuseLinks can be applied to implement the
middle fusion smoothly, it is non-trivial to decide the "opti-
mal" way to deploy the fuseLinks. By comprehensive charac-
terizations, we explore the deployment principles from: the
fusion direction, the fusion distance and the fusion number as
well as fusion location. Speciﬁcally, bidirectional fuseLinks
outperforms unidirectional one and the more fuseLinks are
applied, the more accuracy beneﬁt are obtained. Meanwhile,
the fuseLink with distance = 1 can bring the largest beneﬁt
compared to the fuseLinks with other distance. Besides, the
fuseLinks connecting the low-level blocks typically outper-
forms the fuseLinks on the high-level blocks. By following
the principles, fuseLinks deployment can be adapted to dif-
ferent scenarios and be wisely adjusted optimally to improve
the segmentation accuracy.

2.3 Latency-aware fuseLinks Deployment

As fuseLinks unavoidably incur extra computation cost
and data dependency between RGB and depth, which may
cause extra latency compared to the individual RGB or depth
network, this is unacceptable to autonomous driving safety.
Therefore, we propose a latency-aware fuseLink pruning ap-
proach to prune the fuseLinks that contributes little to the

ultimate accuracy to reduce the computation cost and en-
hance the latency performance of the segmentation. As Fig. 1
shows, we construct a hypernetwork (HN) that can decide the
importance of each link to the ultimate accuracy depending
on the input data. HN is composed of one convolution layer,
average Pooling layer and ﬂattening layer as well as one FC
layer and wl can change dynamically depending on the spe-
ciﬁc input in inference, and we set a threshold, th, which can
be adjusted to wisely decide which fuseLinks can be pruned
for a given input to trade-off the accuracy with latency of the
segmentation. In practice, autonomous driving may drive in
different scenarios, such as the rural road, urban intersection,
and each scenario imposes different requirements on the accu-
racy and latency of vehicle’s perception function and reaction.
By adapting threshold, we can trade off the latency and accu-
racy such that the vehicle can drive safely and efﬁciently in
different scenarios.

3. FUSE-MULTITASKING ACCELERATOR

DESIGN

As the fuseLinks can be adapted to speciﬁc fusion scenar-
ios and support dynamic pruning, leading to the hardware
underutilization. Meanwhile, the remained fuseLinks im-
poses data dependencies between the two fused networks,
causing computation inefﬁciency when running in traditional
DNNs accelerators. Therefore, we develop an aligned ac-
celerator architecture on FPGA, to thoroughly explore the
fuseLinks’ contribution to the segmentation accuracy while
ease the extra computation and latency cost.

Fig. 2 shows our customized aligned accelerator archi-
tecture. Basically, we split a large PE array into half and
each half forms an individual processing system which is
conﬁgured with its own buffers and controlling logic. Even
though the two "independent" parts can execute in parallel to
implement RGB and depth network on their own hardware
system, executing the RGB and depth networks on their own.
Meanwhile, the specialized fuseLink buffer effectively con-
nect the two PE part and shares the partial sum results during
the fusion such that the RGB and depth network can almost
execute in an aligned mode.

4. KEY RESULTS AND CONTRIBUTIONS

2

w1w2w3w4w5w8w7w6RGBHypernetwork (HN)[w1, w2, …, w8]Weight Vector for each fuesLinkDepthConcatenateInput Dynamic Pruningth=0.2th=0.4…⊕DRAMInput bufferWeight bufferPEPEPEPEPEPEPEPEInput bufferWeight bufferPEPEPEPEPEPEPEPEIFmapWeightIFmapWeightSystem ContrllerPsumPsumReLuPoolReLuPoolFuselink bufferFuselink bufferThreshold >=Accelerator setup All hardware modules of A3 Fusion are
implemented in Verilog and synthesized on a XC7VX690T-
2FFG1761C Xilinx FPGA. The frequency is 80MHz. Each
PE performs one MAC operation with total 5 hard macro on-
chip DSP IPs. In addition, we adopt 32-bit ﬂoating-point DSP
operation in order to maintain the calculation accuracy. The
performance results including inference latency, resources
utilization and power consumption are evaluated via Vivado
2018.3 version.
Segmentation accuracy analysis and its impact on vehicle
safety Firstly, by extensively evaluate how our fuseLinks
improve the segmentation accuracy and how the different
deployment principles impact the accuracy speciﬁcally, we
observe that the bidirectional fuseLink with distance = 2
outperforms all the other fuseLink deployment, and reaches
37.66%. Compared to the baseline FuseNet [9], it improves
2.19%, indicating signiﬁcant risk reduction in vehicle driving.
More details can be found in full paper.
A3Fusion Accelerator in terms of latency, resource uti-
lization and power A3 Fusion hardware architecture can
reduce the segmentation latency up to 17.97% compared to
the baseline. If the dynamic pruning techniques applied, the
more fuseLink are deployed, the more beneﬁt our A3 Fusion
architecture can bring. Concerning the resources utilization,
20.84% LUT and 27.43% FF overhead are incurred due to
that one large PE arracy (i.e., baselien) is split into half and
extra controlling logic is introduced for the fuseLink. Also,
as each half PE array is conﬁgured with sperate fuseLink
buffer for effective communication, 73.78% storage overhead
is incurred. More details can be found from full paper.

5. CONCLUSION

The sensor fusion is playing a vital role in future driving
automation system as it guarantees a more accurate object de-
tection and semantic segmentation. However, trading-off the
accuracy and latency of multi-modal fusion is still unexplored.
We propose an optimized multi-model fusion network in this
work and comprehensively characterize the design space. The
key to our approach is to co-design the neural network, al-

gorithm, and hardware accelerator to achieve the optimized
trade-off between accuracy and latency in sensor fusion. Our
work also promises to extend to a more generalized multi-
modal fusion scenario.

REFERENCES
[1] Feng, D., Haase-Schütz, C., Rosenbaum, L., Hertlein, H., Glaeser, C.,
Timm, F., ... & Dietmayer, K. (2020). Deep multi-modal object
detection and semantic segmentation for autonomous driving:
Datasets, methods, and challenges. IEEE Transactions on Intelligent
Transportation Systems, 22(3), 1341-1360.

[2] Sun, L., Yang, K., Hu, X., Hu, W., & Wang, K. (2020). Real-time
fusion network for RGB-D semantic segmentation incorporating
unexpected obstacle detection for road-driving images. IEEE Robotics
and Automation Letters, 5(4), 5558-5565.

[3] Zeng, X., Wang, Z., & Hu, Y. (2022). Enabling Efﬁcient Deep

Convolutional Neural Network-based Sensor Fusion for Autonomous
Driving. arXiv preprint arXiv:2202.11231.

[4] Wang, Z., Wang, Z., Liu, C., & Hu, Y. (2020). Understanding and
tackling the hidden memory latency for edge-based heterogeneous
platform. In 3rd USENIX Workshop on Hot Topics in Edge
Computing (HotEdge 20).

[5] Wang, Z., Jiang, Z., Wang, Z., Tang, X., Liu, C., Yin, S., & Hu, Y.
(2020). Enabling Latency-Aware Data Initialization for Integrated
CPU/GPU Heterogeneous Platform. IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, 39(11),
3433-3444.

[6] Bateni, S., Wang, Z., Zhu, Y., Hu, Y., & Liu, C. (2020, April).

Co-optimizing performance and memory footprint via integrated
cpu/gpu memory management, an implementation on autonomous
driving platform. In 2020 IEEE Real-Time and Embedded Technology
and Applications Symposium (RTAS) (pp. 310-323). IEEE.

[7] Baek, E., Kwon, D., & Kim, J. (2020, May). A multi-neural network

acceleration architecture. In 2020 ACM/IEEE 47th Annual
International Symposium on Computer Architecture (ISCA) (pp.
940-953). IEEE.

[8] Maor, G., Zeng, X., Wang, Z., & Hu, Y. (2019, November). An FPGA
implementation of stochastic computing-based LSTM. In 2019 IEEE
37th International Conference on Computer Design (ICCD) (pp.
38-46). IEEE.

[9] Hazirbas, C., Ma, L., Domokos, C., & Cremers, D. (2016, November).

Fusenet: Incorporating depth into semantic segmentation via
fusion-based cnn architecture. In Asian conference on computer vision
(pp. 213-228). Springer, Cham.

3

