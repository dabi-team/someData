2
2
0
2

g
u
A
8

]
E
S
.
s
c
[

1
v
3
0
0
4
0
.
8
0
2
2
:
v
i
X
r
a

Preprint of the paper accepted at Journal of Systems and Software.
The ﬁanl version of the paper can be accessed here:
− https://doi.org/10.1016/j.jss.2022.111479 OR

− https://www.sciencedirect.com/science/article/abs/pii/S0164121222001637
—————————————————————————————-
—————————————————————————————–
On the Beneﬁts And Problems Related to Using
Deﬁnition of Done — A Survey Study

Sylwia Kopczy´nskaa,∗, Miros(cid:32)law Ochodeka, Jakub Piechowiaka, Jerzy Nawrockia

aPoznan University of Technology
Faculty of Computing, Institute of Computing Science
ul. Piotrowo 2, 60-965 Pozna´n, Poland

Abstract

Context: Deﬁnition of Done (DoD) is one of the fundamental concepts of Scrum. It expresses a shared
view of a Scrum Team on what makes an increment of their product complete. DoDs are often deﬁned as
checklists with items being requirements towards software (e.g., quality requirements) or towards activities
performed to make the increment shippable (e.g., code reviews, testing). Unfortunately, the knowledge
about the usefulness of DoD is still very limited.
Objective: The goal is to study what beneﬁts using the DoD practice can bring to an agile project, what
problems it may trigger, and how it is created and maintained.
Method: In the survey among members of agile software development projects, 137 practitioners from all
over the globe shared their experience with us.
Results: 93% of the respondents perceive DoD as at least valuable for their ventures. It helps them to
make work items complete, assure product quality, and ensure the needed activities are executed. However,
they indicated that every second project struggles with infeasible, incorrect, unavailable, or creeping DoD.
Conclusions: It follows from the study that DoD is important but not easy to use and more empirical
studies are needed to identify best practices in this area.

Keywords: Deﬁnition of Done; DoD; Agile; Scrum; Survey

1. Introduction

Organizations are increasingly adopting agile
methods. From the results of the PMI survey, it
follows that over 70% of organizations report using
agile methods [1], and Scrum is one of the most
popular among them.

∗Corresponding author
Email address: skopczynska@cs.put.poznan.pl

(Sylwia Kopczy´nska)

In Scrum, Developers are to deliver increment(s)
of “done” product in each Sprint.
It means that
an increment must be complete and in usable con-
dition (often referred to as working software) and
meet the agreed Deﬁnition of Done (DoD). Accord-
ing to the Scrum Guide [2] “the Deﬁnition of Done
is a formal description of the state of the Incre-
ment when it meets the quality measures required
for the product.” DoD might be standardized at
the level of organization, or could be created sepa-
rately for each product. Also, it usually evolves in

Preprint submitted to Journal of Systems and Software

August 9, 2022

 
 
 
 
 
 
time to include more stringent criteria as the team
matures. Although the use of DoDs is characteris-
tic to the Scrum framework, it is also recommended
by other agile/lean frameworks like The Kanban
Method [3], and appears in several Agile Maturity
Models (AMMs) [4].

The most recent version of Scrum Guide [2] states
that a DoD “creates transparency by providing ev-
eryone a shared understanding of what work was
completed as part of the Increment.” Therefore, it
is essential that a DoD is formulated in such a way
that all team members understand it in the same
way. Although there are no oﬃcial guidelines on
how to achieve this, most of the teams end up deﬁn-
ing their DoDs as lists of “items”, which need to
be satisﬁed to claim that a given Sprint is “done”.
DoD items concern, e.g., source code quality, per-
forming code review, testing activities, completing
work items, or non-functional requirements regard-
ing cross-cutting concerns, like security or perfor-
mance. [5, 6]

The existing body of knowledge regarding the
practical usage of DoD is very limited [5]. Taking
into account little awareness of the importance of
working towards the shared understanding of what
“done” means in some teams [7, 8], it would be es-
pecially valuable to study the potential beneﬁts and
problems related to using DoD to bring new argu-
ments to the discussion about how much we should
invest in the preparation of DoDs.

The goal of this paper is to investigate how prac-
titioners use Deﬁnition of Done (DoD) in software
projects and product teams. In the further para-
graphs, for simplicity, we will refer to both software
projects and product teams as projects 1. The spe-
cial focus of the paper is on the usefulness and the
problems DoD might trigger. To achieve the goal,
we conducted a survey among practitioners from
all over the world. The main contributions of our
study are as follows:

• we investigated to what extent the practition-
ers perceive the DoD practice as valuable,

• we studied 19 (and identiﬁed two more) ben-
eﬁts of using DoDs by investigating their fre-
quency of appearance in agile projects,

1We investigate both software development initiatives
planned for a certain period of time (projects) as well
as those continuously developing some products (product
teams).

2

• we identiﬁed 19 problems that are triggered by

lack of using DoD in a project,

• we studied the frequency and signiﬁcance of
19 problems that agile teams might encounter
while using DoDs,

• ﬁnally, we investigated what DoD items are,
and the process of creating and maintaining
DoDs, focusing on the steps of that process,
the roles that are involved, and the tools that
are used.

This paper is organized as follows. First, we ex-
plain what DoD is in Section 2. Then, in Section
3, we discuss the related studies. Next, in Section
4, we describe the design of the survey and discuss
the validity threats. The demographic information
about our respondents is presented in Section 5,
while the results of the survey are presented in Sec-
tion 6. The implications of our study are discussed
in Section 7. Section 8 concludes our ﬁndings.

2. Background

According to Scrum Guide [2], each Sprint is
to deliver an Increment of a potentially releasable
product which is usable and adheres to the Scrum
Team’s “Deﬁnition of Done”. However, the guide
does not provide speciﬁc guidelines on how DoD
should look like. However, it draws attention to its
high value, i.e., complying with DoD is required at
the end of each Sprint to ensure transparency and
assure product quality. Practitioners describe DoD
as a “social contract in agile teams” that “acts as
a check before work is allowed to leave the Sprint”
[9].

However, the concept of DoD is not only used
in Scrum but also in other agile approaches. For
example, The Kanban Method recommends to de-
ﬁne the workﬂow, that is the speciﬁc process each
item-to-do needs to undergo with the deﬁned steps
(stages) and the policies specifying the require-
ments that need to be satisﬁed to let the item ﬂow
from one step to another. One type of such policies
is DoD [10].

In practice, DoD frequently has a form of a check-
list with requirements that need to be assessed
to determine whether the work is “done”, see ex-
amples in Figure 1 (the examples (cid:192)-(cid:201) are taken
from the DoD items the respondents shared with
us in the survey). DoD items mainly focus on

[5, 11, 12, 13, 6]:
(1) business or functional re-
quirements, i.e., require certain requirement(s) to
be implemented or tested or veriﬁed, e.g., (cid:193) ; (2)
quality aspects, e.g., concern the status of testing,
the level of technical debt, the results of code re-
views such as (cid:194), (cid:195), (cid:196), (cid:197), and (3) non-functional
requirements, i.e., they state if the desired charac-
teristics of a system are met, e.g., (cid:200). They might
concern diﬀerent types of entities that appear in
software development (e.g., test ((cid:194)), code ((cid:196)), de-
fects ((cid:195)), documentation ((cid:200))) and be deﬁned at
multiple “levels”, e.g., for a single task, for a user
story/feature ((cid:192)), for all user stories/features ((cid:193)),
for a whole increment ((cid:198)), for a release ((cid:199)).

Figure 1: Excerpts from DoDs presenting diﬀerent types of
DoD items.

DoD items are sometimes confused with Ac-
ceptance Criteria (AC). However, those two have
slightly diﬀerent goals. AC is a set of requirements
that must be fulﬁlled for a given Product Back-
log Item, so it can be accepted by Product Owner,
customer, user, or other stakeholder [14]. Testing
against AC produces a clear pass/fail result if the
product can be accepted or not [15]. Thus, ACs
are item-speciﬁc and orthogonal to DoD. Viscardi
[16] states that DoD is a quality goal, while AC
are functional or behavioral expectations. AC are
not directly mentioned in the Scrum Guide, how-
ever they are an important and useful tool in ag-
ile projects. Example ACs for an e-commerce site,
which follow from the experience of the authors,
could be AC1:“Lists 1.000 products on a one page”
or AC2: “Sorts the listed products with respect to

3

price and name in both ascending and descending
order ” while DoD items could be DoDI1: “Deployed
to the production environment”, DoDI2: “Each fea-
ture was code reviewed by at least one dev who is not
the author and the improvements are introduced ”.
While the notations of user stories or use cases
are frequently used to document software require-
ments [17], there is no common format in which
DoD items are speciﬁed. Frequently, DoD items are
short statements or short sentences such as those
presented in Figure 1 or mentioned in the work of
Silva et al. [18].

3. Related work

The usage of DoD and its value were discussed
in several papers describing lessons learned from
industry projects.
For example, according to
Davis [19], using DoD has a positive impact on re-
ducing the number of defects and limiting technical
debt. Power [20] claims that DoDs help with as-
sessing the completion of work. Taipale [21] found
that their software development with DoD results
in the product of high quality and is a well-managed
development process. Masood et al.
[22] observed
that a well-deﬁned DoD supports self-assignment of
[23] proposed
tasks in agile teams. Kasauli et al.
using DoD items at the level of user stories to fos-
ter requirements traceability in a project. Several
other agile practitioners claim that DoD is an essen-
tial practice and recommend using it, e.g.,[13], [24],
[17]. Although these papers mention some beneﬁts
of using DoD, they base their observations either
on the authors’ experience or on analyses of indi-
vidual cases. Our study complements the previous
ﬁndings by identifying and helping to understand
the beneﬁts of using DoDs in agile projects. We
also scale up the existing research by surveying a
large sample of projects.

While the majority of authors discuss the advan-
tages of creating and using DoD, only a few of them
mention the problems that accompany that process.
For instance, O’Connor [7] states that there might
be some conﬂicts between business and developers
on what “done” means, while Igaki et al. [25] em-
phasize the need of making sure that the DoD items
are followed. Igaki et al.
[25] and Alsaquaf et al.
[6] found that DoD might become too extensive and
as a result negatively impact team velocity or make
validation more complex.

Although these papers present valuable lessons
learned from implementing and teaching agile

① Feature merged to develop branch ② All features/tasks completed ③ All tests pass ④ No open defects ⑤ No TODOs in code ⑥ For coding/development tickets you must         have satisfied Code Coverage ⑦ Build passes ⑧ Deployed to Production ⑨ Functional and Technical Specifications         completed ⑩ All documentation updated methods, neither of them provides a comprehen-
sive and well-evaluated set of problems related to
using DoDs. With our study, we ﬁll this knowledge
gap. We use the identiﬁed problems and beneﬁts as
the basis, extend them with those from our knowl-
edge and experience, and evaluate their importance
among practitioners all over the world. We com-
pared the related works mentioned in two previous
paragraphs with each other and with our work in
Table 1.

There are two works in which the focus was given
to DoD. First, Silva et al. [5] conducted a Sys-
tematic Literature Review on the papers published
prior to 2017. The review aimed at investigating
what the done criteria are, what is the context of
the teams using DoD, and what the characteristics
of the studies investigating the topic of DoD are.
They found 8 relevant studies and analyzed them.
As a result, they characterized the items that are
commonly present in DoDs. For instance, they ob-
served that DoD items are often deﬁned at four
levels: story, Sprint, release, and project. They
found that four primary studies mention the ben-
eﬁts of using DoD. Finally, based on the analysis
of gaps in the literature regarding DoD, they con-
cluded that “there is a need for more and better
empirical studies documenting and evaluating the
use of the DoD in agile software development.” In
their follow-up study, Silva et al. [18] conducted a
survey on a sample of 20 respondents. The study
provided some insights on how DoD is created and
maintained, e.g., they learned that 80% of respon-
dents had their DoDs explicitly documented, the
DoDs were mostly created by the team and evolved
throughout the project. Our study scales-up and
complements both of these studies, providing some
new insights on DoD items, on how DoDs are cre-
ated and maintained, and investigates the impor-
tance of the identiﬁed beneﬁts and problems. The
comparison of the studies with each other and with
our study is presented in Table 2.

4. Research methodology

4.1. Research aims and questions

product team.2 Secondly, we want to identify and
evaluate the problems associated with implement-
ing the DoD practice. Finally, we would like to
investigate whether there are any relationships be-
tween the problems encountered while using DoD
and the beneﬁts of using it. The existence of such
relationships would mean that when certain prob-
lems with DoD exist, some beneﬁts are less/more
likely to be observed. Thus, it might help to make
decisions about problem-mitigation strategies.

Based on these three goals, we formulate the fol-

lowing research questions:

• RQ1. What are the beneﬁts of using DoD?

• RQ2. a) What are the problems encountered
while using DoD and b) how are they miti-
gated?

• RQ3. Are there any relationships between the

problems and beneﬁts while using DoD?

As follows from the literature review presented
in Section 3, the body of knowledge regarding the
process of creating and maintaining DoDs is very
limited. Therefore, we formulate one more research
question to broaden our understanding of that pro-
cess:

• RQ4. How the DoD practice is implemented?

– a) How DoD is created and maintained?

– b) What are the DoD items?

Additionally, answering this question would allow
us to characterize the context in which the DoDs
were created and used. Consequently, it might help
us understand the origins of the beneﬁts and prob-
lems.

4.2. Research method

We chose questionnaire-based Survey Research as
our research method of choice. Since this method
allows for collecting and analyzing large samples of
projects in a cost-eﬀective way, it gives us the pos-
sibility to draw an overall picture of what beneﬁts
and problems one might expect to encounter when
adopting the DoD practice in a project. While de-
signing our study, we followed the guidelines pro-
[26] and by Moll´eri et al.
vided by Wohlin et al.

Our goal is to study the usefulness of DoDs from
two perspectives. Firstly, we are interested in learn-
ing what beneﬁts a DoD might bring to a project or

2Scrum and other agile approaches can be used both in
software projects and in product teams. For simplicity, we
will refer to both as projects in this paper.

4

Table 1: Comparison of related work concerning experience in agile software development that revealed beneﬁts or problems
concerning the usage of DoD

Object of study

Type
study

of

Participants

Domain

Problems or Ben-
eﬁts

Power et al. [9]

using Deﬁnition of
Ready

case study

one company, overall
opinion

networking

identiﬁed a beneﬁt

Taipale et al. [21] process used in Lean

case study

startup

one company, overall
opinion

software devel-
opment, retail

identiﬁed a beneﬁt

Massod et al. [22] self-assignment

to

work

grounded the-
ory

companies,

23
practitioners

42

software devel-
opment

identiﬁed a beneﬁt

Kasauli et al. [23] challenges

and
strategies using agile
approaches

case study

one company, two de-
partments

automotive

identiﬁed a beneﬁt

Saddington et al.
[13]

scaling product own-
ership

case study

military

identiﬁed a beneﬁt

a program by the De-
partment of Defense
and US Air Force, 4
teams

Jakobsen
Johnson [24]

and

combining
with Scrum

CMMI

case study

one company

software
systems
pany

and
com-

identiﬁed a beneﬁt

Cohn [17]

theory and experi-
ence about agile

O’Connor [7]

agile transformation

Igaki [25]

ticket driven devel-
opment for teaching
Scrum

no study, book own experience

–

share possible bene-
ﬁts

experience de-
scription

company,

one
project

experiment
with students

company,

one
project

one

education

identiﬁed a problem

one

education

identiﬁed a problem

Alsaqaf et al. [6] way developers treat
quality requirements
in agile

multi-case
study,
views

inter-

17 agile practitioners
from 6 organizations

sector,

identiﬁed a problem

public
government,
commercial,
banking,
commercial
navigation,
health
insurance,
telecom

care,

Our Study

Practice of DoD

survey

137 agile practition-
ers

over 20 diﬀer-
ent domains

problems
analyzed
and beneﬁts and the
process of using DoD

[27]. Our survey can be classiﬁed as a descriptive
study, however, since the topic we investigated has
not been deeply studied, it could also, to some de-
gree, be treated as an exploratory study.

The questionnaire and the collected data are

available as supplemental material for this paper.

4.3. Population and sample representatives

Our target population constitutes participants of
agile software development projects and product
teams that use DoD practice. We assume that a

representative sample of the target population shall
have the following characteristics:

1. Using the DoD practice — the sample includes
responses from the participants that used the
DoD practice in the projects they refer to.
2. Year — projects being referred to are contem-
porary agile projects. Referring to old projects
would constitute a serious threat to internal
and external validity since the usage of agile
methods and their popularity has changed vis-
ibly over the last years [28].

3. Context (country, domain, etc.) — the distri-

5

Table 2: Comparison of related work concerning the practice of using DoD with each other and with our work.

Silva et al. [5]

Silva et al. [18]

Type of study

SLR

survey

Research
Questions

1. What are done criteria?
2. In which environment DoD
was used?
3. What types of study tackle
DoD?

1. What done criteria are?
2. What process is used to
deﬁne DoD?
3. What levels of DoD are used?
4. Is DoD emergent during
a project?
5. Are DoDs explicitly
documented?
6. How are DoD criteria
assessed?
7. Eﬀectiveness of DoD
8. Challenges in using DoD

Our work

survey

1. What are the beneﬁts
of using DoD?
2. What are the problems
encountered while using DoD
and how they are mitigated?
3. Problems–Beneﬁts
relationships
4. What DoD items are?
5. How is DoD established
and maintained?

Subjects/
Participants

8 primary studies

20 agile practitioners from 16
countries

137 agile practitioners from 45
countries

#DoD items
analyzed

62

22

143

Problems
or Beneﬁts

Found the beneﬁts mentioned in
four primary studies

Asked respondents about beneﬁts
and challenges

Used the identiﬁed beneﬁts and
problems to survey practitioners
from all over the world about per-
ceived importance and identify
any new items.

Practice of using
DoD

–

DoD item analysis Categorized DoD items and in-

vestigated the frequency

Asked with open questions re-
spondents about the deﬁnition,
evolution,
and if
DoD is documented

assessment,

Asked with open questions about
the whole process of using and
maintaining DoD, person respon-
sible, and format.

Listed the DoD items using the
categorization previously pro-
posed

Analyzed DoDs using the previ-
ously proposed categorization

butions of diﬀerent project context factors in
the sample of projects should be similar to the
corresponding distributions in the IT industry.
4. Agile method — we assume that the DoD prac-
tice is mainly used by the teams working ac-
cording to the Scrum guidelines (since the DoD
practice originates from Scrum). Therefore,
we assume that our target population is dom-
inated by Scrum teams.

We are going to evaluate the representatives of
the sample based on the above given characteristics.
The ﬁrst two characteristics (1 and 2) are ﬁltering
criteria that should be included in the survey instru-
ment. The two remaining characteristics (3 and 4)
are more diﬃcult to evaluate since we do not have
accurate and precise information on how often given
context factors appear in IT projects. Therefore,
we are going to base our evaluation on the avail-
able data from surveys concerning agile methods

[29, 30, 31] and the characteristics of the large sam-
ple of IT projects collected in the ISBSG database
and reported by Hill [32]. In particular, we are go-
ing to take into account the geographic dispersion
of the respondents/projects and the agile methods
that are reported in the surveys, while the ISBSG
database allow us to assess the representatives of
diﬀerent domains and types of applications in the
sample.

4.4. Survey instrument

We designed an online questionnaire divided into
four parts and implemented it using the Survey
Monkey platform.

The ﬁrst part consisted of three pages: a welcome
page (presenting the goal of the study and provid-
ing contact information to the researchers conduct-
ing the study), a page providing some basic infor-
mation about the concept of DoD (as one of the

6

means to ensure construct validity), and a page ask-
ing the respondents to focus on one of the projects
or product teams they participated in which a DoD
was used. We based the description of DoD on the
Scrum Guide [2]. Also, we asked about the date of
the last participation of the respondent in a project
in which a DoD was used. It had two purposes: (1)
we wanted each respondent to focus on an individ-
ual project, and (2) we wanted to verify whether
the responses regard new developments.

The second part of the questionnaire regarded the
value of using a DoD in a project. First, we asked
the respondents to evaluate how valuable was us-
ing a DoD in their project (in general) by using
a ﬁve-point Likert scale. Second, there was a list
of 19 potential beneﬁts to be assessed individually.
The list of beneﬁts resulted from our literature re-
view and was extended by a few potential beneﬁts
that we brainstormed. Also, the respondents could
select the option “other” to provide their own ex-
amples of beneﬁts. For each beneﬁt, the respondent
was asked if the beneﬁt to their project is the result
of using a DoD. They responded using a ﬁve-point
ordinal scale (“Deﬁnitely YES”, “YES”, “Neither
YES nor NO”, “NO”, “Deﬁnitely NO”). We also
allowed for the answer “I don’t know” to avoid in-
troducing bias by forcing respondents to provide
answers (e.g., if the respondents lacked the knowl-
edge to answer the question reliably or did not want
to express their opinion).

The third part of the questionnaire contained a
series of questions concerning problems with using
DoDs. The ﬁrst two questions asked the respon-
dent to assess the negative impact of the possible
problems in their projects concerning management
and DoD items, respectively. The list of problems
had two sources. We had extracted the problems
mentioned in the literature. Next, since DoD items
are requirements, we extended the set with the
common problems by specifying requirements from
Wiegers [33]. For each problem, the respondents
could select one of the following answers: “Prob-
lem did not appear in the project”, “Problem had
a positive impact” (e.g., there was a problem with
using DoD but its presence triggered some correc-
tive actions that were beneﬁcial for the project) or
assess the signiﬁcance of the negative impact of the
problem if it had appeared using a ﬁve-point Lik-
ert scale (“Deﬁnitely signiﬁcant”, “Rather signif-
icant”, “Neither signiﬁcant nor NOT signiﬁcant”,
“Rather NOT signiﬁcant”, “Deﬁnitely NOT signif-
icant”). Again, we also allowed for the answers “I

7

don’t know” and “Other”. Finally, we asked the
respondents how the problems with using a DoD
were solved (an open question).

In the fourth part, we placed open questions con-
cerning the process of creating and maintaining
DoDs (the process, the roles involved, the frequency
of updating the DoD, and the ways of publishing it).
The ﬁfth part contained 11 questions asking
about demographic information. We used them
to characterize the sample of respondents. In par-
ticular, we asked about the experience, domains,
type of applications, methods, size of the project,
and size of the organization. The answers to the
questions could be provided using ordinal scales
or as multiple-response true/false questions (more
than one choice was allowed). For the questions
concerning the domains and application types, we
had adopted the classiﬁcation scheme used by IS-
BSG [34], while for the size of the organization,
we used the classiﬁcation deﬁned by the European
Commission [35].

The last part of the questionnaire asked the re-
spondents to leave any comments, questions, or re-
marks. We also asked them to voluntarily share
their DoD and the problems they met in some other
project caused by the lack of DoD. Optionally, ev-
ery respondent might have provided their email ad-
dress to get a summary of the results after the sur-
vey is completed.

4.5. Survey instrument validation and evolution

The prepared questionnaire underwent multiple
internal and external reviews. First,
it was ex-
amined by the authors of this paper and the ini-
tial version was subjected to a pilot study with 7
participants (four members of our research group
and three external agile software development pro-
fessionals). Their feedback allowed us to improve
the wording of some questions, and a few spelling
mistakes were found. One wording issue concerned
that we asked about “software projects” and two
practitioners had doubts if they are eligible to par-
ticipate in the survey. They pointed out that ag-
ile approaches are also used in the context where
there is no deﬁned period of time to ﬁnish the
work (projects) and they are usually called “prod-
uct teams”. Next, we found out that many soft-
ware engineers have the same opinion, e.g., Sriram
Narayan or Martin Fowler [36]. Thus, we added
the information to the questionnaire. However, no
major problems were identiﬁed. The questionnaire
was not further modiﬁed during the study.

4.6. Ethical considerations

While designing our study, we have considered
a series of ethical considerations, especially those
discussed by Vinson and Singer [37].

Informed consent. Participation in the study was
voluntary. The invitation letter and the introduc-
tion page of the questionnaire form informed po-
tential participants about the goal of the study, the
research group conducting the study, the research
procedure (including information on how the re-
sponses will be processed, and the results communi-
cated), the beneﬁts of participating, the estimated
time to complete the survey, and the contact e-mail
addresses of the members of the research team.

Anonymity.

Participation in the study was
anonymous. We did not ask about any personal
information or the names of the companies. How-
ever, the participants could provide us with their
e-mail addresses that might contain their names or
surnames. Therefore, we excluded these data from
further analyses.

Beneﬁcence. A direct beneﬁt of participating in
our study was the early access to the survey results
before they are oﬃcially published.

Conﬁdentiality. The online survey was conducted
using the Survey Monkey platform, which we con-
sider to be a secure service. From our analysis of the
security features, it follows that they are ISO 27001
certiﬁed, EU and US Privacy Shield Certiﬁed. They
use AES 256 based encryption and declare to im-
plement appropriate technical, organizational, and
administrative systems, policies, and procedures to
ensure the security, integrity, and conﬁdentiality of
the questionnaire and collected data to mitigate the
risk of unauthorized access to or usage. The only
sensitive data concerned e-mail addressees of the
respondents. They were processed separately from
the remaining data and used only to send the re-
sults back to the respondents.

4.7. Data collection

Since there is no one ‘place’ that provides access
to the representatives of the population, we decided
to target the respondents using Internet-based
channels. We decided to conduct an invitation-
based online survey, and, thus, we sent messages
to people we knew to have experience in Agile and
posted a request to participate in the survey in so-
cial network groups related to Agile on LinkedIn,
Facebook, and MeetUp. After three to four weeks,
we sent kind reminders to the groups. The data col-
lection took place between February 21, 2020 and

8

September 21, 2020. The exact response rate can-
not be calculated due to the usage of public invita-
tions, but given that we collected 276 responses, the
response rate can be interpreted to be low, which
is characteristic to online surveys.

There were 137 complete and 139 incomplete re-
sponses. The respondents spent on average ca.
17min. (median) to complete the survey (the max-
imum duration was 72h 31min. while the minimum
duration was 4min.). There were three subgroups
among those who have not completed the survey
(we will refer to them as incomplete respondents).
The majority of incomplete respondents 69% (96)
answered just the ﬁrst question, that is about the
year of their project and devoted to the survey ca.
1min. (median). Another 26% (36) of incomplete
respondents spent ca. 4min.
(median) and an-
swered the next question—the ﬁrst question that
assessed the beneﬁts of DoD. Finally, 5% (7) of
incomplete respondents dedicated ca.
to
the survey and left it after sharing their opinion
about the problems concerning using DoD. Thus,
it seems that the major issues that could discour-
age our respondents were either (1) the topic of
the survey that after reading the introduction was
not compelling enough to continue, or they did not
have experience in the area, (2) the lists of bene-
ﬁts gave not a good impression, e.g., seemed time-
consuming or complicated, or (3) answering the ﬁrst
three groups of questions (year, beneﬁts, problems)
made respondents not willing to continue, e.g., tired
or bored.

7min.

4.8. Data analysis methods

We used the following criteria to validate re-
sponses: (1) a respondent has to answer all obliga-
tory questions (to eliminate drop-outs), (2) the an-
swers to all open questions are relevant (to elimi-
nate misleading data), and (3) the last participa-
tion in the project the respondent focuses on in the
survey was no earlier than 2015. This validation
approach resulted in having no missing data and
allowed us to apply quantitative analysis methods.
Figure 2 presents a map of analyses performed to
answer RQ1–RQ3 and the analysis methods em-
ployed for that purpose.

We used frequency analysis to analyze the direct
responses to the multi-choice questions. For the
questions related to problems and beneﬁts, we in-
troduced additional measures by normalizing the
number of responses with respect to the total num-
ber of responses (we introduce each of these mea-

Figure 2: Analyses performed and methods used to answer RQ1, RQ2, and RQ3.

sures in Section 6 while discussing the results).
Also, we used correlation analysis and association
rules to analyze the relationships between beneﬁts
(RQ1), problems (RQ2), and both of them (RQ3).
In particular, we decided to base the correlation
analysis on the Spearman rank-order correlation co-
eﬃcient (Spearman’s ρ) since the responses were
expressed on ordinal scales. We employed statisti-
cal inference testing to ﬁlter the relationships that
are unlikely to be observed by chance only (we set
the signiﬁcance level α = 0.05) and followed the
guidelines for interpreting the eﬀect size of rela-
tionships provided by Akoglu [38]. Finally, we aug-
mented the correlation analysis by mining associa-
tion rules with the use of the Apriori algorithm [39]
as a tool for identifying new or conﬁrming the pre-
viously identiﬁed multi-factor relationships between
the problems and beneﬁts. When mining associa-
tion rules, we converted the ordinal response scales
to dichotomous scales to express whether a given
beneﬁt/problem was present or absent in a project
(positive responses, e.g., “rather signiﬁcant” and
“deﬁnitely signiﬁcant”, were mapped to “one” while
remaining scale items were replaced with “zero”).
We set the minimum support to 0.1 (support is an
indication of how frequently the itemset appears
in the dataset) and conﬁdence to 0.9 (conﬁdence
is the likelihood that item Y is present if item X
is present). Unfortunately, the number of inferred
rules might be too large to allow for their direct
analysis. Therefore, we choose correlation analy-
sis as our main tool for performing the analyses

and consider association rules as a supporting one.
Finally, although we have to emphasize that iden-
tifying a relationship (correlation) between factors
does not indicate the existence of causality between
them, we attempted to hypothesize about the pos-
sible reasons behind the observed relationships.

For

the open-text questions we used the
grounded theory techniques of coding (open and
axial coding) and constant comparison as recom-
mended by Charmaz [40]. To assure reliability
of the coding process, we executed the following
steps.
Step 1: Preparation—one of the authors (the
3rd author) extracted all responses to open-text
questions and moved them into separate ﬁles (one
ﬁle per each question).
Step 2:
Individual coding–two researchers (the
1st and the 3rd author) performed (individu-
ally) open coding of the text responses, and
each code summarized a single key concept. For
example in the answer of respondent R15 to
the question Q10 about the process of creating
DoD:“ Initially we had a generic template. Then,

over time we discussed each point and adjusted it
to our situation. We also discussed whether there
is something missing and added our own points.”,
the codes of Team and Initial template were
identiﬁed.
Step 3: Creating the coding schema—the two
researchers compared their coding schemes and
discussed their ﬁndings. They applied constant

9

Problems while using DoDBeneﬁts from using DoDProjectHow much do the problems with using DoDs affected projects?(Significance & Frequency)Were the problems andbenefits related?(Correlations)How much did the projects benefit from DoD?(Value)Which problems appeared together?(Correlations,Association rules)Which benefits appeared together?(Correlations,Association rules)RQ2RQ1RQ3RQ2RQ1How the problems were resolved?(open and axial coding)RQ2Were there any problemswhen DoD was not defined?(open and axial coding)directly based on responsesindirectly based on the follow-up analysescomparison to group similar codes. As a result,
they created the ﬁnal coding schema and formu-
lated guidelines on how to interpret and analyze
the quotes.
In the example from Step 2, the
coding schema of categories and subcategories was
developed containing:
• Who creates DoD? (Category)
→ Team, Product Owner, Scrum Master (SubCateg.)
• What is used to create DoD?
→ Initial template, Standard of organization
• What activities?
→ Adjust
etc.
Step 4: Coding conﬁrmation and ﬁnal coding—one
researcher (the 1st researcher) performed coding
using the ﬁnal coding scheme and the guidelines.
Step 5: Validation of the coding schema—yet
another author of the paper (the 2nd author) went
through the data and the coding schema, which
resulted in clariﬁcation of three codes descriptions
and not ﬁnding any coding errors.
Step 6: Identiﬁcation of the relationships between
(the 1st and the 2nd
codes—two researchers
author) were identifying relationships between and
within codes. For example, we identiﬁed several
activities concerning using DoD in all answers
to the question 10 mentioned in Steps 2&3 but
also in other open text questions, such as Adjust
template, Discuss, Document, Analyze.
In this
step number 6 we analyzed them to investigate
which one follows or proceeds others, which shall be
treated as obligatory, and discover any conditions
under which the relationships hold. Although, the
step was applied to all open questions, it turned
out to be the most valuable for question Q10
concerning the process of using DoD.

To analyze the responses to the open question
asking to voluntarily provide the DoD, we used the
the following four step approach:
Step 1: Preparation—the ﬁrst author of the study
extracted DoD items from the answers and placed
them in a table in an Excel spreadsheet.
Step 2: Categorization—the ﬁrst author assigned
the category proposed by Silva et. al in [5] and
[18] to each DoD item. The second author went
through the assigned items and veriﬁed the cor-
rectness of assignment. After veriﬁcation, during a
meeting 4 changes were introduced.
Step 3: Subject identiﬁcation—the ﬁrst author
extracted subjects from each DoD item, next, the
second author went through the subjects assigned

10

to DoD items and veriﬁed the correctness of
assignment. During a meeting of the authors, no
change was introduced, but it was decided that
one item can tackle two subjects.
Step 4: Similarity and variability analysis—the
ﬁrst author extracted statements from same or
similar DoD items and noted them down using
the NoRT notation [41]. Next, the second author
went through the statements and DoD items and
was to verify if he can formulate each DoD item
using the assigned statement. During a meeting of
the authors to discuss the results of their work no
change was introduced.

4.9. Validity threats

The analysis of validity threats is based on the

guidelines provided by Wohlin et al. [26].

Construct validity. We identiﬁed several threats
to construct validity of our study. The ﬁrst one
relates to the understanding of the term “Deﬁni-
tion of Done” by the participants. To mitigate this
threat, we provided a deﬁnition of DoD (based on
the Scrum Guide) in the questionnaire. It does not
guarantee that all participants had a common un-
derstanding of that term; however, it should pre-
vent the situation when a person confuses the term
with some other related concepts (e.g., acceptance
criteria).

Since none of the previous studies reported com-
prehensive lists of beneﬁts or problems related to
using DoDs, there is a threat that our list is largely
incomplete or partially irrelevant. To mitigate this
threat, we based the proposed list of beneﬁts and
problems on the literature regarding DoD / require-
ments and brainstorming sessions among the au-
thors of this paper. We also allowed for the “other”
answer so that the respondents could name the ben-
eﬁts and problems that were not on our lists. There-
fore, if our list was missing some important bene-
ﬁts or problems, we would most likely be informed
about it by at least some participants.

As a respondent might not be aware of a cer-
tain beneﬁt or problem, we allowed for the “I don’t
know” answer to the multichoice questions regard-
ing beneﬁts and problems. Also, we introduced a
measure called Awareness (A) that is calculated as
the percentage of the number of answers other than
“I don’t know” with respect to the total number
of answers for each question regarding beneﬁts or
problems. Observing low Awareness would indi-
cate that the respondents had diﬃculty in reliably

assessing whether or not a given beneﬁt/problem
appeared in their project. This could be caused by
a vague deﬁnition of the beneﬁt/problem, respon-
dents’ lack of knowledge, or the elusive nature of
the beneﬁt/problem.

We asked the participants if the problems they
encountered had signiﬁcant negative impact on
their projects, and if using DoD was valuable for
their projects. We have to accept the fact that ev-
ery person might have a diﬀerent understanding of
what these terms mean in this context.

Also, we made sure that the participants under-
stood the goal of the study by clarifying the goal
at the beginning of the questionnaire, so they were
motivated to provide comprehensive and true an-
swers to the questions.

Finally, we decided to run the survey anony-
mously, only after
respondents completed all
research-related questions they were asked to vol-
untarily provide their email address to receive a re-
port of the results (still it gave the possibility to
stay anonymous).
In this way, we mitigated the
evaluation apprehension threat so that the partic-
ipants were guaranteed that they could be sincere
about all their answers.

To mitigate the risk concerning Content valid-
ity, i.e., the questions we asked our respondents are
not representative of what they aim to measure, we
asked our experts in the pilot study if they see any
necessary changes to introduce to achieve the goals
of our study. We need also to accept that we could
have added some more questions to the question-
naire. However, there is always a trade-oﬀ between
the number of questions respondents would be will-
ing to answer and the thoroughness of answering
the research questions. In our opinion and taking
into consideration the exploratory goal of our study,
we selected the most important questions and left
the space for future research to explore, e.g., the
causes of the identiﬁed problems.

Internal validity. Although we did not seek to
establish causal relationships, we believe that there
are some threats that we can classify as belonging
to internal validity.

First of all, we partially relied on inviting mem-
bers of agile social networks (LinkedIn, Facebook,
MeetUp) to participate in our survey. As a result,
it limited our control over the response collection
process. Consequently, we were not able to deter-
mine neither the response rate nor who received
our invitation. Also, there is a question about the
trustworthiness of the respondents. However, since

11

a high percentage of respondents (45%) left their
email addresses to be informed about the survey
results, we expect that the topic was interesting to
them, and they had no reason to intentionally pro-
vide false responses.

Also, informing the participants about the results
of the study was the only incentive we oﬀered for
participating in the study. It could have a double-
edged impact on the responses we collected. The
use of monetary incentives could have increased the
response rate, however, it could also harm the qual-
ity of the responses since some of the respondents
might have been interested in completing the sur-
vey to be rewarded rather than motivated by the
will of sharing their opinions with the community.
The other threat concerns the skills required to
ﬁll in the questionnaire. We assumed that the re-
spondents would not have problems in responding
to an on-line survey which is created using one of
the most popular survey tool (Survey Monkey) and
consisting of a commonly-used type of questions.
Moreover, we assumed that they are ﬂuent-enough
in English to understand the questions. We con-
ducted a pilot study to ensure that the question-
naire is easy to understand.

We continuously monitored the process of ﬁlling
in the survey (using a quick analysis of the answers
in the survey tool) and, especially, the time that
the respondents spent on answering the questions.
We did not observe any disturbing cases and it took
ca. 17min. 4sec. (median) to complete the survey,
which seems to be a reasonable duration for this
kind of survey (we informed the participants on the
ﬁrst page of the questionnaire that the estimated
time of completing the survey is between 15 and 20
minutes). We also monitored those who dropped
out. Since a signiﬁcant proportion of respondents
left the survey after the ﬁrst question (see the anal-
ysis in Section 4.7) it seems that either the topic was
not interesting or they did not have experience in it,
or the list of beneﬁts of using DoD discouraged the
respondents from further work (e.g., seemed time-
consuming or diﬃcult). Thus, we might suspect
that those who provided complete answers were
those most interested in the topic.

External validity. The main threat to external
validity concerns the representatives of the respon-
dents and their projects. As it follows from our
study design (the goal to draw an overall view of
how DoDs are used) and the analysis of the demo-
graphic data (see Section 6) the respondents repre-
sent diverse proﬁles of software project participants

(i.e., they have diﬀerent experience, work in various
industry sectors, projects were developed in diﬀer-
ent countries, etc.), which is essential to mitigate
the risk of skewing the observations towards some
particular context. The sample seems appropriate
for the goal of our study, which was to get a general
overview of the potential beneﬁts and problems re-
lated to using DoDs in agile projects. However, a
side eﬀect of surveying such a broad population is
that we were not able to relate certain beneﬁts or
problems to the presence of speciﬁc context factors
in the projects. Therefore, based on our results, we
cannot tell which beneﬁts / problems one should
expect to see in their particular agile project.

Also, we focused only on recent projects (i.e., the
last time a respondent participated in the project
was no earlier than 2015), which to some degree
narrowed down the population under study and
might introduce some selection bias to our study.
However, our focus was not to study how the im-
plementation of the DoD practice evolved over the
years, but rather to study the current trends of how
it is used and how it impacts agile software devel-
opment projects.

Conclusion validity. The threat concerns the
scales used to evaluate the beneﬁts and problems
of using DoD. They are subjective and could be in-
terpreted diﬀerently by respondents depending on
their knowledge, experience, character, etc.
(this
is also a threat to construct validity). We also al-
lowed for providing open-text answers or stating “I
don’t know” to avoid biasing the results by forcing
the respondents to answer about the provided sets.
We employed a qualitative coding technique to an-
alyze open-text responses. Although, three authors
of the paper performed a multi-step process of an-
alyzing the responses (see Section 4.8 for details),
such an approach might have introduced some bias
to the conclusions.

the last three years (69% in 2020, 24% in 2019, and
4% in 2018).

Figure 9 presents the countries in which the re-
spondents’ projects were developed. The most
dominating areas were North America, Europe, In-
dia, and Australia, while the two most underrep-
resented regions were China and Central Africa.
A similar geographic distribution of responses was
reported by other recent surveys regarding agile
methods [29, 30]. However, we see that, in our
case, Central Europe could be overrepresented in
the Europe region.

As it is presented in Table 4, the projects were de-
veloped for diﬀerent business domains, with bank-
ing and ﬁnance being the two dominating domains
(Table 4 a). In addition, the two most frequently
developed types of applications were ﬁnancial sys-
tems and web applications (Table 4 b). However,
we can see that the responses quite evenly covered
most of the application types. The distribution of
domains we observed in the studied sample is sim-
ilar to the one reported by Hill [32] for the ISBSG
database.

By looking at the project teams, we can see that
the vast majority of them worked in Scrum (89%),
conﬁrming that the DoD practice is strongly related
to this agile framework (Table 4 c). In other surveys
regarding agile methods, Scrum was reported to be
used by 67% [29, 31] to 94% [30] of the participants.
In nearly 60% of the projects, there were more
than ten people involved (Table 4 d). Therefore,
we can suspect that many of the projects were de-
veloped in multiteam environments.

The analysis of the collected demographic infor-
mation did not reveal strong evidence against the
sample representativeness of the target population.
However, we can see that our sample of projects de-
veloped in Europe could be slightly skewed towards
Central Europe.

5. Demographic information

6. Results and discussion

As it can be seen in Table 3 a), the survey re-
spondents performed a large variety of project roles.
However, most of them were responsible for Scrum
Master-related and management-related tasks, pro-
gramming, or requirements elicitation and analysis.
Also, more than 66% of the participants had 5 or
more years of experience, while only 3% of them
worked in IT for less than a year.

The respondents mainly referred to their recent
projects since 96% of them were developed within

6.1. Process of creating and maintaining DoDs

The results of the analysis of the open questions
(regarding the process of creating and maintaining
DoDs) and three closed questions (concerning the
roles involved in that process, the frequency of up-
dating DoDs, and the ways of publishing them) are
summarized in Figure 4. Also, the ﬁgure reports the
number of answers to the multichoice questions and
the frequency of codes resulted from the analyses of

12

Figure 3: The countries in which the respondents’ projects were developed.

Table 3: Respondents’ characteristics

Responsibilities
(N=137, multiple choices allowed)

Scrum Master’s tasks
Programming
Project management
Requirements

68 Designing software
60 Testing/QA
56 Other
40

(a) Responsibilities of the respondents in projects

Experience
(N=137, single choice allowed)

0–1 year
1–3 years
3–5 years

4 5–10 years
27 over 10 years
15

(b) Experience of the respondents in projects

31
25
22

14
77

the responses to the open questions. However, since
responding to the open questions was optional, and
we cannot guarantee the completeness of each pro-
vided answer (i.e., mentioning all relevant concepts
in the considered project), we will not attempt to
evaluate the importance or popularity of the men-
tioned concepts based on the frequency of the codes.
Still, the frequency analysis is possible for multi-
choice questions.

Based on the analysis of the codes regarding the
process of creating DoDs, we identiﬁed that the ac-
tivities mentioned by the participants create a pro-
cess consisting of three stages (I–III) and ﬁve steps
(1–5) (see Figure 4 Part How): I. (1) proposing a

DoD, (2) analyzing DoD items, (3) adjusting DoD
items, II. (4) documenting, agreeing on, and pub-
lishing the DoD, and III. (5) evaluating and im-
proving the DoD.

Forty-four respondents stated that the process of
creating DoD in their projects started from prepar-
ing a DoD proposal, e.g., by a single team member
“Single member post item in backlog with DoD”, or
by the whole team “Initially look at what Utopia
would look like”. Also, a few of them shared the
triggers that initiated the process. The process
of creating DoDs could be triggered by project
events like project planning or code reviews, dis-
cussion between team members, or be a result of a
single person’s initiative (either a customer or team
member). According to the respondents, the work
on a DoD is equally distributed between the early
stages of product development (e.g., project plan-
ning or kick-oﬀ meetings) and Sprints (e.g., Sprint
planning, retrospective, and backlog reﬁnement
meetings). The most often used toolbox (means)
of supporting this process are teamwork techniques
like brainstorming (e.g., “We sit together and de-
ﬁned it”, “Brainstormed possible items) or reusing
knowledge from previous projects in the form of
templates, organization standards, or DoD items
from the past projects (e.g.“I copy-paste DoD from
other projects as good starting point”). Some DoD
items are also deﬁned based on requirements (e.g.,
based on non-functional requirements).

13

00-11-55-1010-2020-3030-4040-5050-6038%29%23%17%15%7%6%4%4%6%3%3%3%3%2%1%1%1%1%1%1%1%1%1%1%1%1%1%1%1%Figure 4: Process of creating and maintaining DoDs (codes from the axial coding and answers to multi-choice questions).

The second step of the DoD process (marked also
with green in Figure 4), according to 20 partici-
pants, is a further analysis after the initial proposal
is created. The goal is to clarify ambiguities, assess
the feasibility of the DoD items, prioritize them,
and, ﬁnally, select those that should be preserved
(e.g., “then discussion with Team where still adjust-
ments can happen”, “then voted on must-haves vs.
nice to haves”, “then a dedicated “reﬁnement ses-
sio” on DoD is organized (the sooner the better) in
which shortcomings/limitations/inﬂexibilities and
other faults are discussed and resolved ”). After an-
alyzing the proposed DoD, teams introduce the nec-
essary changes. Once the DoD document is ready,
it is made available in the second stage of the DoD
process. According to the respondents, it usually
has a form of a checklist that is published on
the team’s web page or wiki. Another form is to
store it as “tickets” in task management tools or
as backlog items. Some teams prefer to have DoDs
printed out and hang on the wall. Also, some parts
of DoD could be expressed in other forms—e.g., as
pull-request templates. Still, 26 respondents stated
that in their projects DoDs were not explicitly doc-
umented.

Finally, the last step of the DoD process (see

Figure 4 the last step colored with red), the re-
spondents mentioned was about the need for fur-
ther maintenance and improvement of DoDs in
their projects. In most cases, the DoDs were up-
dated on demand whenever there was a reason to
do so (e.g., “then retrospected at various points to
make tweaks”). However, performing regular DoDs
reviews (e.g., after every Sprint or release) was
also a frequently reported practice (e.g., “We re-
ﬁne/revisit occasionally in team retros”). Interest-
ingly, 15 respondents stated that the DoDs in their
projects have never been updated.

According to the respondents, the three roles de-
ﬁned in Scrum (Developers, Product Owner, and
Scrum Master) are most often involved in the pro-
cess of creating DoDs (multichoice question, see
Figure 4 Who?). However, we could still see that in
22% of the projects, the DoDs were created without
involving the developers. It is a surprising observa-
tion since the DoD is used by developers everyday,
and every increment they produce needs to adhere
to it.

6.2. What DoD items are

24 respondents shared with us their DoDs, which
in total had 143 DoD items. To better understand

14

in useWhen created?•Project initiation (9) (e.g., planning, kick-off meeting)•Sprint planning (5)•Product Backlog refinement (2)•Sprint retrospective (2)codes (open question)Toolbox•Teamwork (e.g., brainstorming) (5)•Templates (4)•Organization standards (4)•DoD from other projects (3)•Incorporate customer feedback (1)•Requirements (2)•Goal (product, increment sprint, release) (1)codes (open question)Form•Checklist (e.g., a bullet list) (3)•Ticket (e.g., Trello, Jira) (3)•Pull request template (1)•A poster (1)•Backlog item (1)codes (open question)Who?•Development Team (107)•Product Owner (77)•Scrum Master (56)•Architect (24)•Stakeholders (22)•Customer (10)•A team member (9)•Future maintainers (8)•Other (16)* Beyond the Development Team (30)multi-choice questionHow published?•Team/organization webpage (49)•Task management tool (68)•On the wall in the office (30)•Was not explicitly documented (26)•Wiki (other) (9)multi-choice questionWhen updated?•After every Sprint (31)•After every Release (21)•At  some agreed intervals (4)•When a problem occurred (72)•When needed (other) (12)•Never (15)•I don’t know (6)multi-choice questionPropose (44)Discussion, brainstorming, defining the ideal stateAnalyze (20)clarify ambiguities, possibility of meeting DoD, prioritize, select itemsAdjust (10) adjust the proposal, adjust templatesDocument, sign off,publish (20)Evaluate, improve (16)How? What triggered?•Discussion  (1)•Customer (1)•A team member (1)•Project planning (1)•Code review (1)codes (open question)Table 4: Projects’ characteristics

Domains
(N=137, multiple choice allowed)

Banking
Financial
Services
Telecommunications
Medical & health care
Manufacturing
Electronics & comput-
ers

33 Government
16 Insurance
13 Trading
12 Construction
12 Entertainment
9 Others
7

5
4
3
2
1
20

(a) Domains in which the respondents’ projects were con-
ducted

Types of applications
(N=137, multiple choice allowed)

18 Sales and marketing
Financial
Web or e-business
18 Logistics
Document management 16 Trading
informa-
Management
tion (MIS)
Transaction or produc-
tion
Electronic data inter-
change
Billing

13 Mobile application

13 Personnel

12 Other

11

10
4
3
1

0

0

(b) Types of applications developed in the respondents’
projects

Methodologies
(N=137, multiple choices allowed)

Scrum
Kanban
Other

122 DSDM

8 XP
4 Crystal Clear Methods

(c) Methodologies used in the respondents’ projects

People participating in projects
(N=137, single choice allowed)

up to 3 people
3–9 people
10–18 people

4 19–27 people
52 over 27 people
33

1
2
0

21
27

(d) Sizes of teams in the respondents’ projects
what DoD items are, we analyzed them on three
levels: (1) categories of DoD items, (2) what DoD
items are about, and (3) contents of DoD items.

First, we divided the DoD items into groups us-
in [5]
ing the categories proposed by Silva et. al.
and [18]. We noticed that we had to extend the
proposal by adding one more category, namely AC
check, which would be used to group the items that
concern the fulﬁllment of acceptance criteria.

As it follows from Figure 5, the greatest num-
ber of DoD items concerned Quality Management
(64) and appeared in almost all DODs (21 out of
24). The second most popular category was Process

15

Management with 42 DoD items from 18 diﬀerent
DoDs. The category proposed by us, AC Check,
was present in 7 DoDs in 15 DoD items. There
were also classiﬁed over ten DoD items (11 to be
precise) from 8 diﬀerent DoDs to the Deploy cate-
gory. However, only one DoD contained DoD items
(3) concerning Regulatory Compliance.

Second, we decided to understand better what
the DoD items are and extracted from each item
its subject that is the thing about which is the DoD
item. It follows from Table 5 that DoD items are
most frequently about tests (29 items), code review
(15), acceptance criteria (15), and documentation
(12).

Third, we decided to better understand the con-
tents of DoD items. We noticed that diﬀerent teams
(respondents) formulate either similar or the same
DoD items. Thus, we focused on the similarities
and variability in DoD items. To document the
phenomenon we decided to use the NoRT notation,
which allows to specify such diﬀerences in natural
language [41]. Each statement in this notation con-
sists of: (1) the part that is the unchangeable core;
(2) parameters represented by items in angle brack-
ets (e.g., <number> ); (3) alternatives (options)
that can be selected, are presented in brackets and
are separated with bar characters (e.g., (millisec-
onds | seconds | minutes)). Each option can be
either a parameter, a static (a statement that re-
mains unchanged), or a combination of the two. To
allow choosing more than one option, the ‘}’ option
modiﬁers are used, and to allow omitting a certain
option the option modiﬁer ’]’ is used.

Table 6 contains 10 statements in the NoRT no-
tation showing the similarities and variability of the
DoD items. The statements are based on the DoD
items most frequently mentioned by our respon-
dents, i.e., each one is based on at least 3 DoD
items (so it might be referred to as the top 10 most
frequent). We added also examples of DoD items
that constituted the basis for the statements, some
of which were changed to make the contents anony-
mous.

All statements but one show both similarities and
variability. The highest number of similar DoD
items (13) concerned the code review process. The
S11 statement shows that teams either focus on a
speciﬁc unit of work (e.g., task) or just generally
state that the code must be reviewed. Some respon-
dents added information about the positive result of
the review process or about who will conduct the re-
view. The S3 statement shows that diﬀerent teams

Figure 5: Number of DoD items and DoDs per category of DoD item.

use diﬀerent types of testing and sometimes focus
their testing eﬀort on a deﬁned scope (like feature)
or on a certain branch. No variability was identiﬁed
in the DoD items about completion of coding – the
S23 statement was formulated based on three same
DoD items from diﬀerent respondents.

6.3. Beneﬁts of using DoDs

As it follows from Figure 6, the great majority
93% (128) of the respondents regarded using DoD
as valuable for their project (“Rather valuable” or
“Deﬁnitely valuable”), while 64% of them (88) per-
ceived it as deﬁnitely valuable.

Figure 6: Perceived value of using DoD.

To investigate how often certain beneﬁts of using
DoDs appear in the projects, we deﬁned a mea-
sure called Perceived Value (Val). It is calculated
as the percentage of the answers conﬁrming the

16

presence of a given beneﬁt in a project (“Deﬁnitely
YES” or “Rather YES”) with respect to the to-
tal number of the responses, but excluding the “I
don’t know” answers. Excluding the “I don’t know”
answers from the denominator means that we fo-
cused only on the cases for which the respondents
felt competent enough to judge the causality be-
tween using the DoD practice and the presence of
a given beneﬁt. Please note that there was also a
neutral answer available for the respondents (“Nei-
ther signiﬁcant nor NOT signiﬁcant”). Still, it does
not mean that we completely ignored the “I don’t
know” answers. These answers were incorporated
into the Awareness measure which is the percent-
age of the number of answers other than “I don’t
know” with respect to the total number of answers
for each question. It complements Perceived Value
by showing the level of support for the given ben-
eﬁt in the collected data. For instance, a beneﬁt
with very high Perceived Value and very low Aware-
ness should be taken with a grain of salt (especially,
since we search for beneﬁts that are common across
diﬀerent project contexts). Of course, there might
still be several valid reasons for making such an ob-
servation (e.g., a beneﬁt might be visible only to
some speciﬁc roles, or it could be intangible by its
nature—be something diﬃcult to measure).

The top ﬁve beneﬁts of the highest Perceived
Value concern organizational (management) and
technical areas are (see Table 7): making work
items complete, assuring product quality, ensur-
ing that the activities other than coding were ex-
ecuted, ensuring that all quality gates are passed,

 010203040506070Regulatory ComplianceSoftware architecture designNFR checkConfiguration managementDeployAC CheckProcess managementQuality management#DoDs#Items64%29%4%2%1%Definitely ValuableRather ValuableNeither Valuable norNOT ValuableRather NOT ValuableDefinitely NOTValuableTable 5: Categories and subjects of DoD items with the
number of items and number of DoDs in which they were
present (multiple labels allowed).

Subject of Item

# DoD Items

# DoDs

Quality Management

Tests
Code review
Code
Feature
Defect
(Key) Metrics
Test coverage
Increment
Monitoring
Review
Technical Debt

Process Management
Documentation
Signoﬀ
Feature
Tasks
Demo
Code
Release
User Stories
Knowledge of team
Assessment
Brieﬁng

AC Check
AC

Deploy

Deployment
Release
Release notes

NFR Check
NFR
NFRs

Conﬁguration Management

Merge
Build
Code
Feature
Repository

Regulatory Compliance
External standard

Software architecture design

Design
API Documentation

29
15
4
4
4
2
2
1
1
1
1

12
7
4
4
4
3
2
2
2
1
1

15

7
3
1

6
1

3
2
1
1
1

3

2
1

16
13
4
3
3
2
2
1
1
1
1

8
5
4
4
3
3
2
2
1
1
1

7

4
3
1

3
1

3
2
1
1
1

1

1
1

and keeping the product releasable. All but one of
the beneﬁts appeared in over 50% of the projects.
The one exceptional case was the B19 “Less time
It appeared in
spent on manual testing” beneﬁt.

17

around 1/3 of the projects (32%). Finally, the re-
spondents extended the provided list of beneﬁts by
adding two new proposals: B20 “DoD helped with
creating clean code” and B21 “DoD caused that the
whole team felt the ownership of quality.”

The calculated Awareness (A) (see the preced-
ing paragraphs and Section 4.9 for the explanation
of the measure) for the beneﬁts ranged between
90% and 99%. Thus, we conclude that the respon-
dents were well-informed and knowledgeable about
the consequences of using DoD. The beneﬁt with
the lowest Awareness was: B9 “Increase in cus-
tomer’s satisfaction.” The second lowest Awareness
was observed for B17 “Balance between short-term
delivery of features and long-term product quality.”
Both of these beneﬁts might be diﬃcult to judge by
a project team member who is not closely collabo-
rating with the customer or is not involved in the
planning and assessment of product quality.

As we can see in Figure 7, the beneﬁts of using
DoD seem to be highly correlated with each other
(only positive correlations were observed). The cal-
culated Spearman’s ρ for the statistically signiﬁcant
relationships ranged between 0.13 and 0.75 (mean
= 0.39). According to Akoglu [38] the mean value of
ρ = 0.39 could be interpreted as a moderate/strong
relationship, while the strongest observed relation-
ships could be classiﬁed as moderate to very strong.

The strongest correlation (ρ between 0.64 and
0.75) was observed between a group of three ben-
eﬁts related to code quality (B11: ensuring coding
standards, B16: transparency of code quality, and
B18: clean repositories). The second highly corre-
lated group of beneﬁts (ρ ca. 0.61) regarded the
quality-assurance process (B2: helping assure the
quality of product, B3: ensuring that the activities
other than coding are executed, and B4: ensuring
that all quality gates are passed). Finally, one more
correlation with ρ ≥ 0.60 was observed that shows
a relationship between the beneﬁt of a smaller num-
ber of defects being released (B8) and the increase
in customer satisfaction (B9). Finally, there were
only two beneﬁts that did not correlate visibly with
many other beneﬁts, i.e., less time spent on man-
ual testing (B19) and making team members aware
of the current project status (B10). Unfortunately,
since so many beneﬁts were highly correlated, the
number of identiﬁed association rules did not allow
us to reveal any additional interesting and mean-
ingful groups of beneﬁts.

Table 6: Statements showing variability and similarity in DoD items.

• S11 (13 DoD items):
person, two reviewers}]

{Task | Code | <unit of work>} must be reviewed [and passed] [by {at least one other

e.g., Code must be reviewed by at least one other person

• S3 (11):
test>}] [on branch <name>]

{Unit | Integration | Regression | Manually | System | <type of test>} tested [{feature | <scope of

e.g., Manually tested on branch develop

• S14 (9): [{Functional | Technical | Integration | Deployment instructions | <name of documentation>}] Docu-
mentation is completed.

e.g., Technical documentation is completed

• S19 (9): The acceptance criterion is satisfied: <contents of the AC>

e.g., The acceptance criterion is satisﬁed: sorting table by all columns

• S2 (7): {Functional | <type of test>} Tests were executed [and pass] [in <environment name>]

e.g., Functional tests were executed and pass
• S22 (5): Deployed to <environment name>
e.g., Deployed to development environment

• S24 (4): No {open defects | <type of issue>} present

e.g., No open defects present

• S25 (3): Released [to <environment name>]

e.g., Released to production
• S23 (3): Coding is complete
• S17 (3): <unit of work> must satisfy the acceptance criteria

e.g., Feature must satisfy the acceptance criteria

6.4. Problems resulting from not using DoDs

6.5. Problems encountered while using DoDs

When deciding on adopting a practice, it is im-
portant to know what beneﬁts it might bring to the
project, however, it is also useful to understand the
consequences of neglecting it.

Thirty-one respondents shared some problems
they had encountered in their other projects that
occurred due to the lack of DoD. Those aspects con-
cern technical-, team-, and project-level issues and
are listed in Figure 8. The top ﬁve most frequently
mentioned problems were:

• not meeting deadlines (e.g.,“we went beyond

the timeline and budget”),

• lack of shared understanding of what done
means (e.g., “Some member have diﬀerent view
of what to achieve, which sometimes resulting
in over commit to task, team member ﬁghting
on what task to cover in each feature”, “Not
having alignment on what “done” means for a
team causes lots of problems and friction be-
tween dev team and PO and team and stake-
holders.”),

• defects,

low quality,

and technical debt
(e.g.,“technical debt creeping in”, “they haven’t
delivered the expected quality and ended up in
lot of rework post UAT ”, “Finally, defects were
commonplace”).

We analyzed the responses regarding the prob-
lems that might appear while using DoDs by inves-
tigating how often certain issues appeared in the
projects and how harmful they were.
In particu-
lar, we calculated two measures that allowed us to
quantify these two aspects:

• Frequency (Freq) is the percentage of all re-
sponses except for the “Problem did not ap-
pear” and “I don’t know” with respect to the
total number of responses other than the “I
don’t know” answers;

• Signiﬁcant impact (Sig) is the percentage of
the responses claiming that the problem had
a “Deﬁnitely Signiﬁcant” or “Rather Signiﬁ-
cant” negative impact on the project with re-
spect to the total number of responses other
than the “I don’t know” answers.

Freq tells us how likely it is that a given problem
appears in a project which uses the DoD practice.
We excluded the “I don’t know” answers for the
same reasons as we did it when calculating Per-
ceived Value (see Section 6.3). These answers can
either mean that the respondents had not encoun-
tered a given problem before or that they had in-
suﬃcient knowledge to conﬁrm its presence or lack
of thereof. Sig allows us to evaluate the severity

18

Figure 7: Correlations between the reported beneﬁts of using DoD (only statistically signiﬁcant correlations are presented).

of problems. We focused only on the situations
when a given problem leads to serious negative con-
sequences. However, even when the respondents
stated that the impact of a given problem was not
signiﬁcant, it does not mean that it was negligible.
As it follows from Table 8, the top 5 most
frequent problems (P1 “It was diﬃcult or time-
consuming to implement DoD item(s)”, P2 “Some
team members had diﬀerent understanding of
DoD”, P3 “It was diﬃcult or time-consuming to
verify if DoD item(s) are satisﬁed”, P4 “Problem
with deﬁning universal DoD”, and P5 “Some DoD
item(s) were imprecise”) were reported to appear

in 70% or more of the projects. The next two
frequently appearing problems were missing (P6),
unclear / diﬃcult to understand (P7), or impossi-
ble to verify (P8) DoDs, which were reported to be
present in over 60% of projects. The least frequent
problems, i.e., infeasible DoD items (P19), incorrect
DoD items (P18), and not written down DoD (P17)
were present in more than 45% of the projects.

With respect to signiﬁcance, we observed that
the problems concerning the unavailability of DoD
(P17),
imprecise DoD items (P5), missing DoD
items (P6), and reaching an agreement between the
team and stakeholders (P12) were perceived as hav-

19

Quality assurance processDefects and customer satisfactionCode qualityTable 7: Beneﬁts of using Deﬁnition of Done

Beneﬁt from using DoD

ID
B1 Help to make work items complete
B2 Help to assure quality of product
B3 Help to ensure that the activities
other than coding were executed
(e.g., code review, manual testing,
build)

B4 Help to ensure that all quality gates
are passed (e.g., performance test-
ing, code review, security check)
B5 Help to keep product releasable
B6 Help in eﬀort estimation
B7 Promotion on the meaning of “com-

plete” work between stakeholders

B8 Fewer bugs and issues get released
B9
Increase in customer’s satisfaction
B10 Help to make team members aware
of the current status of the project
B11 Help to ensure organization’s cod-

ing standards

B12 Reduction of technical debt (early

spotting the defects)

B13 Increase of Development Team pro-

ductivity

B14 Reduction in time for reworks of im-

plemented features

B15 Help to keep the documentation up-

to-date

B16 Transparency of code quality (e.g.,

explained what it is)

B17 Balance between short-term deliv-
ery of features and long-term prod-
uct quality

B18 Help to keep code repository clean
B19 Less time spent on manual testing
*B20 “DoD helped with creating clean

code”

*B21 “DoD caused that the whole team
felt the ownership of quality”

Val% A%
99
98
99

93
92
90

90

98

86
79
79

76
72
66

66

64

63

61

56

55

55

52
32
–

–

97
98
96

98
90
97

97

98

96

96

96

96

93

98
96
–

–

* beneﬁts identiﬁed by some the participants
(using the “other” option).

ing a signiﬁcant, negative impact on 44%–48% of
the projects.

To evaluate the importance of the problems, we
should consider both Freq and Sig measures. Fig-
ure 9 presents each of the problems in the two-
dimensional space of these two measures with arbi-
trary guiding lines added for each of the measures
at 33%, 50%, and 67%. Based on the plot, we could
state that the most important problems are P2, P4,
P5, and P6 since they are all frequently appearing
and have signiﬁcant impact. All of them relate to
the issues with specifying DoDs (missing, imprecise,
or ambiguous items that are understood diﬀerently

20

Figure 8: Problems that software projects struggle with
when the DoD practice is not used.

by team members).

To investigate if the ﬁndings concerning the prob-
lems are reliable, similarly like in the case of
the earlier analysis of beneﬁts, we calculated the
Awareness (A), which was between 94% and 100%.
Thus, it seems that the respondents were also well-
informed and knowledgeable to answer the ques-
tions about the problems concerning using DoD.

Figure 10 shows the correlation matrix for prob-
lems while using DoD. The calculated Spearman’s ρ
for the statistically signiﬁcant relationships ranged
between 0.17 and 0.80 (mean = 0.39). The mean
value of ρ = 0.39 could be interpreted as mod-
erate/strong relationship while the strongest ob-
served relationships could be classiﬁed as strong to
very strong. The analysis of the frequent itemsets,
association rules, and the calculated correlation co-
eﬃcients allowed us to identify a group of eight
problems with strong intra-group correlations. All
these problems relate to how DoD-items are formu-
imprecise (P5), missing (P6), un-
lated/speciﬁed:
clear (P7), impossible to verify (P8), out of date
(P14),
incorrect (P18), and in-
feasible (P19). When we combined all these prob-
lems into a single meta-problem, it appeared to be
correlated with many other problems (at least one
of the problems covered by the meta-problem was
correlated with another problem), e.g., a diﬀerent
understanding of DoD (P2), diﬃculties and eﬀort

irrelevant (P15),

TechnicalTeamProject•Defects (6)•Low  quality (6)•Technical debt (6)•Different coding standards(4)•Issues around dependencies, difﬁculties in regression testing (1)•Lack of shared understanding what done means (6)•Less or no accountability and commitment (5)•Misunderstandings (5)•Assumptions that somebody else will do the work (2)•Not meeting deadline (7)•Estimation issues(4)•Lack of QA (4)•Missing no technical •items (e.g., doc) (4)•Rework needed (4)•Stakeholders not satisﬁed (4)•Additional costs (3)•Scope creep (2)•Chaos (2)Figure 9: The importance of identiﬁed problems related to using DoDs (color—importance, size—the number of responses
assessing the negative impact of problems as deﬁnitely signiﬁcant).

while verifying DoD satisfaction (P3), too exten-
sive DoD (P10), or problems with the agreement be-
tween team and stakeholders on DoD. We were also
able to identify some other relationships by analyz-
ing Figure 10 and association rules. For instance,
in the association rules having “P2: Some team
members had a diﬀerent understanding of DoD”
as consequent, the antecedents were (apart of the
previously mentioned meta-problem with how DoD
items are formulated) problems like a too exten-
sive set of DoD items (P10), problems with agree-
ing on DoD within stakeholders and team (P12),
diﬃculties and eﬀort required to verify DoD satis-
faction (P3), or problems with deﬁning DoD at the
organizational level (P4). Although these relation-
ships are derived based on correlations, we could
hypothesize that some causality relationships be-
tween these problems exist. For example, if a set
of DoD items is too extensive, it might be hard to
memorize it, and consequently, it could lead to a
diﬀerent understanding of the DoD by team mem-
bers. Another example could be “P10: Too exten-
sive set of DoD items” which was a consequence
of rules having antecedents such as problems with
deﬁning a DoD at the level of organization (P4),
omitting some activities deﬁned in DoD (P9), prob-
lems with the common understanding of DoD (P2),
and the meta-problem regarding how DoD is formu-

lated. One more example would be “P12: Problem
with agreement between the Team and stakeholders
on the DoD” which was a consequent of rules hav-
ing antecedents such as diﬃculties in verifying DoD
items satisfaction (P3), a diﬀerent understanding of
DoD by team members (P2), problems with creat-
ing universal DoD (P4), changing DoD during iter-
ations (P13), or when team members did not care
about DoD (P11).

Fortunately, many of the problems could be re-
solved. Twenty-three respondents shared their ex-
perience on how the problems concerning DoD were
solved in their projects. The majority of them (11)
explicitly stated that the problems were solved by
team (e.g., “Fix them!
It’s just a matter of sit-
ting down with the team”, “we called in a team
meeting and worked out issues.” Two respondents
reckoned that a person with a coach/Scrum mas-
ter role was needed to support the team or initi-
ate the process. Six respondents stated that the
problems were solved while team members were in-
teracting, mostly while team meetings/workshops
such as retrospectives. What is interesting is that
two respondents highlighted that solving problems
with DoDs is not a “one-shot” activity, but an incre-
mental process since DoDs evolve in time. During
the process of solving the problems, DoD items were
clariﬁed (5), “started to have shared understanding”

21

P1P3P8P9P7P2P4P5P10P6P11P12P14P13P15P16P19P18P17It was difficult or time-consuming to implement DoD item(s)Some team members had different understanding of DoDIt was difficult or time-consuming to verify if DoD item(s) are satisfiedProblem with deﬁning universal DoD (e.g. DoD at organizational level) Some DoD item(s) were imprecise Some DoD item(s) were missingDoD item(s) were unclear or difﬁcult to understandSome DoD item(s) were impossible to be veriﬁedSome activities were omitted as DoD stated that they do not have to be executedToo extensive set of DoD itemsTeam members did not care about DoDProblem with agreement between the Team and stakeholders on the DoDDoD was changing during Sprint/increment Some DoD item(s) were out of dateSome DoD item(s) were irrelevantDoD was creeping DoD was not documented, unavailableSome DoD item(s)were incorrect Some DoD item(s) were infeasibleFigure 10: Correlations between the reported problems while using DoD (only statistically signiﬁcant correlations are presented).

(2), made relevant (2), adjusted (1), discussed (1),
negotiated (1), made useful (1), some external de-
pendencies were removed (1), or at one extreme the
DoD was removed (1).

6.6. Problems and beneﬁts relationships

The survey questions asked directly about the
problems that appeared in the participants’ project
while using DoD and about their signiﬁcance. How-
ever, by analyzing the relationships between the
problems and beneﬁts, we can investigate whether
the beneﬁts from using DoD are less/more likely
to be observed when certain problems with DoD

materialize (or alternatively, whether they are in-
dependent).

The correlation matrix in Figure 11 shows that
both positive and negative (statistically signiﬁcant)
correlations could be observed between problems
and beneﬁts. The calculated Spearman’s ρ ranged
between 0.18 and 0.32 (mean = 0.23) for positive
correlations and between -0.17 and -0.33 (mean =
-0.23) for negative ones and could be interpreted as
weak to moderate relationships.

Observing negative correlations means that the
more signiﬁcant problems with using DoD are ob-
served, the fewer beneﬁts one should see in a

22

Problems related to how DoD items are formulatedTable 8: Problems with using Deﬁnition of Done

ID Problem with using DoD Freq% Sig% A%
97
P1 It was diﬃcult or

33

77

time-
consuming to implement
DoD item(s)

P2 Some team members had
diﬀerent understanding of
DoD

P3 It was diﬃcult or

time-
consuming to verify if DoD
item(s) are satisﬁed

P4 Problem with deﬁning uni-
versal DoD (e.g. DoD on
organization level)

P5 Some DoD item(s) were
(loosely stated

imprecise
goals)

P6 Some DoD item(s) were

missing

P7 DoD item(s) were unclear

or diﬃcult to understand

P8 Some DoD item(s) were im-
possible to be veriﬁed
P9 Some activities were omit-
ted as DoD stated that they
do not have be executed

P10 Too extensive set of DoD

items

P11 Team members did not care
about DoD (they omitted
some or all DoD items)
P12 Problem with agreement
between the Team and
stakeholders on the DoD

P13 DoD was changing during

Sprint/increment

P14 Some DoD item(s) were out

of date

P15 Some DoD item(s) were ir-

relevant

P16 DoD was creeping (contin-
uously growing in uncon-
trolled manner)

P17 DoD was not documented,
unavailable (DoD was es-
tablished verbally, not writ-
ten down)

P18 Some DoD item(s) were in-

correct

P19 Some DoD item(s) were in-
feasible (impossible to satis-
ﬁed)

76

40

100

75

27

97

71

40

94

70

47

97

66

65

65

62

61

59

44

38

30

37

43

43

99

97

97

97

99

99

55

45

98

55

55

53

52

39

44

39

41

99

98

97

97

47

48

99

46

46

40

38

98

98

project. Most of the beneﬁts are correlated with
at most a few problems. Beneﬁt “B4: Help to en-
sure that all quality gates are passed” is an excep-
tion since it is negatively correlated with nearly all
the problems. Another beneﬁt that is negatively
correlated with numerous problems (seven) is “B2:

23

Help to assure quality of product.” Both of these
beneﬁts are related to the quality assurance process
and both correlate with the problem “P11: Team
members did not care about DoD (they omitted
some or all DoD items).” We hypothesize that the
fact that a team does not care about DoD might
be an indicator that the team could be also reluc-
tant to perform other pro-quality activities. Both
of these beneﬁts are also correlated with some prob-
lems related to how DoD is formulated (our meta-
problem). Finally, we could see that beneﬁt “B8:
Fewer bugs and issues get released” is negatively
correlated with problems such as “P7: DoD item(s)
were unclear or diﬃcult to understand”, “P8: Some
DoD item(s) were impossible to be veriﬁed”, “P17:
DoD was not documented, unavailable (DoD was
established verbally, not written down)”, and again
with “P11: Team members did not care about DoD
(they omitted some or all DoD items).” We hypoth-
esize that there could be some causality between
the problems and beneﬁt B8 since items that are
unclear, diﬃcult to understand (P7), impossible to
verify (B8), and agreed only verbally (P17) could
make the testing process diﬃcult (not to mention
that the presence of problem P11 characterizes the
team’s attitude to quality assurance).

The presence of positive correlations between
problems and beneﬁts might seem surprising at
ﬁrst, however, we believe that some of these rela-
tionships could be justiﬁed. For instance, observ-
ing positive correlations between “P1: It was diﬃ-
cult or time-consuming to implement DoD item(s)”
and numerous beneﬁts show that “quality does not
come for free” and teams need to invest time and
resources into pro-quality activities to see the ben-
eﬁts. Another reason for observing positive cor-
relations could be that the appearance of some
DoD-related problems might be a good indicator of
deeper problems in projects or organizations (e.g.,
problems with communication, decision making, or
resigning from pro-quality activities)—DoD could
be perceived as even more valuable by teams that
struggle with quality assurance. Finally, the prob-
lems with using DoD could be temporal, and in
some cases, their presence could even trigger pos-
itive changes and lead to implementing corrective
actions.

Figure 11: Correlations between the reported problems and beneﬁts of using DoD (only statistically signiﬁcant correlations are
presented).

7. Study implications

7.1. Implications to research

Our study conﬁrms that using DoDs is useful,
as has already been claimed by several researchers
and practitioners based on their experience, e.g.,
[13], [24], [17].

We identiﬁed and evaluated the importance of
several beneﬁts that DoDs might bring. However,
our ﬁndings are a trigger for opening a new discus-
sion about when such beneﬁts are present and how
to create an eﬀective DoD to help materialize these
beneﬁts.

Our study extends the body of knowledge related
to the problems encountered while using DoDs by
providing an initial evaluation of their frequency
and signiﬁcance. Since several problems might ap-
pear frequently and have a negative impact on ag-
ile projects, a further and more in-depth analysis
would be valuable. We identify an opportunity for
more ﬁne-grained research on why the problems ap-
pear, what we can do to prevent them, and how to
solve them.

Also, the results of the survey complement the
previous studies on investigating how the practice

24

B1: Help to make work items completeB2: Help to assure quality of productB3: Help to ensure that the activities other than coding were executed (e.g., code review,...B4: Help to ensure that all quality gates are passed (e.g., performance testing, code revi...B5: Help to keep product releasableB6: Help in effort estimationB7: Promotion on the meaning of "complete" work between stakeholdersB8: Fewer bugs and issues get releasedB9: Increase in customer's satisfactionB10: Help to make team members aware of the current status of the projectB11: Help to ensure organization's coding standardsB12: Reduction of technical debt (early spotting the defects)B13: Increase of Development Team productivityB14: Reduction in time for reworks of implemented featuresB15: Help to keep the documentation up-to-dateB16: Transparency of code quality (e.g., explained what it is)B17: Balance between short-term delivery of features and long-term product qualityB18: Help to keep code repository cleanB19: Less time spent on manual testingP1: It was difficult or time-consuming to implement DoD item(s)P2: Some team members had different understanding of DoDP3: It was difficult or time-consuming to verify if DoD item(s) are sa...P4: Problem with defining universal DoD (e.g. DoD on organization leve...P5: Some DoD item(s) were imprecise (loosely stated goals)P6: Some DoD item(s) were missingP7: DoD item(s) were unclear or difficult to understandP8: Some DoD item(s) were impossible to be verifiedP9: Some activities were omitted as DoD stated that they do not have b...P10: Too extensive set of DoD itemsP11: Team members did not care about DoD (they omitted some or all DoD...P12: Problem with agreement between the Team and stakeholders on the D...P13: DoD was changing during Sprint/incrementP14: Some DoD item(s) were out of dateP15: Some DoD item(s) were irrelevantP16: DoD was creeping (continuously growing in uncontrolled manner)P17: DoD was not documented, unavailable (DoD was established verbally...P18: Some DoD item(s) were incorrectP19: Some DoD item(s) were infeasible (impossible to satisfied)101of DoD is applied and what DoD items are, e.g.,
[5], [18]. The respondents reported that there are
multiple ways of creating and maintaining DoDs.
Therefore, as future research, it would be worth
investigating the advantages and disadvantages of
diﬀerent approaches. Our analysis of 24 DoDs pro-
vided by the respondents showed that DoD items
concern diﬀerent subjects (like code, code reviews,
tasks etc.)
and their contents might be similar
across diﬀerent organizations. Thus, studying more
DoDs might be valuable to broaden the knowledge
about what DoD items are and how they are formu-
lated. Possibly, it might additionally lead to learn
how to eﬃciently and eﬀectively specify DoD items.

Moreover, new research directions are opened by
two groups of problems—the problems related to
the speciﬁcation of DoDs and the cost of creating
and maintaining DoD. It would be worth consider-
ing the possibility of supporting or even automating
to some degree the process of creating DoDs.

Finally, from the empirical software engineering
perspective, we would like to draw the attention of
other researchers to the usability of the question-
naire used in their survey, especially the presenta-
It might have a signiﬁcant
tion of the questions.
It follows from the
impact on the response rate.
analysis of the drop-outs in our study that a sig-
niﬁcant number of our respondents left when they
were presented a list of beneﬁts or a list of prob-
lems to assess (i.e., a list of several items to assess
on a scale with more than 5 values). It seems that
it might be caused by the presentation method of
the list-type of questions, e.g., the impression that
answering the question would be overwhelming or
time-consuming.

7.2. Implications for practitioners

Our ﬁndings show that the DoD practice is per-
ceived as useful by nearly all practitioners who used
it. The primary beneﬁts are about helping to make
work items complete to ensure product quality and
that the activities other than coding are executed,
and all quality gates are passed to keep the product
releasable.

However, when deciding to use DoD or when
thinking about improving that practice, one needs
to understand that there might appear some prob-
lems along the way. The list of problems and their
evaluation from the frequency and signiﬁcance per-
spectives might help practitioners to assess the risks

25

related to adopting the DoD practice and prepare
in advance.

The results of our study showed that problems
related to how DoD items are formulated correlate
with nearly all other DoD-related problems (includ-
ing those that could lower the chances of materi-
alizing beneﬁts from using DoD). We believe that
such problems can be easily mitigated by review-
ing DoD documents. We propose a set of DoD
quality criteria called VIRUPCUS that correspond
to each of the aforementioned problems (Veriﬁable
(P8), Identiﬁed (P6), Relevent (P15), Up-to-date
(P14), Precise (P5), Correct (P18), Unambigious
(P7), Satisﬁable (P19)). The criteria can be used
as a checklist. For instance, when reviewing one
of the DoD items provided by respondents stating
that “burndown [ should be ] as close to deadline
as possible,” we could argue that it is imprecise
and impossible to verify. Another example could
be a DoD item stating that “automated test [ are ]
written.” We perceive this DoD item as incorrect
because we suspect that the real intent behind for-
mulating it was to have automated test cases that
pass. Also, it is imprecise because we do not know
“how many” tests should be written. Finally, the
word “written” seems ambiguous in this context—
does it mean implemented?

It is also important to be aware of the fact that
when a team dismisses to use DoD in their project,
several important risks might materialize, such as,
lack of shared under-
not meeting the deadline,
standing of what “done” means, defects, low qual-
ity, and technical debt.

Moreover, the results of our study show how prac-
titioners establish and maintain DoD which can be
It might be espe-
used for educational purposes.
cially useful for novice Scrum adepts who look for
guidelines on how to create a DoD or for those who
seek some ideas on how to improve their process.
Together with the information about the beneﬁts,
it might provide a strong argument in favor of us-
ing DoD for those who are reluctant (e.g., for Scrum
Masters or agile coaches who would like to convince
those undecided).

Finally, another educational application of the
results of our study could be for those looking for
what their DoD might contain or what they can add
to their DoD. Those practitioners can look into the
categories or the subjects of DoD items to support
their, e.g., brainstorming session in their team. Ad-
ditionally, the ten statements that can be used to
formulate DoD items might also trigger a discussion

of whether they are relevant in their context or not.

8. Conclusions

The goal of the study presented in this paper was
to investigate the usefulness of using the Deﬁni-
tion of Done (DoD) practice. To achieve that goal,
we conducted a questionnaire-based survey. In the
research study 137 practitioners from all over the
world took part.

Within the study, we looked at the usefulness of
DoD from two perspectives: the beneﬁts it might
bring to a project or product team, and the prob-
lems it might cause. Additionally, we investigated
how DoDs are created and maintained to extend
the existing body of knowledge.

Our ﬁndings show that the vast majority of prac-
titioners (93%) perceive DoD as at least valuable
and conﬁrm that its presence in the project help
to make work items complete, assure product qual-
ity, ensure the needed activities are executed, en-
sure that all quality gates are passed, and help to
keep product releasable. The usefulness of DoD
can be also conﬁrmed by an analysis of problems
that might appear when DoD is not used. Our re-
spondents mentioned defects, low quality, technical
debt, lack of shared understanding of what done
means, and not meeting the deadline as those most
frequent.

On the other hand, the results of the survey show
that there are some common problems related to
using DoDs, and some of them might even signiﬁ-
cantly impact an agile project (e.g. high eﬀort of
implementing and using DoDs or diﬀerent under-
standing of DoD items).

Moreover, the most important problems with us-
ing DoDs relate to imprecise, missing, or unclear
DoD items. These problems are getting solved by
agile practitioners mainly internally within a team
through some collaborative activities like work-
shops or, in some cases, with the help of an agile
coach. We proposed a set of DoD quality criteria
(VIRUPCUS) that might be used as a tool to sup-
port reviewing DoD documents.

Furthermore, the respondents characterized the
process of creating and using a DoD and what DoD
items are. Although there are some variations in
that process, the most general steps are: (1) pro-
pose a DoD, (2) analyze it, (3) adjust the pro-
posal, (4) document, sign oﬀ and publish the DoD,
(5) use it, (6) evaluate, and improve the DoD. In

most cases, the DoD was created by the members of
Scrum Team, however, we observed that in nearly
22% of the studied projects, the developers were
not involved in that process. We also found that
most DoD items concern quality management and
process management, and are about tests, code re-
views, documentation and acceptance criteria. The
subjects and contents of some DoD items across dif-
ferent companies is the same or similar with some
variability.

The ﬁndings from the study allowed us to iden-
tify new research directions, i.e., the necessity of
in-depth analysis of the DoD practice, of the iden-
tiﬁcation of the means of preventing or mitigating
the problems related to using DoDs, and, ﬁnally,
of the investigation of the ways to maximize the
beneﬁts that come from using DoDs.

Acknowledgment

The authors would like to thank all participants
of the study for sharing their experience and opin-
ions.

References

[1] Project Management

Institute, Pulse of Profes-
sion, Success Rates Rise, Transforming the high
cost of
https://www.pmi.org/-
/media/pmi/documents/public/pdf/learning/thought-
leadership/pulse/pulse-of-the-profession-2017.pdf,
last
access March 24, 2021 (2017).

low performance,

[2] K. Schwaber, J. Sutherland, The Scrum Guide. The
Deﬁnitive Guide to Scrum: The Rules of the Game.
Scrum.org (2020).

[3] S. Theobald, A. Schmitt, P. Diebold, Comparing scaling
agile frameworks based on underlying practices, in: In-
ternational Conference on Agile Software Development,
Springer, 2019, pp. 88–96.

[4] I. Nurdiani, J. B¨orstler, S. Fricker, K. Petersen,
P. Chatzipetrou, Understanding the order of agile prac-
tice introduction: Comparing agile maturity models
and practitioners’ experience, Journal of Systems and
Software 156 (2019) 1–20.

[5] A. Silva, T. Ara´ujo,

J. Nunes, M. Perkusich,
E. Dilorenzo, H. Almeida, A. Perkusich, A systematic
review on the use of Deﬁnition of Done on agile software
development projects, in: ACM Int. Conf. Proceeding
Ser., Vol. Part F1286, 2017. doi:10.1145/3084226.
3084262.

[6] W. Alsaqaf, M. Daneva, R. Wieringa, Quality re-
quirements challenges in the context of large-scale dis-
tributed agile: An empirical study, Information and
Software Technology 110 (2019) 39–55.

[7] C. P. O’Connor, Letters from the edge of an agile transi-
tion, Proc. ACM Int. Conf. Companion Object Oriented
Program. Syst. Lang. Appl. Companion, SPLASH ’10
(2010) 79–84doi:10.1145/1869542.1869557.

26

driven development for teaching scrum framework, 36th
Int. Conf. Softw. Eng. ICSE Companion 2014 - Proc.
(2014) 372–381doi:10.1145/2591062.2591162.

[26] C. Wohlin, P. Runeson, M. H¨ost, M. C. Ohlsson,
B. Regnell, A. Wessl´en, Experimentation in software
engineering, Springer Science & Business Media, 2012.
[27] J. S. Moll´eri, K. Petersen, E. Mendes, An empirically
evaluated checklist for surveys in software engineer-
ing, Information and Software Technology 119 (2020)
106240.

[28] R. Hoda, N. Salleh, J. Grundy, The rise and evolution of
agile software development, IEEE software 35 (5) (2018)
58–63.

[29] Digital.ai, 15th annual state of agile report (2021).

URL https://stateofagile.com

[30] M. Ochodek, S. Kopczy´nska, Perceived importance of
agile requirements engineering practices–a survey, Jour-
nal of Systems and Software 143 (2018) 29–43.

[31] G. S. Matharu, A. Mishra, H. Singh, P. Upadhyay,
Empirical study of agile software development method-
ologies: A comparative analysis, SIGSOFT Softw.
Eng. Notes 40 (1) (2015) 1–6. doi:10.1145/2693208.
2693233.
URL https://doi.org/10.1145/2693208.2693233
[32] P. R. Hill, Practical software project estimation: a
toolkit for estimating software development eﬀort & du-
ration, McGraw-Hill Education, 2011.

[33] K. E. Wiegers, J. Beatty, Software Requirements 3, Mi-

crosoft Press, USA, 2013.
Software

[34] ISBSG,

Project

https://www.isbsg.org/software-project-data/,
access February 27, 2020.

Data,
last

[35] European commision,

Structural business

statis-
tics - small and medium-sized enterprises (smes),
https://ec.europa.eu/eurostat/web/structural-
business-statistics/structural-business-statistics/sme,
last access February 20, 2020.

[36] S.

Narayan,

Products

Over

Projects,

https://martinfowler.com/articles/products-over-
projects.html, last accessed 10/03/2022 (2018).

[37] N. G. Vinson, J. Singer, A practical guide to ethical
research involving humans, in: Guide to Advanced Em-
pirical Software Engineering, Springer, 2008, pp. 229–
256.

[38] H. Akoglu, User’s guide to correlation coeﬃcients,
Turkish journal of emergency medicine 18 (3) (2018)
91–93.

[39] R. Agrawal, R. Srikant, et al., Fast algorithms for min-
ing association rules, in: Proc. 20th int. conf. very large
data bases, VLDB, Vol. 1215, Citeseer, 1994, pp. 487–
499.

[40] K. Charmaz, Constructing grounded theory: A practi-
cal guide through qualitative analysis, sage, 2006.
[41] S. Kopczy´nska, J. Nawrocki, M. Ochodek, An empiri-
cal study on catalog of non-functional requirement tem-
plates: Usefulness and maintenance issues, Information
and Software Technology 103 (2018) 75–91.

[8] M. Paasivaara, B. Behm, C. Lassenius, M. Hallikainen,
Large-scale agile transformation at Ericsson: a case
study, Empirical Software Engineering 23 (5) (2018)
2550–2596.

[9] K. Power, Deﬁnition of Ready: An Experience Report
from Teams at Cisco, in: Agil. Process. Softw. Eng.
Extrem. Program., Springer, 2014, pp. 312–319.
[10] D. Anderson, Essential Kanban Condensed, Blue Hole

Press, 2016.

[11] S. Madan, Done understanding of

the deﬁnition
of done, https://www.scrum.org/resources/blog/done-
understanding-deﬁnition-done, last access August 20,
2021 (2019).

[12] S. Kopczy´nska, M. Ochodek, J. Nawrocki, On impor-
tance of non-functional requirements in agile software
projects—a survey, in: Integrating Research and Prac-
tice in Software Engineering, Springer, 2020, pp. 145–
158.

[13] P. Saddington, Scaling agile product ownership through
team alignment and optimization: A story of epic pro-
portions, Proc. - 2012 Agil. Conf. Agil. 2012 (2012) 123–
130doi:10.1109/Agile.2012.19.

[14] K. Rubin, Essential Scum: A Practical Guide to the
Most Popular Agile Process, Pearson Education, Inc.,
2013.

[15] S.

Povilaitis,

Acceptance

criteria,

https://www.leadingagile.com/2014/09/acceptance-
criteria, last access August 22, 2021 (2014).

[16] S. Viscardi, The professional ScrumMaster’s hand-
book : a collection of tips, tricks, and war stories to
help the professional ScrumMaster break the chains
of traditional organization and management, Packt
Publishing, 2013.
URL
application_development/9781849688024

https://www.packtpub.com/mapt/book/

[17] M. Cohn, Succeeding with agile: software development

using Scrum, Pearson Education, 2010.

[18] A. Silva, E. Dilorenzo, M. Perkusich, D. Santos,
H. Almeida, A. Perkusich, Deﬁnition of Done from
Academy to the Industry: An Exploratory Survey,
Int. J. Eng. Trends Technol. 58 (2). doi:10.14445/
22315381/ijett-v58p214.

[19] N. Davis, Driving quality improvement and reducing
technical debt with the deﬁnition of done, Proc. - Agil.
2013 (2013) 164–168doi:10.1109/AGILE.2013.21.
[20] K. Power, Social contracts, simple rules and self-
organization: A perspective on agile development, in:
Agile Processes in Software Engineering and Extreeme
Programming Conference, Springer, 2014.

[21] M. Taipale, Huitale – a story of a ﬁnnish lean startup,
in: International Conference on Lean Enterprise Soft-
ware Systems, 2010.

[22] Z. Masood, R. Hoda, K. Blincoe, How agile teams make
self-assignment work: a grounded theory study, Empir-
ical Software Engineering 25 (6) (2020) 4962–5005.

[23] R. Kasauli, E. Knauss,

J. Nakatumba-Nabende,
B. Kanagwa, Agile islands in a waterfall environment:
Challenges and strategies in automotive, in: Proceed-
ings of the Evaluation and Assessment in Software En-
gineering, 2020, pp. 31–40.

[24] C. R. Jakobsen, K. A. Johnson, Mature Agile with a
twist of CMMI, in: Agil. 2008. Agil. Conf., IEEE, 2008,
pp. 212–217.

[25] H.

Igaki, N. Fukuyasu, S. Saiki, S. Matsumoto,
S. Kusumoto, Quantitative assessment with using ticket

27

