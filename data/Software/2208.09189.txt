Cross-Domain Evaluation of a Deep
Learning-Based Type Inference System

1st Bernd Gruner
Institute of Data Science
German Aerospace Center
Jena, Germany
bernd.gruner@dlr.de

2nd Tim Sonnekalb
Institute of Data Science
German Aerospace Center
Jena, Germany
tim.sonnekalb@dlr.de

3rd Thomas S. Heinze
Cooperative University
Gera-Eisenach
Gera, Germany
thomas.heinze@dhge.de

4th Clemens-Alexander Brust
Institute of Data Science
German Aerospace Center
Jena, Germany
clemens-alexander.brust@dlr.de

2
2
0
2

g
u
A
9
1

]
E
S
.
s
c
[

1
v
9
8
1
9
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Optional type annotations allow for enriching dy-
namic programming languages with static typing features like
better Integrated Development Environment (IDE) support, more
precise program analysis, and early detection and prevention
of type-related runtime errors. Machine learning-based type
inference promises interesting results for automating this task.
However, the practical usage of such systems depends on their
ability to generalize across different domains, as they are often
applied outside their training domain.

In this work, we investigate the generalization ability of
Type4Py as a representative for state-of-the-art deep learning-
based type inference systems, by conducting extensive cross-
domain experiments. Thereby, we address the following prob-
lems: dataset shifts, out-of-vocabulary words, unknown classes,
and rare classes.

To perform such experiments, we use the datasets Many-
Types4Py and CrossDomainTypes4Py. The latter we introduce
in this paper. Our dataset has over 1,000,000 type annotations
and enables cross-domain evaluation of type inference systems in
different domains of software projects using data from the two
domains web development and scientiﬁc calculation.

Through our experiments, we detect shifts in the dataset
and that it has a long-tailed distribution with many rare and
unknown data types which decreases the performance of the deep
learning-based type inference system drastically. In this context,
we test unsupervised domain adaptation methods and ﬁne-tuning
to overcome the issues. Moreover, we investigate the impact of
out-of-vocabulary words.

Index Terms—type inference, dataset, cross-domain, python,
long-tailed, out-of-vocabulary, repository mining, deep learning

I. INTRODUCTION
Dynamically typed programming languages allow the
annotation of optional data types by language extensions,
like Python with PEP 484 [1],
to compensate for their
shortcomings [2], [3]. While various static and dynamic type
inference approaches exist, they either suffer from imprecision
due to applied abstraction or missing coverage [4]. Recent
machine learning-based approaches try to mitigate these
issues and provide promising results [5]–[8]. For the practical
usage of such systems, it is important to investigate whether
the prediction performance remains stable if the systems are
applied to data from other domains than the training data.

In this context, problems can occur connected to dataset
shifts, out-of-vocabulary words, unknown classes, and rare
classes. In this paper, we investigate these problems and their

implications by performing cross-domain experiments with
a state-of-the-art type inference system, called Type4Py [5].
For our experiments, we use the benchmark dataset Many-
Types4Py [9] and our new CrossDomainTypes4Py dataset,
which we present in this paper. Our dataset covers the two dis-
tinct, but most widely used code domains of web development
and scientiﬁc calculation according to the Python Developers
Survey [10]. It allows us to examine the differences between
the domains and how they affect the performance of type
inference systems.

Through an exploratory analysis of the data, we experience
how the usage of data types differs across domains. In com-
bination with this, we have a closer look at unknown and
rare data types, which are typically hard to predict for deep
learning-based systems [11].

Moreover different kinds of shifts are introduced to our
dataset caused by the domain-speciﬁc selection of the projects
[12]. These shifts harm the performance of the system. We
analyze them in order to mitigate them by transfer learning
methods. In addition, we study the out-of-vocabulary-words
problem, which is a general issue when using source code as
input for machine learning systems [13].

In summary, we contribute the following:
CrossDomainTypes4Py Dataset

Our dataset is publicly available1 and contains 7,912
repositories from the scientiﬁc calculation and the
web domain with 682,354 and 341,029 type anno-
tations, respectively. For the preprocessed version of
the data, we removed duplicate repositories and ﬁles.
We split the remaining data into training, validation
& test, extract relevant information and prepare them
as input for the Type4Py system (see Section IV-C).

Cross-domain Experiments with Type4Py

We perform extensive cross-domain experiments
type inference system
with the state-of-the-art
Type4Py and provide a detailed evaluation (see Sec-
tion V). We investigate how well the system can
generalize across domains, which problems occur,
what has to be considered, and possible ways to
mitigate these issues.

1https://doi.org/10.5281/zenodo.5747024

 
 
 
 
 
 
In order to ensure the reproducibility of our experiments,
we make our mining and preprocessing scripts to create
the dataset avaiable2, as well as our experimental pipeline.
Furthermore, we provide a repository list of our dataset and
a ready-to-use version of the preprocessed data.

The paper is structured as described in the following para-
graph. In Section II, a literature review on deep learning-
based type inference systems and existing datasets is given.
This is followed by Section III, which covers the basics of
dataset shift and the Type4Py method. Section IV describes
the creation of the dataset including the preprocessing steps.
We use Section V to present our research questions, evaluate
the experiments and afterward answer the research questions.
In the succeeding Section VI, the limitations of our approach
are described. Finally, a summary with an outlook is given in
Section VII.

II. RELATED WORK

The ﬁrst part of the section contains an overview of deep
learning-based type inference systems. In the second part,
available datasets for deep learning-based type inference are
presented.

A. Deep Learning-based Type Inference Systems

The majority of publications in the area of deep learning-
based type inference address the programming languages
JavaScript/TypeScript [6], [14]–[17] and Python. In this study,
we focus on the latter.

One of the ﬁrst deep learning-based type inference systems
for Python is DLType [18], which is similar to the approach
presented in [15]. DLType additionally uses natural language
elements of the code, like comments and identiﬁer names to
make type prediction more accurate. The network architecture
is based on a Recurrent Neural Network (RNN). However, it
can only predict the 1000 most frequent data types. Another
method is PyInfer [19], which uses additional code context
and Byte Pair Encoding (BPE) [20]. The latter helps mitigate
the out-of-vocabulary (OOV) problem. Again, the number of
predictable data types is limited to 500. A further improvement
in classiﬁcation accuracy is achieved in the TypeWriter [7]
approach by combining a probabilistic guessing component
and a type checker that veriﬁes the proposed annotations. The
method is limited to the 1000 most frequent data types. Typilus
[8] addresses this problem, through the use of deep similarity
learning, which makes it possible to predict user-deﬁned and
rare data types that occur in the training data. For feature
generation, a Graph Convolutional Neural Network (GCNN)
is used. A similar approach is presented in [21], where a
combination of Graph Neural Network (GNN) and FastText
[22] embeddings is investigated. For the processing of the
features, a Text Convolutional Network is applied.

The Type4Py [5] method uses hierarchical Long Short-
feature extraction

Term Memories (LSTM) networks for

2https://gitlab.com/dlr-dw/type-inference

in combination with a deep similarity learning approach.
Thus, all data types seen in the training can be predicted,
similar to [8]. Another method is HiTyper [23], which uses a
staged approach of static inference and deep neural network
prediction. The two approaches are used alternately and
complement each other. This method mitigates the problem of
the ﬁxed type set, which all methods discussed above suffer
from, by using a user-deﬁned type correction algorithm. The
consequence is that only data types that have already been
seen in the training set can be predicted by a neural network.

None of the previously mentioned papers conducts a cross-
domain evaluation or investigates the OOV problem and its
effects on the results of the system. Such studies are relevant
to examine the performance of the systems when using them
outside of their trained domain, which is the case in practical
applications. A related method [24] transfers the knowledge
of a type inference system across programming languages, but
the authors of this paper are faced with fundamental problems
caused by the difference in programming languages.

To the best of the authors’ knowledge, no previous study
conducts a cross-domain evaluation of deep learning-based
type inference systems and investigates the corresponding
problems. We choose the Type4Py approach for our inves-
tigations because its source code is available and according to
the evaluation by Mir et al. [5], it outperforms other state-of-
the-art deep learning-based type inference systems.

B. Datasets

There are already some extensive Python corpora [25]–
[27]. However, these were not created speciﬁcally for type
inference and thus no focus was placed on whether the
projects had type annotations. These are needed as ground
truth data for supervised learning and evaluating the systems.
Many projects do not have type annotations and are therefore
unsuitable for this task.

The authors of machine learning-based type inference meth-
ods for Python usually present
their own datasets. These
datasets have the following downsides: only partly publicly
available [7], not very comprehensive [8], [21] and partly
designed for special preprocessing steps [8], [18], [21]. An
example that does not come with these downsides is the large
and publicly available ManyTypes4Py dataset [9]. However,
for a cross-domain evaluation, several datasets containing
various domains are required. Therefore, we present Cross-
DomainTypes4Py with two subsets from different domains.

III. THEORETICAL BACKGROUND

In the ﬁrst section, we explain the dataset shift and its
important sub-types. In the following part, we describe the
structure and operation of the Type4Py system.

A. Dataset Shift

Dataset shift

topic in machine learning
since many real-world applications are affected by shifts and

is an important

TABLE I
KEY CHARACTERISTICS FROM THE CROSSDOMAINTYPES4PY DATASET
ARE DISPLAYED IN THIS TABLE, BROKEN DOWN BY DOMAIN. HERE CAL
STANDS FOR THE SCIENTIFIC CALCULATION SUBSET.

Metrics

Total

Cal

Web

Repositories
Total Files
Python Files
Files after Deduplication

7,912
8,580,167
2,791,989
636,516

4,783
6,103,661
2,111,694
470,011

3,129
2,476,506
680,295
166,505

this harms the performance of the systems [28], [29]. For
a common understanding, we will brieﬂy explain the most
important terms here.

According to [12] a dataset shift appears when training and
test joint distributions are different, which can be deﬁned as
follows:

Ptrain(y, x) (cid:54)= Ptest(y, x),

(1)

where x is a set of features or covariates, y is a target variable,
and P (y, x) is a joint distribution. The dataset shift is very
general and includes all possible changes between training
and test distribution. This can be the case, for instance,
when training and test data come from different datasets
or domains. The dataset shift can be divided into different
types and two relevant for this work will be presented in the
following.

The covariate shift is one subtype, where the distribution
of the input changes between the training and the test set, but
the underlying relationship between input and output stays the
same. It is deﬁned as follows:

Ptrain(y|x) = Ptest(y|x), Ptrain(x) (cid:54)= Ptest(x).

(2)

Another subtype is the prior probability shift, which is a
change in the distribution of the class variable. This is shown
in the following:

Ptrain(x|y) = Ptest(x|y), Ptrain(y) (cid:54)= Ptest(y).

(3)

The root causes of dataset shifts are selection biases and
non-stationary environments. The latter refers to environments
that are non-stationary in time or space. For instance, if there
are seasonal changes in remote sensing data between training
and test set.

The selection bias is a systematic ﬂaw in the data collection
process and comes due to a biased method for sample selec-
tion. Thus, the data do not reliably represent the operating
environment of the classiﬁer. This is for example the case
when a class is sampled at a lower rate than it actually appears
to save resources.

B. Type Inference System

We use the Type4Py system as the basis for our investi-
gation. In this section we provide a brief overview, for more
detailed information please refer to Mir et al. [5].

During the preprocessing, the Python source code ﬁles are
used to generate an Abstract Syntax Tree (AST). Based on

this, so-called type hints are extracted and used as input for
the model, which consists of two Long Short-Term Memories
(LSTM) and a dense layer. The ﬁrst type hint is the name of
the variable. Furthermore, the second type hint is obtained
from the code context of the variable. For the third type
hint (visible type hint) the data types present in the source
code ﬁle are analyzed and encoded into a vector. Only the
1024 most frequent data types given in the training data
set are considered. The ﬁrst
two type hints are encoded
using Word2Vec [30], which is a static embedding learned
on the training data. A drawback of this method is that only
words which are present in the training set can be embedded.
We investigate the impact of the out-of-vocabulary words in
Section V-B.

Afterward, the embedded vectors are taken as input for
two separate LSTMs and the output is concatenated with the
visible type hints. Next, the feature vector is processed by a
fully connected layer and then used for a k-nearest neighbor
search [31] in the type cluster. Thus, it is possible to predict
all data types from the training dataset. To train the system,
deep similarity learning is performed using a triplet loss [32]
function L deﬁned as follows:

L(ta, tp, tn) = max(0, m + ||ta − tp|| − ||ta − tn||)

(4)

with a positive scalar margin m. To measure the distances
between the samples the Euclidean metric is used. The goal
of L is to move similar samples closer together (ta & tp) and
different samples further apart (ta & tn) in the cluster.

IV. CROSSDOMAINTYPES4PY

This section addresses the creation of our dataset and used
methods. The ﬁrst part explains how we select our dataset
domains and ﬁnd corresponding repositories on the platforms
GitHub3 and Libraries4. Afterward, we discuss the applied
preprocessing steps.

A. Domain Selection

Code domains can be deﬁned at varying granularity, for
example, projects, developers, categories of the software (e.g.
embedded, web, scientiﬁc calculation), companies, etc. For
our dataset, we focus on the category of software (application
areas) as domain, since we expect differences between code
from different application domains with respect to structure,
programming patterns, used libraries and also data types.
Additionally, there is sufﬁcient data available in public repos-
itories to train and test a machine learning-based system (see
Table I).

The domains are chosen based on a survey with more
than 23,000 Python developers and enthusiasts conducted by
JetBrains and the Python Software Foundation [10]. According
to this, Python is most commonly used for web development
and data analysis. The most utilized libraries in these domains
are Flask (web framework) [33] and NumPy (fundamental

3https://github.com
4https://libraries.io

TABLE II
TWO EXAMPLES OF THE DATASET LIST

URL

Hash

https://github.com/arXiv/arxiv-base.git
https://github.com/Double327/CDCSonCNN.git

b20db1f41731f841106a0b53fb64fc3faa056b4f
77d28b074d67e9f96ffdfcb94e24762fbe749457

package for scientiﬁc computing) [34], respectively. Hence,
for our research, we select the web domain (web) with the
library Flask and the library NumPy which is generally used
for the domain of scientiﬁc calculation (cal). These libraries
are used to ﬁnd dependent repositories which belong to one
of those domains (see Section IV-B).

We publicly provide the scripts and tools to generate
domain-speciﬁc datasets to foster research and researchers for
other domains besides the two domains investigated in this
paper.

B. Mining Repositories

For mining the repositories, we choose the platforms GitHub
and Libraries, on which we search for repositories that depend
on the static type checking tool Mypy5. The intention is to
ensure that optional type annotations are present in at least
a part of the repository (see Section VI). We extend this
procedure and check also for dependencies to the libraries
Flask and NumPy, in order to be able to assign the repository
to a domain.

Since the platforms do not support searching for multiple
dependencies at the same time, so we utilize the method ex-
plained in the following paragraph. First, we search separately
for repositories with dependencies to the three frameworks.
For mining the platform Libraries, we consume its API6 and
query the frameworks separately.

The GitHub API offers no suitable way to query for
dependent repositories. Hence, we use web scraping to extract
the dependency graph from the website7. The queries for
both platforms are automatically executed and the resulting
repositories are stored in temporary lists. We limit the search
to 50,000 repositories per framework (see Section VI). Af-
terward, these lists are sorted by repository stars and can be
ﬁltered if required. The stars are an indicator of popularity and
can reﬂect a tendency about the quality of the repository [35],
[36].

The temporary lists from both platforms are merged and
then used to determine intersections between NumPy & Mypy
and Flask & Mypy. If repositories have dependencies on all
three frameworks, they are included in both subsets, because
they are removed during the preprocessing depending on the
task (see Section IV-C). The resulting lists are the basis of the
dataset.

The published dataset includes the links to the repositories
and a commit hash in order to keep the dataset reproducible.
Two example entries are shown in Table II.

C. Preprocessing Steps

In this section, we discuss necessary preprocessing steps
to make the dataset usable for the Type4Py type inference
system. We use the ManyTypes4Py8 pipeline as a base and
adapt it where necessary for our cross-domain setup.

1) Deduplication: An essential preprocessing step is to
remove duplicates from the dataset, as this harms the per-
formance of machine learning systems [37]. In particular, for
our cross-domain setup, we have to control code duplicates
additionally across the datasets. In the ﬁrst step, we create
a list of repositories, which are present in both datasets and
randomly remove one half from one dataset and the other half
from the other dataset. The resulting repository lists of the
datasets are disjoint.

In the second step, we apply the tool CD4Py9 to detect ﬁle-
level duplicates. This tool is also used in the ManyTypes4Py
pipeline. It creates a vector representation using the Term
Frequency-Inverse Document Frequency (TF-IDF) method to
convert the tokenized identiﬁers of the source code ﬁles. The
outputs are clusters of duplicates by performing a k-nearest
neighbor search. From each cluster, we randomly select one
ﬁle to remain in the dataset, all others are deleted.

2) Dataset Split: The two subsets are randomly split into
training, validation and testing with 70, 10 and 20 percent,
respectively. We deviate at this point from the ManyTypes4Py
approach and split on project-level rather than on ﬁle-level.
In the area of type inference splitting on ﬁle-level is widely
used [5], [7], [18], but projects may be split into training
and test set. This can lead to leakage of information into
the test set, also known as group leakage [38]. Furthermore,
by splitting on ﬁle-level, project-speciﬁc data types can be
distributed across training and test set, resulting in a higher
number of predictable data types, which is not the case in a
realistic scenario.

These two problems lead to an overestimation of the
performance of the system when the goal
is to perform
cross-project or more general cross-domain prediction and
therefore we conduct the split on project-level.

3) Feature Extraction: For

further preprocessing we
take advantage of the LibSA4Py library10. It parses the
source code and extracts features of interest for machine
learning-based type inference systems. The extracted ﬁelds

5https://github.com/python/mypy
6https://libraries.io/api/pypi/(cid:104)Framework(cid:105)/dependent repositories
7https://github.com/(cid:104)Username(cid:105)/(cid:104)Framework(cid:105)/network/dependents

8https://github.com/saltudelft/many-types-4-py-dataset
9https://github.com/saltudelft/CD4Py
10https://github.com/saltudelft/libsa4py

TABLE III
THE FIELDS OF THE JSON-FILE EXTRACTED BY THE LIBSA4PY LIBRARY.

Name of the ﬁeld
Project-Object
author & repository
src ﬁles
ﬁle path
Module-Object
untyped seq
typed seq
imports
variables
classes
funcs
set
Class-Object
name
variables
funcs
Function-Object
name
params
ret exprs
ret type
variables
params occur
docstring
docstring.func
docstring.ret
docstring.long descr

Description

author and project name on GitHub
path of the project’s source code ﬁles
path of the source code ﬁle

normalized seq2seq representation
type of identiﬁers in untyped seq
name of imports
name and type of variables
classes of the module (JSON class object)
functions of the module (JSON func object)
set of the ﬁle (train, valid, test)

class name
class variables and corresponding type
functions of the class (JSON func object)

function name
parameter name and corresponding type
return expression
return type
local variables and corresponding type
parameters and their usage in the function
docstring (with the following three subﬁelds)
one-line function description
description of what the function returns
long description

and a corresponding description are given in Table III. For
more detailed information we refer to [9].

4) Feature Preparation: For the preparation of the fea-
tures, we follow previous works [5], [7], [8]. We remove
trivial functions like __len__ with straightforward return
types. Furthermore, we exclude the data types Any and
None because they are not helpful to predict. Moreover, we
resolved type aliasing to make the same data types con-
sistent, for example, [] to List. In order to reduce the
number of different data types, we make a simpliﬁcation and
limit the nested level of data types to two, as Type4Py [5]
does. For example List[List[Set[int]]] is rewritten
to List[List[Any]]. In general, we use fully qualiﬁed
names for our type annotations to make them consistent across
the dataset.

In Table IV the ﬁnal amount of samples in all datasets are
shown. We see that number of samples of the ManyTypes4Py
dataset is in between our two domains web and scientiﬁc
calculation.

V. EXPERIMENTAL RESULTS AND EVALUATION
This section starts with details about the experiment setup
and a description of the evaluation process. In the following
our research questions will be motivated, raised, answered, and
discussed:

1) Are there differences in the distribution of data types

between the domains?

2) Are the results in the cross-domain setup comparable to
the results when training on the corresponding software
domain?

3) How do the results change when the evaluation is

performed only on predictable data types?

4) What effect does the frequency of the data types have

on the results?

5) What is the impact of the out-of-vocabulary problem on

system performance?

6) Is there a dataset shift and how can it be mitigated?

A. Experiment and Evaluation Setup

To perform the experiments, we take the available
implementation of Type4Py as a template and extend it
to our cross-domain setup. We utilize Python 3.6 and the
deep learning framework PyTorch. In order to determine the
hyperparameters, we conduct a grid search and reuse the
conﬁguration for all experiments. We train for 30 epochs and
use adam as an optimizer with a learning rate of 0.002 and a
batch size of 2,536. The complete conﬁguration can be found
in our public repository. For the experiments, an NVIDIA
Tesla V100 GPU and an Intel Xeon Platinum 8260 are used.

To perform cross-domain experiments, we train the system
on one domain and evaluate it on another domain. To have
a comparison of what results can ideally be achieved, we
perform a second experiment using only the latter domain,
both for training and evaluation. Our two main setups are:

1) Setup: Web2Cal

a) Training on web domain and evaluation on scien-

tiﬁc calculation domain (cal)

b) Training and evaluation on scientiﬁc calculation

domain

2) Setup: M4p2Cal

a) Training on ManyTypes4Py (m4p) and evaluation

on scientiﬁc calculation domain

b) Training and evaluation on scientiﬁc calculation

domain

the second setup M4p2Cal

Setup 1.b and 2.b use slightly different datasets because of
the deduplication step in the preprocessing (see Section IV-C).
The ﬁrst setup Web2Cal investigates the generalizability of
the system from one software domain to another unseen
is expected
one. In contrast,
to be an easier task, as the ManyTypes4Py dataset which
contains various domains, is used for training and a speciﬁc
domain for evaluation. This also corresponds to a realistic
the system should be
application scenario. For example,
used in a company with different departments working in
various ﬁelds. They likely use a pretrained system that is not
ﬁne-tuned for the speciﬁc ﬁelds of the departments. Hence, it
is interesting to know for the company how well the system
can generalize and what performance could be expected.

For the evaluation,

the top-1 F1-score weighted by the
number of samples is used. Thus, the inﬂuence of the more
frequent data types on the result is magniﬁed.

All experiments are executed three times to enable a useful
signiﬁcance test and conﬁrm the soundness of our results. To

TABLE IV
A DETAILED OVERVIEW OVER THE CHARACTERISTICS OF ALL DATASETS

Characteristics

Samples
Common Samples
Rare Samples

Unique Types
Common Types
Rare Types

Web

All

341,029
240,074
100,955

15,177
242
14,935

Scientiﬁc Calculation

ManyTypes4Py

Train

Val

Test

All

Train

Val

Test

All

Train

Val

Test

251,064
179,877
71,187

7,588
232
7,356

27,987
20,639
7,348

1,195
158
1,037

61,978
39,558
22,420

8,475
192
8,283

682,354
493,813
188,541

27,611
381
27,230

476,768
347,520
129,248

14,973
363
14,610

56,854
42,786
14,068

2218
252
1966

148,732
103,507
45,225

14,960
332
14,628

532,522
363,553
168,969

24,565
302
24,263

398,152
274,200
123,952

13,803
286
13,517

46,577
34,420
12,157

1,820
168
1,652

87,793
54,933
32,860

11,271
236
11,035

determine whether two results differ signiﬁcantly, the student’s
t-test [39] with a p-value threshold of 0.05 is used. In the
tables, the mean and standard deviation of the results in percent
are reported.

B. Results and Research Questions

RQ 1: Are there differences in the distribution of data

types between the domains?

In order to answer the research question, we analyze the
class distributions of the datasets. Figure 1 shows the ten most
frequent data types from the web and scientiﬁc calculation set.
The three most common data types are built-in data types and
are equal for both subsets. As expected, it is noticeable that
data types needed for calculations such as bool, int, ﬂoat and
numpy.ndarray occur much more frequently in the scientiﬁc
calculation subset. In the web subset, on the other hand, the
data types string, Optional[str] and dict are used more often.
We can see the different usage of the data types as well
when we compare the list of visible type hints containing the
1,024 most frequent data types from the different domains (see
Section III-B). For example, ManyTypes4Py and the scientiﬁc
calculation domain share only 502 out of 1,024 data types.

When considering the whole datasets, we see for instance
that
the web and scientiﬁc calculation domain share only
3,755 classes out of 15,177 and 27,611, respectively (see
Table IV). The reason for this is the long-tailed data type
distribution with a lot of less frequent data types which are
likely to be project- or domain-speciﬁc. The aforementioned
issue can be observed in the M4p2Cal setup between
ManyTypes4Py and the scientiﬁc calculation set as well.

We can conclude from our ﬁndings that the data types are
used differently across the domains and that the distribution of
the data types differs. This is referred to as prior probability
shift (see Section III-A). In our next research question, we
investigate if these differences between the domains decrease
the performance of the type inference system.

Answer to RQ 1: Yes, there is a clear difference in the
distribution of data types. We present them qualitatively
and quantitatively.

Fig. 1. The chart shows the ten most common data types from the web and
the scientiﬁc calculation domain with their frequency. The trivial data types
None and Any are omitted, because they are not predicted later by the type
inference systems.

RQ 2: Are the results in the cross-domain setup com-
parable to the results when training on the corresponding
software domain?

We use our setups deﬁned in the Section V-A to answer this
question. The results of the experiments are shown in Table
V. Using the Web2Cal setup 1.a, we measured an F1-score
of 49.06% in comparison to setup 1.b with an F1-score of
55.27%, which is signiﬁcantly higher. Consequently,
the
system has problems generalizing from one speciﬁc domain
to another.

A more realistic scenario is addressed by our second setup
M4p2Cal. We expect a better generalization ability due to the
domain diversity in the ManyTypes4Py dataset. However, the
results in Table V do not conﬁrm our assumption. Setup 2.a
achieves an F1-score of 45.19% and, in comparison, setup
2.b achieves 59.34%. We observe signiﬁcantly worse results
when the system is used on a domain on which it is not
trained. We assume the problems are due to a prior probability
shift, which is introduced by our domain-speciﬁc datasets.
When using the system on another than the training domain
a decreased performance must be expected. In the following
research questions, we analyze the problem in more detail.

Answer to RQ 2: No, when evaluating on another domain,
the F1-score decreases by up to 14.15 percentage points
compared to training on the corresponding domain.

05101520FrequencyoftheDataTypesperDataset[%]bytesOptional[int]Dict[str,Any]List[str]dictnumpy.ndarrayOptional[str]ﬂoatboolintstrDataTypesScientiﬁcCalculationWebTABLE V
THE RESULTS OF THE CROSS-DOMAIN EXPERIMENTS WITH BOTH SETUPS
ARE SHOWN. THE MEAN OF THE F1-SCORE AND THE CORRESPONDING
STANDARD DEVIATION IN PERCENT ARE REPORTED.

types for the web and scientiﬁc calculation set can be seen in
Figure 1. Rare data types are mostly user-deﬁned or nested
data types, which are application-speciﬁc.

Eval Set

Cal

Train Set

Setup 1

Setup 2

Web
Cal
M4p
Cal

All Types
49.06 ± 0,13
55.27 ± 0,07
45.19 ± 0,01
59.34 ± 0,06

Predictable Types
66.05 ± 0.17
69.98 ± 0,11
62.29 ± 0.02
72.97 ± 0,13

RQ 3: How do the results change when the evaluation

is performed only on predictable data types?

This question is motivated by an analysis of the test sets.
We found that in the second setup M4p2Cal about 88 percent
of the data types in the scientiﬁc calculation test set cannot
be predicted because they are not in the training set (see
Table IV). Thus, about 27 percent of the samples cannot
be predicted at all, which affects the performance of the
system. The same patterns are conﬁrmed in our ﬁrst setup
Web2Cal. Furthermore, we found that this is also the case
within datasets, for example in the ManyTypes4Py dataset
only 1,801 out of 11,271 data types from the test set can be
predicted (see Table VIII).

For our next experiment, we removed the unpredictable data
types from the test set. This allows us to assess their inﬂuence
on the result. In M4p2Cal setup 2.a the F1-score increases by
16.99 percentage points to 66.05%, as well as in setup 2.b,
where it increases by 14.71 percentage points to 69.98% F1-
score (see Table V). We observe similar results in our Web2Cal
setup 1.a and 1.b. Thus, we conclude that the unpredictable
data types have a great impact on the results and the problem
should be addressed. As possible solutions, we propose to use
methods from zero-shot learning [40] or novelty detection [41]
and consider the human-in-the-loop for a life-long learning
process [42].

At

the same time, we note that

the unpredictable data
types do not fully explain the gap between the cross-domain
evaluation (setup a) and the training on the corresponding
domain (setup b).

Answer to RQ 3: By removing unpredictable data types,
the results of the system could improve by an F1-score of
up to 16.99 percentage points.

Table IV provide evidence for the long-tailed distribution
of the data types. For instance, the scientiﬁc calculation test
set consists of 332 common and 14,960 rare data types. This
is called a long-tailed distribution [43]. It can be assumed
that common data types are predicted much better than rare
data types because they have much more examples available
for learning.

For our experiment, keep all data types in the training set
and evaluate the experiment from RQ 3 according to common
and rare data types, illustrated in Table VI.

The rare data types can be predicted signiﬁcantly worse than
the common data types for both setups. If we then evaluate
only the predictable data types, we see that only the result
of the rare data types improves signiﬁcantly, since the non-
predictable data types consist of 99 percent rare data types.
The results of the common data types stay the same because
they consist mostly of predictable data types. Nevertheless, the
performance of the system is still much better on the common
than on the rare data types.

d e f

i n i t

( s e l f ,

t a b l e s : {1} = None ,

a l l
t a b l e s w i t h s t r i n g s : {2} = None ,
d a t a b a s e d i r e c t o r y : {3} = None ) :

t a b l e s = a l l

s e l f . a l l
s e l f . t a b l e s w i t h s t r i n g s = t a b l e s w i t h s t r i n g s
i f d a t a b a s e d i r e c t o r y :

t a b l e s

s e l f . d a t a b a s e d i r e c t o r y = d a t a b a s e d i r e c t o r y
s e l f . c o n n e c t i o n = s q l i t e 3 . c o n n e c t ( d a t a b a s e d i r e c t o r y )
s e l f . c u r s o r = s e l f . c o n n e c t i o n . c u r s o r ( )
s e l f . g r a m m a r s t r = s e l f . i n i t i a l i z e g r a m m a r
s e l f . grammar = Grammar ( s e l f . g r a m m a r s t r )
s e l f . v a l i d a c t i o n s = s e l f . i n i t i a l i z e v a l i d a c t i o n s ( )

s t r ( )

Listing 1. Example method from the ManyTypes4Py dataset

Ground truth label and prediction:

1) Label: Dict[str, List[str]]

Prediction: List[str]

2) Label: Dict[str, List[str]]
Prediction: Optional[str]

3) Label: str

Prediction: str

RQ 4: What effect does the frequency of the data types

have on the results?

We have divided the data types into two groups based on
their frequency to address this question. The ﬁrst group we
call common data types. It contains data types that occur at
least 100 times in the dataset. All others belong to the group
of rare data types. This is done separately for every dataset.
If we consider the distribution of common and rare data types
in the datasets, we notice that there are few common data
types with many examples and a lot of rare data types with
few examples (see Table IV). Examples of common data

In Listing 1, we see an example function from the Many-
Types4Py dataset. For simpliﬁcation, we report only the three
arguments of the function predicted by the Type4Py system.
In this qualitative example, it is easy for the system to predict
the common type string but complicated to predict nested data
types. This is in line with our quantitative results.

We summarize that it is important to work on the problem
with the rare data types to achieve better results with the
system and that the gap in the results between setup a and
setup b is independent of the data type occurrence frequency.

TABLE VI
A DETAILED EVALUATION OF THE CROSS-DOMAIN EXPERIMENTS FOR BOTH SETUPS IS SHOWN. THE MEAN OF THE F1-SCORE AND THE
CORRESPONDING STANDARD DEVIATION IN PERCENT ARE REPORTED.

Eval Set

Train Set

Setup 1

Setup 2

Web
Cal
M4p
Cal

Cal

All Types
Common
75.46 ± 0,04
80.25 ± 0,02
73.23 ± 0,01
82.55 ± 0,04

Rare
12.35 ± 0,18
22.66 ± 0,11
8.32 ± 0,02
31.40 ± 0,06

Predictable Types
Common
75.46 ± 0,04
80.29 ± 0,03
73.23 ± 0,01
82.55 ± 0,05

Rare
45.52 ± 0,21
48.80 ± 0,13
33.92 ± 0,03
55.02 ± 0,15

Answer to RQ 4: Data types that occur more than 100 times
in the data set can be predicted up to an F1-score of 64,91
percentage points better than data types that occur less
frequently. When removing the unpredictable data types,
this is still the case but the gap between the common and
rare data types is smaller.

TABLE VII
RESULT OF THE OUT-OF-VOCABULARY EXPERIMENT, WHERE WE USE
DIFFERENT SETS TO TRAIN THE WORD2VEC (W2V) MODEL AND REPORT
THE CORRESPONDING F1-SCORE IN PERCENT AFTER TRAINING THE
TYPE4PY SYSTEM WITH THE DIFFERENT EMBEDDED DATASETS. IN
ADDITION, WE REPORT THE PERCENTAGE OF OUT-OF-VOCABULARY
WORDS (OOV).

RQ 5: What is the impact of the out-of-vocabulary

problem on system performance?
When embedding source code,

the out-of-vocabulary
problem plays a major role in many software engineering
tasks [13], [44] since user-deﬁned data types, identiﬁers, and
method names make the vocabulary practically inﬁnite. The
embedding method Word2Vec, which is used in the Type4Py
system, cannot embed unknown words. Thus, vocabulary
that does not appear in the training set of the Word2Vec
model
is not embedded and the information is lost. We
assume that in our cross-domain setup this effect is ampliﬁed,
since domain-speciﬁc vocabulary may be used in the domains.

For our experiments, we create three Word2Vec models for
each setup, trained with different data. For the ﬁrst model, we
use in setup Web2Cal the training set from the web domain
and in setup M4p2Cal the training set from ManyTypes4Py.
The second Word2Vec model is trained with the training sets
of both domains, which are web and scientiﬁc calculation
for the ﬁrst setup Web2Cal and ManyTypes4Py and scientiﬁc
calculation for the second setup M4p2Cal. In order to train
the third model, we do not only use the training sets like in
model 2, we utilize all data from both domains.

In the evaluation, we see that in a realistic scenario where
we train the embedding only on the training set of one domain,
there are 7.6% in setup Web2Cal and 5.6% in setup M4p2Cal
unknown words on the other domain (see Table VII). This
is more than double the number of words that cannot be
embedded than in the domain Word2Vec is trained. In the
conﬁguration where we train on the training data of both
domains, the percentage of unembeddable words decreased
signiﬁcantly and has leveled off for both domains. In the last
conﬁguration, it drops even further. However, there remain
some unknown words, because words that occur less than three
times in the dataset are excluded from the Word2Vec training.
We use different Word2Vec models, trained on different
sets of data, to embed the vectors for the Type4Py system
and found that the results of the system are not inﬂuenced
signiﬁcantly. When we evaluate according to common and rare

W2V
Train Data

Setup 1

Setup 2

OOV

F1-score

OOV

F1-score

Source Train Set
Both Train Sets
All Sets

7.6
1.8
0.9

48.57 ± 0.08
49.06 ± 0.13
49.25 ± 0.04

5.6
1.3
0.8

44.88 ± 0.05
45.19 ± 0.01
45.22 ± 0.08

data types there is also no difference in the results. Thus we
can say that the important information is not stored in the
domain-speciﬁc vocabulary and in general it is not necessary
to further investigate or mitigate this issue.

Answer to RQ 5: We have discovered that in the cross-
domain setup there are signiﬁcantly more words that cannot
be embedded. However, this has no signiﬁcant effect on the
performance of the system.

RQ 6: Is there a dataset shift and how can it be

mitigated?

In connection with this research question, we investigate the

prior probability and covariate shift.

In our ﬁrst experiment, we examine the inﬂuence of the
visible type hints, which are explained in Section III-B. We
compare the visible type hints between ManyTypes4Py and the
scientiﬁc calculation set. We determine that they share only
502 out of 1024 data types and so they are domain-speciﬁc.
We expect an inﬂuence on the results of the system when using
the visible type hints from another domain but the results do
not signiﬁcantly differ from each other in both setups.

In Table IV and Table VIII, we see that ManyTypes4Py
has 11 percent less data types at all but 71.1 percent less
predictable data types inside the dataset compared to the
scientiﬁc calculation domain. Considering M4p2Cal setup
2.a, it is evident that there are more predictable data types in
the scientiﬁc calculation test set than in the ManyTypes4Py
test set. The reason for this could be the larger size of the
scientiﬁc calculation test set. We can conclude based on our
insights from RQ1 and the visible type hints, that there is
also a prior probability shift.

TABLE VIII
THE TABLE SHOWS THE SHARED DATA TYPES BETWEEN SET 1 AND SET 2,
THEIR CORRESPONDING SUPPORT IN THE SETS, AND THE F1-SCORE IN
PERCENT OF A CLASSIFIER, WHICH CLASSIFIES THE FEATURES OF BOTH
SET FROM WHICH SET THEY COME.

Set 1

Set 2

Types

Samples

Common

Rare

Set 1

Set 2

Setup 1

Web-Train Web-Test
Cal-Test
Cal-Train
Cal-Test
Web-Train
Web-Test
Cal-Train

Setup 2

M4p-Train M4p-Test
Cal-Test
Cal-Train
Cal-Test
M4p-Train
M4p-Test
Cal-Train

185
322
198
243

225
398
215
267

1,356
3,273
2,193
1,244

1,576
5,828
2,034
1,425

193,441
384,637
205,712
343,293

287,874
436,554
291,431
370,843

43,688
119,026
110,362
43,738

60,540
132,218
111,993
59,146

F1

0.72
0.62
0.71
0.72

0.70
0.59
0.71
0.73

To gain new insights regarding the covariate shift, we
follow the authors of [45] and [46], which say that
the
features produced by the system should not contain domain-
speciﬁc information. Therefore we learn a simple classiﬁer
to assign the features to a domain and test how accurate the
results are to ﬁnd out if the features are domain invariant.
For our experiments, we use a tree-based classiﬁer [47]
in combination with 6-fold cross-validation. The results in
Table VIII indicate that inside the speciﬁc software domain
of scientiﬁc calculation the features are more alike because
they are more difﬁcult to distinguish. The features inside the
ManyTypes4Py dataset and across the domains are easier
to predict for the classiﬁer and subsequently contain more
information about
their source set or domain from which
they come. According to the results, the performance of the
Type4Py system inside the ManyTypes4Py dataset should be
similar to the cross-domain setup. We can summarize that the
features differ more across domains than inside the scientiﬁc
calculation domain.

In order to mitigate the covariate shift, we evaluate two
popular methods for unsupervised domain adaptation DANN
[48] and WDGRL [49] to align the features of the domains in
the feature space without the need for additional annotations.
We observe that these approaches do not provide better results
(see Figure 2). If we evaluate them according to common and
rare data types, we see that the results of both data type groups
decreased. Furthermore, we test if transfer learning works in
a cross-domain setup and applied ﬁne-tuning. We can achieve
in both setups similar results to the model learned directly
on the corresponding domain. The drawback of this approach
is that we need labeled training data from the destination
domain but in a real-world scenario labeled training data is
often unavailable.

Fig. 2. The chart shows the results of the unsupervised domain adaptation
methods DANN and WDGRL in comparison to the corresponding setup a
(SO) and b (TO).

Answer to RQ 6: We observe a prior probability shift
between the domains. Furthermore, we showed that the fea-
tures across domains differ more than inside the scientiﬁc
calculation domain, which is an indicator for a covariate
shift. If we have labeled data from the destination domain,
we can use ﬁne-tuning to mitigate the issues.

Summary
We experience that when using the Type4Py system on
another than the training domain the results decrease by up
to an F1-score of 14.15 percentage points in comparison to
a training on the corresponding domain. Due to the long-
tailed distribution of the datasets, the classiﬁcation accuracy
of rare data types is signiﬁcantly worse than on common data
types. The high amount of rare data types also cause a lot of
data types that can not be predicted by the system because
they are not present in the training set. They decrease the
performance by an F1-score up to 16.99 percentage points.
Another common issue we investigate is the out-of-vocabulary
problem which is present but has no signiﬁcant inﬂuence on
the results of the system. Finally, we showed that there is a
prior probability shift between the datasets and based on our
feature analysis likely also a covariate shift. To mitigate the
issues we test different transfer learning methods and ﬁnd out
that ﬁne-tuning on the destination domain works best.

VI. LIMITATIONS

Our dataset contains only two domains. However, these have
been systematically identiﬁed through a survey [10] and are
the two largest application domains for Python. By providing
our tools, an easy extension of our dataset is possible.

While mining our dataset, we search for 50,000 repositories
per domain in order to keep the subsets comparable in size
to state-of-the-art datasets, e.g. [9].

Web2CalSetupM4p2CalSetup010203040506070F1-score[%]SODANNWDGRLTOlimit

Our results on the ManyTypes4Py dataset differ from
those presented in the Type4Py paper [5]. However, this does
the outcome of this paper because we compare
not
the results from different setups across domains and do
not aim to improve the results of the Type4Py paper. The
differences in the results are caused by differences in the
preprocessing of ManyTypes4Py, which are described in
the following. Not all repositories on the dataset
list are
still available. Additionally, we have to remove duplicates
across the datasets. Furthermore, the data split into training,
validation, and test is executed on project-level because in a
realistic scenario there will not be half of the project in the
training and the other half of the project in the test set. A ﬁle
level-split lead to leakage of information and project-speciﬁc
data types across the sets, which increased the F1-score in
our experiment by 7 percentage points.

While mining our CrossDomainTypes4Py dataset, we want
to increase the number of repositories, which contain type
annotations by searching for projects that depend on the type
checker Mypy. This biases the sampling of the repositories,
but is an approved method used by ManyTypes4Py [9] and
TypeWriter [7].

In our preprocessing pipeline, we want to maintain compa-
rability to the ManyTypes4Py approach, that is why we use the
LibSA4Py library for information extraction. It is restricted by
its parsing module, which can only handle Python 3, but not
the older version Python 2.

VII. CONCLUSION

We perform the ﬁrst study of cross-domain generalizability
in the ﬁeld of type inference. We enable this by our publicly
available CrossDomainTypes4Py dataset, which consists
of two subsets from the two domains web and scientiﬁc
calculation with in total over 1,000,000 type annotations.

We gain new insights by conducting extensive cross-domain
experiments in various setups. For instance, we observe that
the system performs signiﬁcantly worse when evaluating
on another than the training domain. This is due to the
differences between the domains and datasets. We discover
that they do not share the same data types and also that
the features differ between the domains, which lowers the
accuracy of the system. In our investigations, we also showed
that a large number of out-of-vocabulary words and the
domain-speciﬁc visible type hints have no signiﬁcant impact
on the results of the system. Moreover, due to the long-tailed
distribution of the dataset, there are many rare data types that
the system can only predict with low accuracy.

Based on our ﬁndings, we encourage the user of the type
inference method to consider the practical environment in
which the system is to be deployed. We recommend collecting
labeled data of this domain and using it for ﬁne-tuning the
system.

Another important aspect that we would like to emphasize is
that for a realistic scenario and evaluation of a type inference
system the dataset has to be split into training, validation, and
test on a project-level. Splitting it on ﬁle-level overestimates
the performance of the classiﬁer when we are conducting
cross-project or more general cross-domain evaluation.

Future Work

In this section, we want

to give some suggestions for
the further development and application of our dataset
CrossDomainTypes4Py, as well as possible solutions for the
investigated problems regarding the rare data types, dataset
shifts, out-of-vocabulary words, and unpredictable data types.

From a research perspective, the performance of the un-
predictable data types should be improved by extending the
system to detect them. As a possible solution, we propose
methods from the ﬁeld of novelty detection [41] or zero-shot
learning [40] and consider the human-in-the-loop for a life-
long learning process [42].

To counteract the problem with the rare data types, we
recommend using a resampling method like SMOTE [50] or
using importance weighting for the data types during training.
Another aspect for improvement is to replace the static
embedding with a contextual embedding to capture more
information like TypeBert [6] does.

Furthermore, it is possible to study how the size of the

dataset affects the results of the system.

Besides improving the Type4Py system, it is also possible to
further develop our dataset by adding new domains. Moreover,
the dataset may also be processed further to enable its usage
for related downstream tasks like, e.g. code completion.

In addition, a descriptive empirical analysis of the repos-
itories and the associated artifacts is also possible, e.g. for
empirical analysis of the usage and requirements of type
systems in various application domains [4].

REFERENCES

[1] G. van Rossum, J. Lehtosalo, and Łukasz Langa. (2014) Python
[Online]. Available:

type hints.

developer’s guide: Pep 484 -
https://www.python.org/dev/peps/pep-0484/

[2] S. Hanenberg, S. Kleinschmager, R. Robbes, ´E. Tanter, and A. Steﬁk,
“An empirical study on the impact of static typing on software main-
tainability,” Empirical Software Engineering, vol. 19, pp. 1335–1382,
2013.

[3] Z. Gao, C. Bird, and E. T. Barr, “To type or not to type: Quantifying
detectable bugs in javascript,” in 2017 IEEE/ACM 39th International
Conference on Software Engineering (ICSE), 2017, pp. 758–769.
[4] T. S. Heinze, A. Møller, and F. Strocco, “Type safety analysis
for dart,” in Proceedings of
the 12th Symposium on Dynamic
Languages, ser. DLS 2016. New York, NY, USA: Association
[Online]. Available:
for Computing Machinery, 2016, p. 1–12.
https://doi.org/10.1145/2989225.2989226

[5] A. M. Mir, E. Latoskinas, S. Proksch, and G. Gousios, “Type4py: Deep

similarity learning-based type inference for python,” 2021.

[6] K. Jesse, P. T. Devanbu, and T. Ahmed, “Learning type annotation:
Is big data enough?” in Proceedings of the 29th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, ser. ESEC/FSE 2021. New York,
NY, USA: Association for Computing Machinery, 2021, p. 1483–1486.
[Online]. Available: https://doi.org/10.1145/3468264.3473135

[7] M. Pradel, G. Gousios, J. Liu, and S. Chandra, “Typewriter: Neural type
prediction with search-based validation,” in Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering,
ser. ESEC/FSE 2020. New York, NY, USA: Association for
Computing Machinery, 2020, p. 209–220. [Online]. Available: https:
//doi.org/10.1145/3368089.3409715

[8] M. Allamanis, E. T. Barr, S. Ducousso, and Z. Gao, “Typilus: Neural
type hints,” in Proceedings of the 41st acm sigplan conference on
programming language design and implementation, ser. PLDI 2020.
New York, NY, USA: Association for Computing Machinery, 2020, p.
91–105. [Online]. Available: https://doi.org/10.1145/3385412.3385997

[9] A. M. Mir, E. Latoskinas, and G. Gousios, “Manytypes4py: A bench-
mark python dataset for machine learning-based type inference,” in
IEEE/ACM 18th International Conference on Mining Software Reposi-
tories (MSR).

IEEE Computer Society, May 2021, pp. 585–589.

[10] JetBrains. (2022) Python developers survey 2021 results. [Online].
Available: https://lp.jetbrains.com/python-developers-survey-2021/
[11] K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “Learning imbal-
anced datasets with label-distribution-aware margin loss,” Advances in
neural information processing systems, vol. 32, 2019.

[12] J. G. Moreno-Torres, T. Raeder, R. Ala´ız-Rodr´ıguez, N. Chawla, and
F. Herrera, “A unifying view on dataset shift in classiﬁcation,” Pattern
Recognit., vol. 45, pp. 521–530, 2012.

[13] V. J. Hellendoorn and P. Devanbu, “Are deep neural networks the best
choice for modeling source code?” in Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering, 2017, pp. 763–773.
[14] V. J. Hellendoorn, C. Bird, E. T. Barr, and M. Allamanis, “Deep
learning type inference,” in Proceedings of the 2018 26th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ser. ESEC/FSE 2018.
New York, NY, USA: Association for Computing Machinery, 2018, p.
152–162. [Online]. Available: https://doi.org/10.1145/3236024.3236051
[15] R. S. Malik, J. Patra, and M. Pradel, “Nl2type: Inferring javascript
function types from natural language information,” in 2019 IEEE/ACM
41st International Conference on Software Engineering (ICSE), 2019,
pp. 304–315.

[16] J. Wei, M. Goyal, G. Durrett, and I. Dillig, “Lambdanet: Probabilistic
in International
type
Conference on Learning Representations, 2020. [Online]. Available:
https://openreview.net/forum?id=Hkx6hANtwH

inference using graph neural networks,”

[17] I. V. Pandi, E. T. Barr, A. D. Gordon, and C. Sutton, “Opttyper: Prob-
abilistic type inference by optimising logical and natural constraints,”
2021.

[18] C. Boone, N. de Bruin, A. Langerak, and F. Stelmach, “Dltpy: Deep
learning type inference of python function signatures using natural
language context,” 2019.

[19] S. Cui, G. Zhao, Z. Dai, L. Wang, R. Huang, and J. Huang, “Pyinfer:
Deep learning semantic type inference for python variables,” 2021.
[20] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of
rare words with subword units,” arXiv preprint arXiv:1508.07909, 2015.
[21] V. Ivanov, V. Romanov, and G. Succi, “Predicting type annotations for

python using embeddings from graph neural networks,” 2021.

[22] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word
vectors with subword information,” Transactions of the Association for
Computational Linguistics, vol. 5, pp. 135–146, 2017.

[23] Y. Peng, Z. Li, C. Gao, B. Gao, D. Lo, and M. Lyu, “Hityper: A hybrid

static type inference framework with neural prediction,” 2021.

[24] Z. Li, X. Xie, H. Li, Z. Xu, Y. Li, and Y. Liu, “Cross-lingual adaptation

for type inference,” 2021.

[25] V. Raychev, P. Bielik, and M. Vechev, “Probabilistic model for code
with decision trees,” SIGPLAN Not., vol. 51, no. 10, p. 731–747, oct
2016. [Online]. Available: https://doi.org/10.1145/3022671.2984041
[26] S. Biswas, M. J. Islam, Y. Huang, and H. Rajan, “Boa meets python:
A boa dataset of data science software in python language,” in 2019
IEEE/ACM 16th International Conference on Mining Software Reposi-
tories (MSR), 2019, pp. 577–581.

[27] M. Orr´u, E. Tempero, M. Marchesi, R. Tonelli, and G. Destefanis, “A
curated benchmark collection of python systems for empirical studies on
software engineering,” in Proceedings of the 11th International Confer-
ence on Predictive Models and Data Analytics in Software Engineering,
2015, pp. 1–4.

[28] S. G. Finlayson, A. Subbaswamy, K. Singh, J. Bowers, A. Kupke,
J. Zittrain, I. S. Kohane, and S. Saria, “The clinician and dataset shift in

artiﬁcial intelligence,” The New England journal of medicine, vol. 385,
no. 3, p. 283, 2021.

[29] F. J´a˜nez-Martino, R. Alaiz-Rodr´ıguez, V. Gonz´alez-Castro, E. Fidalgo,
and E. Alegre, “A review of spam email detection: analysis of spammer
strategies and the dataset shift problem,” Artiﬁcial Intelligence Review,
pp. 1–29, 2022.

[30] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of
word representations in vector space,” arXiv preprint arXiv:1301.3781,
2013.

[31] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

Transactions on Information Theory, vol. 13, no. 1, pp. 21–27, 1967.

[32] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng, “Person re-
identiﬁcation by multi-channel parts-based cnn with improved triplet loss
function,” in 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016, pp. 1335–1344.

[33] M. Grinberg, Flask web development: developing web applications with

python.

” O’Reilly Media, Inc.”, 2018.

[34] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers,
P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J.
Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett,
A. Haldane, J. F. del R´ıo, M. Wiebe, P. Peterson, P. G´erard-Marchant,
K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke,
and T. E. Oliphant, “Array programming with NumPy,” Nature,
vol. 585, no. 7825, pp. 357–362, Sep. 2020. [Online]. Available:
https://doi.org/10.1038/s41586-020-2649-2

[35] H. Borges and M. T. Valente, “What’s in a github star? understanding
repository starring practices in a social coding platform,” Journal of
Systems and Software, vol. 146, pp. 112–129, 2018.

[36] N. Munaiah, S. Kroh, C. Cabrey, and M. Nagappan, “Curating github for
engineered software projects,” Empirical Software Engineering, vol. 22,
no. 6, pp. 3219–3253, 2017.

[37] M. Allamanis, “The adverse effects of code duplication in machine
learning models of code,” in Proceedings of the 2019 ACM SIGPLAN
International Symposium on New Ideas, New Paradigms, and Reﬂections
on Programming and Software, ser. Onward! 2019. New York, NY,
USA: Association for Computing Machinery, 2019, p. 143–153.
[Online]. Available: https://doi.org/10.1145/3359591.3359735

[38] S. Kaufman, S. Rosset, C. Perlich, and O. Stitelman, “Leakage in data
mining: Formulation, detection, and avoidance,” ACM Transactions on
Knowledge Discovery from Data (TKDD), vol. 6, no. 4, pp. 1–21, 2012.
[39] Student, “The probable error of a mean,” Biometrika, pp. 1–25, 1908.
[40] W. Wang, V. W. Zheng, H. Yu, and C. Miao, “A survey of
zero-shot learning: Settings, methods, and applications,” ACM Trans.
Intell. Syst. Technol., vol. 10, no. 2, jan 2019. [Online]. Available:
https://doi.org/10.1145/3293318

[41] M. A. Pimentel, D. A. Clifton, L. Clifton, and L. Tarassenko, “A review

of novelty detection,” Signal processing, vol. 99, pp. 215–249, 2014.

[42] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter,
“Continual
lifelong learning with neural networks: A review,”
Neural Networks, vol. 113, pp. 54–71, 2019. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0893608019300231

[43] W. J. Reed, “The pareto, zipf and other power laws,” Economics letters,

vol. 74, no. 1, pp. 15–19, 2001.

[44] R.-M. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes, “Big
code!= big vocabulary: Open-vocabulary models for source code,” in
2020 IEEE/ACM 42nd International Conference on Software Engineer-
ing (ICSE).

IEEE, 2020, pp. 1073–1085.

[45] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, “Analysis of
representations for domain adaptation,” Advances in neural information
processing systems, vol. 19, 2006.

[46] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and
J. W. Vaughan, “A theory of learning from different domains,” Machine
learning, vol. 79, no. 1, pp. 151–175, 2010.

[47] P. Geurts, D. Ernst, and L. Wehenkel, “Extremely randomized trees,”

Machine learning, vol. 63, no. 1, pp. 3–42, 2006.

[48] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavi-
olette, M. Marchand, and V. Lempitsky, “Domain-adversarial training
of neural networks,” The journal of machine learning research, vol. 17,
no. 1, pp. 2096–2030, 2016.

[49] J. Shen, Y. Qu, W. Zhang, and Y. Yu, “Wasserstein distance guided
representation learning for domain adaptation,” in Proceedings of the
AAAI Conference on Artiﬁcial Intelligence, vol. 32, no. 1, 2018.

[50] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “Smote:
synthetic minority over-sampling technique,” Journal of artiﬁcial intel-
ligence research, vol. 16, pp. 321–357, 2002.

