1

CMOS Circuit Implementation of Spiking Neural
Network for Pattern Recognition Using On-chip
Unsupervised STDP Learning

Sahibia Kaur Vohra, Student Member, IEEE, Sherin A Thomas, Student Member, IEEE,
Mahendra Sakare, Member, IEEE, and Devarshi Mrinal Das, Senior Member, IEEE

2
2
0
2

r
p
A
9

]

V

I
.
s
s
e
e
[

1
v
0
3
4
4
0
.
4
0
2
2
:
v
i
X
r
a

Abstract—Computation on a large volume of data at high speed
and low power requires energy-efﬁcient computing architectures.
Spiking neural network (SNN) with bio-inspired spike-timing-
dependent plasticity learning (STDP) is a promising solution
for energy-efﬁcient neuromorphic systems than conventional
artiﬁcial neural network (ANN). Previous works on SNN with
STDP learning primarily uses memristive devices which are
difﬁcult to fabricate. Some reported works on SNN makes use of
memristor macro models, which are software-based and cannot
give complete insight into circuit implementation challenges. This
article presents for the ﬁrst time, a full circuit-level implemen-
tation of the SNN system featuring on-chip unsupervised STDP
learning in standard CMOS technology. It does not involve the
use of FPGAs, CPUs or GPUs for training the neural network. We
demonstrated the complete circuit-level design, implementation
and simulation of SNN with on-chip training and inference
for pattern classiﬁcation using 180 nm CMOS technology. A
comprehensive comparison of the proposed SNN circuit with
the previous related work is also presented. To demonstrate
the versatility of the CMOS synapse circuit for application
scenarios requiring rate-based learning, we have tuned the pair-
based STDP circuit to obtain Bienenstock-Cooper-Munro (BCM)
characteristics and applied it to heart rate classiﬁcation.

Index

Terms—Spiking

neural

networks,

dependent plasticity (STDP), memristor
learning, pattern recognition.

spike-timing-
crossbar, on-chip

I. INTRODUCTION

S PIKING neural network (SNN) is the third generation arti-

ﬁcial neural network that provides a promising solution for
replacing area and power-hungry hardware for neuromorphic
computing. The brain’s superior energy efﬁciency for decision-
making cognitive tasks made scientists to focus their efforts on
building non-Von Neumann computer systems that imitate the
biological brain. Neurons process information as asynchronous
event-driven spikes and retain memories as synaptic strengths
of their connection in the brain. In this regard, a spiking
neural network (SNN) is more bio-plausible than other neural
networks that can pave a new way for future intelligent devices
for low-power computing applications.

Spiking neuromorphic computing architecture as shown in
Fig. 1, processes information in the form of spikes. Therefore,
input sensory analog signal should be converted to spikes

Sahibia Kaur Vohra, Sherin A Thomas, Mahendra Sakare and De-
varshi Mrinal Das are with the Department of Electrical Engineer-
ing, Indian Institute of Technology, Ropar, Rupnagar 140001, India (e-
mail: sahibia.19eez0002@iitrpr.ac.in, sherin.19eez0001@iitrpr.ac.in, mahen-
dra@iitrpr.ac.in, devarshi.das@iitrpr.ac.in).

Fig. 1. The simpliﬁed architecture of SNN for pattern recognition comprising
input layer LIF neurons, memristive crossbar and output layer LIF neurons
with WTA mechanism.

which can be performed by various neuron models [1]. Out of
all the neuron models, the LIF neuron model gives a good bal-
ance between accuracy and ease of hardware implementation,
and also it resembles much of biological neurons [1], [2]. The
other important block of the SNN is the synapse circuit that
stores the synaptic weight and deﬁnes the strength of the con-
nection between the neurons. Memristor proves to be the most
suitable candidate for emulating a synapse as it provides the
tunable and non-volatile storage of synaptic weights [3], [4].
Memristive neuromorphic systems outperform Von-Neumann
systems in power efﬁciency and learning capabilities [5].

The learning mechanism used for updating the synaptic
weights is a crucial aspect of the neural network. The training
algorithm such as backpropagation used in literature [6], [7]
is widely established but needs extensive hardware resources
and hard to be fully implemented in analog circuits. Many
works have shown the training of memristive neural network
using the ex-situ method [8], [9] where an external circuit
or computational platform is required for weight calculation.
In contrast to these learning methods, a bio-plausible learning
method spike-timing-dependent plasticity (STDP) can be used.
It has been proved that STDP can be used to train the
SNN in-situ with unsupervised learning without compromising
parallelism [10]–[14]. Inference in SNN by taking pre-trained
memristor array consumes signiﬁcantly less power; however,
training the synaptic weights of the SNN before inference
efﬁciently remains challenging [15], [16]. The approach of
training the non-SNN ﬁrst and then converting it to an SNN
has several limitations in terms of accuracy loss and large
inference latency [15], [17]. The more efﬁcient way is to do

 
 
 
 
 
 
both training and inference in SNN hardware. Some works
have shown the training by mapping the values of the ob-
tained weight using learning algorithms in software to the
neural crossbar [18]. Some make use of GPUs, FPGAs or
microcontrollers, which increases the complexity and power
consumption [18], [19]. As a result, doing the training on the
implemented hardware for SNN (known as in-situ training) is
the more power-efﬁcient way.

The realisation of neuromorphic circuits with memristive
synapse have shown wider applications for low power and
energy and area-efﬁcient computing neuromorphic system-
on-a-chip (NeuSoc) implementation. However,
the varying
threshold of the memristive devices, stochastic switching and
variable resistance states put challenges on the memristive
devices [20]. In addition, unlike CMOS-based circuits, such
memristive devices require additional fabrication steps [21],
which makes them challenging to integrate with standard
CMOS circuit components in commercial foundries. The
memristive devices are yet not available in any standard
CMOS technology PDK. As a result, while memristive device
technology matures, CMOS-based memristive synapse circuits
can be investigated for real-time hardware implementation of
neuromorphic circuits. Many works [9], [22]–[24] show the
SNN architecture for pattern recognition applications using the
SPICE model of the memristor, which cannot be used for real
hardware implementation. These challenges and limitations
motivated us to design an SNN system with CMOS memristor
emulators to get full insight into the circuit implementation
challenges for memristive SNN based neuromorphic comput-
ing (NMC) system. In work [25], [26] we have used the CMOS
memristor emulator given in [27] for implementing the full
CMOS memristive neural network.

The main contributions of this work are:

1) A complete CMOS based SNN system for pattern recog-
nition is designed. To the best of our knowledge, the
complete CMOS based circuit design and implementa-
tion of an analog SNN system with memristor emulator
circuit as a synapse has not been reported yet.

2) The proposed SNN system shows the unsupervised on-
chip STDP learning and inference with transistor-level
circuit
in
literature. The functionality of the proposed system is
demonstrated by a pattern recognition task.

implementation, which is not reported yet

3) To demonstrate the versatility of the CMOS memristive
synapse circuit, we have tuned the CMOS based STDP
circuit by exploiting the rate-based Bienenstock-Cooper-
Munro (BCM) learning characteristics for heart rate
classiﬁcation.

The rest of the paper is organised as follows. Section II
gives the detail of the LIF neuron circuit for converting the
input pixel of an image into a spike train. Section III explains
the STDP learning obtained using CMOS memristive STDP
circuit. Section IV explains the proposed CMOS based SNN
system. Section V discusses the simulation, robustness and
comparison with the other works. Section VI contains the
prospect for the used STDP circuit for heart rate classiﬁcation
application. The conclusion is dawn in section VII.

2

Fig. 2. CMOS circuit of LIF neuron. It comprises various sections for current
injection, leaky integration, schmitt trigger for ﬁring and reset [2].

II. LIF NEURON CIRCUIT

In our SNN system, the low-power, low-complexity neuron
circuit proposed in [22] is employed as a presynaptic and post-
synaptic neuron. It contains a lateral inhibition interface, which
is used for implementing winner-take-all (WTA) mechanism
(refer Fig. 1) in SNN. In Fig. 2 input current Iin, (controlled by
voltage given at input terminals TEX and TF F ) is injected into
the current integration section via the current mirror, which
charges the capacitor Cu. When membrane voltage Vu reaches
the neuron’s threshold, the spike is generated with the help
of the Schmitt trigger, and simultaneously Rst will be low,
which turns on M11 and resets the voltage Vu through Cref
and M14. The circuit can also get reset through external pin
TIN H . The switching voltage (VSV ) of the Schmitt trigger
[28] is the neuron’s ﬁring threshold, which can be calculated
as

IDn2 = βn2(VSV − Vn − VT H )2
IDp2 = βp2(VDD − VSV − |Vtp|)2

(1)

(2)

where IDn,pi is the drain current and βn,pi represents the
transconductance of transistor Mn,pi. Now, equating (1) and
(2), we get

VSW = Vn +

VDD + VT (1 − R) − Vn
R + 1

IDn1 = βn1(Vn − VT H )2

Vn =

+ VT H

Equating (1) and (4), we get
VHL
Rn + 1

Rn − 1
Rn + 1
After substituting (5) in (3), VSV is given as
Rn + 1
Rn(R + 1) + 1
(cid:113) βn2
βp2

VSV = VDD

(cid:113) βn1
βn2

+ VT H

Rn(2R − 1) − 1
Rn(R − 1) + 1

; Rn =

where R =
. Whenever the neuron’s
membrane voltage (Vu) reaches the switching voltage VSV ,
neuron outputs a spike. The width of the spike can be
controlled by the switching voltage of the Schmitt trigger given
by (6).

Further, we have adapted this circuit to meet the desired
requirements of our proposed CMOS based SNN system. The
simulation results of the LIF circuit implemented in 180 nm
CMOS technology is shown in Fig. 3. It can be observed that
the neuron will not generate spikes for input voltage VF F <

(3)

(4)

(5)

(6)

M3M1M2M13M12M4M14M5Mp2Mp3Mn3Mn1CUCrefTEXTFFVbVbRstVddM11Schmitt TriggerCurrent InjectionIntegrationResetLeakyMn2VnVuVinTINHIinVspike3

Fig. 5. STDP characterstic curve obtained from the synapse circuit for all
process corners with parameters listed in Table I.

Fig. 3. Response of spiking LIF neuron to different input step voltages. Rate
of spiking is increasing with each step of input voltage VF F .

Fig. 4. CMOS circuit of memristive STDP learning synapse.

500mV (given at TF F ) and spiking rate is proportional to the
input voltage for VF F > 500mV . This property is used to
convert the input signal into rate encoded spike train.

TABLE I
CIRCUIT COMPONENTS AND PARAMETER VALUES OF STDP SYNAPSE

Name
Rp,m
Cp,m, C1
A+, A−
VCM

Values
1M Ω
1p F
1.8 V
900 mV

Name
HRS
LRS
Gm
W/LMf

Values
1.6 MΩ
114 KΩ
18 µA/V
0.42/10µ

III. MEMRISTIVE SYNAPSE CIRCUIT FEATURING STDP
LEARNING MECHANISM

STDP learning mechanism observed in biological synapses
has the potential to train the SNN in an unsupervised manner
[10]. The phenomenon of STDP learning depends on the spike
timing of presynaptic and postsynaptic neuron. It strengthens
(weakens) the synaptic weight between two neurons if the
presynaptic neuron ﬁres earlier (later) than the postsynaptic
neuron. Fig. 4 shows the compact memristive STDP learning
circuit given in [29] which is used as a synapse for our
proposed CMOS SNN system. The RP , CP and Rm, Cm
shown in Fig. 4 implements the two exponential decay circuits
(EDCs) for pre spike and post spike respectively. The post
spike (pre spike) samples the Vpre,exp (Vpost,exp) trace and
update the voltage at the positive (negative) terminal of the op-
erational transconductance ampliﬁer (OTA). The exponential
traces are translated to the current through transconductance
Gm, which will charge the capacitor C1 when the switches
(controlled by φ, Slearnj ) are closed. The voltage VG across

Fig. 6. Transient simulation of a single STDP synapse for Long-term
depression (LTD) and long-term potentiation (LTP)

C1 is the state of the synapse and models the synaptic weight
(W ) by controlling the conductance (G) of transistor Mf as
given by (7), (8)

G = Kn

(Vg − VCM − VT Hn)

W
L
W = G(Vg)

(7)

(8)

The synapse circuit is implemented in 180 nm CMOS tech-
nology with parameters given in Table I. The values of
Rp,m, Cp,m have to be selected according to the spiking
frequency of neurons so that it can show the synaptic plasticity
for the frequency range of the LIF neurons. The simulated
result of the synapse circuit shown in Fig. 5 veriﬁes that the
STDP learning curve is retained in all process corners. All
postsynaptic spikes preceding (succeeding) presynaptic spikes
with the delay ∆t < tmaxpot (|∆t| < |tmaxdep |) result in
long-term potentiation (LTP) (Long-term depression (LTD))
as shown in Fig. 6. tmaxpot and |tmaxdep | are the maximum
timing difference between presynaptic and postsynaptic spikes
after which net change in weight (∆W ) is 0 for potentiation
and depression respectively. LTP and LTD lead to the low
resistance state (LRS) and high resistance state (HRS) of the
STDP memristive synapse, respectively.

IV. PROPOSED CMOS BASED SNN SYSTEM

The proposed transistor-level circuit design of unsupervised
SNN with in-situ STDP learning is shown in Fig. 7. The
design can be divided into two parts: the circuit design for
updating weight (or training phase), and the circuit design for
the inference phase (or recognition phase).

A. Circuit Design of Training Phase

The long-term potentiation (LTP) obtained using the STDP
learning circuit is used to update the synaptic weights accord-
ing to the input. During the training phase, switch Sinf erence

VCMVpostVpreVpre,expVpostVpost,expVpreGmRpRmCpC1Slearn_jABA+A-OVpre= Vpre orVpostVpostVCMVCMVCMCmVgMf4

Fig. 7. The proposed CMOS based SNN system architecture combining 15 input LIF neurons, 15∗6 memristive crossbar array and 6 output LIF neurons. (a)
Input LIF neuron circuit. (b) Delay circuit designed for the delay of 1 µs. (c) CMOS memristive STDP synapse circuit (d) 2-stage OTA used for summing
the synaptic currents. (e) Output LIF neuron circuit with inhibitory interface. (f) OR gate for sending the inhibitory signal to output neurons. (g) six 5 X 3
pixels patterns for recognition.

is open and switch Slearn is closed. Thus the output layer
is disconnected from the synaptic crossbar resulting in the
equivalent circuit shown in Fig. 8(a). The B terminal of each
STDP synapse is biased at a constant voltage. Each STDP
circuit in the crossbar will receive presynaptic spikes from the
input layer LIF neuron. The postsynaptic spikes to the STDP
circuit will be a delayed version of the presynaptic spikes given
from the input LIF neuron. Initially, all the synaptic weights
are set as slow or at HRS. The process of updating weight
based on the input can be divided into two different situations
1) If the input is high (corresponding to a black pixel)
or greater than the threshold voltage of LIF neurons,
spikes will be generated by the input LIF neuron. These
spikes will act as presynaptic spikes to the STDP circuit.
The delay unit will send these spikes with the delay
∆t to the postsynaptic terminal of the STDP circuit.
Each postsynaptic spike will precede presynaptic spike
with the timing difference ∆t. Thus, the weight change
will monotonically increase with each postsynaptic spike
leading to LTP.

2) When the input is low (corresponding to a white pixel) or
less than the threshold voltage of the LIF neuron, there
will be no spike generated from the input LIF neuron,
and the weight will remain unchanged or in HRS.

Fig. 8. The proposed system in (a) training mode and (b) inference mode.

B. Circuit Design of Inference Phase

During the inference phase, switch Sinf erence is closed
and switch Slearn is open. The input
layer is thus fully
connected to the output layer through the synaptic crossbar,

5

(a)

(b)

Fig. 9. Simulation results of training phase (a) Control signals for training each column’s synapses sequentially. (b) Synaptic state change of all STDP
memristors corresponding to the input pattern.

as shown in Fig 8(b). The input corresponding to a pattern
is given at the input neurons and processed with the synaptic
crossbar. The weighted currents are summed column-wise and
converted to a voltage using a summing ampliﬁer (Asum)
and an inverter ampliﬁer (Ainv). The output LIF neuron (i.e.
the winner neuron) with the highest summed current will ﬁre
ﬁrst and inhibit other neurons from ﬁring. Unlike input layer
LIF neurons, inhibitory interface (using VIN H ) is required
in output LIF neurons for implementing the winner-take-all
(WTA) mechanism. Whenerever a winner neuron ﬁres, the
output of an OR gate turns high which resets other neurons
through inhibitory terminals (VIN H ).

V. SIMULATION RESULTS

This section presents the simulation results of the proposed
CMOS based SNN system for the training and recognition
phases. To validate the working of the proposed SNN circuit
for pattern recognition, we have performed the image classi-
ﬁcation of six patterns (‘0’, ‘1’, ‘2’, ‘3’, ‘4’, ‘5’) shown in
Fig. 7(g). Each pattern is a binary image of 5 X 3 pixels,
requiring 15 input LIF neurons, 6 output LIF neurons, and the
crossbar of 90 STDP synapse circuits for classiﬁcation.

A. Training Phase

The network is trained for each pattern sequentially and is
in a conﬁguration shown in Fig. 8(a). Initially, all the STDP
synapses are in the high resistance state (HRS). The control
signals applied during the training phase is shown in Fig. 9(a).
All the synapses in the jth column will be trained in the jth
learning cycle (Slearn j, j=1 to 6). As an example shown in
Fig. 9(a), during high input of Slearn 3 if input pattern ‘2’

Fig. 10. Simulation result of inference phase

is given in the sequence (shown in Fig. 7(g)), the synaptic
state of the STDP circuit in 1st row, 3rd column (Vg1,3)
will increase according to STDP learning. Simultaneously, all
other synaptic states of 3rd column will be updated (Vgi,3, i=1
to 15) as illustrated in Fig. 9(b). The synaptic states (Vg1,3,
Vg2,3, Vg3,3, Vg6,3, Vg7,3, Vg8,3, Vg9,3, Vg10,3, Vg13,3, Vg14,3,
Vg15,3) emulated by the gate voltage of the ﬂoating transistor
of the STDP circuit (refer to Fig. 4) increases (or the synaptic
resistance decreases to LRS) while remaining states are in their
initial HRS. The change in the synaptic states of other columns
in jth learning cycle corresponding to the given pattern is
also shown in Fig. 9(b). The process of changing the synaptic
weights of each column sequentially according to the given
pattern shows the unsupervised in-situ STDP learning of the

TABLE II
RECOGNITION RATE FOR VARIOUS NOISY PATTERNS.

Noise
(%)

Total
patterns

noisy

0
6.67
13.3
20.0

1 × 6 = 6
15 × 6 = 90
105 × 6 = 630
455× 6 = 2730

Expected recogni-
tion rate from an-
alytical results(%)
100
77.8
55.4
46.2

Recognition rate
from
circuit
simulation(%)
100
77.8
55.4
46.2

network.

B. Inference Phase

After training the network for all patterns, each synaptic
crossbar column (j=1 to 6) will store the weights according to
the corresponding pattern (0 to 5). Each pattern is presented to
the network for 10 µs. Only the winner neuron will ﬁre when
its corresponding pattern is given at the input while inhibiting
the other neurons. Fig. 10 shows the ﬁring response of the
output LIF neurons, which conﬁrms that each output neuron
is able to recognise the corresponding pattern.

C. Robustness

In order to analyse the robustness of the proposed SNN
system for pattern recognition of digits (‘0’, ‘1’, ‘2’, ‘3’, ‘4’,
‘5’), we have evaluated the recognition rate of the circuit
for different noisy patterns. Also, we have considered the
limitations of CMOS fabrication technology by adding process
and temperature variations of all the circuit components in
the architecture. The noise is added by inverting the pixel
of the original pattern from 0 to 1 and vice versa in the
percentage of 6.67%, 13.3% and 20% as done in [18]. We
have tested all possible combinations of noisy patterns listed
in Table II. Taking the case of 13.3% noise patterns, i.e.
change of 2 pixels in 5×3 pixel image, there are 105 cases
of noisy patterns for one digit patterns and 630 cases for six
digit patterns. Response of output neuron in inference phase
shown in Fig. 11 veriﬁes the correctness of the circuit under
extreme process corners. Each case of noisy pattern is also
tested for process corners (SS, SF, FS, FF, TT) and different
temperatures (0◦C, 27◦C, 50◦C, 100◦C). As shown in Table II,
the recognition rate of the circuit-based simulations matches
closely with the analytical results using MATLAB. This proves
the robustness of the circuit for noisy patterns in the presence
of process and temperature variation.

D. Comparison and Discussion

6

Fig. 11. Response of output neurons in recognition phase for all extreme
process corners at temperature = 27◦C.

and synaptic circuits. However, they have used the SPICE
model of the memristor in their simulations. Also, the weights
were calculated in an ex-situ fashion and were mapped to
the memristive crossbar using digital pulses. This makes the
use of an extra circuit to read/write a weight value from/to a
synaptic circuit. In view of this, our work implements in-situ
learning avoiding the use of any additional circuit overhead.
In paper [23], [29], SNN is designed for digit recognition
using in-situ supervised STDP learning. Supervised learning
requires a teacher signal for each output neuron. However, in
our designed SNN circuit for digit recognition, no interference
of output neuron or teacher signal is required. The WTA
mechanism implemented in [23], [29] requires a bus interface
circuit made of digital gates and D-FFs. In contrast, our system
uses only a CMOS OR gate with output neurons to perform
the inhibition. In [29], CMOS based memristor circuit with
STDP learning is proposed but simulated using memristive
synapse and WTA neuron macro models using brian2 libraries
in Python. Work [24] implements the non-spiking memristive
neural network for pattern recognition using Hebbian in-situ
learning. They have used the SPICE model of memristor and
programmed it using high magnitude bipolar digital pulses.
Also, their architecture requires two memristors per synapse.
In contrast, our work uses a more bio-plausible way of
implementing the pattern recognition application using SNN
with STDP learning and LIF neuron at input and output. In
[19], instead of taking any SPICE model of the memristor, a
fabricated PCMO based memristive device is used. However,
full SNN hardware was not implemented in one chip. Also, it
requires the use of FPGA as a controlling unit. In comparison,
our proposed work does not require FPGA and gives the on-
chip design of SNN for pattern recognition.

This paper presents the full CMOS-based transistor level
implementation of SNN for pattern recognition which makes
it different from other existing literature listed in Table III.
The proposed SNN system uses more bio-plausible mecha-
nisms, including unsupervised STDP in-situ learning of CMOS
memristive synapses. The employment of LIF neurons [22] in
the output layer, which have an inhibitory interface, has aided
in achieving the WTA mecha nism discussed in section IV-
B in a more straightforward manner. Authors in [22] have
proposed the hardware architecture using spiking LIF neurons

VI. PROSPECT
Although the proposed CMOS SNN system is used for
pattern recognition of six digits, it can also be used for visual
pattern recognition. The complete neuromorphic system for
visual pattern recognition will contain CMOS photoreceptor
which converts optical data into a spike train. The generated
spike train will be input to the memristive neural network for
recognition.

In view of the CMOS STDP circuit used in this paper, we
have also applied it to demonstrate the heart rate classiﬁcation.

TABLE III
COMPARISON OF CIRCUIT IMPLEMENTATION OF DIFFERENT MEMRISTIVE CROSSBAR-BASED NEURAL NETWORK CIRCUITS

7

Z. Chen et al., M. Chu et al.,
TVLSI,
2021 [24]
Biolek SPICE
model
Hebbian,
in-situ
Digital pulses
of ±5 V

TIE,
2015 [19]
PCMO based fabricated
memristive device
Modiﬁed STDP learning

Digital pulses of ±3 V

Unsupervised
Control logic

This work

STDP

CMOS
synapse circuit
STDP, in-situ

Spikes

Unsupervised
Using a CMOS
OR gate

(5 × 6) / 10

(5 × 3) / 6

300

90

implemen-
Hardware
tation using memristor
crossbar
array in one
chip, FPGA and LIF
neurons
in
another chip

integrated

CMOS

Full
transistor
level
implementation
of system

circuit

Unsupervised
Using PMOS
transistors
with
layer neurons
(5 × 3) / 3

output

(2

90
memristors
per synapse)
Not
implemented

Type of mem-
ristor
Learning rule

Type of data
for
updating
memristor
weight
Learning type
WTA mecha-
nism

Image size/no.
of patterns
No. of synapse
in crossbar

CMOS
Full
transistor level
circuit
imple-
mentation

Shamshi et al.,
TVLSI,
2018 [22]
Yakopcic
SPICE model
STDP, ex-situ

Programming
memristor
using
pulses
Supervised
-

digital

(5 × 5) / 4

X. Wu et al.,
JETCAS,
2015 [23]
Yakopcic
SPICE model
STDP, in-situ

V. Saxena et al.,
ISCAS,
2018 [29]
STDP synapse circuit

STDP, in-situ

Spikes

Spikes

Supervised
Bus
interface
circuit made
of digital gates
and D-FF
(8 × 8) / 10

Supervised
Bus interface circuit made
of digital gates and D-FF

(8 × 8) / 10

100

640

640

Not
implemented
(used
memristor
model)

Not
implemented
(used
memristor
model)

implemented
Not
(simulated
using macro-
models memristive synapse
and WTA neuron macro-
models
Brian2
using
libraries in python

TABLE IV
PARAMETERS USED FOR DIFFERENT THRESHOLD BCM LEARNING

Parameter
A+
τ+
A−
τ−

θ1 (For 60 BPM)
0.267
0.7
0.175
1.7

θ2 (For 120 BPM)
0.19
0.7
0.138
1.7

We have explored this circuit to obtain Beinenstock-Cooper-
Munro (BCM) characteristics for rate-based learning other
than timing based learning. Unlike spike-timing (STDP) based
learning, BCM learning modiﬁes the synaptic weights based
on pre and postsynaptic spike frequencies. It has been reported
that by limiting the interaction of pre and postsynaptic spikes
to the nearest-neighbour spike interaction only, BCM learning
can be replicated from pair-based STDP learning circuit [31],
[32]. The synaptic weight change is determined by the thresh-
old frequency θ, if postsynaptic ﬁring rate fx < θ, depression
occurs (change in synaptic weight is negative), if ﬁring rate
fx > θ, potentiation occurs (change in synaptic weight is
positive). The threshold frequency between potentiation and
depression (at which the rate change of the synaptic weight is
zero) is given by (9), where A+, A−, τ+, τ− are the parameters
of the STDP curve [31];

θ = −

A+/τ− + A−/τ+
A+ + A−

(9)

In rested condition, the lower and the upper safe limits of
heart rate (in beats per minute) are 60 bpm and 100 bpm,
which corresponds to 1 Hz and 1.667 Hz respectively. The
two BCM curves corresponding to θ1, θ2 are shown in Fig. 12,
which are obtained by optimising the two STDP circuits for

Fig. 12. BCM learning curve of the nearest-neighbour pair-based STDP circuit
with different thresholds (θ1 = 1Hz, θ2 = 1.667Hz)

TABLE V
HEART RATE CLASIFICATION OUTPUT FOR ECG DATABASE

S.
No.
1
2
3
4
5
6
7
9
1
10

Actual Heart
Rate (bpm)
70.36
82.15
87.42
96.84
56.00
43.06
134.87
108.42
106.72
51.2

60

∆w1(For
bpm)
5.783∗10−3
1.073∗10−2
1.275∗10−2
1.601∗10−2
-1.642∗10−3
-7.143∗10−3
2.582∗10−2
1.952∗10−2
1.903∗10−2
-3.708∗10−3

∆w2(For
100 bpm)
-6.416∗10−3
-2.966 ∗10−3
-1.546∗10−3
-7.544 ∗10−4
-1.154∗10−2
-1.501∗10−2
7.722∗10−3
3.239 ∗10−3
2.895∗10−3
-1.288∗10−2

Classiﬁer
Output
NORMAL
NORMAL
NORMAL
NORMAL
LOW
LOW
HIGH
HIGH
HIGH
LOW

parameters listed in Table IV. The classiﬁer (containing two
BCM learning enabled STDP circuits) has been tested with
ECG datasets taken from PhysioBank ATM. ∆W1, ∆W2
correspond to the weight change observed in STDP circuit
tuned for θ1 = 1 Hz and θ2 = 1.667 Hz respectively. The
input spike data is generated in MATLAB by detecting the Q
peaks from the ECG signal. Based on the rate of weight change
the proposed heart rate classiﬁer correctly classiﬁes the ECG

database (present on PhysioBank ATM) as shown in Table V.
The classiﬁed rate is normal if potentiation is seen in ∆W1
(positive value of ∆W1) and depression in ∆W2 (negative
value of ∆W2).

VII. CONCLUSION

This paper presents the complete CMOS based transistor-
level implementation of SNN for pattern recognition. It inte-
grates 15 input CMOS LIF neurons, a crossbar array of 90
CMOS memristive synapse circuits and 6 output LIF neurons
for classiﬁcation of six 5×3 pixel images. This system embeds
on-chip learning and inference using STDP mechanism and
WTA property. The designed CMOS based SNN system is
validated with the training and inference simulation results for
the classiﬁcation of six patterns. The proposed circuit is shown
to be robust for process and temperature variations. Moreover,
the recognition rate obtained from the circuit simulation under
noisy patterns matches the analytical results, proving the
accuracy of the proposed CMOS-based SNN circuit. Other
than the timing based STDP learning, rate-based learning is
also explored using the same pair-based STDP circuit and
shown its application for heart rate classiﬁcation by obtaining
the BCM characteristics of the STDP circuit.

ACKNOWLEDGMENT

The authors thank Ministry of Education (MoE), MeitY
and SCL, Govt. of India for providing institute fellowship,
availing the tools through SMDP-C2SD project and 180 nm
PDK, respectively.

REFERENCES

[1] C. Zhao, Y. Yi, J. Li, X. Fu, and L. Liu, “Interspike-Interval-Based
Analog Spike-Time-Dependent Encoder for Neuromorphic Processors,”
IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol.
25, no. 8, pp. 2193-2205, 2017.

[2] O. Krestinskaya, A. P. James, and L. O. Chua, “Neuromemristive Circuits
for Edge Computing: A Review,” IEEE Transactions on Neural Networks
and Learning Systems, vol. 31, no. 1, pp. 4-23, 2020.

[3] H. Liang et al., “Memristive Neural Networks: A Neuromorphic Paradigm
for Extreme Learning Machine,” in IEEE Transactions on Emerging
Topics in Computational Intelligence, vol. 3, no. 1, pp. 15-23, 2019.
[4] B. Rajendran and F. Alibart, “Neuromorphic Computing Based on Emerg-
ing Memory Technologies,” in IEEE Journal on Emerging and Selected
Topics in Circuits and Systems, vol.6, no. 2, pp.198-211, 2016.

[5] B. Rajendran, A. Sebastian, M. Schmuker, N. Srinivasa, and E. Elefthe-
riou, “Low- Power Neuromorphic Hardware for Signal Processing Appli-
cations: A review of architectural and system-level design approaches,”
IEEE Signal Processing Magazine, vol. 36, no. 6, pp. 97-110, 2019.

[6] D. Negrov,

I. Karandashev, V. Shakirov, Y. Matveyev, W. Dunin-
Barkowski, and A. Zenkevich, “An approximate backpropagation learning
rule for memristor based neural networks using synaptic plasticity,”
Neurocomputing, vol. 237, pp. 193–199, 2017.

[7] Y. Zhang, X.Wang, and E. G. Friedman, “Memristor-based circuit design
for multilayer neural networks,” IEEE Trans. Circuits Syst. I: Reg. Papers,
vol. 65, no. 2, pp. 677–686, Feb. 2017.

[8] F. Alibart, E. Zamanidoost, and D. B. Strukov, “Pattern classiﬁcation by
memristive crossbar circuits using ex-situ and in-situ training,” Nature
Commun., vol. 4, 2013, Art. no. 2072.

[9] C. Yakopcic, R. Hasan, and T. M. Taha, “Memristor based neuromorphic
circuit for ex-situ training of multi-layer neural network algorithms,” in
Proc. Int. Joint Conf. Neural Netw., 2015, pp. 1–7.

[10] P. U. Diehl and M. Cook, “Unsupervised learning of digit recognition
using spike-timing-dependent plasticity,” Frontiers in computational neu-
roscience, vol. 9, p. 99, 2015.

8

[11] O. Krestinskaya, K. N. Salama and A. P. James, “Analog Backpropa-
gation Learning Circuits for Memristive Crossbar Neural Networks,” in
IEEE International Symposium on Circuits and Systems (ISCAS), 2018,
pp. 1-5.

[12] B. Nessler, M. Pfeiffer, L. Buesing, and W. Maass, “Bayesian com-
putation emerges in generic cortical microcircuits through spike-timing-
dependent plasticity,” PLOS Comput. Biol., vol. 9, no. 4, 2013.

[13] D. Querlioz, O. Bichler, P. Dollfus and C. Gamrat, “Immunity to Device
Variations in a Spiking Neural Network With Memristive Nanodevices,”
in IEEE Transactions on Nanotechnology, vol. 12, no. 3, pp. 288-295,
2013.

[14] N. Zheng and P. Mazumder, “Learning in Memristor Crossbar-Based
Spiking Neural Networks Through Modulation of Weight-Dependent
Spike-Timing-Dependent Plasticity,” in IEEE Transactions on Nanotech-
nology, vol. 17, no. 3, pp. 520-532, May 2018.

[15] K. Roy, A. Jaiswal, and P. Panda, “Towards spikebased machine intel-
ligence with neuromorphic computing,” Nature 575, 7784, pp. 607–617,
2019.

[16] A. Sengupta and K. Roy. “Encoding neural and synaptic functionalities
in electron spin: A pathway to efﬁcient neuromorphic computing,”
Applied Physics Reviews, p.041105, 2017.

[17] P. U. Diehl, D. Neil, J. Binas, M. Cook, S. Liu and M. Pfeiffer,
“Fast-classifying, high-accuracy spiking deep networks through weight
and threshold balancing,” in International Joint Conference on Neural
Networks (IJCNN), 2015, pp. 1–8.

[18] Y. Jiang et al., “Design and Hardware Implementation of Neuromorphic
Systems With RRAM Synapses and Threshold-Controlled Neurons for
Pattern Recognition,” in IEEE Transactions on Circuits and Systems I:
Regular Papers, vol. 65, no. 9, pp. 2726-2738, Sept. 2018.

[19] M. Chu et al., “Neuromorphic hardware system for visual pattern
recognition with memristor array and CMOS neuron,” IEEE Trans. Ind.
Electron., vol. 62, no. 4, pp. 2410–2419, Apr. 2015.

[20] V. Saxena, “High LRS-Resistance CMOS Memristive Synapses for
Energy-Efﬁcient Neuromorphic SoCs,” in IEEE 62nd International Mid-
west Symposium on Circuits and Systems (MWSCAS), Dallas, TX, USA,
2019, pp. 1-4, pp. 1143-1146, 2019.

[21] N. Dey, J. Sharda, U. Saxena, D. Kaushik, U. Singh, and D. Bhowmik.
2019, “On-chip Learning In A Conventional Silicon MOSFET Based
Analog Hardware Neural Network,” in IEEE Biomedical Circuits and
Systems Conference (BioCAS), 2019, pp. 1-4.

[22] J. Shamsi, K. Mohammadi and S. B. Shokouhi, “A Hardware Archi-
tecture for Columnar-Organized Memory Based on CMOS Neuron and
Memristor Crossbar Arrays,” in IEEE Transactions on Very Large Scale
Integration (VLSI) Systems, vol. 26, no. 12, pp. 2795-2805, Dec. 2018.
[23] X. Wu, V. Saxena and K. Zhu, “Homogeneous Spiking Neuromorphic
System for Real-World Pattern Recognition,” in IEEE Journal on Emerg-
ing and Selected Topics in Circuits and Systems, vol. 5, no. 2, pp. 254-266,
June 2015.

[24] Z. Chen, J. Zhang, S. Wen, Y. Li and Q. Hong, “Competitive Neural
Network Circuit Based on Winner-Take-All Mechanism and Online
Hebbian Learning Rule,” in IEEE Transactions on Very Large Scale
Integration (VLSI) Systems, vol. 29, no. 6, pp. 1095-1107, June 2021.
[25] S. K. Vohra, S. Thomas, M. Sakare and D. M. Das, “Full CMOS Im-
plementation of Bidirectional Associative Memory Neural Network with
Analog Memristive Synapse,” in IEEE International Midwest Symposium
on Circuits and Systems (MWSCAS), 2021, pp. 445-448.

[26] S. A. Thomas, S. K. Vohra, R. Kumar, R. Sharma and D. M. Das,
“Analysis of Parasitics on CMOS based Memristor Crossbar Array for
Neuromorphic Systems,” in IEEE International Midwest Symposium on
Circuits and Systems (MWSCAS), 2021, pp. 309-312.

[27] V. Saxena, “A Compact CMOS Memristor Emulator Circuit and its
Applications,” IEEE 61st International Midwest Symposium on Circuits
and Systems (MWSCAS), 2018, pp. 190-193.

[28] S. Al-Sarawi, “Low power schmitt trigger circuit,” Electronics letters,

vol. 38, no. 18, pp. 1009-1010, 2002.

[29] V. Saxena, X. Wu and K. Zhu, “Energy-Efﬁcient CMOS Memristive
IEEE
Synapses for Mixed-Signal Neuromorphic System-on-a-Chip,”
International Symposium on Circuits and Systems (ISCAS), 2018, pp. 1-5.
[30] J. M. Cruz-Albrecht, M. W. Yung and N. Srinivasa, “Energy-Efﬁcient
Neuron, Synapse and STDP Integrated Circuits,” in IEEE Transactions on
Biomedical Circuits and Systems, vol. 6, no. 3, pp. 246-256, June 2012.
[31] E. M. Izhikevich and N. S. Desai, “Relating STDP to BCM,” in Neural

Computation, vol. 15, no. 7, pp. 1511-1523, 1 July 2003.

[32] M. Rahimi Azghadi, S. Al-Sarawi, N. Iannella and D. Abbott, “Design
and implementation of BCM rule based on spike-timing dependent plas-
ticity,” in The 2012 International Joint Conference on Neural Networks
(IJCNN), 2012, pp. 1-7.

