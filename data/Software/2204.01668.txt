Scalable Spike-and-Slab

Niloy Biswas 1 Lester Mackey 2 Xiao-Li Meng 1

2
2
0
2

n
u
J

5
2

]

O
C

.
t
a
t
s
[

2
v
8
6
6
1
0
.
4
0
2
2
:
v
i
X
r
a

Abstract

Spike-and-slab priors are commonly used for
Bayesian variable selection, due to their inter-
pretability and favorable statistical properties.
However, existing samplers for spike-and-slab
posteriors incur prohibitive computational costs
when the number of variables is large. In this
article, we propose Scalable Spike-and-Slab (S3),
a scalable Gibbs sampling implementation for
high-dimensional Bayesian regression with the
continuous spike-and-slab prior of George & Mc-
Culloch (1993). For a dataset with n observations
and p covariates, S3 has order max{n2pt, np}
computational cost at iteration t where pt never
exceeds the number of covariates switching spike-
and-slab states between iterations t and t − 1 of
the Markov chain. This improves upon the or-
der n2p per-iteration cost of state-of-the-art im-
plementations as, typically, pt is substantially
smaller than p. We apply S3 on synthetic and
real-world datasets, demonstrating orders of mag-
nitude speed-ups over existing exact samplers and
signiﬁcant gains in inferential quality over approx-
imate samplers with comparable cost.

1. Introduction

1.1. Bayesian computation in high dimensions

We consider linear, logistic, and probit regression in high
dimensions, where the number of observations n is smaller
than the number of covariates p. This setting is common
in modern applications such as genome-wide association
studies (Guan & Stephens, 2011; Zhou et al., 2013) and as-
tronomy (Kelly, 2007; Sereno, 2015). In the non-Bayesian
paradigm, sparse point estimates such as the LASSO (Tibshi-
rani, 1996), Elastic Net (Zou & Hastie, 2005) and SLOPE
(Bogdan et al., 2015) offer a route to variable selection.

1Department of Statistics, Harvard University 2Microsoft
Research New England. Correspondence to: Niloy Biswas
<niloy biswas@g.harvard.edu>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

These estimates are based on optimization based approaches,
which are computationally efﬁcient and scale to datasets
with hundreds of thousands of covariates.

In the Bayesian paradigm, which will be our focus, one
places a prior on the unknown parameters of interest and con-
siders the corresponding posterior distribution. Sampling
algorithms such as Markov chain Monte Carlo (MCMC)
are then used to simulate from the posterior distribution. In
modern high dimensional settings, general-purpose MCMC
algorithms can have high computational cost per iteration.
This has kindled a line of work on tailored algorithms for
Bayesian regression (e.g., Polson et al., 2013; Yang et al.,
2016; Narisetty et al., 2019; Johndrow et al., 2020; Biswas
et al., 2022). Our manuscript participates in this wider ef-
fort to scale Bayesian inference to large data applications.
Speciﬁcally, we propose computationally efﬁcient MCMC
algorithms for high-dimensional Bayesian linear, logistic
and probit regression with spike-and-slab priors.

1.2. Variable selection with spike-and-slab priors

1

(cid:80)n

regression,

(2πσ2)n/2 exp (cid:0) −
exp(yix(cid:62)
1+exp(x(cid:62)

logistic regres-
Consider Gaussian linear
sion, and probit regression with n observations and p
The respective likelihoods are given by
covariates.
(cid:1),
Llin(β, σ2; X, y) =
Llog(β; X, y) = (cid:81)n
i β)
i β) , and Lprob(β; X, y) =
(cid:81)n
i β))1−yi. Here X ∈ Rn×p is
i , y ∈ Rn (for linear re-
the design matrix with rows x(cid:62)
gression) or y ∈ {0, 1}n (for logistic and probit regres-
sion) is the response vector, β ∈ Rp is the unknown signal,
σ2 ∈ (0, ∞) is the unknown Gaussian noise variance, and
Φ is the cumulative density function of N (0, 1).

i β)yi(1 − Φ(x(cid:62)

i=1(yi−x(cid:62)
2σ2

i=1 Φ(x(cid:62)

i β)2

i=1

We focus on the high-dimensional setting with n (cid:28) p,
where β ∈ Rp is assumed to be sparse. We use a continuous
spike-and-slab prior on β to capture sparsity:

σ2 ∼ InvGamma

(cid:16) a0
2

,

(cid:17)

,

b0
2
(for logistic and probit regression)

(for linear regression)

σ2 = 1,
i.i.d.∼
j=1,...,p

zj

Bernoulli(q),

βj|zj,σ2

ind∼
j=1,...,p

(1−zj)N (0,σ2τ 2

0 )+zjN (0,σ2τ 2

1 ),

(1)

 
 
 
 
 
 
Scalable Spike-and-Slab

1 (cid:29) τ 2

0 ) and N (0, σ2τ 2

where q ∈ (0, 1), τ 2
0 > 0, and a0, b0 > 0 are hyper-
parameters. Here, N (0, σ2τ 2
1 ) correspond
to the spike and slab parts of the prior respectively. In the
high-dimensional setting, a small constant q (cid:28) 1 is often
chosen, but the algorithms in this manuscript also readily ex-
tend to hierarchical variants of (1) with a hyperprior placed
on q (Scott & Berger, 2010; Castillo & van der Vaart, 2012).

Catalyzed by the works of George & McCulloch (1993;
1997), continuous spike-and-slab priors are now a mainstay
of Bayesian variable selection (see the recent reviews of
Tadesse & Vannucci (2021, Section I) and Banerjee et al.
(2021)). The posterior probabilities P(zj = 1|y) provide
a natural interpretable approach to variable selection. The
median probability model selects all covariates j such that
P(zj = 1|y) > 1/2, and it is easily ﬁtted using Monte Carlo
samples and provides the optimal predictive model in the
case with orthogonal design matrix, as well as extensions
with certain correlated matrices (Barbieri & Berger, 2004;
Barbieri et al., 2021). Narisetty & He (2014) have further
ﬁne-tuned the optimal scaling of q, τ 2
1 with respect
to the number of covariates and sample size to establish
model selection consistency for linear regression with gen-
eral design matrices in high dimensions.

0 , and τ 2

One could alternatively consider point-mass spike-and-slab
priors (e.g., Mitchell & Beauchamp, 1988; Johnson &
Rossell, 2012), where βj|zj ∼ (1−zj)δ0(·)+zjN (0, σ2τ 2
1 )
such that a degenerate Dirac distribution about zero is cho-
sen for the spike part. Point-mass priors have favorable
statistical properties (e.g., Johnstone & Silverman, 2004;
Castillo & van der Vaart, 2012) and we hope to extend our
algorithms to point-mass priors in follow-up work.

1.3. Our contributions

Our contributions are summarized below. Throughout, we
use O and Ω to respectively denote asymptotic upper and
lower bounds on computational complexity growth rates.

Section 2 introduces Scalable Spike-and-Slab (S3), a com-
putationally efﬁcient implementation of Gibbs samplers
for linear and logistic regression with the prior given in
(1). Section 2.1 investigates the computational bottlenecks
of state-of-the-art (SOTA) implementations, which require
Ω(n2p) computational cost per iteration for datasets with
n observations and p covariates. Section 2.2 develops S3,
which overcomes existing computational bottlenecks by
employing a pre-computation based strategy and requires
O(max{n2pt, np}) computational cost at iteration t, where
pt is no greater than the number of covariates switching
spike-and-slab states between iterations t and t − 1 of the
Markov chain. Section 2.3 analyzes the favourable compu-
tational complexity of S3, showing that pt is typically much
smaller than p and that it can remain constant and even
approach zero under various limiting regimes as p increases.

Section 3 compares S3 with the SOTA exact MCMC sam-
pler and a recently proposed approximate MCMC sampler,
which does not converge to the posterior distribution of in-
terest. We demonstrate that S3 offers substantially faster
numerical runtimes compared to the SOTA exact MCMC
sampler, reporting 50× speedups on synthetic datasets. In
the same experiment, S3 and the approximate sampler have
comparable runtimes, but the asymptotically exact S3 pro-
cedure provides more accurate variable selection.

Section 4 demonstrates the beneﬁts of S3 on a diverse suite
of datasets, including two synthetic datasets and eight real-
world experiments. For example, on a genome-wide as-
sociation study (GWAS) dataset with with n ≈ 2000 and
p ≈ 100000, we again observe 50× computational speedups
over the SOTA exact MCMC sampler. Finally, Section 5 dis-
cusses directions for future work. The open-source packages
ScaleSpikeSlab in R and Python (www.github.com/
niloyb/ScaleSpikeSlab) implement our methods
and recreate the experiments in this paper.

2. Scalable Spike-and-Slab

2.1. Status quo and computational bottlenecks

Gibbs samplers have long been employed to sample from the
posterior distributions corresponding to the prior in (1) (e.g.,
George & McCulloch, 1993; O’Brien & Dunson, 2004; Held
& Holmes, 2006; Polson et al., 2013). The computational
bottleneck of existing Gibbs samplers is linked to sampling
from the full conditional of β ∈ Rp. This is given by
t ∼ N (cid:0)Σ−1

t X (cid:62)y, σ2

βt+1|zt, σ2

t Σ−1
t

(2)

(cid:1)

for Σt = X (cid:62)X + Dt, where t indexes the iteration of the
Markov chain, and Dt is the diagonal matrix with the vector
ztτ −2

populating its diagonal elements.

1 + (1p − zt)τ −2

0

Sampling from (2) using standard matrix multiplication and
a generic Cholesky decomposition that ignores the speciﬁc
structure of Σt requires Ω(p3) computational cost, which
quickly becomes prohibitive for large p. Hereafter we will
refer to this generic method as the Na¨ıve Sampler. Ishwaran
& Rao (2005) recommend separating the components of β
into B blocks of size p/B each, and then updating each block
using Gibbs sampling, which gives a reduced Ω(p3B−2)
computational cost. However, this cost remains prohibitive
for large p, and using a larger number of blocks B induces
higher auto-correlation between successive iterations of the
Gibbs sampler. Recently, Bhattacharya et al. (2016) devel-
oped an algorithm based on the Woodbury matrix identity
(Hager, 1989) to sample from multivariate Gaussian distri-
butions of the form in (2), which requires a more favourable
Ω(n2p) computational cost and is given in Algorithm 1.

For large-scale datasets with n in the thousands and p in the
hundreds of thousands, as found in modern scientiﬁc appli-

Scalable Spike-and-Slab

Algorithm 1 An Ω(n2p) sampler of (2) (Bhattacharya et al.,
2016)

Sample r ∼ N (0, Ip), ξ ∼ N (0, In).
Set u = D− 1
t r and calculate v = Xu + ξ.
Set v∗ = M −1
t
Return β = σt(u + D−1

t X (cid:62)v∗).

y − v) for Mt = In + XD−1

( 1
σt

2

t X (cid:62).

cations, the Ω(n2p) cost per iteration is still too high. This
has spurred recent work on approximate MCMC (Narisetty
et al., 2019) for the continuous spike-and-slab prior on logis-
tic regression and on variational inference methods for the
point-mass spike-and-slab prior (Titsias & L´azaro-Gredilla,
2011; Ray et al., 2020; Ray & Szab´o, 2021). Such approxi-
mate samplers can provide improved computational speeds
but do not converge to the posterior distribution of interest.

2.2. A scalable Gibbs sampler

t

We now develop S3. Our key insight is that successive pre-
computation can be used to reduce the computational cost
of Algorithm 1. In Algorithm 1, the Ω(n2p) computational
cost per iteration arises from the calculation of the matrix
Mt. Current algorithms calculate Mt = In + XD−1
t X (cid:62)
from scratch every iteration at Ω(n2p) cost under standard
matrix multiplication and then solve an n by n linear system
to obtain v∗ at Ω(n3) cost. We propose instead to use the
previous state zt−1 and the pre-computed matrices Mt−1
and M −1
t−1 at each step to aid the calculation of M −1
. Our
strategy is given below.
Speciﬁcally, denote X = [X1, ..., Xp] ∈ Rn×p and its sub-
(cid:44)
(cid:44) [Xj : j ∈ At] ∈ Rn×(cid:107)zt(cid:107) and XAc
matrices XAt
t ] ∈ Rn×(p−(cid:107)zt(cid:107)), where At (cid:44) {j : zj,t = 1}
[Xj : j ∈ Ac
and Ac
t = {j : zj,t = 0}, are the ordered index sets of
covariates corresponding to slab states and to spike states
respectively at iteration t, and (cid:107) · (cid:107)1 is the L1 norm on
{0, 1}p. Also denote ∆t (cid:44) {j : zj,t (cid:54)= zj,t−1}, the ordered
index set of covariates which switch spike-and-slab states
between iterations t and t − 1; δt (cid:44) (cid:107)zt − zt−1(cid:107)1 = |∆t|,
(cid:44) Diag((Dt)j,j : j ∈ ∆t) ∈
the number of switches; D∆t
Diag(Rδt×δt), the diagonal sub-matrix of Dt composed of
(cid:44)
the diagonal entries with ordered indices in ∆t; and C∆t
0 XX T ∈ Rn×n
(cid:44) In + τ 2
D−1
∆t
and ˜Mτ1
1 XX T ∈ Rn×n, which are ﬁxed for all
iterations. Under this notation, there are three expressions
for Mt:

. Finally, let ˜Mτ0

∆t−1
(cid:44) In + τ 2

− D−1

t

Mt = ˜Mτ0 + (τ 2
0 )XAt X (cid:62)
At
= ˜Mτ1 + (τ 2
X (cid:62)
1 )XAc
Ac
t
= Mt−1 + X∆t C∆t X (cid:62)
∆t .

1 − τ 2
0 − τ 2

t

(3a)

(3b)

(3c)

In (3a) – (3c), calculating the matrix products XAtX (cid:62)
,
At
requires O(n2(cid:107)zt(cid:107)1),
XAc

and X∆tC∆tX (cid:62)
∆t

X (cid:62)
Ac
t

,

t

O(n2(p − (cid:107)zt(cid:107)1)), and O(n2δt) cost respectively. Given
˜Mτ0, ˜Mτ1 , Mt−1, and zt−1, we evaluate whichever matrix
product in (3a) – (3c) has minimal computational cost and
thereby calculate Mt at the reduced cost of O(n2pt) where
pt (cid:44) min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1, δt}.
To calculate M −1
, we consider the cases n ≤ pt and pt < n
separately. When n ≤ pt, we calculate M −1
by directly
inverting the calculated matrix Mt from (3), which requires
O(n3) cost. When pt < n, we apply the Woodbury matrix
identity on (3). This gives

t

t

M −1

t = ˜M −1
τ0
− ˜M −1

1
(cid:0)
τ0XAt
1 −τ 2
τ 2
0

I(cid:107)zt(cid:107)1 +X (cid:62)
At

˜M −1

τ0XAt

(cid:1)−1X (cid:62)

At

˜M −1
τ0

(4a)

= ˜M −1
τ1
− ˜M −1

1
(cid:0)
τ1XAc
0 −τ 2
τ 2
t
1

Ip−(cid:107)zt(cid:107)1 +X (cid:62)
Ac
t

˜M −1

τ1XAc

t

(cid:1)−1X (cid:62)
Ac
t

(4b)
˜M −1
τ1

= M −1
t−1
− M −1

t−1X∆t

(cid:0)C−1

∆t + X (cid:62)

∆tM −1

t−1X∆t

(4c)

(cid:1)−1X (cid:62)

∆tM −1
t−1.

τ0 , ˜M −1

Given ˜M −1

τ1 , M −1

t−1 and zt−1, we evaluate
whichever expression in (4a) – (4c) has minimal compu-
tational cost to calculate M −1
. Similar to (3), this requires
O(n2pt) computational cost, which arises from matrix in-
version and multiplication.

t

t

τ0 , ˜Mτ1, ˜M −1

Overall, this strategy of using the previous state zt−1 and
the pre-computed matrices ˜Mτ0 , ˜M −1
τ1 , Mt−1
and M −1
t−1, reduces the computational cost of calculating
the matrices Mt and M −1
from Ω(n2p) (as in all current
implementations of Algorithm 1) to O(n2pt). As we show
in Sections 2.3 and 3, in many large-scale applications pt
is orders of magnitude smaller than both n and p, yielding
substantial improvements in computational efﬁciency. Fur-
thermore, we emphasize that the matrices ˜Mτ0, ˜M −1
τ0 , ˜Mτ1 ,
˜M −1
τ1 are ﬁxed for all iterations, and the state zt−1 and ma-
trices Mt−1 and M −1
t−1 only need to be stored temporarily
to generate samples for iteration t and can be deleted after.
Therefore S3 requires minimal additional memory compared
to current implementations.

The full Gibbs samplers for Bayesian linear, logistic, and
probit regression which make use of this pre-computation
are given in Algorithms 2 and 3. The Gibbs samplers for lo-
gistic and probit regression are based on data augmentation
strategies (see, e.g., O’Brien & Dunson (2004); Narisetty
et al. (2019)), and the Gibbs sampler for logistic regres-
sion requires an adjusted pre-computation strategy with
O(cid:0) max{n2pt, n3, np}(cid:1) cost. Appendix B contains deriva-
tions and details of Algorithms 2 and 3 (the implementation
of logistic regression is based on a scaled t-distribution ap-
proximation to the logistic distribution, as commonly done
in the literature; see (Narisetty et al., 2019)).

Scalable Spike-and-Slab

Algorithm 2 Bayesian linear regression with S3
Input: State Ct (cid:44) (βt, zt, σ2
state zt−1, and matrices Mt−1, M −1
t−1.

t ) ∈ Rp × {0, 1}p × (0, ∞),

1: Calculate pt and use (3) to calculate Mt.

if pt ≥ n then invert Mt to calculate M −1
(4) to calculate M −1
.
t
2: Sample βt+1|zt, σ2
t using Algorithm 1 from
N (cid:0)Σ−1
t Σ−1
t X (cid:62)y, σ2
t

t

3: Sample each zj,t+1|βt+1, σ2
(cid:16)

Bernoulli

qN (βj,t+1;0,σ2

(cid:1) for Σt = X (cid:62)X + Dt.
t independently from
t τ 2
1 )
1 )+(1−q)N (βj,t+1;0,σ2

t τ 2
0 )

qN (βj,t+1;0,σ2

t τ 2

(cid:17)

.

else use

for j = 1, ..., p.

4: Sample σ2

t+1|βt+1, zt+1 from

InvGamma

(cid:16) a0+n+p
2

,

b0+(cid:107)y−Xβt+1(cid:107)2
2+β(cid:62)
2

t+1Dt+1βt+1

(cid:17)

.

Output: Ct+1 = (βt+1, zt+1, σ2

t+1), zt, Mt, M −1

t

.

Algorithm 3 Bayesian logistic & probit regression with S3
t ) ∈ Rp × {0, 1}p × Rn ×
Input: State Ct (cid:44) (βt, zt, ˜yt, ˜σ2
(0, ∞)n, states zt−1, ˜σ2

t−1, and matrices Mt−1, M −1
t−1.

1: Logistic

to

Use

calculate Mt

pre-computation
(cid:44)
for Wt = Diag(˜σ2
t ).

regression:
(see Appendix B.3)
In + W −1/2
t XD−1
t
Invert Mt to calculate M −1
.
t
Probit regression: Calculate pt and use (3) to calculate
Mt (cid:44) In + XD−1
t X (cid:62). if pt ≥ n then invert Mt to
else use (4) to calculate M −1
calculate M −1

t X (cid:62)W −1/2

t

2: Sample βt+1|zt, ˜yt, ˜σ2
t X (cid:62)W −1

N (cid:0)Σ−1

t ˜yt, Σ−1

t

.
t using Algorithm 1 from
(cid:1) for Σt = X (cid:62)W −1

t

t X+Dt.

3: Sample each zj,t+1|βt+1, ˜yt, ˜σ2
(cid:16)

Bernoulli

qN (βj,t+1;0,τ 2

qN (βj,t+1;0,τ 2
1 )
1 )+(1−q)N (βj,t+1;0,τ 2
0 )

t independently from

(cid:17)

for j = 1, ..., p.

4: Sample each ˜yi,t+1|βt+1, zt+1, ˜σ2
i,t)I[0,+∞)
i,t)I(−∞,0)

i βt+1, ˜σ2
i βt+1, ˜σ2

N (x(cid:62)
N (x(cid:62)
for i = 1, ..., n.

t independently from

if yi = 1,
if yi = 0

2.3. Analysis of computational complexity

We now investigate the favorable computational complexity
of Algorithms 2 and 3. Proposition 2.1, proved in Appendix
A, gives the computational cost of these Gibbs samplers for
linear and logistic regression, showing an improvement over
existing implementations which have Ω(n2p) cost.
Proposition 2.1 (Computational cost). Algorithm 2 and
Algorithm 3 for probit regression both have a compu-
tational cost of O(cid:0) max{n2pt, np}(cid:1) at iteration t, and
Algorithm 3 for logistic regression has a computational
cost of O(cid:0) max{n2pt, n3, np}(cid:1) at iteration t, where pt =
min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1, δt} for δt = (cid:107)zt − zt−1(cid:107)1.

In Proposition 2.1, (cid:107)zt(cid:107)1 and p − (cid:107)zt(cid:107)1 are the number
of slab covariates and the number of spike covariates re-
spectively at iteration t of the Markov chain, and δt is the
number of covariates switching spike-and-slab states be-
tween iterations t and t − 1 of the Markov chain. Note that
pt ≤ min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1} ≤ p/2 directly. In practice,
there are a variety of scenarios under which pt is signiﬁ-
cantly smaller than p/2.

Sparse zt. Whenever zt is sparse relative to the full di-
mensionality p, we have pt ≤ (cid:107)zt(cid:107)1 (cid:28) p. Sparsity in zt is
a common occurrence in high-dimensional regression with a
sparse signal vector β∗ ∈ Rp, as (cid:107)zt(cid:107)1 often closely approx-
imates the true sparsity s (cid:44) (cid:107)β∗(cid:107)0 with high probability.
This occurs, for instance, in the settings of Narisetty & He
(2014) and Narisetty et al. (2019), where strong model selec-
tion consistency of the continuous spike-and-slab posterior
is established for linear and logistic regression respectively.

Posterior concentration. Even when min{(cid:107)zt(cid:107)1, p −
(cid:107)zt(cid:107)1} is comparable to p/2, concentration of the spike-
and-slab posterior targeted by the Gibbs sampler can lead to
δt and hence pt remaining much smaller than p/2. Proposi-
tion 2.2, proved in Appendix A, calculates the expectation
of δt explicitly in terms of the posterior distribution that is
targeted by our algorithms and the auto-correlation of the
states (zt)t≥0.
Proposition 2.2 (Expected spike-and-slab swap count). For
δt as given in Proposition 2.1,

5: Logistic
Sample
˜σ2
i,t+1|βt+1, zt+1, ˜yt+1 independently from

regression:

InvGamma

(cid:16) v+1

2 , w2ν+(˜yi,t+1−x(cid:62)

2

i βt+1)2

each

(cid:17)

E[δt] =

p
(cid:88)

j=1

P(zj,t = 1)P(zj,t−1 = 0)+

P(zj,t = 0)P(zj,t−1 = 1) − 2cov(zj,t, zj,t−1).

(5)

for i = 1, ..., n, where ν (cid:44) 7.3 and w2 (cid:44) π2(ν − 2)/(3ν)
are constants.
Probit regression: Set each ˜σ2

i,t+1 = 1 for i = 1, ..., n.

Output: Ct+1 = (βt+1, zt+1, ˜yt+1, ˜σ2

t+1), zt, Mt, M −1
t−1

Suppose the Markov chain generated by Algorithm 2 or 3 is
at its stationary distribution π at iteration t − 1. Then,

E[δt] = 2

p
(cid:88)

j=1

varπ(zj,t)(1 − corrπ(zj,t, zj,t−1)).

(6)

Scalable Spike-and-Slab

In (6), note varπ(zj,t) = Pπ(zj,t = 0)Pπ(zj,t = 1) ≤ 1/4
for each component j, with equality only when zj,t ∼
Bernoulli(1/2). Therefore all components j with Pπ(zj,t =
1) close to 0 or 1 do not contribute signiﬁcantly towards δt
in expectation. Such posterior concentration is guaranteed
whenever zt is convergent, be it to the true model selec-
tion vector as in Narisetty & He (2014) and Narisetty et al.
(2019) or to any other value. In such circumstances we can
have δt = (cid:107)zt − zt−1(cid:107)1 = o(p) and even δt = o((cid:107)zt(cid:107)1),
regardless of the magnitude of (cid:107)zt(cid:107)1.

High auto-correlation. High auto-correlation of the
Gibbs sampler can also lead to smaller values of δt and
hence pt. We already see from Proposition 2.2 that, even
for components j with bimodal marginal posterior distribu-
tions, high auto-correlation between successive states zj,t−1
and zj,t can yield lower δt in expectation. Proposition 2.3,
proved in Appendix A, provides an additional exact expres-
sion for δt in terms of the empirical correlation between zt
and zt−1.
Proposition 2.3 (Swap count decomposition). Let τt =
(cid:112)(cid:107)zt(cid:107)1(p − (cid:107)zt(cid:107)1) and ρt be the empirical correlation
between zt and zt−1 (that is, the correlation between zJ,t
and zJ,t−1 when J is uniform on {1, . . . , p}). Then, for δt
as given in Proposition 2.1,

δt = (cid:107)zt(cid:107)1 + (cid:107)zt−1(cid:107)1 −

2(cid:107)zt(cid:107)1(cid:107)zt−1(cid:107)1 + 2ρtτtτt−1
p

.

(7)

Since |ρt| ≤ 1, Proposition 2.3 implies

δt ≥

δt ≤

(cid:0)(cid:107)zt(cid:107)1 − (cid:107)zt−1(cid:107)1
p
(cid:0)(cid:107)zt(cid:107)1 − (cid:107)zt−1(cid:107)1
p

(cid:1)2

(cid:1)2

+

+

(τt − τt−1)2
p
(τt + τt−1)2
p

and

(8)

.

(9)

The lower bound in (8) is a good approximation to δt when
ρt is close to one, which is the case either when the Gibbs
sampler is converging (such that zt becomes stable) or when
it gets stuck (such that zt changes slowly with t). In either
case, τt exhibits similar “stable/stuck” behavior, implying
that the lower bound itself will be close to zero. This sug-
gests δt and hence pt is close to zero when ρt is close to 1,
even if min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1} is not negligible.

Motivated by such discussions and theoretical analysis, we
now empirically examine how pt grows as the number of
observations n, the number of covariates p, and the sparsity
s of the true signal varies. Figure 1 is based on synthetic lin-
ear regression datasets. For each dataset, one Markov chain
is generated using Algorithm 2 to target the correspond-
ing spike-and-slab posterior, from which the mean and one
standard error bars of (pt)10000
t=5000 are plotted. For Figure 1
(Left), we consider datasets with n = 100, varying p with

Figure 1. The S3 cost parameter pt for averaged over iterations
5000 < t ≤ 10000 with one standard error bars, for synthetic
linear regression datasets with varying number of covariates p
(Left), varying number of observations n (Center), and varying
sparsity s (Right). The ground truth sparsity s is also plotted for
comparison. See Section 2.3 for details.

p ≥ n, and a sparse true signal β∗ ∈ Rp with components
β∗
j = 2I{j ≤ s} for sparsity s = 10, and noise standard de-
viation σ∗ = 2. For Figure 1 (Center), we consider datasets
with p = 1000, varying n with n ≤ p, s = 10, and σ∗ = 2.
For Figure 1 (Right), we consider datasets with n = 10s,
p = 1000, and σ∗ = 2 for varying s ≥ 1. Details of the
synthetically generated datasets are in Appendix D.

Figure 1 (Left) shows that both pt is substantially smaller
than both p and n and that it does not increase with the
number of covariates p. Figure 1 (Center) shows that pt
tends to zero as n increases. Figure 1 (Right) shows that
pt decreases as the sparsity s increases. All ﬁgures suggest
that pt is controlled by δt in these settings, because (cid:107)zt(cid:107)1
takes values close to s, and p − (cid:107)zt(cid:107)1 tends to be much
larger than (cid:107)zt(cid:107)1. Overall, Figure 1 highlights that not only
does pt tend to be substantially smaller than p, but it also
tends to be smaller than both n and s. By Proposition 2.1,
this showcases the substantially lower computational cost of
S3 compared to current implementations which cost Ω(n2p)
per iteration.

2.4. Extensions to Scalable Spike-and-Slab

With additional memory capacity and pre-computation, we
can further improve the per-iteration costs of S3.
For the matrices ˜M −1
τ0 and ˜M −1
in Section 2.2, suppose
τ1
X, and X (cid:62) ˜M −1
the matrices X (cid:62)X, X (cid:62) ˜M −1
X are
τ1
τ0
pre-computed. This initial step requires O(np2) computa-
tional cost and O(p2) memory. Then the matrices X (cid:62)
XAt
At
and X (cid:62)
t in (3a) – (3b) correspond to pre-computed
Ac
t
sub-matrices of X (cid:62)X, and calculating Mt using (3a) –
(3b) at iteration t involves matrix addition which only re-
quires O(n2) cost. Similarly, matrices X (cid:62)
XAt and
At

˜M −1
τ0

XAc

048120100020003000Dimension pAverage cost parameter pt048121004007001000No. observations n051015202501020Sparsity sptSparsity sScalable Spike-and-Slab

Table 1. Comparison of S3 with alternatives
min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1, δt} for δt = (cid:107)zt − zt−1(cid:107)1).

(pt

=

MCMC SAMPLER

COST

NA¨IVE
STATE-OF-THE-ART
SKINNY GIBBS
S3 (LINEAR AND PROBIT)
S3 (LOGISTIC)

Ω(p3)
Ω(n2p)
Ω(max{n(cid:107)zt(cid:107)2
1, np})
O(max{n2pt, np})
O(max{n2pt, n3, np})

CONVERGES TO
POSTERIOR
√
√

×
√
√

˜M −1
τ1

X and X (cid:62) ˜M −1
τ1

X (cid:62)
XAc
t in (4a) – (4b) correspond to pre-computed
Ac
t
sub-matrices of X (cid:62) ˜M −1
X respectively
τ0
and do not need to be recalculated at iteration t. Therefore
calculating (cid:0)(τ 2
1 − τ 2
0 )−1I(cid:107)zt(cid:107)1 + X (cid:62)
or
At
(cid:1)−1
(cid:0)(τ 2
˜M −1
1 )−1Ip−(cid:107)zt(cid:107)1 +X (cid:62)
XAc
Ac
τ1
t
each iteration t only requires O((cid:107)zt(cid:107)3
cost respectively.

˜M −1
τ0
in (4a) – (4b) at
1) or O((p − (cid:107)zt(cid:107)1)3)

0 −τ 2

XAt

(cid:1)−1

t

t

t

To sample from (2), consider the cases n ≤ min{(cid:107)zt(cid:107)1, p −
(cid:107)zt(cid:107)1} and min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1} < n separately. When
n ≤ min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1}, we calculate M −1
by di-
rectly inverting the calculated matrix Mt from (3), which
requires O(n3) cost. When min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1} < n,
we avoid calculating M −1
explicitly and instead calcu-
late the matrix vector product M −1
y − v) in Algo-
rithm 1 right-to-left, using whichever expression in (4a) –
(4b) has minimal computational cost. Overall, now the
Gibbs samplers for linear and probit regression require
only O(max{min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1, n}3, np}) computa-
tional cost at iteration t. This provides lower computa-
tional cost for S3 linear and probit regression whenever
min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1, n}3 < n2pt. A similar extension for
logistic regression requires only O(max{n3, np}) compu-
tational cost at iteration t and is given in Appendix B.3.

( 1
σt

t

3. Comparison with Alternatives

In this section we compare S3 with the na¨ıve sampler, the
SOTA exact MCMC sampler based on the sampling algo-
rithm of Bhattacharya et al. (2016), and the Skinny Gibbs
approximate MCMC sampler of Narisetty et al. (2019) for
logistic regression. Table 1 highlights the favorable compu-
tational cost of S3 compared to the na¨ıve and SOTA sam-
plers. The Skinny Gibbs sampler typically has lower com-
putational cost compared to S3 for logistic regression, and
can have lower or higher computational cost than S3 for
probit regression depending on whether (cid:107)zt(cid:107)2
1 ≤ npt or
not. However, unlike S3, the Skinny Gibbs sampler does
not converge to the correct posterior distribution.

Figure 2. Comparison of time per iteration between S3, state-of-
the-art (SOTA) exact MCMC sampler and the Skinny Gibbs ap-
proximate sampler of Narisetty et al. (2019) on synthetic binary
classiﬁcation datasets. See Section 3 for details.

runtimes and statistical performance of S3 with the SOTA
sampler and the Skinny Gibbs sampler. For the Skinny
Gibbs sampler, we use the skinnybasad R package of
Narisetty et al. (2019), which implements only logistic re-
gression. We consider synthetically generated datasets with
a true signal β∗ ∈ Rp where β∗
j = 2I{j ≤ s} for sparsity
s. We consider datasets with n = 10s observations and
p = 100s covariates for varying sparsity s ≥ 1. Details
of the synthetically generated dataset are in Appendix D.
For each synthetic dataset, we run S3 for logistic and probit
regression and the Skinny Gibbs sampler for 1000 iterations,
run the SOTA sampler for 100 iterations, and record the
average time taken per iteration. All timings were obtained
using a single core of an Apple M1 chip on a Macbook Air
2020 laptop with 16 GB RAM.

Figure 2 highlights that the numerical runtimes of S3 are
orders of magnitude faster than the SOTA sampler and com-
parable to the Skinny Gibbs sampler. For example, for
n = 4000 observations, p = 40000 covariates, and sparsity
s = 400, S3 for logistic regression requires 21500ms per
iteration on average, which is approximately 15 times faster
than the SOTA sampler for probit regression (which requires
335000ms per iteration on average) and 2.5 times faster than
the Skinny Gibbs sampler (which requires 55600ms per it-
eration on average), and S3 for probit regression requires
1100ms per iteration on average, which is approximately
50 times faster than the SOTA sampler for probit regres-
sion (which requires 55800ms per iteration on average).
For larger real-life datasets with hundreds of thousands of
covariates, the numerical runtimes of S3 are similarly favor-
able compared to the SOTA sampler. This is showcased in
Section 4, where for a genetics dataset, S3 is 50 times faster
than the SOTA sampler.

To assess the practical impact of computational cost and
asymptotic bias, Figures 2 and 3 compares the numerical

Figure 3 plots the true positive rate (TPR) and the false
discovery rate (FDR) of variable selection based on samples

0e+001e+052e+053e+0510000200003000040000Dimension pTime per iteration (ms)SamplerS^3 LogisticSOTA LogisticSkinny Gibbs LogisticS^3 ProbitSOTA ProbitScalable Spike-and-Slab

Table 2. Synthetic and real-life datasets considered in Section 4.

DATASET

BOROVECKI
CHIN
CHOWDARY
GORDON
LYMPH
MAIZE
MALWARE
PCR
SYNTHETIC BINARY
SYNTHETIC CONTINUOUS

n

31
118
104
181
148
2266
373
60
1000
1000

p

RESPONSE TYPE

22283
22215
22283
12533
4514
98385
503
22575
50000
50000

BINARY
BINARY
BINARY
BINARY
BINARY
CONTINUOUS
BINARY
CONTINUOUS
CONTINUOUS
BINARY

marizes the two synthetic and eight real-world datasets con-
sidered, with further details in Appendix D.

We ﬁrst consider the Gordon microarray dataset (Gordon
et al., 2002) with n = 181 observations (corresponding to
a binary response vector indicating presence of lung can-
cer) and p = 12533 covariates (corresponding to genes
expression levels). Figure 4 shows the marginal posterior
probabilities estimated using samples from S3 and the SOTA
sampler for logistic and probit regression and the Skinny
Gibbs sampler for logistic regression, as well as the cor-
responding average runtimes per iteration. The marginal
posterior probabilities πj (cid:44) Pπ(zj = 1) are estimated by
t=S+1 z(i)
ˆπj =
t )t≥0 are sam-
ples from i = 1, ..., I independent Markov chains generated
using S3. We sample I = 5 independent chains of length
T = 5000 iterations with a burn-in of 1000 iterations for
both S3 and the SOTA sampler. The average runtime per
iteration with one standard error bars are calculated based
on these independent chains.

j,t , where (z(i)

1
I(T −S)

(cid:80)T

(cid:80)I

i=1

Figure 4 (Left) plots ˆπj against j in the decreasing order of
ˆπjs. It shows ˆπjs based on samples from both S3 and the
SOTA sampler. We simulate both S3 and the SOTA sampler
with the same random numbers at each iteration, so that any
differences will be due to numerical imprecision. Figure
4 (Left) shows that the estimates using S3 and the SOTA
sampler are indistinguishable. Furthermore, in this exam-
ple all components of zt are identical between S3 and the
SOTA sampler chains for all iterations t. Despite producing
Markov chains with indistinguishable marginal distributions
and hence statistical properties, Figure 4 (Right) shows
that S3 has approximately 20 and 6 times faster runtime
per iteration than SOTA for logistic and probit regression
respectively. Furthermore, S3 for logistic regression has
approximately 100 times faster runtime per iteration than
the Skinny Gibbs sampler. Overall, Figure 4 highlights the
practical value of S3 over the SOTA sampler and the Skinny
Gibbs sampler.

Figure 3. Average true positive rate (TPR) and false discovery rate
(FDR) of S3 and the Skinny Gibbs approximate sampler (Narisetty
et al., 2019) across 20 independently generated datasets with one
standard error bars. See Section 3 for details.

9−j
4

j = 2

for j ≤ s and β∗

from S3 for logistic and probit regression and Skinny Gibbs
on synthetic binary classiﬁcation datasets. To assess vari-
able selection for signals of varying magnitude, we consider
an exponentially decaying sparse true signal β∗ ∈ Rp such
that β∗
j = 0 for j > s for sparsity
s. The corresponding synthetically generated datasets have
n = 100 observations, varying number of covariates p with
p ≥ n, and sparsity s = 5. For each synthetic dataset,
we implement S3 for logistic and probit regression and the
Skinny Gibbs sampler for 5000 iterations with a burn-in of
1000 iterations and calculate the TPR and FDR from the
samples. We use the same prior hyperparameters for all the
algorithms, which are chosen according to Narisetty et al.
(2019). Additional experimental details are included in Ap-
pendices C. The SOTA sampler is not shown in Figure 3, as
SOTA and S3 are alternative implementations of the same
Gibbs sampler and by deﬁnition have the same statistical
performance. Figure 3 shows that in higher dimensions,
samples from S3 yield signiﬁcantly higher TPR and lower
FDR than the Skinny Gibbs sampler. We observe similar re-
sults for other choices of prior hyperparameters, which give
S3 to either have comparable or more favorable statistical
performance to the Skinny Gibbs sampler.

Overall, Figures 2 and 3 highlight that S3 can have com-
parable or even favorable computational cost to the Skinny
Gibbs sampler, whilst having the correct stationary distri-
bution and more favorable statistical properties in higher
dimensions. Appendix E contains additional simulation re-
sults showcasing S3 performance for individual datasets as
the chain length and the total time elapsed varies.

4. Applications

We now examine the beneﬁts of S3 on a diverse suite of
regression and binary classiﬁcation datasets. Table 2 sum-

Figure 5, plotted with the y-axis on the log-scale, compares
the runtimes of S3 and the SOTA sampler for linear and

0%25%50%75%100%2004006008001000Dimension pTPR0%1%2%3%4%2004006008001000Dimension pFDRSamplerS3 LogisticS3 ProbitSkinny Gibbs LogisticScalable Spike-and-Slab

Figure 4. Comparing Bayesian logistic and probit regression with
S3, SOTA, and Skinny Gibbs on the Gordon microarray dataset
with n = 181 observations and p = 12533 covariates. (Left
and Middle) Marginal posterior probabilities Pπ(zj = 1) esti-
mated using samples from each chain: the recovered S3 and SOTA
probabilities are indistinguishable but differ signiﬁcantly from the
Skinny Gibbs probabilities. (Right) Average runtime per sampler
iteration with one standard error bars. See Section 4 for details.

probit regression on ten regression and binary classiﬁcation
datasets respectively. It plots the average runtimes with one
standard error bars based on 10 independent chains each
of length 1000 and 100 for S3 and the SOTA sampler re-
spectively. Figure 5 shows that S3 has lower runtimes per
iteration compared to the SOTA sampler for all the datasets
considered, with the most substantial speedups for larger
datasets. For example, for the Maize GWAS dataset (Ro-
may et al., 2013; Liu et al., 2016; Zeng & Zhou, 2017) with
n = 2266 observations (corresponding to average num-
ber of days taken for silk emergence in different maize
lines) and p = 98385 covariates (corresponding to single
nucleotide polymorphisms (SNPs) in the genome), S3 re-
quires 650ms per iteration on average, which is 48 times
faster than the SOTA sampler requiring 31300ms per itera-
tion. For researchers, such speedups can reduce algorithm
runtime from days to hours, giving substantial time and
computational cost savings at no compromise to inferential
quality. Appendix E contains additional results of S3 applied
to these datasets including effective sample size (ESS) cal-
culations, marginal posterior probabilities, and performance
under 10-fold cross-validation.

5. Further Work

The following questions arise from our work.

Extensions of S3 to point-mass spike-and-slab priors, as
well as to non-Gaussian tails. Whilst priors given in (1)
are one of the most common formulations employed in
practice, a number of alternatives are available. This in-
cludes point-mass spike-and-slab priors (e.g., Mitchell &

Figure 5. Average runtime per iteration with one standard error
bars for S3 and the SOTA sampler for linear and probit regression
applied to the ten continuous and binary response datasets. See
Section 4 for details.

Beauchamp, 1988; Johnson & Rossell, 2012), where a de-
generate Dirac distribution about zero is chosen for the
spike part, and extensions which consider the heavier-tailed
Laplace distribution for the slab part instead of a Gaussian
distribution (Castillo et al., 2015; Roˇckov´a, 2018; Ray et al.,
2020; Ray & Szab´o, 2021). An extension of S3 would to be
employ similar pre-computation based strategy of Section 2
to MCMC samplers for these alternative formulations.

Convergence complexity analysis of S3. An important
question that is not addressed in this article is the number
of iterations required for S3 or other similar samplers to
converge to the their target posterior distributions. For Gibbs
samplers targeting posteriors corresponding to continuous
shrinkage priors (e.g., Carvalho et al., 2010; Bhattacharya
et al., 2015; Bhadra et al., 2019), much theoretical progress
has been made (Pal & Khare, 2014; Qin & Hobert, 2019;
Bhattacharya et al., 2022; Biswas et al., 2022). Convergence
of Gibbs samplers targeting spike-and-slab posteriors has
been less extensively studied and requires more attention.

Diagnostics to assess the convergence of and asymptotic
variance of S3. Given some time and computational bud-
get constraints, an immediate beneﬁt of S3 is the ability to
run longer Markov chains targeting spike-and-slab posteri-
ors. This can alleviate some concerns linked to burn-in and
asymptotic variance, but convergence and effective sample
size diagnostics (Johnson, 1998; Biswas et al., 2019; Vats &
Knudson, 2021; Vehtari et al., 2021) remain an important
consideration particularly in high-dimensional settings. We
hope to investigate convergence diagnostics in future work.

0.000.050.100.150.20110100100010000Gordon CovariatesMarginal posterior probabilities0.00.10.20.30.4110100100010000Gordon CovariatesMarginal posterior probabilities101001000S3 LogisticSOTA LogisticSkinny Gibbs LogisticS3 ProbitSOTA ProbitSamplerTime per iteration (ms)SamplerS3 LogisticSOTA LogisticSkinny Gibbs LogisticS3 ProbitSOTA Probit10100100010000BoroveckiMalwarePCRLymphChowdaryChinGordonSynthetic BinarySynthetic ContinuousMaizeDatasetsTime per iteration (ms)SamplerS3SOTAScalable Spike-and-Slab

Acknowledgments. We thank Juan Shen for sharing the
PCR and the Lymph Node datasets, Xiaolei Liu and Xiang
Zhou for sharing the Maize GWAS dataset, and Marina Van-
nucci for helpful feedback. NB was supported by the NSF
grant DMS-1844695, a GSAS Merit Fellowship, and a Two
Sigma Fellowship Award. XLM was partially supported by
the NSF grant DMS-1811308.

References

Banerjee, S., Castillo, I., and Ghosal, S. Bayesian inference
in high-dimensional models. Springer volume on Data
Science, 2021.

Barbieri, M. M. and Berger, J. O. Optimal predictive model
selection. Annals of Statistics, 32(3):870 – 897, 2004.
doi: 10.1214/009053604000000238. URL https://
doi.org/10.1214/009053604000000238.

Barbieri, M. M., Berger, J. O., George, E. I., and Roˇckov´a,
V. The Median Probability Model and Correlated Vari-
ables. Bayesian Analysis, 16(4):1085 – 1112, 2021. doi:
10.1214/20-BA1249. URL https://doi.org/10.
1214/20-BA1249.

Bhadra, A., Datta, J., Polson, N. G., and Willard, B. Lasso
Meets Horseshoe: A Survey. Statistical Science, 34
(3):405 – 427, 2019. doi: 10.1214/19-STS700. URL
https://doi.org/10.1214/19-STS700.

Bhattacharya, A., Pati, D., Pillai, N. S., and Dunson,
D. B. Dirichlet–Laplace Priors for Optimal Shrink-
age. Journal of the American Statistical Association,
110(512):1479–1490, 2015. doi: 10.1080/01621459.
2014.960967. URL https://doi.org/10.1080/
01621459.2014.960967.

Bhattacharya, A., Chakraborty, A., and Mallick, B. K.
Fast sampling with Gaussian scale mixture priors
Biometrika, 103
in high-dimensional regression.
(4):985–991, 2016.
10.
1093/biomet/asw042. URL https://doi.org/10.
1093/biomet/asw042.

ISSN 0006-3444.

doi:

Bhattacharya, S., Khare, K., and Pal, S. Geometric er-
godicity of Gibbs samplers for the Horseshoe and its
regularized variants. Electronic Journal of Statistics,
16(1):1 – 57, 2022. doi: 10.1214/21-EJS1932. URL
https://doi.org/10.1214/21-EJS1932.

Biswas, N., Jacob, P. E., and Vanetti, P. Estimating
convergence of markov chains with l-lag cou-
In Advances in Neural Information Pro-
plings.
cessing Systems, volume 32. Curran Associates,
URL https://proceedings.
Inc.,
neurips.cc/paper/2019/file/

2019.

aec851e565646f6835e915293381e20a-Paper.
pdf.

Biswas, N., Bhattacharya, A., Jacob, P. E., and Johndrow,
J. E. Coupling-based convergence assessment of some
gibbs samplers for high-dimensional bayesian regression
with shrinkage priors. Journal of the Royal Statistical So-
ciety: Series B (Statistical Methodology), 2022. doi:
10.1111/rssb.12495. URL https://doi.org/10.
1111/rssb.12495.

Bogdan, M., van den Berg, E., Sabatti, C., Su, W., and
Cand`es, E. J. SLOPE—Adaptive variable selection via
convex optimization. The Annals of Applied Statistics, 9
(3):1103 – 1140, 2015. doi: 10.1214/15-AOAS842. URL
https://doi.org/10.1214/15-AOAS842.

Carvalho, C. M., Polson, N. G., and Scott, J. G. The
horseshoe estimator for sparse signals. Biometrika, 97
(2):465–480, 04 2010.
doi: 10.
1093/biomet/asq017. URL https://doi.org/10.
1093/biomet/asq017.

ISSN 0006-3444.

Castillo, I. and van der Vaart, A. Needles and Straw in a
Haystack: Posterior concentration for possibly sparse
sequences. Annals of Statistics, 40(4):2069 – 2101,
2012. doi: 10.1214/12-AOS1029. URL https://
doi.org/10.1214/12-AOS1029.

Castillo, I., Schmidt-Hieber, J., and van der Vaart, A.
Bayesian linear regression with sparse priors. Annals
of Statistics, 43(5):1986–2018, 2015. doi: 10.1214/
15-AOS1334. URL https://doi.org/10.1214/
15-AOS1334.

Dua, D. and Graff, C. UCI machine learning repository,
2017. URL http://archive.ics.uci.edu/ml.

Flegal, J. M., Hughes, J., Vats, D., Dai, N., Gupta, K., and
Maji, U. mcmcse: Monte Carlo Standard Errors for
MCMC. Riverside, CA, and Kanpur, India, 2021. R
package version 1.5-0.

George, E. I. and McCulloch, R. E. Variable Selec-
Journal of the American
1993.
88(423):881–889,
URL

tion via Gibbs Sampling.
Statistical Association,
doi:
10.1080/01621459.1993.10476353.
https://www.tandfonline.com/doi/abs/
10.1080/01621459.1993.10476353.

George, E. I. and McCulloch, R. E. Approaches for
Bayesian Variable Selection. Statistica Sinica, 7(2):
339–373, 1997.
ISSN 10170405, 19968507. URL
http://www.jstor.org/stable/24306083.

Gordon, G. J. G., Jensen, R. V. R., Hsiao, L.-L. L., Gul-
lans, S. R. S., Blumenstock, J. E. J., Ramaswamy, S. S.,
Richards, W. G. W., Sugarbaker, D. J. D., and Bueno,

Scalable Spike-and-Slab

R. R. Translation of Microarray Data into Clinically
Relevant Cancer Diagnostic Tests Using Gene Expres-
sion Ratios in Lung Cancer and Mesothelioma. Cancer
Research, 62(17):4963–4967, September 2002.

Guan, Y. and Stephens, M. Bayesian variable selection
regression for genome-wide association studies and other
large-scale problems. The Annals of Applied Statistics, 5
(3):1780 – 1815, 2011. doi: 10.1214/11-AOAS455. URL
https://doi.org/10.1214/11-AOAS455.

Hager, W. W. Updating the inverse of a matrix. SIAM
Review, 31(2):221–239, 1989. ISSN 00361445. URL
http://www.jstor.org/stable/2030425.

Hans, C., Dobra, A., and West, M. Shotgun Stochastic
Search for “Large p” Regression. Journal of the American
Statistical Association, 102(478):507–516, 2007. doi:
10.1198/016214507000000121. URL https://doi.
org/10.1198/016214507000000121.

Held, L. and Holmes, C. C. Bayesian auxiliary variable
models for binary and multinomial regression. Bayesian
Analysis, 1(1):145 – 168, 2006. doi: 10.1214/06-BA105.
URL https://doi.org/10.1214/06-BA105.

Ishwaran, H. and Rao, J. S. Spike and slab variable se-
lection: Frequentist and Bayesian strategies. Annals
of Statistics, 33(2):730 – 773, 2005. doi: 10.1214/
009053604000001147. URL https://doi.org/10.
1214/009053604000001147.

Johndrow, J., Orenstein, P., and Bhattacharya, A. Scal-
able Approximate MCMC Algorithms for the Horseshoe
Prior. Journal of Machine Learning Research, 21(73):1–
61, 2020. URL http://jmlr.org/papers/v21/
19-536.html.

Johnson, V. E. A coupling-regeneration scheme for diagnos-
ing convergence in Markov chain Monte Carlo algorithms.
Journal of the American Statistical Association, 93(441):
238–248, 1998.

Johnson, V. E. and Rossell, D. Bayesian Model Selection
in High-Dimensional Settings. Journal of the American
Statistical Association, 107(498):649–660, 2012. doi:
10.1080/01621459.2012.682536. URL https://doi.
org/10.1080/01621459.2012.682536.

Johnstone, I. M. and Silverman, B. W. Needles and straw in
haystacks: Empirical Bayes estimates of possibly sparse
sequences. Annals of Statistics, 32(4):1594 – 1649, 2004.
doi: 10.1214/009053604000000030. URL https://
doi.org/10.1214/009053604000000030.

Kelly, B. C. Some Aspects of Measurement Error in Lin-
ear Regression of Astronomical Data. The Astrophys-
ical Journal, 665(2):1489–1506, aug 2007. doi: 10.

1086/519947. URL https://doi.org/10.1086/
519947.

Liang, F., Song, Q., and Yu, K. Bayesian Subset Mod-
eling for High-Dimensional Generalized Linear Mod-
els. Journal of the American Statistical Association,
108(502):589–606, 2013.
doi: 10.1080/01621459.
2012.761942. URL https://doi.org/10.1080/
01621459.2012.761942.

Liu, X., Huang, M., Fan, B., Buckler, E. S., and Zhang,
Z. Iterative Usage of Fixed and Random Effect Models
for Powerful and Efﬁcient Genome-Wide Association
Studies. PLOS Genetics, 12(2):1–24, 2016. doi: 10.1371/
journal.pgen.1005767. URL https://doi.org/10.
1371/journal.pgen.1005767.

Mitchell, T. J. and Beauchamp, J. J. Bayesian Variable
Selection in Linear Regression. Journal of the Ameri-
can Statistical Association, 83(404):1023–1032, 1988.
ISSN 01621459. URL http://www.jstor.org/
stable/2290129.

Narisetty, N. N. and He, X. Bayesian variable selection with
shrinking and diffusing priors. Annals of Statistics, 42
(2):789 – 817, 2014. doi: 10.1214/14-AOS1207. URL
https://doi.org/10.1214/14-AOS1207.

Narisetty, N. N., Shen, J., and He, X. Skinny Gibbs:
A Consistent and Scalable Gibbs Sampler for Model
Journal of the American Statistical Asso-
Selection.
ciation, 114(527):1205–1217, 2019.
doi: 10.1080/
01621459.2018.1482754. URL https://doi.org/
10.1080/01621459.2018.1482754.

O’Brien, S. M. and Dunson, D. B. Bayesian multivariate
logistic regression. Biometrics, 60(3):739–746, 2004. doi:
10.1111/j.0006-341X.2004.00224.x. URL https://
doi:10.1111/j.0006-341X.2004.00224.x.

Pal, S. and Khare, K. Geometric ergodicity for Bayesian
shrinkage models. Electronic Journal of Statistics, 8(1):
604–645, 2014. doi: 10.1214/14-EJS896. URL https:
//doi.org/10.1214/14-EJS896.

Polson, N. G., Scott, J. G., and Windle, J. Bayesian Infer-
ence for Logistic Models Using P´olya–Gamma Latent
Variables. Journal of the American Statistical Associa-
tion, 108(504):1339–1349, 2013. doi: 10.1080/01621459.
2013.829001. URL https://doi.org/10.1080/
01621459.2013.829001.

Qin, Q. and Hobert, J. P. Convergence complexity analysis
of Albert and Chib’s algorithm for Bayesian probit regres-
sion. Annals of Statistics, 47(4):2320–2347, 2019. doi:
10.1214/18-AOS1749. URL https://doi.org/10.
1214/18-AOS1749.

Scalable Spike-and-Slab

2011.

URL https://proceedings.

Inc.,
neurips.cc/paper/2011/file/
b495ce63ede0f4efc9eec62cb947c162-Paper.
pdf.

Vats, D. and Knudson, C. Revisiting the Gelman–Rubin
Diagnostic. Statistical Science, 36(4):518 – 529, 2021.
doi: 10.1214/20-STS812. URL https://doi.org/
10.1214/20-STS812.

Vats, D., Flegal, J. M., and Jones, G. L. Multivariate output
analysis for Markov chain Monte Carlo. Biometrika,
106(2):321–337, 2019.
doi: 10.
1093/biomet/asz002. URL https://doi.org/10.
1093/biomet/asz002.

ISSN 0006-3444.

Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., and
B¨urkner, P.-C. Rank-Normalization, Folding, and Lo-
calization: An Improved (cid:98)R for Assessing Convergence
of MCMC (with Discussion). Bayesian Analysis, 16
(2):667 – 718, 2021. doi: 10.1214/20-BA1221. URL
https://doi.org/10.1214/20-BA1221.

Yang, Y., Wainwright, M. J., and Jordan, M. I. On the
computational complexity of high-dimensional Bayesian
variable selection. Annals of Statistics, 44(6):2497–2532,
2016. doi: 10.1214/15-AOS1417. URL https://doi.
org/10.1214/15-AOS1417.

Zeng, P. and Zhou, X. Non-parametric genetic prediction
of complex traits with latent Dirichlet process regression
models. Nature Communications, 8(1):456, 2017. doi:
10.1038/s41467-017-00470-2. URL https://doi.
org/10.1038/s41467-017-00470-2.

Zhou, X., Carbonetto, P., and Stephens, M. Polygenic
Modeling with Bayesian Sparse Linear Mixed Models.
PLOS Genetics, 9(2):1–14, 02 2013. doi: 10.1371/journal.
pgen.1003264. URL https://doi.org/10.1371/
journal.pgen.1003264.

Zou, H. and Hastie, T. Regularization and variable se-
lection via the elastic net. Journal of the Royal Sta-
tistical Society: Series B (Statistical Methodology), 67
(2):301–320, 2005.
doi: 10.1111/j.1467-9868.2005.
00503.x. URL https://doi.org/10.1111/j.
1467-9868.2005.00503.x.

Ray, K. and Szab´o, B. Variational bayes for high-
dimensional linear regression with sparse priors. Journal
of the American Statistical Association, 0(0):1–12, 2021.
doi: 10.1080/01621459.2020.1847121. URL https://
doi.org/10.1080/01621459.2020.1847121.

Ray, K., Szabo, B., and Clara, G. Spike and slab variational
bayes for high dimensional logistic regression.
In
Advances in Neural Information Processing Systems,
volume 33, pp. 14423–14434. Curran Associates,
URL https://proceedings.
Inc.,
neurips.cc/paper/2020/file/
a5bad363fc47f424ddf5091c8471480a-Paper.
pdf.

2020.

Romay, M. C., Millard, M. J., Glaubitz, J. C., Peiffer, J. A.,
Swarts, K. L., Casstevens, T. M., Elshire, R. J., Acharya,
C. B., Mitchell, S. E., Flint-Garcia, S. A., McMullen,
M. D., Holland, J. B., Buckler, E. S., and Gardner, C. A.
Comprehensive genotyping of the USA national maize
inbred seed bank. Genome Biology, 14(6):R55, 2013.
doi: 10.1186/gb-2013-14-6-r55. URL https://doi.
org/10.1186/gb-2013-14-6-r55.

Roˇckov´a, V. Bayesian estimation of sparse signals with a
continuous spike-and-slab prior. Annals of Statistics, 46
(1):401 – 437, 2018. doi: 10.1214/17-AOS1554. URL
https://doi.org/10.1214/17-AOS1554.

Scott, J. G. and Berger, J. O. Bayes and empirical-Bayes
multiplicity adjustment in the variable-selection prob-
lem. Annals of Statistics, 38(5):2587 – 2619, 2010. doi:
10.1214/10-AOS792. URL https://doi.org/10.
1214/10-AOS792.

Sereno, M. A Bayesian approach to linear regression in
astronomy. Monthly Notices of the Royal Astronomi-
cal Society, 455(2):2149–2162, 11 2015. ISSN 0035-
doi: 10.1093/mnras/stv2374. URL https:
8711.
//doi.org/10.1093/mnras/stv2374.

Tadesse, M. G. and Vannucci, M. Handbook of Bayesian
Variable Selection. Chapman and Hall/CRC, 2021. doi:
10.1201/9781003089018. URL https://doi.org/
10.1201/9781003089018.

Tibshirani, R.

Journal of

Regression Shrinkage and Selection
the Royal Statistical
Via the Lasso.
Society: Series B (Statistical Methodology), 58(1):
10.1111/j.2517-6161.1996.
267–288, 1996.
tb02080.x. URL https://doi.org/10.1111/j.
2517-6161.1996.tb02080.x.

doi:

Titsias, M. and L´azaro-Gredilla, M.

Spike and slab
variational inference for multi-task and multiple kernel
In Advances in Neural Information Pro-
learning.
cessing Systems, volume 24. Curran Associates,

A. Proofs

Scalable Spike-and-Slab

Proof of Proposition 2.1. Consider Step 1 of Algorithm 2 and Algorithm 3 for probit regression. Given pre-computed
matrices ˜Mτ0 , ˜Mτ0 , Mt−1, M −1
requires O(n2pt) cost by (3) and (4), where
pt = min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1, δt} for δt = (cid:107)zt − zt−1(cid:107)1.
Consider Step 1 of Algorithm 3 for logistic regression. This requires O(max{n2pt, n3}) cost, where the O(n2pt) cost
arises from the calculation of Mt using (21) and the O(n3) cost arises from inverting Mt to calculate M −1

t−1 and state zt−1, calculating Mt, M −1

.

t

t

t

, Step 2 of Algorithms 2 and 3 then requires O(np) cost, which arises from the matrix vector product XD− 1
Given M −1
r
for r ∼ N (0, Ip) in Algorithm 1. By component-wise independence, Step 3 of Algorithms 2 and 3 costs O(p) and Step 4 of
Algorithm 3 cost O(n). Step 4 of Algorithm 3 and Step 5 of Algorithm 3 for probit regression both cost O(1). Step 5 of
Algorithm 3 for logistic regression both costs O(n).

t

2

This gives an overall computational cost of O(max{n2pt, np}) for Algorithm 2 and Algorithm 3 for probit regression at
iteration t, and a cost of O(max{n2pt, n3, np}) for 3 for logistic regression.

Proof of Proposition 2.2. By linearity, E[δt] = (cid:80)p
zj,t and zj,t−1 are on {0, 1}. For each j, we obtain

j=1

P(zj,t (cid:54)= zj,t−1) where for each component j the random variables

P(zj,t (cid:54)= zj,t−1) = P(zj,t = 1, zj,t−1 = 0) + P(zj,t = 0, zj,t−1 = 1)

= (cid:0)P(zj,t = 1) − P(zj,t = 1, zj,t−1 = 1)(cid:1) + (cid:0)P(zj,t−1 = 1) − P(zj,t = 1, zj,t−1 = 1)(cid:1)
= (cid:0)P(zj,t = 1) − cov(zj,t, zj,t−1) − P(zj,t = 1)P(zj,t−1 = 1)(cid:1)+

(cid:0)P(zj,t−1 = 1) − cov(zj,t, zj,t−1) − P(zj,t = 1)P(zj,t−1 = 1)(cid:1)

= P(zj,t = 1)P(zj,t−1 = 0) + P(zj,t = 0)P(zj,t−1 = 1) − 2cov(zj,t, zj,t−1).

When zj,t−1 follows the stationary π, zj,t ∼ zj,t−1 and var(zj,t) = P(zj,t = 1)P(zj,t−1 = 0). Consequently,

P(zj,t (cid:54)= zj,t−1) = 2varπ(zj,t) − 2cov(zj,t, zj,t−1) = 2varπ(zj,t)(1 − corrπ(zj,t, zj,t−1)).

Proof of Proposition 2.3. Note that a2 = a if a only takes the value 0 and 1. This gives

I{zj,t (cid:54)= zj,t−1} = (zj,t − zj,t−1)2 = zj,t + zj,t−1 − 2zj,tzj,t−1.

(10)

Let J be the uniform random variable on the integers {1, . . . , p}. Then,

δt = p(cid:0)EJ (zJ,t) + EJ (zJ,t−1) − 2EJ (zJ,tzJ,t−1)(cid:1),
where the expectation is taken with respect to the random index J. Note that EJ (zJ,t) = (cid:107)zt(cid:107)1/p and VarJ (zJ,t) =
((cid:107)zt(cid:107)1/p)(1 − (cid:107)zt(cid:107)1/p) = τ 2

t /p2. We obtain

(11)

EJ (zJ,tzJ,t−1) = CovJ (zJ,t, zJ,t−1) + EJ (zJ,t)EJ (zJ,t−1) =

ρtτtτt−1 + (cid:107)zt(cid:107)1(cid:107)zt−1(cid:107)1
p2

,

(12)

where ρt = corrJ (zJ,t, zJ,t−1). Combining (11)-(12) yields (7).

B. Algorithm Derivations

B.1. Linear regression with spike-and-slab priors

For linear regression with the continuous spike-and-slab priors in (1), the posterior density of (β, z, σ2) ∈ Rp × {0, 1}p ×
(0, ∞) is given by

π(β, z, σ2|y) ∝N (y; Xβ, σ2)InvGamma

,

(cid:17)

(cid:16)

σ2;

a0
2
1 )(cid:1)zj (cid:0)(1 − q)N (βj; 0, σ2τ 2

b0
2

0 )(cid:1)1−zj .

p
(cid:89)

j=1

(cid:0)qN (βj; 0, σ2τ 2

(13)

(14)

From (13), we can calculate the conditional distributions. We have

Scalable Spike-and-Slab

π(β|z, σ2, y) ∝ N (y; Xβ, σ2In)

p
(cid:89)

j=1

N (βj; 0, σ2τ 2

1 )zj N (βj; 0, σ2τ 2

0 )1−zj

∝ N (y; Xβ, σ2In)N (β; 0, σ2D−1) for D (cid:44) Diag(zτ −2
1 + (1p − z)τ −2
0 )
(cid:27)

(cid:26)

∝ exp

−

(cid:0)β(cid:62)X (cid:62)Xβ − 2β(cid:62)X (cid:62)y + β(cid:62)Dβ(cid:1)

1
2σ2In

∝ N (β; Σ−1X (cid:62)y, σ2Σ−1) for Σ = X (cid:62)X + D,

(cid:0)qN (βj; 0, σ2τ 2

1 )(cid:1)zj (cid:0)(1 − q)N (βj; 0, σ2τ 2

0 )(cid:1)1−zj

π(z|β, σ2, y) ∝

∝

p
(cid:89)

j=1
p
(cid:89)

j=1

(cid:16)

Bernoulli

zi;

π(σ2|β, z, y) ∝ N (y; Xβ, σ2)InvGamma

(cid:17) n

2

∝

(cid:16) 1
σ2

(cid:26)

exp

−

∝ InvGamma

(cid:16)

as given in Algorithm 2.

σ2;

(cid:17)

, and

(cid:17)

(cid:16)

σ2;

qN (βj; 0, σ2τ 2

qN (βj; 0, σ2τ 2
1 )
1 ) + (1 − q)N (βj; 0, σ2τ 2
0 )
a0
b0
,
2
2
(cid:27) (cid:16) 1
1
2σ2 (cid:107)y − Xβ(cid:107)2
σ2
b0 + (cid:107)y − Xβ(cid:107)2
a0 + n + p
2

1
2σ2 b0
2 + β(cid:62)Dβ
(cid:17)

N (β; 0, σ2D−1)
(cid:26)

(cid:17) a0

exp

−

2

2

,

2

(cid:27) (cid:16) 1
σ2

(cid:17) p

2

(cid:26)

exp

−

(cid:27)

1
2σ2 β(cid:62)Dβ

B.2. Probit regression with spike-and-slab priors

Consider the probit regression likelihood, where for each observation i = 1, ..., n, P(yi = 1|xi, β) = 1 − P(yi =
0|xi, β) = Φ(x(cid:62)
i the i-th row of the design matrix X. and Φ that cumulative density function of a univariate
Normal distribution. We obtain yi = I{˜yi > 0} for ˜yi|β ∼ N (x(cid:62)
i β, 1). The Bayesian probit regression model is then given
by

i β) for x(cid:62)

for all j = 1, ..., p

zj

i.i.d.∼ Bernoulli(q)
ind∼ (1 − zj)N (0, τ 2

βj|zj
˜yi|β ind∼ N (x(cid:62)

i β, 1)
yi = I{˜yi > 0}

0 ) + zjN (0, τ 2
1 )

for all i = 1, ..., n
for all i = 1, ..., n.

for all j = 1, ..., p

(15)

For the prior and likelihood in (15), the posterior density of (β, z, ˜y) ∈ Rp × {0, 1}p × Rp is given by

π(β, z, ˜y|y) ∝

(cid:16) n
(cid:89)

I(cid:8)I{˜yi > 0} = yi

(cid:9)N (˜yi; x(cid:62)

i β, 1)

(cid:17)

i=1
p
(cid:89)

(cid:0)qN (βj; 0, τ 2

1 )(cid:1)zj (cid:0)(1 − q)N (βj; 0, τ 2

0 )(cid:1)1−zj .

(16)

From (16), we can calculate the conditional distributions. We obtain

j=1

π(β|z, ˜y, y) ∝ N (˜y; Xβ, In)N (β; 0, D−1) for D (cid:44) Diag(zτ −2

1 + (1p − z)τ −2
0 )

∝ N (β; Σ−1X (cid:62) ˜y, Σ−1) for Σ = X (cid:62)X + D,

π(z|β, ˜y, y) ∝

π(˜y|β, z, y) ∝

p
(cid:89)

j=1
n
(cid:89)

i=1

(cid:16)

Bernoulli

zi;

qN (βj; 0, τ 2

qN (βj; 0, τ 2
1 )
1 ) + (1 − q)N (βj; 0, τ 2
0 )

(cid:17)

, and

N (˜yi; x(cid:62)

i β, 1)I(cid:8)I{˜yi > 0} = yi

(cid:9).

as required for probit regression in Algorithm 3.

B.3. Logistic regression with spike-and-slab priors

Scalable Spike-and-Slab

We ﬁrst describe the Bayesian logistic regression model considered. Consider the logistic regression likelihood, where for
each observation i = 1, ..., n, P(yi = 1|xi, β) = 1 − P(yi = 0|xi, β) = exp(x(cid:62)
i β)
i β) for x(cid:62)
i the i-th row of the design
1+exp(x(cid:62)

matrix X. We obtain yi = I{˜yi > 0} where ˜yi
about x(cid:62)

i β and scale parameter 1.

ind∼ Logistic(x(cid:62)

i β, 1), corresponding to the logistic distribution centered

B.3.1. STUDENT’S t-DISTRIBUTION BASED APPROXIMATION OF THE LOGISTIC REGRESSION LIKELIHOOD.

Following O’Brien & Dunson (2004) and Narisetty et al. (2019), we can approximate Logistic(x(cid:62)
i β + wtν,
where tν denotes a t-distribution with ν degrees of freedom and w is a multiplicative factor. The constants ν (cid:44) 7.3 and
w2 (cid:44) π2(ν−2)
are chosen following O’Brien & Dunson (2004), in order to match the variance of the logistic distribution
and to minimize the integrated squared distance between the respective densities. The Gaussian scale representation of this
t-distribution is

i β, 1) with x(cid:62)

3ν

˜yi|xi, β, ˜σi ∼ N (x(cid:62)

i β, ˜σ2

i ),

˜σ2
i ∼ InvGamma

(cid:16) v
2

,

w2v
2

(cid:17)

,

(17)

where each ˜σ2

i is an augmented variable. The Bayesian logistic regression model is then given by

zj

βj|zj

i.i.d.∼ Bernoulli(q)
ind∼ (1 − zj)N (0, τ 2

˜σ2
i

i.i.d.∼ InvGamma(

ν
2

for all j = 1, ..., p

0 ) + zjN (0, τ 2
1 )
w2ν
2

)

,

for all j = 1, ..., p

(18)

ind∼ N (x(cid:62)
˜yi|β, ˜σ2
i β, ˜σ2
i )
i
yi = I{˜yi > 0}

for all i = 1, ..., n

for all i = 1, ..., n.

Let ˜σ2 denote the vector with entries ˜σ2
(β, z, ˜y, ˜σ2) on Rp × {0, 1}p × Rn × (0, ∞)n is given by

i for i = 1, ..., n. For the prior and likelihood in (18), the posterior density of

π(β, z, ˜y, ˜σ2|y) ∝

p
(cid:89)

j=1
n
(cid:89)

i=1

(cid:0)qN (βj; 0, τ 2

1 )(cid:1)zj (cid:0)(1 − q)N (βj; 0, τ 2

0 )(cid:1)1−zj

I(cid:8)I{˜yi > 0} = yi

(cid:9)N (˜yi; x(cid:62)

i β, ˜σ2

i )InvGamma

(cid:16)

˜σ2
i ;

ν
2

,

w2ν
2

(cid:17)

.

(19)

From (19), we can calculate the conditional distributions. Let W = Diag(˜σ2). We obtain

π(β|z, ˜y, ˜σ2, y) ∝ N (˜y; Xβ, W )N (β; 0, D−1) for D (cid:44) Diag(zτ −2

1 + (1p − z)τ −2
0 )

∝ N (β; Σ−1X (cid:62)W −1 ˜y, Σ−1) for Σ = X (cid:62)W −1X + D,

π(z|β, ˜y, ˜σ2, y) ∝

π(˜y|β, z, ˜σ2, y) ∝

π(˜σ2|β, z, ˜y, y) ∝

∝

p
(cid:89)

j=1
n
(cid:89)

i=1
n
(cid:89)

i=1
n
(cid:89)

i=1

(cid:16)

Bernoulli

zi;

qN (βj; 0, τ 2

qN (βj; 0, τ 2
1 )
1 ) + (1 − q)N (βj; 0, τ 2
0 )

(cid:17)

,

N (˜yi; x(cid:62)

i β, ˜σ2

i )I(cid:8)I{˜yi > 0} = yi

(cid:9), and

N (˜yi; x(cid:62)

i β, ˜σ2

i )InvGamma

(cid:16)

˜σ2
i ;

(cid:17)

ν
2

,

w2ν
2

InvGamma

(cid:16)

˜σ2
i ;

ν + 1
2

,

w2ν + (˜yi − x(cid:62)

i β)2

(cid:17)

2

.

as required for logistic regression in Algorithm 3.

Algorithm 4 An Ω(n2p) sampler of (20) (Bhattacharya et al., 2016)

Scalable Spike-and-Slab

2

Sample r ∼ N (0, Ip), ξ ∼ N (0, In).
Set u = D− 1
(W −1/2
Set v∗ = M −1
t
Return β = u + D−1

t r and calculate v = W −1/2

t X (cid:62)W −1/2

v∗.

t

t

t Xu + ξ.

˜y − v) for Mt = In + W −1/2

t XD−1

t X (cid:62)W −1/2

t

.

A scalable Gibbs sampler for logistic regression. The computational bottleneck of existing Gibbs samplers for logistic
regression is linked to sampling from the full conditional of β ∈ Rp. This is given by

βt+1|zt, ˜σ2

t ∼ N (cid:0)Σ−1

t X (cid:62)W −1

t ˜y, Σ−1

t

(cid:1)

for Σt = X (cid:62)W −1

t X + Dt,

(20)

where t indexes the iteration of the Markov chain, Wt is the diagonal matrix with the vector ˜σ2
elements, and Dt is the diagonal matrix with the vector ztτ −2
from (20), we can use the Ω(n2p) sampler of Bhattacharya et al. (2016), which is given in Algorithm 4.

t populating its diagonal
populating its diagonal elements. To sample

1 + (1p − zt)τ −2

0

Following the strategy in Section 2.2, S3 for logistic regression uses pre-computation to reduce the computational cost of
Algorithm 4. Using the notation from Section 2.2 with Mt (cid:44) In + W −1/2

t X (cid:62)W −1/2

t XD−1

, we note

t

t

Mt = In + W −1/2
= In + W −1/2
= In + W −1/2

t

t

(cid:0) ˜Mτ0 − In + (τ 2
(cid:0) ˜Mτ1 − In + (τ 2
(cid:0)W 1/2

1 − τ 2
0 − τ 2

t−1(Mt−1 − In)W 1/2

t

t

0 )XAc

(cid:1)W −1/2
(cid:1)W −1/2

X T
Ac
t
X T
1 )XAc
Ac
t
t−1 + X∆tC∆tX T
∆t

t

t

(cid:1)W −1/2

t

.

(21a)

(21b)

(21c)

requires O(n2(cid:107)zt(cid:107)1), O(n2(p −
In (21a) – (21c), calculating the matrix products XAtX (cid:62)
, XAc
At
(cid:107)zt(cid:107)1)), and O(n2δt) cost respectively. Given ˜Mτ0 , ˜Mτ1, Mt−1, and zt−1, we evaluate whichever matrix product in
(21a) – (21c) has minimal computational cost and thereby calculate Mt at the reduced cost of O(n2pt) where pt (cid:44)
min{(cid:107)zt(cid:107)1, p − (cid:107)zt(cid:107)1, δt}. To calculate M −1
, we calculate M −1
by directly inverting the calculated matrix Mt, which
requires O(n3) cost. Overall, this strategy reduces the computational cost of calculating the matrices Mt and M −1
from
Ω(n2p) to O(max{n2pt, n3}).

, and X∆tC∆tX (cid:62)
∆t

X (cid:62)
Ac
t

t

t

t

t

Extensions to Scalable Spike-and-Slab for logistic regression. Suppose the matrices X (cid:62)X is pre-computed. This
initial step requires O(np2) computational cost and O(p2) memory. Then the matrices X (cid:62)
t in (21a) –
At
(21b) correspond to pre-computed sub-matrices of X (cid:62)X, and calculating Mt using (21a) – (21b) each iteration t involves
matrix addition and diagonal matrix multiplication which only requires O(n2) cost. To sample from (20), we calculate
M −1
by directly inverting the calculated matrix Mt from (3), which requires O(n3) cost. Overall, now the Gibbs samplers
t
for logistic regression requires O(max{n3, np}) computational cost at iteration t, which is an improvement compared to S3.

XAt and X (cid:62)
Ac
t

XAc

C. Experiment Details

n , τ 2

1 = max{ p2.1

100n , 1} and q = P(zj = 1) such that P((cid:80)p

Figure 3 of Section 3.
In Figure 3, we use the same prior hyperparameters for all the algorithms. Following Narisetty
0 = 1
et al. (2019), we choose τ 2
j=1 I{zj = 1} > K) = 0.1 for
K = max{10, log n}. The true positive rate (TPR) and the false discovery rate (FDR) correspond to the proportion of non-
j=1 I{Pπ(zj = 1) > 0.5}
j that are correctly selected respectively. They are calculated as 1
zero and zero components of β∗
s
j=s+1 I{Pπ(zj = 1) > 0.5} respectively, where the marginal posterior probabilities πj (cid:44) Pπ(zj = 1) are
and 1
p−s
estimated by ˆπj (cid:44) 1
t=1001 zj,t for sample points (zt)t≥0 generated using S3 or Skinny Gibbs. The lines in Figure 3
4000
correspond to the average TPR and FDR across 20 independently generated datasets, and the grey bands correspond to one
standard error of the averages.

(cid:80)5000

(cid:80)p

(cid:80)s

D. Dataset Details

Synthetic continuous response dataset in Section 2.3.
In Figure 1, synthetic linear regression datasets are considered.
For number of observations n and number of covariates p, we generate a design matrix X ∈ Rn×p such that each

Scalable Spike-and-Slab

i.i.d.∼ N (0, 1) for all 1 ≤ i ≤ n and 1 ≤ j ≤ p, which is then scaled to ensure each column has a mean of 0 and
[X]i,j
a standard error of 1. We choose the true signal β∗ ∈ Rp such that β∗
j = 2I{j ≤ s}, where s is the sparsity parameter
corresponding to the number of non-zero components. Given X and β∗, we generate y = Xβ∗ + σ∗(cid:15) for (cid:15) ∼ N (0, In),
where σ∗ = 2 is the Gaussian noise standard deviation.

Synthetic binary response dataset in Section 3.
In Figures 2 and 3, synthetic binary classiﬁcation datasets are considered.
For number of observations n and number of covariates p, we generate a design matrix X ∈ Rn×p such that each
i.i.d.∼ N (0, 1) for all 1 ≤ i ≤ n and 1 ≤ j ≤ p, which is then scaled to ensure each column has a mean of 0
[X]i,j
and a standard error of 1. We choose the true signal β∗ ∈ Rp such that β∗
4 I{j ≤ s}, where s is the sparsity
parameter corresponding to the number of non-zero components. Given X and β∗, we generate yi = I{˜yi > 0} for
˜yi ∼ Logistic(x(cid:62)
i β∗, 1) is the Logistic distribution
with mean x(cid:62)

i is the i-th row of X and Logistic(x(cid:62)

i β∗, 1) for i = 1..., n, where x(cid:62)

i β∗ and scale parameter 1.

j = 2

9−j

Datasets in Section 4. The Malware detection dataset from the UCI machine learning repository (Dua & Graff, 2017) has
n = 373 observations with binary responses and p = 503 covariates, and is publicly available on www.kaggle.com/
piyushrumao/malware-executable-detection.

The Borovecki, Chowdary, Chin and Gordon datasets are all high-dimensional microarray datasets. They are publicly
available on the datamicroarray package in R. The Borovecki dataset has n = 31 observations with binary responses and
p = 22283 covariates. The Chowdary dataset has n = 104 observations with binary responses and p = 22283 covariates.
The Chin dataset has n = 118 observations with binary responses and p = 22215 covariates. The Gordon dataset has
n = 181 observations with binary responses and p = 12533 covariates.

The PCR GWAS dataset has n = 60 observations with continuous responses and p = 22575 covariates, and is publicly
available on www.ncbi.nlm.nih.gov/geo (accession number GSE3330). The Lymph Node GWAS dataset has
n = 148 observations with binary responses and p = 4514 covariates, and has been previously considered (Hans et al.,
2007; Liang et al., 2013; Narisetty et al., 2019). The Maize GWAS dataset has n = 2266 observations with continuous
responses and p = 98385 covariates, and has been previously considered (Romay et al., 2013; Liu et al., 2016; Zeng &
Zhou, 2017). The Lymph Node GWAS and the Maize GWAS datasets are not publicly available.

The synthetic continuous dataset has n = 1000 observations and p = 50000 covariates. The design matrix X is generates
i.i.d.∼ N (0, 1) for all 1 ≤ i ≤ n and 1 ≤ j ≤ p, which is then scaled to ensure each column has a
such that each [X]i,j
mean of 0 and a standard error of 1. The true signal β∗ ∈ Rp is chosen such that β∗
4 I{j ≤ s}, where s is the
sparsity parameter corresponding to the number of non-zero components. Given X and β∗, we generate y = Xβ∗ + σ∗(cid:15)
for (cid:15) ∼ N (0, In), where σ∗ = 2 is the Gaussian noise standard deviation. The synthetic binary classiﬁcation dataset is
generated as in Section 3, with n = 1000 observations and p = 50000 covariates.

j = 2

9−j

E. Additional Experiments

Variable selection performance as a function of time or number of iterations. Figure 6 plots the average true positive
rate (TPR) and the false discovery rate (FDR) of variable selection based on samples from S3 and Skinny Gibbs as the
length of the chains are varied. The TPR and FDR are averaged over 10 independent chains, and one standard error bars are
shown. We consider a synthetic binary classiﬁcation dataset generated using a logistic regression model as in Section 3, with
n = 200 observations, p = 1000 covariates, sparsity s = 10, and an exponentially decaying sparse true signal β∗ ∈ Rp
such that β∗
j = 0 for j > s. The TPR and FDR are calculated as in Section 3 with the marginal
posterior probabilities πj (cid:44) Pπ(zj = 1) now estimated by ˆπj (cid:44) 1
t=1000 zj,t for a burn-in of 1000, a varying chain
length T ≥ 1000, and sample points (zt)t≥0 generated using S3 or Skinny Gibbs. We use the same prior hyperparameters
for all the algorithms, which are chosen according to Narisetty et al. (2019).

for j ≤ s and β∗

j = 2

T −999

(cid:80)T

9−j
4

Figure 6 Left and Center-Left plot the TPR and FDR against the chain length T . It shows that S3 for both logistic and probit
regression have higher TPR and lower FDR than Skinny Gibbs for all chain lengths. Furthermore, S3 for logistic regression
has higher TPR and lower FDR than S3 for probit regression, which is expected as the synthetic dataset for this example is
generated using a logistic regression model. The SOTA sampler is omitted from the Left and Center-Left plots as its output
has the same marginal distribution and statistical performance as S3. Figure 6 Center-Right and Right plot the TPR and FDR
against total time elapsed in seconds to generate samples using S3, SOTA, or Skinny Gibbs chains with a burn-in of 1000

Scalable Spike-and-Slab

Figure 6. Avrage rue positive rate (TPR) and false discovery rate (FDR) plotted against the number of iterations and the total time elapsed
in seconds. We consider S3, SOTA, and the Skinny Gibbs approximate sampler (Narisetty et al., 2019) applied to a synthetic binary
classiﬁcation dataset with n = 200 observations and p = 1000 covariates. The TPR and FDR are averaged over 10 independent chains,
and one standard error bars are shown on the left and center-left plots and omitted on the right and center-right plots for visibility. The
SOTA sampler is omitted from the Left and Center-Left plots as its output has the same marginal distribution and statistical performance
as S3. See Section E for details.

iterations. The standard error bars are now omitted for better visibility. For each time budget, we observe better variable
selection performance from S3 when compared with the slower SOTA implementation or with Skinny Gibbs.

Effective Sample Size of S3 for the datasets in Section 4. Figure 7 shows the Effective Sample Size per iteration and per
unit of time (in seconds) of S3 and the SOTA sampler for the datasets in Section 4. The ESS is calculated using the mcmcse
package (Flegal et al., 2021; Vats et al., 2019) for one S3 chain of length 10000 iterations with a burn-in of 1000 iterations
for each dataset. The average ESS of the β components are then plotted. Figure 7 Right shows that S3 has signiﬁcantly
higher ESS per second compared to the corresponding SOTA sampler for all the datasets considered.

Performance metrics for the datasets in Section 4. Figures 8 – 14 show various performance metrics of S3 for some of
the datasets considered in Section 4. Figures 8 – 14 (Left) plot the marginal posterior probability estimates ˆπj against j in
the decreasing order of ˆπjs, following the setup in Figure 4. For datasets with continuous valued responses, ˆπjs are based on
samples from S3 for linear regression. For datasets with binary valued responses, ˆπjs are based on samples from S3 for
logistic and probit regression, and the Skinny Gibbs sampler from logistic regression. We use samples from 5 independent
chains of length 10000 iterations with a burn-in of 1000 iterations. Estimates based on samples from the SOTA sampler are
not shown, as they implement the same Gibbs sampler as S3 (other than possible numerical discrepancies, as discussed in
Section 4).

Figures 8 – 14 (Center) show the average time taken per iteration with one standard error bars for S3, the SOTA sampler,
and the Skinny Gibbs sampler based on 5 independent chains of length 10000 iterations.

Figures 8 – 14 (Right) show the 10-fold cross-validation average root-mean-square error (RMSE) against the total time
elapsed to run one S3 and one SOTA chain. To compute this evaluation, we partition the observed dataset into 10 folds
uniformly at random and, for each fold k, run a chain conditioned on all data outside of fold k and evaluate its performance on
the held-out data in the k-th fold. The average RMSE is calculated as 1
k=1 rk, where rk is the RMSE for the kth fold. For
10
datasets with continuous valued responses, the quantities rk for linear regression are calculated as ( 1
(yi − ˆyi)2)1/2
|Dk|
where Dk is the kth fold, ˆyi (cid:44)
i βt are the predicted responses, and (βt)t≥0 are samples from S3 and
SOTA targeting the posterior distribution of the kth training set. For datasets with binary valued responses, the quantities

t=1001 xT

1
T −1000

(cid:80)10

(cid:80)T

i∈Dk

(cid:80)

0%20%40%60%050001000015000Chain lengthTPR0.0%2.5%5.0%050001000015000Chain lengthFDRSamplerS^3 LogisticS^3 ProbitSkinny Gibbs Logistic0%20%40%60%0255075Time elasped (s)TPR0.000.020.040.060255075Time elasped (s)FDRSamplerS^3 LogisticS^3 ProbitSkinny Gibbs LogisticSOTA LogisticSOTA ProbitScalable Spike-and-Slab

Figure 7. Effective sample size (ESS) per iteration and per second of S3 and the SOTA sampler for some of the datasets in Section 4. The
ESS is calculated using one S3 chain of length 10000 iterations with a burn-in of 1000 iterations. The ESS per iteration of the SOTA
sampler is omitted from the Left plot as it implements the same Gibbs sampler as S3.

Figure 8. Borovecki dataset with n = 31 observations, p = 22283 covariates and binary valued responses.

(cid:80)

i∈Dk

(cid:80)T

1
T −1000

t=1001 Φ(xT

(yi − ˆpi)2)1/2, where Dk is the kth fold, ˆpi (cid:44)

rk are calculated as ( 1
i βt) and
|Dk|
ˆpi (cid:44)
i βt) are the predicted probabilities for logistic and probit regression respectively, and (βt)t≥0
are samples from S3 and SOTA targeting the posterior distribution of the kth training set. Figures 8 – 14 (Right) plot the
average RMSE against total time elapsed in seconds to generate samples using S3 or SOTA chains with a burn-in of 1000
iterations. The RMSE of the Skinny Gibbs sampler is not available, as the skinnybasad package does not output the full
chain trajectories required for RMSE calculations.

t=1001 Logistic(xT

1
T −1000

(cid:80)T

60%70%80%90%100%BoroveckiChinChowdaryGordonLymphMaizeMalwarePCRSynthetic BinarySynthetic ContinuousDatasetESS per iteration of bSamplerS3 LinearS3 LogisticS3 Probit1e+021e+041e+06BoroveckiChinChowdaryGordonLymphMaizeMalwarePCRSynthetic BinarySynthetic ContinuousDatasetESS per second of bS3 LinearS3 LogisticS3 ProbitSOTA LinearSOTA LogisticSOTA Probit0.00.20.40.60.8110100100010000Borovecki CovariatesMarginal posterior probabilitiesSamplerS3 LogisticS3 ProbitSkinny Gibbs Logistic101001000SamplerTime per iteration (ms)S3 LogisticSOTA LogisticS3 ProbitSOTA ProbitSkinny Gibbs Logistic0.350.400.450.500.55020406080Time elasped (s)Cross−validation RMSES3 LogisticSOTA LogisticS3 ProbitSOTA ProbitScalable Spike-and-Slab

Figure 9. Chin dataset with n = 118 observations, p = 22215 covariates and binary valued responses.

Figure 10. Chowdary dataset with n = 104 observations, p = 22283 covariates and binary valued responses.

Figure 11. Gordon dataset with n = 181 observations, p = 12533 covariates and binary valued responses.

0.00.10.20.30.4110100100010000Chin CovariatesMarginal posterior probabilitiesSamplerS3 LogisticS3 ProbitSkinny Gibbs Logistic301003001000SamplerTime per iteration (ms)S3 LogisticSOTA LogisticS3 ProbitSOTA ProbitSkinny Gibbs Logistic0.550.600.650.700100200300400500Time elasped (s)Cross−validation RMSES3 LogisticSOTA LogisticS3 ProbitSOTA Probit0.000.250.500.751.00110100100010000Chowdary CovariatesMarginal posterior probabilitiesSamplerS3 LogisticS3 ProbitSkinny Gibbs Logistic101001000SamplerTime per iteration (ms)S3 LogisticSOTA LogisticS3 ProbitSOTA ProbitSkinny Gibbs Logistic0.360.400.440.480.520100200300400Time elasped (s)Cross−validation RMSES3 LogisticSOTA LogisticS3 ProbitSOTA Probit0.00.10.20.30.4110100100010000Gordon CovariatesMarginal posterior probabilitiesSamplerS3 LogisticS3 ProbitSkinny Gibbs Logistic101001000SamplerTime per iteration (ms)S3 LogisticSOTA LogisticS3 ProbitSOTA ProbitSkinny Gibbs Logistic0.40.50.70100200300400500Time elasped (s)Cross−validation RMSES3 LogisticSOTA LogisticS3 ProbitSOTA ProbitScalable Spike-and-Slab

Figure 12. Lymph dataset with n = 148 observations, p = 4514 covariates and binary valued responses.

Figure 13. Malware dataset with n = 373 observations, p = 503 covariates and binary valued responses.

Figure 14. PCR dataset with n = 60 observations, p = 22575 covariates and continuous valued responses.

0.00.10.20.31101001000Lymph CovariatesMarginal posterior probabilitiesSamplerS3 LogisticS3 ProbitSkinny Gibbs Logistic5103050SamplerTime per iteration (ms)S3 LogisticSOTA LogisticS3 ProbitSOTA ProbitSkinny Gibbs Logistic0.50.60.7020406080Time elasped (s)Cross−validation RMSES3 LogisticSOTA LogisticS3 ProbitSOTA Probit0.00.10.20.3110100Malware CovariatesMarginal posterior probabilitiesSamplerS3 LogisticS3 ProbitSkinny Gibbs Logistic31030SamplerTime per iteration (ms)S3 LogisticSOTA LogisticS3 ProbitSOTA ProbitSkinny Gibbs Logistic0.050.060.07020406080Time elasped (s)Cross−validation RMSES3 LogisticSOTA LogisticS3 ProbitSOTA Probit0e+001e−042e−043e−04110100100010000PCR CovariatesMarginal posterior probabilitiesSamplerS3 Linear15202530S3 LinearSOTA LinearTime per iteration (ms)S3 LinearSOTA Linear35363701020304050Time elasped (s)Cross−validation RMSES3 LinearSOTA Linear