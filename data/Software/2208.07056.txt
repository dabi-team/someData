2
2
0
2

g
u
A
5
1

]
E
S
.
s
c
[

1
v
6
5
0
7
0
.
8
0
2
2
:
v
i
X
r
a

“To Clean-Code or Not To Clean-Code” A Survey
among Practitioners

Kevin Ljung1 and Javier Gonzalez-Huerta1[0000−0003−1350−7030]

Blekinge Institute of Technology, 371 79, Karlskrona, Sweden
kevinljung98@gmail.com, javier.gonzalez.huerta@bth.se

Abstract.

Context: Writing Clean Code understandable by other collaborators has
become crucial to enhancing collaboration and productivity. However,
very little is known regarding whether developers agree with Clean Code
Principles and how they apply them in practice.
Objectives: In this work, we investigated how developers perceive Clean
Code principles, whether they believe that helps reading, understanding,
reusing, and modifying Clean Code, and how they deal with Clean Code
in practice.
Methods: We conducted a Systematic Literature Review in which we
considered 771 research papers to collect Clean Code principles and a
survey among 39 practitioners, some of them with more than 20 years of
development experience.
Results: So far, the results show a shared agreement with Clean Code
principles and its potential beneﬁts. They also show that developers tend
to write messy code to be refactored later.

Keywords: Clean Code · Survey · Code Quality

1

Introduction

The development of software systems has turned into a collective endeavour
that, in some cases, involves thousands of engineers, distributed globally into
hundreds of teams that have to work with code written by others. In this scenario,
writing code that is understandable by others becomes crucial. The selection of
identiﬁers, and the length of methods and classes, are, among others, principles
that developers should have in mind when writing code.

However, there are no measures to assess code quality universally, and there
is a lack of standards for code quality—even the understanding of what code
quality is somehow diﬀuse [8].

Clean Code [24] has become one of the most relevant craftsmanship practices
for developers worldwide, and several research studies have analyzed its nature
and eﬀects. The principles and practices described in the book have been widely
embraced as a synonym for code quality by many software developers and soft-
ware development organizations. However, the evidence of its use in practice,

 
 
 
 
 
 
2

Ljung and Gonzalez-Huerta

how developers perceive its principles, and how they apply them is scarce in the
software engineering literature.

Although there are studies reporting the Clean Code beneﬁts (e.g., [11,17]),
how to support it (e.g., [18]), analyzing challenges and hindrances of its adop-
tion in practice (e.g., [27]), or diﬀerent aspects of refactoring and how it impacts
on Clean Code or code quality (e.g.,[2,3,4,5,36,6,9,10,15,26,28,30,33]), it is still
unclear how professional developers perceive Clean Code. In this paper, we re-
port a Questionnaire Survey study that explores the practitioners’ perceptions
regarding Clean Code. The goal of the survey is to gain an understanding of:
(i) the degree of agreement with its principles and practices amongst practition-
ers; (ii) whether they believe that Clean Code can help them be more eﬃcient and
eﬀective while reading, understanding, reusing and maintaining code; (iii) and
how they deal with Clean Code in their daily work.

To gather a more complete list of Clean Code principles and practices,
we conducted a Snowballing [34] Systematic Literature Review using a hybrid
method [25], in which we selected 28 papers in addition to the Clean Code sem-
inal book [24].

The remainder of the paper is structured as follows: Section 2 discusses re-
lated works in the area. Section 3 describes the details of the Systematic Liter-
ature Review and the Questionnaire Survey planning and execution. Section 4
reports the results of the study. Section 5 discusses the main ﬁndings. In Sec-
tion 6 discusses the limitations and threats to the validity. Finally, Section 7
draws the main conclusions and discusses further works.

2 Related Work

The Clean Code seminal book somehow reﬁnes one of the aspects of Software
Craftsmanship, with a deep emphasis on writing high-quality, understandable
code, all surrounded by a shared professional culture.

Since then its publication in 2009 there have been several research stud-
ies assessing its beneﬁts (e.g., [11,17]), how to support it (e.g., [18]), analyz-
ing challenges and hindrances to its adoption in practice (e.g., [27]), or diﬀer-
ent aspects of refactoring and how it impacts on Clean Code or code quality
(e.g.,[2,3,4,5,36,6,9,10,15,26,28,30,33]).

Several studies also assess what aﬀects code readability, understandabil-
ity, and maintainability (e.g., [5,7,10,14,18,19,20,29]) and complexity (e.g., [1]).
Börstler et al. [8] also carried out an exploratory study focusing on understanding
code quality.

Some other studies, like the ones reported by Stevenson et al. [32], or Ya-
mashita and Moonen [35], follow a similar methodology, a questionnaire survey
study, but with a diﬀerent focus: code quality aspects or whether developers care
about code smells.

However, we know very little about the practitioners’ perceptions of the Clean
Code and whether and how they use it in practice [27]. It is still unclear how

“To Clean-Code or Not To Clean-Code” A Survey among Practitioners

3

professional developers perceive Clean Code, whether they agree with its prin-
ciples and practices, and whether they believe that Clean Code can help them
be- ing more eﬃcient and eﬀective while reading, understanding, reusing and
maintaining code.

3 Research Methodology

In this paper, we employ two research methods: (i) a Snoballing Systematic
Literature Review (SLR) [34] with a hybrid search method [25] and (ii) a Ques-
tionnaire Survey, following the guidelines by Kitchenham and Pﬂeeger [16].

We focus on the following research questions:

– RQ1: Do developers agree with Clean Code principles?
– RQ2: Do developers believe that clean code eases the process of reading,

understanding, modifying, or reusing code?

– RQ3: How do developers keep their code “Clean”?

3.1 Sistematic Literature Review Planning and Execution

To answer the RQ1, we ﬁrst wanted to gather a more complete list of clean code
principles, going beyond the ones presented by “Uncle” Bob Martin in his Clean
Code seminal book [24]. We conducted the SLR using snowballing [34], using
a hybrid approach, combining the database search to deﬁne the start set with
the iterative citations and references analysis (snowballing) [25]. The objective
of this SLR is not to describe the state-of-the-art regarding the Clean Code, but
rather to identify Clean Code principles in addition to the ones presented in the
Clean Code book.

We deﬁned the following inclusion criteria1:

– Is the paper published in a peer-reviewed English-language journal, confer-

ence or workshop proceedings indexed in Google Scholar?

– Does the paper include the terms “clean code” or “code quality” in the title,

abstract, or full text?

– Is the paper published after 2009?
– Does the paper deﬁne principles and practices of clean code or report their

usage in practice?

We also excluded papers talking only about static analysis techniques unless
there is a strong emphasis on their use in practice. We opted for excluding
papers written before and during 2009 since the Clean Code book was originally
published in 2009. The only exception to the criteria above is the Clean Code
book, which appears in Table 1 as B.

To deﬁne the start set (seed), we carried out a database search in March 2021
using Google Scholar with the search string: “clean code” OR “code quality”.

1 We applied the abovementioned acceptance criteria to deﬁne the start set and during

the snowballing iterations.

4

Ljung and Gonzalez-Huerta

The automated search on Google Scholar found 723 papers that were analyzed
applying the abovementioned inclusion criteria, which resulted in the inclusion
of 9 papers as starting set (designated as S01 to S09 in Table 1).

We then performed four snowballing iterations summarized in Table 1 and
stopped when we achieved saturation (i.e., we did not ﬁnd new papers to in-
clude), applying the inclusion and exclusion criteria following the process de-
scribed above, resulting in the inclusion of 18 papers. Each snowballing iteration
consisted of backward (i.e., references analysis) and forward snowballing (cita-
tions analysis), which improved precision and recall, respectively. In the citation
analysis, we found that some papers had hundreds of citations, most of them
irrelevant, and therefore we narrowed the scope of the citations inspection to the
ones that included “clean code” OR “code quality”, similar to the one used in the
start set deﬁnition.

Stage

Seed

Citations
Screened

and

References

Papers Included

Iteration 1

23 references and 6 citations

S01[18], S02[27], S03[8], S04[20], S05[11], S06[21],
S07[5], S08[10], S09[23]

P1[32], P2[31], P3[35], P4[1], P5[7], P6[19], P7[6],
P8[3], P9[2], P10[15], P11[9]

Iteration 2

10 references and 6 citations

P12[28], P13[30], P14[26], P15[33]

Iteration 3

0 references and 3 citations

P16[4], P17[29], P18[14]

Iteration 4

0 references and 0 citations
Table 1. SLR Snowballing Iteration Statistics and Results

3.2 Questionnaire Survey Design and Execution

The Questionaire Survey allowed us to gather developers’ opinions regarding
Clean Code practices, their beneﬁts, and the way they keep their code “clean”.
The survey was designed following the guidelines by [16]. The questionnaire
contained a mixture of closed and open questions to understand the participants’
views and opinions better. However, for the sake of brevity and clarity, the results
presented in this paper focus only on the closed questions.2

The closed questions in the questionnaire mainly were seven items Likert-
scale questions, including a neutral response, which avoids forcing a positive
or negative choice, which seems adequate for an exploratory survey. The sur-
vey questions were grouped into pages to prevent respondents from being over-
whelmed with a long list of questions.

The questionnaire was developed, distributed, and analyzed using Quest-
back survey software3. The questionnaire was distributed by email and using

2 The questionnaire is available for download in the companion materials in Zenodo

DOI: 10.5281/zenodo.6973656.

3 https://www.questback.com

“To Clean-Code or Not To Clean-Code” A Survey among Practitioners

5

social networks but also spread out to contacts within some companies that re-
distributed the survey within their respective organizations. Therefore we used
non-probabilistic sampling, convenience and snowballing.

Following the guidelines in [22] the Target Audience, Unit of Analysis, Unit
of Analysis, Unit of Observation, and Search Unit are software developers with
industrial experience, whilst the Source of Sampling are the authors’ contacts in
Swedish and Spanish software industry.

The survey began on April 19th, 2021 and had a programmed end on May
18th, 2021. The Total Gross Sample was 645 potential invitees. A total of 110
respondents (i.e., 17.05%) started the questionnaire (Net Participation). How-
ever, only 39 completed the questionnaire (35.35% of the Net Participation).
The completion rate from the Total Gross Sample was 6.05%.

We examined the partially completed questionnaires and found out that there
was a wide range of cases, but mostly few questions were answered. Therefore
we decided to exclude the non-completed questionnaires from the analysis.

For answering RQ1, in which we assess the degree of agreement with the
clean code principles, we applied the Wilcoxon test to check if the responses
were greater than the neutral value=4 (agreement) to see if the responses were
statistically signiﬁcant.

4 Results

4.1 SLR Results: Clean Code Principles

In Table 2 we report the Clean Code principles extracted from the papers in-
cluded in the SLR. We also list the papers in which principles are mentioned,
and whether the papers report evidence of their usage in practice. Most of the
principles listed in Table 2 come from the Clean Code book, with one exception:
Minimize Nesting [29]. The principles listed in Table 2 were the input for the
creation of the survey questionnaire questions that aim at answering RQ1.

4.2 Survey Results

Demographics As shown in 1, from the 39 participants that have completed the
survey, the majority are in the age group 31-40 (Figure 1.(a)), 36 are male 3 are
female (Figure 1.(b)), they have more than 20 years of experience (Figure 1.(c)),
and most of them have a BSc degree, and even some have a PhD (Figure 1.(d)).
In addition to that, as shown in Figure 1.(e), the vast majority of the respondents
are familiar with the Clean Code concept.

Based on these demographics, although the number of participants is not
very big, we believe the participants constitute a relevant group of respondents
for addressing the research questions.

6

Ljung and Gonzalez-Huerta

Type

Principle

Source

General

Naming

Function
and
Method

Comments

Formatting

Object and
Data Structures

Error Handling

Unit Tests

Class

B, P02, S05
P17
B, P01, S02
B, P01, S06

The Boy Scout Rule
Minimize nesting
KISS - Keep It Simple, Stupid!
OCP - Open Closed Principle
Separate Constructing a System from Its use B
B, S01, S02, S04, S06
Use Meaningful Names
B
Use Intention-Revealing Names
B
Pronounceable Names
B
Searchable Names
B
Avoid Disinformation
B
Avoid Mental Mapping
B, P02,S06
Do One Thing
B
Command Query Separation
B
Extract Try-Catch Block
B, S06
Have No Side Eﬀects
B, S02, S07, P09, P11, P14, P17
DRY - Don’t Repeat Yourself
B, S06
Function Arguments
B
Structured Programming
B,S06, P01∗
Methods/Functions should be small
B
Ampliﬁcation
B
Clariﬁcation
B
Explain Yourself in Code
B
Explanation of Intent
B
TODO Comments
B
Warning of Consequences
B, S03, S08, S09, P17
Team Coding Standards
B
Horizontal Formatting - Indentation
B
Dependent Functions
B
Vertical Distance and Ordering
B
Organizing for Change
B
Data/Object Anti-Symmetry
Law of Demeter
B
Prefer Exceptions to Returning Error Codes B
B
Don’t Pass Null
B
Don’t Return Null
B
Write Your Try-Catch Statement First
B
Keeping Tests Clean
B
One Assert per Test
B
Single Concept per Test
B
Class Organization
B, S02, S03, S04, S05, S06, S08,
High Cohesion
P01∗, P8, P9, P11, P12, P14, P16
B, S02, S03, S04, S05, S06, S08,
P1∗, P8, P9, P11, P14, P16
B, P11, P14
B
B, S02, S06, S07, S08, P01∗, P3,
P16
B
B, P4∗, P14, P16
B, S02, S06, P01∗

Encapsulation - Separation of Concerns
Isolating from Change
SRP - Single Responsibility Principle

Minimal Classes and Methods
One Level of Abstraction per Function
Classes should be small

Low Coupling

∗ Reports evidence of the use of the principle in practice.
Table 2. Clean Code Principles extracted from the SLR, including sources where the
principle is mentioned, and wether there is evidence of its usage in practice

“To Clean-Code or Not To Clean-Code” A Survey among Practitioners

7

Fig. 1. Participants’ Demographics Including Age Group, Gender, Years of Program-
ming Experience, Education, and Familiarity with Clean Code.

RQ1: Do developers agree with Clean Code principles? Figure 2 shows
the developers’ degree of agreement with the General, Naming, and Function
an Method Principles. The majority of the participants tend to agree with the
principles listed, being OCP - Open Closed Principle [24] and Extract Try-Catch
block [24] the most controversial in these groups. The Wilcoxon signed-rank test
results (p − value < 0.05) show that the answers were signiﬁcantly greater than
the neutral value (i.e., the answers were greater than the neutral value equals 4)
for all the principles listed in Figure 2. These results conﬁrm that the participants
agree with these Clean Code principles4.

Similarly, Figure 3 shows the degree of agreement with the Comments, For-
matting and Object and Data Structures Clean Code principles. Although there
are some disagreements with the Comments principles. Again, the Wilcoxon
signed-rank test results are statistically signiﬁcant (p − value < 0.05), conﬁrm-
ing that the participants tend to agree with these principles.

Finally,Figure 4 shows the degree of agreement with the Error Handling, Unit
Test, and the Class Clean Code principles. In this case, all the principles were

4 The complete results of the Wilcoxon signed-rank test are available in the companion

materials in Zenodo DOI: 10.5281/zenodo.6973656

8

Ljung and Gonzalez-Huerta

Fig. 2. Developers’ degree of agreement with the General, Naming, and Function, and
Method Principles.

statistically signiﬁcant except two: Write Your Try-Catch First (p − value =
0.47) and One Assert Per Test (p − value = 0.41). Therefore we can conclude
that developers agree with the majority of the Clean-Code principles except 2
(Write Your Try-Catch First and "One Assert Per Test").

RQ2: Do developers believe that clean code eases the process of read-
ing, understanding, modifying, or reusing code? As shown in Figure 5,
participants agree that Clean Code eases the diﬀerent code-related developer
activities, i.e., reading, understanding, reusing and maintaining the code. The
participants also believe that Clean Code improves the quality attributes of the
code, i.e., understandability, reusability, and maintainability (see Figure 5.(b)).
They also agree that reading, reusing, and modifying Clean Code takes a shorter
time than working with “messy” code. The Wilcoxon signed-rank test results

024682468101214161820222426283032343638Number of ResponsesSeparate Constructing fromUsingOCPKISSMinimize NestingBoy Scout Rule(a) - General principles0242468101214161820222426283032343638Number of ResponsesAvoid Mental MappingAvoid DisinformationSearchable NamesPronounceable NamesIntention Revealing NamesMeaningful Names(b) - Naming principles0246810122468101214161820222426283032343638Number of ResponsesMethods / Functions Should BeSmallStructured Programming (1entry & 1 return)Function ArgumentsDRYHave No Side EffectsExtract Try-Catch BlockCommand Query SeparationDo one thing(c) - Function and Method principles1 - Strongly Disagree2 - Disagree3 - Slightly Disagree4 - Neither Agree nor Disagree5 - Slightly Agree6 - Agree7 - Strongly Agree“To Clean-Code or Not To Clean-Code” A Survey among Practitioners

9

Fig. 3. Developers’ degree of agreement with the Comments, Formatting and object
and Data Structures Principles.

(p − value < 0.05) show that the answers were signiﬁcantly greater than the
neutral value for all these questions.

RQ3: How do developers tackle the process of having Clean Code?
Figure 6 shows the participants’ responses to the questions regarding how they
deal with Clean Code:

– whether respondents ﬁnd it more diﬃcult to write clean code initially as

compared to writing “messy” code to be later refactored.

– their perceptions about the impact that refactoring has on code quality.
– whether they believe that requirements need to be clear to be able to write

Clean Code initially.

– whether they believe that it is easier to write clean code at the beginning

(i.e., in early phases) of a project.

– whether writing clean code makes it easier to modify the code later.
– whether they perceive they have less time to write clean code towards the

end of a project due to deadlines (i.e., time pressure).

The participants generally agree with all those statements, but the one states
that deadlines prevent them from writing code. Indeed the Wilcoxon signed-rank
test results (p − value < 0.05) show that the answers were signiﬁcantly greater
than the neutral value for all these questions but the last one. In this case, we also

10

Ljung and Gonzalez-Huerta

Fig. 4. Developers’ degree of agreement with the Error Handling, Unit Test, and the
Class Clean Code principles

Fig. 5. Developers’ perceptions regarding whether Clean Code eases the tasks of read-
ing, understanding, reusing, and maintaining the code, its impact on readability, un-
derstandability, and maintainability, and whether it is faster to interact with Clean
Code.

“To Clean-Code or Not To Clean-Code” A Survey among Practitioners

11

Fig. 6. Participant’s perceptions regarding how they deal with Clean Code.

ran the test with the symmetric hypothesis (to check whether the disagreement
was statistically signiﬁcant), since the visual inspection of Figure 6 seems to have
more disagreeing answers. However, the results were not statistically signiﬁcant
(p − value > 0.05).

5 Discussion

RQ1: Do developers agree with Clean Code principles? As reported
in Section 4.2, the participants agree with most of the Clean Code principles;
there are only two in which the statistical test did not ﬁnd any agreement:
Write Your Try-Catch First and One Assert Per Test. Write Your Try-Catch
First might be controversial since it probably dictates too much about how to
do TDD (or one can argue that it forces you even to do it). TDD is not just
tests-ﬁrst or test-last but making sure developers take ﬁne-grained develop-test
steps and shorten the feedback loops [13]. One might prefer the exception to be
thrown until they have ﬁnished writing and testing the actual code. Regarding
One Assert Per Test, it can also be argued that it can be a source of test clones,
and developers might prefer to have several asserts in one test. The Comments
principles probably generate reluctance since the principles advocate for not
commenting on the code but instead writing Clean and ‘clear”, self-explanatory
code. In the open questions, some of the participants refer to the use of comments
to clarify what the code does, which contradicts the Clean Code principles.
However, most participants tend to agree with the Comments principles.

12

Ljung and Gonzalez-Huerta

RQ2: Do developers believe that clean code eases the process of read-
ing, understanding, modifying, or reusing code? As shown in Figure 5,
developers believe that Clean Code eases the diﬀerent code-related developer
activities, i.e., reading, understanding, reusing and maintaining the code. The
participants also agree with the fact that Clean Code improves the quality at-
tributes in the code (i.e., understandability, reusability, and maintainability). Al-
though there are results that report improvements in maintainability (e.g., [6]),
there are also other studies that report that the impact on understandability is
not that obvious. For example, Ammerlan et al.,[5] report that understandability
can sometimes be hindered when we refactor to clean our code.

We also asked our participants in the questionnaire survey if they believe
that it takes a shorter time to read, understand, modify, or reuse clean code
compared to unclean code. The respondents strongly agreed with this. Only
some developers disagreed that clean code would take a shorter time to read
and understand than unclean code. Arif and Rana [6] reported that if developers
remove code smells in advance and make the code clean, it will take 7% less eﬀort
to add new features to the code than with unclean code. Other research studies
(e.g., [17]) suggest that Clean Code has an impact on the time required to change
current functionality, although it does not seem to have an impact on the time
used to implement new signiﬁcant functionality or solve bugs, or to solve small
coding tasks. Therefore more research seems to be required to analyze these
phenomena in industrial settings.

RQ3: How do developers tackle the process of having Clean Code? As
shown in Figure 6, participants ﬁrst acknowledge that they ﬁnd it more diﬃcult
to write Clean Code and that they tend to write “messy” code that they refactor
later. All participants agree with the fact that refactoring has a positive eﬀect on
code quality and, therefore, helps keeping the code clean. Refactoring is probably
the most popular technique to keep the code clean and repay Technical Debt [11].
However, it can also negatively aﬀect code quality and introduce Technical Debt
Items [36]. In the open questions, in the question “What are/would be the chal-
lenges with refactoring unclean code to become clean code?”, some participants
mentioned “Breaking the Functionality”. However, having enough test coverage
might solve that issue since tests are the “safe net” when refactoring [12].

In addition, we also found that developers agreed that the requirements must
be speciﬁed clearly to write clean code. Only a few developers disagreed with
the previous statement. Therefore, the ﬁndings from both the literature and the
results seem to be aligned. Lucena and Tizzei [23] also mention that integrating
all of the previously mentioned methodologies can help the development team
write the requirements more precisely. It also showed that the development had
a more sustainable velocity and could deliver a more valuable project to the
customer when applying these practices.

There was a mixture of opinions regarding whether participants feel they have
less time to write clean code towards the end of the project due to deadlines (or
when they approach deadlines). These results somehow contrast previous works

“To Clean-Code or Not To Clean-Code” A Survey among Practitioners

13

(e.g., [27]), in which time pressure was one of the main reasons for developers
not writing clean code.

6 Threats to Validity

In this section, we discuss threats to construct, internal, external validity, and
reliability.
Construct Validity concerns mapping the constructs (research questions) to
the questions in the survey questionnaire. To illustrate this mapping, we have
reported the results and discussion per research question. However, it is still
possible that some of the questions do not connect to the construct.
Internal Validity is mainly aﬀected by the fact that we derive the ﬁndings and
conclusions from several questions by ‘merging’ or ‘abstracting out’ the conclu-
sions. This derivation of the results might open the door to researcher bias since
we might cherry-pick and give more value to some questions. We have tried to
minimize this threat by describing and analyzing the questions separately and
also using the Wilcoxon signed-rank test to test the signiﬁcance of each vari-
able individually. The selection of participants in this study is another threat to
the internal validity. The survey was shared through social networks (LinkedIn,
Twitter) and the authors’ industrial contacts. This publication strategy trans-
lates into a limited selection procedure. In addition to that, more than half of
the participants did not ﬁnish the questionnaire (we discarded their responses),
which also represents a self-selection protocol, and might also bias the results.
Respondents with more negative views toward Clean Code or less mainstream
opinions might have left the questionnaire unﬁnished. Finally, there might be
a tendency to respond with best practices instead of reporting bad behaviours.
Once distributed, the researchers had very little control over the questionnaire,
which had only a closing deadline, which somehow removed some validity threats.
Internal Validity is concerned with the potential generalizability of the re-
sults. The number of respondents compared to the potential target population
(the software developers working in the industry around the world) is minimal.
However, it is in line with similar studies in the area. The question is whether
the sample is representative of the Target Audience [16]. It seems that the re-
spondents’ experience is higher as compared to other studies (e.g., [32]).
reliability to enhance reliability, we have published the answers to multiple
choice questions and the results of the statistical test as companion materials in
Zenodo. We keep the answers to the open questions anonymous since, in some
cases, those can help identify the respondents’ aﬃliations, and that would break
the anonymity clauses on the pre-questionnaire consent form.

7 Conclusions and Further Work

This paper reports a Questionnaire Survey with 39 participants that explores
how professional developers perceive Clean Code. To collect Clean Code princi-
ples beyond the seminal book, we conducted a Snowballing Systematic Literature

14

Ljung and Gonzalez-Huerta

Review using a hybrid search strategy. We inspected 771 research papers: 723
papers to deﬁne the start set and 48 papers in the four snowballing iterations.
The Systematic Literature Review resulted in including in the questionnaire one
Clean Code principle not listed in the seminal book (i.e., Minimize Nesting).

The survey results indicate that developers tend to agree with most of the
Clean Code principles, except for two, namely Write Your Try-Catch First and
One Assert Per Test. The results also indicate that developers believe that Clean
Code eases reading, understanding, reusing and maintaining the code. They also
believe that clean code improves readability, understandability, reusability and
maintainability, and that it shortens the time required to read, understand, reuse,
and modify the code (in this case, there exists empirical evidence that conﬁrms
and disproof these results). The results also indicate that developers tend to
write messy code that they refactor later and that refactoring positively aﬀects
code quality. They ﬁnd it more challenging to write clean code initially, that at
the beginning of a project seems to be easier to write clean code. There is no
consensus on whether time pressure impacts the time they can devote to writing
Clean Code.

These results are the ﬁrst step toward understanding what principles devel-
opers are more prone to adopt and how they try to clean their code. Our results
can also help us to understand the support they need and the potential risks
they might incur when refactoring. However, these results are only valid in the
context of the study, and it is, at this point, diﬃcult to establish generalizations.
Therefore there is still a need to conduct similar studies, or replications of this
survey in other contexts, even among contributors to Open-Source systems, to
strengthen the evidence.

Acknowledgements This research was supported by the KKS foundation
through the SHADE KKS Hög project (ref: 20170176) and through the KKS
SERT Research Proﬁle project (ref. 2018010) Blekinge Institute of Technology.

References

1. Ajami, S., Woodbridge, Y., Feitelson, D.G.: Syntax, predicates, idioms — what
really aﬀects code complexity? Empirical Software Engineering 24, 287–328 (2
2019). https://doi.org/10.1007/S10664-018-9628-3

2. Almogahed, A., Omar, M., Zakaria, N.H.: Impact of software refactoring on soft-
ware quality in the industrial environment: A review of empirical studies. In: 2018
Knowledge Management International Conference. (KMICe). pp. 25–27 (2018)
3. Almogahed, A., Omar, M., Zakaria, N.H.: Categorization refactoring techniques
based on their eﬀect on software quality attributes. International Journal of Inno-
vative Technology and Exploring Engineering (IJITEE) 8 (2019)

4. Alomar, E.A., Alrubaye, H., Mkaouer, M.W., Ouni, A., Kessentini, M.: Refactoring
practices in the context of modern code review: An industrial case study at xerox.
Proceedings - International Conference on Software Engineering pp. 348–357 (2021)

“To Clean-Code or Not To Clean-Code” A Survey among Practitioners

15

5. Ammerlaan, E., Veninga, W., Zaidman, A.: Old habits die hard: Why refactor-
ing for understandability does not give immediate beneﬁts. In: 2015 22nd Interna-
tional Conference on Software Analysis, Evolution, and Reengineering. pp. 504–507
(2015)

6. Arif, A., Rana, Z.A.: Refactoring of code to remove technical debt and reduce main-
tenance eﬀort. In: 2020 14th International Conference on Open Source Systems and
Technologies, ICOSST 2020. Institute of Electrical and Electronics Engineers Inc.
(12 2020)

7. Avidan, E., Feitelson, D.G.: Eﬀects of variable names on comprehension: An em-
pirical study. In: 25th International Conference on Program Comprehension. pp.
55–65 (2017)

8. Börstler, J., Störrle, H., Toll, D., van Assema, J., Duran, R., Hooshangi, S., Jeuring,
J., Keuning, H., Kleiner, C., MacKellar, B.: "i know it when i see it" – perceptions
of code quality. In: Proceedings of the 2017 ITiCSE Conference - Working Group
Reports. pp. 70–85. ACM (2017)

9. Dallal, J.A., Abdin, A.: Empirical evaluation of the impact of object-oriented code
refactoring on quality attributes: A systematic literature review. IEEE Transac-
tions on Software Engineering 44, 44–69 (1 2018). https://doi.org/10.1109/TSE.
2017.2658573

10. Dibble, C., Gestwicki, P.: Refactoring code to increase readability and maintain-

ability: A case study. J. Comput. Sci. Coll. 30(1), 41–51 (oct 2014)

11. Digkas, G., Chatzigeorgiou, A.N., Ampatzoglou, A., Avgeriou, P.C.: Can clean new
code reduce technical debt density. IEEE Transactions on Software Engineering 48,
1–18 (2020). https://doi.org/10.1109/TSE.2020.3032557

12. Fowler, M., Beck, K.: Refactoring Improving the Design of Existing Code. Addison

Wesley, 2nd edn. (2018)

13. Fucci, D., Erdogmus, H., Turhan, B.: A dissection of test-driven development :
Does it really matter to test-ﬁrst or to test-last? IEEE Transactions on Software
Engineering 6, 1–20 (2015). https://doi.org/10.1109/TSE.2016.2616877

14. Johnson, J., Lubo, S., Yedla, N., Aponte, J., Sharif, B.: An empirical study as-
sessing source code readability in comprehension. In: Proceedings - 2019 IEEE
International Conference on Software Maintenance and Evolution, ICSME 2019.
pp. 513–523. IEEE (9 2019)

15. Kim, M., Zimmermann, T., Nagappan, N.: A ﬁeld study of refactoring challenges
and beneﬁts. In: Proceedings of the ACM SIGSOFT 20th International Symposium
on the Foundations of Software Engineering - FSE ’12. p. 1. ACM Press (2012)
16. Kitchenham, B.A., Pﬂeeger, S.L.: Principles of survey research: Part 3 - construct-
ing a survey instrument. ACM SIGSOFT Software Engineering Notes p. 20 (2002).
https://doi.org/10.1145/511152.511155

17. Koller, H.G.: Eﬀects of Clean Code on Understandability An Experiment and
Analysis. Master’s thesis, Department of Informatics, University of Oslo (2016)
18. Latte, B., Henning, S., Wojcieszak, M.: Clean code: On the use of practices
and tools to produce maintainable code for long-living. In: Proceedings of the
Collaborative Workshop in Evolution and Maintenance of Long-Living Systems
EMLS2019. vol. Vol-2308, pp. 96–99 (2019), http://CEUR-WS.org

19. Lee, T., Lee, J.B., Peter, H.I.: Eﬀect analysis of coding convention violations on
readability of post-delivered code. IEICE Transactions on Information and Systems
98, 1286–1296 (2015). https://doi.org/10.1587/transinf.2014EDP7327

20. Lerthathairat, P., Prompoon, N.: An approach for source code classiﬁcation to en-
hance maintainability. In: 2011 Eighth International Joint Conference on Computer
Science and Software Engineering (JCSSE). pp. 319–324 (2011)

16

Ljung and Gonzalez-Huerta

21. Lerthathairat, P., Prompoon, N.: An approach for source code classiﬁcation using
software metrics and fuzzy logic to improve code quality with refactoring tech-
niques. In: Proceedings of the International Conference on Software Engineering
and Computer Systems. pp. 478–492. Springer Berlin Heidelberg (2011)

22. Linåker, J., Sulaman, S.M., Höst, M., Mello, R.M.D.: Guidelines for conducting
surveys in software engineering v. 1.1. Tech. rep., Department of Computer Science,
Lund University (2015)

23. Lucena, P., Tizzei, L.P.: Applying software craftsmanship practices to a scrum
project: an experience report. In: Proceedings of 2016 Workshop on Social, Human
and Economics Aspects of Software (2016), http://arxiv.org/abs/1611.05789
24. Martin, R.C.: Clean Code - A Handbook of Agile Software Craftmanship. Prentice

Hall (2009)

25. Mourão, E., Kalinowski, M., Murta, L., Mendes, E., Wohlin, C.: Investigating
the use of a hybrid search strategy for systematic reviews. In: Empirical Software
Eninneering and Measurements ESEM (2017)

26. Pantiuchina, J., Zampetti, F., Scalabrino, S., Piantadosi, V., Oliveto, R., Bavota,
G., Penta, M.D.: Why developers refactor source code. ACM Transactions on Soft-
ware Engineering and Methodology (TOSEM) 29 (9 2020). https://doi.org/10.
1145/3408302

27. Rachow, P., Schroder, S., Riebisch, M.: Missing clean code acceptance and support
in practice - an empirical study. In: Proceedings - 25th Australasian Software
Engineering Conference, ASWEC 2018. pp. 131–140. IEEE (2018)

28. Sae-Lim, N., Hayashi, S., Saeki, M.: Toward proactive refactoring: An exploratory
study on decaying modules. In: Proceedings - 2019 IEEE/ACM 3rd International
Workshop on Refactoring, IWOR 2019. pp. 39–46. IEEE (5 2019)

29. Sedano, T.: Code readability testing, an empirical study. Proceedings - 2016 IEEE
29th Conference on Software Engineering Education and Training, CSEEandT
2016 pp. 111–117 (5 2016). https://doi.org/10.1109/CSEET.2016.36

30. Sharma, T., Suryanarayana, G., Samarthyam, G.: Challenges to and solutions
for refactoring adoption: An industrial perspective. IEEE Software 32, 44–51 (11
2015). https://doi.org/10.1109/MS.2015.105

31. Steidl, D., Deissenboeck, F., Poehlmann, M., Heinke, R., Uhink-Mergenthaler, B.:
Continuous software quality control in practice. In: 2014 IEEE International Con-
ference on Software Maintenance and Evolution. pp. 561–564 (2014)

32. Stevenson, J., Wood, M.: Recognising object-oriented software design quality: a
practitioner-based questionnaire survey. Software Quality Journal 26, 321–365 (6
2018). https://doi.org/10.1007/s11219-017-9364-8

33. Vakilian, M., Chen, N., Negara, S., Rajkumar, B.A., Bailey, B.P., Johnson, R.E.:
Use, disuse, and misuse of automated refactorings. In: International Conference on
Software Engineering. pp. 233–243 (2012)

34. Wohlin, C.: Guidelines for snowballing in systematic literature studies and a repli-
cation in software engineering. In: Proceedings of the 18th International Conference
on Evaluation and Assessment in Software Engineering - EASE ’14. pp. 1–10 (2014)
35. Yamashita, A., Moonen, L.: Do developers care about code smells? an exploratory
survey. In: 2013 20th Working Conference on Reverse Engineering (WCRE). pp.
242–251. IEEE (2013)

36. Zabardast, E., Gonzalez-huerta, J., Smite, D.: Refactoring , bug ﬁxing , and new
development eﬀect on technical debt : An industrial case study. In: 46th Euromicro
Conference on Software Engineering and Advanced Applications (SEAA). pp. 376–
384. IEEE (2020)

