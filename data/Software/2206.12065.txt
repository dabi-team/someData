2
2
0
2

t
c
O
7

]

O
R
.
s
c
[

2
v
5
6
0
2
1
.
6
0
2
2
:
v
i
X
r
a

ORIGINAL ARTICLE

Eco-driving for Electric Connected Vehicles at Signalized
Intersections: A Parameterized Reinforcement Learning approach

Xia Jianga Jian Zhanga and Dan Lib

aSchool of Transportation, Southeast University, Nanjing, Jiangsu, China; bSchool of
Engineering, Tibet University, Tibet, China

ARTICLE HISTORY
Compiled October 10, 2022

ABSTRACT
This paper proposes an eco-driving framework for electric connected vehicles (CVs)
based on reinforcement learning (RL) to improve vehicle energy eﬃciency at sig-
nalized intersections. The vehicle agent is speciﬁed by integrating the model-based
car-following policy, lane-changing policy, and the RL policy, to ensure safe opera-
tion of a CV. Subsequently, a Markov Decision Process (MDP) is formulated, which
enables the vehicle to perform longitudinal control and lateral decisions, jointly op-
timizing the car-following and lane-changing behaviors of the CVs in the vicinity
of intersections. Then, the hybrid action space is parameterized as a hierarchical
structure and thereby trains the agents with two-dimensional motion patterns in a
dynamic traﬃc environment. Finally, our proposed methods are evaluated in SUMO
software from both a single-vehicle-based perspective and a ﬂow-based perspective.
The results show that our strategy can signiﬁcantly reduce energy consumption
by learning proper action schemes without any interruption of other human-driven
vehicles (HDVs).

KEYWORDS
eco-driving, reinforcement learning, connected vehicles, signalized intersections

1.

Introduction

Transportation system has been recognised as one of the major sources of energy
consumption and air pollution. The authorities have tried their utmost to build a
sustainable and eﬃcient transportation system by applying advanced technologies.
One promising way to facilitate the progress is to increase the market penetrating rate
(MPR) of electric vehicles with eﬀective energy management strategies. On one hand,
the electriﬁcation of vehicles promote the use of cleaner energy, which is of beneﬁt to
achieve low-emission outcomes (Li et al. 2019). On the other hand, making full use
of existing energy can help reduce energy consumption during the operation period of
the vehicles. As an energy eﬃcient purpose is inseparable from advanced technologies,
the application of vehicle-to-everything (V2X) communication to traﬃc control and
management have triggered a possible revolution towards an ecological and eﬃcient
transportation system. A plenty of research-based evidences can be found to support
the view that such connected technologies are promising to enhance traﬃc performance
(Jia and Ngoduy 2016; Talebpour and Mahmassani 2016; Ard et al. 2021; Shi et al.

CONTACT Jian Zhang. Email: jianzhang@seu.edu.cn

 
 
 
 
 
 
2021; Jiang et al. 2022). Based on the belief that the communication modules will be
deployed simultaneously on both vehicles and traﬃc infrastructures , the vehicle-to-
infrastructure (V2I) technique can be widely implemented in the near future, which
enables the connected vehicles (CVs) to obtain the information transmitted from traﬃc
infrastructures such as traﬃc lights and ramp meterings, with the intent to help CVs
make better decisions to improve energy eﬃciency.

One of a successful applications of V2I communication in urban traﬃc is the deploy-
ment of signal phase and timing (SPaT) message, furnishing CVs with signal status
information in a consistent manner (Mintsis, Vlahogianni, and Mitsakis 2020). Since
traﬃc lights in urban signalized intersections can interrupt the operation of vehicles,
CVs may avoid the red phases and achieve energy-eﬃcient driving through the use of
real-time SPaT data, while idling at the traﬃc signals is one major cause of increased
energy consumption and greenhouse gas emissions(Kim and Kim 2020). Given this,
the integration of SPaT information and advanced control policies initiate eco-driving
concept, which has potential to signiﬁcantly reduce energy consumption in the prox-
imity of signalized intersections (Li et al. 2018). Generally, the control policies take
vehicle location, SPaT data, and local traﬃc state as input and generate speed proﬁle
for the controlled vehicle (i.e., the ego vehicle), and thus enhance the performance
of the vehicle in terms of energy conservation (Asadi and Vahidi 2011; Mintsis et al.
2021; Dong et al. 2021; Ma et al. 2021).

Eco-driving strategy at signalized intersections is usually accomplished by keeping
up a smooth driving speed, refraining from aggressive acceleration, and avoiding long-
time idling at the intersections, resulting in higher performance in energy economy and
travel eﬃciency (Gao et al. 2019; Typaldos, Papamichail, and Papageorgiou 2020). By
drawing upon the V2I communication, relevant strategies place value on the design of
control algorithms in assorted traﬃc contexts, aiming at explicitly optimize the energy-
saving speed proﬁles of vehicles. As such, the existing eco-driving strategies can be
fundamentally classiﬁed into three broad categories: rule-based strategy, optimization-
based strategy, and learning-based strategy. The mathematical formulation of each
kind of strategy is diﬀerent, but energy-saving performances with varying degrees are
reported for all of the above methods.

Predeﬁned rules are provided for a CV in rule-based strategies when the ego vehi-
cle approaching the intersection. The basic ideas of rule-based methods are similar:
they all divided the state into several categories according to the signal status and
vehicle status, provided that the SPaT messages are available from V2I communica-
tion (Rakha and Kamalanathsharma 2011; Xia, Boriboonsomsin, and Barth 2013; Ci
et al. 2019). As a result, the trajectory of the ego vehicle can be simply planned with
classical kinematics formulas, trying to let the vehicle cross the junction within green
phase.

An advantage of the rule-based methods is that they are easy to be implemented.
However, the approaches cannot adapt to dynamic traﬃc, because it is diﬃcult to take
the surrounding vehicles into account. For example, if the vehicle ahead of the ego CV
slow down unexpectedly or the vehicle in adjacent lane insert to the placement in front
of the ego CV, the rule-based methods may lose their eﬃcacy, whereas some of them
only consider the signal status and the vehicle status. Moreover, the rule-based nature
is prone to lead local optimum as the speed change rate is usually ﬁxed. In this case,
the results can leave much to be desired.

In order to achieve better performance, the optimization-based approaches are ex-
tensively studied. One representative way in optimization-based strategies is to for-
mulate an optimal control model and generate corresponding speed proﬁles, which is

2

usually attained by pontryagin’s minimum principle (Jiang et al. 2017) or dynamic
programming (Kuriyama, Yamamoto, and Miyatake 2010; Feng, Yu, and Liu 2018).
Since it is diﬃcult to deal with the nonlinearness, time varying and environment un-
certainty in such vehicle control problems, some studies adopt model predictive control
(MPC) to obtain near-optimal control laws (Asadi and Vahidi 2011; Kamal, Taguchi,
and Yoshimura 2015; Zhao et al. 2018; Wang and Lin 2020). Another way is estab-
lishing a stage-wise approximation mathematical programming model and solving the
model through heuristic algorithms, thereby providing an optimal speed control strat-
egy. Chen et al. (2014) developed an optimization model to determine the optimal
longitudinal trajectory, which is resolved by a genetic algorithm (GA). Li et al. (2018)
applied a hybrid algorithm merging GA and particle swarm optimization (PSO) to
solve the multi-stage optimization model. Whereas the analytical solving process of a
complex mathematical program model is time-consuming, the MPC approaches and
heuristic algorithms try to come to an equitable compromise among optimality and
time cost.

There is no doubt that the optimization-based strategies have shown great potential
to implement eco-driving control. Nevertheless, there still exists some major issues in
this kind of strategies. Firstly, solving the mathematical models in each time step
will impose a heavy burden on the on-board calculation units. The strategies may
have limited applicability if the algorithm cannot be real-time. Secondly, it is hard to
consider multiple vehicle behaviors (e.g., car following, lane changing, overtaking, etc)
by adding a series of cost function items and constraints to the model, while a complex
model can be too burdensome to be numerically solved. Thus, more adaptive strategy
is supposed to be implemented to meets the requirements of real-time and dynamic.
Recently, with the rapid development of artiﬁcial intelligence, the reinforcement
learning (RL) algorithms have attracted the gaze of researchers in both optimal con-
trol community and transportation community. Through conducting the learning pro-
cedure, an agent can select an optimal action for each observation to maximize its
cumulative expected reward (i.e., usually is the optimization goal), and this process
can be usually achieved in a real-time fashion (Chow et al. 2021). By leveraging deep
neural networks, the integrating outcome, which is known as deep reinforcement learn-
ing (DRL), has the ability to approximate the optimal policy in most of the control
tasks, even with high-dimension state space . Consequently, the potent DRL-based
methods are introduced to control the vehicle in the vicinity of signalized intersec-
tions.

Shi et al. (2018) used a regular Q-learning algorithm to implement eco-driving for
a CV in free ﬂow condition. They took the total CO2 emission as the reward signal,
aiming at optimize the driving behavior of the ego vehicle by yielding a discrete ac-
celeration rate in each time step. As one of the value-based reinforcement learning
algorithms, the Q-learning approach cannot control the vehicle in continuous acceler-
ation space, and thereby causes local optimum and uneven trajectory in most cases.
The framework developed by Mousa et al. (2020) provided insight into the DRL-based
eco-driving system, which introduced deep Q network (DQN) to improve the fuel per-
formance of the controlled CV. However, major disadvantage similar to the work of Shi
et al. (2018) was encountered in their study, namely, the losing eﬃcacy in continuous
action space. As the policy-based algorithms are capable of learning control policies
with continuous action sets, researchers tend to apply typical policy-based DRL algo-
rithms to surmount the barrier of continuous space. Zhou, Yu, and Qu (2020) proposed
a car following model based on the deep deterministic policy gradient (DDPG) algo-
rithm, which is proved can improve travel eﬃciency, fuel consumption and safety at

3

Table 1.: The summary of existing RL-based eco-driving frameworks

Study

Vehicle type

RL algorithm

car follow lane change

action space

Shi et al. (2018)
Mousa et al. (2020)

GV
GV
Zhou, Yu, and Qu (2020) GV
EV
EV
GV
EV

Qu et al. (2020)
Wegener et al. (2021)
Guo et al. (2021)
Bai et al. (2022)

Q-learning
DQN
DDPG
DDPG
TD3
DDPG+DQN
Dueling DQN

√
√
√
√
√
√
√

-
-
-
-
-
√
√

Discrete
Discrete
Continuous
Continuous
Continuous
Hybrid
Discrete

* GV - Gasoline Vehicle; EV - Electric Vehicle

an isolated signalized intersection. Similar study was also formulated for EV when
traﬃc oscillations were concerned (Qu et al. 2020). Guo et al. (2021) combined DDPG
and DQN to formulate a hybrid reinforcement learning framework, and the developed
approach can not only control the longitudinal motion, but also optimize the lateral
decision of the ego vehicle. Wegener et al. (2021) studied the application of twin-
delayed deep deterministic policy gradient (TD3) algorithm, which conformed to the
similar basic idea of DDPG, but introduced a series of tricks to tackle the Q function
overestimate problem of DDPG. Their simulation study proclaimed that the DRL al-
gorithm can adapt to the eco-driving scenarios with the presence of other surrounding
vehicles.

Focusing on the energy-eﬃcient driving process at signalized intersections, the ex-
isting RL-based frameworks diﬀer from each other in multiple dimensions, such as the
category of the algorithm, the studied vehicle type, the driving scenario, the design of
the action space, etc. Table 1 presents the summary of relevant studies. It is found that
the lane changing behavior is rarely taken into account when developing eco-driving
strategies. Lane changing during the driving process may bring about a long-term en-
ergy beneﬁt, especially under the situations that the preceding vehicle or queue can
interfere with the running of a CV. Although Guo et al. (2021) considered the combi-
nation of car following and lane changing as their hybrid RL-based approach simply
formulated two Markov Decision Processes (MDP) for longitudinal motion and lateral
decision of the ego vehicle. Therefore, they only control the two dimensional motions
separately by DQN and DDPG. The inherent mechanism of the hybrid MDP is not
clear, and it may lead to local optimum because the movements in two dimensions
are not optimize jointly. It should be noted that the work of Bai et al. (2022) also
considers lane changing, but the nature of DQN determines that it cannot guarantee
a smooth trajectory and corresponding global optimum, whereas the running of a ve-
hicle should be continuous process. In this case, in order to handle the application of
discrete-continuous hybrid action space, we introduce the parametrized action space
and compose an uniﬁed MDP for eco-driving on the signalized urban road. Diﬀer-
ing from traditional RL methods, the proposed parametrized reinforcement learning
(PRL) controller is capable of optimizing the longitudinal motion and the lateral mo-
tion jointly and stipulate a better performance, further exploiting the energy-saving
potential of electric and connected vehicles through DRL-based speed control.

In addition, since the inappropriate driving behaviors usually undermine the holis-
tic performance of traﬃc, the operation of surrounding normal human-driven vehicles
(HDVs) may be interrupted by the CVs controlled by eco-driving controllers (Zhao
et al. 2018). We call this kind of strategy ”selﬁsh” eco-driving, which may caused
by frequent lane changing or unexpected deceleration. Compared with most of the

4

other learning-based strategies, which usually pay close attention to single-vehicle
performance, in this paper, we will restrict CVs from frequent lane changing and un-
expected change of acceleration when designing the reward function, and test whether
the proposed strategy has detrimental inﬂuence on other HDVs.

In summary, the particular contributions and the novelty of this paper lie within

the following points:

• An MDP with discrete-continuous hybrid action space is established for CVs in
signalized urban scenario, further tapping the energy-saving potential of CVs by
searching in the longitudinal acceleration/deceleration space and the lateral lane
changing space.

• Instead of simply blending algorithms or discretize the hybrid action space, the
proposed PRL algorithm can be naturally applied to MDP with hybrid action
space, which provides a paradigm for such microscopic CV control problems.
• The proposed strategy can be harmonious by designing a restricted reward func-
tion so that to eliminate the potential adverse impact of the selﬁsh behaviors of
the ego vehicles.

• The developed methodology is tested in both microscopic (single vehicle-based)
perspective and macroscopic (mixed traﬃc ﬂow-based) perspective, illuminating
the energy-saving capability of the learning-based approach.

The organization of the remainder paper is as follows: The preliminary of RL is
presented in Section 2. Section 3 introduces the system components regarding the
eco-driving process of CVs based on RL controller. Section 4 describes the establish-
ment of the MDP with the implementation of the PRL algorithm. Numerical results
are presented and discussed in Section 5 by carrying out simulations in microscopic
simulation platform SUMO (Lopez et al. 2018). Finally, some concluding remarks are
provided in Section 6.

2. Background of Reinforcement Learning

The environment (i.e., traﬃc environment) of the RL agent (i.e., the ego vehicle) is
modeled by an MDP M = {S, A, P, r, γ} in reinforcement learning framework, where
S denotes the state space, A denotes the action space, P is the Markov transition
probability distribution, r is the reward function, and γ ∈ [0, 1] is the discount factor
that measures the relative value of future rewards and current reward.

In the general setup of RL, an agent interacts with the environment sequentially
as follows. At time step t, the agent observes a state st ∈ S and selects an action
at ∈ A, and then the agent receives an immediate reward value calculated by r(st, at).
Subsequently, the next state becomes st+1 P (st+1|st, at). Let Rt = (cid:80)
j≥t γj−tr(sj, aj)
be the cumulative discounted reward starting from time step t, for any policy π, we
deﬁne the state-action value function as Qπ(s, a) = E[Rt|st = s, at = a, π], where
at+k ∼ π(·|st+k) for all at+k and st+k for k ∈ [t + 1, ∞). Meanwhile, we deﬁne the
state value function as vπ(s) = E[Rt|st = s, π]. According to the Bellman equation,
we have vπ(s) = (cid:80)
a∈A π(a|s)Qπ(s, a), which illuminates the relationship between the
two function. The goal of the agent is to learn the optimal policy π∗ that can maxi-
mize its expected cumulative discounted reward from the initial state, which can be
represented as J(π) = E(R0|π∗), while the learning procedure is usually accomplished
by estimating the optimal state-action value function Qπ∗.

Value-based methods and policy-based methods are two categories of RL algorithms,

5

the former estimates Qπ∗ and generates actions in a greedy way, while the latter di-
rectly learns the optimal policy π∗ by optimizing J(π). As one typical representative of
value-based methods, the Q-learning algorithm (Watkins and Dayan 1992) is deduced
by Bellman equation:

Q(s, a) = E

rt,st+1

[rt + γ max
a(cid:48) ∈A

Q(st+1, a

(cid:48)

)|st = s, at = a]

(1)

which takes Qπ∗ as the unique solution. The learning process iteratively updates the
function Q(s, a) through Monte-Carlo method over a series of sample transitions. Con-
ventionally, Q(s, a) is stored in tabular data structure for ﬁnite state space conﬁgura-
tion. Nevertheless, the problem of large state space arises along with the increase of
the task complexity, When the state space is too large to be stored in computer mem-
ory or Q-value cannot be retrieved in real time, the function approximation approach
is introduced, especially promoted by the development of deep neural networks. DQN
utilizes a deep neural network Q(s, a; θ) ≈ Q(s, a) with parameter set θ to approxi-
mate Qπ∗ (Mnih et al. 2015). The learning of the agent is fundamentally based on the
gradient descent theory, which takes the following loss function:

Lt(θ) = {Q(st, at; θ) − [rt + γ max
a(cid:48) ∈A

Q(st+1, a

(cid:48)

; θt)]}2

(2)

Diﬀering from the value-based approaches, the policy-based algorithms with deep
neural network directly model the parametrized policy πθ. The parameter set θ is
updated iteratively via gradient descent to maximize J(πθ). A typical application is
REINFORCE algorithm, which takes the corresponding loss function:

∆θJ(πθ) = E
st,at

[∆θlogπθ(at|st)Qπθ (st, at)]

(3)

3. Methodology

3.1. Scenario description

In this paper, we consider a fully electric vehicle environment to better measure the
holistic energy performance of the mixed road traﬃc. which is composed of HDVs and
CVs in urban scenario, while only CVs can receive the SPaT data and be controlled by
eco-driving controller. By incorporating such a mixed traﬃc condition, the developed
strategy is closer to the urban traﬃc in the near future so that it can be more practical.
Figure 1 shows the general eco-driving process of the ego vehicle. The left half of
the ﬁgure depicts a typical longitudinal trajectory and speed proﬁle of a CV with eco-
driving system. Compared with HDVs, the CV with eco-driving controller can generate
more smooth speed proﬁle and navigate the vehicle cross the junction in green phase,
in order to minimize its electricity consumption. Considering a more speciﬁc scenario,
which is shown in the right half of Figure 1, the CV on the road can constantly detect
the surrounding traﬃc state within its perceived range. Inspired by (Wang et al. 2021),
we deﬁne the perceived range of the CV as a two-dimensional occupancy grid. The grid
cells reﬂect the occupancy situation of corresponding road segments around the CV, so
we call the grid local traﬃc state. Whereas the CVs are equipped with multiple sensor
devices, such as camera, Lidar, and radar, this process can be certainly achieved. In
addition to local traﬃc state, the CV could also acquire the SPaT data based on V2I

6

Figure 1.: The driving scenario for controlled vehicle in the proximity of a signalized
intersection.

communication, provided that it enters the V2I communication range. The local traﬃc
state and SPaT-related features are expected to serve as the state of the CV agent,
which is one of the main components in the MDP model.

According to the position of the vehicle, the operation of the CV is categorized
into two cases: (1) When the position of the CV is not within the V2I communication
range of the ﬁrst downstream traﬃc signal, it operates by leveraging local traﬃc state
and default value of the SPaT information. This is also true of the circumstance when
the ego vehicle is inside the intersection for a multi-intersection scenario. (2) During
the period that the V2I communication is available for the ego CV, it continuously
acquire traﬃc signal state, together with local traﬃc state, to adjust its longitudinal
acceleration and make proper lateral decision under control of the eco-driving system.
For longitudinal dimension, the ego vehicle is supposed to cruise with smooth speed
proﬁle and crossing the intersection in green phase; for lateral dimension, the ego
vehicle can stay at current lane or change to adjacent lane to mitigate the interruption
caused by other HDVs or vehicle queues.

3.2. Agent framework

The CVs are deemed as agents, which are supposed to adapt to the dynamic traﬃc
environment and make proper decisions. An agent is a combination of a PRL policy
and several model-based policies. The motivation of the hybrid policy is: (1) The DRL
algorithms always suﬀer from credibility due to its unexplainability, and this may
cause issues related to driving safety, while model-based policies explicitly constraint
the behavior of vehicles to ensure a collision-free driving. (2) The model-based driving
policies cannot react to traﬃc environment adaptively with pre-deﬁned rules, while
DRL policy with ”trial-and-error” mechanism can explore the action space more ef-
fectively. This hybrid form endows the agent with the ability to cope with complex
driving scenarios on the premise of safety.

When the agent enters the V2I communication range, its workﬂow can be seen
in Figure 2. In each step, the agent can observe a state st and perform action at
which is output by the PRL policy, and subsequently receive a reward value rt. We
abstract the lateral decision and the longitudinal control of the CV as discrete process
and continuous process, respectively. The action at thereby can be deﬁned as a tuple

7

(kt, xkt), where kt is an integer value that represents the possible lateral decision that
is shown in Figure 1, and xkt is an ﬂoat value that represents longitudinal acceleration.
During the learning process of the agent, rear-end or lateral collision may be caused
by unreasonable actions. Concerning that the original action generated by the PRL
policy at = (kt, xkt) may be unsafe, we propose the clip operation and mask opera-
.
at
tion to preprocess the original action at, and then decompose the processed action
to perform reliable motion control in both longitudinal and lateral dimension. This
facilitates more eﬃcient agent training as apriori knowledge is added to prevent the
vehicle from unexpected collision.

Figure 2.: The workﬂow of the agent when V2I communication is available.

In order to provide full explanation of how the framework works, we need to specify

the formulation of the MDP for the agent:

Firstly, the state space S of the agent comprises two parts, namely, the local traﬃc

state and the logic state, which are speciﬁed as:

• Local traﬃc state M i

t for agent i at time t is deﬁned as a two-dimensional matrix
that reﬂects the occupancy situation of the perceived range of the agent, which is
illustrated in Figure 1. We assume the perceived range of the CV is Lp, and it can
be divided into forward range Lhead and backward range Ltail. The forward range
Lhead can help the CV maintain a reasonable speed, when the vehicle agent can
obtain the relative distance between itself and the front vehicle. The backward
range Ltail can prompt the CV to ﬁnd a reasonable lane-changing interval that
can be safe and harmonious. The shape of the local traﬃc state matrix is ( Lp
, 5)
Lc
for a 5-lane road with the cell length Lc. The elements in the matrix are ﬁlled
with ”0” if the associated detected cells are occupied by vehicles, otherwise they
are ﬁlled with ”1”.

• Logic state V i

t, vi

t, di

t for agent i at time t is deﬁned as a vector that includes the physical
state of the ego vehicle and the state of the traﬃc signal. The logic state of the
agent is denoted as V i
t , gi
t = [Ei
t is the one-hot encoding of
the lane index that the ego vehicle i located; di
t describes the distance between
agent i and the downstream traﬃc signal at time t; vi
t is the velocity of the agent
at time t; f i
t is a boolean variable that indicates whether the next traﬃc signal
is in red phase; gi
t denotes the time duration between time t to the time that the
next traﬃc signal turn to green phase for the agent. When V2I communication
is unavailable, the value of f i

t are all set to default value -1.

t]T , where Ei

t and gi

t, f i

As a result, the state in time t for agent i is deﬁned as si

t = (M i

t , V i

t ), in which the

8

former element is expected to be processed by a convolutional neural network block to
extract latent features, and the latter element will concatenate to the extracted local
traﬃc state features and be input to a multi-layer perceptron block to train the PRL
policy.

Secondly, the action space in this paper is considered as a hybrid form that sup-
port both longitudinal control and lateral decision. As mentioned above, the basic
representation of the action at time t is at = (kt, xkt), where kt ∈ {−1, 0, 1} and
xkt ∈ [amin, amax]. The value set {−1, 0, 1} of kt represent that the ego vehicle can
change to the left lane, stay at the current lane, and change to the right lane. We as-
sume that the lane changing process can be ﬁnished in a ﬁxed duration Tc. The space
of xkt limits the acceleration of the CV to the range of its maximal deceleration amin
and maximal acceleration amax. Instead of combining discrete speed variation rate
and lane changing decision to formulate an overall discrete action space, we utilize
the hybrid discrete-continuous action space to eﬀectively explore the solution space
and obtain better performance. Since imposing the original at on the ego vehicle may
make it be exposed to traﬃc accident, we introduce conventional car-following model
and lane changing model to modify the dangerous driving behavior. In this paper, we
utilize the Krauss car-following model (Krauß 1998) and the ”LC2013” lane-changing
model (Erdmann 2015), which are embedded in SUMO simulation software, to perform
clip and mask operations and obtain the modiﬁed action

.
at = (

.
kt,

.
xkt):

• Clip operation aims at clipping the potential risking acceleration to a safe value
to avert rear-end collisions or beyond the road speed limit Vmax. We assume
that the acceleration output by the utilized car-following model is xCF
at time
t according to current driving status, and then the corrected acceleration is
CF , xkt). Hence, the velocity of agent i at time t is
transformed to
.
clipped as vi
xkt), 0), which conﬁnes the velocity of the
CV to a reasonable range.

.
xkt = min(xt
t = max(min(Vmax, vi

t−1 +

t

• Mask operation is used to judge whether the lateral decision produced by PRL
policy is safe, in order to prevent lateral collisions or deadlocks. For each non-zero
discrete action kt, the model-based lane changing policy output an bool value
ξ to resolve that if the agent could change lane. The corrected lateral action is
denoted as

.
kt = kt × ξ.

Thirdly, we discuss diﬀerent settings of the reward function rt, while diﬀerent reward
signal may trigger various learning directions of the agent. The general goal of the
eco-driving strategy is to make a trade-oﬀ between energy consumption and travel
time, as the energy-eﬃcient driving mode may cause the increase of the delay (Wang
et al. 2020). An intuitive way to set the reward function is simply returning a linear
combination of total electricity consumption Ef and travel time Tf :

r1
t =

(cid:40)

−(Ef + ϕTf ), if t = tf
0, otherwise

(4)

where, ϕ is a weighting parameter to measure the relative importance of energy indi-
cator and mobility indicator; tf is the terminal time step (i.e., the time that the ego
vehicle leaves the junction).

However, the travel time can only be calculated at the end of the travel, and it
is diﬃcult for the vehicle agent to learn eﬀectively from such a delay reward signal.

9

Given this, Guo et al. (2021) used a step-wise travel distance li
travel time. Incorporating with the step-wise energy consumption ei
reward function is deﬁned as:

t as the proxy of the
t, the instantaneous

t = li
r2

t − ϕei
t

(5)

Despite signs of an improvement in the energy-saving performance, we ﬁnd that
it is hard to generate a smooth trajectory for the agent by deploying r2
t . Meanwhile,
this design can be defective because the agent tends to maximize its future cumula-
tive travel distance, while the remaining distance from the location of the CV to the
intersection is ﬁxed so that the overall travel time cannot be well reﬂected. Thus, we
devise a novel reward function composed of step-wise reward and terminal reward to
facilitate the progress of the eco-driving task in a hybrid action space. It is based on
the following considerations:

• The velocity is one of the most important factors when we place value on the
mobility of the CVs, when continuously low speed will lead to an increase of
travel delay. Thus, we set a penalty item with a constant value rs
p when the
velocity of the agent is lower than a threshold value vζ.

• Frequent changes of acceleration can cause traﬃc oscillation by ﬂuctuating the
speed proﬁle of vehicles, and impair the driving comfort at the same time (Zhao
et al. 2018). The diﬀerential form of the acceleration is usually called jerk,
which manifests the variation rate of the acceleration. Given this, we set an-
other penalty item rj
p when the value of jerk is higher than a threshold value
jζ.

• Since frequent lane changing will interfere with the normal operation of other
p for to penal each lane changing

CVs or HDVs, we set a harmony coeﬃcient rc
behavior of the agent so that it can change lane in necessary cases.

The three penalty items instruct the agent maintain a smooth speed proﬁle with as
little stopping and lane changing as possible. Finally, our reward function is deﬁned
in Equation 6. It should be noted that the calculation of Ef and Tf is only triggered
when the episode reaches the terminal state.

rt = rs

p + rj

p + rc

p − (Ef + ϕTf )

(6)

4. PRL for eco-driving

4.1. Reinforcement Learning in Hybrid Action Space

In addition to normal discrete control and continuous control, which are widely studied
by previous works, some control tasks intrinsically have hybrid action spaces. Control-
ling of CV is one representative in this ﬁeld, as the vehicle can move longitudinally
and laterally on the road. While the majority of the existing studies are conﬁned to car
following motion (Shi et al. 2018; Mousa et al. 2020; Zhou, Yu, and Qu 2020; Wegener
et al. 2021), this paper develops a parameterized action space to naturally describe
the control problem with hybrid actions and thereby implement joint optimization of
car-following and lane-changing movement.

10

Figure 3.: The illustration of a parameterized action space.

Figure 3 presents an example of parameterized action space. We assume that the
lane changing process of the vehicle agent is controlled in a discrete space (i.e., changing
to the left lane, staying at current lane, changing to the right lane), which is marked
with rectangles in orange. Each discrete action has a continuous parameter space
(i.e., the longitudinal acceleration/deceleration) marked with rectangles in grey. The
hierarchical structure formulates the parameterized action space in which the agent
select actions.

Considering an MDP with parameterized action space, the agent ﬁrst selects a
discrete action kt ∈ K at time t, and then selects an associated continuous parameter
xkt ∈ Xk. In this case, the Bellman equation can be transformed to:

Q(st, kt, xkt) = E

rt,st+1

[rt + γ max
k∈K

sup
xk∈Xk

Q(st+1, k, xk)|st = s, at = (kt, xkt)]

(7)

Note that searching for an optimal action parameter xk over continuous space in
each time step is computationally intractable. Therefore, by setting up a function xQ
k :
S → Xk to represent arg supxk∈Xk Q(s, k, xk), the Bellman equation can be rewritten
as Equation 8 (Xiong et al. 2018).

Q(st, kt, xkt) = E

rt,st+1

[rt + γ max
k∈K

Q(st+1, k, xQ

k (st+1))|st = s]

(8)

Given this, a deep neural network with parameter set θ can be used to approxi-
mate Q function: Q(s, k, xk; θ) → Q(s, k, xk). For such a Q(s, k, xk; θ), another neural
network with parameter set ω can serve as the a deterministic policy network to ap-
proximate xQ
k (s), that is, xk(: |ω) : S → Xk. When θ is determined, the purpose of the
algorithm is to ﬁnd ω such that

Q(s, k, xk(s; ω); θ) ≈ sup
xk∈Xk

Q(s, k, xk; θ)

for each

k ∈ K

(9)

Subsequently, we can estimate θ by minimizing the mean-squared Bellman error

11

via typical gradient descent, and this resembles the learning process of DQN. The
formation of the loss function for θ is similar to Equation 2:

LQ

t (θ) =

1
2

[Q(st, kt, xkt; θ) − yt]2

(10)

where, yt denotes the cumulative future reward rt + γ maxa(cid:48) ∈A Q(st+1, a(cid:48); θt) in time
t. When θ is ﬁxed, we aim to regulate ω to maximize Q(s, k, xk(s; ω)), so the loss
function for ω is as following

Lω

t (ω) = −

K
(cid:88)

k=1

Q(s, k, xk(s; ω); θt)

(11)

We thereby deduce the reinforcement learning theory in a hybrid action space. To
sum up, the framework utilizes a parameterized action space to integrate the hybrid
action sets in a hierarchical fashion. Then we formulate two neural networks to learn
the optimal discrete action and its associated continuous action, respectively.

4.2. PRL algorithm

The DRL algorithm with parameterized action space should not only accommodates
the eco-driving environment, but also considers the implementation details, such as
the exploration process, the design of replay buﬀer, the setting of target networks, to
make the training process eﬀective and stable.

Firstly, the architecture of the policy network for the agent should be speciﬁed
by several neural networks, which is shown in Figure 4, in order to formulate the
backbone of the agent. It takes an action parameter network with weights ω to output
the parameter vector x, and uses an action network with weights θ to approximate
the optimal parameterized Q function. For each neural network, the local traﬃc state
is taken as the input of a convolutional neural network to extract latent features, and
then the ﬂatten features is concatenated with logical state. For the action network,
the parameter vector x, which is produced by action parameter network, is ﬁtted
together in the concatenation operation. After a series of fully-connected layers, the
corresponding output is obtained.

In addition, target network is introduced for both action parameter network and
action network to curd instability during training. A target network is a copy of the
original network that is held ﬁxed to serve as a stable target for some number of steps.
We update the target network in a soft way:

(cid:48)

θ

= τ1θ + (1 − τ1)θ

(cid:48)

(cid:48)

ω

= τ2ω + (1 − τ2)ω

(cid:48)

(12)

(13)

where, θ(cid:48) and ω(cid:48) are the parameter set of target action network and target action
parameter network; τ1 and τ2 are hyperparameters to determine the averaging extents.

12

Figure 4.: The architecture of the policy network.

As both longitudinal decision and lateral decision should be optimized, it is nec-
essary to perform exploration for the hybrid action set during training so that more
eﬀective motion patterns can be searched. We deploy two exploration strategies for
the agent, namely, (cid:15)-greedy strategy and Ornstein–Uhlenbeck process noise. The for-
mer strategy is usually utilized in value-based DRL algorithm, similar to the DQN
approach, while the latter resembles that in DDPG algorithm. Meanwhile, a linear de-
cay method is achieved to regulate the trade-oﬀ between exploration and exploitation
over the whole training process with numerous episodes. In this case, the formulation
of the decaying (cid:15)-greedy strategy is

k =

(cid:40)

arg maxk∈K Q(s, k, xk(s)), with probability (1 − (cid:15))
exploration, with probability (cid:15)

(14)

where, exploration represents a randomly selected discrete action. The value of (cid:15) is
determined by

(cid:15) = ((cid:15)init − (cid:15)end) max(

N − nstep
N

, 0) + (cid:15)end

(15)

where, (cid:15)init and (cid:15)end are the initial value and the terminal value of exploration factor
(cid:15) during the overall learning episodes; N denotes the total decaying episodes; nstep
denotes the index of current episode.

Ornstein–Uhlenbeck process (Uhlenbeck and Ornstein 1930) is implemented to gen-
erate temporally correlated noise, which embodies the exploration mechanism for ac-
tion parameter network. By adding noise sampled from a noise process N , the contin-
uous action produced by action parameter network µ(s|ω) can be slightly disturbed,
that is

13

xk = µ(s|ω) + Nk

(16)

The holistic process of the proposed PRL algorithm is presented in Algorithm 1.
Before the main loop, the weights of the action network, action parameter network, and
the target networks are initiated. For each episode of training, line 3 shows the initial
operation of the simulation traﬃc environment. The initialization of the simulation
should consider diﬀerent introductory states to enhance generalization ability of the
agent, and this will be introduce in section 5. The updating of the weights of the
neural networks follows line 17 and line 18 in Algorithm 1. By iteratively updating
the parameters, the ego vehicle, which is seen as the training agent, is capable of
conforming to the dynamic traﬃc environment and the changing of downstream traﬃc
signals.

Algorithm 1 The PRL algorithm for eco-driving agent training
Hyperparameters: averaging rate τ1 and τ2, exploration factor (cid:15), initial exploration
factor (cid:15)init, terminal exploration factor (cid:15)end, the number of decaying episodes N , step-
sizes {αt, βt}t≥0, and minibatch size B
Initialize: action network Q(s, k, xk|θ) and action parameter network µ(s|ω), target
action network Q(cid:48) with weights θ(cid:48) ← θ and target action parameter network µ(cid:48) with
weights ω(cid:48) ← ω, replay buﬀer R, nstep ← 0
1: for each episode do
nstep ← nstep + 1
2:
loading initial traﬃc environment and observe current state st
if nstep < N then

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

update exploration factor (cid:15) according to Equation 15

end if
initialize Ornstein–Uhlenbeck process N for continuous action exploration
for each step in simulation do

compute action parameters xkt according to Equation 16
compute action kt according to Equation 14
take action at = (kt, xkt), observe reward rt and next state st+1
store transition {st, at, rt, st+1} into R
sample B transitions {sb, ab, rb, sb+1}b∈B randomly from R
decompose ab into kb and xkb
compute future cumulative reward yb for batch B based on Q(cid:48) and u(cid:48)
use data {yb, sb, kb}b∈B and {yb, sb, xkb}b∈B to compute the stochastic gra-

t (θt) and ∇ωLω

t (ωt)
update θ and ω by θt+1 ← θt − αt∇θLQ
t (ωt)
update the parameters of target neural networks according to Equation 12

t (θt) and ωt+1 ← ωt − βt∇ωLω

dient ∇θLQ

and Equation 13
end for

19:
20: end for

The hyperparameters of the algorithm are collected in Table 2, which are resolved
after running a number of simulations. Meanwhile, the conﬁguration of the neural net-
works is determined as follows: the ﬁlter number of layer ”Conv 1 1” and ”Conv 1 2”
are 8 and 16, respectively. For each convolutional layer, the kernel size is set to 3,
while for layer ”Conv 1 2” an one-dimension padding operation is added to support

14

the convolutional calculation. In addition to the convolutional layers, the kernel sizes
of max-pooling layers are all set to 2 with a 2-step stride, while an one-dimension
padding operation is also attached to the second max-pooling layer. The conﬁguration
of layer ”Conv 2 1” and ”Conv 2 2” is the same as those in action parameter network.
The number of neurons for ”Dense 1 1” and ”Dense 1 2” is set to 128 and 64 , and
for ”Dense 2 1” and ”Dense 2 2” is 256 and 64.

Table 2.: Hyperparameters of the PRL algorithm

Hyperparameter

Discount factor
Averaging rate for Q
Averaging rate for µ
Step size for Q
Step size for µ
Initial exploration factor
Terminal exploration factor
The number of decaying episodes
Minibatch size
Replay buﬀer size

Symbol

γ
τ1
τ2
α
β
(cid:15)init
(cid:15)end
nstep
B
R

Value

0.99
10−2
10−3
10−4
10−5
1
10−2
103
128
5 × 105

5. Simulation and discussion

5.1. Simulation conﬁguration

The simulation environment is built in SUMO (Lopez et al. 2018) to test the per-
formance of both training phase and evaluation phase. As an highly portable micro-
scopic traﬃc simulation platform, SUMO oﬀers abundant pre-deﬁned driver models,
and meanwhile the operation of vehicles can be controlled through Traci interface
by Python programs. Therefore, we delimit the HDVs and CVs by adding PRL pol-
icy programmed in Python module to CVs, while the operation of HDVs is based
on the default Krauss car-following model and LC2013 lane-changing model. The en-
ergy model that produces transient electricity consumption is derived from (Kurczveil,
L´opez, and Schnieder 2013), which leverages a braking energy recovery mechanism.
It means that the value of instantaneous energy consumption ei
t is negative when the
vehicle perform a braking operation, which presents challenges to searching a globally
optimal eco-driving policy. The speciﬁc form and parameters of the energy model are
given in our previous article (Zhang et al. 2022). The proposed PRL policy do not
need to explicitly analysis the energy model, while it only receive a transient signal
that indicates the energy consumption in a model-agnostic manner.

We create a coordinated signal control scenario and a non-coordinated signal control
scenario in a multi-intersection environment to evaluate the proposed framework. As
illustrated in Figure 5, the corridor contains ﬁve consecutive signalized intersections,
for which a V2I communication range is set to 300m. The traﬃc lights with cycle
time of 90s change synchronously for non-coordinated scenario, while for coordinated
scenario a series of oﬀset is set up to ensure that a vehicle can encounter a green wave
under free ﬂow condition.

The parameters related to the simulation follow those in Table 3 if no otherwise
speciﬁed. Besides, the values of the penalty items is set as rs
p = −30, and
rc
p = −50. Together with the weighting parameter ϕ, the reward-related parameters

p = −30, rj

15

Figure 5.: The road network scenario used for simulation.

will be investigated in sensitivity analyses.

Table 3.: Parameter conﬁguration for the simulations

Simulation parameter

Symbol

Value

Maximum deceleration
Maximum acceleration
Road speed limit
Cell length
Forward perceived range
Backward perceived range
Vehicle length
Lane changing duration
Velocity penalty threshold
Jerk penalty threshold

amin
amax
Vmax
Lc
Lhead
Ltail
-
Tc
vζ
jζ

-4
3
50
1
50
10
5
3
1.5
4

Unit

m/s2
m/s2
km/h
m
m
m
m
s
m/s
m/s3

In addition to the general parameter settings of the simulation platform, we still
need to take measures to provide a more dynamic traﬃc condition, with the intent
to train the agent eﬃcaciously. On one hand, traﬃc ﬂow with 1000veh/h is generated
in the simulation environment in a random departure mode, which aims at modelling
a random road traﬃc situation; on the other hand, the ego vehicle is inserted to
the beginning of the road network in a random fashion during training period. More
precisely, the insertion time of the ego vehicle adheres to an uniform distribution
U (150s, 300s), making the agent encounter diﬀerent traﬃc signal status and vehicle
queue situations, which is of beneﬁt to enhance the robustness of the algorithm.

5.2. Training phase discussion

At ﬁrst, we discuss about the settings of the reward function. The study investigate
not only the forms of the reward function (i.e., r1
t , and rt), but also the impact
of weighting parameter ϕ, to analyze the optimal reward conﬁguration by empirical
experiments. It should be noted that we also add penalty rc
t to prevent
repeated lane changing behaviors.

t and r2

p for r1

t , r2

Figure 6 compares three reward conﬁgurations in terms of the energy consumption
and travel time for the ego vehicle throughout the 1000-episode training period. Dif-
ferent patterns are seen in the variation of energy and time consumption for r1
t and
the other reward settings. No matter how the parameter ϕ changes, the downward

16

(a)

(b)

Figure 6.: The changing trend of corresponding indicators during training period. The
data collected in episode Dk is smoothed by Dk ← 0.98Dk−1 + 0.02Dk. (a) Energy
consumption, (b) Travel time.

trend of the metrics is not signiﬁcant for r1
t , illuminating that the agent cannot learn
eﬀective information with feedback only at the end of the episodes. In contrast, the
converged energy consumption of r2
t and our rt are much lower, but the gaps between
the those two rewards are likely to widen. In addition to the energy consumed by the
ego vehicle, the changing trend of time consumption of both r2
t and rt are similar, but
the ﬁgures of rt see a relatively stable pattern.

On the other side, the impact of the weighting parameter ϕ is likely to be insignif-
icant, because the results remain basically unchanged with the increase of ϕ. It is
possibly due to that the optimal driving patterns are identical when we give similar
importance to mobility and energy saving. As a result, we will use ϕ = 1 in the rest
of the simulations since the numerical magnitudes of the two indicators are close.
p, and rc

p, a series of simulation-based training
are carried out to explore the optimal conﬁguration. We ﬁx rc
p and
rj
p from 10 to 50 with step = 10 on the grounds that the result may be changed by

With regard to the values of rs

p = 50 and vary rs

p, rj

17

(a)

(b)

Figure 7.: The heatmap with diﬀerent value of rs
Travel time.

p and rj

p. (a) Energy consumption, (b)

diﬀerent relative ratio of the three items. The simulation investigation is based on
the environment with non-coordinated traﬃc signal setting, since this scenario can
be more challenging. The results are shown in Figure 7, which utilizes average value
among 100 test cases for each setting. According to the graph, we ﬁnd that the growth
of rS
p can signiﬁcantly reduce travel time but increase the energy consumption. This
is reasonable because the vehicle may speed up as soon as possible if no suﬃcient
penalty is imposed on its reward system, which also resembles some human drivers.
By comparison, the variation of rj
p seems to have much lower impact on travel time and
electricity consumption, but it helps to smooth the trajectory of the CV, as negative
reward will be imposed if the vehicle perform violent acceleration or deceleration. As
a result, we choose rs
p = 30 to keep a trade-oﬀ between energy eﬃciency
and traﬃc mobility.

p = 40 and rj

5.3. Overall performance of single vehicle

After obtaining the well-trained models of the agent, we can deploy them in new
environment to test the performance of the proposed method. We ﬁrstly introduce
a baseline that embedded in SUMO to verify if our model can outperform human-
driving behaviors. In this case, there is no diﬀerence between CVs and HDVs, whereas
the CVs follow the default Krauss car-following model and the LC2013 lane-changing
model to simulate the driving behaviors of human drivers. Meanwhile, we deploy the
well-trained agent to the environments with the same insertion time of the ego vehicle,
and record the data of trajectory, speed, and cumulative energy consumption.

For both coordinate and non-coordinate signal environment, we generate three sce-
narios respectively to test the method and make visually comparisons, and the results
are illustrated in Figure 8 and Figure 9. It can be found that the agent trained by PRL
can adapt to diﬀerent traﬃc environment: (1) in coordinate traﬃc signal environment,
the speed proﬁles of PRL-based vehicle are similar to those based on human-driving
model, but it learns a strategy of early deceleration to decrease energy consumption.
(2) in non-coordinate traﬃc signal environment, the energy-eﬃciency of the vehicle
can be better reﬂected. While the vehicle with baseline model is obliged to stop at
every red signal, the CV controlled by PRL policy can improve its energy-eﬃciency by

18

early deceleration and braking-reacceleration in small range to recover the electricity
through the braking-recovery mechanism. In addition to the energy indicator, the in-
crease of travel time is not signiﬁcant for PRL policy. Due to the fact that the vehicle
with default model always drives in its maximum acceptable velocity, it is diﬃcult
to improve mobility and energy-eﬃciency at the same time. Therefore, our learning-
based framework is capable of reducing the consumed energy with similar traﬃc delay
of baseline, which is much more practical in real situations.

Figure 8.: The comparison of baseline and PRL in coordinate signal environment.

Figure 9.: The comparison of baseline and PRL in non-coordinate signal environment.

In addition, 100 test cases are simulated to compare the general performance of the
proposed algorithm. Table 4 presents the overall performance of the PRL algorithm.
Compared with the manual driving behavior, although a fraction of travel time is
sacriﬁced, the proposed framework can signiﬁcantly reduce the electricity consump-
tion, showing a decrease of 4.1% in coordinate signal environment and 27.13% in
non-coordinate signal environment.

19

Table 4.: Average performance of the PRL algorithm among 100 test cases

Method Energy consumption (Wh)

Travel time (s)

Baseline
PRL
Imp. ↑

coord.
231.13
221.75
4.1%

non-coord.
253.79
184.93
27.13%

coord.
189.08
204.79
-7.93%

non-coord.
260.21
289.82
-11.25%

In order to show the superiority of our method in a more general way, we make

further comparison studies with the following strategies:

• DDPG+LC2013: the longitudinal motions of the CVs are expected to be con-
trolled by a DDPG policy with the same MDP conﬁgurations of our PRL (the
harmony coeﬃcient rc
p is excluded), while the lateral decisions are based on the
classical LC2013 model.

• DQN: the CVs are controlled by a DQN policy in this case. Due to that the value-
based DRL algorithm can only handle with discrete action space, a predeﬁned
longitudinal action set is given as:

ADQN

lon = {1.0amax, 0.75amax, 0.5amax, 0.25amax, 0.0,
0.25amin, 0.5amin, 0.75amin, 1.0amin}

Combining with the lateral action set ADQN
we get a 27-dimensional vector to represent the action space.

lat = {−1, 0, 1} by Cartesian product,

With the intent to make a fair comparison, the hyperparameters for DDPG and
DQN, such as γ, α, B, and R are the same as our PRL, as well as the general archi-
tectures of the neural networks. The exploration mechanism of DDPG and DQN are
identical to the OU process noise and decaying (cid:15)-greedy strategy in the PRL algorithm,
respectively.

(a)

(b)

Figure 10.: The performance of the ego vehicle with diﬀerent control methods. (a)
Energy consumption, (b) Travel time.

To alleviate the impact caused by randomness, the results are collected in 10 inde-
pendent group of simulations with diﬀerent value of random seed. Figure 10 shows the
collected data in a violin plot form, which depicts summary statistics and the density
of the results. From the ﬁgure, we can draw following conclusions: (1) DQN cannot
satisfactorily be applied in the task, whereas it sees an unacceptable performance in
terms of travel time. This can be caused by the large discrete action space of the DQN

20

agent when we incorporating longitudinal and lateral decisions. (2) The performance
of DDPG is close to the baseline approach, but much worse in non-coordinate signal
settings due to the increased travel time. (3) In contrast, the proposed PRL method is
more impressive with dynamic traﬃc ﬂow and signal changing status. Compared with
other DRL-based methods, the distribution of energy consumption for PRL agent is
close to a smaller value, with only a modest growth of travel time.

5.4. Lateral performance of single vehicle

Figure 11.: The scenarios to test the lateral decision of the agent.

It is diﬃcult to illuminate the beneﬁt of lane changing accurately as this bene-
ﬁt is not instantaneous. One feasible way to demonstrate the eﬀectiveness of proper
lateral decisions is to observe the behavior of the agent on some speciﬁc occasions.
Thus, we take two synthetic scenarios as examples from a microscopic perspective and
demonstrate that the agent is capable of making adaptive lateral decisions. As shown
in Figure 11, scenario A presents a situation that ego vehicle follows HDV1, which
will be imposed a braking with deceleration of 2m/s2 in three consecutive time steps;
scenario B depicts the ego vehicle follows a platoon composed of HDVs. As the queues
of HDVs at the intersection is detected by the agent, it is supposed to change lane
and plan its velocity in order to mitigate the interruption of the queue.

Figure 12 depicts the simulation results in coordinate graphs, the time-distance
graphs, and the time-speed graphs of scenario A and scenario B. Figure 12(a) shows
the lane changing behaviors of the ego vehicle after HDV1 performing deceleration.
Due to the existence of HDV2, the agent choose to decelerate and then change to the
second lane. In order to shun the interruption of HDV2, it immediately change to the
third lane and keep a smooth speed proﬁle to cross the intersections. Figure 12(b)
shows that the agent can select the lane with the least number of vehicles and slow
down early to shun the potential inﬂuence of the leading HDV.

5.5. Performance of mixed traﬃc

Deploying the proposed strategy in mixed traﬃc ﬂow is an essential part to verify the
applicability of the study. Based on the belief that the co-existence of CVs and HDVs
will last for many years, it is necessary to explore the potential impact of the strategy
with diﬀerent MPR of the CV.

21

(a) Scenario A

(b) Scenario B

Figure 12.: The performance of the ego vehicle in two synthetic scenarios.

Figure 13 illustrates the traﬃc performance in terms of average electricity con-
sumption and average speed with diﬀerent MPR of CV and traﬃc volume. The time
duration of single episode of simulation is set to 1800s, and a group of simulation
contains 5 independent experiments to collect data and calculate average results. It is
obvious that marginal increase of MPR of CV will bring about relatively considerable
energy reduction of the traﬃc. Compared with pure HDV scenario, the mean electric-
ity consumption of scenario with CV MPR=1 decrease to 240Wh/veh-250Wh/veh,
showing a decline of around 13.8%. By deploying the PRL algorithm to the agent,
the average energy consumption and average speed can maintain a stable value with
diﬀerent level of traﬃc demand when the MPR is ﬁxed. Meanwhile, we can observe
that the decrease of mean speed is not signiﬁcant with the growth of MPR, demon-
strating the sacriﬁce of vehicle mobility is negligible. Another ﬁnding is that the loss of
mean speed in non-coordinate setting is lower than that in coordinate setting with the
increase of MPR. This is due to the HDVs in non-coordinate signal environment are
more likely to encounter red light and wait at the intersection. As such, the proposed
method manages to transform this part of time to the duration that vehicles running
with relatively lower speed to avert red traﬃc signals.

Since many eco-driving strategies may impede the normal operation of HDVs so
as to exert an adverse impact on traﬃc ﬂow, we run simulations to observe that if
the CVs with PRL strategy interfere with other vehicles. Figure 14 categorise the
vehicles into CVs and HDVs, and compare the metrics in non-coordinate signal set-
tings. The situations of low traﬃc volume (200veh/h/lane) and high traﬃc volume

22

(a) Average energy consumption (coordinate
signal control)

(b) Average speed (coordinate signal control)

(c) Average
coordinate signal control)

energy

consumption

(non-

(d) Average speed (non-coordinate signal con-
trol)

Figure 13.: The performance of traﬃc with diﬀerent traﬃc volume and CV MPR in
diﬀerent traﬃc signal environment.

(700veh/h/lane) are compared. The ﬁgures demonstrate that the operations of HDVs
are not inﬂuenced by CVs, while thier metrics keep at a steady level, no matter how
the volume and MPR of CVs changes.

6. Conclusions

Eco-driving is one of the potent approaches to improve the energy eﬃciency of the
electric vehicles. This paper propose a PRL-based framework, which can cope with the
vehicle control task with hybrid action space, to implement eco-driving in urban con-
nected intersections. Incorporating with model-based car-following and lane-changing
mechanism, the PRL algorithm trains the CV to learn to make joint decisions in
terms of longitudinal dimension and lateral dimension. By analysing diﬀerent reward

23

(a)

(b)

(c)

Figure 14.: Average performance of CVs and HDVs in diﬀerent MPR and traﬃc vol-
ume. (a) Average energy consumption, (b) Average travel time, (c) Average speed.

settings, which are the core part of the DRL algorithm, this paper combines step-wise
reward and terminal reward, and formulates an MDP model in a hybrid action space.
Simulations are carried out in both coordinate signal setting and non-coordinate
signal setting, with microscopic perspective and macroscopic perspective. The micro-
scopic simulations investigate the performance of single agent in the dynamic traﬃc
environment and show that the consumed energy can be reduced by 4.1% with coor-
dinate signal setting and 27.13% with non-coordinate signal setting. Meanwhile, the
PRL-based framework can outperform DQN and DDPG, which are used in some pre-
vious works to control CVs. The macroscopic simulations study the performance of
the methodology when deploying the algorithm to multi-agent situations. It is found
that the increase of CV MPR is of beneﬁt to the overall performance of the traﬃc
ﬂow, while the operations of HDVs are not impaired by CVs with the PRL controller.
These ﬁndings make the framework more practical when considering to apply DRL-
aided policies in real-world intelligent vehicles.

Further exploration of the learning-based eco-driving strategies is very much needed,
including the ways to consider the social welfare of the application of connected tech-
nologies and learning algorithms, so as to improve the performance of HDVs. In addi-
tion, the interactions between CVs is another area that deserves more attentions, which
can promote cooperative eco-driving of CVs. By employing cooperative multi-agent
DRL methods with speciﬁc communication protocols, the collaborative perception and
cooperative decisions can be achieved to further dig the potential value of advanced
eco-driving systems, especially under low CV MPR circumstances.

Disclosure statement

The authors declare that they have no known competing ﬁnancial interests or personal
relationships that could have appeared to inﬂuence the work reported in this paper.

Funding

This research was supported by the National Key R&D Program of China (Grant
2021TFB1600500), and the Key R&D Program of Jiangsu Province in China (Grant
No. BE2020013).

24

References

Ard, Tyler, Longxiang Guo, Robert Austin Dollar, Alireza Fayazi, Nathan Goulet, Yunyi Jia,
Beshah Ayalew, and Ardalan Vahidi. 2021. “Energy and ﬂow eﬀects of optimal automated
driving in mixed traﬃc: Vehicle-in-the-loop experimental results.” Transportation Research
Part C: Emerging Technologies 130: 103168.

Asadi, Behrang, and Ardalan Vahidi. 2011. “Predictive Cruise Control: Utilizing Upcoming
Traﬃc Signal Information for Improving Fuel Economy and Reducing Trip Time.” IEEE
Transactions on Control Systems Technology 19 (3): 707–714.

Bai, Zhengwei, Peng Hao, Wei Shangguan, Baigen Cai, and Matthew J. Barth. 2022. “Hybrid
Reinforcement Learning-Based Eco-Driving Strategy for Connected and Automated Vehicles
at Signalized Intersections.” IEEE Transactions on Intelligent Transportation Systems 1–14.
Chen, Zhi, Yunlong Zhang, Jinpeng Lv, and Yajie Zou. 2014. “Model for Optimization of
Ecodriving at Signalized Intersections.” Transportation Research Record 2427 (1): 54–62.
Chow, Andy H.F., Z.C. Su, E.M. Liang, and R.X. Zhong. 2021. “Adaptive signal control
for bus service reliability with connected vehicle technology via reinforcement learning.”
Transportation Research Part C: Emerging Technologies 129: 103264.

Ci, Yusheng, Lina Wu, Jiafa Zhao, Yichen Sun, and Guohui Zhang. 2019. “V2I-based car-
following modeling and simulation of signalized intersection.” Physica A: Statistical Me-
chanics and its Applications 525: 672–679.

Dong, Haoxuan, Weichao Zhuang, Boli Chen, Guodong Yin, and Yan Wang. 2021. “Enhanced
Eco-Approach Control of Connected Electric Vehicles at Signalized Intersection With Queue
Discharge Prediction.” IEEE Transactions on Vehicular Technology 70 (6): 5457–5469.
Erdmann, Jakob. 2015. “SUMO’s Lane-changing model.” In 2nd SUMO User Conference,
edited by Michael Behrisch and Melanie Weber, Vol. 13 of Lecture Notes in Control and
Information Sciences, 105–123. Springer Verlag.

Feng, Yiheng, Chunhui Yu, and Henry X. Liu. 2018. “Spatiotemporal intersection control in a
connected and automated vehicle environment.” Transportation Research Part C: Emerging
Technologies 89: 364–383.

Gao, Zhiming, Tim LaClair, Shiqi Ou, Shean Huﬀ, Guoyuan Wu, Peng Hao, Kanok Boriboon-
somsin, and Matthew Barth. 2019. “Evaluation of electric vehicle component performance
over eco-driving cycles.” Energy 172: 823–839.

Guo, Qiangqiang, Ohay Angah, Zhijun Liu, and Xuegang (Jeﬀ) Ban. 2021. “Hybrid deep
reinforcement learning based eco-driving for low-level connected and automated vehicles
along signalized corridors.” Transportation Research Part C: Emerging Technologies 124:
102980.

Jia, Dongyao, and Dong Ngoduy. 2016. “Enhanced cooperative car-following traﬃc model
with the combination of V2V and V2I communication.” Transportation Research Part B:
Methodological 90: 172–191.

Jiang, Huifu, Jia Hu, Shi An, Meng Wang, and Byungkyu Brian Park. 2017. “Eco approach-
ing at an isolated signalized intersection under partially connected and automated vehicles
environment.” Transportation Research Part C: Emerging Technologies 79: 290–307.

Jiang, Xia, Jian Zhang, Qingyang Li, and Tianyi Chen. 2022. “A Multiobjective Cooperative
Driving Framework Based on Evolutionary Algorithm and Multitask Learning.” Journal of
Advanced Transportation 2022.

Kamal, Md Abdus Samad, Shun Taguchi, and Takayoshi Yoshimura. 2015. “Intersection Vehi-
cle Cooperative Eco-Driving in the Context of Partially Connected Vehicle Environment.”
In 2015 IEEE 18th International Conference on Intelligent Transportation Systems, 1261–
1266.

Kim, Minjeong, and Hoe Kyoung Kim. 2020. “Investigation of environmental beneﬁts of traﬃc
signal countdown timers.” Transportation Research Part D: Transport and Environment 85:
102464.

Krauß, Stefan. 1998. “Microscopic modeling of traﬃc ﬂow: Investigation of collision free vehicle

dynamics.” .

25

Kurczveil, Tam´as, Pablo ´Alvarez L´opez, and Eckehard Schnieder. 2013. “Implementation of an
Energy Model and a Charging Infrastructure in SUMO.” In Simulation of Urban MObility
User Conference, 33–43. Springer.

Kuriyama, Motoi, Sou Yamamoto, and Masafumi Miyatake. 2010. “Theoretical study on Eco-
Driving Technique for an Electric Vehicle with Dynamic Programming.” In 2010 Interna-
tional Conference on Electrical Machines and Systems, 2026–2030.

Li, Fangyi, Rui Ou, Xilin Xiao, Kaile Zhou, Wu Xie, Dawei Ma, Kunpeng Liu, and Zhuo Song.
2019. “Regional comparison of electric vehicle adoption and emission reduction eﬀects in
China.” Resources, Conservation and Recycling 149: 714–726.

Li, Ming, Xinkai Wu, Xiaozheng He, Guizhen Yu, and Yunpeng Wang. 2018. “An eco-driving
system for electric vehicles with signal control under V2X environment.” Transportation
Research Part C: Emerging Technologies 93: 335–350.

Lopez, Pablo Alvarez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang
Fl¨otter¨od, Robert Hilbrich, Leonhard L¨ucken, Johannes Rummel, Peter Wagner, and Eva-
marie Wießner. 2018. “Microscopic Traﬃc Simulation using SUMO.” In The 21st IEEE
International Conference on Intelligent Transportation Systems, IEEE.

Ma, Fangwu, Yu Yang, Jiawei Wang, Xinchen Li, Guanpu Wu, Yang Zhao, Liang Wu, Bilin
Aksun-Guvenc, and Levent Guvenc. 2021. “Eco-driving-based cooperative adaptive cruise
control of connected vehicles platoon at signalized intersections.” Transportation Research
Part D: Transport and Environment 92: 102746.

Mintsis, Evangelos, Eleni I. Vlahogianni, and Evangelos Mitsakis. 2020. “Dynamic Eco-Driving
near Signalized Intersections: Systematic Review and Future Research Directions.” Journal
of Transportation Engineering, Part A: Systems 146 (4): 04020018.

Mintsis, Evangelos, Eleni I. Vlahogianni, Evangelos Mitsakis, and Seckin Ozkul. 2021. “En-
hanced speed advice for connected vehicles in the proximity of signalized intersections.”
European Transport Research Review 13 (2).

Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, et al. 2015. “Human-level control through deep reinforcement learn-
ing.” Nature 518 (7540): 529–533.

Mousa, Saleh R., Sherif Ishak, Ragab M. Mousa, Julius Codjoe, and Mohammed Elhenawy.
2020. “Deep Reinforcement Learning Agent with Varying Actions Strategy for Solving the
Eco-Approach and Departure Problem at Signalized Intersections.” Transportation Research
Record 2674 (8): 119–131.

Qu, Xiaobo, Yang Yu, Mofan Zhou, Chin-Teng Lin, and Xiangyu Wang. 2020. “Jointly damp-
ening traﬃc oscillations and improving energy consumption with electric, connected and
automated vehicles: A reinforcement learning based approach.” Applied Energy 257: 114030.
Rakha, Hesham, and Raj Kishore Kamalanathsharma. 2011. “Eco-driving at signalized in-
tersections using V2I communication.” In 2011 14th International IEEE Conference on
Intelligent Transportation Systems (ITSC), 341–346.

Shi, Haotian, Yang Zhou, Keshu Wu, Xin Wang, Yangxin Lin, and Bin Ran. 2021. “Connected
automated vehicle cooperative control with a deep reinforcement learning approach in a
mixed traﬃc environment.” Transportation Research Part C: Emerging Technologies 133:
103421.

Shi, Junqing, Fengxiang Qiao, Qing Li, Lei Yu, and Yongju Hu. 2018. “Application and
Evaluation of the Reinforcement Learning Approach to Eco-Driving at Intersections un-
der Infrastructure-to-Vehicle Communications.” Transportation Research Record 2672 (25):
89–98.

Talebpour, Alireza, and Hani S. Mahmassani. 2016. “Inﬂuence of connected and autonomous
vehicles on traﬃc ﬂow stability and throughput.” Transportation Research Part C: Emerging
Technologies 71: 143–163.

Typaldos, Panagiotis, Ioannis Papamichail, and Markos Papageorgiou. 2020. “Minimization of
Fuel Consumption for Vehicle Trajectories.” IEEE Transactions on Intelligent Transporta-
tion Systems 21 (4): 1716–1727.

Uhlenbeck, G. E., and L. S. Ornstein. 1930. “On the Theory of the Brownian Motion.” Phys.

26

Rev. 36: 823–841.

Wang, Guan, Jianming Hu, Zhiheng Li, and Li Li. 2021. “Harmonious Lane Changing via
Deep Reinforcement Learning.” IEEE Transactions on Intelligent Transportation Systems
1–9.

Wang, Guangjun, Keita Makino, Arek Harmandayan, and Xinkai Wu. 2020. “Eco-driving be-
haviors of electric vehicle users: A survey study.” Transportation Research Part D: Transport
and Environment 78: 102188.

Wang, Siyang, and Xianke Lin. 2020. “Eco-driving control of connected and automated hybrid

vehicles in mixed driving scenarios.” Applied Energy 271: 115233.

Watkins, Christopher JCH, and Peter Dayan. 1992. “Q-learning.” Machine learning 8 (3):

279–292.

Wegener, Marius, Lucas Koch, Markus Eisenbarth, and Jakob Andert. 2021. “Automated eco-
driving in urban scenarios using deep reinforcement learning.” Transportation Research Part
C: Emerging Technologies 126: 102967.

Xia, Haitao, Kanok Boriboonsomsin, and Matthew Barth. 2013. “Dynamic Eco-Driving for
Signalized Arterial Corridors and Its Indirect Network-Wide Energy/Emissions Beneﬁts.”
Journal of Intelligent Transportation Systems 17 (1): 31–41.

Xiong, Jiechao, Qing Wang, Zhuoran Yang, Peng Sun, Lei Han, Yang Zheng, Haobo Fu, Tong
Zhang, Ji Liu, and Han Liu. 2018. “Parametrized deep q-networks learning: Reinforcement
learning with discrete-continuous hybrid action space.” arXiv preprint arXiv:1810.06394 .
Zhang, Jian, Xia Jiang, Suping Cui, Can Yang, and Bin Ran. 2022. “Navigating Electric
Vehicles Along a Signalized Corridor via Reinforcement Learning: Toward Adaptive Eco-
Driving Control.” Transportation Research Record 0 (0): 03611981221084683.

Zhao, Weiming, Dong Ngoduy, Simon Shepherd, Ronghui Liu, and Markos Papageorgiou.
2018. “A platoon based cooperative eco-driving model for mixed automated and human-
driven vehicles at a signalised intersection.” Transportation Research Part C: Emerging
Technologies 95: 802–821.

Zhou, Mofan, Yang Yu, and Xiaobo Qu. 2020. “Development of an Eﬃcient Driving Strategy
for Connected and Automated Vehicles at Signalized Intersections: A Reinforcement Learn-
ing Approach.” IEEE Transactions on Intelligent Transportation Systems 21 (1): 433–443.

27

