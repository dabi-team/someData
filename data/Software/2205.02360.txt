GitRank: A Framework to Rank GitHub Repositories
Niranjan Hasabnis
Intel Labs
Santa Clara, California, USA
niranjan.hasabnis@intel.com

2
2
0
2

y
a
M
4

]
E
S
.
s
c
[

1
v
0
6
3
2
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Open-source repositories provide wealth of information and are
increasingly being used to build artificial intelligence (AI) based
systems to solve problems in software engineering. Open-source
repositories could be of varying quality levels, and bad-quality
repositories could degrade performance of these systems. Evalu-
ating quality of open-source repositories, which is not available
directly on code hosting sites such as GitHub, is thus important.
In this hackathon, we utilize known code quality measures and
GrimoireLab toolkit to implement a framework, named GitRank, to
rank open-source repositories on three different criteria. We discuss
our findings and preliminary evaluation in this hackathon report.

ACM Reference Format:
Niranjan Hasabnis. 2022. GitRank: A Framework to Rank GitHub Reposito-
ries. In 19th International Conference on Mining Software Repositories (MSR
’22), May 23–24, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 3 pages.
https://doi.org/10.1145/3524842.3528519

1 INTRODUCTION
Open-source code repositories, such as those hosted on popular
hosting sites like GitHub, provide wealth of information about
software projects such as team dynamics, project schedule and de-
liverables, among others [1, 2, 4, 22]. Such information is routinely
used by researchers working on problems in the field of software
engineering. More recently, researchers working at the intersection
of machine learning (or AI) and software engineering are also using
such information to build systems that automatically learn to solve
problems from data [6, 11–13, 18, 27, 33, 34].

Unfortunately, deriving insights using open-source code reposi-
tories could be wrong or misleading because popular hosting sites
such as GitHub typically do not indicate quality of code reposito-
ries. This could partly be because quality of a repository could be
subjective. Nonetheless, measuring code quality a well-researched
problem in the field of software engineering, and several, popu-
lar source code metrics such as Halstead volume [10], cyclomatic
complexity [21], maintainability index [25], etc., exists [17, 19, 20].
Although, a number of frameworks and source code metrics to
measure repository/code quality exist [5, 16, 17, 28–30], we could
not find publicly-available, open-source implementation of a frame-
work that ranks code repositories on quality. That is why we decided

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9303-4/22/05. . . $15.00
https://doi.org/10.1145/3524842.3528519

Category

Quality

Maintainability

Popularity

Description of the measure
Average Cyclomatic complexity of a repository
Number of style errors per LoC
Number of security errors of low severity per SLoC
Number of security errors of medium severity per SLoC
Number of security errors of high severity per SLoC
Average maintainability index of a repository
Number of closed issues and pull requests over last 2 years
Number of closed issues and pull requests over last 1 year
Number of closed issues and pull requests over last 6 months
Number of closed issues and pull requests over last 1 month
Number of commits per day
Number of subscribers per day
Number of stargazers per day
Number of forks per day

Table 1: Measures used in GitRank
to build such a framework, named GitRank1, in this hackathon. Ad-
ditionally, GrimoireLab toolkit [3, 9] that is available as a part of
this hackathon incentivized such an implementation as it makes
retrieving some of the source code metrics easy. We, nonetheless,
faced several challenges, which we discuss in this report.

2 GITRANK: DESIGN AND IMPLEMENTATION
We now describe our framework to rank open-source repositories
based on quality, maintainability, and popularity. In addition to
source code quality metrics, GitRank also considers popularity and
maintainability metrics (similar to NPM registry [24]). GitRank
currently supports ranking of repositories that use C/C++ as their
primary language. Nonetheless, the design of our framework is easy
to extend to other languages as well. GitRank is implemented in 700
lines of Python code and can be broken down into two phases.

Phase 1: Obtaining values of metrics for every repository.
Given a set of open-source repositories to be ranked, in the first
phase, we obtain values of quality, maintainability, and popularity
metrics of every repository individually. Table 1 contains the set
of measures that we consider. We use a subset of the typical code
measures used in software engineering that are applicable to open-
source repositories and are common across a set of languages.

Cyclomatic complexity. Cyclomatic complexity of a source code
is a well-known metric of the structural code complexity [21]. We
use Python APIs of lizard project [31] to obtain function-level
cyclomatic complexity and then average it over a repository to
obtain repository-level cyclomatic complexity2.

Style errors. We consider code formatting as a code quality metric
and use cpplint project [15] to obtain the number of style errors
(of the highest severity level)3. We divide the total number of errors

1Available at https://github.com/nirhasabnis/gitrank
2We ended up using lizard API directly as CoCom backend of Graal component in
GrimoireLab [3] offered commit-specific code complexity instead of function-level or
repository-level complexity.
3Graal’s CoQua backend does not support obtaining this information for C/C++.

 
 
 
 
 
 
MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

Niranjan Hasabnis

Figure 1: An HTML output of GitRank

by the lines of source code (SLoC) [23] to obtain the error density.
Static analysis tools such as cpplint are known to suffer from false
positives [26], and although, GitRank does not account for them
right now, we believe that they could be accounted by adjusting
the weight assigned to style errors in the overall quality score.

Security errors. We also consider security errors to determine
code quality. Security issues, however, could be of varying severity
levels. In our framework, we consider three severity levels: low,
medium, and high. We used FlawFinder tool [7] to obtain the
security errors in C/C++ programs4 and divide the total number of
errors reported for every level by the lines of source code (SLoC)
to obtain the level-specific error density.

Maintainability index. Maintainability index is a well-known
metric that incorporates a number of traditional source code metrics
into a single number that indicates relative ease of maintaining
source code [25]. We use the modified formula of MI [32] to obtain
MI for individual modules from a repository5. MI for a repository
is then obtained by averaging over MI for individual modules.

Closed issues. We use a combination of GitHub’s REST APIs and
GrimoireLab’s Perceval [8] to obtain the number of closed issues
over a period of time (last 2 years, last 1 year, last 6 months, and last
1 month, with increasing importance in that order) from the date of
evaluation to determine maintenance activity of a repository. This
is because we want to value current maintenance activity more.

Stars, watches, and forks. Existing approaches consider GitHub
stars, subscriptions, and forks as popularity metrics [4]. We follow
the same terminology. We use a combination of GitHub’s REST
APIs [14] and GrimoireLab’s Perceval to obtain this information.

Phase 2: Obtaining quality, popularity, and maintainabil-
ity score of repositories. In the second phase, we compare metrics
across repositories to obtain overall score for every repository. Be-
fore we calculate scores for all the repositories, we first normalize
values of all the measures to the range of 0% (lowest) to 100% (high-
est). This is performed by obtaining the lowest and the highest value
of every measure and computing the position of a value within the
range of the lowest and the highest value.

4We used a standalone tool to obtain this information as we found that Graal’s CoVuln
backend did not support our specific case.
5We extend lizard project with a plugin to calculate Halstead volume in MI.

Normalized values of the measures are then used to compute
the quality score and the popularity score as a weighted average,
with equal weights for all the measures. For the maintainability
score, we use different weights for the measures based on their
significance6. We then compute overall score of a repository as a
mean of all three scores. All of these scores are also normalized to
the range of 0% (lowest) to 100% (highest).

3 EVALUATION
GitRank accepts a list of GitHub repositories (their URLs) as input
and generates their ranked list as output (in CSV and HTML format).
In our preliminary evaluation of GitRank, we ranked randomly-
selected 500 GitHub repositories that use C++ as primary language.
We dropped 12 repositories out of 500 as they had some peculiarities
(such as unusual extensions of C++ files) because of which the tools
used by GitRank could not obtain their metrics.

Performance. To support a large number of repositories, we
have implemented phase 1 of GitRank to be highly-parallel (e.g.,
multiple machines and/or CPU cores). In our experiment, we di-
vided the list of 500 repositories into 5 separate lists of 100 reposi-
tories each and processed them separately on 5 different machines.
Phase 1 of GitRank took 1 hr and 25 KB of memory, while phase 2,
implemented in a serial manner, took 0.1 sec and 1.5 KB of memory.

Output. Figure 1 shows the HTML-formatted ranked list of
repositories in our experiment. The report also contains next level
of details such as quality, maintainability, and popularity scores.
Further levels of details (containing values of individual metrics)
can be obtained with additional verbosity levels.

4 CONCLUSION
In this report, we described our efforts to build GitRank, a frame-
work to rank open-source repositories using known source code
metrics. So far, GitRank supports repositories using C/C++ as their
primary languages and uses a combination of GrimoireLab toolkit
and publicly-available tools for the implementation. As per our eval-
uation, GitRank is highly-parallel and performs reasonably well
on commodity CPUs. We, however, did not explore applications of
GitRank in this hackathon and consider this as a separate project
that we plan to explore in the future.

6We, however, envision that the weights could be adjusted by the GitRank user.

GitRank: A Framework to Rank GitHub Repositories

MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

REFERENCES
[1] Rajiv D. Banker, Gordon B. Davis, and Sandra A. Slaughter. Software development
practices, software complexity, and software maintenance performance: A field
study. Manage. Sci., apr 1998.

[2] Christian Bird, Nachiappan Nagappan, Brendan Murphy, Harald Gall, and
Premkumar Devanbu. Don’t Touch My Code! Examining the Effects of Owner-
ship on Software Quality. In Proceedings of the 19th ACM SIGSOFT Symposium and
the 13th European Conference on Foundations of Software Engineering, ESEC/FSE
’11, 2011.

[3] Bitergia Inc. GrimoireLab: Free, Libre, Open Source Tools for Software Develop-

ment Analytics. Available at https://chaoss.github.io/grimoirelab/.

[4] Hudson Borges and Marco Tulio Valente. What’s in a GitHub Star? Understanding
Repository Starring Practices in a Social Coding Platform. CoRR, abs/1811.07643,
2018.

[5] Kyriakos Chatzidimitriou, Michail Papamichail, Themistoklis Diamantopoulos,
Michail Tsapanos, and Andreas Symeonidis. npm-Miner: An Infrastructure for
Measuring the Quality of the npm Registry. In 2018 IEEE/ACM 15th International
Conference on Mining Software Repositories (MSR), 2018.

[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy
Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens
Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,
Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss,
Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,
Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike,
Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight,
Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario
Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating
Large Language Models Trained on Code. 2021.

[7] David Wheeler. FlawFinder. Available at https://https://dwheeler.com/flawfinder.
[8] Santiago Dueñas, Valerio Cosentino, Gregorio Robles, and Jesus M Gonzalez-
Barahona. Perceval: software project data at your will. In Proceedings of the 40th
International Conference on Software Engineering: Companion Proceeedings, pages
1–4. ACM, 2018.

[26] Sebastiano Panichella, Venera Arnaoudova, Massimiliano Di Penta, and Giuliano
Antoniol. Would static analysis tools help developers with code reviews? In
2015 IEEE 22nd International Conference on Software Analysis, Evolution, and
Reengineering (SANER), pages 161–170, 2015.

[27] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lam-
ple. Unsupervised Translation of Programming Languages. Advances in Neural
Information Processing Systems, 2020.

[28] Ioannis Samoladas, Georgios Gousios, Diomidis Spinellis, and Ioannis Stamelos.
The SQO-OSS Quality Model: Measurement Based Open Source Software Eval-
uation. In Barbara Russo, Ernesto Damiani, Scott Hissam, Björn Lundell, and
Giancarlo Succi, editors, Open Source Development, Communities and Quality,
Boston, MA, 2008. Springer US.

[29] Diomidis Spinellis, Georgios Gousios, Vassilios Karakoidas, Panagiotis Louridas,
Paul Adams, Ioannis Samoladas, and Ioannis Stamelos. Evaluating the Quality of
Open Source Software. Electr. Notes Theor. Comput. Sci., 03 2009.

[30] Ioannis Stamelos, Lefteris Angelis, Apostolos Oikonomou, and Georgios L. Bleris.
Information

Code Quality Analysis in Open Source Software Development.
Systems Journal, 2002.

[31] Terry Yin and other contributors. A Simple Code Complexity Analyser. Available

at https://github.com/terryyin/lizard.

[32] Kurt D. Welker, Paul W. Oman, and Gerald G. Atkinson. Development and
Application of an Automated Source Code Maintainability Index. Journal of
Software Maintenance, may 1997.

[33] Michihiro Yasunaga and Percy Liang. Graph-based, Self-supervised Program
Repair from Diagnostic Feedback. In International Conference on Machine Learning
(ICML), 2020.

[34] Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marucs, Nesime Tatbul,
Jesmin Jahan Tithi, Niranjan Hasabnis, Paul Petersen, Timothy Mattson, Tim
Kraska, Pradeep Dubey, Vivek Sarkar, and Justin Gottschlich. MISIM: An End-to-
End Neural Code Similarity System. arXiv preprint arXiv:2006.05265, 2020.

[9] Santiago Dueñas, Valerio Cosentino, Jesus M. Gonzalez-Barahona, Alvaro del
Castillo San Felix, Daniel Izquierdo-Cortazar, Luis Cañas-Díaz, and Alberto Pérez
García-Plaza. GrimoireLab: A toolset for software development analytics. 7(e601).
[10] Maurice H. Halstead. Elements of Software Science (Operating and Programming

Systems Series). Elsevier Science Inc., USA, 1977.

[11] Niranjan Hasabnis and Justin Gottschlich. ControlFlag: A Self-Supervised Idiosyn-
cratic Pattern Detection System for Software Control Structures. In Proceedings of
the 5th ACM SIGPLAN International Symposium on Machine Programming, MAPS
2021, 2021.

[12] Niranjan Hasabnis and R Sekar. Extracting Instruction Semantics via Symbolic Ex-
ecution of Code Generators. In Proceedings of the 24th ACM SIGSOFT International
Symposium on Foundations of Software Engineering, FSE, 2016.

[13] Niranjan Hasabnis and R. Sekar. Lifting Assembly to Intermediate Representation:
In Proceedings of the Twenty-First
A Novel Approach Leveraging Compilers.
International Conference on Architectural Support for Programming Languages and
Operating Systems, ASPLOS, 2016.

[14] GitHub Inc. GitHub REST API. Available at https://docs.github.com/en/rest.
[15] Google Inc. cpplint - static code checker for C++. Available at https://github.

com/cpplint/cpplint.

[16] ISO. ISO/IEC 25000:5000. Available at https://www.iso.org/standard/35683.html.
[17] Oskar Jarczyk, Błażej Gruszka, Szymon Jaroszewicz, Leszek Bukowski, and Adam
Wierzbicki. GitHub Projects. Quality Analysis of Open-Source Software. Springer
International Publishing, Cham, 2014.

[18] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. Aroma:
Code Recommendation via Structural Code Search. Proceedings of the ACM on
Programming Languages, 2019.

[19] Jeremy Ludwig and Devin Cline. CBR Insight: Measure and Visualize Source
In 2019 IEEE/ACM International Conference on Technical Debt

Code Quality.
(TechDebt). IEEE, 2019.

[20] Jeremy Ludwig, Steven Xu, and Frederick Webber. Compiling Static Software
Metrics for Reliability and Maintainability from GitHub Repositories. In 2017
IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2017.
[21] T.J. McCabe. A Complexity Measure. IEEE Transactions on Software Engineering,

SE-2(4):308–320, 1976.

[22] Nuthan Munaiah, Steven Kroh, Craig Cabrey, and Meiyappan Nagappan. Curating
Github for Engineered Software Projects. Empirical Software Engineering, 2017.
[23] Vu Nguyen, Sophia Deeds-rubin, Thomas Tan, and Barry Boehm. A SLOC

Counting Standard. In COCOMO II Forum, 2007.
[24] NPM, Inc. NPM. Available at https://www.npmjs.com/.
[25] P. Oman and J. Hagemeister. Metrics for Assessing a Software System’s Main-
tainability. In Proceedings Conference on Software Maintenance 1992, 1992.

