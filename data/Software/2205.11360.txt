2
2
0
2

y
a
M
3
2

]
P
S
.
s
s
e
e
[

1
v
0
6
3
1
1
.
5
0
2
2
:
v
i
X
r
a

Persistent Weak Interferer Detection for WiFi Networks: A Deep
Learning Based Approach

1

Andrew Adams, Richard F. Obrecht, Miller Wilt,
Daniel Barcklow, Bennett Blitz, Daniel Chew

The Johns Hopkins University Applied Physics Laboratory

Abstract—In this paper, we explore the use of multiple deep learning techniques to detect weak interference in WiFi networks.
Given the low interference signal levels involved, this scenario tends to be difﬁcult to detect. However, even signal-to-interference
ratios exceeding 20 dB can cause signiﬁcant throughput degradation and latency. Furthermore, the resultant packet error rate may
not be enough to force the WiFi network to fallback to a more robust physical layer conﬁguration. Deep learning applied directly
to sampled radio frequency data has the potential to perform detection much cheaper than successive interference cancellation,
which is important for real-time persistent network monitoring. The techniques explored in this work include maximum softmax
probability, distance metric learning, variational autoencoder, and autoreggressive log-likelihood. We also introduce the notion of
generalized outlier exposure for these techniques, and show its importance in detecting weak interference. Our results indicate
that with outlier exposure, maximum softmax probability, distance metric learning, and autoreggresive log-likelihood are capable
of reliably detecting interference more than 20 dB below the 802.11 speciﬁed minimum sensitivity levels. We believe this presents a
unique software solution to real-time, persistent network monitoring.

Index Terms—Anomaly detection, artiﬁcial neural networks, autoregressive, bit error rate, deep learning, distance learning,
electromagnetic interference, information entropy, NOMA, OFDM, packet loss, radio spectrum monitoring, statistical distribution,
statistical learning, wireless LAN

I. INTRODUCTION

W IRELESS connectivity has become ubiquitous in mod-

ern society. This is illustrated by the vast amounts of
internet trafﬁc we produce. In 2020, global internet trafﬁc
reached an average of 175 exabytes per month, and is expected
to reach 300 exabytes per month by 2022 [1]. This is an enor-
mous amount of data being trafﬁcked, which indicates the level
of integration internet access has in our daily lives. Although
our consumption of wireless services continues to increase,
spectrum regulations have lagged in supporting the increased
trafﬁc rates [2]. Nowhere is this more evident than the un-
licensed Industrial, Scientiﬁc, and Medical (ISM) frequency
bands, where WiFi is forced to operate. Here WiFi competes
with itself (co-located networks) and other popular wireless
standards for spectrum access. These include Long Term
Evolution License Assisted Access (LTE-LAA), Bluetooth,
ZigBee, cordless phones, radio frequency identiﬁcation (RFID)
tags, and many proprietary wireless devices and protocols. As
such, the ability to detect and mitigate sources of interference
is central to any successful WiFi network deployment.

Interference occurs when WiFi transmissions collide with
other trafﬁc attempting to use the same or adjacent overlapping
channels. This induces various signal processing errors within
the intended receiver resulting in reduced network throughput
and increased network latency. As discussed in Ref. [3], even
relatively weak levels of interference can severely degrade
WiFi network performance. The authors demonstrate a signif-
icant reduction in throughput and an increase in latency with
signal-to-interferer (SIR) ratios as large as 20 dB. However,
these can be the most difﬁcult to detect from a network mon-

itoring perspective, whereby the necessary mitigation strategy
is never initiated.

In this paper, we explore multiple deep learning (DL)
methods to enable persistent detection of weak interferers. Our
motivation to explore DL for this application is twofold. First,
detection of weak interferers enables network administrators
to execute appropriate mitigation strategies, either manually
or autonomously, with the most effective being frequency
agility [3]. Furthermore, given the consistent use of WiFi
to facilitate internet trafﬁc, a persistent monitoring capability
can run 24/7, and alert network administrators only when
mitigation is required.

Second, DL has the potential to provide a persistent mon-
itoring capability at a fraction of the computational cost of
existing techniques, including successive interference cancel-
lation (SIC) [4]. When employing SIC, any unwanted trafﬁc
is ﬁrst demodulated to determine content, re-modulated, and
then subtracted from the input sample stream. The resulting
residue is then analyzed for indications of desired signal trafﬁc
or potential signals of interest (SOI). This is a highly complex
process, where the performance depends heavily upon accurate
estimates of both hardware and channel impairments by the in-
tended receiver. This process is discussed at length in Ref. [5],
where in-phase/quadrature (IQ) imbalance, direct current (DC)
offset, and power ampliﬁer (PA) non-linearities are shown
to reduce the performance of non-orthogonal multiple-access
(NOMA) communications systems. Another example is the bit
error rate (BER) ﬂoor induced by uncompensated transmit-
ter windowing in orthogonal frequency division multiplexing
(OFDM) communications systems [6].

0000–0000/00$00.00 © 2022 IEEE

Given these motivating factors, we demonstrate the use of
select DL models to detect weak interferers directly from the

 
 
 
 
 
 
2

Fig. 2: Victim STA interference levels as a function of aggres-
sor distance.

speciﬁed operating signal-to-noise ratio, SNRmin(dB), for each
subcarrier modulation type speciﬁed. These include binary
phase shift keying (BPSK), quadrature phase shift keying
(QPSK), and quadrature amplitude modulation (QAM) as
listed in Table I. For our experiments, SNR and SIR are
combined into a metric termed signal-to-interference-and-
noise ratio (SINR). Since the interference level is not enough
to force the victim STA to a lower MCS value, the SINR is
assumed to include an SIR range between SNRmin(dB) and
SNRmin − 20 dB when present. This is expressed mathemati-
cally as

SINR = 10−SNRmin/10

= 10−SIR/10 + 10−SNR/10,

(1)

−SNRmin−20
10

where SNRmin is deﬁned in Table I. SIR is drawn from
], and SNR
a uniform distribution ∈ [10
consumes the difference. The result is a range of continuous
SINR values dominated by SIR or SNR on either extreme. The
outcome of each experiment is thus the quantitative measure
of our ability to use DL to accurately detect co-channel
intereference at a given SIR when present.

−SNRmin
10

, 10

III. RELEVANT WORKS

To our knowledge, our DL based approach to weak in-
terferer detection is novel. However, relevant performance
and complexity comparisons can be made to multiple access
schemes employing SIC. Brieﬂy, SIC allows multiple transmit-
ters to occupy a common channel simultaneously. At the re-
ceiver, any unwanted trafﬁc is ﬁrst demodulated, re-modulated,
and then subtracted from the input sample stream. To do
so, the desired trafﬁc is treated as co-channel interference

TABLE I: Minimum SINR per subcarrier modulation type.

Modulation

SNRmin (dB)

BPSK
QPSK
16-QAM
64-QAM

3.92
6.92
11.92
19.92

Fig. 1: An example of a co-channel interference scenario.

received sample stream without the need for SIC or any other
unwanted signal removal or impairment compensation.

II. EXPERIMENT SETUP

For experimentation purposes, we chose to replicate the
common occurrence of co-channel interference caused by fre-
quency reuse of wireless local area network (WLAN) channels
between co-located WiFi networks. This scenario is illustrated
in Figure 1, where two simple networks, each consisting of
a single access point (AP) and wireless station (STA), are
separated geographically but attempt trafﬁc exchange on the
same WLAN channel. The victim network is so named because
in this scenario it began trafﬁc exchange on a particular WLAN
channel ﬁrst; the aggressor network is so named because in
this scenario it began trafﬁc exchange on the same WLAN
channel at some point thereafter. Victim and aggressor roles
hold relative meaning only, and could be reversed at any time.
In this scenario, the aggressor network operates in close
proximity, such as an adjacent ofﬁce space or outdoor block.
As the aggressor network begins trafﬁc exchange, the vic-
tim STA experiences co-channel interference; the magnitude
of which depends upon the aggressor STA transmit power,
standoff, and signal attenuation due to intervening walls or
partitions. Assuming a victim AP power level of +20 dBm,
aggressor STA transmit power levels ranging from 0 dBm
to +20 dBm, aggressor STA standoffs ranging from 1 m to
100 m, and 6 dB partition loss, Figure 2 shows the resultant
signal-to-interference ratios (SIR) at the victim STA. These
levels suggest the victim STA packet error rates (PER) may
not exceed 10%, which means the co-channel interference may
not be enough to force it to a lower modulation coding scheme
(MCS) [7]. However, as demonstrated in Ref. [3], it is certainly
enough to reduce victim network throughput and quality of
service (QoS). Hence our desire to pursue detection methods
relevant to weak interferers.

We therefore explore the ability of select DL techniques
to detect weak interferers for various static MCS values.
Assuming a victim STA OFDM physical layer (PHY) con-
ﬁguration, Ref. [7] speciﬁes the minimum required receiver
sensitivity for each MCS value. Assuming the noise ﬁgure and
implementation loss speciﬁed there, room temperature, and
a noise bandwidth of 20 MHz, we can derive the minimum

while demodulating the undesired trafﬁc; hence our basis for
comparison. Finally, the desired trafﬁc is then recovered from
the resulting residue when present. For packetized transmis-
sions, this is done in succession to recover the entire message,
hence the term successive interference cancellation. We do
not present a quantitative analysis of relative complexity here,
as that entails algorithmic implementation details for either
approach. We do however highlight relevant works discussing
current perceived limitations of SIC as motivation to explore
other approaches.

Ref. [4] discusses the medium access control (MAC) layer
throughput gains provided by SIC as compared to a time shar-
ing approach such as time division multiple access (TDMA).
Their conclusion is that throughput gains are marginal for dis-
tinct transmitter-receiver pairs. That is, as a means to combat
co-channel interference, SIC provides little to no beneﬁt. The
reason being that for SIC to be feasible, certain requirements
regarding transmitter bit rates and signal strength must be met.
For multiple transmitters and a common receiver, SIC then
provides modest gains as the transmitters can coordinate to
meet said feasibility requirements.

Ref. [4] assumes perfect cancellation, which is of course
unobtainable. Ref. [5] illuminates this challenge while dis-
cussing performance degradation within NOMA schemes due
to RF front-end impairments; NOMA relies heavily upon
SIC to eliminate multi-user interference. To minimize receiver
complexity, impairments such as IQ imbalance, DC offset,
and PA non-linearities are often ignored. While these often
have little effect on TDMA schemes, these present signiﬁcant
performance degradation to NOMA schemes in the form of
imperfect cancellation. Ref. [6] discusses a similar concern
with OFDM windowing techniques, which are often not stan-
dardized or known a priori by the intended receiver.

IV. DEEP LEARNING BASED APPROACHES

In this work, we demonstrate detection of weak interferers
using various deep learning approaches. Deep learning is
a form of representation learning, where complex concepts
are built from combinations of simpler ones [8]. These are
commonly implemented using artiﬁcial neural networks with
numerous hidden layers, i.e. deep, where each layer tends to
learn different features from the preceding layers. This results
in a speciﬁc concept being learned by iteratively traversing
the network, depth-wise, as part of an optimization algorithm,
such as stochastic gradient descent. Once trained, the inferred
network approximates a function that produces the desired
output from a given input, and can be expressed mathemat-
ically as y = f (x), where f (x) is not
limited to linear
functions. Deep learning has been shown adept in performing
various learning tasks across many disciplines and applica-
tions. Examples related to wireless communications include
modulation recognition directly from baseband samples [9–
11] and reinforcement learning Q-function approximation for
network resource scheduling [12, 13]. We experiment with
deep anomaly detection as a means to detect weak co-channel
interference within WiFi networks. As such, we train neu-
ral networks to produce multiple feature representations and

3

Fig. 3: An OOD detection application.

anomaly scores indicative of situations where weak interferers
may be reducing network throughput or inducing additional
trafﬁc latency.

For the interested reader, Ref. [14] provides a comprehen-
sive review of deep anomaly detection methods. We instead
focus on a closely related form of deep anomaly detection
called out-of-distribution (OOD) detection, which is attrac-
tive for this application because it enables anomaly detection
is, we assume we have
in an open-world scenario;
sufﬁcient labeled examples of what normal looks like, but
make no assumptions with respect to the types of anomalies
in-distribution (ID)
encountered once deployed. As such,
refers to the trafﬁc exchanged between nodes within our own
network without any weak interferers present; OOD then refers
to similar trafﬁc exchange, but in the presence of weak co-
channel interference.

that

Since we are processing baseband (IQ) samples directly,
normal necessarily depends upon the various network physical
layer conﬁgurations. Since this is an in-network monitoring
application, we assume we can detect each packet and recover
the PHY and MAC headers such that the MCS and packet
duration are known prior to network inference. This reduces
the scope of our experiments to OOD detection, and prevents
the detection algorithms from processing noise only. Figure 3
illustrates how this application may be deployed alongside
regular trafﬁc processing on a given observation node or AP.
including Refs. [14–
16], distinguish OOD detection from other anomaly detection
methods by the capability to do so alongside primary learning
tasks such as classiﬁcation. While we believe this to be beneﬁ-
cial, it is not the focus of our experimentation. However, future
work may include this capability to detect weak interferers
with little to no header information. This would certainly be
attractive when attempting to monitor WiFi networks without
ﬁrst joining the network being monitored.

Note that other published works,

A. Out of Distribution Detection

OOD refers to examples drawn from a different distribution
than that which are considered to be ID or normal [14–17].
Detecting OOD examples can be done by training deep neural
networks to generate various representations or metrics of
the input data, which adequately differ between OOD and
ID examples. However, for OOD examples to be detected
within an open-world deployment scenario, we can make no
assumptions regarding the types of anomalies during network
inference. Since that is our goal here, we focus on (4) speciﬁc
methods of OOD, assuming only the availability of labeled

4

2) Signal Model
Examples exhibiting weak interference still primarily rep-
resent valid classes. As such, the use of disjoint data sets in
Ref. [16], or the sample replacement strategy in Ref. [18],
are not applicable here. We therefore need to deﬁne OE for
OFDM signals experiencing co-channel interference.

The continuous time OFDM signal r(t) is described by

r(t) = A

K−1
(cid:88)

∞
(cid:88)

k=0

−∞

+ n(t),

sk,lej2πkδfk(t−lT −(cid:15)T )g(t − lT − (cid:15)T )

(3)

where A = aejθej2πδfct, K is the number of subcarriers, δfk
is the subcarrier spacing, T is the OFDM symbol period, and
sk,l is the modulation symbol value for the kth subcarrier
of the lth OFDM symbol. The function g(t) is the pulse
shape resulting from any transmit or receive ﬁltering, and n(t)
represents zero-mean, complex additive Gaussian noise. a, δfc,
and (cid:15) are the power factor, carrier frequency offset, and time
offset, respectively. In the presence of co-channel interference
I(t), Eqn 3 may be rewritten as

K−1
(cid:88)

∞
(cid:88)

rOE(t) = A

sk,lej2πkδfk(t−lT −(cid:15)T )g(t − lT − (cid:15)T )

−∞
k=0
+ n(t) + I(t),

(4)

where A = aejθej2πδfct. The goal now is to select a model
for I(t), which will serve as DOE

out for OE purposes.

Recall our constraint of making no assumptions about
the types of interference encountered; we desire our OE to
generalize to a broad class of co-channel interference. For in-
spiration, we look to the OFDM jammer effectiveness models
discussed in Ref. [19]. There, Luo et al. compare the effect of
multiple interference models on OFDM receiver BER under
varying channel conditions and SIR. Their conclusion was that
multi-tone interference (MTI) was more effective than either
barrage noise interference (BNI) or partial band interference
(PBI) under additive white Gaussian noise (AWGN). Further-
more, MTI makes no attempt to model a speciﬁc waveform
or protocol, and provides enough conﬁguration ﬂexibility to
model both co-channel and adjacent channel interference. We
therefore model I(t) as

I(t) = ai

J
(cid:88)

j=1

e2πfj t+φj ,

(5)

where ai is the power factor, fj is the frequency of the jth
tone, and φj is the phase of the jth tone drawn from a uniform
distribution over [0, 2π].

The fraction of tones overlapping OFDM subcarriers is
deﬁned as ρ = q/K, where q is drawn from a uniform
distribution over [ K
8 , K]; these were chosen to represent the
continuum between partial and full overlap with each OFDM
symbol in the spectrum. Note that we have no requirement
for J to be a contiguous set of integers with respect to K or
otherwise.

Fig. 4: Illustrating the various OOD notations used throughout
the text.

ID training data: maximum softmax probabilities (MSP),
distance metric learning (DML) similarity scores, varia-
tional autoencoder (VAE) reconstruction loss, and autore-
gressive (AR) model likelihood. We elaborate on each method
following an introduction to outlier exposure.

B. Outlier Exposure

1) Motivation
While many OOD detection models detect anomalies using
learned representations of ID examples only, an auxiliary OOD
dataset may be used to learn heuristics of OOD examples,
a technique known as outlier exposure (OE) [16]. This is
motivated by the use of complex datasets, where anomalies
are difﬁcult to detect, which is particularly of concern here
given the diversity of WiFi signal formats. Hendrycks et al.
represent outliers using disjoint datasets Din and DOE
out ; that
method is not applicable here as each anomalous example
still represents a valid class. In fact, one may ﬁnd parallels
between this application and the notion of domain shift as
discussed in Ref. [17]. Unfortunately, we do not have the same
beneﬁt of using multiple examples to detect weak interferers as
that has the potential to introduce signiﬁcant latency into our
monitoring system. We still utilize OE to train our networks to
be sensitive to the presence of weak interferers; however, our
outliers must be representative of relatively weak co-channel
interference.

Given x drawn from an ID dataset Din, a class label y, x(cid:48)
drawn from an OOD dataset DOE
out , a model f , and the original
learning objective L, outlier exposure is deﬁned by minimizing
the following objective:
(cid:104)

(cid:105)
[LOE (f (x(cid:48)), f (x), y)]

,

E(x,y)∼Din

L (f (x), y) + λE

x(cid:48)∼DOE
out

(2)

where λ is a scaling factor and LOE is a function of f (x(cid:48)),
f (x), and y. In other words, the process is to ﬁrst maximize
the classiﬁcation accuracy using data drawn from Din, and then
to minimize conﬁdence using data drawn from DOE
out .

Note that the complete set of out-of-distribution samples
is represented by Dout, which is difﬁcult or impossible to
know a priori. When applying OE to training procedures,
we are intending to parametrize a subset of Dout, labeled as
DOE
out . During testing, another subset of OOD samples that are
out is introduced, and is deﬁned to be Dtest
different from DOE
out .
The nuances in notation are illustrated by Figure 4.

C. Maximum Softmax Probability

As discussed above, several works derive an OOD detection
capability from a primary task, such as classiﬁcation. MSP,
developed by Hendrycks and Gimpel, is an effective OOD
baseline built on top of classiﬁcation models [15]. Speciﬁcally,
this approach utilizes the predicted (i.e., maximum) softmax
score to determine if a given example is ID or OOD. The
authors demonstrate that while deep neural networks are over-
conﬁdent in their predictions, even those generated from OOD
data, the scores from OOD data are generally lower than
scores from ID data. This enables the detection of OOD data
via a surprisingly simple, but effective method: just compare
the softmax score of a given example to prediction statistics
generated from a set of known ID data.

Unlike many of the classiﬁcation-based OOD detection
works, our primary goal is to determine if an example is
ID or OOD, not to classify it. However, given that MSP
is built on top of a classiﬁcation model, we must create
an auxiliary classiﬁcation task to enable its use. We take
the approach of classifying the (4) OFDM physical
layer
subcarrier modulation types discussed in Section II. However,
given the strong similarity between ID and OOD signal trafﬁc,
we add outlier exposure to make this approach more sensitive
to weak interferers.

1) Outlier Exposure
We leverage the OE approach for multiclass classiﬁcation
as described in Ref. [16], where LOE is deﬁned as the cross-
entropy between the softmax scores of the outlier data and a
uniform distribution. Theoretically (and as we demonstrate be-
low, empirically), this results in the model being less conﬁdent
when classifying signals with weak interferers present.

2) Implementation
For experimentation purposes, we utilize a modiﬁed pre-
activation ResNet-34 [20, 21] architecture, which we describe
here. First, all 2D convolutions are replaced with 1D con-
volutions to handle the shape of the signal data. Second,
similar to Ref. [22], dropout [23] is included in each residual
block right before the second convolutional layer. Lastly, all
strided convolutions are removed and replaced with dilated
convolutions [24]. We take inspiration from the approach
described in Ref. [25] and increase the dilation factor by a
power of two with each residual block up to a factor of 128
before repeating.

D. Distance Metric Learning

DML attempts to learn a distance consistent with semantic
similarity [26]. In other words, a network is trained to generate
representations in an embedding space, where inputs drawn
from the same class or distribution are close to each other,
but also separated from those drawn from a different class or
distribution. The relevant task here is to learn an embedding
representing normal network trafﬁc, which is separable from
that experiencing weak co-channel
interference. Ref. [27]
introduced the triplet loss, which optimized the relative dis-
tance between select triples consisting of an anchor and both
positive (similar) and negative (dissimilar) examples. Ref. [26]

5

introduced the proxy-NCA loss, which improved upon triplet-
loss performance and convergence rates by utilizing proxies
for entire subsets of training data. We utilize the proxy-anchor
loss [28], which takes advantage of both the data-to-data
relations described by the triplet loss and the data-to-proxy
relations described by the proxy-NCA loss.

The proxy-anchor loss is deﬁned as

(cid:96)(X) =

1
|P +|

(cid:88)

p ∈P +



log

1 +

(cid:88)

x ∈X +
p

e−α(s(x,p)−δ)





+

1
|P |

(cid:88)

p ∈P



log

1 +

(cid:88)

x ∈X −
p



eα(s(x,p)+δ)

 ,

(6)

where P is the set of all proxies, P + is the set of positive
proxies within a given batch of data, X +
p is the set of positive
embedding vectors for proxy p, and X −
p is the disjoint set X −
X +
p . Using the cosine similarity s between embeddings x and
p is close to p ∈ P +, and increases
p, (cid:96) decreases when x ∈ X +
otherwise. As a result, p and its positive examples are pulled
closer together, while p and its negative examples are pushed
further apart. The proxies are learned while training [26], and
α and δ are used for scaling and margin, respectively,

Using proxy-anchor loss, we can train a network to learn
unique embeddings representing normal network trafﬁc for
each of the valid subcarrier modulation types. This becomes
a classiﬁer-based training approach, where the (4) classes
are deﬁned in Table I. However, given the relatively weak
interference levels we expect to identify for this application,
we utilize OE during training to make the trained network
sensitive to weak interferers.
1) Outlier Exposure
For our application, we seek to learn unique embeddings for
the (4) classes representing valid subcarrier modulation types
and the (4) classes representing valid subcarrier modulation
types experiencing weak interference. This results in (8)
classes total, with a single classiﬁer-based learning objective
utilizing the proxy-anchor loss. Therefore, Eqn 2 may be
reformulated as

E

(x,y)∼Din,DOE
out

[L(f (x), y)] ,

(7)

where L is deﬁned by Eqn 6.

2) OOD Score
An OOD metric is based on the distance between embed-
dings representing normal network trafﬁc and those experienc-
ing weak interference. Each embedding ∈ Rd, where d is the
number of dimensions, e.g. 64, 128, 256, etc. Many distance
metrics however lose meaning as d increases. For example,
Ref. [29] attempts to mitigate this limitation through the
use of fractional distance metrics. Cosine similarity does not
have the same limitation, but substantial information may be
lost when calculating the similarity between each embedding
and its mean values only. We therefore look to model the
probability density function (PDF) of each dimension in order
to fully characterize normal behavior. Since each dimension
has no incentive to strictly follow a normal distribution, each

6

of VAEs are summarized below; see Refs. [33–37] for more
information.

A VAE is a continuous latent variable model. An unobserved
latent representation z is used to generate an observation x.
The probability distribution pθ(x|z), parameterized by θ, de-
scribes the process of generating x from a lower-dimensional
representation z described by prior p(z). Assuming the latent
probability distribution p(z) is known, the goal is to learn the
marginal likelihood of the data:

max
φ,θ

Eqφ(z|x) [log pθ(x|z)] ,

(9)

which may be rewritten as

Fig. 5: Various examples of the Johnson SU PDF.

log pθ(x|z) = DKL(q(z|x) || p(z)) + L(θ, φ; x, z),

(10)

dimension is modeled as an unbounded Johnson distribution
(Johnson SU) with the following PDF:

f (x) =

√

δ
√

e− 1

2 (γ+δ sinh−1 z)2

,

(8)

λ

2π

z2 + 1
where z is parametrized as x−ξ
λ , ξ is a location parameter, λ
is a scaling parameter, and γ and δ are shape parameters [30].
The Johnson SU PDF is capable of modeling various asym-
metric distributions; see Figure 5, where each curve represents
the indicated (γ, δ, λ, ξ) tuple.

3) Implementation
For experimentation purposes, we utilize a ResNet-34 [21]
architecture, but with the convolution operations replaced by
the dilated convolution operation [24]. These expand the re-
ceptive ﬁeld without loss of resolution. Since we are operating
on complex data, the I and Q components of each sample are
separated and placed in separate input channels, similar to the
separate R, G, and B channels utilized for image processing.
The ResNet output is pooled and ﬂattened, and a dropout of
0.25 applied. The result is then run through a dense layer with
an output width equal to our desired embedding size of 128.
Training is performed using AdamW [31] optimization and a
cosine annealing [32] period of 10.

E. Variational Autoencoder

A variational autoencoder (VAE) [33] is a popular unsu-
pervised learning technique, and is capable of generating or
reconstructing examples similar to the input. The model is
trained on signals that are considered normal or expected.
In inference mode, when the model is given a signal ex-
the reconstruction should be
periencing a weak interferer,
relatively poor, allowing for a method of detection. The loss
metric for a traditional autoencoder is often the mean squared
error between the input x and the reconstructed output ˆx.
The variational approach, however, encodes the inputs as
probability distributions, which regularizes the latent space by
constraining each dimension to be approximately Gaussian.
The decoder then samples the latent space to generate new
noisy examples; sampling is used to overcome the overﬁtting
nature of conventional autoencoders. The mathematical details

where φ, θ parameterize the the encoder and decoder distri-
butions respectively. The DKL(||) term is the non-negative
Kullback-Leibler (KL) divergence between the true and ap-
proximate posterior. To make the optimization tractable,
L(θ, φ; x, z) is maximized:

log pθ(x|z) ≥ L(θ, φ; x, z) = Eqφ(z|x) [log pθ(x|z)]
−DKL (q(z|x)||p(z))

(11)

where the prior p(z) and posterior qφ(z|x) are approximated
as Gaussian. Rewritten more simply, the VAE loss is

LVAE = LMSE + βLKL,

(12)

where LMSE ≡ Eqφ(z|x) [log pθ(x|z)] is commonly taken as the
mean squared error between the input x and the output ˆx, and
LKL has an analytical form under Gaussian assumptions. The
empirical factor β is often introduced to scale LKL, referred
to as a β-VAE. As β → 0, the VAE reduces to the traditional
deterministic autoencoder. Increasing the value of β has been
motivated to further constrain the capacity of the encoder in
order to allow a more factorized latent representation [38, 39].

1) Outlier Exposure
The goal is to minimize LVAE using ID data while simul-
taneously maximizing a loss associated with outlier exposure,
deﬁned to be LOE, which utilizes both ID and OOD data. OE
is incorporated into Eqn 12 as a penalty term:

Ex∼Din LVAE(x) + λE

x∼Din, x(cid:48)∼DOE
out

LOE(x, x(cid:48)),

(13)

where λ is a scaling factor, Din represents the ID dataset, and
DOE
out represents the OOD dataset used during training. The loss
function with the OE modiﬁcation is

L = LVAE − λLOE,

(14)

where LOE may be calculated using the approaches described
in Ref. [37]. We choose to calculate LOE using the input x
and the corresponding reconstructed output ˆx:

LOE = σ (MSEOOD(x(cid:48), ˆx(cid:48)) − MSEID(x, ˆx)) ,

(15)

where σ is the typical sigmoid function, and x and x(cid:48) live
in Din and DOE
out , respectively. Eqn 14 wants to reconstruct ID
data well, while poorly reconstructing OOD data.

p(x) =

T
(cid:89)

t=1

p(xt|x1, ..., xt−1).

(16)

A. Signal Synthesis

2) Implementation
The encoder is used to build two latent representations, one
each for I and Q separately. Then, two identical decoding
networks are used to regenerate both I and Q, which are then
concatenated to reconstruct the complex input. The encoder
block is constructed out of strided convolutional layers fol-
lowed by batch normalization and a ReLU activation function.
It is relatively shallow, built from only two encoding blocks,
which effectively reduces the IQ input length by a factor
of four while increasing the channel dimension by a factor
of four. A ﬂattening operation is then performed to create
the latent linear layer, which funnels the encoding features
from 512 to an empirically determined size of 128. The
decoder is symmetric to the encoder, but uses strided transpose
convolutional layers.

F. Autoregressive Log Likelihood

Deep autoregressive models are a form of generative model
that learn a distribution over a given input space. Unlike many
other generative models, e.g., GANs and VAEs, autoregressive
models produce a tractable likelihood for a given input. Specif-
ically, the joint probability of each input x = {x1, ..., xT } is
factorized into a product of conditional probabilities:

Stated another way, each input element xt is conditioned on all
data from previous time steps. Traditionally, these conditional
probability distributions are modeled using either recurrent
neural networks (e.g., PixelRNN [40]) or convolutional neural
networks (e.g., PixelCNN, WaveNet [25, 40]), the latter trading
an unbounded receptive ﬁeld for reduced computation cost at
training due to parallelization.

Using generative models for anomaly detection is a well
established approach [41], and deep autoregressive models
are especially appealing for anomaly detection since they can
tractably compute the likelihood of complex inputs. Similar
to the MSP method, this theoretically1 facilitates the detection
of OOD data by just comparing the computed likelihood of
a given example to likelihood statistics generated from a set
of known ID data. As in our other methods, we make use of
outlier exposure to make this approach more sensitive to weak
interferers.

1) Outlier Exposure
We leverage the OE approach for density estimation as
described in Ref. [16], where LOE is deﬁned as the margin
loss over the log-likelihood difference between ID and OOD
data:

LOE = max(0, m + NLL(fθ(x)) − NLL(fθ(x(cid:48)))),

(17)

where fθ is a neural network with parameters θ, NLL is the
negative log-likelihood, x and x(cid:48) are ID and OOD examples
respectively. m is a margin hyperparameter.

1Recently, Nalisnick et al. demonstrated that deep generative models often
assign higher likelihoods to OOD data compared to ID data [42]; however,
we ﬁnd our model does not suffer from this problem. We hypothesize this
may due to the similarity of our ID and OOD data.

7

2) Implementation
The autoregressive model is based on temporal convolu-
tional networks (TCNs), a family of architectures for sequence
modeling tasks [43]. TCNs are inspired by the WaveNet
architecture [25] while being less complex to implement. Like
WaveNet, they make use of appropriate zero padding and
causal convolutions to ensure the model is autoregressive in
nature. In this work, we eschew the original architecture devel-
oped in Ref. [43] and instead use building blocks common to
our other methods to enable a fairer comparison. Speciﬁcally,
the 1D ResNet-34 architecture developed in the MSP and
DML methods is used as the TCN backbone, modifying all
convolutions to be causal. Note that this backbone is not
enough by itself, as we must model the conditional proba-
bilities p(xt|x1, ..., xt−1). To accomplish this, the backbone is
connected to a mixture density network (MDN) [44], which
models the conditional distributions as a mixture of Gaussians.
Note that this is in contrast to the PixelRNN, PixelCNN,
and WaveNet architectures, which model the conditional prob-
abilities using a softmax distribution. Through testing, we
found that the MDN improved detection performance while
providing faster training and inference speed.

V. EXPERIMENTS

The samples generated for each experiment follow the
scenario described in Section II and the signal processing
chain described in Section IV. The processing chain, including
the OOD detection sub-system, is described in Figure 3. ID
samples are generated by following the 802.11 OFDM speci-
ﬁcation, and represent the nominal case when an aggressor is
not currently interfering with the victim STA. At the point in
the receive chain where OOD detection takes place, we assume
coarse synchronization and equalization have already been
achieved; therefore, synchronization error is not considered
in this study.

OOD samples are generated in a similar manner as ID
samples, except that a weak interfering signal is also added.
We test with two different sources of co-channel interference;
another 802.11 OFDM signal and an 802.11 DSSS signal. The
OFDM interferer is identical in structure to the victim STA
OFDM signal described by Eqn 3. The DSSS interferer is a
DBPSK DSSS signal carrying a randomly generated payload,
and is described as

rDSSS(t) = aejθej2πδfctspxmg(t − mT ),

(18)

where sp represents a pseudo-random spreading sequence of
length P ; the duration of each value is T /P seconds and can
take values of ±1 sec. The term xm represents each DBPSK
symbol of duration of T seconds, which also takes on values
of ±1 sec. g(t) represents the impulse response of any receive
or transmit ﬁltering, θ is a constant phase offset in the range
[0, 2π), δfc is the carrier frequency offset, and a is a power
factor.

As discussed in Section II,

the victim STA subcarrier
modulation schemes are selected from Table I, which also
deﬁnes the required SINR. An appropriate SIR is then selected

TABLE II: OOD test dataset notations used during experimen-
tation.

Dataset

Interferer

Apply Channel Effects to Interferer

(Dtest
(Dtest
(Dtest
(Dtest

out )1
out )2
out )3
out )4

DSSS
DSSS
OFDM
OFDM

No
Yes
No
Yes

and applied, and the remaining signal power is consumed
by SNR. Additionally, the victim and aggressor STAs are
not co-located, meaning that each signal experiences different
channel effects. Assuming channel equalization in the receiver,
corrections made to the desired OFDM signal result in further
degradation to the interfering signal. An indoor WLAN TGn
channel is used to model degradation to the aggressor STA
signal at the receiver.

B. Dataset Generation

Two of the four datasets described in Section IV-B1 are
used to evaluate the performance of our approaches to weak
interferer detection: Din and Dtest
out . Din is used as an ID ref-
erence, and the corresponding output considered the nominal
ID representation. Dtest
out contains examples with interferers at
varying SIR values. To evaluate the ability of each model
to generalize across different types of interferers, each Dtest
out
contains either an 802.11 OFDM interferer or an 802.11 DSSS
interferer. To better understand how channel effects to each
interferer affects model performance, four datasets are used,
and include examples with and without channel effects; see
Table II for a summary.

In addition to varying interferer types, detector performance
is evaluated with and without OE. Networks trained with
OE require additional datasets, labeled DOE
out , which represent
cases where parameterized co-channel interference is present;
Section IV-B2 provides a detailed description of the MTI-
based interferers used during training. Note that the goal is for
DOE
out )n, even though the datasets have
distinct signal models sharing minimal physical structure.

out to generalize to (Dtest

The datasets are formatted to have the following dimen-

sions:

[Dataset dimensions] × [Input sample dimensions] =

[n mod, n sir bins, n batches, batch size] × [ block size, n channels],

where the n mod dimension supports detector characterization
for speciﬁc MCS values, allowing us to focus on model
performance across various SIR values. To provide detailed
detector performance across SIR values, SIR is binned into
(14) equi-spaced linear values, annotated by n sir bins. The
variables n batches and batch size represent the number of
batches per dataset and number of items per batch, respec-
tively. Lastly, block size represents the number of complex
samples comprising each example, and n channels are the real
and imaginary components of the complex data.

C. Inference

To generate a comprehensive set of performance curves,
inference results are obtained for all combinations of models

8

and datasets (Dtest
out )n. Note that all models take input samples
of size [960, 2] and produce a single inference output score,
which represents a metric that is used to ﬂag OOD samples.
The eight models used for inference are the four architectures
described in this paper, trained both with and without OE. The
trained models are listed in Table IV.

For each of the (8) trained models, the Din and each of the
four Dtest
out datasets is provided as input to produce a total of
(5) sets of inference output scores per model: (1) set of ID
scores and (4) sets of OOD scores. Since there are a total
of 4 × 14 × 1024 input samples per dataset, there are also
a total of 4 × 14 × 1024 inference output scores per dataset.
Figure 6 shows the expansion of inference output data for all
combinations of models and datasets.

Fig. 6: Combining datasets and models from Tables II and IV
for inference.

D. Evaluation

We deﬁne an experiment as the evaluation of performance
metrics given a choice of model from Table IV and Dtest
out from
Table II. For each experiment, a set of both ID and OOD
inference scores are generated, which are then combined to
construct a receiver operating characteristic (ROC) curve. This
is an appealing performance metric given that a threshold does
not need to be speciﬁed, but can be chosen later for a speciﬁc
operational use-case.

A ROC curve is deﬁned as the relationship between the
true positive rate (TPR) and false positive rate (FPR) over a
varying threshold. TPR and FPR are calculated as

TPR =

TP
TP + FP

,

FPR =

FP
FN + TN

,

(19)

where TP, TN, FP, FN are the number of true positives, true
negatives, false positives, and false negatives, respectively. We

TABLE III: Deﬁnitions of dataset dimensions.

Name

Value

Description

n mod
n sir bins
n batches
batch size

block size
n channels

4
14
16
64

960
2

number of mod. schemes used by victim STA
number of SIR bins for plotting
number of batches per SIR bin
number of items in a batch

number of complex signal samples considered
number of channels (real and imaginary)

TABLE IV: The models that have been evaluated.

TABLE V: Common training parameters.

9

Model

Section OE Training

# Parameters

AR
AR, OE
DML
DML, OE
MSP
MSP, OE
VAE
VAE, OE

IV-F
IV-F
IV-D
IV-D
IV-C
IV-C
IV-E
IV-E

No
Yes
No
Yes
No
Yes
No
Yes

7556k
7556k
7250k
7250k
7219k
7219k
278k
278k

deﬁne OOD and ID samples to be the positive and negative
classes respectively. Since we have discretized the SIR range
into 14 bins, there are 14 ROC curves per modulation experi-
ment, where each curve is constructed from 1024 ID and 1024
OOD samples.

The area under the ROC curve (AUROC) metric is used in
order to reduce each curve to a scalar performance indicator
for ease of comparison across experiments and SIR bins. Since
there are 4 × 14 ROC curves generated for each experiment,
one for each subcarrier modulation type and SIR bin, there
are also 4 × 14 AUROC scores for each experiment. This is
further illustrated in Figure 7.

VI. RESULTS

Figure 8 summarizes the detection performance of each of
the DL approaches discussed in Section IV. Each graph plots
AUROC scores vs. SIR bin values for one of the subcarrier
modulation types listed in Table I. The datasets used to
calculate AUROC scores were (Dtest
out )4. Note that
each point on each plot represents the area under a single ROC
curve generated from 2048 inference scores. Experiments on
models trained with OE are plotted as solid lines, and those
without OE plotted as dotted lines.

out )2 and (Dtest

Overall, the AR, DML, and MSP models all perform well
with OE applied. For all subcarrier modulation types, each
exhibit a monotonically increasing shape as SIR decreases.
The DML model does deviate slightly for some QPSK SIR
values, but exhibits the best overall performance for 64-QAM.
This may be an indication that more than 2048 inference scores
are required for each SIR value. The AR model exhibits the
least difference in performance between its base model and
that with OE applied. The VAE model clearly needs greater
SIR levels to reach similar AUROC values, especially for

Fig. 7: Combining inference scores in order to evaluate an
experiment.

Optimizer
Weight Decay
LR
Batch Size

AdamW
0
0.001
64

TABLE VI: Model speciﬁc training parameters.

Parameter

AR

DML

LR Scheduler
OE Lambda
OE Margin
Epochs
Batches per Epoch

Cosine
1.0
1.0
50
1024

CosAnWR
N/A
N/A
100
512

MSP

Cosine
1.0
N/A
50
1024

VAE

CosAnWR
500.0
2.0
100
65

higher constellation orders. It also appeared to beneﬁt little
from OE. However, given its smaller size as indicated in
Table IV, the VAE model may be an appealing approach for
resource constrained platforms.

Clearly the DML and MSP models perform much better
in this application with OE applied, with the MSP model
exhibiting the greatest beneﬁt. Without it, Figure 8 shows two
prominent peaks in MSP AUROC scores at various SIR values
for both QPSK and 16-QAM subcarrier modulation types. We
believe this to be caused by the network’s failure at generaliz-
ing across perturbations caused by co-channel interference and
overconﬁdence in classiﬁcations. Furthermore, this behavior
seems to occur at lower SIR values, where perturbations would
be at their greatest.

VII. CONCLUSION

In this work we explored multiple DL approaches to per-
sistent weak interference detection in WiFi networks. We also
demonstrated a novel training approach whereby each model
is taught the notion of outliers using a generalized signal
interference model. The result was the ability to detect weak
co-channel and adjacent channel interference at SIR levels
at least 20 dB below speciﬁed minimum sensitivity levels.
As discussed in Ref. [3], this capability has the potential to
signiﬁcantly improve both network throughput and latency. We
also suspect this approach can be ﬁelded at a fraction of the
computational cost of current techniques, including SIC [4],
which is highly susceptible to estimation errors [5] [6]. We
suggest exact computational costs of both approaches be the
subject of future work in this area.

APPENDIX

APPENDIX A: TRAINING PARAMETERS

All models are trained using the AdamW optimizer with
batches of size 64. Two types of learning rate schedulers are
used: cosine and cosine annealing with warm restarts. Table V
shows the list of common training parameters, and Table VI
shows training parameters that varied over model architecture.
Note that OE lambda and OE margin only apply for models
trained with outlier exposure.

10

Fig. 8: AUROC scores over SIR bins (dB) for all models and datasets. SIR bins are displayed in descending order to show
increase in aggressor STA signal strength relative to victim STA signal strength.

APPENDIX B: INTERMEDIATE RESULTS

Figure 9 displays the complete set of ROC curves used to
calculate the AUROC scores in the ﬁrst column of Figure 8,
which corresponds to dataset (Dtest
out )1. The color of each ROC
curve describes the SIR bin index. Table VII provides the SIR
range for each ROC curve given a bin index and subcarrier
modulation type.

TABLE VII: SIR bin ranges (dB) for Figure 9.

Bin Index

BPSK
(max, min)

QPSK
(max, min)

16-QAM
(max, min)

64-QAM
(max, min)

1
2
3
4
5
6
7
8
9
10
11
12
13
14

(24.2, 15.1)
(15.1, 12.4)
(12.4, 10.7)
(10.7, 9.5)
(9.5, 8.6)
(8.6, 7.8)
(7.8, 7.2)
(7.2, 6.6)
(6.6, 6.1)
(6.1, 5.6)
(5.6, 5.2)
(5.2, 4.9)
(4.9, 4.5)
(4.5, 4.2)

(27.2, 18.2)
(18.2, 15.4)
(15.4, 13.8)
(13.8, 12.6)
(12.6, 11.6)
(11.6, 10.8)
(10.8, 10.2)
(10.2, 9.6)
(9.6, 9.1)
(9.1, 8.7)
(8.7, 8.3)
(8.3, 7.9)
(7.9, 7.5)
(7.5, 7.2)

(32.3, 23.2)
(23.2, 20.5)
(20.5, 18.8)
(18.8, 17.6)
(17.6, 16.7)
(16.7, 15.9)
(15.9, 15.2)
(15.2, 14.7)
(14.7, 14.2)
(14.2, 13.7)
(13.7, 13.3)
(13.3, 12.9)
(12.9, 12.6)
(12.6, 12.3)

(40.4, 31.3)
(31.3, 28.5)
(28.5, 26.9)
(26.9, 25.7)
(25.7, 24.7)
(24.7, 24.0)
(24.0, 23.3)
(23.3, 22.7)
(22.7, 22.2)
(22.2, 21.8)
(21.8, 21.4)
(21.4, 21.0)
(21.0, 20.7)
(20.7, 20.4)

ACKNOWLEDGMENT

We would like to thank Matthew Kinsey, Ryan Sawyer, Kate
Tallaksen, and Richard Shaner for their valued contributions
to the various research topics and modeling efforts comprising
this work.

11

Fig. 9: All ROC curves for the DSSS weak interferer with channel effects.

REFERENCES

[1] 2019 Internet Statistics, Trends & Data.

dailywireless.org/internet/usage-statistics/, 2002.
cessed: 2021-05-18.

https://
Ac-

[2] Daniel Chew, Andrew L. Adams, and Jason Uher. Wire-
less Coexistience: Standards, Challenges, and Intelligent
Solutions. IEEE Standards Association, 2021.

[3] Ramakrishna Gummadi, David Wetherall, Ben Green-
stein, and Srinivasan Seshan. Understanding and Mitigat-
ing the Impact of RF Interference on 802.11 Networks.
SIGCOMM Comput. Commun. Rev., 37(4):385–396, Au-
gust 2007.

[4] Souvik Sen, Naveen Santhapuri, Romit Roy Choud-
hury, and Srihari Nelakuditi. Successive Interference
Cancellation: A Back-of-the-Envelope Perspective.
In
Proceedings of the 9th ACM SIGCOMM Workshop on
Hot Topics in Networks, Hotnets-IX, New York, NY,
USA, 2010. Association for Computing Machinery.
[5] Bassant Selim, Sami Muhaidat, Paschalis C. Sofotasios,
Arafat Al-Dweik, Bayan S. Sharif, and Thanos Stouraitis.
Radio-Frequency Front-End Impairments: Performance
Degradation in Nonorthogonal Multiple Access Commu-
nication Systems. IEEE Vehicular Technology Magazine,
14(1):89–97, 2019.

[6] Pinchieh Huang and Yumin Lee. Adaptive Decision
Feedback Orthogonality Restoration Filter for Windowed
OFDM. In IEEE 54th Vehicular Technology Conference.
VTC Fall 2001. Proceedings (Cat. No.01CH37211), vol-
ume 2, pages 1106–1110 vol.2, 2001.

[7] IEEE

for

Standard

Information

Technology–
Telecommunications and Information Exchange between
Systems - Local and Metropolitan Area Networks–
Speciﬁc Requirements - Part 11: Wireless LAN Medium
Access Control
(PHY)
Speciﬁcations. IEEE Std 802.11-2020 (Revision of IEEE
Std 802.11-2016), pages 1–4379, 2021.

(MAC) and Physical Layer

[8] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
http://www.

Deep Learning. MIT Press, 2016.
deeplearningbook.org.

[9] Timothy J O’Shea, Johnathan Corgan, and T. Charles
Clancy. Convolutional Radio Modulation Recognition
Networks, 2016.

[10] Krishna Karra, Scott Kuzdeba, and Josh Petersen. Mod-
ulation Recognition using Hierarchical Deep Neural Net-
works. 2017 IEEE International Symposium on Dynamic
Spectrum Access Networks (DySPAN), pages 1–3, 2017.
[11] S. Rajendran, Wannes Meert, D. Giustiniano, Vincent
Lenders, and S. Pollin. Deep Learning Models for
Wireless Signal Classiﬁcation With Distributed Low-
Cost Spectrum Sensors. IEEE Transactions on Cognitive
Communications and Networking, 4:433–445, 2018.
[12] Hao-Hsuan Chang, Hao Song, Yang Yi, Jianzhong
Zhang, Haibo He, and Lingjia Liu. Distributive Dynamic
Spectrum Access Through Deep Reinforcement Learn-
IEEE
ing: A Reservoir Computing-Based Approach.
Internet of Things Journal, 6(2):1938–1948, 2019.
[13] Pallavi K. Tathe and Manish Sharma. Dynamic Actor-

12

Critic: Reinforcement Learning Based Radio Resource
Scheduling for LTE-Advanced. In 2018 Fourth Interna-
tional Conference on Computing Communication Control
and Automation (ICCUBEA), pages 1–4, 2018.

[14] Guansong Pang, Chunhua Shen, Longbing Cao, and
Anton Van Den Hengel. Deep Learning for Anomaly
Detection. ACM Computing Surveys, 54(2):1–38, Apr
2021.

[15] Dan Hendrycks and Kevin Gimpel. A Baseline for De-
tecting Misclassiﬁed and Out-of-Distribution Examples
in Neural Networks, 2018.

[16] Dan Hendrycks, Mantas Mazeika, and Thomas Diet-
terich. Deep Anomaly Detection with Outlier Exposure,
2019.

[17] Engkarat Techapanurak and Takayuki Okatani. Practical
Evaluation of Out-of-Distribution Detection Methods for
Image Classiﬁcation, 2021.

[18] Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan
Poplin, Mark A. DePristo, Joshua V. Dillon, and Bal-
aji Lakshminarayanan. Likelihood Ratios for Out-of-
Distribution Detection, 2019.

[19] Jung Shim. Wireless Technology: Applications, Manage-

ment, and Security, volume 44. 01 2009.

[20] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun.
Identity Mappings in Deep Residual Networks. ArXiv,
abs/1603.05027, 2016.

[21] K. He, X. Zhang, S. Ren and J. Sun. Deep Residual

Learning for Image Recognition. 2015.

[22] Sergey Zagoruyko and Nikos Komodakis. Wide Residual

Networks. ArXiv, abs/1605.07146, 2016.

[23] Nitish Srivastava, Geoffrey E. Hinton, A. Krizhevsky,
Ilya Sutskever, and R. Salakhutdinov. Dropout: A Simple
Way to Prevent Neural Networks from Overﬁtting. J.
Mach. Learn. Res., 15:1929–1958, 2014.

[24] Fisher Yu and Vladlen Koltun. Multi-Scale Context
In International
Aggregation by Dilated Convolutions.
Conference on Learning Representations (ICLR), May
2016.

[25] Aaron Van Oord, Sander Dieleman, Heiga Zen, Karen
Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbren-
ner, Andrew Senior, and Koray Kavukcuoglu. WaveNet:
A Generative Model for Raw Audio, 2016.

[26] Yair Movshovitz-Attias, Alexander Toshev, Thomas K.
Leung, Sergey Ioffe, and Saurabh Singh. No Fuss
Distance Metric Learning using Proxies, 2017.

[27] Florian Schroff, Dmitry Kalenichenko, and James
Philbin. FaceNet: A Uniﬁed Embedding for Face Recog-
nition and Clustering. 2015 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), Jun 2015.
[28] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha
Kwak. Proxy Anchor Loss for Deep Metric Learning,
2020.

[29] Charu C. Aggarwal, Alexander Hinneburg, and Daniel A.
Keim. On the Surprising Behavior of Distance Metrics
in High Dimensional Space. In Database Theory, pages
420–434. Springer Berlin Heidelberg, 2001.
Julian Cayton and Dennis Mapa.

Time-
Varying Conditional Johnson SU Density in Value-at-

[30] Peter

13

Risk Methodology. Philippine Review of Economics,
51(1):23–44, 2015.

[31] Ilya Loshchilov and Frank Hutter. Fixing Weight Decay
Regularization in Adam. CoRR, abs/1711.05101, 2017.
[32] I. Loshchilov and F. Hutter. SGDR: Stochastic Gradient
Descent with Restarts. CoRR, abs/1608.03983, 2016.
[33] Diederik P Kingma and Max Welling. Auto-Encoding

Variational Bayes, 2013.

[34] Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. Stochastic Backpropagation and Approximate
Inference in Deep Generative Models, 2014.

[35] Carl Doersch. Tutorial on Variational Autoencoders,

2016.

[36] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe.
Inference: A Review for Statisticians.
Statistical Association,

Variational
Journal
the American
112(518):859–877, Feb 2017.

of

[37] Taoli Cheng, Jean-Franc¸ois Arguin, Julien Leissner-
Martin, Jacinthe Pilette, and Tobias Golling. Variational
Autoencoders for Anomalous Jet Tagging, 2021.
[38] Irina Higgins, Loic Matthey, Arka Pal, Christopher
Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mo-
hamed, and Alexander Lerchner. β-VAE: Learning Basic
Concepts with a Constrained Variational Framework.
November 2016.

[39] Christopher P. Burgess, Irina Higgins, Arka Pal, Loic
Matthey, Nick Watters, Guillaume Desjardins, and
Alexander Lerchner. Understanding Disentangling in β-
VAE, 2018.

[40] Aaron Van Oord, Nal Kalchbrenner,

and Koray
Kavukcuoglu. Pixel Recurrent Neural Networks.
In
Maria Florina Balcan and Kilian Q. Weinberger, editors,
Proceedings of The 33rd International Conference on
Machine Learning, volume 48 of Proceedings of Ma-
chine Learning Research, pages 1747–1756, New York,
New York, USA, 20–22 Jun 2016. PMLR.

[41] Christopher M. Bishop. Novelty Detection and Neural

Network Validation, 1994.

[42] Eric T. Nalisnick, Akihiro Matsukawa, Y. Teh, Dilan
G¨or¨ur, and Balaji Lakshminarayanan. Do Deep Gen-
erative Models Know What They Don’t Know? ArXiv,
abs/1810.09136, 2019.

[43] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An
Empirical Evaluation of Generic Convolutional and
Recurrent Networks for Sequence Modeling. ArXiv,
abs/1803.01271, 2018.

[44] Christopher M. Bishop. Mixture Density Networks.

Technical report, 1994.

