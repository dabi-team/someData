2
2
0
2

p
e
S
2

]

R
C
.
s
c
[

1
v
5
8
2
1
0
.
9
0
2
2
:
v
i
X
r
a

Security Best Practices: A Critical Analysis Using IoT as a Case Study

DAVID BARRERA, Carleton University, Canada
CHRISTOPHER BELLMAN‚àó, Carleton University, Canada
PAUL C. VAN OORSCHOT, Carleton University, Canada

Academic research has highlighted the failure of many Internet of Things (IoT) product manufacturers to follow accepted practices,

while IoT security best practices have recently attracted considerable attention worldwide from industry and governments. Given

current examples of security advice, confusion is evident from guidelines that conflate desired outcomes with security practices

to achieve those outcomes. We explore a surprising lack of clarity, and void in the literature, on what (generically) best practice

means, independent of identifying specific individual practices or highlighting failure to follow best practices. We consider categories

of security advice, and analyze how they apply over the lifecycle of IoT devices. For concreteness in discussion, we use iterative

inductive coding to code and systematically analyze a set of 1013 IoT security best practices, recommendations, and guidelines collated

from industrial, government, and academic sources. Among our findings, of all analyzed items, 68% fail to meet our definition of an

(actionable) practice, and 73% of all actionable advice relates to the software development lifecycle phase, highlighting the critical

position of manufacturers and developers. We hope that our work provides a basis for the community to better understand best

practices, identify and reach consensus on specific practices, and find ways to motivate relevant stakeholders to follow them.

Additional Key Words and Phrases: IoT security, best practices, security advice, inductive coding, device lifecycle

1 INTRODUCTION

Internet of Things (IoT) is commonly described as adding network connectivity to traditionally non-networked items

or ‚Äúthings‚Äù [77]. It surrounds us with a variety of network-connected devices such as smart light bulbs, door locks, web

cameras, audio speakers, thermostats, and less obvious objects like fridges, traffic lights, or sensors and controllers built

into critical infrastructure systems. The importance of IoT in marketing and sales has resulted in a wide variety of devices

with arguably unnecessary functionality (e.g., internet-connected toasters and toys). These devices, while offering

convenience or new functionality, have acquired a reputation [6] of poor security and misconfiguration, leading to huge

numbers of network-accessible devices being vulnerable. As IoT devices may be more isolated or resource-constrained

(e.g., battery power, processors, memory) than their Internet of Computers (IoC‚Äîi.e., pre-IoT devices such as mobile

phones, laptop/desktop computers, servers) counterparts, or lacking in software update support, their security issues

are often hard to address. The cyberphysical nature of IoT‚Äîinterfacing with physical world objects‚Äîalso results in

threats to our physical world, as well as to networks and other internet hosts [46]. This has resulted in considerable

attention (e.g., [6, 22, 25, 52]) to best practices for IoT security.

The term best practice is commonly assumed to be intuitively understood, yet academic work in this area (as noted

below) lacks consensus on informal definitions for the term, and closer inspection suggests a clear explicit definition is

needed. We argue that this assumption results in ambiguity, and contributes to security problems, and that intuitive

understandings are at best foggy and differ considerably across even experts. For example, in considering Cloud Security

Providers (CSPs), Huang et al. [40] refer to: ‚Äúsecurity mechanisms that have been implemented across a large portion of

the CSP industry [are thus] considered standardized into a ‚Äòbest-practice‚Äô.‚Äù Here, best practice appears to mean widely

‚àóAuthors listed in alphabetical order. Contact author: Christopher Bellman (chris@ccsl.carleton.ca). A version of this paper is to appear in ACM TOPS. A
preliminary version of part of this work appeared previously as a tech report [12].

Authors‚Äô addresses: David Barrera, Carleton University, Ottawa, Ontario, Canada; Christopher Bellman, Carleton University, Ottawa, Ontario, Canada;
Paul C. van Oorschot, Carleton University, Ottawa, Ontario, Canada.

1

 
 
 
 
 
 
2

Barrera, Bellman, and van Oorschot

implemented. In their evaluation of home-based IoT devices, Alrawi et al. [6] note numerous violations of security

design principles, and assert ‚ÄúBest practices and guidelines for the IoT components are readily available‚Äù, but offer neither

citations for best practices among 108 references, nor their own definition. In a recent national news article [43] on

banks disclaiming liability for customer losses from e-transfer fraud, and one-sided online banking agreements, a

defensive bank representative is quoted: ‚ÄúWe regularly review our policies and procedures to ensure they align with best

practices.‚Äù This quote appears to be not about security, but rather legal best practices in the sense of our agreements

are no worse than our competitors‚Äô. Large collections of documents from industrial, government, and academic sources

also conflate best practice with common terms such as recommendation and guideline [21]. How do best practices,

good practices, and standard practices differ? Or guidelines, recommendations, and requirements? If something is not

actionable, does it make sense to recommend it as a best practice?

In this paper, we provide what we believe is the first in-depth technical examination of intended meanings of the

term security best practice, and the surrounding related terms noted above. We argue that confusion and ambiguity

result from the lack of a common understanding and precise definition of these terms, and that this confusion permeates

official best practice recommendations. We support this argument by first investigating current use of terms related to

best practices, and explain how meanings of each term differ qualitatively (Section 3). We classify these descriptive

terms into three categories and separately define (actionable) security practices distinct from desired security outcomes

and security principles. Our examination of terminology highlights ambiguity and conflation of established terms,

contributing to the challenges we uncover in current IoT security advice and technical literature.

As further contributions, we offer uniform, consistent terminology, and then consider the UK government‚Äôs Code

of Practice for Consumer IoT Security [73]. A preliminary informal analysis finds that the guidelines comprising it

are not actionable practices by our definition (Section 3.2), despite being positioned as ‚Äúpractical steps‚Äù to be taken

by IoT security stakeholders, featuring 13 ‚Äúoutcome-focused guidelines‚Äù derived from industry advice. We develop

a new security advice coding methodology (SAcoding method) in Section 4, for systematically categorizing security

advice based in part on the terminology refined herein. Applying it to a dataset of 1013 IoT security advice items from

industrial, government, and academic sources compiled by the UK government [74], we find only 32% of the 1013 items

are actionable (Section 5), highlighting a gap between the expectations of entities providing advice and those intended

to implement it. Our goal is not to criticise the UK group‚Äîtheir advice dataset simply aggregates other sources‚Äîbut to

demonstrate what we view as the ineffective state of IoT security advice as a whole, and to take first steps to repair this.

We believe that our contributions may be of use to the broader security community, beyond IoT itself.

2 BACKGROUND AND OVERVIEW OF ESTABLISHED IOT SECURITY ADVICE

In this section we discuss key areas of IoT and their role in the adoption of security best practices. These areas include:

the IoT device lifecycle (and when in a device‚Äôs lifecycle security advice is applicable), which stakeholders have the

most significant impact on the security of a device, and existing security advice, which we later analyze.

2.1 Lifecycle of IoT Devices

The lifecycle of a consumer IoT device includes phases it goes through from early design to the time it is discarded

(possibly re-used, or never used again) [33]. We model the full lifecycle of a device, as decisions made within one part

of the lifecycle (particularly the pre-deployment stages) may affect later phases.

Once IoT products have left the hands of manufacturers, it becomes more challenging to address vulnerabilities. We

believe it is important to understand the stages within each phase, as security advice to be followed relates directly to

Security Best Practices: A Critical Analysis Using IoT as a Case Study

3

the processes carried out within specific stages. Fig. 1 presents our model of a typical lifecycle of an IoT device based on

existing work [33], modified to incorporate what we believe are the most relevant phases (and stages within them). Our

model highlights our interpretation of the four major phases where IoT security advice is generally applicable. Our

analysis (Section 5) includes a discussion on the impact of security advice followed at each lifecycle stage.

Fig. 1. Typical IoT device lifecycle (initial design to end-of-life). Broad phases (1‚Äì4) encapsulate multiple stages (e.g., 2.1, 2.2).

The Creation phase takes place under the authority of the manufacturer, where a device is designed, developed, and

pre-configured. The Creation phase happens pre-deployment, i.e., before the device is sold to an end-user. This excludes

when a user receives the device from another user.

In the Installation phase, the user has received the device and readies it for normal use. This is the first post-

deployment phase, and contains onboarding or bootstrapping (often used interchangeably or meaning slightly different

things, depending on who uses it) [66] which includes technical details such as key management, registration and

identification of devices, establishing trust relationships, and other device configuration.

The Usage phase involves using the device as intended (e.g., a light bulb provides light, a smart thermostat controls

temperature, a home security camera provides live camera access). While containing only two stages, the device is

expected to spend most of its life in this phase. Software/firmware updates take place in this phase.

The Decommissioning phase is where a device ends its life with respect to a single user or organization. The device

is readied for removal from its environment (data/key removal from the device), physically removed, and leaves the

end-user‚Äôs ownership (either via disposal or transfer of ownership to another user). If device ownership is transferred

to another user, the device returns to the Installation phase where the post-deployment ownership phases begin again.

1.1 Design1.2a Hardware Manufacture1.3 Integration & Pre-Configuration2.1 Installation2.2 Configuration3.1a Normal Use3.1b Software / Firmware Update4.1 Data/Key Removal4.2b. Transfer Ownership4.2a Disposal1. Creation2. Installation3. Usage4. Decommissioning1.2b OS/App DevelopmentORANDpost-deploymentpre-deployment4

Barrera, Bellman, and van Oorschot

Table 1. Assignment of DCMS guidelines to IoT device lifecycle phases. For full detailed guideline descriptions, see the DCMS 13
guidelines document [73].

Guideline

Guideline Title

UK-1
UK-2
UK-3
UK-4
UK-5
UK-6
UK-7
UK-8
UK-9
UK-10
UK-11
UK-12
UK-13

No default passwords
Implement a vulnerability disclosure policy
Keep software updated
Securely store credentials and security-sensitive data
Communicate securely
Minimise exposed attack surfaces
Ensure software integrity
Ensure that personal data is protected
Make systems resilient to outages
Monitor system telemetry data
Make it easy for consumers to delete personal data
Make installation and maintenance of devices easy
Validate input data

Example
Sources

[19, 22, 28]
[15, 28, 49]
[4, 15, 16]
[19, 20, 28]
[15, 20, 28]
[5, 9, 18]
[28, 38, 70]
[3, 5, 17]
[15, 16, 28]
[18, 19, 28]
[27, 58, 68]
[49, 68, 70]
[28, 59, 68]

Lifecycle
Phase (Fig. 1)

1.3
1.1 3.1a
1.1 1.2b 3.1b
1.2a 1.2b 1.3
1.2b 1.3
1.2a 1.2b 1.3
1.2b
1.2a 1.2b 1.3
1.1 1.2b
1.1 1.2b 3.1a
1.1 1.2b
1.1 1.2a 1.2b
1.2b

2.2 Established IoT Security Advice and the DCMS 1013 dataset

The IoT security advice analyzed in Section 5, which we call the ‚ÄúDCMS 1013-item dataset‚Äù [11], is derived from an

effort led by the UK government‚Äôs Department for Digital, Culture, Media & Sport (DCMS). Their base dataset [21]

(Version 3 in particular ) consists of individual IoT security advice items compiled from earlier academic, industry,

and government documents for manufacturers of IoT products [74]. The dataset is positioned as security guidance

rather than extremely detailed specification-level advice [74]; we say more about this in Section 3. From a starting set

of 1052 security advice items, we manually removed duplicates‚Äîif two items were word-for-word identical, we kept

only one. This left 1013 items [11]. The items originated from 69 documents (most were informal), from 49 different

organizations. Groups represented in the collection include the IoT Security Foundation [41], the European Union

Agency for Network and Information Security (ENISA) [28], and the GSM Association (GSMA) [38]. While the most

heavily referenced sources are industrial organizations whose main focus is security, others include the US Senate [76],

the US National Telecommunications and Information Administration (NTIA) [75], and Microsoft [52]. As this suggests,

a vast range of organizations have offered advice on IoT security. We chose this dataset for our analysis as it represents,

to our knowledge, the most comprehensive publicly available collection of IoT security advice.

This 1013-item dataset is related to two other documents from DCMS, and one from the European Telecommunications

Standards Institute (ETSI). For further context, we describe these here also.

DCMS 13 guidelines document. The DCMS Code of Practice for Consumer IoT Security [73] proposes 13 guidelines

(these are also used in Australia [10], with minor changes). Each includes a summary title and a more detailed guideline.

The guideline descriptions typically do not express how a guideline is to be executed (i.e., what specific steps to take),

but rather an outcome or goal to reach. We note that this is contradicted by the guidelines also being positioned as

‚Äúpractical steps‚Äù [73], which leads us to expect something practical to do, i.e., an action to take rather than an outcome.

We discuss actions and outcomes in Section 3.2.

The DCMS 13 guidelines are intended to provide practical advice on securing IoT devices, and target four stakeholders:

device manufacturers, IoT service providers, mobile application developers, and retailers [73]. We note end-users are

not mentioned as a targeted stakeholder. Table 1 lists the guideline titles along with our assignment of the IoT lifecycle

stages in which they are applicable, and examples of (unrefereed) primary source documents suggesting such advice.

Security Best Practices: A Critical Analysis Using IoT as a Case Study

5

DCMS mapping document. The DCMS Mapping of IoT Security Recommendations, Guidance and Standards to the

UK‚Äôs Code of Practice for Consumer IoT Security document [74] maps each advice item in the 1013-item dataset (1052 items

before our pre-processing, as noted above) to one of the 13 guidelines from the DCMS 13 guidelines document. While

we do not analyze this document itself, we mention it for context, being referenced in both the DCMS 13 guidelines

document and the ETSI provisions document (next).

ETSI provisions document. ETSI has also published a document of ‚Äúbaseline requirements‚Äù for IoT security [25]

that appears to be an evolution of the DCMS 13 guidelines document. It includes all 13 categories as major headers

(most with slightly modified wording), and elaborates on each with a set of requirements that fit the theme of that

category, much as is done in the DCMS mapping document. For example, Provision 5.1-2 from the section titled 5.1 No

universal default passwords (comparable to the DCMS‚Äô No default passwords guideline header) states [25]:

Where pre-installed unique per device passwords are used, these shall be generated with a mechanism that

reduces the risk of automated attacks against a class or type of device.

EXAMPLE: Pre-installed passwords are sufficiently randomized.

As a counter-example, passwords with incremental counters (such as ‚Äúpassword1‚Äù, ‚Äúpassword2‚Äù and so on)

are easily guessable. Further, using a password that is related in an obvious way to public information (sent

over the air or within a network), such as MAC address or Wi-Fi SSID, can allow for password retrieval using

automated means.

The majority of the advice items‚Äô topics therein are represented in the DCMS 1013-item dataset.

While early portions of this paper deal primarily with terminology, these documents are introduced as examples of

established IoT security advice, and in the case of the 1013-item dataset, we also analyze it in later sections.

3 DEFINING WHAT ‚ÄòBEST PRACTICE‚Äô MEANS

In this section we consider definitions for best practice, including our own definition taking into account the concepts of

outcomes, actions, and actionable practices, and discuss related terms commonly appearing in the literature. Through

this, we provide a refined, self-consistent vocabulary for security best practices and also disambiguate a wide variety of

qualifying terms into three semantic categories.

3.1 Definition and Discussion

The definition of best practice is largely taken for granted. Few documents that use it make any effort to explicitly define

it. Of note, even RFC 1818/BCP 1 [61], the first of the IETF RFCs specifying what a Best Current Practice document is,

fails to define best practice. Thus, the term (and concept of) best practice is, at least in security, almost always used

casually, versus scientifically‚Äîthe implicit assumption being that everyone understands what it means well enough to

not require an explicit definition.

A negative consequence of this is that different experts also implicitly redefine best practice to suit their own needs

or context (examples are given in Section 1 and the examples below supplement this). This leads to ambiguity, where

certain uses of best practice have different meanings and connotations, while elsewhere different phrases may imply

the same concept. To address this, we first propose a definition for practice (separate from best practice): A practice is a

specific means intended to achieve a given desired outcome. We build on this definition below.

A practice specifies actions (as explained in Section 3.2) to reach an outcome, but does not necessarily imply any level

of quality or security with respect to the means or mechanism used or the outcome (as discussed further in Section 3.5).

6

Barrera, Bellman, and van Oorschot

Building on this definition, to reduce ambiguity and provide more precision to analyze security best practices, we

propose the following practical definition for best practice: For a given desired outcome, a best practice is a specific

means intended to achieve that outcome, and that is considered to be better than, or at least as ‚Äúgood‚Äù, as the best of other
widely-considered means to achieve that same outcome.1

Note that by our definition, a best practice is something that can be done (an action), not something that is desired to

be achieved (an outcome). A community in which a best practice is developed may have their own measure for quality,

and quality requirements may vary based on the community, environment, and context. Further, as by our definition,

best practices are specific to the outcome they aim to achieve, there is generally no ‚Äúsilver-bullet‚Äù best practice for

use across all applications‚Äîpractices typically must be tailored for the context [67, Chapter 2] (e.g., a surgeon has

different hand-washing best practices than an individual preparing a family meal). Best practices may be intended for

manufacturers, but for the benefit of end-users (other stakeholders are often involved). Stakeholders that benefit may

or may not be involved in a best practice‚Äôs implementation. For example, a user of a product might not care how a

manufacturer implements a best practice, despite relying on it for security.

We now review four notable definitions of best practice and compare them to our definition above.

King [45] provides extension discussion of security best practices, using the term best security practices and giving

particular attention to human aspects. In his view [45]:

[Best practices are] practices that have proven effective when used by one or more organizations and which,

therefore, promise to be effective if adapted by other organizations.

King notes several central concepts, including that effectiveness is based on evidence of multiple instances (implying

some degree of consensus and generality), that a practice must be applicable to real (not only theoretical) situations,

and that it may exist among a set of others of equal quality for a given purpose [45].

McGraw [51] discusses best practices in the context of development activities aiming to improve software security.

He offers that best practices are:

[...] usually described as those practices expounded by experts and adopted by practitioners.

In his view, best practices are created by a group of experts and often intended for use by non-experts. His book also

regularly refers to touchpoints, described as ‚Äúa set of software security best practices‚Äù [51], implying that touchpoints

are best practices. However, discussion throughout the book indicates that these touchpoints are general categories of

recommended processes and activities (e.g., code review or penetration testing [51]) rather than specific procedures

that our definitions would recognize as actionable practices.

Garfinkel et al. [35] describe best practices (in the context of operating system and internet security) as:

[...] a series of recommendations, procedures, and policies that are generally accepted within the community

of security practitioners to give organizations a reasonable level of overall security and risk mitigation at a

reasonable cost.

As a defining feature of a practice this emphasizes general agreement, within a community (however difficult that

may be), on its quality. Note the hint that a best practice may involve a trade-off between cost and quality, some

middleground acceptable for an organization. Related to this is feasibility, discussed in Section 3.2.

If we consider the definitions by McGraw [51] and Garfinkel et al. [35] on a continuum of how specific they envision

best practices to be (the scope of practices themselves), we find McGraw‚Äôs description at one end (the coarse or general

1While one view of ‚Äúbest‚Äù might imply being above all known others, another is that ‚Äúbest‚Äù is a category that may have more than one member. It is thus
reasonable to allow (by definition) that there are multiple best practices for a given desired outcome (consistent with King, above).

Security Best Practices: A Critical Analysis Using IoT as a Case Study

7

end), with Garfinkel et al. at the other end (specific). McGraw‚Äôs best practices are comprised of general categories of

activities to be executed at different stages of the software development lifecycle. Garfinkel discusses fine-grained

practices, e.g., at the level of configuration commands, command-line arguments and precise details specific to a given

OS and version (while not immediately clear from the quoted definition, this follows from examples throughout the

cited book [35].)

Shostack and Stewart [67, pp.36‚Äì38], in their book on security, describe best practices as:

[...] activities that are supposed to represent collective wisdom within a field [and] designed to be vague

enough to apply in the general case.

Our definition agrees regarding ‚Äúcollective wisdom within a field‚Äù, but we call for best practices to be more specific than

vague. Shostack and Stewart‚Äôs perspective fits somewhere between the earlier two on the above spectrum, being more

specific than McGraw [51], and less specific than Garfinkel et al. [35],

While we avoid declaring these other characterizations of best practice ‚Äúwrong‚Äù,2 herein we use our own definition
to serve as a concrete, explicit reference, also allowing focus on concepts that we aim to highlight. We acknowledge

that others may opt for a definition that does not in all cases require, as we do, a specific set of steps to reach a desired

outcome. We require that beyond a goal (outcome) alone, a best practice includes a means to achieve a goal (preferring

a specific set of steps). Our definition is more specific than informal views of best practices as lists of general ‚Äúgood

things to do‚Äù.

One can also consider the implications of best practices from legal, technical, and social angles. From a legal

perspective, following a best practice may be used as an argument to escape or limit liability, as in ‚Äúfollowing the crowd‚Äù

or consensus as surely being reasonable. For example, financial institutions citing ‚Äúindustry best practices‚Äù to disclaim

liability, per our example in Section 1 [43]. Technically, a best practice is often the best way known to technical experts

or researchers for achieving an outcome (supported by some form of consensus), or as a way to limit risk [35]. Less

formally, best practice often implies the most common (if not necessarily best) way to do something. At one level, one

might argue that each of these are similar, but at a semantic level, they are different uses of the same term.

3.2 Outcomes vs. Actions

We define an outcome as the statement of the desired end goal that a stakeholder aims to reach, and an action as an

operation of one or more steps carried out by a person or computer, perhaps in order to achieve a desired outcome. For

example, an outcome may be having created a strong password, and an action to (partially) achieve this outcome may

be to enforce a minimum password length of 8 characters [37]. In practice, outcomes or goals that are vague or broad

may not give stakeholders a clear idea of any concrete set of actions that can be taken to achieve the goal. A desired

outcome of ‚Äústrong security‚Äù, for example, is nebulous and cannot be mapped to specific actions to achieve the goal.

Defining tightly-scoped outcomes or specifying an objective to withstand specific attacks allows for successful mapping

to corresponding actions.

A given practice may be viewed as actionable if it can be carried out without guesswork by an advice target. We

argue that being actionable is the crucial characteristic, the key point being to formulate advice such that the steps to be

executed are explicit or well understood by targeted advice recipients. This leads to our next definition. By actionable

practice we mean a practice that involves a known, unambiguous sequence of steps, whose means of execution are

understood (by the target advice recipients).

2Absent a consensus definition for a term, how good or bad a particular definition is usually depends on how well it meets the purpose at hand.

8

Barrera, Bellman, and van Oorschot

An explicit declaration (and characterization) of the target audience is also important, in our view. This impacts

whether advice is actionable, as advice must often be tailored to an audience and their knowledge level. A sequence

of steps described as generally understood (in the above definition of actionable) implies that the target audience

has the appropriate level of knowledge to execute the advice. Advice not understandable by the target audience or

not sufficiently specific becomes non-actionable to that audience. Wording and outcomes must be understood from

both a (semantic) language and a technical perspective. For example, advice targeted at security experts (e.g., the

AES specification [56]) may be difficult to follow by non-expert audiences. Without a declaration of target audience,

inappropriate audiences may attempt to execute advice and misinterpret or fail to successfully execute it, resulting in

flawed implementations.

Advice that simply mentions a general technique by name (without details) is non-actionable, by our definition. How-

ever, pointers to ‚Äúnext-level‚Äù implementation details may meet our requirement of unambiguous steps (for actionability),

e.g., with details in an external reference. In this way, advice may direct advice recipients to non-prescriptive techniques

or approaches (e.g., key management techniques as in the UK-5 example below), but then link to further sources for

specific details. This allows specifying how to carry out an advice item generally (how to approach a problem), while

avoiding fine detail and lengthy descriptions, yet remaining actionable via links to detailed unambiguous steps. This

avoids advice items that dictate an exhaustive number of individual steps, and avoids needing advice updates due to,

e.g., parameter changes or algorithm upgrades; low-level details can be more frequently updated in external sources,

without need to re-issue higher-level best practice advice (as it remains valid for longer periods).

While we use actionable practice (above) to emphasize that a practice must be one that a target subject can actually

carry out, describing a practice as actionable is redundant, as all practices are necessarily actionable by our earlier

definition of a practice as a specific means to achieve a desired outcome. An outcome (alone) cannot be a best practice

(or even a practice) as an outcome alone does not dictate a specific means to an end. In what follows, when we use the

term practice, we generally mean a practice that is actionable.

It follows that a recommendation specifying an outcome, but the path to which is an open research problem,

cannot (and in our view should not) be considered a practice. Specifying ‚Äúadvice‚Äù that implies use of techniques that

are experimental or unproven introduces ambiguity in how to carry out the advice and may result in inconsistent

execution of the advice (which is not, by our definition, a practice). We argue that it is important for the security

community‚Äîwhether by academic, industrial, or government efforts‚Äîto identify and agree on practices with concrete

desired outcomes for use by those targeted by the advice. Best practices adopted for specific use-cases will ideally lead

to more reliable (correct) execution of the practices and thereby improve security. Our heavy focus on (actionable)

practices arises from our belief that, if stated clearly, they may be the most promising, direct way to help pre-deployment

stakeholders improve security.

Manually applying the above definition of actionable to the full descriptions [73] of each of the 13 DCMS guidelines

whose titles are given in Table 1, we found that only one of the guidelines fit our definition of actionable, that being

UK-1: ‚ÄúNo default passwords‚Äù. This leads us to question how many of these guidelines can be reliably implemented from

the guideline descriptions alone. For example, UK-5 (‚ÄúCommunicate securely‚Äù) states [73]:

Security-sensitive data, including any remote management and control, should be encrypted in transit,

appropriate to the properties of the technology and usage. All keys should be managed securely. The use of

open, peer-reviewed internet standards is strongly encouraged.

Security Best Practices: A Critical Analysis Using IoT as a Case Study

9

We find this guideline non-actionable, as it is vague and non-specific about which actions to take to follow it, and is

unfocused on a single security topic (discussed in Section 5.3). Guidelines may have implementation details inferred

based on the experience of the implementer, but this does not appear to be the way these 13 guidelines are positioned. The

associated DCMS mapping document [74] is intended to provide additional details and context for how the guidelines

should be followed, but as we later find (Section 5), the advice found in the mapping document is largely non-actionable.

A practice does not necessarily need to specify a full sequence of low-level, specific, detailed steps; it may be

acceptable to state high-level steps, provided they are still actionable. For example, a practice involving the use of AES

does not necessarily require a line-by-line implementation as specified by NIST [56], but it may be enough for the

practice to state a library or function to use, how it should be used, and specify any desired configuration details. To

use a non-security example, a car mechanic does not need to build a car‚Äôs alternator from scratch, but they are expected

to be able to follow a guide to install and configure a pre-assembled one. This further highlights the importance of

an appropriate target audience selection‚Äîdepending on the context, specification-level details are important for, e.g.,

those building libraries and toolkits (to use the AES example), while others may require only the details needed to

properly use the libraries provided to them. Both situations can have practices developed for them, while reaching

desired outcomes and being appropriate for their respective audiences.

While our definition of an actionable practice is designed to match what we expect is practically followable, clearly

indicating what advice recipients must do, in some cases recipients can infer how to execute advice even if it lacks

details. For example, depending on a target‚Äôs experience, what a ‚Äústandard algorithm‚Äù is may be understood. While we

retain our definition of a practice being actionable, we acknowledge that in some cases, some advice recipients have

sufficient experience to infer actionable detail from otherwise non-actionable advice‚Äîmaking explicit step-by-step

instructions unnecessary. Nonetheless, because security experts are not always the audience responsible for executing

security advice (e.g., at an IoT device manufacturer), we encourage development of actionable practices for specific

targeted audiences.

Infeasible advice. We separate the concepts of a practice being actionable, and an implementer having the means

by which to put said practice into place. Implementers must have the resources (technical, financial, personnel) available

before a practice can be implemented, but availability of resources does not affect the generic actionability of a practice

by our definition. (In other words: though a practice is actionable in general, that does not guarantee that a given

party themselves has the resources to adopt the practice.) A practice that has a significant cost may be ruled out as a

best practice by a recommending group, governing body, or peer community. Similarly, while still actionable by our

definition, a practice that has (for example) 300 well-defined, unambiguous steps and takes 14 years to complete would

likely not be considered as a best practice. Such a practice would be considered infeasible (i.e., a practice that remains

actionable, but viewed as impractically inefficient by a party lacking the resources to carry it out). Note this is illustrated

by the continuum of the actionability of practices (Fig. 2, discussed later in Section 4.1). An Infeasible Practice (P3) is

actionable, but by fewer parties (suggested by its placement toward the actionable by fewer parties labelled end of the

Fig. 2 continuum) than a practice requiring a Security Expert (P4)‚Äîas practically speaking, high costs reduce the number

of parties able to implement a practice.

3.3 Imperative and Declarative Advice vs. Actions and Outcomes

By our definition (Section 3.1), the statement of a best practice includes specifying a means to reach a desired outcome.

We briefly consider now the utility of advice items that do not specify any such means or specific set of actions. As a

particular case, consider an advice item that specifies an outcome whose attainment can be verified (but leaving it to an

10

Category
Focus

Quality

Commonality

Stipulation

Barrera, Bellman, and van Oorschot

Table 2. Categories of commonly used qualifying terms related to best practices.

Qualifying Terms (examples)

Suggested Use

√úber

Best

Good

‚Äústate-of-the-art‚Äù
‚Äúgold standard‚Äù

‚Äúbest current practice‚Äù
‚Äúbest practice‚Äù

‚Äúrecommended practice‚Äù
‚Äúsuggested practice‚Äù
‚Äúgood practice‚Äù

‚Äúminimum expectation‚Äù
‚Äúbaseline practice‚Äù
‚Äúaccepted practice‚Äù
‚Äúcommon practice‚Äù
‚Äústandard practice‚Äù

‚Äúregulation‚Äù
‚Äúmandatory practice/requirement‚Äù
‚Äúformal standard‚Äù
‚Äúcode of practice‚Äù
‚Äúrecommendation‚Äù
‚Äúguideline‚Äù

For practices considered superior to all others, even if not widely adopted. These terms
imply elite quality, possibly at high cost or complexity.
For practices widely-considered to be high quality (plus widely adopted, ideally).

For practices that are beneficial (e.g., to improve security), without implying that better
practices do not exist. Here, ‚Äúrecommended‚Äù and ‚Äúsuggested‚Äù do not imply a formal
endorsement.

For practices not necessarily implying quality, but reflecting wide use. Alternatively,
these may be de facto practices or functionality, informally recognized by experts as
generally expected.

For practices endorsed (formally) or mandated in some capacity by an organization or
individual. Includes practices that may be, in some way, enforced by an entity such that
there implies a negative consequence if the advice is not followed.

advice recipient to determine a specific means). Some advice recipients may still be able to attain the outcome, and

auditors could verify attainment. Would such advice‚Äîwhich we call declarative advice, next paragraph‚Äîbe equivalent

to a best practice? Not by our best practice definition, which requires a specific means; by our definition, an outcome

and a best practice are categorically different. Nonetheless, if the means used to reach the outcome is of secondary

importance to an advice giver or authority, and their primary interest is attaining the outcome, then advice items in the

form of (verifiable) declarative outcomes may be useful alternatives to (actionable) best practices‚Äîfor advice recipients
who can independently determine a means to reach the outcome. Having made this observation,3 we proceed herein to
use our (Section 3.1) definition of best practice.

As supporting context, we note that actions and outcomes can respectively be mapped to imperative advice (advice

that includes specific steps or actions to reach an outcome), and declarative advice (advice that specifies an end result or

outcome to reach, but not any specific method by which to reach it) [14, 32]. Depending on their nature, some outcomes

may be verifiable, e.g., through a test that yields a yes/no answer to whether the goal was reached, or a measure used

against a pass-fail threshold. For example, consider the advice item [68]: Where a device or devices are capable of having

their ownership transferred to a different owner, all the previous owner‚Äôs Personal Information shall be removed from the

device(s) and registered services. This could be verified, for example, by checking that any memory region designated for

storing user personal information has zeros in every byte.

In contrast, an example of a non-verifiable advice item is use a randomly generated salt with a minimum length of

32 bits for hashing with passwords [37]. If we assume a verifier is only presented with the fixed-length output from a
hashing function (i.e., ‚Ñé from ‚Ñé = ùêª (ùëù, ùë†), where ùëù is a password and ùë† is a salt value), this practice is not verifiable, as the
output provides no indication of the salt‚Äôs length or method of generation. For example, a password of ‚Äúùëùùëéùë†ùë†ùë§ùëúùëüùëë123‚Äù
and a salt of ‚Äúùê¥ùêµùê∂ùê∑‚Äù produces a SHA3-256 hash of ‚Äúùëí0ùëéùëé...ùëèùëíùëí ùëì ‚Äù (truncated), which does not reveal any characteristics
of the salt. While we use this as an example, in practice, salts are typically stored with the hash output‚Äîif a verifier

were to recover one, they would then have access to the other.

3We thank an anonymous referee for raising this question.

Security Best Practices: A Critical Analysis Using IoT as a Case Study

11

3.4 Commonly-Used Qualifying Terms

A number of what we call qualifying terms are widely used as an adjective before the word practice (e.g., common, good,

best) but without definition of the qualifying term itself. Being widely used might suggest that readers know (and are in

universal agreement on) what authors mean when they use these terms. Like best practice, while security community

members are apparently expected to have a general intuitive understanding of the meanings of these terms, consensus

has not been reached on the meanings of these terms either.

For example, IETF BCP draft Best Current Practices for Securing Internet of Things Devices [53] contains advice that is

arguably positioned in three different ways: (1) as advice within a best current practices document (containing advice

considered to be the best current practices); (2) as recommendations (suggesting that use of the advice items is endorsed);

and (3) as minimum requirements (their use is a minimum expectation).

In an effort to both highlight existing terminology and move toward more consistent use of terminology, we associate

these highlighted terms (among others) with one of three distinct categories of qualifying terms, summarized in Table 2:

quality, commonality, and stipulation. These three categories, discussed in separate subsections below, can be used to

characterize a given advice item. Table 2 also suggests where/when each qualifying term should be used and gives

examples. Greater consistency in use of terminology may reduce ambiguity, misunderstandings, and consequent errors,

within the academic and industrial security communities. From an advice recipient‚Äôs perspective, it may also clarify the

expected outcomes of following advice (e.g., what outcomes will be reached), and expectations surrounding its use (e.g.,

how the advice should be carried out).

While we primarily categorize terms by what we view as each term‚Äôs dominant goal (i.e., identifying the quality of

an advice item, how commonly an advice item is used, and acknowledging a governing authority‚Äôs stipulation of the

advice item), an advice item can share the characteristics of more than one category. For example, an advice item that is

considered to be a good practice (quality category) can also be a standard practice (commonality category) through wide

use, and a best practice (quality) can be included in a formal standard (stipulation).

Table 2 does not explicitly define the commonly-used qualifying terms contained therein; rather, it describes how we

suggest each term (belonging to a category) be most appropriately used. For example, here our Suggested Use for best

practice expresses its relationship to being widely considered of high quality (albeit a higher quality tier exists), while

our definition (Section 3.1) explicitly notes that best practices are better than (or at least as good as) other high quality

practices with wide consideration. In what follows, we discuss these terms in greater detail.

3.5 Category 1: Quality-based Terms

Quality-based terms provide a natural basis on which to differentiate practices. Conceptually, we order √ºber, best, and

good practices along a quality continuum. We note that terms used to describe practices of low quality (i.e., below good)

receive less attention in literature as documents promoting security advice focus more on good than bad practices. Our

definition of a good practice (the lowest quality we formally recognize) implies that anything lower does not improve

security.

√úber practices. The sub-category or group √úber suggests practices that are in some way superior to best practices,

or beyond what would be considered already high quality. State-of-the-art or gold standard implies something of

elite technical quality, but perhaps not yet widely adopted. Consider as a practical example: in luxury cars, a heated

steering wheel. While more comfortable on a cold winter day, best practice would likely be to ensure correct function

12

Barrera, Bellman, and van Oorschot

and adequate steering grip to reduce the likelihood of accidents. A heating function may be the ‚Äúgold standard‚Äù or

‚Äústate-of-the-art‚Äù (typically at higher cost).

Best practices. The group Best suggests practices widely considered to be high quality, and often, widely adopted.

While technically better practices may exist, best practices are widely accepted within a community to be high quality.

Good practices. The group Good suggests practices that improve security but are not necessarily the best practices

available. They generally are not lauded for high quality per se. A good practice often either does not have wide

acceptance as being the best, or is perhaps not widely practiced or not considered essential even if easy and beneficial

(e.g., a good practice is to apply the emergency brake when parking facing down a hill, while a best practice is to both

apply the emergency brake and turn the wheels to the curb). Further context may prove useful for understanding

their use. For example, access control to a low-value free online newspaper account may not require a best practice

authentication method (per our definition); a good practice may suffice [35]. In other words [45]: ‚Äúsometimes the good is

good enough‚Äù.

3.6 Category 2: Commonality-based Terms

Commonality-based terms also often include the word practice (e.g., accepted practice, common practice), but their

unifying trait is frequency of use rather than quality.

Baseline practice/minimum expectation. These terms suggest a minimum level to be reached. We assign these

to the commonality category, as it is expected that the minimum acceptable level of advice is commonly followed.

Common/standard/accepted practices. These terms reflect broad usage. For example, it may (unfortunately) be

common to store passwords in plaintext within a database (thus being a common practice), but that is not best practice

(or even a good practice).

We repeat that commonality does not necessarily imply quality. Terms in this category are less clearly ordered than

in the quality category, and some terms are used interchangeably (e.g., common/standard/accepted). As baseline and

minimum expectation both imply a lowest reasonable threshold to start from, these may be considered more of a priority

to be followed, thus we order them higher in the group than the common/standard/accepted practices.

Correlated with commonality is the maturity of advice, typically reflecting the length of time that advice has been,

or continues to be, given or known. To follow an earlier example, while not considered even a good practice, storing

passwords in plaintext has become a mature practice [60]. Ideally, a best practice would be mature as well as widely

considered to be high quality (Section 3.5), but greater maturity of an advice item does not always imply higher quality

(e.g., DES is a mature cipher, but no longer best practice).

Security design principles. Security design principles are a known set of guiding rules which aim to improve

security [65]. These principles are generally based on experience, suggesting their maturity. Security design principles

are also generally expected (by experts) to be followed, and are complementary to the existing categories, but we

intentionally omit them from Table 2, and discuss them further in Section 4.3.

3.7 Category 3: Stipulation-based Terms

Distinct from quality and commonality, some terms related to best practices have more to do with the endorsement by

an authority, the authority‚Äôs jurisdiction, and whether the advice is mandatory (i.e., a firm requirement). Note that the

entity creating advice is not necessarily the authority mandating its use. Our stipulation category contains qualifying

terms describing advice that is mandated or endorsed by an entity in some way. These too can be ordered along a

continuum. On the strict end are terms that imply a negative consequence for not following the advice (e.g., mandatory

Security Best Practices: A Critical Analysis Using IoT as a Case Study

13

practice, requirement, regulation). On the looser end are terms that are stipulated, but not necessarily enforced (e.g.,

guideline/guidance, recommendation). As with a best practice, stipulations should, in our view, ideally be accompanied

by an explanation of the intended outcome.

Regulation. We use regulation to mean a directive from an authority stating specific advice that must be followed

to be allowed to operate within a jurisdiction. Here, a jurisdiction refers to the legal or authoritative domain, or the

context of the deployment environment or use cases (e.g., home IoT may require different practices than IoT devices for

government; physical locations, e.g., to meet certain requirements to be allowed to be sold in a country; or scope of

technology, e.g., certain practices may be more appropriate for IoT devices rather than desktop computers).

Mandatory practices/requirements. Hereafter just ‚Äúrequirements‚Äù, these do not necessarily imply the quality of a

given practice, but rather that it is stipulated by some governing body or regulation, suggesting official endorsement.

These may be considered ‚Äúenough‚Äù for some purposes (e.g., enough to not be sued or enough to pass inspection). Practices

across a range of qualities may be requirements depending on the governing body or motivation, although a practice

established as high quality is more likely to become a requirement.

Formal standard. We take formal standard to mean a formally documented (endorsed by some authority) specifica-

tion. This typically (but not necessarily) implies acceptable quality; the main point is to officially specify details and

recognize, e.g., a particular method or measurement. The purpose of a standard may be interoperability‚Äîe.g., standards

for the gauge of rail tracks or pipes. In this context, formal standard differs from standard practice (i.e., common practice,

above) and is not related to frequency of use, e.g., it is standard (practice) to eat at 12 noon. Standards are typically

sufficiently detailed such that conformance or compliance can be judged by, e.g., an auditor, or interoperability tests

[35].

Code of practice. We take code of practice to mean a set of guidelines designed to help inform others (traditionally

within a profession) of expectations. They often pertain to ethical or safety issues. Codes of practice are often stipulated

(within an organization or industry), but may be viewed as voluntary in that, e.g., failure to follow them typically does

not result in major penalties unlike stricter terms (mandatory practices/requirements). In our use, a code of practice is

distinct from formal regulations such as an ‚Äúelectrical code‚Äù or ‚Äúbuilding code‚Äù.

Recommendations and guidelines. A recommendation is an endorsement of, e.g., a practice by an individual or

organization as their suggested way to do something. Recommendations (depending on the recommending entity) may be

subject to bias or be self-serving, and do not necessarily reflect expertise or universal consensus. Some recommendations,

depending on their sources, are, in essence, requirements. Recommendations commonly suggest following a standard

[74]. Similarly, a guideline or guidance is often given to promote a suggested way to achieve a goal (or as described by

Garfinkel et al. [35], something that should be done). A guideline may be used in the spirit of a recommendation‚Äîoffered

as help, versus imposing rules.

As final thoughts on best practices and related terminology, we noted inconsistency in the use of common terms

(Section 3) and the situations in which they are used, with the same terms used with different meanings in different

situations. We argue that supporting consistent use of terms, and separating the concepts of quality, commonality, and

stipulation provides a better foundation to discuss and analyze security advice within the community. Additionally, we

discussed what we argue is an important characteristic for security advice: whether it is actionable.

14

Barrera, Bellman, and van Oorschot

4 SECURITY ADVICE CODING TREE METHODOLOGY AND DEVELOPMENT

Our methodology for the systematic analysis of existing IoT security advice begins with iterative inductive coding. Our
specific goal is to categorize (code) 1013 IoT security advice items4 for further analysis and thereby to characterize the
current state of IoT security advice. Such categorization is commonly done on, e.g., qualitative data from interview

responses, to allow further analysis. The iterative inductive coding process begins with reviewing and understanding

the dataset and developing codes (collectively, a codebook) to assign to dataset items, and iteratively refining the codes

as new themes, relationships, and insights emerge from further review [71]. Inductive coding techniques are commonly

used in computer science and security research in the analysis of qualitative data (e.g., [44, 47, 55]).

4.1 Establishing Analysis Tools

Establishing an initial codebook. To begin development of a codebook for inductive coding, a first coder (C1)5
initially reviewed the 1013-item IoT security advice dataset (Section 2.2) to extract unrefined categories (codes) that

characterize advice items. Discussion and preliminary test codings based on the extracted categories by the first and

second coder (C2) resulted in the following coarse codebook (set of codes): Practice, Incompletely specified practice,

Outcome, Security design principle, Too vague to tell, Out of scope.

After establishing this early codebook (with associated definitions; the final codebook definitions are discussed in

the next paragraph), test sets consisting of ten new, mechanically selected items from the dataset (to test across topics

within the dataset; e.g., item numbers 100, 200, [...], 1000) were coded to determine agreement between the two coders.

This consisted of each coder reading, interpreting, and assigning each item in the test set to a code (informally called a

tag). This process was done a second time on a distinct test set of 10 items. From this process we refined the codebook

by creating new codes for items that did not fit well into existing codes. Assigning an item directly to one of the 6 codes

resulted in low inter-coder agreement of between 30‚Äì40% for the first two test sets. This motivated the development of

the coding tree (described next).

Establishing the coding tree. In an effort to reduce subjectivity, what we call a coding tree was built to more

objectively guide coders toward codes based on a sequence of yes/no questions (the final version of the coding tree is
Fig. 5; the final codebook is in Fig. 4).6 For a given advice item, starting at the top of the tree, each question progressively
directs coders to a next question via branches down the tree, finally arriving at a leaf node (containing the resulting

code). An additional coder (C3) was used to assist in test codings and further refinement of the coding tree.

This method was iteratively refined through five test coding sets, where coders refined the codebook codes, questions,

and organization of the tree. Improvement was measured based on inter-coder agreement after modification (i.e.,

addition/modification of codes and/or questions) of the tree. Over several iterations, coders discussed the results to

resolve ambiguities and gaps in code definitions or questions in the coding tree, refining questions difficult to reliably

answer and coming to an agreement on the codes [39], improving inter-coder agreement through a relatively concise

decision path. To code items that required more reflection, coders consulted a further detailed annotation (see App. A),

which was created by the first coder during the refinement phase and used by coders during the full coding exercise.

Many advice items positioned as practices in the DCMS 1013-item dataset were explicit about technique or technical

method to address security, but lacked actionable detail. For example, Item #50 in the 1013-item dataset [70] states:

4This is not to be confused with the DCMS 13 guidelines detailed in Section 2.2.
5 A coder is a researcher that conducts iterative inductive coding as described in this section. The three coders (C1‚ÄìC3) described are the three authors.
6To our knowledge, we are the first to build a coding tree that uses questions to guide coders to tags for qualitative data. Typically, a codebook is built
iteratively and then used directly (e.g., [39, 44, 47, 55]).

Security Best Practices: A Critical Analysis Using IoT as a Case Study

15

Endpoints must always use standard cryptographic algorithms. This led us to develop the continuum of Fig. 2. On its left

side, practices are less widely actionable (an incompletely specified practice, a general practice/policy). These often

specify vague technical directions to take or methods to use (‚Äústandard cryptographic algorithms‚Äù in the example), but

not explicit actions as typically needed to allow successful execution. On the right side are practices that even end-users

would be able to carry out (P6), suggesting that if an end-user would be able to carry out the practice, so would a more

experienced implementer. Moving left requires more in-depth knowledge and experience to understand (or infer a

direction from) a practice, and implementation details become more ambiguous or unclear (even to a security expert).

Coders did not use this continuum for coding, but we use it to visually represent where each category of practice may

exist in relation to each other as a companion to the coding tree.

Fig. 2. Actionability continuum: practice categories. Terms defined in Fig. 4. An asterisk (*) indicates categories we define as actionable.

The binary yes/no decisions made by coders (through use of the coding tree) resulted in codes being assigned to

advice items; as a side effect, by reaching one of the codes also in Fig. 2, advice items were indirectly placed onto the

continuum. In contrast, where to directly (manually) place an advice item on the continuum (P1‚ÄìP6 in Fig. 2) may be

less clear or might result in some point between two codes. For example, Item #90 states [68]: Communications protocols

should be latest versions with no publicly known vulnerabilities and/or appropriate for the product. This advice item might

be (manually, i.e., directly) coded as either P4 or P5 depending on knowledge of the coder.

Making a second code available to coders. To further address reproducibility in use of the coding tree, a second

code (for a given advice item) is optionally available to a coder. If the coder reaches a question that, based on their

interpretation of the item, could be answered both as yes and no, this option allows both the yes and no edges in the
coding tree to be followed to their respective leaf node code.7 As a result, both codes (first and second codes) would be
assigned to the advice item. For example, Item #977 states [62]: The RTOS makes use of secure storage to protect sensitive

application data and secrets and additionally binds the data to a specific device instance. A coder may answer no to

Question 5 if they believe this advice does not describe or imply actions to take (thus resulting in a code of Incompletely

Specified Practice, P1), but if they could argue it does, they could answer yes to Question 5, leading them toward codes

P3‚ÄìP6. As an argument could be made by a coder that a question could be answered yes and no, we made the decision

to consider both codes as equals in our analyses, i.e., neither code is considered more or less important than the other;

both codes assigned to an advice item are counted in the results (Section 5). For calculating agreement in trial codings,

in cases where two coders availed themselves to a second coding on the same item, if coders agreed on at least one

code, it was counted as an agreement for that item.

Determining test set inter-coder agreement. For the development of the coding tree, i.e., during test codings, as

opposed to the full coding of the 1013-item dataset, by convention, we considered agreement between two test coders if:

‚Ä¢ their coding resulted in agreement on at least one code (one coder‚Äôs first or second code matches either the first

or second code of the other); OR

7Coders were limited to at most two codes per advice item (i.e., at most a single extra yes/no question answer was allowed for any one advice item).

P1, P2*P5 *P4Actionable by fewer partiesActionable by more parties*P6*P4. Practice (Security Expert) P1. Incompletely Specified Practice P2. General Practice or General Policy*P3. Infeasible Practice*P3*P5. Practice (IT Specialist)*P6. Practice (End-User)16

Barrera, Bellman, and van Oorschot

‚Ä¢ both coders‚Äô decisions resulted in an item coded into any code category from P1 to P6 per Fig. 2; AND those two

codes were within plus-or-minus one code distance away on the Fig. 2 continuum.

For example, if one coding was Infeasible Practice (P3) and the other Specific Practice‚ÄîSecurity Expert (P4, one position

right of P3), we declared this an agreement on the basis that their proximity on the continuum implies equivalence, taking

into account the subjective nature of coders answering the decision questions. We refer to this as the ‚Äúplus-or-minus
one rule‚Äù (‚Äú¬±1‚Äù). For advice items where at least one coder used a second code (resulting in a total of 3 or 4 codes on an
advice item‚Äî2 from one coder and 1 from the other, or 2 from both coders), we counted it as an agreement if either
code from a coder was within ¬±1 distance from one of the other coder‚Äôs codes. Fig. 3 relates practices in our continuum
(Fig. 2) with other concepts.8

Final test set coding and inter-coder agreement. A final test coding was done with a set of 20 items. Based on
first/second codes and ¬±1 rule, the mean agreement rate between the three coders was 73% (C1 and C2, 80% agreement,
Cohen‚Äôs Kappa [50] ùúÖ = 0.74; C1 and C3, 75%, ùúÖ = 0.67; C2 and C3, 65%, ùúÖ = 0.59). After the final test coding by the
three coders, a detailed technical analysis and full coding of the 1013 item dataset was done by the first coder using the

coding tree of Fig. 5.

In contrast to typical inductive coding exercises where a code is manually assigned to an item by a coder, when we

say an advice item is ‚Äúcoded‚Äù by a coder, we mean they used the coding tree, and the associated methodology assigned

the resulting code. The coding software interface tool that we developed to ease coding and record results displayed:

the coding tree (Fig. 5), code definitions (Fig. 4), detailed annotations for each question (App. A), and two drop-down

boxes where coders were asked to input the codes delivered through use of the coding tree.

Coding tree methodology summary. In summary, an iterative inductive coding methodology was used both to

derive codes and build the coding tree, and the coding tree was used to code the 1013 advice items (Section 5). While

the coders were asked to follow the coding tree down to the leaf nodes and then enter the code delivered into the

drop-down boxes, our software implementation did not prevent coders from immediately selecting a code, e.g., based

on simply reading the advice item. (While coding reported here generally avoided use of such short-cut coding, this

was not enforced by software. In retrospect, a preferred implementation would force coders to select yes/no answers

until a code was automatically assigned to an advice item.)

We note that many of the terms discussed in Section 3 (e.g., categories from Section 3; the quality, commonality, and

stipulation categories) are not represented in our codes. Where no extra context is provided about an individual advice

item, and we use only the text of the advice (as was our case with analysis herein), it is difficult to know if an item

belongs to any of these categories. For example, determining the quality of an advice item would require knowledge of

how a community rates a practice, to know its commonality among practitioners requires knowledge of how frequently

8This is discussed further in Section 4.3.

Fig. 3. Relationship between terms based on inferred focus of advice‚Äôs intent. The ‚òÖ denotes where we suggest policies [34] fit on this
continuum; arguably, they might alternatively be located in parallel or just to the right of principles.

Focus on end-results;Less technical/more generalFocus on mechanism;More technical/specific detailRequirementGuideline/Recommendation‚óè‚óè‚óè‚óè‚ñ≤‚ñ≤OutcomePractice (Fig. 2)PrincipleStandardPotential target for:‚óè‚ñ≤‚òÖSecurity Best Practices: A Critical Analysis Using IoT as a Case Study

17

P1. Incompletely Specified Practice: Advice that suggests a technical direction of a practice (e.g., a technical method/technique,
software tool, specific rule), but lacks clear indication of any steps to be taken, and fails to meet our definition of actionable.
P2. General Practice or General Policy: Advice that is not explicit about any techniques or tools, but is considered a general
approach to improving security. This may also be policy-related advice. These are not considered actionable (despite being labeled
as a practice) due to their general, less specific nature.
*P3. Infeasible Practice: A practice, but one whose execution would require an unreasonable amount of resources (e.g., time,
financial, human), or cost vastly more than what benefit would be gained.
*P4. Specific Practice‚ÄîSecurity Expert: A practice requiring an expert in security to execute. These may require in-depth
knowledge and experience of security topics, and often rely on the advice recipient to infer steps that are not clearly defined in the
advice.
*P5. Specific Practice‚ÄîIT Specialist: A practice that IT specialists (dedicated IT and development employees) developing or
maintaining a product would be able to execute. These practices do not require the advice recipient to be a security expert, but
assumes basic knowledge of computer security such as that gained through coursework in formal or informal education.
*P6. Specific Practice‚ÄîEnd-User: A practice an end-user would be able to execute. These are actionable by that audience, and
typically executed by the user via direct interaction with the device, using a mobile app, or cloud service.
N1. Security Principle: Advice that suggests a generic (as in applying to many situations) rule to follow that has shown through
experience to improve security outcomes or reduce security exposures.Discussed in Section 4.3.
N2. Security Design Principle: Advice that suggests a Security Principle, but specifically for the Design phase of the lifecycle
(therefore a subset of security principle).
O1a/b. Desired Outcome: Advice that suggests a generic, high-level end goal that a stakeholder would like to attain (as opposed
to a means by which to reach a goal).
M1. Not Useful (too vague/unclear or multiple items): Advice that does not make sense from a language perspective (e.g.,
not full sentence, unclear grammar), or is not focused on a specific task/action to complete.
M2. Beyond Scope of Security: Advice that is not clearly an item that would be implemented for the benefit of security.

Fig. 4. Codes and descriptions for coding tree of Fig. 5. As discussed (Section 5.3), coders who reach M1 for an item may use an
optional sub-label to denote the item as Unfocused. An asterisk (*) indicates categories we define as actionable (matching Fig. 2).

that advice is used, and to know if it is stipulated requires knowledge of how that practice is mandated in possibly

widely varying real world environments. As such, terms like best practice, common practice, or regulation are not used

in our codes (Fig. 4). We have instead used codes that can be applied to advice items without requiring (unavailable)

contextual information.

A decision was made to use a single coder for the 1013-item coding exercise described in this paper, which was based

on all test coders reaching a consensus on the final set of codes and questions in the coding tree (consistent with the

methodology of, e.g., Huaman et al. [39]), the acceptably high level of agreement during test codings, and the work

effort required to manually code (via the coding tree) 1013 items. Using a single coder is noted as a limitation of this

work. Supplemental work (to be reported separately) will explore a second full coding of the 1013 items by at least one

additional coder and pursue detailed explanations of any major deviations found.

As the advice items in the dataset are grouped by the 13 guidelines in the DCMS mapping document [74], all advice

items in the coding of the full 1013 item set were randomly ordered to avoid bias from reading similar advice in

repetition.

4.2 Advice Categorization by Lifecycle Phase

Separate from the inductive coding of Section 4.1, the first coder assigned each actionable item (P3‚ÄìP6) to a stage in the

IoT lifecycle (Fig. 1) where the item could be best carried out (in the subjective opinion of the coder), independent of the

codes defined and used in the coding tree. This was done by determining which stakeholder would be in a position (in

18

Barrera, Bellman, and van Oorschot

Q1. Is the item conveyed in unambiguous language, and relatively focused?
Q2. Is it arguably helpful for security?
Q3. Is it focused more on a desired outcome than how to achieve it?
Q4. Does it suggest a security technique, mechanism, software tool, or specific rule?
Q5. Does it describe or imply steps or explicit actions to take?
Q6. Is it viable to accomplish with reasonable resources?
Q7. Is it intended that the end-user carry out this out?
Q8. Is it intended that a security expert carry out this item?
Q9. Is it a general policy, general practice, or general procedure?
Q10. Is it a broad approach or security property?
Q11. Does it relate to a principle in the design stage?

Fig. 5. Decision tree for assigning codes to advice items (coding tree). Leaf node codes explained in Fig. 4. Black shading (M1, M2, P1,
P3) denotes advice considered not helpful to include (for lack of actionability or feasibility); white codes (P4, P5, P6) are desirable
actionable practices (excluding infeasible P3); grey-codes (O1a, O1b, N1, N2, P2) are considered useful context, but non-actionable.

our view) to carry out the item, and matching where this would appear to best occur in the lifecycle. This determination

was based on which stakeholders could reasonably execute a practice (within reason‚Äîan end-user given an API would

not be likely to implement a best practice or fix a vulnerability), not necessarily the single stakeholder in the best/most

effective position to implement it, thus allowing for items to be associated with multiple stages. For example, Item #191

[38] states:

When a product is being developed it is often enabled with debugging and testing technologies to facilitate

the engineering process. This is entirely normal. However, when a device is ready for production deployment,

these technologies should be stripped from the production environment prior to the definition of the Approved

Configuration.

This item could either be executed in the OS/App Development stage (1.2b) where code is stripped from software

before completion, or during the Integration & Pre-Configuration (1.3) stage where features may be disabled or left

out of device integration. We considered only practices (being implicitly actionable) for this categorization, as without

actionability, it is difficult to determine what steps would need to be taken and when (in the lifecycle) they would be

executed.

NYQ1M1YNQ2M2YQ4NYQ11N2N1YNQ8P6P4P5NYQ6YNQ7P1Start: Advice itemfrom datasetNYQ5NYQ10O1bYNQ9P2YNQ3O1aP3Security Best Practices: A Critical Analysis Using IoT as a Case Study

19

While an indication of where the advice item would be carried out in the lifecycle was included by many items in

the advice statement itself (e.g., do not hard-code secret access codes for testing/debugging in software [42]), others

required subjective judgement for placement (e.g., ‚Äúkeep software updated‚Äù [24] could be targeting the Creation phase or

Usage phase depending on which stakeholder it implies should maintain software). If the item did not have an obvious

or implied associated lifecycle phase, we categorized it as Unclear (see Fig. 7).

4.3 Relationship to Security Principles

We observed that many security advice items were rephrasings of established security principles. In our context of

computer security, we define a principle to be a generic (applying to many situations) rule shown through experience to

improve security outcomes or reduce exposures, and a design principle to be a subset specifically guiding the design of a

system. Other subsets may relate to other lifecycle phases. For context, note Saltzer and Schroeder [65] define eight ‚Äú[...]

useful principles that can guide the design and contribute to an implementation without security flaws‚Äù. NIST [69] notes
‚Äúthe primary focus of these principles is the implementation of technical controls‚Äù,9 suggesting that security principles are
appropriate targets to be implemented via practices. In our coding tree, both security principles and (more specifically)

security design principles are leaf nodes.

Fig. 3 conveys our view of the relationship between security principles, practices, outcomes, and other terms. On one

extreme (left) are concepts more focused on end-result (outcomes); on the other are the most actionable items that focus

on mechanisms to reach outcomes, often specified in fine detail (standards). As Fig. 3 indicates, some items on this

continuum may serve as a guideline or requirement (Table 2). Ideally, in our view, what is imposed as requirements by

governing bodies should be practices or standards (versus principles or outcomes), as requirements should be actionable

so those subject to them have a clear understanding of how to follow them (cf. Section 3.2 for actionable). This is

represented on this continuum by the labeling of Practice and Standard as potential targets for requirements (denoted

by the triangle).

4.4 Actual Use of Security Advice Coding Tree Methodology

We envision the methodology described in Section 4 to be used primarily in two ways. The first is for measuring the

actionability of existing advice as a means to establish a general view of the current state of IoT security advice, and

to determine where advice fails to meet the needs of security practitioners. This is the primary focus of the coding

exercise described in Section 4, and analyzed in Section 5.

The methodology can be used in a second way for the analysis of new IoT security advice, as a tool to assist advice

authors in creating actionable advice. If advice authors themselves use the coding tree on their own advice items, they

can differentiate actionable from non-actionable advice (among other more fine-grained characteristics of advice). After
using the coding tree, security advice items that analysis tags with an undesirable code10 can then be revisited by advice
authors to revise, reword, and clarify the explanation of the advice.

Once an advice item is revised, the questions in the coding tree may yield different answers, giving advice authors

feedback about whether their changes have had a positive impact on the actionability of their advice, or if it follows a

9Examples of these principles (from the quote) are ‚Äúprotect against all likely classes of ‚Äòattacks‚Äô‚Äù, ‚Äúuse unique identities to ensure accountability‚Äù, and ‚Äúlimit
or contain vulnerabilities‚Äù [69].
10 While we view the creation of actionable practices as a preferred objective for advice authors, and in this paper advocate the pursuit of actionable
codes P4‚ÄìP6, we acknowledge that creating actionable practices may not be the goal of all advice authors‚Äîsome may intentionally craft non-actionable
guidance in the form of Outcomes (O1a/b), General Practices/General Policies (P2), or Security Principles (N1/N2). Fig. 4 describes these categories. In any
case, whether actionable or not, advice authors may use the SAcoding method to cross-check that their advice items match the categories that they intend.

20

Barrera, Bellman, and van Oorschot

path down the tree to a more desirable code. If the coding tree outputs an undesirable code (e.g., non-actionable), those

giving the advice may be able to observe (from the coding tree) at which question the advice diverged from a path to a

desired code.

For example: ‚Äúencrypt stored passwords‚Äù gives vague advice and is ambiguous on how to achieve encryption. Question

5 (from the coding tree, Fig. 5) sends this advice to Incompletely Specified Practice (P1), as it lacks actions to take. The

advice item could be reworded to specify a particular encryption algorithm and mode. An accompanying document or

note could also provide explicit references to aid implementation, thus now passing Q5 as actionable.

5 EMPIRICAL ANALYSIS OF IOT SECURITY ADVICE DATASET

In this section we carry out our analysis of the 1013-item IoT security advice dataset, which for context, is the dataset of

security advice from which the DCMS created their 13 guidelines [73]. We coded the items in this collection using the

methodology of Section 4. The primary goal of assigning each item to codes (and associated definitions) is to provide a

general sense of how well existing advice dataset lists and literature specify practices (as opposed to advice positioned

as practices, but failing to be actionable, as required by our definition). Identifying where practices are carried out

throughout the IoT lifecycle allows us to see which stakeholders are in the best position, or have the greatest number of

items to address regarding contributing to overall device security.

5.1 Results of Coding

Fig. 6 summarizes the distribution of codes given to all advice items in the DCMS 1013-item dataset, as coded by the first

coder (C1). For distinguishing actionable vs. non-actionable advice (bottom of figure), if an item had two codes (first and

second) and at least one was an actionable code (i.e., P3‚ÄìP6), we declared the item actionable based on the reasoning

that an argument could be made for its actionability. For example, if an item was coded both as an Incompletely Specified

Practice (P1) and Specific Practice‚ÄîSecurity Expert (P4), we declared the item actionable as P4 is defined to be actionable.

As such, the sum of actionable and non-actionable items in Fig. 6 adds to 1013.

Fig. 6. Number of items receiving each code, from coding of 1013-item advice dataset [11]. Sum of counts in top portion exceeds 1013
as each item could be assigned one or two codes (first/second, per Section 4.1). Removing resulting double-counting, the number of
actionable items is 320/1013 = 32% (not: 214 + 139 + 3 = 356/1013 = 35.1%). Shading intensity follows the scheme of Fig. 5.

The coding tree‚Äôs software interface allowed a coder to designate whether an item was specific to IoT. None of the

items in the dataset were designated in this way. While herein the coding tree is used to explore IoT security, we believe

M1. Not UsefulM2. Beyond Scope of Sec.N1. Sec. PrincipleN2. Sec. Design PrincipleO1a/b. Desired OutcomeP1. Inc. Specified PracticeP2. General Practice/Policy*P3. Infeasible Practice*P4. Security Expert*P5. IT Specialist*P6. End-UserNon-Actionable (items)*Actionable (items)119980.3% (3)69332026%1% (13)2% (22)10%0013914%21421%31531%11411%32% (removing double-counting)68% (removing double-counting)140Unfocused (14%)259Security Best Practices: A Critical Analysis Using IoT as a Case Study

21

that its design and resulting structure apply more broadly, to analysis of security advice in general. Restated, the design

intent is a generic (non IoT-specific) method.

5.2 Proportion of Non-Actionable Advice

Our analysis shows that organizations‚Äîoften highly credible ones‚Äîare producing recommendations for manufacturers

that are not, by our definitions and analysis, actionable, thus we believe at greater risk of being improperly (or not at

all) executed or implemented. This low proportion of actionable practices (of the 1013-item set) is, in our view, a signal

that the security community must significantly improve how we capture and state ‚Äúbest practices‚Äù if manufacturers are

expected to follow recommendations.

The methodology used declares any code after the yes branch of ùëÑ5 in the coding tree (P3‚ÄìP6) to be actionable
(per Section 3.2). As shown in Fig. 6, in total 32% (320/1013) of advice items were found to be actionable (at least one

actionable code, per Section 4.1). This includes the following tags (Fig. 6 caption explains over-counting):

‚Ä¢ 21% of items (214/1013) had one tag (of possibly two) being P4 (Specific Practice‚ÄîSecurity Expert);
‚Ä¢ 14% of items (139/1013) had one tag (of possibly two) being P5 (Specific Practice‚ÄîIT Specialist); and
‚Ä¢ <1% of items (3/1013) had one tag (of possibly two) being P6 (Specific Practice‚ÄîEnd-User).

The Infeasible Practice (P3) code went unused; this was encouraging, suggesting that advice providers have an under-

standing of what sorts of practices are feasible (in both resources and knowledge) for their target audience. Similarly,

the code Beyond the Scope of Security (M2) was also unused; however, this is arguably due to source documents being

generally targeted at computer security.

As Fig. 6 notes, 68% of the advice was declared to be non-actionable. We expect that this significant majority of

advice items may often be poorly implemented (or not at all, a failure in both cases) despite advice recipients‚Äô best

efforts to understand the advice. This does not imply that non-actionable advice is in all cases detrimental‚Äîoutcomes,

principles, and general practices still specify desirable end-results (outcomes) and generic goals. Actionability may not

be essential in all use cases (cf. footnote 10); however, our underlying assumption is that advice givers (for the advice

datasets under discussion) intend to be offering advice positioned as best practices. We argue that actionability should

be considered a high (if not the highest) priority among the characteristics of such security advice (see Section 3.2).

5.3 ‚ÄòNot Useful‚Äô Advice

During the iterative development of our coding tree methodology, we observed many advice items in the DCMS

1013-item dataset (described in Section 2.2) tended to not be actions to take, but descriptions of security techniques

(e.g., a hardware security module, public-key encryption) or threats to a system (e.g., unused but accessible network

ports), and offered no suggestion for any action to take or execute. Item #387 [70] provides an example of this:

Network firewalls are message-oriented filtering gateways used extensively to segment IIoT [industrial IoT]

systems. Most firewalls are Layer 2, 3 or 4 IP routers/message forwarders with sophisticated message filters.

Firewalls may be deployed as either physical or virtual network devices. A firewall‚Äôs filtering function examines

every message received by the firewall. If the filter determines that the message agrees with the firewall‚Äôs

configured traffic policy, the message is passed to the firewall‚Äôs router component to be forwarded.

One could make the argument that the description of a technique implies that the advice giver wants a follower to

use the technique, but the italic text block above reads quite different from ‚Äúdo this‚Äù security advice and is lacking

in actionable detail. As such, we consider advice of this nature to be not sufficient for a stakeholder to execute. 26%

22

Barrera, Bellman, and van Oorschot

(259/1013) of items were coded as Not Useful (M1); see Fig. 6 and description of M1 in Fig. 4. Note that M1 is also used

for advice items that are judged to ‚Äúnot make sense‚Äù from a grammar or language perspective.

Similarly, individual advice ‚Äúsub-items‚Äù are commonly given in rapid succession within a single advice item (which

may take the form of several sentences or a paragraph). As a sub-category of the Not Useful (M1) code, we added an

Unfocused supplementary code for coders to use when they find multiple sub-items within one item (represented as the

left sub-bar of the Not Useful code in Fig. 6). For example, Item #84 [15] is, in our view, an example of this:

IoT Devices Should Follow Security & Cryptography Best Practices. [1] BITAG recommends that IoT device

manufacturers secure communications using Transport Layer Security (TLS) or Lightweight Cryptography

(LWC). Some devices can perform symmetric key encryption in near-real time. In addition, Lightweight

Cryptography (LWC) provides additional options for securing traffic to and from resource constrained devices.

[2] If devices rely on a public key infrastructure (PKI), then an authorized entity must be able to revoke

certificates when they become compromised, as web browsers and PC operating systems do. Cloud services can

strengthen the integrity of certificates issued by certificate authorities through, for example, participating in

Certificate Transparency. [3] Finally, manufacturers should take care to avoid encryption methods, protocols,

and key sizes with known weaknesses. [4] Vendors who rely on cloud-hosted support for IoT devices should

configure their servers to follow best practices, such as configuring the TLS implementation to only accept the

latest TLS protocol versions.

This advice item jumps across four topics (we inserted the numbers for exposition): (1) the use of TLS or lightweight

cryptography, (2) certificate revocation, (3) avoiding weak or vulnerable key sizes, and (4) avoiding outdated TLS

versions. Trying to successfully code advice such as this (i.e., as a coder) was a challenge, as different sub-items could

be coded differently.

We found that advice items with a longer word length were often unfocused in this way. In total, 54% (140/259) of the

items that were tagged with Not Useful (M1) codes, or 13.8% (140/1013) of all items were coded as Unfocused, implying

that in the judgement of the coder they contained multiple distinct topics within the advice item (similar to the above

example). Had we extracted each sub-item from the original dataset (making them more narrowly focused, but as a

result, potentially removing them from surrounding context), these may have been coded differently, suggesting a

limitation to this portion of our work.

5.4 Associating Advice Items with IoT Lifecycle Stages

Fig. 7 shows the results of one coder‚Äôs manual association of each of the 320 actionable practices (Fig. 6) with one or

more of the lifecycle stages. All lifecycle phases associated with each practice (Section 4.2) were combined to yield the

count shown for each bar. For example, if an item was coded as taking place in both the OS/App Development (1.2b)

and Design (1.1) stages, each of these counts were increased by one (1).

The practice distribution among phases reveals important information about the overall execution of best practices:

89% (284/320) of practices that were deemed actionable could be implemented in at least one lifecycle phase within the

manufacturer‚Äôs control (i.e., the Creation phase), i.e., the designers and manufacturers are in a position to implement

them. As a subset of this, the OS/app developers alone (Phase 1.2b, Fig. 1) are in a position to implement 73% (234/320)

of practices. This follows from many advice items being software-related, thus suitable for implementation by one or

more of several stakeholders involved in software development, before the product is in end-user hands.

Security Best Practices: A Critical Analysis Using IoT as a Case Study

23

Fig. 7. Number of actionable practices that we declared as suitable to implement at each lifecycle phase (Fig. 1). Total of all numbers
and percentages exceed actionable practice total (320), phase totals, and 100% as practices may be suitable to implement in multiple
stages. Percentages are proportion of 320 actionable practices.

While this finding may seem self-evident, it draws focus to the importance of attention to security during the

product (device) Creation phase, and in particular, the importance of IoT security advice being implementable (and

understandable) by IoT device manufacturers and their software development partners.

6 RELATED WORK

Explicit formal definitions for the term best practice are rare in the security literature. Literature about the nature

and definition of security best practices (as opposed to examples of best practices) is discussed in Section 3. In the

remainder of this section we discuss related work on establishing security practices (for IoT and other areas). Tschofenig

and Baccelli [72] discuss efforts by The European Union Agency for Cybersecurity (ENISA) and the IETF to provide

recommendations and specifications on IoT security. They categorize technical and organizational areas to be considered

for the secure development and use of IoT devices. Moore et al. [53] pursue specific best practices for IoT, specifically

regarding network-based attacks. Based on our definitions herein, most of their advice items are not actionable (thereby

not what we consider to be practices). Dingman et al. [23] examined six sets of IoT security advice and looked to

determine whether three large-scale security events may have been averted if their collected security advice was

followed.

Alrawi et al. [6] analyze and systematize work on home-based IoT security and propose a methodology for evaluating

the security of home-based systems. They note that best practices are ‚Äúreadily available‚Äù, but provide neither definitions

nor specific references. Assal and Chiasson [8] note that for software development security practices, technical detail is

inconsistent, and find common security advice is often not followed. Redmiles et al. [63] analyze security advice for

end-users and find through a user-study that most advice is viewed (by users) as being actionable, but it is unclear to

them which of the actionable advice is the most important to follow (i.e., which advice to prioritize from the set). Acar

et al. [1] analyze software development security advice and find it is often inadequate for software developer needs,

and lacking resources (e.g., implementation examples, tutorials) to help them understand the guidance.

A number of government and industrial agencies provide security advice for IoT, for both manufacturers and

groups looking to acquire IoT devices for their organizations. ENISA [28] published an expansive document about

IoT security. This includes a substantial set of security recommendations, but also useful contextual and informative

1.1 Design1.2a Hardware Manufacture1.2b OS/App Development1.3 Integration & Configuration2.1 Installation2.2 Configuration3.1a Normal Use3.1b Software/Firmware Update4.1 Data/Key Removal4.2a Disposal4.2b Transfer Ownership3223481010%73%25%<1% (1)22%8%01% (4)165%3% (10)69Unclear30251. Creation(Total: 284)2. Install(Total: 16)3. Usage(Total: 35)4. Decomm.(Total: 4)9%24

Barrera, Bellman, and van Oorschot

sections including (to single out a select few) the document‚Äôs target audience (cf. Section 3.2), an overview of what

IoT is and the relevant components, threat and risk analyses, and technical measures for executing the advice (these

measures appear to be positioned as technical steps to complement other security advice).

ETSI [25] (cf. Section 2.2) provides a series of baseline requirements for IoT security. These requirements use the 13

DCMS guidelines [73] as general topic headers (adding a new one of their own), but provide more detail about technical

steps to be taken. To supplement the baseline requirements document, ETSI provides a document describing how to

confirm conformance with the advice therein, noting that the advice in the support document is independent of an

assurance scheme [26]. Assurance is historically associated with products for governmental use [13, Chapters 18‚Äì21]

[36, Chapter 13], but is typically considered too expensive or otherwise unsuitable to the consumer space. For DCMS

documents [21, 73, 74] used in this paper, see Section 2.2.

NIST published three documents surrounding the interaction between US federal government agencies and IoT

manufacturers [57]. Two of these NIST documents [29, 30] aim to assist IoT manufacturers to produce secure devices

specifically for use in the US federal government by offering technical and non-technical baseline guidance. One of the

NIST documents [31] is intended to help government agencies learn what features or characteristics they should seek

when procuring IoT devices.

RFC 8576 [33] proposes a generic lifecycle model of an IoT device, presented as a simplified model. Other descriptions

of lifecycles may include the key functional components that describe its primary function (versus the entirety of

its life), e.g., Alrawi et al.‚Äôs IoT malware lifecycle components [7]. NIST‚Äôs SP 800-27 [69] outlines five major general

computer and IT system lifecycle phases and suggests 33 IT security principles. While developed independently, our

lifecycle of Section 2.1 unsurprisingly has similarities, e.g., design/development, primary usage, and disposal/end-of-life

phases. The NIST SP suggests that many individual principles are vital to positive security outcomes across multiple

phases, implying there are important principles for phases other than the design phase. NIST SP 800-160 [64] outlines a

taxonomy of 32 security design principles covering three areas of systems security: security architecture and design,

security capability and intrinsic behaviors, and lifecycle security; the latter two are not specific to the design phase.

Morgner et al. [54] explore the relationship between efforts in formal IoT technical standards and the (unfortunate)

reality of the economics of IoT security and its implications for the general security of manufacturers.

7 CONCLUDING REMARKS

The basic concept of best practices is familiar to non-experts. Our analysis of a wide selection of security advice found

conflation of the ideas of security goals (outcomes) and the steps or methods by which they may be reached (practices).

We highlighted an important characteristic for security advice: whether or not it is actionable. We offer uniform,

consistent terminology (Section 3) that characterizes and separates concepts. This allowed systematic exploration that

began with generic discussion and analytic classification and was cross-checked through specific focus on consumer

IoT devices and their lifecycle.

A main contribution of this paper is the development of the security advice coding tree methodology and its use to

systematically analyze a large collection of IoT security advice. In particular, we examined how actionable current advice

is (we use the DCMS 1013-item dataset as representative of current IoT security advice), and what advice characteristics

(i.e., corresponding to the codes of Fig. 4) emerge from this dataset. Our main focus has been the DCMS 1013-item IoT

security advice dataset, which itself originates from other organizations.

For our analysis, iterative inductive coding was used to create a codebook that represents the characteristics of

security advice (e.g., whether they are objectives to reach or practices to follow). To more objectively assign a code to

Security Best Practices: A Critical Analysis Using IoT as a Case Study

25

each item in the dataset, we designed a coding tree. We suggest that IoT security advice-giving organizations consider

using the coding tree and methodology of Section 4 to measure whether potential advice is actionable, and take steps to

improve the advice‚Äôs actionability, unless their explicit goal is to target, e.g., security principles or outcomes to reach.

From our analysis of 1013 advice items from industrial, governmental, and academic sources, we were surprised to

find that the majority of advice items are not actionable practices, but rather, what we deem to be non-actionable advice.

Among the practices we identified as actionable, 73% are suitable to implement in the OS/App Development lifecycle

phase of an IoT device (Section 5.4), thus by the product manufacturer and its software development partners. It is

generally recognized that poor security practices early in the lifecycle accrue what we might call a security debt, with

negative consequences in later phases (analogous to tech debt where technical shortcuts during development incur later

costs [48]); from this, our results herein highlight the fundamental role of pre-deployment stakeholders to underpin

security for aspects that they alone are in a position to control.

As the Internet of Computers has grown into the Internet of Things, an old problem remains: how to ensure that

security best practices are followed. An open question is whether the research community can find ways to help

advice-givers (including governments) to compile more effective guidance, and have manufacturers embrace and

execute advice given. Our work argues that currently, even in the security and technology communities (not to mention

the general public), ambiguity surrounds the language of technical best practices‚Äîsuch that arguably, the term does as

much harm to the security community as good. One hypothetical path forward (to provoke thoughts, more than as a

practical suggestion) is to seek agreement within the technical security community that the term itself is vague and

nebulous, and its use should be boycotted. Another path forward is to work towards consensus on definitions (as we

pursue herein).

We suggest that organizations proposing and endorsing ‚Äúbest practice‚Äù advice have a clear idea of whether they are

recommending practices, specifying baseline security requirements, or simply offering advice about good principles to

think about. If the goal is that relevant stakeholders adopt and implement specific practices aiming to reduce security

exposures, we believe it is imperative that (actionable) best practices be identified and clearly stated, versus vague

outcomes‚Äîlest the target stakeholders be unable to map advice to a concrete practice, even if so motivated. In summary:

if security experts do not find guidelines clearly actionable, we should not expect (security non-expert) manufacturers

to magically find a way to adopt and implement the advice. The economic motivation of manufacturers [54] (keeping

in mind markets for lemons [2]), their poor track record in IoT security, and lack of accountability for vulnerabilities,

point to a worrisome future. We hope that our work is a step towards improving the efficacy of advice on best practices.

ACKNOWLEDGMENTS

We thank anonymous referees for helpful comments. This work was supported by the Natural Sciences and Engineering

Research Council of Canada (NSERC), which is acknowledged for Discovery Grants to the first and third authors, and

for funding the third author as Canada Research Chair in Authentication and Computer Security.

REFERENCES

[1] Yasemin Acar, Christian Stransky, Dominik Wermke, Charles Weir, Michelle L. Mazurek, and Sascha Fahl. 2017. Developers Need Support, Too: A

Survey of Security Advice for Software Developers. In Cybersecurity Development (SecDev). IEEE, 22‚Äì26.

[2] George A. Akerlof. 1970. The Market for ‚ÄúLemons‚Äù: Quality Uncertainty and the Market Mechanism. The Quarterly Journal of Economics 84, 3

(1970), 488‚Äì500.

[3] Alliance for Internet of Things Innovation (AIOTI). 2015. Report: Working Group 4‚ÄîPolicy. https://aioti.eu/wp-content/uploads/2017/03/

AIOTIWG04Report2015-Policy-Issues.pdf.

26

Barrera, Bellman, and van Oorschot

[4] Alliance for Internet of Things Innovation (AIOTI). 2016. AIOTI Digitisation of Industry Policy Recommendations. https://aioti.eu/wp-content/

uploads/2017/03/AIOTI-Digitisation-of-Ind-policy-doc-Nov-2016.pdf.

[5] Alliance for Internet of Things Innovation (AIOTI). 2016.

Report on Workshop on Security and Privacy in the Hyper-connected
World. https://aioti-space.org/wp-content/uploads/2017/03/AIOTI-Workshop-on-Security-and-Privacy-in-the-Hyper-connected-World-Report-
20160616_vFinal.pdf.

[6] Omar Alrawi, Chaz Lever, Manos Antonakakis, and Fabian Monrose. 2019. SoK: Security Evaluation of Home-Based IoT Deployments. In IEEE

Symp. Security and Privacy. IEEE, 1362‚Äì1380.

[7] Omar Alrawi, Charles Lever, Kevin Valakuzhy, Ryan Court, Kevin Snow, Fabian Monrose, and Manos Antonakakis. 2021. The Circle Of Life: A

Large-Scale Study of The IoT Malware Lifecycle. In USENIX Security Symp. USENIX, 3505‚Äì3522.

[8] Hala Assal and Sonia Chiasson. 2018. Security in the Software Development Lifecycle. In Symp. on Usable Privacy and Security (SOUPS). USENIX,

281‚Äì296.

[9] AT&T. 2016. The CEO‚Äôs Guide to Securing the Internet of Things. https://www.business.att.com/cybersecurity/docs/exploringiotsecurity.pdf.
[10] Australian Department of Home Affairs and Australian Cyber Security Centre. 2020. Code of Practice‚ÄîSecuring the Internet of Things for Consumers.

https://www.homeaffairs.gov.au/reports-and-pubs/files/code-of-practice.pdf.

[11] Christopher Bellman. 2022. cb1013-dataset. https://github.com/ChristopherBellman/SecurityAdvice/blob/main/cb1013-dataset-TOPS.json.
[12] Christopher Bellman and Paul C. van Oorschot. April 25, 2020. Best Practices for IoT Security: What Does That Even Mean? Technical report,

arXiv:2004.12179.

[13] Matt Bishop. 2003. Computer Security: Art and Science. Addison-Wesley.
[14] Harold Boley, Micha Meier, Chris Moss, Michael M Richter, and A. A. Voronkov. 1991. Declarative and Procedural Paradigms - Do They Really

Compete?. In International Workshop on Processing Declarative Knowledge. Springer, 383‚Äì398.

[15] Broadband Internet Technical Advisory Group (BITAG). 2016. Internet of Things (IoT) Security and Privacy Recommendations. http://www.bitag.

org/documents/BITAG_Report_-_Internet_of_Things_(IoT)_Security_and_Privacy_Recommendations.pdf

[16] CableLabs. 2017. A Vision for Secure IoT. https://www.cablelabs.com/insights/vision-secure-iot/.
[17] City of New York (NYC) Guidelines for the Internet of Things. 2019. Privacy + Transparency. https://iot.cityofnewyork.us/privacy-and-transparency/
[18] City of New York (NYC) Guidelines for the Internet of Things. 2019. Security. https://iot.cityofnewyork.us/security/
[19] Cloud Security Alliance (CSA). 2015. Security Guidance for Early Adopters of the Internet of Things (IoT). https://downloads.cloudsecurityalliance.

org/whitepapers/Security_Guidance_for_Early_Adopters_of_the_Internet_of_Things.pdf

[20] Cloud Security Alliance (CSA). 2016.

Future-proofing the Connected World: 13 Steps to Developing Secure IoT.

https://downloads.

cloudsecurityalliance.org/assets/research/internet-of-things/future-proofing-the-connected-world.pdf

[21] Copper Horse Ltd. 2019. Mapping Security & Privacy in the Internet of Things. https://iotsecuritymapping.uk/wp-content/uploads/Mapping-of-

Code-of-Practice-to-recommendations-and-standards_v3.json. Version 3 dataset.

[22] George Corser, Glenn A. Fink, Mohammed Aledhari, Jared Bielby, Rajesh Nighot, Sukanya Mandal, Nagender Aneja, Chris Hrivnak, and Lucian
Cristache. 2017. IoT Security Principles and Best Practices. https://internetinitiative.ieee.org/images/files/resources/white_papers/internet_of_
things_feb2017.pdf. IEEE.

[23] Andrew Dingman, Gianpaolo Russo, George Osterholt, Tyler Uffelman, and L Jean Camp. 2018. Poster Abstract: Good Advice That Just Doesn‚Äôt

Help. In 2018 IEEE/ACM Third International Conference on Internet-of-Things Design and Implementation (IoTDI). IEEE, 289‚Äì291.

[24] European Telecommunications Standards Institute (ETSI). 2019. CYBER; Cyber Security for Consumer Internet of Things. https://www.etsi.org/

deliver/etsi_ts/103600_103699/103645/01.01.01_60/ts_103645v010101p.pdf

[25] European Telecommunications Standards Institute (ETSI). 2020. CYBER; Cyber Security for Consumer Internet of Things: Baseline Requirements

(ETSI EN 303 645). https://www.etsi.org/deliver/etsi_en/303600_303699/303645/02.01.01_60/en_303645v020101p.pdf

[26] European Telecommunications Standards Institute (ETSI). 2021. CYBER; Cyber Security for Consumer Internet of Things: Conformance Assessment
of Baseline Requirements (ETSI TS 103 701 V1.1.1). https://www.etsi.org/deliver/etsi_ts/103700_103799/103701/01.01.01_60/ts_103701v010101p.pdf.
[27] European Union Agency for Network and Information Security (ENISA). 2015. Security and Resilience of Smart Home Environments. https:

//www.ENISA.europa.eu/publications/security-resilience-good-practices

[28] European Union Agency for Network and Information Security (ENISA). 2017. Baseline Security Recommendations for IoT. https://www.ENISA.

europa.eu/publications/baseline-security-recommendations-for-iot

[29] Michael Fagan, Jeffrey Marron, Kevin G. Brady Jr., Barbara B. Cuthill, Katerina N. Megas, and Rebecca Herold. 2020. Draft NISTIR 8259C‚ÄîCreating a

Profile Using the IoT Core Baseline and Non-Technical Baseline. NIST.

[30] Michael Fagan, Jeffrey Marron, Kevin G. Brady Jr., Barbara B. Cuthill, Katerina N. Megas, and Rebecca Herold. 2021. NISTIR 8259B‚ÄîIoT Non-Technical

Supporting Capability Core Baseline. NIST.

[31] Michael Fagan, Jeffrey Marron, Kevin G. Brady Jr., Barbara B. Cuthill, Katerina N. Megas, Rebecca Herold, David Lemire, and Brad Hoehn. 2020. SP

800-213‚ÄîIoT Device Cybersecurity Guidance for the Federal Government: Establishing IoT Device Cybersecurity Requirements. NIST.

[32] Dirk Fahland, Daniel L√ºbke, Jan Mendling, Hajo Reijers, Barbara Weber, Matthias Weidlich, and Stefan Zugal. 2009. Declarative Versus Imperative
Process Modeling Languages: The Issue of Understandability. In Enterprise, Business-Process and Information Systems Modeling. Springer, 353‚Äì366.
[33] Oscar Garcia-Morchon, Sandeep S. Kumar, and Mohit Sethi. 2019. State-of-the-Art and Challenges for the Internet of Things Security. https:

//datatracker.ietf.org/doc/draft-irtf-t2trg-iot-seccons/.

Security Best Practices: A Critical Analysis Using IoT as a Case Study

27

[34] Simson Garfinkel, Gene Spafford, and Alan Schwartz. 2003. Chapter 3: Policies and Guidelines. In [35].
[35] Simson Garfinkel, Gene Spafford, and Alan Schwartz. 2003. Practical UNIX and Internet Security (3rd edition). O‚ÄôReilly Media, Inc.
[36] Dieter Gollmann. 2011. Computer Security, 3rd Edition. Wiley.
[37] Paul A. Grassi and 12 others. 2017. SP 800-63B‚ÄîDigital Identity Guidelines: Authentication and Lifecycle Management. NIST.
[38] GSM Association. 2017. IoT Security Guidelines for Endpoint Ecosystems‚ÄîVersion 2.0. https://www.gsma.com/iot/wp-content/uploads/2017/10/

CLP.13-v2.0.pdf.

[39] Nicholas Huaman, Sabrina Amft, Marten Oltrogge, Yasemin Acar, and Sascha Fahl. 2021. They Would do Better if They Worked Together: The Case

of Interaction Problems Between Password Managers and Websites. In IEEE Symp. Security and Privacy. IEEE, 1626‚Äì1640.

[40] Wei Huang, Afshar Ganjali, Beom Heyn Kim, Sukwon Oh, and David Lie. 2015. The State of Public Infrastructure-as-a-Service Cloud Security. ACM

Computing Surveys (CSUR) 47, 4 (2015), 1‚Äì31.

[41] IoT Security Foundation. 2017. IoT Security Compliance Framework 1.1. https://www.iotsecurityfoundation.org/wp-content/uploads/2017/12/IoT-

Security-Compliance-Framework_WG1_2017.pdf

[42] IoT Security Initiative. 2018. Security Design Best Practices. https://www.iotsi.org/security-best-practices.
[43] Erica Johnson. Feb 9 2020. Online Banking Agreements Protect Banks, Hold Customers Liable for Losses, Expert Says. Canadian Broadcasting

Corporation, https://www.cbc.ca/news/business/online-banking-agreements-1.5453192.

[44] Ruogu Kang, Laura Dabbish, Nathaniel Fruchter, and Sara Kiesler. 2019. ‚ÄúMy data just goes everywhere‚Äù: User Mental Models of the Internet and

Implications for Privacy and Security. In Symposium on Usable Privacy and Security (SOUPS). USENIX, 39‚Äì52.

[45] Guy King. 2000. Best Security Practices: An Overview. In National Information Systems Security Conference. NIST, 12 pages.
[46] Constantinos Kolias, Georgios Kambourakis, Angelos Stavrou, and Jeffrey Voas. 2017. DDoS in the IoT: Mirai and Other Botnets. Computer 50, 7

(2017), 80‚Äì84.

[47] Katharina Krombholz, Wilfried Mayer, Martin Schmiedecker, and Edgar Weippl. 2017. ‚ÄúI Have No Idea What I‚Äôm Doing‚Äù‚ÄîOn the Usability of

Deploying HTTPS. In USENIX Security Symp. USENIX, 1339‚Äì1356.

[48] Philippe Kruchten, Robert L. Nord, and Ipek Ozkaya. 2012. Technical Debt: From Metaphor to Theory and Practice. IEEE Software 29 (2012), 18‚Äì21.
[49] Greg Lindsay, Beau Woods, and Joshua Corman. 2016. Smart Homes and the Internet of Things. https://www.atlanticcouncil.org/wp-content/

uploads/2016/03/Smart_Homes_0317_web.pdf.

[50] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and Inter-rater Reliability in Qualitative Research: Norms and Guidelines

for CSCW and HCI Practice. Proc. ACM Hum.-Comput. Interact 3, CSCW (Nov 2019), 1‚Äì23.

[51] Gary McGraw. 2006. Software Security: Building Security In (First edition). Addison-Wesley Professional.
[52] Microsoft. 2018. Security best practices for Internet of Things (IoT). https://docs.microsoft.com/en-us/azure/iot-fundamentals/iot-security-best-

practices

[53] Keith Moore, Richard Barnes, and Hannes Tschofenig. July, 2017. Best Current Practices (BCP) for IoT Devices. https://www.ietf.org/archive/id/draft-

moore-iot-security-bcp-01.txt. IETF Internet-Draft (Expired).

[54] Philipp Morgner and Zinaida Benenson. 2018. Exploring Security Economics in IoT Standardization Efforts. In Workshop on Decentralized IoT

Security and Standards (DISS). Internet Society, 6 pages.

[55] Alena Naiakshina, Anastasia Danilova, Christian Tiefenau, Marco Herzog, Sergej Dechand, and Matthew Smith. 2017. Why Do Developers Get

Password Storage Wrong? A Qualitative Usability Study. In ACM CCS. ACM, 311‚Äì328.

[56] NIST. 2001. FIPS PUB 197: Announcing the Advanced Encryption Standard (AES). US Department of Commerce.
[57] NIST. 2020. NIST Releases Draft Guidance on Internet of Things Device Cybersecurity. https://www.nist.gov/news-events/news/2020/12/nist-

releases-draft-guidance-internet-things-device-cybersecurity

[58] Online Trust Alliance (OTA). 2017. IoT Security & Privacy Trust Framework v2.5. https://www.internetsociety.org/wp-content/uploads/2018/05/

iot_trust_framework2.5a_EN.pdf

[59] Open Web Application Security Project (OWASP). 2010. OWASP Secure Coding Practices Quick Reference Guide. https://www.owasp.org/images/

0/08/OWASP_SCP_Quick_Reference_Guide_v2.pdf

[60] PlainTextOffenders.com. 2021. https://plaintextoffenders.com/.
[61] Jon Postel, Yakov Rekhter, and Tony Li. 1995. RFC 1818: Best Current Practices. IETF.
[62] PSA Certified. 2019. PSA Certified Level 1 Questionnaire. Critical security questions for chip vendors, OS providers and OEMs. https://www.

psacertified.org/app/uploads/2019/02/JSADEN001-PSA_Certified_Level_1-1.0Web.pdf

[63] Elissa M. Redmiles, Noel Warford, Amritha Jayanti, Aravind Koneru, Sean Kross, M. Morales, R. Stevens, and Michelle L. Mazurek. 2020. A

Comprehensive Quality Evaluation of Security and Privacy Advice on the Web. In USENIX Security Symp. USENIX, 89‚Äì108.

[64] Ron Ross, Michael McEvilley, and Janet Carrier Oren. 2016. SP 800-160 (Vol. 1)‚ÄîSystems Security Engineering: Considerations for a Multidisciplinary
Approach in the Engineering of Trustworthy Secure Systems. https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-160v1.pdf. NIST.

[65] Jerome H. Saltzer and Michael D. Schroeder. 1975. The Protection of Information in Computer Systems. Proc. IEEE 63, 9 (1975), 1278‚Äì1308.
[66] Behcet Sarikaya, Mohit Sethi, and Dan Garcia-Carrillo. 2019. Secure IoT Bootstrapping: A Survey. Internet Draft, draft-sarikaya-t2trg-sbootstrapping-

05.

[67] Adam Shostack and Andrew Stewart. 2008. The New School of Information Security. Pearson Education.

28

Barrera, Bellman, and van Oorschot

[68] Abhay Soorya and 21 others. 2018. IoT Security Compliance Framework 2.0. https://www.iotsecurityfoundation.org/wp-content/uploads/2018/12/

IoTSF-IoT-Security-Compliance-Framework-Release-2.0-December-2018.pdf

[69] Gary Stoneburner, Clark Hayden, and Alexis Feringa. 2004. SP 800-27 RevA‚ÄîEngineering Principles for Information Technology Security (A Baseline

for Achieving Security). NIST.

[70] Sven Schrecker and 14 others. 2016. Industrial Internet of Things Volume G4: Security Framework v1.0. https://www.iiconsortium.org/pdf/IIC_

PUB_G4_V1.00_PB-3.pdf

[71] David R. Thomas. 2006. A General Inductive Approach for Analyzing Qualitative Evaluation Data. American Journal of Evaluation 27, 2 (Jun 2006),

237‚Äì246.

[72] Hannes Tschofenig and Emmanuel Baccelli. 2019. Cyberphysical Security for the Masses: A Survey of the Internet Protocol Suite for Internet of

Things Security. IEEE Security & Privacy 17, 5 (Sep 2019), 47‚Äì57.

[73] UK Government, Department

Code of Practice for Consumer IoT Secu-
rity. https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/773867/Code_of_Practice_for_Consumer_
IoT_Security_October_2018.pdf.

for Digital, Culture, Media & Sport

(DCMS). 2018.

[74] UK Government, Department for Digital, Culture, Media & Sport (DCMS). 2018. Mapping of IoT Security Recommendations, Guidance
and Standards to the UK‚Äôs Code of Practice for Consumer IoT Security.
https://assets.publishing.service.gov.uk/government/uploads/
system/uploads/attachment_data/file/774438/Mapping_of_IoT__Security_Recommendations_Guidance_and_Standards_to_CoP_Oct_2018.pdf.
https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/774438/Mapping_of_IoT__Security_
Recommendations_Guidance_and_Standards_to_CoP_Oct_2018.pdf

[75] US National Telecommunications and Information Administration (NTIA). 2017. Voluntary Framework for Enhancing Update Process Security.

https://www.ntia.doc.gov/files/ntia/publications/ntia_iot_capabilities_oct31.pdf

[76] US Senate. 2017. Bill‚ÄîS.1691 - Internet of Things (IoT) Cybersecurity Improvement Act of 2017 (Bill). https://www.congress.gov/bill/115th-

congress/senate-bill/1691/text?format=txt.

[77] Felix Wortmann and Kristina Fl√ºchter. 2015. Internet of Things. Business & Information Systems Engineering 57 (2015), 221‚Äì224.

A DETAILED ANNOTATIONS FOR CODING TREE QUESTIONS

Items Q1‚ÄìQ11 below and the adjacent text are the detailed annotations for the coding tree questions, noted in Section 4.

These were consulted if a coder was unclear what a question was asking or for further details, to answer a question for

an advice item being coded.

Q1. Is the advice conveyed in unambiguous language, and relatively focused?

Does the advice make sense from a language perspective (e.g., it is a sentence that you can read and makes sense),

unambiguous (i.e., you can tell what they are trying to convey from a language perspective, not technical), and

not multiple items grouped into one piece of advice? Is the advice focused on one topic, whether it is a step to

take, an outcome to achieve, or security principle? If the advice seems to have multiple topics being discussed or

has multiple outcomes it wants an implementer to reach, this would be considered unfocused.

Q2. Is it arguably helpful for security?

Is the advice arguably useful for pursuing security in some way? Does it seem like it will help improve security

outcomes rather than processes unrelated to security?

Q3. Is it focused more on a desired outcome than how to achieve it?

Is the advice a high-level outcome rather than some method (or meta-outcome) for how to achieve an outcome?

E.g., data is secured in transit would be an outcome because it is a desired goal or state, whereas encrypt data in

transit is not because it explains a method for achieving that outcome (in this case, encryption). Encryption may

be considered a meta-outcome, as it is not meaningful to the end-user‚Äôs ultimate goal of protected data.

Q4. Does it suggest a security technique, mechanism, software tool, or specific rule?

Is the item a method used in achieving/following the advice? E.g., encryption or replacing a password with black

dots are techniques/mechanisms, but secure data or making the password unreadable are not. An example of a

Security Best Practices: A Critical Analysis Using IoT as a Case Study

29

specific rule: no hard-coded credentials‚Äîthis is a rule that is fairly specific as to its goal and would be followed

like a practice, but not necessarily with actionable steps.

Q5. Does it describe or imply steps or explicit actions to take?

Does the advice suggest actionable technical steps (one or more) that suffice to follow the advice? It has sufficient

detail to suggest a step/action to take.

Actionable: Involving a known, unambiguous sequence of steps, whose means of execution is generally under-

stood.

Q6. Is it viable to accomplish with reasonable resources?

Could the advice item be followed with an acceptable cost?. E.g., the advice would not take years to follow, or

have cost out of line with the anticipated benefit.

Q7. Is it intended that the end-user carry this advice out?

Does the item suggest that the end-user will be responsible for carrying out this practice? Note that end-users

first interact with devices after the Creation phase.

Q8. Is it intended that a security expert carry this item out?

Does following this advice require an expert understanding of security and security implementation in order to

properly follow the advice? Someone following this advice item would have to be an expert in security to be

able to understand it and successfully follow it, or be capable of extracting actionable steps from an otherwise

non-actionable item based on their experience.

Q9. Is it a general policy, general practice, or general procedure?

Is the item a security policy (general rule) to improve security, but is not explicit about what technical means

is used? These are less actionable (akin to incompletely specified practices‚Äîsee definition in Q5), and are not

technically explicit. A general policy often has more emphasis on what is (dis)allowed (or may be a general rule

closely related to a desired outcome), rather than on how to achieve it.

Q10. Is it a broad approach or security property?

Is the item a general way or general strategy, or a property that would improve security? A security property is

a characteristic or attribute of a system related to security. E.g., an open design.

Q11. Does it relate to a principle in the design?

Some principles relate to the core design phase of the product/system rather than later lifecycle phases.

