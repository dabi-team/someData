2
2
0
2

r
a

M
0
3

]

R
C
.
s
c
[

4
v
0
8
9
2
1
.
3
0
2
2
:
v
i
X
r
a

MERLIN - Malware Evasion with Reinforcement LearnINg

Tony Quertier∗,1, Benjamin Marais1,2, St´ephane Morucci1, and Bertrand Fournel1

1Orange Innovation, Rennes, France
2Department of Mathematics, LMNO, University of Caen Normandy, France

March 31, 2022

Abstract

In addition to signature-based and heuristics-based detection techniques, machine learning (ML) is
widely used to generalize to new, never-before-seen malicious software (malware). However, it has been
demonstrated that ML models can be fooled by tricking the classiﬁer into returning the incorrect label.
These studies, for instance, usually rely on a prediction score that is fragile to gradient-based attacks. In
the context of a more realistic situation where an attacker has very little information about the outputs
of a malware detection engine, modest evasion rates are achieved [1]. In this paper, we propose a method
using reinforcement learning with DQN and REINFORCE algorithms to challenge two state-of-the-art
ML-based detection engines (MalConv & EMBER) and a commercial antivirus (AV) classiﬁed by Gartner
as a leader AV [2]. Our method combines several actions, modifying a Windows portable execution (PE)
ﬁle without breaking its functionalities. Our method also identiﬁes which actions perform better and
compiles a detailed vulnerability report to help mitigate the evasion. We demonstrate that REINFORCE
achieves very good evasion rates even on a commercial AV with limited available information.

1

Introduction

Malicious software detection has become an important topic in business, as well as an important area of
research due to the ever-increasing number of successful attacks using malware. According to Sophos [3]
73% of ransomware attacks committed in 2020 were successful to encrypt computer ﬁles. AV-TEST [4]
estimates that about 450,000 new malware are discovered every day, and 93% of them are Windows
malicious ﬁles with a vast majority of portable executable (PE) ﬁles. Moreover, traditional signature-
based methods cannot keep up with the rampant inﬂation of novel malware [5]. Polymorphic malware use
a mutation engine with self-propagating code to continually change its signature. As many anti-malware
vendors use traditional signature-based detection methods to detect and block malicious code, it means
that by the time they identify a new signature, the malware has already evolved into something new.

From an oﬀense perspective, evading an antivirus (AV) is crucial for an attack to succeed. From a
defense perspective, being assured that an AV is resilient to evasion techniques is essential. The goal
of this paper is to address both challenges: making Windows PE malicious ﬁles undetected by AVs
while providing explanations on why such evasion is successful. Identifying the root cause of evasions
( “explainability”) is a great value for AV technical designers when enhancing their heuristic detection
modules.

We use a binary blackbox testing approach for attacking AVs according to coarse attacks classiﬁcation
suggested by Anderson and al in [1]: the system under attack is only required to report a malign or
benign result for a given PE ﬁle. This makes an attack the most generic one, with low a priori knowledge
requirements (neither the adversary model, the PE features space, nor the PE malware scores are required
to conduct our attacks). Our system generates a modiﬁed version (an adversarial malware example) of

∗tony.quertier@orange.com

1

 
 
 
 
 
 
a given PE ﬁle that leads to misclassiﬁcation by AVs. By design, our semantic-preserving modiﬁcations
do not corrupt the PE ﬁle substitute, i.e., the resulting evaded PE ﬁle keeps its malicious functionalities.
In this work, we are focusing on static malware detection solutions since they are often the only
security layers located on user devices and because such a static analysis is highly eﬃcient to support
large-scale analysis. This work shows that bypassing AVs can easily be automated with a sequence of
clearly deﬁned actions that may be of interest for AV designers when improving their detection engines.

1.1 Background and related work

Malware detection has become one of the top priorities of security actors since single incidences can cause
millions of dollars worth of damages [6]. The advances in the ﬁelds of artiﬁcial intelligence, ML, and
deep learning make it possible to improve malware detection, and classiﬁcation [7,8]. In particular, some
notable datasets have been made publicly available, such as Ember [9], SOREL-20M [10] or recently
BODMAS [11]. These open datasets motivate new works, help in resolving existing challenges, and are
very useful to benchmark new research proposals. ML-based classiﬁers can be designed and compared
to keep track of the progress made by the research community. In [9], Anderson et al. trained a feature-
based malware detection model using a non-optimized LightGBM algorithm, whereas Raﬀ et al. [12]
introduced MalConv, a featureless deep learning classiﬁer using a dense neural network processing raw
bytes of entire executable ﬁles.

Nevertheless, ML-based models may not be resilient to some attacks, namely “adversarial examples”
which were ﬁrstly introduced by Szegedy et al. [13] and formalized by Goodfellow et al. [14]. These
examples are speciﬁcally crafted to perturb detection or classiﬁcation models in a controlled way. The
principle was initially described for image classiﬁcation, but it was later generalized to other objects,
and malicious ﬁles are not an exception. Figure 1 illustrates how to generate an adversarial example by
adding some perturbations to malware. Several approaches have been proposed to generate undetectable
malicious ﬁles such as using DL techniques as GAN with MalGan [15,16] or MalFox [17]. Other adversarial
examples have already been described in literature [18–22].
In particular, Demetrio et al. [18] use a
particularly interesting method based on a constrained minimization problem while preserving executable
ﬁles functionalities.

Figure 1: Malware adversarial example illustrated by Huang et al. [23]

Anderson et al. [1] used reinforcement learning (RL) to automatically generate malicious ﬁles. They
release on GitHub an RL framework ( “Gym-Malware”) that achieves an evasion rate of 16% on 200
holdout samples when attacking a gradient boosted decision tree model trained on 100,000 malicious
and benign samples. Song et al. [5] propose an open-source RL framework called “MAB-malware”
also available on GitHub. Results show this framework can achieve a high evasion rate (over 75%) for
two state-of-the-art ML malware detectors (EMBER [9], and MalConv [12]) and over 32% to 48% for
commercial AVs in a pure black-box setting. Authors focus on re-implementing actions to transform
malware without breaking its functionality since they found that more than 60% of the transformed
binaries with Gym-Malware were corrupted. Fang et al. [24] propose a novel method for extracting PE ﬁle
features and build two models called DeepDetectNet (defense) and RLAttackNet (attack), which contest
with each other in the same way as Generative Adversarial Networks behave; they attack DeepDetectNet
with RLAttackNet and achieve a success attack rate of 19.13%. They retrain DeepDetectNet with these
new evaded malicious ﬁles, and the success attack rate dropped from 19.13% to 3.1%. One of their
conclusions is that ML-based models for malware detection can be improved using synthetically generated
malware.

2

1.2 Contributions

In this work, we ﬁrst propose an RL framework to evade malicious ﬁle static-detectors based on state-of-
the-art ML models. We begin by implementing a well-known RL model, Deep Q Network (DQN), and
train it to evade malware on diﬀerent models such as Malconv [12], LGBM Ember [9], and Grayscale [25].
The DQN model achieves very good results with Malconv and Grayscale with a respective evasion rate
of 100% and 98%. On Ember, its evasion rate reaches 67%, which motivated us to develop a better
technique using the REINFORCE algorithm [26]. To our knowledge, it is the ﬁrst time such an algorithm
has been used for malware evasion. We train REINFORCE against Ember, and our results show a slight
improvement over DQN with an increase of the evasion rate from 67% to 74.2% without any impact on
training time. We then challenge a well-known commercial AV. Once again, REINFORCE shows that it
performs better than DQN with a signiﬁcant increase of the evasion rate from 30% to 70%.

A key element of our work is our ability to compile a vulnerability report listing the most eﬃcient
actions to transform a malicious PE ﬁle and make it undetectable by the model under attack. In other
words, we can identify the detection model weaknesses and the most eﬀective actions to defeat a given
AV. Security experts can then leverage these insights to understand why a detection engine failed and
react accordingly.

Finally, our RL framework makes it possible to generate new malware variants and thus create a
database of never-before-seen malicious ﬁles. This database could be used as a preventive asset to
manage proactively potential malware variants.

1.3 Outline

In section 2, we introduce our RL approach. We describe the context, the agents, the environments and
the actions considered for evading an AV. In section 3, we present the results of the training and testing
phases. Next, in section 4, we analyze the eﬀectiveness of each agent and action for the diﬀerent detection
solutions by highlighting their weaknesses. Finally, section 5 summarizes our results and describes future
work.

2 Algorithms and RL framework

Our main objectives are (1) to bypass malware detection engines by generating undetected malicious
PE ﬁles using an RL approach and (2) to provide an automated audit highlighting the weaknesses of
a given AV. As shown in Figure 2, an RL framework consists of the interaction of an Agent with an
Environment. At a given time t, the Agent observes the state s of the Environment and performs an
Action a that modiﬁes it. The Environment returns a Reward r, which depends on the eﬀectiveness of
this Action. In section 2.1, we introduce the two agents that we have selected. Their job is to interact
with a given PE ﬁle to modify it and make it undetectable by an ML solution or a commercial AV. Then,
in section 2.2, we present environments that can output their states and rewards associated with two
kinds of detection engines: ML models and a commercial AV. Finally, we detail some possible actions
used to modify malicious ﬁles in section 2.3.

Figure 2: Reinforcement learning framework representation

3

2.1 RL agents

An agent is backed by diﬀerent types of RL algorithms. Its function is to interact with an environment,
modifying it in order to identify which actions perform best (see section 2.3). Two RL models have been
implemented at the agent side: Deep Q-Network (DQN) and REINFORCE. These two algorithms have
their own characteristics and are presented in the following subsections.

2.1.1 Deep Q-Network

The DQN algorithm, introduced by Mnih et al. [27], is a popular deep RL model. Based on Q-learning
algorithm [28], its main advantage is that it can handle numerous and large states. The purpose of Q-
learning algorithms and, by extension, Deep Q-learning algorithms is to predict the action that provides
the best reward. For that, these two models approximate the optimal action-value function Q∗(s, a).
In the case of Deep Q-learning, the action-value function is modeled by a neural network called Q-
Networks. Based on the Q-learning update equation, presented in Equation 1, we update the weights of
the Q-Network in an iterative way. After training our Q-network, our agent will return the best action a
associated with the best reward r, for a given state s. From a technical perspective, it is the best action
to make our malicious ﬁle undetectable by the targeted AV.

Q(st, at) ← (1 − α)Q(st, at) + α(r + γ max

a

Q(st+1, a))

(1)

2.1.2 REINFORCE

REINFORCE, also known as the Monte Carlo Policy Gradient method, was introduced by Williams
in [26]. It is another RL approach that consists in maximizing the expected returned J with respect to
the policy π, depending on parameters θ. For this, it is necessary to apply the gradient descent method
to the function

J(θ) = Eτ ∼πθ [

T
(cid:88)

t=0

Qπθ (st, at)].

The gradient of J(θ), with respect to parameters θ, is speciﬁed by

∇J(θ) = Eτ ∼πθ [

T
(cid:88)

t=0

∇θ log πθ(at|st)Qπθ (st, at)],

(2)

(3)

where τ is the trajectory, also known as episode. The trajectory consists of a sequence of states, actions
and rewards indexed by t ∈ [0, T ], and Q is the action-value function of the policy πθ. So, for each state
s, the REINFORCE agent returns a probability distribution, following the policy πθ, which is expected
to lead to the best expected reward. Figure 3 illustrates this concept by comparing two diﬀerent policies
applied to the same problem.

2.2 Environments

ML detection solutions are a convenient way to formalize our problem because they provide us with a
lot of useful information. For each of them, we have the features used to detect whether the scanned
ﬁle is malicious or not. These models also return a prediction score, or detection score, p ∈ [0, 1] where
the closer to 1, the more likely it is that the ﬁle is malicious. This score provides important information
during the RL process. We analyse two state-of-the-start detection engines (Ember [9] and Malconv, two
references in the research community, [12]) together with our own model “Grayscale” [25].

We also select a commercial AV since we hope that our work could help designers improve their
detection capabilities. Unlike ML models, commercial AVs are blackboxes and they usually return very
little information after a scan: a hard detection label l ∈ {0, 1}, 0 if the ﬁle is benign or 1 if it is malicious.
As a consequence, our environments must take into consideration the quantity of the available infor-

mation when using a detection tool. Table 1 summarizes this information.

4

Figure 3: Two diﬀerent policies [29]

Table 1: Knowledge available for each detection tool

Models
Original ﬁle
Extracted
features
Prediction score
Detection label

Ember
X
X

X
X

ML solutions

Commercial AV

Malconv Grayscale

X
X

X
X

X
X

X
X

X

X

Figure 4: Reinforcement learning framework representation with detailed environment

2.2.1 ML solution

The ﬁrst step is to design some environments with which the agents can interact. These environments
must be able to:

5

1. generate a new episode i.e. pick a new malicious ﬁle to work on;

2. apply a number of modiﬁcations (i.e. the actions at selected by our agents) on the malicious ﬁle;

3. for any modiﬁcation, return:

• a reward rt+1 depending on the eﬀectiveness of the action,
• some features, extracted from the new PE ﬁle, corresponding to the state st+1.

Table 2 summarizes the diﬀerent characteristics of the detection models. To make our approach
generic, we create a dedicated environment for each one. As shown in ﬁgure 4, each environment contains
a list of malicious PE ﬁles required for the training and testing phases and the detection model that the
agent must bypass.

We also need to deﬁne the reward function were the more eﬀective an action, the greater the reward.

This function is given below:

For each iteration t ∈ [0, T ], we set

rt := r(pt, pt−1) =

(cid:40)

R ∈ R
pt − pt−1

if pt < φ
otherwise

(4)

where pt is the detection score returned by the detection model, φ ∈ R is a threshold depending on

the model (see Table 2) and R is an hyperparameter chosen according to our tests.

Table 2: ML detection models summary

Type of model
Type of data

Ember
LGBM
Features extract
from PE ﬁles [9]

Size of state1
Threshold φ

2381
0.8336

Malconv
DNN
Raw bytes
extract from PE
ﬁles [12]
≈1M
0.50

Grayscale
CNN
PE ﬁles
converted into
images [30]
64 × 64
0.50

2.2.2 Commercial AV

The main challenge in creating a suitable environment for the commercial AV is related to a lack of
information. We only have at our disposal the PE malicious ﬁle, which does not provide any proper
observable space, i.e., a state s. To get around this limitation, the missing state s is replaced by some
features of the PE ﬁle. These characteristics are returned by Ember features extractor, but any other
solution could also be used if necessary. The key here is to have a features vector that models states to
apply RL algorithms on the AV.

Another problem is that the commercial AV does not return a score p ∈ [0, 1], as for ML solutions,
but a hard label l ∈ {0, 1}. As a consequence, the reward function in equation 4 does not apply. We
need to deﬁne a new reward function dedicated to commercial AV as follows:

for each iteration t ∈ [0, T ], we set

rt := r(lt) =

(cid:40)

R ∈ R
0

if lt = 0
otherwise

(5)

where lt is the label returned by commercial AV.

1corresponding to the size of the features vector extracted from PE ﬁle

6

Table 3: Action table. Actions are 0-indexed

Action
modify machine type
pad overlay
append benign data overlay
append benign binary overlay
add bytes to section cave
add section strings
add section benign data
add strings to overlay
add imports
rename section
remove debug
modify optional header
modify timestamp
break optional header checksum
upx unpack
upx pack

Id
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

2.3 Actions

In this section, we are listing the actions that are applied to modify a given PE ﬁle, with the constraint
of not corrupting its functionalities. Among other things, we use the actions introduced by Anderson et
al. [1]. They are listed in Table 3. In particular, these actions include:

• packing and unpacking the malicious ﬁle,

• adding benign strings to the end of sections,

• adding random bytes to the unused section space,

• adding import functions,

• modifying PE ﬁles timestamp.

Because PE ﬁle modiﬁcations must be coherent and legitimate, we have previously constructed a
database containing relevant information about 5000 benign ﬁles (section names, imports, section strings,
etc.). When speciﬁc content is required by an action (like an import, the adding section or strings, etc.),
it is randomly picked from this database. While testing, we reﬁned and reduced the size of this database,
keeping only the elements that were the most eﬃcient in order to limit the action space of our agent.

LIEF [31] is used to apply actions and make modiﬁcations on a PE ﬁle, but, as mentioned by Song et
al. in [5], this library may sometimes break PE functionalities. To circumvent this problem, Song et al.
analyzed their samples in the Cuckoo sandbox. During our RL process, we follow a similar method: if the
ﬁle is broken after some modiﬁcations performed by the agent, we mark the evasion as failed and start
again. When an evasion is successful, we validate that the malware functionalities have not changed by
checking with an oﬀ-the-shelf sandbox and with an internal custom-built one where all system calls are
logged, making it possible to compare execution ﬂows of both modiﬁed and original ﬁles. We also verify
on ANY.RUN that the behavior graphs of the modiﬁed and the original are similar. Such a behavior
graph is presented in ﬁgure 5 and shows the graph of a modiﬁed Wannacry malware.

In the next section, we detail our results with a particular focus on the eﬀectiveness of each action

during the training and testing phases.

3 Experiments

During our ﬁrst experiments, we noticed that malware from diﬀerent open datasets were not equal from
an evasion perspective: some were quite simple to evade, whereas others from the BODMAS Malware

7

Figure 5: Behavior graph of the modiﬁed Wannacry malware from malware sandbox ANY.RUN

Dataset [11] (created and maintained by Blue Hexagon and UIUC) led to some evasion diﬃculties. We
suppose that these diﬀerences may be related to the age of the malware: the BODMAS dataset is quite
recent as it contains 57,293 malware samples collected from August 2019 to September 2020. In this
paper, we decided to rely only on this dataset for malware selection.

As mentioned in the previous section, the results provided by ML solutions and a commercial AV are
quite diﬀerent (a scoring label vs. a hard label). Moreover, our commercial AV has signiﬁcantly higher
response times than ML solutions. As a consequence, we separate our experiments: for ML solutions
(Malconv, Grayscale, and Ember), we use 1,000 malicious ﬁles for training and 500 malicious ﬁles for
testing. For the commercial AV experiment and mainly due to its response times, we use only 100
malware for training and 50 malware for testing.

3.1 Malconv Evasion

We begin by training the DQN agent to evade Malconv. Results in ﬁgure 6 display the resulting cumu-
lative scores when using a given action (left hatched bar) and the number of times such an action has
been applied (right dotted bar). Since we rely thoroughly on this kind of visualization in the rest of this
article, we are detailing hereafter how it can be interpreted. In ﬁgure 6, the action “modify machine
type”, indexed by 0, is used about 100 times and it does not generate any score. This information is
useful in itself even if the score is null because it shows that this action is useless during training wherever
it has been applied (we are combining actions into a sequence to evade an AV and the positions in this
sequence where this action is applied may inﬂuence the evasion score diﬀerently). The so-called “add
section strings” action, indexed by 5, is the most used and produces a high cumulative score, which
means that this action is really eﬃcient to bypass Malconv. We can also notice that action 6 (“add
section benign data”) has an interesting ratio between its number of usage and its cumulative score.
Since Malconv is a Natural Language Processing (NLP)-based DL algorithm, it is, therefore, no surprise
that Malconv is not very robust to strings injection. Our agent reaches an evasion rate of 96% against
Malconv by identifying its main weakness (being the “add section strings”).

During the testing phase, the agent only uses action 5 to achieve an evasion rate of 100%. We do
not discuss the Malconv detection algorithm any further since this result shows that Malconv is easy
to evade with just one action, and some articles have already detailed its strengths and weaknesses as
in [32–34] for example.

8

Figure 6: Cumulative score (left) and number of action (right)
Training of DQN agent on Malconv

3.2 Grayscale Evasion

Grayscale detection algorithm [25] relies on transforming binary PE ﬁles into grayscale images and
detecting malware using a trained CNN (Convolutional Neural Network) model. The evasion rate during
the learning phase is 90%. Unlike Malconv where the agent overﬁts by relying on just one action, this
grayscale-based agent uses a combination of actions. The average number of steps to evade malware is
about 10 for the 1, 000 malware samples. Moreover, as we can see in ﬁgure 7, several hatched bars lead
to positive cumulative scores, meaning that the associated actions of evading the AV are quite diversiﬁed
(actions {2,3,6,7}), which is an advantage as suggested later in this paper. During the testing phase, the
agent achieves an evasion rate of 98% with the actions identiﬁed during the training phase.

As with Malconv, the Grayscale model is easy to bypass mainly due to the fact that it uses images

representation of malicious ﬁles, which can be assimilated to ﬁle-signatures.

9

Figure 7: Cumulative score (left) and number of action (right)
Training of DQN agent on Grayscale

3.3 Ember LGBM Evasion

Ember [9] is a LGBM model trained on the Ember dataset which contains one million malicious ﬁles.
Due to this large sample size, we assumed that Ember would be harder to evade. We ﬁrst trained the
DQN agent against Ember and then against the REINFORCE agent.

3.3.1 DQN algorithm

The DQN agent reaches an evasion rate of 40% during the training phase. As assumed, Ember is more
robust when compared to Malconv or Grayscale. Moreover, the average number of steps to make a
malicious ﬁle undetectable is higher and reaches 30, versus only 10 on average for Malconv. This shows
that the DQN agent has some diﬃculties in learning what the best required actions to evade are. In
ﬁgure 8 we notice that action 5 is the action that produces the best cumulative score. To a lesser degree,
some actions like, for example, “upx pack”, indexed by 15, have some impact and may be considered.
Since this action has a higher relative impact than on Malconv and Grayscale, one could infer that their
detection techniques are diﬀerent.

10

Figure 8: Cumulative score (left) and number of action (right)
Training of DQN agent on Ember

These results are conﬁrmed during the testing phase. The agent only uses action 5 until the malware
is no longer detected by Ember. As for Malconv, DQN tends to overﬁt by relying on just one action.
It reaches an evasion rate of 67% for an average number of steps of 8. Results are better than during
the training phase because the agent has already learned and found one Ember weakness ( “add section
strings”). Nevertheless, results are not as good as for Malconv and Grayscale.

3.3.2 Reinforce algorithm

During training, the REINFORCE agent reaches an evasion rate of 54.7% with 8 steps on average. At
ﬁrst sight, the REINFORCE agent performs better than the DQN agent (DQN evasion rate is 40%). As
shown in ﬁgure 9, actions {5, 7} are largely dominant, but we can see that eﬀective actions are more
diversiﬁed, and their cumulative scores are better than during the training phase of the DQN agent. As
with Malconv, adding strings in the binary ﬁle is an eﬀective technique to perturb Ember. This lack
of robustness is consistent with the study of Oyama et al. [35] which explains that only a few features
contribute to explain Ember’s results.

Test results are also better with the REINFORCE agent than with the DQN agent, with a slight

11

Figure 9: Cumulative score (left) and number of action (right)
Training of REINFORCE agent on Ember

increase of the evasion rate from 67% (DQN) to 80% (REINFORCE) and an average number of steps of
7.5 (8 for DQN). We also observe in ﬁgure 10 that not only action 5 perturbs Ember, but also actions
{0, 7, 12}.

3.4 Commercial AV

As explained in section 2.2, an important limitation of our commercial AV is that it only returns a hard
label (0 or 1) when scanning a ﬁle. Therefore, when our agents try to bypass it, the real impact and
contribution of each action are much more diﬃcult to assess than with an ML model. The following
sections present our results with a commercial AV when training the DQN and the REINFORCE agents.

3.4.1 DQN algorithm

The DQN agent reaches an evasion rate of 61% in 9 steps, on average, during the training phase.
Nevertheless, as for Ember, we observe in ﬁgure 11 that action 8 is dominant and returns the best
cumulative score, even if some other actions are worth considering since they output non-zero rewards.

12

Figure 10: Cumulative score (left) and number of actions (right)
Testing of REINFORCE agent on Ember

As a reminder, for each episode, only the last action of the sequence gets a non-zero score due to the
hard labeling of the commercial AV.

As for Ember, during the testing phase, only the action that performed best during the training phase
is used. But unlike Ember, the DQN agent does not perform well with an evasion rate of 30%. This
poor performance is due to the fact that we no longer have a score when scanning a modiﬁed PE ﬁle.
The algorithm learned that only action 8 is useful to evade malware, but this result can be erroneous:
let us imagine that the action applied before action 8 did most of the evasion job and that the last
action (action 8) just terminates the RL job with a very limited impact on the internal (and unavailable)
commercial AV prediction score. In this situation, only the last action will get the reward instead of the
previous legitimate action. With Ember, the algorithm showed that a single action contributed greatly
to the drop in detection score, but this is not the case here with the commercial AV. The low evasion
rate is due to the fact that DQN agent focuses only on the action that evades the malware (because we
have a label and not a score as with Ember) and not on those before.

13

Figure 11: Cumulative score (left) and number of action (right)
Training of DQN agent on AV

3.4.2 REINFORCE algorithm

In section 3.3 we show that the REINFORCE agent reaches a high evasion rate using more diversiﬁed
actions. Unlike DQN, it has the advantage of not focusing on a single action. With a commercial AV,
the REINFORCE algorithm reaches an evasion rate of 63% during training, which is close to the results
of the DQN agent (61%). Also, the most impactful action is action 8, demonstrating that adding new
imports to a malicious ﬁle is the best way to bypass this commercial AV.

During the test, the REINFORCE agent uses not only action 8 but also some other actions that have
positive cumulative scores. During the testing phase, the agent reaches an evasion rate of 70% for an
average number of steps of 8. As in the case of Ember, the REINFORCE agent performs better than
the DQN agent in terms of evasion success.

We argue that the REINFORCE algorithm outperforms other algorithms and that the identiﬁcation

of diversiﬁed actions is an additional and strong advantage of this agent.

14

Figure 12: Cumulative score (left) and number of action (right)
Training of REINFORCE agent on AV

4 Analysis and explainability of results

In this section, we ﬁrstly discuss and summarize the results of the experiments. Then, we present our so-
called “vulnerability report” which is a complementary tool to the charts presented in section 3. Indeed,
besides identifying which actions perform better, we can analyze what they actually did, what are the
relevant imports for each algorithm, which strings have the most inﬂuence on the detection score, etc.
We can also visualize which parts of the binary have been modiﬁed by a given action by transforming
the malware into an image. Such insights are key elements when improving malware detection engines.
Finally, we provide examples of malware evasion with concrete cases.

4.1 Summary of the results

Table 4 summarises the evasion rates of each agent for each detection solution during the tests. We also
add a random agent, who selects actions randomly according to a uniform distribution, to compare the
results. The REINFORCE agent outperforms every other agent on Ember and a commercial AV.

15

Figure 13: Cumulative score (left) and number of action (right)
Testing of REINFORCE agent on AV

Table 5 presents the average number of steps to evade a malicious ﬁle. Except for the commercial
AV, REINFORCE is slightly more eﬃcient. Nevertheless, it takes only 3 more steps on average for a
major increase in the evasion rate.

One possible explanation for the eﬃciency of REINFORCE is that it learns sequences of actions and
not just the best action at a given time t. As a reminder, REINFORCE uses a policy to determine which
actions are most appropriate to successfully bypass a targeted model, as illustrated in ﬁgure 3. Indeed,
as we can see in the ﬁgure 13, the agent uses diﬀerent actions that do not provide a positive reward.
Nevertheless, even if the last action of each episode gets a positive reward when the evasion is successful,
the other actions appear to be useful to achieve this goal.

4.2 Explainability method

Explainability is a key advantage when dealing with ML results, especially in critical ﬁelds of application
In addition to bypassing the detection solution, we introduced an
like Cybersecurity, for instance.

16

Table 4: Evasion rate of each algorithm during testing phase

DQN
REINFORCE
Random

Ember
67%
80%
25%

ML solutions

Malconv Grayscale

100%
100%
76%

98%
100%
90%

Commercial AV
commercial AV
30%
70%
50%

Table 5: Average number of step to evade malicious ﬁles for each agent

DQN
REINFORCE
Random

Ember
8
7.5
7

ML solutions

Malconv Grayscale

9.5
8
20

10
9
14

Commercial AV
commercial AV
5
8
12

additional component to record all modiﬁcations done to a PE ﬁle to make it undetectable. The purpose
is to understand (explain) how an action transforms the binary and why this change allows bypassing
the detection solution.

Some actions add information to the binary ﬁle like actions {5, 6, 7, 8}, while others modify some
parts of the ﬁle like actions {9, 11, 12}. Since action 8 “add imports” for example, randomly picks a
library in a predeﬁned set, it does not generate the same reward. By recording the diﬀerent impact events
related to each distinct import, it is now possible to determine which imports are the more eﬀective. This
kind of information is very important for security analysts to help them improve their detection engines
and locate potential weaknesses.

Figure 14 illustrates how we store the relevant information, i.e., the actions, the states, and the
rewards, after modifying the PE ﬁle in a vulnerability report. Any other useful information is also
recorded in this report. For example, if the agent applies action 8, we add to the report the speciﬁc
import that has been selected.

Figure 14: Reinforcement learning framework with the “vulnerability report” component

This report allows us to keep track of all changes that make malware undetectable. Also, it comple-
ments the information provided by all our visualization tools about the eﬀectiveness of an action. The
visualization tool and the report could give cybersecurity experts a turnkey solution to enhance detection

17

solutions by ﬁxing their weaknesses. It is a preventive way to reduce cyber threats.

4.3 Malware evasion insights

In this section, we describe a malware evasion on a commercial AV with the REINFORCE agent. As we
can see in ﬁgure 15, the REINFORCE agent generates an undetectable malicious ﬁle in only four steps.
The sequence is composed of the following actions {8, 8, 0, 6}. This ﬁgure also presents information
extracted from the vulnerability report with the modiﬁcations and insertions made to the PE ﬁle. Two
benign libraries are added with action 8 {msvcrl20.dll, gdi32.dll} and action 0 changes the machine
type into AMD64. Then, the strings added with action 6 are extracted from a benign ﬁle named
“vsiXinstaller.txt”

Figure 15: Extract of the vulnerability report where four steps are required to evade a malware

Figure 16 shows the details of the evasion of a second malicious ﬁle. Here, the agent takes only
three steps to make the ﬁle undetectable. The required actions are {12, 0, 8}. Action 12 modiﬁes the
timestamps to “993636360” and action 0 changes the machine type to ARM64. Finally, action 8 adds
the library “rpcrt4.dll”.

Figure 16: Extract of the vulnerability report where three steps are required to evade a malware

We notice that the action “modify machine type” is used a few times in the test but has a zero
cumulative score. However, in our two examples, the action is used as the second last action, so we can
think it is signiﬁcant in the evasion. This is a strength of REINFORCE compared to DQN, which learns
action sequences and not only the optimal action. In the future, it will be interesting to isolate some
actions, to really understand their impact on the evasion of the malware.

5 Conclusion and future works

5.1 Conclusion

In this paper, we have analysed diﬀerent agents capable of evading several malware detection engines.
Our prototype has the advantages of not requiring any prior knowledge about the detection model
and having a high evasion rate even on commercial AV, thanks to the use of REINFORCE algorithm.
Our agents perform better than the random agent except on the commercial AV where the random
agent is surprisingly better than DQN. This shows a limitation of DQN when we have only a hard
label because DQN does not analyze the sequence of actions but only the last action that leads to an
evasion. The conclusion of these results is that the DQN algorithm can be very eﬀective in identifying
the most impactful actions but has some strong limitations in black-box settings. On the contrary, the
REINFORCE algorithm is more versatile as it gives very good results in all situations. In addition to this,
we are able to generate a useful vulnerability report to help security analysts mitigate an evasion. The
prototype can also be used to generate new datasets of undetectable malware to re-train ML detection
models. We believe that this work will improve malware detection tools in the future and strengthen
antivirus software by providing analysts with vulnerability reports. We hope that this article will be able
to contribute to the security of all in our ﬁght against malware.

18

5.2 Future works

We aim to improve the technology presented here in several ways :

• optimizing the current agents, for example by training on particular malware families, like trojans

or ransomware, because they are more widespread and more dangerous;

• implementing other RL algorithms type actor-critic (A2C, A3C [36]) to analyse their eﬃciency;
• in relation with defense, training, or re-training detection models with the new malicious ﬁles

generated to determine if this increases their robustness and eﬃciency.

5.3 Code and database

Despite the potential beneﬁts of our work for strengthening AV defenses, we do not provide any open-
access to our prototype since it may be used by malicious actors. The prototype is highly eﬀective on
every commercial AVs we have evaluated it on, so we believe that the risks of sharing our code exceed
the possible beneﬁts. However, we are open to dialogue with AV vendors to share our ﬁndings and to
envision collaboration in the context of malware detection enhancement.

5.4 Acknowledgments

We want to thank the following colleagues Daniel Juteau, Philippe Calvet, Sok-yen Loui and Adam
Ouorou. We are very grateful to BODMAS team for their valuable dataset [11].

19

References

[1] H. S Anderson, Bobby Filar, and Phil Roth. Evading Machine Learning Malware Detection. Black-

Hat DC, page 6, 2017.

[2] Magic quadrant for endpoint protection platforms. https://www.microsoft.com/security/blog/

2021/05/11/gartner-names-microsoft-a-leader-in-the-2021-endpoint-protection-platforms-magic-quadrant/.
Accessed: 2021-12-29.

[3] Sophos Ltd. The State of Ransomware: Results of an independent study of 5,000 IT managers

across 26 countries. White Paper May 2020, (Abingdon, England):1–19, 2020.

[4] Av-test. https://www.av-test.org/fr/?r=1. Accessed: 2021-09-30.

[5] Wei Song, Xuezixiang Li, Sadia Afroz, Deepali Garg, Dmitry Kuznetsov, and Heng Yin. Mab-
malware: A reinforcement learning framework for attacking static malware classiﬁers. arXiv preprint
arXiv:2003.03100, 2020.

[6] Ross Anderson, Chris Barton, Rainer B¨ohme, Richard Clayton, Michel JG van Eeten, Michael
Levi, Tyler Moore, and Stefan Savage. Measuring the cost of cybercrime. In The Economics of
Information Security and Privacy, pages 265–300. 2013.

[7] Edward Raﬀ and Charles Nicholas. A Survey of Machine Learning Methods and Challenges for

Windows Malware Classiﬁcation. 2020.

[8] Daniele Ucci, Leonardo Aniello, and Roberto Baldoni. Survey of machine learning techniques for

malware analysis. Computers and Security, 81:123–147, 2019.

[9] H. S. Anderson and P. Roth. EMBER: An Open Dataset for Training Static PE Malware Machine

Learning Models. ArXiv e-prints, April 2018.

[10] Richard Harang and Ethan M. Rudd. SOREL-20M: A Large Scale Benchmark Dataset for Malicious

PE Detection. 2020.

[11] Limin Yang, Arridhana Ciptadi, Ihar Laziuk, Ali Ahmadzadeh, and Gang Wang. Bodmas: An open
In 4th Deep Learning and Security

dataset for learning based temporal analysis of pe malware.
Workshop, 2021.

[12] Edward Raﬀ, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, and Charles Nicholas.

Malware detection by eating a whole EXE. arXiv, 2017.

[13] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,

and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.

[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.

[15] Weiwei Hu and Ying Tan. Generating Adversarial Malware Examples for Black-Box Attacks Based

on GAN. 2017.

[16] Masataka Kawai, Kaoru Ota, and Mianxing Dong.

Improved MalGAN: Avoiding Malware De-
tector by Leaning Cleanware Features. 1st International Conference on Artiﬁcial Intelligence in
Information and Communication, ICAIIC 2019, pages 40–45, 2019.

[17] Fangtian Zhong, Xiuzhen Cheng, Dongxiao Yu, Bei Gong, Shuaiwen Song, and Jiguo Yu. Mal-
Fox: Camouﬂaged Adversarial Malware Example Generation Based on C-GANs Against Black-Box
Detectors. pages 1–14, 2020.

[18] Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, and Alessandro Armando.
Functionality-preserving black-box optimization of adversarial windows malware. IEEE Transac-
tions on Information Forensics and Security, 16:3469–3478, 2021.

[19] Zixiao Kong, Jingfeng Xue, Yong Wang, Lu Huang, Zequn Niu, and Feng Li. A Survey on Adversarial

Attack in the Age of Artiﬁcial Intelligence. 2021.

[20] Daniel Park and B¨ulent Yener. A survey on practical adversarial examples for malware classiﬁers,

volume 1. Association for Computing Machinery, 2020.

20

[21] Kshitiz Aryal, Maanak Gupta, and Mahmoud Abdelsalam. A Survey on Adversarial Attacks for

Malware Analysis. 2021.

[22] Luca Demetrio, Scott E. Coull, Battista Biggio, Giovanni Lagorio, Alessandro Armando, and Fabio
Roli. Adversarial EXEmples: A Survey and Experimental Evaluation of Practical Attacks on Ma-
chine Learning for Windows Malware Detection. ACM Transactions on Privacy and Security, 24(4),
2021.

[23] Yonghong Huang, Utkarsh Verma, Celeste Fralick, Gabriel Infantec-Lopez, Brajesh Kumar, and
Carl Woodward. Malware evasion attack and defense. In Proceedings - 49th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks Workshop, DSN-W 2019, pages
34–38, 2019.

[24] Yong Fang, Yuetian Zeng, Beibei Li, Liang Liu, and Lei Zhang. DeepDetectNet vs RLAttackNet: An
adversarial method to improve deep learningbased static malware detection model, volume 15. 2020.

[25] Benjamin Marais, Tony Quertier, and Christophe Chesneau. Malware analysis with artiﬁcial in-
telligence and a particular attention on results interpretability.
In Kenji Matsui, Sigeru Omatu,
Tan Yigitcanlar, and Sara Rodr´ıguez Gonz´alez, editors, Distributed Computing and Artiﬁcial Intel-
ligence, Volume 1: 18th International Conference, pages 43–55, Cham, 2022. Springer International
Publishing.

[26] Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforce-

ment Learning. Machine Learning, 8(3):229–256, 1992.

[27] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-

stra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning. pages 1–9, 2013.

[28] Christopher J C H Watkins and Peter Dayan. Q-learning. 8:279–292, 1992.

[29] Noufal

kvs.

Reinforce

algorithm:

Taking baby

steps

in reinforcement

learning.

https://medium.com/analytics-vidhya/reinforce-algorithm-taking-baby-steps-in-reinforcement-
learning-994b2bf46b0e.

[30] L Nataraj, S Karthikeyan, G Jacob, and B S Manjunath. Malware images: Visualization and

automatic classiﬁcation. In ACM International Conference Proceeding Series, 2011.

[31] Romain Thomas. Lief - library to instrument executable formats. https://lief.quarkslab.com/, April

2017.

[32] William Fleshman, Edward Raﬀ, Jared Sylvester, Steven Forsyth, and Mark McLean. Non-negative

networks against adversarial attacks. arXiv preprint arXiv:1806.06108, 2018.

[33] Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia
Eckert, and Fabio Roli. Adversarial malware binaries: Evading deep learning for malware detection
in executables. European Signal Processing Conference, 2018-Septe:533–537, 2018.

[34] Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, and Alessandro Armando. Explaining
vulnerabilities of deep learning to adversarial malware binaries. CEUR Workshop Proceedings, 2315,
2019.

[35] Yoshihiro Oyama, Takumi Miyashita, and Hirotaka Kokubo. Identifying useful features for mal-
ware detection in the ember dataset. In 2019 seventh international symposium on computing and
networking workshops (CANDARW), pages 360–366. IEEE, 2019.

[36] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learn-
ing. In International conference on machine learning, pages 1928–1937. PMLR, 2016.

21

