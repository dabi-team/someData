2
2
0
2

r
p
A
8
2

]

G
L
.
s
c
[

1
v
1
9
2
3
1
.
4
0
2
2
:
v
i
X
r
a

A Decision Model for Federated Learning
Architecture Pattern Selection

Sin Kit Lo1,2[0000−0002−9156−3225], Qinghua Lu1,2[0000−0002−7783−5183],
Hye-Young Paik2[0000−0003−4425−7388], and Liming Zhu1,2[0000−0001−5839−3765]

1 Data61, CSIRO, Sydney, Australia
2 University of New South Wales, Sydney, Australia

Abstract. Federated learning is growing fast in both academia and in-
dustry to resolve data hungriness and privacy issues in machine learn-
ing. A federated learning system being widely distributed with diﬀerent
components and stakeholders requires software system design thinking.
For instance, multiple patterns and tactics have been summarised by re-
searchers that cover various aspects, from client management, training
conﬁguration, model deployment, etc. However, the multitude of patterns
leaves the designers confused about when and which pattern to adopt or
adapt. Therefore, in this paper, we present a set of decision models to
assist designers and architects who have limited knowledge in federated
learning, in selecting architectural patterns for federated learning archi-
tecture design. Each decision model maps functional and non-functional
requirements of federated learning systems to a set of patterns. we also
clarify the trade-oﬀs that may be implicit in the patterns. We evaluated
the decision model through a set of interviews with practitioners to assess
the correctness and usefulness in guiding the architecture design process
through various design decision options.

Keywords: Software architecture · Federated learning · Decision models
· Pattern · Machine learning · Artiﬁcial Intelligence.

1

Introduction

The expansion of industrial IoT, smartphones and the internet [9] resulted in the
exponential growth in data dimensions, which in turn accelerated the adoption
of machine learning for data analysis in multiple areas. However, many ma-
chine learning systems suﬀer from insuﬃcient training data due to data privacy
concerns. Data privacy as an important ethical principles of machine learning
systems [6] induced the regularisation of the access of privacy-sensitive data. For
instance, the General Data Protection Regulation (GDPR)3 stipulates a range of
data protection measures that limits the ability of machine learning applications
to obtain data for training models. Furthermore, trustworthy AI has become
an emerging topic lately due to the new ethical, legal, social, and technological
challenges brought on by the technology [18].

3 https://gdpr-info.eu/

 
 
 
 
 
 
2

SK. Lo et al.

Federated learning [15] was introduced by Google in 2017 as an approach
to solve the limited training data and data sharing restriction challenges. It
addresses not only the data privacy issue but also the high communication
costs. However, a federated learning system, as a large-scale distributed system,
presents more architectural design challenges [12], especially when dealing with
the interactions between the central server and client devices and understanding
and managing the tradeoﬀs amongst the software quality attributes.

In our previous work, we conducted a systematic literature review (SLR) on
federated learning systems from the software engineering perspectives [12] to
articulate the diﬀerent software architectural challenges and propose approaches
to tackle the challenges. We have summarised a set of software architectural
patterns [13] to address the diﬀerent requirements from diﬀerent research arti-
cles and industrial practices. Despite having a collection of patterns, designers
and architects may ﬁnd it diﬃcult to choose when and how to use a pattern.
Hence, We aim to provide a method to structure the patterns to assist architects
in selecting appropriate patterns during the federated learning system design.
This paper as an extension of our previous work, introduces a set of federated
learning architectural patterns selection decision models. The goal is to provide
guidance for federated learning architecture design decisions that meet the in-
tended requirements while taking tradeoﬀs and constraints into considerations.
The contribution of the paper are as follows. We propose:

– A process ﬂow that covers four high-level design decisions for a federated

learning system.

– Patterns selection decision models that map the functional and non-functional
requirements of federated learning architecture to a set of patterns and the
dependencies between patterns.

The remainder of the paper is organised as follows: Sec. 2 presents the design
methodology for our decision models. Sec. 3 shows an overview of the proposed
decision models. Sec. 4 to Sec. 7 elaborate the 4 decision models for diﬀerent
aspects of architecture design. The evaluation of the decision models is presented
in Sec. 8. We discuss the limitation to validity of this research in Sec. 9. Sec. 10
covers the related work on federated learning architectural patterns and decision
models. Finally, Sec. 11 concludes the paper.

2 Decision Models Design Methodology and Notations

To design a decision model that elicits the functional and non-functional require-
ments with the respective patterns, we need to map the elements of the problem
space to the elements of the solution space. The problem space can be presented
as a set of functional (FR) or non-functional requirements (NFR), whereas the
solution space as a set of patterns targeting to solve the problems. We have
adopted the decision model design methodology from [8] and [19], the adopted
the notation method from [8] that involves the mapping of requirements and
patterns, as shown in Fig. 1. The elements of the problem and solution spaces

A Decision Model for Federated Learning Architecture Pattern Selection

3

Fig. 1: Decision model notations

are created based on the categories in [13]. An single-headed arrow from the
pattern to the requirement indicates that the pattern satisﬁes the requirement.
All pattern decisions will have beneﬁts which are indicated by a plus sign (+)
and tradeoﬀs which are indicated by a minus sign (-).

To present patterns combination, an double-headed arrow is used to point
from one pattern to another pattern. It signiﬁes that the originating pattern
complements the targeted pattern, with the label [complements]. In the case
of showcasing one pattern being an alternative to another pattern, we use a
double-headed arrow with the label [alternatives] to represent their relationship.
When a pattern complements another pattern it means that the initial pattern
is required. Therefore, the qualities of using the initial pattern also apply to
the combination of the patterns. If a system quality is associated to both the
initial and the complementary pattern but with a diﬀerent qualiﬁcation, the
qualiﬁcation of the complementary pattern overrides the qualiﬁcation of the
initial pattern. If there are conditions or constraints to the adoption of a pattern
to satisfy certain requirements, it is represented in a trapezium with a dashed
line connected to the respective pattern.

3 Design Model Overview

Fig. 2 presents an overview of the federated learning process. We can observe
that a central server needs to manage multiple client devices and the models
being received and transferred out. Moreover, the model training and aggregation
process is also a key design concern. Finally, the training process needs to be
conﬁgurable by the system owner. Hence, we have categorised 4 main design
decisions of a federated learning system, as shown in Fig. 3. The followings are
the high-level decisions:

– Client management patterns selection decision
– Model management & conﬁguration patterns selection decision

RequirementsFR FunctionalRequirementPattern 1Pattern 2+ Quality 1 - Tradeoff 1 PatternsNFR Non-FunctionalRequirementConstraintsatisfies+ Quality 2 - Tradeoff 2 satisfies+ Quality 2 - Tradeoff 2 [complements]Pattern 3[alternatives]+ Quality 3 - Tradeoff 3 PatternConstraintRequirement4

SK. Lo et al.

Fig. 2: Federated Learning Overview [11]

– Model aggregation patterns selection decision
– Model training patterns selection decision

The order of initiating a high-level decision begins from client management
decisions and model management & system conﬁguration decisions that can be
performed in parallel, followed by the model aggregation and training decisions.
During each lower level decision within the high-level decisions, the designers
need to consider how each quality is positively or negatively aﬀected by another.
Whenever a conﬂict exists, the designers should analyze the previous decisions
made and look for additional or alternative patterns to address the shortcomings,
according to their design requirements.

4 Client Management Decision Model

The client management decision model covers the design decisions that involve
the management of client devices information, the connection between client
devices and the central server, and the selection of client devices for the train-
ing process, as shown in Fig. 4. Federated learning systems require the central
server to interact with a large number of client devices to perform model train-
ing, aggregation, and updates. Due to the diﬀerence in ownership, geo-location,
and usage patterns, the connection might not be consistent, and this causes
low training eﬃciency. Hence, the client devices’ information needs to be man-
aged and maintained. The client registry pattern could be adopted to enhance
the system’s maintainability, reliability, and trustworthiness. The client reg-
istry records the information of the client devices that currently or previously

LocalmodeltrainingModelaggregationLocalmodeltrainingLocalmodeltrainingTask scriptLocalmodelGlobalmodelClientSystemownerClientdeviceClientdeviceClientdeviceCentralServerA Decision Model for Federated Learning Architecture Pattern Selection

5

Fig. 3: High-level Overview of Decision Models

Fig. 4: Client Management Patterns Decision Model

StartClient Management PatternsDecisionsModel Management &Configuration Pattern DecisionsModel AggregationPatterns DecisionsModel Training PatternsDecisionsEndClientclusterClientregistry+ maintainability + reliability + trustworthiness - data privacy Clientselector+ model quality + convergence speed - computationefficiency + resource optimisation + system efficiency + model quality - model bias NFR Maintain trainingefficiency under data heterogeneity,location, etc. NFR Enable systemmaintainability forhigh number of clientdevicesNFR Maintain trainingefficiencyunder differentcomputation &communicationresourcesStorage cost,computation cost,  and privacy  constraint due to record and  update of the  client devices info + client informationavailability - data privacy - storage cost efficiency + client informationavailability - data privacy - storage cost efficiency [complements][complements][alternative]+ computation efficiency - model bias 6

SK. Lo et al.

have interacted with the central server. The information may include clients’
ID, connection uptime/downtime, operating system version, available memory,
bandwidth, etc., which are essential for the client selection, ﬁltering, or model
communication scheduling task by the central server. The client registry can be
implemented using centralised database or decentralised blockchain, while tak-
ing the privacy sensitivity of the information recorded into consideration. By
entrusting the device information to the central server, the data privacy of the
clients’ information is compromised. Hence, a consensus from the device owners is
required before the collection of their information. As for the approach that uses
blockchain, the trustworthiness is further enhanced due to its immutability prop-
erty but blockchain as an approach to implement client registry still requires
more studies and consideration, such as usability and suitability of blockchain to
record the clients’ information, the read/write performance and the transparency
of blockchain. While both blockchain and database system can be used together
to complement each other in most scenarios, architects and designers need to
calculate the cost eﬀectiveness of implementing both approaches. This pattern
also need to satisfy the storage cost, computation cost, and privacy constraint
where continuous record and update of client devices info and status by the cen-
tral server throughout the training process is required. The owners’ data privacy
will be exposed by a certain degree where consensus to collect the information
is needed.

Suppose the central server collects all the models, or randomly selects client
devices to receive and submit model updates [15] when there is no information
regarding the client devices to be utilised for selection mechanisms. With the
clients’ information recorded by the central server, the client devices can be
actively managed. Hence, the client registry pattern complements the patterns
that manage the connection between the central server and the client devices.
To actively select the client devices for the training process according to the
system owner’s desired criteria, the client selector pattern can be adopted.
This pattern intends to fulﬁll the non-functional requirement on the training
eﬃciency when interacting with client devices that have high diﬀerences in their
available computation, communication and memory capacity [12,7]. By selecting
client devices with higher available resources, nearer in location, and have longer
connection up time for model training, the resource optimisation improves the
overall system eﬃciency and model quality. However, the adoption of client
selector may induced model bias as the selection criteria will always favours
the devices that have higher resources, regardless of the data quality or class
distributions. This might harm the model’s generalisability [13,7,11,10]. The bias
may also worsen the data distribution unbalancedness, which is known as non-
IID4. The patterns to solve non-IID issue will be covered in Sec. 6 and Sec. 7.
The client selector pattern is an alternative pattern to the client cluster
pattern. Both patterns intends to tackle data and system heterogeneity issue
to improve the system eﬃciency but the client selector pattern oﬀers better

4 Non-Identical and Independent Distribution: Skewed and personalised data distri-
bution that diﬀers across diﬀerent clients and restricts the model generalisation [17].

A Decision Model for Federated Learning Architecture Pattern Selection

7

computational eﬃciency but may induce higher model bias due to the exclusion
of low resource client devices.

Another patterns that is complemented by the client registry pattern is the
client cluster pattern. This pattern target to fulﬁll the non-functional require-
ment on training eﬃciency when client devices inherit diﬀerent characteristics
such as data heterogeneity, location, etc. that need to be considered during train-
ing. Similar to client selector, the client cluster groups the client devices with
similar characteristics, such as operating time zone, geolocation, etc. The local
models from the same group will be aggregated. This enhances the model quality
and the convergence speed of the training process. In contrast, the extra compu-
tational cost is required to access and group the client devices which may reduce
the computation eﬃciency of the system.

5 Model Management and Conﬁguration Decision Model

Fig. 5: Model Management and Conﬁguration Patterns Decision Model

Fig. 5 showcased the model management and conﬁguration decision model.
The federated learning process needs to be easily conﬁgurable by the system
owner. A training conﬁgurator provides a user-friendly interface, state-of-
the-art practices, computational resources, and technical support to the system

+ model performance + upgradability + trustworthiness - computation efficiency ModelcompressorModels co-versioningregistryModelreplacementtriggerDeploymentselector+ accountability + traceability - trustworthiness - data privacy - storage cost efficiency - computation efficiency TrainingconfiguratorNFR Maintainupgradability of theoutdated model FR Deploy the trainedmodels to theclient devices+ model suitability - computation efficiency - data privacy NFR Increasecommunicationefficiency undercontinuous exchanges  of modelsNFR Enhance traceabilityof local and globalmodelsFR Provide easilyconfigurablefederated learningprocess+ accessibility + usability - flexibility - scalability + communication efficiency - model quality + ease ofdeployment + usability + usability [complements][complements][complements]+ storage cost efficiency [complements]8

SK. Lo et al.

owners to conﬁgure the training parameters, client devices management, model
management and conﬁguration, and the model aggregation mechanisms which
we will cover in Sec. 6. This pattern enhances the accessibility and usability of
the federated learning system. Many user-friendly machine learning (ML) system
conﬁguration tools such as the Microsoft Azure Machine Learning Designer 5, the
Amazon SageMaker 6, and the Alibaba Machine Learning Platform 7 are available
for centralised or distributed ML systems. However, a preset system may have
a relatively lower ﬂexibility. Users might not have the freedom to conﬁgure or
design the system that suits all their requirements. Moreover, the system may
face scalability issues to support more users and devices associated with the
expansion of the systems. The training conﬁgurator pattern complements
the model co-versioning registry, model replacement trigger, and the
deployment selector patterns, in terms of usability.

Another requirement is to enable traceability of the local and global model
versions in a federated learning system. The model co-versioning registry
pattern could be adopted for model provenance. This approach uses a registry
to actively track and record all the model versions and performance of all the
client devices that have interacted with the central server. The timestamp and
the global model version that is created by the local models are also linked and
recorded for model provenance. This eﬀectively increases the accountability, and
traceability of the federated learning system. One downside to this approach is
the low storage cost eﬃciency due to the requirement to store the models when
the model size with highly-complex architecture can easily multi-folds with the
increase in the number of client devices [13]. Furthermore, designers may need
to decide where should the registry be located. Users’ data privacy may be com-
promised despite only providing the local models [12,7], whereas placing the
registry in each client device will certainly reduces the devices’ storage cost eﬃ-
ciency and computation eﬃciency. Trustworthiness issue between the 2 parties
also occurs when only one party holds the registry. One solution to that is to
use blockchain to host the registry so that all the parties can audit the models’
record [10,20]. A storage eﬃcient method using blockchain and smart contracts
to track and record only the hashed representations of the data and model ver-
sions is mentioned in [10] that has eﬀectively increased the storage eﬃciency
for model provenance. In addition, the usage of a combination of decentralised
blockchain and database for provenance purposes mentioned in [10] and [20] have
also resolved the data privacy and trustworthiness issues.

A model replacement trigger pattern fulﬁlls the requirement in enabling
upgradability of outdated models. This pattern monitors the performance of the
model that is deployed for real-world usage and when the model’s performance
(accuracy, precision, etc.) degrades, a request for new model will be generated to
trigger for a new model training task. This maintains the model performance and
upgradability, and the trustworthiness of the systems. However, the continuous

5 https://azure.microsoft.com/en-au/services/machine-learning/designer/
6 https://aws.amazon.com/sagemaker/
7 https://www.alibabacloud.com/product/machine-learning

A Decision Model for Federated Learning Architecture Pattern Selection

9

monitor and update of the deployed models will aﬀect the computation eﬃciency
of the central server and the client devices.

A deployment selector pattern fulﬁlls the requirement to deploy the trained
model to the client devices for real-world usage. This pattern is complemented
by the training conﬁgurator to increase the model’s ease of deployment. The
deployment selector will deploy a model that has been trained to convergence
to the client devices based on a series of client selection criteria. Conventionally,
the central server will distribute the converged model to all the client devices
that have participated in the training process. However, one global model may
not suit all the client devices’ applications or tasks. This is especially crucial
in multi-task and multi-model training scenario [13,7,12] where the local data
used to train the model comes from multiple, diﬀerent-but-related applications,
or client devices each with a dataset from one local application that can collab-
orate with the local dataset of the other application of other client devices to
train one or multiple models. More about multi-task training will be covered in
Section 7. Some client devices may not be able to actually deploy models with
high complexity (high number of neural-network layers or parameters) and a
simpler version of the same model is required. When there is more than one con-
verged model to be deployed, the deployment selector pattern selects clients
that are most suitable to receive the model, according to their speciﬁcations,
resources, applications, etc. This enhances the model suitability for each client
device’s application. However, the data privacy is compromised as more applica-
tion and devices information are required by the central server for the selection
criteria and causes lower computational eﬃciency.

Lastly, the continuous communication between the client devices and the
central server resulted in lower communication eﬃciency. To tackle this issue, a
model compressor pattern can be adopted to reduce the data size of the model
before being transferred between the two parties. Model pruning and compres-
sion can increases the communication eﬃciency but may negatively impact the
model quality due to the lower data precision [12,7].

6 Model Aggregation Decision Model

Fig. 6 showcased the model aggregation decision model. Ideally, the central server
waits for all the local models to reach to perform model aggregation. Hence, the
model aggregation time depends on the arrival time of the last local model. To
reduce the latency and increase the system eﬃciency of the model aggregation
process, the asynchronous aggregator pattern can be adopted. This pattern
performs model aggregation whenever a local model update is received, with
the currently available global model in the central server. The aggregation la-
tency can be greatly reduced which enhances the system eﬃciency. However,
a relatively higher number of aggregation rounds may be required and causes
lower communication eﬃciency. Furthermore, due to the diﬀerence in aggre-
gation round, the model update version received by each client device will be
diﬀerent across one another. Some client devices with extremely scarce commu-

10

SK. Lo et al.

Fig. 6: Model Training Patterns Decision Model

nication bandwidth may also struggle from being too outdated to join the latest
aggregation. Thus, the model produced may be biased. Client devices with low
computational resources will require longer local training time which makes their
local models outdated.

The non-IID data distribution is a main challenge of federated learning [12,13,10].

A hierarchical aggregator pattern introduces the inclusion of several edge
servers as an intermediate layer between the central server and the client de-
vices to resolve this issue. The edge servers are deployed at several locations to
group the client devices within the same area, where they have similar data and
location characteristics. The edge server then performs an intermediate model
aggregation using the local models from the client devices that they host. Finally,
the intermediate models are aggregated on the central server. By performing an
intermediate aggregation, the communication eﬃciency and scalability of the
system can be improved. The statistical and system heterogeneity are also re-
duced [13]. However, the constraint of this pattern is to have more devices added
to the system and since more layers and devices are added to the system, the
points-of-failure increase, and the system becomes vulnerable to failure and ad-
versarial attacks, which compromises reliability and security.

A federated learning system is vulnerable to security and reliability issues,
both due to the central server being the single-point-of-failure. The central server
needs to be robust and secured throughout the training process. To fulﬁll this
requirement, a secure aggregator can be adopted. This pattern utilises the
state-of-the-art security approaches for multiparty computation such as homo-
morphic encryption to encrypt and decrypt the model prior to exchanges, or

Decentalisedaggregator+ security - low latency - model quality - computation efficiency AsynchronousaggregatorHierarchicalaggregator+ system efficiency + low latency - model bias - communication efficiencySecureaggregator+ communication efficiency + scalability + low statistical heterogeneity + low system heterogeneity - reliability- security + reliability + accountability - low latency - storage cost efficiency NFR Reduce latency inmodels are aggregateddue to late modelupdatesNFR Maintain trainingefficiency under dataheterogeneity, location,etc.NFR Enhance robustnessof central server as asingle-point-of-failureCost constraint  to add intermediate  layer (e.g., edgeservers) System efficiency  constraint due to the  lack of central server.  Cost constraint to adopt alternatives for model  management and  aggregationA Decision Model for Federated Learning Architecture Pattern Selection

11

local diﬀerential privacy that add noise to the models before exchanges. How-
ever, diﬀerential privacy approaches may reduce the model quality due to the
noise added to the models. The computation eﬃciency drops and latency occurs
to perform encryption and decryption for every model updates received.

Another pattern that aims to solve the single-point-of-failure issue is the de-
centralised aggregator pattern. It removes the central server entirely but it
is constrained by low system eﬃciency and cost of using alternatives to execute
model aggregation. These alternatives include using peer-to-peer communica-
tion between neighboring client devices [16], or blockchain and smart contract
to manage the models [2,20]. This patterns increases the reliability and account-
ability in comparison with the centralised federated learning approach but it
suﬀers more in terms of latency and storage cost eﬃciency, especially due to the
peer-to-peer connection and the read/write performance of blockchain.

7 Model Training Decision Model

Fig. 7: Model Training Patterns Decision Model

Fig. 7 showcased the model training decision model. When a model is trained
in federated learning settings, there are 2 main requirements to be considered:
(1) reduce the eﬀect of non-IID or statistically heterogeneous data distribution
due to the diversity of client devices; and (2) increase participation rate and
trustworthiness of client devices. Firstly, to solve statistical heterogeneity issue,

Multi-taskmodel trainerHeterogeneousdata handlerIncentiveregistry+ client motivatability + trustworthiness + system fairness + model quality - security + model generalisability + training efficiency + robustness - data privacy + low statistical heterogeneity - computation efficiency FR Perform federatedlearning usingdifferent-but-relateddataNFR Maintain trainingefficiency under dataheterogeneity, location,etc.NFR Maintain model  quality &trustworthiness  under the changes ofclients' participation  rate Data collection costconstraint from different  applications need to  be matched across allclient devices+ trustworthiness + model fairness + data privacy - computation efficiency [alternative] 12

SK. Lo et al.

the heterogeneous data handler pattern can be adopted. The pattern can
implement data augmentation for federated learning [5] to generate more data
points to create a more balanced dataset, or adopting the federated knowledge
distillation method [1] that obtains the knowledge from other devices during the
distributed training process, without accessing the raw data. Apart from solving
statistical heterogeneity, this pattern also enhances model fairness and trust-
worthiness, which serves as an alternative to the incentive registry pattern
in terms of trustworthiness improvements. For instance, the weighted random
sampling of training data approach was proposed by [10] to create a more bal-
anced and fair dataset by randomly sampling more data points from class with
less number of samples. Low computation eﬃciency is the drawback for the
implementation of this pattern, especially when the client devices with fairly
limited amount of computation and storage resources needs to generate more
data through augmentation or perform knowledge distillation explicitly.

To improve the model quality by increasing the participation rate of client de-
vices, the incentive registry can be implemented. By providing a fair amount
of reward or compensation to the clients, the overall client motivatability is in-
creased and this translates to better model quality. With more client devices and
training data, the model generalisability is also enhanced. Furthermore, giving
rewards according to the clients’ contribution can also improves the system’s fair-
ness and trustworthiness. Blockchain and smart contract technology are adapted
to realise the incentive registry. Blockchain can store and record the contribu-
tion and model quality information while smart contracts can issue the reward
according to the compensation rate agreed between the central server and the
client devices. However, the provision of reward may harm the system security
as dishonest clients may submit fraudulent results to earn rewards illegally and
distort the training process.

The implementation of a multi-task trainer pattern allows the utilisation of
the client’s local data of diﬀerent-but-related applications to training. Hence, the
model generalisability and robustness are enhanced. Furthermore, the training
of the model on related or overlapping representations improves the training
eﬃciency of the system by reducing the cost required to train diﬀerent tasks
individually. However, this pattern is constrained by the requirement to collect
and match the data from diﬀerent applications across all participating client
devices to perform multi-task model training. This also means that the collected
data needs to be accessible by the central server for pre-processing, which will
be challenging in terms of data privacy.

8 Evaluation

We conducted a set of interviews to assess the usefulness, correctness, com-
prehensiveness of the decision models. We have generated a series of interview
questions for a group of experts in federated learning research and software
engineering ﬁeld. We identiﬁed the participants for interviews by searching rel-
evant organisations and databases online (CSIRO Portal, Google Scholar, etc).

A Decision Model for Federated Learning Architecture Pattern Selection

13

We have also sought recommendations from colleague researchers and contacted
the potential participants using publicly available contact details. 7 participants
agreed to be interviewed and the interviews were conducted from February to
March 2022. The interviewees are from various backgrounds, with a high varia-
tion in the degree of experience and responsibility. 2 interviewees are specialised
in federated learning research, 1 interviewee worked in federated learning and
trustworthy AI research and engineering, 1 interviewee works as a software en-
gineer in a web company, 2 interviewees are specialised in software engineering
research and 1 interviewee works in the machine learning research area. The job
positions of the interviewees included: postgraduate student (1), senior research
scientist (2), principal research scientist (3), and software engineer (1). All the
interviews were conducted through video teleconferencing.

8.1 Interview Questions

We asked the following questions to get feedback on the overall usefulness of the
proposed decision models:

– Are the decision models useful to support the decision-making in the design

and development of a federated learning architecture? Why?

– Are the decision models helpful in architectural evaluation, e.g., identiﬁca-

tion of the system qualities and tradeoﬀs? Why?

– Have the decision models covered the important aspects of a federated learn-

ing architecture? Is there any aspect missing?

– What are your suggestions to enhance the design process or the decision

models?

8.2 Key Findings

The overall feedback and opinions of the participants were positive and con-
structive. The followings are the key ﬁndings from the interviews:

Usefulness for Guidance to Design: The interviewees aﬃrmed that the pro-
posed decision models could provide design guidance to federated learning soft-
ware architects and developers with limited understandings of federated learning.
One participant mentioned that the decision models highlighted the path of se-
lecting patterns, and make the relation between the patterns more explicit. One
participant expressed that the overall decision models are useful but may be too
generic for architecture designs for speciﬁc use-case or application domains.

Usefulness for Architecture Evaluation and Documentation: The in-
terviewees felt that the proposed decision models are useful in evaluating and
documenting the architecture as the quality tradeoﬀs of applying the patterns
can be highlighted, and the impact of a pattern being applied to another pattern
is also well laid out. This assists the developers to identify the relevant properties
and analyse the potential conﬂicting properties.

14

SK. Lo et al.

Correctness and missing patterns/information: Most feedback on indi-
vidual decision models were clariﬁcations on how patterns relate to each other
and their requirement tradeoﬀs. We have integrated all the comments into the
decision models presented above. Some missing patterns suggested by the inter-
viewees are sampling strategy for client management patterns, representation
aggregator and results aggregator for model aggregation patterns. Nevertheless,
we did not include these to be architectural patterns. Instead, these patterns are
more in-depth and speciﬁc strategies for speciﬁc requirements. One participant
highlighted his confusion on the computation eﬃciency tradeoﬀ of the imple-
mentation of multi-task model trainer patterns, which we have updated after
the discussion.

Suggested Improvements: Three participant suggested providing more con-
text to the decision models and compiling a propagating decision model to
present how each patterns selection decision aﬀects the next pattern selections.
One interviewee suggested rephrasing the non-functional requirement and con-
straint statements with the associative system quality attributes. One partici-
pant questioned how to incorporate all the system quality attributes that con-
tradict each other in a system. Speciﬁc use-case or application scenarios should
be considered when applying the decision models to better understand the ben-
eﬁts and tradeoﬀs of applying each pattern. The participant also questioned the
applicability of the decision models on cross-silo federated learning scenarios.
Our decision models currently still main focus on the cross-devices settings of
federated learning and we intend to expand the decision models to cover more
requirements that are speciﬁc for cross-silo federated learning settings. We will
take all the comments and suggestions into account and plan to extend our
current decision models with more case studies.

9 Threats to Validity

The interview participants were selected from the industry and academia to re-
duce the subjective bias in expert feedback. The selected participants are all
experienced machine learning practitioners or federated learning and software
engineering researchers active in the industry and academia. All participants
had 3-10 years of experience on multiple AI, federated learning, or software
engineering research projects. The design models, high-level descriptions, and
questions were shared with the interviewees at least one week before the inter-
view. They were required to answer the questions before the interview, where
the answers and opinions were further discussed during the interview session.
All the participants were either direct or indirect professional contacts of the
investigators of this work. We applied our knowledge to identify the relation-
ships and selection paths among the patterns and this could aﬀect the proposed
decision models’ reproducibility. Nevertheless, this tradeoﬀ is inevitable as we
had to bring a structure into the decision model while pooling patterns from
multiple collections.

A Decision Model for Federated Learning Architecture Pattern Selection

15

10 Related work

A decision model is an approach in software engineering that maps the prob-
lems to the solution to guide design decision-making. A well-known approach
for creating decision models is Questions-Options-Criteria (QOC) [14] where the
questions represent problems, the options map to solutions, and the criteria are
used to determine the options’ suitability with respect to the questions. Another
popular approach from the ﬁeld of Software Measurement is Goal Questions Met-
ric (GQM) [4]. It models the problem according to the goals and questions and
provides the metrics to be used for assessing an object and subsequently mak-
ing decisions to improve it. There are much research works on multiple software
engineering and architecture domains that have adopted decision models. For
instance, Grace et al. [8] proposed a decision model for cyber-foraging systems’
architectural tactics. Xu et al. [19] propose a decision model for selecting ap-
propriate patterns for blockchain-based applications. These researchers designed
their decision models in extension to the series of patterns or tactics that they
have previously published. In practice, patterns selection is challenging due to
the fragmented information provided regarding the relationships across diﬀerent
patterns. In contrast, decision models can guide the selection of multiple pat-
terns while considering each pattern’s quality tradeoﬀs and relationships among
patterns.

A federated learning system design was introduced by Bonawitz et. al [3]. It
focuses on the high-level design of a basic federated learning system. [13] presents
a comprehensive and systematic collection of federated learning architectural
patterns to provide guidance for practitioners to better design and develop fed-
erated learning systems. Motivated by the aforementioned works, we focus to
design decision models for the selection of patterns based on the requirements,
while understanding the eﬀects on system qualities.

11 Conclusion and Future Work

This paper presented a decision model for federated learning systems that maps
the functional and non-functional requirements to the architectural patterns.
Each mapping of requirements and the patterns are qualiﬁed with the beneﬁts
and tradeoﬀs to improve the designers’ understanding on the eﬀects of the deci-
sions. The decision models have been evaluated by experts in terms of correct-
ness, usefulness, and comprehensiveness. According to the feedback, the decision
models are able to bring structure to the architecture design process, and help
explicitly articulate design rationale. For future works, we plan to expand the de-
cision models by including more patterns and collecting more experts’ feedback
to improve the decision models.

References

1. Ahn, J., Simeone, O., Kang, J.: Wireless federated distillation for distributed edge

learning with heterogeneous data. In: PIMRC 2019. pp. 1–6 (2019)

16

SK. Lo et al.

2. Bao, X., Su, C., Xiong, Y., Huang, W., Hu, Y.: Flchain: A blockchain for auditable
federated learning with trust and incentive. In: BIGCOM ’19. pp. 151–159 (2019)
3. Bonawitz, K.A., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V.,
Kiddon, C.M., Koneˇcn´y, J., Mazzocchi, S., McMahan, B., Overveldt, T.V., Petrou,
D., Ramage, D., Roselander, J.: Towards federated learning at scale: System design.
In: SysML 2019 (2019), to appear

4. Caldiera, V.R.B.G., Rombach, H.D.: The goal question metric approach. Encyclo-

pedia of software engineering pp. 528–532 (1994)

5. Jeong, E., Oh, S., Kim, H., Park, J., Bennis, M., Kim, S.L.: Communication-
eﬃcient on-device machine learning: Federated distillation and augmentation under
non-iid private data (2018)

6. Jobin, A., Ienca, M., Vayena, E.: The global landscape of AI ethics guidelines.

Nature Machine Intelligence 1(9), 389–399 (2019)

7. Kairouz, P., McMahan, H.B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N.,
Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.: Advances and open
problems in federated learning. arXiv preprint arXiv:1912.04977 (2019)

8. Lewis, G.A., Lago, P., Avgeriou, P.: A decision model for cyber-foraging systems.
In: 2016 13th Working IEEE/IFIP Conference on Software Architecture (WICSA).
pp. 51–60 (2016)

9. Lo, S.K., Liew, C.S., Tey, K.S., Mekhilef, S.: An interoperable component-based

architecture for data-driven iot system. Sensors 19(20) (2019)

10. Lo, S.K., Liu, Y., Lu, Q., Wang, C., Xu, X., Paik, H.Y., Zhu, L.: Towards trust-
worthy ai: Blockchain-based architecture design for accountability and fairness of
federated learning systems. IEEE Internet of Things Journal pp. 1–1 (2022)
11. Lo, S.K., Lu, Q., Paik, H.Y., Zhu, L.: FLRA: A reference architecture for feder-
ated learning systems. In: Software Architecture. pp. 83–98. Springer International
Publishing (2021)

12. Lo, S.K., Lu, Q., Wang, C., Paik, H.Y., Zhu, L.: A systematic literature review
on federated machine learning: From a software engineering perspective. ACM
Comput. Surv. 54(5) (May 2021)

13. Lo, S.K., Lu, Q., Zhu, L., Paik, H.y., Xu, X., Wang, C.: Architectural patterns for
the design of federated learning systems. arXiv preprint arXiv:2101.02373 (2021)
14. MacLean, A., Young, R.M., Bellotti, V.M., Moran, T.P.: Questions, options, and
criteria: Elements of design space analysis. Human–computer interaction 6(3-4),
201–250 (1991)

15. McMahan, H.B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.:
Communication-eﬃcient learning of deep networks from decentralized data (2017)
16. Roy, A.G., Siddiqui, S., P¨olsterl, S., Navab, N., Wachinger, C.: Braintorrent: A

peer-to-peer environment for decentralized federated learning (2019)

17. Sattler, F., Wiedemann, S., M¨uller, K., Samek, W.: Robust and communication-
eﬃcient federated learning from non-i.i.d. data. IEEE Transactions on Neural Net-
works and Learning Systems pp. 1–14 (2019)

18. Thiebes, S., Lins, S., Sunyaev, A.: Trustworthy artiﬁcial intelligence. Electronic

Markets (2020). https://doi.org/10.1007/s12525-020-00441-4

19. Xu, X., Dilum Bandara, H., Lu, Q., Weber, I., Bass, L., Zhu, L.: A decision model
for choosing patterns in blockchain-based applications. In: 2021 IEEE 18th Inter-
national Conference on Software Architecture (ICSA). pp. 47–57 (2021)

20. Zhang, W., Lu, Q., Yu, Q., Li, Z., Liu, Y., Lo, S.K., Chen, S., Xu, X., Zhu, L.:
Blockchain-based federated learning for device failure detection in industrial iot.
IEEE Internet Things J. pp. 1–12 (2020)

