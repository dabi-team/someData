Distributed Estimation for Interconnected Systems
with Arbitrary Coupling Structures

Yuchen Zhang, Bo Chen, Li Yu, and Daniel W.C. Ho

1

2
2
0
2

n
u
J

1

]

Y
S
.
s
s
e
e
[

1
v
1
2
2
0
0
.
6
0
2
2
:
v
i
X
r
a

Abstract—This paper is concerned with the problem of dis-
tributed estimation for time-varying interconnected dynamic
systems with arbitrary coupling structures. To guarantee the
robustness of the designed estimators, novel distributed stability
conditions are proposed with only local
information and the
information from neighbors. Then, simpliﬁed stability conditions
which do not require timely exchange of neighbors’ estimator
gain information is further developed for systems with delayed
communication. By merging these subsystem-level stability con-
ditions and the optimization-based estimator gain design, the
distributed, stable and optimal estimators are proposed. Quite
notably, these optimization solutions can be easily obtained by
standard software packages, and it is also shown that the designed
estimators are scalable in the sense of adding or subtracting
subsystems. Finally, an illustrative example is employed to show
the effectiveness of the proposed methods.

Index Terms—Time-varying interconnected systems; Dis-
tributed stability conditions; Distributed estimation; Optimal
estimators.

I. INTRODUCTION

With the development of communication and sensor tech-
nology, the scale of systems is consistently increasing as they
are getting more and more connected. As early as the 1960s,
the concept of interconnected systems had been proposed [1],
and interconnected systems have received more and more
attention in recent decades due to their wide applications in
power systems [2], multi-robot systems [3], complex networks
[4, 5], and biological networks [6]. Generally, interconnected
systems are high-dimensional complex systems composed of
numerous dispersed subsystems, which can be state-coupled
with their neighboring subsystems. The increased complexity
of interconnected systems in terms of both system topologies
and dynamics has prevented traditional estimation approaches
from achieving satisfactory performance [7]. This can be
mainly attributed to the poor scalability of centralized structure
in traditional approaches. Firstly, the spatial distribution of
subsystems will lead to high communication burden and ﬁeld
deployment cost for centralized methods. Meanwhile,
the
centralized methods also suffer heavy computational burden
with the increase of the dimensions of interconnected systems.
In addition, the intricate coupling structures of interconnected
systems are not exploited in centralized methods, making it
necessary to re-ensure stability when adding or subtracting
subsystems. Therefore, it is imperative to consider advanced

Y. Zhang, B. Chen and L. Yu are with the Department of Automation,
Zhejiang University of Technology, Hangzhou 310023, China.(email: Yuchen-
Zhang95@163.com, bchen@aliyun.com, lyu@zjut.edu.cn).

D. W. C. Ho is with the Department of Mathematics, City University of

Hong Kong, Hong Kong, 999077. (email: madaniel@cityu.edu.hk).

estimation approaches for interconnected systems to guarantee
the accuracy and the stability of estimators.

Over

the past

several decades, different decentral-
ized/distributed estimation approaches have been developed
in the ﬁelds of multi-agent systems [8, 9], multi-sensor sys-
tems [10–12], and interconnected systems [13–23] to decrease
communication overhead and computational complexity. In
these approaches,
local estimators are designed based on
their own information and the information form their neigh-
boring subsystems. However, the arbitrary couplings among
subsystems impose more signiﬁcant challenge to distributed
analysis for interconnected systems, especially in terms of
stability. For this reason, most of existing distributed esti-
mation approaches for interconnected systems are based on
special coupling structures or communication structures. With
structural assumptions, the designed distributed estimators can
provide better estimation performance and their stability can
be ensured by local analysis. For example, the optimal locally
unbiased ﬁlter was proposed in [13] with speciﬁc structure
for information exchange, while the centralized and distributed
moving horizon estimators were developed in [14] for sparse
banded interconnected systems. The sparsity structure was
also exploited to decompose interconnected systems into in-
terconnected overlapping subsystems with coupled states that
can be locally observed, then the distributed Kalman ﬁlter
[15] and the consensus based decentralized estimator [16]
were designed. Meanwhile, a sub-optimal distributed Kalman
ﬁltering problem was addressed in [17] for a class of sequen-
tially interconnected systems. Note that it is difﬁcult for most
interconnected systems to transform into these structures. A
hopeful idea to address distributed estimation problem without
any structure constraints is to combine stability conditions and
distributed estimator design methods. For instant, by adding
constraints on stability conditions for general interconnected
systems, distributed estimators with decoupling strategy were
designed in [18–20] and a moving horizon estimator was
proposed in [21] with the assumption of uncorrelated local
estimation errors. Besides, the distributed estimators with plug-
and-play fashion were developed in [22, 23] by exploiting
the properties of inﬁnity norm for small gain based stability
conditions. However, how to design stable and distributed es-
timation methods based on local and neighboring information
for general interconnected systems is still an open question.

Since the 1960s, the stability problem for general intercon-
nected systems has received a great deal of attention [24–26].
To the best of our knowledge, besides the centralized analysis
of stability for overall systems, the stability analysis methods
for general interconnected systems can be divided into three

 
 
 
 
 
 
2

categories: 1) methods based on scalar or vector Lyapunov
function [1, 27, 28]; 2) methods based on small gain theorem
[29, 30]; 3) methods based on dissipativity theory [31, 32].
For the ﬁrst category, the stability conditions involving M -
matrices are derived by investigating the internal stability
for both subsystems and the overall interconnected systems.
Unfortunately, tests for M -matrices are successful only when
the couplings among subsystems are weak. In contrast, the
stability conditions for the second category are obtained by
analyzing the input-output stability of subsystems, where
the couplings are treated as input terms from neighboring
subsystems. It also requires weak coupling conditions for
small gain theorem based methods, but the results are less
conservative and can lead to relatively simple design guideline
[24]. Another kind of input-output stability results in the third
category are based on the concept of dissipativity, which
are not necessarily weak coupling conditions due to their
centralized analysis. However, the above stability conditions
are not fully distributed, which means the knowledge of the
dynamics and the couplings from neighboring subsystems is
not enough in the analysis process. How to develop these
stability conditions into scalable distributed conditions is still
challenging. One way to address this problem is to derive
the distributed stability conditions by totally local analysis
[18–23], where the stability of subsystems is locally and se-
quentially analyzed. Nevertheless, this approach is much more
conservative than the centralized results, i.e., weak coupling
conditions or structural assumptions are still required. Another
promising idea is the subsystem-level analysis for centralized
stability conditions by decomposing them into distributed
ones. For example, the work in [33] focused on decomposing a
centralized dissipativity condition into distributed dissipativity
conditions of individual subsystems. Note that these conditions
require huge communication burden to exchange message
matrices among subsystems and cannot be generalized to time-
varying interconnected systems.

It should be pointed out that the distributed estimator de-
signed in [19] only provides stability conditions with speciﬁc
subsystem coupling structures, which are further interpreted as
the directed acyclic graphs of couplings in [20]. As for general
subsystem connection structures,
the design of distributed
estimators with subsystem-level stability conditions is still
challenging, and has not yet been fully solved. Motivated by
the above analysis, we shall investigate the distributed estima-
tion problem for general time-varying interconnected systems.
The main contributions of this paper can be summarized as
follows:

• Distributed stability analysis. The distributed stability
conditions, which only require subsystem-level knowl-
edge of dynamics and couplings, are proposed for local
estimators. Then, the effect of couplings on distributed
conditions is discussed.

• Distributed stability under delayed communication.
The simpliﬁed distributed stability conditions are pro-
posed for time-varying interconnected systems with one-
step communication delay. It is shown that the simpliﬁed
conditions do not need real-time exchange of subsystems’

gain information and can ease communication burden.
• Distributed estimators design. By combining the dis-
tributed stability conditions and the optimization-based
estimator gain design, a recursive, stable and optimal
estimators for time-varying noisy interconnected systems
are proposed, where an upper bound of local estimation
error covariance is minimized. The proposed estimators
are fully distributed, that is, only based on local and
neighboring information.

Notations: Deﬁne Nl := {1, 2, ..., l}, where l is a natural
number excluding zero, and denote the set of n-dimensional
real vectors by Rn. Give sets A and B, A \ B represents the
set of all elements of A that are not in B, and A ∩ B is the
intersection set of A and B. The superscript ‘T’ represents the
transpose, while the symmetric terms in a symmetric matrix
are denoted by ‘∗’. The inverse of the matrix A is denoted
by A−1, and Tr(A) represents the trace of the matrix A.
The identity matrix with appropriate dimensions is represented
as ‘I’, and the matrix with all zero elements is denoted by
‘0’. The notation X > (<)0 is a positive deﬁnite (negative
deﬁnite) matrix, and X ≥ (≤)0 is a positive semi-deﬁnite
(negative semi-deﬁnite) matrix. The notation col{a1, ..., an}
means a column vector whose elements are a1, ..., an, while
diag{·} stands for a block diagonal matrix. The mathematical
expectation is denoted by E{·}, and kAk2 is the 2-norm of
matrix A. Given a block matrix A = [Ai,j]i∈Nn,j∈Nm , Ai,j
represents the (i, j)th block. The maximum eigenvalue of
matrix A is represented as λmax(A).

II. PROBLEM FORMULATION
A. Time-varying Interconnected System Model

Consider a time-varying interconnected system S con-
structed by l subsystems, where the state and measurement
dynamics of the ith subsystem Si, i ∈ Nl is described as
follows:

xi(k + 1) =Ai(k)xi(k) + Γi(k)wi(k)

i ∈ Nl

(1)

+

Ai,iρ

κ (k)xiρ

κ (k)

Si : 

Xiρ
κ∈Ωi
yi(k) = Ci(k)xi(k) + Di(k)vi(k)


The vectors xi(k) ∈ Rni and yi(k) ∈ Rmi denote the state and
the measurement of the subsystem Si, respectively. Moreover,
κ (k), Ci(k) and Di(k) are bounded matrices
Ai(k), Γi(k), Ai,iρ
with appropriate dimensions, while the system noise wi(k) and
the measurement noise vi(k) are uncorrelated Gaussian white
noises satisfying

E [wi(k)wj (k1)] = δi,jδk,k1 Qwi
E [vi(k)vj(k1)] = δi,jδk,k1 Qvi
E [wi(k)vj (k1)] = 0(∀i, j, k, k1)




(2)

where Qwi and Qvi are the known covariances of wi(k) and

vi(k), respectively. δk,k1 = 0 if k 6= k1 and δk,k1 = 1
otherwise. The set of neighbors for subsystem Si is denoted
by Ωi, and the number of elements is θi (θi < l). Therefore,
the set Ωi can be described as
Ωi = {iρ

(3)

}

1, ..., iρ

κ, ..., iρ
θi

The coupling structure of the system is determined by whether
κ (k) is a null matrix. Since there is no con-
the matrix Ai,iρ
straints on the spatial distribution of subsystems, the coupling
structure can be arbitrary. Then, the following subset Σi is
deﬁned:

Σi := {iσ

κ | iσ

κ ∈ Ωi \ Ni}

(4)

where the number of elements for Σi is ξi (ξi ≤ θi).

Remark 1. Compared with the work in [13–17], the addressed
interconnected system model in this paper does not require
any structural assumptions (i.e., the sparsity assumption on
couplings). In this case, the model in (1) is more general and
can cover a large part of practical situations. For example,
the heavy duty vehicle systems [34] with aerodynamic
interconnections can be modeled as interconnected systems
with strongly connected topologies in the form of (1). On the
other hand, there is no constraint on the coupling strength
for the interconnected system model in this paper, which is
different from the work in [22, 23]. In other words, the upper
bound of kAi,j(k)k2 can be arbitrarily large. However, the
analysis of distributed stability and the distributed estimation
problem for general interconnected system without any weak
coupling assumptions will be more challenging.

To collaboratively achieve system tasks, subsystems need
to exchange their information via communication networks.
Therefore,
the distributed communication structure in the
following assumption is required.

Assumption 1 (Communication). Each subsystem can
communicate with its neighbors.

Remark 2. Notice that the communication structure in As-
sumption 1 is distributed and has a limited range of infor-
mation broadcast due to the limitation on network bandwidth
and energy constraints for subsystems. Unlike the centralized
communication structure with one subsystem communicates
with all the other subsystems, the considered distributed com-
munication structure is more practical. On the other hand, we
restrict ourselves to the time-varying interconnected systems
with constantly varying dynamics and couplings due to its
wider applications. Take blocked power systems as an ex-
ample, the couplings among different blocks are changing
with the real-time power dispatching [35]. For time-varying
interconnected systems, the distributed stability conditions in
[33] are not suitable anymore, and thus novel distributed
stability analysis approaches are required.

B. Problem of Interest

The structure of distributed estimators for interconnected
systems with local information ﬂows is depicted in Fig. 1. It is
assumed that subsystems can only know their own dynamics,
and thus the local measurements and the estimates form

neighbors are used for state reconstruction. The estimator Ei
for the ith subsystem is proposed as

3

ˆxp
i (k) = Ai(k − 1)ˆxi(k − 1)
iρ
κ∈Ωi

Ai,iρ

κ (k − 1)

κ (k − 1)ˆxiρ

+
i (k) + Ki(k) [yi(k) − Ci(k)ˆxp
ˆxi(k) = ˆxp
P

Ei : 

where ˆxp
i (k) and ˆxi(k) are the one-step prediction and the

estimate of subsystem state xi(k), respectively. Then,
the
estimation error iteration for the ith subsystem is calculated
by (1) and (5) as

i (k)]

(5)

˜xp
i (k) = Ai(k − 1)˜xi(k − 1) + Γi(k − 1)wi(k − 1)

(6)

P

Ai,iρ

+
iρ
κ∈Ωi
˜xi(k) = KCi(k)˜xp

κ (k − 1)
κ (k − 1)˜xiρ
i (k) − Ki(k)Di(k)vi(k)



where KCi(k) := I − Ki(k)Ci(k), while ˜xp
i (k) and ˜xi(k)

are the one-step prediction error and the estimation er-
ror, respectively. The one-step prediction error covariance
P p
and the estimation error co-
variance Pi(k) := E

can be calculated as

i (k) := E

i (k) [˜xp
˜xp

n

i (k)]T
˜xi(k)˜xT
i (k)
o
P p
i (k) = Ai(k − 1)Pi(k − 1)AT
i (k − 1)
(cid:9)
+ Γi(k − 1)QwiΓT
+

Ai(k − 1)Pi,iρ

κ (k − 1)AT
i,iρ
κ

i (k − 1)

(k − 1)



(cid:8)

Xiρ
κ∈Ωi

Xiρ
κ∈Ωi

+

+

Ai,iρ

κ (k − 1)Piρ

κ,i(k − 1)AT

i (k − 1)

Ai,iρ

κ1 (k − 1)

(7)

κ1 ∈Ωi Xiρ
Xiρ
Piρ
κ1 ,iρ
Pi(k) =KCi(k)P p

κ2 ∈Ωi n
κ2 (k − 1)AT
(k − 1)
i,iρ
κ2
i (k) [KCi(k)]T

+ Ki(k)Di(k)Qvi DT

o
i (k)K T

i (k)

The major concern of the distributed estimation problem is
to design suitable gain matrices Ki(k) (i ∈ Nl) such that
the estimation error is stable and the estimation performance
index Ji(k) is minimized. Speciﬁcally, the following deﬁnition
is introduced to describe the property of stability for local
estimators.




Deﬁnition 1 (Mean-square uniformly bounded). For the
interconnected system in (1), the proposed estimator (5) is
mean-square uniformly bounded if for arbitrarily large δp0
,
there is δpi (δp0

) > 0 (independent of k0) such that

i

i

kPi(k0)k2 ≤ δp0

i

⇒ kPi(k)k2 ≤ δpi

(8)

it

is usually difﬁcult for subsystems to timely
However,
obtain the cross-covariances Pi,j(k) := E{˜xi(k)˜xT
j (k)} by
only local communication. Therefore, an upper bound of the
estimation error covariance ˆPi(k) ≥ Pi(k) is used instead
and the performance index for local estimation is designed as
Ji(k) = Tr{ ˆPi(k)}. Here, the optimal estimator gain design
for subsystems can be formulated as an optimization problem:

Tr{ ˆPi(k)}

min
Ki(k)
s.t. ˆPi(k) ≥ Pi(k) and Ki(k) ∈ Ki(k)

(9)

4

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:4)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:2)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:4)

(cid:1)(cid:2)

(cid:1)(cid:5)

(cid:1)(cid:3)

(cid:1)(cid:4)

(cid:1)(cid:6)

(cid:12)
(cid:9)(cid:8)(cid:10)(cid:8)(cid:11)

(cid:12)
(cid:13) (cid:14)(cid:8)(cid:11)

(cid:12)
(cid:13) (cid:15)(cid:15)(cid:8)(cid:11)

(cid:16) (cid:17)(cid:8)

(cid:14)(cid:8)(cid:18)(cid:13)(cid:19)

(cid:1)(cid:8)

(cid:20)(cid:8)(cid:18)(cid:13)(cid:19)

(cid:21)(cid:22)(cid:8) (cid:13) (cid:15)(cid:15)(cid:14)(cid:23)(cid:8)(cid:18)(cid:13)(cid:19)

(cid:17)(cid:15)(cid:18)(cid:11)(cid:19)(cid:8)(cid:20)(cid:6)(cid:14)(cid:11)(cid:15) (cid:21)(cid:13)(cid:11)(cid:22)(cid:4)

(cid:12)
(cid:21)(cid:22)(cid:8)(cid:11)

(cid:12)
(cid:13) (cid:15)(cid:15)(cid:14)(cid:23)(cid:8)(cid:11)

(cid:12)
(cid:13) (cid:15)(cid:15)(cid:8)(cid:11)

(cid:16) (cid:17)(cid:8)

(cid:7)(cid:2)

(cid:7)(cid:5)

(cid:12)
(cid:24)(cid:8)(cid:11)

(cid:12)
(cid:13) (cid:15)(cid:8)(cid:11)

(cid:16) (cid:17)(cid:8)

(cid:7)(cid:8)

(cid:24)(cid:8) (cid:13)

Fig. 1. An example of the structure for interconnected systems and distributed estimators.

(cid:7)(cid:3)

(cid:7)(cid:4)

(cid:7)(cid:6)

(10)

(11)

where Ki(k) is a subspace of stable estimator gains for
subsystem Si at the instant k.

In what follows, the augmented system dynamics and the
augmented estimator iteration will be presented. By deﬁning
x(k) := col{x1(k), ..., xl(k)} ∈ Rn, we can obtain the overall
system dynamics as

S :

(

x(k + 1) = A(k)x(k) + Γ(k)w(k)
y(k) = C(k)x(k) + D(k)v(k)

where A(k) := [Ai,j (k)]i,j∈Nl with Ai,i(k) = Ai(k) and

y(k) := col{y1(k), ..., yl(k)}
Γ(k) := diag{Γ1(k), ..., Γl(k)}
C(k) := diag{C1(k), ..., Cl(k)}
D(k) := diag{D1(k), ..., Dl(k)}
v(k) := col{v1(k), ..., vl(k)}
w(k) := col{w1(k), ..., wl(k)}






The upper bounds of bounded matrices are kA(k)k2 ≤
δa, kΓ(k)k2 ≤ δγ, kC(k)k2 ≤ δc, kD(k)k2 ≤ δd,
kAi,j(k)k2 ≤ αi,j and kAi(k)k2 ≤ αi, respectively. Then,
let us denote ˆx(k) := col{ˆx1(k), ..., ˆxl(k)} and ˜x(k) :=
col{˜x1(k), ..., ˜xl(k)}. The following augmented estimator and
the augmented estimation error iteration are obtained:

ˆx(k) =A(k − 1)ˆx(k − 1)

+ K(k) [y(k) − C(k)A(k − 1)ˆx(k − 1)]

˜x(k) =KC (k)A(k − 1)˜x(k − 1) − K(k)D(k)v(k)




where K(k) := diag{K1(k), ..., Kl(k)} and KC (k) := I −
K(k)C(k).

+ KC(k)Γ(k − 1)w(k − 1)

(12)

Note that the stability of each local estimator depends on the
stability of its neighboring estimators due to the interconnected
κ (k − 1). Hence, it is difﬁcult to determine
estimation error ˜xiρ
Ki(k) and design a stable estimator in a totally local analysis.
On the other hand, a totally centralized analysis for the
augmented estimation error system needs the knowledge of
dynamics and couplings from the overall system, which cannot
apply to large-scale interconnected systems.

Consequently,
following problems:

the aim of this paper is to address the

1) Distributed stability conditions analysis: Analyze the
distributed stability conditions such that the proposed
estimator is mean-square uniformly bounded, where only
subsystem-level knowledge of dynamics and couplings is
required for each estimator.

2) Distributed estimator design: Design distributed, stable
and optimal estimators for time-varying interconnected
systems with arbitrary coupling structures, where an
upper bound of local estimation error covariance is
minimized.

Remark 3. To design a fully distributed estimator, both the it-
eration form and the stability conditions for the estimator need
to achieve local communication, computation and storage.
Though the estimator in (5) only uses the information of local
measurement and neighboring estimates, the local estimation
errors are still interconnected. Therefore, the major difﬁculty
for the distributed estimator design for general interconnected
systems is to calculate the optimal estimator gain and maintain
the stability without any globally interconnection information
of estimation errors.

III. MAIN RESULTS

In this section, we ﬁrstly present distributed conditions to
guarantee the stability of local estimators. Then, a fully dis-
tributed estimation approach is proposed by merging optimal
and stable estimator gain designs.

A. Distributed Stability Conditions

Let us denote the augmented estimation error covariance as

P (k) := E{˜x(k)˜xT(k)} and it is calculated by
P (k) =KC(k)A(k − 1)P (k − 1)AT(k − 1)K T
C (k)

+ KC (k)Γ(k − 1)QwΓT(k − 1)K T
+ K(k)D(k)QvDT(k)K T(k)

C (k)

(13)

where the matrices Qw
Qv

:= diag{Qv1, ..., Qvl} are

:= diag{Qw1, ..., Qwl} and
augmented noise

the

covariances. Then,
centralized stability conditions
are derived by the following proposition. Its proof appears in
the Appendix.

the

Proposition 1. If the following centralized stability condition
is satisﬁed

kKC(k)A(k − 1)k2 ≤ λ < 1
kK(k)k2 ≤ η

(

(∀k ≥ k0)

(14)

where η is a ﬁnite positive number,
then the proposed
distributed estimator (5) is stable in the sense of mean-square
uniformly bounded (8).

the

Under

distributed communication structure,

the
knowledge of dynamics and couplings from the overall
system is hard to obtain for local estimators. Therefore, the
following theorem provides distributed conditions to ensure
the stability for all local estimators.

Theorem 1 (Distributed stability conditions). The following
distributed conditions are sufﬁcient to ensure the stability for
the proposed distributed estimator (5):
C1) For each subsystem

kKCi(k)Ai(k − 1)k2 ≤ λ < 1
kKi(k)k2 ≤ η

(

(i ∈ Nl)

(15)

C2) For each pair of neighbors (i, iρ
κ)
κ (k)N −1
iρ
κ

κ,i(k)Ni(k)−Ni,iρ

κ (k)ǫiρ

ǫi,iρ

(k)N T
i,iρ
κ

(k) ≤ 0 (16)

where η is a ﬁnite positive number, while parameter ǫi,j(k)
satisﬁes

κ (k) = 1 and

ǫi,iρ

iρ
κ∈Ωi

P
Ni(k) :=

−λI KCi(k)Ai(k − 1)

∗

"

−λI

Ni,iρ

κ (k) :=

0
κ,i(k−1)K T
AT
iρ
Ci
ρ
κ

"

(k)

#
KCi(k)Ai,iρ
0

κ (k−1)

#

These conditions are equivalent to the following inequalities:






κ (k) ≤ 0 (i ∈ Nl, iσ

Mi,iσ
kKi(k)k2 ≤ η (i ∈ Nl)

κ ∈ Σi)

(

where

Mi,iσ

κ (k) ∆=

ǫi,iσ

κ (k)Ni(k)
∗

(cid:20)

Ni,iσ
κ (k)
κ,i(k)Niσ

ǫiσ

κ (k)

(cid:21)
lemma [36],

Proof. According to Schur complement
the
condition (C2) and the ﬁrst inequality in the condition (C1)
κ,i(k) ≤ 0 for each pair
κ (k) ≤ 0 or Miρ
are equivalent to Mi,iρ
of neighbors (i, iρ
κ). By the deﬁnition of Σi in (4), one can
κ (k) ≤ 0 (i ∈ Nl, iσ
conclude that Mi,iσ
κ ∈ Σi). Then, deﬁne
the permutation matrix

Ini
0
0
0

0
0
Iniσ
0

κ

0
Ini
0
0

Q := 





0
0
0
Iniσ

κ







(20)

5

By left and right multiplication of Mi,iσ
the following equivalent inequality is derived:

κ (k) with Q and QT,

ˆMi,iσ

κ (k) =

κ (k) Vi,iσ
Ui,iσ
Ui,iσ
∗

κ (k)
κ (k)

(cid:20)

(cid:21)

where

≤ 0 (i ∈ Nl, iσ

κ ∈ Σi) (21)






Ui,iσ

κ (k) :=

κ (k)λI
−ǫi,iσ
∗

"

0

−ǫiσ

κ,i(k)λI#

Vi,iσ

κ (k) :=
κ (k)KCi(k)Ai(k−1)

(k)Aiσ

κ,i(k−1)

ǫi,iσ
KCiσ

κ

"

KCi(k)Ai,iσ

κ (k−1)

ǫiσ

κ,i(k)KCiσ

κ

(k)Aiσ

κ (k−1)#

By augmenting all the matrices in (21), one has that
1 (k), ..., ˆM1,1σ
ˆM (k) = diag{ ˆM1,1σ
1 (k), ..., ˆM2,2σ
ˆM2,2σ

(k), ...} ≤ 0

(k),

ξ1

ξ2

(22)

(23)

Then, deﬁne the permutation matrix

R := row{e1,1σ

1 , ..., e1,1σ

ξ1

, e2,2σ

1 , ..., e2,2σ

ξ2

, ...}

(24)

ei
0

ej
0

0
ei

0
ej

where ei,j

:=

and ei

is a matrix with

(cid:21)
(cid:20)
dimension n×ni that contains all zero elements, but an identity
i
j=1 nj).
matrix of dimension ni at rows (
By the property of positive deﬁnite matrix, if ˆM (k) < 0, then
the matrix by left and right multiplication of ˆM (k) with R
and RT is negative deﬁnite, i.e.,

i−1
j=1 nj + 1) : (

P

P

KC1(k)A1(k − 1) · · · KC1(k)A1,l(k − 1)
KC2(k)A2,1(k − 1) · · · KC2(k)A2,l(k − 1)
. . .
KCl(k)Al,1(k − 1) · · · KCl(k)Al(k − 1)
−λI

...

...



−λI







≤ 0

∗










(25)

(26)

(27)

(17)

Inequality (25) is equivalent to

−λI KC(k)A(k − 1)

∗

−λI

≤ 0

(cid:20)
By Schur complement lemma, one has that

(cid:21)

(18)

kKC(k)A(k − 1)k2 ≤ λ

According to Proposition 1, inequality (27) and the second
inequality in the condition (C1) are sufﬁcient to ensure the
mean-square boundedness (8). This completes the proof.

(19)

Intuitively,

the stability condition for

Remark 4.
the stability of an independent
inﬂuenced by its
subsystem without any couplings is not
neighboring subsystems, and thus local mean-square uniform
boundedness condition is enough to ensure the stability.
However,
the subsystem that
coupled with its neighbors will be tighter than the local
mean-square uniform boundedness condition. Therefore, the
distributed stability conditions in Theorem 1 contain two
parts, the condition (C1) ensures that local estimation error
interconnected terms is stable, while the
system without
condition (C2) is an additional requirement for the stability

6

of systems with some coupling relationships.

5. For

systems with
for

known
stabilizing systems

Remark
the distributed control problem of
system states,
interconnected
can
the distributed conditions
similar derivation of Theorem 1.
be obtained by a
When the process dynamics
is controlled by an
term “Bi(k)ui(k)”,
the distributed state
additional
feedback controllers ui(k) = −
(k)xi(k) and
iρ
κ∈Ωi
“ui(k) = −K u
i (k)xi(k)” can be designed, then the distributed
stabilization conditions can be obtained by decomposing
the matrix inequality kA(k) − B(k)K u(k)k2 ≤ λ with the
property of positive deﬁnite matrix, where B(k) and K u(k)
are the corresponding augmented matrices.

input

K u

(1)

i,iρ
κ

P

The determination of the parameter λ is a trade-off between
the stability and the performance of estimators, where smaller
λ can provide more conservative margin of distributed stability
and potentially worse estimation performance. On the other
hand, the parameter η only inﬂuences the ultimate bounded-
ness of estimators and can be chosen as a large number to
avoid estimation performance degradation.

κ (k) from subsystem S

Notice that the above distributed stability conditions need
subsystem Si to know the gain Kiρ
iρ
κ
timely. However, one-step communication delay, naturally
risen from networked environments,
inevitable and
need to be taken into account when the estimator gain
information is transmitted over the communication network
from neighboring subsystems. To extend the result of
Theorem 1 to more general communication environments
with one-step transmission delay, the following conditions
without synchronously knowing neighboring estimator gains
are further proposed.

is

Corollary 1. For each subsystem, if the following inequalities
are satisﬁed:

kKCi(k)k2 ≤ βi
kKi(k)k2 ≤ η

(i ∈ Nl)

(
where η is a ﬁnite positive number, and βi ≤ λ
αi
by

(28)

is constrained

the upper bounds of Ni(k) and N −1

i

Hence,
obtained as

Ni(k) ≤ (βiαi − λ) I
N −1
(k) ≥ 1
i

βiαi−λ I

(

(k) can be

(32)

By the second and the third inequalities of (30), one has the
following inequality:

Ni,iρ

κ (k)N T
i,iρ
κ

(k) ≤

β2
iρ
κ
Therefore, it can be concluded that

(cid:20)

I

i α2
β2
i,iρ
κ
0

0
α2
κ,iI
iρ

(cid:21)

κ (k)N −1
iρ
κ

(k)N T
i,iρ
κ

(k)

κ ¯ǫiρ
¯ǫi,iρ
≤ ¯ǫi,iρ

κ,iNi(k) − Ni,iρ
κ ¯ǫiρ

κ,i (βiαi − λ) I
i α2
β2
i,iρ
κ
0

1
κ − λ
κ βiρ

(cid:20)

−

αiρ

I

0
α2
κ,iI
iρ

β2
iρ
κ

(cid:21)

(33)

(34)

Under the constraints in (29) and taking ǫi,iρ
κ , the
condition (C2) in Theorem 1 is derived. This completes the
proof.

κ (k) = ¯ǫi,iρ

To balance the stability margin of each subsystem,
the
parameter ǫi,j(k) and ¯ǫi,j should be proportional to the size
of couplings, and a feasible parameter selection is given by

ǫi,j(k) =

kAi,j (k−1)k2+kAj,i(k−1)k2
i (cid:16)kAi,i

(k−1)k2+kAi
ρ

ρ
κ

κ,i(k−1)k2(cid:17)

(35)

ρ
κ ∈Ω
Pi
αi,j +αj,i
i (cid:16)αi,i

ρ
κ

¯ǫi,j =

ρ
κ ∈Ω
Pi

+αi
ρ
κ ,i(cid:17)



Remark 6. By Corollary 1, the distributed stability conditions

are simpliﬁed into ﬁnding an appropriate time-invariant
parameter βi. Unlike the conditions in [33] that require huge
communication burden to exchange message matrices among
subsystems, the calculation of βi in this paper only needs
subsystems to communicate with their neighbors to exchange
the knowledge of βiρ
this procedure
can be achieved ofﬂine with less communication overhead.
Notice that
the stability result with less communication
and computational burden is more suitable for time-varying
interconnected systems with different couplings and dynamics
at each instant.

κ . Therefore,

κ and αiρ

(αiβi − λ)

αiρ

κ βiρ

κ − λ

≥

i α2
β2
i,iρ
κ
¯ǫi,iρ
κ ¯ǫiρ

κ,i

iρ
κ ∈ Ωi

The distributed calculation of βi can be implemented in the

(29)

following Algorithm.

(cid:0)

(cid:1)

with parameter ¯ǫi,j satisﬁes
proposed distributed estimator
mean-square uniformly bounded (8).

P

¯ǫi,iρ

then the
κ = 1,
iρ
κ∈Ωi
is stable in the sense of

Proof. The following upper bounds can be derived from the
inequality (28):

kKCi(k)Ai(k − 1)k2 ≤ βiαi ≤ λ
κ (k − 1)k2 ≤ βiαi,iρ
kKCi(k)Ai,iρ
κ,i(k − 1)k2 ≤ βiαiρ
(k)Aiρ
kKCi



By the Schur complement lemma, the ﬁrst inequality in (30)

can be converted to
−βiαiI KCi(k)Ai(k − 1)

(30)

κ,i

ρ
κ

κ

∗

(cid:20)

−βiαiI

(cid:21)

≤ 0

Algorithm 1 Distributed calculation for βi

1: for i := 1 to L do
if i 6= 1 then
2:

3:

4:

5:

6:

κ and αiρ

κ from subsystem

Subsystem Si receives βiρ
S
κ ∈ Ωi ∩ Ni−1);
κ (iρ
iρ
end if
Subsystem Si calculates βi < λ that satisﬁes (29) for
each coupling pair (i, iρ
Subsystem Si sends the calculated βi and αi to subsys-
tem Siσ

κ ∈ Ωi ∩ Ni−1;

κ), iρ

κ ∈ Σi;

κ iσ

7: end for

(31)

Remark 7. The small gain theorem for
interconnected
systems can be stated as follows. Suppose that each local

it

estimation error system (6) satisﬁes the local mean-square
uniform boundedness condition kKCi(k)Ai(k − 1)k2 < 1,
then the augmented estimation error system in (12) is stable
if the set of small gain conditions kAi1,i2 Ai2,i3 ...Air ,i1 k < 1
(1 ≤ is ≤ l, is 6= is′ if s 6= s′) holds for each r = 2, ..., l.
The small gain conditions mean that
the composition of
the coupling matrices along every closed cycle is stable.
However,
is hard to apply the small gain theorem to
design distributed estimator or controller for interconnected
systems with arbitrary coupling structures. For distributed
the
estimation problem, feedback is introduced to adjust
size of KCi(k)Ai(k − 1) (i ∈ Nl) in a distributed manner
such that the augmented estimation error system is stable,
while the coupling matrices Ai,iρ
κ (k) cannot be adjusted. The
small gain theorem requires that kAi1,i2Ai2,i3...Air ,i1 k < 1,
which is not always satisﬁed and irrelevant to the estimator
design. A natural problem is what distributed conditions does
a subsystem need to meet with its neighbors such that the
overall system is stable. To address the above problem, the
distributed stability conditions are derived by decomposing
the centralized stability condition kKC(k)A(k − 1)k2 < λ
in the paper. The result in Theorem 1 turns out to be the
matrix inequalities for each pair of neighbors. Therefore, each
subsystem only needs to satisfy these matrix inequalities with
its neighbors, then the stability for the overall system can be
ensured.

Remark 8. Compared with the stability analysis by Lyapunov
functions [1, 27, 28], the proposed distributed stability condi-
tions in Corollary 1 are less conservative in the requirement
of weak coupling assumptions. According to the inequality
(29), the strength of coupling αi,iρ
κ can be arbitrarily large as
long as the stability parameter βi is designed small enough.
On the other hand, the conditions in Corollary 1 can be
directly applied to the distributed estimation problem when
the parameters are determined by Algorithm 1 ofﬂine.

B. Optimization-based Distributed Estimator

In what follows, we would like to design optimal estima-
tors for time-varying interconnected systems in a distributed
way. We have the following results on optimization-based
distributed estimator design. First of all, let us deﬁne the
following matrices:

DPi (k) := col

[Pi(k)]1, ...,

[Pi(k)]ni

(36)

q



, ...,




D ˆPi

(k) := col

np
(cid:26)rh

ˆPi(k)
1
i
where ˆPi(k) is an upper bound of Pi(k) and [Pi(k)]τ is
the τ th diagonal element of Pi(k). Then, the gain design
for the proposed distributed estimator (5) is provided in the
following Theorem.

o
ˆPi(k)
ni (cid:27)
i

rh

Theorem 2. For the time-varying interconnected system (1),
the gain matrix K opt
(k) of the proposed distributed estimator
(5) is obtained by minimizing an upper bound of estimation

i

7

error covariance and keeping the designed estimator mean-
square uniformly bounded, as the following optimization prob-
lem:

Tr{ ˆGi(k)}

min
Ki(k)

i (k) Ki(k)Di(k)Qvi

0
−Qvi

(37)

< 0








− ˆGi(k) KCi(k) ˆP p
− ˆP p
i (k)
∗

∗
∗


(18) or (28)

s.t. 


where ˆP p
covariance and is calculated as

i (k) is an upper bound of one-step prediction error

ˆP p
i (k) = Ai(k − 1) ˆPi(k − 1)AT
(k − 1)DT
+
ˆPi

Ai(k−1)D ˆPi

ρ
κ

i (k − 1)

(k − 1)AT
i,iρ
κ

(k − 1)

Ai,iρ

κ (k−1)D ˆPi

ρ
κ

(k − 1)DT
ˆPi

(k − 1)AT

i (k−1)

Xiρ
κ∈Ωi

+

Xiρ
κ∈Ωi

+
κ1 ∈Ωi Xiρ
Xiρ

κ2 ∈Ωi (cid:26)

Ai,iρ

κ1 (k − 1)D ˆPi

ρ
κ1

(k − 1)

(38)

×DT
ˆPi

ρ
κ2

(k − 1)AT
i,iρ
κ2

+ Γi(k − 1)QwiΓT

i (k − 1)

(k − 1)

(cid:27)

with the upper bound of estimation error covariance ˆPi(k − 1)
calculated as

i

I − K opt

ˆPi(k − 1) =
I − K opt
(k − 1)Ci(k − 1)
(cid:2)
(k−1)Di(k−1)QviDT
i (k−1)
(cid:3)

(k − 1)Ci(k − 1)
T

×
+ K opt
(cid:2)
i

i

(cid:3)
K opt
i

ˆP p

i (k − 2)

(39)

T

(k−1)

(cid:2)

(cid:3)

Proof. The cross-covariances Pi,j (k) among subsystems are
difﬁcult to online calculate by local communication, which
means that direct calculation of the one-step prediction error
covariance P p
i (k) in (7) is not feasible. Therefore, an upper
bound of the estimation error covariance ˆPi(k) ≥ Pi(k)
is constructed and used for the gain design problem. Let
[˜xi(k)]τ1 ∈ R be the τ1th component of ˜xi(k), while [˜xj(k)]τ2
is deﬁned as the τ2th component of ˜xj(k). By resorting to the
well-known H¨older inequality, one has that

E

{[˜xi(k)]τ1 [˜xj(k)]τ2

≤ E

[˜xi(k)]τ1 [˜xj(k)]τ2

n

≤

o

E

r

n(cid:12)
(cid:12)
[˜xi(k)]2
(cid:12)
τ1
n

E
or

(cid:12)
(cid:12)
(cid:12)

o
[˜xj(k)]2
τ2
n

o

(40)

Thus, the following upper bound of Pi,j(k) is derived:

Pi,j(k) ≤ DPi (k)DT

Pj (k)

(41)

8

Then, applying the inequality (41) to (7), it turns out that
P p
i (k) ≤ Ai(k − 1)Pi(k − 1)AT
Ai(k−1)DPi (k − 1)DT
+
Pi
ρ
κ

(k − 1)AT
i,iρ
κ

i (k − 1)

(k − 1)

Xiρ
κ∈Ωi

+

+

Xiρ
κ∈Ωi

κ1 ∈Ωi Xiρ
Xiρ

κ2 ∈Ωi n

Ai,iρ

κ (k−1)DPi

ρ
κ

(k − 1)DT

Pi(k − 1)AT

i (k−1)

Ai,iρ

κ1 (k − 1)DPi

ρ
κ1

(k − 1)

(42)

×DT
Pi
ρ
κ2

(k − 1)AT
i,iρ
κ2

+ Γi(k − 1)QwiΓT

i (k − 1)

(k − 1)

(cid:27)

i (k) is constructed by
i (k). In this case, an upper bound of local

Therefore, an upper bound of P p
P p
i (k) ≤ ˆP p
estimation error covariance is derived as
ˆPi(k) =KCi(k) ˆP p

i (k) [KCi(k)]T

(43)

+ Ki(k)Di(k)Qvi DT

i (k)K T

i (k)

Then, it is proposed to construct an upper bound of ˆPi(k) as
ˆGi(k) satisfying
ˆPi(k) − ˆGi(k) < 0

(44)

The optimal estimator gain is obtained by minimizing this up-
per bound ˆGi(k), which turns to be an optimization problem:

K opt
i

(k) = arg min
Ki(k)

Tr{ ˆGi(k)}

(45)

s.t. ˆPi(k) − ˆGi(k) < 0
By Schur complement lemma, the inequality constraint in (45)
is converted into
Ki(k)Di(k)Qvi DT
i (k)K T
∗

i (k) − Gi(k)

−

−1

< 0 (46)

KCi (k)
ˆP p
i (k)

#

"

Then, the ﬁrst inequality constraint in (37) is further derived
by using Schur complement
lemma again. Adding the
distributed stability constraints in Theorem 1 or Corollary 1,
the optimization problem in Theorem 2 is formulated. This
completes the proof.

h

i

Remark 9. The inequality constraints (18) and (28) can be
rewritten as linear matrix inequality forms, so the optimization
problem in Theorems 2 can be directly solved by the function
“mincx” of MATLAB LMI toolbox [36]. In addition, the
information used in the optimization problem (37) is from
subsystem Si and its neighbors, and the computational
complexity is mainly determined by the dimensions of
the proposed estimators are
these subsystems. Therefore,
recursive and fully distributed such that can be deployed for
large-scale interconnected systems with local communication
and computation requirements.

conditions with

Remark 10. Notice that the work in [19, 20] only provides
subsystem coupling
stability
structures, while the designed distributed estimators
in
Theorem 2 directly use the newly proposed subsystem-level
stability conditions for general interconnected systems. This

speciﬁc

in terms of

design methdology that combines the optimality and stability
can overcome the disadvantages of totally local estimator
the stability problem. Moreover,
analysis
the developed stability conditions enable plug-and-play
operations, which means newly added subsystem does
not
inﬂuence the stability of previous subsystems and its
own stability can be ensured by collecting its neighbors’
information. Thus, there is no need to redesign the stability
parameters and this property is helpful for deployment of
distributed estimators.

Algorithm 2 Distributed Estimation for Time-varying inter-
connected systems

1: if Communication is one-step delayed then
Ofﬂine calculation of βi by Algorithm 1;
2:
3: end if
4: for i := 1 to L do
5:

Subsystem Si collects local measurement yi(k), neigh-
bors’ estimated states ˆxiρ
κ (k − 1) and error covariance
bounds ˆPiρ
Calculate ˆP p
if Communication is one-step delayed then

κ (k−1), and Kiσ
i (k) by (38);

κ ∈ Σi(k − 1));

κ (k) (iσ

Determine the estimator gain Ki(k) by solving the
optimization problem (37) with constraints in (28);

else

Determine the estimator gain Ki(k) by solving the
optimization problem (37) with constraints in (18);

end if
Calculate ˆPi(k) by (39);
Calculate the distributed estimate ˆxi(k) by (5);
Subsystem Si sends the calculated ˆxi(k), ˆPi(k) (only
for Gaussian noise situation) and Ki(k) to its neighbors.

6:

7:
8:

9:

10:

11:

12:

13:
14:

15: end for
16: Return to Step 4 and implement Steps 4-15 for calculating

ˆxi(k + 1)(i = 1, 2, ..., l).

From Theorem 2,

the computational procedures of the
distributed estimation for general interconnected systems with
and without one-step communication delay can be summarized
by Algorithm 2. For systems with ideal communication, the
real-time transmission of estimator gains is feasible and the
required information for the inequality constraints in (18) can
be obtained timely. However, the stability conditions in (18)
will not work any more when one-step communication delay
is taken into consideration. Instead, ofﬂine calculation of βi
for the inequality constraints in (28) can solve the problem
caused by communication delay.

IV. SIMULATION EXAMPLES

To illustrate the effectiveness of the proposed distributed
estimators, a numerical study result is reported in this section.
Let us consider the following interconnected system with three

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:4)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:2)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:4)

(cid:11)(cid:3)

(cid:1)(cid:2)(cid:10)(cid:4)(cid:3)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:7)(cid:8)(cid:9)

(cid:11)(cid:10)

(cid:1)(cid:2)(cid:5)(cid:4)(cid:10)(cid:6)(cid:10)(cid:7)(cid:8)(cid:9)

(cid:11)(cid:5)

Fig. 2. The couplings among subsystems for an interconnected system.

TABLE I
VALUES OF βi UNDER DIFFERENT COUPLING STRENGTH

Coupling
Strength g

0.5

1

1.5

2

2.5

3

3.5

4

β1
β2
β3

1.08
1.21
1.84

1.08
1.01
1.57

1.08
0.90
1.36

1.08
0.81
1.19

1.08
0.75
1.05

1.08
0.70
0.94

1.08
0.66
0.86

1.08
0.63
0.78

subsystems:

x1(k + 1) = A1(k)x1(k) + gA1,3x3(k) + Γ1w1(k)
x2(k + 1) = A2(k)x2(k) + gA2,1x1(k) + Γ2w2(k)
x3(k + 1) = A3(k)x3(k) + gA3,2x2(k) + Γ3w3(k)
y1(k) = C1(k)x1(k) + D1v1(k)
y2(k) = C2(k)x2(k) + D2v2(k)
y3(k) = C3(k)x3(k) + D3v3(k)

(47)

S :






where

A1(k) =

A2(k) =

"

"

0.2
0.2 + 0.1 sin(k)

0.2 + 0.2 cos(k)
0.2

0.3
0.2 + 0.2 sin(k)

0.1 + 0.3 cos(k)
0.2

0.3
0.1 + 0.1 cos(k)

"
0.3 + 0.3 cos(k)

0.1 + 0.2 sin(k)
0.2

0.4

#

#

#

(48)

A3(k) =

C1(k) =

C2(k) =

h

"

0.6 + 0.2 cos(k)
0.2

C3(k) =

0.5 + 0.1 sin(k)
0.1

"

A1,3 = A2,1 = A3,2 =

0.1
0

"

0
0.1#

i

0.3
0.7 + 0.1 sin(k)#
0.3
0.7 + 0.1 cos(k)#

and Γi and Di are identity matrices. The parameter g is used
to adjust the strength of couplings, and the coupling structure
is described in Fig. 2. The process noise wi(k) (i ∈ {1, 2, 3})
is Gaussian noises with covariance diag{0.1, 0.1}, and the
measurement noises vi(k) (i = 1, 2, 3) are Gaussian noises
with covariances 0.1, diag{0.1, 0.1} and diag{0.1, 0.1}, re-
spectively. The values for βi are calculated for different
coupling strengths by tuning the parameter g, and the result is
shown in Table 1. As the connection of subsystems gets more
and more strong, the calculated value of βi decreases such that
the stability conditions become stricter. This observation is






9

consistent with the fact that the convergence rate and stability
margin of the overall system is related to the size of its
transition matrix.

Then, the proposed optimization-based distributed estima-
tors are deployed to estimate the states of this interconnected
system. Under one-step communication delay, the trajectories
of the states and the corresponding estimated values by Theo-
rem 2 for this interconnected system are plotted in Fig. 3 when
g = 4. As shown in Fig. 3, the proposed distributed estimator
can track the real states well under a large coupling strength
with communication delay. To compare the performance of the
distributed estimators with and without the inﬂuence of one-
step communication delay, Monte Carlo simulations with 100
runs have been performed by randomly varying the realization
of process and measurement noises. The mean square error
(MSE) is introduced to evaluate the performance, where

S

MSE(k) =

s=1
X

kes(k)k2
S

(49)

with es(t) being the state estimation error at the instant k
in the sth simulation. Fig. 4 depicts the MSE performance
comparison for two cases in Algorithms 2. The result shows
that the estimation accuracy can maintain at a satisfactory level
when one-step communication delay is taken into considera-
tion. Moreover, to evaluate the dependence of performance on
different coupling strengths, the AMSE (i.e., the asymptotic
MSE deﬁned as the average of the MSE computed in the whole
time interval) is reported in Fig. 5. As the stability constraints
are getting stricter under stronger couplings, the estimation
accuracy is becoming worse. The performance degradation
is caused by its conservatism from time-invariant stability
parameters.

V. CONCLUSION

In this paper, we presented new results for subsystem-level
stability analysis and distributed estimator design for time-
varying interconnected systems with arbitrary coupling struc-
tures. The proposed distributed stability conditions can ensure
mean-square uniform boundedness without the requirement for
the knowledge of dynamics and couplings from the overall
interconnected systems. Then, the simpliﬁed conditions that do
not need real-time exchange of subsystems’ gain information
were developed for systems with one-step communication
delay. Particularly, we showed that the distributed stability
conditions do not need any coupling structure assumption
and can be easily extended when a new subsystem is added
to the original interconnected system. These conditions are
applied to distributed estimator design problem for time-
varying interconnected systems, and novel optimization-based
estimator design approaches were proposed. Notice that the
designed estimators are fully distributed, where only local
information and the information from neighbors are required
for the estimator iteration form and the stability conditions.
Finally, an illustrative example was employed to show the
effectiveness of the proposed methods.

Several topic for future research is left open. Extensions of
the presented distributed stability conditions to co-design of

10

2

0

-2

0

2

0

-2

0

2

0

-2

0

x

11

xe

11

2

0

x

21

xe

21

50

100

50

100

150

x

12

150

x

13

200

-2

0

xe

12

2

0

200

-2

0

xe

13

2

0

-2

0

50

100

150

x

22

200

xe

22

50

100

150

x

23

200

xe

23

50

100

150

200

50

100

150

200

Fig. 3. The trajectories of the states and the corresponding estimated values by Theorem 2 for the interconnected system.

By the augmented estimation error covariance in (13),
kP (k)k2 ≤ kP (k + 1)k2, then

if

Fig. 4. MSE comparison of the distributed estimators in Algorithms 2 with
and without communication delay.

kP (k)k2 ≤

η2δ2

dkQvk2 + (1 + ηδc)2 δ2

γkQwk2

1 − λ2

It turns out that

0 ≤

λ2 − 1
+ η2δ2
(cid:0)

kP (k)k2

dkQvk2 + (1 + ηδc)2 δ2
(cid:1)

γkQwk2

(51)

:= δp1

(52)

If kP (k − 1)k2 ≤ kP (k)k2, then kP (k − 1)k2 ≤ δp1 and

kP (k)k2 ≤ λ2δp1 + η2δ2

dkQvk2 +(1 + ηδc)2 δ2

γkQwk2

:= fp(δp1 )

(53)

Fig. 5. AMSE performance under different coupling strengths.

distributed estimator and controller will be important. Another
interesting extension is the development of secure estimator
for interconnected systems with stability constraints. Due to
the frequent information exchange among subsystems and
the broadcast nature of communication medium, it is more
interconnected systems to various
vulnerable for practical
attacks. To prevent system information from being collected
by eavesdroppers to generate sophisticated attacks, the design
of defense mechanisms is required and will be one of our
future work. Meanwhile, the inﬂuence of cyber attacks will
propagate among subsystems, and how to detect cyber attacks
by subsystem cooperation is an important and interesting
problem.

VI. APPENDIX

Proof of Proposition 1. From the conditions in (14), an upper
bound of KC(k) can be derived as

kKC(k)k2 < 1 + ηδc,

(50)

If kP (k − 1)k2 ≥ kP (k)k2 ≥ kP (k + 1)k2 at all instants, then
kP (k)k2 ≤ kP (k − 1)k2 ≤ ... ≤ kP (k0)k2 := δp0 . Now, we
can conclude that kP (k)k2 is bounded as

kP (k)k2 ≤ max{δp1, fp(δp1 ), δp0}

(54)

By the boundedness of kP (k)k2, one also has that kPi(k)k2
is bounded. This completes the proof.

REFERENCES
[1] F. N. Bailey, “The application of Lyapunov’s second method to inter-
connected systems,” Journal of the Society for Industrial and Applied
Mathematics, Series A: Control, vol. 3, no. 3, pp. 443–462, 1965.
[2] V. Kekatos and G. B. Giannakis, “Distributed robust power system state
estimation,” IEEE Transactions on Power Systems, vol. 28, no. 2, pp.
1617–1626, 2013.

[3] Z. Feng, G. Hu, Y. Sun, and J. Soon, “An overview of collaborative
robotic manipulation in multi-robot systems,” Annual Reviews in Con-
trol, vol. 49, pp. 113–127, 2020.

[4] M. Dickison, S. Havlin, and H. E. Stanley, “Epidemics on interconnected
networks,” Physical Review. E, Statistical, Nonlinear, and Soft Matter
Physics, vol. 85, no. 6, p. 066109, 2012.

[5] W. Li, Y. Jia, and J. Du, “State estimation for stochastic complex
networks with switching topology,” IEEE Transactions on Automatic
Control, vol. 62, no. 12, pp. 6377–6384, 2017.

[6] Y. Huang, I. Tienda-Luna, and Y. Wang, “Reverse engineering gene
regulatory networks,” IEEE Signal Processing Magazine, vol. 26, no. 1,
pp. 76–97, 2009.

[7] J. Lian, “Special section on control of complex networked systems
(CCNS): Recent results and future trends,” Annual Reviews in Control,
vol. 47, pp. 275–277, 2019.

11

[33] E. Agarwal, S. Sivaranjani, V. Gupta, and P. J. Antsaklis, “Distributed
synthesis of local controllers for networked systems with arbitrary
interconnection topologies,” IEEE Transactions on Automatic Control,
vol. 66, no. 2, pp. 683–698, 2021.

[34] A. A. Alam, A. Gattami, and K. H. Johansson, “An experimental study
on the fuel reduction potential of heavy duty vehicle platooning,” in 13th
International IEEE Conference on Intelligent Transportation Systems,
pp. 306–311, Sept. 2010.

[35] J. Yang, W. A. Zhang, and F. Guo, “Dynamic state estimation for power
networks by distributed unscented information ﬁlter,” IEEE Transactions
on Smart Grid, vol. 11, no. 3, pp. 2162–2171, 2020.

[36] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan, eds., Linear
Matrix Inequalities in System and Control Theory. Philadelphia, PA,
USA: Society for Industrial and Applied Mathematics, 1994.

[8] C. Kwon and I. Hwang, “Sensing-based distributed state estimation
for cooperative multiagent systems,” IEEE Transactions on Automatic
Control, vol. 64, no. 6, pp. 2368–2382, 2019.

[9] P. Yang, R. A. Freeman, and K. M. Lynch, “Multi-agent coordination by
decentralized estimation and control,” IEEE Transactions on Automatic
Control, vol. 53, no. 11, pp. 2480–2496, 2008.

[10] R. Olfati-Saber, “Distributed Kalman ﬁltering for sensor networks,” in
2007 46th IEEE Conference on Decision and Control, (New Orleans,
LA, USA), pp. 5492–5498, IEEE, Dec. 2007.

[11] B. Chen, W. A. Zhang, and L. Yu, “Distributed ﬁnite-horizon fusion
Kalman ﬁltering for bandwidth and energy constrained wireless sensor
networks,” IEEE Transactions on Signal Processing, vol. 62, no. 4, pp.
797–812, 2014.

[12] W. A. Zhang and L. Shi, “Sequential fusion estimation for clustered

sensor networks,” Automatica, vol. 89, pp. 358–363, 2018.

[13] C. Sanders, E. Tacker, T. Linton, and R. Ling, “Speciﬁc structures for
large-scale state estimation algorithms having information exchange,”
IEEE Transactions on Automatic Control, vol. 23, no. 2, pp. 255–261,
1978.

[14] A. Haber and M. Verhaegen, “Moving horizon estimation for large-scale
interconnected systems,” IEEE Transactions on Automatic Control, vol.
58, no. 11, pp. 2834–2847, 2013.

[15] U. A. Khan, “Distributing the Kalman ﬁlter for large-scale systems,”
IEEE Transactions on Signal Processing, vol. 56, no. 10, pp. 4919–4935,
2008.

[16] S. S. Stankovi´c, M. S. Stankovi´c, and D. M. Stipanovi´c, “Consensus
based overlapping decentralized estimation with missing observations
and communication faults,” Automatica, vol. 45, no. 6, pp. 1397–1406,
2009.

[17] B. Chen, G. Hu, D. W. C. Ho, and L. Yu, “Distributed Kalman ﬁltering
for time-varying discrete sequential systems,” Automatica, vol. 99, pp.
228–236, 2019.

[18] B. Chen, G. Hu, D. W. Ho, and L. Yu, “Distributed estimation for
discrete-time interconnected systems,” in 2019 Chinese Control Con-
ference (CCC), (Guangzhou, China), pp. 3708–3714, IEEE, July 2019.
[19] B. Chen, G. Hu, D. W. C. Ho, and L. Yu, “Distributed estimation and
control for discrete time-varying interconnected systems,” IEEE Trans-
actions on Automatic Control, 2021. doi: 10.1109/TAC.2021.3075198.
[20] Y. Zhang, B. Chen, L. Yu, and D. W. C. Ho, “Distributed Kalman
ﬁltering for interconnected dynamic systems,” IEEE Transactions on
Cybernetics, 2021. doi: 10.1109/TCYB.2021.3072198.

[21] M. Farina, G. Ferrari-Trecate, and R. Scattolini, “Moving horizon
partition-based state estimation of large-scale systems,” Automatica, vol.
46, no. 5, pp. 910–918, 2010.

[22] S. Riverso, M. Farina, R. Scattolini, and G. Ferrari-Trecate, “Plug-
and-play distributed state estimation for linear systems,” in 52nd IEEE
Conference on Decision and Control, (Firenze), pp. 4889–4894, IEEE,
2013.

[23] S. Riverso, D. Rubini, and G. Ferrari-Trecate, “Distributed bounded-
error state estimation based on practical robust positive invariance,”
International Journal of Control, vol. 88, no. 11, pp. 2277–2290, 2015.
[24] N. Sandell, P. Varaiya, M. Athans, and M. Safonov, “Survey of decen-
tralized control methods for large scale systems,” IEEE Transactions on
Automatic Control, vol. 23, no. 2, pp. 108–128, 1978.

[25] A. N. Michel, “On the status of stability of interconnected systems,”
IEEE Transactions on Systems, Man, and Cybernetics, vol. 13, no. 4,
pp. 439–453, 1983.

[26] H. Ito, “A geometrical formulation to unify construction of Lyapunov
functions for interconnected iISS systems,” Annual Reviews in Control,
vol. 48, pp. 195–208, 2019.

[27] A. N. Michel, “Stability analysis of interconnected systems,” SIAM

Journal on Control, vol. 12, no. 3, pp. 554–579, 1974.

[28] W. M. Haddad and S. G. Nersesov, Stability and Control of Large-Scale
Dynamical Systems: A Vector Dissipative Systems Approach. Princeton:
Princeton University Press, 2011.

[29] S. N. Dashkovskiy, B. S. R¨uffer, and F. R. Wirth, “Small gain
theorems for large scale systems and construction of ISS Lyapunov
functions,” SIAM Journal on Control and Optimization, vol. 48, no.
6, pp. 4089–4118, 2010.

[30] H. Ito, “State-dependent scaling problems and stability of interconnected
iISS and ISS systems,” IEEE Transactions on Automatic Control, vol.
51, no. 10, pp. 1626–1643, 2006.

[32] M. Vidyasagar,

[31] P. Moylan and D. Hill, “Stability criteria for large-scale systems,” IEEE
Transactions on Automatic Control, vol. 23, no. 2, pp. 143–149, 1978.
inter-
connected systems. Decomposition, well-posedness and stability.
Berlin/Heidelberg: Springer-Verlag, 1981.

Input-output analysis of

large-scale

ed.,

