Go-Explore Complex 3D Game Environments for Automated Reachability Testing

Cong Lu1*, Raluca Georgescu2†, Johan Verwey2†
1 University of Oxford
2 Microsoft Research
cong.lu@stats.ox.ac.uk, raluca.georgescu@microsoft.com, joverwey@microsoft.com

2
2
0
2

p
e
S
1

]
I

A
.
s
c
[

1
v
0
7
5
0
0
.
9
0
2
2
:
v
i
X
r
a

Abstract

Modern AAA video games feature huge game levels and
maps which are increasingly hard for level testers to cover
exhaustively. As a result, games often ship with catastrophic
bugs such as the player falling through the ﬂoor or being
stuck in walls. We propose an approach speciﬁcally targeted
at reachability bugs in simulated 3D environments based
on the powerful exploration algorithm, Go-Explore, which
saves unique checkpoints across the map and then identiﬁes
promising ones to explore from. We show that when coupled
with simple heuristics derived from the game’s navigation
mesh, Go-Explore ﬁnds challenging bugs and comprehen-
sively explores complex environments without the need for
human demonstration or knowledge of the game dynamics.
Go-Explore vastly outperforms more complicated baselines
including reinforcement learning with intrinsic curiosity in
both covering the navigation mesh and number of unique po-
sitions across the map discovered. Finally, due to our use of
parallel agents, our algorithm can fully cover a vast 1.5km x
1.5km game world within 10 hours on a single machine mak-
ing it extremely promising for continuous testing suites.

Introduction
With the scope and size of modern AAA games ever in-
creasing, QA teams are struggling to exhaustively test these
games (Nantes, Brown, and Maire 2008; Chang, Aytemiz,
and Smith 2019; Bulut 2015). It is becoming increasingly
common for games to ship with many bugs that are discov-
ered by end-users and only get ﬁxed over the course of sev-
eral months following the title’s release (Lewis, Whitehead,
and Wardrip-Fruin 2010; Truelove, Santana de Almeida, and
Ahmed 2021). The challenge of covering every corner of a
multi-square mile game world is exacerbated by the fact that
the maps constantly change during development which re-
quires tests to be repeated hundreds of times.

A large proportion of bugs that occur in game worlds are
related to reachability (Albaghajati and Ahmed 2020). Game
testers need to verify that every area of a map that should be
reachable through the player’s abilities, are in fact reachable
and haven’t been inadvertently blocked off during iteration.
Some areas are designed to test the player’s skill and are

*Work done at Microsoft Research, Cambridge, UK.
†Joint senior authorship.

Figure 1: Go-Explore caches discovered positions during
training and then intelligently resets to promising saved po-
sitions to explore from them. The navigation mesh provides
Go-Explore with a natural heuristic targeted towards un-
reached goals (marked in gray) which may be combined
with visitation count. Map shown is a top-down view of dis-
covered positions on the hard traversal map (depicted in Fig-
ure 2c) 10% through training.

hard to reach, but not impossible. Finally, there are those ar-
eas of the map that should not be reachable—in these areas,
players could fall through the ﬂoor or ﬁnd themselves inside
geometry that is not part of the gameplay area. Automating
reachability testing for large game maps would alleviate a
huge burden during game development and allow bugs to be
identiﬁed earlier without human labor.

A large body of prior work has considered ways to au-
tomate these tests including by imitating recorded human
actions (Ariyurek, Betin-Can, and Surer 2021; Politowski,
Petrillo, and Gu´eh´eneuc 2021; Hern´andez B´ecares, Costero
Valero, and G´omez Mart´ın 2017). However, recorded ac-
tions are not robust to level changes, not exhaustive and can-
not verify that forbidden areas are unreachable. Other ap-
proaches include those that combine reinforcement learning
with curiosity-based reward bonuses (Gordillo et al. 2021;
Sestini et al. 2022; Liu et al. 2022). Even so, these methods
must use approximations for the vast observation spaces that
feature in modern games, and have been shown under these
settings to be brittle to catastrophic forgetting (Ecoffet et al.
2019).

In contrast, we propose to use Go-Explore (Ecoffet et al.
2019), a simple but powerful exploration algorithm which

 
 
 
 
 
 
(a) Small Map

(b) Large Map

(c) Traversal Map

Figure 2: We evaluate Go-Explore for reachability testing on three proprietary test game levels of increasing difﬁculty designed
in Unreal Engine 5. The smallest is 100m x 100m and the largest 1.5km x 1.5km which matches the scale of modern game
maps. We initially sample points from the nav-mesh which act as ‘goals’ for exploration. Reached and unreached goals at the
end of training are colored in green and red respectively. We additionally color non-goal points we discover as magenta if they
are within 3m of the nav-mesh and yellow otherwise. These yellow points are discovered positions that an agent would not be
able to reach by simply following the nav-mesh, and are often indicative of an unexpected bug. For example, on the small map,
the yellow points on the south wall point to an incorrectly conﬁgured section which allows the agent to reach the top of the wall
and fall off the map leading to undeﬁned behavior. Yellow points on the left and bottom of the large map show places where
the agent can escape the intended geometry.

maintains a cache of discovered locations, and identiﬁes
promising locations with heuristics to reset and explore
from. We show that Go-Explore can readily handle paral-
lelization and rapidly explore vast game worlds of many
square miles. Furthermore, we ﬁnd empirically that random
exploration is asymptotically better than learned exploration
such as Random Network Distillation (Burda et al. 2019)
under the same conditions which makes our approach easier
to implement.

The core contributions of this paper are as follows:

1. We apply a simple and efﬁcient algorithm, Go-Explore,
for discovering reachable positions in hard exploration
maps without human demonstrations or knowledge of the
game dynamics.

2. We propose a criteria to classify discovered points into
expected and unexpected by comparison to the game
navigation mesh. Undiscovered regions of the navigation
mesh may also be ﬂagged.

3. We show that by parallelizing agents within a single
game instance, we can exhaustively cover vast game
maps of the size 1.5km x 1.5km within 10 hours on a sin-
gle machine, making our algorithm extremely promising
for continuous testing suites.

Preliminaries
We begin by introducing traditional pathing in video games
via navigation meshes, the reinforcement learning (RL)

paradigm, and exploration algorithms in RL. In this paper,
we consider the problem of automatically testing for “reach-
ability” which we deﬁne to be determining the set of loca-
tions of the map an agent can reach. Note this is different
from the traditional deﬁnition of graph reachability as we
assume no knowledge of the environment dynamics to con-
nect adjacent positions, and actions may have stochasticity.

Navigation Meshes. AI agents in modern video games
typically use a navigation mesh (Snook 2000; Tozour and
Austin 2002; Graham, McCabe, and Sheridan 2003) or
“nav-mesh” to navigate from one area of a map to another.
A nav-mesh is a collection of two-dimensional convex poly-
gons (Dunn and Parberry 2011) which deﬁne areas of the
map that are traversable by agents. Within each polygon, an
agent can freely move without being obstructed by any en-
vironment obstacles such as trees and rocks. Adjacent poly-
gons are connected together in a graph and pathﬁnding be-
tween them can be done with classic graph search algorithms
such as A* (Hart, Nilsson, and Raphael 1968).

A na¨ıve method of verifying reachability may be to send
an agent to every node in the nav-mesh, one by one. How-
ever, the nav-mesh may not cover the entire map and there
are often gaps between patches of the nav-mesh that can-
not be traversed by just walking. Instead, we may use the
nav-mesh as an initial starting point of positions we expect
our agent to reach. By comparing the results of an explo-
ration algorithm versus this ground truth, we can categorize

observed positions as expected or unexpected and identify
positions on the nav-mesh that we expect to reach but have
not been reached which we illustrate in Figure 2.

Algorithm 1: Compute Reachability (Exploration Phase of
Go-Explore)
1: Input: reset priority function η : S → R, total timesteps T ,

Reinforcement Learning. A natural way to accelerate
game testing would be to try and train agents to explore
and ﬁnd novel states. Reinforcement Learning (Kaelbling,
Littman, and Moore 1996; Mnih et al. 2013; Sutton and
Barto 2018; Tesauro 1995) is a successful paradigm for
learning intelligent agents where optimal behavior may be
speciﬁed by a reward function. We model the game environ-
ment as a Markov Decision Process (MDP, Sutton and Barto
(2018)), deﬁned as a tuple M = (S, A, P, R, ρ0, γ), where
S and A denote the state space and action space respec-
tively, P (s(cid:48)|s, a) the transition dynamics, R(s, a) the reward
function, ρ0 the initial state distribution, and γ ∈ (0, 1)
the discount factor. The goal in RL is to optimize a pol-
icy π(a|s) that maximizes the expected discounted return
Eπ,P,ρ0 [(cid:80)∞

t=0 γtR(st, at)].

Exploration in RL. Count-based exploration describes a
family of algorithms that assign reward bonuses based on
visitation count to encourage agents to seek out novel states.
For ﬁnite state MDPs, we deﬁne the visitation count n(s) of
a state s ∈ S to be the number of times a particular state
has been encountered. Prior methods (Ostrovski et al. 2017;
Bellemare et al. 2016; Tang et al. 2017) have proposed to
assign a reward bonus of rt = 1/n(s) or rt = 1/(cid:112)n(s) at
each time step. These methods may be extended to MDPs
with continuous state spaces by treating n(s) as a density.
Even so, count-based methods are hard to scale to larger
state spaces and cannot naturally handle environments in
parallel.

Random Network Distillation (RND, Burda et al. (2019))
is a ﬂexible way to compute an exploration bonus for high-
dimensional state spaces by estimating the error of a neural
network predicting features of the game observations given
by a ﬁxed randomly initialized neural network. Concretely,
the ﬁxed randomly initialized target network f : S → Rk
computes an embedding and the predictor network ˆfθ : S →
Rk is trained by gradient descent to minimize the prediction
error ||f (s) − ˆfθ(s)||2 with respect to its parameters, θ. This
prediction error is then used as the reward bonus. Over the
course of training the randomly initialized neural network
f is distilled into ˆfθ. Intuitively, the prediction error is ex-
pected to be higher for novel states dissimilar to the ones
that the predictor has been trained on.

Go-Explore. Go-Explore (Ecoffet et al. 2019, 2021) is
an alternate approach for hard-exploration problems which
maintains a cache of previously explored states and then
uses heuristics to periodically select promising states and
then explore from them. Once sufﬁciently high return tra-
jectories are found, the discovered behavior is made robust
with imitation learning (Hussein et al. 2017). This algo-
rithm aims to avoid the phenomenon of “derailment” that
may occur in algorithms which do not have explicit memory
such as RND where the predictor network may lose infor-
mation about previously reached states due to catastrophic

Sdisc.insert(st)

Observe state st
if Sdisc = ∅ or d(Sdisc, st) > K then

reset interval treset, distance threshold K
2: Initialize: Sdisc = ∅, set of visited positions
3: for t = 1, . . . , T do
4:
5:
6:
7:
8:
9:
10:
11:
end if
12:
13: end for

end if
if t mod treset = 0 then

Sample state st+1 from Sdisc using η

else

Take random action from st to transition to st+1

forgetting (Kirkpatrick et al. 2017; French 1999). This phe-
nomenon could be even likelier to occur in large maps.

The ﬁrst version of Go-Explore (Ecoffet et al. 2019) as-
sumes full access to a simulator of the environment and the
ability to reset to any state previously seen, i.e. being able to
choose ρ0 throughout training. This assumption is satisﬁed
for the video games we test as we are able to save and restore
simulator states.

Go-Explore for Simulated 3D Environments

In this section, we describe our adaptation of Go-Explore
to 3D video games and the speciﬁc reset heuristics we use
to identify promising previously discovered locations. Since
we do not need to follow the second phase of Go-Explore
and imitate an optimal trajectory, the algorithm listed in Al-
gorithm 1 simply builds a set of discovered positions Sdisc
and thus resembles the ﬁrst phase of Go-Explore with a tai-
lored reset strategy.

Given a set of 3D positions X and a distance metric
d, we deﬁne the distance from the set X to a point y,
d(X, y) := min{d(x, y)|x ∈ X}.1 We deﬁne two styles of
reset heuristic: one based on state visitation counts, and the
other guided by distance to the closest undiscovered point
on the nav-mesh.
Visitation-Based. We discretize the s = (x, y, z) posi-
tions in the game to a ﬁxed granularity K and then main-
tain visitation counts of each state. This allows us to reset to
1
a state in Sdisc with probability proportional to
n(s) where
n(s) is the visitation count. By default, we measure distance
in meters and use a granularity of K = 1, i.e. a new position
is deemed to be reached if it is more than one meter away
from any previous position.

Nav-Mesh Goal Guided. We use the game nav-mesh to
produce a set of points Snav-mesh or “goals” that we expect
to be reachable but have not reached yet as a guide to ex-
ploration. This set is constructed by sampling points at a

1This can be efﬁciently computed by data structures such as
R*-Trees (Beckmann et al. 1990) which support nearest neighbor
queries in amortized log time.

Figure 3: Comparative evaluation of Go-Explore against the baselines showing the percentage of nav-mesh goals (illustrated
in Figure 2) reached on the top and unique positions discovered (up to 1m discretization) on the bottom. We note that Go-
Explore vastly outperforms RND and pure random exploration especially on the larger maps and is reasonably stable to reset
heuristic. Results show mean and standard deviation computed over 4 seeds.

pre-speciﬁed threshold at initialization. Throughout train-
ing, we update Snav-mesh by draining any goals that are within
one meter of a discovered position. We may then reset to
discovered states in Sdisc with probability proportional to
1
d(Snav-mesh,·) .

We may also consider a weighted combination of the two
reset heuristics as in Ecoffet et al. (2019) to deﬁne a custom
reset priority function η : S → R. This allows agents to bal-
ance between the two reset styles and avoid the failure mode
of persistently attempting to reset close to an unreachable
nav-mesh goal. Once we reset to a promising state, we fol-
low Ecoffet et al. (2019) and take random actions to explore.
We visualize the reset heuristics in Figure 1. The notion of a
goal generated from the nav-mesh could also be extended to
goals on any interesting part of the map.

Experimental Evaluation
We evaluate our approach on proprietary test game levels
designed in Unreal Engine 5. We consider three maps of in-
creasing difﬁculty:

• Small Map: A small test bed with obstacles and two in-

tentionally placed bugs (approx. 100m x 100m)

• Large Map: A vast cityscape to test the scalability of
each algorithm (approx. 1.5km x 1.5km). This map is
among the largest considered by modern automated ap-
proaches.

• Traversal Map: A large multi-level map with challeng-

ing geometry as a hard exploration challenge.

The game uses rich 3D observations with optional raycast,
position and rotation features. Agents move using a multi-
discrete action space matching that of a standard game con-
troller. The maps are illustrated in Figure 2.

For Go-Explore, we evaluate all reset heuristics previ-
ously described in the previous section—visitation-based,

goal-based and mixed. We use a default distance threshold
of K = 1 for adding a new point and use the same thresh-
old for evaluating how many unique positions an agent vis-
its over the course of training. The agents are reset every
treset = 128 steps. We ﬁrst evaluate the performance of Go-
Explore against the baselines, RND (optimized using Prox-
imal Policy Optimization, Schulman et al. (2017)) and ran-
dom exploration, across the three test maps. Next, we ablate
various components showing ﬁrst, that Go-Explore is stable
to hyperparameter choice and secondly, random exploration
is critical for strong performance on large-scale maps.

For each experiment, we accelerate training by running
16 independent agents within each map that cannot inter-
act with or see each other. Each agent is synchronized with
a central cache of discovered positions. We train for 200K
timesteps on the small map and 10M timesteps for both large
maps, and repeat each experiment with 4 random seeds. Due
to our use of parallelization, wall-clock time for the small
map is around 30 minutes, whereas the large maps only take
10 hours to cover on a single F8 Azure Virtual Machine. We
use a GeForce RTX 2080 GPU only for the RND baseline.

Main Evaluation
We ﬁrst show that our algorithm can discover the intention-
ally placed bugs on the small map and that Go-Explore re-
liably outperforms the baselines across a variety of conﬁgu-
rations. Next, on the large maps, we ﬁnd the difference be-
tween Go-Explore and the baselines grows more stark with
the baselines making little progress. On these maps, Go-
Explore can cover the vast game worlds and discover many
unexpected regions off-map.

Bugs on the Small Map. We compare Go-Explore with
all three reset heuristics on the left of Figure 3 evaluating
coverage of the nav-mesh goals and unique positions found.
On this map, all three reset heuristics are comparable and

0.00.51.01.52.01e50255075100Goals Found (%)Small Map0.00.20.40.60.81.01e70255075100Large Map0.00.20.40.60.81.01e70255075100Traversal Map0.00.51.01.52.01e5010002000Positions Reached0.00.20.40.60.81.01e701000002000000.00.20.40.60.81.01e7010000200003000040000TimestepsGo-Explore (Visitation)Go-Explore (Goals)Go-Explore (Mixed Visit+Goals)RNDRandomTable 1: Ablations on the Small Map showing Go-Explore
is robust to hyperparameter changes with all setups achiev-
ing 100% of the goals before 200K timesteps. The default
setting uses a threshold of 1m and a reset weight power of 1.
Mean and standard deviation shown over 4 seeds.

Reset
Type

Hyper-
parameters

Timesteps to
All Goals

Positions
Found

Goals

Position

Default
K = 5
p = 1/2
p = 2

Default
K = 5
p = 1/2
p = 2

128K
110K
167K
106K

119K
90K
156K
111K

2485.2 ± 21.4
2564.0 ± 7.9
2430.8 ± 37.6
2422.5 ± 14.8

2649.5 ± 30.6
2736.0 ± 14.8
2541.2 ± 21.3
2705.8 ± 36.8

as we use a R*-Tree (Beckmann et al. 1990) to cache posi-
tions. At the end of exploration on the large map, the ∼200K
discovered points take around 1MB to store. In a tree this
size, checking and inserting a newly discovered point also
takes less than a millisecond on average which is far less
than the rate of simulation for the game.

Ablation Studies
In this subsection, we present ablation studies showing that
Go-Explore is stable to hyperparameter choice; and show-
ing the beneﬁt of random exploration over RND even when
both have smart resetting. We further show ablations on Go-
Explore using uniform sampling to show that the design of
reset heuristic is important.

Hyperparameters. We present ablations on the distance
threshold K required to add a new point to our set of visited
positions Sdisc on the small map in Table 1. We also include
ablations on the power of the reset heuristic p, i.e. resetting
using the heuristic ηp instead of η. The original Go-Explore
paper used a power of p = 1
2 , i.e. taking the square root
of all scores, however we ﬁnd that p = 1 works better for
our use case which matches Kolter and Ng (2009). We see
a marginal beneﬁt from using K = 5 for position based
resetting, however we believe K = 1 could lead to better
checkpointing on hard exploration environments.

RND vs. Random Exploration. We show that even when
allowing the RND agent to reset to promising positions
in the same way as Go-Explore on the large map, labeled
“RND + Go-Exp. Sampling” in Figure 5, the agent still
under-performs Go-Explore (Random + Go-Exp. Sampling)
in terms of ﬁnal performance. However, we do note that
RND-based exploration (in dashed lines) does tend to be
initially better but then trails off. This could imply that the
learned exploratory behavior from RND does not transfer to
later regions of the map, perhaps preferring areas it has ob-
tained reward from in the past. This lends further support
to our use of Go-Explore style random exploration which
requires no neural network overhead and is simpler to im-
plement.

Figure 4: Illustration of a bug on the small map surfaced by
Go-Explore. The goal (in green) on the wall means the agent
is unexpectedly able to reach the top of the wall and then fall
off the map.

are strong in terms of positions reached and goals found. We
manage to consistently ﬁnd the two bugs that were placed on
the small map, one where an incorrectly conﬁgured point led
to agents being able to reach the top of the bounding wall and
fall off the map which we illustrate in Figure 4 and another
where an unreachable volume was incorrectly placed so an
agent could incorrectly enter into a solid block. The wall bug
can also be identiﬁed in Figure 2 where the cluster of yellow
points on the south wall of the map are unexpected and then
can easily be ﬂagged to the level developer.

Large and Traversal Maps. We perform the same com-
parison as before on the larger maps as shown in the center
and right side of Figure 3. On these maps, the difference be-
tween Go-Explore and the baselines grows more stark with
Go-Explore being the only algorithm that can reliably cover
at least the nav-mesh positions. We also notice a difference
between different styles of reset heuristic with Go-Explore.
On the large map, whilst the goal-based reset heuristic leads
to the quickest coverage of the nav-mesh, it discovers far less
unique positions than the reset heuristics based on visitation
count. This advocates for reset heuristics which can jointly
encode information about undiscovered nav-mesh goals and
visitation.

In Figure 2, we can see discovered regions of both maps
which are far away from the nav-mesh which are unex-
pected. In the case of the traversal map, these correspond
to being able to reach the top of large structures and fall
off to unexpected regions. This is a common source of
reachability bugs and the automated ﬂagging of problem-
atic areas promises to save level designers and QA signiﬁ-
cant amounts of time. We additionally note that Go-Explore
scales well with map size—when comparing the small and
large map, Go-Explore only takes ∼50x more timesteps to
cover ∼200x more surface area, when comparing both nav-
meshes. Therefore, our algorithm may readily be integrated
into nightly continuous testing suites as it only takes 10
hours on a single machine to fully cover a vast 1.5km x
1.5km game world. This could be further accelerated with
parallel game instances.

Go-Explore is also efﬁcient in terms of memory and time

space. CA-RRT (Zuo et al. 2022) improves on RRT by aug-
menting search with human demonstrations but is still only
evaluated on relatively small maps. SPTM (Savinov, Doso-
vitskiy, and Koltun 2018) navigates by building a graph of
the environment and learning to retrieve nodes for planning.

Conclusion
In this paper, we show that Go-Explore allows for scalable,
efﬁcient and learning-free reachability testing of the vast 3D
game maps typically found in modern AAA video games
without the need for any human demonstration or knowledge
of the game dynamics. In contrast to prior curiosity-based
RL approaches, Go-Explore avoids the issue of catastrophic
forgetting by maintaining a cache of discovered positions.
The random exploration advocated by Ecoffet et al. (2019)
is signiﬁcantly simpler to implement and avoids the use of
neural networks. By ﬂagging discovered points that are far
away from the nav-mesh, we are able to cheaply identify
reachability issues and highlight problematic points on the
map. Due to the fast run-time and ease of implementation
of the algorithm, we expect Go-Explore may be readily in-
tegrated into a continuous testing framework. In doing so,
bugs may be found as each level is updated and signiﬁcant
resources may be saved in quality assurance.

Acknowledgments
We would like to thank Sam Devlin, Anssi Kanervisto,
Tabish Rashid, Tim Pearce, Mingfei Sun, and John Langford
for reviewing earlier drafts of this work and many insightful
discussions. We are also grateful to the anonymous AIIDE
2022 Workshop on Experimental AI in Games reviewers for
helpful and constructive feedback which helped to improve
the paper.

References
Albaghajati, A. M.; and Ahmed, M. A. K. 2020. Video game
automated testing approaches: An assessment framework.
IEEE Transactions on Games.
Ariyurek, S.; Betin-Can, A.; and Surer, E. 2021. Auto-
mated Video Game Testing Using Synthetic and Humanlike
Agents. IEEE Transactions on Games, 13(1): 50–67.
Beckmann, N.; Kriegel, H.-P.; Schneider, R.; and Seeger, B.
1990. The R*-tree: An efﬁcient and robust access method
for points and rectangles. In Proceedings of the 1990 ACM
SIGMOD international conference on Management of data,
322–331.
Bellemare, M. G.; Srinivasan, S.; Ostrovski, G.; Schaul, T.;
Saxton, D.; and Munos, R. 2016. Unifying Count-Based
Exploration and Intrinsic Motivation.
Bergdahl, J.; Gordillo, C.; Tollmar, K.; and Gissl´en, L. 2020.
Augmenting Automated Game Testing with Deep Rein-
forcement Learning. In 2020 IEEE Conference on Games
(CoG), 600–603.
Bulut, E. 2015. Playboring in the Tester Pit: The Conver-
gence of Precarity and the Degradation of Fun in Video
Game Testing. Television & New Media, 16(3): 240–258.

Figure 5: Ablations for both random and RND-based explo-
ration under visitation-based sampling (in blue) and uniform
sampling (in green) on the large map. This provides further
support for purely random exploration, and shows that well-
designed reset heuristics beat na¨ıve uniform sampling. Re-
sults show mean and standard deviation computed over 4
seeds.

Go-Explore Heuristics vs. Uniform Sampling. We fur-
ther analyze the design choices involved in selecting sam-
pling heuristics and ask how much better they are than just
sampling visited points uniformly at random. This corre-
sponds to the comparison between Go-Exp. Sampling (in
green) vs. Unif. Sampling in Figure 5 (in blue). Although
uniform sampling hugely boosts performance over simply
resetting from the start position (in red), it does not reach
the same levels of performance as with the carefully chosen
heuristics.

Related Work
Go-Explore has seen extensive use in discovering optimal
policies for video games like Atari (Ecoffet et al. 2019); but
to our knowledge, it has not seen use in reachability testing
for large-scale modern game environments.

Learning to Explore. Curiosity and imitation-based ap-
proaches where agents learn to explore have been success-
fully used to automate game testing in past work. Gordillo
et al. (2021) showed that a count-based exploration bonus
with additional bonuses for reaching corners of the map ap-
proach with visitation-based resets was successful in cov-
ering a 500m x 500m map. In contrast, we don’t need a
reward signal and our resets happen at a much higher fre-
quency. CCPT (Sestini et al. 2022) used a hybrid approach
combining RND and imitating human trajectories. However,
imitation-based approaches are liable to break with large
map changes during development. Inspector (Liu et al. 2022)
combines RND with a separate module that attempts to iden-
tify and interact with key objects in a scene. The interaction
module could be a promising orthogonal extension to Go-
Explore. Bergdahl et al. (2020) consider an approach where
an agent is reward on reaching pre-speciﬁed goals which in-
herently biases against exploring other parts of the game.

Graph-based Search. Traditional search-based methods
have shown promise in small games but are harder to scale.
Rapidly-Exploring Random Trees (RRT, Zhan, Aytemiz,
and Smith (2018)) grows a state tree and labels edges with
actions; however is limited by the size of the game action

0.00.20.40.60.81.01e7020406080100Goals Found (%)0.00.20.40.60.81.01e7050000100000150000200000250000Positions ReachedRandom + Go-Exp. SamplingRND + Go-Exp. SamplingRandom + Unif. SamplingRND + Unif. SamplingRandomRNDReset Heuristic AblationsTimestepsFirst return,

then explore.

Burda, Y.; Edwards, H.; Storkey, A.; and Klimov, O. 2019.
Exploration by random network distillation. In International
Conference on Learning Representations.
Chang, K.; Aytemiz, B.; and Smith, A. M. 2019. Reveal-
More: Amplifying Human Effort in Quality Assurance Test-
In 2019 IEEE Confer-
ing Using Automated Exploration.
ence on Games (CoG), 1–8.
3D Math Primer for
Dunn, F.; and Parberry, I. 2011.
Graphics and Game Development. Boca Raton, FL: A
K Peters/CRC Press, second edition.
ISBN 1439869812
9781439869819.
Ecoffet, A.; Huizinga, J.; Lehman, J.; Stanley, K. O.; and
Clune, J. 2019. Go-Explore: a New Approach for Hard-
Exploration Problems.
Ecoffet, A.; Huizinga, J.; Lehman, J.; Stanley, K. O.; and
Nature,
Clune, J. 2021.
590(7847): 580–586.
French, R. M. 1999. Catastrophic forgetting in connectionist
networks. Trends in Cognitive Sciences, 3(4): 128–135.
Gordillo, C.; Bergdahl, J.; Tollmar, K.; and Gissl´en, L. 2021.
Improving Playtesting Coverage via Curiosity Driven Rein-
forcement Learning Agents.
Graham, R.; McCabe, H.; and Sheridan, S. 2003. Pathﬁnd-
ing in computer games. ITB Journal, 8: 57–81.
Hart, P. E.; Nilsson, N. J.; and Raphael, B. 1968. A For-
mal Basis for the Heuristic Determination of Minimum Cost
IEEE Transactions on Systems Science and Cyber-
Paths.
netics, 4(2): 100–107.
Hern´andez B´ecares, J.; Costero Valero, L.; and G´omez
Mart´ın, P. P. 2017. An approach to automated videogame
beta testing. Entertainment Computing, 18: 79–92.
Hussein, A.; Gaber, M. M.; Elyan, E.; and Jayne, C. 2017.
Imitation learning: A survey of learning methods. ACM
Computing Surveys (CSUR), 50(2): 1–35.
Kaelbling, L. P.; Littman, M. L.; and Moore, A. W. 1996.
Reinforcement Learning: A Survey. Journal of artiﬁcial in-
telligence research, 4: 237–285.
Kirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Des-
jardins, G.; Rusu, A. A.; Milan, K.; Quan, J.; Ramalho,
T.; Grabska-Barwinska, A.; Hassabis, D.; Clopath, C.; Ku-
maran, D.; and Hadsell, R. 2017. Overcoming catastrophic
forgetting in neural networks. Proceedings of the National
Academy of Sciences, 114(13): 3521–3526.
Kolter, J. Z.; and Ng, A. Y. 2009. Near-Bayesian Explo-
ration in Polynomial Time. In Proceedings of the 26th An-
nual International Conference on Machine Learning, ICML
’09, 513–520. New York, NY, USA: Association for Com-
puting Machinery. ISBN 9781605585161.
Lewis, C.; Whitehead, J.; and Wardrip-Fruin, N. 2010. What
went wrong: a taxonomy of video game bugs. In Proceed-
ings of the ﬁfth international conference on the foundations
of digital games, 108–115.
Liu, G.; Cai, M.; Zhao, L.; Qin, T.; Brown, A.; Bischoff,
J.; and Liu, T.-Y. 2022. Inspector: Pixel-Based Automated
Game Testing via Exploration, Detection, and Investigation.
ArXiv, abs/2207.08379.

Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.;
Antonoglou, I.; Wierstra, D.; and Riedmiller, M. A. 2013.
Playing Atari with Deep Reinforcement Learning. CoRR,
abs/1312.5602.
Nantes, A.; Brown, R.; and Maire, F. 2008. A framework
for the semi-automatic testing of video games. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence and
Interactive Digital Entertainment, volume 4, 197–202.
Ostrovski, G.; Bellemare, M. G.; van den Oord, A.; and
Munos, R. 2017. Count-Based Exploration with Neural
Density Models. In Precup, D.; and Teh, Y. W., eds., Pro-
ceedings of the 34th International Conference on Machine
Learning, volume 70 of Proceedings of Machine Learning
Research, 2721–2730. PMLR.
Politowski, C.; Petrillo, F.; and Gu´eh´eneuc, Y.-G. 2021. A
Survey of Video Game Testing. In 2021 IEEE/ACM Inter-
national Conference on Automation of Software Test (AST),
90–99.
Savinov, N.; Dosovitskiy, A.; and Koltun, V. 2018. Semi-
parametric topological memory for navigation. In Interna-
tional Conference on Learning Representations.
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov, O. 2017. Proximal Policy Optimization Algorithms.
Sestini, A.; Gissl´en, L.; Bergdahl, J.; Tollmar, K.; and Bag-
danov, A. D. 2022. CCPT: Automatic Gameplay Testing
and Validation with Curiosity-Conditioned Proximal Trajec-
tories.
Snook, G. 2000. Simpliﬁed 3D Movement and Pathﬁnding
Using Navigation Meshes. In DeLoura, M., ed., Game Pro-
gramming Gems, 288–304. Charles River Media.
Sutton, R. S.; and Barto, A. G. 2018. Reinforcement Learn-
ing: An Introduction. The MIT Press, second edition.
Tang, H.; Houthooft, R.; Foote, D.; Stooke, A.; Chen, X.;
Duan, Y.; Schulman, J.; Turck, F. D.; and Abbeel, P. 2017.
#Exploration: A Study of Count-Based Exploration for Deep
Reinforcement Learning.
In Guyon, I.; von Luxburg, U.;
Bengio, S.; Wallach, H. M.; Fergus, R.; Vishwanathan, S.
V. N.; and Garnett, R., eds., Advances in Neural Informa-
tion Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA, 2753–2762.
Tesauro, G. 1995. Temporal difference learning and TD-
Gammon. Communications of the ACM, 38(3): 58–68.
Tozour, P.; and Austin, I. 2002. Building a near-optimal nav-
igation mesh. AI game programming wisdom.
Truelove, A.; Santana de Almeida, E.; and Ahmed, I. 2021.
We’ll Fix It in Post: What Do Bug Fixes in Video Game Up-
date Notes Tell Us? In 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE), 736–747.
Zhan, Z.; Aytemiz, B.; and Smith, A. M. 2018. Taking the
Scenic Route: Automatic Exploration for Videogames.
Zuo, M.; Schick, L.; Gombolay, M.; and Gopalan, N. 2022.
Efﬁcient Exploration via First-Person Behavior Cloning As-
sisted Rapidly-Exploring Random Trees.

