Few-shot Adaptation Works with UnpredicTable Data

Jun Shern Chan1 2 Michael Pieler1 2 Jonathan Jao1 2

Jérémy Scheurer1 2

Ethan Perez1 2 3∗
1New York University, 2Fund for Alignment Research, 3Anthropic
{junshern,perez}@nyu.edu

2
2
0
2

g
u
A
8

]
L
C
.
s
c
[

2
v
9
0
0
1
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

Prior work on language models (LMs) shows
that training on a large number of diverse
tasks improves few-shot learning (FSL) perfor-
mance on new tasks. We take this to the ex-
treme, automatically extracting 413,299 tasks
from internet tables - orders of magnitude
more than the next-largest public datasets.
Finetuning on the resulting dataset leads to
improved FSL performance on Natural Lan-
guage Processing (NLP) tasks, but not propor-
tionally to dataset scale. In fact, we ﬁnd that
narrow subsets of our dataset sometimes out-
perform more diverse datasets. For example,
ﬁnetuning on software documentation from
support.google.com raises FSL perfor-
mance by a mean of +7.5% on 52 downstream
tasks, which beats training on 40 human-
curated NLP datasets (+6.7%). Finetuning on
various narrow datasets leads to similar broad
improvements across test
tasks, suggesting
that the gains are not from domain adapta-
tion but adapting to FSL in general. We do
not observe clear patterns between the datasets
that lead to FSL gains, leaving open questions
about why certain data helps with FSL.

1

Introduction

Brown et al. (2020) showed that language models
(LMs) learn to perform new tasks from a few exam-
ples (“few-shot learning”; FSL). Explicitly training
LMs for FSL further improves performance (Min
et al., 2021; Chen et al., 2021b), and prior work
has found that increasing the size and diversity of
training tasks improves generalization to new tasks
(Sanh et al., 2021; Aribandi et al., 2021; Agha-
janyan et al., 2021a; Wang et al., 2022). We push
size and diversity to the extreme by ﬁnetuning on
a large dataset of automatically-curated FSL tasks,
and surprisingly ﬁnd that certain narrow datasets
of tasks (e.g. software documentation) outperform
much larger and more diverse datasets.

Figure 1: We convert a wide variety of tables into
tasks for few-shot learning (FSL), then use these tasks
via ﬁnetuning to adapt language models for FSL. Un-
expected tables lead to strong task transfer results:
ﬁnetuning GPT2 on software documentation from
support.google.com outperforms ﬁnetuning on
40 curated NLP datasets on average across 52 test tasks,
with strong improvements across diverse tasks includ-
ing article classiﬁcation (+47%), sentiment classiﬁca-
tion (+31%) and scientiﬁc question-answering (+23%).

Investigations into dataset size and diversity re-
quires a large dataset of FSL tasks. To this end, we
explore tables as a naturally-occurring source of
diverse FSL tasks. Given a table where each row is
a list of ﬁelds, we hold out one row as the test exam-
ple and treat all other rows as task training exam-
ples. We apply this idea to automatically convert in-
ternet tables into UnpredicTable1, a dataset of

∗Work done primarily at NYU and FAR.

1https://github.com/JunShern/few-shot-adaptation

If you want to ...Then ...Report spamSubmit a spam report.Get a page or site removeSubmit a URL removal request.Tell Google to crawl your sRequest a change in crawl rate.. . .. . .[If you want to ...] Report spam [Then ...]Submit a spam report.InputInputOutput[If you want to ...] Get a page or site removSubmit a URL removal request.InputInputOutput. . .Input. . .}4)   Outperform  in few-shot task transfer?!multi-task training with 
40 NLP datasetsScrape HTML tables from .support.google.com1Convert tables to few-shot tasks.2    Fine-tune an LM on the generated tasks.34 
 
 
 
 
 
413,299 diverse few-shot tasks. We ﬁnetune GPT-2
to perform a new task given a few task examples
in its context (“MetaICL”; Min et al., 2021). Fine-
tuning on UnpredicTable leads to strong FSL
performance on average over 52 NLP test tasks,
comparable to ﬁnetuning on human-curated NLP
datasets. However, the observed gains fall short of
expectations for such a large dataset.

To understand why our gains were limited, we
perform various ablations on dataset size, diver-
sity, and content. In this process, we ﬁnd that ﬁne-
tuning on narrow subsets of UnpredicTable
outperforms ﬁnetuning on our diverse dataset and
on curated NLP data. Surprisingly, datasets that
we handpick according to what we expect to be
helpful are not strongly correlated with perfor-
mance. In fact, the training datasets that lead
to strong improvements are often counterintu-
itive, covering trivia content (e.g. video games on
mmo-champion.com and software documenta-
tion from support.google.com; see Fig. 1)
that are unrelated to downstream test tasks. Finetun-
ing on these narrow datasets cause broad improve-
ments similar to ﬁnetuning on curated NLP datasets
when compared on the same test tasks. This sug-
gests that these aren’t domain- or task-speciﬁc im-
provements, but improvements in general few-shot
ability (“few-shot adaptation”). Our work calls into
question common wisdom that adapting LMs to
FSL requires diverse, high-quality training data.

2 Web Tables as a Source of Few-Shot

Learning Tasks

We begin by describing FSL, which is the problem
of learning from a small number of training exam-
ples. We make the case that web tables can be used
as a diverse source of few-shot tasks. Then, we
introduce our algorithm for converting tables into
tasks and apply this to produce UnpredicTable,
a dataset of 413,299 few-shot tasks.

2.1 Few-Shot Learning Tasks

We deﬁne a task T as a set of input-output pairs
T = {(xi, yi)}k
i=1 where inputs xi map to outputs
yi. Task types can be very diverse, from question-
answering (Questions → Answers), to summariza-
tion (Books → Summaries), to translation (French
→ English). In FSL, k is small. LMs can be used to
perform FSL by providing k known example pairs
{(xi, yi) : i = 1, . . . , k} in the LM context at infer-
ence time. Then, we give the model a new example

xtarget for which ytarget is unknown, and we use the
model to predict ytarget.

2.2 Tables Dataset

Motivated by prior work on FSL adaptation (Min
et al., 2021; Chen et al., 2021b) and multi-task
learning (Sanh et al., 2021; Aribandi et al., 2021;
Aghajanyan et al., 2021a), we hypothesize that we
can extend the results of multi-task FSL ﬁnetun-
ing with an even larger set of few-shot tasks. We
make the case that web tables are a large and di-
verse source of few-shot tasks. Consider a table
where each row is an instance of a similar class
and columns describe the attributes of an instance.
We use each row as an example of a task, where
the task is ﬁlling in missing attributes in a row. For
a table with k rows, each table becomes a k-shot
dataset for a particular task.

As a source of table data, we use tables from the
English-language Relational Subset of the WDC
Web Table Corpus 2015 (WTC)2. The WTC dataset
was extracted from the July 2015 Common Crawl
web corpus, and contains 50M tables from 323K
web domains. We focus on relational tables, which
describe a set of similar items along with their at-
tributes. For example, a table listing national dishes
by country is a relational table. On the other hand,
a table describing a single item where each row de-
scribes a different attribute is not relational. WTC
also provides helpful metadata including the source
URL, title, and header rows.

2.3 Turning Tables Into Tasks

In practice, there are important design choices for
converting a table into a task of input-output pairs.
Here, we describe our chosen procedure. We start
with the assumption that items in the relational ta-
ble are listed row-wise (as in Fig. 2) instead of
column-wise. Where necessary, we transpose the
tables to suit our requirement. To convert a row
into an input-output task pair, we consider a single
column as a potential output target yi and concate-
nate the remaining columns to form the input xi.
For additional context, we preﬁx each value with
its column header (see Fig. 2). Since any column is
a potential output target, we create multiple tasks
per table. For example, a table with 3 columns
A, B, and C may be cast as three different tasks:
P (A|B, C), P (B|A, C) and P (C|A, B).

2webdatacommons.org/webtables/2015/EnglishStatistics.html

Figure 2: An algorithm to convert tables into tasks for FSL: Given the task of "Predict this column value given the
other column values as input," each row in the table can be used as an example for that task.

Filtering tables We reject tables with fewer than
2 unique columns (one for the task output and at
least one more for the input) or 6 unique rows
(at least 5 examples + 1 target row). We ﬁnd a
large number of tables containing junk data or only
numerical values. To remove these, we reject tables
with ≥ 20% of tokens tagged as either Numeral,
Proper Noun, Symbol, Punctuation, or Other by the
spaCy part-of-speech classiﬁer.3 The tables that
pass this ﬁltering stage are converted into tasks.

Filtering tasks Given a set of candidate tasks,
we require that the output space contains at least
two unique answers, and reject tasks with severe
class imbalance.4 To narrow our scope to tasks
with a single correct answer, we reject tasks where
any input appears more than once with different
outputs. Finally, we only accept up to 2500 tasks
per website to counter imbalance5 in the source
website of generated tasks. Appendix A shows the
breakdown of ﬁltered tables and tasks at each stage.
We apply our tables-to-tasks procedure to pro-
duce UnpredicTable, a dataset with 413,299
tasks from 23,744 unique websites. The shape
of our dataset is very different from most NLP
datasets: NLP datasets typically contain a handful
of tasks, with thousands of examples per task. On
the other hand, UnpredicTable contains 400K
tasks but most tasks have fewer than 50 examples.
Thus, our dataset has a large variety of tasks but
each task has limited training examples, true to the
small-k FSL setting. Our data-generation code and
corresponding dataset are open-source.6

3spacy.io/usage/linguistic-features#pos-tagging
4We measure class imbalance using Shannon Diversity

Index and reject scores lower than 0.7.

5Without data rebalancing, cappex.com makes up 41% of

the tasks.

6github.com/JunShern/few-shot-adaptation

3 Multitask Training with Few-shot
Tasks for Few-shot Adaptation

The shape of our dataset makes it suitable for mul-
titask learning algorithms. In multitask learning,
we have a training dataset Dtrain = {Ti}Mtrain
i=1 con-
taining Mtrain training tasks T , and a test dataset
Dtest with Mtest tasks which are disjoint to Dtrain.
The key idea is to use Dtrain to train a model to be
generalizable to new tasks in Dtest.

Here, we focus on the MetaICL algorithm (Min
et al., 2021) for few-shot adaptation, which has
shown strong FSL results across a variety of down-
stream tasks. We show additional experiments on
the CrossFit (Ye et al., 2021) and FLEX (Bragg
et al., 2021) benchmarks in Appendix C, to study
the generalization of our results across different
models, training algorithms and test tasks.

3.1 MetaICL

MetaICL (Min et al., 2021) trains LMs to pre-
dict the output for a target input, given a few
input-output pairs provided in the LM context.
On each training iteration, one task Ti is sam-
pled from Dtrain and k + 1 training examples
{(x1, y1), . . . , (xk+1, yk+1)} are sampled from Ti.
MetaICL trains an LM with parameters θ to maxi-
mize log P (yk+1|x1, y1, . . . , xk, yk, xk+1). At test
time, for a new task in Dtest we draw a set of exam-
ples {x1, y1, . . . , xk, yk} and a query xk+1. Given
this context, the LM uses θ to select the most likely
yk+1 from a discrete set of possible labels.

3.2 Experiments

investigate

how ﬁnetuning

on
Here, we
UnpredicTable compares
to ﬁnetuning
on human-curated NLP datasets. We ﬁnetune
the 774M parameter pretrained GPT2-large LM
(Radford et al., 2019), following Min et al. (2021).

y → oArchive anArchives you. . .. . .. . .g → aGo to ‘All Mail’Takes you to ‘All Mailg → dGo to ‘Drafts’Takes you to all draftShortcutDefinitionAction[Shortcut] g → s [Definition] Go to ‘Starred’ [Action]Takes you to all conversations you have starred.InputInputOutput[Shortcut] g → c [Definition] Go to ‘Contacts’[Action]Takes you to your Contacts list.InputInputOutput. . .Task 1DefinitionActionShortcut[Shortcut] g → s [Definition] Go to ‘Starred’ [Action]Takes you to all conversations you have starred.InputInputOutput[Shortcut] g → c [Definition] Go to ‘Contacts’[Action]Takes you to your Contacts list.InputInputOutput. . .Task 2ShortcutActionDefinition[Shortcut] y → o [Definition] Go to ‘Drafts’ [Action]Takes you to all drafts you have saved.InputInputOutput[Shortcut] g → a [Definition] Go to ‘All Mail’ [Action]Takes you to ‘All Mail’, the storage site for all maiInputInputOutput. . .Task 3ShortcutDefinitionActionRecipe to convert  into - tasks:
Simply predict a column value given the other columns!arbitrary tablesfewshotSee Appendix B for details on our hyperparameter
and ﬁnetuning setup.

NLP datasets and evaluation settings Min et al.
(2021) use 142 unique NLP tasks from Ye et al.
(2021) and Khashabi et al. (2020) to form Dtrain
and Dtest for 5 different NLP task categories: 26
Low Resource (LR) tasks with <1000 examples per
task, 8 Natural Language Inference (NLI) tasks to
test entailment between a premise and hypothesis
clause, 4 Paraphrase (Para) tasks that test the equiv-
alence of two differently-worded phrases, 20 Clas-
siﬁcation (Class) tasks, and 22 Question-Answering
(QA) tasks. We show results on each category. See
Appendix B for a full list of tasks.

MetaICL methods MetaICL evaluates perfor-
mance on each task category in two ways. First,
they consider an out of distribution (“OOD”) set-
ting, where they ﬁnetune a model on a dataset Dtrain
consisting of tasks from all other categories exclud-
ing the target task category. Second, for Class and
QA categories, they consider an in-domain (“IID”)
setting, where they ﬁnetune a model on a dataset
Dtrain consisting of only tasks from the same cate-
gory as the target task category.

Our dataset We sample M = 5000 tasks from
UnpredicTable, choosing M based on results
on a development set of tasks (Appendix B). We re-
fer to this dataset as UnpredicTable-5k. Min
et al. (2021) train one model per task category,
while we ﬁne-tune a single GPT2-large model
on UnpredicTable-5k and test the resulting
model on all task categories.

3.3 Results

Method
GPT2 0-shot
GPT2 k-shot
MetaICL k-shot trained with
NLP (OOD)
NLP (IID)
UnpredicTable-5k
(our dataset)

43.2
-
43.7

Task category [# test tasks]
LR Class QA NLI Para
34.2
40.4
34.9
33.7
40.2
38.2

25.5
34

34.2
37.4

38.2
43.4
46.1

38.7
45.9
42.3

49
-
36.3

33.1
-
45.7

Table 1: Columns represent different test settings; rows
represent different methods. MetaICL k-shot with ﬁne-
tuning on our dataset improves pretrained model per-
formance (GPT2 k-shot) on all test categories. Further-
more, ﬁnetuning on our tasks beats ﬁnetuning on out-
category NLP datasets (OOD) on 4/5 settings, and in-
category NLP datasets (IID) on 1/2 settings.

For each task category, we compute the mean
accuracy per task and report the average task ac-
curacy for all tasks in the category. Tab. 1 shows
the results. MetaICL ﬁnetuning on our table tasks
improves FSL performance on all test settings. Fur-
thermore, ﬁnetuning on our dataset outperforms
ﬁnetuning on OOD NLP tasks on 4/5 settings, and
IID NLP tasks on 1/2 settings. Overall, ﬁnetuning
on our data results in comparable performance to
ﬁnetuning on curated NLP tasks.

4 Why Is UnpredicTable Helpful?

To understand why UnpredicTable is helpful
training data, we construct subsets of the dataset
varying features we wish to study. For each sub-
dataset, we ﬁnetune on that dataset individually
following the setup as before (Appendix B) and
measure FSL performance on MetaICL test tasks
from all categories (52 total). All experiments are
repeated for 3 random seeds to minimize the effects
of random task sampling in each dataset. We report
the mean accuracy from each experiment in Fig. 3.
We discuss our results in the following sections.

4.1 Does increasing dataset size improve

ﬁnetuning performance?

datasets

sampled

Fig. 3a shows FSL performance for differently-
sized
from
randomly
UnpredicTable. Each dataset has a max-
imum number of examples per task N = 10
and varies the number of tasks T . Increasing
the number of tasks from T = 40 does not help
and performance deteriorates beyond T = 5000,
contrary to results in Wang et al. (2022).7 Overall,
the number of tasks does not seem to be the key
factor for our ﬁnetuning transfer success.

4.2 Does dataset diversity improve

performance?

Next, we study the effect of task diversity on
FSL performance. Tasks from the same web-
site tend to be similar in content, so we con-
struct more diverse datasets by sampling tasks
from UnpredicTable-unique, a version of
UnpredicTable ﬁltered to have a maximum

7For additional dataset scaling results, we randomly sam-
ple human-curated NLP tasks from the MetaICL training set
(Fig. 3b). Since there are only 90 NLP training tasks, we use
T = 40 tasks and vary N to match the total number of exam-
ples in Fig. 3a. At an equal number of tasks and examples per
task (T = 40, N = 10), NLP datasets outperform our dataset
by ∼ 1%. (The results in Tab. 1 differ due to the choices of
train and test tasks in different task categories.)

Figure 3: Each bar represents a GPT2 model ﬁnetuned on a different dataset. The y-axis shows mean improvement
of a ﬁnetuned LM over the pretrained LM. Comparing dataset helpfulness: Datasets made of diverse tasks from
UnpredicTable (a) and NLP datasets (b) lead to +5–7% improvement. Narrow clusters (c) and websites (d) within
UnpredicTable vary signiﬁcantly, with the best narrow datasets matching the best multi-task NLP datasets (b).

of one task per website (vs. up to 2500 in
UnpredicTable). Fig. 3a shows that the differ-
ence between UnpredicTable-unique and
UnpredicTable at matching sizes is small, sug-
gesting that dataset diversity is not an important
factor for our ﬁnetuning transfer success.

To examine narrow datasets in contrast to the
uniformly-sampled ones, we consider 3 types of
datasets grouped by content. We sample tasks from
20 websites of different genres, forming a dataset
from each website (Fig. 3d). Secondly, we also
form datasets of semantically similar tasks by clus-
tering UnpredicTable-unique tasks into 30
clusters using HDBSCAN8 (McInnes et al., 2017)
(Fig. 3c). Finally, we also sample 20 NLP tasks
from the 90 MetaICL training tasks and use each
task as a separate training dataset (Fig. 3e). Single-
website and single-NLP datasets have T × N =
10000 total examples, and cluster datasets have dif-
ferent T due to the clustering algorithm.

We ﬁnd there is signiﬁcant variance among
the narrow datasets. Some single-website or clus-
ter datasets are better than diverse datasets, such
as support.google.com which is our best
dataset overall (even outperforming diverse NLP
datasets). This suggests that diverse task datasets
are less important than careful selection of a narrow
training dataset for FSL improvement.

4.3 Can we select good tasks by hand?

Padmakumar et al. (2022) found that some training
tasks can negatively impact downstream perfor-
mance, which could explain why aggregating many
random tasks may be less successful than individ-
ual tasks. We manually categorize 2,000 tasks from
UnpredicTable-unique into High, Mid, and
Low-quality.9 We deﬁne low-quality tasks as tasks
where the content is junk or relies on missing con-
text. High-quality tasks are ones where an annotator
could pick the correct answer from a list of options,
and tests useful abilities (logic, general knowledge,

8See Appendix D for details of our clustering setup.

9See Appendix E for details of our annotation setup.

402001k5k25k125k1050250125062507823019924282716265154172561029131822122112014112-130246810Mean score change (abs %)(a) Random tables# tasks(b) Random NLP# examples per task(c) Clustered tablesCluster IDUnpredicTable-5k datasetUnpredicTableUnpredicTable-unique (1 task per website)NLP taskssupport.google.comw3.orgwiki.openmoko.orgmmo-champion.commsdn.microsoft.combulbapedia.bulbagarden.netdummies.comstudystack.comgamefaqs.comcram.comphonearena.comensembl.orgen.wikipedia.orgsporcle.commgoblog.combaseball.fantasysports.yahoo.comsittercity.comdividend.comcappex.comwkdu.orgnumer_sensespiderlama-trexyelp_review_fulltrecgigawordtweet_eval-sentimentsocial_i_qaemotionglue-sst2artdiscoveryfreebase_qarace-middlesquad-with_contextboolqtweet_eval-offensiveyahoo_answers_topicspiqaLow-qualityMed-qualityHigh-quality202468Mean score change (abs %)(d) Single-website tables(e) Single NLP(f) Hand-pickedcomprehension, etc.). Mid-quality tasks are the re-
maining tasks. For each class, we randomly sample
T = 200 tasks to form its own dataset.

Surprisingly, our manual annotations of quality
are not strongly correlated with downstream task
performance (Fig. 3f). Our handpicked dataset of
high-quality tasks does not even surpass the scores
of randomly-sampled tasks, and the difference in
performance between our low and high-quality
datasets are <1%. These results suggest that tasks
that look helpful are not necessarily helpful.

4.4 How do helpful and unhelpful tasks look?

Examples of Helpful Tasks

w3.org
input

[Keyword] password [Data type] Text with no
line breaks (sensitive information) [Control type]
A text ﬁeld that obscures data entry [State]

output

Password

bulbapedia.bulbagarden.net
input
output Never ends, screen freezes with the words

[Move] Odor Sleuth [Effect]

"Wild/Foe (Pokémon) used Odor Sleuth!"

cluster 7
input
output

[Cookie] guest_id, ki [Information]

These cookies allow you to access the Twitter
feed on the homepage.

wiki.openmoko.org contain software doc-
umentation; cluster 7 describes
informa-
tion related to internet cookies. Unhelpful
datasets are more varied. The two least-helpful
datasets are NLP datasets: piqa (question-
answering task for physical knowledge) and
yahoo_answers_topics (topic-classiﬁcation
task) both yield negative transfer results. The least
helpful table datasets include highly-repetitive soft-
ware tables (cluster 2 & 3), tasks classiﬁed
as noise by the clustering algorithm (cluster
-1), college review posts (cappex.com), and mu-
sic database entries (wkdu.org).
appear

datasets
tasks (e.g.
test

to
there are no software-
our
examples
related
this: mmo-champion.com and
highlight
bulbapedia.bulbagarden.net are video
game trivia sites that do not seem useful for
other tasks, yet these datasets are on par with
UnpredicTable-5k. Conversely, websites
containing high-quality question-answer pairs such
as cram.com and studystack.com, as well
as en.wikipedia.org which contains many
real-world facts, yield subpar improvements. We
include examples of helpful and unhelpful tasks in
Tab. 2, and more examples in Appendix F.

tasks). Additional

unrelated

The

test

top

Examples of Unhelpful Tasks

4.5 Which tasks are our datasets helpful for?

wkdu.org
input
output

5 Years Time

[Artist] Noah and the Whale [Title]

cappex.com
input

[Comments] The school is located near town so
anything you would want to do is just an easy
ten minute drive away. [Categories]

output What to do for fun

yahoo_answers_topics
input

question_title: bungee jumping site in victo-
ria??? [SEP] question_content: i am trying to
ﬁnd a site for bungee jumping ... (Truncated)
Sports

output

Table 2: Helpful and unhelpful datasets are highly var-
ied and do not always match our intuitions on task qual-
ity.

We look for

features of helpful and un-
helpful datasets with examples from cluster,
single-website and single-NLP datasets. 4/5
of
the most helpful datasets are software-
related. support.google.com, w3.org and

Table-5k NLP-1250

support.google

Test tasks counts (# out of 52)
32
33
20
19

23

31

37
15

34

Score change (ﬁnetuned - pre) (%)
+6.7
+3.5
+44.7
-12.5

+5.6
+2.8
+43.0
-17.3

+7.5
+3.6
+47.1
-10.0

>Pretrained
<Pretrained
>Chance
(pre: 23)

Mean
Median
Max
Min

Table 3: Top (counts): First two rows indicate the
number of test tasks that improved or not (vs the
pretrained model) after ﬁnetuning. Third row shows
the number of test tasks that score greater than ran-
dom chance (on multiple-choice answers). Fine-tuning
improves the pretrained model on more than 60%
of test tasks. Bottom (scores): Improvements are not
evenly distributed; the maximum score increase on
support.google.com is +47.1% but median im-
provement is only +3.6%.

Here, we investigate which test tasks beneﬁt
from our ﬁnetuning. Fig 4 shows score improve-

NLP-125010,
and support.google.com.
Summary statistics are shown in Tab. 3. Across
the 3 datasets, 60-70% of tasks have improved
scores over the pretrained model. The distribution
of test score improvements appear to be highly
concentrated on a few tasks, with 20% of test tasks
accounting for 60-80% of all improvement. The
median score change for UnpredicTable-5k
is only +2.8%, though the max is +43.0%.

Fig. 5 shows the 10 most-improving test
tasks (median improvement across all 90 train-
ing datasets in Fig. 4). The tasks are highly var-
ied, spanning topics from news to ﬁnance to sci-
ence, and have binary or multiple-choice (MCQ)
output labels. It is difﬁcult to draw a consistent
relationship between test tasks and the ﬁnetun-
ing datasets that lead to their largest improve-
ment (Best dataset). For example, cluster 7
is a dataset about web cookies, yet it is the most
helpful ﬁnetuning dataset for both ag_news and
amazon_polarity which are news classiﬁca-
tion and sentiment classiﬁcation tasks respectively.
Our examples of unintuitive task transfer contradict
prior work that suggest domain similarity is key for
successful task transfer (Gururangan et al., 2020).
Vu et al. (2020) observed that “Out-of-class transfer
succeeds in many cases, some of which are unin-
tuitive.” In our experiments, unintuitive transfer
appears to be the norm rather than the exception.

4.6 Do different datasets lead to different

improvements?

We wish to understand if ﬁnetuning on different
datasets lead to different test task improvements.
Fig. 6 illustrates that the same set of 10 test tasks
make up the majority of the top-10 improving test
tasks for each of our best training datasets (the top-
performing datasets for each category in Fig. 4).
For example, training on wiki.openmoko.org
(software documentation) leads to strong improve-
ments on broadly similar tasks as training on
lama-trex (factual knowledge). This suggests
that the improvements learned from these highly
different training datasets are domain-agnostic.
However, it remains unclear why these improve-
ments can be learned from these particular training
datasets but not others, and why these particular
test tasks beneﬁt most from the improvements.

Figure 4: Breakdown of model scores across 52 test
tasks for models ﬁnetuned on three different datasets.
Scores are relative to the initial pretrained model.

ments on all 52 test tasks relative to the pretrained
model after ﬁnetuning on UnpredicTable-5k,

10Random NLP tasks with T = 40, N = 1250 to match

the total number of examples in UnpredicTable-5k.

2002040Score change (abs %)quailtab_factquartz-no_knowledgeanliglue-wnliethos-national_originwino_grandequartz-with_knowledgeunifiedqa:openbookqawiqaunifiedqa:openbookqa_with_iremoopenbookqaai2_arcsickscitailglue-mnlicosmos_qaethos-religionswagunifiedqa:ai2_science_middleclimate_feverdreamglue-qnliunifiedqa:mctestpoem_sentimentglue-rtesuperglue-cbpawsquarelhellaswaghate_speech18medical_questions_pairsunifiedqa:qasctweet_eval-stance_feministglue-mrpcsuperglue-copacodahglue-qqpunifiedqa:qasc_with_irtweet_eval-stance_atheismtweet_eval-hatewiki_qaethos-raceqasccommonsense_qaag_newssciqamazon_polarityyelp_polarityfinancial_phrasebankdbpedia_14Pretrained modelUnpredicTable-5kNLP-1250support.google.comTask
ag_news
dbpedia_14
commonsense_qa
sciq
amazon_polarity
qasc
ﬁnancial_phrasebank
tweet_eval-stance_atheism
yelp_polarity
ethos-race

Type
News class
Wikipedia class
General QA
Scientiﬁc QA
Review class
General QA
Financial class
Tweet class
Review class
Hate speech class

Output space
World / Sports / Business / SciTech
14 classes (plant / athlete / ...)
MCQ
MCQ
positive / negative
MCQ
positive / negative / neutral
none / against / favor
positive / negative
true / false

Best dataset
Chance (%) Median (%) Max (%)
cluster 7
63 (+50)
w3.org
47 (+42)
cluster 12
51 (+30)
cluster 0
87 (+29)
cluster 7
92 (+34)
cluster 8
38 (+25)
68 (+40)
support.google.com
44 (+25) msdn.microsoft.com
84 (+36)
55 (+23)

42 (+29)
31 (+25)
44 (+23)
81 (+23)
77 (+18)
30 (+17)
41 (+14)
31 (+13)
61 (+12)
43 (+12)

w3.org
support.google.com

25
7
20
25
50
13
33
33
50
50

Figure 5: Most-improving tasks in the MetaICL test set: The tasks span a wide variety of topics and output spaces.
There is no clear connection to the training datasets that most strongly improve FSL performance (Best dataset),
yet score improvements are signiﬁcant. We show absolute scores for random Chance as well as the Median and
Max scores across different training datasets. Improvements w.r.t. to the pretrained model are shown in parentheses.

(Min et al., 2021) as the training method for our
main experiments and support our results with ad-
ditional few-shot benchmarks, CrossFit (Ye et al.,
2021) and FLEX (Bragg et al., 2021).

Our work also connects with other work in do-
main adaptation. Gururangan et al. (2020) show
that ﬁne-tuning on domains related to the down-
stream task leads to performance gains. Recent
examples of successful domain adaptation in-
clude Chen et al. (2021a) for coding tasks and
Lewkowycz et al. (2022) for mathematics tasks. So-
laiman and Dennison (2021) demonstrated this for
less explicit domains, ﬁnetuning LMs on values-
aligned text to generate text in accordance with
intrinsic human values. In contrast, we show that
LMs can be ﬁnetuned on unrelated domains yet im-
prove on the downstream task. Other work in adap-
tation focus on speciﬁc task formats: Khashabi et al.
(2020); Huber et al. (2021); Zhong et al. (2021b)
convert broad NLP tasks into question-answering
tasks and ﬁnetune to excel at question-answering;
Zhong et al. (2021a) ﬁnetunes models to perform
classiﬁcation tasks; Gao et al. (2020) introduce
prompt templates and ﬁnetune the model to per-
form tasks within those templates. More generally,
LMs have been ﬁnetuned to follow instructions
(Ouyang et al., 2022; Wei et al., 2021) which al-
lows for more diverse tasks in various formats. Our
adaptation to FSL can be seen as adaptation to the
FSL prompt format, though the tasks themselves
can be diverse in domain and structure.

Multi-task literature have shown that training
on a wide variety of tasks improves generalization
to new task settings, which motivates our explo-
ration of a large scale few-shot task dataset. Sanh
et al. (2021); Aribandi et al. (2021); Mishra et al.
(2021); Aghajanyan et al. (2021a); Padmakumar
et al. (2022) demonstrate that increasing the num-

Figure 6: Finetuning on different datasets leads to
broadly similar improvements. For example, ﬁnetuning
on wiki.openmoko.org (software documentation)
and lama-trex (factual knowledge) lead to 8 of the
same test tasks being in their respective top-10 most-
improved test tasks. (Out of 52 total test tasks)

5 Related Work

We focus on the FSL setting where a small number
of training samples are available to learn a given
task. Pretrained LMs can learn from few-shot ex-
amples in-context (Brown et al., 2020; Scao and
Rush, 2021) but have weaknesses including prompt
sensitivity (Lu et al., 2021; Perez et al., 2021) and
miscalibration (Zhao et al., 2021). Min et al. (2021)
and Chen et al. (2021b) adapt to the FSL setting
by ﬁne-tuning LMs to predict the target given few-
shot examples in the prompt. This improves FSL
performance and reduces sensitivity to example
ordering and example choice. We adopt MetaICL

ag_newsdbpedia_14commonsense_qasciqamazon_polarityqascfinancial_phrasebanktweet_eval-stance_atheismyelp_polarityethos-raceTest TasksUnpredicTable-5kNLP-1250cluster 7cluster 8cluster 23 support.google.com w3.org wiki.openmoko.org numer_sense spider lama-trexTrain Datasets : Whenfinetuning onTrain Dataset,Test Task isits top-10 mostimproved tasks.ber of tasks for multi-task training improves gener-
alization in the zero-shot setting. Xu et al. (2022);
Wang et al. (2022) have extended this result to
more than 1,000 tasks. We were inspired by these
results to obtain a training dataset with 100x more
tasks, but found diverse task datasets less helpful
than certain narrow datasets. Padmakumar et al.
(2022) showed that a poor choice of training task
can negatively impact downstream performance,
which could explain why mixing diverse tasks un-
derperform well-chosen narrow tasks. This begs
the question of how to select training datasets to
improve downstream task performance. Vu et al.
(2020) show that domain similarity can be used
as a predictor for successful transfer. Our results
highlight a gap in this explanation, and suggest that
there may be some domain-agnostic improvements
to be gained from training tasks that are unrelated
to the test tasks. Other attempts to understand the
effect of training datasets on FSL also struggle
to uncover clean rules; this includes analyses of
pretraining datasets (Shin et al., 2022), varying
datasets alongside model architectures (Chan et al.,
2022), and inﬂuence functions to trace gradient up-
dates to training datapoints (Akyürek et al., 2022).
Our use of structured datasets to generate train-
ing tasks is inspired by other work, though others
have focused on a limited set of task types. Yoran
et al. (2021) also turn tables into tasks, using hand-
written templates to extract question-answer pairs
from tables. Aghajanyan et al. (2021b) train LMs to
predict masked spans in HTML webpages, then use
HTML markup to prompt language models to do
summarization and classiﬁcation tasks. Chen et al.
(2022) transform ordinary (non-table) text into sen-
tence completion, masked phrase prediction, and
classiﬁcation tasks. In contrast, our approach cap-
tures any tasks that occur in tables.

6 Conclusion

We produced UnpredicTable, a dataset of
413,299 diverse few-shot learning tasks from inter-
net tables. Finetuning on UnpredicTable im-
proves the FSL ability of LMs. However, the size
of our dataset is not the key factor in its success.
We ﬁnd that certain narrow datasets (even ones
made of trivia) are even more helpful than diverse,
curated NLP datasets. Finetuning on these narrow
datasets leads to strong improvements on the same
test tasks as ﬁnetuning on diverse, curated NLP
datasets. This suggests that ﬁnetuning on these

datasets cause domain-agnostic FSL gains, though
we were unable to ﬁnd clear patterns to explain
why this happens for some data and not others. Our
results question common wisdom that task diver-
sity is necessary for adapting LMs to FSL. We hope
our work spurs investigation on what data causes
few-shot learning to emerge, both to develop better
datasets and to better understand how training data
leads to unexpected behaviors or failures.

7 Acknowledgements

We are grateful to Owain Evans, Mary Phuong,
Seraphina Nix, and Sam Bowman for helpful con-
versations and feedback, as well as to Kath Lupante
for task quality annotations. We thank Open Philan-
thropy for funding that enabled this research. Ethan
Perez thanks the National Science Foundation and
Open Philanthropy for fellowship support.

References

Armen Aghajanyan, Anchit Gupta, Akshat Shrivas-
tava, Xilun Chen, Luke Zettlemoyer, and Sonal
Gupta. 2021a. Muppet: Massive multi-task rep-
arXiv preprint
resentations with pre-ﬁnetuning.
arXiv:2101.11038.

Armen Aghajanyan, Dmytro Okhonko, Mike Lewis,
Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettle-
moyer. 2021b. Htlm: Hyper-text pre-training and
arXiv preprint
prompting of language models.
arXiv:2107.06955.

Ekin Akyürek, Tolga Bolukbasi, Frederick Liu, Bin-
bin Xiong, Ian Tenney, Jacob Andreas, and Kelvin
Guu. 2022. Tracing knowledge in language mod-
arXiv preprint
els back to the training data.
arXiv:2205.11482.

Tiago A. Almeida, José María G. Hidalgo, and Akebo
Yamakami. 2011. Contributions to the study of sms
spam ﬁltering: New collection and results. In Pro-
ceedings of the 11th ACM Symposium on Document
Engineering.

Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,
Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-
glei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo
Ni, et al. 2021.
Ext5: Towards extreme multi-
task scaling for transfer learning. arXiv preprint
arXiv:2111.10952.

Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In Proceedings of the sec-
ond PASCAL challenges workshop on recognising
textual entailment.

Francesco Barbieri, Jose Camacho-Collados, Luis Es-
pinosa Anke, and Leonardo Neves. 2020. TweetE-
val: Uniﬁed benchmark and comparative evaluation
for tweet classiﬁcation. In Findings of the Associa-
tion for Computational Linguistics: EMNLP 2020.

Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo
Giampiccolo. 2009. The ﬁfth pascal recognizing tex-
tual entailment challenge. In TAC.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In EMNLP.

Chandra Bhagavatula, Ronan Le Bras, Chaitanya
Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-
nah Rashkin, Doug Downey, Wen tau Yih, and Yejin
Choi. 2020. Abductive commonsense reasoning. In
ICLR.

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian-
feng Gao, and Yejin Choi. 2020. Piqa: Reasoning
about physical commonsense in natural language. In
AAAI.

Michael Boratko, Xiang Li, Tim O’Gorman, Rajarshi
Das, Dan Le, and Andrew McCallum. 2020. Pro-
toQA: A question answering dataset for prototypical
common-sense reasoning. In EMNLP.

Jonathan Bragg, Arman Cohan, Kyle Lo, and Iz Belt-
agy. 2021. Flex: Unifying evaluation for few-shot
nlp. Advances in Neural Information Processing
Systems, 34:15787–15800.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.

Stephanie CY Chan, Adam Santoro, Andrew K
Lampinen, Jane X Wang, Aaditya Singh, Pierre H
Richemond, Jay McClelland, and Felix Hill. 2022.
Data distributional properties drive emergent few-
arXiv preprint
shot
arXiv:2205.05055.

learning in transformers.

Ankush Chatterjee, Kedhar Nath Narahari, Meghana
Joshi, and Puneet Agrawal. 2019. SemEval-2019
task 3: EmoContext contextual emotion detection in
text. In Proceedings of the 13th International Work-
shop on Semantic Evaluation.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Evaluating large lan-
Brockman, et al. 2021a.
arXiv preprint
guage models trained on code.
arXiv:2107.03374.

Michael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-
nandez, and Doug Downey. 2019. CODAH: An
adversarially-authored question answering dataset

for common sense. In Proceedings of the 3rd Work-
shop on Evaluating Vector Space Representations
for NLP.

Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor
Mihaylov, Srini Iyer, Veselin Stoyanov, and Zornitsa
Kozareva. 2022.
Improving in-context few-shot
learning via self-supervised training. arXiv preprint
arXiv:2205.01703.

Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
William Yang Wang. 2020. Tabfact: A large-scale
dataset for table-based fact veriﬁcation. In ICLR.

Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,
Meta-learning via lan-
arXiv preprint
tuning.

and He He. 2021b.
guage model
arXiv:2110.07814.

in-context

Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. BoolQ: Exploring the surprising
In NAACL-
difﬁculty of natural yes/no questions.
HLT.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. arXiv
preprint arXiv:1803.05457.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Machine Learning Challenges Work-
shop.

Pradeep Dasigi, Nelson F. Liu, Ana Marasovi´c, Noah A.
Smith, and Matt Gardner. 2019. Quoref: A read-
ing comprehension dataset with questions requiring
coreferential reasoning. In EMNLP.

Thomas Davidson, Dana Warmsley, Michael Macy, and
Ingmar Weber. 2017. Automated hate speech detec-
tion and the problem of offensive language. In Pro-
ceedings of the 11th International AAAI Conference
on Web and Social Media.

Ona de Gibert, Naiara Perez, Aitor García-Pablos, and
Montse Cuadros. 2018. Hate Speech Dataset from a
White Supremacy Forum. In Proceedings of the 2nd
Workshop on Abusive Language Online (ALW2).

Marie-Catherine de Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse.
Proceedings of Sinn und Bedeutung.

T. Diggelmann, Jordan L. Boyd-Graber, Jannis Bu-
lian, Massimiliano Ciaramita, and Markus Leippold.
2020. Climate-fever: A dataset for veriﬁcation of
real-world climate claims. ArXiv.

William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).

Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel
Stanovsky, Sameer Singh, and Matt Gardner. 2019.
DROP: A reading comprehension benchmark requir-
ing discrete reasoning over paragraphs. In NAACL.

Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and
Yejin Choi. 2019. Cosmos QA: Machine reading
comprehension with contextual commonsense rea-
soning. In EMNLP.

Matthew Dunn, Levent Sagun, Mike Higgins, V. U.
Güney, Volkan Cirik, and Kyunghyun Cho. 2017.
Searchqa: A new q&a dataset augmented with
arXiv preprint
context from a search engine.
arXiv:1704.05179.

Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci,
Christophe Gravier, Jonathon Hare, Frederique
Laforest, and Elena Simperl. 2018. T-REx: A large
scale alignment of natural language with knowledge
base triples. In LREC.

Manaal Faruqui and Dipanjan Das. 2018. Identifying
well-formed natural language questions. In EMNLP.

Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.
Making pre-trained language models better few-shot
learners. arXiv preprint arXiv:2012.15723.

Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
ACL-PASCAL workshop on textual entailment and
paraphrasing.

Andrew Gordon, Zornitsa Kozareva, and Melissa
Roemmele. 2012. SemEval-2012 task 7: Choice
of plausible alternatives: An evaluation of common-
In The First Joint Confer-
sense causal reasoning.
ence on Lexical and Computational Semantics (Se-
mEval).

Harsha Gurulingappa, Abdul Mateen Rajput, Angus
Roberts, Juliane Fluck, Martin Hofmann-Apitius,
and Luca Toldo. 2012. Development of a bench-
mark corpus to support the automatic extraction of
drug-related adverse effects from medical case re-
ports. Journal of Biomedical Informatics.

Suchin Gururangan, Ana Marasovi´c,

Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A Smith. 2020. Don’t stop pretraining:
adapt language models to domains and tasks. arXiv
preprint arXiv:2004.10964.

Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.
Question-answer driven semantic role labeling: Us-
ing natural language to annotate natural language.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named en-
tities in text. In EMNLP.

Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-
Yew Lin, and Deepak Ravichandran. 2001. Toward
In Proceed-
semantics-based answer pinpointing.
ings of the First International Conference on Human
Language Technology Research.

Patrick Huber, Armen Aghajanyan, Barlas O˘guz,
Dmytro Okhonko, Wen-tau Yih, Sonal Gupta, and
Xilun Chen. 2021. Ccqa: A new web-scale ques-
tion answering dataset for model pre-training. arXiv
preprint arXiv:2110.07731.

Kelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Free-
baseQA: A new factoid QA data set matching
trivia-style question-answer pairs with Freebase. In
NAACL-HLT.

Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018. Looking be-
yond the surface: A challenge set for reading com-
In NAACL-
prehension over multiple sentences.
HLT.

Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. 2020. Uniﬁedqa: Crossing format
boundaries with a single qa system. arXiv preprint
arXiv:2005.00700.

Tushar Khot, Peter Clark, Michal Guerquin, Peter
Jansen, and Ashish Sabharwal. 2019. QASC: A
dataset for question answering via sentence compo-
sition. In AAAI.

Tushar Khot, Peter Clark, Michal Guerquin, Peter
Jansen, and Ashish Sabharwal. 2020. Qasc: A
dataset for question answering via sentence compo-
sition. In AAAI.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
Scitail: A textual entailment dataset from science
question answering. In AAAI.

Tomás Kociský, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, Gábor Melis, and
Edward Grefenstette. 2018. The narrativeqa reading
comprehension challenge. TACL.

Neema Kotonya and Francesca Toni. 2020.

Ex-
plainable automated fact-checking for public health
claims. In EMNLP.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur P. Parikh, Chris Al-
berti, Danielle Epstein, Illia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones,
Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,
Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: A benchmark for question an-
swering research. TACL.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard H. Hovy. 2017. RACE: Large-scale
reading comprehension dataset from examinations.
In EMNLP.

Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
D. Kontokostas, Pablo N. Mendes, Sebastian Hell-
mann, M. Morsey, Patrick van Kleef, S. Auer, and
C. Bizer. 2015. Dbpedia - a large-scale, multilingual
knowledge base extracted from wikipedia. Semantic
Web.

Hector J. Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge.
In
Proceedings of the Thirteenth International Confer-
ence on Principles of Knowledge Representation
and Reasoning.

Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In CoNLL.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.
Bart: Denoising sequence-to-sequence pre-training
for natural
translation, and
comprehension. arXiv preprint arXiv:1910.13461.

language generation,

Aitor Lewkowycz, Anders Andreassen, David Dohan,
Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,
Ambrose Slone, Cem Anil, Imanol Schlag, Theo
Gutman-Solo, et al. 2022.
Solving quantitative
reasoning problems with language models. arXiv
preprint arXiv:2206.14858.

Xin Li and Dan Roth. 2002. Learning question classi-

ﬁers. In COLING.

Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-
ang Ren. 2020. Birds have four legs?! NumerSense:
Probing Numerical Commonsense Knowledge of
Pre-Trained Language Models. In EMNLP.

Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-
ner. 2019. Reasoning over paragraph effects in situ-
ations. In Proceedings of the 2nd Workshop on Ma-
chine Reading for Question Answering.

Annie Louis, Dan Roth, and Filip Radlinski. 2020. “I’d
rather just go to bed”: Understanding indirect an-
swers. In EMNLP.

Yao Lu, Max Bartolo, Alastair Moore, Sebastian
Riedel, and Pontus Stenetorp. 2021. Fantastically
ordered prompts and where to ﬁnd them: Overcom-
ing few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014. A SICK cure for the evaluation of com-
positional distributional semantic models. In LREC.

Binny Mathew, Punyajoy Saha, Seid Muhie Yi-
mam, Chris Biemann, Pawan Goyal, and Ani-
mesh Mukherjee. 2020. Hatexplain: A benchmark
dataset for explainable hate speech detection. arXiv
preprint arXiv:2012.10289.

Julian McAuley and J. Leskovec. 2013. Hidden factors
and hidden topics: understanding rating dimensions
with review text. Proceedings of the 7th ACM con-
ference on Recommender systems.

Clara H. McCreery, Namit Katariya, Anitha Kannan,
Manish Chablani, and Xavier Amatriain. 2020. Ef-
fective transfer learning for identifying similar ques-
tions: Matching user questions to covid-19 faqs. In
Proceedings of the 26th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data
Mining.

Leland McInnes, John Healy, and Steve Astels. 2017.
hdbscan: Hierarchical density based clustering. J.
Open Source Softw., 2(11):205.

Leland McInnes, John Healy, Nathaniel Saul, and
Lukas Grossberger. 2018. Umap: Uniform manifold
approximation and projection. The Journal of Open
Source Software, 3(29):861.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question answer-
ing. In EMNLP.

Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2021. Metaicl: Learning to learn
in context. arXiv preprint arXiv:2110.15943.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2021. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
arXiv preprint arXiv:2104.08773.

Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,
and Grigorios Tsoumakas. 2020. Ethos: an on-
line hate speech detection dataset. arXiv preprint
arXiv:2006.08328.

Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked
language models. In EMNLP.

Courtney Napoles, Matthew Gormley, and Benjamin
In Pro-
Van Durme. 2012. Annotated Gigaword.
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction (AKBC-WEKEX).

Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-
lenius, and Pyry Takala. 2014. Good debt or bad
debt: Detecting semantic orientations in economic
texts. J. Assoc. Inf. Sci. Technol.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In EMNLP.

Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-
versarial NLI: A new benchmark for natural lan-
guage understanding. In ACL.

Anna Rogers, Olga Kovaleva, Matthew Downey, and
Anna Rumshisky. 2020. Getting closer to ai com-
plete question answering: A set of prerequisite real
tasks. In AAAI.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
Training language models to follow in-
2022.
arXiv preprint
structions with human feedback.
arXiv:2203.02155.

Vishakh Padmakumar, Leonard Lausen, Miguel Balles-
teros, Sheng Zha, He He, and George Karypis.
2022. Exploring the role of task transferability
in large-scale multi-task learning. arXiv preprint
arXiv:2204.11117.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In ACL.

Dimitris Pappas, Petros Stavropoulos, Ion Androut-
sopoulos, and Ryan McDonald. 2020. BioMRC: A
dataset for biomedical machine reading comprehen-
sion. In Proceedings of the 19th SIGBioMed Work-
shop on Biomedical Language Processing.

Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.
True few-shot learning with language models. Ad-
vances in Neural Information Processing Systems,
34:11054–11070.

Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim
Rocktäschel, Yuxiang Wu, Alexander H. Miller, and
Sebastian Riedel. 2020. How context affects lan-
In Automated
guage models’ factual predictions.
Knowledge Base Construction.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-
vatula, and Yejin Choi. 2020a. WINOGRANDE: an
adversarial winograd schema challenge at scale. In
AAAI.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2020b. Winogrande: An adver-
sarial winograd schema challenge at scale. In AAAI.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun
Raja, et al. 2021. Multitask prompted training en-
ables zero-shot task generalization. arXiv preprint
arXiv:2110.08207.

Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019a. Social IQa: Com-
monsense reasoning about social interactions.
In
EMNLP.

Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019b. Social iqa: Com-
monsense reasoning about social interactions.
In
EMNLP-IJCNLP.

Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,
Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-
textualized affect representations for emotion recog-
nition. In EMNLP.

Teven Le Scao and Alexander M Rush. 2021. How
many data points is a prompt worth? arXiv preprint
arXiv:2103.08493.

Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In EMNLP.

Emily Sheng and David Uthus. 2020. Investigating so-
cietal biases in a poetry composition system. In Pro-
ceedings of the Second Workshop on Gender Bias in
Natural Language Processing.

Mohammad Taher Pilehvar and Jose Camacho-
Collados. 2019. WiC: the word-in-context dataset
for evaluating context-sensitive meaning representa-
tions. In NAACL-HLT.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for squad. In ACL.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In EMNLP.

Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sung-
dong Kim, HyoungSeok Kim, Boseop Kim,
Kyunghyun Cho, Gichang Lee, Woomyoung Park,
Jung-Woo Ha, et al. 2022. On the effect of pretrain-
ing corpora on in-context learning by a large-scale
language model. arXiv preprint arXiv:2204.13509.

Damien Sileo, Tim Van De Cruys, Camille Pradel, and
Philippe Muller. 2019. Mining discourse markers
for unsupervised sentence representation learning.
In NAACL-HLT.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP.

Matthew Richardson, Christopher J. C. Burges, and
Erin Renshaw. 2013. Mctest: A challenge dataset
for the open-domain machine comprehension of text.
In EMNLP.

Irene Solaiman and Christy Dennison. 2021. Process
for adapting language models to society (palms)
with values-targeted datasets. Advances in Neural
Information Processing Systems, 34:5861–5873.

Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,
and Claire Cardie. 2019. DREAM: A challenge data
set and models for dialogue-based reading compre-
hension. TACL.

Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-
hananey, Wei Peng, Sheng-Fu Wang, and Samuel R.
Bowman. 2020. Blimp: The benchmark of linguistic
minimal pairs for english. TACL.

Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau
Yih, and Ashish Sabharwal. 2019a. Quarel: A
dataset and models for answering questions about
qualitative relationships. In AAAI.

Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter
Clark. 2019b. QuaRTz: An open-domain dataset of
qualitative relationship questions. In EMNLP.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. Commonsenseqa: A ques-
tion answering challenge targeting commonsense
knowledge. In NAACL-HLT.

Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-
ter Clark, and Antoine Bosselut. 2019. WIQA: A
dataset for “what if...” reasoning over procedural
text. In EMNLP.

James

Andreas

Christos
Thorne,
Christodoulopoulos,
2018.
FEVER: a large-scale dataset for fact extraction and
VERiﬁcation. In NAACL-HLT.

and Arpit Mittal.

Vlachos,

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2017. Newsqa: A machine compre-
hension dataset. In Rep4NLP@ACL.

Sowmya Vajjala and Ivana Luˇci´c. 2018.

On-
eStopEnglish corpus: A new corpus for automatic
readability assessment and text simpliﬁcation.
In
Proceedings of the Thirteenth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, volume 30. Curran Associates, Inc.

Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan-
dro Sordoni, Adam Trischler, Andrew Mattarella-
Micke, Subhransu Maji, and Mohit Iyyer. 2020.
Exploring and predicting transferability across nlp
tasks. arXiv preprint arXiv:2005.00770.

William Yang Wang. 2017. “liar, liar pants on ﬁre”: A
new benchmark dataset for fake news detection. In
ACL.

Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
TACL.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652.

Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.
Crowdsourcing multiple choice science questions.
In Proceedings of the 3rd Workshop on Noisy User-
generated Text.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In NAACL-
HLT.

Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulka-
rni, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and
William Yang Wang. 2019. TWEETQA: A social
media focused question answering dataset. In ACL.

Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-
gang Wang, Haiyu Li, and Zhilin Yang. 2022. Zero-
prompt: Scaling prompt-based pretraining to 1,000
arXiv
tasks improves zero-shot generalization.
preprint arXiv:2201.06910.

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
WikiQA: A challenge dataset for open-domain ques-
tion answering. In EMNLP.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D. Manning. 2018. HotpotQA: A dataset for di-
verse, explainable multi-hop question answering. In
EMNLP.

Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren.
2021. Crossﬁt: A few-shot learning challenge for
arXiv preprint
cross-task generalization in nlp.
arXiv:2104.08835.

Ori Yoran, Alon Talmor, and Jonathan Berant.
2021.
Turning tables: Generating examples
from semi-structured tables for endowing language
arXiv preprint
models with reasoning skills.
arXiv:2107.07261.

Yizhong Wang, Swaroop Mishra, Pegah Alipoor-
molabashi, Yeganeh Kordi, Amirreza Mirzaei,
Anjana Arunkumar, Arjun Ashok, Arut Selvan
Dhanasekaran, Atharva Naik, David Stap, et al.
2022. Benchmarking generalization via in-context
arXiv
instructions on 1,600+ language tasks.
preprint arXiv:2204.07705.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao, Shanelle Roman, Zilin Zhang, and
Spider: A large-scale
Dragomir Radev. 2018.
human-labeled dataset
for complex and cross-
domain semantic parsing and text-to-SQL task. In
EMNLP.

Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin
SWAG: A large-scale adversarial
In

Choi. 2018.
dataset for grounded commonsense inference.
EMNLP.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can a
machine really ﬁnish your sentence? In ACL.

Sheng Zhang, X. Liu, J. Liu, Jianfeng Gao, Kevin
Duh, and Benjamin Van Durme. 2018. Record:
Bridging the gap between human and machine com-
monsense reading comprehension. arXiv preprint
arXiv:1810.12885.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
In Advances in neural information pro-
siﬁcation.
cessing systems, pages 649–657.

Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
PAWS: Paraphrase adversaries from word scram-
bling. In NAACL-HLT.

Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In In-
ternational Conference on Machine Learning, pages
12697–12706. PMLR.

Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.
2021a. Adapting language models for zero-shot
learning by meta-tuning on dataset and prompt col-
lections. arXiv preprint arXiv:2104.04670.

Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.
2021b. Meta-tuning language models to answer
prompts better. CoRR.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017.
Seq2sql: Generating structured queries
from natural language using reinforcement learning.
arXiv preprint arXiv:1709.00103.

Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan
Roth. 2019. “going on a vacation” takes longer than
“going for a walk”: A study of temporal common-
sense understanding. In EMNLP.

A Tables-to-tasks ﬁltering

Tab. 4 shows the number of tables and tasks ﬁltered
at various stages of our tables-to-tasks procedure.

tables initial
rejected min rows
rejected non-english
tables remaining
tasks initial
rejected max domain
rejected min rows
rejected one-to-many
rejected min classes
rejected non-english output
rejected class balance
tasks remaining

50, 820, 216
−25, 638, 244
−23, 034, 542
2, 147, 532
5, 646, 614
−4, 054, 764
−99, 226
−322, 536
−157, 199
−561, 622
−38, 505
413, 299

Table 4: Converting 50M tables into 400k tasks.

B MetaICL experiment details

This section provides training and evaluation de-
tails for our MetaICL experiments in §3 and §4. The
datasets used in MetaICL train and test settings are
taken from CROSSFIT (Ye et al., 2021) and UNI-
FIEDQA (Khashabi et al., 2020), which in turn have
been compiled from various other sources. The full
list for all datasets and their citations are provided
in Fig. 7. We make use of 3 different task splits:

Test Tasks (52 tasks) The union of all test tasks
from the 7 task settings in Min et al. (2021).

Train Tasks (90 tasks) Contains all tasks in Min
et al. (2021) except those which are Test Tasks.
These tasks are only used as a source of NLP
datasets in §4.

For

hyperparameter

the GPT2-large model

Dev Tasks (50 tasks) Contains all our Train
Tasks except those which are not multiple-choice.
These tasks are used for hyperparameter selection.
selection, we ﬁne-
(774M)11 on
tune
UnpredicTable-5k
over
sweep
rates
learning
batch
{5e−5, 5e−6, 5e−7}. We select batch size =
1 and learning rate = 5e−6 based on Dev scores and
use this for all MetaICL experiments. We train for
5 epochs and evaluate after each epoch, selecting
the checkpoint with the highest mean Dev Tasks

{1, 8, 64}

and
and

sizes

11GPT2-large LM https://huggingface.co/gpt2-large

score. We report scores of the selected checkpoint
evaluated on the Test Tasks. Each training and
inference run is done on a single RTX8000 GPU.
The duration of training varies by dataset size
(training 5 epochs on UnpredicTable-5k
takes ∼24 hours).

C Do Other Learning Algorithms Beneﬁt

from Table Data?

Our main experiments use the MetaICL algorithm
and benchmarks for training and evaluation. To
understand how well our ﬁndings hold in other
settings, we report additional experiments compar-
ing UnpredicTable-5k against NLP datasets
using different models, multi-task learning algo-
rithms, and evaluation settings.

C.1 CrossFit

Ye et al. (2021) introduce the Few-Shot Gym, a
collection of 160 NLP tasks, and a problem setup
called CrossFit. We focus on the Random task
partition of CrossFit where Dtrain and Dtest con-
tain 120 and 20 tasks respectively, sampled IID
from the Few-Shot Gym. For our learning algo-
rithm, we adopt the best-performing method in
Ye et al. (2021), MTL, which ﬁnetunes on Dtrain
followed by ﬁnetuning on the few-shot training
examples from a given target task in Dtest (ﬁne-
tuning a separate model for each target task in
Dtest). We compare three different methods: MTL
with Dtrain from the Few-Shot Gym, MTL with
UnpredicTable-5k as Dtrain, and Direct Fine-
tuning (DF) which is a baseline without ﬁnetuning
on any Dtrain. All experiments ﬁnetune a BART-
Base (Lewis et al., 2019), a pretrained encoder-
decoder transformer model (Vaswani et al., 2017).

Results Tab. 5 shows the full results. Compared
to DF, MTL with our dataset improves results by
a mean of +1.1%. 3 out of 20 tasks improve by
more than +10% including amazon_polarity
and yelp_polarity, which are also among the
tasks with the largest improvements in MetaICL.
MTL with UnpredicTable-5k is less helpful
than MTL with curated NLP datasets (+2.4% rel-
ative to DF), but still recovers 46% of the relative
improvement from ﬁnetuning on 120 curated NLP
tasks. Our results show that ﬁnetuning on Unpre-
dicTable helps even with MTL (a different learning
algorithm) on BART (a different LM). We see large
gains on similar tasks as in MetaICL, which sug-
gests that our data helps consistently on these tasks

Task
glue-cola
crawl_domain
ag_news
ai2_arc
wiki_split
amazon_polarity
blimp-..._present
tweet_eval-irony
ethos-disability
sglue-rte
circa
ethos-sexual_orient.
hatexplain
race-high
glue-qnli
quoref
blimp-...npi_scope
break-QDMR
yelp_polarity
freebase-qa
mean

DF MTL Ours
0.0
1.0
0.0
29.5
25.6
30.6
84.9
82.6
86.1
15.7
25.4
16.1
78.4
80.0
79.6
90.8
92.1
79.4
97.8
98.5
99.4
52.5
56.4
55.0
71.3
77.7
75.8
49.9
56.2
49.5
48.3
44.8
46.3
60.9
69.9
57.7
41.0
45.5
42.0
14.2
32.4
16.5
56.9
74.2
60.5
23.3
41.8
24.7
82.6
97.1
70.9
1.7
4.8
2.3
56.2
93.5
40.6
0.4
1.2
0.5
47.8
49.1
46.7

Table 5: Results on the CrossFit benchmark. We com-
pare the Direct Finetuning DF baseline (no multi-task
learning) against multi-task learning on the NLP Few-
shot Gym dataset (MTL) and multi-task learning with
UnpredicTable-5k (Ours).

(and the observed gains are not just an artifact of
MetaICL training).

C.2 FLEX

FLEX (Bragg et al., 2021) is a FSL bench-
mark that provides 11 NLP training tasks and
20 NLP test tasks, carefully chosen to evalu-
ate various task transfer settings. The baseline
model is UniFew, which uses a UniﬁedQA model
(Khashabi et al., 2020) with a prompt that con-
verts task examples into a multiple-choice question-
is
answer format. The primary FLEX model
UniFewMeta, which is UniFew ﬁnetuned with
the 11 FLEX training tasks. As in MetaICL,
UniFewMeta ﬁnetuning uses k examples in the input
to maximize log P (yk+1|x1, y1, . . . , xk, yk, xk+1).
Our approach (Ours) uses the same setup as
UniFewMeta but replaces the FLEX training tasks
with UnpredicTable-5k. Evaluation for all
models is done with FSL on the FLEX test tasks.

Task
FewRel
HuffPost
Amazon
20News
Reuters
MR
CR
SNLI
SciTail
SUBJ
TREC
CoNLL
Mean

UniFew Ours UniFewMeta
87.2
68.0
82.1
67.3
96.3
89.4
93.3
80.9
83.6
68.7
60.0
58.6
77.9

79.4
63.1
79.4
63.4
95.5
83.1
92.0
56.5
65.5
63.7
62.9
44.0
70.7

79.2
62.8
79.5
63.1
94.5
78.6
90.1
55.8
64.9
60.5
58.1
44.3
69.3

Table 6: Results on the FLEX benchmark. We compare
the pretraining-only UniFew model against the same
model ﬁnetuned on the FLEX dataset (Unifew-Meta)
and UnpredicTable-5k (Ours).

(mean +1.4%, max +5.5%). However, we do not
approach the level of UniFewMeta (mean improve-
ment +8.6%). This discrepancy is likely because
the FLEX training and test tasks have been cho-
sen with overlapping domains/task types to study
various transfer learning settings (see Bragg et al.
(2021) for details). Nevertheless, the results show
that our table tasks still lead to improvements in
FLEX with a different model and test tasks.

D Clustering

Here, we describe the clustering procedure used
to group UnpredicTable-unique tasks into
narrow data subsets based on content. For all ex-
amples in all tasks, we concatenate each (x, y) ex-
ample and obtain their embeddings from a pre-
trained GPT-2 model12. We average the resulting
1024-dimensional embeddings at a task level. We
normalize each task embedding and apply a two-
stage dimensionality reduction consisting of a PCA
transformation to 128 dimensions followed by fur-
ther reduction using UMAP (McInnes et al. (2018),
nneighbors = 4, dmin = 0.0) to 32 dimensions. We
cluster the 32D task embeddings using the HDB-
SCAN algorithm (McInnes et al., 2017) with a min-
imum cluster size of 60 and 400 minimum samples.
This setup results in 30 task clusters plus an ad-
ditional cluster (cluster -1) containing tasks

Results Tab. 6 shows our results. Training on
our dataset improves over UniFew for 10/12 tasks

12The stanford-crfm/eowyn-gpt2-medium-x777

model via the HuggingFace Transformers library.

that HDBSCAN rejected as noise. The cluster sizes
range from T = 61 to T = 5700. We tested several
hyperparameters for our clustering pipeline until
we arrived at a setup with reasonable in-cluster
content similarity (manual inspection).

E Task Quality Annotation Instructions

Below, we display a condensed version of the in-
structions given to annotators for annotating the
dataset into different task quality levels. The full
instructions are available online13.

Introduction Thank you for agreeing to con-
tribute annotations to our dataset! Here are some
brief instructions to help you successfully complete
this work.

Context We have a large number of Tasks cre-
ated for training language models to learn a variety
of skills. A standard example of a task is shown in
Tab. 7 as Task 1. This example closely resembles
the Question-Answer form that is commonly en-
countered in human competency tests, but this is
not the only valid form. More generally, a Task is
simply a set of input-output pairs where the inputs
map to outputs in a common and (given knowledge
of the mapping) predictable way; given an input,
an individual skilled in this task should be able to
respond with the correct output. Another example
of a valid task is shown in Tab. 7 as Task 2. In
this case, the inputs are a set of issues that a user
might be having, and the outputs suggest actions to
address each issue.

The Problem Our pool of tasks has been curated
in an automated way from natural internet content,
so they vary greatly in quality and form. It would
be valuable to label each task’s quality so that we
may investigate (1) what is the overall quality in
our pool of tasks, and (2) how task quality affects
the ability of language models to learn from it.

The Work In this session, you will classify a
number of tasks in terms of how feasible and useful
they are. Each task should be rated from 0-2, where
0 is “This task is not valid or useful at all” and 2 is
“This task demonstrates an interesting and useful
skill”.

Criteria of Class 0 (low rating) Tasks

13Full instructions for task quality annotations: https:

//bit.ly/3veIWf7

Examples of Tasks for Annotation
Task 1

input

output

input

output

input

output

[Question] The parotid glands are lo-
cated: [Answer]
cheek

[Question] The roof of the mouth is
called the: [Answer]
hard palte

[Question] The bone that forms the pos-
terior portion of the skull is the [An-
swer]
occipital bone

input

[Question] The lower jawbone is the
[Answer]
output mandible

Task 2

input

output

input

output

input

output

[If you want to ...] Get a page or site
removed from Google [Then ...]
Submit a URL removal request.

[If you want to ...] Report spam [Then
...]
Submit a spam report.

[If you want to ...] Report a copyright
violation or the misuse of your content
[Then ...]
File a DMCA takedown request.

input

[If you want to ...] Tell Google to crawl
your site more slowly [Then ...]
output Request a change in crawl rate.

input

output

[If you want to ...] Tell Google that your
content is mistakenly being ﬁltered by
SafeSearch [Then ...]
Submit a SafeSearch issue.

Table 7: Example tasks provided with the instructions
for the task-quality annotation

• The input-output mapping appears nonsensi-

cal and/or arbitrary.

• The task is not in English.

• Would never be useful in any realistic set-
ting / practicing this task does not build any
generally-useful skills.

• Tests highly obscure knowledge that is not
correlated with the input text (highly context-
dependent knowledge, entertainment trivia on

fan sites, product speciﬁcations, . . . )

• You would not even be able to tell if all output

labels have been shufﬂed.

Criteria of Class 1 (medium rating) Tasks

• The "Examples from real NLP datasets" sec-
tion in the full instructions13 show the kinds
of interesting tasks we would like to see in
Class 2, but we expect (and encourage) that
our tasks will span a wider variety that are still
interesting and valuable.

• This class is a catch-all for tasks that are nei-

ther squarely Class 0 nor Class 2.

F Examples of tasks

In the following pages, we provide examples from
various datasets discussed in the text:

1. Quality-annotated (High)

2. Quality-annotated (Med)

3. Quality-annotated (Low)

4. Single-website (support.google.com)

5. Single-website (w3.org)

6. Single-website (mmo-champion)

7. Single-website (studystack.com)

8. Cluster 7

9. Cluster 8

10. Cluster -1

11. Cluster 3

12. NLP train (2 best and 2 worst)

13. NLP test (10 most-improving)

• The task is quite interesting, but its current
form contains ﬂaws that make it confusing or
lacks enough context to do a good job of the
task.

• You could narrow the space of possible op-
tions and guess the right answer with better-
than-random accuracy (especially with the
help of multiple-choice options).

• The task makes sense but is trivial or not in-
teresting enough to be Class 2. For example,
the output is just a copy of the input.

Criteria of Class 2 (high rating) Tasks

• The task is well-posed with enough context
that an expert could give a reasonably correct
answer most of the time.

• Demonstrates a skill that is deﬁnitely useful
for real-world tasks, i.e. might be tested in an
exam or competency test, or part of a job.

• Resembles the type of skill that is tested
in typical NLP datasets. See "Examples
from real NLP datasets" section in the full
instructions13.

Further notes

• These criteria are not a complete set of rules
for membership, so based on the above you
may make your own judgement regarding a
new task that does not perfectly ﬁt any criteria.

• We expect that the majority of our tasks will
fall into either Class 0 or Class 1; fewer than
20% of the tasks will meet the standard for
Class 2.

• A single input may not always be enough to
know what the task expects in the output; this
is acceptable (even for Class 2) as long as the
input-output mapping is clear after observing
several demonstration pairs.

Train Tasks (90 tasks)
ade_corpus_v2-classiﬁcation (Gurulingappa et al., 2012), ade_corpus_v2-dosage (Gurulingappa et al., 2012), art (Bha-
gavatula et al., 2020), biomrc (Pappas et al., 2020), blimp-anaphor_number_agreement (Warstadt et al., 2020), blimp-
ellipsis_n_bar_2 (Warstadt et al., 2020), blimp-sentential_negation_npi_licensor_present (Warstadt et al., 2020), blimp-
sentential_negation_npi_scope (Warstadt et al., 2020), boolq (Clark et al., 2019), circa (Louis et al., 2020), crows_pairs (Nangia
et al., 2020), discovery (Sileo et al., 2019), emotion (Saravia et al., 2018), ethos-directed_vs_generalized (Mollas et al.,
2020), ethos-disability (Mollas et al., 2020), ethos-gender (Mollas et al., 2020), ethos-sexual_orientation (Mollas et al., 2020),
freebase_qa (Jiang et al., 2019), gigaword (Napoles et al., 2012), glue-cola (Warstadt et al., 2019), glue-sst2 (Socher et al.,
2013), google_wellformed_query (Faruqui and Das, 2018), hate_speech_offensive (Davidson et al., 2017), hatexplain (Mathew
et al., 2020), health_fact (Kotonya and Toni, 2020), hotpot_qa (Yang et al., 2018), imdb (Maas et al., 2011), kilt_ay2 (Hof-
fart et al., 2011), kilt_fever (Thorne et al., 2018), kilt_hotpotqa (Yang et al., 2018), kilt_nq (Kwiatkowski et al., 2019),
kilt_trex (Elsahar et al., 2018), kilt_zsre (Levy et al., 2017), lama-conceptnet (Petroni et al., 2019, 2020), lama-google_re (Petroni
et al., 2019, 2020), lama-squad (Petroni et al., 2019, 2020), lama-trex (Petroni et al., 2019, 2020), liar (Wang, 2017),
mc_taco (Zhou et al., 2019), numer_sense (Lin et al., 2020), onestop_english (Vajjala and Luˇci´c, 2018), piqa (Bisk et al.,
2020), proto_qa (Boratko et al., 2020), qa_srl (He et al., 2015), quoref (Dasigi et al., 2019)12, race-high (Lai et al., 2017),
race-middle (Lai et al., 2017), ropes (Lin et al., 2019), rotten_tomatoes (Pang and Lee, 2005), search_qa (Dunn et al., 2017),
sms_spam (Almeida et al., 2011), social_i_qa (Sap et al., 2019a), spider (Yu et al., 2018), squad-no_context (Rajpurkar et al.,
2016), squad-with_context (Rajpurkar et al., 2016), superglue-multirc (Khashabi et al., 2018), superglue-record (Zhang et al.,
2018), superglue-rte (Dagan et al., 2005; Bar-Haim et al., 2006)(Giampiccolo et al., 2007; Bentivogli et al., 2009), superglue-
wic (Pilehvar and Camacho-Collados, 2019), superglue-wsc (Levesque et al., 2012), trec (Li and Roth, 2002; Hovy et al., 2001),
trec-ﬁnegrained (Li and Roth, 2002; Hovy et al., 2001), tweet_eval-emoji (Barbieri et al., 2020), tweet_eval-emotion (Barbieri
et al., 2020), tweet_eval-irony (Barbieri et al., 2020), tweet_eval-offensive (Barbieri et al., 2020), tweet_eval-sentiment (Bar-
bieri et al., 2020), tweet_eval-stance_abortion (Barbieri et al., 2020), tweet_eval-stance_climate (Barbieri et al., 2020),
tweet_eval-stance_hillary (Barbieri et al., 2020), tweet_qa (Xiong et al., 2019), uniﬁedqa:boolq (Clark et al., 2019), uni-
ﬁedqa:commonsenseqa (Talmor et al., 2019), uniﬁedqa:drop (Dua et al., 2019), uniﬁedqa:narrativeqa (Kociský et al., 2018),
uniﬁedqa:natural_questions_with_dpr_para, uniﬁedqa:newsqa (Trischler et al., 2017), uniﬁedqa:physical_iqa (Bisk et al.,
2020), uniﬁedqa:quoref (Dasigi et al., 2019), uniﬁedqa:race_string (Lai et al., 2017), uniﬁedqa:ropes (Lin et al., 2019), uni-
ﬁedqa:social_iqa (Sap et al., 2019b), uniﬁedqa:squad1_1 (Rajpurkar et al., 2016), uniﬁedqa:squad2 (Rajpurkar et al., 2018), uni-
ﬁedqa:winogrande_xl (Sakaguchi et al., 2020a), web_questions (Berant et al., 2013), wikisql (Zhong et al., 2017), xsum (Narayan
et al., 2018), yahoo_answers_topics (link), yelp_review_full (Zhang et al., 2015)
Test Tasks (52 tasks)
ag_news Gulli (link), ai2_arc (Clark et al., 2018), amazon_polarity (McAuley and Leskovec, 2013), anli (Nie et al.,
2020), climate_fever (Diggelmann et al., 2020), codah (Chen et al., 2019), commonsense_qa (Talmor et al., 2019), cos-
mos_qa (Huang et al., 2019), dbpedia_14 (Lehmann et al., 2015), dream (Sun et al., 2019), emo (Chatterjee et al., 2019),
ethos-national_origin (Mollas et al., 2020), ethos-race (Mollas et al., 2020), ethos-religion (Mollas et al., 2020), ﬁnan-
cial_phrasebank (Malo et al., 2014), glue-mnli (Williams et al., 2018), glue-mrpc (Dolan and Brockett, 2005), glue-qnli (Ra-
jpurkar et al., 2016), glue-qqp (data.quora.com/First-Quora-Dataset-Release-Question-Pairs), glue-
rte (Dagan et al., 2005; Bar-Haim et al., 2006)(Giampiccolo et al., 2007; Bentivogli et al., 2009), glue-wnli (Levesque et al.,
2012), hate_speech18 (de Gibert et al., 2018), hellaswag (Zellers et al., 2019), medical_questions_pairs (McCreery et al.,
2020), openbookqa (Mihaylov et al., 2018), paws (Zhang et al., 2019), poem_sentiment (Sheng and Uthus, 2020),
qasc (Khot et al., 2020), quail (Rogers et al., 2020), quarel (Tafjord et al., 2019a), quartz-no_knowledge (Tafjord
et al., 2019b), quartz-with_knowledge (Tafjord et al., 2019b), sciq (Welbl et al., 2017), scitail (Khot et al., 2018),
sick (Marelli et al., 2014), superglue-cb (de Marneffe et al., 2019), superglue-copa (Gordon et al., 2012), swag (Zellers
et al., 2018), tab_fact (Chen et al., 2020), tweet_eval-hate (Barbieri et al., 2020), tweet_eval-stance_atheism (Barbieri
et al., 2020), tweet_eval-stance_feminist (Barbieri et al., 2020), uniﬁedqa:ai2_science_middle (data.allenai.org/
ai2-science-questions), uniﬁedqa:mctest (Richardson et al., 2013), uniﬁedqa:openbookqa (Mihaylov et al., 2018),
uniﬁedqa:openbookqa_with_ir, uniﬁedqa:qasc (Khot et al., 2019), uniﬁedqa:qasc_with_ir, wiki_qa (Yang et al., 2015),
wino_grande (Sakaguchi et al., 2020b), wiqa (Tandon et al., 2019), yelp_polarity (Zhang et al., 2015)
Dev Tasks (50 tasks)
ade_corpus_v2-classiﬁcation,
blimp-
sentential_negation_npi_licensor_present, blimp-sentential_negation_npi_scope, boolq, circa, crows_pairs, discovery,
emotion, ethos-directed_vs_generalized, ethos-disability, ethos-gender, ethos-sexual_orientation, glue-cola, glue-sst2,
google_wellformed_query, hate_speech_offensive, hatexplain, health_fact, imdb, kilt_fever, liar, mc_taco, numer_sense,
onestop_english, piqa, race-high, race-middle, rotten_tomatoes, sms_spam, social_i_qa, superglue-multirc, superglue-rte,
superglue-wic, superglue-wsc, trec, trec-ﬁnegrained, tweet_eval-emoji, tweet_eval-emotion, tweet_eval-irony, tweet_eval-
tweet_eval-stance_hillary,
offensive,
yahoo_answers_topics, yelp_review_full

blimp-anaphor_number_agreement,

tweet_eval-stance_abortion,

tweet_eval-stance_climate,

blimp-ellipsis_n_bar_2,

tweet_eval-sentiment,

biomrc,

art,

Figure 7: All the task datasets used in our MetaICL experiments, along with citations of their original source. Dev
Tasks are a subset of Train Tasks so citations are not repeated.

quality_annotated : High
Task 1 (6 examples)

input
output

[Format option] Heading 3 [What it will look like]
is a sub-header and can be used as a sub-section heading

input
[Format option] Code / preformatted [What it will look like]
output Technical text that should be displayed in a ﬁxed-width font

input
output

[Format option] Heading 5 [What it will look like]
is the smallest sub-header option

Task 2 (10 examples)

[No.] 07 [Answer] Sahara desert [Question]

input
output The biggest desert in the world is the

[No.] 02 [Answer] Nile [Question]
input
output The longest river in the world is the

[No.] 05 [Answer] Everest [Question]

input
output The highest mountain in the world is the

input
output

input
output

input
output

input

Task 3 (6 examples)

[property] monitorType [applies to] all [description] one of counter, guage, string [type]
enum

[property] observedAttribute [applies to] all [description] the attribute being observed [type]
string

[property] initThreshold [applies to] counter [description] initial threshold value [type]
number

Task 4 (14 examples)

[Verse] 14 [King James Version] And she lay at his feet until the morning: and she rose up
before one could know another. And he said, Let it not be known that a woman came into the
ﬂoor. So she lay at his feet until morning. She got up before either could know the other. He
said, "Don’t let it be known that a woman came into the threshing-ﬂoor." [Analysis]

output Boaz wants to avoid scandal.

input

[Verse] 5 [King James Version] And she said unto her, All that thou sayest unto me I will do.
Ruth said to her, "I will do everything you say." [Analysis]
output What Ruth must have thought of these orders, none can speculate.

input

[Verse] 1 [King James Version] Then Naomi her mother in law said unto her, My daughter,
shall I not seek rest for thee, that it may be well with thee? Now Naomi, mother-in-law of Ruth,
said to her, "My daughter, I should ﬁnd you a place of rest, that will be good for you. [Analysis]

output Naomi wants to settle Ruth properly.

quality_annotated : Med
Task 1 (11 examples)

input

output

input

[Symptom] Sore Throat [Cold] Sore throat is commonly present with a cold. [Flu] Sore throat
is not commonly present with the ﬂu. [Allergies]
Sore throat is sometimes present if enough post-nasal drainage occurs.

[Symptom] Sudden Symptoms [Cold] Cold symptoms tend to develop over a few days. [Flu]
The ﬂu has a rapid onset within 3-6 hours. The ﬂu hits hard and includes sudden symptoms like
high fever, aches and pains. [Allergies]

output Rapid onset.

input

[Symptom] Aches [Cold] Slight body aches and pains can be part of a cold. [Flu] Severe aches
and pains are common with the ﬂu. [Allergies]

output No aches and pains.

Task 2 (9 examples)

input

[0] Space Requirements Larger due to the existence of aggregation structures and history data;
requires more indexes than OLTP

output Can be relatively small if historical data is archived

input

[0] Backup and Recovery Instead of regular backups, some environments may consider simply
reloading the OLTP data as a recovery method

output Backup religiously; operational data is critical to run the business, data loss is likely to entail

signiﬁcant monetary loss and legal liability

[0] Queries Often complex queries involving aggregations

input
output Relatively standardized and simple queries Returning relatively few records

Task 3 (7 examples)
input
[Action] Add a point to an editable shape [Shortcut]
output Option-click the shape edge where you want to add a point

[Action] Change a curved point of an editable shape into a corner point [Shortcut]

input
output Double-click the curved point

[Action] Delete a point of an editable shape [Shortcut]

input
output Click point and press Delete

Task 4 (8 examples)

input
output

input
output

input
output

[0] Length [1] meter [2]
distance light travels in a vacuum

[0] Time [1] second [2]
oscillations of the cesium atom

[0] Electric current [1] ampere [2]
attraction between two wires

quality_annotated : Low
Task 1 (285 examples)

input

[Career Cluster] Manufacturing [Career Title] Stationary Engineers and Boiler Operators
[Nontraditional for...]

output Women

input

[Career Cluster] Health Science [Career Title] Health Care Social Workers [Nontraditional
for...]
output Men

input

[Career Cluster] Government and Public Administration [Career Title] Government Program
Eligibility Interviewers [Nontraditional for...]

output Men

[RESTRICTED] YES CONFIDENTIAL [UNRESTRICTED]

input
output NO (Sensitive/need to know)

Task 2 (8 examples)

[RESTRICTED] Available COUNSELING SERVICES [UNRESTRICTED]

input
output Available

[RESTRICTED] Active Duty Military Only ELIGIBILITY [UNRESTRICTED]

input
output All personnel

[Talent Cards] Beat Back [Type]

input
output Melee

Task 3 (6 examples)

input
output

input
output

input
output

input
output

input
output

[Type]
Insanity

[Talent Cards] Clear Minded [Type]
Focus

[Directive] odbc.default_db [Master Value] no value [Local Value]
no value

Task 4 (10 examples)

[Directive] odbc.defaultlrl [Master Value] return up to 4096 bytes [Local Value]
return up to 4096 bytes

[Directive] odbc.defaultbinmode [Master Value] return as is [Local Value]
return as is

single_website_tables : support.google.com
Task 1 (6 examples)

input
output

[If you want to ...] Report a copyright violation or the misuse of your content [Then ...]
File a DMCA takedown request.

[If you want to ...] Tell Google to crawl your site more slowly [Then ...]

input
output Request a change in crawl rate.

input
output

[If you want to ...] Get a site added back to Google [Then ...]
If your site was distributing malware, and is now clean, request a malware review. If your
site was showing spam, but is now clean, submit a reconsideration request. If your site was in
violation of the Webmaster Guidelines, but is now clean, submit ... (Truncated)

Task 2 (6 examples)

input

[Term] Impressions [Search Console usage] Used exclusively for Google Search impressions
[Analytics usage]

output Used for both AdWords impressions and Google Search impressions

input

[Term] CTR [Search Console usage] Clickthrough rate. Clicks/Impressions for Google Search
clicks. [Analytics usage]

output Clickthrough rate. Clicks/Impressions for both AdWords and Google Search clicks.

input

[Term] Average Position [Search Console usage] Average ranking in Google Search results
[Analytics usage]

output Average ranking in Google Search results

Task 3 (7 examples)

input

[Setting] Devices [Description] Campaigns target all types of devices, which include desktops,
tablets, and mobile devices. Later, you can choose to customize ads for different devices. [Learn
more]

output Types of mobile ads

input

[Setting] Locations and languages [Description] Your campaign’s ads are eligible to show
to customers in your targeted geographic locations, or to customers who have selected your
targeted language as their interface language. We recommend choosing t ... (Truncated)

output Location and language targeting

input

[Setting] Type [Description] The campaign type determines which settings we’ll show you as
you create or edit your campaign. The type you choose tailors the campaign setup to just what’s
appropriate for your goals, eliminating unrelated features. We ... (Truncated)

output Choosing the campaign type that’s right for you

Task 4 (6 examples)

[Then ...] File a DMCA takedown request. [If you want to ...]

input
output Report a copyright violation or the misuse of your content

[Then ...] Submit a URL removal request. [If you want to ...]

input
output Get a page or site removed from Google

input

[Then ...] If your site was distributing malware, and is now clean, request a malware review. If
your site was showing spam, but is now clean, submit a reconsideration request. If your site
was in violation of the Webmaster Guidelines, but is now cle ... (Truncated)

output Get a site added back to Google

single_website_tables : w3.org
Task 1 (23 examples)

input

[Keyword] week [Data type] A date consisting of a week-year number and a week number with
no time zone [Control type] A week control [State]

output Week

[Keyword] hidden [Data type] An arbitrary string [Control type] n/a [State]

input
output Hidden

input

output

[Keyword] password [Data type] Text with no line breaks (sensitive information) [Control type]
A text ﬁeld that obscures data entry [State]
Password

Task 2 (6 examples)

[Attribute Name] next [Details]
an ECMAScript expression which returns the URI of the CCXML document to be fetched.

[Attribute Name] timeout [Details]
is an ECMAScript expression returning a string in CSS2 [CSS2] format interpreted as a time
interval. The interval begins when the is executed. The fetch will fail if not completed at the
end of this interval. A failed fetch will return the error.fetc ... (Truncated)

[Attribute Name] synch [Details]
is an ECMAScript left-hand-side expression that is set to the fetch completion event. The
speciﬁcation of this attribute in a implies a blocking fetch, which will be executed synchronously.
If this attribute is not speciﬁed, the fetch is asynchrono ... (Truncated)

Task 3 (7 examples)

[Function] DeleteScope [Arguments] name(optional) [Description] Removes a scope from the
scope stack. If no name is provided, the topmost scope is removed. Otherwise the scope with
provided name is removed. A Failure status is returned if the stack i ... (Truncated)
Success or Failure

[Function] CreateScope [Arguments] name(optional) [Description] Creates a new scope object
and pushes it on top of the scope stack. If no name is provided the scope is anonymous and
may be accessed only when it on the top of the scope stack. A Failur ... (Truncated)
Success or Failure

[Function] UpdateVariable [Arguments] variableName, newValue, scopeName(optional) [De-
scription] Assigns a new value to the variable speciﬁed. If scopeName is not speciﬁed, the
variable is accessed in the topmost scope on the stack. A Failure status ... (Truncated)
Success or Failure

[Event Type] help [Action] reprompt [Audio Provided]
yes

Task 4 (9 examples)

[Event Type] noinput [Action] reprompt [Audio Provided]
no

[Event Type] exit [Action] exit interpreter [Audio Provided]
no

input
output

input
output

input
output

input

output

input

output

input

output

input
output

input
output

input
output

single_website_tables : mmo-champion.com
Task 1 (15 examples)

[Level] 384 [Type] Leather [Spec] Feral [Slot] Legs [Name]

input
output Deep Earth Legguards

[Level] 384 [Type] Leather [Spec] Feral [Slot] Chest [Name]

input
output Deep Earth Raiment

[Level] 384 [Type] Leather [Spec] Restoration [Slot] Shoulder [Name]

input
output Deep Earth Mantle

input

[Level] 384 [Type] Tier 13 [Slot] Token [Name] Crown of the Corrupted Protector [Instance]
Dragon Soul [Boss] LFR Warmaster Blackhorn [Spec]

Task 2 (23 examples)

output Armor

input

[Level] 384 [Type] Trinket [Slot] Trinket [Name] Bone-Link Fetish [Instance] Dragon Soul
[Boss] LFR All Bosses Except Deathwing [Spec]

output Melee

input

[Level] 384 [Type] Mace [Slot] Two-Hand [Name] Ataraxis, Cudgel of the Warmaster [Instance]
Dragon Soul [Boss] LFR Warmaster Blackhorn [Spec]

output Melee

input
output

input
output

input
output

input

[ilvl] 85 [Type] Enchant [Item] Lesser Inscription of Charged Lodestone [Slot]
Shoulder

Task 3 (12 examples)

[ilvl] 346 [Type] Finger [Spec] Physical DPS [Item] Terrath’s Signet of Balance [Slot]
Finger

[ilvl] 346 [Type] Finger [Spec] Melee [Item] Gorsik’s Band of Shattering [Slot]
Finger

[Level] 522 [Type] Mail [Spec] Physical DPS [Slot] Chest [Name] Carapace of Segmented
Scale [Req. Standing]

Task 4 (77 examples)

output Revered

input

[Level] 522 [Type] Leather [Spec] Physical DPS [Slot] Waist [Name] Darkfang Belt [Req.
Standing]

output Revered

input

output

[Level] 522 [Type] Trinket [Slot] Trinket [Name] Steadfast Talisman of the Shado-Pan Assault
[Req. Standing]
Friendly

single_website_tables : studystack.com
Task 1 (24 examples)

[Answer] hard palte [Question]

input
output The roof of the mouth is called the:

[Answer] middle ear [Question]

input
output The malleus, incus, and stapes are located in the:

[Answer] Volar [Question]
input
output The palm of the hand is called what?

Task 2 (15 examples)

[Answer] Evert/eversion [Question]

input
output Turning outward, typically used to describe ankle motion.

[Answer] Gliding motion [Question]

input
output Occurs when one bone slides over another. EX. kneecap

[Answer] Invert/inversion [Question]

input
output Turning inward, typically used to describe ankle motion,

[Deﬁnition] freewriting, clustering, mapping, questioning, brainstorming [Term]
prewriting techniques.

Task 3 (13 examples)

[Deﬁnition] 5 senses, be speciﬁc, use comparisions, similes, metophores. Eliminate ﬂuff words
[Term]
good writing techniques

input
output

input

output

[Deﬁnition] (1) a topic and (2) a controlling idea [Term]

input
output Two parts of a topic sentence

[Deﬁnition] the amount of space something takes up [Term]

input
output Mass

Task 4 (9 examples)

input
output

[Deﬁnition] a mixture made up of particles that are uniformly y distributed [Term]
homogeneous mixture

[Deﬁnition] the science of matter and how it changes [Term]

input
output Chemistry

cluster_tables : 7
Task 1 (7 examples)

[Cookie Name] __utmb [Cookie Length] 30 minutes [Description]

input
output Establish and continue a user session on the site

[Cookie Name] __utmz [Cookie Length] 6 months [Description]

input
output Used to track trafﬁc sources and page navigation

[Cookie Name] _UKWM [Cookie Length] 2 years [Description]

input
output Used to identify trafﬁc sources

Task 2 (8 examples)
input
[Cookie Name or Service] MoodleSessionTest MoodleSession MoodleID_ [Purpose]
output Our virtual learning environment, Moodle, uses cookies to record when visitors have success-

fully logged into the service.

[Cookie Name or Service] ASPSESSIONIDCQBSDQCQ [Purpose]

input
output This is a functional cookie that does not contain any personal information and is automatically

removed when the visitor closes their web browser.

[Cookie Name or Service] CAKEPHP [Purpose]

input
output This is a functional cookie that does not contain any personal information and is automatically

removed when the visitor closes their web browser.

Task 3 (9 examples)

[Cookie] guest_id, ki [Information]

input
output These cookies allow you to access the Twitter feed on the homepage.

[Cookie] use_hitbox [Information]

input
output This is downloaded when you play an embedded YouTube video.

[Cookie] BX, localization [Information]

input
output These cookies are downloaded by Flickr if you visit the page with the MEI Conference 2010

Photographs slideshow.

Task 4 (12 examples)

input

[Cookie] pmx_cbtstat{ID} [Origin] www.whymsical.com [Persistency] Current session only
[Information and Usage]

output These cookies are set to records the expand/collapse state for a CBT Navigator block content.

input

[Cookie] pmx_YOfs [Origin] www.whymsical.com [Persistency] Page load time [Information
and Usage]

output This cookie will probably never see you. It is set on portal actions like click on a page number.
The cookie is evaluated on load the desired page and then deleted. It is used to restore the
vertical screen position as before the click.

input

output

[Cookie] AWNUTSWhymsicalcom [Origin] www.whymsical.com [Persistency] Expires ac-
cording to user-chosen session duration [Information and Usage]
If you log-in as a member of this site, this cookie contains your user name, an encrypted hash of
your password and the time you logged-in. It is used by the site software to ensure that features
such as indicating new Forum and Private messages are ... (Truncated)

cluster_tables : 8
Task 1 (7 examples)

input

[0] Appearance [Scholarly Journals] Plain, “serious” cover Text with black & white graphs,
charts, and photographs which ... (Truncated)

output Generally glossy cover Color photographs and illustrations used to support the article as well

as draw in readers

input

output

input

[0] Examples [Scholarly Journals] American Journal of Education Journal of the Evangelical
Theological Society Modern Fiction Studies [Trade Journals]
Indiana Business Instrumentalist Preaching

[0] Validity [Scholarly Journals] Articles reviewed and evaluated by other experts in the ﬁeld /
discipline (peer reviewed / ... (Truncated)

output Articles may be reviewed by one editor with knowledge related to the topic

Task 2 (15 examples)

input

output

input

output

input

output

[DATABASE TITLE] Engineered Materials Abstracts [FULL DESCRIPTION] Comprehensive
index to world literature on engineered ... (Truncated)
no

[DATABASE TITLE] Engineering Research Database [FULL DESCRIPTION] The ProQuest
Engineering Research Database covers the ... (Truncated)
no

[DATABASE TITLE] ENGnetBASE [FULL DESCRIPTION] The ENGnetBase eBook collec-
tion includes over 2300 cutting-edge and bestselling ... (Truncated)
yes

Task 3 (20 examples)

input

[Access] Website [2] Choose My Plate The new food and dietary guidelines! Also included are
related links such as: farmer’s markets, nutrition labels and food safety. Created by the USDA.
[Subject]
output Health & Nutrition

input

[Access] Website [2] Library of Congress; Performing Arts Encyclopedia This is an amzing
guide to the performing arts. You can ... (Truncated)

output Art

input

[Access] Library Card Required [2] Encyclopedia Britannica This encyclopedia has A LOT of
information, which is great, but ... (Truncated)

output Cultures

Task 4 (6 examples)

input

[Time Frame of Event] Seconds/minutes/hours Provides sketchy details, may be inaccurate but
good for ﬁrsthand accounts [Information Resource]

output Television/radio/internet

input

output

input

[Time Frame of Event] Six months or more In depth analysis of event written by experts in
their ﬁeld. In most cases, ... (Truncated)
Scholarly Journals

[Time Frame of Event] Next day or two More details and greater accuracy, the ﬁrst rough draft
of history [Information Resource]

output Newspapers

cluster_tables : -1
Task 1 (7 examples)
[Domain Name] TinyHomeForSale.com [Price] $1,999 [Buy] Buy it Now [Keyword]

input
output Tiny Home For Sale

[Domain Name] DomainSalesHistory.com [Price] Offer [Buy] Buy it Now [Keyword]

input
output Domain Sales History

[Domain Name] NearbyForSale.com [Price] $999 [Buy] Buy it Now [Keyword]

input
output Nearby For Sale

Task 2 (8 examples)

[You are...] Supportive [You should have...]

input
output A strong stomach

[You are...] Dependable [You should have...]

input
output Good ethical standards

[You are...] Organized [You should have...]

input
output Excellent attention to detail

Task 3 (10 examples)

input
output

input
output

input
output

input

[Indonesian] perangko [English]
stamp

[Indonesian] surat [English]
letter

[Indonesian] terdaftar [English]
registered mail

[Endpoint/Outcome Measure] Vertebral Morphometry (6-point, 95-point) [Modality] X-Ray,
DXA, CT [Description]

Task 4 (9 examples)

output Automatic identiﬁcation of vertebral body margins

input

[Endpoint/Outcome Measure] Microarchitecture [Modality] MRI, High resolution QCT (HR-
pQCT) [Description]

output Measurement of trabecular and cortical bone microarchitecture

input

[Endpoint/Outcome Measure] Bone Marrow Edema (BME) [Modality] X-Ray, MRI [Descrip-
tion]

output Detection of pathogenic changes in the bone marrow of the femoral head

input
output

[COOKIE name] CATEGORY_INFO [COOKIE Description]
Stores the category info on the page, that allows to display pages more quickly.

cluster_tables : 3
Task 1 (25 examples)

[COOKIE name] FRONTEND [COOKIE Description]

input
output You sesssion ID on the server.

[COOKIE name] CART [COOKIE Description]

input
output The association with your shopping cart.

Task 2 (25 examples)

[COOKIE name] WISHLIST_CNT [COOKIE Description]

input
output The number of items in your Wishlist.

input
output

[COOKIE name] NO_CACHE [COOKIE Description]
Indicates whether it is allowed to use cache.

[COOKIE name] GUEST-VIEW [COOKIE Description]

input
output Allows guests to edit their orders.

[COOKIE name] CUSTOMER_AUTH [COOKIE Description]

input
output An indicator if you are currently logged into the store.

Task 3 (25 examples)

[COOKIE name] CUSTOMER [COOKIE Description]

input
output An encrypted version of your customer id with the store.

[COOKIE name] STORE [COOKIE Description]

input
output The store view or language you have selected.

input
output

[COOKIE name] NO_CACHE [COOKIE Description]
Indicates whether it is allowed to use cache.

Task 4 (25 examples)

[COOKIE name] LAST_CATEGORY [COOKIE Description]

input
output The last category you visited.

[COOKIE name] POLL [COOKIE Description]
input
output The ID of any polls you have recently voted in.

nlp_train
numer_sense (100 examples)
input All scorpions have an additional [MASK] segments after the initial seven, ending in a sharp

sting.
ﬁve

output

input Heart failure affects about [MASK] million people in the United States.
output

ﬁve

input
output

Ribosomes have [MASK] subunits - small and large.
two

input What are the names of the climbers, ordered by points descending?
SELECT Name FROM climber ORDER BY Points DESC
output

spider (100 examples)

input

Find the ﬁrst names and ofﬁces of all instructors who have taught some course and also ﬁnd the
course description.

output SELECT T2.emp_fname , T4.prof_ofﬁce , T3.crs_description FROM CLASS AS T1 JOIN
employee AS T2 ON T1.prof_num = T2.emp_num JOIN course AS T3 ON T1.crs_code =
T3.crs_code JOIN professor AS T4 ON T2.emp_num = T4.emp_num

input What is the county that produces the most wines scoring higher than 90?
output

SELECT T1.County FROM APPELLATIONS AS T1 JOIN WINE AS T2 ON T1.Appelation
= T2.Appelation WHERE T2.Score > 90 GROUP BY T1.County ORDER BY count(*) DESC
LIMIT 1

yahoo_answers_topics (100 examples)

question_title: man date women but has serious secret interest exclusively in men who are
women from waist up? [SEP] question_content: and who wear make-up etc - is he really
interested in men, and too afraid to come out of the closet or what? [SEP ... (Truncated)
Society & Culture

question_title: bungee jumping site in victoria??? [SEP] question_content: i am trying to ﬁnd a
site for bungee jumping around melbourne. i went thru the internet but couldnt ﬁnd much. can
anyone give me some info pls coz i ve been dreaming for t ... (Truncated)
Sports

question_title: celebs criminal conviction? [SEP] question_content: can anybody suggesting
some famous celebs or successful persons who’s got criminal conviction? [SEP] best_answer:
Lots of celebrity activists have had criminal convictions, usuall ... (Truncated)
Politics & Government

piqa (100 examples)

goal: Preserve expensive lipstick. [SEP] solution 1Keep in clothes drawer. [SEP] solution 2Keep
in fridge.
1

goal: How to wash a dog. [SEP] solution 1Wet the dog with warm water, apply shampoo, lather
and massage into fur, no need to rinse out all shampoo. Repeat process with conditioner if
desired. [SEP] solution 2Wet the dog with warm water, apply shampoo ... (Truncated)
1

goal: To add a light inside a lamp. [SEP] solution 1Get wire with a plug, and chain, and feed
the chain on. Then put on a washer -this should be decently big, and this is how the shade part
will be attached. Then tape the wire to the socket, and scre ... (Truncated)
1

input

output

input

output

input

output

input

output

input

output

input

output

nlp_test
ag_news (100 examples)
input Delegation Is Delayed Before Reaching Najaf AGHDAD, Iraq, Aug. 17 A delegation of Iraqis
was delayed for security reasons today but still intended to visit Najaf to try to convince a
rebellious Shiite cleric and his militia to evacuate a shrine in t ... (Truncated)

output World

input

Restive Maldives eases curfew after rounding up dissidents (AFP) AFP - A curfew in the capital
of the Maldives was eased but parliament sessions were put off indeﬁnitely and emergency rule
continued following last week’s riots, ofﬁcials and residen ... (Truncated)

output World

input Another Major Non-Factor Another major, another disappointment for Tiger Woods, the No.
1 ranked player in the world who has not won a major championship since his triumph at the
2002 U.S. Open.
Sports

output

amazon_polarity (100 examples)

input

output

input

output

input

output

title: Prompt shipment [SEP] content: I still haven’t had time to watch the video to comment
about the quality, but it was shipped promptly and seems to be in good order.
positive

title: Hey, we gotta talk [SEP] content: well, i gotta say this is one of her better albums. I’m
real is da bomb and so is the I’m real (murder remix) she and ja rule sound SOOOOOO ﬁne
together. Love dont’ cost a thing is hott too but Play is almost ... (Truncated)
positive

title: absolute lemon [SEP] content: I probably have as much experience with 11x17 capable
color printers as anyone in the world and I’ve got to say this is easily the most difﬁcult and
unsatisfactory printer I have ever dealt with. HP’s last 11x17 ... (Truncated)
negative

commonsense_qa (100 examples)

input What is the main purpose of farmers?
output

supply food

input When drinking booze what can you do to stay busy?
output

examine thing

input
output

If you are prone to postpone work what will you have to do in order to ﬁnish on time?
hasten

dbpedia_14 (100 examples)

input

Edward Albert Eddie Nicholls (born 10 December 1947 in British Guiana) is an international
Test and One Day International cricket umpire. Nicholls ofﬁciated in 17 Tests from 1997 to
2001 and 46 ODIs from 1995 to 2005. Nicholls is one of four West In ... (Truncated)

output Athlete

input
output

Cryptolepis is a plant genus in the family Apocynaceae. It includes some 42 species.
Plant

input Mike Enneking is a retired American soccer player who played professionally in the Western
Soccer Alliance and Major Indoor Soccer League. He is the Vice President of Crossﬁre Premier
Soccer Club.Enneking graduated from Bellarmine Preparatory School ... (Truncated)

output Athlete

nlp_test
ethos-race (87 examples)

input
output

input
output

In my dream I wished you have died! Don’t call yourself man, while talking like queen.
false

Is that your nose or is that Thor’s Hammer?
false

input All girls like you should have a pimp to guide them to successful prostitution
output

false

input
output

input
output

input
output

ﬁnancial_phrasebank (100 examples)

Proﬁt before taxes was EUR 5.4 mn , up from EUR 3.6 mn a year earlier .
positive

The order was valued at USD12 .2 m.
neutral

The company expects net sales to signiﬁcantly increase from 2009 .
positive

qasc (100 examples)

input what is tourette syndrome?
output

trait

input Animals that are _ provide little if any care to their young.
output

cold blooded

input What can be used for transportation?
output

trailers and boats

sciq (100 examples)
input All alkaline Earth metals have similar properties because they all have two valence electrons.
They readily give up their two valence electrons to achieve a full outer energy level, which is
the most stable arrangement of electrons. As a result, the ... (Truncated)
valence electrons

output

input

output

Exposure gives an indication of the amount of radiation that travels through the air. Two factors
inﬂuence the amount of exposure a person may receive – time and intensity. Acute exposure
indicates a large amount of radiation received over a short ... (Truncated)
chronic exposure

input Ventricular Systole Ventricular systole (see Figure 19.27) follows the depolarization of the
ventricles and is represented by the QRS complex in the ECG. It may be conveniently divided
into two phases, lasting a total of 270 ms. At the end of atrial ... (Truncated)
pulmonary and aortic semilunar

output

nlp_test
tweet_eval-stance_atheism (52 examples)

input

output

input

output

The worst day of my life so far is here, setting my Nan to rest. Even as a physicist, times like
these make you wonder. #SemST
none

I will dwell in a peaceful habitation, in secure dwellings, and in quiet resting places -Isa. 32:18
#SemST
against

input @user sweet! Congratulations to a rational decision. #SemST
output

none

yelp_polarity (100 examples)
input Very disappointed in this salon. Set an appt 4 days ahead of time. Area were I for my set put on
was dirty from a past client. The mail tech did not talk, I felt rushed through my appt which
resulted in me leaving unhappy. I won’t be returning.
negative

output

input Our ﬂight arrived to Vegas earlier than excepted, so we expected our room not to be ready.
When we arrived at the hotel on May 19th, the front desk girl offered us a room that was ready
on the 28th ﬂoor that wasn’t facing the Bellagio fountain. I b ... (Truncated)
positive

output

input My poor children who live out of state, have no idea how cheap and ugly the ﬂowers I just
received from Carmel Florist are. They do not resemble the online photo at all. I actually
laughed at the gentleman who delivered them to my door. They spent ... (Truncated)
negative

output

