2
2
0
2

n
u
J

4

]

G
L
.
s
c
[

1
v
9
0
9
1
0
.
6
0
2
2
:
v
i
X
r
a

Toward Learning Robust and Invariant Representations with
Alignment Regularization and Data Augmentation

Haohan Wang
Carnegie Mellon University
Pittsburgh, PA, USA
haohanw@cs.cmu.edu

Xindi Wu
Carnegie Mellon University
Pittsburgh, PA, USA
xindiw@andrew.cmu.edu

Zeyi Huang
University of Wisconsin-Madison
Madison, WI, USA
zeyih@andrew.cmu.edu

Eric P. Xing
Carnegie Mellon University
Pittsburgh, PA, USA
epxing@cs.cmu.edu

ABSTRACT
Data augmentation has been proven to be an eﬀective technique
for developing machine learning models that are robust to known
classes of distributional shifts (e.g., rotations of images), and align-
ment regularization is a technique often used together with data
augmentation to further help the model learn representations in-
variant to the shifts used to augment the data. In this paper, mo-
tivated by a proliferation of options of alignment regularizations,
we seek to evaluate the performances of several popular design
choices along the dimensions of robustness and invariance, for
which we introduce a new test procedure. Our synthetic exper-
iment results speak to the beneﬁts of squared ℓ2 norm regular-
ization. Further, we also formally analyze the behavior of align-
ment regularization to complement our empirical study under as-
sumptions we consider realistic. Finally, we test this simple tech-
nique we identify (worst-case data augmentation with squared ℓ2
norm alignment regularization) and show that the beneﬁts of this
method outrun those of the specially designed methods. We also
release a software package in both TensorFlow and PyTorch for
users to use the method with a couple of lines1.

CCS CONCEPTS
• Computing methodologies → Regularization; Computer vi-
sion; Supervised learning.

KEYWORDS
machine learning, data augmentation, robustness, trustworthy

ACM Reference Format:
Haohan Wang, Zeyi Huang, Xindi Wu, and Eric P. Xing. 2022. Toward
Learning Robust and Invariant Representations with Alignment Regular-
ization and Data Augmentation. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’22), August

1https://github.com/jyanln/AlignReg

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’22, August 14–18, 2022, Washington, DC, USA
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9385-0/22/08.
https://doi.org/10.1145/3534678.3539438

14–18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 13 pages.
https://doi.org/10.1145/3534678.3539438

1 INTRODUCTION
Data augmentation, i.e., to increase the dataset size through gener-
ating new samples by transforming the existing samples with some
predeﬁned functions, is probably one of the most often used tech-
niques to improve a machine learning model’s performance. It has
helped machine learning models achieve high prediction accuracy
over various benchmarks [e.g., 8, 19, 31, 38, 42, 70].

In addition to improving prediction accuracy, the community
has also leveraged data augmentation to help the models learn
more robust representations that can generalize to the datasets dis-
tributed diﬀerently [e.g., 32, 45, 51, 69]. To improve robustness, the
community usually designed the transformation functions used to
augment the data in correspondence to the transformations we see
in the real world [28], such as the changes of image texture or con-
trast. Thus, models trained with these augmented data are more
likely to be invariant to these designed transformations, such as
the texture or contrast variations of the input images.

To further help a model learn representations invariant to the
transformations, we can regularize the model so that the distance
between representations learned by the model from a pair of data
(the original one and the transformed counterpart) will be small.
This regularization has been used extensively recently to help mod-
els learn more robust and invariant representations [e.g., 26, 36, 40,
65]. Motivated by this popularity, this paper mainly studies the
behaviors of this regularization, which we refer to as alignment
regularization (AR). In particular, we seek to answer the question:
how should we use alignment regularization to take advantage of the
augmented data to the fullest extent to learn robust and invariant
models?

To answer this, we ﬁrst conduct a range of experiments over im-
age classiﬁcation benchmarks to evaluate how popular variants of
AR contribute to learning robust and invariant models. We test for
accuracy, robustness, and invariance, for which we propose a new
test procedure. Our empirical study favors the squared ℓ2 norm.
Our contributions of this paper are as follows.

 
 
 
 
 
 
KDD ’22, August 14–18, 2022, Washington, DC, USA

Wang, et al.

• With a new invariance test, we show that alignment regulariza-
tion is important to help the model learn representations invari-
ant to the transformation function, and squared ℓ2 norm is con-
sidered the favorable choice as assessed by a variety of empirical
evaluations (Section 4).

• We formalize a generalization error bound for models trained

with AR and augmented data (Section 5).

• We test the method we identiﬁed (squared ℓ2 norm as AR) in
multiple scenarios. We notice that this generic approach can
compete with methods specially designed for diﬀerent scenarios,
which we believe endorses its empirical strength (Section 6).

2 RELATED WORK AND KEY DIFFERENCES
Tracing back to the earliest convolutional neural networks, [60],
we notice that even early models for the MNIST dataset have been
boosted by data augmentation [1, 39, 50]. Later, the rapidly grow-
ing machine learning community has seen a proliferate develop-
ment of data augmentation techniques that have helped models
climb the state-of-the-art ladder [49]. Among the augmentation
techniques, the most relevant one to this paper is to generate the
samples (with constraint) that maximize the training loss along
with training [16].

While the above paragraph mainly discusses how to generate
the augmented samples, we mainly study how to train the models
with augmented samples. For example, instead of directly mixing
augmented samples with the original samples, one can consider
regularizing the representations (or outputs) of original samples
and augmented samples to be close under a distance metric (which
we refer to as alignment regularization, AR). Many concrete ideas
have been explored in diﬀerent contexts. For example, ℓ2 distance
and cosine similarities between internal representations in speech
recognition [40], squared ℓ2 distance between logits [36], or KL
divergence between softmax outputs [65] in adversarially robust
vision models, Jensen–Shannon divergence (of three distributions)
between embeddings for texture invariant image classiﬁcation [26].
These are but a few highlights of the concrete and successful imple-
mentations for diﬀerent applications out of a vast collection (e.g.,
[4, 24, 47, 48, 62, 63, 68, 69]), and we can expect methods permuting
these three elements (distance metrics, representation or outputs,
and applications) to be discussed in the future. Further, given the
popularity of GAN [21] and domain adversarial neural network
[17], we can also expect the distance metric generalizes to a spe-
cialized discriminator (i.e., a classiﬁer), which can be intuitively
understood as a calculated (usually maximized) distance measure,
and one example here is the Wasserstein-1 metric [3, 23].

Key Diﬀerences: With this rich collection of regularizing choices,

which one method should we consider in general? More impor-
tantly, do we need the regularization at all? These questions are
important for multiple reasons, especially since sometimes AR may
worsen the results [35]. In this paper, we ﬁrst conduct an empirical
study to show that AR (especially squared ℓ2 norm) can help learn
robust and invariant models, we then also derive generalization
error bounds to complement our empirical conclusion.

There are also several previous discussions regarding the de-
tailed understandings of data augmentation [10, 15, 20, 30, 46, 64,

66], among which, [64] is probably the most relevant as it also de-
fends the usage of the AR. In addition to what is reported in [64],
our work also connects to invariance and shows that another ad-
vantage of AR is to learn invariant representations.

3 ACCURACY, ROBUSTNESS, AND

INVARIANCE

This section discusses the three major evaluation metrics we will
use to test AR. We will ﬁrst recapitulate the background of accu-
racy and robustness. Then we will introduce our deﬁnition of in-
variance and our proposed evaluation.

Notations (X, Y) denotes the data, where X ∈ R𝑛×𝑝 and Y ∈
{0, 1}𝑛×𝑘 (one-hot vectors for 𝑘 classes). (x, y) denotes a sample.
𝑓 (, 𝜃) denotes the model, which takes in the data and outputs the
softmax (probabilities of the prediction) and 𝜃 denotes the corre-
sponding parameters. 𝑔() completes the prediction (i.e., mapping
softmax to one-hot prediction). 𝑎() denotes a function for data aug-
mentation, i.e., a transformation function. 𝑎 ∈ A, which is the
set of transformation functions of interest. P denotes the distribu-
tion of (x, y). For any sampled (x, y), we can have (𝑎(x), y), and
we use P𝑎 to denote the distribution of these transformed samples.
𝜃 ) for
Further, we use Q
(x, y) ∼ P. 𝐷 (·, ·) is a distance measure over two distributions. 𝑟 (𝜃)
· denotes the estimation of the term ·,
denotes the risk of model 𝜃.
e.g.,

𝜃 ) denotes the empirical risk of the estimated model.

to denote the distribution of 𝑓 (𝑎(x);

𝑎 (x),
𝜃

b

b

𝑟P (

b

b

b
3.1 Accuracy
The community studying the statistical property of the error bound
usually focuses on the expected risk deﬁned as

𝑟P (

𝜃) = E(x,y)∼P
b

I[𝑔(𝑓 (x;

𝜃 )) ≠ y],

(1)

where I[c] is a function that returns 1 if the condition c holds.

b

In practice, the error is evaluated by replacing P with a hold-out

test dataset, and the accuracy is 1 − 𝑟P (

𝜃 ).

3.2 Robustness
We deﬁne robustness as the worst-case expected risk when the test
data is allowed to be transformed by functions in A, following [e.g.,
22, 52]. Formally, we study the worst-case error as

b

𝑟P,A (

𝜃 ) = E(x,y)∼P max
𝑎∼A
b

b

I[𝑔(𝑓 (𝑎(x);

𝜃 )) ≠ y],

(2)

where we use 𝑟P,A (
𝜃 ) to denote the robust error as it will depend
on A. In practice, the robust error is also evaluated by replacing P
with a hold-out dataset.

b

3.3 Invariance
Further, high robustness performances do not necessarily mean the
model is truly invariant to the transformation functions [29], and
we continue to introduce a new test to evaluate the model’s behav-
ior in learning representations invariant to the transformations.

Invariance. If the model can learn a representation invariant to
the transformation functions, it will map the samples of diﬀerent
transformations to the same representation. Intuitively, to measure

Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation

KDD ’22, August 14–18, 2022, Washington, DC, USA

how invariant a model is to the transformations in A, we can cal-
culate the distances between each pair of the two transformed sam-
ples when a sample is transformed with functions in A. Thus, we
deﬁne the following term to measure invariance:

𝐼P,A (

𝜃) = sup

𝑎1,𝑎2 ∈A

b

𝐷 (Q

𝑎1 (x),
𝜃

, Q

𝑎2 (x),
𝜃

),

(3)

b

b

We suggest using Wasserstein metric as 𝐷 (·, ·), considering its fa-
vorable properties (e.g., see practical examples in Figure 1 of [14]
or theoretical discussions in [57]).

In practice, we also need to replace P with a hold-out dataset so
that the evaluation can be performed. In addition, we notice that
𝐼P,A (
𝜃 ), although intuitive, is not convenient in practice because
the evaluated values are not bounded. Thus, we reformulate it into
the following invariance test procedure, whose ﬁnal score will be
bounded between 0 and 1 (the higher, the better). Therefore, the
score can be conveniently discussed together with accuracy and
robust accuracy, which are also bounded between 0 and 1.

b

𝑎2 , . . . , X(𝑖)

Invariance test. Given a family of transformation functions used
in data augmentation A = {𝑎1(), 𝑎2 (), . . . , 𝑎𝑡 ()} of 𝑡 elements, and
a collection of samples (from the hold-out dataset) of the same la-
bel 𝑖, denoted as X(𝑖) , the evaluation procedure is as follows. We
ﬁrst generate the transformed copies of X(𝑖) with A, resulting in
X(𝑖)
𝑎1 , X(𝑖)
𝑎𝑡 . We combined these copies into a dataset, de-
noted as X (𝑖) . For every sample x in X (𝑖) , we retrieve its 𝑡 nearest
neighbors of other samples in X (𝑖), and calculate the overlap of
the retrieved samples with the transformed copies of x by A, i.e.,
{𝑎1 (x), 𝑎2 (x), . . . , 𝑎𝑡 (x)}. The calculated overlap score will be in
[0, 1] in general, but since the identity map is usually in A, this
score will usually be in [1/𝑡, 1].

During the retrieval of nearest neighbors, we consider the dis-

tance function of the two samples (namely x and x′) as

𝑑 (𝑓 (x;

𝜃 ), 𝑓 (x′;

𝜃 ))

b
𝜃 is the model we are interested in examining. In the em-
, where
pirical study later, we consider 𝑑 (u, v) = ku − vk1, with u and v
denoting two vectors. If we use other distance functions, the re-
ported values may diﬀer, but we notice that the rank of the meth-
ods compared in terms of this test barely changes.

b

b

Finally, we iterate through label 𝑖 and report the averaged score
for all the labels as the ﬁnal score. A higher score indicates the
model

𝜃 is more invariant to the transformation functions in A.
This invariance test procedure is formally presented in Algo-

b
rithm 1 below.

4 EMPIRICAL STUDY
In this section, we conduct experiments to study the relationship
between robustness and invariance, as well as how training with
AR can help improve the invariance score. In short, our empirical
study in this section will lead us to the following three major con-
clusions:

• High robust accuracy does not necessarily mean a high invari-

ance score and vice versa.

• AR can help improve the invariance score.

Algorithm 1: Invariance test

𝐼 (

𝜃 )

Result:
Input: a family of transformation functions A = {𝑎1 (), 𝑎2 (), . . . , 𝑎𝑡 () }, a hold-out

b

b

dataset (X, Y) , the model of interest

𝜃 , and a distance metric 𝑑 () ;

for every label 𝑖 do

identify all the samples from (X, Y) with label 𝑖, name this set of samples X(𝑖) ;
for every 𝑎 () ∈ A do
generate X

(𝑖)
𝑎 by applying 𝑎 () to every x ∈ X(𝑖) ;

b

end
(𝑖)
X (𝑖) = X
𝑎1
for every x ∈ X(𝑖) do

(𝑖)
𝑎2

∪ X

∪, . . . , ∪X

(𝑖)
𝑎𝑡 ;

generate set 𝑇 = {𝑎1 (x), 𝑎2 (x), . . . , 𝑎𝑡 (x) };
for every x′ ∈ X (𝑖) do

calculate the distance between x and x′ with 𝑑 ( 𝑓 (x;

𝜃 ), 𝑓 (x′;

𝜃 )) ;

end
b
retrieve the 𝑡 nearest neighbors of x out of X (𝑖) , and name this set of
samples 𝐾 ;

b

calculate the score for x with |𝑇 ∩ 𝐾 |/|𝑇 |, where |𝑇 | denotes the cardinality

of the set 𝑇 ;

end
calculate the score for label 𝑖 as the average score across all x ∈ X(𝑖) ;

end

calculate the ﬁnal score

𝐼 (

𝜃 ) as the average score across all the labels;

b

b

• Squared ℓ2 norm over logits is considered the empirically most
favorable AR option for learning robust and invariant represen-
tations.

4.1 Experiment Setup
Our empirical investigation is conducted over two benchmark datasets
(MNIST dataset with LeNet architecture and CIFAR10 dataset with
ResNet18 architecture) and three sets of the transformations.

Transformation Functions. We consider three sets of transforma-

tion functions:
• Texture: we use Fourier transform to perturb the texture of
the data by discarding the high-frequency components cut-oﬀ
by a radius 𝑟 , following [61]. The smaller 𝑟 is, the fewer high-
frequency components the image has. We consider

A = {𝑎0(), 𝑎12 (), 𝑎10 (), 𝑎8 (), 𝑎6 ()}
, where the subscript denotes the radius 𝑟 except that 𝑎0 () is the
identity map. We consider A during test time, but only 𝑎0 () and
𝑎6 () during training.

• Rotation: we rotate the images clockwise 𝑟 degrees with A =
{𝑎0(), 𝑎15 (), 𝑎30 (), 𝑎45 (), 𝑎60 ()}, where the subscript denotes the
degree of rotation and 𝑎0 () is the identity map. We consider A
during test time, but only 𝑎()0 and 𝑎60 () during training.

• Contrast: we create the images depicting the same visual in-
formation, but with diﬀerent scales of the pixels, including the
negative color representation. Therefore, we have A = {𝑎0 (x) =
x, 𝑎1 (x) = x/2, 𝑎2 (x) = x/4, 𝑎3(x) = 1−x, 𝑎4 (x) = (1−x)/2, 𝑎5(x) =
(1 − x)/4, where x stands for the image whose pixel values have
been set to be between 0 and 1. We consider A during test time,
but only 𝑎0 () and 𝑎3() during training.

Alignment Regularizations. We consider the following popular

choices of AR (with u and v denoting two vector embeddings):
L: ℓ1 norm of the vector diﬀerences, i.e., ku − vk1
S: squared ℓ2 norm of the vector diﬀerences, i.e., ku − vk2
2
C: cosine similarity, i.e., u𝑇 v/kuk · kvk

KDD ’22, August 14–18, 2022, Washington, DC, USA

Wang, et al.

Table 1: Test results on MNIST dataset for diﬀerent ARs over three evaluation metrics and three distribution shifts. B denotes
Baseline, i.e., the model does not use any data augmentation; V denotes vanilla augmentation, i.e., the model uses data aug-
mentation but not AR; L denotes ℓ1 norm; S denotes squared ℓ2 norm; C denotes cosine similarity; K denotes KL divergence;
W denotes Wasserstein-1 metric; D denotes GAN discriminator.

Texture

Rotation

Contrast

Accuracy
Robustness
Invariance
Accuracy
Robustness
Invariance
Accuracy
Robustness
Invariance

B
99.2±0.0
98.3±0.3
92.4±0.0
99.2±0.0
28.9±0.6
20.6±0.4
99.2±0.0
26.0±1.0
20.7±1.1

V
99.2±0.0
99.0±0.0
99.2±0.0
99.0±0.1
93.6±0.3
58.3±2.2
98.9±0.3
95.4±2.6
37.5±6.9

L
99.0±0.1
99.0±0.0
100±0.0
99.3±0.0
95.2±0.1
66.0±3.8
99.4±0.0
96.8±0.8
41.4±0.3

S
99.1±0.0
99.0±0.0
100±0.0
99.3±0.0
95.1±0.1
65.4±3.5
99.4±0.0
97.4±0.6
41.3±0.4

C
99.4±0.0
99.1±0.0
99.0±0.0
99.0±0.0
93.5±0.1
29.1±0.6
99.2±0.0
97.9±0.4
26.3±1.1

K
68.7±41
68.7±41
76.0±34
98.8±0.0
94.5±0.2
71.9±2.8
98.9±0.0
88.4±4.5
40.3±0.9

W
98.7±0.1
98.4±0.1
60.7±2.9
98.5±0.4
92.3±0.8
48.7±1.9
98.7±0.0
87.2±9.6
28.4±1.7

D
99.1±0.1
98.8±0.1
35.0±6.7
98.9±0.0
93.2±0.7
39.3±6.9
99.1±0.0
97.7±0.6
20.0±0.1

Table 2: Test results on CIFAR10 dataset for diﬀerent ARs over three evaluation metrics and three distribution shifts. Notations
are the same as in Table 1.

Texture

Rotation

Contrast

Accuracy
Robustness
Invariance
Accuracy
Robustness
Invariance
Accuracy
Robustness
Invariance

B
88.5±1.7
38.3±0.7
44.7±0.5
88.5±1.7
15.3±1.1
47.3±0.6
88.5±1.7
54.5±0.8
53.1±1.6

V
86.3±0.3
76.5±0.0
94.1±0.6
81.1±2.3
49.0±0.4
54.6±2.0
85.2±4.5
77.7±0.8
67.5±0.1

L
82.8±0.4
79.1±0.2
100±0.0
80.3±4.7
50.7±2.6
40.7±1.4
88.8±1.3
83.1±1.3
69.8±4.8

S
82.0±0.0
79.4±0.1
100±0.0
78.1±2.2
51.1±1.0
55.2±1.6
86.9±2.0
83.4±1.1
71.3±2.3

C
86.8±0.1
76.8±0.1
94.2±1.2
80.7±1.8
47.0±0.6
55.7±1.1
89.6±0.9
80.7±2.4
53.6±4.7

K
84.6±0.4
75.6±0.1
96.7±1.0
80.4±6.8
46.6±4.5
55.5±1.0
83.4±3.7
79.5±2.9
73.0±2.2

W
86.8±0.1
76.8±0.1
93.4±0.4
87.5±1.6
49.4±0.9
55.7±1.3
87.2±2.2
82.8±1.0
74.6±0.8

D
86.5±0.4
77.3±0.2
95.4±0.8
83.7±4.0
47.7±1.7
54.5±0.7
89.7±0.9
80.8±5.6
66.6±4.3

K: KL divergence over a batch of paired embeddings; the second

• Robustness: the worst-case accuracy when each sample can be

argument are augmented samples.

transformed with 𝑎 ∈ A.

W: Wasserstein-1 metric over a batch of paired embeddings, with

implementation following Wasserstein GAN [3, 23]

D: a vanilla GAN discriminator over a batch of paired embeddings,
the one-layer discriminator is trained to classify samples vs. aug-
mented samples.

We mainly discuss applying the AR to logits (embeddings prior
to the ﬁnal softmax function). We have also experimented with
applying to the ﬁnal softmax output and the embeddings one layer
prior to logits. Both cases lead to substantially worse results, so we
skip the discussion.

Hyperparameters. We use standard data splits. We ﬁrst train the
baseline models to get reasonably high performances (for MNIST,
we train 100 epochs with learning rate set to be 10−4; for CIFAR10,
we train 200 epochs with learning rate initialized to be 10−1 and
reduced one magnitude every 50 epochs; batch sizes in both cases
are set to be 128). Then we train other augmented models with the
same learning rate and batch size etc. The regularization weight is
searched with 8 choices evenly split in the logspace from 10−7 to 1.
For each method, the reported score is from the weight resulting
in the highest robust accuracy. We test with three random seeds.

Evaluation Metrics: We consider the three major evaluation met-

rics as we discussed in Section 3:

• Accuracy: test accuracy on the original test data.

• Invariance: the metric to test whether the learned represen-
tations are invariant to the transformation functions, as intro-
duced in Section 3.3.

4.2 Results and Discussion
Tables 1 and 2 show the empirical results across the three distribu-
tion shifts and the three evaluation metrics. First of all, no method
can dominate across all these evaluations, probably because of the
tradeoﬀ between accuracy and robustness [54, 61, 65]. Similarly,
the tradeoﬀ between accuracy and invariance can be expected from
the role of the regularization weight: when the weight is small, AR
has no eﬀect, and the model is primarily optimized for improving
accuracy; when the weight is considerable, the model is pushed
toward a trivial solution that maps every sample to the same em-
bedding, ignoring other patterns of the data. This is also the reason
that results in Tables 1 and 2 are selected according to the robust
accuracy.

Due to the tradeoﬀs, it may not be strategic if we only focus
on the highest number of each row. Instead, we suggest studying
the three rows of each test case together and compare the trade-
oﬀs, which is also a reason we reformulate the invariance test in
Section 3.3 so that we can have bounded invariance scores directly
comparable to accuracy and robust accuracy.

For example, in the texture rows of Table 1, while cosine simi-
larity can outperform squared ℓ2 norm in accuracy and robustness

Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation

KDD ’22, August 14–18, 2022, Washington, DC, USA

with 0.3 and 0.1 margins, respectively, it is disadvantageous in in-
variance with a larger margin (1.0). Similarly, for rotation rows of
Table 1, KL-divergence shows the overall highest scores, followed
by ℓ1 norm and squared ℓ2 norm. For contrast rows, both ℓ1 norm
and squared ℓ2 norm stand out. Overall, experiments in MNIST sug-
gest the desired choice to be ℓ1 norm or squared ℓ2 norm, and we
believe squared ℓ2 norm is marginally better.

On the other hand, the experiments in CIFAR10 in Table 2 mostly
favor ℓ1 norm and squared ℓ2 norm. Good performances can also be
observed from Wasserstein-1 metric for rotation. Also, we notice
that squared ℓ2 norm, in general, outperforms ℓ1 norm.

Thus, our empirical study recommends squared ℓ2 norm for AR
to learn robust and invariant models, with ℓ1 norm as a runner-up.

5 ANALYTICAL SUPPORT
According to our discussions in Section 3.3, Wasserstein metric is
supposed to be a favorable option as AR. However, its empirical
performance does not stand out. We believe this disparity is mainly
due to the diﬃculty in calculating the Wasserstein metric in prac-
tice.

On the other hand, norm-based consistency regularizations stand
out. We are interested in studying the properties of these metrics.
In particular, this section aims to complement the empirical study
by showing that norm-based AR can lead to bounded robust gen-
eralization error under certain assumptions.

Also, our experiments only use two transformation functions
from A during training but are tested with all the functions in A.
We also discuss the properties of these two functions and argue
that training with only these two functions can be a good strategy
when certain assumptions are met.

5.1 Overview of Analytical Results
All together, we need six assumptions (namely, A1-A6). Out of
these assumptions, A1 is necessary for using data augmentation,
and A4 is necessary for deriving machine learning generalization
error bound. A2 and A3 are properties of data transformation func-
tions, and A5 and A6 are technical assumptions. Along with intro-
ducing these assumptions, we will also oﬀer empirical evidence
showing that these assumptions are likely to hold in practice.

In particular, we will show that

• With A2 holds, ℓ1 norm can replace the empirical Wasserstein
metric to regularize invariance (Proposition A.2) and then we
can derive a bounded robust error if all the functions in A are
available (Theorem 5.1).

• With the above result and A3, we can derive a bounded robust
error if only two special functions in A (which we refer to as
vertices) are available (Lemma 5.2).

5.2 Assumptions Setup
5.2.1 Assumptions on Data Augmentation Functions. Our ﬁrst three
assumptions are for the basic properties of the data transforma-
tion functions used. These properties are formally introduced as
assumptions below.

A1: Dependence-preservation:

the transformation function will
not alter the dependency regarding the label (i.e., for any 𝑎() ∈ A,

𝑎(x) will have the same label as x) or the features (i.e., 𝑎1 (x1) and
𝑎2 (x2) are independent if x1 and x2 are independent).

Intuitively, “Dependence-preservation” has two perspectives: Label-
wise, the transformation cannot alter the label of the data, which
is a central requirement of almost all the data augmentation func-
tions in practice. Feature-wise, the transformation will not intro-
duce new dependencies between the samples.

We consider the label-wise half of this argument as a fundamen-
tal property of any data augmentations. It has to be always true
for data augmentation to be a useful technique. On the other hand,
the feature-wise half of this argument is a fundamental property
required to derive the generalization error bounds. Intuitively, we
believe this property holds for most data augmentation techniques
in practice.

A2: Eﬃciency: for

𝜃 and any 𝑎() ∈ A, 𝑓 (𝑎(x);

𝜃 ) is closer to x than

any other samples under a distance metric 𝑑𝑒 (·, ·), i.e.,

𝑑𝑒 (𝑓 (𝑎(x);

b
𝜃 ), 𝑓 (x;

𝜃 )) ≤ min
x′ ∈X−x

b
𝑑𝑒 (𝑓 (𝑎(x);

𝜃 ), 𝑓 (x′;

𝜃 )).

b
We deﬁne 𝑑𝑒 (·, ·) to be ℓ1 norm.
Intuitively, the eﬃciency property means the augmentation should

b

b

b

only generate new samples of the same label as minor perturba-
tions of the original one. If a transformation violates this property,
there should exist other simpler transformations that can generate
the same target sample.

A3: Vertices: For a model

𝜃 and a transformation 𝑎(), we use P

𝜃
𝑎,
𝜃 ) for (x, y) ∼ P. “Vertices”
to denote the distribution of 𝑓 (𝑎(x);
b
argues that exists two extreme elements in A, namely 𝑎+ and 𝑎−,
with certain metric 𝑑𝑥 (·, ·), we have
) = sup

, P

, P

b

b

)

𝑑𝑥 (P

𝑑𝑥 (P

𝑎+,
𝜃

𝑎−,
𝜃

b

b

𝑎1,𝑎2 ∈A

𝜃
𝑎1,

𝜃
𝑎2,

b

b

We deﬁne 𝑑𝑥 (·, ·) to be Wasserstein-1 metric.
Intuitively, “Vertices” suggests that there are extreme cases of
the transformations. For example, if one needs the model to be
invariant to rotations from 0◦ to 60◦, we consider the vertices to be
0◦ rotation function (thus identity map) and 60◦ rotation function.
In practice, one usually selects the transformation vertices with
intuitions or domain knowledge.

Notice that we do not need to argue that A3 always holds. All
we need is that A3 can sometimes hold, and when it holds, we can
directly train with the regularized vertex augmentation. Thus, any-
time RVA empirically performs well is a favorable argument for
A3. To show that RVA can sometimes perform well, we compare
the RVA with vanilla (non-regularized) worst-case data augmen-
tation (VWA) method across our synthetic experiment setup. We
notice that out of six total scenarios ({texture, rotation, contrast}
× {MNIST, CIFAR10}), RVA outperforms VWA frequently. This sug-
gests that the domain-knowledge of vertices can help in most cases,
although not guaranteed in every case.

5.2.2 Assumptions on Background and Generalization Error Bound.
In an abstract manner, when the test data and train data are from
the same distribution, several previous analyses on the generaliza-
tion error can be sketched as (see examples in A4):

𝑟P (

𝜃 ) ≤

𝑟P (

𝜃 ) + 𝜙 (|Θ|, 𝑛, 𝛿)

(4)

b

b

b

KDD ’22, August 14–18, 2022, Washington, DC, USA

Wang, et al.

b

b

𝜃) can be bounded by
which suggests that the expected risk 𝑟P (
𝜃 ) and a function of hypothesis space |Θ|
𝑟P (
the empirical risk
b
and number of samples 𝑛; 𝛿 accounts for the probability when the
bound holds. 𝜙 () is a function of these three terms. Dependent on
the details of diﬀerent analyses, diﬀerent concrete examples of this
generic term will need diﬀerent assumptions. We use a generic as-
sumption A4 to denote the assumptions required for each example.
Following our main goal to study how alignment regularization
and data augmentation help in accuracy, robustness, and invari-
ance, our strategy in theoretical analysis is to derive error bounds
for accuracy and robustness, and the error bound directly contains
terms to regularize the invariance. Further, as robustness naturally
bounds accuracy (i.e., 𝑟P (
𝜃) following the deﬁnitions in
𝜃 ) ≤ 𝑟P,A (
(1) and (2) respectively), we only need to study the robust error.

To study the robust error, we need two additional technical as-
sumptions. A5 connects the distribution of expected robust risk
and the distribution of empirical robust risk, and A6 connects the
0-1 classiﬁcation error and cross-entropy error.

b

b

A4: We list two examples here:

– when A4 is “Θ is ﬁnite, 𝑙 (·, ·) is a zero-one loss, samples are i.i.d”,

𝜙 (|Θ|, 𝑛, 𝛿) =

(log(|Θ|) + log(1/𝛿))/2𝑛

– when A4 is “samples are i.i.d”, 𝜙 (|Θ|, 𝑛, 𝛿) = 2R (L)+

p

(log 1/𝛿)/2𝑛,

where R (L) stands for Rademacher complexity and L = {𝑙𝜃 | 𝜃 ∈
Θ}, where 𝑙𝜃 is the loss function corresponding to 𝜃.

p

For more information or more concrete examples of the generic
term, one can refer to relevant textbooks such as [7].

A4 stands for the fundamental assumptions used to derive stan-
dard generalization bounds. We rely on this assumption as how
previous theoretical works rely on them.

A5: the distribution for expected robust risk equals the distribution for

empirical robust risk, i.e.,

arg max
P′ ∈𝑇 (P,A)

𝑟P′ (

𝜃 ) = arg max
P′ ∈𝑇 (P,A)

b

𝑟P′ (

𝜃 )

b

b

where 𝑇 (P, A) is the collection of distributions created by elements
in A over samples from P.

b

Eq. (2) can be written equivalently into the expected risk over a
pseudo distribution P′ (see Lemma 1 in [55]), which is the distribu-
tion that can sample the data leading to the expected robust risk.
𝜃 ) as a surro-
Thus, equivalently, we can consider supP′ ∈𝑇 (P,A) 𝑟P′ (
gate of 𝑟P,A (
𝜃), where 𝑇 (P, A) denotes the set of possible result-
ing distributions. Following the empirical strength of techniques
such as adversarial training [44], we introduce an assumption re-
lating the distribution of expected robust risk and the distribution
of empirical robust risk (namely, A5, in Appendix 5.2). Thus, the
bound of our interest (i.e., supP′ ∈𝑇 (P,A) 𝑟P′ (
𝜃 )) can be analogously
analyzed through supP′ ∈𝑇 (P,A)

𝑟P′ (
A5 is likely to hold in practice: Assumption A5 appears very
b
strong, however, the successes of methods like adversarial training
[44] suggest that, in practice, A5 might be much weaker than it
appears.

𝜃 ).

b

b

b

A6: With (x, y) ∈ (X, Y), the sample maximizing cross-entropy loss
𝜃 follows:

and the sample maximizing classiﬁcation error for model

∀x,

y⊤ 𝑓 (x;
inf𝑎 ∈A y⊤ 𝑓 (𝑎(x);

𝜃 )

b

𝜃 )

≥ exp

I(𝑔(𝑓 (x;

b
𝜃 )) ≠ 𝑔(𝑓 (x′;

𝜃 )))

(cid:0)

b

b

(cid:1)
(5)

where x′ stands for the sample maximizing classiﬁcation error, i.e.,

b

x′ = arg min

x

y⊤𝑔(𝑓 (x;

𝜃 ))

b

Also,

∀x,

| inf
𝑎 ∈A

y⊤ 𝑓 (𝑎(x);

𝜃 )| ≥ 1

(6)

b

Intuitively, although Assumption A6 appears complicated, it de-
scribes the situations of two scenarios:
𝜃 )) = 𝑔(𝑓 (x′;

𝜃 )), which means either the sample is
If 𝑔(𝑓 (x;
𝜃 or A is not rich enough for a transformation
misclassiﬁed by
function to alter the prediction, the RHS of Eq. 5 is 1, thus Eq. 5
always holds (because A has the identity map as one of its ele-
ments).

b

b

b

b

b

𝜃 )) ≠ 𝑔(𝑓 (x′;

If 𝑔(𝑓 (x;

𝜃 )), which means a transformation alters
the prediction. In this case, A6 intuitively states that the A is rea-
sonably rich and the transformation is reasonably powerful to cre-
ate a gap of the probability for the correct class between the orig-
inal sample and the transformed sample. The ratio is described as
the ratio of the prediction conﬁdence from the original sample over
the prediction conﬁdence from the transformed sample is greater
than 𝑒.

5.3 Analytical Support

Regularized Worst-case Augmentation. To have a model with a
small invariance score, we should probably directly regularize the
empirical counterpart of Eq. (3). However, Wasserstein distance is
diﬃcult to calculate in practice. Fortunately, Proposition A.2 con-
veniently allows us to use ℓ1 norm to replace Wasserstein metric.
With Proposition A.2, now we can oﬀer our main technical result
to study the robust error 𝑟P,A

𝜃 (as deﬁned in Eq. (2)).

Theorem 5.1. With Assumptions A1, A2, A4, A5, and A6, with

b

probability at least 1 − 𝛿, we have

𝑟P,A

𝜃 ≤

𝑟P (

𝜃 ) +

||𝑓 (x𝑖;

𝜃 ) − 𝑓 (x′
𝑖 ;

𝜃 )||1 + 𝜙 (|Θ|, 𝑛, 𝛿)

b

Õ𝑖
and x′ = 𝑎(x), where 𝑎 = arg min𝑎 ∈A y⊤ 𝑓 (𝑎(x);
deﬁned in A4.

b

b

b

b

𝜃 ). 𝜙 (|Θ|, 𝑛, 𝛿) is

b

This technical result immediately inspires the method to guaran-
tee worst case performance, as well as to explicitly enforce the con-
cept of invariance. The method 𝑎 = arg min𝑎 ∈A y⊤ 𝑓 (𝑎(x);
𝜃 ) is se-
lecting the transformation function maximizing the cross-entropy
loss (notice the sign diﬀerence between here and the cross-entropy
loss), which we refer to as worst-case data augmentation. This
method is also closely connected to adversarial training [e.g., 44].

b

Regularized Vertex Augmentation. As A in practice usually has
a large number of (and possibly inﬁnite) elements, we may not
always be able to identify the worst-case transformation function

Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation

KDD ’22, August 14–18, 2022, Washington, DC, USA

with reasonable computational eﬀorts. We further leverage the ver-
tex property (boundary cases of transformation functions, discussed
as Assumption A3 in the appendix) of the transformation function
to bound the worst-case generalization error:

Lemma 5.2. With Assumptions A1-A6, assuming there is a 𝑎′() ∈
𝑟P𝑎− (
, with probability at least

𝜃 ) +

𝜃 )

𝜃 ) = 1
2

𝑟P𝑎+ (
(cid:0)
b
𝜃 ) +

b
𝑟P𝑎− (

(cid:1)

b

b
𝜃 )

A where
𝑟P𝑎′ (
1 − 𝛿, we have:
1
2
+

b
𝜃 ) ≤

𝑟P,A (

b

b
𝑟P𝑎+ (
(cid:0)
b
Õ𝑖

b

||𝑓 (𝑎+(x𝑖 );
b

(cid:1)

𝜃 ) − 𝑓 (𝑎−(x′);
b

𝜃 )||1 + 𝜙 (|Θ|, 𝑛, 𝛿),

b
where 𝑎+ () and 𝑎−() are deﬁned in A3, and 𝜙 (|Θ|, 𝑛, 𝛿) in A4.

b

This result corresponds to the method that can be optimized con-
veniently without searching for the worst-case transformations.
However, the method requires good domain knowledge of the ver-
tices (i.e., boundary cases) of the transformation functions.

Thus, our theoretical discussions have complemented our em-
pirical ﬁndings in Section 4 by showing that norm-based regular-
izations can lead to bounded robust error. There is a disparity that
our analytical result is about ℓ1 norm while our empirical study
suggests squared ℓ2 norm. We conjecture the disparity is mainly
caused by the diﬃculty in passing the gradient of ℓ1 norm in prac-
tice.

6 EXPERIMENTS WITH ADVANCED

METHODS

We continue to test the methods we identiﬁed in comparison to
more advanced methods. Although we argued for the value of in-
variance, for a fair comparison, we will test the performances eval-
uated by the metrics the previous methods are designed for. Our
method will use the same generic approach and the same transfor-
mation functions as in the previous empirical study, although these
functions are not necessarily part of the distribution shift we test
now. In summary, our method can outperform (or be on par with)
these SOTA techniques in the robustness metric they are designed
for (Section 6.2). In addition, we run a side test to show that our
method can also improve accuracy (Section 6.3).

6.1 Methods
Section 4 and Section 5 lead us to test the following two methods:
• RVA (regularized vertex augmentation): using squared ℓ2 norm
as AR over logits between the original samples and the aug-
mented samples of a ﬁxed vertex transformation function (orig-
inal samples are considered as from another vertex).

• RWA (regularized worst-case augmentation): using squared ℓ2
norm as AR over logits between the original samples and the
worst-case augmented samples identiﬁed at each iteration. Worst-
case samples are generated by the function with the maximum
loss when we iterate through all the transformation functions.

6.2 Robustness

Rotation. We compare our results with rotation-invariant mod-
els, mainly Spatial Transformer (ST) [34], Group Convolution (GC)
[12], and Equivariant Transformer Network (ETN) [53]. We also

tried to run CGNet [37], but the method does not seem to scale to
the CIFAR10 and ResNet level. All these methods are tested with
ResNet34 following popular settings in the community. The results
are in Table 3. We test the models every 15◦ rotation from 0◦ ro-
tation to 345◦ rotation. Augmentation-related methods use the A
of “rotation” in synthetic experiments, so the testing scenario goes
beyond what the augmentation methods have seen during train-
ing.

We report two summary results in Table 3. “main” means the
average prediction accuracy from images rotated from 300◦ to 60◦
(passing 0◦), when the resulting images are highly likely to pre-
serve the class label. “all” means the average accuracy of all rota-
tions.

Our results can be interpreted from two perspectives. First, by
comparing all the columns in the ﬁrst panel to the ﬁrst column
of the other three panels, data augmentation and AR can boost
a vanilla model to outperform other advanced techniques. On the
other hand, by comparing the columns within each panel, data aug-
mentation and AR can further improve the performances of these
techniques.

Interestingly, the baseline model with our generic approach (RWA

in the ﬁrst panel) can almost compete with the advanced methods
even when these methods also use augmentation and AR (RWA in
GC panel). We believe this result strongly indicates the potential
of this simple augmentation and regularization method to match
the advanced methods.

In summary, RWA can boost the vanilla model to outperform ad-
vanced methods. Data augmentation and squared ℓ2 AR can further
improve the performances when plugged onto advanced methods.

Texture & Contrast. We follow [5] and compare the models for a
nine super-class ImageNet classiﬁcation [33] with class-balanced
strategies. Also, we follow [5] to report standard accuracy (Acc.),
weighted accuracy (WAcc.), a scenario where samples with un-
usual texture are weighted more, and accuracy over ImageNet-A
[27], a collection of failure cases for most ImageNet trained mod-
els. Additionally, we also report the performance over ImageNet-
Sketch [58], an independently collected ImageNet test set with only
sketch images. As [5] mainly aims to overcome the texture bias,
we also use our texture-wise functions in Section 4 for augmen-
tation. However, there are no direct connections between these
functions and the distribution shift of the test samples. Also, we
believe the distribution shifts here, especially the one introduced
by our newly added ImageNet-Sketch, are more than texture, and
also correspond to the contrast case of our study.

Following [5], the base network is ResNet, and we compare with
the vanilla network (Base), and several methods designed for this
task: including StylisedIN (SIN) [18], LearnedMixin (LM) [11], RUBi
(RUBi) [9] and ReBias (RB) [5]. Results are in Table 4.

The results favor our generic method in most cases. RVA outper-
forms other methods in standard accuracy, weighted accuracy, and
ImageNet-Sketch, and is shy from ReBias on ImageNet-A. RWA
shows the same pattern as that of RVA and further outperforms
RVA. Overall, these results validate the empirical strength of data
augmentation (even when the augmentation is not designed for
the task) and squared ℓ2 norm AR for learning robust models.

KDD ’22, August 14–18, 2022, Washington, DC, USA

Wang, et al.

Table 3: Comparison to advanced rotation-invariant models. We report the accuracy on the test sets rotated. “main” means the
resulting images are highly likely to be semantically the same as the original ones. “all” means the average accuracy of all rota-
tions. The underlined scores show that data augmentation and AR can help a vanilla model to compete with advanced methods.
The bold scores (highest at each row) show that data augmentation and AR can further improve the advanced methods.

ResNet

Base
45.4
31.2

RVA RWA Base
38.5
71.1
66.5
26.7
52.8
48.1

main
all

GC
RVA RWA Base
45.9
73.8
72.2
32.1
55.0
54.4

ST
RVA RWA Base
56.9
62.9
58.3
39.5
42.7
40.2

ETN
RVA RWA
57.7
65.1
46.1
52.6

Table 4: Comparison to advanced methods on 9 super-class
ImageNet classiﬁcation with diﬀerent distribution shifts.

Acc. WAcc.
Base
88.8
90.8
SIN
86.6
88.4
LM 67.9
65.9
RUBi
88.6
90.5
RB
90.5
91.9
RVA
91.2
92.2
RWA
91.6
92.8

ImageNet-A ImageNet-S

24.9
24.6
18.8
27.7
29.6
28.0
28.8

41.1
40.5
36.8
42.3
41.8
42.5
43.2

Table 5: The generic methods can also improve standard ac-
curacy.

ResNet18

ResNet50

ResNet101

B
75.6
93.1

RVA RWA
77.2
100
93.8
100

B
77.4
93.9

RVA RWA
78.2
100
94.4
100

B
77.8
94.4

RVA RWA
78.7
100
94.9
100

Top-1
Top-5

6.3 Accuracy
Further, these experiments help us notice that the generic tech-
nique can also help improve the accuracy, although the technique
is motivated by robustness and invariance. Therefore, we follow
the widely accepted CIFAR100 test pipeline and test the performances
of diﬀerent architectures of the ResNet family. The results are re-
ported in Table 5, where Base stands for the baseline model with
the default accuracy boosting conﬁgurations.

For both top-1 and top-5 accuracies and across the three ResNet
architectures, our techniques can help improve the accuracy. In
addition, we notice that our techniques can help bridge the gap of
diﬀerent architectures within the ResNet family: for example, RWA
helps ResNet50 to outperform the vanilla ResNet101.

7 CONCLUSION
In this paper, we seek to answer how to train with augmented data
so that augmentation can be taken to the fullest extent. We ﬁrst de-
ﬁned a new evaluation metric called invariance and conducted a
line of empirical studies to show that norm-based alignment regu-
larization can help learn robust and invariant models. Further, we
complement our observations with formal derivations of bounded
generalization errors. We notice that regularizing squared ℓ2 norm
between the logits of the originals samples and those of the aug-
mented samples is favorable: the trained model tends to have the
most favorable performances in robust accuracy and invariance. In

general, the method we recommend is “regularized worst-case aug-
mentation” with squared ℓ2 norm as the alignment regularization.
One can also consider “regularized vertex augmentation” when
extra assumptions on the vertex properties of the transformation
functions are met. Lastly, we would like to remind a potential limi-
tation of alignment regularization: it may not always help improve
the i.i.d accuracy due to the tradeoﬀ between accuracy and robust-
ness or invariance. In addition, to simplify the procedure of users
in leveraging our contribution, we also release a software pack-
age in both TensorFlow and PyTorch for users to use our identiﬁed
methods with a couple lines of code.

ACKNOWLEDGMENTS
This work was supported by NIH R01GM114311, NIH P30DA035778,
and NSF IIS1617583; NSF CAREER IIS-2150012 and IIS-2204808.
The authors would like to thank Hanru Yan for the implementa-
tion of the software package.

REFERENCES
[1] Yaser S Abu-Mostafa. 1990. Learning from hints in neural networks. Journal of

complexity 6, 2 (1990), 192–198.

[2] Alessandro Achille and Stefano Soatto. 2018.

Information dropout: Learning
optimal representations through noisy computation. IEEE transactions on pattern
analysis and machine intelligence 40, 12 (2018), 2897–2905.

[3] Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein GAN.

arXiv:1701.07875 [stat.ML]

[4] Akari Asai and Hannaneh Hajishirzi. 2020.

Logic-Guided Data Aug-
for Consistent Question Answering.

mentation
arXiv:2004.10157 [cs.CL]

and Regularization

[5] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh.
2020. Learning De-biased Representations with Biased Representations. 119
(2020), 528–539.

[6] Sergey Bobkov and Michel Ledoux. 2019. One-dimensional empirical measures,
order statistics, and Kantorovich transport distances. Vol. 261. American Mathe-
matical Society.

[7] Olivier Bousquet, Stéphane Boucheron, and Gábor Lugosi. 2003.

Introduction
to statistical learning theory. In Summer School on Machine Learning. Springer,
169–207.

[8] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov,
Mikhail Druzhinin, and Alexandr A. Kalinin. 2020. Albumentations: Fast and
Flexible Image Augmentations. Inf. 11, 2 (2020), 125.

[9] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. 2019. Rubi:
Reducing unimodal biases for visual question answering. In Advances in neural
information processing systems. 841–852.

[10] Shuxiao Chen, Edgar Dobriban, and Jane H Lee. 2019. A Group-Theoretic Frame-

work for Data Augmentation. arXiv:1907.10905 [stat.ML]

[11] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. 2019. Don’t Take the
Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases.
arXiv preprint arXiv:1909.03683 (2019).

[12] Taco Cohen and Max Welling. 2016. Group equivariant convolutional networks.

In International conference on machine learning. 2990–2999.

[13] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le.
2019. Autoaugment: Learning augmentation strategies from data. In Proceedings
of the IEEE conference on computer vision and pattern recognition. 113–123.

[14] Marco Cuturi and Arnaud Doucet. 2014.

Fast computation of Wasserstein

barycenters. (2014).

Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation

KDD ’22, August 14–18, 2022, Washington, DC, USA

[15] Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christo-
pher Ré. 2019. A Kernel Theory of Modern Data Augmentation. In Proceedings of
the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA (Proceedings of Machine Learning Research, Vol. 97),
Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 1528–1537.
[16] Alhussein Fawzi, Horst Samulowitz, Deepak S. Turaga, and Pascal Frossard. 2016.
Adaptive data augmentation for image classiﬁcation. In 2016 IEEE International
Conference on Image Processing, ICIP 2016, Phoenix, AZ, USA, September 25-28,
2016. IEEE, 3688–3692.

[17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. The Journal of Machine Learn-
ing Research 17, 1 (2016), 2096–2030.

[18] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A.
Wichmann, and Wieland Brendel. 2019. ImageNet-trained CNNs are biased to-
wards texture; increasing shape bias improves accuracy and robustness.. In In-
ternational Conference on Learning Representations.

[19] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk,
Quoc V. Le, and Barret Zoph. 2020. Simple Copy-Paste is a Strong Data Aug-
mentation Method for Instance Segmentation. CoRR abs/2012.07177 (2020).
arXiv:2012.07177

[20] Atin Ghosh and Alexandre H. Thiery. 2021. On Data-Augmentation and
Consistency-Based Semi-Supervised Learning. In International Conference on
Learning Representations.

[21] Ian Goodfellow. 2016. NIPS 2016 tutorial: Generative adversarial networks. arXiv

preprint arXiv:1701.00160 (2016).

[22] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and

harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).

[23] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin,
Improved Training of Wasserstein GANs.

and Aaron Courville. 2017.
arXiv:1704.00028 [cs.LG]

[24] Hao Guo, Kang Zheng, Xiaochuan Fan, Hongkai Yu, and Song Wang. 2019. Vi-
sual Attention Consistency Under Image Transforms for Multi-Label Image Clas-
siﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2019, Long Beach, CA, USA, June 16-20, 2019. Computer Vision Foundation / IEEE,
729–739.

[25] Dan Hendrycks and Thomas Dietterich. 2019. Benchmarking Neural Network
Robustness to Common Corruptions and Perturbations. Proceedings of the Inter-
national Conference on Learning Representations (2019).

[26] Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer,
and Balaji Lakshminarayanan. 2020. AugMix: A Simple Data Processing Method
to Improve Robustness and Uncertainty. In 8th International Conference on Learn-
ing Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net.

[27] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
2019. Natural adversarial examples. arXiv preprint arXiv:1907.07174 (2019).
[28] Alex Hernandez-Garcia. 2020. Data augmentation and image understanding.

arXiv preprint arXiv:2012.14185 (2020).

[29] Alex Hernández-García, Peter König, and Tim C Kietzmann. 2019. Learning ro-
bust visual representations using data augmentation invariance. arXiv preprint
arXiv:1906.04547 (2019).

[30] Alex Hernández-García and Peter König. 2018. Data augmentation instead of

explicit regularization. arXiv:1806.03852 [cs.CV]

[31] Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. 2019. Population
Based Augmentation: Eﬃcient Learning of Augmentation Policy Schedules. In
Proceedings of the 36th International Conference on Machine Learning, ICML 2019,
9-15 June 2019, Long Beach, California, USA (Proceedings of Machine Learning
Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR,
2731–2741.

[32] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. 2020.

Self-
challenging Improves Cross-Domain Generalization. In Computer Vision - ECCV
2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings,
Part II (Lecture Notes in Computer Science, Vol. 12347), Andrea Vedaldi, Horst
Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer, 124–140.
[33] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
Tran, and Aleksander Madry. 2019. Adversarial examples are not bugs, they are
features. In Advances in Neural Information Processing Systems. 125–136.
[34] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. 2015. Spatial trans-
former networks. In Advances in neural information processing systems. 2017–
2025.

[35] Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. 2019. Consistency-
based Semi-supervised Learning for Object detection. In Advances in Neural In-
formation Processing Systems. 10758–10767.

[36] Harini Kannan, Alexey Kurakin, and Ian Goodfellow. 2018. Adversarial Logit

Pairing. arXiv:1803.06373 [cs.LG]

[37] Risi Kondor, Zhen Lin, and Shubhendu Trivedi. 2018. Clebsch–gordan nets: a
fully fourier space spherical convolutional neural network. In Advances in Neu-
ral Information Processing Systems. 10117–10126.

[38] Ilya Kostrikov, Denis Yarats, and Rob Fergus. 2020.

Image Augmentation Is
All You Need: Regularizing Deep Reinforcement Learning from Pixels. CoRR
abs/2004.13649 (2020).

[39] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–
2324.

[40] Davis Liang, Zhiheng Huang, and Zachary C Lipton. 2018. Learning noise-
invariant representations for robust speech recognition. In 2018 IEEE Spoken
Language Technology Workshop (SLT). IEEE, 56–63.

[41] Percy Liang. 2016. CS229T/STAT231: Statistical Learning Theory (Winter 2016).
[42] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. 2019.
Fast AutoAugment. In Advances in Neural Information Processing Systems 32: An-
nual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, De-
cember 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle,
Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett
(Eds.). 6662–6672.

[43] Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin D Cubuk.
Improving robustness without sacriﬁcing accuracy with patch gaussian

2019.
augmentation. arXiv preprint arXiv:1906.02611 (2019).

[44] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net.

[45] Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler, and Tal Linzen.
2020. Syntactic Data Augmentation Increases Robustness to Inference Heuristics.
In ACL.

[46] Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papail-
iopoulos. 2019. Does Data Augmentation Lead to Positive Margin?. In Interna-
tional Conference on Machine Learning. 5321–5330.

[47] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. 2016. Regularization
with stochastic transformations and perturbations for deep semi-supervised
learning. In Advances in neural information processing systems. 1163–1171.
[48] Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. 2019. Cycle-
Consistency for Robust Visual Question Answering. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

[49] Connor Shorten and Taghi M Khoshgoftaar. 2019. A survey on image data aug-

mentation for deep learning. Journal of Big Data 6, 1 (2019), 60.

[50] Patrice Simard, Bernard Victorri, Yann LeCun, and John S Denker. 1991. Tangent
prop-a formalism for specifying selected invariances in an adaptive network. In
NIPS.

[51] Sining Sun, Ching-Feng Yeh, Mari Ostendorf, Mei-Yuh Hwang, and Lei Xie. 2018.
Training Augmentation with Adversarial Examples for Robust Speech Recogni-
tion. In ISCA.

[52] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Er-
han, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural net-
works. arXiv preprint arXiv:1312.6199 (2013).

[53] Kai Sheng Tai, Peter Bailis, and Gregory Valiant. 2019. Equivariant Transformer

Networks. arXiv preprint arXiv:1901.11399 (2019).

[54] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
Aleksander Madry. 2018. Robustness May Be at Odds with Accuracy. In Interna-
tional Conference on Learning Representations.

[55] Zhuozhuo Tu, Jingwei Zhang, and Dacheng Tao. 2019. Theoretical analysis of
adversarial learning: A minimax approach. In Advances in Neural Information
Processing Systems. 12259–12269.

[56] Cédric Villani. 2003. Topics in optimal transportation. Number 58. American

Mathematical Soc.

[57] Cédric Villani. 2008. Optimal transport: old and new. Vol. 338. Springer Science

& Business Media.

[58] Haohan Wang, Songwei Ge, Zachary C. Lipton, and Eric P. Xing. 2019. Learning
, 10506–

Robust Global Representations by Penalizing Local Predictive Power.
10518 pages.

[59] Haohan Wang, Zexue He, Zachary C. Lipton, and Eric P. Xing. 2019. Learning
Robust Representations by Projecting Superﬁcial Statistics Out. In 7th Interna-
tional Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019. OpenReview.net.

[60] Haohan Wang and Bhiksha Raj. 2017. On the origin of deep learning. arXiv

preprint arXiv:1702.07800 (2017).

[61] Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P. Xing. 2020. High Fre-
quency Component Helps Explain the Generalization of Convolutional Neural
Networks. In Computer Vision and Pattern Recognition (CVPR).

[62] Xindi Wu, Yijun Mao, Haohan Wang, Xiangrui Zeng, Xin Gao, Eric P. Xing, and
Min Xu. 2019. Regularized Adversarial Training (RAT) for Robust Cellular Elec-
tron Cryo Tomograms Classiﬁcation. In 2019 IEEE International Conference on
Bioinformatics and Biomedicine, BIBM 2019, San Diego, CA, USA, November 18-21,
2019, Illhoi Yoo, Jinbo Bi, and Xiaohua Hu (Eds.). IEEE, 1–6.

[63] Saining Xie, Tianbao Yang, Xiaoyu Wang, and Yuanqing Lin. 2015. Hyper-class
augmented and regularized deep learning for ﬁne-grained image classiﬁcation.

KDD ’22, August 14–18, 2022, Washington, DC, USA

Wang, et al.

In Proceedings of the IEEE conference on computer vision and pattern recognition.
2645–2654.

[64] Fanny Yang, Zuowen Wang, and Christina Heinze-Deml. 2019.

Invariance-
inducing regularization using worst-case transformations suﬃces to boost ac-
curacy and spatial robustness. In Advances in Neural Information Processing Sys-
tems. 14757–14768.

[65] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and
Michael I. Jordan. 2019. Theoretically Principled Trade-oﬀ between Robustness
and Accuracy. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA (Proceedings of
Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhut-
dinov (Eds.). PMLR, 7472–7482.

[66] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou.
2021. How Does Mixup Help With Robustness and Generalization?. In Interna-
tional Conference on Learning Representations.

[67] Richard Zhang. 2019. Making convolutional networks shift-invariant again.

arXiv preprint arXiv:1904.11486 (2019).

[68] Zhirui Zhang, Shuangzhi Wu, Shujie Liu, Mu Li, Ming Zhou, and Tong Xu. 2019.
Regularizing neural machine translation by target-bidirectional agreement. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, Vol. 33. 443–450.

[69] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. 2016. Improving
the robustness of deep neural networks via stability training. In Proceedings of
the ieee conference on computer vision and pattern recognition. 4480–4488.
[70] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. 2020. Random

Erasing Data Augmentation. In AAAI.

Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation

KDD ’22, August 14–18, 2022, Washington, DC, USA

APPENDICES
A PROOF OF THEORETICAL RESULTS
A.1 Lemma A.1 and its proof

A.3 Proof of Theorem 5.1

Theorem. With Assumptions A1, A2, A4, A5, and A6, with prob-

ability at least 1 − 𝛿, we have

Lemma A.1. With Assumptions A1, A4, and A5, with probability

𝑟P,A (

𝜃) ≤

𝑟P (

𝜃 ) +

at least 1 − 𝛿, we have

b

b

b

||𝑓 (x𝑖;

𝜃 ) − 𝑓 (x′
𝑖 ;

𝜃 )||1 + 𝜙 (|Θ|, 𝑛, 𝛿)

(7)

b

b

Õ𝑖

I(𝑔(𝑓 (𝑎(x);

𝜃 )) ≠ y) + 𝜙 (|Θ|, 𝑛, 𝛿)

and x′ = 𝑎(x), where 𝑎 = arg min𝑎 ∈A y⊤ 𝑓 (𝑎(x);

𝜃 ).

𝑟P,A (

𝜃 ) ≤

1
𝑛 Õ(x,y)∼P

sup
𝑎 ∈A

b

b
Proof. With Assumption A5, we have

arg max
P′ ∈𝑇 (P,A)

𝑟P′ (

𝜃 ) = arg max
P′ ∈𝑇 (P,A)

𝑟P′ (

𝜃 ) = P𝑤

b

b

b

we can analyze the expected risk following the standard classical
techniques since both expected risk and empirical risk are studied
over distribution P𝑤.

Now we only need to make sure the classical analyses (as dis-

cussed in A4) are still valid over distribution P𝑤:
• when A4 is “Θ is ﬁnite, 𝑙 (·, ·) is a zero-one loss, samples are i.i.d”,

log(|Θ|) + log(1/𝛿)
2𝑛

𝜙 (|Θ|, 𝑛, 𝛿) =
uses Hoeﬀding’s inequality, which only requires independence
of random variables. One can refer to Section 3.6 in [41] for the
detailed proof.

. The proof of this result

r

• when A4 is “samples are i.i.d”, 𝜙 (|Θ|, 𝑛, 𝛿) = 2R (L) +

r

The proof of this result relies on McDiarmid’s inequality, which
also only requires independence of random variables. One can
refer to Section 3.8 in [41] for the detailed proof.

log 1/𝛿
2𝑛

Assumption A1 guarantees the samples from distribution P𝑤 are
still independent, thus the generic term holds for at least these two
concrete examples, thus the claim is proved.

(cid:3)

A.2 Proposition A.2 and Proof

Proposition A.2. With A2, for any 𝑎 ∈ A, we have

| (X,Y) |

𝑊1 (

Q

,

x,
𝜃

Q

𝑎 (x),
𝜃

) =

Õ𝑖

||𝑓 (x𝑖;

𝜃 ) − 𝑓 (𝑎(x𝑖);

𝜃 )||1,

b

b

b
denotes the empirical distribution of 𝑓 (x;

b

b

b

𝜃 ) for (x, y) ∈

b

where
(X, Y).

Q

b

x,
𝜃

b

Proof. We use the order statistics representation of Wasser-
stein metric over empirical distributions (e.g., see Section 4 in [6])

| (X,Y) |

Õ𝑖

𝑊1 (

Q

,

x,
𝜃

Q

𝑎 (x),
𝜃

)) = inf
𝜎

b

b

b

b

||𝑓 (x𝑖;

𝜃 ) − 𝑓 (𝑎(x𝜎 (𝑖)),

𝜃 )||1

b

b

where 𝜎 stands for a permutation of the index, thus the inﬁmum is
taken over all possible permutations. With Assumption A2, when
𝑑𝑒 (·, ·) in A2 chosen to be ℓ1 norm, we have:

||𝑓 (x𝑖 ;

𝜃 ) − 𝑓 (𝑎(x𝑖);

𝜃 )||1 ≤ min
𝑗≠𝑖

||𝑓 (x𝑖;

𝜃 ) − 𝑓 (𝑎(x𝑗 ),

𝜃 )||1

b

b

Thus, the inﬁmum is taken when 𝜎 is the natural order of the sam-
(cid:3)
ples, which leads to the claim.

b

b

.

With A6, we can continue with:

Proof. First of all, in the context of multiclass classiﬁcation,
where 𝑔(𝑓 (x, ; 𝜃)) predicts a label with one-hot representation, and
y is also represented with one-hot representation, we can have the
empirical risk written as:

b

𝑟P (

𝜃) = 1 −

b

b

1
𝑛 Õ(x,y)∼P

y⊤𝑔(𝑓 (x;

𝜃 ))

b

Thus,

sup
P′ ∈𝑇 (P,A)

𝑟P′ (

𝜃 ) =

𝑟P (

𝜃 ) +

b

b

b
=
𝑟P (

b
𝜃 ) +

sup
P′ ∈𝑇 (P,A)
1
𝑛

sup

𝑟P′ (

𝜃 ) −

𝑟P (

𝜃)

b

b

b

b
y⊤𝑔(𝑓 (x;

𝜃 ))

b

b
−

Õ(x,y)∼P′

P′ ∈𝑇 (P,A) (cid:0) Õ(x,y)∼P
y⊤𝑔(𝑓 (x;

𝜃 ))

(cid:1)

b

b

sup
P′ ∈𝑇 (P,A)

1
𝑛

𝑟P′ (

𝜃 ) ≤

𝑟P (

𝜃) +

b

b

b

b
−

sup

P′ ∈𝑇 (P,A) (cid:0) Õ(x,y)∼P
y⊤ log(𝑓 (x;

𝜃 ))

y⊤ log(𝑓 (x;

𝜃 ))

b

Õ(x,y)∼P′

(cid:1)

b

If we use 𝑒 (·) = −y⊤ log(·) to replace the cross-entropy loss, we
simply have:

sup
P′ ∈𝑇 (P,A)

1
𝑛

𝑟P′ (

𝜃 ) ≤

𝑟P (

𝜃) +

b

b

b

b
−

Õ(x,y)∼P

sup

𝑒 (𝑓 (x;

𝜃 ))

P′ ∈𝑇 (P,A) (cid:0) Õ(x,y)∼P′
𝜃 ))
𝑒 ((𝑓 (x;

b

(cid:1)

b

Since 𝑒 (·) is a Lipschitz function with constant ≤ 1 (because of A6,
Eq.(6)) and together with the dual representation of Wasserstein
metric (See e.g., [56]), we have

sup
P′ ∈𝑇 (P,A)

𝑟P′ (

𝜃 ) ≤

𝑟P (

𝜃 ) + 𝑊1 (

Q

,

x,
𝜃

Q

𝑎 (x),
𝜃

))

b

b

b

b

b
where x′ = 𝑎(x), where 𝑎 = arg min𝑎 ∈A y⊤ 𝑓 (𝑎(x);
notes the empirical distribution of 𝑓 (𝑎(x);
Note that 𝑟P,A (

b
𝜃), by deﬁnition, is a shorthand notation for

b

𝜃 ) for (x, y) ∈ (X, Y).
b

b

𝜃 );

Q

x,
𝜃

de-

b

b

b

b

sup
P′ ∈𝑇 (P,A)

𝑟P′ (

𝜃 )

b

.

Further, we can use the help of Proposition B.2 to replace Wassertein

metric with ℓ1 distance. Finally, we can conclude the proof with As-
(cid:3)
sumption A5 as how we did in the proof of Lemma B.1.

KDD ’22, August 14–18, 2022, Washington, DC, USA

Wang, et al.

Clean

23.9
23.6
22.4
24.5
22.8
23
27.2
22.4
25.2

Gauss
79
78
61
67
69
73
69
65
61

B
RVA
RWA
SU
AA
MBP
SIN
AM
AMS

Noise
Shot
80
78
63
68
68
74
70
66
62

Impulse
82
79
63
70
72
76
70
67
61

Blur

Weather

Digital

Defocus
82
74
68
74
77
74
77
70
69

Glass
90
87
75
83
83
86
84
80
77

Motion
84
79
65
81
80
78
76
66
63

Zoom
80
76
66
77
81
77
82
66
72

Snow
86
78
70
80
79
77
74
75
66

Frost
81
75
69
74
75
72
75
72
68

Fog
75
69
64
75
64
63
69
67
63

Bright
65
58
56
62
56
56
65
58
59

Contrast
79
68
55
77
70
68
69
58
52

Elastic
91
85
70
84
88
86
80
79
74

Pixel
77
75
61
71
57
71
64
69
60

JPEG
80
75
63
71
71
71
77
69
67

mCE

80.6
75.6
64.6
74.3
72.7
73.4
73.3
68.4
64.9

Table 6: Comparison to advanced models over ImageNet-C data. Performance reported (mCE) follows the standard in
ImageNet-C data: mCE is the smaller the better.

B
0.1204
0.2408

InfoDrop
0.1224
0.256

HEX
0.1292
0.2564

PAR
0.1306
0.2627

VA
0.1362
0.2715

RVA
0.1405
0.2793

RSC
0.1612
0.3078

VWA
0.1432
0.2846

RWA
0.1486
0.2933

Top-1
Top-5

Table 7: Comparison to advanced cross-domain image classiﬁcation models, over ImageNet-Sketch dataset. We report top-1
and top-5 accuracy following standards on ImageNet related experiments.

A.4 Proof of Lemma 5.2
Lemma. With Assumptions A1-A6, assuming there is a 𝑎′() ∈ A
, with probability at least 1 −𝛿,
where
we have:
b
𝜃 ) ≤

𝑟P𝑎+ (
(cid:0)
b
𝜃 )

𝜃 ) = 1
2

𝑟P𝑎− (

𝑟P𝑎′ (

𝜃) +

(8)

𝜃 )

b

b

b

(cid:1)

𝑟P,A (

b
1
2
+

𝑟P𝑎+ (
(cid:0)
𝑟P𝑎− (
b

b
𝜃 )

(cid:1)

b

b

b

+

Õ𝑖

||𝑓 (𝑎+(x𝑖 );

𝜃 ) − 𝑓 (𝑎−(x′);

𝜃 )||1 + 𝜙 (|Θ|, 𝑛, 𝛿)

b

b

(9)

Proof. We can continue with

sup
P′ ∈𝑇 (P,A)

𝑟P′ (

𝜃 ) ≤

𝑟P (

𝜃 ) + 𝑊1 (

Q

,

x,
𝜃

Q

𝑎 (x),
𝜃

)),

b

b

Q

b
denotes the empirical distribution of 𝑓 (𝑎(x);
where
(X, Y). from the proof of Theorem 5.2. With the help of Assump-
tion A3, we have:

x,
𝜃

b

b

b

b

b

𝜃 ) for (x, y) ∈

b

b

b

𝑑𝑥 (𝑓 (𝑎+ (x),

𝜃 ), 𝑓 (𝑎−(x),

𝜃 )) ≥ 𝑑𝑥 (𝑓 (x,

𝜃 ), 𝑓 (x′,

𝜃 ))

When 𝑑𝑥 (·, ·) is chosen as Wasserstein-1 metric, we have:
b

b

b

b

sup
P′ ∈𝑇 (P,A)

𝑟P′ (

𝜃) ≤

𝑟P (

𝜃 ) + 𝑊1 (

Q

𝑎+ (x),
𝜃

,

Q

𝑎− (x),
𝜃

))

b

b

b

b

b

b

b

b

,

Q

Q

𝑎+ (x),
𝜃

Further, as the LHS is the robust risk generated by the transfor-
𝜃) is independent of the term
mation functions within A, and
𝑟P (
𝑊1 (
𝜃 ) with the risk
)), WLOG, we can replace
b
of an arbitrary distribution generated by the transformation func-
tion in A. If we choose to use
,
we can conclude the proof with help from Proposition B.2 and As-
(cid:1)
b
(cid:3)
sumption A5 as how we did in the proof of Theorem 5.2.

b
b
𝜃 ) +
𝑟P𝑎+ (
(cid:0)
b

𝜃) = 1
2

𝑟P𝑎− (

𝑎− (x),
𝜃

𝑟P𝑎′ (

𝑟P (

𝜃 )

b

b

b

b

b

b

b

b

b

B ADDITIONAL RESULTS FOR

COMPARISONS WITH ADVANCED
METHODS

We have also conducted two full ImageNet level experiments. How-
ever, due to the limitation of resources, we cannot tune the models
substantially. Our current trial suggest that our techniques can im-
prove the vanilla model to compete with SOTA models, limited by

our resources, we cannot do wide-range hyperparameters search
to outperform them. Also, considering the fact that many of these
methods are signiﬁcantly more complicated than us and also uses
data augmentation specially designed for the tasks, we consider
our experiments a success indication of the empirical strength of
our methods.

Texture-perturbed ImageNet classiﬁcation. We also test the per-
formance on the image classiﬁcation over multiple perturbations.
We train the model over standard ImageNet training set and test
the model with ImageNet-C data [25], which is a perturbed ver-
sion of ImageNet by corrupting the original ImageNet validation
set with a collection of noises. Following the standard, the reported
performance is mCE, which is the smaller the better. We compare
with several methods tested on this dataset, including Patch Uni-
form (PU) [43], AutoAugment (AA) [13], MaxBlur pool (MBP) [67],
Stylized ImageNet (SIN) [25], AugMix (AM) [26], AugMix w. SIN
(AMS) [26]. We use the performance reported in [26]. Again, our
augmention only uses the generic texture with perturbation (the
A in our texture synthetic experiments with radius changed to
20, 25, 30, 35, 40). The results are reported in Table 6, which shows
that our generic method outperform the current SOTA methods
after a continued ﬁnetuning process with reducing learning rates.

Cross-domain ImageNet-Sketch Classiﬁcation. We also compare
to the methods used for cross-domain evaluation. We follow the
set-up advocated by [59] for domain-agnostic cross-domain pre-
diction, which is training the model on one or multiple domains
without domain identiﬁers and test the model on an unseen do-
main. We use the most challenging setup in this scenario: train the
models with standard ImageNet training data, and test the model
over ImageNet-Sketch data [58], which is a collection of sketches
following the structure ImageNet validation set. We compare with
previous methods with reported performance on this dataset, such
as InfoDrop [2], HEX [59], PAR [58], RSC [32] and report the perfor-
mances in Table 7. Notice that, our data augmentation also follows
the requirement that the characteristics of the test domain cannot
be utilized during training. Thus, we only augment the samples

Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation

KDD ’22, August 14–18, 2022, Washington, DC, USA

with a generic augmentation set (A of “contrast” in synthetic ex-
periments). The results again support the usage of data augmenta-
tion and alignment regularization.

