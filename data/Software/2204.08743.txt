ON THE USE OF CAUSAL GRAPHICAL MODELS FOR DESIGNING
EXPERIMENTS IN THE AUTOMOTIVE DOMAIN

2
2
0
2

r
p
A
5
2

]
E
S
.
s
c
[

3
v
3
4
7
8
0
.
4
0
2
2
:
v
i
X
r
a

David Issa Mattos
Volvo Cars
Gothenburg, Sweden
david.mattos@volvocars.com

Yuchu Liu
Volvo Cars
Gothenburg, Sweden
yuchu.liy@volvocars.com

April 26, 2022

ABSTRACT

Randomized ﬁeld experiments are the gold standard for evaluating the impact of software changes on
customers. In the online domain, randomization has been the main tool to ensure exchangeability.
However, due to the different deployment conditions and the high dependence on the surrounding
environment, designing experiments for automotive software needs to consider a higher number of
restricted variables to ensure conditional exchangeability. In this paper, we show how at Volvo Cars
we utilize causal graphical models to design experiments and explicitly communicate the assumptions
of experiments. These graphical models are used to further assess the experiment validity, compute
direct and indirect causal effects, and reason on the transportability of the causal conclusions.

Keywords Experimentation, Causality, Causal Graphical Models, DAG

1

Introduction

Randomized ﬁeld experiments, such as A/B testing, have been extensively used by online companies to assess and
validate product change ideas [Fabijan et al., 2017]. In the simplest case, users are randomized between two groups: the
control (the existing software system), and the treatment (the software system with the desired change) groups.

The randomization process is a simple and reliable way to allow the control and treatment groups to be exchangeable
and to estimate the (unbiased) causal effect of the software change. However, in several practical applications fully
randomized experiments are not desirable or even possible to be conducted. In this context, different tools can be used
to estimate the causal effect of software changes, such as quasi-experiments, matching, instrumental variables, etc [Liu
et al., 2021a,b, Xu and Chen, 2016].

In the automotive domain, several conditions prohibit the use of full randomization experiments in most cases, such as
the high degree of interaction of the cyber-physical system with the environment, the deployment limitations, and the
limited sample size. Therefore, experiments designed for automotive software development need to be restricted to
several confounders that can potentially inﬂuence the desired outcome metric.

To address these limitations at Volvo Cars, we utilize causal graphical models [Glymour et al., 2016], to help design
experiments and make the assumptions taken explicit for all. Moreover, these causal models can be used to assess
the experiment validity, compute potential direct and indirect effects, and reason about the transportability of the
experimental results for other populations.

2 Background

Assessing causality in online experiments is traditionally conducted in the Rubin-Neyman potential outcomes framework
[Holland, 1986]. This framework assesses the causal effect by using counterfactual, what would be the impact of a
treatment in a population if it had not been exposed to the treatment. To achieve that, some conditions need to be

 
 
 
 
 
 
MATTOS AND LIU

fulﬁlled such as positivity (there are samples for both the treatment and the control), exchangeability (there is an
independence between the counterfactual outcome and the observed treatment) and consistency (the treatment is the
same and well-deﬁned) [Hernán and Robins, 2010]. While in randomized ﬁeld experiments positivity and consistency
are fulﬁlled by design, proper randomization is used to achieve exchangeability.

When multiple variables need to be restricted or they cannot be randomized, it is necessary to control for them in the
design to ensure conditional exchangeability holds, which means all backdoor paths are blocked in a causal Directed
Acyclic Graph (DAG) [Hernán and Robins, 2010, Glymour et al., 2016]. For this reason, we utilize graphical causal
models based on DAG to aid the design of experiments in automotive software engineering. Several books are dedicated
to review of graphical causal models, its relation to the potential outcomes framework, and its applications in different
areas of science [Hernán and Robins, 2010, Glymour et al., 2016].

3 Using causal graphical models

Experiments conducted in automotive software development have many conditions that need to be restricted or cannot
be randomized. While we have explored some of them in our previous work [Liu et al., 2021b,a, Xu and Chen, 2016],
in this paper, we utilize graphical causal models to improve the design and communication of experiments.

Our general process for using graphical causal models consists of:

1. Value mapping: identiﬁcation of the different aspects (which we call values) the change is expected to impact,

such as overall evaluation criteria, guardrails, confounders etc.

2. Causal mapping: utilizing the subsystems domain knowledge, we create a graphical causal model that maps
how the change impacts the different systems and subsystems related to the mapped values. In this step, we
differentiate which variables are the main metrics, guardrails, intervention, latent/ non-observable, and other
measurable metrics.

3. Causal mapping validation: The causal map is validated in an iterative two-stage process. We ﬁrst review
the causal map with a group of experts followed by a consistency check with existing collected data (e.g.,
checking if the conditional independence that the graph includes holds). These two steps are iterated until we
reach a consensus on the quality of the causal map. In other words, we combine the knowledge-driven and
data-driven causal discovery processes in our practice.

4. Experimental design and validity: based on the causal map we can start restricting variables that cannot be
randomized. Utilizing algorithms for identifying conditional exchangeability in a DAG. [Glymour et al., 2016,
Hernán and Robins, 2010], we can verify which variables are required to be controlled in the experiment
design. When designing the experiment and determining how the treatment assignment process will occur,
such as a combination of randomization and restricted variables, additional conditional independence relations
will arise that help verify the validity of the design. These conditional independence relations are an extra
check similar to the results of an A/A test and different sample-ratio-mismatch criteria (which are also derived
automatically from the DAG).

5. Analysis: after the experiment, data is collected. We query the DAG to guide the analysis, for instance, we
might be interested in separating the direct and the indirect effects, as opposed to the total effect obtained in
the experiment, as well as evaluating causal transportability questions [Hernán and Robins, 2010].

We provide below an illustrative example of simpliﬁed case conducted at Volvo Cars in Figure 1. In this example, a new
software modiﬁcation on the climate system was aimed at reducing energy consumption (the direct effect). However,
the new software could potentially affect how users interact with the climate system and generate a potential indirect
effect of increasing energy consumption. The causal diagram also contains latent and non-measurable variables.

Using this causal graph, we could ﬁnd the necessary adjustment sets of confounding factors (controlling for the vehicle
variant) required to identify the unbiased total causal effect of the software change that is the result of the A/B test.
For example, if we want to identify the direct effect, we need to adjust for the city, temperature, and vehicle variant.
Assuming linearity, the indirect effect, the potential degrading effect of the climate software, can be calculated by
subtracting the direct effect from the total effect.

We can control for the adjustment sets by identifying the conditional causal effect by strata, adjusting it through
inverse probability weighting, or if assuming linearity of the causal effects we can add the variable in the linear
model. The following link contains a short appendix on how the analysis of this example was conducted: https:
//davidissamattos.github.io/ease-2022-causal/.

2

MATTOS AND LIU

Figure 1: A simpliﬁed causal graphical model.

The graphical model from Figure 1 is used internally to communicate the experiment assumptions between different
teams which increases transparency of the experiment design and analysis decisions. The use of graphical models
serves as a boundary object to facilitate the communication interface between several teams [Heyn and Knauss, 2022,
Wohlrab et al., 2019].

4 Conclusion

Causal models are a powerful tool to assess causality in any application. It is general enough to encompass and leverage
experiments, quasi-experiments, and observational studies in a single consistent framework. The main disadvantage
of such a framework is the need to construct a correct causal graphical model and the real causal structure might be
hard or impossible to obtain in certain cases. However, by combining tools for automatic causal discovery from data
with domain knowledge, we believe we can provide a meaningful and actionable causal graphical model for most
applications.

References

Aleksander Fabijan, Pavel Dmitriev, Helena Hölmstrom Olsson, and Jan Bosch. The evolution of continuous experi-
mentation in software product development: From data to a data-driven organization at scale. In 2017 IEEE/ACM
39th International Conference on Software Engineering (ICSE). IEEE, may 2017.

Yuchu Liu, David Issa Mattos, Jan Bosch, Helena Hölmstrom Olsson, and Jonn Lantz. Size matters? or not: A/b testing
with limited sample in automotive embedded software. In 2021 47th Euromicro Conference on Software Engineering
and Advanced Applications (SEAA). IEEE, sep 2021a.

Yuchu Liu, David Issa Mattos, Jan Bosch, Helena Hölmstrom Olsson, and Jonn Lantz. Bayesian propensity score
matching in automotive embedded software engineering. In 2021 28th Asia-Paciﬁc Software Engineering Conference
(APSEC). IEEE, dec 2021b.

Ya Xu and Nanyu Chen. Evaluating mobile apps with a/b and quasi a/b tests. In Proceedings of the 22nd ACM SIGKDD

International Conference on Knowledge Discovery and Data Mining, pages 313–322, 2016.

3

EnergyClimate perform.Climate SWTemp. settingBase climate preferen.Main metricGuardrailsInterventionLatent/non- measurableRandom assignm.Restriction to subgroupsVehicle variantCity and weatherMeasurable variableMATTOS AND LIU

Madelyn Glymour, Judea Pearl, and Nicholas P Jewell. Causal inference in statistics: A primer. John Wiley & Sons,

2016.

Paul W. Holland. Statistics and causal inference. 81(396):945–960, dec 1986.
Miguel A Hernán and James M Robins. Causal inference. CRC Boca Raton, 2010.
Hans-Martin Heyn and Eric Knauss. Structural causal models as boundary objects in ai system development. In 1st

International Conference on AI Engineering - Software Engineering for AI, 2022.

Rebekka Wohlrab, Patrizio Pelliccione, Eric Knauss, and Mats Larsson. Boundary objects and their use in agile systems

engineering. Journal of Software: Evolution and Process, 31(5):e2166, 2019.

4

