A Probabilistic Framework for Mutation Testing in Deep Neural
Networks

Florian Tambon1,∗, Foutse Khomh1, Giuliano Antoniol1

aDepartment of Software Engineering - Polytechnique Montreal, 2500, chemin de Polytechnique, H3T1J4, Canada

2
2
0
2

g
u
A
1
1

]
E
S
.
s
c
[

1
v
8
1
0
6
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

Context: Mutation Testing (MT) is an important tool in traditional Software Engineering (SE)
white-box testing. It aims to artiﬁcially inject faults in a system to evaluate a test suite’s capability
to detect them, assuming that the test suite defects ﬁnding capability will then translate to real
faults. If MT has long been used in SE, it is only recently that it started gaining the attention of
the Deep Learning (DL) community, with researchers adapting it to improve the testability of DL
models and improve the trustworthiness of DL systems.

Objective: If several techniques have been proposed for MT, most of them neglected the stochas-
ticity inherent to DL resulting from the training phase. Even the latest MT approaches in DL,
which propose to tackle MT through a statistical approach, might give inconsistent results. In-
deed, as their statistic is based on a ﬁxed set of sampled training instances, it can lead to diﬀerent
results across instances set when results should be consistent for any instance.

Methods: In this work, we propose a Probabilistic Mutation Testing (PMT) approach that alle-
viates the inconsistency problem and allows for a more consistent decision on whether a mutant is
killed or not.

Results: We show that PMT eﬀectively allows a more consistent and informed decision on
mutations through evaluation using three models and eight mutation operators used in previously
proposed MT methods. We also analyze the trade-oﬀ between the approximation error and the
cost of our method, showing that relatively small error can be achieved for a manageable cost.

Conclusion: Our results showed the limitation of current MT practices in DNN and the need
to rethink them. We believe PMT is the ﬁrst step in that direction which eﬀectively removes the
lack of consistency across test executions of previous methods caused by the stochasticity of DNN
training.

Keywords:
Deep learning, Mutation Testing, Bayesian Probability

1. Introduction

∗Corresponding author
Email address:

florian-2.tambon@polymtl.ca (Florian Tambon)

Artiﬁcial Intelligence (AI) and Machine
Learning (ML) are gaining traction with
countless applications, Deep Neural Networks

Preprint submitted to Information and Software Technology

August 15, 2022

 
 
 
 
 
 
(DNN) being one of the most prominent com-
ponents. DNN provides unprecedented capa-
bility, tackling complex classiﬁcation and re-
gression tasks, especially in computer vision.
Nonetheless, they also pose new veriﬁcation
and validation challenges [1]. DNN behav-
ior is dictated by its internal logic, a logic not
coded by a human, but “learned” from data.
In traditional software development,

test-
ing is an essential set of activities aiming to
identify defects and verify/validate that a sys-
tem meets speciﬁc requirements [2]. How-
ever, despite the eﬀort to adapt traditional soft-
ware testing techniques [3] to the new DNN
paradigm [4], to the best of the authors’ knowl-
edge, there is no convincing proof of real ef-
fectiveness in improving DNN dependability.
Indeed, the stochastic nature of DNN chal-
lenges traditional software testing approaches.
Mutation Testing (MT) [5] is a proven tech-
nique in Software Engineering (SE); it is the
de facto standard to compare diﬀerent testing
criteria [6, 7] or to evaluate the quality of a
test set [7]. MT’s basic assumption is that if
a program P and its mutated version M, ob-
tained by introducing a small artiﬁcial change
to P, diﬀer on an input x (i.e., P(x) (cid:44) M(x))
then the mutant M is killed, that is a defect
was detected. This allows establishing the per-
formance of a test suite, assuming it will then
transfer to real faults as well as comparing dif-
ferent testing criteria.

MT appealing idea has been initially ap-
plied to DNN to assess test data eﬀectiveness
and detect mutated DNN by works such as
[8, 9, 10]. However, Jahangirova et al. [11], ar-
gued that the DNN stochastic nature imposes
an MT reformulation. Given a DNN trained
instance Ni and its mutant M j, it is hard to as-
sess whether for a given input x, Ni(x) (cid:44) M j(x)
is caused by the input discovering the mu-
tant or simply a result of the stochastic train-
ing process. It is well known that for a given

2

model, architecture, hyper-parameters, train,
and test sets, two trained instances N1 and N2
will exhibit diﬀerent results on a set of inputs.
To overcome this limitation Jahangirova et al.
[11] proposed to adopt a statistical testing pro-
cedure where n trained instances of a DNN
{N1, ..., Nn} are compared against n trained in-
stances of a mutated DNN {M1, ..., Mn} over
their accuracy on the test set using a statisti-
cal test. The decision is no longer based on a
single instance but rather on the distribution of
instances. In summary, instead of a point-wise
decision, the new criteria are based on the dis-
tribution of tests results and quantify the eﬀec-
tiveness of the test set to kill mutants on any
instances of the DNN. A tool and a replication
package have been made available, including
a set of real-faults-based mutation operators,
DeepCrime [12].

We concur that MT needs to be adapted and
the decision should not be based on a single
instance, however, we also argue that the ap-
proach such as in DeepCrime [12] is limited.
In fact, when comparing n healthy (i.e., non-
mutated) DNN instances and n mutated DNN
instances, the decision (whether or not the test
set kill the mutation) depends on the given set
of DNN instances (both the n healthy and n
mutated). This is to say, if we keep every-
thing constant but we change the instance sets,
the decision may change when it should not.
Worse, it can even be the case that, by chance,
such an approach may declare a DNN mu-
tated when comparing against itself, through
the choice of healthy instances, which raises
an interesting problem as we are not able to
recognize the entity identity. In a nutshell, we
argue that current existing MT frameworks in
the context of DNN, due to the inherent ran-
domness of the paradigm, resembles a sort of
ﬂaky test [13]; meaning that diﬀerent mutation
test results may be returned upon a new test
run for the same test set.

In this paper we propose a Probabilistic MT
(PMT) framework, adapting MT to DNN in
the context of Bayesian estimation. PMT ex-
ploits Bayesian estimation to deﬁne a proba-
bilistic decision criterion to identify mutated
models.

We evaluate our PMT framework using
three models/datasets and eight mutation op-
erators and show how our proposed approach
can alleviate the ﬂakiness issue. The goal of
this evaluation is to provide evidence that pre-
vious MT methods iterations have some con-
sistency issues across multiple test executions
and show how it can be tackled using PMT.
We also investigate the trade-oﬀ between the
approximation error and the number of train-
ing instances (the cost) required for computa-
tion of PMT, by repeating experiments multi-
ple times with diﬀerent sampled populations
of diﬀerent sizes.

This paper makes the following contribu-

tions:

• A new probabilistic framework for MT
which accounts for the stochasticity of
DNN, with a replication package [14] that
can be easily adapted to any new model-
s/mutations/datasets.

• An analysis of the mutation operators
with PMT and a comparison to simple
MT.

• An empirical analysis of the trade-oﬀ be-
tween the number of instances required
for the testing and the approximation er-
ror.

The rest of the paper is organized as fol-
lows: We ﬁrst present a concrete motivating
example to illustrate the ﬂakiness issue occur-
ring in current DNN MT frameworks in Sec-
tion 2. We then deﬁne the problem tackled by
our approach in Section 3. In Section 4, we

3

introduce our probabilistic framework, a po-
tential decision function leveraging the proba-
bilistic approach, as well as a way of estimat-
ing the error caused by the limited number of
samples. In Section 5, we elaborate upon our
experiments and results. Section 6 discusses
threats to the validity of our work. Related
works are described in Section 7. Finally, Sec-
tion 8 concludes the paper and discusses some
future works.

2. Motivating Example

2.1. Experiment

To understand the need for PMT let us repli-
cate the MT process. We followed the ap-
proach proposed in DeepCrime on one model
and mutation operator, as they provided a com-
prehensive replication package and their tool
constitutes one of the latest iterations of MT
to date. Bear in mind, that we exactly repli-
cated DeepCrime process as provided in their
replication package. We chose the model us-
ing MNIST [15], with the same DeepCrime ar-
chitecture and hyper-parameters, as well as the
delete training data mutation operator (which
removes a percentage of the data proportion-
ally for each class). The choice of model/op-
erator does not matter, as similar behavior oc-
curs for any model/operator we tested on (see
Section 5).

First, we built and trained multiple sets of
200 model instances. A ﬁrst set is the set
of healthy instances (i.e., non-mutated); we
then produce ﬁve diﬀerent sets of instances ap-
plying the mutation delete training data with
magnitudes ranging from 3.12 to 30.93, mag-
nitudes being used by DeepCrime. At the
end of the process, we obtain 1200 model in-
stances. Finally, let us perform six experi-
ments applying exactly DeepCrime statistical
test (see Equation 1) to assess whether a mu-

tant is killed or not, using the same test set in
all cases.

A description of the example can be found
in Figure 1. In the ﬁrst experiment, we divided
“healthy” instances into two disjoint sets of
100. We pretended one of the two “healthy”
set contains “unknown” instances. We then
randomly sampled k instances from the 100
“healthy” subset and k out of the “unknown”
set and compared them, where k = 20 sim-
ilarly to DeepCrime’s method. We repeated
the sampling 100 times. We then averaged the
number of times each “unknown” sample was
declared “mutant” according to the statistical
test used in DeepCrime, which gives us an es-
timation of the probability that a given “un-
known” sample will be declared “mutant”. To
avoid potential sampling eﬀect when choosing
the initial two partitions of “healthy” and“un-
known”, we repeat the entire process 50 times.
For all other experiments, we did the same as
above, sampling sets from the 100 “healthy”
instances but this time contrasting them with
sets obtained by sampling real “mutant” in-
stances (separating experiments for each pa-
rameter magnitude). For the rest, we applied
the same procedure as in the ﬁrst experiment.

2.2. Results

Results of the procedure can be found in Ta-
ble 1. Remember, if the mutation test is sta-
ble and not prone to the ﬂakiness issue we de-
scribed, we should have an averaged killing
probability of 0 or 1 (within a small epsilon),
that is the result of the mutation test is not re-
liant on the instances (both “healthy” and “mu-
tated”) used, and it either always return that the
mutation is killed or always that it is not.

If for the mutated instances with 30.93% of
train data removed it is indeed the case, it is not
so for all other mutation magnitudes with the
probability ranging from 0.13 to 0.85. Worse,
the “healthy” instances, if we were to use them

Figure 1: Replication of DeepCrime’s mutation test
with diﬀerent
“Unknown” means either
“Healthy” or “Mutant”.

instances.

Table 1: Average Probability of declaring “unknown”
instances mutant after applying the experiments de-
scribed in Figure 1 for both “healthy” (I) and “mutated”
(with “delete training data” mutation operator and dif-
ferent removal percentage) as “unknown” instances.

I

0.06

3.12
0.13

Mutation parameters
18.57
9.29
0.85
0.45

12.38
0.47

30.93
1.00

to see if the test would consider them as mu-
tants, are considered as such in 6% of the mu-
tation test cases. Thus, it is clear that the cur-
rent mutation test is not reliable in this form
as it would imply diﬀerent decisions depend-
ing on the instances one would use.
If we
were to put it into perspective: two users using
the mutation test would end up with diﬀerent
results on a given mutation operator just be-
cause of the instances they trained for the test,
even though the architecture of the model, the
dataset, the learning process, and even the test
set are the same, hence the ﬂakiness we men-
tioned earlier.

Nonetheless, in all cases, we have some in-
stances for which the mutation test results is
that the mutation is killed. As such, if we
follow MT deﬁnition, the mutation is killed.

4

HealthyTrainingInstances UnknownTrainingInstances100instances 100instances Mutation Test kkx50x100However, one can see that this answer is not
satisfactory given that, for instance with the
identity mutation, there is only a 6% proba-
bility on average that it happens. Thus, we ar-
gue that the question for MT, in the context of
ML, is not as much whether the mutation
is killed or not, but rather how likely it is
killed or not. To put it into perspective, the
idea is similar to traditional statistical test with
the concept of p-value and eﬀectsize: having a
signiﬁcant p-value at a given threshold means
there is some statistical diﬀerence, yet the dif-
ference can be so small that it is not practically
signiﬁcant, which is why eﬀectsize is generally
used to complement p-value. Thus, with MT,
we showed it is likely there is always some dif-
ferences, yet the eﬀect of the diﬀerences is not
always the same.

3. Problem deﬁnition

After illustrating concretely the issue with
current MT methods through the motivating
example, we will introduce a few concepts rel-
evant to our approach.

Deﬁnition 1. For a DNN N,
let D be its
training dataset, A its architecture (layers,
hyperparameters,. . .) and P its learning pro-
cess (optimizer,. . .). Let R be the set of all pos-
sible pseudo-random number generators ini-
tialization (seeds), initial values, and random
values (e.g., weights initialization, order of
batch data,. . .). We deﬁne an instance f =
(D, A, P, r) of the DNN as the model obtained
after initializing the DNN and performing the
stochastic process of training it by using the
initialization r ∈ R .

The set of all instances of the DNN achiev-
able through the learning process P of archi-
tecture A over dataset D as:

F = {(D, A, P, r)|r ∈ R}

5

The essential concept here is that r cap-
tures and models all the stochastic elements
of the training process. For example, assum-
ing all random values used in the training pro-
cess (e.g., gradient descendant, weight initial-
ization, and others) are derived from a pseudo-
random number generator, just the initial ran-
dom seed and the pseudo-random algorithm
knowledge will suﬃce to ensure the determin-
istic replication of the entire process. Notice
that there are inﬁnite possible seeds and thus
inﬁnite possible concrete models (i.e., instanti-
ations) each parameterized by a seed. If we use
the object-oriented programming paradigm as
a metaphor: a DNN N is a class whose at-
tributes are of type D, P and A; and any in-
stance of it corresponds to an initialization r of
the attributes (the weights of the layers, the or-
der of data batch,. . .) followed by applying P
on N. Note that for practical purpose, despite
F being inﬁnite, it is represented with a ﬁnite
number of bits and thus its realization (on a
computer) contains a large but ﬁnite number
of instances.

Deﬁnition 2. Let F be the set deﬁned in Deﬁ-
nition 1 for a given DNN N. Let M be a muta-
tion of the DNN N induced either over A, P or
D. We note the set of all instances achievable
of the mutant M as:

FM = {(M(D, A, P), r)|r ∈ R}

To simplify notation and for generality, we
also consider the identity mutation M = I that
is the mutation that doesn’t alter the DNN, in
which case FI = F .

For instance, M can be “delete 3% of the
training dataset”. In that case, M is induced
over D. Note that, if F ∩ FM = ∅, it does
not mean that elements of F and FM disagree
on all possible values and for all possible in-
stances. For example, it is possible that given

( f1, f2) ∈ F and fM ∈ FM, ∃x input such as
f1(x) = fM(x) and f2(x) (cid:44) f1(x) just as de-
scribed in Section 1. Yet, fM is a mutated in-
stance. In other words, on certain inputs, two
“healthy” instances may disagree while they
agree with a mutated instance. We can then
deﬁne MT for DNN as follows:

Deﬁnition 3. Let F and FM be the two sets of
(non-empty and ﬁnite) instances as deﬁned in
Deﬁnition 1 and 2. Let # represent the cardi-
nality of a set. Let T be a test set, and n1, n2
two positive integers (non-zeros, not necessar-
ily equal). We deﬁne S F = {X ⊂ F | #X = n1}
= {X ⊂ FM| #X = n2}. MT for DNN
and S FM
is a function ZT deﬁned as:

ZT : S F × S FM −→ {0, 1}

In other words, given two sets of models’
instances, the test decides if one is a mutated
version of the other. Practically speaking, ZT
is a deﬁnition formulated to accommodate pre-
viously published MT functions. For example,
testing one single healthy instance Ns against
one (single) mutant instance Nm, (i.e.,
tradi-
tional MT), is captured in our deﬁnition by set-
ting S F = {X ⊂ F | #X = 1},
S FM
ZT : S F × S FM −→ δNs(T ),Nm(T ),
where δ is the Kronecker delta.

= {X ⊂ FM| #X = 1} and

Similarly, DeepCrime [12] mutation test is
modeled by setting S F = {X ⊂ F | #X = n},
S FM

= {X ⊂ FM| #X = n} and ZT :

ZT =





1
0

if p-value < 0.05 and eﬀectSize ≥ 0.5
else

(1)
Where the p-value is obtained by using Gen-
eralised Linear Model (GLM) [16] and the ef-
fectSize is calculated using Cohen’s d [17] over
distributions of accuracy values obtained over
test set T .

Note that PMT results depend on the test set
T and on applied mutation operator M, just
like in traditional software engineering, but
also on the sampled and compared DNN in-
stances (“healthy” versus “mutated”). In prac-
tice, due to resources limitation, we don’t have
access to F and FM (and neither do we have
S F and S FM). Rather, we are working with
Ds ⊂ F and Dm ⊂ FM representing the total
number of “healthy” (respectively “mutated”)
trained and available instances. Thus ZT turns
out, in practice, to be ZT|S ,S (cid:48) where S = {X ⊆
Ds| #X = n(cid:48)
2}. To
avoid over-complicating the notations, we will
refer to ZT|S ,S (cid:48) and ZT as Z, since the objective
is to have an approximation of a general func-
tion over S F and S FM and T is the same in all
cases.

1} and S (cid:48) = {X ⊆ Dm| #X = n(cid:48)

Ideally, this function Z should return 0 if
M = I and 1 otherwise, or, at the very least,
return consistent results across any sets of in-
stances for the same mutation operator M.
Yet, example in Section 2 showed it was not
the case.

4. Probabilistic MT

Having deﬁned the setting we were working
in, as well as showing concretely the issue of
current MT, we now describe our PMT frame-
work. An overview describing the complete
process is presented in Figure 2.

4.1. A probabilistic framework for MT

Remember that MT compares some healthy
instances of a DNN {N1, ..., Nn} against some
mutated instances of a DNN {M1, ..., Mn},
where n is a strictly positive integer, from a
pool of instances that we deﬁned in Section 3
as S and S (cid:48). The key observation is that for
any instance of dsi ∈ S , dmi ∈ S (cid:48) computing
the decision function Z(dsi, dmi) correspond to
performing a Bernoulli trial.

6

Figure 2: PMT methodology overview.

As such, instead of proposing a determinis-
tic decision, it is possible instead to consider
the probability of the outcome over S , S (cid:48). By
repeating the comparison (i.e., experiment) N
times, resampling at random each time, we de-
ﬁne Y = X1 + ... + XN ∼ Binomial(N, π) where
Xi ∼ Bernoulli(π) be the random variable rep-
resenting the ith realization of the mutation test
Z ((1) in Fig 2).

Using Bayes rules, probability p(π|S , S (cid:48)) es-
timation can be expressed as a Bayesian esti-
mation problem for the parameter π knowing
observed data S and S (cid:48), that is:

p(π|S , S (cid:48)) ∝ p(S , S (cid:48)|π)p(π)

Since p(S , S (cid:48)|π) is Binomial, we can use
the Beta distribution as conjugate prior. Since
we have no information on the distribution,
we can use a non-informative prior. Kerman
[18] recommends using either the neutral prior
(Beta( 1
3 )) or the uniform prior (Beta(1,1)).
In our experiment, we adopted the latter choice
(i.e., Beta(1,1)). Overall, p(π|S , S (cid:48)) distribu-
tion is Beta(a+k, N −k+b) where a = 1, b = 1
are pseudo counts and k is the number of suc-
cesses (see (2) in Fig 2).

3, 1

4.2. Bayes Bagging

p(π|S , S (cid:48)) and thus the distribution for pa-
rameter π are estimated over S , S (cid:48) rather than
over S F , S FM. Remember that S F , S FM can-
not be accessed in practice; we are limited to
ﬁnite subsets. A workaround to improve esti-
mates is to exploit Bayes Bag [19] which con-
sists in applying bagging to the bayesian pos-
terior. Huggins [20] showed that Bayes Bag
can result in more accurate uncertainty even
with a limited number of replications (B =
50 or 100). Furthermore, under the assumption
that a given sample is representative enough of
an unknown population, we obtain, similarly
to traditional bootstrap [21], an approximation
of the errors of the estimates (Monte-Carlo er-
ror).

In the context of PMT, using bayes bagging,
we can obtain multiple bootstrapped posterior
p(π|S b, S (cid:48)
b) which can be then aggregated into
1
b). This allows us to obtain
B
the p∗(π|S F , S FM) ((3) in Fig 2) approximation
as we wanted.

b=1 p(π|S b, S (cid:48)

(cid:80)B

4.3. PMT posterior analysis

Once we plug the approximation of the pos-
terior probability p∗(π|S F , S FM) into the PMT
framework, Deﬁnition 3 is extended as:

7

HealthyTrainingInstances randomseeds  randomseeds  MutatedTrainingInstancesMutation Test  BinomialExperiment BayesianBagging Posterior DistributionAnalysis*(1)(2)(3)Deﬁnition 3.bis. For S F and S FM, T a test
set, and Z a mutation function as deﬁned in
Deﬁnition 3. We deﬁne a probabilistic MT
function ProbZ as:

ProbZT ,Z,N : (S F × S FM)N −→ π|S F , S FM

Deﬁnition 3.bis provides a means to analyze
the behavior of the test set against the muta-
tions through the analysis of the obtained pos-
terior. Posterior can be analyzed leveraging es-
timates widely used in Bayesian settings, for
instance:

Point estimate: One can derive a point esti-
mate relaying on the posterior distribution. For
instance, the Maximum A Posteriori (MAP),
i.e., πMAP = argmaxπ p∗(π|S F , S FM) or the
Minimum Mean Square Error (MMSE) ˆπ =
E(π|S F , S FM).

Credible Interval: A point estimate is
complemented using a Credible Interval CI.
This is the interval within which an unob-
served parameter value is present with a given
probability 1 − (cid:15)%, that is p(π ∈ CI) = 1 − (cid:15).
Notice that, CI diﬀers from a conﬁdence in-
terval [22]. Multiple CI exists, which can be
tailored based on the point estimate used, such
as the Equal-tailed CI for the median estima-
tor, the Highest density interval (HDI) CI for
the mode estimator (MAP) or the CI centered
around the mean. The chosen CI can for in-
stance be used in a way to measure the un-
certainty of the previous probability (the wider
the CI, the more uncertain the beliefs).

4.4. Eﬀect analysis

If posterior analysis can shed some lights on
the behavior of the mutations, we propose a
practical criteria to establish if a mutation is
likely killed.

Remember, in the ideal case, we would like
our mutation test to return either always not-
mutant or always mutant for any instance used.

The motivating example of Section 2 showed
it was not the case. However, the two result-
ing posteriors we could derive from those ideal
cases can be leveraged as comparison points to
calculate some form of similarity with regard
to the bagged posterior obtained for a given
mutation. One way to compute such proba-
bility similarity involves using the Hellinger
distance [23] which can be deﬁned for two
beta distributions P ∼ Beta(α1, α2), Q ∼
Beta(α2, β2) as:

(cid:118)(cid:116)

H(P, Q) =

1 −

(cid:112)

B( α1+α2
2

, β1+β2

2

)

B(α1, β1)B(α2, β2)

where B is the beta function.

We have that 0 ≤ H(P, Q) ≤ 1, with a
distance H(P, Q) of 0 implying that both P, Q
are the same. Thus, it’s possible to calculate
the distance between the bagged posterior of
a given mutation and both ideal posteriors we
mentioned earlier. Then, we can compute a ra-
tio of similarity between the two distances:

R = H(P, QH)
H(P, QM)
where P is the bagged posterior, QH the
ideal posterior with all non-mutant results and
QM the ideal posterior with all mutant results.
A ratio R of 1 means that the bagged poste-
rior is as similar to both ideal posterior, and so
we have little information on the practical ef-
fect of the mutation is killed, which can for in-
stance happens for a posterior centered around
0.5 (i.e., 50% chance on average that the mu-
tation test returns mutant as a result for any
instance). A ratio higher than 1 implies the
posterior is more similar to the ideal mutant
posterior and the opposite if the ratio is lower
than 1.

In order to decide the magnitude of the ef-
fect, we elaborated the following empirical
scale based of our results, inspired by existing

8

empirical scale elaborated for eﬀect size crite-
ria such as Cohen’s d [24], where |d| > 1.20
is very large, |d| > 0.8 is large, |d| > 0.5 is
medium and |d| > 0.2 is small. In our case, we
found empirically the ratio of similarity calcu-
lated with the healthy posterior to be around
0.82 at most. Thus, since we know the healthy
instances are not mutation and should not be
considered as such, any ratio below 0.82 illus-
trate a very strong evidence against the muta-
tion being killed. From there, we can build the
scale using the above-mentioned rule of thumb
mirroring our ratio, with 0.82 in our case being
1.20 for cohen’s d and 1 in our case being their
0. For the case above 1, we simply take the
invert of the boundaries we would get in the
case below 1. This leads to: 0.82 − 0.87 (resp.
1.15 − 1.22) strong, 0.87 − 0.92 (resp. 1.09 −
1.15) medium, 0.92 − 0.97 (resp. 1.03 − 1.09)
weak, 0.97 − 1.03 negligible. The decision is
left to the user when to consider a mutation
likely killed based on the posterior or similar-
ity ratio obtained, using some thresholds. Note
that, in that conﬁguration, we actually have
three potential outcomes: the mutation is likely
killed, the mutation is likely not killed, and no
evidence points in either direction, which can
happen when the thresholds are not met in ei-
ther way (killed or not killed), that is we do not
have enough evidence to point in either direc-
tion. In practice, this choice can default to not
killing the mutation.

With what was said before, it appears that
we need to redeﬁne the traditional mutation
score. The mutation score is generally deﬁned
as:

MS = #mutations killed

#mutations

(2)

that is the number of mutations killed over the
total number of mutations. With our approach,
we extend the mutation score to:

MS = #mutations | R > θ

#mutations

(3)

9

that is, the number of mutations for which the
similarity ratio is above a certain threshold θ
(i.e., likely killed) over the total number of mu-
tations considered.

An example that

leverages the complete
methodology for the decision will be presented
in Section 5.4.

4.5. Error estimation

There are two types of error we aim to quan-
tify while using PMT: the error of the bag-
ging process (i.e., if the choice of the boot-
strapped data inﬂuence the bagged posterior
results) and the error of the sample representa-
tivity (i.e., given a certain size, does the choice
of the sampled population of instances aﬀect
the bagged posterior results). The two errors
will be analyzed in Section 5.4.2.

Once the bagged posterior p∗(π|S F , S FM)
is available, one can obtain error approxima-
tion. This is formulated as an estimation of a
Monte-Carlo Error (MCE) [20]. MCE estima-
tion has been widely studied [25].
We evaluate the MCE as follows: consider R
replications of our bagged posterior as Monte-
Carlo simulations from which we will derive
the values of Deﬁnition 3.bis. One can es-
timate the MCE using for instance jackknife
In other words, we con-
bootstrapping [26].
sider our R replications of the bagged poste-
rior {π1|S F , S FM, . . . , πR|S F , S FM} from which
we can extract a desired estimate ei such as
X = {e1, . . . , eR}.

We used the jackknife formula as described
in [25] and complemented the error estima-
tion with a traditional conﬁdence interval over
the estimate as recommended by Koehler and
Brown [25].

The estimates we will track are both the
mean µ and variance var of the bagged poste-
riors obtained. A straightforward Monte-Carlo
estimator for both is simply the average over
each µi and vari of each replicate.

5. Evaluation of PMT

The goal of this evaluation is to investigate
how much insight the PMT decision process
can bring when dealing with DNN mutations
and to shed some light on the limitations of
current DNN MT frameworks, by comparing
both frameworks and how the test is tackled in
both cases. In a second time, we also analyze
the trade-oﬀ between the approximation error
and the cost of our method.

5.1. Datasets and Mutations

In our evaluations, we use three models/-
datasets, previously leveraged by DeepCrime
to show how PMT alleviates the problem of
ﬂakiness mentioned earlier, and to compare it
to the latest designed MT, i.e., DeepCrime’s
deﬁnition of MT (see Equation 1). More
precisely, the following model/dataset com-
binations: MNIST [15] (MN) along with a
8 layered convolutional neural network [27];
MovieLens dataset [28] to train the Movie
Recommender [29] (MR) model; A synthe-
sized UnityEyes (UE) dataset [30] along with
a speciﬁc model [31].

Table 3 shows the average metric values ob-
tained on the test set across all our “healthy”
DNN trained instances; ﬁgures are in line with
DeepCrime reported values. As UE and MR
systems are regression-based, we considered
(as in DeepCrime) that a prediction is accurate
if it diﬀers from the correct one by no more
than one rating (for MR) or if the angle is no
more than 5 degrees (for UE).

Regarding mutations, we chose both source-

level and model-level mutations.

Source-level mutations are extracted from
the detailed mutation operators proposed in
DeepCrime. To select which mutation to in-
vestigate in priority (and limit the number of
instances to train), we used DeepCrime Kill-
In their paper
ability and Triviality metrics.

[12], Killability is deﬁned as whether or not
a mutation operator conﬁguration is killed by
the training data using the statistical test pre-
sented in Equation 1. Triviality roughly quan-
tiﬁes how easily a mutation operator can be
killed by any test input of the test set. To
push the method to its limits, for each dataset/-
model, we selected mutations with Killabili-
ty/Triviality that is the highest/lowest possible.
Finally, we also chose some mutations that are
common to all models, to have common points
of comparison for all models.

Regarding Model-level mutations, Deep-
Crime proposes some of those mutations but
does not analyze or implement them. There-
fore, we chose instead to leverage some mu-
tation operators proposed and analyzed in
MuNN [32] and DeepMutation [8]. Table 2
provides an overall view of the selected muta-
tions ((cid:88)) for a given model/dataset, with the
mutation acronym being described below:

• change label (TCL): Modify a percent-
age of training data labels,
replacing
them with the most frequent label in the
dataset.

• delete training data (TRD): Remove a
portion of the training dataset from each
class proportionally.

• change weights initialisation

(WCI):
Change the way weights are initialized in
all the layers of the model.

• change activation function

(ACH):
Change the (non-linear) activation func-
tion of a layer by another (non-linear)
activation function.

• unbalance training data (TUD): Remove
a portion of data belonging to the classes
whose frequency of apparition is less than
average.

10

• change loss function (LCH): Change the
loss function by another loss function.

• change optimisation function

(OCH):
Change the optimisation function by
another optimisation function.

• add weights fuzzing (AWF): Add gaus-
sian noise of magnitude σ to a certain per-
centage of weights of a layer.

• freeze neurons output

(FNO): Freeze
(delete) a percentage of neurons of a
given layer.

DeepCrime’s mutations were reused exactly
as provided in the replication package. MuNN
[32]/DeepMutation [8] based mutations (the
two last ones) were implemented based on the
description/parameters provided in the papers.

5.2. Instrumentation and parameters

To carry out the experiments, we use the
same requirements as documented in Deep-
Crime [12], namely, Python (3.8), Keras
(2.4.3), and Tensorﬂow (2.3). We also used the
models/datasets, mutations operators as well
as the MT procedure used in their replication
package. For each mutated/healthy model, we
train 200 instances and then evaluate the accu-
racy of each instance on the dataset test set.

Unless speciﬁed otherwise, all experiments
use the following default parameters: N = 100
the number of trials for each Binomial exper-
iment and B = 100 the number of bootstrap
repetitions. Again, such values are a com-
promise to ensure a trade-oﬀ between having
a suﬃcient number of evaluations and keep-
ing the computation within a manageable time.
Moreover, we used the same number of in-
stance as in DeepCrime (n = 20) for the MT
with the same MT function Z that they used
(see Equation 1).

5.3. Experiments

In the following, we will introduce the de-
scription of two experiments we did to evalu-
ate our framework.

5.3.1. First Experiment

The ﬁrst experiment aims to apply PMT to
the previously listed models/mutations and to
draw a comparison with MT. To do this, we
leveraged 200 training instances per model/-
dataset/mutation for our method. As a point
of comparison, we will apply MT on Deep-
Crime’s instances provided in their replication
package [12]. We implemented and applied
the procedure detailed in Section 4. The proce-
dure was needed to obtain a bagged posterior
for each mutation (including the identity mu-
tation, that is the “healthy” instances) of each
dataset/model. From there, we can leverage
the eﬀect analysis method we introduced in
Section 4.3 to calculate the ratio of similarity
obtained for each mutation and compare it to
the results one would obtain with simple MT
in order to nuance them.

5.3.2. Second Experiment

The second experiment aims to evaluate the
error over the bagged posterior approximation
and the sampled population representativity.

To estimate the bagged posterior approxi-
mation, we repeated Nexp = 100 times the
calculation of the bagged posterior approxi-
mation, using the jackknife estimation as ex-
plained in Section 4.5.

To evaluate the representativity, we consid-
ered the following. Since the trained instances
are “sampled” at random when trained (i.e.,
the random seed used in training are equally
likely to be picked), the representativity of the
sampled instances will depend on their num-
ber. Thus, we repeated the MCE estimation
we used to estimate the bagged posterior ap-
proximation, with a diﬀerent number of sam-

11

Table 2: Mutations (Source / Model Level) chosen for each dataset/model. ’(cid:88)’ means selected, ’-’ means not selected.

TCL TRD WCI ACH TUD LCH OCH AWF FNO
-
(cid:88)
-

(cid:88)
-
-

(cid:88)
-
-

(cid:88)
(cid:88)
(cid:88)

(cid:88)
-
(cid:88)

(cid:88)
-
(cid:88)

-
(cid:88)
(cid:88)

-
-
(cid:88)

MN (cid:88)
MR (cid:88)
(cid:88)
UE

Table 3: Systems under test. For each model, we provide the average metric value as well as standard deviation (in
parenthesis).

ID Training Data Test Data Epochs
MN
MR
UE

60,000
72,601
103,428

10,000
18,151
25,857

12
12
50

Metric
Accuracy
MSE
Angle based

Value
99.15 (0.06)
0.047 (0.001)
2.6◦ (0.2)

ple instances (from 25 to 190). We repeated
this process Npop = 30 times to account for
the possible eﬀect of the choice of the sam-
In
ples over the obtained bagged posterior.
other words, from the 200 training instances,
we repeated the jackknife estimation 30 times;
each time with a diﬀerent sampled population
of the same size. This allowed us to exam-
ine the evolution of the average parameters es-
timate µ and var across the bagged posterior
as well as the average of their approximation
error boundaries based on the sample size as
well as the sampled instances. As described
in Section 4.5, we also compute conﬁdence in-
terval values as recommended in the literature
[25].

5.4. Results

In this section, we only present a sample
of our overall results because of space limita-
tions. However, we provide all the results, in
our replication package [14].

5.4.1. First experiment: PMT application and

comparison with simple MT

We report results for two mutation operators
for each of the models, the rest of the results
can be found in the replication package [14].

We ﬁrst report in Figure 3 the obtained poste-
riors. Each curve represents the posterior dis-
tribution for a given mutation operator magni-
tude following the procedure described in Sec-
tion 4. For instance, in Figure 3a, the plain
orange curve represents the bagged posterior
distribution of the probability of killing the
delete training data mutation with magnitude
Its MMSE point estimate is ˆπ = 0.8
18.57.
(vertical dash line) and the credible interval
width is |CI| = 0.4 (colored area). Transparent
lines are the bootstrapped posteriors obtained
from each bootstrapped data S b, S (cid:48)
b (see Sec-
tion 4.2). We report in a second time in Ta-
ble 4 a comparison between simple MT results
for each of the mutation operators (i.e., 1 for
the mutation is killed, 0 if it’s not) and the ra-
tio of similarity (with the eﬀect) as we deﬁned
in Section 4.3 for PMT. For instance, for the
mutation MN − T RD, the magnitude 9.29 was
considered killed by MT, yet we found a neg-
ligible eﬀect when using our ratio metric, i.e.,
there is no strong argument to point out that
the mutation is either likely killed or likely not
killed. By default, the user can consider it not
to be killed, in order to avoid potential false
positives (i.e., considering a mutation killed
when it is not). Both those results will allow us

12

Figure 3: Posterior distribution for mutation operators of diﬀerent magnitudes. Vertical dash lines symbolize the point
estimate value of each posterior. Plain lines curve represent the bagged posteriors, with the colored area underneath
being the CI, while transparent lines are each posterior obtained from bootstrapped data.

(a) MR - TRD

(d) MN - ACH

(b) MR - TRD

(e) MR - TUD

(c) UE - TRD

(f) UE - TCL

13

Table 4: Example. For MT, “(cid:88)” means the mutation is killed and “(cid:55)” the mutation is not killed. For PMT, we give
the ratio of similarity value and the eﬀect based on the scale presented in Section 4.3 with the following meaning:
“◦” negligible, “-” weak, “±” medium, “+” strong and “++” very strong. red is when the eﬀect is the likeliness of the
mutation being killed (R > 1), and blue when the eﬀect is the likeness of the mutation not being killed (R < 1). We
also present the similarity ratio when comparing healthy instances against themselves (identity mutation I).

I
0.76
MT
PMT
I
0.81
MT
PMT
I
0.73
MT
PMT

3.1
(cid:55)
0.91 (±)

3.1
(cid:55)
0.95 (-)

9.29
(cid:88)
1.00 (◦)

6.19
(cid:88)
1.00 (◦)

3.1
(cid:55)
0.94 (-)

9.29
(cid:55)
0.72 (++)

MN - TRD
12.38
(cid:55)
1.00 (◦)
MR - TRD
9.29
(cid:88)

18.57
(cid:88)
1.05 (-)

30.93
(cid:88)
>2 (++)

12.38
(cid:88)

elu
(cid:88)
0.99 (◦)

12.5
(cid:55)
0.96 (-)

1.86 (++) >2 (++)
UE - TRD
12.38
(cid:55)
0.95 (-)

18.57
(cid:55)
1.00 (◦)

24.75
(cid:88)
1.00 (◦)

3.12
(cid:55)
0.74 (++)

MN - ACL
sigmoid
(cid:88)

exp
(cid:88)

21.88
(cid:55)
1.00 (◦)

1.00 (◦) >2 (++)
MR - TUD
25
(cid:55)
1.00 (◦)
UE - TCL
9.38
(cid:55)
1.00 (◦)

6.25
(cid:55)
0.95 (-)

tanh
(cid:88)
1.00 (◦)

softmax
(cid:88)
>2 (++)

50.0
(cid:88)
>2 (++)

12.5
(cid:88)
1.05 (-)

18.75
(cid:88)
>2 (++)

to showcase the advantage of PMT over MT.

present.

Stability: One thing we ﬁrst showed with
the motivating example and that we show
again here is the lack of stability of the sim-
ple MT, i.e., the ﬂakiness we mentioned ear-
lier. Indeed, the fact that all posterior distri-
butions do not translate to the ideal mutant or
not-mutant posterior we described (that is, MT
returning always 0 or always 1 no matter the
instances used) can have dire consequences.
For instance, looking at Figure 3c, PMT shows
that the posterior distribution of the mutation
of magnitude 9.29 is very similar to the healthy
one. More directly, in the Table 4, we see that
the mutation is likely not killed with a simi-
larity ratio of 0.72 (very strong). Nonethe-
less, the point estimate is non-zeros and so
that means that, for some instances, there is
a chance that simple MT returns “mutant” as a
result, despite PMT showing strong evidence
the mutation should not be considered killed.
This is similar to our motivating example in
Section 2 where we for instance found out that
6% of tests done on healthy instances returned
mutant as a result despite no mutation being

Finding 1: PMT allows stability over
test results contrary to MT. That is the
decision made over a given mutation is
taken while accounting for any instance
possible, which prevents the ﬂakiness
issue we illustrated previously, i.e., MT
returning 0 and 1 for the same mutation
depending on the instances used in the
test.

Consistence: Besides mitigating stability
problems, PMT allows tackling another issue
of MT: the potential lack of consistency across
Indeed, as MT outcome is binary,
the tests.
one can not ensure that mutations that behave
similarly lead to the same MT outcome, since
there is no information available for the pos-
terior of the distribution. On the contrary, us-
ing PMT, one can compare results for diﬀerent
mutations, whether from the same operators or
from a diﬀerent one. For instance, in Figure
3a both the mutation of magnitude 9.29 and
12.38 exhibit the same posterior and a simi-
lar ratio of similarity of 1.00 (negligible), as

14

such, logically, a decision made over these two
mutations should be the same. However, us-
ing Deepcrime’s instances for MT yield oppo-
site results, once again probably because of the
training instances used in the test.

Finding 2: PMT allows for coherence
across results, making sure the outcome
of the test will be similar for mutations
that exhibit similar posterior distribu-
tions/similarity ratios contrary to MT.

Granularity: Finally, note that using PMT,
one can quantify if a mutation operator is more
or less likely to be killed or not killed. For
instance,
in Figure 3f, for UnityEyes (UE)
TCL − 3.12, the posterior distribution is sim-
ilar to the one of the original (i.e., healthy)
model and exhibits a similarity ratio of 0.74,
thus it surely would not be killed by the test
set no matter the instances and so there is a
very strong proof for considering the mutation
likely not killed. This analysis is not some-
thing MT would tell us, as it returns a de-
terministic decision over the given instances.
Similarly, for TCL−6.25, we only have a weak
eﬀect to consider the mutation likely not killed,
which is still more than for TCL − 9.38 where
the eﬀect is negligible with a ratio of 1.00. As
such, if there are some incentives to say that
indeed TCL − 6.25 can be considered likely
not killed (and thus there is evidence that it
should not be considered killed), such incen-
tives do not exist for TCL − 9.38. Nonethe-
less, MT would just consider those two muta-
tions to be similarly “not killed”, which limits
the potential to analyze them. Similarly be-
tween TCL − 12.5 and TCL − 18.75, which
are both considered killed by MT, yet our ap-
proach highlights that we have more incentives
to consider the mutation likely killed for the
latter rather than for the former.

Finding 3: PMT delivers a ﬁner grain
analysis of the mutations, which allows
to compare them on the likeliness of
the mutation being killed. This might
enable, for instance, to adjust potential
thresholds one user would select to con-
sider a mutation likely killed, depending
on the similarity ratio obtained. This is
not possible with MT, which will con-
sider all the killed mutations as similar.

5.4.2. PMT trade-oﬀ study

Figure 4 and 5 show the error estimation
of the estimates when computing the bagged
posterior with diﬀerent samples and diﬀerent
sample sizes. Figure 4 focuses on one mod-
el/mutation operator and varies the magnitude
of the mutation, while Figure 5 shows the re-
sults for the same mutation operator and same
magnitude for diﬀerent models. Similar trends
can be noticed for other models/mutation op-
erators. First, from these graphs, we can make
the general following observations:

• The larger the sample size, the lower the
error across the diﬀerent samples of the
same size. This resonates with the intu-
ition that the bootstrap hypothesis is in-
creasingly valid. In other words, the sam-
ple is increasingly more representative of
the unknown underlying population as we
increase the number of instances in the
sample.

• The average across the Npop = 30 runs
of the diﬀerent estimates, as well as the
average of their lower bound and upper
bound (dot on the plots), are close. This
suggests that for a given sample, the in-
dividual conﬁdence interval, on average,
is not large.
In a nutshell, this means
that there is not a huge variation between

15

the bagged posterior obtained from the
monte-carlo simulation for a given sam-
ple. Overall, our ﬁndings suggest that
there is a low monte-carlo error when
estimating the bagged posterior approx-
imation error for B = 100 bootstrapped
datasets, similarly to Huggins [20] obser-
vations.

As a consequence of these ﬁndings, if the
bagged posterior approximation error is low,
the error due to the representativity (and so the
size) of the sampled population is big, yet it
will decrease logically as the sample size in-
creases. Of course, the larger the number of
available trained instances the better. In prac-
tice, it seems that our choice of 200 instances
is indeed warranted, as the variation across
samples decreases with the sample size, and
for 190 the conﬁdence intervals are relatively
small.

Finding 4: When applying PMT, fol-
lowing Huggins [20] observation of set-
ting B = 100 for the bootstrapped repe-
titions is a sound choice. Moreover, our
choice of leveraging a sampled popula-
tion of size 200 also seems warranted as
the approximation error over the bagged
posterior is relatively small with a sam-
ple size of 190.

Secondly, we can now compare the evolu-
tion of the error estimation across models and
mutations.
In Figure 4 we can compare the
evolution through the increased magnitude of
the mutations. We note for instance that the
error estimation tends to be lower for a mu-
tation operator with a low or high magnitude
compared to a medium one (3.1 and 30.93 vs
9.29, glorot normal and zeros vs he normal).
Most likely, mutations with medium magni-
tude (for a given mutation operator) are more
prone to divergence among the instances and

16

more likely to have larger diﬀerences across
In Figure 5, we compare the er-
samples.
ror estimation for the same mutation opera-
tor and magnitude across the models. There
does not seem to be necessarily a similar evo-
lution across models for the same mutation op-
erator (see for instance “change label”, with
UnityEyes and the others), as such, the error
estimation does not seem to be based on the
mutation operator, but rather to be model de-
pendent.

Finding 5: Medium magnitude muta-
tion operators tend to have a higher er-
ror for the same sampled size when
compared to mutation operators with
low/high magnitude. Moreover, there
is no explicit trend in the decrease of
the error for the same mutation op-
erator across the diﬀerent models, so
the error reduction seems more model-
dependent.

5.5. Discussion

In Section 5, we have shown that using a de-
terministic test over a set of instances will not
oﬀer stable results for MT, hence the ﬂakiness
issue we mentioned. On the contrary, using
PMT to calculate the posterior distribution of
the test allows for better insights into the muta-
tion operators under test. One important point
is that, as mutations can be both killed and not
killed by a test for some particular instances,
the notion of killing a mutation as used in tra-
ditional MT does not seem to be very relevant
in the context of ML. As such, we prefer to re-
fer to the notion of mutation being likely killed,
i.e., do we have suﬃcient evidence in a direc-
tion to assert it, similarly to how eﬀect size cri-
terion would be used in a statistical test.

One consequence of this decision led us to
introduce the similarity ratio metric R which

Figure 4: Error of estimates when testing on “delete training data” (left) and “change weights initialisation” (right)
mutation operators for MNIST on three magnitudes (top to bottom: 3.1, 9.29, 30.93 and glorot normal, he normal
and zeros). For each estimate, we present the average over Npop = 30 of the monte-carlo estimate (blue), monte-carlo
lower bound (green) and monte-carlo upper bound (orange) as calculated in Section 4.5. We also display the 95%
conﬁdence interval.

17

Figure 5: Error of estimates when testing on “delete training data” (left) and “change label” (right) mutation operators
for the three models the same magnitude (top to bottom: MNIS T , MovieRecomm, UnityEyes). For each estimate,
we present the average over Npop = 30 of the monte-carlo estimate (blue), monte-carlo lower bound (green) and
monte-carlo upper bound (orange) as calculated in Section 4.5. We also display the 95% conﬁdence interval.

18

allows for practical decision over the calcu-
lated posterior. We showed in the results of
Section 5.4.1 that this metric yields a more
insightful and ﬁner grain analysis than the
simple binary outcome of current MT frame-
works. In particular, with PMT, we are able
to get stable and coherent test results over
the mutation operators, which would not be
the case with MT because of the ﬂakiness
stemming from the selection/choice of the in-
stances. We proposed an empirical scale to
quantify the eﬀect given by R, based on scales
that are used for eﬀect size in statistical test.
This scale gives a rough idea to the user of
the level of conﬁdence attached to how likely
the mutation is killed, which can help the user
make a decision about whether or not to con-
sider the mutation likely killed. In practice, a
conservative choice would be to accept only
mutations with a R of at least 1.15, that is a
strong eﬀect, which results in posterior distri-
butions being very similar to the ideal mutant
posterior. For instance, taking the mutation
MN − T RD we used in the motivating exam-
ple of Section 2, using the results from Table 4,
we would consider only the mutation of mag-
nitude 30.93 to be killed, as the eﬀect is very
strong, the rest of the mutations being by de-
fault considered as not killed in order not to
have false positive.

Note that, aside from a ratio R around 1 or
below (above) 0.82 (1.22), intermediate levels
of the scale might be regarded as arbitrary. Yet,
they serve the practical purpose of allowing to
at least be able to compare the eﬀect of the
diﬀerent mutations, in a more meaningful way
than the binary outcome of MT and in a more
direct way than the more complicated analysis
of the posterior distributions obtained through
the bagging process.

Although PMT requires more computations
since it needs access to more training instances
to obtain a stable posterior, it is fully auto-

mated, can be easily adapted to any new muta-
tion/models using our provided framework in-
side the detailed replication package [14] and,
given trained instances, PMT does not have a
huge time overhead (∼ 1 minutes/mutations to
make a decision), especially using paralleliza-
tion. Our results have been computed with
200 instances and we found the error to be
relatively small for a sample size of > 190
when verifying empirically the MCE over the
estimates. In practice, a lower number of in-
stances may suﬃce depending on the precision
required and the mutation operator/model un-
der test.

In any case, we believe the ability to better
analyze mutations in DNN settings (in particu-
lar, to avoid potential tests yielding that a mu-
tation is killed when it’s not) out-weights the
increase in cost due to the higher number of
instances needed. This is especially true for
DNNs used in safety-critical systems, where
the reliability of tests is crucial.

6. Threats to Validity

Construct validity. PMT relies on some
approximations in which error is empirically
evaluated. As such, there is an intrinsic er-
ror that we cannot reduce to theory and which
depends on the model/dataset/mutation used.
The rest of the assumptions are grounded in
theory or previous research works. However,
we showed empirically that for a suﬃcient
number of instances,
the error is relatively
small and thus does not impact much the de-
cision.

Regarding the empirical scale, if it is mostly
based on our experimental results, its main
purpose is to allow us to draw a fair compari-
son among diﬀerent mutations, in order to as-
sess their diﬀerent eﬀects. Moreover, the scale
was designed to be a direct way of interpreting
and comparing the posterior distributions our

19

approach was built, in a more practical way
for the user. As such, the absolute value of the
scale is less important than the relative com-
parison we can draw from it.

Internal validity. Because of the computa-
tion overhead induced by our method, a high
number of instances are needed for each mu-
tation operator, we had to choose which mu-
tation to evaluate, and on which model. As
such, the choice of the mutations and models
could have an impact on the results. To miti-
gate this threat, we made sure to choose muta-
tion operators based on DeepCrime’s Killabil-
ity / Triviality analysis performed for their mu-
tation operator. For Model level mutation, we
used mutation operators listed in both Deep-
Mutation and MuNN. Regarding mutation pa-
rameters, we used parameters provided in the
replication package of DeepCrime [12] and the
one mentioned in MuNN [32]. We also made
sure to keep similar mutations across models/-
datasets, to allow for a point of comparison.

For the speciﬁc choice of models/datasets,
we chose the models/datasets used in Deep-
Crime as they provided them in their replica-
tion package, along with mutation operators
which were designed to work on such mod-
els, which is more practical and ensure bet-
ter replication ability. The particular choice
of models/dataset was then motivated by the
number of epochs to reduce the computation
overhead, but we made sure to choose diverse
enough subjects (regression and classiﬁcation,
image-based and non-image-based...)
to im-
prove generalization.

Finally, note that although we have used the
same mutation test as DeepCrime because it is
the latest MT approach designed and because
we leveraged their instances for comparison,
the mutation function could be anything the
user deems ﬁt, as long as it respects the def-
inition provided in Section 1. As such, PMT
is general enough to be applicable in a wide

variety of scenarii.

External validity. We chose the same mod-
els/datasets as in DeepCrime, based on the
popular framework Keras, which may limit the
generalization of the study. Yet, it was neces-
sary to ensure that we use mutations, models,
and datasets in the same way as DeepCrime,
to draw a fairer comparison as we used their
instances, mutation test, and some of their op-
erators. Nonetheless, we expect the results to
generalize because the process is independent
of the model/dataset/mutation used.

Reliability validity. Not to overwhelm the
paper with results and due to space limitations,
we did not include all the results of our exper-
iments. Therefore, we have provided the com-
plete results in our replication package. We
also provide all necessary details required to
replicate our study, as well as the implementa-
tion of PMT in our replication package [14].

7. Related works

MT is an established technique in SE
[33, 34].
It has also been applied in set-
tings where non-determinism is present, for
instance, Probabilistic Finite Sates Machine
[35]. Recently, researchers have been apply-
ing MT to DNN, with Nour et al. [36] eval-
uating MT tool’s eﬀectiveness on DNN, and
DeepMutation [8], DeepMutation++ [9], or
MuNN [32] proposing MT framework speciﬁc
for DNN. These approaches notably distin-
guished between source-level mutations, i.e.,
mutation acting on model before training (for
instance, removing part of the training data),
and model-level mutations, i.e., mutations act-
ing on an already trained model (for instance,
adding noise to weights of the model). How-
ever, DeepMutation and similar approaches do
not necessarily take the stochastic nature of
DNN into account and do not oﬀer real faults-
based mutation operators. Based on this obser-

20

vation, Jahangirova et al. [11] introduced the
statistical mutation test we described in Sec-
tion 1 and compared empirically previous MT
frameworks. This work was then further ex-
tended leading to DeepCrime [12], where au-
thors proposed a training set-based analysis of
killability and redeﬁned the notion of trivial-
ity using fuzzy logic. Although they proposed
new measures for mutation test analysis, they
still used the same statistical test as in the pre-
vious work, which resulted in the ﬂakiness we
described in Section 1. In this paper, we pro-
pose a novel formulation of MT for DNN. Our
proposed approach allows for stable test re-
sults, mitigating the ﬂakiness issue, as well as
a ﬁner grain analysis of the mutations’ behav-
ior which makes possible a more coherent de-
cision over the test results across mutations.

Note that there is still a debate on what con-
stitutes an acceptable mutation in DNN. For
instance, according to Panichella et al. [37],
source-level mutations such as those evaluated
in DeepCrime may not be regarded as muta-
tions in the classical sense. The argument is
that since DNN can be seen as a test-driven
development procedure and the training data
as a test suite, source-level mutation operators
(for instance, removing a percentage of train
data) aﬀect the test suite rather than the pro-
duction code. This interpretation is up to de-
bate since the training data is a crucial part of
a DNN speciﬁcation [38, 39] and not simply
a part of a test suite. In fact, the training data
is responsible for what the DNN “learns” con-
trary to a simple test data that evaluates what
the DNN has learned. Hence, mutating train-
ing data can be considered similar to mutat-
ing the production code, since modifying the
speciﬁcation leads to a diﬀerent model. In this
paper, similarly to [12, 11], we consider muta-
tions over the training process to be proper for
MT.

8. Conclusion

This paper introduced PMT, a probabil-
ity framework for MT in deep learning, to
solve the ﬂakiness inherent to current MT ap-
proaches. Using real-faults-based mutations as
well as the mutation test proposed by Deep-
Crime, we evaluated PMT, showing how to
leverage it to decide whether a mutation can be
considered killed or not, in a more reliable way
than what was proposed before. Moreover,
we showed that, for a suﬃcient number of in-
stances, the approximation made in PMT (ap-
proximate bagged posterior and sample size
eﬀect) can be neglected, eﬀectively stabilizing
the posterior obtained by the process. Finally,
the approach is fully automated, the decision
does not introduce a huge time overhead once
instances are trained and can be extended eas-
ily to any model/dataset/mutation.

In future work, we plan on investigating
how to reduce the higher number of instances
needed by PMT, possibly by evaluating if it is
possible to predict the mutation behavior.

Funding Sources

This work is supported by the DEEL project
CRDPJ 537462-18 funded by the National
Science and Engineering Research Council of
Canada (NSERC) and the Consortium for Re-
search and Innovation in Aerospace in Qu´ebec
(CRIAQ), together with its industrial partners
Thales Canada inc, Bell Textron Canada Lim-
ited, CAE inc and Bombardier inc.

References

[1] D. Marijan, A. Gotlieb, M. Kumar Ahuja, Chal-
lenges of testing machine learning based systems,
in: 2019 IEEE International Conference On Artiﬁ-
cial Intelligence Testing (AITest), 2019, pp. 101–
102. doi:10.1109/AITest.2019.00010.

21

[2] M. A. Jamil, M. Arif, N. S. A. Abubakar, A. Ah-
mad, Software testing techniques: A literature re-
view, in: 2016 6th International Conference on
Information and Communication Technology for
The Muslim World (ICT4M), 2016, pp. 177–182.
doi:10.1109/ICT4M.2016.045.

[3] M. Shahid, S. Ibrahim, M. N. Mahrin, A study
on test coverage in software testing, Advanced
Informatics School (AIS), Universiti Teknologi
Malaysia, International Campus, Jalan Semarak,
Kuala Lumpur, Malaysia (2011).

[4] K. Pei, Y. Cao, J. Yang, S. Jana, Deepxplore: Au-
tomated whitebox testing of deep learning sys-
tems, Commun. ACM 62 (11) (2019) 137–145.
doi:10.1145/3361566.
URL https://doi.org/10.1145/3361566
[5] R. DeMillo, R. Lipton, F. Sayward, Hints on test
data selection: Help for the practicing program-
mer, Computer 11 (4) (1978) 34–41. doi:10.
1109/C-M.1978.218136.

[6] J. Andrews, L. Briand, Y. Labiche, A. Namin, Us-
ing mutation analysis for assessing and compar-
ing testing coverage criteria, IEEE Transactions
on Software Engineering 32 (8) (2006) 608–624.
doi:10.1109/TSE.2006.83.

[7] M. Papadakis, M. Kintis, J. Zhang, Y. Jia, Y. L.
Traon, M. Harman, Chapter six - mutation testing
advances: An analysis and survey, in: A. M.
Memon (Ed.), Advances in Computers, Vol. 112
of Advances in Computers, Elsevier, 2019, pp.
275–378.
doi:https://doi.org/10.1016/
bs.adcom.2018.03.015.
URL
science/article/pii/S0065245818300305

https://www.sciencedirect.com/

[8] L. Ma, F. Zhang, J. Sun, M. Xue, B. Li, F. Juefei-
Xu, C. Xie, L. Li, Y. Liu, J. Zhao, Y. Wang,
Deepmutation: Mutation testing of deep learning
systems (2018). doi:10.48550/ARXIV.1805.
05206.
URL https://arxiv.org/abs/1805.05206
[9] Q. Hu, L. Ma, X. Xie, B. Yu, Y. Liu, J. Zhao,
Deepmutation++: A mutation testing frame-
work for deep learning systems, in: 2019 34th
IEEE/ACM International Conference on Auto-
mated Software Engineering (ASE), 2019, pp.
1158–1161. doi:10.1109/ASE.2019.00126.

[10] X. Xie,

J. W. Ho, C. Murphy, G. Kaiser,
B. Xu, T. Y. Chen, Testing and validating
machine learning classiﬁers by metamorphic
testing, Journal of Systems and Software 84 (4)
the Ninth International Con-
(2011) 544–558,

ference on Quality Software.
//doi.org/10.1016/j.jss.2010.11.920.
URL
science/article/pii/S0164121210003213

https://www.sciencedirect.com/

doi:https:

[11] G. Jahangirova, P. Tonella, An empirical evalua-
tion of mutation operators for deep learning sys-
tems, in: 2020 IEEE 13th International Confer-
ence on Software Testing, Validation and Veriﬁ-
cation (ICST), 2020, pp. 74–84. doi:10.1109/
ICST46399.2020.00018.

[12] N. Humbatova, G.

Jahangirova, P. Tonella,
Deepcrime: Mutation testing of deep learning
systems based on real faults,
in: Proceedings
of the 30th ACM SIGSOFT International Sym-
posium on Software Testing and Analysis,
ISSTA 2021, Association for Computing Ma-
chinery, New York, NY, USA, 2021, p. 67–78.
doi:10.1145/3460319.3464825.
URL https://doi.org/10.1145/3460319.
3464825

[13] W. Zheng, G. Liu, M. Zhang, X. Chen, W. Zhao,
Research progress of ﬂaky tests, in: 2021 IEEE
International Conference on Software Analysis,
Evolution and Reengineering (SANER), 2021, pp.
639–646.
doi:10.1109/SANER50967.2021.
00081.

[14] F. T. (FlowSs), available at https://github.

com/FlowSs/PMT (2022).

[15] Y. Lecun, L. Bottou, Y. Bengio, P. Haﬀner,
Gradient-based learning applied to document
recognition, Proceedings of the IEEE 86 (11)
(1998) 2278–2324. doi:10.1109/5.726791.

[16] J. A. Nelder, R. W. M. Wedderburn, Generalized
linear models, Journal of the Royal Statistical So-
ciety. Series A (General) 135 (3) (1972) 370–384.
URL
http://www.jstor.org/stable/
2344614

[17] K. Kelley, K. J. Preacher, On eﬀect size, Psy-
chological Method 17 (2) (2012) 137–152. doi:
https://doi.org/10.1037/a0028086.
[18] J. Kerman, Neutral noninformative and informa-
tive conjugate beta and gamma prior distributions,
Electronic Journal of Statistics 5 (none) (2011)
1450 – 1470. doi:10.1214/11-EJS648.
URL https://doi.org/10.1214/11-EJS648
[19] P. B¨uhlmann, Discussion of big bayes stories and
bayesbag, Statistical Science 29 (1) (2014) 91–94.
URL
http://www.jstor.org/stable/
43288454

[20] J. H. Huggins, J. W. Miller, Robust inference and
model criticism using bagged posteriors (2019).

22

doi:10.48550/ARXIV.1912.07104.
URL https://arxiv.org/abs/1912.07104

[21] B. Efron, Bootstrap Methods: Another Look
at the Jackknife, The Annals of Statistics 7 (1)
(1979) 1 – 26. doi:10.1214/aos/1176344552.
URL
https://doi.org/10.1214/aos/
1176344552

[22] C. L. M. Hespanhol L., Vallio C. S., S. B. T.,
Understanding and interpreting conﬁdence and
credible intervals around eﬀect estimates, Brazil-
ian journal of physical therapy 23 (4) (2019) 290–
301. doi:10.1016/j.bjpt.2018.12.006.
URL
2018.12.006

https://doi.org/10.1016/j.bjpt.

[23] H. Cramer, Mathematical Methods of Statistics,

Princeton University Press, 1946.

[24] S. S. Sawilowsky, New eﬀect size rules of thumb,
Journal of modern applied statistical methods 8 (2)
(2009) 26.

[25] S. H. E. Koehler, E. Brown, On the assessment
of monte carlo error in simulation-based statistical
analyses, The American statistician 63 (2) (2009)
155–162. doi:10.1198/tast.2009.0030.
[26] B. Efron, Jackknife-after-bootstrap standard er-
rors and inﬂuence functions, Journal of the Royal
Statistical Society: Series B (Methodological)
54 (1) (1992) 83–111.

[27] K. M. C. Model, available at https://keras.
io/examples/vision/mnist_convnet/
(2022).

[28] M. R. Dataset, available at http://files.
grouplens.org/datasets/movielens/
ml-latest-small.zip (2022).

[29] K. M. R. Dataset,

available

at https:

//keras.io/examples/structured_data/
collaborative_filtering_movielens/
(2022).

[30] E. Wood, T. Baltruˇsaitis, L.-P. Morency, P. Robin-
son, A. Bulling, Learning an appearance-based
from one million synthesised
gaze estimator
images, in: Proceedings of the Ninth Biennial
ACM Symposium on Eye Tracking Research &;
Applications, ETRA ’16, Association for Com-
puting Machinery, New York, NY, USA, 2016, p.
131–138. doi:10.1145/2857491.2857492.
URL https://doi.org/10.1145/2857491.
2857492

[32] W. Shen, J. Wan, Z. Chen, Munn: Mutation anal-
ysis of neural networks, in: 2018 IEEE Interna-
tional Conference on Software Quality, Reliabil-
ity and Security Companion (QRS-C), 2018, pp.
108–115. doi:10.1109/QRS-C.2018.00032.

[33] D. Schuler, A. Zeller, Javalanche: Eﬃcient mu-
tation testing for java, in: Proceedings of the 7th
joint meeting of the European software engineer-
ing conference and the ACM SIGSOFT sympo-
sium on The foundations of software engineering,
2009, pp. 297–298.

[34] R. Baker, I. Habli, An empirical evaluation of
mutation testing for improving the test quality
of safety-critical software, IEEE Transactions on
Software Engineering 39 (6) (2012) 787–805.
[35] R. M. Hierons, M. G. Merayo, Mutation test-
ing from probabilistic ﬁnite state machines, in:
Testing: Academic and Industrial Conference
Practice and Research Techniques - MUTATION
(TAICPART-MUTATION 2007), 2007, pp. 141–
150. doi:10.1109/TAIC.PART.2007.20.
[36] N. Chetouane, L. Klampﬂ, F. Wotawa, Investigat-
ing the eﬀectiveness of mutation testing tools in
the context of deep neural networks, in: I. Rojas,
G. Joya, A. Catala (Eds.), Advances in Computa-
tional Intelligence, Springer International Publish-
ing, Cham, 2019, pp. 766–777.

[37] A. Panichella, C. C. S. Liem, What Are We
Really Testing in Mutation Testing for Machine
Learning? A Critical Reﬂection, IEEE Press,
2021, p. 66–70.
URL
ICSE-NIER52604.2021.00022

https://doi.org/10.1109/

[38] L. Gauerhof, R. Hawkins, C. Picardi, C. Pater-
son, Y. Hagiwara, I. Habli, Assuring the safety
of machine learning for pedestrian detection at
crossings, in: A. Casimiro, F. Ortmeier, F. Bitsch,
P. Ferreira (Eds.), Computer Safety, Reliability,
and Security, Springer International Publishing,
Cham, 2020, pp. 197–212.

[39] R. Salay, K. Czarnecki, Using machine learning
safely in automotive software: An assessment and
adaption of software process requirements in iso
26262, ArXiv abs/1808.01614 (2018).

[31] A.

implementation of

a multimodal CNN
avail-
at https://github.com/dlsuroviki/

for appearance-based gaze estimation,
able
UnityEyesModel (2022).

23

