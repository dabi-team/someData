Hybrid Intelligent Testing in Simulation-Based
Veriﬁcation

Nyasha Masamba
Faculty of Engineering
Trustworthy Systems Laboratory
University of Bristol
United Kingdom
Email: nyasha.masamba@bristol.ac.uk

Kerstin Eder
Faculty of Engineering
Trustworthy Systems Laboratory
University of Bristol
United Kingdom
Email: kerstin.eder@bristol.ac.uk

Tim Blackmore
Inﬁneon Technologies
Stoke Gifford
Bristol
United Kingdom
Email: tim.blackmore@inﬁneon.com

2
2
0
2

t
c
O
6
1

]

R
A
.
s
c
[

3
v
2
5
5
9
0
.
5
0
2
2
:
v
i
X
r
a

Abstract— Efﬁcient and effective testing for simulation-based
hardware veriﬁcation is challenging. Using constrained random
test generation, several millions of tests may be required to
achieve coverage goals. The vast majority of
tests do not
contribute to coverage progress, yet they consume veriﬁcation
resources. In this paper, we propose a hybrid intelligent testing
approach combining two methods that have previously been
treated separately, namely Coverage-Directed Test Selection and
Novelty-Driven Veriﬁcation. Coverage-Directed Test Selection
learns from coverage feedback to bias testing toward the most
effective tests. Novelty-Driven Veriﬁcation learns to identify and
simulate stimuli that differ from previous stimuli, thereby reduc-
ing the number of simulations and increasing testing efﬁciency.
We discuss the strengths and limitations of each method, and
we show how our approach addresses each method’s limitations,
leading to hardware testing that is both efﬁcient and effective.

Keywords—Design Veriﬁcation, Intelligent Testing, Coverage-
Directed Test Selection, Novelty-Driven Veriﬁcation, Machine
Learning for Veriﬁcation, CDS, CDG, EDA

I. INTRODUCTION

Functional veriﬁcation is the process of reconciling a de-
sign’s functional speciﬁcation with its implementation to gain
conﬁdence in the functional correctness of the design [1].
Simulation-based veriﬁcation is a type of functional veriﬁ-
cation which involves generating test stimuli, stressing the
design under test (DUT) with those tests in simulation, and
checking that the resulting DUT behaviour adheres to the
speciﬁcation. Deviations between DUT behaviour are ﬂagged
and investigated, as they could be the result of faults –
commonly referred to as bugs – in the design logic.

When stimuli exercise some given DUT functionality, the
event
is recorded in a coverage database. Coverage is an
important measure of veriﬁcation progress and test quality
in simulation-based veriﬁcation. Even modest sized DUTs
have vast functionality that renders exhaustive veriﬁcation
impractical. Coverage helps the veriﬁcation engineer to narrow
down veriﬁcation efforts to areas of the DUT deemed crucial
for operation. By monitoring coverage, veriﬁcation engineers
know when sufﬁcient testing has been done. Based on analysis
of the coverage reports, veriﬁcation engineers also know where
to focus their efforts by biasing test generation to target
uncovered areas.

There are two hard challenges in simulation-based veriﬁca-
tion, namely generating effective test stimuli, and increasing
veriﬁcation efﬁciency. Effective tests are those that are legal
with respect to the DUT speciﬁcation, and useful at exercising
important and hard-to-reach DUT functionality. Being efﬁcient
means reducing consumption of veriﬁcation resources such
as computation, EDA licences, simulation time and manual
engineer effort.

Constrained random test generation is currently the state-of-
the-art test generation method in simulation-based veriﬁcation.
However, constrained random test generation can be inefﬁcient
because a large number of constrained random tests end up
exploring the same functionality. Such inefﬁciencies lead to
needless consumption of veriﬁcation resources. Constrained
random test generation tends to be an ineffective method
for targeting coverage holes. Coverage holes are functional
areas of the DUT that have been speciﬁcally identiﬁed for
veriﬁcation, but currently remain unveriﬁed. Manual, time-
consuming constraint biasing by senior veriﬁcation engineers
is usually employed to overcome the ineffectiveness of con-
strained random testing.

Intelligent

testing offers a solution for improving con-
strained random test generation. Intelligent testing refers to
methods employing artiﬁcial intelligence (AI) [2] to increase
the efﬁcacy and efﬁciency of constrained random test gen-
eration. Earlier intelligent testing approaches were based on
AI techniques such as Bayesian Networks [3] and Inductive
Logic Programming [4]. These earlier approaches can be
broadly categorised as Coverage-Directed Test Generation
(CDG) methods [5]. CDG methods generally use AI to
discover relationships between coverage feedback and test
generation constraints. The insights derived from CDG are
used to increase effectiveness of generated stimuli by biasing
test generation towards plugging coverage holes. However,
CDG has historically required signiﬁcant manual encoding
of both DUT and domain knowledge into the AI model.
Consequently, these earlier works on intelligent testing could
only manage to experiment with relatively small DUTs, aside
from replacing one form of manual work (writing constraints)
with another (encoding knowledge into an AI model).

An adaptation of CDG, coverage-directed test selection [6]

 
 
 
 
 
 
introduced an approach for modelling relationships between
functional coverage and test stimuli through supervised learn-
ing. Coverage-directed test selection (CDS) is ideal when a
large set of constrained random test stimuli can be generated at
little effort and cost. If signiﬁcantly more veriﬁcation resources
are required to simulate those stimuli, then it is prudent to
select effective stimuli from the large set, and then prioritise
them for simulation. CDS focuses on biasing test selection
towards the most effective tests. It uses supervised learning to
extract constraints from labelled coverage feedback. Extracted
constraints are subsequently used to select stimuli that have
the highest probability of exercising targeted coverage holes.
By automatically biasing test selection and simplifying the
modelling process to a point where any binary classiﬁer can
be deployed, CDS reduces manual constraint writing while
improving on CDG’s manual encoding of knowledge into an
AI model.

Novelty-driven veriﬁcation [7] utilises unsupervised nov-
elty detection to identify novel stimuli from a large set of
constrained random tests, then prioritises them for simula-
tion. Similar to CDS, novelty-driven veriﬁcation (NDV) is
ideal for veriﬁcation environments in which test generation is
signiﬁcantly cheaper than simulation. Unlike CDS, the NDV
approach does not build a model relating functional coverage
to test stimuli. Instead, an unsupervised model of similarity
is constructed from previously simulated tests. The model
is used to rank new simulation candidates, with preference
given to the most dissimilar – i.e., novel – tests. The focus
of NDV is to determine the order in which to simulate test
stimuli such that functional coverage progress is accelerated
due to redundancy reduction. Efﬁciency is realised in the form
of reduced simulation, as high coverage is achieved using
fewer test stimuli. However, the efﬁcacy of NDV tests depends
on the efﬁcacy of the test biasing mechanism. This issue
arises because of NDV’s usage of novelty as a proxy for
effectiveness: it is possible for stimuli to be novel, but not
effective at targeting coverage holes. In light of this issue,
CDS has the potential to enhance the efﬁcacy of the NDV
approach.

CDG and NDV have evolved separately in the literature
to become the two most prominent categories of intelligent
testing methods within simulation-based veriﬁcation. We show
that treating CDS and NDV as complementary methods, as
suggested in [8], has the potential to increase both the efﬁcacy
of stimuli, and overall veriﬁcation efﬁciency. This results in
higher veriﬁcation productivity, accelerated coverage closure,
and more time for veriﬁcation engineers to hunt bugs in
unexplored parts of the DUT. We combine CDS with NDV to
create two types of hybrid intelligent testing frameworks. The
ﬁrst hybrid framework performs CDS and NDV sequentially
at different stages of the veriﬁcation process. This sequential
mode of operation simulates the union of CDS and NDV
test stimuli. The second hybrid framework performs CDS and
NDV in parallel during the ﬁnal stages of the veriﬁcation pro-
cess. This parallel mode of operation simulates the intersection
of CDS and NDV stimuli. We refer to the novel intelligent

Fig. 1: Coverage-Directed Test Selection

testing approaches as the Uniﬁed Hybrid Approach, and the
Intersected Hybrid Approach, respectively. To the best of our
knowledge, this has not been done before.

This paper is structured as follows. Section II explains the
objectives of intelligent testing, and then introduces formalised
CDS and NDV theory. CDS and NDV are introduced as
part of a hybrid intelligent
testing framework in Section
III, where the Uniﬁed Hybrid Approach and the Intersected
Hybrid Approach are presented in detail. In Section IV, the
DUT is presented, along with experiment information. The
experimental results are presented in Section V. Section VI
concludes the paper by discussing implications and limitations
of the research, along with planned future research.

II. INTELLIGENT TESTING

When it comes to intelligent testing, veriﬁcation engineers
might aim to achieve two objectives aimed at exercising
functionality of interest, as deﬁned by the veriﬁcation plan
and the coverage model. The ﬁrst objective is to identify and
prioritise tests likely to trigger previously unexplored DUT
functionality prior to simulation. Identifying and simulating
diverse and dissimilar test stimuli is likely to result in wider
areas of DUT logic being triggered. The wider the spread
of DUT logic triggered, the higher the chances of exposing
hidden bugs and exercising new coverage. The redundancy
inherent to constrained random testing is reduced because
novel stimuli are less likely to repeatedly trigger the same
DUT logic. Therefore, novel test prioritisation leads to redun-
dancy reduction which, in turn, leads to efﬁciency gains during
simulation as fewer tests are simulated and high coverage is
achieved.

The second objective is to estimate, before simulation,
the probability that a newly generated test will exercise one
or more coverage holes by analysing data from previous
simulations. Estimating this probability may enable us to
predict which tests are most likely to exercise coverage holes
before simulating them, and it provides us with a measure
of how conﬁdent we can be about the prediction. Selecting
and simulating tests that have a high probability of exercising
coverage holes can boost coverage progress while increasing
the likelihood of discovering bugs early. Testing is more
effective because stimuli that exercise interesting and hard-
to-reach functionality are given priority. A good indication of

DriversCheckersCoverageTestbenchDUTMonitorCode CoverageCoverage FeedbackFunctional CoverageBug FrequencyBug RateBug DataCoverage DataCoverage-Directed Test SelectionTest GeneratorTargeted ConstraintsEffective Test StimuliOutput SpaceInput SpaceSimulationeffective testing is a shifting of the coverage curve upwards
and leftwards, meaning that higher coverage is being achieved
quicker. When the horizontal axis represents the number of
tests simulated, such a shift of the coverage curve is analogous
to that exhibited when higher coverage is achieved with a
smaller number of tests. The reduction in the number of
simulated tests is therefore a suitable metric for both efﬁciency
and efﬁcacy experiments.

A. Coverage-Directed Test Selection

Coverage-Directed Test Selection (CDS) is a method de-
vised to automate the feedback loop from coverage analysis
to test selection, depicted in Figure 1. It is adapted from
feedback-based CDG [3], whose main goal
is to produce
constraints that automatically bias the test generator based on
coverage feedback. The CDS engine uses supervised learning
to model the relationship between coverage and test stimuli.
Constraints are produced to automatically bias test selection
towards stimuli that have the highest probability of exercising
coverage holes. From this perspective, CDS automates the
veriﬁcation engineer’s manual work of writing constraints by
hand. CDS is ﬂexible enough to be used with any binary
classiﬁer with minimal tuning, and can easily be extended to
test generation problems. This reduces the effort required to
encode domain knowledge into an AI model, and renders the
method useful in contexts beyond test selection (although such
contexts are beyond the scope of this paper).

More formally, let T denote the set of previously simulated
test stimuli derived from a constrained random test generator.
For each test ti ∈ T , there exists a set Xi of k constraints
Xi = {x1, x2, . . . , xk} that generated the test. Moreover, let
C represent the complete coverage model. After simulation of
T , each coverage point c ∈ C is assigned a binary value in
[0, 1] depending on whether c has not, or has, been exercised,
respectively. Suppose that intelligent testing begins when we
derive a set of simulation candidates T (cid:48) from the test generator.
The ﬁrst task of CDS is to estimate the conditional prob-
ability P , of exercising a target coverage point cj ∈ C, by
generating a new test tj, using a new set of constraints Xj.
This conditional probability is written as shown in (1) below.

P (cj|Xj, tj)

(1)

To make the coverage space more manageable, the coverage
model, C, is partitioned [9] into a set, G, of m non-overlapping
coverage groups, as shown in (2).

G = {g1, g2, . . . , gm} = C

(2)

Coverage groups are based on information from the veriﬁca-
tion plan. They enable model training and coverage analysis to
be performed at the coverage group level. After simulation, the
coverage groups G are inspected to ﬁnd target coverage groups
that have been exercised by some minimum number of test
stimuli, but still contain coverage holes. Each target coverage
group, gj, has its n exercising stimuli, Tpos ∈ T , isolated and
used as positive examples for training the group’s classiﬁcation
model. For negative examples, n stimuli, Tneg ∈ T , are

randomly sampled from the tests that did not exercise any
coverage within gj. We train a supervised classiﬁcation model
for each target coverage group gj using tests ti in Tpos + Tneg
as training examples, constraints Xi as training features for
each ti, and labels [0, 1] for each ti depending on whether or
not it triggered coverage in gj.

The ﬁnal CDS task is to classify a new test tj as one of
two classes Y = [1, 0] depending on whether or not we expect
coverage holes in target coverage group gj to be exercised by
tj, generated using constraints Xj. Applying a threshold, (cid:15),
enables us to derive classes based on the computed conditional
probability. A threshold of 0.5, for example, means that if
P (cj|Xj, tj) > 0.5 then we predict y = 1. Otherwise, we
predict y = 0.

CDS has three major limitations. First, it is very difﬁcult
to learn a relationship between coverage and test stimuli. This
is attributable to many reasons. Direct and well-understood
relationships tend to be difﬁcult to learn due the difference
in abstraction levels between the coverage and test spaces.
For example, in processor veriﬁcation, test stimuli might be
deﬁned at the architectural level and generated in assembly
language. Coverage tasks, on the other hand, are usually
deﬁned as cross-products [9] based on micro-architectural
requirements. Abstracting test stimuli further by represent-
ing them as tuples or vectors mitigates the issue, but the
difﬁculty of learning meaningful relationships still persists.
Alternatively, the relationship might not be learnable because
the function to be approximated is too complex. The lack of
positive training examples [10] is also problematic in CDS be-
cause, by aiming to exercise hard-to-reach and yet-uncovered
DUT functionality, we are trying to make predictions about
events that have occurred rarely, if at all, in the past.

The second limitation of CDS is that it directly relies on
learning from simulation traces. Simulation is expensive in
terms of computation and simulation time, and selecting tests
that exercise the hardest-to-reach coverage tasks might require
at
tens of thousands of simulation cycles to collect
adequate data to learn from. As observed in [11], learning from
coverage feedback in such circumstances leads to resource
inefﬁciencies linked to obtaining the required data.

least

The third limitation of CDS arises from lack of a suitable
distance metric. When targeting yet-unexercised coverage for
which useful data are unavailable, an alternative approach is
to relax the problem and learn from coverage points deemed
‘similar’ to the unexercised coverage. This approach imme-
diately raises challenges because a suitable distance metric
(which clearly deﬁnes ‘similarity’ in the coverage space) is
unavailable in simulation-based veriﬁcation. The traditional,
syntax-based distance metrics such as Euclidean and Hamming
are inherently limited in this context. While various approx-
imations have been devised such as in [12], the lack of a
semantically meaningful distance metric continues to be an
issue in intelligent testing methodology.

We will now show that NDV, which we examine below, is
capable of addressing some of the limitations associated with
CDS.

negative φtj is. The more negative φtj is, the more novel tj
is deemed to be relative to T .

The second task of NDV is to take each test tj’s dissimilar-
ity score φtj and use it to rank test stimuli based on decreasing
dissimilarity, such that test tjA is selected for simulation before
tjB if φtjA < φtjB . NDV can be extended to classify tj as
one of two classes Y = [−1, 1] depending on whether tj
is estimated to be novel or normal, respectively. The novelty
threshold θ enables us to classify tj as being novel if φtj < θ.
Otherwise, tj is classed as being normal.

NDV is computationally advantageous because it operates
directly on test stimuli in the input space, without recourse
to coverage data. NDV is unsupervised, simpler, and more
‘lightweight’ than CDS. Reducing the number of simulation
candidates prior to simulation in a lightweight, unsupervised
manner is extremely powerful when generation is cheap and
simulation is expensive. Through NDV, we gain the ability to
generate as many tests as our resources permit, but prioritise
the most promising subset of those tests for simulation.

The main limitation of standalone NDV methods is that, by
not being able to relate exercised coverage to their triggering
test stimuli, and by solely relying on the stimuli data set
to train an unsupervised model, the method likely loses a
potentially valuable source of information. CDS can work in
conjunction with NDV to mitigate this weakness by provid-
ing a coverage feedback component. However, the fusion of
methods such as CDS and NDV in the literature is unexplored.
Hence, we will now present a hybrid approach to intelligent
testing incorporating both CDS and NDV.

III. HYBRID INTELLIGENT TESTING FRAMEWORK

CDS methods automatically produce targeted constraints
and effective tests speciﬁcally targeted at coverage closure, but
suffer from issues largely stemming from supervised function
approximation on simulation traces. On the other hand, NDV
methods reduce redundancy without relying on supervised
function approximation, but potentially lose useful information
by not analysing coverage feedback. This suggests that CDS
limitations can be addressed by NDV strengths, and vice-versa.
Employing both CDS and NDV in a hybrid intelligent test-
ing framework harnesses the strengths, while simultaneously
limiting the drawbacks, of both CDS and NDV methods. An
example of hybrid intelligent testing encompassing both CDS
and NDV is depicted in Figure 3, where it is contrasted to
‘traditional’ constrained random testing.

Fig. 2: Novelty-Driven Veriﬁcation

B. Novelty-Driven Veriﬁcation

Novelty-Driven Veriﬁcation (NDV) methods [7], [8] use
novelty detection to identify and prioritise potentially useful
tests for simulation. Our main assumption when applying
NDV in simulation-based veriﬁcation is as follows: tests that
in the input space are more likely to exercise
are novel
novel DUT functionality, and correspondingly lead to novel
coverage in the output space. NDV leads to a smaller set of
test stimuli being simulated without the coverage suffering
a corresponding reduction. The NDV process is depicted
in Figure 2. Similar to CDS, NDV requires that a stimuli
generation source such as a constrained random test generator
is available to create a large set of test stimuli as simulation
candidates. The learning component operates on tests that have
previously been simulated, by learning a model of similarity
for these tests. When presented with newly generated test
stimuli, the model ranks them based on their dissimilarity to
previous stimuli. By identifying tests that are most dissimilar
to those already simulated, then giving preference to those tests
for simulation, there is a higher chance of exercising new DUT
functionality and accelerating coverage progress.

The ﬁrst task of NDV is to estimate a dissimilarity score,
φtj , for each newly generated test tj ∈ T (cid:48), relative to T . In
this paper, this task is performed by training the One-Class
Support Vector Machine (OCSVM) [13] unsupervised novelty
detection algorithm using the set of previously simulated test
stimuli T .

The novelty detection algorithm builds a model of similarity
Z, deﬁned by a novelty threshold z(T ) = θ, where θ is a real
number representing the decision boundary. The dissimilarity
score φtj , for a given new test tj, is computed as the signed
distance of tj from the decision boundary, such that

φtj =

(cid:32)

(cid:88)

ti∈T

(cid:33)

αtiL(tj, T ) − θ

∈ (−∞, +∞)

(3)

A. Uniﬁed Hybrid Approach

In (3), αti
is a parameter that determines the importance
of each ti ∈ T in the model, and L is a kernel function
that deﬁnes the distance between tj and T . If |αti| > 0,
then the test ti is considered a support vector, meaning that
it is considered in the model calculation. If |αti| = 0, ti
is considered a non-support vector and ignored. In general,
the further away tj is from the decision boundary, the more

The uniﬁed hybrid approach to intelligent testing combines
CDS and NDV in sequential order. When intelligent testing
begins, only one of the methods is operating on stimuli at any
given time. The uniﬁcation procedure is as follows:

1) Initialise simulation-based veriﬁcation with traditional
constrained random testing. Beginning with constrained
random testing potentially exercises the DUT with a

DriversCheckersCoverageTestbenchDUTMonitorCode CoverageEfficient StimuliCoverage FeedbackFunctional CoverageBug FrequencyBug RateBug DataCoverage DataNovelty-Driven VerificationTest GeneratorConstraintsVerification EngineerOutput SpaceInput SpaceSimulationConstrained Random Stimuli(a) Traditional Testing

(b) Intelligent Testing

Fig. 3: Traditional testing vs intelligent testing in simulation-based veriﬁcation

wide variety of randomly ordered tests, thereby increas-
ing the likelihood of discovering hidden bugs that might
not be easily found when targeting coverage.

2) Switch from constrained random to intelligent testing
to exercise coverage points of intermediate difﬁculty.
Intelligent testing can be activated at a pre-determined
point (e.g., when some level of functional coverage has
been achieved), or when a given number of random
stimuli have been simulated), or it can be activated
dynamically (such as when functional coverage has not
increased for a given number of iterations). The decision
regarding which of the intelligent testing methods to ac-
tivate ﬁrst depends on the context in which the methods
are being used. A good rule-of-thumb is to begin with
NDV because it is unsupervised and requires less data
to be operational.

3) Increase chances of exercising yet-uncovered coverage
points by switching from one method of intelligent
testing to the other. Step 3 is performed using CDS if
NDV was used for step 2, and vice-versa. Similar to
step 2 above, this switch can also be pre-determined
or dynamic. Switching the intelligent testing method is
crucial for step 3 because it ‘refreshes’ the testing dy-
namic to one that has not been previously encountered,
raising the chances of new coverage being exercised.
For example, if NDV was used in step 2, coverage
feedback was not taken into account. Switching from
NDV to CDS means that coverage feedback starts being
considered when selecting test stimuli, which inevitably
changes the tests that will be simulated and increases the
chances that new coverage will be exercised. The same
reasoning works when CDS is used for step 2, and NDV
for step 3. While the decision regarding which method
to use in this step depends on context, a good rule-of-
thumb is to use NDV in step 2 to gather as much data
as possible for CDS supervised learning in step 3.

B. Intersected Hybrid Approach

The intersected hybrid approach to intelligent testing com-
bines CDS and NDV in parallel. When intelligent testing
begins, both methods operate concurrently. The intersection
procedure is as follows:

1) Utilise constrained random testing during initial testing

iterations.

2) Switch from constrained random to intelligent testing.
In each iteration, simulate test stimuli that meet both
CDS and NDV criterion – i.e., the intersection of CDS
and NDV. The resulting test stimuli depend on the order
of the intelligent testing operations, of which there are
two possible conﬁgurations: (i) CDS-First, or (ii) NDV-
First. The CDS-First pipeline ﬁrst uses CDS to select
stimuli that have the highest probability of exercising
target coverage. The selected stimuli become inputs to
a NDV process that selects the highest novelty scoring
tests for simulation. An example of a CDS-First pipeline
iteration is shown in Figure 3b. In a NDV-First pipeline,
the order is reversed, with NDV being performed ﬁrst,
and the output of NDV being the input of CDS.

IV. EXPERIMENTAL SETUP

We evaluated our uniﬁed approach to intelligent testing
on an Inﬁneon Technologies’ Radar Signal Processing Unit
(RSPU). The RSPU is a large, complex and highly conﬁg-
urable IP supporting the Advanced Driver Assistance Systems
(ADAS) in Inﬁneon’s AURIX family of microcontrollers. The
RSPU was previously used as the DUT in similar work that
investigated an autoencoder-based approach [7], and a one-
class support vector machine-based approach [14] to NDV.
However, it is worth noting that the coverage model and the
training data used in this paper were different from those used
in previous NDV work. We used a different coverage model
largely containing hard-to-exercise whitebox coverage points.
Our training data were subjected to less domain expertise-
based feature engineering – aside from encoding of categori-
cals and binning of large ﬁelds to reduce their cardinality – to
test the efﬁcacy of this approach on data with minimal feature
engineering. Therefore, the results can reasonably be expected
to be different from those derived in previous NDV work.

Each test is represented as a tuple of approximately 300
ﬁelds. The metric we use to assess the success of these
experiments is functional coverage. We use a functional cov-
erage model containing about 6,000 whitebox cross-product
coverage points. We frame CDS as a process of producing
constraints that maximise our chances of selecting effective

DriversCheckersTest GeneratorCoverageTestbenchDUTMonitorCode CoverageOutput SpaceConstrained Random Test StimuliInput SpaceFunctional CoverageBug FrequencyBug RateSimulationBug DataCoverage DataVerification EngineerConstraintsDriversCheckersCoverageTestbenchDUTMonitorCode CoverageEfficient + Effective Test StimuliCoverage FeedbackCHEAPFunctional CoverageBug FrequencyBug RateEXPENSIVEBug DataCoverage DataCoverage-Directed Test SelectionNovelty-DrivenVerificationTest GeneratorTargeted ConstraintsEffective Test Stimulitests from the test database. With CDS, we learn which
constraints have the highest probability of biasing new tests to
exercise target coverage, then select the next iteration’s tests
based on their ability to satisfy learnt constraints. We frame
NDV as a process of prioritising novel tests for simulation by
ranking test stimuli in the test database and simulating them in
order of decreasing dissimilarity. With NDV, we learn which
tests have the highest novelty score (relative to previously
simulated tests), then select the next iteration’s tests in order
of decreasing novelty.

Achieving 100% functional coverage in real world veriﬁ-
cation requires approximately 2 million constrained random
test stimuli. Because simulating each test takes an average
of 2 hours, roughly 1,000 machines and EDA licences are
consumed nearly continuously over the course of the RSPU’s
6-month veriﬁcation project. Additionally, several months of
effort are spent by veriﬁcation engineers writing constraints
to target coverage holes. To enable rapid experimentation, a
database was created to store each test and its coverage. A
ranked regression of approximately 3,000 stimuli were found
to enable coverage closure. These 3,000 stimuli and their
coverage were added to the database for experimentation,
along with 83,000 other constrained random test stimuli. This
experimentation database enabled us to mimick simulation by
querying for the coverage exercised by each simulated test, and
reduced the total time required to run complete experiments
from weeks to hours.

The NDV approach we use in this paper is similar to that
employed by [14]. After initially sampling and simulating a
number of constrained random tests, those tests are used to
batch train a one-class support vector machine (OCSVM) [13]
novelty detector. A dissimilarity score is subsequently obtained
for all the remaining unsimulated tests. The unsimulated tests
are then ranked for simulation according to dissimilarity score,
with the most dissimilar tests being prioritised for simulation
ﬁrst.

Our CDS approach is implemented using a decision tree
[15] algorithm. We adopt a rule-based approach to learning
the relationships between test generation and coverage. As
previously discussed, an immediate problem with applying
CDS in practice is the lack of positive samples. By def-
inition, we are trying to make predictions for events that
are rare or have never happened. In other words, we cannot
directly learn anything about coverage holes because there
are no data to learn from. The approach we take to solve
this problem is inspired by [4], who proposed clustering the
coverage space ﬁrst. Because the clustering led to intuitive
coverage groupings, it became possible to target a coverage
hole in a given coverage group by tracing tests which exercise
coverage points deemed close to the hole; inductively mining
interpretable rules that describe those tests; and using the rules
as constraints for generating new tests with the aim of covering
the targeted hole. The lack of a well-deﬁned distance metric
also becomes apparent in CDS practice. The coverage space
clustering approach in [4] seeks to learn from tests which
have previously exercised coverage points deemed close to the

hole. However, it is difﬁcult to determine ‘closeness’ without
a well-deﬁned distance metric. Coverage hole analysis [9]
suggests a user-deﬁned partitioning of the coverage model
instead. Partitioning divides the coverage space into manage-
able partitions based on readily available information, such
as from the veriﬁcation plan, in which coverage events tend
to be grouped together based on shared traits such as their
triggering DUT behaviour. By partitioning the coverage space
apriori, we derive meaningful coverage groups that enable
us to ﬁnd and target coverage holes, isolate stimuli which
previously exercised neighbouring coverage points (within the
same partition), and learn common rules that can be used as
constraints for targeting those coverage holes. Partitioning also
happens to be beneﬁcial because mapping coverage groups to
triggering stimuli requires less effort than attempting to map
individual coverage points directly to generation constraints.
The rules induced by the decision tree during during this map-
ping exercise can also be interpreted by veriﬁcation engineers
if required, making them useful in situations where constraint
transparency is required.

V. RESULTS

Hybrid intelligent testing experiments compared the per-
formance of constrained random testing, CDS, NDV, against
the Uniﬁed Hybrid Approach and the Intersected Hybrid
Approach. The metric used to compare performance is the
number of tests required by each method to achieve given
levels of functional coverage. Because simulation is expensive
in this context, reducing the number of tests simulated without
sacriﬁcing coverage is a desirable goal. The lower the number
of tests required to reach a given functional coverage level,
the more effective and efﬁcient the intelligent testing method
is considered to be.

A. Standalone CDS and NDV

Table I shows the number of tests required to reach 95%,
98%, and 99% functional coverage on the RSPU when com-
paring random selection to CDS and NDV as standalone
methods. We regard 99% as the most
important of these
functional coverage levels because, at that level, we reason
that a veriﬁcation engineer can intervene and manually close
coverage on the remaining 60 coverage points. Random se-
lection is the performance baseline against which the other
selection methods are compared. The results show that NDV
leads to the lowest number of simulations when compared to
random selection, managing to save almost 16% of tests at
99% functional coverage. CDS required 6% less tests than
random selection to reach 99% functional coverage. Although
better than random, CDS savings were lower than NDV across
all three functional coverage levels. There are two possible
reasons for CDS underperformance: a lack of data to learn
from (particularly during earlier iterations), and underﬁtting by
the decision tree classiﬁer. Both of the issues leading to CDS
underperformance are addressed by the hybrid approaches.

B. Uniﬁed Hybrid Approach

Table II shows the number of tests required to reach
different levels of functional coverage on the RSPU during
uniﬁed hybrid approach (UHA) experiments. The table groups
the results according to the intelligent test selection method
performed ﬁrst. To be clear, random selection initiates the
veriﬁcation process, and the switch from random selection
to intelligent
test selection occurs at 90%. Stating that a
method was performed ﬁrst during UHA experiments means
it was performed when functional coverage was between 90%
and 98%. The last method to be performed as part of UHA
experiments is performed between 98% and 99% functional
coverage.

The highest savings across all functional coverage levels
were achieved when NDV was performed ﬁrst during UHA
experiments. The NDV-First UHA achieved 99% functional
coverage with 17% less tests than random selection, and with
almost 2% less tests than standalone NDV. Such a result is
to be expected due to two reasons. The ﬁrst reason is that
the performance of the NDV-First UHA at 95%, and at 98%,
will be similar to the performance of standalone NDV. The
second reason to expect this result is that it mitigates the
problem of lack of quality supervised training data associated
with CDS. NDV promotes test diversity, and the quantity of
training data is not as much of an issue for this unsupervised
method. As a result, NDV is extremely effective, even during
earlier iterations. When the switch from NDV to CDS occurs
at 98%, the classiﬁer immediately beneﬁts from the relatively
large quantities of data gathered during NDV. CDS-First UHA
results show that reversing this process will not yield signiﬁ-
cant simulation savings. Therefore, NDV should be performed
ﬁrst in a UHA pipeline, thereby enabling data gathering for
the ﬁnal stage when CDS is performed.

C. Intersected Hybrid Approach

Table III shows the number of tests required to reach
different levels of functional coverage on the RSPU during
intersected hybrid approach (IHA) experiments. The table
shows a high reduction in simulated tests regardless of the
intelligent testing method that is used in the pipeline ﬁrst. The
CDS-First intersected hybrid approach is better at reducing
simulations overall, reaching 99% functional coverage with
18% less tests than random selection, and 0.5% less tests
than the NDV-First IHA. At 99% functional coverage, NDV-
First IHA results show an improvement of 2% over standalone
NDV.

The results in Table III are a signiﬁcant improvement for the
standalone CDS method. This improvement can be attributed
to the integration of NDV into the test selection pipeline. The
decision tree classiﬁer used to perform CDS is constrained,
and it tends to underﬁt the data. Due to underﬁtting, the
classiﬁer can allocate the same high probability score to
several tests, and consequently return them all as possible
simulation candidates. During CDS-First IHA experiments,
these simulation candidates would be piped as inputs to
NDV. NDV allocates granular novelty scores to the simulation

Functional Coverage
95%
98%
99%
Savings (vs. Random)
95%
98%
99%

Random
12866
29300
44200

CDS
11770
27179
41458

NDV
9049
22132
37227

-8.52% -29.67%
-7.24% -24.46%
-15.78%
-6.2%

TABLE I: Number of tests required to reach given levels of
functional coverage – Random vs CDS and NDV

Functional Coverage
95%
98%
99%
Savings (vs. Random)
95%
98%
99%

CDS-First UHA
11948
27256
41871

NDV-First UHA
9294
22036
36601

-7.14%
-6.98%
-5.27%

-27.76%
-24.79%
-17.19%

TABLE II: Number of tests required to reach given levels of
functional coverage – Uniﬁed Hybrid Approach

Functional Coverage
95%
98%
99%
Savings (vs. Random)
95%
98%
99%

CDS-First IHA
9914
21848
36223

NDV-First IHA
9373
22038
36440

-22.94%
-25.43%
-18.05%

-27.15%
-24.78%
-17.56%

TABLE III: Number of tests required to reach given levels of
functional coverage – Intersected Hybrid Approach

candidates, making it highly unlikely that no two novelty
scores will be exactly the same. Novelty scores make it easy
to objectively select 1 simulation candidate per target coverage
group, instead of resorting to breaking ties randomly. In this
way, the issue of CDS underﬁtting is mitigated by performing
CDS followed by NDV in the same IHA pipeline.

The CDS-First IHA achieves lower simulation savings at
95% functional coverage (23%) compared to NDV-ﬁrst IHA
(27%). This result is also likely to be caused by the lack of
data typical in earlier stages of veriﬁcation, although such lack
of data seems to be mitigated by employing NDV as part of
the IHA pipeline. CDS-First IHA performs better than NDV-
First IHA when more data has been gathered, as evidenced
by the higher savings achieved by CDS-First IHA at 98% and
99% functional coverage. Training data availability is clearly
an important aspect to consider when deciding where to run
CDS in an intelligent testing pipeline.

VI. CONCLUSION

In this work, we have proposed incorporating coverage-
directed test selection and novelty-driven veriﬁcation methods
into a hybrid intelligent testing framework for simulation-
based veriﬁcation. We have shown that enhancing constrained
random test generation, through joint use of coverage-directed

[2] S. Russell and P. Norvig, Artiﬁcial Intelligence: A Modern Approach,

3rd ed. Prentice Hall, 2010.

[3] S. Fine and A. Ziv, “Coverage directed test generation for functional
veriﬁcation using Bayesian networks,” in Proceedings of the 40th Design
Automation Conference, 2003, pp. 286 – 291.

[4] K. Eder, P. Flach, and H. Hsueh, “Towards automating simulation-
based design veriﬁcation using ILP,” in Inductive Logic Programming,
S. Muggleton, R. Otero, and A. Tamaddoni-Nezhad, Eds.
Berlin,
Heidelberg: Springer Berlin Heidelberg, 2007, pp. 154–168.

[5] C. Ioannides and K. I. Eder, “Coverage-directed test generation au-
tomated by machine learning – a review,” ACM Trans. Des. Autom.
Electron. Syst, vol. 17, no. 7, 2012.

S.

and

close

[6] N. Masamba, K. Eder, and T. Blackmore, “Supervised learning for
coverage-directed test selection in simulation-based veriﬁcation,” 2022.
[Online]. Available: https://arxiv.org/abs/2205.08524
Schaal,

[7] T. Blackmore, R. Hodson,

“Novelty-driven
stimuli
the
2021 Design
[Online]. Available:

coverage,”

in Proceedings

learning to identify novel
veriﬁcation: Using machine
of
and
and Veriﬁcation Conference (Virtual), 2021.
https://www.dropbox.com/s/iulpk8kba3f7xxn/DVCon%20US%202021
Proceedings-FINAL.zip?dl=0&ﬁle subpath=%2FPapers%2F7060.pdf
[8] W. Chen, N. Sumikawa, L. C. Wang, J. Bhadra, X. Feng, and M. S.
Abadir, “Novel test detection to improve simulation efﬁciency — a
commercial experiment,” in 2012 IEEE/ACM International Conference
on Computer-Aided Design (ICCAD), Nov 2012, pp. 101–108.

[9] O. Lachish, E. Marcus, S. Ur, and A. Ziv, “Hole analysis for functional
coverage data,” in Proceedings 2002 Design Automation Conference
(IEEE Cat. No.02CH37324), June 2002, pp. 807–812.

[10] L.-C. Wang, Learning from Limited Data in VLSI CAD. Cham:
[Online].

International Publishing, 2019, pp. 375–399.

Springer
Available: https://doi.org/10.1007/978-3-030-04666-8 13

[11] R. Gal, E. Haber, and A. Ziv, “Using DNNs and smart sampling for
coverage closure acceleration,” in Proceedings of the 2020 ACM/IEEE
Workshop on Machine Learning for CAD, ser. MLCAD ’20. New
York, NY, USA: Association for Computing Machinery, 2020, p.
15–20. [Online]. Available: https://doi.org/10.1145/3380446.3430627

[12] D. Baras, L. Fournier, and A. Ziv, “Automatic boosting of cross-
product coverage using Bayesian networks,” in Proceedings of the 4th
International Haifa Veriﬁcation Conference on Hardware and Software:
Veriﬁcation and Testing, ser. HVC ’08. Berlin, Heidelberg: Springer-
Verlag, 2009, pp. 53–67.

[13] B. Sch¨olkopf, R. Williamson, A. Smola, J. Shawe-Taylor, and J. Platt,
“Support vector method for novelty detection,” in Proceedings of the
12th International Conference on Neural Information Processing Sys-
tems, ser. NIPS’99.
Cambridge, MA, USA: MIT Press, 1999, p.
582–588.

[14] X. Zheng, “Advanced stimulus for coverage closure,” Master’s thesis,

2019.

[15] L. Breiman, J. Friedman, R. Olshen, and C. Stone, Classiﬁcation and

Regression Trees. Wadsworth, 1984.

test selection and novelty-driven veriﬁcation, yields beneﬁts in
automation and productivity by reducing veriﬁcation resource
consumption and accelerating coverage progress. Coverage-
directed test selection biases test selection towards tests that
plug coverage holes. Novelty-driven veriﬁcation identiﬁes
novel test stimuli likely to exercise new functionality. Hy-
brid intelligent testing adds value through gains in testing
effectiveness, and increased resource efﬁciency attributable
to reducing test redundancy. These beneﬁts are achieved by
harnessing data which are already being gathered from the
current functional veriﬁcation workﬂow.

The best performing intelligent testing method, intersected
hybrid approach with coverage-directed selection being per-
formed ﬁrst, led to 99% functional coverage being achieved
with 18% less tests than random selection. At production-
scale RSPU veriﬁcation, this would translate to approximately
reducing simulation by 360,000 tests. This translates to 1.5
months of veriﬁcation resource savings. We still have multiple
ways of improving these intelligent testing methods, includ-
ing feature selection, feature engineering, and dimensionality
reduction. The results achieved by hybrid intelligent testing,
combined with the potential for improvement, make this an
encouraging research avenue to keep pursuing.

Both hybrid approaches were unable to signiﬁcantly im-
prove standalone novelty-driven veriﬁcation in these experi-
ments. This speaks to novelty-driven veriﬁcation’s ability to
reduce simulation and drive coverage progress, with little
tuning and computation. Novelty-driven veriﬁcation perfor-
mance is even more impressive considering the fact
that
coverage feedback is not taken into account. Coverage-directed
test selection beneﬁts more from the integration of novelty-
driven veriﬁcation, while the reverse is not necessarily true.
We can conclude that focusing on coverage holes is not
as good an accelerant of coverage progress as focusing on
test diversity. However, novelty-driven veriﬁcation does not
produce constraints (which might be required to be human-
readable by veriﬁcation engineers). It
to
estimate how novelty-driven veriﬁcation will perform apriori,
whereas metrics such as classiﬁcation accuracy could be used
to estimate the future performance of coverage-directed test
selection. We are currently conducting further research to
develop this line of reasoning: how can we correlate learning
metrics (e.g., accuracy, precision) to veriﬁcation metrics (e.g.
coverage, number of tests simulated)?

is also difﬁcult

Future research will focus on optimising the performance
of our hybrid intelligent
testing framework. We intend to
review the data used in the learning pipeline, and to tune the
hyperparameters of our models in order to increase learning
performance. We will also extend the techniques we used
in this paper to other classiﬁcation models such as neural
networks, and to other novelty detection models such as
isolation forests.

REFERENCES

[1] J. Bergeron, Writing Testbenches: Functional Veriﬁcation of HDL Mod-

els. Springer, 2003.

