Noname manuscript No.
(will be inserted by the editor)

Development Eﬀort Estimation in Free/Open Source
Software from Activity in Version Control Systems

Gregorio Robles · Andrea Capiluppi ·
Jesus M. Gonzalez-Barahona · Bj¨orn
Lundell · Jonas Gamalielsson

Received: date / Accepted: date

Abstract Eﬀort estimation models are a fundamental tool in software man-
agement, and used as a forecast for resources, constraints and costs associated
to software development. For Free/Open Source Software (FOSS) projects, ef-
fort estimation is especially complex: professional developers work alongside
occasional, volunteer developers, so the overall eﬀort (in person-months) be-
comes non-trivial to determine.

The objective of this work it to develop a simple eﬀort estimation model
for FOSS projects, based on the historic data of developers’ eﬀort. The model
is fed with direct developer feedback to ensure its accuracy.

After extracting the personal development proﬁles of several thousands of
developers from 6 large FOSS projects, we asked them to ﬁll in a questionnaire
to determine if they should be considered as full-time developers in the project
that they work in. Their feedback was used to ﬁne-tune the value of an eﬀort
threshold, above which developers can be considered as full-time.

With the help of the over 1,000 questionnaires received, we were able to
determine, for every project in our sample, the threshold of commits that
separates full-time from non-full-time developers.We ﬁnally oﬀer guidelines
and a tool to apply our model to FOSS projects that use a version control
system.

G. Robles, J.M. Gonzalez-Barahona
Department of Telematic and Computational Systems Engineering
Universidad Rey Juan Carlos, Spain
E-mail: gregorio.robles@urjc.es, jesus.gonzalez.barahona@urjc.es

A. Capiluppi
Department of Computer Science
University of Groningen, The Netherlands
E-mail: a.capiluppi@rug.nl

B. Lundell, J. Gamalielsson
School of Informatics
University of Sk¨ovde, Sweden
E-mail: bjorn.lundell@his.se, jonas.gamalielsson@his.se

2
2
0
2

r
a

M
8
1

]
E
S
.
s
c
[

1
v
8
9
8
9
0
.
3
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Gregorio Robles et al.

Keywords Eﬀort estimation · open source · free software · mining software
repositories · versioning system · commits

1 Introduction

Eﬀort estimation models are invaluable tools in software management: they
help gaining insights on past resources and associated costs; and they serve
as models to forecast resource demand and allocation, as well as dealing with
predicted constraints. Traditionally, eﬀort estimation is generally used by com-
panies in the earlier stages of a software project to estimate the number of
developers, and the amount of time that it will require to develop a software.
In the context of Free/Open Source Software (FOSS) development, there
has hardly been a formal way of tracking the eﬀort of developers, especially
in its original volunteering form. It was argued that, in some circumstances,
small-team development could eventually evolve [12] into a distributed devel-
opment (e.g., ‘bazaar’). In such cases, eﬀort is even more complex to track,
since the level of contributions varies along an onion-type model [16], where
the outer the layer the smaller the code contribution, but the larger the user
base, and the bug reporting facility. The inner the layer, the more pronounced
the development eﬀort.

This volunteer-based model for FOSS development was later complicated
by the participation of companies, alongside volunteers [44]. Companies de-
voted staﬀ and eﬀort into a FOSS project to enter a saturated market, or to
gain a foothold in a specialised development, without deploying vast amounts
of resources and recreate a competitor product from scratch [20]. This hybrid
model introduces further complexity in the eﬀort estimation modeling: if the
volunteers can still be modeled alongside the basic onion-model, the involve-
ment of external companies and their focused eﬀort become quite challenging
to describe and model.

The eﬀort estimation model presented depends on the threshold value of
a FOSS project, and on the analysis of its publicly available data on version
control systems (VCSs), such as git. Other projects can easily create their
tailored model, and survey their developers to obtain their threshold value.
As a further contribution of this work, we propose the threshold values of six
large FOSS projects. These values can be re-used, compared, or validated with
other projects.

Given the ﬂuid nature of FOSS development, all the existing approaches to
FOSS eﬀort estimation are currently missing two pieces of crucial information.
The ﬁrst is a precise measure of past eﬀort of the developers, even in the more
sophisticated models. In traditional eﬀort estimation models, software projects
are able to estimate future eﬀort, because they know their past eﬀort. Given
the increasing involvement of companies in FOSS development, this aspect is
fundamental to (i) allow potential new participant companies to evaluate the
current developers eﬀort, and (ii) ﬁne tune their participatory eﬀort in the
FOSS projects.

Development Eﬀort Estimation from Activity in Version Control Systems

3

The second piece of crucial information that is missing is some sort of
validation by the developers themselves. Assigning developers to the wrong
tiers, or using a too wide (or too small) time window to evaluate the commits
activity, produces diﬀerent values of estimated eﬀort.

For this purpose, in this paper we surveyed over 1,000 developers working
on 6 large FOSS projects, with the aim to count how many developers consider
themselves as ‘full-timers’ in each project. The number of commits (in a six
months’ timespan) of these self-identiﬁed full-time developers was later used
as the threshold θ that minimises the error to diﬀerentiate full-timers from
other contributors. We used that threshold θ to estimate the eﬀort produced
by developers in each of the analysed FOSS projects. The contribution of this
work is, to the knowledge of the authors, the ﬁrst developer-validated model
to FOSS eﬀort estimation available in literature.

The present work is an extension of a previously published paper [46].
Its Future Work section posited that: “We envisage to expand this study by
1) studying other FOSS projects to ascertain if our method is applicable in
general, and if it is to what extent; 2) performing a scientiﬁc experiment to
obtain margins of error for the estimation of error for full-time developers
(...)”.

The current paper should therefore be considered as the enactment of the
future work proposed in [46], and provides the following speciﬁc contributions:

– We provide analysis and results for ﬁve more industrial FOSS projects.
– We oﬀer an extension of a previous SLR on eﬀort estimation including

papers from 2016-2020.

– We clarify the model, oﬀering an example of how it can be used in practice.
– We further develop on the concepts of goodness and compensation
– We discuss about the estimation error of the model.
– We discuss the repercussions for practitioners.

This paper is articulated as follows: Section 2 introduces the vision of this
research. Section 3 deals with related work, especially focusing on the eﬀort
estimation models built for the various generations of FOSS (e.g., fully vol-
unteered projects, hybrid FOSS projects, company-led FOSS projects). Sec-
tion 4 introduces the model for eﬀort estimation that we propose, alongside
the terms and deﬁnitions we used throughout this study; Section 5 introduces
the characteristics of the analysed projects, while Section 6 describes the sur-
vey that was circulated to the developers of the FOSS projects, and how the
threshold θ was evaluated for each project. Section 7 shows how the model
is deployed for the OpenStack project, while Section 8 proposes further repli-
cation, when considering ﬁve other projects. Section 9 discusses the ﬁndings,
especially considering what sort of beneﬁts we envisage for FOSS communities
and companies. Section 10 ﬁnally concludes.

4

Gregorio Robles et al.

2 Vision of the research

The model that we propose below is based on an overarching vision, that is
set to maximise the applicability and reproducibility of our model in other
FOSS projects. As depicted in the ﬂowchart of Figure 2, the model requires
the parsing of a project’s VCS, and that is a relatively simple resource to have
an access to, with modern software engineering tooling. The model then uses
the responses of developers to a survey to establish a value of commits (θ)
above which developers should be considered as full-time.

Fig. 1 Flowchart to run our eﬀort estimation model in other FOSS projects.

Development Eﬀort Estimation from Activity in Version Control Systems

5

The second data source of our eﬀort estimation model (i.e., creating, dis-
tributing, collecting and analysing the developer surveys) can be time con-
suming and requires commitment from a FOSS project’s management board.
To combat this, our model has been built also for those projects that cannot
(or do not want) to directly and regularly survey their developers: the vision
of this research is to build enough evidence from several FOSS projects, and
to obtain threshold values for classes of similar projects. Those shared thresh-
olds can then be reused for other projects, thus removing the need to regularly
survey developers.

3 Related Research

Traditional eﬀort estimation models are generally used by organizations in the
early stages of a project to estimate total eﬀort, number of developers and time
(e.g., duration) needed to develop the software. Eﬀort estimation in traditional
commercial software has been approached by means of models, either top-down
(e.g., by analogy or by expert judgement) or bottom up (e.g., via regression
models using past data). In the area of eﬀort estimation, a comprehensive
and systematic literature review on eﬀort estimation was presented in 2007
in [24]. The most evident result was to observe that over half of the papers
surveyed are based on history-based evaluations to build models to estimate
future eﬀort.

Similarly to the systematic literature review (SLR) of 2007 [24], the re-
search works around eﬀort estimation (speciﬁcally for FOSS systems) were
extensively analysed in 2016 by Wu et al. [54]. The type of study proposed by
the authors, alongside the object of the predictions, were analysed and clus-
tered, as visible in the top part of Figure 2. Diﬀerent types of studies were
identiﬁed, based on the goal of the relative research (‘Guidelines and Mea-
surement’, ‘Measure individual contribution’, ‘Predict maintenance activity
resource’, ‘Predict direct eﬀort’ and ‘Predict indirect eﬀort’) and the approach
used in the research (‘Development of estimation method’, ‘Case study’, ‘Ex-
periment’, ‘History-based evaluation’, ‘Theory’ and ‘Comparison study’). The
largest set of research studies was found to be under the ‘Development of
estimation method’ to ‘predict maintenance activity resources’ (comparably
to [24]).

In order to include the more recent literature, we repeated the process pre-
sented in the 2016 SLR. We analysed the papers that were written between
2016 and 2020, using the same search string1, and the same search engines2.

1 As per the original paper: (“Open source” OR OSS OR FOSS OR FLOSS OR open-
source OR libre OR free) AND (maintain OR maintenance OR evolve OR evolution OR “bug
ﬁx*” OR “bug-ﬁxing” OR “defect ﬁxing” OR “defect correction” OR “defect resolution”
OR eﬀort OR cost OR estimat*, predict*) AND (empirical OR validation OR experiment
OR evaluation OR “case study”).

2 Inspec, Compendex, IEEE Xplore, ACM Digital Library, Science Direct and Google

Scholar.

6

Gregorio Robles et al.

We obtained 2,279 candidate studies from the database search (without dupli-
cates). After reading abstract and title, we obtained 41 articles; after reading
each, we gathered 16 studies that comply with the search criteria from the
original SLR. The labels of the SLR were applied to these papers, as visible
in the bottom part of Figure 2.

Similarly to the original study, the largest category was found again to be
the studies that predict maintenance activity resources: although most of the
papers design and/or deploy a new estimation method ([22,37,51,57], mostly
focused on bug-ﬁxing eﬀorts), we also found three studies that used history-
based evaluations to directly predict eﬀort [36, 43, 55].

Applicability of Traditional Eﬀort Estimation techniques to FOSS The FOSS
paradigm, at least in its original form, was deemed as diﬃcult, if not impos-
sible, to ﬁt under the most well-known estimation models (for instance the
COCOMO I or COCOMO II models [9,10]). Various research attempts have
showed that fundamental assumptions and constructs could not be applied to
the FOSS domain; whereas other aspects were simply absent from commercial
organizations [6,11,26,46].

More speciﬁcally, Amor et al. [6] proposed to measure the total eﬀort in-
vested in a project by characterizing all the activities of developers that can
be traced: commits, e-mails, bug activities, among others. Kalliamvakou et
al. [26] replicated the study and included the contribution of developer eﬀort.
Capiluppi et al. [11] determined the average hours worked per day by the
Linux kernel developers. They characterized the Linux kernel by the time of
day when the commits were observed in the repository and when the author
worked most frequently. They divided a working day into traditional oﬃce
hours (from 9am to 5pm), after oﬃce hours (5pm to 1am) and late hours (from
1am to 8am). The authors found that within the Linux kernel community the
eﬀort was constant throughout the week, which suggested the need for diﬀerent
estimation models from the ones traditionally used in industrial settings, where
the work schedule is presumed to be 9am-5pm, Monday to Friday.

A more general work to address the commit frequency was presented in [30].
Using the Ohloh.net repository, the authors evaluated the overall commit
frequency, and the average number of commits that developers featured on.
The same repository was also analysed to evaluate the size of a typical com-
mit [31], where the authors found power laws underlying the patterns of com-
mit size. Later on, the frequency of commits was analysed in [23] for two
Apache projects, POI and Tomcat. The authors also found that commits fol-
low power laws, with large bursts and long tails. Power laws were again found
in 4 Apache projects, when analysing the time interval between commits and
ﬁles committed [35].

Mockus et al. [40] showed that, out of nearly 400 programmers in the
Apache project, the 15 most productive ones contributed 88 percent of the to-
tal lines of code (LOC). They compared those 15 Apache developers with pro-
grammers in ﬁve other commercial projects. They deﬁned code productivity as
the mean number of lines of code per developer per year (KLOC/developer/year).

Development Eﬀort Estimation from Activity in Version Control Systems

7

Fig. 2 Topic of the SLR conducted in 2016 by [54] (top); additional SLR performed for
this study (bottom)

Koch [29,28] reported that in FOSS projects, the distribution of eﬀort be-
tween participants (programmers) is skewed as in Mockus et al. [39]. Indeed,
the majority of code was written and most of the eﬀort spent by just a few
contributors. This paper presents and illustrates the use of a similar approach,
when identifying who these ‘major contributors’ are, but instead of imposing
an a-priori threshold above which we consider a developer to be a ‘super’ or

8

Gregorio Robles et al.

a ‘major’ contributor (or even a hero [3]), we validate that threshold with the
help of the developer surveys. To the best of our knowledge, the identiﬁcation
of developer types in this way is novel.

Moulla et al. [41,42] applied the COCOMO model to the TRIADE (version
7a) FOSS project, using LOCs and regression models with COSMIC Function
Points as the independent variable [18,2]. They reported that in terms of eﬀort,
development of software based on FOSS has advantages over development from
scratch. Fernandez-Ramil et al. [19] used linear regression models to estimate
eﬀort and duration for FOSS projects with LOC as the independent variable.
Yu [56] also used linear regression models with LOC as the independent vari-
able to indirectly predict maintenance eﬀort in free and FOSS projects. Eﬀort
was measured by examining the LOC added, deleted and modiﬁed. Anbalagan
et al. [7] investigated predicting how much time developers spend in correc-
tive maintenance of FOSS projects. Their study focused on 72,482 bug reports
from over nine releases of Ubuntu, a Linux distribution system. They used a
linear regression model with ‘bug reports corrected by developer’ as the in-
dependent variable to predict the time taken to correct bugs. They observed
that for FOSS estimation maintenance eﬀort is lower than for proprietary
development due to higher code quality.

According to Capra et al. [13,14, 15], FOSS shows a slower growth of main-
tenance eﬀort over time. Studies by Koch et al. [29,27,28] on eﬀort modelling
and developer participation in FOSS projects show that the number of par-
ticipants, other than programmers, was about one order of magnitude larger
than the number of programmers. In summary, a number of studies are avail-
able on software eﬀort estimation but few discuss eﬀort and duration in FOSS
projects using linear regression models. The categorization of contributors and
the size and choice of the datasets also present a challenge for research on FOSS
projects estimation.

Eﬀort Estimation in hybrid FOSS systems The terminology around FOSS
started to change as long as volunteer communities began to collaborate with
industrial entities. One of the ﬁrst research studies that identiﬁed the possi-
bility of a mixed development approach was the seminal work by Lerner and
Tirole [34]. Various strategies were formulated, where commercial enterprises
could be proﬁtable using FOSS as a strategic asset.

A more formal description of the grades of involvement of commercial com-
panies into FOSS projects was formulated in two research works [48,14]. The
ﬁrst work was focused on the motivations of developers and how that was
aﬀected by the type of FOSS development. In the second work, and using
‘governance’ as the term of reference, Capra et al. produced various levels of
involvement, from volunteer-based, to company-driven FOSS projects.

Summary of past work Previous work on FOSS eﬀort estimation has been
typically run by detecting eﬀort using direct measurements (e.g., number of
developers) and applying proxies to those in order to derive a measurement
of eﬀort. In other cases, eﬀort was derived indirectly, by using various other

Development Eﬀort Estimation from Activity in Version Control Systems

9

Ref Type

Attributes

[56]

INDIRECT

lag time, source code changes

[5]

[1]

INDIRECT

INDIRECT

lag time, delta LOCs, delta
ﬁles
Bugs data

[4]
[29]

INDIRECT
DIRECT

Bugs data
Proxy of {lag time, LOCs}

[13]

DIRECT

[28]

DIRECT

[6]

DIRECT

Proxy of {lag time, LOCS,
methods}
Proxy of {active developers,
lag time, LOCS, methods}
Proxy of {LOCs,
emails, bug activity}

commits,

Validated w
developers
NO

NO

NO

NO
NO

NO

NO

[11]

DIRECT

[46]

DIRECT +
INDIRECT

Proxy of {active developers,
lag time, LOCs}
Proxy of {commits}, surveys

NO

YES

ours DIRECT +

Proxy of {commits}, surveys

YES

INDIRECT

Source
code

Process
based

People-
based
estimation

Activity-
based
estimation

People- &
activity-
based
People- &
activity-
based

Table 1 Summary of past works based on the use of direct and indirect measurements.
Highlighted are the characteristics of the present paper.

measurements, and inferring a relationship to the eﬀort devoted to a software
project. Table 3 summarises the past works in terms of how direct and indi-
rect measurements (and which ones) were used when deriving eﬀort estimation
models. As visible, the approach that we propose in our paper (and that has
been trialled in its earlier inception at [46]) is grounded in both direct and
indirect measurements. Furthermore, its results have been validated with de-
velopers’ responses: this alone is the one aspect that sets our model apart from
any other attempt at estimating the eﬀort produced by FOSS developers.

In general, eﬀort estimation models for FOSS projects are based on the data
that are collected in the various development activities; these data (especially
when they have large variance, or are heterogeneous) need transformation pa-
rameters (e.g., logarithmic) that might be needed to normalise a distribution,
or to take into account diﬀerent types of data for the same model. One ex-
ample of the latter would be devising a transformation parameter to consider
all the diﬀerent activities (emails, blogs, online discussions, etc., as well as the
actual code commits) that a developer is engaged into [6].

This type of models suﬀers from a couple of issues: ﬁrst, it is well known
that contributions to FOSS projects are not uniform. A few developers are
responsible for a major amount of the work, while a large number of developers

10

Gregorio Robles et al.

contribute with a comparatively small one [29, 39]. So, one of the problems
when measuring eﬀort of the overall FOSS development process consists in
translating the uneven nature of contributions [39] into a consistent estimation
model. Inferring the eﬀort devoted by regular and occasional contributors is
not easy, and in addition is a source of inaccuracy.

The second issue is that the conversion of that data to eﬀort is far from
simple, since it is not possible to exactly determine how much time or eﬀort the
developer actually spent on those activities. In general, the information we can
gather consists of points in time (i.e., timestamps) where speciﬁc actions have
occurred (e.g., when the commit was performed, when the e-mail was sent,
when a comment to a bug notiﬁcation was submitted), but not how long these
actions actually took (e.g., performing the changes in the commit, reading and
writing the e-mail, debugging the software to add information on the bug).

4 Empirical approach

In this section we introduce the concepts and terminology at its base (4.1),
and the two types of input (4.2) that the model uses to evaluate the developers
eﬀort. Using an example scenario, we show how to evaluate the threshold θ to
identify full-time developers (4.3).

In addition, we discuss how to estimate the error in our model (4.4), and
contribute the novel goodness performance measure (4.5) that helps in ﬁnding
the optimal θ. We conclude the section by evaluating the total estimated eﬀort
of the example scenario, as well as the relative estimation error (4.6).

4.1 Concepts and Terminology

Our model is build on top of following concepts that are well known in the
Software Engineering research literature:

– Commit: action by which a developer synchronizes a set of changes to the
versioning system repository. A commit is given by a point in time (the
timestamp can be obtained from its metadata).

– Author (or developer, or contributor): individual who contributes
the changes, but may not be the one who performs the commit (which is
done by the committer).

– Committer: developer who actually performs the commit to the reposi-
tory, but may have not been the real author of the changes. In our model,
we do not consider committers, since it would decrease its accuracy.

– Full-time developer: Developer who devotes the relative amount of time
of a 40 hour-week to the project3. In some research literature on FOSS,
full-time developers are called “professional” developers as compared to

3 In some countries there is a diﬀerent work week, e.g., in the UK it is 35 hours. The

model presented can be adjusted if needed.

Development Eﬀort Estimation from Activity in Version Control Systems

11

volunteers [49,50,52]. In this paper, we prefer full-time and non-full-time
developer, as the latter can be professionals as well, just with a minor
participation in the project.

– Person-Month: measure of eﬀort. A person-month is observed when a

full-time developer devotes 1 month of eﬀort to the project.

In addition, for our model we use the following terminology:

– Period of study n: timespan (in months) during which the activity (i.e.,

commits) by developers is aggregated.

– Threshold θ: minimum amount of activity (i.e., commits) during the pe-
riod of study by a developer to be considered as a full-time developer.
– Eﬀort Ed in a period of study (by developer d ): any developer who
performs more commits than θ in a period of study of n months will be
considered as full-time with n person-months. A developer who committed
a commits (with a < θ), will be assigned an eﬀort of n ∗ a
θ person-months.
– Eﬀort En in a period of study n (for the whole project): The overall
eﬀort En is evaluated by summing up the eﬀort in a period of study of all
developers (full-time and non-full-time). The formula to obtain the overall
eﬀort is as follows:

En =

df t
(cid:88)

d=1

n +

dnf t
(cid:88)

d=1

n ∗

ad
θ

where df t is the number of full-time developers, dnf t the number of non-
full-time developers, and ad the number of commits that a non-full-time
developer has contributed in the period n, smaller than θ.

– Maximum possible eﬀort M in a period of study (for the whole
project): This is the eﬀort when all developers are considered full-time,
e.g., θ = 1. It can be considered as an upper bound of the total eﬀort, and
is calculated as follows:

M ax(En) =

df t+dnf t
(cid:88)

n

d=1

4.2 Surveying developers

The ﬁrst basic input to our model is the self-identiﬁcation of developers as
full- or non-full-time. This is done by means of a short survey that tries to
obtain this information.

One of the questions of the survey explicitly asks respondents to classify
themselves in one or the other category (‘What do you consider yourself in the

12

Gregorio Robles et al.

project? (full-time, part-time, occasional contributor ’). But as this question
can be easily misunderstood, we recommend to use additional, alternative
questions that allow to check the input given by developers for consistency.
Thus, we asked them how many hours per week they have usually devoted
in the last months to the project (‘On average, how many hours in a week
have you spent in the project in the last six months? ; developers could choose
among following options: >40h, 40h, 30h, 20h, 10h, <5h).

In addition, on a separate web page of the survey, we also showed developers
a chart containing their own personal activity (in number of commits per
month). We added a line to the chart with the estimated value of θ, and asked
them if we had identiﬁed them correctly. Developers could answer in an open
text box. Although the aim of this question was to see if developers corroborate
our ﬁndings, we did not achieve it. This was because (1) our estimated value
of θ was far away from the value ﬁnally obtained; and (2) developers were
not aware of the details of the model and commented on other issues of lesser
interest4. However, the feedback received clearly indicated that using a period
of study of 1 month was not a good choice. For the analysis presented here, we
have set 6 months as the most convenient “Period of study n” , as this is the
usual time between releases for many projects [38]. By doing this, all cycles of
six months will be aﬀected by the same process (development, feature freeze,
release, etc.).

4.3 Determining the threshold value θ

Our eﬀort estimation model depends on how accurately we can identify full-
time and non-full-time developers. To illustrate its evaluation, we use an exem-
plary scenario: let’s assume that the development team of a software project is
made up of 8 developers, and that they responded to the questionnaire (“Are
you a full-time or a non-full-time developer? ”) as in the second column of
Table 2. In the third column, we report the amounts of commits that each
worked on during the period n, as extracted by parsing the versioning system
of their project [47].

The ﬁrst three developers (D1, D2 and D3) self-identiﬁed as full-time (F).
From the data analysis of their commits, we detected that these developers
actually worked on 12, 10 and 13 commits respectively during the period under
study (for example, a month). Using the data analysis, and if only these three
developers were active, the minimum θ to identify a full-time developer would
be 10 commits in the period.

On the other hand, when we consider all the other developers, especially
D5, the threshold θ = 10 becomes less eﬀective: D5 classiﬁes his/her work
as non-full-time, but he/she is responsible for 11 commits in the period n. If
we use θ = 10 to detect full-time developers also for D5, we wrongly classify
him/her as full-time; if, on the contrary, if we use θ = 11 to identify full-
time developers (and to correctly classify D5), we wrongly classify D2 and

4 Details can be found in the reproduction package in an anonymized way

Development Eﬀort Estimation from Activity in Version Control Systems

13

Dev # Full-time or non-full-time?

(From questionnaires)

D1
D2
D3
D4
D5
D6
D7
D8

F
F
F
NF
NF
NF
F
NF

Eﬀort (# commits)
(From data analysis)
12
10
13
3
11
8
10
5

Table 2 Example scenario to determine θ: developers identify themselves as either full-time
(F) or non-full-time (NF). We evaluate their eﬀort by counting the number of their commits.

D7, who both contributed 10 commits, but self-identiﬁed themselves as full-
time. Determining the value of θ becomes therefore an exercise to minimise
the classiﬁcation errors.

4.4 Determining the estimation error

In the example above, we have the typical information retrieval problem with
true/false positives and true/false negatives. In particular, and depending on
how the threshold θ has been chosen:

– A true positive (tp) means that, using a speciﬁc θ, the developer identiﬁed
as full-time is indeed full-time. Using θ = 10, the example above comprises
4 true positives (e.g., D1, D2, D3 and D7).

– A false positive (fp) means that the developer is ﬂagged as full-time, but
s/he is not. Using θ = 10, the example above comprises 1 false positive
(e.g., D5).

– A false negative (fn) means that the developer has not been identiﬁed as
full-time, albeit s/he is. Using θ = 10, the example above does not comprise
any false negatives. If we used θ = 11, instead of θ = 10, the example above
would contain 2 false negatives (e.g., D2 and D7), but no false positives.
– A true negative (tn) means that a non-full-time contributors has been
correctly ﬂagged. Using θ = 10, the example above contains 3 true nega-
tives (e.g, D4, D6 and D8).

The visual representation of these terms is displayed in Figure 3, using the
full-time and non-full-time subsets of developers. Within the realm of infor-
mation retrieval, we can also evaluate the values of well-known performance
measures:
– The precision of the model is the ratio of true positives (i.e., the cor-
rectly identiﬁed full-time developers) and the number of all the identiﬁed
(correctly or not) full-time developers, or Pd = tp

.

tp+fp

– The recall of the model (or its sensitivity) is the ratio of correctly identiﬁed
full-time developers and the total number of full-time developers (whether
they have been correctly identiﬁed or not), as in Rd = tp

.

tp+fn

14

Gregorio Robles et al.

– The accuracy is the ratio of all the correctly predicted developers, and

the overall set of observations: Ad =

tp+tn
tp+tn+fp+fn

.

– The F-measure is a combination of both precision and recall, Fd = 2 ∗

Pd∗Rd
Pd+Rd

.

Fig. 3 Visual representation of the tp, fn, fp and tn values for the full-time and non-full-time
developer sets

However, these ‘classic’ performance measurements do not take into ac-
count the fact that classiﬁcation errors can compensate each other: depending
on the project, the same θ could classify a non-full-time developer as full-
time (e.g., a false positive) and at the same time a full-time developer as
non-full-time (e.g., a false negative). In this case, we state that the two errors
compensate each other.

4.5 Compensating the classiﬁcation errors: the goodness measure

The formulation of the goodness measurement is designed to explicitly consider
the compensation of classiﬁcation errors: this fact alone sets our model apart
from traditional information retrieval techniques. Below we illustrate (i) how
the goodness performance measure can accommodate the compensation of
classiﬁcation errors, and (ii) how it is pivotal in selecting the most probable
threshold θ to detect the full-time developers for our model.

The evaluation of the goodness measurement stems from the intrinsic distri-
bution of the developers’ work patterns. Depending on their contributions and
what type of contributors they consider themselves to be, a larger or smaller
number of wrongly classiﬁed developers could seriously skew the evaluation of
the overall eﬀort. It is also worth noticing that there is currently no mention
in the literature of the possibility of classiﬁcation errors, and the need of error
compensation. As it stands, this is the ﬁrst research work that addresses this
open issue. The formula of the goodness measure is as follows:

– Goodness = 1 − |(tp+fp)−(tp+fn)|

tp+fn+fp

= 1 − |fp−fn|
tp+fn+fp

Development Eﬀort Estimation from Activity in Version Control Systems

15

The goodness measure depends on the number of non-compensated clas-
siﬁcations (the numerator, an absolute value, since compensation works both
ways), divided by the total number of positives and false negatives. Subtract-
ing this fraction to 1, the goodness becomes larger as long as false positives
and negatives have been compensated (or if no compensation was necessary).
The measure of goodness could also consider the true negatives (tn) instead of
the true positives (tp). The reason of not including (tn) is due to the typical
distribution of FOSS developers: a large number of FOSS developers have an
extremely low level of contribution. As a result, including a large number of tn
into the deﬁnition of goodness would inevitably lower its discriminative power,
as compared to using tp.

Using the same example scenario depicted in Table 2, we report in Table 3
the values of tp, fp, fn and goodness, using diﬀerent values of θ (that is, number
of commits). As visible, the relatively high number of false positives (fp) has
a negative impact on the goodness measure, especially when θ is small. As
long as more false positives get compensated by false negatives, the goodness
measure ﬁnds its maximum: we posit that the best values of θ lie where the
goodness is maximised (highlighted in Table 3).

θ
1
2
3
4
5
6
7
8
9
10
11
12
13

tp
4
4
4
4
4
4
4
4
4
4
2
2
1

fp
4
4
4
3
3
2
2
2
1
1
1
0
0

fn
0
0
0
0
0
0
0
0
0
0
2
2
3

goodness
0.50
0.50
0.50
0.57
0.57
0.67
0.67
0.67
0.80
0.80
0.80
0.50
0.25

Table 3 True positives, false positives and false negatives (as well as the resulting goodness
measure) at diﬀerent values of θ. The data is based on the example from Table 2.

4.6 Total estimated eﬀort and estimation error

The identiﬁcation of θ allows to evaluate the eﬀort by developers for a cer-
tain period, or for the entire project. In the latter case, an adjustment of the
threshold might become necessary. In the example scenario that was discussed
above (see Table 2 and 3), the threshold that maximises the goodness measure
lies between 9 and 11. If we choose θ = 10, the overall eﬀort by developers is
6.6 PM, as in the sum of:

16

Gregorio Robles et al.

– 5 PM (1 PM each for the D1, D2, D3, D5 and D7 developers)5
– 3
– 8
– 5

10 PM (for the D4 developer)
10 PM (for the D6 developer)
10 PM (for the D8 developer)
Choosing a diﬀerent threshold, but still within the values that maximise
the goodness function (i.e., the shaded rows in Table 3), has a small eﬀect on
the overall eﬀort, that sums up to 6.78 PM in case of θ = 9, and 6.27 PM in
case of θ = 11. The estimation errors grow larger as long as one moves away
from the ‘safe’ range of θ. The sum to evaluate the overall eﬀort, as presented
above for the example scenario, is the same that was used to evaluate the eﬀort
of the projects under study.

Table 4 lists the estimation errors (in percentage) if a diﬀerent θ is se-
lected, as compared to the θ = 10 that maximises the goodness of our example
scenario.

θ
1
2
3
4
5
6
7
8
9
10
11
12
13

Eﬀort (PM)
8.00
8.00
8.00
7.75
7.60
7.33
7.14
7.00
6.78
6.60
6.27
5.92
5.54

Estimation error
21.21%
21.21%
21.21%
17.42%
15.15%
11.11%
8.23%
6.06%
2.69%
–
-4.96%
-10.35%
-16.08%

Table 4 Values of global eﬀort (in PM) by developers, and the percentages of error when
choosing a diﬀerent threshold. The data is based on the example from Table 2.

5 Deployment of the model

The model presented above is deployed using 6 large FOSS projects that have
a diﬀerent degree of participation of commercial companies. The selection of
these projects is based on convenience sampling: two of the authors of this
paper have a direct contact with the communities under study.

All selected projects follow common practices found nowadays in large
FOSS software development projects: they have code review in process (using
an external tool like Gerrit6, except Ceph that uses GitHub’s pull-request
driven process) and recommend to do commit squashing [25]. Squashing a

5 Full-time developers are assumed to work 1 PM every month, even if they work more

than the chosen θ. We call this aspect the saturation of a developer’s eﬀort.

6 https://www.gerritcodereview.com/

Development Eﬀort Estimation from Activity in Version Control Systems

17

commit refers to move changes introduced in a commit into its parent, to end
up with one commit instead of two. If you repeat this process multiple times, n
commits can be reduced (i.e., ‘squashed’) to a single one. It is common practice
in many FOSS projects to squash all commits that belong to a change, after
it is reviewed and accepted [32].

Table 5 Main parameters of the studied projects, January 2014.

Project

Type

First release

Consortium
Ceph
Consortium
Linux
Organization
Moodle
Consortium
OpenStack
WebKit
Consortium
MediaWiki Organization

07/2008 (v0.2)
08/1991
08/2002 (v1.0)
10/2010
06/2005
01/2001

# Authors
(distinct)

158
3,665
174
1,410
690
605

SLOC

# Commits
(w/o bots)
37,254

418K
540,263 > 10M
1.28M
1.65M
5.0M
340K

52,523
68,587
107,471
357,786

Summary statistics are provided in Table 5, with an additional column stat-
ing whether it is a consortium with commercial participation (‘consortium’),
or an organization (commercial or not) driving the project (‘organization’).

OpenStack – OpenStack is a software project to build a SaaS (software as a
service) platform. Over 200 companies have become involved in the project.
AMD, Brocade Communications Systems, Canonical, Cisco, Dell, Ericsson,
Groupe Bull, HP, IBM, InkTank, Intel, NEC, Rackspace Hosting, Red Hat,
SUSE Linux, VMware Yahoo! and many others have been participating in
OpenStack.

Moodle – Moodle is an FOSS project that implements a PHP-based learning
management system: it started as a volunteer project in 2002, but it has since
evolved into a company-driven FOSS project. Moodle HQ is the company that
keeps developing it, and there is an estimated 80 international companies that
help in its development.

Ceph – Ceph is a freely available storage platform7, and it also started as a
volunteer based FOSS project. After being merged into the Linux kernel in
2010, it was incorporated by RedHat that is now responsible for its in-house
development. Various large organisations contribute to its development, from
Canonical to Cisco, Fujitsu and Intel.

Linux – Linux represents the most well-known success story for FOSS projects.
Started as a volunteer project back in 1991, Linux grew exponentially, although
it is still a community-driven project. The amount of contributions from com-
mercial companies have slowly outgrown those of the individual developers:
the latter account now for only 10% of the developers of the operating system.

7 https://github.com/ceph/ceph

18

Gregorio Robles et al.

WebKit – WebKit is an FOSS project started by Apple, although it started as
a fork of an existing KDE project. It implements the engine that sits behind the
Safari web browser. KDE, as well as other FOSS communities, still participates
in the development of WebKit, but large commercial organisations (Google,
Nokia) have also contributed to it. In April 2013, Google forked the WebKit
creating to Blink. Since then it is mainly a project backed by Apple.

MediaWiki – Wikimedia is the non-proﬁt organisation that supports the growth
of the Wikipedia online encyclopedia. Wikimedia is directly in charge of the
MediaWiki FOSS project8, that implements the wiki engine that allows pages
to be added, modiﬁed and contributed to by anyone.

6 Data Gathering

As mentioned above, the estimation model that we propose in this paper
obtains data from two sources: (i) the VCS of the project under study, and (ii)
the questionnaire data that was sent to the developers identiﬁed in the VCS
of each project. Below we describe the steps that were performed to gather
the data from each source.

6.1 Versioning System Data

The information from the VCS is obtained by means of Perceval [17], which is
part of the MetricsGrimoire toolset9. Perceval retrieves development informa-
tion and metadata from repositories10 of the project, and provides it as JSON
ﬁles, ready for analysis.

The data obtained from the VCS repositories was subject to two cleaning
operations: one related to the extraction and reconciliation of duplicated devel-
oper identities [45,53]; and one related to the exclusion of commits generated
by automated bots. We performed those operations for all the projects in the
sample.

Using OpenStack as an example, from the analysis of its commits we ob-
served that there are 1,840 distinct author identiﬁers in the VCS of OpenStack.
After applying a merging algorithm [33], manual inspection and feedback from
the community, 1,410 distinct real authors have been identiﬁed, once dupli-
cate IDs were merged. Additionally, the total number of commits is almost
90,000, although many of these have been done automatically by bots. The
committers identiﬁed as bots, as well as their commits, were excluded from the
analysis. Bots are responsible for 21,284 commits, almost 25% of all commits

8 https://phabricator.wikimedia.org/source/mediawiki/
9 http://metricsgrimoire.github.io
10 The Ceph, Linux, Moodle, MediaWiki and OpenStack datasets were retrieved from git
repositories. The WebKit dataset was retrieved from a Subversion repository.

Development Eﬀort Estimation from Activity in Version Control Systems

19

in the project: those bots and their commits were excluded from the data used
to evaluate θ.

OpenStack, as many other FOSS projects, has an uneven contribution pat-
tern where a small group of developers have authored a major part of the
project. In summary, 80% of the commits have been authored by slightly less
than 8% of the authors, while 90% of the commits correspond to about 17%
of all the authors. In addition, as the corporate involvement in OpenStack is
signiﬁcant, this should allow us to identify full-time developers from compa-
nies: the OpenStack Foundation estimates that around 250 developers work
professionally in the project.

At the end of this data analysis, for each project in our sample we obtained

the number of monthly commits per developer.

6.2 Online survey of developers eﬀort

To obtain data of the time commitment of the FOSS developers in the sampled
projects, we designed an on-line survey (as described in Section 4.2). We ob-
tained the e-mail addresses of the developers from their authorship information
in the VCS, and sent them an e-mail with an invitation to participate.

Gathering all the distinct developers from the 6 sampled FOSS projects,
an overall 6,700 personalized mails were sent, and the survey was answered by
a grand total of 1,028 respondents. We removed 55 survey responses because
they were empty or we could see from the answer in the free text box that the
respondent had misunderstood/misinterpreted the questions (e.g., there was a
developer referring to his contributions to the LibreOﬃce project, which is not
part of the analysed sample of projects). We also amended 32 surveys (e.g.,
some respondents stated to be professional developers hired by a company
and devoted 40 or more hours a week to the project, but had left empty their
status, which we set to “full time”).

Table 6 reports the number of responses (and the ratio) obtained from the

developers of the sampled projects.

Table 6 Summary of emails sent to the developers of the sampled projects, and ratio of
completed questionnaires

Ceph
Linux
Moodle
OpenStack
WebKit
MediaWiki

Sent Received
24
158
652
3,665
42
174
131
1,407
85
690
94
605

Ratio
15.19%
17.79%
24.14%
9.31%
12.32%
15.54%

Total

6,699

1,028

15.35%

20

Gregorio Robles et al.

7 Model deployment – OpenStack

This section reports the results of our analysis: we ﬁrst present an in-depth
analysis of the OpenStack project, and later an overall view of all the projects
in the sample.

7.1 Identiﬁcation of full-time developers

In this section we describe how the full-time developers were identiﬁed within
the OpenStack contributors, by using the threshold θ and calibrating the
results of the questionnaire. The same approach was followed for the other
projects in the sample.

As a ﬁrst step we analysed the questionnaires: we gathered that 37 devel-
opers identify themselves as full-time, while 54 developers consider themselves
as non-full-time. As a second step, we extracted the number of commits of
each developer, in the six months prior the questionnaire. Given those two
sources of information, we need to answer the question: what is the minimum
number of commits that more precisely separates full-time from non-full-time
developers?

To answer that question, the two plots in Figure 4 classify the developers

as following (at a certain level of θ):

– true positives: number of full-time OpenStack developers that are classiﬁed

as full-timers;

– false negatives: number of full-time OpenStack developers wrongly classi-

ﬁed as non-full-timers;

– false positives: number of non-full-time OpenStack developers that were

classiﬁed as full-timers;

– true negatives: number of non-full-time OpenStack developers correctly

classiﬁed as non-full-timers.

Full-time developers – When θ is set to 1, developers who contribute at least
one commit in a six-month time-span will be considered as full-time by our
algorithm. From Figure 4, the precision to detect a full-time developer with
θ = 1 is maximum: this means that each of the 37 self-identiﬁed full-time
developer has contributed at least one commit during the allocated time-span
(i.e., six months).

The number of true positives (tp) decreases once θ gets larger: for exam-
ple, out of the 37 self-identiﬁed full-timers, only 30 developers contribute at
least 8 commits every six months (i.e., θ = 8). The remaining 7 developers
are classiﬁed as false negatives: even if they are full-time developers, our ap-
proach considers them as non-full-timers. In this case, the precision of using a
threshold θ = 8 to detect full-time developers is 30
37 .

Development Eﬀort Estimation from Activity in Version Control Systems

21

Non-full-time developers – The same approach applies to non-full-time de-
velopers, but based on false positives and true negatives (see bottom plot of
Figure 4): setting θ = 1, all the self-identiﬁed non-full-time developers are false
positives, and our algorithm considers them as full-timers.

As seen in the bottom plot of the same Figure, also the number of true
negatives (tn) increases, as long as θ gets larger. At θ = 8, for example, 38 out
of 54 non-full-time developers are correctly identiﬁed as non-full-timers (i.e.,
true negatives); while 6 out of 54 non-full-time developers are still considered
as full-timers, at that level of θ (i.e., false positives).

Fig. 4 (Top) True positives (tp) and false negatives (fn) for full-time developer identiﬁcation
for several values of θ (OpenStack project). (Bottom) False positives (fp) and true negatives
(tn) for non-full-time developer identiﬁcation for several values of θ. The dimension of the
vertical axis has been maintained to ease the comparison of the size of both populations.

7.2 Compensation of error and the goodness metric

Figure 5 displays graphically the compensation pattern in the OpenStack
project, and for diﬀerent values of θ. The number of wrongly classiﬁed non-full-

22

Gregorio Robles et al.

time developers are subtracted from the number of wrongly classiﬁed full-time
developers.

Fig. 5 Compensation between false positives and false negatives for values of t. Negatives
values of the y axis indicate false positives that do not get compensated by false negatives,
while positive values of the y axis indicate false negatives not compensated by false positives.

As it can be seen, for θ = 1 no full-time developer has been incorrectly
classiﬁed, but all non-full-time developers have. No compensation occurs then.
However, as θ increases, the error of incorrectly classifying full-time developers
is compensated by the error of incorrectly classifying non-full-time developers.
The value for which the number of non-compensated errors is minimum corre-
sponds to t=12, where the diﬀerence is 1 (i.e., there is one false negative more
than false positives).

Figure 6 shows that, for the OpenStack project, the value of goodness
peaks at θ = 12 (0.979). Thus, the error for our eﬀort estimation should be
smaller for θ = 12, due to the eﬀect of compensation that the goodness factor
recognizes.

7.3 Eﬀort estimation

Having obtained feedback from the OpenStack developers through the survey,
the identiﬁcation of full-time developers via θ becomes more precise. Depend-
ing on the θ chosen, the eﬀort estimation will provide more or less accurate
values.

Table 7 shows the results of the estimated eﬀort in OpenStack, and using
several values of θ. Considering the global eﬀort through the whole lifespan
of the project, we can see that -as expected- the number of person-months
decreases while the threshold increases: less and less developers are consid-
ered as full-time, thus pushing the eﬀort down. The upper bound of eﬀort for
OpenStack, where a single commit in a semester would have been coded as 6
person-months, results in 17,400 person-months.

Development Eﬀort Estimation from Activity in Version Control Systems

23

Fig. 6 Relevance of results (for many threshold values) for the OpenStack project.

Table 7 Estimated total eﬀort (in PM) for the OpenStack project, given per threshold
value (column θ). Estimated eﬀort values for all semesters. The darker the background, the
higher the value of goodness (white < 0.8 ≤ lightblue < 0.9 ≤ greenblue ≤ 1.0)

θ
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

Eﬀort (total, PM)
17,400
15,255
13,804
12,747
11,891
11,213
10,629
10,128
9,683
9,298
8,957
8,655
8,370
8,112
7,876
7,662
7,466
7,284
7,109
6,945

10s2
426
420
402
389
378
370
362
355
345
337
330
324
318
313
308
303
298
294
291
288

11s1
816
813
798
785
768
757
743
732
721
711
700
690
680
672
663
656
648
641
634
628

Eﬀort (semester, PM)
12s1
11s2
1,890
1,242
1,647
1,170
1,486
1,110
1,383
1,056
1,295
1,008
1,224
964
1,164
927
1,112
896
1,065
866
1,025
841
987
819
955
800
924
782
896
767
872
753
850
740
830
729
812
719
794
708
777
698

12s2
2,742
2,385
2,140
1,964
1,830
1,726
1,635
1,560
1,493
1,434
1,381
1,334
1,289
1,247
1,208
1,173
1,140
1,107
1,077
1,048

13s1
4,368
3,783
3,384
3,074
2,821
2,618
2,451
2,310
2,189
2,086
1,997
1,919
1,847
1,781
1,721
1,666
1,615
1,568
1,522
1,481

13s2
5,916
5,037
4,484
4,098
3,791
3,554
3,346
3,164
3,003
2,864
2,743
2,634
2,531
2,437
2,351
2,275
2,206
2,142
2,083
2,025

According to the analysis, the best values for θ are between 9 and 12. The
highlighted value of θ = 12 in Table 7 is what minimizes the error of considering
developers as full-time using the feedback from the OpenStack developers. For
the θ = 12 scenario, the estimation of the amount of eﬀort that was invested
in the OpenStack project lies around 9,000 person-months (750 person-years).
This can be considered as the most accurate, and it reaches only half of the
estimation value produced in the upper bound estimation (with θ = 1).

24

Gregorio Robles et al.

The table also provides information for every semester. As can be observed,
the eﬀort devoted to the OpenStack project is increasing with time, with a
maximum for all the thresholds in the second semester of 2013. If we take the
value of 12 as the threshold, 2,634 person-months have been worked during the
last six months. This implies that we estimate that the actual eﬀort on Open-
Stack has been around 440 person-months during the last six months. When
asked about these ﬁgures, OpenStack members conﬁrmed that they sound rea-
sonable as the estimation of the OpenStack foundation is that currently over
250 professional developers work on the project hired by companies11.

On a side note, it should be noted that the ratio between the eﬀort es-
timated as the upper bound (θ = 1) and the one with θ = 12 is steadily
increasing. In the second semester of 2010 the ratio was 1.31, while for the
second semester of 2013 it has grown to 2.25. This is because the number of
non-full-time contributors has grown as well. Thus we can see how much the
“minor contributors” in the project aﬀect the estimations given by our model.
While the error (noise) introduced by these small contributions is included
using the upper bound, higher values of θ ﬁlter it out. The result is not only a
more realistic estimation, but also an estimation where the more error-prone
assumptions (i.e., non full-time developers) have a smaller weight in the total
eﬀort.

8 Further model deployment

The estimation model deployed in the OpenStack project was also evaluated
for the other systems composing our sample. In this section, we perform the
model deployment for 5 other systems to

(i) identify their full-time and non-full-time developers (8.1),
(ii) extract the values of θ where compensation happens more frequently for

each system (8.2), and

(iii) ﬁnd the goodness value that maximises the accuracy of the (project-speciﬁc)

model (8.3).

Table 8 shows the summary of the calculations of both the θ value, and the
corresponding eﬀort for the other projects in the sample. In the next sections
we describe how the estimation model was deployed in each of the additional
systems in the sample.

8.1 Identiﬁcation of full-time developers

The approach that was presented for OpenStack was repeated for all the sys-
tems. In all the cases, when θ increases, the more precise it becomes to identify
the non-full time developers, based to their responses to the questionnaires.

11 This type of informal feedback was gathered only from OpenStack developers, but not
repeated for the other projects in the sample.

Development Eﬀort Estimation from Activity in Version Control Systems

25

Table 8 Estimated total eﬀort (in PM) for all projects from 2010s2-2013s2 (3.5 years),
given per threshold value (column θ). The darker the background, the higher the value of
goodness (white < 0.8 ≤ lightblue < 0.9 ≤ greenblue ≤ 1.0)

OStack
17,400
...

10,128
9,683
9,298
8,957
8,655
8,370
8,112
7,876
7,662
7,466
7,284

θ
1
...
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

t Moodle
2,688
1
...
...
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

1,846
1,803
1,765
1,731
1,698
1,666
1,636
1,609
1,584
1,561

t WebKit
11,976
1
...
...
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

8,022
7,887
7,753
7,619
7,493
7,372
7,257
7,146
7,037
6,933
6,833
6,737
6,646
6,558
6,472
6,390

t
1
...
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

Linux
57,390
...
32,156
31,157
30,240
29,393
28,602
27,865
27,173
26,524
25,914
25,334
24,781
24,256
23,761
23,290
22,839
22,411
22,002
21,609

t
1
...
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35

Ceph
1,056
...
574
567
560
554
548
542
536
530
525
520
516
511
506
501
496
492
488
484

t MWiki
5,508
1
...
...
3,348
30
3,324
31
3,302
32
3,279
33
3,258
34
3,237
35
3,217
36
3,198
37
3,179
38
3,161
39
3,142
40
3,124
41
3,106
42
3,089
43
3,072
44
3,056
45
3,040
46
3,025
47

Similarly, when θ grows, the accuracy at identifying full-time developers de-
creases for all but one project. Ceph (second from bottom plots in Figure 7)
is an expected variation to this trend: all its self-identiﬁed full-time develop-
ers have an extremely high number of commits in the six-month time-frame,
higher than the 50 commits depicted in the ﬁgure.

8.2 Compensation

Similarly to what achieved in Section 7.2, the compensation levels were evalu-
ated to identify the minimum between false negatives and false positives. The
plots of the compensation values, per project, are available in Figure 8. Given
a compensation plot, one can obtain the value of θ for each project, and more
precisely where the compensation is closest to zero. In the extreme case of the
Ceph project, there are no false negatives, and only false positives, therefore
the compensation is negative for any threshold value.

8.3 Goodness (and threshold values obtained)

In Figure 9 we have plotted similar metrics to Figure 6 (i.e., precision, re-
call and goodness) for the rest of the analysed systems. The θ value for a
project is selected in correspondence of the maximum value of the goodness:
as demonstrated above, that threshold value minimizes the estimation error.
We collected the values of θ, per project, in Table 9. In the Discussion (in
particular in Section 9.5), we elaborate about why projects might have diﬀerent

26

Gregorio Robles et al.

Fig. 7 Correct identiﬁcation of full-time (l) and non-full-time (r) developers depending
on the threshold value (x-axis). The darker shade gives those developers that have been
correctly identiﬁed, while the lighter one is for those that were not. The vertical axis has
been kept the same for each project in order to ease visual comparison of the population of
each group.

values of θ, and how this knowledge can be leveraged to extend our model to
other projects, and without having to directly survey their developers.

Development Eﬀort Estimation from Activity in Version Control Systems

27

Fig. 8 Compensation between false positives and false negatives for values of θ. Y-axis is
given in number of developers; the x-axis corresponds to the θ value. Negatives values of the
y axis indicate false positives that do not get compensated by false negatives, while positive
values of the y axis indicate false negatives not compensated by false positives. Note that
the magnitude of the vertical axis is diﬀerent for each plot.

Project
OpenStack
Moodle
Linux
WebKit
Ceph
MediaWiki

θ
12
14
17
[16-19]
24
[36-37]

Table 9 Values of θ for all the projects analysed

9 Discussion

In this section we discuss the limitations, implications and further possibilities
of our research. We will start presenting the threats to validity (9.1), and
then elaborate further on the sources of error of our model (9.2). Finally,
possible impact for practitioners and managers (9.3), the representativeness of
our survey (9.4) and the possibilities of generalizing to other FOSS projects
(9.5) are presented and discussed.

28

Gregorio Robles et al.

Fig. 9 Values of precision, recall and goodness relative to θ. Legend: Precision ((cid:59)), Recall
((cid:111)), Goodness ((cid:108)). Note that in Ceph the values of goodness coincide with the ones of
precision.

Development Eﬀort Estimation from Activity in Version Control Systems

29

9.1 Threats to Validity

All empirical studies, such as this one, are subject to threats to validity. Here,
we point the most relevant ones and discuss how to mitigate or control these
threats if possible.

Conceptual Assumptions and assertions have been done. We have tried
to make them explicit, as for instance the assumptions that a person-month
equates to 40h/week. Some of them may require further support, but this does
not invalidate the plausibility of the argument.

Internal Having only considered one of the many sources of activity (and
data) may result in our estimation model being not accurate. Further work is
expected to be of broad interest and supportive of the claims presented.

External Originally, OpenStack was chosen as the illustrative case study.
We consider that this project is representative of many FOSS projects, es-
pecially those where many industry players cooperate. There may be certain
practices that could only be found for OpenStack: for instance, their review
process produces large (and few) commits, so values of θ should be selected
diﬀerently for other FOSS projects. We have applied the model to other ﬁve
FOSS projects, and the results are similar.

The model has been conceived to work well when participation follows a
power-law and the number of full-time developers that can be easily identiﬁed
because their activity is much higher then by the rest of developers – else we
could have many false negatives (i.e., many full-time developers not identiﬁed
as such). What we have assumed is usually the case in FOSS development,
but has not to be true for all FOSS projects. We have applied our model to
large project (with hundreds of committers), but cannot ensure it will oﬀer
meaningful results for other FOSS projects, especially small ones.

The selection of the further 5 systems was based on convenience sam-
pling: as said before, the communities of the projects selected were more easily
reached, due to two of the authors of the being paper in direct contact with
all of them. Although the sample might be unrepresentative, the systems were
only used to deploy the model, not to show its absolute value for any other
system. In any case, the generalization of the model to other FOSS projects
remains to be investigated.

Construct A replication package12, following the guidelines in [21], is of-
fered to others to reproduce our ﬁndings. The replication package contains
all scripts and public information, but not all the results of the surveys, as
personal information was collected. Aggregated and anonymized information
from the surveys is available.

It is important to note that it is not possible to establish a unique ground
truth for the total eﬀort made by developers in FOSS projects. This is because
FOSS projects do not typically make use of timesheets to track the actual
eﬀort of developers. Therefore, a ‘traditional’ measure of eﬀort is not clearly
computable. The measures that we propose do not help in establishing what

12 http://gsyc.urjc.es/~grex/repro/2022-emse-effort-estimation

30

Gregorio Robles et al.

is the ground truth, but rather to minimise the error in the evaluation of the
estimated eﬀort based on actual input by the developers of the project.

A larger threat to construct validity was identiﬁed, based on the represen-
tativeness of the survey sample. It should be, however, noted that for deter-
mining the best value of θ, we do not need a representative sample of the whole
project population, but just a representative sample that allows us to eﬀec-
tively discriminate between full-time and non-full-time developers. We thus
require basically data that is representative of (very) active developers to per-
form this analysis. With a very high probability, all the developers with a low
activity (i.e., few commits) are non-full-time developers – and our model will
accurately label them as such. We check if this aﬀects our results in more
detail in Section 9.4.

9.2 On the sources of estimation error

Our eﬀort estimation model is based on the identiﬁcation of full-time devel-
opers. Once identiﬁed, full-timers are assigned the maximum possible eﬀort
(1PM every month): non-full-timers are only assigned a share of it, depending
on the number of their commits and the θ. In this situation, the sources of
error are the following:

1. Wrong identiﬁcation of full-time developers (false negatives) or non-full-

time developers (false positives).

2. Wrong assignment of eﬀort to non-full-timers.

Note that we assume that the assignment of eﬀort to full-timers is error-
free, as we argue that by deﬁnition the eﬀort of a full-time developer is 1PM.
We argue below (sections 9.2.1 and 9.2.2) why both sources of error do not
pose a major threat to the validity of our model and its results.

9.2.1 Wrong identiﬁcation

In our model false negatives (FN) refer to full-time developers that we have
not identiﬁed as such. We have included a short comment in the threats to
validity (Section 9.1), stating that results may not hold if there are many false
negatives in a certain project.

However, we consider it safe to assume that the number of false negatives
will be small in the general case of FOSS development. The reasons for this
are as follows:

1. The number of full-time developers should be (very) low compared to the
amount of total participants in a FOSS project. This is because the distri-
bution of participation is very skewed, as in a power-law or Rayleigh-type
curve (as reported in the research literature [28, 49]), in FOSS. So, in gen-
eral, full-time developers will have proﬁles with high commit activity, well
above the threshold value, that is, the lowest bound of activity for full-
timers.

Development Eﬀort Estimation from Activity in Version Control Systems

31

2. A false negative might be compensated by a false positive. Some full-time
developers might have a low activity, below θ. When they are below θ,
compensation (i.e., non-full-time developers that are incorrectly identiﬁed
as full-time (false positives)) softens this situation. In other words, because
we have chosen θ where goodness is highest (i.e., the one that maximizes
compensation), the number of FNs that are not compensated should be
the lowest possible.

3. If not compensated, the impact of the error is small, as an aggregate. Hav-
ing a FN that is not compensated means that a developer is assigned a
fraction of the eﬀort depending on the number of commits and the thresh-
old. So, if for OpenStack θ is 12 commits in a 6-month period, and we have
a full-time developer with 6 commits in that period, we would have a false
negative and this developer would be accounted with an eﬀort of 6/12 *
6PM = 3 PM instead of with 6PM. This would imply an error of 100% at
the individual level. But our estimation model is based on aggregation of
all developers, so even if the error for one individual developer might be
high, the fact that it is low for many other developers will pay oﬀ for the
whole project.

A similar reasoning applies to false positives (FP) in our model, i.e., to non-
full-time developers that we have not identiﬁed as such. Even if the population
of non-full-time developers was large, using the highest goodness value ensures
highest compensation and, thus, the lowest aggregated number of FPs. In
this case, the individual error introduced would be because a non-full-time
developer is assigned 6PMs in a period of six months, when the real eﬀort is
less (e.g., 3PMs). Again, the error at the individual level might be large, but
its impact on the whole project will be small.

For the six projects under study in this paper, the number of non-compensated

FNs or FPs are: 1 developer for OpenStack, Linux and Ceph and 0 developers
for WebKit, MediaWiki and Moodle. This can be seen from the Figures 5 and 8,
This strengthens our argument: the impact of an incorrect identiﬁcation of de-
velopers in our model is very low, even negligible in large FOSS projects.

9.2.2 Wrong assignment of eﬀort to non-full-timers

Another source of error may come with the eﬀort estimation assigned to non-
full-time developers. In our model we assign non-full-time developers the frac-
tion of commits/threshold (i.e., if they did 2 commits in 6 months and θ is 12,
as in OpenStack, then they will be assigned an eﬀort of 1/6 * 6 = 1 PM for
the 6 months).

The rationale for this estimation is easy to understand: these developers
are not directly estimated for their eﬀort, but as its fraction to the minimum
activity that is required to be considered as a full-time developer. In other
words, if a non-full-time developer has authored 2 commits and we expect
full-time developers to submit at least 12 commits (θ = 12), our model will
assign this developer 1/6 of the minimum eﬀort needed to be considered a
full-time developer.

32

9.3 Impact

Gregorio Robles et al.

Based on the models of FOSS development, the analysis that was presented,
and its ﬁndings, can have a direct impact on the decisions taken by prac-
titioners and managers. The estimation of past eﬀorts, with the addition of
an understanding of what is already implemented, what is missing, and what
needs to be maintained, can help to plan for the future.

If managers can estimate past eﬀort, they can also evaluate the eﬀort that
is likely to be needed in the near future. This is particularly important for
those projects where many developers, from diﬀerent companies, contribute
with very diﬀerent levels of dedication, and where estimating past eﬀort is
very diﬃcult. In those contexts, the impact of a company leaving the project
can be estimated; in a similar way, the impact of a new company allocating
its developers at diﬀerent levels of eﬀort (e.g, full-time, part-time, occasional,
etc.). All these levels of developer engagement can be re-estimated periodically,
so that managers can account from divergences from the plan.

In particular, from our experience the focus on the estimation of past eﬀort

in FOSS is important usually in two situations:

1. When a company is evaluating the overall eﬀort put in the development of
a FOSS project, because they have a direct interest in assuming all or part
of that cost. This is the case, for example, when the software is strategic
to a company, and they want to evaluate how much eﬀort was put in it
in the past. This evaluation will be then used as an estimation of future
eﬀort, in the short and medium term, and considering to hire a part of the
developing team: knowing the past eﬀort gives also an evaluation of which
fraction of the development eﬀort they are hiring.

2. When some FOSS is developed mainly by developers hired by companies,
usually in the context of some Foundation or managing board, where com-
panies involved decide about the resources they allocate to the project. In
those case, having estimations about past eﬀorts, and allocations of those
eﬀorts to companies (and to developers hired by those companies) is fun-
damental for the negotiations where new companies want to gain inﬂuence
in the project, a seat in the board, or just negotiate how to move forward
features that are important for them. We have speciﬁcally observed this
kind of negotiations in projects such as Xen and OpenStack, and in some
projects under the Linux Foundation umbrella.

In addition to this, when companies participate to FOSS endeavours, they
are fully aware of the eﬀort produced by their own developers, and how they
engage with the FOSS projects that they have an interest in. Having a better
estimate of how much eﬀort (overall) has been produced into a FOSS project
would make a clearer case for a company’s Return-on-Investment (RoI). As
companies are usually aware of the eﬀort they put into the project, they can
compare this with the total eﬀort obtaining a ratio of how much of this eﬀort
comes from the community. The returned eﬀort is what a company gains
for its stake over a FOSS, as contributed by the community around it. That

Development Eﬀort Estimation from Activity in Version Control Systems

33

gain would necessarily inﬂuence a company’s strategic decision of investing its
own developer eﬀorts (or procure and pay for development eﬀorts from other
specialist companies) into the FOSS project, and at what level. Even more
importantly, in case of other commercial enterprises participating to the same
development, a company would be able to tailor and adapt their own input to
the project, and based on other companies’ behaviour.

9.4 Representativeness of the survey sample

The rate of responses obtained in the questionnaires to the developers is around
25% at best (in the case of the Moodle project). This means that most devel-
opers have not responded to the questionnaires, and the representativeness of
the sample could be put in question. Limited to the OpenStack project, Fig-
ure 10 shows two box-plots with the analysis of the developers who responded
to the survey (left) and all the active developers (right). The measured activity
in number of commits considers the six months preceding the survey.

In order to check whether the two distributions of commits come from the
same population, we applied the Kolmogorov-Smirnov’s test, a non-parametric
test that considering two samples, evaluates the null hypothesis H0: are the two
samples extracted from the same population?. We considered various activity
levels (e.g., developers committing 0 or more commits; developers committing
1 or more commits and so on) and tested their distribution against the overall
active population of developers, at the same activity level. The signiﬁcance
level for each of those statistical tests was set to a standard α = 0.5: Table 10
summarises the p-values (last column) along with other attributes.

Fig. 10 Boxplot with the activity (in number of commits during the last 6 months before
the survey) for the active developers surveyed in the OpenStack project (left) and for all
the active developers (right) in the last 6 months before the survey.

34

Gregorio Robles et al.

Fig. 11 Boxplot with the activity (in number of commits during the last 6 months before
the survey) for active surveyed developers and for all the active developers in the last 6
months before the survey.

Table 10 Summary of population measures. Several populations have been selected, de-
pending on a minimum number of commits. D and p-value as given by the Two-sample
Kolmogorov-Smirnov test.

Commits
≥ 0

≥ 1

≥ 2

≥ 3

≥ 4

≥ 5

≥ 8

≥ 11

Population
all (1,626)
survey (125)
all (986)
survey (92)
all (693)
survey (74)
all (563)
survey (64)
all (490)
survey (63)
all (427)
survey (58)
all (314)
survey (46)
all (256)
survey (41)

Min.
0.00
0.00
1.00
1.00
2.00
2.00
3.00
3.00
4.00
4.00
5.00
5.00
8.00
8.00
11.00
11.00

1st Q Median Mean
9.62
1.00
0.00
14.12
4.00
0.00
13.76
3.00
1.00
16.35
5.00
1.00
19.15
7.00
3.00
23.61
12.00
5.00
23.11
9.00
5.00
26.98
13.50
6.75
26.11
11.00
6.00
27.37
14.00
7.00
29.37
13.00
7.00
29.38
16.50
8.50
37.77
19.00
12.00
35.57
22.00
13.00
44.34
24.00
14.75
38.83
26.00
14.00

3rd Q Max.
491.00
6.00
201.00
14.00
491.00
11.00
201.00
19.00
491.00
18.00
201.00
27.00
491.00
22.00
201.00
31.00
491.00
26.00
201.00
31.00
491.00
30.00
201.00
34.75
491.00
41.00
201.00
42.75
491.00
52.00
201.00
50.00

D
0.203

p-value
0.0001

0.197

0.0028

0.194

0.0128

0.196

0.0243

0.136

0.2562

0.121

0.4406

0.119

0.6229

0.084

0.9631

As visible from Table 10, we reject the base H0 at all activity levels up
to 4 commits per period: hence, between 1 and 4 commits, we can reject the

Development Eﬀort Estimation from Activity in Version Control Systems

35

hypothesis that the surveyed developers represent the overall population. With
higher commit activities (5 commits or more), the H0 cannot be rejected at the
α = 0.05 level: from 5 commits up, we cannot reject the hypothesis that the
surveyed developers represent the overall population. As the values of activity
increase, the surveyed population becomes more representative of the project:
since our model is based on activity and the classiﬁcation is performed only
on active developers, these results give a stronger support to our model.

The same analysis was extended to the other sampled systems, and sum-
marised in the boxplots of Figure 11. From top left, the activity of the respond-
ing developers (in the 6 months preceding the questionnaire) for the Linux,
WebKit, MediaWiki, Ceph and Moodle systems was compared with the activ-
ity of all developers in each project. Similarly to OpenStack, we cannot reject
the hypothesis that the surveyed developers of those projects represent the
overall population, for activities of more than 3 commits.

9.5 Generalization to other projects

If a project wants to know its own θ with accuracy, the amount of data that
they need is limited and is easy to gather. By polling their developers for
their full-time/part-time status, they could use our model to ﬁnd their par-
ticular θ. Some FOSS projects regularly survey their developers for knowing
their community better, including personal, academic, working, community
and other matters. So, for instance, since 2011 OpenStack performs a yearly
developer survey13. Gathering information for our model would just require
two questions to be added to those surveys (e.g., ‘do you consider yourself to
be a full-time developer ’ and ‘how many commits have you performed in the
last 6 months’) .

We think, however, that the value of θ is dependent on intrinsic properties
of the project, and that a FOSS project can use the model without having to
perform a developers survey, but to use a θ from a similar project. The results
of applying our model to the 6 FOSS projects in this study make us think
that θ depends on the process that the project follows. Thus, depending on
the project’s practices, commits may require more or less eﬀort to be approved
before being merged, depending on whether a review process is in practice.
If that is the case, code is not formally committed to the repository until
it has been through extensive review, usually including several revisions. In
addition, and particularly in those projects that have more strict code review
practices, projects require to squash all commits into a single one once a change
is accepted; then, this only commit can be pushed to the repository. This has
as a consequence that commits are larger and more costly (in time and eﬀort)
for those projects, than for others that do not follow this practice.

This can be conﬁrmed from the projects that we have studied in this paper.
The values of θ shown in Table 9 are similar for those projects that have strict

13 https://insights.stackoverﬂow.com/survey/

36

Gregorio Robles et al.

code reviewing practices, such as OpenStack (θ=12), Moodle (14), Linux (17)
and WebKit (16-19). All of them use Gerrit to support code review. Ceph
(24) and MediaWiki (36-37) have higher numbers of θ, because their code
review practices are lighter (i.e., less demanding) and commit squashing is
not that strictly followed as in the former projects. For instance, Ceph uses
GitHub’s pull-request mechanism, which requires just one approval from a
maintainer, while for OpenStack changes require to be approved by at least
two maintainers.

Projects that do not use commit squashing, could use ‘active days’ instead
of ‘commits’ in the model. That way, all commits during the same day would
be considered as a contribution, mitigating the impact of diﬀerent commit
behaviours of committers [30].

10 Conclusion and Future Work

This paper has tackled two challenges. The ﬁrst is how to design a simple, but
sound estimation model to measure the eﬀort provided in a sparse, distributed
and uneven development scenario, like the FOSS one. The second challenge is
how to design the model so that it oﬀers not only a reasonable prediction, but
also credible.

In order to maximise the simplicity of our estimation model for FOSS
development, we only discriminate between two types of developers: full-timers
and the rest. For harnessing the credibility of our model, we have obtained
feedback data from over a thousand developers, who work at diﬀerent levels
across six large FOSS projects.

The model establishes a threshold θ that separates the level of activity of
full-timers from the rest of developers. The value of θ has been optimised with
the developers’ responses: thanks to their feedback, we have achieved a much
more realistic separation of developer types, minimizing the estimation error.
Using θ in our model, the estimation of the overall eﬀort results in a simple
calculation, and just using two developer types.

We conjecture that the model being dependent on this value of θ is what
allows the model to be useful for diﬀerent projects: the relationship of com-
mits to eﬀort (which is at the core of the model) may be very diﬀerent from
project to project, but we have shown that it can be captured with this single
parameter.

Using 6 large FOSS projects as case studies, we have shown how the model
can be applied and ﬁne-tuned. Although further research on this is needed,
the results obtained make us hypothesize that the value of θ depends on the
development practices – basically, how strict code reviewing practices are and
if commit squashing is frequently used when merging changes into the source
code. If so, the future applicability of our model would not require to survey
developers, as it would suﬃce to use a value of θ obtained from projects that
follow similar practices.

Development Eﬀort Estimation from Activity in Version Control Systems

37

We envisage to expand this study by 1) studying other FOSS projects to
ascertain if our method is applicable in general, and if so, to what extent;
2) performing a scientiﬁc experiment to obtain margins of error for the es-
timation of error for non-full-time developers; 3) comparing our results with
the ones provided by traditional software estimation models used in industry,
such as COCOMO; 4) after quantifying the eﬀort required in a FOSS project,
establishing whether it is more proﬁtable for a prospective adopting company
to redo (“make”) their own system, or to invest (“buy”) in the existing FOSS
system as discussed in [8]; and 5) comparing our approach with previous eﬀort
estimation techniques for FOSS projects, as the one proposed in [13] based on
the measure of entropy to calculate maintenance costs.

Acknowledgements We want to express our gratitude to Bitergia14 for the support they
have provided when questions have arisen. We acknowledge the support of the Government
of Spain through the “BugBirth” project (RTI2018-101963-B-100). We also acknowledge the
work by Carlos Cervig´on on an earlier version of the manuscript.

References

1. W. Abdelmoez, M. Kholief, and F. M. Elsalmy. Bug ﬁx-time prediction model using
na¨ıve bayes classiﬁer. In 2012 22nd International Conference on Computer Theory and
Applications (ICCTA), pages 167–172. IEEE, 2012.

2. A. Abran, J.-M. Desharnais, and F. Aziz. 3.5 measurement convertibility—from function
points to cosmic ﬀp. Cosmic Function Points: Theory and Advanced Practices, page
214, 2016.

3. A. Agrawal, A. Rahman, R. Krishna, A. Sobran, and T. Menzies. We don’t need another
hero? the impact of” heroes” on software development.
In Proceedings of the 40th
International Conference on Software Engineering: Software Engineering in Practice,
pages 245–253, 2018.

4. S. N. Ahsan, J. Ferzund, and F. Wotawa. Program ﬁle bug ﬁx eﬀort estimation using

machine learning methods for oss. In SEKE, pages 129–134, 2009.

5. H. Alomari. A slicing-based eﬀort estimation approach for open-source software projects.
International Journal of Advance Computational Engineering and Networking (IJA-
CEN), 3(8):1–7, 2015.

6. J. J. Amor, G. Robles, and J. M. Gonzalez-Barahona. Eﬀort estimation by characterizing
developer activity. In Proceedings of the 2006 international workshop on Economics
driven software engineering research, pages 3–6. ACM, 2006.

7. P. Anbalagan and M. Vouk. On predicting the time taken to correct bug reports in
Open Source projects. In Software Maintenance, 2009. ICSM 2009. IEEE International
Conference on, pages 523–526. IEEE, 2009.

8. J. Asundi. The need for eﬀort estimation models for open source software projects.

ACM SIGSOFT Software Engineering Notes, 30(4):1–3, 2005.

9. B. Boehm. Software engineering economics, 1981.

10. B. W. Boehm, R. Madachy, B. Steece, et al. Software Cost Estimation with COCOMO

II with CDROM. Prentice Hall PTR, 2000.

11. A. Capiluppi and D. Izquierdo-Cort´azar. Eﬀort estimation of FLOSS projects: a study

of the Linux kernel. Empirical Software Engineering, 18(1):60–88, 2013.

12. A. Capiluppi and M. Michlmayr. From the cathedral to the bazaar: An empirical study
of the lifecycle of volunteer community projects. In IFIP International Conference on
Open Source Systems, pages 31–44. Springer, 2007.

14 http://bitergia.com/

38

Gregorio Robles et al.

13. E. Capra, C. Francalanci, and F. Merlo. The economics of open source software: an
empirical analysis of maintenance costs. In Software Maintenance, 2007. ICSM 2007.
IEEE International Conference on, pages 395–404. IEEE, 2007.

14. E. Capra, C. Francalanci, and F. Merlo. An empirical study on the relationship between
software design quality, development eﬀort and governance in Open Source Projects.
Software Engineering, IEEE Transactions on, 34(6):765–782, 2008.

15. E. Capra, C. Francalanci, and F. Merlo. The economics of community open source
software projects: an empirical analysis of maintenance eﬀort. Advances in Software
Engineering, 2010, 2010.

16. K. Crowston and J. Howison. The social structure of free and open source software

development. First Monday, 10(2), 2005.

17. S. Due˜nas, V. Cosentino, G. Robles, and J. M. Gonzalez-Barahona. Perceval: Software
In Proceedings of the 40th International Conference on

project data at your will.
Software Engineering: Companion Proceeedings, pages 1–4, 2018.

18. R. Dumke and A. Abran. COSMIC Function Points: Theory and Advanced Practices.

Auerbach Publications, 2016.

19. J. Fernandez-Ramil, D. Izquierdo-Cortazar, and T. Mens. What does it take to develop
a million lines of Open Source code? In Open Source Ecosystems: Diverse Communities
Interacting, pages 170–184. Springer, 2009.

20. B. Fitzgerald. The transformation of Open Source Software. Mis Quarterly, pages

587–598, 2006.

21. J. M. Gonz´alez-Barahona and G. Robles. On the reproducibility of empirical software
engineering studies based on data retrieved from development repositories. Empirical
Software Engineering, 17(1-2):75–89, 2012.

22. S. H¨onel, M. Ericsson, W. L¨owe, and A. Wingkvist. A changeset-based approach to as-
sess source code density and developer eﬃcacy. In Proceedings of the 40th International
Conference on Software Engineering: Companion Proceeedings, pages 220–221, 2018.

23. Q. Hou, Y. Ma, J. Chen, and Y. Xu. An empirical study on inter-commit times in svn.

In SEKE, pages 132–137, 2014.

24. M. Jorgensen and M. Shepperd. A systematic review of software development cost

estimation studies. Software Engineering, IEEE Transactions on, 33(1):33–53, 2007.

25. E. Kalliamvakou, G. Gousios, K. Blincoe, L. Singer, D. M. German, and D. Damian.
The promises and perils of mining github. In Proceedings of the 11th working conference
on mining software repositories, pages 92–101, 2014.

26. E. Kalliamvakou, G. Gousios, D. Spinellis, and N. Pouloudi. Measuring developer con-

tribution from software repository data. MCIS, 2009:4th, 2009.

27. S. Koch. Proﬁling an open source project ecology and its programmers. Electronic

Markets, 14(2):77–88, 2004.

28. S. Koch. Eﬀort modeling and programmer participation in open source software

projects. Information Economics and Policy, 20(4):345–355, 2008.

29. S. Koch and G. Schneider. Eﬀort, co-operation and co-ordination in an open source

software project: GNOME. Information Systems Journal, 12(1):27–42, 2002.

30. C. Kolassa, D. Riehle, and M. A. Salim. The empirical commit frequency distribution
of open source projects. In Proceedings of the 9th International Symposium on Open
Collaboration, pages 1–8, 2013.

31. C. Kolassa, D. Riehle, and M. A. Salim. A model of the commit size distribution of
open source. In International Conference on Current Trends in Theory and Practice
of Computer Science, pages 52–66. Springer, 2013.

32. O. Kononenko, T. Rose, O. Baysal, M. Godfrey, D. Theisen, and B. De Water. Studying
pull request merges: a case study of shopify’s active merchant. In Proceedings of the 40th
International Conference on Software Engineering: Software Engineering in Practice,
pages 124–133, 2018.

33. E. Kouters, B. Vasilescu, A. Serebrenik, and M. G. van den Brand. Who’s who in
GNOME: Using LSA to merge software repository identities. In Software Maintenance
(ICSM), 2012 28th IEEE International Conference on, pages 592–595. IEEE, 2012.
34. J. Lerner and J. Tirole. Some simple economics of open source. The journal of industrial

economics, 50(2):197–234, 2002.

Development Eﬀort Estimation from Activity in Version Control Systems

39

35. Y. Ma, Y. Wu, and Y. Xu. Dynamics of open-source software developer’s commit
behavior: an empirical investigation of subversion. In Proceedings of the 29th Annual
ACM Symposium on Applied Computing, pages 1171–1173, 2014.

36. R. Malhotra and K. Lata. Using ensembles for class-imbalance problem to predict
maintainability of open source software. International Journal of Reliability, Quality
and Safety Engineering, page 2040011, 2020.

37. Q. Mi and J. Keung. An empirical analysis of reopened bugs based on open source
projects. In Proceedings of the 20th International Conference on Evaluation and As-
sessment in Software Engineering, pages 1–10, 2016.

38. M. Michlmayr, B. Fitzgerald, and K.-J. Stol. Why and how should open source projects

adopt time-based releases? IEEE Software, 32(2):55–63, 2015.

39. A. Mockus, R. T. Fielding, and J. D. Herbsleb. Two case studies of open source software
development: Apache and mozilla. ACM Transactions on Software Engineering and
Methodology (TOSEM), 11(3):309–346, 2002.

40. A. Mockus and L. G. Votta.

Identifying reasons for software changes using historic
databases. In Software Maintenance, 2000. Proceedings. International Conference on,
pages 120–130. IEEE, 2000.

41. D. Moulla and Kolyang. COCOMO model for software based on open source: Appli-
cation to the adaptation of triade to the university system. International Journal on
Computer Science and Engineering (IJCSE), 5(6):522–527, 2013.

42. D. K. Moulla, I. Damakoa, and D. T. Kolyang. Application of function points to software
based on open source: A case study.
In 2014 Joint Conference of the International
Workshop on Software Measurement and the International Conference on Software
Process and Product Measurement, pages 191–195. IEEE, 2014.

43. S. Porru, A. Murgia, S. Demeyer, M. Marchesi, and R. Tonelli. Estimating story points
from issue reports. In Proceedings of the The 12th International Conference on Predic-
tive Models and Data Analytics in Software Engineering, pages 1–10, 2016.

44. D. Riehle, P. Riemer, C. Kolassa, and M. Schmidt. Paid vs. volunteer work in open
source. In 2014 47th Hawaii International Conference on System Sciences, pages 3286–
3295. IEEE, 2014.

45. G. Robles and J. M. Gonzalez-Barahona. Developer identiﬁcation methods for inte-
grated data from various sources. ACM SIGSOFT Software Engineering Notes, 30(4):1–
5, 2005.

46. G. Robles, J. M. Gonz´alez-Barahona, C. Cervig´on, A. Capiluppi, and D. Izquierdo-
Cort´azar. Estimating development eﬀort in free/open source software projects by mining
software repositories: a case study of openstack. In Proceedings of the 11th Working
Conference on Mining Software Repositories, pages 222–231. ACM, 2014.

47. G. Robles, S. Koch, J. M. GonZ ´AlEZ-BARAHonA, and J. Carlos. Remote analysis and
measurement of libre software systems by means of the cvsanaly tool. In Proceedings of
the 2nd ICSE Workshop on Remote Analysis and Measurement of Software Systems
(RAMSS), pages 51–56. IET, 2004.

48. S. K. Shah. Motivation, governance, and the viability of hybrid forms in open source

software development. Management science, 52(7):1000–1014, 2006.

49. S. K. Sowe, I. Stamelos, and L. Angelis. Understanding knowledge sharing activities
in free/open source software projects: An empirical study. Journal of Systems and
Software, 81(3):431–446, 2008.

50. I. Steinmacher, T. Conte, M. A. Gerosa, and D. Redmiles. Social barriers faced by
newcomers placing their ﬁrst contribution in open source software projects.
In Pro-
ceedings of the 18th ACM conference on Computer supported cooperative work & social
computing, pages 1379–1392, 2015.

51. F. Thung. Automatic prediction of bug ﬁxing eﬀort measured by code churn size. In
Proceedings of the 5th International Workshop on Software Mining, pages 18–23, 2016.
52. G. Von Krogh, S. Spaeth, and K. R. Lakhani. Community, joining, and specialization in
open source software innovation: a case study. Research policy, 32(7):1217–1241, 2003.
53. I. S. Wiese, J. T. da Silva, I. Steinmacher, C. Treude, and M. A. Gerosa. Who is who in
the mailing list? comparing six disambiguation heuristics to identify multiple addresses
of a participant. In 2016 IEEE international conference on software maintenance and
evolution (ICSME), pages 345–355. IEEE, 2016.

40

Gregorio Robles et al.

54. H. Wu, L. Shi, C. Chen, Q. Wang, and B. Boehm. Maintenance eﬀort estimation for open
source software: A systematic literature review. In Software Maintenance and Evolution
(ICSME), 2016 IEEE International Conference on, pages 32–43. IEEE, 2016.

55. Y. Yang, M. Harman, J. Krinke, S. Islam, D. Binkley, Y. Zhou, and B. Xu. An empir-
ical study on dependence clusters for eﬀort-aware fault-proneness prediction. In 2016
31st IEEE/ACM International Conference on Automated Software Engineering (ASE),
pages 296–307. IEEE, 2016.

56. L. Yu. Indirectly predicting the maintenance eﬀort of Open-Source Software. Journal

of Software Maintenance and Evolution: Research and Practice, 18(5):311–332, 2006.

57. Y. Zhao, F. Zhang, E. Shihab, Y. Zou, and A. E. Hassan. How are discussions associated
with bug reworking? an empirical study on open source projects. In Proceedings of the
10th ACM/IEEE International Symposium on Empirical Software Engineering and
Measurement, pages 1–10, 2016.

