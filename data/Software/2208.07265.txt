2
2
0
2

g
u
A
5
1

]

G
L
.
s
c
[

1
v
5
6
2
7
0
.
8
0
2
2
:
v
i
X
r
a

Combining Gradients and Probabilities for Heterogeneous
Approximation of Neural Networks

Elias Trommer
elias.trommer@infineon.com
Infineon Technologies
Dresden, Germany

Bernd Waschneck
bernd.waschneck@infineon.com
Infineon Technologies
Dresden, Germany

Akash Kumar
akash.kumar@tu-dresden.de
Center for Advancing Electronics
Dresden (cfaed)
Dresden, Germany

ABSTRACT
This work explores the search for heterogeneous approximate multi-
plier configurations for neural networks that produce high accuracy
and low energy consumption. We discuss the validity of additive
Gaussian noise added to accurate neural network computations as
a surrogate model for behavioral simulation of approximate multi-
pliers. The continuous and differentiable properties of the solution
space spanned by the additive Gaussian noise model are used as a
heuristic that generates meaningful estimates of layer robustness
without the need for combinatorial optimization techniques. In-
stead, the amount of noise injected into the accurate computations
is learned during network training using backpropagation. A prob-
abilistic model of the multiplier error is presented to bridge the gap
between the domains; the model estimates the standard deviation of
the approximate multiplier error, connecting solutions in the addit-
ive Gaussian noise space to actual hardware instances. Our experi-
ments show that the combination of heterogeneous approximation
and neural network retraining reduces the energy consumption for
multiplications by 70% to 79% for different ResNet variants on the
CIFAR-10 dataset with a Top-1 accuracy loss below one percentage
point. For the more complex Tiny ImageNet task, our VGG16 model
achieves a 53 % reduction in energy consumption with a drop in
Top-5 accuracy of 0.5 percentage points. We further demonstrate
that our error model can predict the parameters of an approximate
multiplier in the context of the commonly used additive Gaussian
noise (AGN) model with high accuracy. Our software implementa-
tion is available under https://github.com/etrommer/agn-approx.

KEYWORDS
neural networks, approximate computing, energy efficiency

ACM Reference Format:
Elias Trommer, Bernd Waschneck, and Akash Kumar. 2022. Combining
Gradients and Probabilities for Heterogeneous Approximation of Neural
Networks. In IEEE/ACM International Conference on Computer-Aided Design
(ICCAD â€™22), October 30-November 3, 2022, San Diego, CA, USA. ACM, New
York, NY, USA, 9 pages. https://doi.org/10.1145/3508352.3549329

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ICCAD â€™22, October 30-November 3, 2022, San Diego, CA, USA
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9217-4/22/10. . . $15.00
https://doi.org/10.1145/3508352.3549329

1 INTRODUCTION
The power consumption of neural networks (NNs) has long been
a major obstacle for their deployment inside power-constrained
edge devices [37]. Particularly their computational complexity is
a concern. Approximate arithmetic units have been proposed by
the research community to address these issues [20, 24, 40]: relax-
ing the constraints imposed on the accuracy of operations enables
new optimizations of arithmetic hardware, allowing for improved
latency, energy consumption and area usage. Because of the dom-
inant impact of multiplications, most efforts have been focused on
improving the performance of multipliers using approximationâ€”a
rationale that this work builds upon as well.

Recent findings in the field of quantization demonstrate that op-
timizing quantization bit widths individually for each layer can
provide higher accuracy compared to solutions that uniformly
quantize the entire network. While some of these approaches rely
on reinforcement learning [7, 36], an alternative route is the optim-
ization of quantization parameters using gradient-based methods
during network training [19, 38, 39]. To the best of our knowledge,
the only work that investigates a non-uniform (i.e. heterogeneous)
approach to approximation of NNs is Mrazek et al. [25], which
uses a multi-objective genetic algorithm. By relying on behavioral
simulation to evaluate a large number of candidate solutions, this
method can not perform retraining, as it would render the already
lengthy search procedure computationally intractable. Recent work
by De la Parra et al. [3] demonstrates that solutions employing
a single approximate multiplier (AM) throughout the entire net-
work can outperform heterogeneous solutions if lost accuracy is re-
covered using retraining. Naturally, this raises the question whether
performance could be improved even further if heterogeneous ap-
proximation and retraining were combined. It is obvious that this
requires a different approach to finding a heterogeneous multiplier
configuration in order for the search procedure to be feasible. To
address this problem, we propose a gradient-based search algorithm
that is capable of finding high-quality heterogeneous AM configur-
ations. The search results in only a single candidate solution (or a
small set acquired by varying a hyperparameter) that can easily be
retrained. The key novel contributions of this work are:

â€¢ An efficient method for jointly determining the robustness
to approximate multiplications for each NN layer during
training. Through transformation into a more favorable solu-
tion space, our algorithm allows for a fast traversal of the
heterogeneous multiplier assignment problem in a NN.

 
 
 
 
 
 
ICCAD â€™22, October 30-November 3, 2022, San Diego, CA, USA

Elias Trommer, Bernd Waschneck, and Akash Kumar

â€¢ A probabilistic error model that can give a precise estimate
of the performance of an AM in a NN layer in terms of re-
coverable and non-recoverable error. Besides knowledge of
the difference between the accurate and approximate multi-
plication results for all operand combinations, no behavioral
simulation is required. The model is data-driven and does not
assume any particular distribution of input operands. This
makes its performance agnostic to methods that impact these
distributions such as pruning [10, 33], quantization [41], etc.

Our evaluation of several convolutional neural network (CNN)
models on the CIFAR-10 and Tiny ImageNet datasets shows that
our method consistently manages to push the boundary of energy
efficiency and network performance for various networks. We also
improve upon existing models that express AM properties as addit-
ive Gaussian noise (AGN) parameters. This boosts the accuracy of
AGN as a faster and simpler replacement for behavioral simulation
of AMs.

2 BACKGROUND AND RELATED WORK
Due to the significant complexity of accurately simulating AMs
during the training procedure of NNs [4, 35], several works propose
the use of random noise as a replacement for the inaccurate com-
putations. Hammad et al. [9] use a model based on the multiplierâ€™s
Mean Relative Error (MRE) to generate random data which per-
turbs the output of an accurate computation. To enable retraining
without the need for hardware simulation, De la Parra et al. [5] pro-
pose a data-driven noise model with higher granularity. The model
constructs a noise tensor individually for each neuron by observing
the approximation error on sample data. Similar to behavioral simu-
lation, this model is not generalizable across AM instances because
it only captures the dynamics of the AM it was constructed for.

Noisy intermediate results have also been used to demonstrate
that the robustness in a NN is not uniform but varies for different
layers [2]. These findings are corroborated by Hanif et al. [11], who
determine the individual robustness of layers to approximation by
injecting AGN into individual layers and observing the change in
accuracy. By optimizing one layer at a time, the work does not
take into account interdependencies between the robustness of
layers. It also lacks a method that connects the robustness of a
given layer to a concrete hardware instance. A similar method in
the context of Capsule Networks is discussed by Marchisio et al.
[21]. The proposed model for connecting a layerâ€™s robustness to
noise with AM instances requires Monte Carlo (MC) simulations
to be carried out for each combination of layer and AM. Other
models that describe the error incurred by a single approximate
multiplication were put forward by Mazahir et al. [22] and Ullah
et al. [34]. These models, however, do not consider the compounding
effects of multiple operations in a neural network.

To leverage the varying robustness of layers Mrazek et al. [25]
demonstrate the use of a multi-objective evolutionary algorithm
as a means to tackle the large search space of heterogeneous mul-
tiplier assignment in a NN. This solution relies on the evaluation
of numerous candidate solutions, requiring the use of a weight
remapping scheme rather than retraining to recover the degraded
accuracy. Even without retraining, the vast number of simulations
makes the method prohibitively slow for non-trivial networks. The

same is true for more recent approaches that optimize the network
architecture itself for use with AMs [27].

3 PROPOSED METHODOLOGY
We provide an analysis of the aggregate error at a neuronâ€™s output
and conclude that AGN is a meaningful surrogate model for beha-
vioral simulation of AMs. Using the properties of this model, we
demonstrate how to simultaneously optimize the amount of AGN
across the entire network, considering the complex interactions
between perturbations in different layers. An error model is de-
veloped in order to make the abstract AGN parameter comparable
with the computation errors exhibited by concrete AM instances.
Using the learned robustness parameters and the error model, we
can determine which AMs will produce the required accuracy and
match appropriate AM instances based on each layerâ€™s individual
sensitivity.

3.1 Modeling approximate multiplication as

noise

The error imposed by a single approximate multiplication can be
considered additive to the output of an accurate multiplication

Ëœğ‘“ (ğ‘¥, ğ‘¤) = ğ‘¥ Â· ğ‘¤ + ğ‘’ (ğ‘¥, ğ‘¤)

(1)

where ğ‘’ (ğ‘¥, ğ‘¤) is an error function that is unique to each AM in-
stance. For a NN application we are, however, not concerned with
the error of each individual operation, but with the aggregate er-
ror over several multiplications. With the definition of the pre-
activation output of an accurate neuron

ğ‘¦ =

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘¥ğ‘–ğ‘¤ğ‘– + ğ‘

(2)

Equation (1) can be substituted to obtain the output of the same
neuron using approximate multiplication as:

ğ‘›
âˆ‘ï¸

Ëœğ‘¦ =

ğ‘¥ğ‘–ğ‘¤ğ‘– + ğ‘’ (ğ‘¥ğ‘–, ğ‘¤ğ‘– ) + ğ‘

ğ‘–=1
ğ‘›
âˆ‘ï¸

=

ğ‘¥ğ‘–ğ‘¤ğ‘– + ğ‘

+

ğ‘›
âˆ‘ï¸

ğ‘’ (ğ‘¥ğ‘–, ğ‘¤ğ‘– )

(3)

(4)

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

ğ‘–=1
(cid:125)
(cid:123)(cid:122)
(cid:124)
Accurate Neuron

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

ğ‘–=1
(cid:124)
(cid:125)
(cid:123)(cid:122)
Aggregate Error

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

Assuming that ğ‘¥ and ğ‘¤ exhibit sufficiently random properties, we
conjecture that, as ğ‘› grows, the distribution of the aggregate error
will converge to a normal distribution, thus:

Ëœğ‘¦ â‰ˆ

=

ğ‘›
âˆ‘ï¸

ğ‘–=1
ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘¥ğ‘–ğ‘¤ğ‘– + ğ‘ + N (ğœ‡ğ‘’, ğœğ‘’ )

ğ‘¥ğ‘–ğ‘¤ğ‘– + ğ‘ + ğœ‡ğ‘’ +

âˆš

ğœğ‘’ Â· N (0, 1)

(5)

(6)

Furthermore, the systematic portion of the error ğœ‡ğ‘’ will be ab-
sorbed by the bias or subsequent batch normalization for any non-
degenerate case when the network is retrained to match the ap-
proximate configuration [31] s.t. ğ‘ â€² = ğ‘ âˆ’ ğœ‡ğ‘’ . To simulate the effect
of approximation on a fully retrained network, we can therefore
assume that ğœ‡ğ‘’ = 0.

Combining Gradients and Probabilities for Heterogeneous Approximation of Neural Networks

ICCAD â€™22, October 30-November 3, 2022, San Diego, CA, USA

In the context of this model, the task loss Lğ‘‡ can be differentiated
directly w.r.t ğœğ‘™ using the chain rule:

Â·

ğœ• (cid:174)Ëœğ‘¦ğ‘™
ğœ•ğœğ‘™

=

=

(8)

(9)

ğœ•Lğ‘‡
ğœ•ğœğ‘™

Â· ğœ ( (cid:174)ğ‘¦ğ‘™ ) Â· (cid:174)ğ‘

ğœ•Lğ‘‡
ğœ• (cid:174)Ëœğ‘¦ğ‘™
ğœ•Lğ‘‡
ğœ• (cid:174)Ëœğ‘¦ğ‘™
Because this formulation is continuous and differentiable, it is pos-
sible to optimize all layer perturbations simultaneouslyâ€”just like all
other network parametersâ€”using backpropagation. Computation
errors in an early layer can impact the overall accuracy dispropor-
tionately as they might alter the results in all subsequent layers;
these complex dynamics of propagating errors are already captured
by this model because of the chained computation of gradients.
The loss function is evaluated based on layers being perturbed by a
certain amount of AGN, resulting in model convergence towards
higher robustness to small perturbations. Through the added noise,
the output of each individual neuron will be less reliable. Informa-
tion relevant to the modelâ€™s task will therefore have to be spread
out over more neurons when propagating through the network,
similar to the popular Dropout regularization [30]. We assume that
this increased robustness will also be beneficial when deploying
AMs: because the model has learned to be less reliant on the pre-
cise output of individual neurons, it will be able to handle slight
deviations in the intermediate results caused by AMs better.

Only using the task loss to drive model optimization is not suffi-
cient, however, since ğœğ‘™ could always be driven towards zero. To
avoid this, we introduce an additional noise loss Lğ‘ that incentiv-
izes the optimizer to explore solutions with higher perturbation.
The noise loss takes into account the current values of ğœğ‘™ as well as
the relative cost of each layer ğ‘ğ‘™ = ğ‘ (ğ‘™)/(cid:205)ğ‘™ âˆˆğ¿ ğ‘ (ğ‘™). The relative cost
scales the importance of the amount of perturbation in each layer;
it is clear that we care most about high values of ğœğ‘™ in layers with a
high complexity, while it is preferable to allow for higher relative
accuracy in layers that do not contribute much to overall resource
consumption. We choose the amount of multiplications in a layer as
an easy to implement cost function ğ‘ (ğ‘™). The additional noise loss
solves the problem of the optimizer converging to solutions with no
AGN, but it creates a new one: The optimizer could now decrease
the total loss indefinitely by adding ever-growing amounts of noise
to the intermediate results, making the task loss insignificant. We
avoid this by upper-bounding the maximum allowable noise loss
to some fixed value ğœmax. This gives us the total noise loss as

Lğ‘ = âˆ’

âˆ‘ï¸

ğ‘™ âˆˆğ¿

min {|ğœğ‘™ |, ğœmax} Â· ğ‘ğ‘™

(10)

The overall loss L is then simply the weighted sum of task and
noise loss

L = Lğ‘‡ + ğœ† Â· Lğ‘

(11)

where ğœ† is a hyperparameter that determines the relative import-
ance of network accuracy and perturbation. Equation (10) can be
differentiated w.r.t. ğœğ‘™ as

ğœ•Lğ‘
ğœ•ğœğ‘™

(cid:40)

âˆ’ğ‘ğ‘™ ,
0,

=

|ğœğ‘™ | â‰¤ ğœmax
otherwise

(12)

Figure 1: Substituting behavioral simulation of approxim-
ate multiplication with parameterized injection of AGN in
a single layer (bias/batch normalization omitted for clarity).
Highlighted operations mark backward pass of gradient for
ğœğ‘™ w.r.t. task loss.

Using this surrogate model instead of a behavioral simulation of
the approximation error has two important benefits for the search
procedure: First, AGN can easily be constructed using primitives
that are available in most common deep learning toolkits and does
not require low-level integration of handwritten kernels. Instead, it
leverages the highly optimized accurate kernel implementations
maintained by the framework and therefore adds very little runtime
overhead to the training procedure. Secondly, instead of selecting
instances with unique characteristics out of a discrete (and poten-
tially large) set of AMs, the AGN model captures their most relevant
propertyâ€”the non-recoverable errorâ€”in a single, continuous para-
meter. Only after solving the optimization problem in the more
favorable AGN space, the found error robustness is used to select
matching hardware instances.

3.2 Gradient-based Robustness Optimization
Every layer in our network has a perturbation factor ğœğ‘™ which
determines the amount of noise that is added to its accurate output.
For each batch, the amount of perturbation is scaled by the standard
deviation of the batch as proposed by Marchisio et al. [21]. The
relative scaling avoids a situation where the effect of noise of a
fixed magnitude would be diminished during training through the
network optimizing towards greater pre-activation outputs [8]. To
perturb the accurate calculation (cid:174)ğ‘¦ğ‘™ âˆˆ Rğ‘š of a layer ğ‘™, a tensor with
random values of the same dimensions (cid:174)ğ‘ âˆ¼ Nğ‘š (0, 1) is drawn from
a normal distribution with a mean of zero and a standard deviation
of one. The standard deviation still needs to be adjusted to match
the standard deviation of the current batch; to achieve this relative
scaling, the tensor is multiplied with the standard deviation of the
accurate computationâ€™s output for the current batch ğœ ( (cid:174)ğ‘¦ğ‘™ ) and then
weighted with the layerâ€™s perturbation factor ğœğ‘™ before being added
to the result of the accurate computation as shown in Figure 1.

(cid:174)Ëœğ‘¦ğ‘™ = (cid:174)ğ‘¦ğ‘™ + ğœğ‘™ Â· ğœ ( (cid:174)ğ‘¦ğ‘™ ) Â· (cid:174)ğ‘

(7)

ActivationsÃ—ÌƒÃ—Ã—+ApproximateInferenceAGN modelWeightsActivationÏƒÏƒlActivationsWeightsICCAD â€™22, October 30-November 3, 2022, San Diego, CA, USA

Elias Trommer, Bernd Waschneck, and Akash Kumar

After optimizing the amount of injected noise per layer we arrive
at a configuration where each layerâ€™s learned robustness to noise ğœğ‘™
has been tuned to maximize the amount of overall AGN throughout
the network while minimizing the degradation of accuracy. Each
layerâ€™s ğœğ‘™ can be considered a proxy for the actual (but much harder)
optimization task: determining how sensitive the networkâ€™s overall
performance is to inaccurate computation results in each layer

3.3 Probabilistic Multi-Distribution Error

Model

An important question that arises is how the abstract optimiza-
tion factor ğœğ‘™ in the AGN model relates to the error produced by a
given AM. In order to link both, we propose the use of a probabil-
istic model that treats each error as a random event with certain
probabilities; instead of truthfully simulating the error function for
each pair of input values, its output can be considered a discrete
random variable ğ‘ = ğ‘’ (ğ‘¥, ğ‘¤) that maps the outcome of randomly
selecting ğ‘¥ and ğ‘¤ to the error produced by these operands. Determ-
ining the standard deviation of ğ‘ makes it comparable with the
learned perturbation factor ğœğ‘™ that simulates the non-recoverable
portion of the approximation error in the AGN space. Assuming
the commonly used 8-bit Integer multipliers, both ğ‘¥ and ğ‘¤ can take
28 = 256 distinct values for their sample spaces Î©ğ‘¥ and Î©ğ‘¤. In
total, there are 2562 possible combinations of input values in the
joint sample space Î©ğ‘ = Î©ğ‘¥ Ã— Î©ğ‘¤. For any multiplier of interest,
we need to know the error (i.e. the difference between accurate and
approximate output) for each of these input combinations, from
hereon referred to as the multiplierâ€™s error map. This is the only
part of the method that requires at least a high-level model of the
hardware to be simulated. If simulating the hardware is expensive,
this can be done once and the resulting error map stored for later
use.

Next, we need to consider that not all 256 values that ğ‘¥ and ğ‘¤
might take are equally likely to appear. The relative frequencies of
values in the weights tensor and an input sample can be used to
model their probability distributions ğ‘ğ‘¥ (ğ‘¥) and ğ‘ğ‘¤ (ğ‘¤). In practice,
this means building a histogram with the count of each possible
8-bit Integer value for both tensors and normalizing it to one. The
data-driven construction of operand probabilities means that no
assumptions have to be made with regards to their underlying
distribution. Given the error for each combination of values and
their respective likelihoods, mean and standard deviation of the
error can be estimated as

ğœ‡ğ‘ =

ğœ 2
ğ‘ =

âˆ‘ï¸

âˆ‘ï¸

ğ‘¥ âˆˆÎ©ğ‘¥
âˆ‘ï¸

ğ‘¤ âˆˆÎ©ğ‘¤
âˆ‘ï¸

ğ‘¥ âˆˆÎ©ğ‘¥

ğ‘¤ âˆˆÎ©ğ‘¤

ğ‘ğ‘¥ (ğ‘¥) Â· ğ‘ğ‘¤ (ğ‘¤) Â· ğ‘’ (ğ‘¥, ğ‘¤)

(13)

ğ‘ğ‘¥ (ğ‘¥) Â· ğ‘ğ‘¤ (ğ‘¤) Â· (ğ‘’ (ğ‘¥, ğ‘¤) âˆ’ ğœ‡ğ‘ )2

(14)

which describes the mean and standard deviation of the error
for a single multiplication. From Equation (2), it is clear that the
neuron output is the sum over ğ‘› multiplications. ğ‘› is the fan-in of
the neuron, or more simply the number of incoming connections.
For CNNs and most other common NN architectures the fan-in is
identical for all neurons in a layer. Under the assumption that ğ‘¥ and
ğ‘¤ are independent and identically distributed (an assumption that
we will address in more detail in the next paragraph), we can apply

Figure 2: Process of deriving a global estimate of multiplier
error properties from multiple local estimates.

the central limit theorem to the growth of the standard deviation.
With this, the error mean at the neuron output scales linearly, while
the standard deviation scales with the square root of the fan-in:

ğœ‡ğ‘’ = ğ‘› Â· ğœ‡ğ‘
âˆš
ğœğ‘’ =

ğ‘› Â· ğœğ‘

The second observation is important because it implies that layers
with a higher fan-in will produce an error that is smaller relative to
the magnitude of the output value (which grows with ğ‘›), all other
factors being equal.

So far, we have assumed ğ‘¥ and ğ‘¤ to be independent and identic-
ally distributed. Empirically validating this assumption shows that
it holds well for weights, but not for activations. Intuitively, this
can be explained by a higher amount of local correlations in the
activations. To make this clearer, we can imagine a black and white
image passing through a convolutional layer: because pixels of the
same color tend to be grouped together, the individual patches are
more likely to be all-black or all-white than the global distribution
of pixels in the image would suggest. More concisely, the local dis-
tribution of feature values can deviate strongly from their global
distribution.

The problem of diverging local and global distributions of oper-
and values can be addressed through sampling; mean and standard
deviation are first calculated for several samples of the local distri-
bution. The sample is drawn from the receptive field of a neuron
in the target layer, i.e. a sample is either a randomly selected input
feature vector for Fully-Connected (FC) layers or a patch for convo-
lutional layers. These individual observations are then integrated
into an estimate of the global distributionâ€™s mean and standard de-
viation as shown in Figure 2. We randomly sample ğ‘˜ vectors from
the layerâ€™s input activations. For each input vector, a frequency
distribution ğ‘ğ‘¥ (ğ‘¥) is generated. Equations (13) and (14) are then
used to calculate its mean and standard deviation. As an additional

TargetMultipliersm1â€¦Local EstimateGlobalEstimateRepeat for k sample inputsActivationsWeightsActivation Freq.Weight Freq.Error Mapm2m3mnCombining Gradients and Probabilities for Heterogeneous Approximation of Neural Networks

ICCAD â€™22, October 30-November 3, 2022, San Diego, CA, USA

benefit, this removes the need to build a global histogram of the
input value distribution in favor of building multiple distributions
on very small input samples. Calculating the combined mean of the
local distributions is simple. Special care needs to be taken when
combining group standard deviations: Our method must account
for the effect of different means in each sample on the combined
standard deviation [17].

ğœ‡ğ‘ =

ğœ 2
ğ‘ =

1
ğ‘˜

1
ğ‘˜

ğ‘˜
âˆ‘ï¸

ğœ‡ğ‘ğ‘–

ğ‘–=1
ğ‘˜
ï£®
âˆ‘ï¸
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ğ‘–=1

(cid:16)
ğœ 2
ğ‘ğ‘–

+ ğœ‡2
ğ‘ğ‘–

(cid:17)

âˆ’

1
ğ‘˜

(cid:32) ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğœ‡ğ‘ğ‘–

(cid:33) 2ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

(15)

(16)

The degree with which this method is applicable is determined by
the fan-in ğ‘› of the respective layer, as convergence of the error
towards a normal distribution depends primarily on the fan-in. If
layers with a very low fan-in were present, some care would have
to be taken to confirm that the AGN model is still valid.

3.4 Multiplier Matching
Our method focuses on Integer AMs with a low bit width which are
typically used in low-energy inference settings like edge devices and
accelerators. Because of the small amount of operand combinations,
a full error map should normally be easy to obtain. To increase the
degrees of freedom of the solution, having a large set of different
AMs that cover a wide range of accuracy and resource consumption
is desirable. As long as these conditions are met, our method is
applicable to many different AMs designs.

With an optimized configuration of ğœğ‘™ as a measure of a layerâ€™s
sensitivity, we can match an appropriate AM to each layer. For
every AM in our search space, we calculate an estimate of the error
it produces using the method described in detail in Section 3.3.
The estimate produced by the error model is directly comparable
to the learned ğœğ‘™ . The multipliers for which the error lies above
the accuracy threshold ğœğ‘™ are discarded since they do not produce
the required accuracy. From the remaining set of multipliers that
produce sufficiently accurate results, we can pick the one that
optimizes another metric of interest.

4 RESULTS AND DISCUSSION
Unless stated otherwise, all experiments were carried out on a
host system equipped with an AMD Ryzen 9 3900X CPU and
a single nVidia RTX 2080Ti GPU. We implement our search al-
gorithm [32] as an extension of the popular deep learning frame-
work PyTorch [26].

4.1 Multi-Distribution Error Model
In order to assess the accuracy with which our error model can infer
the standard deviation of the approximation error on the layerâ€™s
output, we evaluate its performance on the layers of a ResNet8
model. The model is trained using the parameters laid out in Sec-
tion 4.2. We do not explicitly evaluate the estimate of the error
mean, as it is less relevant to our method for the reasons discussed
in Section 3.1. From Equation (14), it should be clear that an ac-
curate estimate of the error mean is a necessary condition for an

Table 1: Comparison of predictive methods for multiplier
error standard deviation on ResNet8 layers

Error Model

Pearson Correlation

Multiplier MRE [9]
Single-Distribution MC [21]
Probabilistic Multi-Dist. (ours)

0.546
0.767
0.997

Median Relative Error
Â± Interquartile Range

n.a.
(42.9 Â± 53.2) %
(4.6 Â± 8.8) %

accurate estimate of the standard deviation. The ground truth for
the approximation error is obtained through a behavioral simula-
tion of all 13 unsigned multipliers from the EvoApprox library [23].
The simulation generates the approximated pre-activation output
of each layer, given its weights and activations. We compare the
measured errorâ€™s standard deviation to the estimate of the standard
deviation generated by our probabilistic multi-distribution error
model for ğ‘˜ = 512 input samples. The results are also compared to
the single-distribution MC method discussed in [21]. We further
include the respective multiplierâ€™s MRE in the evaluation as it is a
commonly proposed [3, 9] single-value metric, used as an indica-
tion of multiplier performance. The results of our evaluation can
be found in Table 1.

We find that the results of the Single-Distribution MC method
are very similar to the results produced by using our probabilistic
method without accounting for differing local distributions, i.e.
based on the global frequency distribution of activation values alone.
Single-Distribution MC and our probabilistic model converging to
similar results in this scenario is very plausible, given that the
former method is a MC simulation of exactly the same process
that is analytically described in the latter. This suggests that taking
local divergence of the input activations into account provides
a significant boost in the predictive performance of our model,
compared to methods that assume the global distribution to be
present for all operations. The observed values for the standard
deviation occupy a very wide numerical range of approximately
5 orders of magnitude. On such a large range, a median relative
error of 4.6 % is negligible, which is reflected in the almost perfect
correlation of 0.997.

Our evaluation shows that the predictive performance of the
MRE for the errorâ€™s standard deviation is very limited. This can be
explained by the fact that the MRE is only a metric of a multiplier
performance over the entire numerical range and for operands
with equal probabilities. It does not distinguish between systematic
and non-systematic portions of the error and can include neither
knowledge about the different likelihoods of operands in NN models
nor the impact of a layerâ€™s fan-in on the aggregate error at the layer
output.

4.2 ResNet on CIFAR-10
In line with previous works [3, 25], our method is evaluated on sev-
eral sizes of the ResNet [12] architecture, trained on the CIFAR-10
data set [15] with 36 8-bit unsigned multipliers from the EvoApprox
library [23] as the AM search space. Relative power numbers were
generated using the pdk45_pwr property of the EvoApprox multipli-
ers, normalized to the power consumption of the reference accurate

ICCAD â€™22, October 30-November 3, 2022, San Diego, CA, USA

Elias Trommer, Bernd Waschneck, and Akash Kumar

Table 2: Comparison of energy reduction and accuracy loss
for different methods

Model

Method

Energy Reduction

Top-1 Accuracy
Loss [p.p.]

ResNet8

ResNet14

ResNet20

ALWANN [25]
Uniform Retraining [3]
Gradient Search (ours)

ALWANN [25]
Uniform Retraining [3]
Gradient Search (ours)

LVRM [31]
Uniform Retraining [3]
Gradient Search (ours)

ResNet32

Gradient Search (ours)

30 %
58 %
70 %

30 %
57 %
75 %

17 %
53 %
71 %

79 %

1.7
0.9
0.5

0.9
0.9
0.9

1.0
0.7
0.9

0.8

multiplier and weighted with the number of multiplications in each
layer.

The training and augmentation procedure laid out in the ori-
ginal work is used to generate a floating-point reference model.
Next, Quantization-aware Training (QAT) is carried out to obtain a
baseline model that is quantized to 8 Bit. On the quantized baseline,
we uniformly initialize each layerâ€™s perturbation factor ğœğ‘™ to 0.1
with a cap at ğœmax = 0.5. Layer perturbation factors and other
network parameters are then jointly optimized for 30 epochs using
the Stochastic Gradient Descent (SGD) optimizer with an initial
learning rate of 1 Ã— 10âˆ’2 and a decay of 0.9 after every 10 epochs.
After matching AMs with appropriate accuracy to each layer, we
retrain the entire network using behavioral simulation of the se-
lected AMs for another 5 epochs with an initial learning rate of
1 Ã— 10âˆ’3 and a decay of 0.9 after every other epoch. During this
phase, the Straight-Through Estimator (STE) [1, 13] is used to de-
rive valid gradients for the AMs. We repeat the Gradient Search and
retraining several times while varying the ğœ† parameter between 0
and 0.6 in steps of 0.05. By adjusting the value of ğœ†, we can gen-
erate several solutions, each of which strikes a different balance
between accuracy and perturbation (and thus, by extension, energy
consumption).

In the comparison of results in Figure 3 it can be seen that the
accuracy is above the baseline for all ResNet variants for an en-
ergy reduction of up to 45%. We attribute this to the perturbation
acting as a form of learnable regularization; since all parameters
are trained in the Gradient Search phase, the network converges
towards a configuration that is both more resilient to approxima-
tion and generalizes better to unseen data. The drop in accuracy
for higher degrees of approximation becomes increasingly steep
for deeper models, most likely due to the accumulating effect of the
approximation error.

The comparison of the accuracy in the AGN space (â€˜AGN Modelâ€™)
with the accuracy that is achieved after retraining using behavioral
simulation (â€˜Approx., Gradient Search Weightsâ€™) in Figure 4 reveals
another interesting property: the accuracy of a retrained model
under approximation is very similar to the accuracy exhibited by
the model that is perturbed using AGN up to energy savings of
around 60 %. For more aggressive approximation, the relationship

Figure 3: Pareto Front and dominated points of energy and
accuracy for different sizes of ResNet [12] on the CIFAR-10
data set [15]. Horizontal line marks 8-bit quantized baseline
accuracy.

Figure 4: Comparison of perturbed accuracy and accuracy
after retraining with weights and biases of AGN model and
baseline model for ResNet20. Horizontal line marks 8-bit
quantized baseline accuracy.

between both values deteriorates up to a gap of several percentage
points. These models suffer from a significant degradation of accur-
acy under approximation, while the same is not true for the same
model perturbed with comparable amounts of AGN. This can be
interpreted as a shortcoming of the AGN model: AMs with very
low energy consumption are more likely to produce localized error
patterns which are insufficiently captured by the AGN model. The
AGN model assumes an even spread of errors over all neurons. It
is likely that a structured error has a more adverse effect on the
propagation of information for some specific connections, widening
the gap between predicted and achieved accuracy.

Figure 4 also provides some insight into the question of whether
training in the AGN space has a positive carryover to model ac-
curacy when deploying AMs. To make the impact of training with
AGN on the achieved accuracy visible, we repeat the retraining
using behavioral simulation based on the weights and biases of
the baseline model (â€˜Approx., Baseline Weightsâ€™) instead of the
weights and biases learned during the Gradient Search phase. We
can see that the models trained using AGN consistently achieve

102030405060708090100Relative Energy Consumption for Multiplications [%]86889092Top-1 Accuracy [%]ResNet8ResNet14ResNet20ResNet32102030405060708090100Relative Energy Consumption for Multiplications [%]8590Top-1 Accuracy [%]AGN ModelApprox., Gradient Search WeightsApprox., Baseline WeightsCombining Gradients and Probabilities for Heterogeneous Approximation of Neural Networks

ICCAD â€™22, October 30-November 3, 2022, San Diego, CA, USA

satisfactory results when optimizing the layer perturbations. Only
a single run of Gradient Search with ğœ† = 0.3 and an initial perturb-
ation factor of ğœğ‘™ = 0.025 for all layers is performed to account
for the larger dataset. We also lower the amount of epochs for the
Gradient Search and approximate retraining to 9 and 2 respectively.
Based on the result in the AGN space, both the signed and unsigned
8-bit multipliers from the EvoApprox library are used separately as
search spaces for matching AM instances.

Results in Table 3 show that heterogeneous configurations out-
perform uniform solutions when trading off energy savings and
loss of accuracy. Given the more complex classification task, we do
not find an improvement over the accuracy of the baseline model
anymore, both in the AGN space as well as after deployment of AMs
and retraining. The much lower reduction of energy consumption
when using signed multipliers can be attributed their lower overall
energy reduction for similar performance as well as the smaller
search space of only 13 available signed 8-bit multipliers.

For the unsigned solution, the search procedure identifies 13
different AMs from the search space. The comparison of layer com-
plexity and the respective reduction in energy consumption per
layer in Figure 5 shows that the Gradient Search and subsequent
multiplier assignment deploys the highest degrees of approximation
on the networkâ€™s inner layers with high amounts of multiplications.
In contrast, particularly the first and last layers are assigned highly
accurate hardware instances. A relatively high sensitivity of these
layers is in line with common heuristics in non-uniform quantiza-
tion schemes [36].

We confirm that this effect is consistent by repeating the experi-
ment with other CNN models. Performing the same optimization
procedure on AlexNet [16] and MobileNetV2 [28] models yields
similar results: Inner layers with higher computational complexity
are assigned less accurate hardware instances with up to 84.4 %
reduction in energy consumption. In contrast, highly accurate AMs
with a reduction between 1.3 % and 5.4 % are mapped to the first
and last layers of each model.

5 CONCLUSION
In this work, we have demonstrated the feasibility of combining
heterogeneous approximation and retraining for NNs. The use of
AGN as a surrogate model for behavioral simulation is motivated
from the mathematical formulation of NNs. Our findings suggest
that there is a strong connection between perturbation of neural
networks with AGN and low to medium degrees of approximation.
This connection, however, weakens for higher degrees of approx-
imation. Whether the AGN model can be adapted to capture more
structured errors without sacrificing generalizability remains to be
answered by further research. Despite this, we show that the trade-
off between energy and accuracy can be improved consistently
and significantly by considering the varying robustness of different
parts of a NN during the deployment of AMs. As a means to tackle
the enormous search space of heterogeneous AM assignment, the
flexibility of the AGN model is used to abstract over many different
AMs with only a single parameter. This parameter is differentiable,
so it can be learned using backpropagation. Learning the different
degrees of robustness to errors during network training is the key
to efficiently exploring different solutions, especially for deeper

Figure 5: Comparison of energy reduction per layer and rel-
ative multiplications for VGG16, based on output configura-
tion of unsigned EvoApprox multipliers

Table 3: Comparison of homogeneous and heterogeneous
solutions for VGG16 [29] architecture on the Tiny ImageNet
dataset [18]

Configuration

Energy Reduction

Top-5 Val. Accuracy

Baseline
AGN Model, ğœ† = 0.3

Uniform Retraining, mul8s_1KVB
Heterogeneous, signed

Uniform Retraining, mul8u_19DB
Uniform Retraining, mul8u_185Q
Heterogeneous, unsigned

n.a.
n.a.

3.5 %
11.6 %

52.7 %
52.7 %
52.6 %

80.6 %
80.2 %

80.3 %
80.5 %

79.6 %
79.7 %
80.1 %

higher accuracies after retraining using behavioral simulation com-
pared to models that directly attempt approximate retraining on
the baseline model. This indicates that models trained using AGN
have converged to a configuration that is more robust to the errors
produced by approximation.

A comparison of the loss of accuracy and reduction in energy con-
sumption to other state-of-the-art methods can be found in Table 2.
In addition to improved energy efficiency, our algorithm adds little
runtime overhead. The Gradient Search only takes between 6 min
for ResNet8 and 16 min for ResNet32. Across all networks, this
puts the overhead between 41 % to 45 % of the time taken to train
the respective floating-point reference networksâ€”much lower than
the hours to days required by Mrazek et al. [25]. The multiplier
matching algorithm that generates the estimated error standard
deviation for each combination of layer and multiplier completes in
around one minute for all surveyed networks on our target system.

4.3 VGG16 on Tiny ImageNet
To demonstrate the scalability of our method, we apply it to the
more complex Tiny ImageNet dataset [18]. Tiny ImageNet is a
smaller version of the ImageNet challenge [6] in which the num-
ber of classes is reduced from 1000 to 200. Each class contains
500 training and 50 validation images and images are down-scaled
from 224 Ã— 224 pixels to 64 Ã— 64 pixels. The VGG16 CNN architec-
ture [29] with additional batch normalization is used as a reference
architecture. The SGD optimizer is used for all training runs as
we find that the popular ADAM optimizer [14] does not produce

Layers20%0%100%Energy ReductionMultiplicationsICCAD â€™22, October 30-November 3, 2022, San Diego, CA, USA

Elias Trommer, Bernd Waschneck, and Akash Kumar

networks. To connect AGN to concrete hardware, we introduced a
probabilistic model of the approximation error. This model allows
for an accurate prediction of the noise properties which the AM
error will exhibit on any given layer without having to rely on
time-consuming simulations. Through combining these methods,
we provide a path towards NN hardware that makes inference both
accurate and power-efficient.

REFERENCES
[1] Yoshua Bengio, Nicholas LÃ©onard, and Aaron C. Courville. 2013. Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation.
CoRR abs/1308.3432 (2013).

[2] Nicholas Cheney, Martin Schrimpf, and Gabriel Kreiman. 2017. On the Robust-
ness of Convolutional Neural Networks to Internal Architecture and Weight
Perturbations. CoRR abs/1703.08245 (2017).

[3] Cecilia De la Parra, Andre Guntoro, and Akash Kumar. 2020. Full Approximation
of Deep Neural Networks through Efficient Optimization. In IEEE International
Symposium on Circuits and Systems, ISCAS 2020, Sevilla, Spain, October 10-21,
2020.

[4] Cecilia De la Parra, Andre Guntoro, and Akash Kumar. 2020. ProxSim: GPU-
based Simulation Framework for Cross-Layer Approximate DNN Optimization.
In 2020 Design, Automation & Test in Europe Conference & Exhibition, DATE 2020,
Grenoble, France, March 9-13, 2020.

[5] Cecilia De la Parra, Andre Guntoro, and Akash Kumar. 2021. Efficient Accuracy
Recovery in Approximate Neural Networks by Systematic Error Modelling. In
ASPDAC â€™21: 26th Asia and South Pacific Design Automation Conference, Tokyo,
Japan, January 18-21, 2021.

[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im-
ageNet: A large-scale hierarchical image database. In 2009 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June
2009, Miami, Florida, USA.

[7] Ahmed T. Elthakeb, Prannoy Pilligundla, Fatemehsadat Mireshghallah, Amir
Yazdanbakhsh, and Hadi Esmaeilzadeh. 2020. ReLeQ : A Reinforcement Learning
Approach for Automatic Deep Quantization of Neural Networks. IEEE Micro 40,
5 (2020), 37â€“45.

[8] Ian J. Goodfellow, Yoshua Bengio, and Aaron C. Courville. 2016. Deep Learning.

MIT Press.

[9] Issam Hammad, Kamal El-Sankary, and Jason Gu. 2019. Deep Learning Training
with Simulated Approximate Multipliers. In 2019 IEEE International Conference
on Robotics and Biomimetics, ROBIO 2019, Dali, China, December 6-8, 2019.
[10] Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both Weights
and Connections for Efficient Neural Network. In Advances in Neural Information
Processing Systems 28: Annual Conference on Neural Information Processing Systems
2015, December 7-12, 2015, Montreal, Quebec, Canada.

[11] Muhammad Abdullah Hanif, Rehan Hafiz, and Muhammad Shafique. 2018. Er-
ror resilience analysis for systematically employing approximate computing in
convolutional neural networks. In 2018 Design, Automation & Test in Europe
Conference & Exhibition, DATE 2018, Dresden, Germany, March 19-23, 2018.
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016.

[13] Xin He, Liu Ke, Wenyan Lu, Guihai Yan, and Xuan Zhang. 2018. AxTrain:
Hardware-Oriented Neural Network Training for Approximate Inference. In
Proceedings of the International Symposium on Low Power Electronics and Design,
ISLPED 2018, Seattle, WA, USA, July 23-25, 2018.

[14] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optim-
ization. In 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015.

[15] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features

from tiny images. (2009).

[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Clas-
sification with Deep Convolutional Neural Networks. In Advances in Neural
Information Processing Systems 25: 26th Annual Conference on Neural Information
Processing Systems.
[17] Ingo Lange. 2018.

Standardabweichungen und Kovarianzen Ã¼ber mehr-
https://wettermast.uni-hamburg.de/Downloads/

ere Mittelungsintervalle.
StdAbwKovIntervalle.pdf

[18] Ya Le and Xuan S. Yang. 2015. Tiny ImageNet Visual Recognition Challenge.
[19] Yuhang Li, Wei Wang, Haoli Bai, Ruihao Gong, Xin Dong, and Fengwei Yu. 2020.
Efficient Bitwidth Search for Practical Mixed Precision Neural Network. CoRR
abs/2003.07577 (2020).

[20] Uros Lotric and Patricio Bulic. 2012. Applicability of approximate multipliers in

hardware neural networks. Neurocomputing 96 (2012), 57â€“65.

[21] Alberto Marchisio, Vojtech Mrazek, Muhammad Abdullah Hanif, and Muhammad
Shafique. 2020. ReD-CaNe: A Systematic Methodology for Resilience Analysis and
Design of Capsule Networks under Approximations. In 2020 Design, Automation
& Test in Europe Conference & Exhibition, DATE 2020, Grenoble, France, March
9-13, 2020.

[22] Sana Mazahir, Osman Hasan, Rehan Hafiz, and Muhammad Shafique. 2017. Prob-
abilistic Error Analysis of Approximate Recursive Multipliers. IEEE Trans. Com-
puters 66, 11 (2017), 1982â€“1990.

[23] Vojtech Mrazek, Radek Hrbacek, Zdenek VasÃ­cek, and LukÃ¡s Sekanina. 2017.
EvoApprox8b: Library of approximate adders and multipliers for circuit design
and benchmarking of approximation methods. In Design, Automation & Test in
Europe Conference & Exhibition, DATE 2017, Lausanne, Switzerland, March 27-31,

Combining Gradients and Probabilities for Heterogeneous Approximation of Neural Networks

ICCAD â€™22, October 30-November 3, 2022, San Diego, CA, USA

2017.

[24] Vojtech Mrazek, Syed Shakib Sarwar, LukÃ¡s Sekanina, Zdenek VasÃ­cek, and
Kaushik Roy. 2016. Design of power-efficient approximate multipliers for ap-
proximate artificial neural networks. In Proceedings of the 35th International
Conference on Computer-Aided Design, ICCAD 2016, Austin, TX, USA, November
7-10, 2016.

[25] Vojtech Mrazek, Zdenek VasÃ­cek, LukÃ¡s Sekanina, Muhammad Abdullah Hanif,
and Muhammad Shafique. 2019. ALWANN: Automatic Layer-Wise Approxima-
tion of Deep Neural Network Accelerators without Retraining. In Proceedings of
the International Conference on Computer-Aided Design, ICCAD 2019, Westminster,
CO, USA, November 4-7, 2019.

[26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas KÃ¶pf, Edward Z. Yang, Zach DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning
Library. CoRR abs/1912.01703 (2019).

[27] Michal Pinos, Vojtech Mrazek, and LukÃ¡s Sekanina. 2021. Evolutionary Neural Ar-
chitecture Search Supporting Approximate Multipliers. In Genetic Programming -
24th European Conference, EuroGP 2021, Virtual Event, April 7-9, 2021.

[28] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-
Chieh Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In
2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt
Lake City, UT, USA, June 18-22, 2018.

[29] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Net-
works for Large-Scale Image Recognition. In 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015.
[30] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The journal of machine learning research 15, 1 (2014), 1929â€“1958.
[31] Zois-Gerasimos Tasoulas, Georgios Zervakis, Iraklis Anagnostopoulos, Hussam
Amrouch, and JÃ¶rg Henkel. 2020. Weight-Oriented Approximation for Energy-
Efficient Neural Network Inference Accelerators. IEEE Trans. Circuits Syst. (2020).
[32] Elias Trommer. 2022. agn-approx Software Repository. https://github.com/

etrommer/agn-approx. GitHub repository (2022).

[33] Elias Trommer, Bernd Waschneck, and Akash Kumar. 2021. dCSR: A Memory-
Efficient Sparse Matrix Representation for Parallel Neural Network Inference.
In IEEE/ACM International Conference On Computer Aided Design, ICCAD 2021,
Munich, Germany, November 1-4, 2021.

[34] Salim Ullah, Siva Satyendra Sahoo, and Akash Kumar. 2021. CLAppED: A Design
Framework for Implementing Cross-Layer Approximation in FPGA-based Em-
bedded Systems. In 58th ACM/IEEE Design Automation Conference, DAC 2021, San
Francisco, CA, USA, December 5-9, 2021.

[35] Filip Vaverka, Vojtech Mrazek, Zdenek VasÃ­cek, and LukÃ¡s Sekanina. 2020. TFAp-
prox: Towards a Fast Emulation of DNN Approximate Hardware Accelerators on
GPU. In 2020 Design, Automation & Test in Europe Conference & Exhibition, DATE
2020, Grenoble, France, March 9-13, 2020.

[36] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. 2019. HAQ: Hardware-
Aware Automated Quantization With Mixed Precision. In IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June
16-20, 2019.

[37] Xiaowei Xu, Yukun Ding, Sharon Xiaobo Hu, Michael Niemier, Jason Cong, Yu
Hu, and Yiyu Shi. 2018. Scaling for edge inference of deep neural networks.
Nature Electronics 1, 4 (2018), 216â€“222.

[38] Kohei Yamamoto. 2021. Learnable Companding Quantization for Accurate Low-
Bit Neural Networks. In IEEE Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2021, virtual, June 19-25, 2021.

[39] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. 2018. LQ-Nets:
Learned Quantization for Highly Accurate and Compact Deep Neural Networks.
In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany,
September 8-14, 2018, Proceedings, Part VIII.

[40] Qian Zhang, Ting Wang, Ye Tian, Feng Yuan, and Qiang Xu. 2015. ApproxANN: an
approximate computing framework for artificial neural network. In Proceedings
of the 2015 Design, Automation & Test in Europe Conference & Exhibition, DATE
2015, Grenoble, France, March 9-13, 2015.

[41] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, and Zhiru Zhang.
2019. Improving Neural Network Quantization without Retraining using Outlier
Channel Splitting. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA.

