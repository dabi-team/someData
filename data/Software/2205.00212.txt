2
2
0
2

r
p
A
0
3

]
E
S
.
s
c
[

1
v
2
1
2
0
0
.
5
0
2
2
:
v
i
X
r
a

Aggregation of Stack Trace Similarities
for Crash Report Deduplication

Nikolay Karasov
HSE University
nikolay.karasoff@gmail.com

Aleksandr Khvorov
HSE University
JetBrains
aleksandr.khvorov@jetbrains.com

Roman Vasiliev
JetBrains
roman.vasiliev@jetbrains.com

Yaroslav Golubev
JetBrains Research
yaroslav.golubev@jetbrains.com

Timofey Bryksin
JetBrains Research
timofey.bryksin@jetbrains.com

Abstract—The automatic collection of stack traces in bug
tracking systems is an integral part of many software projects
and their maintenance. However, such reports often contain a
lot of duplicates, and the problem of de-duplicating them into
groups arises. In this paper, we propose a new approach to solve
the deduplication task and report on its use on the real-world
data from JetBrains, a leading developer of IDEs and other
software. Unlike most of the existing methods, which assign the
incoming stack trace to a particular group in which a single most
similar stack trace is located, we use the information about all
the calculated similarities to the group, as well as the information
about the timestamp of the stack traces. This approach to
aggregating all available information shows signiﬁcantly better
results compared to existing solutions. The aggregation improved
the results over the state-of-the-art solutions by 15 percentage
points in the Recall Rate Top-1 metric on the existing Net-
Beans dataset and by 8 percentage points on the JetBrains
data. Additionally, we evaluated a simpler k-Nearest Neighbors
approach to aggregation and showed that it cannot reach the
same levels of improvement. Finally, we studied what features
from the aggregation contributed the most towards better quality
to understand which of them to develop further. We publish the
implementation of the suggested approach, and will release the
newly collected industrial dataset upon acceptance to facilitate
further research in the area.

I. INTRODUCTION

Software systems often use built-in bug tracking tools or
crash reporting systems [1–3] to get the information about the
occurring crashes and errors from their users. An important
feature of such reports is that they often do not have a textual
description of the problem, and the stack trace remains as the
main source of information [4]. An example of a stack trace
is presented in Figure 1. The stack trace consists of an ID, a
timestamp, an error, and a list of frames — an ordered list of
methods that constitute the stack of calls that lead to the error.
For the exact same error, several reports can be recorded
that become fuzzy duplicates [4–6]. Duplicates are caused by
multiple users having a problem caused by the same error.
For example, 72% of crash reports received for the IntelliJ
Platform (a platform for building integrated development envi-
ronments developed by JetBrains) are duplicates [6]. Allowing
developers to efﬁciently work with the reports requires us to

Fig. 1. An example of a stack trace from the NetBeans dataset [4].

de-duplicate them and group them by error type. Since there
are usually a lot of reports [7], [8], it is not feasible to group
them manually, as this implies a detailed study of each crash
report, which is too time-consuming [9].

Most modern methods [4–7], [10] assign the incoming stack
trace to one group or another based on the choice of the most
similar stack trace in the group: locate a stack trace which is
the most similar to the new one, and put the new one into
that group. The disadvantage of this approach is that despite
having all the similarity values to all stack traces in the group,
we only use the information about the nearest one, and the
overall structure of the group is not taken into account. Some
techniques also use the information about the group structure.
For example, the authors of CrashGraphs [11] represent the
group and the incoming stack trace as a directed graph.
However, in this case, the situation is somewhat opposite: the
information about the nearest stack trace in the group is lost.
The results of these works that study different methods—
based on the nearest stack trace [4–7], [10] and based on
the group structure [11]—lead to the idea that both of these
sources of information are important. In this work, we aim
to develop an approach that would take into account
the
advantages of all the existing methods and test it on real-world
data from a large software company.

The idea behind our approach is to use one of the existing
methods to ﬁnd the similarity between the incoming stack trace

{  : ,"id"1350  : ,"timestamp"922303751.0  : ["class"    "java.lang.NullPointerException"  ],  : ["frames"    [      ,"com.netbeans.ide.awt.HtmlBrowser$9.run"      ,"javax.swing.SystemEventQueueUtilities.processRunnableEvent"      "javax.swing.SystemEventQueueUtilities.access$0",      "javax.swing.SystemEventQueueUtilities$RunnableTarget.processEvent",      "java.awt.Component.dispatchEventImpl",      "java.awt.Component.dispatchEvent",      "java.awt.EventQueue.dispatchEvent",      "java.awt.EventDispatchThread.run"    ]  ]} 
 
 
 
 
 
and stack traces in the group, and then aggregate the received
information (the values of the calculated similarity, the number
of stack traces in the group, etc.). Another improvement
that we propose is to use the information about the time of
occurrence (timestamps) of the stack traces. This is important
because it gives us an idea of how far in time the group’s stack
traces are relative to the incoming stack trace. By aggregating
all this information, we build an Aggregation Model, which
determines the similarity between the stack trace and the
group, thus making it possible to further rank the similarity to
groups and select the closest one.

The approach is implemented as a simple and straight-
forward linear model that calculates the weighted sum of
the constructed features. The architecture of our approach is
designed speciﬁcally to be as quick and efﬁcient as it could
to be used in production. To evaluate the increase in the
performance that the Aggregation Model brings about, we test
it on two different datasets: the open-source dataset from the
NetBeans IDE collected in our previous work [4], as well
as a new industrial dataset collected within JetBrains, a large
developer of IDEs and other software. Upon acceptance, we
will release this dataset to facilitate further research in the
area. The Aggregation Model shows the increase in the Recall
Rate Top-1 metric (RR@1) of 15 percentage points for the
NetBeans dataset and of 8 percentage points for the JetBrains
dataset, compared to existing state-of-the-art solutions. Such
a signiﬁcant difference demonstrates the usefulness and the
potential of the proposed approach.

Next, we compare our approach with another simple way of
aggregating the information about the similarities — namely,
various k-NN-based approaches. However, our experiments
show that, when compared to the baselines, these approaches
only reach the increase of 4 percentage points in the RR@1
metric on the NetBeans dataset, and 2 percentage points on
the JetBrains dataset, indicating that the Aggregation Model
and the temporal information lead us to better performance.

Finally, to better understand the performance of the Ag-
gregation Model, we study the coefﬁcients of the model with
which it aggregates the features. As can be expected, the most
important feature is the similarity to the nearest stack trace, the
feature used in the majority of existing methods. However, the
results also show that the histogram of the similarities to all the
stack traces in the group that takes timestamps into account
helps the model as well. This analysis can help researchers
improve the Aggregation Model in the future.

Overall, our contributions in this paper are as follows:

• Aggregation Model: We propose a new approach that
the available stack traces and
takes into account all
their time of occurrence. The implementation of the
model with documentation is available online on GitHub:
https://github.com/nkarasovd/AggregationModel.

• Dataset: We collected a new large dataset using the
anonymized proprietary data from the JetBrains software
company,
it contains 236,174 stack traces and 9,163
groups. We will release the dataset upon acceptance.

• Evaluation: We describe our experience with the Aggre-
gation Model, which experimentally shows its usefulness
for stack trace grouping. Our approach outperforms exist-
ing methods by 15 and 8 percentage points in Recall Rate
Top-1 metric on the NetBeans and JetBrains datasets,
respectively.

II. BACKGROUND

A. Grouping Stack Traces

There are various approaches for grouping stack traces that
use different techniques, from edit distance to deep learning
methods. In this section, we describe the key ones. It should
be noted right away that most methods assign the incoming
stack trace to the group that contains the most similar stack
trace and therefore such approaches mainly differ in the way
of calculating the similarity between stack traces.

Modani et al. [10] use algorithms on strings to calculate
stack trace similarity. The authors present three similarity mea-
sures: edit distance, Longest Common Subsequence (LCS),
and preﬁx match. These algorithms are variants of classical
string matching algorithms, with some modiﬁcations for a
speciﬁc task of stack trace similarity. In addition, the authors
propose approaches for identifying uninformative functions
(e.g., redundant recursion) and their further removal.

Lerch and Mezini [5] propose to use the approach based on
the term frequency and the inverse document frequency (TF-
IDF) [12]. In their work, each stack trace is tokenized, and the
similarity value for the incoming stack trace q and the given
stack trace d is calculated according to the following formula:

similarity(q, d) =

tf d(t) · idf(t)2.

(cid:88)

t∈q

The advantage of the proposed approach is the speed of
learning and the ease of implementation.

Sabor et al. [7] developed DURFEX, a technique that
substitutes function names with the names of the packages in
which they are deﬁned to calculate the stack trace similarity,
and then segments the resulting stack traces into N-grams
of variable length. This approach signiﬁcantly reduces the
amount of vocabulary used. This technique works with the
JVM languages.

In our previous work, we proposed TraceSim [6], an ap-
proach for determining the similarity between two stack traces,
which combines TF-IDF and string distance. To obtain the
values of hyperparameters used in the calculation of local and
global frame weights, we formulate an optimization problem
and use machine learning approaches to solve it.

Later, building on the ideas of processing stack traces, we
proposed a new approach called S3M [4]. S3M uses a Siamese
neural network with biLSTM [13], [14] encoder to build the
embeddings of stack traces and determine their similarity. As
a classiﬁer, two fully-connected layers with ReLU activation
are used. This is one of the ﬁrst works in this ﬁeld that uses
deep learning methods.

Unlike previous works, Kim et al. [11] proposed an ap-
proach called CrashGraphs that differs from most modern

solutions described above in that it ﬁnds the similarity directly
between the incoming stack trace and the groups themselves.
To do this, they represent the stack trace and the group in
the form of a graph. Firstly, all stack traces in the group are
divided into pairs of consecutive calls. For example, the group
of two stack traces ABC and ACD will be parsed into pairs
of frames A → B, B → C, A → C, and C → D. Then,
an oriented graph is built from these edges, and the similarity
of a stack trace to the group is calculated by ﬁnding their
sub-graph similarity. The main advantage of this approach is
that we immediately get the similarity between the stack trace
and the group, avoiding counting the similarity between all the
individual stack traces. In addition, such a representation of
the group allows us to take into account its various features:
composition, size, and uniqueness of stack traces.

The methods considered earlier determine the similarity
value between the incoming stack trace and a certain group
of stack traces as the similarity value between the incoming
stack trace and the most similar stack trace in this group.
The main disadvantage of this technique is that ﬁnding this
one most similar stack trace still requires calculating all the
similarity values for all stack traces, and this information is
then discarded. At the same time, the calculated similarity
values can become a new source of information, which can
improve the quality of many algorithms. In addition, none of
the considered works uses the information about the time of
occurrence of stack traces in any way, which can also become
a new source of information for future methods.

In this work, we aim to combine the advantages of the
proposed approaches, create an Aggregation Model based on
them that uses the time of occurrence of stack traces, and
evaluate its usefulness on the real-world data from JetBrains,
a large software company.

B. Aggregating the Information with k-NN

Note that placing the incoming stack trace into the group
that contains the most similar stack trace is similar to the
approach of the k-Nearest Neighbors algorithm (k-NN) with
k = 1. Let us brieﬂy recall the idea of the k-NN approach.

Let D be a set of labeled objects with labels from a certain
set Y . For a new object x, it is necessary to determine whether
it belongs to a particular class. The choice of a class is carried
out according to the following decision function:

h(x, D) = arg max
y∈Y

(cid:88)

xi∈D

[yi = y]w(xi, x),

(cid:40)

w(xi, x) =

1, if xi is in the k nearest neighbours of x
0, otherwise

.

In the classical version of k-NN,
is
assigned to the class whose objects were the most present
among the k closest to the new object. If we take k equal to
1, then we get an approach similar to choosing the group that
contains the most similar stack trace.

the incoming object

The advantage of methods based on the idea of k-NN is that
they do not require the presence of a metric space, with the

main requirement being the ability to determine the distance
(in our case, similarity) between a pair of objects. Thus, the
idea of k-NN can be transferred to the problem of grouping
stack traces. In this case, taking into account multiple closest
stack traces among the available groups will mean considering
not only the single closest stack trace, but also other similiarity
values in the groups.

A common modiﬁcation to a basic k-NN approach is
the weighted k-NN approach. The idea is that each object
from the k nearest neighbours is included in the decision
rule with a certain non-zero weight. A common approach to
determining weight is to use kernel functions. A lot of different
kernel functions have been used in research: uniform, trian-
gle, epanechnikov [15], quartic, triweight, gaussian, cosine,
tricube [16], logistic, sigmoid, silverman [17]. These functions
weight objects differently depending on their distance.

In addition to classical approaches, there are works that
offer new types of weighting objects based on the distance.
Hyukjun et al. [18] propose the approach which calculates
the distance between a new instance and the k-th nearest
neighbor from each class, estimates posterior probabilities
of class memberships using the distances, and assigns the
instance to the class with the largest posterior. Ekin et al. [19]
propose several methods for weighting distances. For example,
in the Adjusted Weighting Method, the distance to a certain
class Yj is deﬁned as the sum of the distances to the γ nearest
neighbours in this class with some decreasing weight w:

h(x, Yj) =

γ
(cid:88)

wi · di

i=1
Overall, k-NN-based approaches allow us to take into
account the similarity to all stack traces in the groups. To
make sure that the proposed Aggregation Model is necessary,
and that the same performance cannot be obtained by simply
using a k-NN-based approach, we evaluate them also.

III. MOTIVATING EXAMPLE

Consider the following example, presented in Figure 2.
This example was taken from the NetBeans data, a labeled
dataset collected in our previous work [4]. Using the approach
proposed by Lerch and Mezini [5], we calculated the values
of the similarity between a potential incoming stack trace and
all stack traces in all groups, two of which are shown in the
ﬁgure. It can be seen that the values of similarity differ among
themselves very strongly inside each group. For Group1, the
stack trace with the highest similarity to the incoming one
(the right-most one) is very different from the rest of the
group, indicating that it might not be suitable to make a
judgement about the entire group. The values of the maximum
similarity for the Group1 and Group2 are equal to 4211 and
4044, respectively. Thus, if selecting the group based on the
most similar stack trace, the stack trace should be assigned to
Group1. However, in this case, Group2 is the ground truth
group, into which the stack trace should actually be deﬁned
based on the historical data. This example clearly demonstrates
the drawbacks of the most common way of assigning groups.

Fig. 2. An example distributions of similarity values of a certain incoming
stack trace to stack traces of two groups.

We believe that if we use the information about the values
of all the similarities and the information about the structure
of the group, as well as the time of occurrence of the stack
traces, then this will avoid an error and correctly assign the
incoming stack trace to Group2.

IV. APPROACH

of stack traces, etc. The main motivation for using the time
of occurrence is that
the last added stack trace probably
characterizes the group better than the ones that were added
at the very beginning, since the group can change over time.
Finally, after the similarity between the incoming stack trace
and all the groups is determined, we rank the groups according
to the similarity and assign the stack trace to the most similar
group. Let us now walk through the pipeline in greater detail.

B. Preprocessing

In order to calculate the similarity between stack traces, it
is necessary to construct their vector representation. Initially,
each stack trace is represented as a sequence of frames S =
{f1, f2, . . . , fn}, where fi is the i-th frame. For each used
similarity model, we apply the preprocessing as described in
their work. Most similarity models represent the stack trace as
a sequence of tokens. We take the full name of the method as
its tokens, however, some models use the name of the class
or the package.

C. Features

After choosing a particular similarity model, we can cal-
culate the values of the similarity between the incoming
stack trace and all stack traces in all groups. Based on these
similarities, we construct various features,
is
presented in Table I.

the full

list

Fig. 3. A high-level pipeline of our approach.

A. Overview

An important feature of working with stack traces in large
industrial projects is that the number of groups and their
composition is constantly changing, new stack traces come
in, new groups are formed. Therefore, we treated the task of
assigning the new incoming stack trace to the best group as a
ranking problem, and not a classiﬁcation problem.

The general pipeline of the proposed approach is presented
in Figure 3. The ﬁrst step in our approach is to choose
the similarity model for calculating the similarity between
stack traces. A model from any modern approach described
above [4–7], [10] can ﬁt this role, making it possible for
our technique to improve any of them. Then, it is necessary
to calculate the similarity between the incoming stack trace
and all the stack traces of the group, for all groups. The key
difference between our approach and existing approaches is
the Aggregation Model.

The Aggregation Model is a decision function that deter-
mines the similarity between the stack trace and the group.
This is done not based on the choice of the most similar stack
trace, but by aggregating all the available information, which
we transfer to it in the form of pre-calculated features. We
propose to build features based not only on the information
about the values of similarity between stack traces, but also
taking into account more complex information, such as the size
of the group, its structure, as well as the time of occurrence

1) Features based on similarity: The key feature that is
used in the majority of works is the similarity value of the
most similar stack trace in the group (we will refer to this
feature as the ﬁrst maximum).

2) Features based on timestamps: The idea behind using
timestamps of stack traces lies in the assumption that not all
stack traces in a group represent it the same. The groups tend
to evolve over time, and, as we showed in Section III, can
include stack traces that are very different. For this reason, we
believe that newer, more recent stack traces can characterize
the group better. The information about the time of occurrence
of stack traces will give our model the ability to independently
decide which similarity value characterizes the similarity to the
group the best.

Consider two stack traces: the Sq stack trace (the incoming
query stack trace that needs to be assigned to one of the
groups) and the Sg stack trace (the stack trace that is in one
of the groups). Let us designate the time of their occurrence
(timestamp) as Tq and Tg, respectively. For similarity(Sq, Sg),
we deﬁne the weight w(Sq, Sg) as follows:

w(Sq, Sg) =

1
log(|Tq − Tg| + 1) + 1

.

(1)

The idea behind this scale is that the closer in time two
stack traces are to each other, the greater the weight is, with
the values of the weight laying between 0 and 1. This allows
prioritizing the similarity of stack traces that are closest in
time to the incoming stack trace.

The following features can be constructed based on the
proposed weights. The maximum weight shows how long ago

1500200025003000350040000.000.050.100.150.200.250.30  FrequencySimilarity values  Group 11500200025003000350040000.000.050.100.150.200.250.30    Group 240444211Create Similarity ModelCalculateSimilarityBuildFeaturesCreate Aggregation ModelCalculate and Rank ScoresGroup Stack TracesTABLE I
THE DESCRIPTION OF FEATURES USED IN THE AGGREGATION MODEL.

No.

Feature name

Description

Features based on similarity

1

2

3

4

5

First maximum

The value of similarity to the most similar stack trace in the group. This feature constitutes the base of
the majority of methods and gives a very good ranking quality [4], [5], [7].

Features based on timestamps

Maximum weight

Minimum weight

Mean weight

The maximum weight corresponds to the stack trace in the group that is closest in time to the incoming
one, which allows us to determine how long ago the group was updated relative to it.

The minimum weight corresponds to the stack trace in the group that is the farthest away in time from
the incoming one, which allows us to determine how long ago the group was formed.

The average value of all weights in the group. This shows us how far the incoming stack trace is from
the group in time on average.

Difference between the maximum
and minimum weights

This shows the overall timespan of the group.

6-15

Histogram of all weights

The histogram shows the distribution of time difference from the incoming stack trace to all stack traces
in the group. In our experiments, we used the number of bins equal to 10. Each unique feature constitutes
the value of the histogram in the corresponding bin.

Features based on both similarity and timestamps

16

Weight of the ﬁrst maximum

This shows how long ago the most similar stack trace was added to the group relative to the incoming
stack trace, which can also affect how relevant it is.

17-28 Weighted similarity histogram

The weighted similarity histogram allows to estimate the distribution of similarities to the stack traces
in the group, taking into account the time of their occurrence relative to the incoming stack trace. In our
experiments, we used the number of bins equal to 12. Each unique feature constitutes the value of the
histogram in the corresponding bin.

the group was updated relative to the incoming stack trace,
the minimum weight shows when the group was formed, and
their difference shows the timespan of the group. The mean
weight shows how far the given stack trace is from the group
in time on average. The histogram of all weights will also
indicate the overall relation in time between the group and the
incoming stack trace. To transform a histogram into a set of
features, one simply needs to take the value of the histogram
in each bin as a separate feature. Based on our preliminary
experiments, we used a histogram with 10 bins.

3) Features based on both similarity and timestamps:
Combining both the similarity and the temporal aspect, ﬁrstly,
we can use the weight of the ﬁrst maximum — the weight
corresponding to the ﬁrst maximum. This will allow us to
understand how far the most similar stack trace is located in
time from the incoming one.

Using the intuition presented in Section III, we would also
like to have an idea about the distribution of the similarity
values to all stack traces in the group. This will make it
possible to understand how widely the similarity values are
scattered, which values prevail, and how much the value of
the ﬁrst maximum differs from all others. For that, a similarity
histogram can be employed (see Figure 2).

However, to take into account more information, instead
of using the similarity histogram itself, we can use its
modiﬁcation that employs the proposed weights. To build
a weighted similarity histogram, it is necessary to assign a
weight to each similarity value, which will allow us to move

from the frequencies of occurrence of each similarity to their
weighted frequencies. This means that each similarity value is
included in the histogram not with a weight of 1, but with a
weight calculated using Equation 1. Based on our preliminary
experiments, we used a histogram with 12 bins.

Overall, this results in 28 features, the full list of which
is presented in Table I. The selected features describe the
relationships between stack traces and groups very fully, both
in terms of similarity and time. We apply standard scaling to
all the features before processing them.

D. Linear Aggregation Model

After all the described features are built, the only remaining
thing is to transfer them to an Aggregation Model, rank the
obtained similarities to groups, and assign the stack trace to
the most similar group. We have tried several approaches,
and in the result, selected a simple linear Aggregation Model,
which is a weighted sum of constructed features. The practical
advantages of such a model are that it is easy and fast to
train, and is easy to introduce into production. To train the
Aggregation Model, we used a RankNet Loss [20], and the
feature weights were updated using the Adam optimizer [21].

V. EVALUATION

To test the applicability of our approach in the real world,
we evaluated it not only on the open-source data, but also
collected a dataset from JetBrains, a large software engineering
company. Speciﬁcally, we formulated three research questions:

RQ1. Does the proposed Aggregation Model improve the

performance of existing similarity models?

RQ2. Does the proposed Aggregation Model work better

than the k-NN-based approach?

RQ3. Which features are the most important for the per-

formance of the Aggregation Model?

In this section, we describe how the experiments were set
up, what data and metrics were used, as well as discuss the
results of our experiments.

A. Experimental setup

Our goal is to compare several baseline similarity models
from the literature with the Aggregation Model that employs
them to consider all stack traces in the group instead of just
the nearest one. We evaluated ﬁve different similarity models
described before: Modani et al. [10], Lerch and Mezini [5],
DURFEX [7], TraceSim [6], and S3M [4]. Additionally, we
used CrashGraphs [11] as a baseline, however, it cannot be
aggregated since it does not calculate the similarity between
individual stack traces.

It is necessary for all the models to see the same amount of
information during training. The data has a temporal compo-
nent, used to construct some features, so it is very important
for the train set to be located before the test set in time.

Fig. 4. The way the data was split.

The proposed way of splitting the data is shown in Figure 4.
All the models are validated on the Validation data span
and tested on the Test data span, which contains the latest
stack traces. Each baseline similarity model
is trained on
the older data Train. The same data is then split into two
intervals — TrainSim and TrainAgg. On the TrainSim, the
same similarity model
is trained, with the help of which
the similarity values on TrainAgg are obtained for training
the Aggregation Model. This way, the comparison is fair,
the training for the Aggregation Model
is merely divided
into training the employed similarity model and training the
aggregation itself.

B. Data

We carried out our experiments on two different datasets.
The ﬁrst one is an open-source NetBeans dataset from our
previous work [4]. At the same time, we build our approach
to be used in production, so we collected a new dataset from
the proprietary data of JetBrains, a large software engineering
company. We plan to release this dataset upon acceptance to
facilitate the further development of new approaches.

Table II shows the breakdown of datasets into train, valida-
tion, and test sets. We split Train into TrainSim and TrainAgg

TABLE II
THE DETAILS OF SPLITTING THE DATA INTO TRAIN,
VALIDATION (Val.), AND TEST SETS.

Measure

NetBeans

JetBrains

Train

Val.

Test

Train

Val.

Test

Groups
Reports
Days

30,975
39,417
4,200

1,695
1,973
140

6,084
7,792
700

6,356
205,284
124

1,077
5,243
7

1,731
25,647
28

at the ratio of 20:1 for the NetBeans dataset and 9:1 for the
JetBrains dataset. This table also shows a strong difference
between the datasets. The NetBeans dataset generated an
average of 7.7 groups per day while the JetBrains dataset
generated an average of 57.6 groups per day. Each group in
the NetBeans dataset contains an average of 1.3 reports, while
for JetBrains, it is 25.8 reports. In short, the JetBrains data is
less sparse than the NetBeans data. This diversity makes our
data valuable for further research. Let us now describe each
dataset a bit more.

1) NetBeans dataset: This dataset is open-source and was
collected in our previous work [4]. NetBeans is an integrated
development environment written in Java, the dataset includes
their crash reports submitted before 2016. Originally, the users
attached ﬁles with a description of the failures that occurred,
so the approach proposed by Lerch and Mezini [5] was used
to generate crash reports by extracting stack traces from the
attached ﬁles and description ﬁeld using regular expressions.
2) JetBrains dataset: JetBrains has its own system for
handling automatically generated error reports. This system
collects error reports from various projects written in JVM
languages (Java, Kotlin, Scala), which helps to unify the
work with them due to the homogeneous nature of the data.
When a new report arrives, it is pre-processed. The following
information is extracted from it: the product version, the point
in time when the error occurred, the information about the
system, and the stack trace itself.

New groups are formed as follows. When a new stack
trace arrives, the Lerch and Mezini’s similarity model [5]
determines the stack trace’s similarity to each of the groups
and assigns the stack trace to the corresponding group. If the
maximum similarity value is less than a certain threshold,
then the corresponding stack trace is shown to the company
developer who is experienced in this software component,
and they themselves make a decision whether to deﬁne this
stack trace in one of the existing groups or create a new
one. To make a decision, the developer is shown a ranked
list of groups. Thus, data labeling is automatically performed
whenever the developer has manually formed a new group or
determined the stack trace into one of the existing groups.
When working with our data, we do it in a similar way.

However, the usage of complex models in real-life large-
scale projects introduces certain difﬁculties. Since the amount
of data is really large, calculating all similarity values becomes
very difﬁcult, unacceptably long for production. For this
reason, we use an additional ﬁltration. It works as follows.

TimeTrainValidationTestTrainSimTrainAggValidationTestBaselineAggregationUsing the Lerch and Mezini’s similarity model [5], we ﬁnd
similarity values between the incoming stack trace and all
stack traces in all groups. This model is very straightforward
and fast, and can work the quickest. Then, having sorted the
obtained values of similarity, we begin to select most similar
stack traces until there are N unique groups. This approach
allows us to reduce the number of groups and reports used
when inferencing the models. In our experiments, we used
N = 1000. When choosing the value of N, we were guided
by the performance of the entire system (the smaller the
value of N, the faster the inference of the models goes) and
the value of the RR@N metric, which is obtained using the
similarity model. “Heavier” similarity models, for example,
deep learning-based ones, can then be used on this ﬁltered
subset of data much faster. Evaluating models with ﬁltering is
crucial for their use in industrial systems.

Finally, it should be noted that the company’s system has the
following important property. If a group has not been updated
for more than two months (62 days), then it is automatically
considered closed for new error reports. That is, groups that
have not been updated in the last 62 days are not considered
for a newly arrived stack trace. Thus, this feature must be
taken into account when carrying out experiments.

C. Performance Metrics

We used the Mean Reciprocal Rank (MRR) [22] and the
Recall Rate (RR@k) [23] as comparison metrics. The choice
of these metrics is based on research from previous works [5],
[7], [24]. The Mean Reciprocal Rank (MRR) reﬂects the
ranking quality of the entire list, as it calculates the average
reciprocal position of the correct item:

MRR =

1
|Q|

|Q|
(cid:88)

i=1

1
ranki

.

(2)

That is, for each incoming stack trace ∈ Q, we get a ranked
list of groups and ﬁnd the position of the only correct answer
in it. If the stack trace was the ﬁrst in the group, then there
will be no correct answer in the list and its position can be
considered equal to inﬁnity. After that, we take the average
over all queries from the reciprocal values of the positions of
the correct elements. In the absence of a valid element, the
inverse value will be zero.

Another important metric, Recall Rate at the ﬁrst k positions
(RR@k) counts the proportion of cases when the correct group
was among the ﬁrst k options:

RR@k =

1
|Q|

|Q|
(cid:88)

i=1

[ranki ≤ k].

(3)

This metric is more interpretable, which helps to better
predict the future behavior of the system. However, it only
counts the ﬁrst k elements of the list,
ignoring the rest.
The RR@1 metric is especially important for us, since the
automatic report processing system selects exactly one closest
group and places the report
there. Accordingly, for such
the main quality criterion will be precisely the
a system,

accuracy in the ﬁrst position, since all the others will not
be taken into account. In addition, we are interested in the
RR@5 and RR@10 metrics, since in the case of manual group
selection, the developer is shown the top relevant groups.

D. Training

Since we are solving the ranking problem, we used RankNet
Loss [20] to train the Aggregation Model. For every positive
example, we pick ten random negative examples as a nega-
tive sampling, which allows the model to better distinguish
between similar stack traces. Initially, the feature coefﬁcients
are randomly initialized. We used the Adam optimizer [21]
with the learning rate of 1e−3 and the weight decay of 1e−3.

E. k-NN-based Approach

it

To test the usefulness of the proposed Aggregation Model
and make sure that
is necessary, we also implemented
an alternative, simpler version of aggregating the data from
different stack traces in the group. We use the k-NN algorithm
described in Section II-B to take into account the distances to
all the stack traces in all groups.

Since most approaches based on the idea of k-NN use
distance values between objects, it is ﬁrst necessary to trans-
form the obtained similarity values in such a way that they
satisfy the following property: the larger the similarity value,
the smaller the distance value. Thus, the more similar stack
traces are, the closer they will be located to each other. To
comply with the required property, we used the following
transformation. Consider the similarity values {si}M
i=1 from
a given stack trace to all stack traces from all groups. Then,
the new values of the distances will be determined using the
following formula:

di = max

j=1,...,M

{sj} − si, ∀i ∈ 1, . . . , M.

(4)

Some k-NN approaches use a support from 0 to 1, that is,
if the distance is greater than or equal to 1, then the weight of
the object will be zero. To take this into account, all distances
obtained through Equation 4 are divided by the distance to
(k + 1)-th neighbor.

Note that the distances obtained by Equation 4 satisfy the
requirement: the distance to the most similar stack trace will
be minimal, and the less similar the stack traces are, the greater
the resulting distance value will be.

triweight, gaussian, cosine,

In our experiments, we used weighted k-NN and evalu-
ated the following eleven kernel functions: uniform, triangle,
epanechnikov, quartic,
tricube,
logistic, sigmoid, and silverman, as well as the apporaches
proposed by Hyukjun et al. [18] and Ekin et al. [19] For both
datasets and all the tested similarity models, we tested these
weighting methods and values of k from 1 to 15, since larger
values of k always demonstrated worse performance.

F. Results

Let us now describe the results of our experiments and

answer the posed research questions.

TABLE III
THE RESULTS OF THE EXPERIMENTS OF DIFFERENT SIMILARITY MODELS WITH AND WITHOUT THE AGGREGATION. BASE INDICATES THE METRIC
VALUE FOR THE GIVEN BASELINE SIMILARITY MODEL, AGG INDICATES THE RESULT WITH THE AGGREGATION MODEL, ∆A INDICATES THE INCREASE
THAT THE AGGREGATION BRINGS TO THE BASELINE, k-NN INDICATES THE RESULT WITH THE k-NN AGGREGATION, ∆K INDICATES THE INCREASE
THAT THE k-NN BRINGS TO THE BASELINE.

Dataset

Model

NetBeans

JetBrains

CrashGraphs [11]
Modani et al. [10]
Lerch and Mezini [5]
DURFEX [7]
TraceSim [6]
S3M [4]

CrashGraphs [11]
Modani et al. [10]
Lerch and Mezini [5]
DURFEX [7]
TraceSim [6]
S3M [4]

Dataset

Model

NetBeans

JetBrains

CrashGraphs [11]
Modani et al. [10]
Lerch and Mezini [5]
DURFEX [7]
TraceSim [6]
S3M [4]

CrashGraphs [11]
Modani et al. [10]
Lerch and Mezini [5]
DURFEX [7]
TraceSim [6]
S3M [4]

Best k-NN
Conﬁguration

—
Triweight, k = 5
Triweight, k = 3
Triweight, k = 13
Triweight, k = 7
Quartic, k = 13

—
Triweight, k = 3
Triweight, k = 3
Triangle, k = 3
Triweight, k = 3
Uniform, k = 3

Best k-NN
Conﬁguration

—
Triweight, k = 5
Triweight, k = 3
Triweight, k = 13
Triweight, k = 7
Quartic, k = 13

—
Triweight, k = 3
Triweight, k = 3
Triangle, k = 3
Triweight, k = 3
Uniform, k = 3

Base

0.33
0.36
0.37
0.42
0.39
0.46

0.73
0.80
0.78
0.81
0.80
0.87

Base

0.43
0.46
0.49
0.56
0.50
0.60

0.83
0.89
0.90
0.90
0.89
0.96

Agg

—
0.48
0.49
0.50
0.48
0.56

—
0.81
0.83
0.84
0.81
0.91

Agg

—
0.57
0.58
0.60
0.60
0.68

—
0.89
0.91
0.92
0.89
0.97

MRR

∆A

k-NN

∆K

Base

—
0.39
0.38
0.45
0.41
0.49

—
0.80
0.79
0.82
0.81
0.88

—
+ 0.08
+ 0.12
+ 0.08
+ 0.09
+ 0.10

—
+ 0.01
+ 0.05
+ 0.03
+ 0.01
+ 0.04

RR@5

—
+ 0.03
+ 0.01
+ 0.03
+ 0.02
+ 0.03

—
0
+ 0.01
+ 0.01
+ 0.01
+ 0.01

0.25
0.28
0.26
0.32
0.30
0.35

0.64
0.72
0.68
0.73
0.72
0.80

∆A

k-NN

∆K

Base

—
+ 0.11
+ 0.09
+ 0.04
+ 0.10
+ 0.08

—
0
+ 0.01
+ 0.02
0
+ 0.01

—
0.49
0.51
0.58
0.52
0.61

—
0.89
0.90
0.90
0.89
0.97

—
+ 0.03
+ 0.02
+ 0.02
+ 0.02
+ 0.01

—
0
0
0
0
+ 0.01

0.50
0.51
0.57
0.61
0.55
0.66

0.86
0.91
0.92
0.93
0.91
0.97

RR@1

∆A

k-NN

∆K

—
+ 0.08
+ 0.15
+ 0.10
+ 0.08
+ 0.12

—
+ 0.03
+ 0.08
+ 0.05
+ 0.02
+ 0.06

—
0.30
0.28
0.35
0.32
0.39

—
0.73
0.69
0.74
0.74
0.81

—
+ 0.02
+ 0.02
+ 0.03
+ 0.02
+ 0.04

—
+ 0.01
+ 0.01
+ 0.01
+ 0.02
+ 0.01

RR@10

∆A

k-NN

∆K

—
+ 0.13
+ 0.08
+ 0.04
+ 0.12
+ 0.07

—
0
+ 0.01
+ 0.01
+ 0.01
0

—
0.54
0.58
0.62
0.57
0.67

—
0.91
0.92
0.93
0.91
0.97

—
+ 0.03
+ 0.01
+ 0.01
+ 0.02
+ 0.01

—
0
0
0
0
0

Agg

—
0.36
0.41
0.42
0.38
0.47

—
0.75
0.76
0.78
0.74
0.86

Agg

—
0.64
0.65
0.65
0.67
0.73

—
0.91
0.93
0.94
0.92
0.97

1) RQ1: Aggregation Model vs. Baselines:

In the ﬁrst
research question, we wanted to see how the proposed Aggre-
gation Model improves the performance of different similarity
models. The results of the experiments are shown in Ta-
ble III. For all four metrics (MRR, RR@1, RR@5, RR@10),
the column ∆A shows the improvement of the Aggregation
Model over the given baseline model. Our Aggregation Model
showed the largest improvement for the approach of Lerch and
Mezini [5]: using the Aggregation Model improves the quality
metric RR@1 by 15 percentage points on the NetBeans dataset
and by 8 percentage points on the JetBrains dataset. At the
same time, the highest value in the RR@1 metric overall was
obtained with the aggregation of similarities obtained using
S3M (0.47 and 0.86 on the NetBeans dataset and the JetBrains
dataset, respectively). Overall, it can be seen that using the
Aggregation Model allows us to signiﬁcantly improve the
performance of all the models.

It can also be seen that the improvement is smaller on
the new dataset than on the open-source NetBeans dataset.
This difference in the inﬂuence demonstrates its importance
for further research: improving the aggregation or developing
brand new methods both require representative industrial data.
Finally, it should be mentioned that the ﬁltration described
in Section V-B2 greatly speeds up the processing of large
amounts of data in the real run in production. We tested the

approach on a machine with 8 Intel Xeon CPUs @ 2.30 GHz
and 60 GB of RAM, and the report processing speed increased
up to 200 times.

Answer to RQ1: According to our results, the proposed
Aggreation Model increases the quality of the consid-
ered similarity models up to 8 percentage points on the
JetBrains dataset and up to 15 percentage points on
the open-source NetBeans dataset. The approach is very
in a real-world project and can
simple to implement
signiﬁcantly increase the quality of the employed models.

2) RQ2: Aggregation Model vs. k-NN: In the second re-
search question, we studied how the performance of our Ag-
gregation Model compares with k-NN based approaches. The
results of applying k-NN-based approaches are also presented
in Table III. For each similarity model, the table shows the
k-NN conﬁguration that showed the best results (the kernel
function and k). The column ∆K shows the improvement of
the best k-NN model over the given baseline, thus, to compare
our Aggregation Model and the k-NN-based approach, one
needs to compare columns ∆A and ∆K.

The increase in the RR@1 metric when using k-NN-
based approaches is not as signiﬁcant as in the case of the
Aggregation Model. On the open-source NetBeans data, the
maximum increase in the RR@1 metric turned out to be 4

Fig. 5. The heatmap of feature importance for various models on the both datasets.

percentage points for the S3M [4] model, while in the case of
the Aggregation Model, the maximum increase in the RR@1
metric turned out to be 15 percentage points for the Lerch and
Mezini [5] model. As for the JetBrains data, the k-NN-based
approaches demonstrated even worse results. The maximum
increase in the RR@1 metric turned out to be 2 percentage
points for the TraceSim [6] model, while in the case of the
the maximum increase in the RR@1
Aggregation Model,
metric was 8 percentage points for the Lerch and Mezini [5]
model. Overall, the best-performing k-NN-based models never
outperformed the Aggregation Model.

Answer to RQ2: The results of applying the k-NN-
based approach are inferior in quality to the Aggregation
Model for all the similarity models except for TraceSim.
This indicates the usefulness of the aggregation and the
importance of taking into account the temporal aspect of
stack traces.

3) RQ3: Feature Importance: Finally, in the third research
question, we assessed the importance of the features used
in the Aggregation Model. Since we used standard scaling,
and all the features have the same scale, we can analyze
the coefﬁcients of the used linear model. Figure 5 shows the
coefﬁcients of each of the features listed in Table I for each
similarity model, trained for both datasets.

First of all, it can be seen that for all similarity models, the
feature ﬁrst maximum (i.e., the similarity to the most similar
stack trace) has the largest coefﬁcient on both datasets. This
conﬁrms the importance of its use and explains the good
results of baseline similarity models.

It can also be seen that for all similarity models,
the
following is true: the feature weighted similarity histogram
bin #12 has the largest coefﬁcient out of all the bins of the
weighted similarity histogram on the JetBrains dataset. This is
also true for almost all similarity models on the open-source
NetBeans data. Bin #12 contains the most similar stack traces,
including the ﬁrst maximum, and the more similar the stack
traces are, the greater the score the Aggregation Model gives
to this group.
Finally,

it can be seen that most of the coefﬁcients of
the weights histogram features have negative values. This is
consistent with our assumption that the more stack traces there
are that are far removed in time, and the longer the group
has not been updated, the less likely it is that the stack trace
should be assigned to this group. Thus, the Aggregation Model
introduces some penalty to the ﬁnal similarity value.

Answer to RQ3: Expectedly, the Aggregation Model gives
the greatest preference to the ﬁrst maximum feature, which
is the feature that is used in virtually all existing methods.
However, the increase in performance and the analysis
show that the model uses all the information given to it.

VI. THREATS TO VALIDITY

In our study, the following threats to validity can be found:

• Subject selection bias. The performance of machine
learning algorithms depends on the data on which the
algorithms are trained and on which they are applied.
In the case of our approach, we have two machine

7.2019.9800.9225.8765.1110.1960.3950.0290.2340.378-0.0600.083-0.170-0.082-0.1240.4240.5600.2500.232-0.1050.2070.1850.2160.1050.457-0.270-0.197-0.211-0.156-0.112-0.049-0.050-0.107-0.050-0.018-0.020-0.0170.015-0.0340.022-0.097-0.131-0.105-0.096-0.050-0.099-0.047-0.103-0.0580.016-0.135-0.357-0.211-0.102-0.0710.0120.004-0.0270.0000.039-0.0400.080-0.0000.0600.004-0.064-0.120-0.178-0.116-0.069-0.273-0.173-0.265-0.209-0.1390.164-0.1300.0140.1780.0840.1490.2900.3330.1040.0620.0710.0240.0490.0110.0100.0080.0920.0900.0190.0150.0660.0230.1240.090-0.0080.0580.0990.2930.0200.0920.0460.0480.0050.0740.0260.0880.2640.104-0.0460.0290.1160.2220.1300.069-0.0370.1100.1030.0600.0710.0920.2280.0230.0540.1680.0580.2790.0550.2120.1650.0080.4050.3350.4380.3700.2830.6820.9800.2710.8526.0110.0690.067-0.0170.166-0.101-0.0650.028-0.035-0.145-0.0390.0350.0140.0570.0360.484-0.0160.001-0.003-0.0320.041-0.089-0.111-0.097-0.063-0.070-0.031-0.033-0.047-0.027-0.064-0.029-0.045-0.041-0.0300.001-0.029-0.050-0.027-0.0100.008-0.087-0.123-0.115-0.049-0.176-0.102-0.208-0.126-0.098-0.284-0.014-0.083-0.020-0.033-0.026-0.012-0.105-0.047-0.006-0.088-0.029-0.013-0.035-0.020-0.069-0.026-0.064-0.088-0.0170.0600.0010.021-0.010-0.004-0.0150.0270.0550.0620.0430.0740.030-0.3360.0360.0170.0170.0120.0230.0090.0110.0200.0000.0090.0080.0100.0780.0250.004-0.0040.0120.0440.0270.0110.0510.0200.0210.0510.1190.0750.0470.1260.0170.0160.0190.013-0.020-0.0030.0130.0060.002-0.0390.0170.0100.005-0.0030.0550.031-0.013-0.0020.0190.0200.0380.0610.0680.0490.128Modaniet al.Lerch &MeziniDURFEXTraceSimS3M0.010.010.020.030.040.060.080.110.160.220.320.450.630.891.261.782.513.555.017.0810.00FeaturecoefficientJetBrains datasetNetBeans datasetModaniet al.Lerch &MeziniDURFEXTraceSimS3MFirst MaximumMaximum WeightMinimum WeightMean WeightDifference between Max and Min WeightsWeights Histogram Bin #1Weights Histogram Bin #2Weights Histogram Bin #3Weights Histogram Bin #4Weights Histogram Bin #5Weights Histogram Bin #6Weights Histogram Bin #7Weights Histogram Bin #8Weights Histogram Bin #9Weights Histogram Bin #10Weight of the First MaximumWeighted Similarity Histogram Bin #1Weighted Similarity Histogram Bin #2Weighted Similarity Histogram Bin #3Weighted Similarity Histogram Bin #4Weighted Similarity Histogram Bin #5Weighted Similarity Histogram Bin #6Weighted Similarity Histogram Bin #7Weighted Similarity Histogram Bin #8Weighted Similarity Histogram Bin #9Weighted Similarity Histogram Bin #10Weighted Similarity Histogram Bin #11Weighted Similarity Histogram Bin #12learning algorithms that are used to solve the deduplica-
tion problem: the similarity model and the Aggregation
Model itself, which uses the calculated similarity values.
To mitigate this threat, we tested our approach on two
datasets: open-source NetBeans data and the new data
from the JetBrains software company. The difference in
results highlights the importance of sharing new data with
the community.

• Limited scope of application. Our approach requires a
sufﬁcient number of crash reports containing the informa-
tion about stack traces to train the similarity model and
the Aggregation Model, while also having information
about their time of occurrence. However, this allows our
approach to be useful for any moderately large industrial
system. It is also possible to experiment with using a
pre-trained model in a new project.

• Programming language bias. We evaluated our ap-
proach on two stack trace datasets that were both col-
lected for JVM languages. For stack traces in other pro-
gramming languages, both the importance of the features
used and the results of applying the Aggregation Model
may differ, because the result of its work directly depends
on the similarity values obtained from the used similarity
methods. Further research is needed to assess how well
the proposed approach generalizes to other languages and
systems.

VII. CONCLUSION

In this paper, we described a new approach for solving
the problem of grouping stack traces that uses not only the
information about the values of similarity between stack traces,
but also the information about their time of occurrence. We
evaluated our approach on the open-source NetBeans dataset,
but also collected a new dataset from the proprietary data
of JetBrains, a large software engineering company. The
implementation of our approach is available online on GitHub:
https://github.com/nkarasovd/AggregationModel. Upon accep-
tance, we plan to publish our dataset
to facilitate further
research and its industrial application.

Our experiments have demonstrated the superiority of our
approach over the state-of-the-art methods by 15 and 8 per-
centage points in Recall Rate Top-1 metric on both the open-
source NetBeans data and our dataset, respectively. Also, using
simpler k-NN-based approaches did not allow us to obtain the
same increase in performance.

Our work can be continued in different directions. Firstly,
we plan to improve the Aggregation Model itself, experi-
menting with different features. Secondly, we want to exper-
iment with a neural network architecture that would create
embeddings of stack traces. This will allow us to create an
aggregated representation of a group using the embeddings
of stack traces in it and the incoming stack trace. Finally,
to improve the results of different
it
models for real-world applications. In this regard, the provided
JetBrains dataset will be of particular use, since both types of
aggregation demonstrated smaller improvements on such data.

is of great

interest

REFERENCES

[1] K. Satvat, M. Shirvanian, M. Hosseini, and N. Saxena, “Crepe: A
privacy-enhanced crash reporting system,” Proceedings of the Tenth
ACM Conference on Data and Application Security and Privacy, 2020.
[2] “Mozilla crash reporter.” [Online]. Available: https://support.mozilla.

org/en-US/kb/mozillacrashreporter

[3] K. Glerum, K. Kinshumann, S. Greenberg, G. Aul, V. Orgovan,
G. Nichols, D. Grant, G. Loihle, and G. C. Hunt, “Debugging in the
(very) large: ten years of implementation and experience,” in SOSP ’09,
2009.

[4] A. Khvorov, R. Vasiliev, G. A. Chernishev, I. M. Rodrigues, D. V.
Koznov, and N. Povarov, “S3m: Siamese stack (trace) similarity mea-
sure,” 2021 IEEE/ACM 18th International Conference on Mining Soft-
ware Repositories (MSR), pp. 266–270, 2021.

[5] J. Lerch and M. Mezini, “Finding duplicates of your yet unwritten bug
report,” 2013 17th European Conference on Software Maintenance and
Reengineering, pp. 69–78, 2013.

[6] R. Vasiliev, D. V. Koznov, G. A. Chernishev, A. Khvorov, D. V. Luciv,
and N. Povarov, “Tracesim: a method for calculating stack trace similar-
ity,” Proceedings of the 4th ACM SIGSOFT International Workshop on
Machine-Learning Techniques for Software-Quality Evaluation, 2020.

[7] K. K. Sabor, A. Hamou-Lhadj, and A. Larsson, “Durfex: A feature
extraction technique for efﬁcient detection of duplicate bug reports,”
2017 IEEE International Conference on Software Quality, Reliability
and Security (QRS), pp. 240–250, 2017.

[8] J. L. Davidson, N. Mohan, and C. Jensen, “Coping with duplicate bug
reports in free/open source software projects,” 2011 IEEE Symposium
on Visual Languages and Human-Centric Computing (VL/HCC), pp.
101–108, 2011.

[9] J. Anvik, L. Hiew, and G. C. Murphy, “Coping with an open bug

repository,” in eclipse ’05, 2005.

[10] N. Modani, R. Gupta, G. M. Lohman, T. F. Syeda-Mahmood, and
L. Mignet, “Automatically identifying known software problems,” 2007
IEEE 23rd International Conference on Data Engineering Workshop,
pp. 433–441, 2007.

[11] S. Kim, T. Zimmermann, and N. Nagappan, “Crash graphs: An aggre-
gated view of multiple crashes to improve crash triage,” 2011 IEEE/IFIP
41st International Conference on Dependable Systems & Networks
(DSN), pp. 486–493, 2011.

[12] K. S. Jones, “A statistical interpretation of term speciﬁcity and its
application in retrieval,” Journal of Documentation, vol. 60, pp. 493–
502, 1972.

[13] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

Computation, vol. 9, pp. 1735–1780, 1997.

[14] A. Graves, S. Fern´andez, and J. Schmidhuber, “Bidirectional lstm net-
works for improved phoneme classiﬁcation and recognition,” in ICANN,
2005.

[15] V. A. Epanechnikov, “Non-parametric estimation of a multivariate prob-
ability density,” Theory of Probability and Its Applications, vol. 14, pp.
153–158, 1969.

[16] N. S. Altman, “An introduction to kernel and nearest-neighbor non-
parametric regression,” The American Statistician, vol. 46, pp. 175–185,
1992.

[17] B. W. Silverman, “Density estimation for statistics and data analysis,”

1986.

[18] H. Gweon, M. Schonlau, and S. H. Steiner, “The k conditional nearest
neighbor algorithm for classiﬁcation and class probability estimation,”
PeerJ Computer Science, vol. 5, 2019.

[19] O. Ekin, P. L. Hammer, A. Kogan, and P. Winter, “Distance-based

classiﬁcation methods,” Infor, vol. 37, pp. 337–352, 1999.

[20] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamil-
ton, and G. Hullender, “Learning to rank using gradient descent,”
Proceedings of the 22nd international conference on Machine learning,
2005.

[21] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

CoRR, vol. abs/1412.6980, 2015.

[22] N. Craswell, Mean Reciprocal Rank. Boston, MA: Springer US,
[Online]. Available: https://doi.org/10.1007/

2009, pp. 1703–1703.
978-0-387-39940-9 488

[23] M. Sanderson, “Test collection based evaluation of information retrieval

systems,” Found. Trends Inf. Retr., vol. 4, pp. 247–375, 2010.

[24] I. M. Rodrigues, D. Aloise, E. R. Fernandes, and M. Dagenais, “A
soft alignment model for bug deduplication,” Proceedings of the 17th
International Conference on Mining Software Repositories, 2020.

