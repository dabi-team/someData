DRL-M4MR: An Intelligent Multicast Routing Approach Based on DQN Deep
Reinforcement Learning in SDN

Chenwei Zhaoa, Miao Yea,b,∗, Xingsi Xuec, Jianhui Lvd, Qiuxiang Jianga,b, Yong Wange

aSchool of Information and Communication, Guilin University of Electronic Technology, Guilin 541004, China
bGuangxi Key Laboratory of Wireless Wideband Communication and Signal Processing, Guilin University of Electronic Technology, Guilin 541004, China
cFujian Provincial Key Laboratory of Big Data Mining and Applications, Fujian University of Technology, Fuzhou, Fujian, 350118, China
dPeng Cheng Lab. Shenzhen, Guangdong, 518038, China
eSchool of Computer Science and Information Security, Guilin University of Electronic Technology, Guilin 541004, China

2
2
0
2

l
u
J

1
3

]
I

N
.
s
c
[

1
v
3
8
3
0
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

Traditional multicast routing methods have some problems in constructing a multicast tree, such as limited access to network state
information, poor adaptability to dynamic and complex changes in the network, and inﬂexible data forwarding. To address these
defects, the optimal multicast routing problem in software-deﬁned networking (SDN) is tailored as a multi-objective optimization
problem, and an intelligent multicast routing algorithm DRL-M4MR based on the deep Q network (DQN) deep reinforcement
learning (DRL) method is designed to construct a multicast tree in SDN. First, the multicast tree state matrix, link bandwidth
matrix, link delay matrix, and link packet loss rate matrix are designed as the state space of the DRL agent by combining the global
view and control of the SDN. Second, the action space of the agent is all the links in the network, and the action selection strategy
is designed to add the links to the current multicast tree under four cases. Third, single-step and ﬁnal reward function forms are
designed to guide the intelligence to make decisions to construct the optimal multicast tree. The experimental results show that,
compared with existing algorithms, the multicast tree construct by DRL-M4MR can obtain better bandwidth, delay, and packet loss
rate performance after training, and it can make more intelligent multicast routing decisions in a dynamic network environment.

Keywords:
Multicast, Multicast tree, Deep Reinforcement Learning, Software-Deﬁned Networking

1. Introduction

In many applications, such as video websites, teleconferenc-
ing, and multi-copies of cloud storage systems, if a unicast
routing strategy is adopted to transmit the same data to mul-
tiple destinations, the same transmission link of the redundant
data consumes extra network resources, making it easier for the
transmission link to produce problems such as network conges-
tion and increased traﬃc consumption. Therefore, in the face
of these ”one-to-many” scenarios, the multicast technique can
better solve these problems. In multicast, when the same data
packet is sent, the source node sends only one of the data to
multiple destination nodes, and the destination node receives a
copy of the sent data [1], thus redundancy consumption can be
reduced during data transmission [2]. It is necessary to con-
struct a Steiner tree (ST) or a multicast tree from the source
node to the destination nodes to meet the quality of service
(QoS) requirements to determine the multicast routing path.
The Steiner tree problem is an NP-complete problem [3]. The
design and implementation of eﬃcient and reasonable multicast
routing are inseparable from the timely and convenient acqui-
sition of various network status information, including remain-

∗Corresponding author
Email addresses: zhaochenwei96@foxmail.com (Chenwei Zhao),

yemiao@guet.edu.cn (Miao Ye)

Preprint submitted to Nuclear Physics B

ing link bandwidth, link packet loss rate, and link delay. How-
ever, traditional methods of obtaining network state informa-
tion, such as open shortest path ﬁrst (OSPF) and internal gate-
way routing protocol (IGRP) have complicated conﬁgurations
and steps for measuring the network state. With an increasing
node scale, the cost of obtaining network state information will
increase rapidly, and the network convergence will be slow [4].
Compared with the traditional network measurement method,
software-deﬁned networking (SDN) is a new network architec-
ture that abstracts the network system model and provides a
more ﬂexible and convenient way to measure and obtain net-
work link information (NLI). The SDN decouples the control
plane from the data plane and leverages centralized network
control to make it easy to conﬁgure and manage the network
by programming on the controller [5]. In the SDN architecture,
the controller communicates with the data plane through a uni-
ﬁed and open southbound interface (SBI) for operations such
as routing ﬂow table delivery and obtaining network status in-
formation [6]. Routing is a fundamental function of the net-
work. Ineﬃcient routing decisions will result in increased data
transfer delays and link loads and will aﬀect the overall perfor-
mance of the SDN. The centralized control and programming of
the SDN controller enable multicast routing algorithms to eﬃ-
ciently utilize and reprogram network resources [7]. Therefore,
optimizing routing in SDN is an important study [8]. There has

August 2, 2022

 
 
 
 
 
 
been much research on SDN unicast routing, but little research
on optimizing SDN multicast routing performance.

Currently, some approaches to SDN multicast routing apply
only the classic multicast tree methods. In Paper [9] the shortest
path tree (SPT) algorithm is used to construct multicast trees,
but the SPT algorithm cannot make full use of the network
resources. Additionally, many heuristic algorithms for con-
structing multicast trees have been proposed, such as the KMB
(Kou, Markousky, and Berman proposed) algorithm [10] and
the minimum cost path heuristic (MPH) algorithm [11], which
can solve the problem in polynomial time but cannot obtain the
optimal ST. Therefore, there are many intelligent optimization
methods, such as the ant colony optimization (ACO) algorithm
[12], genetic algorithm (GA) [13], particle swarm optimization
(PSO) [14], etc. can solve the ST problem well. But when mul-
tiple multicast trees need to be constructed at the same time,
greater computing capacity is needed, which brings a compu-
tational burden to the controller and leads to a longer solving
time.

In recent years, many studies have applied reinforcement
learning (RL) methods to routing problems. RL is a goal-
oriented learning method that automatically processes problems
through sequential decision-making. RL does not require com-
plete modeling of the environment, it emphasizes agent learning
through direct interaction with the environment. The environ-
ment feeds back the reward and the agent seeks to optimize the
goal to maximize the reward. Compared with the heuristic al-
gorithm, RL can provide routing policies quickly after training,
instead of requiring online recalculation each time the multicast
tree is constructed. The RL algorithm achieves signiﬁcant per-
formance improvements in routing (e.g., delay, loss, through-
put, and other metrics) and can adapt more intelligently to dy-
namic environments than traditional static routing algorithms.
[15, 16]. The solution in References [17, 18] still need to rely
on the traditional shortest path algorithm and the agent needs
to be retrained when the network environment changes. Ref-
erences [19, 20] use only hop to construct the multicast tree,
without considering other network state information.

Compared with the existing work in this ﬁeld, we propose
an intelligent multicast routing algorithm, which utilizes deep
reinforcement learning methods to construct multicast trees in
SDN by using four matrices as state spaces (DRL-M4MR). The
algorithm abstracts the construction of a multicast forwarding
path as a Markov decision process (MDP). The state space, ac-
tion space, and reward function are designed and multiple state
information of network links are used to construct multicast
routes. The DRL-M4MR takes the available link bandwidth,
link delay, link packet loss rate matrix, and multicast tree state
matrix as the state space of the agent. The agent uses all links
in the network as action spaces, and the action selection pol-
icy is designed to add a link to the current multicast tree in four
cases. The DRL-M4MR agent interacts with the environment to
obtain rewards and punishments in three cases and learns from
experience to calculate an optimal multicast tree from the set
of known source nodes to destination nodes. After training, the
agent can ﬁnd the optimal multicast tree in the environment of
network state change. Finally, the controller reversely traverses

2

the multicast tree from the leaf nodes and installs the multicast
routing ﬂow table to the switch on the data plane through the
southbound interface.

The contributions of this paper are as follows:

1) Combining the programmable feature of SDN and the deep
reinforcement learning method, three kinds of link-state in-
formation (available bandwidth, delay, and packet loss rate)
are measured and used as factors to design the state space
and reward function of the agent.

2) The matrix form of agent state space is designed, including
multicast tree state matrix, remaining bandwidth matrix, de-
lay matrix and packet loss rate matrix. In the state space
design, all links in the network are used as outputs of ﬁxed
dimensions of the neural network, and actions correspond
to links one by one, and an action strategy corresponding to
the four cases is selected for each action. The single-step
reward function and the ﬁnal reward function are designed
to guide agents to explore the multicast tree based on the in-
formation measured in the SDN network and to make better
decisions to build the multicast tree.

3) The controller ignores redundant nodes by reversely travers-
ing the multicast tree constructed by the agent from the des-
tination node to the source node when installing the ﬂow
table. The ﬂow entries on fork nodes are conﬁgured with
multiple output actions to forward packets to diﬀerent ports.
4) To improve the convergence eﬃciency and stability of
DRL, double dueling DQN and prioritized experience re-
play (PER) are adopted. Additionally, decay (cid:15)-greedy is
adopted to better balance exploration and exploitation.

The remainder of this paper is organized as follows. Related
works are discussed in Section 2. The SDN multicast problem
is described in Section 3. The architecture and modeling of in-
telligent SDN multicast routing optimization are introduced in
Section 4. The design of the DRL-M4MR algorithm agent and
the details of the algorithm are presented in Section 5 to imple-
ment multicast routing. The evaluation and comparison results
are provided in Section 6. The conclusions and suggestions for
future work are presented in Section 7.

2. Related works

In this section, we will discuss previous related studies that

explored diﬀerent techniques to improve multicast routing.

Classical algorithm: The classical heuristic algorithms in-
clude the KMB algorithm, MPH algorithm, and the average dis-
tance heuristic (ADH) algorithm. The KMB algorithm is based
on a minimum spanning tree and shortest path algorithm [10].
First, the complete distance graph G(cid:48) of graph G is constructed.
Then, the subgraph G1 containing the destination node in G(cid:48)
and the minimum spanning tree T1 of G1 is found. T1 is ex-
panded to the subgraph G(cid:48)(cid:48) of G according to the shortest path
in G. Finally, the minimum spanning tree of G(cid:48)(cid:48) is found, and
the redundant codon nodes are deleted; that is, the ﬁnal quasi-
Steiner tree is obtained. The time complexity of the KMB al-
gorithm is O
, and the approximate optimal Steiner tree is

mn2(cid:17)

(cid:16)

generally obtained. The MPH algorithm adds nodes from the
shortest path to the spanning tree each time until all destination
nodes are included. This algorithm has a similar problem to
the KMB algorithm, and the complexity is O
. The ADH
[21] algorithm constructs a Steiner tree by connecting two sub-
trees with the shortest path. The time complexity of the ADH
algorithm is O

mn2(cid:17)

n3(cid:17)

(cid:16)

(cid:16)

.

Many subsequent algorithms are based on classical heuris-
tic algorithm improvement. The delay-constrained minimum
cost path heuristic (DCMPH) algorithm [22] considers the de-
lay constraint condition when generating the lowest cost Steiner
tree according to the MPH algorithm. If the delay threshold is
met, the multicast path is directly added to the multicast tree. If
the delay threshold is not met, the multicast path is added to the
multicast tree through the minimum delay path generated by the
minimum spanning tree. However, the algorithm requires addi-
tional operations to eliminate the introduced loops. The key
node-based minimum cost path heuristic (KBMPH) algorithm
[23] by ﬁnding the key node with more shortest path, if the path
to be added to the multicast tree passes through more than one
key node then correct the cost of the path, and then do a compar-
ison whether to join the tree. This algorithm will increase the
number of shared nodes, but the complexity is O
. These al-
gorithms all have high computational complexity, because they
rely on classical algorithms, and also have problems of slow
convergence and diﬃculty in obtaining optimal solutions when
the network scale is large.

n3(cid:17)

(cid:16)

Kotachi et al. [24] regard determining multicast tree routing
as an integer linear programming (ILP) problem to minimize
the total number of ﬂow entries. Latif et al. [25] utilized ILP to
optimize multicast trees and improve fabric utilization in CLOS
networks. Touihri et al.
[26] expressed the computation of
multicast trees as a mixed integer linear programming (MILP)
to maximize the minimum remaining bandwidth and the num-
ber of links in the SDN-based CamCube Data Center Network.
Linear programming requires considerable computation, and its
constraints are often designed for speciﬁc scenarios.

Intelligent optimization algorithm: Hassan et al. [12] used
the ACO algorithm to solve the problem of multicast rout-
ing based on total cost, delay, and hop count, which required
[13] introduced a
considerable time to solve. Zhang et al.
new crossover mechanism, the leaf crossover genetic algorithm
(LCGA), to obtain better QoS multicast routing. Although LC
has brought performance improvement, it takes a long time to
implement. Shakya et al. [14] adopted the bi-velocity parti-
cle swarm optimization (BVDPSO) algorithm with a two-speed
strategy to solve the multicast routing problem so that the algo-
rithm retained the global search ability of the original particle
swarm and the fast convergence speed. Sahoo et al. [27] em-
bedded the PSO algorithm into the bacterial foraging (BFA) al-
gorithm to construct QoS multicast routing according to end-to-
end delay, bandwidth, delay jitter, and cost; however, iteration
time is expensive. Murugeswari et al. [28] proposed the multi-
cast algorithm by combining the PSO algorithm and GA, which
can obtain the tree of the minimum cost from the source node
to the destination node according to the delay and bandwidth.

3

These algorithms all face the same problem. When multiple
multicast trees need to be built at the same time, more computa-
tional power is required. Therefore it is not suitable to compute
a routing policy for each new ﬂow in the controller [8].

Reinforcement learning method: There has been some re-
search on the reinforcement learning routing problem, and the
general framework is as follows [15]. The data plane is respon-
sible for packet forwarding. On the control plane, the SDN con-
troller is responsible for sensing the network topology, collect-
ing network state information and installing ﬂow tables. The
management plane stores statistics and processes them into a
form that can be used for RL. The knowledge plane uses this
data to train the agent to ﬁnd the best routing strategy [29, 30].
The controller then installs ﬂow tables to the switch to complete
routing tasks.

According to the diﬀerently designed action space, the rout-
ing algorithm of RL is divided into three types. (1) The agent
selects a node from the neighboring nodes of the current routing
node as the next switch for forwarding packets until the data is
forwarded to the destination node [16, 31, 32]. (2) The action
space is the value of weights for each link in the network topol-
ogy, and each action assigns diﬀerent values to these weights.
Finally, the shortest path is calculated based on the weights us-
ing the Dijkstra algorithm or Floyd algorithm [18, 33]. (3) The
third type of action space is based on the k shortest paths cal-
culated by the shortest path algorithm, and one path is chosen
for each action [17]. However, these methods predominantly
consider the discussion of SDN unicast routing and cannot be
directly used to solve the problem of multicast routing.

Forster et al. [34] used Q-learning in a wireless sensor net-
work (WSN) to solve the multicast problem of the sink node.
The information of neighbor nodes of the destination node and
the current routing node was used as the state space. The ac-
tion space decomposes actions into sub-actions to represent the
selection of a node or a diﬀerent neighbor node. After the re-
ward calculates the reward for the entire action, backpropaga-
tion calculates the reward for each sub-action. Heo et al. [19]
use DQN to solve the multicast routing problem in SDN. A
matrix with a special identiﬁer is used to represent the current
state of the network. However, only the hop count of the mul-
ticast tree is considered without taking into account the band-
width, latency, packet loss rate, etc. in a dynamic network en-
vironment. Chae et al. [20] proposed Advantage Actor-critic
(A2C) to solve the multicast tree problem with dynamic net-
work topology changes. The network topology, multicast tree
and multicast set information as the state. The action is to select
an edge that is not in the current tree and add it every time. If
the tree reaches a destination node, a return value will be ob-
tained. Only the length of the tree is used as a metric, other
network information is not considered.

To make multicast routing better adapt to the dynamic net-
work environment and make full use of network resources. In
this paper, bandwidth, delay and packet loss rate are considered
as the metrics, and the DRL method is adopted to construct the
multicast tree in SDN.

3. Description of SDN multicast problems

losstree is the packet loss rate of the multicast tree, which is

3.1. Problem formulation

Multicast means that data packets are sent from a source node
and received by a group of target nodes. When the data packets
start from the source node, there is only one piece of data. Sub-
sequently, a node (which can be called a fork node) replicates
the data to nodes in multiple branches and then forwards it. The
transmission path of the packet is a tree with the source node as
the root and the destination node as the leaf node, as shown in
Figure.1. The topological tree structure is called a multicast tree
(or Steiner tree). The optimization of a multicast tree involves
ﬁnding a certain multicast tree to achieve optimal performance,
usually called the minimum Steiner tree problem [23].

Figure 1: An example of a multicast tree.

In a given network G = (V, E), where V is the nodes in graph
G, n = |V| denotes the number of nodes, E is a collection of
links in G, ei j ∈ E denotes the link between node i and node j,
m = |E| denotes the number of links, and S ⊆ V denotes the set
of multicast nodes. The source node is denoted by s ∈ S , and
D = (d1, d2, . . .) ∈ S denotes the set of destination nodes. The
minimum Steiner tree of the source node is deﬁned as ﬁnding
the minimum span tree T from s to D of G, which makes S ∈ T ,
S ⊆ T ⊆ V, and the cost of the tree minimum. Considering the
characteristics of SDN, multicast trees are mainly measured in
this paper from the following parameters:

bwtree is the average bandwidth of the multicast tree, which
is the average of the minimum remaining bandwidth from the
source node s to each destination node d. It is deﬁned according
to Formula 1.

f1(T ) = bwtree = average(

(cid:88)

d∈D

min
ei j∈psd

(bwi j))

(1)

where bwi j is the bandwidth from node i to node j, and psd ∈ T
denotes the path from the source node s to the destination node
d in the multicast tree T .

delaytree is the cumulative delay of the multicast tree and rep-
resents the sum of the delays of all links in T , which is deﬁned
by the following Formula 2.

calculated by Formula 3.

f3(T ) = losstree = 1 −

(cid:89)

ei j∈T

(1 − lossi j)

(3)

where lossi j denotes the packet loss rate of the link from i to j.
The objective is to maximize bwtree and minimize delaytree
and losstree. These objectives may be independent of each
other. Therefore, the multicast problem under the SDN archi-
tecture considered to be solved in this paper is essentially a
mathematical multi-objective optimization problem, as shown
in Formula 4 below.

max F(T ) = ( f1(T ), f2(T ), f3(T ))

(4)

where T is a Steiner tree in G; and F is the 3-dimensional target
vector, which is the mapping of the decision variable T to the
3-dimensional target space.

4. SDN intelligent multicast routing architecture

In this section, we introduce the overall structure of the intel-
ligent SDN multicast routing method model. As illustrated in
Figure 2, the design of the intelligent multicast routing archi-
tecture includes the data plane, control plane, and management
plane. Each component module and its function design are as
follows:

Figure 2: SDN intelligent multicast routing architecture.

f2(T ) = delaytree =

(cid:88)

ei j∈T

delayi j

where delayi j denotes the delay from i to j.

(2)

4.1. Data plane

This plane consists of OpenFlow switches. The forwarding
of data packets is achieved in this plane. Each SDN switch

4

destination nodefork nodeSBINLI StorageNLI2NLInNLI1...Knowledge PlaneNBINBILearnData PlaneNetwork TopologyDRL AgentRouting StrategyMutlicastTreeSDN ControllerTopologyAwarenessLink InformationDetectionFlow InstallationControl Planecompletes packet parsing and forwards packets from the input
port to the output port based on matches in the internal ﬂow ta-
ble. In addition, the data plane can respond to requests from the
control plane through the southbound interface. When forward-
ing multicast data, the switch matches the multicast address and
forwards the data to one or more ports based on the multicast
routing ﬂow entries.

4.2. Control plane

This plane contains the centralized SDN controller. The con-
troller is connected to each switch on the data plane and com-
municates via the SBI. In this way, the controller can control
the switches centrally and easily gather information about the
switches and links from a global view.

The controller provides three modules: Topology Awareness,
Link Information Detection, and Flow Installation. First, the
Topology Awareness module actively monitors the joining and
leaving of switches, the addition, removal and modiﬁcation of
ports, and the addition and removal of links on the data plane.
When these events occur, the controller sends link layer dis-
covery protocol (LLDP) packets to the switches. The packets
are propagated between topologies and back to the controller.
The controller parses the LLDP data to obtain switch IDs, input
and output port numbers and establishes the network topology
based on the information.

Then, the link information is detected on the established net-
work topology. The detected link information includes the re-
maining link bandwidth bwi j, the link delay delayi j, and the link
packet loss rate lossi j. The controller sends a request for port
information to the data plane every t seconds. And the reply
is parsed to obtain statistics on the number of sent bytes tb, the
received bytes rb, the sent packets tp, the received packets rp,
and the validity time tdur for each port tb. The two statistics cal-
culate the instantaneous throughput bwu, as shown in Formula
5. The remaining bandwidth bwi j is the diﬀerence between the
maximum bandwidth bwc and bwu of the current link, which
employs Formula 6.

bwu =

|(tb2 + rb2) − (tb1 + rb1)|
tdur2 − tdur1
bwi j = bwc − bwu

(5)

(6)

The link packet loss rate lossi j calculates the maximum value

from i to j and from j to i. The formula is shown as follows:

lossi j = max

(cid:32) tpi − rp j
tpi

,

tp j − rpi
tp j

(cid:33)

(7)

where tp∗ denotes the number of packets sent from port ∗,

and rp∗ indicates the number of packets received from port ∗.

The LLDP and echo request with timestamps are used to
measure the route delay [35]. The controller sends echo re-
quests to the switches s1 and s2. The delays between the con-
troller and the switch (decho1 and decho2) can be calculated by
minus the timestamp in the echo reply. And the delays between
switches (dlld p1 and dlld p2) are calculated based on LLDP send

and receive times. So the delayi j of ei j is expressed in Formula
8.

delayi j =

(cid:16)

dlld p1 + dlld p2 − decho1 − decho2
2

(cid:17)

(8)

The current network link information (NLI) is calculated by
the above methods and then saved to the network information
storage of the knowledge plane.

The control plane is also responsible for installing the ﬂow
table.
If the packet fails to match the ﬂow table, the switch
sends a packet-in message to the controller [5]. The controller
then installs ﬂow tables with forwarding information to the cor-
responding switches on the data plane via SBI based on the
routing policy. When the switch is at the fork point of the mul-
ticast tree, the instructions in the ﬂow entry set multiple apply
actions to forward the ﬂow to diﬀerent ports.

4.3. Knowledge plane

The key work done in this paper is mainly concentrated here.
The knowledge plane learns and decides the optimal multicast
path based on the collected network link status information and
sends the multicast tree to the control plane through the north-
bound interface.

First, the source node, multiple destination nodes and the
NLIs stored in the network information storage are processed
into matrix form as the state of the DRL agent. The state matrix
inputs into the neural network, and after training, the reward
value converges, i.e., the agent learns the optimal strategy in
the interaction with the environment, the optimal strategy π∗ is
related to the remaining link bandwidth, link delay, and link
packet loss rate in the reward function. We hope to construct
a multicast tree based on π∗ with larger remaining bandwidth
and a smaller delay and packet loss rate. Finally, the knowl-
edge plane calculates the optimal multicast tree and submits to
the controller, the routing decision corresponding to the multi-
cast group is sent to the switches in the domain to implement
the multicast routing. The detailed multicast routing algorithm
based on deep reinforcement learning is introduced in Section
5.

5. Design of DRL-M4MR intelligent SDN multicast routing

algorithm

In this paper, the process of constructing a multicast tree is
abstracted as a MDP. The MDP can be expressed as a quin-
tuple (S, A, P, R, γ), where S is the state set, A is the action
set, P is the state transition matrix, R is the reward function,
and γ is the reward discount factor. At the current moment t,
the agent observes the environment to obtain the current state
S t ∈ S and makes an action At ∈ A according to the strategy
π to obtain the corresponding reward Rt+1, and enters the next
state S t+1. However, the environment model, namely, the state
transition function P, is unknown, so a model-free reinforce-
ment learning method based on the value function is used in
this paper. When the state space and the action space are large,
the tabular reinforcement learning method is very ineﬃcient,

5

so deep neural networks are used to approximate the action-
value function [36]. In the state, S t inputs the neural network
(policy network) and outputs the state-action value Q (S t, At) of
diﬀerent actions. The agent selects and executes action At ac-
cording to the ε-greedy. The target value is outputted from the
target network according to the environment feedback reward
Rt+1 and the next state S t+1. The expected action value and the
current action value calculate the loss, and the network parame-
ter θ is updated by gradient descent. Then, the optimal strategy
is found according to the next state S t+1 iterative training.

The design of the agent mainly includes the design of the
state space, action space, and reward function. In this paper,
improvement methods of deep reinforcement learning, such as
decay ε-greedy and priority experience replay, are added to im-
prove the learning eﬀect and eﬃciency of the agent. The DRL-
M4MR agent framework is shown in Figure 3. First, the policy
network input is a minibatch of the state taken from PER, the
output is the Q value, and action is selected according to the
decay ε-greedy algorithm. The agent then interacts with the
environment to produce a reward and the next state. The next
state inputs into the target network and outputs the Q value. The
value corresponding to the action is taken with the maximum Q
value as the target value. Second, the parameters of the policy
network are updated after the loss is calculated. Finally, the
new state, action, reward, and next state are stored in the PER.
The details of the algorithm design are as follows.

Figure 3: DRL-M4MR framework.

5.1. State space

The state space is the current state that the DRL agent can
observe through the environment. The current network in the
data plane is regarded as a picture of four channels. The matrix
of each channel contains the state information of the current
multicast tree and multiple pieces of information for measure-
ment on the data plane. The row number i and column number
j of the matrix represent the node number. When i (cid:44) j, the ele-
ment of the ith row and jth column in the matrix represents the
link e(i, j). And when i is equal to j, the corresponding element
represents node i, where i, j ≤ n, e(i, j) = e( j,i) ∈ E. The matrix
MT of the ﬁrst channel is a symmetric matrix that represents
the state of the current multicast tree. The elements on the di-
agonal use the tag Ns to represent the source node and the tag
Nd to represent the destination node. Moreover, the elements

6

(a) MT matrix

(b) MB,Mdelay,Mloss matrixes

Figure 4: The matrixes of state space.

on the nondiagonal use the tag NT to represent the edge that is
currently in the multicast tree. The tag NB represent the edges
that can be added to the multicast tree and will not form a loop
when added. After adding new edges, the matrix is modiﬁed
as the next state S t+1 according to the current tree. When all
edges of the destination nodes are in the multicast tree, S t+1 is
set to terminate, i.e., S t+1 = None. As shown in Figure 4(a), the
source node is Node 2, the destination nodes are Node 4 and
Node 5, and the edges that constitute the current multicast tree
are e(2,3), e(2,4), e(3,5), and the edges that can be added are e(3,4),
e(1,5).

The other three channel matrices represent the remaining link
bandwidth matrix MBW , the link delay matrix MDelay, and the
link packet loss rate matrix Mloss. The diagonal of the matrix is
0, and the remaining elements represent the normalized values
of the measured link parameters in the network using Min-Max
normalization, as shown in Figure 4(b).

5.2. Action space

The action space is the set of actions that the agent can per-
form when interacting with the environment. The agent moves
to the next state depending on the action. In the network topol-
ogy with diﬀerent degrees of nodes, the next-hop node cannot
be used as the action space because the output of the neural net-
work is a tensor of ﬁxed shape. Inspired by the idea of the Prim
algorithm [37], an edge that will not form a loop is added to the
current tree at each iteration.

We design all sets of edges as action spaces, A =
{e1, e2, . . . , ek}, where ek ∈ E, k = 1,
. . . , m. There are four
cases to select an edge from A and join it to the multicast tree.
(1) If the selected edge will not form a loop after being added to
the tree (hereafter called optional branch), the added edge will
be added to the multicast tree of S t, then the multicast tree state
matrix will be changed, and the agent will enter the next state
S t+1. (2) This edge will form a loop, so it is not added to the
multicast tree, and the agent does not change the current state.
(3) This edge is already in the tree, and the agent also does not
change the current state. (4) This edge is not adjacent to the
current tree and does not change the current state.

As shown in Figure 5, blue nodes and edges represent the
current multicast tree. If the action is e(3,5) or e(3,6), indicating
Case (1), the edge can be added to the multicast tree. If the
action is e(4,3), then Case (2) is indicated. If the action is e(1,2),
Case (3) is indicated. If the action is e(5,7), as in Case (4), then,
e(5,7) should not join the multicast tree.

PERPolicy NetworkTarget NetworkStateconcatenateconcatenateActionNext StateStateRewardExperience Minibatch ActionEnviromentNext State updateTarget Q value Q_valueRewardLossLearnNBNsNTNTNTNBNTNTNBNdNBNdNT12345612345600.00.460.150.56 0.980.000.360.800.0 0.190.460.360 0.84 0.221.00.150.80 0.8400.15 0.350.560.0 0.220.1500.63 0.98 0.19 1.0 0.350.63012345612345600.00.460.150.56 0.980.000.360.800.0 0.190.460.360 0.84 0.221.00.150.80 0.8400.15 0.350.560.0 0.220.1500.63 0.98 0.19 1.0 0.350.63000.00.460.150.56 0.980.000.360.800.0 0.190.460.360 0.84 0.221.00.150.80 0.8400.15 0.350.560.0 0.220.1500.63 0.98 0.19 1.0 0.350.630Figure 5: The four cases about action.

5.3. Reward function

The reward value will guide the learning direction of the
agent, and the agent will enter the t + 1 after performing At
from the S t. According to At and S t+1, the corresponding re-
ward value Rt+1 can be obtained. Before the construction of
the multicast tree is completed, the information of the whole
tree cannot be obtained, so we can only evaluate the current
decision from the link perspective. The agent can only be eval-
uated from a whole-tree perspective if it is in a ﬁnal state. Three
outcomes are generated when the agent interacts with the envi-
ronment, and considering the optimized objective function, the
reward function is designed as follows.

1) The new branch e(i, j) is added to the multicast tree, and the
next state is not a complete state, that is, S t+1 (cid:44) None, and the
reward value is Rstep (Formula 9). The agent is guided to choose
the optimal link at each step, which is calculated according to
the network link information, including the link available band-
width bwi j, the link delay delayi j and the link packet loss rate
lossi j. Min-Max normalization is performed for bwi j, delayi j
and lossi j to prevent one factor from having too much inﬂuence
on decision-making.

Rstep = β1bwi j + β2

(cid:16)

1 − delayi j

(cid:17) + β3

(cid:16)

1 − lossi j

(cid:17)

(9)

where β1, β2 and β3 ∈ [0, 1] are tunable parameters to provide a
weight value for the reward calculation.

2) If the edge selected by the policy cannot be added to the
current tree or is already in the multicast tree, a constant value
penalty is applied, Rtrap = C.

3) When a new branch is added to the multicast tree, S t+1
becomes the ﬁnal state, indicating that the agent completes the
construction of the multicast tree from the source node s to the
destination node set D. After that, the reward value R f inish (For-
mula 10) is calculated from the four factors of bandwidth, delay,
packet loss rate, and redundant branches of the whole tree.

R f inish = β1bwtree + β2 (1 − delaytree) + β3(1 − losstree)

(10)

and delaytree are reduced to between 0 and 1 according to For-
mula 11 and Formula 12.

(cid:99)bwtree =

bwtree − min(bwi j)
max(bwi j) − min(bwi j)

(cid:91)delaytree

=

delaytree − min(delayi j) · |S |
(cid:80)
delayi j − min(delayi j) · |S |
ei j∈E

(11)

(12)

where θv and θa denote the parameters of the fully connected
layer of the state-value function and advantage function in the
policy network, respectively.

5.4. Q value function

The action-value function Q (s, a) evaluates the value of ac-
tion a performed by an agent in state s, updates the current
Q (s, a) from the next observed state, and returns by temporal
diﬀerence.

To reduce the overestimation problem of a single network
and improve the stability of the Q value, the double dueling
DQN [38] method is adopted. The double DQN calculates a
temporal diﬀerence (TD) error between the current Q value and
target value, as Formula 13. Then, the parameter θ is updated
by gradient descent, where θ and ˆθ assume the parameters of
two deep neural network. where θ and ˆθ denote the parameters
of the policy network and target network, respectively.
(cid:17)

(cid:17)

(cid:16)

T Derror =Rt+1 + γ ˆQ

S t+1, argmaxa(cid:48) ˆQ

(cid:16)
S t, a(cid:48); ˆθ

; ˆθ

(13)

− Q (S t, At; θ)

The dueling DQN decoupled the Q value into the state value
function V and action advantage function A. V and A evaluate
the current state and the importance of each action, respectively,
making the training more stable.



Q (S t, At; θ, θv, θa) = V (S t; θ, θv)
A (S t, At; θ, θa) −

(cid:88)

1
|A|

+

a(cid:48)

A (cid:0)S t, a(cid:48); θ, θa


(cid:1)



(14)

where θv and θa denote the parameters of the fully connected
layer of the state-value function and advantage function in the
policy network, respectively.

5.5. Exploration method

In the early stages of training, the agents are expected to ex-
plore more possible results, while there is good convergence
in the later stages. The decay ε-greedy (Formula 15) is used
to select actions, x ∼ U (0, 1). When x ≥ ε, the largest value
function of the current action is utilized; otherwise, an action is
randomly selected in the action space.

(cid:40)

at =

argmaxaQ (st, a) ,
random choice,

x ≥ ε
i f
otherwise

(15)

(16)

The reward value R f inish is proportional to the bwtree and in-
versely proportional to the delaytree and losstree. β1, β2 and β3
are deﬁned the same as Formula 9. Additionally, to prevent one
factor from inﬂuencing the reward value too much, the bwtree

where ε is calculated by Formula 16.

ε = ε f inal + (cid:16)

εstart − ε f inal

(cid:17)

(cid:32)

· exp

−

(cid:33)

episodecurr
episodedecay

7

23456 17case (2): at=e(4,3)case (1): at=e(3,5) or at=e(3,6) case (3): at=e(1, 2) case (4): at=e(5,7)current multicast tree nodeother nodewhere εstart denotes the value of ε at the beginning of train-
ing. The episodecurr is the current training episode. The
episodedecay is used to adjust the episode of convergence, as the
training continues and ε tends to ε f inal, and εstart, ε f inal ∈ [0, 1].

5.6. Prioritized experience replay

The agent will produce the transition (cid:104)S t, At, Rt+1, S t+1(cid:105) as
the learning experience is stored in the experience pool at every
decision. The PER [39] assigns diﬀerent sampling priorities
to each transition according to its TD error; the larger the TD
error is, the more important the transition will be. PER selects
eﬀective actions as learning experiences, which can improve
the learning eﬃciency of agents.

The sampling probability is proportional to the TD error, i ∼
P (i) ∝ |T Derror|α, where α is a hyperparameter that determines
the shape of the distribution. Finally, the sampling parameter
ωi is important because it is used to correct the deviation of the
distribution.

5.7. Multi-step learning

Traditional DQN agents estimate the target value through a
one-step reward and the S t+1 value. When the decision at S t
produces a deviation, it will lead to a deviation in future de-
cisions. Therefore, when calculating the current return Rt+1,
considering the future n steps return Rt:t+n (Formula 17) will
make the estimation more accurate and can speed up the learn-
ing speed [40].

Rt:t+n =

n−1(cid:88)

k=0

γkRt+1+k

(17)

The TD(n) error needs to be calculated after using multi-step
learning:

T Derror (n) = Rt:t+n
+ γmaxaQ

S t+n, a; ˆθ

(cid:16)

(cid:17)

− Q (S t, At; θ)

(18)

5.8. DRL-M4MR multicast routing algorithm based on DQN

According to the input source node s and destination node-
set D, DRL-M4MR ﬁnds the optimal multicast tree from s to
D in the currently observed network environment topology G.
The agent follows Algorithm 1 during training and needs the
following inputs: the learning rate α; the batch-size k of each
collection from the PER pool; the updated frequency nupdate,
which indicates that the parameters of the policy network are
updated to the target network at each nupdate step during train-
ing; and the total number of training episodes M. The output is
the multicast tree from s to D in G.

Line 1 to Line 3 are initialization. The parameter θ of the
policy network is randomly initialized, and this parameter is
copied to initialize the target network parameter ˆθ. The PER
pool is initialized to empty. In Line 6, the agent initializes the
multicast state matrix MT based on the inputs Ns and Nd, and
the current optional branch of the tree is the link connected to
the initial node. In Line 7, MT , Mbw, Mdelay and Mloss are con-
nected in the channel dimension, and the matrix of dimensions
(4nn) is taken as the initial state S t. Then, the training begins

in the environment of diﬀerent residual bandwidth, delay, and
packet loss rates.

Line 4 to Line 28 conduct the training of an episode. Af-
ter adding all destination nodes to the multicast tree, the agent
considers that it has completed the training of the current envi-
ronment and observes the initialization state of the environment
S t for new training when the data stored in the network infor-
mation storage are traversed, and the training of the episode will
be completed.

Algorithm 1: DRL-M4MR
Input: source node Ns, destination nodes set Nd, learning rate
α, n-step n, batch-size k, soft-update frequency nupdate,
training episodes M.

Output: optimal multicast tree for (Ns, Nd).

1 Initialize policy network with random weight θ;
2 Initialize target network with weight ˆθ = θ;
3 Initialize PER pool PER, n-steps buﬀer B;
4 for episode = 1 ← M do
5

for Mbw, Mdelay, Mloss in Network Information Storage do

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

Reset enviroment with (Ns, Nd) ;
Get st ← cat(MT , Mbw, Mdelay, Mloss);
// concatenate in the channel dimension
while True do

Take an action at from st according to decay

ε-greedy;

Execute action at and observe reward rt and next

state st+1;
Store (cid:104)st, at, rt, st+1(cid:105) in B;
if len(PER) ≥ k then

Sample minibatch (cid:104)si, ai, ri:i+n, si+n(cid:105) and get
ωi from PER;

// ωi is importance-smapling weight
Get Q(si, ai) from policy network;
if si+n is not None then
G ← Ri:i+n + γmaxa(cid:48) ˆQ(si+n, a(cid:48)) ;
else G ← Ri:i+n ;
Compute TD-error δi ← G − Q(si, ai);
Update PER transition priority by δi;
Update policy network paramters
θ ← θ + αωiδi∇θQ(si, ai) ;

end
if done then // all destination nodes in

multicast tree

Break;

end
st ← st+1;
In every nupdate steps update, set ˆθ ← θ;

end

end

27
28 end
29 Use ﬁnal policy network with parameter θ to construct

multicast tree from Ns to Nd, the agent execute the max
Q-value action for each decision.

From Line 9 to Line 11, the agent selects action at in the
current state st according to the decay ε-greedy, executes at and
returns rt from the environment and observes the next state st+1,
storing the transition in the PER pool.

8

From Line 12 to Line 19, a minibatch of size k is sampled
from the PER if the number of stores in the PER is greater than
or equal to the batch size. The Q (si, ai) and maxa(cid:48) ˆQ (si+1, a(cid:48))
are calculated from the policy network and target network, re-
spectively, where i ≤ k. If si+1 is not a ﬁnal state, then accord-
ing to the action-value function, the target reward R(cid:48) is calcu-
lated; otherwise, R(cid:48) is the ﬁnal reward value. Then, the priority
in the PER is updated according to the calculated TD error δi.
The policy network parameters are updated using gradient de-
scent and backpropagation algorithms to minimize the TD er-
ror. From Line 21 to Line 23, the loop is broken if it is in the
ﬁnal state; otherwise, st is set equal to st+1, and the next action
selection starts until all destination nodes are added to the mul-
ticast tree. After each nupdate generation training, the agent can
use θ to update the target network parameter ˆθ bearing (Line
25).

Using the parameters completed by the policy network train-
ing, the agent selects the action with the maximum value of the
action state in each state as the decision of each step to select
the multicast tree from Ns to Nd (Line 29). After the training,
because an edge is selected to be added to the multicast tree at
each step of decision-making, the complexity in the worst case
is O(N), where N is the number of edges in G.

After training through Algorithm 1, the agent uses Algorithm
2 to construct a multicast tree and deliver ﬂow entries. When
constructing a multicast tree (Line 5 to Line 13), the parent
of the current node is stored in the dictionary agent route dict
by unpacking the link corresponding to an action At into two
nodes.

Redundant branches are generated in the construction of the
multicast tree. To ignore the redundant branches, only the re-
verse traverse from the leaf node is the destination node. In a
tree structure, there is a unique path from the root node to any
leaf node, so there is a unique path from the leaf node to the root
node. If the leaf node is not the destination node, the node is
redundant, and the subtree in which it is located in a redundant
branch. If the root of the subtree is only one subtree, the subtree
where the root resides is also a redundant branch. When delet-
ing redundant branches (Line 15 to Line 25), the parent node
is queried at the agent route dict and traversed in reverse from
each destination node to the root node. If the node is already
traversed, the parent and child nodes of the current node are
stored in install In f o dict.

Finally, Algorithm 2 traverses install In f o dict (Line 26 to
Line 31), obtains the corresponding inbound and outbound
ports according to the parent node and child node, and delivers
the ﬂow table entry that forwards data from the inbound port to
the outbound port to the switch.

6. Experiments and evaluation

The control plane of the experiment uses the SDN controller
Ryu[41]. The data plane uses Mininet 2.3.0 [42] to simulate the
network. It is deployed on the Ubuntu 20.04.3 server that runs
the GeForce RTX 2080 graphics card. Ryu provides the south-
bound interface OpenFlow 1.3 to communicate with Mininet’s
Open vSwitches. Python scripts are utilized to collect network

Algorithm 2: Install multicast ﬂow entry
Input: DRL-M4MR agent with parameter θ, source node Ns,
destination nodes set Nd, current network information
MBW ,Mdelay,Mloss.

1 Initialize agent route dict={Ns: None} ;
2 Initialize install info dict={Ns:{’parent’: None, ’child’: []}} ;
3 Reset enviroment with (Ns, Nd) ;
4 Get st ← cat(MT , MBW , Mdelay, Mloss);
5 while True do
6

// Contruct multicast tree

Take a max Q value action at from st ;
Execute action at and observe next state st+1 ;
parent node, current node=at ;
agent route dict[current node]=parent node ;
if all destination nodes in multicast tree then

7

8

9

10

11

12

Break;

end
st = st+1 ;

13
14 end
15 for node in Nd do
16

// Delete redundant node

while node is not None do

17

18

19

20

21

22

23

parent node =agent route dict[node] ;
install info dict[node][’parent’] = parent node ;
install info dict[parent node][’child’].append(node) ;
node ← parent node ;
if node in install info dict then

// node have been travesed
Break;

end

end

24
25 end
26 for node in install info dict do
27

// Install flow entry

parent node = install info dict[node][’parent’];
child node = install info dict[node][’child’];
in port, out ports ← get ports(node, parent node,

child node);

Install multicast ﬂow to node corresponding switch ;

28

29

30
31 end

information and store it as Comma-Separated Values (CSV)
ﬁles and pickled ﬁles. Reinforcement learning and deep neu-
ral networks are implemented using Python 3.8 and PyTorch
1.9.0.

The experimental topology Figure 6 shows the edge data cen-
ter topology in New York City [43], which contains 14 nodes
and 23 links. The link parameters are randomly generated, and
the bandwidth and delay are set in the range of 5∼30 Mbps and
1∼20 ms, respectively, which obey a uniform distribution.

To simulate network traﬃc, we use the traﬃc matrix genera-
tion tool [44] to generate 24 traﬃc matrices between 14 nodes.
Each element of the matrix represents the amount of traﬃc sent
from the source node i to the destination node j. As shown in
Figure 7, the ordinate represents the average amount of traﬃc
sent by each node in Kbits/sec. In Mininet, each node is re-
garded as a server node and a client nod. The Iperf [45] tool is
used to send UDP packets with speciﬁed data sizes between dif-
ferent nodes to simulate traﬃc and achieve dynamic changes in
the network environment. Finally, the Ryu controller measures

9

The reward settings include Rstep, Rtrap, and R f inish. Agents
tend to obtain larger reward values, and diﬀerent ratios of re-
ward and penalty values will aﬀect the convergence and even
lead to nonconvergence.

We set Rtrap = −1, and experiments show that when the upper
limit for R f inish and Rstep is set to 1, R f inish : Rstep = 1 : 1, the
reward value of each step decision is as important as the ﬁnal
reward value. In this case, the agent will choose the link with
the higher reward value in the ﬁrst few steps of the state so that
the network parameters will receive many updates, and thus the
Q value will be updated, although this decision will lead to a
redundancy link in the future.

Figure 6: The experimental topology.

the network parameters and stores the measurement results.

When the agent reaches the ﬁnal state S t+k and selects an
action, a link with a higher reward value than that of reaching
the ﬁnal state can be selected, and the agent chooses this link
in preference, resulting in the construction of the multicast tree
containing other redundant branches.

Setting R f inish : Rstep = 1 : 0.1 causes both the reward value
of each step decision and the ﬁnal reward value to take eﬀect.
Rstep guides the agent to choose the current optimal link, and
R f inish evaluates and awards the agent from the perspective of
the whole tree. The agent will seek the optimal parameter of
the whole tree. However, fewer redundant links are generated
for the same reasons described in the previous section.

Setting R f inish : Rstep = 1 : 0.01 causes the reward value
of each step decision to be far less than the ﬁnal reward value.
Thus, the update of the network parameters mainly depends on
the Q value of the ﬁnal decision, so that the Q value of the
state is closer to the ﬁnal state S t+k updates more easily, and the
intelligence chooses the shortest path to reach the destination
node possible. However, the bandwidth, delay, and packet loss
rate of this tree are not optimal.

We also tried diﬀerent reward value designs, which are sum-

marized in Table 1.

Table 1: reward value setting

R f inish : Rstep
1:1
1:0.1
1:0.01
1:-0.01
1:-0.1
1:-1

bwtree
13.69
14.35
14.04
14.04
14.04
14.04

delaytree
13.31
13.22
13.07
13.07
13.07
13.07

losstree
3.69e-4
3.69e-4
4.91e-3
4.91e-3
4.91e-3
4.91e-3

steps
8.0
7.5
7.0
7.0
7.0
7.0

redundancy
5.0
3.5
0.0
0.0
0.0
0.0

Redundancy indicates the average number of redundant
links. The bwtree, delaytree, and losstree are calculated according
to Formula 1, Formula 2, and Formula 3, respectively, and take
the average value after calculation in diﬀerent scenarios. Steps
indicate the average number of steps an agent takes to decide
on constructing a multicast tree. When the single-step reward
value is negative, the value is calculated according to Formula
19.

Rstep = −(β1(1 − bwi j) + β2delayi j + β3lossi j)

(19)

The design of the reward function is the reason why the agent
can obtain redundant links when constructing a multicast tree.
It will select the action with the maximum Q value from all

10

Figure 7: Mean bandwidth of the traﬃc matrix.

6.1. DRL paramters setting

The input state matrix passes through two convolution lay-
ers with two convolution kernels, which extract features using
convolution kernels of size 5×1 and 1×5 After ﬂattening the
features into a one-dimensional tensor, they are combined in
the element dimension through a concatenation operation. Af-
ter the combination, two fully connected layers are input. The
output of the fully connected layer is inputted into the fully con-
nected layer of the advantage function and the fully connected
layer of the value function. The policy network and target net-
work are shown in Figure 3. Finally, the output of the Q value
is calculated according to Formula 14. The LeakyReLU activa-
tion function is used after each convolution layer and full con-
nection layer. The optimizer uses adaptive moment estimation
(Adam), which has the advantages of an adaptive learning rate
and momentum gradient to prevent falling into local solutions
and helps the neural network to better converge.

This part of the experiment is tested under three network sta-
tus information scenarios, that is, the controller collected three
network status information and converted it into three groups of
MBW , Mdelay and Mloss. Because it is easy to obtain the optimal
multicast tree under a small number of scenarios, we can better
observe whether the convergence of the agents is optimal. The
source node is 12, and the destination node-set is {2, 4, 11} be-
cause the multicast tree from the source node to the destination
node is expected to be not unique in the experiment, and the
agent is expected to face more link choices when constructing
the multicast tree to test the eﬀectiveness of the algorithm.

s4s1s11s3s2s5s7s8s14s9s13s10s12s605101520time025005000750010000125001500017500mean_traffic(a) lr

(b) batchsize

(c) nsteps

(d) updatefrequency

(e) gamma

(f) decay e-greedy

Figure 8: Diﬀerent parameters setting.

links according to the current state S t, and selecting this link
has little inﬂuence on the ﬁnal reward value, which leads to the
agent making decisions that are not conducive to the future.

The reason why it cannot converge to the optimal solution
is that the agent will select the action that can complete the
multicast tree before the state S t+k because the Q value of this
action in the current state is larger than that of other edges, and
it will not choose the longer but better link.

Exploring the inﬂuence of diﬀerent hyperparameters on the
convergence eﬀect of the ﬁnal reward value of an agent is dis-
cussed as follows.

The learning rate α determines the step size of the neural
network parameter update, a larger setting leads to premature
convergence or non-convergence of the model, and a setting
too small will lead to slow convergence of the model and even
lead to the model falling into local optimum. α is set to 10−4,
10−5, 10−6, and 10−7 to observe the convergence eﬀect of the
ﬁnal reward value. Figure 8(a) shows that when α is 10−6 or
10−7, the agent falls into the local optimum; when α is 10−4 or
10−5, the agent searches in the direction of the global optimum.
The batch-size k determines the size of each sample taken
from the PER by an agent. Figure 8(b) shows that there is little
inﬂuence on the overall curve of agent training. The ﬁnal re-
ward convergence is smoother with less ﬂuctuation when k is 8
or 16.

Figure 8(c) shows that the larger n is, the easier it is to pro-
duce redundancy and even fall into a local optimum. When
constructing a path in unicast, there is only one path to the desti-
nation node, and every decision on this path is an indispensable
step after the future construction is completed. However, in the
construction of a multicast tree, the link with the maximum Q

value is selected from all links and added to the current tree at
each decision. The reward value after n steps is calculated and
ampliﬁed according to Formula 17. This link may be redundant
from the perspective of the whole tree in the future. Therefore,
n = 1 is set.

The update frequency nupdate of the target network is set to 1,
10, and 100. Figure 8(d) shows that when nupdate is set to the
above value, there is little eﬀect on the reward value.

The discount factor γ is set to 0.3, 0.5, 0.7, and 0.9. Figure
8(e) presents that the eﬀect is best when γ = 0.9. The agent
cannot gain eﬀective experience from the attempt, causing the
agent to fall into a local optimum when γ is small.

In Figure 8(f), the results show that there is little inﬂuence on
curve convergence. Because the transitions generated by each
decision are stored in the PER, the PER will preferentially se-
lect transitions with large TD errors when sampling a minibatch
each time.

6.2. Performance analysis

The DRL-M4MR is compared with the classical Steiner tree
KMB algorithm.
In the KMB algorithm, the residual band-
width (K MBbw), link delay (K MBdelay), and packet loss rate
(K MBloss) are used as weights for comparison. The three NLIs
are used for DRL-M4MR agent training. After the training is
completed according to Algorithm 1, the network parameter θ
of the policy network is used for testing. The bandwidth, de-
lay, packet loss rate, and link number of the multicast tree are
compared, as shown in Figure 9.

The metric in Figure 9(a) is the average of the minimum re-
maining bandwidth of the path from the source node to each

11

05001000150020002500300035004000episodes1.21.41.61.82.02.22.4final rewardlr 0.0001lr 1e-05lr 1e-06lr 1e-0705001000150020002500300035004000episodes1.01.21.41.61.82.02.22.4final rewardbatchsize 8batchsize 16batchsize 32batchsize 6405001000150020002500300035004000episodes1.01.21.41.61.82.02.22.4final rewardnsteps 1nsteps 2nsteps 3nsteps 405001000150020002500300035004000episodes1.01.21.41.61.82.02.22.4final rewardupdate 1update 10update 100update 100005001000150020002500300035004000episodes1.21.41.61.82.02.22.4final rewardgamma 0.3gamma 0.5gamma 0.7gamma 0.905001000150020002500300035004000episodes1.01.21.41.61.82.02.22.4final rewardegreedy 100egreedy 1000egreedy 500(a) bandwidth

(b) delay

(c) loss

(d) length

Figure 9: A comparison between DRL-M4MR and KMB algorithms.

(a) number=60

(b) number=120

Figure 10: Diﬀerent number of network link information

destination node in the multicast tree, that is, the average bottle-
neck bandwidth. The average bottleneck bandwidth produced
by the DRL-M4MR algorithm is on average 19.37% and at
most 55.35% higher than those produced by the K MBbw al-
gorithm. The K MBdelay and K MBloss algorithms show that
the value of the average bottleneck bandwidth is 0 at worst,
whereas the DRL-M4MR does not. The results show that the
DRL-M4MR algorithm prefers to select the link with higher
remaining bandwidth to construct a multicast tree.

Figure 9(b) reveals that the average delay value of the mul-
ticast tree produced by the DRL-M4MR algorithm is on av-
erage 3.76% and 5.8% higher than K MBbw and K MBdelay,
and 65.47% lower than K MBloss. At most, it is 10.24% and
25.17% higher than K MBbw, K MBdelay, and 10.03% lower than
K MBloss, respectively. Although the average delay produced by
the DRL-M4MR algorithm is higher than that of K MBdelay, the
values are very similar. The results show that the DRL-M4MR
algorithm has a good delay performance.

In Figure 9(c), the metric is the average packet loss rate
of multicast trees. The results show that the average packet
loss rate produced by the DRL-M4MR algorithm on average is
33.33% and 66.67% lower than K MBbw and K MBdelay, respec-

tively. The value is equal to those produced by K MBloss, which
indicates that the DRL-M4MR algorithm has a good packet loss
rate performance.

Figure 9(d) presents that the link number of a multicast tree
produced by the DRL-M4MR algorithm is on average 15.08%,
21.43% and 30.32%, and at most 16.67%, 33.33% and 60.0%,
higher than the K MBbw, K MBdelay and K MBloss algorithms,
respectively. The results indicate that DRL-M4MR selects a
larger number of links to construct a multicast tree.

In the comparison of training of more NLIs, the training re-
sults under 60, and 120 NLIs are shown in Figure 10. The com-
parison results show that the agent of the DRL-M4MR can ﬁnd
the multicast tree with the maximum reward value according to
the set reward function when constructing the multicast tree.

Figure 10(a) The bandwidth generated by the DRL-M4MR
is on average 6.77% higher than that of the K MBbw and much
higher than that of the K MBdelay and K MBloss algorithms. The
delay is on average 5.67%, 39.4%, lower than K MBbw and
K MBloss, 18.31% higher than K MBdelay. The packet loss rate is
on average 1.97%, 20.25% lower than K MBbw and K MBdelay,
17.71% higher than K MBloss. The length is on average 10.17%,
9.32%, 32.11% higher than K MBbw, K MBdelay and K MBloss.

12

123traffic0.02.55.07.510.012.515.017.5mean bwkmb_bwkmb_delaykmb_lossDRL-M4MR123traffic255075100125150175200mean delaykmb_bwkmb_delaykmb_lossDRL-M4MR123traffic0.00000.00250.00500.00750.01000.01250.01500.0175mean losskmb_bwkmb_delaykmb_lossDRL-M4MR123traffic5.05.56.06.57.07.58.0mean lengthkmb_bwkmb_delaykmb_lossDRL-M4MR0102030405060traffic05101520mean bwkmb_bwkmb_delaykmb_lossDRL-M4MR0102030405060traffic050100150200mean delaykmb_bwkmb_delaykmb_lossDRL-M4MR0102030405060traffic0.000.020.040.060.08mean losskmb_bwkmb_delaykmb_lossDRL-M4MR0102030405060traffic5.05.56.06.57.07.58.08.59.0mean lengthkmb_bwkmb_delaykmb_lossDRL-M4MR020406080100120traffic05101520mean bwkmb_bwkmb_delaykmb_lossDRL-M4MR020406080100120traffic050100150200mean delaykmb_bwkmb_delaykmb_lossDRL-M4MR020406080100120traffic0.000.020.040.060.08mean losskmb_bwkmb_delaykmb_lossDRL-M4MR020406080100120traffic5.05.56.06.57.07.58.08.59.0mean lengthkmb_bwkmb_delaykmb_lossDRL-M4MRThe results of Figure 10(b) are similar to Figure 10(a).

The average bottleneck bandwidth is much better than that
of the traditional KMB algorithm, the packet loss rate is also
better, and the delay is not diﬀerent from that of the traditional
algorithm. However, for better transmission performance, mul-
ticast tree length is sacriﬁced, and the DRL-M4MR algorithm
has the largest number of links in the multicast tree.

Training episodes M are set to 4000, and the cost of the train-
ing time with diﬀerent numbers of NLIs are shown in Figure 11.
The results show that the larger the number of NLIs, the longer
the training time, in a linear increasing relationship.

Figure 11: Training time cost with diferent number of NLIs.

7. Conclusion

In this paper, a multicast routing method DRL-M4MR based
on DRL in SDN is proposed. In a dynamic SDN environment,
DRL-M4MR utilizes the double dueling DQN reinforcement
learning method. After training, the agent can construct a mul-
ticast tree with optimal metrics according to the measured net-
work link information. The experimental results show that in
most cases, DRL-M4MR can construct multicast trees with a
higher average bottleneck bandwidth, a lower average delay,
and a lower average packet loss rate but more links. When the
traﬃc in the data plane changes, the agent can intelligently ad-
just the multicast tree to make the optimal metrics of the mul-
ticast tree in the current network link information, and the per-
formance of data transmission is guaranteed.

For future work, ﬁrst, we intend to improve the reward mech-
anism. The design of the step reward and ﬁnal reward cause
redundant branches; if only the ﬁnal reward is used to measure
the quality of the multicast tree, it will cause the agent to fall
into the local optimum, so hierarchical reinforcement learning
will be considered in the future. Second, distributed reinforce-
ment learning is considered to improve training eﬃciency and
reduce training time.

Acknowledgement

This work obtained the subsidization of National Nat-
ural Science Foundation of China (No.62161006),
In-
novation Project of Guangxi Graduate Education (No.

13

YCSW2022271), Guangxi Natural Science Foundation of
China (No. 2018GXNSFAA050028), and Guangxi Key Lab-
oratory of Wireless Wide band Communication and Signal Pro-
cessing (No. GXKL06220110).

References

[1] P. Paul, S. Raghavan, Survey of multicast routing algorithms and proto-
cols, in: Proceedings of the international conference on computer com-
munication, Vol. 15, Citeseer, 2002, p. 902.

[2] A. J. Kadhim, S. A. H. Seno, Energy-eﬃcient multicast routing protocol
based on sdn and fog computing for vehicular networks, Ad Hoc Net-
works 84 (2019) 68–81.

[3] M. R. Garey, R. L. Graham, D. S. Johnson, The complexity of comput-
ing steiner minimal trees, SIAM journal on applied mathematics 32 (4)
(1977) 835–859.

[4] A. Manzoor, M. Hussain, S. Mehrban, Performance analysis and route
optimization: redistribution between eigrp, ospf & bgp routing protocols,
Computer Standards & Interfaces 68 (2020) 103391.

[5] C. L. Zewei Yang, Reconstructing the Network: SDN architecture and

implementation, Publishing House of Electronics Industry, 2017.

[6] J. H. Cox, J. Chung, S. Donovan, J. Ivey, R. J. Clark, G. Riley, H. L.
Owen, Advancing software-deﬁned networks: A survey, IEEE Access 5
(2017) 25487–25526.

[7] R. Masoudi, A. Ghaﬀari, Software deﬁned networks: A survey, Journal

of Network and computer Applications 67 (2016) 1–25.

[8] J. Xie, F. R. Yu, T. Huang, R. Xie, J. Liu, C. Wang, Y. Liu, A survey
of machine learning techniques applied to software deﬁned networking
(sdn): Research issues and challenges, IEEE Communications Surveys &
Tutorials 21 (1) (2018) 393–430.

[9] J. R¨uckert, J. Blendin, R. Hark, D. Hausheer, Flexible, eﬃcient, and scal-
able software-deﬁned over-the-top multicast for isp environments with
dynsdm, IEEE Transactions on Network and Service Management 13 (4)
(2016) 754–767.

[10] L. Kou, G. Markowsky, L. Berman, A fast algorithm for steiner trees,

Acta informatica 15 (2) (1981) 141–145.

[11] H. Takahashi, et al., An approximate solution for the steiner problem in

graphs (1980).

[12] M. Hassan, A. Hamid, M. Alkinani, Ant colony optimization for multi-
objective multicast routing, Computers, Materials & Continua 63 (3)
(2020) 1159–1173.

[13] Q. Zhang, L. Ding, Z. Liao, A novel genetic algorithm for stable multicast
routing in mobile ad hoc networks, China Communications 16 (8) (2019)
24–37.

[14] S. Shakya, L. N. Pulchowk, A novel bi-velocity particle swarm optimiza-
tion scheme for multicast routing problem, IRO J. Sustain. Wireless Syst
2 (2020) 50–58.

[15] D. M. Casas-Velasco, O. M. C. Rendon, N. L. da Fonseca, Intelligent
routing based on reinforcement learning for software-deﬁned networking,
IEEE Transactions on Network and Service Management 18 (1) (2020)
870–881.

[16] S.-C. Lin, I. F. Akyildiz, P. Wang, M. Luo, Qos-aware adaptive routing
in multi-layer hierarchical software deﬁned networks: A reinforcement
learning approach, in: 2016 IEEE International Conference on Services
Computing (SCC), IEEE, 2016, pp. 25–33.

[17] D. M. Casas-Velasco, O. M. C. Rendon, N. L. da Fonseca, Drsir: A deep
reinforcement learning approach for routing in software-deﬁned network-
ing, IEEE Transactions on Network and Service Management (2021).
[18] C. Yu, J. Lan, Z. Guo, Y. Hu, Drom: Optimizing the routing in software-
deﬁned networks with deep reinforcement learning, IEEE Access 6
(2018) 64533–64539.

[19] H.-J. Heo, N. Kim, B.-D. Lee, Multicast tree generation technique us-
ing reinforcement learning in sdn environments, in: 2018 IEEE Smart-
World, Ubiquitous Intelligence & Computing, Advanced & Trusted
Computing, Scalable Computing & Communications, Cloud & Big
Data Computing, Internet of People and Smart City Innovation (Smart-
World/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), IEEE, 2018, pp. 77–
81.

[20] J. Chae, N. Kim, Multicast tree generation using meta reinforcement

                1 / ,            K R X Unyc-metro-data-sheet/.

[44] P. Tune, M. Roughan, Spatiotemporal traﬃc matrix synthesis, in: Pro-
ceedings of the 2015 ACM Conference on Special Interest Group on Data
Communication, 2015, pp. 579–592.

[45] Iperf, [Online], accessed: Mar. 10, 2022. https://iperf.fr/.

learning in sdn-based smart network platforms, KSII Transactions on In-
ternet and Information Systems (TIIS) 15 (9) (2021) 3138–3150.

[21] V. J. Rayward-Smith, The computation of nearly minimal steiner trees in
graphs, International Journal of Mathematical Education in Science and
Technology 14 (1) (1983) 15–23.

[22] S. Y. Zhou Ling, A delay-constrained steiner tree algorithm using mph,
Journal of Computer Research and Development 45 (5) (2008) 810–816.
[23] Y. Yanping, An improved algorithm for steiner trees, Journal of China

Institute of Communications 23 (11) (2002) 35–40.

[24] S. Kotachi, T. Sato, R. Shinkuma, E. Oki, Multicast routing model to min-
imize number of ﬂow entries in software-deﬁned network, in: 2019 20th
Asia-Paciﬁc Network Operations and Management Symposium (AP-
NOMS), IEEE, 2019, pp. 1–6.

[25] A. Latif, P. Kathail, S. Vishwarupe, S. Dhesikan, A. Khreishah, Y. Jarar-
weh, Multicast optimization for clos fabric in media data centers, IEEE
Transactions on Network and Service Management 16 (4) (2019) 1855–
1868.

[26] R. Touihri, S. Alwan, A. Dandoush, N. Aitsaadi, C. Veillon, M-crp: Novel
multicast sdn based routing scheme in camcube server-only datacenter, in:
2019 IEEE Global Communications Conference (GLOBECOM), IEEE,
2019, pp. 1–6.

[27] S. P. Sahoo, M. R. Kabat, The multi-constrained multicast routing im-
proved by hybrid bacteria foraging-particle swarm optimization, Com-
puter Science 20 (2019).

[28] R. Murugeswari, K. A. Kumar, S. Alagarsamy, An improved hybrid dis-
crete pso with ga for eﬃcient qos multicast routing, in: 2021 5th Interna-
tional Conference on Electronics, Communication and Aerospace Tech-
nology (ICECA), IEEE, 2021, pp. 609–614.

[29] J. Zhang, M. Ye, Z. Guo, C.-Y. Yen, H. J. Chao, Cfr-rl: Traﬃc engineering
with reinforcement learning in sdn, IEEE Journal on Selected Areas in
Communications 38 (10) (2020) 2249–2259.

[30] W.-x. Liu, J. Cai, Q. C. Chen, Y. Wang, Drl-r: Deep reinforcement
learning approach for intelligent routing in software-deﬁned data-center
networks, Journal of Network and Computer Applications 177 (2021)
102865.

[31] X. Guo, H. Lin, Z. Li, M. Peng, Deep-reinforcement-learning-based qos-
aware secure routing for sdn-iot, IEEE Internet of things journal 7 (7)
(2019) 6242–6251.

[32] T. Hendriks, M. Camelo, S. Latr´e, Q 2-routing: A qos-aware q-routing
algorithm for wireless ad hoc networks, in: 2018 14th International Con-
ference on Wireless and Mobile Computing, Networking and Communi-
cations (WiMob), IEEE, 2018, pp. 108–115.

[33] Z. Ning, N. Wang, R. Tafazolli, Deep reinforcement learning for nfv-
based service function chaining in multi-service networks, in: 2020 IEEE
21st International Conference on High Performance Switching and Rout-
ing (HPSR), IEEE, 2020, pp. 1–6.

[34] A. F¨orster, A. L. Murphy, Froms: A failure tolerant and mobility enabled
multicast routing paradigm with reinforcement learning for wsns, Ad Hoc
Networks 9 (5) (2011) 940–965.

[35] L. Liao, V. C. Leung, Lldp based link latency monitoring in software
deﬁned networks, in: 2016 12th International Conference on Network
and Service Management (CNSM), IEEE, 2016, pp. 330–335.

[36] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
et al., Human-level control through deep reinforcement learning, nature
518 (7540) (2015) 529–533.

[37] R. C. Prim, Shortest connection networks and some generalizations, The

Bell System Technical Journal 36 (6) (1957) 1389–1401.

[38] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, N. Freitas, Du-
eling network architectures for deep reinforcement learning, in: Interna-
tional conference on machine learning, PMLR, 2016, pp. 1995–2003.
[39] T. Schaul, J. Quan, I. Antonoglou, D. Silver, Prioritized experience replay,

arXiv preprint arXiv:1511.05952 (2015).

[40] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dab-
ney, D. Horgan, B. Piot, M. Azar, D. Silver, Rainbow: Combining im-
provements in deep reinforcement learning, in: Thirty-second AAAI con-
ference on artiﬁcial intelligence, 2018.

[41] Ryu, [Online], accessed: Mar. 10, 2022. https://ryu-sdn.org/.
[42] Mininet, [Online], accessed: Mar. 14, 2022. http://mininet.org/.
[43] New york metro ibx data center data sheet, [Online], accessed: Mar.
14, 2022. https://www.equinix.com/resources/data-sheets/

14

