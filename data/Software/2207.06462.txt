2
2
0
2

l
u
J

3
1

]
h
p
-
t
n
a
u
q
[

1
v
2
6
4
6
0
.
7
0
2
2
:
v
i
X
r
a

Quantum Metropolis Solver:
A Quantum Walks Approach to Optimization Problems

Roberto Campos,1, 2, ‚àó P. A. M. Casares,1, ‚Ä† and M. A. Martin-Delgado1, 3, ‚Ä°
1Departamento de F¬¥ƒ±sica Te¬¥orica, Universidad Complutense de Madrid.
2Quasar Science Resources, SL.
3CCS-Center for Computational Simulation, Universidad Polit¬¥ecnica de Madrid.
(Dated: July 15, 2022)

The eÔ¨Écient resolution of optimization problems is one of the key issues in today‚Äôs industry. This
task relies mainly on classical algorithms that present scalability problems and processing limitations.
Quantum computing has emerged to challenge these types of problems. In this paper, we focus on
the Metropolis-Hastings quantum algorithm that is based on quantum walks. We use this algorithm
to build a quantum software tool called Quantum Metropolis Solver (QMS). We validate QMS with
the N-Queen problem to show a potential quantum advantage in an example that can be easily
extrapolated to an ArtiÔ¨Åcial Intelligence domain. We carry out diÔ¨Äerent simulations to validate the
performance of QMS and its conÔ¨Åguration.

I.

INTRODUCTION

Optimization problems are solved daily: select-
ing which products are aÔ¨Äordable to purchase while
maximizing their quantity and quality (knapsack
problem [1]), choosing the best public transport
combination to save commuting time (a variant of
the TSP problem [2]), or visiting the most interest-
ing landmarks when traveling to a new city, since
vacation time is limited (goal oversubscription prob-
lem [3]). Similarly, diÔ¨Äerent industries face complex
optimization problems routinely. However, not all
optimization problems have a simple solution. The
diÔ¨Äerence between daily problems and industrial op-
timization problems is that the second ones require
large computation capabilities. It is due to the huge
number of possible combinations that it is necessary
to check to reach an optimal solution.

Optimization problems of interest to the industry
often involve multiple variables with a high number
of dimensions and complex optimization functions,
making each possible problem conÔ¨Åguration diÔ¨Écult
to evaluate. In general, the function to be optimized
is a resource that can ultimately have important eco-
nomic consequences for companies and individuals.
Some examples of these problems are the routing
problem, which consists in Ô¨Ånding an optimum path
between the start and the end in a given set of loca-
tions [4‚Äì6], portfolio optimization in Ô¨Ånance to de-
cide when and which products are bought and sold
based on a risk-reward balance [7, 8] or protein fold-
ing problem that requires to minimize the energy by
rotating the protein structure [9].

‚àó robecamp@ucm.es
‚Ä† pabloamo@ucm.es
‚Ä° mardel@ucm.es

These problems also suÔ¨Äer the ‚Äòcurse of dimension-
ality [10, 11] deÔ¨Åned by Bellman. This phenomenon
occurs when the dimensionality of the data grows
very fast, causing the volume of the data to grow as
well. As a result, the data becomes scattered and
diÔ¨Écult to cluster.

Small optimization problems can be solved by
brute force. Unfortunately, many others scale ex-
ponentially and brute force is no longer useful [12].
The procedure to Ô¨Ånd the solution to larger prob-
lems checks a minimal subset of all possibilities, but
it still requires evaluating many possible combina-
tions. The technique used means, for example, the
possibility of reducing the time to obtain a solu-
tion from centuries to hours or days, which turns
the problem from intractable to solvable. Moreover,
it is often impossible to check that the best solution
found so far is indeed optimal.

Optimization problems have also been extensively
studied from a theoretical point of view, and sev-
eral toy problems have been developed as simpliÔ¨Åca-
tions of real problems with fundamental similarities,
which allow testing the performance of new algo-
rithms on easy-to-execute instances.
It is possible
to Ô¨Ånd in this category the traveling salesman prob-
lem (TSP) [2], knapsack problem, [1] or N-Queen
problem [13], which have similarities with the rout-
ing problem, risk/reward Ô¨Ånance problem and the
problem of selecting the best action to execute, re-
spectively.

The key aspect of optimization problems is the
necessity of tuning various parameter values until a
minimum is reached. This process of trial, error,
and reÔ¨Ånement can be automated with a computer
to obtain an acceleration by simulating the problem.
Therefore, it is important to have an accurate map-
ping between the real problem and the simulation.
Some problems like the connections between cities

 
 
 
 
 
 
(in the TSP) are easy to represent on a computer,
while others, like modeling the air around the wing
of an airplane, are more challenging, so simpliÔ¨Åed
simulations are used. In some cases, oversimpliÔ¨Åca-
tion might even be needed because of the intractabil-
ity of the original problem, as historically was the
case with lattice models of protein folding [14].

There are some diÔ¨Äerent representation options
to convert the problem in an algorithm-solvable in-
stance. In this work, a four-element representation
is chosen. The elements are:

‚Ä¢ States: Possible values that the system can

take for a given problem.

‚Ä¢ Transitions: Possible states generated from

each state.

‚Ä¢ Evaluation function: Function to calculate the

reward of each state.

‚Ä¢ Goal: Objective of the problem, minimize or

maximize the reward function.

The majority of the complex optimization prob-
lems belong to the NP-complexity class [15]. Thus,
polynomial advantages are often the best one may
hope to attain. Quantum computing is a natural ap-
proach, based on the fact that quantum walks can
achieve a quadratic speedup in the hitting time over
their classical counterparts [16].

In order to introduce the quantum algorithm that
we have selected in this work, we have to explain
before some classical algorithms. Classical random
walks are not only very powerful, but they form the
basis for also very widely used Monte Carlo algo-
rithms, routinely used for optimization problems.
The Monte Carlo method consists of a random sam-
pling of state space to approximate a function.
It
works better with a larger population because the
error classically decreases as 1/
N [17]. The most
relevant aspect of the Monte Carlo method is that
it serves as a basis for optimization algorithms.

‚àö

A related technique also based on random walks
is the simu-
and inspired by statistical physics,
lated annealing algorithm [18]. The core concept
is a search algorithm that always accepts transitions
that lower the energy, but with a certain probabil-
ity it also does to higher energies. The probability
to move to a higher energy state at the beginning
of the execution is high because the simulated tem-
perature starts warmer. However, as steps are exe-
cuted, the algorithm goes cooler and the probability
is reduced. That process helps the algorithm ex-
plore many states at the beginning, avoiding local
minima. Because of this, the algorithm converges
slowly to the minimum energy state.

2

Combining random sampling of Monte Carlo
method from a probability distribution and the
guided stochastic search of random walks and sim-
ulated annealing results in an algorithm called
Metropolis-Hasting [19, 20]. It is used to approxi-
mate a probability distribution œÄx by mixing it with
a random walk W until an equilibrium is reached,
W œÄ = œÄx.

The Metropolis-Hastings algorithm requires three
(i) a procedure to sample initialization
methods:
states, (ii) a procedure to propose state transitions,
and (iii) an evaluation function that scores how good
is a given state. The latter is often called ‚Äòenergy‚Äô E
due to its connection to statistical physics. It will de-
termine the acceptance probability of the transition
proposed in point (ii), min(1, exp(‚àíŒ≤‚àÜE)), where Œ≤
is a parameter called inverse temperature.

There exists a quantum version of random walks.
Quantum walks can also be understood as a gener-
alization of Grover‚Äôs algorithm [21]. The Ô¨Årst pro-
posal, by Ambainis [22], was restricted to Johnson
graphs. Soon, Szegedy presented bipartite quan-
tum walks generalizable to any ergodic-chain prob-
lem [23]. Both can be shown to oÔ¨Äer Grover-like
quadratic speedup in the hitting time,
in other
words, in the time required to Ô¨Ånd the marked item.
The latter quantum walk has been widely used, for
example in the context of Quantum Metropolis algo-
rithms [16, 24]. Also, Szegedy‚Äôs proposal has found
a variety of applications [25‚Äì28].

Most of the previous quantum Metropolis algo-
rithms assumed a slowly changing Œ≤ parameter and
often phase estimation to evolve the state from the
uniform superposition to the target stationary dis-
tribution [29, 30]. However, this is diÔ¨Äerent from
how classical algorithms operate, where Œ≤ is changed
much rapidly, and no additional techniques other
than random walks are used.
It is for this reason
that Lemieux et al. proposed a quantum version of
the Metropolis-Hastings algorithm that makes use of
quantum walks heuristically, similar to how random
walks are used classically [31].

In this work, we discuss and analyze the behav-
ior of the quantum Metropolis-Hastings (M-H) algo-
rithm in an optimization problem arising in the Ô¨Åeld
of ArtiÔ¨Åcial Intelligence (AI). To test the M-H algo-
rithm and facilitate other users the usage of quantum
M-H, we implemented a software tool called Quan-
tum Metropolis Solver (QMS). Our tool can get as
input a description of an optimization problem and
generate the minimum cost solution. Besides, it has
extra functionalities like plotting, classical solution
comparison, and deep analysis of quantum M-H al-
gorithm solutions. The tool is open-source philoso-
phy oriented and were coded in Python using Qiskit
modules.

QMS application is threefold. First,

it can be
used as a metric tool to test the performance of the
quantum M-H algorithm in a concrete search prob-
lem. Additionally, QMS can be integrated into hy-
brid classical-quantum algorithms since QMS input
is a classical problem description, but it can gener-
ate classical or quantum output. Finally, comparing
classical and quantum variants to computationally
assess potential quantum advantages.

II. THE METROPOLIS-HASTINGS
ALGORITHM: CLASSICAL VS. QUANTUM

The M-H algorithm is a Markov Chain procedure
because transition probabilities depend only on the
current state, and it is a Monte Carlo technique due
to the generation of a random sequence of samples
from a probability distribution g(x). The result of
these two properties is a Markov Chain Monte Carlo
algorithm, able to rapidly mix and generate low en-
ergy states.

The Metropolis-Hastings algorithm is used to
sample the stationary state of the Markov chain œÄx.
As a starting point, a uniformly random sample x
is generated. Then, new samples (x(cid:48)) are gener-
ated from the previous sample using some gener-
ation function g(x(cid:48)|x), and accepted according to
their energy diÔ¨Äerences. If the energy of x(cid:48) is lower
than x it is accepted, else an acceptance probabil-
ity is calculated using an evaluation function f (x)
as Œ± = f (x)/f (x(cid:48)). This process is similar to a
random walk with steps dictated by W , preparing
a stationary state œÄx.
Its stochasticity allows the
algorithm to explore a larger search area and avoid
getting trapped in local energy minima. Fig. 1 shows
a scheme of this algorithm.

The M-H algorithm is an optimization technique
with great utility in domains ruled by a minimization
or maximization function and a well-known proba-
bility distribution to generate successors. Particu-
larly interesting are problems with uncertainty in
the outputs, in which the challenge is to Ô¨Ånd the
goal state, not the path to reach it, as it could be
in TSP. The distinguishing advantage of M-H over
other heuristic algorithms is the rapid generation of
successors due to its stochasticity and its unique de-
pendence on the previous state [32]. On the other
hand, that high-speed generation may penalize the
M-H algorithm with a poorly guided search far from
the optimal, thus M-H algorithm is better suited
for highly complex and unstructured conÔ¨Åguration
spaces where the stochastic state generation is ad-
vantageous.

The process of Ô¨Ånding a solution by iteratively try-
ing diÔ¨Äerent steps to reÔ¨Åne the current state to an

3

acceptable solution is an old and well-known process.
It has been one of the foundations of human reason-
ing and problem-solving. However, the big change
comes when there are machines capable of perform-
ing thousands of these reÔ¨Ånement steps quickly or
even at the same time. Using this fast step execu-
tion, the M-H algorithm takes the brute-force phi-
losophy and incorporates Markov theory to achieve
an algorithm capable of testing many states with a
minimum guided search. For this reason, one of the
strengths of M-H lies in the ability to evaluate suc-
cessive states quickly.

A. The Classical MH Method

The performance of the classical M-H algorithm
is strongly inÔ¨Çuenced by two factors: mixing time
(MT) of the Markov chain deÔ¨Åned in Eq. 1 and
Monte Carlo error. These two aspects determine
the number of necessary steps to get an acceptable
solution, and they are key in creating a quantum
version that enhances both. Markov chain mixing
time is the time that a Markov chain takes to reach
a stationary distribution. Ref. [33] explains the im-
portance of this value optimization. In contrast, the
Monte Carlo error is the distance between the cal-
culated distribution at step N and the stationary
distribution. In Ref. [34] there are examples to re-
duce the error of the Monte Carlo methods.

The Mixing Time M T ((cid:15)) can be interpreted as the
minimum number of steps t of the Markov chain that
should be applied to any initial distribution (œÄ0),
such that the result is (cid:15)-close to the stationary dis-
tribution (œÄ), under distance D (Eq. 2).

M T ((cid:15)) = min{t|‚àÄt(cid:48) > t, ‚àÄœÄ0, D(P t(cid:48)

œÄ0, œÄ) < (cid:15)}, (1)

Distance D(p, q) between probability distributions
p and q is in turn deÔ¨Åned as the sum of probability
diÔ¨Äerences at each vertex of the Markov chain, under
those distributions,

D(p, q) =

1
2

N
(cid:88)

v=1

|pv ‚àí qv|.

(2)

In this work, we have focused on an M-H algo-
rithm that converges into the lowest-energy state.
However, since the successor generation in M-H is
governed by a probability distribution function, it
can be also used to sample the unknown stationary
distribution of a Markov chain. The algorithm can
generate samples from the probability distribution,
which can be used to approximate a probability den-
sity function [35]. SpeciÔ¨Åcally, it can be applied to
problems that prohibit a complete enumeration of all

4

FIG. 1. This Ô¨Ågure shows all steps of the Metropolis-Hastings algorithm. The search for the minimum energy state
starts in the state xt. Then, the Ô¨Årst step is to propose a new candidate x(cid:48) related with xt by the distribution function
g(xt). Once the candidate is generated, it is necessary to calculate a numeric value Œ± of how much better/worse is it
over the current state. Concurrently, a random number r between 0 and 1 is generated. Then, both values, Œ± and r
are compared. If Œ± is greater or equal than r, the change is accepted and x(cid:48) becomes in xt, else x(cid:48) is discarded and
the new x(cid:48) will be calculated again over the same xt. This process is repeated until a Ô¨Åxed number of w is reached
or the evolved distribution, over time, s0 is less than an (cid:15) value, diÔ¨Äerent from the objective distribution œÄ.

paths [36]. Again, this M-H sampling can be quan-
tum versioned naturally, since the quantum circuit
of the M-H can be executed repeatedly until getting
a distribution of the result of each execution shot.

As it is explained above, the limiting factor to get-
ting a speed-up with the M-H algorithm is the mix-
ing time and the error reduction. Going one step
further, it is possible to identify three points that
we can optimize in the M-H execution: reduce the
number of evaluated states, avoid getting stuck at
local minima, and evaluate states faster. Quantum
computing can help in these points as the eigenvalue
gap of the quantum walk is quadratically smaller
than the classical eigenvalue gap as explained in [37]
with the formula ‚àÜ = ‚Ñ¶(Œ¥1/2) being Œ¥ the eigen-
value gap of the classical walk and ‚àÜ the phase gap
of the quantum walk. Although there are classical
approaches to solve this problem [38], Grover‚Äôs al-
gorithm and quantum walks add amplitudes instead
of probabilities, and their diÔ¨Äerence ends up showing
as a quadratic speedup [23].

Algorithm 1: Metropolis-Hastings
algorithm

Initialize xt ‚àº g(x)
for iteration step = 1,2,..., W do

Propose: x(cid:48) ‚àº g(x(cid:48)|xt)
Acceptance Probability
Œ±(x(cid:48)|xt) = min{1, f (x(cid:48))
f (xt) }
Random variable r: r ‚àà [0, 1]
if r ‚â§ Œ± then

Accept the proposal: xt+1 ‚Üê x(cid:48)

else

Accept the proposal: xt+1 ‚Üê xt

end

end

TABLE I. Algorithm of Quantum Metropolis-Hastings,
detailing how the new candidate is proposed and the ac-
ceptance probability and random number are calculated.
This algorithm executes several steps W.

123455Initializationùë•ùë°ùëñùëìùëü‚â§Œ±B. A Quantum version of the MH Method

A quantum version of the Metropolis-Hastings
algorithm exploits a reduced complexity due to a
smaller eigenvalue gap in comparison to its classical
counterparts. That fact helps to infer the minimum
energy state quicker. The essence of this advantage
comes from the application of the quantum walk op-
erator to an initial uniform superposition of all pos-
sible states such that the number of steps to mix the
chain is reduced.

Quantum walks can be understood as a general-
ization of Grover‚Äôs algorithm [39]. With two Grover-
like reÔ¨Çections, Szegedy [23] constructs a quantum
walk on a bipartite graph. However, in this work,
we substitute the bipartite graph with a coin |c(cid:105) via
an isomorphism [31], which creates an entanglement
with the states |s(cid:105). This produces a quantum walk
|Œ®(cid:105) = |s, c(cid:105), where the states are represented as a
superposition |s(cid:105) of possible states.

|s(cid:105) =

(cid:40)

(cid:88)

x‚ààN

(cid:41)

Œ±x |x(cid:105)

‚àà Hs,

(3)

and the coin (|c(cid:105)) is in the coin space C2

|c(cid:105) = {Œ±1 |‚Üë(cid:105) + Œ±2 |‚Üì(cid:105)} ‚àà Hc.

(4)

In [31], a Szegedy quantum walk is used as a basis
to construct the circuit of a Quantum Metropolis-
Hastings algorithm. In this work, we show the im-
plementation complexity of the W in a quantum cir-
cuit using unitary operators. The main challenge is
the application of the operator and its inverse (uni-
tary operator) in each W because the inverse of the
operator depends on whether the change is accepted
or not. In the ideal case, it would be necessary to
apply a conditional inverse operator in each step. In
the M-H algorithm, after a change is proposed, there
are two options, accept or reject a proposed change
and this duality is a key problem to create a uni-
tary operator. The solution proposed by Lemieux
et al. [31] is a diÔ¨Äerent unitary operator for W that
is isomorphic to the original Szegedy walk operator
UW and is represented as:

ÀúU = RV ‚Ä†B‚Ä†F BV,

(5)

where R is the reÔ¨Çection operator, V is the move
preparation operator, B is the coin operator and F
is the spin-Ô¨Çip operator. These discrete operators
simplify the implementation to a circuit with dis-
crete variables and have similar behavior as in the
classical random walk in a Metropolis-Hastings al-
gorithm. However, here these operators are used to
generate Grover-like rotations with the potential to
exhibit a polynomial advantage.

5

The metric proposed and used by [31] is called
Time To Solutions and denoted as TTS. It is a Ô¨Åg-
ure of merit that measures the expected number of
steps required to Ô¨Ånd a solution. It is helpful to com-
pare procedures that need to be repeated in case of
failure, like this sampling algorithm. TTS strikes a
balance between probability increase and the num-
ber of steps in each execution, which means that
lower TTS implies less expected execution time.

T T S(t) := t

log(1 ‚àí Œ¥)
log(1 ‚àí p(t))

,

(6)

where t is the number of steps executed, Œ¥ is the suc-
cess probability, and p(t) is the probability of hitting
the ground state after t steps. With this metric and
a scaling law exponent analysis, Lemieux et al. got
a polynomial speedup of 0.75, e.g. classical TTS =
O(quantum TTS0.75), arguing that their proposal
scales better than the classical Metropolis-Hastings,
and can thus be advantageous in bigger problem in-
stances. The exponent indicates how the relation-
ship between the classical and quantum algorithms
scales. Lower than 1 means that quantum complex-
ity scales more favorably than the classical. We can
estimate this exponent with a linear least-square Ô¨Åt-
ting in the logarithmic scale for both classical and
quantum minimum TTS. Since we want to see the
scaling law exponent of quantum TTS against clas-
sical TTS, we follow the equation y = bxa, being x
and y, classical (cT T S) and quantum (qT T S) TTS
respectively. In logarithmic scale:

log(qT T S) = log(b) + a log(cT T S),

(7)

being a the exponent to deÔ¨Åne the scaling between
qT T S and cT T S. This exponent deÔ¨Ånes three re-
gions,

a =

Ô£±
Ô£¥Ô£≤

Ô£¥Ô£≥

quantum TTS < classical TTS,
> 1
1,
quantum TTS = classical TTS,
< 1, quantum TTS > classical TTS.

(8)

We present a software framework whose core is the
circuit proposed by [31] and build a library around
the quantum Metropolis-Hastings, allowing any user
to solve optimization problems with the quantum M-
H algorithm. We called this software architecture,
Quantum Metropolis Solver (QMS). We implement
it in a quantum simulator running on a classical com-
puter.

Our contributions are:

‚Ä¢ Software tool: We give the community easy-
to-use software to solve problems in which
Metropolis-Hastings has a proven advantage.

‚Ä¢ Study of Quantum Metropolis-Hastings
application to an optimization problem
related with ArtiÔ¨Åcial Intelligence: We
provide evidence that the quantum advantage
of quantum walks and QM-H algorithm can
be applied to ArtiÔ¨Åcial Intelligence to optimize
the search process inherent in any AI tech-
nique. The case study to validate this idea
is the N-Queen problem, which has been used
recurrently and is still used, as a benchmark
for new classical AI algorithms.

‚Ä¢ Scaling law: We analyze the performance of
QMS by Ô¨Ånding the scaling law the N-Queen
problem that has NP-complexity. This way, we
add up another instance of applying quantum
M-H algorithm to the previously analyzed case
study of the Protein Folding problem [40].

‚Ä¢ Comparison of diÔ¨Äerent quantum walks
implementations: We compared three dif-
ferent
implementations of quantum walks
and Quantum Metropolis-Hastings on the N-
[23]
Queen problem. The proposal of Ref.
uses a bipartite graph to search in the state
space. This algorithm was later on modiÔ¨Åed
in Ref. [31] with discrete operators and substi-
tuting the n-dimensional search with a binary
search performed with a coin. Finally, we also
compared the results with a diÔ¨Äerent operator
ordering similar to the qubitization method in
Ref. [41].

III. QMS: QUANTUM METROPOLIS
SOLVER SOFTWARE TOOL

The underlying motivation for implementing QMS
is to give the scientiÔ¨Åc and industrial community a
test-bed to solve optimization algorithms with quan-
including a comparison with its
tum algorithms,
classical counterpart. This software tool abstracts
the inherent complexity of implementing quantum
algorithms. Users can deÔ¨Åne a problem to solve with
just an input Ô¨Åle. In this Ô¨Åle, the problem is deÔ¨Åned
as a set of states and associated costs. Then, QMS
will return the state with the least cost as a solution.
It is assumed that states are connected among them
in a graph so that a Markov chain algorithm can be
applied.

QMS has been designed, not just to oÔ¨Äer a Ô¨Ånal
result, but also to show statistics that help to under-
stand the performance of the algorithm. There are
three possible outputs of QMS: the minimum cost
state, the TTS result, or the probability distribution.
Stressing the point of a test-bed, QMS allows a TTS
comparison between the quantum algorithm and its

6

classical version. In such a mode, the same problem
is executed in a classical Metropolis-Hastings algo-
rithm and a quantum one, and both minimum TTS
achieved by each is shown. Additionally, a plot is
generated with the TTS curve as a function of t in
(6) for both the quantum and classical M-H algo-
rithms.

Our software tool, detailed in Fig. 2, has been
thought of as a user-friendly library that simpliÔ¨Åes
its use. The user just deÔ¨Ånes the problem in an input
JSON, a Ô¨Åle that stores simple data structures and
objects in a standard format. Then, it is possible to
conÔ¨Ågure the QMS execution with a conÔ¨Åguration
Ô¨Åle where some parameters are deÔ¨Åned. For exam-
ple, the number of steps, which is equivalent to the
number of times that operator W is applied, can be
tuned just by modifying the value of the parameters
initial step and Ô¨Ånal step. Any number of steps in
between will then be analyzed.

The core of QMS is an implementation of the cir-
cuit proposed in [31] with some optimizations and
modiÔ¨Åcations. From Eq. 5 the operators are the
same:

‚Ä¢ V is move preparation: This operator creates
a superposition of all possible state transitions.

‚Ä¢ B is coin preparation: This operator creates
the superposition of the coin and rotates the
coin in those states where the change is ac-
cepted.

‚Ä¢ F is conditional move: This operator performs
the change in the states in which the coin
has been rotated. This is the M-H accep-
tance/rejection step.

‚Ä¢ R is reÔ¨Çection: Similar to a Grover reÔ¨Çec-
tion for amplitude ampliÔ¨Åcation of the marked
states.

However, the representation in the register and
operators changes slightly, we substitute unary rep-
resentation in the move register with binary rep-
resentation. That change allows us to reduce the
number of qubits in the whole system. Besides, the
heuristic that guides the algorithm is based on an
inverse temperature parameter Œ≤. The acceptance
value comes from the condition:

Aij = min

1, e‚àíŒ≤(Cj ‚àíCi)(cid:17)
(cid:16)

,

(9)

where Aij is the acceptance ratio of the proposed
changed, Œ≤ = 1/T , Cj is the candidate cost and Ci
is the old state cost. It implies that if the cost of the
candidate is lower, the change is accepted. Other-
wise, the change is only accepted with exponentially
decreasing probability in the inverse temperature.

The input Ô¨Åle of QMS is a set of tuples (state,
cost). States are represented with a binary notation
starting from 0 and cost is a real number. One ex-
ample tuple could be [(101): 65.53], 101 is the states
in binary notation and 65.53 is the cost.

For the internal representation, QMS requires
(cid:100)log2(n)(cid:101) qubits to represent n input states. The
cost associated with each state is not directly repre-
sented, by contrast, what is stored in QMS is the cost
diÔ¨Äerence between connected states, represented as
‚àÜ. ‚àÜij is set to 1 if the cost of candidate state j is
lower than the cost of the state i. Otherwise, ‚àÜij
stores a codiÔ¨Åcation of the cost diÔ¨Äerence between
the states, as shown in this equation:

‚àÜij =

(cid:40)
1
Ej < Ei,
e‚àíŒ≤(Ej ‚àíEi) Ej ‚â• Ei,

(10)

this codiÔ¨Åcation corresponds to the probability of
change considering the Œ≤ of the step. Then, all prob-
abilities are stored in a QRAM as an oracle that can
be accessed in each W .

We deÔ¨Åne four registers to store all information in

QMS:

‚Ä¢ States |¬∑(cid:105)S: Contains each possible state af-
fected by operator move preparation (V ).

‚Ä¢ Move |¬∑(cid:105)M : Contains the candidate move
state aÔ¨Äected by operator move preparation
(V ).

‚Ä¢ Coin |¬∑(cid:105)C: Coin aÔ¨Äected by operator coin

preparation (B).

‚Ä¢ Oracle |¬∑(cid:105)O: Contains the acceptance proba-
bility between all connected states aÔ¨Äected by
operators conditional move (F ) and reÔ¨Çection
(R).

The move register |¬∑(cid:105)M can be divided into two
registers: move id register |¬∑(cid:105)M i to know which state
is going to be changed and move value register |¬∑(cid:105)M v
that indicates how the register is to be changed (+1,
-1, swap left, swap right, etc.).

In any optimization problem,

it is critical to
choose the initial state distribution. The gap be-
tween the initial state and the goal state determines
the execution time or the solution quality. QMS re-
ceives an input set of states from which it has to Ô¨Ånd
the least energetic one. QMS takes an initial point,
and it applies the W operator the number of times
deÔ¨Åned by the user. The state reached after the ap-
plication of W operators is the solution returned by
QMS. For that reason, our library allows diÔ¨Äerent
initial states to leave to the user the freedom of se-
lecting at which point the search should start. It is
even possible not to select just a point, but to select a

7

probability distribution that will determine the ini-
tial point for the search. The preparation of such
probability distribution is carried out by a process
called initialization.

‚Ä¢ Fixed: QMS starts the search in a state se-
lected by the user. This option is useful for re-
Ô¨Ånement problems in which there is a Ô¨Årst ap-
proximate solution and the optimization con-
sists of a search for a more quality solution
close to the initial approximation.

‚Ä¢ Random: QMS starts in a uniform superpo-
sition of states that is equivalent to a random
state because no one has more probability than
others to be selected.

QMS initialization can also be classiÔ¨Åed by how

states are generated, as follows:

‚Ä¢ Sequential: This mode generates candidates
sequentially, just adding or subtracting one
unity to the coordinate. For example, if the
problem is a pawn moving into a chess board,
states could be represented as the row and col-
umn occupied by the pawn (column, row). So,
if the pawn is in (4, 5) one possible candidate
is to add 1 position in the column, resulting in
a state (5, 5).

‚Äì Circular: This option allows connect-
ing frontier states with periodic boundary
conditions. For example, if the previous
pawn is in position (5, 7), so it is in the
upper row, it would be possible to add
one row to place the pawn in the lower
row, state (5, 0).

‚Äì Non-circular: This option does not al-
low connecting frontier states, so it is con-
Ô¨Ågured with non-periodic boundary con-
ditions. For example, if the pawn is in
the row frontier (5, 7), it is not allowed
to sum 1 in rows.

‚Ä¢ Swap: This mode generates candidates ex-
changing coordinates, and it does not allow
collisions. For example, if there are 3 queens
placed on the board and each queen is in one
column, such that there are no two queens in
the same column. The state will represent the
row of the queens. One possible state is (1, 0,
2), Ô¨Årst queen in row 1, second queen in row 0,
etc. The candidates are generated by exchang-
ing queen positions, for example, a candidate
switching the Ô¨Årst and second queen would re-
sult in (0, 1, 2), which means the Ô¨Årst queen
in row 0, the second queen in row 1, etc.

QMS is a software tool with a white box design
and modular architecture. It means that it is com-
posed of an aggregation of modules that receive in-
put, process it, and serve the result to other modules.
These modules can be analyzed, modiÔ¨Åed, or even
substituted by other modules with the same data in-
terface following similar input/output format rules.
This open and Ô¨Çexible architecture is an advantage
that can enhance the use of the tool by the commu-
nity because it is easy to understand, including new
functionalities that may be added in the future or
Ô¨Åxing bugs. QMS architecture is detailed in Fig. 2.
The whole architecture has been implemented us-
ing Python3 because it is an open-source language
and very used by the developers‚Äô community. The
quantum module calls Qiskit and Qiskit-Aer for
the quantum simulations in the classical computer.
Qiskit is also an open-source Python module that
simpliÔ¨Åes circuit creation and simulation, and it has
proven good performance with large circuits with
more than 25 qubits and multi-threads execution,
as can be seen in Ô¨Ågure 11 of [42].

IV. CASE STUDY: QUANTUM
ARTIFICIAL INTELLIGENCE

Any Machine Learning (ML) algorithm modiÔ¨Åes
its internal state and world representation follow-
ing a deliberative process. This process is based
on reasoning about the input data to obtain some
knowledge model of the problem that the algorithm
is solving. During the reasoning process, the system
generates diÔ¨Äerent hypotheses to explain the envi-
ronment and execute the correct action. For exam-
ple, adjusting connection weights in an artiÔ¨Åcial neu-
ral network or modifying the value of the action in
a Reinforcement Learning agent.

The hypothesis generation and selection steps re-
quire a fast search in the state space (hypothesis
space) to evaluate which of all hypotheses available
Ô¨Åts better with the problem to be solved. This search
is one of the bottlenecks of any ML algorithm, and
quantum computing can speed it up. For this rea-
son, we consider that QMS can help in the ArtiÔ¨Åcial
Intelligence domain, improving the performance of
the search process. We decided to validate our tool
in a problem that has been used many times as an AI
benchmark, the N-Queen problem. Since this prob-
lem is in essence an NP-complexity search problem
with multiple similarities with the search of an AI
algorithm, any algorithm which gets a good perfor-
mance in N-Queen can be easily adapted to other AI
problems based on search, as search of hypothesis in
a ML algorithm.

The N-Queen problem is a spin-oÔ¨Ä of the classical

8

chess problem [43]. The N-Queen goal state, is a
chess board of n rows and n columns with n queens
such that no queen attacks the others. Therefore,
there are no two queens in the same row, column, or
diagonal as shown in Fig. 3. This problem has been
extensively studied, and many solutions have been
proposed [44, 45]. One of the most common solu-
tions is a search between the diÔ¨Äerent possible con-
Ô¨Ågurations until an acceptable distribution is found.
It is similar to Ô¨Ånding the best hypothesis between
all the generated ones to explain the perceived world
by an agent (input data).

This kind of reasoning, searching for a hypothe-
sis that explains and generalizes input data, can be
seen as an abstraction of general knowledge from
concrete examples, and it is known as inductive rea-
soning. As it is explained in Ref. [46], the induc-
tive reasoning goal is to Ô¨Ånd the hypothesis with the
best balance between the classiÔ¨Åcation of the exist-
ing examples and the generalization of the new ex-
amples. The search of this hypothesis with the best
reward generalization/classiÔ¨Åcation is equivalent to
the N-Queen search and, for this reason, N-Queen is
a typical problem used as a benchmark in classic AI
papers [13]. For example, this problem was used to
introduce the backtracking search [47].

During environment interaction, any rational
agent generates a set of hypotheses or associations
from input data, which is similar to human learn-
ing from the environment or describing a problem.
Then, it searches for which hypothesis is the best
one to explain the world. This process is known
as search in a decision tree. Sometimes, the agent
selects one hypothesis and adapts it to new input
data, but even that is similar to a search of vari-
ations around the Ô¨Åxed point performed by QMS.
Both symbolic learning (e.g. using Ô¨Årst-order logic)
and non-symbolic (e.g. using weights in a neural
network) require the hypothesis space search to Ô¨Ånd
the best next decision to take.

That process is similar to most machine learning
algorithms. As Mitchell describes it in Ref. [48], the
process of learning can be understood as a searching
task in a large space of hypotheses and the to Ô¨Ånd
the hypothesis that best Ô¨Åts the training and upcom-
ing examples. This is the main reason why if it is
possible to speed up the search process using quan-
tum computing, it is possible to get advantages in
AI algorithms. The reason is that what is commonly
called the ‚Äúlearning process‚Äù is just the process of
searching for the best explanation (hypothesis) for
the data received, trying to be as ready as possible
for the future data entering the system.

Since the Ô¨Ånal state of the problem solved by QMS
is unknown because it has uncertainty in the out-
puts, QMS algorithm makes a state-space search

9

FIG. 2. Scheme of the QMS architecture. It receives a JSON Ô¨Åle with a description of the possible states and the
values associated with them. Then, the deltas between states (‚àÜij) are calculated and sent to the Quantum M-H
module. This QM-H module constructs an initialization circuit to get the initial state xt. Besides, the circuit of
operator W is generated many times with a Ô¨Åxed parameter. Once the circuit is created, QM-H executes the circuit.
The results obtained after execution using raw amplitudes are processed to get a plot with the evolution of the TTS,
a numeric TTS value, or the probabilities. In parallel with the Quantum M-H execution, QMS has the option to
execute a classical M-H, to compare both versions of the M-H algorithm. This classical M-H module has the same
structure and connections with input and output as its quantum counterpart.

guided by the objective of minimizing a cost func-
tion. An AI agent does a similar search to optimize
its interaction with the environment, to speed up its
goal attainments. Due to the similarities between
the search in hypothesis space and the QMS search
around an initial state (initial hypothesis), it is pos-
sible to understand QMS as a technique that im-
plements one of the most fundamental steps in any
ArtiÔ¨Åcial Intelligence agent.

As it was explained in section III, QMS requires
an input Ô¨Åle which is a set of tuples (state, cost). In
the N-Queen problem, the states are all the possible
board combinations that can appear for a given n.
To reduce the size of the state space, we represent
each state as a list with n positions (one per queen),
and each position represents the row of the queen.
Assuming that it is not possible to have two queens
in the same column, a movement is just a permuta-
tion in the position (row) of two queens. The scheme
shown in Fig. 4 will be represented with the list (0,
1, 2, 3) indicating that the Ô¨Årst queen is in row 0,
the second one in row 1, etc.

This representation is an eÔ¨Écient codiÔ¨Åcation of

the problem that reduces the number of states. The
possible movements are swaps between positions.
For example, in Fig. 4, the Ô¨Årst and second queens
are swapped between them. Once the board codiÔ¨Å-
cation has been explained, it is necessary to explain
the heuristic value associated with each board po-
sition. The heuristic that we designed counts the
number of attacking queens, and it penalizes them
with extra cost if one queen attacks more than one
other queen. The heuristic (H) is deÔ¨Åned by the
Eq. 11 and the objective is to minimize the heuristic
value. If the heuristic is 0, this board conÔ¨Åguration
is a solution.

H =

n
(cid:88)

n
(cid:88)

i=0

j=i+1

[Œ¥rowi,rowj + Œ¥diagi,diagj ] ‚àó Œ≥,

(11)

where the Ô¨Årst sum counts the heuristic value for all
queens, the second sum counts the heuristic value
for each queen. Œ¥rowi,rowj is 1 if queeni and queenj
are in the same row (or same for diagonal), else it is
0. Œ≥ is an accumulative value that counts the num-
It
ber of queens that queeni is already attacking.

Quantum M-HDeltas calculatorOutputprocessorClassical M-HPositions+EnergiesTTS valueand/orTTS plotand/orProbabilitiesINPUTOUTPUTDeltasŒîùëñùëóprocessorTTS calculatorPlotterQuantum Metropolis-Hastings circuit executionPositions+DeltasProbabilities distributionEnergies oraclePositions+DeltasProbabilities distributionClassical Metropolis-Hastings circuit execution10

FIG. 3. N-Queen problem explanation with 8 queens in a chessboard of 8√ó8. On the left, is a valid solution for the
problem because no queens are attacking one another as is explained for queen in D3. On the right, is an example of
the same chess board but with 4 queens attacking one another (A4, C5, E7, F6, and G4)

n Number solutions
4
5
6
7

2
10
4
40

TABLE II. Table of number of solutions for N-Queen
problem by n.
In general, the number of solutions is
increased with the number of n, except in n = 6 that it
is reduced which aÔ¨Äects the complexity

that n = 6 has signiÔ¨Åcant fewer solutions than the
previous case, n = 5, and the next case, n = 7. This
aÔ¨Äects the complexity of the problem. As it can
be seen in the simulations plot in Fig. 5, the TTS
obtained for n = 6 and n = 7 is similar, despite the
fact that for n = 7 the state space is much bigger
than for n = 6.

V.

SIMULATION RESULTS

To validate QMS tool with the N-Queen prob-
lem, we execute diÔ¨Äerent simulations with both clas-
sical and quantum Metropolis-Hastings algorithms.
These simulations were executed using the frame-
work Qiskit [50] and the included free noise simu-
lator QASM. The N-Queen problem requires many
qubits to represent its states, and the actual sim-
ulator has reduced capacities to represent large

FIG. 4. This state for n = 4 is represented, in the left
as (0,1,2,3) because the Ô¨Årst queen is in row O (A1),
second queen is in row 1 (B2), etc. In the right, a swap
between the Ô¨Årst and second queen was executed, and
the resulting state is (1, 0, 2, 3) because the Ô¨Årst queen
is in row 1 (A2), the second queen is in row 0 (B1), etc.

is a multiplicative factor, which is increased by 1
for each extra attacked queen. Œ≥ = 1 for the Ô¨Årst
attacked queen by queeni, Œ≥ = 2 for the second at-
tacked queen by queeni, etc. This heuristic returns
a high penalty for boards in which a queen is very
badly placed, such that, it is attacking multiple other
queens, which it is something it is necessary to avoid.
In the N-Queen problem, the number of solutions
for each size n determines its complexity. A higher
number of solutions implies more goal states and a
more guided search because the gradient between
states is bigger and the transition to the goal state
is faster. It results in a faster solution generation.
In table II extracted from [49], it is possible to see

11

N-Queen codiÔ¨Åcation

coordinates
move id
move value
coin
ancilla
Total

n ‚àó (cid:100)log2(n)(cid:101)
(cid:100)log2(n)(cid:101)
1
1
3
n ‚àó (cid:100)log2(n)(cid:101) + (cid:100)log2(n)(cid:101) + 5

QMS
N Qubits RAM Memory
4
5
6
7
8

0.1 GB
0.8 GB
1.5 GB
8 GB
65 GB

15
23
26
29
32

TABLE III. The number of qubits for each register in
the QMS software tool

TABLE IV. The number of qubits and memory RAM
consumption of QASM simulator for each size n instance
problem

amounts of data. For that reason, we calculated the
number of qubits that our codiÔ¨Åcation needs and
the memory consumption in the QASM simulator
for the number of qubits. In table III the number of
qubits necessary for each register in the QMS soft-
ware tool is detailed.

The list of necessary registers is as follows:

‚Ä¢ Coordinates register represents each state. It
is necessary to codify in binary each state, so
the number of qubits is the number of regis-
ters multiplied by the qubits necessary for the
binary representation.

‚Ä¢ Move id register represents the coordinate to
move, so it is an index that requires a binary
representation of the number of registers.

‚Ä¢ Move value register indicates whether the
movement is up or down (left or right in the
swap case). It only requires one qubit.

‚Ä¢ Coin register represents the binary decision of
acceptance or rejection of the proposed candi-
date.

‚Ä¢ Ancilla register is used to store the change
probability of the proposed candidate. It can
be represented with 3 or more qubits depend-
ing on the selected precision in the probability.

Table IV represents the number of qubits for each
size n in the N-Queen problem. This table is use-
ful to understand the complexity of the circuit and
the necessary resources to execute it in a classical
computer. In our case, we have available 128 GB of
RAM, but we only have results of n = 7 due to the
execution times that are around 2 weeks per instance
of the problem with n = 7.

We execute the simulations using the TTS met-
ric, explained in Eq. 6, as a Ô¨Ågure of merit. We test
QMS for n =4, 5, 6 and 7. The decision to stop
at n = 7 is directly connected with the time con-
sumption of each execution. To get more cases of
the problem with diÔ¨Äerent initial conÔ¨Ågurations, we
slightly modify the N-Queen rules. In each instance,
we Ô¨Åxed one queen in one position, considering that

this queen is stuck in the position for any reason. It
is common to have this kind of restriction in an ML
problem as, for example, a mandatory point to visit
in a route generated with Deep Learning. This new
rule gives us extra points to evaluate our problem.
Without this N-Queen modiÔ¨Åcation, we would only
have 4 diÔ¨Äerent samples of the problem, one per n
value.
In the simulation, we have 74 diÔ¨Äerent in-
stances of the N-Queen problem with 9 instances for
n = 4, 42 instances for n = 5, 21 instances for n = 6
and 2 instances for n = 7. The reduced number of
instances for n = 7 results in 4 weeks of execution.
The results are shown in Fig. 5. This plot
shows the relationship between classical and quan-
tum TTS. It is divided into two triangles by a gray
dashed line. The upper triangle shows the classical
advantage region where the majority of the n = 4
points are and the lower triangle with quantum ad-
vantage where all the points of n = 6 and n = 7 are.
The plot shows that the resulting points present a
tendency to move towards the region of quantum
advantage as the problem size n is growing, we can
conclude that there is a possible quantum advan-
tage of quantum M-H against classical M-H. We can
quantify it using a linear least-square Ô¨Åtting with an
exponent a (deÔ¨Åned in Eq. 8) of 0.939.

We also test the core of QMS, quantum walks,
to Ô¨Ånd the best performing algorithm. Due to the
in the
discretization carried out by Lemieux et al.
quantum M-H unitary operator ÀúU , we test whether
the sorting of the operators could aÔ¨Äect the results.
We also include two other alternative sorting options
in the comparative. We deÔ¨Åne Preparation with the
operators V B, Selection with the operator F , In-
verse Preparation with B‚Ä†V ‚Ä† and ReÔ¨Çection with
R.

‚Ä¢ Lemieux et al.:

Inverse Preparation-ReÔ¨Çection. Namely,
corresponds to the sorting:

Preparation-Selection-
it

ÀúU = RV ‚Ä†B‚Ä†F BV.

(12)

‚Ä¢ Qubitization: Explained in [41].

Inverse

12

FIG. 6. This Ô¨Ågure shows the three diÔ¨Äerent sorting
options that we test for the quantum walk operator W
in eqs. 12, 13 and 14. In blue, the sorting proposed by
Lemieux et al. 12 gets a minimum TTS lower than the
other two sorting options for n = 4, n = 5 and n = 6. It
is possible to observe a similarity between the evolution
of W in eq. 12 and the other two sorting options in 13
and 14

VI. CONCLUSIONS AND OUTLOOK

We have studied quantum optimization algo-
rithms to test how they could challenge existing clas-
sical algorithms for industrial problems.Classical op-
timization algorithms have been contributing to Ô¨Ånd
solutions that are otherwise impossible to generate
without computational force. However, classical op-
timization algorithms have limitations in scalability,
and they can be optimized with quantum comput-
ing. Specially, we have focused on the Metropolis-
Hastings algorithm that has many applications for
industrial problems and its quantum counterpart,
the quantum Metropolis-Hastings.

A quantum version of the M-H was proposed
by [23] and modiÔ¨Åed to get an implementable version
by [31]. We use both works to construct a software
tool that has the M-H algorithm at the core and
which can solve any optimization problem given in
simple format, as a list of state-cost tuples. This tool
is an easy-to-use Python module that receives a de-
scription of the problem and returns the minimum
cost position. Besides, QMS evaluates to return a
Time To Solution (TTS) value of the search to Ô¨Ånd
the solution. This TTS metric is a Ô¨Ågure of merit to
evaluate the performance of the tool and to compare
it against the classical algorithm.

As we have shown, it is possible to use QMS with
any combinatorial optimization problem which has
uncertainty in the output. Thus, the goal is to Ô¨Ånd
an unknown state with some properties. This re-

FIG. 5. Comparison between the classical and quantum
TTS for N-Queen problem with n=4, 5, 6, and 7 with
74 samples of the problem. The dashed gray line sepa-
rates the spaces of classical advantage (upper triangle)
and quantum advantage (lower triangle). The key as-
pect to notice in this Ô¨Ågure is that for n=4 most of the
points are in the classical advantage region but, when the
problem increases its diÔ¨Éculty, most of the points are in
the quantum advantage region. The scaling exponent is
0.939.

Preparation-ReÔ¨Çection-Preparation-Selection.
Namely, it corresponds to the sorting:

ÀúU = F V BRB‚Ä†V ‚Ä†.

(13)

‚Ä¢ Alternative: Selection-Inverse Preparation-
it corre-

Namely,

ReÔ¨Çection-Preparation.
sponds to the sorting:

ÀúU = V BRB‚Ä†V ‚Ä†F.

(14)

To compare these three diÔ¨Äerent sorting options,
we execute the N-Queen problem with n = 4, 5, and
6 including several instances of Ô¨Åxed queens. For
each sorting and each n, we get the mean and the
standard deviation. Using these parameters, it is
easy to compare whether the TTS have signiÔ¨Åcant
diÔ¨Äerences between them. We show the results in
Fig. 6. The operator sorting election selected by
Lemieux et al. achieves a lower TTS value for all
problem size tested. Besides, it is possible to ob-
serve a similar tendency between the diÔ¨Äerent sort-
ing options but separated by a gap. Thus, these
simulations show that the Lemieux et al.
sorting
works better.

quirement is met in most optimization problems at
the industrial level (knapsack problem, TSP, rout-
ing, etc.). The search process to Ô¨Ånd the state with
the minimum cost in these problems converts them
into an NP-complexity problem. It is in this fam-
ily of problems in which quantum computing poly-
nomial advantages can be the most useful, that is
the reason why we selected them. The works by
Szegedy and Lemieux et al. show a polynomial ad-
vantage using quantum walks, which we extrapolate
to a general-purpose tool for any optimization prob-
lem.

We validate our QMS quantum software tool in
the ArtiÔ¨Åcial Intelligence domain. It is well-known
that one of the bottlenecks in Machine Learning al-
gorithms is the search process that the algorithm
performs to Ô¨Ånd the explanation that best Ô¨Åts the
input data. This search is very similar to the pro-
cess done to solve an optimization problem, as it has
been explained in the literature [46, 48]. Therefore,
we consider that QMS could be applied to speed
up some processes of an ArtiÔ¨Åcial Intelligence algo-
rithm.

Since we want to show how QMS can help AI al-
gorithms using quantum search, we identify the N-
Queen problem as a good benchmark for this task.
The N-Queen problem is considered a benchmark
for AI [13, 46] and also can be solved by a quantum
algorithm, as we show in this work. In the simula-
tions, we observe that the quantum algorithm gets
better results than its classical counterparts. We
also execute an analysis of the scaling with a lin-
ear least-square Ô¨Åtting, getting an exponent of 0.939.
While the classical Metropolis-Hastings algorithm is

[1] K. M. Bretthauer and B. Shetty, ‚ÄúThe nonlinear
knapsack problem‚Äìalgorithms and applications,‚Äù
European Journal of Operational Research, vol. 138,
no. 3, pp. 459‚Äì472, 2002.

[2] K. L. HoÔ¨Äman, M. Padberg, G. Rinaldi, et al.,
‚ÄúTraveling salesman problem,‚Äù Encyclopedia of op-
erations research and management science, vol. 1,
pp. 1573‚Äì1578, 2013.

[3] D. E. Smith,

in over-
subscription planning.,‚Äù in ICAPS, vol. 4, p. 393,
2004.

‚ÄúChoosing objectives

[4] T. Bekta¬∏s and G. Laporte, ‚ÄúThe pollution-routing
problem,‚Äù Transportation Research Part B: Method-
ological, vol. 45, no. 8, pp. 1232‚Äì1250, 2011.

[5] S. N. Kumar and R. Panneerselvam, ‚ÄúA survey on
the vehicle routing problem and its variants,‚Äù Intel-
ligent Information Management, 2012.

[6] P. Toth and D. Vigo, Vehicle routing: problems,

methods, and applications. SIAM, 2014.

13

not the state-of-the-art procedure to solve the N-
Queen problem, we emphasize that our goal is to
show the quantum advantage that QMS can get in a
search problem. This N-Queen problem case study
for QMS is added to the case study we proposed in
a previous paper [40], also with quantum advantage.
Another study that we carried out was to under-
stand why the discrete operators were sorted in a
non-standard way [31], and we have also compared
the TTS results for diÔ¨Äerent sorting options. We
used again the N-Queen problem as a benchmark to
test the ÀúU operator deÔ¨Åned by Szegedy [23], Lemieux
et al. [31], and Low et al. [41].

Finally,

future work should include more case
studies to test QMS tools and possible applications.
It would be interesting to analyze in multiple do-
mains if the exponent is always above 0.5, which is
the value required for a quadratic advantage.

VII. ACKNOWLEDGEMENTS

R.C.

Project

and P.A.M.C contributed equally to
this work. We acknowledge support from the
CAM/FEDER
No.S2018/TCS-4342
(QUITEMAD-CM),
Spanish MINECO grants
MINECO/FEDER Projects, PGC2018-099169-B-
I00 FIS2018, MCIN with funding from European
Union NextGenerationEU (PRTR-C17.I1)
and
Ministry of Economic AÔ¨Äairs Quantum ENIA
project. M. A. M.-D. has been partially supported
by the U.S. Army Research OÔ¨Éce through Grant
No. W911NF-14-1-0103. P. A. M. C. thanks the
support of a MECD grant FPU17/03620, and R.C.
the support of a CAM grant IND2019/TIC17146.

[7] H. M. Markowitz, Portfolio selection. Yale univer-

sity press, 1968.

[8] M. Rubinstein, ‚ÄúMarkowitz‚Äôs‚Äù portfolio selection‚Äù:
A Ô¨Åfty-year retrospective,‚Äù The Journal of Ô¨Ånance,
vol. 57, no. 3, pp. 1041‚Äì1045, 2002.

[9] A. Kryshtafovych, T. Schwede, M. Topf, K. Fi-
delis, and J. Moult, ‚ÄúCritical assessment of methods
of protein structure prediction (casp)‚Äîround xiii,‚Äù
Proteins: Structure, Function, and Bioinformatics,
vol. 87, no. 12, pp. 1011‚Äì1020, 2019.

[10] R. Bellman, ‚ÄúDynamic programming and lagrange
multipliers,‚Äù Proceedings of the National Academy
of Sciences, vol. 42, no. 10, pp. 767‚Äì769, 1956.
[11] F. Y. Kuo and I. H. Sloan, ‚ÄúLifting the curse of
dimensionality,‚Äù Notices of the AMS, vol. 52, no. 11,
pp. 1320‚Äì1328, 2005.

[12] P. G. Kolaitis and M. N. Thakur, ‚ÄúLogical deÔ¨Ånabil-
ity of np optimization problems,‚Äù Information and
Computation, vol. 115, no. 2, pp. 321‚Äì353, 1994.

[13] I. P. Gent, C. JeÔ¨Äerson, and P. Nightingale, ‚ÄúCom-
plexity of n-queens completion,‚Äù Journal of Arti-
Ô¨Åcial Intelligence Research, vol. 59, pp. 815‚Äì848,
2017.

[14] A. Robert, P. K. Barkoutsos, S. Woerner, and
I. Tavernelli, ‚ÄúResource-eÔ¨Écient quantum algorithm
for protein folding,‚Äù npj Quantum Information,
vol. 7, no. 1, pp. 1‚Äì5, 2021.

[15] P. Crescenzi, V. Kann, and M. Halld¬¥orsson, ‚ÄúA com-
pendium of np optimization problems,‚Äù 1995.
[16] A. Montanaro, ‚ÄúQuantum speedup of monte carlo
methods,‚Äù Proceedings of
the Royal Society A:
Mathematical, Physical and Engineering Sciences,
vol. 471, no. 2181, p. 20150301, 2015.

[17] G. Daniell, A. J. Hey, and J. Mandula, ‚ÄúError analy-
sis for correlated monte carlo data,‚Äù Physical Review
D, vol. 30, no. 10, p. 2230, 1984.

[18] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi,
‚ÄúOptimization by simulated annealing,‚Äù Science,
vol. 220, no. 4598, pp. 671‚Äì680, 1983.

[19] N. Metropolis, A. W. Rosenbluth, M. N. Rosen-
bluth, A. H. Teller, and E. Teller, ‚ÄúEquation of state
calculations by fast computing machines,‚Äù The jour-
nal of chemical physics, vol. 21, no. 6, pp. 1087‚Äì
1092, 1953.

[20] W. K. Hastings, ‚ÄúMonte carlo sampling methods us-
ing markov chains and their applications,‚Äù Oxford
Journals, 1970.

[21] L. K. Grover, ‚ÄúQuantum mechanics helps in search-
ing for a needle in a haystack,‚Äù Physical Review Let-
ters, vol. 79, no. 2, p. 325, 1997.

[22] A. Ambainis, ‚ÄúQuantum walk algorithm for ele-
ment distinctness,‚Äù in Proceedings of the 45th An-
nual IEEE Symposium on Foundations of Computer
Science, FOCS ‚Äô04, (USA), p. 22‚Äì31, IEEE Com-
puter Society, 2004.

[23] M. Szegedy, ‚ÄúQuantum speed-up of markov chain
based algorithms,‚Äù in 45th Annual IEEE sympo-
sium on foundations of computer science, pp. 32‚Äì41,
IEEE, 2004.

[24] K. Temme, T. J. Osborne, K. G. Vollbrecht,
D. Poulin, and F. Verstraete, ‚ÄúQuantum metropo-
lis sampling,‚Äù Nature, vol. 471, no. 7336, pp. 87‚Äì90,
2011.

[25] G. D. Paparo and M. Martin-Delgado, ‚ÄúGoogle in a
quantum network,‚Äù ScientiÔ¨Åc reports, vol. 2, no. 1,
pp. 1‚Äì12, 2012.

[26] G. D. Paparo, M. M¬®uller, F. Comellas, and M. A.
Martin-Delgado, ‚ÄúQuantum google in a complex
network,‚Äù ScientiÔ¨Åc reports, vol. 3, no. 1, pp. 1‚Äì16,
2013.

[27] G. D. Paparo, V. Dunjko, A. Makmal, M. A.
Martin-Delgado, and H. J. Briegel, ‚ÄúQuantum
speedup for active learning agents,‚Äù Physical Review
X, vol. 4, no. 3, p. 031002, 2014.

[28] K. Kadian, S. Garhwal, and A. Kumar, ‚ÄúQuantum
walk and its application domains: A systematic re-
view,‚Äù Computer Science Review, vol. 41, p. 100419,
2021.

[29] R. D. Somma, S. Boixo, H. Barnum, and E. Knill,
‚ÄúQuantum simulations of classical annealing pro-

14

cesses,‚Äù Physical Review Letters, vol. 101, no. 13,
p. 130504, 2008.

[30] M.-H. Yung and A. Aspuru-Guzik, ‚ÄúA quantum‚Äì
quantum metropolis algorithm,‚Äù Proceedings of the
National Academy of Sciences, vol. 109, no. 3,
pp. 754‚Äì759, 2012.

[31] J. Lemieux, B. Heim, D. Poulin, K. Svore, and
M. Troyer, ‚ÄúEÔ¨Écient quantum walk circuits for
metropolis-hastings algorithm,‚Äù Quantum, vol. 4,
p. 287, 2020.

[32] Z. B. Zabinsky et al., ‚ÄúRandom search algorithms,‚Äù
Department of Industrial and Systems Engineering,
University of Washington, USA, 2009.

[33] S. Boyd, P. Diaconis, and L. Xiao, ‚ÄúFastest mixing
markov chain on a graph,‚Äù SIAM review, vol. 46,
no. 4, pp. 667‚Äì689, 2004.

[34] U. WolÔ¨Ä, A. Collaboration, et al., ‚ÄúMonte carlo er-
rors with less errors,‚Äù Computer Physics Communi-
cations, vol. 156, no. 2, pp. 143‚Äì153, 2004.

[35] I. Yildirim,

‚ÄúBayesian inference: Metropolis-
hastings sampling,‚Äù Dept. of Brain and Cognitive
Sciences, Univ. of Rochester, Rochester, NY, 2012.
[36] G. Fl¬®otter¬®od and M. Bierlaire, ‚ÄúMetropolis‚Äìhastings
sampling of paths,‚Äù Transportation Research Part
B: Methodological, vol. 48, pp. 53‚Äì66, 2013.

[37] F. Magniez, A. Nayak, J. Roland, and M. Santha,
‚ÄúSearch via quantum walk,‚Äù SIAM journal on com-
puting, vol. 40, no. 1, pp. 142‚Äì164, 2011.

[38] B. Calderhead, ‚ÄúA general construction for paral-
lelizing metropolis- hastings algorithms,‚Äù Proceed-
ings of the National Academy of Sciences, vol. 111,
no. 49, pp. 17408‚Äì17413, 2014.

[39] A. Galindo and M. A. Martin-Delgado, ‚ÄúFamily
of grover‚Äôs quantum-searching algorithms,‚Äù Physi-
cal Review A, vol. 62, no. 6, p. 062303, 2000.
[40] P. A. M. Casares, R. Campos, and M. A. Martin-
Delgado, ‚ÄúQfold: quantum walks and deep learn-
ing to solve protein folding,‚Äù Quantum Science and
Technology, vol. 7, no. 2, p. 025013, 2022.

[41] G. H. Low and I. L. Chuang, ‚ÄúHamiltonian simula-
tion by qubitization,‚Äù Quantum, vol. 3, p. 163, 2019.
[42] Y. Suzuki, Y. Kawase, Y. Masumura, Y. Hiraga,
M. Nakadai, J. Chen, K. M. Nakanishi, K. Mitarai,
R. Imai, S. Tamiya, et al., ‚ÄúQulacs: a fast and versa-
tile quantum circuit simulator for research purpose,‚Äù
Quantum, vol. 5, p. 559, 2021.

[43] C. Bowtell and P. Keevash, ‚ÄúThe n-queens prob-
lem,‚Äù arXiv preprint arXiv:2109.08083, 2021.
[44] Z. Luria and M. Simkin, ‚ÄúA lower bound for the n-
queens problem,‚Äù arXiv preprint arXiv:2105.11431,
2021.

[45] M. Simkin, ‚ÄúThe number of n-queens conÔ¨Ågura-
tions,‚Äù arXiv preprint arXiv:2107.13460, 2021.
[46] S. Russell and P. Norvig, ArtiÔ¨Åcial Intelligence: A
Modern Approach. Prentice Hall, 3 ed., 2010.
[47] R. Walker, ‚ÄúAn enumerative technique for a class
of combinatorial problems,‚Äù in Proceedings of Sym-
posia in Applied Mathematics, 1960.

[48] T. M. Mitchell, Machine Learning. New York:

McGraw-Hill, 1997.

[49] ‚ÄúNumber

solutions of

the n-queen problem.‚Äù

https://www.durangobill.com/N_Queens.html.
Accessed: 2022-07-08.

[50] G. Aleksandrowicz, T. Alexander, P. Barkoutsos,
L. Bello, Y. Ben-Haim, D. Bucher, F. J. Cabrera-
Hern¬¥andez, J. Carballo-Franquis, A. Chen, C.-
F. Chen, J. M. Chow, A. D. C¬¥orcoles-Gonzales,
A. J. Cross, A. Cross, J. Cruz-Benito, C. Culver,
S. D. L. P. Gonz¬¥alez, E. D. L. Torre, D. Ding,
E. Dumitrescu, I. Duran, P. Eendebak, M. Everitt,
I. F. Sertage, A. Frisch, A. Fuhrer, J. Gambetta,
B. G. Gago, J. Gomez-Mosquera, D. Greenberg,
I. Hamamura, V. Havlicek, J. Hellmers,
(cid:32)Lukasz
Herok, H. Horii, S. Hu, T. Imamichi, T. Itoko,
A. Javadi-Abhari, N. Kanazawa, A. Karazeev,

15

K. Krsulich, P. Liu, Y. Luh, Y. Maeng, M. Mar-
ques, F. J. Mart¬¥ƒ±n-Fern¬¥andez, D. T. McClure,
D. McKay, S. Meesala, A. Mezzacapo, N. Moll,
D. M. Rodr¬¥ƒ±guez, G. Nannicini, P. Nation, P. Olli-
trault, L. J. O‚ÄôRiordan, H. Paik, J. P¬¥erez, A. Phan,
M. Pistoia, V. Prutyanov, M. Reuter, J. Rice,
A. R. Davila, R. H. P. Rudy, M. Ryu, N. Sathaye,
C. Schnabel, E. Schoute, K. Setia, Y. Shi, A. Silva,
Y. Siraichi, S. Sivarajah, J. A. Smolin, M. Soeken,
H. Takahashi, I. Tavernelli, C. Taylor, P. Taylour,
K. Trabing, M. Treinish, W. Turner, D. Vogt-Lee,
C. Vuillot, J. A. Wildstrom, J. Wilson, E. Winston,
C. Wood, S. Wood, S. W¬®orner, I. Y. Akhalwaya, and
C. Zoufal, ‚ÄúQiskit: An Open-source Framework for
Quantum Computing,‚Äù Jan. 2019.

