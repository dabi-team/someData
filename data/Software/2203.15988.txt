2
2
0
2

r
p
A
1

]
E
S
.
s
c
[

2
v
8
8
9
5
1
.
3
0
2
2
:
v
i
X
r
a

Does Configuration Encoding Matter in Learning Software
Performance? An Empirical Study on Encoding Schemes

Jingzhi Gong
Loughborough University
Loughborough, UK
j.gong@lboro.ac.uk

Tao Chenâˆ—
Loughborough University
Loughborough, UK
t.t.chen@lboro.ac.uk

Abstract

Learning and predicting the performance of a configurable software
system helps to provide better quality assurance. One important
engineering decision therein is how to encode the configuration
into the model built. Despite the presence of different encoding
schemes, there is still little understanding of which is better and
under what circumstances, as the community often relies on some
general beliefs that inform the decision in an ad-hoc manner. To
bridge this gap, in this paper, we empirically compared the widely
used encoding schemes for software performance learning, namely
label, scaled label, and one-hot encoding. The study covers five
systems, seven models, and three encoding schemes, leading to 105
cases of investigation.

Our key findings reveal that: (1) conducting trial-and-error to
find the best encoding scheme in a case by case manner can be rather
expensive, requiring up to 400+ hours on some models and systems;
(2) the one-hot encoding often leads to the most accurate results
while the scaled label encoding is generally weak on accuracy over
different models; (3) conversely, the scaled label encoding tends
to result in the fastest training time across the models/systems
while the one-hot encoding is the slowest; (4) for all models studied,
label and scaled label encoding often lead to relatively less biased
outcomes between accuracy and training time, but the paired model
varies according to the system.

We discuss the actionable suggestions derived from our findings,
hoping to provide a better understanding of this topic for the com-
munity. To promote open science, the data and code of this work
can be publicly accessed at https://github.com/ideas-labo/MSR2022-
encoding-study.

CCS Concepts

â€¢ Software and its engineering â†’ Software performance.
Keywords

Encoding Scheme, Machine Learning, Software Engineering, Per-
formance Prediction, Performance Learning, Configurable Software

âˆ—Corresponding Author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9303-4/22/05. . . $15.00
https://doi.org/10.1145/3524842.3528431

ACM Reference Format:
Jingzhi Gong and Tao Chen. 2022. Does Configuration Encoding Matter in
Learning Software Performance? An Empirical Study on Encoding Schemes.
In 19th International Conference on Mining Software Repositories (MSR â€™22),
May 23â€“24, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages.
https://doi.org/10.1145/3524842.3528431

1 Introduction

Configurable software systems allow software engineers to tune a
set of configurations options (e.g., the cache_size in MongoDB),
which can considerably influence their performance, such as la-
tency, runtime and energy consumption, etc. [14, 41]. This is, in fact,
a two-edged sword: on one hand, these configuration options offer
the flexibility for software to deal with different needs, and even
create the foundation to achieve runtime self-adaptation; on the
other hand, their combinatorial implications to the performance
are often unclear, which may result in severe complication and
consequences for software maintenance. For example, Xu et al. [55]
have discovered that software engineers find it generally difficult
to adjust the configurations options in order to adapt the perfor-
mance. Han and Yu [27] have further shown that over 59% of the
performance bugs nowadays are due to inappropriate configura-
tions. Therefore, to take full advantage of the configurability and
adaptability of the software, a performance model, which takes a
possible configuration as inputs to predict the likely performance,
is of high importance.

Classic performance model has been relying on analytical meth-
ods, but soon they become ineffective due primarily to the soaring
complexity of modern software systems. In particular, there are
two key reasons which prevent the success of analytical methods:
(1) analytical models often work on a limited type of configuration
options, such as CPU and memory settings [8, 18], which cannot
cope with the increasing complexity of modern systems. For exam-
ple, it has been reported that configurable software systems often
contain more complex and diverse types of configuration options
that span across different modules, including cache, threading, and
parallelism, etc [55]. (2) Their effectiveness is highly dependent
on assumptions about the internal structure and the environment
of the software being modeled. However, many modern scenarios,
such as cloud-based systems, virtualized and multi-tenant software,
intentionally hide such information to promote ease of use, which
further reduces the reliability of the analytical methods [4]. To over-
come the above, machine learning based performance modelings
have been gaining momentum in recent years [35], as they require
limited assumption, work on arbitrary types of configurations op-
tions, and do not rely on heavy human intervention.

 
 
 
 
 
 
MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

Jingzhi Gong and Tao Chen

A critical engineering decision to make in learning performance
for configurable software is how to encode the configurations. In the
literature, three encoding schemes are prevalent: (1) embedding the
configuration options without scaling (label encoding) [40, 41, 48];
(2) doing so with normalization (scaled label encoding) [4, 8, 26] or
(3) converting them into binary ones that focus on the configuration
values of those options, each of which serves as a dimension (one-
hot encoding) [2, 25, 49].

Existing work takes one of these three encoding schemes without
systematic justification or even discussions, leaving us with little
understanding in this regard. This is of concern, as in other domains,
such as system security analysis [32] and medical science [28], it has
been shown that the encoding scheme chosen can pose significant
implications to the success of a machine learning model. Further,
choosing one in a trial-and-error manner for each case can be
impractical and time-consuming, as we will show in Section 4. It
is, therefore, crucial to understand how the encoding performs
differently for learning performance of configurable software.

To provide a better understanding of this topic, in this paper,
we conduct an empirical study that systematically compares the
three encoding schemes for learning software performance and
discuss the insights learned. Our hope is to provide more justified
understandings towards such an engineering decision in learning
software performance under different circumstances.

1.1 Research Questions

Our study covers seven widely used machine learning models for
learning software performance, i.e., Decision Tree (DT) [46] (used
by [4, 8, 25, 41]), ğ‘˜-Nearest Neighbours (ğ‘˜NN) [21] (used by [35]),
Kernel Ridge Regression (KRR) [52] (used by [35]), Linear Regres-
sion (LR) [23] (used by [4, 8, 49]), Neural Network (NN) [53] (used
by [20, 26]), Random Forest (RF) [30] (used by [45, 50]), and Support
Vector Regression (SVR) [17] (used by [4, 50]), together with five
popular real-world software systems from prior work [15, 16, 41, 44],
covering a wide spectrum of characteristics and domains. Naturally,
the first research question (RQ) we ask is:

RQ1: Is it practical to examine all encoding methods for
finding the best one under every system?

RQ1 seeks to confirm the significance of our study: if it takes an
unreasonably long time to conduct trial-and-error in a case-by-case
manner, then guidelines on choosing the best encoding scheme
under different circumstances become rather important.

What we seek to understand next is:

RQ2: Which encoding scheme (paired with the model) helps
to build a more accurate performance model?

We use Root Mean Squared Error (RMSE), which is commonly
used for software performance modeling [24, 31], as the metric
for accuracy. In particular, to make a comparison under the best
possible situation, we follow the standard pipeline in software per-
formance learning [4, 8, 40, 41, 48] that tunes the hyperparameters
of each model-encoding pair using grid search and cross-validation,
which is a common way for parameter tuning [29].

While prediction accuracy is important, the time taken for train-
ing can also become an integral factor in software performance
learning. Our next RQ is, therefore:

RQ3: Which encoding scheme (paired with the model) helps
train a performance model faster?

We examine the training time of each model-encoding pair, in-
cluding all processes in the learning pipeline such as hyperparame-
ter tuning and validation.

Since it is important to understand the relationship between

accuracy and training time, in the final RQ, we ask:

RQ4: What are the trade-offs between accuracy and training
time when choosing the encoding and models?

With this, we seek to understand the Pareto-optimal choices that
are neither the highest on accuracy nor has the fastest training
time (the non-extreme points), especially those that achieve a well-
balanced between accuracy and training time, i.e., the knee points.

1.2 Contributions

In a nutshell, we show that choosing the encoding scheme is non-
trivial for learning software performance and our key findings are:

â€¢ To RQ1: Performing trial-and-error in a case by case man-
ner for finding the best encoding schemes can be rather
expensive under some cases, e.g., up to 400+ hours.

â€¢ To RQ2: The one-hot and label encoding tends to be the best
choice while the scaled label encoding performs generally
the worst.

â€¢ To RQ3: Opposed to RQ2, the scaled label encoding is gener-
ally the best choice while the one-hot encoding often exhibit
the slowest training.

â€¢ To RQ4: Over the models studied, the label and scaled label
encoding often lead to less biased results, particularly the
latter, but the paired model varies depending on the system.

Deriving from the above, we provide actionable suggestions for
learning software performance under a variety of circumstances:

(1) When the model to be used involves RF, SVR, KRR, or NN, it
is recommended to avoid trial-and-error for finding the best
encoding schemes. However, this may be practical for ğ‘˜NN,
DT, and LR.

(2) When the accuracy is of primary concern,

â€¢ using neural network paired with one-hot encoding if all

models studied are available to choose.

â€¢ using one-hot encoding for deep learning (NN), lazy mod-

els (ğ‘˜NN), and kernel models (KRR and SVM).

â€¢ using label encoding for linear (LR) and tree models (DT

and RF).

(3) When the training time is more important,

â€¢ using linear regression paired with scaled label encoding

if all models studied are available to choose.

â€¢ using scaled label encoding for deep learning (NN), linear

(LR), and kernel models (KRR and SVR).

â€¢ using label encoding for lazy (ğ‘˜NN) and tree models (DT

and RF).

Does Configuration Encoding Matter in Learning Software Performance?

MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

Table 1: An example of configurations and performance for
MongoDB. ğ‘¥ğ‘– is the ğ‘–th configuration option and â„™ is the per-
formance value (runtime).

ğ‘¥1
0
0
Â· Â· Â·
0
0

ğ‘¥2
0
1
Â· Â· Â·
0
0

ğ‘¥3
0
0
Â· Â· Â·
1
1

Â· Â· Â·

Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·

ğ‘¥ğ‘›âˆ’2
0
0
Â· Â· Â·
0
1

ğ‘¥ğ‘›âˆ’1
3
2
Â· Â· Â·
9
8

ğ‘¥ğ‘›

10
11
Â· Â· Â·
23
65

â„™

1200 seconds
2100 seconds
Â· Â· Â·
1260 seconds
1140 seconds

(4) When a trade-off between accuracy and training time is

unclear while an unbiased outcome is preferred,
â€¢ using scaled label encoding for achieving a relatively well-
balanced result if considering all models studied, but the
paired model requires some efforts to determine.

â€¢ if the model is fixed, only the kernel models (KRR and
SVR) and lazy model (ğ‘˜NN) have a more balanced outcome
achieved by label encoding and scaled label encoding, re-
spectively.

The remaining of this paper is organized as follows: Section 2
introduces the background information. Section 3 elaborates the
details of our empirical strategy. Section 4 discusses the results
and answers the aforementioned research questions. The insights
learned and actionable suggestions are specified in Section 5. Sec-
tion 6 discusses the implications of our study. Section 7, 8, and 9
present the threats to validity, related work, and conclusion, respec-
tively.

2 Preliminaries

In this section, we elaborate on the necessary background informa-
tion and the motivation of our study.

2.1 Learning Software Performance

A configurable software system comes with several configuration
options, such as the interval for MongoDB. Each of these options
can be configured using a set of predefined values, and therefore
they are often treated as discrete values, including binary, categori-
cal or numeric options, e.g., we may set (1, 2, 3, 4) on the interval.
Without loss of generality, as shown in Table 1, learning perfor-
mance for a configurable software often aims to build a regression
model that predicts a performance attribute â„™ [6, 8, 11], e.g., run-
time, written as:

â„™ = ğ‘“ (x), ğ‘“ : x â†’ R

(1)

whereby ğ‘“ is the actual function learned by a machine learning
model; x is the vector that represents a configuration. Given that
configurable software runs under an environment, the aim is to
train a model that minimizes the generalization error on new con-
figurations which have not been seen in training.

2.2 Encoding Schemes

In machine learning, the steps involved in the automated model
building forms a learning pipeline [39]. For learning software
performance, the standard learning pipeline setting consists of
preprocessing, hyperparameter tuning, model training (using all
configuration options), and model evaluation [4, 8, 40, 41, 48] (see
Section 3 for details).

In all learning pipeline phases, one critical engineering decision,
which this paper focuses on, is how the x can be encoded. In general,
existing work takes one of the following three encoding schemes:
Label Encoding: This is a widely used scheme [40, 41, 48],
where each of the configuration options occupies one dimension
in x. Taking MongoDB as an example, its configuration can be
represented as x = (cache_size, interval, ssl, data_strategy)
where cache_size= (1, 10, 10000), interval= (1, 2, 3, 4), ssl=
(0, 1) and data_strategy= (ğ‘ ğ‘¡ğ‘Ÿ _ğ‘™1, ğ‘ ğ‘¡ğ‘Ÿ _ğ‘™2, ğ‘ ğ‘¡ğ‘Ÿ _ğ‘™3). A configura-
tion that is used as a training sample could be (10000, 2, 1, 1),
where the data_strategy can be converted into numeric values
of (0, 1, 2).

, 1, 0.5).

Scaled Label Encoding: This is a variant of the label encoding
used by a state-of-the-art approach [4, 8, 26], where each configu-
ration also takes one dimension in x. The only difference is that all
configurations are normalized to the range between 0 and 1. The
same example configuration above for label encoding would be
scaled to (1, 1
3

One-hot Encoding: Another commonly followed scheme [2, 25,
49] such that each dimension in x refers to the binary form of a par-
ticular value for a configuration option. Using the above example
of MongoDB, the representation becomes x = (cache_size_v1,
cache_size_v2, ...). Each dimension, e.g., cache_size_v1, would
have a value of 1 if it is the one that the corresponding config-
uration chooses, otherwise it is 0. As such, the same configura-
tion (10000, 2, 1, 1) in the label encoding would be represented as
(0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0) in the one-hot encoding.

Clearly, for binary options, the three encoding methods would
be identical, hence in this work, we focus on the systems that also
come with complex numeric and categorical configuration options.

2.3 Why Study Them?

Despite the prevalence of the three encoding schemes, existing
work often use one of them without justifying their choice for
learning software performance [2, 4, 8, 25, 40, 41, 48, 49], partic-
ularly relating to the accuracy and training time required for the
model. Some studies have mentioned the rationals, but a common
agreement on which one to use has not yet been drawn. For exam-
ple, Bao et al. [2] state that for categorical configuration options,
e.g., cache_mode with three values (memory, disk, mixed), the label
encoding unnecessarily assume a natural ordering between the val-
ues, as they are represented as 1, 2, and 3. Here, one-hot encoding
should be chosen. However, Alaya et al. [1] argue that the one-hot
encoding can easily suffer from the multicollinearity issue on cate-
gorical configuration options, i.e., it is difficult to handle options
interaction. For numeric configuration options, label encoding may
fit well, as it naturally comes with order, e.g., the cache_size in
MongoDB, which has a set of values (1, 10, 10000). However, the
values, such as the above, can be of largely different scales and thus
degrade numeric stability. Indeed, using one-hot encoding could be
robust to this issue, but it loses the ordinal property of the numeric
configuration option [48]. Similarly, scaled label encoding could
reduce the instability and improve the prediction performance [42],
but it also weakens the interactions between the scaled options and
the binary options (as they stay the same). Therefore, there is still
no common agreement (or insights) on which encoding scheme to
use under what circumstances for learning performance models.

MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

Jingzhi Gong and Tao Chen

Unlike other domains, software configuration is often highly
sparse, leading to unusual data distributions. Specifically, a few
configuration options could have large influence on the software
performance, while the others are trivial, which makes the deci-
sion of encoding scheme difficult. Moreover, it is often the case
that we may not fully understand the nature of every configura-
tion option, as the software may be off-the-shelf or close-sourced;
hence, one may not be able to choose the right encoding based on
purely theoretical understandings. As such, a high-level guideline
on choosing the encoding scheme for performance modeling, which
gives overall suggestions for the practitioners, is in high demand.
The above thus motivates this empirical study, aiming to analyz-
ing the effectiveness of encoding schemes across various subject
systems and machine learning models, summarizing the common
behaviors of the encoding methods, and providing actionable advice
based on the learning models applied as well as the requirements,
e.g., accuracy and training time.

3 Methodology

In this section, we will discuss the methodology and experimental
setup of the empirical strategy for our study.

3.1 System and Data Selection

We set the following criteria to select sampled data of configurable
systems and their environments when comparing the three encod-
ing schemes:

(1) To promote the reproducibility, the systems should be open-
sourced and the data should be hosted in public repositories,
e.g., GitHub.

(2) The system and its environment should have been widely

used and tested in existing work.

(3) To ensure a case where the encoding schemes can create
sufficiently different representations, the system should have
at least 10% configuration options that are not categorical/bi-
nary.

(4) To promote the robustness of our experiments, the subject
systems should have different proportions of configuration
options that are numerical.

(5) To guarantee the scale of the study, we consider systems

with more than 5,000 configuration samples.

We shortlisted systems and their data from recent studies on
software configuration tuning and modeling [41, 44], from which
we identified five systems and their environment according to the
above criteria, as shown in Table 2. The five systems contain differ-
ent percentages of categorical/binary and numeric configuration
options while covering five distinct domains.

Note that since the measurement and sampling process for config-
urable software is usually rather expensive, e.g., Zuluaga et al. [56]
report that the synthesis of only one software configuration design
can take hours or even days, in practice it is not necessarily al-
ways possible to gather an extremely large number of data samples.
Further, using the full samples for some systems with a large config-
uration space can easily lead to unrealistic training time for certain
models, e.g., with Neural Network, it took several days to complete
only one run under our learning pipeline on the full datasets of
Trimesh. Therefore in this work, for each system, we randomly

Table 2: Datasets of configurable software systems used. |ğ’ª|
(C/N) denotes the number of categorical (including binary) /
numerical options.

Dataset

|ğ’ª | (C/N) Performance

Description

Used by

MongoDB
Lrzip
Trimesh
ExaStencils
x264

14/2
9/3
9/4
4/6
4/13

runtime (ms)
runtime (ms)
runtime (ms)
latency (ms)
energy (mW)

NoSQL database
compression tool
triangle meshes library
stencil code generator
a video encoder

[44]
[44]
[16, 41]
[44]
[16, 41]

sample 5,000 configurations from the dataset as our experiment
data, which tends to be reasonable and is also a commonly used
setting in previous work [19, 22, 34, 47].

3.2 Machine Learning Models

In this work, we choose the most common models that are of dif-
ferent types as used in prior studies:

â€¢ Linear Model: This type of model build the correlation be-
tween configurations options and performance under certain
linear assumptions.
â€“ Linear Regression (LR): A multi-variable linear regres-
sion model that linearly correlate the configurations and
their options to make prediction. It has been used by [4, 8,
49]. There are three hyperparameters to tune, e.g., n_jobs.
â€¢ Deep Learning Model: A model that is based on multiple
layers of perceptrons to learn and predict the concepts.
â€“ Neural Network (NN): A network structure with lay-
ers of neurons and connections representing the flow of
data. The weights incorporate the influences of each input
unit and interactions between them. The NN models have
been shown to be successful for modeling software per-
formance, e.g., [20, 26]. In this work, we utilize the same
network setting and hyperparameter tuning method from
the work by Ha and Zhang [26].

â€¢ Tree Model: This model constructs a tree-like structure

with a clear decision boundary on the branches.
â€“ Decision Tree (DT): A regression tree model that recur-
sively partitions the configurations space to predict the
output, which is used by [4, 8, 25, 41]. We tune a DT using
three hyperparameters, e.g., min_samples_split.

â€“ Random Forest (RF): An ensemble of decision trees,
each of which learns a subset of samples or configurations
space. It is a widely used model for performance learn-
ing [45, 50]. There are three hyperparameters to tune, e.g.,
n_estimators

â€¢ Lazy Model: This model delays the learning until the point

of prediction.
â€“ ğ‘˜-Nearest Neighbours (ğ‘˜NN): A model that considers
only already measured neighboring configurations to pro-
duce a prediction, which has been commonly used [35]. It
has four hyperparameters to be tuned, such as n_neighbors.
â€¢ Kernel Model: This model performs learning and predic-

tion by means of a kernel function.
â€“ Kernel Ridge Regression (KRR): A model of kernel
transformation that is combined with ridge regression,
which is the ğ¿2-norm regularization. It has been used
by [35]. There are three hyperparameters to tune, such as
alpha.

Does Configuration Encoding Matter in Learning Software Performance?

MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

41, 48]. In this study, we use the GridSearchCV function from
Sklearn, which is an exhaustive grid search that evaluates
the model quality via 10-fold cross-validation on the training
dataset. The one that leads to the best result is used. Note that
the default values are always used as a starting point.

(3) Bootstrapping: To achieve a reliable conclusion, we conducted
out-of-sample bootstrap (without replacement). In particular,
we randomly sampled 90% of the data as the training dataset,
those samples that were not included in the training were used
as the testing samples. The process was repeated 50 times, i.e.,
there are 50 runs of RMSE (on the testing dataset) and training
time to be reported. For each run, all encoding schemes are
examined, thereby we ensure that they are evaluated under the
same randomly sampled training and testing dataset.

(4) Statistical Analysis: To ensure statistical significance in mul-
tiple comparisons, we apply Scott-Knott test [38] on all compar-
isons of over 50 runs and produce a score. In a nutshell, Scott-
Knott sorts the list of treatments (the learning model-encoding
pairs) by their median RMSE/training time. Next, it splits the list
into two sub-lists with the largest expected difference [54]. Sup-
pose that we compare NN_onehot, RF_onehot, and NN_label, a
possible split could be: {NN_onehot, RF_onehot}, {NN_label},
with the score of 2 and 1, respectively. This means that, in
the statistical sense, NN_onehot and RF_onehot perform simi-
larly, but they are significantly better than NN_label. Formally,
Scott-Knott test aims to find the best split by maximizing the
difference Î” in the expected mean before and after each split:

Î” =

|ğ‘™1|
|ğ‘™ |

(ğ‘™1 âˆ’ ğ‘™)2 +

|ğ‘™2|
|ğ‘™ |

(ğ‘™2 âˆ’ ğ‘™)2

(3)

whereby |ğ‘™1| and |ğ‘™2| are the sizes of two sub-lists (ğ‘™1 and ğ‘™2)
from list ğ‘™ with a size |ğ‘™ |. ğ‘™1, ğ‘™2, and ğ‘™ denote their mean RMSE/-
training time values.
During the splitting, we apply a statistical hypothesis test ğ»
to check if ğ‘™1 and ğ‘™2 are significantly different. This is done by
using bootstrapping and ^ğ´12 [51]. If that is the case, Scott-Knott
recurses on the splits. In other words, we divide the treatments
into different sub-lists if both bootstrap sampling and effect
size test suggest that a split is statistically significant (with a
confidence level of 99%) and not a small effect ( ^ğ´12 â‰¥ 0.6). The
sub-lists are then scored based on their mean RMSE/training
time. The higher the score, the better the treatment.

Since there are five systems and environments, together with
seven models and three encoding schemes, our empirical study
consists of 105 cases of investigation. All the experiments were
performed on a Windows 10 server with an Intel Core i5-9400 CPU
2.90GHz and 8GB RAM.

4 Analysis and Results

In this section, we discuss the results of the empirical study with
respect to the RQs. All data and code can be accessed at the github
repository: https://github.com/ideas-labo/MSR2022-encoding-study.

4.1 RQ1: Cost of Trial-and-Error

4.1.1 Method To answer RQ1, for each encoding scheme, we
record the time taken to complete all 50 runs under a model and

Figure 1: The learning pipeline in this study.

â€“ Support Vector Regression (SVR): A model that trans-
forms the configurations space into a higher- dimensional
space via the kernel function, as used by [4, 50]. It contains
hyperparameters, e.g., kernel_func.

The reasons for the choices are two-fold: (1) they are prominently
used in previous work; and (2) they exist standard implementa-
tion under the same and widely-used machine learning library,
i.e., Sklearn [43] and Tensorflow, which reduces the possibility
of bias. Note that we did not aim to be exhaustive, but focusing
on those that are the most prevalent ones such that the potential
impact of this study can be maximized.

3.3 Metrics

Different metrics exist for measuring the accuracy of a prediction
model. In this study, we use RMSE because of two reasons: (1) it
is a widely used metric for performance modeling of configurable
systems in prior work [24, 31]; and (2) it has been reported that
RMSE can reveal the performance difference better, compared with
its popular counterparts such as Mean Relative Error [3]. RMSE is
calculated as:

(cid:118)(cid:117)(cid:116)

ğ‘…ğ‘€ğ‘†ğ¸ =

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘–=1

(ğ‘¥ğ‘– âˆ’ ^ğ‘¥ğ‘– )2

(2)

whereby ğ‘¥ğ‘– and ^ğ‘¥ğ‘– are the actual and predicted performance value,
respectively; ğ‘ denotes that total number of testing data samples.
As for the training time, we report the time taken for completing
the training process, including hyperparameter tuning and prepos-
sessing as necessary.

3.4 Learning Pipeline Setting

As shown in Figure 1, the standard learning pipeline setting in our
empirical study has several key steps as specified below:
(1) Preprocessing: For label and one-hot encoding, we utilize the
standard encoding functions from the Sklearn library. For the
scaled label encoding, we normalize the configurations using
the max-min scaling, such that an option value ğ‘£ is standard-
ized as ğ‘£ =
, where ğ‘£ğ‘šğ‘ğ‘¥ and ğ‘£ğ‘šğ‘–ğ‘› denote the maxi-
mum and minimum bound, respectively. In this way, the values
of each configuration option can be normalized within the
range between 0 and 1. We follow the state-of-the-art learning
pipeline such that all configuration options and their values are
considered in the model [2, 8, 40, 41, 48].

ğ‘£âˆ’ğ‘£ğ‘šğ‘–ğ‘›
ğ‘£ğ‘šğ‘ğ‘¥ âˆ’ğ‘£ğ‘šğ‘–ğ‘›

(2) Hyperparameter Tuning: It is not uncommon that a model
comes with at least one hyperparameter [36]. Therefore, the
common practice of the pipeline for learning software perfor-
mance is to tune them under all encoding schemes [2, 8, 40,

EncodingSchemesModelsMSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

Jingzhi Gong and Tao Chen

Table 3: Scott-Knott test, Med (median), and Interquartile Range (IQR) on the RMSE of all models and systems. â€œonehotâ€,
â€œlabelâ€ and â€œscaledâ€ stand for one-hot, label and scaled label encoding, respectively. A higher score means the RMSE is lower
and, therefore, better. For (a) to (e), the pairs are sorted by score, median, and then IQR. For (f), red highlights the best encoding
scheme for a model over all systems.

Pair
NN_label
NN_scaled
NN_onehot
KRR_onehot
RF_label
LR_label
RF_scaled
LR_onehot
KRR_label
LR_scaled
KRR_scaled
RF_onehot
DT_scaled
DT_onehot
DT_label
ğ‘˜NN_scaled
ğ‘˜NN_onehot
ğ‘˜NN_label
SVR_onehot
SVR_scaled
SVR_label

Pair
NN_onehot
NN_label
NN_scaled
RF_scaled
RF_label
RF_onehot
DT_scaled
DT_label
DT_onehot
ğ‘˜NN_scaled
KRR_onehot
ğ‘˜NN_onehot
ğ‘˜NN_label
KRR_label
SVR_onehot
KRR_scaled
SVR_label
SVR_scaled
LR_scaled
LR_label
LR_onehot

Score
10
10
10
9
8
8
8
8
8
8
8
7
6
6
6
5
4
3
2
1
1

Score
14
13
13
12
12
12
11
11
11
10
10
9
8
7
6
5
4
3
2
2
1

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Med
2943.38
2944.45
2951.74
3014.68
3019.52
3019.55
3019.63
3020.36
3024.03
3026.49
3028.21
3060.11
3093.12
3095.11
3097.61
8968.09
15772.52
22114.97
41917.30
42228.88
42229.54

IQR
0.30
0.39
0.31
0.19
0.20
0.19
0.20
0.21
0.19
0.19
0.19
0.20
0.21
0.25
0.22
1.54
0.85
0.99
5.73
5.85
5.86

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Pair
NN_onehot
NN_scaled
NN_label
DT_label
DT_scaled
RF_scaled
RF_label
RF_onehot
DT_onehot
ğ‘˜NN_label
ğ‘˜NN_scaled
ğ‘˜NN_onehot
KRR_onehot
LR_onehot
LR_scaled
KRR_scaled
LR_label
KRR_label
SVR_label
SVR_onehot
SVR_scaled

Score
12
11
10
9
9
8
8
7
7
6
5
4
3
2
2
2
2
2
1
1
1

Med
2497.07
2887.66
3019.11
15621.69
15621.77
25417.20
25457.00
57173.71
59610.67
121668.37
151716.91
168726.75
192199.55
193658.25
194094.27
194588.83
194648.94
195128.40
321782.26
321878.92
321923.45

IQR
0.10
0.20
0.34
5.92
5.92
3.37
3.18
5.31
9.35
3.96
3.54
2.81
3.73
3.88
3.76
3.84
3.79
3.88
6.84
6.86
6.86

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Pair
NN_scaled
NN_label
NN_onehot
RF_label
RF_scaled
RF_onehot
DT_onehot
DT_label
DT_scaled
ğ‘˜NN_onehot
ğ‘˜NN_scaled
KRR_onehot
KRR_scaled
KRR_label
LR_onehot
LR_scaled
LR_label
ğ‘˜NN_label
SVR_onehot
SVR_scaled
SVR_label

Score
12
12
11
10
10
9
8
7
7
6
5
4
3
3
3
3
3
2
1
1
1

Med
398.39
398.39
645.28
893.39
896.18
947.02
1104.22
1140.59
1140.63
1302.37
1359.37
1378.64
1410.46
1411.00
1411.28
1411.56
1412.19
1502.74
1521.08
1525.33
1525.57

IQR
8.37
8.37
9.91
9.60
9.25
11.43
12.32
17.93
17.93
7.61
7.14
8.30
9.14
9.15
10.07
9.14
9.08
10.07
11.72
11.75
11.74

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(a). MongoDB

(b). Lrzip

(c). Trimesh

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Med
67.62
86.18
92.37
158.61
159.83
164.02
179.07
181.96
182.42
363.34
369.58
438.83
1008.23
1113.89
1210.97
1306.24
1495.85
1507.12
4562.24
4483.84
7814.81

IQR
0.88
0.61
1.60
4.00
4.00
3.99
3.46
3.40
3.42
2.40
0.95
1.24
1.43
1.61
1.25
1.56
1.82
1.92
1.97
2.12
99

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Pair
NN_onehot
RF_label
NN_label
RF_onehot
NN_scaled
ğ‘˜NN_onehot
RF_scaled
DT_label
KRR_onehot
DT_onehot
DT_scaled
LR_label
ğ‘˜NN_scaled
LR_scaled
KRR_label
KRR_scaled
ğ‘˜NN_label
SVR_label
SVR_onehot
SVR_scaled
LR_onehot

Score
19
19
18
17
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1

Med
446.49
581.18
481.45
665.35
671.84
1021.07
735.36
739.20
910.47
803.96
950.32
1046.27
939.14
1084.04
1110.85
1183.43
1455.11
1487.67
1566.62
1714.36
9999.99

IQR
7.94
6.07
6.97
6.60
5.32
4.49
4.61
6.96
4.64
8.26
6.04
3.38
3.79
3.01
3.61
3.39
4.34
4.93
5.47
5.89
17.91

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Pair
NN_onehot
NN_label
NN_scaled
RF_onehot
RF_label
RF_scaled
DT_onehot
DT_label
DT_scaled
ğ‘˜NN_onehot
ğ‘˜NN_label
ğ‘˜NN_scaled
LR_onehot
LR_label
LR_scaled
KRR_onehot
KRR_label
KRR_scaled
SVR_onehot
SVR_label
SVR_scaled

Total Score
66
63
63
52
57
53
44
47
44
39
24
34
15
25
23
39
27
24
13
11
8

(d). ExaStencils

(e). x264

(f). Total Scott-Knott scores over all systems

time taken for evaluating a model under all encoding schemes for
a system.

4.1.2 Results Figure 2 shows the result, from which we obtain
some clear evidence:

â€¢ Finding 1: It can take an extremely long time to conclude
which encoding scheme is better depending on the models:
this is almost 100 hours (median) for RF and around 80 hours
(median) for SVR in general; it can go up to 400+ hours on
some systems. For KRR and NN, which takes less time to
do so, still requires around at least two and a half days (60+
hours).

â€¢ Finding 2: For certain models, it may be possible to find
the best encoding scheme. For example, it takes less than an
hour for ğ‘˜NN, DT, and LR due to their low computational
needs. Yet, whether one would be willing to spend valuable
development time for this is really case-dependent.

Figure 2: The boxplot of the total time required for identify-
ing the best encoding scheme (with respect to a model) over
all systems studied.

system (including training, hyperparameter tuning, and evalua-
tion). To identify the best encoding scheme using trial-and-error
in a case-by-case manner, the â€œeffortsâ€ required would be the total

The above confirms that finding the best encoding scheme for
learning software performance can be non-trivial and the needs of
our study. Therefore, for RQ1, we say:

RFkNNSVRDTLRKRRNNModel0100200300400Time (hours)Does Configuration Encoding Matter in Learning Software Performance?

MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

Table 4: Scott-Knott test, Med (median), and Interquartile Range (IQR) on the training time (minutes) of all models and systems.
The format is the same as that for Table 3.

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Pair
LR_scaled
LR_label
LR_onehot
DT_label
DT_scaled
ğ‘˜NN_label
ğ‘˜NN_onehot
DT_onehot
ğ‘˜NN_scaled
NN_scaled
NN_onehot
RF_label
KRR_scaled
NN_label
KRR_onehot
SVR_label
RF_scaled
SVR_onehot
KRR_label
RF_onehot
SVR_scaled

Pair
LR_scaled
LR_label
LR_onehot
ğ‘˜NN_label
ğ‘˜NN_scaled
DT_scaled
DT_label
ğ‘˜NN_onehot
DT_onehot
KRR_scaled
KRR_onehot
RF_label
RF_scaled
SVR_scaled
SVR_label
NN_onehot
SVR_onehot
KRR_label
NN_scaled
NN_label
RF_onehot

Score Med
<0.01
<0.01
0.02
0.49
0.52
0.84
0.86
1.07
1.36
21.61
23.76
28.71
28.86
29.02
29.82
30.45
34.20
34.75
42.28
47.84
60.96

19
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

IQR
0.00
0.00
0.01
0.01
0.01
0.03
0.02
0.04
0.05
1.30
0.20
0.33
0.08
0.24
0.08
0.54
0.62
1.61
0.18
1.62
47.97

(a). MongoDB

Score Med
<0.01
<0.01
0.01
0.18
0.22
0.23
0.26
0.49
0.73
13.96
15.30
15.64
15.86
16.98
16.48
18.02
18.50
21.01
25.70
29.11
32.51

20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
5
4
3
2
1

IQR
0.00
0.00
0.00
0.00
0.02
0.00
0.01
0.05
0.01
0.15
2.39
0.35
0.26
0.10
3.21
4.46
0.07
21.64
19.07
24.35
44.27

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Pair
LR_scaled
LR_label
LR_onehot
ğ‘˜NN_label
ğ‘˜NN_scaled
DT_scaled
DT_label
ğ‘˜NN_onehot
DT_onehot
KRR_label
KRR_scaled
RF_scaled
RF_label
SVR_label
SVR_scaled
KRR_onehot
SVR_onehot
NN_scaled
NN_label
NN_onehot
RF_onehot

Pair
LR_label
LR_scaled
DT_label
ğ‘˜NN_label
DT_scaled
ğ‘˜NN_scaled
LR_onehot
ğ‘˜NN_onehot
DT_onehot
KRR_label
SVR_scaled
NN_scaled
KRR_onehot
KRR_scaled
NN_label
RF_scaled
SVR_label
RF_label
NN_onehot
RF_onehot
SVR_onehot

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Score Med
<0.01
<0.01
0.01
0.23
0.30
0.30
0.31
0.80
0.85
14.72
14.92
17.01
17.15
17.73
18.69
22.22
28.41
27.85
28.03
35.36
36.77

20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

IQR
0.00
0.00
0.01
0.00
0.06
0.01
0.01
0.03
0.01
0.57
0.03
0.48
0.38
0.58
0.35
1.09
0.98
4.20
8.81
5.16
0.50

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Pair
LR_scaled
LR_label
LR_onehot
ğ‘˜NN_label
ğ‘˜NN_scaled
DT_scaled
DT_label
ğ‘˜NN_onehot
DT_onehot
KRR_label
KRR_scaled
KRR_onehot
RF_scaled
RF_label
SVR_scaled
SVR_label
SVR_onehot
NN_label
NN_scaled
NN_onehot
RF_onehot

Score Med
<0.01
<0.01
0.01
0.17
0.25
0.26
0.26
0.47
1.36
13.78
13.75
14.35
16.75
16.90
17.11
17.33
21.01
22.25
23.07
22.13
63.34

20
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1

IQR
0.00
0.00
0.01
0.00
0.02
0.00
0.00
0.00
0.03
0.03
0.05
0.50
0.15
0.27
0.04
0.14
0.14
9.76
9.06
10.26
1.15

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(b). Lrzip

(c). Trimesh

(cid:118)(cid:102)

Score Med
<0.01
<0.01
0.41
0.46
0.48
0.48
0.57
1.98
9.17
18.87
19.69
16.83
23.76
33.79
29.38
30.28
35.82
34.27
81.93
244.98
519.99

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
5
5
4
3
2
1

IQR
0.00
0.00
0.00
0.00
0.00
0.01
0.02
0.04
0.05
0.88
0.42
0.82
0.21
0.33
2.17
0.07
0.15
3.76
4.25
10.31
23.35

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

(cid:118)(cid:102)

Pair
LR_onehot
LR_label
LR_scaled
ğ‘˜NN_onehot
ğ‘˜NN_label
ğ‘˜NN_scaled
DT_onehot
DT_label
DT_scaled
KRR_onehot
KRR_label
KRR_scaled
SVR_onehot
SVR_label
SVR_scaled
RF_onehot
RF_label
RF_scaled
NN_onehot
NN_label
NN_scaled

Score
86
96
97
66
83
75
61
77
77
38
39
46
18
29
30
6
38
35
21
20
28

(d). ExaStencils

(e). x264

(f). Total Scott-Knott scores over all systems

RQ1: Depending on the model, finding the best encoding
scheme using trial-and-error can be highly expensive, as it
may take 60+ hours (median) and up to 400+ hours. However,
for other cases, the â€œeffort" only need less than one hour, which
may be acceptable depending on the scenario.

4.2 RQ2: Accuracy

4.2.1 Method To study RQ2, we compare all RMSE values for the
three encoding schemes under the models and systems. That is, for
each subject system, there are 3 Ã— 7 = 21 pairs of model-encoding
(50 RMSE repeats each). To ensure statistical significance among
the comparisons, we use Scott-Knott test to assign a score for each
pair, hence similar ones are clustered together (same score) while
different ones can be ranked (the higher score, the better).

4.2.2 Results As illustrated in Table 3, we observe some interesting
findings:

â€¢ Finding 3: From Table 3f, overall, label and one-hot encod-
ing are clearly more accurate than scaled label encoding
across the models, as the former two have the best total

Scott-Knott scores for all models over the systems studied.
Between these two, one-hot encoding tends to be slightly
better across all models, as it wins on 4 models against the
3 wins by label encoding. We observe similar trend from
Table 3a to 3e over the systems.

â€¢ Finding 4: For different models in Table 3f, we observe
that one-hot encoding is the best for deep learning, lazy, and
kernel models while label encoding is preferred on linear and
tree models. The same has also been registered in Table 3a
to 3e.

â€¢ Finding 5: From Table 3a to 3e, NN is clearly amongst the
top models on Scott-Knott score and RMSE regardless of the
encoding schemes and systems. In particular, when NN is
chosen, NN_onehot is the best, as it has a better Scott-Knott
score than the other two on 3 out of 5 systems, draw on one
system and lose on the remaining one, leading to a 75% cases
of no worse outcome than NN_label and NN_scaled.

To conclude, we can answer RQ2 as:

MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

Jingzhi Gong and Tao Chen

(a) MongoDB

(b) Lrzip

(c) Trimesh

(d) ExaStencils

(e) x264

Figure 3: The trade-off between RMSE and training time over all Pareto-optimal model-encoding pairs.

RQ2: In general, the one-hot encoding tends to have the best
accuracy and the scaled label encoding should be avoided.
In particular, NN_onehot is the safest option for the overall
optimal accuracy among the subjects studied.

â€¢ Finding 8: From Table 4a to 4e, unexpectedly LR has the
fastest training time over all systems and this model works
the best with scaled label encoding since LR_scaled is the
fastest on 4 out of 5 systems; its difference to LR_label tends
to be marginal though.

4.3 RQ3: Training Time

Therefore, we say:

4.3.1 Research Similar to RQ2, here we measured the training time
over 50 runs for all 21 pairs of model-encoding for each system.

4.3.2 Results With Table 4, we can observe some patterns:

â€¢ Finding 6: Overall, from Table 4f, label and scaled label
encoding are much faster to train than their one-hot coun-
terpart, which has never won the other two under any model
across the systems. In particular, scaled label encoding ap-
pears to have the fastest training than others in general, as
the former wins on 4 models, draws on one, and loses only
on two. Similar results have been obtained in Table 4a to 4e.
â€¢ Finding 7: For different model types, in Table 4f, the label
encoding tends to be the best option in terms of training time
for tree model and lazy model; the scaled label counterpart
is faster on deep learning model, linear model, and kernel
model. This is similar to that from Table 4a to 4e.

RQ3: The scaled label encoding tends to have the fastest
training while one-hot encoding takes the longest time to train.
In particular, LR_scaled is the best choice for the overall
fastest training time over the subjects studied.

4.4 RQ4: Trade-off Analysis

4.4.1 Method Understanding RQ4 requires us to simultaneously
consider the accuracy and training time achieved by all 21 pairs
of model-encoding. According to the guidance provided by Li and
Chen [37], for each system, we seek to analyze the Pareto optimal
choices as those are the ones that require trade-offs. Suppose that a
pair ğ‘ƒğ‘¥ has {ğ´ğ‘¥ ,ğ‘‡ğ‘¥ } and another ğ‘ƒğ‘¦ comes with {ğ´ğ‘¦,ğ‘‡ğ‘¦ }, whereby
ğ´ğ‘¥ and ğ´ğ‘¦ are their median RMSE (over 50 runs) while ğ‘‡ğ‘¥ and ğ‘‡ğ‘¦
are their median training time, respectively. We say ğ‘ƒğ‘¥ dominates
ğ‘ƒğ‘¦ if ğ´ğ‘¥ â‰¤ ğ´ğ‘¦ and ğ‘‡ğ‘¥ â‰¤ ğ‘‡ğ‘¦ while there is either ğ´ğ‘¥ < ğ´ğ‘¦ or

0102030Training time (minutes)29402960298030003020RMSELR_labelLR_scaledNN_labelNN_scaled0102030Training time (minutes)050000100000150000200000RMSEkNN_labelDT_labelDT_scaledLR_onehotLR_scaledNN_onehotNN_scaled01020Training time (minutes)400600800100012001400RMSERF_labelRF_scaledkNN_scaledDT_onehotDT_labelDT_scaledLR_onehotLR_labelLR_scaledNN_onehotNN_labelNN_scaled051015Training time (minutes)100200300400RMSERF_labelRF_scaledkNN_scaledDT_scaledLR_labelLR_scaledNN_onehot020406080Training time (minutes)5006007008009001000RMSEDT_labelLR_labelNN_onehotNN_labelNN_scaledDoes Configuration Encoding Matter in Learning Software Performance?

MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

(a) DT

(b) NN

(c) KRR

(d) ğ‘˜NN

(e) LR

(f) RF

(g) SVR

Figure 4: The total Scott-Knott scores for each model over
all systems. Only the Pareto-optimal choice are presented.

ğ‘‡ğ‘¥ < ğ‘‡ğ‘¦. A pair, which is not dominated by any other pairs from
the total set of 21, is called a Pareto-optimal pair therein. The set
of all Pareto-optimal points is called the Pareto front (Figures 3).
We also plot the Pareto front with respect to the total Scott-Knott
scores (over all systems) under each model (Figures 4).

Here, a Pareto-optimal pair that has the best accuracy or the
fastest training time is called a biased point (or an extreme point).
Among others, we are interested in the non-extreme, less biased
points, especially those with a well-balanced trade-off.

4.4.2 Results The results are illustrated in Figures 3 and 4, from
which we obtain some interesting observations:

â€¢ Finding 9: Over all the model-encoding pairs (Figures 3),
label and scaled label encoding can more commonly lead to
less biased results (non-extreme points) in the Pareto front
than their one-hot counterpart. This can clearly offer more
trade-off choices.

â€¢ Finding 10: In Figures 3, the scaled label encoding tends
to achieve more balanced trade-off than the others, but the
paired model may vary, i.e., it can be NN, DT, or RF, across
the systems. For example, it is NN_scaled on MongoDB but
becomes DT_scaled on Lrzip.

â€¢ Finding 11: Kernel models like KRR and SVR have never
produced Pareto-optimal outcomes over the 5 systems stud-
ied (Figures 3).

â€¢ Finding 12: From Figure 4, only kernel and lazy models
have relatively less biased point in their own Pareto front,
which are achieved by label (Figure 4c and 4g) and scaled
label encoding (Figure 4d), respectively.

In summary, we have:

RQ4: For all the model-encoding pairs, label and scaled la-
bel encoding tend to be less biased to accuracy or training
time than their one-hot counterpart. In particular, scaled la-
bel encoding can lead to relatively more balanced outcomes.
However, the paired model for the above may differ depend-
ing on the system, but it would never be KRR or SVR, which
produce no Pareto-optimal pairs.

5 Actionable Suggestions

In this section, we discuss the suggestions on the encoding scheme
for learning software performance under a variety of circumstances.

Suggestion 1: When RF, SVR, KRR or NN is to be used, we do
not recommend trial-and-error to find the best encoding scheme.
However, for ğ‘˜NN, DT or LR, it may be practical to â€œtry them allâ€.

From RQ1, it can be rather time-consuming for comparing all
three encoding schemes under RF, SVR, KRR or NN. Indeed, the
â€œeffortsâ€ may be reduced if we consider, e.g., less repeated runs or
even reduced data samples. However, to provide a reliable choice,
what we consider in this study is essential, and hence further reduc-
ing them may increase the instability of the result. The process can
be even more expensive if different models are also to be assessed
during the trial-and-error. In contrast, when ğ‘˜NN, DT, or LR is to
be used, it only requires less than one hour each â€” an assumption
that may be more acceptable within the development lifecycle.

Suggestion 2: When the accuracy is all that matters, among all
possible models studied, we recommend using NN paired with one-
hot encoding. When a certain model needs to be used, we suggest
avoiding scaled label encoding in general and following one-hot
encoding for deep learning and kernel models; label encoding for
linear and tree models.
Reflecting on RQ2, when only the accuracy is of concern, we
can make suggestions for practitioners to infer the best choice of
encoding schemes when experimental assessment is not possible or
desirable. Among others, it is clear that NN tends to offer the best
accuracy, and NN paired with one-hot encoding, i.e., NN_onehot,
is the most reliable choice. In contrast, scaled label encoding often
performs the worst, and hence scaled label encoding can be ruled
out from the suggestions.

Besides the fact that the one-hot encoding can generally lead
to the best accuracy over the models, we do observe some specific
patterns when the model to be used is fixed: one-hot encoding for
deep learning and kernel models while label encoding for linear
and tree models.

Suggestion 3: When faster training time is more preferred (e.g.,
the model needs to be rapidly retrained at runtime), over all models
studied, we recommend using linear regression paired with scaled
label encoding. When the model is fixed, we suggest adopting
scaled label encoding in general (especially for deep learning,
linear, and kernel models) and label encoding for tree and lazy
models; one-hot encoding should be avoided.

Deriving from the findings for RQ3, if the training time is of
higher importance, we can also estimate the suitable choice of
encoding scheme in the absence of experimental evaluation. Over
all possible models studied, linear regression is unexpectedly the

7580Time score44454647RMSE score22.525.027.5Time score63646566RMSE score4045Time score253035RMSE score7080Time score253035RMSE score96.096.597.0Time score23.023.524.024.525.0RMSE score363840Time score54565860RMSE score202530Time score81012RMSE scoreMSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

Jingzhi Gong and Tao Chen

fastest to train and when it is paired with scaled label encoding
(LR_scaled) the training is the fastest. One-hot encoding is often
the slowest to train, and hence can be avoided.

Although the scaled label encoding appears to be faster to train
than its label counterpart, they remain competitive. In fact, when the
model to be used has been pre-defined, we observe some common
patterns: the scaled label encoding is the best for deep learning,
linear, and kernel models while the label encoding is preferred for
tree and lazy models.

Suggestion 4: When the preference between accuracy and train-
ing time is unclear while the unbiased outcome is preferred, over
all models studied, we recommend using scaled label encoding,
but the paired model needs some efforts to determine. We certainly
suggest avoiding one-hot encoding and kernel model (KRR and
SVR regardless of its encoding schemes). When the model is fixed
to the kernel and lazy models, the label and scaled label encoding
can be chosen to reduce the bias, respectively.

It is not uncommon that the preference between accuracy and
training time can be unclear, and hence an unbiased outcome is
important. According to the findings for RQ4, this needs the label
and scaled label encoding. Because in this case, as we have shown,
they often lead to results that are in the middle of the Pareto front
for the pairs. In particular, scaled label encoding can often lead to
well-balanced results in contrast to the other, but the paired model
may vary. We would also suggest avoiding one-hot encoding and
kernel model (SVR and KRR), as the former would easily bias to
accuracy or training time while the latter leads to no Pareto optimal
choice at all over the systems studied.

However, when the model needs to be fixed, only the kernel and
lazy models can have less biased choices, which are under the label
and scaled label encoding, respectively.

6 Discussion

We now discuss a few interesting points derived from our study.

6.1 Practicality of Performance Models

The performance models built can be used in different practical
scenarios, under each of which the accuracy and training time can
be of great importance (and thereby the choice of encoding schemes
are equally crucial).

6.1.1 Configuration debugging Ill-fitted Configurations can lead
to bugs such that the resulted performance is dramatically worse
than the expectation. Here, a performance model can help software
engineers easily inspect which configuration options are likely to
be the root cause of the bug and identify the potential fix [55].
The fact that the model makes inferences without running the
system can greatly improve the efficiency of the debugging process.
Further, by analyzing the models, software engineers can gain a
better understanding of the systemâ€™s performance characteristics
which helps to prevent future configuration bugs.

Speed up automatic configuration tuning Automatic config-
6.1.2
uration tuning is necessary to optimize the performance of the
software system at deployment time. However, due to the expen-
siveness of measuring the performance, tuning is often a slow
and time-consuming process. As one resolution to that issue, the

performance model can serve as the surrogate for cheap evalua-
tion of the configuration. Indeed, there have been a few successful
applications in this regard, such as those that rely on Bayesian
optimization [33, 41].

6.1.3 Runtime self-adaptation Self-adapting the configuration at
runtime is a promising way to manage the systemâ€™s performance
under uncertain environments. In this context, the performance
model can help to achieve the adaptation in a timely manner, as it
offers a relatively cheap way to reason about the better or worse of
different configurations under changing environmental conditions.
From the literature of self-adaptive systems, it is not uncommon to
see that the performance models are often used during the planning
stage [5, 7, 9, 10, 12, 13].

6.2 Why Considering Different Models?

We note that some learning models perform overwhelmingly better
than the others, such as NN. Yet, our study involves a diverse set
of models because, in practice, there may be other reasons that a
learning model is preferred. For example, linear and tree models
may be used as they are directly interpretable [49, 50], despite that
they can lead to inferior accuracy overall. Therefore, our results on
the choice of encoding schemes provide evidence for a wide set of
scenarios and the possibility that different models may be involved.
The other reason for considering different models is that we seek
to examine whether the choice of machine learning model matters
when deicing what encoding schemes to use. Indeed, our results
show that the paired model is an integral part and we provide
detailed suggestions in that regard.

6.3 On Interactions between Configuration

Options

The encoding schemes can serve as different ways to represent the
interactions between configuration options. Since the one-hot en-
coding embeds the values of options as the feature dimensions and
captures their interactions, it models a much more finer-grained
feature space compared with that of the label and scaled-label coun-
terparts. Our results show that, indeed, such a finer-grained capture
of interactions enables one-hot encoding to become the most re-
liable scheme across the models/software as it has the generally
best accuracy. This confirms the current understanding that the
interaction between configuration options is important and the way
how they are handled can significantly influence the accuracy [49].
Most importantly, our findings show that it is possible to better
handle the interactions at the level of encoding.

7 Threats to validity

Similar to many empirical studies in software engineering, our
work is subject to threats to validity. Specifically, internal threats
can be related to the configuration options used and their ranges.
Indeed, a different set may lead to a different result in some cases.
However, here we follow what has been commonly used in state-of-
the-art studies, which are representatives for the subject systems.
The hyperparameter of the models to tune can also impose this
threat. Ideally, widening the set of hyperparameters to tune can
complement our results. Yet, considering an extensive set of hyper-
parameters is rather expensive, as the tuning needs to go through

Does Configuration Encoding Matter in Learning Software Performance?

MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

the full training and validation process. To mitigate such, we have
examined different hyperparameters in preliminary runs for finding
a balance between effectiveness and overhead.

Construct threats to validity can be related to the metric used.
While different metrics exist for measuring accuracy, here we use
RMSE, which is a widely used one for learning software perfor-
mance. The results are also evaluated validated Scott-Knott test [38].
We also set a data samples of 5, 000, which tends to be reasonable as
this is what has been commonly used in prior work [19, 22, 34, 47].
Indeed, using other metrics or different sample size may offer new
insights, which we plan to do in future work.

Finally, external threats to validity can raise from the subjects
and models used. To mitigate such, we study five commonly stud-
ied systems that are of diverse characteristics, together with seven
widely-used models. This leads to a total of 105 cases of investi-
gation. Such a setting, although not exhaustive, is not uncommon
in empirical software engineering and can serve as a strong foun-
dation to generalize our findings, especially considering that an
exhaustive study of all possible models and systems is unrealistic.
Yet, we agree that additional subjects may prove fruitful.

8 Related Work

A most widely used representation for building machine learning-
based software performance model is the one-hot encoding [2, 25,
49]. The root motivation of such encoding is derived from the fact
that a configurable system can be represented by the feature model
â€” a tree-liked structure that captures the variability [49]. In a feature
model, each feature can be selected or deselected, which is naturally
a binary option. Note that categorical and numeric configuration
options can also be captured in the feature model, as long as they can
be discretized [14]. Following this, several approaches have been
developed using machine learning. Among others, Guo et al. [25]
use the one-hot encoding combined with the DT to predict software
performance, as it fits well with the feature model. Bao et al. [2]
also use the same encoding, and their claim is that it can better
capture the options which have no ordinal relationships.

The other, perhaps more natural, encoding scheme for learn-
ing software performance is the label encoding, which has also
been followed by many studies, either with [4, 8, 26] or without
scaling [40, 41, 48]. For example, Chen and Bahsoon [8] directly
encode the configuration options to learn the performance model
with normalization to [0, 1]. Siegmund et al. [48] also follows the
label encoding, but the binary and numeric configuration options
are treated differently in the model learned with no normalization.
However, the choice between those two encoding schemes for
software performance learning often lacks systematic justification,
which is the gap that this empirical study aims to bridge.

In the other domains, the importance of choosing the encoding
schemes for building machine learning models has been discussed.
For example, Jackson and Agrawal [32] compare the most common
encoding schemes for predicting security events using logs. The
result shows that it is considerably harmful to encode the represen-
tation without systematic justification. Similarly, He and Parida [28]
study the effect of two encoding schemes for genetic trait prediction.
A thorough analysis of the encoding schemes has been provided,
together with which could be better under what cases. However,

those findings cannot be directly applied in the context of software
performance learning, due to two of its properties:

â€¢ Sampling from the configurable systems is rather expen-
sive [33, 41, 56], thus the sample size is often relatively
smaller.

â€¢ Software configuration is often sparse, i.e., the close con-
figurations may have rather different performance [33, 41].
This is because options like cache, when enabled, can cre-
ate significant implications to the performance, but such a
change is merely represented as a one-bit difference in the
model. Therefore, the distribution of the data samples can
be intrinsically different from the other domains.

Most importantly, this work provides an in-depth understand-
ing of this topic for learning software performance, together with
insights and suggestions under different circumstances.

9 Conclusions

This paper bridges a gap in the understating of encoding schemes
for learning performance for highly configurable software. We
do that by conducting a systematic empirical study, covering five
systems, seven models, and three widely used encoding schemes,
giving a total of 105 cases of investigation. In summary, we show
that

Choosing the encoding scheme is non-trivial for perfor-
mance learning and it can be rather expensive to do it
using trial-and-error in a case-by-case manner.

Our findings provide actionable suggestions and â€œrule-of-thumbâ€
when a thorough experimental comparison is not possible or desir-
able. Among these, the most important ones over all models and
encoding schemes are:

â€¢ using neural network paired with one-hot encoding for the

best accuracy.

â€¢ using linear regression paired with scaled label encoding for

the fastest training.

â€¢ using scaled label encoding for a relatively well-balanced

outcome, but mind the underlying model.

We hope that this work can serve as a good starting point to raise
the awareness of the importance of choosing encoding schemes
for performance learning, and the actionable suggestions are of
usefulness to the practitioners in the field. More importantly, we
seek to spark a dialog on a set of relevant future research directions
for this regard. As such, the next stage on this research thread is
vast, including designing specialized models paired with suitable
encoding schemes or even investigating new, tailored encoding
schemes derived from the findings in the paper.

References

[1] Mokhtar Z. Alaya, Simon Bussy, StÃ©phane GaÃ¯ffas, and Agathe Guilloux. 2019.
Binarsity: a penalization for one-hot encoded features in linear supervised learn-
ing. J. Mach. Learn. Res. 20 (2019), 118:1â€“118:34. http://jmlr.org/papers/v20/17-
170.html

[2] Liang Bao, Xin Liu, Fangzheng Wang, and Baoyin Fang. 2019. ACTGAN: Auto-
matic Configuration Tuning for Software Systems with Generative Adversarial
Networks. In 34th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019. IEEE, 465â€“476.
https://doi.org/10.1109/ASE.2019.00051

[3] Tianfeng Chai and Roland R Draxler. 2014. Root mean square error (RMSE) or
mean absolute error (MAE)?â€“Arguments against avoiding RMSE in the literature.
Geoscientific model development 7, 3 (2014), 1247â€“1250.

MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

Jingzhi Gong and Tao Chen

[4] Tao Chen. 2019. All versus one: an empirical comparison on retrained and
incremental machine learning for modeling performance of adaptable software.
In Proceedings of the 14th International Symposium on Software Engineering for
Adaptive and Self-Managing Systems, SEAMS@ICSE 2019, Montreal, QC, Canada,
May 25-31, 2019, Marin Litoiu, SiobhÃ¡n Clarke, and Kenji Tei (Eds.). ACM, 157â€“168.
https://doi.org/10.1109/SEAMS.2019.00029

[5] Tao Chen. 2022. Lifelong dynamic optimization for self-adaptive systems: fact or
fiction?. In SANER â€™22: 29th IEEE International Conference on Software Analysis,
Evolution and Reengineering, Hawaii, United States, March 15-18 2022. IEEE.
[6] Tao Chen and Rami Bahsoon. 2013. Self-adaptive and sensitivity-aware QoS
modeling for the cloud. In Proceedings of the 8th International Symposium on
Software Engineering for Adaptive and Self-Managing Systems, SEAMS 2013, San
Francisco, CA, USA, May 20-21, 2013, Marin Litoiu and John Mylopoulos (Eds.).
IEEE Computer Society, 43â€“52. https://doi.org/10.1109/SEAMS.2013.6595491
[7] Tao Chen and Rami Bahsoon. 2014. Symbiotic and sensitivity-aware architecture
for globally-optimal benefit in self-adaptive cloud. In 9th International Symposium
on Software Engineering for Adaptive and Self-Managing Systems, SEAMS 2014,
Proceedings, Hyderabad, India, June 2-3, 2014, Gregor Engels and Nelly Bencomo
(Eds.). ACM, 85â€“94. https://doi.org/10.1145/2593929.2593931

[8] Tao Chen and Rami Bahsoon. 2017. Self-Adaptive and Online QoS Modeling for
Cloud-Based Software Services. IEEE Trans. Software Eng. 43, 5 (2017), 453â€“475.
https://doi.org/10.1109/TSE.2016.2608826

[9] Tao Chen and Rami Bahsoon. 2017. Self-Adaptive Trade-off Decision Making
for Autoscaling Cloud-Based Services. IEEE Trans. Serv. Comput. 10, 4 (2017),
618â€“632. https://doi.org/10.1109/TSC.2015.2499770

[10] Tao Chen, Rami Bahsoon, Shuo Wang, and Xin Yao. 2018. To Adapt or Not to
Adapt?: Technical Debt and Learning Driven Self-Adaptation for Managing Run-
time Performance. In Proceedings of the 2018 ACM/SPEC International Conference
on Performance Engineering, ICPE 2018, Berlin, Germany, April 09-13, 2018, Katinka
Wolter, William J. Knottenbelt, AndrÃ© van Hoorn, and Manoj Nambiar (Eds.).
ACM, 48â€“55. https://doi.org/10.1145/3184407.3184413

[11] Tao Chen, Rami Bahsoon, and Xin Yao. 2014. Online QoS Modeling in the
Cloud: A Hybrid and Adaptive Multi-learners Approach. In Proceedings of the 7th
IEEE/ACM International Conference on Utility and Cloud Computing, UCC 2014,
London, United Kingdom, December 8-11, 2014. IEEE Computer Society, 327â€“336.
https://doi.org/10.1109/UCC.2014.42

[12] Tao Chen, Rami Bahsoon, and Xin Yao. 2018. A Survey and Taxonomy of Self-
Aware and Self-Adaptive Cloud Autoscaling Systems. ACM Comput. Surv. 51, 3
(2018), 61:1â€“61:40. https://doi.org/10.1145/3190507

[13] Tao Chen, Rami Bahsoon, and Xin Yao. 2020. Synergizing Domain Expertise
With Self-Awareness in Software Systems: A Patternized Architecture Guideline.
Proc. IEEE 108, 7 (2020), 1094â€“1126. https://doi.org/10.1109/JPROC.2020.2985293
[14] Tao Chen, Ke Li, Rami Bahsoon, and Xin Yao. 2018. FEMOSAA: Feature-Guided
and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software. ACM
Trans. Softw. Eng. Methodol. 27, 2 (2018), 5:1â€“5:50. https://doi.org/10.1145/3204459
[15] Tao Chen and Miqing Li. 2021. MMO: Meta Multi-Objectivization for Software
Configuration Tuning. CoRR abs/2112.07303 (2021). arXiv:2112.07303 https:
//arxiv.org/abs/2112.07303

[16] Tao Chen and Miqing Li. 2021. Multi-objectivizing software configuration tuning.
In ESEC/FSE â€™21: 29th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, Athens, Greece, August
23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha Chechik, and Massim-
iliano Di Penta (Eds.). ACM, 453â€“465. https://doi.org/10.1145/3468264.3468555
[17] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine

learning 20, 3 (1995), 273â€“297.

[18] Diego Didona, Francesco Quaglia, Paolo Romano, and Ennio Torre. 2015. Enhanc-
ing Performance Prediction Robustness by Combining Analytical Modeling and
Machine Learning. In Proceedings of the 6th ACM/SPEC International Conference
on Performance Engineering, Austin, TX, USA, January 31 - February 4, 2015, Lizy K.
John, Connie U. Smith, Kai Sachs, and Catalina M. LladÃ³ (Eds.). ACM, 145â€“156.
https://doi.org/10.1145/2668930.2688047

[19] Johannes Dorn, Sven Apel, and Norbert Siegmund. 2020. Mastering Uncertainty
in Performance Estimations of Configurable Software Systems. In 35th IEEE/ACM
International Conference on Automated Software Engineering, ASE 2020, Melbourne,
Australia, September 21-25, 2020. IEEE, 684â€“696. https://doi.org/10.1145/3324884.
3416620

[20] Jingzhou Fei, Ningbo Zhao, Yong Shi, Yongming Feng, and Zhongwei Wang. 2016.
Compressor performance prediction using a novel feed-forward neural network
based on Gaussian kernel function. Advances in Mechanical Engineering 8, 1
(2016), 1687814016628396.

[21] Evelyn Fix. 1985. Discriminatory analysis: nonparametric discrimination, consis-

tency properties. Vol. 1. USAF school of Aviation Medicine.

[22] Ilias Gerostathopoulos, Christian Prehofer, and TomÃ¡s Bures. 2018. Adapting a sys-
tem with noisy outputs with statistical guarantees. In Proceedings of the 13th Inter-
national Conference on Software Engineering for Adaptive and Self-Managing Sys-
tems, SEAMS@ICSE 2018, Gothenburg, Sweden, May 28-29, 2018, Jesper Andersson
and Danny Weyns (Eds.). ACM, 58â€“68. https://doi.org/10.1145/3194133.3194152

[23] Rebecca F. Goldin. 2010. Review: Statistical Models: Theory and Practice (Revised
Edition). Cambridge University Press, New York, New York, 2009, xiv + 442 pp.,
ISBN 978-0-521-74385-3, $40. by David A. Freedman. Am. Math. Mon. 117, 9
(2010), 844â€“847. https://doi.org/10.4169/000298910X521733

[24] Johannes Grohmann, Daniel Seybold, Simon Eismann, Mark Leznik, Samuel
Kounev, and JÃ¶rg Domaschka. 2020. Baloo: Measuring and Modeling the Per-
formance Configurations of Distributed DBMS. In 28th International Sympo-
sium on Modeling, Analysis, and Simulation of Computer and Telecommunica-
tion Systems, MASCOTS 2020, Nice, France, November 17-19, 2020. IEEE, 1â€“8.
https://doi.org/10.1109/MASCOTS50786.2020.9285960

[25] Jianmei Guo, Krzysztof Czarnecki, Sven Apel, Norbert Siegmund, and Andrzej
Wasowski. 2013. Variability-aware performance prediction: A statistical learning
approach. In 2013 28th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2013, Silicon Valley, CA, USA, November 11-15, 2013, Ewen
Denney, Tevfik Bultan, and Andreas Zeller (Eds.). IEEE, 301â€“311. https://doi.org/
10.1109/ASE.2013.6693089

[26] Huong Ha and Hongyu Zhang. 2019. DeepPerf: performance prediction for
configurable software with deep sparse neural network. In Proceedings of the
41st International Conference on Software Engineering, ICSE 2019, Montreal, QC,
Canada, May 25-31, 2019, Joanne M. Atlee, Tevfik Bultan, and Jon Whittle (Eds.).
IEEE / ACM, 1095â€“1106. https://doi.org/10.1109/ICSE.2019.00113

[27] Xue Han and Tingting Yu. 2016. An Empirical Study on Performance Bugs for
Highly Configurable Software Systems. In Proceedings of the 10th ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement,
ESEM 2016, Ciudad Real, Spain, September 8-9, 2016. ACM, 23:1â€“23:10. https:
//doi.org/10.1145/2961111.2962602

[28] Dan He and Laxmi Parida. 2016. Does encoding matter? A novel view on the
quantitative genetic trait prediction problem. BMC Bioinform. 17, S-9 (2016), 272.
https://doi.org/10.1186/s12859-016-1127-1

[29] Geoffrey E. Hinton. 2012. A Practical Guide to Training Restricted Boltzmann
Machines. In Neural Networks: Tricks of the Trade - Second Edition, GrÃ©goire
Montavon, Genevieve B. Orr, and Klaus-Robert MÃ¼ller (Eds.). Lecture Notes in
Computer Science, Vol. 7700. Springer, 599â€“619. https://doi.org/10.1007/978-3-
642-35289-8_32

[30] Tin Kam Ho. 1995. Random decision forests. In Third International Conference on
Document Analysis and Recognition, ICDAR 1995, August 14 - 15, 1995, Montreal,
Canada. Volume I. IEEE Computer Society, 278â€“282. https://doi.org/10.1109/
ICDAR.1995.598994

[31] Francesco Iorio, Ali B. Hashemi, Michael Tao, and Cristiana Amza. 2019. Transfer
Learning for Cross-Model Regression in Performance Modeling for the Cloud. In
2019 IEEE International Conference on Cloud Computing Technology and Science
(CloudCom), Sydney, Australia, December 11-13, 2019. IEEE, 9â€“18. https://doi.org/
10.1109/CloudCom.2019.00015

[32] Eric Jackson and Rajeev Agrawal. 2019. Performance Evaluation of Different
Feature Encoding Schemes on Cybersecurity Logs. In 2019 SoutheastCon. 1â€“9.
https://doi.org/10.1109/SoutheastCon42311.2019.9020560

[33] Pooyan Jamshidi and Giuliano Casale. 2016. An Uncertainty-Aware Approach to
Optimal Configuration of Stream Processing Systems. In 24th IEEE International
Symposium on Modeling, Analysis and Simulation of Computer and Telecommuni-
cation Systems, MASCOTS 2016, London, United Kingdom, September 19-21, 2016.
IEEE Computer Society, 39â€“48. https://doi.org/10.1109/MASCOTS.2016.17
[34] Andreas Johnsson, Farnaz Moradi, and Rolf Stadler. 2019. Performance Prediction
in Dynamic Clouds using Transfer Learning. In IFIP/IEEE International Symposium
on Integrated Network Management, IM 2019, Washington, DC, USA, April 09-11,
2019, Joe Betser, Carol J. Fung, Alex Clemm, JÃ©rÃ´me FranÃ§ois, and Shingo Ata
(Eds.). IFIP, 242â€“250. http://dl.ifip.org/db/conf/im/im2019/189279.pdf

[35] Christian Kaltenecker, Alexander Grebhahn, Norbert Siegmund, and Sven Apel.
2020. The Interplay of Sampling and Machine Learning for Software Performance
Prediction.
IEEE Softw. 37, 4 (2020), 58â€“66. https://doi.org/10.1109/MS.2020.
2987024

[36] Ke Li, Zilin Xiang, Tao Chen, Shuo Wang, and Kay Chen Tan. 2020. Understanding
the automated parameter optimization on transfer learning for cross-project
defect prediction: an empirical study. In ICSE â€™20: 42nd International Conference on
Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020, Gregg Rothermel
and Doo-Hwan Bae (Eds.). ACM, 566â€“577.
https://doi.org/10.1145/3377811.
3380360

[37] Miqing Li, Tao Chen, and Xin Yao. 2020, in press. How to Evaluate Solutions
in Pareto-based Search-Based Software Engineering? A Critical Review and
Methodological Guidance. IEEE Transactions on Software Engineering (2020, in
press). https://doi.org/10.1109/TSE.2020.3036108

[38] Nikolaos Mittas and Lefteris Angelis. 2013. Ranking and Clustering Software
Cost Estimation Models through a Multiple Comparisons Algorithm. IEEE Trans.
Software Eng. 39, 4 (2013), 537â€“551. https://doi.org/10.1109/TSE.2012.45
[39] Felix Mohr, Marcel Wever, Alexander Tornede, and Eyke Hullermeier. 2021.
Predicting Machine Learning Pipeline Runtimes in the Context of Automated
Machine Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence
(2021).

Does Configuration Encoding Matter in Learning Software Performance?

MSR 2022, May 23â€“24, 2022, Pittsburgh, PA, USA

[40] Vivek Nair, Tim Menzies, Norbert Siegmund, and Sven Apel. 2017. Using bad
learners to find good configurations. In Proceedings of the 2017 11th Joint Meeting
on Foundations of Software Engineering, ESEC/FSE 2017, Paderborn, Germany,
September 4-8, 2017, Eric Bodden, Wilhelm SchÃ¤fer, Arie van Deursen, and Andrea
Zisman (Eds.). ACM, 257â€“267. https://doi.org/10.1145/3106237.3106238
[41] Vivek Nair, Zhe Yu, Tim Menzies, Norbert Siegmund, and Sven Apel. 2020. Finding
Faster Configurations Using FLASH. IEEE Trans. Software Eng. 46, 7 (2020), 794â€“
811. https://doi.org/10.1109/TSE.2018.2870895

[42] Jiaqi Pan, Yan Zhuang, and Simon Fong. 2016. The impact of data normalization
on stock market prediction: using SVM and technical indicators. In International
Conference on Soft Computing in Data Science. Springer, 72â€“88.

[43] Fabian Pedregosa, GaÃ«l Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Courna-
peau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-
learn: Machine Learning in Python. J. Mach. Learn. Res. 12 (2011), 2825â€“2830.
http://dl.acm.org/citation.cfm?id=2078195

[44] Kewen Peng, Christian Kaltenecker, Norbert Siegmund, Sven Apel, and Tim
Menzies. 2021. VEER: Disagreement-Free Multi-objective Configuration. CoRR
abs/2106.02716 (2021). arXiv:2106.02716 https://arxiv.org/abs/2106.02716
[45] Rodrigo Queiroz, Thorsten Berger, and Krzysztof Czarnecki. 2016. Towards
predicting feature defects in software product lines. In Proceedings of the 7th In-
ternational Workshop on Feature-Oriented Software Development, FOSD@SPLASH
2016, Amsterdam, Netherlands, October 30, 2016, Christoph Seidl and Leopoldo
Teixeira (Eds.). ACM, 58â€“62. https://doi.org/10.1145/3001867.3001874

[46] Lior Rokach and Oded Maimon. 2014. Data Mining with Decision Trees - The-
ory and Applications. 2nd Edition. Series in Machine Perception and Artificial
Intelligence, Vol. 81. WorldScientific. https://doi.org/10.1142/9097

[47] Jingyu Shao, Qing Wang, and Fangbing Liu. 2019. Learning to Sample: An Active
Learning Framework. In 2019 IEEE International Conference on Data Mining, ICDM
2019, Beijing, China, November 8-11, 2019, Jianyong Wang, Kyuseok Shim, and
Xindong Wu (Eds.). IEEE, 538â€“547. https://doi.org/10.1109/ICDM.2019.00064

[48] Norbert Siegmund, Alexander Grebhahn, Sven Apel, and Christian KÃ¤stner. 2015.
Performance-influence models for highly configurable systems. In Proceedings
of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE

2015, Bergamo, Italy, August 30 - September 4, 2015, Elisabetta Di Nitto, Mark
Harman, and Patrick Heymans (Eds.). ACM, 284â€“294. https://doi.org/10.1145/
2786805.2786845

[49] Norbert Siegmund, Sergiy S. Kolesnikov, Christian KÃ¤stner, Sven Apel, Don S.
Batory, Marko RosenmÃ¼ller, and Gunter Saake. 2012. Predicting performance
via automated feature-interaction detection. In 34th International Conference
on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland, Martin
Glinz, Gail C. Murphy, and Mauro PezzÃ¨ (Eds.). IEEE Computer Society, 167â€“177.
https://doi.org/10.1109/ICSE.2012.6227196

[50] Pavel Valov, Jianmei Guo, and Krzysztof Czarnecki. 2015. Empirical comparison of
regression methods for variability-aware performance prediction. In Proceedings
of the 19th International Conference on Software Product Line, SPLC 2015, Nashville,
TN, USA, July 20-24, 2015, Douglas C. Schmidt (Ed.). ACM, 186â€“190. https:
//doi.org/10.1145/2791060.2791069

[51] AndrÃ¡s Vargha and Harold D. Delaney. 2000. A Critique and Improvement of the

CL Common Language Effect Size Statistics of McGraw and Wong.

[52] Vladimir Vovk. 2013. Kernel ridge regression. In Empirical inference. Springer,

105â€“116.

[53] Sun-Chong Wang. 2003. Artificial neural network. In Interdisciplinary computing

in java programming. Springer, 81â€“100.

[54] Tianpei Xia, Rahul Krishna, Jianfeng Chen, George Mathew, Xipeng Shen, and
Tim Menzies. 2018. Hyperparameter Optimization for Effort Estimation. CoRR
abs/1805.00336 (2018). arXiv:1805.00336 http://arxiv.org/abs/1805.00336
[55] Tianyin Xu, Long Jin, Xuepeng Fan, Yuanyuan Zhou, Shankar Pasupathy, and
Rukma Talwadker. 2015. Hey, you have given me too many knobs!: understanding
and dealing with over-designed configuration in system software. In Proceedings
of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE
2015, Bergamo, Italy, August 30 - September 4, 2015, Elisabetta Di Nitto, Mark
Harman, and Patrick Heymans (Eds.). ACM, 307â€“319. https://doi.org/10.1145/
2786805.2786852

[56] Marcela Zuluaga, Guillaume Sergent, Andreas Krause, and Markus PÃ¼schel. 2013.
Active Learning for Multi-Objective Optimization. In Proceedings of the 30th
International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-
21 June 2013 (JMLR Workshop and Conference Proceedings, Vol. 28). JMLR.org,
462â€“470. http://proceedings.mlr.press/v28/zuluaga13.html

