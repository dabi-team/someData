Restructurable Activation Networks

Kartikeya Bhardwaj1, James Ward1, Caleb Tung2,*,†, Dibakar Gope1,∗, Lingchuan Meng3,†,
Igor Fedorov4,†, Alex Chalﬁn1, Paul Whatmough1, and Danny Loh1
1Arm Inc., 2Purdue University, 3Amazon, 4Meta
kartikeya.bhardwaj@arm.com, dibakar.gope@arm.com

2
2
0
2

p
e
S
7

]

V
C
.
s
c
[

2
v
2
6
5
8
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

Is it possible to restructure the non-linear activa-
tion functions in a deep network to create hardware-
efﬁcient models? To address this question, we propose a
new paradigm called Restructurable Activation Networks
(RANs) that manipulate the amount of non-linearity in mod-
els to improve their hardware-awareness and efﬁciency.
First, we propose RAN-explicit (RAN-e) – a new hardware-
aware search space and a semi-automatic search algo-
rithm – to replace inefﬁcient blocks with hardware-aware
blocks. Next, we propose a training-free model scaling
method called RAN-implicit (RAN-i) where we theoretically
prove the link between network topology and its expres-
sivity in terms of number of non-linear units. We demon-
strate that our networks achieve state-of-the-art results on
ImageNet at different scales and for several types of hard-
ware. For example, compared to EfﬁcientNet-Lite-B0, RAN-
e achieves a similar accuracy while improving Frames-Per-
Second (FPS) by 1.5× on Arm micro-NPUs. On the other
hand, RAN-i demonstrates up to 2× reduction in #MACs
over ConvNexts with a similar or better accuracy. We also
show that RAN-i achieves nearly 40% higher FPS than
ConvNext on Arm-based datacenter CPUs. Finally, RAN-i
based object detection networks achieve a similar or higher
mAP and up to 33% higher FPS on datacenter CPUs com-
pared to ConvNext based models1.

1. Introduction

Tremendous progress has been made towards build-
ing efﬁcient deep networks using either model compres-
sion [26,28,33], manual model design [39,45], or automatic
Neural Architecture Search (NAS)-based techniques [41,
51]. Despite these advances, signiﬁcant challenges re-
main in (1) hardware-aware model design especially for
AI accelerators like Neural Processing Units (NPUs) [1, 2],

*Equal Contribution (alphabetical order). †Work done while at Arm.
1The code to train and evaluate RANs and the pretrained networks are

available at this Github repository.

and (2) cost of ﬁnding optimized models for a given
#MACs/#parameters constraint once a good base model is
known. We discuss both of these challenges in detail below.

The ﬁrst challenge relates to the lack of good hardware-
aware building blocks. Speciﬁcally, even though excel-
lent NAS methods [4, 10, 32, 34, 41, 51–55, 58] exist to ﬁnd
highly efﬁcient deep networks within large search spaces,
there has been limited focus on building a hardware-aware
search space itself, particularly for AI accelerators. Most
search spaces for computer vision tasks rely on Inverted
Bottleneck (IBN) blocks – the main building blocks used
in MobileNet-V2 [45] and EfﬁcientNet [51] – since they re-
sult in highly accurate yet compact models. Existing NAS
works search over number of channels, expansion ratio, and
kernel sizes for IBN blocks [10, 51]. However, it has been
established that while IBN blocks are great for generic pro-
cessors like phone CPUs, they are not always well-suited
for AI accelerators due to poor utilization of the accelera-
tor hardware [58, 63]. To address this, recent NAS tech-
niques use fused convolutions [58, 63] which combine the
ﬁrst two layers of the IBN to form a large, regular convo-
lution. This leads to layers that do not present hardware
utilization issues but are computationally very expensive in
terms of #MACs/#parameters. Hence, a new search space
is needed which contains blocks that (a) enable hardware-
aware (i.e., high hardware utilization) models for AI accel-
erators with low #MACs while achieving high accuracy, and
(b) are accompanied by a simple search algorithm.

The second challenge relates to the fact that even if a
good model has been designed (either using NAS or man-
ually), it is still very costly to scale it up or down to sat-
isfy various #MACs/#parameters constraints. For example,
some existing works perform model scaling using ad-hoc
methods, which can result in suboptimal networks (e.g.,
in ConvNexts [39], the number of blocks is scaled from
[3,3,9,3] for ConvNext-Tiny to [3,3,27,3] for ConvNext-
Small model without an explanation). Other methods rely
on extremely costly EfﬁcientNet-like NAS [51] to ﬁnd op-
timal width and depth scaling. Therefore, more focus is re-
quired on inexpensive model scaling techniques that result

 
 
 
 
 
 
in high accuracy.

In this paper, we propose a new paradigm to create
hardware-efﬁcient deep networks. Speciﬁcally, we show
that manipulating the amount of non-linearity in deep net-
works can be a new way to achieve hardware-awareness
and/or signiﬁcant reduction in computational cost. Non-
linear activation functions have been a fundamental part of
deep neural networks since their inception. While many
advanced activation functions have been proposed in liter-
ature [27, 29, 43], several important questions remain un-
addressed. For instance, how much non-linearity can we
remove from a network without signiﬁcant accuracy loss?
Activation functions have been viewed as cheap operations
in deep learning from a computational cost standpoint. Con-
sequently, they have not been used to build efﬁcient deep
networks in prior work. In view of the above challenges,
we ask the following key questions in this paper:

1. Is it possible to manipulate the non-linearities in a deep
network to create accelerator-hardware-aware models?

2. Given a good base model, can changing the amount
of non-linearity in a network allow us to quickly scale
it up or down to any target resource constraints in a
training-free way to obtain highly accurate models?

To address the above questions, we propose Restruc-
turable Activation Networks (RANs). Our approach is
based on explicit and implicit restructuring of the non-linear
activation functions to improve the model efﬁciency while
still achieving high accuracy. Speciﬁcally, for the ﬁrst ques-
tion, we propose a search space that contains new building
blocks that can restructure IBN blocks into small, regular
convolutions to generate hardware-aware networks. This is
highly useful to improve the hardware utilization (e.g., for
AI accelerators) without increasing #MACs/#parameters
signiﬁcantly. Since the amount of non-linearity and struc-
ture of the model explicitly changes with this technique, we
call these models RAN-e (RAN-explicit).

For the second question, we look into a recent study that
theoretically analyzes the topological properties of deep
networks and shows how accuracy of different models is
related to their structural characteristics (e.g., presence of
skip connections, etc.) [7]. Since no training is required to
evaluate these topological properties, we use this method to
scale a given base model in a training-free way. We also
show that for a certain class of networks, such topological
properties are related to the total number of non-linear units
in a network. Therefore, changing the topological struc-
ture of networks also impacts the amount of non-linearity
and, thus, affects the expressivity of deep networks. Hence,
we exploit the metrics in [7] to scale ConvNext class of
models and show that they can be scaled in a signiﬁcantly
better way than the ad-hoc method used in ConvNext [39].

Since our method results in an implicit restructuring of non-
linearity, we call these models RAN-i (RAN-implicit).

We emphasize that our work is not a full-blown NAS.
The objective of this paper is to (a) demonstrate the power
of manipulating the amount of non-linearity in networks to
create hardware-efﬁcient models, and (b) highlight the ef-
fectiveness of a new search space that comes with its own
lightweight search algorithm towards building accelerator
hardware-aware networks. As such, unlike the majority of
the NAS literature, we do not focus on building the most
effective search algorithm. That is, our lightweight search
technique is limited only to the proposed blocks within our
new search space, is often semi- (and not fully-) automatic,
and does not search over important factors like number of
channels, number of blocks, kernel sizes, expansion ratios,
etc. Nevertheless, we demonstrate that our search space and
preliminary algorithm result in highly accurate models that
perform extremely well on NPUs. Hence, the scope of this
work is to design only the new search space that can result in
more hardware-aware models. Integrating this search space
into a full-blown NAS is left as a future work.

We make the following key contributions in this work:

1. We propose Restructurable Activation Networks
(RANs), a new paradigm that improves hardware ef-
ﬁciency of deep networks by manipulating the amount
of non-linearity in the models.

2. We ﬁrst create RAN-explicit (RAN-e) models that rely
on a new search space and result in high accuracy
and signiﬁcantly improved accelerator hardware uti-
lization without increasing MACs. We then create
RAN-implicit (RAN-i) models that scale existing base
models like ConvNexts in a training-free way to satisfy
certain #MACs/#parameters. We also present an initial
attempt at co-designing a restructurable block with its
own activation function for ConvNext networks.

3. While RAN-e results in explicit changes in model
structure with direct non-linear unit manipulation,
RAN-i modiﬁes the topology (depth/width) of the base
model and, hence, implicitly also changes the amount
of non-linearity. Towards this implicit non-linearity
manipulation, we theoretically prove the link between
the topological metric in [7] and expressivity of deep
networks like ResNets and ConvNexts.

4. Finally, we achieve state-of-the-art results on Ima-
geNet at several #MACs/#parameters scales and for
multiple types of hardware ranging from micro-NPUs
to datacenter CPUs. RAN-e leads to 1.5× higher FPS
than EfﬁcientNet-Lite-B0 on an Arm micro-NPU with
a similar accuracy. Also, RAN-i outperform Con-
vNexts by nearly 2× fewer MACs with a minor drop in
accuracy (∼ 0.2%). We also achieve up to 40% higher

Figure 1. (a) Two blocks: BatchNorm (BN)→ReLU→Conv and a standard Inverted Bottleneck (IBN) block. Even if the #MACs and
#parameters are similar between these two blocks, the IBN block has many more non-linear units than BN→ReLU→Conv. Indeed, the
IBN block is much more expressive than a regular convolution layer which is also clear from the high accuracy achieved by the IBN-based
models. (b) We formulate the problem of how much non-linearity we can remove from a network as a search between the blocks in (a).

FPS than ConvNext on an Arm-based datacenter CPU.
When used as backbones in object detection, RAN-i
achieve a similar or higher mAP with 33% higher FPS
on datacenter CPUs compared to ConvNexts.

The paper is organized as follows. The RAN-e models
are proposed in Section 2 along with their results. Section 3
proposes the RAN-i model scaling and shows its effective-
ness. Section 4 demonstrates an initial attempt towards co-
designing the restructurable blocks with a new activation
function. After some discussion on future directions in Sec-
tion 5, we review the related work in Section 6. Finally, we
conclude the paper in Section 7.

2. RAN-explicit and New Search Space

In this section, we propose RAN-explicit, a new class
of models whose architecture can be restructured by ma-
nipulating the amount of non-linearity in the network. We
accomplish this by proposing a search space that contains
new blocks. We start by formulating the problem below.

2.1. Problem Formulation

How much non-linearity can we remove from a deep net-
work without losing signiﬁcant accuracy? To address this
question, we ﬁrst create a setup that can allow us to system-
atically experiment with the amount of non-linearity in a
given model. To this end, consider the two standard blocks
shown in Fig. 1(a): BN→ReLU→Conv and IBN. Assuming
both blocks receive a feature map of same height and width
(H × W ), the number of MACs for BN→ReLU→Conv =
H × W × 3 × 3 × m × m = 9m2HW . For the IBN blocks,
ignoring the MACs in depthwise layers2 and assuming the

2Depthwise layer has far fewer MACs than pointwise 1 × 1 layers.

expansion ratio e = 6 (similar to [45, 51]), the number of
MACs for IBN = H × W × [1 × 1 × n × 6n + 1 × 1 × 6n × n]
= 12n2HW . With a simple calculation, it is easy to see
3)n ≈ 1.155n, #MACs/#parameters for
that if m = (2/
BN→ReLU→Conv and IBNs are equal.

√

Even if a regular BN→ReLU→Conv layer has similar
#MACs/#parameters as an IBN block, it is well known that
IBN blocks achieve signiﬁcantly higher accuracy [45]. This
is because the IBN blocks have a much higher expressivity
than the regular convolution layers due to a large number
of non-linear units in IBN [40, 42]. Speciﬁcally, for our ex-
ample above (see Fig. 1(a)), the total number of non-linear
ReLU units in BN→ReLU→Conv = m, whereas the IBN
has 6n+6n = 12n ReLU units. Hence, even if m = 1.155n
(so that both IBN and regular convolution layer have similar
#MACs/#parameters), the IBN has more than 10× higher
number of non-linear units than BN→ReLU→Conv. This
results in better expressivity of IBN-based models. Note
that, for AI accelerators, a regular convolution executes
much faster than an IBN block due to a signiﬁcantly better
hardware utilization [58, 63], especially if they have simi-
lar #MACs/#parameters. Therefore, it is still preferable to
have some regular convolution layers in the model.

Based on the above observations, we formulate the
task of how much non-linearity can be removed from
a network as a novel search problem that chooses be-
tween low non-linearity blocks like regular convolutions
(e.g., BN→ReLU→Conv) and high non-linearity blocks
like IBNs (see Fig. 1(b)). Hence, our search problem is:

L(y, f (θ, α)) + λ||I(α)||0,

(1)

min
θ,α

where, L is the cross-entropy loss, y is the true label for
the given classiﬁcation task, f is the function represented
by the SuperNet created using the search space, θ are the

AvgPoolBlock 1Block 2Block 3Block iBlock N-2…Block N224×224…1x1 Conv……Class: CAR1000Block N-1…How to select between IBN block and regular convolutions?3x3 Conv1x1 Conv1x1 Conv3x3 DSConvBatchNorm(BN) àReLUàConvk×kConvk×kDSConvFeature MapRegular k ×k ConvBN àReLUDepthwisek ×k ConvInverted Bottleneck (IBN) Blocka.b.Legendminput channelsmoutput channelsninput channelse.nintermediate channelsnoutput channelsstructure with two PReLUs. For simplicity, we remove the
non-linear activation function after the ﬁrst 1 × 1 (expan-
sion) layer. Also, the ﬁrst PReLU appears before the ﬁrst
1 × 1 convolution and uses (1 − α) as its trainable parame-
ter. On the other hand, the second PReLU uses (α) param-
eter and appears after the depthwise separable convolution
(DSConv). Both PReLUs share the same α value.

The PReLU activation function is very interesting be-
cause of its trainable α parameter. Speciﬁcally, as shown
in Fig. 2(b), if α = 0, it behaves as a ReLU and if α = 1,
it becomes linear (y = x), i.e., no activation. Therefore, if
we control the trainable parameter α for the PReLU, we can
use it to systematically remove the non-linear units from a
network and analyze its impact on accuracy. In other words,
using AFRB in our search space allows us to prune out the
non-linear units from a network in a fully trainable way.

Let us now examine how AFRB enables an explicit re-
structuring of the blocks. In Fig. 2(a), if α = 0, the ﬁrst
PReLU becomes linear and gets removed, while the sec-
ond PReLU becomes a ReLU that appears after the DSConv
layer. Clearly, this resembles an IBN except for the miss-
ing ReLU after the expansion layer (see Fig. 2(c)); we can
easily bring back that ReLU once the search phase is over,
i.e., during the ﬁnal training of the searched subnetwork.
In contrast, if α = 1 in Fig. 2(a), the ﬁrst PReLU be-
comes a ReLU and the second PReLU becomes linear (see
Fig. 2(d)). In this case, after the very ﬁrst ReLU, there are
no more non-linear units in the block. That is, the three
layers (1 × 1 expansion layer, DSConv, 1 × 1 projection
layer, along with their BatchNorms) are all linear opera-
tions. We know from prior art [8, 22], these linear layers
can be analytically collapsed into a single regular convolu-
tion. Hence, for α = 1, the AFRB block restructures into a
BN→ReLU→Conv block.

Therefore, AFRB is a unique building block that pro-
motes an explicit trainable restructuring of entire oper-
ations and directly chooses between an IBN block or a
BN→ReLU→Conv. Hence, an AFRB-based search space
encourages the discovery of completely new kinds of deep
networks. We next discuss the direct implications of our
block from a hardware perspective.

Hardware Advantages. The restructuring of AFRB from
IBN to BN→ReLU→Conv reduces the computational and
memory costs and also improves the hardware utilization
on AI accelerators.
Speciﬁcally, as mentioned earlier,
the #MACs for IBN in Fig. 2(c) = 12n2HW , whereas
for BN→ReLU→Conv in Fig. 2(d) = 9n2HW .
that
Hence, the restructuring directly results in 25% savings
in #MACs/#parameters. Since the regular convolutions
do not have utilization issues on AI accelerators, these
lower MACs execute at much higher rate on the accelerator,
thereby signiﬁcantly boosting the hardware performance.

Figure 2. (a) Proposed Activation Function Restructuring Block
(AFRB) consists of an IBN structure with PReLU activation func-
tions. The ﬁrst PReLU uses (1 − α) and the second one uses α pa-
rameter (same α is shared between the two PReLUs). (b) PReLU
activation function: if α = 0, it becomes ReLU, and if α = 1, the
(c, d) For α = 0
function becomes linear (i.e., no activation).
(α = 1), AFRB becomes an IBN-like (a BN→ReLU→Conv)
block. Thus, AFRB can restructure an IBN block into a regular
convolution by removing non-linear units from the network. This
is accomplished using a single scalar trainable α parameter.

weight parameters of the SuperNet, and α are the param-
eters that select between low and high non-linearity blocks
inside the SuperNet (e.g., BN→ReLU→Conv vs. IBNs).
The indicator function I produces a vector whose elements
are 1 if a non-linear unit is present, and 0 otherwise; thus,
the l0 norm of this indicator function quantiﬁes the num-
ber of non-linear units in the model. Finally, λ is a hyper-
parameter that controls the contribution of the second loss.
Therefore, the goal of the above problem is to minimize the
cross-entropy loss during training and also reduce the total
number of non-linear units in the network.

A standard way to solve problem (1) can be using any
NAS algorithm including differentiable NAS methods like
DARTS [37]. Towards this, a traditional SuperNet to se-
lect between IBN and regular convolution layer would in-
volve putting IBNs and regular convolutions as branches
and then an α parameter can select among those options.
However, this SuperNet is computationally expensive and
requires high GPU memory due to multiple branches. To
overcome these issues, we introduce new Activation Func-
tion Restructuring Blocks (AFRB) that do not require mul-
tiple branches (and, hence, are low cost), and enable us to
study the non-linearity problem in a very systematic way.

2.2. New Activation Function Restructuring Blocks

In this section, we propose our Activation Function Re-
structuring Blocks (AFRB) that automatically restructure
IBNs into small, regular 3 × 3 convolutions. Fig. 2(a) il-
lustrates the proposed AFRB that consists of an IBN-like

y= xy= α.xα=0 (IBN)Linearα=0α=1BNBN+PReLU(α)1x1 Conv1x1 Conv3x3 DSConvBN+PReLU(1-α)BNninput channelsnoutput channelsBNBN+ReLU1x1 Conv1x1 Conv3x3 DSConvBNBNnnBNBN1x1 Conv1x1 Conv3x3 DSConvBN+ReLUBNnnBatchNorm(BN) àReLUàConva.3x3 Convninput channelsnoutput channelsBN+ReLUIBN-like blockb.c.Activation Function Restructuring BlockPReLUActivation Functiond.α=1 (Regular Conv)ReLUAFRBFigure 3. Proposed search space: (a) AFRB-1 is used when residual is not present, i.e, if there is a feature map downsampling with stride>1
or if #input and #output channels are different. (b) AFRB-2 is used when a residual can be present (#input channels = #output channels
and stride=1). (c) AFRB-3 can collapse into a residual like block (with half intermediate channels, unlike standard residual blocks). This
makes the #MACs/#parameters for AFRB-3 same as AFRB-1 but with additional expressivity (i.e., more non-linear units than AFRB-1).

2.3. Proposed Hardware-Aware Search Space

We now exploit AFRB to create a novel search space that
will be used to generate accelerator hardware-aware mod-
els. Fig. 3 shows different blocks used to create our Super-
Net. As evident, we use three kinds of blocks: (1) AFRB-
1 is used when #input channels are different from #output
channels or if there is a stride to downsample feature maps;
(2) AFRB-2 is used when a valid residual skip connection
can be added to the block (i.e., #input channels = #output
channels and stride = 1); (3) AFRB-3 can collapse into a
residual-like block if α = 1. Also, AFRB-3 reduces the
number of intermediate channels to half the input channels.
The idea here is to increase the number of non-linear units
while still keeping #MACs/#parameters the same as AFRB-
1. A search over the SuperNet containing the above blocks
results in accelerator hardware-aware deep networks.

2.4. Lightweight Search Algorithm

Let us now explain how to search using our proposed
blocks. We ﬁrst create a SuperNet using AFRBs where each
block i (see Fig. 1(b)) has its own αi parameter. Starting
with problem (1), it is easy to see that the trainable αi pa-
rameters make it possible to pick between low non-linearity
blocks (BN→ReLU→Conv) and high non-linearity blocks
(IBN). The search problem now becomes:

L(y, f (θ, α)) + λ||I(α ∈ {0, 1})||0,

(2)

min
θ,α

where, the indicator function I now takes non-zero values
only if αi = 0 or αi = 1 for each block i in the network.
In practice, we relax this problem further by trying to make
αi = 1 where possible and not putting any constraint to
make αi = 0. That is, instead of searching between linear
(no activation, αi = 1) or ReLU (αi = 0) in problem (2),

we use the following loss to perform a binary search be-
tween linear (αi = 1) or non-linear (αi (cid:54)= 1):

min
θ,α

L(y, f (θ, α)) + λ||α − 1||2
2,

(3)

where, 1 is a vector of all 1’s. The above objective func-
tion aims to minimize the cross entropy loss while making
as many αi = 1 as possible. The blocks where αi (cid:54)= 1 are
assumed to be high non-linearity blocks like IBNs. There-
fore, minimizing problem (3) can directly restructure some
of the IBNs into regular convolutions.

We make a clear distinction between our search phase
and ﬁnal ﬁnetuning phases of the initial searched model
(in terms of ﬁnal minor changes to the model architecture).
Again, since our search problem only focuses on whether
we can remove non-linear units from each block or not, we
do not conduct a full-blown NAS in this work. Speciﬁcally,
number of channels, expansion ratios, number of blocks,
etc., are all decided manually when designing the SuperNet.
Our goal is to start with this given SuperNet and make it
accelerator-hardware-aware without losing signiﬁcant ac-
curacy and not to build the most effective search algorithm
over traditional factors like number of channels, expansion
ratios, number of blocks, etc. To this end, our search will
be semi-automatic, and we will show how each design deci-
sion affects the quality of the model. Nevertheless, we will
demonstrate the effectiveness of the proposed search space
and our lightweight search algorithm in creating highly ef-
ﬁcient deep networks. In the next section, we present this
semi-automatic process and show the results on ImageNet.

2.5. RAN-e: ImageNet Evaluation

We now exploit our proposed search space to create
state-of-the-art networks for the ImageNet image classiﬁ-
cation task. Fig. 4(a) shows the structure of our SuperNet

BNBN+ReLU1x1 Conv1x1 Conv3x3 DSConvBNBNmnα=0α=1BNBN+PReLU(α)1x1 Conv1x1 Conv3x3 DSConvBN+PReLU(1-α)BNminput channelsnoutput channelsa.3x3 Convminput channelsnoutput channelsBN+ReLUAFRB-1BNBN+ReLU1x1 Conv1x1 Conv3x3 DSConvBNBNnnα=0α=1BNBN+PReLU(α)1x1 Conv1x1 Conv3x3 DSConvBN+PReLU(1-α)BNninput channelsnoutput channelsb.3x3 Convninput channelsnoutput channelsBN+ReLUAFRB-2+++c.AFRB-3+BN+PReLU(α)1x1 Conv1x1 Conv3x3 DSConvBN+PReLU(1-α)BNninput channelsn/2output channelsBNBN+PReLU(α)1x1 Conv1x1 Conv3x3 DSConvBN+PReLU(1-α)BNn/2input channelsnoutput channelsBNBN+ReLU1x1 Conv1x1 Conv3x3 DSConvBNBNnn/2α=0+BNBN+ReLU1x1 Conv1x1 Conv3x3 DSConvBNn/2nα=13x3 Convninput channelsn/2intermediate channelsBN+ReLU+3x3 Convnoutput channelsRESIDUAL-LIKE BLOCKFigure 4. (a) PReLU SuperNet shows the SuperNet structure in terms of AFRB blocks. A1, A2, A3 stand for AFRB-1, AFRB-2, and
AFRB-3, respectively. The search is conducted over these blocks to see which of them can be restructured into regular convolutions.
(b) SuperNet (all IBNs) consists of 1 regular stem convolution and 17 IBN blocks with 3 × 3 kernel sizes (“3” in 3-R or 3-I denotes kernel
size). (c) SubNetwork-A: The PReLU-based accuracy-driven search reveals that 5 out of 17 blocks can be made regular convolutions
without signiﬁcant accuracy loss. (d) SubNetwork-B: The ﬁrst two IBNs are very heavy in MACs due to large feature maps and expansion
ratio e = 6; these blocks are a very natural choice to be collapsed to regular convolutions (on top of SubNetwork-A) to improve hardware
performance with some accuracy drop. (e) SubNetwork-C: We now modify the kernel sizes on some of the IBNs (since the depthwise
MACs are still low) to recover the lost accuracy. (f) Ground Truth: Manual model designed on the SuperNet performs even better than
SubNetwork-C (same accuracy and even better hardware performance), thus, highlighting the importance of the proposed search space.

Table 1. Results on ImageNet (100 epoch training) for the SuperNet and different RAN-e networks (structures shown in Fig. 4). These
models are suitable for microcontroller- and mobile-scale AI accelerators. Green/Red indicate improvements or worsening of various
metrics compared to the original EfﬁcientNet-Lite-B0 (with ReLU6) [21]. Normalized throughput is Frames Per Second (FPS) normalized
w.r.t. EfﬁcientNet-Lite-B0 estimated for Arm Ethos-U55 (M-class) and Arm Ethos-U65 (A-class) microNPUs.

100 epoch training

#Parameters
(in Millions)

#MACs
(in Millions)

Top-1
Accuracy

EfﬁcientNet-Lite-B0 [21] (original with ReLU6)
EfﬁcientNet-Lite-B0 (with H-Swish)
SuperNet (all IBNs)
SubNetwork-W (reduced width SuperNet, all IBNs)
RAN-e SubNetwork-A (PReLU search, trained with H-Swish)
RAN-e SubNetwork-B (SubNetwork-A + ﬁrst 2 IBNs collapsed)
RAN-e SubNetwork-C (SubNetwork-B + new kernel sizes)
RAN-e Ground Truth (manual model within same SuperNet)

4.7M
4.7M
4.7M
4.13M
4.6M
4.6M
4.7M
4.5M

385M
385M
590M
488M
561M
482M
488M
433M

72.81%
73.82% (+1.01%)
74.24% (+1.43%)
73.75% (+0.94%)
73.26% (+0.45%)
72.63% (−0.18%)
73.13% (+0.32%)
73.04% (+0.23%)

Normalized Throughput (FPS)
Ethos-U65
Ethos-U55
A-class systems
M-class systems
1×
1×
1×
1×
0.87×
0.86×
0.90×
0.91×
0.91×
0.88×
1.34×
1.10×
1.28×
1.06×
1.49×
1.16×

in terms of the location and types of AFRBs. The complete
details of the SuperNet (e.g., strides, channel counts, ex-
pansion ratios, etc.) are shown in Table 5 in Appendix A.
Note that, our SuperNet uses only 3 × 3 kernel sizes for all
blocks (see green “3-R” blocks for regular convolutions or
blue “3-I” blocks for IBNs in Fig. 4(b)). As we go through
our semi-automatic search process, we will make a num-
ber of simple ﬁnetuning steps (including kernel sizes) to ar-
rive at the ﬁnal architecture. More details about the training
setup for RAN-e are given in Appendix B.

Table 1 shows the #parameters, #MACs, accuracy, and
normalized throughput (FPS) of SuperNet and various
RAN-e models w.r.t. EfﬁcientNet-Lite-B0 [21]. The nor-
malized throughput is obtained using performance estima-
tors for Arm Ethos-U55 [1] (with M-class system conﬁgu-
ration) and Arm Ethos-U65 [2] (with A-class system con-
ﬁguration) microNPUs3. All models in Table 1 use Hard-

3The performance estimator for Ethos-U55 is available at: https:
//git.mlplatform.org/ml/ethos-u/ethos-u-vela.git/
about/. The performance estimator for Ethos-U65 is proprietary.

Swish (H-Swish) [29] activation function unless indicated
otherwise. Finally, Table 1 results are only for 100 epoch
training. We will present 350 epoch training results later.

As evident from Table 1, our SuperNet (assuming all
blocks are selected as IBNs, see Fig. 4(b)) achieves 74.24%
top-1 accuracy on ImageNet in 100 epochs with 4.7M pa-
rameters and 590M MACs. Our SuperNet is slower than
EfﬁcientNet-Lite-B0, e.g., 0.87× the FPS of EfﬁcientNet-
Lite-B0 on Ethos-U65. This is because the SuperNet in-
curs a signiﬁcantly heavier MAC cost (590M vs. 385M)
while still only using IBNs in its architecture which present
hardware utilization issues for NPUs at some of the lay-
ers. Staying within the same IBN-based search space, we
further created SubNetwork-W which is simply a reduced
width version of the SuperNet (width multiplier = 0.91×,
see Table 1). Clearly, this model achieves a similar accuracy
as EfﬁcientNet-Lite-B0-H-Swish (around 73.8%). Again,
since this model also uses IBNs only, even with 17% lower
MACs than the SuperNet (488M vs. 590M), SubNetwork-
W achieves only 3% boost in normalized FPS (0.9× vs.

3-R3-I3-I3-I3-I3-I3-I3-I3-I3-I3-I3-I3-I3-I3-I3-I3-I3-Ib.SuperNet(all IBNs):Top3-R3-I3-I3-I3-R3-I3-R3-I3-R3-I3-I3-I3-R3-I3-I3-R3-I3-Ic.SubNetwork-A:Top3-R3-R3-R3-I3-R3-I3-R3-I3-R3-I3-I3-I3-R3-I3-I3-R3-I3-Id.SubNetwork-B:Top3-R3-R3-R3-I3-R5-I3-R5-I3-R3-I3-I3-I3-R7-I3-I3-R5-I5-Ie.SubNetwork-C:Top3-R3-R3-R3-R3-R5-I5-I3-I3-I3-R3-R3-R7-I3-I3-I3-I5-I5-If.Ground Truth:Top3-RA1A1A1A1A1A2A1A2A1A3A1A1A2A2A1A2TopPReLUSearch Spacea.PReLUSuperNet:0.87×) on Ethos-U65. Therefore, hardware utilization is-
sues make it very difﬁcult to improve FPS even if MACs are
greatly reduced. Hence, our objective is to make the Super-
Net accelerator-hardware-aware using our proposed AFRB
blocks and the semi-automatic search algorithm.

2.5.1 The Initial Search

We now perform the search, i.e., problem (3) on the PReLU
SuperNet shown in Fig. 4(a). The λ hyperparameter in
problem (3) controls the tradeoff between accuracy and the
amount of non-linearity pruning. We performed a search on
λ ∈ (6 × 10−4, 2 × 10−3) and found that λ = 1 × 10−3
results in minimal accuracy drop (∼ 1%) over the SuperNet
while collapsing 5 out of 17 IBN blocks into regular convo-
lutions. We call this model SubNetwork-A (see Fig. 4(c)).
Recall that, our search only chooses between α = 1 (linear)
and α (cid:54)= 1 (non-linear). The linear case automatically re-
structures to a regular convolution (see Fig. 3), whereas we
set all the high non-linearity α (cid:54)= 1 blocks as IBNs. Since
the ﬁrst activation function is missing in AFRB blocks (see
Fig. 3), we bring it back for all α (cid:54)= 1 blocks and retrain the
models from scratch once the search phase is complete4.
Table 1 shows that SubNetwork-A achieves 1% lower ac-
curacy than the all-IBN SuperNet, reduced the #MACs by
29M, and slightly improves the normalized FPS from 0.87
to 0.91 on Ethos-U65.

Clearly, the performance is not signiﬁcantly better than
the SuperNet. This is because we have only conducted an
accuracy-driven search and the total number of MACs is
still very high compared to the SuperNet (561M vs. 590M).
Hence, we next ﬁnetune the SubNetwork-A architecture ob-
tained with our initial PReLU search.

2.5.2 Finetuning Stage 1: Collapse High MAC Blocks

It is easy to see that our search algorithm chooses to not re-
structure some MAC-heavy blocks, e.g., the ﬁrst two IBNs
in Fig. 4(c). This decision by our PReLU search algorithm
makes intuitive sense. Having powerful initial blocks can
have a signiﬁcant impact on accuracy. Since, unlike most
hardware-aware NAS works [10, 51], we have not done any
MAC- or latency-aware search, the objective in problem (3)
attempts to maximize only the accuracy without trying to
reduce MACs. This is why it chooses to keep the ﬁrst two
blocks as IBNs even if they are very expensive in MACs.
Hence, as our ﬁrst ﬁnetuning stage, we directly restruc-
ture the ﬁrst two IBN blocks – two of the highest MAC

4SubNetwork-A (and others) use H-Swish everywhere during retrain-
ing and not PReLU. The PReLU is used only during the search phase.
In practice, for the automatically restructuring (α = 1, linear) case, the
PReLU search yields α values close to 1 and not exactly 1 when we solve
problem (3), e.g., α’s are within a reasonable boundary like [0.8, 1.3].
When we train the searched SubNetwork-A from scratch, we replace the
AFRBs with α ∈ [0.8, 1.3] with regular convolutions.

blocks due to large feature map sizes and high expansion
ratios (e = 6) – into regular convolutions. This results in
SubNetwork-B model (see Fig. 4(d)).

Table 1 demonstrates that SubNetwork-B immediately
reduces about 80M more MACs over SubNetwork-A and
collapsing these two IBN blocks leads to a top-1 accu-
racy of 72.63% (about 0.6% lower than SubNetwork-A).
However, because these newly restructured regular convo-
lutions execute on the AI acclerators without hardware uti-
lization issues, the normalized FPS greatly increases from
0.91 to 1.34 on Ethos-U65 (about 1.47× increase compared
to SubNetwork-A). This also highlights the power of our
hardware-aware search space: Even with similar MACs as
SubNetwork-W which uses only the traditional IBNs, our
model has 1.49× better FPS than SubNetwork-W.

2.5.3 Finetuning Stage 2: Change Kernel Sizes

Inevitably, the accuracy drops when we collapse the ﬁrst
two IBNs in the last ﬁnetuning stage. To make up for this
lost accuracy, recall that it is very common to use kernel
sizes larger than 3 × 3 in modern NAS search spaces (e.g.,
EfﬁcientNet-Lite also uses 5 × 5 kernel sizes, etc.). There-
fore, in this ﬁnetuning stage, we increase kernel sizes to 5 ×
5 and 7 × 7 for a few IBN blocks. Since the depthwise con-
volutions in IBNs still incur a low MAC cost, this does not
increase the overall MACs signiﬁcantly. The new updated
model is called SubNetwork-C and its structure is shown in
Fig. 4(e). As evident from Table 1, SubNetwork-C recov-
ers nearly all of the accuracy to the level of SubNetwork-A
(73.13% vs. 73.26%) while still improving normalized FPS
from 0.91 to 1.28. SubNetwork-C is the last stage of our
semi-automatic search process and clearly demonstrates a
systematic way to go from a completely hardware-unaware
SuperNet to a very hardware-friendly SubNetwork.
In
summary, compared to EfﬁcientNet-Lite-B0 (original, with
ReLU6) trained on our 100 epoch setup, we achieve slightly
better accuracy and nearly 1.28× higher FPS.

2.5.4

Is this the best we can do?

So far, the process has been semi-automatic with most of
the design decisions being very straightforward after the ini-
tial search. We now evaluate if we can do any better with
a manual design, within the same search space and Super-
Net. To this end, we created a model shown in Fig. 4(f)
called the Ground Truth network. Table 1 demonstrates that
RAN-e Ground Truth achieves slightly better accuracy than
EfﬁcientNet-Lite-B0 (ReLU6) with nearly 1.5× higher FPS
which is even better than that for the SubNetwork-C.

We call this manual model the Ground Truth network be-
cause ideally we expect the search algorithm to discover this
or a better network on its own. However, since our semi-
automatic algorithm produced a sub-optimal network (even

Table 2. ImageNet results. †RepVGG and ResNet accuracies are
taken directly from [15] which were trained to 120 epochs. RAN-
e accuracies for 100 epoch experiments (Table 1) are higher than
ResNet and RepVGG below. ‡EfﬁcientNet-Lite-B0 as reported
by [21] using a different training recipe. The remaining RAN-e
and EfﬁcientNet-Lite-B0 are trained to 350 epochs on our setup.

Params MACs

Top-1

ResNet-18† [15, 24]
RepVGG-A0† [15]
EffNet-Lite-B0-R6‡ [21]
EffNet-Lite-B0-R6
EffNet-Lite-B0-HS
RAN-e-C (Ours)
RAN-e-GT (Ours)

71.2%
11.7M 1.8B
72.4%
1.4B
8.3M
4.7M 385M 75.1%
4.7M 385M 74.4%
4.7M 385M 75.6%
4.7M 488M 74.6%
4.5M 433M 74.6%

Normalized FPS

Ethos-U55
M-class
0.46×
0.72×
1×
1×
1×
1.06×
1.16×

Ethos-U65
A-class
0.48×
0.69×
1×
1×
1×
1.28×
1.49×

if it is 1.28× better than a very strong baseline), it highlights
the current limitations of the search process, i.e., lack of a
full-blown NAS and no MAC-aware losses. It is possible
that with a complete NAS (e.g., channel counts, expansion
ratios, kernel sizes, strides, etc.) on the AFRB-based search
space, along with explicit MAC-driven losses, the search al-
gorithm may produce an even better network than the RAN-
e Ground Truth model. Integrating AFRBs into a full NAS
and MAC-constraints is left as a future work.

2.5.5 Comparison against reference models

The original EfﬁcientNet-Lite-B0 (ReLU6) is used only as
a reference in the previous sections to show that AFRB-
based search space can come up with competitive mod-
els. For a fair comparison with our models (that use H-
Swish), Table 1 also presents the accuracy for EfﬁcientNet-
Lite-B0 (H-Swish) trained on our setup. As evident, our
SubNetwork-C (RAN-e-C) and Ground Truth (RAN-e-GT)
networks are within 1% accuracy of EfﬁcientNet-Lite-B0
(H-Swish) while improving FPS by up to 1.28×-1.5×.
Note that, EfﬁcientNet-Lite-B0 was obtained using a full-
blown NAS that searched over number of blocks, channel
counts, expansion ratios, and kernel sizes. Again, because
our search is not a complete NAS, better networks in AFRB
search space could have been potentially obtained if we
had also searched over width, depth, expansion ratios, etc.
More interestingly, the FPS gain in RAN-e is despite the
fact that both SubNetwork-C and Ground Truth networks
require more MACs than EfﬁcientNet-Lite-B0. This is per-
haps a surprising result because most of the prior art tries to
minimize the #MACs/#parameters to obtain efﬁcient mod-
els. Therefore, a hardware-aware search space can signiﬁ-
cantly boost performance even with slightly higher MACs.
Next, Table 2 shows comparisons against a few exist-
ing baselines that also rely on regular convolutions, e.g.,
ResNets [24] and RepVGG [15]. We also train our RAN-e
networks and EfﬁcientNet-Lite-B0 (ReLU6 and H-Swish)

to 350 epochs and report their accuracy. As evident from
Table 2, while models like ResNet-18 and RepVGG-A0
use only 3 × 3 convolutions, they result in extremely com-
pute intensive models. Speciﬁcally, compared to RAN-e-
GT (Ground Truth), ResNet-18 and RepVGG require 4.15×
and 3.23× more MACs, respectively. The accuracy for
these models reported in Table 2 is taken directly from [15]
which trains them to 120 epochs only (without advanced
data augmentation techniques like Mixup [62]). We also do
not use Mixup in our experiments and our 100 epoch ac-
curacies in Table 1 already exceed ResNet-18 and RepVGG
accuracies. Furthermore, the normalized FPS on Ethos-U55
and Ethos-U65 clearly demonstrate the superiority of RAN-
e-GT compared to ResNet-18 and RepVGG (we achieve up
to 3.1× and 2.1× higher FPS, respectively). Note that, the
improvements for M-class Ethos-U55 are signiﬁcant but not
as much as the A-class Ethos-U65 because M-class systems
are at tiny microcontroller-scale and are limited in memory.
Finally, we have also reproduced EfﬁcientNet-Lite-B0
(ReLU6 (R6) and H-Swish (HS)) accuracies on our setup.
Again, while we are 1% away in top-1 accuracy com-
pared to EfﬁcientNet-Lite-B0-HS, we achieve about 1.5×
higher FPS. On the other hand, compared to the original
EfﬁcientNet-Lite-B0-R6 (trained on our 350 epoch setup),
our proposed RAN-e networks achieve about 0.2% higher
top-1 accuracy on ImageNet. Of note, with a different train-
ing recipe, [21] reports an accuracy of 75.1% for EffNet-
Lite-B0-R6 (compared to 74.4% for our setup). Therefore,
the accuracy for our models may improve even more if the
training recipe is optimized further.

Other Remarks. We emphasize that the quality of Super-
Net matters in terms of compute costs of different SubNet-
works. Speciﬁcally, even though both SubNetwork-W and
EfﬁcientNet-Lite-B0 (H-Swish) are based on IBNs-only
search space and achieve a similar accuracy, SubNetwork-
W requires 100M more MACs than EfﬁcientNet-Lite-B0
(see Table 1). Hence, if we had a better, more efﬁcient Su-
perNet (e.g., in terms of number of blocks, channel counts,
expansion ratios, stride locations, etc.), our results could
be improved further. This also highlights that (1) our new
hardware-aware search space will likely offer best results
when used in conjunction with full NAS, and (2) perhaps
newer, cheap activation functions can also be proposed in
future that are speciﬁcally designed to work with AFRBs to
obtain even higher accuracy (see discussion in Section 4).

Is it possible to manipulate the non-linearities in deep
networks to create accelerator-hardware-aware models? We
have demonstrated that an AFRB-based search space en-
ables us to accomplish this goal. The discussion so far
completes the proof-of-concept of our novel search space
and shows that RAN-explicit is a new direction to achieve
hardware-awareness. Next, we propose the implicit restruc-
turing of non-linearities to reduce compute cost of models.

3. RAN-implicit and Training-Free Scaling

Given a good base model, can changing the amount of
non-linearity in a network allow us to scale it up or down
in a training-free way to obtain highly accurate deep net-
works that satisfy speciﬁc #MAC/#parameter constraints?
To address this question, we ﬁrst revisit recent literature [7]
that links topological characteristics of deep networks (i.e.,
structural properties such as presence of skip connections,
etc.) with their gradient propagation and model perfor-
mance. We also brieﬂy review the literature that studies
expressivity of deep neural networks [40, 42, 46].

3.1. Preliminaries

We start by discussing the topological metric and the

main theoretical result in [7].

Deﬁnition 1 (NN-Mass [7]). NN-Mass is deﬁned as the sum
(over all Nb blocks) of product of total #input channels (ib)
at all layers in a block and Cell-Density (ρb).

m =

Nb(cid:88)

b=1

ib × ρb,

ρb =

#Actual Skip Connections
Total Possible Skip Connections
(4)

For DenseNet-type models5, NN-Mass was shown to be
related to the average degree, i.e., average number of con-
nections for each channel in the network – both short-range,
i.e., layer-by-layer connections, and long-range, i.e., skip
connections going across multiple layers. Speciﬁcally, for a
network with width w at all blocks, [7] proved that the aver-
age degree ˆk = w + m/2. Intuitively, [7] argues that if we
have two networks with similar average connectivity (e.g.,
same width and NN-Mass), then the amount of information
ﬂowing through them is constrained similarly. Therefore,
such topological constraints can also have a profound im-
pact on how learning happens in different networks.

Proposition 1 (NN-Mass vs. Dynamical Isometry [7]).
Given a deep linear DenseNet-type MLP network, the Lay-
erwise Dynamical Isometry (LDI) is deﬁned as the mean
singular value of
initialization.
Then, the LDI is bounded as follows:

layerwise Jacobians at

(cid:113)

qˆk −

√

qw ≤ E[σ] ≤

(cid:113)

qˆk +

√

qw,

(5)

where, ˆk = w + m/2, q is the initialization variance. If q =
1/ˆk, then the mean singular value of layerwise Jacobians
(LDI) is bounded near 1, i.e., the gradients ﬂow through the
network without ampliﬁcation or attenuation [31].

Proposition 1 shows that models with similar width
and NN-Mass have similar gradient ﬂow properties and,

5In [7], DenseNet-type networks contain concatenation-type skip con-

nections and the density of skip connections can be varied.

training convergence (irrespective of their depths,
thus,
#parameters, and #MACs) since their mean singular value
is bounded in a similar way. Therefore, models with simi-
lar width and NN-Mass can achieve a similar accuracy even
if they have highly different #parameters/#MACs/#layers.
Extensive empirical results were presented in [7] to demon-
strate the effectiveness of NN-Mass.

The existing gradient ﬂow-based theory in [7] shows the
relationship between model topology and training conver-
gence but does not say anything about expressivity of deep
networks. Understanding the expressivity of deep networks
is just as important as understanding their gradient proper-
ties [40, 42, 46]. One way to quantify expressivity of deep
networks is to count the number of linear regions that a
function represents. These deﬁnitions are given below:

Deﬁnition 2 (Linear Regions [40]). Given a function f , a
linear region is a maximal connected set of inputs x where
f is linear.

The number of linear regions can be found by counting
the number of unique activation patterns, e.g., how different
ReLU units are activating for different inputs [46]. That is,
counting the number of unique activation patterns quanti-
ﬁes how many different linear regions are contained in the
function represented by the given deep network. Therefore,
this can be directly used as a measure of expressivity of the
model. Mont´ufar et al. [40] provide an aysmptotic lower
bound on maximal number of linear regions as follows:

Proposition 2 (Linear Regions for ReLU Networks [40]).
Given an input x ∈ Rn0 and a rectiﬁer (ReLU) deep net-
work function f : Rn0 → Rn with n0 input neurons, L
layers with n neurons each (n ≥ n0), f can compute func-
tions with Ω((n/n0)(L−1)n0 × nn0 ) linear regions.

In the next section, we demonstrate for ResNet- or
ConvNext-type networks that NN-Mass is related to the ex-
pressivity of deep networks. This is particularly important
because while [7] theoretically analyzes DenseNet-type net-
works, it neither discusses why NN-Mass works for ResNets
and other models with residual additions, nor provides any
connection with the expressivity of deep networks.

3.2. Expressivity vs. NN-Mass

Bhardwaj et al. [7] empirically derive the NN-Mass ex-
pression for ResNet-type networks as shown in Fig. 5(a).
We also adapt the NN-Mass for ConvNexts (see Fig. 5(b)).
Note that, for models with a very uniformly repeating struc-
ture (e.g., residual blocks or ConvNext blocks), the interme-
diate width (w2) is generally related to the input channels
for the block (w1) as w2 = e · w1, where e is the expansion
or shrinking factor and is generally the same for all blocks
in the network. Therefore, for this special class of networks,
we have the following result:

NN-Mass directly impacts the number of linear regions and,
thus, the expressivity of this class of deep networks.

Next, we prove one more result on the relationship be-
tween NN-Mass and expressivity. Speciﬁcally, NN-Mass
has known limitations, e.g., depending on the difﬁculty of
the given task, the models need to be deep enough and wide
enough for NN-Mass to produce best results in practice6.
Proofs in [7] explicitly assumed conditions on depth while
deriving the results, so it is understood that the models need
to be deep. However, it is unclear why the width also im-
pacts the effectiveness of NN-Mass. We analyze this rela-
tionship between width and NN-Mass below.

Corollary 1 (NN-Mass matters less when width is low).
Suppose we are given a deep network with n0 input neu-
rons, L layers with n neurons each and repeating residual
addition skip connections. For such a network, NN-Mass
m = nL. Then, the function represented by this model
f : Rn0 → Rn can compute Ω((n/n0)(m−n) n0
n × nn0) lin-
ear regions. When the width is low (e.g., when n → n0),
model’s expressivity is mostly determined by its width.

Proof. Since all layers have the same width n, and because
the network consists of repeating residual blocks (similar to
Fig. 5(a)), the expansion/shrinking factor e = 1. Substitut-
ing this in Eq. (6), we obtain NN-Mass m = (cid:80)Nb
1 =
nL. For the remaining proof, we simply substitute L =
m/n in Proposition 2. Then, it is easy to see that if n → n0,
n/n0 → 1 and, thus, the term with NN-Mass does not con-
tribute to the maximal number of linear regions. Instead,
the bound is mostly dictated by the second term that de-
pends only on n. This explains why NN-Mass depends on
model width: When the width is low, NN-Mass matters less
and the expressivity depends more on width.

b=1 wb

Therefore, model topology not only impacts the gradient
properties as proved in [7], it also directly affects the ex-
pressive power of deep neural networks. Since computing
metrics like NN-Mass does not require any training or even
a single forward or backward pass, they can serve as excel-
lent training-free methods to search for high quality models.

3.3. Training-Free Scaling with NN-Mass

We now exploit NN-Mass for training-free model scal-
ing. Speciﬁcally, given a base model M and a few different
hardware constraints H = {H1, H2, . . . , Hp} (in terms of
#MACs/#parameters), the problem is to scale the model up
or down in a training-free fashion to ﬁnd high quality mod-
els for all constraints. Once an appropriate model is found
using this training-free search, it is trained to obtain the ﬁnal
model. Note that, all other conditions from the last section

6This was evident in some of the results presented in [7] (e.g., R2
for relationship between accuracy and NN-Mass increases with increas-
ing width, see Fig. 15 in Appendix H.4 of [7]). We saw similar patterns in
our own experiments as well.

Figure 5. (a) NN-Mass calculation for ResNet bottleneck block.
(b) NN-Mass calculation for ConvNext block. In both cases, the
solid skip connection is the one actually present in the blocks. The
dotted skip connections are shown as possible skip connections in
the blocks as required by the cell-density (ρb) deﬁnition. Since the
skip connections for both blocks involve channel-wise additions,
the solid skip connection supplies w1 links, i.e., the skip connec-
tion carries information from all w1 input channels (at ﬁrst layer);
similar ideas apply to possible (dotted) skip connections.

Proposition 3 (NN-Mass vs. Linear Regions). For mod-
els with residual additions and a uniform structure (e.g.,
ResNets/ConvNexts), NN-Mass is proportional to the total
number of non-linear units in the network. That is, if the
total number of non-linear units in the network = X , then
X ∝ m or X = k × m, where k is a constant. Then, upper
bound on maximal number of linear regions = 2X = 2km.

Proof. Assuming w2 = e · w1 for all blocks, NN-Mass for
ResNets (mR) is given by (see Fig. 5(a)):

mR =

Nb(cid:88)

b=1

(wb

1(1+2e))

(cid:18)

wb
1
(2 + e)wb
1

(cid:19)

=

(cid:18) 1 + 2e
2 + e

(cid:19) Nb(cid:88)

wb
1

b=1

(6)

Total number of non-linear units for ResNets (XR) is:

XR =

Nb(cid:88)

b=1

2wb

2 =

Nb(cid:88)

b=1

2e × wb

1 = 2e

Nb(cid:88)

b=1

wb
1

(7)

Therefore, XR ∝ mR or XR = kR × mR, where kR =
(2e(2 + e))/(1 + 2e). A similar calculation for ConvNext
class of networks reveals that the total number of non-linear
units for ConvNexts (XC) is also proportional to its NN-
Mass (mC), i.e., XC = kC ×mC, where kC = (3e/(2+e)).
As a result, for models with residual additions and uni-
form structure, NN-Mass is proportional to the total number
of non-linear units in the network. Using [40], the maximal
number of linear regions is bounded by 2X = 2km. Hence,

w1+3x3 Conva.Bottleneck Residual BlockBN+ReLUBN+ReLUBNw2w2w1w1w2𝑖!=𝑤"!+2𝑤#!𝜌!=𝑤"!2𝑤"!+𝑤#!𝑚=’(𝑤"!+2𝑤#!)×𝑤"!2𝑤"!+𝑤#!$!!%"1x1 Conv1x1 Conv7x7 DSConvw2+b.ConvNextBlockLNGeLUw1w1w1w1w1𝑖!=2𝑤"!+𝑤#!𝜌!=𝑤"!3𝑤"!=	13𝑚=’(2𝑤"!+𝑤#!)×13$!!%"1x1 Conv1x1 ConvFigure 6. ImageNet accuracy for models scaled up or down from the base ConvNext-Tiny network. All networks are trained for 100 epochs.
(a) Top-1 accuracy vs. #MACs for three MAC budgets: 3.3G, 4.5G, and 8.5G MACs (R2 = 0.75). (b) Top-1 accuracy vs. #parameters
for three parameter count budgets: 21M, 28M, and 50M (R2 = 0.73). (c) Accuracy increases with NN-Mass (R2 = 0.85). (d) Each letter
denotes a model: blue shows its test accuracy and red shows its NN-Mass. Clearly, higher NN-Mass results in higher test accuracy for all
models except O and L. Accuracy saturates as NN-Mass becomes high for a given constraint (O and L have less than 0.1% accuracy gap).

must hold true for model M: It should have a large depth
and width, and must consist of a uniform block structure
that repeats throughout the network.

Since computing NN-Mass does not require training, we
scale the base model M as follows: We scale the depth
and width of M using a set of width and depth multipli-
ers, e.g., W ∈ (wmin, wmax) and D ∈ (dmin, dmax). For
each wm ∈ W and dm ∈ D, we compute the {#MACs,
#parameters, NN-Mass}. Then, for the models that satisfy
the given hardware budget constraint Hi ∈ H, we pick the
model with highest NN-Mass and train it. Based on the the-
ory discussed in Section 3.2, we expect this model to have
superior gradient ﬂow properties and expressive power than
other models with similar #MACs/#parameters. Therfore,
model Mi for hardware budget Hi is simply the network
that maximizes NN-Mass for that hardware budget. Since
#MACs/#parameters/NN-Mass do not require training, this
process is completely training free.

Note that, scaling the base model by wm and dm implic-
itly restructures the non-linear activation functions because
it changes NN-Mass which in turn changes the total num-
ber of non-linear units in the network (see Proposition 3).
Therefore, we call our networks RAN-implicit (RAN-i).
We next present detailed experimental results on ImageNet
to show the effectiveness of our training-free scaling.

3.4. RAN-i: ImageNet Evaluation

We start with the base model ConvNext-Tiny [39] and
scale it to three hardware budgets: (1) H1: 3.3B MACs
and 21M parameters, (2) H2: 4.5B MACs and 28M pa-
rameters, i.e., the same hardware budget as ConvNext-Tiny,
and (3) H3: 8.5B MACs and 50M parameters. To sam-
ple models that satisfy the above hardware budgets, we
use width multipliers W ∈ (0.25, 1.6) and depth multi-
pliers D ∈ (0.6, 2.56).
In total, we sampled 800 differ-
ent models using the above width and depth multipliers.
For each network, we then compute {#MACs, #parameters,

NN-Mass}. We found total 15 networks that satisﬁed H =
{H1, H2, H3} budgets deﬁned above (i.e., 5 models for
each of H1, H2, H3). As explained in Section 3.3, for each
budget, the higher NN-Mass models are expected to achieve
the higher accuracy due to better gradient properties and ex-
pressivity. To verify this, we trained all 15 networks on Im-
ageNet for 100 epochs to evaluate if this is indeed the case.
Detailed training setup is given in Appendix C.

We show the top-1 accuracy and its relationship with
#MACs/#parameters/NN-Mass for all 15 networks in
Fig. 6. Note that, all models belong to the same ConvNext
family of networks and the only difference is that their
widths and depths are scaled up or down from ConvNext-
Tiny (ConvNext-T) network. Yet, Fig. 6(a,b) show that for
exactly the same hardware budget, there can be a signif-
icant difference in accuracy. We found that for all three
hardware budgets, models with increasing NN-Mass result
in higher accuracy (see Fig. 6(c)). Speciﬁcally, R2 = 0.85
for NN-Mass is higher than that for #MACs (R2 = 0.75)
and #parameters (R2 = 0.73).

Fig. 6(d) also shows the exact ranking of top-1 accu-
racy and NN-Mass vs. #MACs. Here, each letter denotes a
model, blue color shows its accuracy, and red color shows
its NN-Mass. Going from top to bottom for each hardware
budget, we can see that the ranking for accuracy and NN-
Mass is the same for almost all cases7. Note that, for each
hardware budget, accuracy eventually saturates and, hence,
increasing NN-Mass stops improving the models. This is
visible from models O and L in Fig. 6(d). This also indicates
that NN-Mass should have some lower and upper bounds
for it to work optimally. While we have derived some of
these conditions in Corollary 1, more theoretical analysis
can certainly improve our understanding of NN-Mass. Nev-
ertheless, our results clearly demonstrate that NN-Mass sig-

7For instance, going from top to bottom for 3.3B MACs in Fig. 6(d),
we see the ranking of networks as E-C-A-D-B for both NN-Mass and ac-
curacy, thus showing that higher NN-Mass results in higher accuracy.

468Number of MACs (G)79808182Test Accuracy (Top-1)(a)ConvNext-TIncreasing NN-MassTop-1 vs. MACs, R2 = 0.7520304050Number of parameters (M)79808182Test Accuracy (Top-1)(b)ConvNext-TIncreasing NN-MassTop-1 vs. Params, R2 = 0.731015202530NN-Mass (x103)79808182Test Accuracy (Top-1)(c)Top-1 vs. NN-Mass, R2 = 0.8546810Number of MACs (G)79808182Test Accuracy (Top-1)(d)ABCDEFGHIJKLMNOTop-1 and NN-Mass vs. MACs10152025NN-Mass (x103)ABCDEFGHIJKLMNOTable 3. Object Detection Results. On the COCO dataset, object
detectors backboned by NN-Mass scaled RAN-i models (see Ap-
pendix D) achieve competitive accuracies with signiﬁcantly less
computation requirement than when backboned by ConvNexts.

Backbone
ConvNext-S
RAN-i-S (Ours)
ConvNext-B
RAN-i-B (Ours)

mAP
Params MACs
94.9M 150.6B 34.4%
34.7%
63.6M 82.5B
149.9M 219.4B 35.0%
93.2M 123.4B 34.9%

FPS Improvement

1.33×

1.21×

network (RAN-i or ConvNext, in this case) and then sends
the extracted features to a Region Proposal Network (RPN),
which proposes Regions of Interest (RoIs) around objects to
be classiﬁed by the head.

Our detector architecture is largely the same as that of
Faster-RCNN, but we make the following changes to slim
it down: (1) In the RPN, we only use PyTorch’s design of a
single 3 × 3 convolutional layer, followed by two 1 × 1 con-
volutions [30]. We restrict the model to produce 512 RoIs
instead of 800 RoIs. (2) In the head, we only use two fully
connected layers with 512 neurons each before the output.
For an apples-to-apples comparison, we compare the
performance of our object detector in two cases: (1) the
backbone is RAN-i, and (2) the backbone is ConvNext.
As shown in Table 3, when RAN-i backbones are used,
object detectors run up to 1.33× faster (measured on an
Arm Neoverse-based datacenter CPU) than when ConvNext
equivalents are used, while MACs are reduced by 1.83×
and parameters by 1.49×. On the COCO dataset [36], ac-
curacy of our RAN-i backbone model either exceeds or
is similar to that of the ConvNext backbone model.
It is
also competitive with ResNet50-FPN-Faster-RCNN’s 36%
mAP [13], despite requiring 1.6× fewer MACs. ResNet50-
FPN-Faster-RCNN is a Faster-RCNN model with a ResNet-
50 backbone and a Feature Pyramid Network (FPN), in-
tended to handle objects at different sizes [35]. We do not
use a FPN in our model. Further, our architecture facilitates
easy training, achieving 34.7% mAP in just 26 epochs (15
hours wall-clock time, see Appendix E for training details).
Therefore, RAN-i can be used to signiﬁcantly reduce object
detection compute costs without affecting accuracy.

The beneﬁts of our object detection experiments are fur-
ther emphasized by the architecture’s ease of use and ver-
satility. By incorporating NN-Mass scalable backbones, an
existing detector can be easily modiﬁed to achieve substan-
tial computation improvements. The rapid trainability of
the design (no layer-freezing required) also facilitates train-
ing with fewer resources. The backbone can be replaced
by any other feature extractor, and the RPN and head are
further modularized. Users of this design can thus strike a
comfortable balance between accuracy and computational
requirements in a relatively inexpensive manner.

Figure 7. ImageNet results: RAN-i networks (i.e., highest NN-
Mass models for various hardware budgets) achieve state-of-the-
art accuracy and establish a new pareto frontier over existing net-
works. Models are trained for 300 epochs. (a) MACs vs. Top-1
Accuracy: RAN-i can achieve 83.6% top-1 accuracy which is only
0.2% lower than ConvNext-B [39] while requiring 1.82× fewer
MACs. (b) Parameter count vs. Top-1 Accuracy: RAN-i leads to
signiﬁcant savings in number of parameters as well.

niﬁcantly cuts down the search space of possible depths and
widths by providing us the most promising candidates in a
completely training-free manner.

Finally, we pick the highest NN-Mass networks for each
hardware budget and train them to 300 epochs on ImageNet.
We call these highest NN-Mass networks as RAN-i; more
architecture details are given in Appendix D. The results are
shown in Fig. 7. Clearly, RAN-i establishes a new state-
of-the-art on ImageNet as it beats the Pareto frontier of
ConvNexts. Speciﬁcally, our 8.45B MACs RAN-i network
achieves only 0.2% lower top-1 accuracy than ConvNext-
B that requires 15.4B MACs. This results in up to 1.82×
fewer MACs with nearly the same accuracy. Signiﬁcant
improvements are also obtained in #parameters. More-
over, for a concrete FPS evaluation, we deploy our 4.59B
MACs RAN-i network and 8.7B MACs ConvNext-S on a
single core Arm Neoverse-based datacenter CPU. We found
that RAN-i achieves nearly 40% (1.38×) higher FPS than
ConvNext-S with about 0.5% lower accuracy. Therefore,
NN-Mass is a very inexpensive method to push the Pareto
frontier and to scale models in a training-free way to various
hardware constraints once a good base model is known.

3.5. Object Detection with NN-Mass Scaling

To further illustrate the utility of network scaling with
NN-Mass, we compare the performance of RAN-i models
against that of ConvNext models when deployed as object
detection backbones. We design a simple two-stage object
detector and show that it performs better when it uses our
scaled models than when it uses ConvNexts.

Two-stage object detectors, such as the popular Faster-
RCNN and Mask-RCNN, are common because of their ease
of use and competitive detection accuracies [23,44]. A two-
stage detector funnels the input image through a backbone

102030Number of MACs (G)8081828384Test Accuracy (Top-1)(a)0.2% accuracy drop1.82x fewer MACsMACs vs. AccuracyDeitSwinConvNextRAN-i50100150200Number of parameters (M)8081828384Test Accuracy (Top-1)(b)Parameters vs. AccuracyDeitSwinConvNextRAN-ito (H ×W ×w1×2.4w1)×2+(H ×W ×w2
i.e., 27.5% fewer MACs compared to the initial block.

1) = 5.8HW w2
1,

Inevitably, removing non-linear units would result in
some loss of accuracy. It is natural to ask if there is anything
we can do on the lower branch to recover the lost accuracy.
For instance, can we use a GeLU or some other activation
function ψ on the lower branch? Note that, the inverted bot-
tleneck structure seems to be a common theme in most of
the state-of-the-art models and results in a signiﬁcant im-
provement in accuracy. The main deﬁning characteristic of
the inverted bottleneck is that the non-linearity is applied
in higher dimensions, e.g., after expanding the initial num-
ber of channels using a 1 × 1 convolution. We hypothe-
size that this “applying non-linearity in higher dimensions”
is responsible for high accuracy achieved by most of the
networks, e.g., EfﬁcientNets [51], ConvNext [39], Swin-
Transformers [38], etc. Therefore, we ask the following
question w.r.t. the lower branch in our restructured block: Is
there an inexpensive way we can operate higher dimensions
without increasing computational costs?

The above question has been very well-studied in the ma-
chine learning community. Speciﬁcally, the kernel trick [9]
used in Support Vector Machines [47] can project low-
dimensional inputs into high-dimensional spaces without
ever leaving the original low-dimensional space. Towards
this, we consider the non-linear activation function ψ for
the lower branch as the exponential function:

ψ(x, β) = e(cid:104)x,β(cid:105) =

∞
(cid:88)

n=0

(cid:104)x, β(cid:105)n
n!

,

(8)

where, x is the input data patch and β is the learnable
weight for the 1 × 1 convolution on the lower branch.
Clearly, similar to all kernel tricks, the exponential kernel
implicitly operates in an inﬁnite-dimensional space without
ever explicitly computing the sum in Eq. (8). Note that, by
designing an activation function to make up for the lost ex-
pressivity during our explicit restructuring, we are attempt-
ing to co-design a restructurable block with a novel activa-
tion function. We next evaluate whether this co-design can
help us achieve a higher accuracy.

Table 4 demonstrates the results for explicit restructur-
ing of ConvNext-T network. Here, all models are trained
on ImageNet for 100 epochs. As evident, if we remove
the lower branch completely, this results in a 40% channel
pruned version of ConvNext-T. This network loses about
1.3% accuracy over the baseline. Next, we evaluate two net-
works: (1) A 27.5% channel pruned version of ConvNext-T
which is about 0.9% below the baseline, and (2) A 40% non-
linearity restructured network as shown in Fig. 8(b), right
(we call this as Model A). Both of these networks achieve a
similar accuracy and incur exactly the same #MACs and
#parameters. Next, we train Model B which appends a
GeLU activation function at the end of lower branch (see

Figure 8. Block and Activation Function Co-Design: (a) Typical
ConvNext block consists of an inverted bottleneck with 4× ex-
pansion ratio (see 4w1 GeLUs between the 1 × 1 layers). (b) If
40% non-linear units are removed, the inverted bottleneck can be
analytically separated into two branches. Since the lower branch
does not have any non-linearity, it restructures into a single 1 × 1
convolution with w1 input and output channels, thereby saving
#MACs/#parameters over the base model. (c) Can we regain some
accuracy by re-introducing cheap non-linearity on second branch?

4. Block and Activation Function Co-Design

So far, we have implicitly restructured the amount on
non-linearity in ConvNext.
Is it possible to directly ma-
nipulate the non-linear units in ConvNext to explicitly re-
structure it into a different network architecture? If yes,
can we co-design novel activation functions that make up
for lost expressivity due to restructuring? We note that the
ConvNext block also contains an inverted bottleneck kind
of structure with a 1 × 1 convolution expanding the w1 in-
put channels by a factor of 4, followed by 4w1 non-linear
GeLU units, and then a 1 × 1 convolution projecting back
to w1 channels (see Fig. 8(a)). Total #MACs can be signiﬁ-
cantly reduced if all or at least some of those GeLUs can be
linearized. This can be similar to RAN-e that fully removes
non-linear activations from expanded channels. However,
we found that the accuracy drops signiﬁcantly in ConvNexts
if we linearize all GeLU units in a block.

To this end, we focus on the following task: We ﬁrst
remove 40% GeLUs from each block of ConvNext-T net-
work. As shown in Fig. 8(b), the 40% linearized chan-
nels can then be analytically separated out as a secondary
branch. That is, the primary (upper) branch has 60% chan-
nels (0.6 × 4w1 = 2.4w1) with GeLUs, and the secondary
(lower) branch has 40% channels (0.4×4w1 = 1.6w1) with
no non-linear activation function. Then, similar to RAN-e,
the lower branch explicitly restructures into a single 1 × 1
convolution. This process reduces the number of MACs
from (H × W × w1 × 4w1) × 2 = 8HW w2
1 in ConvNext-T

7x7 DSConv4w1+LNGeLUw1w11x1 Conv1x1 Convw1a.ConvNext7x7 DSConv2.4w1+LNGeLUw1w11x1 Convw11x1 Conv1x1 ConvGeLUor some othernon-linearity 𝜓w1w1c.Activation function on new blockb.Explicit restructuring on ConvNext(40% non-linearities removed)7x7 DSConv2.4w1+LNGeLUw1w11x1 Convw11x1 Conv1x1 Conv1x1 Conv1.6w1No non-linearity7x7 DSConv2.4w1+LNGeLUw1w11x1 Convw11x1 Conv1x1 ConvNo non-linearityw1w1=Table 4. Block and Activation Function Co-Design for ConvNext-
Tiny. Models are trained for 100 epochs on ImageNet.

100 epoch training

Params MACs

Top-1

ConvNext-T
ConvNext-T (40% pruned)
ConvNext-T (27.5% pruned)
Model A (40% restructured, Fig. 8(b) right)
Model B (Model A + [ψ = GeLU], Fig. 8(c))
Model C (Model A + [ψ = EXP], Fig. 8(c))

80.2%
28.6M 4.47B
78.9%
18.2M 2.8B
79.3%
21.5M 3.32B
79.3%
21.5M 3.32B
79.4%
21.5M 3.32B
21.5M 3.32B 79.7%

Fig. 8(c)). Model B achieves only 0.1% higher accuracy
than Model A. Finally, we train Model C using the exponen-
tial activation function. Even though Model C has exactly
the same #MACs and #parameters as Models A, B, and
the 27.5% channel-pruned ConvNext-T, it achieves nearly
0.3%-0.4% higher accuracy. This supports our hypothesis
that operating in higher dimensions can still be beneﬁcial in
deep networks even if it is done using kernel tricks.

While we achieve accuracy improvement with our pro-
posed activation function, there are clear limitations: intro-
ducing an exponential in the network makes it highly unsta-
ble. Speciﬁcally, we observed that during training, it can of-
ten lead to NaN loss. However, when the model does train,
we get better convergence and accuracy than no-activation
and GeLU cases. This is a key limitation of our kernel trick.
Therefore, more stable activation functions that implicitly
operate in high dimensions should be designed in future.

5. Outstanding Problems

So far, we have demonstrated that explicit and implicit
restructuring of non-linear activation functions is valuable
for deep learning. Based on our insights, we now discuss
the following open problems in this new domain:

• Manipulating Non-Linearity as a Key NAS Goal: In
the AI accelerator age, high utilization yet high accu-
racy building blocks are a prerequisite for hardware-
aware networks. However, existing NAS does not use
a search space containing restructurable blocks like
AFRBs. Therefore, future NAS research should ex-
ploit non-linearity manipulation as a key objective.

• Non-Linearity and Model Architecture Co-Design:
More generally (i.e., beyond NAS), a new research
direction is to co-design novel restructurable blocks
along with ways to induce the non-linearity elsewhere
in the network. We have attempted to do this in
our Section 4, e.g., we restructured a known block
with a more powerful, theoretically-grounded activa-
tion function. However, since our exponential function
has clear stability issues, more research is needed in
this area to potentially discover completely new build-
ing blocks that are friendly to AI accelerators.

• Better Theory and Generic Topological Metrics:
Our NN-Mass [7] based method allows us to scale base
models to various hardware constraints in a training-
free manner and achieves state-of-the-art accuracy on
ImageNet. There are still limitations which can be tar-
geted in future work: (i) Currently NN-Mass works
for large networks. There must be speciﬁc bounds on
depth and width to obtain optimal results with NN-
Mass (see Section 3.4). While Corollary 1 is a step
in this direction, more theory is needed to better un-
(ii) Also, NN-Mass works only
derstand NN-Mass.
for uniform structures and does not work for irregular
strides and full NAS search spaces. Thus, better topo-
logical metrics are required for generic NAS as well.

• More Theory for Explicit Restructuring: We need
better theoretical grounding for explicit restructuring
of activation functions. For instance, how does the dual
objective in problem (3) change the deep learning op-
timization landscape? Can we build better optimizers
that speciﬁcally work well for AFRBs? Improvements
in this space can directly improve the overall accuracy
without changing the computational costs.

The research directions above can signiﬁcantly impact

efﬁcient deep learning, particularly for AI accelerators.

6. Related Work

Linear overparameterization in deep networks.
The
beneﬁt of linear overparameterization in accelerating the
training of deep neural networks and improving accuracy
has received considerable attention in recent works [3, 11,
14, 15, 22, 57]. Several of these previous works propose
overparameterizing a convolutional layer during training by
using a series of linear convolutional layers. More recently,
RepVGG [15] demonstrates the importance of linear resid-
ual connections in parallel branches of a neural network
during training, which can be folded during inference to
boost the accuracy of single-branch networks. In contrast
to these prior works on model overparameterization, RAN-e
seeks to identify where in the network the non-linear activa-
tion functions can be removed. This results in a sequence of
linear convolution layers that can be collapsed into a single,
small, regular convolution layer. Overall, our approach pro-
duces networks that use a mix of IBNs and regular convo-
lutions, and achieve signiﬁcantly higher accuracy at lower
computational cost than RepVGG [15].

Non-linearity manipulation. Concurrent with our work,
Fu et al. [20] proposed DepthShrinker, which combines ir-
regular blocks into dense operations to create hardware-
efﬁcient, compact neural networks. DepthShrinker also pro-
poses to replace low-utilization blocks with regular convo-
lutions by pruning non-linear units. Despite the synergies

between our work and DepthShrinker, there are signiﬁcant
differences and advantages of our work:

1. More generality: RANs are much more general than
just non-linearity pruning: We propose a hardware-
aware search space for future NAS methods. We fur-
ther propose training-free model scaling with theoret-
ically grounded non-linearity manipulation, as well as
a co-design between blocks and activation functions
which could also be useful for networks where com-
plete removal of non-linear units may not be possible.

2. Fully differentiable restructuring: Our RAN-e is a
In con-
fully differentiable restructuring algorithm.
trast, DepthShrinker [20] relies on approximate tech-
niques like Straight-Through Estimators (STE) [6]
which can be unstable under certain conditions [61].

3. No self-distillation: DepthShrinker exploits meth-
ods like self-distillation to regain the accuracy drop.
Distillation-based techniques are known to result in
signiﬁcant accuracy improvements [41]. We do not use
any distillation-based methods to improve accuracy.

4. Better accuracy: RAN-e achieves signiﬁcantly higher
In particular, on Ima-
accuracy than DepthShrinker.
geNet, RAN-e-C achieves a 2.1% higher accuracy un-
der comparable MACs compared to DepthShrinker’s
MBV2-1.4-DS-D model (e.g., 488M MACs RAN-
e-C achieves 74.6% accuracy vs.
72.5% accuracy
for 484M MACs DepthShrinker; no self-distillation
used in either network). Additionally, for similar
MACs, RAN-e-GT trained without self-distillation
signiﬁcantly outperforms DepthShrinker trained with
self-distillation by 4.47% in top-1 accuracy (e.g.,
74.6% for 433M MACs RAN-e-GT vs. 70.13% for
DepthShrinker’s 415M MACs MBV2-1.4-DS-F).

5. SotA ImageNet results on multiple scales: Finally,
our techniques result in state-of-the-art results on Ima-
geNet at multiple scales, ranging from micro-NPUs to
datacenter CPUs. In contrast, DepthShrinker does not
cover such a broad range of ImageNet networks.

DNN compression techniques.
Numerous research ef-
forts have been devoted in recent years to compressing neu-
ral networks for increasing hardware efﬁciency in accelera-
tors via ﬁlter pruning [12, 19, 25], layer pruning [16, 17, 59],
quantization [5, 18, 48, 49], and low-rank matrix factoriza-
tion [50, 56, 60]. Nonetheless, because these model com-
pression techniques are orthogonal to our hardware-aware
block search paradigm, they can be combined with our
search space to improve hardware efﬁciency even further.

NAS for improving hardware efﬁciency.
Recent re-
search on automated efﬁcient DNN design has been able to
take advantage of signiﬁcant advances in neural architecture
search (NAS) to select from a variety of hardware-efﬁcient
convolutional blocks, layer widths, depths, connectivities,
and per-layer quantization bitwidths during training while
building a network architecture [4,10,32,34,52–55,58]. For
example, MobileDet’s NAS search space included IBN as
well as other hardware-aware convolutions like fused and
tucker convolutions [58]. While it is possible to naively
construct a NAS search space from parallel branches of IBN
and hardware-friendly regular convolutions, our work pro-
poses using non-linearity manipulation to choose between
IBN and hardware-friendly convolutional blocks from the
same underlying weight-shared block. This, unlike previ-
ous works, will enable the search process between different
convolutional blocks to take advantage of weight-sharing
NAS. Manipulating non-linearity can essentially be added
as another search dimension during NAS. We leave this ex-
ploration for future work.

7. Conclusion

In this paper, we have proposed the new RAN paradigm
that manipulates the amount of non-linearity in networks
to improve their hardware-efﬁciency.
Speciﬁcally, we
have proposed RAN-explicit (RAN-e) and RAN-implicit
(RAN-i) techniques for hardware-aware search spaces and
training-free model scaling, respectively. For certain classes
of networks, we have also theoretically proved the link be-
tween model expressivity as deﬁned by the amount of non-
linearity and its topological properties. With extensive ex-
periments, we have demonstrated that our networks achieve
state-of-the-art results on ImageNet at different scales and
for various types of hardware ranging from micro-NPUs to
datacenter CPUs. Our proposed RAN-e achieves a similar
accuracy as EfﬁcientNet-Lite-B0 while improving FPS by
up to 1.5× on Arm micro-NPUs. Moreover, with a similar
or better accuracy, our RAN-i networks demonstrate nearly
2× reduction in #MACs and about 40% improvement in
FPS on Arm Neoverse-based datacenter CPUs compared
to ConvNexts. When used as backbones in object detec-
tion, RAN-i achieve a similar or higher mAP over Con-
vNexts with 33% higher FPS on datacenter CPUs. Finally,
we have also discussed a new research direction of model
architecture-activation function co-design.

Overall, we have demonstrated several useful scenarios
where manipulating non-linear activation functions in deep
networks directly results in signiﬁcant hardware-awareness
and efﬁciency. For future work, we have outlined several
outstanding research problems in this new area of restruc-
turable deep networks.

References

[1] Arm. Ethos-U55 micro-Neural Processing Unit (micro-
Link: https : / / www . arm . com /
NPU), 2020.
products/silicon-ip-cpu/ethos/ethos-u55.
Accessed: December 8, 2021. 1, 6

[2] Arm. Ethos-U65 micro-Neural Processing Unit (micro-
Link: https : / / www . arm . com /
NPU), 2020.
products/silicon-ip-cpu/ethos/ethos-u65.
Accessed: December 8, 2021. 1, 6

[3] Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the op-
timization of deep networks: Implicit acceleration by over-
parameterization. In Jennifer Dy and Andreas Krause, ed-
itors, Proceedings of the 35th International Conference on
Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pages 244–253, Stockholmsm¨assan,
Stockholm Sweden, 10–15 Jul 2018. PMLR. 14

[4] Colby Banbury, Chuteng Zhou,

Igor Fedorov, Ramon
Matas, Urmish Thakker, Dibakar Gope, Vijay Janapa Reddi,
Matthew Mattina, and Paul Whatmough. Micronets: Neu-
ral network architectures for deploying tinyml applications
on commodity microcontrollers. In Proceedings of Machine
Learning and Systems, 2021. 1, 15

[5] Ron Banner, Yury Nahshan, and Daniel Soudry. Post train-
ing 4-bit quantization of convolutional networks for rapid-
deployment. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems, 2019. 15

[6] Yoshua Bengio, Nicholas L´eonard, and Aaron Courville.
Estimating or propagating gradients through stochastic
arXiv preprint
neurons for conditional computation.
arXiv:1308.3432, 2013. 15

[7] Kartikeya Bhardwaj, Guihong Li, and Radu Marculescu.
How does topology inﬂuence gradient propagation and
model performance of deep networks with densenet-type
In Proceedings of the IEEE/CVF Con-
skip connections?
ference on Computer Vision and Pattern Recognition, pages
13498–13507, 2021. 2, 9, 10, 14

[8] Kartikeya Bhardwaj, Milos Milosavljevic, Liam O’Neil,
Dibakar Gope, Ramon Matas, Alex Chalﬁn, Naveen Suda,
Lingchuan Meng, and Danny Loh. Collapsible linear blocks
for super-efﬁcient super resolution. Proceedings of Machine
Learning and Systems, 4:529–547, 2022. 4

[9] Christopher M. Bishop. Pattern Recognition and Machine
Learning (Information Science and Statistics). Springer-
Verlag, Berlin, Heidelberg, 2006. 13

[10] Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct
neural architecture search on target task and hardware. In In-
ternational Conference on Learning Representations, 2019.
1, 7, 15

[11] Jinming Cao, Yangyan Li, Mingchao Sun, Ying Chen, Dani
Lischinski, Daniel Cohen-Or, Baoquan Chen, and Changhe
Tu. Do-conv: Depthwise over-parameterized convolutional
IEEE Transactions on Image Processing, 31:3726–
layer.
3736, 2022. 14

global ranking. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2020. 15

[13] Torch Contributors. Faster-rcnn-resnet50-fpn documenta-

tion. 12, 19

[14] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong
Han. Acnet: Strengthening the kernel skeletons for powerful
cnn via asymmetric convolution blocks. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV), October 2019. 14

[15] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,
Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style
convnets great again. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
13733–13742, 2021. 8, 14

[16] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune
deep neural networks via layer-wise optimal brain surgeon.
In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett, editors, Advances in
Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017. 15

[17] Sara Elkerdawy, Mostafa Elhoushi, Abhineet Singh, Hong
Zhang, and Nilanjan Ray. To ﬁlter prune, or to layer prune,
that is the question. In Proceedings of the Asian Conference
on Computer Vision (ACCV), November 2020. 15

[18] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani,
Rathinakumar Appuswamy, and Dharmendra S. Modha.
Learned step size quantization. In International Conference
on Learning Representations, 2020. 15

[19] Jonathan Frankle and Michael Carbin. The lottery ticket hy-
pothesis: Finding sparse, trainable neural networks. In In-
ternational Conference on Learning Representations, 2019.
15

[20] Yonggan Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng
Wan, Raghuraman Krishnamoorthi, Vikas Chandra, and
Yingyan Lin. Depthshrinker: A new compression paradigm
towards boosting real-hardware efﬁciency of compact neural
networks. In International Conference on Machine Learn-
ing, ICML 2022, 17-23 July 2022, Baltimore, Maryland,
USA, 2022. 14, 15

[21] Google. EfﬁcientNet-Lite models, 2020. Link: https:
//github.com/tensorflow/tpu/tree/master/
models / official / efficientnet / lite. Ac-
cessed: December 8, 2021. 6, 8

[22] Shuxuan Guo, Jose M. Alvarez, and Mathieu Salzmann.
Expandnets: Linear over-parameterization to train compact
convolutional networks. In Advances in Neural Information
Processing Systems, volume 33, pages 1298–1310, 2020. 4,
14

[23] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), Oct 2017. 12
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 8

[12] Ting-Wu Chin, Ruizhou Ding, Cha Zhang, and Diana Mar-
culescu. Towards efﬁcient model compression via learned

[25] Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang
Zhang, and Yi Yang. Learning ﬁlter pruning criteria for deep

convolutional neural networks acceleration. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR), June 2020. 15

2020s. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 11976–11986,
2022. 1, 2, 11, 12, 13, 18

[26] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
Song Han. Amc: Automl for model compression and ac-
In Proceedings of the Euro-
celeration on mobile devices.
pean conference on computer vision (ECCV), pages 784–
800, 2018. 1

[27] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities
and stochastic regularizers with gaussian error linear units.
2016. 2

[28] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the

knowledge in a neural network, 2015. 1

[29] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh
Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,
Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-
In Proceedings of the IEEE/CVF international
bilenetv3.
conference on computer vision, pages 1314–1324, 2019. 2,
6

[30] N Inkawhich. Finetuning torchvision models-pytorch tutori-

als 1.2.0 documentation, 2021. 12

[31] Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould,
and Philip H. S. Torr. A signal propagation perspective for
In International
pruning neural networks at initialization.
Conference on Learning Representations, 2020. 9

[32] Sheng Li, Mingxing Tan, Ruoming Pang, Andrew Li, Liqun
Cheng, Quoc V. Le, and Norman P. Jouppi. Searching for fast
model families on datacenter accelerators. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 8085–8095, June 2021. 1, 15

[33] Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy.
Fixed point quantization of deep convolutional networks. In
International conference on machine learning, pages 2849–
2858. PMLR, 2016. 1

[34] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, and Song
Han. Mcunet: Tiny deep learning on iot devices. Advances
in Neural Information Processing Systems, 33, 2020. 1, 15

[35] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,
Feature pyra-
Bharath Hariharan, and Serge Belongie.
In Proceedings of the
mid networks for object detection.
IEEE conference on computer vision and pattern recogni-
tion, pages 2117–2125, 2017. 12

[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740–755.
Springer, 2014. 12, 19

[37] Hanxiao Liu, Karen Simonyan,

Darts: Differentiable architecture search.
arXiv:1806.09055, 2018. 4

and Yiming Yang.
arXiv preprint

[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows.
In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 10012–10022, 2021. 13

[40] Guido F Mont´ufar, Razvan Pascanu, Kyunghyun Cho, and
Yoshua Bengio. On the number of linear regions of deep
neural networks. Advances in neural information processing
systems, 27, 2014. 3, 9, 10

[41] Bert Moons, Parham Noorzad, Andrii Skliar, Giovanni Mar-
iani, Dushyant Mehta, Chris Lott, and Tijmen Blankevoort.
Distilling optimal neural networks: Rapid search in diverse
spaces. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 12229–12238, 2021. 1,
15

[42] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli,
and Jascha Sohl-Dickstein. On the expressive power of deep
In international conference on machine
neural networks.
learning, pages 2847–2854. PMLR, 2017. 3, 9

[43] Prajit Ramachandran, Barret Zoph, and Quoc V Le.
arXiv preprint

Searching for activation functions.
arXiv:1710.05941, 2017. 2

[44] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In C. Cortes, N. Lawrence, D. Lee, M.
Sugiyama, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems, volume 28. Curran Associates,
Inc., 2015. 12

[45] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks, 2019. 1, 3

[46] Thiago Serra, Christian Tjandraatmadja, and Srikumar Ra-
malingam. Bounding and counting linear regions of deep
In International Conference on Machine
neural networks.
Learning, pages 4558–4566. PMLR, 2018. 9

[47] Alex J Smola and Bernhard Sch¨olkopf. A tutorial on support
vector regression. Statistics and computing, 14(3):199–222,
2004. 13

[48] Pierre Stock, Angela Fan, Benjamin Graham, Edouard
Grave, R´emi Gribonval, Herve Jegou, and Armand Joulin.
Training with quantization noise for extreme model com-
In International Conference on Learning Repre-
pression.
sentations, 2021. 15

[49] Pierre Stock, Armand Joulin, R´emi Gribonval, Benjamin
Graham, and Herv´e J´egou. And the bit goes down: Revis-
In International
iting the quantization of neural networks.
Conference on Learning Representations, 2020. 15

[50] Cheng Tai, Tong Xiao, Xiaogang Wang, and Weinan E.
Convolutional neural networks with low-rank regularization.
CoRR, abs/1511.06067, 2015. 15

[51] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model
scaling for convolutional neural networks. In International
conference on machine learning, pages 6105–6114. PMLR,
2019. 1, 3, 7, 13

[52] Mingxing Tan and Quoc Le. Efﬁcientnetv2: Smaller models
and faster training. In Proceedings of the 38th International
Conference on Machine Learning, 2021. 1, 15

[39] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the

[53] Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki
Yoshiyama, Javier Alonso Garcia, Stephen Tiedemann,

Thomas Kemp, and Akira Nakamura. Mixed precision dnns:
In International
All you need is a good parametrization.
Conference on Learning Representations, 2020. 1, 15
[54] Arash Vahdat, Arun Mallya, Ming-Yu Liu, and Jan Kautz.
Unas: Differentiable architecture search meets reinforce-
ment learning. In IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), June 2020. 1, 15
[55] Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuan-
dong Tian, Saining Xie, Bichen Wu, Matthew Yu, Tao Xu,
Fb-
Kan Chen, Peter Vajda, and Joseph E. Gonzalez.
netv2: Differentiable neural architecture search for spatial
and channel dimensions. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2020. 1,
15

[56] Wei Wen, Cong Xu, Chunpeng Wu, Yandan Wang, Yiran
Chen, and Hai Li. Coordinating ﬁlters for faster deep neural
networks. In IEEE International Conference on Computer
Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages
658–666, 2017. 15

[57] Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher
Fifty, Tao Yu, and Kilian Q. Weinberger. Simplifying Graph
Convolutional Networks. In Proceedings of the 36th Interna-
tional Conference on Machine Learning, pages 6861–6871.
PMLR, 2019. 14

[58] Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin,
Gabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans,
Mingxing Tan, Vikas Singh, and Bo Chen. Mobiledets:
Searching for object detection architectures for mobile ac-
In Proceedings of the IEEE/CVF Conference
celerators.
on Computer Vision and Pattern Recognition, pages 3825–
3834, 2021. 1, 3, 15

[59] Pengtao Xu, Jian Cao, Fanhua Shang, Wenyu Sun, and Pu
Li. Layer pruning via fusible residual convolutional block
for deep neural networks. CoRR, abs/2011.14356, 2020. 15
[60] Miao Yin, Yang Sui, Siyu Liao, and Bo Yuan. Towards
efﬁcient tensor decomposition-based dnn model compres-
In Proceedings of the
sion with optimization framework.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 10674–10683, June 2021. 15

[61] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher,
Yingyong Qi, and Jack Xin. Understanding straight-through
estimator in training activation quantized neural nets. arXiv
preprint arXiv:1903.05662, 2019. 15

[62] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. arXiv preprint arXiv:1710.09412, 2017. 8, 18

[63] Yanqi Zhou, Xuanyi Dong, Tianjian Meng, Mingxing Tan,
Berkin Akin, Daiyi Peng, Amir Yazdanbakhsh, Da Huang,
Ravi Narayanaswami, and James Laudon. Towards the co-
design of neural networks and accelerators. In Proceedings
of Machine Learning and Systems, volume 4, pages 141–
152, 2022. 1, 3

A. RAN-e: SuperNet Details

The SuperNet architecture details are given in Table 5.
The top architecture for our SuperNet is different from the

typical top convolutions used in networks like EfﬁcientNet.
The detailed structure of our top is given in Table 6.

Table 5. Detailed architecture for the RAN-e SuperNet. Hi
and Wi are height and width of input feature maps, respectively.
{e, s, Co} refer to expansion ratio, stride, and output channels at
the given stage, respectively. All kernel sizes in SuperNet are 3×3.

Stage
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
−

Operator
Conv 3 × 3
AFRB-1
AFRB-1
AFRB-1
AFRB-1
AFRB-1
AFRB-2
AFRB-1
AFRB-2
AFRB-1
AFRB-3
AFRB-1
AFRB-1
AFRB-2
AFRB-2
AFRB-1
AFRB-2
Top

e

Hi × Wi
s
224 × 224 − 2
1
112 × 112
2
112 × 112
2
56 × 56
1
28 × 28
2
28 × 28
1
14 × 14
1
14 × 14
1
14 × 14
1
14 × 14
1
14 × 14
2
14 × 14
1
7 × 7
1
7 × 7
1
7 × 7
1
7 × 7
7 × 7
1
7 × 7

Co
16
32
6
48
6
64
6
80
6
80
6
80
6
96
4
96
4
128
6
128
6
160
6
176
4
176
4
176
4
224
6
6
224
− − 1000

Table 6. Top convolution architecture for RAN-e networks.

Stage
1
2
3
4

Operator
Conv 1 × 1
DSConv 7 × 7
Average Pool
Conv 1 × 1

Hi × Wi
7 × 7
7 × 7
1 × 1
1 × 1

Ci
224
1344
1344
1344

Co
1344
1344
1344
1000

B. RAN-e: Training Details

We train all RAN-e networks and SuperNet on ImageNet
using Autoaugment data augmentation and label smoothing
with value 0.1. We also use RMSprop optimizer with an ini-
tial learning rate 0.005 which follows a cosine annealing de-
cay schedule after an initial warmup of 5 epochs, batch size
768, decay 0.9, momentum 0.9, epsilon 0.001, and weight
decay 5e-6. We do not use Mixup data augmentation [62]
in our experiments. We implement our PReLU search as
well as ﬁnetuning experiments in Tensorﬂow. All models
are trained on 8 NVIDIA V100 GPUs.

C. RAN-i: Training Details

Our training setup for RAN-i networks is nearly identical
to that used in the ConvNext paper and its public code [39].
The only difference is that we reduced the batch size to 80

for our networks in order to ﬁt within the GPU memory.
The batch size was lowered to 80 for both the initial 15 net-
works sampled using NN-Mass in Fig. 6, and the ﬁnal mod-
els trained in Fig. 7. For the initial 15 networks in Fig. 6, we
used drop path = 0.1 for all networks. In the next section,
we provide more details for the ﬁnal RAN-i networks that
were trained to 300 epochs.

D. Final RAN-i Architecture Details

Table 7 more details on the ﬁnal RAN-i networks. The
base width and depth conﬁgurations are the same as that
for ConvNext-Tiny network. For example, the ﬁrst group in
ConvNext-Tiny has 3 blocks with 96 input and output chan-
nels at each block, followed by 3 blocks with 192 chan-
nels each, and so on. To obtain RAN-i networks, these
base widths and depths are multiplied by width multiplier
wm and depth multiplier dm, respectively. As evident, the
resulting network conﬁgurations satisfy the hardware con-
straints like {3.3B, 4.5B, 8.5B} MACs. The RAN-i net-
works in Table 7 are the highest NN-Mass models for the
aforementioned hardware constraints.

Table 7. Detailed conﬁgurations for RAN-i networks. These are
the ﬁnal networks that were trained to 300 epochs in Fig. 7.

Base Width
Conﬁg
Base Depth
Conﬁg
wm
dm
New Width
Conﬁg
New Depth
Conﬁg
#Parameters
#MACs
Drop Path
Top-1

RAN-i-T (Tiny)

RAN-i-S (Small)

RAN-i-B (Base)

[96,192,384,768]

[3,3,9,3]

0.789
1.65

0.666
1.65

0.9105
2.30

[64,128,256,511]

[76,151,303,606]

[87,175,350,699]

[5,5,15,5]

[5,5,15,5]

[7,7,21,7]

20.76M
3.3B
0.1
82.03%

28.93M
4.59B
0.2
82.63%

52.89M
8.45B
0.4
83.61%

E. Object detection: Training Details

Our training setup is nearly identical

to that used
by Facebook to train their ResNet50-FPN-Faster-RCNN
model [13]. We do not freeze any layers in our detec-
tors, and start training the RPN and head against ImageNet-
pretrained backbones. As with Facebook’s procedure,
minimal data augmentation is performed (resize image to
800x800, add 50% probability of horizontal image ﬂips).
We use Facebook’s modiﬁed COCO loss [36] with stochas-
tic gradient descent (learning rate controlled by a LR Sched-
uler, with momentum of 0.9 and weight decay of 0.0001).
Training on the Microsoft COCO 2017 dataset lasts for 26
epochs, using roughly 15 hours of wall-clock time on 8
NVIDIA V100 GPUs, each with a batch size of 2.

