DADApy: Distance-based Analysis of DAta-manifolds in Python

Aldo Glielmoa,b,∗, Iuri Macoccoa, Diego Doimoa, Matteo Carlia, Claudio Zenia,
Romina Wilda, Maria d’Erricoc,d, Alex Rodrigueze, Alessandro Laioa,e,∗

aInternational School for Advanced Studies (SISSA), Via Bonomea 265, Trieste, Italy
bBanca d’Italia, Italy**
cFunctional Genomics Center, ETH Zurich / UZH, Winterthurerstrasse 190, Zurich, Switzerland
dSwiss Institute of Bioinformatics, Quartier Sorge – Batiment Amphipole 1015 Lausanne, Switzerland
eThe Abdus Salam International Centre for Theoretical Physics (ICTP), Strada Costiera 11, Trieste, Italy

2
2
0
2

p
e
S
9
1

]

G
L
.
s
c
[

2
v
3
7
3
3
0
.
5
0
2
2
:
v
i
X
r
a

Abstract

DADApy is a python software package for analysing and characterising high-dimensional data manifolds. It pro-
vides methods for estimating the intrinsic dimension and the probability density, for performing density-based clus-
tering, and for comparing diﬀerent distance metrics. We review the main functionalities of the package and exemplify
its usage in a synthetic dataset and in a real-world application. DADApy is freely available under the open-source
Apache 2.0 license.

Keywords: manifold analysis; intrinsic dimension; density estimation; density-based clustering; metric learning;
feature selection

1. Introduction

The necessity to analyse large volumes of data is
rapidly becoming ubiquitous in all branches of com-
putational science, from quantum chemistry, biophysics
and materials science [1, 2] to astrophysics and particle
physics [3].

In many practical applications, data come in the form
of a large matrix of features, and one can think of a
dataset as a cloud of points living in the very high di-
mensional space deﬁned by these features. The num-
ber of features for each data point can easily exceed
the thousands, and if such a cloud of points were to
occupy the entire space uniformly, there would be no
hope of extracting any kind of usable information from
data [4, 5]. Luckily this never happens in practice, and
real world datasets possess a great deal of hidden intrin-
sic structure. The most important one is that the feature
space, even if very high dimensional, is very sparsely
In fact, the points typically lie on a data
populated.
manifold of much lower dimension than the number of
features of the dataset (Figure 1A). A second impor-
tant hidden structure which is almost ubiquitous in real
world data is that the density of points on such a mani-
fold is far from uniform (Figure 1B). The data points are
instead often grouped in density peaks (Figure 1B-C), at

∗Corresponding authors.

E-mail addresses: aldo.glielmo@bancaditalia.it, laio@sissa.it

∗∗The views and opinions expressed in this paper are those of the
authors and do not necessarily reﬂect the oﬃcial policy or position of
Banca d’Italia.

times well separated from each other, at times organised
hierarchically in “mountain chains”.

DADApy implements in a single and user friendly
software a set of state-of-the-art algorithms to charac-
terise and analyse the intrinsic manifold of a dataset.
In particular, DADApy implements algorithms aimed at
estimating the intrinsic dimension of the manifold (Fig-
ure 1A) and the probability density of the data (Fig-
ure 1B), at inferring the topography and the relative po-
sition of the density peaks by density-based clustering
(Figure 1C) and, ﬁnally, at comparing diﬀerent metrics,
ﬁnding this way the features which are better suited to
describe the manifold (Figure 1D).

All these approaches belong to the class of unsuper-
vised methods and are designed to work also in situa-
tions in which only the distances between data points
are available instead of their features. Therefore, the
same tools can be used for analysing a molecular dy-
namics trajectory (where features are available) but also
a metagenomics or a linguistic database, where one can
only deﬁne a similarity or a distance between the data.
Another important feature of the methods included in
the package is that they are speciﬁcally designed in or-
der to work even when the intrinsic dimension of the
data manifold is relatively high, of order ten or more,
and if the manifold is topologically complex, and, in
particular, not isomorphic to a hyperplane. Therefore,
the package can be considered complementary to other
packages, such as Scikit-learn [6], which implement
classical approaches for unsupervised manifold learn-
ing which should be preferred in simpler cases, such as
PCA [7], kernel-PCA [8] or Isomap [9].

Preprint submitted to Patterns, published article available here

September 21, 2022

 
 
 
 
 
 
Figure 1: An illustration of the four main classes of tasks that DADApy can perform. From A to D: Intrinsic dimension estimation, density
estimation, density peaks estimation (i.e., density-based clustering algorithms), and comparison of distance measures.

In the following, we ﬁrst brieﬂy describe the four
classes of algorithms implemented in DADApy. We
then illustrate the structure of the package and demon-
strate its usage for the analysis of both a synthetic and
a realistic dataset. We will also discuss the computa-
tional eﬃciency of the implementations, demonstrating
that the package can be used to analyse datasets of 106
points or more, even with moderate computational re-
sources.

2. Description of the methods

2.1. Intrinsic dimension estimators

The intrinsic dimension (ID) of a dataset can be de-
ﬁned as the minimum number of coordinates which are
needed in order to describe the data manifold without
In our package
signiﬁcant information loss [10, 11].
we provide the implementation of a class of approaches
which are suitable to estimate the ID using only the dis-
tances between the points, and not the features. Most
of these approaches are rooted in the observation that in
a uniform distribution of points, the ratio µi of the dis-
tances of two consecutive nearest neighbours of a point i
are distributed with a Pareto distribution which depends
only on the intrinsic dimension. This allows deﬁning a
simple likelihood for the N observations of µi, one for
each point of the dataset:

p({µi} | ID) =

N(cid:89)

i=1

ID µ−(ID+1)
i

.

(1)

The ID is then estimated either by maximising the
likelihood [12], by Bayesian inference [13], or by linear
regression after a suitable variable transformation [14].
We refer to these estimators as Two nearest neighbours
(2NN) estimators.

It is possible that the data manifold possesses diﬀer-
ent IDs, depending on the scale of variations considered.

2

For example, a spiral dataset can be one-dimensional
on a short scale, but two-dimensional on a larger scale.
Hence, one might be interested in computing an ID es-
timate as a function of the scale. The package provides
two routines to perform this task. The ﬁrst method al-
lows to probe the ID at increasing length scales by sub-
sampling the original dataset. By virtue of the reduced
number of points considered, the average distance be-
tween them will be larger; this can be then interpreted
as the length scale at which the ID is computed. Obvi-
ously, subsampling the dataset also increases the vari-
ance of the ID estimate. The second method, an al-
gorithm called Generalised ratios id estimator (Gride),
circumvents this issue by generalising the likelihood in
Eq. (1) to directly probe longer length scales without
subsampling [13].

After using one of these algorithms, one can select the
ID of the dataset as the estimate that is most consistently
found across diﬀerent scales. However this choice is of-
ten not straightforward, and for a more in depth discus-
sion on this topic we refer to [14, 13]

ID estimation has been successfully deployed in a
number of applications, ranging from the analysis of
deep neural networks [15], to physical applications such
as phase transition detection [16] and molecular force-
ﬁeld validation [17].

2.2. Density estimators

The goal of density estimation is to reconstruct the
probability density ρ(x) from which the dataset has been
harvested. The package implements a non-parametric
density estimator called Point-adaptive kNN (PAk) [18],
which uses as input only the distances between points
and, importantly, is designed to work under the explicit
assumption that the data are contained in an embed-
ding manifold of relatively small dimension. This al-
gorithm is an extension of the standard kNN estima-
tor [19], which estimates the density on a point as pro-

portional to the empirical density sampled in its imme-
diate surrounding. More precisely, the kNN estimates
can be written as

ρi = 1
N

k
Vi,k

,

(2)

where k is the number of nearest neighbours consid-
ered, and Vi,k is the volume they occupy. The volume
is typically computed as Vi,k = ωIDdID
i,k , where ωID is
the volume of unit sphere in RID and di,k is the distance
between point i and its kth nearest neighbour.

In PAk the number of neighbours k used for estimat-
ing the density around point i is chosen adaptively for
each data point by an unsupervised statistical approach
in such a way that the density, up to that neighbour,
can be considered approximately constant. This trick
dramatically improves the performance of the estimator
in complex scenarios, where the density varies signiﬁ-
cantly at short distances [18]. Importantly, the volumes
which enter the deﬁnition of the estimator are measured
in the low-dimensional intrinsic manifold rather than in
the full embedding space. This prevents the positional
information of the data from being diluted on irrele-
vant directions orthogonal to the data manifold. As-
suming that the data manifold is Riemannian, namely
locally ﬂat, it can be locally approximated by its tan-
gent hyperplane and distances between neighbours, the
only distances used in the estimator, can be measured
in this low-dimensional Euclidean space. This allows
to operate on the intrinsic manifold without any explicit
parametrisation. The only prerequisite is an estimate
of the local intrinsic dimension, since this is needed to
measure the volumes directly on the manifold.

Another key diﬀerence between kNN and PAk esti-
mators is that kNN assumes the density to be exactly
constant in the neighbourhood of each point, while PAk
possesses an additional free parameter that allows to de-
scribe small density variations. The PAk density estima-
tor can be used to reconstruct free energy surfaces, es-
pecially in high dimensional spaces [18, 20, 21, 22], and
it can also be used for a detailed analysis of the data like
in [23], where a distinct analysis of the data points with
diﬀerent densities lead to some physical insight about
the system under study.

The same estimator can be used also for estimat-
ing the density on points which do not belong to the
dataset [24], a procedure that has been recently used to
quantify the degree to which test data are well repre-
sented by a training dataset [25].

Finally, PAk is commonly used within the density-
based clustering algorithms discussed in the following
section.

3

2.3. Density peak clustering

The diﬀerent “peaks” of the probability density can
be considered a natural partition of the dataset into sep-
arate groups or “clusters”. This is the key idea under-
lying Density peak (DP) clustering [26], implemented
in DADApy. This algorithms works by ﬁrst estimat-
ing the density ρi of all points i, for example using the
PAk method described in the previous section. Then,
the minimum distance δi between point i and any other
point with higher density is computed as

δi = min
j | ρ j> ρi

di j.

(3)

The peaks of the density (and hence the cluster cen-
tres) are expected to have both a high density ρi and a
large distance δi from points with higher density, and
are hence selected as the few points for which both ρi
and δi are very large. The selection is typically done by
plotting ρi against δi and visually identifying the outliers
of the distribution. Once the cluster centres are found,
each remaining point is assigned to the same cluster as
its nearest neighbour of higher density.

In DP clustering the density peaks must be speciﬁed
by the user, and this arbitrariness represents an obvious
source of errors. The Advanced density peaks (ADP)
clustering approach [27], also available in DADApy,
In ADP cluster-
proposes a solution to this problem.
ing, all local maxima of the density are initially consid-
ered density peaks, and a statistical signiﬁcance anal-
ysis of each peak is subsequently performed. A peak
c is considered statistically signiﬁcant only if the dif-
ference between the log density of the peak ln ρc and
the log density of any neighboring saddle point ln ρcc(cid:48) is
suﬃciently larger than the sum of the errors on the two
estimated quantities

ln ρc − ln ρcc(cid:48) > Z(σc + σcc(cid:48) ).

(4)

If this is not the case, the two peaks c and c(cid:48) are merged
into a single peak. This process is iterated until no peak
that is not statistically signiﬁcant is remaining. The pa-
rameter Z appearing in Eq. (4) can be interpreted as the
statistical signiﬁcance threshold of the found peaks. A
higher value of Z will give rise to a smaller number
of peaks with a higher statistical signiﬁcance. Typical
values range from 1 to 5. ADP and DP are general
clustering tools, and as such have been used in diﬀer-
ent ﬁelds, including single-cell transcriptomics [28, 29],
spike-sorting [30, 31], word embedding [32], climate
modelling [33], Markov state modelling [34], and the
analysis of molecular dynamics simulations [35, 36],
just to mention some of them.

Figure 2: The class structure of the package. Classes are highlighted in blue boxes, and the main methods and and attributes of each class are
reported in the yellow and red boxes respectively. Relationship of inheritance are indicates as black arrows. The class Data inherits from all other
classes, thus providing easy access to all available algorithms of the package.

Another clustering algorithm available in DADApy is
k-peaks clustering [37]. In short, this method is a vari-
ant of ADP that takes advantage of the observation that
the optimal ki is high in two cases: 1) In high-density re-
gions due to the high concentration of points, and 2) in
vast regions where the density is everywhere constant.
Therefore, the peaks in ki correspond either to peaks in
density or to the centre of large regions with nearly con-
stant density (e.g., metastable states stabilised by en-
tropy). An example application of k-peaks clustering
can be found in [37], where it was used to describe the
free-energy landscape of the folding/unfolding process
of a protein.

2.4. Metric comparisons

In several applications, the similarity (or the distance)
between diﬀerent data points can be measured using
very diﬀerent metrics. For instance, a group of atoms
or molecules in a physical system can be represented
by their Cartesian coordinates, by the set of their inter-
particle distances, or by a set of dihedral angles, and
one can measure the distance between two conﬁgura-
tion with any arbitrary subset of these coordinates. Sim-
ilarly, the “distance” between two patients can be mea-
sured taking into account their clinical history, any sub-
set of blood exams, radiomics features, genome expres-
sion measures, or a combination of those.

It might hence be useful to evaluate the relationships
between all these diﬀerent manners to measure the sim-
ilarity between data points. DADApy implements two
methods for doing this: the neighbourhood overlap and
the information imbalance. Both approaches use only
the distances between the data points as input, making
the approaches applicable also when the features are not

explicitly deﬁned (e.g. a social network, a set of protein
sequences, a dataset of sentences).

The neighbourhood overlap is a simple measure of
equivalence between two representations [38]. Given
two representations a and b, one can deﬁne two k-
adjacency matrices Aa
i j as matrices of dimension
N × N which are all zero except when j is one of the k
nearest neighbours of point i. The neighbourhood over-
lap χ(a, b) is then deﬁned as

i j and Ab

χ(a, b) = 1
N

(cid:88)

i

1
k

(cid:88)

j

Aa
i jAb
i j.

(5)

i jAb

Note that the term Aa
i j is equal to one only if j is
within the k nearest neighbours of i both in a and in b,
otherwise it is zero. For this reason, the neighbourhood
overlap can also be given a very intuitive interpretation:
it is the average fraction of common neighbours in the
If χ(a, b) = 1 the two represen-
two representations.
tations can be considered eﬀectively equivalent, while
if χ(a, b) = 0 they can be considered completely inde-
pendent. The parameter k can be adjusted to improve
the robustness of the estimate but in practice this does
not signiﬁcantly change the results obtained as long as
k (cid:28) N [38].

In the original article [38], the neighbourhood over-
lap was proposed to compare layer representations of
deep neural networks and to analyse in this their inner
workings.

The information imbalance is a recently introduced
quantity capable of assessing the information that a
distance measure a provides about a second distance
measure b [39].
It can be used to detect not only
whether two distance measures are equivalent or not, but

4

Metric comparisonsIdEstimationBaseDataClusteringcompute_density_kNN, compute_density_kstarNN, compute_density_PAk, ...return_inf_imb_full_all_coords, greedy_feature_selection_full, return_overlap, ...compute_id_2NN, return_id_scaling_2NN,return_id_scaling_gride, ...compute_distances, remove_identical_points, ...log_den, log_den_err, ...intrinsic_dim, intrinsic_dim_err, ...coordinates, maxk, distances, ...InheritanceClassesMethodsAttributesDensityEstimationcompute_clustering_DP, compute_clustering_ADP, ...N_clusters, cluster_assignment,cluster_centers, ...also whether one distance measure is more informative
than the other. The information imbalance deﬁnition is
closely linked to information theory and the theory of
copula variables [39]. However, for the scope of this
article it can be empirically deﬁned as

import numpy as np
from dadapy import Data

# initialise the " Data " class
# with a set of coordinates
X = np . load ( " coordinates . npy " )
data = Data ( X )

∆(a → b) = 2
N
= 2
N2

(cid:104)rb | ra = 1(cid:105)

(cid:88)

rb
i j

i, j: ra
i j

=1

(6)

# compute distances
# up to the 100 th neighbour
data . c o mp ut e _di s ta n ces ( maxk = 100)

where ra
i j is the rank matrix of the distance a between
= 1 if j is the nearest neigh-
the points (namely ra
i j
= 2 if j is the second neighbour, and so
bour of i, ra
i j
on). In words, the information imbalance from a to b is
proportional to the empirical expectation of the distance
ranks in b conditioned on the fact that the distance rank
between the same two points in a is equal to one.
If
∆(a → b) ≈ 0 then a can be used to describe b with no
loss of information.

When measuring the information imbalances be-
tween two representations we can have three scenarios.
If ∆(a → b) ≈ ∆(b → a) ≈ 0 the two representa-
tions are equivalent, if ∆(a → b) ≈ ∆(b → a) ≈ 1
the two representations are independent, and ﬁnally if
∆(a → b) ≈ 0 and ∆(b → a) ≈ 1 we have that a is in-
formative about b but not vice versa, therefore a is more
informative than b. The information imbalance allows
for eﬀective dimensional reduction since a small subset
of features that are the most relevant either for the full
set, or for a target property, can be identiﬁed and se-
lected [39]. This feature selection operation is available
in DADApy and can be performed as a pre-processing
step before the tools described in the previous sections
are deployed.

The information imbalance proved successful in deal-
ing with atomistic and molecular descriptors, either to
directly perform compression [39] or to quantify the
information loss incurred by competing compression
schemes [40]. In the original article [39], the informa-
tion imbalance was also proposed for detecting causal-
ity in time series -with illustrative results shown on
Covid-19 time series- and to analyse or optimise the
layer representations of deep neural networks.

3. Software structure and usage

DADApy is written entirely in Python, with the
most computationally intensive methods being sped up
through Cython.
It is organised in six main classes:
Base, IdEstimation, DensityEstimation, Clustering,

5

# compute the intrinsic dimension
# using the 2 NN method
ID , ID_err = data . compute_id_2NN ()

# compute the density
# using the PAk method
den , den_err = data . c o m p u t e _ d e n s i t y _ P A k ()

# find the density peaks using
# using the ADP method
clusters = data . c o m p u t e _ c l u s t e r i n g _ A D P ()

Figure 3: A simple DADApy script.

MetricComparison and Data. The relationships of in-
heritance between these classes, as well as the main
methods and attributes available in each class are sum-
marised in Figure 2. The Base class contains basic
methods of data cleaning and manipulation which are
inherited in all other classes. Attributes containing the
coordinates and/or the distances deﬁning the dataset are
contained here. Then, in a train of inheritance: IdEsti-
mation inherits from Base; DensityEstimation inherits
from IdEstimation and Clustering inherits from Den-
sityEstimation. Each of these classes contains as meth-
ods the algorithms described in the previous section, un-
der the same name. The inheritance structure of these
classes is well motivated by the fact that to perform a
density-based clustering one ﬁrst needs to compute the
density, and to perform a density estimation one ﬁrst
needs to know the intrinsic dimension, which can be
estimated only if the distances are preliminarily com-
puted. The MetricComparison class contains the algo-
rithms described in Section 2.4 used to compare couples
of representations using the distances between points.

The class Data does not implement any extra attribute
or method but, importantly, it inherits all methods and
attributes from the other classes. As such, Data provides
easy access to all available algorithms of the package
and is the main class that is used in practice.

A typical usage of DADApy is reported in Figure 3.
In this simple example a Data object is ﬁrst initialised
with the matrix containing the coordinates of the points

shown in Figure 1B-C, and later a series of methods are
called sequentially to compute the distances, the intrin-
sic dimension, the density (Figure 1B) and ﬁnally the
density peaks (clusters) of the dataset (Figure 1C). In
the example given, Data is initialised with a matrix of
coordinates, and the distances between points are later
computed. Note that, however, the object could have
been equivalently initialised directly with the distances
between points, and all methods in the package would
work equivalently. This is particularly important for
those applications for which coordinates are not avail-
able, but distances can be computed, such as DNA or
protein sequences, or networks.

The main aim of the package is to provide user-
friendly, fast and light routines to extract some of the
most common and fundamental characteristics of a data
manifold through solid statistical and numerical tech-
niques. DADApy oﬀers high speed code with reduced
memory consumption. These features are achieved by
exploiting locality. In particular, it is generally enough
to compute the distances between each point and a small
number of its neighbours (deﬁned in DADApy by an
attribute named maxk), and hence such distances can
be computed and stored with close-to-linear time and
memory requirements.

We believe that the Python interface of DADApy will
encourage its rapid diﬀusion, as Python is by far the
most used language in the computational science com-
munity nowadays. We are aware that Python is, how-
ever, a notoriously ineﬃcient language for large scale
In DADApy we circumvent this short-
computation.
coming by implementing all the heavy numerical rou-
tines using Cython extensions, which essentially gen-
erate C-compilable code that runs with very high eﬃ-
ciency (typically over two orders of magnitude faster in
evaluation time than the pure Python implementation).
In this manner we are able to maintain the user friend-
liness of Python without sacriﬁcing the computational
eﬃciency of a fully compiled language.

All of the mentioned properties allow to easily anal-
yse up to a million points on an ordinary laptop within
minutes. This can be seen in Figure 4, where we report
the time spent by the code on many DADApy routines
as a function of the number N of points of the dataset,
using a neighbourhood of maxk = 100 points. The plot
shows that all methods scale linearly in computational
time with N, with the exception of the ADP clustering,
whose scaling becomes unfavourable for more than 50
thousand points. This is a consequence of the neigh-
bourhood size maxk being much smaller than the num-
ber of points N of the dataset, a condition which forces
the estimation of many ﬁctitious density peaks that take

Figure 4: The time complexity of DADApy. The time required
by the various routines of DADApy grows linearly with the num-
ber of samples N, with the only exception of ADP (see text for de-
tails). The dataset used was 2 dimensional and we set maxk=100. The
benchmark was performed on an ordinary desktop using a single Intel
Xeon(R) CPU E5-2650 v2 @ 2.60GHz.

a long time to be merged together. The problem can
be solved by appropriately increasing maxk when nec-
essary.

The runtime performance for the computation of the
distances also scales linearly with the embedding di-
mension D, while the other routines take as input the
computed distances, and are thus independent on D.
Therefore, when D is very large, say D (cid:38) 104, the
distance computation can represent the actual compu-
tational bottleneck of the package.

The code has been thoroughly commented and docu-
mented through a set of easy-to-run Jupyter notebooks,
an online manual, and an extensive code reference. This
can allow new users approaching DADApy to quickly
learn to use it, as well as to modify or extend it.

4. Illustration on a topologically complex synthetic

dataset

We now illustrate the use of some key DADApy meth-
ods on the synthetic dataset depicted in Figure 5A, and
consisting of a 2D plane with 8 clusters, twisted to form
a 3D M¨obius strip and ﬁnally embedded in a noisy 50D
space. The reference 2D dataset is taken from [27], and
consists of data points sampled from an analytic den-
sity function, with points belonging to a single mode of
this density assigned to the same cluster, and all other
considered unassigned.

In spite of the 2D inner structure of the dataset,
common projection methods can easily fail as a con-
sequence of the nontrivial topological properties of the
data-manifold. This is illustrated in Figure 5B, where
PCA and ISOMAP projections are reported.

6

102103104105106Samples N103102101100101102103Time t [s]distances2NNk*pakclusteringFigure 5: Example usage of DADApy for the analysis of a topologically complex synthetic dataset. Panel A illustrates the dataset analysed,
consisting of clusters lying on a 2D sheet twisted to form a M¨obius strip and immersed in a noisy 50D space. Panel B shows the accuracy of some
common clustering methods on reconstructing the original clusters, as well as two low dimensional projections. Panel C summarises the results
obtained using 2NN ID estimation, PAk density estimation and ADP clustering. The top part shows the estimated density peaks, while the bottom
part shows the dendrogram of the dataset. The y-axis of the dendrogram reports the log density of the density peaks and of the saddle points. The
x-axis provides an indication on the relative cluster sizes, since each cluster is in the middle of a region proportional to its population. This region
is delimited by the links in which these clusters are involved and, in the case of the ﬁrst and last clusters, by the beginning and end of the graph.

One key advantage of the methods implemented in
DADApy is their ability to exploit the low dimensional
structure of the data without any explicit projection. In
this case, for example, we compute the ID using the
Gride method of Section 2.1, which is correctly iden-
tiﬁed around 2. We then use the ID to provide accurate
density estimates using the PAk method of Section 2.2,
and ﬁnally identify the clusters (or density peaks) us-
ing the ADP algorithm of Section 2.3. The end result
is a cluster assignment that is remarkably close to the
ground truth, and often superior to other state-of-the-art
clustering schemes that do not exploit the low dimen-
sional structure of the data (see Figure 5B).

Another unique feature of DADApy is the ability of
compactly representing the cluster structure through a
special kind of dendrogram reporting the log densities
of the density peaks and of the saddle points between
them. The bottom part of Figure 5C depicts the den-
drogram for the M¨obius strip data, which can be seen to
provide a remarkably accurate perspective of the rela-
tionship between the estimated density peaks shown in
the panel above.

Note that the dendrogram can be generated indepen-
dently of the ID of the manifold, unlike most graphi-
cal data representations which are practically limited to
three dimensions, thus providing a robust way to visu-
alise the cluster structure even for the common scenario
of ID > 3 manifolds.

The Jupyter notebook used to perform the analysis
described in this section can be found at https:
//github.com/sissa-data-science/DADApy/

blob/main/examples/notebook_mobius.ipynb.

5. Usage for a realistic application

We now exemplify and showcase the usage of
DADApy for the analysis of a biomolecular trajec-
tory. The dataset is composed of 41580 frames from a
replica-exchange MD simulation (400 ns, 340 K replica,
dt = 2 fs) of the 10-residue peptide CLN025, which
folds into a beta hairpin [41]. Several numerical rep-
resentations are possible for this trajectory. A very high
dimensional one is given by the set of all distances be-
tween the heavy atoms, which amounts to 4278 fea-
tures. Such a representation is possibly very redun-
dant, and in fact typically more compact representations
are used to describe systems of this type. For exam-
ple, a compact representation for this system can be
taken as the set of all its 32 dihedral angles [42, 43].
In Figure 6A we use DADApy to compute the informa-
tion imbalance from the space of heavy atom distances
to the dihedral angles space for an increasing number
of dihedral angles, and vice versa. Not surprisingly,
the compact space of dihedral angles is seen to be al-
most equally informative to the very high dimensional
heavy atom distance space, with information imbalance
∆(Xdihedrals → Xfull) lower than 0.1 when considering
around 15 angles (Figure 6A). We thus select the set of
the 15 most informative dihedral angles as the collective
variables to represent this dataset, since the information
imbalance reaches a plateau around this number.

We then use DADApy to compute the ID of the
dataset along diﬀerent scales through both decimation

7

CIsomapAPCAkmeansSCDBSCANHDBSCANADP0.00.10.20.30.40.50.60.70.8Adjusted Mutual Information02500500075001000012500150001750020000Population3.02.52.01.51.00.50.00.51.0ln()4312670521076435BFigure 6: Example usage of DADApy for the analysis of a biomolecular trajectory. Panel A shows the computation of the information
imbalance between a compact molecular representation Xdihedrals (optimally selected sets of dihedral angles with increasing size) and a much
higher dimensional one Xfull (the full space of heavy atom distances). The inset shows the information imbalance between the space of heavy atom
distances and the space of dihedral angles, and vice versa. For clarity, the depicted points are sparsed out. Panel B shows the computation of the
intrinsic dimension across diﬀerent scales using both 2NN and Gride. The main graph refers to the space of 15 dihedrals, while the inset refers to
the space of 4278 heavy atom distances. Panel C shows a dendrogram visualisation of the peaks and the saddle points of the density, estimated
using PAk and the ADP clustering algorithm. Peptide backbones of cluster centre structures are drawn next to their corresponding peaks. The main
graph refers to the space of dihedrals, while the inset refers to the space of heavy atom distances. In both cases, the central and rightmost peaks
capture the main macro states of the peptide and are much more populated than the leftmost peak. The two cluster assignments are identical for
roughly 90% of the data points.

and Gride algorithm [13] (Figure 6B). The two proce-
dures provide fairly overlapping estimates for the ID,
which is comprised between 5 and 8 within short range
distances, and thus much lower than the original feature
space. We continue by estimating the density through
the PAk algorithm, for which we set the ID to 7. This ID
selection is motivated by the observation that the density
is a local property computed at short scales but, impor-
tantly, selecting a lower ID consistent with Figure 6B
(say, 5 or 6) does not signiﬁcantly aﬀect the results. Fi-
nally, we use DADApy to perform clustering using the
ADP algorithm. The results are shown in Figure 6C.

ADP clustering (Z = 4.5) produces three clusters. The
biggest cluster is the folded beta hairpin state of the pro-
tein, as depicted in Figure 6C (cluster 0). A cluster of
roughly half the size is made of a collapsed twisted loop
structure (Figure 6C, cluster 2). Since CLN025 is sus-
pected to have two main metastable states, the folded
hairpin and a denatured collapsed state [44], we suggest
that the twisted loop could be the dominant topology of
the denatured collapsed ensemble. The high occurrence
of the twisted loop might be due to the simulation tem-
perature of 340 K, which is just below the experimental
melting temperature of CLN025 of 343 K [45]. Less
than one percent of the structures are in cluster 1, which
is composed of denatured extended and less structured
topologies.

The 32-dimensional space of dihedrals used so far in
our analysis is known to be well suited to diﬀerenti-
ate meaningful protein structures, but to showcase the
possibility of using DADApy to work in very high di-
mensional spaces, we performed a similar analysis also

on the 4278-dimensional space of all heavy atoms dis-
tances. Using this alternative data description we per-
formed ID estimation with the 2NN method, density es-
timation with the PAk estimator, and clustering with the
ADP algorithm (ID = 9; Z = 3.5). The resulting dendro-
gram is shown as an inset of Figure 6C.

As clear from the ﬁgure, we ﬁnd a remarkably similar
cluster structure, deﬁned by the two major macrostates
of the molecule, the beta pin and the twisted loop, as
well as the cluster with unstructured conﬁgurations.

The equivalence in the two cluster assignments is
conﬁrmed by the fact that 89% of the data points are
assigned to the same cluster independently of the data
representation.

A Jupyter notebook containing the analyses per-
formed in this Section is available at https://
github.com/sissa-data-science/DADApy/blob/
main/examples/notebook_beta_hairpin.ipynb
along with the necessary datasets.

6. Conclusions

In this work we introduce DADApy, a software pack-
age for quickly extracting fundamental properties of
data manifolds. DADApy is written entirely in Python,
which makes it easy to use and to extend; and it exploits
Cython extensions and algorithms for sparse computa-
tion and sparse memory handling, which make it com-
putationally eﬃcient and scalable to large datasets. The
package is documented by a set of easy-to-run Jupyter
notebooks and by a code-reference and manual avail-
able online.

8

135791113151719Number of input dihedrals0.10.20.30.40.50.60.7Optimal (XdihedralsXfull)0.00.20.40.60.81.0(XfullXdihedrals)0.00.20.40.60.81.0(XdihedralsXfull)No. dihedrals12345678910ABC1.52.02.53.03.5Scale5.05.56.06.57.07.5Estimated ID2NN decimationGride789101112137891011DADApy includes state-of-the-art algorithms for
intrinsic dimension estimation, density estimation,
density-based clustering and distance comparison that
found numerous applications in recent years, but have
not yet found widespread usability. We believe this was,
at least in part, precisely due to the lack of a fast and
easy-to-use software like DADApy, and we hope that
our work will allow a growing number of practitioners
from diﬀerent research domains to approach the ﬁeld of
manifold learning.

The algorithms included in DADApy do not rely on
low dimensional projections or on any strong assump-
tions on the structure of the data. This can be a great
atvantage, as it makes DADApy suited to analyse topo-
logically complex data manifolds, but it also means
that DADApy cannot be used to build low dimensional
maps for data visualisation. Other shortcomings of
the software are in its level of maturity for industrial-
grade standards –DADApy is still a young software–
and in the relatively small number of algorithms imple-
mented in it.

We plan to improve DADApy by addressing both of
these issues. On the one hand we are working on the de-
velopment of algorithms that extend many of the meth-
ods discussed here, including ID estimators for discrete
spaces [46], density estimators that exploit data correla-
tions, and more reﬁned feature selection schemes based
on the information imbalance; and intend to implement
these as new DADApy methods. On the other hand
we intend to improve code quality in a variety of direc-
tions such as by increasing unit test coverage, expanding
documentation and lint checks, and adding static type
checking. Finally, we will greatly welcome open source
contributions to the project.

Code availability

DADApy can be downloaded from the Github
page https://github.com/sissa-data-science/
DADApy.

Acknowledgments

AG and AL acknowledge support from the European
Union’s Horizon 2020 research and innovation program
(Grant No. 824143, MaX ‘Materials design at the eX-
ascale’ Centre of Excellence).

The views and opinions expressed in this paper are
those of the authors and do not necessarily reﬂect the
oﬃcial policy or position of Banca d’Italia.

References

[1] K. T. Sch¨utt, S. Chmiela, O. A. von Lilienfeld, A. Tkatchenko,
K. Tsuda, K.-R. M¨uller, Machine learning meets quantum
physics, Lecture Notes in Physics (2020).

[2] A. Glielmo, B. E. Husic, A. Rodriguez, C. Clementi, F. No´e,
A. Laio, Unsupervised learning methods for molecular simula-
tion data, Chemical Reviews (2021).

[3] G. Carleo,

I. Cirac, K. Cranmer, L. Daudet, M. Schuld,
N. Tishby, L. Vogt-Maranto, L. Zdeborov´a, Machine learning
and the physical sciences, Reviews of Modern Physics 91 (4)
(2019) 045002.

[4] E. Keogh, A. Mueen, Curse of Dimensionality, Springer
doi:10.1007/

US, Boston, MA, 2010, pp. 257–258.
978-0-387-30164-8_192.
URL https://doi.org/10.1007/978-0-387-30164-8_
192

[5] C. C. Aggarwal, A. Hinneburg, D. A. Keim, On the surprising
behavior of distance metrics in high dimensional space, in: In-
ternational conference on database theory, Springer, 2001, pp.
420–434.

[6] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Per-
rot, E. Duchesnay, Scikit-learn: Machine learning in Python, J.
Mach. Learn. Res. 12 (2011) 2825–2830.

[7] H. Abdi, L. J. Williams, Principal component analysis, Wiley
interdisciplinary reviews: computational statistics 2 (4) (2010)
433–459.

[8] B. Sch¨olkopf, A. Smola, K.-R. M¨uller, Kernel principal com-
ponent analysis, in: International conference on artiﬁcial neural
networks, Springer, 1997, pp. 583–588.

[9] M. Balasubramanian, E. L. Schwartz, The isomap algorithm and

topological stability, Science 295 (5552) (2002) 7–7.

[10] P. Campadelli, E. Casiraghi, C. Ceruti, A. Rozza, Intrinsic
dimension estimation: Relevant techniques and a benchmark
framework, Math. Probl. Eng. 2015 (2015).

[11] F. Camastra, A. Staiano, Intrinsic dimension estimation: Ad-

vances and open problems, Inf. Sci. 328 (2016) 26–41.

[12] E. Levina, P. Bickel, Maximum likelihood estimation of intrin-
sic dimension, in: L. Saul, Y. Weiss, L. Bottou (Eds.), Advances
in Neural Information Processing Systems, Vol. 17, MIT Press,
2004.
URL https://proceedings.neurips.cc/paper/2004/
file/74934548253bcab8490ebd74afed7031-Paper.pdf
[13] F. Denti, D. Doimo, A. Laio, A. Mira, Distributional results
for model-based intrinsic dimension estimators, arXiv preprint
arXiv:2104.13832 (2021).

[14] E. Facco, M. d’Errico, A. Rodriguez, A. Laio, Estimating the
intrinsic dimension of datasets by a minimal neighborhood in-
formation, Scientiﬁc reports 7 (1) (2017) 1–8.

[15] A. Ansuini, A. Laio, J. H. Macke, D. Zoccolan, Intrinsic
dimension of data representations in deep neural networks, in:
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,
E. Fox, R. Garnett (Eds.), Advances in Neural Information
Processing Systems, Vol. 32, Curran Associates, Inc., 2019.
URL https://proceedings.neurips.cc/paper/2019/
file/cfcce0621b49c983991ead4c3d4d3b6b-Paper.pdf
[16] T. Mendes-Santos, X. Turkeshi, M. Dalmonte, A. Rodriguez,
Unsupervised learning universal critical behavior via the intrin-
sic dimension, Physical Review X 11 (1) (2021) 011040.
[17] R. Capelli, A. Gardin, C. Empereur-Mot, G. Doni, G. Pavan, A
data-driven dimensionality reduction approach to compare and
classify lipid force ﬁelds, The Journal of Physical Chemistry B
125 (28) (2021) 7785–7796.

9

[18] A. Rodriguez, M. d’Errico, E. Facco, A. Laio, Computing the
free energy without collective variables, Journal of chemical
theory and computation 14 (3) (2018) 1206–1215.

[19] D. O. Loftsgaarden, C. P. Quesenberry, A Nonparametric Esti-
mate of a Multivariate Density Function, The Annals of Math-
ematical Statistics 36 (3) (1965) 1049 – 1051. doi:10.1214/
aoms/1177700079.
URL https://doi.org/10.1214/aoms/1177700079
[20] J. Zhang, M. Chen, Unfolding hidden barriers by active
enhanced sampling, Phys. Rev. Lett. 121 (2018) 010601.
doi:10.1103/PhysRevLett.121.010601.
URL
PhysRevLett.121.010601

https://link.aps.org/doi/10.1103/

[21] F. Marinelli, J. D. Faraldo-G´omez, Force-correction analysis
method for derivation of multidimensional free-energy land-
scapes from adaptively biased replica simulations, Journal of
Chemical Theory and Computation 17 (11) (2021) 6775–6788,
pMID: 34669402. arXiv:https://doi.org/10.1021/acs.
jctc.1c00586, doi:10.1021/acs.jctc.1c00586.
URL https://doi.org/10.1021/acs.jctc.1c00586

[22] D. R. Salahub, Multiscale molecular modelling:

from elec-
tronic structure to dynamics of nanosystems and beyond, Phys.
Chem. Chem. Phys. 24 (2022) 9051–9081. doi:10.1039/
D1CP05928A.
URL http://dx.doi.org/10.1039/D1CP05928A

[23] A. Oﬀei-Danso, A. Hassanali, A. Rodriguez, High-dimensional
ﬂuctuations in liquid water: Combining chemical intuition with
unsupervised learning, Journal of Chemical Theory and Com-
putation 18 (5) (2022) 3136–3150, pMID: 35472272. arXiv:
https://doi.org/10.1021/acs.jctc.1c01292, doi:10.
1021/acs.jctc.1c01292.
URL https://doi.org/10.1021/acs.jctc.1c01292
[24] M. Carli, A. Laio, Statistically unbiased free energy estimates
from biased simulations, Molecular Physics 119 (19-20) (2021)
e1899323.

[25] C. Zeni, A. Anelli, A. Glielmo, K. Rossi, Exploring the robust
extrapolation of high-dimensional machine learning potentials,
Physical Review B 105 (16) (2022) 165141.

[26] A. Rodriguez, A. Laio, Clustering by fast search and ﬁnd of

density peaks, science 344 (6191) (2014) 1492–1496.

[27] M. d’Errico, E. Facco, A. Laio, A. Rodriguez, Automatic topog-
raphy of high-dimensional data sets by non-parametric density
peak clustering, Information Sciences 560 (2021) 476–492.
[28] C. Ziegler, S. Allon, S. Nyquist, I. Mbano, V. Miao, C. Tzoua-
nas, Y. Cao, A. Yousif, J. Bals, B. Hauser, et al., Sars-cov-2
receptor ace2 is an interferon-stimulated gene in human airway
epithelial cells and is detected in speciﬁc cell subsets across tis-
sues, Cell 181 (5) (2020) 1016–1035.

[29] N. Habib, Y. Li, M. Heidenreich, L. Swiech, I. Avraham-
Davidi, J. J. Trombetta, C. Hession, F. Zhang, A. Regev,
Div-seq: Single-nucleus rna-seq reveals dynamics of rare
adult newborn neurons, Science 353 (6302) (2016) 925–928.
arXiv:https://www.science.org/doi/pdf/10.1126/
science.aad7038, doi:10.1126/science.aad7038.
URL
science.aad7038

https://www.science.org/doi/abs/10.1126/

[30] P. Yger, G. L. Spampinato, E. Esposito, B. Lefebvre, S. Deny,
C. Gardella, M. Stimberg, F. Jetter, G. Zeck, S. Picaud,
J. Duebel, O. Marre, A spike sorting toolbox for up to thousands
of electrodes validated with ground truth recordings in vitro and
in vivo, eLife 7 (2018) e34518. doi:10.7554/eLife.34518.
URL https://doi.org/10.7554/eLife.34518

[31] Z. J. Sperry, K. Na, J. Jun, L. R. Madden, A. Socha, E. Yoon,
J. P. Seymour, T. M. Bruns, High-density neural recordings from
feline sacral dorsal root ganglia with thin-ﬁlm array, Journal of
Neural Engineering 18 (2020).

[32] P. Wang, B. Xu, J. Xu, G. Tian, C.-L. Liu, H. Hao, Semantic
expansion using word embedding clustering and convolutional
neural network for improving short text classiﬁcation, Neuro-
computing 174 (2016) 806–814.

[33] G. Margazoglou, T. Grafke, A. Laio, V. Lucarini, Dynamical
landscape and multistability of a climate model, Proceedings of
the Royal Society A 477 (2250) (2021) 20210019.

[34] G. Pinamonti, F. Paul, F. No´e, A. Rodriguez, G. Bussi, The
mechanism of rna base fraying: Molecular dynamics simula-
tions analyzed with core-set markov state models, The Journal
of chemical physics 150 (15) (2019) 154123.

[35] K. Jong, A. A. Hassanali, A data science approach to under-
standing water networks around biomolecules: The case of tri-
alanine in liquid water, The Journal of Physical Chemistry B
122 (32) (2018) 7895–7906, pMID: 30019898. doi:10.1021/
acs.jpcb.8b03644.
URL https://doi.org/10.1021/acs.jpcb.8b03644
[36] M. Carli, G. Sormani, A. Rodriguez, A. Laio, Candidate Bind-
ing Sites for Allosteric Inhibition of the SARS-CoV-2 Main
Protease from the Analysis of Large-Scale Molecular Dynam-
ics Simulations, The Journal of Physical Chemistry Letters 12
(2020) 65–72. doi:10.1021/acs.jpclett.0c03182.
[37] G. Sormani, A. Rodriguez, A. Laio, Explicit characteriza-
tion of the free-energy landscape of a protein in the space
of all its cα carbons, Journal of Chemical Theory and Com-
putation 16 (1) (2020) 80–87, pMID: 31809040.
arXiv:
https://doi.org/10.1021/acs.jctc.9b00800, doi:10.
1021/acs.jctc.9b00800.
URL https://doi.org/10.1021/acs.jctc.9b00800
[38] D. Doimo, A. Glielmo, A. Ansuini, A. Laio, Hierarchical nucle-
ation in deep neural networks, in: H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, H. Lin (Eds.), Advances in Neural
Information Processing Systems, Vol. 33, Curran Associates,
Inc., 2020, pp. 7526–7536.
URL https://proceedings.neurips.cc/paper/2020/
file/54f3bc04830d762a3b56a789b6ff62df-Paper.pdf
[39] A. Glielmo, C. Zeni, B. Cheng, G. Cs´anyi, A. Laio,
distance mea-
2022).

Ranking
(04
sures,
arXiv:https://academic.oup.com/pnasnexus/
article-pdf/1/2/pgac039/44246399/pgac039.pdf,
doi:10.1093/pnasnexus/pgac039.
URL https://doi.org/10.1093/pnasnexus/pgac039
[40] J. P. Darby, J. R. Kermode, G. Cs´anyi, Compressing local atomic
neighbourhood descriptors, arXiv preprint arXiv:2112.13055
(2021).

information
1

of
pgac039

content
(2),

PNAS Nexus

the

[41] S. Honda, K. Yamasaki, Y. Sawada, H. Morii, 10 residue folded
peptide designed by segment statistics, Structure 12 (8) (2004)
1507–1518. doi:10.1016/j.str.2004.05.022.

[42] M. Bonomi, D. Branduardi, G. Bussi, C. Camilloni, D. Provasi,
P. Raiteri, D. Donadio, F. Marinelli, F. Pietrucci, R. A. Broglia,
et al., Plumed: A portable plugin for free-energy calculations
with molecular dynamics, Computer Physics Communications
180 (10) (2009) 1961–1972.

[43] P. Cossio, A. Laio, F. Pietrucci, Which similarity measure is bet-
ter for analyzing protein structures in a molecular dynamics tra-
jectory?, Physical Chemistry Chemical Physics 13 (22) (2011)
10421–10425.

[44] K. A. McKiernan, B. E. Husic, V. S. Pande, Modeling the
mechanism of cln025 beta-hairpin formation, The Journal of
Chemical Physics 147 (10) (2017) 104107. doi:10.1063/1.
4993207.

[45] S. Honda, T. Akiba, Y. S. Kato, Y. Sawada, M. Sekijima,
M. Ishimura, A. Ooishi, H. Watanabe, T. Odahara, K. Harata,
et al., Crystal structure of a ten-amino acid protein, Journal of

10

the American Chemical Society 130 (46) (2008) 15327–15331.
doi:10.1021/ja8030533.

[46] I. Macocco, A. Glielmo, J. Grilli, A. Laio,

Intrinsic di-

mension estimation for discrete metrics,
arXiv:2207.09688 (2022).

arXiv preprint

11

