Evaluating the Practicality of
Learned Image Compression

Hongjiu Yu1, Qiancheng Sun1(cid:63), Jin Hu1(cid:63), Xingyuan Xue1, Jixiang Luo1, Dailan He1, Yilong Li1(cid:63), Pengbo Wang1(cid:63),
Yuanyuan Wang1, Yaxu Dai1, Yan Wang12‚Ä†, Hongwei Qin1
SenseTime Research1, Tsinghua University2
{yuhongjiu, sunqiancheng, hujin, xuexingyuan, luojixiang, hedailan, liyilong, wangpengbo}@sensetime.com
{wangyuanyuan, daiyaxu, wangyan1, qinhongwei}@sensetime.com, wangyan@air.tsinghua.edu.cn

2
2
0
2

l
u
J

9
2

]

V

I
.
s
s
e
e
[

1
v
4
2
5
4
1
.
7
0
2
2
:
v
i
X
r
a

Abstract‚ÄîLearned image compression has achieved extraordi-
nary rate-distortion performance in PSNR and MS-SSIM com-
pared to traditional methods. However, it suffers from intensive
computation, which is intolerable for real-world applications
and leads to its limited industrial application for now. In this
paper, we introduce neural architecture search (NAS) to design-
ing more efÔ¨Åcient networks with lower latency, and leverage
quantization to accelerate the inference process. Meanwhile,
efforts in engineering like multi-threading and SIMD have been
made to improve efÔ¨Åciency. Optimized using a hybrid loss of
PSNR and MS-SSIM for better visual quality, we obtain much
higher MS-SSIM than JPEG, JPEG XL and AVIF over all
bit rates, and PSNR between that of JPEG XL and AVIF.
Our software implementation of LIC achieves comparable or
even faster inference speed compared to jpeg-turbo while being
multiple times faster than JPEG XL and AVIF. Besides, our
implementation of LIC reaches stunning throughput of 145 fps
for encoding and 208 fps for decoding on a Tesla T4 GPU for
1080p images. On CPU, the latency of our implementation is
comparable with JPEG XL.

Index Terms‚Äîimage compression, neural architecture search,

quantization, throughput

I. INTRODUCTION

Learned image compression (LIC) has outperformed tradi-
tional methods like JPEG [1] and BPG [2] in terms of PSNR
and MS-SSIM. Its outstanding rate-distortion performance
mostly derives from jointly training all parameters instead
of handcrafted and independent module design. Hyperprior
and context frameworks [3]‚Äì[6] dramatically improve com-
pression gain across all ranges of bit rates. However, tradi-
tional methods have their own merits for industrial applica-
tions including smaller memory footprint, less computational
complexity and hardware-friendly characteristics. LIC has an
extremely high demand for hardware resources. Large model
size and intensive computation hinder real-world deployment
of LIC. Furthermore, correct decoding across platforms is not
guaranteed due to the non-determinism of model inference
and error can propagate catastrophically in entropy decod-
ing [7]. All these obstacles restrict commercial deployment
of LIC even though it obtains a better trade-off between bit
rate and PSNR or MS-SSIM. In this paper, we report the
performance of our optimized LIC implementation, which

(cid:63) This work is done when the marked authors are interns at SenseTime.
‚Ä† Corresponding author.

outperforms the fastest industrial-level codec implementation,
i.e., jpeg-turbo [8] regarding rate-distortion performance and
visual quality while achieving comparable speed. Compared
with the most competitive next-generation image codec JPEG
XL [9], our model shows superior compression performance
with several times of advantage regarding inference speed.

To obtain a practical LIC model, we apply neural architec-
ture search (NAS) and quantization to make LIC lighter and
faster. NAS has been proved useful in Ô¨Ånding efÔ¨Åcient models
for high-level vision tasks, such as object detection [10],
recognition [11] and segmentation [12], while researches on
NAS for low-level tasks such as denoising, super-resolution
and image compression are still in early stages. In this paper,
we aim to design a more efÔ¨Åcient network and hardware-
friendly architecture for LIC to enable its industrial deploy-
ment. Hardware-aware NAS is dedicated to designing promis-
ing networks for various devices.

Besides, quantization has exhibited promising results for
decreasing model size as well as accelerating inference. Meth-
ods for neural network quantization can be divided into two
categories of quantization-aware training (QAT) and post-
training quantization (PTQ). We exploit quantization not only
to accelerate the inference of LIC but also to ensure the
correctness of cross-platform decoding [7].

The paper is organized as follows. Section II introduces
related works. Section III gives an overview of our architecture
and pipeline. Section IV describes the details of our method.
Experimental results and discussion are given in section V,
and conclusion is drawn in section VI.

II. RELATED WORK

A. NAS

Search space, optimization methods and search strategy
are the major components of NAS. In this paper, we follow
the framework of BigNAS [13], which only trains one big
model and searches efÔ¨Åcient sub-networks for direct deploy-
ment without additional training. For optimization methods,
Yu et al. [14], [15] propose a slimmable neural network
with parameter sharing which can adaptively prune output
channels with little performance degradation. HyperNet [16]
trains an auxiliary network to generate weight parameters for
the trainable model. For search strategy, an effective evolution

 
 
 
 
 
 
Fig. 1: Framework of our architecture. x, ÀÜx are the original
image and the reconstructed one. y, ÀÜy, z, ÀÜz are latent variables.
¬µ, œÉ are mean and variance values of Gaussian distribution.
Q represents quantization. AE and AD are the arithmetic
encoding and decoding. The blue blocks are (transposed)
convolutional layers controlled by supernet in Fig. 2, and
k5s2 means 5 √ó 5 kernel with stride 2. ReLU, ReLU6 are
the activation functions.

method [17] is proposed to obtain architecture with higher
accuracy, and HyperBand [18] can assess candidate neural
architectures to avoid redundant
training which results in
worse performance.

B. Quantization

Quantization-Aware Training (QAT) [19]‚Äì[21] and Post-
Training Quantization (PTQ) [22]‚Äì[24] are two major cate-
gories of model quantization. QAT has attracted much at-
tention because it compensates for the quantization error in
an end-to-end optimization manner and achieves relatively
better performance even in extremely low bit-width scenarios.
Besides speeding up the model inference, the quantization
techniques are also used in LIC for another purpose: elimi-
nating the cross-platform inconsistency. Ball¬¥e et al. [7] Ô¨Årst
introduces a QAT-like integer-only arithmetic technique to
determinize the hyperprior model. Sun et al. [25] instead
adopts PTQ-based approaches and He et al. [26] extends the
deterministic inference to more elaborate joint autoregressive
models.

III. ARCHITECTURE

Our framework is shown in Fig. 1. We follow the pipeline
in [3] but use less complex activation functions and supernet
to search the parameters of convolution layers, whose weight
and bias are generated by supernet in Fig. 2.

IV. METHODOLOGY

A. Conditional and Hardware-aware NAS

Firstly we deÔ¨Åne the search space. The minimal and
maximal numbers of parameters to be searched are given
empirically, and the search stride is predeÔ¨Åned for each
convolutional layer. BigNAS can train several models at the
same time, and the sandwich rule with combinations of
min, random, random, max is embedded to guarantee the
effectiveness of sub-models, where min, random and max

Fig. 2: Supernet of our NAS architecture. w, b are the weight
and bias parameters. The notation kmsn means m √ó m
kernel with stride n. Search Space is the combination of
parameters for channels. Linear is the linear convolutional
layer. GeLU, Softmax are the activation functions where
GeLU (x) = 0.5x(1 + tanh [(cid:112) œÄ
2 (x + 0.004715x3)]), and
exi
Sof tmax(xi) =
i=0 exi , l is the length of input vector x.
The blue block is the convolution adopted in Fig. 1.

(cid:80)l

denote the sub-models with minimal, random and maximal
number of parameters. After training, we can directly deploy
sub-models for validation.

For each convolutional layer in Fig. 1, weight and bias
parameters are generated by supernet in Fig. 2. A group of
weights from the initial transposed convolution or convolution
layers will be combined by concat operation. Then the dot
product of concatenated tensor and latent variables extracted
from search space parameters by multiple layer perception
(MLP), which consists of a single Linear layer and a Softmax
layer, will be calculated to generate weight. Meanwhile, bias
will be directly generated by latent variables from MLP, which
consists of two Linear layers and a GeLU activation layer.
Then the original parameters are replaced with generated
weight and bias for training and testing.

During the searching process, we balance the rate-distortion
performance and computational complexity by taking FLOPS
and inference latency into consideration. FLOPS is calculated
to restrict the model size and a latency lookup table obtained
from the statistical characteristics of various operations on
speciÔ¨Åc hardware is used to constrain the inference latency.

B. Other Optimization

Efforts have been made in engineering, resulting in a
highly optimized software implementation. We leverage multi-
threading and instruction-level parallelism to accelerate our
implementation, reaching throughput higher than the recipro-
cal of latency. For entropy coding, multi-threading and AVX-
512 are also used for acceleration.

V. EXPERIMENTS

A. Setup

We train our model with 8000 images picked from Ima-
geNet [27] dataset and randomly crop each image to 256√ó256.

Convk5s2Convk5s2Convk5s2Convk5s2ReLU6ReLU6ReLU6TConvk4s4TConvk4s2TConvk4s2ReLUReLUConvk3s1Convk5s2Convk5s2ReLUReLUTConvk4s1ReLUTConvk4s2TConvk3s2ReLUùëÑAEADùëÑAEAD100111100111ùë•ùë•(cid:3548)ùë¶ùëßùë¶(cid:3548)ùëßÃÇùúá,œÉSupernetSupernetSupernetSupernet(T)ConvkmsnSearch SpaceLinearSoftmaxLinearGeLULinear(T)Convkmsn(T)Convkmsn(T)Convkmsn(T)Convkmsnùë§ùë§ùë§ùë§concatXùë§ùëèùë§: weightùëè:  biasX:  dot productFor evaluation, we use 24 Kodak [28] images.

For training settings, we set learning rate to 1e ‚àí 4 and train
the network for 2000 epochs. We follow [5] and use the mixed
quantization trick to train our network. Channel numbers in
main and hyper networks are obtained with NAS. In terms of
quantization experiments, we apply LSQ [21] to quantize all
layers in our network following the general quantization rules
of NVIDIA‚Äôs TensorRT Framework. We use the full precision
model to initialize weights. Then we train the network for 400
epochs with learning rate set to 1e‚àí4 and another 100 epochs
with learning rate decreased to its 1
10 .

For traditional codecs, we measure the latency for executing
the whole pipeline including disk i/o and png format conver-
sion for a fair comparison. For JPEG XL [9], we tune the
distance parameter for rate-distortion trade-off and leave other
parameters as default. For AVIF, we test with AOM codec by
adjusting its quantization step tuned by PSNR unless otherwise
speciÔ¨Åed.

To test the average latency of our LIC model and tradi-
tional codecs, we use 100 1080p images captured from a
video and measure its average processing time. For a fair
comparison, when comparing latency between LIC model and
jpeg-turbo [8], we test these images saved in bmp Ô¨Åle format
while we use images saved in png format when comparing
LIC model, JPEG XL and AVIF. Unless otherwise mentioned,
we test on a Intel(R) Core(TM) i9-10900X CPU @ 3.70GHz
and a Tesla T4 GPU for 1080p dataset with png format.

B. Rate Distortion Performance

As shown in Fig. 3a, our LIC model achieves superior per-
formance with a large margin regarding BPP-PSNR compared
with jpeg-turbo, especially in low bit rates. It also defeats
JPEG XL at low bit rates. Its performance is still comparable
to JPEG XL when bpp exceeds 1.0. For AVIF, our LIC model
is comparable at low bit rates but lags as bit rate increases.

Our model surpasses traditional codecs in terms of BPP-
MS-SSIM, as shown in Fig. 3b. MS-SSIM has a closer rela-
tionship with perceptual quality [29] and for deep networks,
they tend to obtain a larger advantage over traditional codecs
in terms of BPP-MS-SSIM, which can be veriÔ¨Åed from the
fact that both LIC (Learned Image Compression) and DVC
(Deep Video Compression) surpass traditional codecs in terms
of BPP-MS-SSIM before BPP-PSNR [4], [30]. We follow the
same rationale by using a hybrid loss of PSNR and MS-
SSIM [31] to optimize perceptual quality, which leads to
our model performing better than traditional codecs including
AVIF tuned by ssim by a large margin in terms of BPP-MS-
SSIM but worse than AVIF in terms of BPP-PSNR.

C. Speed Analysis

We compare the inference latency of our LIC model with
traditional codecs and the results are shown in Fig. 4. Our
model achieves 18/12ms for encoding/decoding latency, which
is at most 20% higher in encoding latency and at least 35%
lower in decoding latency than jpeg-turbo with bmp format
input. Compared with JPEG XL and AVIF, our LIC model is

(a) BPP-PSNR curves of codecs

(b) BPP-MS-SSIM curves of codecs

Fig. 3: Performance of our LIC model compared with tradi-
tional codecs with respect to bpp v.s. psnr and bpp v.s. ms-
ssim. ‚Äúavif-0.10.0 (ssim)‚Äù denotes AVIF tuned by ssim.

several times faster, reaching 50/19ms for encoding and de-
coding processes respectively. To further demonstrate the prac-
ticality of our implementation of the LIC model, we also test
its latency on a less expensive GPU (NVIDIA GeForce GTX
1660 SUPER) and a less powerful CPU (Intel(R) Core(TM)
i7-8700 CPU @ 3.20GHz). Encoding/decoding latency is
58/25ms. In addition, we have also tested the latency of our
LIC model on CPU, reaching encoding/decoding latency of
199/284ms, comparable to JPEG XL as shown in Fig.4b.
We leave the optimization of CPU deployment as future work.
In terms of throughput, with techniques mentioned in
Section IV-B, when processing images in parallel, our im-
plementation reaches stunning 145/208fps for bmp format
is saved in png format
input and 126/199fps when input
for encoding and decoding processes. On a less expensive
GPU (NVIDIA GeForce GTX 1660 SUPER), our model still
reaches throughput of 92/82fps.

We also report latency of each subnetwork in our model
in Table. I which is obtained using TensorRT‚Äôs trtexec bench-

0.00.10.20.30.40.50.60.70.80.91.01.11.2Bpp222426283032343638PSNR/dBBpp‚àíPSNRour LICjpeg-turbo-2.1.3avif-0.10.0avif-0.10.0 (ssim)jpeg-xl-0.7.00.00.10.20.30.40.50.60.70.80.91.01.11.2Bpp46810121416182022‚àí10Log10(1‚àíMSSSIM)/dBBpp‚àíMSSSIMour LICjpeg-turbo-2.1.3avif-0.10.0avif-0.10.0 (ssim)jpeg-xl-0.7.0(a) Comparison of latency between our LIC model and jpeg-
turbo for 1080p bmp format images.

(b) Comparison of latency between our LIC model, AVIF and
JPEG XL for 1080p png format images.

Fig. 4: Latency comparison of our LIC model and traditional
codecs. ‚ÄúQuality‚Äù stands for jpeg qp while ‚ÄúReference Bpp‚Äù
stands for bit rates on 1080p dataset for JPEG XL and AVIF.

GPU Type
Tesla T4
GeForce GTX 1660 SUPER

ga
7.25
10.08

gs
5.48
10.75

ha
1.15
1.68

hs
1.17
2.80

TABLE I: Latency of each subnetwork in our model. ga, gs,
ha and hs above refer to analysis and synthesis transforms of
main and hyperprior autoencoders.

marking tool with an input of resolution 1088 √ó 1920.

D. Memory Analysis

We show the memory footprint of our model during encod-
ing and decoding processes of 100 1080p images in Fig. 5,
peaking at 1.8GB and 1.5GB for CPU and GPU respectively.

E. Visual Quality

In this subsection, we compare the visual quality of our LIC
model with that of traditional codecs on Kodak [28] dataset.
As can be seen in Fig. 6, the image compressed by JPEG [1]
displays blocking artifacts while the image compressed by

Fig. 5: CPU and GPU memory usage during encoding and
decoding.

(a) Original image

(b) JPEG
44.4KB

(c) AVIF
43.75KB

(d) JPEG XL
44.35KB

(e) our LIC
44.44KB

Fig. 6: Visual quality comparison of kodim24.png [28]. Images
on the third row are obtained by zooming in the blue square
box region of images above.

AVIF is oversmoothed, losing details. Image compressed by
JPEG XL [9] suffers from blocking artifacts, similar to JPEG,
but to a lesser extent. The image compressed by our LIC
model, on the other hand, shows better perceptual quality.

F. NAS

For NAS, we reduce about 10% inference time while
improving rate-distortion performance. We get
lower bpp
(0.7281 ‚Üí 0.7278), with higher PSNR (33.5550 ‚Üí 34.0997)
and higher MS-SSIM (0.9876 ‚Üí 0.9880), but with lower
latency (15.2059 ‚Üí 13.9574) as shown in Table. II.

Structure
Origin
NAS
Performance
Origin
NAS

ga
176,246,176 246,176,176
48,96,112,176
32,120,104,220 248,224,256 236,200,220

ha

hs

gs
112,96,3
112,112,3

BPP
0.7281
0.7278

PSNR
33.5550
34.0997

MS-SSIM Lantency(ms)

0.9876
0.9880

15.2059
13.9547

TABLE II: Output channels of each (de)convolution block in
ga, gs, ha and hs and rate-distortion performance with latency
are listed above.

020406080100Quality1216202428323640Latency/msComparisonLatencyLICv.s.jpeg‚àíturboour LIC-encour LIC-decjpeg-turbo-2.1.3-encjpeg-turbo-2.1.3-dec0.40.81.21.62.0Reference Bpp0100200300400500600700Latency/msComparisonLatencyLICv.s.JPEGXLv.s.AVIFour LIC-encour LIC-decavif-0.10.1-encavif-0.10.1-decjpeg-xl-0.7.0-encjpeg-xl-0.7.0-dec012345Time/s0.00.20.40.60.81.01.21.41.61.8MemorySize/GBEncodingDecodingCPUMemoryUsage012345Time/s0.00.20.40.60.81.01.21.4MemorySize/GBEncodingDecodingGPUMemoryUsagemodel
FP32
Quantized
ga-ha-Quantized

bpp
0.728
0.704
0.716

psnr
33.55
32.91
33.49

ms-ssim
0.9876
0.9839
0.9873

TABLE III: BPP-PSNR-MS-SSIM of quantized models.
‚ÄúQuantized‚Äù stands for the model with all layers quantized,
evaluated on a NVIDIA GeForce GTX 1660 SUPER GPU.
‚Äúga-ha-Quantized‚Äù denotes the model with its main and hyper
encoders quantized, evaluated in pytorch‚Äôs fake quantize mode.

G. Quantization

We show the rate-distortion performance of the quantized
models in Table. III. We notice that quantizing main and
hyper encoders barely hurt performance while quantizing all
layers leads to a minor loss in rate-distortion performance.
As for speedup on a real device, quantization brings about
approximately 20% to 35% increase in throughput (images
processed in parallel) on a NVIDIA GeForce GTX 1660
SUPER GPU, from 92/82 fps to 114/112 fps.

VI. CONCLUSION

We evaluate the practicality of learned image compression
by optimizing and assessing rate-distortion performance, la-
tency, throughput, memory footprint and visual quality. We
compare these results with those of traditional codecs. LIC has
comparable or even faster inference speed compared with the
fastest industrial codec, i.e., jpeg-turbo, while being superior
in terms of rate-distortion performance and perceptual quality.
Besides, LIC achieves astonishing throughput of 145/208 fps
for 1080p images, making it applicable for industrial scenarios.
In addition, NAS and quantization show promising results for
obtaining more efÔ¨Åcient and more effective LIC models. All
these results show that learned image compression is becoming
ready for industrial applications. Future works will be devoted
to optimizing LIC for mobile devices.

REFERENCES

[1] G. K. Wallace, ‚ÄúThe jpeg still picture compression standard,‚Äù IEEE
Transactions on Consumer Electronics, vol. 38, no. 1, pp. xviii‚Äìxxxiv,
1992.

[2] F. Bellard, ‚ÄúBpg image format,‚Äù Available: https:// bellard.org/ bpg,

vol. 1, p. 2, 2015.

[3] J. Ball¬¥e, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, ‚ÄúVariational
image compression with a scale hyperprior,‚Äù International Conference
on Learning Representations, 2018.

[4] D. Minnen, J. Ball¬¥e, and G. Toderici, ‚ÄúJoint autoregressive and hierar-

chical priors for learned image compression,‚Äù vol. 31, 2018.

[5] D. Minnen and S. Singh, ‚ÄúChannel-wise autoregressive entropy models
for learned image compression,‚Äù in IEEE International Conference on
Image Processing.
IEEE, 2020, pp. 3339‚Äì3343.

[6] D. He, Z. Yang, W. Peng, R. Ma, H. Qin, and Y. Wang, ‚ÄúElic:
EfÔ¨Åcient
learned image compression with unevenly grouped space-
channel contextual adaptive coding,‚Äù arXiv preprint arXiv:2203.10886,
2022.

[7] J. Ball¬¥e, N. Johnston, and D. Minnen, ‚ÄúInteger networks for data
compression with latent-variable models,‚Äù in International Conference
on Learning Representations, 2018.

[8] I. J. Group, ‚ÄúAvailable: https://libjpeg-turbo.org,‚Äù 2022, accessed: 2022-

06-15.

[9] J. Alakuijala, R. Van Asseldonk, S. Boukortt, M. Bruse, I.-M. Coms, a,
M. Firsching, T. Fischbacher, E. Kliuchnikov, S. Gomez, R. Obryk et al.,
‚ÄúJpeg xl next-generation image compression architecture and coding
tools,‚Äù in Applications of Digital Image Processing XLII, vol. 11137.
SPIE, 2019, pp. 112‚Äì124.

[10] Y. Chen, T. Yang, X. Zhang, G. Meng, C. Pan, and J. Sun, ‚ÄúDetnas:
Neural architecture search on object detection,‚Äù Conference on Neural
Information Processing Systems, vol. 1, no. 2, pp. 4‚Äì1, 2019.

[11] S. Ding, T. Chen, X. Gong, W. Zha, and Z. Wang, ‚ÄúAutospeech:
Neural architecture search for speaker recognition,‚Äù Conference of the
International Speech Communication Association, 2020.

[12] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, ‚ÄúAu-
toaugment: Learning augmentation strategies from data,‚Äù in IEEE / CVF
Computer Vision and Pattern Recognition Conference, 2019, pp. 113‚Äì
123.

[13] J. Yu, P. Jin, H. Liu, G. Bender, P.-J. Kindermans, M. Tan, T. Huang,
X. Song, R. Pang, and Q. Le, ‚ÄúBignas: Scaling up neural architecture
search with big single-stage models,‚Äù in European Conference on
Computer Vision. Springer, 2020, pp. 702‚Äì717.

[14] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang, ‚ÄúSlimmable neural

networks,‚Äù arXiv preprint arXiv:1812.08928, 2018.

[15] J. Yu and T. S. Huang, ‚ÄúUniversally slimmable networks and improved
training techniques,‚Äù in International Conference on Computer Vision,
2019, pp. 1803‚Äì1811.

[16] D. Ha, A. Dai, and Q. V. Le, ‚ÄúHypernetworks,‚Äù arXiv preprint

arXiv:1609.09106, 2016.

[17] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, ‚ÄúRegularized evolution
for image classiÔ¨Åer architecture search,‚Äù in AAAI Conference on ArtiÔ¨Åcial
Intelligence, vol. 33, no. 01, 2019, pp. 4780‚Äì4789.

[18] L. Li, K. G. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar,
‚ÄúHyperband: Bandit-based conÔ¨Åguration evaluation for hyperparameter
optimization,‚Äù in International Conference on Learning Representations,
2017.

[19] J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V. Srinivasan,
and K. Gopalakrishnan, ‚ÄúPact: Parameterized clipping activation for
quantized neural networks,‚Äù arXiv preprint arXiv:1805.06085, 2018.

[20] R. Gong, X. Liu, S. Jiang, T. Li, P. Hu, J. Lin, F. Yu, and J. Yan,
‚ÄúDifferentiable soft quantization: Bridging full-precision and low-bit
neural networks,‚Äù in International Conference on Computer Vision,
2019, pp. 4852‚Äì4861.

[21] S. K. Esser, J. L. McKinstry, D. Bablani, R. Appuswamy, and
D. S. Modha, ‚ÄúLearned step size quantization,‚Äù arXiv preprint
arXiv:1902.08153, 2019.

[22] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort,
‚ÄúUp or down? adaptive rounding for post-training quantization,‚Äù in
International Conference on Machine Learning.
PMLR, 2020, pp.
7197‚Äì7206.

[23] Y. Li, R. Gong, X. Tan, Y. Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and
S. Gu, ‚ÄúBrecq: Pushing the limit of post-training quantization by block
reconstruction,‚Äù 2021.

[24] X. Wei, R. Gong, Y. Li, X. Liu, and F. Yu, ‚ÄúQdrop: Randomly
dropping quantization for extremely low-bit post-training quantization,‚Äù
International Conference on Learning Representations, 2022.

point arithmetic,‚Äù in Picture Coding Symposium.

[25] H. Sun, L. Yu, and J. Katto, ‚ÄúLearned image compression with Ô¨Åxed-
IEEE, 2021, pp. 1‚Äì5.
[26] D. He, Z. Yang, Y. Chen, Q. Zhang, H. Qin, and Y. Wang, ‚ÄúPost-
training quantization for cross-platform learned image compression,‚Äù
arXiv preprint arXiv:2202.07513, 2022.

[27] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:
A large-scale hierarchical image database,‚Äù in IEEE / CVF Computer
Vision and Pattern Recognition Conference.
IEEE, 2009, pp. 248‚Äì255.
[28] E. Kodak, ‚ÄúKodak lossless true color image suite (pho- tocd pcd0992),‚Äù

http://r0k.us/graphics/kodak, 1993.

[29] Z. Cheng, P. Akyazi, H. Sun, J. Katto, and T. Ebrahimi, ‚ÄúPerceptual
quality study on deep learning based image compression,‚Äù in IEEE
International Conference on Image Processing.
IEEE, 2019, pp. 719‚Äì
723.

[30] G. Lu, W. Ouyang, D. Xu, X. Zhang, C. Cai, and Z. Gao, ‚ÄúDvc: An end-
to-end deep video compression framework,‚Äù in IEEE / CVF Computer
Vision and Pattern Recognition Conference, 2019, pp. 11 006‚Äì11 015.

[31] B. Sun, M. Gu, D. He, T. Xu, Y. Wang, and H. Qin, ‚ÄúHlic: Harmonizing
optimization metrics in learned image compression by reinforcement
learning,‚Äù arXiv preprint arXiv:2109.14863, 2021.

