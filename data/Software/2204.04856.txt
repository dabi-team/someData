Defect Identification, Categorization, and Repair:
Better Together

CHAO NIâ€ , School of Software Technology, Zhejiang University, China
KAIWEN YANGâ€ , College of Computer Science and Technology, Zhejiang University, China
XIN XIAâˆ—, Software Engineering Application Technology Lab, Huawei, China
DAVID LOÂ§, Singapore Management University, Singapore
XIANG CHENâ€¡, School of Information Science and Technology, Nantong University, China
XIAOHU YANGâ€ , Computer Science and Technology,Zhejiang University, China

Just-In-Time defect prediction (JIT-DP) models can identify defect-inducing commits at check-in time. Even
though previous studies have achieved a great progress, these studies still have the following limitations: 1)
useful information (e.g., semantic information and structure information) are not fully used; 2) existing work
can only predict a commit as buggy one or clean one without more information about what type of defect it is;
3) a commit may involve changes in many files, which cause difficulty in locating the defect; 4) prior studies
treat defect identification and defect repair as separate tasks, none aims to handle both tasks simultaneously.
In this paper, to handle aforementioned limitations, we propose a comprehensive defect prediction and
repair framework named CompDefect, which can identify whether a changed function (a more fine-grained
level) is defect-prone, categorize the type of defect, and repair such a defect automatically if it falls into several
scenarios, e.g., defects with single statement fixes, or those that match a small set of defect templates. Generally,
the first two tasks in CompDefect are treated as a multiclass classification task, while the last one is treated as
a sequence generation task. The whole input of CompDefect consists of three parts (exampled with positive
functions): the clean version of a function (i.e., the version before defect introduced), the buggy version of a
function and the fixed version of a function. In multiclass classification task, CompDefect categorizes the type
of defect via multiclass classification with the information in both the clean version and the buggy version. In
code sequence generation task, CompDefect repairs the defect once identified or keeps it unchanged.

To verify the effectiveness of CompDefect, we first build a large-scale function-level dataset named
Function-SStuBs4J, which totally contains 21,047 instances with three versions of modified functions and we
then evaluate CompDefect with state-of-the-art approaches in various settings. Experimental results indicate
the promising performance of CompDefect over a set of benchmarks. Specifically, for defect identification
task, on average, CompDefect improves DeepJIT and CC2Vec by 39.0% and 41.7%, by 34.7% and 37.3% in terms
of F1-score and AUC, respectively. For defect categorization task, on average, CompDefect also improves
pre-trained models (i.e., BERT, RoBERTa and CodeBERT) by at least 63.0%. For defect repair task, CompDefect

âˆ—Xin Xia is the corresponding author.
Chao Ni and Kaiwen Yang have equal contribution.

2
2
0
2

r
p
A
1
1

]
E
S
.
s
c
[

1
v
6
5
8
4
0
.
4
0
2
2
:
v
i
X
r
a

Authorsâ€™ addresses: Chao Ni, School of Software Technology, Zhejiang University, Hangzhou, Zhejiang, China, 310007,
chaoni@zju.edu.cn; Kaiwen Yang, College of Computer Science and Technology, Zhejiang University, Hangzhou, China,
kwyang@zju.edu.cn; Xin Xia, Software Engineering Application Technology Lab, Huawei, Hangzhou, China, xin.xia@acm.
org; David Lo, Singapore Management University, Singapore, davidlo@smu.edu.sg; Xiang Chen, School of Information
Science and Technology, Nantong University, Nantong, China, xchencs@ntu.edu.cn; Xiaohu Yang, Computer Science and
Technology,Zhejiang University, Hangzhou, China, yangxh@zju.edu.cn.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Â© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
1049-331X/2021/4-ART39 $15.00
https://doi.org/0000000.0000000

39

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

 
 
 
 
 
 
39:2

Chao Ni and Kaiwen Yang, et al.

still outperforms SequenceR and make an improvement by 23.9% and 29.5% in terms of BLEU and Accuracy,
respectively.

Additional Key Words and Phrases: Just-in-time Defect Prediction, Defect Categorization, Defect Repair

ACM Reference Format:
Chao Ni, Kaiwen Yang, Xin Xia, David Lo, Xiang Chen, and Xiaohu Yang. 2021. Defect Identification, Catego-
rization, and Repair:
Better Together. ACM Trans. Softw. Eng. Methodol. ?, ?, Article 39 (April 2021), 22 pages. https://doi.org/0000000.
0000000

1 INTRODUCTION
Software development process is evolving rapidly with frequently changing requirement, various
development environment, and diverse application scenarios. It tends to release software versions
in a short-term period. Such a rapid software development process and limited Software Quality
Assurance (SQA) resources have formed a strong contradiction. Thus, many continuous code quality
tools (e.g., CI/CD, static analysis) have been widely adopted [33]. However, SQA teams cannot
effectively inspect every commit with limited SQA resources. Therefore, it is important to identify
defects as early as possible to prioritize limited resource (e.g., time and effort) on specific program
modules.

Just-In-Time defect prediction (JIT-DP) [14, 20, 26, 33] is a novel technique to predict whether
a commit will introduce defects in the future and it can help practitioners prioritize limited SQA
resources on the most risky commits during the software development process. Compared with
coarse-grained level (i.e., class/file/module) defect prediction approaches, JIT-DP works at the
fine-grained level to provide hints about potential defects. Though many approaches have been
proposed to make a great process in JIT-DP, there still has several limitations in previous work.

â€¢ Code semantic information and code structure information are not fully used. Many
approaches [16, 28, 44] are proposed based on the commit-level metrics proposed by Kamei et
al. [20]. These metrics are quantitative indicators of modified codes without considering the
semantic of codes. Recently, deep learning based approaches [14, 15] consider the semantic
(i.e., the code tokensâ€™ implication around its modified context) and the structure information
(e.g., the relation among commits, â„ğ‘¢ğ‘›ğ‘˜s1, modified files, modified lines, and tokens) of a
change. However, the semantic information is not comprehensive [14] but only represents the
modified lines and is part of the understanding of the code commit. The structure information
is not real representation of code structure [15] but only represents the structure of git diff .
Therefore, the semantic information (e.g., the code tokensâ€™ implication around its modified
or unmodified context) and the structure information (e.g., data flow information of code)
should be deeply excavated and fully used simultaneously.

â€¢ Prediction type is coarse-grained. Existing work [14, 17, 20] can only predict whether a
commit is buggy or clean without more information about the type of defect. Information
about the defect type (e.g., Missing Throws Exception and Less Specific If ) may help developers
better understand the categories of defect and consequently help fix it.

â€¢ Location of the defect is not accurate enough. Currently, JIT-DP approaches can only
predict the defect-proneness of a commit. However, a commit may involves several hunks
which may modify a few functions existing in many files. Therefore, it may be unclear where
the defect exactly exists if a commit is predicted as buggy one.

â€¢ Solution to automatically fix defects once identified is scarcely provided. Some ap-
proaches [20, 33] focus on defect identification, while some approaches [5, 9] focus on defect

1https://git-scm.com/

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

Defect Identification, Categorization, and Repair:
Better Together

39:3

repair. However, none of prior works treat defect identification task and defect repair task
simultaneously.

In this paper, to address these limitations, we propose a comprehensive defect prediction and
repair framework named CompDefect by building a multi-task deep learning model, which can
identify whether a changed function in a commit is defect-prone, categorize the type of defect,
and repair the defect automatically. Considering that defect repair is an important but difficult
software engineering problem, in this paper, we focus on simple types of defect [5, 22, 34, 40], such
as defects with single statement fixes, or that match a small set of defect templates. Therefore,
to make CompDefect usable in practical applications, we simplify the usage scenario to single
statement defects. In general, CompDefect consists of two stages: an offline learning stage and
an online application stage. In the offline learning stage, we first build a large-scale function-level
dataset named Function-SStuBs4J by extracting the function body where the modified lines exists in
ManySStuBs4J dataset, which is originally collected by Rafael-Michael et al. [22] from 1,000 popular
open-source Java projects and summarized into 16 defect patterns. In particular, we need to extract
three versions of each function body context where the modified lines exists in a commit: the clean
version, the buggy version, and the fixed version. The clean version means the version before the
defect was introduced, the buggy version means the version when defect was introduced into
the function, and the fixed version means the version when the defect was fixed. Then, the three
versions are treated as the input of CompDefect to complete two main tasks: (1) a multiclass
classification task and (2) a code sequence generation task. The first task can help to identify
the buggy function and categorize the type of defect, while the second task can generate the
patch to repair the defect. In online application stage, for a given commit, we firstly identify the
modifications in each â„ğ‘¢ğ‘›ğ‘˜. Then, for each modification, we extract the function body where the
modification exists. After that, we extract the corresponding previous function body before the
modification occurs. Finally, each modified function has two versions of function body (i.e., the
current version and the version before the modification introduced), which are fitted into the
trained CompDefect to predict its defect-proneness and subsequently repair it once identified as
defect-inducing one. Moreover, since CompDefect identifies the defect-proneness of each function
where the modification occurs in a commit, CompDefect can locate the bug in a commit in a more
fine-grained way. That is, CompDefect locates the defect at hunk-level rather than commit-level.
To verify the effectiveness of our proposed model CompDefect, we conduct a comprehensive
studies on Function-SStuBs4J with state-of-the-art approaches on a few tasks involving: defect
identification task [14, 15], defect categorization task [7], and defect repair task [5]. Comparing
with several state-of-the-art baselines, the superiority of CompDefect is highlighted. In particular,
for defect identification task, on average, CompDefect improves DeepJIT and CC2Vec by 39.0% and
41.7%, by 34.7% and 37.3% in terms of F1-score and AUC, respectively. For defect categorization task,
on average, CompDefect also improves pre-trained models (i.e., BERT, RoBERTa and CodeBERT)
by at least 63.0%. For defect repair task, CompDefect still outperforms SequenceR and make an
improvement by 23.9% and 29.5% in terms of BLEU and Accuracy, respectively.

In summary, this paper makes the following main contributions:

â€¢ Technique. We propose a comprehensive defect prediction and repair framework named
CompDefect for function-level software maintenance, which can automatically identify the
defect-proneness of a changed function in a commit, categorize the type the defect, and repair
defect with appropriate patch by studying the relations among three different versions of the
changed method/function. CompDefect can fully use both the code semantic information
(e.g., code tokensâ€™ context) and code structure information (e.g., data flow) of the changed
function simultaneously and the experimental study also highlights the promising ability on

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

39:4

Chao Ni and Kaiwen Yang, et al.

a set of software maintenance tasks: defect identification, defect categorization, and defect
repair.

â€¢ Dataset. We extend and purify a new function-level simple statement dataset named Function-
SStuBs4J on the basic of ManySStuBs4J. This dataset can provide more contextual information
of modified functions in commits. To the best of our knowledge, this is the first function-level
multiclass dataset which can provide comprehensive information of a changed function in a
commit and its corresponding repaired patch.

â€¢ Replication. We have also released our replication package including the extended dataset
and the source code of CompDefect, to facilitate other researchers and practitioners to
replicate our work and evaluate their ideas. This replication package is now publicly
available on Zenodo.

https://zenodo.org/record/5353354#.YS8iYtMzZhE

The rest of the paper is organized as follows. Section 2 introduces the details of our proposed
model CompDefect including the tasks definition and technical details. Section 3 describes the
experimental setting involving dataset, baselines, evaluation metrics, experiment setup and research
questions we want to investigate. Following that, the results to research questions and analysis
are provided in Section 4. Then, the threats to validity are described in Section 5. Finally, the
related work and conclusion of this paper are subsequently presented in Section 6 and Section 7,
respectively.

2 COMPDEFECT: AUTOMATIC SOFTWARE DEFECT IDENTIFICATION,

CATEGORIZATION AND REPAIR

Fig. 1. (a) The overall framework of CompDefect. (b) The detail of encoder. (c) The detail of decoder.

Multi-task deep learning model [6] aims at training with the same dataset for multiple tasks
simultaneously by using the shared representations to learn the common ideas between a few
related tasks, which consequently increase efficiency the data and potentially accelerate learning
speed for related or downstream tasks. In this paper, our CompDefect is a typical multi-task deep
learning model, which aims at automatically identifying whether a modification to a function will
introduce defect, categorize the type of defect and repairing such a defect by generating patches,
simultaneously. Generally, the first two tasks in CompDefect is treated as a multiclass classification
task, while the last one is a sequence generation task. The classification task and the generation
task share one common encoder to extract semantic structure information from the function body.

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

...Encoder[CLS]NTNFFN...............Multi-class Classification...............CLSbuggyCLScleanDecoderSequence GenerationVariable SequenceClean CodeTokens[SEP][CLS]Variable SequenceBuggy CodeTokens[SEP][CLS]Fixed Code TokensCross AttentionMasked Multi-Head AttentionAdd & NormMulti-Head AttentionAdd & NormFeed ForwardAdd & NormLinearLinearLinearDecoderEncoderVariable SequencexxxGraphCodeBERTVariable-alignment across source code and data flowData flow edge among variablesCodeTokens[SEP]InputEmbedding[CLS]v1v2v3v4...1234[CLS]T1Tn[SEP]1234COMPDEFECT(a)(b)(c)Defect Identification, Categorization, and Repair:
Better Together

39:5

When executing the generation task, the knowledge of the classification task is utilized to help
generate a better code repair.

In the rest of this section, we first give the definition of the two tasks. Subsequently, we introduce
details of CompDefect, including its encoder and decoder. We use GraphCodeBERT [11] as the
encoder of CompDefect and use Transformer decoder [38] as the decoder of CompDefect. For
training CompDefect, we need three versions of a function as the input. That is the clean version
of a function, the buggy version of a function and the fixed version of a function. The buggy version
represents the state when the defect was introduced into the function, the clean version represents
the state before the defect was introduced, and the fixed version represents the state when the
defect was fixed. The overall framework of our approach is illustrated in Fig. 1, which contains
three sub-figures. The left one (i.e., Fig. 1(a)) illustrates the structure of CompDefect, while the
other two represent the encoder (i.e., Fig. 1(b) is GraphCodeBERT) and decoder (i.e., Fig. 1(c) is
Transformer Decoder) used in our proposed CompDefect, respectively. Following that, we define
the target multi-task object function since CompDefect tries to maximize two objective functions
of tasks. Finally, we illustrate the workflow how CompDefect works in the application usage.

2.1 Task Definition
2.1.1 Defect Identification and Categorization: Multiclass Classification. The first task of CompDe-
fect is to automatically identify defect in each changed function and categorize the type of defect.
To do so, CompDefect needs to give the types of defect if it is identified as a defect-prone one. We
formulate this task as a multiclass classification learning problem. For a given hunk in a commit, let
Funcbuggy be the source code of buggy version of the specific function where the modified statement
exists, Funcclean be the source code of clean version of corresponding function before the defect is
introduced.

The target is to automatically determine the status ğ‘¦ğ‘– of the change between the two versions of
the function. Let ğ‘¦ğ‘– represents the label of the function (i.e., clean or the type of defect) after the
(cid:11)
modification is introduced. In particular, our goal is to build a model ğœƒ with (cid:10)Funcclean, Funcbuggy
tuple to maximize the probability ğ‘ƒğœƒ (cid:0)ğ‘¦ğ‘– | (cid:10)Funcclean, Funcbuggy
(cid:11) ; ğ‘– âˆˆ [1, ğ‘›ğ‘¢ğ‘š_ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘ ](cid:1), over training
dataset. Mathematically, our task is defined as finding ğ¿1(ğ‘‹ ), such that:

L1 (X ) = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘¦ğ‘–

âˆ‘ï¸ ğ‘™ğ‘œğ‘” ğ‘ƒğœƒ (cid:0)ğ‘¦ğ‘– | (cid:10)ğ¹ğ‘¢ğ‘›ğ‘ğ‘ğ‘™ğ‘’ğ‘ğ‘›, ğ¹ğ‘¢ğ‘›ğ‘ğ‘ğ‘¢ğ‘”ğ‘”ğ‘¦ (cid:11) ; ğ‘– âˆˆ [1, ğ‘›ğ‘¢ğ‘š_ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘ ](cid:1)

(1)

ğ‘ƒğœƒ (cid:0)ğ‘¦ğ‘– | (cid:10)Funcclean, Funcbuggy

(cid:11) ; ğ‘– âˆˆ [1, ğ‘›ğ‘¢ğ‘š_ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘ ](cid:1) can be seen as the conditional likelihood of

predicting the status ğ‘¦ğ‘– given the (cid:10)Funcclean, Funcbuggy

(cid:11) input tuple.

2.1.2 Defect Repair: Sequence Generation. The second task is to immediately and automatically
generate the fixed patch if a changed function is identified as a bug-inducing one. It is a sequence
generation task and the source is buggy function while the target is the corresponding fixed ones,
denoted as ğ‘Œ = {ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘ }. Let ğ‘Œğ‘—âˆ’1 be the generated sequence outputted by the decoder
of CompDefect at position of j-1, mathematically ğ‘Œğ‘—âˆ’1 = {ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ ğ‘—âˆ’1} , and XEncoder be the
encoded output of the encoder with two inputs (i.e., Funcclean, Funcbuggy). Therefore, our task is to
maximize the probability ğ‘¦ğ‘˜
ğ‘— , ğ‘— âˆˆ [1, ğ‘ ], ğ‘˜ âˆˆ [1, vocab_size] with âŸ¨ğ‘Œğ‘—âˆ’1, XEncoder âŸ© tuple over training
dataset. Mathematically, our task is defined as finding ğ¿2(ğ‘‹ ), such that:

L2 (X ) = argmaxyk

j

âˆ‘ï¸

log Pğœƒ

(cid:16)

yk
j

| âŸ¨Yjâˆ’1, XEncoder âŸ©; j âˆˆ [1, N ]; k âˆˆ [1, vocab_size]

(cid:17)

(2)

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

39:6

ğ‘ƒğœƒ

(cid:16)
ğ‘¦ğ‘˜
ğ‘—

| âŸ¨ğ‘Œğ‘—âˆ’1, ğ‘‹ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ âŸ©; ğ‘— âˆˆ [1, ğ‘ ]; ğ‘˜ âˆˆ [1, vocab_size]

(cid:17)

can be seen as the conditional likelihood

Chao Ni and Kaiwen Yang, et al.

of predicting the status ğ‘¦ğ‘˜

ğ‘— given the âŸ¨Yjâˆ’1, XEncoder âŸ© input tuple.

2.2 Classification Task
As for this task, we use GraphCodeBERT [11] as the encoder of CompDefect, which is a pre-trained
model for programming language considering the structure of code. In particular, it utilizes the
semantic-level information of code (i.e., data flow) for pretraining instead of using syntactic-level
structure of code (i.e., abstract syntax tree). Data flow of a code is represented as a graph, in which
nodes represent variables, while edges represent the relation among variables. GraphCodeBERT
learns a good code representation from source code and its code structure through completing two
structure-aware pre-training tasks: data flow edges prediction and variable-alignment across source
code and data flow. GraphCodeBERT is designed on the basic of Transformer neural architecture [38]
by introducing a graph-guided masked attention function to integrate the code structure.

The input of CompDefectâ€™s encoder involves two function bodies: clean version of function and
buggy version of function. Then, we use tree sitter2, a parser generator tool and an incremental
parsing library, to transform the two functions into code tokens to build the data flow graph
(DFG), which can help to construct the dependencies among variables. In particular, for a source
code ğ¶ = {ğ‘1, ğ‘2, . . . , ğ‘ğ‘š }, CompDefect first parses the source code into an abstract syntax tree
(AST), which includes syntax information of the code. Besides, the leaves in AST are used to
identify the sequence of variable ğ‘‰ = {ğ‘£1, ğ‘£2, . . . , ğ‘£ğ‘˜ }. Therefore, the variable is treated as a node
of the graph, while the relationship between ğ‘£ğ‘– and ğ‘£ ğ‘— is treated as an directed edge from ğ‘£ğ‘– to
ğ‘£ ğ‘— of graph ğ‘’ = (cid:10)ğ‘£ğ‘–, ğ‘£ ğ‘— (cid:11), which means the value of ğ‘£ ğ‘— comes from ğ‘£ğ‘– . We denote the collection of
directed edges as ğ¸ = {ğ‘’1, ğ‘’2, . . . , ğ‘’ğ‘™ } and consequently the graph is denoted as ğº (ğ¶) = (ğ‘‰ , ğ¸), which
represents dependency relationship among variables in the source code ğ¶. Then, for the clean
version of function, we concatenate clean code and the collection of variables as the input sequence
Xclean = {[CLS], Cclean, [SEP], Vclean} and accordingly, the buggy version of function is transformed
as Xbuggy = {[CLS], Cbuggy, [SEP], Vbuggy }. [ğ¶ğ¿ğ‘†] is a special token in front of the whole sequence,
[ğ‘†ğ¸ğ‘ƒ] is another special token to split two kinds of data types. After that, we concatenate Xclean
and Xbuggy vertically to assemble the whole input ğ‘‹ for CompDefect.

CompDefect takes the sequence ğ‘‹ as the input and transforms it into input vectors ğ» 0. This
vector will be transformed for ğ‘ times, that is, H n = transformn (H nâˆ’1), n âˆˆ [1, N ], in which ğ‘
represents the maximum number of layers in encoder. Finally, we have the semantic structure
embedding representation in the last hidden layer, ğ» ğ‘ =
ğ¶ =
{â„ğ‘
ğ‘ğ‘› } and â„ğ‘
, ...â„ğ‘
ğ‘£ğ‘› }. Therefore, we get the contextualized representation of
ğ‘1
the buggy function H N

, ..., â„ğ‘
ğ‘‰ = {â„ğ‘
ğ‘£1
buggy and the clean function H N

, in which â„ğ‘

ğ¶ , â„ğ‘

, â„ğ‘
ğ‘£2

clean.

, â„ğ‘
ğ‘2

, â„ğ‘
ğ‘‰

(cid:110)
â„ğ‘

[ğ¶ğ¿ğ‘† ]

[ğ‘†ğ¸ğ‘ƒ ]

, â„ğ‘

(cid:111)

To learn the relation between the vector of the clean function hN

and the vector of buggy
, we adopt neural tensor network (i.e., NTN, denoted as Î“) and denote the relation
function hN
as hNT . The three vectors are combined as hchange, which be fed into feed forward neural network for
feature fusion. CompDefect finally outputs the probability of each class after a softmax operating
on the result of tanh activation function. More precisely, the classification operation sequence is

[CLS]buggy

[CLS]clean

2https://tree-sitter.github.io/tree-sitter/

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

Defect Identification, Categorization, and Repair:
Better Together

formulated as follows:

â„ğ‘ğ‘‡ = ReLU

(cid:18)(cid:16)

â„ğ‘

[CLS]buggy
â„change = â„NT âŠ• â„ğ‘

ğ‘’ = tanh (cid:0)FFN (cid:0)â„change

(cid:17) T

Î“ [1,...,ğ‘›]â„ğ‘

[CLS]clean

(cid:19)

+ ğ‘NT

âŠ• â„ğ‘
[CLS]buggy
(cid:1)(cid:1) = tanh (cid:0)ğ‘Šğ‘’â„change + ğ‘ğ‘’ (cid:1)

[CLS]clean

ğ‘¦ = Softmax(ğ‘) = Softmax(FFN(e)) = Softmax(ğ‘Šğ‘ğ‘’ + ğ‘ğ‘’ )

39:7

(3)

(4)

(5)

(6)

To train a good model for our task, we reuse the pre-trained weights of GraphCodeBERT from

Hugging Face3 to initialize the encoder and fine-tune on our collected dataset.

2.3 Sequence Generation Task
As for this task, we use Transformer decoder [38] as the decoder of CompDefect, which can
generate fix patch for the identified buggy function. The decoder of CompDefect consists of two
phases: training phase and generating phase. In the former phase, the decoder starts from special
token [SOS] to generate fixed code tokens according to the language model. For the result in ğ‘—-th
position ğ‘¦ ğ‘— , the input of decoder is the output of CompDefectâ€™s encoder and the output generated
by decoder in (ğ‘—-1)-th position, denoted as ğ‘Œğ‘—âˆ’1 = (cid:8)ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ ğ‘—âˆ’1
ğ‘— and
subsequently transformed into ğ» ğ‘›
), ğ‘› âˆˆ [1, ğ‘ ] by the decoder network. ğ‘ is the
ğ‘—
maximum number of layers in decoder. Following that, we use softmax function to convert the
output values based on last hidden state into probabilities to choose the most likely token from a
vocabulary. More precisely,

(cid:9). ğ‘Œğ‘—âˆ’1 is converted into ğ» 0

ğ‘— = transfomn (ğ» ğ‘›âˆ’1

ğ‘’ ğ‘— = tanh

(cid:17)(cid:17)

(cid:16)

FFN

(cid:16)
ğ» ğ‘
ğ‘—
ğ‘ ğ‘— = FFN (cid:0)ej

= tanh
(cid:1) = ğ‘Š ğ‘’ ğ‘— + ğ‘

(cid:16)
ğ‘Š ğ» ğ‘

ğ‘— + ğ‘

ğ‘¦ ğ‘— = softmax (cid:0)ğ‘ ğ‘— (cid:1)

(cid:17)

(7)

(8)

(9)

in which, FFN represents a feed-forward network and tanh is used as the activation function. To
fully use the information embedded in encoder, we use a cross multi-head attention vector ğ‘’cross:
(10)

ğ‘’cross = ğ‘’ âŠ• â„ğ‘

âŠ• â„ğ‘

âŠ• â„ğ‘

ğ¶buggy

[SEP ]buggy

ğ‘‰buggy

Notice that we use ğ‘’, the vector in the encoder phase of CompDefect before softmax operation in
to except this attention to focus on the knowledge of classification

Equation (5), instead of â„ğ‘
tasks when performing generation tasks.

[CLS]buggy

In the generation phase, it can be used to generate patches on testing dataset. In the process of
generation, for generating ğ‘¦ ğ‘— , we use beam search strategy to generate multiple potential patches
ğ‘Œğ‘—âˆ’1, as done in prior work [5, 37]. Beam search works through memorizing the ğ‘› best sequences up
to the current state of decoder. ğ‘›, set as 10 in our study, is commonly referred to as width or beam
size, and an infinite ğ‘› beam search corresponds to a complete breath-first-search. The successors of
these memorized states are computed and sorted based on their cumulative probability. Then, the
next ğ‘› best sequences are passed to the next state of decoder. When evaluating CompDefect, we
only choose the sequence with the highest probability.

2.4 Optimization of CompDefect
When a model has more than one task, a few task-specific objective functions need to be combined
into a single aggregated one that the model tries to maximize it. Therefore, it is extremely important

3https://huggingface.co/

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

39:8

Chao Ni and Kaiwen Yang, et al.

to exactly combine various objective functions into one that is the most suitable for multi-task
learning. In CompDefect, there mainly exists two tasks: multiclass classification task and sequence
generation task. Since in our usage scenario (i.e., function-level software defect prediction and defect
repair), we think the two tasks are equivalently important. Thus, CompDefect addresses the multi-
task optimization by balancing the individual objective functions for the two different tasks. That
is, the final optimization function(ğ¿3(ğ‘‹ )) is the sum of two individual objective functions(ğ¿1(ğ‘‹ )
for classification task, and ğ¿2(ğ‘‹ ) for sequence generation task), which is the one that CompDefect
tries to maximize. Formally,

ğ¿3(ğ‘‹ ) = ğ¿1(ğ‘‹ ) + ğ¿2 (ğ‘‹ )

(11)

Notice that our model CompDefect is flexible, we can optimize our model based on how
developers perceive the importance of the two tasks by changing the weights of the objective
functions.

2.5 The Workflow of CompDefect in Application Stage

Fig. 2. The workflow of CompDefect in application stage.

We illustrate how CompDefect can be used to process a new commit in Fig. 2. The figure
provides an example of a commit(7ebbfdf26432552edeb1057555fb51596a1ca5b6) from ğ‘ ğ‘œğ‘›ğ‘ğ‘Ÿğ‘ğ‘¢ğ‘ğ‘’
project. Given a commit, CompDefect firstly identifies hunks. Then, it figures out the corresponding
function body where the modification exists. After that, it filters out those hunks whose modification
is not a single-statement one or the modification occurs outside the scope of a function. Following
that, CompDefect extracts the previous function body of the current version using PyDriller [35].
Finally, the two versions are fed into CompDefect to judge whether it has a defect, to categorize
the type of defect, and to repair it automatically when it is identified as the defective one.

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

commit 7ebbfdf26432552edeb1057555fb51596a1ca5b6Author: Duarte Meneses<duarte.meneses@sonarsource.com>Date:   Fri Aug 2 15:42:49 2019 -0500Set value nullable in new_code_periodsdiff --git a/server/sonar-db-migration/src/â€¦â€¦/CreateNewCodePeriodTable.javab/server/sonar-db-migration/src/â€¦â€¦/CreateNewCodePeriodTable.javaindex 445074eb13..f2b7a1c1cd 100644---a/server/sonar-db-migration/â€¦../CreateNewCodePeriodTable.java+++ b/server/sonar-db-migration/â€¦â€¦/CreateNewCodePeriodTable.java@@ -69,7 +69,7 @@ public class CreateNewCodePeriodTableextends DdlChange{private static final VarcharColumnDefVALUE = newVarcharColumnDefBuilder().setColumnName("value")-.setIsNullable(false)+    .setIsNullable(true).setLimit(40).build();diff --git a/server/sonar-db-migration/â€¦â€¦/PopulateInitialSchemaTest/v79.sql b/server/sonar-db-migration/src/â€¦â€¦/PopulateInitialSchemaTest/v79.sqlindex c6b04cd998..55efd80031 100644---a/server/sonar-db-migration/src/â€¦../PopulateInitialSchemaTest/v79.sql+++ b/server/sonar-db-migration/src/â€¦../PopulateInitialSchemaTest/v79.sql@@ -866,7 +866,7 @@ CREATE TABLE "NEW_CODE_PERIODS" ("PROJECT_UUID" VARCHAR(40),"BRANCH_UUID" VARCHAR(40),"TYPE" VARCHAR(30) NOT NULL,-"VALUE" VARCHAR(40) NOT NULL,+  "VALUE" VARCHAR(40),"UPDATED_AT" BIGINT NOT NULL,"CREATED_AT" BIGINT NOT NULL,-.setIsNullable(false)+    .setIsNullable(true)-"VALUE" VARCHAR(40) NOT NULL,+  "VALUE" VARCHAR(40),ThemodificationisnotrelatedtoJava.Current function bodyThe function body before modification occursBeforeCommit in timeTwo versions of function body are fed into COMPDEFECTExtracting corresponding function bodiesFilter out unsuitable candidatesIdentify modificationsA new commit12345Identify whether it has defect, category the type of defect, and repair it automatically6Defect Identification, Categorization, and Repair:
Better Together

39:9

3 EXPERIMENTAL SETTING
We first introduce the dataset we experiment on, then we present the baselines for different type
of tasks. Following that, the evaluation metrics, experimental settings, and research questions are
presented subsequently.

3.1 Dataset
Program repair is an extremely important but difficult software engineering problem. Fixing simple
defects (e.g., one-line defects or defects that fall into a small set of templates) is a good way to obtain
acceptable performance. ManySStuBs4J [22] is a collection of single-statement defect-fix change
and have two versions: small size version and large size version. The small version contains 25,539
single-statement defect-fix changes mined from 100 popular open-source Java Maven projects,
while the large version contains 153,652 single-statement defect-fix changes mined from 1,000
popular open-source Java projects. This dataset is annotated by whether those changes match any
of a set of 16 defect patterns that appear often. To make our paper self-contained, we provide a
brief introduction of each pattern as follows.

â€¢ Change Identifier Used. It checks whether an identifier that appears in an expression in
a statement is replaced by another. It is easy for developers to accidentally use different
identifiers instead of having the same type of expected identifier. Copying and pasting code
is a potential source of such errors.

â€¢ Change Numeric Literal. It checks whether one numeric literal is replaced by another one.

It is easy for developers to mix two numeric values in a program.

â€¢ Change Boolean Literal. It checks whether a Boolean literal was replaced. That is, True is

replaced with False and vice-versa.

â€¢ Change Modifier. It checks whether a variable, function, or class is declared with the wrong

modifiers.

â€¢ Wrong Function Name. It checks whether the function was incorrectly called. Functions

with similar names and the similar signature are usual pitfall for developers.

â€¢ Same Function More Args. It checks whether an overloaded version of a function with
more parameters is called. Functions with multiple overloads often make it confusing to
developers.

â€¢ Same Function Less Args. It checks whether an overloaded version of the function with
less arguments is called. For example, a developer may call a function with at least one default
parameter and forget to initialize the parameter.

â€¢ Same Function Change Caller. It checks whether the caller object in the function call
expression is replaced by another object. When there are multiple variables having the same
type, the developer may perform an operation unexpectedly.

â€¢ Same Function Swap Args. It checks whether a function is called with some of its parame-
ters swapped. When multiple function parameters are of the same type, developers can easily
swap two of them without realizing if they do not accurately remember what each argument
represents.

â€¢ Change Binary Operator. It checks whether a binary operator is replaced with another
one of the same type by accident. For example, developers may often mix up comparison
operators in expressions.

â€¢ Change Unary Operator. It checks whether a unary operator is replaced with another
operator of the same type by accident. For example, developers may often forget ! operator
in boolean expressions.

â€¢ Change Operand. It checks whether one of the operands in a binary operation is wrong.

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

39:10

Chao Ni and Kaiwen Yang, et al.

â€¢ More Specific If. It checks whether an additional condition (&& operand) is added to the

condition of an if statement.

â€¢ Less Specific If. It checks whether an additional condition which either itself or the original

one needs to hold (|| operand) is added to the condition of the if statement.

â€¢ Missing Throws Exception. It checks whether the defect-fix adds a throw clause in a

function declaration.

â€¢ Delete Throws Exception. It checks whether the defect-fix deletes a throw clause in a

function declaration.

However, the details of this dataset is too simple to satisfy our taskâ€™s requirement. That is,
the original dataset mainly contains the types of defect, the one statement buggy code and one
statement fixed code. However, our model needs the whole information about a changed function
to capture more information inside the code. Besides, our study aims at defect identification, we
also need to collect negative commits from studied projects since the original dataset only contains
the positive commits. Therefore, we need an extension version of the large size of ManySStuBs4J
using the improved toolkit4, which fixes the issues in the process of original dataset extraction.

Fig. 3. The process of single statement commits filtering and function body extraction.

Fig. 4. The different contents of a specific function in different states: clean version, buggy version and fixed
version.

Firstly, we take ManySStuBs4J as the basis of the new dataset. We need more information of
changed function in a commit to extend the dataset from two aspects since we try to combine the
defect identification task, defect categorization task and defect repair task together: 1) extract the
function body where the defect-inducing statement or the defect-fix statement exists; 2) extract the
clean function body before the defect-inducing statement was introduced.

Secondly, the data extraction and extension process is illustrated in Fig. 3. Notice that we start
from the COMMIT rather than the FUNCTION. In particular, as for extracting the positive/negative
functions, we follow the same steps (e.g., selecting the same Java projects, identifying non-defect-
fixing/defect-fixing commits, selecting single statement changes, creating Abstract Syntax Trees
and filtering out clear refactoring) as the authors of ManySStuBs4J [22] took:

4https://github.com/h4iku/repairSStuBs

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

ClassifyingCommitasBug-FixingorNotIdentifyingNon-Bug-Fixing CommitsIdentifyingBug-FixingCommitsNon SStuBPatternsSStuBPatternsFunction  Body ExtractionThree Versions of FunctionThree  Versions of FunctionFilteringOutSingleStatementCommitsCreating Abstract Syntax TreesFiltering Out Clear RefactoringPre-processing on candidate commitspublicInputRepresentationgetStencilset() {InputStreamstencilsetStream= this.getClass().getClassLoader().getResourceAsStream("stencilset.json");InputRepresentationstencilsetResultRepresentation= newInputRepresentation(stencilsetStream);stencilsetResultRepresentation.setMediaType(MediaType.APPLICATION_JSON);returnstencilsetResultRepresentation;}public@ResponseBodyStringgetStencilset() {InputStreamstencilsetStream= this.getClass().getClassLoader().getResourceAsStream("stencilset.json");try{return IOUtils.toString(stencilsetStream);} catch(Exceptione) {thrownewActivitiException("Error while loading stencil set", e);}}public@ResponseBodyStringgetStencilset() {InputStreamstencilsetStream= this.getClass().getClassLoader().getResourceAsStream("stencilset.json");try{return IOUtils.toString(stencilsetStream, "utf-8");} catch(Exceptione) {thrownewActivitiException("Error while loading stencil set", e);}}CleanVersionBuggyVersionFixedVersionDefect Identification, Categorization, and Repair:
Better Together

39:11

â€¢ Step 1. We identify bug-fixing(non-bug-fixing) commits whose commit message contains
one(none) of these keywords (i.e., error, buy, fix, issue, mistake, incorrect, fault, defect, flaw,
and type). That is, we use the opposite selection strategy between selecting positive commits
and negative commits.

â€¢ Step 2. We follow the same strict criteria using in ManySStuBs4J [22] to select the commits

with single statement modification.

â€¢ Step 3. We identify each scope of the modification (i.e., hunk in git) in the filtered commits
and extract their corresponding bug-fixing(non-bug-fixing) function bodies. That is, we split
one commit into hunks and extract the function body of modification in each hunk.

â€¢ Step 4. The SZZ algorithm is used to identify their corresponding bug-inducing(non-bug-
inducing) commits. Right now, we can extract the corresponding bug-inducing(non-bug-
inducing) function bodies.

â€¢ Step 5. We can extract the corresponding clean versions (i.e., the last version in time before
the bug-inducing(non-bug-inducing) functions are modified) using PyDriller. Therefore, we
can obtain the three versions of the function body for positive/negative functions.

Meanwhile, we also design four criteria for filtering unsuitable functions in (Steps 3-5), such
as: 1) Modified statements exist outside a function. This work focuses on the simple scenario,
that is, function-level single statement defect prediction and defect repair. Therefore, we filter out
those statements that lie outside the scope of a function, for example, statements for defining a
global variable or object in a class. 2) Defects are introduced in newly added files or functions.
CompDefect needs three versions of a specific function. For newly added defective one, the clean
version does not exist. Therefore, we filter out such cases. 3) Function names cannot be identified.
On one hand, the modifications in a commit are made to the function name and then it is difficult
to solve. On the other hand, the line of function is mapped incorrectly. In ManySStuBs4J, the
authors label the line number of buggy or fixed code based on the result of AST, which may not be
exactly correct with the original line in source code file. 4) Other failed issues. When errors occur
in PyDriller or in original dataset, we cannot get the correct result.

Take an example from the project of Activiti5 shown in Fig. 4. There exists a function named
getStencilset6. We extract three versions of this function: clean version, buggy version and fixed
version. Buggy version means the state when the defect-inducing statement is introduced, clean
version means the state before the defect-inducing statement is introduced, and fixed version means
the state when the defect is fixed. The relationship among the three version can be illustrated in
Fig. 5. We extract the three versions of function with the help of PyDriller [35], which is a Python
framework that helps developers on mining software repositories.

Finally, we build the dataset and name it as Function-SStuBs4J for defect identification, defect

categorization and defect repair and the statistical information is shown in Table 1.

3.2 Baselines
Since there has no existing work which can identify whether a modification to a function may
introduce defects, subsequently categorize the type of defect, and consequently repair it automat-
ically, we make a comprehensive comparison among three types of baselines and CompDefect:
baselines for defect identification, baselines for defect categorization and baselines for program
repair.

5This Activiti is a light-weight workflow and Business Process Management (BPM) Platform targeted at business people,
developers and system admins.
6This function is located at â€œ modules/activiti-modeler/src/main/java/org/activiti/rest/editor/main/StencilsetRestResource.javaâ€.
This fixed version is submitted at â€œc015d11303339f50254a10be7335fd33546911abâ€, while the buggy version is introduced at
â€œ159d1ef8e0cf059165b17bb546f47f639559dfa9â€.

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

39:12

Chao Ni and Kaiwen Yang, et al.

Fig. 5. The extraction process of the three versions of function and their relationship.

Table 1. The statistics for Function-SStuBs4J

Defect Type
Same Function Change Caller
Change Identifier Used
Change Modifier
Change Numeric Literal
Change Operand
Change Binary Operator
Change Unary Operator
Wrong Function Name

Count Ratio Defect Type

381
1,599
329
991
161
523
338
3,012

Same Function More Args
1.81%
Same Function More Args
7.6%
Same Function Swap Args
1.56%
Change Boolean Literal
4.71%
0.76%
Less Specific If
2.48% More Specific If
1.61% Missing Throws Exception
14.31% Delete Throws Exception

Count Ratio
2.1%
5.55%
0.66%
1.57%
2.3%
2.71%
0.05%
0.15%

441
1,169
139
330
484
570
10
32

ALL
Positive
Negative

21,047
10,509
10,538

For the defect identification task, we consider two well-known methods as baselines: DeepJIT [14]
and CC2Vec [15] since they are deep-learning based methods using the textual information of code.
We briefly introduce the two methods as follows.
â–  DeepJIT is an end-to-end deep learning framework for Just-in-Time defect prediction. DeepJIT
adopts a Convolutional Neural Network (CNN) model to automatically learn high-dimensional
semantic features for commits. In particular, DeepJIT uses two CNN models to learn the represen-
tation of two parts of the input: one CNN for the commit message and another one for the code
commits. Finally, the concatenation of two representations is treated as the input of fully-connected
layer to output the probability of defect-introducing commit.
â–  CC2Vec is a distributed representation learning framework of commit. CC2Vec believes that
the commit has hierarchical structure, which is ignored by DeepJIT. In particular, one commit
is composed of a few changed files, one changed file is composed of a few hunks7, one hunk is
composed of a few changed lines and one changed line is composed of a few changed tokens.
To grasp the information of hierarchical structure in commits, CC2Vec models the Hierarchical
Attention Network (HAN) with the help of the attention mechanism and multiple comparison
functions are used to identify the difference of modified code (i.e., added code and removed code).
CC2Vec targets at learning a representation of code commits guided by their accompanying commit
messages, which represents the semantic intent of the code commits.

For defect categorization task, we choose three methods BERT [7], RoBERTa [27] and Code-

BERT [8] as the baselines and briefly introduce them as follows.
â–  BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly
conditioning on both left and right context in all layers and its capability of capturing the semantic
and context information of sentence has been verified in many work [25, 49, 50]. BERT consists of
12-layer transformers and each of the transformers is composed of a self-attention sub-layer with

7https://git-scm.com/

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

Bug-FixingCommitBug-InducingCommitCleanVersionâ€¦â€¦Non-Bug-FixingCommitNon-Bug-InducingCommitCleanVersionâ€¦â€¦BeforeBeforePositive CommitNegative CommitBug-FixingCommitBug-InducingCommitCleanVersionâ€¦â€¦BeforePositive CommitUsingSZZalgorithmLastoneintimebeforebug-inducingmodificationoccursUsingSZZalgorithmLastoneintimebeforenon-bug-inducingmodificationoccursNon-Bug-FixingCommitNon-Bug-InducingCommitCleanVersionâ€¦â€¦BeforeNegative CommitDefect Identification, Categorization, and Repair:
Better Together

39:13

, ..., â„ğ‘™

multiple attention heads. BERT takes sequence of tokens as the input of embedding component.
Given a sequence of tokens ğ‘¥ = {ğ‘¥1, ..., ğ‘¥ğ‘‡ } with length of ğ‘‡ , BERT will calculate the contextualized
representations ğ» ğ‘™ = {â„ğ‘™
ğ‘‡ } âˆˆ â„œğ‘‡ Ã—ğ· , where ğ‘™ represents the ğ‘™ğ‘¡â„ transformer layer and ğ·
1
represents the dimension of the representation vector. As a result, the pre-trained BERT model
can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide
range of tasks, such as binary or multiclass classification.
â–  RoBERTa is a robustly optimized version of BERT by iterating on BERTâ€™s pretraining procedure
(e.g., training the model longer, using bigger batches over more data) and CompDefectâ€™s encoder
has the same architecture with RoBEATa.
CodeBERT is another pre-trained model on the basic of BERT for programming language, which
is a multi-programming-lingual model pre-trained on NL(Natural Language)-PL(Programming
Language) pairs in six programming languages (i.e., Python, Java, JavaScript, PHP, Ruby and Go).
For defect repair task, we also consider one state-of-the-art method SequenceR [5] as baseline.

We briefly introduce it as follows.
â–  SequenceR is a sequence-to-sequence deep learning model which aims at automatically fixing
defects by generating one-line patches. The one-line patch means that the defect can be fixed
by replacing a single buggy line with a single correct line. SequenceR proposes a novel buggy
context abstraction process to organize the fault localization information into a representation.
Such a representation preserves concise, suitable and valuable information for deep learning
model understanding the context of the bug and consequently be used to predict the fix. Then,
the representation is fed to a trained sequence-to-sequence model to execute patch inference. In
particular, it generates multiple single-lines of code that will be treated as the potential one-line
patches for the defect. Finally, SequenceR generates the most suitable patches by formatting the
code and replacing the buggy line with the proposed lines.

3.3 Evaluation Metrics
There are four statistics with respect to classification task: (i) True Positive (TP) represents the
number of functions classified as defective and they are truly defective ones. (ii) True Negative (TN)
represents the number of functions classified as non-defective and they are truly non-defective
ones. (iii) False Positive (FP) represents the number of functions classified as defective and they are
truly non-defective ones. (iv) False Negative (FN) represents the number of non-defective functions
and they are truly defective ones. Therefore, based on the four possible statistics, three widely
used performance measures (i.e., ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›, ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ and ğ¹ 1-ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’) can be defined to evaluate the
performance of CompDefect and baselines as follows:
â–  Precision: the proportion of functions that are correctly classified as defective among those
labeled as defective: ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = ğ‘‡ ğ‘ƒ
â–  Recall: the proportion of defective functions that are correctly classified: ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ = ğ‘‡ ğ‘ƒ
â–  F1-score: the harmonic mean of precision and recall. It evaluates if an increase in precision (or
recall) outweighs a reduction in recall (or precision), respectively: ğ¹ 1-ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ = 2Ã—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›Ã—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
â–  AUC: the Area Under the receiver operator characteristics Curve (AUC) is also used to measure
the discriminatory power of CompDefect and baselines, i.e., the ability to differentiate between
defective or non-defective functions. AUC calculates the area under the curve plotting the true
positive rate (TPR) versus the false positive rate (FPR), while applying multiple thresholds to
determine if a function is defect-inducing or not. The value of AUC ranges between 0 (the worst
discrimination) and 1 (the perfect discrimination).

ğ‘‡ ğ‘ƒ +ğ¹ ğ‘ .

ğ‘‡ ğ‘ƒ +ğ¹ ğ‘ƒ .

.

Besides, to better evaluate how approaches perform on defect repair task, we consider two widely

used metrics: BLEU (Bilingual Evaluation Understudy) [32] and Accuracy [5].

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

39:14

Chao Ni and Kaiwen Yang, et al.

â–  BLEU: it is a widely used measure for neural machine translation task [24] and software related
task [10, 19]. In our task, it is used to calculate the similarity between the generated code snippet
and the referenced correct code snippet. It has a score range of 0 and 1. The higher the BLEU score,
the closer the generated code is to the referenced one. It first computes the geometric average of
the modified ğ‘›-gram precisions (i.e., ğ‘ğ‘›) by using ğ‘›-grams up to the maximum number of grams
ğ‘ (i.e., set as 4 in our paper) and positive weights ğ‘¤ğ‘› summing to one. Then, it computes the

brevity penalty BP, ğµğ‘ƒ =

(cid:40)

1,
ğ‘’ (1âˆ’ğ‘Ÿ /ğ‘),

ğ‘– ğ‘“ ğ‘ > ğ‘Ÿ
ğ‘– ğ‘“ ğ‘ â‰¤ ğ‘Ÿ

,in which, ğ‘ represents the length of the candidate

code snippet and ğ‘Ÿ represents the effective reference corpus length. Finally, the BLEU score can be
ğ‘¤ğ‘›ğ‘™ğ‘œğ‘”ğ‘ğ‘›
calculated as follows: ğµğ¿ğ¸ğ‘ˆ = ğµğ‘ƒ Â· ğ‘’ğ‘¥ğ‘
â–  Accuracy: the target of defect repair model is to fix as many defects as possible. Therefore, we
also use accuracy to evaluate the effectiveness of models for defect fixing, it is calculated as the
ratio between the number of correctly fixed defects by an approach and the number of total defects
to be fixed.

(cid:16)(cid:205)ğ‘
ğ‘›=1

(cid:17)

3.4 Empirical setting.
We implement our CompDefect in Python with the help of Pytorch framework and pre-trained
model on Huggingface8. The pre-trained GraphCodeBERT model is used as the encoder for embed-
ding training samples, which can leverage semantic structure of code to learn code representation
and also can be easily extended for downstream tasks. Besides, we also use Transformer decoder [38]
as the generator of fixed code. In our model, each version function is embedding as a 768 dimen-
sional vector. During the training phase, the parameters of CompDefect are optimized using Adam
with a batch size of 32. We also use ğ‘…ğ‘’ğ¿ğ‘¢ and ğ‘¡ğ‘ğ‘›â„ as the activation function. A dropout of 0.1 is
used for dense layers before calculating the final probability. The maximum number of epoch in
our experiment is 50. The models (i.e., CompDefect and baselines) with the best performance on
the validation set is used for our evaluations.

As for dataset split, we use 80%, 10% and 10% of original dataset as training data, validation data
and testing data, respectively. Notice that, for each part of the data, we keep the distribution among
each type of function as same as the original one.

3.5 Research Questions
To comprehensively evaluate the effectiveness of CompDefect, we investigate the following
research questions.

â€¢ RQ1: How does CompDefect perform on defect identification compared with state-of-the-art

baselines?

â€¢ RQ2: How does CompDefect perform on defect categorization compared with state-of-the-

art baselines?

â€¢ RQ3: How does CompDefect perform on defect repair compared with the state-of-the-art

baseline?

4 RESULTS AND ANALYSIS
4.1 RQ1: How does CompDefect perform on defect identification compared with

state-of-the-art baselines?

Motivation. Just-in-time (JIT) defect prediction has received much attention in the software
engineering and many state-of-the-art approaches are proposed [14, 15, 17]. These approaches are

8https://huggingface.co/

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

Defect Identification, Categorization, and Repair:
Better Together

39:15

built from simple model (e.g., CBS+) on manually designed features to complex model (e.g., DeepJIT)
on semantic features and have make a great progress in JIT scenario. As for CompDefect based
on neural network, it can also identify whether a commit is a defect-inducing one by predicting if
the modification to a function will introduce a defect. Therefore, we want to make a comparison
between CompDefect with those state-of-the-art semantic features based approaches.
Approach. We treat recently proposed DeepJIT [14] and CC2Vec [15] as the baseline methods and
the introduction of them can be found in Section 3. There are two differences between CompDefect
and the two baselines: 1) the two methods can only estimate the defect-proneness of a commit at
the commit level and a commit may contain a few hunks which changes a few functions, while
CompDefect estimates whether a function in a hunk of a commit is defect-inducing. That is,
CompDefect can estimate the defect-proneness of a commit at the hunk(function)-level, which
means CompDefect is more fine-grained than the baseline ones. 2) the two baselines can only
predict a commit as a defect-inducing one or clean one, while CompDefect can not only predict a
commit defect-proneness but also can categorize the types of defect.

Considering the two differences and to make a fair commit-level comparison, we make the
following two hypothesizes for CompDefect: 1) a function in a hunk of a commit will be treated
as a buggy one if CompDefect predicts it as a non-clean one; 2) a commit is predicted as defect-
inducing one if there exists a least one function in a hunk of a commit predicted by CompDefect
as the defect-inducing one, otherwise the commit is predicted as a clean one. Besides, we use
the widely used performance measures (i.e., Precision, Recall, F1-score, and AUC) to evaluate the
difference among those methods.
Result. The evaluation results are reported in Table 2. The best performance is highlighted in bold.
According to the results, we find that our approach CompDefect has the significant advantage
over DeepJIT and CC2Vec on all performance measures. In particular, CompDefect obtains 0.679
and 0.785 in terms of F1-score and AUC, which improves DeepJIT and CC2Vec by 39.0% and
41.7%, by 34.7% and 37.3% in terms of F1-score and AUC, respectively. As for Precision and Recall,
CompDefect also has a large improvement. Specifically, CompDefect improves DeepJIT and
CC2Vec by 32.8% and 38.2%, by 43.2% and 44.2% in terms of Precision and Recall, respectively.
Besides, we surprisingly find that in our scenario, CC2Vec performs a little worse than DeepJIT,
which, to some extent, means CC2Vec cannot capture more information than DeepJIT for existing
state-of-the-art technique can utilize.

Table 2. Comparison among DeepJIT, CC2Vec and CompDefect

Approach
DeepJIT
CC2Vec
CompDefect

ğ¼ğ‘šğ‘ğ‘Ÿğ‘œğ‘£.

ğ·ğ‘’ğ‘’ğ‘ ğ½ ğ¼ğ‘‡
ğ¶ğ¶2ğ‘‰ ğ‘’ğ‘

Precision Recall
0.352
0.345
0.619
43.2%
44.2%

0.504
0.464
0.750
32.8%
38.2%

F1-score AUC
0.513
0.492
0.785
34.7%
37.3%

0.414
0.396
0.679
39.0%
41.7%

4.2 RQ2: How does CompDefect perform on defect categorization compared with

state-of-the-art baselines?

Motivation. Even though many approaches [4, 15, 20, 26] have been proposed for just-in-time
defect prediction scenario, these approaches can only predict a commit as defect-inducing or
not. They cannot categorize the type of defect. Different from previous work, CompDefect can
categorize the type of defect the defect-inducing function belongs. We totally consider 16 types of

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

39:16

Chao Ni and Kaiwen Yang, et al.

defects, which is introduced in Section 3.1. Reporting the type of defect rather than â€œbuggy-or-cleanâ€
can help developers better understand such a defect. CompDefect makes a few progress in JIT-DP
scenario even we only consider the one-statement function-level modification setting.
Approach. To verify the effectiveness of CompDefect in defect categorization task, we choose
a pre-trained language representation BERT [7] and its optimized variants RoBERTa [27] and
CodeBERT [8], which are widely used in natural language processing and software engineering [31,
50], and then be used for downstream tasks that we care about (like multiclass classification). We
use these pretrained models (i.e., â€œbert-base-uncasedâ€, â€œroberta-baseâ€ and â€œmicrosoft/codebert-baseâ€)
from Huggingface9. For a fair comparison, the input of these baselines is the same as the input of
CompDefect. That is, the buggy version function and the clean version function are treated as the
input. In addition, since the limitation of the maximum length of baselinesâ€™ input, we use the same
strategy in CompDefect to concatenate the two function vertically to assemble the whole input.
Besides, for evaluate the performance of CompDefect and these baselines in defect categorization
scenario, we also use the four performance measures (i.e., Precision, Recall, F1-score and AUC).
These classification metrics are defined for binary cases by default. When extending these binary
metrics to multiclass, we use the â€œmacroâ€ averaging strategies, which are widely adopted in prior
work [2, 18, 31, 42]. â€œmacroâ€ strategy first calculates each metric for each class and then report the
average value among all classes.
Result. The evaluation results are reported in Table 3 and the best results are highlighted in bold.
On average, we find that CompDefect outperforms baselines by 63%, 218% and 239% in terms of
Precision, Recall, and F1-score, respectively. In particular, CompDefect improves BERT by 33%,
250% and 284% in terms of Precision, Recall and F1-score, respectively. Compared with RoBERTa,
CompDefect improves RoBERTa by 77%, 245% and 275% in terms of Precision, Recall and F1-score,
respectively, which means that even though CompDefect and RoBERTa have the same architecture,
CompDefect can learn more information with its related task (i.e., defect fix). Compared with
CodeBERT, CompDefect improves CodeBERT by 78%, 158% and 159% in terms of Precision, Recall
and F1-score, respectively, which also means CompDefect can benefit from multi-task learning. In
terms of AUC, CompDefect still performs best. All the results indicate the priority of CompDefect
on categorizing the types of defects.

Table 3. Comparison among BERT, RoBERTa, CodeBERT and CompDefect on explaining the types of defect

Approach
BERT
RoBERTa
CodeBERT
CompDefect
ğµğ¸ğ‘…ğ‘‡
ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ ğ‘
ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡
ğ´ğ‘£ğ‘”.

ğ¼ğ‘šğ‘ğ‘Ÿğ‘œğ‘£.

Precision Recall F1-score AUCğ‘‚ğ‘‰ ğ‘‚ AUCğ‘‚ğ‘‰ ğ‘…
0.083
0.085
0.124
0.319
284%
275%
159%
239%

0.301
0.227
0.226
0.401
33%
77%
78%
63%

0.693
0.712
0.695
0.723
4%
2%
4%
3%

0.731
0.751
0.754
0.776
6%
3%
3%
4%

0.084
0.086
0.115
0.295
250%
245%
158%
218%

â€œOVOâ€: stands for one-vs-one; â€œOVRâ€: stands for one-vs-rest.

9https://huggingface.co/bert-base-uncased

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

Defect Identification, Categorization, and Repair:
Better Together

39:17

4.3 RQ3: How does CompDefect perform on defect repair compared with the

state-of-the-art baseline?

Motivation. Defect repair research is active and mostly dominated by techniques based on static
analysis [29] and dynamic analysis [40]. Even a great progress has been achieved, currently, the
state of automated defect repair is limited to simple cases, mostly one-line patches [34, 40]. Recently,
defect repair tools based on machine learning especially for deep learning technology are proposed,
which promotes the further development of defect repair. In particular, SequenceR, proposed by
Chen et al. [5], is one of the most outstanding ones, which also mainly focus on one-line patch
scenario. Therefore, we want to evaluate the performance difference between CompDefect and
SequenceR.
Approach. SequenceR can solely address one-line level bug fix. That is, it cannot identify whether
a function is defect-inducing one and cannot categorize the type of defect in such a function.
Therefore, we filter negative functions in original dataset and keep only positive functions. We
refer to it as Function-SStuBs4Jpositive, which contains 16 types of positive functions. Besides,
SequenceR firstly does an abstraction operation on function. However, some function in Function-
SStuBs4Jpositive cannot be executed successfully with the tool provided by SequenceR. Thus, we
filter out these functions and finally Function-SStuBs4Jpositive has the positive function with 14
types. We split 80%, 10% and 10% of Function-SStuBs4Jpositive as training data (7,474 functions),
validation data (934 functions) and testing data (934 functions), respectively, and the distribution
among each type of function as same as the original one. For a fair comparison, we train SequenceR
and CompDefect (referred as CompDefectpositive) on the filtered training data, optimize them on
the filtered validation data, and finally evaluate them on the filtered testing dataset.

Moreover, for fully evaluating the capability of CompDefect, we want to directly evaluate the
CompDefect trained on original training data (i.e., training data from Function-SStuBs4J, denoted
as Trainingoriginal) on the testing data of Function-SStuBs4Jpositive, denoted as Testingpositive. However,
since Function-SStuBs4Jpositive and Function-SStuBs4J are not exactly the same split, Trainingoriginal
may contains the functions in Testingpositive. So, for a fair comparison, we identify the intersection
of Trainingoriginal and Testingpositive, and remove the intersection from Testingpositive and denote it
as Testingpositive_filtered. For evaluating the two methods, we adopt two widely used performance
measures: BLEU and Accuracy.
Result. The evaluation results are reported in Table 4 and the best results are highlighted in
bold. On Testingğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ , CompDefectpositive performs better than SequenceR and outperforms Se-
quenceR by 23.9% and 29.5% in terms of BLEU and Accuracy, respectively. On Testingğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’_ğ‘“ ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘ ,
CompDefectpositive can also improve SequenceR by 29.6% in terms of BLEU and perform similarly
as SequenceR in terms of Accuracy. CompDefect also outperform SequenceR on both performance
measure. Besides, when comparing CompDefect with CompDefectpositive, we find that CompDe-
fect has better performance, which indicates that CompDefect benefits from the multi-task
learning and learns useful information from negative functions for defect repair.

Table 4. Comparison between SequenceR and CompDefect on defect repair

Testing Data

Testingğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’

Testingğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’_ğ‘“ ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘‘

Approach
SequenceR
CompDefectpositive
SequenceR
CompDefectpositive
CompDefect

BLEU Accuracy
0.734
0.964
0.677
0.962
0.972

0.136
0.193
0.147
0.130
0.232

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

39:18

Chao Ni and Kaiwen Yang, et al.

5 THREATS TO VALIDATION
Threats to Internal Validity mainly lie in the potential faults in the implementation of our model.
To minimize such threats, we not only implement these methods by pair programming but also
make full use of the pre-trained models such as GraphCodeBERT [11] and BERT [7]. Besides, we
directly use the original source code of baselines and the same hyperparameters used in original
method are adopted in our paper. All of the datasets used in our study are publicly available from
previous work [22], and we extend these dataset for our investigated scenario.
Threats to External Validity mainly lie in the studied projects used in this study. To reduce
such threats, we opted to selecting high popularity Java projects. The popularity of a project is
determined by computing the sum of ğ‘§-scores of forks and stars [22]. However, all studied projects
are open source projects, it is still unknown whether our CompDefect can work well on commercial
projects. In the future, we plan to reduce this threat by considering more additional commercial
projects.
Threats to Construct Validity mainly lie in the adopted performance metrics in our evaluations.
To reduce such threat, we use different types of performance measure for different tasks. For
classification task, we use the widely-used Area Under the Curve (AUC) score to evaluate the
performance of the each method and it needs to be set a threshold manually. We also consider
some widely performance metrics which need to manually set a threshold e.g., Precision, Recall
and F1-score. For generation task, we use two well-known metrics namely BLEU and Accuracy.

6 RELATED WORK

6.1 Just-in-Time Defect Prediction
JIT defect prediction has been an active research topic in recent years since it can identify defect-
inducing commit at a fine-grained level at check-in time. Mockus et al. [30] extracted historical
information in commit to build a classifier to predict the risk of new commits. Kamei et al. [20]
proposed 14 change-level metrics from five dimensions, which are used to build an effort-aware
prediction model with the help of Logistic Regression. Following that, Yang et al. [44, 45] sub-
sequently proposed two approach for JIT defect prediction. In particular, Yang et al. [45] firstly
used Deep Belief Network (DBN) to extract higher-level information from the initial change-level
metrics, then Yang et al. [44] combined decision tree and ensemble learning to build a two-layer
ensemble learning model for JIT defect prediction. To further improve Yang et alâ€™s model, Young et
al. [47] proposed a new deep ensemble approach by using arbitrary classifiers in the ensemble and
optimizing the weights of the classifiers. Later, Liu et al. [26] proposed code churn and evaluated it
in effort-aware settings. Chen et al. [4] treated the effort-aware JIT defect prediction task as a multi-
objective optimization problem and consequently a set of effective features are selected to build
the prediction model. Then, Cabral et al. [3] proposed a new sampling technology to address the
issues of verification latency and class imbalance evolution in online JIT defect prediction setting.
Besides, Yan et al. [43] proposed a two-phase framework which can handle the identification and
localization tasks at the same time. Recently, Hoang et al. [14, 15] proposed two newly approaches,
which use modern deep learning model to learn the representation of commit message and code
changes.

Apart from above approaches, researchers also conduct studies on JIT defect prediction from
various aspects. McIntosh et al. [28] investigated the impact of systems evolution on JIT defect
prediction models via a longitudinal case study of 37,524 changes from the rapidly evolving QT and
OpenStack systems. They found that the interval between training periods and testing periods has
side-effect on the performance of JIT models and JIT models should be trained using six months
(or more) of historical data. Besides, Wan et al. [39] discussed the drawbacks of existing defect

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

Defect Identification, Categorization, and Repair:
Better Together

39:19

prediction tools and highlighted future research directions through literature review and a survey of
practitioners. After that, Tabassum et al. [36] conducted a study of JIT defect prediction in realistic
online learning scenarios and concluded that the model trained with both within and cross-project
data can outperform the model trained with within-project data only. Recently, Zeng et al. [48]
revisited the deep learning based JIT defect prediction models and found that deep learning based
approaches may not work better than simplistic model LApredict they proposed.

Different from the existing work (i.e., commit-level JIT defect prediction), our model CompDefect
focus on function-level single-statement JIT defect prediction, which is more fine-grained defect
identification task. Besides, previous work only give two coarse-grained outputs: defect-inducing
or clean. CompDefect can categorize the type of defect-inducing functions.

6.2 Defect Repair
Defect repair [9] is an active research topic and achieve a great progress. However, the current state
of automated defect repair is limited to simple small fixes, mostly one-line patches [34, 40]. Gupta
et al. [12] proposed a defect repair tool named DeepFix for fixing compiler errors in introductory
programming courses. DeepFix takes the whole program and outputs a single line fix. Ahmed et
al. [1] also focused on compiler error fix and proposed another defect repair tool named TRACER,
which outperforms DeepFix in terms of success rate. Martin et al. [41] proposed DeepRepair to
leverage the learned code similarities to select repair ingredients from code fragments which are
similar to the buggy code. Tufano et al. [37] performed an empirical study to assess the feasibility
of using neural machine translation techniques for learning bug-fixing patches for real defects.
They trained an Encoder-Decoder model which can translate buggy code into its fixed version.
Hideaki et al. [13] proposed a similar network and applied it to one-line diffs.

Considering the complexity of defect repair, similar to previous work, our model CompDefect
also focus on the one-line code fix scenario. However, different from the existing work, the input of
CompDefect is the source code of changed function in a commit and it aims to provide a foundation
for connecting defect identification and defect repair. CompDefect can categorize the type of a
defect and can fix it at the check-in time.

6.3 Pre-Trained Models in NLP and SE
Pre-trained technologies have achieved a big success in Natural Language Processing (NLP) [7, 27,
46] and pre-trained models associated with programming languages [8, 23, 25] also make a great
process of code intelligence. Kanade et al. [21] pre-trained a BERT model on a massive corpus of
Python source codes through two tasks: masked language modeling and next sentence prediction.
Lewis et al. [25] adopted a standard Transformer-based neural network to train a sequence-to-
sequence model named BART. Later, Feng et al. [8] used Transformer-based neural architecture to
propose a bimodal pre-trained model named CodeBERT for programming language and natural
language by masked language modeling and replaced token detection. CodeBERT aims at learning
general-purpose representations for supporting downstream NL-PL applications (e.g., code search,
code documentation generation). Karampatsis et al. [23] pre-trained contextual embeddings on a
JavaScript corpus by using the ELMo frame, which can be further used for program repair task.
Guo et al. [11] proposed a pre-trained model named GraphCodeBERT which can leverage code
structure to learn code representation to improve code understanding.

Recently, pre-trained models are widely used in software engineering (SE) and these models
are used to learn the representation of source code. Gao et al. [50] proposed an approach named
TDCleaner, a BERT-based neural network, to automatically detect and remove obsolete TODO
comments from software repositories. In this paper, we also use GraphCodeBERT to generate the
representation of function.

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

39:20

Chao Ni and Kaiwen Yang, et al.

7 CONCLUSION AND FUTURE WORK
In this paper, we propose a comprehensive defect prediction and repair framework named CompDe-
fect, which can identify whether a function changed in a commit is defect-prone, categorize the
type of defect, and repair such a defect automatically. Generally, the first two tasks in CompDefect
is treated as a multiclass classification task, while the last one is treated as a sequence generation
task. The whole input of CompDefect consists of three parts: the clean version of function (i.e.,
the version before the defect introduced), the buggy version of function and the fixed version of
function. For the first task, CompDefect identifies the defect type through multiclass classifica-
tion. For the second task, CompDefect repairs the defect once identified or keeps it as the same
originally. Moreover, we also build a new function-level dataset on the basis of ManySStuBs4J to
evaluate the performance of CompDefect. The new dataset is the largest function-level dataset
with comprehensive information. It contains three versions of a certain function and multiple
types of defect. By comparing with state-of-the-art baselines in various settings, CompDefect can
achieve superior performance on classification and defect repair.

In the future, we firstly want to make our approach an online available tool for practical usage.
Then, we will extract more non-single-statement defective functions and further improve our model
to address this situation.

REFERENCES
[1] Umair Z Ahmed, Pawan Kumar, Amey Karkare, Purushottam Kar, and Sumit Gulwani. 2018. Compilation error repair:
for the student programs, from the student programs. In Proceedings of the 40th International Conference on Software
Engineering: Software Engineering Education and Training. 78â€“87.

[2] Deeksha Arya, Wenting Wang, Jin LC Guo, and Jinghui Cheng. 2019. Analysis and detection of information types of
open source software issue discussions. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE).
IEEE, 454â€“464.

[3] George G Cabral, Leandro L Minku, Emad Shihab, and Suhaib Mujahid. 2019. Class imbalance evolution and verifica-
tion latency in just-in-time software defect prediction. In 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE). IEEE, 666â€“676.

[4] Xiang Chen, Yingquan Zhao, Qiuping Wang, and Zhidan Yuan. 2018. MULTI: Multi-objective effort-aware just-in-time

software defect prediction. Information and Software Technology 93 (2018), 1â€“13.

[5] Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-NoÃ«l Pouchet, Denys Poshyvanyk, and Martin Monperrus.
IEEE Transactions on Software

2019. Sequencer: Sequence-to-sequence learning for end-to-end program repair.
Engineering (2019).

[6] Michael Crawshaw. 2020. Multi-task learning with deep neural networks: A survey. arXiv preprint arXiv:2009.09796

(2020).

[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional

transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).

[8] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,
Daxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint
arXiv:2002.08155 (2020).

[9] Luca Gazzola, Daniela Micucci, and Leonardo Mariani. 2017. Automatic software repair: A survey. IEEE Transactions

on Software Engineering 45, 1 (2017), 34â€“67.

[10] Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep API learning. In Proceedings of the 2016

24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. 631â€“642.

[11] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy,
Shengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366
(2020).

[12] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. Deepfix: Fixing common c language errors by

deep learning. In Thirty-First AAAI Conference on Artificial Intelligence.

[13] Hideaki Hata, Emad Shihab, and Graham Neubig. 2018. Learning to generate corrective patches using neural machine

translation. arXiv preprint arXiv:1812.07170 (2018).

[14] Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu Ubayashi. 2019. DeepJIT: an end-to-end
deep learning framework for just-in-time defect prediction. In 2019 IEEE/ACM 16th International Conference on Mining

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

Defect Identification, Categorization, and Repair:
Better Together

39:21

Software Repositories (MSR). IEEE, 34â€“45.

[15] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. Cc2vec: Distributed representations of code changes.

In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 518â€“529.

[16] Qiao Huang, Emad Shihab, Xin Xia, David Lo, and Shanping Li. 2018. Identifying self-admitted technical debt in open

source projects using text mining. Empirical Software Engineering 23, 1 (2018), 418â€“451.

[17] Qiao Huang, Xin Xia, and David Lo. 2017. Supervised vs unsupervised models: A holistic look at effort-aware just-in-
time defect prediction. In 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE,
159â€“170.

[18] Qiao Huang, Xin Xia, David Lo, and Gail C Murphy. 2018. Automating intention mining. IEEE Transactions on Software

Engineering 46, 10 (2018), 1098â€“1119.

[19] Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generating commit messages from diffs using
neural machine translation. In 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 135â€“146.

[20] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E Hassan, Audris Mockus, Anand Sinha, and Naoyasu Ubayashi.
2013. A large-scale empirical study of just-in-time quality assurance. IEEE Transactions on Software Engineering 39, 6
(2013), 757â€“773.

[21] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2019. Pre-trained contextual embedding of

source code. arXiv preprint arXiv:2001.00059 (2019).

[22] Rafael-Michael Karampatsis and Charles Sutton. 2020. How often do single-statement bugs occur? the manysstubs4j

dataset. In Proceedings of the 17th International Conference on Mining Software Repositories. 573â€“577.

[23] Rafael-Michael Karampatsis and Charles Sutton. 2020. Scelmo: Source code embeddings from language models. arXiv

preprint arXiv:2004.13214 (2020).

[24] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. 2017. Opennmt: Open-source toolkit

for neural machine translation. arXiv preprint arXiv:1701.02810 (2017).

[25] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov,
and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation,
translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).

[26] Jinping Liu, Yuming Zhou, Yibiao Yang, Hongmin Lu, and Baowen Xu. 2017. Code churn: A neglected metric in
effort-aware just-in-time defect prediction. In Proceedings of the 11th ACM/IEEE International Symposium on Empirical
Software Engineering and Measurement. IEEE Press, 11â€“19.

[27] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,
and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).

[28] Shane McIntosh and Yasutaka Kamei. 2018. Are fix-inducing changes a moving target?: a longitudinal case study of
just-in-time defect prediction. In Proceedings of the 40th International Conference on Software Engineering. 560.
[29] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: Scalable multiline program patch synthesis

via symbolic analysis. In Proceedings of the 38th international conference on software engineering. 691â€“701.

[30] Audris Mockus and David M Weiss. 2000. Predicting risk of software changes. Bell Labs Technical Journal 5, 2 (2000),

169â€“180.

[31] Shengyi Pan, Lingfeng Bao, Xiaoxue Ren, Xin Xia, David Lo, and Shanping Li. 2021. Automating developer chat mining.

In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 854â€“866.

[32] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311â€“318.
[33] Chanathip Pornprasit and Chakkrit Tantithamthavorn. 2021. JITLine: A Simpler, Better, Faster, Finer-grained Just-
In-Time Defect Prediction. In Proceedings of the International Conference on Mining Software Repositories (MSR). To
Appear.

[34] Ripon K Saha, Yingjun Lyu, Hiroaki Yoshida, and Mukul R Prasad. 2017. Elixir: Effective object-oriented program
repair. In 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 648â€“659.
[35] Davide Spadini, MaurÃ­cio Aniche, and Alberto Bacchelli. 2018. PyDriller: Python framework for mining software
repositories. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering - ESEC/FSE 2018. ACM Press, New York, New York, USA, 908â€“911.
https://doi.org/10.1145/3236024.3264598

[36] Sadia Tabassum, Leandro L Minku, Danyi Feng, George G Cabral, and Liyan Song. 2020. An investigation of cross-
project learning in online just-in-time software defect prediction. In 2020 IEEE/ACM 42nd International Conference on
Software Engineering (ICSE). IEEE, 554â€“565.

[37] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. 2019.
An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Transactions on

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

39:22

Chao Ni and Kaiwen Yang, et al.

Software Engineering and Methodology (TOSEM) 28, 4 (2019), 1â€“29.

[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia
Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998â€“6008.
[39] Zhiyuan Wan, Xin Xia, Ahmed E Hassan, David Lo, Jianwei Yin, and Xiaohu Yang. 2018. Perceptions, Expectations,

and Challenges in Defect Prediction. IEEE Transactions on Software Engineering (2018).

[40] Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2018. Context-aware patch generation for
better automated program repair. In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE,
1â€“11.

[41] Martin White, Michele Tufano, Matias Martinez, Martin Monperrus, and Denys Poshyvanyk. 2019. Sorting and
transforming program repair ingredients via deep learning code similarities. In 2019 IEEE 26th International Conference
on Software Analysis, Evolution and Reengineering (SANER). IEEE, 479â€“490.

[42] Andrew Wood, Paige Rodeghero, Ameer Armaly, and Collin McMillan. 2018. Detecting speech act types in developer
question/answer conversations during bug repair. In Proceedings of the 2018 26th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software Engineering. 491â€“502.

[43] Meng Yan, Xin Xia, Yuanrui Fan, Ahmed E Hassan, David Lo, and Shanping Li. 2020. Just-in-time defect identification

and localization: A two-phase framework. IEEE Transactions on Software Engineering (2020).

[44] Xinli Yang, David Lo, Xin Xia, and Jianling Sun. 2017. TLEL: A two-layer ensemble learning approach for just-in-time

defect prediction. Information and Software Technology 87 (2017), 206â€“220.

[45] Xinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. 2015. Deep learning for just-in-time defect prediction. In

2015 IEEE International Conference on Software Quality, Reliability and Security. IEEE, 17â€“26.

[46] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized
autoregressive pretraining for language understanding. Advances in neural information processing systems 32 (2019).
[47] Steven Young, Tamer Abdou, and Ayse Bener. 2018. A replication study: just-in-time defect prediction with ensemble
learning. In Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software
Engineering. 42â€“47.

[48] Zhengran Zeng, Yuqun Zhang, Haotian Zhang, and Lingming Zhang. 2021. Deep just-in-time defect prediction: how

far are we?. In ISSTA â€™21: 30th ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM, 1â€“1.

[49] Ting Zhang, Bowen Xu, Ferdian Thung, Stefanus Agus Haryono, David Lo, and Lingxiao Jiang. 2020. Sentiment analysis
for software engineering: How far can pre-trained transformer models go?. In 2020 IEEE International Conference on
Software Maintenance and Evolution (ICSME). IEEE, 70â€“80.

[50] Gao Zhipeng, Xia Xin, Lo David, Grundy John, and Zimmermann Thomas. 2021. Automating the Removal of Obsolete
TODO Comments. In Proceedings of the 29th ACM Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering. 1â€“1.

Received XX 2021; revised ?? 2021; accepted ?? 2021

ACM Trans. Softw. Eng. Methodol., Vol. ?, No. ?, Article 39. Publication date: April 2021.

