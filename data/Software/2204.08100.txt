Multi-Model Ensemble Optimization

Anthony-Alexander Christidis
Department of Statistics
University of British Columbia
anthony.christidis@stat.ubc.ca

Stefan Van Aelst
Department of Mathematics
KU Leuven
stefan.vanaelst@kuleuven.be

Ruben Zamar
Department of Statistics
University of British Columbia
ruben@stat.ubc.ca

Abstract

Methodology and optimization algorithms for sparse regression are extended to
multi-model regression ensembles.
In particular, we adapt optimization algorithms
for ℓ0-penalized problems to learn ensembles of sparse and diverse models. To gener-
ate an initial solution for our algorithm, we generalize forward stepwise regression to
multi-model regression ensembles. The sparse and diverse models are learned jointly
from the data and constitute alternative explanations for the relationship between the
predictors and the response variable. Beyond the advantage of interpretability, in pre-
diction tasks the ensembles are shown to outperform state-of-the-art competitors on
both simulated and gene expression data. We study the eﬀect of the number of mod-
els and show how the ensembles achieve excellent prediction accuracy by exploiting the
accuracy-diversity tradeoﬀ of ensembles. The optimization algorithms are implemented
in publicly available R/C++ software packages.

Keywords: Ensemble methods; Sparse methods; Multi-model optimization; High-dimensional
data; Model diversity.

2
2
0
2

r
p
A
7
1

]
E
M

.
t
a
t
s
[

1
v
0
0
1
8
0
.
4
0
2
2
:
v
i
X
r
a

1

 
 
 
 
 
 
1

Introduction

In modern data analysis tasks, a model with good prediction accuracy is typically not

suﬃcient; in high-stakes data-driven decision making it is often necessary to obtain a model

that is also interpretable. Recently, there have been a lot of inﬂuential articles advocating

for the need of statistical procedures that retain a certain degree of interpretability despite

their high prediction accuracy, see e.g. relevant discussions in Rudin (2019), Murdoch et al.

(2019) and Rudin et al. (2022). The issue of interpretability is particularly important for

the analysis of high-dimensional data where the number of predictors p is much greater

than the number of samples n (p

n), and parsimonious models where only a small

subset t
p of the predictors are included are preferred. For example, in the analysis
of deoxyribonucleic acid (DNA) microarrays, the expression levels for thousands of genes

≪

≫

are collected. A valuable model would be one with high prediction accuracy, but also one

in which only a small subset of genes are identiﬁed as relevant to predict the outcome of

interest.

To address the problem of prediction accuracy and interpretability in the presence of

high-dimensional data, sparse regularization methods have been developed over the last

three decades. In essence, sparse regularization methods optimize the goodness-of-ﬁt of a

single model while penalizing its complexity, resulting in an interpretable model with good

prediction accuracy. Many regularization methods have been developed for a large class

of statistical models, see e.g. Hastie et al. (2019) for an extensive and modern treatment.

While prediction has always been a subject of importance for sparse regularization methods,

there is a much stronger emphasis on inference and uncovering the true mechanism of

how the output is generated as a function of the predictor variables. A deep theoretical

treatment of sparse regularization methods, in terms of estimation and variable selection,

may be found in B¨uhlmann and Van De Geer (2011). While sparse regularization methods

have well-established statistical theory and result in interpretable models, they are often

outperformed in terms of prediction accuracy by ensemble methods.

Ensemble methods, where multiple diverse models are generated and aggregated, are

some of the most popular “blackbox” algorithms for the analysis of high-dimensional data.

They have led to a plethora of successful applications in genetics (see e.g. Dorani et al.

(2018); Zahoor and Zafar (2020)), computer vision (see e.g. Rodriguez-Galiano et al. (2012);

Yu and Zhang (2015)), speech recognition (see e.g. Krajewski et al. (2010); Rieger et al.

(2014)), fraud detection (see e.g. Kim and Sohn (2012); Louzada and Ara (2012)), and

many other ﬁelds. Diverse members are essential for the good predictive performance of
ensembles (Brown et al., 2005). Current state-of-the-art methods rely on randomization or

the sequential reﬁtting of residuals to achieve diversity, which results in a large number of

2

uninterpretable models with poor individual prediction accuracy.

In this article, we introduce a unifying framework that combines the interpretability of

sparse regularization methods with the high prediction accuracy of ensemble methods. In

particular, we generalize sparse regression methods to multi-model regression ensembles.

The proposed methodology results in ensembles comprised of a small number of sparse and

diverse models, learned jointly from the data, that each have a high prediction accuracy.

Thus each of the models in the ensembles provide a possible explanation for the relationship

between the predictors and the response. Further, the ensembles achieve high prediction

accuracy compared to state-of-the-art ensemble methods. To convey our ideas, we focus on
regression ensembles.

The remaining of this article is organized as follows. In Section 2 we provide a literature

review of sparse and ensemble methods. In Section 3 we introduce the unifying framework

between sparse and ensemble methods. In Section 4, we generalize stepwise regression to

multi-model ensembles, which will constitute the initialization procedure for the Algorithm

of Section 5.
In Section 5 we introduce a projected subsets gradient descent algorithm,
adapting ℓ0 optimization approaches to multi-model regression ensembles. In Section 6 we
perform an extensive simulation study to benchmark the proposed methodology against

state-of-the-art methods. In Section 8 we benchmark the proposed methodology on a gene

expression data application. Section 9 closes the article with a discussion.

2 Literature Review: Sparse and Ensemble Methods

We study the linear model with a dataset consisting of a response vector y = (y1, . . . , yn)T
Rp for p predictors,
Rn and a design matrix X

∈

∈

Rn×p comprised of n observations xi ∈
yi = x′

iβ0 + σǫi,

n,

1

i

≤

≤

where β0 ∈
We are interested in the high-dimensional scenario where p

Rp and the elements of the noise vector ǫ = (ǫ1, . . . , ǫn)T

Rn have variance 1.
n and the underlying model

∈

is sparse, i.e. only a small fraction of the available predictors are relevant for explaining

≫

the response. For simplicity, we omit the intercept term from the regression model. We
assume that the response y the entries of the design matrix, xij, 1
are standardized so that

n and 1

≤

≤

≤

≤

p,

j

i

1
n

n

Xi=1

xij = 0,

1
n

n

Xi=1

x2
ij = 1,

1

j

≤

≤

p,

1
n

n

Xi=1

yi = 0,

1
n

n

Xi=1

y2
i = 1.

3

2.1 Sparse Methods

Sparse regularization methods penalize model complexity. The purpose of such methods is
to ﬁnd the best sparse model that achieves good prediction accuracy. The most natural ap-

proach for sparse modeling is Best Subset Selection (BSS), ﬁrst mentioned in the literature

by Garside (1965), which solves the nonconvex problem

min
β∈Rpk

y

−

Xβ

2
2
k

subject to

β
k

k0 ≤

t.

(1)

The largest number of nonzero coeﬃcients t
1, p) in the regression coeﬃcients
≤
vector β = (β1, β2, . . . , βp)T is typically determined in a data-driven way, e.g. by cross-
validation (CV). While BSS has been shown to have desirable variable selection and es-

min(n

−

timation properties (see e.g. Bunea et al. (2007); Shen et al. (2013)), it is an NP-hard

problem (Welch, 1982). There are

(p, t) =

K

t

Xj=0 (cid:18)

p
j

(cid:19)

(2)

possible subsets that must be evaluated to determine the exact solution. For example,

(15, 10) = 30,826 which is already a large number of subsets, even in this setting with

K
a small number of predictor variables. While many proposals have been made to deter-

mine the optimal subset based on the training data (see e.g. Mallows, 1973; Akaike, 1974;

Schwarz, 1978), CV is often recommended (Hastie et al., 2009b) which makes the proce-

dure even more computationally intensive. The branch-and-bound algorithm developed by

Furnival and Wilson (1974) was initially the procedure of choice for BSS, but it did not

scale well beyond p > 30. While an improved branch-and-bound algorithm was developed

by Gatu and Kontoghiorghes (2006), the method still does not scale well beyond p > 100.

To address the computational infeasibility of BSS, stepwise algorithms were devel-

oped (see e.g. Jennrich and Sampson (1968); Pope and Webster (1972); Bendel and Aﬁﬁ

(1977)). At each step a variable is added (forward selection) and/or removed (backward

elimination) from a subset of model predictors based on model goodness-of-ﬁt until no fur-

ther step can improve the model to a statistically signiﬁcant extent. Stepwise algorithms

have been heavily criticized and are regarded as a form of data dredging with poor model

selection properties (see e.g. Rencher and Pun (1980); Wilkinson and Dallal (1981); Copas

(1983); Hurvich and Tsai (1990); Roecker (1991)).

To address the shortcomings of stepwise procedures, sparse regularization methods were

subsequently popularized ﬁrst by basis pursuit denoising (Chen and Donoho, 1994) and

then the closely related Lasso (Tibshirani, 1996), a convex relaxation of BSS which solves

4

problems of the form

or in its Lagragian form

min
β∈Rpk

y

−

Xβ

2
2
k

subject to

β
k

k1 ≤

t,

min
β∈Rpk

y

−

Xβ

2
2 + λ
k

β
k

k1.

(3)

(4)

Many diﬀerent sparse regularization methods have been proposed (see e.g. Zou and Hastie

(2005); Candes and Tao (2007); Zhang (2010)). Eﬃcient convex solvers have been devel-

oped for the Lasso (see e.g. Efron et al. (2004); Friedman et al. (2007)), however restrictive

conditions on the covariance of the predictors must hold for the Lasso to have good vari-

able selection properties (see e.g. Zhao and Yu (2006)) and good relative prediction error

compared to BSS (see e.g. Zhang et al. (2014)).

Recent developments by Bertsimas et al. (2016) to study the BSS nonconvex problem
(3) with a modern optimization lens has led to new research avenues in ℓ0-penalized statis-
tical procedures (see e.g. Bertsimas and Van Parys (2020); Takano and Miyashiro (2020);

Kenney et al. (2021); Thompson (2022)). Bertsimas et al. (2016) ﬁrst generate good local

solutions with a projected gradient descent algorithm, and then use them as warm-starts

for a MIO solver. Thompson (2022) adapted their approach approach to develop a ro-

bust version of BSS. Their approach scaled to problems of dimension p > 1,000, but even

with the warm-starts the MIO solver may still take upwards of 30 minutes to compute.

To reduce the need for a MIO solver, Hazimeh and Mazumder (2020) proposed a method

that generates new candidate solutions to reduce the need for a MIO solver. Once a local

(incumbent) solution has been obtained from a projected gradient descent algorithm, they

apply small perturbations to the local solution to yield new starting points and apply the

projected gradient descent algorithm to each one of them. If the best solution obtained

from the new candidates improves on the incumbent solution, it is set as the new solution.

This process is repeated until no signiﬁcant diﬀerence occurs for the objective function.
They showed empirical evidence that their proposal often recovers either the optimal or a

near-optimal solution to BSS for p > 1,000 in a matter of seconds.

To explain the intuition for the potential reduction in prediction error of sparse regular-
T
ization methods, consider an estimator ˆf (x) = ˆβ
0 x.
The mean squared prediction error (MSPE) of ˆf may be decomposed into its bias, variance
and irreducible error,

x of the regression function f (x) = βT

MSPE

ˆf
i

h

= Ex

(f (x)

h

−

ˆf (x))2

+ σ2 = Bias

i

5

2

ˆf
i

h

+ Var

+ σ2.

ˆf
i

h

(5)

Since least squares regression is the best linear unbiased estimator (BLUE), the rationale

for regularized estimation is to exploit the bias-variance trade-oﬀ favorably, i.e. to incur a

small increase in bias in exchange for a larger decrease in variance.

2.2 Ensemble Methods

To understand the competitive advantage of ensemble methods in terms of prediction accu-

racy, we ﬁrst decompose their MSPE. For an ensemble comprised of G regression functions,
¯f =

ˆfg/G, then its MSPE can be decomposed as

G
g=1

P

where

MSPE

¯f

= Bias

¯f

2 + Var

¯f

+ σ2,

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

Bias

¯f

= Bias

and Var

¯f

=

1
G

Var +

G

1

−
G

Cov,

(6)

(7)

(cid:2)
and Bias, Var and Cov are the average biases, variances and pairwise covariances of the G
regression functions in the ensemble (Ueda and Nakano, 1996). From (7) it is clear that an

(cid:3)

(cid:2)

(cid:3)

ensemble can successfully reduce its variance if the models in the ensemble are suﬃciently

diverse (uncorrelated), especially if the number of models is large.

The statistics and machine learning community have seen an increase in algorithmic

approaches to generate ensembles over the last twenty years, with most proposals relying

on randomization (see e.g. Breiman, 2001; Song et al., 2013) or boosting (see e.g. Friedman,

2001; B¨uhlmann and Yu, 2003; Schapire and Freund, 2012; Yu et al., 2020). Interpretabil-

ity of such ensembles is typically unfeasible. However, several ad hoc methods have been

developed to assess predictor importance (see e.g. Hastie et al., 2009a). In an attempt to

bridge the gap between interpretability and ensemble methods, B¨uhlmann et al. (2006) in-
troduced sparse boosting by minimizing some penalized ℓ2-loss function for better variable
selection.

The purpose of these ensemble methods is to generate a collection of diverse models

comprised of diﬀerent subsets of predictors. For example, in Random Forests random sam-

pling of the data (bagging) (Breiman, 1996a) and the random predictor subspace method

(Amit and Geman, 1997; Ho, 1998; Dietterich, 2000) are combined to generate uncorrelated

trees for the purpose of achieving a lower generalization error (Breiman, 2001). In gradient

boosting, diverse members (typically decision trees) are generated by sequentially ﬁtting

the residuals of the previous ﬁt.

6

2.3 A Unifying Methodology

The multiplicity of good models is a phenomenon that has long been acknowledged, see
e.g. relevant discussions in McCullagh and Nelder (1989) and Mountain and Hsiao (1989).

Diﬀerent, yet equally good models can provide distinct explanations for the underlying

relationship between predictors and response. However, based on the philosophy of Rudin

(2019), current state-of-the-art ensemble methods lack interpretability as they typically

consist of either a large number of models generated using random-based approaches or a

(smaller) number of models indirectly generated by sequentially ﬁtting residuals instead of

the data. The models in the ensembles do not have high prediction accuracy on their own;

they only work well when they are pooled together in the ﬁnal ensemble ﬁt. Thus each

model is not insightful or reliable on its own. Hence, there is a gap between interpretable

single model methods such as sparse regularization and algorithmic ensemble methods. We

aim to ﬁll this gap by developing a systematic approach to construct ensembles consisting

of a relatively small number of interpretable sparse models with high individual prediction

accuracy. Each of these models is learned directly from the data and provides a reliable

relationship between the predictors and the response. Diversity between the models is

imposed by restricting the sharing of predictors between diﬀerent models.

3 Best Split Selection

We now formally introduce the unifying framework between sparse and ensemble methods.

Suppose we wish to ﬁnd a collection of G

Denote the matrix of model coeﬃcients

≥

2 sparse and diverse models in an ensemble.

β1:G =

1 β2
β1
1
2 β2
β1
2
...
...
β1
p β2
p









. . . βG
1
. . . βG
2
...
. . .
. . . βG
p

,









(8)

Rp×G and βg

j is the coeﬃcient for predictor j of model g, 1

where β1:G ∈
G. For
notational convenience let βg = (βg
Rp be the coeﬃcients of model g and
RG the coeﬃcients of predictor j across the G models. Then
βj· = (β1
j , . . . , βG
Best Split Selection (BSpS) solves, for a ﬁxed number of sparse models G, the nonconvex

2 , . . . , βg

1 , βg

j , β2

j )T

p )T

≤

≤

∈

∈

g

problem

min
β1,..., βG∈Rp

G

g=1
X

(y

−

Xβg)2

subject to

βg
k0 ≤
k

βj·k0 ≤

k

t,

1

u, 1

g

j

≤

≤

≤

≤

G,

p.

(9)



7

≤

min(n

The parameter t

1, p) restricts the ℓ0-norm of the columns of β1:G and thus the
number of nonzero coeﬃcients in each model. The parameter u
G restricts the ℓ0-norm
of the rows of β1:G and thus the number of models that share any given predictor. Note
that if u = G, then (9) is equivalent to BSS in (3) for the same value of t and there is

≤

−

no diversity among the models. Hence BSpS may be seen as a generalization of BSS to

multiple groups. The tuning parameters may be chosen in a data-driven manner by using

CV for instance.

BSpS thus aims to ﬁnd G sparse models, in such a way that each model explains well

the response and the diﬀerent models do not have much overlap. In this way, the models
complement each other well in an ensemble. While there are many proposals in the literature

to obtain an optimal ensembling function (see e.g. Breiman (1996b)), for simplicity in this

article the ensemble ﬁt corresponding to the G models selected by (9) is given by

¯β =

1
G

G

g
ˆβ

.

g=1
X

(10)

Hence, in contrast to algorithmic ensemble methods, but similarly to regularization meth-

ods, the ensemble model is an interpretable, sparse linear model. The ensemble model

combines the information of the G individual models, which individually provide an expla-

nation for the relationship between a subset of the predictors and the response.

3.1 Split Combinatorics

The total number of possible splits of p variables into G groups, for p

G, was derived

≥

by Christidis et al. (2020). We extend their combinatorics result to the BSpS optimization

problem (9) for the case without overlap between the models (u = 1). Note that the

computational problem for BSpS is even larger if predictors are allowed to be shared between
groups (u > 1). Let pg be the number of variables in group g, 1
q =

G, and let
G
g=1 pg. Also let hi(p1, . . . , pG) be the number of elements in the sequence p1, . . . , pG
t. The number of possible splits of p features into G groups

that are equal to i, 1

≤

≤

g

i

P

≤

≤

comprised of at most t variables is given by

(p, G, t) =

T

p
q

(cid:19) "

q!
p1! . . . pG!

t

Yi=1

1
hi(p1, . . . , pG)! #

.

Xp1≤···≤pG≤t (cid:18)

(11)

T

For example,

(15, 3, 10) = 171,761,941. Thus, even for a relatively small number of

predictor variables, the issue of computational infeasibility of BSpS becomes apparent and

will be magniﬁed further if t and u in (9) are chosen by CV. The BSS optimization problem

in (3) can always make use of MIO to generate global solutions using locally optimal

8

solutions as initial candidates (see e.g. Bertsimas et al. (2016); Thompson (2022)). The

computational infeasibility of BSpS, as seen from the combinatorics in (11), eliminates this

approach for (9).

3.2 Related Work

In related work, Christidis et al. (2020) recently introduced the Split Regularized Regression

(SplitReg) method which can be seen as a computationally more attractive, multi-convex

relaxation of BSpS. While hard thresholds are used in BSpS in (9), soft thresholds are used
in SplitReg which can be incorporated in the objective function more easily.

In detail, SplitReg is a minimizer β1:G = (β1, . . . , βG)

of the form

Rp×G of an objective function

∈

y, X, β1, . . . , βg

=

J

(cid:0)

(cid:1)

G




Xg=1

1
2n k

y

Xβg

2 + λsPd (βg) +
2
k

−

λd
2

G

Xg6=h

Pd

βh, βg
(cid:16)




(cid:17)

(12)

where Ps and Pd are sparsity and diversity penalty functions, and the constants λs, λd > 0,
which may be chosen e.g. by CV, control the magnitude of the eﬀect of the sparsity and





diversity penalties. Christidis et al. (2020) propose to use as sparsity and diversity penalties

Ps(β) =

β
k

k1

and

Pd(βg, βh) =

p

Xj=1

βg
|

βh
||

|

(13)

Hence, Ps(β) is the Lasso penalty while the diversity penalty Pd(βg, βh) is an ℓ1-norm
relaxation of the hard threshold in (9). Note that for λd = 0, SplitReg in (12) is equivalent
to sparse estimation with the penalty Ps, irrespective of the number of groups G.

The ensemble ﬁt corresponding to the solution of (12) is again given by (10). Christidis et al.

(2020) showed that with the penalties in (13) the ensemble estimator yields consistent pre-

dictions and has a fast rate of convergence. Moreover, the general framework (12) allows

the diversity penalty to be combined with alternative sparsity penalties such as the group

Lasso (Yuan and Lin, 2006) for categorical variables or the fused Lasso (Tibshirani et al.,

2005) for data containing spatial or temporal structures. The SplitReg objective function

is multi-convex and can be solved eﬃciently via a block coordinate descent algorithm.

Unlike the parameters t and u in BSpS, the parameters λs and λd do not directly control
the number of predictors in each model and the number of models that can share any given
predictor. There is theoretical and empirical evidence that such sparse regularization meth-

ods can negatively aﬀect variable selection (see e.g. Van De Geer and B¨uhlmann (2009);

Hazimeh and Mazumder (2020)). Further, the penalties (13) in SplitReg induce shrinkage

of the coeﬃcients, and empirical studies and it has been suggested that shrinkage may

9

have a negative eﬀect on prediction in high signal-to-noise scenarios (see e.g. Hastie et al.

(2020)). We develop computational tools to directly optimize BSpS in (9) and alleviate

the issues associated with shrinkage-inducing methods. In Section 4 we generalize forward

stepwise regression to optimize BSpS in (9) for the special case u = 1, which constitutes

an initial starting point for our main algorithm.

In Section 5 we develop a computing

algorithm capable of handling any u

.
1, . . . , G
}

∈ {

4 Stepwise Split (Regularized) Regression

In this section, we generalize forward stepwise regression to multi-model regression ensem-

bles. Our aim is to develop a fast algorithm that generate solutions for BSpS in (9) in the

particular case that u = 1 (i.e. when models are fully disjoint), which will constitute the

initial starting point for our main algorithm in Section 5.

For notational convenience, for any subset S

R|S| the subvector of β
of the set S, βS ∈
the submatrix of X with column indices S. We denote by In ∈
of order n and F −1
degrees of freedom, respectively.

the cardinality
Rn×|S|
Rn×n the identity matrix
(d1,d2)(t) the quantile function of the F -distribution with with d1 and d2

we denote
Rp with element indices S, and XS ∈

1, . . . , p

⊆ {

S
|

∈

}

|

The stepwise split (regularized) regression algorithm is described in detail in Algorithm
1. Initially, each model is comprised of no predictor variables. At each iteration of step

1, the candidate predictor that provides the largest improvement in goodness-of-ﬁt to each

unsaturated model is identiﬁed. The unsaturated model with the most statistically signif-

icant possible goodness-of-ﬁt improvement based on an F -test (at some level γ

[0, 1]) is

updated by adding its optimal candidate predictor to its set of model predictors. Once a

∈

predictor is included in a set of model predictors, it is removed from the set of candidate

predictors and can no longer be used in another model. A model is declared saturated if

there are no remaining candidate predictors providing any statistically signiﬁcant improve-

ment to the goodness-of-ﬁt, or if it contains n

all G models are saturated.

1 predictors. This process is repeated until

−

The least squares or Lasso ﬁt is applied to each model in step 2. If the Lasso is the
ﬁtting method of choice, each model is shrunk by a custom parameter λ(g) chosen by CV.
For completeness we include the stepwise procedure in Algorithm 1, which we refer as Step-

SplitReg, in the simulation study and real data applications of Sections 6 and 8 respectively.

Based on our numerical experiments, a Lasso ﬁt to each model in step 2 produces an

ensemble with better prediction accuracy and better variable selection compared to using

a least squares ﬁt, and so Step-SplitReg results are reported for the case where a Lasso ﬁt

is used for each model.

10

Algorithm 1 Stepwise Split (Regularized) Regression

Input: Design matrix X
∈
and signiﬁcance threshold γ

Rn×p, response vector y

[0, 1].

∈

Rn, number of models G

2,

≥

∈

Initialize: The set of candidate predictors J =
the set of predictors J (g) =
matrix H (g) = 0

G)
. For each model (1
}
, the model saturation indicator T (g) = false and the hat
∅

1, . . . , p
{

Rn×n.

≤

≤

g

∈

1: Repeat the following steps until γ∗

γ or T (g) = true for all 1

1.1: For each model g satisfying T (g) = false:

≥

g

≤

≤

G:

1.1:1: Identify candidate predictor maximizing decrease in residuals sum of squares,

j(g) = arg max

j∈J

τ (g)
j

,

τ (g)
j =

yT
xT
j

In −
In −
(cid:0)
(cid:0)

H (g)
H (g)

xj
xj

.

(cid:1)
(cid:1)

1.1:2: Calculate the p-value γ(g) of predictor j(g) in the enlarged model,

γ(g) = 1

F −1

(1,|J (g)|+1) 

−

1.1:3: If γ(g)

≥


γ set T (g) = true.

(cid:0)

J (g)
|
T
H (g)XJ (g)
(cid:0)

|

y

−

τ (g)
j(g)
H (g)XJ (g)
(cid:1)

+ 1

y

−

(cid:1)

(cid:0)

τ (g)
j(g)

−





(cid:1)

1.2: Identify the unsaturated model g∗ with the smallest p-value γ(g∗).
1.3: If γ(g∗) < γ:

1.3:1: Update the candidates J = J
j(g∗)
= n

1.3:2: If

J (g∗)

∪ {
J (g∗)
|
|

.
}
−

j(g∗)

\ {

}

and the set of model predictors J (g∗) =

1, set T (g∗) = true. Otherwise, update the model hat matrix

H (g∗) = XJ (g∗)

X T

J (g∗)XJ (g∗)

−1

X T

J (g∗).

2: For each model g (1

applied,

g

≤

≤

g
G), set ˆβ

(cid:0)
= 0p ∈

Rp and if (custom) regularization is

(cid:1)

g
ˆβ
|J (g)| = arg min

y
β∈R|J (g∗)|k

−

XJ (g)β

2 + λ(g)
2
k

β
k

k1,

where λ(g) is chosen by CV. Otherwise,

g
ˆβ
|J (g)| = XJ (g)

X T

J (g)XJ (g)

−1 y

g
3: Return the sets of model predictors J (g) and their coeﬃcients ˆβ

(cid:1)

(cid:0)

, 1

g

≤

≤

G.

An R/C++ library implementing Algorithm 1 is available on CRAN (R Core Team, 2022)

11

under the name stepSplitReg (Christidis et al., 2022b). Many more features than those
described in Algorithm 1 are available, including diﬀerent model saturation criteria and dif-

ferent regularization procedures for the ﬁnal sets of model predictors. While the stacking

procedure of Breiman (1996b) is available in the package, in this article we focus on the sim-

ple case where each model is weighted equally using (10). A reference manual with the com-
plete details of the package is available at https://CRAN.R-project.org/package=stepSplitReg.

5 Projected Subsets Gradient Descent

We adapt ideas from ℓ0-penalized optimization to the BSpS problem in (3), and develop an
algorithm that is perfectly suited for the cross-validation of the diversity tuning parameter

u in (9) and minimizes the need for a time-consuming local combinatorial search. For

notational convenience, denote the loss function

(β

y, X) =
|

L

G

Xg=1

Xβg)2 ,

(y

−

where the gradient of the loss is given by

β

∇

L

(β

y, X) = X T (Xβ
|

−

y).

(14)

(15)

We note that (15) is Lipschitz continuous with Lipschitz constant Lβ =
spectral norm of X T X, i.e.

X T X
k

k2 the

β

∇

L

(β

y, X)
|

β

− ∇

L

˜β
y, X
|

Lβ

β

˜β

β, ˜β

Rp.

2 ≤

−

2 ∀

∈

(cid:17) (cid:13)
(cid:13)
The loss function (14) is bounded from above by its quadratic approximation with Lipschitz
constant Lβ,

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:16)

˜β
y, X
|

L

≤ LQ

(cid:17)

(cid:16)
We deﬁne S(g)
excluding model g, i.e.

⊆

(cid:16)

˜β
y, X
|

=

(β

y, X) +
|

L

β

∇

L

(β

y, X)T
|

˜β

−

β

+

(cid:17)

(cid:13)
(cid:13)
J the subsets of predictors that are used in at most u

(cid:16)

(cid:17)

1
2

Lβ

˜β

β

2
2.

−

j

S(g) = 


J :

∈

G

I

j

Xh=1
h6=g

(cid:16)

J (h)

∈

u

−

≤

,

1


(cid:17)

where J (g) =

j
{

∈

g
J : ˆβ
j 6


, 1
= 0
}

g

≤

≤


G, as deﬁned in Section 4. Central to our algorithm

12

(cid:13)
(cid:13)
1 models

−

(16)

J as

S

⊆

The operator

P

is the projected subset operator, which we deﬁne for any vector v

Rp and some subset

∈

(17)

(v; S, t)

P

∈

arg min
w∈Rp
kwk0≤t
{j∈J:wj6=0}⊆S

w
k

−

v

2
2.
k

(v; S, t) retains the t largest elements in absolute value of the vector v that

belong to the set S. It is a set-valued map since more than one possible permutation of the

indices

J : j

S

may exist.

j
{

∈

∈

}

The projected subsets gradient descent (PSGD) algorithm is given in Algorithm 2. The
G
, . . . , ˜β
are provided. In step 1 of the computing

1
algorithm assumes initial solutions ˜β
algorithm, the projected subset gradient descent algorithm is applied to each model once

cyclically until convergence. In particular the update for model g of the projected subset
gradient descent algorithm using subset S(g) and model size t can be written as

g
ˆβ

∈

=

arg min
βg∈Rp
kβgk0≤t

{j∈J:βg

j 6=0}⊆S(g)

arg min
βg∈Rp
kβgk0≤t

{j∈J:βg

j 6=0}⊆S(g)
1
Lβ ∇

−

g
˜β

=

P

(cid:18)

LQ

(cid:16)

˜β
y, X
|

(cid:17)

g
˜β

−

(cid:18)

βg

1
Lβ ∇

β

L

−

(βg

y, X)
|

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

(cid:19) (cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

β

L

g
˜β

(cid:16)

y, X
|

(cid:17)

; S(g), t

.

(cid:19)

We note that for each model the iterative algorithm produces a sequence of converging

solutions (Bertsimas et al., 2016, Proposition 6). After convergence is reached, its set of
predictor variables J (g) =
computed.

is updated and the ﬁnal model coeﬃcient is

g
J : ˆβ
j 6

= 0
}

j
{

∈

In the optional step 2 of Algorithm 2, local combinatorial searches using random per-

mutations of the groups are performed to improve on the solution from step 1. Based on

our numerical experiments, a poor initial solution in step 1 increases the need for step 2

to yield better solutions which come with a high computational cost. However, a carefully

designed algorithm that generates good initial solutions for Algorithm 2 alleviates the need
for the random permutations of models cyclic update in step 2.

To generate good initial solutions for Algorithm 2, in Algorithm 3 we decrement the

diversity of the models progressively until u = G in (9). Initial sets of model predictors

and their coeﬃcients are generated using Algorithm 1. Then Algorithm 2 is applied for

13

Algorithm 2 Projected Subsets Gradient Descent (PSGD)

Rn×p, response vector y

Input: Design matrix X
,
Lipschitz constant Lβ, sparsity and diversity tuning parameters t and u, and tolerance
parameter ǫ > 0, (optional) number of cycling iterations C.
g
J : ˜β
j 6

Initialize: The current model predictors J (g) =

, 1
= 0
}

1
Rn, current solutions ˜β

G
, . . . , ˜β

j
{

G.

≤

≤

∈

∈

∈

g

1: Repeat the following steps for each model g, 1

g

≤

≤

G:

1.1: Update the allowed predictors S(g) =

j

g
1.2: Update ˜β

as

(cid:26)

J :

∈

I(j

G
h=1
h6=g

∈

J (h))

P

u

−

≤

.

1
(cid:27)

g
ˆβ

∈ P

(cid:18)

g
˜β

1
Lβ ∇

β

L

−

g
˜β

(cid:16)

y, X
|

(cid:17)

; S(g), t

(cid:19)

g
( ˜β

L

until

y, X)
|

ǫ.
1.3: Update the model predictors J (g) =
j
{
1.4: Compute the ﬁnal model coeﬃcients

y, X)
|

− L

≤

g
( ˆβ

g
J : ˆβ
j 6

.
= 0
}

∈

g
ˆβ

= arg min

β∈Rp
βg
j =0,j /∈J (g)

(β

y, X)
|

L

2: (Optional) Repeat the following steps C times:

2.1: Draw a random permutation (ω(1), . . . , ω(G)) of (1, . . . , G).

2.2: Repeat step 1 using the new order (ω(1), . . . , ω(G)) and the current solutions as

2.3: If

initial values.
g
( ˆβ
old solutions J (g) =

G
g=1 L

y, X) <
|
j
P
∈
{

P

g
( ˜β
G
g=1 L
g
J : ˜β
j 6

y, X), retain new solutions. Otherwise keep the
|
= 0
}

g
and ˆβ

g
= ˜β

, 1

G.

≤

≤

g

g
3: Return the sets of model predictors J (g) and their coeﬃcients ˆβ

, 1

g

≤

≤

G.

u = 1, 2, . . . , G. Based on our numerical experiments, Algorithm 3 produces competitive

solutions in terms of minimizing the objective function of BSpS in (9) compared to the

local combinatorial searches of Algorithm 2. For the purpose of running large experiments

on simulated and real data, in the remaining of this article we use Algorithm 3 without the

local combinatorial search in Algorithm 2.

The sparsity and diversity tuning parameters in (9), t and u respectively, need to be

determined from the training data. We use 5-fold CV for the grids of candidates t and u,

looking to minimize the CV MSPE. For a ﬁxed sparsity level t, Algorithm 3 is perfectly

suited to generate solutions for a grid of candidates for u. This process is repeated for every

14

candidate t.

Algorithm 3 Decrementing Diversity PSGD

Rn×p, response vector y
Input: Design matrix X
∈
maximum model size t, and tolerance parameter ǫ > 0.

∈

Rn, Lipschitz constant Lβ,

Initialize: Using Algorithm 1 initialize the sets of model predictors and their coeﬃ-
g
cients for u = 1, J (g)(1) and ˆβ

(1), 1

G.

g

≤

≤

g
1: Using Algorithm 2 update J (g)(1) and ˆβ

(1) with the current solutions as initial solu-

tions.

2: For u = 2, . . . , G repeat the following step:

g
2.1: Compute J (g)(u) and ˆβ

g
and ˆβ

(u

−

1), 1

g

≤

≤

(u) using Algorithm 2 with initial solutions J (g)(u
G.

1)

−

g
3: Return the sets of model predictors J (g)(u) and their coeﬃcients ˆβ

(u), 1

u, g

G.

≤

≤

An R/C++ library implementing the PSGD Algorithms 2, 3 and the CV procedure is
available on CRAN under the name PSGD (Christidis et al., 2022a). Multithreading is avail-
able in the library with OpenMP (Chandra et al., 2001) to reduce the computational cost

of the method. The computational cost of the method (using a single thread) as function

of the number of models G is explored in Section 7. A reference manuel with the complete
details of the package is available at https://CRAN.R-project.org/package=PSGD.

6 Simulation Study

For each Monte Carlo replication, we generate data from the linear model

yi = x′

iβ0 + σǫi,

1

i

≤

≤

n,

Rp are multivariate normal with zero mean and correlation matrix Σ

where the xi ∈
∈
Rp×p and the ǫi are standard normal. We consider two combinations of p and n, namely
(p, n) = (500, 50) and (p, n) = (150, 50). For each p, we consider the proportion of active

variables ζ = 0.1, 0.2 and 0.4.

The p0 = [pζ] nonzero elements of the p-dimensional vectors β0 are randomly gener-
ated as described in Fan and Lv (2008), i.e. nonzero coeﬃcients are set to (
),
|
where a = 5 log n/√n, u is drawn from a Bernoulli distribution with parameter 0.2 and
z is drawn from the standard Gaussian distribution. For (p, n) = (500, 50), the ℓ2-norms
β0k
ζ considered. For
k

range from 21.01 to 43.22 for all proportions sparsity levels 1

1)u(a +

z
|

−

−

15

(p, n) = (150, 75), the ℓ2-norms
scenarios for Σ.

β0k
k

range from 11.30 to 22.80. We consider two diﬀerent

Scenario 1:

Scenario 2:

1

if i = j

ρ if i

= j

otherwise.

Σi,j = 


if i = j

0

1

ρ if 1

i, j

p0, i

= j

≤
otherwise.

≤

Σi,j = 



0

In Scenario 1, all the predictors are correlated among each other. In Scenario 2, the

active variables are only correlated with each other. For both scenarios we consider the

values ρ
. Then σ is chosen to give a desired signal to noise ratio (SNR),
0.2, 0.5, 0.8
∈ {
}
deﬁned as SNR = β′
0Σβ0/σ2. We consider SNRs of 1, 3 and 5. We report results for all
scenarios across all considered sparsity levels, correlations, SNRs and the two combinations

of p and n.

6.1 Methods

We ran a simulation study comparing the prediction accuracy of eleven methods. In partic-

ular we consider four sparse regression methods, their analogous split regression methods,

and three “blackbox” regression ensemble methods. All computations were carried out in
R with their default settings.

1. Stepwise forward regression, computed using the lars package (Hastie and Efron,

2013).

2. Fast-BSS, computed using the L0Learn package (Hazimeh et al., 2021) with the ℓ0-ℓ1

penalty option.

3. Lasso, computed using the glmnet package (Friedman et al., 2010).

4. Elastic Net (EN) with α = 3/4 for the ℓ1-ℓ2 mixing parameter, computed using the

glmnet package (Friedman et al., 2010).

5. Step-SplitReg, computed using the stepSplitReg package with a custom Lasso ﬁt

for each model.

6. Fast-BSpS, computed using the PSGD package.

16

6
6
7. SplitReg-Lasso, computed using the SplitReg package (Christidis et al., 2020).

8. SplitReg-EN with α = 3/4 for the ℓ1-ℓ2 mixing parameter, computed using the

SplitReg package.

9. Random GLM (RGLM) (Song et al., 2013), computed using the RGLM package (Song and Langfelder,

2013).

10. Random Forest (RF), computed using the randomForest package (Liaw and Wiener,

2002).

11. Extreme Gradient Boosting (XGBoost) (Chen and Guestrin, 2016), computed using

the xgboost package (Chen et al., 2020).

For a fast computation of BSS, we use the state-of-the-art method of Hazimeh and Mazumder

(2020). In their implementations, Hazimeh et al. (2021) recommend to combine ℓ0 regu-
larization with shrinkage-inducing penalties to avoid overﬁtting and improve predictive
performance, and thus we use the ℓ0-ℓ2 combination of penalties. For the four split re-
gression methods, we use G = 5 models, a potentially suboptimal number of models. For

RGLM and RF we use G = 5 and their default number of models, G = 100 and G = 500

respectively. To reduce the computational burden of the PSGD algorithm in our large sim-

ulation study, we use the grids u

the CV procedure of BSpS. The ﬁner grids t

15, 20, 25
}
{
may be
1, . . . , 5
}
used for optimal predictive performance, however at a higher computational cost.

1, 2, 3, 4, 5
}

0.3n, 0.4n, 0.5n

∈ {
1, . . . , n

and u

and t

1
}

∈ {

∈ {

∈ {

in

=

−

}

6.2 Performance Measures

For each conﬁguration, we randomly generate N = 50 training and test sets and for each of

the methods measure average performance on the test sets. In each replication of a partic-

ular conﬁguration, a training set is generated to ﬁt the procedures, and a large independent

test set of size m = 2,000 is used to compute the MSPE. The MSPEs reported are relative
to the irreducible error σ2, hence the best possible result is 1. We also report the recall
(RC) and precision (PR), deﬁned for each parametric method as

RC =

p
j=1

P

= 0)

I(βj 6

p
j=1

= 0, ˆβj 6
I(βj 6
= 0)

, PR =

p
j=1

P

= 0)

I(βj 6

p
j=1

= 0, ˆβj 6
I( ˆβj 6
= 0)

,

where β and ˆβ are the true and estimated regression coeﬃcients, respectively. For the split
regression methods and RGLM, the average of the models (10) is the vector of coeﬃcients

P

P

used to compute the recall and the precision. For the tree-based ensemble methods RF and

XGBoost, the RC and PR are computed by identifying the predictors used in the trees of

17

the ensembles. We do not report the RC and PR of RGLM and RF when their default

number of models are used since their recall is always 1 and precision the proportion of

active variables ζ. Note that large values of RC and PR are desirable.

6.3 Results

In Table 1 we report the average rank for each performance measure across all simulations

settings. The best two ranks for each performance measure are in bold. The detailed

results of the simulation study are available in the supplementary material. Fast-BSpS had

the best average rank in terms of MSPE for both cases p = 500 and p = 150, whereas
SplitReg-EN had the second best performance in both cases. RGLM-100 had the best

average performance out of the “blackbox” methods, however its performance deteriorated

to the worst average rank when G = 5, the same number of models as Fast-BSpS. In

Section 7, we investigate this phenomenon in greater details by studying the eﬀect of the

number of groups on Fast-BSpS and RGLM. Step-SplitReg was not competitive in terms

of MSPE compared to Fast-BSpS or the SplitReg methods, however it did outperform its

single-model stepwise method consistently.

In terms of RC, Fast-BSpS had the second best rank overall, only beaten slightly by

RGLM-5. However, RGLM-5 had the third worst overall rank in PR, whereas Fast-BSpS

had the best PR rank for p = 500 and the best overall PR rank out of all ensemble methods.

An investigation of the full simulation results in the supplementary material reveals

that Fast-BSpS had its best performances relative to the competitors across all performance

measures in Scenario 2, which is more realistic than Scenario 1 where it is hard to distinguish

active from inactive predictors due to the correlation induced between them.

7 The Number of Models

For an ensemble comprised of a relatively small number of models, a balance of individual

model prediction accuracy and diversity between the models is necessary for overall en-

semble prediction accuracy. To achieve diversity, individual model accuracy must typically

take a hit (Brown et al., 2005). The BSpS framework searches for G diverse models that

have a small loss in the objective function (9). Thus each model in BSpS is learned directly

from the data, sparse, and achieves a high prediction accuracy. Thus in a sense Fast-BSpS

controls the accuracy-diversity tradeoﬀ directly.

In RGLM, bagging and the random subspace method are used to create G bags com-

prised of diﬀerent samples and predictors. Then, a subset of predictors in each bag are

retained based on a measure of correlation with the response, and forward selection is ap-

plied to this subset. RGLM thus resorts to randomization to generate a collection of diverse

18

Table 1: Average rank of the methods over the scenarios, correlations, SNRs and sparsity
levels for (p, n) = (500, 50) and (p, n) = (150, 50). The last column contains the overall
rank over both combinations of (p, n).

p = 500

p = 150

Overall Rank

Method MSPE RC

PR MSPE RC

PR MSPE RC

PR

Stepwise
Fast-BSS
Lasso
EN

Step-SplitReg
Fast-BSpS
SplitReg-Lasso
SplitReg-EN

RGLM-5
RGLM-100
RF-5
RF-500
XGBoost

12.06
4.81
7.20
6.15

9.07
2.67
3.56
2.85

12.24
3.57
10.02
5.67
11.13

11.00
6.89
9.81
8.81

3.87
6.02
4.22
4.19

1.85 10.26
3.02
3.70
6.15
5.00
5.59
3.85

3.24

8.46

−
7.63

−
10.15

−
4.20

−
4.07

11.17
5.50
6.50
5.89

6.96
2.26
3.31
2.65

12.69
6.50
10.30
5.83
11.44

11.00
6.77
9.78
8.72

5.21
2.24
5.58
4.60

1.46

3.07
4.91
3.65
4.30

8.98
5.81
5.00
5.41

9.50

−
6.13

−
10.67

−
4.50

−
4.70

11.62
5.15
6.85
6.02

8.02
2.46
3.44
2.75

12.46
5.04
10.16
5.75
11.29

11.00
6.83
9.80
8.77

3.53
2.97
5.29
4.22

2.35

3.47
5.46
3.93
4.25

9.62
4.42
5.58
5.50

8.98

−
6.88

−
10.41

−
4.35

−
4.38

models, resulting in models that are individually weak and not built to achieve the optimal

accuracy-diversity balance given the number of models G used. By the bias-variance-

covariance decomposition of regression ensembles in (6) and (7), if individual models are

weak, a large number of them would be required for the covariance term to dominate the

variance term of the ensemble and thus for the ensemble to achieve a good prediction

accuracy.

7.1 Accuracy-Diversity Empirical Study

We conduct a simulation to study the eﬀect of the number of models on Fast-BSpS and

RGLM with G = 2, 3, 4, 5, as well as G = 100 (the default) for RGLM. We use N = 50

replications of Scenario 2 in the simulation study of Section 6 with p = 500, ρ = 0.5, and

a SNR of 3. For each replication, Fast-BSpS and RGLM are applied to a training set of

size n = 50 and then a test set of size m = 2,000 is used to compute the ensemble MSPE,

the average mean squared prediction error MSPE of the individual models as well as their

average pairwise correlations Cor. The mean squared prediction errors reported are relative
to the irreducible error σ2, hence the best possible result is 1. The computation is repeated
. The
0.1, 0.2, 0.4
for various values of the proportion of active variables, namely ζ
}

∈ {

19

results are reported in Table 2.

Table 2: MSPE, MSPE and Cor of Fast-BSpS and RGLM as a function of the number of
models under Scenario 2 with ρ = 0.5 and SNR= 3.

ζ = 0.1

ζ = 0.2

ζ = 0.4

Method MSPE MSPE Cor MSPE MSPE Cor MSPE MSPE Cor

Fast-BSpS-2
Fast-BSpS-3
Fast-BSpS-4
Fast-BSpS-5

RGLM-2
RGLM-3
RGLM-4
RGLM-5
RGLM-100

1.29
1.21
1.20
1.19

4.34
3.38
2.86
2.47
1.36

1.57
1.62
1.75
1.76

7.37
7.69
7.74
7.50
7.70

0.85
0.83
0.80
0.80

0.25
0.22
0.22
0.23
0.23

1.31
1.22
1.21
1.16

4.38
3.17
2.63
2.30
1.25

1.55
1.62
1.73
1.75

7.51
7.15
6.95
6.69
7.03

0.87
0.85
0.83
0.82

0.27
0.29
0.30
0.31
0.29

1.27
1.21
1.18
1.16

3.50
2.75
2.37
2.12
1.17

1.53
1.56
1.61
1.63

5.95
6.00
6.07
6.13
6.64

0.85
0.85
0.84
0.83

0.34
0.34
0.33
0.34
0.33

For Fast-BSpS, it can be seen that for all sparsity levels MSPE increases with the number

of models, while MSPE and Cor decrease. As the number of models increases, the average

accuracy of the individual models has less impact on the ensemble MSPE compared to Cor

and Fast-BSpS achieves the proper balance for this tradeoﬀ, resulting in high accuracy for

the ensemble.

For RGLM, the individual models are extremely weak, with the MSPE of the individual

models being between seven to eight times the variance of the noise. The individual strength
of the models are not controlled or learned for the number of models in the ensemble, they

are equally weak regardless of the number of models. However, Cor is much lower than

RGLM for all number of models. When the number of models is increased, the average

pairwise correlation between the models are becoming more important, and only when

G = 100 does RGLM achieve an adequate ensemble MSPE, although still higher than Fast-

BSpS with G = 2 for all sparsity levels. Thus RGLM relies on a large number of weak

decorrelated models to achieve a low ensemble MSPE.

In applied research, investigation of the individual models of Fast-BSpS could therefore

reveal insightful information for the relationship between the predictors and the response,

while RGLM (or other random/indirect methods) do not enjoy this property. Indeed beyond

the high prediction accuracy of the individual models of Fast-BSpS, they learn the models

directly from the data and enjoy good variable selection as do sparse methods. In Table 3

we report the RC and PR of Fast-BSpS and RGLM as function of the number of models.

It can be seen that Fast-BSpS enjoys near-perfect PR and a high RC relative to RGLM.

RGLM has poor PR inherently due to the random nature of its methodology, and only

20

achieves high RC when a lot of models are used. For G = 100, RGLM naturally achieves a

RC of 1 and a PR of ζ.

Table 3: RC and PR of Fast-BSpS and RGLM as a function of the number of models under
Scenario 2 with ρ = 0.5 and SNR= 3.

ζ = 0.1

ζ = 0.2

ζ = 0.4

Method RC PR RC PR RC PR

Fast-BSpS-2
Fast-BSpS-3
Fast-BSpS-4
Fast-BSpS-5

RGLM-2
RGLM-3
RGLM-4
RGLM-5
RGLM-100

0.56
0.79
0.81
0.82

0.26
0.33
0.40
0.47
1.00

1.00
0.99
0.89
0.87

0.22
0.20
0.19
0.19
0.10

0.29
0.44
0.55
0.69

0.25
0.33
0.42
0.49
1.00

1.00
1.00
1.00
1.00

0.43
0.40
0.40
0.39
0.20

0.16
0.20
0.29
0.33

0.22
0.30
0.39
0.46
1.00

1.00
1.00
1.00
1.00

0.77
0.74
0.76
0.77
0.40

7.2 Computational Cost

While Fast-BSpS shows great promise in terms of prediction accuracy, it comes at a high

cost. The computational cost (in seconds) of the CV procedure of Fast-BSpS in Scenario

2 of Section 6, across all sparsity levels ζ

is provided in Table 4 as a

,
0.1, 0.2, 0.4
}

∈ {

function of the number of models. We include for comparison the computational cost
of the multi-convex relaxation of BSpS as described in Section 3.2 using the SplitReg
package (Christidis et al., 2020) as well as step-SplitReg as described in Section 4 using the
stepSplitReg package.

The step-SplitReg method has by far the smallest computational cost. The CPU sec-
onds for the R function calls of Fast-BSpS are signiﬁcantly higher and more sensitive to the
number of models compared to the multi-convex relaxation. We note that no local combi-

natorial search is performed in the execution of Algorithm 2 which may increase the cost

further. The computational cost can also increase substantially if a ﬁne grid for the sparsity
parameter t is used. However, given the diﬃculty in optimizing ℓ0-penalized problems, it
is expected that the cost of Fast-BSpS will be higher.

21

Table 4: Computation time of R function calls for the SplitReg, stepSplitReg and PSGD
packages in CPU seconds for varying number of models. CPU seconds are on a 2.7 GHz
Intel Xeon processor in a machine running Linux 7.8 with 125 GB of RAM.

Number of Models

Package

2

3

4

5

SplitReg

stepSplitReg

PSGD

2.23

0.25

4.67

6.56

0.69

10.41

14.91

1.05

1.17

19.38

31.43

55.92

8 Bardet-Biedl Syndrome Gene Expression Data

We benchmark Fast-BSpS and the competitor methods of Section 6 on the Bardet-Biedl syn-
drome (BBS) gene expression dataset in Li et al. (2020). In Scheetz et al. (2006) mutation

and functional studies were performed and identiﬁed TRIM32 (tripartite motif-containing

protein 32) as a gene with high correlation with BBS. The purpose of this study is to per-

form predict the gene expression level of TRIM32 using the expression levels of p = 200

genes from mammalian-eye tissue samples identiﬁed as relevant in Scheetz et al. (2006).

The dataset contains 120 mammalian-eye tissue samples. To mimic a high-dimensional

scenario, in each of the N = 50 replications we randomly split the data into a training set

of size n = 30 and a test set with the remaining m = 90 samples. For Fast-BSpS we use

u

1, 2, 3, 4, 5
}

. The other methods are computed
9, 12, 15
}
{
using their default settings as in Section 6. We report the MSPE for all the methods and

0.3n, 0.4n, 0.5n

and t

∈ {

∈ {

=

}

MSPE for the ensemble methods.

The results are reported in Table 5, where in each column the two best performances are

in bold. For the ensemble MSPE, RGLM-100 and Fast-BSpS had the best two performances,

with RGLM-100 edging out Fast-BSpS slightly. While RGLM-100 had a slightly lower

MSPE than Fast-BSpS, the individual MSPE of RGLM-5 or RGLM-100 were the worst out

of all the methods, being nearly three times the MSPE of Fast-BSpS or either of the SplitReg

methods. On the other hand, Fast-BSpS achieved the second best individual MSPE slightly

behind SplitReg-EN, matching the MSPE of its base sparse estimator BSS or the Lasso.

Fast-BSpS thus achieved to not only produce a competitive ensemble prediction accuracy

with only G = 5 models, but each models is on average as reliable and accurate as standard

sparse estimators.

As it was seen in Section 7, the individual models of Fast-BSpS not only have good

prediction accuracy, but they also tend to use the relevant predictors with high precision,

whereas RGLM relies on randomness to create diverse models which results in models with

22

Table 5: MSPE and MSPE over the N = 50 random splits into training and testing sets
for the BBS gene expression dataset.

Method MSPE MSPE

Stepwise
Fast-BSS
Lasso
EN

Step-SplitReg
Fast-BSpS
SplitReg-Lasso
SplitReg-EN

RGLM-5
RGLM-100
RF-5
RF-500
XGBoost

0.82
0.65
0.65
0.63

0.57
0.49
0.65
0.62

0.69
0.45
0.73
0.67
0.84

−
−
−
−
0.93
0.65
0.67
0.63

1.71
1.67
1.02
1.03
1.04

poor variable selection. Since the models in Fast-BSpS are learned directly from the data,

each predictor included in a model was included for its relevance in minimizing the error

term of the models in (9). Taking this one step further, we can use the models in Fast-BSpS

to rank the gene sets in order of importance. Deﬁning the sets

Ak =

j :




G

I

j

g=1
X

(cid:16)

S(g)

∈

k

≥




(cid:17)

,

1

k

≤

≤

G,

(18)

where

AG ⊆ AG−1 ⊆ · · · ⊆ A1, we can study the distribution of the genes across the
diﬀerent models. These sets identify genes related to TRIM32 in order of importance, since





these genes appear in more than one model if there are no surrogate genes that may be

used to reduce the loss function of BSpS in (9). Applying Fast-BSpS to the BBS dataset,
,
|A3 = 18
|
same sign across all the models, re-enforcing the understanding of their relationship with

. Genes shared by more than a single model had the
|A1 = 29
|

|A2 = 28
|

and

the gene expression level of TRIM32.

To illustrate that Fast-BSpS can identify important genes that may be missed by sparse
A3. This set contains 18 genes that appear in at least
half of the 5 Fast-BSpS individual models and thus yield an important contribution to the

regression methods, let us consider

ensemble. Interestingly, none of these genes appears in the Fast-BSS and thus would be

considered irrelevant for the prediction of the gene expression level of TRIM32.

23

9 Discussion and Future Directions

We introduced a new data-driven ensemble framework that generates a collection of sparse

and diverse models learned directly from the data.

In particular we introduced BSpS,

a generalization of BSS to multiple groups.

In BSpS the objective is to ﬁnd the best

possible split of predictors in a collection of models such that the sum of their individual

losses is minimized. The split of predictors must satisfy the sparsity constraint, i.e. the

maximum size of each model, and the diversity constraint, i.e. the maximum number of

models that can share any given predictor. Each model has a prediction accuracy that is

competitive with its base sparse estimator BSS, and thus presents a possible explanation

for the relationship between the predictors and the response.

An investigation of the intractable combinatorial problem posed by BSpS reveals the

need for computational tools to generate approximate solutions. Related work in Christidis et al.

(2020) to minimize an objective function with a sparsity and diversity penalty may be

viewed as a multi-convex relaxation of BSpS, which does not allow to directly control the

maximal model size and the number of models that may share predictors, and may have

poorer prediction and variable selection performance compared to the direct optimization

of BSpS. In this article we generalized forward stepwise regression to multiple groups to

generate an initial solution to BSpS in the fully diverse (u = 1) case, and a projected sub-

sets gradient descent algorithm to generate approximate solutions to BSpS for any u in (9)

that is perfectly suited for the CV procedure of the tuning parameters. Our empirical in-

vestigations via our simulation study and real data application reveal that our approach to

optimize BSpS directly yields and ensemble with competitive prediction accuracy and vari-

able selection properties compared to the multi-convex relaxation of BSpS and “blackbox”

regression ensemble methods.

We showed that the proposed methodology is eﬃcient in exploiting the accuracy-

diversity tradeoﬀ of regression ensembles, such that the optimal balance of individual model

accuracy and diversity is achieved by our proposed algorithm. Contrary to other “black-

box” regression ensemble methods such as RGLM, our methodology results in ensembles

that do not rely on a large number of weak models to achieve a high prediction accuracy.

Rather it relies on the search for strong individual models that have a certain degree of

diversity to reduce the variance of the ensemble. This allows for the usefulness of the sets
Ak, 1
accurately. This is particularly important in gene expression data where interpretability

G, in (18) to rank the predictors in order of importance to predict the outcome

≤

≤

k

and the identiﬁcation of the relevant genes is just as important as prediction accuracy.

For applied scientists, problem-speciﬁc knowledge can be incorporated into the BSpS

framework very easily. For example, if certain predictors (e.g. genes) are known to be

24

particularly important or relevant in the prediction of the outcome, it may be easily incor-

porated by generalizing BSpS in (9) to

(19)

p.

j

≤

≤

min
β1,..., βG∈Rp

G

Xg=1

(y

−

Xβg)2

subject to

βg
k0 ≤
k
βj·k0 ≤
k




t,

1

uj, 1

g

j

≤

≤

≤

≤

G,

p.

where uj is the maximum number of models that may share predictor j, 1



The PSGD algorithm we proposed is an eﬃcient way to generate solutions for the BSpS

nonconvex problem in (9). However, compared to Step-SplitReg and SplitReg, it suﬀers

from a slower computation time, particularly as the number of groups or the dimension of

the data are increased. A future area of research is to develop alternative computing proce-

dures for BSpS. One possible improvement on the current algorithm is to apply the general

idea of accelerated proximal gradient descent of Beck and Teboulle (2009) to projected gra-

dients and incorporate it in our algorithm. Perhaps a better initialization procedure than

the Step-SplitReg may be developed, or an eﬃcient way to apply local combinatorics to

search for better solutions to BSpS.

We hope our new data-driven ensemble framework will motivate new and exciting re-

search on this new paradigm for ensemble modeling. For example, BSpS in (9) may be

easily generalized to GLMs or other models with some general loss, i.e.

min
β1,..., βG∈Rp

G

g=1
X

(βg

y, X)
|

L

subject to

βg
k0 ≤
k
βj·k0 ≤
k




t,

1

uj, 1

g

j

≤

≤

≤

≤

G,

p.

(20)

In the analysis of high-dimensional data, sparse modeling was the main focus in the



literature for many years, with the overwhelming majority of proposals being diﬀerent

alternative approaches to the NP-hard BSS problem. Our proposal is a generalization of the

sparse modeling framework, generating more not one single interpretable model with good

prediction accuracy but multiple such models. We hope our introduction to split modeling

will be the central focus for new developments for the analysis of high-dimensional data.

Acknowledgments

Part of this work was conducted while Anthony-Alexander Christidis was a UBC Doctoral

Researcher at KU Leuven’s Department of Mathematics under a Mitacs Globalink Research

Award.

25

Conﬂict of Interests

The authors declare no potential conﬂict of interests.

Data Availability Statement

The R packages stepSplitReg and PSGD created for this article are publicly available on
CRAN together with their respective reference manuals. The data and scripts to replicate
the numerical experiments are available at https://doi.org/10.5281/zenodo.6450556.

Supplementary Material

The supplementary material contains the full results of our simulations and real data ex-

periments.

References

Akaike, H. (1974). A new look at the statistical model identiﬁcation. IEEE transactions

on automatic control 19 (6), 716–723.

Amit, Y. and D. Geman (1997). Shape quantization and recognition with randomized trees.

Neural computation 9 (7), 1545–1588.

Beck, A. and M. Teboulle (2009). A fast iterative shrinkage-thresholding algorithm for

linear inverse problems. SIAM journal on imaging sciences 2 (1), 183–202.

Bendel, R. B. and A. A. Aﬁﬁ (1977). Comparison of stopping rules in forward “stepwise”

regression. Journal of the American Statistical association 72 (357), 46–53.

Bertsimas, D., A. King, and R. Mazumder (2016). Best subset selection via a modern

optimization lens. The annals of statistics 44 (2), 813–852.

Bertsimas, D. and B. Van Parys (2020). Sparse high-dimensional regression: Exact scalable

algorithms and phase transitions. The Annals of Statistics 48 (1), 300–323.

Breiman, L. (1996a). Bagging predictors. Machine learning 24 (2), 123–140.

Breiman, L. (1996b). Stacked regressions. Machine learning 24 (1), 49–64.

Breiman, L. (2001, October). Random forests. Machine Learning 45 (1), 5–32.

26

Brown, G., J. L. Wyatt, and P. Tiˇno (2005). Managing diversity in regression ensembles.

Journal of machine learning research 6 (Sep), 1621–1650.

B¨uhlmann, P. and S. Van De Geer (2011). Statistics for high-dimensional data: methods,

theory and applications. Springer Science & Business Media.

B¨uhlmann, P. and B. Yu (2003). Boosting with the l 2 loss: regression and classiﬁcation.

Journal of the American Statistical Association 98 (462), 324–339.

B¨uhlmann, P., B. Yu, Y. Singer, and L. Wasserman (2006). Sparse boosting. Journal of

Machine Learning Research 7 (6).

Bunea, F., A. B. Tsybakov, and M. H. Wegkamp (2007). Aggregation for gaussian regres-

sion. The Annals of Statistics 35 (4), 1674–1697.

Candes, E. and T. Tao (2007). The dantzig selector: Statistical estimation when p is much

larger than n. The annals of Statistics 35 (6), 2313–2351.

Chandra, R., L. Dagum, D. Kohr, R. Menon, D. Maydan, and J. McDonald (2001). Parallel

programming in OpenMP. Morgan kaufmann.

Chen, S. and D. Donoho (1994). Basis pursuit.

In Proceedings of 1994 28th Asilomar

Conference on Signals, Systems and Computers, Volume 1, pp. 41–44. IEEE.

Chen, T. and C. Guestrin (2016). Xgboost: A scalable tree boosting system. In Proceedings

of the 22nd acm sigkdd international conference on knowledge discovery and data mining,

pp. 785–794.

Chen, T., T. He, M. Benesty, V. Khotilovich, Y. Tang, H. Cho, K. Chen, R. Mitchell,

I. Cano, T. Zhou, M. Li, J. Xie, M. Lin, Y. Geng, and Y. Li (2020). xgboost: Extreme
Gradient Boosting. R package version 1.1.1.1.

Christidis, A., E. Smucler, and R. Zamar (2020). SplitReg: Split Regularized Regression. R

package version 1.0.2.

Christidis, A., S. Van Aelst, and R. Zamar (2022a). PSGD: Projected Subset Gradient

Descent. R package version 1.0.0.

Christidis, A., S. Van Aelst, and R. Zamar (2022b). stepSplitReg: Stepwise Split Regularized

Regression. R package version 1.0.1.

Christidis, A.-A., L. Lakshmanan, E. Smucler, and R. Zamar (2020). Split regularized

regression. Technometrics 62 (3), 330–338.

27

Copas, J. B. (1983). Regression, prediction and shrinkage. Journal of the Royal Statistical

Society: Series B (Methodological) 45 (3), 311–335.

Dietterich, T. G. (2000). An experimental comparison of three methods for constructing en-

sembles of decision trees: Bagging, boosting, and randomization. Machine learning 40 (2),

139–157.

Dorani, F., T. Hu, M. O. Woods, and G. Zhai (2018). Ensemble learning for detecting

gene-gene interactions in colorectal cancer. PeerJ 6, e5854.

Efron, B., T. Hastie, I. Johnstone, and R. Tibshirani (2004). Least angle regression. The

Annals of statistics 32 (2), 407–499.

Fan, J. and J. Lv (2008). Sure independence screening for ultrahigh dimensional feature

space. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 70 (5),

849–911.

Friedman, J., T. Hastie, H. H¨oﬂing, and R. Tibshirani (2007). Pathwise coordinate opti-

mization. The annals of applied statistics 1 (2), 302–332.

Friedman, J. H. (2001, 10). Greedy function approximation: A gradient boosting machine.

Ann. Statist. 29 (5), 1189–1232.

Friedman, J. H., T. Hastie, and R. Tibshirani (2010). Regularization paths for generalized

linear models via coordinate descent. Journal of Statistical Software 33 (1), 1.

Furnival, G. M. and R. W. Wilson (1974). Regressions by leaps and bounds. Technomet-

rics 16, 499–511.

Garside, M. (1965). The best sub-set in multiple regression analysis. Journal of the Royal

Statistical Society: Series C (Applied Statistics) 14 (2-3), 196–200.

Gatu, C. and E. J. Kontoghiorghes (2006). Branch-and-bound algorithms for computing the

best-subset regression models. Journal of Computational and Graphical Statistics 15 (1),

139–156.

Hastie, T. and B. Efron (2013). lars: Least Angle Regression, Lasso and Forward Stagewise.

R package version 1.2.

Hastie, T., R. Tibshirani, and J. Friedman (2009a). Boosting and additive trees. In The

elements of statistical learning, pp. 337–387. Springer.

Hastie, T., R. Tibshirani, and J. Friedman (2009b). Model assessment and selection. In

The elements of statistical learning, pp. 219–259. Springer.

28

Hastie, T., R. Tibshirani, and R. Tibshirani (2020). Best subset, forward stepwise or lasso?

analysis and recommendations based on extensive comparisons. Statistical Science 35 (4),

579–592.

Hastie, T., R. Tibshirani, and M. Wainwright (2019). Statistical learning with sparsity: the

lasso and generalizations. Chapman and Hall/CRC.

Hazimeh, H. and R. Mazumder (2020). Fast best subset selection: Coordinate descent and

local combinatorial optimization algorithms. Operations Research 68 (5), 1517–1537.

Hazimeh, H., R. Mazumder, and T. Nonet (2021). L0Learn: Fast Algorithms for Best

Subset Selection. R package version 2.0.3.

Ho, T. K. (1998). The random subspace method for constructing decision forests. IEEE

transactions on pattern analysis and machine intelligence 20 (8), 832–844.

Hurvich, C. M. and C. Tsai (1990). The impact of model selection on inference in linear

regression. The American Statistician 44 (3), 214–217.

Jennrich, R. and P. Sampson (1968). Application of stepwise regression to non-linear

estimation. Technometrics 10 (1), 63–72.

Kenney, A., F. Chiaromonte, and G. Felici (2021). Mip-boost: Eﬃcient and eﬀective l 0

feature selection for linear regression. Journal of Computational and Graphical Statis-

tics 30 (3), 566–577.

Kim, Y. and S. Y. Sohn (2012). Stock fraud detection using peer group analysis. Expert

Systems with Applications 39 (10), 8986–8992.

Krajewski, J., A. Batliner, and S. Kessel (2010). Comparing multiple classiﬁers for speech-

based detection of self-conﬁdence-a pilot study. In 2010 20th International Conference

on Pattern Recognition, pp. 3716–3719. IEEE.

Li, X., T. Zhao, L. Wang, X. Yuan, and H. Liu (2020). ﬂare: Family of Lasso Regression.

R package version 1.7.0.

Liaw, A. and M. Wiener (2002). Classiﬁcation and regression by randomforest. R

News 2 (3), 18–22.

Louzada, F. and A. Ara (2012). Bagging k-dependence probabilistic networks: An alter-

native powerful fraud detection tool. Expert Systems with Applications 39 (14), 11583–

11592.

Mallows, C. L. (1973). Some comments on cp. Technometrics 15 (4), 661–675.

29

McCullagh, P. and J. A. Nelder (1989). Monographs on statistics and applied probability.

Generalized linear models 37.

Mountain, D. and C. Hsiao (1989). A combined structural and ﬂexible functional ap-

proach for modeling energy substitution. Journal of the American Statistical Associa-

tion 84 (405), 76–87.

Murdoch, W. J., C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu (2019). Deﬁnitions,

methods, and applications in interpretable machine learning. Proceedings of the National

Academy of Sciences 116 (44), 22071–22080.

Pope, P. and J. Webster (1972). The use of an f-statistic in stepwise regression procedures.

Technometrics 14 (2), 327–340.

R Core Team (2022). R: A Language and Environment for Statistical Computing. Vienna,

Austria: R Foundation for Statistical Computing.

Rencher, A. C. and F. C. Pun (1980). Inﬂation of r2 in best subset regression. Technomet-

rics 22 (1), 49–53.

Rieger, S. A., R. Muraleedharan, and R. P. Ramachandran (2014). Speech based emotion

recognition using spectral feature extraction and an ensemble of knn classiﬁers. In The 9th

International Symposium on Chinese Spoken Language Processing, pp. 589–593. IEEE.

Rodriguez-Galiano, V. F., B. Ghimire, J. Rogan, M. Chica-Olmo, and J. P. Rigol-Sanchez

(2012). An assessment of the eﬀectiveness of a random forest classiﬁer for land-cover

classiﬁcation. ISPRS journal of photogrammetry and remote sensing 67, 93–104.

Roecker, E. B. (1991). Prediction error and its estimation for subset-selected models.

Technometrics 33 (4), 459–468.

Rudin, C. (2019). Stop explaining black box machine learning models for high stakes

decisions and use interpretable models instead. Nature Machine Intelligence 1 (5), 206–

215.

Rudin, C., C. Chen, Z. Chen, H. Huang, L. Semenova, and C. Zhong (2022). Interpretable

machine learning: Fundamental principles and 10 grand challenges. Statistics Surveys 16,

1–85.

Schapire, R. E. and Y. Freund (2012). Boosting: Foundations and Algorithms. The MIT

Press.

30

Scheetz, T. E., K.-Y. A. Kim, R. E. Swiderski, A. R. Philp, T. A. Braun, K. L. Knudtson,

A. M. Dorrance, G. F. DiBona, J. Huang, T. L. Casavant, et al. (2006). Regulation of

gene expression in the mammalian eye and its relevance to eye disease. Proceedings of

the National Academy of Sciences 103 (39), 14429–14434.

Schwarz, G. (1978). Estimating the dimension of a model. The annals of statistics, 461–464.

Shen, X., W. Pan, Y. Zhu, and H. Zhou (2013). On constrained and regularized high-

dimensional regression. Annals of the Institute of Statistical Mathematics 65 (5), 807–

832.

Song, L. and P. Langfelder (2013). randomGLM: Random General Linear Model Prediction.

R package version 1.02-1.

Song, L., P. Langfelder, and S. Horvath (2013). Random generalized linear model: a highly

accurate and interpretable ensemble predictor. BMC bioinformatics 14 (1), 5.

Takano, Y. and R. Miyashiro (2020). Best subset selection via cross-validation criterion.

Top 28 (2), 475–488.

Thompson, R. (2022). Robust subset selection. Computational Statistics & Data Analysis,

107415.

Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society: Series B (Methodological) 58 (1), 267–288.

Tibshirani, R., M. Saunders, S. Rosset, J. Zhu, and K. Knight (2005). Sparsity and smooth-

ness via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical

Methodology) 67 (1), 91–108.

Ueda, N. and R. Nakano (1996). Generalization error of ensemble estimators. In Proceedings

of International Conference on Neural Networks (ICNN’96), Volume 1, pp. 90–95. IEEE.

Van De Geer, S. A. and P. B¨uhlmann (2009). On the conditions used to prove oracle results

for the lasso. Electronic Journal of Statistics 3, 1360–1392.

Welch, W. J. (1982). Algorithmic complexity: three np-hard problems in computational

statistics. Journal of Statistical Computation and Simulation 15 (1), 17–25.

Wilkinson, L. and G. E. Dallal (1981). Tests of signiﬁcance in forward selection regression

with an f-to-enter stopping rule. Technometrics 23 (4), 377–380.

31

Yu, B., W. Qiu, C. Chen, A. Ma, J. Jiang, H. Zhou, and Q. Ma (2020). Submito-xgboost:

predicting protein submitochondrial localization by fusing multiple feature information

and extreme gradient boosting. Bioinformatics 36 (4), 1074–1081.

Yu, Z. and C. Zhang (2015). Image based static facial expression recognition with multiple

deep network learning. In Proceedings of the 2015 ACM on international conference on

multimodal interaction, pp. 435–442.

Yuan, M. and Y. Lin (2006). Model selection and estimation in regression with grouped vari-

ables. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68 (1),

49–67.

Zahoor, J. and K. Zafar (2020). Classiﬁcation of microarray gene expression data using an

inﬁltration tactics optimization (ito) algorithm. Genes 11 (7).

Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.

The Annals of statistics 38 (2), 894–942.

Zhang, Y., M. J. Wainwright, and M. I. Jordan (2014). Lower bounds on the performance

of polynomial-time algorithms for sparse linear regression. In Conference on Learning

Theory, pp. 921–948. PMLR.

Zhao, P. and B. Yu (2006). On model selection consistency of lasso. The Journal of Machine

Learning Research 7, 2541–2563.

Zou, H. and T. Hastie (2005). Regularization and variable selection via the elastic net.

Journal of the royal statistical society: series B (statistical methodology) 67 (2), 301–320.

32

