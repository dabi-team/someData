CommitBART: A Large Pre-trained Model for GitHub Commits

Shangqing Liu*, Yanzhou Li*, Yang Liu

Nanyang Technological University
{shangqin001,yannzhou001}@e.ntu.edu.sg, yangliu@ntu.edu.sg

2
2
0
2

g
u
A
7
1

]
E
S
.
s
c
[

1
v
0
0
1
8
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

GitHub commits, which record the code changes with nat-
ural language messages for description, play a critical role
for software developers to comprehend the software evolu-
tion. To promote the development of the open-source soft-
ware community, we collect a commit benchmark including
over 7.99 million commits across 7 programming languages.
Based on this benchmark, we present CommitBART, a large
pre-trained encoder-decoder Transformer model for GitHub
commits. The model is pre-trained by three categories (i.e.,
denoising objectives, cross-modal generation and contrastive
learning) for six pre-training tasks to learn commit frag-
ment representations. Furthermore, we unify a “commit in-
telligence” framework with one understanding task and three
generation tasks for commits. The comprehensive experi-
ments on these tasks demonstrate that CommitBART signiﬁ-
cantly outperforms previous pre-trained works for code. Fur-
ther analysis also reveals each pre-training task enhances the
model performance. We encourage the follow-up researchers
to contribute more commit-related downstream tasks to our
framework in the future.

Introduction
Large pre-trained models such as BERT (Devlin et al. 2019),
GPT (Radford et al. 2019) and T5 (Raffel et al. 2020) have
signiﬁcantly improved state-of-the-art across a variety of
natural language processing (NLP) tasks. These models are
pre-trained on a large-scale of unlabeled data with self-
supervised objectives to learn contextual representations and
then ﬁne-tuned to multiple downstream tasks. Inspired by
the great success of these models in NLP, recently a num-
ber of pre-trained models (Feng et al. 2020; Ahmad et al.
2021; Wang et al. 2021c) for programming languages (PL)
have emerged in software engineering to advance the de-
velopment of code intelligence. However, these works in
the program scenario aim at learning the general program
representation for a single function. The booming develop-
ment of the open-source software industry has led to an un-
precedented amount of projects hosted on Github1, which
produces an extremely massive amount of commits. Github

*These authors contributed equally.

Copyright © 2023, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
1https://github.blog/2018-11-08-100m-repos

commits, which record the changed code (i.e., code changes)
with the commit messages in natural language to describe
the changed code, play a critical role for software develop-
ers to comprehend the evolution of software features at dif-
ferent stages of software development. To promote the de-
velopment of the open-source software community, a spe-
cialized pre-trained model for “commit intelligence” is vital
and meaningful for software developers to understand and
accelerate software development.

The previous work CommitBERT (Jung 2021) proposed
to directly employ CodeBERT (Feng et al. 2020) to ﬁne-tune
a model for commit message generation (Jiang, Armaly, and
McMillan 2017; Liu et al. 2020b) (a task to automatically
generate the natural language description given the input of
the code changes) on the collected commit dataset (345K in
total). However, CommitBERT did not pre-train a commit-
speciﬁc pre-trained model on the commit data, which limits
its performance on commit-related downstream tasks. Fur-
thermore, the amount of the released data is far from satis-
factory to the requirements for pre-training. In this work, we
present CommitBART, a pre-trained encoder-decoder model
based on BART (Lewis et al. 2019) architecture for commits
to support both commit-related understanding and genera-
tion tasks. Compared with current pre-trained models for
code, we are the ﬁrst to provide a large pre-trained model
for commits and unify a “commit intelligence” framework
to support multiple commit-related tasks.

Due to the lack of a large-scale commit benchmark, we
collect a commit benchmark across 7 programming lan-
guages (i.e., C, CSharp, Java, JavaScript, PHP, Python and
Typescript). Speciﬁcally, we extract commits from the top-
500 projects based on the star ranking for each programming
language to construct this benchmark. We obtain over 7.99
million commit data in total and make it public to beneﬁt
the follow-up researchers. We pre-train CommitBART us-
ing six pre-training tasks that can be divided into three cat-
egories. Speciﬁcally, they can be classiﬁed into denoising
objectives (text inﬁlling and graph-guided token masking),
cross-modal generation (PL2NL generation and PLNL2PL
generation) and contrastive learning (NLPL alignment and
SimCSE (Gao, Yao, and Chen 2021)). Text inﬁlling is one
of objectives that BART (Lewis et al. 2019) used for pre-
training and we further introduce the graph-guided token
masking to encourage the model predict these tokens that

 
 
 
 
 
 
both appear in commit message and the changed code to
enhance the semantic mapping between them. Furthermore,
to improve the capacity of the decoder on generating high-
quality NL texts or code snippets, we introduce two cross-
modal generation tasks: PL2NL generation task, which takes
the changed code as the input to generate its commit mes-
sage, and PLNL2PL generation task, which takes the pre-
vious code as well as the commit message as the input to
generate the updated code snippet. Inspired by the recent ad-
vanced works on contrastive learning for code learning (Jain
et al. 2020; Wang et al. 2021b; Guo et al. 2022), we also
introduce two objectives based on contrastive learning. The
ﬁrst NLPL alignment task encourages the embedding of the
changed code produced by the encoder to be closer with its
corresponding message embedding and the second SimCSE
takes the same input sequence to the encoder twice with dif-
ferent dropout mask to aggregate semantic equivalent inputs.
We evaluate CommitBART with two widely concerned
tasks for commits: security patch identiﬁcation (understand-
ing task) and commit message generation (generation task).
Meanwhile, we also propose two new commit-related gener-
ation tasks (i.e., positive code statements generation and up-
dated code snippet generation). The extensive experiments
illustrate that CommitBART achieves state-of-the-art perfor-
mance on these tasks. Further analysis also reveals that each
pre-training task enhances CommitBART to obtain better
performance. The main contributions of our paper are sum-
marized as follows:

• We collect a large-scale commit benchmark (over 7.99
million commits) across 7 programming languages and
make it public for the follow-up researchers.

• We are the ﬁrst to present a large pre-trained model
namely CommitBART for GitHub commits and it is pre-
trained by three categories for six pre-training tasks.

• We unify a “commit intelligence” framework with one un-
derstanding task and three generation tasks on commits.
The extensive experimental results on these tasks have
demonstrated that CommitBART achieves state-of-the-
art performance against the baselines. We encourage the
follow-up researchers to contribute more commit-related
tasks to this framework2.

Background

GitHub Commits
A commit usually consists of the changed code with its com-
mit message to describe the purpose of the current changed
code in natural language. We present a commit to illustrate
each component in Figure 1. The upper rectangle contains
the content of the commit message, which summarizes this
commit in natural language (e.g., “Bugﬁx: Pass threshold to
binarizer”). The lower rectangle is the changed code namely
one chunk, which contains the ﬁle path (i.e., “tpot/tpot.py”)
and the modiﬁed code from line 1025 to line 1031. The
ﬁrst line in this chunk starting with “@@” consists of the

2All the code and data are available at https://anonymous.

4open.science/r/CommitBart-443E

Figure 1: A commit from GitHub with its commit id dbec56.

start line number (i.e., 1025) in the ﬁle, the total line state-
ments (i.e., 7) and the function name (i.e., “ binarizer”). The
changed code in this chunk is marked with “+” in the up-
dated version of the code with its previous version marked
with “-”. The remaining content is the context around the
changed code to reveal its contextual information. Hence,
in summary, for this commit in Figure 1, we can obtain
that there is one line of code at line 1028 in the ﬁle of
“tpot.py” changing the statement from “...copy=False)” to
“...copy=False, threshold=threshold)”. The other statements
(i.e. from line 1025 to 1027 and from line 1029 to 1031)
are the context of the statement at line 1028. Note that we
just utilize a simple commit, which only has one chunk with
single-line statement modiﬁcation as the example for better
illustration. In some cases, a commit may only have the up-
dated statements or the deleted statements and they may not
appear in pairs. Furthermore, a commit can change multi-
ple places of code in a chunk or even change multiple ﬁles.
Since commits play a critical role in the software develop-
ment cycle, many researchers conduct broad research such
as commit message generation (Jiang, Armaly, and McMil-
lan 2017; Liu et al. 2020b; Jung 2021), security patch iden-
tiﬁcation (Zhou et al. 2021b,a; Wu et al. 2022). Compared
with these existing works, which design a specialized neural
model for commit-related tasks, we are the ﬁrst to propose a
pre-trained model based on our large-scale benchmark and
further unify these tasks with one model.

Pre-trained Models in Code
Pre-trained models for code have attracted widespread atten-
tion due to their superior performance against the supervised
techniques. For example, Svyatkovskiy et al. (Svyatkovskiy
et al. 2020) proposed GPT-C, which utilizes the Transformer
decoder (Vaswani et al. 2017) in a left-to-right manner for
code completion. CodeBERT (Feng et al. 2020) and Cu-
BERT (Kanade et al. 2020) pre-trained a bidirectional Trans-
former encoder on source code for some code-related under-
standing tasks. To further improve the model learning capac-
ity, PLBART (Ahmad et al. 2021) and CodeT5 (Wang et al.
2021c) are proposed to pre-train a Transformer encoder-
decoder model based on BART and T5 architecture to sup-
port both the understanding and generation tasks. In addi-
tion, Guo et al. (Guo et al. 2022) proposed UniXcoder, a uni-
ﬁed cross-modal pre-trained model based on a multi-layer
Transformer for “code intelligence”. Speciﬁcally, it utilized
mask attention matrices with preﬁx adapters for code un-

10251025return input_df.copy()1026102610271027#The binarizermust be fit on only the training data1028-binarizer= Binarizer(copy=False)1028+binarizer= Binarizer(copy=False, threshold=threshold)10291029binarizer.fit(training_features.values.astype(np.float64))10301030binarized_features= binarizer.transform(input_df.drop([‘class’, ‘group’, ‘guess‘], axis=1).values.astype(np.float64))10311031Table 1: The statistics of our collected commit benchmark.

Benchmark
Pre-train
Fine-tune
Total

C
1,917,109
71,924
1,989,033

CSharp
660,587
61,902
722,489

Java
935,151
81,126
1,016,277

JavaScript
986,669
87,064
1,073,733

PHP
1,148,074
99,230
1,247,304

Python Typescript
762,760
67,762
830,522

1,029,676
89,502
111,9178

Total
7,440,026
558,510
7,998,536

derstanding and generation. Apart from the aforementioned
works, there are also some pre-trained works for code (Guo
et al. 2020; Jiang et al. 2021; Lu et al. 2021; Wang et al.
2021b; Buratti et al. 2020; Liu et al. 2020a). Compared with
these pre-trained models for code, in this paper, we propose
a large pre-trained model based on BART for GitHub com-
mits and further unify a “commit intelligence” framework
with different commit-related tasks.

A Large-scale Benchmark for Commits
Currently, the existing benchmark from CommitBERT (Jung
2021) only contains 345K commits and its amount is far
from the requirements for pre-training. To address this lim-
itation, we collect a large-scale benchmark for commits in-
stead. Speciﬁcally, we collect commits from the open-source
projects on GitHub across 7 programming languages (i.e., C,
CSharp, Java, JavaScript, PHP, Python, Typescript). To en-
sure the quality of the collected commits, we only keep the
project whose description is in English and further select the
top-500 projects based on their star ranking from January
2010 to December 2021 for each programming language
through GitHub API 3. Given a cloned project, we utilize
the open-source tool GitPython 4 to obtain the raw commits.
We ﬁlter out the commits where the commit messages are
non-English and the token length larger than 2000 in a com-
mit to reduce the sequence length. We further fetch the ﬁrst
sentence in the commit message followed by the existing
works (Jiang, Armaly, and McMillan 2017; Liu et al. 2020b;
Jung 2021) for efﬁcient learning. To alleviate the learning
process of the model overﬁt to the duplicated samples (Al-
lamanis 2019), we conduct a strict de-duplication process
to remove the same samples. Finally, we obtain over 7.99
million commits over 7 programming languages. For these
7.99 million commits, we extract partial data (558K in total)
to construct a dataset for some ﬁne-tuning commit-related
tasks and keep the remaining data (7.4 million in total) for
pre-training. The detailed statistics of our collected bench-
mark are presented in Table 1.

CommitBART

Model Architecture
CommitBART follows the architecture of BART (Lewis
et al. 2019) and we adopt the parameters of PLBART (Ah-
mad et al. 2021) to accelerate the training process. Since a
commit usually consists of multiple different components
such as the commit message, the updated code statements
marked with “+”, the deleted code statements marked with
“-” and their context statements. To distinguish each com-
ponent, we introduce some additional segment identiﬁers.

3https://docs.github.com/en/rest
4https://gitpython.readthedocs.io

Speciﬁcally, we deﬁne the identiﬁer “[MSG]” at the start of
a commit message M = {m0, m1, ..., mi} and “[FILE]” for
ﬁle path F = {f0, f1, ..., fj} where i and j denote the num-
ber of M and F word tokens respectively. Similarly, we also
deﬁne the identiﬁer “[CODE]” at the start of the code to dis-
tinguish from the message and ﬁle path. In addition, to dis-
tinguish the deleted code statements and the updated state-
ments, we further add the identiﬁers (“[NEG]”/“[END]”)
and (“[POS]”/“[END]”) between the start and end of these
deleted/updated statements for distinction. Hence, generally,
the code in the commit can be represented as follows:

C = {c0...[NEG]ni

0...[END][POS]pj
0 ...[END][POS]pj

(cid:48)

(cid:48)

0...[END]ck...

0 ...[END]...cl}

[NEG]ni

(cid:48)

(cid:48)

and pj/pj

where c denotes the context statement and c0 is the ﬁrst to-
ken in c. Furthermore, ni/ni
are the deleted and
updated code statements respectively. We utilize the segment
embedding to embed these segments for ensuring the model
understand and incorporate information well. Hence, the in-
put embedding representation for the model is constructed
by summing the corresponding token, segment and posi-
tional embeddings.

Pre-training Tasks

We introduce six pre-training tasks that can be divided
into three categories for pre-training. We use the example
from Figure 1 for illustration. Speciﬁcally, the denosing and
cross-modal generation tasks are presented in Figure 2, the
contrastive pre-training tasks are presented in Figure 3.

Denoising Objectives Denoising pre-training, which aims
at generating the original sequence given the noisy in-
put,
is an effective technique for pre-training encoder-
decoder models (Lewis et al. 2019; Raffel et al. 2020).
We also use it to pre-train CommitBART. Speciﬁcally, we
apply a noisy function N to the input sequence X =
{[CLS][MSG]M [FILE]F [CODE]C} to get the noisy input
deﬁned as N (X), where the noisy function is to corrupt the
input sequence by some strategies such as token masking,
tokens inﬁlling. The learning process is to ask the model to
recover the original sequence from the noisy input and the
loss function can be calculated as follows:

LDenoise(θ) =

|X|
(cid:88)

t=1

−logPθ(Xt|X<t, N (X))

(1)

where P is a generator that generates the t-th token given the
noisy input N (X), the original sequence before t (i.e., X<t)
and the model parameters θ. We adopted two strategies to
corrupt the semantics of the input sequence (i.e., X).

Figure 2: Denoising and cross-modal generation tasks for CommitBART.

LGen(θ) =

|Y |
(cid:88)

t=1

−logPθ(Yt|Y<t, XGen)

(2)

where XGen is the input sequence and Y is the target se-
quence for generation.

Figure 3: Contrastive pre-training tasks for CommitBART.

Text Inﬁlling. It randomly masks spans with a single
masked token “[MASK]” and then we ask the model to re-
cover the original sequence. Speciﬁcally, we set the corrup-
tion rate as 15% in the input sequence and ensure the average
span length to 3, followed by a Poisson distribution (Lewis
et al. 2019). A simple example is illustrated in Figure 2 (a).
Graph-guided Token Masking (GTM). Inspired by Ya-
sunaga et al. (Yasunaga and Liang 2020), we introduce a
graph-guided token masking task to enhance the relations
between different components in a commit. A simple exam-
ple is shown in Figure 2 (b). Speciﬁcally, by our preliminary
observation, we ﬁnd that there are some critical tokens (e.g.,
variable names, keywords, ﬁle names) that may be shared
in commit message M and changed code C. Hence, to en-
hance the semantic mapping, we construct a commit-graph,
which links shared tokens between the commit message M ,
ﬁle path F and changed code C. Then we randomly mask
half of the nodes in the graph and require the model to pre-
dict these masked nodes by learning the relations from con-
nected neighbors. Through this task, CommitBART learns to
predict the critical tokens in the commit message from the
connected nodes in the changed code or vice versa, which
further improves the model learning capacity and alleviates
the semantic gap between natural language and code.

Cross-Modal Generation We design two bidirectional
conversion generation tasks for the message and code to im-
prove model in generating a coherent commit message or
correct changed code. We formulate both cross-modal gen-
eration tasks as follows:

the

task takes

PL2NL Generation. As

shown in Figure 2 (c),
(i.e. XGen =
this
{[CLS][FILE]F [CODE]C}) as input and requires model to
generate its corresponding commit message Y = M to en-
sure model produce high-quality natural language texts for
downstream natural language generation tasks.

changed code

PLNL2PL Generation. This task aims to generate
the updated code snippet based on its commit message
and the previous code snippet. Speciﬁcally, the previous
code snippet before the modiﬁcation can be expressed
as C − = {c0...[NEG]ni
0 ...[END]...cl}. Hence, the in-
put sequence for this task can be expressed as XGen =
{[CLS][MSG]M [FILE]F [CODE]C −}. The output Y =
{c0...[POS]pj
0 ...[END]...cl}, which is the updated code
snippet for the model to generate. We incorporate this task
to improve the model in generating better code snippets.

(cid:48)

(cid:48)

Contrastive Learning Contrastive learning is an effective
pre-training task which is used in some pre-trained code
models (Jain et al. 2020; Wang et al. 2021b; Guo et al. 2022).
Generally, it aggregates the similar sequence representation
(i.e., (cid:101)h+
i ) while pushes away dissimilar representations after
encoding the input to an encoder. The loss function is:

LContra(θ) =

b−1
(cid:88)

i=0

−log

i )/τ

ecos((cid:101)hi,(cid:101)h+
j=0 ecos((cid:101)hi,(cid:101)hj )/τ

(cid:80)b−1

(3)

where b is batch size, τ
is a temperature hyper-
parameter (Wu et al. 2018) and cos(∗) is cosine similarity
between two vector representations.

NLPL Alignment. As shown in Figure 3 (a), NLPL
Alignment task takes the vector representations of commit
message M with its code C embedded by CommitBART
encoder as a pair of similar sample ((cid:101)hi, (cid:101)h+
i ) while takes the
other representations in the batch as dissimilar samples to

Text InﬁllingMSG: Bugﬁx: [mask] threshold to [mask]FILE: tpot.pyCode Changes:              ...-            binarizer = ([mask](copy=False)+           [mask] = Binarizer(copy=[mask], threshold=[mask])             ...MSG: Bugﬁx: Pass threshold to binarizerFILE: tpot.pyCode Changes:              ...-            binarizer = Binarizer(copy=False)+           binarizer = Binarizer(copy=False, threshold=threshold)             ...(a) Text Inﬁlling(b) Graph-guided Token MaskingMSG: Bugﬁx: Pass threshold to binarizerFILE: tpot.pyCode Changes:              ...-            binarizer = Binarizer(copy=False)+           binarizer = Binarizer(copy=False, threshold=threshold)             ...Graph-guided Token MaskingMSG: Bugﬁx: Pass threshold to [mask]FILE: tpot.pyCode Changes:              ...-            binarizer = Binarizer(copy=False)+           binarizer = Binarizer(copy=False,  [mask]=[mask])             ...PLNL2PL GenerationPL2NL GenerationFILE: tpot.pyMSG: Bugfix: Pass threshold to binarizerFILE: tpot.pyPrior Code:             ...            # The binarizer must be fit on only the training data-            binarizer = Binarizer(copy=False)             ...Updated Code:              ...             # The binarizer must be fit on only the training data+           binarizer = Binarizer(copy=False, threshold=threshold)             ...Code Changes:              ...-            binarizer = Binarizer(copy=False)+           binarizer = Binarizer(copy=False, threshold=threshold)             ...MSG: Bugfix: Pass threshold to binarizer(c) PL2NL Generation(d) PLNL2PL GenerationMean PoolingMean PoolingMean Pooling+Code Changes:              ...-            binarizer = Binarizer(copy=False)+           binarizer = Binarizer(copy=False, threshold=threshold)             ...MSG: Bugfix: Pass threshold to binarizerDifferent hidden dropoutmask in two forward passes(b) SimCSE(a) NLPL Alignment+  COMMITBART-ENCODER COMMITBART-ENCODERMSG: Bugﬁx: Pass threshold to binarizerFILE: tpot.pyCode Changes:              ...-            binarizer = Binarizer(copy=False)+           binarizer = Binarizer(copy=False, threshold=threshold)             ...COMMITBART-ENCODERfurther enhance the model capacity of aligning the paired
commit message and code.

Table 2: The results of security patch identiﬁcation, where *
marks the values from Wu et al. (Wu et al. 2022).

al.

et
same

follow Gao
the
to put

SimCSE. We
(Gao, Yao,
input X =
and Chen 2021)
{[CLS][MSG]M [FILE]F [CODE]C}
different
dropout mask to CommitBART encoder to get vector rep-
resentations as the similar samples ((cid:101)hi, (cid:101)h+
i ) and use other
representations in the same batch as dissimilar samples for
pre-training. An example is shown in Figure 3 (b).

with

Fine-tuning

We transfer CommitBART to commit-related tasks at ﬁne-
tuning phase. Generally, the ﬁne-tuning tasks can be cate-
gorized into two classes: understanding tasks and genera-
tion tasks. For understanding tasks, we directly encode the
source sequence to the encoder and ask the decoder to gen-
erate its predicted label. For generation tasks, CommitBART
can be naturally adapted with its encoder-decoder frame-
work to different commit-related generation tasks.

Experimental Setup
In this section, we ﬁrst introduce the evaluation tasks and
then introduce the compared baselines. The details of pre-
training and ﬁne-tuning settings can be found in Appendix.

Evaluation Tasks

We select one understanding task (i.e., security patch iden-
tiﬁcation) and three generation tasks (i.e., commit message
generation, positive code statements generation, and updated
code snippet generation) as ﬁne-tuning tasks.
Security Patch Identiﬁcation. The task aims to identify
whether a commit ﬁxing a software vulnerability or not and
this task has been extensively researched with some neu-
ral models (e.g., SPI (Zhou et al. 2021b), E-SPI (Wu et al.
2022), PatchRNN (Wang et al. 2021a)). We use the same
dataset provided by E-SPI (Wu et al. 2022) with the same
train-validation-test split for evaluation.
Commit Message Generation. The task targets generating
a commit message to summarize the changed code in natu-
ral language. Although there are some open-source datasets
such as ATOM (Liu et al. 2020b), Jung et al. (Jung 2021),
we also construct a ﬁne-tuning dataset from our benchmark
(See Table 1) across 7 languages with the train-validation-
test split of the ratio of 75%:10%:15% for evaluation.
Positive Code Statements Generation. We propose this
new commit-related task, which targets generating positive
statements marked by “+”, that takes the commit message,
ﬁle path with the code snippet before modiﬁcation as input.
We only consider the commits that have consecutive state-
ment modiﬁcation for evaluation. Speciﬁcally, if a commit in
our constructed ﬁne-tuning dataset from Table 1 has multiple
non-consecutive modiﬁcation (a simple example is shown
in Appendix Figure 8a, which removes two non-consecutive
lines of statements), we remove these samples. This task is
analogous to the task of bug ﬁxing to generate repaired state-
ments to ﬁx buggy code (Yasunaga and Liang 2020) in the
program repair scenario.

Model
E-SPI
CodeBERT
PLBART
CodeT5-base
UniXcoder
Incr-CodeBERT
Incr-PLBART
CommitBART
-w/o SEG
-w/o GTM
-w/o PL2NL
-w/o PLNL2PL
-w/o NLPL Align
-w/o SimCSE

Acc
86.81*
88.72
94.63
94.47
95.06
91.81
95.03
95.09
94.81
94.66
93.84
94.50
94.72
94.34

Pre
84.49*
88.03
94.55
93.96
94.59
91.36
95.01
94.39
94.88
94.08
93.52
94.11
94.14
93.13

Rec
91.52*
90.85
95.22
95.58
96.06
92.97
95.52
96.36
95.22
95.82
94.81
95.46
95.88
96.30

F1
87.86*
89.42
94.88
94.76
95.32
92.15
95.27
95.36
95.05
94.94
94.16
94.78
95.00
94.69

Updated Code Snippet Generation. We further propose a
more challenging task that requires the model to generate
the completed updated code snippet. Compared with posi-
tive code statements generation, this task needs to locate the
position of the updated statements ﬁrst and then generate the
completed code snippet. In this task, we also include these
commits that have multiple inconsecutive modiﬁcation (see
an example in Appendix Figure 8a) for experiments. This
task is valuable for software developers to generate the code
that meets the requirement based on the previous code and
natural language description. We use our ﬁne-tuning dataset
in Table 1 for evaluation.

The statistics of the used datasets for these ﬁne-tuning

tasks are presented in Appendix.

Baselines

For the understanding task of security patch identiﬁcation,
since we use the dataset from E-SPI (Wu et al. 2022) with the
same train-validation-test split, we directly report the best
results from their paper as one of our baselines. For gen-
eration tasks, we add one supervised baseline Transformer
model (Vaswani et al. 2017), which is trained on ﬁne-tuning
commit dataset without pre-training to verify the effective-
ness of pre-training techniques. Furthermore, we compare
CommitBART with four state-of-the-art pre-trained models
for code (i.e., CodeBERT (Feng et al. 2020), PLBART (Ah-
mad et al. 2021), CodeT5-base (Wang et al. 2021c) and
UniXcoder (Guo et al. 2022)), which are trained on large
corpus of code data to validate the effectiveness of utilizing
commit data to pre-train a model for commit-related tasks.
We directly employ these released pre-trained models with
default conﬁguration for comparison. In addition, to conﬁrm
the effectiveness of our pre-training tasks, we further add
two baselines, Incr-CodeBERT and Incr-PLBART, which
are incrementally trained on CodeBERT and PLBART on
our collected commit benchmark using masked language
modeling (MLM) and text inﬁlling respectively.

Table 3: Smoothed BLEU-4 scores on the commit message generation task. The “Overall” column presents the average score
over seven programming languages.

Model
Transformer
CodeBERT
PLBART
CodeT5-base
UniXcoder
Incr-CodeBERT
Incr-PLBART
CommitBART
-w/o SEG
-w/o GTM
-w/o PL2NL
-w/o PLNL2PL
-w/o NLPL Align
-w/o SimCSE

C
1.70
8.75
11.23
13.74
13.02
10.96
11.98
15.99
15.65
15.85
14.54
15.86
15.94
15.95

CSharp
5.24
12.39
15.61
18.82
18.40
13.65
16.38
21.26
21.12
20.46
18.66
21.51
20.62
20.67

Java
10.75
15.14
15.85
20.05
20.22
16.20
16.55
22.00
22.25
22.91
20.33
22.91
22.53
22.54

JavaScript
9.88
14.45
15.96
19.63
20.70
15.70
17.33
26.96
26.10
26.31
22.89
26.97
26.63
26.85

PHP
2.01
8.80
11.02
12.01
12.20
10.34
11.24
13.93
13.38
13.12
12.21
13.29
12.87
13.62

Python Typescript Overall
3.70
20.55
21.60
23.49
23.63
21.45
21.95
24.56
23.17
23.04
22.08
22.20
22.98
23.06

4.95
13.32
15.17
18.04
17.97
14.70
15.79
20.60
20.11
20.05
18.39
20.31
20.10
20.26

1.34
13.19
14.89
18.55
17.63
14.62
15.10
19.50
19.08
18.65
18.00
19.40
19.15
19.11

Table 4: BLEU-4 scores and exact match (EM) accuracies for positive code statements generation task.

Model

Transformer
CodeBERT
PLBART
CodeT5-base
UniXcoder
Incr-CodeBERT
Incr-PLBART
CommitBART
-w/o SEG
-w/o GTM
-w/o PL2NL
-w/o PLNL2PL
-w/o NLPL Align
-w/o SimCSE

C
EM BLEU-4
0.02
6.25
7.92
13.41
14.60
8.77
9.93
16.64
15.50
15.78
15.36
14.22
15.30
16.07

6.10
39.94
47.56
50.94
53.07
41.71
48.12
55.69
55.13
53.83
55.21
53.93
54.99
53.44

CSharp
EM BLEU-4
0.00
20.24
19.36
25.50
26.26
22.31
21.46
28.26
27.38
26.72
26.66
24.86
27.24
28.04

8.32
47.08
50.75
52.98
56.20
48.69
51.21
58.04
56.81
56.39
56.94
57.14
56.44
57.57

Java
EM BLEU-4
0.00
30.33
31.07
38.80
35.80
34.77
33.29
41.02
40.60
40.14
39.90
36.12
40.18
40.18

13.26
48.04
52.19
56.14
57.11
50.32
52.80
61.05
59.99
60.18
59.71
59.28
59.99
59.69

JavaScript
EM BLEU-4
0.00
18.54
18.86
23.71
23.48
20.15
21.42
34.12
31.79
31.15
27.19
24.15
30.35
32.52

9.59
50.85
51.39
59.10
60.62
50.97
54.44
60.50
62.25
60.97
58.28
60.64
59.98
61.67

PHP
EM BLEU-4
0.00
6.63
8.23
10.73
13.07
8.95
9.79
15.21
13.79
14.51
13.40
12.83
14.08
14.86

5.14
48.57
50.81
49.07
53.93
48.97
51.00
56.80
55.35
55.16
57.37
56.01
56.93
56.23

Python
EM BLEU-4
0.00
15.86
16.26
20.61
22.71
18.22
18.41
25.61
24.02
24.46
23.60
23.26
24.17
24.43

6.95
44.74
48.83
51.02
52.51
45.77
48.73
56.17
56.10
55.88
54.69
54.49
56.20
55.79

TypeScript
EM BLEU-4
0.02
38.79
37.40
43.84
42.86
42.36
38.88
47.16
46.63
46.80
44.21
43.74
45.24
46.38

15.37
53.30
53.73
60.51
62.07
54.72
56.38
61.97
62.28
60.37
61.67
61.84
62.00
59.86

Overall
EM BLEU-4
0.01
19.52
19.87
25.23
25.54
22.22
21.88
29.72
28.53
28.51
27.19
25.60
28.08
28.93

9.25
47.50
50.75
54.25
56.50
48.74
51.81
58.60
58.27
57.54
57.70
57.62
58.08
57.75

Table 5: BLEU-4 scores and exact match (EM) accuracies for updated code snippet generation task.

Model

Transformer
CodeBERT
PLBART
CodeT5-base
UniXcoder
Incr-CodeBERT
Incr-PLBART
CommitBART
-w/o SEG
-w/o GTM
-w/o PL2NL
-w/o PLNL2PL
-w/o NLPL Align
-w/o SimCSE

C
EM BLEU-4
0.00
6.16
8.75
8.96
11.25
7.09
10.52
17.18
16.54
16.19
16.60
12.35
16.81
17.14

8.36
48.63
51.99
53.42
52.90
47.98
55.08
56.80
56.05
56.54
56.61
52.72
56.73
57.29

CSharp
EM BLEU-4
0.00
13.11
14.77
12.96
15.04
14.52
16.47
22.45
21.67
21.49
21.56
17.60
21.80
21.95

17.27
48.83
56.56
56.48
53.38
50.08
56.79
57.27
55.75
54.77
56.49
53.26
57.17
55.71

Java
EM BLEU-4
0.00
18.10
22.28
22.23
24.09
19.50
24.32
31.30
30.36
30.20
30.51
24.99
31.29
30.50

17.83
47.64
51.85
52.73
54.05
50.84
55.68
53.99
55.40
54.73
54.74
49.99
54.67
55.10

JavaScript
EM BLEU-4
0.00
14.12
15.98
17.26
20.15
14.65
16.53
29.14
28.36
27.48
27.82
21.61
28.48
28.70

9.71
45.59
50.28
48.92
54.88
45.24
54.21
55.52
55.49
54.42
55.12
50.41
54.28
54.74

PHP
EM BLEU-4
0.00
8.71
10.67
7.72
13.86
9.55
12.05
17.37
17.13
16.53
16.92
14.65
17.45
17.22

10.20
51.59
55.47
53.11
55.64
52.34
57.51
59.16
59.69
57.57
57.86
58.02
59.84
59.52

Python
EM BLEU-4
0.00
10.39
13.04
10.72
18.12
11.36
14.72
19.96
19.97
18.94
19.53
18.13
19.94
19.74

7.38
43.14
49.39
46.81
53.03
46.01
50.87
54.00
51.51
51.64
50.89
51.23
53.50
51.80

TypeScript
EM BLEU-4
0.01
24.89
27.73
24.02
30.97
26.37
29.86
35.43
35.24
34.30
34.53
31.80
35.12
35.18

23.80
56.71
62.57
58.15
61.08
58.61
64.39
66.64
64.86
63.00
64.86
60.28
62.42
65.77

Overall
EM BLEU-4
0.00
13.64
16.17
14.84
19.07
14.72
17.78
24.69
24.18
23.59
23.92
20.16
24.41
24.35

13.51
48.88
54.02
52.80
54.99
50.16
56.36
57.63
56.96
56.10
56.65
53.70
56.94
57.13

Experimental Results and Analysis

Compared with Baselines
We compare the results of these evaluation tasks with the
baselines in Table 2, Table 3, Table 5 and Table 4 respec-
tively. We can conclude the following ﬁndings: 1) Compared
with the supervised techniques such as Transformer, the pre-
trained models improve the performance signiﬁcantly over
these tasks. 2) PLBART and CodeT5 perform better than
CodeBERT. We attribute the gains brought by PLBART and
CodeT5 using an additional pre-trained decoder to improve
the performance. Furthermore, since CodeT5 and UniX-
coder design a cross-modal generation (NL-PL) pre-training
task, while PLBART only uses denoising tasks for pre-
training, CodeT5 and UniXcoder have better performance
on these generation tasks except for the results of CodeT5
on updated code snippet generation. We investigate the rea-
son and conjecture that it may be caused by the speciﬁcity of
this task. The input of different models for this task consists
of the message (NL) and the previous code snippet (PL).
This task requires the model to generate the updated code
snippet, which is different from the code-related tasks that
CodeT5 used. 3) The incremental models trained on Code-
BERT and PLBART with our collected commit dataset have
better performance than their original models (i.e., Incr-
CodeBERT vs CodeBERT, Incr-PLBART vs PLBART), we
believe the improvements are from the incremental models
learn more domain-related (i.e., commit-related) knowledge
at the pre-training phase compared with the original models.
4) CommitBART outperforms these baselines especially on
the generation tasks signiﬁcantly. The improvements come
from two aspects. One is a large amount of commit data. We
use over 7 million commits to pre-train a model for commit-
related tasks. Hence, our model is domain-speciﬁc. Another
reason is the designed pre-training tasks. We utilize three
categories for six pre-training tasks to improve the model
performance on different types of downstream tasks.
Model Analysis
We further conduct an ablation study to investigate each
pre-training task to the ﬁnal performance across these ﬁne-
tuning tasks and the results are reported in the last line of
Table 2, Table 3, Table 5 and Table 4 respectively. Through
analysing the results, we have the following ﬁndings: 1) Seg-
ment embedding is useful in CommitBART, although it is
not widely used in code pre-trained models. We believe it
is due to the commit usually having more complex compo-
nents (e.g., commit message, deleted/updated statements),
hence segment embedding helps the model incorporate each
component well. 2) Two cross-modal generation tasks (i.e.,
PL2NL and PLNL2PL) provide signiﬁcant improvements to
the generation tasks. Taking the commit message generation
task in Table 3 as an example, after removing the PL2NL, the
overall smoothed BLEU-4 drops from 20.60 to 18.39 across
the seven programming languages. 3) Apart from the cross-
modal generation tasks, graph-guided token masking (GTM)
is more critical than the remaining tasks on three genera-
tion tasks. It demonstrates that by constructing a commit-
graph to predict the masked nodes by their connected nodes
in the graph, the semantic gap between the message (NL)

Figure 4: One example from Python for the task of commit
message generation where the commit id is 966b15.

and changed code (PL) is effectively alleviated. 4) Both con-
trastive learning pre-training tasks are effective in improving
performance. Overall, we conclude that each pre-training
task improves CommitBART.
Case Study
We also conduct a case study including an example from
commit message generation with the generation results from
different models to intuitively demonstrate the effectiveness
of CommitBART and it is shown in Figure 4. More ex-
amples of different models for these commit-related gener-
ation tasks can be found in Appendix. From the changed
code in this example, we ﬁnd that this commit is to add a
package namely “xray.backends”. Its commit message (i.e.,
“Add xray.backends to setup.py”) further conﬁrms this. The
results produced by UniXcoder and Incr-PLBART are bet-
ter than other baselines. However, both models fail to gen-
erate the accurate package name (i.e., “xray.backends”). In
contrast, CommitBART produces the same output with the
ground-truth and the package name is accurately generated.
We attribute the improvement to the designed graph-guided
token masking pre-training task, which helps the model cap-
ture the critical semantic information in a commit.

Conclusion
Due to the lack of a large-scale commit data, we collect
a benchmark (over 7.99 million commits) across 7 pro-
gramming languages and make it public for follow-up re-
searchers. Based on this benchmark, we present Commit-
BART, a pre-trained encoder-decoder model for GitHub
commits. The model is pre-trained via three categories
(i.e., denoising objectives, cross-modal generation and con-
trastive learning) for six pre-training tasks to learn commit
representations. We are the ﬁrst to present a large pre-trained

@@ def write_version_py(filename=None):   tests_require=['mock >= 1.0.1', 'nose >= 1.0'],url='https://github.com/akleeman/xray',test_suite='nose.collector',  -packages=['xray'])+packages=['xray', 'xray.backends'])File: setup.pyGround Truth: Add xray.backendsto setup.pyCodeBERT:Add support for py3.xPLBART: Update setup.pyCommitBART: Add xray.backendsto setup.pyChanged Code Snippet:UniXcoder: Add xraypackage to setup.pyIncr-PLBART: Add xraypackage to setup.pyIncr-CodeBERT: Update setup.pyCodeT5-base: Fix variable.valuesmodel for GitHub commits and further unify a “commit in-
telligence” framework with one commit-related understand-
ing task and three generation tasks. Extensive experiments
conﬁrm CommitBART signiﬁcantly outperforms previous
works on these tasks. Further ablation study also reveals the
effectiveness of each pre-training task. We encourage more
commit-related tasks to merge into our framework.

References
Ahmad, W. U.; Chakraborty, S.; Ray, B.; and Chang, K.-W.
2021. Uniﬁed pre-training for program understanding and
generation. arXiv preprint arXiv:2103.06333.
Allamanis, M. 2019. The adverse effects of code duplica-
In Proceedings
tion in machine learning models of code.
of the 2019 ACM SIGPLAN International Symposium on
New Ideas, New Paradigms, and Reﬂections on Program-
ming and Software, 143–153.
Buratti, L.; Pujar, S.; Bornea, M.; McCarley, S.; Zheng, Y.;
Rossiello, G.; Morari, A.; Laredo, J.; Thost, V.; Zhuang, Y.;
et al. 2020. Exploring software naturalness through neural
language models. arXiv preprint arXiv:2006.12641.
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
In Burstein, J.; Doran, C.; and
Language Understanding.
Solorio, T., eds., Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, NAACL-
HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-
ume 1 (Long and Short Papers), 4171–4186. Association for
Computational Linguistics.
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.;
Shou, L.; Qin, B.; Liu, T.; Jiang, D.; et al. 2020. Codebert: A
pre-trained model for programming and natural languages.
arXiv preprint arXiv:2002.08155.
Gao, T.; Yao, X.; and Chen, D. 2021. Simcse: Simple con-
trastive learning of sentence embeddings. arXiv preprint
arXiv:2104.08821.
Guo, D.; Lu, S.; Duan, N.; Wang, Y.; Zhou, M.; and Yin,
J. 2022. UniXcoder: Uniﬁed Cross-Modal Pre-training for
Code Representation. arXiv preprint arXiv:2203.03850.
Guo, D.; Ren, S.; Lu, S.; Feng, Z.; Tang, D.; Liu, S.; Zhou,
L.; Duan, N.; Svyatkovskiy, A.; Fu, S.; et al. 2020. Graph-
codebert: Pre-training code representations with data ﬂow.
arXiv preprint arXiv:2009.08366.
Jain, P.; Jain, A.; Zhang, T.; Abbeel, P.; Gonzalez, J. E.; and
Stoica, I. 2020. Contrastive code representation learning.
arXiv preprint arXiv:2007.04973.
Jiang, S.; Armaly, A.; and McMillan, C. 2017. Automat-
ically generating commit messages from diffs using neu-
In 2017 32nd IEEE/ACM Inter-
ral machine translation.
national Conference on Automated Software Engineering
(ASE), 135–146. IEEE.
Jiang, X.; Zheng, Z.; Lyu, C.; Li, L.; and Lyu, L. 2021. Tree-
BERT: A tree-based pre-trained model for programming lan-
guage. In de Campos, C. P.; Maathuis, M. H.; and Quaeghe-
beur, E., eds., Proceedings of the Thirty-Seventh Conference
on Uncertainty in Artiﬁcial Intelligence, UAI 2021, Virtual
Event, 27-30 July 2021, volume 161 of Proceedings of Ma-
chine Learning Research, 54–63. AUAI Press.
Jung, T.-H. 2021. Commitbert: Commit message genera-
tion using pre-trained programming language model. arXiv
preprint arXiv:2105.14242.
Kanade, A.; Maniatis, P.; Balakrishnan, G.; and Shi, K.
2020. Pre-trained Contextual Embedding of Source Code.
CoRR, abs/2001.00059.

els for code understanding and generation. arXiv preprint
arXiv:2109.00859.
Wu, B.; Liu, S.; Feng, R.; Xie, X.; Siow, J.; and Lin, S.-W.
2022. Enhancing Security Patch Identiﬁcation by Capturing
Structures in Commits. arXiv preprint arXiv:2207.09022.
Wu, Z.; Xiong, Y.; Yu, S. X.; and Lin, D. 2018. Unsuper-
vised feature learning via non-parametric instance discrimi-
nation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, 3733–3742.
Yasunaga, M.; and Liang, P. 2020. Graph-based, self-
In
supervised program repair from diagnostic feedback.
International Conference on Machine Learning, 10799–
10808. PMLR.
Zhou, J.; Pacheco, M.; Wan, Z.; Xia, X.; Lo, D.; Wang, Y.;
and Hassan, A. E. 2021a. Finding A Needle in a Haystack:
In 2021
Automated Mining of Silent Vulnerability Fixes.
36th IEEE/ACM International Conference on Automated
Software Engineering (ASE), 705–716. IEEE.
Zhou, Y.; Siow, J. K.; Wang, C.; Liu, S.; and Liu, Y. 2021b.
SPI: Automated Identiﬁcation of Security Patches via Com-
mits. ACM Transactions on Software Engineering and
Methodology (TOSEM), 31(1): 1–27.

Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mo-
hamed, A.; Levy, O.; Stoyanov, V.; and Zettlemoyer, L.
2019. Bart: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461.
Liu, F.; Li, G.; Zhao, Y.; and Jin, Z. 2020a. Multi-task learn-
ing based pre-trained language model for code completion.
In Proceedings of the 35th IEEE/ACM International Confer-
ence on Automated Software Engineering, 473–485.
Liu, S.; Gao, C.; Chen, S.; Yiu, N. L.; and Liu, Y. 2020b.
ATOM: Commit message generation based on abstract syn-
tax tree and hybrid ranking. IEEE Transactions on Software
Engineering.
Lu, S.; Guo, D.; Ren, S.; Huang, J.; Svyatkovskiy, A.;
Blanco, A.; Clement, C. B.; Drain, D.; Jiang, D.; Tang, D.;
Li, G.; Zhou, L.; Shou, L.; Zhou, L.; Tufano, M.; Gong,
M.; Zhou, M.; Duan, N.; Sundaresan, N.; Deng, S. K.; Fu,
S.; and Liu, S. 2021. CodeXGLUE: A Machine Learn-
ing Benchmark Dataset for Code Understanding and Gen-
In Vanschoren, J.; and Yeung, S., eds., Proceed-
eration.
ings of the Neural Information Processing Systems Track on
Datasets and Benchmarks 1, NeurIPS Datasets and Bench-
marks 2021, December 2021, virtual.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;
Sutskever, I.; et al. 2019. Language models are unsupervised
multitask learners. OpenAI blog, 1(8): 9.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y.; Li, W.; Liu, P. J.; et al. 2020. Explor-
ing the limits of transfer learning with a uniﬁed text-to-text
transformer. J. Mach. Learn. Res., 21(140): 1–67.
Svyatkovskiy, A.; Deng, S. K.; Fu, S.; and Sundaresan, N.
2020.
IntelliCode compose: code generation using trans-
In Devanbu, P.; Cohen, M. B.; and Zimmermann,
former.
T., eds., ESEC/FSE ’20: 28th ACM Joint European Software
Engineering Conference and Symposium on the Foundations
of Software Engineering, Virtual Event, USA, November 8-
13, 2020, 1433–1443. ACM.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-
tention is All you Need.
In Guyon, I.; von Luxburg, U.;
Bengio, S.; Wallach, H. M.; Fergus, R.; Vishwanathan, S.
V. N.; and Garnett, R., eds., Advances in Neural Informa-
tion Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA, 5998–6008.
Wang, X.; Wang, S.; Feng, P.; Sun, K.; Jajodia, S.; Ben-
chaaboun, S.; and Geck, F. 2021a. PatchRNN: A Deep
Learning-Based System for Security Patch Identiﬁcation. In
MILCOM 2021-2021 IEEE Military Communications Con-
ference (MILCOM), 595–600. IEEE.
Wang, X.; Wang, Y.; Mi, F.; Zhou, P.; Wan, Y.; Liu, X.; Li,
L.; Wu, H.; Liu, J.; and Jiang, X. 2021b. Syncobert: Syntax-
guided multi-modal contrastive pre-training for code repre-
sentation. arXiv preprint arXiv:2108.04556.
Wang, Y.; Wang, W.; Joty, S.; and Hoi, S. C. 2021c. Codet5:
Identiﬁer-aware uniﬁed pre-trained encoder-decoder mod-

Appendix

Pre-training Settings
We follow the architecture of BART (Lewis et al. 2019),
which is made up of a 6-layer Transformer encoder and
a 6-layer decoder with the dimension of 768 and 12
heads (∼140M parameters). We adopt the parameters of
PLBART (Ahmad et al. 2021) to initialize the pre-trained
model and add an additional segment embedding layer to en-
code different segment identiﬁers. We also adopt the vocabu-
lary set of PLBART, which contains 50K subtokens and fur-
ther add 5 different segment identiﬁers into the vocabulary
set to represent different components in a commit. We set the
maximum input sequence length to 512 and use the mixed
precision of FP16 to accelerate the pre-training process. We
set the batch size to 512 and employ the AdamW optimizer
to update the model parameters with a learning rate of 2e-
4. We pre-train the model on one DGX server, which has
8 NVIDIA Tesla V100 with 32GB memory. The total steps
are set to 80K where the category of denoising objectives,
cross-modal generation and contrastive learning, each takes
up 60%, 30%, 10% respectively. One category has two dif-
ferent pre-training tasks and each of them accounts for half
of the steps. The total time for the pre-training process is
about 60 hours. We follow Guo et al. (Guo et al. 2020) to
sample each batch from the same programming language ac-
cording to a distribution {qi}i=1...N , where ni is the number
of examples for i-th programming language and α = 0.7 to
alleviate the bias towards high-resource languages.

qi =

pα
i
j=1 pα
j

(cid:80)N

, pi =

ni
k=1 nk

(cid:80)N

(4)

Fine-tuning Settings
Understanding Task For the understanding task of se-
curity patch identiﬁcation, we directly utilize the released
dataset from Wu et al. (Wu et al. 2022), which consists of
26500 training samples, 3301 validation samples and 3294
test samples for evaluation. We employ AdamW optimizer
to ﬁne-tune CommitBART with a 5e-5 learning rate and the
batch size 32 for 4K steps. We set the maximum input se-
quence length to 512 and the target sequence length to 5,
which includes the start token “[CLS]”, the task preﬁx “se-
curity patch”, the label “True” or “False” and the end token
“[EOS]”.

Generation Tasks For the generation tasks of commit
message generation and updated code snippet generation,
we directly utilize the ﬁne-tuning dataset from our bench-
mark (See Table 1) with train-validation-test with the ratio
of 75%:10%:15% for evaluation. For the task of positive
code statements generation, we extract the consecutive mod-
iﬁcation samples from the constructed ﬁne-tuning dataset for
evaluation. The statistics of the train, validation and test are
shown in Table 6. For each task, we utilize AdamW opti-
mizer to ﬁne-tune CommitBART with a 5e-5 learning rate
of batch size 32 for 10K steps. Furthermore, we set the early
stop based on the validation loss. The maximum input se-
quence length is set to 512 for these tasks, while the tar-
get sequence is set to 150 for commit message generation,

Figure 5: One example from C for the task of commit mes-
sage generation where the commit id is 8457d5.

512 for updated code snippet generation and 300 for positive
code statements generation respectively.

Case Study
In this section, we provide more examples to demonstrate
the effectiveness of CommitBART.

Commit Message Generation We further provide one ex-
ample for commit message generation. From Figure 5, we
can see that its commit message is to delete an unused vari-
able, hence there is only the deleted statement (i.e., “const
bool is gles = glext.version.is es”) without the added state-
ment. By comparing the results produced by different mod-
els, we ﬁnd that CommitBART can generate a better result.

Positive Code Statements Generation We provide two
examples for the task of positive code statements genera-
tion in Figure 6a and Figure 6b respectively. The input con-
sists of the ﬁle path, the commit message and the previous
code snippet. We ask the model to generate the positive code
statements marked as “+” in a commit. We can see that both
commits aim to add a security check to avoid code crash.
Compared with the results produced by different models,
CommitBART generates accurate positive code statements.

Updated Code Snippet Generation We provide two ex-
amples for the task of updated code snippet generation. Sim-
ilar to the task of positive code statements generation, the in-
put includes the ﬁle path, the commit message and the pre-
vious code snippet. For the ﬁrst example, the input is pre-
sented in Figure 7a and the ground-truth for this example is
presented in Figure 7b. By Figure 7a, we can get that the
updated code snippet needs to add a variable safety check to
avoid “ph” as empty. The results produced by different mod-
els are presented in Figure 7c to Figure 7i accordingly. We
can observe that PLBART and Incr-PLBART produce bet-
ter results that other baseline models for this example, how-
ever they are both misunderstand the semantics of the com-

@@ static GLTextureFormatInfo*add_texture_format(constGLTextureFormatInfo*fmt) {}void glcommon_init_texture_formats(void) {-constbool is_gles= glext.version.is_es;constbool is_gles3 = GLES_ATLEAST(3, 0);constbool is_gles2 = !is_gles3 && GLES_ATLEAST(2, 0);constbool have_rg= glext.texture_rg;File:src/renderer/glcommon/texture.cGround Truth:glcommon: remove unused variableCodeBERT:glsl: fix buildPLBART: remove codeCommitBART: glcommon: remove unused variableChanged Code Snippet:UniXcoder: glcommon: remove unneeded is_escheckIncr-PLBART: glcommon: remove unused codeIncr-CodeBERT: glcommon: fix buildCodeT5-base: stage4: Remove unused kurumi_spell_bgTable 6: The statistics of ﬁne-tuning dataset for commit message generation, updated code snippet generation and positive code
statements generation. The ﬁrst row is the dataset used for ﬁrst two tasks while second row is the dataset for the last task.

Fine-tune
Train
Validation
Test
Positive-Train
Positive-Validation
Positive-Test

C CSharp
45,425
5,501
10,976
19,726
2,561
2,562

51,504
8,902
11,518
20,746
2,320
2,321

Java
63,189
7,512
10,425
27,619
2,580
2,580

JavaScript
68,338
8,100
10,626
31,809
2,655
2,656

PHP
78,450
9,239
11,541
35,015
2,490
2,491

Python Typescript
51,526
70,137
8,326
6,172
10,064
11,039
26,325
28,198
2,957
2,363
2,958
2,364

Total
428,569
53,752
76,189
189,438
17,926
17,932

(a) The commit id is 054122.

(b) The commit id is 99cfb9.

Figure 6: Two examples from Java for the task of positive code statements generation.

mit message and produce the exact opposite results (i.e., “if
(tPh.isEmpty())” and “if (tPh.length() == 0)”). In contrast,
CommitBART can capture the semantics well and produce
the same code snippet with the ground-truth. For another ex-
ample, which is shown in Figure 8, we can see that this com-
mit removes the variable “isContinuousIntegration” and fur-
ther deletes its usage in the following statement. The mod-
iﬁcation is non-consecutive. The results produced by Code-
BERT and CodeT5 only copy the original statements while
the results produced by the remaining baselines remove the
variable declaration, but fail to delete its usage. In contrast,
CommitBART can produce the same code snippet with the
ground-truth.

Ground Truth:if (adapter != null) {init(false); }CodeBERT:if ( false ) { init( false ) ; } PLBART: if ( !getCardContext( ) .getCardContext)CommitBART:if (adapter != null) {init(false); }UniXcoder: init( true ) ; Incr-PLBART: if ( !init( false ) ) { init( false ) ; }Incr-CodeBERT: if ( false ) { init( false ) ; } CodeT5-base: if (!isInitialized( ) ) { init( false ) ; } @@ public ViewGroupgetTopView() {public void setElevationEnabled(booleanelevationEnabled) {this.elevationEnabled= elevationEnabled; -init(false);}}File:pac4j -mongo/src/main/java/org/pac4j/mongo/credentials/authenticator/MongoAuthenticator.javaPrevious Code Snippet:Message: Check adapter before initializationGround Truth: if (maker != null) { maker.securityCheck() ; }CodeBERT: maker.securityCheck( ) ; PLBART: maker.securityCheck( false ) ; CommitBART:if (maker != null) { maker.securityCheck() ; }UniXcoder: maker.securityCheck( ) ; Incr-PLBART: maker.securityCheck( true ) ;Incr-CodeBERT:maker.securityCheck( ) ;CodeT5-base: if (!maker.isSecurityCheckEnabled( ) ) { maker.securityCheck( ) ; } @@ public DownloadMgrInitialParams() {public DownloadMgrInitialParams(InitCustomMakermaker) {this.mMaker= maker;-maker.securityCheck();}public int getMaxNetworkThreadCount() {File: Libaray/src/main/java/com/liulishuo/filedownloader/services/DownloadMgrInitialParams.javaPrevious Code Snippet:Message: fix: fix the fatal crash when you don't provide the init-params for customize component(a) The input for models to generate the updated code snippet.

(b) The updated code snippet (i.e., ground-truth).

(c) The result provided by CodeBERT.

(d) The result provided by PLBART.

(e) The result provided by CodeT5-base.

(f) The result provided by UniXcoder.

(g) The result provided by Incr-CodeBERT.

(h) The result provided by Incr-PLBART.

Figure 7: One example from Java for the task of updated code snippet generation where the commit id is 0aa027.

(i) The result provided by CommitBART.

Previous Code Snippet:@@ private void updatePhAttributesFromPhElements(Element token) tPh.append(" "+syl.getAttribute("tone"));}}-token.setAttribute("ph", tPh.toString());}File: java/marytts/tools/analysis/TranscriptionAligner.javaMessage: bugfix: prevent TranscriptionAlignerfrom setting empty phattributes (e.g.on punctuation tokens)Updated Code Snippet:@@ private void updatePhAttributesFromPhElements(Element token) tPh.append(" "+syl.getAttribute("tone"));}}+if (tPh.toString().length() > 0) { +token.setAttribute("ph", tPh.toString());+}}CodeBERT:@@ private void updatePhAttributesFromPhElements(Element token) tPh.append(" "+syl.getAttribute("tone"));}}+token.setAttribute("ph", tPh.toString());}PLBART:@@ private void updatePhAttributesFromPhElements(Element token) tPh.append(" "+syl.getAttribute("tone"));}}+if (tPh.isEmpty()) { +token.setAttribute("ph", tPh.toString());+}}CodeT5-base:@@ private void updatePhAttributesFromPhElements(Element token) tPh.append(" "+syl.getAttribute("tone"));}}+if (tPh.length() == 0) { +token.setAttribute(“ph”, “”)+}}UniXcoder:@@ private void updatePhAttributesFromPhElements(Element token) tPh.append(" "+syl.getAttribute("tone"));}}+++}Incr-CodeBERT:@@ private void updatePhAttributesFromPhElements(Element token) tPh.append(" "+syl.getAttribute("tone"));}}++token.setAttribute("ph", tPh.toString());+}Incr-PLBART:@@ private void updatePhAttributesFromPhElements(Element token) tPh.append(" "+syl.getAttribute("tone"));}}+if (tPh.length() == 0) { +token.setAttribute("ph", tPh.toString());+}}CommitBART:@@ private void updatePhAttributesFromPhElements(Element token) tPh.append(" "+syl.getAttribute("tone"));}}+if (tPh.toString().length() > 0) { +token.setAttribute("ph", tPh.toString());+}}(a) The input for models to generate the updated code snippet.

(b) The updated code snippet (i.e., ground-truth).

Figure 8: Another example from Java for the task of updated code snippet generation where the commit id is e640a3.

(c) The results provided by CodeBERT and CodeT5-base.

Previous Code Snippet:@@public class IntegrationTestActiveProfilesResolverimplements ActiveProfilesResopublic String[] resolve(final Class<?> testClass) {          final booleanisCI= Boolean.valueOf(System.getProperty("CI", "false"));    final booleanisTravis= Boolean.valueOf(System.getProperty("TRAVIS", "false"));-final booleanisContinuousIntegration= Boolean.valueOf(System.getProperty("CONTINUOUS_INTEGRATION", "false"));final String[] activeProfiles;-if (isCI|| isTravis|| isContinuousIntegration) {activeProfiles= new String[]{"ci"};} else {activeProfiles= new String[]{"integration"};File: …/web/controllers/IntegrationTestActiveProfilesResolver.javaMessage: Removing redundant ENV variableUpdated Code Snippet:@@public class IntegrationTestActiveProfilesResolverimplements ActiveProfilesResopublic String[] resolve(final Class<?> testClass) {          final booleanisCI= Boolean.valueOf(System.getProperty("CI", "false"));    final booleanisTravis= Boolean.valueOf(System.getProperty("TRAVIS", "false"));final String[] activeProfiles;+if (isCI|| isTravis) {activeProfiles= new String[]{"ci"};} else {activeProfiles= new String[]{"integration"};CodeBERT/CodeT5-base:@@public class IntegrationTestActiveProfilesResolverimplements ActiveProfilesResopublic String[] resolve(final Class<?> testClass) {          final booleanisCI= Boolean.valueOf(System.getProperty("CI", "false"));    final booleanisTravis= Boolean.valueOf(System.getProperty("TRAVIS", "false"));+final booleanisContinuousIntegration= Boolean.valueOf(System.getProperty("CONTINUOUS_INTEGRATION", "false"));final String[] activeProfiles;+if (isCI|| isTravis|| isContinuousIntegration) {activeProfiles= new String[]{"ci"};} else {activeProfiles= new String[]{"integration"};(d) The results provided by PLBART, UniXcoder, Incr-CodeBERT and Incr-PLBART.

Figure 8: Another example from Java for the task of updated code snippet generation where the commit id is e640a3 (cont.).

(e) The result provided by CommitBART.

PLBART/UniXcoder/Incr-CodeBERT/Incr-PLBART:@@public class IntegrationTestActiveProfilesResolverimplements ActiveProfilesResopublic String[] resolve(final Class<?> testClass) {          final booleanisCI= Boolean.valueOf(System.getProperty("CI", "false"));    final booleanisTravis= Boolean.valueOf(System.getProperty("TRAVIS", "false"));final String[] activeProfiles;+if (isCI|| isTravis|| isContinuousIntegration) {activeProfiles= new String[]{"ci"};} else {activeProfiles= new String[]{"integration"};CommitBART:@@public class IntegrationTestActiveProfilesResolverimplements ActiveProfilesResopublic String[] resolve(final Class<?> testClass) {          final booleanisCI= Boolean.valueOf(System.getProperty("CI", "false"));    final booleanisTravis= Boolean.valueOf(System.getProperty("TRAVIS", "false"));final String[] activeProfiles;+if (isCI|| isTravis) {activeProfiles= new String[]{"ci"};} else {activeProfiles= new String[]{"integration"};