Massively Parallel Fitting of Gaussian Approximation Potentials

Sascha Klawohn and James R. Kermode
Warwick Centre for Predictive Modelling, School of Engineering,
University of Warwick, Coventry CV4 7AL, United Kingdom

Albert P. Bart´ok
Department of Physics, University of Warwick, Coventry CV4 7AL, United Kingdom and
Warwick Centre for Predictive Modelling, School of Engineering,
University of Warwick, Coventry CV4 7AL, United Kingdom
(Dated: July 29, 2022)

We present a data-parallel software package for ﬁtting Gaussian Approximation Potentials (GAPs)
on multiple nodes using the ScaLAPACK library with MPI and OpenMP. Until now the maximum
training set size for GAP models has been limited by the available memory on a single compute node.
In our new implementation, descriptor evaluation is carried out in parallel with no communication
requirement. The subsequent linear solve required to determine the model coeﬃcients is parallelised
with ScaLAPACK. Our approach scales to thousands of cores, lifting the memory limitation and also
delivering substantial speedups. This development expands the applicability of the GAP approach
to more complex systems as well as opening up opportunities for eﬃciently embedding GAP model
ﬁtting within higher-level workﬂows such as committee models or hyperparameter optimisation.

I.

INTRODUCTION

Computational materials and molecular modelling
have proved to be an invaluable tool in predicting new
materials and processes and interpreting experimental
phenomena on the microscopic level. The predictive per-
formance of atomistic simulations strongly depends on
the accuracy of the employed atomic interaction model,
of which those based on solving the Schr¨odinger equation
are generally regarded as the most reliable. However,
many problems of interest remain intractable, even when
using approximate solutions of the quantum mechanical
problem, such as Density Functional Theory (DFT), due
to the high computational cost and its scaling with re-
spect to system size. While interatomic potentials based
on simple analytical forms open up access to signiﬁcantly
larger system sizes and longer simulation times, their pa-
rameterisation is often insuﬃciently accurate for predic-
tive modelling. Surrogate models for quantum mechani-
cal calculations, based on highly ﬂexible functional forms
provided by machine learning methods which are ﬁtted
using high-quality ab initio reference data emerged in
the last two decades [1–7]. These machine learning in-
teratomic potentials (MLIPs) reproduce the ab initio po-
tential energy surface to a high accuracy in a computa-
tionally eﬃcient way, allowing access to large time and
length scale simulations [8].

In this work, we focus on the ﬁtting aspect of MLIPs,
i.e. the process that determines the model parameters
based on a set of reference data points. Even though
ﬁtting is typically a one-oﬀ operation, and its compu-
tational cost leaves the cost of a subsequent simulation
using the MLIP largely or completely unaﬀected, it can
use signiﬁcant resources and can be a limiting factor in
applying ever increasing data bases or exploring the space
of model hyperparameters. Depending on the regression
method, some MLIPs are based on solving a linear sys-

tem to obtain the model weights. Here we present a set
of application principles that can be used to distribute
the workload among multiple processes when ﬁtting such
models, allowing eﬃcient utilisation of massively paral-
lel computer resources. We have implemented these in
the Gaussian Approximation Potential (GAP) framework
and demonstrated excellent scaling in both memory and
computational eﬃciency up to thousands of cores. We
note that similar eﬀorts have been made to parallelise the
fitSNAP code used for ﬁtting Spectral Neighbour Analy-
sis Potential (SNAP) [3] linear models [9]. Other MLIP
implementations which are based on kernel methods [10]
or linear regression, such as the Linear Machine Learning
(LML) [11] and Atomic Cluster Expansion (ACE) [4] ap-
proaches, would also beneﬁt from similar developments.

II. THEORY

We provide a brief overview of the GAP framework,
and for more details we refer the reader to a more com-
prehensive review [12] of the method. GAP is a Bayesian
regression method that aims to create surrogate models
for the quantum mechanical interactions by using a ref-
erence database consisting of atomic conﬁgurations and
associated microscopic properties obtained from ab initio
calculations. Strategies to create such databases are dis-
cussed elsewhere [13, 14]; here we start from a set of con-
ﬁgurations, each consisting of Cartesian coordinates with
the corresponding atomic species information, and for ab
initio data we use total energies, forces and virial stress
components. As Cartesian coordinates do not transform
in an invariant fashion when applying energy-conserving
symmetry operations to the atomic positions, such as ro-
tations, translations, inversion and permutation of the
indices of identical atomic species, it is beneﬁcial to ﬁrst
transform the Cartesian coordinates to descriptors, which

2
2
0
2

l
u
J

7
2

]
i
c
s
-
l
r
t

m

.
t
a
m
-
d
n
o
c
[

2
v
3
0
8
3
0
.

7
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

form the input vectors xi of length d to the regression
method.

In general, GAP approximates the total energy of a
conﬁguration A in the form of a sparse Gaussian Process
(GP) [15, 16]

output. Following their suggestions, we ﬁrst deﬁne the
(N + M )

M matrix

×

A =

(cid:21)

(cid:20)Σ−1/2KNM
LT

MM

(5)

EA =

(cid:88)

M
(cid:88)

i∈A

j

cjk(xi, xj)

(1)

where the lower triangular matrix LMM is the result of
the Cholesky decomposition of KMM such that KMM =
LMM LT
MM . Introducing b by padding the vector of tar-
get properties y by an M -long vector of zeros

where the ﬁrst sum includes all descriptor vectors in con-
ﬁguration A, and the second sum is over a set of rep-
resentative descriptor vectors, or sparse points M . The
kernel function k(xi, xj) evaluates the similarity of de-
scriptors xi and xj, and cj are the regression weights that
need to be ﬁtted such that predicted properties match the
ab initio values as closely as possible. Forces and virial
stress components can be obtained by diﬀerentiating this
expression with respect to atomic coordinates or lattice
deformations, which is a trivial, but rather tedious op-
eration and we omit it from here for brevity. Denoting
the N ab initio reference properties by y and predicted
properties by ˜y, we formulate the regression problem as
minimising the loss function

= (y

−

L

˜y)T Σ−1(y

−

˜y) + cT KMM c

(2)

with respect to the weights c. The matrix Σ is diagonal
and its elements are inversely related to the importance
of each data point. While the ﬁrst term is responsible
for achieving a close ﬁt to the data points, the second
term is controlling overﬁtting via a Tikhonov regularis-
ing expression, which forces the elements of c to remain
small. The elements of the matrix KMM are built from
the kernel function values Kij = k(xi, xj) between the
xi}
sparse point set
N .
{
The minimum of the loss function in Eq. (2) can be de-
termined analytically, and the result is

M
i=1 where we typically use M

(cid:28)

c = (KMM + KM N Σ−1KNM )−1KM N Σ−1y

(3)

where the elements of KM N are given by

Kij =

(cid:88)

α∈j

k(xi, xα)

(4)

where xi is a descriptor vector from the sparse set, and
j denotes a target total energy and the sum includes all
descriptors that contribute to yj. For convenience, we
use the notation KT
KNM . Elements of KM N
corresponding to derivative observations are calculated
similarly, using the appropriate gradients of the kernel
function k, for which further details may be found in the
review article by Deringer et al [12].

M N ≡

(M 2N ),
The complexity of solving Eq. (3) scales with
O
(N 3)
which is signiﬁcantly more favourable than the
scaling of a full GP implementation. However, Foster et
al have shown [17] that the solution may lead to numer-
ically unstable results at large data sets, i.e. uncertain-
ties in the input lead to disproportionate errors in the

O

b =

(cid:21)

(cid:20)y
0

(6)

we rewrite Eq. (3) as the solution of the least-squares
problem

min
c

(Ac

−

b)T (Ac

b)

−

that leads to the solution in the form of

c = (AT A)−1AT b.

(7)

(8)

A numerically stable solution can be obtained by ﬁrst
carrying out a QR factorisation of A = QR where Q is
orthogonal, namely, it is formed by orthonormal column
vectors:

QT Q = I,

(9)

while R is an upper triangular matrix. Substituting the
factorised form of A into Eq. (8) results in

c = (RT QT QR)−1RT QT b = R−1QT b.

(10)

The computational complexity of creating A is deter-
mined by the cost of creating its two constituent blocks.
The calculation of the upper block scales as
(M N ), due
to Σ being diagonal, while the Cholesky factorisation re-
(M 3), resulting in
sulting in the lower block scales as
O
an overall scaling
M . The QR factori-
O
(M 2N ) ﬂoating point operations,
sation of A requires
hence dominating the overall cost of evaluating Eq. (8).
We note that multiplying by R−1 can be implemented as
a series of back substitution operations, due to the upper
triangular matrix form of R.

(M N ), as N

(cid:29)

O

O

III.

IMPLEMENTATION

The workﬂow of obtaining the sparse or representa-
tive points and associated vector of weights c from a
set of reference ab initio conﬁgurations is implemented
in the gap fit program, and distributed as part of the
software package Quantum Mechanics and Interatomic
Potentials (QUIP), which is a Fortran package imple-
menting atomistic simulation tools, including low-level
functions to manipulate atomic conﬁgurations, a selec-
tion of interatomic potentials, tight-binding models and
the GAP framework. The source code is publicly avail-
able on Github [18].

3

FIG. 2: Scaling of computing time of a non-MPI
gap fit calculation with the number of OpenMP
threads for diﬀerent chunk sizes. The reference time is
653 s.

KM N and KMM are calculated. From these, matrix A
is constructed and Eq. (8) is solved via QR decomposi-
tion using linear algebra routines, using Linear Algebra
Package (LAPACK) for single node applications.

The intermediate processing, such as the computation
of the elements of covariance matrices had already been
augmented by Open Multiprocessing (OpenMP) direc-
tives along the target data dimension N , which lead to
a thread-based parallelisation on a single process. This,
however, restricts the program to the memory and pro-
cessing resources of a single node, and performance is
further limited by the fact that the speed a computa-
tional core can access an arbitrary memory region is in-
homogeneous due to the physical layout of the memory.
That results in a decrease of eﬃciency when additional
threads are employed, leading to a degradation of perfor-
mance which prevents full utilisation of all available cores
in a node. We present the parallel scalability of a test
problem in Fig. 2, where we varied the size of contiguous
subsets of OpenMP loops, referred to as chunks.

As an example of the limitations imposed by the
OpenMP implementation of gap fit, the practical prob-
lem of ﬁtting a GAP for carbon [21] — one of the largest
single-element training datasets assembled to date —
took more than 6 days on a single node and required more
than 1 TB memory to accommodate the design and co-
variance matrices [22]. This restricted the ability of prac-
titioners to build complete training sets or to experiment
with choices of hyperparameters.

B. Multi-process parallelisation

To go beyond the limitations posed by the shared mem-
ory requirement, poor parallel performance, and special-
ist hardware, we propose a multi-process framework with
data distribution and inter-node communication. We

FIG. 1: Schema of gap fit using serial/thread-parallel
(black arrows) and data-parallel (blue arrows) execution
code paths.

A. Program structure

The gap fit program is controlled via a set of com-
mand line arguments consisting of key-value pairs, which
can also be passed as a conﬁguration ﬁle. The program
also requires a set of reference conﬁgurations in the ex-
tended XYZ format [19], containing any combination of
total energies, forces and virial stresses, and optionally,
the deﬁnition of a baseline potential model. The major
steps of the ﬁtting process are outlined in Fig. 1. Af-
ter initialisation and reading of the command line argu-
ments, the training structures are parsed for the num-
ber of target properties: total energies, forces and virial
stress components, to determine the value of N . Based
on the descriptors, the amount of storage space needed
for the descriptor arrays x and their derivatives x(cid:48) with
respect to Cartesian coordinates are calculated and then
allocated.

From the descriptor vectors, M are chosen as a rep-
resentative (sparse) set. The procedure for choosing can
be controlled by command line arguments, including se-
lecting a random subset, clustering and CUR-based ap-
proaches [20]. It is also possible to provide the chosen
representative points via ﬁles, an option we make use of
for the parallel version (see Section III B).

After setting the sparse points, the covariance matrices

ReadconfigurationsDeterminedatasizeDistributeconfigurationsCalculatedescriptorsandreadtargetdataDeterminesparsepointsCalculatecovariancematricesSolvelinearsystemxn,ynxMKMn,KMMAn+m,M,bn→cMhave established in Section II that both the memory
requirement and the computational eﬀort scale linearly
with the number of target properties N , therefore it is
convenient to distribute memory and tasks along this di-
mension of the problem.

The two most memory intensive operations are the cal-
culation of the descriptor vectors together with their gra-
dients, and the covariance matrices. The ratio of these
depends strongly on the particulars of the ﬁtting prob-
lem, in particular the dimensionality d of descriptor vec-
tors and the number of sparse points M . In our parallel
scheme, we distribute atomic conﬁgurations across inde-
pendent processes, such that the number of target prop-
erties are as even as possible. We note that the size of
individual atomic conﬁgurations may be highly inhomo-
geneous, therefore the number of forces per conﬁguration
can vary substantially across the database, necessitat-
ing an extra step that determines the optimal spread of
data. We have employed a greedy algorithm that ﬁrst
collects conﬁgurations in a list and sorts them by de-
scending number of target properties. We then assign
the largest (by target property) unassigned conﬁguration
to the process which currently has the least total number
of target properties. This process repeats until the list is
exhausted.

With the conﬁgurations allotted to Message Passing
Interface (MPI) processes, the descriptor calculations
may proceed locally, and once completed, individual por-
tions of KM N , denoted as KM n can be evaluated. For
this, local copies of the sparse set of M descriptor values
need to be present locally, the particulars of which we dis-
cuss later in Section III C. The na¨ıve solution of the linear
system represented by Eq. (3) may be adapted trivially
to a distributed KM N : the terms KM N Σ−1KNM and
KM N Σ−1y can be calculated locally and reduced across
processes as

KM N Σ−1KNM =

(cid:88)

n∈N

KM nΣ−1

n KnM

(11)

and

KM N Σ−1y =

(cid:88)

n∈N

KM nΣ−1

n yn

(12)

where we denote distributed blocks of Σ and y by Σn
and yn, respectively. The rest of the calculation only
involves matrices up the size of M
M . However, the
direct solution, as described in Section II is numerically
unstable, therefore we need to adapt the solution based
on the QR-factorisation.

×

The Scalable LAPACK (ScaLAPACK) library provides
some of the linear algebra features of the LAPACK li-
brary for distributed matrices, most commonly lever-
aging the MPI framework for communication between
nodes, which is widely available on computing clusters.
We chose to leverage the ScaLAPACK implementation,
therefore we need to take ScaLAPACK’s data distribut-
ing principles in consideration. The procedure names

4

are the same as for LAPACK but with a preﬁx p, e.g.
the QR-factorisation subroutine is pdgeqrf instead of
dgeqrf. For the rest of our discussion, we will use the
preﬁxed names.

ScaLAPACK asserts that matrices are block-cyclicly
distributed on a 2D processor grid. This is a generalisa-
tion of cyclic and block distribution, both of which can
be used as special cases. Considering a matrix AR×C
with R rows and C columns, we can cut it into blocks
ar×c. The last blocks in each row or column may have
fewer columns or rows. The blocks are then distributed
in a round-robin fashion amongst the processors in the
p
q processors grid, wrapping around the grid until all
blocks have been assigned.

×

(cid:100)

×

R/p
(cid:101)

For our use-case we start by considering a block distri-
1. This
bution along the rows for a processor grid of p
entails a row block size equal to the local number of rows
for each process (ar×C and br×1 with r =
). We
ﬁll these blocks by assigning each structure (i.e. several
rows of a) to a single process, thereby each atomic con-
ﬁguration is local on exactly one process. The solution
to Ac = b is invariant to swapping rows of A as long as
the corresponding entries in b are swapped accordingly.
This allows us to choose the row block size freely while
arriving at the same result irrespective of the assignment
of atomic conﬁgurations to processes. The column block
size is unrestricted, since each row is fully assigned to a
single process.

Our greedy algorithm, as described above and pre-
sented in Fig. 3, distributes atomic conﬁgurations and
rows of LT
MM such that each local An block is as equal
in size as possible. ScaLAPACK requires that all pro-
cesses use a uniform block factor for all their blocks. To
ﬁll the gaps left by the distribution a padding of zero
rows (i.e. rows ﬁlled with zeroes) is inserted into both A
and b. The distribution strategy and the block size set-
tings of ScaLAPACK should ensure that the number of
padding rows are kept to a minimum to prevent aﬄicting
memory and eﬃciency penalties.

Solving the linear system via QR decomposition with
ScaLAPACK is split into three steps. First, A is con-
verted using pdgeqrf into the upper triangular matrix
R and the Householder reﬂectors H, which occupy the
remaining lower triangular elements of A. The lat-
ter is accompanied by an array τ of weights. Reﬂec-
tors and weights are then used by pdormqr to perform
the multiplication QT b. Finally, the linear system rep-
resented by the upper triangle matrix R is solved by
pdtrtrs, utilising the backsubstitution algorithm, to give
c = R−1QT b.

We note that there is a requirement in pdtrtrs that
the row and column block sizes must be equal. Setting
the column block size (c) to the generally much larger
row block size (r) is formally possible, but this drasti-
cally increases the size of the working arrays the ScaLA-
PACK routines require, which scale with the square of the
c2 + rc). Setting instead the row
column block size (
block size (r) to the column block size (c) implies adding

∝

5

vectors – without their gradients – is not computationally
expensive and memory storage is not a concern, sparse
point selection can be performed using serial execution.
We ﬁrst run gap fit on a single process, option-
ally using OpenMP threading, to select sparse points
which are written into ﬁles, and terminating the run,
which can be achieved by the command line argument
sparsify only no fit=T. The output is converted to in-
put via a helper script for the subsequent run using MPI
processes. This step can be skipped if the sparse points
ﬁle has been provided by external means or can be reused
from an earlier calculation.

D. Peak memory usage

One of the pitfalls of running a formerly serial pro-
gram in parallel with distributed data is that duplicate
data may accumulate unnecessarily, especially if multiple
processes are run on the same node, and therefore shared
memory can be utilised. For example, it is convenient to
calculate the matrix KMM on each process because it
only depends on the sparse points, and its size does not
depend on the training set. However, each process re-
quires only a small part of the resulting matrix LMM ,
and storing multiple copies of KMM adds an unneces-
sary constant overhead to the memory requirements. To
prevent the allocation of possibly several GB memory
per process, KMM is only calculated on a single process,
then converted to LMM , and only the necessary rows are
distributed via mpi scatterv calls to independent MPI
processes.

It is also important to avoid inadvertent duplication of
data when converting between data structures used by
diﬀerent parts of the program. This can be alleviated by
performing calculations directly on the data memory as
LAPACK does. For user-deﬁned types we use pointers
to the original matrices to prevent copying of data. Fur-
ther, source data of relevant size is deallocated after ﬁnal
usage. This decreases the memory overhead per MPI
process and therefore also the peak memory requirement
of the program.

Figure 4 shows schematically how the memory usage
of gap fit run changes over time. For our program there
are two parts of the execution which may lead to peak
memory usage. The main one is after allocating the de-
scriptors, especially the derivatives x(cid:48). After the covari-
ance calculation of each descriptor, we deallocate the cor-
responding source data. This is reﬂected by the step-wise
decline of the memory.

The other peak manifests towards the end of a program
run when the matrices KMM and then A are assembled
and the linear system is subsequently solved. ScaLA-
PACK requires additional work arrays for some of its
routines depending on the block sizes of the distributed
matrices, especially the column block size.

FIG. 3: Serial (left) and distributed (right) solution of
Ac = b. The input training data is distributed across
the MPI processes P1, P2, P3 to balance load (but the
original order of rows is preserved on each). The LT

MM

matrix (yellow) in the serial implementation and its
rows are distributed in the parallel implementation.
Each local Ai is ﬁlled with zero rows (white) to adjust
to uniform matrix size, which is a multiple of the block
size.

additional zero rows for padding the local matrices to
maintain the divisibility by the block factor and thus the
assignment of the conﬁgurations to the processes. Both
of these approaches result in increased memory require-
ments and a deterioration of computational eﬃciency.

However,

it is possible to exploit the fact that our
distribution strategy relies on a single processor column
(q = 1), changing the column block size does not aﬀect
the distribution of the data. We can therefore use one col-
umn block size for the ﬁrst two calls (pdgeqrf, pdormqr)
and then change that value for the third call (pdtrtrs)
to fulﬁll its requirement without decreasing the eﬃciency
of the former calls.

Being able to control both block sizes independently
revealed that a moderate column block size of about 100
is optimal for both memory usage and eﬃciency. For such
a setting, the row block size does not have a signiﬁcant
impact on parallel eﬃciency.

C. Sparse point selection

In gap fit, the set of M sparse points are typically de-
termined as a subset of all descriptor values, although for
low-dimensional descriptors such as bond length it is con-
venient to use a uniform grid. Depending on the method,
the selection of sparse points may depend on the values
of descriptor vectors calculated from the entire training
If the descriptors are distributed, clustering
data set.
or CUR-based methods require ﬁne-tuned communica-
tion between the processes and for simplicity, we suggest
a two-step workﬂow. Since the calculation of descriptor

6

each species.

Figure 5 depicts the inverse time relation with respect
to the number of cores for diﬀerent MPI to OpenMP ra-
tios (T:C), e.g. “36:2” means that 36 MPI processes were
used per node (with 72 cores), each with two OpenMP
threads. This resembles Amdahl’s law

S(n) = t(1)/t(n) = 1/[(1

p) + p/n],

(13)

−

where the speedup S describes the relative time for a
serial run t(1) versus a parallelised one t(n) depends on
the number of cores n and the relative time p spent in
portions that beneﬁt from multiple cores. Our training
systems were too large to be run on a single core within
the maximum wall-time limit, so we used the highest time
available instead for our adjusted speedup (S∗). Because
of this, the values are only comparable within the same
training set. Note that these timings may contain some
random noise due to other calculations on the cluster (al-
beit not on the same nodes) and generally unpredictable
behavior of interconnect, network, hardware or the oper-
ating system in general. The insets show the total mem-
ory required for these calculations across all nodes, es-
timated from the average resident set size (AveRSS) as
queried from the queueing system.

A. High-Entropy alloy

The high-entropy MoNbTaVW alloy (HEA) training
set [26] consists of 2329 conﬁgurations – each contain-
ing an uneven number of atoms – with 2329 total en-
ergies, 383 739 forces, and 10 110 virials for a total of
N = 396 178 target properties. With 20 sparse points per
two-body descriptor (15) and 4000 per SOAP descriptor
(5) the total number of sparse points is M = 20 300.
Thus, A consists of nA = 8 454 503 400 elements and oc-
cupies about 67.6 GB.

Looking at Fig. 5, using only a few nodes, it is advan-
tageous to use as many cores for MPI as possible to re-
duce the total runtime. For 72:1 ratio this trend changes
somewhere between 864 and 1152 cores where the time
stays constant with increasing cores. The same happens
for 36:2 ratio between 1728 and 2304 cores, and for 24:3
between 2304 and 3456 cores. This behaviour stems from
the choice of our implementation, which splits the train-
ing set along the structures, which cannot be done arbi-
trarily for a ﬁnite set. For these 2329 conﬁgurations, the
limit is at about two structures per MPI process.

The reference time for this set is 24 449 s (6.8 hours),
obtained from the single (high-memory) node calculation
with 18:4 split of cores. The 72:1 run is about twice
as fast (1.85) for a single node. Using six nodes (432
cores) adds another factor of 4.27 for a total of 7.91.
From there the beneﬁts dwindle from factor 1.16 (10.23)
despite doubling (twelve nodes) to the constant speedup
around 16 nodes of ca. 12.6. The 36:2 conﬁguration
starts slower at 1.57 for one node, achieves almost the
same speedup as 72:1 for eight nodes (8.38) and is even

FIG. 4: Schematic memory usage during gap fit run
over time. Descriptors (5 shown) x and their derivatives
x(cid:48) constitute the majority of the ﬁrst peak. The
memory associated with them is released after each
processing, leading to a step-wise decline. Matrices
KMM and A and working arrays for solving the latter
make up the second peak, which can be more shallow
than depicted here.

IV. PRACTICAL EXAMPLES

Initial proof-of-principle ﬁtting runs of a silicon
dataset [23] showed that the ﬁtting weights from an MPI
run are comparable to those from serial runs when us-
ing the same representative set. The diﬀerence can be
attributed to numerical uncertainties noting that even
two runs initiated with identical parameters may dif-
fer to the same magnitude. The diﬀerence can be at-
tributed to the fact that the order of numerical opera-
tions is non-deterministic and the ﬂoating point arith-
metic is neither associative nor distributive, leading to
small diﬀerences in covariance matrices in diﬀerent exe-
cutions. Ill-conditioning of matrix A ampliﬁes the noise
due to the diﬀerent order of operations, leading to only a
few comparable signiﬁcant digits in the resulting weights.
We have therefore tested the accuracy of the predictions
with the resulting potential models using the ∆ metric
suggested to compare DFT packages [24]. We found that
equivalent GAP models diﬀer only up to 1 µeV, indicat-
ing excellent reproducibility despite the diﬀerences in the
weights.

We then applied the implementation with varying pro-
portions of MPI processes and OpenMP threads on two
example training sets, consisting of an High-Entropy Al-
loy (HEA) and silicon carbide (SiC) datasets.

These calculations were performed on the Raven clus-
ter of the Max Planck Computing and Data Facility.
Each node has 72 cores and either 256 GB or 512 GB of
RAM. For single-node calculations we used high-memory
nodes with 2048 GB of RAM.

In both cases we combined two-body descriptors with
Smooth Overlap of Atomic Positions (SOAP) descrip-
tors [25]. We assign separate two-body descriptors for
each pair of species, and separate SOAP descriptors for

OverheadDescriptorsKMMKMNA,...TimeMemory7

FIG. 5: Adjusted speedup (reference time / current time) and total memory requirements (inset) of gap fit vs
cores (72 per node) with diﬀerent splits between MPI tasks per node vs OpenMP threads per task. Fitting times are
shown for the HEA model (left, 396 178 target properties) and the SiC model (right, 2 482 085 target properties),
with reference times 24 449 s and 31 626 s, respectively. Both models used 20 300 representative (sparse) points.

faster for twelve (11.57 vs 10.23). It achieves a speedup
of 18.59 at its best but decreases to 17.53 for 64 nodes.
The same happens for 24:3, from a maximum of 20.55
down to 19.72. The 18:4 conﬁguration achieves 20.88 at
64 nodes.

This eﬃciency comes at the cost of a memory over-
head, which increases approximately linearly with the
number of cores. The higher the portion of MPI usage,
the steeper this overhead is. In fact, the graphs coincide
if they are plotted against the number of MPI processes
(not shown):
in that case the slope ranges between 1.4
and 1.8 GB per MPI process. For 72:1 this results in
2.60 TB on 16 nodes and 9.42 TB on 64. The former is
comparable to the 2.66 GB 18:4 uses on 64 nodes, since
both apply 1152 MPI processes.

B. Silicon carbide

The 4865 silicon carbide (SiC) systems of this training
set contain 4865 energies, 2 448 030 forces, 29 190 virials
for a total of N = 2 482 085 target properties. With 100
sparse points per two-body descriptor (3) and 10 000 per
SOAP descriptor (2) the total number of sparse points
is M = 20 300. Thus, A consist of nA = 50 798 415 500
elements and occupies 406.4 GB.

Due to the much larger training set, not all node con-
ﬁgurations from the HEA set were viable, especially for
lower node numbers. The reference time is 31 626 s for
a 24:3 run on a single node. The trend that more MPI
processes are more eﬃcient holds up here as well but the
processes are not saturated as rapidly as in the case of
the HEA system. This eﬀect may start between 3456
and 4608 cores for the full MPI run (72:1), which is later
than in the HEA set even taking the structure numbers
into account because of the proportionally lower number
of small structures in this larger set of only two species.

The memory overhead per MPI process is between 2.1
and 2.2 GB.

V. CONCLUSION AND OUTLOOK

The recent addition of MPI parallelisation to our pro-
gram gap fit by using the ScaLAPACK library makes
it possible to split the training data evenly into batches
to be processed independently up to the solving of the
linear system.
It alleviates the need for high-memory
nodes so commodity HPC nodes may be used for arbi-
trarily large ﬁtting problems; while computation time has
been signiﬁcantly reduced the due to the pleasingly par-
allel algorithm. Thus larger training sets do not impede
the computation and more sparse points can be used,
increasing the accuracy of the model.

We showed the time scaling and memory requirements
for varying proportions of MPI processes vs OpenMP
threads in two example training sets, consisting of an
high-entropy alloy (HEA) and silicon carbide (SiC). It
is generally advisable to use most of the processors for
MPI in terms of computational eﬃciency, so even on a
single node beneﬁts from this new feature. It is especially
eﬀective for larger training sets while the sparse points
are covered by the OpenMP threads. One should keep
the total number of MPI processes below some fraction
of the total number of structures, e.g. 0.5 so that an even
distribution is still possible.

The memory overhead due to the parallelisation has
been reduced but is still signiﬁcant. Depending on the
available memory resources, a higher share of OpenMP
threads is preferable. We are conﬁdent that an even
smaller memory footprint will be achieved in a further
development.

The highest impact on both MPI and non-MPI mem-
ory requirements would be to fully restructure the de-

8

scriptor processing loop so that each descriptor is pro-
cessed fully before the next one.

In practical tests we have seen that the parallel
gap fit code can decrease the time required to ﬁt po-
tentials from days to minutes. We anticipate this will be
an important step to enable potential ﬁtting to be em-
bedded within other higher-level workﬂows such as active
learning [13, 14], committee models [27] as well as en-
abling model hyperparameters to be tuned or optimised,
known to be important for improved uncertainty quan-
tiﬁcation [14].

ACKNOWLEDGMENTS

We thank Harry Tunstall for providing the SiC dataset
and early testing as well as G´abor Cs´anyi and Miguel
Caro for useful discussions. This work was ﬁnancially

supported by the NOMAD Centre of Excellence (Euro-
pean Commission grant agreement ID 951786) and the
Leverhulme Trust Research Project Grant (RPG-2017-
191). ABP acknowledges support from the CASTEP-
USER project, funded by the Engineering and Phys-
ical Sciences Research Council under the grant agree-
ment EP/W030438/1. We acknowledge computational
resources provided by the Max Planck Computing and
Data Facility provided through the NOMAD CoE, the
Scientiﬁc Computing Research Technology Platform of
the University of Warwick, the EPSRC-funded HPC
Midlands+ consortium (EP/T022108/1) and ARCHER2
(https://www.archer2.ac.uk/) via the UK Car-Parinello
consortium (EP/P022065/1). We thank the technical
staﬀ at each of these HPC centres for their support. For
the purpose of Open Access, the author has applied a
CC-BY public copyright licence to any Author Accepted
Manuscript (AAM) version arising from this submission.

[1] J. Behler and M. Parrinello, Phys. Rev. Lett. 98, 146401

QUIP.

(2007).

[19] The

extended XYZ format, https://github.com/

[2] A. V. Shapeev, Multiscale Model. Simul. 14, 1153 (2016),

libAtoms/extxyz.

1512.06054.

[3] A. P. Thompson, L. P. Swiler, C. R. Trott, S. M. Foiles,
and G. J. Tucker, J. Comput. Phys. 285, 316 (2015).

[4] R. Drautz, Phys. Rev. B 99, 014104 (2019).
[5] N. Artrith and A. Urban, Comput. Mater. Sci. 114, 135

(2016).

[20] M. W. Mahoney and P. Drineas, Proc. Natl. Acad. Sci.

U.S.A. 106, 697 (2009).

[21] P. Rowe, V. L. Deringer, P. Gasparotto, G. Cs´anyi, and

A. Michaelides, J. Chem. Phys. , 034702 (2020).

[22] G. Cs´anyi, private communication (2022).
[23] A. P. Bart´ok, J. Kermode, N. Bernstein, and G. Cs´anyi,

[6] A. P. Bart´ok, M. C. Payne, R. Kondor, and G. Cs´anyi,

Phys. Rev. X 8, 041048 (2018).

Phys. Rev. Lett. 104, 136403 (2010).

[7] K. T. Sch¨utt, H. E. Sauceda, P. J. Kindermans,
A. Tkatchenko, and K. R. M¨uller, J. Chem. Phys. 148,
241722 (2018).

[8] V. L. Deringer, N. Bernstein, G. Cs´anyi, C. Mahmoud,
M. Ceriotti, M. Wilson, D. A. Drabold, and S. R. Elliott,
Nature 589, 59 (2020).

[9] The ﬁtSNAP repository, https://https://github.com/

FitSNAP/FitSNAP.

[10] O. A. v. Lilienfeld, R. Ramakrishnan, M. Rupp, and
A. Knoll, Int. J. Quantum Chem. 115, 1084 (2015).
[11] A. M. Goryaeva, J.-B. Maillet, and M.-C. Marinica, Com-

put. Mater. Sci. 166, 200 (2019).

[12] V. L. Deringer, A. P. Bart´ok, N. Bernstein, D. M.
Wilkins, M. Ceriotti, and G. Cs´anyi, Chem. Rev. 121,
10073 (2021).

[13] Z. Li, J. R. Kermode, and A. De Vita, Phys. Rev. Lett.

114, 096405 (2015).

[14] J. Vandermause, S. B. Torrisi, S. Batzner, Y. Xie, L. Sun,
A. M. Kolpak, and B. Kozinsky, npj Computational Ma-
terials 6, 1 (2020).

[15] J. Quinonero-Candela and C. Rasmussen, Journal of Ma-

chine Learning Research 6, 1939 (2005).

[16] E. Snelson and Z. Ghahramani, Adv. Neural Inf. Process.

Syst. 18, 1257 (2005).

[17] L. Foster, A. Waagen, N. Aijaz, M. Hurley, A. Luis,
J. Rinsky, C. Satyavolu, M. J. Way, P. Gazis, and A. Sri-
vastava, Journal of Machine Learning Research 10, 857
(2009).

[18] The QUIP repository, https://github.com/libAtoms/

[24] K. Lejaeghere, G. Bihlmayer, T. Bjorkman, P. Blaha,
S. Blugel, V. Blum, D. Caliste, I. E. Castelli, S. J. Clark,
A. D. Corso, S. D. Gironcoli, T. Deutsch, J. K. Dewhurst,
I. D. Marco, C. Draxl, M. D. ak, O. Eriksson, J. A.
Flores-Livas, K. F. Garrity, L. Genovese, P. Giannozzi,
M. Giantomassi, S. Goedecker, X. Gonze, O. Granas,
E. K. U. Gross, A. Gulans, F. Gygi, D. R. Hamann, P. J.
Hasnip, N. A. W. Holzwarth, D. I. an, D. B. Jochym,
F. Jollet, D. Jones, G. Kresse, K. Koepernik, E. Ku-
cukbenli, Y. O. Kvashnin, I. L. M. Locht, S. Lubeck,
M. Marsman, N. Marzari, U. Nitzsche, L. Nordstrom,
T. Ozaki, L. Paulatto, C. J. Pickard, W. Poelmans,
M. I. J. Probert, K. Refson, M. Richter, G. M. Rignanese,
S. Saha, M. Scheﬄer, M. Schlipf, K. Schwarz, S. Sharma,
F. Tavazza, P. Thunstrom, A. Tkatchenko, M. Torrent,
D. Vanderbilt, M. J. v. Setten, V. V. Speybroeck, J. M.
Wills, J. R. Yates, G. X. Zhang, and S. Cottenier, Science
351, aad3000 (2016).

[25] A. P. Bart´ok, R. Kondor, and G. Cs´anyi, Phys. Rev. B

87, 184115 (2013).

[26] J. Byggm¨astar, K. Nordlund, and F. Djurabekova, Phys.

Rev. B 104, 104101 (2021).

[27] G. Imbalzano, Y. Zhuang, V. Kapil, K. Rossi, E. A. En-
gel, F. Grasselli, and M. Ceriotti, J. Chem. Phys. 154,
074102 (2021).

9

Appendix A: gap fit options and timings

The following input is split into separate entries here for
readability. It needs to be expanded for use.

For the HEA set:

0.005
energy parameter name=dft energy

at file=data.xyz gap=GAP default sigma=
{
0.2 0.05 0.0
}
force parameter name=dft forces
sparse jitter=1.0E-8 gp file=gp.xml
rnd seed=999

GAP=

{{

SUBGAP1
}

SUBGAP2
:
{

}}

SUBGAP1=distance 2b cutoff=4.5 delta=10.0
covariance type=ard se theta uniform=0.75
n sparse=20 sparse method=uniform

SUBGAP2=soap l max=4 n max=8 atom sigma=0.5
zeta=2 cutoff=3.5 cutoff transition width=0.5
central weight=1.0 n sparse=4000 delta=0.1
covariance type=dot product
sparse method=cur points

For the SiC set: rnd seed=84616238, n sparse=100
and n sparse=10000, respectively.

10

n

36

18

T
72
24
1.0 1.00 1.22 1.57 1.85
6.0 4.42 5.78 6.79 7.91
8.0 5.96 6.90 8.38 8.80
12.0 8.23 9.47 11.57 10.23
16.0 10.10 11.53 12.78 12.58
24.0 13.65 14.84 15.24 12.64
32.0 16.02 16.88 18.10 12.54
48.0 18.07 20.55 18.59 12.87
64.0 20.88 19.72 17.53 12.77

TABLE II: Pivot table of the adjusted speedups for the
HEA training set.

n T C

t

S∗ Mem

1 1.04
1 18 4 24449
1 24 3 19973 1.22 1.05
1 36 2 15569 1.57 1.06
1 72 1 13245 1.85 1.11
6 18 4 5526 4.42 1.17
6 24 3 4233 5.78 1.23
6 36 2 3601 6.79 1.34
6 72 1 3092 7.91 1.58
8 18 4 4101 5.96 1.23
8 24 3 3542 6.90 1.30
8 36 2 2916 8.38 1.40
8 72 1 2778 8.80 1.75
12 18 4 2972 8.23 1.30
12 24 3 2581 9.47 1.40
12 36 2 2114 11.57 1.58
12 72 1 2391 10.23 2.17
16 18 4 2421 10.10 1.40
16 24 3 2120 11.53 1.52
16 36 2 1913 12.78 1.75
16 72 1 1944 12.58 2.60
24 18 4 1791 13.65 1.58
24 24 3 1647 14.84 1.76
24 36 2 1604 15.24 2.20
24 72 1 1934 12.64 3.67
32 18 4 1526 16.02 1.76
32 24 3 1448 16.88 2.03
32 36 2 1351 18.10 2.63
32 72 1 1950 12.54 4.80
48 18 4 1353 18.07 2.17
48 24 3 1190 20.55 2.60
48 36 2 1315 18.59 3.68
48 72 1 1900 12.87 7.13
64 18 4 1171 20.88 2.63
64 24 3 1240 19.72 3.31
64 36 2 1395 17.53 4.87
64 72 1 1915 12.77 9.42

TABLE I: Time t (in s), adjusted speedup S∗, and
estimated memory (in TB) in the HEA training set. n:
nodes, T: MPI processes per node, C: OpenMP threads
per T.

11

T

n

18

72
1.56

24
1.00

36
1.33
3.47
6.63
8.36
12.21

7.38

7.66

1
3
6
8
12
16 10.58 11.81 13.33 16.12
24 15.57 15.62 19.67 18.70
32 16.72 21.62 22.97 24.59
48 25.63 26.94 27.31 29.89
35.02 33.01
64

TABLE IV: Pivot table of the adjusted speedups for the
SiC training set. n: nodes, T: MPI processes per node.

n T C

t

S∗ Mem

1 24 3 31626 1.00 1.05
1 36 2 23854 1.33 1.08
1 72 1 20220 1.56 1.15
3 36 2 9121 3.47 1.25
6 24 3 4286 7.38 1.23
6 36 2 4768 6.63 1.49
8 18 4 4129 7.66 1.23
8 36 2 3784 8.36 1.62
12 36 2 2590 12.21 1.92
16 18 4 2988 10.58 1.62
16 24 3 2677 11.81 1.82
16 36 2 2373 13.33 2.22
16 72 1 1962 16.12 3.42
24 18 4 2031 15.57 1.92
24 24 3 2025 15.62 2.22
24 36 2 1608 19.67 2.82
24 72 1 1691 18.70 4.62
32 18 4 1891 16.72 2.22
32 24 3 1463 21.62 2.62
32 36 2 1377 22.97 3.43
32 72 1 1286 24.59 5.88
48 18 4 1234 25.63 2.82
48 24 3 1174 26.94 3.42
48 36 2 1158 27.31 4.62
48 72 1 1058 29.89 8.47
903 35.02 5.86
64 36 2
958 33.01 10.78
64 72 1

TABLE III: Time t (in s), adjusted speedup S∗, and
estimated memory (in TB) in the SiC training set. N:
nodes, T: MPI processes per node, C: OpenMP threads
per T.

