2
2
0
2

y
a
M
0
3

]

G
L
.
s
c
[

4
v
2
0
7
9
0
.
5
0
2
2
:
v
i
X
r
a

Parallel and Distributed Graph Neural Networks:
An In-Depth Concurrency Analysis

1

Maciej Besta and Torsten Hoeﬂer
Department of Computer Science, ETH Zurich

Abstract—Graph neural networks (GNNs) are among the most powerful tools in deep learning. They routinely solve complex problems
on unstructured networks, such as node classiﬁcation, graph classiﬁcation, or link prediction, with high accuracy. However, both
inference and training of GNNs are complex, and they uniquely combine the features of irregular graph processing with dense and
regular computations. This complexity makes it very challenging to execute GNNs efﬁciently on modern massively parallel
architectures. To alleviate this, we ﬁrst design a taxonomy of parallelism in GNNs, considering data and model parallelism, and different
forms of pipelining. Then, we use this taxonomy to investigate the amount of parallelism in numerous GNN models, GNN-driven
machine learning tasks, software frameworks, or hardware accelerators. We use the work-depth model, and we also assess
communication volume and synchronization. We speciﬁcally focus on the sparsity/density of the associated tensors, in order to
understand how to effectively apply techniques such as vectorization. We also formally analyze GNN pipelining, and we generalize the
established Message-Passing class of GNN models to cover arbitrary pipeline depths, facilitating future optimizations. Finally, we
investigate different forms of asynchronicity, navigating the path for future asynchronous parallel GNN pipelines. The outcomes of our
analysis are synthesized in a set of insights that help to maximize GNN performance, and a comprehensive list of challenges and
opportunities for further research into efﬁcient GNN computations. Our work will help to advance the design of future GNNs.

Index Terms—Parallel Graph Neural Networks, Distributed Graph Neural Networks, Parallel Graph Convolution Networks, Distributed
Graph Convolution Networks, Parallel Graph Attention Networks, Distributed Graph Attention Networks, Parallel Message Passing
Neural Networks, Distributed Message Passing Neural Networks, Asynchronous Graph Neural Networks.

(cid:70)

1 INTRODUCTION

Graph neural networks (GNNs) are taking over the world
of machine learning (ML) by storm [58], [223]. They have
been used in a plethora of complex problems such as node
classiﬁcation, graph classiﬁcation, or edge prediction [77],
[110]. Example areas of application are social sciences (e.g.,
studying human interactions), bioinformatics (e.g., analyz-
ing protein structures), chemistry (e.g., designing com-
pounds), medicine (e.g., drug discovery), cybersecurity (e.g.,
identifying intruder machines), entertainment services (e.g.,
predicting movie popularity),
linguistics (e.g., modeling
relationships between words), transportation (e.g., ﬁnding
efﬁcient routes), and others [23], [49], [57], [58], [66], [89],
[100], [108], [118], [223], [244], [251]. Some recent celebrated
success stories are cost-effective and fast placement of high-
performance chips [157], simulating complex physics [170],
[177], guiding mathematical discoveries [70], or signiﬁcantly
improving the accuracy of protein folding prediction [120].
GNNs uniquely generalize both traditional deep learn-
ing [15], [95], [138] and graph processing [96], [152], [176]. Still,
contrarily to the former, they do not operate on regular grids
and highly structured data (such as, e.g., image processing);
instead, the data in question is highly unstructured, irreg-
ular, and the resulting computations are data-driven and
lacking straightforward spatial or temporal locality [152].
Moreover, contrarily to the latter, vertices and/or edges are
associated with complex data and processing. For example,
in many GNN models, each vertex i has an assigned k-
dimensional feature vector, and each such vector is combined
with the vectors of i’s neighbors; this process is repeated
iteratively. Thus, while the overall style of such GNN com-
putations resembles label propagation algorithms such as

PageRank [33], [167], it comes with additional complexity
due to the high dimensionality of the vertex features.

Yet, this is only how the simplest GNN models, such
as basic Graph Convolution Networks (GCN) [128], work.
In many, if not most, GNN models, high-dimensional data
may also be attached to every edge, and complex updates
to the edge data take place at every iteration. For exam-
ple, in the Graph Attention Network (GAT) model [202],
to compute the scalar weight of a single edge (i, j), one
must ﬁrst concatenate linear transformations of the feature
vectors of both vertices i and j, and then construct a dot
product of such a resulting vector with a trained parameter
vector. Other models come with even more complexity. For
example, in Gated Graph ConvNet (G-GCN) [47] model, the
edge weight may be a multidimensional vector.

At the same time, parallel and distributed processing have
essentially become synonyms for computational efﬁciency.
Virtually each modern computing architecture is parallel:
cores form a socket while sockets form a non-uniform mem-
ory access (NUMA) compute node. Nodes may be further
clustered into blades, chassis, and racks [25], [87], [180].
Numerous memory banks enable data distribution. All these
parts of the architectural hierarchy run in parallel. Even
a single sequential core offers parallelism in the form of
vectorization, pipelining, or instruction-level parallelism (ILP).
On top of that, such architectures are often heterogeneous:
Processing units can be CPUs or GPUs, Field Programmable
Gate Arrays (FPGAs), or others. How to harness all these rich
features to achieve more performance in GNN workloads?

To help answer this question, we systematically analyze
different aspects of GNNs, focusing on the amount of paral-
lelism and distribution in these aspects. We use fundamental
theoretical parallel computing machinery, for example the

 
 
 
 
 
 
Structure of graph inputs

2

A graph; V and E are sets of vertices and edges.
Numbers of vertices and edges in G; |V | = n, |E| = m.

G = (V, E)
n, m
N (i), N +(i), (cid:98)N (i) Neighbors of i, in-neighbors of i, and (cid:98)N (i) = N (i) ∪ {i}.
di, d
A, D ∈ Rn×n
(cid:101)A, (cid:101)D
(cid:98)A, A

The degree of a vertex i and the maximum degree in a graph.
The graph adjacency and the degree matrices.
A and D matrices with self-loops ( (cid:101)A = A + I, (cid:101)D = D + I).
2 (cid:101)A (cid:101)D− 1
Normalization: (cid:98)A = (cid:101)D− 1

2 and A = D−1A [223].

Structure of GNN computations

The number of GNN layers and input features.
Input (vertex) feature matrix.

L, k
X ∈ Rn×k
Y, H(l) ∈ Rn×O(k) Output (vertex) feature matrix, hidden (vertex) feature matrix.
xi, yi, h(l)
Input, output, and hidden feature vector of a vertex i (layer l).
W(l) ∈ RO(k)×O(k) A parameter matrix in layer l.
σ(·)
×, (cid:12)

Element-wise activation and/or normalization.
Matrix multiplication and element-wise multiplication.

i ∈ Rn

TABLE 1: The most important symbols used in the paper.

Work-Depth model [42], to reveal architecture independent
insights. We put special focus on the linear algebra formu-
lation of computations in GNNs, and we investigate the
sparsity and density of the associated tensors. This offers
further insights into performance-critical features of GNN
computations, and facilitates applying parallelization mech-
anisms such as vectorization. In general, our investigation will
help to develop more efﬁcient GNN computations.

For a systematic analysis, we propose an in-depth taxonomy
of parallelism in GNNs. The taxonomy identiﬁes fundamental
forms of parallelism in GNNs. While some of them have
direct equivalents in traditional deep learning, we also
illustrate others that are speciﬁc to GNNs.

To ensure wide applicability of our analysis, we cover
a large number of different aspects of the GNN land-
scape. Among others, we consider different categories of
GNN models (e.g., spatial, spectral, convolution, atten-
tional, message passing), a large selection of GNN mod-
els (e.g., GCN [128], SGC [219], GAT [202], G-GCN [47]),
parts of GNN computations (e.g., inference, training), build-
ing blocks of GNNs (e.g., layers, operators/kernels), pro-
gramming paradigms (e.g., SAGA-NN [153], GReTA [126]),
execution schemes behind GNNs (e.g., reduce, activate,
different tensor operations), GNN frameworks (e.g., Neu-
Graph [153]), GNN accelerators (e.g., HyGCN [228]) GNN-
driven ML tasks (e.g., node classiﬁcation, edge prediction),
mini-batching vs. full-batch training, different forms of sam-
pling, and asynchronous GNN pipelines.

We ﬁnalize our work with general insights into parallel
and distributed GNNs, and a set of research challenges and
opportunities. Thus, our work can serve as a guide when
developing parallel and distributed solutions for GNNs
executing on modern architectures, and for choosing the
next research direction in the GNN landscape.

1.1 Complementary Analyses

We discuss related works on the theory and applications
of GNNs. There exist general GNN surveys [48], [49],
[58], [100], [223], [241], [244], [251], works on theoretical
aspects (spatial–spectral dichotomy [11], [63], the expres-
sive power of GNNs [178], or heterogeneous graphs [224],
[229]), analyzes of GNNs for speciﬁc applications (knowl-

edge graph completion [5], trafﬁc forecasting [119], [195],
symbolic computing [137], recommender systems [220], text
classiﬁcation [115], or action recognition [3]), explainability
of GNNs [232], and on software (SW) and hardware (HW)
accelerators and SW/HW co-design [1]. We complement
these works as we focus on parallelism and distribution of
GNN workloads.

1.2 Scope of this Work & Related Domains

We focus on GNNs, but we also cover parts of the associated
domains. In the graph embeddings area, one develops
methods for ﬁnding low-dimensional representations of el-
ements of graphs, most often vertices [68], [69], [210], [211].
As such, GNNs can be seen as a part of this area, because
one can use a GNN to construct an embedding [223]. How-
ever, we exclude non-GNN related methods for constructing
embeddings, such as schemes based on random walks [97],
[168] or graph kernel designs [46], [131], [203].

2 GRAPH NEURAL NETWORKS: OVERVIEW

We overview GNNs. Table 1 explains the most important
notation. We ﬁrst summarize a GNN computation and
GNN-driven downstream ML tasks (§ 2.1). We then dis-
cuss different parts of a GNN computation in more detail,
providing both the basic knowledge and general oppor-
tunities for parallelism and distribution. This includes the
input GNN datasets (§ 2.2), the mathematical theory and
formulations for GNN models that form the core of GNN
computations (§ 2.3), GNN inference vs. GNN training
(§ 2.4), and the programmability aspects (§ 2.5). We ﬁnish
with a taxonomy of parallelism in GNNs (§ 2.6) and parallel
& distributed theory used for formal analyses (§ 2.7).

2.1 GNN Computation: A High-Level Summary

We overview a GNN computation in Figure 1. The input is
a graph dataset, which can be a single graph (usually a large
one, e.g., a brain network), or several graphs (usually many
small ones, e.g., chemical molecules). The input usually
comes with input feature vectors that encode the semantics
of a given task. For example, if the input nodes and edges
model – respectively – papers and citations between these
papers, then each node could come with an input feature
vertex being a one-hot bag-of-words encoding, specifying

3

Fig. 1: (§ 2.1) Overview of general GNN computation. Input
comprises the graph structure and the accompanying feature
vectors (assigned to vertices/edges). The input is processed
using a speciﬁc GNN model (training or inference). Output
feature vectors are used in various downstream ML tasks.

the presence of words in the abstract of a given publication.
Then, a GNN model – underlying the training and inference
process – uses the graph structure and the input feature
vectors to generate the output feature vectors. In this process,
intermediate hidden latent vectors are often created. Note
that hidden features may be updated iteratively more than
once (we refer to a single such iteration, that updates all
the hidden features, as a GNN layer). The output feature
vectors are then used for the downstream ML tasks such as
node classiﬁcation or graph classiﬁcation.

A single GNN layer is summarized in Figure 2. In
general, one ﬁrst applies a certain graph-related operation
to the features. For example, in the GCN model [128], one
aggregates the features of neighbors of each vertex v into
the feature vector of v using summation. Then, a selected
operation related to traditional neural networks is applied to
the feature vectors. A common choice is an MLP or a plain
linear projection. Finally, one often uses some form of non-
linear activation (e.g., ReLU [128]) and/or normalization.

Fig. 2: (§ 2.1) Overview of a single GNN layer. The input
set of samples (e.g., vertices or graphs) is processed with a
graph-related operation such as graph convolution, followed
by a neural network related operation such as an MLP, and
ﬁnally (optionally) by a non-linearity such as ReLU, possibly
combined with some normalization.

One key difference between GNNs and traditional deep
learning are possible dependencies between input data samples
which make the parallelization of GNNs much more chal-
lenging. We show GNN data samples in Figure 3. A single
sample can be a node (a vertex), an edge (a link), a subgraph,
or a graph itself. One may aim to classify samples (assign
labels from a discrete set) or conduct regression (assign
continuous values to samples). Both vertices and edges have
inter-dependencies: vertices are connected with edges while
edges share common vertices. The seminal work by Kipf
and Welling [128] focuses on node classiﬁcation. Here, one
is given a single graph as input, data samples are single
vertices, and the goal is to classify all unlabeled vertices.

Fig. 3: (§ 2.1) Overview of GNN samples. GNN downstream
ML tasks aim at classiﬁcation or regression of vertices, edges, or
graphs. While both vertex and edge samples virtually always
have inter-sample dependencies, graphs may be both depen-
dent and independent.

Graphs – when used as basic data samples – are usu-
ally independent [225], [231] (cf. Figure 3, 3rd column).
An example use case is classifying chemical molecules.
This setting resembles traditional deep learning (e.g., image
recognition), where samples (single pictures) also have no
explicit dependencies. Note that, as chemical molecules may
differ in sizes, load balancing issues may arise. This also
has analogies in traditional deep learning, e.g., sampled
videos also may have varying sizes [143]. However, graph
classiﬁcation may also feature graph samples with inter-
dependencies (cf. Figure 3, 4th column). This is useful when
studying, for example, relations between network commu-
nities [142].

2.2 Input Datasets & Output Structures in GNNs

A GNN computation starts with the input graph G, mod-
eled as a tuple (V, E); V is a set of vertices and E ⊆ V × V
is a set of edges; |V | = n and |E| = m. N (v) denotes
the set of vertices adjacent to vertex (node) v, dv is v’s
degree, and d is the maximum degree in G (all symbols
are listed in Table 1). The adjacency matrix (AM) of a graph
is A ∈ {0, 1}n×n. A determines the connectivity of vertices:
A(i, j) = 1 ⇔ (i, j) ∈ E. The input, output, and hidden
feature vector of a vertex i are denoted with, respectively,
xi, yi, hi. We have xi ∈ Rk and yi, hi ∈ RO(k), where k is
the number of input features. These vectors can be grouped
in matrices, denoted respectively as X, Y, H ∈ Rn×k. If
needed, we use the iteration index (l) to denote the latent
features in an iteration (GNN layer) l (h(l)
, H(l)). Some-
i
times, for clarity of equations, we omit the index (l).

2.3 GNN Mathematical Models

A GNN model deﬁnes a mathematical transformation that
takes as input (1) the graph structure A and (2) the input
features X, and generates the output feature matrix Y.
Unless speciﬁed otherwise, X models vertex features. The

InputGNN modelGraph structureInput features+node(vertex)edge(link)Each vertex and o�en alsoevery edge is associatedwith a feature vectorApply one ormore GNN layersGNN drivenDownstreamML tasksClassiﬁca�onNodeEdgeGraph...InferenceTrainingorApply one ormore GNN layers...orRegressionNodeEdgeGraphReLUInputsamplesGraph-relatedopera�on, usuallysparse (e.g., graphconvolu�on)Neural networkrelated opera�on,usually dense(e.g., MLP)NextGNNlayerNormaliza�on,non-linearity(op�onal)A GNN LayerInput samplesVer�cesEdgesDependentIndependentGraphsGraphsVer�ces(dependent)AfeaturevectorA sampleGraphs(independent)Edges(dependent)Subgraphs/ graphs(dependent)Dependenciesbetween samplesusedforGraph classiﬁca�onGraph regressionNode classiﬁca�onNode regressionEdge classiﬁca�onEdge regressionusedforusedforexact way of constructing Y based on A and X is an
area of intense research. Here, hundreds of different GNN
models have been developed [49], [58], [100], [223], [241],
[244], [251]. We now discuss different categories of GNN
models, see Figure 4 for a summary. Importantly for parallel
and distributed execution, one can formulate most GNN models
using either the local formulation (LC) based on functions
operating on single edges or vertices, or the global formulation
(GL), based on operations on matrices grouping all vertex- and
edge-related vectors.

2.3.1 Local (LC) GNN Formulations

In many GNN models, the latent feature vector hi of a
given node i is obtained by applying a permutation invariant
aggregator function (cid:76), such as sum or max, over the feature
vectors of the neighbors N (i) of i. Moreover, the feature vec-
tor of each neighbor of i may additionally be transformed
by a function ψ. Finally, the outcome of (cid:76) may be also
transformed with another function φ. The sequence of these
three transformations forms one GNN layer. We denote such
a GNN model formulation (based on (cid:76), ψ, φ) as local (LC).
Formally, the equation specifying the feature vector h(l+1)
of a vertex i in the next GNN layer l + 1 is as follows:

i

h(l+1)
i

= φ


h(l)

i

,

(cid:77)

(cid:16)
h(l)
i

, h(l)
j

ψ

j∈N (i)

(cid:17)





(1)

Depending on the details of ψ, one can further dis-
tinguish three GNN classes [48]: Convolutional GNNs (C-
GNNs), Attentional GNNs (A-GNNs), and Message-Passing
GNNs (MP-GNNs). In short, in these three classes of models,
ψ is – respectively – a ﬁxed scalar coefﬁcient (C-GNNs),
a learnable function that returns a scalar coefﬁcient (A-
GNNs), or a learnable function that returns a vector coef-
ﬁcient (MP-GNNs).

As an example, consider the seminal GCN model by
Kipf and Welling [128]. Here, (cid:76) is a sum over N (i) ∪
{i} ≡ (cid:98)N (i), ψ acts on each neighbor j’s feature vector
by multiplying it with a scalar 1/(cid:112)didj, and φ is a lin-
ear projection with a trainable parameter matrix W fol-
lowed by ReLU . Thus, the LC formulation is given by

Fig. 4: (§ 2.3) Categories of GNN models. We propose a clas-
siﬁcation into local and global formulations of GNN models.
Red/green refer to formulation details in Figure 5.

4

(cid:19)(cid:19)

(cid:18)

(cid:18)

(cid:80)

h(l)
j

h(l+1)
i

1√

W(l) ×

= ReLU

didj

j∈ (cid:98)N (i)

. Note that
each iteration may have different projection matrices W(l).
There are many ways in which one can parallelize GNNs
in the LC formulation. Here, the ﬁrst-class citizens are “ﬁne-
grained” functions being evaluated for vertices and edges.
Thus, one could execute these functions in parallel over
different vertices, edges, and graphs, parallelize a single
function over the feature dimension or over the graph struc-
ture, pipeline a sequence of functions within a GNN layer or
across GNN layers, or fuse parallel execution of functions.
We discuss all these aspects in the following sections.

2.3.2 Global (GL) GNN Formulations

Many GNN models can also be formulated using operations
on matrices X, H, A, and others. We will refer to this
approach as the global (GL) linear algebraic approach.

For example, the GL formulation of the GCN model is
H(l+1) = ReLU ( (cid:98)AH(l)W(l)). (cid:98)A is the normalized adjacency
matrix with self loops (cid:101)A (cf. Table 1): (cid:98)A = (cid:101)D− 1
2 . This
normalization incorporates coefﬁcients 1/(cid:112)didj shown in
the LC formulation above (the original GCN paper gives
more details about normalization).

2 (cid:101)A (cid:101)D− 1

Many GL models use higher powers of A (or its nor-
malizations). Based on this criterion, GL models can be
linear (L) (if only the 1st power of A is used), polynomial
(P) (if a polynomial power is used), and rational (R) (if a
rational power is used) [63]. This aspect impacts how to
best parallelize a given model, as we illustrate in Section 4.
For example, the GCN model [128] is linear.

Importantly, GNN computations involve both sparse
and dense matrices. As the performance patterns of oper-
ations on such matrices differ vastly [123], [134]–[136], this
comes with potential for different parallelization routines.
We analyze this in more detail in Section 4.

2.4 GNN Inference vs. GNN Training

A series of GNN layers stacked one after another, as detailed
in Figure 2 and in § 2.3, constitutes GNN inference. GNN
training consists of three parts: forward pass, loss compu-
tation, and backward pass. The forward pass has the same
structure as GNN inference. For example, in classiﬁcation,
the loss L is obtained as follows: L = 1
i∈Y loss (yi, ti),
|Y|
where Y is a set of all the labeled samples, yi is the ﬁnal
prediction for sample i, and ti is the ground-truth label
for sample i. In practice, one often uses the cross-entropy
loss [65]; other functions may also be used [99].

(cid:80)

Backpropagation outputs the gradients of all the train-
able weights in the model. A standard chain rule is used
to obtain mathematical formulations for respective GNN
models. For example, the gradients for the ﬁrst GCN layer,
assuming a total of two layers (L = 2), are as follows [197]:

(cid:16)

(cid:98)AX

(cid:17)T (cid:16)

σ(cid:48) (cid:16)

∇W(0) L =

(cid:12) (cid:98)AT loss (Y − T) W(1)T (cid:17)
(cid:98)AXW(0)(cid:17)
where T is a matrix grouping all the ground-truth vertex
labels, cf. Table 1 for other symbols. This equation reﬂects
the forward propagation formula (cf. § 2.3.2); the main
difference is using transposed matrices (because backward
propagation involves propagating information in the re-
verse direction on the input graph edges) and the derivative
of the non-linearity σ(cid:48).

Mathema�cal formula�ons of GNN modelsMessage-Passing (MP-GNNs)Vector edge weights (learnable)Convolu�onal (C-GNNs)Scalar edge weights (preprocessed)A�en�onal (A-GNNs)Scalar edge weights (learnable)Spa�alExplicit use ofadjacency matrixSpectralExplicit use ofLaplacian matrixFormula�ons based on opera�ons on matricesgrouping all vertex and edge related vectorsGlobal Formula�onsFormula�ons based on func�onsopera�ng on single ver�ces & edgesLocal Formula�onsLinearAdjacency matrix usedin its 1st powerRa�onalAdjacency matrix usedin its ra�onal power(s)PolynomialAdjacency matrix usedin its polynomial power(s)LinearLaplacian matrix usedin its 1st powerRa�onalLaplacian matrix usedin its ra�onal power(s)PolynomialLaplacian matrix usedin its polynomial power(s)5

Fig. 5: (§ 2.3–§ 2.5) Basic elements of GNN model formulations
(top part: the local (LC) approach, bottom part: the global
(GL) approach), and how they translate into GNN operators
(central part). SAGA [153], NAU [208], and GReTA [126] are
GNN programming models. Red/green indicate formulations
from Figure 4.

pipeline parallelism (different NN layers are processed in
parallel) and operator parallelism (a single sample or neural
activity is processed in parallel).

We overview the parallelism taxonomy in Figure 6, and
show how it translates to parallelism in GNNs in Figure 7.
It is similar to that of traditional DL, in that it also has data
parallelism and model parallelism. However, there are certain
differences that we identify and analyze.

For example, as we detail in Section 3, data parallelism
in GNNs has two variants: mini-batch parallelism (when one
parallelizes processing a mini-batch, and updates the weights
after each mini-batch) and graph [partition] parallelism (when
one parallelizes a batch due to the inability to store a given

The structure of backward propagation depends on
whether full-batch or mini-batch training is used. Paralleliz-
ing mini-batch training is more challenging due to the inter-
sample dependencies, we analyze it in Section 3.

2.5 GNN Programming Models and Operators

Recent works that originated in the systems community
come with programming and execution models. These
models facilitate GNN computations. In general, they each
provide a set of programmable kernels, aka operators (also
referred to as UDFs – User Deﬁned Functions) that enable
implementing the GNN functions both in the LC formula-
tion ((cid:76), ψ, φ) and in the GL formulation (matrix products
and others). Figure 5 shows both LC and GL formulations,
and how they translate to the programming kernels.

The most widespread programming/execution model
is SAGA [153] (“Scatter-ApplyEdge-Gather-ApplyVertex”),
used in many GNN libraries [246]. In the Scatter operator,
the feature vectors of the vertices u, v adjacent to a given
edge (u, v) are processed (e.g., concatenated) to create the
data speciﬁc to the edge (u, v). Then, in ApplyEdge, this
data is transformed (e.g., passed through an MLP). Scatter
and ApplyEdge together implement the ψ function. Then,
Gather aggregates the outputs of ApplyEdge for each ver-
tex, using a selected commutative and associative opera-
tion. This enables implementing the (cid:76) function. Finally,
ApplyVertex conducts some user speciﬁed operation on the
aggregated vertex vectors (implementing φ).

Note that, to express the edge related kernels Scatter
and UpdateEdge, the LC formulation provides a generic
function ψ. On the other hand, to express these kernels in the
GL formulation, one adds an element-wise product between
the adjacency matrix A and some other matrix being a result
of matrix operations that provide the desired effect. For
example, to compute a “vanilla attention” model on graph
edges, one uses a product of H(l) with itself transposed.

Other operators, proposed in GReTA [126], Flex-
Graph [208], and others, are similar. For example, GReTA
has one additional operator, Activate, which enables a sep-
arate speciﬁcation of activation. On the other hand, GReTA
does not provide a kernel for applying the ψ function.

We illustrate the relationships between operators and
GNN functions from the LC and GL formulations, in Fig-
ure 5. Here, we use the name Aggregate instead of Gather
to denote the kernel implementing the (cid:76) function. This
is because “Gather” has traditionally been used to denote
bringing several objects together into an array [160]1.

Parallelism in these programming and execution models
is tightly related to that of the associated GNN functions
in LC and GL formulations; we discuss it in Section 4.
We also analyze parallel and distributed frameworks and
accelerators based on these models in Section 5.

2.6 Taxonomy of Parallelism in GNNs

In traditional deep learning, there are two fundamental
ways to parallelize processing a neural network [16]: data
parallelism and model parallelism that – respectively –
partition data samples and neural weights among different
workers. Model parallelism can further be divided into

1Another name sometimes used in this context is “Reduce”

Fig. 6: (§ 2.6) Parallelism taxonomy in GNNs.

Sca�erUpdateEdgeUpdateVerteximplemented withimplemented withimplemented withalso called:ApplyEdge(SAGA)also called:Gather (SAGA),Reducealso called:ApplyVertex (SAGA),Transform (GReTA)ProgrammingGNN modelimplemented withimplemented withimplemented withFunc�ons:Operators(kernels):Examples:Sparse,n x nDense, n x O(k)Dense, O(k) x O(k)SpMMGEMMMatrix products:Local (LC) mathema�cal GNN formula�onGlobal (GL) mathema�cal GNN formula�onModel-dependentmatrix opera�onsSee Table 3 & Table 7for more examplesModel-dependenttransforma�on (e.g.,raising A to a power)See Table 5 & Table 6 forexamplesAggregateGraph [par��on] parallelismDataparallelismModelparallelismOperator parallelismFeature parallelismStructure parallelismANN parallelismPipeline parallelismANN-pipeline parallelismANN-model parallelismIndependent mini-batch parallelismDependent mini-batch parallelismMini-batchparallelismMicro-pipeline parallelismMacro-pipeline parallelism6

Fig. 7: (§ 2.6) Overview of parallelism in GNNs. Different colors (red, green, blue) correspond to different workers.

batch on one worker, and only updates the weights after the
whole batch). Note that graph partition parallelism could
also be applied to a large mini-batch, if that mini-batch
cannot be stored on a single worker. Mini-batch parallelism
further divides into dependent mini-batch parallelism (when-
ever samples have dependencies between one another) and
independent mini-batch parallelism (no dependencies between
samples). Graph partition parallelism and dependent mini-
batch parallelism are much more challenging than their
equivalent forms in traditional deep learning because of
dependencies between data samples.

Model parallelism in GNNs also has several variants.
First, in pipeline parallelism, we distinguish macro-pipeline par-
allelism (pipelining the actual GNN layers) and micro-pipeline
parallelism (pipelining the processing of samples within a
single GNN layer). Second, single operators can also be
parallelized (operator parallelism) in different ways (feature
parallelism when updating in parallel different features in a
single feature vector, and graph [structure] parallelism when
processing in parallel a single feature by assigning different

workers to different neighbors of a given vertex). Finally,
the UpdateEdge and UpdateVertex kernels come with dense
neural network operations and thus one can apply tradi-
tional artiﬁcial neural network (ANN) parallelism to these
kernels. We refer to this form in general as ANN parallelism
and we further distinguish ANN-pipeline parallelism and
ANN-model parallelism.

2.7 Parallel and Distributed Models and Algorithms

We use formal models for reasoning about parallelism. For
a single-machine (shared-memory), we use the work-depth
(WD) analysis, an established approach for bounding run-
times of parallel algorithms. The work of an algorithm is
the total number of operations and the depth is deﬁned as
the longest sequential chain of execution in the algorithm
(assuming inﬁnite number of parallel threads executing the
algorithm), and it forms the lower bound on the algorithm
execution time [39], [42]. One usually wants to minimize
depth while preventing work from increasing too much.

In multi-machine (distributed-memory) settings, one
is often interested in understanding the algorithm cost in

Model parallelismGraph [par��on] parallelismDependent mini-batch parallelismParallel processing of a mini-batch, withpoten�al intra-mini-batch dependenciesReLUReLUReLUReLUReLUReLU.........ReLUGraph[structure]parallelismParallelprocessingof a singlefeatureFeatureparallelismParallelprocessingof a singlefeature vectorMicro-pipeline parallelismParallel processingof diﬀerentstages within a single GNN layerMacro-pipeline parallelism: Parallel processing ofdiﬀerent GNN layersANN-pipeline parallelismParallel processing of MLPlayers in Update kernelsIndependent mini-batch parallelism(similar to the tradi�onal ANN parallelism)Parallel processing of amini-batch, withno intra-mini-batch dependenciesANN-model parallelismParallel processing of the MLPmodel in Update kernelsReLUReLUReLU.........Distribu�ng a batch (or poten�ally a large mini--batch) over several workers due to its large sizePipeline parallelismOperator parallelismANN parallelismReLUReLUReLU.........Data parallelismProcessing whole batchUpdatemodelweightsProcessing mini-batchUpdatemodelweightsProcessing mini-batchUpdatemodelweightsMini-batch parallelismPar��onPar��onPar��onMini-batchMini-batchThe data (i.e., the graph structure) is needed at every GNN layer - unlikein tradi�onal ANNs, where data is only needed at the pipeline beginning7

Fig. 8: (§ 3.1, § 3.2) Graph partition parallelism vs. dependent and independent mini-batch parallelism in GNNs. Different
colors (red, green, blue) indicate different graph partitions or mini-batches, and the associated different workers. Black vertices
do not belong to any mini-batch.

terms of the amount of communication (i.e., communicated
data volume), synchronization (i.e., the number of global
“supersteps”), and computation (i.e., work), and minimizing
these factors. A popular model used in this setting is Bulk
Synchronous Parallel (BSP) [200].

3 DATA PARALLELISM

In traditional deep learning, a basic form of data parallelism
is to parallelize the processing of input data samples within
a mini-batch. Each worker processes its own portion of
samples, computes partial updates of the model weights,
and synchronizes these updates with other workers using
established strategies such as parameter servers or allre-
duce [16]. As samples (e.g., pictures) are independent, it
is easy to parallelize their processing, and synchronization
is only required when updating the model parameters. In
GNNs, mini-batch parallelism is more complex because
very often, there are dependencies between data samples (cf. Fig-
ure 3 and § 2.1. Moreover, the input datasets as a whole
are often massive. Thus, regardless of whether and how
mini-batching is used, one is often forced to resort to graph
partition parallelism because no single server can ﬁt the
dataset. We now detail both forms of GNN data parallelism.
We illustrate them in Figure 8.

3.1 Graph Partition Parallelism

Some graphs may have more than 250 billion vertices and
beyond 10 trillion edges [18], [147], and each vertex and/or
edge may have a large associated feature vector [110]. Thus,

one inevitably must distribute such graphs over different
workers as they do not ﬁt into one server memory. We
refer to this form of GNN parallelism as the graph partition
parallelism, because it is rooted in the established problem
of graph partitioning [52], [122] and the associated mincut
problem [84], [90], [122]. The main objective in graph parti-
tion parallelism is to distribute the graph across workers in
such a way that both communication between the workers
and work imbalance among workers are minimized.

We illustrate variants of graph partitioning in Figure 9.
When distributing a graph over different workers and
servers, one can speciﬁcally distribute vertices (edge [struc-
ture] partitioning, i.e., edges are partitioned), edges (ver-
tex [structure] partitioning, i.e., vertices are partitioned), or
edge and/or vertex input features (edge/vertex [feature] par-
titioning, i.e., edge and/or vertex input feature vectors are
partitioned). Importantly, these methods can be combined,
e.g., nothing prevents using both edge and feature vector
partitioning together. Edge partitioning is probably the most
widespread form of graph partitioning, but it comes with
large communication and work imbalance when partition-
ing graphs with skewed degree distributions. Vertex parti-
tioning alleviates it to a certain degree, but if a high-degree
vertex is distributed among many workers, it may also lead
to large overheads in maintaining a consistent distributed
vertex state. Differences between edge and vertex partition-
ing are covered extensively in rich literature [9], [50]–[52],
[55], [73], [73], [75], [94], [104], [121], [122], [124]. Feature
vertex partitioning was not addressed in the graph pro-

Fig. 9: (§ 3.1) Different forms of graph partition parallelism. Different colors (red, green, blue) indicate different graph partitions,
and the associated different workers. The gray graph element is oblivious to a given form of partitioning. Note that different
partitioning schemes can be combined together.

Samples: independent graphsSamples: dependent ver�cesGraph [par��on] parallelism(cf. full-batch training)Dependent mini-batch parallelismEach vertex and/oredge falls into somepar��onNot all samplesnecessarilybelongtoamini-batchIndependent mini-batch parallelism(cf. stochas�c mini-batch training)Samples arepar��onedacross workersbecause thewhole graphdoes not ﬁtinto the memoryof one workerSamples arepar��onedacross workersto accelerateconverganceNot all samplesnecessarilybelong to amini-batchSamples arepar��onedacross workersto accelerateconverganceA sampleA sampleA sampleGraph samples may be of diﬀerent sizes(they s�ll have same-size feature vectors)Samples may fallinto more thanmini-batchSamples may be selected intomore than onemini-batchEdge [structure] par��oningVertex [structure] par��oningEdges can be par��oned(i.e., diﬀerent ver�ces are distributedacross diﬀerent workers / servers)Edge [feature] par��oningVertex [feature] par��oningVer�ces can be par��oned(i.e., diﬀerent edges are distributedacross diﬀerent workers / servers)Input edge feature vectors can be par��oned,(i.e., diﬀerent features are distributedacross diﬀerent workers / servers)Input edge feature vectors can be par��oned,(i.e., diﬀerent features are distributedacross diﬀerent workers / servers)An edgebeloning toone serverAn edgepar��onedacross twoserversA vertexbeloning toone serverA vertexpar��onedacross twoserversA vertexpar��onedacross threeserversAn edge featurevector distributedtothreeserversA vertexfeature vectordistributed tothree serverscessing area because usually traditional distributed graph
algorithms, vertices and/or edges are associated with scalar
values.

Partitioning entails communication when a given part
of a graph depends on another part kept on a different
server. This may happen during a graph related operator
(Scatter, Aggregate) if edges or vertices are partitioned,
and during a neural network related operator (UpdateEdge,
UpdateVertex) if feature vectors are partitioned.

3.1.1 Full-Batch Training

Graph partition parallelism is commonly used to alleviate
large memory requirements of full-batch training. In full-
batch training, one must store all the activations for each
feature in each vertex in each GNN layer). Thus, a common
approach for executing and parallelizing this scheme is
using distributed-memory large-scale clusters that can hold
the massive input datasets in their combined memories,
together with graph partition parallelism. Still, using such
clusters may be expensive, and it still does not alleviate the
slow convergence. Hence, mini-batching is often used.

3.2 Mini-Batch Parallelism

In GNNs, if data samples are independent graphs, then
mini-batch parallelism is similar to traditional deep learn-
ing. First, one mini-batch is a set of such graph samples,
with no dependencies between them. Second, samples (e.g.,
molecules) may have different sizes (causing potential load
imbalance), similarly to, e.g., videos [143]. This setting is
common in graph classiﬁcation or graph regression. We
illustrate this in Figure 8 (right), and we refer to it as in-
dependent mini-batch parallelism. Note that while such graph
samples may have different sizes (e.g., molecules can have
different counts of atoms and bonds), their corresponding
feature vectors are of the same dimensionality.

Yet, in most GNN computations, mini-batch parallelism
is much more challenging because of inter-sample de-
pendencies (dependent mini-batch parallelism). As a concrete
example, consider node classiﬁcation. Similarly to graph
partition parallelism, one may experience load imbalance
issues, e.g., because vertices may differ in their degrees.
Moreover, a key challenge in GNN mini-batching is the
information loss when selecting the target vertices forming
a given mini-batch. In traditional deep learning, one picks
samples randomly. In GNNs, straightforwardly applying
such a strategy would result in very low accuracy. This
is because, when selecting a random subset of nodes, this
subset may not even be connected, but most deﬁnitely it
will be very sparse and due to the missing edges, a lot of
information about the graph structure is lost during the Ag-
gregate or Scatter operator. This information loss challenge
was circumvented in the early GNN works with full-batch
training [128], [223] (cf. § 3.1.1). Unfortunately, full-batch
training comes with slow convergence (because the model is
updated only once per epoch, which may require processing
billions of vertices), and the above-mentioned large memory
requirements. Hence, two more recent approaches that ad-
dress speciﬁcally mini-batching were proposed incorporating
support vertices, and appropriately selecting target vertices.

8

3.2.1 Support Vertices

In a line of works initiated by GraphSAGE [101], one adds
some neighbors of sampled target vertices as so called
support vertices to the mini-batch. These support vertices are
only used to increase the accuracy of predictions for target
vertices (i.e., they are not used as target vertices in that
mini-batch). Speciﬁcally, when executing the Scatter and
Aggregate kernels for each of target vertices in a mini-batch,
one also considers the pre-selected support vertices. Hence,
the results of Scatter and Aggregate are more accurate.
Support vertices of each target vertex v usually come from
not only 1-hop, but also from k-hop neighborhoods of v,
where k may be as large as graph’s diameter. The exact
selection of support vertices depends on the details of each
scheme. In GraphSAGE, they are sampled (for each target
vertex) for each GNN layer before the actual training.

We illustrate support vertices in Figure 10. Here, note
that the target vertices within each mini-batch may be clus-
tered but may also be spread across the graph (depending
on a speciﬁc scheme [60], [61], [101], [139]). Support vertices,
indicated with darker shades of each mini-batch color, are
located up to 2 hops away from their target vertices.

Fig. 10: (§ 3.2) Two variants of mini-batching: with and w/o
support vertices. Light colors (red, green, blue) indicate differ-
ent mini-batches (target vertices) and the associated workers.
Dark colors (dark red, dark green, dark blue) indicate support
vertices for each respective mini-batch. Black vertices do not
belong to any mini-batch. Example schemes: GraphSAGE [101]
(left), Cluster-GCN [65] (right).

One challenge related to support vertices is the overhead
of their pre-selection. For example, in GraphSAGE, one has
to – in addition to the forward and backward propagation
passes – conduct as many sampling steps as there are layers
in a GNN, to select support vertices for each layer and for
each vertex. While this can be alleviated with parallelization
schemes also used for forward and backward propagation,
it inherently increases the depth of a GNN computation by
a multiplicative constant factor.

Another associated challenge is called the neighborhood
explosion and is related to the memory overhead due to
maintaining potentially many such vertices. In the worst
case, for each vertex in a mini-batch, assuming keeping all
its neighbors up to H hops, one has to maintain O(kdH )
state. Even if some of these vertices are target vertices in that
mini-batch and thus are already maintained, when increas-

Mini-batches without support ver�cesTarget ver�ces:Support ver�ces:Mini-batches with support ver�cesTarget ver�ces:Mini-batches are selected such thataccuracy is high without support ver�cesEach mini-batch has support ver�cesthat increase the accuracy of ﬁnal ML tasksDiﬀerent colorsindicate diﬀerentmini-batches andthe associatedworkers9

Fig. 11: (§ 3.2) Neighborhood explosion in mini-batching in GNNs.

ing H, their ratio becomes lower. GraphSAGE alleviates
this by sampling a constant fraction of vertices from each
neighborhood instead of keeping all the neighbors, but the
memory overhead may still be large [234]. We show an
example neighborhood explosion in Figure 11.

Method

Work & depth in one training iteration

Full-batch training schemes:

Full-batch [128] O (cid:0)Lmk + Lnk2(cid:1)
Weight-tying [139] O (cid:0)Lmk + Lnk2(cid:1)
O (cid:0)Lmk + Lnk2(cid:1)
RevGNN [139]

O (L log k + L log d)
O (L log k + L log d)
O (L log k + L log d)

3.2.2 Appropriate Selection of Target Vertices

More recent GNN mini-batching works focus on the appro-
priate selection of target nodes included in mini-batches,
such that support vertices are not needed for high accuracy.
For example, Cluster-GCN ﬁrst clusters a graph and then
assign clusters to be mini-batches [65], [230]. This way, one
reduces (but not eliminates) the loss of information because
a mini-batch usually contains a tightly knit community of
vertices. We illustrate this in Figure 10 (right). However, one
has to additionally compute graph clustering as a form of
preprocessing. This can be parallelized with one of many
established parallel clustering routines [29], [30], [36], [122].

3.2.3 Discussion

We ﬁrst observe that the key difference between graph
partition parallelism and mini-batch parallelism is the
timing of updating model weights. It takes place after the whole
batch (for the former) and after each mini-batch (for the
latter). Other differences are as follows. First, the primary
objective when partitioning a graph is to minimize com-
munication and work imbalance across workers. Contrarily,
in mini-batching, one aims at a selection of target vertices
that maximizes accuracy. Second, each vertex belongs to
some partition(s), but not each vertex is necessarily included
in a mini-batch. Third, while mini-batch parallelism has a
variant with no inter-sample dependencies, graph partition
parallelism nearly always deals with a connected graph and
has to consider such dependencies.

We also note that one could consider the asynchronous
execution of different mini-batches. This would entail asyn-
chronous GNN training, with model updates being con-
ducted asynchronously. Such a scheme could slow down
convergence, but would offer potential for more parallelism.

3.3 Work-Depth Analysis: Full-Batch vs. Mini-Batch

We analyze work/depth of different GNN training schemes
that use full-batch or mini-batch training, see Table 2.

First, all methods have a common term in work being
O(Lmk + Lnk2) that equals the number of layers L times
the number of operations conducted in each layer, which

Mini-batch training schemes:

(cid:16)
Lmk + Lnk2 + cLnk2(cid:17)
Lmk + Lnk2 + cLnk2(cid:17)

(cid:16)

GraphSAGE [101] O

O (L log k + L log c)

VR-GCN [60]

O
O (L log k + L log c)
O (cid:0)Lmk + Lnk2 + cLnk2(cid:1) O (L log k + L log c)

FastGCN [61]
Cluster-GCN [65] O (cid:0)Wpre + Lmk + Lnk2(cid:1) O (Dpre + L log k + L log d)
GraphSAINT [234] O (cid:0)Wpre + Lmk + Lnk2(cid:1) O (Dpre + L log k + L log d)

TABLE 2: (§ 3.3) Work-depth analysis of GNN training meth-
ods. c is the number of vertices sampled per neighborhood or
per GNN layer.

is mk for sparse graph operations (Aggregate) and nk2 for
dense neural network operations (UpdateVertex). This is the
total work for full-batch methods. Mini-batch schemes have
additional work terms. Schemes based on support vertices
(GraphSAGE, VR-GCN, FastGCN) have terms that reﬂect
how they pick these vertices. GraphSAGE and VR-GCN
have a particularly high term O(cLnk2) due to the neigh-
borhood explosion (c is the number of vertices sampled per
neighborhood). FastGCN alleviates the neighborhood ex-
plosion by sampling c vertices per whole layer, resulting in
O(cLnk2) work. Then, approaches that focus on appropri-
ately selecting target vertices (GraphSAINT, Cluster-GCN)
do not have the work terms related to the neighborhood
explosion. Instead, they have preprocessing terms indicated
with Wpre. Cluster-GCN’s Wpre depends on the selected
clustering method, which heavily depends on the input
graph size (n, m). GraphSAINT, on the other hand, does
stochastic mini-batch selection, the work of which does not
necessarily grow with n or m.

In terms of depth, all the full-batch schemes depend
on the number of layers L. Then, in each layer, two bot-
tleneck operations are the dense neural network operation
(UpdateVertex, e.g., a matrix-vector multiplication) and the
sparse graph operation (Aggregate). They take O(log k)
and O(log d) depth, respectively. Mini-batch schemes are
similar, with the main difference being the O(log c) instead
of O(log d) term for the schemes based on support vertices.
This is because Aggregate in these schemes is applied over c
sampled neighbors. Moreover, in Cluster-GCN and Graph-

needed byABCEFGHIJKNDLMOA's 1-hop neighborsA's 2-hop neighborsBCEDAAneeded byneeded byneeded byneeded by...............Vertex A, whoseloss we computeA's dependencystructure whencompu�nglossGround--truthlabelBCEDABKCAGFHBACHDJIADCJEMLODALEONRed ver�ces: an examplemini-batchNeighborhood explosion:#A's dependent ver�cesand their embeddingscan grow exponen�ally,and it can easily extendbeyond A's mini-batchNeighborhood explosion:  #A's dependent ver�cesand their embeddings can grow exponen�ally,and it can easily extend beyond A's mini-batch20% of (L-1) dependenciesare outside A's mini-batchLayer(L-1)Layer(L-2)Layer(L-3)36% of (L-2) dependenciesare outside A's mini-batch>50% of (L-3) dependenciesare outside A's mini-batchSAINT, the neighborhoods may have up to d vertices, yield-
ing the O(log d) term. They however have the additional
preprocessing depth term Dpre that depends on the used
sampling or clustering scheme.

To summarize, full-batch and mini-batch GNN training
schemes have similar depth. Note that this is achieved using
graph partition parallelism in full-batch training methods,
and mini-batch parallelism in mini-batching schemes. Con-
trarily, overall work in mini-batching may be larger due
to the overheads from support vertices, or additional pre-
processing when selecting target vertices using elaborate
approaches. However, mini-batching comes with faster con-
vergence and usually lower memory pressure.

3.4 Tradeoff Between Parallelism & Convergence

The efﬁciency tradeoff between the amount of parallelism in a
mini-batch and the convergence speed, controlled with the
mini-batch size, is an important part of parallel traditional
ANNs [16]. In short, small mini-batches would accelerate
convergence but may limit parallelism while large mini-
batches may slow down convergence but would have more
parallelism. In GNNs, ﬁnding the “right” mini-batch size
is much more complex, because of the inter-sample de-
pendencies. For example, a large mini-batch consisting of
vertices that are not even connected, would result in very
low accuracy. On the other hand, if a mini-batch is small but
it consists of tightly connected vertices that form a cluster,
then the accuracy of the updates based on processing that
mini-batch can be high [65].

4 MODEL PARALLELISM

In traditional neural networks, models are often large. In
GNNs, models (W) are usually small and often ﬁt into the
memory of a single machine. However, numerous forms of
model parallelism are heavily used to improve throughput;
we provided an overview in § 2.6 and in Figure 7.

In the following model analysis, we often picture the
used linear algebra objects and operations. For clarity, we
indicate their shapes, densities, and dimensions, using small
ﬁgures, see Table 3 for a list. Interestingly, GNN models in
the LC formulations heavily use dense matrices and vectors
with dimensionalities dominated by O(k), and the associ-
ated operations. On the other hand, the GL formulations
use both sparse and dense matrices of different shapes
(square, rectangular, vectors), and the used matrix multipli-
cations can be dense–dense (GEMM, GEMV), dense–sparse
(SpMM), and sparse–sparse (SpMSpM). Other operations
are elementwise matrix products or rational sparse matrix
powers. This rich diversity of operations immediately il-
lustrates a huge potential for parallel and distributed tech-
niques to be used with different classes of models.

4.1 Operator Parallelism

When analyzing operator parallelism, we ﬁrst focus on the
LC formulations, and then proceed to the GL formulation.

4.1.1 Parallelism in LC Formulations of GNN Models

We illustrate generic work and depth equations of LC GNN
formulations in Figure 12. Overall, work is the sum of
any preprocessing costs Wpre, post-processing costs Wpost,
and work of a single GNN layer Wl times the number of

Symbol

Description

Used often in

10

,

,

Matrices and vectors

Dense vectors, dimensions:
O(k) × 1, 1 × O(k)
Dense matrices, dimensions:
O(k) × O(k)
Dense matrices, dimensions:
n × O(k), O(k) × n
Sparse matrix, dimensions:
n × n

LC models

GL & LC models

GL models

GL models

Matrix multiplications (dimensions as stated above)

×

×

×

×

×

GEMM, dense tall matrix ×
dense square matrix
GEMM, dense square matrix ×
dense square matrix
GEMM, dense tall matrix ×
dense tall matrix
GEMV, dense matrix ×
dense vector
SpMM, sparse matrix ×
dense matrix

GL models

GL models

GL models

LC models

GL models

Elementwise matrix products and other operations

(cid:12) (...)

Elementwise product of a
sparse matrix and some object

, x ∈ N SpMSpM, sparse matrix ×
sparse matrix

GL models

GL models

, x ∈ Z

Rational sparse matrix power GL models

x

x

·

, (cid:12)

(cid:13)
(cid:13)

, (cid:80)

Vector dot product,
elementwise vector product

Vector concatenation,
sum of d vectors, d ≤ n

LC models

LC models

TABLE 3: Important objects and operations from linear algebra
used in GNNs. We assign these operations to speciﬁc GNN
models in Tables 5, 6, and 7.

layers L. In the considered generic formulation in Eq. (8), Wl
equals to work needed to evaluate ψ for each edge (mWψ),
(cid:76) for each vertex (nW⊕), and φ for each vertex (nWφ).
Depth is analogous, with the main difference that the depth
of a single GNN layer is a plain sum of depths of computing
ψ, (cid:76), and φ (each function is evaluated in parallel for each
vertex and edge, hence no multiplication with n or m).

Work (generic) = Wpre + LWl + Wpost
Depth (generic) = Dpre + LDl + Dpost
Work of Eq. (8) = Wpre + L · (cid:0)mWψ + nW⊕ + nWφ
Depth of Eq. (8) = Dpre + L · (cid:0)Dψ + D⊕ + Dφ

(cid:1) + Dpost

(cid:1) + Wpost

Fig. 12: Generic equations for work and depth in GNN LC
formulations.

We now analyze work and depth of many speciﬁc GNN
models, by focusing on the three functions forming these
models: ψ, (cid:76), and φ. The analysis outcomes are in Tables 5
and 6. We select the representative models based on a recent
survey [63]. We also indicate whether a model belongs to
the class of convolutional (C-GNN), attentional (A-GNN),
or message-passing (MP-GNN) models [169] (cf. § 2.3.1).

Analysis of ψ We show the analysis results in Table 5.
We provide the formulation of ψ for each model, and we

also illustrate all algebraic operations needed to obtain ψ.
All C-GNN models have their ψ determined during pre-
processing. This preprocessing corresponds to the adjacency
matrix row normalization (cij = 1/di), the column nor-
malization (cij = 1/dj), or the symmetric normalization
(cij = 1/(cid:112)didj) [223]. In all these cases, their derivation
takes O(1) depth and O(m) work. Then, A-GNNs and MP-
GNNs have much more complex formulations of ψ than
C-GNNs. Details depend on the model, but - importantly -
nearly all the models have O(k2) work and O(log k) depth.
The most computationally intense model, GAT, despite hav-
ing its work equal to O(dk2), has also logarithmic depth
of O(log k + log d). This means that computing ψ in all
the considered models can be effectively parallelized. As
for the sparsity pattern and type of operations involved in
evaluating ψ, most models use GEMV. All the considered
A-GNN models also use transposition of dense vectors.
GAT also uses vector concatenation and sum of up to d
vectors. Finally, one considered MP-GNN model uses an
elementwise MV product. In general, each considered GNN
model uses dense matrix and vector operations to obtain ψ
for each of the associated edges.

Analysis of (cid:76) The aggregate operator (cid:76)

j∈N (i) is al-
most always a commutative and associative operation such
as min, max, or plain sum [79], [209]. While it operates
on vectors xj of dimensionality k, each dimension can
be computed independently of others. Thus, to compute
(cid:76)
j∈N (i), one needs O(log di) depth and O(kdi) work, using
established parallel tree reduction algorithms [41]. Hence,
(cid:76) is the bottleneck in depth in all the considered models.
This is because d (maximum vertex degree) is usually much
larger than k.

Analysis of φ The analysis of φ is shown in Table 6 (for
the same models as in Table 5). We show the total model
work and depth. All the models entail matrix-vector dense
products and a sum of up to d dense vectors. Depth is
logarithmic. Work varies, being the highest for GAT.

We also illustrate the operator parallelism in the LC
formulation, focusing on the GNN programming kernels,
in the top part of Figure 13. We provide the corresponding
generic work-depth analysis in Table 4, and we also assess
communication and synchronization (discussed separately
in § 4.1.5). The four programming kernels follow the work
and depth of the corresponding LC functions (ψ, ⊕, φ).

Kernel

Work

Depth

Comm. Sync.

O(1)

Scatter (ψ)
UpdateEdge (ψ) O(mWψ) O(Dψ)
Aggregate (⊕)
UpdateVertex (φ) O(nWφ) O(Dφ)

O(mk) O(1)
O(1)
O(1)
O(nW⊕) O(D⊕ log d) O(mk) O(1)
O(1)

O(1)

O(1)

TABLE 4: Work-depth analysis of GNN operators (kernels).

4.1.2 Parallelism in GL Formulations of GNN Models

Parallelism in GL formulations is analyzed in Table 7. The
models with both LC and FG formulations (e.g., GCN) have
the same work and depth. Thus, fundamentally, they offer
the same amount of parallelism. However, the GL formu-
lations based on matrix operations come with potential for
different parallelization approaches than the ones used for
the LC formulations. For example, there are more oppor-
tunities to use vectorization, because one is not forced to

11

vectorize the processing of feature vectors for each vertex or
edge separately (as in the LC formulation), but instead one
could vectorize the derivation of the whole matrix H [31].

There are also models designed in the GL formulations
with no known LC formulations, cf. Tables 5–6. These are
models that use polynomial and rational powers of the
adjacency matrix, cf. § 2.3.2 and Figure 4. These models have
only one iteration. They also offer parallelism, as indicated
by the logarithmic depth (or square logarithmic for ratio-
nal models requiring inverting the adjacency matrix [161]).
While they have one iteration, making the L term vanish,
they require deriving a given power x of the adjacency
matrix A (or its normalized version). Importantly, as com-
puting these powers is not interleaved with non-linearities
(as is the case with many models that only use linear powers
of A), the increase in work and depth is only logarithmic,
indicating more parallelism. Still, their representative power
may be lower, due to the lack of non-linearities.

We overview two example GL models (GCN and vanilla
graph attention) in Figure 13 (bottom). In this ﬁgure, we also
indicate how the LC GNN kernels are reﬂected in the ﬂow
of matrix operations in the GL formulation.

4.1.3 Instantiation of Edge Feature Vectors

The LC formulations as speciﬁed by Eq. (8) (C-GNNs, A-
GNNs, MP-GNNs) enable explicit instantiation of vertex
feature vectors. However, in some cases, one may also want
to explicitly instantiate edge feature vectors. Such instanti-
ation would be used in, for example, edge classiﬁcation or
edge regression tasks. An example GNN formulation that
enables this is Graph Networks by Battaglia et al. [14], also
an LC formulation. The insights about parallelism in such
a formulation are not different than the ones provided in
this section; the central difference lies in the fact that, in
MP-GNNs, edge feature vectors ψ are “transient” and used
primarily as input for obtaining vertex feature vectors.

4.1.4 Feature vs. Structure vs. Model Weight Parallelism

Feature parallelism is straightforward in both LC and GL
formulations (cf. Figure 13). In the former, one can execute
binary tree reductions over different features in parallel
(feature parallelism in (cid:76)), or update edge or vertex features
in parallel (feature parallelism in ψ and φ). In the latter, one
can multiply a row of an adjacency matrix with any column
of the latent matrix H (corresponding to different features)
in parallel. As feature vectors are dense, they can be stored
contiguously in memory and easily used with vectorization.
Graph structure parallelism is also accessible in both
LC and GL formulations. In the former, it is present via
parallel execution of (cid:76) (for a single speciﬁc feature). In the
latter, one simply parallelizes the multiplication of a given
adjacency matrix row with a given feature matrix column.

Traditional model weight parallelism, in which one par-
titions the weight matrix W across workers, is also possible
in GNNs. Yet, due to the small sizes of weight matrices
used so far in the literature [77], [110], it was not yet the
focus of research. If this parallelism becomes useful in the
feature, one could use traditional deep learning techniques
to parallelize the model weight processing [16].

Reference

Class

Formulation for ψ (hi, hj )

Dimensions & density of one
execution of ψ (hi, hj )

Pr?

Work & depth of one
execution of ψ (hi, hj )

12

GCN [128]

C-GNN

1√

didj

hj

GraphSAGE [101]
(mean)
GIN [226]

CommNet [192]

C-GNN hj

C-GNN hj
C-GNN hj

Vanilla
attention [201]

MoNet [158]

A-GNN (cid:0)hT

(cid:1) hj

i · hj
(cid:16)

A-GNN exp

− 1

2 (hj − wj )T W−1

j

(hj − wj )

(cid:17)

GAT [202]

A-GNN

(cid:16)

exp

σ

(cid:16)

aT ·
(cid:16)
(cid:16)

σ

(cid:105)(cid:17)(cid:17)

(cid:13)
(cid:104)
(cid:13)Whj
Whi
(cid:13)
(cid:104)
aT ·
(cid:13)Why
Whi

(cid:80)

y∈ (cid:99)N (i) exp

(cid:105)(cid:17)(cid:17) hj

Attention-based
GNNs [196]

A-GNN w

hT

i ·hj
(cid:107)hi(cid:107)(cid:107)hj(cid:107)

hj

G-GCN [47]

MP-GNN σ (W1hi + W2hj ) (cid:12) hj

GraphSAGE [101]
(pooling)
EdgeConv [216]
“choice 1”
EdgeConv [216]
“choice 5”

MP-GNN σ (Whj + w)

MP-GNN Whj

MP-GNN σ (W1 (hj − hi) + W2hi)

c ·

(cid:18)

·

(cid:18)

exp

(cid:18)

exp

(cid:18)

(cid:80)exp
(cid:18)

·

×

(cid:18)

(cid:19)

·

×
(cid:20)

·

(cid:20)
·
(cid:19)

·

(cid:19)

×

×

×

(cid:150) O(k)

O(1)

(cid:150) O(1)

(cid:150) O(1)
(cid:150) O(1)

O(1)

O(1)

O(1)

Ø O (k)

O(log k)

Ø O (cid:0)k2(cid:1) O(log k)

Ø O (cid:0)dk2(cid:1) O (log k + log d)

Ø O(k)

O(log k)

×

×

×

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21)(cid:19)

(cid:21)(cid:19) ·

×

×

(cid:12)

Ø O(k2)

O(log k)

Ø O(k2)

O(log k)

Ø O(k2)

O(log k)

Ø O(k2)

O(log k)

TABLE 5: Comparison of local (LC) formulations of GNN models with respect to the inner function ψ (hi, hj). For clarity
and brevity of equations, when it is obvious, we omit the matrix multiplication symbol × and the indices of a given iteration
(GNN layer) number (l). “Class”: class of a GNN model with respect to the complexity of ψ, details are in Section 2. All models
considered in this table feature aggregations over 1-hop neighbors (“Type L”, details are in Section 2). “Dimensions & density”:
dimensions and density of the most important tensors and tensor operations when computing ψ (hi, hj) in a given model. “Pr”:
can coefﬁcients in ψ be preprocessed ((cid:150)), or do they have to be learnt (Ø)? When listing the most important tensor operations,
we focus on multiplications.

Reference

Class

Formulation of φ for h(l)
i
ψ (hi, hj ) are stated in Table 5

;

Dimensions & density of
computing φ(·), excluding ψ(·)

Work & depth (a whole training iteration
or inference, including ψ from Table 5)

GCN [128]
GraphSAGE [101]
(mean)

C-GNN W ×

C-GNN W ×

(cid:16)(cid:80)

(cid:16) 1
di

(cid:17)
j∈(cid:99)N (i) ψ (hj )
·

(cid:16)(cid:80)

j∈(cid:99)N (i) ψ (hj )

(cid:17)(cid:17)

×(cid:80)

× (cid:80)

O(Lmk + Lnk2) O(L log d + L log k)

O(Lmk + Lnk2) O(L log d + L log k)

j∈N (i) ψ (hj )

(cid:17)

(cid:122)

Ktimes
(cid:125)(cid:124)
× ... ×

(cid:123)

× (cid:80)

GIN [226]

C-GNN MLP

CommNet [192] C-GNN W1hi + W2 ×
Vanilla
attention [201]

A-GNN W ×

(cid:16)(cid:80)

GAT [202]

A-GNN W ×

Attention-based
GNNs [196]

A-GNN W ×

MoNet [158]

A-GNN W ×

G-GCN [47]

MP-GNN W ×

(cid:16)(cid:80)

(cid:16)(cid:80)

(cid:16)(cid:80)

(cid:16)(cid:80)

(cid:16)
(1 + (cid:15))hi + (cid:80)
(cid:16)(cid:80)

(cid:17)

j∈N +(i) ψ (hj )
(cid:17)
j∈(cid:99)N (i) ψ (hi, hj )
(cid:17)
j∈(cid:99)N (i) ψ (hi, hj )
(cid:17)
j∈(cid:99)N (i) ψ (hi, hj )

(cid:17)
j∈(cid:99)N (i) ψ (hj )
j∈N +(i) ψ (hi, hj )
(cid:13)
(cid:13)
(cid:13)

(cid:17)

GraphSAGE [101]
(pooling)
EdgeConv [216]
“choice 1”
EdgeConv [216]
“choice 5”

MP-GNN

(cid:16)

W ×

(cid:16)

hi

(cid:0)maxj∈N (i) ψ (hi, hj )(cid:1)(cid:17)(cid:17)

MP-GNN (cid:80)

j∈N +(i) ψ (hj )

MP-GNN maxj∈N +(i) ψ (hi, hj )

(cid:80)

(cid:80)

× (cid:80)

× (cid:80)

× (cid:80)

× (cid:80)

× (cid:80)

× (cid:80)

×

(cid:18)

(cid:18) (cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:19) (cid:19)

× (cid:80)

O(Lmk + LKnk2) O(L log d + LK log k)

O(Lmk + Lnk2) O(L log d + L log k)

O(Lmk + Lnk2) O(L log d + L log k)

O(Lmdk2 + Lnk2) O(L log d + L log k)

O(Lmk + Lnk2) O(L log d + L log k)

O(Lmk2 + Lnk2) O(L log d + L log k)

O(Lmk2 + Lnk2) O(L log d + L log k)

O(Lmk2 + Lnk2) O(L log d + L log k)

O(Lmk2 + Lnk2) O(L log d + L log k)

O(Lmk2 + Lnk2) O(L log d + L log k)

TABLE 6: Comparison of local (LC) formulation of GNN models with respect to the outer function φ. For clarity and brevity
of equations, when it is obvious, we omit the matrix multiplication symbol × and the indices of a given iteration number (l);
we also omit activations from the formulations (these are elementwise operations, not contributing to work or depth). “Class”:
class of a GNN model with respect to the complexity of ψ, details are in Section 2. All models considered in this table feature
aggregations over 1-hop neighbors (“Type L”, details are in Section 2). “Dimensions & density”: dimensions and density of the
most important tensors and tensor operations in a given model when computing h(l)
. When listing the most important tensor
i
operations, we focus on multiplications.

13

Fig. 13: Operator parallelism in GNNs for LC formulations (top) and GL formulations (bottom).

Reference

Type

Algebraic formulation
for H(l+1)

Dimensions & density
of deriving H(l+1)

#I

Work & depth (one whole
training iteration or inference)

GCN [128]

GraphSAGE [101]
(mean)

GIN [226]

CommNet [192]

L

L

L

L

(cid:98)AHW

(cid:98)AHW

(cid:16)

MLP

((1 + (cid:15))I + (cid:98)A)H

(cid:17)

AHW2 + HW1

Dot Product [201] L

(cid:0)A (cid:12) (cid:0)HHT (cid:1)(cid:1) HW

EdgeConv [216]
“choice 1”

SGC [219]

DeepWalk [168]

ChebNet [72]

DCNN [6],
GDC [130]

Node2Vec [97]

L

P

P

P

P

P

AHW

(cid:98)AsHW
(cid:16)(cid:80)T

s=0 A

s(cid:17)

HW

s(cid:17)

s=0 θsA

HW

(cid:16)(cid:80)T

(cid:16)(cid:80)T

s(cid:17)

HW

s=1 wsA
(cid:16)

(cid:16) 1

p I +

1 − 1
q

(cid:17)

2(cid:17)

A + 1

q A

L O(mkL + Lnk2) O(L log k + L log d)

L O(mkL + Lnk2) O(L log k + L log d)

L O(mkL + KLnk2) O(LK log k + LK log d)

L O(mkL + Lnk2) O(L log k + L log d)

×

×

L O(Lmk + Lnk2) O(L log k + L log d)

×

×

×

×

×

×

×

×

×

(cid:12)

×

s

(cid:18)

× ... ×

+

×
(cid:19)

×

×

+ ...+

+ ...+

+ ...+

×
0

0

1

0

(cid:18)

(cid:18)

(cid:18)

(cid:18)

T (cid:19)

T (cid:19)

T (cid:19)

2 (cid:19)

HW

+ ... +

L O(mkL + Lnk2) O(L log k + L log d)

1 O(mn log s + nk2) O(log k + log s log d)

1 O(mn log T + nk2) O(log k + log T log d)

1 O(mn log T + nk2) O(log k + log T log d)

1 O(mn log T + nk2) O(log k + log T log d)

×

×

×

×

×

×

×

×

1 O(mn + nk2)

O(log k + log d)

(cid:18)

2 (cid:19)

(cid:16)

2(cid:17)

P

+

(cid:16)

(cid:16)

R

R

α

×

×

−1

(cid:17)−1

(cid:17)−1

HW

HW

A + θA

I − (1 − α) (cid:98)A

(1 + α)I − α (cid:98)A

1 O(mn + nk2)

LINE [148],
SDNE [207]
Auto-Regress
[250], [256]
PPNP
[43], [129], [230]
ARMA [38],
ParWalks [221]
TABLE 7: Comparison of global (GL) linear algebra formulations of GNN models. For clarity and brevity of equations, when it is obvious, we
omit the matrix multiplication symbol × and the indices of a given iteration number (l); we also omit activations from the formulations (these are
elementwise operations, not contributing to work or depth). “Type”: type of a GNN model with respect to the scope of accessed vertex neighbors,
details are in Section 2 (“L”: adjacency matrix is used in its 1st power, “P”: adjacency matrix is used in its polynomial power, “R”: adjacency
matrix is used in its rational power). “#I”: the number of GNN layers (GNN iterations). “Dimensions & density”: dimensions and density of the
most important tensors and tensor operations in a given model. When listing the most important tensor operations, we focus on multiplications.

O(log2 n + log k)

O(log2 n + log k)

O(log2 n + log k)

1 O(n3 + nk2)

1 O(n3 + nk2)

1 O(n3 + nk2)

O(log k + log d)

I − a (cid:98)A

HW

HW

(cid:17)−1

−1

−1

×

×

×

×

×

×

R

(cid:16)

b

Operator Parallelism, local (LC) formula�onSend vertex featurevectors along the edges[]Update edge feature vectorsbased on their previous valuesand the adjacent ver�cesGather the feature vectorsof the neighboring ver�ces andpossibly edges of each vertexUpdate vertex feature vectorsbased on their previous valuesand the aggregated vector(4) UpdateVertex(1) Sca�er(2) UpdateEdge(3) Reduce (Aggregate)LegendRed color:communica�onCommu-nica�ngfeaturevectorGreen color:computa�onUpdaingfeaturevectorOperator Parallelism, global (GL) linear algebra formula�onxx[][][]Legend[][]SparsematrixDensematrixA single GNN layerx[][][]x[]Large dimension:n (#ver�ces)Small dimension:O(k) (#features)We show two example formula�ons (many GNN models, especially fromA-GNN and MP-GNN classes, do not have known GL formula�ons)Highlighted row corresponds to theneighbors of a speciﬁc vertex v, whosefeature vector is being computedExample model: Graph Convolu�on Network(3) Reduce(4) UpdateVertexExample model: Graph A�en�on Network based on Dot Product (Vanilla A�en�on)((x[][](1) Sca�er(2) UpdateEdge(3) Reduce(4) UpdateVertexHighlighted column corresponds to the speciﬁc feature f that isbeing computed for vertex v[][][][][][][][][][][][][][][][][][][][][][][][][][][][][]Highlighted columns correspond to two vertex feature vectors involvedin compu�ng the edge a�en�on scoreAll feature vectors canbe sent inparallelReduc�ons atthe same vertexare synchronizedAll feature vectors canbe updatedin parallelAll feature vectors canbe updatedin parallelEach vertex (row)can be computedin parallelEach feature(column) canbe computedin parallelCombining par�al features ofa vertex may have to be synchronizedAll pairwiseproducts can be conductedin parallelAll forms of parallelismfrom the le� example(GCN) also apply hereAll dot productsrelated to all pairsof ver�ces can becomputed in parallel14

4.1.5 Communication & Synchronization

4.2 Pipeline Parallelism

Communication in the LC formulations takes place in the
Scatter kernel (a part of ψ) if vertex feature vectors are com-
municated to form edge feature vectors; transferred data
amounts to O(mk). Similarly, during the Aggregate kernel
((cid:76)), there can also be O(mk) data moved. Both UpdateEdge
(ψ) and UpdateVertex (φ) do not explicitly move data. How-
ever, they may be associated with communication intense
operations; especially A-GNNs and MP-GNNs often have
complex processing associated with ψ and φ, cf. Tables 5
and 6. While this processing entails matrices of dimensions
of up to O(k) × O(k), which easily ﬁt in the memory of a
single machine, this may change in the future, if the feature
dimensionality k is increased in future GNN models.

In the default synchronized variants of GNN, computing
all kernels of the same type must be followed by global
synchronization, to ensure that all data has been received by
respective workers (after Scatter and Aggregate) or that all
feature vectors have been updated (after UpdateEdge and
UpdateVertex). In Section 4.2.3, we discuss how this require-
ment can be relaxed by allowing asynchronous execution.

Communication and synchronization in the GL formula-
tions heavily depend on the used matrix representations and
operations. Speciﬁcally, there have been a plethora of works
into communication reduction in matrix operations, for
example targeting dense matrix multiplications [86], [134]–
[136] or sparse matrix multiplications [92], [187], [188], [190].
They could be used with different GNN operations (cf. Ta-
ble 3) and different models (cf. Table 7). The exact bounds
would depend on the selected schemes. Importantly, many
works propose to trade more storage for less communication
by different forms of input matrix replication [188]. This
could be used in GNNs for more performance.

Pipelining has two general beneﬁts. First, it increases the
throughput of a computation, lowering the overall process-
ing runtime. This is because more operations can ﬁnish in a
time unit. Second, it reduces memory pressure in the com-
putation. Speciﬁcally, one can divide the input dataset into
chunks, and process these chunks separately via pipeline
stages, having to keep a fraction of the input in memory at a
time. In GNNs, pipelining is often combined with graph
partition parallelism, with partitions being such chunks.
We distinguish two main forms of GNN pipelines: micro-
pipelines and macro-pipelines, see Figure 7 and § 2.6.

4.2.1 Micro-Pipeline Parallelism

In micro-pipeline parallelism, the pipeline stages corre-
spond to the operations within a GNN layer. Here, for sim-
plicity, we consider a graph operation followed by a neural
operation, followed by a non-linearity, cf. Figure 2. One can
equivalently consider kernels (Scatter, UpdateEdge, Aggre-
gate, UpdateVertex) or the associated functions (ψ, ⊕, φ).
Such pipelining enables reducing the length of the sequence
of executed operators by up to 3×, effectively forming a 3-
stage operator micro-pipeline. There have been several prac-
tical works into micro-pipelining GNN operators, especially
using HW accelerators; we discuss them in Section 5.

We show an example micro-pipeline (synchronous) in
the top panel of Figure 14. Observe that each neural opera-
tion must wait for all graph operations to ﬁnish, because – in
the worst case – in each partition, there may be vertices with
edges to all other partitions. This is an important difference
to traditional deep learning (and to a GNN setting with
independent graphs, cf. Figure 3), where chunks have no
inter-chunk dependencies, and thus neural processing of P1

Fig. 14: (§ 4.2) Overview of pipelining combined with graph partition parallelism (top-left panel), and of (§ 4.2.3) asynchronous
pipelined execution (other panels). Each of four example GNN executions processes three graph partitions (P1, P2, P3) on three
stages: red (a sparse graph operation such as convolution), green (a dense neural operation such as MLP), and blue) (a non-
linearity such as ReLU). We use two GNN layers (shades indicate layers). The whole execution is fully pipelined, i.e., there are 6
workers in total (three workers for each stage in each layer).

Synchronous micro-pipeline + Synchronous macro-pipelineP1P2P3P1P2P3P3P1P2P1P2P3P1P2P3P3P1P2P1P2P3P1P2P3P3P1P2P1P2P3P1P2P3P3P1P2P1P2P3P2P3P1P1P2P3P1P2P3P3P1P2P3P1P2P1P2P3P1P2P3P3P1P2P1P2P3P1P2P3P3P1P2Asynchronous micro-pipeline + Synchronous macro-pipelineSynchronous micro-pipeline + Asynchronous macro-pipelineAsynchronous micro-pipeline + Asynchronous macro-pipelineInter-layersynchroniza�onSynchroniza�onbefore backprop.Layer 1Forward passLayer 2Forward passInter-layersynchroniza�onSynchroniza�onbefore backprop.Layer 1Forward passLayer 2Forward passLayer 1 + Layer 2Forward passLayer 1 + Layer 2Forward passTimeSynchroniza�onbefore backprop.Synchroniza�onbefore backprop.ReLURed:sparse graphopera�onThe whole execu�onis fully pipelined:each color and shadeis a diﬀerent worker,there are 6 workersin total (3 in each layer)Colors indicate diﬀerent layer stagesGreen:dense neuralopera�onBlue:non-linearityMLPConvLenghts of blocksindicate dura�onUsuallythe longestShades indicate GNN layersLayer 1(forward)Layer 2(forward)Usuallythe fastestUsually fasterthan sparse op.Graph par��onsP1P2P3TimeTimeTimeA neural opera�onmust wait for all graphopera�ons to ﬁnish dueto poten�al dependenciesInter-layer synchroniza�onis eliminated and a workercan start a graph opera�onon its par��on as soon asit ﬁnishes a non-linearityin the previous GNN layerA worker processingits graph par��onstarts a neuralopera�on withoutwai�ng for graphopera�ons to beﬁnished on otherpar��onsCombining asynchronicityin both micro-pipelinesand in macro-pipelinescomes with even morepoten�al for increasedcomputa�on throughputDevices / workersDevices / workerscould start right after ﬁnishing the graph operation on P1.

The exact beneﬁts from micro-pipelining in depth de-
pend on a concrete GNN model. Assuming a simple GCN,
the four operations listed above take, respectively, O(log d),
O(1), and O(1) depth. Thus, as Aggregate takes asymptoti-
cally more time, one could replicate the remaining stages, in
order to make the pipeline balanced.

4.2.2 Macro-Pipeline Parallelism

In macro-pipeline parallelism, pipeline stages are GNN
layers. Such pipelines are subject to intense research in
traditional deep learning, with designs such as GPipe [116],
PipeDream [164], or Chimera [144]. However, pipelining
GNN layers is more difﬁcult because of dependencies be-
tween data samples, and it is only in its early development
stage [197]. In Figure 14, the execution is fully pipelined, i.e.,
all layers are processed by different workers.

There is an interesting difference between GNN macro-
pipelines and the traditional ANN pipelines. Speciﬁcally,
in the latter, the data is only needed at the pipeline begin-
ning. In the former, the data (i.e., the graph structure) is
needed at every GNN layer.

4.2.3 Asynchronous Pipelining

In asynchronous pipelining, pipeline stages proceed with-
out waiting for the previous stages to ﬁnish [164]. This
notion can be applied to both micro- and macro-pipelines
in GNNs. First, in asynchronous micro-pipelines, a worker
processing its graph partition starts a neural operation
without waiting for graph operations to be ﬁnished on
other partitions (Figure 14, top-right panel). Second, in asyn-
chronous macro-pipelines, the inter-layer synchronization
is eliminated and a worker can start a graph operation on
its partition as soon as it ﬁnishes a non-linearity in the
previous GNN layer (Figure 14, bottom-left panel). Finally,
both forms can be combined, see Figure 14 (bottom-right).
Note that asynchronous pipelining can be used with both
graph partitions (i.e., asynchronous processing of different
graph partitions) and with mini-batches (i.e., asynchronous
processing of different mini-batches).

4.2.4 Theoretical Formulation of Arbitrarily Deep Pipelines

To understand GNN pipelining better, we ﬁrst provide a
variant of Eq. (8), namely Eq. (2), which deﬁnes a syn-
chronous Message-Passing GNN execution with graph partition
parallelism. In this equation, we explicitly illustrate that,

15

when computing Aggregation (⊕) of a given vertex i, some
of the aggregated neighbors may come from “remote” graph
partitions, where i does not belong; such i’s neighbors
form a set N R(i). Other neighbors come from the same
“local” graph partition, forming a set N L(i). Note that
N R(i) ∪ N L(i) = N (i). Moreover, in Eq. (2), we also
explicitly indicate the current training iteration t in addi-
tion to the current layer l by using a double index (t, l).
Overall, Eq. (2) describes a synchronous standard execution
because, to obtain a feature vector in the layer l and in the
training iteration t, all used vectors come from the previous
layer l − 1, in the same training iteration t.

ψ , T R

Different forms of staleness and asynchronicity can be
introduced by modifying the layer indexes so that they
“point more to the past”, i.e., use stale feature vectors from
past layers. For this, we generalize Eq. (2) into Eq. (3) by
incorporating parameters to fully control the scope of such
staleness. These parameters are Lφ (controlling the staleness
of i’s own previous feature vector), LL
ψ (controlling the
staleness of feature vectors coming from i’s local neigh-
bors from i’s partition), and LR
ψ (controlling the staleness
of feature vectors coming from i’s remote neighbors in
other partitions). Moreover, to also allow for staleness and
asynchronicity with respect to training iterations, we introduce
the analogous parameters Tφ, T L
ψ . We then deﬁne the
behavior of Eq. (3) such that these six parameters upper
bound the maximum allowed staleness, i.e., Eq. (3) can use
feature vectors from past layers/iterations at most as old as
controlled by the given respective index parameters.
Now, ﬁrst observe that when setting Lφ = LL

ψ = 1
and Tφ = T L
ψ = 0, we obtain the standard syn-
chronous equation (cf. Figure 14, top-left panel). Setting
any of these parameters to be larger than this introduces
staleness. For example, PipeGCN [206] proposes to pipeline
communication and computation between training iterations
in the GCN model [128] by using T R
ψ = 1 (all other parame-
ters are zero). This way, the model is allowed to use stale fea-
ture vectors coming from remote partitions in previous training
iterations, enabling communication-computation overlap (at
the cost of somewhat longer convergence). Another option
would be to only set LR
ψ = 2 (or to a higher value).
This would enable asynchronous macro-pipelining, because one
does not have to wait for the most recent GNN layer to
ﬁnish processing other graph partitions to start processing
its own feature vector. We leave the exploration of other

ψ = T R

ψ = LR

Standard computation with graph partition parallelism:

h(t,l)

i = φ


h(t,l−1)

i

,

(cid:77)

(cid:16)

ψ

h(t,l−1)

i

, h(t,l−1)
j

(cid:17) (cid:77)

(cid:16)

ψ

h(t,l−1)

i

, h(t,l−1)
j

j∈N L(i)

j∈N R(i)

(cid:17)





Using bounded stale feature vectors with graph partition parallelism (worst case):

h(t,l)

i = φ


h(t−Tφ,l−Lφ)

i

,

(cid:77)

(cid:18)

ψ

h(t−Tφ,l−Lφ)
i

(t−T L
, h
j

ψ)
ψ ,l−LL

j∈N L(i)

(cid:19) (cid:77)

(cid:18)

ψ

j∈N R(i)

h(t−Tφ,l−Lφ)
i

(t−T R
, h
j

ψ )
ψ ,l−LR

(cid:19)





(2)

(3)

Fig. 15: (§ 4.2.3) Message Passing GNN formulation that includes graph partition parallelism combined with fully synchronous
(top) and potentially stale asynchronous computation (bottom). The equation generalizes the Message Passing formulation [91]
and past synchronous GCN models [206].

asynchronous designs based on Eq. (3) for future work.

5.1.1 Single-Machine Architectures

16

Standard computation:
∇h(t,l)

(cid:88)

j =

∇h(t,l+1)
i

(cid:88)

+

∇h(t,l+1)
i

i∈V :
j∈N L(i)

i∈V :
j∈N R(i)

Using bounded stale gradients (worst case):
(cid:88)

(cid:88)

(t−T L,l+LL)
∇h
i

+

∇h(t,l)

j =

(4)

(t−T R,l+LR)
∇h
i

(5)

i∈V :
j∈N L(i)

i∈V :
j∈N R(i)

Fig. 16: (§ 4.2.3) Generalization of computing gradients in
GNNs to include graph partition parallelism combined with
fully synchronous (top) and potentially stale asynchronous
computation (bottom).

Finally, we also obtain the equivalent formulations for
the asynchronous computation of stale gradients, see Fig-
ure 16. This establishes a similar approach for optimizing
backward propagation passes.

4.2.5 Beyond Micro- and Macro-Pipelining

We note that the above two forms of pipelining do not
necessarily exhaust all opportunities for pipelined execution
in GNNs. For example, there is extensive work on parallel
pipelined reduction trees [107] that could be used to further
accelerate the Aggregate operator ((cid:76)).

4.3 Artiﬁcial Neural Network (ANN) Parallelism

in some GNN models,

Finally, note that
for example
GIN [226], the dense UpdateVertex or UpdateEdge kernels
may be MLPs. These MLPs can be parallelized using the
traditional deep learning approaches [16]. Speciﬁcally, we
distinguish two such approaches: ANN-model parallelism
(parallel processing of MLP parameters within one layer)
and ANN-pipeline parallelism (parallel pipelined process-
ing of consecutive MLP layers), cf. Figure 7.

4.4 Other Forms of Parallelism in GNNs

One could identify other forms of parallelism in GNNs.
First, by combining model and data parallelism, one obtains
– as in traditional deep learning – hybrid parallelism [132].
More elaborate forms of model parallelism are also possible.
An example is Mixture of Experts (MoE) [154], in which
different models could be evaluated in parallel. Currently,
MoE usage in GNNs is in its infancy [109], [252].

5 FRAMEWORKS, ACCELERATORS, TECHNIQUES

We ﬁnally analyze existing GNN SW frameworks and HW
accelerators2. For this, we ﬁrst describe parallel and dis-
tributed architectures used by these systems.

5.1 Parallel and Distributed Computing Architectures

There are both single-machine (single-node, often shared-
memory) or multi-machine (multi-node, often distributed-
memory) GNN systems.

2We encourage participation in this analysis. In case the reader possesses addi-
tional relevant information, such as important details of systems not mentioned
in the current paper version, the authors would welcome the input.

Multi- or manycore parallelism is usually included in
general-purpose CPUs. Graphical Processing Units (GPUs)
offer massive amounts of parallelism in a form of a large
number of simple cores. However, they often require the
compute problems to be structured so that they ﬁt the
“regular” GPU hardware and parallelism. Moreover, Field
Programmable Gate Arrays (FPGAs) are well suited for
problems that easily form pipelines. Finally, novel proposals
include processing-in-memory (PIM) [29], [163] that brings
computation closer to data.

GNNs feature both irregular operations that are “sparse”
(i.e., entailing many random memory accesses), such as
reductions over neighborhoods, and regular “dense” op-
erations, such as transformations of feature vectors, that
are usually dominated by sequential memory access pat-
terns [1]. The latter are often suitable for effective GPU
processing while the former are easier to be processed
effectively on the CPU. Thus, both architectures are highly
relevant in the context of GNNs. Our analysis (Table 8, the
top part) indicates that they are both supported by more
than 50% of the available GNN processing frameworks.
We observe that most of these designs focus on executing
regular dense GNN operations on GPUs, leaving the ir-
regular sparse computations for the CPU. While being an
effective approach, we note that GPUs were successfully
used to achieve very high performance in irregular graph
processing [185], and they thus have high potential for also
accelerating sparse GNN operations.

There is also interest in HW accelerators for GNNs
(Table 8, the bottom part). Most are ASIC proposals (some
are evaluated using FPGAs); several of them incorporate
PIM. With today’s signiﬁcance of heterogeneous comput-
ing, developing GNN-speciﬁc accelerators and using them
in tandem with mainstream architectures is an important
thread of work that, as we predict, will only gain more
signiﬁcance in the foreseeable future.

5.1.2 Multi-Machine Parallelism

While shared-memory systems are sufﬁcient for processing
many datasets, a recent trend in GNNs is to increase the
size of input graphs [110], which often requires multi-node
settings to avoid expensive I/Os. We observe (Table 8, the
top part) that different GNN software frameworks support
distributed-memory environments. However, the majority
of them focus on training, leaving much room for devel-
oping efﬁcient distributed-memory frameworks and tech-
niques for GNN inference. We also note high potential in
incorporating high-performance interconnect related mech-
anisms such as Remote Direct Memory Access (RDMA) [87],
SmartNICs [28], [74], [106], or novel network topologies and
routing [26], [34] into the GNN domain.

5.2 General Categories of Systems & Design Decisions

The ﬁrst systems supporting GNNs usually belonged to one
of two categories. The ﬁrst category are systems constructed
on top of graph processing frameworks and paradigms
that add neural network processing capabilities (e.g., Neu-
graph [153] or Gunrock [213]). On the contrary, systems
in the second category (e.g., the initial PyG design [79])
start with deep learning frameworks, and extend it towards

Reference

Arch.

Ds? T?

I? Op? mp? Mp? Dp?

Dpp

PM

Remarks

17

LC

sh
sh

sh
sh

sh
sh

Focus on using disk

Ø Ø (cid:150) (v, sn)

LC (SU)
—
—

(cid:150) (cid:150) (f) Ø Ø (cid:228)

∗Only two servers used.

CPU+GPU (cid:150) (cid:150) (fb) Ø (cid:150) (cid:150) Ø (cid:150)
[SW] PipeGCN [206]
(cid:228) Ø (cid:150)
(cid:150) (cid:150) (fb) Ø (cid:228)
[SW] BNS-GCN [205]
GPU
(cid:150) (cid:150)
(cid:228) (cid:228) (cid:150)
(cid:150) (cid:228)
[SW] PaSca [243]
GPU
Ø (cid:150) (mb) Ø (cid:150) (v) (cid:228) Ø (cid:150)
[SW] Marius++ [204]
CPU
(cid:150) (cid:150) (mb) Ø (cid:150) (cid:150) Ø (cid:150)
[SW] BGL [151]
GPU
[SW] DistDGLv2 [248] CPU+GPU (cid:150) (cid:150) (mb) Ø (cid:150) (cid:150) Ø (cid:150)
(cid:150) (cid:150) (fb) Ø Ø
Ø Ø (cid:150)
[SW] SAR [159]
CPU
(cid:150) (cid:150) (fb) Ø (cid:228)
Ø Ø (cid:150) (v)
[SW] DeepGalois [105] CPU
LC (AU)
LC( AU) (cid:228)
(cid:150) (cid:150) (fb) Ø (cid:228)
Ø Ø (cid:150) (v)
[SW] DistGNN [155]
CPU
(cid:152)∗ (cid:150)
Ø (cid:228)
Ø Ø (cid:150) (v)
[SW] DGCL [54]
LC (AU)
GPU
Ø (cid:150)
Ø (cid:150) (f) Ø Ø (cid:150) (v, t)
[SW] Seastar [222]
LC (VC)
GPU
Ø
(cid:150) (cid:150) (fb) Ø Ø
[SW] Chakaravarthy [56] GPU
Ø
Ø Ø
[SW] Zhou et al. [249]
CPU
(cid:152)∗ (cid:150) (fb) Ø (cid:150) (f) Ø Ø (cid:150) (v)
[SW] MC-GCN [12]
GL
GPU
Ø (cid:150) (cid:150) (v)
(cid:150) (cid:150) (fb) Ø Ø
[SW] Dorylus [197]
LC (SAGA)
CPU
(cid:152)∗ (cid:150) (mb) Ø (cid:228)
Ø (cid:150) (cid:150) (v)
[SW] Min et al. [156]
GL
GPU
(cid:150) (cid:150) (f, s) Ø Ø (cid:228)
Ø (cid:150)
[SW] GNNAdvisor [215] GPU
GL, LC
Ø (cid:228) (cid:150)
(cid:150) (cid:228)
(cid:150) (cid:150)
[SW] AliGraph [255]
LC (NAU)
CPU
(cid:150) (cid:150) (fb) Ø (cid:150) (s) (cid:228) Ø (cid:150)
[SW] FlexGraph [208]
LC (NAU)
CPU
Ø (cid:150) (mb) Ø (cid:150) (s) (cid:228) Ø (cid:150)
[SW] Kim et al. [125]
LC (AU)
CPU+GPU
(cid:150) (cid:150) (cid:150)
(cid:150) (cid:150) (mb) (cid:150) Ø
[SW] AGL [237]
MapReduce
CPU
Ø
Ø Ø (cid:150)
CPU+GPU (cid:150) (cid:150) (fb) (cid:150) Ø
[SW] ROC [117]
Ø
Ø Ø (cid:150)
(cid:150) (cid:150) (mb) Ø (cid:228)
[SW] DistDGL [247]
CPU
(cid:152)∗ (cid:150) (mb) Ø (cid:228)
Ø
Ø Ø (cid:150)
[SW] PaGraph [10], [149] GPU
(cid:150) (cid:228) (cid:150)
(cid:150) (cid:150) (mb) Ø (cid:228)
[SW] 2PGraph [240]
—
GPU
(cid:150) (cid:150) (mb) Ø (cid:228)
Ø (cid:228) (cid:150)
[SW] GMLP [242]
LC
GPU
LC (AU)∗
Ø (cid:150) Ø Ø (cid:150)
Ø (cid:150)
[SW] fuseGNN [64]
GPU
LC (SAGA)∗ ∗A variant called P-TAGS
[SW] P3 [81]
CPU+GPU (cid:150) (cid:150) (mb) Ø (cid:150) (cid:150) Ø (cid:150)
Ø (cid:152) (cid:150)
[SW] QGTC [214]
GL
GPU
CPU+GPU (cid:150) (cid:150) (fb) Ø (cid:150) (f, s) Ø Ø (cid:150) (v, e) sh+rep GL
[SW] CAGNET [199]
[SW] PCGCN [198]
—
CPU+GPU
[SW] FeatGraph [111]
GL
CPU, GPU
[SW] G3 [150]
GL
GPU
[SW] NeuGraph [153]
LC (SAGA)
GPU
[SW] PyTorch-Direct [79] GPU
GL, LC
[SW] PyG [79]
GL, LC
[SW] DGL [209]
GL, LC

(cid:150) (cid:228)
(cid:228) (cid:228) (cid:228)
Ø (cid:152) (cid:150) Ø (cid:150) (v, e) sh
(cid:150) (cid:150) (cid:228) (cid:228) (cid:150) (v, e)
CPU, GPU (cid:150) (cid:150)∗ (cid:150) (cid:150) (cid:228) (cid:228) (cid:150) (v, e)
CPU, GPU (cid:150) (cid:150)∗ (cid:150) (cid:150) (cid:228) (cid:152) (cid:150)

Ø (cid:150)
Ø Ø (cid:150) (e)
Ø (cid:150) (fb) (cid:150) (cid:150) (f, s) Ø Ø (cid:150) (v)
Ø (cid:150)
(cid:150) (cid:150)
(cid:150) (cid:150)

Ø (cid:150)

Ø (cid:228)

Ø Ø

sh
sh

sh

∗Multi-GPU within one node.

∗Multi-GPU within one node.

∗Multi-GPU within one node.

∗Two aggregation schemes are used.

∗Mini-batching for graph components
∗Mini-batching for graph components

(cid:150) (cid:152) (cid:150) Ø (cid:150)
(cid:150) (cid:152) (cid:150) Ø (cid:228)

Ø Ø
(cid:150) (cid:152) (cid:150) Ø (cid:150) (v, e) sh
[HW] ZIPPER [245]
new
Ø (cid:150) (fb) Ø (cid:150) (cid:150) Ø (cid:150) (v, e) sh
[HW] GCNear [253]
new (PIM)
Ø (cid:150)
[HW] BlockGNN [254]
new
new (ReRAM) Ø Ø
[HW] TARe [103]
[HW] Rubik [62]
new
[HW] GCNAX [141]
new
[HW] Li et al. [140]
new
[HW] GReTA [126]
new
[HW] GNN-PIM [217]
new (PIM)
[HW] EnGN [145]
new
[HW] HyGCN [228]
new
[HW] AWB-GCN [85]
new
[HW] GRIP [127]
new
[HW] Zhang et al. [235] new
[HW] GraphACT [233]
new
[HW] Auten et al. [7]
new

Ø (cid:150) (mb) Ø (cid:152) (cid:228) Ø (cid:150) (v, e) sh
Ø Ø
Ø Ø
Ø Ø
Ø Ø
Ø Ø
Ø Ø
Ø Ø
Ø Ø
Ø Ø
Ø (cid:150) (mb) Ø (cid:152) (cid:150) Ø (cid:228)
(cid:228) (cid:228) (cid:228)
Ø Ø

(cid:150) (cid:152) (cid:228) Ø (cid:228)
(cid:150) (cid:150) (cid:150) (cid:150) (cid:150)
(cid:228) (cid:228) (cid:150)
(cid:150) (cid:228)
sh
(cid:150) (cid:228)
(cid:150) Ø (cid:150) (v)
sh
(cid:150) (cid:152) (cid:228) Ø (cid:150) (v, e) sh
(cid:150) (cid:150) (cid:150) Ø (cid:150) (v, e) sh
(cid:150) (cid:152) (cid:150) (cid:150) (cid:150) (v, e) sh
(cid:150) (cid:228)
(cid:150) (cid:152) (cid:150) (v, e) sh
(cid:150) (cid:150) (cid:150) (cid:152) (cid:150) (v, e) sh

(cid:150) (cid:228)

GL, LC
LC (AU)
—
GL
LC (AU)
GL
LC (AU)
LC (GReTA)
LC (SAGA)
LC (AU + “feature extraction” stage)
LC (AU)
GL
GL, LC (GReTA)
GL
GL
LC (AU)

TABLE 8: Comparison of GNN processing frameworks and analyses of GNN implementations. They are grouped by the
targeted architecture. Within each group, the systems are sorted by publication date. “[SW]”: a software framework or package,
“[HW]”: a hardware accelerator. “—”: not relevant for a given system. “Ds?” (distributed memory): does a design target
distributed environments such as clusters, supercomputers, or data centers? “Arch.”: targeted architecture. “new”: a new proposed
architecture. “T”: Focus on training? (if more details are provided, we distinguish between “fb”: full batch, and “mb”: mini
batch). “I”: Focus on inference? “Op”: Support for operator parallelism (“f”: feature parallelism, “s”: structure parallelism)?
“mp”: Support for micro-pipeline parallelism? “Mp”: Support for macro-pipeline parallelism? “Dp”: Support for graph partition
parallelism (if more details are provided, we distinguish between “v”: vertex partitioning (also called 1D partitioning), “e”: edge
partitioning (also called 2D partitioning), “t”: type partitioning (in heterogeneous GNNs, where a vertex can have multiple types),
or “sn”: snapshot partitioning (in dynamic GNNs, where a graph can be stored in multiple snapshots)). “Dpp”: data partitioning
policy (if more details are provided, we distinguish between “sh”: sharding, and “rep”: replication). “PM”: Used programming
model or paradigm: “GL”: Global, linear algebra based, focusing on operations on matrices such as SpMM or GEMM, “LC”: Fine
Grained, focusing on operations of graph elements, such as neighborhood aggregation. If more details are provided, we further
distinguish “AU”: Aggregate + Update. “NAU”: Neighborhood selection + Aggregate + Update. “SAGA”: Scatter + ApplyEdge
(i.e., Update Edge) + Gather + ApplyVertex (i.e., Update Vertex). “GReTA”: Gather + Reduce (i.e., Aggregate) + Transform (i.e.,
Update) + Activate. “(cid:150)”: Support. “(cid:152)”: Partial / limited support. “Ø”: No support. “(cid:228)”: Unknown.

graph processing capabilities. These two categories usually
focus on – respectively – the LC and GL formulations and
associated execution paradigms.

The third, most recent, category of GNN systems, does
not start from either traditional graph processing or deep
learning. Instead,
they target GNN computations from
scratch, focusing on GNN-speciﬁc workload characteristics
and design decisions [113], [171], [172], [215]. For exam-
ple, Zhang et al. [238] analyze the computational graph of
GNNs, and propose optimizations tailored speciﬁcally for
GNNs.

5.3 Parallelism in GNN Systems

The most commonly supported form of parallelism is graph
partition parallelism. Here, one often reuses a plethora
of established techniques from the graph processing do-
main [52]. Unsurprisingly, most schemes used for graph
partitioning are based on assigning vertices to workers (“1D
partitioning”). This is easy to develop, but comes with
challenges related to load balancing. Better load balancing
properties can often be obtained when also partitioning each
neighborhood among workers (“2D partitioning”). While
some frameworks support this variant, there is potential
for more development into this direction. We also observe
that most systems support sharding, attacking a node or
edge classiﬁcation/regression scenario with a single large
input graph. Here, CAGNET [199] combines sharding with
replication, in order to accelerate GNN training by commu-
nication avoidance [189], a technique that have been used to
speed up many linear algebra based computations [30].

The majority of works use graph partition parallelism
on its own, to alleviate large sizes of graph inputs (by
distributing it over different memory resources) and to ac-
celerate GNN computations (by parallelizing the execution
of one GNN layer). Some systems ZIPPER [245] combine
graph partition parallelism with pipelining, offering further
speedups and reductions in used storage resources.

Operator parallelism is supported by the majority of
systems. Both feature parallelism and structure parallelism
have been investigated, and there are systems supporting
both (FeatGraph [111], GNNAdvisor [215], CAGNET [199]).
Most of these systems target these forms of parallelism
explicitly (e.g., by programming binary reduction trees
processing Reduce). For example, Seastar [222] focuses on
combining three types of parallel execution (feature-level,
vertex-level, edge-wise). On the other hand, CAGNET is an
example design that supports operator parallelism implic-
itly, by incorporating 2D and 3D distributed-memory matrix
products and the appropriate partitioning of A and H
matrices. We note that existing works often refer to operator
parallelism differently (e.g., intra-phase dataﬂow” [82]).

Micro-pipeline parallelism is widely supported by HW
accelerators. We conjecture this is because it is easier to use a
micro-pipeline in the HW setting, where there already often
exist such pipeline dedicated HW resources.

Macro-pipeline parallelism is least often supported.
This is caused by its complexity: one must consider how
to pipeline the processing of interrelated nodes, edges, or
graphs, across GNN layers. While it is relatively straight-
forward to use pipeline parallelism when processing graph
samples in the context of graph classiﬁcation or regression,

18

most systems focus on the more complex node/edge clas-
siﬁcation or regression. Here, two examples are GRIP [127]
and work by Zhang et al. [235], where pipelining is enabled
by simply loading the weights of the next GNN layer, while
the current layer is being processed.

5.4 Optimizations in GNN Systems

We also summarize parallelism related optimizations.

Frameworks that enable data parallelism use different
forms of graph partitioning. The primary goal is to min-
imize the edges crossing graph partitions in order to re-
duce communication. Here, some designs (e.g., DeepGa-
lois [105], DistGNN [155]) use vertex cuts. Others (e.g.,
DGCL [54], QGTC [214])
incorporate METIS partition-
ing [122]. ROC [117] proposes an interesting scheme, in
which it uses online linear regression to effectively reparti-
tion the graph across different training iterations, minimiz-
ing the execution time.

There are numerous other schemes used for reducing
communication volume. For example, DeepGalois [105]
and DGCL [54] use message combination and aggregation,
DGCL also balances loads on different interconnect links,
DistGNN [155] delays updates to optimize communication,
Min et al. [156] use zero copy in the CPU–GPU setting (GPU
threads access host memory without requiring much CPU
interaction) and computation & communication overlap,
and GNNAdvisor [215] and 2PGraph [240] renumber nodes
to achieve more locality [194]. GNNAdvisor also provides
locality-aware task scheduling [114]. 2PGraph also increases
the amount of locality with cluster based mini-batching (it
increases the number of sampled vertices that belong to the
same neighborhood in a mini-batch), a scheme similar to the
approach taken by the Cluster-GCN model [65].

Moreover, there are many interesting operator related
optimizations. A common optimization is operator fusion
(sometimes referred to as “kernel fusion” [64]), in which
one merges the execution of different operators, for example
Aggregate and UpdateVertex [54], [64], [209], [222]. This
enables better on-chip data reuse and reduces the number
of invoked operators. While most systems fuse operators
within a single GNN layer, QGTC offers operator fusion also
across different GNN layers [214]. Other interesting schemes
include operator reorganization [238], in which one ensures
that operators ﬁrst perform neural operations (e.g., MLP)
and only then the updated feature vectors are propagated.
This limits redundant computation.

Many systems come with optimizations related to the
used graph representation. For example, PCGCN [198]
provides a hybrid representation, in which it uses CSR to
maintain sparse parts of the adjacency matrix, and dense
bitmaps for dense edge blocks. Further, many designs focus
on ensuring cache friendlines: DistGNN [155] uses tiling (re-
ferred to as “blocking”), PaGraph [149] and 2PGraph [240]
provide effective feature caching, and Lin et al. [146] pro-
pose lossy compression to ﬁt into GPU memory.

Some systems hybridize various aspects of GNN com-
puting. For example, Dorylus [197] executes graph-related
sparse operators (e.g., Aggregate) on CPUs, while dense ten-
sor related operators (e.g., UpdateVertex) run on Lambdas.
fuseGNN [64] dynamically switches between incorporated
execution paradigms: dense operators such as UpdateVertex

are executed with the GL paradigm and the corresponding
GEMM kernels, while sparse computations such as Aggre-
gate use the graph-related paradigms such as SAGA.

Many other schemes exist. For example, Zhang et
al. [238] reduce memory consumption (intermediate data are
recomputed in the backward pass, instead of being cached),
while He et al. [102] incorporate serverless computing [67]
for more efﬁcient GNNs.

5.5 Analyses and Evaluations of GNN Systems

Finally, there are several works dedicated to analyses and
evaluations of various techniques. Garg et al. [82] inves-
tigate different strategies for mapping sparse and dense
GNN operators on various HW accelerators. Seraﬁni et
al. [181] compare sample-based and full-graph trainig. Yan
et al. [227] analyze the performance of GCNs on GPUs.
Wang et al. [218] and Zhang et al. [246] extend this analysis
to a wider choice of GNN models. Baruah et al. [13] intro-
duce GNNMark, a benchmarking suite for GNNs on GPUs.
Tailor et al [193] analyze the performance of point cloud
GNNs. Qiu et al. [171] analyze the impact of sparse matrix
data format on the performance of GNN computations.

6 OTHER CATEGORIES & ASPECTS OF GNNS

We also brieﬂy discuss other categories, variants, and as-
pects of GNN models.

6.1 Classes of Local GNN Models
Depending on the details of (cid:76), ψ, and φ, one distinguishes
three GNN classes [169]: Convolutional GNNs (C-GNNs), At-
tentional GNNs (A-GNNs), and Message-Passing GNNs (MP-
GNNs). Each class is deﬁned by an equation specifying the
feature vector h(l+1)
of a vertex i in the next GNN layer
l + 1. The three equations to update a vertex are as follows:

i

h(l+1)
i

= φ

h(l+1)
i

= φ

h(l+1)
i

= φ


h(l)

i

,


h(l)

i

,


h(l)

i

,

(cid:77)

cij · ψ

(cid:16)

h(l)
j

j∈N (i)

(cid:17)





(cid:77)

(cid:16)

a

h(l)
i

, h(l)
j

(cid:17)

· ψ

(cid:16)

h(l)
j

j∈N (i)

(cid:77)

(cid:16)
h(l)
i

, h(l)
j

(cid:17)

ψ





j∈N (i)

(cid:17)





(C-GNNs)

(6)

(A-GNNs)

(7)

(MP-GNNs)

(8)
The overall difference between these classes lies in the
expressiveness of operations acting on edges. C-GNNs
enable ﬁxed precomputed scalar edge weights cij, A-
GNNs enable scalar edge weights that may be the result
of arbitrarily complex operations on learnable parame-
ters a(hi, hj), and MP-GNNs enable arbitrarily complex
edge weights ψ(hi, hj).

Speciﬁcally, in C-GNNs, Eq. (6), the feature vector h(l)
j
of each neighbor j is ﬁrst transformed using a function ψ.
The outcome is then multiplied by a scalar cij. Importantly,
cij is a value that is ﬁxed, i.e., it is known upfront (before
the GNN computation starts), and it does not change across
iterations. Example GNN models that fall into the class of
C-GNNs are GCN [128] or SGC [219].

In A-GNNs, Eq. (7), h(l)
j

is also ﬁrst transformed with
a function ψ. However, the outcome is then multiplied

19

(cid:16)
h(l)
i

, h(l)
j

(cid:17)

by a value a
which is no longer precomputed.
Instead, a is a parameterized function of the model and is
obtained during training. Importantly, while the derivation
of this weight can be arbitrarily complex (e.g., the weight
can be computed via attention), the edge weight itself is
a scalar. Overall, A-GNNs come with richer expressive
capabilities and can be used with models such as GAT [202]
or Gated Attention Network (GaAN) [239].

Finally, MP-GNNs are the most complex LC class, with
edge weights ψ that can be arbitrary vectors or even more
complex objects, as speciﬁed by ψ. Example MP-GNN mod-
els are G-GCN [47] or different models under the umbrella
of the Message Passing paradigm (MPNN) [91].

Explicit division of GNNs models into three classes (C-
GNNs, A-GNNs, MP-GNNs) has its beneﬁts, as it makes it
easier to apply various optimizations. For example, when-
ever handling a C-GNN model, one can simply precompute
edge weights instead of deriving them in each GNN layer.

We illustrate generic work and depth equations in Fig-
ures 17-18. Overall, work is the sum of any preprocessing
costs Wpre, post-processing costs Wpost, and work of a
single GNN layer Wl times the number of layers L. In the
MP-GNN generic formulation, Wl equals to work needed to
evaluate ψ for each edge (mWψ), (cid:76) for each vertex (nW⊕),
and φ for each vertex (nWφ). In C-GNNs, the preprocessing
cost belongs to Wpre and Dpre. In A-GNNs, one needs to
also consider the work and depth needed to obtain the
coefﬁcients a. Depth is analogous, with the main difference
that the depth of a single GNN layer is a plain sum of depths
of computing ψ, (cid:76), and φ (each function is evaluated in
parallel for each vertex and edge, hence no multiplication
with n or m).

Generic = Wpre + LWl + Wpost

MP-GNNs = Wpre + L · (cid:0)mWψ + nW⊕ + nWφ
C-GNN = Wpre + L · (cid:0)mWψ + nW⊕ + nWφ
A-GNNs = Wpre + L · (cid:0)mWψ + mWa + nW⊕ + nWφ

(cid:1) + Wpost
(cid:1) + Wpost

(cid:1) + Wpost

Fig. 17: Generic equations for work in GNN model classes.

Generic = Dpre + LDl + Dpost

MP-GNNs = Dpre + L · (cid:0)Dψ + D⊕ + Dφ
C-GNN = Dpre + L · (cid:0)Dψ + D⊕ + Dφ
A-GNNs = Dpre + L · (cid:0)Dψ + Da + D⊕ + Dφ

(cid:1) + Dpost
(cid:1) + Dpost

(cid:1) + Dpost

Fig. 18: Generic equations for depth in GNN model classes.

6.2 Reductions over H-Hop Neighborhoods

The LC formulations described by Eq. (6)–(8) enable reduc-
tions over H-hop neighborhoods, where H > 1. This would
enable expressing models such as MixHop [2] or SGC [219].
Such H-hop reductions are similar to using polynomial
powers of the adjacency matrix in the GL formulation,
i.e., they also enable using the knowledge from regions of
the graph located further than the direct 1-hop neighbors,
within one GNN layer. In terms of parallelization, the main
difference from reductions over 1-hop neighborhoods is

that the number of vertices being reduced may be much
larger (up to n), which means that the work and depth of
such a reduction become O(n) and O(log n), respectively.
Simultaneously, such reductions would require large pre-
processing costs, i.e., one needs to derive (and maintain) the
information of H-hop neighbors for each vertex.

6.3 Imposing Vertex Order
Nearly all considered GNN models assume that (cid:76) is
permutation invariant. However, some GNN models, such
as PATCHY-SAN [165], impose explicit vertex ordering. In
such models, the outcome of (cid:76) would be different based
on the order of the input vertices. This still enables paral-
lelism: analogously to the established parallel preﬁx sum
problem [40], one could compute such a (cid:76) in O(log d)
depth and O(d) work (for d input vertices) – assuming that
(cid:76) is associative. However, there may be constant overhead
of 2× for both depth and work, compared to the case where
(cid:76) is permutation-invariant.
6.4 Heterogeneous GNNs

Heterogeneous graphs (HetG) [32], [53], [184], [211], [229]
generalize simple graphs deﬁned as a tuple (V, E) (cf. Sec-
tion 2) in that vertices and edges may have arbitrary types
(e.g., person, paper, journal) and attributes (e.g., age, page
count, ranking). There has been research into HetG learning,
with recent models such as Heterogeneous Graph Neu-
ral Network [236], Heterogeneous Graph Attention Net-
work [212], or Heterogeneous Graph Transformer [112].
Computations on such models can beneﬁt from forms of
parallelism discussed in this work. However, in addition
to this, they also come with potential for novel forms of
parallelism. For example, in HetG, one often uses different
adjacency matrices to model edges of different types. Such
matrices could be processed in parallel, introducing type
parallelism. Similarly, different attributes can also be pro-
cessed in parallel, offering attribute parallelism.

6.5 Dynamic and Temporal GNNs

Another line of works addresses dynamic graph neural
networks [223] with potential for snapshot parallelism.
Speciﬁcally, one may parallelize the processing of different
snapshots of a graph, taken at different time points [56]. Such
snapshots can be processed with both the FG and the GL
approach. This introduces potential for new optimizations,
such as using different approaches with different snapshots
based on their sparsity properties.

6.6 Hierarchical GNNs

Some GNN models are hierarchical,
i.e., one deals with
different input granularities in the same training and in-
ference process. For example, in the SEAL-C and SEAL-AI
models [142], one is primarily interested in graph classiﬁca-
tion. However, before running training and inference using
graphs as data samples, the authors ﬁrst execute GNNs
separately on each graph, using nodes as data samples.
In this process, each graph obtains an embedding vector,
that is then used as input for the training process in graph
classiﬁcation. Such hierarchical approach comes with oppor-
tunities for more elaborate forms of parallelism, for example
parallel pipelining of stages of computations belonging to
the different hierarchy levels.

20

6.7 Spectral GNN Models

GL formulations of GNNs can be spatial or spectral; the
difference is in using either the adjacency or the Laplacian
matrix. As Chen et al. [63] shows, one can transform a
spatial formulation into a spectral one, and vice versa. For
simplicity, we focus on spacial formulations.

6.8 Preprocessing in GNNs

2 (cid:101)A (cid:101)D− 1

There are different forms of preprocessing in GNNs. First,
different GNN models often require preprocessing the ad-
jacency matrix and the degree matrix: by incorporating self-
loops ( (cid:101)A = A + I, (cid:101)D = D + I), with symmetric normalization
(A(cid:48) = (cid:101)D− 1
2 ), or with random-walk normalization (A =
D−1A). Second, some spectral GNN models often require a
spectral decomposition of the Laplacian matrix [63]. Third,
many GNN works propose to reduce or even eliminate
using several GNN layers, and instead push the bulk of
computation to preprocessing. This could involve explicitly
considering multihop neighborhoods [2], [80] or polynomial as
well as rational powers of the adjacency matrix [63], [80], [219].
All these preprocessing routines come with potential for
parallel and distributed execution.

6.9 Global vs. Local Approach for GNNs

On one hand, there exist many GNN models deﬁned using
the GL approach, that cannot be easily expressed with the
LC formulation. Examples are numerous GNN models that
use rational powers of the adjacency or Laplacian matrix,
such as Auto-Regress [17], [250], [256], PPNP [43], [129],
[230], ARMA [38], ParWalks [221], or RationalNet [63]. On
the other hand, many GNN models that were deﬁned with
the LC approach, have no known GL formulations, for
example GAT [202], MoNet [158], G-GCN [47], or Edge-
Conv [216].

7 SELECTED INSIGHTS

We now summarize our insights into parallel and dis-
tributed GNNs, pointing to parts with more details.
• GNNs come with new forms of parallelism Graph par-
tition parallelism (Section 3), closely related to the graph
partitioning problem in graph processing, is more chal-
lenging than equivalent forms of parallelism in traditional
deep learning, due to the dependencies between data
samples. Another form, characteristic to GNNs, is graph
structure parallelism (§ 4.1.4, § 5.3).

• GNNs come with rich diversity of tensor operations
Different GNN models use a large variety of tensor op-
erations. While today’s works focus on the C-GNN style
of computations, that only uses two variants of matrix
products, there are numerous others, listed in Table 3 and
assigned to models in Tables 5, 6, and 7.

• Even local GNN formulations heavily use tensor opera-
tions One could conclude that efﬁcient tensor operations
are crucial only for the global GNN formulations, as these
formulations focus on expressing the whole GNN model
with matrices and operations on matrices (cf. Table 7).
However, many local GNN formulations have complex
tensor operations associated with UpdateEdge or Updat-
eVertex operators, see Tables 5 and 6.

• Both local and global GNN formulations are important
There are many GNN models formulated using the local

approach, that have no known global formulation, and
vice versa. Thus, effective parallel and distributed execu-
tion is important in both formulations.

• Local and global GNN formulations welcome different
optimizations While having similar, or often the same,
work and depth, local and global GNN formulations have
potential for different types of optimizations. For example,
global GNN models focus on operations on large matrices,
which immediately suggests optimizations related to – for
example – communication-avoiding linear algebra [136],
[188], [189]. On the other hand, local GNN models use
operators as the “ﬁrst class citizens”, suggesting that an
example important line of work would be operator par-
allelism such as the one proposed by the Galois frame-
work [133].

8 CHALLENGES & OPPORTUNITIES

Many of the considered parts of the parallel and distributed
GNN landscape were not thoroughly researched. Some were
not researched at all. We now list such challenges and
opportunities for future research.
• Efﬁcient GNN inference Most GNN frameworks focus on
training, but fewer of them also target inference. There is a
large potential for developing high-performance schemes
targeting fast inference.

• Advanced mini-batching in GNNs There is very little
work on advanced mini-batch training and GNN layer
pipelining. Mini-batch training of GNNs is by nature
complex, due to the dependencies between node, edge,
or graph samples. While the traditional deep learning
saw numerous interesting works such as GPipe [116],
PipeDream [164], or Chimera [144], that investigate mech-
anisms such as asynchronous or bi-directional mini-
batching, such works are yet to be developed for GNNs.
• Asynchronous GNNs The landscape of asynchronous
GNNs is largely unexplored. While we outline the overall
framework deﬁned in Eq. (2), (3), (4), and (5), implemen-
tations, optimizations, and analyses are missing. There
could be a plethora of works improving the GNN per-
formance by incorporating different forms of asynchrony.
• More performance in GNNs via replication Many GNN
works have explored graph partitioning. However, very
few (e.g., CAGNET [199]) uses replication for more perfor-
mance (e.g., through 2.5D & -3D matrix multiplications).
• Parallelization of GNN models beyond simple C-GNNs
There is very little work on parallel and distributed GNN
models beyond the simple seminal ones from the C-GNN
or A-GNN classes, such as GCN [128], GAT [202], or
GraphSAGE [101]. One would welcome works on more
complex models from the MP-GNN class, cf. Tables 5-6.
• Parallelization of GNN models beyond linear ones Vir-
tually no research exists on parallel and distributed GNN
models of polynomial and rational types, cf. Table 7.

• Parallelization of other GNN settings Besides very few
attempts [255], there is no work on parallelizing hetero-
geneous GNNs [236], dynamic and temporal GNNs [223],
or hierarchical GNNs [142]. We predict that parallel and
distributed schemes targeting these works will come with
a large number of research opportunities, due to the rich
diversity of these GNN models and the associated graph
related problems. One example idea would be to use the

21

available techniques from dynamic and streaming graph
processing [22] for GNNs.

• Achieving large scales A large challenge is to further
push the scale of GNN computations. When comparing
the scale and parameter counts of models such as CNNs
or Transformers with GNNs, it can be seen that there is a
large gap and a lot of research opportunities.

• New forms of parallelism It would be interesting to in-
vestigate effective utilization of other forms of parallelism
in GNNs, for example using Mixture of Experts [154].
• Incorporating new HW architectures While some initial
works exist, there are not many designs on using GNNs
with architectures such as FPGAs [35], [71], transactional
processing [27], or processing in memory [4], [29], [88],
[93], [98], [162], [163], [166], [182], [183].

• Incorporating high-performance distributed-memory ca-
pabilities CAGNET [199]
illustrated how to scalably
execute GNN training across many compute nodes. It
would be interesting to push this direction and use
high-performance distributed-memory developments and
interconnects, and the associated mechanisms for more
performance of distributed-memory GNN computations,
using – for example – RDMA and RMA programming [87],
[179], SmartNICs [28], [74], serverless computing [67],
of high-performance networking architectures [20], [24],
[26], [34]. Such techniques have been successfully used to
accelerate the related graph processing ﬁeld [191].

• Incorporating techniques from graph processing There
is more potential in using graph processing techniques
for GNNs. While many such schemes have already been
incorporated, there are many more to be tried, for example
sketching and sampling [21], [37], [90] or various forms of
approximate graph processing [8], [19], [44], [45], [59], [76],
[78], [83], [173]–[175], [186].

• Incorporating techniques from linear algebra compu-
tations A lot of work has been done into optimization
algebraic operations such as matrix products [86], [92],
[134]–[136], [188], [190]. Many of them could be applied
in the GNN setting, especially in the context of GL GNN
formulations.

9 CONCLUSION

Graph neural networks (GNNs) are one of the most impor-
tant and fastest growing parts of machine learning. Parallel
and distributed execution of GNNs is a key to GNNs achiev-
ing large scales, high performance, and possibly accuracy.
In this work, we conduct an in-depth analysis of parallelism
and distribution in GNNs. We provide a taxonomy of par-
allelism, use it to analyze a large number of GNN models,
and synthesise the outcomes in a set of insights as well as
opportunities for future research. Our work will propel the
development of next-generation GNN computations.

Acknowledgements We thank Petar Veliˇckovi´c for useful

comments.

REFERENCES

[1]

[2]

S. Abadal, A. Jain, R. Guirado, J. L ´opez-Alonso, and E. Alarc ´on.
Computing graph neural networks: A survey from algorithms to
accelerators. ACM CSUR, 2021.
S. Abu-El-Haija, B. Perozzi, A. Kapoor, N. Alipourfard, K. Ler-
man, H. Harutyunyan, G. Ver Steeg, and A. Galstyan. Mixhop:

Higher-order graph convolutional architectures via sparsiﬁed
neighborhood mixing. In international conference on machine learn-
ing, pages 21–29. PMLR, 2019.
T. Ahmad, L. Jin, X. Zhang, L. Lin, and G. Tang. Graph convo-
lutional neural network for action recognition: A comprehensive
survey. IEEE TAI, 2021.
J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi. A scalable
processing-in-memory accelerator for parallel graph processing.
In ISCA, 2015.
S. Arora. A survey on graph neural networks for knowledge
graph completion. arXiv preprint arXiv:2007.12374, 2020.
J. Atwood and D. Towsley. Diffusion-convolutional neural net-
works. In Advances in neural information processing systems, pages
1993–2001, 2016.
A. Auten, M. Tomei, and R. Kumar. Hardware acceleration of
graph neural networks. In ACM/IEEE DAC, 2020.
D. A. Bader et al. Approximating betweenness centrality.
In
Algorithms and Models for the Web-Graph, pages 124–137. Springer,
2007.
D. A. Bader, H. Meyerhenke, P. Sanders, and D. Wagner. Graph
partitioning and graph clustering, volume 588. American Mathe-
matical Society Providence, RI, 2013.

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10] Y. Bai, C. Li, Z. Lin, Y. Wu, Y. Miao, Y. Liu, and Y. Xu. Efﬁcient
data loader for fast sampling-based gnn training on large graphs.
IEEE TPDS, 2021.

[11] M. Balcilar, G. Renton, P. H´eroux, B. Gauzere, S. Adam, and
P. Honeine. Bridging the gap between spectral and spatial do-
mains in graph neural networks. arXiv preprint arXiv:2003.11702,
2020.

[12] M. F. Balın et al. Mg-gcn: Scalable multi-gpu gcn training

framework. arXiv preprint arXiv:2110.08688, 2021.

[13] T. Baruah, K. Shivdikar, S. Dong, Y. Sun, S. A. Mojumder, K. Jung,
J. L. Abell´an, Y. Ukidave, A. Joshi, J. Kim, et al. Gnnmark: A
benchmark suite to characterize graph neural network training
on gpus. In IEEE ISPASS, 2021.

[14] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez,
V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro,
R. Faulkner, et al. Relational inductive biases, deep learning, and
graph networks. arXiv preprint arXiv:1806.01261, 2018.

[15] T. Ben-Nun, M. Besta, S. Huber, A. N. Ziogas, D. Peter, and
T. Hoeﬂer. A modular benchmarking infrastructure for high-
In IEEE IPDPS,
performance and reproducible deep learning.
2019.

[16] T. Ben-Nun and T. Hoeﬂer. Demystifying parallel and distributed
deep learning: An in-depth concurrency analysis. ACM CSUR,
2019.

22

[27] M. Besta and T. Hoeﬂer. Accelerating irregular computations
In

with hardware transactional memory and active messages.
ACM HPDC, 2015.

[28] M. Besta and T. Hoeﬂer. Active access: A mechanism for high-
performance distributed data-centric computations. In ACM ICS,
2015.

[29] M. Besta, R. Kanakagiri, G. Kwasniewski, R. Ausavarungnirun,
J. Ber´anek, K. Kanellopoulos, K. Janda, Z. Vonarburg-Shmaria,
L. Gianinazzi, I. Stefan, et al. Sisa: Set-centric instruction set
architecture for graph mining on processing-in-memory systems.
In IEEE/ACM MICRO, 2021.

[30] M. Besta, R. Kanakagiri, H. Mustafa, M. Karasikov, G. R¨atsch,
T. Hoeﬂer, and E. Solomonik. Communication-efﬁcient jaccard
similarity for high-performance distributed genome compar-
isons. In IEEE IPDPS, 2020.

[31] M. Besta, F. Marending, E. Solomonik, and T. Hoeﬂer. Slimsell:
A vectorizable graph representation for breadth-ﬁrst search. In
IEEE IPDPS, 2017.

[32] M. Besta, E. Peter, R. Gerstenberger, M. Fischer, M. Podstawski,
C. Barthels, G. Alonso, and T. Hoeﬂer. Demystifying graph
databases: Analysis and taxonomy of data organization, system
designs, and graph queries. arXiv preprint arXiv:1910.09017, 2019.
[33] M. Besta, M. Podstawski, L. Groner, E. Solomonik, and T. Hoeﬂer.
To push or to pull: On reducing communication and synchroniza-
tion in graph computations. In ACM HPDC, 2017.

[34] M. Besta, M. Schneider, K. Cynk, M. Konieczny, E. Henriksson,
S. Di Girolamo, A. Singla, and T. Hoeﬂer. Fatpaths: Routing in
supercomputers and data centers when shortest paths fall short.
ACM/IEEE Supercomputing, 2020.

[35] M. Besta, D. Stanojevic, J. D. F. Licht, T. Ben-Nun, and T. Hoeﬂer.
Graph processing on FPGAs: Taxonomy, survey, challenges. arXiv
preprint arXiv:1903.06697, 2019.

[36] M. Besta, Z. Vonarburg-Shmaria, Y. Schaffner, L. Schwarz,
G. Kwasniewski, L. Gianinazzi, J. Beranek, K. Janda, T. Holen-
stein, S. Leisinger, et al.
Graphminesuite: Enabling high-
performance and programmable graph mining algorithms with
set algebra. VLDB, 2021.

[37] M. Besta, S. Weber, L. Gianinazzi, R. Gerstenberger, A. Ivanov,
Y. Oltchik, and T. Hoeﬂer. Slim graph: Practical lossy graph
compression for approximate graph processing, storage, and
analytics. In ACM/IEEE Supercomputing, 2019.

[38] F. M. Bianchi, D. Grattarola, L. Livi, and C. Alippi. Graph neural

networks with convolutional arma ﬁlters. IEEE TPAMI, 2021.

[39] G. Bilardi and A. Pietracaprina. Models of Computation, Theoretical.

Springer US, 2011.

[17] Y. Bengio, O. Delalleau, and N. Le Roux. 11 label propagation

[40] G. E. Blelloch. Pre x sums and their applications. Technical report,

and quadratic criterion. 2006.

Citeseer, 1990.

[18] M. Besta. Enabling High-Performance Large-Scale Irregular Compu-

[41] G. E. Blelloch. Programming parallel algorithms. Communications

tations. PhD thesis, ETH Zurich, 2021.

[19] M. Besta, A. Carigiet, Z. Vonarburg-Shmaria, K. Janda, L. Gi-
aninazzi, and T. Hoeﬂer. High-performance parallel graph col-
oring with strong guarantees on work, depth, and quality.
In
ACM/IEEE Supercomputing, 2020.

[20] M. Besta, J. Domke, M. Schneider, M. Konieczny, S. Di Girolamo,
T. Schneider, A. Singla, and T. Hoeﬂer. High-performance rout-
ing with multipathing and path diversity in ethernet and hpc
IEEE Transactions on Parallel and Distributed Systems,
networks.
32(4):943–959, 2020.

[21] M. Besta, M. Fischer, T. Ben-Nun, D. Stanojevic, J. D. F. Licht,
and T. Hoeﬂer. Substream-centric maximum matchings on fpga.
ACM TRETS, 2020.

[22] M. Besta, M. Fischer, V. Kalavri, M. Kapralov, and T. Hoeﬂer.
Practice of streaming processing of dynamic graphs: Concepts,
models, and systems. IEEE TPDS, 2020.

[23] M. Besta, R. Grob, C. Miglioli, N. Bernold, G. Kwasniewski,
G. Gjini, R. Kanakagiri, S. Ashkboos, L. Gianinazzi, N. Dryden,
In ACM
et al. Motif prediction with graph neural networks.
KDD, 2022.

[24] M. Besta, S. M. Hassan, S. Yalamanchili, R. Ausavarungnirun,
O. Mutlu, and T. Hoeﬂer. Slim noc: A low-diameter on-chip
network topology for high energy efﬁciency and scalability. In
ACM ASPLOS, 2018.

[25] M. Besta and T. Hoeﬂer. Fault tolerance for remote memory

access programming models. In ACM HPDC, 2014.

[26] M. Besta and T. Hoeﬂer. Slim ﬂy: A cost effective low-diameter

network topology. In ACM/IEEE Supercomputing, 2014.

of the ACM, 39(3):85–97, 1996.

[42] G. E. Blelloch and B. M. Maggs. Parallel Algorithms. 2010.
[43] A. Bojchevski, J. Klicpera, B. Perozzi, A. Kapoor, M. Blais,
B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann. Scaling graph
In ACM KDD,
neural networks with approximate pagerank.
2020.

[44] P. Boldi, M. Rosa, and S. Vigna. Hyperanf: Approximating the
neighbourhood function of very large graphs on a budget.
In
Proceedings of the 20th international conference on World wide web,
pages 625–634. ACM, 2011.

[45] M. Borassi and E. Natale. Kadabra is an adaptive algorithm
arXiv preprint

for betweenness via random approximation.
arXiv:1604.08553, 2016.

[46] K. M. Borgwardt. Graph kernels. PhD thesis, lmu, 2007.
[47] X. Bresson and T. Laurent. Residual gated graph convnets. arXiv

preprint arXiv:1711.07553, 2017.

[48] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veliˇckovi´c. Geometric
deep learning: Grids, groups, graphs, geodesics, and gauges.
arXiv preprint arXiv:2104.13478, 2021.

[49] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Van-
dergheynst. Geometric deep learning: going beyond euclidean
data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.

[50] A. Buluc¸ and E. G. Boman. Towards scalable parallel hypergraph

partitioning. 2008.

[51] A. Buluc and K. Madduri. Graph partitioning for scalable
distributed graph computations. Graph Partitioning and Graph
Clustering, 588:83, 2013.

[52] A. Buluc¸, H. Meyerhenke, I. Safro, P. Sanders, and C. Schulz.
Recent advances in graph partitioning. In Algorithm Engineering,
pages 117–158. Springer, 2016.

[53] H. Cai, V. W. Zheng, and K. C.-C. Chang. A comprehensive sur-
vey of graph embedding: Problems, techniques, and applications.
IEEE Transactions on Knowledge and Data Engineering, 30(9):1616–
1637, 2018.

[55]

[54] Z. Cai, X. Yan, Y. Wu, K. Ma, J. Cheng, and F. Yu. Dgcl: an efﬁcient
communication library for distributed gnn training. In EuroSys,
2021.
¨U. V. C¸ ataly ¨urek and C. Aykanat. A ﬁne-grain hypergraph model
for 2d decomposition of sparse matrices. In IPDPS, 2001.
[56] V. T. Chakaravarthy, S. S. Pandian, S. Raje, Y. Sabharwal, T. Suzu-
mura, and S. Ubaru. Efﬁcient scaling of dynamic graph neural
networks. In ACM/IEEE Supercomputing, 2021.

[57] D. Chakrabarti and C. Faloutsos. Graph mining: Laws, gener-
ators, and algorithms. ACM computing surveys (CSUR), 38(1):2,
2006.
I. Chami, S. Abu-El-Haija, B. Perozzi, C. R´e, and K. Murphy.
Machine learning on graphs: A model and comprehensive tax-
onomy. arXiv preprint arXiv:2005.03675, 2020.

[58]

[60]

[59] B. Chazelle, R. Rubinfeld, and L. Trevisan. Approximating the
minimum spanning tree weight in sublinear time. SIAM Journal
on computing, 34(6):1370–1379, 2005.
J. Chen et al. Stochastic training of graph convolutional networks
with variance reduction. arXiv preprint arXiv:1710.10568, 2017.
J. Chen et al. Fastgcn: fast learning with graph convolutional
networks via importance sampling. arXiv:1801.10247, 2018.
[62] X. Chen, Y. Wang, X. Xie, X. Hu, A. Basak, L. Liang, M. Yan,
L. Deng, Y. Ding, Z. Du, et al. Rubik: A hierarchical architecture
for efﬁcient graph neural network training. IEEE TCAD, 2021.

[61]

[63] Z. Chen, F. Chen, L. Zhang, T. Ji, K. Fu, L. Zhao, F. Chen, L. Wu,
C. Aggarwal, and C.-T. Lu. Bridging the gap between spatial
and spectral domains: A uniﬁed framework for graph neural
networks. arXiv preprint arXiv:2107.10234, 2021.

[64] Z. Chen, M. Yan, M. Zhu, L. Deng, G. Li, S. Li, and Y. Xie. fusegnn:
accelerating graph convolutional neural network training on
gpgpu. In IEEE/ACM ICCAD, 2020.

[65] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh.
Cluster-gcn: An efﬁcient algorithm for training deep and large
graph convolutional networks. In ACM KDD, 2019.

[66] D. J. Cook and L. B. Holder. Mining graph data.

John Wiley &

Sons, 2006.

[67] M. Copik, G. Kwasniewski, M. Besta, M. Podstawski, and T. Hoe-
ﬂer. Sebs: A serverless benchmark suite for function-as-a-service
computing. In ACM Middleware, 2021.

[68] P. Cui, X. Wang, J. Pei, and W. Zhu. A survey on network
embedding. IEEE Transactions on Knowledge and Data Engineering,
31(5):833–852, 2018.

[69] Y. Dai, S. Wang, N. N. Xiong, and W. Guo. A survey on knowl-
edge graph embedding: Approaches, applications and bench-
marks. Electronics, 9(5):750, 2020.

[70] A. Davies, P. Veliˇckovi´c, L. Buesing, S. Blackwell, D. Zheng,
N. Tomaˇsev, R. Tanburn, P. Battaglia, C. Blundell, A. Juh´asz,
et al. Advancing mathematics by guiding human intuition with
ai. Nature, 600(7887):70–74, 2021.
J. de Fine Licht, M. Besta, S. Meierhans, and T. Hoeﬂer. Trans-
formations of high-level synthesis codes for high-performance
computing. IEEE TPDS, 2021.

[71]

23

[77] V. P. Dwivedi, C. K. Joshi, T. Laurent, Y. Bengio, and X. Bres-
arXiv preprint

Benchmarking graph neural networks.

son.
arXiv:2003.00982, 2020.

[78] G. ECHBARTHI and H. KHEDDOUCI. Lasas: an aggregated
search based graph matching approach. In The 29th International
Conference on Software Engineering and Knowledge Engineering,
2017.

[79] M. Fey and J. E. Lenssen. Fast graph representation learning with
pytorch geometric. arXiv preprint arXiv:1903.02428, 2019.
[80] F. Frasca, E. Rossi, D. Eynard, B. Chamberlain, M. Bronstein, and
F. Monti. Sign: Scalable inception graph neural networks. arXiv
preprint arXiv:2004.11198, 2020.
S. Gandhi and A. P. Iyer. P3: Distributed deep graph learning at
scale. In USENIX OSDI, 2021.

[81]

[82] R. Garg, E. Qin, F. Mu ˜noz-Mart´ınez, R. Guirado, A.

Jain,
S. Abadal, J. L. Abell´an, M. E. Acacio, E. Alarc ´on, S. Rajaman-
ickam, et al. Understanding the design space of sparse/dense
multiphase dataﬂows for mapping graph neural networks on
spatial accelerators. arXiv preprint arXiv:2103.07977, 2021.
[83] R. Geisberger, P. Sanders, and D. Schultes. Better approxima-
In Proceedings of the Meeting on
tion of betweenness centrality.
Algorithm Engineering & Expermiments, pages 90–100. Society for
Industrial and Applied Mathematics, 2008.

[84] B. Geissmann and L. Gianinazzi. Parallel minimum cuts in near-
linear work and low depth. arXiv preprint arXiv:1807.09524, 2018.
[85] T. Geng, A. Li, R. Shi, C. Wu, T. Wang, Y. Li, P. Haghi, A. Tumeo,
S. Che, S. Reinhardt, et al. Awb-gcn: A graph convolutional
network accelerator with runtime workload rebalancing.
In
IEEE/ACM MICRO, 2020.

[86] E. Georganas, J. Gonzalez-Dominguez, E. Solomonik, Y. Zheng,
J. Tourino, and K. Yelick. Communication Avoiding and Over-
lapping for Numerical Linear Algebra. In ACM/IEEE Supercom-
puting, 2012.

[87] R. Gerstenberger et al. Enabling highly-scalable remote memory
access programming with mpi-3 one sided. In ACM/IEEE Super-
computing, 2013.
S. Ghose, K. Hsieh, A. Boroumand, R. Ausavarungnirun, and
O. Mutlu. The processing-in-memory paradigm: Mechanisms to
enable adoption. In Beyond-CMOS Technologies for Next Generation
Computer Design, pages 133–194. Springer, 2019.

[88]

[89] L. Gianinazzi, M. Fries, N. Dryden, T. Ben-Nun, and T. Hoeﬂer.
Learning combinatorial node labeling algorithms. arXiv preprint
arXiv:2106.03594, 2021.

[90] L. Gianinazzi, P. Kalvoda, A. De Palma, M. Besta, and T. Hoeﬂer.
Communication-avoiding parallel minimum cuts and connected
components. ACM SIGPLAN Notices, 2018.
J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl.
Neural message passing for quantum chemistry. In International
Conference on Machine Learning, pages 1263–1272. PMLR, 2017.

[91]

[92] N. Gleinig, M. Besta, and T. Hoeﬂer. I/o-optimal cache-oblivious

[93]

[94]

[95]

sparse matrix-sparse matrix multiplication. 2022.
J. G ´omez-Luna, I. E. Hajj, I. Fernandez, C. Giannoula, G. F.
Oliveira, and O. Mutlu. Benchmarking a new paradigm: An ex-
perimental analysis of a real processing-in-memory architecture.
arXiv preprint arXiv:2105.03814, 2021.
J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin.
Powergraph: distributed graph-parallel computation on natural
graphs. In OSDI, volume 12, page 2, 2012.
I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT
press, 2016.

[72] M. Defferrard et al. Convolutional neural networks on graphs

[96] D. Gregor and A. Lumsdaine. The parallel bgl: A generic library

with fast localized spectral ﬁltering. NeurIPS, 2016.

for distributed graph computations. POOSC, 2005.

[73] K. D. Devine, E. G. Boman, R. T. Heaphy, R. H. Bisseling, and
U. V. Catalyurek. Parallel hypergraph partitioning for scientiﬁc
computing. In IEEE IPDPS, 2006.
S. Di Girolamo, K. Taranov, A. Kurth, M. Schaffner, T. Schnei-
der, J. Ber´anek, M. Besta, L. Benini, D. Roweth, and T. Hoe-
ﬂer. Network-accelerated non-contiguous memory transfers. In
ACM/IEEE Supercomputing, 2019.

[74]

[75] C. H. Ding, X. He, H. Zha, M. Gu, and H. D. Simon. A min-
max cut algorithm for graph partitioning and data clustering. In
Proceedings 2001 IEEE international conference on data mining, pages
107–114. IEEE, 2001.
S. Dumbrava, A. Bonifati, A. N. R. Diaz, and R. Vuillemot. Ap-
proximate evaluation of label-constrained reachability queries.
arXiv preprint arXiv:1811.11561, 2018.

[76]

[97] A. Grover and J. Leskovec. node2vec: Scalable feature learning

for networks. In ACM KDD, 2016.

[98] N. Hajinazar, G. F. Oliveira, S. Gregorio, J. D. Ferreira, N. M. Ghi-
asi, M. Patel, M. Alser, S. Ghose, J. G ´omez-Luna, and O. Mutlu.
Simdram: a framework for bit-serial simd processing using dram.
In Proceedings of the 26th ACM International Conference on Architec-
tural Support for Programming Languages and Operating Systems,
pages 329–345, 2021.

[99] W. L. Hamilton. Graph representation learning. Synthesis Lectures

on Artiﬁcal Intelligence and Machine Learning, 14(3):1–159, 2020.

[100] W. L. Hamilton et al. Representation learning on graphs: Methods

and applications. arXiv preprint arXiv:1709.05584, 2017.

[101] W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representa-

tion learning on large graphs. In NeurIPS, 2017.

[102] C. He, E. Ceyani, K. Balasubramanian, M. Annavaram, and
S. Avestimehr. Spreadgnn: Serverless multi-task federated learn-
ing for graph neural networks. arXiv preprint arXiv:2106.02743,
2021.

[103] Y. He, Y. Wang, C. Liu, H. Li, and X. Li. Tare: Task-adaptive in-
In ACM/IEEE DAC,

situ reram computing for graph learning.
2021.

[104] B. Hendrickson and R. Leland. An improved spectral graph
partitioning algorithm for mapping parallel computations. SIAM
Journal on Scientiﬁc Computing, 16(2):452–469, 1995.

[105] L. Hoang, X. Chen, H. Lee, R. Dathathri, G. Gill, and K. Pingali.

Efﬁcient distribution for deep learning on large graphs, 2021.

[106] T. Hoeﬂer, S. Di Girolamo, K. Taranov, R. E. Grant, and
R. Brightwell. spin: High-performance streaming processing in
the network. In ACM/IEEE Supercomputing, 2017.

[107] T. Hoeﬂer and D. Moor. Energy, memory, and runtime tradeoffs
for implementing collective communication operations. Super-
computing frontiers and innovations, 1(2):58–75, 2014.

[108] T. Horv´ath, T. G¨artner, and S. Wrobel. Cyclic pattern kernels for

predictive graph mining. In ACM KDD, 2004.

[109] F. Hu, L. Wang, S. Wu, L. Wang, and T. Tan. Graph classiﬁcation
by mixture of diverse experts. arXiv preprint arXiv:2103.15622,
2021.

[110] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta,
and J. Leskovec. Open graph benchmark: Datasets for machine
learning on graphs. arXiv preprint arXiv:2005.00687, 2020.
[111] Y. Hu, Z. Ye, M. Wang, J. Yu, D. Zheng, M. Li, Z. Zhang, Z. Zhang,
and Y. Wang. Featgraph: A ﬂexible and efﬁcient backend for
In ACM/IEEE Supercomputing,
graph neural network systems.
2020.

[112] Z. Hu, Y. Dong, K. Wang, and Y. Sun. Heterogeneous graph
In Proceedings of The Web Conference 2020, pages

transformer.
2704–2710, 2020.

[113] G. Huang, G. Dai, Y. Wang, and H. Yang. Ge-spmm: General-
purpose sparse matrix-matrix multiplication on gpus for graph
neural networks. In ACM/IEEE Supercomputing, 2020.

[114] K. Huang, J. Zhai, Z. Zheng, Y. Yi, and X. Shen. Understanding
and bridging the gaps in current gnn performance optimizations.
In ACM PPoPP, 2021.

[115] L. Huang, D. Ma, S. Li, X. Zhang, and H. Wang. Text level
arXiv preprint

graph neural network for text classiﬁcation.
arXiv:1910.02356, 2019.

[116] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee,
J. Ngiam, Q. V. Le, Y. Wu, et al. Gpipe: Efﬁcient training of giant
neural networks using pipeline parallelism. NeurIPS, 2019.
[117] Z. Jia, S. Lin, M. Gao, M. Zaharia, and A. Aiken. Improving the
accuracy, scalability, and performance of graph neural networks
with roc. MLSys, 2020.

[118] C. Jiang, F. Coenen, and M. Zito. A survey of frequent subgraph
mining algorithms. The Knowledge Engineering Review, 28(1):75–
105, 2013.

[119] W. Jiang and J. Luo. Graph neural network for trafﬁc forecasting:

A survey. arXiv preprint arXiv:2101.11174, 2021.

[120] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ron-
neberger, K. Tunyasuvunakool, R. Bates, A. ˇZ´ıdek, A. Potapenko,
et al. Highly accurate protein structure prediction with alphafold.
Nature, 596(7873):583–589, 2021.

[121] G. Karypis and V. Kumar. Analysis of multilevel graph parti-
tioning. In Supercomputing’95: Proceedings of the 1995 ACM/IEEE
conference on Supercomputing, pages 29–29. IEEE, 1995.

[122] G. Karypis and V. Kumar. Metis–unstructured graph partitioning

and sparse matrix ordering system, version 2.0. 1995.

[123] J. Kepner, P. Aaltonen, D. Bader, A. Buluc¸, F. Franchetti, J. Gilbert,
D. Hutchison, M. Kumar, A. Lumsdaine, and H. Meyerhenke.
Mathematical foundations of the graphblas. In IEEE HPEC, 2016.
[124] M. Kim and K. S. Candan. Sbv-cut: Vertex-cut based graph
partitioning using structural balance vertices. Data & Knowledge
Engineering, 72:285–303, 2012.

[125] T. Kim, C. Hwang, K. Park, Z. Lin, P. Cheng, Y. Miao, L. Ma, and
Y. Xiong. Accelerating gnn training with locality-aware partial
execution. In ACM SIGOPS APSys, 2021.

[126] K. Kiningham, P. Levis, and C. R´e. Greta: Hardware optimized

graph processing for gnns. In ReCoML 2020, 2020.

[127] K. Kiningham, C. Re, and P. Levis. Grip: a graph neural network

accelerator architecture. arXiv preprint arXiv:2007.13828, 2020.

24

[128] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with
graph convolutional networks. arXiv preprint arXiv:1609.02907,
2016.

[129] J. Klicpera et al. Predict then propagate: Graph neural networks
arXiv preprint arXiv:1810.05997,

meet personalized pagerank.
2018.

[130] J. Klicpera, S. Weißenberger, and S. G ¨unnemann. Diffusion

improves graph learning. NeurIPS, 2019.

[131] N. M. Kriege, F. D. Johansson, and C. Morris. A survey on graph

kernels. Applied Network Science, 5(1):1–42, 2020.

[132] A. Krizhevsky. One weird trick for parallelizing convolutional

neural networks. arXiv preprint arXiv:1404.5997, 2014.

[133] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, K. Bala,
and L. P. Chew. Optimistic parallelism requires abstractions.
ACM SIGPLAN Notices, 42(6):211–222, 2007.

[134] G. Kwasniewski, T. Ben-Nun, L. Gianinazzi, A. Calotoiu,
T. Schneider, A. N. Ziogas, M. Besta, and T. Hoeﬂer. Pebbles,
graphs, and a pinch of combinatorics: Towards tight i/o lower
bounds for statically analyzable programs. ACM SPAA, 2021.

[135] G. Kwasniewski, T. Ben-Nun, A. N. Ziogas, T. Schneider,
M. Besta, and T. Hoeﬂer. On the parallel i/o optimality of linear
In ACM PPoPP,
algebra kernels: near-optimal lu factorization.
2021.

[136] G. Kwasniewski, M. Kabi´c, M. Besta, J. VandeVondele, R. Solc`a,
and T. Hoeﬂer. Red-blue pebbling revisited: near optimal parallel
matrix-matrix multiplication. In ACM/IEEE Supercomputing, 2019.
[137] L. C. Lamb, A. Garcez, M. Gori, M. Prates, P. Avelar, and M. Vardi.
Graph neural networks meet neural-symbolic computing: A sur-
vey and perspective. arXiv preprint arXiv:2003.00330, 2020.
[138] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature,

521(7553):436–444, 2015.

[139] G. Li, M. M ¨uller, B. Ghanem, and V. Koltun. Training graph
In International conference on

neural networks with 1000 layers.
machine learning, pages 6437–6449. PMLR, 2021.

[140] H. Li, M. Yan, X. Yang, L. Deng, W. Li, X. Ye, D. Fan, and Y. Xie.
IEEE
Hardware acceleration for gcns via bidirectional fusion.
Computer Architecture Letters, 20(1):66–4, 2021.

[141] J. Li, A. Louri, A. Karanth, and R. Bunescu. Gcnax: A ﬂexible
and energy-efﬁcient accelerator for graph convolutional neural
networks. In IEEE HPCA, 2021.

[142] J. Li, Y. Rong, H. Cheng, H. Meng, W. Huang, and J. Huang. Semi-
supervised graph classiﬁcation: A hierarchical graph perspective.
In The World Wide Web Conference, pages 972–982, 2019.

[143] S. Li, T. Ben-Nun, S. D. Girolamo, D. Alistarh, and T. Hoeﬂer.
Taming unbalanced training workloads in deep learning with
partial collective operations. In Proceedings of the 25th ACM SIG-
PLAN Symposium on Principles and Practice of Parallel Programming,
pages 45–61, 2020.

[144] S. Li and T. Hoeﬂer. Chimera: efﬁciently training large-scale
In ACM/IEEE

neural networks with bidirectional pipelines.
Supercomputing, 2021.

[145] S. Liang, Y. Wang, C. Liu, L. He, L. Huawei, D. Xu, and X. Li.
Engn: A high-throughput and energy-efﬁcient accelerator for
large graph neural networks. IEEE TOC, 2020.

[146] C.-Y. Lin, L. Luo, and L. Ceze. Accelerating spmm kernel
with cache-ﬁrst edge sampling for gnn inference. arXiv preprint
arXiv:2104.10716, 2021.

[147] H. Lin et al. Shentu: processing multi-trillion edge graphs on
millions of cores in seconds. In ACM/IEEE Supercomputing, 2018.
[148] Y. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu. Learning entity and
relation embeddings for knowledge graph completion. In AAAI,
2015.

[149] Z. Lin, C. Li, Y. Miao, Y. Liu, and Y. Xu. Pagraph: Scaling gnn
training on large graphs via computation-aware caching. In ACM
SoCC, 2020.

[150] H. Liu, S. Lu, X. Chen, and B. He. G3: when graph neural
networks meet parallel graph processing systems on gpus. VLDB,
2020.

[151] T. Liu, Y. Chen, D. Li, C. Wu, Y. Zhu, J. He, Y. Peng, H. Chen,
Bgl: Gpu-efﬁcient gnn training by
arXiv preprint

H. Chen, and C. Guo.
optimizing graph data i/o and preprocessing.
arXiv:2112.08541, 2021.

[152] A. Lumsdaine, D. Gregor, B. Hendrickson, and J. Berry. Chal-
lenges in parallel graph processing. Parallel Processing Letters,
17(01):5–20, 2007.

[153] L. Ma, Z. Yang, Y. Miao, J. Xue, M. Wu, L. Zhou, and Y. Dai.
Neugraph: parallel deep neural network computation on large
graphs. In USENIX ATC, 2019.

[154] S. Masoudnia and R. Ebrahimpour. Mixture of experts: a litera-
ture survey. Artiﬁcial Intelligence Review, 42(2):275–293, 2014.
[155] V. Md, S. Misra, G. Ma, R. Mohanty, E. Georganas, A. Heinecke,
D. Kalamkar, N. K. Ahmed, and S. Avancha. Distgnn: Scalable
distributed training for large-scale graph neural networks. arXiv
preprint arXiv:2104.06700, 2021.

[156] S. W. Min, K. Wu, S. Huang, M. Hidayeto ˘glu,

J. Xiong,
E. Ebrahimi, D. Chen, and W.-m. Hwu. Large graph convolu-
tional network training with gpu-oriented data communication
architecture. arXiv preprint arXiv:2103.03330, 2021.

[157] A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori,
S. Wang, Y.-J. Lee, E. Johnson, O. Pathak, A. Nazi, et al. A
graph placement methodology for fast chip design. Nature,
594(7862):207–212, 2021.

[158] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M.
Bronstein. Geometric deep learning on graphs and manifolds
using mixture model cnns. In IEEE CVPR, 2017.

[159] H. Mostafa. Sequential aggregation and rematerialization: Dis-
tributed full-batch training of graph neural networks on large
graphs. arXiv preprint arXiv:2111.06483, 2021.

[160] MPI Forum. MPI: A Message-Passing Interface Standard. Version
available at: http://www.mpi-forum.org (Sep.

3, Sept. 2012.
2012).

[161] K. Mulmuley. A fast parallel algorithm to compute the rank of a

matrix over an arbitrary ﬁeld. In ACM STOC, 1986.

[162] O. Mutlu et al. Processing Data Where It Makes Sense: Enabling

In-Memory Computation. MicPro, 2019.

[163] O. Mutlu, S. Ghose, J. G ´omez-Luna, and R. Ausavarungnirun.
arXiv preprint

A modern primer on processing in memory.
arXiv:2012.03112, 2020.

[164] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. De-
vanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia. Pipedream:
generalized pipeline parallelism for dnn training. In ACM SOSP,
2019.

[165] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional

neural networks for graphs. In ICML, 2016.

[166] G. F. Oliveira, J. G ´omez-Luna, L. Orosa, S. Ghose, N. Vijaykumar,
I. Fernandez, M. Sadrosadati, and O. Mutlu. Damov: A new
methodology and benchmark suite for evaluating data move-
ment bottlenecks. arXiv preprint arXiv:2105.03725, 2021.

[167] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank
citation ranking: Bringing order to the web. Technical report,
Stanford InfoLab, 1999.

[168] B. Perozzi et al. Deepwalk: Online learning of social representa-

tions. In ACM KDD, 2014.

[169] Petar Veliˇckovi´c. Theoretical foundations of graph neural net-

works. https://petar-v.com/talks/GNN-Wednesday.pdf, 2021.

[170] T. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W. Battaglia.
arXiv
Learning mesh-based simulation with graph networks.
preprint arXiv:2010.03409, 2020.
[171] S. Qiu, Y. Liang, and Z. Wang.

Optimizing sparse ma-
arXiv preprint

trix multiplications for graph neural networks.
arXiv:2111.00352, 2021.

[172] M. K. Rahman, M. H. Sujon, and A. Azad. Fusedmm: A uniﬁed
sddmm-spmm kernel for graph embedding and graph neural
networks. In IEEE IPDPS, 2021.

[173] M. Riondato and E. M. Kornaropoulos.

Fast approximation
of betweenness centrality through sampling. Data Mining and
Knowledge Discovery, 30(2):438–475, 2016.

[174] M. Riondato and E. Upfal. Abra: Approximating betweenness
centrality in static and dynamic graphs with rademacher aver-
ages. ACM TKDD, 12(5):61, 2018.

[175] L. Roditty and V. Vassilevska Williams.

Fast approximation
algorithms for the diameter and radius of sparse graphs.
In
Proceedings of the forty-ﬁfth annual ACM symposium on Theory of
computing, pages 515–524. ACM, 2013.

[176] S. Sakr, A. Bonifati, H. Voigt, A. Iosup, K. Ammar, R. Angles,
W. Aref, M. Arenas, M. Besta, P. A. Boncz, et al. The future
is big graphs: a community view on graph processing systems.
Communications of the ACM, 64(9):62–71, 2021.

25

[178] R. Sato. A survey on the expressive power of graph neural

networks. arXiv preprint arXiv:2003.04078, 2020.

[179] P. Schmid, M. Besta, and T. Hoeﬂer. High-performance dis-
tributed rma locks. In Proceedings of the 25th ACM International
Symposium on High-Performance Parallel and Distributed Computing,
pages 19–30. ACM, 2016.

[180] H. Schweizer et al. Evaluating the cost of atomic operations on

modern architectures. In ACM/IEEE PACT, 2015.

[181] M. Seraﬁni and H. Guan. Scalable graph neural network training:
The case for sampling. ACM SIGOPS Operating Systems Review,
55(1):68–76, 2021.

[182] V. Seshadri, Y. Kim, C. Fallin, D. Lee, R. Ausavarungnirun,
G. Pekhimenko, Y. Luo, O. Mutlu, P. B. Gibbons, and M. A.
Kozuch. Rowclone: fast and energy-efﬁcient in-dram bulk data
copy and initialization. In IEEE/ACM MICRO, 2013.

[183] V. Seshadri, D. Lee, T. Mullins, H. Hassan, A. Boroumand, J. Kim,
M. A. Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry.
Ambit: In-memory accelerator for bulk bitwise operations using
commodity dram technology. In IEEE/ACM MICRO, 2017.
[184] C. Shi, Y. Li, J. Zhang, Y. Sun, and S. Y. Philip. A survey of
heterogeneous information network analysis. IEEE Transactions
on Knowledge and Data Engineering, 29(1):17–37, 2016.

[185] X. Shi, Z. Zheng, Y. Zhou, H. Jin, L. He, B. Liu, and Q.-S. Hua.
Graph processing on gpus: A survey. ACM Computing Surveys
(CSUR), 50(6):1–35, 2018.

[186] G. M. Slota and K. Madduri. Complex network analysis using
In Parallel and Distributed
parallel approximate motif counting.
Processing Symposium, 2014 IEEE 28th International, pages 405–
414. IEEE, 2014.

[187] E. Solomonik, M. Besta, F. Vella, and T. Hoeﬂer. Scaling between-
ness centrality using communication-efﬁcient sparse matrix mul-
tiplication. In ACM/IEEE Supercomputing, 2017.

[188] E. Solomonik, E. Carson, N. Knight, and J. Demmel. Tradeoffs
between synchronization, communication, and work in parallel
linear algebra computations. Technical report, 2014.

[189] E. Solomonik and J. Demmel. Communication-optimal parallel
In

2.5 d matrix multiplication and lu factorization algorithms.
Euro-Par, 2011.

[190] E. Solomonik and T. Hoeﬂer. Sparse Tensor Algebra as a Parallel

Programming Model. CoRR, abs/1512.00066, 2015.

[191] A. Strausz, F. Vella, S. Di Girolamo, M. Besta, and T. Hoeﬂer.
Asynchronous distributed-memory triangle counting and lcc
with rma caching. 2022.

[192] S. Sukhbaatar, R. Fergus, et al. Learning multiagent communica-

tion with backpropagation. NeurIPS, 2016.

[193] S. A. Tailor, R. de Jong, T. Azevedo, M. Mattina, and P. Maji.
Towards efﬁcient point cloud graph neural networks through
architectural simpliﬁcation. In IEEE/CVF ICCV, 2021.

[194] A. Tate, A. Kamil, A. Dubey, A. Gr ¨oßlinger, B. Chamberlain,
B. Goglin, C. Edwards, C. J. Newburn, D. Padua, D. Unat, et al.
Programming abstractions for data locality. PADAL Workshop
2014, 2014.

[195] D. A. Tedjopurnomo, Z. Bao, B. Zheng, F. Choudhury, and A. Qin.
A survey on modern deep neural network for trafﬁc prediction:
Trends, methods and challenges. IEEE TKDE, 2020.

[196] K. K. Thekumparampil, C. Wang, S. Oh, and L.-J. Li. Attention-
based graph neural network for semi-supervised learning. arXiv
preprint arXiv:1803.03735, 2018.

[197] J. Thorpe, Y. Qiao, J. Eyolfson, S. Teng, G. Hu, Z. Jia, J. Wei,
K. Vora, R. Netravali, M. Kim, et al. Dorylus: Affordable, scalable,
and accurate gnn training with distributed cpu servers and
serverless threads. In USENIX OSDI, 2021.

[198] C. Tian, L. Ma, Z. Yang, and Y. Dai. Pcgcn: Partition-centric
processing for accelerating graph convolutional network. In IEEE
IPDPS, 2020.

[199] A. Tripathy, K. Yelick, and A. Buluc¸. Reducing communication
in graph neural network training. In ACM/IEEE Supercomputing,
2020.

[200] L. G. Valiant. A bridging model for parallel computation. Com-

munications of the ACM, 33(8):103–111, 1990.

[201] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need.
In NeurIPS, 2017.

[177] A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec,
and P. Battaglia. Learning to simulate complex physics with
graph networks. In ICML, 2020.

[202] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio,
arXiv preprint

and Y. Bengio. Graph attention networks.
arXiv:1710.10903, 2017.

26

[203] S. V. N. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M.
Borgwardt. Graph kernels. Journal of Machine Learning Research,
11:1201–1242, 2010.

[229] C. Yang, Y. Xiao, Y. Zhang, Y. Sun, and J. Han. Heterogeneous
network representation learning: A uniﬁed framework with sur-
vey and benchmark. IEEE TKDE, 2020.

[204] R. Waleffe, J. Mohoney, T. Rekatsinas, and S. Venkataraman.
Marius++: Large-scale training of graph neural networks on a
single machine. arXiv preprint arXiv:2202.02365, 2022.

[205] C. Wan, Y. Li, A. Li, N. S. Kim, and Y. Lin. Bns-gcn: Efﬁcient full-
graph training of graph convolutional networks with partition-
parallelism and random boundary node sampling sampling.
MLSys, 2022.

[206] C. Wan, Y. Li, C. R. Wolfe, A. Kyrillidis, N. S. Kim, and Y. Lin.
Pipegcn: Efﬁcient full-graph training of graph convolutional
networks with pipelined feature communication. arXiv preprint
arXiv:2203.10428, 2022.

[207] D. Wang, P. Cui, and W. Zhu. Structural deep network embed-

ding. In ACM KDD, 2016.

[208] L. Wang, Q. Yin, C. Tian, J. Yang, R. Chen, W. Yu, Z. Yao, and
J. Zhou. Flexgraph: a ﬂexible and efﬁcient distributed framework
for gnn training. In EuroSys, 2021.

[209] M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou, C. Ma,
L. Yu, Y. Gai, et al. Deep graph library: A graph-centric, highly-
performant package for graph neural networks. arXiv:1909.01315,
2019.

[210] Q. Wang, Z. Mao, B. Wang, and L. Guo. Knowledge graph
IEEE
embedding: A survey of approaches and applications.
TKDE, 2017.

[211] X. Wang, D. Bo, C. Shi, S. Fan, Y. Ye, and P. S. Yu. A survey on het-
erogeneous graph embedding: methods, techniques, applications
and sources. arXiv:2011.14867, 2020.

[212] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. S. Yu.
Heterogeneous graph attention network. In The world wide web
conference, pages 2022–2032, 2019.

[213] Y. Wang et al. Gunrock: A high-performance graph processing

library on the GPU. In ACM PPoPP, 2016.

[214] Y. Wang, B. Feng, and Y. Ding. Qgtc: Accelerating quantized gnn
via gpu tensor core. arXiv preprint arXiv:2111.09547, 2021.
[215] Y. Wang, B. Feng, G. Li, S. Li, L. Deng, Y. Xie, and Y. Ding.
Gnnadvisor: An efﬁcient runtime system for gnn acceleration on
gpus. arXiv preprint arXiv:2006.06608, 2020.

[216] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.
Solomon. Dynamic graph cnn for learning on point clouds. Acm
Transactions On Graphics (tog), 38(5):1–12, 2019.

[217] Z. Wang, Y. Guan, G. Sun, D. Niu, Y. Wang, H. Zheng, and Y. Han.
Gnn-pim: A processing-in-memory architecture for graph neural
networks. In Springer ACA, 2020.

[218] Z. Wang, Y. Wang, C. Yuan, R. Gu, and Y. Huang. Empirical
analysis of performance bottlenecks in graph neural network
training and inference with gpus. Neurocomputing, 2021.

[219] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger. Sim-
plifying graph convolutional networks. In International conference
on machine learning, pages 6861–6871. PMLR, 2019.

[220] S. Wu, F. Sun, W. Zhang, and B. Cui. Graph neural networks in
recommender systems: a survey. arXiv preprint arXiv:2011.02260,
2020.

[221] X.-M. Wu, Z. Li, A. M.-C. So, J. Wright, and S.-F. Chang. Learning
In NIPS, volume 25,

with partially absorbing random walks.
pages 3077–3085, 2012.

[222] Y. Wu, K. Ma, Z. Cai, T. Jin, B. Li, C. Zheng, J. Cheng, and F. Yu.
Seastar: vertex-centric programming for graph neural networks.
In EuroSys, 2021.

[223] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip. A
comprehensive survey on graph neural networks. IEEE TNNLS,
2020.

[224] Y. Xie, B. Yu, S. Lv, C. Zhang, G. Wang, and M. Gong. A sur-
vey on heterogeneous network representation learning. Pattern
Recognition, 116:107936, 2021.

[225] Z. Xinyi and L. Chen. Capsule graph neural network.
International conference on learning representations, 2018.

In

[226] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are

graph neural networks? arXiv preprint arXiv:1810.00826, 2018.

[227] M. Yan, Z. Chen, L. Deng, X. Ye, Z. Zhang, D. Fan, and Y. Xie.
Characterizing and understanding gcns on gpu. IEEE Computer
Architecture Letters, 19(1):22–25, 2020.

[230] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and
J. Leskovec. Graph convolutional neural networks for web-scale
recommender systems. In ACM KDD, 2018.

[231] R. Ying,

J. You, C. Morris, X. Ren, W. L. Hamilton, and
J. Leskovec. Hierarchical graph representation learning with
differentiable pooling. arXiv preprint arXiv:1806.08804, 2018.
[232] H. Yuan, H. Yu, S. Gui, and S. Ji. Explainability in graph neural
networks: A taxonomic survey. arXiv preprint arXiv:2012.15445,
2020.

[233] H. Zeng and V. Prasanna. Graphact: Accelerating gcn training on
cpu-fpga heterogeneous platforms. In ACM/SIGDA FPGA, 2020.
[234] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. Prasanna.
Graphsaint: Graph sampling based inductive learning method.
arXiv preprint arXiv:1907.04931, 2019.

[235] B. Zhang, H. Zeng, and V. Prasanna. Hardware acceleration of

large scale gcn inference. In IEEE ASAP, 2020.

[236] C. Zhang, D. Song, C. Huang, A. Swami, and N. V. Chawla.
Heterogeneous graph neural network. In ACM KDD, 2019.
[237] D. Zhang, X. Huang, Z. Liu, Z. Hu, X. Song, Z. Ge, Z. Zhang,
L. Wang, J. Zhou, Y. Shuang, et al. Agl: a scalable system
arXiv preprint
for industrial-purpose graph machine learning.
arXiv:2003.02454, 2020.

[238] H. Zhang, Z. Yu, G. Dai, G. Huang, Y. Ding, Y. Xie, and
Y. Wang. Understanding gnn computational graph: A coordi-
nated computation, io, and memory perspective. arXiv preprint
arXiv:2110.09524, 2021.

[239] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D.-Y. Yeung. Gaan:
Gated attention networks for learning on large and spatiotempo-
ral graphs. arXiv preprint arXiv:1803.07294, 2018.

[240] L. Zhang, Z. Lai, S. Li, Y. Tang, F. Liu, and D. Li. 2pgraph:
Accelerating gnn training over large graphs on gpu clusters. In
IEEE CLUSTER, 2021.

[241] S. Zhang, H. Tong, J. Xu, and R. Maciejewski. Graph convolu-
tional networks: a comprehensive review. Computational Social
Networks, 6(1):1–23, 2019.

[242] W. Zhang, Y. Shen, Z. Lin, Y. Li, X. Li, W. Ouyang, Y. Tao, Z. Yang,
and B. Cui. Gmlp: Building scalable and ﬂexible graph neural
networks with feature-message passing. arXiv:2104.09880, 2021.
[243] W. Zhang, Y. Shen, Z. Lin, Y. Li, X. Li, W. Ouyang, Y. Tao, Z. Yang,
and B. Cui. Pasca: A graph neural architecture search system
under the scalable paradigm. In ACM TheWebConf, 2022.
[244] Z. Zhang, P. Cui, and W. Zhu. Deep learning on graphs: A survey.
IEEE Transactions on Knowledge and Data Engineering, 2020.
[245] Z. Zhang, J. Leng, S. Lu, Y. Miao, Y. Diao, M. Guo, C. Li, and
Y. Zhu. Zipper: Exploiting tile-and operator-level parallelism for
general and scalable graph neural network acceleration. arXiv
preprint arXiv:2107.08709, 2021.

[246] Z. Zhang, J. Leng, L. Ma, Y. Miao, C. Li, and M. Guo. Archi-
tectural implications of graph neural networks. IEEE Computer
architecture letters, 19(1):59–62, 2020.

[247] D. Zheng, C. Ma, M. Wang, J. Zhou, Q. Su, X. Song, Q. Gan,
Z. Zhang, and G. Karypis. Distdgl: distributed graph neural
network training for billion-scale graphs. In IEEE/ACM IA3, 2020.
[248] D. Zheng, X. Song, C. Yang, D. LaSalle, Q. Su, M. Wang, C. Ma,
and G. Karypis. Distributed hybrid cpu and gpu training for
graph neural networks on billion-scale graphs. arXiv:2112.15345,
2021.

[249] A. Zhou, J. Yang, Y. Gao, T. Qiao, Y. Qi, X. Wang, Y. Chen, P. Dai,
W. Zhao, and C. Hu. Optimizing memory efﬁciency of graph
neural networks on edge computing platforms. arXiv preprint
arXiv:2104.03058, 2021.

[250] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Sch ¨olkopf.
Learning with local and global consistency. In Advances in neural
information processing systems, pages 321–328, 2004.

[251] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li,
and M. Sun. Graph neural networks: A review of methods and
applications. AI Open, 1:57–81, 2020.

[252] X. Zhou and Y. Luo. Explore mixture of experts in graph neural

networks, 2019.

[228] M. Yan, L. Deng, X. Hu, L. Liang, Y. Feng, X. Ye, Z. Zhang, D. Fan,
and Y. Xie. Hygcn: A gcn accelerator with hybrid architecture. In
IEEE HPCA, 2020.

[253] Z. Zhou, C. Li, X. Wei, and G. Sun. Gcnear: A hybrid architecture
for efﬁcient gcn training with near-memory processing. arXiv
preprint arXiv:2111.00680, 2021.

[254] Z. Zhou, B. Shi, Z. Zhang, Y. Guan, G. Sun, and G. Luo. Blockgnn:
Towards efﬁcient gnn acceleration using block-circulant weight
matrices. In ACM/IEEE DAC, 2021.

[255] R. Zhu, K. Zhao, H. Yang, W. Lin, C. Zhou, B. Ai, Y. Li, and
J. Zhou. Aligraph: a comprehensive graph neural network
platform. arXiv preprint arXiv:1902.08730, 2019.

[256] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised
learning using gaussian ﬁelds and harmonic functions. In ICML,
2003.

27

