2
2
0
2

l
u
J

2
2

]
E
S
.
s
c
[

1
v
2
4
9
0
1
.
7
0
2
2
:
v
i
X
r
a

Efficient Testing of Deep Neural Networks via Decision
Boundary Analysis
Qiang Hu1, Yuejun Guo1, Xiaofei Xie2 Maxime Cordy1, Lei Ma3, Mike Papadakis1, and Yves Le
Traon1

1University of Luxembourg, Luxembourg

2Singapore Management University, Singapore

3University of Alberta, Canada

ABSTRACT
Deep learning (DL) plays a more and more important role in our
daily life due to its competitive performance in multiple industrial
application domains. As the core of DL-enabled systems, deep neu-
ral networks (DNNs) automatically learn knowledge from carefully
collected and organized training data to gain the ability to predict
the label of unseen data. Similar to the traditional software sys-
tems that need to be comprehensively tested, DNNs also need to be
carefully evaluated to make sure the quality of the trained model
meets the demand. In practice, the de facto standard to assess the
quality of DNNs in industry is to check their performance (accu-
racy) on a collected set of labeled test data. However, preparing
such labeled data is often not easy partly because of the huge la-
beling effort, i.e., data labeling is labor-intensive, especially with
the massive new incoming unlabeled data every day. Recent stud-
ies show that test selection for DNN is a promising direction that
tackles this issue by selecting minimal representative data to label
and using these data to assess the model. However, it still requires
human effort and cannot be automatic. In this paper, we propose a
novel technique, named Aries, that can estimate the performance of
DNNs on new unlabeled data using only the information obtained
from the original test data. The key insight behind our technique
is that the model should have similar prediction accuracy on the
data which have similar distances to the decision boundary. We
performed a large-scale evaluation of our technique on 2 famous
datasets, CIFAR-10 and Tiny-ImageNet, 4 widely studied DNN mod-
els including ResNet101 and DenseNet121, and 13 types of data
transformation methods. The results demonstrate the usefulness of
our technique that the estimated accuracy by Aries is only 0.03% ‚Äì
2.60% (on average 0.61%) off the true accuracy. Besides, Aries also
outperforms the state-of-the-art selection-labeling-based methods
in most (96 out of 128) cases.

1 INTRODUCTION
Deep learning (DL) has been continuously deployed and applied in
different industrial domains that impact our social society and daily
life, such as face recognition [35, 38], autonomous driving [13, 29],
and video gaming [37, 41]. As the core of the DL-enabled system,
Deep Neural Network (DNN) follows the data-driven development
paradigm and learns the decision logic automatically based on the
incorporated learned knowledge of the training data. Similar to
traditional software that needs to be well tested, DNNs also need
to be comprehensively evaluated before deployment to reduce the
potential risk in the real world [28, 36].

A common de facto standard to assess the quality of DNNs in
the industry is by evaluating DNNs on a collected set of labeled
test data. In practice, when building the DNN model, developers

often split a dataset into the training set, validation set, and test
set. The test set is mainly used to measure the accuracy of the
trained model (as an indicator of performance generality), thus, the
final developed DNN often comes with the reported test accuracy.
However, the original test set often only covers a part of the data
distribution (generally, the same distribution as the training data).
The distribution of future unlabeled data is often unclear in the
practical context, and the reported test accuracy is hard to reflect
the actual model performance in real usage. Therefore, in addition
to testing models on the original test data, it is highly desirable to
also perform performance evaluation of DNNs on the new unla-
beled data, which is often available from a large amount of daily or
monthly incoming data.

However, different from the original test data that already have
been labeled, the new unlabeled data are usually raw data without
label information. It is challenging to assess the model on these data
without labeling information. More importantly, labeling all the
new data (that could be large in size) is labor-intensive and time-
consuming, which is often impossible and impractical. For some
complex tasks, domain knowledge from experts is also necessary.
For example, it can take more than 600 man-hours for experienced
software developers to label the code from 4 libraries [45].

Towards addressing the data labeling issue for more efficient
DNN testing, recently, researchers adapt the test selection concept
[8, 32] from the software engineering community to select and
label a subset of representative data, then test and assess the model
accordingly. For example, Li et al. proposed the cross entropy-based
sampling (CES) [27] to select a subset that has the minimum cross-
entropy with the entire test dataset to test the DNN. In this way,
the labeling effort can be significantly reduced and the testing has
acceptable bias. However, although test selection is a promising
direction for efficient DNN testing, the labeling efforts and costs
persist. To bridge the gap, in this paper, we aim to automatically
estimate the test accuracy without extra manually labeling.

To this end, we propose a novel technique, Aries, to efficiently
estimate the performance of DNNs on the new unseen data based
on existing labeled test data. The intuition behind our technique is:
there can be a correlation between the prediction accuracy
of the data and the distance of the data from the decision
boundary. More specifically, given two datasets whose distribution
of the distance to the decision boundary is close, they could share
a similar prediction accuracy. A preliminary empirical study is
first conducted to validate our assumption. Specifically, we adopt
the existing dropout-based uncertainty to estimate the distance
between the data instances and the decision boundary. By splitting
the uncertainty score into ùëõ intervals, we obtain ùëõ areas of the
distance distribution. Then, we can map the data instances into an
area based on its uncertainty score. With this, we can estimate the

 
 
 
 
 
 
Conference‚Äô17, July 2017, Washington, DC, USA

Hu et al.

accuracy of each area based on the original test data (with labels)
that fall into this area as the supporting evidence (points). Finally,
given the new data without labels, we map them into different areas
and leverage the estimated area accuracy to calculate the overall
accuracy of the new data. Compared to existing techniques that
need the labeled data to calculate the model accuracy, Aries is fully
automatic without extra human labeling effort.

To assess the effectiveness of Aries, we conduct in-depth evalua-
tion on two commonly used datasets, CIFAR-10 and Tiny-ImageNet,
4 different DNN architectures, including ResNet101 and DenseNet121.
Besides the original test data, we also use 13 types of transformed
test sets (e.g., data with adding brightness) to simulate the new unla-
beled data that could occur in the wild, where the transformed data
could follow different data distributions from the original test data
[19]. The results demonstrated that Aries can precisely estimate the
performance of DNN models on new unlabeled data without further
labeling effort. Compared to the real accuracy, the estimated accu-
racy exhibits a difference ranging from 0.03% to 2.60% by using the
default parameter setting. Besides, compared to the state-of-the-art
test selection methods CES [27] and PACE [5], which need to label
a portion of test data, Aries can still achieve competitive results
without extra labeling. Moreover, we conduct ablation studies to
explore how each component affects the performance of Aries.
To summarize, the main contributions of this paper are:

‚Ä¢ We propose a novel DNN testing technique for quality assess-
ment, Aries, that can efficiently estimate the accuracy of DNN
models on test data without extra labeling efforts. To the best of
our knowledge, this is the first technique that can automatically
estimate the accuracy of unlabelled data.

‚Ä¢ We empirically demonstrate that Aries can achieve competitive
results with test selection metrics that need human labeling effort.
‚Ä¢ We also comprehensively explore each potential factor that could
affect the performance of Aries and give the recommendation
parameters.

‚Ä¢ We release all our source code publically available, hoping to

facilitate further research in this direction 1.

2 BACKGROUND
2.1 DNN Testing
DNN testing [42] is an essential activity in the DL-enabled software
development process to ensure the quality and reliability before
deployment. Generally, a deep neural network (DNN) is trained
using a large number of labeled data, the so-called training set, with
a validation set to estimate the performance (accuracy in this paper)
error. Usually, the term ‚Äúvalidation set‚Äù is used interchangeably
with ‚Äútest set‚Äù given the assumption that the validation set and the
test set are derived from the same data distribution as the training
set. A minor difference is that the validation set is mostly used
in the training process to search for better training settings and
configurations. Test data plays as the role of future unseen data
to estimate how the trained DNN performs in future unseen cases.
Although the fundamental assumption of modern machine learning
is that, the test (unseen) data and training data share a similar
distribution, under which the performance obtained on training

1https://github.com/Anony4paper/Aries

data could also generalize to the test data, such an assumption
often does not hold for DNNs deployed in the wilds. For many real-
world applications, the new test set (i.e., future unseen data) can
hold a different distribution [23] that undermines the confidence
of the obtained accuracy. For example, given a DNN trained on
an image set with low contrast, the new selected images are with
high contrast [16]. As a result, the accuracy on the original test set
cannot reflect the actual accuracy on the new test set. Moreover,
the new data are usually raw and unlabeled to directly compute
the accuracy, which requires the developers‚Äô dedicated testing. For
simplicity, in this paper, we use ‚Äúoriginal test data‚Äù to represent the
labeled test data that are accompanied by the training set and ‚Äúnew
unlabeled data‚Äù to indicate the unlabeled test set.

2.2 Test Optimization in DNN Testing
Given an unlabeled test set, the most straightforward way to obtain
the DNN accuracy is to manually label each data and compare the
difference between the ground-truth and predicted labels. However,
the labeling effort can be costly concerning the labor expense and
time. As mentioned in the literature, test selection [5], has been
demonstrated as a promising solution to optimize the testing pro-
cess. Test selection techniques can be divided into two categories:
1)The first one is test prioritization, which tries to identify the data
most likely to be misclassified [39]. After finding these data, they
can be used to further enhance the pre-trained model (e.g., by model
retraining). 2) The other one is to select a subset of data that can
reflect the behavior of the whole set [5]. In this technique, a fixed
number of relevant test data are selected via a specific metric and
manually labeled to calculate the accuracy. The selected set, by
default, is (expected to be) representative of the entire set and, thus,
the obtained accuracy can approximately reflect the DNN accuracy
on the whole given set. Although, in this way, the labeling effort
can be greatly reduced to a few data, it can still be impractical are
too large to be handled under a given budget (e.g., labeling time,
available cost). In this paper, our proposed technique follows into
the second category.

3 METHODOLOGY
In this section, we first introduce the insight and assumption of
Aries, then conduct preliminary studies to empirically validate our
assumptions, and finally present the details of our technique.

3.1 Key Insight and Assumption
Our assumption are that there could be a correlation between the
prediction accuracy of the data and the distance of the data from the
decision boundary. To better understand this assumption, Figure
1 depicts an intuitive example of a binary classification with a
decision boundary (blue solid lines) splitting the data space into 2
regions. Concerning that data falling into the same region will be
predicted as the same label (there are some errors) but with different
confidence, we split each region further into multiple sub-regions
based on its distance to the boundary. In the figure, we evenly split
the space into 3 areas (Area1, Area2, and Area3) by the distance
h. We assume that, for data in the same area, the probability of
making the (in)correct prediction is similar, i.e., the model has the
same performance on these data. Thus, if we can obtain the model

Efficient Testing of Deep Neural Networks via Decision Boundary Analysis

Conference‚Äô17, July 2017, Washington, DC, USA

Definition 2 (Area). Given a dataset ùëã , the model ùëÄ and the ùëõ
intervals, then we define the data that belong to ùë°ùë°‚Ñé area as follows:

ùê¥ùëüùëíùëé(ùëã, ùë°,ùëá ) =

(cid:26)
ùë• | ùë• ‚àà ùëã ‚àß

ùë°
ùëõ

< ùêøùëâ ùëÖ(ùëÄ, ùë•,ùëá ) ‚â§

(cid:27)

ùë° + 1
ùëõ

where 0 ‚â§ ùë° < ùëõ and ùëá is the number of predictions for dropout.

Next, to validate the rationality of our assumption, we conduct
two preliminary studies to check 1) if the data in the same ùê¥ùëüùëíùëé
have similar accuracy, and 2) if there is a relation between the data
size of data with high ùêøùëâ ùëÖ and the model accuracy. The details of
datasets and models can be found in Section 4.

(a) CIFAR10-ResNet20

(b) CIFAR10-VGG16

(c) ImageNet-ResNet101

(d) ImageNet-DenseNet121

Figure 2: Accuracy in different ùê¥ùëüùëíùëéùë†. Original: labeled test
data, New: unlabeled new unlabeled data.

To check if the data in the same ùê¥ùëüùëíùëé have similar accuracy.
We randomly split the test data into two sets and assume one set
we already have the label information and the other is the new
unlabeled data. Then we activate the Dropout layers in each model
and use each model to predict the two sets of data multiple times
(here, we set the time number ùëá as 50, which is the default setting
of Aries). Afterward, we calculate the ùêøùëâ ùëÖ score of each data and
then split the data into different ùê¥ùëüùëíùëéùë† using definition 2. Finally,
we check the accuracy of data that are in the same ùê¥ùëüùëíùëé. Figure 2
presents the results of this study, the x-axis of the figure represents
the range of ùêøùëâ ùëÖ score and the y-axis represents the accuracy. Note
that we eliminate the results of data whose ùêøùëâ ùëÖ scores are smaller
than 0.4 because we found the size of included data is very small
(e.g., there is only one data in this part for CIFAR-10, ResNet20).
We can see that the two sets of data have similar accuracy in the
ùê¥ùëüùëíùëéùë† regardless of the datasets and models, which indicates that
our assumption is reasonable.

Finding 1: A DNN has similar accuracy on the data sets that
have similar distances to the decision boundary.

Figure 1: An example of the assumption of our technique.
Data in the same area are highlighted with the same marker.

accuracy in each area and map new data into the corresponding
area, we can estimate the accuracy of the model on these new data.
The essential insight of our assumption is to measure the dis-
tance between the data and decision boundaries (i.e., how to define
h in figure 1). Given the fact that the data space is usually high
dimensional and complex, it is difficult to directly describe the deci-
sion boundary. Recently, the dropout uncertainty [10, 43] has been
widely used to estimate the distance of the data from the decision
boundary. Therefore, we utilize the uncertainty analysis in Aries
for the distance approximation. At a high level, our insight is sound
and practical in the way that the prediction confidence of a DNN
follows a particular distribution that is automatically learned from
the training data in the training process. The learning distribution
of the prediction confidence largely depends on how the training
data instances as the support evidence during learning. When a test
data instance falls in a distribution and prediction confidence re-
gion, our fine-grained splittedspited region could provide a certain
level of evidence to support DNN quality assessment.

3.2 Preliminary Study
First, we introduce the definition of ùêøùëéùëèùëíùëô ùëâ ùëéùëüùëñùëéùë°ùëñùëúùëõ ùëÖùëéùë°ùëñùëú (ùêøùëâ ùëÖ),
which is used to measure the distance between the data and decision
boundaries.

Definition 1 (Label Variation Ratio (LVR)). Given a model ùëÄ
with dropout layer and an input data ùë•, number of dropout predic-
tions ùëá , the Label Variation Ratio (LVR) of ùë• is defined as:

(cid:12)
(cid:12)
(cid:12)

ùêøùëâ ùëÖ (ùëÄ, ùë•,ùëá ) =

(cid:110)
ùëò | 1 ‚â§ ùëò ‚â§ ùëá ‚àß ùêøùëÄùëò (ùë•) = ùêøùëöùëéùë•
ùëá
where ùêøùëÄùëò (ùë•)
is the ùëò-th predicted label of ùë• by model ùëÄ and ùêøùëöùëéùë•
is the dominant label of ùëá predictions (i.e., the label predicted by
most predictions). Intuitively, the prediction of the data near the
boundary has low confidence, i.e., with lower ùêøùëâ ùëÖ.

(cid:111)(cid:12)
(cid:12)
(cid:12)

We then divide the data space into ùëõ areas by splitting the ùêøùëâ ùëÖ
into ùëõ equal intervals, where the range of ùêøùëâ ùëÖ is (0,1]. For exam-
ple, if ùëõ is 2, then we have two areas, and the corresponding ùêøùëâ ùëÖ
intervals are (0, 0.5] and (0.5, 1]. A data falls into a region based on
its LVR value.

hhPart2hPart3Part4hhhhArea1Area2Area3Area1Area2Area3(0.4,0.5](0.5,0.6](0.6,0.7](0.7,0.8](0.8,0.9](0.9,1.0]020406080Accuracy (%)OriginalNew(0.4,0.5](0.5,0.6](0.6,0.7](0.7,0.8](0.8,0.9](0.9,1.0]020406080100Accuracy (%)OriginalNew(0.4,0.5](0.5,0.6](0.6,0.7](0.7,0.8](0.8,0.9](0.9,1.0]020406080Accuracy (%)OriginalNew(0.4,0.5](0.5,0.6](0.6,0.7](0.7,0.8](0.8,0.9](0.9,1.0]020406080Accuracy (%)OriginalNewConference‚Äô17, July 2017, Washington, DC, USA

Hu et al.

Algorithm 1: Aries: efficient testing of DNNs
: ùëÄ: model with dropout layer
Input
ùëãùëúùëüùëñ : original test data with labels
ùëãùëõùëíùë§ : new data without labels
ùëá : number of dropout predictions
ùëõ: number of areas

Output :ùê¥ùëêùëêùëõùëíùë§ : estimated accuracy of ùëãùëõùëíùë§

(a) CIFAR10-ResNet20

(b) CIFAR10-VGG16

(c) ImageNet-ResNet101

(d) ImageNet-DenseNet121

Figure 3: The linear relation (blue line) between size of data
with highest label variation ratio (ùë¶-axis, unit: ratio) and the
test accuracy (ùë•-axis, unit: %). We apply the least squares
polynomial fit to draw the blue line in each figure.

Second, we investigate if there is a relation between the size
of data with high ùêøùëâ ùëÖ and the accuracy of the model on this set.
Usually, data with a high ùêøùëâ ùëÖ score means the model is super
confident in predicting this data. Intuitively, the size of data that
the model has high confidence could partly reflect the model‚Äôs
performance. Figure 3 depicts the model accuracy (x-axis) and the
percentage of highly confident data (y-axis) on each data sets. Here,
the highly confident data means their ùêøùëâ ùëÖ is 1. We can see that
there is a clear linear relationship between the two values. The
results lead to our basic idea: we can measure the percentage of
highly confident data in the new unlabeled data although we do
not know the truth labels of the new data. Then, based on existing
test data with truth labels, we can measure the accuracy of the
datasets with a certain ratio of highly confident data. Finally, we
can estimate the accuracy of unlabelled data.

Finding 2: There is a linear relationship between the percent-
age of highly confident data and the accuracy of the whole set.
Therefore, given some labeled data, if we know 1) the accuracy of
the DNN in each ùê¥ùëüùëíùëé, and 2) the percentage of highly confident
data (ùêøùëâ ùëÖ = 1), it can be promising to estimate the accuracy of
the new unlabeled data.

3.3 Aries: Efficient Testing of DNNs
Based on the two findings, we propose a novel technique, Aries,
that can test the performance of DNN models without requiring the
label information. The key idea of Aries is to take advantage of the
labeled test sets to approximate the model performance on the new
unlabeled data. Algorithm 1 presents the details of our technique
which contains two main steps.

(1) First, given a model ùëÄ with dropout layers, the original labeled
data ùëãùëúùëüùëñ , the number of areas ùëõ, and the dropout prediction
time ùëá , Aries splits ùëãùëúùëüùëñ into ùëõ areas according to definition 2,

1 ùëêùëúùëüùëüùëíùëêùë° _ùëõùë¢ùëö = 0
2 for ùëñ = 0 ‚Üí ùëõ ‚àí 1 do
3

ùê¥ùëüùëíùëéùê¥ùëêùëêùëñ = ùëíùë£ùëéùëôùë¢ùëéùë°ùëí (ùëÄ, ùê¥ùëüùëíùëé (ùëãùëúùëüùëñ, ùëñ,ùëá ));
ùëêùëúùëüùëüùëíùëêùë° _ùëõùë¢ùëö+ = |ùê¥ùëüùëíùëé (ùëãùëõùëíùë§, ùëñ,ùëá ) | √ó ùê¥ùëüùëíùëéùê¥ùëêùëêùëñ ;

4
5 end
6 ùê¥ùëêùëê1 = ùëêùëúùëüùëüùëíùëêùë° _ùëõùë¢ùëö/ùëôùëíùëõ (ùëãùëõùëíùë§ ) ;
7 ùê¥ùëêùëêùëúùëüùëñ = ùëíùë£ùëéùëôùë¢ùëéùë°ùëí (ùëÄ, ùëãùëúùëüùëñ );
8 ùê¥ùëêùëê2 = |ùê¥ùëüùëíùëé (ùëãùëõùëíùë§ ,ùëõ‚àí1) |/ùëôùëíùëõ (ùëãùëõùëíùë§ )
9 ùê¥ùëêùëêùëõùëíùë§ = (ùê¥ùëêùëê1 + ùê¥ùëêùëê2) /2 ;
10 return ùê¥ùëêùëêùëõùëíùë§ ;

|ùê¥ùëüùëíùëé (ùëãùëúùëüùëñ ,ùëõ‚àí1) |/ùëôùëíùëõ (ùëãùëúùëüùëñ ) √ó ùê¥ùëêùëêùëúùëüùëñ ;

and calculate the accuracy of the data in each area ùê¥ùëüùëíùëéùê¥ùëêùëêùëñ
(lines 2 and 3). Then, the same as the ùëãùëúùëüùëñ , we split ùëãùëõùëíùë§ into
ùëõ areas and use the area accuracy ùê¥ùëüùëíùëéùê¥ùëêùëêùëñ to estimate the
correctly predicted number ùëêùëúùëüùëüùëíùëêùë°_ùëõùë¢ùëö of ùëãùëõùëíùë§ (line 4).
(2) Then, we perform the accuracy estimation. First, according to
finding 1, Aries directly computes the accuracy using the correct
data number of new data in each area and produces the first
estimation ùê¥ùëêùëê1 (line 6). Then, based on finding 2, we calculate
the accuracy of the labeled data first ùê¥ùëêùëêùëúùëüùëñ (line 7). Afterward,
Aries computes the proportion of the high confident data in the
new unlabeled data to the original test data, and then estimates
the second accuracy (line 8). In the end, we compute the average
of the two estimated accuracy as the final output of Aries (line
9).

Example: Taking the cases in Figure 1 as an example with 3 Ar-
eas (ùëõ = 3), ùê¥ùëüùëíùëé1, ùê¥ùëüùëíùëé2, and ùê¥ùëüùëíùëé3. We assume that the accuracy
of original test data (ùê¥ùëêùëêùëöùëéùëù ) in ùê¥ùëüùëíùëé1, ùê¥ùëüùëíùëé2, and ùê¥ùëüùëíùëé3 are 60%,
70%, and 80%, respectively. And the number of original test data
(ùê¥ùëüùëíùëéùëÜùëñùëßùëíùëúùëüùëñ ) in each part is 200, 300, and 400, respectively. Then,
for the new unlabeled data, the number of data in each section
(ùê¥ùëüùëíùëéùëÜùëñùëßùëíùëõùëíùë§) is 300, 400, and 500. Then the ùê¥ùëêùëê1 is calculated by
(200 √ó 60% + 300 √ó 70% + 400 √ó 80%)/(200 + 300 + 400) = 72.22%.
Next, assume that the accuracy of the original test data is 70%, the
ùê¥ùëêùëê2 is calculated by ((500/1200)/(400/900)) √ó 70% = 66.35%. The
final output of Aries is (72.22% + 66.35%)/2 = 69.29%.

In Aries, the dropout prediction plays an important role to esti-
mate the distance to decision boundaries. Therefore, the dropout
rate is the first potential influencing factor. Next, the number of
areas that determines the splitting granularity could be the second
influencing factor. In addition, since our technique uses dropout pre-
diction and label change times to approximate the distance between
the data and the decision boundaries, the distance approximation
method is the third influencing factor. In Section 5.2 we will explore
the influence and the best settings of Aries for these three factors.

0.720.740.760.780.800.820.840.860.6000.6250.6500.6750.7000.7250.7500.7750.780.800.820.840.860.880.4250.4500.4750.5000.5250.5500.5750.6000.6250.500.520.540.560.580.600.620.180.200.220.240.260.280.300.3750.4000.4250.4500.4750.5000.5250.5500.5750.1750.2000.2250.2500.2750.3000.325Efficient Testing of Deep Neural Networks via Decision Boundary Analysis

Conference‚Äô17, July 2017, Washington, DC, USA

Table 1: Details of datasets and DNNs

Table 3: Details of our used model-level mutation operators.

Dataset

Classes Training Test

CIFAR-10

Tiny-ImageNet

10

200

50k

10k

100k

10k

ResNet20
VGG16
ResNet101
DenseNet121

DNN Parameters Accuracy (%)
87.44
91.39
74.09
70.70

274442
2859338
43036360
7242504

Table 2: Details of data transformation methods used for
generating new unlabeled data.

Description
Increase the brightness of the image data
Increase the contrast of the object
Add the defocus blur affect to the image
Elastic deformation of images
Add fog effect to the image
Add frost effect to the image
Add Gaussian Noise perturbation to the image

Data Type
Brightness
Contrast
Defocus Blur (DB)
Elastic Transform (ET)
Fog
Frost
Gaussian Noise (GN)
Jpeg Compression (JC) Change the image to Jpeg format
Motion Blur (MB)
Pixelate
Shot Noise (SN)
Snow
Zoom Blur (ZB)

Add the motion blur affect to the image
Convert image to pixelate style
Add noise by using the Poisson process
Add snow effect
Zoom the image data

4 EXPERIMENTAL SETUP
To evaluate the effectiveness of Aries, we conduct experiments
on 2 popular datasets, 4 DNN architectures, and 13 types of data
transformations that are utilized to generate the new unlabeled
data. Our evaluation answers the following research questions:

RQ1: How effective is Aries in estimating the accuracy of

DNNs?

RQ2: How does each component affect and contribute to

the effectiveness of Aries?

Subject datasets and DNN models. Table 1 presents the de-
tails of used datasets and models. CIFAR-10 [24] is a 10-class dataset
of color images, e.g., airplanes and birds. For this dataset, we build
two models, ResNet20 [15] and VGG16 [33]. Tiny-ImageNet [25] is a
more complex dataset that contains 200 image categories, e.g., gold-
fish and monarch. For this dataset, we build two models, ResNet101
and DenseNet121. In our experiments, we take the original test data
for the decision boundary analysis to estimate the accuracy of new
data.

For the new unlabeled data preparation, we directly use the
popular natural robustness benchmark dataset [16] for our exper-
iments. This benchmark provides two datasets, CIFAR-10-C and
Tiny-ImageNet-C, that are generated by adding common corrup-
tions and perturbations into the original test data, e.g., by increasing
the brightness of the image. It is widely used for evaluating the
model performance on distribution shifted data (unseen data). Be-
sides, a recent study [19] also demonstrates that these kinds of
corrupted data can be regarded as the out-of-distribution data, be-
cause the distance between these data and the original test data is
farther than the distance between the real out-of-distribution data
and the original test data. In our evaluation, we collect 13 common
types of sets that CIFAR-10 and Tiny-ImageNet both include. Table
2 introduces the details of each set.

Baseline. We first compare the estimated accuracy to the real
accuracy to check if Aries can precisely estimate the model perfor-
mance. Then, since there are no existing methods that estimate the

Level

Weight

Neuron

Operator
Gaussian fuzzing
Weight Shuffle
Neuron Effect Block
Neuron Activation Inverse
Neuron Switch

Description
Fuzz the weights using Gaussian noise
Shuffle the weights in the same neuron
Block a neuron effect, i.e., change the output to 0
Invert the activation status
Switch two neurons in the same layer

model performance without using the label information. We take
the state-of-the-art test selection-based model evaluation metrics,
Cross Entropy-based Sampling (CES) [27] and Practical Accuracy
Estimation (PACE) [5], as baselines. Besides, due to that this work
is the first one that evaluates the existing test selection metrics on
the natural robustness benchmark, we also consider the random
selection as the third baseline. Remarkably, all these three baselines
require selecting and labeling a subset from the new unlabeled data
to perform testing. We follow the same configuration as the paper
[5] to set the labeling budget from 50 to 180 in intervals of 10 for
test selection metrics.
(1) Cross Entropy-based Sampling (CES) selects the data that
have the minimum cross-entropy with the entire test dataset. It
starts from a randomly selected small size of data and iteratively
increases the size by adding other data while controlling the
cross-entropy.

(2) Practical Accuracy Estimation (PACE) first clusters the data
based on the output in the last hidden layer into different groups
by using the hierarchical density-based spatial clustering of
applications with noise clustering method, then utilizes the
example-based explanation algorithm MMD-critic to select the
most representative data from each group to label and test the
model.
Aries configuration. To reduce the number of hyperparameters
in Aries, we set the dropout prediction number to be the same as the
number of ùê¥ùëüùëíùëéùë† (ùëá = ùëõ in algorithm 1), which means in each ùê¥ùëüùëíùëé,
all the data have the same ùêøùëâ ùëÖ score. Then, there are two remaining
hyperparameters we need to set 1) the number of ùê¥ùëüùëíùëéùë† we split
and 2) the dropout rate. The default setting of the number of ùê¥ùëüùëíùëéùë†
and the dropout rate in RQ1 is 50 and 0.5, respectively. In RQ2, we
study the different ùê¥ùëüùëíùëé number settings 10, 50, 100, and 150, and
different dropout rate settings 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and
0.9. Since the setting space is infinite and it‚Äôs impossible to study
all, we recommend the best one among our studied settings and
show that by this setting we can already get acceptable estimation
results.

Model Mutation. In RQ2, we investigate if the DNN model
mutation [20, 21] can be used to replace the dropout prediction
for the distance of data to boundary approximation. Similar to the
dropout, mutation can produce a variant of the original model with
a similar accuracy without retraining the model from scratch [20].
The difference is that the dropout technique tends to fully ignore
some randomly selected neurons, while the mutation technique can
also work on the weight of neurons and the neuron status. First, we
randomly use the weight-level and neuron-level mutation operators
provided by [20] to generate mutants. The detailed information
of the operators is presented in table 3. To preserve the quality of
the mutants, we set the mutation ratio as 0.1 (0.01) for CIFAR-10
(Tiny-ImageNet) models, and the accuracy threshold as 0.9. Then,

Conference‚Äô17, July 2017, Washington, DC, USA

Hu et al.

Table 4: Results of accuracy (%) estimation on the test data
(New) used in the preliminary study. ùëÖùëíùëéùëô: real accuracy
of the set, ùê¥ùëêùëê1, ùê¥ùëêùëê2, and ùê¥ùëêùëêùëõùëíùë§ are estimated by line 6,
line 8, and line 9 in Algorithm 1, respectively. ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí
means the absolute difference between the estimated accu-
racy and the real accuracy. Highlighted values indicate the
best among the three estimations.

CIFAR-10

Tiny-ImageNet

ResNet20 VGG16 ResNet101 DenseNet121

ùëÖùëíùëéùëô
ùê¥ùëêùëê1
ùê¥ùëêùëê2
ùê¥ùëêùëêùëõùëíùë§
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí1
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí2
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëíùëõùëíùë§

87.28
87.41
87.35
87.38
0.13
0.07
0.10

91.18
91.33
90.81
91.07
0.15
0.37
0.11

74.16
73.73
73.39
73.56
0.43
0.77
0.60

71.98
71.93
70.96
71.45
0.05
1.02
0.54

we follow the steps in Algorithm 1 to estimate the accuracy of the
model on the new unlabeled data.

Implementation and Environments. We implement Aries in
Python based on TensorFlow [1] framework. For the baselines CES
and PACE, we utilize their original implementation. For the model
mutation, we also use the available mutation framework directly
provided by the authors. We conduct all the experiments on a 2.6
GHz Intel Xeon Gold 6132 CPU with an NVIDIA Tesla V100 16G
SXM2 GPU. To counteract randomized factors, we repeat all the
experiments 5 times and report the average results in this paper.
Due to the page limit, we put more detailed results as well as the
source code for reproducible study of this paper at our anonymous
website https://github.com/Anony4paper/Aries.

5 RESULTS ANALYSIS
5.1 RQ1: Effectiveness of Aries
Effectiveness on in-distribution data. First, we evaluate the ac-
curacy estimation effectiveness of the data (the two sets randomly
split from the original test data) used in our preliminary study
(Section 3.2). Table 4 lists the results where ùê¥ùëêùëê1, ùê¥ùëêùëê2, and ùê¥ùëêùëêùëõùëíùë§
are calculated by the line 6, line 8, and line 9 in Algorithm 1, re-
spectively. Namely, ùê¥ùëêùëê1 is the accuracy estimated by the ùê¥ùëüùëíùëé
accuracy. ùê¥ùëêùëê2 is the accuracy estimated by using the size of high
confident data. ùê¥ùëêùëêùëõùëíùë§ is the final estimated accuracy of ùê¥ùëüùëñùëíùë†, the
weighted sum of ùê¥ùëêùëê1 and ùê¥ùëêùëê2. We report ùê¥ùëêùëê1 and ùê¥ùëêùëê2 to verify
the importance of each component of our technique. The results
demonstrate that all the three estimations can predict the model
accuracy on the new data that follow the same data distribution
as the original test data with the difference slightly ranging from
0.07% to 1.02%. In addition, it‚Äôs hard to determine which estimation
is the best since one can perform better or worse than the others in
different datasets and models.

Effectiveness on data with distribution shift. In real-world
applications, software developers are more interested in the data
that follow various data distributions since after the model has
been deployed in the wild, the distribution of new unlabeled data is
uncontrollable. Thus, we evaluate Aries using the data that contain
different data distributions. Table 5 summarizes the results of the
accuracy estimation on the 13 types of distribution shifted test

sets. The same as the results of the preliminary study, we report
all the three estimations here to analyze the contribution of each
part. Overall, different from the results on the original test data, the
ùê¥ùëêùëêùëõùëíùë§ is closer to the real accuracy in most cases (42 out of 52)
than ùê¥ùëêùëê1 and ùê¥ùëêùëê2. On average, compared to the real accuracy, the
difference of estimated accuracy ùê¥ùëêùëêùëõùëíùë§ ranges from 0.03% to 2.60%
across all the datasets and models. And for each models, the average
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëíùëõùëíùë§ is smaller than 1% (0.52%, 0.91%, 0.27%, and 0.85%
for ResNet20, VGG16, ResNet101, and DenseNet121). Surprisingly,
for Tiny-ImageNet, ResNet101, which has the most complex model
architecture among all the models we considered, all the estimated
biases are smaller than 0.59%. This is different from the common
sense that proposed methods usually perform well on simple tasks
but become worse on complex tasks. However, our technique is
flexible and still effective on complex datasets and models.

More specifically, the results reveal that, when we utilize Aries to
estimate the accuracy of distribution shifted data, only considering
the ùê¥ùëêùëê1 or ùê¥ùëêùëê2 is not enough, especially on the complex task (e.g.,
Tiny-ImageNet). The average of ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí1 and ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí2 of
Tiny-ImageNet is greater than 7%, which is a very big bias. To
understand why ùê¥ùëêùëêùëõùëíùë§ works, we check the results of ùê¥ùëêùëê1 and
ùê¥ùëêùëê2 separately and find that, generally, the ùê¥ùëêùëê1 is over-estimation
(48 out of 52 cases) while the ùê¥ùëêùëê2 is under-estimation (48 out of
52 cases). We conjecture that this is because the learned decision
boundary can not thoroughly work well on the data that have
shifted distribution. The potential reason is that, in fact, the per-
formance of the model on the high confident data (ùêøùëâ ùëÖ is 1) of the
shifted data can be lower than the original test data, e.g., 99.57%
(original test data) vs 95.10% (Brightness data) of CIFAR-10-VGG16
model. The high confident data have a large proportion over all
the data (e.g., there are 7969 data whose label consistent time is 50
for CIFAR-10-VGG16). Then, the results (line 4 in algorithm 1) can
be overestimated. On the other hand, there are more data that the
model has low confidence in but are still correctly predicted, e.g.,
43, 51, 44 (Brightness data) vs 24, 27, 18 (original test data) when
ùêøùëâ ùëÖ times are 0.6, 0.62, and 0.64 of CIFAR-10-VGG16 model. This
can make the size of high confident data in the shifted set as well as
the ùê¥ùëêùëê2 under-estimation. However, ùê¥ùëêùëêùëõùëíùë§ finally averages the
under and over estimation and produces a more precise accuracy.
In the remaining experiments, we only report the results of ùê¥ùëêùëêùëõùëíùë§.
Comparison with test selection metrics. Next, we compare
our technique with test selection metrics. Figure 4 presents the
results of two shifted test sets with the Brightness and Contrast
transformations. Due to the space limitation, we put the whole
results on our site 1. First, only considering the results produced by
each test selection metric, we found that there are some conflicting
conclusions compared to the original papers. For example, [5] re-
ports the PACE outperforms the CES in their evaluation settings.
However, from our results, we can see only two settings, CIFAR-
10-VGG16-Brightness and CIFAR-10-VGG16-Contrast, PACE sig-
nificantly outperforms CES. Under other settings, the results of
these two methods fluctuate greatly. The same conflict occurs in
random selection. In our evaluation, the existing well-designed
test selection metrics cannot consistently perform better than the
random selection. This phenomenon reflects that, the evaluation of
existing test selection metrics for accuracy estimation is insufficient.

Efficient Testing of Deep Neural Networks via Decision Boundary Analysis

Conference‚Äô17, July 2017, Washington, DC, USA

Table 5: Results of accuracy (%) estimation given distribution shifted test sets. ùëÖùëíùëéùëô: real accuracy of the set, ùê¥ùëêùëê1, ùê¥ùëêùëê2, and
ùê¥ùëêùëêùëõùëíùë§ are estimated by line 6, line 8, and line 9 in algorithm 1, respectively. ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí is the absolute difference between the
estimated accuracy and the real accuracy. Highlighted values indicate the best among the three estimations.

Dataset

DNN

CIFAR-10

ResNet20

VGG16

ResNet101

Tiny-ImageNet

DenseNet121

ùëÖùëíùëéùëô
ùê¥ùëêùëê1
ùê¥ùëêùëê2
ùê¥ùëêùëêùëõùëíùë§
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí1
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí2
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëíùëõùëíùë§
ùëÖùëíùëéùëô
ùê¥ùëêùëê1
ùê¥ùëêùëê2
ùê¥ùëêùëêùëõùëíùë§
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí1
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí2
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëíùëõùëíùë§
ùëÖùëíùëéùëô
ùê¥ùëêùëê1
ùê¥ùëêùëê2
ùê¥ùëêùëêùëõùëíùë§
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí1
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí2
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëíùëõùëíùë§
ùëÖùëíùëéùëô
ùê¥ùëêùëê1
ùê¥ùëêùëê2
ùê¥ùëêùëêùëõùëíùë§
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí1
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëí2
ùê∑ùëñ ùëì ùëì ùëíùëüùëíùëõùëêùëíùëõùëíùë§

Brightness Contrast DB
87.22
87.13
86.63
86.88
0.09
0.59
0.34
91.78
91.79
93.25
92.52
0.01
1.47
0.74
55.77
63.57
48.74
56.15
7.80
7.03
0.38
44.9
57.29
37.48
47.39
12.39
7.42
2.49

85.25
86.15
84.00
85.07
0.90
1.25
0.18
90.96
91.21
89.17
90.19
0.25
1.79
0.77
50.18
60.94
39.35
50.15
10.76
10.83
0.03
36.19
44.05
29.08
36.56
7.86
7.11
0.37

87.07
87.33
87.30
87.32
0.26
0.23
0.25
91.41
91.40
90.76
91.08
0.01
0.65
0.33
63.93
68.41
58.87
63.64
4.48
5.06
0.29
58.26
63.93
52.46
58.19
5.67
5.80
0.07

ET
79.11
83.14
76.53
79.83
4.03
2.58
0.72
88.69
89.51
87.70
88.61
0.82
0.99
0.08
55.33
62.94
47.88
55.41
7.61
7.45
0.08
46.94
57.63
35.96
46.79
10.69
10.98
0.15

Fog
86.29
86.71
85.46
86.08
0.42
0.83
0.21
91.15
91.34
90.64
90.99
0.19
0.51
0.16
60.23
65.93
53.42
59.68
5.70
6.81
0.55
53.58
60.99
45.44
53.22
7.41
8.14
0.36

Frost GN
87.2
82.04
87.10
84.77
86.60
80.15
86.85
82.46
0.10
2.73
0.60
1.89
0.35
0.42
91.78
87.39
91.78
89.03
93.04
80.54
92.41
84.79
0.00
1.64
1.26
6.85
0.63
2.60
57.96
57.6
64.99
64.60
51.38
51.16
58.19
57.88
7.03
7.00
6.58
6.44
0.23
0.28
51.85
49.83
58.67
59.75
45.79
42.17
52.23
50.96
6.82
9.92
6.06
7.66
0.38
1.13

JC
81.46
83.77
80.26
82.01
2.31
1.20
0.55
87.12
89.05
83.74
86.39
1.93
3.38
0.73
59.58
65.84
53.61
59.73
6.26
5.97
0.15
52.37
60.69
45.05
52.87
8.32
7.32
0.50

MB
78.21
82.54
74.35
78.44
4.33
3.86
0.23
89.55
90.49
87.56
89.02
0.94
1.99
0.53
56.86
64.13
49.74
56.94
7.27
7.12
0.08
47.54
58.67
40.21
49.44
11.13
7.33
1.90

Pixelate
82.8
85.71
82.96
84.34
2.91
0.16
1.54
88.81
89.86
83.13
86.49
1.05
5.68
2.19
62.37
67.59
57.05
62.32
5.22
5.32
0.05
57.4
63.15
51.72
57.43
5.75
5.68
0.03

SN
78.96
82.71
76.69
79.70
3.75
2.27
0.74
85.97
88.30
84.19
86.24
2.33
1.78
0.27
57.17
64.44
50.51
57.48
7.27
6.66
0.31
51.26
58.69
44.95
51.82
7.43
6.31
0.56

Snow ZB
76.4
80.45
81.44
83.69
71.43
79.09
76.44
81.39
5.04
3.24
4.97
1.36
0.04
0.94
88.55
85.77
89.37
88.08
86.10
80.56
87.74
84.32
0.82
2.31
2.45
5.21
0.81
1.45
53.15
57.81
62.31
64.36
42.90
50.08
52.61
57.22
9.16
6.55
10.25
7.73
0.54
0.59
42.28
49.74
53.22
60.25
32.16
43.06
42.69
51.66
10.94
10.51
10.12
6.68
0.41
1.92

Average
82.50
84.78
80.88
82.83
2.32
1.68
0.50
89.15
90.09
86.95
88.52
0.95
2.62
0.87
57.53
64.62
50.36
57.49
7.09
7.17
0.27
49.40
58.23
41.96
50.10
8.83
7.43
0.79

(a) CIFAR10, ResNet20, Brightness

(b) CIFAR10, ResNet20, Contrast

(c) CIFAR10, VGG16, Brightness

(d) CIFAR10, VGG16, Contrast

(e) ImageNet, ResNet, Brightness

(f) ImageNet, ResNet, Contrast

(g) ImageNet, DenseNet, Brightness

(h) ImageNet, DenseNet, Contrast

Figure 4: Comparison with test selection metrics.

The distribution shifted test sets should be considered during the
evaluation.

By comparison, we can see that Aries achieves competitive re-
sults with the test selection metrics. Even though in some situations,
the selection-based methods achieve better results than our tech-
nique, e.g., in CIFAR-10-ResNet20-Brightness, when the labeling

budget is 90, CES can estimate the accuracy more precisely. Over-
all, our technique performs the best in most cases (96 out of 128).
Besides, Aries achieves more stable performance than the selection-
based methods. For example, in CIFAR-10-ResNet20-Brightness,
the estimation bias of CES can vary from 0.01% to 2.44% by using
different labeling budgets, which could waste human resources
while obtaining unexpected results.

5060708090100110120130140150160170180Labeling Budget0123456Accuracy Diff(%)CESRandomPACEAries5060708090100110120130140150160170180Labeling Budget012345678Accuracy Diff(%)CESRandomPACEAries5060708090100110120130140150160170180Labeling Budget0246810Accuracy Diff(%)CESRandomPACEAries5060708090100110120130140150160170180Labeling Budget123456789Accuracy Diff(%)CESRandomPACEAries5060708090100110120130140150160170180Labeling Budget0510152025Accuracy Diff(%)CESRandomPACEAries5060708090100110120130140150160170180Labeling Budget0510152025303540Accuracy Diff(%)CESRandomPACEAries5060708090100110120130140150160170180Labeling Budget0.02.55.07.510.012.515.017.5Accuracy Diff(%)CESRandomPACEAries5060708090100110120130140150160170180Labeling Budget0510152025Accuracy Diff(%)CESRandomPACEAriesConference‚Äô17, July 2017, Washington, DC, USA

Hu et al.

Answer to RQ1: Given the considered datasets and models,
Aries can estimate the accuracy of models on different test sets
with a slight bias ranging from 0.03% to 2.60%. Besides, our tech-
nique outperforms test selection-based methods in 96 (out of
128) cases and requires no labeling effort.

5.2 RQ2: Influencing Factor Study
Next, we explore how different configurations and settings affect
the performance of Aries. As mentioned in Section 3.3, there are
three main factors we need to consider, the number of areas (ùëõ in
algorithm 1), the dropout rate of the dropout layer, and the distance
approximation method.

(a) CIFAR10-ResNet20

(b) CIFAR10-VGG16

(c) ImageNet-ResNet101

(d) ImageNet-DenseNet121

Figure 5: Accuracy difference (%) between the real accuracy
and the estimated accuracy by using different ùê¥ùëüùëíùëé numbers.
Number of Areas. Figure 5 presents the results of the accuracy
estimation using Aries by different settings of the area number. The
first conclusion we can draw is that, this factor has quite an impact
on the results. For instance, in ImageNet-DenseNet121-Contrast,
using 10 areas can increase the accuracy difference by almost 10%
than using 50 areas. However, it‚Äôs clear that there is no such setting
that performs consistently better than others in all datasets and
models. For example, in CIFAR10-VGG16, ùëõ = 10 is a relatively
good setting. However, in CIFAR10-ResNet20, ùëõ = 10 is the worst
among the four settings which means the setting of ùê¥ùëüùëíùëé number
is highly datasets and model-dependent. But if we check the more
detailed values, we can see that, in total, in 5, 28, 6, and 13 cases,
using 10, 50, 100, and 150 areas can achieve the best estimation
results, respectively. And on average, the differences between the
estimated accuracy and the real accuracy are 2.62%, 0.61%, 1.39%,
and 1.61%, respectively. Therefore, for the number of areas, even
though there can be no best setting, 50 is recommended among the
studied settings. Conclusion: The number of ùê¥ùëüùëíùëé highly impacts
the performance of Aries. However, this hyperparameter setting is
dataset and model-dependent. Thus, users should set this number
according to the real use cases. ùëõ = 50 is a default setting of Aries.

Figure 6: The trend of average difference between real ac-
curacy and estimated accuracy by using different dropout
rates. The rate at 0.5 is a common turning point of the accu-
racy difference for all datasets and models.

Dropout Rate. Table 6 presents the difference between the esti-
mated accuracy by using Aries and the real accuracy under different
dropout rates. From the results, we can see that similar to the study
of the ùê¥ùëüùëíùëé number, there is no dropout rate setting that can con-
sistently outperform others. But still, the relatively better setting
exists by a deeper analysis. In total, in 6, 5, 3, 10, 19, 4, 2, 6, and
1 cases, the dropout rate 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9
achieve the best estimation results, respectively. This indicates that
the dropout rate = 0.5 is the best among all the studied settings in
terms of achieving the most precise estimation. Then, looking into
the average results, we draw a similar conclusion that when we set
the dropout rate as 0.5, in 3 (out of 4) cases, the estimated accuracy
is less different from the real accuracy compared to other settings.
Figure 6 depicts the trend of the average difference between real
accuracy and estimated accuracy by using different dropout rates
(Column Average in Table 6). We can see that, the difference drops
first when the dropout rate increases, and after the dropout rate
reaches around 0.5, the difference increases. Conclusion: There is
no dropout rate setting that always performs the best. We recom-
mend using the dropout rate of around 0.5 for Aries to gain better
results.

Mutant for Distance Estimation. Finally, since at each pre-
diction time, the dropout model can be seen as a mutant of the
original model, we explore if we can utilize model mutation to
replace dropout prediction to approximate the distance between
data and decision boundaries.

Table 7 presents the results of the comparison between the two
ways of approximating the distance between the data and the deci-
sion boundaries. We can see that after we replace the dropout to
mutants (change dropout model ùëÄ in definition 1 to each mutant),
Aries can still work in some cases. For the CIFAR-10 dataset, Aries
with mutants can still produce some acceptable results, e.g., in 10
cases, the difference is smaller than 1%. And compared to the Aries
with dropout, in 10 out of 26 cases, the mutant achieves better re-
sults. And on average, there are only 1.9% and 0.42% effectiveness
gaps between these two ways. However, the performance of Aries
with mutants becomes much worse in the Tiny-ImageNet dataset,
and the difference increases a lot compared to using dropout. In all
the cases, Aries with mutants performs worse than with dropout.
This phenomenon indicates that Aries with mutants can only work
on simple datasets and models. This is reasonable because, at each

BrightnessContrastDBETFogFrostGNJCMBPixelateSNSnowZBData Type0.00.51.01.52.02.53.03.5Accuracy Diff(%)1050100150BrightnessContrastDBETFogFrostGNJCMBPixelateSNSnowZBData Type0123456Accuracy Diff(%)1050100150BrightnessContrastDBETFogFrostGNJCMBPixelateSNSnowZBData Type0.00.51.01.52.02.53.03.54.0Accuracy Diff(%)1050100150BrightnessContrastDBETFogFrostGNJCMBPixelateSNSnowZBData Type0246810Accuracy Diff(%)10501001500.10.20.30.40.50.60.70.80.9Dropout Rate0246810121416Accuracy Diff(%)CIFAR10-ResNet20CIFAR10-VGG16Tiny-ImageNet-ResNet101Tiny-ImageNet-DenseNet121Efficient Testing of Deep Neural Networks via Decision Boundary Analysis

Conference‚Äô17, July 2017, Washington, DC, USA

Table 6: The difference (%) between the real accuracy and by Aries using different dropout rates. DR: Dropout rate.

Dataset

DNN

CIFAR-10

ResNet20

VGG16

ResNet101

Tiny-ImageNet

DenseNet121

DR Brightness Contrast DB
0.10
0.1
0.03
0.2
0.37
0.3
0.29
0.4
0.34
0.5
0.37
0.6
0.55
0.7
0.71
0.8
1.63
0.9
0.24
0.1
0.13
0.2
0.50
0.3
0.58
0.4
0.74
0.5
1.44
0.6
0.13
0.7
3.09
0.8
20.54
0.9
5.14
0.1
3.33
0.2
1.73
0.3
0.11
0.4
0.38
0.5
1.69
0.6
2.92
0.7
3.30
0.8
3.60
0.9
2.15
0.1
2.56
0.2
2.41
0.3
2.71
0.4
2.49
0.5
2.34
0.6
2.39
0.7
2.15
0.8
2.51
0.9

1.27
0.94
0.45
0.14
0.18
0.58
1.22
2.68
5.70
0.03
0.23
0.09
0.29
0.77
0.37
1.19
7.92
18.12
7.44
5.24
3.23
1.29
0.03
1.44
3.10
3.95
2.88
5.41
0.34
1.52
0.83
0.37
0.78
0.55
0.39
0.65

0.31
0.26
0.15
0.19
0.25
0.27
0.38
0.26
0.28
0.09
0.25
0.08
0.04
0.33
0.07
1.37
2.14
4.44
3.12
2.36
1.37
0.18
0.29
1.11
1.82
0.16
5.81
0.24
0.09
0.09
0.36
0.07
0.37
0.21
0.46
0.08

ET
4.79
3.53
2.45
1.55
0.72
0.51
2.38
5.13
10.76
0.15
1.37
2.29
3.28
0.08
6.66
11.16
17.70
27.27
4.86
3.01
1.36
0.60
0.08
2.70
3.36
4.03
8.32
1.40
0.05
0.31
0.46
0.15
0.42
0.01
0.15
0.29

Fog
0.44
0.39
0.20
0.07
0.21
0.35
0.90
1.56
3.91
0.11
0.03
0.02
0.13
0.16
0.13
0.93
6.21
1.19
3.74
2.25
0.75
0.22
0.55
2.01
2.82
2.32
4.12
0.28
0.55
0.52
0.02
0.36
0.38
0.60
0.37
0.60

Frost GN
0.08
3.33
0.11
2.64
0.33
1.88
0.30
1.16
0.35
0.42
0.27
0.36
0.48
1.50
0.56
3.33
1.57
8.06
0.09
0.56
0.25
0.89
0.61
1.78
0.46
3.12
0.63
2.60
0.83
6.15
1.00
9.28
1.31
14.39
7.01
16.61
5.02
4.46
3.60
3.01
2.02
1.81
0.29
0.37
0.23
0.28
1.44
1.65
2.94
2.66
3.03
4.16
1.09
1.02
1.04
1.03
1.66
1.11
1.36
0.84
1.60
1.45
0.38
1.13
1.77
1.30
1.08
1.00
1.35
0.98
1.16
0.97

JC
3.91
3.12
2.39
1.75
0.55
0.42
0.34
2.22
6.40
0.99
0.56
1.47
2.41
0.73
4.96
7.87
14.03
20.87
4.09
2.72
1.23
0.24
0.15
1.75
3.15
2.52
1.13
0.30
0.38
0.34
0.75
0.50
0.63
0.35
0.33
0.23

MB
5.82
3.99
2.58
1.28
0.23
1.37
3.14
5.91
11.21
0.23
0.70
1.09
2.02
0.53
5.28
9.80
16.51
25.63
4.90
2.78
1.42
0.06
0.08
2.14
2.72
2.83
1.86
1.45
1.49
1.62
1.84
1.90
1.85
1.57
1.43
1.58

Pixelate
3.20
2.66
2.29
1.86
1.54
1.33
0.92
0.16
1.65
0.55
0.31
0.83
1.41
2.19
3.46
4.50
6.66
0.79
3.85
2.76
1.57
0.36
0.05
1.30
2.56
1.86
3.06
0.24
0.17
0.14
0.15
0.03
0.06
0.35
0.62
0.01

SN
5.60
4.43
3.40
2.46
0.74
0.52
0.61
2.36
6.66
0.82
0.73
2.09
0.52
0.27
6.20
7.65
10.76
10.75
4.97
3.42
1.78
0.06
0.31
2.36
3.80
4.04
3.37
1.26
1.40
1.62
1.76
0.56
1.85
2.01
1.33
1.95

Snow ZB
6.40
4.84
4.67
4.03
3.04
3.21
1.67
2.47
0.04
0.94
1.48
1.42
3.67
0.80
7.00
0.73
14.16
5.30
0.55
0.79
1.85
1.03
2.97
2.03
4.01
0.00
0.81
1.45
8.01
8.14
12.75
11.22
22.79
15.33
30.86
24.61
6.18
4.02
4.19
2.31
2.57
0.82
0.56
0.53
0.54
0.59
1.68
3.08
3.37
4.26
3.36
3.99
0.83
6.34
3.20
1.76
1.36
1.78
1.09
2.08
0.66
2.42
0.41
1.92
0.33
1.65
1.07
1.92
1.45
1.64
1.52
1.94

Average
3.08
2.37
1.75
1.17
0.50
0.71
1.30
2.51
5.95
0.40
0.64
1.22
1.41
0.87
3.98
6.07
10.68
16.05
4.75
3.15
1.67
0.37
0.27
1.87
3.04
3.04
3.34
1.52
1.00
1.07
1.15
0.79
1.06
1.01
0.97
1.04

Table 7: Comparison between two techniques (dropout and
mutant). Diff: the absolute difference between the real ac-
curacy and estimated accuracy by using mutants. CD: the
difference between the results produced by two techniques.

Data Type

Brightness
Contrast
DB
ET
Fog
Frost
GN
JC
MB
Pixelate
SN
Snow
ZB
Average

CIFAR-10

Tiny-ImageNet

ResNet20
CD
Diff
+0.13
0.12
-0.77
0.95
+0.10
0.24
-2.57
3.30
-0.30
0.51
-2.55
2.97
+0.10
0.25
-2.82
3.38
-3.80
4.04
-1.26
2.80
-3.75
4.49
-2.72
3.66
-4.47
4.50
-1.90
2.40

VGG16

Diff
0.01
0.00
0.15
1.19
0.14
2.36
0.21
2.68
1.17
1.56
3.00
2.95
1.26
1.28

CD
+0.32
+0.77
+0.59
-1.11
+0.02
+0.24
+0.42
-1.96
-0.65
+0.63
-2.73
-1.50
-0.45
-0.42

ResNet101
CD
Diff
-4.20
4.49
-10.63
10.67
-7.42
7.80
-7.64
7.72
-5.47
6.02
-6.80
7.08
-7.05
7.28
-6.24
6.39
-7.27
7.34
-5.38
5.43
-7.04
7.34
-6.08
6.67
-8.73
9.28
-6.92
7.19

DesNet121
CD
Diff
-6.64
6.71
-19.25
19.62
-11.60
14.08
-11.59
11.74
-8.07
8.43
-10.12
11.25
-9.57
9.94
-9.02
9.52
-10.87
12.77
-6.40
6.43
-10.12
10.67
-9.69
11.61
-15.09
15.50
-10.62
11.41

prediction time, the status of the dropout model might be appeared
in the training process due to its design nature [34]. Thus, it can
reflect the learned decision boundary more precisely. However,

the post-training model mutation randomly modifies the model,
which could totally change the learned decision boundary even if
it maintains the accuracy. Conslusion: Aries with using mutants
can achieve similar accuracy estimation results with using dropout
in CIFAR-10 dataset, while fails in Tiny-ImageNet dataset. It needs
more careful mutation operator selection and hyperparameter tun-
ing to ensure the decision boundary does not change too much
after model mutation.

Answer to RQ2: The number of areas and dropout rate affect the
performance of Aries. 50 and 0.5 are the recommended settings
for ùê¥ùëüùëíùëé number and dropout rate. Simply replacing the dropout
with mutant can work on the simple dataset (CIFAR-10) but fails
on the more complex dataset (Tiny-ImageNet).

6 DISCUSSION AND THREAT TO VALIDITY
6.1 Limitations & Future Directions
Limitations. 1) The first potential limitation of Aries we can come
out is that it might suffer from adversarial attacks [31]. A strong
adversarial attack method can control the distance between the
adversarial examples and the decision boundaries by pushing the
original data close to or far away from the decision boundary,
which invalidates our learned boundary information. However,

Conference‚Äô17, July 2017, Washington, DC, USA

Hu et al.

the adversarial attack is a common concern for all methods. For
example, for the output-based methods (PACE and CES), white-box
adversarial attacks can be designed to change the output of the
neurons to force these metrics to select useless data to label. How to
defend against adversarial attacks is an open problem. 2) The second
limitation comes from our assumption that we already have some
labeled test data. In general, this assumption can stand. However,
in extreme cases where only the model and new unlabeled data are
available, we still need to undertake data labeling.

Future directions. 1) We only utilize the original labeled test
data to gain the decision boundary information, and it works well
in our evaluation subjects. There could be a way to do data aug-
mentation based on the labeled data and increase the data space
we have. In this way, we can learn more precise boundary infor-
mation and, therefore, make better accuracy estimations for the
new unlabeled data. 2) Although dropout uncertainty is the widely
studied uncertainty metric and works well in our technique, the
mutants prediction is the closest way to the dropout prediction.
Some other uncertainty metrics can be used to approximate the
distance between data to the decision boundaries, e.g., DeepGini
[9]. We plan to study more metrics and explore if there is a better
way to replace dropout prediction. On the other hand, adversarial
attacks can be used to achieve the same goal [7].

6.2 Threats to Validity
The internal threats to validity are the implementations of our
technique, the baselines, and mutation operators as well as the
preparation of new unlabeled data. Our technique is simple and easy
to implement and its core part is using pure Python. Also, we release
our code for future study. All the implementations of the baselines
and the mutation operators are from the original papers. For the
new unlabeled data, to reduce the influence of parameter settings
(e.g., which levels of brightness should be added), we directly reuse
the released datasets that are widely studied in the literature.

The external threats to validity come from the selected datasets
and models for our evaluation. For the dataset, we use two com-
monly studied ones from the recent research [2, 6, 26]. For each
dataset, we build two different model architectures from simple
to complex. Compared to the existing test selection works [5, 27]
which stop by ResNet-50, our considered model architectures are
more complex (ResNet101 and DenseNet121). And for the new un-
labeled data, we also follow the previous works [17, 18, 22] that
evaluate the model robustness to prepare our test sets.

7 RELATED WORK
We review the related in the following two aspects, which we think
are the most relevant to this paper.

7.1 Coverage Design and Automated Testing of

DNNs

Recently, DNN testing has become a very active research area
[3, 4, 30, 44], with quite a few techniques proposed to ensure the
quality of DNN from different angles. As a very early testing tech-
nique, DeepXplore, proposed by Pei et al., first defined the concept
of neuron coverage and utilized it as a measure to generate test
data to test DNN models. After that, Ma et al. proposed Deep-
Gauge, which contains more multi-granularity neuron coverage

criteria, e.g., k-multisection Neuron Coverage. Then, more prac-
tically, DeepTest [36] and TACTIC [28] utilize the search-based
method by using neuron coverage as a search objective to synthe-
size test data to test the DNN-based autonomous driving systems.
Fuzz testing [12], which is a famous testing technique in conven-
tional software engineering, has also been applied for DL testing
recently [14]. Xie et al. proposed DeepHunter [40], which gener-
ates test data by fuzzing while maximizing the neuron coverage.
Different from other methods, DeepHunter contains seed selection
and fuzzing two phases. Besides, to enhance the model training
and improve the robustness of trained models, Gao et al. proposed
SENSEI [11] that can optimize the data augmentation process by
the genetic algorithm at the model training time. Compared to these
testing techniques that target proposing testing criteria and test
data generation methods to help DL testing, our work focuses on
efficiently testing DL models by performance estimation.

7.2 Test Selection for DNN
As mentioned before, there are two categories of test selection
methods, test selection for data prioritization and test selection
for model performance estimation. For data prioritization, Kim et
al. proposed two test adequacy metrics, Likelihood-based Surprise
Adequacy (LSA) and Distance-based Surprise Adequacy (DSA) by
comparing the likelihood and distance between the training data
and test data. And they demonstrated these metrics can be used to
guide the retraining to produce more accurate models. Feng et al.
proposed DeepGini, a test prioritization technique based on the sta-
tistical perspective of the output of DNN models. The authors show
that compared to the neuron coverage-based methods, DeepGini is
a better technique for misclassified data indication and retraining
guidance. More recently, Wang et al. a mutation-based test prioriti-
zation method that mutates both the input data and models to find
the sensitive test data. On the other hand, for the test selection for
model performance estimation, there are two works, PACE [5] and
CES [27]. We introduced these two methods in Section 4. Compared
to the previous works, the main difference between our technique
is that our technique does not need any extra data labeling effort,
which makes the whole testing process for quality assessment of
DNN automatic and less expensive.

8 CONCLUSION
In this paper, we proposed a novel technique, Aries, that can auto-
matically estimate the accuracy of DNN models on the unlabeled
data without labeling effort. The main idea of Aries is that the model
should have similar prediction accuracy on the data that have simi-
lar distances to the decision boundaries. Specifically, Aries employs
the dropout uncertainty to approximate the distance between data
and decision boundaries and learns this boundary information from
the original labeled test data to estimate the accuracy of new unla-
beled data. Our evaluation of 2 commonly studied datasets, 4 model
architectures, and 13 types of new unlabeled data demonstrated
that Aries can precisely predict the accuracy of data. The differ-
ence between the estimated accuracy and the real accuracy ranges
from 0.03% to 2.60%. Besides, compared to the state-of-the-art test
selection-based methods PACE and CES which require labeling
effort, Aries still achieved better results in most (96 out of 128)
cases.

Efficient Testing of Deep Neural Networks via Decision Boundary Analysis

Conference‚Äô17, July 2017, Washington, DC, USA

REFERENCES
[1] Mart√≠n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
2016. Tensorflow: a system for large-scale machine learning. In 12th {USENIX}
symposium on operating systems design and implementation ({OSDI} 16). 265‚Äì283.
[2] Javad Zolfaghari Bengar, Joost van de Weijer, Bartlomiej Twardowski, and Bogdan
Raducanu. 2021. Reducing label effort: self-supervised meets active learning. In
Proceedings of the IEEE/CVF International Conference on Computer Vision. 1631‚Äì
1639.

[3] Christian Birchler, Sajad Khatiri, Pouria Derakhshanfar, Sebastiano Panichella,
and Annibale Panichella. 2021. Automated test cases prioritization for self-driving
cars in virtual environments. arXiv preprint arXiv:2107.09614 (2021).

[4] Houssem Ben Braiek and Foutse Khomh. 2019. DeepEvolution: a search-based
testing approach for deep neural networks. In 2019 IEEE International Conference
on Software Maintenance and Evolution (ICSME). IEEE, 454‚Äì458.

[5] Junjie Chen, Zhuo Wu, Zan Wang, Hanmo You, Lingming Zhang, and Ming Yan.
2020. Practical accuracy estimation for efficient deep neural network testing.
ACM Transactions on Software Engineering and Methodology (TOSEM) 29, 4 (2020),
1‚Äì35.

[6] Jiequan Cui, Shu Liu, Liwei Wang, and Jiaya Jia. 2021. Learnable boundary guided
adversarial training. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 15721‚Äì15730.

[7] Melanie Ducoffe and Fr√©d√©ric Precioso. 2018. Adversarial active learning
for deep networks: a margin based approach. CoRR abs/1802.09841 (2018).
arXiv:1802.09841

[8] Emelie Engstr√∂m, Per Runeson, and Mats Skoglund. 2010. A systematic review
on regression test selection techniques. Information and Software Technology 52,
1 (2010), 14‚Äì30.

[9] Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and Zhenyu
Chen. 2020. Deepgini: prioritizing massive tests to enhance the robustness of
deep neural networks. In Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis. 177‚Äì188.

[10] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation:
Representing model uncertainty in deep learning. In international conference on
machine learning. PMLR, 1050‚Äì1059.

[11] Xiang Gao, Ripon K Saha, Mukul R Prasad, and Abhik Roychoudhury. 2020. Fuzz
testing based data augmentation to improve robustness of deep neural networks.
In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE).
IEEE, 1147‚Äì1158.

[12] Patrice Godefroid, Michael Y Levin, David A Molnar, et al. 2008. Automated

whitebox fuzz testing.. In NDSS, Vol. 8. 151‚Äì166.

[13] Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. 2020.
A survey of deep learning techniques for autonomous driving. Journal of Field
Robotics 37, 3 (2020), 362‚Äì386.

[14] Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, and Jiaguang Sun. 2018. Dlfuzz:
differential fuzzing testing of deep learning systems. In Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 739‚Äì743.

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). 770‚Äì778. https://doi.org/10.1109/CVPR.2016.90
[16] Dan Hendrycks and Thomas Dietterich. 2019. Benchmarking neural net-
work robustness to common corruptions and perturbations. arXiv preprint
arXiv:1903.12261 (2019).

[17] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and
Balaji Lakshminarayanan. 2019. Augmix: a simple data processing method to
improve robustness and uncertainty. arXiv preprint arXiv:1912.02781 (2019).
[18] Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang, Dawn Song, and
Jacob Steinhardt. 2021. PixMix: dreamlike Pictures Comprehensively Improve
Safety Measures. arXiv preprint arXiv:2112.05135 (2021).

[19] Qiang Hu, Yuejun Guo, Maxime Cordy, Xiaofei Xie, Lei Ma, Mike Papadakis, and
Yves Le Traon. [n. d.]. An empirical study on data distribution-aware test selection
for deep learning enhancement. ACM Transactions on Software Engineering and
Methodology ([n. d.]).

[20] Qiang Hu, Lei Ma, Xiaofei Xie, Bing Yu, Yang Liu, and Jianjun Zhao. 2019. Deep-
mutation++: a mutation testing framework for deep learning systems. In 2019
34th IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 1158‚Äì1161.

[21] Nargiz Humbatova, Gunel Jahangirova, and Paolo Tonella. 2021. Deepcrime:
Mutation testing of deep learning systems based on real faults. In Proceedings of
the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis.
67‚Äì78.

[22] Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, and Jacob Steinhardt.
arXiv preprint

2019. Testing robustness against unforeseen adversaries.
arXiv:1908.08016 (2019).

[23] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang,
Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips,

Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw,
Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson,
Sergey Levine, Chelsea Finn, and Percy Liang. 2021. WILDS: a Benchmark of
in-the-Wild distribution shifts. In Proceedings of the 38th International Conference
on Machine Learning (Proceedings of Machine Learning Research, Vol. 139), Marina
Meila and Tong Zhang (Eds.). PMLR, 5637‚Äì5664. https://proceedings.mlr.press/
v139/koh21a.html

[24] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features

from tiny images. (2009).

[25] Ya Le and Xuan Yang. 2015. Tiny imagenet visual recognition challenge. CS 231N

7, 7 (2015), 3.

[26] Klas Leino, Zifan Wang, and Matt Fredrikson. 2021. Globally-robust neural
networks. In International Conference on Machine Learning. PMLR, 6212‚Äì6222.

[27] Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, and Jian L√º. 2019.
Boosting operational dnn testing efficiency through conditioning. In Proceedings
of the 2019 27th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering. 499‚Äì509.

[28] Zhong Li, Minxue Pan, Tian Zhang, and Xuandong Li. 2021. Testing DNN-
based Autonomous Driving Systems under Critical Environmental Conditions.
In International Conference on Machine Learning. PMLR, 6471‚Äì6482.

[29] Khan Muhammad, Amin Ullah, Jaime Lloret, Javier Del Ser, and Victor Hugo C
de Albuquerque. 2020. Deep learning for safe autonomous driving: Current
challenges and future directions. IEEE Transactions on Intelligent Transportation
Systems 22, 7 (2020), 4316‚Äì4336.

[30] Annibale Panichella and Cynthia CS Liem. 2021. What are we really testing in
mutation testing for machine learning? a critical reflection. In 2021 IEEE/ACM
43rd International Conference on Software Engineering: New Ideas and Emerging
Results (ICSE-NIER). IEEE, 66‚Äì70.

[31] Kui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. 2020. Adversarial attacks and

defenses in deep learning. Engineering 6, 3 (2020), 346‚Äì360.

[32] Gregg Rothermel and Mary Jean Harrold. 1997. A safe, efficient regression test
selection technique. ACM Transactions on Software Engineering and Methodology
(TOSEM) 6, 2 (1997), 173‚Äì210.

[33] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[34] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The journal of machine learning research 15, 1 (2014), 1929‚Äì1958.
[35] Yi Sun, Ding Liang, Xiaogang Wang, and Xiaoou Tang. 2015. Deepid3: face
recognition with very deep neural networks. arXiv preprint arXiv:1502.00873
(2015).

[36] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: automated
testing of deep-neural-network-driven autonomous cars. In Proceedings of the
40th international conference on software engineering. 303‚Äì314.

[37] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha√´l Mathieu, An-
drew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds,
Petko Georgiev, et al. 2019. Grandmaster level in StarCraft II using multi-agent
reinforcement learning. Nature 575, 7782 (2019), 350‚Äì354.

[38] Mei Wang and Weihong Deng. 2021. Deep face recognition: a survey. Neurocom-

puting 429 (2021), 215‚Äì244.

[39] Zan Wang, Hanmo You, Junjie Chen, Yingyi Zhang, Xuyuan Dong, and Wenbin
Zhang. 2021. Prioritizing test inputs for deep neural networks via mutation
analysis. In IEEE/ACM 43rd International Conference on Software Engineering
(ICSE). 397‚Äì409. https://doi.org/10.1109/ICSE43902.2021.00046

[40] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun
Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. Deephunter: a coverage-guided
fuzz testing framework for deep neural networks. In Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis. 146‚Äì157.
[41] Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen,
Zhao Liu, Fuhao Qiu, Hongsheng Yu, et al. 2020. Towards playing full moba games
with deep reinforcement learning. Advances in Neural Information Processing
Systems 33 (2020), 621‚Äì632.

[42] Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020. Machine learning testing:
IEEE Transactions on Software Engineering

survey, landscapes and horizons.
(2020).

[43] Xiyue Zhang, Xiaofei Xie, Lei Ma, Xiaoning Du, Qiang Hu, Yang Liu, Jianjun Zhao,
and Meng Sun. 2020. Towards characterizing adversarial defects of deep learning
software from the lens of uncertainty. In 2020 IEEE/ACM 42nd International
Conference on Software Engineering (ICSE). IEEE, 739‚Äì751.

[44] Jianyi Zhou, Feng Li, Jinhao Dong, Hongyu Zhang, and Dan Hao. 2020. Cost-
effective testing of a deep learning model through input reduction. In 2020 IEEE
31st International Symposium on Software Reliability Engineering (ISSRE). IEEE,
289‚Äì300.

[45] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019.
Devign: effective vulnerability identification by learning comprehensive program
semantics via graph neural networks. In Advances in Neural Information Processing
Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and
R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/

Conference‚Äô17, July 2017, Washington, DC, USA

Hu et al.

paper/2019/file/49265d2447bc3bbfe9e76306ce40a31f-Paper.pdf

