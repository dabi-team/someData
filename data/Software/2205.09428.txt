2
2
0
2

y
a
M
9
1

]
E
S
.
s
c
[

1
v
8
2
4
9
0
.
5
0
2
2
:
v
i
X
r
a

Which bugs are missed in code reviews: An empirical study on
SmartSHARK dataset

Fatemeh Khoshnoud∗
khoshnoud.fa@gmail.com
Department of CSE and IT, Shiraz University, Shiraz, Iran

Ali Rezaei Nasab∗
alirezaei@hafez.shirazu.ac.ir
Department of CSE and IT, Shiraz University, Shiraz, Iran

Zahra Toudeji∗
zahra.toudeji94@gmail.com
Department of CSE and IT, Shiraz University, Shiraz, Iran

Ashkan Sami†
sami@shirazu.ac.ir
Department of CSE and IT, Shiraz University, Shiraz, Iran

ABSTRACT
In pull-based development systems, code reviews and pull
request comments play important roles in improving code
quality. In such systems, reviewers attempt to carefully check
a piece of code by different unit tests. Unfortunately, some-
times they miss bugs in their review of pull requests, which
lead to quality degradations of the systems. In other words,
disastrous consequences occur when bugs are observed af-
ter merging the pull requests. The lack of a concrete under-
standing of these bugs led us to investigate and categorize
them. In this research, we try to identify missed bugs in pull
requests of SmartSHARK dataset projects. Our contribution
is twofold. First, we hypothesized merged pull requests that
have code reviews, code review comments, or pull request
comments after merging, may have missed bugs after the
code review. We considered these merged pull requests as
candidate pull requests having missed bugs. Based on our
assumption, we obtained 3,261 candidate pull requests from
77 open-source GitHub projects. After two rounds of restric-
tive manual analysis, we found 187 bugs missed in 173 pull
requests. In the first step, we found 224 buggy pull requests
containing missed bugs after merging the pull requests. Sec-
ondly, we defined and finalized a taxonomy that is appropri-
ate for the bugs that we found and then found the distribu-
tion of bug categories after analysing those pull requests all
over again. The categories of missed bugs in pull requests
and their distributions are: semantic (51.34%), build (15.5%),
analysis checks (9.09%), compatibility (7.49%), concurrency
(4.28%), configuration (4.28%), GUI (2.14%), API (2.14%), secu-
rity (2.14%), and memory (1.6%).

KEYWORDS
code review, pull request, missed bugs, SmartSHARK dataset

∗Three first authors contributed equally to this research.
†Corresponding Author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

ACM Reference Format:
Fatemeh Khoshnoud, Ali Rezaei Nasab, Zahra Toudeji, and Ashkan Sami.
2022. Which bugs are missed in code reviews: An empirical study on
SmartSHARK dataset. In Proceedings of MSR ’22: Proceedings of the 19th
International Conference on Mining Software Repositories (MSR 2022). ACM,
New York, NY, USA, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Code reviews and pull request comments, as careful checks of
source code by developers, are important parts of modern software
development [6]. These techniques are also recognized as valu-
able tools for reducing software issues (e.g., bugs) in the code and
improving the quality of software projects [2, 3]. Notably, in the
GitHub community, developers can fork from an existing project
into a new repository and then change parts of it, which are their
interest. Using a pull request, they can submit the changes to that
project. During pull request submission, the code reviewers are
responsible for review of the code carefully before accepting and
merging the changes as a contribution [11].

Unfortunately, sometimes code reviewers miss bugs in their
review of pull requests, which leads to quality degradations of
the systems. In other words, disastrous consequences occur in the
system, when bugs are found after merging a pull request. There is
no still a concrete understating of these bugs which encouraged us
to analyze and categorize them. Our research tries to formulate the
following research questions to reach this goal:

RQ1. How prevalent are possible missed bugs in code re-
views? We gathered code reviews, code review comments, and pull
request comments of 77 GitHub projects from the SmarkSHARK
dataset after merging the pull requests. We gained 3,261 candidate
pull requests. After analysing the candidate pull requests by three
analysts, we identified 224 buggy pull requests that are missed in
code reviews.

RQ2. Which categories of bugs are missed in code reviews?
The same analysts analyzed the buggy pull requests for the second
time to find the types of bugs and then categorized them. This
process led to 187 missed bugs from 173 pull requests which are
categorized into ten common bug categories.

RQ3. What are the distribution of the missed bugs in code
reviews? After finalizing the taxonomy of missed bugs, we found
the distributions of them. Our findings show semantic bug had
51.34% as the highest distribution and memory bug had 1.6% as the
lowest distribution. Also, we publicly released the implementation,
the protocol template, and the data collection used in this study
available [4].

 
 
 
 
 
 
MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

F. Khoshnoud et al.

Figure 1: An overview of our study

2 RELATED WORK
In this section, we discuss on relevant studies to this research. Yu
et al. [14] investigated the nature of Continuous Integration (CI)
failures and social factors are associated with them and how they
relate to eventual bugs. Zampetti et al. [15] investigated interplay
between pull request discussions and CI builds. Tan et al. [12]
studied the bug characteristics in pull requests of three large open-
source software projects. In general, they found three types of
bugs from the reports of code reviewers and contributors. WAN
et al. [13] also studied the bug characteristics in eight open source
blockchain systems hosted on GitHub. Lastly, Han et al. [6] focused
on understating code smells in code reviews by analysing 19,146
review comments.

The closest works to our study are [14, 15]. Although they
worked on CI failures after merging the pull requests, we focus on
a concrete understanding of types of bugs after merging the pull
requests, which is still a challenge.

3 DATA EXTRACTION AND COLLECTION
Figure 1 shows a complete overview of our study. The study uses
SmarkSHARK dataset which is hosted on MongoDB database [1].
This dataset contains valuable information of 77 GitHub projects
such as pull requests, code reviews, and etc. From this database, we
first queried the merged pull requests of these projects. As shown
in Figure 1, the query led to 16,779 merged pull requests.

We then ran three search queries which is described as follow:
Search query1. Date of code reviews of a merged pull request
must be more than the date of the merged pull request. This search
shows 2,998 pull requests that have one or more code reviews after
merging the pull requests.

Search query2. Date of code review comments of a merged pull
request must be more than date of the merged pull requests. This
results in 400 pull requests with one or more comments in code
review comments.

Search query3. Date of pull request comments must be more
than date of a merged pull request. Result of this search shows 343

Figure 2: An example of missed bug in pull request #8031 of
kafka project

pull requests that have one or more pull request comments after
merging the pull requests.

After applying these search queries, we removed the duplicate
pull requests. This resulted in 3,261 pull requests that may contain
missed bugs after merging the pull requests. Note that we called
these 3,261 pull requests as candidate pull requests.

4 EXPERIMENTAL RESULTS
4.1 Methodology
Here we describe our manual analysis in two steps:

Step1. After extracting 3,261 candidate pull requests, three ana-
lysts analyzed these pull requests. Out of these 3,261 candidate pull
requests, we assigned 1,087 candidate pull requests to each analyst
to review and find buggy pull requests. They were asked to label
each pull request that reports a bug and label those that do not
report any bugs. After reviewing 200 pull requests by each analyst,
they held a meeting to decrease their ambiguities and dissimilari-
ties. At the end of this analysis step, they attained 224 buggy pull
requests which each buggy pull request had at least one bug report.

SmartSHARK  dataset16,779  Merged pull requestssearch query 3. Date ofcode reviews > Date ofmerged pull requestsearch query 2. Date ofcode reviewcomments > Date of mergedpull requestsearch query 1. Date of pullrequest comments > Dateof merged pull request3,261 Candidate Pull requestsRemoving duplicatesSurface analysis224Buggy pull requestsAnalysisstep1Deep analysisAnalysisstep2187Missed bugTaxonomyand FindingResults3 analystsProtocol templatecrosscheckingMeetingsMeetingsSelecting merged pullrequestsDate of merged pull requestPull request commentCode reviewLink to new pull request to fix the missed bugWhich bugs are missed in code reviews: An empirical study on SmartSHARK dataset

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Figure 3: A taxonomy of missed bugs in code reviews

Step2. In this step, the same analysts were asked to deeply exam-
ine the buggy pull requests and find the type of bugs. Each analyst
reviewed these 224 buggy pull requests independently. They first
created a protocol template that is made of discovered bugs by
[7–10, 12, 13, 15] to find the best type for each bug reported in
the buggy pull requests. The protocol template includes the bug
categories and sub categories of seven manuscripts with their defi-
nitions. Second, they used this template to find and categorize the
bugs reports of buggy pull requests.

After manual analysing, they also had several meetings in this
step and they manually decided to mark some of the bugs reports of
buggy pull requests as unknown because of insufficient information.
51 out of 224 buggy pull requests led to unknown label after reaching
agreement among analysts. For the rest of buggy pull requests, we
used Fleiss kappa [5] to calculate the agreement between three
analysts. The calculation of kappa value between the analysts was
0.78 meaning substantial agreement. Then, three analysts argued
on their disagreements to reach consensus. The result was 187 bug
reports from 173 buggy pull requests. Figure 2 shows an example of
missed bug after merging pull request #8031 from kafka project. As
shown in this Figure, the missed bug is fixed in pull request #8051
as a hotfix pull request.

4.2 Results
We identified 187 missed bugs from 173 buggy pull requests and
then categorized them in a taxonomy of ten common bugs which
are shown in Figure 3. The missed bugs came from 28 open-source
GitHub projects. Among these 28 projects containing the missed
bugs, kafka, fineract, and calcite projects have the most missed bugs
with 94, 20, and 17, respectively. Regarding the taxonomy, semantic
bug has the most distribution (51.34%) and memory bug has the least
distribution (1.6%). Figure 3 also provided the number of missed
bugs in terms of category, subcategory, and subsubcategory. The
category schema of missed bugs presented in Table 1 with their
distributions. The following provides our categories of missed bugs
with their examples.

1) Semantic bug. apache/kafka #8683 reports the exception

handling bug which is fixed in #8990.

2) Build bug. apache/kafka #8312 corresponds to removing a
constructor caused to have broken the build. This fixed in #8866.
3) Analysis checks bug. apache/kylin #1349 reports that the
apache license missed from a new file, which leads to fail the rat
check. This missing license bug is fixed in #1356.

AnalysischecksAPI SecurityConcurrencyBuildCompatibilityConfigurationGUISemantic errorMemoryTaxonomy of missed  bugsIncomparable typeCode compilationWrong referenceLinking errorDependencyConnectionOut of memoryWrong API CompilationBuild hangMemory leakSuppressionbufferingIncorrectclassWrongresourceInconsistent codeVersioningAuthenticationSessionexpirationVulnerabilityDatabaseTimeoutIncorrectvalidatorMissingmechanism IncorrectopratorVersioningDeprecatedmethodsBinarycompatibilityIncompatible version errorCheck styleerrorInfinit loopEqual checktimeMissing nullcheckMissing licenseLicensingRebalancingerrorRaceconditionSynchronousmethodsNone typeerrorComparisonerrorExceptionhandlingNull pointerrorTypoOutputProcessingLogic errorUnusedvariableInconsistentmethodWrong metricWrongmethodInputInvalid variableWrong variableVariableMissing casesMissing codeMissing directoryWrong queryMissing methodMissing operatorWrong controlflow429412211521781410312114411141123281211738961235113723211111114187categorysubcategorysubsubcategoryLegendDocumentation1Incompletemethod1MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

F. Khoshnoud et al.

Table 1: The category schema of types of missed bugs after merging the pull requests

Categories
Semantic

Build

Analysis checks
Compatibility

Concurrency
Configuration

GUI

API
Security

Memory

Descriptions
This category of missed bugs caused by inconsistencies with the requirements or the programmers’ intention that do not belong to
the other categories. For example, output corresponds to bugs when it displays wrong results [12, 13].
This category of missed bugs caused by problems in the build, such as linking errors, connection errors, out of memory errors, or
compilation errors. The problems may break the build [13, 15].
This category of missed bugs caused by checks-related issues, licensing issues, or check style violations [15].
This category corresponds to bugs when a system cannot normally run on a particular CPU architecture, operating system, or Web
browser, etc. It also corresponds to the problems in the adaptation of the code (methods) to the new version of the library [8, 13].
This category of missed bugs caused by synchronization problems in the system [9, 12, 13].
This category of missed bugs caused by errors in dependent libraries, underlying operating systems, or non-code that affects
functionality, including time configurations [9, 13, 15].
This category of missed bugs caused by errors in graphical user interface, including incorrect class, incorrect information in
documentation, inconsistent HTML code in different versions, displaying incorrect images, links, and color [10, 13].
This category of missed bugs caused by problems arising from framework’s API usage such as wrong API usage [7, 8].
This category of missed bugs caused by vulnerabilities of a part of the system, or bugs that violate confidentiality, integrity, or
availability for the system [10, 12, 13].
This category of missed bugs caused by improper handling of memory objects or memory usage problems [9, 12, 13].

Distributions
51.34%

15.5%

9.09%
7.49%

4.28%
4.28%

2.14%

2.14%
2.14%

1.6%

4) Compatibility bug. apache/kafka #6188 reports the incom-

patible version error and then this is fixed in pull request #7101.

5) Concurrency bug. apache/kafka #6178 reports race condi-
tion bug and then #6185 fixes the bug with reverting that pull
request.

6) Configuration bug. apache/kafka #6419 reports timeout bug
which occur in the session timeout. #7072 is a pull request that
fixed it.

7) GUI bug. We put apache/kafka #7825 in GUI bugs that re-
ports using incorrect HTML class. The buggy pull request addressed
in pull request #7837.

8) API bug. apache/commons-io #124 had wrong API bug that
is discussed in #jira as a bug report. The developers of commons-io
project decided to fix it by updating javadocs and removing round
trips.

9) Security bug. apache/tika #219 uses jackson 2.9.4 which had
a vulnerability. The vulnerability details in CVE-2018-7489. Thus,
the developers of tika project opened TIKA-2634 issue in jira to fix
the problem.

10) Memory bug. apache/kafka #8955 reports memory leak
bug that cause the usage of memory was increased. This is fixed in
pull request #9245.

5 DISCUSSION
Various categories of bugs are missed from code reviews. Among
28 open-source GitHub projects, we found 187 bug reports after
merging the pull requests which are categorized in ten different
domain groups. Compared to Tan et al.’s work [12] and Wan et al.’s
work [13], their categories still exist in our categories of missed
bugs, except for performance and hard Fork. Semantic, build, and
analysis checks bugs are dominant missed bugs in code re-
views. As shown in Figure 3, semantic, build and analysis checks
bugs with the distribution of 51.34%, 15.5%, and 9.09% are the most
of reported bugs respectively that are missed after merging the
pull requests. Exception handling bugs with 36.46% out of semantic
bugs, build hang bugs with 58.62% out of build bugs, and missing
null check with 47.06% out of analysis checks bugs still significantly
miss from code reviews after merging the pull requests. Overall
our study recommends that code reviewers focus on reviewing the
categories of missed bugs provided in this research (see Figure 3)

using relevant unit tests before merging the pull requests, espe-
cially the dominant missed bugs. Further research could be done
to understand how is the reaction of code reviewers to the missed
bugs in terms of time reaction or etc.

6 THREATS TO VALIDITY
Construct Validity. Two-step analysis of our study might have
introduced concerns. In the first step, among 3,261 candidate pull
requests, each analyst checked 1,087 candidate pull requests. We ad-
mit that they have missed some buggy pull requests having missed
bugs. To mitigate this concern, they had a meeting after reviewing
200 candidate pull requests. In the second step, they categorized
the missed bugs into different types of bugs. To mitigate this, they
first used the protocol template (see section 4.1) and second they
had several meetings on their disagreements to reduce this threat.
External Validity. Generalizability in this work might be a
threat to external validity. SmartSHARK dataset only has projects
from the apache domain. Although this domain is one of the popular
domains, we acknowledge that our results may change if we use
different domains such as Microsoft1.

7 CONCLUSION AND FUTURE WORK
Since the lack of understanding of missed bugs by code review-
ers that may have hazardous consequences on other parts of a
system. Hence, in this work, we found pull requests of projects
from SmarkSHARK dataset which reports bugs after merging that
pull requests. To this end, we first extracted 3,261 candidate pull
requests which may report bugs in code reviews, code review com-
ments, and pull request comments after merging. After analysing
the candidate pull requests, we obtained 187 bug reports from 173
buggy pull requests and categorized them in ten types. At the end,
we obtained a taxonomy containing categories, subcategories, and
subsubcategories of missed bugs. The distribution of each category
is provided. The final result shows us that the most missed bug is
semantic (51.34%) and the least missed bug is memory (1.6%). In
future work, we plan to investigate different domains of projects
like Microsoft domain to enrich our findings. Also, we would like
to further analyze the missed bugs in different directions such as
reaction time of reviewers.

1https://github.com/microsoft

Which bugs are missed in code reviews: An empirical study on SmartSHARK dataset

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

REFERENCES
[1] [n. d.]. MongoDB Database. https://www.mongodb.com/
[2] A. Frank Ackerman, Lynne S. Buchwald, and Frank H. Lewski. 1989. Software

inspections: an effective verification process. IEEE software 6, 3 (1989), 31–36.

[3] A Frank Ackerman, Priscilla J Fowler, and Robert G Ebenau. 1984. Software
inspections and the industrial production of software. In Proc. of a symposium on
Software validation: inspection-testing-verification-alternatives. 13–40.

[4] Anonymous. [n. d.]. Which bugs are missed in code reviews: An empirical study on

SmartSHARK dataset. https://doi.org/10.5281/zenodo.5993800

[5] Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters.

Psychological bulletin 76, 5 (1971), 378.

[6] Xiaofeng Han, Amjed Tahir, Peng Liang, Steve Counsell, and Yajing Luo. 2021.
Understanding code smell detection via code review: A study of the openstack
community. In 2021 IEEE/ACM 29th International Conference on Program Compre-
hension (ICPC). IEEE, 323–334.

[7] Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, Andrea
Stocco, and Paolo Tonella. 2020. Taxonomy of real faults in deep learning sys-
tems. In Proceedings of the ACM/IEEE 42nd International Conference on Software
Engineering. 1110–1121.

[8] Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repair-
ing deep neural networks: Fix patterns and challenges. In 2020 IEEE/ACM 42nd
International Conference on Software Engineering (ICSE). IEEE, 1135–1146.

[9] Li Jia, Hao Zhong, Xiaoyin Wang, Linpeng Huang, and Xuansheng Lu. 2020.
An Empirical Study on Bugs Inside TensorFlow. In International Conference on
Database Systems for Advanced Applications. Springer, 604–620.

[10] Akond Rahman and Effat Farhana. 2020. An Exploratory Characterization of

Bugs in COVID-19 Software Projects. arXiv preprint arXiv:2006.00586 (2020).
[11] Mohammad Masudur Rahman, Chanchal K Roy, Jesse Redl, and Jason A Collins.
2016. Correct: Code reviewer recommendation at github for vendasta technolo-
gies. In Proceedings of the 31st IEEE/ACM International Conference on Automated
Software Engineering. 792–797.

[12] Lin Tan, Chen Liu, Zhenmin Li, Xuanhui Wang, Yuanyuan Zhou, and Chengxiang
Zhai. 2014. Bug characteristics in open source software. Empirical software
engineering 19, 6 (2014), 1665–1705.

[13] Zhiyuan Wan, David Lo, Xin Xia, and Liang Cai. 2017. Bug characteristics
in blockchain systems: a large-scale empirical study. In 2017 IEEE/ACM 14th
International Conference on Mining Software Repositories (MSR). IEEE, 413–424.

[14] Yue Yu, Huaimin Wang, Gang Yin, and Tao Wang. 2016. Reviewer recommenda-
tion for pull-requests in GitHub: What can we learn from code review and bug
assignment? Information and Software Technology 74 (2016), 204–218.

[15] Fiorella Zampetti, Gabriele Bavota, Gerardo Canfora, and Massimiliano Di Penta.
2019. A study on the interplay between pull request review and continuous
integration builds. In 2019 IEEE 26th International Conference on Software Analysis,
Evolution and Reengineering (SANER). IEEE, 38–48.

