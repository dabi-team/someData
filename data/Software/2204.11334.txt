2
2
0
2

r
p
A
4
2

IACR Transactions on Cryptographic Hardware and Embedded Systems
ISSN XXXX-XXXX, Vol. 0, No. 0, pp. 1–26.

DOI:XXXXXXXX

Hardware Acceleration for Third-Generation FHE
and PSI Based on It

Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

University of Michigan, Ann Arbor, USA, {zhehongw,dmcs,hunseok,blaauw}@umich.edu

]

R
C
.
s
c
[

Abstract. With the expansion of cloud services, serious concerns about the privacy
of users’ data arise due to the exposure of the unencrypted data to the server during
computation. Various security primitives are under investigation to preserve privacy
while evaluating private data, including Fully Homomorphic Encryption (FHE), Pri-
vate Set Intersection (PSI), and others. However, the prohibitive processing time
of these primitives hinders their practical applications. This work proposes and
implements an architecture for accelerating third-generation FHE with Amazon Web
Services (AWS) cloud FPGAs, marking the ﬁrst hardware acceleration solution for
third-generation FHE. We also introduce a novel unbalanced PSI protocol based on
third-generation FHE, optimized for the proposed hardware architecture. Several
algorithm-architecture co-optimization techniques are introduced to allow the com-
munication and computation costs to be independent of the Sender’s set size. The
measurement results show that the proposed accelerator achieves > 21× performance
improvement compared to a software implementation for various crucial subroutines
of third-generation FHE and the proposed PSI.
Keywords: FHE · PSI · FPGA · Acceleration · Hardware

1
v
4
3
3
1
1
.
4
0
2
2
The past decades have witnessed the increasingly wide deployment of data centers. With
:
v
their excellent storage capacity and computational performance, they have assumed a
i
fundamental role in almost every aspect of human life. Nowadays, it is common to outsource
X
data or computation extensive workloads to large data centers. However, for servers to
r
work on the data from a client, the encrypted client data must be decrypted ﬁrst, which
a
reveals the private data to a potentially untrusted cloud server. Thus, as a growing number
of services are moving online, especially after the COVID-19 pandemic, privacy-preserving
computation, which allows secure operation on encrypted client data, is expected to play a
pivotal part of this industry in the future. In this work, two privacy-preserving schemes
and their hardware acceleration are discussed.

Introduction

1

1.1 Fully Homomorphic Encryption

Fully Homomorphic Encryption (FHE) [Gen09a, Gen09b, GH11, BGV12, Bra12, FV12,
CKKS17, SV14, GSW13, DM15, CGGI20], which permits secure computation on encrypted
data without decrypting it, has been given extensive research in recent years to enable
privacy-preserving computation. To be precise, for a given function f (x), a homomorphic
encryption scheme satisﬁes f (Enc(m)) = Enc(f (m)). If it is homomorphic to any function,
it is characterized as fully homomorphic encryption.

The very ﬁrst FHE scheme was not devised until 2009, when Gentry proposed a general
FHE framework [Gen09a, Gen09b]. It has been proven that from a Boolean circuit model
perspective of computation, if an encryption scheme is homomorphic to its own decryption

Licensed under Creative Commons License CC-BY 4.0.

 
 
 
 
 
 
2

Hardware Acceleration for Third-Generation FHE and PSI Based on It

function followed by a universal logic gate, then it is homomorphic to any function. The
operation that fulﬁlls this property by refreshing the noise level of the ciphertext after each
operation, thus transforming a Leveled HE (LHE) into an FHE, is called bootstrapping or
recryption. Based on this idea, Gentry also presented a concrete construction that takes
around 30 minutes per bootstrapping [GH11].

Following Gentry’s blueprint, various schemes have been proposed for better eﬃciency.
Among them, the most well-known are BGV [BGV12], BFV [Bra12, FV12], and CKKS
[CKKS17]. These second-generation schemes diﬀer from Gentry’s approach in relying on
the Ring Learning with Errors (RLWE) problem for its better-studied hardness analysis
and eﬃciency obtained via SIMD-styled computation [SV14]. These schemes focus on
evaluating polynomials homomorphically. Several open-source implementations [PRRC21,
HS14, SEA21] can potentially reduce the recryption time to minutes depending on the
security parameters.

After the proposal of GSW [GSW13] in 2013, FHEW [DM15] and TFHE [CGGI20] were
published as third-generation approaches. The third-generation schemes focus on eﬃcient
homomorphic boolean logic evaluation. Although performance-wise the third generation
may not be superior to earlier schemes (since the amortized cost of the SIMD-styled second
generation is estimated to be within the same order of magnitude as that of the third
generation), the third generation is well accepted for its simplicity and ﬂexibility in terms
of both concept and implementation. Typically, to achieve a 128-bit security level, the
degree of the polynomial is less than 2048, which is much shorter than the ~10,000 in the
second generation. Further, the ciphertext modulus is less than 64 bits, compared to ~200
bits in the second generation. The reported recryption time of the third generation is
generally around 0.1s to 1s [MP21].

Although several improvements have been presented to increase the eﬃciency of FHE,
secure computation on encrypted data is still many orders of magnitudes slower com-
pared to direct computation on plaintext, due to the prohibitive requirement of compute
power that challenges current computing systems, which hinders practical application
of FHE. For example, the time of bootstrapping one homomorphic NAND gate is sub-
seconds for third-generation schemes [DM15, CGGI20], compared to picoseconds of a
CMOS NAND.This still renders FHE largely impractical. Thus, various hardware solu-
tions have been proposed in recent years [STCZ18, YCAR19, BPC19, RLPD20, SRTJ+19,
SRJV+18, MRL+18, MSR+17, JGCM+15, DzS15, TRV20].
[STCZ18, BPC19] focused
on encryption/decryption of RLWE in a post-quantum scenario, which is less computation
demanding due to the smaller size of the polynomial. Reference [YCAR19] presented a
crypto-engine for the encryption/decryption of RLWE for homomorphic encryption, which
is less heavy lifting compared to homomorphic evaluation. The authors in [DzS15] explored
acceleration for large number multiplication, while [MSR+17, JGCM+15] discussed ap-
proaches to accelerate long polynomial multiplications in homomorphic encryption. Other
works [SRTJ+19, SRJV+18, MRL+18, TRV20] implemented accelerators for LHE schemes
based on the BFV scheme, with limited computation depth and security levels. Finally, an
architecture for the CKKS scheme was proposed in [RLPD20]. To date, there has been no
hardware acceleration published for third-generation FHE schemes.

1.2 Private Set Intersection

PSI is another security primitive that preserves privacy of operation. It allows two parties
(Sender and Receiver) to exchange the intersection of their private sets without leaking any
excess information other than the intersection set. Thus, its applications include private
human genome testing, contact list discovery of social media apps, and conversion rate
measuring of online advertisement. Recently, Microsoft introduced Password Monitor in
the latest release of the Edge web browser, which compares a user’s private passwords
saved to Edge with a known database of leaked passwords to ﬁgure out whether there is a

Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

3

leak in the user’s passwords. With the underlying PSI protocol, the server that facilitates
the comparison learns nothing about the user’s passwords.

The PSI problem has been explored extensively, seeking eﬃcient protocols [Mea86,
HFH99, PSZ14, PSZ18, OOS17]. However, in an unbalanced scenario where one set is
signiﬁcantly smaller than the other, these protocols perform linearly on the size of the
large set. In recent years, unbalanced PSI protocols based on second-generation FHE
[CLR17, CHLR18] were proposed that provide signiﬁcant communication overhead reduc-
tion compared to previous approaches but maintain comparable performance. However,
they still suﬀer from the large encryption parameters of second-generation FHE. While
third-generation FHE can perform boolean logic more eﬃciently, it is a natural candidate
for performing the comparison in PSI. However, this has not been explored before.

1.3 Our Contributions

This paper presents the following contributions:

• We present the ﬁrst accelerator architecture for third-generation FHE, targeting the
RLW E N RGSW operation (deﬁned in the following section), which is a fundamen-
tal function of both second-generation and third-generation FHE. By exploiting the
asymmetric nature of the operation, the architecture is capable of maintaining high
throughput with less resource usage while addressing diﬀerent parameter sets. An
extensive analysis of the architecture is included.

• We propose a novel unbalanced PSI protocol that is based on third-generation FHE
and is demonstrated with the proposed hardware. The proposed PSI protocol makes
the computation cost independent of the Sender’s set size. The core block of the
PSI that facilitates the cross comparison of the PSI in [CLR17] is replaced with a
homomorphic lookup table (LUT) implemented with third-generation FHE. Unlike
the multiplication used in [CLR17], which returns a nonzero value when the cross
comparison misses and potentially leaks the content of the Sender’s set, the LUT
only returns one bit indicating whether an element is inside the Sender’s set; and
thus, avoiding sending any excess information about the Sender’s set. Therefore, the
noise ﬂooding process adopted in [CLR17] is not necessary. We introduce several
additional algorithm-architecture co-optimizations to reduce the computation and
communication costs, rendering a practical application of the proposed PSI protocol.

• A prototype of the proposed architecture is implemented with AWS cloud FPGA
service. We develop all necessary high-level functions in C++ and benchmark the
implemented architecture with diﬀerent parameter sets. We make the SystemVerilog
HDL code of the proposed accelerator and supporting software code publicly available
at [HWF21].

• We quantify and analyze the performance of the proposed hardware acceleator and
PSI protocol. The measurements show over 21× performance improvement compared
to a software implementation for various subroutines of the third-generation FHE
and the proposed PSI.

2 Preliminaries

2.1 Notation

Throughout the paper, boldface lower-case letters a, b, c, . . . are used to denote vectors or
polynomials depending on the context, and boldface upper-case letters A, B, C, . . . are
used for matrices. The set of integers is denoted by Z, and the quotient ring of integers

4

Hardware Acceleration for Third-Generation FHE and PSI Based on It

modulo q is denoted by Zq. The polynomial ring is denoted by R = Z[X]/(X N + 1),
where N is a power of two. And RQ = R/QR represents the residue ring of R modulo
an integer Q. “×” denotes the scalar multiplication with either another scalar or a
vector/polynomial. “·” denotes the vector inner product or polynomial product depending
on the context, while “J” denotes the outer product or element wise product of a
polynomial. Lastly, “RLW E N RGSW ” represents the product of an RLWE ciphertext
and an RGSW ciphertext, which will be detailed in the next section.

2.2 Lattice-based Cryptography: LWE, RLWE and RGSW

Almost all FHE schemes published so far are built upon the LWE and/or RLWE problem,
which can be reduced to a lattice problem that is proven to be quantum safe within
polynomial time [Reg09, LPR10].

2.2.1 Learning with Errors Encryption

In practice, given a plaintext modulus t and a ciphertext modulus q, an LWE encryption
of a plaintext m mod t with secret vector s is deﬁned as:

LW Eq/t

s

(m) = [a, b = a · s + e + m × q/t mod q]

(1)

with the vector a, of dimension n, sampled uniformly from the integer vector space Zn
and
q
error e sampled from an error distribution χ [Reg09]. As long as |e| < q/(2t), the plaintext
can be successfully recovered by m = b(b − a · s) × t/qe, which rounds oﬀ the noise.

2.2.2 Ring Learning with Errors Encryption

However, additive homomorphism is not enough to construct the bootstrap function. RLWE
that is potentially multiplicative homomorphic is also incorporated in third-generation FHE.
Similar to the deﬁnition of LWE, given a plaintext modulus T and a ciphertext modulus
Q, an RLWE encryption of a plaintext polynomial m mod T with secret polynomial s is
deﬁned as follows:

RLW EQ/T

z

(m) = [a, b = a · z + e + m × Q/T mod Q]

(2)

with the polynomial a sampled uniformly from the ring RQ, and e, a noise polynomial,
sampled from an error distribution χ [LPR10]. As long as |e|∞ < Q/(2T ), the plaintext
can be successfully recovered by m = b(b − a · z) × T /Qe, which rounds oﬀ the noise. In
some contexts, the scale Q/T is omitted for clarity.

Since RLWE is a special form of LWE, the coeﬃcients of the polynomial b of an RLWE
ciphertext RLW Ez(m) = [a, b] can be converted into multiple separate LWE ciphertexts
under the same secret key with some transformation of polynomial a, which is detailed in
Appendix A.

2.2.3 NTT and INTT

The polynomial multiplication of RLWE can be eﬃciently computed with NTT. NTT is an
adaptation of the well-known FFT algorithm, which reduces the complexity of polynomial
multiplication from O(N 2) to O(N log(N )). However, to perform polynomial multiplication
modulo (X N + 1), negacyclic/anticyclic convolution is adopted [LN16]. The optimized
NTT/INTT algorithms summarized in [LN16] are adopted and implemented on hardware
in this work. The algorithms are given in Appendix E and D.

Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

5

Figure 1: Data Flow of Homomorphic Accumulation in FHEW.

2.2.4 Ring GSW Encryption

Lastly, Ring-GSW (RGSW) encryption is widely adopted in third-generation FHE to
facilitate homomorphic polynomial multiplication [DM15, CGGI20].
It is deﬁned as
a matrix of RLWEs (in some literature, the two columns are concatenated as a one-
dimensional vector):

0
RGSWz(m) = [RLW E
z

(−z · m), RLW E

0
z

(m)],

(3)

0
with RLW E
z

(m) deﬁned as a vector of RLWEs:

0
RLW E
z

(m) = [RLW Ez(m), RLW Ez(BG × m), · · · , RLW Ez(Bdc−1

G × m)],

(4)

(m).

where BG is a predeﬁned decomposition base and dc = logBG Q denotes the length of
0
vector RLW E
z

The multiplication of an RLW E(m1) = [a, b] and an RGSW (m2) is deﬁned in
Equation 5, with the two polynomials of the RLWE being decomposed by the base BG into
0 [i].
two vectors of polynomials, a
Further, the product of a polynomial p and an RLWE ciphertext RLW Ez(m) = [a, b]
is deﬁned as p · RLW Ez(m) = [p · a, p · b]. The N operator is used extensively in the
bootstrap process, and is the main focus of our hardware implementation.

0 , that satisfy a = Pdc−1

0 [i] and b = Pdc−1

0 and b

i=0 a

i=0 b

RLW E(m1) O

RGSW (m2) = a

0

· RLW E

0(−z · m2) + b

0

· RLW E

0(m2)

= RLW E(m1 · m2 + e1 · m2)

(5)

2.3 Bootstrap in Third-Generation FHE
As shown in Section 2.2, LWE is additive homomorphic, meaning that LW E(m1) +
LW E(m2) = LW E(m1 + m2), so it is used to homomorphically evaluate Boolean logic in
third-generation FHE. Take N AN D(m1, m2) for example, with m1, m2 being either 0 or 1.
The result of the NAND can be extracted from the sum m1 + m2 + 2 mod 4. If the sum is

RGSW=logBG(Q)x2 RLWEPoly mult RLWERLWE additionRLWE=2 polydc = logBG(Q)nqaLbLLWE cipherInitialize RLWE cipher(Accumulator)aRbRiNTTaRiNTTbRiNTTiNTTdecompdecompiNTTpolypolydcNTT x dcNTT x dcNTTpolydcPoly MACPoly MACBT keyRGSWLoop: dcRLWELoop: n dcRLWEpolypolyiNTTaRiNTTbR6

Hardware Acceleration for Third-Generation FHE and PSI Based on It

2 or 3, then N AN D(m1, m2) = 0. Otherwise, if the sum is 0, then N AN D(m1, m2) = 1.
Thus, the NAND is encoded in the MSB of the sum. However, further addition cannot
be applied to the resulting LWE ciphertext due to both the mismatch of the data format
(LSB vs. MSB) and the increased noise, which can potentially contaminate the message.
Therefore, the bootstrap process introduced in Section 1.1 is required to reset the data
format and noise level.

The bootstrap process of FHEW [DM15] is implemented in this work for its integer
operation that better serves the purpose of hardware acceleration. It is composed of three
steps, homomorphic accumulation, RLWE to LWE key switch, and LWE modulus switch.
The homomorphic accumulation takes 98% of the processing time [MP21], therefore, this
subroutine is deployed on the hardware, while others are done in software and will not be
discussed here. The reader is referred to [MP21] for further details.

Figure 1 illustrates the data ﬂow of homomorphic accumulation, with the N operation
highlighted in the dotted red box. At ﬁrst, the bootstrap key (BT key, an array of RGSW
ciphertexts) is generated by the local user and transferred to the server. This is a one-time
process. For the server to bootstrap one LWE ciphertext, a homomorphic accumulator is
initialized based on b of the LWE, in INTT domain. Then, the accumulator is multiplied
with a element of the BT key by the N operation. The element of the BT key is indexed
by a[i] of the LWE and i ∈ [0, n − 1]. The product is accumulated and looped back for next
multiplication. After the loop ﬁnishes, the output is passed through RLWE to LWE key
switch function and LWE modulus switch function (not shown in Figure 1) to complete
the whole bootstrap process.

2.4 Augmented Subroutines

To build the proposed PSI protocol, we adopt some additional features from another
third-generation scheme TFHE [CGGI20].

2.4.1 CM U X(RGSWz(m), RLW Ez(p0), RLW Ez(p1))

The N operation between an RLWE and a RGSW, deﬁned in Section 2.2.4, can be
used to construct a homomorphic MUX gate [CGGI20]. Let m = [sel, 0, 0, · · · , 0] be the
selection signal of the MUX gate with sel equal to either 0 or 1, and let RLW Ez(p0)
and RLW Ez(p1) be two input RLWE ciphertexts for the MUX. The CMUX function is
deﬁned in Equation 6, which output an RLWE ciphertext corresponding to the encrypted
selection signal.

CM U X(RGSWz(m), RLW Ez(p0), RLW Ez(p1))
=RGSWz(m) O(RLW Ez(p1) − RLW Ez(p0)) + RLW Ez(p0)
=RLW Ez(m · (p1 − p0) + p0)
=RLW Ez(psel)

(6)

2.4.2 BlindRotate(RGSWz(m), RLW Ez(p), j))

Following the deﬁnition of the CM U X, BlindRotate is formulated to homomorphically
rotate an encrypted polynomial by multiplying the polynomial with a power of X. A
simpliﬁed version is shown in Equation 7. Let m = [sel, 0, 0, · · · , 0] be the selection signal
of the CMUX gate, and let RLW Ez(p) be the input RLWE ciphertext. Parameter j
denotes the number of steps for the rotation. Thus, the output RLWE encrypts a plaintext
that is either rotated or not based on the selection. A comprehensive deﬁnition can be

Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

7

Figure 2: (a) LUT and CMUX Tree for an Arbitrary Binary Function f (x), and (b)
Vertical Packing Scheme of the LUT.

found in [CGGI20].

BlindRotate(RGSWz(m), RLW Ez(p), j)

=CM U X(RGSWz(m), RLW Ez(p), X −j · RLW Ez(p))
=CM U X(RGSWz(m), RLW Ez(p), RLW Ez(X −j · p))
=RLW Ez(m · (X −j · p − p) + p)

(7)

2.4.3 Homomorphic LUT and Plaintext Packing

Intuitively, the CMUX gate can be concatenated into a CMUX tree to evaluate an arbitrary
binary function homomorphically as shown in Figure 2 (a) [CGGI20]. The function is
precomputed and encrypted into an LUT of RLWE ciphertexts, and after traversing the
CMUX tree indexed by RGSW encryptions of the binary representation of an input x, an
RLWE ciphertext that encrypts the corresponding f (x) is output.

However, the size of the LUT is large if each RLWE ciphertext only encrypts one
function value, resulting in 2k RLWE ciphertexts. Also, the amount of CMUX is 2k − 1.
This exponential size can be reduced by a factor of the length of the polynomial in an
RLWE ciphertext if several function values are packed into an RLWE ciphertext. For
example, if each coeﬃcient of a plaintext polynomial m is taken as a plaintext slot,
then a contiguous block of function values can be packed into one polynomial, such as
m = [f (0), f (1), · · · , f (N − 1)], where N is the length of the polynomial. Thus, an RLWE
ciphertext can encrypt at most N function values, which reduces the size of the LUT and
the amount of CMUX by a factor of N . Figure 2 (b) details this packing scheme using
an example in which each RLWE encrypts two function values, reducing the number of
CMUXs by a factor of 2. In the example, the MSBs of the input x are ﬁrst used to ﬁnd
the desired RLWE ciphertext, and then the target slot is rotated to the position 0 dictated
by the LSB of x with BlindRotate. Lastly, the desired slot is extracted from the RLWE
into an LWE ciphertext as described in Section 2.2.2.

010101010101RGSW(x[0])RGSW(x[1])Encrypted LUT of f(x)RLWE(f(x))01RGSW(x[k-1])CMUXRLWE(f(0))RLWE(f(1))RLWE(f(2))RLWE(f(3))RLWE(f(2k-2))RLWE(f(2k-1))f(0)f(1)f(2)f(3)f(2k-2)f(2k-1)010101RGSW(x[1])RGSW (x[k-1])LUT of f(x)CMUXLWE(f(x))RLWE RLWERLWE BlindRotate&Extract LWERGSW(x[0])Packed LUT(a)(b)8

Hardware Acceleration for Third-Generation FHE and PSI Based on It

Figure 3: (a) General Concept of Finding the Intersection of Two Sets, and (b) Homomor-
phic LUT Based PSI.

2.4.4 RLWE Key Switch

The last included subroutine is the RLWE key switch that converts an RLWE ciphertext
encrypted under a secret key z1 into another RLWE ciphertext encrypted by a diﬀerent
secret key z2. Given a decomposition base BKS, an RLWE key-switch key (KS key) is
created by encrypting the secret key z1 into a vector of RLWE ciphertexts, as shown in
Equation 8, with dc = log

BKS Q denoting the length of the vector.

[RLW Ez2

(1 × z1), RLW Ez2

(BKS × z1), · · · , RLW Ez2

(Bdc−1

KS × z1)]

(8)

For an RLWE ciphertext RLW Ez1

)(m) = [a, b] encrypted with key z1, to switch to
key z2, the new ciphertext is calculated by Equation 9, which is basically an inner product
of the decomposed a and the key-switch key, with a = Pdc−1
j=0 a[j]. The multiplication of
a polynomial and an RLWE ciphertext is deﬁned in Section 2.2.4. A formal deﬁnition of
the process can be found in [BGV12, Bra12, FV12]. Note that this operation resembles
the N operation.

RLW Ez2

(m) = [0, b] −

dc−1
X

j=0

a[j] · RLW Ez2

(Bj

KS × z1)

(9)

3 Unbalanced PSI with (Leveled) Augmented FHEW

3.1 High Level Construction

For two parties, the Receiver and the Sender, to ﬁnd the intersection of their private sets
{xi} and {yj} w.l.o.g. assuming each contains some 32-bit integers, as show in Figure 3
(a), each element of the Receiver’s set is compared with the elements of the Sender’s set.
In the case of a match, the element is added to the intersection.

However, in an unencrypted scenario, one of the parties needs to reveal all its content
to the other party, which is undesirable. So, in [CLR17], the comparison is fulﬁlled by
a homomorphic product of the diﬀerence between elements in the two sets. For each

x0x1x2xny0y1y2ymReceiverSender 32bit32bitComparisonx0x1x2xny0y1y2ymReceiverSender 32bit32bit1bit0110LUT23210101010101010101x0[0]x0[1]x0[31]LSBMSBPSI output(a)(b)Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

9

Figure 4: Data Flow of the RLWE Substitution Subroutine (RLWE key Switch Included).

RLWE encrypted xi, the Sender evaluates homomorphically the product of the diﬀerence
yj − xi∀j, as shown in Equation 10. After the Receiver decrypts the result, the product
evaluates to 0 if xi ﬁnds a match in the Sender’s set {yj}.

Y

(cid:0)[0, yj] − RLW Ez(xi)(cid:1) = RLW Ez(Y

(yj − xi))

(10)

j

j

In this work, the comparison is facilitated with the homomorphic LUT described in
Section 2.4. As shown in Figure 3 (b), on the Sender’s side, an LUT is precomputed based
on the content of the Sender’s set {yj}, with LU T [yj] = 1; otherwise, the entry is set to 0.
On the Receiver’s side, each element xi is decomposed into its binary representation and
encrypted with a vector of RGSW ciphertext, [RGSWz(xi[0]), RGSWz(xi[1]), · · · ,
RGSWz(xi[31])] and sent to the Sender. Then, the RGSW encrypted xi is passed into
the CMUX tree to index the LUT on the Sender’s side, and the result is sent back to
the Receiver. After decryption, 1 indicates that xi is in the intersection, otherwise, it
is not. Since the proposed protocol follows the high-level construction of [CLR17], the
attack model, security implications, and proof in [CLR17] also apply to this work with the
exception that the noise ﬂooding process is unnecessary because the LUT only returns
whether xi is in set {yi} and therefore no excess information about set {yj} is leaked.

Let b denote the bit width of the elements inside both sets. The communication
complexity is linearly dependent on b and the size of the Receiver’s set, resulting in
O(b × |{xi}|). The computation cost is O(2b), which is independent of the size of the
Sender’s set. So, this naïve construction is very ineﬃcient in both computation and
communication traﬃc. For example, if b = 32, 32 RGSW ciphertexts have to be transferred
for each element in the Receiver’s set, resulting in a low ciphertext utilization. Additionally,
232 − 1 CMUXs are evaluated for each element in the Receiver’s set. Several optimizations
can be adopted to mitigate these problems and render practical application of the protocol.

3.2 RLWE Substitution and RLWE Expansion

Before tackling the problems, two additional subroutines need to be discussed. The ﬁrst
is RLWE substitution, which transforms an RLWE ciphertext RLW Ez(P m[i]X i) into
RLW Ez(P m[i](X i)k) for an odd integer k. An RLWE key-switch key from z(X k) to z
is precomputed based on the substituted secret key z(X k) = z[i](X i)k. In the process, an

aRbRRLWE inputpolyiNTTaRiNTTbRiNTTsubsiNTTaRiNTTbRiNTTaRdecompiNTTpolydcNTT x dcNTTpolydcPoly MACiNTTbRNTT KS keydcRLWE outputPoly mult RLWERLWE additionRLWE=2 polydc = logBKS(Q)RLWELoop: dcKey switch10

Hardware Acceleration for Third-Generation FHE and PSI Based on It

(cid:0)m(X k)(cid:1) = [a(X k), b(X k)], and
RLWE ciphertext is ﬁrst substituted to get RLW Ez(X k)
then key-switched to RLW Ez(m(X k)) encrypted with the original secret key. A formal
deﬁnition can be found in [CCR19].

The RLWE substitution is used extensively in the RLWE expansion operation [CCR19],
which expands an RLWE ciphertext from RLW Ez(P m[i]X i) into a vector [RLW Ez(m[0]),
RLW Ez(m[1]), · · · , RLW Ez(m[N − 1])]. An example of how RLWE substitution fulﬁlls
the expansion is detailed in Appendix B.

The data ﬂow of RLWE substitution is shown in Figure 4, with the key switch highlighted
in the dotted red box. An RLWE ciphertext in the NTT domain is ﬁrst transformed into
INTT form and substituted. Then it is decomposed with base BKS and key-switched
to the original secret key to get a substituted RLWE ciphertext. After that, the output
ciphertext is postprocessed for RLWE expansion. Based on our experiment, 97% of the
processing time of RLWE expansion is dedicated to substitution and key switch functions,
so these two functions are oﬄoaded to a FPGA. Note that the key switch data ﬂow is very
similar to the bootstrap data ﬂow. Therefore, the proposed architecture merges both data
ﬂows, which will be detailed in Section 4.

3.3 Optimizations for The PSI Protocol

For the proposed PSI, both the computation and communication costs depend directly
on the bit width of the elements in the set. Hence, the ﬁrst optimization is to reduce the
bit width of the element with permutation-based hashing [SAN10]. In permutation-based
hashing, to insert a 32-bit element xi from the Receiver’s set into 2k bins, it is divided
into xiH ||xiL, with xiL consisting of k bits. The position of the element is calculated
by Equation 11, where H(x) is a hash function. Therefore, the position of an element
also stores some information about the element. And instead of inserting xi into the
hash table, only xiH is inserted, which reduces the bit width to 32 − k.The correctness of
the comparison in the homomorphic LUT holds with permutation-based hashing, which
is detailed in Appendix C. With permutation-based hashing, the amount of transferred
RGSWs is reduced by k and the amount of CMUXes is reduced by a factor of 2k.

pos(xi) = H(xiH ) XOR xiL

(11)

The second optimization achieves further computational reduction by exploiting the
vertical packing described in Section 2.4. With vertical packing, at most N LUT elements
can be packed into one RLWE ciphertext, which shrinks the amount of CMUXs by roughly
a factor of N . For example, after the permutation hashing with k = 14, the bit width
of the elements in each bin is 18, reducing the size of the CMUX tree to 218 − 1. At
N = 2048, the vertical packing reduces it to 27 − 1 + 11. Compared to the original size,
232 − 1, a reduction by 225 times is achieved in total.

The last optimization aims at decreasing the communication payload.

Instead of
transferring an RGSW ciphertext, containing 2 log
(Q) RLWE ciphertexts, for each
bit of an element in the Receiver’s set, it is observed that the ﬁrst column of an RGSW
ciphertext, as shown in Equation 3 and Section Equation 4, can be calculated from the
second column, which is detailed in Equation 12. Thus, only the second column needs to
be transferred together with a shared RGSW encryption of the secret key −z [CCR19].
The transaction size is therefore reduced by a factor of 2.

BG

RLW Ez(Bj

G × (−z · m)) = RLW Ez(Bj

G × m) O

RGSWz(−z)

(12)

However, the ciphertext utilization is still very low because for each element in
the Receiver’s set,
BG Q RLWEs are transferred. So, N elements, for example,
[x0, x1, · · · , xN −1], from the Receiver’s set are packed into a 2-D array of RLWE ciphertexts

log

Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

11

Figure 5: Data Flow of the Homomorphic LUT Based PSI.

i xi[k] × Bj

for better utilization. Each element of the array is formed as RLW Ez(P
G × X i)
(Q) − 1], k ∈ [0, 17] (assuming after applying permutation-
and is indexed by j ∈ [0, logBG
based hashing, the bit width is 18). Upon receiving the array, the Sender unpacks it, with
the RLWE expansion described in Section 3.2, into arrays of RLWEs for each element xi,
of the form RLW Ez(xi[k] × Bj
). Finally, the RLWEs are converted into RGSWs with
G
Equation 12 and passed into the LUT to complete the PSI. Note that we set the BG of
the RGSW to be equal to the BKS of the key-switch key used in the RLWE expansion.
Together, compared to transferring complete RGSWs, the communication overhead
(Q) RLWEs per N
(Q) = 6, N = 2048, at the cost of

is reduced from 18 RGSWs (216 RLWEs) per element to 18 × log
elements, amounting to a 4096× reduction if log
increased computation on the Sender’s side to unpack and reconstruct the RGSWs.

BG

BG

Figure 5 shows the data ﬂow of the proposed homomorphic LUT-based PSI. It assumes
that after the permutation-based hashing, the data bit width is 18 bits and the polynomial
length is N = 2048. The Receiver packs all the necessary bits into an array of RLWE
ciphertexts and sends it to the Sender. The Sender then unpacks and the array of RLWEs
into an array of RGSWs in which each column encrypts the binary representation of an
element in the Receiver’s set. Then, each column of RGSW is passed through the LUT,
and an RLWE that encrypts the lookup result at index 0 is generated. Finally, the LWEs
that encrypt lookup results are extracted from the RLWEs, as described in Section 2.2,
and sent back to the Receiver. The hashing process is not shown in the ﬁgure. Other
optimizations utilized in [CLR17] can also be applied to our protocol, such as pre-hashing
both parties’ sets into smaller sets to reduce the set sizes, using modulus switching to
reduce reply ciphertext size, etc.

In summary, after applying the above optimizations, the communication overhead of
the scheme is O( b−k
2N × 2k), assuming, on the Receiver’s side, 2k bins after hashing and
at most one element in each bin, with dummy elements ﬁlling up the empty bins. The
computation cost is O( 2b
N

+ 2k(b − k)).

4 Architecture of The Proposed Accelerator

4.1 Overall Architecture

Figure 6 (a) shows the overall architecture of the proposed accelerator with a zoom-in
view of the compute pipeline in Figure 6 (b). Implemented with AWS F1 instance, the
accelerator is controlled and monitored by the host software running on an x86 processor

Pack & encUnpackCMUX treeLUT18ReconstructFor each Col1RLWEPost process &Send backReceiverSender 204818LogBG(Q)1RLWE204818LogBG(Q)1RLWE1LogBG(Q)1RLWE1LogBG(Q)1RLWE2048181bitLogBG(Q)LogBG(Q)2048181bitLogBG(Q)204811RGSW18204811RGSW18111RGSW18111RGSW1812

Hardware Acceleration for Third-Generation FHE and PSI Based on It

Figure 6: (a) Overall Architecture of the Proposed Accelerator, and (b) Overview of the
Main Compute Chain.

through various AXI interfaces. The conﬁgure parameters and instructions are programed
with the AXI-Lite interface, and the FIFO states are also read from it. The DMA module
communicates with the FPGA through the AXI bus to program the FPGA DDR and
read/write the RLWE FIFOs. The RLWEs streamed in and out of the FPGA are in the
NTT domain. Further, the modulo multiplication in the accelerator is facilitated by the
standard Barrett Reduction [Bar87].

The architecture works in a pipelined fashion, with necessary inter-stage double buﬀering.
Upon an input instruction, the key load module reads the corresponding key from the
preprogramed FPGA DRAM into its own key load FIFO. In parallel, the INTT/NTT
modules inside the compute pipeline manipulate the input RLWEs, hiding the DDR access
delay of the key load module since the keys are only needed at the poly MAC stage, which
facilitates the polynomial and RLWE vector inner product introduced in Section 2.2.4 and
Section 3.2. Once the computation ﬁnishes, the output RLWEs are written back to the
RLWE FIFO dictated by the mode of the accelerator, which will be detailed later, and
then streamed out to the host.

As mentioned in Section 3.2, the accelerator merges the two data ﬂows, the RLWE
substitution and the bootstrap process. Note that the data ﬂow of evaluating the homo-
morphic LUT introduced in Section 2.4.3 is mostly the same as the bootstrap ﬂow since
they both incorporate the N operation, so they will not be diﬀerentiated in the remaining
text. There are three primary diﬀerences between the two data ﬂows. The ﬁrst is the
RLWE key switch versus the RLW E N RGSW operation as highlighted in Figure 4 and
Figure 1, respectively. Second, in RLWE substitution, after INTT, the subroutine that
transforms the RLW Ez(m) into RLW Ez(X k)(m(X k)), as stated in Section 3.2, is needed;
this subroutine is unnecessary in the bootstrap process. Lastly, an RLWE ciphertext,
streamed into the in/out FIFO, only passes through the compute pipeline once for RLWE
substitution and is then streamed out from the output FIFO after the computation. In
contrast, in the bootstrap process, after initialization, the same RLWE (homomorphic
accumulator) must be looped n times through the compute pipeline before being streamed
out, meaning that the output RLWE from the compute pipeline should go to the same
FIFO as the input RLWE.

The ﬁrst two diﬀerences regarding the computation are automatically taken care of
by the diﬀerent instructions passed into the compute pipeline. For the third one, a mode
conﬁguration is added to the FIFOs to diﬀerentiate the situations, as shown by the dotted
lines in Figure 6 (a). In RLW E mode, the in/out FIFO acts only as an input FIFO that
receives the input RLWEs, whereas the output FIFO holds the processed RLWEs. While

Compute PipelineIn/Out RLWE FIFOOutput RLWE FIFOConfig AXILDMAKey Load FIFOSHFPGA DDR IFAXI xbarBT Key/KS keyInstFPGABootstrap ModeRLWE ModeInputRLWEOutput RLWEROBROBROBROBINTTPoly SubsNTTPoly MULTPoly ACCInstCompute PipelineBT Key/KS keyPoly MAC(a)(b)Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

13

Figure 7: (a) Architecture and Dataﬂow of the INTT Module, (b) the Time-Interleaving
of the Polynomial Buﬀers, and (c) Two Data Access Pattern of The INTT Module.

in the bootstrap mode, the output FIFO is turned oﬀ and the in/out FIFO holds the
intermediate RLWEs. The compute pipeline continuously reads and writes the in/out
FIFO until the loop ﬁnishes. Then the RLWEs in the FIFO are streamed out to the host.

4.2 INTT Module

Figure 7 (a) details the structure of the INTT module, which follows the algorithm in
Appendix D, except for the ﬁrst outer loop where the input RLWE is read from the global
in/out RLWE FIFO, and the intermediate result is written to its own two polynomial
buﬀers since each RLWE contains two polynomials. Starting from the second outer loop,
the input is read from the polynomial buﬀers and written back after being processed by
the butterﬂies. Each INTT module stores its own copy of the twiddle factors (TFs) in its
local memory.

Two parallel butterﬂy units are included in each INTT module to achieve better
performance while maintaining a reasonable FPGA resource usage. Thus, to feed enough
data, each address of the polynomial buﬀer contains two consecutive coeﬃcients of a
polynomial. The BRAMs of the FPGA that are used to build the polynomial buﬀers are
inherently composed of two read/write ports, ﬁtting the butterﬂy data access pattern and
allowing it to read/write two diﬀerent addresses at the same time. However, the read
and write can only be done in separate clock cycles, resulting in 50% butterﬂy utilization
and halving the throughput. Therefore, we time interleave the two polynomial buﬀers, as
shown in Figure 7 (b), to achieve full utilization of the butterﬂies.

Due to the variation of the data access pattern of the butterﬂy units in each outer loop
of the INTT algorithm, there is a mismatch between the data access pattern and data
storage pattern, resulting in two diﬀerent data ﬂows from the buﬀers to the butterﬂies.
As shown in Figure 7 (c), in pattern 1, the data passes into a butterﬂy are from diﬀerent
addresses, while in pattern 2, they are from the same address. Therefore, necessary data
MUXs are appended to the butterﬂy units to reorder the input/output data as needed. All
the necessary loop counters and step counters are implemented inside the control block,
together with the control of the MUXs.

TF Init BlkCtrlMUXPoly Buffer 0Poly Buffer 14414Input RLWEOutputPolysInstPoly 0Poly 1RWRWRWCLK(a)(b)P1P2a[0]a[3]a[1]a[2]a[0]a[3]a[2]a[1](c)14

Hardware Acceleration for Third-Generation FHE and PSI Based on It

Figure 8: Architecture and Dataﬂow of the Pipelined NTT Module.

Besides the INTT functionality, the INTT module also incorporates an init block for

the homomorphic accumulator initialization function mentioned in Section 2.3.

4.3 Pipelined NTT Module

The NTT algorithm (Appendix E) is very similar to the INTT algorithm, except for the
last scaling loop [LN16]. But a diﬀerent construction from the INTT module is adopted
for the NTT module. The structure of the module is shown in Figure 8, and a discussion
of this construction is included in a later section.

To achieve higher throughput for the NTT module, the outer loop of the NTT algorithm
is unrolled into log(N ) pipeline stages, with each stage only processing one ﬁxed data
access pattern, greatly reducing the control complexity of each stage. Compared to the
structure of the INTT module, this implementation oﬀers the same processing latency for
an input polynomial but log(N ) times higher throughput.

Each stage reads the input from the polynomial buﬀer of the previous stage and
processes it with a predetermined data access pattern that is speciﬁc to that stage at
design time. So, there is no on-the-ﬂy control/MUXs for the data ﬂow, which not only
reduces resource usage but also allows a better timing requirement. Note that there is no
read/write from/to the same buﬀer memory; therefore, it is not necessary to employ the
time-interleave trick as in the INTT module.

Although the internal structures of the stages are mostly the same, except for the loop
counter and step counter inside the control block, extra care should be taken in actual
implementation. First, to adapt to diﬀerent polynomial lengths, MUXs are needed to
skip the leading stages for short polynomials (Figure 8). Second, the leading stages also
incorporate the decomposition functionality as stated in Section 2.2.4 and Section 2.4.4,
which is just a bitwise AND with a binary decomposition basis and is not detailed in the
ﬁgure.

4.4 Compute Pipeline Analysis: Asymmetric INTT and NTT

In the following section, we refer to the overall latency of the INTT/NTT algorithms as one
NTT latency (ONL) and the latency of one outer loop of the algorithm as one stage latency
(OSL). Therefore, ON L = log(N ) × OSL. And our compute pipeline architecture, utilizing
the pipelined NTT module (Figure 8) with non-pipelined INTT modules, is referred to as
asymmetric structure due to the throughput diﬀerence of the two styles. The conventional
implementation of using similar structures and latencies for both NTT and INTT modules
(Figure 7) is deﬁned as a symmetric structure.

The design of our compute pipeline concentrates on balancing high throughput with
optimized resource usage and parameter ﬂexibility. So, the main compute pipeline is built
around an asymmetric structure, as shown in Figure 6 (b). A comparison of the symmetric

TF CtrlPoly Buffer414Input PolyOutputPolyInstStage log(N)-1Stage 0Stage log(N)-2Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

15

Figure 9: Comparison of Symmetric (a) and Asymmetric (b) Compute Pipelines.

and asymmetric structures is given in Figure 9, with the poly subs block omitted as it is not
a throughput bottleneck. The dataﬂows of the RLW E N RGSW (Figure 1) and RLWE
substitution (Figure 4) can be mapped to both architectures with the same throughput.
However, the asymmetric structure consumes less resources than its symmetric counterpart.
In the symmetric pipeline (Figure 9 (a)), to have balanced throughput, one INTT module
is accompanied with dc-many NTT modules since each input polynomial is decomposed
into dc polynomials after the INTT operation. The throughput of both modules is one
polynomial per ONL due to the non-pipelined construction. The NTTs are also followed
by dc-many polynomial/RLWE multiplication blocks to facilitate the inner product of
the two dataﬂows. Although the trailing stages can operate with higher throughput, the
overall throughput of the whole pipeline is capped by the ﬁrst two stages, resulting in a
throughput of one polynomial per ONL. Higher throughput can be achieved by operating
multiple pipeline instances in parallel.

Most of the prior arts implemented an architecture that is similar to the symmetric
structure with the INTT and NTT modules separated without considering the data ﬂow
connecting the modules. We take it one level up and make use of the asymmetric structure
to cope with the diﬀerent throughput requirements of the INTT and NTT modules, as
shown in Figure 9 (b). Since the throughput of the pipelined NTT is OSL, the overall
throughput of the whole pipeline is one input polynomial per dc × OSL because of the
polynomial decomposition, with one caveat that to balance the throughput between the
INTT and NTT, log(N )/dc INTT modules should operate in parallel. Note that in the
asymmetric scenario, the trailing stages are also changed to the pipelined form (1 vs. dc
poly mult RLWE and an accumulation vs. a wide addition). In practice, log(N ) is always
greater than dc; therefore, the asymmetric pipeline enables higher throughput than a single
instance of the symmetric pipeline.

Though the symmetric structure can achieve the same throughput as the asymmetric
one, with log(N )/dc-many instances operating in parallel, as seen in Figure 9 (a), the
asymmetric pipeline uses less FPGA resources. The reduced resources stem from three
sources. First, it is clear that in both cases, the number of INTT modules is the same,
amounting to log(N )/dc. The number of NTT modules seems to be the same as well
since there are log(N )/dc × dc = log(N ) NTT modules for the symmetric pipeline and the
asymmetric one also incorporates log(N ) NTT stages. However, in the symmetric case,
the NTT module has a similar structure as the INTT module shown in Figure 7 (a), which

Poly decompNTT x dcPoly MultRLWE x dcRLWE AddKey RLWE PolydecompPoly MultRLWE RLWE AccKey RLWE dcLog(n) NTT stagesLoop x dcINTTINTT1/(dc x OSL)1/(dc x OSL)1/(dc x OSL)1/(dc x OSL)1/ONL1/ONL1/OSL1/OSL(a)(b)16

Hardware Acceleration for Third-Generation FHE and PSI Based on It

is much more complex than the NTT stage used in the pipelined NTT. Synthesis shows
that with pipelined NTT, 23% less LUT usage is achieved.

Furthermore, the pipelined NTT module has not only smaller control logic but also
lower memory requirements. Part of the savings comes from less TF memory in pipelined
NTT. Each of the NTT modules used in the symmetric pipeline stores a complete copy of
the polynomial of the TF in its own local memory, similar to what is shown in Figure 7
(a), so that they can operate independently. Therefore, in total, log(N ) copies of the TF
are stored. In contrast, in the asymmetric version, there is only one complete copy of the
TF. Because each stage of the pipelined NTT is only responsible for one outer loop of the
NTT algorithm, it only needs to store the portion of the TF that is used in that outer loop.
For example, in the ﬁrst stage of the pipelined NTT, instead of a complete polynomial of
TF with N coeﬃcients, only one TF needs to be stored. Thus, overall, a log(N ) times
reduction of the TF memory usage is achieved with pipelined NTT, equivalent to over
10× reduction in practice. It is possible to reduce the memory usage in the symmetric
version by sharing one TF memory within one pipeline and force all the NTT modules to
act at the same pace, but that implies stricter timing requirements since the capacitive
load of the memory output is dc times higher, exacerbating performance. Also forcing all
NTTs to synchronize degrades the ﬂexibility of the architecture.

The memory size of the pipelined NTT module is also reduced due to fewer polynomial
buﬀers. In the non-pipelined NTT module, similar to the INTT module in Section 4.2, two
polynomial buﬀers are instantiated for time-interleaved buﬀer access to maintain 100%
butterﬂy utilization. In contrast, each stage of the pipelined NTT module reads and writes
diﬀerent buﬀers; therefore, time-interleaving is unnecessary. So, the pipelined NTT poses
a 50% saving on the polynomial buﬀer compared to the non-pipelined version.

Lastly, the trailing stages of the asymmetric pipeline are also less complex than that
of its symmetric counterpart. As shown in Figure 9, since the pipelined NTT outputs
one polynomial at a time, only one poly mult RLWE module is needed in the asymmetric
structure, compared to log(N ) parallel mult modules in the symmetric one. In practice, it
reduces the amount of mult modules by 11 times, with N = 2048. Although the amount of
poly mult RLWE modules can be reduced in the symmetric pipeline by reusing one mult
module across diﬀerent NTTs in a time-interleaved manner due to the higher throughput
compared to the INTT/NTT, a very wide MUX, log(N ) to one, must be inserted between
the stages, which would greatly impact timing and performance and introduce more control
complexity. In the asymmetric structure, there is a similar MUX between the INTT and
NTT modules; however, it is only log(N )/dc to one, which is much smaller. In addition,
the dc-wide RLWE addition in the symmetric pipeline is also replaced with an RLWE
accumulation with ordinary word-size modulo addition.

Besides the resource savings, the asymmetric construction also automatically adjusts
to diﬀerent parameter settings. In the symmetric pipeline, the number of NTTs should be
set as the largest possible number of dc of the application at design time. If at design time,
the parallelism is 3 for NTT, when dc = 2 at run time, the utilization of the NTTs is only
66.7%. Extra eﬀort can be applied to remap the connection between INTT and NTT to
reach 100% utilization, but that comes with more control overhead, negatively impacting
performance. However, with the pipelined NTT module, as long as the INTT continuously
feeds input to it, 100% utilization is always maintained with no extra control overhead
involved since the design space of the pipelined NTT itself is independent of the parameter
dc. In fact, even when the dc of run time is higher than the designated dc of design time,
the pipelined NTT requires no extra control to handle it. However, it should be noted
that in the above case, the INTT of the asymmetric pipeline can be underutilized. But
since the number of INTT blocks is less than the number of NTT stages in general, it is
not optimized in this work.

Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

17

Table 1: Parameter Sets of The Third Generation FHE

Parameter Set
MEDIUM
STD128_AP
STD192
STD256
STD192Q
STD256Q

n
256
512
512
1024
1024
1024

q
512
512
512
1024
1024
1024

N
1024
1024
2048
2048
2048
2048

log

(Q) Bks BG Br
2
23
27
23
27
23
37
32
29
32
35
32
27

29
29
213
210
212
27

25
25
25
25
25
25

Parameter
Set

Table 2: Measured Processing Time of Homomorphic Accumulation Compared to Software
Amortized
Stream Out
Time (µs)
49
48
54
54
59
58

Amortized
Processing
Time (µs)
6615
13238
26253
52523
52524
70031

MEDIUM
STD128_AP
STD192
STD256
STD192Q
STD256Q

141100
283800
578400
1180800
1270500
1571500

Software
[MP21] (µs)

21.1×
21.3×
22.0×
22.4×
24.2×
22.4×

Improvement

5 Measurement

5.1 Experiment Setup

The proposed architecture is implemented at 125MHz system frequency on an AWS F1
instance. The implementation supports up to 54-bit input data word size. But to reduce
the complexity of the modulo multiplication block in the butterﬂy, only a subset of the bit
widths is implemented,as detailed in the following sections. Two polynomial lengths, 1024
and 2048, which are typical for third-generation FHE and ﬁt our experiment for the PSI
protocol, are supported natively. The polynomial buﬀers in the FIFOs, implemented with
BRAM, are conﬁgured to the size of the longer length, 2048. In addition, since 2 butterﬂy
units operate at the same time in the INTT and NTT, each buﬀer line contains two
consecutive polynomial coeﬃcients. Thus, the size of each polynomial buﬀer is predeﬁned
as 1024 × 108 bit. However, in this prototype, no optimization on the BRAM utilization is
devised, so when the input polynomial length is 1024, only the ﬁrst half of the buﬀer is
used. Following the analysis of Section 4.4, the number of INTT is set to largest possible
log(N )/dc to keep a balanced throughput, which is 4 in our implementation.

5.2 Measurement of Bootstrap of The Third Generation FHE

The parameter sets used to benchmark our implementation of the third-generation FHE
are listed in Table 1 and adopted from [MP21]. Since our work only implements the
homomorphic accumulation of the bootstrap process (including evaluation, accumulation,
and key switch) on the hardware, we only report the measurement of this operation
to emphasize our advancement. It is composed of two parts, the processing time and
the time of streaming out the result to host. Table 2 summarizes the measurement of
the homomorphic accumulation function. Due to the pipelined nature of the proposed
accelerator, the maximum parallelism is 12 accumulations. So. the reported time is
amortized over 12 inputs. Because the homomorphic accumulation function is independent
of the input binary gate, we do not diﬀerentiate it during the measurement. The reported

18

Hardware Acceleration for Third-Generation FHE and PSI Based on It

Table 3: Comparison of The Processing Time of The Two Operations for The Proposed
PSI

Operation
RLWE Substitution
RLW E N RGSW

Proposed
Accelerator (µs)
105
105

Software
(µs)
17616
14739

Improvement
167.8×
140.4×

Scaled
Improvement
27.9×
23.4×

Table 4: Sender’s Processing Time and Communication Size of The Proposed PSI

Parameters
b

k
14
12
10
14
12
10
14
12
10

32

30

28

Sender’s Processing
Time (s)
1642
814
585
1148
410
203
935
287
102

Communication Size
(MB)

b
27.0
7.5
2.1
24.0
6.8
1.9
21.0
6.0
1.7

k
256
64
16
256
64
16
256
64
16

time is averaged over all measured input gates.

The software implementation [MP21] of the FHEW scheme from the PALISADE
library [PRRC21] operates on the same host machine and is used for comparison. Table 2
gives the comparison result of the proposed accelerator over software implementation.
As stated above, only the homomorphic accumulation part is compared. On average, a
21× speed-up for the homomorphic accumulation function is achieved compared with the
software implementation.

5.3 Measurement of The Proposed PSI

In our implementation of the proposed PSI protocol, we set the encryption-related param-
eters to be N = 2048, log2(Q) = 54, with σ = 3.19, which achieves around 128-bit security
level according to the LWE estimator [APS15]. The BG of the RGSW and BKS of the
RLWE key-switch key are both set to 29. Since the proposed PSI is not directly available
in open-source libraries, we developed the necessary components of the scheme ourselves
for baseline comparison.

The average processing time of the two basic operations of the proposed PSI, RLWE
substitution and RLW E N RGSW , as deployed on the hardware are shown in Table 3. A
comparison to our own software implementation is also included in the table. The raw
measurement shows a speed-up factor of over 140, which is much higher compared to the
improvement of the bootstrap process. A discussion of this discrepancy is incorporated in
a later section. The last column, ‘Scaled Improvement,’ is added for this purpose and is
discussed later, as well.

Based on the time consumption of the basic operations from Table 3, the processing
times on the Sender’s side with the proposed accelerator and the communication size of
the proposed PSI are listed in Table 4. Since the complexity of our scheme is only directly
dependent on the bit width b and the hash table size 2k, assuming only one element in
each bin on the Receiver’s side, we only list these two factors as design parameters in the
table, with the security parameters set as above. In the Receiver-to-Sender communication

Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

19

Figure 10: Time Breakdown of the Proposed PSI.

size, the key-switch keys and the RGSW-encrypted −z are not included, which are of size
2.1 MB and 384 KB, respectively. Note that a modulus switch process can be applied
to the returning LWE ciphertexts from the Sender to the Receiver, which can further
reduce the message size by 15~20% [CLR17]. Figure 10 shows a time breakdown of the
proposed PSI operating with the proposed accelerator. Four parts are included: (a) RLWE
substitution; (b)RLW E N RGSW ; (c) RGSW transfer, which transfers the reconstructed
RGSWs to the FPGA DDR; and (d) software post process. The ﬁrst three are attributed
to hardware. The measured time consumption of each part is also included in the diagram.
The software post processing times are raw measurement data and not scaled, which will
be discussed in next section.

5.4 Analysis of The Measurement Results

5.4.1 Software Ineﬃciency Encountered during Measurement

As mentioned in the above section, compared to the improvement of the bootstrap process
listed in Table 2, we see a higher speed-up in the basic PSI operations, as shown in Table 3.
The discrepancy mainly results from the diﬀerent software implementations that are being
used in the comparison. Since the proposed PSI and its basic operations are not directly
available in open-source libraries, we developed the software implementation ourselves
from scratch for both verifying the hardware design and baseline comparison. We also
built our own software for the bootstrap process for the purpose of hardware veriﬁcation
and comparison.

However, due to our relatively limited eﬀort, our own software code may not perform
as eﬃciently as the highly optimized open-source libraries.
In order to estimate the
potential software performance discrepancy, a comparison between our own software and
an open-source library [PRRC21] is conducted with the same host machine using commonly
available operations such as NTT/INTT, polynomial operations, and bootstrap process.
Based on the comparison, our own software code is around 6× slower compared to open-
source library. Hence, the measured improvement in the third column of Table 3 is scaled

185.6151.5614.18164.9946.4012.89144.3641.2511.60442.06288.42245.97247.99110.5272.11183.3062.0027.63189.6452.6814.49168.5747.4113.17147.5042.1411.85825.09422.02310.65566.98206.27105.50460.08141.7551.570%20%40%60%80%100%RLWE Subs (s)RLWE mult RGSW (s)RGSW Transfer (s)Software Post Process (s)kb14123210141230101412281020

Hardware Acceleration for Third-Generation FHE and PSI Based on It

Table 5: Attainable Bound of Sender’s Processing Time of The Proposed PSI

Parameters
b

k
14
12
10
14
12
10
14
12
10

32

30

28

Sender’s Processing (s)
Measured Attainable Bound

1642
814
585
1148
410
203
935
287
102

273
135
97
191
68
33
155
47
17

by 6 to factor in potential software optimization for a more realistic speed-up number for
the basic operations of the proposed PSI. This scaled number is shown in the last column
of Table 3.

The ineﬃciency in our software code includes unoptimized post processing, which takes
about 50% of the total processing time of the proposed PSI operating on the accelerator
(Figure 10). Thus, by factoring out this ineﬃciency, the total time consumption of the
proposed PSI could be reduced by around 42% (which is not accounted for in the reported
performance in Table 4).

5.4.2 I/O Bandwidth Bottleneck of the Implemented Accelerator

During the measurement, we ﬁnd that the latency of processing just one input on
the proposed acceleration hardware is ~350 µs for RLWE substitution and 309 µs for
RLW E N RGSW , which includes 120 µs streaming in and out. Due to the pipelined
nature of the proposed accelerator, a maximum parallelism of 13 can be achieved in the
RLWE mode. Therefore, ideally, the average time consumption of processing one input
on the hardware should be ~17 µs, which is 6× faster compared to the numbers listed
in the ﬁrst column of Table 3. This shows that, in the RLWE mode, the accelerator is
bottlenecked by the I/O bandwidth. In the case that an optimized I/O is achieved, 6×
better performance can be extracted from the proposed accelerator.

Table 5 summarizes the (estimated) attainable bound of processing time of the proposed

PSI, which both factors out software ineﬃciency and operates on an optimized I/O.

6 Conclusion

In conclusion, the ﬁrst hardware acceleration architecture for third-generation FHE is
proposed in this paper. Featuring an asymmetric INTT/NTT conﬁguration, the proposed
compute pipeline achieves less resource usage while maintaining a high throughput. An
extensive analysis of the architecture is presented. An unbalanced PSI protocol based on
third-generation FHE is also proposed to better demonstrate the architecture. Supple-
mented by several optimizations for reducing the communication and computation costs,
the proposed PSI achieves a computation cost independent of the Sender’s set size. Imple-
mented with AWS cloud FPGA, the proposed accelerator achieves over 21× performance
improvement compared with a software implementation on various subroutines of the FHE
and the proposed PSI at 125 MHz.

Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

21

Acknowledgment

Covered for blind review.

A RLWE to LWE Conversion

Since RLWE is a special form of LWE, the coeﬃcients of the polynomial b of an RLWE
ciphertext RLW Ez(m) = [a, b] can be converted into multiple separate LWE ciphertexts
under the same secret key with some transformation of polynomial a. For example, in an
RLWE ciphertext [a = P a[i]X i, b = P b[i]X i], the coeﬃcient of b at index 0:

b[0] = z[0] × a[0] −

N −1
X

1

z[i] × a[N − i] + e[0] + m[0],

(13)

can be viewed as an LWE ciphertext LW Ez(m[0]) = [aLW E, b[0]] encrypted by secret
key z, where aLW E = [a[0], −a[N − 1], −a[N − 2], · · · − a[1]].

B Example of RLWE Expansion with RLWE Substitution

To demonstrate how RLWE substitution fulﬁlls the expansion, k = N + 1 is shown as
an example. For k = N + 1, (X i)k = (−1)iX i. Thus, the addition of the substituted
ciphertext to the original one extracts the even index coeﬃcients of the message m, and
the subtraction extracts the odd index coeﬃcients, as shown in Equation 14.

RLW Ez(X
RLW Ez(X

m[i]X i) + RLW Ez(X
m[i]X i) − RLW Ez(X

m[i](X i)k) = RLW Ez(X 2 × m[2i]X 2i)
m[i](X i)k) = RLW Ez(X 2 × m[2i + 1]X 2i+1)
(14)

Therefore, by recursively substituting with k = n/2s + 1, for s ∈ [0, log

2 N − 1], each
coeﬃcient m[i] of the message is extracted into a separate RLWE ciphertext RLW Ez(N ×
m[i]). The scale can be oﬀset by pre-scaling the message with the multiplicative inverse of
the N in ZQ.

C LUT Comparison with Permutation Based Hashing

The comparison in the homomorphic LUT still holds with permutation-based hashing.
Assuming that xiH from the Receiver and yjH from the Sender are in the same bin after
hashing and xiH = yjH , from Equation 11, it is apparent that

pos(xi) =H(xiH ) XOR xiL
=H(yjH ) XOR xiL
=pos(yj)
=H(yjH ) XOR yjL.

(15)

Therefore, xiL = yjL, resulting in x = y. Thus, the correctness of the LUT based PSI
holds with permutation-based hashing.

22

Hardware Acceleration for Third-Generation FHE and PSI Based on It

D INTT Algorithm

Algorithm 1: Inverse NTT | IN T T (aN T T )

Input: aN T T ∈ ZN
Q

in bit reverse order, Q ≡ 1 mod 2N ; a vector of twiddle

factors T F ∈ ZN
Q

storing the powers of ψ−1
N

in bit reverse order.

Output: a ← IN T T (aN T T ) in INTT domain with normal order.

1 t ← 1;
2 for (m ← N ; m > 1; m ← m/2) do
3

j1 ← 0;
h ← m/2;
for (i ← 0; i < h; i + +) do

j2 ← j1 + t − 1;
S ← T F [h + i];
for (j ← j1; j ≤ j2; j + +) do

// Gentleman-Sande Butterfly
U ← aN T T [j];
V ← aN T T [j + t];
aN T T [j] ← U + V mod Q;
aN T T [j + t] ← (U − V ) × S mod Q;

end
j1 ← j1 + 2t;

4

5

6

7

8

9

10

11

12

13

14

15

end
t ← 2t;

16
17 end
18 for (j ← 0; j < N ; j + +) do
19
20 end
21 a ← aN T T ;

aN T T [j] ← aN T T [j] × N −1 mod Q;

Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

23

E NTT Algorithm

Algorithm 2: NTT | N T T (a)

Input: a ∈ ZN
Q

, Q ≡ 1 mod 2N ; a vector of twiddle factors T F ∈ ZN
Q

storing the

powers of ψN in bit reverse order.

Output: aN T T ← N T T (a) in NTT domain with bit reverse order.

1 t ← N ;
2 for (m ← N ; m < N ; m ← 2m) do
3

t ← t/2;
for (i ← 0; i < m; i + +) do

4

5

6

7

8

9

10

11

12

13

j1 ← 2 · i · t;
j2 ← j1 + t − 1;
S ← T F [m + i];
for (j ← j1; j ≤ j2; j + +) do

// Cooley-Tukey Butterfly
U ← a[j];
V ← a[j + t] × S;
a[j] ← U + V mod Q;
a[j + t] ← (U − V ) mod Q;

end

end

14
15 end
16 aN T T ← a;

References

[APS15]

[Bar87]

[BGV12]

[BPC19]

[Bra12]

Martin R. Albrecht, Rachel Player, and Sam Scott. On the concrete hardness
of learning with errors. Journal of Mathematical Cryptology, 9(3):169–203,
2015.

Paul Barrett. Implementing the rivest shamir and adleman public key en-
cryption algorithm on a standard digital signal processor. In Proceedings on
Advances in Cryptology—CRYPTO ’86, page 311–323, Berlin, Heidelberg,
1987. Springer-Verlag.

Zvika Brakerski, Craig Gentry, and Vinod Vaikuntanathan. (leveled) fully
homomorphic encryption without bootstrapping. In Proceedings of the 3rd
Innovations in Theoretical Computer Science Conference, ITCS ’12, page
309–325, New York, NY, USA, 2012. Association for Computing Machinery.

Utsav Banerjee, Abhishek Pathak, and Anantha P. Chandrakasan. 2.3 an
energy-eﬃcient conﬁgurable lattice cryptography processor for the quantum-
secure internet of things. In 2019 IEEE International Solid- State Circuits
Conference - (ISSCC), pages 46–48, 2019.

Zvika Brakerski. Fully homomorphic encryption without modulus switching
from classical gapsvp. In Reihaneh Safavi-Naini and Ran Canetti, editors,
Advances in Cryptology – CRYPTO 2012, pages 868–886, Berlin, Heidelberg,
2012. Springer Berlin Heidelberg.

[CCR19]

Hao Chen, Ilaria Chillotti, and Ling Ren. Onion ring oram: Eﬃcient constant
bandwidth oblivious ram from (leveled) tfhe. In Proceedings of the 2019
ACM SIGSAC Conference on Computer and Communications Security, CCS

24

Hardware Acceleration for Third-Generation FHE and PSI Based on It

’19, page 345–360, New York, NY, USA, 2019. Association for Computing
Machinery.

[CGGI20]

I. Chillotti, N. Gama, M. Georgieva, and M. Izabachène. Tfhe: Fast fully
homomorphic encryption over the torus. Journal of Cryptology, 33:34–91,
2020.

[CHLR18] Hao Chen, Zhicong Huang, Kim Laine, and Peter Rindal. Labeled psi
from fully homomorphic encryption with malicious security. In Proceedings
of the 2018 ACM SIGSAC Conference on Computer and Communications
Security, CCS ’18, page 1223–1237, New York, NY, USA, 2018. Association
for Computing Machinery.

[CKKS17]

Jung Hee Cheon, Andrey Kim, Miran Kim, and Yongsoo Song. Homomorphic
encryption for arithmetic of approximate numbers. In Tsuyoshi Takagi and
Thomas Peyrin, editors, Advances in Cryptology – ASIACRYPT 2017, pages
409–437, Cham, 2017. Springer International Publishing.

[CLR17]

[DM15]

[DzS15]

[FV12]

[Gen09a]

[Gen09b]

[GH11]

[GSW13]

Hao Chen, Kim Laine, and Peter Rindal. Fast private set intersection from
homomorphic encryption. In Proceedings of the 2017 ACM SIGSAC Confer-
ence on Computer and Communications Security, CCS ’17, page 1243–1255,
New York, NY, USA, 2017. Association for Computing Machinery.

Léo Ducas and Daniele Micciancio. Fhew: Bootstrapping homomorphic
encryption in less than a second. In Elisabeth Oswald and Marc Fischlin,
editors, Advances in Cryptology – EUROCRYPT 2015, pages 617–640, Berlin,
Heidelberg, 2015. Springer Berlin Heidelberg.

Yarkın Doröz, Erdinç Öztürk, and Berk Sunar. Accelerating fully homomor-
phic encryption in hardware. IEEE Transactions on Computers, 64(6):1509–
1521, 2015.

Junfeng Fan and Frederik Vercauteren. Somewhat practical fully homomorphic
encryption. Cryptology ePrint Archive, Report 2012/144, 2012. https:
//ia.cr/2012/144.

Craig Gentry. A fully homomorphic encryption scheme. PhD thesis, Stanford
University, 2009. crypto.stanford.edu/craig.

Craig Gentry. Fully homomorphic encryption using ideal lattices. In Proceed-
ings of the Forty-First Annual ACM Symposium on Theory of Computing,
STOC ’09, page 169–178, New York, NY, USA, 2009. Association for Com-
puting Machinery.

Craig Gentry and Shai Halevi. Implementing gentry’s fully-homomorphic
encryption scheme. In Kenneth G. Paterson, editor, Advances in Cryptology –
EUROCRYPT 2011, pages 129–148, Berlin, Heidelberg, 2011. Springer Berlin
Heidelberg.

Craig Gentry, Amit Sahai, and Brent Waters. Homomorphic encryption from
learning with errors: Conceptually-simpler, asymptotically-faster, attribute-
based. In Ran Canetti and Juan A. Garay, editors, Advances in Cryptology
– CRYPTO 2013, pages 75–92, Berlin, Heidelberg, 2013. Springer Berlin
Heidelberg.

Zhehong Wang, Dennis Sylvester, Hun-Seok Kim, and David Blaauw

25

[HFH99]

Bernardo A. Huberman, Matt Franklin, and Tad Hogg. Enhancing privacy
and trust in electronic communities. In Proceedings of the 1st ACM Conference
on Electronic Commerce, EC ’99, page 78–86, New York, NY, USA, 1999.
Association for Computing Machinery.

[HS14]

Shai Halevi and Victor Shoup. Algorithms in helib. In Juan A. Garay and
Rosario Gennaro, editors, Advances in Cryptology – CRYPTO 2014, pages
554–571, Berlin, Heidelberg, 2014. Springer Berlin Heidelberg.

[HWF21]

Hardware acceleration for third generation fhe and psi based on it repo.
https://github.com/zhehongw/3rd_Gen_FHE_ACC, 2021.

[JGCM+15] C. Jayet-Griﬀon, M.-A. Cornelie, P. Maistri, Ph. Elbaz-Vincent, and R. Leveu-
gle. Polynomial multipliers for fully homomorphic encryption on fpga. In
2015 International Conference on ReConFigurable Computing and FPGAs
(ReConFig), pages 1–6, 2015.

[LN16]

[LPR10]

[Mea86]

[MP21]

Patrick Longa and Michael Naehrig. Speeding up the number theoretic
transform for faster ideal lattice-based cryptography. In Sara Foresti and
Giuseppe Persiano, editors, Cryptology and Network Security, pages 124–139,
Cham, 2016. Springer International Publishing.

Vadim Lyubashevsky, Chris Peikert, and Oded Regev. On ideal lattices
and learning with errors over rings.
In Henri Gilbert, editor, Advances
in Cryptology – EUROCRYPT 2010, pages 1–23, Berlin, Heidelberg, 2010.
Springer Berlin Heidelberg.

Catherine Meadows. A more eﬃcient cryptographic matchmaking protocol
for use in the absence of a continuously available third party. In 1986 IEEE
Symposium on Security and Privacy, pages 134–134, 1986.

Daniele Micciancio and Yuriy Polyakov. Bootstrapping in FHEW-like Cryp-
tosystems, page 17–28. Association for Computing Machinery, New York, NY,
USA, 2021.

[MRL+18] Vincent Migliore, Maria Méndez Real, Vianney Lapotre, Arnaud Tisserand,
Caroline Fontaine, and Guy Gogniat. Hardware/software co-design of an
accelerator for fv homomorphic encryption scheme using karatsuba algorithm.
IEEE Transactions on Computers, 67(3):335–347, 2018.

[MSR+17] Vincent Migliore, Cédric Seguin, Maria Méndez Real, Vianney Lapotre,
Arnaud Tisserand, Caroline Fontaine, Guy Gogniat, and Russell Tessier.
A high-speed accelerator for homomorphic encryption using the karatsuba
algorithm. ACM Trans. Embed. Comput. Syst., 16(5s), sep 2017.

[OOS17]

Michele Orrù, Emmanuela Orsini, and Peter Scholl. Actively secure 1-out-of-n
ot extension with application to private set intersection. In Helena Handschuh,
editor, Topics in Cryptology – CT-RSA 2017, pages 381–396, Cham, 2017.
Springer International Publishing.

[PRRC21] Yuriy Polyakov, Kurt Rohloﬀ, Gerard W. Ryan, and Dave Cousins. Palisade
homomorphic encryption software library. https://palisade-crypto.org/,
August 2021.

[PSZ14]

Benny Pinkas, Thomas Schneider, and Michael Zohner. Faster private set
intersection based on OT extension. In 23rd USENIX Security Symposium
(USENIX Security 14), pages 797–812, San Diego, CA, August 2014. USENIX
Association.

26

Hardware Acceleration for Third-Generation FHE and PSI Based on It

[PSZ18]

[Reg09]

Benny Pinkas, Thomas Schneider, and Michael Zohner. Scalable private set
intersection based on ot extension. ACM Trans. Priv. Secur., 21(2), jan 2018.

Oded Regev. On lattices, learning with errors, random linear codes, and
cryptography. J. ACM, 56(6), sep 2009.

[RLPD20] M. Sadegh Riazi, Kim Laine, Blake Pelton, and Wei Dai. Heax: An architec-
ture for computing on encrypted data. In Proceedings of the Twenty-Fifth
International Conference on Architectural Support for Programming Lan-
guages and Operating Systems, ASPLOS ’20, page 1295–1309, New York, NY,
USA, 2020. Association for Computing Machinery.

[SAN10]

G. Segev, Y. Arbitman, and M. Naor. Backyard cuckoo hashing: Constant
worst-case operations with a succinct representation. In 2013 IEEE 54th
Annual Symposium on Foundations of Computer Science, pages 787–796, Los
Alamitos, CA, USA, oct 2010. IEEE Computer Society.

[SEA21]

Microsoft SEAL (release 3.7).
September 2021. Microsoft Research, Redmond, WA.

https://github.com/Microsoft/SEAL,

[SRJV+18] Sujoy Sinha Roy, Kimmo Järvinen, Jo Vliegen, Frederik Vercauteren, and
Ingrid Verbauwhede. Hepcloud: An fpga-based multicore processor for fv
somewhat homomorphic function evaluation. IEEE Transactions on Comput-
ers, 67(11):1637–1650, 2018.

[SRTJ+19] Sujoy Sinha Roy, Furkan Turan, Kimmo Jarvinen, Frederik Vercauteren, and
Ingrid Verbauwhede. Fpga-based high-performance parallel architecture for
homomorphic computing on encrypted data. In 2019 IEEE International
Symposium on High Performance Computer Architecture (HPCA), pages
387–398, 2019.

[STCZ18]

Shiming Song, Wei Tang, Thomas Chen, and Zhengya Zhang. Leia: A
2.05mm2 140mw lattice encryption instruction accelerator in 40nm cmos. In
2018 IEEE Custom Integrated Circuits Conference (CICC), pages 1–4, 2018.

[SV14]

[TRV20]

[YCAR19]

N.P. Smart and F. Vercauteren. Fully homomorphic simd operations. Designs,
Codes and Cryptography, 71:57–81, 2014.

Furkan Turan, Sujoy Sinha Roy, and Ingrid Verbauwhede. Heaws: An
accelerator for homomorphic encryption on the amazon aws fpga. IEEE
Transactions on Computers, 69(8):1185–1196, 2020.

Insik Yoon, Ningyuan Cao, Anvesha Amaravati, and Arijit Raychowdhury. A
55nm 50nj/encode 13nj/decode homomorphic encryption crypto-engine for
iot nodes to enable secure computation on encrypted data. In 2019 IEEE
Custom Integrated Circuits Conference (CICC), pages 1–4, 2019.

