2
2
0
2

g
u
A
9

]

V
C
.
s
c
[

1
v
0
4
7
4
0
.
8
0
2
2
:
v
i
X
r
a

Aesthetic Language Guidance Generation of Images Using
Attribute Comparison

XIN JIN, Beijing Electronic Science and Technology Institute, China
QIANG DENG, Beijing Electronic Science and Technology Institute, China
JIANWEN LV, Beijing Electronic Science and Technology Institute, China
HENG HUANG, Beijing Electronic Science and Technology Institute, China
HAO LOU, Beijing Electronic Science and Technology Institute, China
CHAOEN XIAO∗, Beijing Electronic Science and Technology Institute, China
With the vigorous development of mobile photography technology, major mobile phone manufacturers
are scrambling to improve the shooting ability of equipments and the photo beautification algorithm of
software. However, the improvement of intelligent equipments and algorithms cannot replace human subjective
photography technology. In this paper, we propose the aesthetic language guidance of image (ALG). We divide
ALG into ALG-T and ALG-I according to whether the guiding rules are based on photography templates or
guidance images. Whether it is ALG-T or ALG-I, we guide photography from three attributes of color, lighting
and composition of the images. The differences of the three attributes between the input images and the
photography templates or the guidance images are described in natural language, which is aesthetic natural
language guidance (ALG). Also, because of the differences in lighting and composition between landscape
images and portrait images, we divide the input images into landscape images and portrait images. Both ALG-T
and ALG-I conduct aesthetic language guidance respectively for the two types of input images (landscape
images and portrait images).

CCS Concepts: • Applied computing → Media arts.

Additional Key Words and Phrases: computational aesthetics, aesthetic guidance, aesthetic template, language
generation, computational photography

1 INTRODUCTION

With the popularity and updating of the digital equipments, more and more people are focusing
on image aesthetics. Especially nowadays the popularity of smart products has made people
have higher and higher requirements for photographic quality. Nowadays, many smart product
manufacturers are accelerating the hardware research and development of shooting equipments, and
there are many beautifying softwares that are constantly improving the performance of beautifying
algorithms. But they are more of beautification of the pictures taken, without any photography
guidance to the photographer.

We propose aesthetic language guidance of image (ALG) that can guide photographers when
taking pictures, and we also provide two different approaches. If the guidance rules are based on
the photography templates, it is ALG-T; if the guidance rules are based on the guidance images,
it is ALG-I. Whether it is a photography template or a guidance image, aesthetic language of the
image is guided from the three attributes of color, composition, and lighting. Because landscape
images and portrait images are different in calculating composition and lighting attributes, these

*Corresponding authors.
Authors’ addresses: Xin Jin, jinxinbesti@foxmail.com, Beijing Electronic Science and Technology Institute, Beijing, Beijing,
China; Qiang Deng, 1352110584@qq.com, Beijing Electronic Science and Technology Institute, Beijing, Beijing, China;
Jianwen Lv, 513415184@qq.com, Beijing Electronic Science and Technology Institute, Beijing, Beijing, China; Heng Huang,
1264123696@qq.com, Beijing Electronic Science and Technology Institute, Beijing, Beijing, China; Hao Lou, 452392771@qq.
com, Beijing Electronic Science and Technology Institute, Beijing, Beijing, China; Chaoen Xiao∗, xcecd@qq.com, Beijing
Electronic Science and Technology Institute, Beijing, Beijing, China.

111

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

 
 
 
 
 
 
111:2

Xin Jin, Qiang Deng, Jianwen Lv, Heng Huang, Hao Lou, and Chaoen Xiao∗

two attributes need to be calculated respectively for these two types of images, while the method
for calculating the color attribute is the same for both types of images, whether in ALG-T or ALG-I.

Fig. 1. aesthetic language guidance based on photography templates (ALG-T)

Figure 1 is an example of using the ALG-T method to guide a portrait. As shown in Figure 1, the
method of using templates for guidance is to calculate the preset image feature values, comparing
it with the different values of the templates, and output it in natural language.

Fig. 2. aesthetic language guidance based on guidance images (ALG-I)

Figure 2 is an example of using the ALG-I method to guide a landscape image. As shown
in 2, our aesthetic guidance idea is to use a search model to search for any image that is similar
to the input image and has a high aesthetic score, and pick the image with the highest aesthetic
score as the guidance image. Then, depending on whether the input image is a landscape image or
a portrait image, three aesthetic attributesare calculated. Finally, compare the differences of the
three attributes between the input image and the guidance image, formulate appropriate rules, and
describe them in natural language.

In conclusion, Main contributions of this paper include:
1.To the best of our knowledge, this is the first work that proposes Aesthetic Language Guidance

(ALG) for photography.

2.We propose ALG via photography templates (ALG-T) for landscape and portrait photography.
3.We propose ALG based on guidance images (ALG-I) for landscape and portrait photography.

2 RELATED WORK

Image aesthetic assessment is chiefly based on the aesthetic perception of image characteristics
and the law of photography. Recent studies [15], [16], [11], [17] have revealed that using data-driven
methods is more efficient because the quantity of training data available has risen from hundreds of
images to millions of images. Researchers first explored the global characteristics. Data et al.[2] and
Ke et al.[10] is one of the first people to transform the aesthetic perception of images into binary
classification. Data et al.[2] combined the low-level features and high-level features commonly in
image retrieval and trained support vector machine classifier for binary classification of image
aesthetic quality.

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Aesthetic Language Guidance Generation of Images Using Attribute Comparison

111:3

Fig. 3. color harmony palette in the color rings

A simple method of using depth learning method is to use the general depth characteristics
obtained from other assignments, and train a new classifier on aesthetic classification tasks. Kong et
al.[11] raised to assist learning aesthetic features by pairwise sorting of image pairs, image attributes
and content information. Specifically, a connected structure with image pair as input is adopted, in
which the two basic networks of the connected structure are configured with alexnet (excluding the
fc8 of 1000 class classification layer from alexnet). Recently, Lu et al.[14] have found that using deep
convolutional networks in the field of image retrieval can better construct the similarity (visually
or semantically) between pairs of images. As we all know, similarity construction of images is the
premise of ratio. Jun-Tae Lee et al.[12] through two steps, constructing a ratio matrix of multiple
sets of pictures and then using some selected images as the reference images, to predict aesthetic
scores. Kao et al.[9] proposed multi-task learning of image aesthetics. Yan et al.[24], [23] aimed to
explain the content removed and transformed by clipping itself, and try to contain the impact of
the beginning composition of the initial image on the ending composition of the clipped image.

3 ALG BASED ON PHOTOGRAPHY TEMPLATES (ALG-T)

3.1 Aesthetic Color Guidelines

3.1.1 Color Palettes of Images Based on Color Harmony.

Most human perceptions of images are from color. Although our perceptions of color depend on
environments and cultures, we are influenced by color harmony instead of mechanically evaluating
images. The degree of color harmony in images not only depends on composition of various colors
and colors’ proportion, but also depends on the relative positions of different colors in color space.
Nowadays, there is no equation to define a harmonic set. However, there is a common sense in
defining the harmonic set: the degree of color harmony is calculated by various colors’ format and
relation in color space. Itten [8]proposed a series of new color rings in Figure 3. If the color in
the image is in the shadow area, we believe the image color is harmonic. Color harmony palette
can be rotated at any angle. Itten described the degree of color harmony. Itten’s color harmony
theory is based on the relative position of colors in the color ring. For example, matching three
basic colors, ultramarine blue, deep red and yellow, can generate a 12-color color ring. If two colors
are symmetric in color space, that can be called color harmony. He also realized that three-color
harmony can generate any triangle; four-color harmony can generate any rectangle; five-color
harmony can generate any pentagon. Based on Itten’s schemes, by combining several types of
color tones and its distributions, Matsuda invented an 80-color scheme. Those schemes were used
to both evaluate the degree of color harmony and design colors. Our color harmony method is also
based on those schemes.

Matsuda proposed color harmony that was well accepted by the field of color of application.
Figure 3 shows that the color tone space, in HSV (Hue, Saturation, Value), defines eight harmonic
types. Every color palette can be space distribution of color harmony palettes. If the color tone

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:4

Xin Jin, Qiang Deng, Jianwen Lv, Heng Huang, Hao Lou, and Chaoen Xiao∗

extracting from the image is in the shadow area of one palette, then, we will harmonize the image
based on this palette. The geometric distributions of those color space are called eight color palettes.
However, those color palettes only defined radial relationships of color ring instead of the given
and continuous color space. In the other word, all color palettes can rotate at any angle; The palette
regulates the relatively geometric distributions between color space. Color harmony palettes may
consist of tones with the same color (type i, V and T), complementary colors (type i, Y, X) , or more
complicated combinations.

For any input image, we can fit color harmony palette Tm and HSV color tone histogram
extracted from the input image. By the distance between the histogram and eight palettes, we can
make sure the most suitable color palette for the input image. The palette Tm and the relative
direction 𝛼 f it can define a color harmony scheme. We use (𝑚, 𝛼) to represent the scheme. For the
given scheme 𝐹 (𝑋, (𝑚, 𝛼)), we can define a function 𝐹 (𝑋, (𝑚, 𝛼)), which measures the degree of
harmony between image x and color harmony palette (𝑚, 𝛼), like equation:

𝐹 (𝑋, (𝑚, 𝛼)) =

∑︁

𝑝 ∈𝑋

(cid:13)𝐻 (𝑝) − 𝐸𝑇𝑚 (𝛼) (𝑝)(cid:13)
(cid:13)

(cid:13) · 𝑆 (𝑝)

(1)

In equation 1, H is hue channel, and S is saturation channel; the distance of color tone represents
the arc distance in the color ring. If HSV color tone histograms are in the shadow area of Tm, the
distance of the input image and the corresponding palette is zero.

𝑀 (𝑋,𝑇𝑚) = (𝑚, 𝛼0) , 𝛼0 = arg min

𝛼

𝐹 (𝑋, (𝑚, 𝛼))

(2)

For the given image X and palette Tm, the best color harmony scheme for X is the angle 𝛼 ∈ [0, 2𝜋]
meeting the minimum of equation 1, shown as equation 2.

𝐵 (𝑋 ) = (𝑚0, 𝛼0) , 𝑚0 = arg min

𝑚

𝐹 (𝑋, 𝑀 (𝑋,𝑇𝑚))

(3)

For the given image X, the best color harmony scheme is the palette meeting the minimum of
equation 2 in all possible palettes, shown as equation 3.

3.1.2 Aesthetic Color Guidelines Based on Color Palettes.

In the 3.1 section, eight color palettes were shown, as well as a color harmonizing algorithm
that relies on color harmony. However, this naive definition does not take into account the spatial
coherence between image pixels, which can lead to regions of discord due to "segmenting" adjacent
regions of the image.

To handle this situation, we use a segmentation method similar to Boykov, using an image
cutting optimization technique. Specify the compressed fan for each pixel, and to ensure that the
colors of the relatively continuous areas of the image are not compressed discontinuously, then
we can first classify each pixel of the image. When there are two fans, the pixel’s label is 0 or 1,
indicating which area should be divided into.

With this technique of fine-tuning colors, we use it as the color guidance in the aesthetic
guidance of images. The input image can be processed using the methods mentioned before to
improve the overall color of the input image.

After we have the original image and its processed image, we can compare the richness of the
colors. The standard we use is the color richness index defined by Hasler and Süsstrunk [4], which
is divided into 7 levels:
1. Not colorful
2. Slightly colorful
3. Moderately colorful
4. Averagely colorful

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Aesthetic Language Guidance Generation of Images Using Attribute Comparison

111:5

Fig. 4. Aesthetic color guidance result

5. Very (Quite colorful)
6. Highly colorful
7. Extremely colorful
Then by comparing the color richness of the two images, we can use language to guide whether

our image should be more vivid or frosty, and a example is shown in Figure 4.

3.2 Aesthetic Lighting Guidelines for Landscape Images

Fig. 5. Lighting Guidance for Landscape Image

This paper uses a deep learning-based technique that can estimate the lighting distribution
of outdoor images[3][7][20]. Through a specially designed CNN model and outdoor panorama
dataset. In these panoramas, an outdoor lighting model is applied to the sky to obtain a set of
parameters (including the location of sunlight exposure, weather conditions such as clouds and
haze, and camera parameters), and then a limited field of view is extracted from the panorama
images, and train a CNN using the light parameters from these input images. Given a test image,
the network can be used to infer lighting parameters, which in turn can be used to reconstruct
outdoor light distribution maps. Lighting plays a pivotal role in photography and plays a vital role
in determining the appearance of objects. Reverse analysis of light sources through images plays
an important guiding role in adjusting the position of light sources in photography.

Using the sky illumination model[21][5], the illumination intensity distribution of the image
in the horizontal direction can be calculated. On this basis, this paper performs discretization
processing and visual display, so that the horizontal distribution of light can be displayed in the
form of a ring, as shown in Figure 5. The CNN structure used in this paper is shown in Table 1: after
7 convolutional layers, a fully-connected layer is finally used to connect two heads of the model:
one for regressing the sun position, and the other for regressing the sky and camera parameters.
The illumination circle obtained in Figure 5 is divided into eight equal parts, corresponding to
eight illumination directions, namely front light, back light, left light, right light, left front light,
right front light, left back light, right back light. The center of each range of radians is used as
its illumination direction. But a new problem will arise at this time. The illumination angle of

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:6

Xin Jin, Qiang Deng, Jianwen Lv, Heng Huang, Hao Lou, and Chaoen Xiao∗

Table 1. Sky Lighting Model Parameters

Layer

Input
Conv7-64
Conv5-128
Conv3-256
Conv3-256
Conv3-256
Conv3-256
Conv3-256

Stride

Resolution

320*240
160*120
80*60
40*30
40*30
20*15
20*15
10*8

2
2
2
1
2
1
2
FC-2048

FC-160 LogSoftMax
Output: sun position Distribution s

FC-5 Linear
Output: sky and camera parameters q

Fig. 6. Spherical harmonics generated by portrait and face illumination

the maximum light intensity of the entire image does not necessarily appear in the above eight
directions. So by calculating the differences between the angle with the largest light intensity and
the angle corresponding to the previously determined light type, the result is converted into natural
language, which is the method to guide our landscape lighting. The result is shown in Figure 5.

3.3 Aesthetic Lighting Guidelines for Portrait Images

Traditional methods of portrait image illumination reconstruction based on physical illu-
mination models need to solve rendering problems and estimate face geometry, reflectivity and
illumination. However, the inaccurate estimation of face components can lead to strong artifacts in
heavy lighting, resulting in suboptimal results. In this work, we apply a physically-based portrait
relighting method to generate a large-scale, high-quality portrait lighting dataset (DPR). This
dataset is then used to train a deep convolutional neural network (CNN) to generate a reshot
portrait image. The model used in this paper is the structure of an hourglass network [18]. It has
encoder and decoder sections. Four skip connections are used to connect features of different
scales in the encoder part to features of corresponding scales in the decoder part. Using the above
calculated portrait, the nine-dimensional spherical harmonic function can be used to characterize
the illumination of the face. The original image and the calculated spherical harmonic function are
shown in Figure 6.

For the spherical harmonic function of face illumination generated in Figure 6, in order to
simplify the calculation, it can be projected in two dimensions, and the plane coordinate of the

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Aesthetic Language Guidance Generation of Images Using Attribute Comparison

111:7

Fig. 7. Portrait lighting guidance

Fig. 8. The calculation result of semantic line display

strongest illumination value is taken as its illumination center. And define the corresponding
coordinate values of three light centers here:

Rembrandt light: [82,172]
butterfly light: [63,127]
lower light: [191,127]
For any portrait image, light spherical harmonic function can be used to calculate the plane
distance between its illumination center and the above three points. And Which coordinate point is
the closest, then we think that the light type of the portrait image belongs to the type corresponding
to this coordinate point. Rembrandt-style lighting technology can divide the light and shadow of
the subject’s face into two parts with obvious contrast, and make the two parts of the face light
look different. If you use undifferentiated overlay lighting, the lighting details of the subject’s face
will be too convergent. So in most cases, Rembrandt lighting can be used to represent the lighting
conditions of most parts of the face.

Using spherical harmonics[19][18], we can calculate the degree of similarity between face
lighting and Rembrandt lighting, butterfly lighting, and bottom lighting. If it is higher than 0.9, we
consider it to be close. Otherwise, the lighting characteristics of such lighting should be strengthened.
The three portrait lighting features are preset in advance, and the results are shown in Figure 7.

3.4 Aesthetic Composition Guidelines for Landscape Images

The semantic line labeling algorithm [13][22][6][1]used in this paper is based on the deep
Huffman transform semantic line detection. In order to distinguish the classification of the compo-
sition, an excellent model is needed to detect the semantic line of the image. The meaningful line
structure in the image can divide the image to a certain extent, which is the semantic line. On this
basis, the standard input size of the image of the model used in this paper is unified to 640*426, so

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:8

Xin Jin, Qiang Deng, Jianwen Lv, Heng Huang, Hao Lou, and Chaoen Xiao∗

Fig. 9. Landscape composition guidance

Fig. 10. Portrait composition guidance

that it can calculate any image and describe it with the original image. Figure 8 is an example of
semantic line computation.

Converting the calculated semantic line to the parameter type, the four coordinate points can

be calculated as: ([0,363], [639,15]), ([0,402], [639,256]).

Each of the two coordinate points corresponds to a semantic line, and the value of each
coordinate corresponds to a pixel point of 640*426 in the image. The coordinate system is that the
upper left corner of the image is the origin, the downward direction is the positive direction of
the y-axis, and the right direction is the positive direction of the x-axis. Next we define several
composition templates and set the parameters of the shooting guide. They are diagonal composition,
thirds composition, and horizontal composition. By defining the positional relationship and angle
between the semantic lines, the types of composition can be divided and guided to adjust the angle
and parameters. The result is similar to Figure 9.

3.5 Aesthetic Composition Guidelines for Portrait Images

In order to accurately identify humans from images and calibrate their positions, this paper
uses a new object detection model named EfficientDet, which can always be achieved under the
constraints of small-scale datasets and high real-time computing power requirements with better
efficiency than existing models. EfficientNet [19] is adopted as the backbone network, BiFPN is
adopted as the feature network, and a shared class/box prediction network is adopted.

By comparing the distance between the extracted facial feature points and the feature points
of different compositions, as a basis for judging the type of composition, the result is shown in
Figure 10.

4 ALG BASED ON GUIDANCE IMAGES (ALG-I)

4.1 Image Search System

OpenAI introduced a neural network called CLIP which efficiently learns visual concepts
from natural language supervision. CLIP stands for Contrastive Language–Image Pre-training. For

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Aesthetic Language Guidance Generation of Images Using Attribute Comparison

111:9

Fig. 11. Image Search System for The Test Landscape Image

our use-case, we need to understand that CLIP is trained with a large set of images with their
corresponding captions. So it learned to predict which image caption (text) matches closely with
which image when a similarity metric (e.g.: Cosine) is applied on encodings of both image and
text. So after training, we can give a random image and find cosine similarity of that image in the
vector space with two vectors of phrases “Is this photo of a dog?”, “Is this photo of a cat?” and see
which one has the highest similarity to find the class of the image. So in a way, it has zero-shot
classification capabilities like GPT-2 and GPT-3. It is precise because of the powerful generalization
ability of the clip model that we can easily classify images. In other words, image features and
text vectors describing images can be converted to each other. Using this feature, we can calculate
the feature value of each photo in the collected image database in advance, which can realize fast
searching. For any image, we can find a batch of similar images through a search module. Through
some image quality evaluation algorithms, we can score the searched images, and the image with
the highest value can be used as our guidance image.

The image on the left in Figure 11 is the input image that needs to be searched for similarity.
The results are displayed as the top nine most similar images, as shown in the image on the right
in Figure 11.

4.2 Aesthetic Landscape Images Guidance By ALG-I

Fig. 12. Landscape Aesthetic Image Guidance process

The basic idea of guidances for aesthetic landscape images is that for any landscape image,
using our image search system in the 4.1, we can search for images similar to the provided landscape
image. They are then scored using a specific aesthetic scoring engine, with the highest scoring
landscape image selected as the guidance image. Now we have a image that needs guidance, and
a guidance image, by comparing the differences in the three attributes of lighting, composition,
and color between the two images, and compiling some rules to describe the differences in the
attributes between the two images. The overall process is shown in Figure 12.

Now, we have an input image that needs guidance, and a guidance image that has been selected.
Next, we apply the landscape aesthetic guidance described in the section 3 to the two images to

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:10

Xin Jin, Qiang Deng, Jianwen Lv, Heng Huang, Hao Lou, and Chaoen Xiao∗

Fig. 13. Night Landscape Aesthetics Guidance

Fig. 14. Daytime Landscape Aesthetics Guide

determine their lighting direction, color palette, and composition type. If a certain attribute is
different, for example, the input image is forward light, and the guidance image is left light, then the
photographic scheme of the guidance image in lighting is using the left light; if a certain attribute
of the two images is the same, use the aforementioned chapters, and output quantitative guidance
results such as adjusting the degree of vividness and adjusting the camera angle. Figure 13 is the
final guidance result of Figure 11.

As shown in Figure 14, we also present a photo of the aesthetic guidance results for daytime

landscape photography.

4.3 Aesthetic Portrait Images Guidance By ALG-I

Fig. 15. Portrait Aesthetic Image Guidance process

The basic idea of guidances for aesthetic portrait images is also that for any portrait image,
using our image search system in the 4.1, we can search for images similar to the provided portrait
image. It is then scored using a specific aesthetic scoring engine, with the highest scoring portrait
image selected as the guidance image. The guiding rules for image color are the same as landscape

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Aesthetic Language Guidance Generation of Images Using Attribute Comparison

111:11

Fig. 16. Image Search System for The Portrait Test Image

Fig. 17. Aesthetic portrait guide by the sea

Fig. 18. Aesthetic Guidance for Portraits in the City

images, but the composition type and lighting type are judged by the position of the face and the
light distribution. Finally, compare the differences of the three attributes between the two images
and output natural language for expression. The overall process is shown in Figure 15.

The overall process is similar to the section 4.2. The image on the left in Figure 16 is a portrait
that needs guidance. The image in the middle of Figure 16 is the 10 candidate images most similar
to the portrait test image obtained by the search module. Then use the aesthetic scoring engine to
score them, and select the image with the highest score as the guidance portrait , as shown in the
image on the right in Figure 16. The differences between the portrait test image and the guidance
portrait image are compared, and the results are shown in Figure 17.

As shown in Figure 18, we also present a urban portrait photography aesthetic guidance result.
Whether it is a landscape photo or a portrait photo, it is necessary to search for similar images
from the 2 million photographic image library based on the image search module, and evaluate the
image aesthetic quality, and select the highest score as the guidance image for the input image.
Then, according to whether it is a portrait image or a landscape image, different eigenvalues are

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:12

Xin Jin, Qiang Deng, Jianwen Lv, Heng Huang, Hao Lou, and Chaoen Xiao∗

calculated, different aesthetic standards are compiled, the differences between the attributes of the
two are compared, and finally the guidance is expressed in natural language.

5 CONCLUSIONS

The work of this paper is mainly divided into two parts: the first part is language aesthetics
shooting guidance based on photography templates. The second part is language aesthetics shooting
guidance based on guidance images. Both of them make a distinction between portrait images
and landscape images. And the color palette is calculated, the lighting distribution, semantic line
classification, portrait composition, and face lighting are used as different guiding attributes. The
guiding rules are designed and compiled, which are expressed in natural language.

On the other hand, there is no broad and unified definition and research scope for aesthetic
guidance at present. The exploration in this paper belongs to only one family, and the detection
indicators and guidelines are still superficial. In the future, more indicators can be added, more
sophisticated data can be collected, more accurate guidance rules can be written, and the data flow
between the running environment and the model can be unified, and the structure can be optimized
on this basis to achieve real-time guidance close to the app’s effects and abilities.

REFERENCES
[1] Chen, Z., Tagliasacchi, A., Zhang, H.: Bsp-net: Generating compact meshes via binary space partitioning. In: 2020

IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)

[2] Datta, R., Joshi, D., Li, J., Wang, J.Z.: Studying aesthetics in photographic images using a computational approach. In:

European conference on computer vision. pp. 288–301. Springer (2006)

[3] Goode, P.R., Qiu, J., Yurchyshyn, V., Hickey, J., Koonin, S.E.: Earthshine observations of the earth’s reflectance.

Geophysical Research Letters 28(9) (2001)

[4] Hasler, D., Suesstrunk, S.E.: Measuring colourfulness in natural images. Proceedings of SPIE - The International Society

for Optical Engineering 5007, 87–95 (2003)

[5] Hosek, L., Wilkie, A.: An analytic model for full spectral sky-dome radiance. Acm Transactions on Graphics 31(4), 1–9

(2012)

[6] Hough, P.: Method and means for recognizing complex patterns. U.s.patent (1962)
[7] Hošekhošek, L., Wilkie, A.: Adding a solar-radiance function to the hošek-wilkie skylight model. IEEE Computer

Graphics & Applications 33(3), 44–52 (2013)

[8] Itten, J.: The art of color: (1961)
[9] Kao, Y., He, R., Huang, K.: Visual aesthetic quality assessment with multi-task deep learning. arXiv preprint

arXiv:1604.04970 5 (2016)

[10] Ke, Y., Tang, X., Jing, F.: The design of high-level features for photo quality assessment. In: 2006 IEEE Computer Society

Conference on Computer Vision and Pattern Recognition (CVPR’06). vol. 1, pp. 419–426. IEEE (2006)

[11] Kong, S., Shen, X., Lin, Z., Mech, R., Fowlkes, C.: Photo aesthetics ranking network with attributes and content

adaptation. In: European Conference on Computer Vision. pp. 662–679. Springer (2016)

[12] Lee, J.T., Kim, C.S.: Image aesthetic assessment based on pairwise comparison a unified approach to score regression,
binary classification, and personalization. In: Proceedings of the IEEE International Conference on Computer Vision.
pp. 1191–1200 (2019)

[13] Liu, Y., Cheng, M.M., Hu, X., Wang, K., Bai, X.: Richer convolutional features for edge detection. IEEE Computer Society

(2016)

[14] Lu, H., Zhang, M., Xu, X., Li, Y., Shen, H.T.: Deep fuzzy hashing network for efficient image retrieval. IEEE Transactions

on Fuzzy Systems (2020)

[15] Lu, X., Lin, Z., Jin, H., Yang, J., Wang, J.Z.: Rapid: Rating pictorial aesthetics using deep learning. In: Proceedings of the

22nd ACM international conference on Multimedia. pp. 457–466 (2014)

[16] Lu, X., Lin, Z., Shen, X., Mech, R., Wang, J.Z.: Deep multi-patch aggregation network for image style, aesthetics, and
quality estimation. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 990–998 (2015)
[17] Mai, L., Jin, H., Liu, F.: Composition-preserving deep photo aesthetics assessment. In: Proceedings of the IEEE conference

on computer vision and pattern recognition. pp. 497–506 (2016)

[18] Newell, A., Yang, K., Jia, D.: Stacked hourglass networks for human pose estimation. In: European Conference on

Computer Vision (2016)

[19] Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neural networks (2019)

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Aesthetic Language Guidance Generation of Images Using Attribute Comparison

111:13

[20] Ward, G.: High dynamic range imaging. In: Color & Imaging Conference (2004)
[21] Xiao, J., Ehinger, K.A., Oliva, A., Torralba, A.: Recognizing scene viewpoint using panoramic place representation. In:

IEEE (2012)

[22] Xie, S., Tu, Z.: Holistically-nested edge detection. In: 2015 IEEE International Conference on Computer Vision (ICCV)

(2016)

[23] Yan, J., Lin, S., Bing Kang, S., Tang, X.: Learning the change for automatic image cropping. In: Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition. pp. 971–978 (2013)

[24] Yan, J., Lin, S., Kang, S.B., Tang, X.: Change-based image cropping with exclusion and compositional features. Interna-

tional Journal of Computer Vision 114(1), 74–87 (2015)

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

