2
2
0
2

l
u
J

1
1

]

M
Q
.
o
i
b
-
q
[

3
v
3
6
6
6
0
.
6
0
2
2
:
v
i
X
r
a

Quantitative Imaging Principles Improves Medical
Image Learning

Lambert T. Leong∗
Department of Molecular Bioscience and Bioengineering
University of Hawaii
lambert3@hawaii.edu

Michael C. Wong
Department of Epidemiology
University of Hawaii Cancer Center
mcwong@hawaii.edu

Yannik Glaser
Department of Computer Science
University of Hawaii
yglaser@hawaii.edu

Thomas Wolfgruber
Department of Epidemiology
University of Hawaii
tomwolf@hawaii.edu

Steven B. Heymsﬁeld
Pennington Biomedical Research Center
Louisiana State University
Steven.Heymsfield@pbrc.edu

Peter Sadowski
Department of Computer Science
University of Hawaii
peter.sadowski@hawaii.edu

John A. Shepherd
Department of Epidemiology
University of Hawaii Cancer Center
johnshep@hawaii.edu

Abstract

Fundamental differences between natural and medical images have recently fa-
vored the use of self-supervised learning (SSL) over ImageNet transfer learning
for medical image applications. Differences between image types are primarily
due to the imaging modality and medical images utilize a wide range of physics
based techniques while natural images are captured using only visible light. While
many have demonstrated that SSL on medical images has resulted in better down-
stream task performance, our work suggests that more performance can be gained.
The scientiﬁc principles which are used to acquire medical images are not often
considered when constructing learning problems. For this reason, we propose
incorporating quantitative imaging principles during generative SSL to improve
image quality and quantitative biological accuracy. We show that this training
schema results in better starting states for downstream supervised training on lim-
ited data. Our model also generates images that validate on clinical quantitative
analysis software.

1 Introduction

Deep learning is a promising methodology for medical imaging applications. However, acquiring
large labeled medical training datasets needed to train robust models is difﬁcult and major barriers
result from necessary laws that protect patient privacy. Transfer learning has been the most common

∗https://github.com/LambertLeong/DXA-VAE

Preprint. Under review.

 
 
 
 
 
 
method used to address data scarcity in which models would ﬁrst leverage ImageNet [1], for instance,
and subsequently be ﬁne-tuned with the limited medical domain data. While some have reported
beneﬁts from transfer learning [2] its efﬁciency and efﬁcacy for medical image learning problems
is questionable [3]. Medical images are inherently different from natural images contained in trans-
fer learning datasets like ImageNet. For example, subtle textures, pixel intensity, scale, and local
structures are signiﬁcant in medical images [4, 5] while large-scale shapes are of relevance in nat-
ural images where a global subject is often present. Stark differences between natural and medical
images highlight the need for improved domain speciﬁc methods.

Curation efforts of larger public medical imaging datasets [6, 7, 8, 9, 10] have made unsupervised
and self-supervised learning (SSL) a more promising alternative method than natural image transfer
learning alone. SSL enables the leveraging of unlabeled image data from the medical domain and
often results in better starting states for downstream task-speciﬁc tuning with limited data [11].
Despite performance improvement as a result of SSL, most medical imaging SSL implementations
mirror implementations used for natural images thus not considering the differences in medical
images.
In other words, medical image learning problems are often framed similar to learning
problems involving natural images and use similar tools, network architectures, or loss functions.
Consequently, medical image model learning may be hindered by the way in which some medical
images, as a data type, are misperceived and the method in which learning tasks are constructed.

Images from modalities which capture internal body components such as X-ray, computed tomogra-
phy (CT), or magnetic resonance imaging (MRI) are often viewed and used by model developers in
a qualitative way similar to the way natural images are used. However, many medical image types
should be used as quantitative maps instead of just qualitative anatomical images [12].
Internal
imaging modalities rely on speciﬁc biophysical interactions to generate images in which each pixel
corresponds to a speciﬁc biological, physical, and physiological property [13]. This is especially
relevant to the case of image reconstruction learning task which includes generative self-supervised
learning and cycle generative adversarial networks (GANs) [14]. Often, image quality metrics such
as peak signal to noise ratios (PSNR) or structural similarity indexes (SSIM) [15] are reported for re-
constructions of medical images yet the accuracy as it relates quantitative imaging measurements or
biology is underexplored. In fact, the lack of quantitative accuracy has limited the adoption of deep
learning generated images into clinical practice since it fails to validate quantitatively on commercial
clinical systems and software.

In this work, we explore an SSL training method to incorporate quantitative medical imaging prin-
ciples with the main objective of improving biological accuracy for image reconstruction and sub-
sequent learning tasks. This work focuses on the dual energy X-ray absorptiometry (DXA) image
type to illustrate the signiﬁcance of our contributions which are as follows:

1. Using quantitative imaging knowledge, we constructed a loss function to constrain the
SSL of a variational auto-encoder (VAE) which resulted in improved image reconstruction
quality as well as improved quantitative biological accuracy.

2. The SSL of the VAE with the custom loss function results in a trained encoder subnetwork
which outperformed an ImageNet transfer learning model on a medical imaging regression
task.

3. The trained VAE generator subnetwork resulting from SSL demonstrated superior perfor-
mance on a cross modality image translation task compared to a model not utilizing quan-
titative medical imaging knowledge during pretraining.

2 Related Work

We provide an overview of prior work in medical imaging pretraining methods as well as works on
quantitative DXA image deep learning.

Pretraining strategies are employed in medical image deep learning to overcome a lack of avail-
able training images, labels, or both. Reviews have highlighted the major differences between medi-
cal and non-medical imaging datasets and conclude that the beneﬁts of natural image (i.e. ImageNet)
transfer learning are not clear [11, 16, 17]. Recent focus has turned to self-supervised methods
which include methods such as contrastive learning [18, 19], auto-encoding [20, 21], and adver-
sarial [22, 23, 24] learning as alternatives strategies. The aforementioned methods allow models

2

to leverage more domain speciﬁc data, i.e. medical images, and these SSL methods have led to
improved image analysis, segmentation, and classiﬁcation [4, 25, 26, 27, 28]. Generative image
tasks such as denoising, image reconstruction, and artifact removal [29, 30] have also beneﬁted
from SSL methods such as multimodal imaging models and unpaired data learning by means of
cycleGANs [31, 32, 33, 34, 35]. This work builds upon the notion that medical images are unique
to images used in common transfer learning datasets and that SSL provides better results in per-
formance for downstream medical imaging tasks [19, 36]. This work differs from previous works
because our generative SSL method utilizes the knowledge of medical imaging physics [13] and
model generated images are evaluated on a quantitative biological imaging basis and not just for
image quality [15, 37, 24].

DXA is a quantitative imaging modality and has long been considered the criterion method for
measuring body composition [38] in addition to its primary and popular use of measuring bone den-
sity and quality for osteoporosis [39]. Deep learning DXA modeling primarily focuses on predicting
bone density metrics from images [40, 41, 42] and work on body composition evaluation or DXA
image reconstruction or generation is limited [43]. Clinical software is used to derive body composi-
tion from DXA images and while work has been done to reconstruct high quality DXA images with
deep learning [44], none have performed quantitative body composition analysis on such images.
Generating quantitatively accurate images is a more difﬁcult problem and we turn to informed deep
learning methods to address this problem. Domain informed learning such as physics, biology, or
etc. [45, 46], has shown to signiﬁcantly improve model learning and performance. These works
demonstrate the beneﬁt of further constraining the learning of models to the concepts, principles,
and law of the domain for which the task it is being trained to accomplish [47, 48].

3 Approach

We ﬁrst detail the DXA-only dataset used for SSL and the paired dataset used to compare subnet-
works trained with and without a custom DXA loss function to networks utilizing ImageNet transfer
learning. We then describe the SSL model architecture, followed by our custom DXA loss function.
Last, we describe how quantitative biological body composition is obtained from our actual and
predicted DXA scans.

3.1 Datasets

Data used for modeling and experiments were curated from multiple National Institutes of Health
(NIH) studies. All participants gave informed consent, and the study protocols were approved by
the Institutional Review Board (IRB) for each respective study. All DXA images used for modeling,
testing, and subsequent experiments were acquired on Hologic DXA systems (Hologic Inc., MA,
USA) and used in the raw image format. The Fit3D Proscanner (Fit3D Inc., CA, USA) was used to
acquire 3D body scans which were then standardized to the same pose and number of vertices with
the Meshcapade API (Meshcapade GmbH, Tübingen, Germany).

The SSL VAE was trained and evaluated using 16,002 whole-body DXA scans acquired as a part of
the Health ABC Study [49, 50] and Bone Mineral Density in Childhood Study [51, 52]. Data was
split into a train, validation, and holdout test set using an 80, 10, 10% split which was stratiﬁed by
patient age. Splits were also performed by patient key to avoid data leakage. Therefore duplicate
scans of the same patient, if any, remained together in the same split.

Experiments conducted to validate our SSL VAE network utilized two additional datasets that had
no overlap with those used to train the VAE. Paired images of DXA and 3D body surface scans were
acquired as a part of the Shape Up! Adults (R01DK109008) and Shape Up! Kids (R01DK111698)
studies. These scans were used to evaluate the trained VAE encoder and generator in two separate
experiments. The 3D body surface scan yielded many 3D anthropometric measurements such as
waist circumference, body volume, and torso length, to name a few. The combined dataset consisted
of 1107 unique DXA/3D scan pairs which were split by patient key into a train, validation, and
holdout test set using an 80, 10, 10% split.

3

3.2 Self-supervised Variational Auto-Encoder (VAE)

The decision to use a VAE was due to the modularity of the architecture and VAEs are commonly
used for SSL generative reconstruction [20]. For instance, the VAE consists of two main subnet-
work components which include the encoder and the decoder or generator. We used the trained
subcomponents to investigate possible improved starting points for training with our small paired
dataset. The Densenet121 [53] architecture was used as the encoder with a single convolutional
layer prepended to handle the six channel DXA input. The generator consisted of consecutive units
of bilinear upsampling and 2D convolutional layers.

3.3 DXA Loss Function

Raw DXA images from this particular manufacturer are acquired by cycling between two X-rays
energies, low and high, and three ﬁlters, air, tissue, and bone, which results in a six channel image
with a pixel depth of 14-bits. The search space of possible pixel values is large and therefore, we
exploit known relationships between the raw DXA channels to constrain the pixel level predictions
of the VAE for improved learned reconstructions.

Different attenuation values are obtained when X-ray photons at both low and high energies pass
through the same material; a body in this case [54, 55, 56]. The observed attenuation at low energy
can be expressed as a ratio (R) to the observed attenuation at high energy [57]. The voltages used
to generate the X-rays on a DXA system are predetermined and ﬁxed and thus, extensive studies
have been performed to map chemical elements to corresponding R values [58, 59, 60]. R values
for molecular components such as fatty acids, protein, and triglycerides found in soft tissue as well
as calcium hydroxyapatite composing bone are also well characterized [61]. As a result, there are
theoretically-derived R values that are speciﬁc to biological composition and this knowledge can be
used to constrain possible pixel values.

Every pixel on all DXA image channels represent X-ray attenuations at that given point in 2D
space [62]. Therefore, the R value can be calculated at each pixel location by dividing the low
energy image channels by the high energy image channels to obtain R images or quantitative maps of
varying fractions of biological molecules. Our DXA loss function evaluates both the reconstruction
of the raw six channel DXA images (low and high energy images plus 4 additional calibration
images) and the R images generated from each reconstruction. Using a perceptual loss [61], feature
maps are obtained for actual and predicted raw DXA and R images using a pretrained VGG-16 with
frozen weights. The mean absolute error (MAE) is computed for both the raw DXA image and the
R image feature maps and the DXA loss is the sum of that MAE for both image types.

3.4 Quantitative DXA Analysis for Body Composition Evaluation

In clinical practice, commercial software is used to analyze DXA images to obtain measures of body
composition for the entire body as well as for standardized and adhoc-deﬁned subregions. Composi-
tional analysis includes the quantiﬁcation of fat, lean, and bone mass as well as bone mineral density
(BMD) and bone mineral content (BMC). In this work, Hologic’s Apex 5.4 software was used to as-
sess biological accuracy, with respect to body composition, of deep learning generated DXA images.
This software is meant to be used in a clinical setting and not designed for high throughput analysis.
Additionally, the automated analysis features still requires trained personnel to assess that regions
of interest segmented the anatomical landmarks properly. Failure to properly identify regions of
interest will result in inaccurate body composition. This manual component limited the number of
images that could be analyzed quantitatively. Qualitative metrics were reported on the entire test-
set for all experiments however, results tables for quantitative body composition comparisons only
report on 117 images.

4 Experiments

In this section we ﬁrst compare image reconstructions from VAE models trained with and without
the DXA loss function. We then conduct two experiments to show that utilizing trained VAE sub-
networks results in better model performance when ﬁne-tuned for speciﬁc tasks on smaller datasets,
See Figure 1. Hyperparameters for all following models were tuned using the SHERPA [63] python
module and all modeling was performed on NVIDIA DGX-1 tesla v100 GPUs.

4

Figure 1: Training setup for SSL of the VAE model on unpaired DXA scans (left), pretraining
encoder experimental setup to predict 3D anthropometry from DXA scans (middle), and generator
pretraining experimental setup to predict DXA scans from 3D scans (right).

4.1 SSL of VAE with DXA Loss

The primary motivation for incorporating the DXA loss function was to improve biological accuracy
of pixel values by enforcing speciﬁc relationships between image channels. Using the same dataset
and data split, two identical VAEs were trained starting from randomly initialized weights. The
noDXA-VAE was trained using only a perceptual loss while the DXA-VAE utilized the DXA loss
detailed in Section3.3. We compared the quality of the image reconstructions by computing the
PSNR, SSIM, and normalized mean absolute error (NMAE) for both the actual and reconstructed
images. Image quality metrics were computed for images resulting from both the noDXA-VAE and
the DXA-VAE and the average values obtained by the test set are shown in Table 1.

Table 1: Image quality comparisons of DXA image predicted by self-supervised models when
trained without (noDXA-VAE) and with (DXA-VAE) the DXA loss functions.

Model
noDXA-VAE
DXA-VAE

PSNR ↑
49.52
51.15

SSIM ↑ NMAE ↓
0.038
0.032

0.997
0.998

The DXA-VAE model resulted in better PSNR, SSIM, and NMAE of 51.15, 0.998, and 0.032, re-
spectively when compared to the noDXA-VAE metrics of 49.52, 0.997, and 0.038, respectively,
shown in Table 1. These results suggest that the DXA loss improves the generation of DXA image
however, the quality metrics indicate only a slight improvement.

Figure 2 shows representative results test images generated with and without training with the DXA
loss. Each column represents one of the six raw DXA channels which. The ﬁrst row contains the
real DXA scan, the second and fourth rows contain a predicted DXA from each SSL VAE training
scenario (no DXA loss and DXA loss), and the third and ﬁfth rows contain the error maps computed
from the percent difference between the actual and predicted images. Error maps indicate more
difference in the prediction from the noDXA-VAE when compared to the network which utilized
the DXA loss.

To fully understand the beneﬁt of training with the DXA loss, we evaluate the biological composition
of the predicted image using clinical software. We report quantiﬁcations of lean and fat tissue as

5

Figure 2: Predicted DXA scan imaging channels and corresponding error maps (represented as
percentages) from VAE models strained without and with the DXA loss function

well as bone for the entire body as well as subregions which include just the arms and just the legs
alone. Compositional measurements are reported in Table 2.

Despite only a modest increase with respect to image quality, training with the DXA loss results in
images with more accurate biological composition. DXA-VAE images resulted in better quantitative
accuracy as captured by higher R2 values for all composition measurements. Using the DXA loss
resulted in an overall better model and reconstructed images were of higher quality and more accu-
rate with respect to body composition. The DXA-VAE which was trained in a SSL fashion was then
used to investigate the effectiveness of pretraining using domain speciﬁc data, i.e. medical images.

4.2 SSL VAE Encoder Performance Evaluation

While DXA scans are 2D image types, the pixel values are related to X-ray attenuation through
varying thickness of soft tissue and bone. As such, the objective of this modeling was to predict 3D
anthropometry from DXA scans. This dataset consisted of DXA scans and corresponding anthropo-
metrics measurements which include circumferences, volumes, and surface areas of various parts of
the body. We trained three models, identical in architecture, that differed only in their pretraining
methodology. One model utilized a common transfer learning strategy by loading the Densenet121

6

Table 2: Quantitative body composition analysis comparing images predicted from the noDXA-VAE
and DXA-VAE.

Measurements

R2
noDXA-VAE

R2
DXA-VAE

RMSE
noDXA-VAE

RMSE
DXA-VAE

Arm BMC
Arm BMD
Arm Fat
Arm Lean
Leg BMC
Leg BMD
Leg Fat
Leg Lean
Total BMC
Total BMD
Total Fat
Total Lean
Total Mass

0.61
0.49
0.22
0.68
0.67
0.47
0.31
0.68
0.63
0.34
0.42
0.73
0.79

0.88
0.84
0.63
0.89
0.87
0.76
0.70
0.88
0.89
0.75
0.80
0.91
0.99

0.04
0.09
0.66
0.73
0.06
0.08
1.43
1.68
0.34
0.10
6.84
8.95
9.82

0.02
0.07
0.64
0.57
0.04
0.03
1.51
1.39
0.21
0.05
7.88
8.46
1.43

portion of the network with ImageNet weights while the other two models were created from the
encoder portion of our trained DXA-VAE, one trained without the DXA loss function and the other
trained with. Hyperparameters, architectures, and data splits were kept consistent across all models
to ensure fair comparisons. Prediction results from the holdout test set are shown in Table 3.

Table 3: Comparisons of ImageNet transfer learning, SSL of VAE without the DXA loss function,
and SSL of VAE with the DXA loss function for predicting 3D anthropometry from DXA scans.

Measurment

Anatomical
Location

R2
ImageNet

R2
noDXA-VAE
Encoder

R2
DXA-VAE
Encoder

RMSE
ImageNet

RMSE
noDXA-VAE
Encoder

RMSE
DXA-VAE
Encoder

Circumference (cm) Bicep Left
Circumference (cm) Bicep Right
Circumference (cm) Calf Left
Circumference (cm) Calf Right
Circumference (cm) Hip
Circumference (cm) Thigh Left
Circumference (cm) Thigh Right
Circumference (cm) Waist
Surface Area (cm2) Whole Body
Volume (L)
Volume (L)
Volume (L)
Volume (L)
Volume (L)
Volume (L)

Arm Left
Arm Right
Leg Left
Leg Right
Torso Volume
Total Volume

0.84
0.86
0.72
0.76
0.88
0.78
0.79
0.92
0.96
0.82
0.86
0.67
0.70
0.94
0.97

0.85
0.88
0.77
0.80
0.89
0.80
0.81
0.92
0.96
0.83
0.86
0.71
0.75
0.94
0.97

0.88
0.94
0.85
0.88
0.93
0.82
0.85
0.94
0.96
0.86
0.91
0.76
0.78
0.96
0.97

2.00
1.80
1.75
1.65
3.80
2.18
2.26
3.53
0.40
0.53
0.40
0.78
0.78
0.35
0.31

1.88
1.63
1.51
1.44
3.69
2.12
2.12
3.51
0.39
0.50
0.39
0.74
0.71
0.34
0.31

1.63
1.22
1.26
1.17
3.17
2.01
1.99
3.40
0.39
0.39
0.29
0.70
0.67
0.27
0.30

The results in Table 3 indicate that transfer learning with ImageNet may be a reasonable approach
for some learning problems involving medical imaging. However, the results also indicate that SSL
on domain speciﬁc data will yield superior performance when compared to transfer learning from
natural images. SSL VAE encoder models trained with the DXA loss resulted in the best R2 for all
anthropometry measurements with the exception of total volume and total surface area. All models
achieved similar performance R2 of 0.97 and 0.96, respectively. The best anthropometry predictions
(bolded in Table 3) resulted from the model which included the VAE encoder pretrained with the
DXA loss function.

4.3 SSL VAE Generator Performance Evaluation

The motivation for this experiment was to explore the notion that one’s exterior 3D body shape is
directly related to the underlying bone and soft tissue distribution or composition. Here we again
used a paired dataset consisting of 3D body scans and DXA scans from the same individual taken
at the same time point. The objective was to derive a mapping to generate DXA scans for a given
3D scan. Just as in the previous section, we are comparing performance improvements brought
about by SSL which incorporated quantitative imaging physics. In this experiment, however, we

7

used the trained VAE decoder or generator instead of the encoder subnetwork. The architecture
of the 3D scan or mesh network was modeled after the Pointnet++ [64] model and it mapped to
the latent space of the DXA generator. For fair comparison, a second model was trained using the
same hyperparameters, data split, and architecture with the only difference being that the generator
resulted from VAE SSL without the DXA loss function. We evaluate both 3D to DXA or Pseudo-
DXA models on the basis of image quality (Table 4) and biological composition (Table 5).

Table 4: Image quality comparisons of DXA image predicted from 3D body surface by networks
generators trained without (noDXA-VAE) and with (DXA-VAE) the DXA loss functions.

Model
noDXA-VAE Generator
DXA-VAE Generator

PSNR ↑
35.14
38.49

SSIM ↑ NMAE ↓
0.201
0.130

0.897
0.938

(a) Example of a participant 3D body
surface scan.

(b) Predicted DXA scan imaging channels and corresponding error
maps (represented as percentages) resulting from the 3D body scan (Fig-
ure 3(a)) and differing modeling methods.

Figure 3: Representative participant 3D scan and predicted DXA image with error map produced by
Pseudo-DXA models in which generators were pretrained with and without the DXA loss.

The Pseudo-DXA which incorporated the trained VAE generator yielded better image quality met-
rics when compared to the Pseudo-DXA models with randomly initialized weights. Figure 3 con-

8

tains the input 3D body scan and Figure 3(a) contains the actual corresponding DXA image and
predicted images from both models accompanied by their error maps. Figure 3(b) shows better
DXA image predictions are produced from the model using the SSL VAE generator. The randomly
initialized weights model fails to produce clear DXA images and large prediction errors can be seen
in the low energy tissue channel. Positioning differences account for some of the signiﬁcant errors
(red and orange regions) visualized on the error maps in Figure 3. Some image quality metrics are in-
ﬂuenced by positioning difference however, body composition assessment is invariant to differences
in position.

Table 5: Body composition analysis of DXA image predicted from 3D body surface by networks
generators trained without (noDXA-VAE) and with (DXA-VAE) the DXA loss functions

Measurments

R2
noDXA-VAE
Generator

R2
DXA-VAE
Generator

RMSE
noDXA-VAE
Generator

RMSE
DXA-VAE
Generator

Arm BMC
Arm BMD
Arm Fat
Arm Lean
Leg BMC
Leg BMD
Leg Fat
Leg Lean
Total BMC
Total BMD
Total Fat
Total Lean
Total Mass

0.31
0.17
0.28
0.40
0.29
0.06
0.38
0.31
0.20
0.02
0.34
0.39
0.53

0.71
0.67
0.52
0.74
0.73
0.52
0.54
0.79
0.66
0.41
0.68
0.80
0.90

0.05
0.14
0.74
1.18
0.12
0.22
1.65
2.68
0.60
0.23
10.05
1.18
12.99

0.03
0.08
0.66
0.65
0.07
0.15
1.55
1.51
0.37
0.14
7.44
8.58
5.28

We deduce from Table 5 that the DXA-VAE resulted in a better generator which was used to con-
struct a Pseudo-DXA model that can produce more biologically accurate images. Pseudo-DXA
containing the trained generator out performed the model containing the noDXA-VAE generator on
all body composition values. It should be noted that image quality and body composition were lower
when comparing outputs from Pseudo-DXA models to outputs from the VAE models. This is likely
caused by a relatively weak mapping between the 3D and 2D DXA space and overﬁtting due to the
smaller paired dataset.

5 Conclusion

To our knowledge, this is the ﬁrst instance in which a deep learning model can generate medical im-
ages on which subsequent quantitative imaging analysis can be performed accurately. In this work
we frame medical images as quantitative maps rather than just anatomical representations. In doing
so, we were able to introduce a simple DXA loss function which constrained SSL of a VAE to learn
the speciﬁc pixel relationships between image channels. These speciﬁc relationships correspond to
biological quantities of soft tissue and bone which allow for the derivation of body composition.
Subnetworks of the trained DXA-VAE demonstrated superior performance when trained for down-
stream speciﬁc tasks which includes predicting 3D anthropology from DXA scans and producing
real analyzable DXA scans from 3D body scans. The broader impact of this work is to demonstrate
that medical image deep learning should consider the principles used to generate or produce images.
This will lead towards more accurate synthesized or reconstructed images that can then be better
integrated into clinical practice.

9

References

[1] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Litajb, and Li Fei-Fei. ImageNet: A large-
scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern
Recognition, pages 248–255, 2009. ISSN: 1063-6919.

[2] Veronika Cheplygina, Marleen de Bruijne, and Josien P.W. Pluim. Not-so-supervised: A survey
of semi-supervised, multi-instance, and transfer learning in medical image analysis. Medical
Image Analysis, 54:280–296, 2019.

[3] Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understand-
In H. Wallach, H. Larochelle, A. Beygelzimer,
ing transfer learning for medical imaging.
F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019.

[4] Olle G. Holmberg, Niklas D. Köhler, Thiago Martins, Jakob Siedlecki, Tina Herold, Leonie
Keidel, Ben Asani, Johannes Schiefelbein, Siegfried Priglinger, Karsten U. Kortuem, and
Fabian J. Theis. Self-supervised retinal thickness prediction enables deep learning from un-
labelled data to boost classiﬁcation of diabetic retinopathy. Nature Machine Intelligence,
2(11):719–726, 2020. Number: 11 Publisher: Nature Publishing Group.

[5] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M.
Summers. ChestX-ray: Hospital-scale chest x-ray database and benchmarks on weakly super-
vised classiﬁcation and localization of common thorax diseases. In Le Lu, Xiaosong Wang,
Gustavo Carneiro, and Lin Yang, editors, Deep Learning and Convolutional Neural Networks
for Medical Imaging and Clinical Informatics, Advances in Computer Vision and Pattern
Recognition, pages 369–392. Springer International Publishing, 2019.

[6] Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy
Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, and Andrew Y. Ng.
CheXNet: Radiologist-level pneumonia detection on chest x-rays with deep learning. arXiv,
2017.

[7] Cathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul
Downey, Paul Elliott, Jane Green, Martin Landray, Bette Liu, Paul Matthews, Giok Ong, Jill
Pell, Alan Silman, Alan Young, Tim Sprosen, Tim Peakman, and Rory Collins. UK biobank:
An open access resource for identifying the causes of a wide range of complex diseases of
middle and old age. PLOS Medicine, 12(3):e1001779, 2015. Publisher: Public Library of
Science.

[8] Varun Gulshan, Lily Peng, Marc Coram, Martin C. Stumpe, Derek Wu, Arunachalam
Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, Ra-
masamy Kim, Rajiv Raman, Philip C. Nelson, Jessica L. Mega, and Dale R. Webster. Devel-
opment and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in
Retinal Fundus Photographs. JAMA, 316(22):2402–2410, 12 2016.

[9] Pranav Rajpurkar, Jeremy Irvin, Aarti Bagul, Daisy Ding, Tony Duan, Hershel Mehta, Brandon
Yang, Kaylie Zhu, Dillon Laird, Robyn L. Ball, Curtis Langlotz, Katie Shpanskaya, Matthew P.
Lungren, and Andrew Y. Ng. Mura: Large dataset for abnormality detection in musculoskeletal
radiographs, 2017.

[10] Nicholas Bien, Pranav Rajpurkar, Robyn L. Ball, Jeremy Irvin, Allison Park, Erik Jones,
Michael Bereket, Bhavik N. Patel, Kristen W. Yeom, Katie Shpanskaya, Safwan Halabi, Evan
Zucker, Gary Fanton, Derek F. Amanatullah, Christopher F. Beaulieu, Geoffrey M. Riley, Rus-
sell J. Stewart, Francis G. Blankenberg, David B. Larson, Ricky H. Jones, Curtis P. Langlotz,
Andrew Y. Ng, and Matthew P. Lungren. Deep-learning-assisted diagnosis for knee mag-
netic resonance imaging: Development and retrospective validation of mrnet. PLOS Medicine,
15(11):1–19, 11 2018.

[11] Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton,
Aaron Loh, Alan Karthikesalingam, Simon Kornblith, Ting Chen, Vivek Natarajan, and Mo-
hammad Norouzi. Big self-supervised models advance medical image classiﬁcation. In 2021
IEEE/CVF International Conference on Computer Vision (ICCV), pages 3458–3468. IEEE,
2021.

[12] Vikas Gulani and Nicole Seiberlich. Quantitative MRI: Rationale and challenges.

In
Nicole Seiberlich, Vikas Gulani, Fernando Calamante, Adrienne Campbell-Washburn, Mariya

10

Doneva, Houchun Harry Hu, and Steven Sourbron, editors, Advances in Magnetic Resonance
Technology and Applications, volume 1 of Quantitative Magnetic Resonance Imaging, pages
xxxvii–li. Academic Press, 2020.

[13] Jerrold T. Bushberg. The Essential Physics of Medical Imaging. Lippincott Williams &

Wilkins, 2002. Google-Books-ID: VZvqqaQ5DvoC.

[14] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image
In Computer Vision (ICCV), 2017

translation using cycle-consistent adversarial networks.
IEEE International Conference on, 2017.

[15] Alain Horé and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th International

Conference on Pattern Recognition, pages 2366–2369, 2010.

[16] Mohammad Amin Morid, Alireza Borjali, and Guilherme Del Fiol. A scoping review of trans-
fer learning research on medical image analysis using ImageNet. Computers in Biology and
Medicine, 128:104115, 2021.

[17] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio,
Francesco Ciompi, Mohsen Ghafoorian, Jeroen A. W. M. van der Laak, Bram van Ginneken,
and Clara I. Sánchez. A survey on deep learning in medical image analysis. Medical Image
Analysis, 42:60–88, 2017.

[18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for

unsupervised visual representation learning. arXiv, 2020.

[19] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big

self-supervised mod[els are strong semi-supervised learners. arxiv, 2020.

[20] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting
and composing robust features with denoising autoencoders. In Proceedings of the 25th inter-
national conference on Machine learning - ICML ’08, pages 1096–1103. ACM Press, 2008.

[21] Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-
encoders as generative models. In Proceedings of the 26th International Conference on Neural
Information Processing Systems - Volume 1, NIPS’13, page 899–907, Red Hook, NY, USA,
2013. Curran Associates Inc.

[22] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-
realistic single image super-resolution using a generative adversarial network, 2016.

[23] Jeff Donahue and Karen Simonyan. Large Scale Adversarial Representation Learning. Curran

Associates Inc., Red Hook, NY, USA, 2019.

[24] Karim Armanious, Chenming Jiang, Marc Fischer, Thomas Küstner, Tobias Hepp, Konstantin
Nikolaou, Sergios Gatidis, and Bin Yang. Medgan: Medical image translation using gans.
Computerized Medical Imaging and Graphics, 79:101684, 2020.

[25] Liang Chen, Paul Bentley, Kensaku Mori, Kazunari Misawa, Michitaka Fujiwara, and Daniel
Rueckert. Self-supervised learning for medical image analysis using image context restoration.
Medical Image Analysis, 58:101539, 2019.

[26] Florin C. Ghesu, Bogdan Georgescu, Awais Mansoor, Youngjin Yoo, Dominik Neumann, Prag-
neshkumar Patel, R. S. Vishwanath, James M. Balter, Yue Cao, Sasa Grbic, and Dorin Comani-
ciu. Self-supervised learning from 100 million medical images. CoRR, abs/2201.01283, 2022.

[27] Krishna Chaitanya, Ertunc Erdil, Neerav Karani, and Ender Konukoglu. Contrastive learning
of global and local features for medical image segmentation with limited annotations. Ad-
vances in Neural Information Processing Systems, 33:12546–12558, 2020.

[28] Nima Tajbakhsh, Yufei Hu, Junli Cao, Xingjian Yan, Yi Xiao, Yong Lu, Jianming Liang,
Demetri Terzopoulos, and Xiaowei Ding. Surrogate supervision for medical image analysis:
Effective deep learning from limited quantities of labeled data. In 2019 IEEE 16th Interna-
tional Symposium on Biomedical Imaging (ISBI 2019), pages 1251–1255. IEEE, 2019.

[29] Yufan He, Aaron Carass, Lianrui Zuo, Blake E. Dewey, and Jerry L. Prince. Autoencoder
based self-supervised test-time adaptation for medical image analysis. Medical Image Analysis,
72:102136, 2021.

11

[30] Franco Matzkin, Virginia Newcombe, Susan Stevenson, Aneesh Khetani, Tom Newman,
Richard Digby, Andrew Stevens, Ben Glocker, and Enzo Ferrante. Self-supervised skull re-
construction in brain CT images with decompressive craniectomy. arxiv, 2020.

[31] Chengjia Wang, Gillian Macnaught, Giorgos Papanastasiou, Tom MacGillivray, and David
Newby. Unsupervised learning for cross-domain medical image synthesis using deformation
invariant cycle consistency networks.
In Ali Gooya, Orcun Goksel, Ipek Oguz, and Ninon
Burgos, editors, Simulation and Synthesis in Medical Imaging, Lecture Notes in Computer
Science, pages 52–60. Springer International Publishing, 2018.

[32] Hongming Shan, Atul Padole, Fatemeh Homayounieh, Uwe Kruger, Ruhani Doda Khera,
Chayanin Nitiwarangkul, Mannudeep K Kalra, and Ge Wang. Competitive performance of
a modularized deep neural network compared to commercial algorithms for low-dose ct image
reconstruction. Nature Machine Intelligence, 1(6):269–276, 2019.

[33] Lingke Kong, Chenyu Lian, Detian Huang, zhenjiang li, Yanle Hu, and Qichao Zhou. Break-
ing the dilemma of medical image-to-image translation. In Advances in Neural Information
Processing Systems, volume 34, pages 1964–1978. Curran Associates, Inc., 2021.

[34] Yuhui Ma, Yonghuai Liu, Jun Cheng, Yalin Zheng, Morteza Ghahremani, Honghan Chen,
Jiang Liu, and Yitian Zhao. Cycle structure and illumination constrained GAN for medical
image enhancement. In Anne L. Martel, Purang Abolmaesumi, Danail Stoyanov, Diana Ma-
teus, Maria A. Zuluaga, S. Kevin Zhou, Daniel Racoceanu, and Leo Joskowicz, editors, Medi-
cal Image Computing and Computer Assisted Intervention – MICCAI 2020, Lecture Notes in
Computer Science, pages 667–677. Springer International Publishing, 2020.

[35] Bing Cao, Han Zhang, Nannan Wang, Xinbo Gao, and Dinggang Shen. Auto-GAN: Self-
supervised collaborative learning for medical image synthesis. Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence, 34(7):10486–10493, 2020. Number: 07.

[36] Alexander Chowdhury, Jacob Rosenthal, Jonathan Waring, and Renato Umeton. Applying self-
supervised learning to medicine: Review of the state of the art and medical implementations.
In Informatics, volume 8, page 59. Multidisciplinary Digital Publishing Institute, 2021.

[37] Hoo-Chang Shin, Neil A Tenenholtz, Jameson K Rogers, Christopher G Schwarz, Matthew L
Senjem, Jeffrey L Gunter, Katherine P Andriole, and Mark Michalski. Medical image syn-
thesis for data augmentation and anonymization using generative adversarial networks.
In
International workshop on simulation and synthesis in medical imaging, pages 1–11. Springer,
2018.

[38] John A Shepherd, Bennett K Ng, Markus J Sommer, and Steven B Heymsﬁeld. Body compo-

sition by dxa. Bone, 104:101–105, 2017.

[39] Glen M Blake and Ignac Fogelman. The role of DXA bone density scans in the diagnosis and

treatment of osteoporosis. Postgraduate Medical Journal, 83(982):509–517, 2007.

[40] Pengwei Xiao, Tinghe Zhang, Xuanliang Neil Dong, Yan Han, Yufei Huang, and Xiaodu Wang.
Prediction of trabecular bone architectural features by deep learning models using simulated
DXA images. Bone Reports, 13:100295, 2020.

[41] Bin Zhang, Keyan Yu, Zhenyuan Ning, Ke Wang, Yuhao Dong, Xian Liu, Shuxue Liu, Jian
Wang, Cuiling Zhu, Qinqin Yu, Yuwen Duan, Siying Lv, Xintao Zhang, Yanjun Chen, Xiaojia
Wang, Jie Shen, Jia Peng, Qiuying Chen, Yu Zhang, Xiaodong Zhang, and Shuixing Zhang.
Deep learning of lumbar spine x-ray for osteopenia and osteoporosis screening: A multicenter
retrospective cohort study. Bone, 140:115561, 2020.

[42] Arun Krishnaraj, Spencer Barrett, Orna Bregman-Amitai, Michael Cohen-Sfady, Amir Bar,
David Chettrit, Mila Orlovsky, and Eldad Elnekave. Simulating dual-energy x-ray absorptiom-
etry in CT using deep-learning segmentation cascade. Journal of the American College of
Radiology, 16(10):1473–1479, 2019.

[43] Hye Jin Yoo, Young Jae Kim, Hyunsook Hong, Sung Hwan Hong, Hee Dong Chae, and Ja-
Young Choi. Deep learning–based fully automated body composition analysis of thigh CT:
comparison with DXA measurement. Eur Radiol, 2022.

[44] Qiyue Wang, Wu Xue, Xiaoke Zhang, Fang Jin, and James Hahn. Pixel-wise body composition
prediction with a multi-task conditional generative adversarial network. Journal of Biomedical
Informatics, 120:103866, 2021.

12

[45] M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving nonlinear partial dif-
ferential equations. Journal of Computational Physics, 378:686–707, 2019.

[46] Alireza Yazdani, Lu Lu, Maziar Raissi, and George Em Karniadakis. Systems biology in-
formed deep learning for inferring parameters and hidden dynamics. PLOS Computational
Biology, 16(11):e1007575, 2020. Publisher: Public Library of Science.

[47] Naoya Takeishi and Alexandros Kalousis. Physics-integrated variational autoencoders for ro-
bust and interpretable generative modeling. Advances in Neural Information Processing Sys-
tems, 34, 2021.

[48] Gyutaek Oh, Hyokyoung Bae, Hyun-Seo Ahn, Sung-Hong Park, and Jong Chul Ye. Cy-
cleqsm: Unsupervised qsm deep learning using physics-informed cyclegan. arXiv preprint
arXiv:2012.03842, 2020.

[49] Health, aging, and body composition study.
[50] Anne B Newman, Catherine L Haggerty, Bret Goodpaster, Tamara Harris, Steve Kritchevsky,
Michael Nevitt, Toni P Miles, and Marjolein Visser. Strength and muscle quality in a well-
functioning cohort of older adults: the health, aging and body composition study. Journal of
the American Geriatrics Society, 51(3):323–330, 2003.

[51] Bone mineral density in childhood study (BMDCS).
[52] Heidi J Kalkwarf, Babette S Zemel, Vicente Gilsanz, Joan M Lappe, Mary Horlick, Sharon
Oberﬁeld, Soroosh Mahboubi, Bo Fan, Margaret M Frederick, Karen Winer, et al. The bone
mineral density in childhood study: bone mineral content and density according to age, sex,
and race. The journal of clinical endocrinology & metabolism, 92(6):2087–2099, 2007.
[53] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely con-

nected convolutional networks, 2016.

[54] Richard B Mazess, John R Cameron, and James A Sorenson. Determining body composition

by radiation absorption spectrometry. Nature, 228(5273):771–772, 1970.

[55] Richard B Mazess, Howard S Barden, Joseph P Bisek, and James Hanson. Dual-energy x-
ray absorptiometry for total-body and regional bone-mineral and soft-tissue composition. The
American journal of clinical nutrition, 51(6):1106–1112, 1990.

[56] Anders Gotfredsen, Jytte Jensen, Jens Borg, and Claus Christiansen. Measurement of lean
body mass and total body fat using dual photon absorptiometry. Metabolism, 35(1):88–93,
1986.

[57] ANGELO Pietrobelli, Carmelo Formica, ZIMIAN Wang, and Steven B Heymsﬁeld. Dual-
energy x-ray absorptiometry body composition model: review of physical concepts. American
Journal of Physiology-Endocrinology And Metabolism, 271(6):E941–E951, 1996.

[58] DR White, LHJ Peaple, and TJ Crosby. Measured attenuation coefﬁcients at low photon ener-

gies (9.88-59.32 kev) for 44 materials and tissues. Radiation research, 84(2):239–252, 1980.

[59] John Howard Hubbell. Photon mass attenuation and energy-absorption coefﬁcients. The Inter-

national Journal of Applied Radiation and Isotopes, 33(11):1269–1290, 1982.

[60] JH Hubbell. Photon cross sections, attenuation coefﬁcients and energy absorption coefﬁcients.

National Bureau of Standards Report NSRDS-NBS29, Washington DC, 1969.

[61] C.R. Richmond. Task group on reference man to revise ICRP publication 23. Health Physics,

49(5):1015, 1985. Place: United States.

[62] John A Shepherd and Thomas L Kelly. Determining body composition using fan beam dual-

energy x-ray absorptiometry, May 15 2001. US Patent 6,233,473.

[63] Lars Hertel, Julian Collado, Peter Sadowski, Jordan Ott, and Pierre Baldi. Sherpa: Robust

hyperparameter optimization for machine learning. SoftwareX, 12:100591, 2020.

[64] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. PointNet++: Deep hierarchical feature

learning on point sets in a metric space. arXiv, 2017.

13

