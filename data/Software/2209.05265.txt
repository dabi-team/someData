Emulation and History Matching using the hmer
Package

Andrew Iskauskas
Durham University

Ian Vernon
Durham University

Michael Goldstein
Durham University

Danny Scarponi
London School of Hygiene
and Tropical Medicine

Nicky McCreesh
London School of Hygiene
and Tropical Medicine

Trevelyan J. McKinley
University of Exeter

Richard G. White
London School of Hygiene
and Tropical Medicine

Abstract

Modelling complex real-world situations such as infectious diseases, geological phe-
nomena, and biological processes can present a dilemma: the computer model (referred
to as a simulator) needs to be complex enough to capture the dynamics of the system, but
each increase in complexity increases the evaluation time of such a simulation, making it
diﬃcult to obtain an informative description of parameter choices that would be consis-
tent with observed reality. While methods for identifying acceptable matches to real-world
observations exist, for example optimisation or Markov chain Monte Carlo methods, they
may result in non-robust inferences or may be infeasible for computationally intensive
simulators. The techniques of emulation and history matching can make such determina-
tions feasible, eﬃciently identifying regions of parameter space that produce acceptable
matches to data while also providing valuable information about the simulator’s struc-
ture, but the mathematical considerations required to perform emulation can present a
barrier for makers and users of such simulators compared to other methods. The hmer
package provides an accessible framework for using history matching and emulation on
simulator data, leveraging the computational eﬃciency of the approach while enabling
users to easily match to, visualise, and robustly predict from their complex simulators.

Keywords: emulation, history matching, calibration, R.

1. Introduction: emulation and history matching

In many scientiﬁc disciplines, complex computer models, or simulators, are necessary to
comprehend or explore the behaviour of physical systems. Examples range from biological
simulations at the microscopic level (Vernon et al. 2018) to physical simulations of galaxy
formation in the observable universe (Vernon et al. 2010); of particular interest and relevance is
the application of computer simulators to models of infectious disease and epidemics. Despite

2
2
0
2

p
e
S
2
1

]

O
C

.
t
a
t
s
[

1
v
5
6
2
5
0
.
9
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

The hmer Package

their disparate subjects most, if not all, of these models share similar characteristics: by
virtue of the complexity of the system being represented the simulators themselves are often
necessarily complex, requiring a large parameter space and a lot of computational time to
run. If one wishes to use the simulator to match to observed data (for instance to gain insight
into the initial conditions of the formation of a particular galaxy, or the characteristics of a
particular strain of an infection), this can be an arduous task where the region of parameter
space with non-negligible support under the data is small, hard-to-ﬁnd, and often requires an
infeasible number of simulator evaluations. Even if a match to observed data can be found,
we may not be able to determine with any certainty whether the found parameter set is truly
representative of the state of the real-world system due to the various uncertainties present.

We deal with these drawbacks with two methodologies. The ﬁrst, emulation, provides a
means by which we can statistically represent the output of the simulator given a sample
of runs from it. The emulators that we construct are fast to evaluate at unseen parts of
parameter space, and their statistical nature allows us to explicitly and rigorously encode
the uncertainty around predictions at any point in parameter space. This alone can be
useful for understanding the structure of the simulator as well as oﬀering a gateway to a
deep analysis of the uncertainty in a (necessarily imperfect) representation of a physical
system given by the simulator. In order to address the issue of ﬁnding acceptable matches
to observed data, we couple the emulation to a process of ‘history matching’: a means of
iteratively removing unacceptable parts of parameter space so that, by complementarity,
we ﬁnd the full space of acceptable parameter combinations. The conjunction of these two
techniques allows us to explore the parameter space systematically and quickly, minimising the
number of computationally expensive simulator evaluations required to identify all possible
combinations of input parameters that could result in a match to observational data.

The history matching and emulation approach has been employed in a variety of epidemiologi-
cal systems, including for models of HIV (Andrianakis et al. 2017a,b) and the implementation
of anti-retroviral therapy therein (McCreesh et al. 2017a,b), models of tuberculosis and HIV
across multiple countries across the world (Clark et al. 2022), and large-scale agent-based
models of Covid in the UK (Aylett-Bullock et al. 2021; Vernon et al. 2022). Emulation has
also been applied successfully in a range of disciplines beyond epidemiology: for example, in
astrophysics (Higdon et al. 2004; Kaufman et al. 2011; Vernon et al. 2014); climate science
(Castelletti et al. 2012; Williamson et al. 2013; Edwards et al. 2021); engineering (Du et al.
2021); and vulcanology (Gu and Berger 2016; Marshall et al. 2019). However, the statistical
machinery required to eﬃciently and accurately apply emulation to such simulators presents
a barrier to most modelling communities. Techniques such as optimisation, Markov-chain
Monte Carlo (MCMC), or Approximate Bayesian Computation (ABC) can be preferred due
to their ease of implementation; however, these methods typically require large numbers of
simulator evaluations and hence may not be viable, or even computationally tractable, for
high-dimensional spaces. Furthermore, such techniques may not ﬁnd the full space of accept-
able matches, making robust inference diﬃcult or even impossible. The creation of the hmer
package is designed to remove the conceptual barrier that precludes modellers from using
emulation and history matching, providing accessible functionality to perform the history
matching procedure while giving the user the ﬂexibility to interact with the mathematical
structure as much as they see ﬁt.

There are a number of options in R that perform emulation on complex models, the majority
of which focus on Gaussian Process (GP) emulation. Notable examples are emulator (Hankin

Andrew Iskauskas et al.

3

2021a) and its successor multivator (Hankin 2021b), stilt (Olson et al. 2018) and RobustGaSP
(Gu et al. 2022). While GP emulation is a powerful tool, it can be challenging for a user
to adequately specify the full Bayesian prior structure for such an emulator and it includes
distributional assumptions that can be hard to justify.
In addition, these packages focus
(understandably) on the emulation of simulator outputs and not on the tools that would
aid a user in understanding and visualising the structure of their simulator via emulation.
They also do not leverage the power of history matching, a key tool when the principal
expected usage of emulation is to ﬁnd acceptable ﬁts to data arising from complex simulators
of complex real-life processes. In contrast, due to the more ﬂexible framework and simpler
prior speciﬁcations used, hmer may be a more accessible tool for emulation and, coupled
with the power of history matching, provides a more straightforward means of approaching
complex calibration problems using emulation.

The structure of this paper is as follows. In Section 2 we brieﬂy outline the fundamentals
of emulation and history matching.
In Section 3 we detail the functionality of the main
components of the hmer package, along with their arguments. In Section 4 we apply the core
functionality, along with some of the visualisation tools, to a toy model to demonstrate its
usage. In Section 5 we discuss one of the more advanced techniques available to an expert
user: namely, variance emulation. Finally, in Section 6 we conclude and discuss considerations
that should be made during the process of emulation.

2. Bayes linear emulation and history matching

In this section we brieﬂy outline the mathematical framework of Bayes linear emulation, as
well as the algorithmic description of history matching. Multiple works exist that provide
a fuller description of emulation; see, for instance, Craig et al. (1997); Vernon et al. (2018);
Santner et al. (2003); Bowman and Woods (2016).

2.1. Emulation

Suppose we have a simulation of a real-world process which takes a set of input parameters,
described as a vector x of length d, and returns a set of m outputs {fi(x)}i=1,...,m. A (univari-
ate) Bayes linear emulator is a fast statistical approximation of the simulator output, built
using a comparatively small set of simulator runs, which provides an expected value for the
simulator output at any point x along with a corresponding uncertainty estimate reﬂecting
our beliefs about the uncertainty in the approximation.
Concretely, we may create a prior representation of a simulator output fi(x) in emulator form
as

fi(x) =

pi
X

j=1

βijgij(xAi

) + ui(xAi

) + wi(x).

(1)

Here, xAi
, Ai ⊆ {1, . . . , d}, are the set of ‘active variables’ for output fi(x); that is, the
components of x that are most inﬂuential in determining the behaviour of fi(x). The gij are
, with the βij the corresponding coeﬃcients; together these
pi known simple functions of xAi
)
two terms deﬁne a regression surface encoding the global behaviour of the output. ui(xAi
and can
is a second-order weakly stationary process which captures residual variation in xAi

4

The hmer Package

be seen as governing the local behaviour of the simulator output. We make the assumption
) is zero-mean and a priori uncorrelated with the regression coeﬃcients βij. We
that ui(xAi
further assume the following covariance structure for ui(xAi

):

COV[ui(xAi

), ui(x0
Ai

)] = (1 − δi)σ2

i c(xAi, x0
Ai

),

where c(x, x0) is a suitable correlation function (common examples can be found in Rasmussen
2003, Ch. 4) and δi ∈ [0, 1]. The ‘nugget’ term, wi(x), represents the eﬀects of the remaining
‘inactive’ inputs; we again assume this is zero-mean and uncorrelated to βij and ui(xAi
), and
that

COV[wi(x), wi(x0)] = δiσ2

i Ix=x0,

(2)

where Ix=x0 is an indicator function with Ix=x0 = 1 if x = x0 and 0 otherwise.
Before we can update the emulated structure with respect to data, we need to complete the
a priori speciﬁcation for the random quantities βij, ui(xAi
), and wi(x). This can be done in
a variety of ways: for instance, if one is willing and able to specify full distributions for these
quantities, we could then use maximum likelihood or maximum a posteriori (MAP) estimates
to determine plug-in estimates for their hyperparameters (Andrianakis and Challenor 2012),
or use cross-validation (Maatouk et al. 2015). We may not be able (or willing) to make full
distributional speciﬁcations for these quantities, whether due to a lack of prior knowledge re-
quired for such a speciﬁcation or a lack of faith in any such speciﬁcation. It is rare, however,
that we have similar reservations about the second-order speciﬁcation of such a system (that
is, expectations and covariances), and the Bayes linear framework requires only these quanti-
ties. We therefore leverage this framework and require, with the assumptions listed already,
speciﬁcation of the expectation and covariance of the regression coeﬃcients E[β] and VAR[β],
as well as the quantities that furnish the covariance structure of ui(x) and wi(x); namely δi,
σi, and the hyperparameters of the correlation function c(x, x0). These can be determined
using a full a priori speciﬁcation or by using pragmatic plug-in estimates (Santner et al. 2003;
Kennedy and O’Hagan 2001; Rasmussen 2003). With these quantities deﬁned, we are in a
position to update our knowledge in light of data using the Bayes linear framework, which
we now describe.

Let us imagine that we have a collection of runs obtained from running the simulator at a
series of points (x(1), x(2), . . . , x(n)), resulting in a collection of simulator outputs

Di = (cid:16)

fi(x(1)), . . . , fi(x(n))(cid:17)

.

The Bayes linear emulator output for fi(x) at a new point x is given by the Bayes linear
update formulae (Goldstein and Wooﬀ 2007):

EDi
VARDi

[fi(x)] = E[fi(x)] + COV[fi(x), Di]VAR[Di]−1(Di − E[Di]),
[fi(x)] = VAR[fi(x)] − COV[fi(x), Di]VAR[Di]−1COV[Di, fi(x)],

(3)

(4)

as well as the covariance between the outputs at two points x, x0

COVDi

[fi(x), fi(x0)] = COV[fi(x), fi(x0)] − COV[fi(x), Di]VAR[Di]−1COV[Di, fi(x0)].

Andrew Iskauskas et al.

5

The emulator expectation given by (3) provides a prediction for f (x) at an unevaluated point
x, while the emulator variance provides us with the corresponding uncertainty of this predic-
tion. Both of these quantities are extremely fast to evaluate, requiring little more than matrix
multiplication, so the emulators allow an extensive exploration of the function’s behaviour
over the input space. We may brieﬂy note that, were we to assume normal and Gaussian pro-
cess priors for β and u(x), respectively, then the approach here is almost identical to Gaussian
process emulation (Conti et al. 2009). However, Gaussian process emulation requires invoking
additional distributional assumptions that may be diﬃcult to justify, force stricter and more
complicated diagnostics to be satisﬁed, and fundamentally aﬀect the ﬁnal inference. This
demonstrates the advantage of Bayes linear emulation: in requiring only second-order spec-
iﬁcations, we are somewhat removed from the associated diﬃculty and pitfalls inherent in a
distributional approach. Finally, the simplicity of the Bayes linear framework allows tractable
consideration of more complex applications of emulation, including multi-level or hierarchical
emulation (Cumming and Goldstein 2009).

Despite its simple structure, it can still be diﬃcult for a model expert to make the second-
order determinations with reasonable certainty.
In Section 3.3 we discuss how the hmer
package creates the prior speciﬁcations and determines hyperparameters (for example those
in the correlation function c(x, x0)), in order to create robust emulators.

Diagnostics on Bayes linear emulators

Simplicity aside, it remains paramount that the emulators are trained on a set of points in
the input space that are suﬃcient to describe the space and the variability of the simulator
across the space. To that end, having performed the Bayes linear update we must perform
validation diagnostics on the trained emulators. A variety of tests can be performed; the most
straightforward is to calculate standardised prediction errors

Ui(x) = fi(x) − EDi
VARDi

[fi(x)]
[fi(x)]

q

(5)

for a ‘holdout’ set of simulator runs. A number of other diagnostics are available: see, for
example, Bastos and O’Hagan (2009). Emulators that perform poorly on diagnostics are not
suitable for inference, which could indicate poorly speciﬁed prior beliefs, an erratic simulator
output which would require more runs to adequately represent, or simply an output that
cannot be reliably emulated over the current range of parameters. A collection of tools to
ensure that the emulators trained by hmer are suitable for inference are included in the
package, and we detail the main such tools in Section 3.4.

2.2. The implausibility measure and history matching

One of the main advantages of the emulation framework described above is speed compared
to the simulator that it approximates. An emulator evaluation at a previously unseen point
is often orders of magnitude faster than the complex simulator from which it is derived. We
can therefore use an emulator to intuit the structure and behaviour of the simulated output
over the parameter space, without resorting to large numbers of computationally expensive
simulator runs. The statistical nature of the emulators means that any such intuition is made
in the presence of the uncertainty of the emulator at a given point, and this must be taken into

6

The hmer Package

account. However, a common aim when using complex simulators is to answer the following
question:

Given observed data corresponding to a simulator output, what combinations of input param-
eters could give rise to output consistent with this observation?
Of course, there are a variety of approaches to matching complex simulators to data, from
optimisation to Monte Carlo methods such as Data-augmented Markov chain (DA-MCMC)
or Sequential (SMC) methods (Gibson and Renshaw 1998; O’Neill and Roberts 1999; Jewell
et al. 2009), to Approximate Bayesian Computation (ABC) (McKinley et al. 2018, 2009;
Toni et al. 2009), to full Bayesian inference (Kennedy and O’Hagan 2001). The beneﬁts and
drawbacks of each are varied; primary in our consideration here is that in any technique
we apply we should best leverage the structure of our emulators to ﬁnd the full space of
acceptable matches. To that end, we use the history matching approach (Craig et al. 1997),
which aims to ﬁnd the space of acceptable parameter sets via complementarity. For further
discussion and comparison with other approaches, see Vernon et al. (2010); McKinley et al.
(2018); Vernon et al. (2018). We ﬁrst must consider the link between an observation of a
real-world process to an emulator before we describe the history matching procedure.

Let us denote the real physical process which one output of the complex simulator represents
by yi. Observations of this physical process are almost certainly made imperfectly (for exam-
ple, in epidemiological models it is common to suspect that case numbers for a disease are
subject to under-reporting issues). We link the observation zi to the physical process via

zi = yi + ei,

where ei is a random quantity reﬂecting the uncertainty about the accuracy of our observation.
Similarly, we should not expect that the simulator is a perfect representation of the real-life
process it represents; the simulator output fi(x) can be linked to the physical process via

yi = f (x?
i

) + (cid:15)i,

where (cid:15)i, the structural model discrepancy, is a random quantity representing our uncertainty
about the imperfections of the simulator (Goldstein et al. 2013; Brynjarsdottir and O’Hagan
2014; Vernon et al. 2010). Here we follow the “best input” approach, which suggests that
there exists a value x? which best represents the real physical system (Goldstein and Rougier
2006). We already have a well-deﬁned means by which to link the emulated output to the
simulator, at least to second-order, due to the inherent uncertainty quantiﬁcation that the
emulator provides. This set of uncertainties provides a chain allowing us to link the emulator
prediction directly to the observation. We frequently assume that ei and (cid:15)i are zero-mean,
constant variance quantities that are uncorrelated with each other and with the emulated
output; the generalisation is straightforward in the equations that follow.

This uncertainty structure allows us to approach the problem of matching in a markedly
diﬀerent way to, e.g., optimisation. Rather than seeking points whose simulator outputs
are likely to be good ﬁts to the observational data, we instead focus on removing parts of
parameter space that are highly unlikely to give rise to good ﬁts, even accounting for the
combined uncertainties linking the observation to the emulator. By systematically removing
bad parts of parameter space and improving the emulator predictions over the remaining ‘non-
bad’ parameter space, we can arrive by complementarity at the complete space of acceptable

Andrew Iskauskas et al.

7

parameter combinations relative to the observational data.
Concretely, we deﬁne an implausibility measure (Vernon et al. 2014) for observation zi

I 2
i

(x) =

VARDi

(EDi
[fi(x)] − zi)2
[fi(x)] + VAR[ei] + VAR[(cid:15)i] .

If I(x) is ‘large’∗ then we may think it unlikely that we would obtain an acceptable match to
observed data were we to run the simulator at the point x; in this case, we term the point x
implausible. Conversely, if I(x) is ‘small’ then we cannot rule out the possibility that x would
give rise to a good match to data; x is deemed non-implausible, or not-yet-ruled-out (NROY).
Note that a point can be deemed non-implausible either because the emulator expectation
E[fi(x)] is close to the observation zi, suggesting a good ﬁt, or because the uncertainties
(particularly VAR[fi(x)]) are large, suggesting that investigation of that part of parameter
space using additional simulator runs would allow us greater insight into the structure of
the non-implausible space. A point is deemed implausible only if, even accounting for the
uncertainties in the simulator, observation, and emulator, a simulator evaluation at this point
is very unlikely to provide a match to the observational data.
For multiple outputs, say m, we may also deﬁne a combined implausibility measure; a natural
approach is to require all implausibility measures across all outputs to be satisﬁed, resulting
in a maximum implausibility measure:

IM (x) = max

i

{Ii(x)}i=1,...,m.

Other obvious, and less restrictive, measures follow immediately: the second-maximum im-
plausibility is deﬁned as

I2M (x) = max

i

{Ii(x) \ IM (x)}

and so on. Other options are available - for example, the deﬁnition of a multivariate im-
plausibility measure (Vernon et al. 2010) which can capture some of the correlations between
outputs; these will be included in a subsequent update of the hmer package.
The history matching algorithm proceeds as follows. We apply a series of iterations, called
waves, which discard regions of the parameter space at each wave. At wave k, a set of
emulators are constructed for a collection of outputs, Qk, based on a representative sample
of points and their corresponding simulator evaluations from wave k − 1 †. These emulators
are used to assess implausibility over the space, Xk, that remained at wave k − 1, discarding
those regions deemed implausible to produce representatives of a smaller parameter space,
Xk+1. These points in turn inform the emulators at wave k + 1 and so on. The algorithm is
laid out below.

∗The question of what we mean by ‘large’ or ‘small’ is a good one. We may appeal to Pukelsheim’s 3σ rule
(Pukelsheim 1994), which suggests that for a unimodal continuous distribution 95% of the probability mass is
within 3σ of the mean, to suggest a good starting point for a cut-oﬀ is I = 3.

†Note that Qk need not be the entire set of outputs of interest — especially at early waves, we may select
an informative (and straightforward to emulate) subset of them with a view to including others at later waves.
The complementarity of the history matching procedure ensures that this is a valid approach, and allows us to
initially focus on ‘stable’ outputs governed by large-scale behaviour across the parameter space (Vernon et al.
2018).

8

The hmer Package

1. Let X0 be the initial domain of interest and set k = 1;

2. Generate an appropriate design for a set of runs over the non-implausible space Xk−1;

3. Identify the collection of informative outputs, Qk, and obtain the corresponding simu-

lator evaluations by running the simulator for this design of points;

4. Construct new, more accurate, emulators deﬁned only over Xk−1 for the collection Qk;

5. Use the emulators to calculate implausibility across the entirety of Xk−1, discarding

points with high implausibility to deﬁne a smaller non-implausible region Xk.

6. If any of our stopping criteria have been met, continue to Step 7. Otherwise, repeat the

algorithm from Step 2 for wave k + 1.

7. Generate a large number of acceptable runs from Xk, sampled appropriately.

By construction, it is the case that Xk ⊆ Xk−1 ⊆ · · · ⊆ X0. Thus the history matching proce-
dure iteratively removes parts of parameter space that are obviously unsuitable, allowing us
to create emulators that are more conﬁdent in the remaining region of interest, and eventually
leaving us with a well-understood, complete, non-implausible region. This approach allows
us to consider only collections of outputs, Qk, at each wave in a way that other methods
do not: since we are removing unsuitable regions of parameter space rather than focusing
on suitable regions, the absence of some outputs in a wave does not aﬀect the validity of
the points chosen. Particularly at early stages of the history matching procedure, when the
parameter space can be large and contain corners of the space where the simulator behaviour
is unstable, it is very useful to be able to omit outputs that cannot be satisfactorily emulated.
Once we have reduced the parameter space to a point where such an output can be reliably
emulated, we can reintroduce that output, with no deleterious consequences on our inference
about the non-implausible space.

The stopping condition touched on in Step 6 depends on the outcome of the waves and of
the eventual aim of the history match. One common stopping condition is if the emulator
uncertainties are small compared to any other uncertainties in the system; if VAR[fi(x)] is
small in contrast to VAR[ei] and VAR[(cid:15)i], then additional waves of emulator training will not
remove any more parameter space and hence are not worth performing. Alternatively, the
waves of history matching can proceed to a point where Xk = ∅; this is informative, suggesting
a conﬂict between the simulator and the real-world observation, but there is obviously no
advantage to performing additional waves to ‘further’ reduce this space. Finally, we may
wish to ﬁnd matches to observational data in order to reliably infer the properties of other
simulated quantities, and depending on the application we may be satisﬁed if M matches are
obtained; in which case, it would be reasonable to stop the history match once we obtain the
desired M parameter sets (subject to considerations about the distribution of those parameter
sets).

The history matching algorithm, coupled with emulation, provides a powerful toolkit for eﬃ-
ciently exploring the output of complex simulators in high-dimensional parameter space. The
emulators allow us to predict the simulator response at points in parameter space in a fraction
of the time a simulator evaluation would take, with the addendum that any such prediction
comes with uncertainty; the history matching approach takes advantage of the speed of eval-
uation and the uncertainty in prediction to systematically, rigorously and comprehensively

Andrew Iskauskas et al.

9

remove parts of parameter space that cannot result in an acceptable match to data. In so
doing, this approach provides an obvious framework in which we can consider the sources of
uncertainty in the simulator as well as that arising from the emulator and the observation.

One caveat appears in Step 2 of the history matching algorithm, where we specify that an
‘appropriate’ design of currently non-implausible points is required to train the emulators. By
‘appropriate’, we mean that the points chosen are space-ﬁlling over Xk, thus mitigating any
problems the emulators may have in determining the regression surface or the uncertainty
therein (Santner et al. 2003). A main aim of the hmer package is to construct an accessible
framework for performing the history matching and emulation procedure, and to this end
ensuring that a representative sample can be elicited at each wave without recourse to user-
deﬁned approaches and the concomitant complications this brings. The function detailed in
Section 3.5, along with its siblings, allow a user to step through the emulation and history
matching process with minimal diﬃculty or external speciﬁcation of priors, and with no
recourse to distributional assumptions that would complicate or undermine any ﬁnal inference.

3. The hmer package: fundamentals

hmer is available from the Comprehensive R Archive Network (CRAN) at https://CRAN.
R-project.org/package=hmer and can be installed and loaded in the usual manner:∗

> install.packages("hmer")
> library("hmer")

The hmer package, at its core, is centred around one R6 object Emulator (itself dependent
on a Correlator object) and three functions that act upon it, corresponding to emulator
construction (emulator_from_data()), emulator validation (validation_diagnostics())
and new point proposal (generate_new_runs()). We detail each in turn, its use, and the
arguments provided to it. The functions detailed below have a variety of optional arguments
that can be passed to it; we do not detail all of them but highlight those that are necessary
to explain the default behaviour of the function. The documentation for each function details
the optional arguments and their usage.

A note on parameter sets

In what follows, we frequently refer to collections of parameter sets. The structure of these
inputs is important to bear in mind; the Emulator object and all associated functions assume
that parameter sets are provided to them as a data.frame whose rows are individual param-
eter sets, with named columns corresponding to the names of the components of a parameter
set. This is crucial for the correct determination of active variables as well as distinguishing a
single d dimensional parameter set from d one dimensional parameter sets. Hence, for a pair
of parameter sets x1 = (1, 1), x2 = (2, −1) with components rate1, rate2, correct syntactical
expressions for it are

> points <- data.frame(rate1 = c(1, 2), rate2 = c(1, -1))
> # Standard usage and ordering

∗The development version of the package is located on GitHub (https://github.com/andy-iskauskas/

hmer)

10

The hmer Package

> points <- data.frame(rate2 = c(1, -1), rate1 = c(1, 2))
> # hmer automatically arranges columns before operations

whereas the following will not work when passed to an Emulator function, due to the lack of
named columns:

> # The below has the same issue if coerced to data.matrix or data.frame
> points <- matrix(c(1, 1, 2, -1), nrow = 2, byrow = TRUE)
> # "dim(X) must have positive length"

3.1. The Correlator object
The Correlator object is an R6 object which performs the function of u(xAi
with unit variance. Its constructor takes three arguments:

) + wi(x), except

> Correlator$new(corr, hp, nug)

where corr is a string corresponding to the name of the desired correlation function, hp is
a named list of hyperparameters for the chosen correlation function, and nug is the size of
the nugget term (equivalent to δi in the emulator speciﬁcation (2)). The default Correlator
object is initialised as if by Correlator$new("exp_sq", list(theta = 0.1), nug = 0).
The type of the ﬁrst argument allows us to specify correlation functions either from a list of
those available: in the hmer package are exponential-squared "exp_sq", Matérn "matern",
Ornstein-Uhlenbeck "orn_uhl", and rational quadratic "rat_quad". The Correlator ob-
ject searches for an available function whose name matches the string provided to the corr
argument, allowing a user to deﬁne their own correlation function user_func(...) and pro-
vide it to the Correlator object. The requirements of a user-deﬁned function are detailed
in the help ﬁle ?Correlator. This approach to the emulator correlation structure allows
for consistent hyperparameter estimation across any conceivable correlation functions within
emulator_from_data().
A Correlator object possesses a print() method, getters and setters for the hyperparame-
ters, and most importantly a function that returns the correlation between two collections of
points, get_corr().

3.2. The Emulator Object

The Emulator object is the central object in the hmer package. An individual emulator is
an R6 object: it is infrequently used directly but the constructor function for an Emulator
object is

Emulator$new(basis_f, beta, u, ranges, ...)

The details of each of these arguments is as follows:

• basis_f: a list of basis functions equivalent to {gi(x)}. At a minimum, this should
include the constant function function(x) 1, and the functions should be designed
for instance, the basis functions gi(x) : R2 → R3,
to act on a vector of numerics:
(x, y) 7→ (1, x, y) can be entered as

Andrew Iskauskas et al.

11

> bf <- c(function(x) 1, function(x) x[[1]], function(x) x[[2]])

• beta: This provides the second-order speciﬁcations for the regression coeﬃcients β.
This consists of a named list list(mu = ..., sigma = ...) corresponding to the
expectation vector and variance matrix for β, respectively. In the above example, we
may have

> b <- list(mu = c(0.1,-1, 1), sigma = matrix(0, nrow = 3))

were we to expect that the regression surface has the form 0.1 − x + y with no variance.

• u: This represents the correlation structure, whose argument is a list list(corr = ...,
sigma = ...), where corr is the Correlator object and sigma the standard deviation
given by pVAR[ui(x)]; for the above example we could decide on a ‘default’ correlation
structure with variance 4, giving

> u <- list(corr = Correlator$new(), sigma = 2)

• ranges: A list of ranges of the parameters. This is required for a variety of reasons,
not least because the Emulator object internally scales all points passed to it so that
they are in the range [−1, 1], as well as to determine the range for proposing points for
later waves. When provided to the Emulator, they should be in a named list where
each element is a pair of numerics, representing the lower and upper bounds of the
parameter.

> range <- list(x = c(-1, 1), y = (2, 4))

There are a number of optional arguments than can be supplied to the Emulator object:
a data.frame named data if we wish to manually provide training runs to the emulator; a
string out_name which identiﬁes the output we are emulating; a vector of booleans a_vars (of
length equal to the number of parameters) which determines which variables are active; and
a list of two numerics discrepancies(internal = ..., external = ...) which encode
the model discrepancies. All of these quantities can be provided with expert prior knowl-
edge of the output, though in many cases this is not possible; we would instead rely on the
emulator_from_data() function described in Section 3.3 to supply these details for us.
As an R6 object, a constructed Emulator inherits the normal clone() function as well as
possessing a custom print() and plot() statement. It also has a number of functions which
allow the Bayes linear adjustment to be performed, and posterior predictions to be obtained.
The function adjust(data, out_name) performs the Bayes linear adjustment relative to
the data D provided in data; the functions get_exp(points) and get_cov(points, ...)
perform the duty of calculating the posterior expectation and covariance matrix for a collection
of parameter sets passed to them; implausibility(points, target) gives the implausibility
at a collection of points relative to an observational target value. The usage of these functions
will be demonstrated in Section 4.

3.3. Training emulators with emulator_from_data()

Often it is not practical or time-eﬃcient to manually determine the prior speciﬁcations for an
emulator, especially in situations where we wish to emulate many outputs from our simula-
tor. Instead, we may provide the data from the simulator to emulator_from_data(), which

12

The hmer Package

furnishes us with a list of Emulator objects appropriately speciﬁed. The minimal syntax
of emulator_from_data(), allowing it to make its best determinations about the emulator
structure, is

> emulator_from_data(input_data, output_names, ranges)

Given a data.frame consisting of all parameter sets and their corresponding simulator out-
puts, the steps performed by this function for a single output fi(x) are as follows.

1. All input parameters are scaled to the range [−1, 1], using the ranges provided. This
guarantees orthogonality of linear and quadratic terms in the putative basis functions.

2. The regression surface is determined: basis functions in the parameters up to quadratic
order (with interaction terms) form the candidate space for the collection gij(x). The
relevant subset is decided upon by either stepwise add or stepwise delete, as appropriate
to the dimensionality of the parameter space compared to the number of simulator runs
available. The ﬁnal collection of terms is examined and thinned based on the proportion
of variance explained by each term. The coeﬃcients of the regression surface and their
variance matrix are also collected. By default, in the presence of reasonably large
numbers of simulator evaluations, we assume a separation between ‘global’ and ‘local’
behaviour - once determined, the regression coeﬃcients are considered ‘known’, so that
VAR[β] = 0. This avoids identiﬁability issues between the regression surface and the
weakly stationary surface ui(x) and can help with our physical understanding of the
simulator behaviour. Other options are supported:
for instance, we could go to the
other extreme and assume a non-informative prior for the regression coeﬃcients, or
take some compromise in between these two choices (Santner et al. 2003).

3. The basis functions determined above are used to deﬁne the collection of active variables,
. A variable is considered active if it contributes to a basis function: parameter xj
, or if it

xAi
is considered active for fi(x) if one of the basis functions for fi(x) is xj or x2
j
appears in an interaction term xixj.

4. The residuals of the regression surface are determined from the above, and the speciﬁca-
tions of the correlation structure are determined via a bounded maximum a posteriori
(MAP) estimate (Sorenson 1980). Unlike in traditional maximum likelihood estima-
tion, we do not believe in the existence of a ‘true’ value for the hyperparameters in
the correlation structure, only reasoning that the correlation length should be no larger
than the greatest distance between roots of polynomial one order higher than the ﬁtted
regression surface, and no smaller than the average distance away from those same roots
(Vernon et al. 2010). This is akin to viewing the contribution of the correlation struc-
ture as a third-order correction to the quadratic regression surface. The hyperparameter
estimation includes an estimate of δi.

5. The prior emulator for fi(x) is created using the above speciﬁcations via a call to
Emulator$new(...), and then the Bayes linear update formulae are applied to obtain
the trained emulators via em$adjust().

Andrew Iskauskas et al.

13

For multiple outputs, this process is applied in the expected fashion: we ﬁt regression surfaces
to each output in turn, use their residuals to deﬁne the speciﬁcations for each correlation
structure, then perform Bayes linear adjustment on each.
One thing to remember is that emulator_from_data() (and its descendants, one of which we
discuss in Section 5) will always return a named list of Emulator objects, even in the event
that we wish to train to only one output. This ensures compatibility with functions in the
package. A particular emulator can be accessed using the usual syntax for named lists; for
example, in a list of three emulators emulating outputs y1, y2, y3, we may access the emulator
for y2 via either of the following commands:

> emulators[[2]]
> emulators$y2

The emulator_from_data function can be customised in almost any conceivable way:
for
details of the arguments that can be provided to it one may consult the associated help ﬁle.
However, in the absence of any strong prior knowledge about the behaviour of the simulator
outputs, the default behaviour is often appropriate to train acceptable emulators to the known
data.

3.4. Validation of emulators: validation_diagnostics()

Having obtained emulators for our series of outputs, we must ensure that they are suitable for
predicting their outputs over the current non-implausible space. As previously mentioned, a
variety of diagnostic tests are possible, but the function validation_diagnostics() collects
three of the most common such into one summary.

> validation_diagnostics(emulators, targets, validation, ...)

Here, emulators is the list of emulators to validate, targets is the set of observations that
we wish to match to, and validation is a set of validation points: parameter sets for which
we have simulator runs which were not provided to the emulators during training. The only
mandatory argument is emulators: the omission of the argument targets precludes us from
performing one of the diagnostic tests and modiﬁes the results of the others; if validation
is not provided then cross-validation is performed using the emulators’ training runs (the
default being leave-one-out cross-validation). For ease of notation, we refer to the set of runs
upon which the emulators are tested as the ‘validation set’, whether or not they comprise a
hold-out set or whether cross-validation has been performed.

The diagnostic tests are as follows:

• The emulator predictions over the validation set are compared to the corresponding
results from the simulator, accounting for the uncertainty. A ‘perfect’ emulator would
[fi(xj)] = fi(xj); as the emulators are statistical approximations with a well-
have EDi
deﬁned uncertainty structure we instead view an emulator prediction at a point xi as
satisfactory if, for a suitable choice of c ∈ R,

fi(xj) ∈

(cid:20)
EDi

[fi(xj)] − c

VARDi

[fi(xj)], EDi

[f (xj)] + c

VARDi

q

(cid:21)
[fi(xj)]

.

q

14

The hmer Package

For example, c = 3 suggests that we deem a prediction satisfactory if the true simulator
evaluation lies within three standard deviations of the emulator prediction. If large num-
bers of points do not satisfy this requirement, it suggests a conﬂict between emulator
and simulator, and warrants investigation - especially if the emulator is systematically
over- or under-estimating the simulator output. The inclusion of the targets argu-
ment in validation_diagnostics() restricts the consideration of bad predictions to a
neighbourhood of the observations, reﬂecting the fact that we are not overly concerned
by bad diagnostics in parts of the input space far from where the simulator itself would
agree with the data.

• The emulator implausibility is compared to the ‘simulator implausibility’, which we

deﬁne in a similar vein for a parameter set xj:

I 2
sim

(xj) =

(fi(xj) − zi)2
VAR[ei] + VAR[(cid:15)i] .

Let Iem(xj, c) be the membership function for the emulator implausibility, Iem(xj, c) = 1
if Ii(xj) ≤ c and 0 otherwise, and similarly for the membership function for the simulator
implausibility Isim(xj, c). Then we have three possibilities.

1. Iem(xj, c) = Isim(xj, c): the emulator and simulator both agree on the categorisa-
tion of the point xj as implausible or non-implausible. This is the ideal situation;
2. Iem(xj, c) = 1 and Isim(xj, c) = 0: the simulator would rule the point xj out as
implausible while the emulator would not. This is to be expected, by virtue of the
additional uncertainty in the emulator predictions: the emulator may not rule out
all points that the simulator does but we anticipate that at later waves, when the
parameter space has been reduced, a more accurate emulator will rule out xj.
3. Iem(xj, c) = 0 and Isim(xj, c) = 1: the simulator deems xj non-implausible but the
emulator does not. These points are worthy of consideration, as it suggests that
the emulator could reject parts of parameter space that should not be rejected.
The location of these points should be examined and the emulator may need to be
modiﬁed, or more training runs in the neighbourhood of xj should be performed,
in order to ensure that the emulator predictions do not unjustiﬁably remove parts
of parameter space.

Due to the fact that the calculation of implausibility requires us to have observational
data zi, this test cannot be run if validation_diagnostics() is not provided a targets
argument.

• The standardised prediction errors, Ui(xj), described in (5) are calculated for each
point in the validation set. Large standardised errors may suggest conﬂict between
the emulator and simulator predictions; in most cases we may appeal to Pukelsheim
once more and consider errors larger than 3 to be of interest. At the same time, these
standardised errors can indicate under-conﬁdence or overﬁtting, if all of the errors are
very small (i.e., less than 1). Broadly speaking, we would expect that the errors would
be unimodally distributed somewhere around 0 with moderate extent.

Andrew Iskauskas et al.

15

The validation_diagnostics() function, by default, calculates all three of these measures
for each emulated output fi(x), plotting the results of the diagnostics (and highlighting prob-
lematic points) for ease of analysis, returning a data.frame of parameter sets which failed
one or more diagnostics. We will see this in action in Section 4.

3.5. Proposal of new points using generate_new_runs()

Suppose we have trained a set of emulators, and by recourse to their diagnostics are satis-
ﬁed that they can suﬃciently emulate their outputs over the parameter region of interest.
We now wish to use these emulators to determine the new non-implausible region. As men-
tioned in Section 2.2, to successfully train a set of emulators for the next wave we require
an ‘appropriate’ design over the non-implausible space: ideally it should be space-ﬁlling and
uniform across the space. For most (if not all) applications, a direct parametrisation of the
non-implausible space is impossible, so we cannot simply deﬁne a membership function for
the non-implausible space Xk at wave k. Instead, we use a variety of methods to try to ensure
that we understand the boundary of Xk as well as suitably populate the interior of the space.
The minimal speciﬁcation for the function generate_new_runs() is

> generate_new_runs(emulators, n_points, targets)

The emulators and targets arguments are the same as that of validation_diagnostics();
the n_points argument simply indicates how many points we wish to generate from the non-
implausible space X . The normal behaviour of the function employs the following techniques:

1. A large Latin hypercube design (LHD) (McKay et al. 2000) is generated across the
minimum enclosing hyperrectangle (MEH) of the non-implausible space X . The points
in the design are rejected if their implausibility exceeds our cut-oﬀ.

2. The points that remain are pairwise-selected (with greater weight given to those points
which have larger separation) and points are sampled along a ray containing the pair
of points which extends to the edge of the MEH. Implausibilities along the ray are
calculated, with points being retained if they themselves are non-implausible and either
neighbouring an implausible point or at either end of the ray. The aim of this step is to
identify points on or close to the boundary of the non-implausible space, including any
internal boundaries.

3. The remaining points from the LHD, supplemented by the boundary points, are used as
a prior for a mixture distribution of uniform ellipsoids, where each ellipsoid is centred
at a non-implausible point and with radii which are determined via a burn-in phase.
Points are sampled from this distribution via importance sampling, in order to ﬁll out
the space.

4. The space is resampled: the complete set of proposal points are thinned to half of the
number desired, according to a maximin argument. Steps 2 and 3 are performed again.

5. We apply thinning once more to obtain n_points points with a maximin argument.

16

The hmer Package

We can see a diagrammatic demonstration of these steps in Figure 1, excepting Step 4 (the
resampling step). For this, a contrived example of a non-implausible space was deﬁned: the
space is a cardioid with a hole removed from inside it. We can see that the line sampling step
(Step 2) picks up both the external and the internal boundaries, as we would expect.

Figure 1: From top-left to bottom-right, the steps in generate_new_runs, applied to an artiﬁcial
example of a non-implausible space: Latin hypercube sampling with rejection; line sampling, uniform
ellipsoid importance sampling, and a ﬁnal thinning stage. Points in grey are rejected points (in the
case of Latin hypercube sampling and line sampling) or the initial set of points deﬁning the mixture
distribution (for importance sampling).

The steps above can eﬃciently provide a uniform sample from a non-implausible space, while
also giving us adequate coverage on the boundary of the space. Were we to train emulators
on the proposed points, the boundary points therein will aid in the learning of the regression
terms while the points in the interior will aid in the learning of the weakly stationary process.

4. Example: Application to an SIRS Model

We now use the core functions and objects described in Section 3 on a simple toy SIRS epi-
demic model, representing movement through a system where an individual can be (S)usceptible
to the infection, (I)nfected with it, or (R)ecovered from it. Recovered individuals can return
to the Susceptible state, representing a loss of immunity over time. While simple, it will suﬃce
to demonstrate the functionality we wish to show. The model in question can be described
by the following diﬀerential equations:

Andrew Iskauskas et al.

17

dS
dt
dI
dt
dR
dt

= αSRR −
= αSI SI

αSI SI
S + I + R

,

− αIRI,

S + I + R
= αIRI − αSRR.

The model has three states S, I, and R representing susceptible, infected, and recovered
individuals; three input parameters, αSI , αIR and αSR representing rate of infection, recovery,
and waning immunity respectively; and we will focus on three outputs nS ≡ S(10), nI ≡
I(10), nR ≡ R(10), namely the numbers of susceptible, infected, and recovered individuals at
time t = 10. The model is initialised at t = 0 with S(0) = 950, I(0) = 50 and R(0) = 0. The
fact that this model is low-dimensional and consists of relatively uncomplicated diﬀerential
equations means that we could generate a vast number of simulator evaluations and forgo
emulation entirely: this is seldom the case in real-world applications. For our purposes, the
speed of the simulator here will serve as a good check on the quality of the results provided
by emulation.

4.1. Initial simulator runs and emulator training

The code required to run this example, and generate the plots therein, is included in the Sup-
plementary Material to this paper. To set up the ﬁrst wave of history matching, we require a
set of simulator runs across the full parameter space in question. One option would be to cre-
ate a Latin hypercube design with the desired number of points before putting them into the
simulator function: sample functions for doing so can be found at the top of the Supplemen-
tary Material (namely the functions ode_results(), get_res(), and get_lhs_design()).
Methods that allow us to cover the space and encompass as much of the interesting simulator
behaviour as possible are reasonable for producing a ﬁrst design ∗ (Santner et al. 2003) .
The ranges of our parameters are αSI ∈ [0.1, 0.8], αIR ∈ [0, 0.5], αSR ∈ [0, 0.05]. The
‘observations’ we wish to match to are synthetic, as this model has no real world analogue;
to these targets we have added some uncertainty reﬂecting the range of observation we would
see in reality. Both ranges and targets are deﬁned as follows.

> ranges <- list(aSI = c(0.1, 0.8), aIR = c(0, 0.5), aSR = c(0, 0.05))
> targets <- list(nS = c(580, 651),

nI = list(val = 169, sigma = 8.45),
nR = c(199, 221))

Note that we have two diﬀerent conventions for specifying targets and their uncertainties.
The one which most aligns with the mathematical discussion of observation error is that for
the output nI, where we observe a value with a corresponding uncertainty. Such a framework
is not always possible; for distributional or other reasons we may only be able to state that

∗However, the hmer package has a collection of pre-built datasets which includes the requisite runs from
this simulator and parameter space: they can be accessed using data("SIRSample"), and we will use these
henceforth.

18

The hmer Package

we know that our target value lies within a speciﬁed interval. These two conventions can be
used concurrently in a set of targets as deemed necessary.

Our initial simulator runs, consisting of 30 training runs and 60 validation runs, are shown
in Figure 2. Note that we plan to use only 30 runs to train the emulators over the entire
space: this is the minimum that we should consider using for a 3 dimensional system such as
this∗. Nevertheless, this is very little information required by the emulators for us to make
inferences about the structure of the entire space.

Figure 2: The initial set of simulator runs: runs that we will use for training are plotted as
circles and those for validation as crosses.

With the initial parameters and data set up, we use hmer to train some emulators to the
three targets.

> ems_wave1 <- emulator_from_data(SIRSample$training, names(targets), ranges)

[1] "Fitting regression surfaces..."
[1] "Building correlation structures.."
[1] "Creating emulators..."
[1] "Performing Bayes linear adjustment..."

There are a number of messages that indicate our progress through emulator training; in
the event where there are many more outputs to train to, further messages are displayed to
indicate which output is being considered. Once trained, we may examine the structure of an
emulator by invoking its built-in print() statement: for example, to view the emulator for
the nI output, we can enter the following.

∗As a general rule of thumb, one should aim to have at least 10d points for a d dimensional parameter

space as training runs (Loeppky et al. 2009).

Andrew Iskauskas et al.

19

> ems_wave1$nI

Parameters and ranges: aSI: c(0.1, 0.8): aIR: c(0, 0.5): aSR: c(0, 0.05)
Specifications:

Basis functions: (Intercept); aSI; aIR; I(aIR^2); aSI:aIR
Active variables aSI; aIR
Regression Surface Expectation: 149.8096; 199.5466; -281.9466;

201.6298; -196.1621

Regression surface Variance (eigenvalues): 0; 0; 0; 0; 0

Correlation Structure:
Bayes-adjusted emulator - prior specifications listed.
Variance (Representative): 3226.426
Expectation: 0
Correlation type: exp_sq
Hyperparameters: theta: 0.9033
Nugget term: 0.05

Mixed covariance: 0 0 0 0 0

The print statement details the basis functions chosen, the corresponding regression coeﬃ-
cient prior speciﬁcations, and the active variables, as well as the speciﬁcs of the Correlator
object: particularly its prior variance σ2, hyperparameters for the chosen correlation func-
tion, and the nugget term δ. Note that the Emulator print statement also informs us that the
emulator has been Bayes-adjusted. We can access the unadjusted emulator, should we wish
to (for example to make adjustments to the correlation structure or regression surface), by
calling ems_wave1$nI$o_em. We may already note from this printout that the variable αSR
is not anticipated to be informative for the number of infected people at t = 10, due to its
absence in the active variable set. This is a partial justiﬁcation for our choice of a ‘known’
regression surface - simply by examining the structure of the regression surface, we can deter-
mine physically interesting properties of the simulator. Here, the constructed emulator lends
weight to a qualitative observation about the system: we would not expect waning immunity
to have had a substantial eﬀect on the output of the simulator at t = 10, since it is unlikely
that we have had a large number of individuals infected, recovered, returned to susceptibility,
and infected once more. Such observations can be extremely useful in motivating further
explorations of particular regions of the parameter space of the simulator, and we may obtain
these insights through a simple print statement in the package.

We, in principle, now have a collection of emulators for the outputs of interest, and can
therefore explore the parameter space without recourse to simulator evaluations.

4.2. Validating the emulators

Before we do anything else with these emulators, we must ensure they are suitable for in-
ference. We use the validation_diagnostics() function to check the quality of their pre-
dictions with respect to unseen simulator runs. Along with the following code output, it
produces a plot of the diagnostics which we include in Figure 3.

> validation_diagnostics(ems_wave1, targets, SIRSample$validation)

20

The hmer Package

[1] aSI aIR aSR
<0 rows> (or 0-length row.names)

Figure 3: Plots of the validation diagnostics for the SIR emulators.

The output of validation_diagnostics() is an empty data.frame, indicating no conﬂict
between the emulators and the simulator outputs. Each row of the plot corresponds to an
emulated output; each column to a diﬀerent diagnostic.
The ﬁrst column compares the simulator output (x-axis) with the emulator prediction (y-
axis): the green line represents the line of ‘perfect’ ﬁt where fi(x) = EDi
[fi(x)]. The error-bars
encode the emulator uncertainty around its prediction. We can see that almost all error-bars
contain the green line. There are a few exceptions to this, where error-bars just miss the
line of perfect ﬁts — in particular, the prediction in the lower range of the nR emulator
(bottom-left plot). The reason these have not been judged to have failed diagnostics is
due to the fact that the simulator predictions are a long way away from the observation —
according to the simulator the parameter set in the bottom of the nR plot gives a value in
the region of 10 recovered people, when in actual fact we anticipate any relevant points to
provide a value closer to 200 as a minimum.
If we omit the targets argument from the
validation_diagnostics() function call, these points will be highlighted in red, and the
function would return a non-empty data.frame.

Andrew Iskauskas et al.

21

The second column considers potential emulator misclassiﬁcation. On the x-axis is emulator
implausibility for each point; on the y-axis is simulator ‘implausibility’ as deﬁned in Sec-
tion 3.4. A vertical and horizontal line are placed on the plot to indicate the relevant cut-oﬀ
values for each of the simulator and emulator. Points in the upper-right or lower-left seg-
ments fall into the ﬁrst category detailed in the relevant part of Section 3.4 (simulator agrees
with emulator) and points in the upper-left segment fall into our second category (emulator
does not rule a point out); neither of these are cause for concern. A point in the lower-right
quadrant suggests that it would be ruled out as implausible by the emulator but not by the
simulator, would be highlighted in red, and would warrant further investigation.

The ﬁnal column provides a histogram of the standardised errors (5). Here, we are simply
looking to see a reasonable spread centred around 0, with no more than a couple of outliers
(points for which Ui(x) has magnitude greater than around 3, say). We see that this is the
case; there is one potentially egregious point in the bottom-right plot, but we expect this due
to the considerations of the corresponding plot in the ﬁrst column.

A number of other diagnostic functions are available in the package, including a series of plots
obtained from individual_errors() which implements many of the tests described in Bastos
and O’Hagan (2009), residual_diag() which allows us to look directly at the performance
of the regression surface, and distributional tests for goodness-of-ﬁt within summary_diag().
Each of these functions takes a single emulator as an argument, and these can be used to
supplement any intuition gained from validation_diagnostics().

4.3. Emulator and History Matching visualisation

The major advantage of emulation is that we can eﬃciently explore the parameter space
without a large number of (usually computationally expensive) simulator runs. Here we
detail a few functions in hmer that allow us to inspect and gain insight into the simulator
behaviour across the full parameter space, not just those parts for which we have runs.
The most useful function for this purpose is emulator_plot(emulators, plot_type, ...)
which produces a contour plot of a desired emulator statistic over a two-dimensional slice of
the parameter space. It can be called with a collection of emulators as its ﬁrst argument, or
directly from a single emulator via the plot command. Either option returns a ggplot2 object
which, by the nature of such objects, can be modiﬁed after the fact to add other plot objects
to it∗.

> emulator_plot(ems_wave1)
> ems_wave1$nR$plot(plot_type = 'sd', params = c('aSI', 'aIR'),

fixed_vals = c(aSR = 0.045)) +
geom_point(data = SIRSample$training, aes(x = aSI, y = aIR))

The output of the ﬁrst of these commands is shown in Figure 4. For multiple output plots,
no scale is shown: however, we can easily see trends across the parameter space which come
from the regression surface. We can also see the perturbations arising from the Bayes linear
update: recall that the regression surfaces consist of constant, linear and quadratic terms,
which would not be suﬃcient to describe the contour lines we see here.

∗The ﬁrst of these commands returns a ggmatrix of plots, so augmentation of individual plots is more
involved but still possible. For details on modifying these objects, consult the ggplot2 (Wickham 2016) and
GGally (Schloerke et al. 2021) packages.

22

The hmer Package

Figure 4: The output from the emulator_plot function. Each plot displays the qualitative
behaviour of the output across the particular two-dimensional slice of parameter space.

Indeed, one can view the unadjusted emulators using a very similar plotting argument so as
to make a direct comparison by using the em$o_em argument: for example to examine the
prior (untrained) emulator for output nR

> plot(ems_wave1$nR$o_em)

The result of this is shown in Figure 5. We have plotted the unadjusted and the adjusted
emulators on their own in order to include the scales over which they are deﬁned, and can see
clearly the eﬀect of the training points on the structure of the output surface as predicted by
the emulator.
The emulator_plot() command takes a variety of arguments alongside the emulators. Some
have been used in the examples above: the params argument determines which two parameters
are to be plotted with respect to, and the fixed_vals argument determines the ﬁxed values
of the remaining parameters. The plot_type command can be one of ﬁve options: "exp" for
emulator expectation (the default); "var" for emulator variance; "sd" for emulator standard
deviation, "imp" for emulator implausibility; and "nimp" for nth maximum implausibility.
The latter two arguments require targets to be speciﬁed; the ﬁnal argument can also accept
an nth argument to determine which maximised implausibility to plot. We may also increase
the ﬁdelity of the plot using the points-per-dimension (ppd) argument.

Andrew Iskauskas et al.

23

Figure 5: The emulator expectation for output nS before (left) and after (right) Bayes linear
adjustment.

Central to the history matching procedure is the deﬁnition of an implausibility measure,
as the choice of a cutoﬀ for a point to be deemed non-implausible is integral to the ex-
tent to which the parameter space is reduced on subsequent waves. Correspondingly, the
plots from emulator_plot() where we visualise implausibility of an emulated output (via
plot_type = ‘imp’) or implausibility of a collection of emulators as considered in Section 2.2
(via plot_type = ‘nimp’) can be critical in making determinations on what constitutes an
acceptable cutoﬀ. We will discuss other means of selecting an appropriate implausibility
cutoﬀ shortly, but we ﬁrst note that we may use the emulator plots to examine the non-
implausible space under a number of putative cutoﬀs, as well as the geometric structure of
the space. The output of each of these commands is shown in Figure 6, and gives an indi-
cation of the contribution of the nS emulator (left) to our overall measure (right): a clear
correlation between acceptable values of aSI and aIR is present if we wish to simply match
to the number of susceptible people, but the structure of the overall space when all outputs
are considered limits this simple relationship considerably. After considering the shapes and
size of the contours, were we to determine that the amount of space that would be reduced at
a given implausibility cutoﬀ was too severe or too conservative we may apply this knowledge
in the history matching procedure by selecting an informed value for the cutoﬀ.

> emulator_plot(ems_wave1$nS, plot_type = "imp", targets = targets)
> emulator_plot(ems_wave1, plot_type = "nimp", targets = targets,

ppd = 40, cb = TRUE)

The emulator_plot() command is very useful for examining the behaviour of the simulator
outputs, identifying global trends and regions of high uncertainty, and gaining an understand-
ing of where acceptable matches might be found.

24

The hmer Package

Figure 6: The emulator implausibility for a single emulated output (in this example, the
number of susceptible people, nS) and the maximum implausibility over all three emulated
outputs.

Implausibility Plots and Colourblindness
In any plots which calculate and display implausibility, the common colour scheme is shading
from red to green, where red indicates a higher (and therefore worse) implausibility value. This
aligns with the common scheme used in previous papers (for example Vernon et al. (2010),
Figure 6), but can cause problems for colourblind users, particularly those with deuteranopia
or protanopia.
In any plot where this issue could arise, the cb option exists, which when
set to TRUE plots instead on a two-colour gradient colour scheme that aligns with the tenets
behind the viridis package (Garnier et al. 2021). Any other plots in the hmer package use
either a single-colour scheme or one of the viridis colour schemes.
There are many other visualisation functions available within the hmer package: for instance,
functionality to visually examine which variables are active for a collection of emulated out-
puts (plot_actives()); plotting tools to identify the most inﬂuential inputs to each output
(effect_strength()); simple plotting functions to demonstrate output-input dependence
based on either the emulator predictions or on the simulator output directly (output_plot,
behaviour_plot()); and a pairs plot to simultaneously examine diagnostics and implausibil-
ity relative to their position in parameter space (validation_pairs())∗. We detail only two
further emulator visualisation tools here, which deal with considering implausibility: the ﬁrst
is the plot_lattice() function and the second is the space_removed() function.
Despite its utility, a two-dimensional slice of parameter space does not give us a deﬁnitive
picture of where acceptable matches might be found, and is highly dependent on the slice
chosen. For instance, suppose the acceptable part of parameter space here lies in a small
neighbourhood of αSI = 0.45, αIR = 0.25, αSR = 0.01 (i.e., the centre of the space in αSI
and αIR but toward the lower end of the range for αSR), and we consider implausibility
plotted over αSI and αIR with αSR = 0.045. The plot thus produced would be unlikely to
show the probable location of the non-implausible region, and in fact may suggest that a

∗For details of any of these, see the documentation (e.g., ?validation_pairs).

Andrew Iskauskas et al.

25

non-implausible region could exist away from the centre of the space since the variance may
be higher at the edges of the space. We could produce multiple slices through the space quite
easily in this example, but for higher-dimensional spaces this becomes infeasible. Instead, we
may turn to the plot_lattice() command.
Taking as a minimum the emulators and the corresponding targets, the plot_lattice()
function returns a grid of plots. Each plot represents a summary of the implausibility over
the entire space, projected in the relevant fashion:

• Minimum maximised implausibility (upper triangle): the maximum (or nth-maximum)
implausibility is calculated across the space. We collect implausibilities based on the
projection of the input points into the particular two-dimensional subspace of interest,
and calculate the minimum of the (nth) maximum implausibilities. This quantity is
plotted;

• Two-dimensional optical depth (lower triangle): Points are collected in the same fashion
as above, but we instead consider the proportion of points projected that have acceptable
implausibility. A value of 1 suggests that all projected points are acceptable; a value of
0 suggests that none are;

• One-dimensional optical depth (diagonal): Similar to the two-dimensional case but
projecting to a single parameter direction. Each of these plots has y-axis range of [0, 1].

The term ‘optical depth’ corresponds to a particular conceptualisation of the non-implausible
region. Let us assume that we have determined an implausibility cutoﬀ that delineates accept-
able and unacceptable points, and choose two parameter directions of interest. The optical
depth is then deﬁned as the thickness of the non-implausible space along the line of sight de-
ﬁned by picking ﬁxed values for the two chosen parameters. This is easiest to visualise in three
dimensions, as the optical depth for any pair of the chosen parameters is the density along a
line in the remaining direction, but the concept applies in higher dimensions. This, combined
with the minimum maximised implausibilities, give the outline of the non-implausible region
as well as a measure of its density: for more details, see Vernon et al. (2018).
The use of plot_lattice() is as below, with the corresponding output in Figure 7.

> plot_lattice(ems_wave1, targets, ppd = 35)

This visualisation can be extremely helpful in visualising the overall acceptability of the
space with respect to pairs of arguments, as well as identifying correlation structure between
input parameters (for example, that between αSI and αIR). The optical depth also gives us
an indication of where in the projected space the strongest chance of ﬁnding an acceptable
match lies: we can see a stronger signal toward the centre of the αSR parameter range.
In this particular example, the inclusion of the αSR parameter in the plot provides little
additional information at this wave, due to its subdominant impact on the emulated outputs
(as remarked on in Section 4.1); we can see this by the predominantly straight lines in those
plots that include it. However, in higher dimensions a combined plot of all parameter pairs
can be crucial in understanding the structure of the non-implausible space, in turn motivating
a choice of outputs to emulate and match to or particular parameter regions to investigate in
greater depth.

26

The hmer Package

Figure 7: The result of a call to plot_lattice(). Each plot is a particular combination
of parameters; the upper diagonal provides minimum implausibility across the space; the
lower diagonal the two-dimensional optical depth; the diagonal one-dimensional optical depth.
We note that the combined eﬀect of αSI and αIR is driving the identiﬁcation of the non-
implausible region.

Finally, the choice of implausibility cut-oﬀ is not always straightforward to determine. As
mentioned, for a single emulator we can appeal to Pukelsheim’s 3σ rule and use I = 3 as
the cut-oﬀ for acceptability; however, for nth maximised implausibility over multiple outputs
may not be so clear. If we can assume that m of the N outputs are independent, and under
the assumption of normality, then a heuristic argument for a reasonable cut-oﬀ can be found
in Goldstein et al. (2013). However, these assumptions may not be valid, or may be hard
to justify.
Instead we can explore the robustness of various implausibility cut-oﬀ choices
by examining the proportion of the current non-implausible region that will be removed by
particular choices. The space_removed() function provides this insight, as well as allowing
us to consider how changes to model discrepancy and observation error can aﬀect the space
removed.

At the most basic level, all that is required for the space_removed() function is the set of
emulators and their corresponding targets: the below command computes the space removal
with a grid of 203 = 8000 points from the current non-implausible region, considering modi-
fying the observational error to varying proportions of its current value. We can, therefore,
choose more aggressive or more cautious cut-oﬀ values with appropriate insight into the eﬀect

Andrew Iskauskas et al.

27

these will have on reduction of the parameter space.

> space_removed(ems_wave1, targets, ppd = 20) +

geom_vline(xintercept = 3, lty = 2)

Figure 8: The result of space_removed() applied to our wave 1 emulators. We can see that
a cut-oﬀ of I = 3 would be suﬃcient to rule out over 95% of the current space, a ﬁgure that
is relatively stable to changes in the choice of observational error. At higher cut-oﬀ values,
the choice of observational error has a more pronounced eﬀect.

The space_removed() function can also provide stratiﬁcation of the space removed with
respect to inﬂation and deﬂation of the prior emulator variances σ2
, the hyperparameters θi,
i
or the model discrepancies σmi
. This is controlled by the argument modified; the details of
the stratiﬁcation are determined by the u_mod argument which by default is c(0.8, 0.9,
1, 1.1, 1.2). As with many of the plots in hmer, they can be augmented with additional
ggplot2 objects; in Figure 8 we have added a vertical line at I = 3 using the geom_vline()
function.

> # An example using the optional parameters
> space_removed(ems_wave1, targets, modified = 'var', ppd = 20,

u_mod = seq(0.5, 1.5, by = 0.1))

The collection of visualisation tools available within the hmer package allow us to gain a
deeper understanding of the structure of the simulator, the input space, and the expected
size and shape of the non-implausible region we expect to obtain from point proposals. For
complex models where evaluations are expensive, this can be an invaluable tool for supple-
menting and augmenting our knowledge about the model. In high-dimensional models (in
input and/or output dimension), some of the visualisations require a moderate amount of
computational time; however, this time should be weighed against the equivalent time it

28

The hmer Package

would take to perform simulator runs at thousands of parameter sets and the ensuing anal-
ysis provided automatically by these functions. In all but the fastest of computer models,
the emulators allow for an exploration of the model behaviour across the parameter space
that simply would not be possible otherwise: an emulator evaluation at a parameter set takes
between 10−4 to 10−2, whereas simulators on which emulation has been applied have varied
in evaluation time between seconds (Scarponi et al. 2022), hours (Andrianakis et al. 2015),
and days (Vernon et al. 2014), representing an eﬃciency gain of 103 to 1010 compared with
running the simulator.

4.4. Proposing points and inspecting results

The point proposal stage, for creating a design of points for the next wave, consists of a single
call to the function generate_new_runs(). As with emulator_from_data(), there are a
multitude of optional arguments which are described in the associated help ﬁle; we detail the
most salient of those arguments here.

Here, we apply the default usage discussed in Section 3.5.

> proposal1 <- generate_new_runs(ems_wave1, 500, targets)

[1] "Proposing from LHS..."
[1] "102 initial valid points generated for I=3"
[1] "Performing line sampling..."
[1] "Line sampling generated 40 more points."
[1] "Performing importance sampling..."
[1] "Importance sampling generated 474 more points."
[1] "Resample 1"
[1] "Performing line sampling..."
[1] "Line sampling generated 40 more points."
[1] "Performing importance sampling..."
[1] "Importance sampling generated 385 more points."
[1] "Selecting final points using maximin criterion..."

The function provides status updates as it proceeds through the stages of point proposal, and
returns a data.frame of the points thus chosen∗. These points can be passed to the simulator
as a representative sample of the non-implausible space X1, and we may start the next wave of
history matching. We may also plot these in whatever fashion we desire: the native R plot()
command produces an adequate pairs plot of inputs, or we can use plot_wrap() to plot the
proposed parameter sets with respect to other ranges: for example, plotting the points of Xk
with respect to the minimum enclosing hyperrectangle of Xk−1 to inspect the shrinkage of
parameter space.

plot_wrap(proposal1, ranges)

∗In this case we requested 500 points, which would certainly be far more points than we would need
for a second wave of history matching for this particular simulator. However, there is little computational
disadvantage to doing so, given the speed of emulator evaluation.

Andrew Iskauskas et al.

29

Figure 9: A simple plot of the points proposed at wave 1, with respect to the original ranges
of the parameters. We can see the parameter restrictions, as well as the correlation between
αSI and αIR.

4.5. Visualising the simulator space

After a few waves of history matching, we will have gained an evolving picture of the structure
of the non-implausible space as well as the behaviour of the corresponding simulator runs.
It is often instructive to look at this changing picture, so as to gain an understanding into
which parameters are driving the reduction of non-implausible space, how the outputs react
to changing parameter values, and indeed the progress towards matching all targets. To this
end, a set of multi-wave visualisation tools are available: wave_values(), wave_points(),
wave_dependencies(), and simulator_plot(). The wave preﬁxed functions return a grid
of plots in output, input, or input-output space (respectively), allowing us to look at the
behaviour and evolution of the space over multiple waves. The ﬁnal plot is a simple heuristic
plot that allows us to evaluate the suitability of simulator runs over the ensemble of points
at each wave. Here, we demonstrate the use of wave_values() and simulator_plot().
These functions are most eﬀective when we have access to a few waves of history matching.
One could quite easily replicate the steps demonstrated above (using get_res to obtain
simulator runs at the end of each wave), but here we will use an hmer dataset which contains
sets of points sampled from Xi for i = 0, . . . , 3, namely SIRMultiWaveData. This dataset
consists of a four-element list, where each element is the relevant data.frame of points. The
wave_values() function, as mentioned, returns a grid of plots (in the fashion of a pairs plot)

30

The hmer Package

with axes corresponding to the simulator outputs. The lower diagonal shows the values of
the simulator runs, coloured by which wave their generating point was proposed at, as well as
an overlay of the target bounds for the two targets in question∗. The upper diagonal shows
the same plot but ‘zoomed in’ to around the target bounds, in order to see the structure in
the region of interest. The diagonal shows density plots for each individual output; here the
target bounds are simply vertical lines. A simple code chunk is shown below, along with the
resulting plot in Figure 10. Here, we have used the default arguments to the function, aside
from using l_wid to slightly thin the target bound lines; options for excluding particular
waves, controlling the level of ‘zoom’, and plotting particular outputs, among others, are
available.

> wave_values(SIRMultiWaveData, targets, l_wid = 0.8)

Figure 10: The result of wave_values() applied to the SIRMultiWaveData dataset. As we
proceed through the waves, we see that more and more of the proposed points are lying within
the bounds of the targets, as well as a strengthening correlation between nS and nI.

As well as providing an insight into the dependencies of the simulator, we may also consider
this a barometer of how many waves we require to generate a large sample from our ﬁnal non-
implausible region. Here, we can see that most of the points proposed at wave 3 lie within the
bounds of every target, and so we could determine that further waves are unnecessary. This
is backed up by a quick inspection of the emulator uncertainties at the ﬁnal wave (accessible
via load("SIRMultiWaveEmulators"); a list of three sets of trained emulators).

> sapply(SIRMultiWaveEmulators[[3]], function(x) x$u_sigma)

nS

nI

nR

1.4776221 2.1416256 0.5041151

∗For targets that have been deﬁned in terms of a value and a sigma, the bounds correspond to 3σ below

and above the value.

Andrew Iskauskas et al.

31

Comparison of these with the uncertainties on the observations themselves suggests that
the emulator uncertainty is vastly subdominant to that of the observation, and subsequent
waves of emulation will not substantially improve on the emulators from wave 3. This is
partly because of the simplicity of the toy model we are using (both in terms of the model
dynamics itself and the low parameter and output dimension) and this three-wave completion
should not be viewed as a ‘standard’; whereas some models can be matched to data within
3 to 5 waves (Vernon et al. 2010; Craig et al. 1997), the necessity of performing around 10
waves (Andrianakis et al. 2015) or even closer to 20 (Scarponi et al. 2022) is not uncommon.
Nevertheless, the techniques to evaluate the progress of the history matching remain true;
plots such as wave_values() give us an easily interpretable measure for the status of the
history match.
One thing that cannot be (directly) determined from the wave_values() plots is whether
a particular parameter set matches all targets. The simulator_plot() function provides a
visualisation of the most fundamental question in the history matching process: are there any
points that match to all outputs?

> simulator_plot(SIRMultiWaveData, targets, barcol = 'black')

All

‘trajectories’ across

Figure 11:
for all waves, plotted using
simulator_plot(). The wild ﬁtting behaviour in our wave 0 (yellow) quickly settles down as
the emulators propose points, until at wave 3 (blue) practically all trajectories pass through
the targets.

the outputs,

In this case, the plot in Figure 11 indicates to us the quality of the points proposed at wave
3. In more complex systems where a match is harder (or impossible) to obtain, this plot in
conjunction with the wave_... plots can give an insight into which outputs are being missed,
whether there are conﬂicts between outputs, or how close we are to obtaining a ﬁt to all
targets. Such an analysis, using these visualisation tools, was performed in Scarponi et al.
(2022), where the inability to ﬁnd matches to observational data in a small number of cases

32

The hmer Package

could be traced to conﬂicts between particular outputs or potential misspeciﬁcation in the
observations.

In this section we have detailed the main stages of emulation and history matching using the
hmer package. This introduction is by no means comprehensive, but aims to highlight the
most important functions and visualisation tools. More involved examples of using the hmer
package can be found in the vignettes, including lower- and higher-dimensional examples,
demonstrations of more visualisation tools, and a brief exploration of some of the advanced
techniques we are about to touch upon.

# List the vignettes
> vignettes(package = "hmer")
> vignette("low-dimensional-examples", package = "hmer")

5. Variance and Hierarchical Emulation

The ﬂexibility of the Bayes linear emulation approach allows us to extend the framework
beyond that of simple emulation of deterministic systems.
In this section, we discuss an
aspect of more advanced functionality currently present in the hmer package — namely, the
emulation of stochastic systems using hierarchical emulation. Other advanced methods in
development are touched on brieﬂy in Section 6.

The example described in Section 4 was deﬁned by a set of diﬀerential equations, which
naturally (up to machine precision in the ODE solver of choice) result in the same output
for repeated runs of the same input parameters. Complex models do not always possess this
deterministic behaviour: in particular, in epidemiology, a stochastic compartmental model or
an agent-based model (ABM) can be used, where transition between states in the system is
probabilistic (Wilkinson 2018). As a result, repeated simulator runs at the same parameter
values (which we term realisations) will result in diﬀerent values of the outputs. Such a
simulator is referred to as stochastic.
Stochastic simulators can (and often do) exhibit heteroscedasticity, where the variance across
the parameter space can be wildly diﬀerent due to the variability in the model. For example,
consider a simple Birth-Death model where individuals are born at a rate λ, and die at a
rate µ. For low values of (λ, µ), the system changes very little over time and the variability is
minimal (the limit where λ, µ → 0 of course gives no variability whatsoever); for high values
of (λ, µ) the variability over multiple repetitions of the simulator is much higher.
For such systems, we may apply a hierarchical approach to emulation, leveraging the ﬂexibility
of the Bayes linear emulation approach (Cumming and Goldstein 2010). Suppose we have
a set of simulator outputs at points (x(1), x(2), . . . , x(n)) where, for each point x(j), we have
repeated the simulator evaluation Nj times. We therefore have a collection of P
j Nj simulator
runs, which can be categorised by where in parameter space they were evaluated. The Nj
need not be the same at each point:
for instance, where a simulator is slower to run in a
part of parameter space we may perform fewer runs than at a more ‘well-behaved’ region of
parameter space. This possible distinction between information at points is handled by the
package.

From the collections of outputs we can determine a collection of sample means and sample
variances, indexed by x(j). The key stage in emulating this system is to treat the sample

Andrew Iskauskas et al.

33

(x)] is the variance of the system, the ‘variance’ VAR[s2
i

variances, which we denote s2
(x(j)), obtained as an output in their own right and create an
i
emulator for the variance of the stochastic simulator itself. This requires an understanding
of, or sensible priors for, the fourth-order quantities of the system; since our ‘expectation’
E[s2
(x)] is the variance of the variance.
i
There are various ways to decide on sensible priors for the relevant quantities (Goldstein and
Wooﬀ 2007, Ch. 8) which provide a set of emulators that can oﬀer predictions as to the
stochasticity of the simulator across the non-implausible region. As with all such emulators,
they know the level of stochasticity at training points, and provide a statistical approximation
of it at unseen points.

Of course, we want to explore the space of simulator outputs, not of its variability. For
example, the primary interest is often in the examination of the mean surface of the stochastic
model. However, the variance emulators trained above can help to correct for any uncertainty
induced by the fact that, due to ﬁnite sample size, we cannot observe the ‘true’ underlying
mean process from a collection of realisations of the stochastic simulator. The use of variance
emulation therefore allows us to train much more accurate emulators to the mean surface
which, moreover, have an intrinsic understanding of the underlying stochastic process.

Training such emulators is a multi-step process, naturally, but aside from some considerations
has much the same structure as the training process for deterministic emulators. To that end,
the function variance_emulator_from_data provides the means by which this hierarchical
structure can be determined, constructed, and adjusted with respect to data. It takes the
same collection of arguments, with one important caveat:

• input_data: the unaggregated results from the simulator, as a data.frame of P

j Nj

points and their corresponding simulator outputs.

The estimates of the fourth-order quantities are determined, dependent on the nature of the
data available, and the correction for sample quantities is incorporated. Unlike the output
of emulator_from_data, this function returns a nested list of emulators: one set named
variance and one named expectation∗, corresponding to the variance and mean emulators
for the outputs, respectively. Once trained, this collection of emulators can be passed as an
argument to any of the usual functions as appropriate: in particular, the core functions will
handle the nested collection of emulators in the manner appropriate to their usage.

This machinery allows us to accurately emulate heavily stochastic systems with minimal mod-
iﬁcation to the history matching workﬂow described in Section 2.2. If our only interest is in
matching to the means of the stochastic outputs, then the emulators for the variance con-
tribute only at the stage of building the mean emulators and do not complicate the procedure
of history matching; if in fact we have observations of the real-life variability of the system in
question then we can use the variance emulators alongside those for the mean to ﬁnd parts
of parameter space with the expected output behaviour and the expected variation, allowing
for more powerful and eﬃcient waves of history matching.

6. Conclusion and discussion

∗A note on terms here: we use ‘expectation’ to denote the mean emulators in order to avoid possible

conﬂicts with R’s native mean function.

34

The hmer Package

The hmer package provides an accessible means of applying the powerful techniques of Bayes
linear emulation and history matching to complex, often computationally intensive, simula-
tions of real-world processes. The framework has been employed to great success in a variety
of modelling situations, and the hmer package itself has been used by epidemiologists to
ﬁnd matches to TB and HIV simulators of varying complexity and evaluation time (Scarponi
et al. 2022; Clark et al. 2022). The core functionality of the package is designed to require, by
default, as little understanding of the mathematical machinery that underpins emulation as
is possible, while still providing the ﬂexibility for advanced users to create bespoke emulators
for their particular circumstance using expert judgements for prior speciﬁcations.

In this article, we have detailed the main, ‘front-facing’, functions of the package, with which
a user can follow the process of training and validating emulators and propose appropriate
representative collections of points from the resulting non-implausible space. We have also
detailed, by demonstration, a sample of the visualisations that can be derived from the com-
putationally fast emulators to gain insight into the structure of the simulator they purport
to replicate. The toy model we have used to demonstrate these features is of simplistic form,
but the principles and processes apply equally to simulators whose run-time per evaluation is
of the order of days, rather than seconds (Williamson et al. 2013; Vernon et al. 2022, 2014).
The comparatively small number of parameter sets required to train the emulator minimises
the computational load for slow simulators and allows a thorough investigation of the non-
implausible space at every stage, resulting in a potentially very large collection of suitable
points without constant recourse to the simulator. Points generated from the non-implausible
space can be used to fully furnish and investigate the space of acceptable matches to data in
a number of diﬀerent ways, which we have outlined.
The hmer package is model-agnostic and code-agnostic, requiring only that the outcome of
multiple simulator runs can be collated into a data.frame (such an object can be easily
obtained from, say, a csv ﬁle, often requiring minimal user-time to produce). We have placed
particular emphasis on epidemiological models and disease modelling, due to the speciﬁc
features of such models and the obvious advantages some aspects of the package can oﬀer;
however, all techniques presented in the package have potential applicability across all ﬁelds
of computer modelling. Many of the techniques employed here have existed in the literature
for some time but remained out of reach of a modeller without a deep understanding of the
emulation framework. The hmer package is designed with the intention of removing that
barrier to usage.

Nevertheless, emulation is not a panacea for all diﬃculties in modelling. To best utilise the
inherent uncertainty structure, it is incumbent on the user to think carefully about their
simulator, any discrepancies between it and reality, and the quality and accuracy of the
observational data that they wish to match to. The framework provided for emulation is
permissive with regards to the speciﬁcation of these quantities; this does not mean that it
should be ignored if one wishes to comprehensively determine the space of acceptable matches,
subject to all the uncertainties that separate the simulator from the real-world process.

Similarly, the automated process of emulator training and even the default choices for im-
plausibility cut-oﬀs, composite measures, and point sampling methods are designed to best
accommodate as large a class of models as is possible. The scope of models and systems that
these processes could be applied to is suﬃciently diverse so as to preclude any uniﬁed auto-
mated approach to emulation and history matching, and the user should see emulation as an
interactive process that allows them to learn about their simulator, strengths and drawbacks,

Andrew Iskauskas et al.

35

while exploring the parameter space of interest. Here, we brieﬂy discuss a few situations
where user-interaction can vastly improve the quality and eﬃciency of a history match.

• Expert knowledge about a simulator can make a big diﬀerence to the quality of emu-
lators. The choice of initial parameter space X0 can hugely impact the ability of the
emulators to accurately represent the simulator, as can the choice of points within that
parameter space upon which to train the emulators. If we anticipate that interesting
behaviour should occur in particular regions of X0, then we should ensure that the
emulators are privy to that information via judicious choice of training points. This
becomes especially pronounced in high-dimensional models, where to fully represent the
behaviour across the parameter space with a reasonable number of simulator evalua-
tions is near-impossible. In such cases, more advanced ‘border-block’ designs can be
used (Cumming and Goldstein 2009).

• The process of training emulators is fast and there is little disadvantage to creating a
collection of emulators and, in light of emulator diagnostics, retraining with additional
points in parameter space designed to overcome any shortcomings identiﬁed. Similarly,
the choice of targets can hugely impact the inferential power of the emulators; choosing
which outputs to emulate at a given wave is currently a determination for the user, as
well as whether there exist any sensible transformations of the data that highlight the
structure of the simulator.

• The default behaviour of generate_new_runs will, for most non-implausible regions,
produce a well-spread representative sample of the space. However, there are circum-
stances under which the algorithm described in Section 3.5 will be ill-suited to the
task. Two obvious examples are when the non-implausible space Xk+1 is many orders
of magnitude smaller than its superset, Xk, or when Xk+1 consists of diﬀerently sized
disconnected regions. In both cases, the default generate_new_runs process may fail
to pick up on the small regions of Xk+1 or indeed fail to ﬁnd the space at all. In such
circumstances, it may be helpful to consider moving to a less restrictive composite mea-
sure of implausibility, relaxing the cut-oﬀ, or identifying the most restrictive outputs
and removing them from this wave. In extremis, the function idemc exists as a com-
putationally intensive search of the space which operates akin to Evolutionary Monte
Carlo methods (Williamson and Vernon 2013). This should be viewed as an approach
of last-resort, coming after an investigation of the space, the emulators, and our own
intuition about the behaviour of the simulator and of the observational data we possess.

In future updates to the hmer package we plan to enhance the functionality to make sur-
mounting some of the issues mentioned above easier, as well as introducing new methodology
for ﬁnding acceptable ﬁts and leveraging the additional information that can be gained from
an inspection of the emulator structure. For example, for models that are ‘easy’ to match to
(where the regression surface accounts for a large part of the behaviour across the space), it
can be helpful to use the properties of the regression surface to heuristically identify promising
regions of parameter space as well as diagnose regions that are underrepresented in training;
we may use the active variable structure to ‘cluster’ emulators and their active variables and
so operate on multiple reduced-dimension parameter spaces when proposing points (Cum-
ming and Goldstein 2010); for heavily stochastic systems where bimodality could be present

36

The hmer Package

it may be appropriate to emulate the ‘modes’ separately in order to obtain a good ﬁt to our
simulator; we may even emulate the derivative of the simulator at a point without further
information or training runs in order to identify best directions of approach for ﬁnding good
parameter sets. These examples of enhanced functionality are present in some form in the
hmer package, and will be further improved as the package evolves.
The hmer package is designed to allow a robust and careful analysis of a simulator, as well as
identify regions of parameter space that produce an acceptable ﬁt to observed data. History
matching with emulation is a powerful tool in performing this delicate task, and our aim has
been to provide a means by which modellers can implement these methods with a minimum of
statistical involvement, while allowing a deeper inspection into the structure of the emulation
for those that wish to do so. During development we have seen considerable success in the use
of hmer to calibrate (both with and without direct user input) complex simulators (Scarponi
et al. 2022; Clark et al. 2022), and we expect that the power that modellers and the wider
community can leverage from these techniques, presented in a user-friendly form, will increase
as the user-base and the scope of the package expands. Furthermore, having access to a tool
that allows careful examination of models without recourse to expensive simulator runs will
allow modellers to incorporate these techniques into model design and debugging, uncertainty
quantiﬁcation and forecasting, and model comparison more generally.

Acknowledgements

AI and DS would like to acknowledge the support and funding provided by the Wellcome Trust
grant 218261/Z/19/Z, as well as the valuable feedback and package testing provided by Re-
becca Clark, Christinah Mukandavire, Chathika Weerasuriya, Arminder Deol, Roel Bakker,
and all in the TB Modelling group at London School of Hygiene and Tropical Medicine.
AI would also like to acknowledge the helpful comments of Dario Domingo on the ﬁnal
manuscript. TJM is supported by an Expanding Excellence in England (E3) award from
Research England. IV would like to acknowledge the support of the UK Research and Inno-
vation grant EP/W011956/1. RGW is funded by the Welcome Trust (218261/Z/19/Z), NIH
(1R01AI147321-01), EDTCP (RIA208D-2505B), UK MRC (CCF17-7779 via SET Blooms-
bury)), ESRC (ES/P009011/1), BMGF (OPP1084276, OPP1135288 & INV-001754), and the
WHO (2020/985800-0).

References

Andrianakis I, Challenor PG (2012). “The Eﬀect of the Nugget on Gaussian Process Emula-
tors of Computer Models.” Computational Statistics & Data Analysis, 56(12), 4215–4228.

Andrianakis I, McCreesh N, Vernon I, McKinley TJ, Oakley JE, Nsubuga RN, Goldstein M,
White RG (2017a). “Eﬃcient History Matching of a High Dimensional Individual-based
HIV Transmission Model.” SIAM/ASA Journal on Uncertainty Quantiﬁcation, 5(1), 694–
719.

Andrianakis I, Vernon I, McCreesh N, McKinley T, Oakley J, Nsubuga R, Goldstein M,
White R (2017b). “History Matching of a Complex Epidemiological Model of Human

Andrew Iskauskas et al.

37

Immunodeﬁciency Virus Transmission by Using Variance Emulation.” Journal of the Royal
Statistical Society. Series C, Applied Statistics, 66(4), 717.

Andrianakis I, Vernon IR, McCreesh N, McKinley TJ, Oakley JE, Nsubuga RN, Goldstein
M, White RG (2015). “Bayesian History Matching of Complex Infectious Disease Models
Using Emulation: A Tutorial and a Case Study on HIV in Uganda.” PLoS computational
biology, 11(1), e1003968.

Aylett-Bullock J, Cuesta-Lazaro C, Quera-Bofarull A, Icaza-Lizaola M, Sedgewick A, Truong
H, Curran A, Elliott E, Caulﬁeld T, Fong K, et al. (2021). “June: Open-source Individual-
based Epidemiology Simulation.” Royal Society open science, 8(7), 210506.

Bastos LS, O’Hagan A (2009). “Diagnostics for Gaussian Process Emulators.” Technometrics,

51(4), 425–438.

Bowman VE, Woods DC (2016). “Emulation of Multivariate Simulators using Thin-plate
Splines with Application to Atmospheric Dispersion.” SIAM/ASA Journal on Uncertainty
Quantiﬁcation, 4(1), 1323–1344.

Brynjarsdottir J, O’Hagan A (2014). “Learning about Physical Parameters: The Importance

of Model Discrepancy.” Inverse problems, 30(11), 114007.

Castelletti A, Galelli S, Ratto M, Soncini-Sessa R, Young PC (2012). “A General Framework
for Dynamic Emulation Modelling in Environmental Problems.” Environmental Modelling
& Software, 34, 5–18.

Clark RA, et al. (2022). “The Impact of Alternative Delivery Strategies for Novel Tuberculosis

Vaccines in Low- and Middle-income Countries: A Modelling Study.” medRxiv.

Conti S, Gosling JP, Oakley JE, O’Hagan A (2009). “Gaussian Process Emulation of Dynamic

Computer Codes.” Biometrika, 96(3), 663–676.

Craig PS, Goldstein M, Seheult AH, Smith JA (1997). “Pressure Matching for Hydrocar-
bon Reservoirs: A Case Study in the Use of Bayes Linear Strategies for Large Computer
Experiments.” In Case studies in Bayesian statistics, pp. 37–93. Springer.

Cumming JA, Goldstein M (2009). “Small Sample Bayesian Designs for Complex High-
dimensional Models Based on Information Gained Using Fast Approximations.” Techno-
metrics, 51(4), 377–388.

Cumming JA, Goldstein M (2010). “Bayes Linear Uncertainty Analysis for Oil Reservoirs
Based on Multiscale Computer Experiments.” The Oxford handbook of applied Bayesian
analysis, pp. 241–270.

Du H, Sun W, Goldstein M, Harrison GP (2021). “Optimization via Statistical Emulation and
Uncertainty Quantiﬁcation: Hosting Capacity Analysis of Distribution Networks.” IEEE
Access, 9, 118472–118483.

Edwards TL, Nowicki S, Marzeion B, Hock R, Goelzer H, Seroussi H, Jourdain NC, Slater
DA, Turner FE, Smith CJ, et al. (2021). “Projected Land Ice Contributions to Twenty-
ﬁrst-century Sea Level Rise.” Nature, 593(7857), 74–82.

38

The hmer Package

Garnier, Simon, Ross, Noam, Rudis, Robert, Camargo, Pedro A, Sciaini, Marco, Scherer,
Cédric (2021). viridis - Colorblind-Friendly Color Maps for R. doi:10.5281/zenodo.
4679424. R package version 0.6.2, URL https://sjmgarnier.github.io/viridis/.

Gibson GJ, Renshaw E (1998). “Estimating Parameters in Stochastic Compartmental Models
Using Markov Chain Methods.” Mathematical Medicine and Biology: A Journal of the IMA,
15(1), 19–40.

Goldstein M, Rougier J (2006). “Bayes Linear Calibrated Prediction for Complex Systems.”

Journal of the American Statistical Association, 101(475), 1132–1143.

Goldstein M, Seheult A, Vernon I (2013). “Assessing Model Adequacy.” Environmental

Modelling: Finding Simplicity in Complexity, pp. 435–449.

Goldstein M, Wooﬀ D (2007). Bayes Linear Statistics: Theory and Methods, volume 716.

John Wiley & Sons.

Gu M, Berger JO (2016). “Parallel Partial Gaussian Process Emulation for Computer Models

with Massive Output.” The Annals of Applied Statistics, 10(3), 1317–1347.

Gu M, Palomo J, Berger J (2022). RobustGaSP: Robust Gaussian Stochastic Process Emu-

lation. URL https://CRAN.R-project.org/package=RobustGaSP.

Hankin R (2021a). emulator: Bayesian Emulation of Computer Programs. URL https:

//CRAN.R-project.org/package=emulator.

Hankin R (2021b). multivator: A Multivariate Emulator. URL https://CRAN.R-project.

org/package=multivator.

Higdon D, Kennedy M, Cavendish JC, Cafeo JA, Ryne RD (2004). “Combining Field Data
and Computer Simulations for Calibration and Prediction.” SIAM Journal on Scientiﬁc
Computing, 26(2), 448–466.

Jewell CP, Kypraios T, Neal P, Roberts GO (2009). “Bayesian Analysis for Emerging Infec-

tious Diseases.” Bayesian analysis, 4(3), 465–496.

Kaufman CG, Bingham D, Habib S, Heitmann K, Frieman JA (2011). “Eﬃcient Emula-
tors of Computer Experiments using Compactly Supported Correlation Functions, with an
Application to Cosmology.” The Annals of Applied Statistics, 5(4), 2470–2492.

Kennedy MC, O’Hagan A (2001). “Bayesian Calibration of Computer Models.” Journal of

the Royal Statistical Society: Series B (Statistical Methodology), 63(3), 425–464.

Loeppky JL, Sacks J, Welch WJ (2009). “Choosing the Sample Size of a Computer Experi-

ment: A Practical Guide.” Technometrics, 51(4), 366–376.

Maatouk H, Roustant O, Richet Y (2015). “Cross-validation Estimations of Hyper-parameters
of Gaussian Processes with Inequality Constraints.” Procedia Environmental Sciences, 27,
38–44.

Andrew Iskauskas et al.

39

Marshall L, Johnson JS, Mann GW, Lee L, Dhomse SS, Regayre L, Yoshioka M, Carslaw KS,
Schmidt A (2019). “Exploring how Eruption Source Parameters Aﬀect Volcanic Radia-
tive Forcing using Statistical Emulation.” Journal of Geophysical Research: Atmospheres,
124(2), 964–985.

McCreesh N, Andrianakis I, Nsubuga RN, Strong M, Vernon I, McKinley TJ, Oakley JE,
Goldstein M, Hayes R, White RG (2017a). “Improving ART Programme Retention and
Viral Suppression are Key to Maximising Impact of Treatment as Prevention–a Modelling
Study.” BMC infectious diseases, 17(1), 1–8.

McCreesh N, Andrianakis I, Nsubuga RN, Strong M, Vernon I, McKinley TJ, Oakley JE,
Goldstein M, Hayes R, White RG (2017b). “Universal Test, Treat, and Keep: Improving
ART Retention is Key in Cost-eﬀective HIV Control in Uganda.” BMC infectious diseases,
17(1), 1–11.

McKay MD, Beckman RJ, Conover WJ (2000). “A Comparison of Three Methods for Se-
lecting Values of Input Variables in the Analysis of Output From a Computer Code.”
Technometrics, 42(1), 55–61.

McKinley T, Cook AR, Deardon R (2009). “Inference in Epidemic Models Without Likeli-

hoods.” The International Journal of Biostatistics, 5(1).

McKinley TJ, Vernon I, Andrianakis I, McCreesh N, Oakley JE, Nsubuga RN, Goldstein M,
White RG (2018). “Approximate Bayesian Computation and Simulation-based Inference
for Complex Stochastic Epidemic Models.” Statistical science, 33(1), 4–18.

Olson R, Chang W, Keller K, Haran M (2018). stilt: Separable Gaussian Process Interpolation

(Emulation). URL https://CRAN.R-project.org/package=stilt.

O’Neill PD, Roberts GO (1999). “Bayesian Inference for Partially Observed Stochastic Epi-
demics.” Journal of the Royal Statistical Society: Series A (Statistics in Society), 162(1),
121–129.

Pukelsheim F (1994). “The Three Sigma Rule.” The American Statistician, 48(2), 88–91.

Rasmussen CE (2003). “Gaussian Processes in Machine Learning.” In Summer school on

machine learning, pp. 63–71. Springer.

Santner TJ, Williams BJ, Notz WI, Williams BJ (2003). The Design and Analysis of Com-

puter Experiments, volume 1. Springer.

Scarponi D, et al. (2022). “Demonstrating Multi-country Calibration of a Tuberculosis Model

Using New History Matching and Emulation Package - hmer.” medRxiv.

Schloerke B, Cook D, Larmarange J, Briatte F, Marbach M, Thoen E, Elberg A, Crowley
J (2021). GGally: Extension to ’ggplot2’. R package version 2.1.2, URL https://CRAN.
R-project.org/package=GGally.

Sorenson HW (1980). Parameter Estimation: Principles and Problems, volume 9. M. Dekker.

Toni T, Welch D, Strelkowa N, Ipsen A, Stumpf MP (2009). “Approximate Bayesian Compu-
tation Scheme for Parameter Inference and Model Selection in Dynamical Systems.” Journal
of the Royal Society Interface, 6(31), 187–202.

40

The hmer Package

Vernon I, Goldstein M, Bower R (2010). “Galaxy Formation: A Bayesian Uncertainty Anal-

ysis.” Bayesian analysis, 5(4), 619–669.

Vernon I, Goldstein M, Bower R (2014). “Galaxy Formation: Bayesian History Matching for

the Observable Universe.” Statistical science, pp. 81–90.

Vernon I, Liu J, Goldstein M, Rowe J, Topping J, Lindsey K (2018). “Bayesian Uncertainty
Analysis for Complex Systems Biology Models: Emulation, Global Parameter Searches and
Evaluation of Gene Functions.” BMC systems biology, 12(1), 1–29.

Vernon I, Owen J, Aylett-Bullock J, Cuesta-Lazaro C, Frawley J, Quera-Bofarull A, Sedgewick
A, Shi D, Truong H, Turner M, et al. (2022). “Bayesian Emulation and History Matching
of JUNE.” medRxiv.

Wickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York.

ISBN 978-3-319-24277-4. URL https://ggplot2.tidyverse.org.

Wilkinson DJ (2018). Stochastic Modelling for Systems Biology. Chapman and Hall/CRC.

Williamson D, Goldstein M, Allison L, Blaker A, Challenor P, Jackson L, Yamazaki K (2013).
“History Matching for Exploring and Reducing Climate Model Parameter Space using
Observations and a Large Perturbed Physics Ensemble.” Climate dynamics, 41(7), 1703–
1729.

Williamson D, Vernon I (2013). “Implausibility Driven Evolutionary Monte Carlo for Eﬃcient
Generation of Uniform and Optimal Designs for Multi-wave Computer Experiments.” arXiv
preprint arXiv:1309.3520.

Aﬃliation:
Andrew Iskauskas
Department of Mathematical Sciences
Mathematical Sciences & Computing Science Building
Durham University
Upper Mountjoy Campus
Stockton Road
Durham DH1 3LE
E-mail: andrew.iskauskas@durham.ac.uk

