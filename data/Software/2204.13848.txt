Repro: An Open-Source Library for Improving the Reproducibility and
Usability of Publicly Available Research Code

Daniel Deutsch and Dan Roth
Department of Computer and Information Science
University of Pennsylvania
{ddeutsch,danroth}@seas.upenn.edu

Abstract

We introduce Repro, an open-source library
which aims at improving the reproducibility
and usability of research code. The library
provides a lightweight Python API for run-
ning software released by researchers within
Docker containers which contain the exact re-
quired runtime conﬁguration and dependen-
cies for the code. Because the environment
setup for each package is handled by Docker,
users do not have to do any conﬁguration them-
selves. Once Repro is installed, users can
run the code for the 30+ papers currently sup-
ported by the library. We hope researchers see
the value provided to others by including their
research code in Repro and consider adding
support for their own research code.1

1

Introduction

Running the code released by the original authors
of a research paper can be difﬁcult. There are of-
ten challenges in replicating the required runtime
environment due to installing the correct versions
of external libraries or putting resource ﬁles in the
correct locations. Further, the software packages
may not have an easy-to-use API, making it hard
to ﬁgure out how to use the released code.

In this work, we describe Repro, an open-source
library which aims at improving the reproducibility
and usability of publicly available research code.
Repro is a library of lightweight Python wrap-
pers around code released by the authors of pa-
pers which provide a simple interface for running
the original code without users needing to main-
tain or setup the necessary runtime environments
themselves.

Repro achieves this using Docker, a platform
for packaging together software applications along
with all of the necessary dependencies. Each of
the papers supported in Repro has a corresponding

1https://github.com/danieldeutsch/

repro

Docker image that contains the exact runtime en-
vironment and dependencies for the code. Then,
Repro exposes a simple Python API for passing
data to the Docker containers, processing the data
with the original code, and returning the output to
the user.

Since each codebase’s dependencies are fully
contained within the Docker images, users of Repro
do not need to put any effort into setting up the
correct environment to run code from a paper. Once
Repro is installed, users can easily run any of the
code from the 30+ papers supported by the library.
This paper describes why Docker is an ideal
platform for releasing reproducible code (§2), the
details about how Repro is implemented (§3), a
discussion of the best practices for ensuring repro-
ducible code we have learned from developing the
library (§4), and some of its limitations (§5). We
hope that users see the value Repro provides and
consider contributing Docker images and Python
wrappers for their own papers’ code.

2 Docker as a Tool for Reproducibility

2.1 Background
Repro is built on top of Docker.2 Docker is a tool
for packaging together software applications into
isolated, standalone environments that contain all
of the dependencies necessary to run the applica-
tions. The environments, called Docker images,
can specify which operating system is used, which
version of software libraries are installed, and can
include data ﬁles.

Docker images are built using Dockerﬁles.
Dockerﬁles are text ﬁles with a speciﬁc syntax that
contain a series of commands which are executed
in sequence to build the image. They begin with a
base image, such as a speciﬁc version of Ubuntu,
followed by commands that can install software
libraries, copy local data ﬁles into the image, etc.

2https://www.docker.com/

2
2
0
2

r
p
A
9
2

]
L
C
.
s
c
[

1
v
8
4
8
3
1
.
4
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Once an image is built, it can be run as a Docker
container, which is similar to a virtual machine.
The container allows the user to run software in
or interact with the environment speciﬁed by the
image from their own host machine. However,
any modiﬁcations made within the container do
not persist when the container terminates; Every
time a container is created, it starts with a fresh
environment deﬁned by the image.

Importantly, Docker images and containers are
platform independent. If two different machines
run the same container, the containers’ environ-
ments will be identical up to differences in the
machines’ hardware.

Docker images can be easily distributed for oth-
ers to use through an image registry server such as
DockerHub.3 Users can upload their image binaries
to DockerHub that others can then download and
run, analogous to how GitHub is used to distribute
software code. Thus, once a developer creates a
Docker image, it is easy for others to replicate that
exact runtime environment on their own machines.

2.2 The Advantages of Docker for

Reproducibility

It is often challenging and time consuming to repro-
duce results from research papers. Papers that pub-
licly release code often include links to download
pre-trained models or resources and written instruc-
tions for replicating the necessary runtime environ-
ment for their software. However, it is not uncom-
mon for the environments to be under-speciﬁed or
not speciﬁed at all, for paths to resource ﬁles to
be hard-coded based to locations on the author’s
machine, for the link to the pre-trained model or re-
quired resources to be broken, etc. These problems
can be exacerbated over time when information
about the original environment conﬁguration is for-
gotten or deleted entirely, making the code difﬁcult
to run.

Docker offers a solution to many of the com-
mon problems researchers encounter when they
try to reproduce a result from a paper using re-
sources released by the authors.
In addition to
releasing code and dependencies, authors could
also release Docker images along with their pa-
pers that is the exact environment necessary to run
their code. Other researchers would then be able to
run the code without having to worry about exact
library versions or the locations of pre-trained mod-

3https://hub.docker.com/

els since these details would be taken care of by
the Docker image. Then, if the images were stored
on a public Docker image registry, the environment
necessary to run the code associated with the paper
would exist indeﬁnitely.

Thus, the advantages of Docker make it an ideal
platform for building a library focused on the re-
producibility of research code.

3 Repro

Repro is a Python-based library that aims at improv-
ing the reproducibility and usability of research
code released along with papers. It provides easy-
to-use Python APIs for running the original code
released by the authors, which can be done from
a single, lightweight Python environment. Once
Repro is installed, users can run the code from any
paper supported by Repro without any additional
setup effort.

Improving Reproducibility Every codebase
supported by Repro is packaged into its own
Docker image, which contains the original source
code, runtime environment, and necessary depen-
dencies. Repro provides a lightweight Python
wrapper around the Docker image which facili-
tates launching a Docker container, transferring
data to the container, running the original code in
the original environment, and returning the results
to the user. Because the paper code is run within a
Docker container, that runtime environment will be
the same for all users of the library. Thus, the en-
vironment conﬁguration which reproduces results
from the original paper can be replicated for all
Repro users, improving the reproducibility of the
original research.

Improving Usability Because the papers’ code
runs in Docker images, users of Repro do not need
to maintain the runtime environments or dependen-
cies themselves since they are encapsulated within
the images. For instance, they do not have to cre-
ate a Python environment speciﬁc to a codebase,
install the software dependencies, or download any
resource ﬁles, such as pre-trained models, to spe-
ciﬁc locations. All of this is taken care of by the
Docker images, and thus Repro makes running the
original research code far easier than before.

The only environment the users need to maintain
is the one which Repro is installed in. However,
since Repro’s wrappers around the Docker images
do not have difﬁcult-to-install dependencies, Re-

from repro. models . lewis2020 import BART

model = BART( device =0)

summary : str = model. predict (

" Serena Williams is an American "

" professional tennis player ..."

)

Figure 1: An example code snippet for generat-
ing a summary with BART (Lewis et al., 2020).
model.predict() launches the Docker container that
contains the runtime environment for BART, uses a
pre-trained BART model to generate a summary of the
input text, and returns the result as a string type to
summary.

pro’s required Python environment from which all
of the supported papers’ code can be run is very
lightweight.

Figure 1 contains an example of how a user can
easily run BART (Lewis et al., 2020) to generate a
summary of an input document. The BART model
corresponds to a Python class that provides a func-
tion to run inference and return the summary. Be-
hind the scenes, Repro launches the Docker con-
tainer which contains the original code and models
released by the BART authors, passes the input to
the container, runs the original code in the container
to produce the summary, and returns the result to
the user in the original Python process. All of
this processing is hidden from the user, so BART’s
Python API looks like any other standard Python
function.

The API for each paper depends on what the
code does. For instance, reference-based text gen-
eration metrics will accept a text to score and some
references, question-answering models will take
some input text and a question, etc. We have tried
to standardize the inputs and outputs formats for the
same types of models so it will be easier to quickly
run multiple papers’ code on the same inputs.

Installation & Running The library itself is
lightweight and has minimal dependencies. Our
goal is to make it as easy-as-possible to install,
which can be done by Python’s pip package man-
ager. Running Repro requires a host machine with
Docker installed.

Communication with Docker Exchanging data
between the host machine’s Python process and
the Docker container is done via the machine’s ﬁle
system. First, the Python process serializes the

data which needs to be processed to a directory on
the host machine. Then, the process launches the
Docker container and mounts that directory to the
container, which gives the container the ability to
read and write to the ﬁle system of the host machine.
The Python process executes a command within
the container to process the data, and the container
serializes the result to the same mounted directory
and terminates. The process then loads the results
and returns them to the user.

While this communication with Docker may
sound complex, it is entirely hidden from users of
Repro. The Python API uses normal Python types
as inputs and outputs even though the data process-
ing is largely done in Docker. As such, it looks
the same as a standard Python function. Therefore,
users do not need to know how to use Docker in
order to use the library.

Distributing Docker Images All of the Docker
images supported by Repro are hosted in Docker-
Hub and have corresponding Dockerﬁles in Repro.
When a new Dockerﬁle is added to Repro, a GitHub
Action4 is triggered which builds an image from
the Dockerﬁle and pushes it to DockerHub.

If a user attempts to run code in a Docker con-
tainer for which the corresponding image is not
present on their machine, Repro will automatically
download that image for them. The images can be
manually downloaded from DockerHub as well.

Papers Implemented in Repro As of this writ-
ing, there is code from 30+ papers supported by
Repro. The majority of them are related to eval-
uating generated text based on our own research
interests, but there are also models for text summa-
rization, question-answering, question generation,
constituency parsing, and more. See Appendix A
for the full list.

Once a user installs Repro on a machine with
Docker, all 30+ of the codebases can be run without
any additional setup. We are continually support-
ing more papers and welcome contributions by the
research community.

Contributing We hope that the research commu-
nity sees the beneﬁts of Repro and contributes their
own Docker images to the library for others to use.
Because many people within the NLP community
may not have experience using Docker, our GitHub
repository contains tutorials that explain how to in-
stall Docker, list basic Docker concepts and useful

4https://github.com/features/actions

commands, and provide instructions for packaging
a codebase into a Docker image. We additionally
explain how to write the Repro wrapper around the
Docker image.

4 Reproducibility Best Practices

During the process of building Docker containers
for various libraries, we have identiﬁed several best
practices that future researchers can use to improve
the ease-of-use of faithfully running their code as
intended.

First, example inputs and expected outputs
should be included in the code’s documentation
to help to ensure that reproductions are faithful to
the original implementation.

The exact programming language environment
should be speciﬁed in the documentation. For ex-
ample, this could include the version of Python
as well as the versions of the Python packages
that were used in the original environment. In the
case of Python, package managers such as pip and
conda provide tools for exporting a list of installed
packages to a ﬁle for distributing to others.

External resources, such as data ﬁles or pre-
trained models, should be stored in locations which
are not likely to be moved or deleted. Anecdotally,
we have found that ﬁles stored in locations owned
by organizations (e.g., universities or companies)
are more stable than those stored in individuals’
personal storage platforms (e.g., Google Drive).

Authors should provide a command line inter-
face for running their code end-to-end that accepts
a ﬁle(s) as input and writes a ﬁle as output instead
of a series of scripts which need to be run in series
to process the data. This makes it easier for other
authors to run the code on their own data as well as
integrate it into the Repro library because it makes
the code easier to use.

5 Hardware Limitations

Although Repro signiﬁcantly improves the ease of
reproducing the correct runtime environment and
the usability of research code, the library has some
limitations surrounding hardware compatibility.

Docker containers are only identical across ma-
chines up to differences in hardware. As new hard-
ware is released, it may not be compatible with
older software, which could potentially make it dif-
ﬁcult to run older Docker images. For example,
a new GPU may require software to use a mini-
mum version of CUDA, and a model may use a

speciﬁc version of PyTorch (Paszke et al., 2019)
that is incompatible with any version of CUDA that
can be used with the GPU. Thus, the model will
not run. However, this issue can be mitigated by
the authors updating their code and Docker images
to be compatible with current hardware.

6 Related Work

There are various other software libraries which
aim at making running a variety of research
The SacreBLEU (Post, 2018),
code easier.
SacreROUGE (Deutsch and Roth, 2020), Hugging-
face Datasets (Lhoest et al., 2021), and the GEM
metrics5 libraries provide wrappers around or im-
plementations of various text generation evaluation
metrics in order to establish standardized imple-
mentations and make the metrics easier to run. Li-
braries such as AllenNLP (Gardner et al., 2018)
or Transformers (Wolf et al., 2020) provide frame-
works for which researchers can train deep learn-
ing models. Once someone is familiar with one of
these frameworks, running a trained model which
uses them is relatively straightforward since mod-
els within these frameworks typically use similar
APIs.

They key difference between these approaches
and Repro is how the environments for the models
or metrics are maintained. These libraries attempt
to either have one complex runtime environment
for all models/metrics they support or have a new
environment for each one. Repro instead has one
lightweight Python environment for the codebase
wrappers and uses Docker to maintain the paper-
speciﬁc runtime environments, making the research
code included in Repro very easy to use since the
environments do not need to be maintained by the
library users.

7 Conclusion

We have introduced Repro, a library built on
Docker that aims at improving the reproducibility
and usability of research code. We hope that other
researchers see how the library makes running their
code more accessible to others and consider con-
tributing their own Docker images.

5https://github.com/GEM-benchmark/

GEM-metrics

References

Anthony Chen, Gabriel Stanovsky, Sameer Singh, and
Matt Gardner. 2020. MOCHA: A Dataset for Train-
ing and Evaluating Generative Reading Comprehen-
In Proceedings of the 2020 Confer-
sion Metrics.
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 6521–6532, Online. As-
sociation for Computational Linguistics.

Pierre Colombo, Chloe Clave, and Pablo Piantanida.
2021a.
InfoLM: A New Metric to Evaluate
Summarization & Data2Text Generation. ArXiv,
abs/2112.01589.

Pierre Colombo, Guillaume Staerman, Chloé Clavel,
and Pablo Piantanida. 2021b. Automatic Text Eval-
uation through the Lens of Wasserstein Barycenters.
In Proceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing, pages
10450–10466, Online and Punta Cana, Dominican
Republic. Association for Computational Linguis-
tics.

Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Speciﬁc Translation Evalua-
tion for Any Target Language. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 376–380, Baltimore, Maryland, USA. Associ-
ation for Computational Linguistics.

Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth.
2021. Towards Question-Answering as an Auto-
matic Metric for Evaluating the Content Quality of a
Summary. Transactions of the Association for Com-
putational Linguistics, 9:774–789.

Daniel Deutsch and Dan Roth. 2020. SacreROUGE:
An Open-Source Library for Using and Developing
Summarization Evaluation Metrics. In Proceedings
of Second Workshop for NLP Open Source Software
(NLP-OSS), pages 120–125, Online. Association for
Computational Linguistics.

Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao
Jiang, and Graham Neubig. 2021. GSum: A General
Framework for Guided Neural Abstractive Summa-
rization. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 4830–4842, Online. Association for
Computational Linguistics.

Liam Dugan, Daphne Ippolito, Arun Kirubarajan, and
Chris Callison-Burch. 2020. RoFT: A Tool for
Evaluating Human Detection of Machine-Generated
In Proceedings of the 2020 Conference on
Text.
Empirical Methods in Natural Language Processing:
System Demonstrations, pages 189–196, Online. As-
sociation for Computational Linguistics.

Esin Durmus, He He, and Mona Diab. 2020. FEQA:
A Question Answering Evaluation Framework for
Faithfulness Assessment in Abstractive Summariza-
In Proceedings of the 58th Annual Meeting
tion.
of the Association for Computational Linguistics,

pages 5055–5070, Online. Association for Compu-
tational Linguistics.

Nicholas FitzGerald, Julian Michael, Luheng He, and
Luke Zettlemoyer. 2018. Large-Scale QA-SRL Pars-
ing. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2051–2060, Melbourne,
Australia. Association for Computational Linguis-
tics.

Yang Gao, Wei Zhao, and Steffen Eger. 2020. SU-
PERT: Towards New Frontiers in Unsupervised
Evaluation Metrics for Multi-Document Summariza-
In Proceedings of the 58th Annual Meeting
tion.
of the Association for Computational Linguistics,
pages 1347–1354, Online. Association for Compu-
tational Linguistics.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-
ters, Michael Schmitz, and Luke Zettlemoyer. 2018.
AllenNLP: A Deep Semantic Natural Language Pro-
In Proceedings of Workshop for
cessing Platform.
NLP Open Source Software (NLP-OSS), pages 1–
6, Melbourne, Australia. Association for Computa-
tional Linguistics.

Tanya Goyal and Greg Durrett. 2020. Evaluating Fac-
tuality in Generation with Dependency-level Entail-
ment. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020, pages 3592–3603,
Online. Association for Computational Linguistics.

Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and
Matt Gardner. 2020. Neural Module Networks for
In International Conference
Reasoning over Text.
on Learning Representations.

Jack Hessel, Ari Holtzman, Maxwell Forbes, Ro-
nan Le Bras, and Yejin Choi. 2021. CLIPScore:
A Reference-free Evaluation Metric for Image Cap-
tioning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 7514–7528, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.

Hassan Kane, Muhammed Yusuf Kocyigit, Ali Abdalla,
Pelkins Ajanoh, and Mohamed Coulibali. 2020. NU-
BIA: NeUral Based Interchangeability Assessor for
In Proceedings of the 1st Work-
Text Generation.
shop on Evaluating NLG Evaluation, pages 28–37,
Online (Dublin, Ireland). Association for Computa-
tional Linguistics.

Nikita Kitaev, Steven Cao, and Dan Klein. 2019. Mul-
tilingual Constituency Parsing with Self-Attention
In Proceedings of the 57th An-
and Pre-Training.
nual Meeting of the Association for Computational
Linguistics, pages 3499–3505, Florence, Italy. Asso-
ciation for Computational Linguistics.

Wojciech Kryscinski, Bryan McCann, Caiming Xiong,
and Richard Socher. 2020. Evaluating the Factual

Consistency of Abstractive Text Summarization. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 9332–9346, Online. Association for Computa-
tional Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising Sequence-to-Sequence Pre-
training for Natural Language Generation, Transla-
In Proceedings of the
tion, and Comprehension.
58th Annual Meeting of the Association for Compu-
tational Linguistics, pages 7871–7880, Online. As-
sociation for Computational Linguistics.

Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario Šaško, Gun-
jan Chhablani, Bhavitvya Malik, Br, Simon eis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,
Sylvain Gugger, Clément Delangue, Théo Matus-
sière, Lys Debut, re, Stas Bekman, Pierric Cis-
tac, Thibault Goehringer, Victor Mustar, François
Lagunas, Alex Rush, er, and Thomas Wolf. 2021.
Datasets: A Community Library for Natural Lan-
guage Processing. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing: System Demonstrations, pages 175–
184, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.

Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summariza-
tion Branches Out, pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.

Yang Liu and Mirella Lapata. 2019. Text Summariza-
In Proceedings of
tion with Pretrained Encoders.
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,
China. Association for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
In Proceed-
Evaluation of Machine Translation.
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318,
Philadelphia, Pennsylvania, USA. Association for
Computational Linguistics.

Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. PyTorch:
An Imperative Style, High-Performance Deep Learn-
In H. Wallach, H. Larochelle,
ing Library.

A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 32, pages 8024–8035. Curran Asso-
ciates, Inc.

Matt Post. 2018. A Call for Clarity in Reporting BLEU
Scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
191, Belgium, Brussels. Association for Computa-
tional Linguistics.

Valentina Pyatkin, Paul Roit, Julian Michael, Yoav
Goldberg, Reut Tsarfaty, and Ido Dagan. 2021. Ask-
ing It All: Generating Contextualized Questions for
In Proceedings of the 2021
any Semantic Role.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1429–1441, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.

Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A Neural Framework for MT
Evaluation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 2685–2702, Online. Associa-
tion for Computational Linguistics.

Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,
Benjamin Piwowarski, Jacopo Staiano, Alex Wang,
and Patrick Gallinari. 2021. QuestEval: Summariza-
In Proceed-
tion Asks for Fact-based Evaluation.
ings of the 2021 Conference on Empirical Methods
in Natural Language Processing, pages 6594–6604,
Online and Punta Cana, Dominican Republic. Asso-
ciation for Computational Linguistics.

Thomas Scialom, Sylvain Lamprier, Benjamin Pi-
wowarski, and Jacopo Staiano. 2019. Answers
Unite! Unsupervised Metrics for Reinforced Sum-
marization Models. In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 3246–3256, Hong Kong, China. As-
sociation for Computational Linguistics.

Thibault Sellam, Dipanjan Das, and Ankur Parikh.
2020. BLEURT: Learning Robust Metrics for Text
In Proceedings of the 58th Annual
Generation.
Meeting of the Association for Computational Lin-
guistics, pages 7881–7892, Online. Association for
Computational Linguistics.

Guillaume Staerman, Pavlo Mozharovskyi, Pierre
Colombo, Stéphan Clémençon,
and Florence
d’Alché Buc. 2021. A Pseudo-Metric between Prob-
ability Distributions based on Depth-Trimmed Re-
gions.

Raymond Hendy Susanto, Hai Leong Chieu, and Wei
Lu. 2016. Learning to Capitalize with Character-
Level Recurrent Neural Networks: An Empirical
In Proceedings of the 2016 Conference on
Study.
Empirical Methods in Natural Language Process-
ing, pages 2090–2095, Austin, Texas. Association
for Computational Linguistics.

Brian Thompson and Matt Post. 2020. Automatic Ma-
chine Translation Evaluation in Many Languages via
Zero-Shot Paraphrasing. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 90–121, Online.
Association for Computational Linguistics.

Oleg Vasilyev, Vedant Dharnidharka, and John Bohan-
non. 2020. Fill in the BLANC: Human-free quality
estimation of document summaries. In Proceedings
of the First Workshop on Evaluation and Compari-
son of NLP Systems, pages 11–20, Online. Associa-
tion for Computational Linguistics.

Thomas Wolf, Lys Debut, re, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cis-
tac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma,
Yacine Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, Alex Rush, and er. 2020. Transformers:
State-of-the-Art Natural Language Processing.
In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 38–45, Online. Association
for Computational Linguistics.

Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
BARTScore: Evaluating Generated Text as Text
Generation. ArXiv, abs/2106.11520.

Shiyue Zhang and Mohit Bansal. 2021. Finding a Bal-
anced Degree of Automation for Summary Evalu-
In Proceedings of the 2021 Conference on
ation.
Empirical Methods in Natural Language Processing,
pages 6617–6632, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. BERTScore:
Evaluating Text Generation with BERT. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net.

Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-
tian M. Meyer, and Steffen Eger. 2019. MoverScore:
Text Generation Evaluating with Contextualized Em-
beddings and Earth Mover Distance. In Proceedings
of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 563–578, Hong
Kong, China. Association for Computational Lin-
guistics.

A List of Supported Papers

• the constituency parser from Kitaev et al.

(2019)

• the recipe generator from Dugan et al. (2020)

• the truecaser from Susanto et al. (2016)

• the QA-SRL parser from FitzGerald et al.

(2018)

The following is a list of papers with publicly avail-
able code that have implementations in Repro:

• BART (Lewis et al., 2020)

• BARTScore (Yuan et al., 2021)

• BERTScore (Zhang et al., 2020)

• BaryScore (Colombo et al., 2021b)

• BertSumExtAbs (Liu and Lapata, 2019)

• BLANC (Vasilyev et al., 2020)

• BLEU (Papineni et al., 2002)

• BLEURT (Sellam et al., 2020)

• CLIPScore (Hessel et al., 2021)

• COMET (Rei et al., 2020)

• DAE (Goyal and Durrett, 2020)

• DepthScore (Staerman et al., 2021)

• FactCC (Kryscinski et al., 2020)

• FEQA (Durmus et al., 2020)

• GSum (Dou et al., 2021)

• InfoLM (Colombo et al., 2021a)

• LERC (Chen et al., 2020)

• Lite3Pyramid (Zhang and Bansal, 2021)

• Meteor (Denkowski and Lavie, 2014)

• MoverScore (Zhao et al., 2019)

• NMN-Drop (Gupta et al., 2020)

• NUBIA (Kane et al., 2020)

• Prism (Thompson and Post, 2020)

• QAEval (Deutsch et al., 2021)

• QuestEval (Scialom et al., 2021)

• ROUGE (Lin, 2004)

• SummaQA (Scialom et al., 2019)

• SUPERT (Gao et al., 2020)

• the question generation model from Pyatkin

et al. (2021)

