Scientiﬁc Computing Plan for the ECCE Detector at the Electron Ion Collider

J. C. Bernauer55,56,57, C. T. Dean32, C. Fanelli37, J. Huang4, K. Kauder4, D. Lawrence28, J.D. Osborn49, C. Paus37, J. K. Adkins36,
Y. Akiba53, A. Albataineh69, M. Amaryan47, I. C. Arsene73, C. Ayerbe Gayoso38, J. Bae61, X. Bai78, M.D. Baker4,28,
M. Bashkanov87, R. Bellwied67, F. Benmokhtar14, V. Berdnikov12, F. Bock49, W. Boeglin16, M. Borysova83, E. Brash10,
P. Brindza28, W. J. Briscoe20, M. Brooks32, S. Bueltmann47, M. H. S. Bukhari27, A. Bylinkin69, R. Capobianco66, W.-C. Chang2,
Y. Cheon59, K. Chen7, K.-F. Chen46, K.-Y. Cheng40, M. Chiu4, T. Chujo76, Z. Citron1, E. Cline55,56, E. Cohen44, T. Cormier49,
Y. Corrales Morales32, C. Cotton78, J. Crafts12, C. Crawford70, S. Creekmore49, C.Cuevas28, J. Cunningham49, G. David4,
M. Demarteau49, S. Diehl66, N. Doshita85, R. Dupr´e23, J. M. Durham32, R. Dzhygadlo19, R. Ehlers49, L. El Fassi38, A. Emmert78,
R. Ent28, R. Fatemi70, S. Fegan87, M. Finger8, M. Finger Jr.8, J. Frantz48, M. Friedman22, I. Friscic88, D. Gangadharan67,
S. Gardner18, K. Gates18, F. Geurts52, R. Gilman54, D. Glazier18, E. Glimos49, Y. Goto53, N. Grau3, S. V. Greene79, A. Q. Guo25,
L. Guo16, S. K. Ha86, J. Haggerty4, T. Hayward66, X. He17, O. Hen37, D. W. Higinbotham28, M. Hoballah23, T. Horn12,
A. Hoghmrtsyan1, P.-h. J. Hsu45, G. Huber74, A. Hutson67, K. Y. Hwang87, C. Hyde47, M. Inaba64, T. Iwata85, H.S. Jo31, K. Joo66,
N. Kalantarians81, G. Kalicy12, K. Kawade60, S. J. D. Kay74, A. Kim66, B. Kim61, C. Kim51, M. Kim53, Y. Kim51, Y. Kim59,
E. Kistenev4, V. Klimenko66, S. H. Ko58, I. Korover37, W. Korsch70, G. Krintiras69, S. Kuhn47, C.-M. Kuo40, T. Kutz37, J. Lajoie26,
S. Lebedev26, H. Lee61, J. S. H. Lee75, S. W. Lee31, Y.-J. Lee37, W. Li52, W. Li55,56,84, X. Li9, X. Li32, Y. T. Liang25, S. Lim51,
C.-h. Lin2, D. X. Lin25, K. Liu32, M. X. Liu32, K. Livingston18, N. Liyanage78, W.J. Llope82, C. Loizides49, E. Long72, R.-S. Lu46,
Z. Lu9, W. Lynch87, D. Marchand23, M. Marcisovsky13, P. Markowitz16, H. Marukyan1, P. McGaughey32, M. Mihovilovic71,
R. G. Milner37, A. Milov83, Y. Miyachi85, A. Mkrtchyan1, P. Monaghan10, R. Montgomery18, D. Morrison4, A. Movsisyan1,
H. Mkrtchyan1, A. Mkrtchyan1, C. Munoz Camacho23, M. Murray69, K. Nagai32, J. Nagle65, I. Nakagawa53, C. Nattrass77,
D. Nguyen28, S. Niccolai23, R. Nouicer4, G. Nukazuka53, M. Nycz78, V. A. Okorokov43, S. Oreˇsi´c74, C. O’Shaughnessy32,
S. Paganis46, Z Papandreou74, S. F. Pate42, M. Patel26, G. Penman18, M. G. Perdekamp68, D. V. Perepelitsa65,
H. Periera da Costa32, K. Peters19, W. Phelps10, E. Piasetzky62, C. Pinkenburg4, I. Prochazka8, T. Protzman34, M. L. Purschke4,
J. Putschke82, J. R. Pybus37, R. Rajput-Ghoshal28, J. Rasson49, B. Raue16, K. Read49, K. Røed73, R. Reed34, J. Reinhold16,
E. L. Renner32, J. Richards66, C. Riedl68, T. Rinn4, J. Roche48, G. M. Roland37, G. Ron22, M. Rosati26, C. Royon69, J. Ryu51,
S. Salur54, N. Santiesteban37, R. Santos66, M. Sarsour17, J. Schambach49, A. Schmidt20, N. Schmidt49, C. Schwarz19,
J. Schwiening19, R. Seidl53, A. Sickles68, P. Simmerling66, S. Sirca71, D. Sharma17, Z. Shi32, T.-A. Shibata41, C.-W. Shih40,
S. Shimizu53, U. Shrestha66, K. Slifer72, K. Smith32, D. Sokhan18,24, R. Soltz35, W. Sondheim32, J. Song9, J. Song51,
I. I. Strakovsky20, P. Steinberg4, P. Stepanov12, J. Stevens84, J. Strube50, P. Sun9, X. Sun7, K. Suresh74, V. Tadevosyan1,
W.-C. Tang40, S. Tapia Araya26, S. Tarafdar79, L. Teodorescu5, A. Timmins67, L. Tomasek13, N. Trotta66, R. Trotta12,
T. S. Tveter73, E. Umaka26, A. Usman74, H. W. van Hecke32, C. Van Hulse23, J. Velkovska79, E. Voutier23, P.K. Wang23,
Q. Wang69, Y. Wang7, Y. Wang63, D. P. Watts87, N. Wickramaarachchi12, L. Weinstein47, M. Williams37, C.-P. Wong32, L. Wood50,
M. H. Wood6, C. Woody4, B. Wyslouch37, Z. Xiao63, Y. Yamazaki30, Y. Yang39, Z. Ye63, H. D. Yoo87, M. Yurov32, N. Zachariou87,
W.A. Zajc11, J. Zhang78, Y. Zhang63, Y. X. Zhao25, X. Zheng78, P. Zhuang63

1A. Alikhanyan National Laboratory, Yerevan, Armenia
2Institute of Physics, Academia Sinica, Taipei, Taiwan
3Augustana University, Sioux Falls, , SD, USA
4Brookhaven National Laboratory, Upton, 11973, NY, USA
5Brunel University London, Uxbridge, , UK
6Canisius College, 2001 Main St., Buﬀalo, 14208, NY, USA
7Central China Normal University, Wuhan, China
8Charles University, Prague, Czech Republic
9China Institute of Atomic Energy, Fangshan, Beijing, China
10Christopher Newport University, Newport News, , VA, USA
11Columbia University, New York, , NY, USA
12Catholic University of America, 620 Michigan Ave. NE, Washington DC, 20064, USA
13Czech Technical University, Prague, Czech Republic
14Duquesne University, Pittsburgh, , PA, USA
15Duke University, , , NC, USA
16Florida International University, Miami, , FL, USA
17Georgia State University, Atlanta, , GA, USA
18University of Glasgow, Glasgow, , UK
19GSI Helmholtzzentrum fuer Schwerionenforschung, Planckstrasse 1, Darmstadt, 64291, Germany
20The George Washington University, Washington, DC, 20052, USA
21Hampton University, Hampton, , VA, USA
22Hebrew University, Jerusalem, , Isreal
23Universite Paris-Saclay, CNRS/IN2P3, IJCLab, Orsay, France
24IRFU, CEA, Universite Paris-Saclay, Gif-sur-Yvette France
25Chinese Academy of Sciences, Lanzhou, , , China

2
2
0
2

y
a
M
7
1

]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[

1
v
7
0
6
8
0
.
5
0
2
2
:
v
i
X
r
a

Preprint submitted to Nuclear Instruments and Methods A

May 19, 2022

 
 
 
 
 
 
26Iowa State University, , , IA, USA
27Jazan University, Jazan, Sadui Arabia
28Thomas Jeﬀerson National Accelerator Facility, 12000 Jeﬀerson Ave., Newport News, 24450, VA, USA
29James Madison University, , , VA, USA
30Kobe University, Kobe, Japan
31Kyungpook National University, Daegu, Republic of Korea
32Los Alamos National Laboratory, , , NM, USA
33Lawrence Berkeley National Lab, Berkeley, , , USA
34Lehigh University, Bethlehem, , PA, USA
35Lawrence Livermore National Laboratory, Livermore, , CA, USA
36Morehead State University, Morehead, , KY,
37Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, 02139, MA, USA
38Mississippi State University, Mississippi State, , MS, USA
39National Cheng Kung University, Tainan, , Taiwan
40National Central University, Chungli, Taiwan
41Nihon University, Tokyo, Japan
42New Mexico State University, Physics Department, Las Cruces, NM, 88003, USA
43National Research Nuclear University MEPhI, Moscow, 115409, Russian Federation
44Nuclear Research Center - Negev, Beer-Sheva, Isreal
45National Tsing Hua University, Hsinchu, Taiwan
46National Taiwan University, Taipei, Taiwan
47Old Dominion University, Norfolk, , VA, USA
48Ohio University, Athens, 45701, OH, USA
49Oak Ridge National Laboratory, PO Box 2008, Oak Ridge, 37831, TN, USA
50Paciﬁc Northwest National Laboratory, Richland, , WA, USA
51Pusan National University, Busan, Republic of Korea
52Rice University, P.O. Box 1892, Houston, 77251, TX, USA
53RIKEN Nishina Center, Wako, Saitama, Japan
54The State University of New Jersey, Piscataway, , NJ, USA
55Center for Frontiers in Nuclear Science, Stony Brook, 11794, NY, USA
56Stony Brook University, 100 Nicolls Rd., Stony Brook, 11794, NY, USA
57RIKEN BNL Research Center, Upton, 11973, NY, USA
58Seoul National University, Seoul, Republic of Korea
59Sejong University, Seoul, Republic of Korea
60Shinshu University, Matsumoto, Nagano, Japan
61Sungkyunkwan University, Suwon, Republic of Korea
62Tel Aviv University, P.O. Box 39040, Tel Aviv, 6997801, Israel
63Tsinghua University, Beijing, China
64Tsukuba University of Technology, Tsukuba, Ibaraki, Japan
65University of Colorado Boulder, Boulder, 80309, CO, USA
66University of Connecticut, Storrs, , CT, USA
67University of Houston, Houston, , TX, USA
68University of Illinois, Urbana, , IL, USA
69Unviersity of Kansas, 1450 Jayhawk Blvd., Lawrence, 66045, KS, USA
70University of Kentucky, Lexington, 40506, KY, USA
71University of Ljubljana, Ljubljana, Slovenia, Ljubljana, , , Slovenia
72University of New Hampshire, Durham, , NH, USA
73University of Oslo, Oslo, Norway
74 University of Regina, Regina, , SK, Canada
75University of Seoul, Seoul, Republic of Korea
76University of Tsukuba, Tsukuba, Japan
77University of Tennessee, Knoxville, 37996, TN, USA
78University of Virginia, Charlottesville, , VA, USA
79Vanderbilt University, PMB 401807,2301 Vanderbilt Place, Nashville, 37235, TN, USA
80Virginia Tech, Blacksburg, , VA, USA
81Virginia Union University, Richmond, , VA, USA
82Wayne State University, 666 W. Hancock St., Detroit, 48230, MI, USA
83Weizmann Institute of Science, Rehovot, Israel
84The College of William and Mary, Williamsburg, VA, USA
85Yamagata University, Yamagata, Japan
86Yarmouk University, Irbid, Jordan
87Yonsei University, Seoul, Republic of Korea
88University of York, York, UK
89University of Zagreb, Zagreb, Croatia

Abstract

2

The Electron Ion Collider (EIC) is the next generation of precision QCD facility to be built at Brookhaven National Laboratory
in conjunction with Thomas Jeﬀerson National Laboratory. There are a signiﬁcant number of software and computing challenges
that need to be overcome at the EIC. During the EIC detector proposal development period, the ECCE consortium began identi-
fying and addressing these challenges in the process of producing a complete detector proposal based upon detailed detector and
physics simulations. In this document, the software and computing eﬀorts to produce this proposal are discussed; furthermore, the
computing and software model and resources required for the future of ECCE are described.

Contents

1

Introduction

2 Online

3 Oﬄine

4 Oﬀsite Processing

5 Resource Requirements Summary

6 Summary

7 Acknowledgements

1. Introduction

3

3

6

9

11

11

12

The Electron Ion Collider (EIC) is the next generation pre-
cision nuclear physics accelerator complex that is being con-
structed in the United States. The EIC is expected to start pro-
ducing data in the early 2030’s, and is unique as it will collide
high energy polarized electrons with polarized protons and a
wide range of nuclei. As such, it will introduce new paradigms
in large scale nuclear physics experiments. Expected luminosi-
ties at the EIC will reach upwards of 1034 cm−2s−1; conse-
quently, there will be an extremely large data sample to pro-
cess. Recent eﬀorts from modern collider physics experiments
have shown the beneﬁts of near real-time analysis [1, 2]. There-
fore, there is a strong desire to develop software and computing
infrastructure that reliably and quickly processes data for anal-
ysis.

As the EIC will start taking data in nearly a decade, there are
a number of new paradigms that have the opportunity to be ex-
plored in this software R&D phase. An example of such a new
paradigm is the use of Artiﬁcial Intelligence (AI) and machine
learning (ML). The EIC has the unique opportunity to be one of
the ﬁrst large-scale facilities to systematically incorporate and
employ AI, starting from the detector design and R&D phases.
The relevance of AI for the EIC has been highlighted among
the Opportunities for Computing of the EIC Yellow Report [3].
AI potentially permeates all aspects of this Computing Plan; in
fact, it already plays a signiﬁcant role in the design and R&D
phases of the EIC [4] [5].

This document presents a proposed computing plan for the
EIC Comprehensive Chromodynamics Experiment (ECCE) de-
tector at the EIC [3]. This includes estimates of the rates from
the detector, the pipeline for processing and storing the data,
and how the collaboration members will access the data. Soft-
ware systems for monitoring, calibration, reconstruction, and

analysis are discussed. Estimates of the computing and storage
requirements are included. AI detector optimization techniques
are also discussed as this is expected to be a large part of the
computing eﬀort over the next few years. While we attempt to
include some forward thinking plans in this regard, we neces-
sarily do need to rely on past experience with other large exper-
iments such as sPHENIX [6] and LHCb [7] to serve as guides.

2. Online

2.1. Data acquisition

We envision a DAQ system following a streaming readout
paradigm, where all data is collected in an unbiased hardware
trigger-less system. In the following, we will describe the in-
dividual components as well as the overall data ﬂow and band-
width model. An overview of the system is shown in Figure
1.

2.2. DAQ components

Front-end electronics (FEE) modules sit inside or on the
detector.
In most cases, detector-speciﬁc ASICs provide the
data conversion from the analog to digital domain, do zero-
suppression and provide an interface to ﬁber transceivers for
data transport to the counting room. For this, we envision
Front-End Link eXchange (FELIX)-type PCIe-based receiver
cards1 which support a large number of high speed ﬁber links
per card [8]. FELIX is designed for new or upgraded detec-
tors and trigger systems in the ATLAS Phase-I upgrade and
High-Luminosity LHC, and is implemented by server PCs with
commodity network interfaces and PCIe cards with large ﬁeld-
programmable gate arrays (FPGAs) and many high speed serial
ﬁber transceivers.

Since some FEE may not utilize the full bandwidth of a ﬁber
link, cost-eﬀective stream aggregator boards (SABs), based ei-
ther on small FPGAs or COTS multiplexer ICs, can bundle mul-
tiple ﬁber links coming from FEE to a single ﬁber connected to
the FELIX cards.

Because of space and services constraints, or because no suit-
able ASIC is available, some FEEs will connect to Front end
processor (FEP) modules via digital (LVDS) or analog links.
The FPGA and possibly analog to digital converter logic on the
FEP will then generate a data stream suitable for ﬁber transport
to the FELIX cards.

1In the following, we will use “FELIX card” as a stand-in for a successor

board of similar architecture.

3

Detector

Exp. hall

Counting room

on-site/off-site

FEE

FEE

FEE

FEE

FEE

FEE

FEE

FEE

FEE

FEE

FEE

FEE

Fiber

FEP

SAB

LVDS O(5)m
analog O(20)m

Local buffer

Monitoring

JLAB,SDCC, OSG, ...

Figure 1: The DAQ electronics deployment can be roughly divided by their location, with Front End Electronics (FEE) modules on/near the detector; Front End
Processor boards (FEP) which digitize or reformat detector information and Stream Aggregator Boards (SAB), which bundle streams, in the hall; and online ﬁltering
and monitoring in the counting room. Long term storage and analysis processing is performed in a federated model on multiple sites.

In the counting room, we expect a number of special servers
which house the FELIX cards. Each FELIX/CPU combina-
tion sees data from a certain subset of detector channels and
can do additional data reduction before sending out the data to
the counting room CPU farm. This CPU farm (with possibly
GPU accelerators) will do further data reduction, for example
via high level data selection algorithms.

The data streams are buﬀered on local hard disks, with
enough capacity to store the data for several days. This local
buﬀer has multiple functions:

• It averages out the changes in data rate from luminosity
changes so that the upstream link only need to provide av-
erage, not peak, bandwidth.

• It allows stand-alone operation for a limited time when the
data transport out of the counting house is not available or
runs at reduced capacity.

• It allows for near-online monitoring and replay of recent
data for quality control, especially for those quantities
which depend on on-going calibrations.

The data are then pushed downstream to on-site or federated

storage as part of the overall EIC project.

the FELIX host CPUs or on the FELIX card themselves. Neces-
sarily, they are limited in scope to counting, summing or similar
type of information. The slow path provides higher-level infor-
mation for quality control. Here, it’s possible to reconstruct
and analyse full events on-the-ﬂy, by copying pre-selected time
segments from the data stream to a dedicated server that per-
forms full event reconstruction. Note here that such a monitor-
ing system does not require the guaranteed reconstruction of all
data, just of a suitable subset. That subset can either be selected
unbiased by selecting periodic time segments, or biased by se-
lecting time-segments tagged by data ﬁlters in the main data
stream. Similar monitoring can be performed on data on the
local buﬀer for those quantities which require calibration data
or two-pass analysis.

For both types of data a system will be needed to evaluate the
monitoring data and inform the experiment operators of poten-
tial issues. This will largely include the creation of histograms
which may be monitored either graphically or by some auto-
mated means. The prevalence of AI will certainly play a large
role in that it will be able to evaluate a wider variation of moni-
toring data and at a much higher rate than could be expected of
humans. Such systems are already deployed and under devel-
opment [9].

2.3. Online monitoring

Online monitoring is divided into a fast path with bound la-
tency and a slow path. The fast path provides low-latency feed-
back for accelerator steering and equipment protection. The
data for this path are generated early in the DAQ chain, either on

We expect that during initial commissioning noise rates will
be signiﬁcantly higher than during established operation, as
accelerator and detector parameters will not yet be tuned op-
timally. Such high noise rates might overwhelm processing
and upstream write capability. To allow progress in this initial

2.4. Risk mitigation

4

phase, the DAQ system will accept as input a bounded-latency
signal on the FELIX card or host CPU level to suppress unin-
teresting time segments.

Such a system also allows us to simply incorporate a ded-
icated collision detector for rate reduction: only time seg-
ments which are ﬂagged to have a collision are kept, others are
dropped early in the processing chain.

This bounded-latency system could either be realized as a
classic hardware ﬁltering signal, or via software messages sent
to the FELIX hosts with a clear advantage with regard to ﬂex-
ibility and ease of implementation, but with possibly a larger
latency. The optimal implementation depends on the capabili-
ties of the future FELIX successor and bandwidth availability
on the FELIX host servers.

2.5. AI-based data selection

Traditionally, online data selection is performed using ﬁxed
topological cuts such as an energy deposit threshold in a
calorimeter or the minimum transverse momentum of a track.
Online data selection can beneﬁt from the introduction of ML
techniques typically used in oﬄine data selection. It is advanta-
geous to introduce these techniques at this stage to keep events
that would be rejected by conventional methods and thereby in-
crease the overall physics output of a detector. This approach
can also simultaneously reject events that have a level of noise
that would render a physics analysis of this event impractical.

The consortium is constructing a system where these selec-
tion techniques can be realised directly in hardware and will
initially be deployed at sPHENIX which will double as a test
system for an EIC detector. This project involves two symbiotic
AI systems; one to identify collisions containing heavy ﬂavor
decays through their unique topology and another to determine
the beamspot for the detector during operation. The latter sys-
tem then feeds back to the data selection system to improve the
physics eﬃciency.

In the system, the accept decisions would be handled by
a separate FELIX system which aggregates information from
several subdetectors but it should be noted that the choice of
technology could evolve along with general hardware develop-
ments in the next decade. The FELIX has 48 bi-directional links
allowing for data from multiple systems to be passed to a sin-
gle FELIX board. The machine learning algorithm will then
be loaded onto the FPGA which will be capable of basic track-
ing (using tracklets from the vertex, sagitta, and forward silicon
tracking detectors) to make decisions that can be fed back to the
initial DAQ or global data selection system to signal processing
should continue. This should be achieved within 6 µs2.

Studies are ongoing, comparing the outputs of algorithms
trained using Convolution Neural Networks (CNNs) and Graph
Neural Networks (GNN). For many applications, GNNs often
represent a more robust ML algorithm than CNN due to using
additional information from the edges of the graph as well as
the node information used by a traditional CNN. This makes

2This requirement is determined by the shaping time of sPHENIX’s vertex

detector and could diﬀer for ECCE

5

them more applicable to sparse data such as tracking and data
selection where the overall occupancy of the detector system is
low.

It is important to note that the beam conditions can change
with time, this can be both during a run and on a longer scale.
Thus it is imperative that, as well as developing a selection algo-
rithm, we develop a feedback system that is capable of monitor-
ing the beam conditions in real time and adapting the input pa-
rameters to this. For example, the position of the collision point
directly impacts the measurement of the track displacement. If
this moves, it will alter the signal and background eﬃciencies
in opposite directions. This monitoring will be achieved using
GPUs which will feedback to the selection system. GPUs typ-
ically perform well using CNNs which motivates the study of
various machine learning methods to ﬁnd the optimal set of al-
gorithms to use for each stage of the selection and monitoring.
Other important features that can appear in a detector, impact-
ing this system and hence must be monitored are the appear-
ance of noisy channels (or pixels) and displacement of parts of
the detector, such as through thermal expansion or vibrations.
It has been demonstrated that algorithms can run fast enough
on GPUs to achieve this form of monitoring [10].

2.6. Expected data rates and reduction steps

Since connections between detector and FEPs might not be
zero-suppressed digital or even analog data, it makes no sense
to specify a data rate at the detector-to-hall border.
Instead,
the following section describes the expected data rate on the
Fiber/FELIX level and downstream from there.

At nominal luminosity, we expect that true signals and beam-
gas interactions produce a total rate of O(100 Gbps) of zero-
suppressed data at the FELIX card level. However, detector
noise and additional backgrounds, especially during early oper-
ation, can completely dominate this rate. We assume therefore a
total rate of O(10 Tbps) bandwidth on the ﬁber level. Next-gen
FELIX cards will have 25 Gbps receivers; for headroom for
burst rates, non-ideal allocation of detector channels to ﬁbers
etc., we assume 12.5 Gbps as the average rate per ﬁber, and as
a consequence, about 800 ﬁbers.

A current generation FELIX card has ports that support 48
ﬁbers. Assuming the same number of ports, this leads to O(20)
next generation FELIX cards. Non-optimal distribution of FEE
bandwidth across the ﬁbers, the uncertainty in achievable re-
duction rate in the FELIX and achievable out-bound bandwidth
may grow the number of required FELIX cards up to 3 fold
for a total of 60. Each card would then receive 600 Gbps. We
assume that at the time of procurement for EIC, cards based
on at least PCIe Gen5 are available, providing 500 Gbps to the
host server, requiring a modest reduction of the data rate in FE-
LIX card itself, for example via cross-channel noise reduction.
In combination with the host CPU, we expect a total reduction
by a factor of 5 to O(2) Tbps total, 100 Gbps per server. We
note that a typical server with 128GB of memory can buﬀer
the full stream for about 2 seconds, ample time for region-of-
interest/time-slice-of-interest communication between the FE-
LIX hosts, making higher reduction factors comparatively easy

to achieve. The data can then be streamed out via a dual 100
Gbps link to the second layer in the compute farm.

decisions will need to be made soon in preparation for develop-
ment of a TDR.

In the compute farm, the data is further analyzed and ﬁltered.
We expect that with inter-detector noise suppression and high-
level data selection the required eﬀective bandwidth to long-
term storage can be reduced to O(100) Gbps.

3. Oﬄine

Oﬄine software encompasses many aspects of any experi-
ment. This includes a number of systems, each of which re-
quires either new development or implementation of existing
systems using dedicated experts(s). These include:

• Calibration system and database

• Reconstruction framework

• Reconstruction algorithms

• Simulation

• Oﬄine Monitoring system

• Reconstruction workﬂow (HPC/HTC job management)

We intend to develop an oﬄine computing model that aims
for “real time analysis” that performs a single reconstruction
pass on the data, producing reduced DSTs that are available
for physics analysis on the time scale of a few weeks. In this
description, the single reconstruction pass includes any relevant
calibrations that are determined from speciﬁc calibration data
sets.

In the following sections we describe some of the above sys-
tems that will constitute larger eﬀorts in terms of person-hours.
It should be noted that at this time certain technology choices
seem likely (e.g. GEANT4 [11]). However, others such as the
choice of database systems, ﬁle formats, and software frame-
works are purposefully left unspeciﬁed at this point in time. It
is a primary goal post proposal period to deﬁne requirements
and resource needs for these tasks.

3.1. Reconstruction

In the past several decades, many reconstruction frameworks
have been developed by diﬀerent experiments within both HEP
and NP. Several features stand out as common to all of these,
which the ECCE software framework must utilize. The most
important of these are modularity and user friendliness, as any
large HEP/NP collaboration will necessarily comprise many
hundreds of scientists with varying levels of software exper-
tise. Therefore, these, and other generic features of excellent
software, will be essential. It will additionally be imperative
to recognize that software technologies change rapidly, and the
ability for the software ecosystem to pivot with ease will be
essential. As an example, while git is the de facto modern
standard for code versioning and storage, it is impossible to say
what versioning technologies will exist ten or more years from
now when the EIC will be taking data. ECCE has not commit-
ted itself to a particular software ecosystem yet; however, these

6

One of the requirements of reconstruction software is repro-
ducibility. ECCE will archive several daily builds that will
provide users with the latest snapshot of the software; addi-
tionally, weekly builds that persist for longer periods of time
will allow tracking of code evolution. In conjunction, special
tagged production builds will be archived for large centrally
produced data samples, such as those that were produced in
preparation for the ECCE proposal. Currently the tagged re-
leases are performed based only on time (e.g. weekly builds).
Future software versions will consider implementing modern
versioning practices such as semantic versioning [12]. In ad-
dition to archived builds, continuous integration is another tool
ensuring reproducibility. ECCE has deployed continuous in-
tegration in certain repositories; however, automated tools en-
abled by services such as Jenkins or GitLab Runners will be
deployed utilizing code checking tools and benchmark physics
analyses.

Making software user friendly requires that it is distributed
in a convenient way. Currently the ECCE framework is dis-
tributed with cvmfs, a package managing software developed
at CERN [13], while the software environment is containerized
and deployed with Singularity [14]. Any software system that
ECCE decides on will necessitate these tools for distribution,
to ensure that all users can easily access the software and that
a reproducible environment is available when deploying oﬄine
analysis and simulation in a federated computing architecture.
The role of hybrid architectures should also be considered in
the ECCE reconstruction framework. Speciﬁcally, the use of
GPU architectures will be important both for integrating ma-
chine learning into reconstruction workﬂows as well as gener-
ically taking advantage of the signiﬁcant computational speed
improvements that GPUs can provide, for example in charged
particle reconstruction [15]. This integration has the added ben-
eﬁt of potentially utilizing the various leadership computing
facilities that are available at national laboratories around the
country, for more see Section 4.

Based on the experience of other experiments, reconstruc-
tion software should also take advantage of common software
projects that are deployed across the world. For example, the
A Common Tracking Software (ACTS) package, initially de-
signed for use at the HL-LHC, has been implemented into
the sPHENIX track reconstruction framework [16]. Several
collider-physics-based open source projects exist within the
broader HEP community and have recently grown in their user
base, examples include ACTS [17], Rucio [18], PanDA [19],
Fun4All [20], JANA [21], Gaudi [22], and others. These should
be evaluated for use within the ECCE software stack in 2022 as
a part of the decision making process for the future of the oﬄine
software framework.

3.2. Calibration

Timely delivery of high-quality calibrations are one of the
main challenges for EIC experiments, in particular given that
many EIC measurements will be systematic uncertainty driven

[3]. ECCE adopts a ﬁxed-latency production model, which re-
quires the ﬁnal calibration within 2-3 weeks of data taking. This
leads to the design of a semi-automatic calibration workﬂow
with minimal human intervention. There is already ongoing
work to improve calibration workﬂows by integrating AI [23].
Similar to the architecture of the sPHENIX computing
model, we envision an oﬄine computing center will provide a
large incoming data buﬀer (e.g. 20PB as in the sPHENIX) that
allows raw data to be used for reconstruction within 2-3 weeks
of data taking, during which calibration will take place. The
calibration tasks and time scale are dependent on the detector
subsystems.

3.2.1. Track Reconstruction

For hits in the tracking detectors, the amplitude and time oﬀ-
set of each tracker channel will be aligned to a uniform response
using an ensemble of collisions. We expect the initial calibra-
tion to be delivered within two weeks of data taking with fre-
quent checks and updates when needed.

3.2.2. Particle Identiﬁcation

Particle ID requires gain and time calibration. The single-
photon and multi-photon per pixel hit from signal hit and noise
will be used to set the gain. We expect a rapid turnaround
for calibration and monitoring of the gain of approximately
one day. The time oﬀset calibration will be initially set
by calibration-speciﬁc pulse lasers, which are applied before
physics data taking. The ﬁnal alignment requires events with
a high multiplicity of tracks and aligning their projected colli-
sion time by adjusting timing shifts for each sensor, which will
be part of the 4D alignment to be discussed at the end of this
section.

3.2.3. Calorimetry

Calorimetric calibration focuses on the gain calibration. The
ﬁrst order of calorimetric energy scale calibration will be per-
formed during production stage using the calorimeter blocks
and SiPM QA database, e.g.
light yield and gain measure-
ments. The ﬁrst iteration for the calorimetric energy scale will
be based on cosmic data during the construction phase (e.g.
sector testing) and pre-collision cosmic runs. This is expected
to be completed before physics data taking. The second itera-
tion of tower-by-tower energy scale variation calibration will be
matching the energy slope of the calorimeter tower energy spec-
trum for the same eta slice. This is expected to be completed
within one week of physics data taking. The ﬁnal energy scale
iteration will utilize real collision data in several channels. The
ﬁrst is using scattered electrons, π◦ → γγ and η → γγ decays to
set the energy scale for the EMCal. The second is using isolated
hadronic shower to calibrate the e/h in EMCal and hadronic en-
ergy scale in the HCal. The third is using semi-inclusive deep-
inelastic scattering single high momentum jet production to set
the calorimetric jet energy scale. This is expected to take one
week of data (O(100) Billion events) and one week of calibra-
tion. During steady-state running, the tower-by-tower gain drift
will be monitored and calibrated using LED ﬂashes and SiPM

temperature monitoring, which can be calibrated in about one
hour.

3.2.4. Alignment

To fully align the entire detector, each subdetector will be
surveyed before and after installation which provides the start-
ing point of the alignment. The ﬁrst iteration of alignment will
use ﬁeld-oﬀ data and cosmic data to adjust major pieces of the
detector component to the ﬁnal installed location. The time la-
tency needed for this task is limited by the availability of such
specialized data, but we expect this step is completed before
the physics quality data taken at ECCE. The second iteration
requires ﬁeld-on physics quality collision data to provide the ﬁ-
nal high precision adjustment for the sensor locations and time
oﬀset (for TOF) to a small fraction of the resolution. The ﬁrst
period of magnetic-ﬁeld-on collision data will be used for this
alignment. Generic purpose global alignment package such as
Millepede II will be used. Other packages, such as alignment
software available in ACTS [17], can be considered as well.
It is expected to take two weeks to complete the iteration of
alignment and checks. Steady-state updates: the vertex tracker
requires O(1) µm alignment precision, which could change over
long periods. Therefore, during steady-state running, we expect
alignment to be checked every few days and possibly updated
every week, depending on the ﬁnal mechanical stability. We
expect steady-state alignment updates can be achieved within
one hour (e.g.
the LHCb vertex tracker is aligned in about 7
minutes [24]).

3.2.5. Calibration database

For the ECCE streaming DAQ, we expect the calibration
record to be time-stamped with a 64-bit beam crossing counter
with the start and end time corresponding to the interval of va-
lidity. The validity window length will be detector and calibra-
tion dependent, but we expect they align with the luminosity
block of ECCE streaming data that is O(1) second, at each of
the electron ring bunch reﬁlls.

The size of the calibration data is much smaller than the
raw data but still sizable. For the highest channel count in
the silicon vertex tracker (O(1)B channels), we do not expect
to have frequent calibration as it presents boolean (hit/no-hit)
pixel data. As a conservative estimate, consider 300 thou-
sand of the calorimeter, tracker, and PID channels that require
a frequent (1 per minute) update of a relative gain and time
shift, each represented by a 4-byte ﬂoat. This gives an over-
all calibration dataset size of O(1) TB per run year (8B*(300e3
chan)*60(minute)*24(hour)*7(day)*20(cryo week)).

The calibration data will be indexed in a relational database.
The actual calibration data ﬁles can exist in a distributed ﬁle
server (e.g. S3 or XROOTD [25]) or in a separated database
table, depending on the size per entry and frequency of calibra-
tion updates. The separation of index database and calibration
data payload allows for eﬃcient database implementation man-
agement that is capable of accommodating a possible large size
of the calibration data. This approach is being deployed in the
sPHENIX ﬁxed latency calibration and reconstruction.

7

At the start of a production job, the job manager will pull the
calibration data relevant for the job into the local disk buﬀer
according to the index table. Then the database is discon-
nected and job processing starts to eﬃciently utilize the con-
nection limit to the database. This also allows ﬂexibility to pre-
assemble the calibration ﬁle package to be sent with raw data to
remote computing centers that are otherwise disconnected from
the ECCE database and ﬁle servers.

3.3. Simulation

All modern high energy physics experiments require highly
detailed detector simulations, both in the design and operational
phases. The volume of simulations required depends on their
speciﬁc uses, from small scale simulations with hundreds of
events to study new sub-detector systems to large scale simu-
lations with over 100 million events to understand physics ca-
pabilities. With this in mind, the ECCE philosophy towards
simulations is user-friendliness, modularity, and no distinction
between large and small scale simulations to avoid disparities.
The current framework is capable of performing simulations
from the generator stage right up to ﬁnal physics analysis,
with intermediate stages for detector simulation, responses, and
track, PID and calorimeter reconstruction. This beneﬁts users
by allowing them to run a full analysis chain in one step if de-
sired and large scale productions by breaking simulations down
into a series of stages. The latter approach improves throughput
and reproducability as the same generator-level simulation can
be run over diﬀerent detector conﬁgurations or more physics
objects can be added later in time when for example the simu-
lation and reconstruction are run separately. It is expected that
any new framework used will try to retain as much of these fea-
tures as possible.

The framework has several inbuilt event generators (a parti-
cle gun, Pythia6 [26], Pythia8 [27] and Sartre [28]) and can
also read in pre-generated events either via the EIC-smear pro-
gram [29] or a ﬁle in HepMC2 format [30]. The framework is
also capable of reading in any previously produced DST, as-
suming the material hits were saved. If any generated particle
has not been decayed by the input generator and is required
to decay in the detector volume, this is handled by the built-in
Pythia6 decayer.
The detector

likely be handled by
Geant4 [31]. The components of the detector can either be ren-
dered in what is called ”fast” or ”full” simulation. Fast simula-
tion is useful for producing passive volumes such as support or
service structures, or testing ideas quickly. Full simulation will
involve complete physics responses and digitization, including
but not limited to Cherenkov photon production and electron
cascades.

simulation will

Eﬀorts are on-going to improve both our simulations,
through work conducted with AI-assisted detector designs [32],
and the infrastructure needed to produce large simulations on
short time scales such as with distributed computing and GPU
implementations of Geant4. Each individually simulated sub-
system is bundled with the software stack which means it is
also saved weekly and with every production build. This allows
any simulation to be reproduced at a future date, if necessary,

and this ability should be maintained throughout the experiment
lifetime and beyond.

The AI-assisted design optimization of the ECCE inner
tracker [32] has been based on evolutionary algorithms; during
the detector proposal multiple optimization pipelines have been
run each with a population size of 100, representing diﬀerent
detector design conﬁgurations. At each iteration, AI updates
the population. The total computing budget for an individual
pipeline amounted to approximately 10k CPU-core hours. Ac-
tivities are planned to continue the detector optimization: new
optimization pipelines can deal with larger parameter space to
include a system of sub-detectors like in the case of the whole
ECCE tracker [32]; we also plan to optimize other sub-detectors
like, e.g., the d-RICH, leveraging on the expertise internal to
the ECCE collaboration regarding speciﬁcally the design of the
dRICH with AI-based techniques [4]. Larger populations may
need to be simulated to cope with the increased complexity in
order to improve the accuracy of the approximated Pareto front.
Diﬀerent AI-based strategies will be compared. We anticipate
roughly 1M CPU-core hours per year for these AI based stud-
ies.

The ECCE consortium conducted two large scale production
campaigns in 2021; the ﬁrst campaign consisted of over 120M
events while the second campaign consisted of over 600M
events. The campaigns were distributed over 3 distinct produc-
tion sites; SDCC at Brookhaven National Laboratory, the Sci-
Comp at Thomas Jeﬀerson National Accelerator Facility and
MIT Bates Research and Engineering Center. The production
sites used a common top-level program which is able to com-
municate with site-speciﬁc lower level programs. With this and
the common simulation framework, production tasks can be as-
signed to any site and down-time at one site can be recovered
with a diﬀerent site. As the only diﬀerence between the sites
is in the batch systems, each production site is capable of cre-
ating output ﬁles and directories in identical formats and hence
the production location is transparent to end users. Finally, the
simulation seeds are uniquely deﬁned by the input options (in-
put ﬁle name, number of events to generate and starting event
number of the input ﬁle) so any site can precisely reproduce
any ﬁle from the other sites which aids in both debugging and
general event production. As well as the large scale produc-
tion at these three sites, another large production of almost 50M
events was generated using computing resources at Oak Ridge
National Laboratory for calorimetry development. This simula-
tion used a diﬀerent production mechanism from that discussed
previously, demonstrating the ﬂexibility and advantage of using
this modular system.

The second simulation campaign featured a far more mature
detector design with full PID, calorimeter responses, optimal
detector placements and support structures. Thus, this cam-
paign is useful for bench-marking the simulation memory usage
and processing times. By comparing several large productions,
it was found that the average time to produce a single ep event
with a 10 GeV electron beam and 100 GeV proton beam using
the in-built Pythia8 generator is 7.8 s with a standard deviation

8

275 GeV proton beam using an external Pythia6 generator and
the internal EIC-smear reader which found an average event
production time of 9.7 s with a standard deviation of 3.3 s. This
is also reﬂected in the simulation memory usage where the col-
lisions with a 10 GeV electron beam and 100 GeV proton beam
had an average memory usage of 2138 MB with a standard de-
viation of 16 MB while a 18 GeV electron beam and 275 GeV
proton beam had an average memory usage of 2275 MB with a
standard deviation of 32 MB. It is also expected that the overall
memory footprint will be reduced through code optimisation
and new hardware. For example, it has already been demon-
strated in sPHENIX (which shares the same framework) that
the mean memory usage for pp simulations can be reduced
from 4 GB to 1.7 GB by selective loading of simulated materi-
als. Currently, ECCE simulations load every material described
in Geant4 into memory. The distributions for event run time
and memory use are given in Figure 2.

The campaigns performed in 2021 can be used to estimate the
simulation requirements for the forthcoming years. These esti-
mates are given in Tables 1 and 2 for the R&D and data taking
periods respectively. The ﬁrst table assumes that a large produc-
tion will occur in 2022 based on reviewers suggestions which
will steadily decrease as detector R&D progresses for several
years before increasing signiﬁcantly in the years leading to data
taking as the collaboration performs as realistic simulations as
possible to exercise the reconstruction, calibration and align-
ment software.The simulation requirements for the data-taking
period assume that the collaboration will need O(10) times the
amount of simulated data for O(10)% of the streaming recorded
minimum bias cross section in the real data for each running
year that is most relevant for the core physics program at ECCE.
The number of expected real events recorded are listed in Ta-
ble 6, which is comparable to the computing need in the oﬄine
reconstruction as discussed in Section 5.

4. Oﬀsite Processing

The EIC will be a large international project with many re-
searchers and stakeholders spread throughout the globe. While
the accelerator and detectors are necessarily placed at a sin-
gle locale, the computing need not be and can better reﬂect the
geographic diversity of the collaborations involved in EIC re-
search. In the modern age, high speed network connectivity has
become very robust. In the time frame of the EIC (∼ 2030) we
can expect even more reliable and even faster networks. This
will make transporting data on the scale the EIC is expected to
produce fairly routine. To give speciﬁc numbers, BNL has a
400Gbps connection to the ESnet backbone in 2021. ECCE is
currently estimated to produce 100Gbps of raw data once it is
in full production sometime around 2030. Network bandwidths
have shown steady growth of about 50% per year over the past
few decades[33]. Thus, over the next 8 years we can expect
existing bandwidths to grow by roughly a factor of 25. Even a
conservative estimate that the BNL external connection band-
width grows by only a factor of 10 means the entire ECCE raw
data volume can be streamed out using only a few percent of
the total available bandwidth.

Figure 2: The per-event run time (top) and per-job memory usage (bot-
tom) for two diﬀerent productions. ep collisions with a 10 GeV elec-
tron beam and 100 GeV proton beam using an internal Pythia8 gen-
erator are shown with red triangles while ep collisions with a 18 GeV
electron beam and 275 GeV proton beam using an external Pythia6
generator are shown with blue diamonds. As each entry in the run
time is the average time to produce 2000 events the multiple peaks for
each production is due to diﬀerent hardware used to process the jobs
on the batch farm.

of 2.2 s3. This value was obtained by studying approximately
20 million events, grouped into production jobs of 2000 events
each and taking the average run time of each job. Thus, this
number also includes the start up of the framework but this im-
pact should be minimized by using the average of 2000 events.
Similarly, the average memory usage of a job was found to be
stable regardless of the number of events that were produced in
each job. A small variation in run time is seen with respect to
the collision energies which is expected; when the beam ener-
gies increase, the event multiplicity increases and hence there
are more objects to simulate and reconstruct. This can be seen
by simulating ep collisions with a 18 GeV electron beam and

3These events involved generator level production, detector simulation, dig-
itization, reconstruction (track, PID and calorimeter) and physics analysis out-
put

9

02468101214161820Run Time [s]0100200300400500600700800Entries20002050210021502200225023002350240024502500Memory Usage [MB]0200400600800100012001400EntriesYear
2022
2023 - 2024
2025 - 2028
2029 - 2030
Total

Number of Events [×106]
200
100
50
500
1600

Storage [TB] CPU-core hours [Mcore-hrs]

50
25
12.5
125
400

45
22.5
11
110
354

Table 1: Estimated simulation requirements for the years 2022 - 2030. The estimates are based on the observed performance in 2021, only include large scale
productions and hence do not include any productions for AI-assisted detector design. The numbers assume that a large scale campaign will take place in 2022,
based on feedback from the proposal. The productions will then decrease as focus moves into hardware development before increasing signiﬁcantly in the years
before initial data taking as ”Mock Data Challenges” are pursued to test the reconstruction, calibration and alignment software.

Year
year-1
year-2
year-3

Number of Events [×109]
120
600
5400

Storage [PB] CPU-core hours [Mcore-hrs]

30
150
1300

11000
55000
490000

Table 2: Estimated simulation requirements during operational years. The storage and CPU time estimates are based on the observed performance in 2021 while
the number of events assume we will need O(10) times the amount of simulated data for O(10)% of the streaming recorded minimum bias cross section in the real
data for each running year that is most relevant for the core physics program at ECCE

This section will brieﬂy describe how a federated computing
model for the EIC might look, how it will be used to process
the raw data, and how it will also be used to process the large
amounts of simulation needed for the program.

4.1. Federated computing

EIC data processing will employ a federated computing
model where multiple facilities will be used. A similar strat-
egy has been successfully deployed by the LHC in the form
of the Worldwide LHC Computing Grid (WLCG) or simply
the Grid[34]. The beneﬁts of distributing the computing across
multiple sites include:

• Each site only needs to handle a fraction of data

• EIC computing becomes a smaller fraction of each com-

pute farm

• One site having diminished capacity temporarily can eas-

ily be absorbed by others without reconﬁguration

Generally speaking, diversifying helps to mitigate certain

risks.

The WLCG model of the LHC is based on multiple Tiers,
structured in a pyramid type fashion. The topmost tier (Tier
0)represents the LHC/CERN where the data is produced and all
data is stored. It also supplies around 20% of the total com-
puting resource and does the initial reconstruction of the data
before distributing it to the Tier 1 sites[35]. The Tier 1 sites
perform large scale reprocessing of the data and distribution to
the Tier 2 sites. The Tier 2 sites do more specialized analysis
and simulations while the Tier 3 sites are end users. Each tier
in the system only communicates with tiers directly above or
below it in the hierarchy.

For the EIC one may consider an alternative model in which
the data producer (the experiments of the EIC) and the BNL
compute facility (e.g. SDCC) are independent. This allows

computing at BNL to become part of a pool of facilities that
handle the computing as a federated resource. Figure 3 illus-
trates such a model referred to here as a “Butterﬂy” model due
to the rough shape of the ﬁgure. In this model, both compute
and storage are distributed with the storage being focused in the
Echelon 1 sites. This means access to the data by the end users
will be done by connecting Echelon 3 sites directly to Echelon
1 sites. The Echelon 1 sites will themselves provide signiﬁcant
compute capability, but may also farm out large campaigns to
Echelon 2 sites. In the simulation campaigns performed for the
ECCE proposal, the model shown in Fig. 3 was successfully
implemented for simulated data production.

Figure 3: Butterﬂy model of federated oﬀsite computing. In this model, nearly
all storage is contained in echelon 1 while large portions of the raw data pro-
cessing is delegated to multiple HTC/HPC facilities. The named facilities in
this graphic are merely examples and do not represent commitments or ﬁnal
plans.

4.2. Raw data compute

Processing of EIC data will occur over multiple sites which
will include HTC facilities at both BNL and JLab and possibly

10

EICJLABSDCCBATESNERSCOSGUniversityUniversityUniversityUniversityUniversityUniversityUniversityUniversityEchelon 0Echelon 1Echelon 2Echelon 3Echelon 3ORNLothers. The plan calls for processing the raw data into recon-
structed objects such as tracks, jets, and calorimeter clusters
within 2-3 weeks of acquisition. The bulk of the few week la-
tency will be due to the time it takes to calibrate the data so
that reconstruction may occur. Figure 4 illustrates how such
a scheme could work. The raw data read from the streaming
DAQ system will need to be reduced over multiple ﬁltering and
compression stages to a rate that is reasonable to transport oﬀ-
site from BNL using its external network connection. Table 3
lists the stages and with in-going and out-going rates and their
respective reduction factors. Potential technologies that could
be applied at each stage are also listed.

The DOE lab systems are connected via the ESNet unclas-
siﬁed network for scientiﬁc research [36]. As of 2021, BNL
has a 400Gbps connection to ESNet and JLab has dual 10Gbps
connections. In 2022 or 2023, JLab is anticipated to increase its
bandwidth to at least 100Gbps. When the EIC begins collecting
data around 2030, one may expect a 1Tbps bandwidth between
the two labs. This is an order of magnitude higher than the an-
ticipated raw data rate after ﬁltering from the ECCE detector.
Thus, transfer of the entire raw data set oﬀsite from BNL in
2030 seems reasonable.

Figure 4: Data ﬂow from detector to reconstructed object ﬁles (left to right).
This diagram illustrates how raw data may be distributed to multiple sites in
near-real time. On the left side of the plot, multiple ﬁlter and buﬀering stages
are used to reduce the data rate. On the right, the data is distributed to multiple
facilities. Each facility would store a portion of the raw data. It would also need
to keep the data live (e.g. on disk) long enough for it to be calibrated and then
processed by the reconstruction software.

5. Resource Requirements Summary

The EIC luminosity is projected to be between 1033cm−2s−1
and 1034cm−2s−1 (see sec. 2.10 of the Yellow Report [3]). As-
sume 30 weeks of operation per year and 60% accelerator op-
eration eﬃciency once it is in full production mode. In the ﬁrst
years, however, we may expect fewer weeks of running and
lower luminosity. Table 4 lists a possible scenario used for the
purposes of estimation in this section. We assume 100Gbps
data rate to storage for 1034cm−2s−1 and 60% operational eﬃ-
ciency of the facility. All other rates are derived by scaling this
value by the luminosity and eﬃciency values indicated in the
table.

11

Temporary disk storage will be needed for raw data during
the 3 week time span during which calibrations are derived and
the raw data processed. In addition, disk storage will be needed
for the reconstructed data that collaborators will be accessing
for analysis. Table 5 gives estimates of the disk resources
needed for the ﬁrst 3 years of running. Note that the values
in the table are cumulative and so represent the total amount
of disk needed for each year which include reconstructed data
from previous years.

The CPU required for processing the data is very diﬃcult
to estimate with any accuracy better than the order of magni-
tude. Nonetheless, an attempt is made here to provide such
an estimate. Table 6 summarizes the important values. The
5.4s/ev comes from estimating an average of 3 hours for recon-
struction of 2k events of ECCE data. The numbers for ECCE
CPU mainly come from the simulation campaigns run for pro-
posal development which include combined simulation and re-
construction. The times to process 2k events ranged from 2 to 9
hours depending on the collision type and the CPU type that the
job was processed on. This corresponds to a range of roughly
4s/ev to 16s/ev. The reconstruction only part is considered to be
half of the roughly 6 hour average time to simulate 2k events.
By way of comparison, sPHENIX estimates 15s/ev for Au+Au
scattering and 10.4s/ev for p+p scattering (see section 5.2 of
[6]). Thus, 5.4s/ev is assumed to be at least the right order of
magnitude. The event size of 250kB is also a rough average
based on the ECCE DST ﬁles for several conﬁgurations sim-
ulated in the major proposal campaigns. Note the DST event
size is larger than the average raw data rate divided by the event
rate, as the streaming readout raw data is more tightly packed in
time-frames which also avoids duplication of information be-
tween the neighboring events. The event size is used, along
with the numbers for the Raw Data Storage from table 4, to cal-
culate the number of events produced in each year. The CPU
needed for calibration is estimated to be roughly 5% of that
needed for full reconstruction. It is noted that the sPHENIX
Computing Plan estimates this to be 25%. The ﬁnal line in ta-
ble 6 estimates the number of CPU cores needed to process the
data for each year assuming it can be done over a 30 week pe-
riod. This would mean in year-3 there would be enough CPU to
keep up with the raw data production rate. In earlier years, this
would not be needed as the production times are much shorter.

6. Summary

The ECCE consortium plans to deploy a federated comput-
ing model for the EIC where multiple facilities are used. ECCE
recognizes the need for a global EIC model and intends to
fully participate in the design and implementation of such a
system. A similar strategy has been successfully deployed by
the LHC in the form of the Worldwide LHC Computing Grid
(WLCG) [34]. ECCE has developed and, during the EIC de-
tector proposal period, deployed a tiered “Butterﬂy” model for
EIC computing that was inspired by the WLCG model, but up-
dated to better reﬂect the computing landscape anticipated for
the EIC. In this model, the EIC detector supplies the data, but
the SDCC at BNL is treated as one of a pool of sites used for

Oﬄine BuﬀerCalibrationrawstorageReconstructionreconstorageSite 1: (e.g. SDCC)Oﬄine BuﬀerCalibrationrawstorageReconstructionreconstorageSite 2: (e.g. JLab)Oﬄine BuﬀerCalibrationrawstorageReconstructionreconstorageSite 3: (e.g. NERSC)DetectorFEB (digitization)O(100Tbps)O(10Tbps)O(1Tbps)FELIX-likeFEP (zero suppression)Online Event FilterCPU, FPGA, GPUOnline BuﬀerEBDC (few days)Oﬄine Event FilterO(0.1Tbps)Stage
Compute Interface (e.g. FE-
LIX)
Online Event Filter
Online Buﬀer
Oﬄine Event Filter
Reconstruction
Total

Input/Output
100Tbps/10Tbps

Reduction Factor
×10−1

Technology options
FPGA

10Tbps/1Tbps
1Tbps/0.5Tbps
0.5Tbps/100Gbps
100Gbps/10Gbps
100Tbps/10Gbps

×10−1
×5x10−1
×2x10−1
×10−1
×10−4

FPGA, (GPU), CPU
< disk >
FPGA, GPU, CPU
(FPGA), GPU,CPU

Table 3: Data rates and reduction factors for proposed near real time data ﬂow. Estimated data rate from ECCE detector is O(100T bps). Raw storage will be
O(100Gbps). Reconstructed object storage will be O(10Gbps). Parentheses indicate technologies that could be used, but seem less likely choices.

New Storage
Luminosity
Weeks of Physics Running
Operational eﬃciency
Data Rate to Storage
Raw Data Storage (no duplicates)
Recon Storage
Total Storage (no duplicates)

year-1
1033cm−2s−1
10
40%
6.7Gbps
4PB
0.4PB
7PB

year-2
2 × 1033cm−2s−1
20
50%
16.7Gbps
20PB
2PB
35PB

year-3
1034cm−2s−1
30
60%
100Gbps
181PB
18PB
317PB

Table 4: Estimate of raw data tape storage needed for ﬁrst 3 years of EIC running (ECCE only). Values are estimates assuming ramp up to full luminosity by year
3. Numbers for the ﬁrst two years are estimated for the purposes of this exercise and do not come from an external source. n.b. each value represents only the needs
for data produced in that year and not a cumulative total.

Total Disk
Disk (temporary)
Disk (permanent)
TOTAL

year-1
year-3
year-2
1.2PB 3.0PB 18.1PB
0.4PB 2.4PB 20.6PB
1.6PB 5.4PB 38.7PB

Table 5: Estimate of disk storage needed for ﬁrst 3 years of EIC running (ECCE
only). The temporary disk is used to hold raw data for a 3 week period while
calibrations are derived and reconstruction is done. The permanent disk is for
holding the reconstructed data. This will be cumulative so collaborators will
have access to recon data from all years.

long term storage and compute resources. Both BNL and JLab
would be considered as Echelon 1 sites with the ability to add
others as appropriate. Raw data would be distributed amongst
multiple Echelon 2 sites for processing with the processed data
being returned to Echelon 1. Researchers would directly access
the processed data at the Echelon 1 sites.

We have adopted a ﬁxed-latency oﬄine computing model
where both the ﬁnal calibration and reconstruction of raw data
occur within 2-3 weeks of acquisition. During this period, raw
data will be buﬀered on disk at all of the Echelon 1 sites, along
with permanent archival copies on tapes. Final calibration will
be performed semi-automatically including accumulating suf-
ﬁcient data for tracker alignment and energy scale calibration
of the calorimeters. Artiﬁcial intelligence and machine learn-
ing will be integrated throughout this model. After calibra-
tion, data processing will be released to multiple sites includ-
ing HTC facilities at both Echelon 1 and 2 sites. The EIC will
also require large simulation samples to aid in understanding
the detector response and physics and background processes be-
ing measured. We expect that the produced simulation sample

will focus on 10% of the EIC collision cross-section that is di-
rectly relevant for the signal and background of the core ECCE
physics program. These physics processes will be simulated to
O(10) times the statistics in real data to constrain systematic
uncertainty from the simulated sample to be much smaller than
the data statistical uncertainty.

A summary of the anticipated resource requirements can be

seen in table 7.

7. Acknowledgements

We thank the EIC Silicon Consortium for cost estimate
methodologies concerning silicon tracking systems, technical
discussions, and comments. We acknowledge the important
prior work of projects eRD16, eRD18, and eRD25 concerning
research and development of MAPS silicon tracking technolo-
gies.

We thank the EIC LGAD Consortium for technical discus-

sions and acknowledge the prior work of project eRD112.

We acknowledge support from the Oﬃce of Nuclear Physics
in the Oﬃce of Science in the Department of Energy, the
National Science Foundation, and the Los Alamos National
Laboratory Laboratory Directed Research and Development
(LDRD) 20200022DR.

References

[1] S. Benson, V. Gligorov, M. A. Vesterinen, J. M. Williams, The LHCb
turbo stream, Journal of Physics: Conference Series 664 (8) (2015)
082004. doi:10.1088/1742-6596/664/8/082004.
URL https://doi.org/10.1088/1742-6596/664/8/082004

12

CPU Compute
Recon process time/core
Streaming-unpacked event size
Number of events produced
CPU-core hours (recon-only, 1 pass)
CPU-core hours (calib-only)
2020-cores needed to process in 30 weeks

year-1
5.4s/ev
33kB
121 billion
181Mcore-hrs
9Mcore-hrs
38k

year-2
5.4s/ev
33kB
605 billion
907Mcore-hrs
45Mcore-hrs
189k

year-3
5.4s/ev
33kB
5,443 billion
8165Mcore-hrs
408Mcore-hrs
1701k

Table 6: Estimates of CPU needed for reconstruction of raw data. The number of seconds per event is highly dependent on the type of processor being used. Number
of events comes from total raw data storage estimate in table 4. Calibration is assumed to be 5% of reconstruction time.

ECCE Runs
Luminosity
Weeks of Running
Operational eﬃciency
Disk (temporary)
Disk (permanent)
Data Rate to Storage
Raw Data Storage (no duplicates)
Recon process time/core
Streaming-unpacked event size
Number of events produced
Recon Storage
CPU-core hours (recon+calib)
2020-cores needed to process in 30 weeks

year-1
1033cm−2s−1
10
40%
1.2PB
0.4PB
6.7Gbps
4PB
5.4s/ev
33kB
121 billion
0.4PB
191Mcore-hrs
38k

year-2
2 × 1033cm−2s−1
20
50%
3.0PB
2.4PB
16.7Gbps
20PB
5.4s/ev
33kB
605 billion
2PB
953Mcore-hrs
189k

year-3
1034cm−2s−1
30
60%
18.1PB
20.6PB
100Gbps
181PB
5.4s/ev
33kB
5,443 billion
18PB
8,573Mcore-hrs
1,701k

Table 7: Estimate of raw data storage and compute needs for ﬁrst 3 years of ECCE, assuming ramp up to full luminosity by year 3.

[2] R. Aaij, S. Benson, M. D. Cian, A. Dziurda, C. Fitzpatrick, E. Govorkova,
O. Lupton, R. Matev, S. Neubert, A. Pearce, H. Schreiner, S. Stahl,
M. Vesterinen, A comprehensive real-time analysis model at the LHCb
experiment, Journal of Instrumentation 14 (04) (2019) P04006–P04006.
doi:10.1088/1748-0221/14/04/p04006.
URL https://doi.org/10.1088/1748-0221/14/04/p04006
[3] R. A. K. et al, Science Requirements and Detector Concepts for the
Electron-Ion Collider: EIC Yellow Report (2021). arXiv:2103.05419.
[4] E. Cisbani, et al., AI-optimized detector design for the future Electron-Ion
Collider: the dual-radiator RICH case, Journal of Instrumentation 15 (05)
(2020) P05009. doi:10.1088/1748-0221/15/05/P05009.

[5] AI4EIC Workshop, Sep. 7-10, 2021.

URL https://indico.bnl.gov/event/10699

[6] sPHENIX Collaboration,

sPHENIX Computing Plan, https://

indico.bnl.gov/event/6659/attachments/package (2019).
[7] D. Campora Perez, et al., The 40MHz trigger-less DAQ for the LHCb Up-
grade, Nuclear Instruments and Methods in Physics Research Section A:
Accelerators, Spectrometers, Detectors and Associated Equipment 824
(2016) 280–283.
doi:https://doi.org/10.1016/j.nima.2015.
10.047.

[8] K. Chen, H. Chen, J. Huang, F. Lanni, S. Tang, W. Wu, A Generic High
Bandwidth Data Acquisition Card for Physics Experiments, IEEE Trans.
Instrum. Measur. 69 (7) (2019) 4569–4577. doi:10.1109/TIM.2019.
2947972.

[9] T. Britton, D. Lawrence, K. Rajput, AI Enabled Data Quality Monitor-
ing with Hydra, EPJ Web Conf. 251 (2021) 04010. doi:10.1051/
epjconf/202125104010.

[10] J. Krupa, et al., GPU co-processors as a service for deep learning infer-
ence in high energy physics, Machine Learning: Science and Technology
2 (3) (2021) 035005. doi:10.1088/2632-2153/abec21.

[11] J. Allison, et al., Recent developments in Geant4, Nuclear Instruments
and Methods in Physics Research Section A: Accelerators, Spectrome-
ters, Detectors and Associated Equipment 835 (2016) 186–225. doi:
10.1016/j.nima.2016.06.125.

[12] T. Preston-Werner, Semantic versioning 2.0.0 (2021).

URL https://semver.org/

[13] Cernvm-fs (2021).

URL https://cernvm.cern.ch/fs/

[14] Singularity (2021).

URL https://sylabs.io/singularity/

[15] X. Ai, G. Mania, H. M. Gray, M. Kuhn, N. Styles, A GPU-Based Kalman
Filter for Track Fitting, Comput. Softw. Big Sci. 5 (1) (2021) 20. doi:
10.1007/s41781-021-00065-z.

[16] J. D. Osborn, et al., Implementation of ACTS into sPHENIX Track Re-
construction, Comput. Softw. Big Sci. 5 (1) (2021) 23. doi:10.1007/
s41781-021-00068-w.

[17] X. Ai, et al., A Common Tracking Software Project,

(2021). arXiv:

2106.13593.

[18] M. Barisits, et al., Rucio - Scientiﬁc data management, Comput. Softw.
Big Sci. 3 (1) (2019) 11. doi:10.1007/s41781-019-0026-3.

[19] PanDA Developers, Panda job submission (2021).

URL https://panda-wms.readthedocs.io/en/latest/

[20] EIC Software Community, Fun4all coresoftware (2021).

URL https://github.com/eic/fun4all_coresoftware

[21] Lawrence, D., Boehnlein, A., Brei, N., Jana2 framework for event based
and triggerless data processing, EPJ Web Conf. 245 (2020) 01022. doi:
10.1051/epjconf/202024501022.

[22] G. Corti, et al., Software for the LHCb experiment, in: IEEE Symposium
Conference Record Nuclear Science 2004., Vol. 4, 2004, pp. 2048–2052
Vol. 4. doi:10.1109/NSSMIC.2004.1462666.

[23] T. Jeske, D. McSpadden, N. Kalra, T. Britton, N. Jarvis, D. Lawrence,
AI for Experimental Controls at Jeﬀerson Lab (3 2022). arXiv:2203.
05999.

[24] S. Borghi, Novel real-time alignment and calibration of the LHCb de-
tector and its performance, Nuclear Instruments and Methods in Physics
Research Section A: Accelerators, Spectrometers, Detectors and Asso-
ciated Equipment 845 (2017) 560–564. doi:10.1016/j.nima.2016.
06.050.

[25] X. Developers, Xrootd software repository, https://github.com/

xrootd/xrootd (2022).

[26] T. Sjostrand, et al., High-energy physics event generation with PYTHIA
6.1, Comput. Phys. Commun. 135 (2001) 238–259. doi:10.1016/

13

S0010-4655(00)00236-8.

[27] T. Sjostrand, S. Mrenna, P. Z. Skands, A Brief Introduction to PYTHIA
8.1, Comput. Phys. Commun. 178 (2008) 852–867. doi:10.1016/j.
cpc.2008.01.036.

[28] T. Toll, T. Ullrich, The dipole model Monte Carlo generator Sartre 1,
Comput. Phys. Commun. 185 (2014) 1835–1853. doi:10.1016/j.
cpc.2014.03.010.

[29] K. Kauder, T. Burton, et al., EIC-Smear (2021).

URL https://eic.github.io/software/eicsmear.html

[30] M. Dobbs, J. B. Hansen, The HepMC C++ Monte Carlo event record
for High Energy Physics, Comput. Phys. Commun. 134 (2001) 41–46.
doi:10.1016/S0010-4655(00)00189-2.

[31] S. Agostinelli, et al., GEANT4: A Simulation toolkit, Nucl. In-
strum. Meth. A506 (2003) 250–303. doi:10.1016/S0168-9002(03)
01368-8.

[32] ECCE Consortium, AI-assisted design of the ECCE detector: the ECCE

Tracker Example, ecce-note-comp-2021-03 (2021).
URL https://www.ecce-eic.org/ecce-internal-notes

[33] J. Nielsen, Nielsen’s Law of Internet Bandwidth (2019).

URL https://www.nngroup.com/articles/law-of-bandwidth/
[34] J. Shiers, The Worldwide LHC Computing Grid (worldwide LCG), Com-
puter Physics Communications 177 (1) (2007) 219–223. doi:10.1016/
j.cpc.2007.02.021.
[35] The Grid: A system of tiers.

URL
grid-system-tiers

https://home.cern/science/computing/

[36] ESNet, accessed: 10-8-2021 (2021).
URL https://www.es.net/

14

