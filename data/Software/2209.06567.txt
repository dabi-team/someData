Cost-eﬃcient Auto-scaling of Container-based Elastic Processes

Gerta Sheganakua, Stefan Schulteb, Philipp Waibela, Ingo Weberc

aTU Wien, Vienna, Austria
bChristian Doppler Laboratory for Blockchain Technologies for the Internet of Things, Hamburg University of Technology, Hamburg, Germany
cTU Berlin, Berlin, Germany

2
2
0
2

p
e
S
4
1

]

C
D
.
s
c
[

1
v
7
6
5
6
0
.
9
0
2
2
:
v
i
X
r
a

Abstract

In business process landscapes, a common challenge is to provide the necessary computational resources to enact the single process
steps. One well-known approach to solve this issue in a cost-eﬃcient way is to use the notion of elasticity, i.e., to provide cloud-
based computational resources in a rapid fashion and to enact the single process steps on these resources. Existing approaches to
provide elastic processes are mostly based on Virtual Machines (VMs). Utilizing container technologies could enable a more ﬁne-
grained allocation of process steps to computational resources, leading to a better resource utilization and improved cost eﬃciency.
In this paper, we propose an approach to optimize resource allocation for elastic processes by applying a four-fold auto-scaling
approach. The main goal is to minimize the cost of process enactments by using containers. To this end, we formulate and
implement a multi-objective optimization problem applying Mixed-Integer Linear Programming and use a transformation step to
allocate software services to containers. We thoroughly evaluate the optimization problem and show that it can lead to signiﬁcant
cost savings while maintaining Service Level Agreements, compared to approaches that only use VMs.

Keywords: Elastic Processes, Business Process Management, Cloud Computing, Elastic Computing, BPM, Auto-scaling

1. Introduction

Today1, cloud-based computational resources are used in
many diﬀerent application areas, e.g., banking [1] or health-
care [2]. One particular use case for cloud computing in these
areas is the utilization of cloud-based computational resources
for the enactment of business processes [3], i.e., the actual exe-
cution and invocation of according software services delivering
process functionalities on provisioned computational resources.
Today’s business process landscapes are highly volatile and
ever-changing, e.g., in terms of the number of arriving process
requests or the varying amount of data to be processed [4].
To ensure that these characteristics of business process land-
scapes are taken into account, Business Process Management
Systems (BPMS) need to make sure that a suﬃcient amount of
computational resources is provided for process enactment, to
also fulﬁll the Quality of Service (QoS) requirements of process
owners [3].

The utilization of cloud-based computational resources for
process enactment leads to the notion of elastic processes. For
the realization of elastic processes, it is necessary to provide
elastic BPMS (eBPMS), i.e., BPMS which control a process
landscape and cloud-based computational resources. eBPMS
need to be able to analyze the cost, quality, and resource con-
straints of a process landscape as well as to monitor the compu-
tational resources in use. Based on this information, an eBPMS

Email address: stefan.schulte@tuhh.de (Stefan Schulte)
1The accepted manuscript is available at https://doi.org/10.1016/j.

future.2022.09.001.

is capable to scale the computational resources up or down, in
or out, based on the current demands of a process landscape [5].
So far, eBPMS have mostly aimed at the utilization of cloud-
based computational resources available as Virtual Machines
(VMs), e.g., [6, 7, 8]. However, with the advent of container
technologies like Docker [9], there is today an alternative ap-
proach to utilize cloud-based computational resources for busi-
ness processes. In fact, containers provide a number of beneﬁts
for scheduling and resource allocation, compared to VMs [10]:
First, containers do not virtualize a full operating system and
are therefore relatively lightweight. Accordingly, containers
can be created signiﬁcantly quicker than VMs. Second, con-
tainers do not need a full, time-consuming reboot, when be-
ing reconﬁgured. Again, this saves time. Third, containers of-
fer ﬂexible conﬁguration possibilities. While cloud providers
mostly oﬀer a predeﬁned set of VM instance types, this is not
the case for containers. This allows a more ﬁne-grained control
over computational resources, which may lead to less wasted
cost because of unused computational resources.

When taking into account container-based elastic processes,
existing solutions for resource allocation and task scheduling
for VM-based elastic processes are not an option any longer:
While existing approaches VM-based elastic processes solely
rely on horizontal scaling, i.e., leasing and releasing VM in-
stances, e.g., [11], container-based elastic processes also allow
to do vertical scaling, i.e., to change the amount of comput-
ing power of individual system components [12]. Hence, task
scheduling and resource allocation need to be considered both
between the containers and the software services used to enact
process steps, and between the containers and VMs.

Preprint submitted to Future Generation Computer Systems

September 15, 2022

 
 
 
 
 
 
Therefore, in this paper, we propose an approach for the
cost-eﬃcient auto-scaling of container-based elastic processes
which is able to take into account these requirements. Since
elastic processes are part of potentially very large process land-
scapes, optimal resource allocation and process scheduling is
a complex task. Finding an optimal solution requires deﬁn-
ing and solving a formal optimization model. For this, we
conceptualize an optimization model using Mixed-Integer Lin-
ear Programming (MILP). The aim of the model is to ﬁnd a
cost-eﬃcient enactment plan in terms of process step (i.e., task)
scheduling and allocation of cloud-based computational resources.
A subsequent transformation step is then used to allocate soft-
ware services to containers. In brief, the contributions of this
paper can be summarized as follows:

• We deﬁne a system model for elastic processes, including

processes, VMs, containers, and services.

• We present the Four-fold Service Instance Placement Prob-
lem (FFSIPP), an optimization model aiming at minimiz-
ing the total cost arising from enacting elastic processes
on containers. The optimization model takes into account
vertical and horizontal scaling of both VMs and contain-
ers in a system landscape.

• We evaluate the prototypical implementation. The eval-
uation assesses the cost eﬃciency of the proposed ap-
proach as well as the adherence to user-deﬁned Service
Level Agreements (SLAs).

The remainder of this paper is organized as follows: In Sec-
tion 2, we provide background information about elastic pro-
cesses and some preliminaries for the subsequent sections. In
Section 3, we deﬁne the system model necessary to conduct
the optimization, while in Section 4, we formulate the multi-
objective optimization problem and present a transformation
step. We thoroughly evaluate the proposed solution in Section 5
and discuss related work in Section 6. Finally, we conclude this
paper in Section 7.

2. Background

2.1. Elastic Processes

In brief, an elastic process is a business process that is car-
ried out using elastic cloud infrastructures and resources [5].
Hence, elastic processes combine concepts from the ﬁeld of
Business Process Management (BPM) with those of cloud com-
puting, while especially taking advantage of the elasticity fea-
tures enabled by the resource allocation and virtualization ca-
pabilities of the cloud.

With regard to elastic processes, diﬀerent elasticity proper-
ties can be regarded [3]: resource elasticity, which describes
the change in the amount of used resources (e.g., CPU, RAM,
network bandwidth) based on the incoming demand, cost elas-
ticity, which refers to the change of cost with the changing
amount, duration, and type of provisioned resources, and qual-
ity elasticity, which describes the change in the delivered QoS
in response to ﬂuctuations in the current load of a system.

2

Naturally, the three discussed elasticity properties are highly
interdependent and changes to one of the properties come with
trade-oﬀs for the other properties. For instance, the achieved
QoS is often in direct relationship to the utilized resources,
which again have an eﬀect on the cost. However, these rela-
tionships are not necessarily linear. If these trade-oﬀs are not
regarded, the behavior of a business process landscape can lead
to unexpected cost or ineﬃcient resource utilization [13]. It is
the task of an eBPMS to circumvent such issues and to make
sure that elastic processes are enacted in an eﬃcient way while
taking into account the QoS demands of the process owners.

2.2. Preliminaries

In the following paragraphs, we discuss the terminology and

basic assumptions used within the work at hand.

In elastic process scenarios, process owners are able to de-
ﬁne a process model. Process models are composed out of dif-
ferent process steps, i.e., single tasks. Each process step can
be enacted by a single software service. A service can be used
in multiple diﬀerent process models. We explicitly only regard
software-based services in our work, i.e., human-provided pro-
cess steps are not taken into account.

A process model can be requested by a process owner, lead-
ing to an executable process instance, which features an SLA.
Process models can comprise diﬀerent workﬂow patterns [14],
e.g., sequences, AND-splits, AND-joins, XOR-splits, XOR-
joins, and loops. We assume that the next step of a process
instance is known as soon as the preceding step is scheduled.

In the SLA, non-functional requirements are deﬁned, which
need to be taken into account during process enactment, i.e.,
the invocation of the single software services representing the
process steps. Most importantly, an SLA deﬁnes the deadline
of a process and therefore the priority of the process.
If the
eBPMS is not able to consider an SLA for a process request,
penalty cost accrue, i.e., the process owner gets a penalty fee
for the delay.

When a service is deployed on a cloud-based infrastruc-
ture, we obtain a service instance. Service instances are hosted
in the cloud in an isolated way, and more than one instance
of the same service may exist at the same time on diﬀerent
cloud resources. A service invocation is the unique invoca-
tion of a service instance to serve a requested process instance.
A service instance can serve multiple incoming service invo-
cations concurrently, as long as it has suﬃcient computational
resources. Thus, multiple process instances of the same or dif-
ferent process models may share service instances for single
process steps.

Since the enactment of a varying number of process in-
stances can be requested at any time, a business process land-
scape can become highly dynamic. An ever-changing process
landscape requires diﬀerent amounts and types of computational
resources during diﬀerent time periods [3]. In the work at hand,
we investigate the usage of cloud-based software containers for
the provisioning of these computational resources. We focus
on the actual process enactment time, i.e., the execution time
for the single services, and assume that communication times,

e.g., between the public and the private cloud, as well as data
transfer times, are negligible. In addition, we assume that the
services are primarily computing-intensive.

In order to take care of the dynamics in a process landscape,
the leased cloud-based computational resources need to be opti-
mized for obtaining a cost- and resource-eﬃcient system land-
scape. For this, we aim at controlling cloud-based computa-
tional resources both at the container and VM level in order to
avoid a waste of resources and therefore unnecessary cost.

The cloud-based computational resources can be leased from
both private and public cloud providers. We assume that both
private and public clouds oﬀer diﬀerent VM types. Once leased,
the resources of a running VM instance of a certain type cannot
be extended during a particular Billing Time Unit (BTU). A
BTU deﬁnes the minimum leasing duration of a VM instance.
An arbitrary number of VM instances with arbitrary resource
conﬁgurations can be leased at any time. Multiple contain-
ers can be deployed on diﬀerent leased VM instances. These
containers host the software services necessary to enact a busi-
ness process in terms of the single process steps. Since we use
Docker as container technology, it is possible to create a Docker
container hosting a particular service by deploying a Docker
image on leased VM instances. The Docker image is down-
loaded from an according Docker registry. Once an image has
been downloaded, it stays available on a VM instance, i.e., does
not need to be downloaded again. Docker containers for the
same software service can be deployed on arbitrary many VM
instances, but each VM instance can only run Docker contain-
ers of diﬀerent service types. Each container can be assigned
with a varying amount of computational resources in terms of
CPU and RAM. The assigned resources of a container can be
adapted on demand and are only limited by the available re-
sources of the underlying VM instance.

In order to provide these functionalities, an eBPMS needs to
manage cloud-based computational resources (i.e., lease and re-
lease VM instances), to deploy the necessary amount and types
of containers with invokable software services on the leased re-
sources, and to schedule incoming process requests (in terms of
service invocations) on the available service instances. This has
to be done in a dynamic fashion, as new process requests arrive,
the process landscape may change, and new process steps have
to be scheduled. Since an eBPMS needs to control both the
cloud and process landscape, it acts as a middleware, which
provides Platform-as-a-Service (PaaS) functionalities to pro-
cess owners. At the same time, the eBPMS acts as the allocator
and controller of cloud-based computational resources. This
means that the entity controlling the process execution at the
same time needs to be able to control services, and utilize cloud
resources. Therefore, we do not take into account services of-
fered in a Software-as-a-Service (SaaS) manner. Instead, the
eBPMS controls the deployment and invocation of the service
instances.

In this paper, we present an approach to compute an opti-
mal resource allocation and task scheduling plan, by providing
a solution based on MILP. In theory, each eBPMS that is able
to provide the functionalities discussed in the last paragraph
can be used together with the proposed solution. Within the

presented work, we make use of an extended version of the Vi-
enna Platform for Elastic Processes (ViePEP), which is a fully-
ﬂedged research eBPMS [11, 15].

3. Basic Approach and System Model

3.1. Basic Approach

As outlined in Section 1, we provide a four-fold auto-scaling
approach for elastic processes. This means that both VMs and
containers are adjusted horizontally and vertically.

Horizontal scaling of VMs is accomplished by changing the
number of leased VM instances of the system, while horizontal
scaling of containers is realized by changing the number of de-
ployed container instances oﬀering a certain software service.
Vertical scaling of VMs refers to the change of computa-
tional resources of a system, while the number of deployed VM
instances remains unchanged. It has to be noted that vertical
scaling in this context does not refer to single VM instances, but
to the system landscape as a whole, as we do not address prob-
lems related to reassigning computational resources to running
VM instances, but allow to exchange running VM instances by
VM instances of diﬀerent types oﬀering diﬀerent resources.

Vertical scaling of containers is realized by resizing deployed
container instances so that they oﬀer more or less computa-
tional resources of the underlying VM instance. This resizing
is achieved by changing the assigned CPU and memory shares
to container instances.

3.2. System Model

In order to deﬁne our system model, we ﬁrst describe the
variables used to deﬁne the processes in Section 3.2.1. After-
wards, we present the necessary time variables in Section 3.2.2
and formally deﬁne VMs (Section 3.2.3), as well as containers
and services (Section 3.2.4). Finally, we introduce further vari-
ables of the system model in Section 3.2.5. An overview of all
applied variables is given in Tables 1-7.

3.2.1. Process Variables

Table 1: Process Variables

P = (cid:110)
Ip = (cid:110)

1, . . . , p#(cid:111)
(cid:111)
1, . . . , i#
p

= (cid:110)

Jip

1, . . . , j#
ip

(cid:111)

J∗
ip
Jrun
ip

⊆ Jip
(cid:49) Jip

DLip
DL j∗
ip

rC
jip

, rR
jip

3

Set of process models p ∈ P
Set of process instances ip ∈ Ip, with p ∈ P
indicating which model is used for a particular
process instance
Set of process steps jip ∈ ip not yet enacted,
with ip indicating the process instance of the
step
Set of next process steps j∗
to be enacted
ip
Set of currently enacted (i.e., running) process
steps jrun
ip
Deadline for the enactment of ip
Deadline for starting the enactment of a next
process step j∗
ip
Resource demands of a process step j ∈ ip in
terms of CPU (rC
jip

) and RAM (rR
jip

∈ Jrun
ip

∈ J∗
ip

)

1, . . . , p#(cid:111)

Business process landscapes are made up from a potentially
very large amount of process instances, which stem from dif-
ferent process models. Therefore, we consider multiple process
models, with P = (cid:110)
representing the set of process
models, and p ∈ P indicating a speciﬁc process model. In order
to diﬀerentiate between diﬀerent process models, p (as well as
the other variables used in order to describe processes, VMs,
containers, etc.) take sequential integer values. The set of pro-
cess instances of a speciﬁc process model p is represented by
Ip, while ip ∈ Ip indicates a speciﬁc process instance of the
process model p.

∈ J∗
ip

∈ Jrun
ip

Process models (and accordingly process instances) are com-
posed of diﬀerent process steps. The set of process steps not
yet enacted (and not yet started) to realize a speciﬁc process
instance ip is represented by Jip, while jip ∈ Jip refers to a
speciﬁc process step of a process instance ip. The set of next
process steps (i.e., the immediately next steps in a particular
process instance) that have to be scheduled for enactment to re-
alize a speciﬁc process instance ip is represented by J∗
, while
ip
j∗
⊆ Jip refers to a speciﬁc next process step of the pro-
ip
(cid:49) Jip is a speciﬁc currently running
cess instance ip. jrun
ip
process step of the process instance ip, with the set of currently
running and not yet ﬁnished process steps of a speciﬁc process
instance ip being represented by Jrun
ip
As written above, process owners are able to deﬁne a dead-
line until when a particular requested process instance needs to
be ﬁnished. The deadline for the enactment of the process in-
stance ip is deﬁned by DLip and refers to a certain point in time.
The deadline for starting the enactment of a next process step
j∗
such that the process instance still meets its process dead-
ip
line DLip , while performing a worst-case analysis considering
all subsequent steps, is deﬁned by DL j∗
, and refers also to a
ip
certain point in time. The overall starting time of a process
instance is deﬁned by its ﬁrst process step. We discuss the ac-
cording computations in Section 4.2.

.

Finally, the resource demands of a process step j of the pro-
cess instance ip in terms of CPU and RAM are indicated by the
respective terms rC
jip

(CPU) and rR
jip

(RAM).

3.2.2. Time Variables

Table 2: Time-related Variables

t A speciﬁc point in time
τt

Current time period starting at t

τt+1 Next time period, starting at t + 1
Point in time jip is scheduled
t jip

(cid:15) Minimum time period between two optimization runs

The planned optimization of resource allocation and task
(i.e., process step) scheduling is done for a particular time pe-
riod τt, i.e., the optimization is carried out for this time period
and subsequently repeated for each new time period. τt+1 refers
to the next time period starting at t +1. At the beginning of each
time period, the optimization model is calculated based on all
currently available information and its results are enacted. How
often the optimization is carried out depends on the volatility

4

of a business process landscape as well as the demands of the
BPMS operator (see Section 4).

The point in time at which a speciﬁc service invocation jip is
scheduled is referred to as t jip . Finally, (cid:15) describes (in millisec-
onds) the minimum time period that is at least needed between
two optimization runs, e.g., in order to cater for the amount of
time the optimization itself needs, or in order to avoid too fre-
quent changes. This parameter can be set manually by a system
operator.

3.2.3. Virtual Machine Variables

Table 3: VM Variables

V = (cid:110)
Kv = (cid:110)

1, . . . , v#(cid:111)
(cid:111)
1, . . . , k#
v
BT Uv

Set of VM types v ∈ V
Set of leasable VM instances kv ∈ Kv of type v
BTU of kv

BT U max Arbitrary large upper bound of possible

cv

β(kv,t) ∈ {0, 1}

g(kv,t) ∈ {0, 1}

d(kv,t)

v , sR
sC
v

f C
kv

, f R
kv

leasable BTUs for any kv
Leasing cost for VM instances of type v for one
full BTU
Indicates if kv is/was already running at the be-
ginning of τt
Indicates if kv was already running at the begin-
ning of τt or is leased during τt
Remaining leasing duration of kv at the begin-
ning of τt
Resource supplies of a VM type v in terms of
CPU (sC
v ) and RAM (sR
v )
Free resources in terms of CPU ( f C
kv
( f R
kv

) and RAM

) of kv

The presented approach allows the utilization of VMs from
multiple public and private cloud providers, which both may
provide VMs in diﬀerent conﬁgurations and at diﬀerent cost.
The set of VM types is represented by V, while v ∈ V indicates
a speciﬁc VM type. We assume that the number of leasable
VM instances in the public cloud is virtually unlimited [16],
but a private cloud provider may only possess limited compu-
tational resources which can be leased. The set of leasable VM
instances of type v is represented by Kv, while kv ∈ Kv is the kth
VM instance of type v.

The BTU of a VM instance kv of VM type v is indicated
by BT Uv. A BTU is a billing cycle and the minimum leasing
duration for a VM instance kv. cv represents the leasing cost for
VM instances of type v for one full BTU. Especially if the BTU
deﬁned by a cloud provider is relatively short, one BTU will
not be enough to cover a complete optimization time period
τt. Therefore, we deﬁne the decision variable γ(v,t) ∈ N+
0 ,
which indicates the total number of BTUs to lease any VMs of
type v in the time period τt. An arbitrary large upper bound
of possible leasable BTUs for any VM instance kv is expressed
by the helper variable BT Umax. This upper bound is needed in
order for the computation of the optimization problem to not
getting stuck in a loop.

The helper variable β(kv,t) ∈ {0, 1} contributes to calculating
g(kv,t) ∈ {0, 1}. g(kv,t) indicates if the VM instance kv has already
been running at the beginning of time period τt or is leased dur-

ing τt. g(kv,t) therefore helps identifying the VM instances kv
that are either already running or are being newly leased in τt.
This is important in order to reuse already running kv instead
of spawning new ones. d(kv,t) indicates the remaining leasing
duration of the VM instance kv at the beginning of the time
period τt. These variables are important in order for the opti-
mization model to be able to calculate how many computational
resources are available for how long.

Finally, the resource supplies of a VM type v in terms of
CPU and RAM are indicated by sC
v (RAM). In
addition, the unused resources of a VM instance kv are indicated
by f C
kv

v (CPU) and sR

(CPU) and f R
kv

(RAM).

3.2.4. Container and Service Variables

Table 4: Container Variables

S T = (cid:110)

1, . . . , st#(cid:111)

Cst = (cid:110)

st j
(cid:111)
1, . . . , c#
st

z(st,kv,t) ∈ {0, 1}

The set of all container types mapping all pos-
sible service types st
Service type of a certain process step j
Set of container instances cst ∈ Cst mapping
speciﬁc service instances of type st
Indicates if any containers of type st have al-
ready been deployed on kv until t

M Arbitrary large upper bound of possible con-

tainer deployments for any kv

N Arbitrary large upper bound of possible ser-

vice invocations for any cst

Following the four-fold auto-scaling approach discussed in
Section 3.1, containers are deployed on leased VMs. Therefore,
in addition to the VM variables described in the last subsection,
we also need to formally deﬁne the containers which are used
to host and invoke the actual software services.

The set of all container types mapping all possible service
types is represented by S T , while st ∈ S T indicates a speciﬁc
container type for a certain service type. Notably, the container
type deﬁnes which service types (i.e., images) can be oﬀered by
a container instance. The service type of a certain process step
j is indicated by st j, i.e., a process step can only be executed by
the according service type.

The set of container instances mapping speciﬁc service in-
stances of type st is represented by Cst, while cst ∈ Cst is the
cth instance of type st. A container instance of a certain service
type st that can run a process step j is given by cst j.

The variable z(st,kv,t) ∈ {0, 1} indicates if any containers of
service type st have already been deployed on a VM instance
kv (and hence the container image for starting new container
instances of service type st has been downloaded on kv) until t.
This information is needed, since downloading an image takes
time and should therefore be avoided (see Section 3.2.5).

Finally, two variables are introduced in order to make sure
that the optimization is not getting stuck in a loop: An arbitrary
large upper bound of possible container deployments for any
VM instance kv is expressed by the helper variable M, and an
arbitrary large upper bound of possible service invocations for
any container instance cst is expressed by the helper variable N.
Again, these parameters are predeﬁned.

5

3.2.5. Further Variables

Table 5: Further Variables

Time to start a new VM of type v

∆ = maxv∈V (∆v) Maximum VM-startup time of any type v

∆v

∆st

∆cst

eip
, eRL
ip

eseq
ip

, eLa
ip

, eLx
ip

L = (cid:110)
1 . . . l#(cid:111)
La ∪ Lx ∪ RL ⊆ L

Time for pulling a container image for st on a
VM instance for the ﬁrst time
Time for starting a container instance from an
existing image
Remaining enactment duration of ip
Remaining enactment duration of sequences
(eseq
), XOR-blocks (eLx
)
ip
ip
and repeat loops (eRL
ip
Set of all possible paths l in ip
Set of possible paths for AND-blocks La,
XOR-blocks Lx, and repeat loops RL
re Maximum repetitions of a repeat loop

), AND-blocks (eLa
ip
) of ip

ˆes
ip

, ˆel
ip

ex jrun
ip

ex j∗
ip

ep
ip

cp
ip

Remaining combined service enactment du-
ration, time for pulling needed container im-
ages, container deployment and VM startup
time for all not yet scheduled service invoca-
tions of sequences or repeat loop blocks (ˆes
)
ip
and paths in AND- or XOR-blocks (ˆel
) that
ip
still need to be invoked to ﬁnalize ip
Remaining enactment time of process steps
already scheduled in previous periods and not
ﬁnished until the start of τt
Combined enactment duration, container de-
ployment and VM-startup time of j∗
ip
Penalty time units of ip, i.e., the enactment
that occurs beyond the deadline DLip
The penalty cost per time unit of delay of ip

Finally, some variables need to be introduced as part of the
system model in order to cater for start, deployment, enactment,
and penalty times of VMs, containers, and services.

To start with, the time in milliseconds to start a new VM
of type v is indicated by ∆v. The maximum VM-startup time
of any type, i.e., maxv∈V (∆v) is expressed by ∆. This variable is
later on needed for the worst-case estimation we perform during
optimization (see Section 4.2). The time for pulling a container
image for a service type st on a VM instance for the ﬁrst time
is expressed by ∆st. The time for starting a container instance
from an existing image for a service type st on a VM instance
is expressed by ∆cst

The remaining enactment time of a process instance ip is
expressed by eip. eip does not include the enactment and de-
ployment time of the process steps that will be scheduled for
the current optimization period τt, nor the remaining enactment
time of the currently already running steps.

To calculate eip , we need to separately calculate the remain-
ing enactment time of sequences, AND-blocks, XOR-blocks,
and repeat loops in a process model. These are given by eseq
ip
(sequences), eLa
(re-
ip
peat loops). These terms do not include the remaining enact-
ment time of the currently running steps nor the enactment and
deployment times for process steps that will be scheduled for

(XOR-blocks), and eRL
ip

(AND-blocks), eLx
ip

Table 6: Weighting Factors

Table 7: Decision Variables

ωDL Weighting factor for controlling the eﬀect of deadline

constraints

ωd Weighting factor for controlling the eﬀect of the remain-
ing leasing duration on the optimization outcome
f Weighting factors for controlling the eﬀect of free re-

ωC

f , ωR

sources in terms of CPU (ωC

f ) and RAM (ωR
f )

ωz Weighting factor for controlling the eﬀect of existing im-

ages on VM instances

the current optimization period τt.

1 . . . l#(cid:111)

To calculate the remaining enactment times for the men-
tioned workﬂow patterns, it is necessary to know the potential
paths (i.e., process steps) in a process instance. The set of all
paths is represented by L = (cid:110)
, while l ∈ L indicates a
speciﬁc path within a process. The set of possible paths for
AND-blocks La, XOR-blocks Lx and repeat loop constructs RL
are expressed by the respective terms, with La∪Lx∪RL ⊆ L. For
the repeat loops, again, we have to introduce a helper variable,
which provides an upper limit. Accordingly, the maximum rep-
etitions of a repeat loop are indicated by re.

As written above, the remaining enactment duration of a
process instance (eip ) is limited to the actual service enactment
times. The remaining combined service enactment duration,
time for pulling needed container images, container deployment
and VM startup time for all not yet scheduled service invoca-
tions of sequences or repeat loop blocks (ˆes
) and paths in AND-
ip
or XOR-blocks (ˆel
) that still need to be invoked to ﬁnalize the
ip
enactment of the process instance ip are expressed by the re-
spective terms. Still, those values do not include the remaining
enactment time of already running process steps ( jrun
) or pro-
ip
cess steps that are being scheduled in the current optimization
period τt. These are provided separately by ex jrun
ip

.

.

The combined enactment duration, container deployment
and VM-startup time of the next to be scheduled process step
j∗
ip

is expressed by ex j∗
ip
To take into account penalty cost which may accrue if a pro-
cess instance does not meet its speciﬁed deadline, two further
parameters are introduced. First, the penalty time of a process
instance ip measured in time units, i.e., the enactment duration
of ip that occurs beyond the deadline DLip of ip, is expressed by
ep
. This variable is used by the optimization model in order to
ip
calculate if it is cheaper to pay the penalty fee than to lease fur-
ther computational resources. We assume linear penalty fees,
i.e., the fees grow with the delay of process enactment. The pe-
nalty cost per time unit of delay of the process instance ip are
represented by cp
ip

We allow a system operator to weight the diﬀerent parts
of the objective function presented in Section 4, e.g., to give
a higher importance to the utilization of leased computational
resources. This allows to control the eﬀects of diﬀerent con-
straints on the optimization outcome. These weighting factors
are shown in Table 6. Their application is explained in more
detail in Section 4.1.

.

Finally, Table 7 provides an overview of the decision vari-

6

γ(v,t) ∈ N+
0

Number of BTUs to lease any VM of type v in
τt

a(cst,kv,t) ∈ {0, 1} Deployment of cst on kv in τt
Invocation of j ∈ ip on kv in τt
x( jip ,kv,t) ∈ {0, 1}
(cid:110)N+
(cid:111)
y(kv,t) ∈
Number of BTUs to lease kv in τt

0

ables needed in our decision model. γ primarily acts as a helper
variable and has already been mentioned in Section 3.2.3. a, x
and y are the core decision variables of our objective function
and accordingly further discussed in the next section.

4. Multi-Objective Problem Formulation

Based on the system model introduced in the last section,
we are now able to formulate our task scheduling and resource
allocation problem FFSIPP as a MILP optimization problem.
The multi-objective optimization problem represents the schedul-
ing problem for one optimization round starting at time t. An
optimization round is triggered whenever new process steps
should be scheduled, for example, whenever new process re-
quests arrive or when previously running steps of process in-
stances are ﬁnished. Furthermore, the optimization is triggered
when SLA violations can be foreseen.

Our optimization problem takes complex workﬂow patterns,
penalty cost, container-based metrics, diﬀerent pricing models,
and BTUs for VMs into consideration. The optimization prob-
lem focuses on minimizing the cost that arise from enacting
elastic process landscapes on container-based cloud infrastruc-
tures while taking into account process SLAs, i.e., QoS in terms
of deadlines. Solutions for the problem describe when and on
which containers and service instances to schedule service in-
vocations, on which VMs to place containers and service in-
stances, and which VM to lease or release.

4.1. Objective Function

(cid:104)
min

+

+

+

+

+

(cid:0)cv × γ(v,t)

(cid:1)

(cid:88)

v∈V
(cid:88)

(cid:88)

p∈P
(cid:88)

ip∈Ip
(cid:88)

(cid:0)cp
ip

(cid:1)

× ep
ip

(cid:88)

(cid:88)

(cid:88)

p∈P
(cid:88)

ip∈Ip
(cid:88)

j∗
∈Jip
ip
(cid:88)

v∈V
(cid:88)

kv∈Kv
(cid:88)

(cid:0)ωz × (1 − z(st j,kv,t)) × x( jip ,kv,t)

(cid:1)

(cid:0)ωd × d(kv,t) × x( jip ,kv,t)

(cid:1)

v∈V

kv∈Kv

f × f C
kv

+ ωR

f × f R
kv

(cid:1)

j∗
ip

∈Jip
(cid:0)ωC

p∈P
(cid:88)

ip∈Ip
(cid:88)

v∈V
(cid:88)

kv∈Kv
(cid:88)

p∈P

ip∈Ip

j∗
ip

∈Jip

v∈V

kv∈Kv

(cid:88)

(cid:88)

(cid:88)

(cid:0)ωDL × (DL j∗

ip

− τt) × x( jip ,kv,t)

(cid:1)(cid:105)

The equation above presents the objective function of our multi-
objective optimization problem FFSIPP. As we are aiming at
cost eﬃciency, the objective function is subject to minimization
under the constraints introduced in Section 4.2.

The objective function considers six terms for calculating
a scheduling plan, indicated by the decision variable x( jip ,kv,t),
which decides on which VM instance kv to schedule a service
invocation for process step j of process instance ip in the time
period τt. In the following paragraphs, the single terms of the
objective function are explained in more detail:

• Term 1 - minimize total VM leasing cost: The ﬁrst term
calculates the total cost for leasing VM instances of each
VM type v at time t, by considering the leasing cost cv for
the VM types and the total number of leased BTUs γ(v,t)
that VM instances of the corresponding type v are leased
at t. The aim is to minimize the total cost for leasing
VMs.

• Term 2 - minimize penalty cost: The second term cal-
culates the total penalty cost that arise from scheduling
decisions that lead to enactment times for process in-
stances beyond their deﬁned deadlines. The penalty time
ep
of a process instance ip is calculated according to a
ip
worst-case analysis. The aim is to minimize such pe-
nalty cost and therefore make sure that incoming process
requests are enacted in time, as deﬁned in their SLAs.
However, it might in some cases be more cost-eﬃcient to
pay the penalty fee than to lease additional computational
resources.

• Term 3 - minimize container deployment time: The third
term minimizes the sum of time needed for deploying
new container instances on VM instances kv. Notably,
we do not consider directly the scheduling of container
instances in the optimization problem (see Section 4.3).
Instead, we consider the placement of service steps on
the VM instances and use the service types of the sched-
uled process steps to indirectly calculate the potential de-
ployment time for container instances. The subterm (1 −
z(st j,kv,t)) makes sure that the weighting factor ωz is only
considered when the cache for container deployment does
not exist on the VM instance kv. As the full term is mini-
mized, the system prefers choosing VM instances for as-
signing service invocations where the cache for deploy-
ing the needed container instance already exists. This
term accordingly describes a trade-oﬀ between the time
for deployment and cost.

• Term 4 - maximize future resource supply of already leased
VMs: The fourth term aims at maximizing the future re-
source supply of already leased VM instances, by giving
a preference to directly scheduling service invocations
on VMs with a shorter remaining leasing duration d(kv,t)
compared to other VMs with already existing longer leas-
ing durations. Although at time t this preference does not
lead to any cost-based advantages, it assures that the al-
ready leased resources oﬀer more long-running resources
for future optimization periods, which can then be used
by services. This reduces the amount of wasted computa-
tional resources. The term calculates the remaining leas-
ing duration of all VMs using the weighting factor ωd.

7

As we want to give a preference to deploying containers
on VM instances with a small leasing duration, we min-
imize the term to maximize the future remaining leasing
durations of available resources.

) and RAM ( f R
kv

• Term 5 - minimize sum of unused present VM resources:
The ﬁfth term minimizes the sum of all leased but un-
used computational resource capacities in terms of CPU
( f C
) of all leased VM instances. Due to
kv
this term, our approach makes sure to not only schedule
all service invocations that cannot be delayed any further
without leading to penalty cost, but to also pre-schedule
service invocations that may already be enacted but have
a distant deadline. Again, this leads to a minimization
of wasted computational resources, and therefore con-
tributes indirectly to the cost optimization. We consider
this term using weighting factors for CPU (ωC
f ) and RAM
(ωR

f ) resources.

• Term 6 - maximize the importance of scheduled service
invocations: The last term calculates the diﬀerence be-
tween the last possible scheduling deadline of the next
schedulable steps when performing a worst-case analysis
based on the current time. The term considers all process
instances and aims at giving a scheduling preference to
process steps with closer deadlines. The rationale behind
this is that process steps with such deadlines could lead
to an SLA breach and according penalty cost. The term
is responsible for deﬁning the importance of schedulable
process steps that do not necessarily have a pressing pro-
cess deadline on remaining free resources. We consider
the actual enactment deadline for the next schedulable
steps based on the worst-case analysis instead of simply
comparing the overall process deadlines. We also deﬁne
the term such that past process deadlines can be consid-
ered and steps that are past their deadline receive a higher
scheduling importance. We consider a weighting factor
ωDL to assign a weight to the term.

In general, the mentioned weighting factors for the single terms
of the objective function allow the user (e.g., an eBPMS oper-
ator) to control the eﬀect of the respective terms on the over-
all optimization outcome. For instance, in some settings, ωDL
might be weighted very high in order to support that all process
instances are carried out in time. In other settings, this might
not be very important, but a system operator might increase ωC
f
to minimize unused CPU resources (Term 5) in order to save
energy.

4.2. Constraints

Apart from the objective function, it is also necessary to
deﬁne constraints for the FFSIPP. To start with, Constraint (1)
makes sure that deadlines are considered for each process in-
stance ip. The constraint does not guarantee that deadlines
are not violated, but it deﬁnes the penalty times ep
of process
ip
instances which are subject to minimization in the presented
Term 2 of the objective function. The constraint demands that

+ ex∗
jip

the current time τt plus the remaining worst-case enactment
+ eip is smaller or equal
time of process instances exrun
jip
to the deﬁned deadline DLip plus the potential penalty time ep
ip
which occurs when process instances ﬁnish after their deadline.
This constraint considers the remaining enactment time includ-
ing the time of process steps that are being scheduled during
the current time period and process steps that have already been
scheduled in previous periods and are still running. Naturally,
the starting time for a complete process instance is deﬁned by
the computed starting time of the very ﬁrst process step within
the process model. For this, diﬀerent workﬂow patterns are
taken into account (see below).

τt + exrun
jip

+ ex∗
jip

+ eip

(cid:54) DLip

+ ep
ip

(1)

Constraint (2) helps to deﬁne the start of the next optimization
period τt+1, by demanding that the start of the next optimization
period τt+1 plus the remaining worst-case enactment time of
process instances eip is smaller or equal to the respectively de-
ﬁned process deadlines DLip plus the possible penalty times ep
.
ip
At this stage, the approach explicitly does not consider the en-
actment time of currently running process steps or process steps
that are scheduled for enactment in τt, as the earliest possible
time when the next steps of a process instance can be sched-
uled is when the previous steps, already running or scheduled
in τt, are ﬁnished. Adding the remaining time of the currently
running or scheduled steps to the left side of Constraint (2) can
potentially lead to triggering premature optimization rounds at
t + 1 when predecessor steps of the next schedulable steps are
not ﬁnished yet (and hence no new steps can be scheduled).

τt+1 + eip

(cid:54) DLip

+ ep
ip

(2)

Constraint (3) makes sure that the next optimization time pe-
riod τt+1 is deﬁned to be in the future. We use the value (cid:15) > 0
to avoid optimization deadlocks arising from a too small value
for τt+1. The start of the next optimization period is directly
calculated as a result of the previous optimization rounds. Nev-
ertheless, the optimization can also be triggered by other events,
like newly incoming requests or ﬁnalized process steps and run
earlier than the calculated time t + 1. In this case, a potentially
still ongoing computation of an optimal task scheduling and re-
source allocation is canceled.

τt+1 (cid:62) τt + (cid:15)

(3)

Constraint (4) calculates the remaining enactment duration eip
of a process instance. The remaining enactment duration does
not include the enactment and deployment time of the process
steps that will be scheduled for the current optimization period
τt, nor the remaining enactment time of the currently already
running steps. The overall remaining enactment duration eip
of a process instance ip is the sum of the enactment times of
the diﬀerent workﬂow patterns that the process instance is com-
posed of. We aggregate the upper-bound enactment times of the
diﬀerent workﬂow patterns, following the approach by [17].

eip

= eseq
ip

+ eLa
ip

+ eLx
ip

+ eRL
ip

The worst-case enactment times for sequences are deﬁned in
Constraint (5), for AND-blocks in Constraint (6), for XOR-
blocks in Constraint (7), and for repeat loop blocks in Con-
straint (8). For AND-blocks, the longest remaining enactment
time of all paths within the block has to be considered. As we
are using a worst-case analysis, we also consider the longest
path for the calculation of the remaining enactment time of
XOR-blocks. For repeat loop blocks, the subpath enactment
time is multiplied by the maximum possible amount of repeti-
tions re.

Constraints (5)–(8) consider the overall remaining enact-
ment time of all not yet running process steps for each process
instance ip as deﬁned by the two helper variables ˆes
for se-
ip
quences and ˆel
for XOR- and AND-blocks in Constraints (10)
ip
and (11), respectively. Since we perform a worst-case analy-
sis, both equations consider the worst-case VM startup time ∆,
i.e., including the time for pulling the respective Docker image
onto a running VM instance ∆st, and the time for starting a new
container instance ∆cst j
for each remaining process step jip . If
a process step j∗
is being scheduled in the current time period
ip
τt, Constraints (5)–(8) subtract the worst-case enactment time
of the scheduled steps ex j∗
as deﬁned in Constraint (9), from
ip
the overall remaining enactment time.

eseq
ip

=





ˆes
ip
ˆes
ip

− ex j∗
ip

,kv,t) = 1

, if x( j∗
ip
, otherwise













=

eLa
ip

=

eLx
ip

eRL
ip

=

max
l∈La
max
l∈La

(ˆel
ip
(ˆel
ip

)

− ex j∗
ip
)

,kv,t) = 1

, if x( j∗
ip
, otherwise

max
l∈Lx
max
l∈Lx

(ˆel
ip
(ˆel
ip

)

− ex j∗
ip
)

,kv,t) = 1

, if x( j∗
ip
, otherwise

− ex j∗
ip

re × ˆes
ip
re × ˆes
ip

,kv,t) = 1

, if x( j∗
ip
, otherwise

=

ex j∗
ip

(cid:88)

(cid:88)

v∈V

kv∈Kv

(cid:0)(e j∗

ip

+ ∆cst j

+ ∆st j

+ ∆) × x( j∗

ip

,kv,t)

(cid:1)

=

ˆes
ip

(cid:88)

jip ∈J seq
ip

(e jip

+ ∆cst j

+ ∆st j

+ ∆)

=

ˆel
ip

(cid:88)

jip ∈Jl
ip

(e jip

+ ∆cst j

+ ∆st j

+ ∆)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

Constraints (12) and (13) consider for each VM instance kv the
sum of CPU and RAM resources required by all service invo-
cations that either already run or are scheduled to run in any
container on the VM instance in τt. These resources need to be
smaller or equal to the resource supply in terms of CPU (sC
v )
and RAM (sR
(cid:88)
(cid:88)

v ) that is oﬀered by VM instances of type v.

(cid:88)

× x( jip ,kv,t)) (cid:54) sC

v

(rC
jip
)

(12)

(4)

8

p∈P

ip∈Ip

jip ∈(Ji∗
p

∪Jrun
ip

(cid:88)

(cid:88)

(cid:88)

p∈P

ip∈Ip

jip ∈(Ji∗
p

∪Jrun
ip

× x( jip ,kv,t)) (cid:54) sR

v

(rR
jip
)

(13)

Constraints (14) and (15) deﬁne for each VM instance v ∈ V,
kv ∈ Kv the remaining free capacities in terms of CPU and
RAM that are not directly allocated to service instances. As
expressed by the variable g(kv,t) ∈ {0, 1}, which is further deﬁned
in Constraints (16) and (17), we only consider VM instances
that are either already running or are being leased in τt.

g(kv,t) × sC

v −

(cid:88)

(cid:88)

(cid:88)

p∈P

ip∈Ip

jip ∈(Ji∗
p

∪Jrun
ip

× x( jip ,kv,t)) (cid:54) f C
kv

(14)

(rC
jip
)

g(kv,t) × sR

v −

(cid:88)

(cid:88)

(cid:88)

p∈P

ip∈Ip

jip ∈(Ji∗
p

∪Jrun
ip

× x( jip ,kv,t)) (cid:54) f R
kv

(15)

(rR
jip
)

Constraints (16) and (17) deﬁne the value of the helper variable
g(kv,t) ∈ {0, 1} which takes the value of 1 if a VM instance is
= 1) or being leased for at least one
either already running (βkv
BTU (y(kv,t) ≥ 1) in τt. g(kv,t) is restricted to be a boolean variable
in Constraint (28).

g(kv,t) (cid:54) β(kv,t) + y(kv,t)

β(kv,t) + y(kv,t) (cid:54) BT Umax × g(kv,t)

(16)

(17)

Constraint (18) ensures that for all VM instances v ∈ V, kv ∈ Kv
a VM instance kv will be leased or is running if service in-
vocations are to be placed on it. Therefore, we calculate the
number of process steps that are scheduled or already running
on kv and make use of our helper variable g(kv,t) as deﬁned in
Constraints (16) and (17), as well as a helper variable (M × N)
that presents an arbitrarily large number, e.g., 100 × 10,000, as
an upper bound of possible container deployments per VM in-
stance times possible parallel service invocations per container
instance. Notably, this upper bound does not imply that an ac-
cording number of service invocations are actually carried out.
Instead, the goal of this variable is to avoid that the optimization
gets stuck in a loop.

(cid:88)

(cid:88)

(cid:88)

p∈P

ip∈Ip

jip ∈(J∗
ip

(cid:83) Jrun
ip

)

x( jip ,kv,t) (cid:54) g(kv,t) × (M × N)

(18)

(cid:83) Jrun
ip

Constraint (19) makes sure that for all VM instances v ∈ V,
kv ∈ Kv and all running or schedulable process steps jip ∈
(J∗
), the scheduled service invocations jip will not be
ip
moved to other VM instances. First, we consider the remaining
enactment time of a process step jip for each process step sched-
uled on a VM instance (x( jip ,kv,t) = 1). In case that the process
step jip is not running yet, we also have to consider the time
to pull a new image ∆st j and to start the needed container ∆cst j
,
if the image does not exist and the container is not running on
the scheduled VM instance (z(st j,kv,t) = 0). Furthermore, if the
required VM instance kv is not up and running yet (β(kv,t) = 0),
we also need to add the time ∆ for starting the new VM instance
kv to our calculation of the remaining enactment time. We de-
mand the calculated remaining enactment time to be smaller or
equal to the remaining leasing duration d(kv,t) of the allocated

(cid:111)

VM instance kv. If the currently remaining leasing duration is
smaller, the corresponding VM instance kv needs to be leased
(cid:110)N+
for y(kv,t) ∈
additional BTUs in the time period τt to satisfy
0
the constraint.
(cid:0)e jip

+ ∆st j) × (1 − z(st j,kv,t)) + ∆ × (1 − β(kv,t))(cid:1)
×x( jip ,kv,t) (cid:54) d(kv,t) × β(kv,t) + BT Ukv × y(kv,t)

+ (∆cst j

(19)

Similar to Constraint (19), Constraint (20) together with Con-
straint (23) ensures that all service invocations that are already
running in a container on a certain VM instance at the start
of τt, will not be migrated to another VM instance (or con-
tainer) during their invocation. We explicitly reference the al-
ready assigned VM instance kv by the additional indices in Con-
straint (20) and do not need to consider additional times for
pulling container images or starting VM instances.

erunkv
jip

(cid:54) d(kv,t) × β(kv,t) + BT Ukv × y(kv,t)

(20)

Constraint (21) deﬁnes the variable γ(v,t) for all VM types v ∈ V.
γ(v,t) indicates the total number of BTUs to lease VM instances
of type v, meaning it includes the decisions which VM instances
kv to lease and for how long. To deﬁne γ(v,t), we build the sum
over all leased BTUs for each VM instance of type v.

(cid:88)

kv∈Kv

y(kv,t) (cid:54) γ(v,t)

(21)

Constraint (22) demands for all next schedulable process steps
jip ∈ J∗
that each service invocation is scheduled on only one
ip
VM instance (and later enacted within only one container in-
stance) or postponed for a later optimization period.

(cid:88)

(cid:88)

(cid:88)

v∈V

kv∈Kv

jip ∈J∗
ip

x( jip ,kv,t) (cid:54) 1

(22)

Constraint (23) makes sure that all steps currently running on
a VM instance kv remain on the same VM instance. This is
achieved by setting the decision variable x( jip ,kv,t) for each al-
ready running service invocation jrun
on a VM instance kv to 1.
ip
This constraint corresponds to inheriting the scheduling deci-
sion already made in a previous period.

x( jrun
ip

,kv,t) = 1

(23)

The remaining Constraints (24)–(28) restrict the values partic-
ular variables can adopt. Constraint (24) restricts the decision
variable x( jip ,kv,t) that decides whether or not to schedule a ser-
vice invocation jip on a certain VM instance kv for all p ∈ P, ip ∈
Ip, jip ∈ J∗
ip

to be a boolean.

x( jip ,kv,t) ∈ {0, 1}

(24)

Constraint (25) restricts the decision variable y(kv,t) that decides
for how many additional BTUs to lease a VM instance kv at
time t for all v ∈ V, kv ∈ Kv to be a positive integer ≥ 0. Con-
straint (26) does the same for γ(v,t) that decides for how many
total BTUs to lease any VM instances

y(kv,t) ∈ N+

0

9

(25)

γ(v,t) ∈ N+

0

(26)

Algorithm 1 Transformation

Constraint (27) restricts the variable that indicates the penalty
enactment time ep
ip

to be a positive real number.

∈ R+

ep
ip

(27)

Constraint (28) restricts the variable g(kv,t) to be a boolean. The
variable has been deﬁned in Constraints (16) and (17) and takes
=
the value of 1 if a VM instance is either already running (βkv
1) or being leased for at least one BTU (y(kv,t) ≥ 1) in τt.

g(kv,t) ∈ {0, 1}

4.3. Transformation Step

(28)

Counterintuitively, the problem discussed so far partly dis-
regards the container allocation: it schedules process steps di-
rectly on VM instances, while considering the current informa-
tion about deployed containers. More formally, the output of
the model is a task scheduling and resource allocation decision
stating for all VM instances v ∈ V, kv ∈ Kv what VM instances
to lease for how many BTUs y(kv,t) and for all next schedula-
ble or running process steps jip∈J∗
on which VM instance
v ∈ V, kv ∈ Kv to schedule the service invocations x( jip ,kv,t).

(cid:83) Jrun
ip

This means that the actual container instances are not yet
part of the scheduling plan after the optimization. While this
information could also be derived from an extended version of
the presented MILP problem, this would make the FFSIPP even
more complex.

ip

We opted against this for several reasons: Most importantly,
the extended optimization model would have to handle a very
large number of decision variables: First, a MILP solver has
to decide for each VM instance in the set of leasable instances
whether or not to lease the VM instance. Second, the solver
would need to decide for each container instance whether or not
to deploy a container with a certain conﬁguration on a certain
VM instance. Third, the solver would need to decide on which
deployed container instance to schedule the service requests.
When doing so, the computational complexity grows exponen-
tially with the number of incoming process requests and also
with the number of VM instances and container instances.

Related work in the ﬁeld of MILP-based optimization prob-
lems has shown that solutions struggle with increasing problem
sizes [18]. In order to avoid this, we introduce a so-called trans-
formation step. This step wraps each scheduled process step of
a certain service type in a container instance of the same ser-
vice type, which is to be deployed on the respective VM as per
the optimization (see Section 3.2.4). This guarantees the exact
resources, as needed by the scheduled service invocations on
the VM. Generally speaking, the transformation step described
in Algorithm 1 decreases the problem size by avoiding that a
MILP solver has to decide which containers with which conﬁg-
urations to deploy on the VM instances.

To transform the result of the optimization model to a re-
sult which takes into account container instances, Algorithm 1
iterates over all VM instances kv of the scheduling plan (line 1)
and extracts all process steps that are scheduled for enactment
on a VM instance (line 2). For all process steps jip of the same

10

V MS erviceMap ← new Map(st, List( jip ));
for all jip where x( jip ,kv,t) = 1 do
V MS erviceMap ← (st j, jip );

end for
for all st in V MS erviceMap.getKeys() do

1: for all kv in the scheduling plan do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end for

end for
cst.setS ize(coS ize);
a(cst,kvt) ← 1;

end for

cst ← new Container(st);
coS ize ← 0;
for all jip in V MS erviceMap.getValues(st) do
coS ize ← (coS ize + jip .getResS ize());
x( jip ,cst,t) ← 1;

service type st (lines 3-5), a container instance cst is deﬁned
(line 7) with a conﬁguration that makes sure that the containers
minimum resource supply coSize is set to be exactly as large
as the sum of scheduled service invocations of type st on kv
(line 13). We can then exchange the variable x( jip ,kv,t) from the
MILP solution for the introduced variables x( jip ,cst,t) and a(cst,kvt)
(indicating if the container c with the service type st should be
deployed on VM kv in the time period τt), which we set to 1
(lines 11, 14). This means, our scheduling plan schedules all
service invocations jip of service type st that were scheduled on
kv on the newly deﬁned container instance cst, while the con-
tainer instance cst is scheduled on kv.

4.4. Enacting the Transformed Scheduling Plan

When enacting the transformed scheduling plan, the main
tasks of the eBPMS are to lease or release VM instances, to
deploy, stop, or adjust containers, and to place the service invo-
cations. For each task, diﬀerent rules have to be considered:

First, whenever the decision variable y(kv,t) takes a value
> 0, the corresponding VM instance kv needs to be leased or the
lease of the VM instance needs to be extended by more BTUs.
If during an optimization round no containers are scheduled on
a VM instance kv, the instance will still continue to be running
until the end of its leasing duration.

Second, whenever the reasoner delivers the solution a(cst,kv,t) =
1 for a container instance cst, either the deployment of the in-
stance is initiated by the eBPMS if a container of type st is not
running on kv at time t, or a currently running container of type
st on kv is potentially resized to match the deﬁned container
conﬁguration. If a container instance of service type st on a
VM instance kv is currently running during τt, but the schedul-
ing plan does not deﬁne a corresponding container scheduling
of the form a(cst,kv,t) = 1, the running container is immediately
shut down to free up resources for other containers.

Third, all scheduled service invocations x( jip ,cst,t) = 1 that
are not yet running are invoked by the eBPMS on the deﬁned
container cst. As the scheduling plan also contains running
steps, the enactment of those service invocations that have al-

ready been scheduled for the ﬁrst time in a previous optimiza-
tion round remains unchanged.

5. Evaluation

Table 8: Evaluation Process Models

Process No.
1
2
3
4
5
6
7
8
9
10

|Steps|
3
2
3
8
3
9
8
3
4
20

|XOR|
0
1
0
0
1
1
0
0
0
0

|AND|
0
0
1
2
0
1
0
1
1
4

|Loops|
0
0
0
0
0
0
0
0
1
0

In this section, we conduct a quantitative analysis of the
implementation of the FFSIPP as presented in Section 4. As
testbed, we use an extended version of the eBPMS ViePEP [11,
15], which also provides the performance metrics needed in our
optimizations. For the predictions of CPU demands, we apply
the values mentioned in Tables 10 and 12. However, we could
also apply a prediction approach for this, e.g., [19].

All evaluation runs have been executed on a MacBook Pro
with an i5 quad core CPU with 2.50 GHz, 8 GB of RAM, run-
ning Ubuntu 16.04 with Linux kernel 4.8.0. As MILP solver,
IBM CPLEX is used. Internally, CPLEX makes use of diﬀerent
(simplex) algorithms.

5.1. Evaluation Setup

5.1.1. Process Models

We perform our evaluation using a subset of the SAP ref-
erence model [20], which has been used for multiple scien-
tiﬁc papers, e.g., [21], and provides a solid foundation for our
evaluations. From the around 600 process models in the refer-
ence model, we select ten models with diﬀerent process pat-
terns and varying levels of complexity, including sequences,
XOR-blocks, AND-blocks, and repeat loops. All XOR- and
AND-blocks start with a split and end with a merge pattern.
The merge for AND-blocks is blocking, while the merge for
XOR-blocks continues the process enactment as soon as the last
process step of one optional branch is ﬁnished. Our chosen pro-
cess models range from simple sequences to complex process
models (see Table 8).

For the single services providing the needed software func-
tionalities of process steps, we take into account two diﬀerent
settings in order to evaluate the performance of our approach:
In Section 5.5.1, we use resource-intensive services for the sin-
In Section 5.5.2, we apply less resource-
gle process steps.
intensive services in order to evaluate whether this aspect leads
to signiﬁcant diﬀerences in the evaluation results. We assume
that the service makespans are rather long, i.e., that the services
cover long-running computational tasks.

5.1.2. Applied SLAs

We evaluate our approach using two diﬀerent process dead-
line scenarios. In the ﬁrst scenario, we use strict deadlines and
demand the latest point in time when the enactment of process
instances has to be ﬁnished to be 1.5 times the process model’s
average makespan from the point in time when the request is
sent. The second scenario allows more lenient SLA values and
we demand 2.5 times of the process model’s average makespan
as the maximum enactment duration. These values were cho-
sen based on preliminary evaluation runs and to account for the
startup time of VM instances and the time to pull Docker im-
ages and deploy container instances.

For calculating penalty cost, we apply a linear cost model
which assigns one unit of penalty cost per 10% of time units of
delay.

5.1.3. Process Request Arrival Patterns

Our experimental design accounts for two diﬀerent process
request arrival patterns. In one scenario, we design a constant
arrival pattern. Speciﬁcally, we choose to request two process
instances every two minutes, alternating process instance re-
quests for process models 1 to 5 and 6 to 10 until 50 process in-
stance requests have been sent. In a second scenario, we follow
a pyramid-like function for designing our arrival pattern. We
send a total of 100 randomly shuﬄed process instance requests
in diﬀerent batch sizes, ranging from one to ﬁve instances at a
time in an interval of one minute. Equation (29) represents the
pyramid pattern, n representing the time in minutes since the
start of the experiment, and a representing the number of new
process requests.

f (n) = a






1
(cid:100)(n + 1)/4(cid:101)
0
1
(cid:100)(n − 9)/20(cid:101)

i f 0 ≤ n ≤ 4
i f 5 ≤ n ≤ 17
i f 18 ≤ n ≤ 19
i f 20 ≤ n ≤ 35
i f 36 ≤ n ≤ 51

(29)

These two diﬀerent arrival patterns have been chosen in order to
evaluate the presented approach in a rather stable environment,
with process requests coming in constantly, and a more volatile
environment, where the requests do not follow such a constant
pattern. This may lead to diﬀerent results, since our optimiza-
tion model does not regard process request arrivals. However,
it might be interesting to integrate such knowledge in the future
(see Section 7).

Notably, we have also tested other arrival patterns and set-
tings in preliminary evaluation runs. The presented settings
have been chosen, since they are representative for the obser-
vations we have made in our preliminary tests.

5.2. Test Environment

For allowing reproducible test scenarios over multiple hours
and with diﬀerent pricing models, we perform the optimiza-
tion runs using the simulation mode of ViePEP. This means that
a complete process landscape is modeled and requested using

11

Table 9: Evaluation VM Types

VM Type Name
1
2
3
4
5
6
7

Provider
Private
Private
Private
Public A
Public A
Public A
Public A

CPU Cores
1
2
4
1
2
4
8

Cost per BTU
10
18
30
15
25
35
50

the eBPMS, and the optimization model is able to operate with
these values. However, the actual leasing of resources is not
activated. Instead, the utilization of VMs is calculated based on
the characteristics of the evaluation services (see Sections 5.5.1
and 5.5.2).

Our system considers seven VM types with the characteris-
tics listed in Table 9. As it can be seen, we allow both privately
hosted and publicly hosted VMs. Regarding the pricing model,
we assume that leasing a VM instance with x cores is cheaper
than leasing two VM instances with x/2 cores, i.e., the marginal
costs are decreasing. We apply a BTU of ﬁve minutes for both
private and public cloud resources. Also, we assume that VMs
from the private cloud are cheaper to lease than equally sized
VMs from the public cloud.

As it can be seen in the table, we do not further regard RAM
utilization, even though RAM is explicitly foreseen in the prob-
lem formulation presented in Section 4. The reason for this is
that preliminary results have shown that since both RAM and
CPU are additive resources, the RAM utilization does not im-
pact the overall cost too much. So, in order to keep the evalua-
tion section concise, we opted to not further discuss it here.

In the evaluation, we assume a VM startup time of 60 sec-
onds, a time to pull Docker images from the central registry
onto a VM instance of 30 seconds, and a time to start a new
Docker container and the contained service from an existing
image of two seconds. These values have been chosen based on
recommendations in the related work [22, 23, 24].

5.3. Metrics

We assess the outcome of our experiments by using diﬀer-
ent metrics: Minimizing the total cost is the core goal of FF-
SIPP. Accordingly, we measure the cost arising for enacting a
complete business process landscape over time. These cost in-
clude the VM leasing and penalty cost over all process enact-
ments during an experiment’s runtime.

In addition, we measure two secondary metrics in order to
assess if FFSIPP also contributes to achieving them: SLA ad-
herence calculates the percentage of process requests which
meet their SLAs, i.e., the process instances that ﬁnish their
enactment before the deﬁned deadline. As described in Sec-
tion 4.1, SLA adherence is not a hard constraint in our opti-
mization model, but is indirectly achieved by taking into ac-
count that penalty cost accrue if an SLA is violated.

In addition, we also measure the makespan, i.e., the total du-
ration for enacting all incoming process requests in a particular
experiment, from the ﬁrst-received request to the last ﬁnished
process step. Since the arrival of a process request is not needed

in our system model, this value is monitored by our evaluation
environment, i.e., ViePEP.

Experiments are repeated three times in order to take into
account that the duration of a service invocation may have some
spread, as described in Section 5.5.1. Accordingly, we calculate
the standard deviations σ for all metrics.

5.4. Baseline

As the baseline for the evaluation of FFSIPP, we apply a
slightly modiﬁed state-of-the-art approach for VM-based re-
source allocation and task scheduling by Hoenisch et al. [11],
called Service Instance Placement Problem (SIPP). The SIPP
approach uses an optimization model that directly schedules
process requests on VM instances, while each leased VM in-
stance is only able to enact process requests of one speciﬁc ser-
vice type. Containers are not regarded.

The modiﬁcations of the originally presented baseline are
made with regard to the utilized time constraints which we de-
ﬁne according to the time constraints also used by our approach
as presented in Constraints (1) and (2). Furthermore, we allow
the change of the oﬀered service type of a VM instance during
an uninterrupted leasing duration whenever no more service re-
quests are being enacted by a VM instance. When changing
the oﬀered service type, a service deployment time of another
30 seconds is considered and again only one service type is of-
fered at any point in time by a VM instance in the baseline.

5.5. Results

5.5.1. Resource-intensive Services

Table 10: Evaluation Services – Resource-Intensive

Service Type Name
A
B
C
D
E
F
G
H
I
J

CPU Load
in % (µcpu)
45
75
75
100
120
125
150
175
250
333

Service Makespan
in sec. (µdur)
40
80
120
40
100
20
40
20
60
30

As mentioned in Section 5.1.1, we apply two diﬀerent types
of services in our evaluation. First, we discuss rather resource-
intensive services, while services with smaller resource demands
are covered in Section 5.5.2.

The characteristics of the services for the scenario at hand
are presented in Table 10. The values in Table 10 represent
mean values of general normal distributions with σcpu = µcpu/10
and σdur = µdur/10. This is done in order to reﬂect that in real-
world settings, services will not always have the same makespan,
e.g., since the base loads of a container or VM may diﬀer. A
services’ CPU load refers to the required amount of CPU re-
sources in percent of a single core, meaning all services with a
CPU requirement of more than 100%, cannot be scheduled on
a single-core VM and the corresponding containers will need

12

to be placed on a multi-core VM instance. We assume that ser-
vices are fully parallelizable.

(a) Pyramid Arrival, Strict SLA

(a) Constant Arrival, Strict SLA

(b) Constant Arrival, Lenient SLA

Figure 1: Evaluation Results – Constant Arrival (Resource-Intensive Services)

Constant Arrival of Resource-Intensive Processes. First, we
compare the results for resource-intensive services for the con-
stant arrival pattern. Figure 1a shows the results for process
requests with a strict deadline, while Figure 1b depicts the re-
sults for the lenient SLA. The charts plot the leased CPU cores
over time and also show the corresponding process request ar-
rival patterns. The horizontal axes show the time in minutes.
The left vertical axes display the number of leased CPU cores
while the right vertical axes represent the number of parallel
process requests. An overview of the results is shown in Ta-
ble 11. The table presents the numerical average values of the
evaluation runs and the standard deviations. It is important to
distinguish between the total makespan in minutes as discussed
in Table 11, which refers to the time for enacting all incoming
process requests, and the times shown in Figure 1 which refer
to time slots when VM instances (cores) were leased.

As can be seen from Table 11, our optimization approach
yields a high SLA adherence of 98.67% with a very small stan-
dard deviation of only 1.15 percentage points (pp) and only 1
unit of penalty cost for process requests with strict enactment
deadlines. Compared to the baseline which only yields an SLA
adherence of 86.00% with a standard deviation of 2.00 pp, and
on average 15 units of penalty cost, the FFSIPP approach is sig-
niﬁcantly improving the detection of potential SLA violations
and also able to rapidly calculate a good scheduling strategy.
Furthermore, regarding the leasing cost, our approach leads to
only about half the cost of the baseline with also a lower asso-
ciated standard deviation.

When comparing the result for more lenient deadlines, we
can observe a much better SLA adherence of 98.67% for the

(b) Pyramid Arrival, Lenient SLA

Figure 2: Evaluation Results – Pyramid Arrival (Resource-Intensive Services)

SIPP baseline (as compared to strict deadlines), but again a bet-
ter SLA adherence of 100.00% for all performed runs using
the FFSIPP approach. This means our approach is able to ﬁn-
ish all incoming process requests within the deﬁned deadline
constraints for all evaluation runs. The leasing cost are again
nearly half as high compared to those of the baseline. The low
leasing cost of our approach are a result of the near-optimal
resource utilization that is realized through the dynamic assign-
ment and reassignment of Docker containers for the enactment
of diﬀerent service types that only reserve as much space from
the underlying VM instances as they actually require.

Notably, the enactment of process requests using our ap-
proach with lenient process deadlines leads to a longer total
makespan and lower total leasing cost compared to the enact-
ment of process requests with a strict deadline. When looking
at the number of leased resources in Figure 1b, we can clearly
observe that our approach leases a smaller amount of computa-
tional resources in terms of CPU cores than the SIPP baseline
at most points in time, and that our approach is able to post-
pone the enactment of process steps with a distant deadline for
as long as possible, if that is beneﬁcial in terms of cost. This
explains why after 33 minutes the number of leased resources
increases after it had already declined. The baseline approach
does not make use of the more distant deadline constraints in
a similarly eﬃcient way, and also does not utilize the existing
resources to the same degree as it is possible with containers.
Here, the shortcoming that the baseline assumes that only one
service can be deployed on one VM at the same time, is one ma-
jor cost driver. The reason why the makespan for the baseline
is similar for both the strict and the lenient process deadlines is
that there are already suﬃcient computational resources leased
at earlier points in time. These resources were not fully utilized
and were able to perform the enactment of process steps ahead
of their scheduling deadlines.

13

FFSIPPBaseline (SIPP)Process Arrivals0510152025303540Time in Minutes051015202530354045# Leased CPU Cores012345678910# Arrived ProcessesFFSIPPBaseline (SIPP)Process Arrivals0510152025303540Time in Minutes051015202530354045# Leased CPU Cores012345678910# Arrived ProcessesFFSIPPBaseline (SIPP)Process Arrivals05101520253035404550556065707580Time in Minutes051015202530354045# Leased CPU Cores012345678910# Arrived ProcessesFFSIPPBaseline (SIPP)Process Arrivals05101520253035404550556065707580Time in Minutes051015202530354045# Leased CPU Cores012345678910# Arrived ProcessesPyramid Arrival of Resource-Intensive Processes. Next, we com-
pare the enactment results for resource-intensive services for
the pyramid arrival pattern as introduced in Equation (29). An
overview of the results is again given in Table 11. Figure 2a
shows the results for process requests with a strict deadline,
while Figure 2b depicts the results for lenient process requests.
Again, when compared to the baseline, our approach results
in a better SLA adherence and lower associated penalty cost as
well as leasing cost that are approximately half as high. This
time, the diﬀerence between the enactment of process requests
with a strict or a lenient deadline is relatively small. The ex-
planation for the large saving of leasing cost of our approach is
the same as for the constant arrival pattern and can be attributed
to the more ﬁne-grained resource utilization which can be ac-
complished through FFSIPP. Once again, our approach was also
able to postpone steps into the future, leading to a longer to-
tal makespan for process requests with lenient deadlines, when
compared to the baseline.

It should be noted that lenient deadlines lead to better results
in both our approach as well as the baseline, but an important
observation can be made when looking at the evaluation more
closely. The performance beneﬁt, especially in terms of cost is
much higher when considering lenient deadlines for the base-
line approach.

Our approach also yields a slightly better result for lenient
deadlines, but in comparison, this diﬀerence is rather small. For
the baseline, a large number of requests that have to be enacted
in close temporal proximity result in having to lease individual
VMs for almost all service types. Some service types might
only occupy a small part of a leased machine while for other
types multiple machines need to be leased. More lenient dead-
lines allow the use of already leased machines when computa-
tional resources are freed up in the near future, leading to the
observed performance beneﬁt. Depending on the deﬁned pref-
erences and cost considering penalties, the system can accept
higher penalties for lower leasing cost. The higher the accepted
penalties are, the more leasing cost could be saved.

In contrast to SIPP, our approach is able to use free re-
sources from any leased VM to deploy a container of any type
with only as many assigned resources as the invocation of ser-
vice requests requires. In contrast, the SIPP assumes that there
is one service per VM running concurrently. This ﬂexible of
FFSIPP resource utilization leads to comparable leasing cost
for strict and lenient deadlines, as the number of leased but un-
used resources is not increased as easily as for the baseline.

The results clearly show the main strength of FFSIPP, which
is the better resource utilization due to a ﬁne-grained schedul-
ing leading to overall lower leasing cost. We show this main
strength while comparing our approach to an already strong
baseline, which delivers considerably better results in terms of
cost and SLA adherence than simple threshold-based ad-hoc
scheduling strategies [11]. The baseline approach already de-
livers a good SLA adherence as it detects potential violations in
time and optimizes the scheduling decision accordingly. Nev-
ertheless, FFSIPP leads to even less SLA violations.

14

(a) Constant Arrival, Strict SLA

(b) Constant Arrival, Lenient SLA

Figure 3: Evaluation Results – Constant Arrival (Less Resource-Intensive Ser-
vices)

Both approaches allow SLA violations if the associated pe-
nalty fee leads to lower total cost than leasing additional re-
sources to ﬁnish process enactments in time. As our approach
allows a more ﬂexible scheduling of containers on VMs, it can
perform cheaper scheduling decisions and make better use of
already leased resources, resulting in lower SLA violations to
leasing cost conﬂicts.

Generally, the results do not only depend on the number and
type of process request arrivals and the available resources, but
also on the attributes of the process steps. In particular, service
types with larger resource requirements will lead to large con-
tainers taking up a great amount of the oﬀered resources of a
VM instance and, therefore, fewer resources remaining avail-
able for other service requests of diﬀerent service types. This
consideration leads to the assumption that our approach is ca-
pable of outperforming the baseline even more when service
instances with smaller resource requirements are considered.

5.5.2. Less Resource-intensive Services

To evaluate this assumption, we perform the same evalu-
ation runs as discussed before. However, we are now using
the less resource-intensive service types introduced in Table 12,
instead of the previously used more resource-intensive service
types from Table 10.

Constant Arrival of Less Resource-Intensive Processes. The
results for the constant arrival pattern and the less resource-
intensive processes are presented in Table 13. Figure 3a shows
the results for process requests with a strict deadline, while Fig-
ure 3b depicts the results for lenient process requests.

As expected, the results for process requests composed of
less resource-intensive process steps show even higher savings
in terms of leasing cost when comparing our approach to the

FFSIPPBaseline (SIPP)Process Arrivals051015202530354045Time in Minutes051015202530354045# Leased CPU Cores012345678910# Arrived ProcessesFFSIPPBaseline (SIPP)Process Arrivals051015202530354045Time in Minutes051015202530354045# Leased CPU Cores012345678910# Arrived ProcessesArrival Pattern

Constant Arrival

Pyramid Arrival

Table 11: Evaluation Results – Resource-Intensive Services

SLA Level
Number of
Total Process Requests
Interval Between
Process Requests
Number of Parallel
Process Requests
SLA Adherence in %
(Standard Deviation)
Total Makespan in Minutes
(Standard Deviation)
Leasing Cost
(Standard Deviation)
Penalty Cost
(Standard Deviation)
Total Cost
(Standard Deviation)

FFSIPP

Baseline (SIPP)

FFSIPP

Baseline (SIPP)

Strict

Lenient

Strict

Lenient

Strict

Lenient

Strict

Lenient

50

120 Seconds

5

98.67
σ = 1.15
27.33
σ = 1.15
1148.33
σ = 20.50
1.00
σ = 1.00
1149.33
σ = 20.01

100.00
σ = 0.00
36.67
σ = 2.08
1102.00
σ = 14.42
0.00
σ = 0.00
1102.00
σ = 14.42

86.00
σ = 2.00
29.33
σ = 0.58
2201.00
σ = 55.56
15.00
σ = 2.65
2216.00
σ = 57.51

98.67
σ = 1.15
29.33
σ = 0.58
2052.67
σ = 147.99
0.67
σ = 0.58
2053.33
σ = 148.37

97.67
σ = 0.58
63.33
σ = 0.58
2322.00
σ = 39.74
3.33
σ = 1.53
2325.33
σ = 38.21

100

60 Seconds

f (n) – see Equation (29)

100.00
σ = 0.00
69.67
σ = 2.89
2181.67
σ = 89.44
0.00
σ = 0.00
2181.67
σ = 89.44

95.33
σ = 0.58
62.00
σ = 0.00
4413.33
σ = 121.49
10.67
σ = 1.15
4424.00
σ = 122.32

99.67
σ = 0.58
61.67
σ = 0.58
3975.00
σ = 118.87
0.33
σ = 0.58
3975.33
σ = 119.43

Table 12: Evaluation Services – Less Resource-Intensive

Service Type Name

A
B
C
D
E
F
G
H
I
J

CPU Load
in % (µcpu)
5
10
15
30
45
55
70
125
125
190

Service Makespan
in sec. (µdur)
40
80
120
40
100
20
40
20
60
30

baseline, with around 2.5 times lower cost. The SLA adherence
of our approach for less resource-intensive processes is compa-
rable to the evaluation of the more resource-intensive processes
as presented above, but the SIPP baseline shows a higher SLA
adherence with the less resource-intensive service types.

The improved SLA adherence for the baseline is easy to
explain when considering that with smaller service types more
service invocations of the same type can be enacted on a VM
instance, leading to a lower number of penalty cost versus leas-
ing cost conﬂicts for the baseline approach. As FFSIPP is able
to place requests on any already leased VM instance, there al-
ready exists a relatively large amount of scheduling ﬂexibility
for the more resource-intensive service types, leading to similar
SLA adherence values, independent of the required resources
for process steps.

All other observations already discussed for more resource-
intensive process steps can also be observed in the evaluation
runs discussed in this section, e.g., the consequences of more
lenient deadlines.

Pyramid Arrival of Less Resource-Intensive Processes. Finally,
we compare the results for less resource-intensive processes for
the pyramid arrival pattern as introduced in Equation (29). Ta-
ble 13 again gives an overview of the results. Figure 4a shows
the results for process requests with a strict deadline while Fig-

15

(a) Pyramid Arrival, Strict SLA

(b) Pyramid Arrival, Lenient SLA

Figure 4: Evaluation Results – Pyramid Arrival (Less Resource-Intensive Ser-
vices)

ure 4b depicts the results for lenient process requests.

Again, we can observe that our approach leads to about 2.5
times lower leasing cost for process requests composed of less
resource-intensive process steps when compared to the base-
line. We can also make the same observations for SLA ad-
herences as we have already done for the constant arrival of
less resource-intensive processes, as well as the other obser-
vations discussed above for more resource-intensive process
steps. However, there is one notable exception: For the strict
deadlines, the baseline provides a better SLA adherence of 98.67%
compared to FFSIPP’s 97.33%. However, this comes at the
price of a cost increase of 156.37%.

FFSIPPBaseline (SIPP)Process Arrivals05101520253035404550556065707580Time in Minutes051015202530354045# Leased CPU Cores012345678910# Arrived ProcessesFFSIPPBaseline (SIPP)Process Arrivals05101520253035404550556065707580Time in Minutes051015202530354045# Leased CPU Cores012345678910# Arrived ProcessesTable 13: Evaluation Results – Less Resource-intensive Services

Arrival Pattern

Constant Arrival

Pyramid Arrival

SLA Level
Number of
Total Process Requests
Interval Between
Process Requests
Number of Parallel
Process Requests
SLA Adherence in %
(Standard Deviation)
Total Makespan in Minutes
(Standard Deviation)
Leasing Cost
(Standard Deviation)
Penalty Cost
(Standard Deviation)
Total Cost
(Standard Deviation)

FFSIPP

Baseline (SIPP)

FFSIPP

Baseline (SIPP)

Strict

Lenient

Strict

Lenient

Strict

Lenient

Strict

Lenient

50

120 Seconds

5

100

60 Seconds

f (n) – see Equation (29)

97.33
σ = 1.15
27.33
σ = 1.15
479.33
σ = 36.07
2.33
σ = 0.58
481.67
σ = 36.12

99.33
σ = 1.15
37.33
σ = 3.06
512.00
σ = 32.14
1.33
σ = 2.31
513.33
σ = 34.44

93.33
σ = 2.31
29.67
σ = 1.15
1316.67
σ = 124.16
6.67
σ = 1.15
1323.33
σ = 125.13

99.33
σ = 1.15
31.00
σ = 4.36
1283.67
σ = 187.79
0.33
σ = 0.58
1284.0
σ = 188.34

97.33
σ = 0.58
63.67
σ = 2.31
1018.67
σ = 25.97
4.33
σ = 1.15
1023.00
σ = 24.88

100.00
σ = 0.00
71.67
σ = 0.58
996.67
σ = 48.33
0.00
σ = 0.00
996.67
σ = 48.34

98.67
σ = 0.58
61.67
σ = 1.15
2619.00
σ = 91.02
3.6
σ = 0.58
2622.67
σ = 90.47

100.00
σ = 0.00
63.33
σ = 4.04
2430.33
σ = 114.29
0.00
σ = 0.00
2430.33
σ = 114.29

5.6. Summary and Discussion

In this section, we presented an extensive evaluation of the
FFSIPP approach. In the evaluation, we considered a number
of evaluation process models, diﬀerent process request arrival
patterns and system settings, and presented the associated en-
actment results. We evaluated the results of the MILP-based
FFSIPP combined with the transformation step, comparing it
to a state-of-the-art approach which does not take into account
containers. We showed that due to the overall more eﬃcient
use of leased computational resources, our approach calculates
scheduling results with 2 to 2.5 times lower leasing cost if com-
pared to the baseline. FFSIPP also outperforms the baseline
with regard to SLA adherence, achieving adherence levels of at
least 97% for all evaluated scenarios.

Despite the promising results, it should be taken into ac-
count that the evaluation could of course be further extended.
For instance, the evaluation setting is relatively small, with 50
and 100 process requests, respectively. As mentioned in Sec-
tion 4.3, MILP-based optimization becomes diﬃcult to achieve
if the problem sizes increase. The reason for this is that the
scheduling of services is an NP-hard problem [11], i.e., it is un-
known if a solution to the optimization problem can be found
in polynomial time. Given the large number of variables, the
optimization problem shows scalability issues if too many pro-
cess requests have to be handled at the same time. This could
be solved by applying a heuristic algorithm to solve the opti-
mization problem, e.g., genetic algorithms [25].

6. Related Work

Elastic processes have gained quite a lot of attention by
the cloud computing and BPM research communities in recent
years.
In the following paragraphs, we discuss relevant con-
cepts for optimized scheduling and resource allocation for elas-
tic processes.

To start with, a multitude of solutions for task scheduling
and resource allocation have been proposed for single services
and applications and aiming at the VM level, e.g., [26, 27].

In addition, some approaches regard resource allocation and
task scheduling on the container level: [10] propose a game-
theoretic model, aiming to ﬁnd a solution which reduces the re-
sponse times while improving the resource utilization of cloud
providers. [28] formulate a multi-objective optimization prob-
lem for ﬁnding an optimal resource allocation and task schedul-
ing. Importantly, the presented solution aims at both the VM
level and the container level, i.e., the authors combine horizon-
tal and vertical scaling, as it is also the foundation for the work
at hand. [29] propose an Integer Linear Programming problem
that considers heterogeneous container requirements and VM
resources when calculating an optimal allocation as well as a
runtime reallocation of containers on a set of leased VMs.

The discussed approaches focus on cost measures based on
the actual utilization of cloud-based computational resources as
well as breaches of negotiated SLAs, but generally do not con-
sider the process perspective. Therefore, ﬁndings from resource
allocation and task scheduling for single services can not be di-
rectly mapped to elastic processes.

[30] propose an auto-scaling mechanism for the orchestra-
tion of containers. For this, the authors present a number of bio-
inspired algorithms which are used to scale the computational
resources. [31] presents a survey and a reference architecture
for container orchestration. It should be noted that container
orchestration is usually not based on complex process patterns
as we have applied in the work at hand.

For the realization of elastic processes, it is especially nec-
essary to account for the data and control ﬂows that come with
the enactment of processes [5]. Again, a number of resource
allocation and task scheduling solutions which aim at VMs (in-
stead of containers) have already been proposed: [6] present
a solution for scheduling multiple workﬂows with QoS con-
straints in the cloud. The proposed scheduling algorithm con-
siders factors that aﬀect the total makespan and cost of work-
ﬂows, and aims at improving the mean enactment time and
enactment cost of all workﬂows in a process landscape, while
taking into account QoS constraints. A multi-objective genetic
scheduling algorithm for BPEL workﬂows in distributed cloud

16

environments is presented by [7]. Their approach considers
data dependencies between BPEL workﬂow steps and uses work-
ﬂow enactment times as well as cost for the needed computa-
tional resources. By using weights, the Pareto-optimal solution
of the multi-objective heuristic can be transformed into a unique
solution. A number of strategies that are based on similar as-
sumptions have also been proposed by [32]. The authors aim
at scheduling process steps of business processes using elas-
tic cloud-based resources, while optimizing enactment time and
cost. Both the approaches of [7] and [32] provide solutions that
allow for a parallel enactment of processes, but in contrast to
our work, do not account for SLAs.

SLAs like deadlines for single tasks and processes have
been considered for sequential processes by [33]. The authors
introduce an adaptive conﬁguration algorithm for dynamically
managing VMs for service-based workﬂows in the cloud.

[8, 34] discuss an ILP-based solution and a genetic algo-
rithm that aim at ﬁnding an eﬃcient resource allocation for elas-
tic processes while reducing the number of migrations from one
VM to another VM. In contrast to the work at hand, processes
are not divided into steps. Hence, in our work, we aim at pro-
viding computational resources on a more ﬁne-grained level.

None of the so-far mentioned solutions makes use of con-
tainers. Hence, in the work at hand, we propose an approach
to exploit the advantages of container-based virtualization ap-
proaches, allowing a more ﬁne-grained model for resource al-
location and task scheduling for elastic processes. To the best
of our knowledge, the related work explicitly aiming at the op-
timization on the container-level is still very limited.

Most existing approaches have been proposed in the area
of scientiﬁc workﬂows (SWFs), e.g., [35, 36]. The main limi-
tation of these approaches is that they do not consider concur-
rent processes, which are the norm in the area of BPM. Fur-
thermore, SWFs are generally more data ﬂow-oriented, while
business process scheduling is used in complex landscapes with
concurrent process requests. Business processes often share
the same services and tend to be more control ﬂow-driven than
SWFs [37]. Hence, solutions from the ﬁeld of SWFs give some
interesting ideas, but cannot be directly applied to BPM.

In our former work [15, 25], we discuss an updated version
of ViePEP, which is able to use containers instead of VMs for
service deployment. We also extend our optimization models
for resource allocation and task scheduling by allowing opti-
mization both on the VM and on the container level. For this,
we apply genetic algorithms, but have so far not provided an
optimal solution on the VM level as we do in the work at hand
by formulating a MILP problem.

The idea of using microservices for realizing business pro-
cess steps is discussed by [38]. The authors make use of a dis-
tributed, actor-based approach, where actors are connected to
each other. This allows high scalability, but makes it necessary
that the actors are aware of each other and of the underlying
process models.

[39, 40] also make use of containers for process enactment.
Similar to the work at hand, the goal is to ﬁnd a cost-optimal
solution to the resource allocation and task scheduling prob-
lem. Notably, the authors allow scaling on the level of VMs and

a subsequent allocation of containers, i.e., also combine hori-
zontal and vertical scaling. For this, the authors apply linear
programming and genetic algorithms. However, optimization
is only done for single processes instead of complete process
landscapes, and containers (respectively the underlying compu-
tational resources) cannot be shared between processes. Pro-
cess deadlines are not regarded. In contrast to the work at hand,
[39, 40] do not allow a trade-oﬀ between SLA breaches and
cost through the integration of penalty fees. Nevertheless, the
approach by [39, 40] comes closest to our work presented here.
Last but not least, it should be noted that our work makes
use of worst-case assumptions for chosen paths in a process
model, startup times of containers, etc. While this is a very
common approach in business process and service research, it
could of course also be interesting to take into account some
predictions about process behavior in order to avoid the com-
plex and conservative worst-case analyses applied in our work.
Most importantly, predicting the next process step, as proposed
by [41] for implicitly deﬁned process models, is necessary for
the work at hand. Going one step further by incorporating ap-
proaches to predict the chosen process path, e.g., [42], would
allow us to decrease the search space of the optimization, which
could result in faster results which ﬁt the actual process enact-
ments even better.

7. Conclusion

Elastic processes, i.e., business processes enacted on elastic
cloud infrastructure, have so far been primarily regarded on a
rather coarse-grained level, with VMs being used as computa-
tional resources. The utilization of container technologies in or-
der to provide more ﬁne-grained control over cloud-based com-
putational resources has the potential to save cost and to beneﬁt
from startup times that are signiﬁcantly smaller. One particu-
lar research question in this area is how to make sure that the
process steps are carried out in a cost-eﬃcient way while taking
into account the QoS requirements of the process owners.

In this paper, we introduced a multidimensional optimiza-
tion approach, which is combined with a transformation step
in order to realize cost-eﬃcient auto-scaling of container-based
elastic processes. Our approach enables ﬁne-grained resource
allocation and task scheduling for elastic processes. Scaling is
achieved over four dimensions, i.e., horizontally and vertically,
both for VM resources and Docker containers. The approach
focuses on the minimization of VM leasing cost as well as pe-
nalty cost for SLA violations. As the evaluation has shown, this
leads to signiﬁcant cost reductions in the enactment of business
process landscapes, up to a factor of 2 to 2.5 compared to a
state-of-the-art approach. Also, our solution further improves
the already very good SLA adherence of the state-of-the-art.

In our future work, we want to integrate approaches which
predict the process request arrival patterns based on historical
data. This way, the FFSIPP could be enhanced from a reactive
into a predictive approach. Also, we want to take into account
that process makespans may also include communication and
data transfer overhead, by extending our system and optimiza-
tion model accordingly. Last but not least, applying serverless

17

approaches could lead to an even higher level of ﬂexibility, and
is therefore an interesting option for the future.

Acknowledgements

The ﬁnancial support by the Austrian Federal Ministry for
Digital and Economic Aﬀairs, the National Foundation for Re-
search, Technology and Development as well as the Christian
Doppler Research Association for the Christian Doppler Labo-
ratory for Blockchain Technologies for the Internet of Things is
gratefully acknowledged.

References

[1] U. Lampe, O. Wenge, A. M¨uller, R. Schaarschmidt, On the relevance of
security risks for cloud adoption in the ﬁnancial industry, in: 19th Amer-
icas Conf. on Information Systems, AIS, 2013.

[2] S. P. Ahuja, S. Mani, J. Zambrano, A survey of the state of cloud comput-

ing in healthcare, Network and Comm. Techn. 1 (2) (2012) 12.

[3] S. Dustdar, Y. Guo, B. Satzger, H.-L. Truong, Principles of elastic pro-

cesses., IEEE Internet Comp. 15 (5) (2011).

[4] R. Breu, S. Dustdar, J. Eder, C. Huemer, G. Kappel, J. K¨opke, P. Langer,
J. Mangler, J. Mendling, G. Neumann, S. Rinderle-Ma, S. Schulte,
S. Sobernig, B. Weber, Towards Living Inter-Organizational Processes,
in: 15th IEEE Conf. on Business Informatics, IEEE, 2013, pp. 363–366.
[5] S. Schulte, C. Janiesch, S. Venugopal, I. Weber, P. Hoenisch, Elastic busi-
ness process management: State of the art and open challenges for bpm
in the cloud, Future Generation Computer Systems 46 (2015) 36–50.
[6] M. Xu, L. Cui, H. Wang, Y. Bi, A Multiple QoS Constrained Scheduling
Strategy of Multiple Workﬂows for Cloud Computing, in: 2009 IEEE Int.
Symp. on Parallel and Distributed Processing with Applications, IEEE,
2009, pp. 629–634.

[7] E. Juhnke, T. D¨ornemann, D. Bock, B. Freisleben, Multi-objective
Scheduling of BPEL Workﬂows in Geographically Distributed Clouds,
in: IEEE 4th Int. Conf. on Cloud Computing, IEEE, 2011, pp. 412–419.
[8] G. Rosinosky, S. Youcef, F. Charoy, Eﬃcient Migration-Aware Algo-
rithms for Elastic BPMaaS, in: 15th Int. Conf. on Business Process Man-
agement, Vol. 10445 of LNCS, Springer, 2017, pp. 147–163.

[9] C. Pahl, Containerization and the PaaS Cloud, IEEE Cloud Comp. 2 (3)

(2015) 24–31.

[10] X. Xu, H. Yu, X. Pei, A Novel Resource Scheduling Approach in Con-
tainer Based Clouds, in: IEEE 17th Int. Conf. on Computational Science
and Engineering, IEEE, 2014, pp. 257–264.

[11] P. Hoenisch, D. Schuller, S. Schulte, C. Hochreiner, S. Dustdar, Optimiza-
tion of complex elastic processes, IEEE T. on Services Computing 9 (5)
(2016) 700–713.

[12] M. Michael, J. Moreira, D. Shiloach, R. Wisniewski, Scale-up x Scale-
out: A Case Study using Nutch/Lucene, in: IEEE Int. Parallel and Dis-
tributed Processing Symp., IEEE, 2007, pp. 1–8.

[13] D. Kossmann, T. Kraska, S. Loesing, An Evaluation of Alternative Ar-
chitectures for Transaction Processing in the Cloud, in: 2010 ACM SIG-
MOD Int. Conf. on Management of Data, ACM, 2010, pp. 579–590.
[14] W. M. P. van der Aalst, A. H. M. Ter Hofstede, B. Kiepuszewski, A. P.
Barros, Workﬂow Patterns, Distributed and Parallel Databases 14 (1)
(2003) 5–51.

[15] P. Waibel, C. Hochreiner, S. Schulte, A. Koschmider, J. Mendling,
ViePEP-C: A Container-based Elastic Process Platform, IEEE T. on
Cloud Comp. 9 (4) (2021) 1657–1674.

[16] P. Mell, T. Grance, The NIST deﬁnition of cloud computing, NIST Spe-

cial Publication 800 (2011).

[17] M. C. Jaeger, G. Rojec-Goldmann, G. Muhl, QoS aggregation for Web
service composition using workﬂow patterns, in: 8th IEEE Int. Enterprise
Distributed Object Comp. Conf., IEEE, 2004, pp. 149–159.

[18] Z. ´A. Mann, Allocation of virtual machines in cloud data centers—a sur-
vey of problem models and optimization algorithms, ACM Comp. Sur-
veys 48 (1) (2015) 11.

[19] M. Borkowski, S. Schulte, C. Hochreiner, Predicting Cloud Resource Uti-
lization, in: 9th IEEE/ACM Int. Conf. on Utility and Cloud Comp., ACM,
2016, pp. 37–42.

[20] T. Curran, G. Keller, A. Ladd, SAP R/3 business blueprint: understanding

the business process reference model, Prentice-Hall, Inc., 1997.

[21] J. Mendling, H. Verbeek, B. F. van Dongen, W. M. P. van der Aalst,
G. Neumann, Detection and prediction of errors in EPCs of the SAP ref-
erence model, Data & Knowledge Engineering 64 (1) (2008) 312–329.

[22] B. Xavier, T. Ferreto, L. Jersak, Time provisioning Evaluation of KVM,
Docker and Unikernels in a Cloud Platform, in: 16th IEEE/ACM Int.
Symp. on Cluster, Cloud and Grid Comp., IEEE, 2016, pp. 277–280.
[23] A. Lingayat, R. R. Badre, A. K. Gupta, Performance Evaluation for De-
ploying Docker Containers On Baremetal and Virtual Machine, in: 3rd
Int. Conf. on Communication and Electronics Systems, IEEE, 2018, pp.
1019–1023.

[24] G. E. de Velp, E. Rivi´ere, R. Sadre, Understanding the performance of
container execution environments, in: 6th Int. Works. on Container Tech-
nologies and Container Clouds, ACM, 2020, pp. 37–42.

[25] P. Waibel, A. Yeshchenko, S. Schulte, J. Mendling, Optimized Container-
based Process Execution in the Cloud, in: 26th Int. Conf. on Cooperative
Information Systems, Vol. 11230 of LNCS, Springer, 2018, pp. 3–21.
[26] L. Wu, S. K. Garg, R. Buyya, SLA-based resource allocation for software
as a service provider (SaaS) in cloud computing environments, in: 11th
Int. Symp. Symposium on Cluster, Cloud and Grid Comp., IEEE, 2011,
pp. 195–204.

[27] R. Van den Bossche, K. Vanmechelen, J. Broeckhove, Online cost-
eﬃcient scheduling of deadline-constrained workloads on hybrid clouds,
Future Generation Computer Systems 29 (4) (2013) 973–985.

[28] P. Hoenisch, I. Weber, S. Schulte, L. Zhu, A. Fekete, Four-fold Auto-
Scaling on a Contemporary Deployment Platform using Docker Contain-
ers, in: 13th Int. Conf. on Service Oriented Comp., Vol. 9435 of LNCS,
Springer, 2015, pp. 316–323.

[29] M. Nardelli, C. Hochreiner, S. Schulte, Elastic provisioning of virtual
machines for container deployment, in: 8th ACM/SPEC Int. Conf. on
Performance Engineering Companion, ACM, 2017, pp. 5–10.

[30] J. Herrera, G. Molt´o, Toward Bio-Inspired Auto-Scaling Algorithms: An
Elasticity Approach for Container Orchestration Platforms, IEEE Access
8 (2020) 52139–52150.

[31] E. Casalicchio, Container Orchestration: A Survey, in: A. Puliaﬁto, K. S.
Trivedi (Eds.), Systems Modeling: Methodologies and Tools, Springer,
2019, pp. 221–235.

[32] K. Bessai, S. Youcef, A. Oulamara, C. Godart, S. Nurcan, Business pro-
cess scheduling strategies in cloud environments with fairness metrics, in:
2013 IEEE Int. Conf. on Services Comp., IEEE, 2013, pp. 519–526.
[33] Y. Wei, M. B. Blake, Proactive virtualized resource management for ser-

vice workﬂows in the cloud, Computing 98 (5) (2016) 523–538.

[34] G. Rosinosky, S. Youcef, F. Charoy, A Genetic Algorithm for Cost-Aware
Business Processes Execution in the Cloud, in: 16th Int. Conf. on Service-
Oriented Comp., Vol. 11236 of LNCS, Springer, 2018, pp. 198–212.
[35] C. Zheng, B. Tovar, D. Thain, Deploying High Throughput Scientiﬁc
Workﬂows on Container Schedulers with Makeﬂow and Mesos, in: 17th
IEEE/ACM Int. Symp. on Cluster, Cloud and Grid Comp., IEEE, 2017,
pp. 130–139.

[36] S. L´opez-Huguet, A. P´erez, A. Calatrava, C. de Alfonso, M. Caballer,
G. Molt´o, I. Blanquer, A self-managed Mesos cluster for data analytics
with QoS guarantees, Future Generation Computer Systems 96 (2019)
449–461.

[37] B. Lud¨ascher, M. Weske, T. McPhillips, S. Bowers, Scientiﬁc Workﬂows:
Business as Usual?, in: 7th Int. Conf. on Business Process Management,
Vol. 5701 of LNCS, Springer, 2009, pp. 31–47.

[38] K. Andrews, S. Steinau, M. Reichert, Engineering a Highly Scalable
Object-Aware Process Management Engine Using Distributed Microser-
vices, in: 26th Int. Conf. on Cooperative Information Systems, Vol. 11230
of LNCS, Springer, 2018, pp. 80–97.

[39] K. Boukadi, R. Grati, M. Rekik, H. Ben-Abdallah, From VM to container:
A linear program for outsourcing a business process to cloud containers,
in: 25th Int. Conf. on Cooperative Information Systems, Vol. 10573 of
LNCS, Springer, 2017, pp. 488–504.

[40] K. Boukadi, R. Grati, M. Rekik, H. Ben-Abdallah, Business process out-
sourcing to cloud containers: How to ﬁnd the optimal deployment?, Fu-
ture Generation Computing Systems 97 (2019) 397–408.

18

[41] J. Evermann, J. Rehse, P. Fettke, Predicting process behaviour using deep

learning, Decision Support Systems 100 (2017) 129–140.

[42] M. Polato, A. Sperduti, A. Burattin, M. de Leoni, Time and activity
sequence prediction of business process instances, Computing 100 (9)
(2018) 1005–1031.

19

