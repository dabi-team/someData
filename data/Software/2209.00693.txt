2
2
0
2

p
e
S
7
2

]
L
D
.
s
c
[

4
v
3
9
6
0
0
.
9
0
2
2
:
v
i
X
r
a

A large dataset of software mentions in the biomedical literature

Ana-Maria Istratea, Donghui Lia, Dario Taraborellia, Michaela Torkara, Boris Veytsmana,b, Ivana Williamsa

aChan Zuckerberg Initiative, 801 Jeﬀerson Street, Redwood City, CA, 94063, USA
bGeorge Mason University, Fairfax, VA, 22030, USA

Abstract

We describe the CZ Software Mentions dataset, a new dataset of software mentions in biomedical papers. Plain-text
software mentions are extracted with a trained SciBERT model from several sources: the NIH PubMed Central collection
and from papers provided by various publishers to the Chan Zuckerberg Initiative. The dataset provides sources, context
and metadata, and, for a number of mentions, the disambiguated software entities and links. We extract 1.12 million
unique string software mentions from 2.4 million papers in the NIH PMC-OA Commercial subset, 481k unique mentions
from the NIH PMC-OA Non-Commercial subset (both gathered in October 2021) and 934k unique mentions from
3 million papers in the Publishers’ collection. There is variation in how software is mentioned in papers and extracted by
the NER algorithm. We propose a clustering-based disambiguation algorithm to map plain-text software mentions into
distinct software entities and apply it on the NIH PubMed Central Commercial collection. Through this methodology,
we disambiguate 1.12 million unique strings extracted by the NER model into 97,600 unique software entities, covering
78% of all software-paper links. We link 185,000 of the mentions to a repository, covering about 55% of all software-paper
links. We describe in detail the process of building the datasets, disambiguating and linking the software mentions, as
well as opportunities and challenges that come with a dataset of this size. We make all data and code publicly available
as a new resource to help assess the impact of software (in particular scientiﬁc open source projects) on science.

Contents

1 Introduction

2 Materials and Methods

2.1 Full text collection . . . . . . . . . . . . . .
2.2 Software Mentions Extraction . . . . . . . .
2.3 Software Mentions Linking and Disambigua-
tion . . . . . . . . . . . . . . . . . . . . . .
2.4 Software Mentions Disambiguation . . . . .

4
4
2.4.1 Keywords-based Synonym Generation 6
6
. . .
SciCrunch Synonyms Retrieval
2.4.2
7
2.4.3
. . . .
String Similarity Algorithms
7
2.4.4 Clustering . . . . . . . . . . . . . . .
8
2.5 Software Mentions Linking . . . . . . . . . .
8
2.6 Dataset Curation . . . . . . . . . . . . . . .
8
2.6.1 Motivation . . . . . . . . . . . . . .
9
2.6.2 Curation Guidelines & Evaluation .
9
Inter-Annotator Agreement . . . . .
2.6.3

3 Evaluation & Results

3.1 Final Dataset . . . . . . . . . . . . . . . . .
3.2 Disambiguation . . . . . . . . . . . . . . . .
3.3 Linking . . . . . . . . . . . . . . . . . . . .

4 Code & Data Availability

5 Discussion

5.1 Extraction . . . . . . . . . . . . . . . . . . .

10
10
10
11

11

11
11

1

5.2 Software Name Variability in the Literature
5.3 Disambiguation . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
5.4 Use cases
5.4.1 Trends in imaging . . . . . . . . . .
5.4.2 Penetration of single-cell methods in
biomedicine . . . . . . . . . . . . . .
5.4.3 Other usage . . . . . . . . . . . . . .

2

3
3
4

6 Next Steps

7 Conclusion

8 Acknowledgements

Appendix A

Schema Normalization

Appendix B

Curation Guidelines

Appendix C

Datasets Description

Raw ﬁles

Appendix C.3

. . . . . . . . . . .
Appendix C.1
Appendix C.2 Disambiguation Results . . .
Appendix C.2.1 Disambiguated File . . .
Linking Results . . . . . . . .
. .
Appendix C.3.1 Raw Metadata Files
Appendix C.3.2 Normalized Metadata
Files . . . . . . . . . . . . . . . . . .
Appendix C.3.3 Master Metadata File .
. . . . . . .
Evaluation Files
Appendix C.4.1 Curated Software Men-
tions . . . . . . . . . . . . . . . . . .

Appendix C.4

13
13
14
14

14
14

14

15

15

17

17

19
19
21
23
23
23

24
24
25

25

 
 
 
 
 
 
Appendix C.4.2 Linking & Disambigua-
tion Evaluation Files . . . . . . . . .
. . . . . .

Intermediate Files

Appendix C.5

Additional Examples of String

Appendix D
Variability

List of Figures

5

4

1
2
3

Variations of a mention . . . . . . . . . . .
Software entity data model
. . . . . . . . .
Examples of string variability for the soft-
. . . . . . . . . . . . . . . . .
ware ‘limma’
Examples of string variability for the soft-
ware ‘BLAST’ . . . . . . . . . . . . . . . . .
Sample of Disambiguation results for the
software entity scikit-learn . . . . . . . . .
Sample of Disambiguation results for the
software entity ImageJ . . . . . . . . . . .
Sample of Disambiguation results for the
software entity SPSS . . . . . . . . . . . .
D.8 Examples of string variability for the soft-
. . . . . . . . . . . . . . .
D.9 Examples of string variability for the soft-
. . . . . . . . . . . . . . .

ware ‘GraphPad’

ware ‘MATLAB’

7

6

25
25

26

4
4

6

7

12

12

13

27

27

List of Tables

7

6

7
9
9

3
4
5
7

1
2
3
4
5

7
8 Metrics of curated datasets
9

PMC OA statistics . . . . . . . . . . . . . .
Publishers collection statistics . . . . . . . .
Papers in Publishers’ Collection . . . . . . .
Keywords used for synonym generation . . .
Examples of software synonym pairs retrieved
through the SciCrunch API . . . . . . . . .
Examples of software synonym pairs retrieved
through keywords synonym generation . . .
Normalized metadata . . . . . . . . . . . . .
. . . . . . . . .
Expert evaluation of most frequent men-
tions extracted by the NER model from the
PMC-OA comm dataset . . . . . . . . . . .
10 Expert evaluation of 1000 most frequent men-
tions extracted by the NER model
. . . . .
IAA curation values for the expert evalua-
tion of the most frequent mentions extraced
by the NER model from the PMC-OA comm
10
dataset . . . . . . . . . . . . . . . . . . . . .
11
12 OA PMC comm disambiguation and linking
11
13 Disambiguation Evaluation . . . . . . . . .
11
Linking Coverage . . . . . . . . . . . . . . .
14
15
11
Linking Evaluation . . . . . . . . . . . . . .
16 Metadata Fields Statistics for linked mentions 12
A.17 Schema Normalization . . . . . . . . . . . .
18
B.18 Curation Guidelines - Examples of mentions:
software & algorithm. These examples were
. . . .
used for training our curation team.

20

11

10

9

B.19 Curation Guidelines - Examples of mentions:
not-software. These examples were used for
. . . . . . . . .
training our curation team.
B.20 Curation Guidelines - Examples of mentions:
“other” mentions. These examples were used
. . . . . . .
for training our curation team.

21

22

List of Algorithms

1
2

Disambiguation and linking . . . . . . . . . .
Clustering and naming . . . . . . . . . . . .

6
8

1. Introduction

The common adage says that the work of the scien-
tist is only as good as their tools. Since the last century
software has become a key tool in a scientist’s toolbox
and in recent years some of the most important break-
throughs in science—from the solution of 50-year-old pro-
tein folding problem [1] to the ﬁrst-ever direct image of
a black hole’s event horizon [2]—have been made possi-
ble through advanced computational methods applied to
large swaths of data. Yet, identifying and crediting the
computational tools that enabled these discoveries, and
rewarding their creators, remains a challenge. While there
are established norms in science for giving formal citation
and credit to the authors of scholarly papers going back
centuries, software—as many types of non-traditional re-
search outputs [3, 4, 5]—is often neglected or treated as
a second-class type of output and eminently hard to cite.
Not being able to measure the impact of critical software
tools that enable scientiﬁc progress makes it hard for their
authors and maintainers to pursue scientiﬁc careers and to
obtain funding for their work [6, 7, 8, 9, 10]. Furthermore,
it makes it more diﬃcult for other scientists to reproduce
results in scientiﬁc papers, and creates barriers for fun-
ders who need to objectively evaluate the impact of their
support [9, 11, 12, 13, 14].

While these problems have been well recognized [15,
9], and a number of recommendations for citing software
have been published [16, 17], in scientiﬁc papers software
is often only credited with informal mentions and phrases
like for analysis ImageJ software was used. Until citation
practices for software are improved, an analysis of the text
of the papers remains necessary in order to get insight into
software usage.

The extraction of software mentions from the text of
scientiﬁc papers has attracted the attention of researchers
for quite some time (see the review [18] and the recent pub-
lications [19, 20]). There are several datasets available, in-
cluding SoftwareKG, a knowledge graph that contains in-
formation about software mentions from more than 51,000
scientiﬁc articles from the social sciences [21]; SoMeSci, a
curated collection of 3756 software mentions in 1367 PubMed
Central articles [22]; Softcite, a dataset of manual anno-
tations of 4971 academic PDFs in biomedicine and eco-
nomics [23]; [24], a dataset of 318,138 software mentions

2

based on CORD-19 dataset; and [25], another dataset
of 77,449 software mentions based on CORD-19 dataset.
These datasets are based on limited samples of scholarly
articles. A paper based on a similar approach [26] was
published while this preprint was in preparation. Simi-
larly to our work, it is based on Pubmed Central Open
Access dataset. The methodology for software extraction
is similar and based on SciBERT [27]. The paper uses a hi-
erarchical multi-task labeling model to extract additional
tags besides software and the version, such as software
type, mention type and additional information. The train-
ing corpora for our datasets are diﬀerent: we trained our
model on SoftCite [23], while Schindler et al. trained the
model on the SoMeSci dataset [22]. Our disambiguation
methodologies are overall similar and built on clustering
algorithms, with diﬀerences in the type of features used.
There is also a diﬀerence in the number of clusters ob-
tained. Our linking methodology is quite diﬀerent. The
authors predict links based on training on the SoMeSci
dataset, whereas we predict links by querying a number
of repositories on names of clusters obtained through dis-
ambiguation. The authors extract 301,825,757 triples de-
scribing 11.8M software mentions and construct a knowl-
edge graph based on the results. The sizes of the resulting
mention datasets are similar: ours has 19.3M mentions
(Table 1). The formats of the ﬁnal datasets are also dif-
ferent: for example, we provide context for each mention
in the dataset. We plan to conduct a more detailed com-
parison of our results and methods.

In this work we describe a series of larger datasets
of software mentions based on (1) the full PubMed Cen-
tral collection [28] downloaded in October 2021 containing
more than 3.8 million papers (Table 1), and (2) a collec-
tion of papers provided by various publishers to the Chan
Zuckerberg Initiative (Table 3). We use a SciBERT-based
model trained on the SoftCite dataset [23] to extract soft-
ware mentions from these corpora. There can be large
variability in how a software entity is mentioned in text or
extracted by the NER model, so we propose a clustering-
based technique to disambiguate the plain-text mentions
into distinct software entities (Section 2.4). We also de-
scribe a methodology to link the mentions to PyPI, CRAN,
Bioconductor, SciCrunch or GitHub (Section 2.5).

We apply the disambiguation and linking methodolo-
gies to the PMC-OA comm subset, from which the NER
model extracts 1.12 million unique string software men-
tions appeared in 2.4 million papers. We are able to dis-
ambiguate 320,000 of these mentions into 97,600 unique
software entities, covering about 78% of all links in the
dataset. For all the other strings extracted by the NER
model, we cannot conﬁdently map them to a software en-
tity through our methodology. We also link about 185,000
mentions to a repository, covering about 55% of all software-
paper links.

We curate a proportion of the ﬁnal datasets by engag-
ing a domain expert team. Through this process, we ﬁnd
that the precision of the NER model ‘in the wild”, on the

3

Table 1: PMC OA statistics

Parameter

Commercial set Non-commercial set

Number of
papers
Number of
papers with
at least one
mention
Number of
mentions
Number of
unique
mentions

2,433,010

1,442,868

1,732,603

758,246

14,770,209

4,546,607

1,120,125

481,972

top 10000 mentions (by frequency) in the PMC-OA comm
subset, or Precision@10k is 69.66% (Precision score of the
model on the SoftCite dataset is 90%). We describe in de-
tail our methodology and make the ﬁnal datasets available
to the community. We hope this work serves as a resource
for assessing the impact of scientiﬁc software.

2. Materials and Methods

2.1. Full text collection

We used two separate full text collections. The ﬁrst
dataset is based on the PubMed Central collection [28]
(PMC OA) downloaded on October 2021. The PMC OA
collection has two subsets: comm subset licensed for both
commercial and non-commercial use, and non comm sub-
set licensed for non-commercial use only, including data
mining. We kept these data separate to ensure that our
data set can be utilized for both commercial and non-
commercial uses (see the discussion of non commercial li-
censing in [29]). The statistics of the data set is shown in
Table 2. Second, there were full-text manuscripts of schol-
arly articles (both Open Access and paywalled manuscripts)
provided to the Chan Zuckerberg Initiative by publishers
under diﬀerent agreements (Publishers’ collection). This
corpus also includes preprints from bioRxiv. The prove-
nance and time span of this collection varies (see Table 3).
As seen from Tables 3 and 1, the datasets have signiﬁcant
overlap.

The Publishers’ collection was stored in LXML format
PMC OA papers were downloaded in NXML format. For
parsing LXML we used lxml [30]; for parsing NXML we
used a modiﬁed pubmed parser software [31]. Our mod-
iﬁcations concerned table caption extraction (not imple-
mented in the original) and speeding up parsing. They,
along with the other modules, are available at the GitHub
site accompanying this publication. The extracted text
was fed into the SciBERT-based NER model (Section 2.2)
and further processed (Section 2.4).

Table 2: Publishers collection statistics

Parameter

Publishers Collection

Number of papers
Number of papers with at
least one mention
Number of mentions
Number of unique mentions

16,809,266
2,893,518

48,160,836
934,704

2.2. Software Mentions Extraction

We use a NER model to extract plain-text software
mentions from our corpus. The SciBERT [27] model has
been ﬁne-tuned on the SoftCite dataset [23] to recognize
mentions of software and respective version. This model
achieves a 10-fold cross-validation F1 score of 0.92. More
details can be found at the address https://github.com/
chanzuckerberg/software-mention-extraction. This
model was previously used on the CORD-19 dataset [25].
For a part of the collection: OA PMC corpus released
under free license for both commercial and non-commercial
use (comm dataset) we performed linking and disambigua-
tion described below.

2.3. Software Mentions Linking and Disambiguation

There can be large variability in how a piece of soft-
ware is being mentioned in text (Figure 1). For example, a
software can be mentioned through its full name (e.g. Sta-
tistical Package for Social Sciences) or its acronym (e.g.
SPSS ). There can also be multiple name variations com-
monly accepted as referring to the same software entity
(e.g.
sklearn and scikit-learn). And there can be vari-
ability in how a software mention is being mentioned by
researchers (eg Image J and ImageJ or GraphPad Prism
and GraphPad and Prism). Moreover, there can be typos,
either introduced by the authors (e.g. scikits-learn) or by
parsing the XML of the papers, and there can be variabil-
ity in how the NER algorithm extracts diﬀerent software
mentions (e.g. partial matches). If we want to assess the
impact of a software entity, we need to be able to map
all of the diﬀerent string variations that correspond to a
particular software entity together.

Our goal is to build a software entity data model and
describe a software entity through its name (eg scikit-learn,
Figure 2), the string variations under which this entity
appears mentioned in the literature, as extracted by the
NER model (e.g. sklearn, Sklearn API, scikit-learn API,
scikits-learn, Python toolbox scikit-learn, Scikit-Learn Li-
brary, etc) and, ideally, a link to a repository or database,
such as https://pypi.org/project/scikit-learn.

To achieve this, after extracting the software mentions
from the PMC-OA corpus using the NER model, we use
two additional methodologies: disambiguation and linking
for the part free for commercial use (Table 1). We deﬁne
disambiguation as the process of mapping various string
variations of the same software entity together. Linking

Figure 1: Variations of a mentions. All of the string variations should
be counted as mentions of the same software entity. In this case, all of
these mentions should be counted as mentions of the software entity
scikit-learn. These examples come from running the NER model on
the PMC-OA corpus. The highlighted sequence is the exact sequence
extracted by the NER model.

Figure 2: Software entity data model

refers to mapping a software entity to a URL in a reposi-
tory or database. Our algorithm is shown as Algorithm 1.
We describe each of these steps in more detail in the fol-
lowing sections.

2.4. Software Mentions Disambiguation

In the PMC-OA comm corpus, MATLAB, the R pack-
age limma and GraphPad are each extracted by the NER
model under more than 200 string variations. BLAST, an
algorithm for comparing primary biological sequence in-
formation, has more than 500 variations extracted by the
NER model (see Figures 3 and 4 with additional exam-
ples in Appendix D). Through disambiguation, our goal
is to group diﬀerent string variations of the same software
entity together. For instance, all the string variations for

4

Table 3: Papers in Publishers’ Collection

Publisher

Number of papers
with at least one
mention

American Association of Neurological Surgeons
American College of Physicians
American Institute of Aeronautics and Astronautics
American Institute of Physics
American Physical Society
American Psychiatric Association Publishing
American Society for Clinical Investigation
American Society for Microbiology
American Society of Agricultural and Biological Engineers
American Society of Civil Engineers
American Thoracic Society
Annual Reviews
BioOne
bioRxiv
Cambridge University Press
CSIRO Publishing
De Gruyter Open
Edinburgh University Press
eLife Sciences Publications, Ltd
Emerald Publishing Limited
Future Medicine Ltd
Hindawi
Hogrefe Publishing
Impact Journals
INFORMS
Institute of Electrical & Electronics Engineers
IntechOpen
International Union of Crystallography
IOS Press
MA Healthcare
Mary Ann Liebert, Inc., publishers
MDPI
MIT Press
Public Library of Science
PubMed Central Open Access
Royal College of Surgeons of England
Royal Society of Chemistry
SAGE Publications
SLACK Incorporated
Society of Photo-Optical Instrumentation Engineers
Springer Nature
Taylor & Francis
The Royal Society
University of California Press
Wolters Kluwer

Total

5

7512
1001
8670
48,565
9649
5171
8787
4943
1132
30,641
2247
8307
61,822
33,136
205
5999
4603
6784
1652
55,367
18,934
23,052
8067
4191
669
43,580
1
21
3989
8284
65,597
14,334
2544
186,313
1,758,247
1458
73,586
358,849
2801
31
329,910
607,702
342
457
32,940

3,852,092

Input Papers P = P1, P2, ...Pn
Output software entities ← {}

for Pi ∈P do

Vi ← NER(Pi) // strings extracted by the NER
algorithm from Pi

end for
S ← V1 ∪ V2 ∪ ... ∪ Vn // set of all strings extracted
M ← get similarity matrix(S)
C1, C2, ...Cn ← get connected components(M )
for Ci ∈ C1, C2, ...Cn do

Mi ← get similarity matrix(Ci)
Di ← 1 − Mi //distance matrix
for Clusterj Di ∈ DBSCAN(Di) do

← highest frequency string(Clusterj Di)

sjDi
software entities[sjDi
software entities[sjDi

end for

][’variations’] ← Clusterj Di
][’link’] ← get link(sjDi

)

end for
Algorithm 1: Disambiguation and linking

limma should be grouped under the limma software entity,
all the string variations for MATLAB should be grouped
under the MATLAB software entity, and so on.

Starting with a set of papers (in our case the PMC-OA
comm corpus), we run the NER algorithm and obtain the
set S of all software strings extracted from the corpus. We
build a similarity matrix M containing similarity scores
between pairs of strings in S. Then we break M down into
distinct connected components, and run DBSCAN [32] on
each. From each connected component, we obtain clusters
of strings corresponding to well-deﬁned software entities.
The ﬁnal list of clusters is the union of all the clusters from
each of the connected components in M . To obtain M ,
we use a combination of Keywords-based synonym gen-
eration, SciCrunch synonyms retrieval and Jaro-Winkler
string similarity scores. We go over our methodology in
more detail below.

2.4.1. Keywords-based Synonym Generation

We query PyPi [33], CRAN [34] and Bioconductor [35]
indices, which contain lists of software packages available
on each of these platforms. For each software entry in
each index, we look for entries in the list of all plain-text
software mentions that contain the match and keywords
relevant to that index. For example, once we ﬁnd limma
as an entry in the Bioconductor package, we look for other
plain-text software mentions in our original list that con-
tain the word limma and keywords relevant to the Biocon-
ductor index, such as limma R package, R package limma,
etc. We consider pairs retrieved this way (e.g. limma and
limma R package and limma and R package limma) to be
high-conﬁdence synonyms.

2.4.2. SciCrunch Synonyms Retrieval

We query the SciCrunch API [36] for each of the men-
tions in our corpus. When queried for a particular en-

6

R microarray package limma
R package “limma”
R package named limma
R package limma: Linear Models for Microarray Data
R package limma –
R package limma package
R package limma (Linear Models
R package limma
R package like limma
R package ‘limma
R package for limma
R package, limma
R packages limma
R/Biconductor package limma
R/ Bioconductor package limma
R-package “limma”
R-package “limma
R-package ‘limma’
limma package removeBatchEﬀect
limma package for bioconductor
limma package for R
limma package R
limma package Bioconductor
limma package (Linear Models for Microarrays)
limma package (Linear Models for Microarray)
Bioconductor R Package limma
Bioconductor R limma package
Bioconductor R package limma
Linear Models for Microarray Data (limma) R
Linear Models for Microarray Analysis (limma) R
LIMMA package (limma)
GNU R limma
R “limma
R ‘limma’ package
R software package “limma”
R packages “limma
R “limma”
R package (limma
R package ’limma’
R package ”limma”
R Bioconductor package ‘limma
R Bioconductor package limma
R Bioconductor limma package
R Bioconductor limma
Package limma
R Bioconductor package “limma”
Model for Microarray data (limma) R

Figure 3: Examples of string variability for the software ‘limma’
(https://www.bioconductor.org/packages/limma), as extracted by
the NER model from the PMC-OA corpus

BLAST engine
BLAST output
BLAST package
BLAST)
BLAST Whole Genome
BLAST) search
BLASTING
BLASTs
BLaST
Blast
Blast -m
Blast Tool
Blast search
BLAST+
BLAST Website
BLAST Tools
BLAST Tool
(BLAST
(BLAST)
BLAST
BLAST (BLAST
BLAST (National
BLAST +
BLAST + tools
BLAST -P
BLAST Genome search
BLAST SEARCH
BLAST Search
BLAST Search tool
BLAST Searching
BLAST Sequence Similarity Search
BLAST TEXT
Blast searches
Blast tools
BLAST Searches
Blast+ package
blast sequence similarity search
blast similarity search

Figure 4: Examples of string variability for the software ‘BLAST’
as ex-
(https://scicrunch.org/browse/resources/SCR_008419),
tracted by the NER model from the PMC-OA corpus

Table 4: Keywords used for synonym generation

Index

PyPI
CRAN

Bioconductor

Keywords Used

’r’,

[’python, ’Python’, ’API’]
[’R’,
’Package’,
’package’,
package’, ’R-Package’, ’r-package’]
’package’,
[’R’,
’Package’,
’r’,
’R-Package’,
package’,
’bioconductor’, ’Bioconductor’]

’R-
’r-package’,

’R-

Table 5: Examples of software synonym pairs retrieved through the
SciCrunch API

Software Mention

Synonym

SPSS

SPSS
SPSS

BLASTN
BLASTN
BLASTN
BLASTN

Statistical Package for the Social
Sciences
IBM SPSS
IBM SPSS Statistics:
Interna-
tional Business Machines SPSS
Statistics
Standard Nucleotide BLAST
Nucleotide BLAST
NCBI BLASTN
BLASTn

Table 6: Examples of software synonym pairs retrieved through Key-
words synonym generation. The synonyms are strings extracted by
the NER algorithm

Software Mention

Synonym

scikit-learn
scikit-learn
scikit-learn
scikit-learn
scikit-learn
scikit-learn
scikit-learn

scikit-learn python package
scikit-learn python library
scikit-learn python
scikit-learn library for Python
scikit-learn Python package2223
scikit-learn Python package for
scikit-learn Python package

try, the SciCrunch API has the option of returning diﬀer-
ent variations under which that software is known. This
includes mappings between acronyms and full names (eg
SPSS and Statistical Package for the Social Sciences, but
also mappings between diﬀerent naming variations, (e. g.
BLASTN, NCBI BLASTN, Nucleotide BLAST, Standard
Nucleotide BLAST ). This allows us to connect diﬀerent
naming variations of the same software together at scale.

2.4.3. String Similarity Algorithms

We use the Jaro-Winkler string similarity algorithm [37]
to generate similarity scores between all pairs of software
mentions from the list of all plain-text software mentions
extracted. Examples are shown in Table 6. We use these
scores in the similarity matrix for pairs of software men-
tions that we didn’t retrieve through keywords-based syn-
onyms generation or through the SciCrunch API.

2.4.4. Clustering

After we generate all the synonym pairs, we build our
similarity matrix M . We consider synonyms retrieved
through keywords-based synonym generation process to be
high-conﬁdence synonyms to the original entries. Hence,
we add them to our similarity matrix with the conﬁdence
of 0.99. We are really conﬁdent in the synonym pairs re-
trieved from SciCrunch, so we give them the conﬁdence of
1. For all other pairs of strings, we use the Jaro-Winkler

7

string similarity [37] values in our matrix. We only use
pairs of synonyms with a conﬁdence ≥ 0.97. We make
this choice so that the largest connected component can
ﬁt into working memory and also to increase the conﬁ-
dence in the pairs of synonyms we consider. A drawback
of this is that we will lose a number of pairs of synonyms.
Future work could include ﬁnding ways to overcome this
limitation. Our similarity matrix is described by:

Mij =






1,
0.99,
mij,

(i, j) ∈ S2
(i, j) ∈ S1 − S2
otherwise, and mij ≥ 0.97

,

(1)

where S1 is the set of pairs based on keywords-based gen-
eration, S2 is the set of pairs obtained from SciCrunch,
and mij is Jaro-Winkler distance. We perform additional
post-processing of M in order to increase data quality for
clustering. Steps we include are: assigning conﬁdences of 1
for pairs of synonyms that are equal when stripped of dig-
its, punctuation or copyright characters, or that have more
than one word token and are equal, lowercase-insensitive.
We also remove pairs that include broad terms that tend
to have a lot of synonyms, such as ’R package’, ’r package’,
or ’interface’. Full details can be found in our code.

Once we have built our similarity matrix M and post-
processed it, we extract the connected components C1, C2,
. . . , Cn of M and then run the DBSCAN [32] algorithm
on each similarity matrix M (n) corresponding to the com-
ponent number n. Computing the connected components
helps us to reduce the size of the matrix we have to run
the clustering algorithm on. For each M (n) we compute a
distance matrix D(n) such that

D(n)

ij = 1 − M (n)

ij

.

(2)

Using DBSCAN we obtain a set of clusters correspond-
ing to each connected component in our graph. For each
cluster, we select the software mention with the highest
frequency in our corpus as the cluster name. Empirically,
we noticed that this string variation is the most likely to be
the real name of the software entity. This also corresponds
to the intuitive idea of the “true software mention”.
Our algorithm is presented as Algorithm 2.
We use the textdistance python package [38] to com-
pute the Jaro-Winkler similarity scores. We use a CSR
sparse matrix format for the similarity matrix and use the
connected components module from scipy.sparse.csgraph
[39] to obtain the connected components. We use the DB-
SCAN implementation from sklearn.cluster [40].

2.5. Software Mentions Linking

Once we cluster diﬀerent software string variations to-
gether, we also want to link them to the corresponding
URLs. For each software mention in our corpus, we do
an exact match search in the PyPI [33], CRAN [34] and
Bioconductor [35] indices, as well as GitHub [41] and Sci-
Crunch [42] APIs . For PyPI, CRAN, Bioconductor and

8

Input distance matrices D(n)
ij
Output all clusters ← {}

for all connected components Cn of M with distance
matrices D(n)
ij do
Cluster(n)
1 , Cluster(n)
for all clusters Cluster(n)

2 , . . . ← DBSCAN(D(n))

i

from component Cn do
) with the

Select vertex s(n)
i ∈ Vertices(Cluster(n)
i
highest frequency on PMC-OA comm
Name(Cluster(n)
) ← s(n)
all clusters ← all clusters ∪ Cluster(n)

i

i

i

end for

end for

Algorithm 2: Clustering and naming

SciCrunch, we also query individual URL pages for matches
we ﬁnd. Combined, we obtain links, as well as addi-
tional metadata, for a number of software mentions. Be-
cause the format and type of metadata available varies
across repositories, we normalize the formats to a com-
mon schema between repositories. The ﬁnal metadata
ﬁelds we retrieve across repositories and normalizing to
contain: source, package_url, description,homepage_
url, other_urls, license, github_repo, github_repo_
license, exact_match, RRID, reference. Note that not
all software mentions will have all of the metadata ﬁelds
present, either because there was no corresponding ﬁeld in
the database for that ﬁeld, or the entry was empty in the
database. We make the schema normalization mappings
between the initial ﬁelds present in a database and the
ﬁnal metadata ﬁelds available in Table 7 and Appendix
A.

We map each cluster to the link to which the cluster
name is pointing to. For instance, we map all the entries
in the scikit-learn cluster to the scikit-learn link. For situ-
ations where the cluster name does not have an associated
link, or a software mention is not mapped to a cluster, we
look for a an exact match link for that particular software
mention itself.

2.6. Dataset Curation

2.6.1. Motivation

The NER model has an F1 score of 0.922, with a preci-
sion of 0.9063 and a recall value of 0.9385. These metrics
are calculated on the SoftCite dataset [23], which has been
used for training the NER model. Since the PMC-OA cor-
pus distribution is diﬀerent from the one of the SoftCite
Dataset, we engaged our in-house team of bio-curators to
ﬁrst evaluate a sample of the top 1000 plain text soft-
ware mentions (by means of frequency) extracted from the
PMC-OA comm subset. Based on this evaluation, we con-
cluded that the Precision@1000 of the NER model “in the
wild”, on the PMC-OA corpus is 79.5%. After this assess-
ment, and in order to eliminate as much noise from the
dataset as possible, we engaged our bio-curator team to

Table 7: List of normalized metadata, together with deﬁnitions, for
ﬁelds obtained by linking through the PyPI, CRAN, Bioconductor
indices and SciCrunch and GitHub APIs.

Table 9: Expert evaluation of most frequent mentions extracted by
the NER model from the PMC-OA comm dataset

Field

database

package url

description

github repository

homepage url

other URLS

reference

scicrunch synonyms

resource type

RRID

Deﬁnition

the software in the

database the software is being
mapped to
URL of
database
description of the software in the
database
GitHub repository for the soft-
ware
homepage for the software (re-
trieved from the database)
list of other URLs retrieved for
the software from the database
journal article linked to the soft-
ware, identiﬁed by DOI, PMID
or RRID
synonyms for software retrieved
from SciCrunch
resource type according to Sci-
Crunch
RRID for the software retrieved
from SciCrunch

Table 8: Metrics of curated datasets. The table shows the coverage
of the 10,000 curated mentions in the PMC-OA and CZ Publishers’
Collection datasets. The curated mentions are the top 10k mentions
in terms of frequency on the PMC-OA comm dataset

Corpus

Mentions covered by the
10k set

Number

% of the set

PMC-OA comm
PMC-OA non comm
CZ publishers collection

9.03 × 106
2.67 × 106
20.11 × 106

61.158
58.923
41.766

curate another set of top 9000 mentions (by frequency),
thereby generating a dataset of 10,000 curated mentions
in total. Each of the 1000 mentions was annotated by a
single curator and checked by a second curator; the 9000
mentions were curated by one curator each. After exclud-
ing terms that our biocurators mark as non-software, we
are left with 6966 mentions that meet our deﬁnition of
software (for deﬁnitions see Appendix B), as shown in
Tables 8, 9, and 10. We curate all three corpora: comm,
non comm and the Publishers’ collection datasets to ex-
clude mentions marked by curators as non-software. We
also make the raw datasets available, together with the
curators’ labeling, and comments.

2.6.2. Curation Guidelines & Evaluation

Our software deﬁnitions, are broadly based on the def-
initions used for the SoftCite dataset [23], on which the

9

Category

Precision@1k, % Precision@10k, %

software
not-software
unclear

79.5
16.5
4

69.66
21.55
8.79

NER model has been trained. We start with ﬁve diﬀerent
categories: software, algorithm, database, web platform,
hardware and other and ask our curators to annotate the
top 1000 most frequent mentions from our corpus with
these categories. To facilitate the curation process, we
provide for each software mention ﬁve diﬀerent sentences
extracted from the articles; where the sentences do not
provide suﬃcient information about the software, the cu-
rators are asked to ﬁnd background information through
Google searches. The curation of the top 1000 mentions
allowed us to (a) understand the variety of mentions that
are present in the dataset (see Table 10) and (b) identify
additional examples for each category (in particular, bor-
derline cases) and reﬁne the curation guidelines further.

We consider mentions in the software and algorithm
categories to be true software, whereas mentions in the
database, hardware, web platform and other categories are
considered as non-software. Based on this human evalu-
ation, 79.5% of 1000 mentions are evaluated as software,
and 16.5% as non-software. For the remaining 4% of men-
tions, a distinction between software and non-software could
not be made; this is either because a software name or
acronym could be pointing to two diﬀerent entities (la-
beled as unclear ), or because the mentions extracted by
the NER model were random symbols. For the evaluation
of the 9000 mentions, we only ask our curators to distin-
guish between the main software/non-software categories.
We provide more detailed curation guidelines in Appendix
B. In the ﬁnal curated dataset of 10000 software mentions,
69.66% of mentions are evaluated as software, 21.55% as
non-software, and for 8.79% an evaluation could not be
made.

2.6.3. Inter-Annotator Agreement

We compute the Inter-Annotator Agreement (IAA) value

by looking at curator evaluations on a random sample of
100 mentions from the larger sample of 1000 (Table 10.
With four curators per mention, we obtain a Fleiss Kappa
IAA value [43] of 0.639 when only the two main cate-
gories (software/non-software) are considered, and 0.504
when the more speciﬁc ﬁve subcategories (software, algo-
rithm, database, web platform, hardware, other) are la-
beled. We also compute the Krippendorﬀ Alpha IAA [43]
values, which are similar: 0.686 for the two main categories
and 0.523 for ﬁve subcategories (Table 11). Since we are
interested in the binary (software/non-software) diﬀeren-
tiation, our IAA values of concern are around 0.639/0.686.
We want to note that the relatively low IAA value demon-

Table 10: Expert evaluation of 1000 most frequent mentions ex-
tracted by the NER model

Main Category

Subcategory

Counts

%

Software
Algorithm
Database
Hardware

Software
Software
Non-Software
Non-Software
Non-Software Web platform
Non-Software
Errors

Other
Unclear

699
95
40
9
13
105
39

69.9
9.5
4.0
9
1.3
10.5
3.9

Table 11: IAA curation values for the expert evaluation of the most
frequent mentions extraced by the NER model from the PMC-OA
comm dataset

Parameter

Five categories Two categories

Fleiss κ
Krippendorﬀ α

0.504
0.523

0.639
0.686

strates that this is a challenging task even for our expert
curator team. The fact that the IAA value is relatively
low should be interpreted as a signal for how hard it is
to distinguish in some cases whether a mention is a true
software or not. Speciﬁcally, it is diﬃcult to diﬀerentiate
between ‘software’ and ‘algorithm’ as these terms are of-
ten used interchangeable by authors describing their tools:
the tool may be called “software” on the tool’s website but
described as an “algorithm” in an article. Similarly, some
online tools consist of a database and software of the same
name, so there may be disagreement how curators anno-
tate the software mention. It is also important to note that
the curation relied on both the context given by 5 example
sentences and background searches. The use of diﬀerent
sources of information can result in diﬀerent annotations;
this is particularly true for software mentions that consist
of acronyms, have ambiguous names or share part of their
name with other software and/or non-software resources.

3. Evaluation & Results

3.1. Final Dataset

The statistics of software mentions extracted from the
OA-PMC comm is shown in Table 1. The results of disam-
biguation and linking are shown in Table 12. As seen from
this table, for about 390,000 mentions we could not gener-
ate synonyms or we discarded them during post-processing.
By analyzing some of these mentions, we hypothesize that
they are likely to be noise or false positives. For about
720,000 mentions we were able to generate synonyms. Again,
about 400,000 of the latter were too ambiguous to dis-
ambiguate. This is likely because the software mentions
did not have signiﬁcant synonyms (with a conﬁdence of
≥ 0.97 during the disambiguation phase). The remaining

10

320,000 mentions were mapped into 97,600 unique soft-
ware entities through the disambiguation algorithm. This
covers 78% of all links in the dataset. Lastly, for about
185,000 mentions we were able to obtain links to GitHub,
PyPI, CRAN, Bioconductor, or SciCrunch. This covers
about 55% of all software-paper links.

3.2. Disambiguation

Perhaps the biggest challenge in disambiguation is that
we don’t have curated labels to learn from. Our methodol-
ogy, based on DBSCAN [32], is unsupervised. We engaged
our biocurators for evaluation of the results.

We created a set of 5884 pairs of generated synonyms
coming from 104 unique software mentions for manual cu-
ration. This dataset was generated from an initial iteration
of the disambiguation algorithm. We asked our curation
team to label each synonym pair with one of the following
categories:

Exact: synonym is an exact match of the assigned Soft-

ware mention

Narrow: synonym is a child term

Not Synonym: synonym is an unrelated software or other

term

The Exact category includes all mentions that can un-
ambiguously be assigned to the software mention, includ-
ing partial terms, typos, variations on spelling (upper/lower
case, space, hyphen etc), and acronyms. Reference num-
bers in the article are often erroneously added to the soft-
ware synonym; these are also labeled as Exact. Versions
of software are annotated as Narrow. Partial versions
are also included here, for example “Autodock Vina1” for
“Autodock Vina1.1.2”. It is worth noting that synonyms
representing software versions can be challenging to distin-
guish from synonyms which include the reference number
(e.g. “Autodock Vina19” where 19 is the reference num-
ber in the article mentioning the software). Synonyms
of software tools that have common names pose another
challenge for this task: for example, it is time-consuming
to work out whether “ClusterM” is a true synonym for a
version of the software tool “Cluster” or represents a com-
pletely distinct software called ClusterM. We considered
pairs that fall into either of Exact or Narrow categories to
be true synonyms. Composition of this curated labels is
under (Table 13).

We used this dataset and the labels assigned by our cu-
rators to validate our clustering technique, by computing
the Precision, Recall and the F1 score for generated pairs
of synonyms. We chose the hyperparamaters that gave the
highest metrics on this curated dataset. Our best model
had an F1 score of 0.704, with a Precision of 0.954 and
Recall of 0.558.

We oﬀer some examples of disambiguated terms ob-
tained through DBSCAN clustering for the scikit-learn,
ImageJ and SPSS software entities in Figure 5, Figure 6
and Figure 7.

Table 12: OA PMC comm disambiguation and linking

Category

Mentions

Paper-software links Notes

Approx. #

%

Approx. #

%

undisambiguated
undisambiguated
disambiguated

393,057
404,493
323,561

35
36.11
28.88

731,529
1,050,017
6,384,430

disambiguated and linked

185,427

16.55

4,555,476

8.95
12.85
78.18

55.78

total unique mentions

1,120,111

100

8,165,976

100

no signiﬁcant synonyms
no output from clustering
97,600 unique software entities

Table 13: Disambiguation Evaluation. Description of the dataset
used for evaluating disambiguation. Labels were assigned by our
curation team on 5886 pairs of generated synonyms coming from
103 unique software mentions

Synonym Pair Label Count

%

Correct - Exact
Correct - Narrow
Incorrect
Unclear
Not software

3147
1094
668
45
930

53.465
18.586
13.484
0.908
15.805

3.3. Linking

As with disambiguation, we don’t have any labeled
data to evaluate our linking, so we engage our team of
biocurators for feedback. We provide our biocurators with
50 software mentions, together with the generated links.
Based on biocurator feedback, 54% the generated links
are correct, 6% are incorrect, and for 40% it is unclear
whether the link is correct. Most notably, 39/40 mentions
for which the link is unclear are retrieved from GitHub and
only one is linked through PyPI, Bioconductor, CRAN and
SciCrunch. This happens because GitHub is a resource
that is not curated, so having an exact match on a soft-
ware name in GitHub does not guarantee that it will be
linking to the actual software. Anyone can upload soft-
ware in GitHub and name it as they wish. A number
of repositories we evaluated were empty, or contained too
little information to be able to decide for sure if the link-
ing was correct. When we consider the evaluation only of
links retrieved through PyPI, Bioconductor, CRAN and
SciCrunch, the accuracy improves considerably: 93.33% of
links are correct and 0.6% are incorrect, 0% unclear. This
suggests that linking software through these four reposito-
ries through an exact match is likely to give correct links.
We note, however, that the sample size for this evaluation
is quite small, and it is possible that results might change
with a larger sample.

4. Code & Data Availability

We make the dataset, as well as all intermediate ﬁles
available at https://doi.org/10.5061/dryad.6wwpzgn2c

11

Table 14: Linking Coverage

Repository

Number of linked mentions

%

GitHub API
SciCrunch API
CRAN
PyPI
Bioconductor

155,506
43,817
20,202
14,154
7801

64.39
18.14
8.36
5.86
3.23

Table 15: Linking Evaluation on a sample dataset of 50 generated
links to PyPI, CRAN, Bioconductor, GitHub and SciCrunch

Link label

All links

Excluding GitHub

Number % Number

%

correct
unclear
incorrect

27
20
3

54
40
6

14
1
0

93.33
6.66
0

under a CC0 license [44]. All the code used for extrac-
tion, disambiguation and linking, as well as instructions
on how to reproduce the results and some starter code is
available at a GitHub repository https://github.com/
chanzuckerberg/software-mentions under the MIT li-
cense with the permanent snapshot at [45].

5. Discussion

The dataset we make available is, to our knowledge,
the largest dataset of software mentions in the scientiﬁc
literature currently available. In this section, we go over
some of the lessons learned in building a dataset this large,
as well as limitations of current work. We oﬀer a view on
opportunities for further work in the Next Steps section.

5.1. Extraction

An important feature of our extraction is the maximal
preservation of the context of the software mention. We
keep the sentence from which the mention was extracted as
well as the reference to the unit (section, caption, abstract)
from which it was extracted. This helped in the curation.
This may facilitate more deep reprocessing of the dataset
the future, including better disambiguation and linking.

Cluster: ‘scikit-learn‘

Scikit-Learn
scikit-learn Python
scikit-learn
Scikit learn
sklearn
scikit-learn Python library
Scikit-learn
Sklearn
Python scikit-learn
scikit learn
scikit-learn
Scikit-learn
Scikit-Learn Python
scikit-learn81
SciKit-Learn
sklearn Python
SkLearn
scikit -learn
Python Scikit-Learn
Scikit-Learn®
Python sklearn library
scikit.learn
Scikits-learn
Sci-kit-learn
scikit-learn Python3 package
Sci-Kit learn
sklearn0
Python Scikit Learn
Scikit-Learn Python Library
Python scikit learn
2scikit-learn
SCikit-learn
SCIKIT-learn
Scikit–Learn
Python Sci-kit learn
Python package Scikit-learn
scikit-Learn
Scikit-Learn Python package
scikit-learn Python Library
sklearn python package
sci-kit learn Python package
Python scikit learn package
skLearn
Scikit-learn python
scikitslearn
scikit-learn Python package for
Python sci-kit learn
Scikit–Learn Python
scikit - learn Python library
....

Figure 5: Sample of Disambiguation results for the software entity
scikit-learn. There are a total of 124 unique string variations ex-
tracted by the NER model that are mapped to the scikit-learn
software entity through disambiguation.

12

Cluster: ‘ImageJ‘

ImageJ
Image J
image J
Image-J
Image J1
Image J2
Image J©
Image/J®
image/J®
image-j
IMAGE-J
ImageJ (Image Processing and Analysis in Java
Image-j
Image J)
IMAGE j/
IMageJ
imageJ1
Image -J
ImageJ®1
ImageJ, Image Processing and Analysis in Java
ImageJ (Image Processing and Analysis in Java)
ImageJ Image Processing and Analysis in Java
ImageJ-145
image J1
IMageJ1
imageJ64
ImageJ -
ImageJ)
IMAGEJ®
Image/J
Image J-Image Processing and Analysis in Java
Image J- Image Processing and Analysis in Java’
....

Figure 6: Sample of Disambiguation results for the software entity
ImageJ. There are a total of 178 unique string variations extracted
by the NER model that are mapped to the ImageJ software entity
through disambiguation.

Table 16: Metadata Fields Statistics for the mentions linked to a
repository. Synonyms retrieved through diasmbiguation that might
link to the same entry are not counted

Normalized metadata ﬁeld

Count

%

package url
github repo
description
homepage url
github repo license
reference
RRID
other urls
license

149,015
143,834
116,071
36,306
39,464
22,134
18,766
18,766
13,485

100
96.52
77.89
24.36
26.48
14.85
12.59
12.59
9.04

Cluster: ‘SPSS‘

Statistical Package for Social Sciences (IBM – SPSS)
Statistical Package for the Social Sciences (SPSS) Package
(Statistical Package for Social Science)
(Statistical Package for the Social Sciences
SPSS®® Statistics
Statistical Package for the Social Sciences [(SPSS
SPSS
Statistical Package for the Social Sciences (SPSS -19
Statistical Package for Social science for Windows (SPSS)
SPSS -24
SPSS27
Statistical Package for Social Sciences (SPSSR-25
Statistical Package for Social Sciences (IM SPSS
SPSS® Statistics®
Statistical Package for Social Sciences-24
Statistics Package for Social Sciences
Statistical Package for the Social Science
SPSS* Statistics
Statistical Package for Social Sciences [SPSSTM]
SPSS]
Statistiscal Package for the Social Sciences
PSS Statistics
Statistical Package for the Social Sciences for Mac
Statistical Package for Social Sciences (SPSS-2
Statistical Package for the Social Sciences (SPSS-23
Statistical Package for the Social Sciences Software (SPSS
SPSS15
Statistical Package for Social Sciences (SPSS-21
spss20
Statistical Package for the Social Science program (SPSS
statistical package for the social sciences (SSPS)
Statistical Package for the Social Sciences Software
Statistical Pack for the Social Science (SPSS)
Statistical Package for Social Science (SPSS) (SPSS
Statistical Package for the Social Sciences for Mac (SPSS-
22
statistical package for Social Science (SPSS PC
Statistical Package for the Social Sciences (SPSS) (PASW
SPSS)
SSPS Statistics
IBM statistical package for the social sciences (SPSS)
Statistical Package for the Social Sciences, SPSS)
SPSS tatistics
....

Figure 7: Sample of Disambiguation results for the software entity
SPSS. There are a total of 1361 unique string variations mapped to
the SPSS cluster.

This feature became available because we had the access
to the structure of the papers, using their XML represen-
tation. Many other studies, for example, [24], process the
text extracted from PDF, which does not show the struc-
ture. The current tendency of the publishers to keep the
archival copies of the papers in the XML format is prob-
ably one of the best things that happened to the ﬁeld of
data mining from scholarly documents.

5.2. Software Name Variability in the Literature

The goal of building a comprehensive dataset of soft-
ware mentions in the literature is to be able to assess soft-
ware impact through informal citations. One of the main
challenges is the variability under which a software can be
mentioned, or ’informally cited’ in a paper. Some of this
variability is to be expected, such as diﬀerences between
using the fullname or the acronym of a software (eg SPSS
and Statistical Package for Social Sciences), authors using
various software name variations (eg SPSS Statistics, IBM
SPSS ) or typos.

But probably the biggest string variability comes from
how the NER model extracts software from text. For in-
stance, the NER model might extract the following men-
tions: R package limma, limma R package, Limma, all
pointing out to the same software.

Lastly, there is variability coming from typos that are
occuring either due to the authors or because of XML pars-
ing. If we want to truly get the impact of a particular piece
of software, we need to be able to map all of these software
naming variations to the same software entity.

5.3. Disambiguation

In order to map software variations to the same soft-
ware entity, we built a software disambiguation model. We
describe the model, based largely on string similarity algo-
rithms and clustering techniques, under Section 2.4. Note
that through this model, we are only able to disambiguate
28.88% mentions that constitute 78.18% of paper-software
links. The other mentions either have no signiﬁcant syn-
onyms or are too ambiguous to disambiguate (Table 12).
We propose this as a baseline model. Because the raw
data is available, more sophisticated disambiguation al-
gorithms can be employed. We want to note that the
main challenge in disambiguating mentions in a dataset
this large is the lack of labels, which means that the dis-
ambiguation tasks would have to be unsupervised. This
poses challenges in terms of evaluation. We evaluated our
disambiguation method using our biocuration team, which
evaluated 5884 synonym pairs that we subsequently used
to validate our model. The other challenge in using un-
supervised methods, such as clustering, on a dataset this
large, is the generation of distance matrices that would ﬁt
into working memory.

We also want to note that we experimented with using
transformer-based models to generate embeddings for soft-
ware mentions and compute similarity scores between pairs

13

of strings by using the dot product or cosine similarity. We
obtained unsatisfactory results creating similarity scores in
this way. Pairs of strings that are semantically diﬀerent
than each other ended up having similar embeddings. This
makes sense, as transformer-based models compute repre-
sentational encodings for sequences based on context. It
is possible that packages that are similar in some way, like
sklearn and scipy, appear in similar contexts, and hence,
will have similar embeddings. We hypothesize that the
level of context around how a particular software is men-
tioned in a research paper is not granular enough to allow
the computation of an embedding speciﬁc enough to diﬀer-
entiate that software from other similar, but diﬀerent soft-
wares. Basically, the language (and hence context) which
authors use to mention sklearn is likely to be very similar
to the language authors use to mention scipy. It makes
sense that their embeddings will be close in n-dimensional
space. We hypothesize that being able to use the context
around a mention would, however, be useful for ambigu-
ous cases when two diﬀerent software can have the same
name, or acronyms. This would be an interesting area of
further work.

5.4. Use cases

As a philanthropic organization supporting and build-
ing technology to serve the computational needs of biomed-
ical scientists, we rely on data on scientiﬁc software to in-
form our programmatic activities [46, 47] and technology
strategy [48]. In this section we discuss a couple of ways
the dataset has already been used in our work at the Chan
Zuckerberg Initiative prior to the public release.

5.4.1. Trends in imaging

The Chan Zuckerberg Initiative considers imaging to
be one of the key technologies in our eﬀort to develop sci-
ence and technologies to measure human biology in action
[49, 50]. To fund this area more eﬃciently and support
the relevant computational tools, we needed to understand
the methods used in the ﬁeld and the penetration of open
source ones, in particular.

Modern biomedical imaging is signiﬁcantly computa-
tional in nature. Thus we were able to use our data to iden-
tify the papers using various imaging software. We used
publication data to get the time trends and the change
of relative market share for both open and closed source
imaging software.

5.4.2. Penetration of single-cell methods in biomedicine

Single-cell methods are another key area of biomedical
technology supported by CZI [50]. To better focus our
eﬀorts, we needed data on the penetration of single-cell
methods in the clinical research literature. The software
used for analysis of single cell data is speciﬁc, so mention of
this software in a paper can be a good marker of the usage
of single cell methods. We were able to get a subset of
papers that used single cell methods, and to estimate the

penetration of these methods in the diﬀerent biomedical
subﬁelds.

5.4.3. Other usage

We would like to note that the use cases mentioned
in this section were also recognized by other people, for
example, Wikidata Scholia project [51]. There are many
other interesting questions, like the identifying the most
inﬂuential authors, co-uses etc. [51]. We hope our dataset
can be used by projects like Scholia and intend to integrate
our results with Wikidata in the future releases.

6. Next Steps

We believe that there are a number of exciting ques-
tions this dataset can help answer. To start with, we can
look into what the most used software is in a particular
ﬁeld (e.g. neurodegeneration, single-cell biology, imaging).
We can go a step further and try to understand what the
software is used for. Potential ideas include getting in-
sights from paper or sentence topics, MESH terms, author-
provided paper keywords, or information found in the sen-
tence in which the software is being mentioned (note that
we make this available). We can also start looking at dif-
ferences between how software usage diﬀers between par-
ticular ﬁelds, if any. Furthermore, we can explore mea-
sures of impact for software, whether those are number
of informal citations (such as software mentions), or more
sophisticated models. For instance, since we are now able
to connect papers and software entities in an underlying
graph, we can start exploring with graph-based models for
impact. One particularly exciting idea would be to extend
the notion of Eigenfactor, which has been proposed mea-
sure impact for journals and authors, to software. Other
potential areas of further research include looking into
open-access policies of most used or impactful software
or exploring diﬀerences in how software usage varies over
time. Last but not least, we can look into the impact of
particular pieces of software and assess their impact.

The approach we took in this paper was to use an NER
model to collect software string variations from a corpus
of papers, and then disambiguate and link these mentions
into clusters of software entities using DBSCAN [32] and
a similarity matrix we built. Using this methodology, we
were able to disambiguate about a third of the total num-
ber of unique mentions in our dataset, which covers about
78% of the total paper-software links. This means that we
are losing information from the rest of the total number of
mentions in our dataset. Future work can include improv-
ing the disambiguation algorithms to be able to cluster a
larger percentage of our dataset under particular software
entities.

For linking, we used an exact match search on a soft-
ware mention in some of the most used software reposito-
ries, such as PyPI, CRAN, Bioconductor, SciCrunch and
GitHub. We want to acknowledge that there are limita-
tions in this approach. For instance, there can be diﬀerent

14

software having the same acronyms, or diﬀerent entities,
whether software, databases, hardware or platforms with
the same name. Moreover, in repositories that are not cu-
rated, such as GitHub, anyone can post a piece of software
under whatever name they want. Being able to distinguish
which is the true link for a piece of context would most
likely require being able to look at the context in the text
around a piece of software is being mentioned. Since we
make the sentences in which a software mention appears
available, we believe linking algorithms based on context
are worth exploring. Improving the quality of linking soft-
ware mentions to corresponding URLs in repositories or
databases is an interesting area for further research.

7. Conclusion

We created one of the largest dataset of software men-
tions in the literature. For a subset of the data we used
string similarity algorithms and unsupervised clustering
techniques to disambiguate the software mentions into dis-
tinct software entities. We used a linking algorithm to
connect the mentions to URLs in the PyPI, CRAN, Bio-
conductor indices and the SciCrunch and GitHub APIs.
We make the dataset available to the community and be-
lieve there are a number of exciting next steps. It is our
hope that this new resource helps foster new insights about
software usage and impact in the literature.

8. Acknowledgements

We thank James L Howison (University of Texas), Daniel

Mietchen (Ronin Institute) and other reviewers for help-
ful comments on earlier versions of this manuscript. We
also thank our team of biocurators, Alison Jee, Celina Liu,
Parasvi Patel, and Ronald Wu for their hard work curating
the ﬁnal dataset.

References

[1] John Jumper, Richard Evans, Alexander Pritzel, Tim Green,
Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvu-
nakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, Alex
Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Bal-
lard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav
Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen,
David Reiman, Ellen Clancy, Michal Zielinski, Martin Steineg-
ger, Michalina Pacholska, Tamas Berghammer, Sebastian Bo-
denstein, David Silver, Oriol Vinyals, Andrew W. Senior, Ko-
ray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly
accurate protein structure prediction with AlphaFold. Na-
ture, 596(7873):583–589, Aug 2021.
doi:
10.1038/s41586-021-03819-2.

ISSN 1476-4687.

[2] Event Horizon Telescope Collaboration. First Sagittarius A*
Event Horizon Telescope results. I. The shadow of the super-
massive black hole in the center of the Milky Way. Astrophys.
J. Lett., 930(2):L12, 2022. doi: 10.3847/2041-8213/ac6674.
[3] J. Priem, D. Taraborelli, P. Groth, and C. Neylon. Altmetrics:

A manifesto, 2010. URL http://altmetrics.org/manifesto.

[4] Terry Bucknell. Recognising inﬂuence: helping authors of non-
traditional research outputs evidence the reach and potential
impacts of their work. Septentrio Conference Series, 1:Posters
section, 2016. doi: 10.7557/5.3979.

15

[5] Juan Pablo Alperin, Lesley A. Schimanski, Michelle La, Mered-
ith T. Niles, and Erin C. McKiernan. The value of data and
other non-traditional scholarly outputs in academic review, pro-
motion, and tenure in Canada and the United States. In An-
drea L. Berez-Kroeker, Bradley McDonnell, Eve Koller, and
Lauren B. Collister, editors, The Open Handbook of Linguistic
Data Management. The MIT Press, 2022. ISBN 9780262362177.
[6] James Howison and James D. Herbsleb. Scientiﬁc software
production:
In Proceedings of
the ACM 2011 Conference on Computer Supported Cooperative
Work, CSCW ’11, page 513–522, New York, NY, USA, 2011.
Association for Computing Machinery.
ISBN 9781450305563.
doi:
10.1145/1958824.1958904. URL https://doi.org/10.
1145/1958824.1958904.

Incentives and collaboration.

[7] James Howison, Ewa Deelman, Michael J. McLennan, Rafael
Ferreira da Silva, and James D. Herbsleb. Understanding the
scientiﬁc software ecosystem and its impact: Current and future
measures. Research Evaluation, 24(4):454–470, 07 2015. ISSN
0958-2029. doi: 10.1093/reseval/rvv014. URL https://doi.
org/10.1093/reseval/rvv014.

[8] Dalmeet Singh Chawla. The unsung heroes of scientiﬁc software.
ISSN 1476-4687. doi:

Nature, 529(7584):115–116, Jan 2016.
10.1038/529115a.

[9] James Howison and Julia Bullard.

Software in the scien-
tiﬁc literature: Problems with seeing, ﬁnding, and using soft-
ware mentioned in the biology literature. JASIST, 67(9):2137–
2155, 2016. doi: 10.1002/asi.23538. URL https://asistdl.
onlinelibrary.wiley.com/doi/abs/10.1002/asi.23538.
[10] Rebecca Knowles, Bilal A. Mateen, and Yo Yehudi. We need
to talk about the lack of investment in digital research infras-
tructure. Nature Computational Science, 1(3):169–171, Mar
2021. ISSN 2662-8457. doi: 10.1038/s43588-021-00048-5. URL
https://doi.org/10.1038/s43588-021-00048-5.

[11] Jill P. Mesirov. Accessible reproducible research. Science, 327
(5964):415–416, 2010. ISSN 0036-8075. doi: 10.1126/science.
1179653. URL http://science.sciencemag.org/content/327/
5964/415.

[12] Lucas N. Joppa, Greg McInerny, Richard Harper, Lara Salido,
Kenji Takeda, Kenton O’Hara, David Gavaghan, and Stephen
Emmott. Troubling trends in scientiﬁc software use. Science,
340(6134):814–815, 2013. doi: 10.1126/science.1231535.

[13] Anna Nowogrodzki. How to support open-source software and
stay sane. Nature, 571:133–134, 2019. doi: 10.1038/d41586-
019-02046-0.

[14] Carly Strasser, Kate Hertweck, Josh Greenberg, Dario Tara-
borelli, and Elizabeth Vu. 10 simple rules for funding scientiﬁc
open source software, June 2022. URL https://doi.org/10.
5281/zenodo.6611500.

[15] Daniel S Katz, Sou-Cheng T Choi, Hilmar Lapp, Ketan Ma-
heshwari, Frank L¨oﬄer, Matthew Turk, Marcus D Hanwell,
Nancy Wilkins-Diehr, James Hetherington, James Howison,
Shel Swenson, Gabrielle D Allen, Anne C Elster, Bruce Ber-
riman, and Colin Venters. Summary of the First Workshop
on Sustainable Software for Science: Practice and Experi-
ences (WSSSPE1). J Open Res Software, 2(1):e6, 2014. doi:
10.5334/jors.an.

[16] Arfon M. Smith, Daniel S. Katz, Kyle E. Niemeyer, and
FORCE11 Software Citation Working Group. Software citation
principles. PeerJ Comp Sci, 2:e86, 2016. doi: 10.7717/peerj-
cs.86.

[17] DS Katz, NP Chue Hong, T Clark, A Muench, S Stall,
D Bouquin, M Cannon, S Edmunds, T Faez, P Feeney, M Fen-
ner, M Friedman, G Grenier, M Harrison, J Heber, A Leary,
C MacCallum, H Murray, E Pastrana, K Perry, D Schuster,
M Stockhause, and J Yeston. Recognizing the value of software:
a software citation guide [version 2; peer review: 2 approved].
F1000Research, 9(1257), 2021. doi: 10.12688/f1000research.
26932.2.

[18] Frank Kr¨uger and David Schindler. A literature review on meth-
ods for the extraction of usage statements of software and data.
Comput Sci Eng, 22(1):26–38, Jan 2020. ISSN 1558-366X. doi:

10.1109/MCSE.2019.2943847.

[19] Patrice Lopez, Caifan Du, Johanna Cohoon, Karthik Ram,
and James Howison. Mining software entities in scientiﬁc
literature: Document-level NER for an extremely imbalance
and large-scale task.
In Proceedings of the 30th ACM Inter-
national Conference on Information & Knowledge Manage-
ment, page 3986–3995, New York, NY, USA, 2021. Associa-
tion for Computing Machinery.
ISBN 9781450384469. URL
https://doi.org/10.1145/3459637.3481936.

[20] Enrique Ordu˜na-Malea and Rodrigo Costas. Link-based ap-
proach to study scientiﬁc software usage: the case of VOSviewer.
Scientometrics, 126(9):8153–8186, Sep 2021. ISSN 1588-2861.
doi: 10.1007/s11192-021-04082-y. URL https://doi.org/10.
1007/s11192-021-04082-y.

[21] David Schindler, Benjamin Zapilko, and Frank Kr¨uger. Investi-
gating software usage in the social sciences: A knowledge graph
approach.
In Andreas Harth, Sabrina Kirrane, Axel-Cyrille
Ngonga Ngomo, Heiko Paulheim, Anisa Rula, Anna Lisa Gen-
tile, Peter Haase, and Michael Cochez, editors, The Semantic
Web, pages 271–286, Cham, 2020. Springer International Pub-
lishing. ISBN 978-3-030-49461-2.

[22] David Schindler, Felix Bensmann, Stefan Dietze, and Frank
Kr¨uger. SoMeSci—a 5 star open data gold standard knowl-
edge graph of software mentions in scientiﬁc articles. In Pro-
ceedings of the 30th ACM International Conference on In-
formation & Knowledge Management, page 4574–4583, New
York, NY, USA, 2021. Association for Computing Machin-
ery.
ISBN 9781450384469. URL https://doi.org/10.1145/
3459637.3482017.

[23] Caifan Du, Johanna Cohoon, Patrice Lopez, and James How-
ison.
SoftCite dataset: A dataset of software mentions in
biomedical and economic research publications. JASIST, 72(7):
870–884, 2021. doi: 10.1002/asi.24454. URL https://asistdl.
onlinelibrary.wiley.com/doi/abs/10.1002/asi.24454.
[24] Patrice Lopez, Caifan Du, Hannah Cohoon, and James How-
ison. Softcite software mention extraction from the CORD-
19 publications, June 2021. URL https://doi.org/10.5281/
zenodo.4961241.

[25] Alex D. Wade and Ivana Williams. CORD-19 software men-
tions, 2021. URL https://doi.org/10.5061/dryad.vmcvdncs0.
[26] David Schindler, Felix Bensmann, Stefan Dietze, and Frank
Kr¨uger. The role of software in science: a knowledge graph-
based analysis of software mentions in PubMed Central. PeerJ
Computer Science, 8:e835, 2022. doi: 10.7717/peerj-cs.835.
URL https://peerj.com/articles/cs-835/.

[27] Iz Beltagy, Kyle Lo, and Arman Cohan.

trained language model
arXiv:1903.10676, 2019.

for scientiﬁc text.

Scibert: A pre-
arXiv preprint

[28] PMC Help. National Center for Biotechnology Information
(US), Bethesda, MD, 2005. URL https://www.ncbi.nlm.nih.
gov/books/NBK3825/.

[29] Boris Veytsman. From the president. TUGboat, 42(1):3, 2021.

doi: 10.47397/tb/42-1/tb130pres.

[30] Stefan Behnel, Martijn Faassen, and Ian Bicking.

lxml: XML

and HTML with Python. https://lxml.de/, 2005.

[31] Titipat Achakulvisut, Daniel Acuna, and Konrad Kording.
Pubmed Parser: A Python parser for PubMed Open-Access
XML subset and MEDLINE XML dataset. J Open Source
Softw, 5(46):1979, 2020. doi: 10.21105/joss.01979. URL https:
//doi.org/10.21105/joss.01979.

[32] Martin Ester, Hans-Peter Kriegel, J¨org Sander, and Xiaowei
Xu. A density-based algorithm for discovering clusters in large
spatial databases with noise. In Proceedings of the Second Inter-
national Conference on Knowledge Discovery and Data Mining,
KDD’96, page 226–231. AAAI Press, 1996.
[33] Python package index. https://pypi.org/, 2022.
[34] The Comprehensive R Archive Network.

https://cran.r-

project.org/, 2022.

[35] Robert C. Gentleman, Vincent J. Carey, Douglas M. Bates, Ben
Bolstad, Marcel Dettling, Sandrine Dudoit, Byron Ellis, Lau-
rent Gautier, Yongchao Ge, Jeﬀ Gentry, Kurt Hornik, Torsten

16

Hothorn, Wolfgang Huber, Stefano Iacus, Rafael Irizarry,
Friedrich Leisch, Cheng Li, Martin Maechler, Anthony J.
Rossini, Gunther Sawitzki, Colin Smith, Gordon Smyth, Luke
Tierney, Jean YH Yang, and Jianhua Zhang. Bioconductor:
open software development for computational biology and bioin-
formatics. Genome Biology, 5(10):R80, Sep 2004. ISSN 1474-
760X. doi: 10.1186/gb-2004-5-10-r80.

[36] SciCrunch API, 2022. URL https://scicrunch.org/browse/
api-docs/index.html?url=https://scicrunch.org/swagger-
docs/swagger.json.

[37] William E. Winkler and Yves Thibaudeau. An application of
the Fellegi-Sunter model of record linkage to the 1990 U.S. de-
cennial census. Statistical Research Report Series RR91/09,
U.S. Bureau of the Census, Washington, D.C., 1991.

[38] TextDistance Python Package, 2021. URL https://pypi.org/

project/textdistance.

[39] SciPy User Guide, 2022. URL https://docs.scipy.org/doc/

scipy/tutorial/.

[40] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Ma-
chine learning in Python. Journal of Machine Learning Re-
search, 12:2825–2830, 2011.

[41] GitHub. https://github.com/, 2022.
[42] FDI Lab. SciCrunch infrastructure. https://scicrunch.org/,

2022.

[43] Kilem Li Gwet. Handbook of inter-rater reliability. The Deﬁni-
tive Guide to Measuring the Extent of Agreement Among
Raters. Advanced Analytics, Gaithersburg, MD, fourth edition,
2014.

[44] Ana-Maria Istrate, Donghui Li, Dario Taraborelli, Michaela
Torkar, Boris Veytsman, and Ivana Williams. A large dataset
of software mentions in the biomedical
literature (the data
ﬁles), September 2022. URL https://doi.org/10.5061/dryad.
6wwpzgn2c.

[45] Ana-Maria Istrate, Donghui Li, Dario Taraborelli, Michaela
Torkar, Boris Veytsman, and Ivana Williams. A large dataset
of software mentions in the biomedical literature (the code),
September 2022.
URL https://doi.org/10.5281/zenodo.
7041594.

[46] Nicholas Sofroniew and Dario Taraborelli.
foundations of biomedicine,

ible
//cziscience.medium.com/the-invisible-foundations-
of-biomedicine-4ab7f8d4f5dd.

The invis-
URL https:

2019.

[47] Chan Zuckerberg Initiatiove Science Programs, 2022. URL
https://chanzuckerberg.com/science/programs-resources/.
[48] Accelerating research through open science and biomedical tech-
nology, 2022. URL https://chanzuckerberg.com/science/
scitech/.

[49] CZI awards over $5m to advance technologies and ex-
pand global access to bioimaging, November 2021. URL
https://chanzuckerberg.com/newsroom/czi-awards-over-
5m-to-advance-technologies-and-expand-global-access-
to-bioimaging/.

[50] Chan Zuckerberg Initiative announces 10-year eﬀort to de-
velop science and technologies to measure human biology in
action, December 2021. URL https://chanzuckerberg.com/
newsroom/chan-zuckerberg-initiative-announces-10-year-
effort-to-develop-science-and-technologies-to-measure-
human-biology-in-action/.

[51] Finn ˚Arup Nielsen, Daniel Mietchen, and Egon Willighagen.
Scholia and scientometrics with wikidata.
In Scientometrics
2017, pages 237–259, November 2017. doi: 10.1007/978-3-319-
70407-4 36. URL https://arxiv.org/pdf/1703.04222.

[52] Guido Van Rossum. The Python Library Reference, release

3.8.2. Python Software Foundation, 2020.

Appendix A. Schema Normalization

In this section, we oﬀer more details about the process
of normalizing schemas across the PyPI, CRAN and the
Bioconductor indices, as well as the SciCrunch and GitHub
APIs. By querying each of these repositories, we retrieve a
number of metadata ﬁelds, which we map to a normalized
set of ﬁelds (Table A.17).

Appendix B. Curation Guidelines

Background

A subset of 10,000 software mentions was curated by
our domain experts, so as to identify the mentions that
meet our deﬁnition of “Software” and exclude false posi-
tives. The curation process included two stages:

Stage 1: Curation of 1000 software mentions using ﬁve
subcategories: Software, Algorithm, Database, Web
Platform, Hardware (and “Other”).

During this ﬁrst stage of labeling, we learned that
it is hard to distinguish between Software and Algo-
rithms as these terms are often used interchangeably
by researchers. For the next labeling stage we con-
sidered them both as “Software”, whereas Database,
Web Platforms and Hardware are considered as “Non-
Software”.

Stage 2: Curation of 9000 software mentions using two
main categories: Software and Not-Software.

This document outlines the deﬁnitions used for the an-
notation guidelines used by our domain experts for the
manual labeling task, including examples of mentions to
include (and exclude) in the dataset. The guidelines evolved
through several iterative annotation cycles, taking into ac-
count feedback and suggestions from the curators where
rules needed to be more explicit.

Deﬁnitions of subcategories

Our SciBERT model was trained on the Softcite Dataset
[23]. We used their curation material as a baseline for our
deﬁnitions:

Software includes all ‘obvious’ software: a good indi-
cation for inclusion is if a tool can be downloaded
and installed; however, “Software as a Service”—
platforms providing up-to-date cloud-based services
for bioinformatic data analysis over a website—are
also included in this category. Programming lan-
guages, such as “Java” or “R”, are annotated as
“Software”; this includes mention of a script writ-
ten in a language (the Softcite coding scheme ex-
plains that “programming languages are themselves
software being used to create software”).

Algorithm is deﬁned as a program, a problem-solving
process that is computerized, or a function. The
programs and algorithms may have implementations
in diﬀerent languages.

Database is a data collection or dataset, knowledgebase
or repository. Note that in some cases, the database
may be supported by a software framework (e.g. with
if the
the data made accessible via an interface);
mention refers to the software component, it should
be labeled as Software (often “Software as a ser-
vice”).

Web platform includes web services such as “Facebook”,
“Google”, or “Amazon”, or other online platforms
that have a web interface.

Hardware is deﬁned as physical instruments, devices, com-

ponents or delivery systems, or other hardware (that
may have software installed on it).

Curation—stage 1: How to annotate

Annotate each Software mention with one of the 5 cat-

egories:

• Software,

• Algorithm,

• Database,

• Web Platform,

• Hardware,

or with “Other” or “Unclear”.

1. For each Software mention 5 example sentences from
papers are included. Use example sentences to un-
derstand the context and whether the mention is re-
ferred to as “software”, “version”, “program”, etc.
2. If the context is not clear, look online for more in-
formation, e.g. the About section of an online tool,
information provided in a GitHub entry, a paper ab-
stract describing the ‘mention’.

3. Where possible, label the mention with one of the
subcategories. Mentions that don’t ﬁt any of the
subcategories 1-5 should be labeled with “Other”
(see examples below).

4. For mentions where more than one subcategory ap-

plies, use the following rules:

• Where the distinction between “Software” and
“Algorithm” is diﬃcult → select “Software”.
• Where the mention could be “Software” and

another category → select “Software”.

• Similarly, where it could “Algorithm” and an-
other category → select “Algorithm”. For ex-
ample, “VISTA is a comprehensive suite of pro-
grams and databases for comparative analysis
of genomic sequences” → label as “Algorithm”.

17

Table A.17: Schema Normalization. Mappings between Normalized ﬁelds and corresponding ﬁelds from each resource: PyPI, CRAN,
Bioconductor, SciCrunch API, GitHub API

Normalized Field PyPI

CRAN

Bioconductor

SciCrunch API

GitHub API

mapped to

PyPI package

CRAN Package

source

PyPI Index

CRAN Index

platform
package url

description

homepage url

PyPI
pypi url

query pypi url
page
query pypi url
page

CRAN
CRAN Link

Title

query CRAN
Link

other urls

None

None

github repo

github repo
license
reference

RRID
scicrunch
synonyms

query pypi url
page

query CRAN
Link

None

None

None
None

None

query CRAN
Link

None
None

Bioconductor
Package
Bioconductor
Index
Bioconductor
Bioconductor
Link
Title

query
Bioconductor
Link
None

query
Bioconductor
Link
None

query
Bioconductor
Link
None
None

Resource Name

best github match

SciCrunch API

GitHub API

SciCrunch
Resource ID
Link
Description

Resource Name
Link

GitHub
github url

Description

github url

Alternate URLs
+ Old URLs
SciCrunch API

None

github url

None

github repo license

Reference Link
or Proper
Citation
Resource ID
synonyms

GitHub API

None
None

18

• Where the mention could be labeled with two
other categories → select either. (Accuracy was
deemed less important here as the ﬁnal goal was
to distinguish between Software+algorithm vs
not-Software).

comma separated, ﬁles with the suﬃx pkl are Python se-
rialized objects [52], ﬁles with the suﬃx gz are gzipped.
Note that tab separated ﬁles may contain embedded quotes,
which do not have special meaning, while in comma sepa-
rated ﬁles they do according to the usual conventions.

5. For ambiguous mentions and acronyms, where the
example sentences appear to refer to multiple diﬀer-
ent things, select “Unclear”.

Curation—stage 2: How to annotate

Annotate each Software mention with one of the two

main categories:

• Software & Algorithm,

• Not Software (including databases, web platforms,

hardware, or Other) Or with “Unclear”

How to annotate:

1. As before, use example sentences to understand the
context and whether a mention is about “software”,
“program”, “algorithm” etc.
If the context is not
clear from the examples, look online for an online
tool, a GitHub entry, a paper abstract, etc.

2. For ambiguous mentions, where the sample sentences
appear to refer to multiple diﬀerent things and don’t
fall clearly into one of two main categories, select
“Unclear”.

3. True software mention that appear as partial terms
should be labeled as Software. For example, the Soft-
ware mention “Random” can be labeled as Software
if the example sentences clearly refer to the same
software (“The superior accuracy of activity mea-
sures was conﬁrmed using Random Forest and pre-
dictive modeling techniques”).

Examples

Examples of tools that should be labeled as “Software
& Algorithm” (Curation stage 2) are shown in Table B.18.
Examples of tools that should be labeled as “Not-Soft-
ware” (Curation stage 2) are shown in Table B.19. The
diﬀerent types (subcategories) used for labeling in stage 1
are indicated in the last column.

Examples of mentions that don’t ﬁt into the ﬁve sub-
categories are liested in Table B.20. They should be la-
beled as “Not-Software” in Curation stage 2 (or as “Other”
in stage 1).

Appendix C.1. Raw ﬁles

The raw directory contains extracted mentions before
disambiguation and linking. It has the following three ﬁles:
comm_raw.tsv.gz (PMC OA commercial subset), non_
comm_raw.tsv.gz (PMC OA non-commercial subset), and
publishers_collection_raw.tsv.gz (CZ Publishers’ col-
lection). Each ﬁle also contains a curation label ﬁeld to
denote the curation label our curation team gave to each
software entry.

The ﬁles comm_raw.tsv.gz (commercial subset) and
non_comm_raw.tsv.gz (non-commercial subset) are tab
separated, gzipped, and have the following ﬁelds:

license either comm or non_comm,

location the location of the ﬁle from which the mentions

are extracted, for example, comm/Micropl/PMC8475362.
nxml,

pmcid PMC id of the paper (with the preﬁx “PMC”

stripped),

pmid PubMed id of the paper,

doi DOI of the paper,

pubdate publication year according to the metadata in

the paper source,

source part of the paper from which the mention was ex-
tracted; this is either a section in the main text of the
paper (like ”introduction” or ”materials and meth-
ods”) or another part of the paper: paper_title,
paper_abstract, tab_caption, fig_caption.

number sequence number of the object, from which the

mention was extracted:

• for body text, paragraph number;
• for ﬁgure and table captions, the number of the

ﬁgure or table;
• otherwise, zero,

text the sentence, from which the software mention was

exracted,

Appendix C. Datasets Description

In this section, we provide details about the datasets

software the extracted software mention,

version the extracted software version,

we make available.

ID software mention ID.

There are six directories: raw, disambiguated, linked,
evaluation and intermediate. Files with the extension
tsv are tab separated, ﬁles with the extension csv are

curation label curation result for the software mention:

• software if the mention was labeled as software

by the curation team

19

Table B.18: Curation Guidelines - Examples of mentions: software & algorithm. These examples were used for training our curation team.

Software
mention
ID

Software
mention

SM2407

ABAQUS

SM6358

gplots

SM5899
SM1591
SM8176

Java
Perl
Keras

Link

Description at the link and notes

https://en.wikipedia.org/wiki/
Abaqus

https://cran.r-project.org/web/
packages/gplots/index.html

“Abaqus FEA (formerly ABAQUS) is a
software suite for ﬁnite element analysis
and computer-aided engineering”
“Various R programming tools for plotting
data”

https://keras.io/

SM1028

I-TASSER

https://zhanggroup.org/I-TASSER/

SM1500

GEPIA

http://gepia.cancer-pku.cn/

SM534

InterProScan

https://www.ebi.ac.uk/interpro/
about/interproscan/

SM14721
SM1693

SurveyMonkey
ARB

SM6650

BiNGO

https://www.surveymonkey.co.uk/
https://en.wikipedia.org/wiki/ARB_
Project
https://www.psb.ugent.be/cbd/
papers/BiNGO/Home.html

SM24091 ADMIXTURE https://bioinformaticshome.com/

SM34896 ABySS

tools/descriptions/ADMIXTURE.html
https://rcc.fsu.edu/software/abyss

SM8175

Adam

SM6044

Blast

https://towardsdatascience.com/
adam-latest-trends-in-deep-
learning-optimization-6be9a291375c
https://blast.ncbi.nlm.nih.gov/
Blast.cgi

SM15716

bowtie

https://bio.tools/bowtie2

SM1735

TopHat

SM4671 MySQL

SM3262

ClueGO

SM4915
SM5658
SM517

R script
CUDA
DNASTAR

SM1004

FASTA

https://ccb.jhu.edu/software/
tophat/index.shtml
https://en.wikipedia.org/wiki/MySQL

https://apps.cytoscape.org/apps/
cluego

https://docs.nvidia.com/cuda/
https://www.dnastar.com/

https://en.wikipedia.org/wiki/
FASTA_format

20

“Keras is an open-source software library
that provides a Python interface for artiﬁ-
cial neural networks”
“server for protein structure and function
prediction”
“interactive web server for analyzing the
RNA sequencing expression data”
“InterProScan is the software package that
allows sequences to be scanned against In-
terPro’s member database signatures.”
online questionnaire tool
“The ARB Project is a free software pack-
age for phylogenetic analysis of rRNA”
“a Java-based tool
to determine which
Gene Ontology (GO) categories are statis-
tically overrepresented in a set of genes or
a subgraph of a biological network”
“Enhancements to the ADMIXTURE algo-
rithm for individual ancestry estimation.”
“ABySS is a Bioinformatics program de-
signed to assemble genomes from small
paired-end sequence reads”
“Adam is a replacement optimization al-
gorithm for stochastic gradient descent for
training deep learning models”
“BLAST is an algorithm and program for
comparing primary biological sequence in-
formation”
“Bowtie 2 is an ultrafast and memory-
eﬃcient tool for aligning sequencing reads
to long reference sequences”
“TopHat-Fusion algorithm”

open-source relational database manage-
ment system
Software plugin

Software script
API
software that has the same name as the
company that develops it
software that has the same name as the
output format (if mention is clearly the
software)

Table B.19: Curation Guidelines - Examples of mentions: not-software. These examples were used for training our curation team.

Link

Description at the link and notes

Subcategory

Software
mention
ID

Software
mention

SM37528 WorldClim

SM15080 ArrayExpress

SM8406
cgMLST
SM15100 ClinVar

SM2352
SM3888

GitHub
Facebook

SM5608
SM5472

Google Earth
Google Play

https://www.worldclim.org/
data/v1.4/worldclim14.html
https://www.ebi.ac.uk/
arrayexpress/
https://www.cgmlst.org/ncs
https://www.ncbi.nlm.nih.
gov/clinvar/
https://github.com/

https://play.google.com/
about/howplayworks/

SM5379

ActiGraph

https://actigraphcorp.com/

SM3147

Leica

SM5200

Kinect

https://leica-camera.com/en-
GB
https://en.wikipedia.org/
wiki/Kinect

“Maps, graphs, tables, and data of
the global climate”
Data archive

Database

Database

information

Database
Database

“Nomenclature Server”
“ClinVar aggregates
about genomic variation”
Software repository
“online social media and social net-
working service”
“Google Earth is a geobrowser”
“Google Play is an online store
where people go to ﬁnd their fa-
vorite apps, games”
“ActiGraph’s wearable accelero-
meter-based biosensors”
Company name and product name Hardware

Hardware

Database
Web platform

Web platform
Web platform

“Kinect is a line of motion sens-
ing input devices produced by Mi-
crosoft”

Hardware

• not software if labeled as not-software by the

text the sentence, from which the software mention was

curation team

exracted,

• unclear if a call could not be made based on

available information

software the extracted software mention.

• not curated if mention has not been curated.

ID software mention ID.

Results of extraction from the CZ Publishers’ collection
are in the ﬁle publishers_collection.tsv.tgz. It is a
tab separated gzipped ﬁle with the following ﬁelds:

doi paper DOI,

pubdate publication year, according to the metadata in

the paper source,

source part of the paper from which the mention was ex-
tracted; this is either a section in the main text of the
paper (like ”introduction” or ”materials and meth-
ods”) or another part of the paper: paper_title,
paper_abstract, tab_caption, fig_caption.

number sequence number of the object, from which the

mention was extracted:

• for body text, paragraph number;
• for ﬁgure and table captions, the number of the

ﬁgure or table;
• otherwise, zero,

curation label curation result for the software mention:

• software if the mention was labeled as software

by the curation team

• not software if labeled as not-software by the

curation team

• unclear if a call could not be made based on

available information

• not curated if mention has not been curated.

Appendix C.2. Disambiguation Results

The linked directory contains the directory synonyms_

files and the ﬁles comm_disambiguated.tsv.gz.

The synonyms_files directory contains synoynms dic-
tionaries used in the disambiguation process, stored in the
pickle format. The following ﬁles are available:

pypi synonyms.pkl Python dictionary mapping from a
pypi package and an array of synonyms generated
through the Keywords Synonym Generation process
2.4.1

21

Table B.20: Curation Guidelines - Examples of mentions: “other” mentions. These examples were used for training our curation team.

Software
mention
ID

Software
mention

Link

SM5111

ANOSIM

SM1779

LOESS

SM5147

ENCODE

SM1166

R Core

SM1901

SM3954

R Foun-
dation for
Statistical
Computing
RE-AIM

https://www.nhm.uio.
no/english/research/
infrastructure/past/help/
anosim.html
https://www.statsdirect.
com/help/nonparametric_
methods/loess.htm
https://www.encodeproject.
org/

https://www.r-project.org/
contributors.html
https://www.r-project.org/
foundation/

https://re-aim.org/

SM5429
SM6157

iOS
Aﬀymetrix

SM16352

nCounter

SM3461

Arduino

SM1127 MiSeq

SM8468

Apache

SM338

QUADAS

SM1908
SM5014

RNAseq
SOLiD

https://www.thermofisher.
com/uk/en/home/life-
science/microarray-
analysis/affymetrix.html

https://nanostring.com/
products/ncounter-analysis-
system/ncounter-pro/
https://www.arduino.cc/

https://emea.illumina.
com/systems/sequencing-
platforms/miseq.html

https://www.apache.org/
licenses/LICENSE-2.0
https://www.bristol.ac.uk/
population-health-sciences/
projects/quadas/quadas-2/

Description at the link

Notes

ANOSIM (ANalysis Of Similari-
ties) is a non-parametric test of
signiﬁcant diﬀerence

Statistical test

method for ﬁtting a smooth curve
between two variables

Statistical test

public
research project which
aims to identify functional ele-
ments in the human genome
a core group, the R Core Team,
with write access to the R source
The R Foundation is a not for
proﬁt organization

Consortium/pro-
ject

Group of people

Group of people

RE-AIM is a framework to guide
the planning and evaluation of
programs according to the 5 key
RE-AIM outcomes: Reach, Eﬀec-
tiveness, Adoption, Implementa-
tion, and Maintenance

“Aﬀymetrix microarray solutions
include necessary components for
a microarray experiment, from ar-
rays and reagents to instruments
and software”

“applications such as targeted re-
sequencing, metagenomics, small
genome sequencing, targeted gene
expression proﬁling”

Project

Operating system
System with mul-
tiple components,
including software

System with mul-
tiple components,
including software
System with mul-
tiple components,
including software
System with mul-
tiple components,
including software

License

A quality assessment tool for di-
agnostic accuracy studies

that

Tools
checklists

are

sequencing technique
SOLiD next-generation sequenc-
ing technology

Method
Method

22

cran synonyms.pkl Python dictionary mapping from a
CRAN package and an array of synonyms generated
through the Keywords Synonym Generation process
2.4.1

bioconductor synonyms.pkl Python dictionary map-
ping from a Bioconductor package and an array of
synonyms generated through the Keywords Synonym
Generation process 2.4.1

scicrunch synonyms.pkl Python dictionary mapping from

a mention found in SciCrunch and its synonyms re-
trieved through the SciCrunch API 2.4.2

extra scicrunch synonyms.pkl Python dictionary map-
ping from a mention found in SciCrunch and its syn-
onyms retrieved by parsing the corresponding URL
in SciCrunch 2.4.2

string similarity synonyms.pkl Python dictionary map-

ping from a mention found in the comm_IDs.tsv.gz
corpus and its synonyms retrieved through the Jaro
Winkler algorithm, together with the corresponding
conﬁdences 2.4.3. Only pairs of synonyms with a
similarity conﬁdence of ≥ 0.9 are kept. The ﬁle has
the following format:

• software mention - mention synonyms are com-

puted for

– scicrunch page query if synonym pair re-
trieved from extra_scicrunch_synonyms.
pkl

– Bioconductor if synonym pair retrieved

from bioconductor_synonyms.pkl

– CRAN if synonym pair retrieved from cran_

synonyms.pkl

– PyPI if synonym pair retrieved from pypi_

synonyms.pkl

– string similarity if synonym pair retrieved
from string_similarity_synonyms.pkl

Appendix C.2.1. Disambiguated File

The comm_disambiguated.tsv.gz ﬁle is a tab sepa-
rated, gzipped ﬁle corresponding to the disambiguation
results for comm.tsv.gz. The ﬁle was obtained by running
the DBSCAN-based clustering algorithm on synonyms.
csv as described in 2. The ﬁle has all the ﬁelds in comm.
tsv.gz with the additional ﬁelds:

mapped to software software entity (or cluster) the soft-
ware mention described in this entry is predicted to
be part of; For example, for the software mention ’R
package limma’, the mapped to software might be
‘limma‘.

mapped to software ID ID of mapped to software

• ([synonyms], [synonyms conﬁdences]) - tuple con-

taining two arrays:

Appendix C.3. Linking Results

– synonyms:
mention

list of synonyms for software

– synonyms conﬁdences: Jaro Winkler sim-
ilarity scores between synonyms and soft-
ware mention, as given by the textdistance
python package [38]

synonyms.tsv.gz tab separated, gzipped comma ﬁle used
as input for the clustering algorithm, after post-pro-
cessing. The ﬁle is built by combining information
from ﬁles pypi_synonyms.pkl, cran_synonyms.pkl,
bioconductor_synonyms.pkl, scicrunch_synonyms.
pkl, extra_scicrunch_synonyms.pkl, and string_
similarity_synonyms.pkl, and performing additional
clean-up. The ﬁle has the following format:

• ID ID for software mention,
• synonym ID ID for synonym,
• software mention software mention to com-

pute synonyms for

• synoynm a synonym for software mention
• synonym conf conﬁdence for this synonym pair

(as described in 1)

• synonym source source for this synonym pair:
– SciCrunch if synonym pair retrieved from

scicrunch_synonyms.pkl

23

The linked directory contains the following directo-
ries: normalized and raw and the ﬁle metadata.tsv.gz.

Appendix C.3.1. Raw Metadata Files

The raw directory contains raw metadata ﬁles obtained
by querying the PyPI, CRAN, Bioconductor, SciCrunch
and GitHub APIs on mentions extracted by the NER al-
gorithm from the comm.tsv.gz corpus (PMC-OA commer-
cial subset). The directory contains the following comma
separated ﬁles:

bioconductor raw df.csv raw metadata ﬁle obtained by
querying Bioconductor on mentions extracted from
the comm.tsv.gz. Has ﬁelds:

• Bioconductor Package
• Bioconductor Link
• Maintainer
• Title

cran raw df.csv raw metadata ﬁle obtained by querying
CRAN on mentions extracted from the comm.tsv.
gz. Has ﬁelds:,

• CRAN Package
• CRAN Link
• Title

github raw df.csv raw metadata ﬁle obtained by query-
ing GitHub on mentions extracted from the comm.
tsv.gz. Has ﬁelds:,

cran df.csv normalized metadata ﬁle obtained by query-
ing CRAN on mentions extracted from the comm.
tsv.gz,

• software mention
• best github match
• description
• github url
• license
• exact match

pypi raw df.csv raw metadata ﬁle obtained by querying
PyPI on mentions extracted from the comm.tsv.gz.
Has ﬁelds:,

• pypi package
• pypi url

scicrunch raw df.csv raw metadata ﬁle obtained by query-

ing SciCrunch on mentions extracted from the comm.
tsv.gz. Has ﬁelds:

• software name
• scicrunch synonyms
• Resource Name
• Resource Name Link
• Description
• Keywords
• Resource ID
• Resource ID Link
• Proper Citation
• Parent Organization
• Parent Organization Link
• Related Condition
• Funding Agency
• Relation
• Reference
• Website Status
• Alternate IDs
• Alternate URLs
• Old URLs
• Reference Link

Appendix C.3.2. Normalized Metadata Files

The normalized directory contains normalized versions
of the raw metadata ﬁles. Files are normalized to a com-
mon schema as described in A.17. The directory contains
the following comma separated ﬁles:

bioconductor df.csv normalized metadata ﬁle obtained
by querying Bioconductor on mentions extracted from
the comm.tsv.gz,

24

github df.csv normalized metadata ﬁle obtained by query-

ing GitHub on mentions extracted from the comm.
tsv.gz,

pypi df.csv normalized metadata ﬁle obtained by query-
ing PyPI on mentions extracted from the comm.tsv.
gz,

scicrunch df.csv normalized metadata ﬁle obtained by
querying SciCrunch on mentions extracted from the
comm.tsv.gz

Appendix C.3.3. Master Metadata File

The metadata.tsv.gz ﬁle is a concatenation of all the
metadata ﬁles in the normalized directory. Each ﬁle in
the normalized directory, as well as the metadata.tsv.gz
ﬁle has the following ﬁelds:

ID a unique identiﬁcatior for each software,

software mention the canonical string for the given soft-

ware mention,

mapped to list of values to which the software was mapped,

source mapping source (PyPI, CRAN, SciCrunch, GitHub,

Bioconductor),

platform list of platforms for the given software,

package url URL for the given package,

description list of descriptions associated with software

in the databases,

homepage url list of homepages for the software,

other urls list of other URLs for the given software mined

from the database,

license list of licenses under which the software is re-

leased,

github repo list of GitHub repositories for the software,

github repo licenses list of licenses listed on the GitHub

repositories,

exact match True if an exact string match was found for
the given software mention, False if a fuzzy match
was used instead,

RRID RRID for the software retrieved from SciCrunch [42],

reference journal articles linked to the software, identi-

ﬁed by DOI, PMID or RRID,

scicrunch synonyms synonyms for software according

to SciCrunch [42].

Appendix C.4. Evaluation Files

The directory evaluation contains ﬁles used for cura-

tion and evaluation.

Appendix C.4.1. Curated Software Mentions

The ﬁle curation_top1k_mentions_multi_labels.csv.

gz contains the results of curation of top 10,000 mentions
over ﬁve categories. It is a comma separated ﬁle with the
following ﬁelds:

Appendix C.4.2. Linking & Disambiguation Evaluation Files
The ﬁle evaluation_linking.csv.gz presents the man-

ual evaluation of linking. Curators were tasked with as-
sessing if the link generated for a particular software
mention was correct. The ﬁle is a gzipped comma-separated
ﬁle with the same ﬁelds as comm_curated.tsv.gz and
metadata.tsv.gz above, and an additional ﬁeld:

software mention manually assigned label by the cura-

tors:

ID unique identiﬁer for software mention,

software˙mention name of software mention,

text ﬁve example sentences from articles where the soft-

ware mention appears,

multi label manually curated label selected for the soft-

ware mention. Possible labels: software, algorithm,
database, hardware, web platform, other, un-
clear,

label manually curated label selected for the software men-

tion. Possible labels: software&algorithm, not˙software,
unclear,

Curation comments curator’s explanation why a spe-
ciﬁc label was chosen, including links to online tools,
spelled-out acronyms, information found through on-
line searches and/or in example sentences.

The ﬁle curation_top10k_mentions_binary_labels.
csv.gz contains the results of curation of top 1000 men-
tions over two categories. It is a comma separated ﬁle with
the following ﬁelds:

ID unique identiﬁer for software mention,

software mention name of software mention,

text ﬁve example sentences from articles where the soft-

ware mention appears,

pmid PubMed identiﬁers for the articles from which the
ﬁve example sentences (in column “text”) were ex-
tracted,

pmcid PubMed Central identiﬁers for the articles,

doi digital object identiﬁer for the articles,

label manually curated label selected for the software men-

tion. Possible labels: software&algorithm, not˙software,
unclear,

curation notes curators’ explanation why a speciﬁc la-
bel was chosen, including links to online tools, spelled-
out acronyms, information found through the online
searches and/or in example sentences.

25

• correct if the package url generated for soft-

ware mention is correct;

• inccorrect if the package url generated for

software mention is incorrect;

• unclear if there isn’t enough information to as-
sess whether or not the package url generated
for software mention is correct;

The ﬁle evaluation_disambiguation.csv.gz presents
the manual evaluation of disambiguation. Curators were
tasked with asssessing if a pair of string software mentions
are synonyms. The ﬁle is a gzipped comma-separated ﬁle
with the following ﬁelds:

link label name of software mention,

synonym synonym generated by the disambiguation al-

gorithm,

text ﬁve example sentences from articles where the syn-

onym mention appears,

pmid PubMed identiﬁers for the articles from which the
ﬁve example sentences (in column “text”) were ex-
tracted,

pmcid PubMed Central identiﬁers for the articles,

doi digital object identiﬁer for the articles,

synonym˙label manually curated label selected for the
software mention, synonym pair. Possible la-
bels:
’Unclear’,
’Exact’,
’Not synonym’

’Not software’,

’Narrow’,

curation notes curators’ explanation why a speciﬁc la-

bel was chosen

Appendix C.5. Intermediate Files

The directory intermediate_files contains the fol-

lowing ﬁles: mention2ID.pkl and freq_dict.pkl

The ﬁle mention2ID.pkl is a mention to ID map-
ping connecting all the plain text software mentions ex-
tracted by the NER algorithm from the comm_raw.tsv.
gz, non_comm_raw.tsv.gz and publishers_collection_
raw.tsv.gz to a unique ID. The ﬁle is in the pickle format
and the data is stored as a Python dictionary. The keys
are plain-text software mentions across all three datasets,
and the values are unique IDs across all three corpora.

mention plain-text software mention

ID unique ID for the plain-text software mention

The ﬁle freq_dict.pkl is a mention to frequency
mapping connecting all plain text software mentions ex-
tracted by the NER algorithm from the comm_raw.tsv.gz
to the their frequency on the comm_raw.tsv.gz dataset.
We deﬁne frequency as the total number of unique papers
a mention appears in the PMC-OA comm dataset. The ﬁle
is in the pickle format and the data is stored as a Python
dictionary. The keys are plain-text software mentions in
comm_raw.tsv.gz, and the values are the corresponding
frequencies.

mention plain-text software mention

frequency total number of unique papers in the comm_
raw.tsv.gz dataset the mention appears in

Appendix D. Additional Examples of String Vari-

ability

Additional examples of string variability can be found

on Figures D.8 and D.9.

26

matrix laboratory
MATLAB
MATLAB, Signal Processing Toolbox and Statistics Tool-
box
MATLAb
MATLab
MATlab
MaTLab
MatLAB
MatLab
Matlab
MATLAB, Statistics and Machine Learning Toolbox
mAtLab)
Matlab)
matlab
matlab - the language of technical computing
matlab)
matlab, signal processing toolbox and statistics toolbox
matlab, statistics and machine learning toolbox
MATLAB
matrix laboratory
matlab
MATLAB -The Language of Technical Computing
MATLAB BGL
MATLAB IDE
MATLAB HMM
MATLAB GUI
MATLAB GPU
MATLAB FDA
MATLAB EFD
MATLAB DIC
MATLAB DGN
MATLAB CVX
MATLAB CPU
MATLAB App
MATLAB® SVM
MATLAB ANN

graphpad, prism
GraPhPad
GraphPAD
GraphPAd
GraphPad
GraphPad)
GRAPHPAD
graphpad
graphpad)
GraphPad, Prism
GraphPad, Prism
GraphPad- Prism
GraphPad-InSTat
GraphPad-InStat
GraphPad-Instat
GraphPad-Prism4
GraphPad-Prism8
GraphPad-Prism7
GraphPad-Prism©
GraphPad-prism5
GraphPad “Prism
GraphPad-prism7
GraphPad-Prism5
GraphPad “PRISM
GraphPad prism8
GraphPad ©Prism
GraphPad prism®
GraphPad prisma
GraphPad prism9
GraphPad.PRISM®
GraphPad prism7
GraphPad prism6
GraphPad prism5
GraphPad prism4
GraphPad prism3
GraphPad primer
GraphPad instat
GraphPad)

Figure D.8: Examples of string variability for the software ‘MAT-
LAB’ (http://www.mathworks.com/products/matlab/), as extracted
by the NER model from the PMC-OA corpus

Figure D.9: Examples of string variability for the software ‘Graph-
Pad’ (http://graphpad.com/), as extracted by the NER model from
the PMC-OA corpus

27

