Accelerating Code Search with Deep Hashing and Code ClassiÔ¨Åcation

Wenchao Gu1‚àó, Yanlin Wang2,‚Ä†, Lun Du2, Hongyu Zhang3,
Shi Han2, Dongmei Zhang2, and Michael R. Lyu1
1 Department of Computer Science and Engineering,
The Chinese University of Hong Kong, China.
2 Microsoft Research Asia, Beijing, China
3 The University of Newcastle, Australia

Abstract

Code search is to search reusable code snippets
from source code corpus based on natural lan-
guages queries. Deep learning-based methods
on code search have shown promising results.
However, previous methods focus on retrieval
accuracy, but lacked attention to the efÔ¨Åciency
of the retrieval process. We propose a novel
method CoSHC to accelerate code search with
deep hashing and code classiÔ¨Åcation, aiming
to perform efÔ¨Åcient code search without sacri-
Ô¨Åcing too much accuracy. To evaluate the ef-
fectiveness of CoSHC, we apply our method
on Ô¨Åve code search models. Extensive experi-
mental results indicate that compared with pre-
vious code search baselines, CoSHC can save
more than 90% of retrieval time meanwhile
preserving at least 99% of retrieval accuracy.

1

Introduction

Code reuse is a common practice during software
development process. It improves programming
productivity as developers‚Äô time and energy can be
saved by reusing existing code. According to pre-
vious studies (Brandt et al., 2009; Lv et al., 2015),
many developers tend to use natural language to
describe the functionality of desired code snippets
and search the Internet/code corpus for code reuse.
Many code search approaches (Brandt et al.,
2009; McMillan et al., 2011; Lv et al., 2015; Du
et al., 2021) have been proposed over the years.
With the rapid growth of open source code bases
and the development of deep learning technology,
recently deep learning based approaches have be-
come popular for tackling the code search prob-
lem (Gu et al., 2018; Husain et al., 2019; Gu et al.,
2021). Some of these approaches adopt neural
network models to encode source code and query
descriptions into representation vectors in the same

‚àó Work done while this author was an intern at Microsoft

Research. Wenchao Gu (wcgu@cse.cuhk.edu.hk).

‚Ä† Yanlin Wang is the corresponding author (yanl-

wang@microsoft.com).

embedding space. The distance between the repre-
sentation vectors whose original code or descrip-
tion are semantically similar should be small. Other
approaches (Feng et al., 2020; Guo et al., 2021; Du
et al., 2021) regard the code search task as a binary
classiÔ¨Åcation task, and calculate the probability of
code matching the query.

In the past, deep learning-based methods focused
on retrieval accuracy, but lacked attention to the
efÔ¨Åciency of retrieval on large-scale code corpus.
However, both types of these deep learning-based
approaches directly rank all the source code snip-
pets in the corpus during searching, which will
incur a large amount of computational cost. For
the approaches that separately encode code and de-
scription representation vectors, the similarity of
the target query vector with all code representation
vectors in the corpus needs to be calculated for ev-
ery single retrieval. In order to pursue high retrieval
accuracy, a high dimension is often set for the repre-
sentation vectors. For example, in CodeBERT, the
dimension of the Ô¨Ånal representation vector is 768.
The similarity calculation between a pair of code
and query vectors will take 768 multiplications and
768 additions between two variables with double
data type. The total calculation of single linear
scan for the whole code corpus containing around
1 million code snippets is extremely large - around
1 billion times of multiplications and additions. As
for the approaches adopting binary classiÔ¨Åcation,
there is no representation vectors stored in advance
and the inference of the target token sequence with
all the description token sequences needs to be
done in real time for every single retrieval. Due
to the large number of parameters in the current
deep learning models, the computation cost will be
signiÔ¨Åcant.

Hashing is a promising approach to improve the
retrieval efÔ¨Åciency and widely adopted in other
retrieval tasks such as image-text search and image-
image search. Hashing techniques can convert high

2
2
0
2

r
a

M
1
3

]
E
S
.
s
c
[

2
v
7
8
2
5
1
.
3
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
dimensional vectors into low dimensional binary
hash code, which greatly reduce the cost of stor-
age and calculation (Luo et al., 2020). Hamming
distance between two binary hash code can also
be calculated in a very efÔ¨Åcient way by running
XOR instruction on the modern computer archi-
tectures (Wang et al., 2016). However, the perfor-
mance degradation is still not avoidable during the
conversion from representation vectors to binary
hash codes even the state-of-the-art hashing models
are adopted. The tolerance of performance degra-
dation from most users is quite low and many of
them are willing to sweep the performance with
efÔ¨Åciency. In order to preserve the performance
of the original code search models that adopt bi-
encoders for the code-query encoding as much as
possible, we integrate deep hashing techniques with
code classiÔ¨Åcation, which could mitigate the perfor-
mance degradation of hashing model in the recall
stage by Ô¨Åltering out the irrelevant data.

SpeciÔ¨Åcally, in this paper, we propose a novel
approach CoSHC (Accelerating Semantic Code
Search with Deep Hashing and Code ClassiÔ¨Åcation)
for accelerating the retrieval efÔ¨Åciency of deep
learning-based code search approaches. CoSHC
Ô¨Årstly clusters the representation vectors into differ-
ent categories. It then generates binary hash codes
for both source code and queries according to the
representation vectors from the original models.
Finally, CoSHC gives the normalized prediction
probability of each category for the given query,
and then CoSHC will decide the number of code
candidates for the given query in each category
according to the probability. Comprehensive exper-
iments have been conducted to validate the perfor-
mance of the proposed approach. The evaluation
results show that CoSHC can preserve more than
99% performance of most baseline models. We
summarize the main contributions of this paper as
follows:

‚Ä¢ We propose a novel approach, CoSHC, to im-
prove the retrieval efÔ¨Åciency of previous deep
learning based approaches. CoSHC is the Ô¨Årst
approach that adopts the recall and re-rank mech-
anism with the integration of code clustering and
deep hashing to improve the retrieval efÔ¨Åciency
of deep learning based code search models.

‚Ä¢ We conduct comprehensive experimental evalua-
tion on public benchmarks. The results demon-
strate that CoSHC can greatly improve the re-
trieval efÔ¨Åciency meanwhile preserve almost the

same performance as the baseline models.

2 Background

2.1 Code Search

In this subsection, we brieÔ¨Çy review some deep
learning based code search approaches. Sachdev
et al. (2018) Ô¨Årstly propose the neural network
based model NCS to retrieve the source code from
a large source code corpus according to the given
natural language descriptions. Cambronero et al.
(2019) propose a neural network model UNIF
based on bag-of-words, which embeds code snip-
pets and natural language descriptions into a shared
embedding space. Gu et al. (2018) propose to
encode source code representation with API se-
quences, method name tokens and code tokens.
Yao et al. (2019) treat code annotation and code
search as dual tasks and utilize the generated code
annotations to improve code search performance.
Husain et al. (2019) explore different neural ar-
chitectures for source code representation and dis-
cover that the self-attention model achieves the
best performance. Gu et al. (2021) extract the pro-
gram dependency graph from the source code and
adopt long short term memory (LSTM) networks
to model this relationship. Feng et al. (2020) pro-
pose a pre-trained model for source code represen-
tation and demonstrate its effectiveness on the code
search task.

2.2 Deep Hashing

In this subsection, we brieÔ¨Çy introduce some repre-
sentative unsupervised cross-modal hashing meth-
ods. In order to learn a uniÔ¨Åed hash code, Ding
et al. (2014) propose to adopt collective matrix
factorization with latent factor model from differ-
ent modalities to merge multiple view information
sources. Zhou et al. (2014) Ô¨Årstly utilize sparse
coding and matrix factorization to extract the latent
features for images and texts, respectively. Then
the learned latent semantic features are mapped to
a shared space and quantized to the binary hash
codes. Wang et al. (2014) suggest using stacked
auto-encoders to capture the intra- and inter-modal
semantic relationships of data from heterogeneous
sources. He et al. (2017) and Zhang et al. (2018)
adopt adversarial learning for cross-modal hash
codes generation. Wu et al. (2018) propose an ap-
proach named UDCMH that integrates deep learn-
ing and matrix factorization with binary latent fac-
tor models to generate binary hash codes for multi-

modal data retrieval. By incorporating Laplacian
constraints into the objective function, UDCMH
preserve not only the nearest neighbors but also the
farthest neighbors of data. Unlike using Laplacian
constraints in the loss function, Su et al. (2019)
construct a joint-semantic afÔ¨Ånity matrix that inte-
grates the original neighborhood information from
different modalities to guide the learning of uniÔ¨Åed
binary hash codes.

3 Method

We propose a general framework to accelerate ex-
isting Deep Code Search (DCS) models by decou-
pling the search procedure into a recall stage and a
re-rank stage. Our main technical contribution lies
in the recall stage. Figure 1 illustrates the overall
framework of the proposed approach. CoSHC con-
sists of two components, i.e., OfÔ¨Çine and Online.
In OfÔ¨Çine, we take the code and description embed-
dings learned in the given DCS model as input, and
learn the corresponding hash codes by preserving
the relations between the code and description em-
beddings. In Online, we recall a candidate set of
code snippets according to the Hamming distance
between the query and code, and then we use the
original DCS model to re-rank the candidates.

3.1 OfÔ¨Çine Stage

Multiple Code Hashing Design with Code Clas-
siÔ¨Åcation Module Since the capacity of binary
hashing space is very limited compared to Eu-
clidean space, the Hamming distance between simi-
lar code snippets will be too small to be distinguish-
able if we adopt a single Hashing model. To be
speciÔ¨Åc, we cluster the codebase using K-Means
algorithm with the code embeddings learned from
the given DCS model. The source code whose rep-
resentation vectors are close to each other will be
classiÔ¨Åed into the same category after the cluster-
ing.
Deep Hashing Module The deep hashing module
aims at generating the corresponding binary hash
codes for the embeddings of code and description
from the original DCS model. Figure 2 illustrates
the framework of the deep hashing module. To
be speciÔ¨Åc, three fully-connected (FC) layers with
tanh(¬∑) activation function are adopted to replace
the output layer in the original DCS model to con-
vert the original representation vectors into a soft
binary hash code.

representations of code pairs and description pairs
approaching the Euclidean distance between the
corresponding embeddings. Thus, we need to cal-
culate the ground truth similarity matrix between
code pairs and description pairs Ô¨Årstly. For perfor-
mance consideration, we calculate the similarity
matrix within a mini-batch.

, ..., v(n)

d , ..., v(n)

To construct such a matrix, we Ô¨Årst deÔ¨Åne
the code representation vectors and the descrip-
tion representation vectors in the original code
search model as VC = {v(1)
c } and VD =
c
{v(1)
d } , respectively. VC and VD repre-
sent the representation vectors matrix for the en-
tire batch, while v(i)
and v(i)
represent the rep-
c
d
resentation vector for the single code snippet or
query. After normalizing VC, VD to ÀÜVC, ÀÜVD with
l2-norm, we can calculate the code similarity matri-
ces SC = ÀÜVC ÀÜV T
C and summary similarity matrices
SD = ÀÜVD ÀÜV T
D to describe the similarity among code
representation vectors and summary representation
vectors, respectively. In order to integrate the simi-
larity information in both SC and SD, we combine
them with a weighted sum:

ÀúS = Œ≤SC + (1 ‚àí Œ≤)SD, Œ≤ ‚àà [0, 1]

(1)

where Œ≤ is the weight parameter. Since the pairwise
similarity among the code representation vectors
and description representation vectors still cannot
comprehensively present the distribution condition
of them in the whole embedding space, we involve
a matrix ÀúS ÀúST to describe a high order neighbor-
hood similarity that two vectors with high similar-
ity should also have the close similarity to other
vectors. Finally, we utilize a weighted equation to
combine both of these two matrices as follows:

S = (1 ‚àí Œ∑) ÀúS + Œ∑

ÀúS ÀúST
m

,

(2)

where Œ∑ is a hyper-parameter and m is the batch
size which is utilized to normalize the second term
in the equation. Since we hope the binary hash
codes of the source code and its corresponding
description to be the same, we replace the diagonal
elements in the similarity matrix with one. The
Ô¨Ånal high order similarity matrix is:

SFij =

(cid:40)

1,
i = j
Sij, otherwise

(3)

The objective of the deep hashing module is
to force the Hamming distance between hashing

Binary Hash Code Training We propose to re-
place the output layer of the original code search

Figure 1: Overview of the proposed CoSHC. 1 Encoding the code token sequence and description token sequence
via original code retrieval models. 2 Clustering the code representation vectors into several categories. 3 Convert-
ing the original code representation vectors into binary hash codes. 5 6 Predicting the category of the query given
by users and set the number of code candidates for different categories. 7 Converting the input query into binary
hash code. 8 Recall the code candidates according to the hamming distance and the number of code candidates
for each category. 9 Re-ranking all the code candidates according to the cosine similarity between the input query
description vectors and code candidates‚Äô representation vectors and return the results to the user.

model with three FC layers with tanh(¬∑) activate
function. We deÔ¨Åne the trained binary hash code
for code and description as BC = {b(1)
, ..., b(n)
c }
c
and BD = {b(1)
d , ..., b(n)
d }, respectively. To ensure
that the relative distribution of binary hash codes
is similar to the distribution of representation vec-
tors in the original embedding space, the following
equation is utilized as the loss function of the deep
hashing module:

L (Œ∏) = min
BC ,BD

(cid:107) min(¬µSF , 1) ‚àí

BCBT
D
d

(cid:107)2
F

+ Œª1(cid:107) min(¬µSF , 1) ‚àí

+ Œª2(cid:107) min(¬µSF , 1) ‚àí

BCBT
C
d
BDBT
D
d

(cid:107)2
F

(cid:107)2
F ,

s.t. BC, BD ‚àà {‚àí1, +1}m√ód,

(4)

where Œ∏ are model parameters, ¬µ is the weighted
parameters to adjust the similarity score between
different pairs of code and description, Œª1, Œª2 are
the trade-off parameters to weight different terms
in the loss function, and d is the dimension of the
binary hash code generated by this deep hashing
module. These three terms in the loss function are
adopted to restrict the similarity among binary hash
codes of the source codes, the similarity among

binary hash codes of the descriptions, and the simi-
larity between the binary hash codes of source code
and description, respectively.

Note that we adopt BCBT

D/d to replace
cos(BC, BD) because cos(BC, BD) only mea-
sures the angle between two vectors but ne-
glects the length of the vectors, which makes
cos(BC, BD) can still be a very large value even
the value of every hash bits is close to zero. Unlike
cos(BC, BD), BCBT
D/d can only achieve a high
value when every bit of the binary hash code is 1
or -1 since the value of BCBT
D/d will be close to
zero if the value of every hash bits is close to zero.
Since it is impractical to impose on the output
of neural network to be discrete values like 1 and
-1, we adopt the following equation to convert the
output of deep hashing module to be strict binary
hash code:

B = sgn(H) ‚àà {‚àí1, +1}m√ód,

(5)

where H is the output of the last hidden layer with-
out the activation function in the deep hashing mod-
ule and sgn(¬∑) is the sign function and the output
of this function is 1 if the input is positive and the
output is -1 otherwise.

However, the gradient of the sign function will
be zero in backward propagation which will induce

Embedding SpaceCodeDescriptionEmbedding SpaceHashing SpaceOffline StageRecallEmbedding SpaceOnline Stage<Code, Description>Hashing SpaceRe-rankQueryCategory PredictionHashingClusteringFigure 2: Architecture of the hashing module. The original representation vectors will be utilized for the joint-
similarity matrix construction at Ô¨Årst. Then the joint-similarity matrix will be utilized as the labels for training
binary hash codes generation. The training objective is to make the Hamming distance similarity matrix to be
identical as the joint-similarity matrix.

the vanishing gradients problem and affect model
convergence. To address this problem, we follow
the previous research (Cao et al., 2017; Hu et al.,
2019) and adopt a scaling function:

B = tanh(Œ±H) ‚àà {‚àí1, +1}m√ód,

(6)

where Œ± is the parameter which is increased dur-
ing the training. The function of tanh(Œ±H) is an
approximate equation of sgn(H) when Œ± is large
enough. Therefore, the output of Eq. 6 will Ô¨Ånally
be converged to 1 or -1 with the increasing of Œ±
during the training and the above problem is ad-
dressed.

3.2 Online Stage

Recall and Re-rank Mechanism The incoming
query from users will be fed into the description
category prediction module to calculate the nor-
malized probability distribution of categories at
Ô¨Årst. Then the number of code candidates Ri for
each category i will be determined according to
this probability distribution. The Hamming dis-
tance between the hash code of the given query and
all the code inside the database will be calculated.
Then code candidates will be sorted by Hamming
distance in ascending order and the top Ri code
candidates in each category i will be recalled. In
the re-rank step, the original representation vectors
of these recalled code candidates will be retrieved
and utilized for the cosine similarity calculation.
Finally, code snippets will be returned to users in
descending order of cosine similarity.
Description Category Prediction Module The
description category prediction module aims to pre-

dict the category of source code that meets user‚Äôs
requirement according to the given natural lan-
guage description. The model adopted for category
prediction is the same as the original code search
model, except that the output layer is replaced with
a one-hot category prediction layer and the cross-
entropy function is adopted as the loss function of
the model.

Since the accuracy of the description category
prediction module is not perfect, we use the prob-
ability distribution of each category instead of the
category with the highest predicted probability as
the recall strategy for code search. We deÔ¨Åne the
total recall number of source code as N , the normal-
ized predicted probability for each code category
as P = {p1, ..., pk}, where k is the number of cat-
egories. The recall number of source code in each
category is:

Ri = min((cid:98)pi ¬∑ (N ‚àí k)(cid:99), 1), i ‚àà 1, ..., k,

(7)

where Ri is the recall number of source code in
category i. To ensure that the proposed approach
can recall at least one source code from each cat-
egory, we set the minimum recall number for a
single category to 1.

4 Experiments

4.1 Dataset

We use two datasets (Python and Java) provided
by CodeBERT (Feng et al., 2020) to evaluate the
performance of CoSHC. CodeBERT selects the
data from the CodeSearchNet (Husain et al., 2019)

Code Embedding SpaceDescription Embedding Space10.40.30.410.20.30.21ùëê1ùëê2ùëê3ùëê1ùëê2ùëê3Code Similarity Matrix ùëÜùê∂10.30.10.310.60.10.61ùë†1ùë†2ùë†3ùë†1ùë†2ùë†3Description Similarity Matrix ùëÜùê∂10.30.20.310.50.20.51ùë†1ùë†2ùë†3ùëê1ùëê2ùëê3Joint-Similarity Matrix ùëÜOriginal Code ModelOriginal Description ModelHashing Layer1111-1-1-1-1-1ùë†1ùë†2ùë†3Description Binary Hash Code1111-1-1-1-1-1ùëê1ùëê2ùëê3Code Binary Hash Code10.60.30.610.60.30.61ùë†1ùë†2ùë†3ùëê1ùëê2ùëê3Hamming Distance Similarity MatrixAlignmentdataset and creates both positive and negative ex-
amples of <description, code> pairs. Since all the
baselines in our experiments are bi-encoder models,
we do not need to predict the relevance score for
the mismatched pairs so we remove all the negative
examples from the dataset. Finally we get 412,178
<description, code> pairs as the training set, 23,107
<description, code> pairs as the validation set, and
22,176 <description, code> pairs as the test set in
the Python dataset. We get 454,451 <description,
code> pairs as the training set, 15,328 <descrip-
tion, code> pairs as the validation set, and 26,909
<description, code> pairs as the test set in the Java
dataset.

4.2 Experimental Setup

In the code classiÔ¨Åcation module, we set the num-
ber of cluster to 10. In the deep hashing module, we
add three fully connected (FC) layer in all the base-
lines, the hidden size of each FC layer is the same
as the dimension of the original representation vec-
tors. SpeciÔ¨Åcally, the hidden size of FC layer for
CodeBERTa, CodeBERT, GraphCodeBERT is 768.
The hidden size of FC layer for UNIF is 512 and
for RNN is 2048. The size of the output binary
hash code for all the baselines is 128. The hyper
parameters Œ≤, Œ∑, ¬µ, Œª1, Œª2 are 0.6, 0.4, 1.5, 0.1, 0.1,
respectively. The parameter Œ± is the epoch num-
ber and will be linear increased during the training.
In the query category prediction module, a cross-
entropy function is adopted as the loss function and
the total recall number is 100.

The learning rate for CodeBERTa, CodeBERT
and GraphCodeBERT is 1e-5 and the learning rate
for UNIF, RNN is 1.34e-4. All the models are
trained via the AdamW algorithm (Kingma and Ba,
2015).

We train our models on a server with four 4x
Tesla V100 w/NVLink and 32GB memory. Each
module based on CodeBERT, GraphCodeBERT
and CodeBERTa are trained with 10 epochs and
Each module based on RNN and UNIF are trained
with 50 epochs. The early stopping strategy is
adopted to avoid overÔ¨Åtting for all the baselines.
The time efÔ¨Åciency experiment is conducted on the
server with Intel Xeon E5-2698v4 2.2Ghz 20-core.
The programming for evaluation is written in C++
and the program is allowed to use single thread of
CPU.

4.3 Baselines

We apply CoSHC on several state-of-the-art and
representative baseline models. UNIF (Cam-
bronero et al., 2019) regards the code as the se-
quence of tokens and embeds the sequence of
code tokens and description tokens into representa-
tion vectors via full connected layer with attention
mechanism, respectively. RNN baseline adopts a
two-layer bi-directional LSTM (Cho et al., 2014)
to encode the input sequences. CodeBERTa 1 is a
6-layer, Transformer-based model trained on the
CodeSearchNet dataset. CodeBERT (Feng et al.,
2020) is a pre-trained model based on Transformer
with 12 layers. Similar to CodeBERT, Graph-
CodeBERT (Guo et al., 2021) is a pre-trained
Transformer-based model pre-trained with not only
tokens information but also dataÔ¨Çow of the code
snippets. As we introduced, the inference efÔ¨Å-
ciency of cross-encoder based models like Code-
BERT is quite low and the purpose of our approach
is to improve the calculation efÔ¨Åciency between the
representation vectors of code and queries. Here
we slightly change the model structure of Code-
BERTa, CodeBERT, and GraphCodeBERT. Rather
than concatenating code and query together and in-
putting them into a single encoder to predict the rel-
evance score of the pair, we adopt the bi-encoder ar-
chitecture for the baselines, which utilize the inde-
pendent encoder to encoding the code and queries
into representation vectors, respectively. Also, co-
sine similarity between the given representation
vector pairs is adopted as the training loss function
to replace the cross entropy function of the output
relevance score.

4.4 Evaluation Metric

SuccessRate@k is widely used by many previ-
ous studies (Haldar et al., 2020; Shuai et al., 2020;
Fang et al., 2021; Heyman and Cutsem, 2020). The
metric is calculated as follows:

SuccessRate@k =

1
|Q|

Q
(cid:88)

q=1

Œ¥(F Rankq ‚â§ k),

(8)

where Q denotes the query set and F Rankq is
the rank of the correct answer for query q. If the
correct result is within the top k returning results,
Œ¥(F Rankq ‚â§ k) returns 1, otherwise it returns 0.
A higher R@k indicates better performance.

1https://huggingface.co/huggingface/

CodeBERTa-small-v1

Python

Java

Total Time

CodeBERT
CoSHC

572.97s
33.87s (‚Üì94.09%)

247.78s
15.78s (‚Üì93.51%)

(1) Vector Similarity Calculation

CodeBERT
CoSHC

531.95s
14.43s (‚Üì97.29%)

234.08s
7.25s (‚Üì96.90%)

(2) Array Sorting

CodeBERT
CoSHC

41.02s
19.44s (‚Üì53.61%)

13.70s
8.53s (‚Üì37.74%)

Table 1: Time EfÔ¨Åciency of CoSHC.

4.5 Experimental Results

In this section, we present the experimental results
and evaluate the performance of CoSHC from the
aspects of retrieval efÔ¨Åciency, overall retrieval per-
formance, and the effectiveness of the internal clas-
siÔ¨Åcation module.

4.5.1 RQ1: How much faster is CoSHC than
the original code search models?
Table 1 illustrates the results of efÔ¨Åciency compari-
son between the original code search models and
CoSHC. Once the representation vectors of code
and description are stored in the memory, the re-
trieval efÔ¨Åciency mainly depends on the dimension
of representation vectors rather than the complexity
of the original retrieval model. Therefore, we select
CodeBERT as the baseline model to illustrate efÔ¨Å-
ciency comparison. Since code search process in
both approaches contains vector similarity calcula-
tion and array sorting, we split the retrieval process
into these two steps to calculate the time cost.

In the vector similarity calculation step, CoSHC
reduces 97.29% and 96.90% of time cost in the
dataset of Python and Java respectively, which
demonstrates that the utilization of binary hash
code can effectively reduce vector similarity calcu-
lation cost in the code retrieval process.

In the array sorting step, CoSHC reduces 53.61%
and 37.74% of time cost in the dataset of Python
and Java, respectively. The classiÔ¨Åcation module
makes the main contribution on the improvement
of sorting efÔ¨Åciency. The sorting algorithm applied
in both original code search model and CoSHC is
quick sort, whose time complexity is O(nlogn).
ClassiÔ¨Åcation module divides a large code dataset
into several small code datasets, reducing the aver-
age time complexity of sorting to O(nlog n
m ). The
reason why the improvement of sorting in the Java
dataset is not so signiÔ¨Åcant as in the Python dataset

is that the size of Java dataset is much smaller than
the size of Python dataset. However, the combi-
nation of the algorithm of divide and conquer and
max-heap, rather than quick sort, is widely applied
in the big data sorting, which can greatly shrink
the retrieval efÔ¨Åciency gap between these two ap-
proaches. Therefore, the improvement of efÔ¨Åciency
in the sorting process will not be as large as what
shown in Table 1.

In the overall code retrieval process, the cost time
is reduced by 94.09% and 93.51% in the dataset
of Python and Java, respectively. Since the vector
similarity calculation takes most of cost time in
the code retrieval process, CoSHC still can reduce
at least 90% of cost time, which demonstrates the
effectiveness on the efÔ¨Åciency improvement in the
code search task.

4.5.2 RQ2: How does CoSHC affect the

accuracy of the original models?
Table 2 illustrates the retrieval performance com-
parison between the original code search models
and CoSHC. We have noticed that the performance
of the conventional approaches like BM25 (Robert-
son and Zaragoza, 2009) is not good enough. For
example, we set the token length for both code
and queries as 50, which is the same as the setting
in CodeBERT, and apply BM25 to recall top 100
code candidates for the re-rank step on the Python
dataset. BM25 can only retain 99.3%, 95.6% and
92.4% retrieval accuracy of CodeBERT in terms
of R@1, R@5 and R@10 on the Python dataset.
Here we only compare the performance of our ap-
proach with the original code search models since
the purpose of our approach is to preserve the per-
formance of the original code search models. As
can be observed, CoSHC can retain at least 99.5%,
99.0% and 98.4% retrieval accuracy of most origi-
nal code search models in terms of R@1, R@5 and
R@10 on the Python dataset. CoSHC can also re-
tain 99.2%, 98.2% and 97.7% of the retrieval accu-
racy as all original code search baselines in terms of
R@1, R@5 and R@10 on the Java dataset, respec-
tively. We can Ô¨Ånd that CoSHC can retain more
than 97.7% of performance in all metrics. R@1 is
the most important and useful metric among these
metrics since most users hope that the Ô¨Årst returned
answer is the correct answer during the search.
CoSHC can retain at least 99.2% of performance
on R@1 in both datasets, which demonstrates that
CoSHC can retain almost the same performance as
the original code search model.

Model

Python

Java

R@1

R@5

R@10

R@1

R@5

R@10

UNIF
CoSHCUNIF
‚àíw/o classiÔ¨Åcation 0.071 (0.0%)
‚àíone classiÔ¨Åcation
0.069 (‚Üì2.8%) 0.163 (‚Üì5.8%) 0.216 (‚Üì8.5%) 0.083 (‚Üì1.2%) 0.183 (‚Üì5.2%) 0.236 (‚Üì7.1%)
‚àíideal classiÔ¨Åcation 0.077 (‚Üë6.9%) 0.202 (‚Üë16.8%) 0.277 (‚Üë17.4%) 0.093 (‚Üë10.7%) 0.222 (‚Üë15.0%) 0.296 (‚Üë16.5%)

0.071
0.072 (‚Üë1.4%) 0.177 (‚Üë2.3%) 0.241 (‚Üë2.1%) 0.086 (‚Üë2.4%) 0.198 (‚Üë2.6%) 0.264 (‚Üë3.9%)

0.174 (‚Üë0.6%) 0.236 (0.0%)

0.085 (‚Üë1.2%) 0.193 (0.0%)

0.254 (0.0%)

0.084

0.254

0.193

0.173

0.236

RNN
0.111
0.112 (‚Üë0.9%) 0.259 (‚Üë2.4%) 0.343 (‚Üë5.0%) 0.076 (‚Üë4.1%) 0.194 (‚Üë5.4%) 0.265 (‚Üë6.0%)
CoSHCRNN
0.186 (‚Üë1.1%) 0.253 (‚Üë1.2%)
‚àíw/o classiÔ¨Åcation 0.112 (‚Üë0.9%) 0.254 (‚Üë0.4%) 0.335 (‚Üë0.6%) 0.073 (0.0%)
‚àíone classiÔ¨Åcation
0.112 (‚Üë0.9%) 0.243 (‚Üì4.0%) 0.311 (‚Üì6.6%) 0.075 (‚Üë2.7%) 0.182 (‚Üì1.1%) 0.240 (‚Üì4.0%)
‚àíideal classiÔ¨Åcation 0.123 (‚Üë10.8%) 0.289 (‚Üë14.2%) 0.385 (‚Üë15.6%) 0.084 (‚Üë15.1%) 0.221 (‚Üë20.1%) 0.302 (‚Üë20.8%)

0.073

0.184

0.250

0.253

0.333

CodeBERTa
0.124
0.123 (‚Üì0.8%) 0.247 (‚Üì1.2%) 0.309 (‚Üì1.6%) 0.090 (‚Üë1.1%) 0.210 (‚Üë3.4%) 0.272 ((‚Üë3.0%)
CoSHCCodeBERTa
0.201 (‚Üì1.0%) 0.258 (‚Üì2.3%)
‚àíw/o classiÔ¨Åcation 0.122 (‚Üì1.6%) 0.242 (‚Üì3.2%) 0.302 (‚Üì3.8%) 0.089 (0.0%)
‚àíone classiÔ¨Åcation
0.116 (‚Üì6.5%) 0.221 (‚Üì11.6%) 0.271 (‚Üì13.7%) 0.085 (‚Üì4.5%) 0.189 (‚Üì6.9%) 0.238 (‚Üì9.8%)
‚àíideal classiÔ¨Åcation 0.135 (‚Üë8.9%) 0.276 (‚Üë10.4%) 0.346 (‚Üë10.2%) 0.100 (‚Üë12.4%) 0.235 (‚Üë15.8%) 0.305 (‚Üë15.5%)

0.089

0.203

0.264

0.250

0.314

CodeBERT
0.683
CoSHCCodeBERT
0.679 (‚Üì0.6%) 0.750 (‚Üì1.2%) 0.318 (‚Üì0.3%) 0.533 (‚Üì0.7%) 0.602 (‚Üì1.0%)
‚àíw/o classiÔ¨Åcation 0.449 (‚Üì0.4%) 0.673 (‚Üì1.5%) 0.742 (‚Üì2.2%) 0.316 (‚Üì0.9%) 0.527 (‚Üì1.9%) 0.593 (‚Üì2.5%)
‚àíone classiÔ¨Åcation
0.425 (‚Üì5.8%) 0.613 (‚Üì10.2%) 0.665 (‚Üì12.4%) 0.304 (‚Üì4.7%) 0.483 (‚Üì10.1%) 0.532 (‚Üì12.5%)
‚àíideal classiÔ¨Åcation 0.460 (‚Üë2.0%) 0.703 (‚Üë2.9%) 0.775 (‚Üë2.1%) 0.329 (‚Üë3.1%) 0.555 (‚Üë3.4%) 0.627 (‚Üë3.1%)

0.451
0.451 (0.0%)

0.319

0.608

0.537

0.759

0.485

GraphCodeBERT
0.792
CoSHCGraphCodeBERT 0.483 (‚Üì0.4%) 0.719 (‚Üì1.0%) 0.782 (‚Üì1.3%) 0.350 (‚Üì0.8%) 0.561 (‚Üì1.8%) 0.625 (‚Üì2.3%)
‚àíw/o classiÔ¨Åcation 0.481 (‚Üì0.8%) 0.713 (‚Üì1.8%) 0.774 (‚Üì2.3%) 0.347 (‚Üì1.7%) 0.553 (‚Üì3.2%) 0.616 (‚Üì3.7%)
‚àíone classiÔ¨Åcation
0.459 (‚Üì5.4%) 0.653 (‚Üì10.1%) 0.698 (‚Üì11.9%) 0.329 (‚Üì7.8%) 0.505 (‚Üì11.6%) 0.551 (‚Üì13.9%)
‚àíideal classiÔ¨Åcation 0.494 (‚Üë1.9%) 0.741 (‚Üë2.1%) 0.803 (‚Üë1.4%) 0.361 (‚Üë2.3%) 0.585 (‚Üë2.5%) 0.649 (‚Üë1.4%)

0.353

0.640

0.571

0.726

Table 2: Results of code search performance comparison. The best results among the three CoSHC variants are
highlighted in bold font.

It is interesting that CoSHC presents a rela-
tively better performance when the performance
of the original code retrieval models is worse.
CoSHCCodeBERTa even outperforms the original
baseline model in Java dataset. CoSHCRNN and
CoSHCUNIF outperform the original model in both
Python and Java datasets. The integration of deep
learning and code classiÔ¨Åcation with recall make
the contribution on this result. The worse perfor-
mance indicates more misalignment between the
code representation vectors and description repre-
sentation vectors. Since the code classiÔ¨Åcation and
deep hashing will Ô¨Ålter out most of irrelevant codes
in the recall stage, some irrelevant code represen-
tation vectors but has high cosine similarity with
the target description representation vectors are Ô¨Ål-
tered, which leads the improvement on the Ô¨Ånal
retrieval performance.

4.5.3 RQ3: Can the classiÔ¨Åcation module
help improve performance?

Table 2 illustrates the performance comparison be-
tween the CoSHC variants which adopt different
recall strategies with query category prediction re-
sults. CoSHCw/o classiÔ¨Åcation represents CoSHC

Model

CoSHCUNIF
CoSHCRNN
CoSHCCodeBERTa
CoSHCCodeBERT
CoSHCGraphCodeBERT

Python
Acc.

0.558
0.610
0.591
0.694
0.713

Java
Acc.

0.545
0.535
0.571
0.657
0.653

Table 3: ClassiÔ¨Åcation accuracy of the code classiÔ¨Åca-
tion module in each model.

without code classiÔ¨Åcation and description pre-
diction module. CoSHCone classiÔ¨Åcation represents
the CoSHC variant that recalls N ‚àí k + 1 candi-
dates in the code category with highest prediction
probability and one in each of the rest categories.
CoSHCideal classiÔ¨Åcation is an ideal classiÔ¨Åcation
situation we set. Assuming the correct descrip-
tion category is known, N ‚àí k + 1 candidates are
recalled in the correct category and one candidate
is recalled in each of the rest categories. Note that
the display of CoSHCideal classiÔ¨Åcation is only to ex-
plore the upper threshold of performance improve-
ment of the category prediction module and will
not be counted as a variant of CoSHC we compare.
be-
the

performance

comparing

By

CoSHCideal classiÔ¨Åcation
and
tween
CoSHCw/o classiÔ¨Åcation, we can Ô¨Ånd that cor-
rect classiÔ¨Åcation can signiÔ¨Åcantly improve
the retrieval performance. With the ideal cat-
egory labels, CoSHC can even outperform all
baseline models. As mentioned in Sec. 4.5.2,
code classiÔ¨Åcation can mitigate the problem of
vector pairs misalignment via Ô¨Åltering out wrong
candidates whose representation vectors has high
cosine similarity with the target representation
vectors in the recall stage. The more serious the
the more effective the
misalignment problem,
code classiÔ¨Åcation. That is the reason why the
improvement of CoSHC with ground-truth labels
on UNIF, RNN, and CodeBERTa is more signiÔ¨Å-
cant than the improvement of it on CodeBERT and
GraphCodeBERT since the retrieval accuracy of
former models is much lower than the latter ones.
Similar conclusions can also be drawn at the aspect
of binary hash code distribution via the comparison
between CoSHC and CoSHCideal classiÔ¨Åcation
since CoSHC utilizes the distribution of the
original representation vectors as the guidance
for model training. Therefore, the distribution of
binary hash codes will be similar to the distribution
of original representation vectors.

Since we have explored the theoretical upper
limit of the effectiveness of code classiÔ¨Åcation
for code retrieval, the effectiveness of code clas-
siÔ¨Åcation for code retrieval in the real applica-
tion will be validated. By comparing the experi-
mental results between CoSHCw/o classiÔ¨Åcation and
CoSHCone classiÔ¨Åcation, we can Ô¨Ånd that the per-
formance of CoSHC with predicted labels is even
worse than the performance of CoSHC without
code classiÔ¨Åcation module. The reason is that the
accuracy of description category prediction is far
from the satisfactory. Table 3 illustrates the accu-
racy of description category prediction module in
all baseline models. We regard the category with
the highest probability as the predicted category
from the description category prediction module
and check whether the module could give a correct
prediction. It can be seen that the classiÔ¨Åcation ac-
curacy is not very high (less than 75%). By observ-
ing the experimental results of CoSHC in Graph-
CodeBERT on the Java dataset, we can also Ô¨Ånd
that low accuracy greatly affect the performance of
CoSHConeclassiÔ¨Åcation, which makes 7.8%, 11.6%,
and 13.9% performance drop in terms of R@1,
R@5, and R@10, respectively.

Fortunately, although the description category
prediction module cannot accurately tell the ex-
act category which this description belongs to,
the module still can give a relative high predicted
probability on the correct category. By compar-
ing the experimental results among all the vari-
ants of CoSHC, we can Ô¨Ånd the performance is
increased signiÔ¨Åcantly once the recall strategy is
replaced to that the number of code candidates
for each category is determined by the normalized
predication probability. CoSHC with new recall
strategy almost achieve the best performance in all
metrics on all baseline models. Even on RNN in
the Python dataset, CoSHC still achieve the same
performance as CoSHC without classiÔ¨Åcation un-
der R@1 and achieve similar performance in other
metrics. Above experimental results have demon-
strated the effectiveness of the adoption of code
classiÔ¨Åcation in code search.

5 Conclusion
To accelerate code search, we present CoSHC, a
general method that incorporates deep hashing tech-
niques and code classiÔ¨Åcation. We leverage the two-
staged recall and re-rank paradigm in information
retrieval Ô¨Åeld and apply deep hashing techniques
for fast recall. Furthermore, we propose to utilize a
code classiÔ¨Åcation module to retrieve better quality
code snippets. Experiments on Ô¨Åve code search
models show that compared with the original code
search models, CoSHC can greatly improve the
retrieval efÔ¨Åciency meanwhile preserve almost the
same performance.

6 Acknowledgement

Wenchao Gu‚Äôs and Michael R. Lyu‚Äôs work de-
scribed in this paper was in part supported by the
Research Grants Council of the Hong Kong Special
Administrative Region, China (CUHK 14210920
of the General Research Fund).

References

Joel Brandt, Philip J. Guo, Joel Lewenstein, Mira
Dontcheva, and Scott R. Klemmer. 2009. Two stud-
ies of opportunistic programming: interleaving web
foraging, learning, and writing code. In Proceedings
of the 27th International Conference on Human Fac-
tors in Computing Systems, CHI 2009, Boston, MA,
USA, April 4-9, 2009, pages 1589‚Äì1598. ACM.

Jos√© Cambronero, Hongyu Li, Seohyun Kim, Koushik
Sen, and Satish Chandra. 2019. When deep learn-
In Proceedings of the ACM
ing met code search.

Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of
Software Engineering, ESEC/SIGSOFT FSE 2019,
Tallinn, Estonia, August 26-30, 2019, pages 964‚Äì
974. ACM.

Zhangjie Cao, Mingsheng Long, Jianmin Wang, and
Philip S. Yu. 2017. Hashnet: Deep learning to hash
by continuation. In IEEE International Conference
on Computer Vision, ICCV 2017, Venice, Italy, Octo-
ber 22-29, 2017, pages 5609‚Äì5618. IEEE Computer
Society.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
In Proceedings of SSST@EMNLP 2014,
proaches.
Eighth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, Doha, Qatar, 25 Oc-
tober 2014, pages 103‚Äì111. Association for Compu-
tational Linguistics.

Guiguang Ding, Yuchen Guo, and Jile Zhou. 2014.
Collective matrix factorization hashing for multi-
modal data. In 2014 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2014, Colum-
bus, OH, USA, June 23-28, 2014, pages 2083‚Äì2090.
IEEE Computer Society.

Lun Du, Xiaozhou Shi, Yanlin Wang, Ensheng Shi, Shi
Han, and Dongmei Zhang. 2021. Is a single model
enough? mucos: A multi-model ensemble learning
In CIKM ‚Äô21:
approach for semantic code search.
The 30th ACM International Conference on Infor-
mation and Knowledge Management, Virtual Event,
Queensland, Australia, November 1 - 5, 2021, pages
2994‚Äì2998. ACM.

Sen Fang, Youshuai Tan, Tao Zhang, and Yepang Liu.
2021. Self-attention networks for code search. Inf.
Softw. Technol., 134:106542.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-
bert: A pre-trained model for programming and nat-
In Findings of the Association for
ural languages.
Computational Linguistics: EMNLP 2020, Online
Event, 16-20 November 2020, volume EMNLP 2020
of Findings of ACL, pages 1536‚Äì1547. Association
for Computational Linguistics.

Wenchao Gu, Zongjie Li, Cuiyun Gao, Chaozheng
Wang, Hongyu Zhang, Zenglin Xu, and Michael R.
Lyu. 2021. Cradle: Deep code retrieval based on
semantic dependency learning. Neural Networks,
141:385‚Äì394.

Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018.
Deep code search. In Proceedings of the 40th Inter-
national Conference on Software Engineering, ICSE
2018, Gothenburg, Sweden, May 27 - June 03, 2018,
pages 933‚Äì944. ACM.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tu-
fano, Shao Kun Deng, Colin B. Clement, Dawn
Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and
Ming Zhou. 2021. Graphcodebert: Pre-training
In 9th Inter-
code representations with data Ô¨Çow.
national Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net.

Rajarshi Haldar, Lingfei Wu, Jinjun Xiong, and Julia
Hockenmaier. 2020. A multi-perspective architec-
ture for semantic code search. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2020, Online, July 5-10,
2020, pages 8563‚Äì8568. Association for Computa-
tional Linguistics.

Li He, Xing Xu, Huimin Lu, Yang Yang, Fumin Shen,
and Heng Tao Shen. 2017. Unsupervised cross-
modal retrieval through adversarial learning.
In
2017 IEEE International Conference on Multimedia
and Expo, ICME 2017, Hong Kong, China, July 10-
14, 2017, pages 1153‚Äì1158. IEEE Computer Soci-
ety.

Geert Heyman and Tom Van Cutsem. 2020. Neu-
ral code search revisited: Enhancing code snippet
retrieval through natural language intent. CoRR,
abs/2008.12193.

Di Hu, Feiping Nie, and Xuelong Li. 2019. Deep bi-
nary reconstruction for cross-modal hashing. IEEE
Trans. Multim., 21(4):973‚Äì985.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search. CoRR, abs/1909.09436.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Xiao Luo, Chong Chen, Huasong Zhong, Hao Zhang,
Minghua Deng, Jianqiang Huang, and Xiansheng
Hua. 2020. A survey on deep hashing methods.
CoRR, abs/2003.03369.

Fei Lv, Hongyu Zhang, Jian-Guang Lou, Shaowei
Wang, Dongmei Zhang, and Jianjun Zhao. 2015.
Codehow: Effective code search based on API un-
In
derstanding and extended boolean model (E).
30th IEEE/ACM International Conference on Auto-
mated Software Engineering, ASE 2015, Lincoln,
NE, USA, November 9-13, 2015, pages 260‚Äì270.
IEEE Computer Society.

Collin McMillan, Mark Grechanik, Denys Poshyvanyk,
Qing Xie, and Chen Fu. 2011. Portfolio: Ô¨Ånding rel-
evant functions and their usage. In Proceedings of

Jile Zhou, Guiguang Ding, and Yuchen Guo. 2014. La-
tent semantic sparse hashing for cross-modal simi-
larity search. In The 37th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ‚Äô14, Gold Coast , QLD, Aus-
tralia - July 06 - 11, 2014, pages 415‚Äì424. ACM.

the 33rd International Conference on Software En-
gineering, ICSE 2011, Waikiki, Honolulu , HI, USA,
May 21-28, 2011, pages 111‚Äì120. ACM.

Stephen E. Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and be-
yond. Found. Trends Inf. Retr., 3(4):333‚Äì389.

Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun
Kim, Koushik Sen, and Satish Chandra. 2018. Re-
trieval on source code: a neural code search. In Pro-
ceedings of the 2nd ACM SIGPLAN International
Workshop on Machine Learning and Programming
Languages, MAPL@PLDI 2018, Philadelphia, PA,
USA, June 18-22, 2018, pages 31‚Äì41. ACM.

Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin
Xia, and Yan Lei. 2020. Improving code search with
In ICPC ‚Äô20:
co-attentive representation learning.
28th International Conference on Program Compre-
hension, Seoul, Republic of Korea, July 13-15, 2020,
pages 196‚Äì207. ACM.

Shupeng Su, Zhisheng Zhong, and Chao Zhang. 2019.
Deep joint-semantics reconstructing hashing for
large-scale unsupervised cross-modal retrieval.
In
2019 IEEE/CVF International Conference on Com-
puter Vision, ICCV 2019, Seoul, Korea (South), Oc-
tober 27 - November 2, 2019, pages 3027‚Äì3035.
IEEE.

Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang.
2016. Learning to hash for indexing big data - A
survey. Proc. IEEE, 104(1):34‚Äì57.

Wei Wang, Beng Chin Ooi, Xiaoyan Yang, Dongx-
iang Zhang, and Yueting Zhuang. 2014. Effec-
tive multi-modal retrieval based on stacked auto-
encoders. Proc. VLDB Endow., 7(8):649‚Äì660.

Gengshen Wu, Zijia Lin,

Jungong Han, Li Liu,
Guiguang Ding, Baochang Zhang, and Jialie Shen.
2018. Unsupervised deep hashing via binary latent
factor models for large-scale cross-modal retrieval.
In Proceedings of the Twenty-Seventh International
Joint Conference on ArtiÔ¨Åcial Intelligence, IJCAI
2018, July 13-19, 2018, Stockholm, Sweden, pages
2854‚Äì2860. ijcai.org.

Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan
Sun. 2019. Coacor: Code annotation for code re-
In The World
trieval with reinforcement learning.
Wide Web Conference, WWW 2019, San Francisco,
CA, USA, May 13-17, 2019, pages 2203‚Äì2214.
ACM.

In Proceedings of

Jian Zhang, Yuxin Peng, and Mingkuan Yuan. 2018.
Unsupervised generative adversarial cross-modal
the Thirty-Second
hashing.
AAAI Conference on ArtiÔ¨Åcial Intelligence, (AAAI-
18), the 30th innovative Applications of ArtiÔ¨Åcial In-
telligence (IAAI-18), and the 8th AAAI Symposium
on Educational Advances in ArtiÔ¨Åcial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February
2-7, 2018, pages 539‚Äì546. AAAI Press.

