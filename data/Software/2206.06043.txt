2
2
0
2

t
c
O
0
2

]
E
S
.
s
c
[

4
v
3
4
0
6
0
.
6
0
2
2
:
v
i
X
r
a

Combining BMC and Fuzzing Techniques for
Finding Software Vulnerabilities in Concurrent
Programs

Fatimah K. Aljaafari1,2, Rafael Menezes1,3 Edoardo Manino1 Fedor Shmarov1
Mustafa A. Mustafa1,4 and Lucas C. Cordeiro1,3
1Department of Computer Science
The University of Manchester
Manchester, UK
2King Faisal University, Al-Hofuf, Saudi Arabia
3Federal University of Amazonas, Manaus, Brazil
4imec-COSIC, KU Leuven, Leuven, Belgium

Abstract—Finding software vulnerabilities in concurrent pro-
grams is a challenging task due to the size of the state-space
exploration, as the number of interleavings grows exponentially
with the number of program threads and statements. We propose
and evaluate EBF (Ensembles of Bounded Model Checking with
Fuzzing) – a technique that combines Bounded Model Checking
(BMC) and Gray-Box Fuzzing (GBF) to ﬁnd software vulnerabili-
ties in concurrent programs. Since there are no publicly-available
GBF tools for concurrent code, we ﬁrst propose OpenGBF – a new
open-source concurrency-aware gray-box fuzzer that explores
different thread schedules by instrumenting the code under test
with random delays. Then, we build an ensemble of a BMC tool
and OpenGBF in the following way. On the one hand, when the
BMC tool in the ensemble returns a counterexample, we use it as
a seed for OpenGBF, thus increasing the likelihood of executing
paths guarded by complex mathematical expressions. On the
other hand, we aggregate the outcomes of the BMC and GBF
tools in the ensemble using a decision matrix, thus improving
the accuracy of EBF. We evaluate EBF against state-of-the-art
pure BMC tools and show that it can generate up to 14.9% more
correct veriﬁcation witnesses than the corresponding BMC tools
alone. Furthermore, we demonstrate the efﬁcacy of OpenGBF, by
showing that it can ﬁnd 24.2% of the vulnerabilities in our eval-
uation suite, while non-concurrency-aware GBF tools can only
ﬁnd 0.55%. Finally, thanks to our concurrency-aware OpenGBF,
EBF detects a data race in the open-source wolfMqtt library and
reproduces known bugs in several other real-world programs,
which demonstrates its effectiveness in ﬁnding vulnerabilities in
real-world software.

Index Terms—Concurrency-aware Gray-Box Fuzzer, Bounded
Model Checking, Concurrent Programs, Instrumentation, LLVM
pass.

I. INTRODUCTION

Concurrency is becoming prevalent in present-day software
systems thanks to the performance beneﬁts provided by multi-
core hardware [1]. Examples of such software systems include
online banking, auto-pilots, computer games and railway ticket
reservation systems [2]. Ensuring the correctness and safety
of such software is crucial since software failures may lead to
signiﬁcant ﬁnancial losses and affect people’s well-being [3].

the OpenSSL library had a Heartbleed1
As an example,
vulnerability that allows a remote attacker to get access to
sensitive information.

Despite the signiﬁcant resources invested into software test-
ing, much of existing software still features security vulnera-
bilities [4] This is because the different possible threads’ inter-
leavings cause the program execution to be non-deterministic,
thus making the process of testing and verifying concurrent
programs an inherently difﬁcult task [5] (e.g., some bugs may
occur only for a speciﬁc threads order, making them harder to
detect). Furthermore, there exists a wide variety of unwanted
concurrent behaviors. On the one hand, the non-determinism
of the thread interleavings introduces concurrency bugs such as
data races, deadlocks, thread leaks, and resource starvation [4],
which may cause the program to produce abnormal results or
unforeseen hangs. On the other hand, speciﬁc program inputs
and thread interleavings may lead to memory corruption and
security violations (e.g., access out of bounds) [6].

Due to this complexity, manual testing of concurrent soft-
ware is not always adequate, and so automated veriﬁcation and
testing are often employed. In this respect, there is a myriad of
different automated techniques such as control engineering [7],
abstract interpretation [8] and data-ﬂow analysis [9] for de-
tecting bugs and vulnerabilities in concurrent programs [10],
[11]. Among those, two methods have seen signiﬁcant devel-
opment in recent years: Bounded Model Checking (BMC) and
fuzzing [12].

BMC [13] searches for violations in bounded executions
(up to some given depth k) of the given program. If no
property violation is detected, then k is increased until a bug is
found, the veriﬁcation problem becomes intractable, or a pre-
set upper bound is reached. Although many industrial-grade
bounded model checkers [14]–[18] have been successfully
used for software veriﬁcation, BMC has several fundamen-
tal drawbacks in general. Namely, BMC often experiences

1https://heartbleed.com/

 
 
 
 
 
 
difﬁculties with achieving high path coverage (especially for
multi-threaded programs) and reaching deep statements within
the code because of state-space explosion and its dependency
on Boolean Satisﬁability (SAT) [19] or Satisﬁability Modulo
Theories (SMT) solvers [20].

Fuzzing [11] is an automated software testing technique
that involves the repeated generation of inputs (based on some
initial guess – seed value) to a Program Under Test (PUT). The
PUT is then executed for each given sequence of input values;
its behavior is checked for abnormalities, such as crashes or
failures [21]. The main advantages of fuzzing include relative
ease of integration with the existing testing frameworks, high
scalability, and most importantly, exploring the deep execution
paths is not as costly as in BMC. However, fuzzing often
suffers from low branch coverage since the input generation
is based on random mutations [22]. Typically this occurs when
a program features conditional statements with complex con-
ditions (e.g., input validation functions). As a result, providing
a good initial seed for the fuzzing process is crucial. Moreover,
fuzzing techniques face challenges detecting vulnerabilities in
multi-threaded programs [23] since existing fuzzing techniques
do not focus on thread interleavings that affect execution
states.

Efforts toward developing a combined veriﬁcation technique
harnessing the strengths of both BMC and fuzzing have
been made in the past. For example, Ognawala et al. [24]
combine symbolic execution and fuzzing and apply it
to
general-purpose software. Alshmrany et al. [25] use BMC
to guide a fuzzer in the analysis of sequential C programs.
Chowdhury et al. [26] improve the seeding of gray-box fuzzing
(GBF) by using BMC as a constraint solver to ﬁnd execution
paths through complex blocks of code. Nevertheless, given the
current knowledge in software veriﬁcation, there are no tech-
niques that harness both BMC and fuzzing for veriﬁcation of
concurrent programs, and the question of whether combining
BMC and fuzzing improves bug ﬁnding in concurrent programs
remains open.

The challenge in answering this fundamental question is
twofold. First, while there are many available BMC tools in
the literature, all existing concurrency fuzzers are (at least
partially) closed source. As a result, employing any of these
concurrency fuzzers requires a major reproducibility effort.
Second, combining BMC and fuzzing for concurrency is not
straightforward. Given the lack of existing baselines, we take
inspiration from portfolios [27], [28], the practice of running
an ensemble of similar tools in parallel and picking the best
result. At the same time, BMC and fuzzing are very dissimilar
approaches, thus their cooperation inside the ensemble has to
be carefully coordinated.

This paper addresses these challenges and makes the fol-

lowing original contributions:

1) We develop OpenGBF – a new open-source state-of-the-
art concurrency-aware gray-box fuzzer [29]. Our main
technique is instrumenting the PUT with random delays
obtained from a random number generator whose seed
value is controlled by the fuzzer. In this way, we can

discover different thread interleavings and explore deep
execution paths. Furthermore, our fuzzer is capable of
generating crash reports containing the full program ex-
ecution path.

2) We introduce EBF – Ensembles of Bounded Model
Checking with Fuzzing. This technique combines the
strengths of BMC in resolving complex conditional
guards with the ﬂexibility of our concurrency-aware gray-
box fuzzer. EBF incorporates a result decision matrix for
coping with the potentially conﬂicting verdicts produced
by the tools in the ensemble. Furthermore, EBF efﬁciently
distributes the available computational resources between
the tools to enhance its bug-ﬁnding capabilities.

3) We demonstrate that

the combination of BMC and
fuzzing improves veriﬁcation outcomes compared to ei-
ther technique applied separately. More speciﬁcally, EBF
improves the bug-ﬁnding abilities of all state-of-the-
art concurrent BMC tools considered in this work by
up to 14.9%. Similarly, EBF can ﬁnd 24.2% of the
vulnerabilities in our evaluation suite, whereas the state-
of-the-art gray-box fuzzer AFL++ can only ﬁnd 0.55%.
4) We apply EBF to the wolfMQTT open-source library
that implements the MQTT messaging protocol, and we
discover the presence of a data race bug. We reported
the bug to the developers of the wolfMQTT library, who
ﬁxed it in June 2021. Also, EBF successfully reproduced
known bugs in several real-world concurrent programs
(i.e., pfscan [30], bzip2smp [31] and swarm 1.1 [32]).
This demonstrates the real-world capabilities of EBF.
5) We report that the bug-ﬁnding capabilities of EBF are sta-
ble across a wide range of parameter values. In detail, we
run a comparison experiment along three different axis:
time allocation between the BMC tool and OpenGBF,
maximum delay inserted by OpenGBF and maximum
number of threads allowed by OpenGBF. Our results
show a large sweet spot of parameter values that allows
EBF to ﬁnd nearly 50-fold more bugs than the worst
setting.

This remaining of the paper is structured as follows: Sec-
tion II contains the preliminaries on concurrent programs,
bounded model checking and fuzzing, while Section III states
the main research question of this work. Section IV dis-
cusses the main design choices and implementation details of
OpenGBF, our state-of-the-art fuzzer for concurrent programs.
Section V presents EBF, our ensemble veriﬁcation technique.
Section VI presents the experimental results, while Section VII
lists the related work, and Section VIII draws the ﬁnal con-
clusions.

II. PRELIMINARIES

A. Common Software Vulnerabilities

Concurrent programs feature multiple processes or threads
simultaneously operating on shared computing resources [33].
As a result, such programs can feature vulnerabilities spe-
ciﬁc to sequential problems (e.g., invalid memory accesses,

memory leaks [34]) as well as types of bugs that only occur
in concurrent programs (e.g., data races, deadlocks, thread
leaks [35]). Some software vulnerabilities are considered more
dangerous than others. For example, writing out of bounds (a
type of invalid memory access) is ranked number 1 in the top
25 MITRE ranking in 2022 2, while data races are in the 22nd
place.

Invalid memory accesses comprise a vast family of mem-
ory safety violations. They include accessing memory outside
the bounds of the intended buffer for either reading (potentially
revealing some sensitive data to the attacker) or writing
(causing memory corruptions or injections of executable code),
accessing previously freed memory (aka “use-after-free”), and
dereferencing of invalid pointers or NULL pointers (causing
the program to crash or exit unexpectedly).

Data race is a condition when the program execution
results in an undesired behavior due to a particular sequence
and/or timing of the instructions executed by each thread.
For instance, when a thread modiﬁes the shared memory
without acquiring a lock ﬁrst, causing memory corruption
when another thread tries to update the same memory location
(see Figure 1a).

Deadlock occurs when the program is not in the terminal
state and it cannot progress to any other state. For instance,
when a thread does not release a lock after accessing the
shared memory, therefore, denying memory access for any
other thread (see Figure 1b).

Thread leak is a vulnerability speciﬁc to multi-threaded
programs that happens when a terminated thread never joins
the calling thread, thus never releasing the occupied resources
(see Figure 1c).

Similarly, memory leaks are caused by repeated memory
allocations which are never released during the program’s
execution. This may lead to memory exhaustion resulting in
the system hanging or crashing.

B. Bounded Model Checking

Bounded model checking is a veriﬁcation technique that
has been successfully applied to software and hardware ver-
iﬁcation over the past decades [13]. BMC works with the
underlying program’s mathematical model (represented as a
ﬁnite state transition system). It explores the model’s evolution
up to some ﬁnite positive bound k and determines whether the
given safety property (e.g., absence of deadlocks, data races,
buffer overﬂows, assertion violations, etc.) holds. In short,
BMC symbolically executes the given program up to the given
bound k and encodes all the obtained traces C together with
the given property P as a SAT/SMT [36] formula C
P .
A decision procedure (often referred to as automated theorem
prover or solver) then checks the obtained formula and returns
the satisﬁability verdict. If the formula is satisﬁable, then
it means that the safety property is violated, and a witness
(counter-example) is produced. Otherwise, a proof can be
obtained that the program is safe up to the given bound k.

∧ ¬

2https://cwe.mitre.org/top25/archive/2022/2022 cwe top25.html

(a) Data race occurs when T1 and T2 are trying to write to the memory region
A simultaneously with no synchronization between the operations.

(b) The program ends in a deadlock since T1 acquires a lock for the memory
region A and then tries to write to the memory region B. At the same time, T2
performs the opposite acquiring a lock for B and attempting to write to A. This
will result in both threads waiting indeﬁnitely for each other to release their
corresponding locks before the program’s execution can continue.

(c) T3 is a source of thread leak since, unlike T2, it terminates but never
joins T1. Therefore, the number of unused threads increases with time causing
a potential resource exhaustion.

Fig. 1: Concurrency bugs.

Several drawbacks of BMC include state-space explosion
as the veriﬁcation depth grows, which becomes even more
challenging for multi-threaded programs since it is required to
explore the combined search space of thread interleavings and
program states. Moreover, the veriﬁcation of logical formulae
consumes more CPU time and computer memory as the
size of the formulae grows with the increasing veriﬁcation
depth. Finally, since BMC works with a symbolic abstraction
(over-approximation) of the underlying program, it may report
incorrect results when the devised model does not precisely
represent the given program. For example, this can be caused
by external libraries whose implementation in the language
supported by the given BMC tool does not exist. Consequently,
their behavior must be modelled (approximated) inside the
BMC tool.

Thus, existing BMC tools like ESBMC [37], CBMC [38] and
Cseq [39] differ mainly in their choices of program encoding
and symbolic abstractions. We provide more details on their
strategies to deal with concurrent programs in Section VII.

C. Gray-Box Fuzzing

Fuzzing is an automated testing technique that discov-
ers vulnerabilities by repeatedly executing a program with
randomly-generated inputs [40]. Since most inputs generated
this way are invalid, state-of-the-art fuzzers let users specify
a small set of valid program inputs (the seeds) and employ
a mutation-based strategy to generate new ones. Gray-box
fuzzing improves on this idea by guiding the mutation pro-
cess with program-speciﬁc metrics. To do so, the program

writeto(A)......T1T2writeto(A)......timewriteto(B)......T1lock(A)unlock(A)......T2writeto(A)......lock(B)unlock(B)......timecreate(T3)create(T2)join(T2)............T1T2T3time............}

III. PROBLEM STATEMENT

under test must be instrumented with some additional code
that tracks the required metric (e.g., code coverage) during
execution.

–

–

seed

crash

queue, SI

Algorithm 1 Gray-Box Fuzzing
Input: P U T – program under test, M – corpus of initial
seeds.
Output: QS
found
1: Pf
2: QS
3: SI =
4: while not timeout do
t
5:
6: N
7:
8:

instrument the PUT
}
{
initialize the seed queue
}
{

instrument(P U T )
M

pick seed from queue
{

get mutation chance(Pf , t)

select next seed(QS)

←
←
∅

inputs

←

}

mutate the seed
}
{
execute the PUT
}
{

←
1 . . . N do
for all i
∈
t(cid:48)
mutate input(t)
←
run(Pf , t(cid:48), Mc)
rep
if is crash(rep) then
SI

SI

←

t(cid:48)

←

∪

else if covers new trace(t(cid:48), rep) then

new vulnerable input found!
{

QS

t(cid:48)

⊕

←

add promising seeds to queue
}
{

9:
10:
11:
12:
13:
14:

QS
end if
end for
15:
16: end while

Algorithm 1 [23], [41] shows the standard workﬂow of a
gray-box fuzzer. It takes a target PUT and initial seeds M as
inputs. Then, it instruments the PUT (line 1) by inserting some
additional code that allows the fuzzer to collect code coverage
statistics in the PUT. At every iteration of the main fuzzing
loop (line 4), it selects a seed t (line 5) and chooses a random
number N of mutations (line 6). Then, the fuzzer repeatedly
executes the instrumented program Pf (line 9) with different
mutated seed t(cid:48) (line 8) as input and obtains the execution
statistics. If t(cid:48) triggers a crash in the instrumented program Pf
(line 10), it is added to the set of vulnerable inputs (line 11).
Otherwise, if t(cid:48) does not cause a crash but covers a new branch
in the PUT (line 12), it is added to the seed queue QS (line
13). This may help the fuzzer discover more vulnerabilities
in the subsequent iterations. Finally, the execution of the main
fuzzing loop continues until the predeﬁned timeout is reached.
Multiple attempts have been made to detect security vul-
nerabilities in concurrent programs with fuzzing [23], [34],
[42]–[44]. Here, we organize these past efforts according to
ﬁve categories in the taxonomy of Table I. The ﬁrst three
categories concern the usability of each fuzzer: whether they
apply to user programs or operating system code (Scope),
which type of bugs they are able to detect (Vulnerabilities), and
whether their code is easily accessible (Open Source). In this
regard, none of the existing state-of-the-art fuzzers satisfy our
research requirements. That is, there is no fully open-source
fuzzer that can detect multiple concurrency vulnerabilities in
user programs. We address this gap by introducing our own
concurrency-aware fuzzer in Section IV.

The last

two categories concern the fuzzing techniques
themselves. Speciﬁcally, the general fuzzing strategy in Algo-
rithm 1 requires some adaptations to produce good results on
concurrent programs. First and foremost, a mechanism to force
the execution of a large number of different interleaving is re-
quired (Interleaving Control). Existing fuzzers like MUZZ [23]
and ConAFL [34] manipulate the thread priorities at assembly
level, others like Krace [44] inject sleep instruction to force
a context switch, while AutoInter-fuzzing [42] and Conzzer
[45] instrument the code with explicit synchronization barriers
or thread locks. Alternatively, the interleaving exploration can
be left to the natural non-determinism of the operating system
like in ConFuzz [43]. Lastly, some authors propose to change
the feedback to the input mutation engine in an attempt
to guide the fuzzer towards more interesting interleavings
(Mutation Feedback). We mark such attempts as Thread-Aware
as opposed to the default Branch Coverage metrics used in
sequential fuzzing. We provide more information on these
state-of-the-art fuzzers in Section VII-B.

In general, BMC and GBF tackle the problem of ﬁnding
vulnerabilities in fundamentally different ways. Consequently,
it is natural to ask whether combining the two techniques can
lead to better coverage of the search space. More precisely, in
this study, we ask the following research question:

Research Question. Does an ensemble of bounded model
checkers and gray-box fuzzers discover more concurrency
vulnerabilities and do it faster than either approach on their
own?

In addressing this question, we are confronted with many
practical design challenges, the solution of which is central in
the remainder of our paper:

• Concurrency-aware gray-box fuzzer. As detailed in
Section II-C, there are some recent existing efforts to
fuzz concurrent programs, but no mature open-source
tool exists. Consequently, designing such a tool is an
important step towards answering our research question.
In doing so, we aim to draw from the lessons learned
in the literature and implement OpenGBF, a tool that is
representative of state-of-the-art concurrency-aware GBF.
We do so in Section IV.

• Aggregating BMC and GBF results. By creating an
ensemble of different tools, we run into the risk of them
returning conﬂicting results. The main reason is that BMC
relies on abstractions of the program execution states and
symbolic execution (see Section II-B), whereas GBF tests
concrete inputs and execution schedules. When the two
approaches disagree, we have an opportunity to make
an informed choice about the veriﬁcation outcome. We
propose to do so via a decision matrix, as detailed in
Section V.

• Resource allocation trade-off. The main drawback of
using an ensemble of different
they all
compete for the same computational resources. We must
choose how many resources to allocate to each tool for

tools is that

Fuzzer Name
OpenGBF
MUZZ [23]
ConAFL [34]
AutoInter-fuzzing [42]
ConFuzz [43]
Krace [44]
Conzzer [45]

Scope
User Space
User Space
User Space
User Space
User Space
Kernel Space
Kernel Space

Vulnerabilities
Multiple
Multiple
Invalid Mem. Acc.
Multiple
Multiple
Data Races
Data Races

Open Src.
Yes
No
Partial
No
No
Yes
No

Interleaving Control Mutation Feedback

Delay Injection
Thread Priority
Thread Priority
Barrier/Lock
None
Delay Injection
Barrier/Lock

Branch Coverage
Thread-Aware
Branch Coverage
Thread-Aware
Thread-Aware
Thread-Aware
Thread-Aware

TABLE I: Taxonomy of existing state-of-the-art concurrency-aware gray-box fuzzers.

applications with limited time, memory, or computational
power. In general, our decisions depend not only on the
problem at hand but also on the partial results we obtain
from the tools in the ensemble. We discuss strategies to
optimize our ensembles in Section V.

Note that the design challenges listed above are not orthog-
onal. We clarify when our choices impact multiple of them
in Sections IV and V. Furthermore, we mention reasonable
alternatives; these are left as future work.

IV. DESIGNING A STATE-OF-THE-ART
CONCURRENCY GRAY-BOX FUZZER

This section describes the main design challenges we ad-
dress in implementing our concurrency-aware gray-box fuzzer
OpenGBF. Namely, we discuss how we control the thread
interleavings (Section IV-B) and how we generate witness
information when a violation is found (Section IV-C). Both of
these goals require the instrumentation of the PUT as detailed
in Sections IV-A and IV-D.

Note that our GBF is based on established techniques:
fuzzer-controlled delay injection to force interleaving explo-
ration and branch coverage to guide the fuzzer mutation
engine (see Table I). At the same time, we believe that our
design is worth reporting for two reasons. On the one hand,
our GBF is the only user-space concurrency fuzzer that is
currently available as fully open-source software; thus the
present section is a useful reference for future users. On the
other hand, our GBF is a transparent effort to reproduce the
claims of the existing literature, which are currently impossible
to conﬁrm given the lack of open-source codebases.

A. Custom LLVM Pass Instrumentation

We build our concurrency-aware fuzzer on top of the widely
used gray-box fuzzer AFL++ [46], which is designed to ﬁnd
vulnerabilities in sequential programs. AFL++ minimizes the
fuzzing overhead by instrumenting the PUT via an LLVM pass
[47]. The LLVM pass is an essential framework of the LLVM
compiler. It works with the program translated into the LLVM
intermediate representation (IR) language and adds additional
code to monitor the program behavior [48].

We combine the standard LLVM pass of AFL++ with our
custom independent LLVM pass to make our fuzzer aware of
concurrent execution (see Algorithm 2). More speciﬁcally, we
inject ﬁve different function calls: a delay function (see line 4),
two thread-monitoring functions (see lines 6 and 8) and two

information-collecting functions (see lines 10 and 12). The ﬁrst
function controls the interleaving schedule, and we explain
its implementation details in Section IV-B. The second and
the third functions monitor the number of active threads (see
Section IV-C) in the PUT during run-time by tracking when
the functions pthread create and pthread join are called. The
last two functions record the information required to generate
a witness ﬁle containing the execution trace. We present a full
example of instrumented code in Section IV-D.

We bundle these ﬁve instrumentation functions in a run-
time library. We compile and link both the runtime library and
the instrumented PUT using the AFL++ clang wrapper. The
resulting executable can be fuzzed to detect reachability and
memory corruption bugs in the default setting. Optionally, the
ThreadSanitizer ﬂag can be enabled for ﬁnding concurrency
bugs.

B. Controlling the thread interleaving

As previously mentioned in Section I, our main algorithmic
idea is to introduce random delays in the PUT to force context
switches between threads. However, there are several major
corner cases that OpenGBF needs to take care of.

First, if the program features many active threads, we need
to limit their number during the PUT execution. Limiting the
number of threads is an unfortunate but necessary approxima-
tion of the PUT run-time behavior. Increasing the number of
active threads slows down the PUT execution and consumes
more compute resources during fuzzing. Furthermore, the PUT
may attempt to create an “inﬁnite” number of threads, which
can either be an undeﬁned behavior or just undecidable to
solve. We limit the number of threads by assuming that inter-
leavings that create more threads than a pre-deﬁned threshold
are safe and start a new run with different interleavings. We
discuss the effect of different threshold values on the bug-
ﬁnding capabilities of our fuzzer in Section VI-B5.

Secondly, deadlocks in the PUT may cause the current
interleaving to be stuck during execution. To avoid this prob-
lem, we force the fuzzer to terminate non-deterministically by
introducing a probability p of exiting at every instruction.

Thirdly,

in EBF we

a mechanism for
deﬁning atomic blocks
(via EBF atomic begin and
EBF atomic end functions) within the PUT. They can be
used to ensure that all instructions inside these blocks are

provide

Algorithm 2 LLVM Pass Instrumentation
Input: P U T – program under test.
Output: M – instrumented program.
Shorthands:
λd
λa
λj
λe
λl

delay f unction();
−
pthread add();
−
pthread release();
−
EBF add store pointer();
−
EBF alloca();
−
1: M
P U T
2: for all Function F
3:
4:

for Instruction I in F do

P U T do

←

∈

←

instrument (λd, I, M )

insert
M
{
a call to delay f unction() (Algorithm 3) after each
instruction to run a delay at run-time
}
if I == pthread create() then
instrument (λa, I, M )

to
M
pthread add() (Algorithm 4) to increase the ac-
tive threads counter at run-time
}
else if I == pthread join() then

insert a call

←

{

←

instrument (λj, I, M )

to
M
pthread release() (Algorithm 5) to decrease the
active threads counter at run-time
}

insert a call
{

else if I is DECLARATION then
instrument (λl, I, M )

←

to
M
EBF alloca() function (Algorithm 6) to record
a pair of the name and address of the variable
declaration.
}

insert a call

{

else if I is STORE then

←

instrument (λe, I, M )

to
M
EBF add store pointer() (Algorithm 7) func-
tion to record the assignment information for wit-
ness generation
}

insert a call

{

5:

6:

7:
8:

9:
10:

11:
12:

13:

end if
end for

14:
15: end for
16: return M

executed atomically3. To this end, our delay function will
force all other threads to wait until the atomic block has
ﬁnished. We achieve this by initializing a global mutex (i.e.,
EBF mutex) which the active thread can lock. If the global
mutex is locked and the current interleaving does not own
the global mutex, then we wait for the mutex owner to ﬁnish
its execution. Additionally, EBF will not insert delays inside
the atomic blocks (thus, improving the performance of the
instrumented program) since no thread interleavings can take
place within these blocks.

Finally, we force different interleavings by changing the
amount of delay (in milliseconds) inserted after each instruc-
tion. The delay values are drawn uniformly at random from
a pre-set range. More speciﬁcally, we let AFL++ produce a

3This is useful since not all versions of C language provide the means for

deﬁning atomic instructions.

Algorithm 3 Function delay f unction()
Global: TT – thread threshold, TN – number of threads
thread,
running, p – probability of exiting, TC – current
EBF mutex – global mutex.

1: Function delay function()
2: if TN > TT or Bernoulli(p) == 1 then
exit
3:
4: end if
5: if TC == EBF mutex then
6:
7:
8: end if
9: φ

run instruction
return

wait f or timeout

exit this analysis normally
}
{

run the current instruction
}
{

wait until EBF mutex is

{

←
released
}
10: if φ is timeout then
exit
11:
12: end if
13: sleep(
∗
14: EndFunction

)

exit this analysis normally
}
{

run a delay for * nanoseconds
{

}

seed value for the random number generator providing the
delay values. We explore the impact of different delay ranges
on the bug-ﬁnding ability of our fuzzer in Section VI-B5.

The above design choices have been incorporated into the
implementation of function delay f unction, whose deﬁnition
is illustrated in Algorithm 3. In lines 2-4 we implement our
strategy for limiting the number of active threads. If the
number of active threads TN is greater than the given threshold
TT or we extract 1 from a Bernoulli distribution with success
probability p,
then the fuzzer exits this analysis normally
(see line 3) and starts a new run with different interleavings
(i.e., different delay values). Otherwise, we check whether the
current thread owns the global mutex (line 5), and if so, we
let it ﬁnish its execution and release the mutex (lines 6 and 7).
If the global mutex is not released before the timeout (line 9),
we also allow the fuzzer to exit this analysis normally (line
11). This is done to prevent deadlocks if the global mutex
is never released. Finally, the delay is executed by running a
sleep function for the duration value produced by the fuzzing
engine (line 13).
Additionally,

in the
PUT is monitored by the functions pthread add and
pthread release, whose deﬁnitions are shown in Algorithms
4 and 5, respectively. The former (the latter) increments (decre-
ments) the active threads counter TN (see line 3) atomically
by locking the current thread (see line 2) before changing the
value of TN and unlocking it afterwards (see line 4).

the number of

threads

active

C. Witness Generation

If OpenGBF ﬁnds a violation, we need to support
the
users and tools in reproducing the identiﬁed bug. To do
so, we generate a crash report ﬁle with all the necessary
information to reproduce the property violation. We use func-
tions EBF alloca and EBF add store pointer to record
all the information needed for automated witness generation:

Algorithm 4 Function pthread add()
Global: M utex lock,
-
counter.

TN

active

threads

1: Function pthread add()
2: lock thread
3: TN ++
4: unlock thread
5: EndFunction

←

←

M utex lock

M utex lock

Algorithm 5 Function pthread release()
Global:
counter.

M utex lock,

TN

-

active

thread

1: Function pthread release()
2: lock thread
M utex lock
3: TN - -
4: unlock thread
5: EndFunction

←

←

M utex lock

assumption values, thread ID, variable names, and function
names as shown in Algorithms 6 and 7.

As the fuzzing process begins, we run an initialization
function before the main method is called in the PUT. This
function creates a witness ﬁle uniquely identiﬁed by the
current process ID (i.e., witnessInf oAF Lpid) and sets the
environment (i.e., initializes the global mutex EBF mutex,
getting process id (pid)). Then, our custom LLVM pass inserts
a function call to EBF alloca (see line 10 in Algorithm 2) after
each declaration instruction in the PUT, and a function call to
EBF pointer add store pointer (see line 12 in Algorithm 2)
after each loading store instruction.

Algorithms 6 and 7 demonstrate the deﬁnitions of functions
EBF alloca and EBF add store pointer,
respectively.
The former records the declared variable’s name, its address,
and the name of the function where it has been declared in
the PUT (see line 3). The latter records the assigned variable’s
address, the assigned value, the name of the function and the
line of code where the assignment takes place in the PUT
(see line 3). Both functions record information atomically –
the thread is locked (see line 2) before the writing occurs and
unlocked afterwards (see line 4). If the fuzzing run ﬁnishes
normally (i.e., a timeout is reached or the process ﬁnishes
with the exit code 0), we delete the created witness ﬁles in a

Algorithm 6 Function EBF alloca()
Inputs: a – variable name, f – function name, &a – variable
address.
Global: M utex lock, witnessInf oAF Lpid – witness ﬁle for
the process with ID = pid.

1: Function EBF alloca(a, f, &a)
2: lock thread
3: witnessInf oAF Lpid
4: unlock thread
5: EndFunction

←
M utex lock

M utex lock

←

←

write(a, f, &a)

Algorithm 7 Function EBF add store pointer()
Inputs: &a - variable address, l - line number in the code, f
- function name, v - variable value.
Global: M utex lock, witnessInf oAF Lpid – witness ﬁle for
the process with ID = pid.

1: Function EBF add store pointer(&a, l, f, v)
2: lock thread
M utex lock
3: witnessInf oAF Lpid
4: unlock thread
5: EndFunction

←
M utex lock

write(&a, l, f, v)

←

←

destructor function [49]. If the fuzzer causes a crash in one of
the PUT executions, we save the ID of the process that has
crashed and generate a crash report by extracting the data from
the witness ﬁle associated with this process ID. The resulting
crash report contains the exact sequence of operations (i.e.,
memory accesses) that led to the PUT’s crash (see Appendix
B for more details).

D. Full illustrative Example

To tie all

these design choices together, we present an
illustrative example. Assume that we have a concurrent PUT
that has one reachability bug, as illustrated in Listing 1. The
program contains two threads calling the same function foo
(see line 3), which contains a loop of 5 iterations. At the end
of the execution, the value of a should be 10: Line 18 consists
of a conditional statement that checks whether this is not the
case, and reports an error (property violation). This error can
only be reached when the reads and writes over a are not
correctly synchronized between the two threads.

Figure 2a illustrates an interleaving that causes a violation:
thread 1 (T1) reads the variable a = 0 which is initialized to
0 in line 2, then thread two (T2) reads a = 0 before T1 writes
a = 1 (see line 7). This pattern repeats until the end, when
the value of a will be 5 rather than the expected 10.

Ideally, each thread will read the content of variable a and
increment it without interference from the other thread. Figure
2b, illustrates an interleaving scenario where the two threads
are synchronized: thread 1 (T1) reads a = 0 and writes the
new value a = 1, then thread two (T2) reads the updated value
a = 1 and increments it to a = 2. This pattern yields a ﬁnal
value of a = 10, which makes the property hold.

When we verify the code in Listing 1 with EBF, we ﬁnd
two different bugs. Namely, OpenGBF reports the data race,
whereas all BMC tools we tested report a reachability bug at
Line 18.

Let us now present an example of our instrumentation.
Recall that we instrument the PUT at the LLVM-IR level. For
our example of Listing 1, we report the LLVM-IR encoding
for function foo in Listing 2. Listing 3 illustrates the IR after
the instrumentation. In lines 7 and 20, we call the function
EBF add store pointer, which is inserted after each load
instruction and saves both the variable name and its value in
a ﬁle to use it for generating the witness ﬁle. In lines 10,13
and 16, we call a function called EBF alloca, which stores the

metadata of any variable declared in the PUT. This information
is also used to generate the witness ﬁle. In lines 11,14,17 and
22, we inserting a function call to the delay function(), as
we describe in Algorithm 3.

Listing 1: Original multi-threaded C code

1 void reach_error() { assert(0);}
2 int a=0; //shared variable
3 void* foo(void* arg) {
4

int tmp, i=1;
while (i<=5) {
tmp = a;
a = tmp + 1;
i++;
}
return 0;

10
11 }
12 int main () {
13

pthread_t t1, t2;
pthread_create(&t1, 0, foo, 0);
pthread_create(&t2, 0, foo, 0);
pthread_join(t1, 0);
pthread_join(t2, 0);
if ((a) != 10) reach_error();

Listing 2: Fragment of the corresponding IR before instru-
mentation

1 define dso_local i8* @foo(i8* %0) #0 {
2

%2 = alloca i8*, align 8
%3 = alloca i32, align 4
%4 = alloca i32, align 4
store i8* %0, i8** %2, align 8
store i32 1, i32* %4, align 4
br label %5
%6 = load i32, i32* %4, align 4
%7 = icmp sle i32 %6, 5
br i1 %7, label %8, label %14
%9 = load i32, i32* @a, align 4
store i32 %9, i32* %3, align 4
%10 = load i32, i32* %3, align 4
%11 = add nsw i32 %10, 1
store i32 %11, i32* @a, align 4
%12 = load i32, i32* %4, align 4
%13 = add nsw i32 %12, 1
store i32 %13, i32* %4, align 4
br label %5
ret i8* null

5

6

7

8

9

14

15

16

17

18
19 }

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20
21 }

Listing 3: Fragment of the corresponding IR after instrumen-
tation

1 define dso_local i8* @foo(i8* %0) #0 {
2

%2 = alloca i8*, align 8
%3 = alloca i32, align 4
%4 = alloca i32, align 4
%5 = bitcast i8* %0 to i1*
%6 = bitcast i8** %2 to i8*
call void @EBF_add_store_pointer(i8* %6, i64 0, ←(cid:45)
i8* getelementptr inbounds ([4 x i8], [4 x ←(cid:45)
i8]* @0, i32 0, i32 0), i1* %5)

store i8* %0, i8** %2, align 8
%7 = bitcast i8** %2 to i8*
call void @EBF_alloca(i8* getelementptr inbounds←(cid:45)
([4 x i8], [4 x i8]* @1, i32 0, i32 0), i8*←(cid:45)
getelementptr inbounds ([4 x i8], [4 x i8]*←(cid:45)
@2, i32 0, i32 0), i8* %7),

call void @_delay_function()
%8 = bitcast i32* %3 to i8*

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

call void @EBF_alloca(i8* getelementptr inbounds←(cid:45)
([4 x i8], [4 x i8]* @3, i32 0, i32 0), i8*←(cid:45)
getelementptr inbounds ([4 x i8], [4 x i8]*←(cid:45)
@4, i32 0, i32 0), i8* %8),

call void @_delay_function()
%9 = bitcast i32* %4 to i8*
call void @EBF_alloca(i8* getelementptr inbounds←(cid:45)
([2 x i8], [2 x i8]* @5, i32 0, i32 0), i8*←(cid:45)
getelementptr inbounds ([4 x i8], [4 x i8]*←(cid:45)
@6, i32 0, i32 0), i8* %9),

call void @_delay_function()
%10 = sext i32 1 to i64
%11 = bitcast i32* %4 to i8*
call void @EBF_add_store_pointer(i8* %11, i64 6,←(cid:45)
i8* getelementptr inbounds ([4 x i8], [4 x ←(cid:45)

i8]* @7, i32 0, i32 0), i64 %10),

store i32 1, i32* %4, align 4,
call void @_delay_function(),

V. EBF: ENSEMBLES OF BMC AND FUZZING
Thanks to our OpenGBF, we now have access to both state-
of-the-art BMC and GBF tools. This section explains how we
combine them in EBF and maximize their effectiveness in
ﬁnding vulnerabilities in concurrent software.

We remark that types of software vulnerabilities that can
be detected by EBF solely depend on the capabilities of
each individual
tool used in the ensemble. For example,
most BMC tools can detect most types of illegal memory
accesses (e.g., buffer overﬂows, use-after-free, invalid pointer
dereference) and memory leaks, as well as some BMC tools,
can detect concurrency bugs (i.e., thread leaks, data races,
and deadlocks). Regarding OpenGBF, its main function is
exploring different executions of the program by sampling
different thread schedules and different program inputs. In
order to evaluate whether each such execution leads to a
bug, OpenGBF relies on the bug-detecting capabilities of
sanitizers. They perform some additional instrumentation to
the PUT, making it crash when a vulnerability has been
detected. Namely, AddressSanitizer [50]is capable of identify-
ing memory-related vulnerabilities, while ThreadSanitizer [51]
can identify concurrency bugs.

We present a high-level overview of the structure of our
ensembles in Figure 3. Overall, the ensemble executes both
BMC and GBF tools on the PUT. The execution of these two
tools is not fully independent like it would be in a portfolio
[27], [28]. In fact, the result of the BMC tool run can be used
to seed the GBF tool under speciﬁc conditions. We elaborate
on this in Section V-A.

Furthermore, our ensemble structure in Figure 3 requires
addressing the two main challenges outlined in Section III.
First, the results of the BMC and GBF runs must be aggregated
in a coherent assessment of the safety of the PUT. Second, the
BMC and GBF tools in the ensemble compete for the same
computational resources, which might reduce the ability of
each tool to ﬁnd violations. We present our solution to these
challenges in Sections V-B and V-C.

A. Seeding

Before discussing our solution to the two problems of
aggregation and resource sharing, let us propose a further opti-

(a) Mis-synchronized memory accesses.

(b) Synchronized memory accesses.

Fig. 2: Visualization of the memory accesses on variable a in Listing
1 for two different interleavings. In Figure 2a, the accesses are not synchronized: both T1 and T2 read a before
simultaneously incrementing it to a = 1. This pattern continues until the end, where the ﬁnal value will be a = 5.
Conversely, Figure 2b depicts synchronized accesses: T1 reads a and increments it to a = 1, then T2 reads a and increments
it to a = 2. In the end, the ﬁnal value will be a = 10.

other hand, this allows OpenGBF to explore other randomly
generated schedules that may lead to other bugs.

However,

if the BMC engine timeouts, proves (partial)
correctness or produces Unknown, then we generate the fuzzer
seed with pseudo-random integer numbers ranging between
0 to 5000. We determined experimentally that
this range
provides a good trade-off between functionality and efﬁciency
since larger numbers (e.g., more than 5000) lead the fuzzer
to generate a lot of inputs that do not result in triggering a
bug. Note that these values are not directly used inside the
delay function. The delays are produced by a different random
number generator that is seeded from one of the inputs given
by the fuzzer.

B. Aggregation

After running all ensemble members, we need to aggregate
their outcomes. This is especially challenging since BMC and
GBF may disagree on the safety of the PUT (cf. Section III).
We summarize our aggregation rule in the decision matrix in
Table II. Some decisions are straightforward: we must trust
the other when either method cannot conclude. Accordingly,
when GBF reports Unknown, our decision matrix aligns with
the outcome of BMC. Vice versa, when BMC cannot prove or
disprove the PUT’s safety, we trust the bugs found by GBF. A
more interesting scenario happens when there is a conﬂict in
the ensemble: BMC can declare a PUT as Safe, but GBF may
still be able to ﬁnd a Bug. In general, we report such instances
as Conﬂict. This can be caused by the over-approximations in
the computational models used by the BMC tool or by the
bugs from the code instrumentations introduced by the GBF
tool. Each such Conﬂict may be resolved by analyzing the
witness ﬁle produced by the GBF.

We remark that

the decision matrix proposed in Table
II was motivated by the SV-COMP [52] competition rules

Fig. 3: High-level overview of EBF.

mization of the ensemble. Speciﬁcally, if sequential execution
of the ensemble is possible, we can improve the GBF seeds by
initializing them with the counterexample produced by BMC.
This is only possible when the BMC tool reports a failed
veriﬁcation outcome. These seeds are concrete values that
cause an assertion to fail. It is important to mention that despite
the BMC tools providing thread scheduling information in
their bug reports, it is not always straightforward which delay
values must be applied for replicating these bug-inducing
schedules. In general, it is impossible to guarantee a particular
thread order by only injecting time delays and without an
explicit scheduling algorithm or another locking technique.
This is because the effect of the introduced delays on the
thread order depends on the implementation of multi-threading
in the corresponding operating system and its current workload
(e.g., sometimes,
the same delay values may lead to the
execution of different thread schedules). As a result, OpenGBF
uses only the bug-inducing inputs and not the thread schedule
information as the seed. On the one hand, this means that
OpenGBF cannot reproduce every bug produced by the BMC
tool since it might not be able to sample the sequence of
delays replicating the bug-inducing thread schedule. On the

R(a)a=0W(a)a=1R(a)a=1W(a)a=2R(a)a=2W(a)a=3R(a)a=3W(a)a=4R(a)a=4W(a)a=5T1T2R(a)a=0W(a)a=1R(a)a=1W(a)a=2R(a)a=2W(a)a=3R(a)a=3W(a)a=4R(a)a=4W(a)a=5timeR(a)n=0W(a)a=1R(a)a=2W(a)a=3R(a)a=4W(a)a=5R(a)a=6W(a)a=7R(a)a=8W(a)a=9T1T2R(n)n=1W(n)n=2R(n)n=3W(n)n=4R(n)n=5W(n)n=6R(n)n=7W(n)n=8R(n)n=9W(n)n=10timeBMCGBFPUTSeedVerdictBugTraceAggregationGBF

EBF

Bug
Conﬂict
Unsafe
Unknown Unsafe Unknown

Unknown
Safe
Unsafe

Safe
Bug

C
M
B

Veriﬁcation
outcome
Correct True
Correct False
Correct False Unconﬁrmed
Incorrect True
Incorrect False
Unknown
Overall SV-COMP 2022 score

Tool
EBF 2.3 CBMC

139
234
55
0
1
334
496

148
212
90
0
3
310
460

Score per
benchmark
2
1
0
-32
-16
0

×
×
×
×
×
×

TABLE II: EBF declares a program Safe, Unknown, Unsafe
or reports a Conﬂict

by aggregating the outputs of BMC and GBF.

TABLE III: The results demonstrated by EBF 2.3 and CBMC
5.43 in the Concurrency Safety category of SV-COMP 2022.

where interactive veriﬁcation is not available, while incorrect
answers are punished by deducting competition points (see
Section VI-B3 for more details). However, veriﬁcation of more
complex software systems can beneﬁt from a more descriptive
decision matrix. For example, it may be useful to distinguish
between different Unsafe outcomes in Table II.

C. CPU time allocation

CPU time allocation is another important design choice
in optimizing the performance of EBF. More speciﬁcally,
we need to split the available CPU time between the two
components of the ensemble in order to increase the search
space coverage as much as possible for each of them. In
Section VI-B5, we discuss how different CPU time distribution
strategies affect the overall EBF performance.

VI. EXPERIMENTAL EVALUATION

In this section, we demonstrate the effectiveness of BMC
and GBF ensembles in a diverse set of scenarios. We will reit-
erate our experimental objectives before detailing the deployed
benchmarks and our results.

A. Objectives

The present experimental evaluation has the following goals:

EG1 - Detection of violations in concurrent programs

Demonstrate that EBF can detect more violations in
concurrent programs than state-of-the-art BMC tools on
their own.

EG2 - Real-world performance of OpenGBF

Demonstrate that the concurrency-aware GBF we imple-
ment in EBF can ﬁnd violations in real-world programs.
EG3 - Parameter trade-offs in our concurrency-aware

fuzzer
Demonstrate that EBF produces consistent results across
a wide range of parameter settings.

Note that the latter two objectives EG2 and EG3 are ori-
ented towards demonstrating that OpenGBF (see Section IV) is
representative of state-of-the-art gray-box fuzzing techniques.

B. Results

We gathered our experimental results over a substantial
period, beginning in February 2021. During this period, the
design of EBF has evolved and improved. To avoid confusion,
we report our results separately for each version of EBF.

Namely, we start with the participation of EBF 2.3 in the
Concurrency Safety category of SV-COMP 2022 (see Sec-
tion VI-B1). This EBF version was based on CBMC v5.43
and a more rudimentary implementation of our concurrency-
aware fuzzer. For comparison, we also report the performance
of our latest version EBF 4.0 on the same set of benchmarks
(see Section VI-B2). EBF 4.0 includes the full implementation
of OpenGBF described in Section IV, and a large number of
different BMC tools. Then, we demonstrate the ability of our
fuzzer to ﬁnd a data race in the wolffMQTT cryptographic
library (see Section VI-B3). Historically, we ﬁrst found this
bug in February 2021 with an earlier version of our fuzzer.
Here, we repeat our previous experiment with the latest version
of OpenGBF included in EBF 4.0. Finally, we run an extensive
comparison of the performance of EBF 4.0 across a wide range
of parameter settings (see Section VI-B5).

1) EBF 2.3 participation in SV-COMP 2022: EBF 2.3 took
part in SV-COMP 2022 in the Concurrency Safety category
[53]. This category features a set of 763 concurrent C pro-
grams, 398 of which are safe. The bugs in the remaining 365
programs are formulated in terms of reachability conditions:
the program is deemed unsafe if a predeﬁned error function
is reachable within the given program, and safe otherwise.
These programs contain a number of intrinsic functions [52].
We explain how we model them in Appendix A.

In the SV-COMP 2022 Concurrency Safety category, each
participating tool is asked to produce one of the following six
veriﬁcation outcomes for a given concurrent benchmark (see
the ﬁrst column in Table III):

• Correct True. The tool correctly conﬁrms that the pro-

gram is safe.

• Correct False. The tool correctly conﬁrms the presence

of a bug.

• Correct False Unconﬁrmed. The tool correctly conﬁrms
the presence of a bug, but the associated counterexam-
ple cannot be reproduced by the witness validator tool
developed by the competition organizers.

• Incorrect True. The tool conﬁrms that a program is safe

when it contains a bug.

• Incorrect False. The tool conﬁrms that
contains a bug when it is, in fact, safe.

the program

• Unknown. The tool cannot conclude within the given

CPU time and memory limit.

Every veriﬁcation outcome is assigned a score value (see

the fourth column in Table III), which strongly discourages
incorrect results. The resulting score for each tool is comprised
of the sum of the scores obtained for all benchmarks.

The competition took place on the SV-COMP servers fea-
turing 8 CPUs (Intel Xeon E3-1230 v5 @ 3.40 GHz) and 33
GB of RAM. Each benchmark veriﬁcation task was limited to
15 minutes of CPU time and 15 GB of RAM.

The version of our tool that we submitted to the competition,
EBF 2.3, is based on CBMC v5.43 as a BMC engine and
an earlier implementation of OpenGBF. Namely, we selected
CBMC as it is a state-of-the-art BMC tool that has consistently
been achieving high rankings in the concurrency category of
SV-COMP over the past decade. Also, the implementation of
OpenGBF we used in EBF 2.3 was more rudimentary. Namely,
it had no limit on the number of threads, no probability of
terminating early, and no mechanism to avoid injecting delays
inside atomic blocks.

EBF 2.3 reached 7th place out of 20 participants in SV-
COMP 2022, by scoring a total of 496 points. Crucially,
EBF 2.3 outperformed CBMC 5.43, which ﬁnished 10th with
460 points. We report the ofﬁcial SV-COMP 2022 results of
these two tools in Table III. Note that CBMC achieved a
higher score than EBF in predicting programs safety (148 vs
139, respectively). This is an expected outcome, since EBF
dedicates only 6 minutes out of 15 minutes to BMC, and the
rest are used by OpenGBF, which cannot prove whether a
program is safe. At the same time, EBF was better than CBMC
at detecting bugs that could be conﬁrmed by the witness
validator (234 vs 212), thus scoring extra points.

Moreover, EBF reported only one Incorrect False outcome,
while CBMC produced 3 incorrect verdicts resulting in 48
penalty points. Interestingly, EBF avoided reproducing the
latter three incorrect outcomes (returning Unknown instead)
since CBMC did not have enough time to wrongly detect these
bugs running as a part of the ensemble (reporting Unknown
as the result), while OpenGBF also could not ﬁnd any bugs in
these benchmarks within the remaining time (hence, another
set of Unknown’s). In contrast, the only incorrect outcome
(different from the three false positives obtained by CBMC)
produced by EBF was caused by a bug inside OpenGBF,
which made it generate a spurious counterexample. This issue
has been resolved in our most recent EBF 4.0 version.

∼

Overall, EBF 2.3 improved the competition result of CBMC
7.8%. In addition, EBF could detect and conﬁrm
5.43 by
a property violation in one benchmark, which could not be
detected by any other dynamic tool in the competition. These
results are a ﬁrst positive answer to goal EG1; we present
further experimental evidence in Section VI-B2.

2) EBF 4.0 with different state-of-the-art BMC tools: After
the participation of EBF in SV-COMP 2022, we improved
OpenGBF following the algorithmic ideas we describe in
Section IV. Here, we present the results of further experiments
that test whether any BMC tool can be improved by adding
our latest version of OpenGBF on top of it (goal EG1). To
avoid confusion, we refer to the latest implementation of our
ensemble technique as EBF 4.0.

In our evaluation, we run EBF 4.0 over the same bench-
marks from the SV-COMP 2022 Concurrency Safety category
(see Section VI-B1). However, we omit the SV-COMP aggre-
gate scoring system (see Table III), since its different weights
can obfuscate the advantages of each veriﬁcation technique.
Instead, we focus on analyzing the trade-off between proving
safety4 (BMC only) and bug-ﬁnding abilities (both BMC and
GBF) from the raw results.

Furthermore, we consider three additional BMC tools in
our experiments (see Table IV), rather than just CBMC [38].
Namely, ESBMC [37] is a powerful BMC tool that has been
successfully participating in SV-COMP over the past decade.
Similarly, Deagle [54] and Cseq [39] achieved 1st and 2nd
place, respectively, in the Concurrency Safety category at SV-
COMP 2022.

We conduct all our experiments on a virtual machine
running Ubuntu 20.04 LTS with 160 GB RAM and 25 CPU
cores of Intel Core Processor (Broadwell, IBRS) @ 2.1 GHz.
Moreover, we run EBF 4.0 with the following parameters:
to
maximum thread threshold 5, delay range from 0 [µs]
105 [µs]. Additionally, we distribute the available runtime in
the following way: we allocate 6 minutes to the BMC engine,
5 minutes to OpenGBF, and 4 minutes for the seeding, aggre-
gation, and witness ﬁle generation. These parameter setting is
optimal for the SV-COMP 2022 benchmark we are using, as
we discuss in Section VI-B5. Note that the user can specify
the time distribution between the tools in the ensemble in EBF
via command-line arguments.

∼

∼

Table IV reports a pair-wise comparison between EBF 4.0
and the four different BMC tools on their own. The results
demonstrate that EBF ﬁnds more bugs than all four BMC
engines on their own while reducing the number of Unknown
instances. More in detail, EBF achieves the best improvement
concerning ESBMC, by ﬁnding
14.9% more bugs and
correcting one wrong outcome while reducing the number of
safety proofs by only
7.6%. Similarly, the ability to double-
check any counterexample produced by BMC allows EBF
to correct all three erroneous outcomes produced by CBMC
while showing a marginal difference between the improvement
in bug-ﬁnding (
5.6%) and the degradation in safety proof
5%). In contrast, when compared to Deagle, EBF shows
(
∼
no decrease in the Correct True outcomes, but can increase
5.3%. As for Cseq, the
the number of discovered bugs by
number of safety proofs produced by EBF declines by only
2.9%, while the number of Correct False results rises by
6.3%.
Overall, EBF provides a better trade-off between bug-
ﬁnding and safety proving than each BMC engine. On average,
EBF ﬁnds over 8% more concurrency bugs while reducing the
number of programs declared safe by only 3.8%. Hence, this
evaluation achieves our ﬁrst experimental goal (EG1).

∼
∼

∼

∼

3) Detecting a data race in wolfMQTT: We evaluate EBF
4.0 on the wolfMQTT library [55]. MQTT (Message Queuing

4The term “prove safety” means that the BMC procedure could verify all
reachable states and could not ﬁnd an execution path that violates the safety
property.

Veriﬁcation
outcome
Correct True
Correct False
Incorrect True
Incorrect False
Unknown

Tool

EBF Deagle
240
336
0
0
187

240
319
0
0
204

EBF Cseq
177
172
333
313
0
0
0
0
258
273

EBF
65
308
0
0
390

ESBMC
70
268
0
1
424

EBF CBMC
139
320
0
0
304

146
303
0
3
311

TABLE IV: Pair-wise comparison of the veriﬁcation outcomes for EBF 4.0 with different BMC tools “plugged in” against
their individual performance on the benchmarks from the Concurrency Safety category of SV-COMP 2022.

Telemetry Transport) is a lightweight messaging protocol
developed for constrained environments like the Internet of
Things (IoT). It employs the publish-subscribe messaging
pattern of publishing messages and subscribing to topics. The
wolfMQTT library is a client implementation of the MQTT
protocol written in C for embedded devices. We use its API
to verify the concurrent part of the protocol implementation.
OpenGBF detects a data race5 in wolfMQTT after running
for 15 minutes and consuming 24 MB of RAM. In detail,
MQTT contains 4 packet structures (i.e., Connect, Publish,
Subscribe and Unsubscribe). The Subscribe function accepts
an acknowledgment from the server (i.e., broker). This ac-
knowledgment was received through an unprotected pointer
due to the data race detected in function MqttClient WaitType,
which can lead to an information leak or data corruption. This
issue has been successfully replicated and consequently ﬁxed6
by the wolfMQTT developers.

Our setup for the experiment

is the following. We run
EBF 4.0 on an Intel Core i7 2.7Ghz machine with 8 GB of
RAM running Ubuntu 18.04.5 LTS. We use a Mosquito server
for the communication with the wolfMQTT client [56]. We
enable ThreadSanitizer on top of OpenGBF for detecting the
concurrency bugs that are not formulated explicitly in terms
of reaching a predeﬁned error function (i.e., like it is done
in the SV-COMP 2022 concurrency benchmarks) or violating
a safety assertion. Finally, we run our fuzzer with a thread
threshold of T h = 5 and a delay range from 0 [µs] to 105 [µs].
Other tools fail to discover the same vulnerability. More
speciﬁcally, both bounded model checkers CBMC v5.43 and
ESBMC v6.8 are unable to detect the data race within the given
time limit. Moreover, the BMC tool Deagle v1.3 cannot parse
the program correctly, since it is using an outdated version
of C parser. Similarly, Cseq v3.0 does not support programs
featuring multiple source ﬁles. Finally, neither the fuzzer AFL
nor AFL++ can detect this bug in wolfMQTT.

As a result of this experiment, we can conclude that our

second evaluation goal (EG2) has been achieved.

4) Detecting memory violations in real world concurrent
programs: To show scalability and robustness of EBF, we
evaluate it on several real-world concurrent programs using
the same machine as in Section VI-B3. We consider three

5https://github.com/wolfSSL/wolfMQTT/issues/198
6https://github.com/wolfSSL/wolfMQTT/pull/209

multi-threaded real-world programs: pfscan [30], a multi-
threaded ﬁle scanner; bzip2smp [31], a parallel implementation
of bzip2 compressor; swarm1.1 [32], a library that provides a
framework for parallel programming on multi-core systems.
Table V presents the number of lines of code (LOC) for each
PUT, the number (NN ) and types of vulnerabilities detected
by EBF, and the median time EBF takes to ﬁnd these bugs.
We give more detailed information on the latter in Section
VI-B6.

Both tools in EBF detect a NULL pointer deference in
pfscan that is caused by a malloc instruction whose result
is not checked for successful memory allocation leading to a
crash due to writing to a NULL pointer. As for bzip2smp, EBF
ﬁnds two bugs. The BMC engine detects a vulnerability in the
BZ2_bzclose() function, which receives a pointer that can
be NULL. Meanwhile, OpenGBF ﬁnds a memory leak in the
writerThread() function of bzip2smp. Regarding swarm
1.1, EBF (in particular, the fuzzer) ﬁnds an invalid pointer
dereference caused by an incorrect thread initialization (i.e.,
calling the pthread_create function with a NULL pointer
as an argument).

5) Optimizing EBF’s settings:

In the following experi-
ments, we explore different settings for EBF and OpenGBF.
For the ﬁrst two experiments, we run EBF with the BMC
engine switched off, allowing the fuzzer to run for 11 minutes.
While for the third evaluation, we run EBF with both engines
enabled but with a different amount of time allotted (out of
total 11 minutes) to each of them.
Maximum number of threads in OpenGBF. Figure 4 shows
the result of choosing different values for the thread threshold
on the number of bugs (i.e., the number of Correct False
outcomes) discovered by OpenGBF. We ran this experiment
with the delay range from 0 [µs] to 105 [µs] and probability of
exiting p = 0.01%. It can be seen that the most optimal value
lies in the region around T h = 5, and raising the threshold
value leads to fewer bugs being detected due to the increase
in the number of computer resources required to maintain a
more signiﬁcant number of active threads. Perhaps, we can
suggest that many bugs can be discovered without considering
a large number of threads, which was also demonstrated by
the wolfMQTT data race that was discovered with T h = 5.
However, drawing a more robust conclusion applicable to any
concurrent program requires a more extensive evaluation of
our GBF on a larger set of benchmarks.

Real-world programs
wolfMQTT
pfscan

LOC NN
1
9.3k
1
1.1k

bzip2smp

swarm 1.1

5.3k

2.8k

2

1

NT
Data Race
Invalid pointer dereference
Invalid pointer dereference
Memory leak
Invalid pointer dereference

Median Time
361.7s
3.98s

10.6s

339.6s

ESBMC OpenGBF

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

TABLE V: Evaluation of EBF on real-world concurrent programs. For each program we present its size in terms of the number
of lines of code (LOC), the number of vulnerabilities detected by EBF (NN ), types of corresponding vulnerabilities (NT ), the
median time (in seconds) of 20 EBF re-runs, and which EBF engine (i.e., ESBMC or OpenGBF) detects the corresponding
vulnerability.

Fig. 4: Number of bugs (i.e., Correct False outcomes) discov-
ered by OpenGBF in EBF 4.0 for different values of threshold
on the maximum number of active threads.

Fig. 5: Number of bugs (i.e., Correct False outcomes) discov-
ered by OpenGBF in EBF 4.0 for different upper bounds of
the distributions for the random delay.

Maximum amount of delay in OpenGBF. Figure 5 il-
lustrates the effect of the amount of delay we insert
to
force scheduling in OpenGBF. We use a logarithmic scale
to compare different delay ranges in OpenGBF. Similar to
thread thresholds, we use the
the evaluations of different
number of Correct False to assess the efﬁcacy of a given delay
bound. We set the thread threshold to 5 active threads in this
experiment. We change the upper bound of the delay’s range
from 0 [µs] (i.e., no delay) to 107 [µs] (i.e., 10 seconds). The
results show that increasing the delay upper bound from 0
to 105 [µs] gradually improves the bug-ﬁnding capabilities of
OpenGBF from 68 to 88 benchmarks. When we set a large
upper bound on the delay value, we increase the time range for
a thread to stay inactive before it is rescheduled again, which
increases the number of threads interleavings that our GBF
explores. At the same time, choosing a larger upper bound
(e.g., 106 or 107) leads to a decrease in the number of bugs
found due to a higher number of timeouts. This is expected, as
with larger delays the fuzzer spends the majority of the time
waiting rather than executing the code. In general, we believe
that ﬁnding the correct trade-off in delay range is benchmark-
dependent.
Comparison with the “non-instrumenting” GBF. As a
sanity check, we compare our GBF implementation against

the “non-instrumenting” version of the fuzzer, which does not
feature the PUT instrumentation stage described in Algorithm
2. We run this experiment with the optimal set of parameters
reported above: i.e., T h = 5, p = 0.01% and the random delay
upper bound of 105 [µs]. The results show a nearly 50-fold
increase in the number of detected bugs. More speciﬁcally,
OpenGBF detects 88 out of 365 vulnerabilities (i.e., 24.2%
of the total), while the “non-instrumenting” GBF detects only
2 out of 365 (0.55%). This highlights the necessity of using
concurrency-aware fuzzers in our EBF.
CPU time allocation inside EBF. In this experiment we
compare different ways of distributing the total veriﬁcation
time (11 minutes overall) between the fuzzer and the BMC
engines in EBF. The results demonstrate a relatively wide
range of values (between 3 and 8 minutes per engine) within
which EBF 4.0 produces identical results detecting 320 bugs
out of 365. At the same time, when the entire 11 minutes
are allocated to the BMC engine, the number of detected bugs
drops by
5% to 303 out of 365. Conversely, the overall bug-
ﬁnding performance of EBF 4.0 decreases drastically by over
72.5% when all 11 minutes are devoted to OpenGBF. On the
whole, this result conﬁrms that BMC tools are better than our
GBF tool when used in isolation. However, combining them
both in an ensemble is going to yield better results across a

∼

101102103707580859095100100MaximumnumberofthreadsNumberofbenchmarksCorrectFalse204060801000Numberofbenchmarks100101102103104105106107Delayrange(µs)CorrectFalseFig. 6: Number of bugs (i.e., Correct False outcomes) discov-
ered by EBF 4.0 for different time allocations between the
fuzzer and the BMC.

Fig. 7: Number of bugs (i.e., Correct False outcomes) dis-
covered by OpenGBF in EBF 4.0 for different early thread
termination strategies.

very different time allocation choices.
Early thread termination in OpenGBF. Recall
that our
GBF fuzzer terminates the execution of each thread with
probability p (see Section IV-B). This implementation detail
is crucial for avoiding potential deadlocks in the PUT. Figure
7 shows the impact of different values of p on the bug-ﬁnding
performance of our GBF on the SV-COMP 2022 concur-
rency benchmark suite. For comparison, we implemented an
alternative mechanism, which deterministically terminates the
execution of each thread after n instructions. Note that both
termination mechanisms are local to each thread; thus, they
do not introduce any synchronization overhead. Furthermore,
we align the plots according to each thread’s average number
of instructions, which is the mean 1/p of an exponential
distribution.

The results in Figure 7 show that

the performance of
our fuzzer is stable across a wide range of values of p.
Interestingly, removing the termination mechanism altogether
causes only minimal degradation in the fuzzer performance.
Moreover,
there is no signiﬁcant difference between the
probabilistic and deterministic termination mechanisms as the
average number of instructions per thread increases. However,
the performance of the probabilistic mechanism degrades more
slowly as the average number of instruction decrease. We
speculate that the probabilistic termination mechanism allows
our GBF to explore a large number of shallow paths and a
few deeper ones, thus slightly increasing the chance of ﬁnding
a bug for a low average number of instructions. Finally, we
select the best parameter setting p = 0.01% for the rest of our
experiments.

6) Analyzing the non-determinism of our fuzzer: Fuzzers
are fundamentally non-deterministic programs. As such, the
performance of EBF may vary across different runs. We show
the effects of non-determinism by re-running our GBF 20
times on the benchmarks of the present experimental section.
Non-determinism on SV-COMP 2022 suite. We run our GBF

20 times with the same optimal settings described in VI-B5.
In the worst case, our fuzzer ﬁnds only 82 bugs, whereas in
the best case, it ﬁnds 89. Given that there are 365 bugs in
the SV-COMP 2022 suite, we expect the distribution to be
approximately Gaussian, with an empirical mean 85.2 and
standard deviation 2.0. Given that the variance in the total
number of bugs is small, we can trust the results of Figures
4, 5, 6 and 7 to give us robust values for the optimal EBF
setting.

the impact of

Furthermore, we inspect

fuzzer non-
determinism on each individual ﬁle in the SV-COMP 2022
benchmark suite. Speciﬁcally, there are 74 ﬁles for which our
fuzzer always ﬁnds a bug across the 20 independent runs.
Among those, we select the ones with the smallest, median,
and largest variance. We plot the performance of our GBF in
these three representative cases in Figure 8. Note that the violin
shows the extremes of the distributions, together with their
median and kernel density estimation. Since these distributions
are highly non-Gaussian, we omit the mean.
Non-determinism on wolfMQTT and real-world programs.
We re-run our GBF 20 times on the wolfMQTT library and the
three real-world programs listed in Table V. The results are
shown in the violin plot of Figure 8. In the case of pfscan and
bzip2smp, our GBF is able to ﬁnd bugs almost instantly (see
also Table V). In contrast, we can observe more variance on
wolfMQTT and swarm 1.1. In the former case, the distribution
is fairly compact in its support [10.6s, 66.8s]. In the latter case,
the distribution has a long tail. More speciﬁcally, the median
time is 9.5s, 75% of the runs ﬁnd a bug in less than 40s,
but there are also occasional outliers where the ﬁrst bug is
reported after 300s.

C. Limitations

We identify several possible limitations to our current work.
Incompleteness of fuzzing for proving safety. Fuzzing works
by executing the program along many concrete paths, hoping

035.5811100200300400BMC(vsGBF)timeallocation(minutes)Numberofbenchmarks(11)(8)(5.5)(3)(0)CorrectFalseReachableBugs1011021031041051062040608010010%1%0.1%0.01%0.001%0.0001%Max(average)instructionsperthreadNumberofbenchmarksCorrectFalse(hardthreshold)CorrectFalse(exitprobability)∞0%A. Bounded Model Checking (BMC)

Bounded model checking has been successfully applied
to the veriﬁcation of concurrent C programs over the past
years [57]. There exist several state-of-the-art bounded model
checkers, such as ESBMC [14] and CBMC [15] that can handle
both sequential and multi-threaded C programs and detect
concurrency bugs (e.g., data races, deadlocks, etc.) and other
vulnerabilities (e.g., buffer overﬂows, dangling pointers, etc.).
In particular, ESBMC handles concurrency by performing a
depth-ﬁrst search through all possible thread interleavings,
up to the given maximum number of context switches [58].
In contrast, CBMC encodes each concurrent execution unit
separately and joins them with partial order formulae [59].

Many other BMC tools demonstrate their efﬁcacy in the
veriﬁcation of concurrent programs at the annual SV-COMP
software veriﬁcation competition [60]. For example, Lazy-
CSeq [18], [61] - one of the leaders in the concurrency cate-
gory at SV-COMP over the past decade - works by translating
a multi-threaded C program into a non-deterministic sequential
program that considers all round-robin schedules up to a given
number of rounds. Then the obtained sequential program is
veriﬁed using a bounded model checker for sequential pro-
grams (e.g., CBMC, ESBMC). Similarly, Deagle - the winner
in the concurrency category in SV-COMP 2022 [62] - intro-
duces a novel ordering consistency theory for multi-threaded
programs [17], and implements a more efﬁcient solver for this
theory on top of CBMC (front-end) and MathSAT [63] (back-
end).

B. Fuzzing

Traditional techniques for fuzzing sequential programs do
not translate well for concurrent programs since they let the
fuzzer control only the input of the program and not the
scheduling of its threads [23]. Existing proposals towards
concurrency-aware fuzzing attempt to rectify this issue (see
Table I for an overview).

ConAFL [34] is a gray-box fuzzer that specializes on
user-space multi-threaded programs. ConAFL employs static
analysis to locate sensitive concurrent operations to determine
the execution order, focusing on three types of invalid memory
access vulnerabilities: buffer-overﬂow, double-free, or use-
after-free. The thread interleavings are controlled indirectly
by changing the execution priority of each thread at assembly
level. As an alternative, the authors mention the possibility
of injecting sleep commands at the code level, but they do
not test it. Finally, the authors rely on the default mutation
feedback of the sequential fuzzer AFL [64], which is based
on branch coverage. Due to its heavy thread-aware static and
dynamic analysis, ConAFL cannot scale to large programs.
Furthermore, the authors’ static analysis tool is not publicly
available [65].

Similarly, the gray-box fuzzer MUZZ [23] employs static
analysis to identify blocks of code that have a higher chance
of triggering a concurrency vulnerability. When the code
is instrumented, such blocks receive heavier instrumentation
which helps the fuzzer dynamically track the execution of

Fig. 8: Non-determinism of OpenGBF across 20 re-runs of the
SV-COMP’22 benchmark suite, wolfMQTT and the real-world
programs from Table V.

to ﬁnd the one that leads to vulnerability detection. Conse-
quently, it cannot formally guarantee that we can exhaustively
explore the entire state-space of the program. As a result, by
design, EBF prioritizes bug-ﬁnding over proving a program’s
safety.
Sources of incorrect verdicts in EBF. Although EBF does
not produce conﬂicting verdicts using the aggregation matrix
from Table II, the correctness of EBF’s veriﬁcation verdicts
largely depends on the implementation of the tools used in
the assembly. For example, if the BMC engine produces a
wrong Safe outcome while the GBF cannot ﬁnd any violations
within the given time limit (thus returning Unknown) the ﬁnal
veriﬁcation verdict becomes Safe. Similarly, our GBF may
become a source of an incorrect Bug verdict when BMC
reports Unknown and the GBF crashes because of an internal
bug within the GBF’s implementation rather than an actual
vulnerability inside the PUT. Fortunately, this is not critical
since EBF generates a witness ﬁle that can be further evaluated
using witness validators (see Appendix B).
Choice of parameter settings in EBF. Although we conduct
our evaluations over a set of more than 700 multi-threaded C
programs (see description in Section VI-B1), this benchmark
might not represent the real-world picture of concurrent soft-
ware. Thus, the optimal parameter settings for our GBF are
likely to differ on another set of multi-threaded benchmarks.
Nevertheless, we expect that the parameter tuning procedure
on a different set of benchmarks will follow similar patterns
to the ones shown in Figures 4 and 5.

VII. RELATED WORK

Throughout our paper, we describe various existing studies
that cover relevant tools and techniques. In this section, we
collate and expand on these references. Our goal here is to
clarify the context in which our research occurs.

 6 9  & R P S   P L Q  6 9  & R P S   P H G   6 9  & R P S   P D [  Z R O I 0 4 4 7 S I V F D Q E ] L S  V P S V Z D U P                           7 L P H   V  0 H G L D Q 0 D [  0 L Q ' H Q V L W \different schedules. To encourage the exploration of a large
number of interleavings, MUZZ manipulates the execution
order by assigning random priorities to the threads at assembly
level. Despite the promising experimental results, MUZZ is not
yet publicly available.

A simpler approach is implemented in the tool ConFuzz
[43], which lets the natural non-determinism of the operating
system guide the exploration of different
interleavings by
random chance. To compensate for that, ConFuzz modiﬁes the
standard branch coverage feedback of the mutation engine by
measuring how far each block of code is from a thread-related
instruction. Seeds that execute blocks of code closer to such
instructions have a higher chance of survival at each mutation.
Unfortunately, the ConFuzz tool [43] is not publicly available.

Recently, another concurrency-aware gray-box fuzzer has
been proposed in [42]. This tool, called AutoInter-fuzzing,
uses static analysis to identify instruction pairs that access the
same memory location but are executed by different threads.
Then, the program code is instrumented with synchronization
barriers that control the order of execution of the instruction
in each pair. Every time one such pair is encountered during
regular fuzzing, the program is re-run, forcing the opposite
execution order of the pair. Unfortunately, this strategy for
exploring interleavings makes AutoInter-fuzzing suffer from
low path coverage compared to other fuzzers. In line with most
of the fuzzers listed in the present section, AutoInter-fuzzing
is not publicly available.

Conzzer [45] improves upon the ideas of AutoInter-fuzzing.
More speciﬁcally, the instruction pairs are obtained at runtime
and contain information about the execution trace. The authors
argue that the fuzzer can be used to explore different interleav-
ings for a critical region by being context-aware. They also
implemented their own mutation algorithm, resulting in the
fuzzer being able to explore more interleaving than AutoInter-
fuzzing.

On a different note, Krace [44] is a fuzzer for kernel ﬁle
systems that specializes in ﬁnding data races. We mention it
here because it also employs the interleaving control strategy
it
of injecting delays in the program code. Furthermore,
augments the standard branch coverage metrics by explicitly
tracking the order of execution of any pair of instructions
that access the same memory location. This feedback induces
the mutation engine to explore a larger number of thread
interleavings. The source code of [44] is available but cannot
be used in our research as it targets data races in the kernel
space.

OpenGBF (see Section IV) implements many of these ideas,
including instrumenting the code with sleep instructions,
forcing the exploration of random interleavings and letting the
fuzzer control the randomness through its mutation engine. In
the future, if the aforementioned concurrency-aware fuzzers
become open source [29], it will be possible to test their
efﬁcacy when paired with BMC tools, as we do here with
our GBF tool.

C. Hybrid Techniques

Recently, several efforts have combined fuzzing with vari-
ous forms of symbolic execution and static analysis [66]. The
rationale behind these efforts is that fuzzing alone struggles
to ﬁnd “deep” bugs and vulnerabilities because the random
mutations introduced in the input have a low probability of
hitting complex paths in the program. In contrast, if the fuzzer
is given a set of input seeds that are already close to the
correct target, the evolutionary algorithm has a higher chance
of exposing the bugs and vulnerabilities.

To this end, Ognawala et al. [24] propose to increase the
coverage of fuzzing by augmenting the set of input seeds with
a round of concolic execution. With it, the code coverage rises
signiﬁcantly. There are other examples of tools employing
concolic execution, such as Driller [67] and QSYM [68].
Similarly, Chowdhury et al. [26] are concerned with the
inability of off-the-shelf fuzzers to discover inputs that pass
complex blocks of program logic. Their solution is using a
bounded model checker to solve the corresponding reachability
problem and produce concrete input seeds that satisfy the
complex conditions of the program under analysis. The fuzzer
is then free to explore the search space beyond that. On a
different note, Alshmrany et al. [25] employs a selective fuzzer
if the model checker of their FuSeBMC tool fails to ﬁnd all
vulnerabilities. Such fuzzer uses the statistics collected by the
model checker to create a particular set of input seeds.

EBF is similar to these hybrid tools in the sense that
it exploits the combined advantages of fuzzing and model
checking. However, the aforementioned hybrid tools are built
around a close integration between the two techniques, often
requiring speciﬁc assumptions about the veriﬁcation task at
hand. In contrast, our ensembles are more ﬂexible and allow
virtually any existing tool to be combined together. Finally,
none of the existing hybrid approaches can verify concurrent
programs.

D. Other techniques

Other techniques for ﬁnding vulnerabilities in concurrent
programs have been proposed. Wen et al. [69] propose a
controlled concurrency testing technique called Period, which
uses a periodical execution to model the execution of con-
current programs. They feed the periodical executor with a
key point slice of the target program and apply an analyzer to
collect feedback on runtime information. In contrast, Peahen
[70] is an approach to combine context-sensitive and context-
insensitive static techniques, namely context reduction. This
context reduction consists of ﬁltering vulnerabilities found by
a context-insensitive technique with a path feasibility check.
Afterward, a context-sensitive approach is used to validate the
vulnerability. Finally, QL [71] is a tool that employs reinforce-
ment learning to guide the exploration of interleavings. This
tool uses an explicit scheduler.

On a different note, there are a few methods that improve
on classic veriﬁcation techniques. For example, in dynamic
analysis, some works focus on improving soundness and
completeness [72], [73], while other works focus on creating

a new value ﬂow analysis for interprocedural data ﬂow that
detects concurrency issues [74]. At the same time, there are
techniques that employ a different ﬂavor of Model Checking,
speciﬁcally stateless model checking (SMC) [75].The method
was born from the intuition that caching states in Model
Checking was not as effective as a stateless approach. For
example, RCMC [76] and GenMC [77] rely on having a code
interpreter that is able to compute a reachability graph over the
program, and use system calls during the analysis to provide
more accurate results.

VIII. CONCLUSIONS

Discovering vulnerabilities in concurrent programs remains
a challenging problem due to the extreme explosion of the
search space in the number of possible interleavings. In this
paper we focus on two existing approaches to this problem:
Bounded Model Checking (BMC) and Gray-Box Fuzzing
(GBF). When used on their own, each approach can only
ﬁnd a subset of the vulnerabilities present in state-of-the-art
concurrent benchmarks. Our contribution is building ensem-
bles comprising both BMC and GBF tools, thus exploiting the
complementary advantages of these two approaches. We call
such ensembles EBF.

A major hindrance to the use of EBF ensembles is the
current lack of mature open-source GBF tools that support
concurrent testing. For this reason, we ﬁrst propose our own
implementation of state-of-the-art concurrency-aware fuzzing
techniques, and make OpenGBF publicly available. Then, we
combine it with a large variety of state-of-the-art BMC tools,
and show that the EBF ensembles so created can ﬁnd up to
14.9% more concurrency vulnerabilities than the BMC tools
on their own. Furthermore, thanks to OpenGBF, we are able to
discover a data race vulnerability in the open-source wolfMqtt
library.

Overall, we demonstrate that EBF is an effective technique
for ﬁnding vulnerabilities in concurrent programs. Still, the
capability of each ensemble is directly related to the com-
plementary qualities of its BMC and GBF building blocks.
As a consequence, we believe that improving and specializing
each of the two ensemble components is the most promising
direction for future works. More in detail, we need faster
BMC tools that rely on rougher approximations of the program
under test, in order to produce a larger number of meaningful
counterexamples that the GBF tool can exploit as seeds.

APPENDIX A
HARNESSING FUNCTION

Evaluating the SV-COMP 2022 benchmarks [52] requires
speciﬁc functions that must be supported by every tool par-
ticipating in the competition. As a result, we model some
functions for non-determinism and synchronization. The non-
determinism is used to get the value of the input from the
fuzzer. The synchronization is implemented using a set of
functions that guarantee atomicity (i.e., to ensure no thread
interleavings during a block of instructions). In order to
make AFL++ understand the SV-COMP speciﬁc semantics,

we implement these functions as a run-time C library and
link it with the benchmark at compilation time. We make
the non-deterministic input functions to read the values from
stdin (i.e., standard input) when AFL++ fuzzes the PUT. To
support atomicity, we rely on functions EBF atomic begin
and EBF atomic end described in Section IV-B.

APPENDIX B
COUNTER EXAMPLE EXTRACTION
EBF needs to convert the crash reports discussed in Section
IV-C into GraphML-based format to allow automatic witness
checkers to validate the produced witness by tracking the
execution path leading to the reported bug [78]. This feature of
EBF is utilized in two cases: 1) when OpenGBF reports a bug,
and/or 2) when the BMC engine produces a counterexample.

ACKNOWLEDGMENT

The work in this paper is partially funded by the EPSRC
grants EP/T026995/1, EP/V000497/1, EU H2020 ELEGANT
957286, and Soteria project awarded by the UK Research
and Innovation for the Digital Security by Design (DSbD)
Programme. M. A. Mustafa is supported by the Dame Kathleen
Ollerenshaw Fellowship of The University of Manchester. F.
A. acknowledges the scholarship she is receiving from King
Faisal University (KFU).

COPYRIGHT

This work has been submitted to the IEEE for possible
publication. Copyright may be transferred without notice, after
which this version may no longer be accessible.

REFERENCES

[1] A. C. Sodan, J. Machina, A. Deshmeh, K. Macnaughton, and B. Es-
baugh, “Parallelism via multithreaded and multicore cpus,” Computer,
vol. 43, no. 3, pp. 24–32, 2010.

[2] vinod, “Multithreading realtime examples,” https://androidmaniacom.
wordpress.com/2016/12/16/multithreading-realtime-examples/, 2022.
[3] L. C. Cordeiro, E. B. de Lima Filho, and I. V. de Bessa, “Survey
on automated symbolic veriﬁcation and its application for synthesising
cyber-physical systems,” IET Cyper-Phys. Syst.: Theory & Appl., vol. 5,
no. 1, pp. 1–24, 2020.

[4] S. Lu, S. Park, E. Seo, and Y. Zhou, “Learning from mistakes: a
comprehensive study on real world concurrency bug characteristics,”
in ASPLOS, 2008, pp. 329–339.

[5] P. A. Pereira, H. F. Albuquerque, I. da Silva, H. Marques, F. R. Monteiro,
R. Ferreira, and L. C. Cordeiro, “Smt-based context-bounded model
checking for CUDA programs,” Concurr. Comput. Pract. Exp., vol. 29,
no. 22, 2017.

[6] F. R. Monteiro, E. H. da S. Alves, I. da Silva, H. Ismail, L. C. Cordeiro,
and E. B. de Lima Filho, “ESBMC-GPU A context-bounded model
checking tool to verify CUDA programs,” Sci. Comput. Program., vol.
152, pp. 63–69, 2018.

[7] T. Kelly, Y. Wang, S. Lafortune, and S. Mahlke, “Eliminating concur-
rency bugs with control engineering,” IEEE Computer, vol. 42, pp. 52–
60, 12 2009.

[8] Q. Stievenart, J. Nicolay, W. De Meuter, and C. De Roover, “Detecting
concurrency bugs in higher-order programs through abstract interpreta-
tion,” in PPDP, 2015, p. 232–243.

[9] M. B. Dwyer and L. A. Clarke, “Data ﬂow analysis for verifying
properties of concurrent programs,” SEN, vol. 19, no. 5, pp. 62–75,
1994.

[10] C. Cadar and K. Sen, “Symbolic execution for software testing: three
decades later.” Commun. ACM, vol. 56, no. 2, pp. 82–90, 2013.
[11] Y. Li, S. Ji, C. Lv, Y. Chen, J. Chen, Q. Gu, and C. Wu, “V-fuzz:

Vulnerability-oriented evolutionary fuzzing,” CoRR, 2019.

[12] M. Aizatulin, A. D. Gordon, and J. J¨urjens, “Extracting and verifying
cryptographic models from C protocol code by symbolic execution,”
CoRR, vol. abs/1107.1017, 2011.

[13] A. Biere, “Bounded model checking,” in Handbook of Satisﬁability,

2009, pp. 457–481.

[14] M. R. Gadelha, R. S. Menezes, and L. C. Cordeiro, “ESBMC 6.1:
automated test case generation using bounded model checking,” STTT,
pp. 1–5, 2020.

[15] D. Kroening and M. Tautschnig, “CBMC–c bounded model checker,”

in TACAS, 2014, pp. 389–391.

[16] D. Beyer and M. E. Keremoglu, “Cpachecker: A tool for conﬁgurable
software veriﬁcation,” in CAV, G. Gopalakrishnan and S. Qadeer, Eds.,
2011, pp. 184–190.

[17] F. He, Z. Sun, and H. Fan, “Satisﬁability modulo ordering consistency
theory for multi-threaded program veriﬁcation,” in PLDI, 2021, pp.
1264–1279.

[18] O. Inverso, E. Tomasco, B. Fischer, S. La Torre, and G. Parlato, “Lazy-
cseq: a lazy sequentialization tool for c,” in TACAS, 2014, pp. 398–401.
[19] M. Vanhoef and F. Piessens, “Symbolic execution of security protocol
implementations: Handling cryptographic primitives,” in WOOT, 2018.
[20] D. Trabish, A. Mattavelli, N. Rinetzky, and C. Cadar, “Chopped sym-

bolic execution,” in ICSE, 2018, pp. 350–360.

[21] P. Tsankov, M. T. Dashti, and D. Basin, “SECFUZZ: Fuzz-testing

security protocols,” in AST, 2012, pp. 1–7.

[22] B. S. Pak, “Hybrid fuzz testing: Discovering software bugs via fuzzing

and symbolic execution,” 2012.

[23] H. Chen, S. Guo, Y. Xue, Y. Sui, C. Zhang, Y. Li, H. Wang, and Y. Liu,
“{MUZZ}: Thread-aware grey-box fuzzing for effective bug hunting in
multithreaded programs,” in SEC, 2020, pp. 2325–2342.

[24] S. Ognawala, T. Hutzelmann, E. Psallida, and A. Pretschner, “Improving
function coverage with munch: A hybrid fuzzing and directed symbolic
execution approach,” in SAC, 2018, pp. 1475–1482.

[25] K. M. Alshmrany, R. S. Menezes, M. R. Gadelha, and L. C. Cordeiro,
“Fusebmc: A white-box fuzzer for ﬁnding security vulnerabilities in c
programs,” FASE, 2020.

[26] A. B. Chowdhury, R. K. Medicherla, and R. Venkatesh, “Verifuzz:
Program aware fuzzing,” in TACAS. Springer, 2019, pp. 244–249.
[27] L. Xu, F. Hutter, H. H. Hoos, and K. Leyton-Brown, “Satzilla-07: The
design and analysis of an algorithm portfolio for sat,” in Principles
and Practice of Constraint Programming – CP 2007, C. Bessi`ere, Ed.
Berlin, Heidelberg: Springer Berlin Heidelberg, 2007, pp. 712–727.
[28] D. Beyer, S. Kanav, and C. Richter, “Construction of veriﬁer combina-
tions based on off-the-shelf veriﬁers,” in Fundamental Approaches to
Software Engineering, E. B. Johnsen and M. Wimmer, Eds. Cham:
Springer International Publishing, 2022, pp. 49–70.

[29] https://github.com/fatimahkj/EBF, 2022.
[30] https://manpages.ubuntu.com/manpages/focal/man1/pfscan.1.html,

2022.

[31] http://bzip2smp.sourceforge.net/, 2022.
[32] D. A. Bader, V. Kanade, and K. Madduri, “Swarm: A parallel program-
IEEE, 2007, pp.

ming framework for multicore processors,” in IPDPS.
1–8.

[33] P. A. Pereira, H. F. Albuquerque, H. Marques, I. da Silva, C. B. Carvalho,
L. C. Cordeiro, V. Santos, and R. Ferreira, “Verifying CUDA programs
using smt-based context-bounded model checking,” in SAC, S. Ossowski,
Ed., 2016, pp. 1648–1653.

[34] C. Liu, D. Zou, P. Luo, B. B. Zhu, and H. Jin, “A heuristic framework to
detect concurrency vulnerabilities,” in ACSAC ’18, 2018, pp. 529–541.
[35] M. Ben-Ari, Principles of Concurrent and Distributed Programming,

2006.

[36] R. Nieuwenhuis, A. Oliveras, and C. Tinelli, “Solving sat and sat modulo
theories: From an abstract davis–putnam–logemann–loveland procedure
to dpll(t),” JACM, vol. 53, no. 6, p. 937–977, 2006.

[37] https://github.com/esbmc/esbmc, 2021.
[38] “Cbmc,” https://github.com/diffblue/cbmc, 2022.
[39] “Cseq,” https://www.southampton.ac.uk/∼gp1y10/cseq/cseq.html, 2022.
[40] K. M. Alshmrany, M. Aldughaim, A. Bhayat, and L. C. Cordeiro,
“Fusebmc: An energy-efﬁcient test generator for ﬁnding security vul-
nerabilities in C programs,” in TAP, F. Loulergue and F. Wotawa, Eds.,
vol. 12740. Sprnger, 2021, pp. 85–105.

[42] Y. Ko, B. Zhu, and J. Kim, “Fuzzing with automatically controlled
interleavings to detect concurrency bugs,” JSS, p. 111379, 2022.
[43] N. Vinesh and M. Sethumadhavan, “Confuzz—a concurrency fuzzer,”
in ICTSCI e, A. K. Luhach, J. A. Kosa, R. C. Poonia, X.-Z. Gao, and
D. Singh, Eds., 2020, pp. 667–691.

[44] M. Xu, S. Kashyap, H. Zhao, and T. Kim, “Krace: Data race fuzzing

for kernel ﬁle systems,” in IEEE SP.

IEEE, 2020, pp. 1643–1660.

[45] Z.-M. Jiang, J.-J. Bai, K. Lu, and S.-M. Hu, “Context-sensitive and

directional concurrency fuzzing for data-race detection,” NDSS, 2022.

[46] https://github.com/AFLplusplus/AFLplusplus/blob/stable/

instrumentation/README.llvm.md/, 2022.

[47] “Fast

llvm-based instrumentation for aﬂ-fuzz,” https://github.com/
AFLplusplus/AFLplusplus/blob/stable/instrumentation/README.llvm.
md, 2022.

[48] T. Kempf, K. Karuri, and L. Gao, Software Instrumentation, 09 2008.
[49] “Declaring attributes of functions,” https://gcc.gnu.org/onlinedocs/gcc-4.

7.0/gcc/Function-Attributes.html, 2022.

[50] K. Serebryany, D. Bruening, A. Potapenko, and D. Vyukov, “Address-
sanitizer: A fast address sanity checker,” in USENIX, USA, 2012, p. 28.
[51] K. Serebryany and T. Iskhodzhanov, “Threadsanitizer: Data race detec-

tion in practice,” in WBIA, 2009, p. 62–71.
(2021) Sv-comp rules.

[52] G. D. Maayan.

//sv-comp.sosy-lab.org/2022/rules.php

[Online]. Available: https:

[53] https://sv-comp.sosy-lab.org/2022/benchmarks.php, 2022.
[54] “Deagle,” https://https://github.com/thufv/Deagle, 2022.
[55] https://github.com/wolfSSL/wolfMQTT, 2021.
[56] “Mosquitto,” https://mosquitto.org/, 2021.
[57] I. Rabinovitz and O. Grumberg, “Bounded model checking of concurrent
programs,” in CAV, K. Etessami and S. K. Rajamani, Eds., 2005, pp.
82–97.

[58] L. Cordeiro, J. Morse, D. Nicole, and B. Fischer, “Context-bounded
model checking with esbmc 1.17,” in TACAS, 2012, pp. 534–537.
[59] J. Alglave, D. Kroening, and M. Tautschnig, “Partial orders for efﬁcient
bounded model checking of concurrent software,” in CAV, 2013, pp.
141–157.

[60] D. Beyer, “Software veriﬁcation: 10th comparative evaluation (SV-

COMP 2021),” in TACAS, 2021, pp. 401–422.

[61] O. Inverso, E. Tomasco, B. Fischer, S. La Torre, and G. Parlato,
“Bounded model checking of multi-threaded c programs via lazy se-
quentialization,” in CAV, A. Biere and R. Bloem, Eds., 2014, pp. 585–
602.

[62] D. Beyer, “Progress on software veriﬁcation: SV-COMP 2022,” in

TACAS, 2022.

[63] R. Bruttomesso, A. Cimatti, A. Franz´en, A. Griggio, and R. Sebastiani,

“The mathsat 4 smt solver,” in CAV, 2008, p. 299–303.

[64] https://github.com/google/AFL, 2021.
[65] C. Lie, “personal communications,” 2022.
[66] J. Li, B. Zhao, and C. Zhang, “Fuzzing: a survey,” Cybersecurity, vol. 1,

no. 1, pp. 1–13, 2018.

[67] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang, J. Corbetta,
Y. Shoshitaishvili, C. Kruegel, and G. Vigna, “Driller: Augmenting
fuzzing through selective symbolic execution.” in NDSS, vol. 16, 2016,
pp. 1–16.

[68] I. Yun, S. Lee, M. Xu, Y. Jang, and T. Kim, “{QSYM}: A practical
concolic execution engine tailored for hybrid fuzzing,” in USENIX),
2018, pp. 745–761.

[69] C. Wen, M. He, B. Wu, Z. Xu, and S. Qin, “Controlled concurrency

testing via periodical scheduling,” in ICSE, 2022, p. 474–486.

[70] Y. Cai, C. Ye, Q. Shi, and C. Zhang, “Peahen: Fast and precise static

deadlock detection via context reduction,” 2022.

[71] S. Mukherjee, P. Deligiannis, A. Biswas, and A. Lal, “Learning-based
[Online].

controlled concurrency testing,” PACMPL, vol. 4, 2020.
Available: https://doi.org/10.1145/3428298

[72] Y. Cai, H. Yun, J. Wang, L. Qiao, and J. Palsberg, “Sound and
efﬁcient concurrency bug prediction,” in ESEC/FSE, 2021, p. 255–267.
[Online]. Available: https://doi.org/10.1145/3468264.3468549

[73] U. Mathur, A. Pavlogiannis, and M. Viswanathan, “Optimal prediction
jan 2021.

of synchronization-preserving races,” PACMPL, vol. 5,
[Online]. Available: https://doi.org/10.1145/3434317

[41] C. Lemieux and K. Sen, “Fairfuzz: A targeted mutation strategy for
increasing greybox fuzz testing coverage,” in ASE ’18, 2018, pp. 475–
485.

[74] Y. Cai, P. Yao, and C. Zhang, “Canary: Practical static detection of
inter-thread value-ﬂow bugs,” in PLDI, 2021, p. 1126–1140. [Online].
Available: https://doi.org/10.1145/3453483.3454099

[75] P. Godefroid, “Model checking for programming languages using
verisoft,” in SIGPLAN-SIGACT. Association for Computing Machinery,
1997, p. 174–186. [Online]. Available: https://doi.org/10.1145/263699.
263717

[76] M. Kokologiannakis, O. Lahav, K. Sagonas, and V. Vafeiadis, “Effective
stateless model checking for c/c++ concurrency,” PACMPL., vol. 2,
2017. [Online]. Available: https://doi.org/10.1145/3158105

[77] M. Kokologiannakis and V. Vafeiadis, “Genmc: A model checker for
weak memory models,” in CAV, A. Silva and K. R. M. Leino, Eds.,
2021, pp. 427–440.

[78] D. Beyer and K. Friedberger, “Violation witnesses and result valida-
tion for multi-threaded programs,” https://www.sosy-lab.org/research/
witnesses-concurrency/, 2022.

