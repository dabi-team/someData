2
2
0
2

g
u
A
3

]
E
S
.
s
c
[

1
v
5
9
8
1
0
.
8
0
2
2
:
v
i
X
r
a

The Role of Diversity in Cybersecurity Risk Analysis: An
Experimental Plan

Katja Tuma
Vrije Universiteit Amsterdam
k.tuma@vu.nl

Romy Van Der Lee
Vrije Universiteit Amsterdam
r.vander.lee@vu.nl

ABSTRACT
Cybersecurity threat and risk analysis (RA) approaches are used to
identify and mitigate security risks early-on in the software devel-
opment life-cycle. Existing approaches automate only parts of the
analysis procedure, leaving key decisions in identification, feasibil-
ity and risk analysis, and quality assessment to be determined by
expert judgement. Therefore, in practice teams of experts manually
analyze the system design by holding brainstorming workshops.
Such decisions are made in face of uncertainties, leaving room for
biased judgement (e.g., preferential treatment of category of ex-
perts). Biased decision making during the analysis may result in
unequal contribution of expertise, particularly since some diversity
dimensions (i.e., gender) are underrepresented in security teams.
Beyond the work of risk perception of non-technical threats, no
existing work has empirically studied the role of diversity in the risk
analysis of technical artefacts. This paper proposes an experimental
plan for identifying the key diversity factors in RA.

KEYWORDS
secure design, threat modeling, risk analysis, cybersecurity, diver-
sity

ACM Reference Format:
Katja Tuma and Romy Van Der Lee. 2022. The Role of Diversity in Cyberse-
curity Risk Analysis: An Experimental Plan. In Third Workshop on Gender
Equality, Diversity, and Inclusion in Software Engineering (GE@ICSEâ€™22),
May 20, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 7 pages.
https://doi.org/10.1145/3524501.3527595

1 INTRODUCTION
Security-by-design techniques strive to avoid risks before the soft-
ware is actually developed to reduce the security incurred costs
in later stages of development. For instance, threat and risk analy-
sis (RA, for short) can be performed by adopting systematic tech-
niques to scrutinize the software system (e.g., STRIDE [32], attack
trees [30], CORAS [20], to name a few). Due to development trends
(Agile, DevOps), analysis automation is a much needed direction
for improvement [6]. However, many automation efforts automate
parts of the analysis procedure and rely on an extended manual mod-
eling effort [35]. Key decisions in the identification, feasibility and
risk analysis, and quality assessment are often left to the experts. In
practice, teams of experts often analyze the system design manually

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
GE@ICSEâ€™22, May 20, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9294-5/22/05.
https://doi.org/10.1145/3524501.3527595

1

by holding brainstorming workshops where they discuss potential
risks and decide which risks will be mitigated and how. Risk based
decisions must be made in face of uncertainties [2], leaving room
for biased judgement (e.g., preferential treatment of category of
experts). In addition, the quality of analysis outcomes is ultimately
determined by (potentially biased) expert judgement [3, 16]. For
instance, evidence of groupthink [44] in safety analysis sessions
suggests that knowledge is not always contributed (equally) by all
participants, but exact variables that effect the technical quality of
the analysis are not yet understood. Further, the type of judgement
bias present in risk-based decision making and more generally the
role of diversity has not yet been sufficiently explored in RA (as
discussed in the related work, Section 3).

In many engineering domains, there is a lack of security ex-
perts [36]. In addition, security expert teams are not diverse, espe-
cially for what concerns gender. According to a news post by the
International Consortium of Minority Cyber Professionals (non-
profit) in August 2021 [7], the cybersecurity workforce in the USA is
currently only 14% female. Generally speaking, there is a significant
mismatch between the cultural notion of a successful technical pro-
fessional and the perceived skills that are needed for success in the
computer science domain. Wynn and Correll [47] show that men are
more likely than women to believe they possess the stereotypical
traits and skills of a successful tech employee. This mismatch can
explain the low technical confidence of young female professionals
that are deciding about their career path in computer science [1]. We
believe there is still untapped potential in empowering women to
pursue their career as security experts. This motivates us to gather
empirical evidence that could help in (i) raising more interest and
confidence levels of female students in cybersecurity topics, and (ii)
encouraging organizations to build gender balanced security teams.
The purpose of the laid out research track is to identify the di-
versity dimensions (see Section 2) that play a role during RA of
more sophisticated technical artefacts. To simulate RA in the exper-
imental setup, the ACM Code of Ethics scenarios can be used as RA
task when interacting with the communication science population
as opposed to traditional economics/social experiments. Second,
case studies with detailed information about the software design
(e.g., IoT home monitoring system used in [37]) can be used as RA
task when interacting with the computer science population. The
experimental conditions will differ depending on the measure and
target audience (as described in Section 5).

This paper discusses the challenges (see Section 4) and outlines
the experimental plan (see Section 5) for identifying the role of
diversity in the technical domain of RA. Our experimental plan
is three-fold. First, we plan to characterize the key differentiating
factors in diversity dimensions (e.g., gender) that are present to
RA. Second, we plan to evaluate their effects in a non-technical

 
 
 
 
 
 
GE@ICSEâ€™22, May 20, 2022, Pittsburgh, PA, USA

Katja Tuma and Romy Van Der Lee

population and technical population. Third, we plan to develop and
validate diversity interventions (given to participants in the form of
training) in realistic (technical) environments. We briefly mention
the next steps in the laid out plan and present the concluding
remarks in Section 7.

2 DIVERSITY DIMENSIONS IN RA
Diversity processes are well understood in the social science disci-
plines, such as, the role of diversity factors in group dynamics or
effects of diversity biases in organizational processes. In contrast
to the technical domain, studies have demonstrated how diver-
sity (gender, nationality) can be beneficial for decision making and
progress, once effectively incorporated [24, 40, 41].

Gender. Gender is an individualâ€™s own gender identity, which is
typically, man and woman, but can also be non-binary. Rodriguez et
al. [28] found evidence of bias against women in some software en-
gineering communities, and sometimes negative perceptions about
women working in teams. Thus, this is an interesting dimension to
further investigate in the context of cybersecurity.

Education. Education is an individualsâ€™ achieved level and topic
of specialization (e.g., computer security vs AI) of academic stud-
ies. Risk-based decisions have to be made in organizations by the
managerial layers, who typically have a good understanding of the
product, but do not necessarily posses the technical skills of security
experts or engineers. Therefore, it is interesting to investigate this
dimension and include participants from a different domain (e.g.,
with background in communication sciences). Education turned out
to be a non-significant variable in the study of the impact of com-
mercial Antivirus on peopleâ€™s awareness of security incidents [15].
However, it is not clear whether this dimension has an impact in
performing a RA task.

Nationality & Race. Nationality is the country of origin, which
is often coupled with the culture and language that categorizes
social groups. Race is a social construct linked with individualâ€™s
physical characteristics such as skin color and is used to categorize
populations. Determining the effect of racial (and nationality) bias
in cybersecurity practices is to date an open question. Thomas et
al. [34] conducted semi-structured interviews with 14 Black women
in computing and report that Black women experienced isolation
(though it is not clear whether due to gender or race). But, few
studies have focused on racial (and nationality) diversity in the
software engineering discipline [28].

Age & Seniority. Age and seniority (i.e., position in the organi-
zation and years of experience) may have an impact on both the
technical outcomes of a RA task and on the judgment of outcomes
quality (e.g., over-favoring results submitted by senior analysts).
For instance, a category of experts (e.g., junior vs senior) may un-
derestimate the ease with which a mitigation can be implemented
in reality [46]. This may hurt later-on when infeasible mitigations
have to be replaced with cheaper (and perhaps) less insecure so-
lutions. We remark that though this dimension is indeed relevant,
empirically observing seniority (in a realistic setting) requires in-
volving professionals. Our experimental plan is designed in an
academic (controlled) setting with student participants. Thus we
do not attempt to observe this dimension as part of this work.

2

3 RELATED WORK
There is an extensive account of existing RA techniques, their main
characteristics, information sources, and limitations. We refer the
interested reader to the literature reviews by Tuma et al. [35] and
Ling et al. [19]. For an overview of research on diversity theory and
the relevant social processes involved in organizations (e.g., group
dynamics) see studies [24, 40, 41].

3.1 Diversity Studies in Risk Analysis
Beyond the gender theory about different risk perception [10], there
was little scientific inquiry in the computer science domain about
gender diversity (or other diversity processes) in risk analysis in
the past 10 years. When considering non-technical scenarios (e.g.,
returning from work at night or natural disasters) different risk
perceptions between men and women (and ethnic minorities) have
been investigated, mainly in the USA but also in Sweden [25]. While
the studies conducted in the USA suggest that white males perceived
lower risk compared to women and minorities, the same was not ob-
served by Olofsson and Rashid for Sweden [25]. A recent study [15]
investigated the impact of commercial Antivirus on peopleâ€™s aware-
ness of security incidents with 1000 respondents. They found that
the only relevant variables were online insecurity, risk aversion
and gender (other non-significant variables were education, age, ex-
perience with information systems). Other empirical studies [8, 23]
claim a diversity based on participation in simple games.

Thus, we discern that existing research has focused on measuring
diversity aspects in RA when participants consider non-technical
artefacts while there is still the need to study diversity factors when
considering technical (e.g., cybersecurity threats and mitigations)
artefacts.

3.2 Diversity Studies in Computer Science &

Software Engineering

Rodriguez et al. [28] have recently conducted a systematic literature
review to identify the diversity aspects that have been studied in
software engineering and development. Gender has been the most
widely researched dimension (61% of the papers in [28]), while
others (nationality, age, race) are less explored. This does not come
as a surprise, considering that computer science, engineering, and
physics are less gender balanced compared to other STEM fields
(e.g., biology) [5]. In addition, existing research has focused on
studying diversity in the context of open source communities and
developers (e.g., see the study on gender bias on GitHub [14]),
while other domains are less explored. For instance the role of
gender-diversity in the domain of software architecture is unex-
plored [27, 33]. Some tools have been proposed to assess diversity
dimensions of software or software development processes (Gen-
derMag, InclusiveMag, and AID Tool, to name a few). However,
most studies have focused on identifying bias through case stud-
ies, rather than conducting experiments or proposing new models,
tools, or processes [28].

GenderMag [43] is a method to inspect software (e.g., interface
design) to identify gender-inclusiveness issues. InclusiveMag [21]
is a generalization of GenderMag that can be used to generate in-
clusiveness methods for a particular dimension of diversity. AID

The Role of Diversity in Cybersecurity Risk Analysis: An Experimental Plan

GE@ICSEâ€™22, May 20, 2022, Pittsburgh, PA, USA

Outcome

Threat

Assumption
Attack surface
Effected security ob-
jective
Threat impact

Risk priority
Threat mitigation

Outcome type

spoofing, tampering, repudiation, information
disclosure, denial of service, elevation of privi-
lege
domain, security
physical, close-proximity, remote
confidentiality, integrity, availability, account-
ability
financial, safety, operational, reputation, legal
(incl. privacy)
high, medium, low
preventative, detective/reactive, corrective

Table 1: Type of RA outcomes investigated in this work

Tool [4] is an automation of the GenderMag method to detect â€in-
clusivity bugsâ€ which are software features that disproportionately
disadvantage particular groups of contributors in OSS. The tool
was evaluated on 20 GitHub projects.

4 CHALLENGES
This section outlines the challenges in establishing the role of di-
versity in RA.

4.1 Evidence of Biased Judgement
The first challenge is capturing whether diversity dimensions have
an effect on biased judgment. To observe this we plan to measure
the type (i.e., gender bias, education bias, and nationality and race
bias) and amount of bias the analyst may be projecting when mak-
ing (subjective) judgements about another analystâ€™s work (report
of outcomes). We are interested to observe the evidence of biased
judgement in studies with and without the technical artefact and in
different populations (i.e., computer science and communication sci-
ence). Measuring such bias is relevant considering that sometimes
a preferential (managerial) choice must be made between analysis
reports of two different analysts which may belong to different
diversity groups (e.g., man vs woman). Lastly, capturing the inter-
play between the evidence of biased judgment and actual diversity
effects on analysis outcomes is an unexplored direction. We may
find there are no actual differences in the outcomes produced, but
there is evidence of biased judgments (or vice-versa).

4.2 Evidence of Diversity Effects on Outcomes
The second challenge is to investigate whether diversity dimensions
have an actual effect on the task of RA. To this aim, we will measure
the effects of diversity dimensions on the RA outcomes. Since the
quality of analysis lacks a formalised definition (e.g., often natural
language is used to describe attack scenarios and informal notations
are used for modeling [35]), we will use measures that can be easily
reproduced. Namely, we can observe how diversity dimensions ef-
fect the type of analysis outcomes. Table 1 shows various outcomes
that are often included in the analysis report. In what follows, we
describe the outcomes and motivate their inclusion for observation.
Threats. A malicious attacker (with means and motivations)
poses a threat to harm the desired security properties of system

assets. As part of the analysis outcomes, the identified threats (e.g.,
spoofing an authorized user) are accompanied by a detailed descrip-
tion of the attack scenarios (e.g., CAPEC-656: Voice Phishing 1) that
realize the threat. We borrow the threat categories from the docu-
mentation of STRIDE, a threat modeling methodology developed
by Microsoft [32].

Analyses conducted by experts tend to be more balanced in terms
of the categories of identified threats, whereas RA novices tend to
report disproportionately more tampering, denial of service and
information disclosure threats [36, 37]. We are interested to observe
whether category distribution patterns emerge for other diversity
dimensions.

Assumptions. Assumptions are statements about the system
under analysis that may or may not be true [36]. While system
requirements and constraints are typically explicitly defined and
static, assumptions are often implicit and dynamic in nature (i.e.,
they can be invalidated and modified as the project evolves). Due
to little support offered to the analysts regarding the assumption
definition, quality assessment, and management, a recent study
investigated the role of assumptions in context of STRIDE [42].
Van Landuyt and Joosen [42] find that the majority of assumptions
(created by students during STRIDE) were used to either justify an
existence of threats or are used to eliminate threats. In [42] a sub-
stantial subset (78%) of the assumptions was in direct reference to
security-related concepts (i.e., security assumptions), however also
domain assumptions (statements about component functionalities)
were made.

In the safety science domain, managerial over-reliance on rou-
tine technical inspection was identified as a common underlying
characteristic of human-created disasters [45]. In RA, a category
of experts may make assumptions about different properties of the
system and similar over-reliance on verifying assumptions could be
present. We are interested to investigate the effect of other diversity
dimensions on the type of assumptions that are made during RA.
Attack surface. Security analysts often rely on defining the
attack surface and the required attacker profile to exploit it to deter-
mine the feasibility of an attack scenario. Intuitively, consider the
feasibility of an information disclosure threat on internal commu-
nication channels to expose intercepted emails by a â€™script kiddieâ€™
with little computing resources and off-the-shelf hacking tools. The
attack may require gaining physical access to the organizations in-
ternal network and breaking TLS encryption. In addition, the â€™script
kiddieâ€™ may not have a motive to launch this attack. Determining
feasibility is subjective, domain-specific and not always obvious.

Giddens et al. [9] found that when analyzing internal threats,
managers will consider male employees as exhibiting greater ma-
licious intentions (associated with computer abuse) compared to
female employees). Thus, it is interesting to measure whether di-
versity dimensions play a role in identifying the core variables for
determining threat feasibility.

Effected security objective. The attacker goal is to compro-
mise a security objective (confidentiality, integrity, availability, and
accountability) of an asset of value. Since spoofing and elevation
of privilege threats may compromise multiple security objectives
(e.g., a lack of authentication due to spoofing may result in loss of

1https://capec.mitre.org/data/definitions/656.html

3

GE@ICSEâ€™22, May 20, 2022, Pittsburgh, PA, USA

Katja Tuma and Romy Van Der Lee

confidentiality, integrity, or availability), we are also interested to
observe possible patterns of the threatened security objectives per
diversity factor.

Threat impact. Given a successful attack, threat impact refers
to the damage it causes to the organization. Estimating the type
(and amount) of impact is useful for organizations to put the threat
into perspective of the domain (e.g., when safety-critical systems
are analyze, the safety impact can not be left unmitigated). Some
impacts may be easier to estimate depending on the background
and previous education. For instance, for an engineer the number
of effected units may seem easier to determine compared to legal
and financial loss estimates, for which other department expertise
may be needed [6]. Thus, we are interested to investigate whether
diversity dimensions (e.g., education) have an effect on the type of
impacts that our participants identify.

Risk priority. Since the number of identified threats quickly
explodes in realistic projects [36], they must be prioritized based
on estimations of risk (e.g., the last step of STRIDE [32]). The most
critical threats are red flags and consequently larger investments
are made to develop and test their mitigations. We refer to risk
as a product of threat probability and impact. How individuals
assess risk priorities may be related to their risk perception which
is already well understood [10].

Mitigations. Mitigations of a cybersecurity risk can be pre-
ventative (e.g., implementation of two-factor authentication), de-
tective/reactive (e.g., using intrusion detection and access revoca-
tion techniques) and corrective (such as maintaining audit trails or
restoring from a secure state). Multiple strategies can be adopted to
counter a cybersecurity threat, and the final choice may depend on
domain-related factors, as the cost of implementing the mitigation
has to be reasonable. A category of experts (e.g., man vs woman)
may underestimate the ease with which a mitigation is actually im-
plemented, as observed in [46]. By doing so, they in fact contribute
to the actual mitigation of the threat. Thus, we are interested to
observe how diversity dimensions effect the type of mitigations
that are identified during the analysis.

4.3 Identification of countermeasures to

identified effects

Because risk decisions must be made in face of uncertainties for
which no statistical bias is available [2], organizational issues play
a major role in which the final technical artefact is chosen [26]
(i.e., security mitigations), and thus we must put in place techni-
cal/organizational measures to counterbalance it. Our final chal-
lenge is to identify:

i RA practices (e.g., methods, tools, processes borrowed from

adjacent disciplines) that help foster diversity, and

ii feasible and effective countermeasures for organizations to

adopt.

Figure 1: Design of the experiment with two versions of the
survey for computer science (Comp. Sci) and communica-
tion science (Comm. Sci) populations

5 EXPERIMENTAL PLAN
Study of diversity effects and their prevalence in RA. There
is a fairly complete account and understanding of the existing
RA techniques, their main characteristics, information sources,
and limitations [19, 35]. On the other hand, research has shown,
with regard to gender diversity, that there are differences in the
perceived competence of men and women [40]. In the context of
academic funding competitions, to illustrate this process, there is
evidence that men and women produce ideas of similar quality,
yet women are underestimated for their competence [39]. In other
words, this implies that women are likely to be equally competent
in identifying security threats as men, yet their assessments might
not be sufficiently valued. Hence, this might have far reaching
consequences for the RA as this might result in an underestimation
of the security threats. It is thus pivotal to investigate whether
diversity processes are at play during RA.
RQ1. What diversity processes play a role during security threat
analysis and risk assessment of IT systems?

Experimentation and validation of diversity interventions
in RA. Previous studies have experimented with RA techniques
with practitioner and student participants [17, 18, 36, 37] measur-
ing (among others) technique productivity, analysis outcomes, per-
ceived ease of use. But, diversity relevant measures, controls and
relevant training for RA techniques have not yet been defined for
such experiments [28]. In contrast, Van der Lee and Ellemers [40]
have identified characteristics of effective diversity interventions,
and applied those to practical decision making processes. One of
the key features of diversity intervention effectiveness is the degree
to which they are applicable to the current situation. In other words,
whether participants can use the knowledge that they gained [12].
The interventions thus need to promote ways in which analysts
can use their diversity to better their decision making. A factor
that might hinder this process is the perceived objectivity of the
analysts, thereby paradoxically increasing rather than decreasing
implicit biases rooted in stereotypes [38]. Hence, to strengthen
the quality of RA the diversity interventions must be grounded in
theory and tested in the context of RA.
RQ2. What experimental protocols, metrics, and controls capture and
effectively incorporate diversity in the context of RA of IT systems?

Identifying diversity fostering practices could be partially achieved
by conducting a more systematic review of the existing literature.
However, to evaluate the identified countermeasures empirically
more future efforts will be needed.

5.1 Hypotheses & Methods
To answer the research questions RQ1 and RQ2, we formulate
two type of hypotheses. First, we pose three hypotheses about
the equivalence of the sample means for biased judgement

4

Survey material for computer & communication science populationComp. SciComm. SciSummaryof RA outcome + persona of analystSummaryof RA outcome + persona of analystSummary of RA outcome + persona of analyst + Technical AppendixSummary of RA outcome + persona of analyst + Technical AppendixThe Role of Diversity in Cybersecurity Risk Analysis: An Experimental Plan

GE@ICSEâ€™22, May 20, 2022, Pittsburgh, PA, USA

Dimension
$ğ‘ ğ‘ğ‘šğ‘’

$ğ‘†ğ‘’ğ‘›ğ‘–ğ‘œğ‘Ÿğ‘–ğ‘¡ ğ‘¦

$ğ‘€ğ‘–ğ‘¡ğ‘–ğ‘”ğ‘ğ‘¡ğ‘–ğ‘œğ‘›

Level

â€¢ (male) Frank
â€¢ (female) Ana

â€¢ (senior) Senior Analyst
â€¢ (junior) Junior Analyst

â€¢ (detective/reactive) coordinating the web browsers with black-
lists which block any incoming traffic from server hosting mali-
cious activity.
â€¢ (corrective) engineering a worm which targets behavior resem-
bling â€œhackerâ€ activity on the effected server.

Table 2: The vignette dimensions and levels for the survey
experiment designed to measure bias in the judgement of
RA outcomes.

(e.g., by using Two One-Sided T-Tests (TOST) [22]) in presence and
absence of the technical artefact.

To test hypotheses ğ»1 âˆ’ ğ»3, we designed a survey experiment
with two populations, computer science BSc students and communi-
cation science BSc students. The survey is targeting students taking
courses taught by the experimenters. Individual participation to the
survey will be voluntary. The survey will be based on a high-level
scenario (such as the ACM ethics Malware Disruption case 2) and
will require the participants to understand basic principles of secu-
rity and privacy. We adopted a blocked design for the experiment
with two versions of the survey which are distributed to both pop-
ulations, as shown in Figure 1. The first will include a high-level
executive summary of the RA outcomes (in security layman terms)
including the final recommendation (i.e., threat mitigation) and a fic-
titious persona description of the RA analyst. The fictitious personas
(also called vignettes) can be used to measure biased judgements.
The second survey version will include the executive summary, the
final RA recommendation, the fictitious persona, and a technical
appendix with the actual RA outcomes in full detail. Table 2 shows
the dependent variables used to measure the biased judgement. The
level of each vignette dimension (gender, seniority, and mitigation)
is randomly sampled and presented to the participant (similar to
survey experiment in [11]) in a context of the ACM Code of Ethics
scenario. The participant will be then asked to rate their trust in the
analyst and agreement with the suggestion of the analyst. Several
control questions will also follow. The independent variables are
the participant gender and education background.
ğ»1 : ğ¶ğ‘œğ‘šğ‘.ğ‘†ğ‘ğ‘– = ğ¶ğ‘œğ‘šğ‘š.ğ‘†ğ‘ğ‘–
First, we will measure whether the judgement bias (i.e., the type
of bias and the effect size) in the computer science population is
equivalent to the judgement bias in the communication science
population in absence of the technical artefact.
ğ»2 : ğ¶ğ‘œğ‘šğ‘š.ğ‘†ğ‘ğ‘– + ğ´ğ‘Ÿğ‘¡ğ‘’ ğ‘“ ğ‘ğ‘ğ‘¡ = ğ¶ğ‘œğ‘šğ‘š.ğ‘†ğ‘ğ‘–
If we find that the bias is in fact equivalent across the two domains,
it is interesting to also investigate the equivalence of judgement
bias for the communication science population with the techni-
cal artefact compared to the communication science population
without the technical artefact.
ğ»3 : ğ¶ğ‘œğ‘šğ‘.ğ‘†ğ‘ğ‘– + ğ´ğ‘Ÿğ‘¡ğ‘’ ğ‘“ ğ‘ğ‘ğ‘¡ <> ğ¶ğ‘œğ‘šğ‘.ğ‘†ğ‘ğ‘–

2https://ethics.acm.org/code-of-ethics/using-the-code/case-malware-disruption/

Figure 2: Possible (?) diversity effects and expected absence
of effects in RA outcome type studies in presence of the tech-
nical artefact

If we find that the bias is in fact equivalent within the non-technical
domain, we must (naturally) also find bias equivalence within the
technical domain. Therefore, it is interesting to investigate the
equivalence of judgement bias for the computer science popula-
tion with the technical artefact compared to the computer science
population without the technical artefact. If our statistical analy-
sis does not support the equivalence hypothesis, we must further
investigate how the two samples differ. For instance, if compared
to the computer science population without the technical artefact
the judgement bias of the computer science population with the
technical artefact is greater, this is evidence of a systematic bias in
the computer science community.

Second, we pose hypotheses about the equivalence of the sam-
ple means for the RA outcomes in presence of the technical
artefact. Figure 2 depicts hypotheses ğ»4 : ğ»6. To test hypotheses
ğ»4 âˆ’ ğ» 6 we will conduct controlled experiments with computer
science MSc students. Concretely, we will hand out the documen-
tation of a software system (e.g., requirements, component and
deployment diagrams etc.) to individual students. After a training
session, the students will be tasked to perform a RA following a
prescribed technique and hand-in the RA outcomes.

ğ»4 : ğ¶ğ‘œğ‘šğ‘.ğ‘†ğ‘ğ‘–ğ¹ = ğ¶ğ‘œğ‘šğ‘.ğ‘†ğ‘ğ‘–ğ‘€
Regarding gender, we expect that the outcome types reported by
women are equivalent to the outcome types reported by men. Stud-
ies of risk perception suggest that women perceive certain risks
differently compared to men. Though we do not foresee strong dif-
ferences, we might find some effects when it comes to risk priority.
ğ»5 : ğ¶ğ‘œğ‘šğ‘.ğ‘†ğ‘ğ‘–1 = ğ¶ğ‘œğ‘šğ‘.ğ‘†ğ‘ğ‘–2
Regarding education, we expect that the students of various special-
ization tracks report equivalent outcome types for the same system
under analysis. Our population is computer science students, with
some differences in the elective courses and program choices (e.g.,
we plan to include students from various master programs, such
as IA, computer Security, and Software Engineering). We may find
some outliers in the proportions of domain vs security assumptions
made, depending on the previous completed courses or the master
specialization. For example, possibly AI students make less security

5

ThreatsSecurity concernAssumptionThreat impactRiskpriorityAttack surfaceMitigationGenderEducationNationality/Race??Type of OutcomesDiversity Factor??GE@ICSEâ€™22, May 20, 2022, Pittsburgh, PA, USA

Katja Tuma and Romy Van Der Lee

assumptions, compared to Computer Security students. Though we
do not have string convictions, we may find that previous courses
may have an effect on the threat impact (e.g., students with robotics
and AI background may focus on safety impacts). Similarly, we
may find come differences in the type of mitigations considered by
students of different background.
ğ»6 : ğ¶ğ‘œğ‘šğ‘.ğ‘†ğ‘ğ‘–ğ‘…ğ‘ = ğ¶ğ‘œğ‘šğ‘.ğ‘†ğ‘ğ‘–ğ‘…ğ‘
We expect that the students of various race and nationality re-
port statistically equivalent outcome types. We have not found any
research that would suggest otherwise.

The findings from investigating ğ» 1 âˆ’ ğ» 6 will play a key role
in developing diversity interventions in the form of training. To
validate the diversity training, we plan to conduct comparative
experiments in a similar controlled setting, where we administer
an additional diversity training to half of the participants.

6 POTENTIAL THREATS TO VALIDITY
Small female population. There is typically around 20% (or less)
female students enrolled in computer science programs. We are
aware of the validity threats caused by an unbalanced population
sample, which is omnipresent in all gender diversity studies in
STEM disciplines [28]. To partially mitigate this threat, we planned
a sister experiment (with a less technically demanding task) with
a communication science population (see Section 5). Second, we
will rally female computer scientist students towards participation
through local steminist groups and similar community organized
channels.

The small female population in computer science is also the ra-
tionale for organizing the controlled experiments with computer
science population (testing hypotheses ğ»4 âˆ’ğ» 6) with individual stu-
dents. A more realistic setup (as was done in previous studies [37])
would include dividing participants into teams as in practice RA is
performed in teams of experts. But measuring the individual female
contribution to the outcomes inside gender-mixed teams becomes
very hard and possibly not reliable.

Generalizability of results to practitioners. We do not plan
to include practitioners experienced in RA to participate in our
studies. This limits us in observing the full complexity of the di-
versity effects (e.g., including seniority) that are actually present
in organizations where RA is routinely performed. Still, studies
have shown [13, 29, 31] that the differences between the perfor-
mance of professionals and graduate students are often limited. To
extend our observations we plan to conduct industrial cases studies
in the future (e.g., similar to the study performed by Wang and
Wagner [44]).

7 CONCLUSION
This paper outlines an experimental plan to discover the role of
diversity in cybersecurity risk analysis. We present the relevant di-
versity dimensions that will be observed and discuss the challenges
of capturing evidence about biased judgement during RA and about
diversity effects on the actual RA outcomes. As a first step, we pro-
pose to measure the key differentiating diversity factors in RA by
conducting survey experiments with a technical and non-technical
population. Second, we plan to conduct controlled experiments
with the computer science population to record diversity effects

6

on the outcomes and to validate diversity interventions. Finally, an
interview study with practitioners would be beneficial for identify-
ing the most urgent diversity dimensions to be addressed first with
the interventions.

ACKNOWLEDGMENTS
We express our deepest appreciation to prof. Fabio Massacci for his
insightful suggestions for the experimental plan.

This research is partially supported by the Network Institute
Academy Assistant program (NIAA) at the Vrije Universiteit and
by the H2020 AssureMOSS project that received funding from the
European Unionâ€™s Horizon 2020 research and innovation program
under grant agreement No 952647. This paper reflects only the
authorâ€™s view and the Commission is not responsible for any use
that may be made of the information contained therein.

REFERENCES
[1] Sylvia Beyer, Kristina Rynes, Julie Perrault, Kelly Hay, and Susan Haller. 2003.
Gender differences in computer science students. In Proceedings of the 34th SIGCSE
technical symposium on Computer science education. 49â€“53.

[2] Vicki Bier. 2020. The Role of Decision Analysis in Risk Analysis: A Retrospective.

Risk Analysis 40, S1 (2020), 2207â€“2217.

[3] Mario P Brito and Ian GJ Dawson. 2020. Predicting the validity of expert judg-
ments in assessing the impact of risk mitigation through failure prevention and
correction. Risk analysis 40, 10 (2020), 1928â€“1943.

[4] Amreeta Chatterjee, Mariam Guizani, Catherine Stevens, Jillian Emard, Mary Eve-
lyn May, Margaret Burnett, and Iftekhar Ahmed. 2021. AID: An Automated
Detector for Gender-Inclusivity Bugs in OSS Project Pages. In 2021 IEEE/ACM
43rd International Conference on Software Engineering (ICSE). 1423â€“1435. https:
//doi.org/10.1109/ICSE43902.2021.00128

[5] Sapna Cheryan, Sianna A Ziegler, Amanda K Montoya, and Lily Jiang. 2017. Why
are some STEM fields more gender balanced than others? Psychological bulletin
143, 1 (2017), 1.

[6] Daniela Soares Cruzes, Martin Gilje Jaatun, Karin Bernsmed, and Inger Anne
TÃ¸ndel. 2018. Challenges and experiences with applying Microsoft threat mod-
eling in agile development projects. In Proceedings of the Australasian Software
Engineering Conference (ASWEC). IEEE, 111â€“120.

[7] Cyversity. 2021. The International Consortium of Minority Cyber Professionals

(non-profit). https://www.cyversity.org/. (Accessed on 07/01/2022).

[8] Graham D Fenwick and Derrick J Neal. 2001. Effect of gender composition on

group performance. Gender, Work & Organization 8, 2 (2001), 205â€“225.

[9] Laurie Giddens, Laura C Amo, and Dianna Cichocki. 2020. Gender bias and
the impact on managerial evaluation of insider security threats. Computers &
Security 99 (2020), 102066.

[10] Per E Gustafsod. 1998. Gender Differences in risk perception: Theoretical and

methodological erspectives. Risk analysis 18, 6 (1998), 805â€“811.

[11] Hanan Hibshi, Travis D Breaux, and Stephen B Broomell. 2015. Assessment of risk
perception in security requirements composition. In 2015 IEEE 23rd International
Requirements Engineering Conference (RE). IEEE, 146â€“155.

[12] Astrid C Homan, Claudia Buengeler, Robert A Eckhoff, Wendy P van Ginkel, and
Sven C Voelpel. 2015. The interplay of diversity training and diversity beliefs on
team creativity in nationality diverse teams. Journal of Applied Psychology 100, 5
(2015), 1456.

[13] Martin HÃ¶st, BjÃ¶rn Regnell, and Claes Wohlin. 2000. Using students as subjectsâ€”a
comparative study of students and professionals in lead-time impact assessment.
Empirical Software Engineering 5, 3 (2000), 201â€“214.

[14] Nasif Imtiaz, Justin Middleton, Joymallya Chakraborty, Neill Robson, Gina Bai,
and Emerson Murphy-Hill. 2019.
Investigating the effects of gender bias on
GitHub. In 2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 700â€“711.

[15] Eric Jardine. 2020. The Case against Commercial Antivirus Software: Risk Home-
ostasis and Information Problems in Cybersecurity. Risk Analysis 40, 8 (2020),
1571â€“1588.

[16] Johannes G Jaspersen and Gilberto Montibeller. 2015. Probability elicitation
under severe time pressure: A rank-based method. Risk Analysis 35, 7 (2015),
1317â€“1335.

[17] Katsiaryna Labunets, Fabio Massacci, and Federica Paci. 2017. On the equivalence
between graphical and tabular representations for security risk assessment. In
International Working Conference on Requirements Engineering: Foundation for
Software Quality. Springer, 191â€“208.

The Role of Diversity in Cybersecurity Risk Analysis: An Experimental Plan

GE@ICSEâ€™22, May 20, 2022, Pittsburgh, PA, USA

[18] Katsiaryna Labunets, Fabio Massacci, Federica Paci, et al. 2013. An experimental
comparison of two risk-based security methods. In Proceedings of the International
Symposium on Empirical Software Engineering and Measurement. IEEE, 163â€“172.
[19] Engla Ling, Robert LagerstrÃ¶m, and Mathias Ekstedt. 2020. A systematic liter-
ature review of information sources for threat modeling in the power systems
domain. In International Conference on Critical Information Infrastructures Security.
Springer, 47â€“58.

[20] Mass Soldal Lund, BjÃ¸rnar Solhaug, and Ketil StÃ¸len. 2010. Model-driven risk

analysis: the CORAS approach. Springer Science & Business Media.

[21] Christopher Mendez, Lara Letaw, Margaret Burnett, Simone Stumpf, Anita Sarma,
and Claudia Hilderbrand. 2019. From GenderMag to InclusiveMag: An inclusive
design meta-method. In 2019 IEEE Symposium on Visual Languages and Human-
Centric Computing (VL/HCC). IEEE, 97â€“106.

[43] Mihaela Vorvoreanu, Lingyi Zhang, Yun-Han Huang, Claudia Hilderbrand, Zoe
Steine-Hanson, and Margaret Burnett. 2019. From gender biases to gender-
inclusive design: An empirical investigation. In Proceedings of the 2019 CHI Con-
ference on Human Factors in Computing Systems. 1â€“14.

[44] Yang Wang and Stefan Wagner. 2018. On groupthink in safety analysis: An in-
dustrial case study. In Proceedings of the 40th International Conference on Software
Engineering: Software Engineering in Practice. 266â€“275.

[45] Alan Waring. 2015. Managerial and non-technical factors in the development of
human-created disasters: A review and research agenda. Safety science 79 (2015),
254â€“267.

[46] George Wright, Fergus Bolger, and Gene Rowe. 2002. An empirical test of the
relative validity of expert and lay judgments of risk. Risk Analysis: An International
Journal 22, 6 (2002), 1107â€“1122.

[22] Michael Meyners. 2012. Equivalence testsâ€“A review. Food quality and preference

[47] Alison T Wynn and Shelley J Correll. 2017. Gendered perceptions of cultural and

26, 2 (2012), 231â€“245.

skill alignment in technology companies. Social Sciences 6, 2 (2017), 45.

[23] Larissa Myaskovsky, Emily Unikel, and Mary Amanda Dew. 2005. Effects of
gender diversity on performance and interpersonal behavior in small work groups.
Sex Roles 52, 9 (2005), 645â€“657.

[24] Mathias Wullum Nielsen, Sharla Alegria, Love BÃ¶rjeson, Henry Etzkowitz, Holly J
Falk-Krzesinski, Aparna Joshi, Erin Leahey, Laurel Smith-Doerr, Anita Williams
Woolley, and Londa Schiebinger. 2017. Opinion: Gender diversity leads to better
science. Proceedings of the National Academy of Sciences 114, 8 (2017), 1740â€“1742.
[25] Anna Olofsson and Saman Rashid. 2011. The white (male) effect and risk percep-
tion: can equality make a difference? Risk Analysis: An International Journal 31,
6 (2011), 1016â€“1032.

[26] Justin Pence and Zahra Mohaghegh. 2020. A Discourse on the Incorporation of
Organizational Factors into Probabilistic Risk Assessment: Key Questions and
Categorical Review. Risk Analysis 40, 6 (2020), 1183â€“1211.

[27] Maryam Razavian and Patricia Lago. 2015. Feminine expertise in architecting

teams. IEEE Software 33, 4 (2015), 64â€“71.

[28] Gema RodrÃ­guez-PÃ©rez, Reza Nadri, and Meiyappan Nagappan. 2021. Perceived
diversity in software engineering: a systematic literature review. Empirical
Software Engineering 26, 5 (2021), 1â€“38.

[29] Per Runeson. 2003. Using students as experiment subjectsâ€“an analysis on gradu-
ate and freshmen student data. In Proceedings of the International Conference on
Empirical Assessment in Software Engineering. 95â€“102.

[30] Vineet Saini, Qiang Duan, and Vamsi Paruchuri. 2008. Threat modeling using
attack trees. Journal of Computing Sciences in Colleges 23, 4 (2008), 124â€“131.
[31] Iflaah Salman, Ayse Tosun Misirli, and Natalia Juristo. 2015. Are students repre-
sentatives of professionals in software engineering experiments?. In Proceedings
of the International Conference on Software Engineering-Volume 1. IEEE Press,
666â€“676.

[32] Adam Shostack. 2014. Threat Modeling: Designing for Security. John Wiley &

Sons. 590 pages.

[33] Maria Spichkova, Heinz Schmidt, and Catia Trubiani. 2017. Role of women in
software architecture: an attempt at a systematic literature review. In Proceedings
of the 11th European Conference on Software Architecture: Companion Proceedings.
31â€“34.

[34] Jakita O Thomas, Nicole Joseph, Arian Williams, Jamika Burge, et al. 2018. Speak-
ing truth to power: Exploring the intersectional experiences of Black women in
computing. In 2018 Research on Equity and Sustained Participation in Engineering,
Computing, and Technology (RESPECT). IEEE, 1â€“8.

[35] Katja Tuma, GÃ¼l Calikli, and Riccardo Scandariato. 2018. Threat analysis of
software systems: A systematic literature review. Journal of Systems and Software
144 (2018), 275â€“294.

[36] Katja Tuma, Christian Sandberg, Urban Thorsson, Mathias Widman, Thomas
Herpel, and Riccardo Scandariato. 2021. Finding security threats that matter:
Two industrial case studies. Journal of Systems and Software 179 (2021), 111003.
[37] Katja Tuma and Riccardo Scandariato. 2018. Two Architectural Threat Analysis
Techniques Compared. In Proceedings of the European Conference on Software
Architecture (ECSA). Springer, 347â€“363.

[38] Eric Luis Uhlmann and Geoffrey L Cohen. 2007. â€œI think it, therefore itâ€™s trueâ€:
Effects of self-perceived objectivity on hiring discrimination. Organizational
Behavior and Human Decision Processes 104, 2 (2007), 207â€“223.

[39] Romy Van der Lee and Naomi Ellemers. 2015. Gender contributes to personal
research funding success in The Netherlands. Proceedings of the National Academy
of Sciences 112, 40 (2015), 12349â€“12353.

[40] Romy van der Lee and Naomi Ellemers. 2018. Perceptions of gender inequality
in academia: Reluctance to let go of individual merit ideology. In Belief Systems
and the Perception of Reality. Routledge, 63â€“78.

[41] Hans Van Dijk, Marloes L Van Engen, and Daan Van Knippenberg. 2012. De-
fying conventional wisdom: A meta-analytical examination of the differences
between demographic and job-related diversity relationships with performance.
Organizational Behavior and Human Decision Processes 119, 1 (2012), 38â€“53.
[42] Dimitri Van Landuyt and Wouter Joosen. 2021. A descriptive study of assumptions
in STRIDE security threat modeling. Software and Systems Modeling (2021), 1â€“18.

7

