2
2
0
2

n
u
J

9
2

]
E
S
.
s
c
[

1
v
1
0
7
4
1
.
6
0
2
2
:
v
i
X
r
a

Debiasing architectural decision-making: a
workshop-based training approach

Klara Borowa1[0000−0002−7160−5950], Maria Jarek1, Gabriela Mystkowska1,
Weronika Paszko1, and Andrzej Zalewski1[0000−0001−5254−4761]

Warsaw University of Technology, Institute of Control and Computation Engineering,
Warsaw, Poland
klara.borowa@pw.edu.pl

Abstract. Cognitive biases distort the process of rational decision-making,
including architectural decision-making. So far, no method has been em-
pirically proven to reduce the impact of cognitive biases on architectural
decision-making. We conducted an experiment in which 44 master’s de-
gree graduate students took part. Divided into 12 teams, they created
two designs – before and after a debiasing workshop. We recorded this
process and analysed how the participants discussed their decisions. In
most cases (10 out of 12 groups), the teams’ reasoning improved after the
workshop. Thus, we show that debiasing architectural decision-making is
an attainable goal and provide a simple debiasing treatment that could
easily be used when training software practitioners.

Keywords: Cognitive bias · Architectural decisions · Debiasing

1

Introduction

Cognitive bias is a term that describes an individual’s inability to reason en-
tirely rationally; as such, it prejudices the quality of numerous decisions [18].
Researchers have observed the inﬂuence of cognitive biases on day-to-day soft-
ware development over two decades ago [13]. Since then, it was proven that
almost all software development activities are aﬀected by cognitive biases to
some extent [9]. Architectural decision-making in particular is not exempt from
the inﬂuence of cognitive biases [14], [18]. However, research on debiasing ar-
chitectural decision-making is scarce [9], with a clear lack of empirically proven
debiasing methods that could be used in practice [10]

In this paper, we endeavour to create an eﬀective debiasing treatment, through
expanding on our previous work [1]. The debiasing treatment that we designed
consists of an hour-long workshop during which individuals learn about cognitive
biases in architectural decision-making (ADM) and take part in three practical
exercises. We tested the eﬀectiveness of this debiasing treatment in an exper-
iment, in which 44 master’s level graduate students took part. Our study was
aimed at answering the following research question:

RQ. Is a training workshop an eﬀective method of reducing the

impact of cognitive biases on architectural decision-making?

 
 
 
 
 
 
2

K. Borowa et al.

Through our study, we show that debiasing ADM is an attainable goal, since
in most cases (10 groups out of 12) the debiasing treatment was successful. Our
workshop provides a debiasing eﬀect and, because of its simplistic design – it can
easily be used to train software practitioners to make more rational decisions.

This paper is organised as follows. In Section 2 we describe research related
to the subject of our study. Section 3 presents the research method, and in
particular: the design of the debiasing workshop, our experiment, the study
participants and how we analysed the obtained data. Section 4 contains the
results of our experiment. In Section 5 we discuss our ﬁndings. The threats to
validity are explained in Section 6. Finally, in Section 7 we provide a conclusion
and describe possible future work.

2 Related work

Cognitive biases impact how decisions are made by every human being. In par-
ticular, they heavily inﬂuence intuitive decisions made under uncertainty [17].
This eﬀect occurs due to the dual nature of the human mind, which comprises
intuitive and rational decision-making subsystems [8]. Fischoﬀ [5] describes four
levels of debiasing (reducing the eﬀect of biases) treatments: (A) warning about
the biases, (B) describing typical biases, (C) providing personalised feedback
about the biases, (D) an extended programme of debiasing training.
Cognitive biases inﬂuence on architectural decision-making. Tang [14]
described how distorted reasoning may impact software design, by providing a
set of examples of biased statements that software designers may use during
their work [14]. As software architecture is actually a set of design decisions
[7], it may be heavily aﬀected by architects’ biases. This makes reducing the
impact of biased decision-making an important endeavour in the area of software
architecture (see also [18], [19]).
Debiasing architectural decision-making. Although there are various guide-
lines and practices for improving architectural decision-making [16], [15], there
is a severe lack of empirical research on treatments for undesirable behavioural
factors in the realm of ADM [10]. There is a small amount of research on debi-
asing in Software Engineering. So far, the existing research has rarely proposed
debiasing approaches, and empirical validation of the proposed debiasing meth-
ods [9] is even less frequent. Notably, Shepperd et al. [12] proposed a successful
treatment that improved software eﬀort estimation, through a two- to three-
hour-long workshop about cognitive biases. Our team attempted an empirical
validation of an anti-bias treatment for ADM [1], but it turned out not to be
successful. However, it had several major weaknesses:

1. We informed the participants about biases through a simple presentation.
This treatment is on the lower levels (A and B) of Fischoﬀ’s debiasing scale
[5]. In comparison, the successful treatment proposed by Shepperd et al. [12]
included a workshop and giving personalised feedback (level C debiasing).
2. In order to evaluate whether the treatment provided the desired eﬀect, we
compared the performance of two groups of students – one that was shown

Debiasing architectural decision-making

3

the presentation, and one that was not. However, this approach does not take
into account the teams’ individual traits. Those traits may make them more
or less susceptible to cognitive biases from the start. It is possible that, when
comparing a single team’s performance before and after the presentation, the
results may have been signiﬁcantly diﬀerent.

3. The sample (2 groups consisting of 5 students) was rather small.

This paper summarises our subsequent research that was aimed at developing a
successful debiasing treatment by overcoming the above shortcomings.

3 Research Method

The three-hour-long experiment was performed during a meeting on the MS
Teams platform. The experiment plan has been made available online [2]. While
planning the experiment, we enhanced most steps from our previous approach [1]
to both improve the debiasing treatment itself and the validity of the experiment.
The basic steps of the experiment included:

1. Preparing the debiasing workshop.
2. Gathering participants.
3. A series of three-hour long meetings during which we conducted the experi-

ment, which consisted of three steps:
(a) Task 1 – a 1 hour-long ADM task.
(b) The debiasing workshop.
(c) Task 2 – a 1 hour-long ADM task.

4. Analysing the teams’ performance during the ﬁrst and second tasks.

Biases The debiasing workshop was designed to counter three biases that in
previous research turned out to be exceptionally inﬂuential on architectural de-
cisions [18], [3] and their impact on software engineering overall has already been
researched extensively [9] :

1. Anchoring – a biased preference towards initial starting points, ideas, solu-

tions [17].

2. Conﬁrmation bias – when the currently desired conclusion leads the individ-
ual to search for conﬁrming evidence, or omitting other information [19].
3. Optimism bias – an inclination towards overly optimistic predictions and

judgements [9].

Architectural decision-making task Each team of participants performed
the task twice: before and after the debiasing workshop. The theme (the problem
that was to be solved) was diﬀerent in each task. The task was to design an
architecture that could be used as a solution to a given theme, and to record
the design using the C4 model notation [4]. The task itself was known to the
participants before they took part in the experiment, in order to allow them

4

K. Borowa et al.

to prepare and learn more about the C4 model. This was not the case for the
themes. All the tasks were supposed to be graded as part of the students’ software
architecture course. However, the students were given over a week after the
experiment to ﬁnish and polish their design. During both architectural design
tasks, the researchers did not take an active part in the architecting.

Debiasing Workshop Design The full workshop plan with instruction for
workshop organisers have been made available online [2]. The workshop was
designed to teach three debiasing techniques:

– The anti-anchoring technique: having proposed an architectural solution,
the individual that presents it must explicitly list one disadvantage of the
solution.

– The anti-conﬁrmation bias technique: one team member has to monitor the
discussion for unjustiﬁed statements that dismiss new information and ideas.
Such as “We already decided that”.

– The anti-optimism bias technique: the team must explicitly mention the risks

associated with the design decisions.

These speciﬁc techniques were proposed previously as a result of our previous
work [1] where we analysed, in detail, how each of the three researched biases
usually impacted the teams that took part in the study. However, during that
study, the eﬀectiveness of these techniques was not validated. For each of these
three techniques, the participants had to actively perform a practical exercise. In
this phase of the experiment, the researchers actively facilitated the workshop
by encouraging participants to use the debiasing techniques, providing them
with examples of the techniques’ use, and prompting the participants when they
forgot to use the technique that they were supposed to apply.

Sample The participants were recruited from among master’s level graduate
students majoring in Computer Science in our Faculty. These graduate students
in particular, were taking a Software Architecture course. Although participation
could be part of their graded project, it was voluntary. There was an alterna-
tive, traditional way, to obtain a grade. At the start of the MS Teams meeting,
participants ﬁlled a questionnaire that allowed us to obtain basic data about
them. Overall, 61% of the participants had prior experience in software develop-
ment, ranging from 0.3 to 3 years. The questionnaire and its results, containing
detailed information about the participants, is available online [2].

Analysis For the analysis, we used a modiﬁed approach of our method from our
previous study [1]. In order to analyse the results, we transcribed all recordings
of the tasks, during which the participants’ created their design. In order to
inspect how biases impacted architectural decisions, we applied the hypothesis
coding method [11]. This means that we deﬁned a set of codes to be used to
mark relevant segments in the transcript in advance, prior to the analysis. The
coding scheme and all speciﬁc code counts have been made available online [2].

Debiasing architectural decision-making

5

Each transcript was ﬁrst coded by two researchers separately. Then, all of
the codes were negotiated [6] until the coders reached a consensus on each code.
Additionally, no transcript was coded by the same researcher that conducted
the particular meeting with the participants. Furthermore, we summarised the
overall number of codes only twice, after coding 6 and 12 transcripts, to avoid a
situation where we would unconsciously chase after a desired number of biased
or non-biased arguments in a particular transcript.

Having coded the transcripts, we compared how many biased and non-biased
statements/decisions were present before and after the workshop. We deﬁned:
(a) a biased decision as one impacted by more biased statements than rational
arguments, (b) a non-biased decision as one impacted by more rational argu-
ments than biased statements, (c) neutral decisions as ones impacted by an
equal amount of biased and non-biased statements. We also counted the amount
of bias inﬂuences and the usage of the debiasing techniques during the tasks.

4 Results

Through the analysis process we uncovered the speciﬁcs about arguments, de-
cisions, bias occurrences and the use of debiasing techniques in the teams’ Task
1 and Task 2 transcripts. All p-values mentioned in this section were calculated
using the non-parametric Wilcoxon Signed Rank Test. Through this test, we
evaluated whether the changes in speciﬁc measured values were statistically dif-
ferent (when the p-value was less than 0.05). All speciﬁc numbers for code counts
for each team are available online [2].

Arguments. We classiﬁed these arguments as either biased (i.e. aﬀected by
one or more of the researched biases) or non-biased. Overall, we found 1470
arguments and 487 counterarguments. 54% of the statements before the work-
shop were biased, compared to 36% after. In general, the percentage of biased
arguments decreased after the workshop in the cases of all teams except one.

The increased number of non-biased arguments (p-value = 0.0024) and non-
biased counterarguments (p-value = 0.0005), and the decrease of the percentage
of biased statements (p-value = 0.002) were signiﬁcant. However, the changes
in the number of biased arguments (p-value = 0.1973) and counterarguments
(p-value = 0.8052) can not be considered signiﬁcant.

Decisions. Overall we found 641 decisions - 266 biased, 281 non-biased and 94
neutral. 52% of decisions before the workshop were biased, compared to 31%
after. Only one had a larger percentage of biased decisions after the workshop.
In the case of all the other teams, the percentage of biased decisions decreased.
The increase in the number of non-biased decisions (p-value = 0.0024) and the
decreased percentage of biased decisions (p-value = 0.0020) were signiﬁcant.
However, the change in the number of biased decisions (p-value = 0.0732) can
not be considered signiﬁcant.

6

K. Borowa et al.

Cognitive biases. Overall, we found 1110 bias occurrences - 558 before and
552 after the workshop. The sum of these counts is diﬀerent from the number of
arguments since: (a) it was possible for various biases to inﬂuence one argument,
(b) some biased statements were not connected to any architectural decision.

There was no signiﬁcant change in the overall number of biases between Task
1 and Task 2 (p-value = 0.8647). This means that the debiasing eﬀect (the smaller
percentage of biased decisions and arguments) was not achieved by decreasing
the number of bias occurrences during the tasks. In fact, the eﬀectiveness of the
debiasing treatment comes from increasing the number of non-biased arguments.

Debiasing techniques. We compared the amounts of technique uses before
and after the workshop, since it was possible for participants to spontaneously
use a speciﬁc technique during Task 1. We identiﬁed 133 uses of the proposed
techniques - 26 techniques before and 107 after the workshop.

The number of uses of the practices increased signiﬁcantly during Task 2 (p-
value = 0.0005). However, three teams did not increase their use of the anti-bias
techniques substantially after the workshop. Despite this, these teams’ percent-
age of biased arguments and decisions decreased during Task 2. Additionally,
two teams, despite using a higher number of debiasing techniques during Task
2, had more biased decisions and more biased arguments during Task 2.

Overall, the anti-optimism technique was used most often (15 before and 57
after workshop), while the anti-anchoring technique was used less often (3 before
and 30 after workshop), with the anti-conﬁrmation bias technique rarely being
used at all (8 before and 20 after workshop). This may be because listing risks
came most naturally, while the other two techniques may require much more
eﬀort to be used correctly.

5 Discussion

Our results show that the debiasing treatment through the debiasing workshop
we designed was successful, both improving the quality of argumentation and
design decisions. However, there are some particularities worth discussing in
detail, which may help to signiﬁcantly improve our approach in the future.

Firstly, our approach did not signiﬁcantly decrease the number of cognitive
bias occurrences, biased arguments and biased decisions that impacted our par-
ticipants (see Section 4). Instead, we managed to improve the number of rational
arguments and decisions present in the teams’ discussions, through which the
percentage of biased arguments and decisions decreased. This means that, while
it may not be possible to completely get rid of cognitive biases, other ways of
rationalising decision-making are possible and could be pursued.

Secondly, the team whose decisions improved the most was the one that had
the worst result in Task 1. Furthermore, the team that improved the least was
the one with the best result in Task 1. This may mean that, while our treatments
successfully improve the performance of initially biased individuals, it may not
be as impactful in the case of individuals that were initially less impacted by

Debiasing architectural decision-making

7

biases. Since our participants were students with up to three years of professional
experience, we do not know yet how diﬀerent the debiasing eﬀect would be on
experienced practitioners (who may be initially less impacted by biases).

Finally, the failure of two teams led us to explore the transcripts in detail.
After that, we noticed that their performance dropped at one point, when the
participants simply became tired (which they expressed verbally). This is in line
with Kahneman’s [8] explanation for the existence of two systems – using System
2 is physically exhausting and no human can use it indeﬁnitely. Thus, time for
rest may be a crucial factor to bear in mind while attempting debiasing.

6 Threats to Validity

Conclusion validity: We used non-parametric tests to examine whether the
observed changes in the measured values were signiﬁcant.

Internal validity: We put signiﬁcant care into designing the experiment and
setting the environment so that no factors other than the workshop inﬂuenced
the students. The teams did not know the themes for their tasks before the
study, and did not have more than 10 minutes of time to interact with the
environment outside during the experiment. Finally, to decrease the chances of
the researchers distorting the results during the analysis, we used negotiated
coding [6] and calculated the results (code counts) for the transcripts only twice.
Construct validity: Our method also improves on the one used in our
previous study [1] by taking into account not only arguments and biases but
also decisions. This factor is crucial since it is possible that, while the overall
argumentation may improve, a team can lack regularity when using rational
arguments, thus still making numerous biased decisions nonetheless.

External validity: Our study’s weakness is that our participants were all

students. While most of them had professional experience, it was limited.

7 Conclusion and Future Work

In this paper we explored whether debiasing through a training workshop is an
eﬀective method of reducing the impact of cognitive biases on ADM (RQ). We
designed such a workshop and examined its eﬀectiveness (Section 3). The results
show that the debiasing treatment is eﬀective (Section 4), although it does not
completely eliminate the impact of the biases (Section 5).

Through this work, we show that designing a successful debiasing treatment
for cognitive biases in ADM is possible, and propose an eﬀective treatment that
can become a foundation for future research. Future research can focus on: test-
ing diﬀerent debiasing techniques, debiasing that takes into account other cog-
nitive biases, exploring the workshop’s eﬀectiveness in debiasing experienced
practitioners.

Practitioners can use the presented debiasing workshop for training purposes.

8

K. Borowa et al.

References

1. Borowa, K., Dwornik, R., Zalewski, A.: Is knowledge the key? an experiment on
debiasing architectural decision-making-a pilot study. In: International Conference
on Product-Focused Software Process Improvement. pp. 207–214. Springer (2021)
2. Borowa, K., Jarek, M., Mystkowska, G., Paszko, W., Zalewski, A.: Ad-
ditional Material
for Debiasing architectural decision-making: a workshop-
based training approach (Jun 2022). https://doi.org/10.5281/zenodo.6751990,
https://doi.org/10.5281/zenodo.6751990

3. Borowa, K., Zalewski, A., Kijas, S.: The inﬂuence of cognitive biases on archi-
tectural technical debt. In: 2021 IEEE 18th International Conference on Software
Architecture (ICSA). pp. 115–125. IEEE (2021)

4. Brown,

S.: The

c4 model

for

software

architecture

(Jun

2018),

https://www.infoq.com/articles/C4-architecture-model/

5. Fischhoﬀ, B.: Debiasing. judgment under uncertainty: Heuristics and biases. Judg-

ment under uncertainty: Heuristics and biases pp. 422–444 (1982)

6. Garrison, D.R., Cleveland-Innes, M., Koole, M., Kappelman, J.: Revisiting
methodological issues in transcript analysis: Negotiated coding and reliability. In-
ternet and Higher Education 9(1), 1–8 (2006)

7. Jansen, A., Bosch, J.: Software architecture as a set of architectural design
decisions. In: 5th Working IEEE/IFIP Conference on Software Architecture
(WICSA’05). pp. 109–120. IEEE (2005)

8. Kahneman, D.: Thinking, fast and slow. Macmillan (2011)
9. Mohanani, R., Salman, I., Turhan, B., Rodriguez, P., Ralph, P.: Cognitive Biases
in Software Engineering: A Systematic Mapping Study. IEEE Transactions on
Software Engineering 5589(c) (2018)

10. Razavian, M., Paech, B., Tang, A.: Empirical research for software architecture
decision making: An analysis. Journal of Systems and Software 149, 360–381 (2019)
11. Salda˜na, J.: The coding manual for qualitative researchers. The coding manual for

qualitative researchers pp. 1–440 (2021)

12. Shepperd, M., Mair, C., Jørgensen, M.: An Experimental Evaluation of a De-
biasing Intervention for Professional Software Developers. In: Proceedings of the
33rd Annual ACM Symposium on Applied Computing (2018)

13. Stacy, W., Macmillan, J.: Cognitive Bias in Software Engineering. Communications

of the ACM 38(6), 57–63 (1995)

14. Tang, A.: Software designers, are you biased? In: Proceedings of the 6th interna-

tional workshop on sharing and reusing architectural knowledge (2011)

15. Tang, A., Bex, F., Schriek, C., van der Werf, J.M.E.: Improving software design
reasoning–a reminder card approach. Journal of Systems and Software 144, 22–40
(2018)

16. Tang, A., Kazman, R.: Decision-making principles for better software design deci-

sions. IEEE Software 38(6), 98–102 (2021)

17. Tversky, A., Kahneman Daniel: Judgment under uncertainty: Heuristics and biases.

Science (1974)

18. Van Vliet, H., Tang, A.: Decision making in software architecture. Journal of Sys-

tems and Software 117, 638–644 (2016)

19. Zalewski, A., Borowa, K., Ratkowski, A.: On cognitive biases in architecture de-
cision making. In: European Conference on Software Architecture. pp. 123–137.
Springer (2017)

