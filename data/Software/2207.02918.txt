2
2
0
2

n
u
J

2
2

]
E
S
.
s
c
[

1
v
8
1
9
2
0
.
7
0
2
2
:
v
i
X
r
a

Heterogeneous Anomaly Detection for Software Systems via
Attentive Multi-modal Learning

Baitong Li
cheryllee626@gmail.com
The Chinese University of Hong Kong
Hong Kong, China

Tianyi Yang
tyyang@cse.cuhk.edu.hk
The Chinese University of Hong Kong
Hong Kong, China

Zhuangbin Chen
zbchen@cse.cuhk.edu.hk
The Chinese University of Hong Kong
Hong Kong, China

Yuxin Su‚àó
suyx35@mail.sysu.edu.cn
Sun Yat-sen University
Guangzhou, Guangdong, China

Yongqiang Yang
yangyongqiang@huawei.com
Huawei Cloud BU
China

Michael R. Lyu
lyu@cse.cuhk.edu.hk
The Chinese University of Hong Kong
Hong Kong, China

ABSTRACT
Prompt and accurate detection of system anomalies is essential to
ensure the reliability of software systems. Unlike manual efforts
that exploit all available run-time information, existing approaches
usually leverage only a single type of monitoring data (often logs or
metrics) or fail to make effective use of the joint information among
multi-source data. Consequently, many false predictions occur. To
better understand the manifestations of system anomalies, we con-
duct a comprehensive empirical study based on a large amount of
heterogeneous data, i.e., logs and metrics. Our study demonstrates
that system anomalies could manifest distinctly in different data
types. Thus, integrating heterogeneous data can help recover the
complete picture of a system‚Äôs health status. In this context, we pro-
pose HADES, the first work to effectively identify system anomalies
based on heterogeneous data. Our approach employs a hierarchical
architecture to learn a global representation of the system status by
fusing log semantics and metric patterns. It captures discriminative
features and meaningful interactions from multi-modal data via
a novel cross-modal attention module, enabling accurate system
anomaly detection. We evaluate HADES extensively on large-scale
simulated and industrial datasets. The experimental results present
the superiority of HADES in detecting system anomalies on het-
erogeneous data. We release the code and the annotated dataset for
reproducibility and future research.

1 INTRODUCTION
Recent years have witnessed the scale and complexity of software
systems expand dramatically. However, anomalies are inevitable
in large-scale software systems, resulting in considerable revenue
and reputation loss [36]. The core competence of service providers
stems from guaranteeing the reliability of software systems, where
automated anomaly detection is a primary step and has been picked
up extensively within the community. In real-world scenarios, many
types of monitoring data, including system/software metrics, logs,
alerts, and traces, play an essential role in software reliability en-
gineering [8, 34]. In particular, metrics and logs have been widely
used for anomaly detection. Metrics (e.g., response time, number
of threads, CPU usage) are real-valued time series measuring the
system status. Logs are semi-structured text messages printed by
logging statements to record the system‚Äôs run-time status.

*Corresponding Author.

Tremendous efforts have been devoted to detecting anomalies
automatically since manual troubleshooting is impractical and error-
prone. Some approaches rely on metrics [10, 24, 43, 45], while
others rely on logs [14, 29, 38, 51]. However, as a single source
of information is often insufficient to depict the status of a soft-
ware system precisely, existing methods could produce many false
predictions [40]. The situation becomes even worse in large-scale
distributed systems, where anomaly patterns are more complex.
Intuitively, the combination of multiple sources of monitoring data
can allow fuller utilization of run-time information to analyze the
system status holistically.

To verify our intuition and understand how heterogeneous mon-
itoring data characterize the system status, we conduct a compre-
hensive empirical study based on large amount of multi-source
data. The data are generated via fault injection on Apache Spark,
where we run various workloads and inject 21 typical types of
faults. The resulting dataset contains 1,435,139 log messages and 11
monitoring metrics with a length of 64,422. Compared to existing
open-access datasets, e.g., [15, 22, 40, 41], ours possesses temporally
aligned heterogeneous run-time information with rich semantics
and annotations (i.e., abnormal or not).

Our study reveals three interesting findings. First, although the
presence of critical logs often indicates problems, their absence
does not necessarily imply a healthy system status (RQ1). An im-
portant reason is that it is sometimes hard to determine where and
how to place an informative log statement [7]. Second, in some
cases, failures do not affect metrics, while in other cases, metrics
exhibit unusual patterns even if the system is experiencing minor
performance fluctuations instead of failures. So it is insufficient to
infer the system status only by metrics (RQ2). Third, faults can
cause unexpected behaviors involving either logs or metrics, or
both of them. Hence, the two data sources should be analyzed com-
prehensively to reveal the actual anomalies (RQ3). These insights
necessitate considering heterogeneous multi-source data for anom-
aly detection, and high-level patterns (i.e., log semantics and metric
patterns) inside the observed data deserve our full attention.

However, we identify two major challenges to building a model
that can extract and integrate essential information from heteroge-
neous data. (1) Complex intra-modal information. Logs reflect
system anomalies mainly through their semantics (e.g., keywords)
and sequential dependencies across events. Besides, metrics reflect
different aspects of the system performance, and metrics belonging

 
 
 
 
 
 
ACM Conference, 2022,

Baitong Li, Tianyi Yang, Zhuangbin Chen, Yuxin Su, Yongqiang Yang, and Michael R. Lyu

to different aspects tend to develop distinct behavioral patterns.
For example, when a system works normally, the disk usage of-
ten moves steadily, but the CPU usage can fluctuate dramatically.
Such complicated and diverse information introduces complexity
in feature extraction, requiring the model to be highly competent in
complicated information processing. (2) Significant inter-modal
gap. Logs and metrics are in different forms, i.e., textual and time-
series. Such a discrepancy poses a huge challenge to effectively
using the joint information for downstream anomaly detection. To
this end, it is critical to align the log semantics and metric patterns.
To tackle these challenges, we propose HADES, a Heterogeneous
Anomaly DEtector for Software systems, which is the first work to
to investigate anomaly detection with heterogeneous data sources.
The key idea is to learn a discriminative representation of the system
status based on logs and metrics. HADES first captures intra-modal
dependencies using a hierarchical architecture. Then it generates a
global representation of the most discriminative latent information
of logs and metrics via a modal-wise attentive fusion module. More
specifically, HADES involves four components: (1) For logs, we
adopt the FastText algorithm [6] and the Transformer encoder [46]
to model lexical semantics and sequential dependencies of logs. (2)
In terms of metrics, we employ a hierarchical encoder to learn met-
ric representations based on the causal convolution network [30]. It
jointly learns aspect-oriented temporal dependencies, cross-metric
relationships, and inter-aspect correlations. (3) We design a novel
modal-wise attention mechanism to facilitate learning a global rep-
resentation retaining meaningful intra- and inter-modal properties.
(4) Finally, the framework infers the application-agnostic system
status and triggers an alarm upon detecting system anomalies.

We evaluate HADES using both simulated (Dataset A) and indus-
trial (Dataset B and C) datasets. The experimental results demon-
strate the superiority of HADES, which achieves an average F1-
score of 0.93, outperforming all state-of-the-art competitors, includ-
ing log-based and metric-based ones. In addition, the effectiveness
of our designs (including the exploitation of heterogeneous infor-
mation, multi-modal learning, attentive fusion, and hierarchical
feature extraction) is confirmed by extensive ablation experiments.

In summary, the main contributions of this paper are:

‚Ä¢ We conduct a comprehensive empirical study to investigate the
manifestation of system anomalies caused by practical faults. The
findings greatly motivate our work.

‚Ä¢ We propose a novel end-to-end approach, HADES, the first work
to detect system anomalies accurately and efficiently based on
heterogeneous monitoring data via attentive multi-modal learn-
ing. The code is publicly available [3] for reproducibility.

‚Ä¢ Extensive experiments on simulated and industrial datasets show
that HADES beats all compared approaches. Also, ablation studies
further validate the effectiveness of each design in our model.
‚Ä¢ Lastly, we release an annotated dataset on [3] containing hetero-
geneous run-time information with complex log semantics and
diverse metric patterns.

2 PROBLEM STATEMENT
To begin, we introduce some essential terminologies. A log message
is a line of the standard output of logging statements, composed
of constant strings (written by the developers) and variable values

(determined by the system) [19]. Parsing a log message is to remove
all variables to obtain a log event, which describes system run-time
events [21]. Log messages chronologically collected within a certain
time period constitute a log sequence. Metrics are the numerical
measurement of system performance that are sampled uniformly.
Consecutive points within a certain period make up a metric seg-
ment. By collecting both the logs and metrics in a given period of
length ùëá , we obtain a chuck with time-aligned heterogeneous data.
The value of ùëá is determined according to real-world requirements.
Anomalies are system abnormal behaviors, events, or observations
that do not conform to the expected patterns in the run-time in-
formation [21]. These anomalies often indicate system issues and
could evolve to errors or failures. Anomaly detection is the process
of identifying the software system anomalies.

ùëñ , ùëø ùíé

We formulate the task of heterogeneous anomaly detection as
follows. Given a data chunk, we need to determine the current
system status as abnormal or normal, denoted as 1 and 0, respec-
tively. Let ‚ü®ùëø1:ùëÅ , ùëå1:ùëÅ ‚ü© = {(ùëø1, ùë¶1), (ùëø2, ùë¶2), ..., (ùëøùëÅ , ùë¶ùëÅ )} be the
training data chunks with corresponding status labels, where ùëøùëñ =
ùëñ ), (ùëñ = 1, 2, ..., ùëÅ ) is the ùëñ-th data chunk, and ùë¶ùëñ ‚àà {0, 1}
(ùëø ùíç
denotes the status. In the ùëñ-th chunk, ùëø ùíç
ùëñ,ùêø]
denotes the log sequence, where ùêø is the number of log events gen-
erated during the period of length ùëá ; the metric segment is denoted
ùëñ,ùëá ] ‚àà Rùëá √óùëÄ , where ùëÄ and ùëá are the
by ùëø ùíé
number and length of monitoring metrics, respectively. The goal
is to model the relations behind ‚ü®ùëø1:ùëÅ , ùëå1:ùëÅ ‚ü©, and then for each
incoming unseen instance ùëãùëÅ +1, we can predict the status ùë¶ùëÅ +1.

ùëñ,1:ùëá = [ùíôùíé
ùëñ,1

ùëñ,1:ùêø = [ùíô ùíç
ùëñ,1

, ..., ùíôùíé

, ..., ùíô ùíç

, ùíô ùíç
ùëñ,2

, ùíôùíé
ùëñ,2

3 EMPIRICAL STUDY AND MOTIVATION
In practice, engineers usually analyze multiple sources of system
run-time information for troubleshooting. However, manual inspec-
tion is tedious and fallible, especially when facing massive data.
To explore the opportunity to automate this process, we conduct
a detailed empirical study on simulated and real-world industrial
datasets to understand how system anomalies affect the multi-
source monitoring data. Moreover, most industrial datasets are
access-restricted, and the publicly accessible data is often too small
or single-source due to security and privacy concerns. To allevi-
ate this problem, we have released our collected dataset on [3]. It
contains large-scale logs and metrics generated from a distributed
computing system, which underpins our study and will facilitate
the advancement and openness of the community.

3.1 Data Collection
The process of data collection comprises four steps: 1) deploy the
infrastructure, 2) conduct workloads to generate monitoring data,
3) inject typical faults to simulate industrial production anomalies,
and 4) collect multi-source monitoring data simultaneously.

Infrastructure and Data Generation. Apache Spark is a widely-
3.1.1
used framework for big data processing [17]. We deploy Spark 3.0.3
on a distributed systems cluster containing a master node and five
worker nodes (virtual nodes supported by Docker [39]) in our labo-
ratory environment. Then, we employ a big data benchmark suite,
HiBench [26], to conduct 22 kinds of workloads, involving basic
(e.g., word counting, sorting) and sophisticated (e.g., random forest,

Heterogeneous Anomaly Detection for Software Systems via Attentive Multi-modal Learning

ACM Conference, 2022,

k-means) applications. They are diverse in terms of resource usage,
including CPU (e.g., random forest performs complex computa-
tion), I/O (e.g., word counting requires transaction-intensive file
inputting), and network (e.g., sorting involves frequent data trans-
ferring). Unlike existing collections (e.g., [22, 44, 47]) running only
word counting, our data cover more service application scenarios.
We repeat each workload without fault injection seven times to
obtain sufficient data. In this paper, we refer to the run-time data as
standard data when no fault is injected during the entire workload.
We mainly collect two types of run-time data, i.e., logs and met-
rics. Logs are aggregated by Spark automatically, and metrics are
sampled (per second) and collected via an open-source monitoring
tool [12]. In the following analysis, we focus on 11 monitoring met-
rics reflecting four critical aspects of the system status (i.e., CPU,
I/O, memory, and network), for example, CPU system usage, device
read speed (‚Äúrkb/s‚Äù), memory usage, and network throughput rate.

Fault Injection. After gathering standard data, we injected 21
3.1.2
common types of faults into the workloads. These faults are selected
based on previous research [15, 44, 47] and our investigation on the
typical service failures at Huawei. Each fault lasts for one minute,
and the time from fault injection to fault clearance is called fault
duration. The data collected for the rest of the time is regarded as
fault-free. The injected faults fall into four categories:
‚Ä¢ Process suspension: suspend a process for one minute, includ-
ing the Application Worker, Master, Node Manager, Datanode,
Secondary Datanode, Resource Manager, and Namenode.

‚Ä¢ Process killing: kill each process once, including the Applica-
tion Worker, Master, Node Manager, Datanode, and Secondary
Datanode. Killing these processes does not terminate the running
workload immediately.

‚Ä¢ Resource stress: load the system by injecting CPU, I/O, memory,
and other resource hogs, supported by the stress-ng tool [27].
‚Ä¢ Network faults: use Linux traffic control to inject network faults
like dropping packets, high latency, and flashing disconnections.
Totally, (21 + 7) √ó 22 = 616 log files (with 1,435,139 valid messages)
and 11 metric time series (with a length of 64,422) are collected. It is
also worth noting that we obtain 118 log templates via Drain [20].

3.1.3 Data Annotation. We invite two Ph.D. students experienced
in software reliability as annotators. They are not aware of when
the faults are injected or cleaned, so the labeling is completely based
on the data without off-site information. The principle to label an
abnormal chunk is that the in-process data manifest discrepancies
from the standard data, e.g., error logs and unusual metric jitters.
Meanwhile, the abnormal manifestation should align with the ex-
pected impact of the injected fault, which is manually checked by
annotators. We do not simply regard all data generated during the
fault duration as abnormal because many faults can be immediately
tolerated by the system‚Äôs fault-tolerant mechanisms, incurring no
anomalies to the system. In this case, we treat the corresponding
logs and metrics as normal. The label is accepted if the two stu-
dents give the same label for one chunk independently. Otherwise,
they will discuss with a post-doc until reaching a consensus. The
inter-annotator agreement [11] achieves 0.864 before adjudication.

3.2 Empirical Study
To investigate how different types of run-time data manifest anom-
alies, we conduct an empirical study based on the above data. Specif-
ically, we aim to answer the following research questions (RQs):
‚Ä¢ RQ1: How do logs manifest system anomalies?
‚Ä¢ RQ2: How do metrics manifest system anomalies?
‚Ä¢ RQ3: How does run-time information reflect the system status?

3.2.1 RQ1: Log Manifestations of Anomalies. In general, logs are
not susceptible to system faults. Only 3.62% of positively labeled
chunks are anomalous from the log‚Äôs perspective. A typical example
provides a closer look. If the network drops some packets, the ser-
vice response becomes slow, but may not hit the timeout threshold,
thereby no anomalous log event will be reported. The main reason
lies in the inherent deficiency of logs. Logging [18, 21, 49, 50] is a
human activity heavily relying on developers‚Äô knowledge. While it
is relatively easy to write logs describing severe failures, subtle per-
formance issues, e.g., gray failures [23], are often hard to identify.
Thus, logs capturing such anomalies could be missing.

Digging into logs, we observe that the lexical semantics are note-
worthy. Specifically, the appearance of some log tokens indicates
anomalies. These tokens only occur in abnormal sequences, and
their semantics describe unusual system events, e.g., ‚Äúuncaught‚Äù
and ‚Äúexception‚Äù in the event ‚ÄúUncaught exception in thread‚Äù. This
observation is in line with the programming habits of developers,
as well as previous studies [9, 25, 51]. It also validates that log
semantics can reflect the current system status to some extent.

Moreover, the contexts of logs are crucial for detecting anomalies
since logs carry the information of program control flows [48]. For
example, a normal sequence of log events is an event reporting
the ‚Äúfinal application‚Äù state followed by the log event ‚ÄúShutdown
hook called‚Äù. When anomalies happen, the event ‚ÄúShutdown hook
called‚Äù may occur before reporting the ‚Äúfinal application‚Äù state. In
this case, the application state will be regarded as undefined or
failed because the master has not received the message informing
the application state. Hence, the contextual semantics of logs also
contain information that indicates whether the system is healthy.

Finding 1: Logs are relatively coarse-grained and cannot mani-
fest all system anomalies. Moreover, both lexical and contextual
semantics are important characteristics with respect to reflect-
ing the system status.

3.2.2 RQ2: Metric Manifestations of Anomalies. Metrics are more
responsive to anomalies as they continuously record fine-grained
system status. However, there are still some anomalous periods
ignored by many existing metric-based detectors.

Existing anomaly detectors usually try to identify novel metric
patterns through a comparison with normal system behaviors, i.e.,
novelty detection [13]. They mainly focus on local patterns (e.g.,
spikes, level shifts) rather than global patterns spanning the entire
workload. However, system faults are not necessarily manifested by
local novel patterns. Figure 1 displays an example that the metric
‚Äúrkb/s‚Äù during the fault ‚ÄúDatanode suspension‚Äù, as shown by the
red line. The metric ‚Äúrkb/s‚Äù is abnormal as a whole, but its local
patterns are hard to identify as anomalies. Specifically, the red line
remains zero after fault injection, which is unexpected as there

ACM Conference, 2022,

Baitong Li, Tianyi Yang, Zhuangbin Chen, Yuxin Su, Yongqiang Yang, and Michael R. Lyu

should have been I/O activities going on. The blue line depicts the
normal (standard) status as a comparison. However, the blue line
also remains zero after the time of 90s because the system does not
exchange data near the end of the workload. So ‚Äúrkb/s‚Äù in either
the normal or abnormal status can stay at zero for a while. This
indicates that a metric can behave very similarly when the system
is in the opposite status. Such patterns (may reflect the opposite
system statuses) will confuse most existing methods relying on
novel pattern mining, leading to performance degradation. The
inherent reason is that system metrics cannot completely reflect
the software‚Äôs inner executive logic. Fortunately, such anomalies
can be detected by referring to the logs, where suspicious events
like ‚ÄúException in createBlockOutputStream‚Äù are reported.

Figure 1: Suspending the Datanode incurs anomalies mani-
fested in the metric ‚Äúrkb/s‚Äù, but no novel pattern exists.

On the flip side, unusual metric fluctuations may trigger alarms
even when no anomaly exists currently. Among all metric segments
manually labeled as positive, 8.87% of them are collected in fault-free
periods, indicating that relying solely on metrics may cause false
alarms due to the overreactions of metrics. For example, Figure 2
displays that even in the fault-free period, the metric ‚ÄúCPU iowait‚Äù
still generates a rare heartbeat spike. Nevertheless, such sporadic
and transient fluctuation is acceptable without affecting the service,
thereby no alarm should be triggered. This case suggests that other
information should be involved to mitigate the issues caused by
the over-sensitivity of metrics to avoid unnecessary engineering
resource waste.

Figure 2: An acceptable outlier of ‚ÄúCPU iowait‚Äù in a fault-
free period may trigger a false alarm by most automated
tools.

In addition, we find that metric patterns presented at the seg-
ment level sketch issues much better than single-point outliers.
For example, the metric ‚Äúmemory usage‚Äù performs noticeably ab-
normal jitters when injecting a ‚Äúvirtual memory hog‚Äù (Figure 3).
However, these jittering points are not outliers because their values
are not extraordinarily high or low. Clearly, the outlier perspective
ignores high-order data variations, such as the scope and denseness,
resulting in missing alarm issues.

Figure 3: The irregular metric jitters cannot be detected by
single point-based detectors.

Finding 2: Metrics are more responsive to faults but still in-
sufficient in many cases. Also, their over-sensitivity may cause
false alarms on uncommon yet acceptable fluctuations. Besides,
segment-level metric patterns are more useful in anomaly de-
tection than single-point outliers.

3.2.3 RQ3: Overall Anomalous Manifestations. To answer this RQ,
we summarize representative faults and their effects on multi-
source monitoring data, as listed in Table 1.

We find that multi-source data can be affected by faults simul-
taneously. For example, when a resource hog is posed, anomalous
manifestations appear in both metrics (e.g., sudden spikes) and
logs (e.g., warnings for limited resources). Also, there is an evident
complementary relationship between the two data sources in some
cases. For instance, when the Datanode or Secondary Namenode
is killed, the related metrics‚Äô abnormal manifestations cannot be
detected (i.e.silent) since these metrics also plummet to zero if the
application ends normally. Metric-based anomaly detectors cannot
distinguish such abnormal drops from normal ones. In this case,
logs can serve as an additional source of information to help de-
termine the system status. On the contrary, when the connection
between nodes flashes, no warnings or errors are generated in logs
since the network disconnection time is too short to affect program
operation. Nevertheless, network-related metrics can faithfully re-
flect these transient anomalies, such as the network throughout
that drops rapidly during flashes. Such observations align with intu-
ition. Basically, logs record the software‚Äôs internal execution logic,
while metrics provide an external view by measuring software ser-
vices‚Äô performances, resource usage, etc. Thus, combining the two
can better portray the system status due to their collaborative and
complementary relationships.

Furthermore, a fault can affect different types of monitoring data
to varying degrees. For example, killing the Datanode during the
Latent Dirichlet Allocation (LDA) application causes 29 abnormal
metric segments while only one log sequence reports anomalies.
Another example is that when suspending the Namenode in word
counting, the related metrics experience a sharp drop and remain
unchanged since most of the computation has been done. Yet tens
of logs reporting a failed state are generated because the worker
nodes keep warning while the master node cannot receive messages.
Hence, simply regrading all types of data equally important is
unreasonable since the more severely affected part may deserve
more attention. In brief, we should combine and assign appropriate
weights to metrics and logs to promote effective anomaly detection.

0204060801001201400K1K2Krkb/sInjectfaultRemovefaultAbnormalNormalTime/s050100150200250300350010203040CPU	iowait38.74OutlierInjectfaultRemovefaultAbnormalNormalTime/s0501001502503003504001015Memory	UtilizationInjectfaultRemove	faultAbnormalNormal200 Time/sHeterogeneous Anomaly Detection for Software Systems via Attentive Multi-modal Learning

ACM Conference, 2022,

Table 1: Typical faults and the corresponding anomalous manifestations of multi-source monitoring data

Faults
Memory hog
Virtual memory hog
I/O hog
Network delay
Connection flash
Datanode killed
Secondary namenode killed

Anomalous manifestations in logs
Warnings (reaches the memory limit; dropping blocks)
Errors (reporter thread fails)
Warnings (slow ReadProcessor)
Warnings (executor heartbeat timeout)
Nothing (silent)
Errors (excluding datanode)
Errors (failed to connect to <IP>)

Anomalous manifestations in metrics
Memory-related metrics rise steeply
CPU and memory-related metrics jitter
I/O-related metrics rise steeply
Network-related metrics suddenly drop
Network-related metrics suddenly drop and quickly restore
Related metrics plummet to zero (silent)
Related metrics plummet to zero (silent)

Finding 3: Metrics and logs can both respond to anomalies,
but neither is sufficiently informative. They have collaborative
and complementary relationships in providing clues for the
system‚Äôs health. Also, the degree to which they are affected by
the same anomaly can vary greatly.

These findings support our motivation to propose an automated
approach for detecting anomalies based on heterogeneous mon-
itoring data, i.e., logs and metrics. The approach should enable
comprehensive information extraction and inference for effectively
detecting anomalies in software systems. This results in HADES,
our solution to attack the above-mentioned challenges.

4 METHODOLOGY
Figure 4 presents the overview of HADES, which is a Heterogeneous
Anomaly DEtector for Software systems via attentive multi-modal
learning. It consists of four components: Log Modeling, Metric
Modeling, Heterogeneous Representation Fusion, and Detec-
tion. The core idea is to infer the system status from current het-
erogeneous monitoring data based on historical extracted patterns.
We incorporate domain knowledge and the insights obtained from
our previous empirical study to design a more practical model
architecture. Specifically, for logs, HADES captures lexical and
contextual log semantics and maps each raw log sequence into
a low-dimensional representation. For metrics, HADES preserves
aspect-aware information at the segment level along the timeline
and learns cross-aspect correlations. Our framework also employs
an attention-based fusion module with multi-modal learning to
acquire a global representation, which is fed into a successive detec-
tion component. Consequently, it will trigger an alarm to operations
engineers when a noteworthy anomaly occurs.

4.1 Log Modeling
Log modeling contains three steps, including log parsing, log vec-
torization, and log representation learning, aiming at learning
log representations with respect to lexical and contextual semantics,
whose importance has been demonstrated in ¬ß 3.2.1.

4.1.1 Log Parsing. In this step, we transform unstructured log
messages into structured log events. As aforementioned, raw log
messages are unstructured and contain variables that can hinder
log analysis [19]. Therefore, we first employ a widely-used parser
Drain [20] to extract log events since it has shown effectiveness and
efficiency in the previous evaluation study [53]. Next, we conduct a
stable sorting based on the log timestamps. As a result, all valid log
messages are transferred into chronologically arranged log events.

4.1.2 Log Vectorization. This phase turns textual log events into
numerical vectors while preserving lexical log semantics. We utilize
FastText [6] to capture the intrinsic relationships of log vocabulary
since the importance of log semantics has been demonstrated in
our empirical study (¬ß 3.2.1). FastText is a popular, lightweight, and
efficient technique for producing word embeddings that can repre-
sent semantic similarities between words. After training, FastText
maps every token into a ùê∏-dimension vector, so a log event ùíô ùíç is
transformed into a token embedding list ùëΩ1:ùúî = [ùíó1, ùíó2, ..., ùíóùúî ] ‚àà
Rùúî√óùê∏ , where ùúî is the token number of an event. Subsequently,
we average all elements inside ùëΩ to acquire a sentence embedding
¬ØùëΩ = 1
ùëñ=1 ùíóùëñ . Consequently, a log sequence ùëø ùíç
1:ùêø can be denoted
by a sentence embedding list ¬ØùëΩ1:ùêø = [ ¬ØùëΩ1, ¬ØùëΩ2, ..., ¬ØùëΩùêø] ‚àà Rùêø√óùê∏ .

ùúî √ó (cid:205)ùúî

4.1.3 Log Representation Learning. This step models the log con-
textual semantics and generates log representations with learned
information. In particular, the sentence embeddings of a sequence
obtained from the previous phase are fed into a sequence encoder,
which is composed of two Transformer encoder layers [46]. This
encoder captures contextual dependencies across the events. Af-
terward, a fully-connected (FC) layer maps the output into a ùê∑-
dimensional feature space. Hence, we obtain the log representation
of a chunk, denoted by ùëπùíç ‚àà Rùêø√óùê∑ . Note that if the log sequence is
too long, we partition it into fixed-size sub-sequences and conduct
the above steps. For a too-short sequence, we pad it with zeros.

4.2 Metric Modeling
We model metrics in a hierarchical manner with respect to segment-
level patterns based on the finding in ¬ß 3.2.2. The module comprises
an intra-aspect encoder and an inter-aspect encoder (Figure 4).
The rationale behind the design is summarized in three points:
(1) Metrics sketching the same aspect of the system should be mod-
eled together. Monitoring metrics can reflect various aspects of
system performance, e.g., CPU utilization, memory utilization, etc.
Generally, metric patterns of the same aspect share certain similar-
ities (e.g., CPU user usage and CPU system usage both characterize
CPU usage). Such metrics should be grouped and regarded as multi-
variate time series (MTS) to be analyzed together. (2) Metrics depict-
ing different aspects should be modeled separately. If two metrics
belong to different aspects, their patterns can be very different. For
example, the disk usage tends to be stable while the I/O throughput
may fluctuate violently even under the normal status. So metrics of
different aspects should be fed into separate models to capture fine-
grained information. (3) While metrics of different aspects tend
to develop distinct patterns, they still exhibit some inter-aspect
correlations when anomalies occur. For instance, if a worker node

ACM Conference, 2022,

Baitong Li, Tianyi Yang, Zhuangbin Chen, Yuxin Su, Yongqiang Yang, and Michael R. Lyu

Figure 4: Overview of HADES.

loses connection with the master node, many metrics such as the
CPU utilization and route cost will drop precipitously and stay at
zero since data exchange or computation can be interrupted.

According to (1) and (2), we propose an intra-aspect encoder
to capture the aspect-oriented temporal dependencies and cross-
metric relationships. Based on (3), we design an inter-aspect encoder
for learning the correlations across aspects integrally. The two
encoders learn the aspect-aware features of metrics hierarchically.

Intra-Aspect Encoder. As stated before, metrics should be
4.2.1
modeled in an aspect-oriented manner, i.e., modeling metrics of
the same aspect together while modeling metrics of different as-
pects separately. Such architecture requires the internal model to
be computationally efficient and hardware friendly. We adopt 1D
causal convolution [30] thereby, which belongs to the family of
convolutions. Conventional convolution networks face the problem
of information leakage (i.e., the output depends on future inputs)
and the inability of sequential dependency modeling. Causal con-
volution is designed to mitigate these limitations, which meets our
needs by being parallelizable, lightweight, and accurate [4]. This
phase decomposes the metrics into ùõæ groups according to their
corresponding aspects based on our domain knowledge. After that,
metrics of the same aspect are taken as an MTS and fed into a
separate intra-aspect encoder composed of a multi-layer causal
convolution network. After appropriate padding and chomping,
the ùõæ intra-aspect encoders output ùõæ feature vectors ùíâùíé. Finally,
we conduct a max-pooling operation on the feature dimension and
stack the outputs to form a latent feature vector ùëØ ùíé ‚àà Rùëá √óùõæ .

Inter-Aspect Encoder. This module also leverages causal con-
4.2.2
volution to learn the inter-aspect features. Such structure helps

model complex patterns by capturing multi-level information. We
take ùëØ ùíé outputted by the intra-aspect encoder as an MTS and
feed it into the inter-aspect encoder to model the correlations be-
tween metric aspects. In this way, the metrics ùëø ùíé
1:ùëá inside a chunk
are embedded into a ùê∑-dimensional representation, denoted by
ùëπùíé ‚àà Rùëá √óùê∑ . Note that data of all modalities should be embedded
into the same feature space for alignment.

4.3 Heterogeneous Representation Fusion
We design a fusion module with a novel cross-attention mechanism
to bridge the time-semantics gap between the representations of
logs and metrics. We fuse heterogeneous information to mitigate the
negative effects brought by the lack of information of single-source
data (¬ß 3.2.3) and the over-sensitivity of metrics (¬ß 3.2.2).

In the previous phases, logs and metrics are both embedded into a
ùê∑-dimensional feature space. These representations are fed together
into this fusion module, defined by two attention layers [46]. The
first one (Attn-ùõº) takes the log representation ùëπùíç as the Query while
the metric representation ùëπùíé as the Key and Value. It matches the
log events explaining the metric changes. Symmetrically, in the
second attention layer (Attn-ùõΩ), ùëπùíé plays the role of the Query, and
ùëπùíç serves as the Key and Value. It helps to find the performance
variations aligned with log contents to enhance log expressiveness.
Mathematically, given the Query, Key, and Value, we calculate:

Fuse(ùëÑ, ùêæ, ùëâ ) = tanh

(cid:16) (cid:104)

softmax(ùëÑùëäùë†ùêæ T)ùëâ ; ùëÑ

(cid:105)

(cid:17)

ùëäùëé

(1)

where ùëäùëé and ùëäùë† are learnable parameters; [¬∑; ¬∑] denotes concatena-
tion. Afterward, outputs from Attn-ùõº and Attn-ùõΩ are concatenated
inside the ùê∑-dimensional space to constitute a global representation
ùëπùíà ‚àà R(ùëá +ùêø)√óùê∑ for each data chunk, defined as Equation 2.

AttnŒ±AttnŒ≤Fused RŒ±Fused RŒ≤ConcatGlobal RgFCAbnormal?INFO util.SignalUtils: Registered signal WARN netlib.BLAS: Failed to load implementation INFO storage.BlockManager: Removing RDD 36 INFO util.Utils: Successfully started serviceINFO storage.BlockManager: Removing RDD 18TransTransTransEvent EmbeddingsToken EmbeddingsAvg PoolingParsing FastTextSequence EncoderNoYesDetails  ==KVRm==KVRl=QRm=QRlDDDMultivariate MetricsConvConvConvConvConvConvMax PoolingGroupingIntra-aspect EncoderInter-aspect EncoderLog ModelingHeterogeneous FusionDetectionMetric ModelingHeterogeneous Anomaly Detection for Software Systems via Attentive Multi-modal Learning

ACM Conference, 2022,

ùëπùíà = [Fuse(ùëπùíç, ùëπùíé, ùëπùíé); Fuse(ùëπùíé, ùëπùíç, ùëπùíç )]

(2)

Above all, switching the roles of the two modalities allows de-
voting more attention to the features of different modalities that
convey similar information, which is more likely to be responsive
to changes in the system status. Also, it can retain meaningful intra-
modal patterns explicitly by directly concatenating the Query with
the attended Value. In this way, the global representation reserves
not only the shared information and cross-modal interactions but
also the salient intra-modal dependencies and the inferred features
due to the complementary relationship between logs and metrics.

4.4 Detection
Finally, we feed the representation ùëπùíà of the in-process chunk into
stacked FC layers followed by a softmax layer. The output ÀÜùë¶ ‚àà {0, 1}
represents the status being normal or abnormal, computed by:

ÀÜùë¶ = argmax[softmax(ùëà ùúé (ùëâ ¬∑ ùëπùíà + ùëè) + ùëê)]
(3)
where ùëà and ùëâ are learnable weight matrices; ùëè and ùëê are bias terms;
ùúé (¬∑) is the ReLU activation function [1]. This module will generate
an alarm if it detects an anomaly. Engineers can then conveniently
locate the logs and metrics of the suspicious chunk via a visual
interface for further analysis. We display a demo in Figure 5.

Figure 5: A demo for reviewing the detected chunk.

5 EVALUATION
We evaluate HADES by answering the following research questions:
‚Ä¢ RQ4: How effective is HADES in anomaly detection?
‚Ä¢ RQ5: How much does each design of HADES contribute to its

overall performance?

‚Ä¢ RQ6: How sensitive is HADES to the length of a chunk?

5.1 Experiment Setup
5.1.1 Datasets. Besides the in-lab dataset (Dataset A in ¬ß 3.1), we
also evaluate HADES on two industrial datasets (Dataset B and
C) containing heterogeneous monitoring data from the produc-
tion cloud system of Huawei. Table 2 shows the statistics of our
datasets. All datasets are randomly split into three subsets (training,
validation, and testing) in a 7:1:2 ratio. To generate the industrial
data, we use an internal tool to inject faults into two different cloud
services. We invite three experienced reliability engineers to select
the typical faults in the production systems. Eight types of faults

are injected, including CPU stress, memory stress, high disk I/O
latency, disk partition full, network flashing, long network latency,
high packet loss rate, and zombie process. As suggested by the
engineers, we set each fault duration to two hours and repeat the
injection twice. The data generation process covers 5.5 days for
B and eleven days for C. Metrics are sampled once per minute.
Particularly, we obtain 72 templates from B and 104 templates from
C via Drain [20]. Lastly, we invite the aforementioned engineers to
annotate the data following a similar procedure described in ¬ß 3.1.3.

Table 2: Dataset statistics

Dataset
A
B
C

Log Messages Metric Length Abnormal ratio
64,422
7,473
15,945

1,435,139
76,724
1,148,563

24.47%
27.25%
23.44%

Implementations. For logs, the hidden size of the encoder is
5.1.2
1024. We use Gensim [42] to train 32-dimensional word embeddings
for 50 epochs. For metrics, the intra-aspect encoder comprises two
causal convolution layers, with a kernel size of two and a stride of
one. The inter-aspect encoder has three layers with a dropout rate
of 0.1. The decoder consists of four FC layers with a hidden size of
128. We employ the binary cross-entropy loss as the loss function.
Also, we train the model using the Adam optimizer with an initial
learning rate of 0.001, a batch size of 64, and an epoch number of 50.
It turns out that HADES is not sensitive to the above model hyper-
parameters. In addition, the data-setting hyper-parameter (e.g., the
length of a chunk) may affect the framework‚Äôs performance by
affecting the dataset distribution. We will discuss the sensitivity of
HADES to the chunk length in RQ6. All approaches are trained on
an NVIDIA GeForce GTX 1080 GPU. Our codes for reproducing the
experiments are publicly available at [3].

ùëá ùëÉ

5.1.3 Evaluation Measurements. As we tackle anomaly detection in
a binary classification manner, we adopt the widely-used measure-
ments to gauge models‚Äô performances: ùëüùëíùëêùëéùëôùëô = ùëá ùëÉ
ùëá ùëÉ +ùêπ ùëÅ , ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ =
ùëá ùëÉ +ùêπ ùëÉ , ùêπ 1-ùë†ùëêùëúùëüùëí = 2¬∑ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ ¬∑ùëüùëíùëêùëéùëôùëô
ùëá ùëÉ +ùëá ùëÅ
ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ+ùëüùëíùëêùëéùëôùëô , ùëéùëêùëêùë¢ùëüùëéùëêùë¶ =
ùëá ùëÉ +ùëá ùëÅ +ùêπ ùëÅ +ùêπ ùëÉ ,
where ùëá ùëÉ is the number of successfully detected abnormal chunks;
ùëá ùëÅ is the number of correctly predicted normal chunks; ùêπ ùëÉ is the
number of normal chunks incorrectly triggering alarms; ùêπ ùëÅ is the
number of abnormal chunks that failed to be discovered.

5.2 RQ4: Overall Performance of HADES
We compare HADES with a multi-modal baseline and five single-
modal baselines. In particular, SCWarn [52] is the only multi-modal
approach for binary classification in the software engineering liter-
ature, as far as we know, despite not being designed for anomaly
detection. LogRobust [51], LogAnomaly [38], and Deeplog [14] are
log-based, whereas Telemanom [24] and Adsketch [10] are metric-
based. For baseline implementation, we employ the codes released
by their papers for [10, 24, 52], and a toolkit [9] for [14, 38, 51]
that do not provide codes. All baselines use the same classifier and
hyper-parameter searching space as HADES for a fair comparison.
We adopt the parameters achieving the highest validation ùêπ 1-ùë†ùëêùëúùëüùëí.
Note that for log-based methods, we drop the chunks without log
messages and adopt the sliding window mode as the original papers.

OverviewDetailsKey MetricsAspectMetricsCPUtx/brx/bLog FileSettingsÂÆûÈôÖ‚Ω§‰æã‚Ω∞ÊÑèPathhttp://127.0.0.1/ root/work‚Ä¶WorkloadIDc0d17d481f47bdd9StatusRunningStart at22/03/01T07:00:00Chunk Info22/03/01T09:28:00ÔΩû22/03/01T09:38:00TimeStatusSourceAbnormalLog, MetricDownloadName%userI/OI/OLog PreviewINFO storage.BlockManager: Found block rdd_2_3 locallyINFO storage.BlockManager: Found block rdd_2_4 locallyINFO util.SignalUtils: Registered signal WARN netlib.BLAS: Failed to load implementation INFO storage.BlockManager: Removing RDD 36 INFO util.Utils: Successfully started serviceINFO storage.BlockManager: Removing RDD 18INFO python.PythonRunner: Times: total = 42, boot = -4131, init = 4172, Ô¨Ånish = 110Auto FreshHADESACM Conference, 2022,

Baitong Li, Tianyi Yang, Zhuangbin Chen, Yuxin Su, Yongqiang Yang, and Michael R. Lyu

Table 3: Performance comparison between HADES and the baselines.

Data source Approaches

Dataset A

Dataset B

Dataset C

F1-score

Rec.

Prec.

Acc.

F1-score

Rec.

Prec.

Acc.

F1-score

Rec.

Prec.

Acc.

Multi-source

Logs

Metrics

HADES

0.8503

0.8251

0.8772

0.9340

0.9790

0.9722

0.9859

0.9899

0.9612

0.9525

0.9700

0.9719

SCWarn
LogRobust
LogAnomaly
Deeplog
Telemanom
Adsketch

0.7926
0.4569
0.4454
0.4396
0.6063
0.6535

0.7822
0.4737
0.5579
0.4211
0.5437
0.5747

0.8034
0.4412
0.3706
0.4598
0.6850
0.7573

0.9070
0.8199
0.7778
0.8283
0.8306
0.6560

0.9041
0.5854
0.4681
0.4651
0.5366
0.6667

0.9167
0.6316
0.5789
0.5263
0.6197
0.5634

0.8919
0.5455
0.3929
0.4167
0.4731
0.8163

0.9532
0.8759
0.8175
0.8321
0.7424
0.8644

0.8955
0.5091
0.4511
0.4463
0.5317
0.6409

0.8831
0.5283
0.5660
0.5094
0.5938
0.5354

0.9082
0.4912
0.3750
0.3971
0.4813
0.7982

0.9489
0.6376
0.5101
0.5503
0.7405
0.8511

Table 3 presents the overall performance comparison. HADES
outperforms all baselines by a significant margin, achieving the best
result on every evaluation measurement. Specifically, its ùêπ 1-ùë†ùëêùëúùëüùëís
are 0.8503, 0.9790, and 0.9612, 7.65%‚àº106.55% higher than competi-
tors on average. The high ùëüùëíùëêùëéùëôùëô, ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ and ùëéùëêùëêùë¢ùëüùëéùëêùë¶ scores of
HADES illustrate that there are very few missed anomalies or false
alarms. Thus, HADES is considerably effective to detect system
anomalies, redounding to economizing engineering resources.

Compared with the multi-modal baseline, SCWarn, the success
of HADES can be attributed to its powerful capability in hetero-
geneous representation learning, summarized as three points. (1)
HADES utilizes multi-level log semantics to generate more infor-
mative representations, as log semantics are important in reflecting
anomalies (¬ß 3.2.1). (2) HADES represents log events via succinct
and low-dimensional embeddings, facilitating effective representa-
tion learning. In comparison, SCWarn transforms logs into event oc-
currence sequences, generating an over-large sparse feature matrix
for log events. Such a design poses barriers to extracting meaningful
features and wastes computation and space resources, especially
when hundreds of events exist. (3) HADES devises an attentive
fusion to capture significant cross-modal interactions and bridge
temporal and textual representations. By contrast, SCWarn simply
concatenates the heterogeneous representations. It ignores the vast
gap between metrics and logs, i.e., the information form and the in-
put size discrepancies, thereby degrading the model‚Äôs performance.
The superiority of HADES over single-modal baselines stems
from the effective use of logs and metrics. Compared with the best
single-modal model (Adsketch), the ùêπ 1-ùë†ùëêùëúùëüùëí, ùëüùëíùëêùëéùëôùëô, ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ, and
ùëéùëêùëêùë¢ùëüùëéùëêùë¶ of HADES is increased by 42.29%, 64.39%, 19.40%, 22.49%
on average, respectively. Such improvement is exciting and reason-
able. Our empirical study reveals that metrics and logs can both
reflect anomalies, and neither of them is sufficient (¬ß 3.2.3). However,
these baselines only review one data source and omit much impor-
tant information hidden in the other source. So they cannot infer
the system status precisely and deliver satisfying performances.

In brief, HADES effectively detects system anomalies on all
datasets. It significantly improves the detection performance com-
pared to all baselines concerning every evaluation measurement,
especially compared to the single-modal competitors.

5.3 RQ5: Individual Contribution of Modules
5.3.1 Derived Models. We conduct an extensive ablation study
on HADES. Particularly, we derive seven models based on the
original HADES to investigate the contribution of the introduction

of heterogeneous information, multi-modal learning, cross-modal
attentive fusion, and single-modal feature extraction.
‚Ä¢ Heterogeneous information: log-HADES and metric-HADES.
log-HADES is log-based, containing a log modeling module (du-
plicated from ¬ß 4.1) and a decoder. Similarly, metric-HADES is
metric-based, containing a metric modeling module (duplicated
from ¬ß 4.2) and a decoder. The decoders adopt the detection mod-
ule‚Äôs structure of HADES (¬ß 4.4). Representations of logs and
metrics are directly fed into the respective decoder separately.

‚Ä¢ Multi-modal learning: or-HADES. or-HADES performs a Boolean
OR operation on the results from log-HADES and metric-HADES.
It is built on the motivation that it‚Äôs natural to process each
type of data individually and then aggregate results instead of
multi-modal learning when involving various sources of data.
‚Ä¢ Attentive fusion: concat-HADES and sep-HADES. These two
models share the same log and metric representation learning
modules with HADES but change the way of fusing representa-
tions (¬ß 4.3). concat-HADES simply concatenates the representa-
tions of metrics and logs as the global representation; sep-HADES
operates conventional self-attention on the two representations
separately and then concatenates them.

‚Ä¢ Single-modal feature extraction: agn-HADES and onehot-
HADES. agn-HADES is designed for validating the contribution
of the specially-designed aspect-aware metric modeling module
(¬ß 4.2). It models metrics in an aspect-agnostic manner based
on causal convolutions to deal with all metrics simultaneously.
onehot-HADES replaces the word embedding with one-hot en-
coding to present the usefulness of log semantics (¬ß 4.1.2). Other
modules of agn-HADES and onehot-HADES all keep unchanged
from HADES for comparison.

5.3.2 Experimental Results. We adopt the same measurements as
RQ4 to assess the contribution of the above-mentioned components.
The results are shown in table 4, underpinning four key conclusions.
(1) Introducing heterogeneous information contributes incredi-
bly to enhancing anomaly detection in view of two observations.
First, HADES outperforms log-HADES and metric-HADES con-
siderably, mainly in ùëüùëíùëêùëéùëôùëô. Such results indicate that HADES can
discover suspicious situations while reducing false alarms because
it leverages the relationships between logs and metrics. Also, all
derived models based on multi-source data deliver better perfor-
mances than single-source-based variants (log-HADES and metric-
HADES), validating that heterogeneous data goes far towards char-
acterizing the system health more fully.

Heterogeneous Anomaly Detection for Software Systems via Attentive Multi-modal Learning

ACM Conference, 2022,

Table 4: Performance comparison between HADES and the derived models.

Models

Dataset A

Dataset B

Dataset C

F1-score

Rec.

Prec.

Acc.

F1-score

Rec.

Prec.

Acc.

F1-score

Rec.

Prec.

Acc.

HADES

0.8503

0.8251

0.8772

0.9340

0.9790

0.9722

0.9859

0.9899

0.9612

0.9538

0.9688

0.9809

log-HADES
metric-HADES
or-HADES
concat-HADES
sep-HADES
agn-HADES
onehot-HADES

0.4646
0.7564
0.8243
0.8307
0.8331
0.8401
0.8299

0.4842
0.7327
0.7822
0.7855
0.8152
0.8152
0.7888

0.4466
0.7817
0.8713
0.8815
0.8517
0.8667
0.8755

0.8215
0.8928
0.9243
0.9273
0.9258
0.9295
0.9265

0.6061
0.8281
0.9296
0.9517
0.9577
0.9722
0.9379

0.5263
0.7465
0.9167
0.9583
0.9444
0.9722
0.9444

0.7143
0.9298
0.9429
0.9452
0.9714
0.9722
0.9315

0.9051
0.9254
0.9663
0.9764
0.9798
0.9865
0.9697

0.5567
0.8179
0.9173
0.9374
0.9419
0.9567
0.9209

0.5094
0.7877
0.9046
0.9446
0.9477
0.9508
0.9138

0.6136
0.8505
0.9304
0.9303
0.9362
0.9626
0.9281

0.7114
0.9130
0.9595
0.9687
0.9710
0.9786
0.9611

(2) Two shreds of evidence highlight the benefits of multi-modal
learning: 1) HADES performs better than or-HADES (4.28% higher
in ùêπ 1-ùë†ùëêùëúùëüùëí on average); 2) the variants using multi-modal learn-
ing (concat-HADES and sep-HADES) outperform or-HADES. It
is not surprising since or-HADES cannot fully mine cross-modal
interactions. For example, the over-sensitivity of metrics (stated
in ¬ß 3.2.2) may cause false alarms, and or-HADES fails to overcome
such inaccuracy, while HADES alleviates this issue by utilizing logs
as supplementary information to make more reasonable inferences.
(3) Our attentive fusion module shows the extraordinary value
since HADES achieves better results than concat-HADES and seq-
HADES. Compared with the variants, HADES can filter more infor-
mative features and exploit higher-order cross-modal interactions.
It allows heterogeneous data to complement each other to prob a
stronger ability to characterize the system‚Äôs health.

(4) The devised encoders make contributions via fuller- and finer-
grained feature extraction. Specifically, agn-HADES generate more
false predictions (0.77% lower in ùêπ 1-ùë†ùëêùëúùëüùëí than HADES on aver-
age) since mixing patterns of diverse aspects degrades the model‚Äôs
detecting ability. Besides, onehot-HADES has a relatively poor per-
formance because lexical semantics are crucial for analyzing logs.
To sum up, the results conform to the findings of our previous
empirical study in ¬ß 3, and they further reveal that heterogeneous
data is significantly valuable for anomaly detection, and our designs
for intra- and inter-modal representation learning are competent.

5.4 RQ6: Sensitivity to Chunk Length
We herein evaluate the sensitivity of HADES to the pre-determined
chunk length ùëá (¬ß 2) as it influences our end-to-end framework by
affecting the data distribution.

We change the value ofùëá while keeping all other hyper-parameters

unchanged and conduct experiments as in RQ4. In detail, for Dataset
A, the default value of ùëá is 10 (sec), ranging from 5 to 25; for Dataset
B and C, ùëá is 5 (min) by default and ranges from 3 to 11, since
the sampling frequencies of the simulated dataset and industrial
datasets are different.

Figure 6 presents the experimental results. Overall, HADES is
fairly stable under different settings of ùëá , further confirming the
robustness of HADES. It makes HADES easy to deploy and come
into play in practice. Yet, when the value of ùëá deviates from the
default configuration, the scores of ùëüùëíùëêùëéùëôùëô and ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ render
imbalance, mainly reflected by the increase in missing anomalies.

Figure 6: The sensitivity to chunk length

Since our fault duration periods are one minute and one hour,
respectively, the default granularity of chunks may felicitously fit
the anomalous patterns. In practice, ùëá can be selected according to
the validation result or the engineering expertise and requirements.

6 CASE STUDY
We share two representative cases of industrial anomalies that
HADES successfully detected. In order to display the core patterns
of metrics more clearly, we eliminate the jitters inside the curves.

6.1 Case I
We inject a CPU hog into a cloud cache service of Huawei (Dataset
B), then the metric ‚ÄúCPU usage‚Äù is abnormally high, indicating a
performance degradation (Figure 7-top). However, no suspicious
event is reported in logs since the execution logic of the service
remains correct. This case demonstrates that some performance
anomalies cannot be reflected by logs in practice, making log-based
detectors less responsive. By fusing heterogeneous information,
HADES successfully detects such anomalies.

6.2 Case II
Another case in Dataset C is that after removing the fault ‚Äúhigh
network latency‚Äù, the related metric ‚ÄúPing latency‚Äù catches back up
with its long-run trend immediately, as shown in Figure 7-bottom.
However, in practice, the service is still unable to function properly
for a short period after the fault removal because it takes some time
to process the previously blocked requests in the request queue.
Since we expect that issues affecting the quality of service can all
be identified, an alarm should be triggered during such a period.

Evaluation Metrics0.700.760.830.890.95510152025Avg. Evaluation Metrics0.90.9250.950.9751357911Chunk Length TF1-scoreAccuracyRecallPrecisionDataset  Industrial DatasetsACM Conference, 2022,

Baitong Li, Tianyi Yang, Zhuangbin Chen, Yuxin Su, Yongqiang Yang, and Michael R. Lyu

Relying solely on metrics cannot achieve this goal. Fortunately, logs
can provide clues for troubleshooting by reporting events ‚ÄúGet user
info failed‚Äù. Thus, HADES can keenly capture such anomalies.

two log events to have identical tokens with different orders repre-
senting the opposite system statuses. Thus, omitting intra-event
sequentiality will not cause an apparent adverse effect.

7.2.2 External Validity. The external threat mainly comes from the
representativeness and quality of datasets. Though we validated
the effectiveness of HADES on simulated and industrial data, it
is yet unknown how it generalizes to different datasets. Also, the
industrial data is relatively limited and simple. We have run various
workloads and injected diverse typical faults to improve the repre-
sentativeness of our datasets. The services we selected in Huawei
are also representative. We may evaluate our approach on a variety
of datasets in the future. Another concern is that there may be
annotation noise. Our datasets are manually labeled, so labeling
noise may be introduced when human mistakes happen. To allevi-
ate this issue, we invite experienced Ph.D. students (Dataset A) and
professional engineers (Dataset B and C) to annotate our datasets,
who are all experts in troubleshooting. Besides, since the faults are
typical in the real world, they are familiar with the corresponding
abnormal patterns, making errors less likely to occur.

8 RELATED WORK
Many efforts have been devoted to automated anomaly detection
for large-scale system reliability insurance [2, 5, 10, 28, 29, 31, 33,
37, 43, 45]. Logs, metrics, and other monitoring data are pivotal
for troubleshooting since they maintain the run-time information
of systems. Many advanced log-based anomaly detectors adopt
deep learning techniques. For example, Du et al. [14] proposed
DeepLog to learn normal log sequential patterns via LSTM, based
on which Meng et al. [38] considered log semantics behind syn-
onyms and antonyms. LogRobust [51] tackled log instability by
introducing the attention mechanism, as well as employing word
embedding and TF-IDF. Also, many outstanding metric-based meth-
ods employ machine learning. They attempt to capture the temporal
dependencies [28, 43], mine representative patterns [10], and learn
inter-series relationships [2, 24, 33, 45]. For instance, Hundman et al.
[24] modeled multi-variate metrics via LSTM networks with non-
parametric dynamic thresholding. Recently, Adsketch [10] achieved
superior performances by discovering anomalous metric patterns
that sketch particular performance issues. These methods only
leverage single-source data, ignoring rich information from diverse
sources and their interactions. Our approach overcomes this prob-
lem by capturing meaningful features from heterogeneous data.

Some approaches employ more than one source of data, though
they are not for anomaly detection [16, 32, 35, 52]. Gandalf [32]
identified bad software rollouts by consuming multi-source data,
whereas it used each source separately rather than analyzing all
sources generically. [16, 35] explored the correlations between logs
and metrics to identify rolling upgrade operations that incur failures.
They regard logs as operation records and metric variations as
the consequences of operations. Zhao et al. [52] transformed logs
into event occurrence sequences, which are modeled together with
metrics to alarm bad software changes. However, these methods
convert heterogeneous data into a homogeneous form and ignore
the gap and high-order interactions among different data types,
resulting in performance degradation. Our approach outperforms

Figure 7: Case I: CPU Usage is abnormally high (top); Case
II: Ping latency immediately returns to normal, while the
service is still latent (bottom).

7 DISCUSSION
7.1 Lessons Learned
7.1.1 Beware of the semantic gap between natural language and
logs. Recent studies [29, 38, 51] have gradually adopted models pre-
trained in natural language corpus to encode log texts. However, the
semantics of natural language and logs are not exactly identical. No-
tably, the semantic gap of keywords may have a more severe impact.
For example, the word ‚Äúsuccessfully‚Äù usually expresses positive emo-
tions in natural languages, yet it indicates an anomaly when occur-
ring in the special event ‚ÄúSuccessfully connected to /<IP>:<NUM>
for BP-<NUM>-<IP><NUM>:blk_<NUM>_<NUM>‚Äù. The following
condition triggers this event: after killing the Datanode process,
the Application Worker process loses the connection and tries to
reconnect with the Data node process. If the re-connection is suc-
cessful, then this event is triggered. However, such a condition does
not exist in normal, so the re-connection should not happen. To
mitigate this problem, HADES uses self-trained word embeddings
and considers not only word-level semantics but also event-level
semantics and cross-event sequential dependencies. Integrating
multi-level semantics of logs can help draw a more fundamental
picture of system events.

7.1.2 Generalize the approach. We find that in some large compa-
nies, metrics and logs are collected separately by different depart-
ments, and the sampling/logging frequency of different data sources
varies dramatically. Sometimes a certain data source is even absent
for a while. Thus, we add an extra mode to allow alternate use of
single-source and multi-source anomaly detection in a streaming
manner, thus extending the applicability of our approach.

7.2 Threats to Validity
Internal Validity. A potential threat lies in the acquisition of
7.2.1
log event embeddings. We use the average token embeddings as
an event embedding, ignoring the sequential information inside an
event. Still, it is too time-consuming to extract the sequential de-
pendencies event by event, so we choose to balance the information
granularity and efficiency. Fortunately, it is almost impossible for

[INFO] ‚Ä¶ cache-thread Received message successfully [WARN] ‚Ä¶ cache-thread Get user info failed [WARN] ‚Ä¶ cache-thread Get user info failed [INFO] ‚Ä¶ scheduler Start run updateSynVmInfoHeterogeneous Anomaly Detection for Software Systems via Attentive Multi-modal Learning

ACM Conference, 2022,

by learning cross-modal representations while narrowing the gap,
delivering more reasonable inference thereby.

9 CONCLUSION
To profoundly comprehend the system‚Äôs health, we conduct a com-
prehensive empirical study on large-scale heterogeneous monitor-
ing data to investigate how typical faults affect system behaviors.
The findings reveal that anomalies can be manifested in multi-
source data, and neither source is sufficient. Motivated by the em-
pirical study, we propose a novel end-to-end approach, HADES,
which is the first work to detect anomalies effectively by synthesiz-
ing heterogeneous information. HADES has a powerful ability of
heterogeneous representation learning. It leverages not only multi-
level log semantics and aspect-aware dependencies of metrics but
also learns meaningful inter-modal interactions via attentive fu-
sion. Extensive experiments on both simulated and industrial data
validate the effectiveness of HADES, which vastly outperforms all
compared approaches by achieving an average ùêπ 1-ùë†ùëêùëúùëüùëí of 0.93.
Our ablation study further displays the contribution of each design
of our approach to the overall performance. Lastly, we also release
our code and dataset to facilitate future research.

REFERENCES
[1] Abien Fred Agarap. 2018. Deep Learning using Rectified Linear Units (ReLU).

CoRR abs/1803.08375 (2018). http://arxiv.org/abs/1803.08375

[2] Julien Audibert, Pietro Michiardi, Fr√©d√©ric Guyard, S√©bastien Marti, and Maria A.
Zuluaga. 2020. USAD: UnSupervised Anomaly Detection on Multivariate Time
Series. In KDD ‚Äô20: The 26th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, Virtual Event, CA, USA, August 23-27, 2020. ACM, 3395‚Äì3404.
https://doi.org/10.1145/3394486.3403392

[3] Anonymous Author(s). 2021. Hades: Heterogeneous Anomaly Detection for Software
Systems via Multimodal Learning. https://anonymous.4open.science/r/HADES-
820

[4] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. An Empirical Evaluation of
Generic Convolutional and Recurrent Networks for Sequence Modeling. CoRR
abs/1803.01271 (2018). http://arxiv.org/abs/1803.01271

[5] Christophe Bertero, Matthieu Roy, Carla Sauvanaud, and Gilles Tr√©dan. 2017.
Experience Report: Log Mining Using Natural Language Processing and Applica-
tion to Anomaly Detection. In 28th IEEE International Symposium on Software
Reliability Engineering, ISSRE 2017, Toulouse, France, October 23-26, 2017. IEEE
Computer Society, 351‚Äì360. https://doi.org/10.1109/ISSRE.2017.43

[6] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. En-
riching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606
(2016).

[7] Boyuan Chen and Zhen Ming (Jack) Jiang. 2017. Characterizing and detecting anti-
patterns in the logging code. In Proceedings of the 39th International Conference on
Software Engineering, ICSE 2017, Buenos Aires, Argentina, May 20-28, 2017. IEEE /
ACM, 71‚Äì81. https://doi.org/10.1109/ICSE.2017.15

[8] Zhuangbin Chen, Yu Kang, Liqun Li, Xu Zhang, Hongyu Zhang, Hui Xu, Yangfan
Zhou, Li Yang, Jeffrey Sun, Zhangwei Xu, Yingnong Dang, Feng Gao, Pu Zhao, Bo
Qiao, Qingwei Lin, Dongmei Zhang, and Michael R. Lyu. 2020. Towards intelligent
incident management: why we need it and how we make it. In ESEC/FSE ‚Äô20:
28th ACM Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, Virtual Event, USA, November 8-13, 2020.
ACM, 1487‚Äì1497. https://doi.org/10.1145/3368089.3417055

[9] Zhuangbin Chen, Jinyang Liu, Wenwei Gu, Yuxin Su, and Michael R. Lyu. 2021.
Experience Report: Deep Learning-based System Log Analysis for Anomaly
Detection. CoRR abs/2107.05908 (2021). https://arxiv.org/abs/2107.05908
[10] Zhuangbin Chen, Jinyang Liu, Yuxin Su, Hongyu Zhang, Xiao Ling, Yongqiang
Yang, and Michael R. Lyu. 2022. Adaptive Performance Anomaly Detection
for Online Service Systems via Pattern Sketching. CoRR abs/2201.02944 (2022).
https://arxiv.org/abs/2201.02944

[11] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and
psychological measurement 20 (1960), 37‚Äì46. https://w3.ric.edu/faculty/organic/
coge/cohen1960.pdf

[12] Olasoji Denloy, Yingqi (Lucy) Lu, Eric Kaczmarek, and Agata Gruza. 2015. PAT.

https://github.com/asonje/PAT

[13] Xuemei Ding, Yuhua Li, Ammar Belatreche, and Liam P. Maguire. 2014. An
experimental evaluation of novelty detection methods. Neurocomputing 135

(2014), 313‚Äì327. https://doi.org/10.1016/j.neucom.2013.12.002

[14] Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. 2017. DeepLog: Anomaly
Detection and Diagnosis from System Logs through Deep Learning. In Proceedings
of the 2017 ACM SIGSAC Conference on Computer and Communications Security,
CCS 2017, Dallas, TX, USA, October 30 - November 03, 2017. ACM, 1285‚Äì1298.
https://doi.org/10.1145/3133956.3134015

[15] Faraz Faghri, Sobir Bazarbayev, Mark Overholt, Reza Farivar, Roy H. Campbell,
and William H. Sanders. 2012. Failure Scenario as a Service (FSaaS) for Hadoop
Clusters (SDMCMM ‚Äô12). Association for Computing Machinery, New York, NY,
USA, Article 5, 6 pages. https://doi.org/10.1145/2405186.2405191

[16] Mostafa Farshchi, Jean-Guy Schneider, Ingo Weber, and John C. Grundy. 2015.
Experience report: Anomaly detection of cloud application operations using log
and cloud metric correlation analysis. In 26th IEEE International Symposium on
Software Reliability Engineering, ISSRE 2015, Gaithersbury, MD, USA, November 2-5,
2015. IEEE Computer Society, 24‚Äì34. https://doi.org/10.1109/ISSRE.2015.7381796

[17] Apache Software Foundation. 2018. Apache Spark. https://spark.apache.org/
[18] Pinjia He, Zhuangbin Chen, Shilin He, and Michael R Lyu. 2018. Characterizing
the natural language descriptions in software logging statements. In Proceedings
of the 33rd ACM/IEEE International Conference on Automated Software Engineering.
178‚Äì189.

[19] Pinjia He, Jieming Zhu, Shilin He, Jian Li, and Michael R. Lyu. 2018. Towards
Automated Log Parsing for Large-Scale Log Data Analysis. IEEE Trans. Dependable
Secur. Comput. 15, 6 (2018), 931‚Äì944. https://doi.org/10.1109/TDSC.2017.2762673
[20] Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R. Lyu. 2017. Drain: An
Online Log Parsing Approach with Fixed Depth Tree. In 2017 IEEE International
Conference on Web Services, ICWS 2017, Honolulu, HI, USA, June 25-30, 2017. IEEE,
33‚Äì40. https://doi.org/10.1109/ICWS.2017.13

[21] Shilin He, Pinjia He, Zhuangbin Chen, Tianyi Yang, Yuxin Su, and Michael R.
Lyu. 2021. A Survey on Automated Log Analysis for Reliability Engineering.
ACM Comput. Surv. 54, 6 (2021), 130:1‚Äì130:37. https://doi.org/10.1145/3460345
[22] Shilin He, Jieming Zhu, Pinjia He, and Michael R. Lyu. 2020. Loghub: A Large
Collection of System Log Datasets towards Automated Log Analytics. CoRR
abs/2008.06448 (2020). arXiv:2008.06448 https://arxiv.org/abs/2008.06448
[23] Peng Huang, Chuanxiong Guo, Lidong Zhou, Jacob R Lorch, Yingnong Dang,
Murali Chintalapati, and Randolph Yao. 2017. Gray failure: The achilles‚Äô heel
of cloud-scale systems. In Proceedings of the 16th Workshop on Hot Topics in
Operating Systems. 150‚Äì155.

[24] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and
Tom S√∂derstr√∂m. 2018. Detecting Spacecraft Anomalies Using LSTMs and Non-
parametric Dynamic Thresholding. In Proceedings of the 24th ACM SIGKDD Inter-
national Conference on Knowledge Discovery & Data Mining, KDD 2018, London,
UK, August 19-23, 2018. ACM, 387‚Äì395. https://doi.org/10.1145/3219819.3219845
[25] Yintong Huo, Yuxin Su, Baitong Li, and Michael R. Lyu. 2021. SemParser: A
Semantic Parser for Log Analysis. CoRR abs/2112.12636 (2021). https://arxiv.org/
abs/2112.12636

[26] Intel Inc. 2004. HiBench. https://github.com/Intel-bigdata/HiBench
[27] Colin Ian King. 2020. Stress-ng. https://github.com/ColinIanKing/stress-ng
[28] Nikolay Laptev, Saeed Amizadeh, and Ian Flint. 2015. Generic and Scalable
Framework for Automated Time-series Anomaly Detection. In Proceedings of the
21th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, Sydney, NSW, Australia, August 10-13, 2015. ACM, 1939‚Äì1947. https:
//doi.org/10.1145/2783258.2788611

[29] Van-Hoang Le and Hongyu Zhang. 2021. Log-based Anomaly Detection Without
Log Parsing. CoRR abs/2108.01955 (2021). arXiv:2108.01955 https://arxiv.org/
abs/2108.01955

[30] Colin Lea, Ren√© Vidal, Austin Reiter, and Gregory D. Hager. 2016. Temporal Con-
volutional Networks: A Unified Approach to Action Segmentation. In Computer
Vision - ECCV 2016 Workshops - Amsterdam, The Netherlands, October 8-10 and
15-16, 2016, Proceedings, Part III (Lecture Notes in Computer Science, Vol. 9915).
47‚Äì54. https://doi.org/10.1007/978-3-319-49409-8_7

[31] Xiaoyun Li, Pengfei Chen, Linxiao Jing, Zilong He, and Guangba Yu. 2020.
SwissLog: Robust and Unified Deep Learning Based Log Anomaly Detection
for Diverse Faults. In 31st IEEE International Symposium on Software Reliability
Engineering, ISSRE 2020, Coimbra, Portugal, October 12-15, 2020. IEEE, 92‚Äì103.
https://doi.org/10.1109/ISSRE5003.2020.00018

[32] Ze Li, Qian Cheng, Ken Hsieh, Yingnong Dang, Peng Huang, Pankaj Singh, Xin-
sheng Yang, Qingwei Lin, Youjiang Wu, Sebastien Levy, and Murali Chintalapati.
2020. Gandalf: An Intelligent, End-To-End Analytics Service for Safe Deployment
in Large-Scale Cloud Infrastructure. In 17th USENIX Symposium on Networked
Systems Design and Implementation, NSDI 2020, Santa Clara, CA, USA, February
25-27, 2020. USENIX Association, 389‚Äì402. https://www.usenix.org/conference/
nsdi20/presentation/li

[33] Zhihan Li, Youjian Zhao, Jiaqi Han, Ya Su, Rui Jiao, Xidao Wen, and Dan Pei.
2021. Multivariate Time Series Anomaly Detection and Interpretation using
Hierarchical Inter-Metric and Temporal Embedding. In KDD ‚Äô21: The 27th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,
Singapore, August 14-18, 2021. ACM, 3220‚Äì3230. https://doi.org/10.1145/3447548.
3467075

ACM Conference, 2022,

Baitong Li, Tianyi Yang, Zhuangbin Chen, Yuxin Su, Yongqiang Yang, and Michael R. Lyu

[52] Nengwen Zhao, Junjie Chen, Zhaoyang Yu, Honglin Wang, Jiesong Li, Bin Qiu,
Hongyu Xu, Wenchi Zhang, Kaixin Sui, and Dan Pei. 2021.
Identifying bad
software changes via multimodal anomaly detection for online service systems.
In ESEC/FSE ‚Äô21: 29th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, Athens, Greece, August
23-28, 2021. ACM, 527‚Äì539. https://doi.org/10.1145/3468264.3468543

[53] Jieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie, Zibin Zheng, and Michael R.
Lyu. 2019. Tools and benchmarks for automated log parsing. In Proceedings of
the 41st International Conference on Software Engineering: Software Engineering in
Practice, ICSE (SEIP) 2019, Montreal, QC, Canada, May 25-31, 2019. IEEE / ACM,
121‚Äì130. https://doi.org/10.1109/ICSE-SEIP.2019.00021

[34] Jian-Guang Lou, Qingwei Lin, Rui Ding, Qiang Fu, Dongmei Zhang, and Tao
Xie. 2013. Software analytics for incident management of online services: An
experience report. In 2013 28th IEEE/ACM International Conference on Automated
Software Engineering, ASE 2013, Silicon Valley, CA, USA, November 11-15, 2013.
IEEE, 475‚Äì485. https://doi.org/10.1109/ASE.2013.6693105

[35] Chen Luo, Jian-Guang Lou, Qingwei Lin, Qiang Fu, Rui Ding, Dongmei Zhang,
and Zhe Wang. 2014. Correlating events with time series for incident diagnosis.
In The 20th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD ‚Äô14, New York, NY, USA - August 24 - 27, 2014. ACM, 1583‚Äì1592.
https://doi.org/10.1145/2623330.2623374

[36] Trevor Maynard. 2018. Cloud Down Impacts on the US economy. Technical Report.
Lloyd‚Äôs and AIR Worldwide, London. https://assets.lloyds.com/assets/pdf-air-
cyber-lloyds-public-2018-final/1/pdf-air-cyber-lloyds-public-2018-final.pdf
[37] Weibin Meng, Ying Liu, Yuheng Huang, Shenglin Zhang, Federico Zaiter, Bingjin
Chen, and Dan Pei. 2020. A Semantic-aware Representation Framework for
Online Log Analysis. In 29th International Conference on Computer Communica-
tions and Networks, ICCCN 2020, Honolulu, HI, USA, August 3-6, 2020. IEEE, 1‚Äì7.
https://doi.org/10.1109/ICCCN49398.2020.9209707

[38] Weibin Meng, Ying Liu, Yichen Zhu, Shenglin Zhang, Dan Pei, Yuqing Liu, Yihao
Chen, Ruizhi Zhang, Shimin Tao, Pei Sun, and Rong Zhou. 2019. LogAnomaly:
Unsupervised Detection of Sequential and Quantitative Anomalies in Unstruc-
tured Logs. In Proceedings of the Twenty-Eighth International Joint Conference
on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019. ijcai.org,
4739‚Äì4745. https://doi.org/10.24963/ijcai.2019/658

[39] Dirk Merkel. 2014. Docker: lightweight linux containers for consistent develop-

ment and deployment. Linux journal 2014, 239 (2014), 2.

[40] Sasho Nedelkoski, Jasmin Bogatinovski, Ajay Kumar Mandapati, Soeren Becker,
Jorge Cardoso, and Odej Kao. 2020. Multi-source Distributed System Data for
AI-Powered Analytics. In Service-Oriented and Cloud Computing. Springer Inter-
national Publishing, Cham, 161‚Äì176.

[41] NetMan Lab of Tsinghua University. 2021. 2021 International AIOps Challenge.

http://iops.ai/competition_detail/?competition_id=17&flag=1

[42] Radim ≈òeh≈Ø≈ôek and Petr Sojka. 2010. Software Framework for Topic Modelling
with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges
for NLP Frameworks. ELRA, Valletta, Malta, 45‚Äì50.

[43] Hansheng Ren, Bixiong Xu, Yujing Wang, Chao Yi, Congrui Huang, Xiaoyu Kou,
Tony Xing, Mao Yang, Jie Tong, and Qi Zhang. 2019. Time-Series Anomaly De-
tection Service at Microsoft. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK,
USA, August 4-8, 2019. ACM, 3009‚Äì3017. https://doi.org/10.1145/3292500.3330680
[44] Muntadher Saadoon, Siti hafizah Ab hamid, Hazrina Sofian, Hamza Altarturi, Nur
Daud, Zati Azizul Hasan, Asmiza Sani, and Asefeh Asemi. 2021. Experimental
Analysis in Hadoop MapReduce: A Closer Look at Fault Detection and Recovery
Techniques. Sensors 21 (05 2021), 3799. https://doi.org/10.3390/s21113799
[45] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust
Anomaly Detection for Multivariate Time Series through Stochastic Recurrent
Neural Network. In Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August
4-8, 2019. ACM, 2828‚Äì2837. https://doi.org/10.1145/3292500.3330672

[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA. 5998‚Äì6008. https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html

[47] Yahoo. 2011. Anarchyape. https://github.com/david78k/anarchyape
[48] Xiao Yu, Pallavi Joshi, Jianwu Xu, Guoliang Jin, Hui Zhang, and Guofei Jiang.
2016. CloudSeer: Workflow Monitoring of Cloud Infrastructures via Interleaved
Logs. In Proceedings of the Twenty-First International Conference on Architectural
Support for Programming Languages and Operating Systems, ASPLOS 2016, Atlanta,
GA, USA, April 2-6, 2016. ACM, 489‚Äì502. https://doi.org/10.1145/2872362.2872407
[49] Ding Yuan, Soyeon Park, Peng Huang, Yang Liu, Michael M Lee, Xiaoming
Tang, Yuanyuan Zhou, and Stefan Savage. 2012. Be conservative: Enhancing
failure diagnosis with proactive logging. In 10th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 12). 293‚Äì306.

[50] Ding Yuan, Jing Zheng, Soyeon Park, Yuanyuan Zhou, and Stefan Savage. 2012.
Improving software diagnosability via log enhancement. ACM Transactions on
Computer Systems (TOCS) 30, 1 (2012), 1‚Äì28.

[51] Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang,
Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, Junjie Chen, Xiaoting He,
Randolph Yao, Jian-Guang Lou, Murali Chintalapati, Furao Shen, and Dong-
mei Zhang. 2019. Robust log-based anomaly detection on unstable log data. In
Proceedings of the ACM Joint Meeting on European Software Engineering Con-
ference and Symposium on the Foundations of Software Engineering, ESEC/SIG-
https:
SOFT FSE 2019, Tallinn, Estonia, August 26-30, 2019. ACM, 807‚Äì817.
//doi.org/10.1145/3338906.3338931

