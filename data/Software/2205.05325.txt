Open Problems in Fuzzing RESTful APIs:
A Comparison of Tools

Man Zhang, Andrea Arcuri
Kristiania University College, Norway

2
2
0
2

l
u
J

7

]
E
S
.
s
c
[

2
v
5
2
3
5
0
.
5
0
2
2
:
v
i
X
r
a

Abstract

RESTful APIs are a type of web services that are widely used in industry. In the last few
years, a lot of effort in the research community has been spent in designing novel techniques to
automatically fuzz those APIs to find faults in them. Many real faults were automatically found in
a large variety of RESTful APIs. However, usually the analyzed fuzzers treat the APIs as black-box,
and no analysis of what is actually covered in these systems is done. Therefore, although these
fuzzers are clearly useful for practitioners, we do not know what are their current limitations and
actual effectiveness. Solving this is a necessary step to be able to design better, more efficient and
effective techniques. To address this issue, in this paper we compare seven state-of-the-art fuzzers
on 18 open-source and one industrial RESTful APIs. We then analyzed the source code of which
parts of these APIs the fuzzers fail to generate tests for. This analysis points to clear limitations of
these current fuzzers, listing concrete challenges for the research community to follow up on.

Keywords: Automated test generation, SBST, fuzzing, REST, comparison

1

Introduction

RESTful APIs are widely used in industry. Arguably, REST is the most common kind of web service,
used to provide functionality (i.e., an API) over a network. Many companies worldwide provide services
via a RESTful API, like Google [8], Amazon [1], LinkedIn [12], Twitter [23], Reddit [15], etc. The
website ProgrammableWeb [14] currently lists more than 24 thousand APIs available on internet, where
the vast majority of them is implemented with REST. Furthermore, REST APIs are also commonly
used when developing backend enterprise applications using a microservice architecture [63].

Due to their wide use in industry, in recent years there has been a large interest in the research
community to design novel techniques to automatically test this kind of applications (e.g., [53, 28, 59,
66, 40, 65]). Compared to other kinds of applications (e.g., data parsers) and type of testing (e.g.,
unit testing), system testing of RESTful APIs has some unique challenges, like dealing with network
communications (e.g., HTTP over TCP) and with accesses to databases (e.g., Postgres and MySQL).
In the literature, different techniques have been proposed, which have been able to automatically
find many faults in existing APIs (e.g., [53, 28, 59, 66, 40, 65]). However, so far, most of the investigated
techniques are black-box, applied on remote web services on the internet. Although those fuzzers have
been useful to detect several faults, it is unclear how effective they truly are at testing these APIs. For
example, most studies do not report any form of code coverage, as the system under tests (SUTs) are
typically remote black-boxes with no access to their source code. For example, it can be possible that
most found faults are simply in the first layer of input validation of these SUTs, with little-to-no code
of their businesses logic executed [56]. Without an in-depth analysis of which parts of the SUT code is
not exercised by these fuzzers, it is not possible to understand their limitations. This hinders further
research development for novel techniques to achieve better results.

To address these issues, in this paper we compare seven state-of-the-art fuzzers, namely (in
alphabetic order) bBOXRT [53], EvoMaster [28], RESTest [59], RestCT [66], RESTler [40],
RestTestGen [65] and Schemathesis [50]. We applied them on 13 RESTful APIs running on the
JVM (i.e., 12 open-source and one industrial) and six open-source RESTful APIs running on NodeJS,
for which we collected line coverage metrics using JaCoCo [10] (for JVM) and c8 [5] (for NodeJS).
Experiments are carried both for black-box and white-box testing.

1

 
 
 
 
 
 
This large set of experiments (which takes roughly 71.2 days if run in sequence) enables us to
analyze in details what are the current limitations of the state-of-the-art in fuzzing RESTful APIs.
These tools achieve different degrees of code coverage, but there are still many issues that need to
be addressed to achieve better results. These include for example how to deal with underspecified
schemas, and how to deal with interactions with external services.

In this paper, we provide the following novel research contributions:

• We carried out one of the largest and most variegated to date empirical comparison of fuzzers

for RESTful APIs.

• Instead of using black-box metrics, we report actual coverage results on the employed APIs.

• We provide in-depth analyses of the current challenges in this testing domain.

The paper is organized as follows. Section 2 provides background info on the seven compared
tools. Related work is discussed in Section 3. Section 4 presents the details of the tool comparisons.
Section 5 provides the main scientific contribution of this paper, with an in-depth analysis of the
current open-problems identified by our empirical study. Threats to validity are discussed in Section 6.
Finally, Section 7 concludes the paper.

2 Background: Used Tools

In this paper, we compare seven different fuzzers for RESTful APIs, namely bBOXRT, EvoMaster,
RESTest, RestCT, RESTler, RestTestGen and Schemathesis. Here in this section they are
briefly described, in alphabetic order. For the tool comparisons, we used their latest released versions,
as per 23rd of May 2022.

To the best of our knowledge, this selection represents the current state-of-the-art in fuzzing RESTful
APIs, as those are the most used, most popular (e.g., number of stars and download stats from GitHub)
and cited work in the literature. For example, some tools in the literature like QuickRest [51] do
not seem available online, whereas others like ApiTester [46] have no documentation and have not
been updated in years [3]. The recent tool Morest [54] is not open-source, and its replication package
seems that provide no executable nor code to replicate its experiments1, only the data analysis. So, no
comparison seems possible with these other tools. In addition, there exist some open-source fuzzers
made by practitioners in industry, such as APIFuzzer [2], Dredd [11] and Tcases [22]. However, since
these fuzzers do not appear in the scientific literature (i.e., they do not have scientific articles describing
the techniques they use to generate test cases), and considering that in a previous study [52] these
tools achieve worse results compared to the other fuzzers, we do not include them in this study.

All these tools require a OpenAPI/Swagger [13] schema to operate. Based on such a schema, these
tools send syntactically valid HTTP requests, using different strategies (e.g., random and search-based)
to choose how to create the input data (e.g., query parameters and JSON body payloads).

bBOXRT [53] aims at black-box robustness testing of RESTful APIs. The tool is written in Java,
and its source code is available online on the authors’ institute pages [4], since 2020. We could not
find any released version, so we used the latest version on its Git master branch (commit 7c894247).
EvoMaster [28, 29, 30, 33, 31, 37, 72, 69, 35] is a search-based tool that uses evolutionary
computation to generate test cases for RESTful APIs. It supports both black-box and white-box
testing [31] (but this latter only for programs running on the JVM and JavaScript/TypeScript). The
tool is written in Kotlin, and open-sourced on GitHub [6], since 2016. For this study, we used version
1.5.0. We are the authors of this tool.

RESTest [59, 60, 58, 61, 64] is black-box testing tool, open-sourced on GitHub [17], since 2018.
One of the main features of this tool is the ability to analyze inter-dependencies among input parameters.
The tool is written in Java. We used its 1.2.0 version.

RestCT [66] uses Combinatorial Testing for black-box testing of RESTful APIs. It is open-sourced

on GitHub [16], since 2022. The tool is written in Python. We used its 1.0 version.

1https://anonymous.4open.science/r/morest-rest-8CAE/ currently gives a ‘‘The repository is expired’’ error

2

RESTler [40, 41, 48, 47] is a black-box fuzzer for RESTful APIs. It is open-sourced on GitHub [18],
since 2020. The tool is written in Python and F#. It does not have any published release, although its
Git repository has tagged commits. We use the version of the repository with the latest tag v8.5.0.

RestTestGen [65, 45] is another black-box fuzzer for RESTful APIs. It is written in Java, but
the original versions used in [65] are not open-source. Since late 2021, a new rewrite of the tool as
open-source is available on Github [19]. Then, in this study, we use a version with the latest tag
v22.01.

Schemathesis [50] is a black-box fuzzer which employs property-based testing techniques [55],
whose development started in 2019. The fuzzer is capable of deriving structure and semantics of SUTs
based on their API schemas, such as OpenAPI [13]. It is written in Python, and can be installed with
pip. In this study, we use the version 3.15.2 [21].

3 Related Work

To achieve better results (e.g., higher code coverage and fault detection), we need to understand what
are the limitations of current testing techniques. Throughout the years, different studies have been
carried out to provide this insight, like for example studying the limitations of Dynamic Symbolic
Execution (DSE) [68, 67] and Search-Based Software Testing (SBST) for unit testing [27, 49, 25].
However, to the best of our knowledge, no work has been carried out to study what are the limitations
of fuzzing RESTful APIs (most work is for black-box testing, where achieved code coverage is usually
not reported, and no analysis of what was not covered is carried out).

Regarding tool comparisons for fuzzing RESTful APIs, there has been some work in the literature.
For example, the authors of RestTestGen compared four black-box tools (RestTestGen, RESTler,
bBOXRT and RESTest) on 14 APIs [44]. These APIs are written in different languages (e.g., Java,
C#, PHP, JavaScript and Go), with the largest having up to 24044 lines of code. However, no code
coverage was reported. From this comparison, RestTestGen seems the most effective tool, whereas
RESTler is the most robust (i.e., could be used on more APIs without crashing).

The authors of RESTest and EvoMaster compared the two tools [57], aiming at studying
the tradeoffs between black-box and white-box testing. For example, they studied the impact on
performance of using ‘‘custom generators’’, with test data provided by the users.

The authors of RestCT, when they introduced their tool [66], compared it with RESTler on 11

APIs coming from two projects (i.e., GitLab and Bing Maps), showing better results for RestCT.

The authors of Morest [54] compared it against with black-box EvoMaster, RestTestGen and
RESTler, on 6 APIs. They claimed Morest gives the best results. However, as Morest is not available
for comparisons (in contrast to EvoMaster, RestTestGen and RESTler), such claims cannot be
independently verified.

In previous work, we compared EvoMaster’s black-box and white-box mode on 8 SUTs [31], 7
open-source and 1 industrial, showing better results for its white-box mode. The SUTs used in this
paper are a super-set of the open-source SUTs used in [31] (i.e., for the experiments on the JVM we
used the same SUTs plus another 5).

In parallel, at the same time of the first arXiv version of this paper [70], Kim et al. [52] made a
comparison of tools for fuzzing RESTful APIs. A total of different 9 tools (the same as here, minus
the recent RestCT, but plus the aforementioned APIFuzzer [2], Dredd [11] and Tcases [22]) were
compared on 20 APIs running on the JVM (where half of them are the same as here, which come from
our own EMB [7] repository of APIs we use to experiment with EvoMaster). To better differentiate
from this existing work that was done in parallel, to better generalize our results, here we include
as well 6 JavaScript/TypeScript APIs running on NodeJS, as well one industrial API coming from
one of our industrial partners. Furthermore, where the focus of [52] seems to be the comparison of
tools, our focus is on the in-depth analysis of the open problems in fuzzing RESTful APIs (Section 5).
The comparison of tools is only a mean to identify the one that gives the best results, as the tests
generated by such tool are the starting point of the in-depth analyses. In both studies [70, 52] black-box
EvoMaster gives the best results, closely followed by Schemathesis, and white-box testing gives
better results than black-box. Kim et al. [52] do not seem to be authors of any of the compared tools.

3

On the one hand, their comparison is therefore unbiased. This is different from our case, as we are the
authors of EvoMaster, which turned out to be the best in all these comparisons. One should always
be a bit skeptical of studies where the tool of the authors gives the best results, especially if such results
are not possible to be replicated by third-parties. This is one of the main reasons why EvoMaster
is open-source, with all its experiment scripts stored in its Git repository, automatically updated on
Zenodo for long term storage at each new release (e.g., version 1.5.0 [38]), as well preparing all the
SUTs for experiments in a single repository (i.e., EMB [7]). On the other hand, being the authors of
the most performant tool gives us a unique insight when analyzing and discussing the current open
problems in fuzzing RESTful APIs (Section 5).

4 Empirical Analysis

In this paper, we aim at answering the following research questions:

• RQ1: How do the seven compared black-box fuzzers fare in terms of line coverage?

• RQ2: What line coverage and fault detection results are obtained with white-box fuzzing?

• RQ3: What are the main open problems currently hindering the results?

To answer these research questions, we carried out 2 different sets of experiments: first for black-box
testing (RQ1 in Section 4.2), and then for white-box testing (RQ2 in Section 4.3), both on the same
case study (describe in Section 4.1). From the results of these experiments, the in-depth analysis of
the results for RQ3 follows in Section 5.

4.1 Case Study

Table 1: Statistics on 18 RESTful APIs from EMB [7] and one industrial API.

SUT

cyclotron
disease-sh-api
js-rest-ncs
js-rest-scs
realworld-app
spacex-api
catwatch
cwa-verification
features-service
gestaohospital-rest
ind0
languagetool
ocvn-rest
proxyprint
rest-ncs
rest-news
rest-scs
restcountries
scout-api

Total

Language

JavaScript
JavaScript
JavaScript
JavaScript
TypeScript
JavaScript
Java
Java
Java
Java
Java
Java
Java
Java
Java
Kotlin
Java
Java
Java

Files

25
57
8
13
37
63
106
47
39
33
75
1385
526
73
9
11
13
24
93

File LOCs

c8/JaCoCo LOCs

5803
3343
775
1046
1229
4966
9636
3955
2275
3506
5687
174781
45521
8338
605
857
862
1977
9736

2458
2997
768
1044
1077
3144
1835
711
457
1056
1674
45445
6868
2958
275
144
295
543
2673

2637(203,2434)

284898(17162,267736)

76422(11488,64934)

Note that, regarding Total(X,Y), Total represents the total of the statistic on all case studies, whereas X represents the
total on just the JavaScript/TypeScript APIs, and Y represents the total on the JVM APIs.

To carry out experiments in this paper, we used a collection of 19 RESTful APIs. As we need to
measure code coverage and analyze the source code to check which parts are not covered, we needed

4

open-source APIs that we could run on a local machine. Furthermore, to simplify the collection of code
coverage results, it is easier to use APIs written in the same programming language (e.g., Java), or at
least use not too many different languages, as each programming language would need to configure its
own code-coverage tool to analyze the test results. Considering that we wanted to do comparisons also
with white-box testing, which currently only EvoMaster supports, and that requires some manual
configurations (e.g., to set up bytecode instrumentation for the white-box heuristics), we decided to
use the same case study that we maintain for EvoMaster. In particular, we maintain a repository of
RESTful APIs called EMB [7], which is stored as well on Zenodo [39]. Table 1 shows the statistics of
these 19 APIs. Note that one of these APIs is coming from one of our industrial partners, which of
course we are not allowed to store on EMB.

These 19 APIs are written in four different languages: Java, Kotlin, JavaScript and TypeScript.
They run on 2 different environments/runtimes: the JVM and NodeJS. For each SUT, we report the
number of total source files (i.e., ending in either .java, .kt, .js or .ts), and their number of lines
(LOCs). As these include as well import statements, empty lines, comments and tests, for the APIs
running on the JVM we also report the number of actual line targets for code coverage (measured
with the tool JaCoCo [10]).

For the APIs running on the NodeJS, the code coverage is measured with the tool c8, which
uses native V8 coverage. By default, the tool c8 will count code coverage only for the files which
are loaded by the engine [5]. For instance, regarding cyclotron, different LOCs between Files
and c8 are due to unreachable files (i.e., api.analyticselasticsearch.js, api.analytics.js,
api.statistics-elasticsearch.js). However, all of the files will be only reached by manually
modifying a configuration in config.js, i.e., the default value is false. Therefore, we report the
number of line targets measured by c8 that could be loaded with the default SUT settings.

4.2 RQ1: Black-Box Testing Experiments

For each SUT, we created Bash scripts to start them, including any needed dependency (e.g., as
ocvn-rest uses a MongoDB database, this is automatically started with Docker). Each SUT is started
with either JaCoCo (for JVM) or c8 (for NodeJS) instrumentation, to be able to collect code coverage
results at the end of each tool execution.

Each of the seven compared tools was run on each of the 19 SUTs, repeated for 10 times to keep
into account the randomness of these tools. This results in a total of 7 × 19 × 10 = 1330 Bash scripts.
Each script starts a SUT as a background process, and then one of the tools. Each script runs the
SUT on a different TCP port, to enable running any of these scripts in parallel on the same machine.
The code coverage is computed based on all the HTTP calls done during the fuzzing process, and
not on the output of the generated test files (if any). This was done for several reasons: not all tools
generate test cases in JUnit on JavaScript format, the generated tests might not compile (i.e., bugs
in the tools), and setting up the compilation of the tests and running them for collecting coverage
would be quite challenging to automate (as each tool behaves differently). This also means that, if a
tool crashes, we are still measuring what code coverage it achieves. If a tool crashes immediately at
startup (e.g., due to failures in parsing the OpenAPI/Swagger schemas), we are still measuring the
code coverage achieved by the booting up of the SUT.

In each script, each tool was run for 1 hour, for a total of 1330 hours, i.e., 55.4 days of computation
efforts. The experiments were run on a Windows 10 machine with a processor Intel(R) Xeon(R),
2.40GHz, 24 Cores with 192G of RAM. Note: the choice of the runtime for each experiment might
impact the results of the comparisons. The choice of 1 hour is technically arbitrary, but based on what
practitioners might want to use these fuzzers in practice, and also not too long to make running all
these experiments unviable in reasonable time.

Regarding the selected seven fuzzers, there exist fuzzers which do not provide an option to configure
a global time budget to terminate fuzzing (e.g., Schemathesis [20] and RESTest [17]). Also, although
some fuzzers provide the option of a timeout (e.g., RestTestGen), they might terminate much earlier
than the specified timeout value [45]. To make the comparison of the fuzzers more fair by applying
the same time budget, for each fuzzer we run it in a loop with 1 hour timeout (i.e., if the fuzzer runs
out of time, it would be terminated, and if the fuzzer completes but there is still some time remaining,

5

it would be re-started to generate more tests). Thus, all coverage we collected are based on 1 hour
time budget for all the fuzzers.

To compare these tools, we use the line coverage reported by JaCoCo and c8 as metric. Another
important metric would be fault detection. However, how to compute fault detection in unbiased way,
applicable for all compared tools, is far from trivial. Each tool can self-report how many faults they
find, but how such fault numbers are calculated might be very different from tool to tool, making any
comparison nearly meaningless. Manually checking (possibly tens of) thousands of test cases is not a
viable option either. Therefore, for this type of experiments line coverage was the most suitable metric
for the comparisons. We are still going to compare fault detection, but for the white-box experiments
(Section 4.3), as we use the same tool (i.e., EvoMaster).

Some authors like in [52] use the number of unique exception stack traces in the SUT’s logs as a
proxy of detected faults. On the one hand, a tool that leads the SUT to throw more exceptions could
be considered better. On the other hand, using such metric as proxy for fault detection has many
shortcomings, like for example: (1) not all exceptions reported in the logs are related to faults; (2)
not all SUTs actual print any logs by default (e.g., this is the case for several SUTs in our study); (3)
crashes (which lead to responses with HTTP status code 500) are only one type of faults in RESTful
APIs detected by these fuzzers [56], where for example all faults related to response mismatches with
the API schema would leave no trace in the logs. For all these reasons, we did not do this kind of
analysis on the logs, as we do not think they provide much more info compared to line coverage.
Especially considering that the infrastructure to do such analysis would need to be implemented, which
might not be a trivial task.

Regarding experiment setup, as a black-box testing tool for fuzzing REST APIs, all tools are required
to configure where the schema of the API is located. In all these SUTs used in our case study, the
schemas are provided by the SUTs directly via an HTTP endpoint. But, we found that most of the tools
do not support to fetch the schema with a given URL, such as http://localhost:8080/v2/api-docs.
To conduct the experiments with these tools, after the SUT starts, we developed a Bash script which
manages to fetch the schema and download it to the local file system, and then configure a file-path
for the tools to specify where the schema can be accessed.

Regarding additional setups to execute the tools, EvoMaster, RestCT and Schemathesis were
the simplest to configure, as they require only one setup step, as all of their options can be specified with
command-line arguments. However, RestCT currently does not work directly on Windows [16]. So,
for these experiments, we simply ran it via the Windows Subsystem for Linux (WSL). RestTestGen
requires having a JSON file (i.e., rtg_config.json) to configure the tool with available options [19].
RESTler requires multiple setup steps, e.g., RESTler needs to generate grammar files first, and
then employ such grammars for fuzzing. However, with its online available documentation, we could
write a Python script about how to run the tool. For RESTest, it requires a pre-step to generate a
test configuration in order to employ the tool, and such generation could be performed automatically by
a utility CreateTestConf provided by the tool. In order to use bBOXRT, a Java class file is required
to load and set up the API specification. At the time of writing this paper, there does not exist specific
documentation about how to specify such Java class. However, in its open-source repository, there
exist many examples that helped us to create these Java classes for the SUTs in our case study. Note
that, for these experiments in this paper, all the above setups were performed automatically with our
Bash scripts.

The first time we ran the experiments, we could collect results only for EvoMaster, RESTler
and Schemathesis. All the other tools failed to make any HTTP calls. This was due for example
to mismatched schema format, or missing/misconfigured info in the schemas. More specifically,
bBOXRT only allows a schema with YAML format. In the SUTs used in this study, there is only
one specified with YAML (i.e., restcountries) out of the 19 schemas (the remaining ones use JSON).
RestTestGen only accepts a schema with OpenAPI v3 in a JSON format [19]. There are only two
(i.e., cwa-verification and spacex-api) out of the 19 SUTs which expose OpenAPI v3 in the JSON
format. In addition, RESTest and RestTestGen need the protocol info (e.g., http or https with
servers/schemes tag) in the OpenAPI/Swagger schema. But since the servers/schemes tag is not
mandatory, such info might not be always available in the schema. For example, seven (i.e., cyclotron,

6

disease-sh-api, js-rest-ncs, js-rest-scs, realworld-app, features-service and restcountries) out of the
19 SUTs have such protocol info specified in their schemas. In addition, to create HTTP requests,
RestCT, RESTest and RestTestGen require info specified in host (for schema version 2) and
servers (for schema version 3), but such info (typically related to TCP port numbers) might not
be fully correct (e.g., the host and TCP port might refer to the production settings of the API, and
not reflecting when the API is running on the local host on an ephemeral port for testing purposes).
Those three tools do not seem to provide ways to override such info. For instance, in 19 SUTs , 10
SUTs (i.e., cyclotron, disease-sh-api, js-rest-ncs, js-rest-scs, realworld-app, spacex-api, cwa-verification,
features-service, languagetool and restcountries) are specified with a hard-coded TCP port, and in
one SUT (i.e., scout-api) the TCP port is unspecified. To avoid these issues in accessing the SUTs,
we developed a utility using swagger-parser library that facilitates converting formats of schemas
between JSON and YAML (only applied for bBOXRT and RestTestGen), converting OpenAPI v2
to OpenAPI v3 (only applied for RestTestGen), adding missing schemes info, and correcting/adding
host and servers info in the schemas.

Once these changes in the schemas were applied, we repeated the experiments, to collect data from
all the seven tools. Ideally, these issues should be handled directly by the tools. But, as they are rather
minor and only required changes in the OpenAPI/Swagger schemas, we decided to address them, to
be able to collect data from all the seven tools and not just from three of them.

In addition, we needed to configure authentication info for five APIs, namely proxyprint, scout-api,
ocvn-rest, realworld-app and spacex-api. For proxyprint, scout-api and spacex-api, they need static
tokens sent via HTTP headers. This was easy to setup in RestCT, EvoMaster and Schemathesis,
just by calling these tools with a --header input parameter. RESTest required to rewrite the test
configuration file by appending authentication configuration. RESTler and RestTestGen required
to write some script configurations. bBOXRT has no documentation to setup authentication, but we
managed to setup it up by studying the examples it provides in its repository.

Regarding ocvn-rest and realworld-app, for authentication, it requires to make a POST call to a
form-login endpoint, and then use the received cookie in all following HTTP requests. Out of the seven
compared tools, it seems RESTler, bBOXRT, Schemathesis, and RestTestGen could directly
support this kind of authentication by setting up it with an executable script. Given the provided
documentation, we did not manage to configure it, as it requires to write different scripts for different
fuzzers to manually make such HTTP login calls then handle responses. Technically, by writing
manual scripts, it could be possible to use EvoMaster, RestCT and RESTest as well, by passing
the obtained cookie with --header option or the test configuration file. As doing all this was rather
cumbersome, and considering that for this API the authentication is needed only for admin endpoints,
we decided to do not spend significant time in trying to setup this kind dynamic authentication tokens.
Table 2 shows the results of these experiments. For each tool, we report the average line coverage,
as well as the min and max values out of the 10 runs. Each tool is then ranked (1 to 7) based on their
average performance on each SUT (where 1 is the best rank).

From these results we can infer few interesting observations. First, regarding the ranking, Evo-
Master seems the best black-box fuzzer (best in 11 out of 19 SUTs, with an average coverage of
56.8%,), closely followed by Schemathesis (best in 7 SUTs, with an average coverage of 54.5%).
Then, the remaining tools can be divided in 2 groups: bBOXRT and RestTestGen with similar
coverage 41.6-45.4%, and then RESTler, RESTest and RestCT with similar coverage 33.6-35.4%.
These results confirm a previous study [44] showing that RestTestGen gives better results than
RESTler and RESTest, as well as RestCT being better than RESTler [66] (although in this case
the difference in average coverage is small, only 0.8%). Compared to the analyses in [52], interestingly
the ranking of the tools is exactly the same (recall that, out of the combined 29 APIs between our and
their study, only 10 APIs used in these empirical studies are the same).

On all APIs but 1, either EvoMaster or Schemathesis gives the best results. The exception is
the industrial API, where 5 tools achieve the same coverage of 8.2%, on all their 10 runs. We will
discuss this interesting case in more details in Section 5. In 12 APIs, either EvoMaster is the best
followed by Schemathesis, or the other way round. There is no single API in which EvoMaster
and Schemathesis were not at least the third best.

7

The other 7 APIs (including the industrial one) show some interesting behavior for the other 5
tools. For example, for js-rest-scs, RestTestGen gives the second-best results, with an average
86.4%, compared to the 86.1% of Schemathesis. The interesting aspect here is that, out of the
10 runs, RestTestGen has worse minimum coverage (85.4% vs. 85.9%) and worse maximum
coverage (87.1% vs. 87.4%), although the average is higher (86.4% vs. 86.1%). This can happen
when randomized algorithms are used. In gestaohospital-rest, RestTestGen and Schemathesis
have similar performance (i.e., 57.2% and 58.7%), whereas EvoMaster is quite behind (i.e., 50.8%).
Similarly, the performance of RestTestGen and Schemathesis are very similar on rest-scs (65.3%
and 64.8%) and restcountries (75.4% and 73.9%), where EvoMaster is better only by a small
amount (66.9% on rest-scs and 76.1% on restcountries). In languagetool, RESTest is better than
Schemathesis, but the difference is minimal (only 0.3%). In rest-ncs, there is large gap in performance
between EvoMaster (64.5%) and Schemathesis (94.1%), where the second best results are given by
RestCT (85.5%). Finally, on scout-api, RESTler is better than Schemathesis (26.5% vs. 23.0%),
although it is way behind EvoMaster (36.7%).

Another interesting observation is that there is quite a bit variability in the results of these
fuzzers, as they use randomized algorithms to generate test cases. Let us highlight some of the most
extreme cases in Table 2, for EvoMaster and Schemathesis. On languagetool, out of the 10 runs,
EvoMaster has a gap of 9.1% between the best (35.1%) and worst (26.0%) runs. On scout-api, the
gap is 8.3%. For Schemathesis, the gap on features-service is 10.5%, and 6.4% on gestaohospital-rest.
This is yet another reminder of the peculiar nature of randomized algorithms, and the importance of
how to properly analyze them. For example, doing comparisons based on a single run is unwise.

Statistical tests [32] are needed when claiming with high confidence that one algorithm/tool is
better than another one. In this particular case, we compare EvoMaster’s performance with all the
other tools, one at a time on each SUT (so 6 × 19 = 114 comparisons), and report the p-values of
the Mann-Whitney-Wilcoxon U-Test in Table 3. Apart from very few cases, the large majority of
comparisons are statistically significant at level α = 0.05. Often, 10 repetitions might not be enough to
detect statistically significant differences, and higher repetition values like 30 and 100 are recommended
in the literature [32]. However, here the performance gaps are large enough that 10 repetitions were
more than enough in most cases.

When looking at the obtained coverage values, all these tools achieve at least a 30% coverage on
average. Only two of them (i.e., EvoMaster and Schemathesis) achieve more than 50%. But no
tool goes above 60% coverage. This means that, although these tools might be useful for practitioners,
there are still several research challenges that need to be addressed (we will go in more details on this
in Section 5). However, what level of coverage can be reasonably expected from black-box tools (which
have no info on the source code of the SUTs) is a question that is hard to answer.

RQ1: all compared tools achieve at least 30% line coverage on average, but none goes above 60%.
Of the 7 compared tools, EvoMaster seems the one giving the best results, closely followed by
Schemathesis.

8

Table 2: Experiment results of the seven black-box fuzzers on the 19 RESTful APIs. For each tool we report the average line coverage, as well as its
[min,max] values out of the 10 runs. Each tool also has a (rank) based on its performance on each SUT. For each SUT, the results of the best tools (e.g.,
rank (1)) are in bold.

9

SUT

cyclotron
disease-sh-api
js-rest-ncs
js-rest-scs
realworld-app
spacex-api
catwatch
cwa-verification
features-service
gestaohospital-rest
ind0
languagetool
ocvn-rest
proxyprint
rest-ncs
rest-news
rest-scs
restcountries
scout-api

bBOXRT

EvoMaster BB

RestCT

RESTler

RESTest

RestTestGen

Schemathesis

41.3 [41.3,41.3] (5) 70.3 [70.1,72.3] (1)
60.8 [60.8,60.9] (2)
56.4 [55.4,57.4] (3)
70.2 [67.3,71.1] (5)
93.0 [89.8,95.8] (2)
83.2 [83.0,83.5] (5) 88.7 [87.5,89.5] (1)
69.4 [69.4,69.4] (2)
64.2 [62.8,66.4] (4)
76.1 [76.1,76.2] (6)
84.7 [84.7,84.8] (2)
31.0 [29.2,31.8] (3) 35.9 [34.3,36.9] (1)
43.4 [42.9,43.5] (4)
49.4 [49.1,49.6] (2)
35.7 [35.7,35.7] (4) 59.7 [58.4,62.1] (1)
50.8 [44.8,54.3] (3)
36.2 [36.2,36.2] (4)
8.2 [8.2,8.2] (3)
8.2 [8.2,8.2] (3)
1.7 [1.7,1.7] (6) 32.6 [26.0,35.1] (1)
10.1 [10.1,10.1] (5) 27.5 [27.5,27.6] (1)
4.2 [4.2,4.2] (5) 34.0 [32.5,35.0] (1)
55.0 [52.4,56.4] (5)
64.5 [64.4,64.7] (3)
34.9 [34.0,36.8] (5) 69.4 [69.4,69.4] (1)
60.2 [59.3,61.4] (6) 66.9 [64.4,70.2] (1)
65.5 [63.7,68.5] (5) 76.1 [76.1,76.1] (1)
18.1 [18.1,18.1] (5) 36.7 [32.8,41.1] (1)

41.3 [41.3,41.3] (5)
48.4 [48.4,48.4] (6.5)
92.2 [87.5,92.7] (3)
83.2 [83.1,83.2] (6)
59.7 [59.7,59.7] (6)
76.1 [76.1,76.2] (7)
9.7 [9.7,9.7] (7)
21.9 [21.9,21.9] (6)
21.0 [21.0,21.0] (6)
19.9 [19.9,19.9] (7)
7.6 [7.6,7.6] (6.5)
1.5 [1.5,1.5] (7)
10.1 [10.1,10.1] (5)
4.2 [4.2,4.2] (5)
85.5 [85.5,85.5] (2)
13.9 [13.9,13.9] (6.5)
60.5 [60.3,61.0] (5)
3.5 [3.5,3.5] (7)
12.0 [12.0,12.0] (6.5)

41.3 [41.3,41.3] (5)
48.5 [48.5,48.5] (4)
44.3 [44.3,44.3] (6.5)
54.1 [54.1,54.1] (7)
66.5 [66.5,66.5] (3)
76.3 [76.3,76.4] (3)
17.3 [14.5,20.5] (5)
21.9 [21.9,21.9] (6)
21.0 [21.0,21.0] (6)
21.5 [21.5,21.5] (6)
7.6 [7.6,7.6] (6.5)
1.9 [1.9,1.9] (4.5)
10.1 [10.1,10.1] (5)
4.2 [4.2,4.2] (5)
40.7 [40.7,40.7] (6)
44.4 [44.4,44.4] (4)
58.3 [58.3,58.3] (7)
50.6 [50.6,50.6] (6)
26.5 [26.4,26.6] (2)

41.3 [41.3,41.3] (5)
48.5 [48.4,48.5] (5)
44.3 [44.3,44.3] (6.5)
84.1 [83.3,84.6] (4)
59.7 [59.7,59.7] (6)
76.3 [76.3,76.3] (5)
20.8 [15.9,23.8] (4)
21.9 [21.9,21.9] (6)
21.0 [21.0,21.0] (6)
29.0 [28.1,32.0] (5)
8.2 [8.2,8.2] (3)
2.5 [2.5,2.5] (2)
10.1 [10.1,10.1] (5)
4.2 [4.2,4.2] (5)
5.1 [5.1,5.1] (7)
13.9 [13.9,13.9] (6.5)
61.7 [61.4,62.4] (4)
73.0 [71.5,74.2] (4)
12.0 [12.0,12.0] (6.5)

41.3 [41.3,41.3] (5)
48.4 [48.4,48.4] (6.5)

69.1 [67.7,69.7] (2)
61.5 [61.4,61.6] (1)
88.6 [85.8,93.0] (4) 100.0 [100.0,100.0] (1)
86.1 [85.9,87.4] (3)
86.4 [85.4,87.1] (2)
69.7 [69.1,69.8] (1)
59.7 [59.7,59.7] (6)
85.4 [85.3,85.6] (1)
76.3 [76.3,76.4] (4)
35.8 [33.6,39.1] (2)
15.1 [12.3,18.3] (6)
49.5 [49.5,49.5] (1)
43.8 [43.3,43.9] (3)
52.0 [46.6,57.1] (2)
45.9 [45.1,46.8] (3)
58.7 [55.9,62.3] (1)
57.2 [51.7,58.7] (2)
8.2 [8.2,8.2] (3)
8.2 [8.2,8.2] (3)
1.9 [1.9,1.9] (4.5)
2.2 [2.1,2.5] (3)
27.5 [27.5,27.7] (2)
10.1 [10.1,10.1] (5)
4.4 [4.4,4.4] (2)
4.2 [4.2,4.2] (5)
94.1 [93.1,94.5] (1)
64.3 [64.0,64.7] (4)
68.8 [67.4,70.8] (2)
47.2 [47.2,47.2] (3)
64.8 [64.4,65.1] (3)
65.3 [64.4,67.1] (2)
73.9 [72.4,75.0] (3)
75.4 [73.5,76.6] (2)
23.0 [22.1,25.1] (3)
23.0 [21.4,23.8] (4)

Average

41.9 (4.6)

56.8 (1.6)

35.4 (5.8)

34.6 (5.1)

33.6 (5.0)

45.4 (3.9)

54.5 (1.9)

Table 3: p-values of the Mann-Whitney-Wilcoxon U-Test of EvoMaster’s results compared to all
the other tools. Values lower than the α = 0.05 threshold are reported in bold.

SUT

cyclotron
disease-sh-api
js-rest-ncs
js-rest-scs
realworld-app
spacex-api
catwatch
cwa-verification
features-service
gestaohospital-rest
ind0
languagetool
ocvn-rest
proxyprint
rest-ncs
rest-news
rest-scs
restcountries
scout-api

bBOXRT RestCT RESTler RESTest RestTestGen Schemathesis

0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
NaN
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001

0.001
0.001
0.721
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001

0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001

0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
NaN
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001

0.001
0.001
0.003
0.001
0.001
0.001
0.001
0.001
0.001
0.001
NaN
0.001
0.001
0.001
0.179
0.001
0.039
0.417
0.001

0.001
0.001
0.001
0.001
0.001
0.001
0.909
0.434
0.001
0.001
NaN
0.001
0.033
0.001
0.001
0.069
0.007
0.001
0.001

4.3 RQ2: White-Box Testing Experiments

Out of the seven compared tools, only EvoMaster supports white-box testing. EvoMaster uses
evolutionary computation techniques, where the bytecode of the SUTs is instrumented to compute
different kinds of heuristics. Due to possible conflicts with JaCoCo and c8 instrumentation, and due to
the fact that EvoMaster uses its own driver classes (which need to be written manually) to start and
stop the instrumented SUTs, this set of experiments was run differently compared to the black-box
ones.

We ran EvoMaster on each SUT for 10 repetitions (so, 190 runs), each one for 1 hour (like for
the black-box experiments). However, JaCoCo and c8 are not activated. After each run, EvoMaster
generates statistic files, including information on code coverage and fault detection. However, as there
can be differences on how line coverage is computed between EvoMaster and JaCoCo/c8, it would
be difficult to reliably compare with the results in Table 2. Therefore, for the comparisons, we ran
EvoMaster as well in grey-box mode (for another 190 runs). This mode generates test cases in
exactly the same way as black-box mode, with difference the SUT is instrumented, and coverage
metrics are computed at each test execution. A further benefit is that, besides code coverage, we can
reliably compare fault detection as well, as this metric is computed in exactly the same way (as it is the
same tool). As EvoMaster is the black-box fuzzer that gives the highest code coverage (Section 4.2),
it is not a major validity threat to compare white-box results only with EvoMaster. These 380 runs
added a further 15.8 days of computational effort.

Table 4 shows these results. Few things appear quite clearly. First, on average, line coverage goes
up by 7.5% (from 45.4% to 52.9%). Even with just 10 runs per API, results are statistically significant
in most cases.

For APIs like rest-ncs, average coverage can go even higher than 90%. For other APIs like
features-service improvements are more than 13% (e.g., from 68.8% to max 81.1%). In the case of
ind0 , although achieved coverage is relative low (i.e., less than 20%), it still more than double (i.e.,
from 8.4% to 18.8%).

Although results are significantly improved compared to black-box testing, for nearly half of these
SUTs it was still not possible to achieve more than 50% line coverage. Also, there are two interesting

10

Table 4: Experiment results for white-box and grey-box testing using EvoMaster on the 19 RESTful
APIs. We report the average line coverage measured with EvoMaster itself (not JaCoCo, nor c8), as
well as the number of faults detected in these APIs. When the differences are statically significant at
α = 0.05 level, the effect-sizes A12 are reported in bold.

SUT

Line Coverage %

# Detected Faults

RS WB

RS WB

cyclotron
disease-sh-api
js-rest-ncs
js-rest-scs
realworld-app
spacex-api
catwatch
cwa-verification
features-service
gestaohospital-rest
ind0
languagetool
ocvn-rest
proxyprint
rest-ncs
rest-news
rest-scs
restcountries
scout-api

31.2
18.1
60.7
60.9
24.6
41.5
41.1
40.4
68.8
39.4
8.4
28.9
35.3
53.3
61.9
55.4
63.3
74.9
54.7

ˆA12
0.69
31.3
19.5 1.00
83.2 1.00
76.2 1.00
24.4 0.03
41.5
0.58
50.0 1.00
46.9 1.00
81.8 1.00
39.5 0.72
18.8 1.00
39.5 0.93
24.8 0.00
51.6
0.28
93.0 1.00
66.5 1.00
86.2 1.00
77.1 1.00
0.37
53.4

p-value

0.158
0.001
0.001
0.001
0.001
0.540
0.001
0.001
0.001
0.024
0.001
0.001
0.001
0.120
0.001
0.001
0.001
0.001
0.346

32.2
30.0
6.0
1.0
25.0
52.2
18.0
4.0
22.2
15.0
1.0
16.6
279.9
84.1
5.0
5.0
1.0
2.0
94.0

4.0

6.0
1.0

ˆA12
30.9 0.13
36.2 1.00
0.50
0.50
30.5 1.00
50.9
0.32
23.7 1.00
0.50
31.4 1.00
22.0 1.00
48.1 1.00
0.55
12.8
257.9 0.00
86.3
0.59
6.0 1.00
7.8 1.00
12.0 1.00
2.0
0.50
88.6 0.14

p-value

0.004
0.001
1.000
1.000
0.001
0.170
0.001
1.000
0.001
0.001
0.001
0.731
0.001
0.531
0.001
0.001
0.001
1.000
0.008

Average

45.4

52.9

0.77

36.5

39.9

0.67

cases in which results with white-box testing are actually significantly worse, i.e., for realworld-app
and ocvn-rest. In the former case, the difference is minimal, just 0.2%. In the latter case, though, the
difference is substantial, as it is 10.5% (from 35.3% down to 24.8%). This is not an unusual behavior
for search algorithms, as it all depends on the quality of the fitness function and the properties of
the search landscape [43]. If a fitness function gives no gradient to the search algorithm, it can easily
get stuck in local optima. In such cases, a random search can give better results. EvoMaster uses
several heuristics in its fitness function to try to maximize code coverage, but those do not work for
ocvn-rest, as we will discuss in more details in Section 5. However, improving the fitness function in
this case can be done (which we will address in future versions of EvoMaster).

Regarding fault detection, white-box testing leads to detect more faults. This is not unexpected, as
usually there is a strong correlation between code coverage and fault detection [42]. For example, you
cannot detect a fault if the code it hides in is never executed. The interesting aspect here is that the
improvement is not much, just 3.4 more faults on average. In these experiments, random testing can
find 36.5 faults on average, and so the relative improvement is less than 10%. For the problem domain
of fuzzing RESTful APIs (and likely Web APIs in general), this is not surprising [56]. Many of these
APIs do crash as soon as an invalid input is provided, instead of returning a proper user error response
(e.g., with HTTP status code in the 4xx family). And random search is quite good at generating invalid
input values. So, many faults can be easily found this way with a fuzzer at the very first layer of input
validation in the SUT’s code, even when the achieved code coverage is relatively low.

RQ2: White-box fuzzing leads to significantly higher results compared to black-box fuzzing, up to
7.5% more code coverage and 3.4 more detected faults on average. But, still, for many APIs
coverage results were less than 50%.

11

5 RQ3: Open Problems

We ran seven tools/configurations on 19 RESTful APIs, with a budget of 1 hour per experiment.
However, only in one single case it was possible to achieve 100% line coverage (i.e., Schemathesis on
js-rest-ncs, recall Table 2). In general, it might not be possible to achieve 100% line coverage, as some
code can be unreachable. This is for example a common case for constructors in static-method-only
classes, and catch blocks for exceptions that cannot be triggered with a test. However, for several of
these SUTs it was not even possible to reach 50% line coverage.

It is not in the scope of this paper to define what would be a good coverage target to aim for (80%?
90%?). However, clearly the higher the code coverage the better it would be for practitioners using
these fuzzers. So, to improve those fuzzers, it is of paramount importance to understand what are their
current limitations. To answer this question, we studied in details the logs of those tools in Section 5.1,
and large parts of the source code of the SUTs in Section 5.2 (recall that those are more than 280
thousand LOCs, see Table 1). An in-depth analysis of the current open-problems in fuzzing RESTful
APIs is an essential scientific step to get more insight into this problem. This is needed to be able to
design novel, more effective techniques.

To be useful for researchers, these analyses need to be ‘‘low-level’’, with concrete discussions about
the source code of these APIs. No general theories can be derived without first looking at and analyzing
the single instances of a scientific/engineering phenomenon. As the following software engineering
discussions might be considered as ‘‘dull’’ for some readers, or if the reader is not currently active in
the development of a fuzzer, we suggest to jump directly to our summarizing discussions in Section 5.3.
It is important to stress out that the goal of these analyses is to identify general problems, or
instances of problems that are likely going to be present in other SUTs as well. Designing novel
techniques that just overfit for a specific benchmark is of little to no use. Ultimately, what is important
will be the results that the practitioners can obtain when using these fuzzers on their APIs.

5.1 Analysis Of The Logs

From the tool logs, at least four common issues are worth to discuss. First, OpenAPI/Swagger
schemas might have some errors (e.g., this is the case for cyclotron, disease-sh-api, cwa-verification,
features-service proxyprint, and ocvn-rest). This might happen when schemas are manually written,
as well as when they are automatically derived from the code with some tools/libraries (as those
tools might have faults). In these cases, most fuzzers just crash, without making any HTTP call or
generating any test case. It is important to warn the users of these issues with their schema, but
likely fuzzers should be more robust and not crash, e.g., endpoints with schema issues could be simply
skipped.

The second issue can be seen in languagetool. Most fuzzers for RESTful APIs support HTTP
body payloads only in JSON format. But, in HTTP, any kind of payload type can be sent. JSON
is the most common format for RESTful APIs [62], but there are others as well like XML and the
application/x-www-form-urlencoded used in languagetool. From the results in Table 2, it looks like
only EvoMaster supports this format. On this API, EvoMaster achieves between 26% and 35.1%
code coverage, whereas no other fuzzer achieves more than 2.5%.

The third issue is specific to scout-api, which displays a special case of the JSON payloads.
Most tools assume a JSON payload to be a tree: e.g., a root object A that can have fields that are
object themselves (e.g., A.B), and so on recursively (e.g., A.B.C.D and A.F.C), in a tree-like structure.
However, an OpenAPI/Swagger schema can define objects that are graphs, as it is the case for scout-api.
For example, an object A can have a field of type B, but B itself can have a field of type A. This recursive
relation creates a graph, that needs to be handled carefully when instantiating A (e.g., optional field
entries can be skipped to avoid an infinite recursion, which otherwise would lead the fuzzers to crash).
The fourth issue is related to the execution of HTTP requests towards the SUTs. To test a REST
API, the fuzzers build HTTP requests based on the schema, and then use different HTTP libraries
to send the requests toward the SUT, e.g., JerseyClient is used in EvoMaster. By analyzing
the logs, we found that for several case studies, some fuzzers seem to not have problems in parsing
schemas, but they failed to execute the HTTP requests. For instance, javax.net.ssl.SSLException:

12

1 2022 -06 -12 12:30:28.112: Sending : ’ GET // api / fisher /1/1/1.23 HTTP /1.1\ r \ nAccept :

application / json \ r \ nHost : localhost :25900\ r \ nContent - Length : 0\ r \ nUser - Agent : restler
/8.5.0\ r \ n \ r \n ’

2
3 2022 -06 -12 12:30:28.126: Received : ’ HTTP /1.1 404 Not Found \ r \ nX - Powered - By : Express \ r \

nContent - Security - Policy : default - src \ ’ none \ ’\ r \ nX - Content - Type - Options : nosniff \ r \
nContent - Type : text / html ; charset = utf -8\ r \ nContent - Length : 159\ r \ nDate : Sun , 12 Jun
2022 10:30:28 GMT \ r \ nConnection : keep - alive \ r \ nKeep - Alive : timeout =5\ r \ n \ r \n <! DOCTYPE
html >\ n < html lang =" en " >\n < head >\ n < meta charset =" utf -8" >\ n < title > Error </ title >\ n </ head
>\ n < body >\ n < pre > Cannot GET // api / fisher /1/1/1.23 </ pre >\ n </ body >\ n </ html >\ n ’

(a) RESTler on js-rest-ncs built with NodeJS and Express. By default, a path with extra slash will not
match the correct endpoint.

1 2022 -06 -11 22:55:56.651: Sending : ’ GET // api / fisher /1/1/1.23 HTTP /1.1\ r \ nAccept :

application / json \ r \ nHost : localhost :24850\ r \ nContent - Length : 0\ r \ nUser - Agent : restler
/8.5.0\ r \ n \ r \n ’

2
3 2022 -06 -11 22:55:56.697: Received : ’ HTTP /1.1 200 \ r \ nContent - Disposition : inline ; filename =
f . txt \ r \ nContent - Type : application / json ; charset = UTF -8\ r \ nTransfer - Encoding : chunked \ r \
nDate : Sat , 11 Jun 2022 20:55:55 GMT \ r \ n \ r \ n38 \ r \ n {" resultAsInt ": null ," resultAsDouble
" : 0 . 5 3 2 8 8 8 6 5 4 0 7 2 0 1 4 1 } \ r \ n0 \ r \ n \ r \n ’

(b) RESTler on rest-ncs built with Spring Boot 2.0.3 whose default path matching strategy allows the
double slash.

Figure 1: Different handling of requests with different techniques

Unsupported or unrecognized SSL message was thrown when RestTestGen processed realworld-
app with OkHttpClient. For js-rest-ncs, js-rest-scs and ind0 , we found that 404 Not Found responses
were always returned when fuzzing them with RESTler v8.5.0. By checking the logs obtained by
RESTler, we found that it might be due to a problem in generating the right URLs for making the
HTTP requests. For the SUT whose basePath is /, RESTler seems to generate double slash (i.e.,
//) in the URL of the requests. However, whether to accept the double slash to match a real path
depends on the SUTs, i.e., js-rest-ncs, js-rest-scs and ind0 do not allow it. In Figure 1, we provide the
logs obtained by RESTler, representing the processed requests which contain the double slash and
responses returned by js-rest-ncs (Figure 1a) and rest-ncs (Figure 1b).

5.2 Analysis Of The Source Code

For each SUT, we manually ran the best (i.e., highest code coverage) test suite (which are the ones
generated by EvoMaster) with code coverage activated. Then, in an IDE, we manually looked at
which branches (e.g., if statements) were reached by the test case execution, but not covered. As
well as looking at the cases in which the test execution is halted in the middle of a code block due to
thrown exceptions. This is done to try to understand what are the current issues and challenges that
these fuzzers need to overcome to get better results. We will discuss here each SUT, in alphabetic
order, one at a time.

Note that, when we refer to the code coverage achieved by white-box EvoMaster, we refer to the
results in Table 4. These coverage values are computed with the instrumentation of EvoMaster
itself, and they are not exactly the same as what other coverage tools like JaCoCo and c8 would report
(as there can be differences on how coverage is computed). For example, coverage results reported
by c8 might be significantly higher, as those are computed only on the script files that are loaded
(based on all the experiments), whereas for EvoMaster all source files were considered. In other
words, if on each run i a tool cover Xi lines out of Ni total reported, then the coverage percentage ci is
computed as ci = Xi/N , where N = max(Ni). Furthermore, a current limitation of EvoMaster for
JavaScript is that the recorded coverage does not include the coverage achieved at boot-time, but only
from when the search starts. This was an issue that has been fixed for the JVM, but not for JavaScript
yet. Therefore, the numbers in Table 4 are not directly comparable with the numbers in Table 2.

13

consumes = MediaType .APPLICATION_JSON_VALUE,
p r o d u c e s = MediaType .APPLICATION_JSON_VALUE

1 @PostMapping ( v a l u e = TAN_ROUTE,
2
3
4 )
5 public D e f e r r e d R e s u l t <R es ponse Entity <Tan>> generateTan (
6
7
8
9
10
11
12
13
14

}
StopWatch stopWatch = new StopWatch ( ) ;
stopWatch . s t a r t ( ) ;
Op tional <V e r i f i c a t i o n A p p S e s s i o n > a c t u a l

@Valid @RequestBody R e g i s t r a t i o n T o k e n r e g i s t r a t i o n T o k e n ,
@RequestHeader ( v a l u e = " cwa−f a k e " ,
( ( f a k e != null ) && ( f a k e . e q u a l s ( " 1 " ) ) ) {

return f a k e R e q u e s t S e r v i c e . generateTan ( r e g i s t r a t i o n T o k e n ) ;

i f

r e q u i r e d = f a l s e ) S t r i n g f a k e ) {

= a p p S e s s i o n S e r v i c e . getAppSessionByToken ( r e g i s t r a t i o n T o k e n . g e t R e g i s t r a t i o n T o k e n ( )

) ;
i f

15

( a c t u a l . i s P r e s e n t ( ) ) {

Figure 2: A function snippet from the ExternalTanController class in the API cwa-verification.

5.2.1 catwatch

With an average line coverage of 50%, this can be considered a non-trivial API. The main issue is that
this SUT makes a call to an external service (more specifically, to the GitHub APIs to fetch project
info). But such call seems to get stuck for a long time (and possibly timeout), which leads to none of
the code parsing the responses (and do different kind of analyses) being executed. Calling external
services is a problem for fuzzers. External services can go down or change at any time. They can
return different data at each call, making assertions in the generated tests become flaky. Although
testing with the actual external services is useful and should be done, the generated tests would likely
not be suitable for regression testing. This is a known problem in industry, and there are different
solutions to address it, like mocking the external services (e.g., in the JVM ecosystem, WireMock [24]
is a popular library to do that). Enhancing fuzzers to deal with mocked services (e.g., to setup the
mock data they should return) is going to be an important venue of future research.

5.2.2

cwa-verification

On this API, achieved coverage is less than 50% (i.e., 46.9%). There are two interesting cases to

discuss here, which have major impact on the achieved coverage.

for one of

First, Figure 2 shows a snippet of

(i.e.,
code
ExternalTanController, but the same issue happens as well in ExternalTestStateController
and InternalTanController). Here, the condition actual.isPresent() is never satisfied, which
leads to miss large part of the code. A token is given as input as part of the body payload, and then a
record matching such token in the database is searched for. However, none is found, and EvoMaster
is unable to create it directly. In theory, such case should be trivial to handle with SQL support [34],
but it was not the case. The reason is the peculiar properties of such token. Such token has the given
constraint in the OpenAPI schema:

endpoint handlers

the

1 " r e g i s t r a t i o n T o k e n " : {
2 " p a t t e r n " :
3 " type " :
4 }

" s t r i n g "

" ^ [ a−f 0 −9]{8} −[ a−f 0 −9]{4} −4[ a−f 0 −9]{3} −[89aAbB ] [ a−f 0 −9]{3} −[ a−f 0 −9]{12} $ " ,

This

results

in the token having a length of 36 characters.

EvoMaster has no
problem in sampling strings
example:
for
that match such a regular
bfe8b80b-dca1-458d-B312-96ecae446be0. However, such constraint is not present in the SQL
database, where the column for these tokens is simply declared with:

expression,

like

1 − column :
name :
2
type : v a r c h a r ( 6 4 )
3

r e g i s t r a t i o n _ t o k e n _ h a s h

14

6

7
8
9
10
11
12
13
14
15

p r o d u c e s = MediaType .APPLICATION_JSON_VALUE

1 @PostMapping ( v a l u e = TELE_TAN_ROUTE,
2
3 )
4 public ResponseE nti ty <TeleTan> c r e a t e T e l e T a n (
5

@RequestHeader ( J w t S e r v i c e .HEADER_NAME_AUTHORIZATION) @Valid A u t h o r i z a t i o n T o k e n
a u t h o r i z a t i o n ,
@RequestHeader ( v a l u e = TELE_TAN_TYPE_HEADER,
teleTanType ) {

r e q u i r e d = f a l s e ) @Valid TeleTanType

L i s t <A u t h o r i z a t i o n R o l e > r e q u i r e d R o l e s = new A r r a y L i s t <>() ;

i f

( teleTanType == null ) {

teleTanType = TeleTanType . TEST ;
r e q u i r e d R o l e s . add ( A u t h o r i z a t i o n R o l e .AUTH_C19_HOTLINE) ;

} e l s e i f

( teleTanType == TeleTanType .EVENT) {

r e q u i r e d R o l e s . add ( A u t h o r i z a t i o n R o l e .AUTH_C19_HOTLINE_EVENT) ;

}

Figure 3: A function snippet from the InternalTanController class in the API cwa-verification.

When EvoMaster generates data directly into the database, they would be random strings that
do not satisfy such regular expression. Again, this should not a problem thanks to taint analysis [35].
The reason it is not working is due to the length of the string, which is 36 characters. By default,
EvoMaster does not generate random strings with more than 16 characters. Even if with taint
analysis we could inject the right string into the database, currently this does not happen due to 36
being greater than 16.

Note that having a constraint on the length of strings is essential. Sampling unbound random
strings with billions of characters would have too many negative side effects. The choice of the value
16 is arbitrary: could had been higher, or lower. Still, whatever limit is chosen, an SUT could need
strings longer than such limit, as it happens in this case for cwa-verification.

A solution to address this issue is to distinguish between 2 different max-length limits: an hard
one that should never be violated (e.g., a constraint in the OpenAPI or SQL schemas), and a soft one
(e.g., 16). The soft limit would be used when sampling random strings, but could be violated in some
specific circumstances (like for example in taint analysis).

The second interesting case to discuss is present in the endpoint handler InternalTanController,
shown in Figure 3. Here, an HTTP header with value TELE_TAN_TYPE_HEADER="X-CWA-TELETAN-TYPE"
can be provided as input. However, such info is missing from the OpenAPI schema. Therefore, the
object teleTanType is always null. This is an example of underspecified schema.

5.2.3 cyclotron

On this API, there is not much difference between white-box and grey-box testing. There are at least
3 major issues that are worth to discuss.

First, a non-negligible amount of code is executed only if some configuration settings are on. But
those are off by default (like for example config.analytics.enable and config.enableAuth in
config.js). All the code related to these functionality cannot be tested via the RESTful endpoints.
Second, like for several other APIs in this study, the schema here is not fully correct/complete.
For example, Figure 4 shows a snippet for the handler of the endpoint /data/{key}/upsert, where
the two fields data and keys are read from the input object in the HTTP body payload of the request.
However, the schema has no formal definition about those fields, as they are just mentioned as a
comment (see Line 17 in Figure 5).

Third, the API does several accesses to a MongoDB database. Several endpoints start by re-
trieving data from the database, like for example with commands like Dashboards.findOne({ name:
dashboardName }). If the data is missing, no following code is then executed. But it is hard to get
the right id (e.g., dashboardName in this example) by chance. Although EvoMaster has support for
SQL databases (i.e., to analyze all executed queries at runtime), it does not for MongoDB, and not for

15

}

i f

r e s ) {

var u p s e r t D a t a = r e q . body . data ;
var k e y s = r e q . body . k e y s ;

1 e x p o r t s . u p s e r t D a t a = f u n c t i o n ( req ,
( r e q . body == null ) {
2
return r e s . s t a t u s ( 4 0 0 ) . send ( ’ M i s s i n g data . ’ ) ;
3
4
5
6
7
8
9
10
11
12
13
14

( u p s e r t D a t a == null ) {
return r e s . s t a t u s ( 4 0 0 ) . send ( ’ M i s s i n g data . ’ ) ;

( k e y s == null ) {
return r e s . s t a t u s ( 4 0 0 ) . send ( ’ M i s s i n g k e y s . ’ ) ;

i f

i f

}

}

Figure 4: A function snippet from the api.data.js file in the API cyclotron, for the endpoint
/data/{key}/upsert.

5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

" p o s t " : {

1 " / data /{ key }/ u p s e r t " : {
2
3
4

" U p s e r t s an o b j e c t

" summary " :
" d e s c r i p t i o n " :
Bucket . The r e v p r o p e r t y w i l l be i n c r e m e n t e d . " ,
" t a g s " :

" U p s e r t s

( i n s e r t s o r u p d a t e s ) an o b j e c t

i n t h e Data Bucket Data " ,

[
" Data "

i n t h e data f o r a g i v e n Data

] ,
" p a r a m e t e r s " :

[ {

" key " ,

" path " ,

" name " :
" i n " :
" d e s c r i p t i o n " :
" r e q u i r e d " : true ,
" s t r i n g "
" typ e " :

" The Data Bucket key . " ,

} , {

" body " ,

" data " ,

" name " :
" i n " :
" d e s c r i p t i o n " :
" r e q u i r e d " : true

"An o b j e c t c o n t a i n i n g ’ k e y s ’ and ’ data ’ . " ,

} ] ,

Figure 5: Snippet of the OpenAPI schema for the API cyclotron.

NodeJS (i.e., SQL support is currently only implemented for the JVM).

5.2.4 disease-sh-api

On this API, there is not much difference between EvoMaster (both black-box and white-box) and
Schemathesis. After an analysis of the generated tests, practically all achievable coverage is obtained,
as the remaining code is not reachable through the REST endpoints in the API.

First, the schema refers only to version v3 of the API, but in the source code there is still all the
implementation of the endpoints for version v2. Those endpoints are not executable based on the info
in the schema. Second, large part of the code deals with the fetching of disease records from online
databases, and store them in a local Redis instance. But such code is executed only via scripts, and not
from the RESTful endpoints. Note that, when the experiments are run, the database was populated
with a selection of valid data, as there is no REST endpoint to add it.

5.2.5

features-service

With an average line coverage of 81.8%, features-service can be considered one of the easier APIs.
Quite a bit of code cannot be reached with any generated system test, like for example getters/setters

16

r e s u l t ,

f e a t u r e C o n s t r a i n t
:

r e s u l t = f e a t u r e C o n s t r a i n t . e v a l u a t e C o n f i g u r a t i o n (

E v a l u a t i o n R e s u l t
P r o d u c t C o n f i g u r a t i o n c o n f i g u r a t i o n ,
Set<F e a t u r e C o n s t r a i n t > f e a t u r e C o n s t r a i n t s ) {

1 private E v a l u a t i o n R e s u l t a d d E v a l u a t i o n s T o R e s u l t (
2
3
4
5 f o r ( F e a t u r e C o n s t r a i n t
6
7
8
9 }
10
11 i f ( r e s u l t . i s V a l i d && ! r e s u l t . d e r i v e d F e a t u r e s . isEmpty ( ) ) {
12
13
14
15

c o n f i g u r a t i o n . a c t i v e ( d e r i v e d F e a t u r e ) ;
r e s u l t . d e r i v e d F e a t u r e s . remove ( d e r i v e d F e a t u r e ) ;
r e s u l t = t h i s . a d d E v a l u a t i o n s T o R e s u l t (

f o r ( S t r i n g d e r i v e d F e a t u r e :

r e s u l t . d e r i v e d F e a t u r e s ) {

r e s u l t , c o n f i g u r a t i o n ) ;

f e a t u r e C o n s t r a i n t s ) {

Figure 6: A function snippet from the ConfigurationEvaluator class in the API features-service.

that are never called, or some functions that are only used by the manually written unit tests. There
are still some branches that are not covered, related to properties of data in the database. In other
words, when a GET is fetching some data, some properties are checked one at a time, but, to be able to
pass all these checks, a previous POST should had created such data. Figure 6 shows one such case,
where the code inside if statement does not get executed. Tracing how database data is impacting the
control flow execution, and which operations should be first called to create such data, is a challenge
that needs to be addressed.

5.2.6

gestaohospital-rest

On this API, there is no much difference between white-box and grey-box EvoMaster (i.e., average
coverage 39.5% vs. 39.4%, Table 4). Furthermore, RestTestGen and Schemathesis give better
results than EvoMaster (Table 2).

One specific property of this API compared to most of the other SUTs in our empirical study is
that such API uses a MongoDB database. In contrast to SQL databases, white-box EvoMaster
has no support for NoSQL databases (such as MongoDB). As this API does many interactions with
the database (e.g., there is a lot of calls like service.findByHospitalId(hospital_id)), not much
coverage is achieved if such accessed data is not present in the database, and the evolutionary search
has no gradient to create it. In this regard, it seems that RestTestGen and Schemathesis do a
better job at creating resources with POST commands and use such created data for their following
GET requests. Still, although quite good, the coverage is at most 62.3% (measured with JaCoCo for
Schemathesis), which means there still some challenges to overcome.

We could not analyze in details the output of these tools in an IDE for the Java program
gestaohospital-rest (e.g., using a debugger), as RestTestGen generated only JSON files with the
descriptions of the tests (and no JUnit file), and the outputs of Schemathesis are in YAML for the
VCR-Cassette format (which is mainly for Ruby and Python, where the port for Java seems has not
been under maintenance for several years).

5.2.7

ind0

This is a closed-source API provided by one of our industrial partner. Therefore, we cannot provide any
code example for it, but still we can discuss it at a high level. This API is a service in a microservice
architecture. Although in terms of size it is not particularly big (recall Table 1), it is the most
challenging API in our case study. No black-box fuzzer achieves more than 8.2% line coverage, and
white-box testing goes only up to 18.8% (on average).

Out of 20 endpoints, all but one endpoint require string inputs satisfying a complex regular
expression, but such info is not present in the OpenAPI schema. So, a random string is extremely

17

unlikely to satisfy such constraint. Furthermore, the constraint is expressed in a @ annotation, which
is handled by the Spring framework before any of the HTTP handlers are executed.

Thanks to taint-analysis, EvoMaster can handle these cases [35], even when constraints are
evaluated in third-party libraries (and not just in the business logic of the SUT). Still, there is quite
a bit of variability in the results, i.e., given the average coverage 18.8%, the standard variation is
7.0, with coverage going from a minimum of 11.9% to a maximum of 32.6% (out of the 10 repeated
experiment runs). EvoMaster manages to solve these constraints, but not in all runs, as such strings
not only get matched with a regular expression, but then are farther checked with other constraints.
However, as the regular expression is exactly the same for all these 19 endpoints, applied on a URL
path variable with the same name, it should be possible to design strategies to re-use such information
to speed up the search, instead of resolving the same constraints each time for each endpoint. Even
with the run with highest coverage 32.6%, many endpoints are not covered due to this issue.

5.2.8

js-rest-ncs

This is an artificial API, using numerical-computation functions (e.g., Triangle Classification [26])
behind REST endpoints. As this is a re-implementation in JavaScript of rest-ncs, we will defer to
Section 5.2.14 for the analyses.

One thing to notice though is that the coverage values are higher for a very simple reason: the
fetching of the OpenAPI schema is part of business logic of the API, and code coverage is computed
for it (as the schema is inside a JavaScript file as a string inside an HTTP endpoint declaration). On
the other hand, for Java, the schema is handled as a JSON resource.

5.2.9

js-rest-scs

Similarly to the case of js-rest-ncs, js-rest-scs is a JavaScript re-implementation of rest-scs, which
involved string-based computation functions. We therefore will defer to Section 5.2.16 for the analyses.
As for js-rest-ncs, the fetching of the schema impacts the computed code coverage. However,
for white-box EvoMaster, coverage is worse for js-rest-scs than for rest-scs. The reason is rather
simple: the support for white-box testing of JavaScript code in EvoMaster is a much more recent
addition [71], and not all features that are implemented for the JVM (e.g., different forms of taint
analysis [36]) are currently supported.

5.2.10 languagetool

On this API, coverage is up to 39.5%. This is the largest and most complex API in our study (recall
Table 1). This is a CPU bound (e.g., no database) application, doing complex text analyses. This
API has only two endpoints: /v2/languages which is a simple GET with no parameters (and so it is
trivially covered by just calling it once), and /v2/check which is a POST with 11 input parameters.
However, looking at the source code of ApiV2.handleRequest it seems there are some more endpoints,
although those are not specified in the OpenAPI/Swagger schema. Without an extensive analysis of
this SUT, it is unclear how much of its code would not be coverable due to this issue.

Following the execution from the entry point /v2/check, there are few missed branches that
In Figure 7, the last statement crashes due to mapper.readTree throw-
are worthy to discuss.
ing an exception (and so all statements after it cannot be executed). Here, when a form in a
application/x-www-form-urlencoded payload is received, one out of the 11 parameters is treated
as JSON data (i.e., the input called data). Such info is written in the schema as a comment, although
the type is registered as a simple string. It is unlikely that a random string would represent a valid
JSON object.

Another issue is when entering in the class TextChecker. Several if statements refer to input param-
eters that are undefined in the schema, such as textSessionId, noopLanguages, preferredLanguages,
enableTempOffRules, allowIncompleteResults, enableHiddenRules, ruleValues, sourceText,
sourceLanguage and multilingual. Dealing with underdefined schemas is a major issue with fuzzers,
especially black-box ones. Technically, these can be considered as ‘‘faults’’ in the schemas, which
should be fixed by the users. However, schemas can be considered as documentation, and issues in

18

HttpExchange httpExchange ,
Map<S t r i n g , S t r i n g > parameters ,
E r r o r R e q u e s t L i m i t e r e r r o r R e q u e s t L i m i t e r ,
S t r i n g remoteAddress ) throws E x c e p t i o n {

1 private void handleCheckRequest (
2
3
4
5
6 AnnotatedText aText ;
7 i f ( p a r a m e t e r s . c o n t a i n s K e y ( " t e x t " ) &&
8
9
10
11 } e l s e i f
12
13
14 } e l s e i f
15
16
17

aText = new A n n o t a t e d T e x t B u i l d e r ( )

throw new I l l e g a l A r g u m e n t E x c e p t i o n (

" S e t o n l y ’ t e x t ’ o r

. addText ( p a r a m e t e r s . g e t ( " t e x t " ) ) . b u i l d ( ) ;
( p a r a m e t e r s . c o n t a i n s K e y ( " data " ) ) {
ObjectMapper mapper = new ObjectMapper ( ) ;
JsonNode data = mapper . r e a d T r e e (

( p a r a m e t e r s . c o n t a i n s K e y ( " t e x t " ) ) {

p a r a m e t e r s . c o n t a i n s K e y ( " data " ) ) {

p a r a m e t e r s . g e t ( " data " ) ) ;

’ data ’ parameter , not both " ) ;

Figure 7: A function snippet from the ApiV2 class in the API languagetool.

the documentations might receive less priority compared to faults in the API implementation. Still,
as fuzzers heavily rely on such schemas, a more widespread use of fuzzers might lead in changes in
industrial practice by giving compelling reasons to timely update and fix those schemas.

As there are tens of thousands of line that were not covered in this SUT, there are many other
missed branches that would be interesting to discuss in details. But it is unclear how related or
independent they are from the aforementioned issues. Once the issue of dealing of underdefined
schemas is solved, it will be important to re-run these experiments to identify which major issues are
still left.

5.2.11 ocvn-rest

On this API, with black-box testing line coverage was up to 35.3%, which is significantly higher
than the 24.8% achieved by white-box testing. It is the second largest API in our study, following
languagetool, based on number of lines of code. But, in terms of endpoints, it is the largest with
192 of them. It would not be possible, given the space, to discuss each single of these 192 endpoints.
To analyze the achieved coverage, we hence used the test suites generated with both white-box and
black-box testing. These have hundreds of test cases. Even calling each endpoint just once would
result in at least 192 HTTP calls in the generated tests. Considering the complexity of this SUT,
maybe using a larger search budget over 1 hour could be recommended.

of

the

35%

than

More

codebase

(packages

org.devgateway.ocvn

and
org.devgateway.ocds.persistence) deal with connections to databases such as SQL and
MongoDB, but such code does not seem to be executed. This might happen if the endpoint executions
fail before writing/reading from the databases (e.g., due to input validation). The definitions of HTTP
handlers in the package org.devgateway.ocds.web.rest.controller takes more than 30% of the
codebase. But only half of it (i.e., around 15%) gets covered by the tests. Few endpoints (e.g., in the
class UserDashboardRestController) were not covered because they require admin authentication,
which was not setup for these experiments (recall the discussion about authentication in Section 4.2).
The class CorruptionRiskDashboardIndicatorsStatsController defines several HTTP endpoints,
but none of them appears in the OpenAPI/Swagger schema. So, they cannot be called by the fuzzers.
Around 5% of the codebase seems to deal with the generation of Excel charts, but that code is not
covered. Out of the 14 HTTP endpoints dealing with this functionality, they all fail on the very first
line (and so a substantial part of the codebase is not executed, possibly much higher than 5%), which
is a variation of the following statement with different string inputs as second parameter:

1 f i n a l S t r i n g c h a r t T i t l e = t r a n s l a t i o n S e r v i c e . g e t V a l u e (
f i l t e r . getLanguage ( ) ,
2
" c h a r t s : c a n c e l l e d F u n d i n g : t i t l e " ) ;
3

19

1 @ApiOperation ( v a l u e = " Count t h e t e n d e r s and group t h e r e s u l t s by y e a r . The y e a r

i s

c a l c u l a t e d from "
+ " t e n d e r . t e n d e r P e r i o d . s t a r t D a t e . " )

2
3 @RequestMapping ( v a l u e = " / a p i / countTendersByYear " , method = { RequestMethod . POST,

RequestMethod .GET } ,
p r o d u c e s = " a p p l i c a t i o n / j s o n " )

4
5 public L i s t <DBObject> countTendersByYear ( @ModelAttribute @Valid f i n a l

Y e a r F i l t e r P a g i n g R e q u e s t

f i l t e r ) {

Figure 8: A function snippet from the CountPlansTendersAwardsController class in the API
ocvn-rest.

1 @EachRange ( min = MIN_REQ_YEAR, max = MAX_REQ_YEAR)
2 protected TreeSet<I n t e g e r > y e a r ;
3
4 @EachRange ( min = MIN_MONTH, max = MAX_MONTH)
5 protected TreeSet<I n t e g e r > month ;
6
7 @EachPattern ( r e g e x p = " ^ [ a−zA−Z0 −9]∗ $ " )
8 private TreeSet<S t r i n g > bidTypeId ;
9
10 @EachPattern ( r e g e x p = " ^ [ a−zA−Z0 −9]∗ $ " )
11 private TreeSet<S t r i n g > notBidTypeId ;
12
13 @EachPattern ( r e g e x p = " ^ [ a−zA−Z0 −9]∗ $ " )
14 private TreeSet<S t r i n g > p r o c u r i n g E n t i t y I d ;
15
16 @EachPattern ( r e g e x p = " ^ [ a−zA−Z0 −9]∗ $ " )
17 private TreeSet<S t r i n g > n o t P r o c u r i n g E n t i t y I d ;
18
19 @EachPattern ( r e g e x p = " ^ [ a−zA−Z0 −9]∗ $ " )
20 private TreeSet<S t r i n g > contrMethod ;
21
22 @Min ( 0 )
23 protected I n t e g e r pageNumber ;
24
25 @Range ( min = 1 , max = MAX_PAGE_SIZE)
26 protected I n t e g e r p a g e S i z e ;

Figure 9: All fields with declared constraints (9 in total) in the class YearFilterPagingRequest in
the API ocvn-rest.

Here, the language is given as input in the HTTP requests, which then gets matched with a regular
expression. This makes the translationService.getValue call crash when the regular expression is
not satisfied. White-box testing can analyze these cases, but it can be hard for black-box techniques if
the info on such regular expressions is not available in the OpenAPI/Swagger schemas.

Another large part of the code that is not covered (around another 5%) is in the package
org.devgateway.ocds.web.spring. However, most of this code seems only be called by the frontend
of the OCVN application (and the jar file for ocvn-rest does not include such module). From the point
of view of the HTTP endpoints, this can be considered dead-code, because it cannot be reached from
those endpoints.

It might be surprising that white-box testing gives significantly worse results than grey-box testing,
by a large margin (i.e., 10.5%). In a combinatorial optimization problem this can happen when the
fitness function provides little to no gradient to the search, generating the so called fitness plateaus.
Mutation operators that only do small changes to the chromosome of an individual (i.e., an evolved
test case in this context) would have lower chances to escape from such local optima. EvoMaster
has some advance mechanism to address these cases, like for example adaptive hypermutation [69].
However, it was not enough to handle ocvn-rest. The problem here is that most endpoints take a

20

s l u g ) {

1 u n F a v o r i t e ( id ,
2
3
4
5

return __awaiter ( this , void 0 , void 0 ,

f u n c t i o n ∗ ( ) {

l e t a r t i c l e = y i e l d t h i s . a r t i c l e R e p o s i t o r y . findOne ( { s l u g } ) ;
const u s e r = y i e l d t h i s . u s e r R e p o s i t o r y . findOne ( i d ) ;
const d e l e t e I n d e x = u s e r . f a v o r i t e s . f i n d I n d e x ( _ a r t i c l e => _ a r t i c l e . i d === a r t i c l e .

i d ) ;
i f

6

( d e l e t e I n d e x >= 0 ) {

Figure 10: A function snippet from the article.service.js file in the API realworld-app.

JSON object as input, with several constraints that must be satisfied. But those constraints are not
evaluated in the business logic of the SUT, but rather in a third-party library. These constraints
are not specified in the OpenAPI schema. Figure 8 shows an example, in which the JSON input is
unmarshalled into an YearFilterPagingRequest instance. Because this input is marked with the
annotation @Valid, its validity is evaluated before the method countTendersByYear() is called. If it
is not valid, then the Spring framework returns an HTTP response with status 400 (i.e., user error),
without calling the method countTendersByYear().

As shown in Figure 9, the class YearFilterPagingRequest has 9 fields on which javax.validation
constraints are declared. Only need 1 (out of 9) violated constraint to invalidate the whole object.
Because the constraints are evaluated in a third-party library, the fitness function has no gradient
to guide the search to solve all of them. Looking at the results, it seems like that generating valid
instances at random is feasible, albeit with low probability. On the other hand, small mutations on an
existing invalid instance have little to no chance to make the object valid.

To address these issues, there could be at least three strategies:

• (White-Box) Compute the fitness function to optimize the coverage (using different search
heuristics) on all the code, including third-party libraries, and not just the business logic of the
SUT. In this way, there would be gradient for generating tests in which the code that evaluates
@Valid constraints returns true. Although in theory possible, this does not sound like a very
promising approach (unless only the needed code in the third-party libraries is instrumented).
In most cases, the code of the business logic of the SUT is just tiny compared to the source
code of all used third-party libraries, which includes application frameworks such as Spring,
HTTP servers such as Tomcat, and ORM libraries such as Hibernate. Instrumenting everything
to compute advance heuristics on all third-party libraries would likely have huge performance
overhead. This is particularly the case for all code that is executed before the business logic, e.g.,
the unmarshalling of incoming HTTP requests.

• (White-Box) Natively support @Valid in the fitness function, by creating a branch distance for
each field, with the objective of minimizing the distance for all fields. Different branch distances
can be computed for the different constraints which are part of the standard javax.validation
(e.g., javax.validation.constraints.Min). A new testing target (to optimize for) could be
created for each @Valid annotation in the business logic of the SUT. However, one major
challenge here is that javax.validation allows to create custom constraint annotations. This
is for example the case for @EachPattern and @EachRange used in Figure 9.

• (Black-Box) Even if the info on the field constraints are missing in the OpenAPI schema, it can
be detected in the schema that the same object type is used as input in more than one endpoint.
If that is the case, and if for an endpoint it is difficult to create a valid instance (e.g., all evaluated
HTTP calls so far return a response with HTTP status 400), then a possible strategy could be to
re-use as input an instance created for another endpoint for which a 2xx status code was returned
(if any has been created so far during the search).

5.2.12 realworld-app

21

method = RequestMethod .GET)

J s o n O b j e c t
Consumer consumer = consumers . findByUsername (

1 @Secured ( { "ROLE_USER" } )
2 @RequestMapping ( v a l u e = " / consumer / r e q u e s t s " ,
3
4 public S t r i n g g e t R e q u e s t s ( P r i n c i p a l p r i n c i p a l ) {
5
6
7
8
9
10
11

r e s p o n s e . addProperty ( " s u c c e s s " ,
return GSON. t o J s o n ( r e s p o n s e ) ;

r e s p o n s e = new J s o n O b j e c t ( ) ;

i f ( consumer == null ) {

f a l s e ) ;

}

p r i n c i p a l . getName ( ) ) ;

Figure 11: Snippet of function handler for the endpoint /consumer/request in proxyprint.

On this API, there is a small difference between EvoMaster black-box and Schemathesis (c8
instrumentation: 69.4% vs. 69.7%), and between grey-box and white-box EvoMaster (EvoMaster
instrumentation without bootstrap coverage: 24.6% vs.24.4%).

Most of the code is covered. What is left are either unfeasible branches, or branches related to
fetching data from the database (MySQL in this case) with some given properties. Figure 10 shows
one such example, where 3 queries into the database are executed, based on the 2 inputs id and slug.
The condition of if statement at the end of that code snippet is never satisfied, as it depends on these
3 SQL queries. Recall that, although EvoMaster has support for SQL query analyses (which could
help in this case), it is currently only for the JVM, and not NodeJS.

5.2.13 proxyprint

With a line coverage of up to 53.3%, this is an API in which reasonable results are achieved, but more
could be done. Several functions in this API are never called, and so they are impossible to cover, like
for example the methods sendEmailFinishedPrintRequest and sendEmailCancelledPrintRequest
in the class MailBox, and the private methods singleFileHandle and calcBudgetsForPrintShops
in the class ConsumerController. So, 100% coverage is not possible on this API.

This API provides a selection of various branches that are not covered. However, it is not
straightforward to find out the interesting challenges here. One problem is that the number of HTTP
calls on this API is low, and so not much search is actually carried out compared to the other SUTs
when using a 1 hour budget. For example, the generated statistic files of EvoMaster report an
average of 37 393 HTTP calls per experiment on proxyprint, whereas for example for rest-news it is
372 733, i.e., nearly 10 times as more. So, many of these missed branches could be covered if running
EvoMaster for longer. Still, some of these branches seem quite unlikely to be covered even with
larger search budgets. Let us discuss a few of them. Some are related to authentication. For example,
in Figure 11, the code inside the if statement is never executed. To reach that statement, an HTTP
call with valid authentication information needs to be provided. Such authentication info is validated
with the database when the Principal object is instantiated by the framework (Spring Security in
this case). Authentication information needs to be available when the SUT starts (unless the API has
some way to register new users directly on-the-fly from the API itself). Therefore, to deal with security
(especially when involving hashed passwords), some initialization script is required, e.g., to register a
set of users with valid username/password information. In the case of proxyprint, also some other data
in other tables is created as well, e.g., in the Consumer table. To be able to cover such branch, either
a fuzzer should find a way to create new valid users, or modify the existing data in the database (e.g.,
EvoMaster can add new data, but not modify the existing one [34]).

Figure 12 shows an example in which the line defining the variable quantity throws an exception,
due to Double.valueOf being called on a null input. Here, an HTTP object request is passed as
input to the constructor of IPNMessage, which is part of PayPal SDK library. Inside such library,
request.getParameterMap() is called to extract all the parameters of the HTTP request, which are
used to populate the map object returned by ipnlistener.getIpnMap(). However, as such parameters
are read dynamically at runtime, the OpenAPI/Swagger schema has no knowledge of them (as for this

22

IOException {

1 @RequestMapping ( v a l u e=" p ayp al / i p n / consumer /{ consumerID } " ,
method=RequestMethod .POST)
2
3 protected void consumerLoadUpConfirmation (
@PathVariable ( v a l u e = " consumerID " ) long c i d ,
4
H t t p S e r v l e t R e q u e s t
r e q u e s t ,
5
H t t p S e r v l e t R e s p o n s e r e s p o n s e
6
) throws S e r v l e t E x c e p t i o n ,
7
8 Map<S t r i n g , S t r i n g > c o n f i g u r a t i o n M a p =
9
10
11
12
13
14
15
16 Map<S t r i n g , S t r i n g > map = i p n l i s t e n e r . getIpnMap ( ) ;
17
18
19 Double q u a n t i t y = Double . v a l u e O f (map . g e t ( " mc_gross " ) ) ;

boolean i s I p n V e r i f i e d = i p n l i s t e n e r . v a l i d a t e ( ) ;
S t r i n g t r a n s a c t i o n T y p e = i p n l i s t e n e r

IPNMessage i p n l i s t e n e r = new IPNMessage (

S t r i n g payerEmail = map . g e t ( " payer_email " ) ;

r e q u e s t ,
c o n f i g u r a t i o n M a p ) ;

C o n f i g u r a t i o n . g e t C o n f i g ( ) ;

. g e t T r a n s a c t i o n T y p e ( ) ;

Figure 12: Snippet of function handler for the endpoint /paypal/ipn/consumer/{consumerID} in
proxyprint.

@PathVariable ( v a l u e = " i d " ) long id ,
@RequestBody RingTableItem r t i ) {

PrintShop pshop = p r i n t s h o p s . findOne ( i d ) ;
J s o n O b j e c t

1 @Secured ( "ROLE_MANAGER" )
2 @RequestMapping ( v a l u e=" / p r i n t s h o p s /{ i d }/ p r i c e t a b l e / r i n g s " , method=RequestMethod .PUT)
3 public S t r i n g e d i t R i n g s I t e m (
4
5
6
7
8
9
10
11
12
13
14

r t i . getRingType ( ) ) ,
r t i . g e t I n f L i m ( ) ,
r t i . getSupLim ( ) ) ;

BindingItem newBi = new BindingItem (

r e s p o n s e = new J s o n O b j e c t ( ) ;

Item . RingType . v a l u e O f (

i f ( pshop != null ) {

Figure 13: Snippet of function handler for the endpoint /printshops/id/pricetable/rings in
proxyprint.

SUT the schema is created automatically with a library when the API starts). Therefore, there is no
info to use an HTTP parameter called mc_gross of type double. As such parameter is used directly
without being transformed, testability transformations with taint analysis might be able to address
this problem [35], but such techniques would need to be extended to support getParameterMap() and
Map.get.

Figure 13 shows another interesting example, where the value of a string field in a JSON payload
is used directly to instantiate an enum value, i.e., the statement Item.RingType.valueOf(rti.get-
RingType()). This fails, as a random string is extremely unlikely to represent a valid value from a
restricted set. This might be handled by providing such info in the OpenAPI/Swagger schema (which
supports defining enumerations on string fields), or also by handling valueOf() in enumeration by
extending the techniques in [35] to support it.

The handler calcBudgetForPrintRequest for the endpoint /consumer/budget is never called
(and so all the business logic related to this endpoint remains uncovered by the tests). Such endpoint
requires as input a payload with type multipart/form-data, but the schema wrongly specifies the
application/json type. A fuzzer might be able to handle this case automatically, but it could be
considered as a major issue in the schema, that should be fixed by the users like a fault in the SUT.
In other words, there is a big difference between not providing all information (e.g., missing enum

23

s u b j e c t ( S t r i n g d i r e c t o r y , S t r i n g f i l e ) {

f i l e p a r t s = null ;

1 public s t a t i c S t r i n g
2
3
4
5
6
7
8

i n t r e s u l t = 0 ;
S t r i n g [ ]
i n t l a s t p a r t = 0 ;
S t r i n g s u f f i x = null ;
f i l e p a r t s = f i l e . s p l i t ( " . " ) ;
l a s t p a r t = f i l e p a r t s . l e n g t h − 1 ;
i f

( l a s t p a r t > 0 ) {

Figure 14: Snippet of subject function from the FileSuffix class in the API rest-scs.

value constraints like in the case of /printshops/{id}/pricetable/rings) and providing wrong
information (i.e., wrong payload type as in the case of /consumer/budget).

5.2.14 rest-ncs

On this SUT, white-box testing with EvoMaster achieves an average of 93%. An analysis of the
non-covered lines shows that those are not possible to reach, i.e., dead-code. An example is checking
twice if a variable is lower than 0, and return an error if so. In such case, it is impossible to make the
second check true. Therefore, as the maximum achievable coverage is obtained in each single run, this
can be considered as a solved problem.

Regarding black-box testing, all but 2 fuzzers achieve more than 50% line coverage, with Schemath-
esis achieving 94.1% coverage (computed with JaCoCo), followed by RestCT achieving 85.5% average
coverage. On this API, black-box EvoMaster gives worse results, i.e., 64.4%.

This is a rather interesting phenomenon, which is strongly dependent on some design choices these
fuzzers make. For example, some branches in this API depend on whether 2 or more integer inputs
are equal (e.g., in the case of Triangle Classification). Given two 32-bit integers X and Y , there is
only a 1/232 chance that X = Y . On average, it will need to sample more than 2 billion values before
obtaining X = Y . With such constraints, it will be very difficult for a black-box fuzzer to cover this
kind of branches if integer inputs are sampled at random. However, a fuzzer does not need to sample
from the whole spectrum of all possible integers. For example, it could sample from a restricted range,
e.g., [−100, 100]. On the one hand, the shorter the range, the highest chances to sample X = Y . On
the other hand, a short range could make impossible to cover branches that require values outside
such range (e.g., 12345). This is a tradeoff that needs to be made.

5.2.15 rest-news

On this API, it was possible to achieve up to 66.5% line coverage. As for the other SUTs, most of
the missing coverage is due to dead-code, which is impossible to execute. However, there are some
branches that should be possible to cover, but they were not. These seem all related to the same
issue, which is the update of existing data in the database. Databases allow to define constraints on
their data (e.g., a number should be within a certain range), using SQL commands such as CHECK.
White-box EvoMaster can generated test data directly into SQL databases [34], taking into account
all these constraints (as all this info is available in the schema of the database). If any constraint is not
satisfied, then the INSERT SQL commands will fail. The problem here is that the SUT might define
further constraints on such data. In JVM projects, this is commonly done with javax.validation
annotations when using ORM libraries such as Hibernate [9]. But EvoMaster does not seem to
handle this, and generates data that is valid for the database (as all the CHECK operations pass and
the INSERTs do not fail), but not for these further constraints. So, in an update endpoint, invalid
(from the point of view of javax.validation constraints) data can be read from the database, which
then the SUT fails to write it back (as these javax.validation constraints are check on each write
operation) when the update endpoint has done its computations.

24

( ! c o u n t r i e s . isEmpty ( ) ) {
return p a r s e d C o u n t r i e s ( c o u n t r i e s ,

1 L i s t <Country> c o u n t r i e s = C o u n t r y S e r v i c e . g e t I n s t a n c e ( ) . getByCodeList ( c o d e s ) ;
2 i f
3
4 }
5 return g e t R e s p o n s e ( Response . S t a t u s .NOT_FOUND) ;

f i e l d s ) ;

Figure 15: Code snippet from the CountryRestV2 class in the API restcountries.

1 @POST @Timed @UnitOfWork
2 @Consumes ( MediaType . APPLICATION_JSON)
3 public M e d i a F i l e c r e a t e (
4
5
6
7
8
9
10

doAuth ( a u t h R e s u l t ,
try {
URI u r i = new URI( m e d i a F i l e . g e t U r i ( ) ) ;
( " data " . e q u a l s ( u r i . getScheme ( ) ) ) {
i f

@Auth @ApiParam ( hidden = true ) AuthResult a u t h R e s u l t ,
@Context H t t p S e r v l e t R e s p o n s e r e s p o n s e ,
M e d i a F i l e m e d i a F i l e ) {

r e s p o n s e , P e r m i s s i o n . m e d i a i t e m _ c r e a t e ) ;

Figure 16: Snippet of create function from the MediaFileResource class in the API scout-api.

5.2.16 rest-scs

With a line coverage of up to 86.2%, this is an API in which white-box testing is highly effective.
There were two types of branches that were not covered: the ones involving regex checks with
Pattern.matches, and the other involving the dot character ‘‘.’’. In this latter case, it is due to
EvoMaster not using such character when sampling random strings, which make the if statement in
Figure 14 impossible to cover. This might sound like something rather simple to fix. However, as soon
as having special characters used in random strings, extra care needs to be taken into consideration
when outputting test cases (e.g., the character ‘$’ has special meaning in Kotlin, and would need to be
escaped when used in strings, which is not the case for Java). Character escaping rules can be very
different based on the context in which they are used in a test case (e.g., inside a URL, inside a JSON
object passed as HTTP body payload or data injected into a SQL database). All these cases have to
be handled, otherwise the tools would end up generating test cases that do not compile.

5.2.17 restcountries

On this API, high coverage is achieved (i.e., 77.1%). Like the other SUTs, there is quite a bit of
dead-code due to getters/setters that are never called and catch blocks for exceptions that might not
be possible to throw via test cases. The class StripeRest presents an interesting case, as it defines
the endpoint /contribute, but its info is not in the schema (and so such endpoint is never called).

Figure 15 shows a code snippet which is repeated several times with small variations in few
endpoints. Here, this API returns a list of countries based on different filtering criteria, like for example
the country codes. However, the response with NOT_FOUND is never returned. The problem is that, even
if the HTTP requests provide invalid inputs (e.g., a country code that does not exist), the countries
list is wrongly populated with null values, and so the list is never empty. This is an interesting bug in
the API, which then result in a crash (i.e., a returned 500 status code), as parsedCountries throws
an exception. However, until this fault in the API is fixed, all these statements with NOT_FOUND are
technically dead-code that cannot be reached with the tests.

Similarly to rest-ncs, the fuzzing of this API might be considered as a solved problem, at least for

its current faulty version.

5.2.18 scout-api

On this API, white-box EvoMaster achieves slightly worse results (53.4% vs. 54.7%), although the
difference is not statistically different (based on 10 runs). Large part of its codebase (more than a third)

25

( ! r e s u l t ) {
c t x . throw ( 4 0 4 ) ;

const r e s u l t = a w a i t Core . f i n d B y I d ( c t x . params . i d ) ;
i f

1 r o u t e r . g e t ( ’ / : i d ’ , c a c h e ( 3 0 0 ) , async ( c t x ) => {
2
3
4
5
6
7
8 } ) ;

}
c t x . s t a t u s = 2 0 0 ;
c t x . body = r e s u l t ;

Figure 17: Snippet of the handle for the GET /cores/:id endpoint in the API spacex-api.

cannot be executed. For example, scout-api can start in the background a thread to fetch and analyse
some data, independently from the RESTful endpoints (i.e., the whole module data-batch-jobs).
Such thread is deactivated by default, and so nearly 30% of the whole codebase is not executed.
Another non-negligible part of the codebase (around 5%) is related to authentication using OAuth via
Google APIs (scout-api provides different ways to authenticate).

There is a large part of the codebase that could be executed, but it is not due to an if statement
at the beginning of one of the HTTP endpoints. This is shown in Figure 16. Here an incoming JSON
payload is unmarshalled into a MediaFile object, which has a string field that is treated as a URI.
Most random strings are a valid URI, as a URI can just be a name. But, it is extremely unlikely that
a random string would represent a URI with a valid scheme component, and so uri.getScheme()
returns null. However, this is a case that likely would be trivial to solve with taint analysis, e.g., by
extending the techniques in [35] to support URI objects. This does not mean though that much higher
coverage would be achieved, it depends on all the remaining code that is executed once that predicate
is satisfied.

Even without code analyses, this type of challenge could also be addressed with black-box techniques.
For example, in this API the string field representing the URI is called uri. An analysis of the names
of the fields can be used to possibly infer their expected type. For example, any field whose name
starts or ends with uri could be treated as an actual URI when data is generated. Also, some types are
quite common in RESTful APIs, like strings representing URLs and dates. Instead of sampling strings
completely at random, it could make sense to sample with some probability strings with given types
that are commonly used. This might now work in all cases, but it is something worth to investigate.

5.2.19 spacex-api

Similarly to realworld-app, in this API there is only a small difference between EvoMaster black-box
and Schemathesis (c8 instrumentation: 84.7% vs. 85.4%), and between grey-box and white-box
EvoMaster (EvoMaster instrumentation without bootstrap coverage: 41.5% vs. 41.5%, but with
ˆA12 = 0.58). There are two main issues affecting the coverage results here.

First, this API requires authentication, where each API endpoint requires specific authorization
roles to be able to be executed. For these experiments, a user with the right credentials was created
in the database, and the fuzzers were given the authorization info to send HTTP messages on behalf
of this user. The setting up of this user was done manually, in a script. However, after running the
experiments and analyzing the generated tests, we realized that some authorization roles were missing
(e.g., capsule:create). So, few endpoints were not covered due to misconfigured authentication.

The second main issue is related to the database. Figure 17 shows an example in which a record is
search by id. No test generated by EvoMaster was able to make a call that returned a 200 HTTP
status code. For white-box testing, EvoMaster fails to do this because it is not able to analyze
queries on MongoDB databases. Even with black-box testing, it could be possible to create a new
record with a POST, and use its id for a following GET. Fuzzers can exploit this kind of information to
create sequences of HTTP calls where ids are linked. But, this all depends on how the link is defined.
Figure 18 shows the implementation of the POST endpoint to create such record. EvoMaster has
no issue to fully cover the code of such endpoint, and create valid records. Also, EvoMaster uses
different strategies to link endpoints that manipulate the same resources [30, 72]. But those are based

26

try {

const c o r e = new Core ( c t x . r e q u e s t . body ) ;
a w a i t c o r e . s a v e ( ) ;
c t x . s t a t u s = 2 0 1 ;

1 r o u t e r . p o s t ( ’ / ’ , auth , authz ( ’ c o r e : c r e a t e ’ ) , async ( c t x ) => {
2
3
4
5
6
7
}
8
9 } ) ;

c t x . throw ( 4 0 0 , e r r o r . message ) ;

} catch ( e r r o r ) {

Figure 18: Snippet of the handle for the POST /cores endpoint in the API spacex-api.

on the recorded interactions with the database (not done here for NodeJS and MongoDB), and on best
practices in RESTful API design. For example, it is a recommended practice that POSTs on collections
of resources (e.g., /cores) create a new resource, whose id (chosen on the server, to guarantee it is
unique, unless it is a UUID) is present in the location HTTP header of the response (e.g., location:
/cores/42). EvoMaster can use these location headers to create POST requests followed with the
appropriate linked GET. However, on this API, the POST implementation in Figure 18 does not seem to
follow best practices in REST API design. For example, it does not setup the location header in the
response, and neither it returns the generated id (e.g., in the body payload). However, it could still
be possible to test this kind of API by doing the following: create a new resource with POST /cores,
followed by a GET of the whole collection (i.e., GET /cores), and then extract the id fields from this
response to call GET /cores/:id with a valid id. Note: this is assuming that the collection is empty.
If it is not, then the first POST is not even necessary. However, this also assumes that the name of the
field representing the id is the same (or at least very similar) in both the body payload and endpoint
path parameter (so they can be matched).

5.3 Discussion

Based on the analyses of the logs and tests generated for the 19 APIs, some general observations can
be made:

• Many research prototypes are not particularly robust, and can crash when applied on new SUTs.
For example, we have faced this issue with EvoMaster many times, like when adding a new
API to EMB [7]. Although we add new APIs to EMB each year, EMB has been available as
open-source since 2017, and anyone can use it for their empirical studies and make sure their tools
do not crash on it. However, it is important to stress out that, in this paper, we have compared
tool implementations rather than techniques. For example, a tool with low performance (e.g.,
due to crashes) could still feature novel techniques that could be very useful, e.g., if integrated or
re-implemented in a more mature tool.

• Like software, also schemas can have faults, and/or omissions (e.g., constraints on some inputs
might be missing). And this issue seems quite common, especially when schemas are written
manually. Although this problem could be addressed by white-box testing (currently supported
only by EvoMaster) by analyzing the source code of the SUTs, it looks like a major issue for
black-box testing, which might not have a viable solution.

• Interactions with databases are common in RESTful APIs. To execute the code to fetch some
data, such data should be first present in the database. The data could be created with endpoints
of the API itself (e.g., POST requests), as well as inserted directly into the database with SQL
commands. Both approaches have challenges: e.g., how to properly link different endpoints that
work on the same resources, and how to deal with data constraints that are specified in the code
of the API and not in the SQL schema of the database. The compared fuzzers provide different
solutions to address this problem, but it is clear that more still need to be done.

27

• Currently, no fuzzer deals with the mocking of external web services (e.g., using libraries such
as WireMock). Testing with external live services has many shortcomings (e.g., the generated
tests can become flaky), but it might be the only option for black-box strategies. For white-box
strategies, mocking of web services will be essential when testing industrial enterprise systems
developed with a microservice architecture (as in this class of software interactions between web
services are very common).

RQ3: There are several open challenges in fuzzing RESTful APIs, including for example how to
deal with underspecified schemas, and how to deal with interactions with external services (e.g.,
databases and other APIs).

6 Threats To Validity

Internal Validity. Besides writing the scaffolding to run and analyze the experiments, before running
the experiments we did not make code modifications in any existing tool for this study. Although
we are the authors of EvoMaster, to try to be fair, we used the latest release before running these
experiments, without re-running them after fixing any new issues. However, as we have used all these
APIs in our previous studies, EvoMaster did not crash on any of them.

We used seven existing tools, with their latest release versions when possible. However, there is a
possibility that we might have misconfigured them, especially considering that some of those tools have
minimal documentation. To avoid such possibility, we carefully look at their execution logs, to see if
there was any clear case of misconfiguration. Furthermore, we release all our scripts as open-source in
the repository of EvoMaster, so anyone can review them, and replicate the study if needed.

Any comparison of tools made by the authors of one of these tools is bound to be potentially bias,
especially if such tool turns out to give the best results (as in our case with EvoMaster). However,
the relative performance of the other six tools among them would be not affected by this issue. Likewise,
the in-depth analysis of the SUTs is not affected by this issue either.

All the compared fuzzers use randomized algorithms. To take this into account, each experiment

was repeated 10 times, and we analyzed them with the appropriate statistical tests.

External Validity. The chosen seven fuzzers are arguably representing the state-of-the-art in
testing RESTful APIs. However, results on 19 APIs might not generalize to other APIs as well, although
we selected different programming runtimes (e.g., JVM and NodeJS) as well including one industrial
API. This kind of system testing experiments are expensive (nearly 71.2 days of computational effort
in our case), which makes using more APIs challenging. Furthermore, finding RESTful APIs in
open-source repositories is not so simple, and considerable effort might then be needed to configure
them (e.g., setup external dependencies like databases and find out how/if they use any form of
authentication).

7 Conclusions

RESTful APIs are widely used in industry, and so several techniques have been developed in the
research community to automatically test them. Several reports in the literature show the usefulness
in practice of these techniques, by reporting actual faults automatically found with these fuzzers.
However, not much has been reported on how the different techniques compare, nor on how effective
they are at covering different parts of the code of these APIs. This latter is usually the case due to the
testing of remote APIs, for which researchers do not have access to their source code.

To address these issues, in this paper we have compared the state-of-the-art in fuzzing RESTful
APIs, using seven fuzzers to test 19 APIs, totaling more than 280 thousands of lines of code (for their
business logic, and not including the millions of lines of code of all the third-party libraries they use).
Each fuzzer was run for 1 hour, and each experiment was repeated 10 times, to take into account the
randomness of these tools.

The results show different degrees of coverage for the different black-box testing tools, where
EvoMaster seems the tool giving the best results (i.e., highest code coverage on average) on this

28

case study, closely followed by Schemathesis. However, no black-box fuzzer was able to achieve
more than 60% line coverage on average. Furthermore, the experiments show that white-box testing
gives better results than black-box testing. Still, large parts of these APIs are left uncovered, as the
fuzzers do not manage to generate the right data to maximize code coverage. Although these fuzzers
are already useful for practitioners in industry, more needs to be done to achieve better results.

To address this issue, this large empirical analysis has then be followed by an in-depth analysis of
the source code of each of these 19 APIs, to understand what are the main issues that prevent the
fuzzers from achieving better results. Several issues were identified, including for example how to deal
with underspecified schemas, and how to deal with interactions with external services (e.g., other APIs
and databases). This provides a useful list of common problems that researchers can use to drive new
research effort in improving performance on this problem domain.

For few of these issues, we discussed possible solutions, but those will need to be empirically
validated. Future work will aim at using the new knowledge and insight provided in this paper to
design new variants of these tools, to be able to achieve better results. In particular, the insight
provided in this study is shaping the current research efforts in EvoMaster, pointing to clear research
issues that need to be prioritized.

All the tools and APIs used in this study are available online. To enable the replicability of this
study, all our scripts used in our experiments are published as open-source, available online in the
repository of EvoMaster, at www.evomaster.org. These scripts get automatically stored on Zenodo
at each new release (e.g., version 1.5.0 [38]).

Acknowledgments

This work is funded by the European Research Council (ERC) under the European Union’s Horizon
2020 research and innovation programme (EAST project, grant agreement No. 864972).

References

[1] [n.d.].

Amazon Gateway API.
developerguide/apigateway-rest-api.html.

https://docs.aws.amazon.com/apigateway/latest/

[2] [n.d.]. APIFuzzer — HTTP API Testing Framework. https://github.com/KissPeter/APIFuzzer.

[3] [n.d.]. ApiTester. https://github.com/opendata-for-all/api-tester.

[4] [n.d.]. bBOXRT. https://git.dei.uc.pt/cnl/bBOXRT.

[5] [n.d.]. C8. https://github.com/bcoe/c8.

[6] [n.d.]. EvoMaster. https://github.com/EMResearch/EvoMaster.

[7] [n.d.]. EvoMaster Benchmark (EMB). https://github.com/EMResearch/EMB.

[8] [n.d.]. Google Drive API. https://developers.google.com/drive/api/v3/about-sdk.

[9] [n.d.]. Hibernate. http://hibernate.org.

[10] [n.d.]. JaCoCo. https://www.jacoco.org/.

[11] [n.d.]. Language-agnostic HTTP API Testing Tool. https://github.com/apiaryio/dredd.

[12] [n.d.]. LinkedIn API. https://docs.microsoft.com/en-us/linkedin/.

[13] [n.d.]. OpenAPI/Swagger. https://swagger.io/.

[14] [n.d.]. Programmable Web. https://www.programmableweb.com/.

[15] [n.d.]. Reddit API. https://www.reddit.com/dev/api.

29

[16] [n.d.]. RestCT. https://github.com/GIST-NJU/RestCT.

[17] [n.d.]. RESTest. https://github.com/isa-group/RESTest.

[18] [n.d.]. RESTler. https://github.com/microsoft/restler-fuzzer.

[19] [n.d.]. RestTestGen. https://github.com/SeUniVr/RestTestGen.

[20] [n.d.]. Schemathesi - Command Line Interface. https://schemathesis.readthedocs.io/en/stable/cli.html.

[21] [n.d.].

Schemathesis:

Property-based

testing

for

API

schemas.

https://schemathesis.readthedocs.io/.

[22] [n.d.].

for
https://github.com/Cornutum/tcases/tree/master/tcases-openapi.

OpenAPI:

Tcases

From

REST-ful

to

Test-ful.

[23] [n.d.]. Twitter API. https://developer.twitter.com/en/docs/twitter-api.

[24] [n.d.]. WireMock. https://wiremock.org/.

[25] Nasser Albunian, Gordon Fraser, and Dirk Sudholt. 2020. Causes and effects of fitness landscapes
in unit test generation. In Proceedings of the 2020 Genetic and Evolutionary Computation
Conference. 1204--1212.

[26] A. Arcuri. 2009. Full Theoretical Runtime Analysis of Alternating Variable Method on the Triangle
Classification Problem. In International Symposium on Search Based Software Engineering
(SSBSE). 113--121.

[27] A. Arcuri. 2009. Insight Knowledge in Search Based Software Testing. In Genetic and Evolutionary

Computation Conference (GECCO). 1649--1656.

[28] Andrea Arcuri. 2017. RESTful API Automated Test Case Generation. In IEEE International

Conference on Software Quality, Reliability and Security (QRS). IEEE, 9--20.

[29] Andrea Arcuri. 2018. EvoMaster: Evolutionary Multi-context Automated System Test Generation.
In IEEE International Conference on Software Testing, Verification and Validation (ICST).
IEEE.

[30] Andrea Arcuri. 2019. RESTful API Automated Test Case Generation with EvoMaster. ACM

Transactions on Software Engineering and Methodology (TOSEM) 28, 1 (2019), 3.

[31] Andrea Arcuri. 2020. Automated Black-and White-Box Testing of RESTful APIs With EvoMaster.

IEEE Software 38, 3 (2020), 72--78.

[32] A. Arcuri and L. Briand. 2014. A Hitchhiker’s Guide to Statistical Tests for Assessing Randomized
Algorithms in Software Engineering. Software Testing, Verification and Reliability (STVR) 24, 3
(2014), 219--250.

[33] Andrea Arcuri and Juan P. Galeotti. 2020. Handling SQL Databases in Automated System Test
Generation. ACM Transactions on Software Engineering and Methodology (TOSEM) 29, 4 (2020),
1--31.

[34] Andrea Arcuri and Juan P Galeotti. 2020. Handling SQL databases in automated system test
generation. ACM Transactions on Software Engineering and Methodology (TOSEM) 29, 4 (2020),
1--31.

[35] Andrea Arcuri and Juan P Galeotti. 2021. Enhancing Search-based Testing with Testability
Transformations for Existing APIs. ACM Transactions on Software Engineering and Methodology
(TOSEM) 31, 1 (2021), 1--34.

30

[36] Andrea Arcuri and Juan P Galeotti. 2021. Enhancing Search-based Testing with Testability
Transformations for Existing APIs. ACM Transactions on Software Engineering and Methodology
(TOSEM) 31, 1 (2021), 1--34.

[37] Andrea Arcuri, Juan Pablo Galeotti, Bogdan Marculescu, and Man Zhang. 2021. EvoMaster: A
Search-Based System Test Generation Tool. Journal of Open Source Software 6, 57 (2021), 2153.

[38] Andrea Arcuri, ZhangMan, asmab89, Bogdan, Amid Gol, Juan Pablo Galeotti, Seran, Al-
berto Martín López, Agustina Aldasoro, Annibale Panichella, and Kyle Niemeyer. 2022. EMRe-
search/EvoMaster:. https://doi.org/10.5281/zenodo.6651631

[39] Andrea Arcuri, ZhangMan, Amid Gol, and asmab89. 2022. EMResearch/EMB:. https://doi.

org/10.5281/zenodo.6106830

[40] Vaggelis Atlidakis, Patrice Godefroid, and Marina Polishchuk. 2019. RESTler: Stateful REST
API Fuzzing. In ACM/IEEE International Conference on Software Engineering (ICSE) (ICSE).
IEEE, 748–758.

[41] Vaggelis Atlidakis, Patrice Godefroid, and Marina Polishchuk. 2020. Checking security properties
of cloud service rest apis. In IEEE International Conference on Software Testing, Verification
and Validation (ICST). IEEE, 387--397.

[42] Marcel Böhme, László Szekeres, and Jonathan Metzman. 2022. On the Reliability of Coverage-
Based Fuzzer Benchmarking. In 44th IEEE/ACM International Conference on Software Engi-
neering, ser. ICSE, Vol. 22.

[43] José Campos, Yan Ge, Nasser Albunian, Gordon Fraser, Marcelo Eler, and Andrea Arcuri. 2018.
An empirical evaluation of evolutionary algorithms for unit test suite generation. Information and
Software Technology (IST) 104 (2018), 207--235.

[44] Davide Corradini, Amedeo Zampieri, Michele Pasqua, and Mariano Ceccato. 2021. Empirical com-
parison of black-box test case generation tools for RESTful APIs. In 2021 IEEE 21st International
Working Conference on Source Code Analysis and Manipulation (SCAM). IEEE, 226--236.

[45] Davide Corradini, Amedeo Zampieri, Michele Pasqua, Emanuele Viglianisi, Michael Dallago, and
Mariano Ceccato. 2022. Automated black-box testing of nominal and error scenarios in RESTful
APIs. Software Testing, Verification and Reliability (2022), e1808.

[46] Hamza Ed-douibi, Javier Luis Cánovas Izquierdo, and Jordi Cabot. 2018. Automatic Generation
of Test Cases for REST APIs: A Specification-Based Approach. In 2018 IEEE 22nd International
Enterprise Distributed Object Computing Conference (EDOC). 181--190.

[47] Patrice Godefroid, Bo-Yuan Huang, and Marina Polishchuk. 2020. Intelligent REST API Data
Fuzzing. In ACM Symposium on the Foundations of Software Engineering (FSE) (ESEC/FSE
2020). ACM, 725–736.

[48] Patrice Godefroid, Daniel Lehmann, and Marina Polishchuk. 2020. Differential regression testing
for REST APIs. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software
Testing and Analysis. 312--323.

[49] M. Harman and P. McMinn. 2010. A Theoretical and Empirical Study of Search Based Testing:
Local, Global and Hybrid Search. IEEE Transactions on Software Engineering (TSE) 36, 2
(2010), 226--247.

[50] Zac Hatfield-Dodds and Dmitry Dygalo. 2022. Deriving Semantics-Aware Fuzzers from Web
API Schemas. In 2022 IEEE/ACM 44th International Conference on Software Engineering:
Companion Proceedings (ICSE-Companion). IEEE, 345--346.

31

[51] Stefan Karlsson, Adnan Causevic, and Daniel Sundmark. 2020. QuickREST: Property-based Test
Generation of OpenAPI Described RESTful APIs. In IEEE International Conference on Software
Testing, Verification and Validation (ICST). IEEE.

[52] Myeongsoo Kim, Qi Xin, Saurabh Sinha, and Alessandro Orso. 2022. Automated Test Generation
for REST APIs: No Time to Rest Yet. https://doi.org/10.48550/ARXIV.2204.08348

[53] Nuno Laranjeiro, João Agnelo, and Jorge Bernardino. 2021. A black box tool for robustness

testing of REST services. IEEE Access 9 (2021), 24738--24754.

[54] Yi Liu, Yuekang Li, Gelei Deng, Yang Liu, Ruiyuan Wan, Runchao Wu, Dandan Ji, Shiheng Xu,
and Minli Bao. 2022. Morest: Model-based RESTful API Testing with Execution Feedback. In
ACM/IEEE International Conference on Software Engineering (ICSE).

[55] David R MacIver, Zac Hatfield-Dodds, et al. 2019. Hypothesis: A new approach to property-based

testing. Journal of Open Source Software 4, 43 (2019), 1891.

[56] Bogdan Marculescu, Man Zhang, and Andrea Arcuri. 2022. On the Faults Found in REST APIs
by Automated Test Generation. ACM Transactions on Software Engineering and Methodology
(TOSEM) 31, 3 (2022), 1--43.

[57] Alberto Martin-Lopez, Andrea Arcuri, Sergio Segura, and Antonio Ruiz-Cortés. 2021. Black-Box
and White-Box Test Case Generation for RESTful APIs: Enemies or Allies?. In 2021 IEEE 32nd
International Symposium on Software Reliability Engineering (ISSRE). IEEE, 231--241.

[58] Alberto Martin-Lopez, Sergio Segura, Carlos Muller, and Antonio Ruiz-Cortés. 2021. Specification
and automated analysis of inter-parameter dependencies in web APIs. IEEE Transactions on
Services Computing (2021).

[59] Alberto Martin-Lopez, Sergio Segura, and Antonio Ruiz-Cortés. 2020. RESTest: Black-Box
Constraint-Based Testing of RESTful Web APIs. In International Conference on Service-Oriented
Computing.

[60] Alberto Martin-Lopez, Sergio Segura, and Antonio Ruiz-Cortés. 2021. RESTest: Automated
Black-Box Testing of RESTful Web APIs. In ACM Int. Symposium on Software Testing and
Analysis (ISSTA). ACM.

[61] A Giuliano Mirabella, Alberto Martin-Lopez, Sergio Segura, Luis Valencia-Cabrera, and Antonio
Ruiz-Cortés. 2021. Deep Learning-Based Prediction of Test Input Validity for RESTful APIs. In
2021 IEEE/ACM Third International Workshop on Deep Learning for Testing and Testing for
Deep Learning (DeepTest). IEEE, 9--16.

[62] Andy Neumann, Nuno Laranjeiro, and Jorge Bernardino. 2018. An analysis of public REST web

service APIs. IEEE Transactions on Services Computing (2018).

[63] Sam Newman. 2015. Building Microservices. " O’Reilly Media, Inc.".

[64] Juan Carlos Alonso Valenzuela, Alberto Martin-Lopez, Sergio Segura, Jose Maria Garcia, and
Antonio Ruiz-Cortes. 2022. ARTE: Automated Generation of Realistic Test Inputs for Web APIs.
IEEE Transactions on Software Engineering (2022).

[65] Emanuele Viglianisi, Michael Dallago, and Mariano Ceccato. 2020. RESTTESTGEN: Automated
Black-Box Testing of RESTful APIs. In IEEE International Conference on Software Testing,
Verification and Validation (ICST). IEEE.

[66] Huayao Wu, Lixin Xu, Xintao Niu, and Changhai Nie. 2022. Combinatorial Testing of RESTful

APIs. In ACM/IEEE International Conference on Software Engineering (ICSE).

32

[67] Xusheng Xiao, Sihan Li, Tao Xie, and Nikolai Tillmann. 2013. Characteristic studies of loop prob-
lems for structural test generation via symbolic execution. In 2013 28th IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 246--256.

[68] Xusheng Xiao, Tao Xie, Nikolai Tillmann, and Jonathan de Halleux. 2011. Precise identification
of problems for structural test generation. In Proceeding of the 33rd International Conference on
Software Engineering (ICSE ’11). ACM, New York, NY, USA, 611--620.

[69] Man Zhang and Andrea Arcuri. 2021. Adaptive Hypermutation for Search-Based System Test
Generation: A Study on REST APIs with EvoMaster. ACM Transactions on Software Engineering
and Methodology (TOSEM) 31, 1 (2021).

[70] Man Zhang and Andrea Arcuri. 2022. Open Problems in Fuzzing RESTful APIs: A Comparison

of Tools. arXiv preprint arXiv:2205.05325 (2022).

[71] Man Zhang, Asma Belhadi, and Andrea Arcuri. 2022. JavaScript Instrumentation for Search-Based
Software Testing: A Study with RESTful APIs. In IEEE International Conference on Software
Testing, Verification and Validation (ICST). IEEE.

[72] Man Zhang, Bogdan Marculescu, and Andrea Arcuri. 2021. Resource and dependency based test
case generation for RESTful Web services. Empirical Software Engineering 26, 4 (2021), 1--61.

33

