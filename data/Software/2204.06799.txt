Environment Imitation: Data-Driven Environment Model Generation Using
Imitation Learning for EÔ¨Écient CPS Goal VeriÔ¨Åcation

Yong-Jun Shina, Donghwan Shinb,‚àó, Doo-Hwan Baea

aKorea Advanced Institute of Science and Technology
bUniversity of Luxembourg

2
2
0
2

r
p
A
4
1

]
E
S
.
s
c
[

1
v
9
9
7
6
0
.
4
0
2
2
:
v
i
X
r
a

Abstract

Cyber-Physical Systems (CPS) continuously interact with their physical environments through software controllers that
observe the environments and determine actions. Engineers can verify to what extent the CPS under analysis can achieve
given goals by analyzing its Field Operational Test (FOT) logs. However, it is challenging to repeat many FOTs to
obtain statistically signiÔ¨Åcant results due to its cost and risk in practice. To address this challenge, simulation-based
veriÔ¨Åcation can be a good alternative for eÔ¨Écient CPS goal veriÔ¨Åcation, but it requires an accurate virtual environment
model that can replace the real environment that interacts with the CPS in a closed loop. This paper proposes a novel
data-driven approach that automatically generates the virtual environment model from a small amount of FOT logs.
We formally deÔ¨Åne the environment model generation problem and solve it using Imitation Learning (IL) algorithms.
In addition, we propose three speciÔ¨Åc use cases of our approach in the evolutionary CPS development. To validate our
approach, we conduct a case study using a simpliÔ¨Åed autonomous vehicle with a lane-keeping system. The case study
results show that our approach can generate accurate virtual environment models for CPS goal veriÔ¨Åcation at a low cost
through simulations.

Keywords: Cyber-Physical System (CPS), Environment model generation, Imitation Learning (IL)
2010 MSC: 00-01, 99-00

1. Introduction

Cyber-Physical Systems (CPS) utilize both physical
and software components deeply intertwined to continu-
ously collect, analyze, and control physical actuators at
runtime [1]. CPS has been increasingly studied for many
applications, such as autonomous vehicles [2, 3], robots [4,
5], smart factories [6, 7], and medical devices [8, 9].

One of the essential problems in CPS development is
to verify to what extent the CPS under development can
achieve its goals. To answer this, a developer could de-
ploy a CPS (e.g., an autonomous vehicle) into its opera-
tional environment (e.g., a highway road) and verify the
CPS‚Äôs goal achievement (e.g., lane-keeping) using the logs
collected from the Field Operational Tests (FOTs). How-
ever, conducting FOTs is expensive, time-consuming, and
even dangerous, especially when hundreds of repeats are
required to achieve a certain level of statistical signiÔ¨Åcance
in the veriÔ¨Åcation results. An alternative is a simulation-
based approach where the software controller of the CPS
is simulated with a virtual environment model. Though
it can reduce the cost and risk of the CPS goal veriÔ¨Åca-
tion compared to using FOTs, it requires a highly crafted

‚àóCorresponding author
Email addresses: yjshin@se.kaist.ac.kr (Yong-Jun Shin),
donghwan.shin@uni.lu (Donghwan Shin), bae@se.kaist.ac.kr
(Doo-Hwan Bae)

virtual environment model based on deep domain knowl-
edge. Furthermore, it may not be possible at all if a high-
Ô¨Ådelity simulator for the problem domain does not exist. It
prevents the simulation-based approach from being better
used in practice.

To solve the diÔ¨Éculty of manually generating virtual
environment models, we propose an automated data-driven
environment model generation approach for CPS goal ver-
iÔ¨Åcation by recasting the problem of environment model
generation as the problem of imitation learning. We call
this novel approach ENVironment Imitation (ENVI ). In
machine learning, Imitation Learning (IL) has been widely
studied to mimic complex human behaviors in a given
task only from a limited amount of demonstrations [10].
Our approach leverages IL to mimic how the real envi-
ronment interacts with the CPS under analysis from a
small set of log data collected from FOTs. Since the log
data records how the CPS and the real environment inter-
acted, our approach can generate an environment model
that mimics a state transition mechanism of the real envi-
ronment according to the CPS action as closely as possible
to that recorded in the log data. The generated environ-
ment model is then used to simulate the CPS software
controller as many times as needed to statistically analyze
the CPS goal achievement.

We evaluate the feasibility of our novel approach while
comparing various imitation learning algorithms on a case

Preprint submitted to Journal of LATEX Templates

April 15, 2022

 
 
 
 
 
 
study of a lane-keeping system of an autonomous robot ve-
hicle. The evaluation results show that our approach can
automatically generate an environment model that mimics
the interaction mechanism between the lane-keeping sys-
tem and physical environment, even using minimal amounts
of FOT log data (e.g., less than 30 seconds execution log).
In summary, below are the contributions of this paper:

1) We shed light on the problem of environment model
generation for CPS goal veriÔ¨Åcation with a formal prob-
lem deÔ¨Ånition.

2) We propose ENVI, a novel data-driven approach for

environment model generation utilizing IL.

3) We assess the application of our approach through a
case study with a real CPS and various IL algorithms.

The remainder of this paper is organized as follows:
Section 2 illustrates a motivating example. Section 3 pro-
vides background on representative imitation learning al-
gorithms considered in our experiments. Section 4 formal-
izes the problem of the data-driven environment model
generation. Section 5 describes the steps of ENVI. Sec-
tion 6 reports on the evaluation of ENVI. Section 7 dis-
cusses implications and open issues. Section 8 introduces
related work. Section 9 concludes the paper.

2. Motivating Example

We present a simple example of CPS goal veriÔ¨Åcation

to demonstrate a use case of our approach.

Consider a software engineer developing a lane-keeping
system of an autonomous vehicle. The engineer aims to
develop and test the vehicle‚Äôs software controller (i.e., lane-
keeping system) that continuously monitors the distance
from the center of the lane and computes the steering angle
that determines how much to turn to keep the distance as
small as possible.

Once the software controller is developed, the engineer
must ensure that the vehicle equipped with the controller
continues to follow the center of the lane while driving.
To do this, the engineer deploys the vehicle on a safe road
and collect an FOT log, including the distance dt and the
steering angle at at time t = 1, . . . , T where T is a pre-
deÔ¨Åned FOT duration. Based on the collected data, the
engineer can quantitatively assess the quality of the lane-
keeping system by calculating the sum of the distances the
vehicle deviated from the center of the lane, i.e., Œ£T
t=1|dt|.
The quantitative assessment is used to verify precisely a
goal of the system, i.e., whether Œ£T
t=1|dt| < (cid:15) holds or not
for a small threshold (cid:15). Notice that, due to the uncertain-
ties in FOT, such as non-uniform friction between the tires
and the ground, the same FOT must be repeated multi-
ple times, and statistical analysis should be applied to the
results.

It takes a lot of time and resources to repeat the FOTs
enough to obtain statistically signiÔ¨Åcant results. To ad-
dress this issue, the engineer may decide to rely on sim-
ulations. However, using high-Ô¨Ådelity and physics-based
simulators, such as Webots [11] or Gazebo [12], is very
challenging, especially for software engineers who do not
have enough expertise in physics. It is not easy to accu-
rately design the physical components of the system (e.g.,
the size of wheels and the wheelbase) and the road in the
simulator so that the simulation results are almost identi-
cal to the FOT results.

Our approach, ENVI, enables the CPS goal veriÔ¨Åcation
without using such a high-Ô¨Ådelity simulator. The engi-
neer can simply provide ENVI with the software controller
(i.e., the lane-keeping system under analysis) and a small
amount of FOT logs collected from the beginning, which is
far less than the data required for statistically signiÔ¨Åcant
results using FOTs. Then ENVI automatically generates
a virtual environment model that imitates the behavior of
the real environment of the lane-keeping system; speciÔ¨Å-
cally, the virtual environment model can simulate dt+1 for
given dt and at for t = 2, . . . , T such that Œ£T
t=1|dt| calcu-
lated based on the virtual model is almost the same as the
value calculated based on the FOTs. Therefore, by quickly
re-running the simulation multiple times, the engineer can
have statistically signiÔ¨Åcant results about the quality of the
software controller at little cost. Furthermore, if multiple
software controller versions make diÔ¨Äerent CPS behaviors,
the virtual environment model generated by ENVI can be
reused to verify the CPS goal achievements of new con-
troller versions that have never been tested in the real
environment.

The challenge for ENVI is automatically generating
a virtual environment model that behaves as similar as
possible to the real environment using a limited amount
of data. To address this, we leverage imitation learning
detailed in Section 3.

3. Background: Imitation Learning

Imitation Learning (IL) is a learning method that al-
lows an agent to mimic expert behaviors for a speciÔ¨Åc task
by observing demonstrations of the expert [10]. For exam-
ple, an autonomous vehicle can learn to drive by observing
how a human driver controls a vehicle. IL assumes that
an expert decides an action depending on only the state
that the expert encounters. Based on this assumption, an
expert demonstration is a series of pairs of states and ac-
tions, and IL aims to extract the expert‚Äôs internal decision-
making function (i.e., a policy function that maps states
into actions) from the demonstration [10]. We introduce
two representative IL algorithms in the following subsec-
tions: Behavior Cloning (BC) and Generative Adversarial
Imitation Learning (GAIL).

2

3.1. Behavior Cloning

Behavior Cloning (BC) infers the policy function of the
expert using supervised learning [13, 14]. Training data
can be organized by pairing states and corresponding ac-
tions in the expert‚Äôs demonstration. Then existing super-
vised learning algorithms can train the policy function that
returns expert-like actions for given states. Due to the sim-
plicity of the BC algorithm, BC can create a good policy
function that mimics the expert quickly if there are suÔ¨É-
ciently much demonstration data. However, if the training
data (i.e., expert demonstration) does not fully cover the
input state space or is biased, the policy function may not
mimic the expert behavior correctly [14].

3.2. Generative Adversarial Imitation Learning

Generative Adversarial Imitation Learning (GAIL) [15]
utilizes the idea of Generative Adversarial Networks [16]
to evolve the policy function using iterative competitions
with a discriminator that evaluates the policy function.
Therefore, both the policy function and the discriminator
are trained in parallel.

The policy function gets states in the expert demon-
stration and produces simulated actions. The discrimina-
tor then gets the policy function‚Äôs input (i.e., states) and
output (i.e., simulated actions) and evaluates how the pol-
icy function behaves like the real expert, as shown in the
demonstration. The more similar the simulation is to the
expert demonstration, the more rewarded the policy func-
tion is by the discriminator. The policy function is trained
to maximize the reward from the discriminator.

On the other hand, the discriminator is trained using
both the demonstration data and the simulation trace of
the policy function. The state and action pairs, which is
the input and output of the policy function, in the demon-
stration data are labeled as real, but the pairs in the sim-
ulation trace are labeled as fake. A supervised learning
algorithm trains the discriminator to quantitatively eval-
uate whether a state and action pair is real (returning a
high reward) or fake (returning a low reward).

After numerous learning iterations of the policy func-
tion and the discriminator, the policy function Ô¨Ånally mim-
ics the expert well to deceive the advanced discrimina-
tor. GAIL uses both the expert demonstration data and
the simulation trace data of the policy function generated
internally, so it works well even with small demonstra-
tion data [15]. However, because of the internal simula-
tion of the policy function, its learning speed is relatively
slow [17].

4. Problem DeÔ¨Ånition

This section introduces a mathematical framework for
modeling how the CPS under analysis interacts with its
environment to achieve its goals. Based on the formal
framework, we then deÔ¨Åne the environment model gener-
ation problem for CPS goal veriÔ¨Åcation.

Figure 1: Formal framework for CPS goal veriÔ¨Åcation

4.1. A Formal Framework for CPS Goal VeriÔ¨Åcation

A CPS achieves its goals by interacting with its physi-
cal environment. SpeciÔ¨Åcally, starting from an initial state
of the environment, the CPS software controller observes
the state and decides an appropriate action to maximize
the likelihood of achieving the goals. Then, taking action
causes a change in the environment for the next step, which
the CPS will observe again to decide an action for the next
step. We assume the CPS and the environment interact
in a closed loop without interference by a third factor. To
formalize this process, we present a novel CPS-ENV in-
teraction model inspired by Markov Decision Process [18]
that models an agent‚Äôs sequential decision-making process
under observation over its environmental states.

SpeciÔ¨Åcally, a CPS-ENV interaction model is a tuple
M = (S, A, œÄ, Œ¥, s0), where S is a set of observable states
of the environment under consideration, A is a set of pos-
sible CPS actions, œÄ : S ‚Üí A is a policy function that
captures the software controller of the CPS, Œ¥ : S √ó A ‚Üí S
is a transition function that captures the transitions of en-
vironmental states over time as a result of CPS actions
and its previous states1, and s0 is an initial environmental
state. For example, starting from s0, the CPS makes an
action a0 = œÄ(s0), leading to a next state s1 = Œ¥(s0, a0).
By observing s1, the CPS again makes the next action
a1 = œÄ(s1), and so on.

a2‚àí‚Üí ...

an‚àí1‚àí‚àí‚àí‚Üí sn over n steps where st‚àí1

For a CPS-ENV interaction model M = (S, A, œÄ, Œ¥, s0),
a1‚àí‚Üí
a0‚àí‚Üí s1
we can think of a sequence of transitions s0
at‚àí1‚àí‚àí‚àí‚Üí st denotes
s2
a transition from a state st‚àí1 to another state st of the
environment by taking an action at‚àí1 of the CPS. More
formally, we deÔ¨Åne a trajectory of M over T time ticks as
a sequence of tuples tr (M, T ) = (cid:104)(s0, a0), . . . , (sT , aT )(cid:105).

Since a trajectory of a CPS-ENV interaction model
concisely captures the sequential interaction between the
CPS under analysis and its environment, one can easily
verify whether CPS goals are achieved or not by analyz-
ing the trajectory. Figure 1 visualizes how a CPS-ENV
interaction model is used for simulation-based CPS goal

1Though we use deterministic policy and transition functions for
simplicity, they can be easily extended in terms of probability density,
i.e., œÄ : S √ó A ‚Üí [0, 1] and Œ¥ : S √ó A √ó S ‚Üí [0, 1], to represent
stochastic behaviors if needed.

3

veriÔ¨Åcation. SpeciÔ¨Åcally, let œÜ be a requirement that pre-
cisely speciÔ¨Åes a goal under veriÔ¨Åcation. The achievement
of œÜ is quantiÔ¨Åable. For a CPS-ENV interaction model M ,
the veriÔ¨Åcation result of œÜ for M , denoted by œà(M, œÜ), is
computed by evaluating the achievement of œÜ on the tra-
jectory of M . Depending on the type of œÜ, the value of
œà(M, œÜ) can be Boolean (expressing the success or failure
of a requirement with clear-cut criteria) or Float (express-
ing the measurement of an evaluation metric of œÜ). For
example, one of the evaluation metrics of the lane-keeping
requirement is the distance the vehicle is away from the
center of the lane. As a result of the veriÔ¨Åcation of the
lane-keeping goal, the average or maximum distance from
the center is computed.

4.2. Problem Statement

The problem of virtual environment model generation
for simulation-based CPS goal veriÔ¨Åcation is to Ô¨Ånd an
accurate virtual environment model that can replace the
real environment of the CPS goal under veriÔ¨Åcation while
maintaining the same level of veriÔ¨Åcation accuracy. Specif-
ically, for the same CPS under analysis, let a CPS-ENV
interaction model Mr = (S, A, œÄ, Œ¥r, s0) representing the
interaction between the CPS and its real environment (in
FOT) and another model Mv = (S, A, œÄ, Œ¥v, s0) represent-
ing the interaction between the same CPS and its virtual
environment (in simulations). Notice that we have the
same S, A, œÄ, and so for both Mr and Mv since they are
about the same CPS2, whereas Œ¥r and Œ¥v are diÔ¨Äerent since
they represent how the corresponding environments react
to the actions performed by the CPS. For a requirement œÜ,
we aim to have Œ¥v that minimizes the diÔ¨Äerence between
œà(Mr, œÜ) and œà(Mv, œÜ). Therefore, the problem of virtual
environment model generation for CPS goal veriÔ¨Åcation is
to Ô¨Ånd Œ¥v such that |œà(Mr, œÜ)‚àíœà(Mv, œÜ)| is the minimum.
The virtual environment model generation problem has
three major challenges. First, the number of possible states
and actions is often very large, making it infeasible to build
a virtual environment model (i.e., represented by a tran-
sition function Œ¥v : S √ó A ‚Üí S) by exhaustively analyzing
individual states and actions. Second, since the virtual
environment model continuously interacts with the CPS
under analysis in a closed-loop, even a small diÔ¨Äerence be-
tween the virtual and real environments can signiÔ¨Åcantly
diÔ¨Äer in veriÔ¨Åcation results as it accumulates over time, the
so-called compounding error problem. This means that
simply having a transition function Œ¥v that mimics the be-
havior of Œ¥r in terms of individual input and output pairs,
without considering the accumulation of errors for sequen-
tial inputs, is not enough. Third, generating Œ¥v should not
be as expensive as using many FOTs; otherwise, there is
no point in using simulation-based CPS goal veriÔ¨Åcation.
Recall that manually crafting virtual environment mod-
els in a high-Ô¨Ådelity simulator requires a lot of expertise,

2Note that S can be the same for Mr and Mv because it is a set
of observable states from the perspective of the CPS under analysis.

4

which takes longer than doing FOTs many times for hav-
ing statistically signiÔ¨Åcant veriÔ¨Åcation results. Therefore,
a practical approach should generate an accurate virtual
environment model eÔ¨Éciently and automatically.

To address the challenges mentioned above, we sug-
gest leveraging IL to automatically generate virtual envi-
ronment models from only a small amount of data. The
data is the partial trajectory of Mr, which can be collected
from a few FOTs for the CPS under test in its real appli-
cation environment. Since IL can eÔ¨Éciently extract how
experts make sequential actions for given states from a lim-
ited amount of demonstrations while minimizing the com-
pounding errors, it is expected to be an excellent match
to our problem. Therefore for our problem, IL will extract
Œ¥v, instead of œÄ (which is the original goal of IL), that can
best reproduce given trajectories of Mr (i.e., FOT logs).
Generated Œ¥v may diÔ¨Äer depending on the amount of the
trajectory, so we analyze it in the experiment.

5. Environment Imitation

This section provides ENVI, a novel approach to the
problem of environment model generation for CPS goal
veriÔ¨Åcation, deÔ¨Åned in Section 4. We solve the problem
by using IL to automatically infer a virtual environment
state transition function from the log recorded during the
interaction between the CPS under test and its applica-
tion environment.
In this context, the real application
environment is considered an ‚Äúexpert,‚Äù and the FOT log
demonstrates the expert.

Figure 2 shows the overview of the environment model
generation and simulation-based CPS goal veriÔ¨Åcation pro-
cess using our approach.
It is composed of three main
stages: (1) FOT log collection for model generation, (2)
environment model generation using an IL algorithm, and
(3) CPS goal veriÔ¨Åcation using the generated environment
model. In the Ô¨Årst stage, engineers collect FOT logs of a
CPS controller under analysis œÄ deployed in its real ap-
plication environment. The interaction between the CPS
and the real environment is abstracted as Mr, including
the unknown Œ¥r. The trajectory of Mr recorded in the
logs is then used by IL algorithms in the second stage to
generate a virtual environment model Œ¥v that imitates Œ¥r
automatically.
In the last stage, the simulation of œÄ in
the virtual environment described by Œ¥v is performed to
generate simulation logs as many as needed for statistical
veriÔ¨Åcation. As a result, engineers can statistically verify
to what extent a requirement œÜ of the CPS is satisÔ¨Åed us-
ing only a few FOT logs. In the following subsections, we
explain each of the main steps in detail with the example
introduced in Section 2.

5.1. FOT Log Collection

The Ô¨Årst stage of ENVI is to collect the interaction
data between the CPS controller and its real environment,
which will be used as the ‚Äúdemonstrations‚Äù of imitation

Figure 2: Environment Imitation process and simulation-based CPS goal veriÔ¨Åcation

learning to generate the virtual environment later. For a
CPS-ENV interaction model Mr = (S, A, œÄ, Œ¥r, s0) deÔ¨Åned
in Section 4, the interaction data collected over time T can
be represented as the trajectory of Mr over T steps, i.e.,
(cid:104)(s0, a0), (s1, a1), . . . , (sT , aT )(cid:105) where st+1 = Œ¥r(st, at) and
at = œÄ(st) for t ‚àà {0, 1, . . . , T ‚àí 1}. The trajectory can be
easily collected from an FOT, since it is common to record
the interaction between the CPS controller and its real
environment as an FOT log [19]. For example, the lane-
keeping system records time-series data of the distances
the vehicle deviated from the center of the lane dt and the
steering angles at over t = 0, 1, . . . , T during an FOT.

In practice, the trajectory of the same Mr is not nec-
essarily the same due to the uncertainty of the real envi-
ronment, such as the non-uniform surface friction. There-
fore, it is recommended to collect a few FOT logs for the
same Mr. Since the virtual environment model generated
by imitation learning will mimic the given trajectories as
much as possible, the uncertainty of the real environment
recorded in the trajectories will also be imitated. Section 6
will investigate to what extent virtual environment models
generated by ENVI can accurately mimic the real environ-
ment in terms of CPS goal veriÔ¨Åcation when the size of the
given FOT logs varies.

5.2. Environment Model Generation

The second stage of ENVI is to generate a virtual en-
vironment model from the collected FOT logs using an IL
algorithm.
It consists of two steps: (1) deÔ¨Åne the envi-
ronment model structure and (2) run an IL algorithm to
generate a trained model.

5.2.1. DeÔ¨Åning Environment Model Structure

We implement an environment model as a neural net-
work to leverage imitation learning. Before training the
environment model, users deÔ¨Åne the neural network struc-
ture.

The virtual environment model structure is based on
the environmental state transition function Œ¥ : S √ó A ‚Üí S
deÔ¨Åned in Section 4. It assumes that the ideal (real) en-
vironment generates the next state st+1 ‚àà S by taking
the current environment state st ‚àà S and the current CPS

Figure 3: The environment model structure

action at ‚àà A only, meaning that (st, at) is suÔ¨Écient to de-
termine st+1 in the ideal environment at time t. However,
in practice, s may not include suÔ¨Écient information since
it is observed by the sensors of the CPS under veriÔ¨Åcation
and the sensors have limited sensing capabilities. To solve
this issue, we extend Œ¥ for virtual environment models as
Œ¥v : (S √ó A)l ‚Üí S where l is the length of the state-action
pairs required to predict the next state. This means that
Œ¥v uses (cid:104)(st‚àíl+1, at‚àíl+1), . . . , (st, at)(cid:105) to predict st+1. No-
tice that Œ¥v is equal to Œ¥ when l = 1. To account for the
extension of Œ¥, we also extend the CPS-ENV interaction
model M = (S, A, œÄ, Œ¥, s0) to Mv = (S, A, œÄ, Œ¥v, œÉ0) where
œÉ0 = (cid:104)(s0, a0), . . . , (sl‚àí1, al‚àí1)(cid:105) is a partial trajectory of
Mr over l steps starting from s0. Intuitively speaking, œÉ0
is the initial input for Œ¥v similar to s0 (and a0 = œÄ(s0)) for
Œ¥.

Based on the extended deÔ¨Ånition of Œ¥v, the structure
of Œ¥v is shown in Figure 3. The input and output of Œ¥v
are (cid:104)(st‚àíl+1, at‚àíl+1), . . . , (st, at)(cid:105) and st+1, respectively, as
deÔ¨Åned above. Recall that an environmental state s and
a CPS action a can be vectors in general; let |x| be the
length of a vector x. Then, the number of input neurons
of the neural network is l √ó (|s| + |a|), and the number of
output neurons is |s|.

DeÔ¨Åning the environment model structure involves two

5

manual tasks. The Ô¨Årst task is to choose a proper value
for the history length l. If the value of l increases, more
information can be captured in environmental states while
the cost of training and executing Œ¥v increases. Therefore,
it is important to balance the amount of information and
the cost of computation. For example, one can visualize
the FOT log and see if there are any cyclic patterns in the
sequence of environmental states. The second task is to
design the hidden layers of Œ¥v. The hidden layers specify
how the output variables are calculated from the input
variables, so-called forward propagation. The design of
hidden layers is speciÔ¨Åc to a domain, but general guidelines
of the neural network design exist for practitioners [20‚Äì22].

5.2.2. Environment Model Training using IL Algorithms
Once the structure of Œ¥v is determined, we can train
Œ¥v using an IL algorithm with a proper set of training
data D = {(X1, Y1), . . . , (Xn, Yn)}, where n is the number
of FOT logs, Xi is the sequence of inputs collected from
i-th FOT log and Yi is the corresponding sequence of out-
puts (i.e., the expected value of Œ¥v(xj) is yj for all j ‚àà
{1, . . . , |Xi|} and |Xi| = |Yi| for i ‚àà {1, . . . , n}). Since x ‚àà
X is an l-length sequence of state-action pairs, we can gen-
erate D from an FOT log using a sliding window of length
l. SpeciÔ¨Åcally, for an FOT log (cid:104)(s0, a0), . . . , (sT , aT )(cid:105), xj =
(cid:104)(sj, aj), . . . , (sl‚àíj+1, al‚àíj+1)(cid:105) for j ‚àà {0, . . . , T ‚àí l + 1}.

In the following subsections, we explain how each of
the representative IL algorithms, i.e., BC, GAIL, and the
combination of BC and GAIL, can be used for training Œ¥v.

Using BC. As described in Section 3.1, BC trains an
environment model Œ¥v using supervised learning. Pairs of
the input and output of the real environment recorded in
FOT logs are given to Œ¥v as training data, and Œ¥v is trained
to learn the real environment state transition shown in the
training data.

SpeciÔ¨Åcally, the BC algorithm (whose pseudocode is
shown in Algorithm 1) takes as input a randomly initial-
ized environment model Œ¥v and a training dataset D; it
returns an environment model Œ¥v trained using D.

Algorithm 1: ENVI BC algorithm

Input : ENV model (randomly initialized) Œ¥v,

Training data D = {(X1, Y1), . . . , (Xn, Yn)}

Output: ENV model (trained) Œ¥v
1 while not(stoping condition) do
foreach (X, Y ) ‚àà D do
2

3

4

5

Sequence of model outputs Y (cid:48) ‚Üê Œ¥v(X)
Float loss BC ‚Üê getLoss(Y, Y (cid:48))
Œ¥v ‚Üê update(Œ¥v, loss BC )

end

6
7 end
8 return Œ¥v

(X, Y ) ‚àà D, the algorithm repeats the following (lines 2‚Äì
6): (1) executing Œ¥v on X to predict a sequence of outputs
Y (cid:48) (line 3), (2) calculating the training loss lossBC based
on the diÔ¨Äerence between Y (cid:48) and Y (line 4), and (3) up-
dating Œ¥v to minimize lossBC (line 5). The algorithm ends
by returning Œ¥v (line 8).

Algorithm 1 is intuitive and easy to implement.
In
addition, the model‚Äôs loss converges fast because it is a
supervised learning approach. However, if the training
data does not fully cover the input space or is biased, the
model may not accurately imitate the real environment.

Using GAIL. As described in Section 3.2, GAIL itera-
tively trains not only Œ¥v but also the discriminator Œ∂ that
evaluates Œ¥v in terms of the CPS controller œÄ. SpeciÔ¨Åcally,
for a state s, Œ∂ evaluates Œ¥v with respect to Œ¥r (captured by
D) by comparing Œ¥v(s, œÄ(s)) and Œ¥r(s, œÄ(s)). To do this,
Œ∂ is trained using D by supervised learning3, and Œ¥v is
trained using the evaluation results of Œ∂.

Algorithm 2 shows the pseudocode of GAIL. Similar
to Algorithm 1, it takes as input a randomly initialized
environment model Œ¥v and a training dataset D = (X, Y );
however, it additionally takes as input a randomly initial-
ized discriminator Œ∂ and the CPS controller under analysis
œÄ. It returns a trained virtual environment model Œ¥v.

Algorithm 2: ENVI GAIL algorithm

Input : ENV model (randomly initialized) Œ¥v,
Discriminator (randomly initialized) Œ∂,
Function of CPS decision-making logic œÄ,
Training data D = {(X1, Y1), . . . , (Xn, Yn)}

Output: ENV model (trained) Œ¥v
1 while not(stoping condition) do
foreach (X, Y ) ‚àà D do
2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

// Discriminator training
Sequence of model outputs Y (cid:48) ‚Üê Œ¥v(X)
Float loss d ‚Üê getDisLoss(Œ∂, X, Y, Y (cid:48))
Œ∂ ‚Üê update(Œ∂, loss d)

// Environment model training
Sequence of model rewards R ‚Üê ‚àÖ
Model input x(cid:48) ‚Üê X[0]
for |X| ‚àí 1 do

Model output y(cid:48) ‚Üê Œ¥v(x(cid:48))
Reward r ‚Üê Œ∂(x(cid:48), y(cid:48))
R ‚Üê append (R, r)
CPS action a ‚Üê œÄ(y(cid:48))
x(cid:48) ‚Üê updateInput(x(cid:48), y(cid:48), a)

end
Float loss GAIL ‚Üê aggregate(R)
Œ¥v ‚Üê update(Œ¥v, loss GAIL)

end

17
18 end
19 return Œ¥v

The algorithm iteratively trains Œ¥v using D until a stop-
ping condition (e.g., a Ô¨Åxed number of iterations or con-
vergence of the model‚Äôs loss) is met (lines 1‚Äì7). For each

3The structure of Œ∂ is similar to Œ¥v, but the input of Œ∂ is

(s, Œ¥v(s, œÄ(s)) and the output of Œ∂ is a reward value r.

6

The algorithm iteratively trains both Œ¥v and Œ∂ using
D and œÄ until a stopping condition is met (lines 1‚Äì18).
To train Œ∂, for each (X, Y ) ‚àà D (lines 2‚Äì17), the algo-
rithm executes Œ¥v on X to predict a sequence of outputs
Y (cid:48) (line 3), calculates the discriminator loss loss d indicat-
ing how well Œ∂ can distinguish Y and Y (cid:48) for X (line 4),
and updates Œ∂ using loss d (line 5). Once Œ∂ is updated,
the algorithm trains Œ¥v using Œ∂ and œÄ (lines 6‚Äì16). Specif-
ically, the algorithm initializes a sequence of rewards R
(line 6) and a model input x(cid:48) (line 7), collects r ‚àà R for
each x(cid:48) using Œ¥v, œÄ, and Œ∂ (lines 8‚Äì14), calculates the envi-
ronment model loss lossGAIL by aggregating R (line 15),
and updates Œ¥v using lossGAIL (line 16). To collect r ‚àà R
for each x(cid:48) (lines 8‚Äì14), the algorithm executes Œ¥v on x(cid:48)
to predict an output y(cid:48) (line 9), executes Œ∂ on x(cid:48) and
y(cid:48) to get a reward r (line 10), appends r at the end of
R (line 11), executes œÄ on y(cid:48) to decide a CPS action a
(line 12), and updates x(cid:48) = (cid:104)(s1, a1), (s2, a2) . . . , (sl, al)(cid:105)
as x(cid:48) = (cid:104)(s2, a2) . . . , (sl, al), (y(cid:48), a)(cid:105) by removing (s1, a1)
and appending (y(cid:48), a) (line 13). The algorithm ends by
returning Œ¥v (line 19).

Notice that, to train Œ¥v, GAIL uses the input-output
pair (x(cid:48), y(cid:48)) simulated by œÄ and Œ∂, in addition to the real
input-output pair (x, y) in D. This is why it is known to
work well even with a small amount of training data [15,
17]. However, the algorithm is more complex to implement
than BC, and the environment model converges slowly or
sometimes fails to converge depending on hyperparameter
values.

Using BC and GAIL together. Notice that BC trains
Œ¥v using the training data only, but GAIL trains Œ¥v using
the simulated data as well; BC and GAIL can be combined
to use both training and simulated data without algorith-
mic conÔ¨Çict. This idea is suggested by Ho and Ermon [15]
to improve learning performance, and Jena et al. [17] later
implemented the idea as an algorithm BCxGAIL.

The BCxGAIL algorithm is the same as GAIL in terms
of its input and output, and it also trains both Œ¥v and Œ∂
similar to GAIL. In particular, Œ∂ is updated as the same as
in GAIL. However, Œ¥v is updated using both loss BC (line
4 in Algorithm 1) and loss GAIL (line 15 in Algorithm 2).
By doing so, BCxGAIL can converge fast (similar to BC)
with a small amount of training data (similar to GAIL).

5.3. Simulation-based CPS Goal VeriÔ¨Åcation

Using the virtual environment model Œ¥v generated from
the previous stage, an engineer can statistically verify if
the CPS controller œÄ under analysis satisÔ¨Åes a goal œÜ (i.e.,
compute œà(Mv, œÜ)) through many simulations of Mv =
(S, A, œÄ, Œ¥v, œÉ0).

To simulate Mv, the initialization data œÉ0 should be
given. Since œÉ0 is the partial trajectory of Mr over l steps,
the engineer should conduct partial FOTs over l steps to
get œÉ0. Notice that acquiring œÉ0 is much cheaper than hav-
ing full FOTs for FOT-based CPS goal veriÔ¨Åcation since
l is much shorter than T (i.e., the full FOT duration).

The engineer then run Mv as many times as needed for
statistical veriÔ¨Åcation4. For example, to verify if a vehicle
equipped with a lane-keeping system under development is
not more than 1 m away from the center of the lane, engi-
neers simulate the lane-keeping system several times with
the generated environment model. The engineers then an-
alyze the distance farthest from the center of the lane in
each simulation and verify whether the requirement is sta-
tistically satisÔ¨Åed.

In practice, it is common to develop multiple versions of
the same CPS controller, for example, developed sequen-
tially during its evolutionary development [23‚Äì25]. Let
us consider a lane-keeping system controller implemented
with a conÔ¨Åguration parameter indicating the minimum
degree of steering for lane-keeping. Then, one can develop
a new version of the lane-keeping system by changing the
parameter value based on the CPS goal veriÔ¨Åcation results
of its previous versions. In such an evolutionary develop-
ment process, for the veriÔ¨Åcation of the new version, we
can consider diÔ¨Äerent use cases depending on which ver-
sion of the FOT logs is used to generate the environment
model. SpeciÔ¨Åcally, we can consider three diÔ¨Äerent use
cases:

Case 1: One version is used for training, and veriÔ¨Åcation
is performed on the same version as training. This is the
basic use case, shown in Figure 4 (a). For example, for the
veriÔ¨Åcation of the Ô¨Årst version of the lane-keeping system
controller, some FOT logs of that version must be collected
since there are no previous versions (and their FOT logs).
Since Training involves One version and VeriÔ¨Åcation is for
the Known version, we refer to this case TOVK.

Case 2: Multiple versions are used for training, and veri-
Ô¨Åcation is performed on one of the versions used for train-
ing. Multiple versions of the CPS controller can be used
for training, as shown in Figure 4 (b). For example, when
there are diÔ¨Äerent sets of FOT logs collected by previously
developed versions of the lane-keeping system in addition
to the FOT logs collected by the new version, all the logs
associated with diÔ¨Äerent parameter values can be used to-
gether to generate a single environment model. This al-
lows us to best utilize all FOT logs for virtual environ-
ment model generation. Since Training involves Multiple
versions and VeriÔ¨Åcation is for one of the Known versions,
we refer to this case TMVK.

Case 3: Multiple versions are used for training, and veri-
Ô¨Åcation is performed on a new version that has never been
used for training. As shown in Figure 4 (c), this is sim-
ilar to the TMVK use case, but without using FOT logs
collected by the new version.
In other words, only the
previously collected FOT logs are used for the veriÔ¨Åcation
of the new version. This allows us to signiÔ¨Åcantly reduce

4This is because Œ¥v can be non-deterministic and the same œÉ0 can

lead to diÔ¨Äerent simulation results.

7

Figure 4: Use cases of simulation-based veriÔ¨Åcation using ENVI

the cost of new FOTs for the new version for CPS goal
veriÔ¨Åcation. Since Training involves Multiple versions and
VeriÔ¨Åcation is for an Unknown version, we refer to this
case TMVU.

6. Case Study

This section provides a case study to evaluate the ap-
plicability of our approach in various use cases introduced
in Section 5.3. SpeciÔ¨Åcally, we Ô¨Årst investigate the accu-
racy of CPS goal veriÔ¨Åcation results when ENVI is used
for a single CPS controller version (i.e., the TOVK use
case). We then analyze if ENVI can eÔ¨Éciently generate
a single environment model that can be used for the CPS
goal veriÔ¨Åcation of multiple CPS controller versions (i.e.,
the TMVK use case). Last but not least, we also investi-
gate if the single environment model can be used for the
CPS goal veriÔ¨Åcation of a new CPS controller version that
has never been used for training (i.e., the TMVU use case).
To summarize, we answer the following research questions:

RQ1: Can ENVI generate a virtual environment model
that can replace the real environment in the CPS
goal veriÔ¨Åcation for a single CPS controller version?
(TOVK )

RQ2: Can ENVI generate a virtual environment model
that can replace the real environment in the CPS
goal veriÔ¨Åcation for multiple CPS controller ver-
sions? (TMVK )

RQ3: Can ENVI generate a virtual environment model
that can replace the real environment in the CPS
goal veriÔ¨Åcation for a new CPS controller version?
(TMVU )

6.1. Subject CPS

To answer the research questions in the context of a
real CPS development process, we implement a simpliÔ¨Åed
autonomous vehicle equipped with a lane-keeping system.

8

Figure 5: Case study subject CPS: a LEGO-lized autonomous vehicle

We utilize an open physical experimental environment [26]
that abstracts an autonomous vehicle as a programmable
LEGO robot and a road as a white and black paper lane,
as shown in Figure 5. The goal of the lane-keeping system
is to keep the center of the lane, indicated by the border
between white and black areas while driving, so we aim
to verify the goal achievement of the lane-keeping system
(e.g., how smoothly it drives following the lane center).
Similar to many other CPS, the LEGO-lized autonomous
vehicle comprises three parts: sensor, controller, and ac-
tuator. A sensor (e.g., a color sensor) gives data observing
the CPS environment to a controller. A controller (e.g.,
a Python program in a LEGO brick) controls actuators
(e.g., a motor of a wheel) that make CPS act.

As for the controller, we aim to consider multiple ver-
sions of the same lane-keeping system and compare them
using simulation-based CPS goal veriÔ¨Åcation to Ô¨Ånd the
best one that allows the ego vehicle to drive the smoothest
along the center of the lane. To do this, we develop a
template of rule-based lane-keeping system logic and in-
stantiate it into multiple versions of the same lane-keeping
system with diÔ¨Äerent parameter values. Algorithm 3 shows
the template logic with a conÔ¨Ågurable parameter x indi-
cating the degree of rotation; the algorithm takes as input
a color c (range from 0 meaning the darkest to 100 mean-
ing the brightest) from the color sensor and returns an
angle a for the rotation motor. Positive/negative angle
means turning right/left, respectively. The algorithm sim-
ply turns right if the value of c is greater than 50 (i.e., the
color is darker than gray) and turns left if the value of c is
less than 50 (i.e., the color is lighter than gray); otherwise
(i.e., the color is exact gray), the algorithm goes straight.
The parameter value of x determines the degree of turning
right and left. We consider Ô¨Åve diÔ¨Äerent parameter values
of x, i.e., from 10‚ó¶ to 50‚ó¶ in steps of 10‚ó¶ in our case study.
Although the algorithm simpliÔ¨Åes the logic of lane-
keeping systems with a conÔ¨Åguration parameter x, making
a parameterized controller and optimizing the controller‚Äôs
conÔ¨Åguration are common in practice [27, 28].
In addi-
tion, engineers experience that changing the conÔ¨Åguration
changes the CPS behavior in the real environment. Fig-
ure 6 shows the partial FOT logs of the lane-keeping sys-
tem with diÔ¨Äerent x values used in our case study; we can
see how the interaction between the lane-keeping system

Algorithm 3: Lane-keeping system controller logic
ConÔ¨Åg.: Positive Ô¨Çoat of unit rotation degree x
Input : Float of lane color value c
Output: Float of rotation angle value a

1 Float gray ‚Üê 50
2 if c > gray then
a ‚Üê x
3
4 end
5 else if c < gray then
6
7 end
8 else
9

a ‚Üê ‚àíx

a ‚Üê 0

// Turn right

// Turn left

// Go straight

10 end
11 return a

Figure 7: Driving quality metrics in a lane-keeping system log

the vehicle stays in the lane center thresholds

2) total steady-state duration (cid:80)sc

i=1 sdi:

indicating how

long the vehicle stays in the lane center thresholds
3) number of overshooting oc: indicating how many times
the vehicle overshoots the upper threshold of the lane
center

4) sum of overshooting amplitudes (cid:80)oc
how much the vehicle overshoots
5) total overshooting duration (cid:80)oc
long the vehicle overshoots

i=1 odi:

i=1 oai:

indicating

indicating how

Figure 6: DiÔ¨Äerent interactions between the CPS and its real envi-
ronment depending on diÔ¨Äerent CPS controller versions

and the real environment varies depending on the value of
x.

Based on Algorithm 3, we implement Ô¨Åve diÔ¨Äerent CPS
controllers to cover the three use cases (i.e., TOVK, TMVK,
and TMVU) described in Section 5.3. SpeciÔ¨Åcally, we fol-
low an evolutionary development [24] scenario where (1)
a CPS controller with x = 30‚ó¶ is developed, and its goal
achievement is veriÔ¨Åed Ô¨Årst (i.e., TOVK), (2) two versions
with x = 10‚ó¶, and x = 50‚ó¶ are additionally developed,
and an environment model is generated using FOT logs of
x = 10‚ó¶, x = 30‚ó¶, and x = 50‚ó¶ together and is used to
verify each developed version (i.e., TMVK), and (3) two
more versions with x = 20‚ó¶ and x = 40‚ó¶ are additionally
developed, and their goal achievements are veriÔ¨Åed using
the previously generated virtual environment model with-
out using any FOT logs for x = 20‚ó¶ and x = 40‚ó¶ (i.e.,
TMVU).

To assess the goal achievement of the diÔ¨Äerent CPS
controllers (i.e., how smoothly the vehicle drives following
the lane), we deÔ¨Åne multiple driving performance metrics
by investigating driving traces collected during our prelim-
inary experiments [26, 29]. SpeciÔ¨Åcally, given time length
T , eight driving quality metrics are deÔ¨Åned as follows (vi-
sualized in Figure 7):
1) number of steady-state sc: indicating how many times

6) number of undershooting uc: indicating how many times
the vehicle undershoots the lower threshold of the lane
center

7) sum of undershooting amplitudes (cid:80)uc
how much the vehicle undershoots
8) total undershooting duration (cid:80)uc
long the vehicle undershoots

i=1 uai: indicating

i=1 udi: indicating how

It is straightforward that the smaller the metrics about
the overshooting and undershooting (i.e., metric 3‚Äì8), the
better. In addition, if the vehicle does not deviate from
the lane center, the steady-state continues uninterrupted,
and its duration becomes T . Therefore, at the ideal case
(e.g., driving exactly on the lane center), the Ô¨Årst metric
sc = 1, the second metric (cid:80)sc
i=1 sdi = T , and the other
metrics are all 0.

6.2. ENVI Experimental Setup

As described in Section 5, the CPS goal veriÔ¨Åcation
using ENVI follows three main stages: (1) FOT log collec-
tion, (2) environment model generation, and (3) simulation-
based CPS goal veriÔ¨Åcation. In the following subsections,
we explain our experimental setup for each stage in detail.

6.2.1. FOT Log Collection

For each of the Ô¨Åve CPS controller versions, we conduct
30 FOTs of the simpliÔ¨Åed autonomous vehicle and collect
30 logs to capture how the vehicle interacts with the real
environment. At each time t, the following information
is recorded in the logs: (1) a lane color value ct as an
environmental state observed by the vehicle‚Äôs color sensor
and (2) a steering angle at as a CPS action decided by the
vehicle‚Äôs controller. Therefore, an FOT log is a sequence of

9

       7 L P H                 / D Q H  F R O R Ux=10¬∞x=20¬∞x=30¬∞x=40¬∞x=50¬∞Table 1: Hyperparameter values for IL algorithms

Table 3: Discriminator structure

Algorithm Hyperparameter

BC

Epoch
Learning rate

GAIL

Epoch
Model learning rate
PPO num. policy iteration
Discriminator learning rate
PPO num. discriminator iteration
PPO reward discount Œ≥
PPO GAE parameter Œª
PPO clipping (cid:15)

Value

300
0.00005

300
0.00005
10
0.01
10
0.99
0.95
0.2

Table 2: Environment model structure

# layer

# output units

input
fully connected layer
tanh
fully connected layer
tanh
fully connected layer
tanh

1
2
3
4
5
6

20
256
256
256
256
1
1

state-action pairs (cid:104)(c0, a0), (c1, a1), . . . , (cT , aT )(cid:105) where T
is the FOT duration. According to the vehicle‚Äôs hardware
spec, it records 25 state-action pairs per one second. Since
a sequence of 25 state-action pairs is enough to observe
the behavior of a CPS controller, we set T to 25 (i.e., one
FOT log is collected by one second).

6.2.2. Environment Model Generation

To investigate the impact of using diÔ¨Äerent IL algo-
rithms, we generate diÔ¨Äerent environment models using
BC, GAIL, and BCxGAIL. We implement the algorithms
in PyTorch [30]. BC uses the ADAM optimizer [31] to
update environment models. Since GAIL needs a pol-
icy gradient algorithm to update models [15], we use a
state-of-the-art Proximal Policy Optimization (PPO) al-
gorithm [32]. As for the hyperparameters of the IL algo-
rithms, we best use default values from the original pa-
pers [15, 32]. Table 1 shows the hyperparameter values
used in our evaluation.

As for the model structure, we set the length of history
l as 10, meaning that the input of a virtual environment
model is a 20-dimensional vector (i.e., a sequence of 10
state-action pairs). We use a simple design for hidden lay-
ers for both the virtual environment model and discrimi-
nator. Tables 2 and 3 summarize the structures of virtual
environment model and discriminator, respectively.

As for the model training, to better understand the
training data eÔ¨Éciency of each of the IL algorithms, we
vary the number of FOT logs to be used and compare
the resulting models. SpeciÔ¨Åcally, among the 30 FOT logs
collected in Section 6.2.1, we randomly select n logs for
training a virtual environment model and vary n from 3
to 30 in steps of 3. For all models, normalized state and

10

# layer

# output units

input
fully connected layer
ReLU
fully connected layer
ReLU
fully connected layer
Sigmoid

1
2
3
4
5
6

21
256
256
256
256
1
1

action values (ranging between ‚àí1 and +1) recorded in
the FOT logs are used for training.

6.2.3. Simulation-based CPS Goal VeriÔ¨Åcation

To verify the CPS goal achievement, each of the Ô¨Åve
CPS controller versions is simulated multiple times with
the environment models generated by ENVI, and the sim-
ulation logs are used to assess the degree of CPS goal
achievement in terms of the eight driving performance
metrics deÔ¨Åned in Section 6.1.

6.3. CPS Goal VeriÔ¨Åcation Accuracy

To evaluate how accurate the simulation-based veriÔ¨Å-
cation using ENVI is with respect to the FOT-based veriÔ¨Å-
cation using enough FOTs for a set of veriÔ¨Åcation require-
ments, we measure the similarity between the FOT-based
veriÔ¨Åcation and simulation-based veriÔ¨Åcation results. Specif-
ically, for a set of CPS goals (requirements) Œ¶, the CPS
goal veriÔ¨Åcation accuracy acc of a virtual environment
model Œ¥v for the CPS (controller) œÄx is deÔ¨Åned as

acc(Œ¥v, œÄx) = 1 ‚àí

Œ£œÜ‚ààŒ¶|œà(Mx,r, œÜ) ‚àí œà(Mx,v, œÜ)|
|Œ¶|

where Mx,r = (S, A, œÄx, Œ¥r, s0) represents the interaction
between the CPS under veriÔ¨Åcation (represented by œÄx)
and its real environment and Mx,v = (S, A, œÄx, Œ¥v, œÉ0) rep-
resents the interaction between the same CPS and Œ¥v. As
for the set of requirements Œ¶, we consider eight CPS goals
(requirements) œÜ1, œÜ2, . . . , œÜ8 based on the eight driving
performance metrics deÔ¨Åned Section 6.1. Since individual
requirements œÜ ‚àà Œ¶ have diÔ¨Äerent ranges, we normalize
œà(¬∑, œÜ) to a value between 0 and 1 using the possible min-
imum and maximum values. As a result, acc(Œ¥v) ranges
between 0 and 1; the higher its value, the more accurate
the virtual environment model.

To compute œà(Mr, œÜ) for œÜ ‚àà Œ¶, we perform 100 FOTs
using our autonomous robot vehicle and collect the FOT
logs. Note that these logs are for evaluating the simulation-
based veriÔ¨Åcation accuracy, and therefore diÔ¨Äerent from
the 30 FOT logs used for training virtual environment
models described in Section 6.2.

To compute œà(Mv, œÜ) for œÜ ‚àà Œ¶, we perform 100 sim-
ulations using a virtual environment model Œ¥v generated
by ENVI. To make the FOT-based veriÔ¨Åcation and the
simulation-based veriÔ¨Åcation compatible, the same initials

œÉ0 must be used. To achieve this, we provide Œ¥v with the
initial ten pairs of states and actions of each of the 100
FOT logs as œÉ0 for each simulation.

Notice that œÉ0 is the only real data given to Œ¥v to com-
pute œà(Mv, œÜ). From Œ¥v‚Äôs point of view, the CPS under
veriÔ¨Åcation œÄx is black-box and the value of its conÔ¨Ågu-
ration parameter x is unknown to Œ¥v, meaning that Œ¥v
predicts how the real environment continuously interacts
with a black-box œÄx given œÉ0 only.

To account for the randomness in measuring œà(Mr, œÜ)
and œà(Mv, œÜ), we repeat the experiment 30 times and re-
port the average.

6.4. Comparison Baseline

It is ideal to compare ENVI with other existing envi-
ronment model generation approaches. However, to the
best of our knowledge, there is no such approach. There-
fore, we make a random environment model for an alterna-
tive comparison baseline. The random environment model
changes the environmental state randomly regardless of
CPS actions. As a result, in addition to the three IL al-
gorithms, we use four diÔ¨Äerent virtual environment model
generation approaches (BC, GAIL, BCxGAIL, and Ran-
dom) and compare them in terms of acc.

6.5. Experiment Results

6.5.1. RQ1: TOVK Use Case

RQ1 aims to evaluate whether ENVI can generate a
virtual environment model for the CPS goal veriÔ¨Åcation
of a single CPS controller version. To answer RQ1, we
generate a virtual environment model Œ¥v,30‚ó¶ using the FOT
logs of the controller version with x = 30‚ó¶ and measure
acc(Œ¥v,30‚ó¶ , œÄ30‚ó¶ ) where œÄ30‚ó¶ indicates the CPS controller
version with x = 30‚ó¶.

Before we investigate acc(Œ¥v,30‚ó¶ , œÄ30‚ó¶ ) with diÔ¨Äerent
conÔ¨Ågurations (i.e., the diÔ¨Äerent model generation algo-
rithms and the diÔ¨Äerent numbers of FOT logs used for
training), we Ô¨Årst visualize the behaviors of the real and
virtual environments when interacting with the CPS. Fig-
ure 8 shows the behaviors of real (red) and virtual (blue)
environments in terms of the environmental states (y-axis)
generated by the continuous interaction with the lane-
keeping system over time (x-axis), when the number of
FOT logs used for training is 3 (Figure 8(a)), 15 (Fig-
ure 8(b)), and 30 (Figure 8(c)). When compared to ran-
dom, we can see that BC, GAIL, and BCxGAIL gener-
ate virtual environment models that can closely mimic the
real environment. Considering the fact that a slight diÔ¨Äer-
ence between the virtual and real lane colors at a moment
can be accumulated over time due to the closed-loop in-
teraction between the virtual environment model and the
lane-keeping system, the visualization shows that all the
IL algorithms can learn how the real environment inter-
acts with the lane-keeping system over time without sig-
niÔ¨Åcant errors. Moreover, for each of the IL algorithms,
the more FOT logs are used, the closer the virtual and real

11

Table 4: VeriÔ¨Åcation accuracy results for the TOVK use case. The
best accuracy for each number of FOT logs is highlighted in bold.

Algorithm Logs

acc(Œ¥v,30‚ó¶ , œÄ30‚ó¶ )

Random

-

BCxGAIL

GAIL

BC

3
15
30

3
15
30

3
15
30

82.47%

98.69%
99.20%
99.30%

96.15%
97.62%
97.83%

97.53%
97.27%
97.49%

environment models‚Äô behaviors are. Given the promising
visualization results, we continue to investigate the CPS
goal veriÔ¨Åcation accuracy below.

Figure 9 shows how the CPS goal veriÔ¨Åcation accuracy
varies depending on the number of FOT logs used for train-
ing the virtual environment model when diÔ¨Äerent model
generation algorithms are used. Table 4 additionally pro-
vides the accuracy values for representative cases (i.e.,
when the number of FOT logs is 3, 15, and 30). Overall,
due to the characteristics of the eight driving performance
metrics and their normalizations, the random approach‚Äôs
veriÔ¨Åcation accuracy is around 82.5%. Nevertheless, all
environment models generated by ENVI achieve higher
than 96% veriÔ¨Åcation accuracy, which is much higher than
that of the random approach. This means that the IL al-
gorithms used in ENVI are signiÔ¨Åcantly better than the
random baseline in terms of generating accurate virtual
environment models. Regarding the training data eÔ¨É-
ciency, the veriÔ¨Åcation accuracy only slightly increases as
the number of used FOT logs increases,
implying that
ENVI can generate an accurate environment model using
even a very small number of FOT logs (e.g., three). Com-
paring the IL algorithms, we can clearly see that BCxGAIL
outperforms the others regardless of the number of used
FOT logs. This is because the convergence speed and data
eÔ¨Éciency of the model training have been complemented
by using BC and GAIL algorithms together, as explained
in Section 5.2. This suggests that engineers can expect
the highest model accuracy in this use case through the
BCxGAIL algorithm.

The answer to RQ1 is that ENVI can generate an
accurate virtual environment model that can re-
place the real environment in FOTs using only a
small number of FOT logs. Among the three IL
algorithms used in ENVI, BCxGAIL outperforms
the others in terms of the CPS goal veriÔ¨Åcation ac-
curacy.

Figure 8: Comparison of real (i.e., FOT) and simulation log data

6.5.2. RQ2: TMVK Use Case

RQ2 aims to evaluate whether ENVI can generate a
virtual environment model for the CPS goal veriÔ¨Åcation
of multiple CPS controller versions. To answer RQ2, we
measure the veriÔ¨Åcation accuracy of the same virtual envi-
ronment model for diÔ¨Äerent lane-keeping system controller
versions. SpeciÔ¨Åcally, we Ô¨Årst train Œ¥v,10‚ó¶|30‚ó¶|50‚ó¶ using the
FOT logs of three lane-keeping system controller versions
(i.e., x = 10‚ó¶, x = 30‚ó¶, and x = 50‚ó¶) and then assess
acc(Œ¥v,10‚ó¶|30‚ó¶|50‚ó¶ , œÄx) for each x ‚àà {10‚ó¶, 30‚ó¶, 50‚ó¶}.

Figure 10 shows the veriÔ¨Åcation accuracy results de-
pending on the number of training FOT logs for the three
diÔ¨Äerent controller versions. Table 5 provides the accuracy
values when the number of FOT logs is 3, 15, and 30. In
the table, the best accuracy for each controller version and
the number of FOT logs is highlighted in bold. Overall,
the virtual environment models generated by ENVI using
the FOT logs of multiple controller versions achieve much
higher veriÔ¨Åcation accuracy (at least 95%) than the ran-
dom model, even when only a few FOT logs are used for
training them. Considering the diÔ¨Äerent interaction pat-
terns for diÔ¨Äerent CPS controller versions as shown in Fig-
ure 6, the high veriÔ¨Åcation accuracy indicates that, even
with small FOT logs, the IL algorithms can learn how the

12

Figure 9: VeriÔ¨Åcation accuracy of environment model generation ap-
proaches for TOVK use case

                  1 X P E H U  R I  ) 2 7  O R J V                                  9 H U L I L F D W L R Q  D F F X U D F \     ) 2 7 5 D Q G R P % &  [  * $ , / * $ , / % &Table 5: VeriÔ¨Åcation accuracy results for the TMVK use case. The
best accuracy for each number of FOT logs and for each controller
is highlighted in bold.

Algorithm Logs

acc(Œ¥v,10‚ó¶|30‚ó¶|50‚ó¶ , œÄx)
x = 30‚ó¶

x = 50‚ó¶

x = 10‚ó¶

Avg.

Random

BCxGAIL

GAIL

BC

-

3
15
30

3
15
30

3
15
30

69.63%

82.04%

80.96%

77.54%

97.38% 98.78% 98.54% 98.23%
96.95% 99.20% 99.24% 98.46%
97.47% 99.33% 99.34% 98.71%

96.16%
98.21%
98.52%

95.77%
96.14%
97.17%

96.64%
98.32%
98.59%

97.74%
97.91%
98.15%

97.25%
97.92%
98.80%

97.80%
97.85%
97.71%

96.68%
98.15%
98.63%

97.10%
97.30%
97.67%

real environment interacts with diÔ¨Äerent CPS controller
versions and generate a single virtual environment model
that covers all the diÔ¨Äerent interaction patterns. Com-
paring the IL algorithms, BCxGAIL generates the most
accurate environment models using the same number of
logs than the other algorithms in general, whereas GAIL
sometimes outperforms BCxGAIL when x = 10‚ó¶ and BC
never outperforms the others. This is because GAIL can
infer an accurate environment model with small training
data (e.g., less than 30) better than BC, as already demon-
strated by Ho and Ermon [15].

The answer to RQ2 is that ENVI can generate an
accurate virtual environment model that can be
shared in the CPS goal veriÔ¨Åcation of diÔ¨Äerent CPS
controller versions. Among the IL algorithms used
in ENVI, BCxGAIL generally outperforms the oth-
ers in terms of the CPS goal veriÔ¨Åcation accuracy.

6.5.3. RQ3: TMVU Use Case

RQ3 aims to evaluate whether ENVI can generate a
virtual environment model for the CPS goal veriÔ¨Åcation of
a new controller version that has never been used for train-
ing the model. To answer RQ3, we measure the veriÔ¨Åcation
accuracy of the virtual environment models generated by
multiple controller versions for a new controller version.
SpeciÔ¨Åcally, we Ô¨Årst generate Œ¥v,10‚ó¶|30‚ó¶|50‚ó¶ as the same as
RQ2 and then assess acc(Œ¥v,10‚ó¶|30‚ó¶|50‚ó¶ , œÄx) for each new
x ‚àà {20‚ó¶, 40‚ó¶}.

Similar to RQ2, Figure 11 and Table 6 show the veriÔ¨Å-
cation accuracy results. In all cases, Œ¥v,10‚ó¶|30‚ó¶|50‚ó¶ achieves
more than 96% accuracy, which is much higher than ran-
dom. This means that the virtual environment model gen-
erated using the FOT logs of the previously developed CPS
controller versions (x = 10‚ó¶, x = 30‚ó¶, and x = 50‚ó¶) can
be used for the CPS goal veriÔ¨Åcation for newly developed
versions (x = 20‚ó¶ and x = 40‚ó¶) with high accuracy. This
implies that the virtual environment model can learn inter-
action patterns between the real environment and diÔ¨Äerent

(a) Controller under veriÔ¨Åcation x = 10‚ó¶

(b) Controller under veriÔ¨Åcation x = 30‚ó¶

(c) Controller under veriÔ¨Åcation x = 50‚ó¶

Figure 10: VeriÔ¨Åcation accuracy of environment model generation
approaches for TMVK use case

13

                  1 X P E H U  R I  ) 2 7  O R J V                9 H U L I L F D W L R Q  D F F X U D F \     ) 2 7 5 D Q G R P % &  [  * $ , / * $ , / % &                  1 X P E H U  R I  ) 2 7  O R J V                9 H U L I L F D W L R Q  D F F X U D F \     ) 2 7 5 D Q G R P % &  [  * $ , / * $ , / % &                  1 X P E H U  R I  ) 2 7  O R J V                9 H U L I L F D W L R Q  D F F X U D F \     ) 2 7 5 D Q G R P % &  [  * $ , / * $ , / % &Table 6: VeriÔ¨Åcation accuracy results for the TMVU use case. The
best accuracy for each number of FOT logs and for each controller
is highlighted in bold.

Algorithm Logs

Random

BCxGAIL

GAIL

BC

-

3
15
30

3
15
30

3
15
30

acc(Œ¥v,10‚ó¶|30‚ó¶|50‚ó¶ , œÄx)
x = 40‚ó¶
x = 20‚ó¶

avg.

79.63%

81.60%

80.61%

98.44%
97.47%
97.51%

97.03%
98.94%
99.21%

97.09%
96.62%
96.71%

98.76% 98.60%
98.40%
99.32%
98.52%
99.53%

96.85%
96.94%
98.18% 98.56%
98.73% 98.97%

97.79%
98.18%
98.03%

97.44%
97.40%
97.37%

The answer to RQ3 is that ENVI can generate
an accurate virtual environment model to verify
unknown controller versions that have never been
Ô¨Åeld-tested for training. Regarding the IL algo-
rithms, BCxGAIL and GAIL outperform BC in all
cases.

6.6. Threats to Validity

In terms of external validity, our case study focused
on only a lane-keeping system in a simpliÔ¨Åed CPS im-
plemented as a LEGO-lized autonomous vehicle and used
only one parameter (i.e., the degree of rotations x) for
representing diÔ¨Äerent versions of software controllers. Al-
though the subject CPS of our case study may diÔ¨Äer from
the real CPS (e.g., autonomous vehicle), our simpliÔ¨Åed
CPS represents a CPS in practice in terms of continuous
interaction with the environment and distinction between
diÔ¨Äerent controller versions by multiple parameter values.
Applying ENVI to more complex CPS could show diÔ¨Äerent
results, but the applicability of ENVI for various use cases
(i.e., TOVK, TMVK, and TMVU) shown in this paper is
still valid for CPSs with such characteristics. However, ad-
ditional case studies with more complex CPS are required
to improve our results‚Äô generalizability.

In terms of internal validity, the goal achievement mea-
sure deÔ¨Åned based on speciÔ¨Åc driving quality metrics could
be a potential threat since the evaluation of the lane-
keeping system‚Äôs goal could be biased to a speciÔ¨Åc aspect of
driving. To mitigate this threat, in our case study, we de-
Ô¨Åned eight driving qualities from the FOT logs motivated
by Cherrett and PitÔ¨Åeld [29] and aggregated the results on
the qualities to comprehensively understand whether the
lane-keeping system under analysis works well or not. Hy-
perparameter value settings for machine learning models
(e.g., number of iterations, learning rates, Etc.) could be
another potential threat to the internal validity since the
performance of machine learning models can largely de-
pend on hyperparameter values [33, 34]. We used the de-

14

(a) Controller under veriÔ¨Åcation x = 20‚ó¶

(b) Controller under veriÔ¨Åcation x = 40‚ó¶

Figure 11: VeriÔ¨Åcation accuracy of environment model generation
approaches for TMVU use case.

CPS controller versions and generalize the patterns to un-
known CPS controller versions. Therefore, only simulat-
ing the new CPS controller versions many times, without
much FOT, is required for the CPS goal veriÔ¨Åcation of
the new versions if an accurate virtual environment model
has been created in the TMVK use case. This can signiÔ¨Å-
cantly reduce the cost of the CPS goal veriÔ¨Åcation in prac-
tice. Regarding the IL algorithms, GAIL and BCxGAIL
outperform BC as the same as in RQ2. This implies that
GAIL and BCxGAIL are recommended for the IL algo-
rithm when ENVI is used for the CPS goal veriÔ¨Åcation of
new CPS versions.

                  1 X P E H U  R I  ) 2 7  O R J V            9 H U L I L F D W L R Q  D F F X U D F \     ) 2 7 5 D Q G R P % &  [  * $ , / * $ , / % &                  1 X P E H U  R I  ) 2 7  O R J V            9 H U L I L F D W L R Q  D F F X U D F \     ) 2 7 5 D Q G R P % &  [  * $ , / * $ , / % &fault values provided in the original studies [15, 32]. Nev-
ertheless, hyperparameter tuning is an important research
Ô¨Åeld, so it remains an interesting future work.
In addi-
tion, the veriÔ¨Åcation accuracy evaluation results could be
aÔ¨Äected by the simulation duration T because small errors
of the environment model can be accumulated and cause
signiÔ¨Åcant errors in a long simulation, as mentioned in Sec-
tion 4. However, in our case study, we could not see the
problem in all ENVI algorithms even when T is ten times
longer than the current setting in this paper. Nevertheless,
analyzing the performances on mitigating the compound-
ing error of various IL algorithms for ENVI in diÔ¨Äerent
systems remains an interesting future work.

7. Discussion

IL algorithm selection: In this paper, we considered
three representative IL algorithms (BC, GAIL, and BCx-
GAIL) for the environment model generation. In practice,
a speciÔ¨Åc IL algorithm should be selected when implement-
ing ENVI considering its characteristics, as described in
Section 3. Based on our case study results, we recommend
engineers use the BCxGAIL algorithm in practice since
the environment models generated by BCxGAIL were the
most accurate in terms of CPS goal veriÔ¨Åcation. However,
there are other factors for the IL algorithm selection, such
as learning speed or sensitivity to hyperparameters, and
therefore providing more empirical guidelines for selecting
a speciÔ¨Åc IL algorithm still remains an interesting future
work.

Knowledge-based approach vs. Data-driven ap-
proach: When there is a high-Ô¨Ådelity simulation engine
based on well-known principles in the CPS domain, engi-
neers can manually create an accurate virtual environment
in the simulator for CPS goal veriÔ¨Åcation. In contrast to
such knowledge-based environment modeling, ENVI is a
data-driven approach where only a few FOT logs are re-
quired to automatically generate an accurate virtual en-
vironment model. This is a huge advantage when there
are no high-Ô¨Ådelity simulators or well-deÔ¨Åned principles in
the CPS domain. Therefore, the data-driven approach can
complement the knowledge-based approach depending on
the application domain.

Open challenges: Though we successfully developed
and evaluated ENVI, there are three main open challenges
for data-driven environment model generation approaches.
First, sample eÔ¨Éciency is essential. This is because
conducting FOTs to collect logs is the most expensive task
in the data-driven approach. In our case study, BCxGAIL
that combines BC and GAIL to improve sample eÔ¨Éciency
indeed outperforms the other IL algorithms in most cases.
Using state-of-the-art techniques for increasing sample ef-
Ô¨Åciency [17, 35, 36] could further help.

Second, it should be robust to noise in FOT logs. We
utilized IL techniques, and many IL studies assume the
correctness of the expert demonstration [37‚Äì39]. However,

the demonstrator for IL algorithms in the data-driven en-
vironment model generation is the real environment, and
therefore some level of noise can appear (e.g., due to sen-
sor noise. In our case study, though we used noisy data
collected by the real CPS, systematically investigating the
impact of noise was not in the scope of our work. Never-
theless, as many studies have already considered the noise
issue in machine learning [40‚Äì42], they could better guide
how to address noisy data.

Third, Ô¨Ånding a proper level of abstraction for complex
environmental behaviors is important. We abstracted the
environment as a state-transition function in a closed-loop
simulation and recast the environment model generation
problem as the IL problem (see Section 4). This is a typical
level of abstraction when an environment is modeled [43‚Äì
45]. However, this simple representation may not be suf-
Ô¨Åcient to model complex environmental behaviors, such
as structural changes in the environment during the FOT
or responses to factors other than the system. Therefore,
an extension of the CPS-ENV interaction model could be
needed for some domains. It is an interesting future work,
and we can also refer to some IL studies that imitate com-
plex expert behaviors (e.g., multi-task or concurrent be-
havior) [46‚Äì48].

8. Related work

Instead of conducting FOTs, assessing CPS on a sim-
ulation environment is widely used in CPS engineering.
Therefore, many studies have been presented in modeling
CPS environments.

Qin et al. [43] and Reichstaller and Knapp [44] mod-
eled the interaction between the CPS and environment as
a closed-loop similar to our CPS-ENV model. They gener-
ated environmental testing inputs to predict and evaluate
the CPS runtime behavior. Fredericks [49] also speciÔ¨Åed
uncertain situations the CPS may face at runtime, such
as inaccurate or delayed cognition of the environment,
to evaluate the CPS with adverse environmental inputs.
These approaches can generate the initial environmental
inputs that the CPS observes using sensors, but the state
transition of the environment during the simulation should
be manually modeled by domain experts or engineers in an
external simulator.

Some studies model the environment state transition
for CPS simulation similar to our approach. P¬®uschel et al.
[50] modeled the change of the environment state as a pro-
cess model and reconÔ¨Ågured the environment based on the
model during CPS simulation. Yang et al. [51] explicitly
speciÔ¨Åed how the environmental state is changed after CPS
action on a state machine. C¬¥amara et al. [52] and Moreno
et al. [53] also modeled the probabilistic environment state
transition in Markov Decision Process (MDP) and veriÔ¨Åed
the CPS goal achievement in the dynamic environment.
Though they modeled environmental state transition func-
tions, domain experts have to manually design these mod-
els, which require suÔ¨Écient domain knowledge and eÔ¨Äorts.

15

There are studies utilizing environmental data to model
the environment. Ding et al. [54] modeled the continuous
environment state transition as a continuous place in an
extension of Petri nets, and the parameters in the model
were learned from data. Aizawa et al. [55] and Sykes et al.
[56] modeled the changing environment as a labeled transi-
tion system (LTS) and a logic program, respectively. The
initial environment models are revised by execution trace
data of the system so that the models represent the chang-
ing environment of reality as accurately as possible. How-
ever, in these studies, the revised environment model is
still highly dependent on the initial models made by ex-
perts because data update only the partial information in
the model.

Unlike the previous studies that modeled the environ-
ment of CPS, we abstract the complex state transition of
the environment into a black box function implemented as
a neural network. As a result, the environment model can
be automatically generated with execution trace samples
of CPS without prior knowledge of the environment.

Independently from CPS, model-based Reinforcement
Learning (RL) uses a notion of the environment model gen-
erally deÔ¨Åned as anything that informs how the RL agent‚Äôs
environment will respond to the agent‚Äôs actions [57]. Though
the concept is similar to our environment model that in-
teracts with the CPS under veriÔ¨Åcation, the purposes of
training (learning) the environment model are diÔ¨Äerent.
The primary objective of the environment model in model-
based RL is to better learn the agent‚Äôs policy function,
so an inaccurate environment model is acceptable as long
as it can promote the policy learning process. Naturally,
supervised learning is used for learning the environment
model [58] without considering possible accumulations of
errors over time. In contrast, the environment model in
ENVI is to replace the real FOT environment for CPS
goal veriÔ¨Åcation, and therefore making the environment
model the same as the real environment in a closed-loop
simulation is our primary objective, which is why we lever-
age Imitation Learning (IL) in our approach.

9. Conclusion

In this paper, we present ENVI, a novel data-driven
environment imitation approach that eÔ¨Éciently generates
accurate virtual environment models for CPS goal veriÔ¨Åca-
tion. Instead of conducting expensive FOTs many times,
ENVI requires only a few FOTs to collect some FOT logs
for training a virtual environment model. By leverag-
ing the representative IL algorithms (i.e., BC, GAIL, and
BCxGAIL), an accurate virtual environment model can
be generated automatically from the collected FOT logs.
Our case study using a LEGO-lized autonomous vehicle
equipped with a lane-keeping system shows that the CPS
goal veriÔ¨Åcation accuracy of the virtual environment mod-
els generated by our approach is very accurate, even when
only a few FOT logs are used for training the models. The

case study also shows that when the same CPS has multi-
ple versions from an evolutionary development process, an
ENVI -generated environment model can be used for the
CPS goal veriÔ¨Åcation of new versions whose FOT logs are
never collected before for the model training.

In future work, we plan to provide practical guidelines
for using ENVI with diÔ¨Äerent IL algorithms by further in-
vestigating the characteristics of individual IL algorithms
and conducting more case studies with complex CPS (e.g.,
an automated driving system composed of machine learn-
ing components). We further expect that ENVI is not
limited to the purpose of CPS controller veriÔ¨Åcation, so
we also plan to suggest new applications of ENVI, such
as an optimal CPS control predicting the environmental
reaction.

Acknowledgements

This research was supported by the MSIT (Ministry
of Science and ICT), Korea, under the ITRC (Informa-
tion Technology Research Center) support program (IITP-
2022-2020-0-01795) and (SW Star Lab) Software R&D for
Model-based Analysis and VeriÔ¨Åcation of Higher-order Large
Complex System (No. 2015-0-00250) supervised by the
IITP (Institute of Information & Communications Tech-
nology Planning & Evaluation). This research was also
partially supported by the Basic Science Research Pro-
gram through the National Research Foundation of Korea
(NRF) funded by the Ministry of Education (2019R1A6A-
3A03033444).

References

[1] R. Baheti, H. Gill, Cyber-physical systems, The impact of

control technology 12 (2011) 161‚Äì166.

[2] D. An, J. Liu, M. Zhang, X. Chen, M. Chen, H. Sun, Uncer-
tainty modeling and runtime veriÔ¨Åcation for autonomous vehi-
cles driving control: A machine learning-based approach, Jour-
nal of Systems and Software 167 (2020) 110617.

[3] G. E. Mullins, P. G. Stankiewicz, R. C. Hawthorne, S. K. Gupta,
Adaptive generation of challenging scenarios for testing and
evaluation of autonomous vehicles, Journal of Systems and Soft-
ware 137 (2018) 197‚Äì215.

[4] D. Bozhinoski, D. Di Ruscio, I. Malavolta, P. Pelliccione,
I. Crnkovic, Safety for mobile robotic systems: A systematic
mapping study from a software engineering perspective, Jour-
nal of Systems and Software 151 (2019) 150‚Äì179.

[5] A. Ahmad, M. A. Babar, Software architectures for robotic
systems: A systematic mapping study, Journal of Systems and
Software 122 (2016) 16‚Äì39.

[6] Y.-R. Shiue, K.-C. Lee, C.-T. Su, Real-time scheduling for a
smart factory using a reinforcement learning approach, Com-
puters & Industrial Engineering 125 (2018) 604‚Äì614.

[7] W. Wang, Y. Zhang, J. Gu, J. Wang, A proactive manufactur-
ing resources assignment method based on production perfor-
mance prediction for the smart factory, IEEE Transactions on
Industrial Informatics 18 (2022) 46‚Äì55.

[8] M. Zema, S. Rosati, V. Gioia, M. KnaÔ¨Çitz, G. Balestra, Devel-
oping medical device software in compliance with regulations,
in: 2015 37th Annual International Conference of the IEEE En-
gineering in Medicine and Biology Society (EMBC), 2015, pp.
1331‚Äì1334. doi:10.1109/EMBC.2015.7318614.

16

[9] K. Fu, Trustworthy medical device software, Public Health

doi:10.1007/978-3-642-28305-5_8.

EÔ¨Äectiveness of the FDA 510 (2011) 102.

[10] A. Hussein, M. M. Gaber, E. Elyan, C. Jayne, Imitation learn-
ing: A survey of learning methods, ACM Comput. Surv. 50
(2017).

[11] O. Michel, Cyberbotics ltd. webots‚Ñ¢: Professional mobile robot
simulation, International Journal of Advanced Robotic Systems
1 (2004) 5.

[12] N. Koenig, A. Howard, Design and use paradigms for gazebo, an
open-source multi-robot simulator,
in: 2004 IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems (IROS)
(IEEE Cat. No.04CH37566), volume 3, 2004, pp. 2149‚Äì2154
vol.3. doi:10.1109/IROS.2004.1389727.

Learning from demonstration,

[13] S. Schaal,
vances
pp.
1224-learning-from-demonstration.

Ad-
Information Processing Systems, 1996,
URL:
http://papers.nips.cc/paper/

in Neural
1040‚Äì1046.

in:

[14] B. D. Argall, S. Chernova, M. Veloso, B. Browning, A survey of
robot learning from demonstration, Robotics and Autonomous
Systems 57 (2009) 469‚Äì483.

[15] J. Ho, S. Ermon, Generative adversarial imitation learning,
in: Proceedings of the 30th International Conference on Neural
Information Processing Systems, NIPS‚Äô16, Curran Associates
Inc., Red Hook, NY, USA, 2016, p. 4572‚Äì4580.

[16] J. Ho, S. Ermon, Generative adversarial imitation learning,
in: Proceedings of the 30th International Conference on Neural
Information Processing Systems, NIPS‚Äô16, Curran Associates
Inc., Red Hook, NY, USA, 2016, p. 4572‚Äì4580.

[17] R. Jena, C. Liu, K. Sycara, Augmenting gail with bc for sample

eÔ¨Écient imitation learning, arXiv (2020).

[18] R. S. Sutton, A. G. Barto, et al., Introduction to reinforcement

learning, volume 135, MIT press Cambridge, 1998.

[19] L. D. Xu, L. Duan, Big data for cyber physical systems in in-
dustry 4.0: a survey, Enterprise Information Systems 13 (2019)
148‚Äì169.

[20] M. T. Hagan, H. B. Demuth, M. Beale, Neural network design,

PWS Publishing Co., 1997.

[21] M. RaÔ¨Åq, G. Bugmann, D. Easterbrook, Neural network design
for engineering applications, Computers & Structures 79 (2001)
1541‚Äì1552.

[22] A. Schilling, C. Metzner, J. Rietsch, R. Gerum, H. Schulze,
P. Krauss, How deep is deep enough? ‚Äì quantifying class sep-
arability in the hidden layers of deep neural networks, arXiv
(2019).

[23] A. Basden, I. Watson, P. Brandon, The evolutionary develop-
ment of expert systems, in: Research & Development In Expert
Systems Vlll, Cambridge University Press, 1991, pp. 67‚Äì81.
[24] R. Helps, F. N. Mensah, Comprehensive design of cyber physical
systems,
in: Proceedings of the 13th Annual Conference on
Information Technology Education, SIGITE ‚Äô12, Association for
Computing Machinery, New York, NY, USA, 2012, p. 233‚Äì238.
doi:10.1145/2380552.2380618.

[25] M. Sirjani, L. Provenzano, S. A. Asadollah, M. H. Moghadam,
M. Saadatmand, Towards a veriÔ¨Åcation-driven iterative devel-
opment of software for safety-critical cyber-physical systems,
Journal of Internet Services and Applications 12 (2021) 1‚Äì29.

[26] Y.-J. Shin, L. Liu, S. Hyun, D.-H. Bae, Platooning legos:
An open physical exemplar for engineering self-adaptive cyber-
physical systems-of-systems, in: 2021 International Symposium
on Software Engineering for Adaptive and Self-Managing Sys-
tems (SEAMS), 2021, pp. 231‚Äì237. doi:10.1109/SEAMS51251.
2021.00038.

[27] Y. Tao, Z. Bin, A novel self-tuning cps controller based on
in: 2008 IEEE Power and Energy Society
q-learning method,
General Meeting - Conversion and Delivery of Electrical En-
ergy in the 21st Century, 2008, pp. 1‚Äì6. doi:10.1109/PES.2008.
4596654.

[28] R.-C. David, R.-E. Precup, S. Preitl, J. K. Tar, J. Fodor,
Three evolutionary optimization algorithms in pi controller
tuning,
in: Applied Computational Intelligence in Engineer-
ing and Information Technology, Springer, 2012, pp. 95‚Äì106.

17

[29] T. Cherrett, D. PitÔ¨Åeld, Extracting driving characteristics from
heavy goods vehicle tachograph charts, Transportation Plan-
ning and Technology 24 (2001) 349‚Äì363.

[30] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Des-
maison, A. K¬®opf, E. Yang, Z. DeVito, M. Raison, A. Tejani,
S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala, Py-
Torch: An Imperative Style, High-Performance Deep Learning
Library, Curran Associates Inc., Red Hook, NY, USA, 2019, pp.
8026‚Äì8037.

[31] D. P. Kingma, J. Ba, Adam: A method for stochastic optimiza-

tion, arXiv (2017).

[32] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov,

Proximal policy optimization algorithms, arXiv (2017).

[33] B. Wang, N. Z. Gong, Stealing hyperparameters in machine
in: 2018 IEEE Symposium on Security and Privacy

learning,
(SP), 2018, pp. 36‚Äì52. doi:10.1109/SP.2018.00038.

[34] P. Probst, A.-L. Boulesteix, B. Bischl, Tunability: Importance
of hyperparameters of machine learning algorithms, J. Mach.
Learn. Res. 20 (2019) 1934‚Äì1965.

[35] X. Zhang, Y. Li, Z. Zhang, Z.-L. Zhang,

f-gail: Learn-
ing f-divergence for generative adversarial imitation learning,
in: H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
H. Lin (Eds.), Advances in Neural Information Processing Sys-
tems, volume 33, Curran Associates, Inc., 2020, pp. 12805‚Äì
12815. URL: https://proceedings.neurips.cc/paper/2020/
file/967990de5b3eac7b87d49a13c6834978-Paper.pdf.

[36] Z. W. Robertson, M. R. Walter, Concurrent training improves
the performance of behavioral cloning from observation, arXiv
(2020).

[37] F. Codevilla, M. M¬®uller, A. L¬¥opez, V. Koltun, A. Dosovitskiy,
End-to-end driving via conditional imitation learning, in: 2018
IEEE International Conference on Robotics and Automation
(ICRA), 2018, pp. 4693‚Äì4700. doi:10.1109/ICRA.2018.8460487.
[38] M. Abdou, H. Kamal, S. El-Tantawy, A. Abdelkhalek, O. Adel,
imita-
K. Hamdy, M. Abaas, End-to-end deep conditional
tion learning for autonomous driving,
in: 2019 31st Interna-
tional Conference on Microelectronics (ICM), 2019, pp. 346‚Äì
350. doi:10.1109/ICM48031.2019.9021288.

[39] X. B. Peng, P. Abbeel, S. Levine, M. van de Panne, Deepmimic:
Example-guided deep reinforcement learning of physics-based
character skills, ACM Trans. Graph. 37 (2018).

[40] A. Atla, R. Tada, V. Sheng, N. Singireddy, Sensitivity of diÔ¨Äer-
ent machine learning algorithms to noise, J. Comput. Sci. Coll.
26 (2011) 96‚Äì103.

[41] S. Gupta, A. Gupta, Dealing with noise problem in machine
learning data-sets: A systematic review, Procedia Computer
Science 161 (2019) 466‚Äì474. The Fifth Information Systems In-
ternational Conference, 23-24 July 2019, Surabaya, Indonesia.
[42] Z. Zeng, Y. Liu, W. Tang, F. Chen, Noise is useful: Exploiting
data diversity for edge intelligence, IEEE Wireless Communi-
cations Letters 10 (2021) 957‚Äì961.

[43] Y. Qin, C. Xu, P. Yu, J. Lu, Sit: Sampling-based interactive
testing for self-adaptive apps, Journal of Systems and Software
120 (2016) 70‚Äì88.

[44] A. Reichstaller, A. Knapp, Risk-based testing of self-adaptive
systems using run-time predictions, in: 2018 IEEE 12th Interna-
tional Conference on Self-Adaptive and Self-Organizing Systems
(SASO), 2018, pp. 80‚Äì89. doi:10.1109/SASO.2018.00019.
[45] Y.-J. Shin, J.-Y. Bae, D.-H. Bae, Concepts and models of en-
vironment of self-adaptive systems: A systematic literature re-
view,
in: 2021 28th Asia-PaciÔ¨Åc Software Engineering Con-
ference (APSEC), 2021, pp. 296‚Äì305. doi:10.1109/APSEC53868.
2021.00037.

[46] S. Agrawal, M. van de Panne, Task-based locomotion, ACM

Trans. Graph. 35 (2016).

[47] J. Harmer, L. Gissl¬¥en, J. del Val, H. Holst, J. Bergdahl,
Imitation learning with con-
T. Olsson, K. Sj¬®o¬®o, M. Nordin,
current actions in 3d games,
in: 2018 IEEE Conference on
Computational Intelligence and Games (CIG), 2018, pp. 1‚Äì8.

doi:10.1109/CIG.2018.8490398.

[48] A. Singh, E. Jang, A. Irpan, D. Kappler, M. Dalal, S. Levinev,
M. Khansari, C. Finn, Scalable multi-task imitation learning
with autonomous improvement,
in: 2020 IEEE International
Conference on Robotics and Automation (ICRA), 2020, pp.
2167‚Äì2173. doi:10.1109/ICRA40945.2020.9197020.

[49] E. M. Fredericks, Automatically hardening a self-adaptive
system against uncertainty,
in: 2016 IEEE/ACM 11th In-
ternational Symposium on Software Engineering for Adap-
tive and Self-Managing Systems (SEAMS), 2016, pp. 16‚Äì27.
doi:10.1109/SEAMS.2016.010.

[50] G. P¬®uschel, C. Piechnick, S. G¬®otz, C. Seidl, S. Richly,
T. Schlegel, U. A√ümann, A combined simulation and test case
generation strategy for self-adaptive systems, Journal On Ad-
vances in Software 7 (2014) 686‚Äì696.

[51] W. Yang, C. Xu, Y. Liu, C. Cao, X. Ma, J. Lu, Ver-
ifying self-adaptive applications suÔ¨Äering uncertainty,
in:
Proceedings of the 29th ACM/IEEE International Confer-
ence on Automated Software Engineering, ASE ‚Äô14, Associa-
tion for Computing Machinery, New York, NY, USA, 2014,
p. 199‚Äì210. URL: https://doi.org/10.1145/2642937.2642999.
doi:10.1145/2642937.2642999.

[52] J. C¬¥amara, W. Peng, D. Garlan, B. Schmerl, Reasoning about
sensing uncertainty in decision-making for self-adaptation,
in:
A. Cerone, M. Roveri (Eds.), Software Engineering and Formal
Methods, Springer International Publishing, Cham, 2018, pp.
523‚Äì540.

[53] G. A. Moreno, J. C¬¥amara, D. Garlan, B. Schmerl, Flexible
and eÔ¨Écient decision-making for proactive latency-aware self-
adaptation, ACM Trans. Auton. Adapt. Syst. 13 (2018).
[54] Z. Ding, Y. Zhou, M. Zhou, Modeling self-adaptive software sys-
tems with learning petri nets, IEEE Transactions on Systems,
Man, and Cybernetics: Systems 46 (2016) 483‚Äì498.

[55] K. Aizawa, K. Tei, S. Honiden,

Identifying safety properties
guaranteed in changed environment at runtime, in: 2018 IEEE
International Conference on Agents (ICA), 2018, pp. 75‚Äì80.
doi:10.1109/AGENTS.2018.8460083.

[56] D. Sykes, D. Corapi, J. Magee, J. Kramer, A. Russo, K. In-
oue, Learning revised models for planning in adaptive systems,
in: 2013 35th International Conference on Software Engineering
(ICSE), 2013, pp. 63‚Äì71. doi:10.1109/ICSE.2013.6606552.
[57] R. S. Sutton, A. G. Barto, Reinforcement learning: An intro-

duction, MIT press, 2018.

[58] T. M. Moerland, J. Broekens, C. M. Jonker, Model-based rein-

forcement learning: A survey, arXiv (2021).

18

