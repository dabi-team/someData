A Comprehensive Empirical Investigation on Failure Clustering in Parallel Debugging

Yi Songa, Xiaoyuan Xiea,

∗, Quanming Liua, Xihao Zhanga, Xi Wua

aSchool of Computer Science, Wuhan University, China

2
2
0
2

l
u
J

9
1

]
E
S
.
s
c
[

2
v
2
9
9
7
0
.
7
0
2
2
:
v
i
X
r
a

Abstract

The clustering technique has attracted a lot of attention as a promising strategy for parallel debugging in multi-fault scenarios, this
heuristic approach (i.e., failure indexing or fault isolation) enables developers to perform multiple debugging tasks simultaneously
through dividing failed test cases into several disjoint groups. When using statement ranking representation to model failures for
better clustering, several factors inﬂuence clustering eﬀectiveness, including the risk evaluation formula (REF), the number of
faults (NOF), the fault type (FT), and the number of successful test cases paired with one individual failed test case (NSP1F). In
this paper, we present the ﬁrst comprehensive empirical study of how these four factors inﬂuence clustering eﬀectiveness. We
conduct extensive controlled experiments on 1060 faulty versions of 228 simulated faults and 141 real faults, and the results reveal
that: 1) GP19 is highly competitive across all REFs, 2) clustering eﬀectiveness decreases as NOF increases, 3) higher clustering
eﬀectiveness is easier to achieve when a program contains only predicate faults, and 4) clustering eﬀectiveness remains when the
scale of NSP1F is reduced to 20%.

Keywords: failure clustering, fault isolation, multiple-fault, parallel debugging

1. Introduction

Programs often produce unexpected results that deviate from
oracles during software testing, such anomalous behavior indi-
cates that at least one fault resides in the program. However,
locating these faults is generally labor-intensive and tedious in
the debugging process [1, 2]. Generally, in multi-fault scenar-
ios, there are two commonly adopted strategies:

• Sequential debugging.

Ignoring the linkage between
failed test cases and faults, this strategy detects, local-
izes, and ﬁxes one fault, and then reruns the test suite
(TS, which contains all test cases) on the semi-repaired
program under test (PUT) again, iterates these steps until
a failure-free program is delivered.

• Parallel debugging. This strategy ﬁrst mines the link-
age that exists between failed test cases and faults, that
is, divides all failed test cases into several disjoint fault-
focused clusters through clustering techniques (with the
goal of the failed test cases in a cluster to be triggered
by the same root cause, and the failed test cases in diﬀer-
ent clusters to be triggered by diﬀerent root causes), and
combines each fault-focused cluster with all successful
test cases to form several fault-focused TS, ﬁnally assigns
them to diﬀerent developers for parallel localization [3].

Many empirical studies have shown that sequential debug-
ging does not perform well in localizing multiple faults [4–6],
while parallel debugging shows promise in this area. The core

∗Corresponding author.
Email address: xxie@whu.edu.cn (Xiaoyuan Xie)

Preprint submitted to Journal of Systems and Software

of parallel debugging lies in clustering. Only by properly cap-
turing the linkage between failed test cases and faults, as well as
heuristically dividing failed test cases, can a hunk of localiza-
tion task be decomposed into several sub-tasks with high qual-
ity. However, most previous research in terms of parallel de-
bugging concentrated on the localization process after cluster-
ing, with only a few studies investigating the clustering process,
one of the most critical steps that may aﬀect the overall parallel
debugging performance. Several factors may aﬀect the failure
clustering step, but there is a lack of comprehensive empirical
studies investigating these variables.

Therefore, in this paper, we conduct the ﬁrst comprehen-
sive empirical investigation, aiming at the clustering step by
selecting four factors that could inﬂuence clustering eﬀective-
ness: the risk evaluation formula (REF) that represents failed
test cases, the number of faults (NOF) and the fault type (FT)
contained in the program, and the number of successful test
cases paired with one individual failed test case (NSP1F), and
further proposing four research questions as follows to guide
our extensive experiments.

Table 1: Abbreviations and their full forms

Abbreviations
REF
NOF
FT
NSP1F

TS
PUT
AF
PF

Full forms
Risk Evaluation Formula
the Number Of Faults
the Fault Type
the Number of Successful test cases Paired with
ONE Failed test case
Test Suite
Program Under Test
Assignment Fault
Predicate Fault

July 20, 2022

 
 
 
 
 
 
RQ1: Do diﬀerent REFs have the same capability to

•

representing failed test cases?

RQ4: Will clustering eﬀectiveness be reduced using a

•

lower NSP1F?

Failed test cases are typically too unstructured and abstract
to be used directly for clustering. Many approaches, such as
coverage vector representation (CVR) and statement ranking
representation (SRR), have been utilized to convert failed test
cases into structured and mathematical forms. CVR is simi-
lar to T-proximity (Trace-proximity) in [7], in which a vector
with a length equal to the number of executable statements in
PUT is created to represent a failed test case, with the value of
the ith element being set to 1 if this failed test case covers the
ith statement, and 0 otherwise. SRR is similar to R-proximity
(Rank-proximity) in [7], in which one failed test case and suc-
cessful test cases are executed on PUT, and the coverage in-
formation of the program execution is collected and organized
in the form of notations deﬁned in spectrum-based fault local-
ization (SBFL) [8]. The coverage is then input into an REF to
produce a ranking list that reﬂects statements’ suspiciousness,
which is employed to represent this failed test case ﬁnally. SRR
has been proved to be superior to CVR in representing failed
test cases [7], which has also been adopted by a number of pre-
vious research due to its advantage in translating a failed test
case into a clustering-friendly proxy [9–12].

In SRR, REF is used to produce a ranking list that contains
the execution features of a failed test case. Obviously, a bet-
ter REF should extract more discriminative features for failed
test cases caused by diﬀerent root causes, in other words, the
distance between ranking lists that represent failed test cases
triggered by diﬀerent faults should be greater than the distance
between ranking lists that represent failed test cases triggered
by a same fault. However, almost all existing studies only sim-
ply chose a speciﬁc REF to generate the ranking list. To the
best of our knowledge, no research has contrasted the capabil-
ities of various REFs in representing failed test cases. To that
end, we analyze 35 commonly-used REFs through extensive
experiments in this RQ from this perspective.

RQ2: How NOF aﬀects clustering eﬀectiveness?

•
Although it is diﬃcult to know whether a faulty program
contains a single fault or multiple faults exactly, we can intu-
itively infer the more faults it has, the more eﬀort and time the
debugging process will take [5, 13]. Many studies have investi-
gated the eﬀect of NOF on the eﬀectiveness of fault localization
techniques [5, 6, 14], but few have explored the inﬂuence of
NOF on the clustering process. We analyze how clustering ef-
fectiveness changes as NOF grows in 2-bug, 3-bug, 4-bug, and
5-bug scenarios (i.e., programs that contain 2, 3, 4, and 5 bugs,
respectively).

RQ3: Is clustering eﬀectiveness aﬀected by FT?

•
In addition to NOF, FT is also an essential factor in the de-
bugging process. Although the randomness and uncertainty of
the programming process determine the diversity of the intro-
duced faults, the most common FTs typically refer to assign-
ment faults [15] and predicate faults [16].
If a program has
only assignment faults, only predicate faults, or both of them,
how will the clustering eﬀectiveness be aﬀected? We discuss
each of the three scenarios separately.

When using SRR to represent failed test cases, almost all
researchers pair one individual failed test case with all success-
ful test cases [9, 10] without giving any reason or explaining
the rationality behind this strategy.
If one failed test case is
paired with part of rather than all successful test cases, the cost
of debugging will be probably reduced, but will this reduction
harm clustering eﬀectiveness? We contrast the clustering eﬀec-
tiveness in ﬁve scenarios by pairing a failed test case with X
percent of successful test cases (X = 100, 80, 60, 40, 20).

Furthermore, the distance metric, the estimation of the num-
ber of clusters and the assignment of initial medoids, as well as
the clustering algorithm are also critical factors in determining
clustering eﬀectiveness in parallel debugging. Gao and Wong
have proposed a parallel debugging approach, MSeer [12], to
solve the aforementioned concerns. In particular, they revised
the traditional Kendall tau distance [17], presented an inno-
vative strategy to assign initial medoids during predicting the
number of clusters based on the mountain method [18, 19], and
reﬁned the K-medoids clustering algorithm [20]. We will dis-
cuss our four research questions and conduct experiments based
on MSeer due to its innovation and high eﬀectiveness. A further
introduction regarding MSeer is given in Section 2.3.

We create 1060 faulty versions of nine programs, f lex, grep,
gzip, sed, Chart, Closure, Lang, Math, and T ime, as our bench-
mark. The experimental results show that1:

1) GP19 (the 19th formula evolved by Genetic Programming
in [21]) is highly competitive across all REFs when rep-
resenting failed test cases.

2) Clustering eﬀectiveness decreases as NOF grows.
3) Higher clustering eﬀectiveness is easier to achieve when

a faulty program contains only predicate faults.

4) Clustering eﬀectiveness remains when NSP1F is reduced

to 20%.

The main contributions of this paper are as follows:
1) Unlike previous studies that contrasted REFs from the
perspective of fault localization eﬀectiveness, we con-
trast 35 REFs (including the latest Crosstab, Dstar, and
GP02, GP03, GP19 evolved by genetic programming) in
terms of how well they represent failed test cases. We
recommend GP19, an REF with strong competitiveness
in extracting failed test cases’ execution features for fu-
ture researchers.

2) Our controlled experiments reveal that the eﬀectiveness
of clustering failed test cases will reduce when NOF in-
creases.

3) We analyze two typical types of faults, assignment faults

and predicate faults, and discover that it is easier to achieve
higher clustering eﬀectiveness when a program contains
only predicate faults.

4) We pair 100%, 80%, 60%, 40%, and 20% of successful
test cases with one failed test case, and contrast the clus-
tering eﬀectiveness in these ﬁve scenarios. The ﬁndings

1The replication package of this empirical study is available at this website.

2

indicate that cutting the scale of successful test cases has
little eﬀect on clustering eﬀectiveness, suggesting a way
worth trying to lower the cost of SRR representation for
future researchers.

The remainder of this paper is organized as follows: Sec-
tion 2 introduces the background knowledge. Section 3 de-
scribes the experimental dataset and setup. Section 4 analyzes
the experimental results. Section 5 discusses some interesting
topics. Section 6 is the threats to validity. Section 7 reports
related works. Conclusions and directions for future work are
proposed in Section 8.

2. Background

We explain why clustering failed test cases is essential and
present the rationale of parallel debugging in Section 2.1. The
principles and technical details of SRR are given in Section 2.2,
followed by a motivating example showing the application of
SRR-based failure clustering in Section 2.3.

2.1. Why Clustering?

In general, the possibility of a program being faulty and the
number of faults it contains are proportional to its size [22].
With the increasing volume and the explosive growth of code
in modern software systems, most faulty programs usually have
multiple faults.

In multi-fault scenarios, various failed test cases2 may be
caused by diﬀerent faults. If failed test cases with distinct root
causes are not divided properly, fault localization techniques
could be confused by the impure test suite signiﬁcantly, for ex-
ample, SBFL techniques extract execution features of all faults
guided by the impure spectrum information, which will lower
the rank of each fault in the generated ranking list. According to
Wang et al., failed test cases that are not related to speciﬁc fault
are the main reason to reduce the eﬀectiveness of SBFL [23],
and similarly, Keller et al. have drawn a similar conclusion,
when using SBFL techniques, the number of lines that need to
be inspected can be reduced by high quality test cases that exe-
cute the bug [24]. Therefore, the purpose of dividing failed test
cases in a multi-fault scenario is to allow failed test cases with
diﬀerent root causes to target their corresponding faults sepa-
rately, to put it another way, reduce the interferences among
multiple faults in a program, enhance the pertinence of fault
localization techniques and thus achieve parallel debugging.

Many researchers have attempted to employ the clustering
technique to divide failed test cases [3, 12, 25–27]. Ideally, fail-
ures caused by the same fault should be grouped into a cluster,
then the failed test cases in a cluster are combined with all suc-
cessful test cases to form a fault-focused TS targeting a speciﬁc
fault, as deﬁned in Formula 1 and Formula 2. This strategy is
often called failure indexing or fault isolation.

Ft = F1 ∪

F2 ∪ · · · ∪

Fr

(1)

f ault- f ocused T S i = Fi

S (i = 1, 2, ..., r)

(2)

∪

Where Ft and S represent all failed test cases and all suc-
cessful test cases in TS, respectively. F1, F2, ..., Fr are gen-
erated fault-focused clusters, and r is the number of clusters
(which is expected to be equal to the number of faults).

Clustering failed test cases is a heuristic strategy for im-
proving the pertinence of TS and the eﬀectiveness of fault lo-
calization, this widely acknowledged method has been adopted
by many previous studies in the ﬁeld of multi-fault localiza-
tion [12, 28, 29].

It is vital to encode failed test cases in an intermediate rep-
resentation due to their unfriendly form for clustering. Cur-
rently, the most widely used representation methods are afore-
mentioned CVR and SRR. The technical details of SRR, which
is employed to conduct experiments in this paper, are described
below.

2.2. Statement Ranking Representation

After TS have been executed on PUT, the coverage infor-
mation of each test case that contains two components will be
collected in SRR:

• Execution Path: A binary vector that records which pro-
gram entities (statements3, branches, functions, or basic
blocks) [30, 31] have been covered by a test case.

• Execution result: A binary value denotes whether or not
the actual output of a test case matches its expected out-
put.

Suppose there is a PUT containing j executable statements
si (i = 1, 2, . . . , j) and a TS containing p test cases ti (i = 1,
2, . . . , p), the coverage generated by running TS against PUT
should be a matrix of size j × p. In SRR, the coverage gath-
ered against a failed test case and successful test cases will be
converted into spectrum information according to the notations
deﬁned in SBFL [8], as shown in Table 2 4.

Table 2: Notations in spectrum information

Notation Meaning
NCF
NUF
NCS
NUS
NC
NU
NS
NF
N

the number of failed test cases covering a statement
the number of failed test cases not covering a statement
the number of successful test cases covering a statement
the number of successful test cases not covering a statement
the number of test cases covering a statement
the number of test cases not covering a statement
total number of successful test cases
total number of failed test cases
total number of test cases

To incorporate several notations in spectrum information
into a suspiciousness value that measures the risk of a state-
ment being faulty, researchers have constructed a series of risk

2In this paper, we use “failed test case”, “anomalous execution”, and “fail-

and “statement” are interchangeable hereafter.

ure” interchangeably.

4Also referred to as ae f , an f , aep, anp, ae, an, ap, a f , a, respectively.

3We implement the statement granularity in our experiments, hence “entity”

3

lists) based on data winsorization, and developed an algorithm
to judge whether a failed test case should be chosen as one of
medoids; 3) relieved the shortcoming of examining all possi-
ble combinations of data points as initial medoids that exists in
the traditional K-medoids clustering algorithm. We conduct our
experiments based on MSeer because it has been recognized as
one of the state-of-the-art parallel debugging techniques, along
with its availability and reliability.

, t2 =
1,2,4
}
{
, t7 =
6,5,1
}
{

, t3 =
4,3,2
}
{
, t8 =
7,5,8
}
{

Let us use a motivating example to illustrate the details of
SRR as well as demonstrate the promise of failure clustering.
As shown in Table 3, the PUT that contains 11 statements, is
designed to calculate the product of the smaller two of the three
numbers, in which two faults have been induced by statements
s6 and s9, respectively. Give a TS containing 10 test cases: t1
, t5 =
, t4 =
=
, t6
5,1,6
3,2,4
2,6,5
}
}
{
{
}
{
=
, t10 =
, t9 =
,
8,6,9
8,1,2
5,7,3
}
{
}
{
}
{
six of them are labelled as f ailed due to the unexpected outputs
(t3, t4, t5, t7, t8, t10). The 11×10 matrix composed of rows s1 to
s11 and columns t1 to t10 in Table 3 is the coverage obtained by
t10 columns represent the
running TS against PUT, where t1 ∼
execution paths of 10 test cases. The symbol “·” denotes that a
test case covers an innocent statement, while “N” and “
” de-
note that a test case covers the statements containing Fault1 and
Fault2, respectively. The coverage information is reorganized
to spectrum information according to the notations deﬁned in
Table 2, as shown in the 11×9 matrix composed of rows s1 to
s11 and columns NCF to N in Table 3.

△

∪

Each statement’s suspiciousness is then generated by Ochiai,
as shown in column Ft
S in Table 3. We can immediately
sort these statements in descending order of suspiciousness, and
then get a ranking list of them: s9, s1, s2, s7, s8, s5, s6, s3, s4,
s10, s11. The statement s9 containing Fault2 has the highest sus-
piciousness of 0.82, hence it will be inspected ﬁrst. However,
the statement s6 containing Fault1 is ranked seventh, innocent
statements s1, s2, s7, s8 and s5 will be examined before s6. This
simple example reveals that the impure TS has a limited capa-
bility to delivering a promising fault localization output.

Now we depict how fault localization eﬀectiveness will be
improved by grouping failed test cases into distinct fault-focused
clusters. This is also a step-by-step elaboration of Figure 1.

• For the failure representation. We employ SRR to rep-
resent all six failed test cases. Take t5 as an example.
S ,
Pairing t5 with S to form a failure-speciﬁc TS, t5 ∪
executing this TS on PUT to obtain coverage and con-
vert it into spectrum information6, and then utilizing a
risk evaluation formula (e.g., Ochiai) to incorporate the
spectrum information for obtaining each statement’s sus-
piciousness, ﬁnally, a ranking list can be produced to rep-
resent t5, as shown in Table 4, which will be invoked
in the subsequent clustering process as a proxy of t5. It
should be noted that there are many ways for producing a
ranking list according to statements’ suspiciousness [33].
Considering the intuition that a ranking list should clearly

Figure 1: The workﬂow of SRR-based failure clustering

evaluation formulas. For example, Ochiai proposed by Abreu
et al. is deﬁned in Formula 3 [32]:

suspiciousness Ochiai =

NCF
√NF NC

(3)

The statements 5 in PUT are ranked according to their sus-
piciousness in descending order to deliver a ranking list. This
type of ranking list, which is produced by an REF from spec-
trum information that reﬂects the execution features of a failed
test case and successful test cases, is employed to represent this
failed test case in SRR.

2.3. Motivating Example

The workﬂow of SRR-based failure clustering is illustrated
in Figure 1. Test cases in the test suite can be determined as
failed or successful after being executed against the program,
according to the inconsistency or consistency between actual
and expected outputs, respectively. Each of failed test cases
will be combined with successful test cases and then be fed
into a risk evaluation formula, for delivering a ranking list that
could represent it in a mathematical form. Once fault-focused
clusters are produced by clustering these ranking lists, they will
be immediately sent to diﬀerent handlers for the following step.
It should be noted that after failed test cases have been trans-
formed to ranking lists, it is necessary to preprocess such data
by measuring distances between them, estimating the number
of clusters, and assigning the initial medoids, and only after all
of these procedures have been fulﬁlled can the clustering algo-
rithm begin to work. MSeer, an advanced framework for local-
izing multiple faults in parallel that alleviated these challeng-
ing jobs, has been proposed by Gao and Wong [12]. Specif-
ically, they 1) claimed that in the classic Kendall tau distance
metric, discordant pairs of more suspicious statements should
contribute more to the distance between two ranking lists, and
proposed a modiﬁed distance metric based on this intuition; 2)
assigned a potential value to each of failed test cases (ranking

5Unless otherwise speciﬁed, “statement” refers to “executable statement” in

6This failure-speciﬁc TS’s coverage and the corresponding spectrum infor-

this paper.

mation are omitted due to limited space.

4

Table 3: The sample PUT and its coverage against the given TS

S

s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11

Program

input a, b, c
if (a < b):

if (b < c):

z = a * b

else:

z = b * c //Fault1 Xz = a * c

else:

if (a <c)

z = a * c //Fault2 Xz = a * b

else

z = b * c

t1
·
·
·
·

t2
·
·

·
·

·
·

Coverage Information
t8
t3
·
·
·
·
·

t5
·
·
·

t6
·
·

t4
·
·

t7
·
·

·
N

·
N

·
·

·
·

△ △

·
·

△

·
·

·
·

t9
·
·

·
·

·
·

Spectrum Information

t10 NCF NUF NCS NUS NC NU NS NF N
10
·
10
·
10
10
10
10
10
10
10
10
10

10 0
10 0
7
3
9
1
8
2
8
2
3
7
3
7
6
4
7
3
7
3

4
4
4
4
4
4
4
4
4
4
4

6
6
6
6
6
6
6
6
6
6
6

0
0
4
6
4
4
2
2
2
6
6

4
4
1
1
0
0
3
3
0
3
3

0
0
3
3
4
4
1
1
4
1
1

6
6
2
0
2
2
4
4
4
0
0

△

·
·

Suspiciousness
F1 ∪
Ft
S
S
∪
0.58
0.77
0.58
0.77
0.82
0.47
0
0
1
0.58
1
0.58
0
0.62
0
0.62
0.82
0
0
0
0
0

F2 ∪
S
0.71
0.71
0
0
0
0
0.76
0.76
1
0
0

S and the corresponding ranking list

Table 4: Statements’ suspiciousness calculated by Ochiai in the sample PUT
against t5 ∪
Statement
Suspiciousness
Ranking list

s4
s1
0.45 0.45 0.71 0
6
4

s10
0
6

s11
0
6

s9
0
6

s7
0
6

s8
0
6

s5
1
1

s6
1
1

s3

s2

4

3

reﬂect the priority of a statement being inspected, as well
as other previous studies’ experience [33], we adopt the
following ranking strategy: if several statements with the
same suspiciousness form a T ie [34], the rankings of all
statements in the T ie will be set to the beginning position
of this T ie.

• For the distance metric. Given two ranking lists that
represent failed test cases, the classical Kendall tau dis-
tance counts the number of pairwise disagreements be-
tween them. Considering the characteristic of ranking
lists in the context of failure representation, discordant
pairs of more risky statements (i.e., at lower positions in
the ranking lists) should be paid more attention. Based
on this intuition, we use the revised Kendall tau distance,
which takes the reciprocal of the position of statements
in the discordant pairs [12], to measure the similarity be-
tween each pair of failed test cases.

• For the estimation of the number of clusters and the
assignment of initial medoids. We assign a potential
value for each failed test case according to the density of
its surrounding, to reﬂect the possibility of it being set as
a medoid, and the failed test case with the highest poten-
tial value will be selected as the ﬁrst medoid. Then, all
failed test cases’ potential values will be updated based
on how far they are from the newest medoid. Repeat-
ing these steps iteratively until the highest potential value
falls within a predeﬁned threshold, and as a consequence
of which, the number of clusters and initial medoids can
be determined at the same time [12].

• For the clustering algorithm. The K-medoids cluster-
ing approach sets practical (not virtual) data points as
medoids, aiming at minimizing the distance between failed
test cases and the medoid of the cluster where they re-
side. Its traditional version suﬀers from two tricky prob-
lems, namely, the diﬃculty of choosing a proper distance

metric and the overhead caused by examining all possi-
ble combinations of data samples as initial medoids. The
aforementioned two strategies can properly handle these
two points, respectively, thus an improved K-medoids al-
gorithm can be delivered and used in our failure cluster-
ing [12]. In the motivating example, failed test cases t5
and t8 are triggered by Fault1, and t3, t4, t7, and t10 are
triggered by Fault2. Ideally, the clustering results should
7.
be F1 =
t3, t4, t7, t10}
{

t5, t8}
{

, F2 =

S and F2 ∪

• For the bug triage. Two fault-focused TSs, F1 ∪

S ,
S , can be produced by combining F1 and F2 with all
F2 ∪
successful test cases S separately, and two sets of spec-
trum information can be collected by executing them on
PUT accordingly8. The suspiciousness of statements cal-
culated by Ochiai using these two sets of spectrum infor-
S in Table 3,
mation is shown in columns F1 ∪
S ,
respectively. In the ranking list produced against F1 ∪
the statement s6 where Fault1 lies in is given the highest
suspiciousness, while in the ranking list produced against
S , the statement s9 where Fault2 lies in is given the
F2 ∪
highest suspiciousness. Surprisingly, each faulty state-
ment appears at the top of the corresponding ranking list.
Guided by such fault localization outputs with strong per-
tinence, a developer (in sequential debugging), or two de-
velopers (in parallel debugging), only need(s) to inspect
at most three statements (the suspiciousness of s5 and s6
S is identical) for localizing all
calculated against F1 ∪
two faults. However, at least six statements have to be
examined for ﬁnding two faults in the confusing ranking
list produced without clustering failed test cases.

This motivating example not only highlights the promise of
clustering failed test cases but also indicates some key factors in
such a process: the risk evaluation formula (REF) that produces
ranking lists to representing failed test cases, the number of
successful test cases paired with one individual failed test case
(NSP1F), may inﬂuence clustering eﬀectiveness. Furthermore,

7For more details about the distance metric, the estimation of the number
of clusters and the assignment of initial medoids, and the clustering algorithm,
please refer to [12].

8These two fault-focused TSs’ coverage and the corresponding spectrum

information are omitted due to limited space.

5

Table 5: Subject Programs

Project
ﬂex
grep
gzip
sed
Chart
Closure
Lang
Math
Time

Version
2.5.3
2.4
1.2.2
3.02
2.0.0
2.0.0
2.0.0
2.0.0
2.0.0

kLOC
14.5
13.5
7.3
10.2
96.3
90.2
22.1
85.5
28.4

No. of faults
30AF + 46PF
27AF + 20PF
24AF + 20PF
21AF + 40PF
18
36
38
29
20

Description
lexical analyzer
ﬁle patterns searcher
data compressor
text processor
Chart library
Closure compiler
Apache commons-lang
Apache commons-math
Date and time library

Figure 2: Two fault types

considering that the eﬀect of the number of faults (NOF) and
the fault type (FT) in PUT on software debugging has caught
the attention of fault localization communities [5, 6, 14], we
conjecture these two points are also likely to aﬀect the results
of clustering. We conduct extensive controlled experiments to
explore how these four factors aﬀect the clustering process in
the next section.

3. EXPERIMENTAL SETUP

Section 3.1 provides the dataset used in our experiments and
the mechanism for generating multi-fault versions via mutation-
based strategies. Section 3.2 describes experimental setups for
four RQs. Section 3.3 introduces four metrics for evaluating the
experimental results.

3.1. The generation of faulty versions

We choose four benchmark programs from SIR [35]: f lex,
grep, gzip, and sed, and ﬁve benchmark programs from De-
fects4J [36]: Chart, Closure, Lang, Math, and T ime, for the
generation of multi-fault versions, as shown in Table 5.

3.1.1. SIR programs

SIR (Software-artifact Infrastructure Repository) contains
a series of programs written in C that can be expropriated for
the use of fault localization. We employ mutation-based strate-
gies to inject multiple artiﬁcial faults into four SIR benchmark
programs for generating faulty versions [37]. Research such
as [38–43] has conﬁrmed that mutation-based faults can sim-
ulate real-world faults and provide credible results for experi-
ments in the ﬁeld of software testing and debugging. The fol-
lowing two fault types are deﬁned to mutate source code, which
is exempliﬁed in Figure 2:

6

• Assignment Fault (AF): Editing a variable’s value in the
statement, or replacing the operators such as addition,
subtraction, multiplication, division, etc. with each other
(Figure 2(a));

• Predicate Fault (PF): Reversing the i f -else predicate,
or deleting the else statement, or modifying the decision
condition, and so on. (Figure 2(b)).

After a mutation-based fault is seeded into a benchmark
program, a 1-bug faulty version has been generated. To create
an r-bug faulty version, the faults from r individual 1-bug faulty
versions are injected into the same program. This method of
generating a multi-fault version by synthesizing multiple 1-bug
faulty versions has been adopted by many studies [10, 33, 44].
A total of 960 multi-fault versions have been generated us-
ing 228 faults on SIR programs9. From the perspective of NOF,
they can be categorized into four classes, i.e., 2-bug, 3-bug, 4-
bug, and 5-bug, according to how many faults a faulty version
contains. On the other hand, from the perspective of FT, they
can be categorized into three classes, i.e., TypeA, TypeP, and
TypeH, according to the fault type(s) involved in a faulty ver-
sion.

• TypeA: This type of multi-fault version is generated by r
1-bug faulty versions that contain assignment fault (each
of r faults contained in a TypeA faulty version is AF).

• TypeP: This type of multi-fault version is generated by r
1-bug faulty versions that contain predicate fault (each of
r faults contained in a TypeP faulty version is PF).

• TypeH: This type of multi-fault version is generated by r
1-bug faulty versions that contain both assignment fault
and predicate fault (AF and PF are hybridly contained in
a TypeH faulty version).

3.1.2. Defects4J programs

Defects4J gathers a collection of real-world bugs from some
open-source projects, due to the realism and ease-to-use, it has
been becoming one of the most popular benchmarks in the cur-
rent ﬁeld of fault localization. Nonetheless, Defects4J is often
utilized in single-fault rather than multi-fault environments, be-
cause each of its faulty versions only targets a speciﬁc fault.
Recently, researchers revisited this benchmark and concluded a
new point, that is, many of Defects4J faulty versions actually
contain more than one fault, but only one of them can be re-
vealed by the provided test suite. To adapt Defects4J to multi-
fault scenarios, An et al.
transplanted the fault-revealing test
case(s) of another faulty version or other faulty versions to a
basic faulty version, that is, enabling a strengthened test suite to
detect more faults in the original program (i.e., the basic faulty
version) [45].

9When two or more speciﬁc faults exist in a program, the program may fail
to compile, enter an inﬁnite loop, or run for an excessive amount of time. These
faulty versions were removed.

Following this strategy, a total of 100 multi-fault versions
have been generated using 141 faults on Defects4J programs.
It should be highlighted that the generation of multi-fault De-
fects4J programs involves two limitations. First, it is more dif-
ﬁcult to generate multi-fault versions that contain more bugs.
The faults in Defects4J come from real-world programming
practice, to preserve such a characteristic, we use test cases
transplantation instead of source code modiﬁcation during the
generation of multi-fault versions. Speciﬁcally, the majority of
Defects4J faulty versions are indexed chronologically accord-
ing to the revision date, a lower ID indicates a more recent
version [45], thus the fault in a newer version is also likely to
be contained in an older version. For example, we ﬁnd that
the fault in Lang-27 also appears in Lang-28, thus we can add
the failed test case of Lang-27 to the test suite of Lang-28, for
the generation of a 2-bug version, Lang-27-28. However, it is
more diﬃcult to search for a 5-bug version than a 2-bug version,
since the more faults, the less likely they co-exist in a same pro-
gram originally. For this reason, in the created 100 Defects4J
multi-fault versions, half of them are 2-bug, and 25, 16, and
9 ones are 3-bug, 4-bug, and 5-bug, respectively. Second, as
mentioned above, the faults in Defects4J are not obtained by
artiﬁcial simulation, thus they cannot be properly categorized
into assignment fault or predicate fault. As the consequence of
these two problems, Defects4J programs are not suitable for ex-
ploring RQ2 (How NOF aﬀects clustering eﬀectiveness?) and
RQ3 (Is clustering eﬀectiveness aﬀected by FT?).

In summary, RQ1 and RQ4 will be investigated on all faulty
versions that comprise both SIR and Defects4J, considering that
these two topics do not involve the number of faults and fault
types. And RQ2 and RQ3 will be investigated on SIR, since we
can hardly set a proper and fair environment to explore the two
questions on Defects4J.

3.2. Experiment setup

In this section, we elaborate on the experimental setups of

the four RQs deﬁned in Section 1.

3.2.1. The risk evaluation formulas in SRR (RQ1)

Countless research has been conducted to investigate var-
ious REFs in the last four decades [1, 46]. However, most
of these studies proposed a novel REF or contrasted existing
REFs empirically or theoretically in terms of its/their fault lo-
calization eﬀectiveness, that is, analyzing the REF’s capability
to ranking the faulty statement(s) at the top of the list [47–49].
For example, some novel REFs have emerged in the past
ten years, including Crosstab [50] and DStar [51] that were de-
veloped by Wong et al.
in 2011 and 2013, respectively. The
former constructs a crosstab for each statement in PUT to deter-
mine their suspiciousness by calculating the chi-square statistic
and the coeﬃcient of contingency, while the latter exponentially
strengthens the function of NCF in spectrum information, mak-
ing it more eﬀective in fault localization than any other tech-
niques compared with it according to the authors. Yoo created
30 novel REFs via genetic programming in 2012 [21], exper-
imental results proved that GP-evolved REFs can consistently

outperform many of the human-designed REFs. Xie et al. eval-
uated these 30 GP-evolved REFs using the theoretical frame-
work in [48] and discovered three REFs with strong human
competitiveness: GP02, GP03, and GP19 [52].

Apart from developing new REFs, some researchers have
dedicated their eﬀort to investigating a corpus of existing REFs.
For example, Naish et al. investigated more than 30 REFs and
extracted several equivalence relations guided by the strictest
equivalence deﬁnition (i.e., only REFs that generate the same
statement ranking lists are considered equivalent) [47]. Xie et
al. ﬁrst excluded some REFs that are not intuitively justiﬁed
in the context of SBFL, then selected 30 REFs from Naish et
al.’s research to contrast them using a novel theoretical frame-
work [48]. According to Naish et al. and Xie et al.’s conclu-
sions, 30 REFs are divided into six equivalent groups that in-
clude 22 REFs and eight individual REFs.

To the best of our knowledge, no empirical study has been
published to investigate how diﬀerent REFs, which produce
ranking lists that represent failed test cases, aﬀect the cluster-
ing eﬀectiveness in SRR-based parallel debugging. To ﬁll this
gap, we perform the ﬁrst empirical study on the capability of 35
REFs in Table 6 to representing failed test cases.

3.2.2. The number of faults in PUT (RQ2)

The eﬀect of the number of faults contained in a program on
fault localization eﬀectiveness has been investigated by many
prior researchers [5, 6, 14], but how NOF aﬀects the clustering
stage in parallel debugging is still poorly explored. Although
it is intuitive to assume that more bugs will lead to more fail-
ures, making it more diﬃcult to divide them, we do not know
whether this is reasonable from an empirical standpoint. To that
purpose, we observe and compare the eﬀectiveness of cluster-
ing in 2-bug, 3-bug, 4-bug, and 5-bug scenarios.

3.2.3. The fault type in PUT (RQ3)

Programmers may introduce various types of faults when
coding due to unintentional mistakes or misunderstandings of
programming logistics, as a result, FT is typically unpredictable
because of the randomness and uncertainty of onsite program-
ming. Lamraoui and Nakajima categorized common faults in
multi-fault scenarios into several types, including data-ﬂow de-
pendent faults and control-dependent faults [44]. Similar to
these, we deﬁne assignment faults and predicate faults, two
types of faults that are most likely to occur in programming as
our research objects, and accordingly generate a series of Ty-
peA faulty versions with only assignment faults, TypeP faulty
versions with only predicate faults, and TypeH faulty versions
with both two types of faults to observe clustering eﬀectiveness.

3.2.4. The number of successful test cases paired with one in-

dividual failed test case (RQ4)

While clustering failed test cases via SRR, many prior stud-
ies paired one failed test case with all successful test cases and
input them into an REF to produce a ranking list represent-
ing this failed test case, without explaining why all success-
ful test cases are employed here. In fact, many studies includ-
ing [60, 61] have managed to utilize test case selection or test

7

Name

Naish1 [47]

Jaccard [53]

Sørensen-Dice [47]

Goodman [47]

qe [55]

Wong2 [57]
Simple Matching [47]

Rogers & Tanimoto [47]
Euclid [47]

Russel & Rao [47]

Scott [47]

Kulczynski2 [47]

M2 [47]

Wong3 [57]

Cohen [47]
Crosstab [50] *

GP02 [21]

Table 6: 35 risk evaluation formulas

Formula expression
1
−
NS

if NCF < NF
if NCF = NF

NCS

(

−

NCF
NF +NCS
2NCF
2NCF +NUF +NCS
2NCF −
NCS
NUF −
2NCF +NUF +NCS

NCF
NC

NCF

NCS

−
NCF +NUS
N
NCF +NUS
NCF +NUS +2(NUF +NCS )
√NCF + NUS
NCF
N
NCS )2
4NUF NCS −
4NCF NUS −
(2NCF +NUF +NCS )(2NUS +NUF +NCS )
2 ( NCF
1
NF

(NUF −
)

+ NCF
NC

NCF
NCF +NUS +2(NUF +NCS )

Name

Naish2 [47]

Anderberg [47]

Dice [47]

Tarantula [54]

CBI Inc. [56]

Hamann [47]
Sokal [47]

Hamming etc. [47]
Wong1 [57]

Binary [47]

Rogot1 [47]

Ochiai [32]

Ample2 [47]

1
2 (

Formula expression

NCF

NCS
NS +1

−
NCF
NCF +2(NUF +NCS )
2NCF
NF +NCS
NCF
NF
NCS
NCF
+
NF
NS
NCF
- NF
NC
N
NCF +NUS −
NUF −
N
2(NCF +NUS )
2(NCF +NUS )+NUF +NCS
NCF + NUS
NCF
if NCF <NF
if NCF =NF

NCS

0
1
(
NCF
2NCF +NUF +NCS

NUS
2NUS +NUF +NCS

)

+
NCF
√NF NC

NCS
NS

NCF
NF −
2NCF NUS −

2NUF NCS

NC NU +NF NS

NCS )2

(NUF −

4NUF NCS −
4NCF NUS −
(2NCF +NUF +NCS )+(2NUS +NUF +NCS )
N∗CF
NUF +NCS
N2

√NCS

q|

CF −

|

NCF

−

NCS
2+0.1(NCS
2)
2.8+0.001(NCS
2NCF NUS −
ECS )2

NC NS +NF NU

h, where h= 


+ (NCS −
ECS

ECF )2

−

−
2NUF NCS

+ (NUF −
EUF
2(NCF + √NUS ) + √NCS

10)

EUF )2

if NCS
2
≤
if 2<NCS
≤
if NCS >10

10

+ (NUS −
EUS

EUS )2

Arithmetic Mean [47]

Fleiss [47]
DStar [51] **

GP03 [21]

χ2 = (NCF −
ECF

GP19 [21]
* Crosstab will ﬁrst calculate ϕ for each statement to quantify its association with failed and successful executions, and then use ϕ to determine if a statement

NCF √
|

NCF + NUF

NUS

NCS

−

−

|

should be assigned χ2,

χ2 or 0. Please refer to [50] for more details about this REF.

** Considering the preference for DStar in many other studies (such as [58, 59]), we set * = 2, the most thoroughly-explored value in our experiments.

−

suite reduction techniques to lower debugging expenses, some
recent studies have also investigated the impact of test suites on
fault localization [62, 63]. For example, as Fu et al. argued, if
the number of successful test cases is too large, the noise will
be introduced into the fault localization process [64]. However,
these works only evaluated the eﬀect of the number of test cases
on fault localization, not fault isolation built upon SRR. We try
to cut the scale of successful test cases utilized in SRR by pair-
ing 100%, 80%, 60%, 40%, and 20% of successful test cases
with one failed test case, respectively, to monitor if the cluster-
ing eﬀectiveness declines as NSP1F falls.

3.3. Metrics

Two classes of metrics, external metrics [65] and internal
metrics [66], are typically implemented to measure the eﬀec-
tiveness of clustering techniques. The former contrast clus-
tering results with the oracle, while the latter examine inher-
ent properties of clustering results, such as compactness and
separation, without using an oﬀ-the-shelf baseline [67]. While
clustering failed test cases in parallel debugging, ideal outputs
should exhibit linkages between each failed test case in TS and
each fault in PUT, which is available in our controlled exper-
iments. Therefore, we employ four widely-used external met-
rics, JC, FMI, PR and RR, to evaluate the experimental results.

3.3.1. Pair of cases-based metric

The pair of cases-based metric refers to compare the index-
ing consistency of each pair of failed test cases in the generated

8

Table 7: Four scenarios in the pair of cases-based metric

Notation

SS
SD
DS
DD

Results of failure indexing

In the generated cluster
Same
Same
Diﬀerence
Diﬀerence

In the oracle cluster
Same
Diﬀerence
Same
Diﬀerence

cluster with the oracle cluster. Four scenarios in which are de-
picted in Table 7.

Assuming that there are n failed test cases that need to be
clustered, a total of C2
n pairs will be examined in the pair of
cases-based metric. The numbers of pairs that fall into SS, SD,
DS, and DD categories are denoted as XS S , XS D, XDS , and XDD,
respectively.

The above notations can be incorporated into the Jaccard
Coeﬃcient (JC) and the Fowlkes and Mallows Index (FMI),
which are deﬁned in Formula 4 and Formula 5, respectively.
JC and FMI are used to determine the similarity between the
generated cluster and the oracle cluster, for measuring the clus-
tering results [68].

JC =

XS S
XS S + XS D + XDS

F MI =

XS S
XS S + XS D ×

XS S
XS S + XDS

r

(4)

(5)

The generated 
cluster
The oracle 
cluster

(cid:19)(cid:3)(cid:19)(cid:3)(cid:20)(cid:3)(cid:19)(cid:3)(cid:20)(cid:3)(cid:20)
(cid:19)(cid:3)(cid:19)(cid:3)(cid:19)(cid:3)(cid:20)(cid:3)(cid:20)(cid:3)(cid:19)

Figure 3: SS pairs in the generated cluster and the oracle cluster

Table 8: Four scenarios in the single case-based metric

Notation

TP
FP
TN
FN

Results of failure indexing

In the generated cluster
Positive
Positive
Negative
Negative

In the oracle cluster
Positive
Negative
Negative
Positive

It can be proved that the intervals of JC and FMI are both [0,
1], and that the larger the value in this range, the more eﬀective
clustering is. A simple example is given below to describe JC
and FMI.

As shown in Figure 3, six failed test cases (A, B, C, D,
E, and F) are indexed divergently in the generated cluster and
the oracle cluster. Among the C2
6 = 15 pairs of cases (A-B,
A-C, A-D, ···, E-F), A-B and C-F are in the same cluster in
the generated cluster, and also in the same cluster in the oracle
cluster, which meets the scenario SS in Table 7, therefore, XS S
= 2. Similarly, we can get XS D = 4, XDS = 5, and XDD = 4.
Incorporating these notations into Formulas 4 and Formula 5,
JC and FMI will be set to 0.182 and 0.309, respectively.

3.3.2. Single case-based metric

The single case-based metric refers to compare the classi-
ﬁcation result of each failed test case in the generated cluster
with the oracle cluster. Four scenarios in which are depicted in
Table 8.

The numbers of failed test cases that fall into TP, FP, TN,
and FN categories are denoted as XT P, XFP, XT N, and XFN , re-
spectively.

The above notations can be incorporated into the Precision
Rate (PR) and the Recall Rate (RR), which are deﬁned in For-
mula 6 and Formula 7, respectively, for measuring the cluster-
ing results.

PR =

XT P
XT P + XFP

(6)

RR =

XT P
XT P + XFN
It can be proved that the intervals of PR and RR are both [0,
1], and that the larger the value in this range, the more eﬀective
clustering is.

(7)

As shown in Figure 3, failed test cases D and E are labelled
as positive, and the remaining four ones are labelled as negative
in the oracle cluster. But in the generated cluster, failed test
cases C and F are wrongly labelled as positive, thus the value
of XFP can be determined as 2. Similarly, we can get XT P =
1, XT N = 2, and XFN = 1. Incorporating these notations into

Formulas 6 and Formula 7, PR and RR will be set to 0.333 and
0.5, respectively.

3.3.3. The virtual mapping problem

It should be noted that the diﬀerent permutations between
generated clusters and oracle clusters will result in diﬀerent out-
puts of the external metrics, and the diversity of permutations
will signiﬁcantly grow with the number of faults increases. For
example, in a 2-bug scenario, the permutations between two
generated clusters and two oracle clusters are A2
2 = 2, while
in a 5-bug scenario, the permutations between ﬁve generated
clusters and ﬁve oracle clusters are A5
5 = 120. Such diversity
of permutations does not exist in practical parallel debugging,
it only occurs in the contrast between the output and the or-
acle.
In other words, each developer will be allocated to a
fault-focused TS and will be responsible for localizing the cor-
responding fault independently, thus regardless of how many
potential permutations exist, there is only one real combina-
tion of generated clusters and oracle clusters. As a result, the
permutation between generated clusters and oracle clusters in
experiments is the combination in practice (we call this problem
the virtual mapping problem). In our experiments, we extract
faulty versions in which the number of faults has been precisely
estimated, i.e., the number of faults equals the number of gen-
erated clusters, to perform analyses. For each of these faulty
versions, we enumerate all feasible permutations followed by
picking the optimal one based on the value of JC, FMI, PR, or
RR for evaluation, because which permutation reﬂects the real
mapping relations is unknown.

4. RESULT AND ANALYSIS

We conduct extensive controlled experiments according to
the research questions in Section 1 and predesigned setups in
Section 3. Experimental results and analyses are given in this
section.

4.1. The capability of diﬀerent REFs to representing failed test

cases (RQ1)
We reorganize 35 REFs in Table 6 into 12 disjoint groups,
as shown in Table 9, because we ﬁnd that some REFs have the
same performance in representing failed test cases (details are
omitted to conserve space). Only one REF (in bold) in each
group is selected for analyses since its capability to representing
failed test cases is equal to the others in the group it belongs to.
For each of faulty versions, we implement the workﬂow
shown in Figure 1 to estimate the number of clusters based on
the ranking lists produced by an REF. There are three scenarios
in this stage:

• Under: The estimated number of clusters is fewer than

NOF (i.e., k < r).

• Equal: The estimated number of clusters is equal to NOF

(i.e., k == r).

• Over: The estimated number of clusters exceeds NOF

(i.e., k > r).

9

Table 9: 12 groups of risk evaluation formulas with the same capability to rep-
resenting failed test cases

REFs

Name
Group1
Group2

Naish2
Jaccard, Anderberg, Sørensen-Dice, Dice, Good-
man, M2, Naish1, DStar
Tarantula, qe, CBI Inc, Kulczynski2, Ochiai

Group3
Group4 Wong2, Hamann, Simple Matching, Sokal, Rogers &

Tanimoto, Hamming etc., Euclid

Scott, Rogot1
Ample2, Arithmetic Mean, Cohen, Crosstab

Group5 Wong1, Binary, Russel & Rao
Group6
Group7
Group8 Wong3
Group9
Fleiss
Group10 GP02
Group11 GP03
Group12 GP19

Spectrum Information

FMI

JC

PR

RR

Risk Evaluation Formula
GP19 or Crosstab or Ochiai (cid:258)

Oracle
Clus ter
1

Oracle
Clus ter
2

(cid:258)

Oracle
Clus ter
r

(cid:258)

Generated
Clus ter
1

Generated
Clus ter
2

(cid:258)

Generated
Clus ter
k

Ranking1

Ranking2

Rankingn

Clustering 
Algorithm

(cid:258) (omitted)
Estimation of the 
Number of Clusters k
(cid:258) (omitted)

Y

k == r ?

N

Discard

Figure 4: The virtual mapping process (with checking whether the estimated
number of clusters equals NOF)

If the estimated number of clusters k in a faulty version is
equal to NOF r, we send this faulty version to the next cluster-
ing step. Otherwise, if k is not equal to r, this faulty version
is discarded. In a real multi-fault localization scenario, even if
the estimated number of clusters k and the number of faults r
are not identical, the whole process can also be continued: if
k > r, localization can be stopped when all failures disappear,
and if k < r, localization can be carried out more than one itera-
tion. This paper focuses on clustering rather than the following
localization stage, the evaluation of clustering eﬀectiveness is
the main purpose, thus we do not take “k , r” scenarios into
account. The ﬁltering, as well as the follow-up virtual mapping
process, are illustrated in Figure 4.

When estimating the number of clusters based on the rank-
ing lists produced by an REF R, we denote the numbers of faulty
versions that fall into the Under, Equal, and Over categories as
V R
under, V R
equal, and V R
over, respectively. For an REF R, a greater
equal, as well as a fewer V R
V R
over, partly indicate
that R captures the execution features of failed test cases more
eﬀectively thus can better represent them. V R
equal, and
V R
over of each group of REFs on all faulty versions are shown in
Figure 5 10.

under and a fewer V R

under, V R

Figure 5: V R

under, V R

equal, and V R

over of 12 groups of REFs

equal

It can be seen that based on the ranking lists produced by
Group12, NOF is accurately estimated on 25% of the 1060
faulty versions (VGroup12
= 265). Besides, based on the rank-
ing lists produced by Group7, the estimated numbers of clus-
ters on 65% of faulty versions exceed the NOF (VGroup7
= 687),
implying this REF is over-representing in modeling failed test
cases (i.e., too sensitive to model failures to a nicety). Based
on the ranking lists produced by Group4, the estimated num-
bers of clusters in 85% of faulty versions are fewer than the
NOF (VGroup4
under = 896), indicating that this REF appears under-
representing in modeling failed test cases (i.e., too deﬁcient to
model failures distinguishably).

over

We select only faulty versions that fall into the Equal cate-
gory (i.e., satisfy the “k == r” criteria) for clustering, as illus-
trated in Figure 4. The capability of an REF R to representing
failed test cases can be assessed by two indicators: the value
of V R
equal faulty
versions. We deﬁne the S um MetricR
M, as shown in Formula 8,
to incorporate the two metrics into a single value.

equal and the clustering eﬀectiveness on these V R

S um MetricR

M =

V R

equal

Xi

Mi

(8)

Where R represents the REF, Mi is the value of the cluster-
ing metric M (M takes FMI, JC, PR or RR) on the ith faulty
version. For instance, if we want to evaluate Group12 from the
standpoint of FMI (i.e., R takes Group12 and M takes FMI) us-
ing Formula 8, we can ﬁrst get the value of VGroup12
(265), and
then calculate S um MetricGroup12
by adding up F MI1, F MI2,
..., F MI265. Speciﬁcally, the values of F MI on the ﬁrst, the
second, ..., and the 265th version are 0.78, 0.77, ..., and 1.00,
respectively, thus the value of S um MetricGroup12
can be deter-
mined by adding up 0.78, 0.77, ..., and 1.00, that is, 221.33.
Obviously, the greater the V R
equal value of the REF R, the more
possibility it has to obtain a greater S um MetricR
M.

equal

FMI

FMI

10The longer the green band, the more faulty versions’ NOF can be accu-

rately estimated based on the ranking lists produced by the corresponding REF.

10

Table 10: Contrast of the capability of 12 groups of REFs to representing failed test cases

Group1 Group2 Group3 Group4 Group5 Group6 Group7 Group8 Group9 Group10 Group11 Group12

XXXX

XXXX XXXX
XXXX XXXX XXXX

XXXX
XXXX XXXX
XXXX
XXXX XXXX
XXXX XXXX XXXX XXX
XXXX XXXX
XXXX XXXX XXXX XXXX XXXX XXXX

XXXX XXXX
XXXX XXXX

X

XXXX XXXX

XXXX
XXXX XXXX
XXXX XXXX XXXX
XX X
XXXX XXXX XXXX

X

X

XXXX
XXXX
XXXX XXXX

XXXX

XXXX
XXXX
XXXX
XXXX
X XX
XXXX
XXXX
XXXX
XXXX
XXXX

XXXX
XXXX
XXXX
XXXX
XXXX
XXXX
XXXX
XXXX
XXXX
XXXX
XXXX

Versus

R1

R2
Group1
Group2
Group3
Group4
Group5
Group6
Group7
Group8
Group9
Group10
Group11
Group12

Table 11: The values of S um Metric of 12 groups of REFs
S um MetricR

M M

FMI

JC

PR

RR

R

Group1
Group2
Group3
Group4
Group5
Group6
Group7
Group8
Group9
Group10
Group11
Group12

173.71
187.3
128.32
90.11
205.81
193.78
164.79
140.31
196.54
150.27
206.48
221.33

153.84
166.28
116.7
82.65
182.04
170.71
146.17
123.99
172.78
133.76
181.21
196.62

174.06
190.2
122.89
89.7
194.5
192.12
164.8
135.45
194.8
147.51
205.38
227.15

131.56
144.81
108.87
78.22
157.79
151.15
125.25
107.17
152.4
117.59
171.01
177.47

For two REFs, R1 and R2, if S um MetricR1

M > S um MetricR2
M ,
it means that according to the metric M, R1 is better than R2 in
representing failed test cases. We contrast 12 groups of REFs
according to their S um MetricR
M in Table 1011, as well as list
the S um MetricR
M values in Table 11. It can be seen that Group
12 outperforms the other 11 groups of REFs regardless of being
evaluated by FMI, JC, PR, or RR.

Now we can draw the conclusion of RQ1: Group12 is
highly competitive across all REFs when representing failed
test cases. The list of 12 groups of REFs ranked by their ca-
pability to representing failed test cases is as follows:

Group12 > Group11 > Group5 > Group9 > Group6 > Group2
> Group1 > Group7 > Group10 > Group8 > Group3 > Group4

4.2. The impact of NOF contained in PUT on the clustering

eﬀectiveness (RQ2)
Similar to deﬁnitions depicted in Section 4.1, we ﬁrst use
V N
equal (N = 2, 3, 4, 5) to denote how many N-bug faulty ver-
sions’ NOF can be accurately estimated by a speciﬁc REF, then
deﬁne and employ the S um MetricN
M, as illustrated in Formula 9,
to observe the clustering eﬀectiveness on these V N
equal versions.

11In the cell of [R1, R2], X, X, X, Xindicate that REF R1 is better than REF

R2 in terms of FMI, JC, PR, or RR, respectively.

11

S um MetricN

M =

V N

equal

Xi

Mi

(9)

The clustering eﬀectiveness is visualized using box-and-
whisker plots in terms of upper quartile, lower quartile, median,
and mean, where each vertical column’s color reﬂects the value
of V N
equal). The
color is regulated by adjusting the opacity using the procedures
below12

equal (a darker color indicates a greater value of V N

• Step-1: Set the color of each vertical column to black

(RGB: 0, 0, 0).

• Step-2: Count the values of V N

equal in N-bug scenarios (N
= 2, 3, 4, 5), and set the maximum value to MAX, as
deﬁned in Formula 10.

MAX = max

(N = 2, 3, 4, 5)

(10)

V N

equal

n

o

• Step 3: Calculate the opacity OpacityN of each vertical

column, as deﬁned in Formula 11.

OpacityN =

V N

equal
MAX

(N = 2, 3, 4, 5)

(11)

The clustering eﬀectiveness in 2-bug, 3-bug, 4-bug, and 5-
bug scenarios is shown in Figure 6 13. From this, we can draw
the conclusions of RQ2:

1) As NOF increases, the similarities (FMI, JC) between

generated clusters and oracle clusters decrease.

2) As NOF increases, the Recall Rate (RR) falls, while the

Precision Rate (PR) changes little.

3) As NOF increases, the dispersion of FMI, JC, and RR

narrows.

4) Based on the ranking lists produced by Group12, a greater
equal tends to be obtained if N equals 3 (Group3,

value of V N

12This color setting scheme is also applicable to Section 4.3.
13Due to space limitations, we only display the clustering results of Group12,
despite the fact that the clustering results of the other 11 groups of REFs all con-
ﬁrm the conclusions in Section 4.2. Please refer to the supplementary material
for a complete list of conclusions.

Table 12: The contrast of clustering eﬀectiveness among various NSP1Fs

the values of M X

M

FMI

JC

PR

RR

mean
median
mean
median
mean
median
mean
median
VX

equal

100% 80% 60% 40% 20%

0.82
0.79
0.72
0.67
0.82
0.85
0.68
0.64
251

0.82
0.79
0.71
0.66
0.81
0.82
0.67
0.61
255

0.82
0.79
0.71
0.67
0.81
0.80
0.66
0.59
261

0.81
0.79
0.71
0.66
0.81
0.81
0.67
0.60
261

0.81
0.79
0.70
0.66
0.80
0.79
0.65
0.57
263

The clustering eﬀectiveness in TypeA, TypeP, and TypeH
scenarios is shown in Figure 7 14. From this, we can draw the
conclusions of RQ3:

1) Compared with TypeA and TypeH, better clustering ef-
fectiveness is easier to obtain in the TypeP scenario con-
cerning FMI, JC, and RR. No signiﬁcant diﬀerences in
terms of PR among the three scenarios are observed.
equal and T have no evident relations.

2) The values of V T
The list of FTs ranked by the clustering eﬀectiveness under

them is as follows:

TypeP > TypeA

TypeH

≈

4.4. The impact of NSP1F on the clustering eﬀectiveness (RQ4)

Unlike the ﬁrst three RQs in which we pair one failed test
case with all (i.e., 100%) successful test cases, we randomly
sample X% (X = 80, 60, 40, 20) of successful test cases to pair
with one failed test case in this RQ. Similar to deﬁnitions de-
picted in Section 4.1, we ﬁrst use V X
equal to denote how many
faulty versions’ NOF can be accurately estimated by a speciﬁc
REF when the proportion of successful test cases is set to X%,
then deﬁne and employ the S um MetricX
M, as illustrated in For-
mula 13, to observe the clustering eﬀectiveness on these V X
equal
versions.

S um MetricX

M =

V X

equal

Xi

Mi

(13)

The clustering eﬀectiveness when X is set to 100, 80, 60,
40, 20 is shown in Table 12 15. For example, “FMI-mean-80%:
0.82” implies that when pairing one failed test case with 80%
of successful test cases, the mean of the values of F MI on 255
“k == r” faulty versions is 0.82. From this, we can draw the
conclusions of RQ4:

14Due to space limitations, we only display the clustering results of
Group10, despite the fact that the clustering results of many of the other
11 groups of REFs conﬁrm the conclusions in Section 4.3. Please refer to
the supplementary material for a complete list of conclusions.

15Due to space limitations, we only display the clustering results of Group11,
despite the fact that the clustering results of the other 11 groups of REFs all con-
ﬁrm the conclusions in Section 4.4. Please refer to the supplementary material
for a complete list of conclusions.

Figure 6: The contrast of clustering eﬀectiveness among 2-bug, 3-bug, 4-bug,
and 5-bug scenarios

Figure 7: The contrast of clustering eﬀectiveness among TypeA, TypeP, and
TypeH scenarios

Group5, Group8, and Group10 also support this conclu-
sion).

The list of NOFs ranked by the clustering eﬀectiveness un-

der them is as follows:

2-bug > 3-bug > 4-bug > 5-bug

4.3. The impact of FT contained in PUT on the clustering ef-

fectiveness (RQ3)

Similar to deﬁnitions depicted in Section 4.1, we ﬁrst use
V T
equal (T takes A, P, H) to denote how many TypeT faulty ver-
sions’ NOF can be accurately estimated by a speciﬁc REF, then
deﬁne and employ the S um MetricT
to observe the clustering eﬀectiveness on these V T

M, as illustrated in Formula 12,

equal versions.

S um MetricT

M =

V T

equal

Xi

Mi

(12)

12

1) Lowering NSP1F (to as low as 20%) has no evident eﬀect

on clustering eﬀectiveness.

2) The eﬀect of X on the value of V X

equal is neither evident

nor decisive.

The list of NSP1Fs ranked by clustering eﬀectiveness under

them is as follows:

100%

80%

60%

40%

≈

≈

≈

≈

20%

This conclusion indicates that 100% clustering eﬀectiveness
can be achieved with only 20% of successful test cases. When
developers use SRR to clustering failed test cases in parallel
debugging, they can feel free to cut the scale of successful test
cases for lower debugging costs without worrying about the loss
of eﬀectiveness.

5. DISCUSSION

Some interesting topics related to our empirical study are

further discussed in this section.

5.1. An in-depth analysis of clustering failed test cases

Given a TS and a PUT, the numbers of failed test cases and
successful test cases will be immediately determined. If multi-
ple faults are contained in the PUT, all existing failed test cases
might be caused by diﬀerent faults, that is, each failed test case
will be linked to its root cause(s). The more the faults, the lower
proportion of failed test cases caused by each fault to all failed
test cases16. However, the intuition of designing risk evalu-
ation formulas in SBFL is to assign higher suspiciousness to
statements that are covered by more failed test cases [69, 70],
which would be disturbed by the presence of multiple faults,
and the degree of disturbance magniﬁes as the number of faults
increases. Zheng et al. presented a similar opinion in [71],
they claimed when there is only one faulty statement, it is more
likely to be covered by more failing executions, whereas the
failing executions are diluted by multiple faults so less accurate
results are obtained.

To tackle this challenge, it is natural to categorize failed
test cases according to their root cause(s), in other words, build
linkages between failed test cases and faults. As a classic tech-
nique for unsupervised data grouping, clustering is typically
employed to accomplish this failure indexing process, with the
goal of fault isolation.

We use Figure 8 to simulate the eﬀectiveness of fault iso-
lation. In a single-fault scenario, the proportion of failed test
cases caused by the unique fault F1 (denoted as valid failed test
cases for F1) to all failed test cases is 100%, that’s to say, all
failed test cases fed into a risk evaluation formula for F1, thus
SBFL techniques are easier to push the statement that contains
F1 towards the top of the ranking list, as shown in Figure 8(a).
In a multi-fault scenario, assume there are r bugs, Fi (i = 1, 2,

16We discuss this problem under the condition of the number of failed test

cases has been determined.

13

A ranking list via 
all failed cases

A ranking list via 
all failed cases

A ranking list via
valid failed cases for F1

A ranking list via
valid failed cases for F2

A ranking list via
valid failed cases for F3

A ranking list via
valid failed cases for Fr

F1

F1

Fr

Clean Statements

(a) A single-fault scenario

F1

F2

F3

(cid:258)

Fr

(b) A multi-fault scenario without clustering

F2

F3

Clean Statements

Clean Statements

(cid:258)

Clean Statements

(c) A multi-fault scenario with clustering

Figure 8: Fault localization eﬀectiveness with and without clustering in a multi-
fault scenario.

|

|

|

Fi

..., r) in a PUT, n failed test cases in a TS, and the number of
failed test cases caused by Fi (denoted as valid failed test cases
. The proportion of failed test cases linked to Fi to
for Fi) is
Fi
|
/n, which is ordinarily less than 100%.
all failed test cases is
Furthermore, if all failed test cases are utilized in SBFL with-
out being reﬁned, the process of localizing a single fault, Fi,
will be interrupted by failed test cases caused by the other faults
(denoted as redundant failed test cases for Fi). Consequently,
SBFL techniques’ capability is diminished since linkages be-
tween a single fault and its responsible failed test cases have
been diluted (simulated by the opacity of faulty statements in
Figure 8(b)), potentially lowering the rankings of statements
that contain faults.

After all failed test cases are divided into several disjoint
fault-focused clusters, only failed test cases triggered by Fi, as
well as successful test cases, will be fed into a risk evaluation
formula to localize Fi. That’s to say, when ideal clustering re-
sults are delivered, the proportion of valid failed test cases for
Fi to all failed test cases regains 100%, since redundant failed
test cases for Fi have been indexed to their own root cause,
which enables the position of the statement that contains Fi to
be higher in the corresponding ranking list, as shown in Fig-
ure 8(c).

5.2. Revisit of V R

over and V R

under

When evaluating the capability of REFs to representing failed

test cases, we consider only faulty versions that fall into the
Equal category, in other words, if the NOF of a faulty version
is not accurately estimated based on an REF R (i.e., falls into
the Under or the Over category), this faulty version will be dis-
carded, and thus will not be dedicated to R’s capability to clus-
tering failed test cases. It is obvious that the larger the value
of V R
over and V R
under), the
greater the possibility that R will be highly competitive.

equal (that is, the lower the values of V R

Nonetheless, the same values of V R

under should not
be treated equally since they can reﬂect diﬀerent deviations
from the NOF. For example, assume that the NOFs of ten 5-
bug faulty versions are being estimated based on the ranking
lists produced by two REFs, R1 and R2, respectively, we can

over and V R

Table 13: The values of Deviation of 12 groups of REF

Value

Metrics

R

DeviationR

over

DeviationR

under

mean

Group1
Group2
Group3
Group4
Group5
Group6
Group7
Group8
Group9
Group10
Group11
Group12

3.32
2.89
4.81
1.95
2.02
2.37
3.46
4.21
2.35
3.80
1.26
2.07

1.57
1.70
1.81
2.39
1.73
1.85
1.66
2.15
1.87
2.04
2.05
1.80

2.95
2.53
3.72
2.36
1.87
2.13
3.09
3.15
2.13
3.20
1.94
1.95

immediately get ri (i = 1, 2, . . . , 10) are all equal to 5. If the es-
timate results generated by R1 are kR1
(i = 1, 2, . . . , 10), which
i
are 9, 9, 8, 9, 5, 5, 1, 1, 2, 2, respectively, and the estimate re-
sults generated by R2 are kR2
(i = 1, 2, . . . , 10), which are 6,
i
6, 7, 6, 5, 5, 3, 3, 4, 4, respectively. According to the preced-
ing deﬁnitions in Section 4.1, the values of V R1
over and V R2
over are
equal to 4, the values of V R1
equal are equal to 2, and
the values of V R1
under are equal to 4. Although both R1
and R2 estimate the NOF on eight faulty versions inaccurately,
it is visible that R2 delivers a closer result, implying R2 has a
stronger capability to representing failed test cases to some ex-
tent. We deﬁne two metrics, DeviationR
over in Formula 14 and
DeviationR
under in Formula 15, to quantify this type of diﬀerence
among all REFs.

equal and V R2

under and V R2

DeviationR

over =

1
V R
over ×

V R

over

Xi

(ki

ri)

−

(14)

V R

under

under =

DeviationR

1
V R
under ×
Where ki is the estimated number of clusters on the ith faulty
version, and ri represents the NOF contained in the ith faulty
version.

(15)

Xi

ki)

(ri

−

Using Formula 14 and Formula 15 to contrast R1 and R2 in
the aforementioned example, we can get DeviationR1
over = 3.75,
over = 1.25, DeviationR2
DeviationR1
under = 1.5.
Hence, the diﬀerence between R1 and R2 hidden behind the k ,
r faulty versions is captured and quantiﬁed.
over and V R

under = 3.5; DeviationR2

We revisit the values of V R

under for 12 groups of

REFs presented in Figure 5, as shown in Table 13.
It can be seen that the value of DeviationR

over of Group11 is
1.26, indicating when the estimated number of clusters exceeds
the NOF, Group11 has the lowest degree of over-representing.
The value of DeviationR
under of Group1 is 1.57, indicating when
the estimated number of clusters is fewer than the NOF, Group1
has the lowest degree of under-representation. The mean of
Group5 is 1.87, indicating when the estimated number of clus-
ters is not equal to the NOF, Group5 has the lowest deviation.

Notice that such analyses are non-trivial for parallel debug-
ging. In real multi-fault localization scenarios, it is expected

14

that the predicted number of faults k is identical to the num-
ber of faults r. If such ideal situations cannot be attained, the
smaller the deviation, the lower the time and labor cost. Specif-
ically, one cannot judge whether the prediction result is cor-
rect since the value of r is unknown in practice. Thus, k fault-
focused clusters will be directly input to the following local-
ization stage. If k exceeds r, k developers will be employed to
locate r faults, resulting in waste of human labor (k - r devel-
opers are redundant). On the contrary, if k is less than r, more
r/k
than one (
) iteration of debugging is needed, resulting in
waste of time.

⌈

⌉

5.3. A heuristic perspective to contrast REFs

We further discuss the relation between the virtual mapping
problem and the evaluation of clustering eﬀectiveness. Assume
that REF R is utilized to represent failed test cases in a faulty
version. If the estimated number of clusters k is equal to the
NOF r, there will be Ar
k permutations between generated clus-
ters and oracle clusters. The four metrics, FMI, JC, PR, and
RR, will appear diﬀerent values on diﬀerent permutations. If
the highest values of the four metrics all appear on the same
permutation, it means that the four metrics can easily achieve
a consensus, which indicates that the ranking lists produced by
R represent failed test cases distinguishably. On the contrary, if
the highest values of the four metrics are dispersed onto diﬀer-
ent permutations, divergences among these four metrics are re-
vealed, which just demonstrates that the ranking lists produced
by R are too analogous to be divided.

We regard the evaluation of four metrics for all permuta-
tions as a voting process, in which each metric votes for the
permutation with its highest value. For example, a permutation
will get four votes if the highest values of all four metrics occur
on it. Obviously, in the aforementioned r-bug faulty version, Ar
k
permutations will each be assigned a value of votes. This r-bug
faulty version’s votes will be referred to as the highest value of
votes among Ar

k permutations.

We design the S um VoteR metric to count the votes of faulty
versions that satisfy the “k == r” criteria for each REF R in Fig-
ure 5, as shown in Formula 16. We believe that the S um VoteR
metric reﬂects the capability of the risk evaluation formula R to
representing failed test cases from a heuristic perspective.

S um VoteR =

V R

equal

Xi

votei

(16)

Where votei is the value of votes of the ith faulty version.
The values of S um VoteR of 12 groups of REFs are given
in Figure 9. For instance, on 265 “k == r” faulty versions of
Group12, 204, 31, and 30 of them get 4, 3, and 2 votes, re-
spectively, we can immediately obtain S um VoteGroup12 = 969
according to Formula 16. The direction of the circular arrow
in Figure 9 indicates the ranking of S um VoteR values of 12
groups of REFs: Group12 > Group11 > Group5 > Group9 >
Group6 > Group2 > Group1 > Group7 > Group10 > Group8 >
Group3 > Group4, double-conﬁrming the conclusion of RQ1.

as low as 20%) indeed has no evident eﬀect on clustering ef-
fectiveness. As a result, while performing SRR-based failure
clustering, developers can reduce debugging costs by pairing
only a portion of the successful test cases with one failed test
case since too many successful test cases will not help represent
failed test cases.

Even though failed test cases have gotten a lot of attention in
testing and debugging, successful test cases can also play a vital
role. For example, metamorphic testing enables successful test
cases to expose failures via metamorphic relations [72, 73]. We
only illustrate the redundancy of successful test cases in SRR-
based failure clustering, without denying their signiﬁcance in
localization, testing, or the other software quality assurance ac-
tivities.

Figure 9: The values of S um VoteR of 12 groups of REFs

6. THREATS TO VALIDITY

5.4. Why is it easier to obtain better clustering eﬀectiveness in

TypeP faulty versions?

The conclusions in Section 4.3 reveal that when a program
has only predicate faults, the overall clustering eﬀectiveness is
higher than when it has only assignment faults and both two
types of faults coexist. Take Group10 as an example (Figure 7),
the number of “k == r” faulty versions of TypeP is 25.0% and
28.6% greater than that of TypeA and TypeH, respectively, ac-
cording to their opacity. TypeP scenarios also have better clus-
tering eﬀectiveness (the mean and median of FMI, JC, and RR)
than the other two fault types.

In SRR-based failure clustering, a ranking list, which is
produced by a risk evaluation formula, serves as a proxy for
a failed test case. The basis of generating a ranking list is spec-
trum information, while the latter originates from coverage. In
other words, SRR-based failure clustering heavily depends on
the failed test cases’ execution paths on the PUT. For failed test
cases caused by diﬀerent faults, the more distinctive execution
paths they have, the more distinguishable ranking lists an REF
can generate, and the easier they are to be indexed. A TypeP
faulty version has only predicate faults, which involve reversing
the i f -else predicate, deleting the else statement, or modifying
the decision condition, etc., according to the deﬁnition in Sec-
tion 3.1.1. All of the three classes could cause unwanted code to
be executed, resulting in a diﬀerent trace. Thus, failed test cases
in TypeP faulty versions are more likely to appear diverse cov-
erage, which will be beneﬁcial to isolate these predicate faults.
However, this assistance, on the one hand, does not exist when
a program contains only assignment fault, on the other hand, is
diminished when the two types of faults coexist.

5.5. The function of successful test cases in SRR

In Section 3.2.4, we assume that the function of successful
test cases in SRR-based failure clustering is to assist risk evalu-
ation formulas in generating ranking lists (some REFs will lose
their deﬁnition without being fed into successful test cases),
that’s to say, they serve as complements in failure indexing.
The conclusions in Section 4.4 reveal that lowering NSP1F (to

Similar to previous empirical studies on parallel debugging,
a hard-clustering strategy is used in this paper to divide failed
test cases, that is, a failed test case can only be categorized into
one cluster. However, in real-world debugging processes, the
relations between faults and failures are quite complex since
several faults might trigger the same failure (i.e., one failed test
case links to multiple faults). Therefore, the clustering eﬀec-
tiveness will be reduced since the inherent conﬂict between the
property of hard-clustering techniques and the one-to-many or
many-to-many linkages. Nonetheless, the reliability of our con-
clusions is not aﬀected by this threat since we contrast diﬀerent
variables based on the same clustering technique.

In addition, to build the virtual linkages between generated
clusters and oracle clusters, we ﬁlter out faulty versions with the
estimated number of clusters not equal to the NOF. Although
this strategy guarantees the availability of clustering results, it
also causes various variables in each RQ to be contrasted based
on diﬀerent numbers of faulty versions. This threat seems to
introduce additional uncertainties for the experiments, however,
we believe that 1) how many faulty versions are selected by var-
ious variables in each RQ (i.e., 12 groups of REFs in RQ1, 2-
bug, 3-bug, 4-bug, and 5-bug scenarios in RQ2, TypeA, TypeP,
and TypeH scenarios in RQ3, 100%, 80%, 60%, 40%, and 20%
of successful test cases in RQ4) reﬂect these variables’ capabil-
ity to representing failed test cases, and 2) the distinction in di-
verse benchmarks avoids the bias caused by a standard dataset,
which makes the conclusions more universal.

Although we collected four datasets with varied scales and
functions, they are all written in C. Besides, when utilizing the
mutation-based strategy to inject faults into the original pro-
gram, the number of predeﬁned mutation operators is limited,
which lowers the diversity of faulty versions to some extent.

7. RELATED WORK

Clustering failed test cases into various fault-focused groups
that target diﬀerent faults is not a newborn method. As early
as 2003, Podgurski et al. observed that open-source software
developers had received a large number of bug reports from

15

end-users every day, but many of these bug reports are actu-
ally caused by the same fault although they have distinct trig-
ger paths and diﬀerent anomalous behaviors. To that end, they
suggested grouping together failures with the same root cause
based on supervised and unsupervised pattern classiﬁcation, which
avoids potentially unwanted and redundant debugging labor [28].
Considering the suggestions of Podgurski et al. [28], Jones et al.
proposed two parallel debugging techniques in [3]. Speciﬁcally,
they ﬁrst divided failed test cases into several disjoint clusters
based on similarities, and then separately combined these clus-
ters with all successful test cases to generate specialized test
suites that are expected to target diﬀerent faults. These fault-
focused TSs are ﬁnally assigned to several developers for lo-
calizing multiple faults in parallel. DiGiuseppe and Jones then
conducted an empirical study to conﬁrm the necessity of clus-
tering failed test cases and to explore the inﬂuence of the pres-
ence of multiple faults on fault localization. They pointed out
clustering failed test cases is necessary and beneﬁcial despite
the fact that this process may incur additional computational
costs, since their ﬁndings demonstrated that multi-fault indeed
had a negligible eﬀect on the eﬀectiveness of fault localiza-
tion [5].

H¨ogerle et al. ﬁrst quoted an important opinion concluded
by Jones et al. in [3], that is, parallelization can speed up de-
bugging signiﬁcantly, even if the derived parallel tasks are con-
ducted sequentially, and then pointed out that the method of
dividing failed test cases should be carefully chosen because
it will have a signiﬁcant impact on the division eﬀectiveness
through large-scale experiments [74]. The eﬀectiveness of par-
allel debugging will be directly determined by the outcomes of
the clustering process. Zakari and Lee investigated commonly-
used parallel debugging techniques and found that most research
1) employed CVR as failure proximity to represent failed test
cases, and 2) used Euclidean, Jaccard, or Hamming distance to
measure the similarities between failed test cases. They ﬁrst
coined the term problematic approach to describe debugging
approaches that adopted the above techniques, and then con-
ducted an empirical study on the eﬀectiveness of several prob-
lematic approaches adopting the K-means clustering algorithm.
Their results showed that clustering built upon CVR and Eu-
clidean distance reduced the eﬀectiveness of multi-fault local-
ization [75].

Liu et al. conducted systematic research on failure prox-
imity in [7] and [76], in which they summarized or proposed
six representative failure proximities, i.e., Failure-based, Stack
Trace-based, Code Coverage-based, Predicate Evaluation-based,
Dynamic Slicing-based, and Statistical Debugging-based. The
CVR utilized in most studies is similar to the above-mentioned
Trace-proximity, which has been proven to be less eﬀective
in clustering failed test cases. To tackle this limitation, Gao
and Wong employed SRR, which is similar to Rank-proximity
in [7], to represent failed test cases. Speciﬁcally, 1) they paired
each failed test case with all successful test cases and input them
into an REF, Crosstab [50], to generate a ranking list that rep-
resents the corresponding failed test case. 2) They stated that
the clustering algorithm’s performance highly depends on the
distance metric, thus revised the original Kendall tau distance

based on the premise that discordant pairs of more suspicious
statements contribute more to the distance between two ranking
lists. 3) To tackle the long-standing problem of estimating the
number of clusters, as well as relieving the uncertainty intro-
duced by randomly generating initial centroids, an approach of
selecting initial medoids while predicting the number of clus-
ters was presented inspired by prior studies [18, 19]. 4) They
claimed that their initial medoids selection approach reduced
the high computational costs to a large extent compared with the
original K-medoids clustering algorithm, due to the latter exam-
ines all possible combinations of data points as initial medoids.
Gao and Wong integrated the above four innovations and de-
veloped a novel technique for localizing multiple faults in par-
allel [12].

In addition, some researchers have developed a series of
novel parallel debugging strategies by integrating techniques
from other domains into fault localization. For example, Zakari
et al. proposed a fault localization technique that is suited for
both single-fault and multi-fault scenarios based on the com-
plex network theory (FLCN), where developers can localize
multiple faults at the same time in a single diagnosis ranking
list [77]. In another study, they adopted the divisive network
community algorithm to cluster failed test cases, as well as em-
ployed a weighting and selecting mechanism to prioritize gen-
erated fault-focused communities [78]. Based on one-fault-at-
a-time via OPTICS (Ordering Points To Identify the Clustering
Structure) clustering, Wu et al. proposed to 1) divide failed test
cases in each iteration and calculate the density of each cluster,
2) combine the failed test cases in the cluster with the highest
density value with all successful test cases to form a new test
suite, and 3) localize a single fault based on the ranking list pro-
duced by the new test suite, iterating these steps until all bugs
are ﬁxed. Based on their ﬁndings, they further concluded that
using the clustering algorithm with the highest accuracy can
achieve the best performance of multi-fault localization [25].
Inspired by the multiple-fault-at-a-time strategy, Zheng et al.
converted fault localization tasks into search problems and pro-
posed a fast software multi-fault localization framework using
genetic algorithms [71]. Pei et al. introduced the dynamic ran-
dom testing (DRT) strategy and proposed distance-based DRT,
which vectorized test cases and divided them into disjoint sub-
domains using distance information from inputs and a speciﬁc
clustering algorithm [79].

There are also some researchers who carried out empirical
comparisons of diﬀerent techniques in the ﬁeld of multi-fault
localization. For instance, Gao et al. contrasted the eﬀec-
tiveness of 22 machine learning algorithms typically used in
multi-fault localization and found that random forests, BP neu-
ral networks, and logit boost machine learning models based
on ensemble learning performed well [80]. Huang et al. ﬁrst
created 12 types of setup by combining 6 REFs and 2 widely-
used clustering algorithms, and then conducted empirical re-
search in multi-fault scenarios using CVR. Their experimen-
tal results showed that Wong1 paired with K-means outper-
formed the other combinations [33]. Zakari et al. conducted a
systematic literature review on classic parallel debugging tech-
niques [81]. They investigated oﬀ-the-shelf studies and cate-

16

gorized them into three prominent types of strategy, one-fault-
at-a-time debugging, parallel debugging, and multiple-fault-at-
a-time debugging. Among them, they pointed out parallel de-
bugging alleviated fault interferences through clustering failed
test cases. However, many studies such as [3] and [33] claimed
these existing strategies were insuﬃcient for isolating faults as
well as listed some challenges related to clustering eﬀective-
ness in parallel debugging, including the method of represent-
ing failed test cases, the initial set of fault-focused clusters, the
clustering algorithm, and the distance metric.

8. CONCLUSION AND FUTURE WORK

We extract and analyze four essential factors, i.e., the risk
evaluation formula that produces ranking lists, the number of
faults in a program, the fault types, and the number of suc-
cessful test cases paired with one individual failed test case,
to investigate how these variables aﬀect clustering eﬀective-
ness. Four research questions are presented in this paper, the
corresponding controlled experiments show that: 1) GP19 is
highly competitive across all REFs, thus we recommend that
researchers or developers who adopt SRR for parallel debug-
ging use GP19 to represent failed test cases; 2) clustering eﬀec-
tiveness decreases as NOF increases, indicating that a greater
number of faults reduces the eﬀectiveness not only in fault lo-
calization but also in fault isolation; 3) higher clustering ef-
fectiveness is easier to achieve when a program contains only
predicate faults, which points out the challenge of isolating as-
signment faults; and 4) clustering eﬀectiveness remains when
NSP1F is reduced to 20%, future researchers and developers
are suggested to cut the scale of successful test cases while us-
ing SRR for a lower debugging expense.

In the future, we plan to further explore the internal mech-
anisms of risk evaluation formulas to representing failed test
cases, followed by proposing a novel REF for the representa-
tion of failed test cases. We also consider investigating the four
factors that may inﬂuence clustering eﬀectiveness with larger
datasets and broader experiment setups, as well as introducing
new evaluation metrics.

Acknowledgment

This work was partially supported by the National Natural
Science Foundation of China under the grant numbers 61972289
and 61832009. And the numerical calculations in this work
have been partially done on the supercomputing system in the
Supercomputing Center of Wuhan University.

References

[1] W. E. Wong, R. Gao, Y. Li, R. Abreu, F. Wotawa, A survey on software
fault localization, IEEE Transactions on Software Engineering 42 (8)
(2016) 707–740.

[2] Y. Xiaobo, B. Liu, W. Shihai, An analysis on the negative eﬀect of
multiple-faults for spectrum-based fault localization, IEEE Access 7
(2018) 2327–2347.

[3] J. A. Jones, J. F. Bowring, M. J. Harrold, Debugging in parallel, in: Pro-
ceedings of the 2007 international symposium on Software testing and
analysis, 2007, pp. 16–26.

[4] N. DiGiuseppe, J. A. Jones, Fault interaction and its repercussions,
in: 2011 27th IEEE international conference on software maintenance
(ICSM), IEEE, 2011, pp. 3–12.

[5] N. DiGiuseppe, J. A. Jones, On the inﬂuence of multiple faults on
coverage-based fault localization, in: Proceedings of the 2011 interna-
tional symposium on software testing and analysis, 2011, pp. 210–220.

[6] N. DiGiuseppe, J. A. Jones, Fault density, fault types, and spectra-based
fault localization, Empirical Software Engineering 20 (4) (2015) 928–
967.

[7] C. Liu, X. Zhang, J. Han, A systematic study of failure proximity, IEEE

Transactions on Software Engineering 34 (6) (2008) 826–843.

[8] X. Xie, B. Xu, Essential Spectrum-based Fault Localization, Springer,

2021.

[9] H. L. Cao, S. J. Jiang, Multiple-fault localization based on chameleon
clustering, Tien Tzu Hsueh Pao/Acta Electronica Sinica 45 (2) (2017)
394–400.

[10] Z. Yu, C. Bai, K.-Y. Cai, Does the failing test execute a single or multiple
faults? an approach to classifying failing tests, in: 2015 IEEE/ACM 37th
IEEE International Conference on Software Engineering, Vol. 1, IEEE,
2015, pp. 924–935.

[11] Y. Wang, R. Gao, Z. Chen, W. E. Wong, B. Luo, Was: A weighted
attribute-based strategy for cluster test selection, Journal of Systems and
Software 98 (2014) 44–58.

[12] R. Gao, W. E. Wong, Mseer—an advanced technique for locating multi-
ple bugs in parallel, IEEE Transactions on Software Engineering 45 (3)
(2017) 301–318.

[13] X. Xue, A. S. Namin, How signiﬁcant is the eﬀect of fault interactions
on coverage-based fault localizations?, in: 2013 ACM/IEEE International
Symposium on Empirical Software Engineering and Measurement, IEEE,
2013, pp. 113–122.

[14] J. A. Jones, M. J. Harrold, J. Stasko, Visualization of test information to
assist fault localization, in: Proceedings of the 24th International Confer-
ence on Software Engineering. ICSE 2002, IEEE, 2002, pp. 467–477.
[15] D. Jeﬀrey, N. Gupta, R. Gupta, Fault localization using value replace-
ment, in: Proceedings of the 2008 international symposium on Software
testing and analysis, 2008, pp. 167–178.

[16] J. Xuan, M. Martinez, F. Demarco, M. Clement, S. L. Marcote,
T. Durieux, D. Le Berre, M. Monperrus, Nopol: Automatic repair of con-
ditional statement bugs in java programs, IEEE Transactions on Software
Engineering 43 (1) (2016) 34–55.

[17] M. Kendall, D. Gibbons, Rank correlation methods (1990).
[18] R. R. Yager, D. P. Filev, Approximate clustering via the mountain method,
IEEE Transactions on Systems, Man, and Cybernetics 24 (8) (1994)
1279–1284.

[19] S. L. Chiu, Fuzzy model identiﬁcation based on cluster estimation, Jour-

nal of Intelligent & fuzzy systems 2 (3) (1994) 267–278.

[20] L. Kaufman, P. J. Rousseeuw, Finding groups in data: an introduction to

cluster analysis, Vol. 344, John Wiley & Sons, 2009.

[21] S. Yoo, Evolving human competitive spectra-based fault localisation tech-
niques, in: International Symposium on Search Based Software Engineer-
ing, Springer, 2012, pp. 244–258.

[22] Q. Wang, S. Wu, M. Li, Software defect prediction, Journal of Software

19 (7) (2008) 1565–1580.

[23] X. Wang, S. Jiang, P. Gao, K. Lu, B. Lili, X. Ju, Y. Zhang, Fuzzy c-means
clustering based multi-fault localization, Chinese Journal of Computers
43 (2) (2020) 206–232.

[24] F. Keller, L. Grunske, S. Heiden, A. Filieri, A. van Hoorn, D. Lo, A criti-
cal evaluation of spectrum-based fault localization techniques on a large-
scale software system, in: 2017 IEEE International Conference on Soft-
ware Quality, Reliability and Security (QRS), IEEE, 2017, pp. 114–125.
[25] Y.-H. Wu, Z. Li, Y. Liu, X. Chen, Fatoc: Bug isolation based multi-fault
localization by using optics clustering, Journal of Computer Science and
Technology 35 (5) (2020) 979–998.

[26] M. Golagha, C. Lehnhoﬀ, A. Pretschner, H. Ilmberger, Failure clustering
without coverage, in: Proceedings of the 28th ACM SIGSOFT Interna-
tional Symposium on Software Testing and Analysis, 2019, pp. 134–145.
[27] N. DiGiuseppe, J. A. Jones, Concept-based failure clustering, in: Pro-
ceedings of the ACM SIGSOFT 20th international symposium on the

17

foundations of software engineering, 2012, pp. 1–4.

1–40.

[28] A. Podgurski, D. Leon, P. Francis, W. Masri, M. Minch, J. Sun, B. Wang,
Automated support for classifying software failure reports, in: 25th Inter-
national Conference on Software Engineering, 2003. Proceedings., IEEE,
2003, pp. 465–475.

[29] F. Steimann, M. Frenkel, Improving coverage-based localization of mul-
tiple faults using algorithms from integer linear programming, in: 2012
IEEE 23rd International Symposium on Software Reliability Engineering,
IEEE, 2012, pp. 121–130.

[30] T. Reps, T. Ball, M. Das, J. Larus, The use of program proﬁling for soft-
ware maintenance with applications to the year 2000 problem, in: Soft-
ware Engineering—Esec/Fse’97, Springer, 1997, pp. 432–449.

[31] M. J. Harrold, G. Rothermel, K. Sayre, R. Wu, L. Yi, An empirical in-
vestigation of the relationship between spectra diﬀerences and regression
faults, Software Testing, Veriﬁcation and Reliability 10 (3) (2000) 171–
194.

[32] R. Abreu, P. Zoeteweij, A. J. Van Gemund, An evaluation of similar-
ity coeﬃcients for software fault localization, in: 2006 12th Paciﬁc Rim
International Symposium on Dependable Computing (PRDC’06), IEEE,
2006, pp. 39–46.

[33] Y. Huang, J. Wu, Y. Feng, Z. Chen, Z. Zhao, An empirical study on clus-
tering for isolating bugs in fault localization, in: 2013 IEEE International
Symposium on Software Reliability Engineering Workshops (ISSREW),
IEEE, 2013, pp. 138–143.

[34] X. Xu, V. Debroy, W. Eric Wong, D. Guo, Ties within fault localization
rankings: Exposing and addressing the problem, International Journal of
Software Engineering and Knowledge Engineering 21 (06) (2011) 803–
827.

[35] H. Do, S. Elbaum, G. Rothermel, Supporting controlled experimentation
with testing techniques: An infrastructure and its potential impact, Em-
pirical Software Engineering 10 (4) (2005) 405–435.

[36] R. Just, D. Jalali, M. D. Ernst, Defects4j: A database of existing faults to
enable controlled testing studies for java programs, in: Proceedings of the
2014 International Symposium on Software Testing and Analysis, 2014,
pp. 437–440.

[37] M. Papadakis, M. Kintis, J. Zhang, Y. Jia, Y. Le Traon, M. Harman, Muta-
tion testing advances: an analysis and survey, in: Advances in Computers,
Vol. 112, Elsevier, 2019, pp. 275–378.

[38] J. H. Andrews, L. C. Briand, Y. Labiche, Is mutation an appropriate tool
for testing experiments?, in: Proceedings of the 27th international confer-
ence on Software engineering, 2005, pp. 402–411.

[39] H. Do, G. Rothermel, On the use of mutation faults in empirical assess-
ments of test case prioritization techniques, IEEE Transactions on Soft-
ware Engineering 32 (9) (2006) 733–752.

[40] C. Liu, L. Fei, X. Yan, J. Han, S. P. Midkiﬀ, Statistical debugging: A
hypothesis testing-based approach, IEEE Transactions on software engi-
neering 32 (10) (2006) 831–848.

[41] J. H. Andrews, L. C. Briand, Y. Labiche, A. S. Namin, Using muta-
tion analysis for assessing and comparing testing coverage criteria, IEEE
Transactions on Software Engineering 32 (8) (2006) 608–624.

[49] S. Yoo, X. Xie, F.-C. Kuo, T. Y. Chen, M. Harman, Human competitive-
ness of genetic programming in spectrum-based fault localisation: Theo-
retical and empirical analysis, ACM Transactions on Software Engineer-
ing and Methodology (TOSEM) 26 (1) (2017) 1–30.

[50] W. E. Wong, V. Debroy, D. Xu, Towards better fault localization: A
crosstab-based statistical approach, IEEE Transactions on Systems, Man,
and Cybernetics, Part C (Applications and Reviews) 42 (3) (2011) 378–
396.

[51] W. E. Wong, V. Debroy, R. Gao, Y. Li, The dstar method for eﬀective
software fault localization, IEEE Transactions on Reliability 63 (1) (2013)
290–308.

[52] X. Xie, F.-C. Kuo, T. Y. Chen, S. Yoo, M. Harman, Provably optimal
and human-competitive results in sbse for spectrum based fault localisa-
tion, in: International Symposium on Search Based Software Engineer-
ing, Springer, 2013, pp. 224–238.

[53] M. Y. Chen, E. Kiciman, E. Fratkin, A. Fox, E. Brewer, Pinpoint: Prob-
lem determination in large, dynamic internet services, in: Proceedings
International Conference on Dependable Systems and Networks, IEEE,
2002, pp. 595–604.

[54] J. A. Jones, M. J. Harrold, Empirical evaluation of the tarantula automatic
fault-localization technique, in: Proceedings of the 20th IEEE/ACM inter-
national Conference on Automated software engineering, 2005, pp. 273–
282.

[55] H. J. Lee, L. Naish, K. Ramamohanarao, Study of the relationship of
bug consistency with respect to performance of spectra metrics, in: 2009
2nd IEEE International Conference on Computer Science and Informa-
tion Technology, IEEE, 2009, pp. 501–508.

[56] B. Liblit, M. Naik, A. X. Zheng, A. Aiken, M. I. Jordan, Scalable statisti-

cal bug isolation, Acm Sigplan Notices 40 (6) (2005) 15–26.

[57] W. E. Wong, Y. Qi, L. Zhao, K.-Y. Cai, Eﬀective fault localization using
code coverage, in: 31st Annual International Computer Software and Ap-
plications Conference (COMPSAC 2007), Vol. 1, IEEE, 2007, pp. 449–
456.

[58] S. Pearson, J. Campos, R. Just, G. Fraser, R. Abreu, M. D. Ernst,
D. Pang, B. Keller, Evaluating and improving fault localization,
in:
2017 IEEE/ACM 39th International Conference on Software Engineer-
ing (ICSE), IEEE, 2017, pp. 609–620.

[59] A. Arrieta, S. Segura, U. Markiegi, G. Sagardui, L. Etxeberria, Spectrum-
based fault localization in software product lines, Information and Soft-
ware Technology 100 (2018) 18–31.

[60] X. Sun, X. Peng, B. Li, B. Li, W. Wen, Ipsetful: an iterative process of
selecting test cases for eﬀective fault localization by exploring concept
lattice of program spectra, Frontiers of Computer Science 10 (5) (2016)
812–831.

[61] N. Mottaghi, M. R. Keyvanpour, Test suite reduction using data mining
techniques: A review article, in: 2017 International Symposium on Com-
puter Science and Software Engineering Conference (CSSE), IEEE, 2017,
pp. 61–66.

[62] Y. Lei, C. Sun, X. Mao, Z. Su, How test suites impact fault localisation

[42] M. Pradel, K. Sen, Deepbugs: A learning approach to name-based bug

starting from the size, IET software 12 (3) (2018) 190–205.

detection. pacmpl 2, oopsla (2018), 147: 1–147: 25 (2018).

[43] R. Just, D. Jalali, L. Inozemtseva, M. D. Ernst, R. Holmes, G. Fraser, Are
mutants a valid substitute for real faults in software testing?, in: Proceed-
ings of the 22nd ACM SIGSOFT International Symposium on Founda-
tions of Software Engineering, 2014, pp. 654–665.

[44] S.-M. Lamraoui, S. Nakajima, A formula-based approach for automatic
fault localization of multi-fault programs, Journal of Information Process-
ing 24 (1) (2016) 88–98.

[45] G. An, J. Yoon, S. Yoo, Searching for multi-fault programs in defects4j,
International Symposium on Search Based Software Engineering,

in:
Springer, 2021, pp. 153–158.

[46] H. A. de Souza, M. L. Chaim, F. Kon, Spectrum-based software fault
localization: A survey of techniques, advances, and challenges, arXiv
preprint arXiv:1607.04347.

[47] L. Naish, H. J. Lee, K. Ramamohanarao, A model for spectra-based soft-
ware diagnosis, ACM Transactions on software engineering and method-
ology (TOSEM) 20 (3) (2011) 1–32.

[48] X. Xie, T. Y. Chen, F.-C. Kuo, B. Xu, A theoretical analysis of the risk
evaluation formulas for spectrum-based fault localization, ACM Transac-
tions on Software Engineering and Methodology (TOSEM) 22 (4) (2013)

[63] A. Perez, R. Abreu, A. van Deursen, A test-suite diagnosability metric for
spectrum-based fault localization approaches, in: 2017 IEEE/ACM 39th
International Conference on Software Engineering (ICSE), IEEE, 2017,
pp. 654–664.

[64] W. Fu, H. Yu, G. Fan, X. Ji, X. Pei, A test suite reduction approach to
improving the eﬀectiveness of fault localization, in: 2017 International
Conference on Software Analysis, Testing and Evolution (SATE), IEEE,
2017, pp. 10–19.

[65] J. Wu, H. Xiong, J. Chen, Adapting the right measures for k-means clus-
tering, in: Proceedings of the 15th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, 2009, pp. 877–886.
[66] P.-N. Tan, M. Steinbach, V. Kumar, Introduction to data mining, Pearson

Education India, 2016.

[67] J. XIE, Y. ZHOU, M. WANG, W. JIANG, New criteria for evaluating the
validity of clustering, CAAI Transactions on Intelligent Systems 12 (6)
(2017) 873–882.

[68] Y. Huang, A. Flynt, Exploration of common clustering methods and the
behavior of certain agreement indices, Ball State Undergraduate Mathe-
matics Exchange 12 (1) (2018) 35–50.

[69] C. M. Tang, W. Chan, Y. T. Yu, Z. Zhang, Accuracy graphs of spectrum-

18

based fault localization formulas, IEEE Transactions on Reliability 66 (2)
(2017) 403–424.

[70] Y. Pang, X. Xue, A. S. Namin, Debugging in parallel or sequential: An

empirical study, J. Softw. 10 (5) (2015) 566–576.

[71] Y. Zheng, Z. Wang, X. Fan, X. Chen, Z. Yang, Localizing multiple soft-
ware faults based on evolution algorithm, Journal of Systems and Soft-
ware 139 (2018) 107–123.

[72] T. Y. Chen, S. C. Cheung, S. M. Yiu, Metamorphic testing: a new ap-

proach for generating next test cases, arXiv preprint arXiv:2002.12543.

[73] X. Xie, W. E. Wong, T. Y. Chen, B. Xu, Metamorphic slice: An applica-
tion in spectrum-based fault localization, Information and Software Tech-
nology 55 (5) (2013) 866–879.

[74] W. H¨ogerle, F. Steimann, M. Frenkel, More debugging in parallel, in:
2014 IEEE 25th International Symposium on Software Reliability Engi-
neering, IEEE, 2014, pp. 133–143.

[75] A. Zakari, S. P. Lee, Parallel debugging: An investigative study, Journal

of Software: Evolution and Process 31 (11) (2019) e2178.

[76] C. Liu, J. Han, Failure proximity: a fault localization-based approach,
in: Proceedings of the 14th ACM SIGSOFT international symposium on
Foundations of software engineering, 2006, pp. 46–56.

[77] A. Zakari, S. P. Lee, C. Y. Chong, Simultaneous localization of software
faults based on complex network theory, IEEE Access 6 (2018) 23990–
24002.

[78] A. Zakari, S. P. Lee, I. A. T. Hashem, A community-based fault isolation
approach for eﬀective simultaneous localization of faults, IEEE Access 7
(2019) 50012–50030.

[79] H. Pei, B. Yin, M. Xie, K.-Y. Cai, Dynamic random testing with test
case clustering and distance-based parameter adjustment, Information and
Software Technology 131 (2021) 106470.

[80] M. Gao, P. Li, C. Chen, Y. Jiang, Research on software multiple fault
localization method based on machine learning, in: MATEC web of con-
ferences, Vol. 232, EDP Sciences, 2018, p. 01060.

[81] A. Zakari, S. P. Lee, R. Abreu, B. H. Ahmed, R. A. Rasheed, Multiple
fault localization of software programs: A systematic literature review,
Information and Software Technology 124 (2020) 106312.

19

