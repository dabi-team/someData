2
2
0
2

r
p
A
7

]
h
p
-
t
n
a
u
q
[

2
v
0
1
7
0
0
.
4
0
2
2
:
v
i
X
r
a

Improving quantum state detection with adaptive sequential
observations

Shawn Geller,1, 2 Daniel C. Cole,1, ∗ Scott Glancy,1 and Emanuel Knill1, 3
1National Institute of Standards and Technology, Boulder, Colorado 80305, USA
2Department of Physics, University of Colorado, Boulder, Colorado 80309, USA
3Center for Theory of Quantum Matter,
University of Colorado, Boulder, Colorado 80309, USA

For many quantum systems intended for information processing, one detects the
logical state of a qubit by integrating a continuously observed quantity over time.
For example, ion and atom qubits are typically measured by driving a cycling tran-
sition and counting the number of photons observed from the resulting ﬂuorescence.
Instead of recording only the total observed count in a ﬁxed time interval, one can
observe the photon arrival times and get a state detection advantage by using the
temporal structure in a model such as a Hidden Markov Model. We study what
further advantage may be achieved by applying pulses to adaptively transform the
state during the observation. We give a three-state example where adaptively cho-
sen transformations yield a clear advantage, and we compare performances on an
ion example, where we see improvements in some regimes. We provide a software
package that can be used for exploration of temporally resolved strategies with and
without adaptively chosen transformations.

1.

INTRODUCTION

Quantum information processing requires high ﬁdelity single-shot readout of states. In a
typical example, readout of the state of an ion or atom qubit is performed by observing the
ﬂuorescence from driving a cycling transition. For a qubit whose states are superpositions
of two atomic levels, a goal is to distinguish between the two levels with the highest possible
ﬁdelity. A common way to deﬁne the ﬁdelity is as the average probability of correctly deter-
mining the level when the levels are prepared uniformly randomly. High ﬁdelity readout is
helpful in quantum error-correction for syndrome measurements to minimize the probabil-
ity of misidentifying errors. High ﬁdelity readout can also signiﬁcantly reduce the number
of measurements needed to characterize states or processes in quantum tomography. The
improvement can be substantial when the states measured have high ﬁdelity with respect
to a target state such as a Bell state [1].

A standard readout method for ‘bright’ and ‘dark’ atomic levels (labeled |b(cid:105) and |d(cid:105)
respectively) is to drive a cycling transition such that |b(cid:105) ﬂuoresces while |d(cid:105) does not. The
emitted photons are detected with some eﬃciency, and the output of the measurement is
the number of photons detected. The presence of background photons, unwanted transitions
between states and the desire to have short observation times prevent arbitrarily high ﬁdelity
measurement. The atom is inferred to be in the state |b(cid:105) if the number of photons detected
exceeds a threshold, otherwise it is inferred to be in the state |d(cid:105). One strategy to improve the

∗ Current address: ColdQuanta, Inc., Boulder, Colorado 80301, USA

 
 
 
 
 
 
2

readout ﬁdelity is to use repetitive readout with ancillary atoms as demonstrated in Ref. [2].
A less demanding strategy is to record and use the arrival time of photons as discussed
in Ref. [3]. The arrival-time record can also be used with machine learning strategies to
improve readout, as demonstrated in Ref. [4–8]. All these methods take advantage of the
fact that measurements are processes and can yield a time-resolved record that contains
valuable information about the initial logical state. While we focus on atoms and ﬂuorescence
measurement, this situation is also common for other systems being investigated for quantum
information processing, such as superconducting qubits [9].

The measurement processes of interest are well described by hidden Markov models
(HMMs)[10]. An HMM is a stochastic discrete-time process on a ﬁnite number of “hid-
den” states with stochastic output. It is characterized by a ﬁnite state space S, an output
space Y, a stochastic transition matrix A that describes the probabilities of transitioning
from the current state to the next in a time step, an output process matrix B that determines
the probability of an output given the current state, and an initial state distribution ν that
determines the probability of the HMM starting in a particular state. For applications to
quantum measurement, the goal is to infer the starting state of the HMM from the sequence
of outputs observed during the process. It is usually possible to infer the parameters of the
HMM by ﬁtting to experimental data using standard algorithms [11], and one can assist
such an algorithm by initializing it with parameters that are motivated by theory. Once the
parameters are known, statistical decision methods for HMMs can be applied to estimate
the initial state, and this has been demonstrated in several recent works [12–15].

But it is possible to do even better if the systems of interest permit suﬃcient control to
modify the steps during the measurement. For atoms, the simplest such modiﬁcation is to
apply pulses between otherwise identical steps to permute the levels. This can improve the
measurement ﬁdelity or shorten average measurement times by taking advantage of the fact
that not all levels are equally distinguishable by their output ﬂuorescence. For example, two
non-ﬂuorescing levels are diﬃcult to distinguish, but if the output observed so far suggests
that the atom was prepared in a non-ﬂuorescing level, we can swap one of these levels with
a ﬂuorescing one to learn which one was present initially, provided the measurement process
has not yet induced transitions between levels. While in principle swaps can be applied
in any system that permits such control, we expect them to be most useful for improving
detection ﬁdelity in systems where the time it takes to apply a swap is small compared to
the lifetime of the system. An example of such a system, which we discuss in this paper, is
a 9Be+ hyperﬁne qubit with microwave controls.

An improvement in measurement ﬁdelity by applying a ﬁxed swap was demonstrated
in [16, 17], where the authors use a single π pulse to implement a level swap in the middle
of the readout cycle and postselect on the event that the two halves of the readout cycle
give diﬀerent outputs. Strategies that use postselection can provide conditional advantages
in measurement ﬁdelity over those that do not, but we do not consider postselected mea-
surement ﬁdelities here. In this work, rather than using only a ﬁxed swap, we formalize and
explore measurement processes with state permutations adaptively chosen during a single
readout cycle based on the outputs observed so far in that cycle.

To support exploration of measurement processes with adaptively chosen state permuta-
tions, we developed a software package to simulate and optimize policies for choosing permu-
tations [18]. We consider two examples to illustrate and compare measurement strategies.
The ﬁrst is a 9Be+ ion, and the second is an idealized three-state example. For these two
examples, we computed the optimal policy for small numbers of steps, and for the 9Be+

3

example, we compared the performance of a heuristic policy that is easier to compute than
the optimal one. Improvements for the 9Be+ example are moderate, but they illustrate what
can be achieved with adaptive control policies in an experimentally relevant system. Im-
provements for the three-state example are more signiﬁcant, showing that adaptive policies
can be very useful in some cases.

The heuristic policy is suboptimal for a range of parameters, demonstrating that heuristic
approaches can yield undesirable results. It is possible to formulate the problem in terms
of partially observed Markov decision processes (POMDPs) [19], which generalize HMMs
by including output-dependent actions. In general, such problems are hard [20], but ap-
proximate solutions exist [21]. Because the problems considered here are small, we did not
need to apply approximate solutions, but applications of the proposed techniques to other
systems with larger numbers of steps or larger output spaces will require approximation.

In Sect. 2, we describe the problem of initial state inference under repeated measure-
ments, subject to adaptive control of the underlying states. We discuss requirements for
an experimental implementation of the proposed protocol in Sect. 3. The two examples
are introduced and analyzed in Sect. 4 and Sect. 5. We discuss the results and suggest
extensions of this work in Sect. 6, and we oﬀer concluding comments in Sect. 7. Mathe-
matical formalisms, and technical descriptions of policies and their computations are in the
appendices.

2. STATEMENT OF PROBLEM

Consider a generic atom being measured by observing ﬂuorescence from driving a cycling
transition. We can divide the measurement into equal-time intervals referred to as ‘steps’.
For each step, we record the number of photons detected. Many measurement conﬁgurations
have the property that the system observed can be treated as decohered in the measurement
basis before each measurement step. For atoms, the measurement basis is determined by the
atomic levels, and the eﬀective decoherence is a consequence of the dynamics of the levels
and the nature of the driving light. Here we assume that the levels are non-degenerate, and
each level can be treated as a single quantum state.

When decoherence between the levels does not arise naturally, in many cases it is possible
to enforce this decoherence and eliminate memory of coherences between levels by active
means such as random phase changes or appropriately randomized pulses between steps.
Given the lack of phase memory, the action of a step has a classical description. In particular,
the initial quantum state is a probability distribution over the levels S that constitute the
measurement basis elements. The output and next level have a joint probability distribution
that depends on the current level.

Under these conditions the problem is reduced to a classical one, and we describe the
measurement dynamics with a Hidden Markov Model (HMM), which is a discrete-time
stochastic process. From here on “state” refers to a classical deterministic state, and we
use probability distributions over states and outputs to describe processes. We describe the
mathematics of HMMs in App. B, and an exposition of how to describe measurement of a
9Be+ hyperﬁne qubit in terms of an HMM is given in App. C.

The description of a sequence of n measurement steps requires the sequence of states sn =
(s1, . . . , sn) and the sequence of outputs or observations yn = (y1, . . . , yn). The corresponding
random variables (RVs) are denoted by Sn and Y n. We use the usual convention that RVs
are denoted by capital letters and their values by the corresponding lower-case letters. For

4

any sequence xn, we use xj to denote the initial subsequence that has j elements, x1, . . . , xj.
After observing the outputs yn, we make an inference of the state occupied at the be-
ginning of the ﬁrst step, also called the initial state. The random variable for the inferred
initial state is denoted ˆS1. Using the HMM, we apply the maximum likelihood method to
infer the initial state. To characterize the performance of our inference method, we use the
inﬁdelity P(S1 (cid:54)= ˆS1). The symbol P denotes “probability of” and its argument is an event.
To compute the inﬁdelity, we use the fact that the inﬁdelity is the expectation value of the
indicator function of the event that the inference is incorrect, averaged over the prior P(s1)
and over the model P(yn|s1) of the event that the inference is incorrect. That is,

P(S1 (cid:54)= ˆS1) =

(cid:88)

yn,s1

I[s1 (cid:54)= ˆs1(yn)]P(yn|s1)P(s1),

(2.1)

where the indicator function I[E] ∈ {0, 1} is 1 if and only if E is true. As discussed in App. D,
maximum likelihood inference minimizes the inﬁdelity for the uniform prior. Eq. 2.1 can be
computed exactly using the HMM if the number of steps is not too large.

· · ·

σk−1

· · ·

sk−1

· · ·

yk−1

σk

sk

yk

σk+1

· · ·

sk+1

· · ·

yk+1

· · ·

FIG. 1. A schematic diagram describing the problem considered in this work. The arrows in the
diagram indicate the dependencies between the diﬀerent variables. The states sk are identiﬁed
with the classical state of the experimental system during step k of the measurement process, the
outcomes yk are the corresponding outcomes of each measurement step, and the permutations
σk of each state sk are chosen adaptively in each step. The dependence of σk on y1, . . . , yk−1 is
not depicted. This dependence is used in the experimenter’s rule for choosing permutations to
apply. In this work, we study the problem of ﬁnding σk’s that minimize the probability (Eq. 2.1)
of incorrectly inferring the initial state s1.

It is possible to modify the measurement sequence by using the data observed thus far
to act on the system using controls of the underlying states. We specialize to a set A of
permutations of underlying states, usually implemented with π-pulses between the states.
As shown in Fig. 1, at each step, we apply a permutation of the underlying states (this can
be the identity permutation) that depends on the data seen thus far. A rule that determines
which permutation to apply at each step is called a policy. We would like to minimize the
inﬁdelity Eq. 2.1 over the space of all possible policies. In general, computing the optimal
policy requires an exponential time calculation. This motivates the search for policies that
are easier to compute, but that are possibly suboptimal. One possible policy is to choose
the permutation that maximizes the mutual information between the initial state and the

5

next g outputs. We call this the minimum posterior entropy heuristic. The mathematical
descriptions of the HMM with adaptive permutations, the optimal permutation policy, and
the minimum posterior entropy heuristic policy are given in App. E.

In Sect. 4 we compare the performance of the optimal policy and the minimum posterior
entropy heuristic policy for a certain set of permutations to a policy that applies no per-
mutations in the context of a 9Be+ ion qubit under repeated ﬂuorescence measurement. In
Sect. 5, we compare the performance of the optimal policy to that of the trivial policy that
does not apply permutations in an idealized three state example.

3. PROSPECTS FOR EXPERIMENTAL IMPLEMENTATION

In this work, we focus on the 9Be+ hyperﬁne qubit as a motivating example. The ﬂuores-
cence detection process in 9Be+ can be paused; by turning oﬀ the detection laser, the qubit
retains its state for a very long time. By turning oﬀ the laser, applying the permutation
pulse, then turning the laser back on, we can justify the assumption that the permutations
occur instantaneously. In other systems such as superconducting qubits, the instantaneous
action assumption does not hold. To account for the ﬁnite duration of actions, one can use
the formalism of partially observable Markov decision processes (POMDPs) [19] to account
for the possibility that the transition probabilities and the output probabilities directly de-
pend on the action. The connection between the problem considered in this work with
POMDPs is discussed in App. E.

The policies presented in this work are computed in advance. The advantage of computing
a policy in advance is that it can be turned into a look-up table, so that the logic involved
can be implemented on a ﬁeld-programmable gate array (FPGA). One downside is that if the
number of steps is too large, the look-up table can grow to the point that it cannot be stored
in memory. Another strategy is to compute a policy as the data comes in. Such strategies
are not memory-limited, but have the downside that typically ﬂoating-point calculations are
involved, making them diﬃcult to implement and run them in real time on an FPGA.

4. EXAMPLE: 9Be+ HYPERFINE QUBIT

The 9Be+ ground-state hyperﬁne qubit is measured by distinguishing a level that ﬂuo-
resces from ones that do not. The level that ﬂuoresces does so by participating in a cycling
transition, driven by a detection laser. A cycling transition involves driving the internal
state to one that is outside the ground-state manifold, which in turn immediately decays
back to the state from which it came, emitting a ﬂuorescence photon in the process. Ideally,
the other states remain undisturbed in this process. In reality, imperfect polarization of the
detection laser can drive transitions from the bright state to dark states, while oﬀ-resonant
driving leads to transitions between other pairs of levels. Since we only consider the case of
perfect polarization in this work, if the ion ever enters the bright state, it remains there for
the rest of the measurement sequence.

A level diagram of the 9Be+ system is in Fig. 2. The ﬂuorescing, ‘bright’, level is (F, mF ) =
(2, 2), and the level with the least probability of transitioning to (2, 2) is (1, −1), so we use
these two levels as possible initial states, with the uniform prior P(s1 = (2, 2)) = P(s1 =
(1, −1)) = 1/2. The rate equations for this system are derived in [22]. The HMM parameters
were obtained by integrating the rate equations for a 9Be+ qubit tuned to the ﬁrst-order

6

FIG. 2. The ground state hyperﬁne manifold of the 9Be+ ion in an applied magnetic ﬁeld. The
dashed arrows show levels that diﬀer by ∆F = ±1, and ∆mF = 0, ±1. Also indicated are the
|dark(cid:105) and |bright(cid:105) levels, which are the levels on which we assume a uniform prior. The prior
assigns zero probability to all other levels. All unlabeled levels ﬂuoresce at the same low rate as
the |dark(cid:105) level. Also indicated are the hyperﬁne splitting of approximately 1.2 GHz between the
F = 1 and F = 2 levels, and the Zeeman splitting of approximately 80 MHz at 0.0119 T. By
composing pairwise swaps along the arrows in the order shown, we can move the dark state to the
bright state. This is the permutation τ considered in the main text. Reversing the order gives its
inverse τ −1. When constructing policies, we allow the actions (cid:8)τ, τ −1, (cid:15)(cid:9) where (cid:15) is the identity
permutation.

magnetic-ﬁeld-insensitive point for the |F = 1, mF = 1(cid:105) ↔ |F = 2, mF = 0(cid:105) transition [3,
23]. The rate equations are parameterized by the background detected photon rate γbg,
the bright-state detected photon rate γc, the fractions of σ− and π polarizations of the
detection laser, and the on-resonance saturation parameter for the cycling transition r that
parameterizes the number of scattered photons per unit time. To simplify the equations
we assume that the polarization of the detection laser is perfectly σ+. We set the other
parameters for the rate equation as follows: γc = 30 photons per 330 µs, γbg = 0.06γc, and
r = 1/2.

By integrating the rate equations for a time per step ∆t, we derive an HMM that describes
the repeated measurement setup, as detailed in App. C. We display the resulting transition
matrices in Fig. 3, for diﬀerent values of ∆t considered. We note that in Fig. 3 (b), we
observe large probabilities of transitioning between states. This is to be expected, as the
detection laser is assumed to have σ+ polarization, which leads to oﬀ-resonant driving of
population to states with larger mF .

Because the complexity of the calculations grows very rapidly with the number of possible
outcomes, it is expedient to modify the output distributions to have fewer possible outputs.
We reduce the possible outputs by identifying sets of numbers of photons collected in a step,
and keeping only the information of which set the photon count lies in. A set of photon
numbers is called a ‘bin’. To select a set of bins, we minimized the inﬁdelity over possible
choices of bins, at n = 6 steps, when applying no permutations, for nb = 4 bins. To reduce
computation time, we restrict the search to bins containing consecutive numbers of photons.
The optimization for selecting bins is presented formally in App. C. We show the resulting
binned distributions in Fig. 4 for the largest ∆t considered. We performed this optimization
separately for each ∆t we considered. Once we ﬁnd the optimal binning, we use the same
binning for all the diﬀerent policies considered.

The set of actions A used consists of the permutations τ , which sends the dark level to

7

FIG. 3. The entrywise log10 of the transition matrix between levels in the ground state hyperﬁne
manifold of a 9Be+ ion. If a transition is forbidden by the assumption of perfect polarization, its
entry R(j|i) = 0 is shown in white. The rows correspond to the state before the transition, while
the columns correspond to that after. The labels indexing the rows and columns of the matrix
refer to (F, mF ) labels for the ion levels. (a) shows the transition matrix at the smallest time per
step ∆t considered, while (b) shows that at the largest ∆t considered.

the bright level, its inverse τ −1, and (cid:15) the identity permutation. The permutation τ is a
composition of three pairwise swaps as shown in Fig. 2. The pairwise swaps that are used in
creating τ are a subset of the transitions allowed by selection rules that are experimentally
convenient to implement. While in principle we could add more actions to the set, we found
that adding more actions does not appreciably improve measurement ﬁdelity. We assume
that the error introduced by applying actions is negligible. In the 9Be+ system, the state of
the system is very well preserved when the detection laser is turned oﬀ. By assuming that
the detection laser is turned oﬀ when the permutations are applied, we can treat the actions
as eﬀectively happening instantaneously.

We wish to analyze the eﬀectiveness of using permutation actions in improving measure-
ment ﬁdelity. For n = 6 steps, we are able to compute the optimal policy (see App. E) using
nb = 4 bins. However, the computational work required in computing the optimal policy
grows as O((|A||Y|)n) so this computation becomes intractable as the number of bins |Y|
or the number of steps n grows. We are thus also interested in the performance of heuristic
policies that are more easily computed, especially in the regime where they can be compared
to the optimal policy. An heuristic policy we choose for this comparison is the minimum
posterior entropy policy that minimizes the average entropy of the initial state conditional
on the next g observations, as detailed in App. E. Here we pick g = 2. To understand the
eﬀectiveness of using active permutation policies, we would like to compare these policies
to the policy of applying no actions at all. To demonstrate the gain in ﬁdelity achieved by
using timing information, we also compare to a discrimination method that uses only aggre-
gate probability of collecting numbers of photons over a total detection period, a method
we refer to as the histogram method. We do not bin the distributions of total photons when
computing inﬁdelities for the histogram method.

8

FIG. 4. Bar charts of the probabilities of the various measurement outcomes, for a time per step
∆t of 53.9µs. This is the largest ∆t that we consider in Fig. 5. On the left in subﬁgures (a) and
(c), we show the unbinned distributions for collecting n photons, conditioned on being in the state
(F, mF ) = (1, −1) and (F, mF ) = (2, 2) respectively. These states are referred to as “dark” and
“bright” in Fig. 2, and the initial state distribution is uniform over these two states. On the right
in subﬁgures (b) and (d) are the binned versions of the distributions shown in (a) and (c) obtained
by choosing non-uniform bins for the histogram counts of the number of photons observed. The
vertical black bars deﬁne the edges of the bins, and summing the probabilities of outcomes that lie
within a bin yields the probability of observing that bin.

To summarize, the four methods we compare are as follows.

Name
Histogram
No Perms

Permutation policy
None applied
None applied

Min Entropy Minimize expected entropy of next two observations
Exhaustive

Minimize inﬁdelity

Model
Total counts
HMM
HMM
HMM

(4.1)

The “Model” column indicates which model is used for the diﬀerent methods. “Total counts”
means that the model ignores timing information, instead using just the total number of
photons observed. “HMM” means that the model used is an HMM with a binned output
distribution. We infer the initial state using maximum likelihood in every method, which
incorporates the corresponding model and applied permutations.

We have plotted in Fig. 5 the measurement inﬁdelity P( ˆS1 (cid:54)= S1) of various methods of
estimating the initial state, as a function of n∆t. For very small total times, transitions
are rare overall, and therefore there is nothing to be gained by performing the time-resolved
readout as we do in this work; it suﬃces to instead use the histogram method that performs

9

FIG. 5.
A comparison of various measurement inﬁdelities (Eq. 2.1) under the uniform prior
over the two states (F, mF ) = (2, 2) and (F, mF ) = (1, −1), for a range of collection times. The
inﬁdelities are computed after binning the output distributions into 4 bins (except the Histogram
method, which is not binned), for 6 steps of the HMM. Subﬁgures (a), (b), and (c) all show the same
inﬁdelities plotted for diﬀerent time ranges with re-scaled vertical axes to highlight the relevant
features. The dash-dotted red curve, labeled “Histogram”, is the inﬁdelity when we do not apply
permutations and do not time-resolve the measurements into steps. This involves collecting photons
for the time shown and determining the measurement outcome based on the total number collected.
We note that in subﬁgure (c), Histogram curve begins to curve upwards. The dashed green curve
is the inﬁdelity obtained by using an HMM, without applying any actions. In subﬁgure (a), it
agrees with the Histogram curve, but in subﬁgure (c), it outperforms the Histogram curve. The
solid blue curve is the exhaustive search policy, which is guaranteed to be optimal. It outperforms
all other curves in the ﬁgure, but has essentially no gain over the No Perms curve in subﬁgure
(c). The dotted orange curve is the minimum posterior entropy heuristic policy, which performs
well in subﬁgure (a), performs much worse than all other curves in subﬁgure (b), then recovers
performance in subﬁgure (c) end, coinciding with the No Perms and Exhaustive curves there. The
Min Entropy policy was computed with a look ahead of g = 2 steps.

state classiﬁcation based solely on the aggregate number of photons. As shorthand, we refer
to the various curves in Fig. 5 by their legends.

Note that the regime of small times leads to large measurement error for any strategy
chosen. At around 20µs, the Min Entropy and Exhaustive curves diverge from the other
two. Here there is a gain due to applying the permutations, but the histogram method still
coincides with the HMM method that does not apply permutations. Near 70µs, the Min
Entropy curve diverges from the Exhaustive curve, indicating that our heuristic minimum
posterior entropy policy underperforms the optimal one. At about 100µs, the minimum
posterior entropy heuristic underperforms even the histogram method. At around 145µs,
the Min Entropy curve rejoins the No Perms curve. Near 155µs, the No Perms and Histogram
curves slightly diverge, indicating that our method of binning the outcomes into 4 bins causes
us to incur a loss of distinguishability. Close to 200µs, the Min Entropy and No Perms curves

10

diverge from the Histogram curve, indicating that it is worth using a time-resolved method
for inference in this regime. Finally, beyond this point, the non-histogram methods begin
to coincide. This indicates that there is nothing further to be gained from active policies for
large ∆t.

We see that the Min-entropy heuristic policy performs unpredictably in the diﬀerent
regimes. This is due to the fact that it does not incorporate information from all possible
future paths. If the policy diverges from the optimal policy at any step, the resulting states
that are realized are diﬀerent, and the likelihoods of diﬀerent possible yn change dramatically.
The best method to check for the performance of a heuristic policy is to directly calculate
the inﬁdelity, as we do in this paper.

5. EXAMPLE: THREE-STATE MODEL

As a toy example, we consider a simple three-state model that illuminates the advantages
that can be gained with permutation policies. The model is shown in Fig. 6. The model has
three states S = {0, 1, 2} and three possible outputs Y = {0, 1, 2}.

To keep the number of HMM parameters low, it is symmetric under interchange of the
labels 0 and 2 of both states and outputs. The transition rates and probability of outcomes
shown in Fig. 6 depend on two parameters, a and b, which are intended to be small compared
to 1. In this case, the state s = 1 has a short lifetime in terms of the number of steps and
quickly transitions to s = 0 or s = 2 with equal probability, at a much larger rate than 0
and 2 transitioning to 1. States 0 and 2 do not transition directly between each other and
have small probabilities of transitioning back to s = 1 compared to the rates out of s = 1.
The output y = 0 is only possible for states s = 0 and s = 1, while the output y = 2 is
only possible for s = 1 and s = 2. The output y = 1 is equally likely for all states, and has
low probability. Thus the output y = 0 excludes s = 2 and y = 2 excludes s = 0, while
y = 1 has no information about which state produced it. We assume that the initial state
distribution is uniform on the three states and aim to infer the initial state from the outputs
produced after up to six steps.

We computed the measurement ﬁdelities both for the policy without permutations and
the policy with the permutations selected according to the optimal policy at n = 6 steps
for a range of parameters a and b. Here the permutations we allow are the transpositions
between any two states. We choose to stop after n = 6 steps because we found that the
measurement inﬁdelity changes very little after reaching 6 steps, for the range of parameters
shown in Fig. 7.

The gain in distinguishability is shown in Fig. 7 (b), which shows the ratio of the mea-
surement inﬁdelities without and with the permutation policy. We see that for small a and
small b this ratio is large, demonstrating the relative improvement achieved. We can reason
through this gain in distinguishability as follows. Consider the case where the ﬁrst obser-
vation is y1 = 0. This excludes the possibility that s1 = 2, but the posterior distribution
has support on both s0 (P(S1 = 0|y1 = 0) = 2/3) and s1 (P(S1 = 1|y1 = 0) = 1/3). If
s1 = 1 and no action is taken, the state is likely to transition to s2 = 0 or s2 = 2, with equal
probability. The transition to s2 = 0 results in loss of memory that s1 = 1. This is prevented
by swapping the states 1 and 2. In this case the likely next output is y2 = 2 which excludes
the possibility that s1 = 0. On the other hand, if s1 = 0, then the next outcome is likely
again y2 = 0, and we can exclude the possibility that s1 = 1. A similar improvement in
distinguishing the initial states is obtained when y1 = 2, in which case it helps to swap the

11

1 − a

s = 0

a

b

1 − b

1−b
2

1−a
2

a

1−a
2

a

1 − a

s = 2

s = 1

b

y = 1

b

1−b
2

1 − b

y = 0

y = 2

FIG. 6. A toy example of a Hidden Markov Model on three states. States are depicted by empty
circles and outputs by colored circles. Arrows between states indicate allowed transitions and
are labeled with the probabilities in the transition matrix. Arrows from states to outputs indicate
possible outputs and are labeled with the output probabilities. We take the initial state distribution
to be uniform on the three states.

states 0 and 1. Thus, an adaptive choice of permutation based on the ﬁrst output improves
the measurement ﬁdelity for this model. Furthermore, because the swap required to achieve
the improvement depends on the ﬁrst outcome, any outcome-independent choice of action
performs less well for two steps.

The pattern leading to the adaptive improvement of the three state model generalizes.
Suppose that the outputs so far signiﬁcantly narrow the likely initial states to a subset of
states, with memory of the initial state still present but the states within the subset not
easily distinguished by future outputs. Whenever this is the case, one can gain an advantage
by moving some of the states of this subset to another set of more distinguishable states.
The three-state model provides a situation where an adaptive permutation policy is strictly
better than every non-adaptive such policy.

6. DISCUSSION

We have investigated policies for applying permutations between steps of a repetitive
In our treatment of the measurement process, we have assumed that the
measurement.
HMM parameters are known.
In atomic systems, it is often possible to determine these
parameters from the physical constants associated with the atomic levels, along with mea-
sured parameters such as Rabi rates. In quantum systems with superconducting qubits [24]
or electrically-deﬁned quantum dots [25], the physics is less constrained, and it may be dif-
ﬁcult to infer the HMM from the measured or calibrated parameters. Instead, it is possible
to learn the HMM by observing the measurement process for many steps. Many tools are
available for inferring the transition and output matrices from such observations [10]. We
recommend investigating use of these tools, while taking advantage of known physical con-
straints and measured or calibrated parameters, with the aim of improving the modeling
of quantum measurement processes. As mentioned in Sec. 3, when implementing permuta-
tions in systems with shorter lifetimes, the transition matrices and output matrices become

12

FIG. 7. (a): The measurement inﬁdelity (Eq. 2.1) of the measurement at 2 and 6 steps, for both
the maximum likelihood inference strategy without permutations and that with permutations.
Although not demonstrated in this ﬁgure, the measurement inﬁdelity in both cases stabilizes after
6 steps, and we therefore do not show larger numbers of steps. The inﬁdelity is calculated under
the uniform prior, P(s1) = 1/3. The horizontal axis is a cut in the parameter space given by
the line a = b, or in other words along the dashed line in (b). (b): The logarithm of the ratio
of the measurement inﬁdelity of the maximum likelihood estimator without permutations to that
with permutations, for a range of parameters a, b. This metric is a measure of ﬁdelity gained by
implementing permutations over doing the same task without applying permutations. The gain is
substantial for small a and b.

directly dependent on which permutation is applied. Tools used for inference of HMMs can
be adapted to the problem of inference of these action dependent models.

In specifying and applying permutation policies for improving measurement ﬁdelity, we
have assumed that the permutations are applied perfectly. As noted in [16, 17], this as-
sumption may not be realistic. The model can be readily adapted to take account of errors
in applying the permutations. Changing the transition matrix associated with the chosen
permutation to a general Markov process poses no diﬃculty and makes it possible to account
for known errors or noise in applying the chosen permutation.

In the absence of transitions, and where any permutation is an allowed action, optimizing
the measurement inﬁdelity is an instance of an active sequential hypothesis testing prob-
lem [26]. Computing the optimal policy for this type of problem is substantially simpler.
The complexity is dominated by the number of possible posterior distributions on the input
states given the outputs. This number grows as a polynomial with the number of steps,
although its degree may be high depending on the number of allowed actions. For these
problems, there are guarantees that the greedy policy used in this work has close to optimal
mutual information between the initial state and the data [27]. One may also be interested
in minimizing the number of measurement rounds, instead of ﬁxing it as we have done in this
work. One way to study this problem is to introduce a “discount factor” that exponentially
suppresses rewards that are reached after many measurement rounds. One can then obtain

13

upper and lower bounds on the minimal discounted measurement inﬁdelity [28]. A potential
line of future work is to extend these results to the case of non-trivial transition matrices.

In general, a quantum measurement process can be described by a quantum instrument.
A quantum instrument has classical outputs and output-conditional side-eﬀects on the quan-
tum system being observed. One way to specify the instrument is as the composition of a
minimally disturbing general quantum measurement, given as a positive operator-valued
measure (POVM), followed by output-conditional quantum operations. See Ref. [29] for
technical details. Because the processes of interest in this work were totally dephasing, it
was suﬃcient to use a classical description, but it is also of interest [30] to study optimal
policies for measurement processes whose instruments are not totally dephasing.

In many situations, including those involving 9Be+ qubits, the physically implemented
measurements involve continuous monitoring, and events such as detection times of photons
are recorded. The formalism used in our work requires discretizing time to allow us to
model measurement of 9Be+ as a discrete time Markov process. To take full advantage
of the measurement process requires modeling by a continuous-time process[31],[32]. This
problem can be formalized with the Hamilton-Jacobi-Bellman equation [33],[34].

The 9Be+ example has the property that the graph of allowable transitions is directed
and acyclic. An HMM with this property is called a left-right HMM, and simpler algorithms
exist for computing likelihoods in this degenerate case [10]. While we did not take advantage
of this structure in this work, it is likely that using this structure would lead to simpler
algorithms for computing the optimal policy.

In this study, we focused on optimizing measurement ﬁdelity given the number of steps of
the HMM measurement model that are observed. The HMMs relevant for atomic measure-
ment eventually lose memory of the initial state, so observing for more steps yields rapidly
In many applications, it is desirable to minimize the average
diminishing ﬁdelity gains.
time required to complete a measurement, which implies a trade-oﬀ between measurement
time and measurement ﬁdelity. Examples where measurement time matters are for appli-
cations involving feed-forward such as quantum error correction, and in characterization
experiments dominated by measurement time. The techniques discussed here can be used
to explore the measurement-ﬁdelity measurement-time trade-oﬀ. However, there is a way
in which one can reduce average measurement time, possibly without losing measurement
ﬁdelity. In particular, it is possible to terminate the measurement early if the outputs so
far indicate a particular initial state suﬃciently strongly. Such a scheme was introduced
in Ref. [35] and related approaches are in current use [36]. HMMs can be used to improve
these schemes’ time and ﬁdelity performance beyond what can be achieved using likelihood
ratio tests computed from models with independent and identical outputs. One can incor-
porate the cost of an additional measurement explicitly in the cost function to minimize,
and again consider the advantage that could be gained by implementing an adaptive strat-
egy. In the case of a transition matrix equal to the identity, this problem has been studied
in [37]. There, the authors take as their cost function the expected number of measurements
plus the expected measurement inﬁdelity, with a variable weighting between the two terms.
Upper and lower bounds on the optimal cost are then obtained. It would be interesting to
extend these results to the scenarios with general transition matrices considered here.

14

7. CONCLUSION

We have investigated the use of adaptively chosen actions to improve measurement ﬁ-
delity in quantum measurements that are realized as sequential observations with complete
decoherence. We considered two examples, one motivated by 9Be+ ion qubits, the other a
three-state toy example. We focused on actions consisting of permuting the states of the
system.

Our study of 9Be+ measurements indicates parameter regimes where an improvement is
achieved and suggests future work to take advantage of adaptive permutation policies. We
discussed a number of paths forward and open problems, such as that of optimizing the
trade-oﬀ between measurement times and measurement ﬁdelity, ﬁnding better policies, and
extensions to continuous-time measurement processes.

Appendix A: Outline of Appendix

In the appendices below, we explain the mathematical formalism involved in the calcu-
lations used in the main text. In App. B we introduce repetitive quantum measurements.
We discuss why, in many experiments, it is possible to describe such measurements with an
eﬀective classical model, namely an HMM. In App. C we discuss a few subtleties involved
in discretizing continuous-time dynamics so that it can be modeled as an HMM, illustrating
our solution in the context of the 9Be+ example. In Sect. D we describe the problem of
inferring the initial state from a sequence of measurements and its solution by the maxi-
mum likelihood estimate. The method of using adaptively chosen actions during repetitive
measurements is deﬁned in App. E. Therein, we discuss how to compute the optimal policy
using the Bellman Equation, and present our heuristic policy that chooses actions based
on minimizing the entropy of the initial state. We also sketch how to reduce the problem
of computing an optimal policy to a POMDP in App. E. Finally, our implementation and
simulation is outlined in App. F.

Appendix B: Repetitive measurement models

Here we discuss the formalism of Hidden Markov Models (HMMs), and show how they
describe repeated measurements of quantum systems. As discussed in the main text, for
a quantum system under interrogation by a measurement process, such as an atom under
ﬂuorescence detection, the state of the system can be eﬀectively dephased, and thus has a
classical description. The initial quantum state is a probability distribution over the levels
S (assumed to be nondegenerate) that constitute the measurement basis elements, and that
the output has a probability distribution conditional on the current level and previous level,
and the next level has a probability distribution conditional on the current level. This is
immediately in the form of an HMM step, except that the current outcome may depend on
the previous level as well as the current level. The dependence on the previous level can
be accounted for by expanding the HMM state space to include memory of the previous
outcome, as explained below. For atoms with cycling transitions, the transitions between
levels result from non-ideal cycling. Having reviewed the reduction from a quantum model
to a classical stochastic one, we now use the term “state” to refer to a classical level.

We use the convention that an upper case variable X refers to a random variable (RV),

15

while its corresponding lower case variable refers to a particular instantiation of the RV.
We use P to denote “probability of”, whose argument is an event, so that the expression
P(X = x) refers to the probability that the RV X has a particular value x. Because we
are interested in stochastic processes, we also use the notation for sequences of RVs as in
the main text. A variable Si is the RV for the stochastic process S at the step i, while the
variable Si indicates the subsequence of the ﬁrst i steps of the process S, Si = S1, . . . , Si.
We adopt similar conventions for particular sequences of values, so that si = s1, . . . , si.

We now introduce notation for HMMs. An HMM has state space S, output space Y,
transition matrix A, and output matrix B. We use the notation A(s(cid:48)|s) for the transition
probability to next state s(cid:48) given current state s, and B(y|s) for the probability of output y
given current state s. We denote the state and observation at step i as si and yi, respectively.
Let ν be the initial state distribution deﬁned by ν(s1) = P(s1), where the expression s1
abbreviates the event that S1 = s1. The probabilities of the state and output sequences are
determined by unraveling the transitions according to

P(sn) = A(sn|sn−1)P(sn−1) = ν(s1)

n
(cid:89)

i=2

A(si|si−1),

P(yn|sn) =

n
(cid:89)

i=1

B(yi|si).

(B.1)

Appendix C: Example: Fluorescence detection of 9Be+ hyperﬁne qubit

To describe the measurement dynamics of 9Be+ measurement as an HMM, we address
the following issues. While dynamics of physical systems usually take place continuously
in time, an HMM is a discrete-time model, and therefore the physical dynamics need to
be discretized. Second, the distributions of outcomes can contain more information than is
useful, which can complicate computations. We thus simplify the outcome distributions by a
binning procedure. Third, as mentioned in the introduction, the probability of transitioning
to a diﬀerent state sometimes depends on the measurement outcome. This is accounted for
by expanding the state space to include the current measurement outcome, allowing the
transition matrix A to depend on both the physical state and the outcome.

Reduction from continuous to discrete time An HMM description of a ﬂuorescence mea-
surement can be derived from the continuous time Markov process modeling the stochastic
dynamics of the levels and the detection of ﬂuorescence from the cycling level while driving
the cycling transition.

Transitions between levels are described by the transition-rate matrix Q. The oﬀ-diagonal
entries Qss(cid:48) of Q are the non-negative transition rates, and the diagonal entries Qs(cid:48)s(cid:48) =
− (cid:80)
s(cid:54)=s(cid:48) Qss(cid:48) are the total rates of departure from level s(cid:48) to other levels. The transition-
rate matrix can be integrated to obtain the probabilities R(s|s(cid:48)) of starting in level s(cid:48) and
ending in level s for a measurement step of period ∆t. Then R(s|s(cid:48)) = (eQ∆t)s,s(cid:48). The
photon emission rate for level s is Es ≥ 0. For simplicity, we make the approximation that
at most one transition occurs in any given step. If the ion is in level si−1 at the beginning
of step i, and in level si at the end of the step, and transitions at a particular time t, the
distribution of collected photons is Poissonian with mean determined by Esi, Esi−1, and t.
The distribution J(o|s, s(cid:48)) of the number o of collected photons given the system starts in
level s(cid:48) and ends in level s is then the mixture of these Poisson distributions [3]. We can

16

compute the probability of the system starting the step in the level s(cid:48), observing o photons
during the step and ending in level s as U (s, o|s(cid:48)) = J(o|s, s(cid:48))R(s|s(cid:48)).

The output distributions are supported on all nonnegative integers, but collecting a large
number of photons in a single step is very unlikely. We thus restrict the output space to be
the set Y = {0, . . . , nmax − 1, nmax}, where we obtain outcome nmax if at least nmax photons
were collected during the step. We chose nmax = 15 so that the probability of collecting
nmax or more photons from the bright state in a time step of duration ∆t = 53.9µs is less
than 10−3.

Outcome dependence

In the model described in the previous paragraph, the output
depends on the previous and current state. In order to describe such models, we introduce
notation for HMM state spaces that expand the physically relevant state space. Let S
be the set of physical states. The state space T of the expanded HMM is related to the
physical states by a map α : T → S. In the 9Be+ example, we let S be the set of levels,
S = {(F, mF )|F ∈ {1, 2}, mF ∈ {−F, . . . , F }}. We let T consist of pairs (s, o) of physical
states s ∈ S and outputs o ∈ Y, and we deﬁne α((s, o)) = s. Here, s represents the current
physical state and the o is the output observed during the step resulting in state s. For this
example, it is also useful to deﬁne the map ρ : T → Y, ρ((s, o)) = o. We then deﬁne the
transition and output matrices as follows. The transition probability from previous state s(cid:48),
with previously recorded observation o(cid:48), to the current state s with observation o recorded
in transitioning to s is

A(s, o|s(cid:48), o(cid:48)) = U (s, o|s(cid:48)).

(C.1)

Because the transition matrix now captures both the state transition and the probability
of a state emitting an outcome, the outcome process matrix on the expanded state is de-
terministic, so that B(o|l) = δo,ρ(l) is the new outcome process matrix. Thus, the outcome
matrix merely describes the fact that the physical state is unobservable. For the initial state
distribution, we must pick a convention for the mapping from the physical initial state dis-
tribution to that for the HMM. Let S1 be the random variable describing the initial physical
state. For the ﬂuorescence example we take the distribution over initial HMM states to be
ν(s, o) = P(S1 = s)δo,0.

Binning For our purposes, the outcome distributions contain more information than
necessary, and the computations grow rapidly in complexity with the number of outcomes.
Given an HMM M with an output space Y, we thus seek a related model that has fewer
possible outputs. We accomplish this by partitioning the outcome space into bins.

A partition µ = {bi} of Y with nb bins is a set of nb nonempty subsets of the set Y, such
that the sets bi are pairwise disjoint, i (cid:54)= j =⇒ bi ∩ bj = ∅, and cover the set Y, ∪ibi = Y.
An element bi ∈ µ is called a bin. The set of bins µ becomes the output space of the binned
model Mµ. By keeping only the information of which bin the outcome lies in, the resulting
distribution is again an HMM.

For the 9Be+ example, the binned model has initial state distribution νµ, transition matrix

Aµ, and output matrix Bµ, deﬁned by

νµ(s, bi) =

Aµ(s, bi|s(cid:48), bj) =

Bµ(bi|s) =

ν(s, y)

U (s, y|s(cid:48))

B(y|s).

(cid:88)

y∈bi
(cid:88)

y∈bi
(cid:88)

y∈bi

17

(C.2)

(C.3)

(C.4)

Note that the bins cannot depend on the state.

To determine what bins to use, it is useful to have a quantitative cost function to apply to
the bins. Speciﬁcally, if f is a function that takes an HMM Mµ and gives a cost associated
with that model, we wish to choose the binning µ∗
with nb bins that solves the minimization
nb
problem

argmin
µ

f (Mµ)

subject to µ (cid:96) Y

|µ| = nb.

where the notation µ (cid:96) Y denotes that µ is a partition of Y. We use the inﬁdelity (Eq. 2.1)
of the model induced by the binning as our cost function f to determine the optimal binning
when we compute policies in Sect. 4, but in general diﬀerent heuristic costs can be used [38].

Appendix D: Initial state inference

Given the measurement process described in App. B, our goal is to infer, after some
number of observations, the initial physical state of the system. So consider now a generic
HMM with state space T , physical state space S identiﬁed by α : T → S, initial physical
state space L, transition matrix A and output matrix B. We assume that the initial state
is uniformly distributed over L. Let ˆS1 = φ(Y n) be the random variable that is the output
of the initial-state estimation procedure φ determined by the observed outputs Y n. Recall
the deﬁnition of the measurement inﬁdelity in Eq. 2.1, P(S1 (cid:54)= ˆS1). We will sometimes also
refer to the ﬁdelity F = P(S1 = ˆS1) = 1 − P(S1 (cid:54)= ˆS1).

The maximal measurement ﬁdelity is achieved by choosing this function to be the
Bayesian, maximum a-posteriori (MAP) estimate [39]. The MAP estimate ˆs1 is the physical
state with the highest posterior probability given the n observed outcomes:

ˆs1 = argmax

s1∈S

P(s1|yn).

(D.1)

According to Bayes’s rule, P(s1|yn) = P(yn|s1)P(s1)/P(yn). Because the denominator is
independent of s1 and the prior distribution is uniform, The MAP estimate is the same as
the maximum likelihood (ML) estimate [40]:

ˆs1 = argmax

s1∈S

P(yn|s1).

(D.2)

This estimate can be computed step-by-step by keeping track of the list of values (cid:0)P(yk, lk|s1)(cid:1)

lk∈T ,s1∈L

18

for k = 1 to k = n, where the list is updated by applying the recursive expression

P(yk, lk|s1) =

=

=

(cid:88)

lk−1∈T
(cid:88)

lk−1∈T
(cid:88)

lk−1∈T

P(yk−1, lk−1|s1)P(yk, lk|yk−1, lk−1, s1)

P(yk−1, lk−1|s1)P(yk, lk|lk−1)

P(yk−1, lk−1|s1)B(yk|lk)A(lk|lk−1),

(D.3)

which takes advantage of the HMM conditional independence properties. The values are
initialized with

P(y1, l1|s1) = B(y1|l1)I[s1 = α(l1)]ν(l1)

(cid:44)

(cid:88)

ν(l(cid:48)

1).

(D.4)

1:s1=α(l(cid:48)
l(cid:48)
1)

From the ﬁnal values P(yn, ln|s1), the MAP estimate is obtained according to

ˆs1 = argmax

s1∈L

(cid:88)

ln

P(yn, ln|s1).

(D.5)

By our construction of the expanded HMM, this is equal to the expression in Eq. (D.2).
The measurement ﬁdelity F can be computed exactly if the number of possible outcome
sequences yn is not too large, or by empirically sampling the HMM output sequences and
computing the Bayesian posterior probabilities for each sample.

Appendix E: Adaptive measurement policies

Modiﬁcation of HMM To accommodate actions that can be taken between steps of
an HMM we introduce a set of possible actions A, where each action a in A is a state-
transforming process. In general, a can be a stochastic process, where the probability that a
results in state s(cid:48) given that the current state is s is denoted by a(s(cid:48)|s). The process resulting
from modifying the HMM by applying action ak after observing yk in the kth step can be
thought of as an HMM with step-dependent transition probabilities, where the transition
matrix A at step k is replaced by the composition of A with the action ak. The modiﬁed
transition probabilities are then

Ak(sk+1|sk) =

(cid:88)

s

A(sk+1|s)ak(s|sk).

(E.1)

The probability of a state sequence is accordingly given by

P(sn) = ν(s1)

n
(cid:89)

i=2

Ai−1(si|si−1).

(E.2)

19

For computing the ﬁnal estimate, it suﬃces to modify the expression for P(yk, sk+1|l1) in
Eq. D.3 by replacing A(sk|sk−1) with Ak−1(sk|sk−1), which depends on the policy’s choice of
action at step k − 1.

The action at step k is chosen based on the observations so far given by yk. A policy
is a speciﬁc strategy for choosing the action. Policies are chosen to maximize a reward. If
the reward can be expressed as a sum of rewards at each step, this ﬁts the framework of
partially observed Markov decision processes (POMDPs) [19]. For our application, policies
are chosen to maximize the measurement ﬁdelity, which is expressed in terms of a decision
made after the last step. It is possible to change the model to an equivalent one ﬁtting the
POMDP formalism, as discussed below.

We consider actions that permute states, so A is a set of state permutations including
the identity permutation. We denote such permutations by σ and write σ(s) for the state
resulting from applying σ to state s. The process is schematically shown in Fig. 1. Note
that there are n − 1 actions for a sequence of n steps.

In the case that the HMM state space T does not coincide with the physical state space
S, it is necessary to relate physical actions to actions on the HMM state space. For the 9Be+
system, recall that the HMM states are given by pairs (s, o) of physical states s ∈ S and
outputs o ∈ Y, where S is the set of levels, S = {(F, mF )|F ∈ {1, 2}, mF ∈ {−F, . . . , F }},
and Y is the set of photon counts, Y = {n|n = 0, 1, . . . , nmax}. Given a permutation σ
that acts on the physical states S, we induce a permutation ˜σ that acts on T , by taking
˜σ((l, o)) = (σ(l), o). The permutations for the 9Be+ example are then {˜σ|σ ∈ {τ, τ −1, (cid:15)}},
where the permutation τ is deﬁned in Fig. 2.

Bellman Equation Our task now is to ﬁnd a policy, consisting of permutations to apply,
that maximizes the measurement ﬁdelity. For sake of clarity, we assume in this appendix
that the HMM state space coincides with the physical state space, but the ideas discussed
here can be extended when this is not the case, by maximizing the probability of inferring
the initial physical state instead of the initial HMM state. In the following, we formally
express the maximization problem for computing the optimal policy, then introduce belief
states, and show their use in computing the optimal policy.

To maximize the measurement ﬁdelity over the choices of σn−1, we use the law of total

expectation to compute that

P

max
σn−1

(cid:16) ˆS1 = S1|σn−1(cid:17)
(cid:18)
(cid:18)

=

EY1

max
σ1

EY2|σ1

· · · max
σn−2

(cid:18)

EYn−1|σn−2

max
σn−1

EYn|σn−1

(cid:16)

P

(cid:16) ˆS1 = S1|Y nσn−1(cid:17)(cid:17)(cid:19)

· · ·

(cid:19)(cid:19)

(E.3)

This is called the Bellman Equation [41], [42]. One way to compute Eq. E.3 is to compute all
possible posterior distributions of the initial state S1, then to compute the expectation of the
reward P( ˆS1 = S1|Y nσn−1) for all possible sets of actions σn−1. The posterior distributions
are computed iteratively. To describe the computation, we introduce the distributions

and

βk(s1, sk|yk, σk−1) = P(s1, sk|yk, σk−1)

ηk(s1, sk+1|yk, σk) = P(cid:0)s1, sk+1

(cid:12)
(cid:12)yk, σk(cid:1).

(E.4)

(E.5)

20

The βk and ηk are called belief states. The belief state βk describes the state of knowledge
of the current and initial HMM states immediately after observing outcome yk, but before
we apply the permutation σk. After applying σk and allowing the system to transition, the
state of knowledge is described by ηk. Because our goal is to infer the initial state of the
system, note that it is necessary to keep track of the state of knowledge of the initial state.
To predict how the dynamics will eﬀect our estimate, it is also necessary to have an estimate
of the current state. We therefore keep track of sk as well in the belief state.

The belief state is iteratively updated according to the observations yn and permutations
σn−1. At step k, we use the observation yk to update the belief state ηk−1 to βk according
to Bayes’ rule

βk(s1, sk|yk, σk−1) = hBayes(ηk−1, yk) =

ηk−1(s1, sk|yk−1, σk−1)B(yk|sk)

ηk−1(s1, sk|yk−1, σk−1)B(yk|sk)

sk

(cid:80)

.

(E.6)

Next, we use the permutation σk to update the belief state βk is updated to ηk according
the transition matrix of the HMM,

ηk(s1, sk+1|yk, σk) = hTrans(βk, σk) =

(cid:88)

sk

βk(s1, sk|yk, σk−1)A(sk+1|σk(sk)).

(E.7)

In Eq. E.6, we used the fact that the current outcome depends only on the current state,
and in Eq. E.7, we used the fact that the next state depends only on the current state, which
are the two deﬁning features of the HMM.

1

. Here, the index s(cid:48)

The initial belief state η0 is determined by the prior distribution, η0(s1, s(cid:48)

1|y0, σ0) =
1 is for the “current” state after having seen no data (y0) and
ν(s1)δs1,s(cid:48)
having applied no permutations (σ0). This index is updated after each application of hTrans,
updating the step index each time. In contrast, the other index s1 is the one that describes
the initial state, and continues to describe the initial state even after the various updates
using hTrans, hBayes.

η0

y 1 = 0

y1 =1

β1 = hBayes(η0, y1 = 0)

β1 = hBayes(η0, y1 = 1)

σ1 = a

σ1 = b

σ1 = a

σ1 = b

η1 = hTrans(β1, σ1 = a)

η1 = hTrans(β1, σ1 = b)

η1 = hTrans(β1, σ1 = a)

η1 = hTrans(β1, σ1 = b)

FIG. 8. A depiction of the belief state tree. For illustration purposes, we consider the case that
Y = {0, 1}, and A = {a, b} both have two elements. To keep the diagram small, we depict only
one step of the calculation. We traverse the tree by using Bayes’ rule to update an η belief state,
or using an action to update a β belief state.

The belief states are labelled by sequences of observations yn and permutations σn−1, and
the set of all accessible belief states has the structure of a tree, see Fig. 8. The children of
a node ηk−1 of the tree are those βk that are obtained from a Bayes update as in Eq. E.6,

21

where the diﬀerent children are labelled by the possible values of yk. Similarly, the children
of a node βk are those ηk that can be obtained from Eq. E.7, where the diﬀerent children
are labelled by the possible values of σk.

Once all the belief states have been computed, the exhaustive search algorithm suggested
by Eq. E.3 can be directly implemented. To perform this computation using the tree, we
start at the leaves by computing the probability of correct inference

P(S1 = ˆS1|yn, σn−1) = max
s1

(cid:88)

sn

βn(s1, sn|ynσn−1).

(E.8)

Next, we compute the expectation

EYn|σn−1

(cid:16)

(cid:17)
P(S1 = ˆS1|yn, σn−1)

=

(cid:88)

P(S1 = ˆS1|yn, σn−1)

yn

(cid:88)

sn

B(yn|sn)

(cid:88)

s1

ηn−1(s1, sn|yn−1σn−1).

(E.9)

This is the innermost expectation in Eq. E.3. We next compute the maximum over the
permutation σn−1 directly. We have thus computed the (n − 1)th step of the optimal policy.
By then iterating expectation and maximization steps, we compute the optimal policy of
Eq. E.3.

Minimum posterior entropy heuristic

In the examples in Sect. 5 and Sect. 4, we compute
the optimal policy using exhaustive search for a small set of actions and a small set of
outcomes, but as the numbers of actions and outcomes increase, the exhaustive search
algorithm becomes extremely expensive to compute. This is because the work required scales
as O((|A||Y|)n), determined by the size of the tree. For more than a few steps, computing
the optimal policy is currently infeasible. This motivates the use of heuristic policies. We
here discuss an heuristic policy that performs well in some regimes, as a possible alternative
to the optimal policy when it is not available.

Our heuristic policy is also a belief-state-based algorithm, but while exhaustive search
constructs a tree of height 2n + 1, we construct O(|Y|n) smaller trees of height 2g + 1 for a
ﬁxed g. To compute our heuristic, at each step we compute a measure of concentration of
the posterior probability distribution of the initial physical state given the output of the next
g steps, and choose the permutations that optimize the expectation of that measure. This
strategy is in a sense greedy and its performance depends on the measure of concentration
used. For this study, we used the posterior entropy as a measure of concentration, with less
entropy indicating higher concentration. We call this the minimum posterior entropy heuris-
tic. This is similar to the policy given in Ref. [43], except that there, the authors minimize
posterior entropy of the HMM state at the next step, not that of the initial state. Note that
measurement ﬁdelity is determined by the maximum probability of the posterior. Maximum
posterior probability is also a measure of concentration but is insensitive to probabilities
other than the maximum one, which may result in blind spots for a greedy algorithm us-
ing maximum posterior probability. We distinguish here between the policy that involves
maximizing the maximum probability of the posterior at n steps and that at g steps. The
former is the optimal policy, since at n steps the maximum probability of the posterior is
the measurement ﬁdelity, while at g steps it is merely an (uncontrolled) approximation of
the ﬁdelity.

As in App. E, we assume in the following discussion that the HMM state space coincides
with the physical state space, but again the calculations can be adapted to a case where

they diﬀer, by changing the cost function to be the posterior entropy of the physical initial
state. For choosing the permutation to be applied at step k after having observed yk and
having applied actions σk−1, we compute all possible belief states after g more steps, given
by

22

(cid:16)

P

s1, sk+g

(cid:12)
(cid:12)yk, σk−1, yk+g
(cid:12)

k+1, σk+g−1

k

(cid:17)
,

(E.10)

for all possible choices of yk+g
subsequence yk+1, . . . yk+g, and similarly for σk+g−1
initial state distributions,

k+1, σk+g−1

k

k

, where we use the abbreviation yk+g

k+1 to mean the
. We can then compute the associated

(cid:16)

P

s1

(cid:12)
(cid:12)yk, σk−1, yk+g
(cid:12)

k+1, σk+g−1

k

(cid:17)

=

(cid:88)

(cid:16)

P

s1, sk+g

sk+g

(cid:12)
(cid:12)yk, σk−1, yk+g
(cid:12)

k+1, σk+g−1

k

(cid:17)

,

(E.11)

and the corresponding entropy

H(S1|yk+g, σk+g−1) = −

(cid:88)

s1

P(cid:0)s1

(cid:12)
(cid:12)yk+g, σk+g−1(cid:1) log(cid:0)P(cid:0)s1

(cid:12)
(cid:12)yk+g, σk+g−1(cid:1)(cid:1).

(E.12)

Viewing this entropy as a cost function, we can then choose the permutations σk+g−1
minimize this cost, and apply the permutation σk.

k

that

To compute the distributions in Eq. E.10, we take the tree-based approach described in
Fig. 8. The root of the tree is the belief state βk(s1, sk|yk, σk−1). By iteratively applying
Eqs. E.6,E.7 for the various choices of yk+g
k+1, σk+g−1
, we can compute the desired distributions
k
in Eq. E.10. The cost function can be written

(cid:16)

H

S1

min
σk+g−1
k

(cid:12)
(cid:12)yk, σk−1, Y k+g
(cid:12)
(cid:18)

k+1 σk+g−1

k

(cid:17)

=

EYk+1|σk

min
σk

· · · min
σk+g−1

EYk+g|σk+g−1

(cid:16)

(cid:16)

H

S1

(cid:12)
(cid:12)yk, σk−1, Y k+g
(cid:12)

k+1 σk+g−1

k

(cid:17)(cid:17)

(cid:19)
,

· · ·

(E.13)

so a method similar to that used to solve the Bellman Equation can be applied to obtain
the solution to Eq. E.13.

After applying σk and observing yk+1, we need to update the tree of belief states. Instead
of recomputing the whole tree, we take a dynamic programming approach and leverage
the fact that we have already computed some of the possible future belief states. Rather
than recomputing the new root of the tree, we can simply use the already computed
βk+1(s1, sk+1|yk+1, σk) as the new root of the tree. All but the last step of the possible
future paths have also already been computed, so we can just apply Eqs. E.7,E.6 to the
(cid:17)
distributions P

to obtain the new belief states

(cid:16)

s1, sk+g

k+1, σk+g−1

k

(cid:12)
(cid:12)yk, σk−1, yk+g
(cid:12)
(cid:16)

P

s1, sk+g+1

(cid:12)
(cid:12)yk+1, σk, yk+g+1
(cid:12)

k+2

, σk+g
k+1

(cid:17)

.

Reduction to POMDP Although we were able to compute the optimal policy for the
examples considered in this paper, in general it requires a very large computation. For the
general case, it is useful to apply the existing framework of partially observable Markov

23

decision processes (POMDPs) [19], to leverage existing approximate algorithms (e.g. [21]).
A POMDP is a generalization of an HMM that includes output-dependent actions, and
includes a reward that is given at each step. The goal of policy planning in POMDPs is to
maximize this reward.

To use the framework of POMDPs, it is necessary to express the reward, in our case the
measurement ﬁdelity, as a sum of rewards at each step. We now sketch how to adjust the
model to accomplish this. We expand the state space to keep track of the initial state and
the number of steps. The new state space is S × L × {1, . . . , n}. The transition and output
matrices are redeﬁned accordingly. The action at each step is either a permutation or, for
the last step, a decision action that is the estimate of the initial state. Possible decision
actions are in one-to-one correspondence with L, so the set of possible actions is the union
of the set of allowed permutations and L. The reward at each step is 0 if the step number
encoded in the extended state is not n or the decision action does not correspond to the
initial state. If the step number is n and the decision action agrees with the initial state,
the reward is 1. With this deﬁnition, the expectation of the sum of the rewards at each step
is the measurement ﬁdelity.

Appendix F: Implementation and simulation

The code used in our simulations of HMMs with permutations is available on Github [18].
The code uses the pyro package [44] and the underlying PyTorch [45] package for compu-
tation of relevant probabilities.

For the examples given in the main text, we computed the measurement inﬁdelity nu-
merically without resorting to Monte Carlo techniques. In particular, for a given number
n of steps, we considered all of the |Y|n possible output sequences. For each such sequence
yn, we computed the probability that it occurs and the probability of incorrectly identifying
the initial state conditional on yn. The measurement inﬁdelity is obtained by summing
the product of these two probabilities over Y n. The number of possible data sequences is
exponential in n, which limits the number of steps for which it is possible to avoid Monte
Carlo sampling. Computation of the probability of an outcome sequence and of the posterior
initial state distribution used the forward-backward algorithm [11], which is built into the
packages we used.

ACKNOWLEDGMENTS

This work includes contributions of the National Institute of Standards and Technology,
which are not subject to U.S. copyright. The use of trade, product and software names
is for informational purposes only and does not imply endorsement or recommendation by
the U.S. government. S. Geller acknowledges support from the Professional Research Expe-
rience Program (PREP) operated jointly by NIST and the University of Colorado. D. C.
C. acknowledges support from a National Research Council postdoctoral fellowship. E. K.
thanks Dietrich Leibfried for introducing them in the early days of ion trap quantum com-
puting to the idea of adaptively chosen pulses for improving measurement ﬁdelity. We thank
Zachary Sunberg for discussions on the POMDP formalism. We thank Giorgio Zarantonello
for computations involving the transition rates in 9Be+. We thank Ting Rei Tan, Moham-
mad Alhejji, Alexander Kwiatkowski, Arik Avagyan, Akira Kyle, and Stephen Erickson for

helpful suggestions and comments.

24

[1] J. A. Tropp, User-Friendly Tail Bounds for Sums of Random Matrices, Found. Comput. Math.

12, 389 (2012).

[2] D. Hume, T. Rosenband, and D. J. Wineland, High-Fidelity Adaptive Qubit Detection through
Repetitive Quantum Nondemolition Measurements, Phys. Rev. Lett. 99, 120502 (2007).
[3] C. E. Langer, High Fidelity Quantum Information Processing with Trapped Ions, Ph.D. thesis,

University of Colorado at Boulder (2006).

[4] G. Liu, M. Chen, Y.-X. Liu, D. Layden, and P. Cappellaro, Repetitive Readout Enhanced by

Machine Learning, Mach. Learn.: Sci. Technol. 1, 015003 (2020).

[5] Z.-H. Ding, J.-M. Cui, Y.-F. Huang, C.-F. Li, T. Tu, and G.-C. Guo, Fast High-Fidelity
Readout of a Single Trapped-Ion Qubit via Machine-Learning Methods, Phys. Rev. Appl. 12,
014038 (2019).

[6] S. Crain, C. Cahall, G. Vrijsen, E. E. Wollman, M. D. Shaw, V. B. Verma, S. W. Nam,
and J. Kim, High-Speed Low-Crosstalk Detection of a 171Yb+ Qubit using Superconducting
Nanowire Single Photon Detectors, Communications Physics 2, 1 (2019).

[7] A. Seif, K. A. Landsman, N. M. Linke, C. Figgatt, C. Monroe, and M. Hafezi, Machine
Learning Assisted Readout of Trapped-Ion Qubits, Journal of Physics B: Atomic, Molecular
and Optical Physics 51, 174006 (2018).

[8] E. Magesan, J. M. Gambetta, A. D. C´orcoles, and J. M. Chow, Machine Learning for Discrim-
inating Quantum Measurement Trajectories and Improving Readout, Physical Review Letters
114, 200501 (2015).

[9] J. Gambetta, W. A. Braﬀ, A. Wallraﬀ, S. M. Girvin, and R. J. Schoelkopf, Protocols for
Optimal Readout of Qubits Using a Continuous Quantum Nondemolition Measurement, Phys.
Rev. A: At. Mol. Opt. Phys. 76 (2007).

[10] Y. Ephraim and N. Merhav, Hidden Markov Processes, IEEE Trans. Inf. Theory 48, 1518

(2002).

[11] L. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recog-

nition, Proc. IEEE 77, 257 (1989).

[12] S. S. Elder, C. S. Wang, P. Reinhold, C. T. Hann, K. S. Chou, B. J. Lester, S. Rosenblum,
L. Frunzio, L. Jiang, and R. J. Schoelkopf, High-Fidelity Measurement of Qubits Encoded in
Multilevel Superconducting Circuits, Phys. Rev. X 10, 011001 (2020).

[13] L. A. Martinez, Y. J. Rosen, and J. L. DuBois, Improving Qubit Readout with Hidden Markov

Models, Phys. Rev. A: At. Mol. Opt. Phys. 102, 062426 (2020).

[14] C. T. Hann, S. S. Elder, C. S. Wang, K. Chou, R. J. Schoelkopf, and L. Jiang, Robust Readout
of Bosonic Qubits in the Dispersive Coupling Regime, Phys. Rev. A: At. Mol. Opt. Phys. 98
(2018).

[15] J. C. Curtis, C. T. Hann, S. S. Elder, C. S. Wang, L. Frunzio, L. Jiang, and R. J. Schoelkopf,
Single-Shot Number-Resolved Detection of Microwave Photons with Error Mitigation, Phys.
Rev. A: At. Mol. Opt. Phys. 103, 023705 (2021).

[16] S. W¨olk, C. Piltz, T. Sriarunothai, and C. Wunderlich, State Selective Detection of Hyperﬁne

Qubits, J. Phys. B: At. Mol. Opt. Phys. 48, 075101 (2015).

[17] B. Hemmerling, F. Gebert, Y. Wan, and P. O. Schmidt, A Novel, Robust Quantum Detection

Scheme, New J. Phys. 14, 023043 (2012).

25

[18] S. Geller, S. Glancy, and E. Knill, Permuted Hidden Markov Models for State Inference,

https://github.com/usnistgov/perm_hmm (2021).

[19] K. J. ˚Astr¨om, Optimal Control of Markov Processes with Incomplete State Information I, J.

Math. Anal. Appl. 10, 174 (1965).

[20] C. H. Papadimitriou and J. N. Tsitsiklis, The Complexity of Markov Decision Processes, Math.

Oper. Res. 12, 441 (1987).

[21] H. Kurniawati, D. Hsu, and W. S. Lee, SARSOP: Eﬃcient Point-Based Pomdp Planning by
Approximating Optimally Reachable Belief Spaces., in Robotics: Science and Systems, Vol.
2008 (Zurich, Switzerland., 2008).

[22] A.-G. Paschke, 9Be+ Ion Qubit Control Using an Optical Frequency Comb, Ph.D. thesis,

Hannover: Gottfried Wilhelm Leibniz Universit¨at Hannover (2017).

[23] M. Acton, K.-A. Brickman, P. Haljan, P. Lee, L. Deslauriers, and C. Monroe, Near-Perfect

Simultaneous Measurement of a Qubit Register, arXiv preprint quant-ph/0511257 (2005).

[24] M. Kjaergaard, M. E. Schwartz, J. Braum¨uller, P. Krantz, J. I.-J. Wang, S. Gustavsson, and
W. D. Oliver, Superconducting Qubits: Current State of Play, Annu. Rev. Condens. Matter
Phys. 11, 369 (2020).

[25] J. Medford, J. Beil, J. Taylor, E. Rashba, H. Lu, A. Gossard, and C. M. Marcus, Quantum-

Dot-Based Resonant Exchange Qubit, Phys. Rev. Lett. 111, 050501 (2013).

[26] H. Chernoﬀ, Sequential Design of Experiments, Ann. Math. Statist. 30, 755 (1959).
[27] Y. Chen, S. H. Hassani, A. Karbasi, and A. Krause, Sequential Information Maximization:
When is Greedy Near-Optimal?, in Conference on Learning Theory (PMLR, 2015) pp. 338–
363.

[28] D. Kartik, A. Nayyar, and U. Mitra, Active Hypothesis Testing: Beyond Chernoﬀ-Stein, in
2019 IEEE International Symposium on Information Theory (ISIT) (IEEE, 2019) pp. 897–
901.

[29] M. M. Wilde, From Classical to Quantum Shannon Theory (Cambridge University Press,

2011).

[30] J. Barry, D. T. Barry, and S. Aaronson, Quantum Partially Observable Markov Decision

Processes, Phys. Rev. A: At. Mol. Opt. Phys. 90, 032311 (2014).

[31] J. L. A. Combes, Rapid Measurement and Puriﬁcation Using Quantum Feedback Control,

Ph.D. thesis, Griﬃth University (2010).

[32] J. Combes, H. M. Wiseman, and K. Jacobs, Rapid Measurement of Quantum Systems Using

Feedback Control, Phys. Rev. Lett. 100, 160503 (2008).

[33] D. E. Kirk, Optimal Control Theory: An Introduction (Courier Corporation, 2004).
[34] B. Alt, M. Schultheis, and H. Koeppl, POMDPs in Continuous Time and Discrete Spaces,

Advances in Neural Information Processing Systems 33, 13151 (2020).

[35] A. H. Myerson, D. J. Szwer, S. C. Webster, D. T. C. Allcock, M. J. Curtis, G. Imreh, J. A.
Sherman, D. N. Stacey, A. M. Steane, and D. M. Lucas, High-Fidelity Readout of Trapped-Ion
Qubits, Phys. Rev. Lett. 100 (2008).

[36] S. L. Todaro, V. Verma, K. C. McCormick, D. Allcock, R. Mirin, D. J. Wineland, S. W. Nam,
A. C. Wilson, D. Leibfried, and D. Slichter, State Readout of a Trapped Ion Qubit using a
Trap-Integrated Superconducting Photon Detector, Phys. Rev. Lett. 126, 010501 (2021).
[37] M. Naghshvar and T. Javidi, Active Sequential Hypothesis Testing, Ann. Statist. 41, 2703

(2013).

[38] A. C. Keith, C. H. Baldwin, S. Glancy, and E. Knill, Joint Quantum-State and Measurement

Tomography with Incomplete Measurements, Physical Review A 98, 042318 (2018).

26

[39] K. P. Murphy, Machine Learning: a Probabilistic Perspective (MIT press, 2012).
[40] J. Shao, Mathematical Statistics (Springer, New York, 2003).
[41] R. Bellman, Dynamic Programming, Rand Corporation Research Study (Princeton University

Press, 1957).

[42] M. J. Kochenderfer, T. A. Wheeler, and K. H. Wray, Algorithms for Decision Making (MIT

Press, 2022).

[43] A. R. Cassandra, L. P. Kaelbling, and J. A. Kurien, Acting Under Uncertainty: Discrete
Bayesian Models for Mobile-Robot Navigation, in Proceedings of IEEE/RSJ International
Conference on Intelligent Robots and Systems. IROS’96, Vol. 2 (IEEE, 1996) pp. 963–972.

[44] E. Bingham, J. P. Chen, M. Jankowiak, F. Obermeyer, N. Pradhan, T. Karaletsos, R. Singh,
P. A. Szerlip, P. Horsfall, and N. D. Goodman, Pyro: Deep Universal Probabilistic Program-
ming, J. Mach. Learn. Res. 20, 28:1 (2019).

[45] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,
S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, Pytorch: an Imperative Style,
High-Performance Deep Learning Library, in Advances in Neural Information Processing Sys-
tems 32 , edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (Curran Associates, Inc., 2019) pp. 8024–8035.

