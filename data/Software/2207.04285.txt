2
2
0
2

l
u
J

9

]
E
S
.
s
c
[

1
v
5
8
2
4
0
.
7
0
2
2
:
v
i
X
r
a

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

A Closer Look into Transformer-Based Code
Intelligence Through Code Transformation:
Challenges and Opportunities

Yaoxian Li, Shiyi Qi, Cuiyun Gao, Yun Peng, David Lo, Zenglin Xu, and Michael R. Lyu

Abstract—Transformer-based models have demonstrated state-of-the-art performance in many intelligent coding tasks such as code
comment generation and code completion. Previous studies show that deep learning models are sensitive to the input variations, but
few studies have systematically studied the robustness of Transformer under perturbed input code. In this work, we empirically study
the effect of semantic-preserving code transformation on the performance of Transformer. Speciﬁcally, 24 and 27 code transformation
strategies are implemented for two popular programming languages, Java and Python, respectively. For facilitating analysis, the
strategies are grouped into ﬁve categories: block transformation, insertion / deletion transformation, grammatical statement
transformation, grammatical token transformation, and identiﬁer transformation. Experiments on three popular code intelligence tasks,
including code completion, code summarization and code search, demonstrate insertion / deletion transformation and identiﬁer
transformation show the greatest impact on the performance of Transformer. Our results also suggest that Transformer based on
abstract syntax trees (ASTs) shows more robust performance than the model based on only code sequence under most code
transformations. Besides, the design of positional encoding can impact the robustness of Transformer under code transformation.
Based on our ﬁndings, we distill some insights about the challenges and opportunities for Transformer-based code intelligence.

Index Terms—Code intelligence, code transformation, Transformer, robustness.

(cid:70)

1 INTRODUCTION

Over the past few years, deep neural networks (DNNs) have
been continuously expanding their real-world applications
for source code intelligence tasks [1], [2], [3], [4]. Due to
the format similarity between source code and text [5],
Transformer [6], an attention-based neural network archi-
tecture for learning textual semantics [7], is now widely
used for source code representation learning [1], [2], [3], [4],
and becomes a state-of-the-art architecture in several code
intelligence tasks, including code completion [8], [9], code
summarization [10], [11], and program repair [12].

Unfortunately, DNNs have demonstrated to be quite
brittle to data changes [13], [14]. For example, previous
studies demonstrate that adding small perturbations to the
original input can readily trick DNNs [15], [16], revealing
that the DNNs are not robust to input variations [17],
[18]. Several studies [19], [20], [21] have been proposed
to understand the robustness of DNNs under input per-
turbations. Recently, Transformer [6] has attracted a lot of
academic attention in code intelligence tasks [8], [10], but
few studies have looked at its robustness when faced with
perturbed code. Given the growing number of Transformer-
based code intelligence models [22], [23], [24], [25], the
robustness of these models under code perturbation is of
great importance. However, developing such a robustness
veriﬁcation method for Transformer-based code intelligence
models is challenging. Directly applying the input pertur-
bation techniques in natural language processing (NLP) or
computer vision (CV) ﬁeld [26], [27] is unreasonable, since

Manuscript received April 19, 2005; revised August 26, 2015.

the perturbation on source code must guarantee that the
changed code follows syntax rules.

In this paper, we propose several semantic-preserving
code transformation strategies, and analyze the impact of
code transformation on the performance of Transformer.
Figure 1 shows an example of semantically equivalent pro-
grams, where the code summaries are produced by the
popular Transformer-based approach [10]. For the code
listed in Figure 1(a), we transform the if statement to equiv-
alent while statement, as shown in Figure 1(b), and conduct
variable renaming, as shown in Figure 1(c). However, the
resulting summarizations of Transformer on the above three
programs are radically different. Since the semantics of the
original program are kept, the model should have the same
prediction as the original program for the transformed pro-
grams. This example suggests that (1) Transformer are not
robust in code intelligence tasks when faced with semantic-
preserving transformation, and (2) different code transfor-
mation strategies have different impacts on Transformer.
Therefore, we aim at investigating whether Transformer
can maintain performance under semantic-preserving code
transformation, and the impact of different transformation
strategies.

In this work, we empirically study the effect of semantic-
preserving code transformation on the performance of
Transformer. We ﬁrstly design and implement 27 and 24
semantic-preserving transformation strategies for Java and
Python languages respectively, and group them into 5 types
of strategies according to the scope of inﬂuence under
the transformation: block transformation, insertion / dele-
tion transformation, grammatical statement transformation,
grammatical token transformation, and identiﬁer transfor-

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

Fig. 1: Examples of semantic-preserving code transformation. The ﬁgures from left to right represent the original code,
semantically equivalent programs under for-to-while transformation and variable rename transformation, respectively.

mation. Then, we apply the transformed code on three
popular code intelligence tasks: code completion (CC), code
search (CS), and code summarization (CoSum). For study-
ing whether involving syntax information such as Abstract
Syntax Trees (ASTs) is beneﬁcial for improving the robust-
ness of Transformer under code transformation, we classify
the Transformer-based code intelligence models into two
types according to the input: Seq-based Transformer and
AST-based Transformer. The seq-based Transformer only
considers sequences of code tokens as input; while AST-
based Transformer also involves parsed ASTs as input.

Besides, the positional encoding is an essential compo-
nent in Transformer [28], and has been proven effective in
Transformer-based code models [5], [10], [29]. Therefore, in
this work, we also study the impact of different positional
encoding strategies on the robustness of Transformer mod-
els under code perturbation. Speciﬁcally, two widely-used
positional encoding strategies, including absolute positional
encoding [6] and relative positional encoding [30], are cho-
sen for analysis. We aim at answering the following research
questions in the work.

RQ1: How do different code transformations impact
the performance of Transformer? (Seq-based Trans-
former)

RQ2: Is AST helpful for reducing the impact of code
transformations on the performance of Trans-
former? (AST-based Transformer)

During answering each research question, we also con-
sider different positional encoding strategies. We achieve
some ﬁndings and summarize the key ﬁndings as below.

• Code transformations such as insertion / deletion
transformation and identiﬁer transformation present
greatest impact on the performance of both seq-based
and AST-based Transformers.

• Transformer based on ASTs shows more robust perfor-
mance than the model based on only code sequence
under most code transformations.

• The relative position encoding can improve the robust-
ness of seq-based Transformer under most code trans-
formations, but has no much beneﬁt for the robustness
of AST-based Transformer.

Based on the ﬁndings, we derive some insights about
the challenges and opportunities that would beneﬁt future
research. For example, future work is expected to better
exploit ASTs to boost the robustness of Transformer mod-
els under code transformation. Besides, we also encourage
future work to explore more effective attention approach
or engage additional external knowledge to eliminate the
distraction of insertion / deletion transformation. Further-
more, future work should eliminate the impact of identiﬁers
during code representation learning, instead of relying on
the semantics of identiﬁers.

The main contributions of this paper are summarized as

follows:

• We empirically study the effect of semantic-preserving
code transformation on the performance of Transformer
for three popular code intelligence tasks.

• We design and implement 27 and 24 code transforma-
tion strategies for Java and Python languages, respec-
tively.

• We study how different aspects can impact the perfor-
mance of Transformer, including the input and posi-
tional encoding strategy.

• We achieve some ﬁndings and implications that would
beneﬁt future research in the robustness of Transformer-
based code intelligence tasks.
The rest of this paper is organized as follows. We present
the background of Transformer and code intelligence tasks
in Section 2. The technical details of our code transformation
strategies are presented in Section 3. The evaluation and
study design are shown in Section 4.Then we present the
experimental results and potential ﬁndings in Section 5.
Based on the ﬁndings, we conclude some implications and
future directions in Section 6. We discuss threats to validity
in Section 7. Finally, we give the review of the literature
related to our research in Section 8 and conclude the work
in Section 9, respectively.

2 BACKGROUND

2.1 Transformer and positional encoding

Transformer employs the typical encoder-decoder struc-
ture [6], and is composed of stacked Transformer blocks.
Each block contains a multi-head self-attention sub-layer

protectedString[] createTypesTableNames(finalClass[] types) {String[] names= newString[types.length];for(int i = 0; i < types.length; i++) {if(types[i] == null) {names[i] = null;continue;}DbEntityDescriptor ded = dbEntityManager.lookupType(types[i]);if (ded != null) {String tableName = ded.getTableName();tableName = tableName.toUpperCase();names[i] = tableName;}}return names;}prediction : create the table names for the given tableprotectedString[] createTypesTableNames(finalClass[] types) {String[] names= newString[types.length];int i=0;while( i < types.length) {if(types[i] == null) {names[i] = null;i++;continue;}DbEntityDescriptor ded= dbEntityManager.lookupType(types[i]);if (ded != null) {String tableName = ded.getTableName();tableName = tableName.toUpperCase();names[i] = tableName;}i++; }return names;}prediction : create a table descriptor for the given tableprotectedString[] createTypesTableNames(finalClass[] var_1) {String[] var_2 = newString[var_1.length];for ( var_3= 0; var_3 < var_1.length; var_3++){if(var_1[var_3] == null) {var_2[var_3] = null;continue;}DbEntityDescriptor var_4 = dbEntityManager.lookupType(var_1 [var_3]);if(var_4 != null) {String var_5= var_4.getTableName();var_5 = var_5.toUpperCase();var_2[var_3] = var_5;}}returnvar_2;}prediction : create the types of the types  (a) original code(b) replace for statement with while statement(c) rename all the variables JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

followed by a fully connected positional-wise feed-forward
network sub-layer. The sub-layers are connected by resid-
ual connections [31] and layer normalization [32]. Different
from the encoder, the decoder has attention sub-layers that
use the key and value matrices from the encoder. Positional
encoding is an essential component in Transformer [28], and
has proven effective in code intelligence tasks [5]. We then
introduce the two popular positional encoding strategies,
including absolute positional encoding [6] and relative po-
sitional encoding [30], as below.

Absolute positional encoding. The original Transformer
is supplemented by positional encoding to accommodate
for the input’s sequential nature. It transposes the sequence
of input vectors X = (x1, x2, ..., xn) into the sequence of
output vectors Z = (z1, z2, ..., zn), where xi, zi ∈ Rdmodel .
When doing self attention, Transformer ﬁrst projects the
input vector X into three vectors: the query Q, key K and
value V by trainable parameters W Q, W K , W V . The at-
tention weight is calculated using dot product and softmax
function. The output vector is the weighted sum of the value
vector:

eij =

(xiW Q)(xjW K)T
√
d

,

αij =

exp eij
k=1 exp eij

(cid:80)n

,

zi =

n
(cid:88)

j=1

αij(xjW V ),

(1)

(2)

(3)

where d is the dimension of each vector, and is used to scale
the dot product.

Relative positional encoding. To encode the pairwise
positional relationships between input elements, Shaw et al.
[30] propose the relative position encoding which models
the relation of two elements through their distance in the
input sequence. Formally, the relative position embedding
between input element xi and xj is represented as aV
ij ∈
Rd. In this way, the self attention calculated in Equ. ( 1) and
Equ. ( 3) can be rewritten as:

ij,aK

eij =

(xiW Q)(xjW K + aK
√

ij )T

dz

zi =

n
(cid:88)

j=1

αij(xjW V + aV

ij).

,

(4)

(5)

Relative position representations take the relative distance
into calculating attention rather than absolute position,
which perform more effectively and ﬂexibly.

2.2 Code intelligence task

Code completion task. Code completion is commonly used
in modern integrated development environments (IDEs) for
facilitating programming [33]. Developers use the code com-
pletion technique to predict expected code elements, such as
class names and methods, based on given code surrounding
the point of prediction [9]. Common code completion tech-
niques include token-level completion and statement-level

3

completion [34]. In our experiment, we focus on the token-
level completion, and the task is to predict the next code
token (ni) based on the previous code tokens [n0,n1,...,ni−1].
Code summarization task. The task of code summariza-
tion is to generate a natural language summary (e.g., a doc-
string) for given source code [4], [35]. Code summary can
help developers to understand the function and purpose
of code without requiring the developers to read the code
itself, which can save them time from comprehending the
details of that code [36]. For a dataset containing a set of
programs C and targeted summaries S, the task of code
summarization is to generate the summary consisting of a
sequence of token ˜s = (s0, s1, ..., sm) by maximizing the
conditional likelihood ˜s = arg maxs P (s|c) for the given
code c = (c0, c1, ..., cn) from C, where s is the correspond-
ing summary in S.

Code search task. The goal of code search is to ﬁnd
the most semantically-related code from a collection of
code based on a given natural language query [37]. In our
experiment, we focus on neural code search [38], [39], which
learns the joint embeddings of natural-language query and
code snippet [40]. The task of neural code search is to return
the expected ranking order of code snippets for the given
natural language.

3 CODE TRANSFORMATION

To verify the robustness of Transformer under input per-
turbations, we have implemented 24 and 27 semantic-
preserving code transformation strategies for Python and
Java languages, respectively. The semantic-preserving code
transformation is implemented on AST, and consists of
three phases: (1) we parse the source code into its AST
using the standard compiler tool (e.g., tree-sitter1 in our
experiments); (2) we directly modify the structure of AST
to the target code formation; (3) we convert the modiﬁed
AST to a transformed source code. This process also needs
to make sure the transformed code can be compilable and
executable.

Table 1 presents all the transformation strategies and
corresponding descriptions. To conduct a thorough inves-
tigation of the impact of transformed code on Transformer,
we classify the code transformation strategies into ﬁve types
according to the scope of inﬂuence under the transforma-
tion:

• Block transformation;
• Insertion / deletion transformation;
• Grammatical statement transformation;
• Grammatical token transformation;
• Identiﬁer transformation.

For instance, the bool to int transformation converts the
Boolean value from True/False to 1/0 and only affects the
changed Boolean token, hence it belongs to grammatical token
transformation.

Block transformation (denoted as T B). This type refers
to the code transformation that impacts the code at block
level, as shown in Figure 2 (b). The example illustrated in
Figure 1 (b) is a block transformation, where the if-statement

1. https://tree-sitter.github.io/tree-sitter/

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

Fig. 2: Example of the code transformation. Figure (a) is an AST schematic, and ﬁgures (b)-(f) illustrate the different
structure changes at the AST level with different code transformations.

is transformed to the equivalent while-statement. The type
contains seven code transformation strategies in total.

Insertion / deletion transformation (denoted as T ID).
This type of transformation is implemented at the statement
level. The transformation generally adds new statements or
deleting existing ones without impacting other statements
in the program. During implementation, only sub-trees are
added or deleted at the AST level, as depicted in Figure 2 (c).
For example, the remove unused variable transformation will
remove the variable declaration statement if the variable is
never used again, and such change will not affect other
statements in the program. This type consists of 7 code
transformation strategies.

Grammatical statement

transformation (denoted as
T GS). This type of transformation is also operated at the
statement level. Different from T ID, T GS changes the state-
ments in the original code, as depicted in Figure 2 (d). For
example, the change the unary operator replaces the statement
i++ by i=i+1. This type has 10 code transformation strate-
gies.

Grammatical token transformation (denoted as T GT).
This type of transformation changes the original code at the
token level, and includes six code transformation strategies.
As illustrated in Figure 2 (e), the transformation only affects
the type and value of associated AST nodes, with the
structure unchanged. Note that the type of transformation
does not involve identiﬁers. For instance, the bool to int trans-
formation converts the Boolean operator from True/False to
1/0.

Identiﬁer transformation (denoted as T I). This type of
transformation is also implemented at token level but manly
operates on identiﬁers, as illustrated in Figure 2 (f). The
type includes two transformation strategies, including func-

tion rename transformation and variable rename transformation.
For example, the the variable rename transformation renames
the identiﬁers (variable name) with placeholders such as
var1 and var2. Additionally, some strategies cannot be im-
plemented for both languages considering the language-
speciﬁc characteristics. For example, the Python language
does not support the increment and decrement operators,
so the change the unary operator transformation strategy is
only allowed in Java. The Java language treats Boolean as
a unique data type with two distinct values: True and False,
so the bool to int transformation strategy is only applicable for
Python.

4 EXPERIMENTAL SETUP
4.1 Datasets and pre-processing

Following the prior work [41], we choose the Java and
Python datasets from CodeSearchNet [37] for evaluation.
CodeSearchNet is a collection of large datasets with code-
document pairs from open source projects on GitHub, which
is widely-used in code intelligence tasks [42], [43]. Detailed
data statistics are illustrated in Table 2. The subject data
consist of 165K / 5K / 11K training / validation / test
code snippets for Java and 252K / 14K / 15K for Python,
respectively. For facilitating analysis, we parse the code
snippets into ASTs, and traverse the ASTs into sequences
of tokens as input in depth-ﬁrst-search order following [5].

4.2 Implementation

In this work, we consider three code intelligence tasks:
code completion, code summarization, and code search. We
elaborate on the detailed implementation of the three tasks
in the following.

identifierblock-levelstatement-level(d) Grammatical statement transformation(b) Block transformation(c) Insertion/deletion transformation(a) Original AST(e) Grammatical token transformation(f) Identifier transformationnewstatementgrammatical tokenJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

No.

Transformation strategy

Description of transformation

Java

Python

TABLE 1: Semantic-preserving code transformation in our experiment.

Block transformation

B-1
B-2

B-3

B-4

B-5

B-6

B-7

ID-1
ID-2
ID-3
ID-4
ID-5
ID-6
ID-7

For statement
While statement

Elseif to if else

Else if to elseif

If to else

Change if statement

Create new function

Add comments
Add junk code
Add return statement
Import library
Delete comment
Delete print
Remove unused variable

GS-1

Change return statement

GS-2

GS-3
GS-4

GS-5

GS-6

GS-7

GS-8
GS-9

For move in variable declaration

For move out variable declaration
Change variable declaration

Add logical operator

Change comparison operator

Change argument assignment operator

Change the unary operator
Add curly bracket

GS-10 Delete curly bracket

GT-1
GT-2
GT-3
GT-4
GT-5
GT-6

Bool to int
Int to bool
Upper integral type
Upper ﬂoating type
Change input API
Change output API

I-1
I-2

Function rename
Variable rename

replace the for statement by equivalent while statement
replace the while statement by equivalent for statement
convert enseif to else if, for example, if(x==1){ }else{if(x==2){ } } becomes if (x==1){ }
else if (x==2){ }
convert else if to elseif, for example, if(x==1){ }else if (x==2) { } becomes if (x==1) { }
else { if(x==2){ } }
use logical not operator to change the condition of if statement and exchange the
block of if and else, for example, if(x==0){ block1 }else{ block2 } becomes if !(x==0)
{ block2 } else { block1 }
if the condition of if statement has logical operator(e.g, && ), it will split the if statement (cid:33)
move the variable initialization statement to generate a new function, and then call
(cid:37)
the function, for example, z = x+y becomes def func(x,y): return x+y z=func(x,y)

(cid:33)

(cid:33)

(cid:33)

(cid:33)
(cid:33)

Insertion / deletion transformation.

.

insert comments not related to the source code
add code that not related to the source code
add a return statement at the end of the source code that returns the default value
import libraries unrelated to source code
remove all the comments from source code
replace the print statement by empty statement, for example, replace print by pass
remove the variable declaration statement if the variable is never used

Grammatical statement transformation

if the return statement returns an integer literal, we will declare a variable, and
return the variable
move the variable declaration into for statement. For instance, int i; for(i=0;i¡10;i++)
becomes for(int i=0;i¡10;i++)
move the variable declaration out of for statement
split the variable declaration and initialization, for example, int i=0 becomes int i i=0
add the logical not operator and use opposite comparison operator, for example, x<y
becomes !(y>=x)
use the opposite comparison operator, for example, x<y becomes y>x
change the argument assignment operator to assignment operator, for example, x+=1
becomes x=x+1
change the increment or decrease operator. For instance, x++ becomes x=x+1
add a curly brace to a single statement and then generate a new compound statement
if the compound statement has only a single statement, delete curly of the compound
statement

Grammatical token transformation.

replace True or False by 1 or 0
replace 1 or 0 by True or False
replace the integral type by a higher type, for example, replace int by long
replace integral type by ﬂoating type or replace ﬂoat by double
change the API for reading input
change the API for writing output

Identiﬁer transformation

rename the function name and class name
rename the variable name

(cid:33)
(cid:33)
(cid:33)
(cid:33)
(cid:33)
(cid:33)
(cid:33)

(cid:33)

(cid:33)

(cid:33)
(cid:33)

(cid:33)

(cid:33)

(cid:33)

(cid:33)
(cid:33)

(cid:33)

(cid:37)
(cid:37)
(cid:33)
(cid:33)
(cid:37)
(cid:33)

(cid:33)
(cid:33)

(cid:33)
(cid:37)

(cid:33)

(cid:33)

(cid:33)

(cid:33)

(cid:33)

(cid:33)
(cid:33)
(cid:33)
(cid:33)
(cid:33)
(cid:33)
(cid:33)

(cid:33)

(cid:37)

(cid:37)
(cid:37)

(cid:33)

(cid:33)

(cid:33)

(cid:37)
(cid:37)

(cid:37)

(cid:33)
(cid:33)
(cid:33)
(cid:33)
(cid:33)
(cid:33)

(cid:33)
(cid:33)

TABLE 2: Statistics of experimental data.

Language

Train

Valid

Test

Python
Java

251,820
164,923

13,914
5,183

14,918
10,955

Code completion. We use the setup, metrics and imple-
mentation of Transformer according to [8]. Besides, we split
code sequence whose length is over 500 following [5].

Code summarization. In our experiments, we select [10]
as Transformer implementation except that we do not split
sub-token following Chirkova et al. [5].

Code search. In our experiments, we implement a Trans-
former architecture for code search task based on [44]. We
process the dataset following the strategy of Evangelos et.
al [44]. For example, we ﬁlter the non-ASCII tokens and
replace symbols by their English names (e.g., the symbol +
in code token is replace by addoperator).

Hyper-parameters. The hyper-parameters setting in our
experiments follows Transformer implementations [8], [10],

[44] and we list the major hyper-parameters for code com-
pletion, code summarization, and code search tasks. Our
Transformer models include 6 layers, 6 heads with the layers
of our models to be 512. We set the maximum distance of
relative attention to 32 for all tasks. We train Transformers
using Adam with a starting learning rate of 0.0001 and a
batch size of 32 / 32 / 128 with the epoch number being 15
/ 20 / 100 for code completion, summarization, and search
respectively. We train all models on 4 GPUs of Nvidia Tesla
V100 with 32G of memory.

Evaluation set. As illustrated in Table 1, not all the
transformation strategies are applicable for both program-
ming languages. For example, the bool to int strategy is only
allowed in Python. During analyzing the impact of each
code transformation strategy on models’ performance, we
only evaluate on the transformable code instead of all the
code in the test set. Besides, to minimize the performance
bias, we run each experiment for three times and report the
average results.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

4.3 Evaluation metrics

4.3.1 Code summarization

We evaluate the source code summarization performance
using three metrics: BLEU, METEOR and ROUGE-L.

BLEU is a widely-used metric in natural language pro-
cessing and software engineering ﬁelds to evaluate the
quality of generated texts, e.g., machine translation, code
comment generation, and code commit message genera-
tion [45]. It computes the frequencies of the co-occurrence
of n-grams between the ground truth ˆy and the generated
sequence y to judge the similarity:
(cid:32) N
(cid:88)

(cid:33)

BLEU-N = b(y, ˆy) · exp

βn log pn(y, ˆy)

,

n=1

where b(y, ˆy) indicates the brevity penalty, and pn(y, ˆy) and
βn represent the geometric average of the modiﬁed n-gram
precision and the weighting parameter, respectively.

ROUGE-L is commonly used in natural language trans-
lation [46], and is a F-measure based on the Longest Com-
mon Subsequence (LCS) between candidate and target se-
quences, where the LCS is a set of words appearing in the
two sequences in same order.

ROU GE-L =

(cid:0)1 + β2(cid:1) RlcsPlcs
Rlcs + β2Plcs

,

where Rlcs = LCS(X,Y )
. X and
Y denote candidate sequence and reference sequence, re-
spectively. LCS(X, Y ) represents the length of the longest
common sub-sequence between X and Y .

and Plcs = LCS(X,Y )

len(X)

len(Y )

Meteor is an evaluation metric proposed based on BLEU
[47]. It introduces synonym, stem, and other information to
replace the precise matching in BLEU, and strengthens the
role of recall in automatic evaluation.

4.3.2 Code search and code completion

For code search and code completion tasks, we use MRR
[48] as the evaluation metric. MRR is the average of the
reciprocal rank of results of a set of queries. The reciprocal
rank of a query is the inverse of the rank of the ﬁrst hit
result.

M RR =

1
N

N
(cid:88)

(
n=1

1
ranki

),

where N is the total number of targets (tokens for code
completion and code snippet for code search) in data pool
and ranki represents the position of the i-th true target in
the ranking results.

5 RESEARCH QUESTIONS AND RESULT

ANALYSIS

Our experimental study aims to answer the following re-
search questions:

RQ1: How do different code transformations impact
the performance of Transformer? (Seq-based Trans-
former)

RQ2: Is AST helpful for reducing the impact of code
transformations on the performance of Trans-
former? (AST-based Transformer)

RQ1 aims at discovering which types of code transfor-
mation show greatest impact on the robustness of Trans-
former. RQ2 aims at analyzing whether AST is beneﬁcial
for improving the robustness of Transformer under different
code transformations. During answering both RQs, we also
consider the impact of different positional encoding strate-
gies.

5.1 Answer to RQ1: Impact on seq-based Transformer

In this section, we compare the performance of Transformer
before and after different code transformations for three
different tasks. Table 3, Table 4 and Table 5 present the
overall results of code completion, code search and code
summarization, respectively.

]

5.1.1 Different types of code transformation on code se-

quence

In the section, we analyze the effects of different types
of code transformation on the performance of seq-based
Transformer. We observe that seq-based Transformer’s per-
formance is affected to varying degrees by different types of
code transformations. We elaborate on the detailed impact
of different types of code transformation in the following.

Block transformation (T B). As shown in Table 3-5, we
observe that Transformer demonstrates robust performance
under block transformation on all code intelligence tasks. For
example, the MRR values for the code completion task
just decrease by 0.81% and 0.65% for Java and Python,
respectively (seen in Table 3).

Insertion / deletion transformation (T ID). From Table 3-
5, we observe that insertion / deletion transformation has a
substantial impact on Transformer on all code intelligence
tasks. For example, the decrease of Transformer on the code
search task is from 23.34% to 26.77% (seen in Table 4). When
generating Java’s code summary, the BLEU, ROUGE-L, and
METEOR values decrease by 5.31%, 7.85%, and 10.84%,
respectively (seen in Table 5).

Grammatical statement transformation (T GS). We ﬁnd
that seq-based Transformer shows robust performance un-
der grammatical statement transformation. For instance, the
impact of this type of code transformation on the code
search task is 0.30% and -0.87% for Java and Python (seen in
Table 4), respectively.

Grammatical token transformation (T GT). From the
experimental results, we observe that the grammatical token
transformation has a slight inﬂuence on all tasks. For ex-
ample, the MRR score has a decrease of 1.02% and 3.19%
respectively for Java and Python on the code completion
task, respectively (seen in Table 3).

Identiﬁer transformation (T I). We observe that this
type of code transformation has a substantial impact on
all code intelligence tasks. For instance, the MRR score has
a decrease of 42.21% and 38.22% for Java and Python un-
der identiﬁer transformation in code search task, respectively
(seen in Table 4).

Finding 1: Transformer’s performance is affected to vary-
ing degrees by different types of code transformations.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
TABLE 3: Results of code transformation on the perfor-
mance of seq-based Transformer for the code completion
task. Column “Pos.” represents the position encoding
strategy, while “abs.” and “rel.” represent the absolute/rel-
ative position encoding, respectively. Column “w. t.” and
“w/o t.” represent the results with and without code
transformation. Different rows represent the results of
different types of code transformation, while the bottom
portion T all presents the average results. The red color
indicates the degree of decrease.

Type

Pos. w/o t.

MRR (Java)
w. t.

imp. (%) w/o t.

MRR (Python)
w. t.

imp. (%)

T B

T ID

T GS

T GT

T I

T all

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

0.7243
0.7343

0.5925
0.5999

0.7201
0.7304

0.7180
0.7277

0.7169
0.7255

0.6944
0.7036

0.7185
0.7290

0.5295
0.5677

0.7082
0.7186

0.7107
0.7191

0.6485
0.6544

0.6631
0.6777

↓ 0.81
↓ 0.72

↓ 10.62
↓ 5.37

↓ 1.66
↓ 1.61

↓ 1.02
↓ 1.18

↓ 9.54
↓ 9.81

↓ 4.50
↓ 3.67

0.6581
0.6708

0.5483
0.5580

0.6488
0.6614

0.6509
0.6637

0.6594
0.6703

0.6331
0.6448

0.6539
0.6672

0.4801
0.5376

0.6377
0.6502

0.6301
0.6565

0.6019
0.6136

0.6007
0.6250

↓ 0.65
↓ 0.54

↓ 12.45
↓ 3.65

↓ 1.72
↓ 1.69

↓ 3.19
↓ 1.08

↓ 8.73
↓ 8.46

↓ 5.12
↓ 3.07

TABLE 4: Results of code transformation on the perfor-
mance of seq-based Transformer for the code search task.
The red and green colors indicate the degree of decrease
and increase, respectively.

Type

Pos. w/o. t.

MRR (Java)
w. t.

imp. (%) w/o. t.

MRR (Python)
w. t.

imp. (%)

T B

T ID

T GS

T GT

T I

T all

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

0.3630
0.3601

0.3249
0.3452

0.4208
0.4350

0.4403
0.4496

0.4363
0.4428

0.3971
0.4066

0.3611
0.3614

0.2379
0.2599

0.4221
0.4367

0.4397
0.4485

0.2521
0.2586

0.3426
0.3530

↓ 0.52
↑ 0.35

↓ 26.77
↓ 24.71

↑ 0.30
↑ 0.39

↓ 0.14
↓ 0.26

↓ 42.21
↓ 41.59

↓ 13.73
↓ 13.17

0.3853
0.3808

0.2802
0.2730

0.3891
0.3861

0.3761
0.3629

0.4093
0.4050

0.3680
0.3616

0.3834
0.3779

0.2148
0.2080

0.3857
0.3838

0.3368
0.3248

0.2529
0.2516

0.3147
0.3092

↓ 0.49
↓ 0.76

↓ 23.34
↓ 23.82

↓ 0.87
↓ 0.60

↓ 10.46
↓ 10.52

↓ 38.22
↓ 37.88

↓ 14.47
↓ 14.47

Based on the above analysis, we achieve that seq-based
Transformer shows robust performance under block transfor-
mation, grammatical statement transformation and grammatical
token transformation, but suffers from obvious performance
degradation under insertion / deletion transformation and
identiﬁer transformation. For example, in code completion
task, the decreases of MRR score for Java under insertion /
deletion transformation and identiﬁer transformation are 10.62%
and 9.54%, respectively, while the decreases caused by
block transformation, grammatical statement transformation and
grammatical token transformation are 0.81%, 1.66%, and 1.02%,
respectively.

Finding 2: The insertion / deletion transformation and
identiﬁer transformation present greatest impact on the
performance of seq-based Transformer.

7

default implementation of ID-2 in other code transforma-
tion experiments. ID-2 front / end represents the junk code
inserted before and after the given code, respectively. The
results of inserting junk code at different locations for the
code search and summarization tasks are shown in Table 6.
We observe that Transformer’s performance degrades more
evidently when junk code is inserted at the front of the given
code than when it is inserted randomly or at the end. For
example, on the code search task, the decrease of the ID-2
front / middle / end on seq-based Transformer are 42.67%
/ 25.31% / 15.95% for Java respectively, while the decrease
for Python are 88.19% / 14.78% / 10.56%, respectively.

Finding 3: Inserting junk code at the front of the given
code can affect Transformer’s performance more than
inserting junk code at other locations.

5.1.2 Absolute position v.s. relative position

The section looks into whether position encoding im-
pacts Transformer’s performance under code transforma-
tion. From Table 3-5, we ﬁnd that Transformer with rela-
tive position encoding (TransformerRel) shows more robust
performance better compared to that with absolute position
encoding (TransformerAbs) on three code intelligence tasks.
For example, the overall MRR score of TransformerAbs has
a decrease of 4.50% and 5.12% for Java and Python on the
code completion task, while the decrease of TransformerRel
is 3.67% and 3.07%, respectively. More speciﬁcally, we ﬁnd
that the inﬂuence of relative position encoding is more
obvious under insertion / deletion transformation than other
types of code transformation on the code completion task.
For example, the TransformerAbs and TransformerRel’ MRR
values under insertion / deletion transformation decrease by
10.62% and 5.37% for Java and 12.45% and 3.65% for Python,
respectively, while the decreases under grammatical state-
ment transformation are are 1.66% and 1.61% for Java and
1.72% v.s. 3.65% for Python, respectively (T ID and T GS in
Table 3). The robustness improvement may be attributed to
that the relative position encoding enhances the relationship
between the tokens with the adjacent preceding tokens [30],
making Transformer’s predictions more accurate on code
completion.

Finding 4: Seq-based Transformer with relative position
encoding shows more robust performance compared to
that with absolute position encoding under most code
transformations.

To sum up, seq-based Transformer shows robust per-
formance under block transformation, grammatical statement
transformation and grammatical token transformation. How-
ever, insertion / deletion transformation and identiﬁer trans-
formation have substantial impact on Transformer’s perfor-
mance. Besides, the relative position encoding can improve
the robustness of seq-based Transformer under most code
transformations.

During experimentation, we also ﬁnd that inserting junk
code at different locations (ID-2 front / middle / end) has
different impact on the performance of seq-based Trans-
former. ID-2 middle represents the cases where the junk
code is inserted inside the given code, and it is also the

5.2 Answer to RQ2: Impact on AST-based Transformer

In this section, we compare the performance of AST-based
Transformer before and after different code transformations
for three different tasks. Table 7, Table 8 and Table 9 present

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
TABLE 5: Results of code transformation on the performance of seq-based Transformer for the code summarization
task. The above part presents the results of Transformer with absolute/relative position encoding strategies (“abs.” and
“rel.”) for Java, and the below part presents results for Python. The red and green color indicate the degree of decrease
and increase, respectively.

8

Java

Type

Pos. w/o. t.

BLEU
w. t

Imp. (%) w/o. t.

ROUGE-L
w. t

Imp. (%) w/o. t. w. t

METEOR

T B

T ID

T GS

T GT

T I

T all

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

11.73
12.09

11.06
11.04

12.73
12.93

12.92
13.18

13.80
13.70

12.45
12.59

11.83
12.08

10.48
10.49

12.68
12.93

12.83
13.16

12.69
12.86

12.10
12.30

↑ 0.81
↓ 0.11

↓ 5.31
↓ 5.03

↓ 0.40
↓ 0.03

↓ 0.63
↓ 0.16

↓ 8.02
↓ 6.13

↓ 2.78
↓ 2.28

21.82
22.36

18.74
19.18

23.30
23.55

22.94
23.75

24.44
24.93

22.25
22.75

21.93
22.31

17.27
17.56

23.19
23.50

22.82
23.68

21.77
22.48

21.40
21.91

Python

Type

Pos. w/o. t.

BLEU
w. t

Imp. (%) w/o. t.

ROUGE-L
w. t

T B

T ID

T GS

T GT

T I

T all

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

12.83
13.49

11.43
11.79

12.83
13.45

13.18
13.71

13.84
14.33

12.82
13.36

12.72
13.37

10.83
11.34

12.78
13.40

12.99
13.58

13.17
13.82

12.50
13.10

↓ 0.90
↓ 0.93

↓ 5.21
↓ 3.81

↓ 0.41
↓ 0.40

↓ 1.41
↓ 0.94

↓ 4.83
↓ 3.57

↓ 2.52
↓ 1.90

21.12
22.46

18.34
19.02

21.60
22.90

21.09
22.40

23.06
24.11

21.04
22.18

21.00
22.26

16.70
17.71

21.53
22.79

20.60
22.07

21.41
22.71

20.25
21.51

↑ 0.50
↓ 0.22

↓ 7.85
↓ 8.45

↓ 0.45
↓ 0.21

↓ 0.50
↓ 0.31

↓ 10.95
↓ 9.84

↓ 3.83
↓ 3.73

Imp.

↓ 0.53
↓ 0.88

↓ 8.95
↓ 6.87

↓ 0.29
↓ 0.49

↓ 2.34
↓ 1.50

↓ 7.13
↓ 5.80

↓ 3.76
↓ 3.02

6.08
6.93

5.59
6.11

6.93
7.53

6.78
7.63

7.53
8.29

6.58
7.30

6.30
6.92

4.98
5.14

7.02
7.58

6.86
7.57

5.04
6.42

6.04
6.72

Imp. (%)

↑ 3.74
↓ 0.03

↓ 10.84
↓ 15.86

↑ 1.35
↑ 0.72

↑ 1.17
↓ 0.78

↓ 33.10
↓ 22.61

↓ 8.20
↓ 7.82

METEOR

w/o. t. w. t

Imp. (%)

5.60
6.76

5.06
5.86

5.59
7.08

4.78
6.42

6.52
7.78

5.51
6.78

5.49
6.58

4.36
5.10

5.51
6.95

4.52
6.29

5.37
6.85

5.05
6.35

↓ 1.87
↓ 2.73

↓ 13.89
↓ 13.09

↓ 1.34
↓ 1.77

↓ 5.41
↓ 1.91

↓ 17.71
↓ 11.99

↓ 8.34
↓ 6.29

TABLE 6: Impact of different insert locations of junk code
on seq-based Transformer. CS and CSM represent code
search and code summarization tasks, respectively. The
“Loc.” represents the insert locations, and middle / front
/ end represent inserting junk code to the given code at
the middle / at the front / at the end, respectively. The red
color indicates the degree of decrease.

CS

Loc.

Pos. w/o. t.

MRR (Java)
w. t.

imp. (%) w/o. t.

MRR (Python)
w. t.

imp. (%)

front

middle

end

abs.
rel.

abs.
rel.

abs.
rel.

0.4404
0.4461

0.4404
0.4461

0.4404
0.4461

0.2525
0.2606

0.3290
0.3305

0.3702
0.3748

↓ 42.67
↓ 41.57

↓ 25.31
↓ 25.91

↓ 15.95
↓ 15.98

0.3901
0.4117

0.3901
0.4117

0.3901
0.4117

0.0461
0.0564

0.3325
0.3324

0.3489
0.3456

↓ 88.19
↓ 86.30

↓ 14.78
↓ 19.26

↓ 10.56
↓ 16.05

CSM

Loc.

Pos. w/o. t.

front

middle

end

abs.
rel.

abs.
rel.

abs.
rel.

13.80
13.70

13.80
13.70

13.80
13.70

BLEU (Java)
w. t.

imp. (%) w/o. t.

BLEU (Python)
w. t.

imp. (%)

12.07
12.29

12.13
12.30

12.24
12.42

↓ 12.49
↓ 10.31

↓ 12.06
↓ 10.27

↓ 11.28
↓ 9.39

13.85
14.35

13.85
14.35

13.85
14.35

11.67
12.43

12.27
13.11

12.47
13.25

↓ 15.76
↓ 13.38

↓ 11.41
↓ 8.64

↓ 9.99
↓ 7.64

the overall results of code completion, code search and code
summarization on ASTs, respectively.

5.2.1 Different types of code transformation on ASTs

In this section, we analyze the effects of different types
of code transformation on the performance of AST-based
Transformer. We observe that insertion / deletion transforma-
tion and identiﬁer transformation also have substantial impact

TABLE 7: Results of code transformation on the perfor-
mance of AST-based Transformer for the code completion
task. The red and green colors indicate the degree of
decrease and increase, respectively.

Type

Pos. w/o. t.

MRR (Java)
w. t.

imp. (%) w/o. t.

MRR (Python)
w. t.

imp. (%)

T B

T ID

T GS

T GT

T I

T all

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

0.8030
0.8050

0.6563
0.6578

0.7996
0.8026

0.8001
0.8017

0.7896
0.7916

0.7697
0.7717

0.8043
0.8066

0.6346
0.6361

0.7951
0.7983

0.7941
0.7961

0.7201
0.7210

0.7496
0.7516

↑ 0.16
↑ 0.20

↓ 3.31
↓ 3.29

↓ 0.55
↓ 0.53

↓ 0.75
↓ 0.70

↓ 8.81
↓ 8.91

↓ 2.61
↓ 2.60

0.7630
0.7646

0.6308
0.6325

0.7554
0.7577

0.7531
0.7547

0.7569
0.7589

0.7319
0.7337

0.7624
0.7644

0.6208
0.6244

0.7504
0.7526

0.7444
0.7477

0.7098
0.7107

0.7175
0.7200

↓ 0.08
↓ 0.01

↓ 1.59
↓ 1.28

↓ 0.67
↓ 0.67

↓ 1.17
↓ 0.92

↓ 6.23
↓ 6.35

↓ 1.95
↓ 1.87

on AST-based Transformer. For example, the decrease of
AST-based Transformer on predicting Java’s code are 3.31%
and 8.81% under insertion / deletion transformation and iden-
tiﬁer transformation, respectively, while the decrease of that
under grammatical statement transformation and grammatical
token transformation are 0.55% and 0.75%, respectively. We
elaborate on the detailed impact of ASTs on different types
of code transformation in the following.

Block transformation (T B) has a minor impact on AST-
based Transformer’s performance as Table 7-9 shows. And
AST-based Transformer shows more robust performance
than seq-based Transformer. For example, when it comes
to generating Python’s code summary, the values of BLEU,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
TABLE 8: Results of code transformation on the perfor-
mance of AST-based Transformer for the code search task.
The red and green colors indicate the degree of decrease
and increase, respectively.

Type

Pos. w/o. t.

MRR (Java)
w. t.

imp. (%) w/o. t.

MRR (Python)
w. t.

imp. (%)

T B

T ID

T GS

T GT

T I

T all

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

0.3594
0.3698

0.3368
0.3625

0.3978
0.4225

0.4388
0.4617

0.4287
0.4439

0.3923
0.4121

0.3599
0.3702

0.3220
0.3462

0.3980
0.4220

0.4366
0.4607

0.2428
0.2538

0.3519
0.3706

↑ 0.14
↑ 0.09

↓ 4.41
↓ 4.49

↑ 0.04
↓ 0.13

↓ 0.49
↓ 0.21

↓ 43.36
↓ 42.83

↓ 10.31
↓ 10.08

0.3990
0.3957

0.2898
0.2847

0.4111
0.4057

0.3924
0.3910

0.4245
0.4235

0.3834
0.3801

0.3997
0.3953

0.2238
0.2168

0.4097
0.4046

0.3448
0.3415

0.2644
0.2678

0.3285
0.3252

↑ 0.18
↓ 0.10

↓ 22.75
↓ 23.85

↓ 0.35
↓ 0.29

↓ 12.14
↓ 12.67

↓ 37.71
↓ 36.77

↓ 14.32
↓ 14.46

ROUGE-L, and METEOR decrease by 0.06%, 0.09%, and
0.44% on AST-based Transformer, while the decreases on
seq-based Transformer are 0.90%, 0.53%, 1.87%, respectively.
Insertion / deletion transformation (T ID). We ﬁnd
that incorporating ASTs into Transformer can increase the
model’s robustness under insertion / deletion transformation
on the code completion task, the Python’s code summariza-
tion task and the Java’s code search task. For example, the
MRR score has a decrease of 10.62% and 12.45% for Java and
Python on seq-based Transformer (Seen in Table 3), while
the decrease is 3.31% and 1.59% on AST-based Transformer
(Seen in Table 7), respectively.

In addition, inserting code snippets (ID-2) at the front
of the given code also affects AST-based Transformer’s
performance more than other locations as the seq-based
Transformer. Besides, we observe that AST-based shows
more robust performance under different locations of ID-
2 than seq-based Transformer as shown in Table 10. For
example, on the Java’s code search task, the decrease of the
ID-2 front / middle / end on seq-based Transformer are 42.67%
/ 25.31% / 15.95% respectively (Seen in Table 6), while the
decrease on AST-based Transformer are 14.79% / 8.66% /
4.17%, respectively.

Grammatical statement transformation (T GS) has a mi-
nor effect on AST-based Transformer. For instance, the MRR
values decrease by 0.55% and 0.67%, respectively, when in
Java’s and Python’s code completion.

Grammatical token transformation (T GT). From the
results, we also ﬁnd that grammatical token transformation has
a much smaller inﬂuence on AST-based Transformer than
seq-based Transformer. For example, the MRR score of AST-
based Transformer under grammatical token transformation
has a decrease of 0.75% and 1.17% for Java and Python
in code completion task, respectively, while the decrease of
seq-based Transformer is 1.02% and 3.19%, respectively.

Identiﬁer transformation (T I) also results in a large
quality drop of Transformer on traverse ASTs. We take
the code search task under identiﬁer transformation as an
example, AST-based Transformer have decreased MRR by
43.36% and 37.71% for Java and Python, respectively, while
seq-based Transformer have decreased MRR by 42.21% and
38.22%. This results shows that AST-based Transformer
is also vulnerable to identiﬁer transformation as seq-based
Transformer.

9

Compared to the results of seq-based Transformer, we
observe that utilizing ASTs can help Transformer maintain
performance better. For example, the overall MRR score of
seq-based Transformer has a decrease of 4.50% on the Java’s
code completion task (seen in Table 3), while the decrease
of AST-based Transformer is 2.61%. Similar results can be
seen in Python (5.12% v.s. 1.95%). In the code search task,
the overall decreases of performance on seq-based Trans-
former are 13.73% and 14.77% (seen in Table 4) for Java and
Python, whereas the decreases on AST-based Transformer
are 10.31% and 14.32% (seen in Table 8), respectively.

Finding 5: Compared to seq-based Transformer, AST-
based Transformer demonstrates more robust perfor-
mance under most code transformations.

5.2.2 Absolute position v.s. relative position on ASTs

This section looks into whether position encoding has an
impact on the AST-based Transformer’s performance under
code transformation. From Table 7-9, we ﬁnd that relative
position encoding does not make obvious improvement on
AST-based Transformer’s robustness under code transfor-
mations. For example, the decrease in overall MRR score
on Java’s code completion task is 2.61% and 2.60% for
absolute and relative position encoding, while the decrease
on Python is 1.95% and 1.87%, respectively. Similar results
can be seen on the code search task (10.31% v.s. 10.08% for
Java and 14.32% v.s. 14.46% for Python).

Finding 6: Relative position encoding does not evidently
improve the robustness of AST-based Transformer under
code transformation.

To sum up, the greatest impact of two types of code
transformations on AST-based Transformer are also inser-
tion / deletion transformation and identiﬁer transformation.
And AST-based Transformer performs more robustly com-
pared to seq-based Transformer. Besides, relative position
encoding does not evidently improve the robustness of AST-
based Transformer when faced with code transformation.

6 DISCUSSION
The experiments in Section 5 have demonstrated that inser-
tion/deletion transformation and identiﬁer transformation have a
substantial impact on the robustness of Transformer. In this
section, we discuss the Transformer’s robustness from three
perspectives: the input-ASTs, insertion / deletion transforma-
tion, and identiﬁer transformation.

6.1 Effect of ASTs on the models’ robustness

Based on the experimental results in Section 5.2, we observe
that AST-based Transformer performs just marginally better
than seq-based Transformer under different code transfor-
mations. Besides, the relative position encoding does not ob-
viously improve the robustness of AST-based Transformer.
Moreover, AST-based Transformers still cannot well under-
stand the underlying semantic information of transformed
insertion / deletion transformation and identiﬁer
code (e.g.,
transformation). The results indicate that exploiting ASTs for
learning robust code semantics is still challenging.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
TABLE 9: Results of code transformation on the performance of AST-based Transformer for the code summarization
task. The above part presents the results of Transformer with absolute/relative position encoding strategies (“abs.”
and “rel.”) for Java, and the below part presents results for Python. The red and green colors indicate the degree of
decrease and increase, respectively.

10

Java

Type

Pos. w/o. t.

BLEU
w. t

Imp. (%) w/o. t.

ROUGE-L
w. t

Imp. (%) w/o. t. w. t

METEOR

T B

T ID

T GS

T GT

T I

T all

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

11.89
12.40

11.03
11.10

12.70
12.99

12.93
13.25

13.94
13.82

12.50
12.71

11.89 → 0.00
↑ 0.22
12.43

10.31
10.51

12.84
13.00

12.88
13.26

12.92
13.06

12.17
12.45

↓ 6.45
↓ 5.31

↑ 1.14
↑ 0.12

↓ 0.39
↑ 0.08

↓ 7.31
↓ 5.53

↓ 2.62
↓ 2.05

21.85
22.92

18.59
19.10

23.06
23.40

23.03
23.65

24.65
24.99

22.23
22.81

21.81
22.92

16.61
16.99

23.32
23.31

22.76
23.52

22.05
22.90

21.31
21.93

↓ 0.19
↓ 0.01

↓ 10.62
↓ 11.02

↑ 1.12
↓ 0.39

↓ 1.17
↓ 0.55

↓ 10.53
↓ 8.36

↓ 4.16
↓ 3.87

6.91
7.53

5.86
6.17

7.67
8.14

7.38
8.06

8.24
8.66

7.21
7.71

7.01
7.54

4.87
5.20

7.76
8.18

7.34
7.93

6.77
7.47

6.75
7.26

Python

Type

Pos. w/o. t.

BLEU
w. t

Imp. (%) w/o. t.

ROUGE-L
w. t

Imp. (%) w/o. t. w. t

METEOR

T B

T ID

T GS

T GT

T I

T all

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

abs.
rel.

12.51
13.11

10.72
11.47

12.39
12.97

12.76
13.41

12.93
13.80

12.26
12.95

12.50
13.05

10.59
11.13

12.37
12.93

12.72
13.31

12.78
13.45

12.19
12.77

↓ 0.06
↓ 0.47

↓ 1.15
↓ 2.99

↓ 0.16
↓ 0.35

↓ 0.38
↓ 0.73

↓ 1.16
↓ 2.56

↓ 0.57
↓ 1.39

18.89
19.66

15.90
16.70

19.09
19.79

19.13
19.79

19.69
21.06

18.54
19.40

18.88
19.60

15.54
15.78

19.03
19.71

19.01
19.74

19.25
20.13

18.34
18.99

↓ 0.09
↓ 0.29

↓ 2.25
↓ 5.50

↓ 0.34
↓ 0.41

↓ 0.68
↓ 0.26

↓ 2.23
↓ 4.39

↓ 1.09
↓ 2.10

4.85
5.94

4.16
5.11

4.97
5.97

4.74
5.68

5.35
6.37

4.81
5.81

4.83
5.94

3.98
4.51

4.90
5.97

4.78
5.59

4.93
5.61

4.68
5.52

Imp. (%)

↑ 1.39
↑ 0.09

↓ 16.88
↓ 15.71

↑ 1.18
↑ 0.43

↓ 0.52
↓ 1.56

↓ 17.93
↓ 13.83

↓ 6.43
↓ 5.84

Imp. (%)

↓ 0.44
↑ 0.10

↓ 4.21
↓ 11.64

↓ 1.46
↓ 0.01

↑ 0.99
↓ 1.65

↓ 7.79
↓ 11.98

↓ 2.65
↓ 4.98

TABLE 10: Impact of different insert locations of junk code
on AST-based Transformer. CS and CSM represent code
search and code summarization tasks, respectively. The
“Loc.” represents the insert locations, and middle / front
/ end represent inserting junk code to the given code at
the middle / at the front / at the end, respectively. The red
color indicates the degree of decrease.

CS

Loc.

Pos. w/o. t.

MRR (Java)
w. t.

imp. (%) w/o. t.

MRR (Python)
w. t.

imp. (%)

front

middle

end

abs.
rel.

abs.
rel.

abs.
rel.

0.4184
0.4242

0.4184
0.4242

0.4184
0.4242

0.3565
0.3448

0.3821
0.3826

0.4009
0.4026

↓ 14.79
↓ 18.72

↓ 8.66
↓ 9.81

↓ 4.17
↓ 5.10

0.4364
0.4200

0.4364
0.4200

0.4364
0.4200

0.0601
0.0389

0.3750
0.3564

0.3981
0.3860

↓ 86.23
↓ 90.74

↓ 14.06
↓ 15.13

↓ 8.78
↓ 8.09

CSM

Loc.

Pos. w/o. t.

front

middle

end

abs.
rel.

abs.
rel.

abs.
rel.

13.94
13.82

13.94
13.82

13.94
13.82

BLEU (Java)
w. t.

imp. (%) w/o. t.

BLEU (Python)
w. t.

imp. (%)

12.66
12.47

12.75
12.68

12.86
12.80

↓ 9.18
↓ 9.77

↓ 8.54
↓ 8.25

↓ 7.75
↓ 7.40

12.93
13.81

12.93
13.81

12.93
13.81

12.47
12.42

12.63
13.08

12.66
13.12

↓ 3.56
↓ 10.09

↓ 2.35
↓ 5.28

↓ 2.11
↓ 5.02

We suggest that researchers can propose more effective
approaches to learn the code representations based on ASTs,
and evaluate their impact on the robustness of code intelli-
gence models.

Implication 1: Transformer still faces challenges in ex-
ploiting ASTs for learning robust code semantics, and
more exploration is needed to effectively capture semantic
information from ASTs.

6.2 Effect of insertion / deletion transformation

From the experimental results in Section 5.1 and 5.2, we
observe that the insertion / deletion transformation has a
substantial impact on all tasks. The results indicate that
inserting some junk data (e.g., comments, code snippets,
or libraries) can greatly bias Transformer-based code intelli-
gence models. This may be because that the models fail to
attend to the important part in the transformed code.

One way to address the issue is to propose a more
effective attention model for identifying the useful part in
the input code. Another way is to incorporate additional
external knowledge such as API documentation to elimi-
nate the distraction of insertion / deletion transformation. For
example, if Transformer-based models can recognize noisy
part in the input based on the external information, the
robustness of the models would be improved under the
insertion / deletion transformation.

Implication 2: Transformer needs a more effective at-
tention approach or additional external knowledge to
eliminate the distraction of noisy information in the input
code.

6.3 Effect of identiﬁer transformation

Since identiﬁers are an important part to express the natu-
ralness of code, their importance has been studied in many
works [49], [50]. Based on the experimental results in Sec-
tion 5.1 and 5.2, we observe that the identiﬁer transformation
have a substantial impact on both seq-based and AST-based
Transformer. The results demonstrate the importance of

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

identiﬁers for code intelligence tasks, and that it is hard
for Transformer to learn the code semantics under identiﬁer
transformation.

Identiﬁers could help models for better code under-
standing [51], [52]. However, most existing code intelligence
models rely on identiﬁers rather than underlying semantic
information, resulting in models being vulnerable to iden-
tiﬁer transformation. For example, some identiﬁer-based ad-
versarial attack strategies [53], [54] for code intelligence have
been proposed in recent years. Future work is encouraged
to explore how to effectively understand the code semantics
under identiﬁer transformation.

Implication 3: Future work is expected to well exploit the
underlying semantics of identiﬁers rather than completely
rely on the literal meanings of the identiﬁers.

7 THREATS TO VALIDITY
In this section, we describe the possible threats we may face
in this study and discuss how we mitigate them.

Internal validity is mainly about the data prepossessing
and training models. To reduce the threats, we conduct
experiments based on the released code scripts, and the
default training hyper-parameters for all models. Besides,
we run each experiment for three times and compute the
average results for all tasks.

Construct validity is mainly about the suitability of
our evaluation metrics. To reduce this risk, we select the
most widely-used metrics for different tasks to evaluate
the impact. For example, we use BLEU [45], ROUGE-L
[46] and Meteor [47] to evaluate the impact of different
transformations on code summarization task.

External validity is mainly concerned with dataset we
use. In our experiments, we select Java and Python datasets
from CodeSearchNet. However, CodeSearchNet has some
problems that will affect our experiments. For example,
some instances of code in CodeSearchNet cannot be parsed
into an abstract syntax tree. To reduce the threats, we ﬁlter
the datasets following the previous work [55]. To further
reduce the threats, we plan to collect more open-source
projects to reproduce our experiments.

8 RELATED WORK
8.1 Code intelligence task

Code completion task. The success of deep learning boosts
a series of code completion works based on deep learning
models [56], [57]. For example, Alon et al. [58] proposed
a structural language model based on Transformer, which
leverages the syntax to model the code snippet as a tree
to complete code. Liu et al. [59] pretrained a language
model with a Transformer-based architecture and ﬁne-tuned
it for code completion. Kim et al. [8] proposed TravTrans, a
Transformer-based approach that leverages ASTs for code
completion.

Code summarization task. In recent years, many works
that applied deep learning models in code summarization
achieved great success and became more and more popular
[60], [61]. For instance, Iyer et al. [62] proposed an LSTM-
based model with attention to generate code summaries for

11

source code. Wei et al. [63] and Zhang et al. [35] further
introduced retrieved information for summarizing source
code with the help of most similar code snippets. Alon
et al. [64] sampled and encoded random AST paths into
LSTMs to generate summaries of source code. Ahmad et al.
[10] utilized Transformer on code summarization for better
mapping the source code to their corresponding natural
language summaries.

Code search task. The goal of code search is to ﬁnd the
most semantically related code from a collection of code
based on a given natural language [37]. The traditional
code search techniques are mainly based on information
retrieval [65], [66], while the most popular approaches in
recent years are based on deep neural networks [38], [39].
Sachdev et al. [67] proposed a neural code search engine
which used a basic word–embedding for a code corpus.
Gu et al. [3] proposed an RNN-based code search model to
represent source code and natural language queries through
joint embedding. Fan et al. [40] utilized a self-attention
network to construct a code representation network for
building the semantic relationship between code snippets
and queries. Besides, large-scale pre-train models such as
Codebert [4] and GraphCodeBERT [68] also demonstrated
good performance on the code search task.

8.2 Code transformation

Code transformations are widely used for compiler opti-
mizations [69], [70], testability transformation [71], [72], [73],
refactoring [74], [75], etc. In recent years, there have been
an amount of works in the ﬁeld of code intelligence that
employs code transformation.

Data augmentation. Yu et al. [76] applied a source-to-
source transformation strategy to increase the generaliza-
tion capacity of deep learning models based on program-
level data augmentation, and Wang et al. [77] incorporated
curriculum learning into program-level data augmentation
in order to optimize the efﬁciency of ﬁne-tuning pre-trained
models. Bui et al. [78] proposed a self-supervised contrastive
learning framework to generate transformed code snippets
and identify semantically-equivalent code snippets from
large-scale unlabeled data of source code, which could
generate additional dataset for ﬁne-turning the pre-training
models.

Adversarial attack. Quiring et al. [79] used semantics-
preserving code transformations to generate adversarial ex-
amples and present a black-box attack that seeks examples
by Monte-Carlo tree search. Li et al. [80] leveraged code
transformations to attack DL-based detectors and decouple
feature learning and classiﬁer learning to present a static
analysis-based vulnerability detector. Zhang et al. [81] ap-
plied reinforcement learning to select semantics-preserving
samples and proposed a black-box attack approach against
GNN malware detection models

Others applications. Chen et al. [82] suggested a sym-
bolic execution acceleration framework by machine-learning
based compiler optimization tuning based on code trans-
formation. Wu et al. [83] proposed an automating CUDA
synchronization framework for bug detection at the LLVM-
bitcode level via program transformation.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8.3 Robustness of code intelligence

Given the increased research interest in code intelligence,
some works on the robustness of source code representation
models are proposed [53], [54], [84], [85], [86], [87].

Yefet et al. [84] proposed an adversarial attacks gener-
ation approach for code via gradient based optimization,
and it was effective in generating both targeted and non-
targeted attacks. Rabin et al. [85] deﬁned the generalizability
in neural program models and studied the generalizability
of method name prediction models using six semantic-
preserving changes. Ramakrishnan et al. [87] proposed an
adversarial-training method for neural models of code to
enhance the robustness. Zhang et al. [53] deﬁned the robust
and non-robust features of DNNs, and proposed an identi-
ﬁer renaming algorithm (namely Metropolis-Hastings Mod-
iﬁer) for adversarial example generation on source code.
Yang et al. [54] designed a black-box attack approach that
was aware of natural semantics when generating adversarial
examples of code, which was better than that of Zhang
et al. [53]. Bielik et al. [86] reﬁned the representation of
source code and applied adversarial-training to improve
robustness of neural models while preserving high accuracy.

9 CONCLUSION AND FUTURE WORK
In this study, we have empirically investigated the robust-
ness and limitations of Transformer on code intelligence.
We implement 27 and 24 code transformation strategies
for Java and Python languages respectively and apply the
transformed code to three code intelligence tasks to study
the effect on Transformer. Experimental results demonstrate
that insertion / deletion transformation and identiﬁer trans-
formation have the great impact on Transformer’s perfor-
mance. Transformer based on ASTs shows more robust
performance than the model based on only code sequence
under most code transformations. Besides, the robustness
of Transformer under code transformation is impacted by
the design of positional encoding. Based on the ﬁndings,
we summarize some future directions for improving the
robustness of Transformer on code intelligence.

In the future, we plan to investigate the robustness of
pre-trained models under code transformation. Moreover,
we plan to collect more open-source projects to reproduce
our experiments and to support more programming lan-
guages in our code transformation.

REFERENCES

[1] M. White, M. Tufano, C. Vendome, and D. Poshyvanyk, “Deep
learning code fragments for code clone detection,” in 2016 31st
IEEE/ACM International Conference on Automated Software Engineer-
ing (ASE).

IEEE, 2016, pp. 87–98.

[2] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment
generation,” in 2018 IEEE/ACM 26th International Conference on
Program Comprehension (ICPC).

IEEE, 2018, pp. 200–20 010.

[3] X. Gu, H. Zhang, and S. Kim, “Deep code search,” in 2018
IEEE/ACM 40th International Conference on Software Engineering
(ICSE).

IEEE, 2018, pp. 933–944.

[4] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong,
L. Shou, B. Qin, T. Liu, D. Jiang et al., “Codebert: A pre-trained
model for programming and natural languages,” arXiv preprint
arXiv:2002.08155, 2020.

[5] N. Chirkova and S. Troshin, “Empirical study of transformers
for source code,” in Proceedings of the 29th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, 2021, pp. 703–715.

12

[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
in Advances in neural information processing systems, 2017, pp. 5998–
6008.

[7] T. Wolf, J. Chaumond, L. Debut, V. Sanh, C. Delangue, A. Moi,
P. Cistac, M. Funtowicz, J. Davison, S. Shleifer et al., “Transformers:
State-of-the-art natural language processing,” in Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing:
System Demonstrations, 2020, pp. 38–45.
S. Kim, J. Zhao, Y. Tian, and S. Chandra, “Code prediction by
feeding trees to transformers,” in 2021 IEEE/ACM 43rd Interna-
tional Conference on Software Engineering (ICSE).
IEEE, 2021, pp.
150–162.

[8]

[9] M. Ciniselli, N. Cooper, L. Pascarella, A. Mastropaolo, E. Aghajani,
D. Poshyvanyk, M. Di Penta, and G. Bavota, “An empirical study
on the usage of transformer models for code completion,” IEEE
Transactions on Software Engineering, 2021.

[10] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “A
transformer-based approach for source code summarization,” in
Proceedings of the 58th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), 2020.

[11] S. Gao, C. Gao, Y. He, J. Zeng, L. Y. Nie, and X. Xia, “Code structure
guided transformer for source code summarization,” arXiv preprint
arXiv:2104.09340, 2021.

[12] B. Berabi, J. He, V. Raychev, and M. Vechev, “Tﬁx: Learning to
ﬁx coding errors with a text-to-text transformer,” in International
Conference on Machine Learning. PMLR, 2021, pp. 780–791.
[13] X. Gao, R. K. Saha, M. R. Prasad, and A. Roychoudhury, “Fuzz
testing based data augmentation to improve robustness of deep
neural networks,” in 2020 IEEE/ACM 42nd International Conference
on Software Engineering (ICSE).

IEEE, 2020, pp. 1147–1158.

[14] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y. Liu, J. Zhao,
B. Li, J. Yin, and S. See, “Deephunter: a coverage-guided fuzz
testing framework for deep neural networks,” in Proceedings of
the 28th ACM SIGSOFT International Symposium on Software Testing
and Analysis, 2019, pp. 146–157.

[15] C.-Y. Ko, Z. Lyu, L. Weng, L. Daniel, N. Wong, and D. Lin,
“Popqorn: Quantifying robustness of recurrent neural networks,”
in International Conference on Machine Learning. PMLR, 2019, pp.
3468–3477.

[16] S. Garg and G. Ramakrishnan, “Bae: Bert-based adversarial exam-

ples for text classiﬁcation,” arXiv preprint arXiv:2004.01970, 2020.

[17] N. Carlini and D. Wagner, “Towards evaluating the robustness of
neural networks,” in 2017 ieee symposium on security and privacy
(sp).

IEEE, 2017, pp. 39–57.

[18] D. Jakubovitz and R. Giryes, “Improving dnn robustness to ad-
versarial attacks using jacobian regularization,” in Proceedings of
the European Conference on Computer Vision (ECCV), 2018, pp. 514–
529.

[19] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing
of deep-neural-network-driven autonomous cars,” in Proceedings
of the 40th international conference on software engineering, 2018, pp.
303–314.

[20] T. S. Borkar and L. J. Karam, “Deepcorrect: Correcting dnn models
against image distortions,” IEEE Transactions on Image Processing,
vol. 28, no. 12, pp. 6022–6034, 2019.

[21] J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, “Hotﬂip: White-
box adversarial examples for text classiﬁcation,” arXiv preprint
arXiv:1712.06751, 2017.

[22] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, “Intellicode
compose: Code generation using transformer,” in Proceedings of the
28th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering, 2020, pp.
1433–1443.

[23] Z. Tang, C. Li, J. Ge, X. Shen, Z. Zhu, and B. Luo, “Ast-transformer:
Encoding abstract syntax trees efﬁciently for code summariza-
tion,” in 2021 36th IEEE/ACM International Conference on Automated
Software Engineering (ASE).

IEEE, 2021, pp. 1193–1195.

[24] Z. Li, Y. Wu, B. Peng, X. Chen, Z. Sun, Y. Liu, and D. Paul,
“Setransformer: A transformer-based code semantic parser for
code comment generation,” IEEE Transactions on Reliability, 2022.

[25] Z. Sun, Q. Zhu, Y. Xiong, Y. Sun, L. Mou, and L. Zhang, “Treegen:
A tree-based transformer architecture for code generation,” in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34,
no. 05, 2020, pp. 8984–8991.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

13

[26] X. Zheng, J. Zeng, Y. Zhou, C.-J. Hsieh, M. Cheng, and X.-J. Huang,
“Evaluating and enhancing the robustness of neural network-
based dependency parsing models with adversarial examples,”
in Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, 2020, pp. 6600–6610.

[27] S. Ren, Y. Deng, K. He, and W. Che, “Generating natural lan-
guage adversarial examples through probability weighted word
saliency,” in Proceedings of the 57th annual meeting of the association
for computational linguistics, 2019, pp. 1085–1097.

[28] G. Ke, D. He, and T.-Y. Liu, “Rethinking positional encoding
in language pre-training,” in International Conference on Learning
Representations, 2020.

[29] V. Shiv and C. Quirk, “Novel positional encodings to enable
tree-based transformers,” Advances in Neural Information Processing
Systems, vol. 32, 2019.

[30] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative

position representations,” arXiv preprint arXiv:1803.02155, 2018.

[31] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in Proceedings of the IEEE conference on computer
vision and pattern recognition, 2016, pp. 770–778.

[32] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv

preprint arXiv:1607.06450, 2016.

[33] A. Svyatkovskiy, S. Lee, A. Hadjitoﬁ, M. Riechert, J. V. Franco,
and M. Allamanis, “Fast and memory-efﬁcient neural code com-
pletion,” in 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR).

IEEE, 2021, pp. 329–340.

[34] M. Izadi, R. Gismondi, and G. Gousios, “Codeﬁll: Multi-token
code completion by jointly learning from structure and naming
sequences,” arXiv preprint arXiv:2202.06689, 2022.

[35] J. Zhang, X. Wang, H. Zhang, H. Sun, and X. Liu, “Retrieval-
based neural source code summarization,” in 2020 IEEE/ACM
42nd International Conference on Software Engineering (ICSE).
IEEE,
2020, pp. 1385–1397.

[36] A. LeClair, S. Haque, L. Wu, and C. McMillan, “Improved code
summarization via a graph neural network,” in Proceedings of the
28th international conference on program comprehension, 2020, pp.
184–195.

[37] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis,

and
M. Brockschmidt, “Codesearchnet challenge: Evaluating the
state of semantic code search,” arXiv preprint arXiv:1909.09436,
2019.

[38] J. Cambronero, H. Li, S. Kim, K. Sen, and S. Chandra, “When deep
learning met code search,” in Proceedings of the 2019 27th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, 2019, pp. 964–974.
[39] J. Gu, Z. Chen, and M. Monperrus, “Multimodal representation
for neural code search,” in 2021 IEEE International Conference on
Software Maintenance and Evolution (ICSME).
IEEE, 2021, pp. 483–
494.

[40] S. Fang, Y.-S. Tan, T. Zhang, and Y. Liu, “Self-attention networks
for code search,” Information and Software Technology, vol. 134, p.
106542, 2021.

[41] M. R. Parvez, W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang,
“Retrieval augmented code generation and summarization,” in
Findings of the Association for Computational Linguistics: EMNLP
2021, 2021, pp. 2719–2734.

[42] A. Mastropaolo, S. Scalabrino, N. Cooper, D. N. Palacio, D. Poshy-
vanyk, R. Oliveto, and G. Bavota, “Studying the usage of text-to-
text transfer transformer to support code-related tasks,” in 2021
IEEE/ACM 43rd International Conference on Software Engineering
(ICSE).

IEEE, 2021, pp. 336–347.

[43] J. Lin, Y. Liu, Q. Zeng, M. Jiang, and J. Cleland-Huang, “Traceabil-
ity transformed: Generating more accurate links with pre-trained
bert models,” in 2021 IEEE/ACM 43rd International Conference on
Software Engineering (ICSE).

IEEE, 2021, pp. 324–335.

[44] E. Papathomas, T. Diamantopoulos, and A. Symeonidis, “Semantic
code search in software repositories using neural machine trans-
lation,” in International Conference on Fundamental Approaches to
Software Engineering. Springer, 2022, pp. 225–244.

[45] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method
for automatic evaluation of machine translation,” in Proceedings of
the 40th annual meeting of the Association for Computational Linguis-
tics, 2002, pp. 311–318.

[46] C.-Y. Lin, “Rouge: A package for automatic evaluation of sum-
maries,” in Text summarization branches out, 2004, pp. 74–81.
[47] S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt
evaluation with improved correlation with human judgments,” in

Proceedings of the acl workshop on intrinsic and extrinsic evaluation
measures for machine translation and/or summarization, 2005, pp. 65–
72.

[48] D. R. Radev, H. Qi, H. Wu, and W. Fan, “Evaluating web-based

question answering systems.” in LREC. Citeseer, 2002.

[49] A. LeClair, S. Jiang, and C. McMillan, “A neural model for generat-
ing natural language summaries of program subroutines,” in 2019
IEEE/ACM 41st International Conference on Software Engineering
(ICSE).

IEEE, 2019, pp. 795–806.

[50] U. Z. Ahmed, P. Kumar, A. Karkare, P. Kar, and S. Gulwani, “Com-
pilation error repair: for the student programs, from the student
programs,” in Proceedings of the 40th International Conference on
Software Engineering: Software Engineering Education and Training,
2018, pp. 78–87.

[51] V. Efstathiou and D. Spinellis, “Semantic source code models
using identiﬁer embeddings,” in 2019 IEEE/ACM 16th International
Conference on Mining Software Repositories (MSR).
IEEE, 2019, pp.
29–33.

[52] J. Lacomis, P. Yin, E. Schwartz, M. Allamanis, C. Le Goues, G. Neu-
big, and B. Vasilescu, “Dire: A neural approach to decompiled
identiﬁer naming,” in 2019 34th IEEE/ACM International Conference
on Automated Software Engineering (ASE).
IEEE, 2019, pp. 628–639.
[53] H. Zhang, Z. Li, G. Li, L. Ma, Y. Liu, and Z. Jin, “Generating
adversarial examples for holding robustness of source code pro-
cessing models,” in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, vol. 34, no. 01, 2020, pp. 1169–1176.

[54] Z. Yang, J. Shi, J. He, and D. Lo, “Natural attack for pre-trained

models of code,” arXiv preprint arXiv:2201.08698, 2022.

[55] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,
C. Clement, D. Drain, D. Jiang, D. Tang et al., “Codexglue: A
machine learning benchmark dataset for code understanding and
generation,” arXiv preprint arXiv:2102.04664, 2021.

[56] A. Svyatkovskiy, Y. Zhao, S. Fu, and N. Sundaresan, “Pythia: Ai-
assisted code completion system,” in Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data
Mining, 2019, pp. 2727–2735.

[57] Y. Wang and H. Li, “Code completion by modeling ﬂattened
abstract syntax trees as graphs,” Proceedings of AAAIConference on
Artiﬁcial Intellegence, 2021.

[58] U. Alon, R. Sadaka, O. Levy, and E. Yahav, “Structural language
models of code,” in International conference on machine learning.
PMLR, 2020, pp. 245–256.

[59] F. Liu, G. Li, Y. Zhao, and Z. Jin, “Multi-task learning based pre-
trained language model for code completion,” in Proceedings of
the 35th IEEE/ACM International Conference on Automated Software
Engineering, 2020, pp. 473–485.

[60] Y. Zhu and M. Pan, “Automatic code summarization: A systematic

literature review,” arXiv preprint arXiv:1909.04352, 2019.

[61] Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S.
Yu, “Improving automatic source code summarization via deep
reinforcement learning,” in Proceedings of the 33rd ACM/IEEE In-
ternational Conference on Automated Software Engineering, 2018, pp.
397–407.

[62] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Summarizing
source code using a neural attention model,” in Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), 2016, pp. 2073–2083.

[63] B. Wei, Y. Li, G. Li, X. Xia, and Z. Jin, “Retrieve and re-
ﬁne: exemplar-based neural comment generation,” in 2020 35th
IEEE/ACM International Conference on Automated Software Engineer-
ing (ASE).

IEEE, 2020, pp. 349–360.

[64] U. Alon, S. Brody, O. Levy, and E. Yahav, “code2seq: Generating se-
quences from structured representations of code,” in International
Conference on Learning Representations, 2019.

[65] R. Sindhgatta, “Using an information retrieval system to retrieve
source code samples,” in Proceedings of the 28th international confer-
ence on Software engineering, 2006, pp. 905–908.

[66] S. E. Sim, M. Umarji, S. Ratanotayanon, and C. V. Lopes, “How
well do search engines support code retrieval on the web?” ACM
Transactions on Software Engineering and Methodology (TOSEM),
vol. 21, no. 1, pp. 1–25, 2011.

[67] S. Sachdev, H. Li, S. Luan, S. Kim, K. Sen, and S. Chandra,
“Retrieval on source code: a neural code search,” in Proceedings of
the 2nd ACM SIGPLAN International Workshop on Machine Learning
and Programming Languages, 2018, pp. 31–41.

[68] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,
A. Svyatkovskiy, S. Fu et al., “Graphcodebert: Pre-training code
representations with data ﬂow,” arXiv preprint arXiv:2009.08366,
2020.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

14

[69] K. D. Cooper, A. Grosul, T. J. Harvey, S. Reeves, D. Subramanian,
L. Torczon, and T. Waterman, “Exploring the structure of the space
of compilation sequences using randomized search algorithms,”
The Journal of Supercomputing, vol. 36, no. 2, pp. 135–151, 2006.
[70] J. Cavazos, G. Fursin, F. Agakov, E. Bonilla, M. F. O’Boyle, and
O. Temam, “Rapidly selecting good compiler optimizations using
performance counters,” in International Symposium on Code Genera-
tion and Optimization (CGO’07).

IEEE, 2007, pp. 185–197.

[71] D. W. Binkley, M. Harman, and K. Lakhotia, “Flagremover: a testa-
bility transformation for transforming loop-assigned ﬂags,” ACM
Transactions on Software Engineering and Methodology (TOSEM),
vol. 20, no. 3, pp. 1–33, 2011.

[72] P. McMinn, D. Binkley, and M. Harman, “Empirical evaluation of a
nesting testability transformation for evolutionary testing,” ACM
Transactions on Software Engineering and Methodology (TOSEM),
vol. 18, no. 3, pp. 1–27, 2009.

[73] A. Baresel, D. Binkley, M. Harman, and B. Korel, “Evolutionary
testing in the presence of loop-assigned ﬂags: A testability trans-
formation approach,” ACM SIGSOFT software engineering notes,
vol. 29, no. 4, pp. 108–118, 2004.

[74] B. Daniel, D. Dig, K. Garcia, and D. Marinov, “Automated testing
of refactoring engines,” in Proceedings of the the 6th joint meeting of
the European software engineering conference and the ACM SIGSOFT
symposium on The foundations of software engineering, 2007, pp. 185–
194.

[75] T. Mens and T. Tourw´e, “A survey of software refactoring,” IEEE
Transactions on software engineering, vol. 30, no. 2, pp. 126–139, 2004.
[76] S. Yu, T. Wang, and J. Wang, “Data augmentation by program

transformation,” Journal of Systems and Software, p. 111304, 2022.

[77] D. Wang, Z. Jia, S. Li, Y. Yu, Y. Xiong, W. Dong, and X. Liao,
“Bridging pre-trained models and downstream tasks for source
code understanding,” arXiv preprint arXiv:2112.02268, 2021.
[78] N. D. Bui, Y. Yu, and L. Jiang, “Self-supervised contrastive learning
for code retrieval and summarization via semantic-preserving
transformations,” in Proceedings of the 44th International ACM SI-

GIR Conference on Research and Development in Information Retrieval,
2021, pp. 511–521.

[79] E. Quiring, A. Maier, and K. Rieck, “Misleading authorship
attribution of source code using adversarial learning,” in 28th
{USENIX} Security Symposium ({USENIX} Security 19), 2019, pp.
479–496.

[80] Z. Li, J. Tang, D. Zou, Q. Chen, S. Xu, C. Zhang, Y. Li, and H. Jin,
“Towards making deep learning-based vulnerability detectors ro-
bust,” arXiv preprint arXiv:2108.00669, 2021.

[81] L. Zhang, P. Liu, and Y.-H. Choi, “Semantic-preserving reinforce-
ment learning attack against graph neural networks for malware
detection,” arXiv preprint arXiv:2009.05602, 2020.

[82] J. Chen, W. Hu, L. Zhang, D. Hao, S. Khurshid, and L. Zhang,
“Learning to accelerate symbolic execution via code transforma-
tion,” in 32nd European Conference on Object-Oriented Programming
(ECOOP 2018).
Schloss Dagstuhl-Leibniz-Zentrum fuer Infor-
matik, 2018.

[83] M. Wu, L. Zhang, C. Liu, S. H. Tan, and Y. Zhang, “Automating
cuda synchronization via program transformation,” in 2019 34th
IEEE/ACM International Conference on Automated Software Engineer-
ing (ASE).

IEEE, 2019, pp. 748–759.

[84] N. Yefet, U. Alon, and E. Yahav, “Adversarial examples for models
of code,” Proceedings of the ACM on Programming Languages, vol. 4,
no. OOPSLA, pp. 1–30, 2020.

[85] M. R. I. Rabin, N. D. Bui, K. Wang, Y. Yu, L. Jiang, and M. A.
Alipour, “On the generalizability of neural program models with
respect to semantic-preserving program transformations,” Infor-
mation and Software Technology, vol. 135, p. 106552, 2021.

[86] P. Bielik and M. Vechev, “Adversarial robustness for code,” in
PMLR, 2020, pp.

International Conference on Machine Learning.
896–907.

[87] G. Ramakrishnan, J. Henkel, Z. Wang, A. Albarghouthi, S. Jha, and
T. Reps, “Semantic robustness of models of source code,” arXiv
preprint arXiv:2002.03043, 2020.

