An Extractive-and-Abstractive Framework for Source Code
Summarization
Chunrong Fangâˆ—
fangchunrong@nju.edu.cn
State Key Laboratory for Novel
Software Technology
Nanjing University, China

Weisong Sun
weisongsun@smail.nju.edu.cn
State Key Laboratory for Novel
Software Technology
Nanjing University, China

Yuchen Chen
yuc.chen@outlook.com
State Key Laboratory for Novel
Software Technology
Nanjing University, China

2
2
0
2

n
u
J

5
1

]
E
S
.
s
c
[

1
v
5
4
2
7
0
.
6
0
2
2
:
v
i
X
r
a

Quanjun Zhang
quanjun.zhang@smail.nju.edu.cn
State Key Laboratory for Novel
Software Technology
Nanjing University, China

Yifei Ge
gyf991213@126.com
State Key Laboratory for Novel
Software Technology
Nanjing University, China

Guanhong Tao
taog@purdue.edu
Purdue University
West Lafayette, Indiana, USA

Tingxu Han
hantingxv@163.com
School of Information Management
Nanjing University, China

Yudu You
nju_yyd@163.com
State Key Laboratory for Novel
Software Technology
Nanjing University, China

Bin Luo
luobin@nju.edu.cn
State Key Laboratory for Novel
Software Technology
Nanjing University, China

ABSTRACT
(Source) Code summarization aims to automatically generate sum-
maries/comments for a given code snippet in the form of natural
language. Such summaries play a key role in helping developers
understand and maintain source code. Existing code summarization
techniques can be categorized into extractive methods and abstrac-
tive methods. The extractive methods extract a subset of important
statements and keywords from the code snippet using retrieval
techniques, and generate a summary that preserves factual details
in important statements and keywords. However, such a subset may
miss identifier or entity naming, and consequently, the naturalness
of generated summary is usually poor. The abstractive methods
can generate human-written-like summaries leveraging encoder-
decoder models from the neural machine translation domain. The
generated summaries however often miss important factual details.
To generate human-written-like summaries with preserved fac-
tual details, we propose a novel extractive-and-abstractive frame-
work. The extractive module in the framework performs a task of
extractive code summarization, which takes in the code snippet and
predicts important statements containing key factual details. The
abstractive module in the framework performs a task of abstrac-
tive code summarization, which takes in the entire code snippet
and important statements in parallel and generates a succinct and

âˆ—Corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference 2022, xxx, 2022, xxx
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/xxx/xxx

human-written-like natural language summary. We evaluate the
effectiveness of our technique, called EACS, by conducting exten-
sive experiments on three datasets involving six programming
languages. Experimental results show that EACS significantly out-
performs state-of-the-art techniques in terms of all three widely
used metrics, including BLEU, METEOR, and ROUGH âˆ’ L.

CCS CONCEPTS
â€¢ Software and its engineering â†’ Software maintenance tools.

KEYWORDS
Code Summarization, Program Comprehension

ACM Reference Format:
Weisong Sun, Chunrong Fang, Yuchen Chen, Quanjun Zhang, Guanhong
Tao, Tingxu Han, Yifei Ge, Yudu You, and Bin Luo. 2022. An Extractive-and-
Abstractive Framework for Source Code Summarization. In Conference 2022:
xxx, xxx, 2022, xxx. ACM, New York, NY, USA, 13 pages. https://doi.org/
xxx/xxx

1 INTRODUCTION
Code comments play a significant role in facilitating code compre-
hension [23, 68, 77, 80] and software maintenance [11, 13, 27, 82].
Writing high-quality code comments has been recognized as a
good programming practice [13, 82]. The code comment is one
of the most common summaries used during software develop-
ment. However, commenting source code is a labour-intensive and
time-consuming task [13, 36]. As a result, good comments are of-
ten absent, unmatched, and outdated during the evolution [32].
The lack of high-quality code comments is a common problem in
software industry [32]. (Source) code summarization is an active
research field [1, 11, 15, 23, 25, 26, 35, 41, 57, 62, 66, 86, 87], which
aims at designing advanced techniques to support automatic gen-
eration of code summaries (i.e., comments). Given a code snippet

 
 
 
 
 
 
Conference 2022, xxx, 2022, xxx

Weisong Sun and Chunrong Fang, et al.

(a method or function) by the developer, code summarization can
generate summaries related to the code snippet. Figure 1 shows
an example. The code snippet in Figure 1(a) is provided by the
developer. The summary â€œremoves the mapping for the specified
key from this cache if present.â€ in Figure 1(b) is a possible sum-
mary that satisfies the developerâ€™s requirement. The summary is
usually a succinct description in natural language summarizing the
intention/functionality of the desired code snippet [11].

Figure 1: Example of Code Snippet and Summary

Code summarization can be viewed as a special kind of text sum-
marization task, where the text is not a traditional natural language
but a programming language. Indeed, as early as ten years ago,
automatic text summarization techniques have been introduced to
automatic code summarization [25], detailed in Section 2.1. There-
fore, similar to text summarization [16, 20], code summarization
can be subdivided into extractive code summarization (extractive
methods) and abstractive code summarization (abstractive methods).
Most of the early code summarization techniques are extractive
methods, which widely use an indexing-retrieval framework to
generate summaries [15, 19, 25, 56]. They first index terms in code
snippets and then retrieve top-ğ‘› key terms as summaries. The terms
in summaries are extracted from the current code snippet [53], con-
text code snippets [49] or similar code snippets [15, 76]. Therefore,
extractive methods can produces a summary that preserves the con-
ceptual integrity and factual information of the input code snippet.
However, as introduced in [30, 32, 76], extractive methods have two
main limitations: 1) they fail to extract accurate keywords when the
identifiers and methods are poorly named; 2) they rely on similar
code snippets can be retrieved and how similar the code snippets
are. Therefore, there may be semantic inconsistencies between the
retrieved summary and the input code snippet [42]. In addition,
the naturalness of the summaries generated by extractive methods
usually is poor [34, 42].

Recently, with the success of deep learning (DL) in abstractive
text summarization, DL-based code summarization techniques have
been proposed one after another [2, 18, 31, 32, 34, 39, 40, 43, 52, 55,
61, 70, 71, 75, 84]. Compared with extractive methods, similar to
abstractive text summarization [63], DL-based code summariza-
tion techniques have stronger abstract expression capabilities and
can generate human-written-like summaries. Therefore, we call
DL-based code summarization techniques as abstractive methods.
Abstractive methods widely adopt DL-based the encoder-decoder
neural network and then train it on a large code-comment cor-
pus. The well-trained encoder-decoder model can transform code
snippets into embedding representations and then decode the em-
bedding representations into short natural language summaries. As

in many research areas and as chronicled by Allamanis et al. [3],
traditional methods have largely given way to deep learning meth-
ods based on big data input. Although the abstractive methods have
the ability to generate novel words and phrases not featured in
the code snippet â€“ as a human-written abstract usually does, the
generated summaries often miss important factual details in the
code snippet, detailed in Section 3.

In this paper, we propose an extractive-and-abstractive frame-
work for code summarization, which inherits the advantages of
extractive methods and abstractive methods and shields their respec-
tive disadvantages. Specifically, we utilize pairs of code snippets
and comments to train an extractor (an extractive method) and an
abstracter (an abstractive method). The well-trained extractor can
be used to predict important statements in code snippets. These
important statements and the entire code snippet are input to the
abstracter to generate a short natural language summary. The well-
trained abstracter first utilizes two separate encoders to transform
important statements and the entire code snippet into two vectors,
called extractive and abstractive embedding representations, re-
spectively. Then, the two embedding representations are fused to
produce a fusion embedding representation, which will be passed
to a decoder to generate a natural language summary. Compared
with existing abstractive methods, our extractive-and-abstractive
framework is equipped with an extractor, substantially balancing
attention on important information and global contextual informa-
tion, respectively, reducing the risk of missing important factual
details and improving the overall performance.

In summary, we make the following contributions.

â€¢ To the best of our knowledge, we are the first to propose
an extractive-and-abstractive framework for code summa-
rization, which inherits the advantages of extractive methods
and abstractive methods and shields their respective disad-
vantages. It is a general framework and can be combined
with multiple advanced models (see experimental results in
Section 5.2.2).

â€¢ We implement a code summarization prototype called EACS
based on the extractive-and-abstractive framework. EACS is
able to generate a succinct natural language summary that
not only is human-written-like but also preserves important
factual details.

â€¢ We conduct extensive experiments on three widely used
datasets to evaluate EACS. Experimental results show that
EACS improves the BLEU score of code summarization by
5.40% to 6.55% compared to state-of-the-art technique. In
addition, EACS achieves METEOR of 30.39 and 23.70, outper-
forming the state-of-the-art by 12.14% to 12.75%. In terms of
ROUGE âˆ’ L, EACS improves by 6% to 7.24%. The source code
of EACS and all the data used in this paper are released and
can be downloaded from the website [5].

2 BACKGROUND
2.1 Automatic Text Summarization
When design EACS, we drew on the advanced ideas and techniques
of automatic text summarization. Therefore, we first introduce the
background of automatic text summarization.

publicVremove(finalKkey){VoldValue=cacheMap.remove(key);if(oldValue!=null){LOG.debug("Removedcacheentryfor'{}'",key);}returnoldValue;}(a) A Code Snippet ğ‘!removes the mapping for the specified key from this cache if present. (b) A Summary ğ‘ !An Extractive-and-Abstractive Framework for Source Code Summarization

Conference 2022, xxx, 2022, xxx

also widely adopt DL-based encoder-decoder architectures. Figure 2
shows the general framework of the code summarization technique
based on the encoder-decoder architecture. From the figure, we can
observe that the DL-based code summarization technique usually
consists of two key components, an encoder and a decoder. Both
encoder and decoder are neural networks, so DL-based code summa-
rization is also known as neural code summarization [62, 84]. The
encoder is an embedding network that can encode the code snippet
ğ‘ given by the developer into a ğ‘‘-dimensional embedding represen-
tation ğ’†ğ‘ âˆˆ Rğ‘‘ . To train such an encoder, existing DL-based code
summarization techniques have tried various neural network archi-
tectures, such as LSTM [30, 34, 71], Bidirectional-LSTM [42, 75, 84],
GRU [31, 40, 72], Transformer [2, 43, 61] and GNN [39, 45]. The
decoder is also a neural network that can decode the embedding
representation ğ’†ğ‘ into a natural language summary. To train such a
decoder, existing DL-based code summarization techniques usually
adopt the same neural network architecture as the encoder. In DL-
based code summarization studies, it is a common practice to use
code comments as summaries during the training process [30, 42].
Code comments are natural language descriptions used to explain
what the code snippets want to do [30]. For example, Figure 1(b) is
a comment for the code snippet ğ‘1. Therefore, we do not strictly dis-
tinguish the meaning of the two terms comment and summary, and
use the term comment during the training process, and summary
at other time.

3 MOTIVATING EXAMPLE

Automatic text summarization is the task of automatically con-
densing a piece of text to a shorter version summary while main-
taining the important points [16, 20]. According to technical char-
acteristics, text summarization is subdivided into extractive text
summarization (extractive methods) and abstractive text summa-
rization (abstractive methods) [48, 63]. Extractive methods assemble
summaries by directly selecting words, phrases, and sentences from
source text. The generated summaries usually persist salient infor-
mation of source text [60, 79, 85]. In contrast, abstractive methods
can generate novel words and phrases not featured from the source
text â€“ as a human-written abstract usually does [60]. Thus, they
have a strong potential of producing high-quality summaries that
are verbally innovative [63].

Automatic text summarization techniques were introduced to au-
tomatic code summarization as early as 10 years ago. For example,
in 2010, Sonia Haiduc et.al [24] proposed an extractive code summa-
rization technique for the automatic generation of extractive sum-
maries for source code entities. Extractive summaries are generated
by selecting the most important terms in code snippets. In addition,
they present a study in [25] to investigate the suitability of various
text summarization techniques for generating code summaries. The
study results indicate that a combination of text summarization
techniques is most appropriate for code summarization and that
developers generally agree with the produced summaries. Recently,
DL-based code summarization techniques have been proposed one
after another [23, 30, 74, 78]. Similar to abstractive text summa-
rization, these techniques also widely adopt the encoder-decoder
models borrowed from neural machine translation (detailed in Sec-
tion 2.2) to generate natural language summaries. Therefore, we can
consider DL-based code summarization techniques as abstractive
methods.

In this paper, we utilize extractive and abstractive methods to-
gether. The former is responsible for extracting important factual
details while the latter is responsible for generating human-written-
like natural language summary, detailed in Section 4.

2.2 Neural Machine Translation

Figure 2: Framework of DL-based Code Summarization

Neural machine translation (NMT) aims to automatically trans-
late one language (e.g., French) into another language (e.g., English)
while preserving semantics [7, 12, 30]. NMT has been shown to
achieve great success for natural language corpora [7, 30]. It is typi-
cally thought of in terms of sequence to sequence (seq2seq) learning,
in which an natural language sentence (e.g., French sentence) is
one sequence and is converted into a semantically equivalent tar-
get sequence (e.g, English sentence) [40]. Code summarization can
also be regarded as a kind of translation task, which translates pro-
gramming language into natural language [23]. Several recent pa-
pers [4, 30, 34, 40] have explored the idea of applying seq2seq model
to translate code snippets into the natural language comments. Sim-
ilar to NMT, code summarization methods based on seq2seq models

Figure 3: Motivation Example
In this section, we take the code snippet ğ‘1 in Figure 1(a) as an
example and apply different tools to generate the summary for
comparison. It is a real-world example from the CodeSearchNet
dataset [33] (see details in Section 5.1.1). Figure 1(b) shows the
comment (ğ‘ 1) written by the developer for ğ‘1. We consider this as a
reference summary (the ground truth), as shown in the first line of
Figure 3(b). According to the grammar rules in natural language, we
can simply divide the reference summary into four parts: â€œremoves
the mappingâ€ (Blue font), â€œfor the specified keyâ€ (Green font), â€œfrom
this cacheâ€ (Red font), and â€œif presentâ€(Orange font).

We study two existing techniques, an extractor method [24] and
an abstracter method [17], in generating summaries for the given
example. The extractor method [24] adopts the Latent Semantic
Analysis (LAS) techniques [67] to determine the informativity of

Code Snippetğ‘’!SummaryEncoderDecoderInput SequenceOutput SequenceSeq2seqModel based on Encoder-Decoder ArchitectureReferenceSummary:removesthemappingforthespecifiedkeyfromthiscacheifpresent.ExtractiveSummary:removedkeycacheoldvalue.AbstractiveSummary:removestheentryfromthecache.ExAbstractiveSummary:removesthevalueforthegivenkeyfromthecacheifitexists.VoldValue=cacheMap.remove(key);if(oldValue!=null){LOG.debug("Removedcacheentryfor'{}'",key);(a) Important Statements Selected byOur Extractor(b) Summaries Generated by Different TechniquesConference 2022, xxx, 2022, xxx

Weisong Sun and Chunrong Fang, et al.

every term in the code snippet and then selects the top 5 important
terms to compose the summary. The second summary in Figure 3(b)
(Extractive Summary) is generated by [24]. Observe that although
the extractive summary has a poor naturalness and is far from
the reference summary, it contains important factual details that
should be included in the summary, e.g., the important terms â€œkeyâ€
and â€œcacheâ€. The abstractive method [17] first trains a model called
CodeBert for obtaining code representations, and then fine-tunes it
on the code summarization task. The third summary in Figure 3(b)
(Abstractive Summary) shows the result by [17]. Observe that 1)
intuitively, the abstractive summary has a good naturalness and
like written by a human; 2) the abstractive summary can cover
the first and the third parts (Blue and Red fonts) of the reference
summary; 3) the abstractive summary can not cover the second
and the fourth parts (Green and Orange fonts), i.e., missing some
factual details.

Our solution. We also use our EACS to generate a summary for
ğ‘1. The last summary (i.e., ExAbstractive Summary) in Figure 3(b)
is generated by EACS. We can observe that 1) compared with the
extractive summary generated by [24], the exabstractive summary
has a good naturalness and like written by a human; 2) compared
with the abstractive summary generated by [17], the exabstractive
summary can cover all of the four parts and is closer to the ref-
erence summary. Based on the above observations, it is obvious
that our method can generate human-written-like summaries while
preserving important factual details. We owe the good performance
of our method on their ability to give more attention to important
statements. Intuitively, the second part (Green font) is a transla-
tion or summary of the factual details (key terms â€œkeyâ€) contained
in the important statements V oldValue = cacheMap.remove(key);
and Log.debug(â€œRemoved cache entry forâ€™â€â€™, key);. The fourth part
(Orange font) is a translation or summary of the factual details con-
tained in the important conditional statement if(oldValue) != null).
The above three important statements are successfully predicted
by our extractive module, as shown Figure 3(b). More details about
the extractive module are introduced in Section 4.2.1).

4 DESIGN
4.1 Overview
Figure 4 illustrates the overview of our approach EACS. The top
part shows the training process of EACS and the bottom part shows
the deployment (usage) of EACS for a given code snippet. EACS
decomposes the training process into two phases: (i) training of
extractor and (ii) training of abstracter. The training of the extrac-
tor aims to produce a well-trained extractor capable of extracting
important statements from a given code snippet. The training of
the abstracter aims to produce a well-trained abstracter capable of
generating a succinct natural language summary for a given code
snippet. Both phases leverage the same two types of input data:
comments and code snippets. To train the extractor, EACS first
produces ground-truth important statements (GIStates) based on
the informativity of each statement in the code snippet, detailed
in Section 4.2.1-Step â€. Then, EACS uses the extractor to extract
predicted important sentences (PIStates), detailed in Section 4.2.1-
Step â. During this procedure, the model parameters of the ex-
tractor is randomly initialized. Based on the loss (Lğ¸ğ‘¥ ) computed

Figure 4: The Overview of EACS

based on PIStates and GIStates, EACS can iteratively update the
model parameters of the extractor, detailed in Section 4.2.1-Step â‚.
To train the abstracter, given a code snippet, EACS first uses the
well-trained extractor to extract the important statements (IStates),
detailed in Section 4.2.2-Step âƒ. These important statements will
be further transformed into the embedding representation ğ’†ğ¸ğ‘¥ by
an encoder named ExEncoder, detailed in Section 4.2.2-Step â„.
Then, EACS uses another encoder named AbEncoder to transform
the entire code snippet into the embedding representation ğ’†ğ´ğ‘ , de-
tailed in Section 4.2.2-Step â…. Further, EACS produces the fused
embedding representation ğ’†ğ¹ğ‘¢ by fusing ğ’†ğ¸ğ‘¥ and ğ’†ğ´ğ‘ , detailed in
Section 4.2.2-Step â†. The fused embedding representation ğ’†ğ¹ğ‘¢ will
be passed to a decoder (Decoder) to generate predicted summaries
(PSummaries). During this procedure, the model parameters of
the abstracter (including ExEncoder, AbEncoder, and Decoder) are
randomly initialized. Finally, based on the loss (Lğ´ğ‘ ) between the
predicted summaries (PSummaries) and ground-truth summaries
(i.e., comments), EACS can iteratively update the model parameters
of the abstracter, detailed in Section4.2.2-Step âˆ. The well-trained
extractor and abstracter are the two core components of EACS to
support the code summarization service. When EACS is deployed
for usage, it takes in a code snippet from the developer and pro-
duces a succinct natural language summary for the code snippet,
detailed in Section 4.3.

4.2 Training of EACS
4.2.1 Part (i): Training of Extractor.
Let ğ‘ denote a code snippet containing a set of statements ğ‘ ğ‘¡ğ‘ğ‘¡ =
[ğ‘ ğ‘¡ğ‘ğ‘¡1, ğ‘ ğ‘¡ğ‘ğ‘¡2, Â· Â· Â· , ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘›], where ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘– is the ğ‘–-th statement in ğ‘. Sim-
ilar to extractive text summarization [46], extractive code summa-
rization can be defined as the task of assigning a label ğ‘™ğ‘– âˆˆ {0, 1} to
each ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘– , indicating whether the factual details contained in the
statement ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘– should be included in the summary. It is assumed
that the summary represents the important content of the code
snippet. Therefore, we can consider the extractor as a classifier

Abstracter(i) Training of ExtractorCode SnippetsCommentsâ‘ GIStatesPIStatesâ‘¡Loss â„’!"â‘¢ExtractorExEncoderğ’†!"AbEncoderâ‘¦ğ’†#$FusionerDecoderğ’†%&PSummariesLoss â„’#$(1) Training of EAtractiveCSA Code Snippet cASummary(2) Deployment of ExAbstractiveCSâ‘£â‘¤Code SnippetsComments(Ground-truth Summaries)Well-trained ExtractorIStatesWell-trained ExtractorWell-trained AbstracterWell-trained Extractorâ‘§InputOutput(ii) Training of AbstracterWell-trained AbstracterWell-trained ExAbstractiveCSâ‘¥â‘¦â‘¨â‘¨c'An Extractive-and-Abstractive Framework for Source Code Summarization

Conference 2022, xxx, 2022, xxx

that takes in all statements of a code snippet and predicts which
statements should be selected as important ones.

As shown in part (i) of Figure 4, the training of the extractor is
completed through three steps: â€ producing ground-truth impor-
tant Statements (GIStates) based on pairs of comments and code
snippets, â producing predicted important statements (PIStates) us-
ing the extractor, and â‚ calculating the loss (Lğ¸ğ‘¥ ) based on GIStates
and PIStates and updating the model parameters of the extractor.
We discuss the three steps in detail in the following sections.

Step â€: Producing Ground-truth Important Statements. As
mentioned earlier, we aim to train an extractor with the ability to
take in a sequence of statements of the code snippet ğ‘ and output
the predicted labels of the statements in ğ‘. We denote the labels of
the statements in ğ‘ as ğ‘™ = [ğ‘™1, ğ‘™2, Â· Â· Â· , ğ‘™ğ‘›], where ğ‘™ğ‘– âˆˆ {0, 1} is the
label of the statement ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘– . ğ‘™ğ‘– = 1 means the ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘– is an informa-
tive (important) statement, otherwise it is not. To train such an
extractor, we need to build a training dataset, where each sample is
a pair of ğ‘ ğ‘¡ğ‘ğ‘¡ and the corresponding labels ^ğ‘™. We consider ^ğ‘™ as the
ground-truth label of ğ‘ ğ‘¡ğ‘ğ‘¡.

To obtain the ground-truth labels, like [29], we first measure
the informativity of each statement ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘– âˆˆ ğ‘ ğ‘¡ğ‘ğ‘¡ by computing the
ROUGE-L recall score [44] between the statement ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘– and the
reference statements. We consider the comments of code snippets
as reference statements. Second, we sort the statements by their
informativity and select the statement in the order of high to low
informativity. We add one statement each time when the new state-
ment can increase the informativity of all the selected statements.
Finally, we obtain the ground-truth labels, i.e., all ground-truth
important statements (GIStates).

Step â: Producing Predicted Important Statements. As shown

in part (i) Figure 4, we use an extractor (Extractor) to predict im-
portant statements in the code snippet. The extractor is a neural
network model that consists of an encoder which transforms state-
ments into embedding representations and a classification layer for
predicting the labels of statements.

The encoder essentially performs the code representation task
of transforming complex source code into a numerical (embed-
ding) representation that is convenient for program computation
while preserving semantics. Therefore, a large number of exist-
ing deep learning-based code representation techniques can be
adopted by our EACS to design the encoder. Specifically, the embed-
ding representations of the statements ğ’†ğ‘ ğ‘¡ğ‘ğ‘¡ can be formalized as
ğ’†ğ‘ ğ‘¡ğ‘ğ‘¡ = ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ (ğ‘ ğ‘¡ğ‘ğ‘¡), where the encoder is a deep learning-based
neural network architecture (e.g., LSTM [28]) or a pre-trained model
(e.g., CodeBert [17]) that can numericalize sequences of code state-
ments with preserving semantics (i.e., embedding representations).
In practice, we tried multiple neural network architectures (e.g.,
LSTM [28] and Transformer [69]) and pre-trained models (e.g.,
CodeBert [17] and CodeT5 [73]), and found that, in the code sum-
marization task, the encoder obtained by fine-tuning a pre-trained
model performed better than that trained from scratch based on
the neural network architecture, detailed in Section 5.2.2. We do
not repeatedly present the design of the neural network architec-
tures or pre-training models involved in the paper. Please read the
corresponding paper for more details. The classification layer is

connected to the embedding representation ğ’†ğ‘ ğ‘¡ğ‘ğ‘¡ . In practice, we
adopt ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ as the classification layer, i.e., ğ‘™ = ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğ’†ğ‘ ğ‘¡ğ‘ğ‘¡ ).
Step â‚: Model Training. During training, we update the model
parameters Î˜ of the extractor regarding the sigmoid cross entropy
loss, i.e., Lğ¸ğ‘¥ (Î˜) computed as:

Lğ¸ğ‘¥ (Î˜) = âˆ’

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘›=1

^ğ‘™ğ‘›ğ‘™ğ‘œğ‘”ğ‘™ğ‘› + (1 âˆ’ ^ğ‘™ğ‘›)ğ‘™ğ‘œğ‘” (1 âˆ’ ğ‘™ğ‘›)

(1)

where ^ğ‘™ğ‘› âˆˆ 0, 1 is the ground-truth label for the ğ‘›th statement and
ğ‘ is the number of statements. When ^ğ‘™ğ‘› = 1, the factual details
contained in the ğ‘›th statement should be attended to facilitate final
summary generation.

4.2.2 Part(ii): Training of Abstracter.
As shown in part (ii) of Figure 4, the training of the abstracter is
completed through five steps: âƒ extracting important statements
(IStates) from the given code snippet using the well-trained ex-
tractor, â„ and â… producing embedding representations (ğ’†ğ¸ğ‘¥ and
ğ’†ğ´ğ‘ ) of the important statements and the entire code snippet, â†
producing the fused representation ğ’†ğ¹ğ‘¢ based on ğ’†ğ¸ğ‘¥ and ğ’†ğ´ğ‘ , â‡
generating predicted summary with the help of the decoder, and â‡
computing the loss Lğ´ğ‘ based on the predicted summaries (PSum-
maries) and the ground-truth summaries (comments) to update
the model parameters of the abstracter. We discuss the six steps in
detail as follows.

Step âƒ: Extracting Important Statements. To generate sum-
maries without missing factual details, our EACS pays more atten-
tion to the important statements of code snippets where the factual
details are contained in. Therefore, different from abstracters in
existing abstract code summarization techniques, the abstracter of
EACS treats important sentences as part of the input. In this step,
we first use the well-trained extractor produced at the phase of
training of the extractor to predict the labels of statements of the
given code snippet. Then, the statements with the label 1 will be
selected as important statements (IStates).

Step â„ and Step â…: Producing Embedding Representations.
Step â„ and Step â… do a similar thing, i.e. leveraging an encoder to
transform the source code into an embedding representation. The
difference is that Step â„ deals with important statements selected
by the extractor and Step â… deals with the entire code snippet.
Therefore, in EACS, we can use the same neural network archi-
tecture or pre-trained model to design ExEncoder and AbEncoder.
Given a code snippet ğ‘ = [ğ‘ ğ‘¡ğ‘ğ‘¡1, ğ‘ ğ‘¡ğ‘ğ‘¡2, Â· Â· Â· , ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘›], let ğ‘ â€² âŠ† ğ‘ de-
note a set of the important statements selected by the extractor
from ğ‘, the tasks performed by ExEncoder and AbEncoder can be
formalized as follows:

ğ’†

ğ’†

ğ´ğ‘ = ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ (ğ‘)

ğ¸ğ‘¥ = ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ (ğ‘â€²),

(2)
where ğ’†ğ¸ğ‘¥ and ğ’†ğ´ğ‘ represent the embedding representations of ğ‘ â€²
and ğ‘; the encoder is a neural network architecture (e.g., LSTM [28],
Transformer [69]) or pre-trained model (e.g., CodeBert [17]) that
can process sequential input. ExEncoder and AbEncoder perform
the similar task â€” producing embedding representations of the code,
so we design them with the same code representation techniques
as the extractorâ€™s encoder.

Step â†: Producing Fused Representation. In this step, EACS
fuses ğ’†ğ¸ğ‘¥ and ğ’†ğ´ğ‘ to produce a fused embedding representation

Conference 2022, xxx, 2022, xxx

Weisong Sun and Chunrong Fang, et al.

ğ‘’ğ¹ğ‘¢ through the component Fusioner. Considering that ğ’†ğ¸ğ‘¥ and
ğ’†ğ´ğ‘ are not aligned, we fuse them in a concatenated fashion. We
try two concatenated ways as follows:

ğ¹ğ‘¢ = [ğ’†

ğ¸ğ‘¥ ; ğ’†

ğ´ğ‘ ] or [ğ’†

ğ´ğ‘ ; ğ’†

ğ¸ğ‘¥ ]

ğ’†

(3)

where [Â·; Â·] denotes the concatenation of two vectors. The effects
of both ways on the performance of EACS are discussed in Sec-
tion 5.2.3.

Step â‡: Generating Predicted Summaries. In this section, we
utilize the decoder to generate natural language summary, which
takes in the fused embedding representation ğ’†ğ¹ğ‘¢ and predicts word
one by one. Specifically, the decoder based on a neural network (e.g.,
LSTM) is to unfold the context vector ğ’†ğ¹ğ‘¢ into the target sequence
(i.e., the word sequence of the summary), through the following
dynamic model,

ğ’‰ğ‘¡ = ğ‘“ (ğ‘¦ğ‘¡ âˆ’1, ğ’‰ğ‘¡ âˆ’1, ğ’†
ğ‘ (ğ‘¦ğ‘¡ |ğ‘Œ<ğ‘¡ , ğ‘‹ ) = ğ‘” (ğ‘¦ğ‘¡ âˆ’1, ğ’‰ğ‘¡ , ğ’†

ğ¹ğ‘¢ )
ğ¹ğ‘¢ )

(4)

where ğ‘“ (Â·) and ğ‘”(Â·) are activation functions, ğ’‰ğ‘¡ is the hidden state
of the neural network at time ğ‘¡, ğ‘¦ğ‘¡ is the predicted target word at
ğ‘¡ (through ğ‘”(Â·) with ğ‘Œ<ğ‘¡ denoting the history {ğ‘¦1, ğ‘¦2, Â· Â· Â· , ğ‘¦ğ‘¡ âˆ’1}.
The prediction process is typically a classifier over the vocabulary.
It can be seen from Equation 4 that the probability of generating
target word is related to the current hidden state, the history of the
target sequence and the context ğ’†ğ¹ğ‘¢ . The essence of the decoder is
to classify the vocabularies by optimizing the loss function in order
to generate the vector representing the feature of the target word
ğ‘¦ğ‘¡ . After the vector passes through a softmax function, the word
corresponding to the highest probability is the result to be output.
Step âˆ: Model Training. During training of the abstracter, the
three components (ExEncoder, AbEncoder, and Decoder) are jointly
trained to minimize the negative conditional log-likelihood, i.e,
Lğ´ğ‘ (Î˜) computed as:

RQ3: How does the fusion way of the extractor and abstracter

affect the performance of EACS?

RQ4: How does the robustness of EACS perform when varying

the code length and comment length?

5.1 Experimental Setup
5.1.1 Dataset. We conduct experiments on three datasets, includ-
ing a Java dataset (JCSD) [32], a Python dataset (PCSD) [10], and
a CodeSearchNet corpus [33], which have been widely used by
existing code summarization studies [2, 17, 22, 74, 78, 84]. The
CodeSearchNet corpus contains codes across six programming lan-
guages (Go, Java, JavaScript, PHP, Python, and Ruby).

Note that existing studies [17, 73] evaluate the effectiveness
of pre-trained models for code representation (e.g., CodeBert [17]
and CodeT5 [73]) through the following two steps: 1) fine-tuning
them on the training set of downstream tasks (e.g. code summa-
rization task) to produce task-specific models; 2) evaluating the
effectiveness of the task-specific models on the corresponding test
set, which indirectly indicates the effectiveness of the pre-trained
models. Therefore, to verify the effectiveness of the pre-trained
model, code representation techniques (e.g., CodeBert) divide the
CodeSearchNet corpus into two parts before training the model.
One part is used for training the pre-trained model, and the other
part is used as downstream task data to evaluate the effectiveness of
the pre-trained model. As mentioned earlier, we can use pre-trained
models as the encoders of EACS. Therefore, the encoders have seen
(learned) the part of the CodeSearchNet corpus used for training
the pre-trained models. For a fair comparison, in this paper, we only
use the unseen data used as downstream code summarization task
data. In practice, we use the cleaned dataset for code summarization
task published by CodeBert [17]. The statistics of the three datasets
are shown in Table 1.

Lğ´ğ‘ (Î˜) = âˆ’

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘™ğ‘œğ‘”ğ‘ (ğ’šğ‘› |ğ’™ğ‘›; Î˜)

(5)

Table 1: Statistics of datasets

ğ‘›=1
where Î˜ is the model parameters of the abstracter and each (ğ’™ğ‘›, ğ’šğ‘›)
is an (code snippet, comment) pair from the training set.

4.3 Deployment of EACS
After EACS is trained, we can deploy it online for code summariza-
tion service. Part (2) of Figure 4 shows the deployment of EACS.
For a code snippet ğ‘ given by the developer, EACS first uses the
well-trained extractor to extract important statements from ğ‘, repre-
sented ğ‘ â€². Then, EACS uses the well-trained abstracter to generate
the summary. In practice, we can consider the well-trained EACS as
a black-box tool that takes in a code snippet given by the developer
and generates a succinct natural language summary.

5 EVALUATION
To evaluate our approach, in this section, we aim to answer the
following four research questions:

RQ1: How does EACS perform compared to the state-of-the-

art baselines?

RQ2: How does EACS perform when combined with different
neural network architectures and pre-trained models?

Dataset
JCSD
PCSD

Go
Java
JavaScript
PHP
Python
Ruby

Training Set Size
69,708
57,203

Validation Set Size
8,714
19,067

Test Set Size
8,714
19,066

167,288
164,923
58,025
241,241
251,820
24,927

7,325
5,183
3,885
12,982
13,914
1,400

8,122
10,955
3,291
14,014
14,918
1,262

5.1.2 Evaluation Metrics. We use three metrics BLEU [54], ME-
TEOR [8], and ROUGE [44], to evaluate the model, which are widely
used in code summarization [22, 32, 34, 69, 70, 78].

BLEU, the abbreviation for BiLingual Evaluation Understudy [54],
is widely used for evaluating the quality of generated code sum-
maries [32, 34, 70]. It is a variant of precision metric, which calcu-
lates the similarity by computing the n-gram precision of a gen-
erated summary to the reference summary, with a penalty for the
overly short length [54]. It is computed as:

ğµğ¿ğ¸ğ‘ˆ = ğµğ‘ƒ âˆ— ğ‘’ğ‘¥ğ‘ (

ğ‘
âˆ‘ï¸

ğ‘›=1

ğ‘¤ğ‘›ğ‘™ğ‘œğ‘”ğ‘ğ‘›)

(6)

An Extractive-and-Abstractive Framework for Source Code Summarization

Conference 2022, xxx, 2022, xxx

ğµğ‘ƒ =

1,
ğ‘’ (1âˆ’ |ğ‘Ÿ |
|ğ‘”| ) ,

if |ğ‘” | > |ğ‘Ÿ |

if |ğ‘” | â‰¤ |ğ‘Ÿ |

(7)

ï£±ï£´ï£´ï£²
ï£´ï£´
ï£³

where ğ‘ = 1, 2, 3, 4 and ğ‘¤ğ‘› = 1
ğ‘ . ğµğ‘ƒ represents the brevity penalty.
ğ‘” and ğ‘Ÿ denote a generated (predicted) summary and a reference
summary, respectively. |ğ‘”| and |ğ‘Ÿ | denote the lengths of ğ‘” and ğ‘Ÿ ,
respectively. BLEU-4 is often of interest as it can reflect the weighted
results from 1 through 4 [11].

METEOR, the abbreviation for Metric for Evaluation of Trans-
lation with Explicit ORdering [8], is also widely used to evaluate
the quality of generated code summaries [71, 81, 84]. For a pair
of summaries, METEOR creates a word alignment between them
and calculates the similarity scores. Suppose ğ‘š is the number of
mapped unigrams between the reference summary ğ‘Ÿ and the gener-
ated summary ğ‘”, respectively. Then, precision (ğ‘ƒğ‘¢ğ‘›ğ‘–ğ‘”), recall (ğ‘ƒğ‘¢ğ‘›ğ‘–ğ‘”),
and METEOR are computed as:

ğ‘ƒğ‘¢ğ‘›ğ‘–ğ‘” =

ğ‘š
|ğ‘” |

, ğ‘…ğ‘¢ğ‘›ğ‘–ğ‘” =

ğ‘š
|ğ‘Ÿ |

ğ‘€ğ¸ğ‘‡ ğ¸ğ‘‚ğ‘… = (1 âˆ’ ğ›¾ âˆ— ğ‘“ ğ‘Ÿğ‘ğ‘”ğ›½ ) âˆ—

ğ‘ƒğ‘¢ğ‘›ğ‘–ğ‘” âˆ— ğ‘…ğ‘¢ğ‘›ğ‘–ğ‘”
ğ›¼ âˆ— ğ‘ƒğ‘¢ğ‘›ğ‘–ğ‘” + (1 âˆ’ ğ›¼) âˆ— ğ‘…ğ‘¢ğ‘›ğ‘–ğ‘”

(8)

(9)

where ğ‘“ ğ‘Ÿğ‘ğ‘” is the fragmentation fraction. As in [84]; ğ›¼, ğ›½, and ğ›¾
are three penalty parameters whose default values are 0.9, 3.0 and
0.5, respectively.

ROUGE-L. ROUGE is the abbreviation for Recall-oriented Un-
derstudy for Gisting Evaluation [44]. ROUGE-L, a variant of ROUGE,
is computed based on the longest common subsequence (LCS).
ROUGE-L is also widely used to evaluate the quality of generated
code summaries [9, 43, 61]. Specifically, the LCS-based F-measure
(ğ¹ğ‘™ğ‘ğ‘  ) is called ROUGE-L [44], and ğ¹ğ‘™ğ‘ğ‘  is computed as:
ğ¿ğ¶ğ‘† (ğ‘Ÿ, ğ‘”)
|ğ‘Ÿ |

ğ¿ğ¶ğ‘† (ğ‘Ÿ, ğ‘”)
|ğ‘”|

, ğ‘ƒğ‘™ğ‘ğ‘  =

ğ‘…ğ‘™ğ‘ğ‘  =

(10)

ğ¹ğ‘™ğ‘ğ‘  =

(1 + ğ›½ 2)ğ‘…ğ‘™ğ‘ğ‘  ğ‘ƒğ‘™ğ‘ğ‘ 
ğ‘…ğ‘™ğ‘ğ‘  + ğ›½ 2ğ‘ƒğ‘™ğ‘ğ‘ 

(11)

where ğ‘Ÿ and ğ‘” also denote the reference summary and the generated
summary, respectively. Notice that ROUGE-L is 1 when ğ‘” = ğ‘Ÿ ; while
ROUGE-L is 0 when ğ¿ğ¶ğ‘† (ğ‘Ÿ, ğ‘”) = 0, i.e., which means ğ‘Ÿ and ğ‘” are
completely different. ğ›½ is set to 1.2 as in [70, 72, 84].

The scores of BLEU, ROUGE-L and METEOR are in the range of
[0,1] and usually reported in percentages. The higher the scores,
the closer the generated summary is to the reference summary, and
the better the code summarization performance.

5.1.3 Experimental Settings. To train models, we first shuffle the
training data and set the mini-batch size to 32. For each batch,
the code snippets are padded with a special token âŸ¨ğ‘ƒğ´ğ·âŸ© to the
maximum length. We set the word embedding size to 512. For LSTM
unit, we set the hidden size to 512. The margin ğ›½ is set to 0.6. We
update the parameters via AdamW optimizer [37] with the learning
rate 0.0003. To prevent over-fitting, we use dropout with 0.1. All
models are implemented using the PyTorch 1.7.1 framework with
Python 3.8. All experiments are conducted on a server equipped
with one Nvidia Tesla V100 GPU with 31 GB memory, running on
Centos 7.7. All the models in this paper are trained for the same

epochs as their original paper, and we select the best model based
on the lowest validation loss.

5.2 Experimental Results
5.2.1 RQ1: EACS vs. Baselines.

1) Baselines: To answer this research question, we compare our
approach EACS to the following DL-based code summarization
techniques.

CODE-NN [34] adopts a LSTM-based encoder-decoder archi-
tecture with attention mechanism. It is a classical encoder-decoder
framework in NMT that encodes tokens of code snippets into em-
bedding representations and then generates summaries in the de-
coder with the attention mechanism.

DeepCom [30] also adopts a LSTM-based encoder-decoder ar-
chitecture with attention mechanism. In addition, to capture the
structural information, DeepCom proposes a structure-based tra-
versal method to traverse AST sequences of the code snippet. The
AST sequences are further passed to the encoder and decoder to
generate summaries.

Hybrid-DRL [70] (also shortened to RL+Hybrid2Seq in [2],
Hybrid2Seq in [78]) also adopts a LSTM-based encoder-decoder
architecture and is trained with reinforcement learning. It also
designs additional encoder based on an AST-based LSTM to capture
the structural information of the code snippet. It uses reinforcement
learning to solve the exposure bias problem during decoding, which
obtains better performance.

TL-CodeSum [32] (also shortened to API+Code in [78])
adopts a GRU-based encoder-decoder architecture with attention
mechanism. It encodes Application Programming Interface (API)
sequence along with code token sequence, then generates summary
from source code with transferred API knowledge. It introduces
API sequence summarization task, aiming to train an API sequence
encoder by using an external dataset so that it can learn more
abundant representations of the code snippet.

Dual Model [74] also adopts a LSTM-based encoder-decoder
architecture with attention mechanism. It treats code summariza-
tion and code generation as a dual task. It trains the two tasks
jointly by a dual training framework to simultaneously improve
the performance of code summarization and code generation tasks.

Transformer-based [2] (also shortened to Transformer in [78],

NCS in [62]) adopts a Transformer-based encoder-decoder archi-
tecture. It incorporates the copying mechanism [60] in the Trans-
former to allow both generating words from vocabulary and copy-
ing from the source code.

SiT [78] adopts a Transformer-based encoder-decoder architec-
ture. It proposes structure-induced transformer to capture long-
range dependencies and more global information in AST sequences
of code snippets.

2) Results: Table 2 shows the performances of our EACS and
baselines in terms of the three evaluation metrics, i.e., BLEU (B),
METEOR (M), and ROUGE-L (R).

From Table 2, in all baselines, SiT performs the best on both
datasets in terms of all three metrics. However, our EACS is more
powerful than SiT and achieves more impressive performance. On
the JCSD dataset, compared with the-state-of-the art (SiT), EACS im-
proves by 5.4% in BLEU-4, 12.14% in METEOR and 6% in ROUGE-L.

Conference 2022, xxx, 2022, xxx

Weisong Sun and Chunrong Fang, et al.

Table 2: Performance of Our EACS and Baselines. âˆ— refers to
methods we rerun. The results of upper part (lines 3â€“9) are
directly reported from [2, 78]. Note that we only rerun SiT
since it is much stronger than the other baselines.

Techniques

CODE-NN
DeepCom
Hybrid-DRL
TL-CodeSum
Dual Model
Transformer-based
SiT
SiTâˆ—
EACS

B
27.60
39.75
38.22
41.31
42.39
44.58
45.76
45.22
47.66

JCSD
M
12.61
23.06
22.75
23.73
25.77
26.43
27.58
27.10
30.39

R
41.10
52.67
51.91
52.25
53.61
54.76
55.58
55.44
58.77

B
17.36
20.78
19.28
15.36
21.80
32.52
34.11
33.75
35.96

PCSD
M
9.29
9.98
9.75
8.57
11.14
19.77
21.11
21.02
23.70

R
37.81
37.35
39.34
33.65
39.45
46.73
48.35
48.33
51.83

(a) Distribution of BLEU Scores

(b) Distribution of METEOR Scores

(c) Distribution of ROUGE-L Scores

Figure 5: Distributions of Metricsâ€™ Scores

On the PCSD dataset, EACS also clearly outperforms SiT, improving
by 6.55% in BLEU-4, 12.75% in METEOR and 7.24% in ROUGE-L. It
should be noted that the values in Table 2 are the average scores of
all test samples. For a more comprehensive comparison, we further
statistic the distribution of the scores of SiT and EACS on all test
samples, and the statistical results are shown in Figure 5. In Figure 5,
â€˜+â€™ denotes the mean, which is the value filled in Table 2. Overall, the
score distribution of EACS is better than that of SiT. To test whether
there is a statistically significant difference between EACS and
SiT, we perform the unpaired two-tailed Wilcoxon-Mann-Whitney
test, at a significance level of 5%, following previously reported
guidelines for inferential statistical analysis involving randomized
algorithms [6, 21]. In Figure 5, â€˜*â€™ (0.01 < ğ‘ < 0.05) and â€˜****â€™
(ğ‘ < 0.0001) represent the differences between two groups are
Significant and Extremely significant, respectively. â€˜nsâ€™ (ğ‘ â‰¥ 0.05)
means Not significant. From the figure, we can intuitively observe
that in all three metrics, EACS outperforms SiT on both the JCSD
and PCSD datasets. In summary, the results and observations on

the above demonstrate that under all experimental settings, our
EACS consistently achieves higher performance in all three metrics,
which indicates better code summarization performance.

5.2.2 RQ2: Effectiveness of EACS When Combing with Different
Networks Architectures or Pre-trained Models.
The extractive-and-abstractive framework we proposed is a gen-
eral code summarization framework that does not depend on a
specific deep learning network/model. In other words, EACS can
be combined with many advanced neural network architectures
or pre-trained models. In this section, we combine EACS with sev-
eral popular and advanced neural network architectures (including
LSTM [28] and Transformer [69]) and pre-trained models (including
CodeBert [17] and CodeT5 [73]) to explore the potential of EACS
in multilingual code summarization tasks. These neural network
architectures and pre-trained models are widely used in code sum-
marization [2, 17, 30, 64, 78]. The experimental results are shown
in Table 3. In the table, rows 3 (â€œLSTMâ€) and 5 (â€œTransformerâ€)
represent that we implement purely LSTM-based and Transformer-
based encoder-decoder frameworks, respectively, to perform the
code summarization task. Rows 4 (â€œEACS + LSTMâ€) and 6 (â€œEACS
+ Transformerâ€) represent that on the basis of the above (Rows
3 and 5), the well-trained extractor module is added. Rows 7 and
9 represent that we fine-tune the pre-trained models released on
the GitHub (i.e., CodeBert [50] and CodeT5 [58]) on the code sum-
marization task. Analogously, rows 8 (â€œEACS + CodeBertâ€) and 10
(â€œEACS + CodeT5â€) represent that on the basis of the above (Rows
7 and 9), the well-trained extractor module is added.

From rows 3-6, we can observe that the performance of the
combination of EACS and architecture is significant better than
that of only architecture-based methods in terms of all three met-
rics. Among them, the combination of EACS and Transformer (i.e.,
â€œEACS + Transformerâ€) performs the best. In addition, from rows
7-10, we can observe that the performance of the combination
of EACS and pre-trained models is significant better than that of
purely pre-trained model-based methods in terms of all three met-
rics. Among them, the â€œEACS + CodeT5â€ performs the best. We can
also observe that the combinations of EACS with pre-trained mod-
els significantly outperforms the combination with neural network
architectures. This is mainly because pre-trained models for code
representation are usually trained on larger-scale datasets, so they
have stronger code representation capabilities and can represent
code semantics more accurately. Based on the above results and
observations, we can make a conclusion that our EACS is a general
code summarization framework and can be easily combined with
advanced neural network architectures or pre-trained models. We
have reasons to believe that EACS can be combined with more
advanced neural network architectures or models to exert more
powerful performance in the future.

5.2.3 RQ3: Influence of Fusion Ways of Extractor and Abstracter
on EACS.
In this section, we further explore the influence of fusion ways
between the extractor and abstracter on the performance of EACS.
In practice, as mentioned earlier, we try to fuse the outputs of the
ExEncoder and AbEncoder by the following two concatenated ways:
[ğ’†ğ¸ğ‘¥ ; ğ’†ğ´ğ‘ ] and [ğ’†ğ´ğ‘ ; ğ’†ğ¸ğ‘¥ ]. The experimental results are shown in

0.20.40.60.81.0SiTEACSSiTEACSBLEU****PCSDJCSDSiTEACS****0.20.40.60.81.0SiTEACSSiTEACSMETEOR********PCSDJCSDSiTEACS0.20.40.60.81.0SiTEACSSiTEACSROUGE-LSiTEACS********PCSDJCSDAn Extractive-and-Abstractive Framework for Source Code Summarization

Conference 2022, xxx, 2022, xxx

Table 3: Effectiveness of EACS when Combined with Different Neural Network Architectures (LSTM and Transformer) and
Pre-trained Models (CodeBert and CodeT5) on the CodeSearchNet corpus (Six Programming Languages)

Method

LSTM
EACS + LSTM
Transformer
EACS + Transformer
CodeBert
EACS + CodeBert
CodeT5
EACS + CodeT5

Go
M
15.1
15.2
16.2
16.8
17.5
18.7
18.9
20.0

B
17.8
17.9
19.8
20.1
21.1
22.2
21.5
23.9

R
35.6
35.8
38.4
39.2
43.6
44.8
45.9
47.3

B
12.2
13.4
15.3
15.8
18.0
19.3
20.2
21.3

Java
M
10.1
10.2
11.8
12.3
12.4
13.8
15.3
16.8

R
24.6
24.7
30.6
31.2
35.5
36.8
39.3
41.1

JavaScript
M
6.2
6.4
7.4
7.6
8.7
9.6
11.2
12.1

R
17.2
17.4
20.5
21.3
24.3
25.1
28.9
30.3

B
10.4
10.5
11.2
11.5
13.3
14.3
15.8
16.7

PHP
M
12.2
12.4
13.9
14.1
15.3
16.1
18.2
18.8

R
29.8
30.0
34.2
34.8
39.4
40.2
43.6
44.2

Python
M
9.1
9.2
10.6
10.9
12.4
13.2
15.1
16.0

R
23.3
23.4
31.3
32.1
34.8
35.8
37.8
38.9

B
13.9
14.1
15.8
16.3
18.7
19.4
20.0
20.5

Ruby
M
5.3
5.3
6.4
6.7
7.1
8.2
10.8
11.8

R
16.3
16.5
18.3
18.8
20.6
22.6
27.9
29.2

B
9.4
9.9
10.3
10.7
11.2
12.4
14.9
15.2

B
19.5
19.6
21.5
22.3
24.6
25.4
25.9
26.9

Table 4. In the table, rows 3 and 4 ([ğ’†ğ¸ğ‘¥ /ğ’†ğ´ğ‘ ; ğ’†ğ´ğ‘ /ğ’†ğ¸ğ‘¥ ]) means that
we use the way of concatenation to fuse the embeddings generated
by ExEncoder/AbEncoder and AbEncoder/ExEncoder.

Table 4: Influence of Fusion Ways on EACS

JCSD
M
32.00
30.39

Techniques

B
35.56
35.96

B
47.29
47.66

[ğ’†ğ¸ğ‘¥ ; ğ’†ğ´ğ‘ ]
[ğ’†ğ´ğ‘ ; ğ’†ğ¸ğ‘¥ ]

R
58.14
58.77
From Table 4, we can observe that in general, the fusion way
[ğ’†ğ´ğ‘ ; ğ’†ğ¸ğ‘¥ ] is better than [ğ’†ğ¸ğ‘¥ ; ğ’†ğ´ğ‘ ]. This is in line with human
intuition. When summering a code snippet, human should first
browse the whole code snippet and then write a summary according
to the key content.

R
51.69
51.83

PCSD
M
23.53
23.70

5.2.4 RQ4: Robustness of EACS.

(a) Code Snippets on Testing Set

(b) Comments on Testing Set

Figure 6: Length Distributions

(a) Varying Code Snippet Lengths

(b) Varying Comment Lengths

Figure 7: Robustness of EACS

To analyze the robustness of EACS, we study two parameters
(i.e., code length and comment length) that may have an influence
on the embedding representations of code snippets and comments.
Figure 6(a) and (b) show the length distributions of code snippets
and comments on the testing set. For a code snippet, its length

refers to the number of lines of the code snippet. For a comment,
its length refers to the number of words in the comment. From
Figure 6(a), we can observe that the lines of most code snippets are
located between 20 to 40. This was also observed in the quote in [47]
â€œFunctions should hardly ever be 20 lines longâ€. From Figure 6(b), it
is noticed that almost all comments are less than 20 in length. This
also confirms the challenge of capturing the correlation between
code snippet with its corresponding comment (summary).

Figure 7 shows the performance of EACS based on different eval-
uation metrics with varying parameters. From Figure 7, we can
observe that in most cases, EACS maintains a stable performance
even though the code snippet length or comment length increases,
which can be attributed to the extractive-and-abstractive frame-
work we proposed. When the length of the comment exceeds 25 (a
common range shown in 6(b)), the METEOR and ROUGE-L scores
of EACS decreases as the length increases. It means that when the
length of the comments exceeds the common range, as the expected
length of the generated comment continues to increase, it will be
more difficult to generate. Overall, the results verify the robustness
of our EACS.

5.3 Case Study
In this section, we provide case studies to understand the generated
summaries of EACS compared to the state-of-the-art SiT, and to
demonstrate the usefulness of our EACS.

5.3.1 Case Study on Java Code Summarization.
In this section, we take the code snippet ğ‘2 in Figure 8(a) as an
example and apply SiT and our EACS to generate summaries for
comparison. It is a real-world example from the test set of the
JCSD dataset. As in Section 3, we consider the comment of ğ‘2 as
the reference summary, as shown in the first line of Figure 8(c).
The summaries generated by SiT and EACS for ğ‘2 are shown in
the second and third lines, respectively. From the figure, we can
observe that compared to the reference summary, the summary
generated by SiT only covers a part of the second part (Red font).
What is even worse is that the first core word â€œremoveâ€ generated
by SiT and that of â€œaddsâ€ in the reference summary have completely
opposite semantics, which makes the semantics of the generated
summary and the reference summary completely opposite. Com-
pared with the summary generated by SiT, the summary generated
by EACS covers all three parts of the reference summary. In addi-
tion, EACS successfully generates the core word â€œaddâ€, which we
attribute to the extractive-and-abstractive framework we proposed.

0204060801001201401600300600900120015001800CountLength0102030405060708002004006008001000CountLength051015202530350.00.20.40.60.81.0ScoreBLEUMETEORROUGE-LLength05101520253035404550550.00.20.40.60.81.0ScoreBLEUMETEORROUGE-LLengthConference 2022, xxx, 2022, xxx

Weisong Sun and Chunrong Fang, et al.

This is demonstrated by the important statement shown in Fig-
ure 8(b) selected by the extractor module in our framework. Only
the important statement contains the core word â€œaddâ€.

Figure 8: Java Case

5.3.2 Case Study on Python Code Summarization.
Figure 9(a) shows a code snippet ğ‘3 from the test set of the PCSD

Figure 9: Python Case

dataset. ğ‘3 is a long piece that consists of 51 lines of code. Figure 9(b)
presents the important statements selected by our extractor from ğ‘3.
Figure 9(c) contains three summaries, the first from the comment of
ğ‘3, the second and last generated by SiT and our EACS, respectively.
From the figure, we can observe that compared to the reference
summary, 1) the summary generated by SiT misses many factual
details, such as â€œfrom startobj to endobjâ€; 2) although it looks a
little different literally, the summary generated by our EACS is
able to cover all of the three parts and are semantically equivalent.
In summary, our EACS significantly outperforms SiT in terms of
naturalness and semantic completeness of the generated summaries.
Based on the above, we can conclude that our EACS is also a very
competitive technique on the Python code summarization task.

6 RELATED WORK
Code summarization has always been one of the hottest research
topics in software engineering. As mentioned earlier, we can cate-
gorize existing work related to code summarization into extractive
methods, abstractive methods, and others.

Extractive methods. Most early (prior to 2016) code summa-
rization techniques [24, 25, 53, 65] are extractive methods. Such
methods work by extracting a subset of the statements and key-
words from the code, and then including information from those
statements and keywords in summary. For example, Sonia Haiduc
et.al [24] first propose an extractive method to generate extractive
summaries for code snippets automatically. Extractive summaries
are obtained from the contents of a document by selecting the most
important information in that document. Extractive methods use
text retrieval (TR) techniques (e.g., Vector Space Model [59], Latent
Semantic Indexing [14], and Hierarchical PAM [51]) to determine
the most important ğ‘› terms for each code snippet. Considering
that the quality of the summaries generated by extractive meth-
ods depends heavily on the process of extracting the subset, Paige
Rodeghero et.al [56] present an eye-tracking study of programmers
and propose a tool for selecting keywords based on the findings of
the eye-tracking study. The extractive methods rely on high-quality
identifier names and method signatures from the source code. These
techniques may fail to generate accurate comments if the source
code contains poorly named identifiers or method names [76].

Abstractive methods. Nowadays, DL-based (abstractive) code
summarization techniques have been proposed one after another.
By combining Seq2Seq models trained on large-scale code-comment
datasets, abstractive methods can generate words that do not appear
in the given code snippet to overcome the limitations of extractive
methods. For example, Srinivasan Iyer et.al [34] present the first
completely abstractive method for generating short, high-level sum-
maries of code snippets. Their experimental results demonstrate
that abstractive methods significantly outperform extractive meth-
ods in the naturalness of generated summaries. In other words, the
summaries generated by abstractive methods are human-written-
like. Code representation plays a key role in abstractive methods.
To produce semantic-preserving code embedding representations,
multiple aspects of the code snippet have been explored, including
tokens [2, 18, 31, 32, 34, 39, 40, 43, 52, 55, 70, 71, 75, 84], abstract
syntactic trees (ASTs) [4, 30, 31, 39, 40, 43, 64, 70, 71, 75, 81, 84],
control flows [71], code property graph [45]. In addition, existing
abstractive methods have tried various neural network architec-
tures, such as LSTM [30, 34, 71], Bidirectional-LSTM [42, 75, 84],
GRU [31, 40], Transformer [2, 43] and GNN [39, 45]. Although deep
learning-based abstractive methods show great potential for gener-
ating human-written-like summaries, we find that the generated
summaries often miss important factual details.

Others. Paul W. McBurney [49] takes the context of code snip-
pets into account when generating summaries. The context includes
the dependencies of the method and any other methods which rely
on the output of the method [38]. The works [9, 61, 72, 83] also
use the information beyond the code snippet itself, e.g., project or
class context [9, 72, 83], application programming interface docu-
mentations/knowledge (API Docs) [61]. These techniques cannot
be tested on existing commonly used datasets (e.g., JCSD, PCSD,

publicsynchronizedvoidlisten(finalSet<NotificationChannel>channelNames){m_channels.addAll(channelNames);}ReferenceSummary:addsthesetofchannelnamestothesetoflistenedchannels.SiT:removethechannelnamesfromthechannelnames.EACS:addasetofchannelnamestothesetoflistenedchannels.m_channels.addAll(channelNames);(b) Important Statements Selected by Our Extractor(a) A Code Snippet ğ‘!from the Test Set of the JCSD Database(c) Summaries Generated by Different TechniquesdeffindRefPath(startObj,endObj,maxLen=8,restart=True,seen={},path=None,ignore=None):refs=[]if(pathisNone):path=[endObj]â€¦newRefs=[rforringc.get_referrers(endObj)if(id(r)notinignore)]â€¦forrinnewRefs:â€¦refs.append((p+[r]))returnrefsReferenceSummary:determineallpathsofobjectreferencesfromstartobjtoendobj.SiT:searchforapath.EACS:findreferencepathsbetweenstartandendobjects.deffindRefPath(startObj,endObj,maxLen=8,restart=True,seen={},path=None,ignore=None):(b) Important Statements Selected by Our Extractor(a) A Code Snippet ğ‘!from the Test Set of the PCSD Database(c) Summaries Generated by Different TechniquesAn Extractive-and-Abstractive Framework for Source Code Summarization

Conference 2022, xxx, 2022, xxx

and CodeSearchNet Corpus) because methods (code snippets) in
these datasets are context-missing. There are techniques using
summaries of similar code snippets as the summary of the cur-
rent snippet or as a starting point to generate a new summary.
For example, Edmund Wong et.al [76] apply code clone detection
techniques to discover similar code snippets and use the comments
from some code snippets to describe the other similar code snippets.
The works [42, 75, 84] retrieve similar code snippets and then use
the information contained in the similar code snippets or their sum-
maries to generate a summary of the current code snippet. They
rely on whether similar code snippets can be retrieved and how
similar they are. All the works mentioned above emphasize using
external knowledge sources (e.g., contexts, API Docs, and similar
code snippets) to improve the quality of the generated comments.
We will explore the effect of combining our EACS with the above
external knowledge in future work.

Our method. EACS is a general code summarization framework
that adopts extractive and abstract schemes at the same time. Dif-
ferent from existing extractive methods that use TR techniques
to extract important statements/words, the extractive module of
EACS applies a deep learning-based classifier to predict the im-
portance of each statement. Unlike existing abstractive methods,
our abstracter receives and processes the entire code snippet and
important statements as input in parallel.

7 CONCLUSION
In this paper, we propose an extractive-and-abstractive framework
named EACS for source code summarization. EACS consists of two
modules, extractor and abstracter. The extractor has the ability to
extract important statements from the code snippet. The important
statements contain important factual details that should be included
in final generated summary. The abstracter takes in the important
statements extracted by the extractor and the entire code snippet
and is able to generate human-written-like summary in natural lan-
guage. We conduct comprehensive experiments on three databases
to evaluate the performance of EACS. And the experimental results
demonstrate that our EACS is an effective code summarization
technique and significantly outperforms the state-of-the-art.

REFERENCES
[1] Nahla J. Abid, Jonathan I. Maletic, and Bonita Sharif. 2019. Using developer eye
movements to externalize the mental model used in code summarization tasks. In
Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications.
ACM, Denver , CO, USA, 13:1â€“13:9.

[2] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020.
A Transformer-based Approach for Source Code Summarization. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics. Asso-
ciation for Computational Linguistics, Online, 4998â€“5007.

[3] Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles Sutton.
2018. A Survey of Machine Learning for Big Code and Naturalness. Comput.
Surveys 51, 4 (2018), 81:1â€“81:37.

[4] Uri Alon, Omer Levy, and Eran Yahav. 2018. Code2seq: Generating Sequences

from Structured Representations of Code. CoRR abs/1808.01400 (2018).

[5] Anonymous. 2022. EACS. site: https://anonymous.4open.science/r/EACS. Ac-

cessed March, 2022.

[6] Andrea Arcuri and Lionel Briand. 2014. A Hitchhikerâ€™s Guide to Statistical Tests
for Assessing Randomized Algorithms in Software Engineering. Software Testing,
Verification and Reliability 24, 3 (2014), 219â€“250.

[7] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Translate. In Proceedings of the 3rd
International Conference on Learning Representations (ICLR 2015). OpenReview.net,
San Diego, CA, USA, 1â€“15.

[8] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for
MT Evaluation with Improved Correlation with Human Judgments. In Proceed-
ings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization. Association for Computational Linguistics,
Ann Arbor, Michigan, USA, 65â€“72.

[9] Aakash Bansal, Sakib Haque, and Collin McMillan. 2021. Project-Level Encoding
for Neural Source Code Summarization of Subroutines. In Proceedings of the
29th International Conference on Program Comprehension. IEEE, Madrid, Spain,
253â€“264.

[10] Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A Parallel Corpus of
Python Functions and Documentation Strings for Automated Code Documenta-
tion and Code Generation. In Proceedings of the 8th International Joint Conference
on Natural Language Processing. Asian Federation of Natural Language Processing,
Taipei, Taiwan, 314â€“319.

[11] Qiuyuan Chen, Xin Xia, Han Hu, David Lo, and Shanping Li. 2021. Why My
Code Summarization Model Does Not Work: Code Comment Improvement with
Category Prediction. ACM Transactions on Software Engineering and Methodology
30, 2 (2021), 25:1â€“25:29.

[12] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio.
2014. On the Properties of Neural Machine Translation: Encoder-Decoder Ap-
proaches. In Proceedings of the 8th Workshop on Syntax, Semantics and Structure
in Statistical Translation. Association for Computational Linguistics, Doha, Qatar,
103â€“111.

[13] Sergio Cozzetti B. de Souza, Nicolas Anquetil, and KÃ¡thia MarÃ§al de Oliveira.
2005. A study of the documentation essential to software maintenance. In Pro-
ceedings of the 23rd Annual International Conference on Design of Communication:
documenting & Designing for Pervasive Information. ACM, Coventry, UK, 68â€“75.
[14] Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas,
and Richard A. Harshman. Journal of the American Society for Information
Science.
Indexing by Latent Semantic Analysis. 1990 41, 6 (Journal of the
American Society for Information Science), 391â€“407.

[15] Brian P. Eddy, Jeffrey A. Robinson, Nicholas A. Kraft, and Jeffrey C. Carver. 2013.
Evaluating source code summarization techniques: Replication and expansion. In
Proceedings of the 21st International Conference on Program Comprehension. IEEE
Computer Society, San Francisco, CA, USA, 13â€“22.

[16] Wafaa S. El-Kassas, Cherif R. Salama, Ahmed A. Rafea, and Hoda K. Mohamed.
2021. Automatic text summarization: A comprehensive survey. Expert Systems
with Applications 165 (2021), 113679.

[17] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Proceedings of
the 25th Conference on Empirical Methods in Natural Language Processing: Findings.
Association for Computational Linguistics, Online Event, 1536â€“1547.

[18] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2018. Structured

Neural Summarization. CoRR abs/1811.01824 (2018).

[19] Jaroslav M. Fowkes, Pankajan Chanthirasegaran, Razvan Ranca, Miltiadis Alla-
manis, Mirella Lapata, and Charles Sutton. 2017. Autofolding for Source Code
Summarization. IEEE Transactions on software Engineering 43, 12 (2017), 1095â€“
1109.

[20] Mahak Gambhir and Vishal Gupta. 2017. Recent automatic text summarization

techniques: a survey. Artificial Intelligence Review 47, 1 (2017), 1â€“66.

[21] Milos Gligoric, Lamyaa Eloussi, and Darko Marinov. 2015. Practical regression test
selection with dynamic file dependencies. In Proceedings of the 24th International
Symposium on Software Testing and Analysis. ACM, Baltimore, MD, USA, 211â€“222.
[22] Zi Gong, Cuiyun Gao, Yasheng Wang, Wenchao Gu, Yun Peng, and Zenglin Xu.
2022. Source Code Summarization with Structural Relative Position Guided
Transformer. CoRR abs/2202.06521 (2022).

[23] David Gros, Hariharan Sezhiyan, Prem Devanbu, and Zhou Yu. 2020. Code to
Comment "Translation": Data, Metrics, Baselining & Evaluation. In Proceedings
of the 35th International Conference on Automated Software Engineering. IEEE,
Melbourne, Australia, 746â€“757.

[24] Sonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting program
comprehension with source code summarization. In Proceedings of the 32nd
International Conference on Software Engineering. ACM, Cape Town, South Africa,
223â€“226.

[25] Sonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. 2010. On the
Use of Automated Text Summarization Techniques for Summarizing Source
Code. In Proceedings of the 17th Working Conference on Reverse Engineering. IEEE
Computer Society, Beverly, MA, USA, 35â€“44.

[26] Sakib Haque, Aakash Bansal, Lingfei Wu, and Collin McMillan. 2021. Action
Word Prediction for Neural Source Code Summarization. In Proceedings of the
28th International Conference on Software Analysis, Evolution and Reengineering.
IEEE, Honolulu, HI, USA, 330â€“341.

[27] Carl S. Hartzman and Charles F. Austin. 1993. Maintenance productivity: obser-
vations based on an experience in a large system environment. In Proceedings of
the 3rd Conference of the Centre for Advanced Studies on Collaborative Research.
IBM, Toronto, Ontario, Canada, 138â€“170.

Conference 2022, xxx, 2022, xxx

Weisong Sun and Chunrong Fang, et al.

[28] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-Term Memory.

Neural Computation 9, 8 (1997), 1735â€“1780.

[29] Wan Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min, Jing Tang, and Min
Sun. 2018. A Unified Model for Extractive and Abstractive Summarization using
Inconsistency Loss. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics. Association for Computational Linguistics, Melbourne,
Australia, 132â€“141.

[30] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment genera-
tion. In Proceedings of the 26th International Conference on Program Comprehension.
ACM, Gothenburg, Sweden, 200â€“210.

[31] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment
generation with hybrid lexical and syntactical information. Empirical Software
Engineering 25, 3 (2020), 2179â€“2217.

[32] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing
Source Code with Transferred API Knowledge. In Proceedings of the 27th Inter-
national Joint Conference on Artificial Intelligence. ijcai.org, Stockholm, Sweden,
2269â€“2275.

[33] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic
Code Search. CoRR abs/1909.09436 (2019).

[34] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of
the 54th Annual Meeting of the Association for Computational Linguistics. The
Association for Computer Linguistics, Berlin, Germany, 2073â€“2083.

[35] Peddamail Jayavardhan Reddy, Yao Ziyu, Wang Zhen, and Sun Huan. 2018. A
comprehensive study of staqc for deep code summarization. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD 2018). ACM, London, UK, 1â€“8.

[36] Mira Kajko-Mattsson. 2005. A Survey of Documentation Practice within Correc-

the 14th International Conference on Evaluation of Novel Approaches to Software
Engineering. SciTePress, Heraklion, Crete, Greece, 15â€“26.

[53] Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori L. Pollock,
and K. Vijay-Shanker. 2013. Automatic generation of natural language summaries
for Java classes. In Proceedings of the 21st International Conference on Program
Comprehension. IEEE Computer Society, San Francisco, CA, USA, 23â€“32.
[54] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:
a Method for Automatic Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Computational Linguistics. ACL,
Philadelphia, PA, USA, 311â€“318.

[55] Long N. Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James T. Anibal, Alec Peltekian,
and Yanfang Ye. 2021. CoTexT: Multi-task Learning with Code-Text Transformer.
CoRR abs/2105.08645 (2021).

[56] Paige Rodeghero, Collin McMillan, Paul W. McBurney, Nigel Bosch, and Sidney K.
Dâ€™Mello. 2014. Improving automated source code summarization via an eye-
tracking study of programmers. In Proceedings of the 36th International Conference
on Software Engineering. ACM, Hyderabad, India, 390â€“401.

[57] Devjeet Roy, Sarah Fakhoury, and Venera Arnaoudova. 2021. Reassessing auto-
matic evaluation metrics for code summarization tasks. In Proceedings of the 29th
Joint European Software Engineering Conference and Symposium on the Founda-
tions of Software Engineering. ACM, Athens, Greece, 1105â€“1116.

[58] Salesforce. 2021. CodeT5. site: https://github.com/salesforce/CodeT5. Accessed

March, 2022.

[59] Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A Vector Space Model

for Automatic Indexing. Commun. ACM 18, 11 (1975), 613â€“620.

[60] Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get To The Point:
Summarization with Pointer-Generator Networks. In Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics. Association for
Computational Linguistics, Vancouver, Canada, 1073â€“1083.

tive Maintenance. Empirical Software Engineering 10, 1 (2005), 31â€“55.

[37] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimiza-
tion. In Proceedings of the 3th International Conference on Learning Representations
â€“ Poster. OpenReview.net, San Diego, CA, USA, 1â€“15.

[38] Jens Krinke. 2006. Effects of context on program slicing. Journal of Systems and

Software 79, 9 (2006), 1249â€“1260.

[39] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Improved
Code Summarization via a Graph Neural Network. In Proceedings of the 28th
International Conference on Program Comprehension. ACM, Seoul, Republic of
Korea, 184â€“195.

[40] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for
generating natural language summaries of program subroutines. In Proceedings of
the 41st International Conference on Software Engineering. IEEE / ACM, Montreal,
QC, Canada, 795â€“806.

[41] Alexander LeClair and Collin McMillan. 2019. Recommendations for Datasets
for Source Code Summarization. In Proceedings of the 23th Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies. Association for Computational Linguistics, Minneapolis,
MN, USA, 3931â€“3937.

[42] Jia Li, Yongmin Li, Ge Li, Xing Hu, Xin Xia, and Zhi Jin. 2021. EditSum: A
Retrieve-and-Edit Framework for Source Code Summarization. In Proceedings
of the 36th International Conference on Automated Software Engineering. IEEE,
Melbourne, Australia, 155â€“166.

[43] Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen, Hui Li, and Rongxin
Wu. 2021. Improving Code Summarization with Block-wise Abstract Syntax
Tree Splitting. In Proceedings of the 29th International Conference on Program
Comprehension. IEEE, Madrid, Spain, 184â€“195.

[44] Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries.
In Proceedings of the 42nd Annual Meeting of the Association for Computational
Linguistics â€“ workshop on Text Summarization Branches Out. Association for
Computational Linguistics, Barcelona, Spain, 74â€“81.

[45] Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. 2020. Automatic
Code Summarization via Multi-dimensional Semantic Fusing in GNN. CoRR
abs/2006.05405 (2020).

[46] Yang Liu. 2019.

Fine-tune BERT for Extractive Summarization.

abs/1903.10318 (2019).

CoRR

[47] Robert C Martin. 2009. Clean code: a handbook of agile software craftsmanship.

Pearson Education.

[48] Mani Maybury. 1999. Advances in automatic text summarization. MIT press.
[49] Paul W. McBurney and Collin McMillan. 2016. Automatic Source Code Summa-
rization of Context for Java Methods. IEEE Transactions on Software Engineering
42, 2 (2016), 103â€“119.

[50] Microsoft. 2021. CodeBert. site: https://github.com/microsoft/CodeBERT. Ac-

cessed March, 2022.

[51] David M. Mimno, Wei Li, and Andrew McCallum. 2007. Mixtures of hierarchical
topics with Pachinko allocation. In Proceedings of the 24th International Conference
Machine Learning. ACM, Corvallis, Oregon, USA, 633â€“640.

[52] Jessica Moore, Ben Gelman, and David Slater. 2019. A Convolutional Neural
Network for Language-Agnostic Source Code Summarization. In Proceedings of

[61] Ramin Shahbazi, Rishab Sharma, and Fatemeh H. Fard. 2021. API2Com: On
the Improvement of Automatically Generated Code Comments Using API Doc-
umentations. In Proceedings of the 29th International Conference on Program
Comprehension. IEEE, Madrid, Spain, 411â€“421.

[62] Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dong-
mei Zhang, and Hongbin Sun. 2021. Neural Code Summarization: How Far Are
We?. In CoRR, Vol. abs/2107.07112.

[63] Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, and Chandan K. Reddy. 2021.
Neural Abstractive Text Summarization with Sequence-to-Sequence Models.
Transactions on Data Science 2, 1 (2021), 1:1â€“1:37.

[64] Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi Miyamoto, and
Tadayuki Matsumura. 2019. Automatic Source Code Summarization with Ex-
tended Tree-LSTM. In Proceedings of the 18th International Joint Conference on
Neural Networks. IEEE, Hungary, 1â€“8.

[65] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori L. Pollock, and K. Vijay-
Shanker. 2010. Towards automatically generating summary comments for Java
methods. In Proceedings of the 25th International Conference on Automated Software
Engineering. ACM, Antwerp, Belgium, 43â€“52.

[66] Sean Stapleton, Yashmeet Gambhir, Alexander LeClair, Zachary Eberhart, Westley
Weimer, Kevin Leach, and Yu Huang. 2020. A Human Study of Comprehension
and Code Summarization. In Proceedings of the 28th International Conference on
Program Comprehension. ACM, Seoul, Republic of Korea, 2â€“13.

[67] Josef Steinberger and Karel Jezek. 2009. Update Summarization Based on Latent
Semantic Analysis. In Proceedings of the 12th International Conference on Text,
Speech and Dialogue. Springer, Pilsen, Czech Republic, 77â€“84.

[68] Ted Tenny. 1988. Program Readability: Procedures Versus Comments.

IEEE

Transactions on Software Engineering 14, 9 (1988), 1271â€“1279.

[69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017. Atten-
tion is All You Need. In Proceedings of the 31st International Conference on Neural
Information Processing Systems. Curran Associates Inc., Long Beach, CA, USA,
5998â€“6008.

[70] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
Philip S. Yu. 2018. Improving automatic source code summarization via deep
reinforcement learning. In Proceedings of the 33rd International Conference on
Automated Software Engineering. ACM/IEEE, Montpellier, France, 397â€“407.
[71] Wenhua Wang, Yuqun Zhang, Yulei Sui, Yao Wan, Zhou Zhao, Jian Wu, Philip
Yu, and Guandong Xu. 2020. Reinforcement-Learning-Guided Source Code
Summarization using Hierarchical Attention.
IEEE Transactions on Software
Engineering (Early Access) (2020), 1â€“19.

[72] Yanlin Wang, Ensheng Shi, Lun Du, Xiaodi Yang, Yuxuan Hu, Shi Han, Hongyu
Zhang, and Dongmei Zhang. 2021. CoCoSum: Contextual Code Summarization
with Multi-Relational Graph Neural Network. CoRR abs/2107.01933 (2021), 1â€“24.
[73] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. 2021. CodeT5:
Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Under-
standing and Generation. In Proceedings of the 26th Conference on Empirical
Methods in Natural Language Processing. Association for Computational Linguis-
tics, Virtual Event / Punta Cana, Dominican Republic, 8696â€“8708.

An Extractive-and-Abstractive Framework for Source Code Summarization

Conference 2022, xxx, 2022, xxx

[74] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code Generation as a Dual
Task of Code Summarization. In Proceedings of the 33rd Annual Conference on
Neural Informatiom Processing Systems (NeurIPS 2019). Vancouver, BC, Canada,
6559â€“6569.

[75] Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020. Retrieve and Refine:
Exemplar-based Neural Comment Generation. In Proceedings of the 35th Interna-
tional Conference on Automated Software Engineering. IEEE, Melbourne, Australia,
349â€“360.

[76] Edmund Wong, Taiyue Liu, and Lin Tan. 2015. CloCom: Mining existing source
code for automatic comment generation. In Proceedings of the 22nd International
Conference on Software Analysis, Evolution, and Reengineering. IEEE Computer
Society, Montreal, QC, Canada, 380â€“389.

[77] Scott N. Woodfield, Hubert E. Dunsmore, and Vincent Yun Shen. 1981. The Effect
of Modularization and Comments on Program Comprehension. In Proceedings of
the 5th International Conference on Software Engineering. IEEE Computer Society,
San Diego, California, USA, 215â€“223.

[78] Hongqiu Wu, Hai Zhao, and Min Zhang. 2021. Code Summarization with
Structure-induced Transformer. In Proceedings of the Findings of the 59th An-
nual Meeting of the Association for Computational Linguistics. Association for
Computational Linguistics, Online Event, 1078â€“1090.

[79] Yuxiang Wu and Baotian Hu. 2018. Learning to Extract Coherent Summary via
Deep Reinforcement Learning. In Proceedings of the 32nd AAAI Conference on
Artificial Intelligence. AAAI Press, New Orleans, Louisiana, USA, 5602â€“5609.
[80] Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E. Hassan, and Shan-
ping Li. 2018. Measuring Program Comprehension: A Large-Scale Field Study
with Professionals.
IEEE Transactions on Software Engineering 44, 10 (2018),
951â€“976.

[81] Zhen Yang, Jacky Keung, Xiao Yu, Xiaodong Gu, Zhengyuan Wei, Xiaoxue Ma,
and Miao Zhang. 2021. A Multi-Modal Transformer-based Code Summarization
Approach for Smart Contracts. In Proceedings of the 29th International Conference
on Program Comprehension. IEEE, Madrid, Spain, 1â€“12.

[82] Juan Zhai, Xiangzhe Xu, Yu Shi, Guanhong Tao, Minxue Pan, Shiqing Ma, Lei
Xu, Weifeng Zhang, Lin Tan, and Xiangyu Zhang. 2020. CPC: automatically
classifying and propagating natural language comments via program analysis. In
Proceedings of the 42nd International Conference on Software Engineering. ACM,
Seoul, South Korea, 1359â€“1371.

[83] Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Raymond J. Mooney,
Junyi Jessy Li, and Milos Gligoric. 2021. Learning to Generate Code Comments
from Class Hierarchies. CoRR abs/2103.13426 (2021).

[84] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based neural source code summarization. In Proceedings of the 42nd
International Conference on Software Engineering. ACM, Seoul, South Korea, 1385â€“
1397.

[85] Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun
Zhao. 2018. Neural Document Summarization by Jointly Learning to Score and
Select Sentences. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics. Association for Computational Linguistics, Melbourne,
Australia, 654â€“663.

[86] Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue Chen, and
Harald C. Gall. 2021. Adversarial Robustness of Deep Code Comment Generation.
ACM Transactions on Software Engineering and Methodology 1, 1 (2021), 1â€“30.
[87] Yuxiang Zhu and Minxue Pan. 2019. Automatic Code Summarization: A System-

atic Literature Review. CoRR abs/1909.04352 (2019).

