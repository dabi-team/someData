CheapET-3: Cost-Efficient Use of Remote DNN Models
Michael Weiss
michael.weiss@usi.ch
Università della Svizzera italiana
Lugano, Switzerland

2
2
0
2

g
u
A
4
2

]
E
S
.
s
c
[

1
v
2
5
5
1
1
.
8
0
2
2
:
v
i
X
r
a

ABSTRACT
On complex problems, state of the art prediction accuracy of Deep
Neural Networks (DNN) can be achieved using very large-scale
models, consisting of billions of parameters. Such models can only
be run on dedicated servers, typically provided by a 3rd party ser-
vice, which leads to a substantial monetary cost for every prediction.
We propose a new software architecture for client-side applications,
where a small local DNN is used alongside a remote large-scale
model, aiming to make easy predictions locally at negligible mon-
etary cost, while still leveraging the benefits of a large model for
challenging inputs. In a proof of concept we reduce prediction cost
by up to 50% without negatively impacting system accuracy.

CCS CONCEPTS
• Software and its engineering → Designing software.

KEYWORDS
neural networks, software architecture, network supervision

ACM Reference Format:
Michael Weiss. 2022. CheapET-3: Cost-Efficient Use of Remote DNN Models.
In Proceedings of the 30th ACM Joint European Software Engineering Confer-
ence and Symposium on the Foundations of Software Engineering (ESEC/FSE
’22), November 14–18, 2022, Singapore, Singapore. ACM, New York, NY, USA,
3 pages. https://doi.org/10.1145/3540250.3559082

1 INTRODUCTION
Advances in machine learning showed a clear trend towards build-
ing ever larger Deep Neural Networks (DNNs): AlexNet [9], the
highly influential imagenet model released in 2012 set a milestone,
with its then considered huge scale 60M parameters. Now, some
models exceed even the trillion parameter threshold [2]. Such huge
models cannot be executed on resource constrained devices and
environments such as microprocessors, mobile devices or in web
browsers, but are instead hosted in server centers with specialized
hardware, causing substantial financial cost. E.g., a single request
to the popular GPT-3 models [1] is billed up to $0.481.

In this research abstract, we present a software architecture
designed to reduce the monetary costs of using large-scale DNNs.

14000 tokens on a fine-tuned davinci-model (0.12$ per 1000 tokens) in July 2022. See
beta.openai.com.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9413-0/22/11. . . $15.00
https://doi.org/10.1145/3540250.3559082

Figure 1: Proposed Architecture

We show a proof of concept (POC) of our approach using the the
Imdb sentiment classification benchmark [10], with 25’000 training
samples and 2000 randomly chosen test samples, where we achieve
a similar prediction performance to using pure GPT-3 predictions
at only half the prediction cost.

2 PROPOSED ARCHITECTURE
Our proposed architecture is shown in Figure 1: Instead of directly
passing every input to the remote DNN, each input is first given to
a small-scale and less accurate local surrogate model, s.t. easy, low
uncertainty predictions are made locally at negligible cost.

A supervisor [3, 5–7, 13, 15, 17, 20] is employed to detect inputs
for which local predictions are confident enough to be trusted. Un-
trusted predictions are forwarded to the large-scale remote model
for a more reliable prediction. Both local model and supervisor
should be designed or chosen to account for the resource con-
straints: Local models can e.g. use a compressed input space (e.g.
small vocabulary sizes in NLP problems) and a reduced number of
layers. For supervision, a wide range of techniques exist, which
we compared in our previous work [15–19]. We identified simple
softmax-based supervisors, such as Vanilla Softmax (SM) [4] (for
classification problems) and Mahalanobis-distance based Surprise

 
 
 
 
 
 
ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore

Michael Weiss

remote model: When sending 74% of the inputs to GPT-3, accuracy
increases by 0.11 while still saving 26% on cost.

4 CONCLUSION & FUTURE WORK
Our POC shows the capability of our architecture to save monetary
costs while only marginally – if at all – impacting the overall sys-
tem performance. The architecture seamlessly generalizes beyond
our POC to other classification and regression problems. It is also
likely to lead to lower energy consumption and, on average, faster
response time. As future work, we plan to evaluate this architecture
on different domains.

ACKNOWLEDGMENTS
Many thanks to Paolo Tonella and Andrea Stocco for their helpful
advice. This work was partially supported by the H2020 project
PRECRIME, funded under the ERC Advanced Grant 2017 Program
(ERC Grant Agreement n. 787703).

REFERENCES
[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.

[2] William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch transformers:

Scaling to trillion parameter models with simple and efficient sparsity.

[3] Raul Sena Ferreira, Jean Arlat, Jeremie Guiochet, and Helene Waeselynck. 2021.
Benchmarking Safety Monitors for Image Classifiers with Machine Learning. In
2021 IEEE 26th Pacific Rim International Symposium on Dependable Computing
(PRDC) (2021-10-04). IEEE. https://doi.org/10.1109/prdc53464.2021.00012
[4] Dan Hendrycks and Kevin Gimpel. 2016. A Baseline for Detecting Mis-
(2016).

classified and Out-of-Distribution Examples in Neural Networks.
arXiv:1610.02136v3 [cs.NE]

[5] Jens Henriksson, Christian Berger, Markus Borg, Lars Tornberg, Cristofer En-
glund, Sankar Raman Sathyamoorthy, and Stig Ursing. 2019. Towards Struc-
tured Evaluation of Deep Neural Network Supervisors. In 2019 IEEE Inter-
https:
national Conference On Artificial Intelligence Testing (AITest). IEEE.
//doi.org/10.1109/aitest.2019.00-12

[6] Jens Henriksson, Christian Berger, Markus Borg, Lars Tornberg, Sankar Raman
Sathyamoorthy, and Cristofer Englund. 2019. Performance Analysis of Out-
of-Distribution Detection on Various Trained Neural Networks. In 2019 45th
Euromicro Conference on Software Engineering and Advanced Applications (SEAA).
IEEE, 113–120.

[7] Manzoor Hussain, Nazakat Ali, and Jang-Eui Hong. 2022. DeepGuard: a frame-
work for safeguarding autonomous driving systems from inconsistent behaviour.
Automated Software Engineering 29, 1 (2022), 1–32. https://doi.org/10.1007/
s10515-021-00310-0

[8] Seah Kim and Shin Yoo. 2020. Evaluating surprise adequacy for question answer-
ing. In Proceedings of the IEEE/ACM 42nd International Conference on Software
Engineering Workshops. 197–202. https://doi.org/10.1145/3387940.3391465
[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-
cation with deep convolutional neural networks. Advances in neural information
processing systems 25 (2012).

[10] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and
Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Association for Computational Linguistics,
Portland, Oregon, USA, 142–150. http://www.aclweb.org/anthology/P11-1015
[11] Prasanta Chandra Mahalanobis. 1936. On the generalized distance in statistics.

National Institute of Science of India.

[12] Zilun Peng and Akshay Budhkar. 2021. GPT-Neo vs. GPT-3: Are Commercialized
NLP Models Really That Much Better? https://medium.com/georgian-impact-
blog/gpt-neo-vs-gpt-3-are-commercialized-nlp-models-really-that-much-
better-f4c73ffce10b

[13] Andrea Stocco, Michael Weiss, Marco Calzana, and Paolo Tonella. 2020. Mis-
behaviour prediction for autonomous driving systems. In Proceedings of the
ACM/IEEE 42nd International Conference on Software Engineering. ACM, 12 pages.
https://doi.org/10.1145/3377811.3380353

[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).

Figure 2: Performance of our architecture (nominal data, SM
supervisor)

Adequacy (MDSA) [8] (for general problems) as efficient supervi-
sors, which can be evaluated at negligible prediction-time cost [19]:
The former is simply using the predicted softmax likelihood as
a confidence score, where the latter measures uncertainty as the
mahalanobis-distance [11] between a specific layer’s activations
for the observed input and the training set.

3 PROOF OF CONCEPT
We evaluate our design using a small transformer [14], trained with
a small vocabulary of 2000 tokens, as local surrogate model; the
text-curie-001 GPT-3 model [1] (estimate 13 billion params [12],
based on vocabulary with size 50257) as remote model, and inde-
pendently evaluate both SM and MDSA as supervisor, using three
different test sets (nominal, corrupted [19] and partially-corrupted).
We then measure the resulting overall system accuracy, i.e., the
accuracy given the local predictions for inputs where the super-
visor trusted the local predictions, and the remote predictions for
all other inputs. In practice, the threshold as to when a supervisor
trusts a local prediction could be continuously adapted to reach a
target performance/cost ratio, hence we report results for a flexible
percentage of inputs forwarded to GPT3.

Results are shown in Figure 2. Due to space constraints, we only
discuss the results for the nominal test set and the SM supervi-
sor2. Clearly, our architecture allows for major cost savings, while
maintaining a high prediction reliability: In fact, by only making
a prediction on 48% of inputs, thus saving more than half of the
GPT-3 cost, we achieve the same accuracy as if all predictions were
made by GPT-3. Being even more cost-reduction oriented, with a
70% cost saving accuracy decreases by only 0.02, thus still being
much better than when using only the local model. Interestingly, we
found that thanks to their complementary predictive capabilities,
there is a combination of local and remote model that can lead to an
overall system performance higher than the standalone, expensive

2The other results are similar, but with lower accuracy (i.e., the curve is shifted down-
ward) for corrupted and partially-corrupted inputs and with slightly worse (i.e., less
curved) results for MDSA.

CheapET-3: Cost-Efficient Use of Remote DNN Models

ESEC/FSE ’22, November 14–18, 2022, Singapore, Singapore

[15] Michael Weiss, Rwiddhi Chakraborty, and Paolo Tonella. 2021. A Review and
Refinement of Surprise Adequacy. In 2021 IEEE/ACM Third International Workshop
on Deep Learning for Testing and Testing for Deep Learning (DeepTest). 17–24.
https://doi.org/10.1109/DeepTest52559.2021.00009

[16] Michael Weiss, André García Gómez, and Paolo Tonella. 2022. A Forgotten
Danger in DNN Supervision Testing: Generating and Detecting True Ambiguity.
arXiv preprint arXiv:2207.10495 (2022).

[17] Michael Weiss and Paolo Tonella. 2021. Fail-Safe Execution of Deep Learning
based Systems through Uncertainty Monitoring. In 2021 14th IEEE Conference
on Software Testing, Verification and Validation (ICST). IEEE, IEEE, 24–35. https:
//doi.org/10.1109/icst49551.2021.00015

[18] Michael Weiss and Paolo Tonella. 2021. Uncertainty-Wizard: Fast and User-
Friendly Neural Network Uncertainty Quantification. In 2021 14th IEEE Conference

on Software Testing, Verification and Validation (ICST). 436–441. https://doi.org/
10.1109/ICST49551.2021.00056

[19] Michael Weiss and Paolo Tonella. 2022. Simple Techniques Work Surprisingly
Well for Neural Network Test Prioritization and Active Learning (Replicability
Study). In Proceedings of the 31st ACM SIGSOFT International Symposium on
Software Testing and Analysis (Virtual, South Korea) (ISSTA 2022). Association for
Computing Machinery, New York, NY, USA, 139–150. https://doi.org/10.1145/
3533767.3534375

[20] Yan Xiao, Ivan Beschastnikh, David S. Rosenblum, Changsheng Sun, Sebastian
Elbaum, Yun Lin, and Jin Song Dong. 2021. Self-Checking Deep Neural Networks
in Deployment. In 2021 IEEE/ACM 43rd International Conference on Software
Engineering (ICSE). IEEE, IEEE, 372–384. https://doi.org/10.1109/icse43902.2021.
00044

