Exploring GPU Stream-Aware Message Passing
using Triggered Operations

Naveen Namashivayam
Hewlett Packard Enterprise, USA
naveen.ravi@hpe.com
Nick Radcliffe
Hewlett Packard Enterprise, USA
nick.radcliffe@hpe.com

Krishna Kandalla
Hewlett Packard Enterprise, USA
krishnachaitanya.kandalla@hpe.com
Larry Kaplan
Hewlett Packard Enterprise, USA
larry.kaplan@hpe.com

Trey White
Hewlett Packard Enterprise, USA
trey.white@hpe.com
Mark Pagel
Hewlett Packard Enterprise, USA
mark.pagel@hpe.com

2
2
0
2

g
u
A
9

]

C
D
.
s
c
[

1
v
7
1
8
4
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Modern heterogeneous supercomputing systems are
comprised of compute blades that offer CPUs and GPUs. On
such systems, it is essential to move data efﬁciently between
these different compute engines across a high-speed network.
While current generation scientiﬁc applications and systems
software stacks are GPU-aware, CPU threads are still required
to orchestrate data moving communication operations and inter-
process synchronization operations.

A new GPU stream-aware MPI communication strategy called
stream-triggered (ST) communication is explored to allow ofﬂoad-
ing both computation and communication control paths to the
GPU. The proposed ST communication strategy is implemented
on HPE Slingshot Interconnects over a new proprietary HPE
Slingshot NIC (Slingshot 11) using the supported triggered oper-
ations feature. Performance of the proposed new communication
strategy is evaluated using a microbenchmark kernel called
Faces, based on the nearest-neighbor communication pattern in
the CORAL-2 Nekbone benchmark, over a heterogeneous node
architecture consisting of AMD CPUs and GPUs.

Index Terms—heterogeneous supercomputing systems, CPU,
GPU, MPI, GPU-NIC Async, GPU Streams, GPU Control Pro-
cessors, Control Path, Data Path

I. INTRODUCTION

C URRENT-generation scientiﬁc applications and systems-

software stacks are using GPU-aware [30] Message
Passing Interface (MPI) [20] implementations. GPU-awareness
for inter-node MPI data movement using Remote Direct
Memory Access (RDMA) [29, 31] allows buffers to directly
move from the GPU memory to a network adapter without
staging through host memory. For intra-node MPI transfers,
buffer move through various Peer-to-Peer (P2P) data transfer
mechanisms [27] supported by different GPU vendors.

Even with such GPU-awareness in the MPI software stack,
CPU threads are still required to orchestrate the data-moving
communication and inter-process synchronization operations.
This requirement results in all communication and synchro-
nization operations occurring at GPU kernel boundaries.

Fig. 1 demonstrates the sequence of events for a typical
GPU-aware parallel application that relies on MPI for inter-
process communication and synchronization operations. An
MPI process running on the CPU a ﬁrst synchronizes with
the local GPU device to ensure completion of the compute

d

kernel (K1) execution. Next, it
b , launches, progresses, and
c completes the inter-process communication/synchronization
operations. Subsequent compute kernels (K2) on the GPU
launched only after the inter-process communication
are
operations have completed. This behavior creates potentially
expensive synchronization points at kernel boundaries that
require the CPU to synchronize with the GPU and Network
Interface Controller (NIC) devices.

Fig. 1. Illustrating sequence of events on a typical GPU-aware parallel appli-
cation that relies on MPI for inter-process communication and synchronization
operations.

A GPU stream [12] is a queue of device operations. GPU
compute kernel concurrency is achieved through creating
multiple concurrent streams. Operations issued on a stream get
executed in the order in which these operations were issued.
Also, the execution of these operations is asynchronous with
respect to operations in other streams. A new GPU stream-
aware MPI communication strategy called stream-triggered
(ST) communication is proposed and explored to ofﬂoad both
the computation and communication control paths to the GPU.
The new proposed communication scheme allows the CPU
to create network command descriptors with deferred execu-
tion semantics and append them to the NIC command queue.
These command descriptors have special attributes that allow
them to get triggered [16] at a later point in time when certain
the CPU also creates
conditions are reached. In addition,

This work is planned for possible publication to the IEEE. Copyright may be transferred without notice, after which this
version may no longer accessible.

 
 
 
 
 
 
control operations and appends them to the GPU stream. These
operations will be executed by the GPU control processor in
sequential order, relative to other operations enqueued in the
GPU stream. When these control operations are executed by
the GPU control processor, they act as triggers that initiate
the execution of the previously appended network command
descriptors in the NIC’s command queue.

This approach also allows the GPU control processor to
synchronize with the NIC to determine the successful comple-
tion of communication operations. This synchronization step
involves the use of hardware counters in the NIC. Thus, ST
tries to minimize the need for synchronization between the
CPU and GPU.

A. Contributions of This Work

The following are the major contributions.

1) Propose a GPU stream-aware MPI communication strat-
egy to ofﬂoad both computation and communication
control paths to the GPU;

2) Implement

the proposed communication strategy on
HPE Slingshot Interconnects exploiting the triggered op-
erations feature [16] available in the new HPE Slingshot
NIC (Slingshot 11) [14, 10, 18];

3) To demonstrate the effectiveness of the new proposed
communication strategy, create a microbenchmark ker-
nel called Faces based on the nearest-neighbor commu-
nication pattern in the CORAL-2 [1] Nekbone bench-
mark [6]; and

4) Experiment with proposed solutions on a heterogeneous
system architecture with support for AMD-based CPU
and GPU processors.

II. BACKGROUND

This section provides an overview of the ST communication
strategy in MPI, along with the deferred execution features
provided by the HPE Slingshot NIC (Slingshot 11) to support
the proposed ST interface.

A. MPI Control and Data Paths

MPI communication operations in GPU-aware applications
are typically comprised of control paths and data paths.
Control paths correspond to coordination operations that occur
between the application process running on the CPU,
the
control processor on the GPU device, application compute
kernels running on the GPU device, and the NIC. Data paths
refer to those operations that involve moving data between
CPU-attached and GPU-attached memory regions. These data
movement operations can occur within the same compute
node or between different compute nodes across a high-speed
network. The data paths are typically handled by the NIC
(for inter-node data transfer operations) and the GPU DMA
engines (for intra-node peer-to-peer data transfer operations).
As detailed in Fig. 1, applications typically experience
expensive synchronization points, and an application process
running on the CPU is closely involved in the progress of
control paths.

Fig. 2 includes a high-level description of the proposed
new ST communication strategy. This strategy enables a
GPU-aware application to ofﬂoad control paths to underlying
implementation and hardware components.

B. Stream Triggered Communication Overview

A parallel application using the ST strategy continues to
manage compute kernels on the GPU via existing mechanisms.
In addition, the ST strategy allows an application process
running on the CPU to deﬁne a set of GPU stream based MPI
communication operations. The new proposed MPI operations
are detailed in Section III.

These communication operations can be scheduled for exe-
cution at a later point in time. More importantly, in addition to
offering a deferred execution model, the ST strategy enables
the GPU control processor (GPU CP) to be closely involved
in the control paths associated with MPI communication oper-
ations. Along with managing compute kernels deﬁned by the
application, the GPU CP coordinates with the NIC to manage
the control paths of MPI communication operations. This
design minimizes the need for the application process running
on the CPU to drive the control paths of MPI communication
operations.

the CPU enqueues

triggered ST-based MPI operations to the NIC,

As shown in the Fig. 2, an application process running on
a GPU kernel K1 to the GPU stream,
the
b
corresponding trigger operations to the GPU stream, and d
GPU kernel K2 to the same stream. In Fig. 2, the enqueue
operations from the CPU are represented via dashed arrows,
and the actual executions of various operations from the GPU
Control Processor are represented as solid arrows.

c

Illustrating the sequence of events on a parallel application that uses

Fig. 2.
ST MPI execution for inter-process communication and synchronization.

1 The GPU CP launches K1 and 2 waits for it to complete.
the GPU CP triggers the execution of
Once K1 completes,
MPI operations and 4 waits for these operations to ﬁnish.
Next,

the GPU CP launches K2.

3

5

Thus, an application process running on the CPU enqueues
operations to the NIC command queue and GPU stream, but
the CPU is not directly involved in the control paths of MPI
communication operations and subsequent kernel launch and

tear-down operations. The CPU also does not directly wait
for MPI communication operations to complete. The GPU
CP manages the control paths, and this potentially eliminates
the expensive synchronization points between an application
process and its GPU device.

C. Slingshot 11 Triggered Operations using Libfabric
Deferred Work Queues

Triggered operations [16] supported by Slingshot 11 are the
key hardware features required for implementing the proposed
ST MPI communication scheme. They are exposed using the
deferred work queue (DWQ) [3] features in the Libfabric [23]
SW stack. DWQ allows an application to enqueue a list
of operation with deferred execution semantics. Each DWQ
descriptor is comprised of a DMA descriptor, a trigger counter
object, a completion counter object, and a trigger threshold
value.

When the MPI implementation submits a DWQ send opera-
tion to the NIC command queue, this operation is not executed
by the NIC immediately. The operation gets executed when
the associated trigger counter object in the DWQ operation
reaches a required threshold value. The GPU CP is responsible
for performing this update. Similarly, the completion of the
DWQ operation is monitored by the GPU CP using the
completion counter object which can be used for performing
synchronizations or execute other enqueued DWQ operations.
The current generation Slingshot 11 NIC provides support
for the following DWQ operations: (1) tagged and untagged
sends, (2) one-sided RMA operations, and (3) fetching and
non-fetching atomic operations. Missing DWQ features re-
quired by the MPI ST semantics, like the tagged and untagged
receive operations, are emulated with an internal asynchronous
progress thread per MPI process.

The trigger and completion counter NIC objects can be
mapped to GPU CP accessible memory pointers for the GPU
CP to directly access these NIC counter objects.

D. Stream Memory Operations

GPU stream memory operations are used to support
the
proposed ST communication operations. Speciﬁcally, they are
used to update and monitor the trigger and completion counter
objects speciﬁed in Section. II-C. There are two common
stream memory operations supported by different GPU ven-
dors: (1) writeValue and (2) waitValue.

A writeValue operation enqueues a write command to the
GPU stream, and the write operation is performed only after
all earlier commands to this stream have completed execution.
A waitValue operation enqueues a wait command to the GPU
stream. All operations enqueued on this stream after the wait
command will not be executed until the deﬁned wait condition
is satisﬁed.

The function prototypes of the hipStreamWriteValue64 and
hipStreamWaitValue64 operations on the AMD HIP run-
time [5] are shown in Fig. 3. For brevity, similar stream mem-
ory operations available in the NVIDIA CUDA runtime [2]
are not shown.

hipError_t hipStreamWriteValue64(

hipStream_t stream, void* ptr, int64_t val);

hipError_t hipStreamWaitValue64(

hipStream_t stream, void* ptr, int64_t val,
uint64_t mask, unsigned int flags);

Fig. 3. Function prototypes for AMD HIP stream memory operations.

E. Mapping Stream Memory and SS11 DWQ Operations

The writeValue and waitValue operations are used by mapping
the trigger and completion counter objects to GPU accessible
memory pointers. In brief, the GPU CP using the enqueued
writeValue operation writes to the trigger counter object based
on the threshold value. This write operation acts as a trigger for
the previously enqueued DWQ(Section II-C) operation to be
executed. Similarly, the GPU CP using the enqueued waitValue
operation monitors the completion counter object associated
with the DWQ operation to wait for its completion.

III. PROPOSED ST MPI INTERFACE

This section provides a brief description of the proposed
MPI operations to support the ST communication strategy.
The proposed ST MPI operations follow semantics similar
to the point-to-point (P2P) two-sided messaging. These ST
operations are used to explore the potential performance of the
new communication strategy with minimal network resource
utilization. Also, these proposed operations are designed to be
used incrementally within existing MPI applications.

A. MPI Queues for P2P ST

MPIX Queue is a new data object created to support ST
MPI operations using P2P messaging semantics. The proposed
MPIX Queue object allows users to map a user-deﬁned GPU
stream handle to the MPI runtime, and it enables batching of
operations1. MPIX Create queue and MPIX Free queue are
the two new MPI operations to create and destroy an MPI
queue object, respectively. The function prototypes of these
operations are provided in Fig. 4. These are local operations
performed without interaction with any other process.

int MPIX_Create_queue(IN void *stream,

int MPIX_Free_queue(IN MPIX_Queue queue);

OUT MPIX_Queue *queue);

Fig. 4. Function prototypes for creating and destroying MPIX Queue objects.

operation

The MPIX Create queue

new
MPIX Queue object, with the GPU stream handle passed by
the user as input. Similarly, the MPIX Free queue allows the
implementation to release an already created MPIX Queue
object. Note that the free operation is used only to release

creates

a

1Batching of operations allows users to trigger execution of multiple
enqueued MPI operations using a single trigger operation. Extracting deferred
semantics on enqueued operations and triggering those deferred operations are
provided as part of the proposed functions in Section III-B.

1) ST Execution Sequence:
Fig. 6 demonstrates a se-
quence of ST operations executed in a simple MPI application.
The ﬁrst queue in Fig. 6 shows the sequence of operations
executed by the application process executing on the CPU,
while the second and third queues show the list of enqueued
operations in the MPIX Queue object and the corresponding
GPU device stream, respectively.

any internal resource maintained by the queue object, and it
is the responsibility of the users to wait for any associated
ST operation to complete before freeing the queue object.

B. MPI Enqueue Operations for P2P ST

of

enqueue

operations, MPIX Enqueue send

and
Set
MPIX Enqueue recv, introduced in this section performs the
actual GPU stream-based data movement operations. Along
with these enqueue operations, MPIX Enqueue start and
MPIX Enqueue wait operations are introduced to trigger
and wait for completion of the enqueued data movement
operations, respectively. The function prototypes of these
operations are provided in Fig. 5 and their semantics are
enumerated below.

int MPIX_Enqueue_send(const void *buf,

int count, MPI_Datatype datatype, int dest,
int tag, MPI_Comm comm, MPIX_Queue queue,
MPI_Request *request);

int MPIX_Enqueue_recv(void *buf, int count,

MPI_Datatype datatype, int source, int tag,
MPI_Comm comm, MPIX_Queue queue,
MPI_Request *request);

int MPIX_Enqueue_start(const MPIX_Queue queue);

int MPIX_Enqueue_wait(const MPIX_Queue queue);

Fig. 5. Function prototypes for enqueuing P2P ST communication.

1) All proposed new ST-based MPI communication opera-
tions are enqueued to a particular MPIX Queue object,
and these operations are executed in FIFO order. Exe-
cution of these operations is asynchronous with respect
to the host process running on the CPU.

2) MPIX Enqueue send and MPIX Enqueue recv are ex-
amples of ST MPI communication operations. These
operations create internal communication descriptors
that correspond to MPI data-movement operations, and
these descriptors are enqueued into the MPIX Queue
object.

3) The MPIX Enqueue start function allows an application
process to specify when the enqueued stream-based
MPI communication operations must be executed by the
GPU CP. All previously enqueued stream-based MPI
communication operations on the MPIX Queue object
are executed in batch by a single start operation. It is
not necessary to create a start operation per enqueued
stream-based MPI communication operation. A stream
memory writeValue is enqueued internally as part of the
enqueue start operation.

4) Similarly, MPIX Enqueue wait allows an application
process to deﬁne when the GPU CP must wait for the
completion of all previously executed stream-based com-
munication operations, and a stream memory waitValue
operation is enqueued internally as part of this operation.

Fig. 6.

Illustrating a sequence of ST MPI operations and their execution.

The application process directly enqueues the device ker-
nels (D1 and D2) to be executed by the GPU stream. The
MPIX Queue object is not relevant in the execution of com-
pute kernels that are ofﬂoaded to the GPU.

When an application process calls MPIX Enqueue send
(S1, S2) and MPIX Enqueue recv (R1, R2) operations,
enqueued in the MPIX Queue object. The
they are
MPIX Enqueue send and MPIX Enqueue recv operations re-
turn immediately. The enqueue operations create the necessary
communication descriptors for executing the ST send and
receive operations in a deferred manner.

MPIX Enqueue start

internally enqueues a writeValue
operation to the speciﬁed GPU stream. The execution
the writeValue operation enqueued as part of T1:
of
MPIX Enqueue start triggers the execution of S1 and R1.
Similarly, the execution of the writeValue operation enqueued
as part of T2: MPIX Enqueue start triggers the execution of
S2 and R2.

When the ST completion operations (W1 and W2) are called
by the application process using the MPIX Enqueue wait
operations, the MPI implementation enqueues these operations
into the MPIX Queue object, and the MPIX Enqueue wait
operations return immediately. For each MPIX Enqueue wait
operation, the MPI implementation also enqueues a waitValue
operation to the corresponding GPU stream. The wait oper-
ation allows the GPU CP to wait for all previously started
stream-based MPI communication operations to complete be-
fore working on other enqueued stream-based MPI operations
and device compute kernels.

2) Non-blocking ST Semantics:
The proposed enqueue
operations are non-blocking with respect to the application
host process. The non-blocking semantics of the proposed
enqueue operations eliminates the expensive synchronization

MPIX_Queue queue;
hipStream_t stream;

/* create a GPU stream object and use it to create an MPIX_Queue object */
hipStreamCreateWithFlags(&stream, hipStreamNonBlocking);
MPIX_Create_queue(MPI_COMM_WORLD_DUP, (void *)stream, &queue);

if (my_rank == 0) {

launch_device_compute_kernel(src_buf1, src_buf2, src_buf3, src_buf4, stream);

MPIX_Enqueue_send(src_buf1, SIZE, MPI_INT, 1, 123, queue, &sreq[0]);
MPIX_Enqueue_send(src_buf2, SIZE, MPI_INT, 1, 126, queue, &sreq[1]);
MPIX_Enqueue_send(src_buf3, SIZE, MPI_INT, 1, 125, queue, &sreq[2]);
MPIX_Enqueue_send(src_buf4, SIZE, MPI_INT, 1, 124, queue, &sreq[3]);

MPIX_Enqueue_start(queue); /* Enqueue_start enables triggering of all prior send ops */
MPIX_Enqueue_wait(queue);

/* wait blocks only the current GPU stream */

} else if (my_rank == 1) {

MPIX_Enqueue_recv(dst_buf1, SIZE, MPI_INT, 0, 123, queue, &rreq[0]);
MPIX_Enqueue_recv(dst_buf2, SIZE, MPI_INT, 0, 126, queue, &rreq[1]);
MPIX_Enqueue_recv(dst_buf3, SIZE, MPI_INT, 0, 125, queue, &rreq[2]);
MPIX_Enqueue_recv(dst_buf4, SIZE, MPI_INT, 0, 124, queue, &rreq[3]);

MPIX_Enqueue_start(queue);
MPIX_Enqueue_wait(queue);

launch_device_compute_kernel(dst_buf1, dst_buf2, dst_buf3, dst_buf4, stream);

}
hipStreamSynchronize(stream);/* wait for all operations on stream to complete */

MPIX_Free_queue(queue);
hipStreamDestroy(stream);

Fig. 7. Example for batched ST communication operation. Illustrate using a single MPIX Enqueue start operation to trigger the execution of multiple ST
send and receive operations.

points between an application process and its GPU device. The
summary of the non-blocking semantics of the new proposed
operations is as follows.

1) All ST enqueue operations return after enqueuing them
into the MPIX Queue object. The execution of these
operations is deferred until the GPU CP triggers the
execution using the enqueued writeValue operation as
part of the MPIX Enqueue start.

2) Once enqueued,

the GPU device kernels can make
changes to these buffers until the execution of the GPU
stream memory write operations in stream order.

3) An application process can synchronize with the GPU
device to identify the completion of previously enqueued
stream operations. This is in addition to the waitValue
operation enqueued as part of the MPIX Enqueue wait
operation. When a process enqueues a waitValue oper-
ation, it is not synchronizing with the GPU. Instead a
wait command is enqueued that gets executed by the
GPU CP.

4) An application process can also call MPI Wait or
MPI Waitall to ensure the completion of ST commu-
nication operations and perform any related cleanup
activities. These host-based synchronization operations
have blocking semantics for an application process.

C. ST Usage Model

This section provides a simple example in Fig. 7 to show
the usage of the proposed ST operations. In this exam-
ple, each process creates an MPIX Queue object on a
given GPU device stream. The MPI process with my rank
value 0 in MPI COMM WORLD DUP ﬁrst launches a com-
pute kernel
launch opera-
to the GPU. Since the kernel
this process can return immediately
tion is non-blocking,
to enqueue further stream-based send operations to the
MPIX Queue and the corresponding GPU stream objects.
This process next calls MPIX Enqueue start, which appends
the writeValue to the corresponding GPU stream. Finally,
this process calls MPIX Enqueue wait, which appends a
waitValue to the GPU stream. Similarly, the MPI process
with my rank value 1 on MPI COMM WORLD DUP per-
forms matching receive operations on the same communicator
by calling MPIX Enqueue recv, MPIX Enqueue start, and
MPIX Enqueue wait operations on the speciﬁed GPU stream.
In this example, for the MPI process with my rank value
0, the enqueued stream-based sends are guaranteed to be exe-
cuted only after the completion of the device kernel execution.
The host process enqueues the kernel and the stream-based
sends and start operations on a given stream handle and returns

immediately. It is the GPU CP that executes these operations
in FIFO order.

Similarly, for the MPI process with my rank value 1, the
enqueued wait operation guarantees that the device kernel is
not executed until the enqueued stream-based receive oper-
ations have completed execution. This also ensures that the
dst buf memory regions contain the result of the stream-based
receive operations for the subsequent GPU kernel enqueued for
execution.

D. ST Extended Usage

This section enumerates the extended usage semantics of the
proposed ST operations:

1) The proposed MPI enqueue operations are fully com-
patible with existing MPI P2P communication oper-
ations. For example, as long as the message match-
ing requirements are satisﬁed,
the proposed API al-
lows the use of existing MPI Irecv along with the
MPIX Enqueue send.

2) The proposed ST operations do not support wildcarding.
Usage of MPI ANY SOURCE and MPI ANY TAG is
restricted.

IV. IMPLEMENTATION DETAILS

This section provides a brief overview of the MPI imple-
mentation details in supporting the proposed ST-based MPI
communication operations. Implementation details are divided
into two different sections, providing an overview of the inter-
node and intra-node implementations.

A. Inter-node ST MPI Implementation

Inter-node ST send operations are executed using the triggered
operations supported by Slingshot 11. This feature is exposed
using the DWQ features supported in Libfabric interface.

When the MPIX Queue object

is created using the
MPIX Create queue operation, each MPI process opens two
Libfabric counters that are associated with hardware coun-
ters in the Slingshot 11 NIC. This implementation uses one
hardware counter as the trigger counter and the other as
the completion counter for all ST-based send and receive
operations enqueued on this MPIX Queue object.

1) Inter-node ST MPI Send Implementation:
During
MPIX Enqueue send operation, a DWQ-based send operation
with the hardware counters associated with the corresponding
MPIX Queue object is created. Appropriate trigger threshold
values are set per enqueued send operation. As per the sup-
ported semantics of the DWQ operations, these enqueued send
operations are not executed immediately. They wait for the
trigger condition on the associated hardware trigger counter
to be satisﬁed.

With MPIX Enqueue start operation, a trigger event using
the writeValue operation is created to satisfy the trigger
condition. The write operation to the associated hardware
counter from the writeValue operation acts as a trigger to any
previously enqueued triggered operations.

2) Inter-node ST MPI Receive Implementation:
The
implementation of the MPIX Enqueue recv operation is simi-
lar to the MPIX Enqueue send operation. But the main differ-
ence is that, as the Slingshot 11 NIC does not provide support
for triggered receives, the deferred execution semantics of the
triggered receives are emulated using a asynchronous progress
thread running on the CPU. The progress thread monitors
the associated hardware trigger and completion counter, and
interprets their state to decide when to execute the enqueued
receive operation and wait for its completion.

While the inter-node send operations are fully ofﬂoaded to
the network, the deferred execution semantics of the inter-node
receive operations are emulated using a progress thread.

B. Intra-node ST MPI Implementation

For both intra-node ST-based send and receive operations,
there are no known peer-to-peer options available to provide
the required deferred work semantics (speciﬁcally the MPI
messaging matching [22, 25, 19, 21] required as part of the
message passing semantics). As mentioned in Section IV-A2,
where the inter-node ST-based receive operations are emulated
using a progress thread, all intra-node ST-based send and
receive operations are also emulated using the same progress
thread.

It is to note that the described implementation is possible,
as the proposed ST operations do not support wildcarding
with MPI ANY SOURCE and MPI ANY TAG options. With
this semantic, both inter-node and intra-node ST trafﬁc are
easily separable. And, the progress thread-based emulation
would allow the asynchronous progress thread to get involved
in the message matching and data movement operations by
monitoring the memory buffer updated by the stream memory
operations mentioned in Section II-D. While the progress
thread handles the control path for the intra-node operations,
the GPU vendor supported DMA engines are used for the data
paths of these operations.

V. PERFORMANCE ANALYSIS

In this section, the Faces microbenchmark is used to eval-
uate the performance of the proposed ST-based MPI P2P
communication operations.

A. Application Overview

The Faces microbenchmark is based on the nearest-neighbor
communication pattern from the CORAL-2 [1] Nekbone [6]
benchmark. The timed loop in Faces performs the following
operations in the baseline HIP implementation.

1) Pre-post non-blocking MPI receives from up to 26

neighbors;

2) Launch kernels to copy into contiguous MPI buffers
from faces, edges, and corners of spectral elements on
the surface of the 3D local block, all in GPU memory;

3) Initiate non-blocking MPI sends to all neighbors;
4) Launch a kernel to perform the sum operation of all
faces of spectral elements that are inside the local block;

5) Wait to receive messages from neighbors; and

6) Launch kernels to add the received messages to the
faces, edges, and corners of spectral elements on the
surface of the local block.

Faces has three nested loops that perform the following

Fig. 8 represents the results of the analysis. It shows the
average, minimum, and maximum overall execution time of
the Faces microbenchmark, comparing the baseline version
against the modiﬁed ST variant.

operations.

1) Outer loop: Allocate MPI buffers, run loops, and de-

allocate MPI buffers;

2) Middle loop: Initialize the values of the spectral elements

and run the inner loop; and

3) Inner loop: Run the communication steps listed above

and accumulate the wall-clock runtime.

Faces conﬁrms correct results by comparing against a refer-
ence CPU-only implementation.

B. Test Details

For this ST performance analysis, the baseline HIP implemen-
tation of the Faces microbenchmark is compared against the
ST modiﬁed variant of the same. The baseline HIP imple-
mentation uses GPU-attached memory regions and attempts
to overlap MPI communication with computation for the inner
domain.

The ST implementation of Faces replaces MPI Isend
calls with MPIX Enqueue send
by
MPIX Enqueue start. These changes eliminate the need
for host-device synchronization before the sends. The
implementation also replaces
to
MPI Waitall for the sends with the host-asynchronous call
MPIX Enqueue wait.

synchronous

followed

calls

call

the

it

can use

For receive operations, Faces can use standard MPI Irecv
operations with appropriate buffer management techniques,
or
the proposed ST receive operations,
MPIX Enqueue recv. For this study,
the modiﬁed version
of Faces uses standard MPI Irecv operations with double
buffering techniques. This is an intentional implementation
choice because the current Slingshot 11 NIC does not support
deferred receive operations. Pre-posting the receive operations
eliminates the need for MPI’s progress threads to respond to
the trigger event (from hipStreamWriteValue64) and post the
receive descriptor to the NIC. Thus, the receive side of the
Faces implementation uses standard MPI Irecv calls.

Fig. 8. Faces execution time in seconds for the baseline and ST variants
of the benchmark (8 nodes with 8 MPI processes per node, 64 × 1 × 1 1D
conﬁguration).

On average, the ST variant of the Faces execution is around
10% slower than the baseline version. In the remainder of
the performance analysis sections, the performance impact
of the various components in the ST design is investigated.
Multiple intra-node and inter-node tests are used to understand
the various factors contributing to the performance overheads
measured in this multi-node analysis.

D. Impact of Progress Threads

To understand the impact of progress thread usage in the ST
implementation, the Faces microbenchmark is tested on a sin-
gle node. For this analysis, the 1D distribution of application
processes is 8 × 1 × 1.

All eight MPI ranks are collocated on a single node, with
each MPI rank using a different GPU on the node. Fig. 9
represents the results of the analysis.

For all tests, the following loop conﬁgurations were used:
10 outer loop × 100 middle loop × 100 inner loop. 5 different
runs of the different variants of the tests were performed, and
the average of the results are reported in this analysis.

C. Initial Performance Evaluation

Fig. 9. Faces execution time in seconds for the baseline and ST variants
of the benchmark (1 node with 8 MPI processes per node, 8 × 1 × 1 1D
conﬁguration).

The results of 1D Faces runs using the baseline and ST variants
are reported in this section. A 64×1×1 (1D) MPI distribution
is used for the analysis. This test uses 64 application processes
(MPI ranks) distributed across 8 heterogeneous nodes, with 8
ranks per node. On each node, a one-to-one mapping between
MPI rank and GPU devices is enforced. On each node, 8 GPU
devices (4 sockets) are attached to a single CPU. The hetero-
geneous node architecture is similar to the node architecture
available in the Frontier exascale supercomputer [4].

With the 8×1×1 1D conﬁguration and all ranks placed on a
single node, MPI messaging operations for the baseline version
involve the use of ROCr [9] (user-mode API interfaces and
libraries necessary for the host applications to launch compute
kernels on AMD GPU devices) IPC (Inter-Process Commu-
nication) for large payloads and a non-temporal memcpy
implementation for small payloads. While the data transfers
for the ST variant also incur similar intra-node transfers, a
progress thread is employed to perform these operations to

satisfy the ST deferred execution semantics. The details of
the progress thread usage in the ST implementation design
are described in Section IV-B.

On average, the ST variant of the Faces execution is around
4% slower than the baseline version. This is observed even
with a dedicated hardware thread on the AMD CPU for each
MPI progress thread. This demonstrates the negative impact
of using a progress thread per MPI process to emulate the
triggered operation semantics to implement the ST semantics.

E. Impact of Network Ofﬂoad

To understand the impact of using HPE Slingshot 11 hardware-
based deferred execution operations in the ST implementation,
the Faces microbenchmark is tested across 8 nodes with 1 MPI
rank per node. For this analysis, the 1D distribution of the
ranks is 8 × 1 × 1. On each node, a given MPI rank uses a
single GPU device, and the Slingshot 11 NIC is co-located on
the same GPU module. Fig. 10 represents the results of this
analysis.

Fig. 10. Faces execution time in seconds for the baseline and ST variants
of the benchmark (8 nodes with 1 MPI processes per node, 8 × 1 × 1 1D
conﬁguration).

With the 8 × 1 × 1 1D conﬁguration and a single MPI
rank per node, all MPI messaging operations for the base-
line version involve inter-node data transfers through the
HPE Slingshot-11 network. While the data transfers for the
ST variant also incur similar inter-node data transfers, HPE
Slingshot 11 hardware-based deferred send operations and
hardware counters are employed to perform these operations
to satisfy the ST semantics. The details of the implementation
are provided in Section IV-A1.

In this test, on average the ST variant shows similar
performance to the baseline Faces variant. This shows that
ofﬂoading signiﬁcant components of the ST operations to
the HPE Slingshot 11 NIC offers better performance when
compared to the use of a progress thread per MPI process
to implement the intra-node ST operations. To conﬁrm this,
the Faces benchmark is run with a 2 × 2 × 2 3D distribution.
The 3D distribution increases the number of MPI messages
generated per rank and the number of target ranks per process.
The number of MPI ranks and the number of nodes used for
this test stays the same; only the Faces distribution is changed
for this analysis. The results of this analysis are shown in
Fig. 11.

Fig. 11. Faces execution time in seconds for the baseline and ST variants
of the benchmarks (8 nodes with 1 MPI processes per node, 2 × 2 × 2 3D
conﬁguration).

With the change in Faces distribution,

the performance
clearly shows the beneﬁt of using NIC ofﬂoading for the
ST implementation. On average, the ST variant of the Faces
execution as shown in Fig. 10 is around 4% better than the
baseline version. This experiment demonstrates the beneﬁts
of using hardware-based deferred execution operations in the
HPE Slingshot 11 NIC to implement ST operations.

Even though the HPE Slingshot 11 NIC offers hardware
support for deferred execution operations and completion
counters, there are scenarios where the MPI implementation
relies on the progress thread for inter-node ST operations.
However,
the number of CPU cycles required to mitigate
some of the missing hardware capabilities is much smaller
when compared to those needed for the intra-node ST imple-
mentation. Speciﬁcally, CPU cycles are needed for emulating
deferred receives and for handling completion counter up-
dates in speciﬁc inter-node scenarios, such as the rendezvous
protocol. The NIC handles the entire progression of the
rendezvous protocol and the data movement operations. In
summary, hardware capabilities in the HPE Slingshot 11 NIC
can offer performance beneﬁts for inter-node ST operations,
even though the current implementation requires CPU cycles
for some scenarios.

F. Impact of Tuned Stream Memory Operations

In this test, the impact of using HIP stream memory operations
as described in Section II-D is analyzed. For this test, the
2 × 2 × 2 3D distribution of Faces execution as described in
Section V-E is extended.

Here 8 MPI ranks are distributed across 8 nodes, with 1 MPI
rank per node and each rank uses a single GPU. The 2 × 2 × 2
3D distribution is employed to show the real beneﬁts of the
ST implementation.

As described in Section V-E, the ST implementation that
uses the HIP stream memory operation (hipStreamWrite-
Value64 and hipStreamWaitValue64) shows 4% better per-
formance when compared to the baseline version. The per-
formance improvements are attributed to the use of HPE
Slingshot 11 hardware-based deferred execution operations in
the ST inter-node data transfer implementation.

For

this test,

the HIP stream memory operations are
swapped with hand-coded shaders that perform a set of oper-

ations to satisfy the semantics of the hipStreamWriteValue64
and hipStreamWaitValue64 operations. The ST-shader variants
of Faces show the best performance when compared to the
baseline, as well as the regular ST variant with HIP stream
memory operations. The results of this analysis are shown in
Fig. 12. The ST-shader version shows 8% better performance
than the baseline version.

Fig. 12. Faces execution time in seconds for the baseline, ST variant with HIP
stream memory operations, and ST implementation with hand-coded shader
kernels (8 nodes with 1 MPI rank per node using 2 × 2 × 2 3D conﬁguration).

This analysis shows the need for further tuning the HIP
stream memory operations to suite the need for ST MPI
operations.

G. Performance Inference

In brief, performance analysis comparing the baseline and
ST variants of the Faces microbenchmark determines the
following.

1) On average, experiments with the ST variant of Faces
with multiple nodes and multiple processes per node
show a signiﬁcant drop in performance when compared
to the baseline variant of Faces. Speciﬁcally, an 8-node
experiment with 8 MPI processes per node experiment
shows a 10% drop in overall performance when using
the ST implementation. This drop in performance is
attributed to the use of progress threads to emulate the
deferred execution semantics needed for intra-node data
transfers.

2) Running the ST variant of the Faces benchmark across
multiple nodes with a single application process per
node shows performance improvements when compared
to the baseline variant of Faces. Speciﬁcally, an 8-node
experiment with the ST version of Faces with one MPI
rank per node shows 4% improvement in overall Faces
execution time. This performance improvement can be
attributed to the use of hardware capabilities in the HPE
Slingshot 11 NIC in the ST implementation.

3) While the performance beneﬁt

from exploiting the
hardware-based deferred execution is an interesting data
point, not all applications can limit the amount of intra-
node communication to mimic the layout that provides
performance beneﬁts. For the baseline implementation,
nearest-neighbor codes beneﬁt from rank ordering that
puts communicating neighbors on the same compute

nodes. But for ST, a rank order that keeps neighbors on
separate nodes shows a greater improvement over the
standard implementation, but the absolute performance
relative to node-localized orders is likely lower. Further
study of this space may be appropriate.

4) The 4% performance improvement while using HPE
Slingshot 11 deferred execution operations can be im-
proved to 8% by employing hand-coded shader ker-
nels replacing the HIP stream memory operations, hip-
StreamWriteValue64 and hipStreamWaitValue64. This
shows the need for further performance tuning of tar-
geted stream memory operations.

VI. RELATED WORK

To the best of our knowledge,

little research has been
performed to explore options for introducing GPU stream-
awareness into message-passing programming models with
deferred execution semantics [16]. The triggered operations
functionality supported by the HPE Slingshot NIC (Slingshot
11) is unique in allowing users to enqueue communication
events into the NIC command queue but defer their execution
until a trigger event is observed. Based on our understanding,
this is the ﬁrst research to explore the usage of a deferred
to introduce GPU stream-
execution programming model
awareness in MPI.

There is other related research exploring different hardware
options to introduce GPU stream-awareness in different pro-
gramming models. Agostini et al. explores the basic building
blocks required for ofﬂoading communication control logic in
GPU accelerated applications into the GPU device. Similarly
to our research, they have explored hardware ofﬂoad capa-
bilities using NVIDIA GPUDirect Async [8] and InﬁniBand
Connect-IB network adapters[11]. The libgdsync[7] library
was created as part of this research.

Oden et al. and Daoud et al. explore the possibilities of
GPUs handling the communication and control paths by host-
ing the verbs layer [11] in the GPU. Similarly, Hsu et al. and
Hamidouche and LeBeane explore the options to host the GPU
centric communication on the GPU using the OpenSHMEM
programming model[13]. This research [28, 17, 26, 24] is
mostly done using the NVIDIA Inﬁniband Host Channel
Adapter.

We conclude that the proposed GPU stream-aware MPI
operations with deferred execution semantics have a potential
for performance beneﬁts. Speciﬁcally, the use of hardware
ofﬂoaded deferred execution operations can beneﬁt in imple-
menting the proposed MPI operations. The missing peer-to-
peer intra-node data transfer support needs similar ofﬂoad
mechanism for exploiting full performance beneﬁt from the
proposed MPI operations.

VII. CONCLUSION

In this work, a new communication strategy called stream-
triggered communication is proposed to introduce GPU
stream-awareness in MPI 2-sided point-to-point communica-
tion operations. The proposed strategy allows an application

to ofﬂoad both the control and data paths to the underlying
implementation and hardware components, and it avoids an
active CPU-GPU synchronization at the GPU compute kernel
boundaries.

The implementation of the proposed ST-based MPI op-
erations shows that ofﬂoading the inter-node ST-based MPI
operations using the triggered operations feature supported in
NICs like HPE Slingshot 11 has shown performance beneﬁts.
Conversely, the usage of progress threads for the intra-node
communication to emulate the required deferred execution se-
mantics is detrimental to performance. With available systems,
it is not yet possible for us to efﬁciently implement intra-
node two-sided MPI operations with the ST semantics without
a progress thread. The intra-node implementation requires a
progress thread per MPI process to perform the MPI message
matching for intra-node communication operations.

Further analysis is required to identify options to fully of-
ﬂoad the ST communication semantics to the NIC hardware to
get the maximum performance beneﬁts from new interfaces.

VIII. ACKNOWLEDGMENT AND DISCLAIMER

We would like to thank the HPE MPI (Steve Oyanagi and
Arm Patinyasakdikul) and HPE Libfabric (Ian Ziemba and
Charles Fossen) developers involved in providing the basic
features required to implement and evaluate the proposed MPI
stream-aware communication strategy. Also, we would like to
thank Duncan Roweth and Keith Underwood for reviewing
our work and providing suggestions for further improvements.
Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reﬂect the views of associated organizations.

REFERENCES

[1] CORAL-2 Benchmarks Summary.

https://asc.llnl.gov/

coral-2-benchmarks.

[2] CUDA Stream Memory Operations.

https:

//docs.nvidia.com/cuda/cuda-driver-api/group CUDA
MEMOP.html.

[3] Libfabric Deferred Work Queue. https://oﬁwg.github.io/

libfabric/v1.9.1/man/ﬁ trigger.3.html.

[4] Frontier, ORNL’s Exascale Supercomputer. https://www.

olcf.ornl.gov/frontier/.

[5] HIP

Stream Memory Operations.
//github.com/ROCm-Developer-Tools/HIP/blob/
develop/docs/markdown/hip programming guide.
md#hip-stream-memory-operations.

[6] CORAL-2

Benchmarks

Summary

- Nekbone.

https://asc.llnl.gov/sites/asc/ﬁles/2020-06/Nekbone
Summary v2.3.4.1.pdf.

[7] NVIDIA GPUDirect
gpudirect/libgdsync, .

libgdsync.

https://github.com/

[8] NVIDIA GPUDirect family.

https://developer.nvidia.

com/gpudirect, .

[9] AMD

ROCr-Runtime Manpage

and

Guide.

https://rocmdocs.amd.com/en/latest/Installation Guide/
ROCR-Runtime.html.

[10] HPE Slingshot Interconnect. https://www.hpe.com/in/en/

compute/hpc/slingshot-interconnect.html.

[11] IB Verbs RDMA programming guide. https://docs.nvidia.
com/networking/display/RDMAAwareProgrammingv17/
RDMA+Aware+Networks+Programming+User+Manual.
[12] NVIDIA CUDA C/C++ Streams and Concurrency.
http://on-demand.gputechconf.com/gtc-express/2011/
presentations/StreamsAndConcurrencyWebinar.pdf,
2013.

[13] OpenSHMEM standard version-1.4. http://openshmem.
org/site/sites/default/site ﬁles/OpenSHMEM-1.4.pdf,
2017.

[14] Cray’s Slingshot Interconnect is at the Heart of HPE’s
https://tinyurl.com/22rt7utz,

HPC and AI Ambitions.
2022.

[15] E. Agostini, D. Rossetti, and S. Potluri. Ofﬂoading Com-
munication Control Logic in GPU Accelerated Applica-
tions. In 2017 17th IEEE/ACM International Symposium
on Cluster, Cloud and Grid Computing (CCGRID), pages
248–257, 2017. doi: 10.1109/CCGRID.2017.29.

[16] B. W. Barrett, R. Brightwell, K. S. Hemmert, K. B.
Wheeler, and K. D. Underwood. Using Triggered Oper-
ations to Ofﬂoad Rendezvous Messages. In Proceedings
of the 18th European MPI Users’ Group Conference
on Recent Advances in the Message Passing Interface,
EuroMPI’11, 2011.

[17] F. Daoud, A. Watad, and M. Silberstein. GPUrdma:
GPU-Side Library for High Performance Networking
from GPU Kernels. In Proceedings of the 6th Interna-
tional Workshop on Runtime and Operating Systems for
Supercomputers, ROSS ’16. Association for Computing
Machinery, 2016. ISBN 9781450343879.

[18] D. De Sensi, S. Di Girolamo, K. H. McMahon,
D. Roweth, and T. Hoeﬂer. An In-Depth Analysis of
the Slingshot Interconnect. In SC20: International Con-
ference for High Performance Computing, Networking,
Storage and Analysis, 2020.

[19] K. Ferreira, R. E. Grant, M. J. Levenhagen, S. Levy, and
T. Groves. Hardware MPI message matching: Insights
into MPI matching behavior to inform design: Hardware
MPI message matching. Concurrency and Computation.
Practice and Experience, 32, 2019.

dard. Technical report, 1994.

[21] S. M. Ghazimirsaeed, R. E. Grant, and A. Afsahi. A
Dedicated Message Matching Mechanism for Collective
In Proceedings of the 47th Interna-
Communications.
tional Conference on Parallel Processing Companion,
ICPP ’18. Association for Computing Machinery, 2018.
[22] T. Groves, N. Ravichandrasekaran, B. Cook, N. Keen,
D. Trebotich, N. J. Wright, B. Alverson, D. Roweth, and
K. Underwood. Not all applications have boring com-
munication patterns: Proﬁling message matching with
BMM. Concurrency and Computation: Practice and
Experience, jun 2021. URL https://doi.org/10.1002%
2Fcpe.6380.

https:

[20] M. P. Forum. MPI: A Message-Passing Interface Stan-

[23] P. Grun, S. Hefty, S. Sur, D. Goodell, R. D. Russell,
H. Pritchard, and J. M. Squyres. A Brief Introduction
to the OpenFabrics Interfaces - A New Network API for
Maximizing High Performance Application Efﬁciency.
Aug 2015.

[24] K. Hamidouche and M. LeBeane.

GPU INitiated
OPenSHMEM: Correct and Efﬁcient Intra-Kernel Net-
working for DGPUs. Association for Computing Ma-
chinery, 2020.

[25] K. S. Hemmert, K. D. Underwood, and A. Rodrigues.
An architecture to perform NIC based MPI matching. In
2007 IEEE International Conference on Cluster Comput-
ing, 2007. doi: 10.1109/CLUSTR.2007.4629234.
[26] C.-H. Hsu, N. Imam, A. Langer, S. Potluri, and C. J.
Newburn. An Initial Assessment of NVSHMEM for High
In 2020 IEEE International
Performance Computing.
Parallel and Distributed Processing Symposium Work-
shops (IPDPSW), 2020. doi: 10.1109/IPDPSW50202.
2020.00104.

[27] K. V. Manian, A. A. Ammar, A. Ruhela, C.-H. Chu,
H. Subramoni, and D. K. Panda. Characterizing CUDA
Uniﬁed Memory (UM)-Aware MPI Designs on Modern

GPU Architectures. In Proceedings of the 12th Workshop
on General Purpose Processing Using GPUs, GPGPU
’19, 2019.

[28] L. Oden, H. Fr¨oning, and F.-J. Pfreundt. Inﬁniband-Verbs
on GPU: A Case Study of Controlling an Inﬁniband Net-
work Device from the GPU. In 2014 IEEE International
Parallel Distributed Processing Symposium Workshops,
2014. doi: 10.1109/IPDPSW.2014.111.

[29] S. Potluri, K. Hamidouche, A. Venkatesh, D. Bureddy,
and D. K. Panda. Efﬁcient Inter-node MPI Communi-
cation Using GPUDirect RDMA for InﬁniBand Clusters
with NVIDIA GPUs. In 2013 42nd International Con-
ference on Parallel Processing, 2013.

[30] H. Wang, S. Potluri, M. Luo, A. K. Singh, S. Sur, and
D. K. Panda. MVAPICH2-GPU: optimized GPU to
GPU communication for InﬁniBand clusters. Computer
Science - Research and Development, 26, 2011.

[31] H. Wang, S. Potluri, D. Bureddy, C. Rosales, and D. K.
Panda. GPU-Aware MPI on RDMA-Enabled Clusters:
Design, Implementation and Evaluation. IEEE Transac-
tions on Parallel and Distributed Systems, 25, 2014.

