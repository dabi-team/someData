Evaluating the Impact of Source Code Parsers on ML4SE Models

Ilya Utkin
utkin.ilya.nickolaevich@gmail.com
Saint Petersburg State University

Egor Bogomolov
egor.bogomolov@jetbrains.com
JetBrains Research

Egor Spirin
spirin.egor@gmail.com
JetBrains Research

Timofey Bryksin
timofey.bryksin@jetbrains.com
JetBrains Research

2
2
0
2

n
u
J

7
1

]
E
S
.
s
c
[

1
v
3
1
7
8
0
.
6
0
2
2
:
v
i
X
r
a

ABSTRACT
As researchers and practitioners apply Machine Learning to in-
creasingly more software engineering problems, the approaches
they use become more sophisticated. A lot of modern approaches
utilize internal code structure in the form of an abstract syntax tree
(AST) or its extensions: path-based representation, complex graph
combining AST with additional edges. Even though the process
of extracting ASTs from code can be done with different parsers,
the impact of choosing a parser on the final model quality remains
unstudied. Moreover, researchers often omit the exact details of
extracting particular code representations.

In this work, we evaluate two models, namely Code2Seq and
TreeLSTM, in the method name prediction task backed by eight
different parsers for the Java language. To unify the process of
data preparation with different parsers, we develop SuperParser, a
multi-language parser-agnostic library based on PathMiner. Super-
Parser facilitates the end-to-end creation of datasets suitable for
training and evaluation of ML models that work with structural
information from source code. Our results demonstrate that trees
built by different parsers vary in their structure and content. We
then analyze how this diversity affects the models‚Äô quality and show
that the quality gap between the most and least suitable parsers for
both models turns out to be significant. Finally, we discuss other
features of the parsers that researchers and practitioners should
take into account when selecting a parser along with the impact
on the models‚Äô quality.

The code of SuperParser is publicly available at https://doi.org/
10.5281/zenodo.6366591. We also publish Java-norm, the dataset we
use to evaluate the models: https://doi.org/10.5281/zenodo.6366599.

ACM Reference Format:
Ilya Utkin, Egor Spirin, Egor Bogomolov, and Timofey Bryksin. 2022. Evalu-
ating the Impact of Source Code Parsers on ML4SE Models. In Proceedings
of ACM Conference (Conference‚Äô17). ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference‚Äô17, July 2017, Washington, DC, USA
¬© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
The field of applying machine learning (ML) algorithms to source
code rapidly grows [1]. Earlier ML models represented code as a
set of explicitly defined metrics or as plain text. The latter makes it
possible to reuse approaches from the Natural Language Process-
ing (NLP) domain [2]. While any file with source code can indeed
be seen as a text document, source code has a richer underlying
structure compared to natural languages. Nowadays, there are in-
creasingly more works that take this underlying code structure into
account [3‚Äì5].

Every programming language has its own strict syntax that can
be represented as a set of predefined rules determining all possible
language constructions. Parsing the raw source code with these
rules allows representing it in the form of a parse tree, and then as
an abstract syntax tree (AST). By working with this structure and
also with its possible extensions (e.g., path-based representation [6],
graph-based representation [7]) researchers improve results for
various software engineering tasks: code summarization [3], type
inference [4], bug detection and repair [8], malware detection [9].
Aside from the ML domain, the representation of code as an AST
is crucial for its analysis and execution: it is used by compilers, static
analysis tools, and integrated development environments (IDEs).
As a result, there exist a lot of different parsers for building ASTs
from source code. Most parsers are tailored for a specific problem
and have their own features. For example, Tree-sitter [10] aims
to work with incremental parsing, ANTLR [11] makes it easier to
create new languages by providing parser generators, and so on. All
the parsers have different use-cases and require different levels of
information from code, therefore, they might build different ASTs.
Even though for most languages there are many available parsers,
not all the research works that use information from ASTs to train
ML models provide enough details on how they build syntax trees.
Machine learning models are very sensitive to the input data, and
as different parsers build ASTs in a different manner, the choice of
a parser might affect the model‚Äôs quality. In turn, it might lead to
unreproducible results or even incorrect model comparison if the
models use different parsers.

In this work, we investigate whether the selection of a source
code parser affects the final quality of ML models. We studied
the impact of the parser choice on two popular models that work
with ASTs in a different manner: TreeLSTM [12] and Code2Seq [3].
TreeLSTM is an extension of the regular LSTM [13] network that
was adapted to work with tree-structured data. Although it falls
behind in quality compared to modern models, it is useful to study
since it utilizes trees without further modifications. On the other

 
 
 
 
 
 
Conference‚Äô17, July 2017, Washington, DC, USA

Ilya Utkin, Egor Spirin, Egor Bogomolov, and Timofey Bryksin

hand, Code2Seq is a popular approach that was reused in various
tasks apart from the original method name prediction [14, 15].
Code2Seq employs path-based representation [6] to work with
code, which also relies on the information from the code‚Äôs AST.

As a comparison benchmark, we use the method name predic-
tion task for the Java programming language. In addition to its
practical applicability, method name prediction suits as a popular
benchmark in the ML for SE domain [3, 5, 16], as it allows evaluat-
ing how good models are in code understanding. We compare both
models backed by eight different Java parsers. In order to do so, we
develop SuperParser, a tool based on PathMiner [17]. SuperParser
supports mining code representations suitable for ML models from
five programming languages and twelve language-parser pairs in a
unified manner.

Our results suggest that parsers can significantly affect the final
quality of model predictions. Proper parser choice for the Code2Seq
model may increase the quality of the model by up to 5.5% in terms
of F1-score compared to the least suitable parser. For a simpler
TreeLSTM model, the parser selection is even more crucial, with
the difference between the most and least suitable parsers being
about 27.0%. Notably, for TreeLSTM and Code2Seq, the relative
ordering of parser is different, meaning there are no universally
good or bad parsers. Therefore, when choosing a parser, researchers
and practitioners should take into account the model‚Äôs properties
and characteristics of trees produced by different parsers.
With this work, we make the following contributions:

‚Ä¢ We investigate the impact of the parser selection step on
the quality of machine learning models working with struc-
tural representations of code. To the best of our knowledge,
this work is the first to study this part of the data prepro-
cessing pipeline. We show that depending on the specific
parser used, results of both TreeLSTM and Code2Seq can
vary significantly.

‚Ä¢ We present SuperParser, a tool based on PathMiner, that al-
lows running multiple parsers for different programming
languages in the same manner. SuperParser allows running
experiments with different parsers by changing a single field
in a YAML configuration file. Our tool separates the parsing
step, allowing further data processing steps (e.g., filtering,
storage) to be parser-agnostic. It makes support of new lan-
guages, parsers, and mining tasks easy for the users.

‚Ä¢ We publish a dataset of open-source Java projects called Java-
norm. It is comparable in size to the commonly used Java-
small dataset, but its validation and testing parts include
more diverse projects making evaluation more robust and
representative.

2 BACKGROUND
In our work, we analyze the impact of choosing a parser on the
quality of ML4SE models. In this section, we first describe the main
concepts of AST and then overview its applications in the ML4SE
domain.

2.1 Abstract Syntax Tree
Each programming language has a set of rules‚Äîits grammar‚Äîthat
describes the language syntax. Rules can be viewed as transfor-
mations from higher-level language abstractions into sequences
of simpler abstractions and code tokens (e.g., literals and variable
names). Figure 1a shows several examples of grammar rules for
method declarations in Java.

The result of parsing source code with the selected grammar
is a parse tree. Each intermediate node in a parse tree represents
the left-hand side of some rule, and the node‚Äôs children correspond
to the sequence on the right-hand side of the same rule. Thus,
intermediate nodes represent abstractions defined by the grammar
and the leaves in the parse tree correspond to tokens that actually
appear in the source code. Figure 1c shows an example of a parse
tree built for code fragment shown in Figure 1b.

Parse trees can be quite verbose, so most parsers can further opti-
mize them, e.g., by removing unnecessary brackets that are already
reflected in the tree structure, or compressing sequences of nodes
into new nodes. After processing the parse tree, the parser con-
structs an abstract syntax tree (AST) that is usually more compact
than the initial tree.

Parsers differ by both the grammar they use and the
post-processing behaviour. Post-processing directly depends on
the parser‚Äôs application. For example, PSI [18] and JDT [19] are
used for language support in IDEs and, therefore, keep all informa-
tion about the source code including formatting. On the opposite,
JavaParser [20] aims to support complex modifications of Java code,
so it optimizes the tree by introducing more layers of abstraction.

2.2 AST-based Representations in ML Models
A straightforward approach to working with source code in ML
models is treating code as a plain text. This approach allows apply-
ing models from the NLP domain with close to none modifications
in various software engineering tasks, e.g., variable misuse, code
summarization. However, as previously discussed, source code has
a rich underlying structure that can be represented as an AST. Ex-
isting works in the ML4SE domain show that taking code structure
into account is indeed beneficial as it leads to significant improve-
ments in multiple SE tasks [3, 5, 16].

Models that directly support the tree-structured input format,
e.g., TreeLSTM, can use raw ASTs. While originally applied to
NLP tasks, the TreeLSTM model served as a baseline in multiple
ML4SE works [3, 21]. Shido et al [22] improved the model further
by handling an arbitrary number of ordered children via another
LSTM cell [13].

Another class of models builds their own code representations
based on ASTs rather than using the trees as is. Alon et al. [6]
suggested path-based representation of code. Path-based represen-
tation extracts a set of path contexts from each AST, where each
context is a triple of two code tokens in AST leaves and a sequence
of intermediate nodes that connect these leaves. Based on this repre-
sentation, the authors suggested models for different tasks [23, 24],
including the Code2Vec [25] and Code2Seq [3] models that show
high quality in method name prediction.

While AST represents the syntax of source code, a code fragment
can also be represented with a graph structure. Control-flow graph

Evaluating the Impact of Source Code Parsers on ML4SE Models

Conference‚Äô17, July 2017, Washington, DC, USA

(a) Examples of rules that define syntax for method dec-
laration.

(b) Java method code snippet example.

(c) An example of a parse tree built for the code snippet on the left. Blue nodes are
intermediate abstraction nodes. Green nodes contain actual code tokens.

Figure 1: An example of a code snippet, its correposnding parse tree, and a subset of grammar rules.

(CFG) contains all possible execution paths of a program. Data-flow
graph (DFG) is built from the dependencies between variables in
code and their usages. AST, DFG, and CFG can be combined into a
single complex graph, as nodes from CFG and DFG also appear in
the code‚Äôs AST. Allamanis et al. [7] used such graph representation
of code to achieve state-of-the-art results in the variable misuse
task. Lately, graph representation was used in the method name
prediction [16], type inference [4], and variable misuse [8] tasks to
achieve superb results.

AST-based representations of code are not limited to the ones
listed above. Existing works also operate with AST traversals [26],
apply convolutions over trees [27], enhance Transformer-based [28]
models with information from ASTs [5]. Despite the large number of
ML4SE works that employ ASTs to improve the results, they rarely
motivate the choice of parsers used for AST extraction. Oftentimes,
papers either omit the details about the parsing step, or just state
the used tool without motivating their choice. In our work, we
focus on analyzing the importance of the parser choice in ML4SE
and outlining things to consider when making it.

3 THE SUPERPARSER TOOL
In order to study the impact of the parser choice on the model‚Äôs
quality, we need a common way to extract input data for models
from the source code with different parsers. To achieve this, we
need to run various parsers in a unified manner. In order to ease the
maintenance and allow other researchers and practitioners to reuse
our data processing pipeline, we integrate all the studied parsers in
a single tool, SuperParser.

As a basis for building SuperParser, we use PathMiner [17] ‚Äî a
tool for mining ASTs and path-based representations from code
in several languages, filtering the data, and storing it in a format
suitable for training ML models. The main advantage of PathMiner
is that the tool is designed to be easily extendable. It provides a lot of

reusable components for building parsing pipelines, which then can
be used for data mining in ML4SE tasks. These components include
wrappers for parsers, classes for storing AST vertices, algorithms
for filtering, classes for storing the extracted trees.

By using the existing extension points, we create a pipeline that
supports parsers directly imported from Kotlin and Java, parsers
in other languages that run as separate processes, and grammars
for ANTLR and Tree-Sitter. SuperParser then runs the parsers to
prepare datasets for training and evaluating ML models in the
method name prediction task. Using the same pipeline, we run all
parsers under equal conditions and ensure reproducibility of our
experiments.

3.1 Multilanguage Parser-agnostic Processing
Parsers greatly vary depending on their purpose. A parser can be
generated from grammar or written from scratch. It can constitute
a library, or be a full-fledged runnable tool. The functionality of a
parser can range from straightforward construction of parse trees to
complex manipulations on source code. Finally, parsers can be writ-
ten in different programming languages depending on the target
applications. Due to the variability of parsers, supporting multiple
parsers in a single data preparation pipeline can be a daunting task.
In SuperParser, we reuse components of PathMiner and also add
new extension points in order to ease the tool‚Äôs usage for other
researchers and practitioners.

In contrast to PathMiner, SuperParser supports parsers written in
languages that do not run on Java Virtual Machine (JVM). In order
to add new languages and parsers, users of PathMiner can either
provide a grammar for the ANTLR parser generator or implement
an adapter class that wraps a third-party tool written in a JVM-
compatible language. However, there are many parsers‚Äîboth for
Java and other languages‚Äîthat do not provide Java bindings. In
order to handle such cases, SuperParser has a new entity called

Methodhead MethodMethodbody Resulttype Methoddecl Simpletype void Id fun ( ) Param list Single var Id Arraytype String Args Expression Methodinvocation Methodname System.out.println Param list Stringliteral Helloworld!Conference‚Äô17, July 2017, Washington, DC, USA

Ilya Utkin, Egor Spirin, Egor Bogomolov, and Timofey Bryksin

ForeignParser. It inherits from the Parser class of PathMiner
and can launch standalone tools implemented in other languages to
further convert their output into the inner tree structure. A similar
idea was used in GumTree[29], a tool that also aggregates multiple
parsers and uses them by deserializing their XML output. In our
implementation, we use JSON, as this format is more common
nowadays.

PathMiner could be easily extended to support new languages by
using automatically generated parsers based on ANTLR grammar
files. Following this idea, we also add the support of Tree-sitter [10]
grammars. Due to the ease of usage, the availability of Python
bindings, and the support of many programming languages, Tree-
Sitter is commonly being used in ML4SE data mining pipelines.

Thus, SuperParser currently supports:
(1) parsers that provide JVM bindings;
(2) parsers that can be run as stand-alone processes or wrapped

in executable programs with console output;

(3) parsers derived from ANTLR and Tree-sitter grammars.

Importantly, each of these methods has the same API and the work
of the parsers is completely hidden from the rest of the SuperParser
components. For now, our tool already supports 5 different lan-
guages with 12 parser/language pairs. While the parser support can
be extended, in this study we mainly focus on Java language as we
selected it for the evaluation of models.

3.2 Data-processing Pipeline
In SuperParser, we reuse and improve many components from Path-
Miner. We combine them into a single pipeline that can be launched
from the command line and configured by providing a YAML file.
Novel approaches often rely on large-scale data analysis, thus, we
significantly speed up the tool by adding multithreading process-
ing and provide an excessive logging system together with the
command-line interface, making it convenient to use SuperParser
on remote servers.

After the parsing stage, SuperParser generates a single tree or
a set of trees from each file depending on the selected granularity
(file or method). The trees have a unified parser-independent form
and can then be processed depending on the target task. In order
to make SuperParser easily extensible to new tasks and ML models,
the following processing pipeline consists of several steps which
can be configured and changed independently. All these steps are
represented by simple programming interfaces that could be imple-
mented to extend SuperParser with new behaviour. Along with the
source code of the tool, we also publish an extensive documentation
describing its main components.

Label extraction. Label extractors associate each tree with
some string (a label) which can then be used as a target for model
training and evaluation. This component of the pipeline can be
replaced depending on the task. SuperParser currently supports
extraction of method names, file names, and parent folder names.
Filtering. At the next stage, SuperParser applies filters to all the
extracted trees and labels. Label filters can be used to remove sam-
ples with the desired labels that cannot be supported by a model,
e.g., names in unsupported languages, too long or too short names.
Tree-level filters can be either structural or content-based. Struc-
tural filters remove samples based on the AST characteristics: depth,

number of vertices. Content-based filters operate with the tree it-
self and filter samples by annotations (e.g., removing overridden
methods or tests), modifiers (e.g., removing abstract methods), and
the size of the method‚Äôs body.

In order to avoid implementing a separate set of filters for each
parser, we accompany parsed trees with a set of features pre-
computed at the parsing stage. This set includes both external and
internal features. External features include names of the file and en-
closing elements (e.g., classes and enumerations). Internal features
are function parameters, its name, modifiers, annotations, and oth-
ers. These features are stored in a parser-independent form, which
allows users to implement new filters without manual analysis of
the parsed structure.

Storing. At the last stage of the pipeline, we convert the trees
to the selected format and finally save the dataset on disk. For
storing the data, we mostly reuse components of PathMiner, but
also add the ability to compress trees before storing. In addition,
we save metadata alongside with the mined dataset, such as the
origin of the tree (i.e., path to the enclosing file) or the location of
the corresponding code snippet in the file.

To compress trees, we use the algorithm presented by Yin et
al. [30]. The authors use production rule closures and concatenate
the names of the rules into a single vertex type. In order to make
the process parser-independent, we imitate this algorithm by com-
pressing ‚Äúbamboos‚Äù, i.e., sequences of nodes in a tree that have a
single child each, into single nodes in the already generated AST.

4 EXPERIMENTAL SETUP
4.1 Research Questions
With this study, we aim to investigate the importance of the initial
source code parsing step for the machine learning on source code al-
gorithms. Moreover, we focus on the differences of how source code
is parsed and how these differences affect the final performance of
ML models. We formulate the following research questions:

‚Ä¢ RQ1: Is there any statistical difference in the ASTs produced

by different parsers?

‚Ä¢ RQ2: How does the parser selection affect machine learning

models that take into account tree structure of code?

‚Ä¢ RQ3: Alongside with the model quality, what else researchers
and practitioners should take into account when choosing a
parser for their data-processing pipeline?

With RQ1, our goal is to understand whether different parsers

indeed lead to varying characteristics of model input.

By answering RQ2, we aim to understand whether the differ-
ences in trees produced by different parsers lead to changes in the
quality of machine learning models. If the answer is positive, fur-
ther research should pay attention to the choice of a parser in order
to maximize the performance of the studied models and to ensure
fair comparison.

In RQ3, we address practical features of code parsers, such as
their speed, size of the generated datasets, and language support.
These aspects become important when choosing between parsers
that allow models to achieve similar quality.

Evaluating the Impact of Source Code Parsers on ML4SE Models

Conference‚Äô17, July 2017, Washington, DC, USA

Figure 2: Overview of the data processing pipeline in SuperParser. Users can configure the pipeline to suit their needs by
providing a YAML configuration file. For example, user can configure the tool to do the following: (1) parse Java code with
ANTLR parser, (2) extract ASTs of all methods, (3) remove overriden methods, (4) associate each tree with method name, and
(5) store the resulting dataset in JSON Lines format.

4.2 Parsers
As a target programming language for this study, we use Java due to
its common adoption in the ML4SE domain. Java is a fairly common
target for syntactic and semantic analysis, as well as for research
works on code summarization [3, 16], code generation [31], and
others [23]. Also, due to the language‚Äôs popularity, long history
of development, and numerous applications, there exist a lot of
different parsers for Java.

There are many tools that convert source code into a tree struc-
ture. The main examples are interpreters and compilers that use
it to make the code executable. Other examples are refactoring
and code highlighting systems (e.g., PSI trees in the IntelliJ Plat-
form), algorithms for comparing source code [29], algorithms for
finding duplicates in code [32], and other tasks where program
structure is important. Due to the variety of applications, there are
many parsers for different scenarios. For this work, we selected the
following parsing tools and libraries:

ANTLR [11] ‚Äî a parser generator that takes a grammar for the
target language as input and generates a parser implementation in
one of the supported languages. In this work, we use ANTLR with
an open-source grammar for Java.1

Eclipse Java Development tools (JDT) [19] ‚Äî a set of plugins
for Eclipse IDE that are responsible for supporting Java. Based on
JDT, the IDE can highlight Java code, refactor it, perform differ-
ent checks, and more. We use GumTree [29] Java API to add JDT
support to SuperParser.

JavaLang [33] ‚Äî an open-source library written in pure Python
for working with the Java 8 source code. Due to the implementation
in Python, JavaLang can be easily integrated into ML and data
science pipelines. The library focuses on parsing and does not
provide tools for further manipulations with the extracted trees.

JavaParser [20] ‚Äî a Java library for parsing, transformation,
and generation of Java code. It allows analyzing Java programs,

generating boiler-plate code, and focuses on complex manipulations
with the generated ASTs. Notably, it has been used in some of the
previous ML4SE studies [3, 5, 16].

Program Structure Interface (PSI) [18] ‚Äî similar to JDT, it is
a tool that is responsible for the language support in IDEs built
upon the IntelliJ Platform. It allows manipulation of source code
inside the IDEs and their plugins, running inspections, refactorings,
and more. We use PSIMiner [34] to extract PSI trees from code.

Spoon [35] ‚Äî an open-source library for parsing, rewriting,
transforming, and transpiling Java code. In contrast to previous
tools, it was developed in the research community and since then
has been used in numerous works on refactoring and program
repair [36‚Äì38].

SrcML [39] ‚Äî a lightweight tool for analyzing and manipulating
source code. It allows transforming C, C++, and Java code by using
trees in a native SrcML format inspired by XML. The authors pro-
vide a set of tools for further manipulation of the extracted trees:
static type resolution, pointer analysis, builder of UML diagrams,
etc. To support this parser in SuperParser, we also use GumTree.
Tree-Sitter [10] ‚Äî a parser generator focused on the usage in
text editors and IDEs in order to provide features like syntax high-
lighting in real time. Due to the need for real-time usage, Tree-sitter
can parse code incrementally, is fast and robust when parsing un-
finished code. Similar to ANTLR, we use its most popular grammar
for Java 8.2

The chosen parsers target different applications. Each parser has
its own features: from being lightweight and fast, to supporting
complex manipulations of ASTs. This diversity allows us to assess
whether ASTs extracted with different parsers are similar or they
can affect the quality of ML models working with source code.

1Java 8 ANTLR grammar: https://github.com/antlr/grammars-v4/tree/master/java/
java8

2Java 8 Tree-sitter grammar: https://github.com/tree-sitter/tree-sitter-java

Build ASTs fromsource code, split bygranularity Associate labelwith each ASTSave ASTs intarget formatFilter ASTsParser 2 Parser n Parser 1 ... Lang 1 Lang 2 Projects' sourcesDatasetYAML config... Conference‚Äô17, July 2017, Washington, DC, USA

Ilya Utkin, Egor Spirin, Egor Bogomolov, and Timofey Bryksin

4.3 Models
In this work, we compare two different ML models, TreeLSTM [12]
and Code2Seq [3], that can solve software engineering problems by
working with tree-based representation of code or its extensions.
With this, we aim to make a diverse selection from the point of
model architecture and tree input format.

TreeLSTM. Originally created for natural language processing,
the TreeLSTM model was then reused and adapted for working
with source code [3]. Although this model is simpler and shows
inferior quality compared to the novel approaches, TreeLSTM works
directly with tree representation of code. This is crucial for our
research, since it allows comparing the impact of parsing differences
directly, without additional transformations of trees (i.e., adding
new edges in DFG or CFG [7]).

Code2Seq. On the other side, Code2Seq is a popular model
showing good results in many SE applications [3, 14, 15]. This
model receives information about source code from a path-based
representation, i.e., a set of contexts where each context is a triple
of tokens in two leaves in a tree and a sequence of intermediate
nodes on a path between them. As Code2Seq does not work directly
with ASTs, we can assess whether the choice of a parser can affect
the models that use code representations derived from the ASTs.

4.4 Dataset
In order to ensure correctness of the results and reduce random-
ness, we run each model five times with different random seeds for
each parser. Thus, train and evaluate models 80 times in total (2
models √ó 5 random seeds √ó 8 parsers). Due to the high number of
experiments, initially we decided to use the Java-small dataset [40].
In our case, using larger datasets such as Java-med and Java-large
is too computationally intensive as they are 5 to 30 times larger
than Java-small.

However, our experiments and previous experience with Java-
small suggest that evaluating the models on this dataset comes with
two important limitations:

(1) The models‚Äô quality reported on the validation data weakly
correlates with the results on the testing data and is often
way lower. It makes tuning of hyper-parameters based on
the validation quality unreliable.

(2) Model evaluation on Java-small is not robust, with small
model modifications leading to large deviations in the final
metrics. As a result, evaluation on this dataset often disagrees
with larger datasets in terms of relative model ranking.

Our investigation of these issues suggests that a significant dis-
advantage of Java-small is that both its validation and testing parts
consist of a single project each. Judging models quality by evaluat-
ing on a single project might lead to non-representative results since
every project is unique in some way. Moreover, the libgdx project
that constitutes the validation set is not a typical Java project, with
more than 45% of code written in C and C++, which might explain
the discrepancy between the validation and testing results.

In order to deal with this issue, we compile a new dataset called
Java-norm that has a size comparable to Java-small, but aims to mit-
igate its issues. Java-norm contains five projects in both validation
and testing parts (see Table 1 for the list of projects). The training
part of the new dataset includes ten open-source Java projects. As

the set of projects in both testing and validation is more diverse,
evaluation on Java-norm is more representative and robust.

We collect projects that are publicly available on GitHub.3 While
collecting the data, we respect all intellectual rights and select only
the projects whose licenses allow to reuse the code for further
analysis. We collect projects that have one of the following licenses:
Apache-2.0, MIT, or BSD-3.0. We consider projects that are not
forks and have at least 20MB of Java code. Across all projects that
meet these criteria, we take the ones with the highest numbers of
GitHub stars.

We make Java-norm publicly available via Zenodo.4 As it mit-
igates some issues of Java-small while being comparable in size,
we believe that the dataset can be useful for future research in the
method name prediction and other ML4SE tasks.

4.5 Evaluation Scheme
In the rest of this section, we describe the evaluation scheme we
use to evaluate the impact of parsers on models‚Äô quality and answer
the research questions.

Tree comparison. In order to verify whether trees built by
different parsers are similar or not, we compare them by several
metrics: tree depth, size, branching factor, etc. The values of a
certain metric for different trees produced by the same parser can
be viewed as a distribution of an unknown form. Therefore, to
compare trees produced by different parsers, we can compare the
distributions of their metrics.

There exist multiple approaches to compare numerical distri-
butions. One of the most popular is the Student‚Äôs t-test [41] that
tests the hypothesis of mean equality between Normal distribu-
tions. Although values of a certain metric for a single parser are
not normally distributed, with a dataset of 300 thousand trees, we
can apply the Central Limit Theorem and work with this test.

Method name prediction. As a benchmark for our study, we
select the method name prediction problem. More formally, the
model should generate a name of the method given its body and
signature with the original name being replaced with a stub token.
This task is a common benchmark for analyzing model capabilities
in code understanding and summarization [3, 5, 16].

As a de-facto standard, we treat the method name prediction task
as a sequence generation problem. In this setting, each method name
is split into a sequence of sub-tokens by CamelCase or snake_case.
Models then generate sub-tokens one by one, taking the code snip-
pet and already predicted sub-tokens into account.

Intersection of datasets. As different parsers use different un-
derlying algorithms, they might parse different sets of files or meth-
ods from the same code corpus. It might happen due to several
reasons: some parsers fail for specific files due to the presence of
unexpected symbols, syntactically incorrect or incomplete code,
unsupported versions of Java, etc. Also, processing of trees for each
parser requires significant manual parser-specific work (e.g., to
correctly split the trees into methods or extract modifiers) which
might fail in corner cases. As a result, the set of parsed ASTs from
the same initial code dataset slightly differs from parser to parser.

3GitHub: https://github.com/
4
Java-norm: https://doi.org/10.5281/zenodo.6366599

Evaluating the Impact of Source Code Parsers on ML4SE Models

Conference‚Äô17, July 2017, Washington, DC, USA

In order to correctly answer our research questions, the resulting
datasets for all the parsers must contain trees for exactly the same
set of methods. Otherwise, the diversity in the models‚Äô quality
may be attributed to the differences in the training and testing
sets. In order to mitigate this, we intersect datasets, i.e., we keep
only those methods that may be successfully parsed by each parser.
For this, we store a source range for each extracted method ‚Äî the
exact position where this method appears in the source code. Based
on the source ranges, we match methods extracted by different
parsers. In total, we dropped less than 1% of the methods after the
intersection, Table 1 provides information about the final dataset.

Training
cassandra
gocd
gradle
hbase
kafka
pentaho-kettle
presto
pulsar
spring-framework
tomcat

Validation Testing

buck

bazel

dl4j

dbeaver

druid

guava

flink

keycloak

j2objc

quarkus

295,425

187,717

144,656

Projects

Number of
methods

Table 1: Sets of projects and number of methods (after the
intersection step) in training, validation, and testing parts
of Java-norm.

Metrics. The most popular approach to measure quality of
method name prediction is F1-score [40]. Originally applied for
classification, it was adopted to evaluate the quality of generated
sequences. F1-score is a harmonic mean of precision and recall.
Given two sequences, ground truth and prediction, precision is a
proportion of sub-tokens from the predicted sequence that appear
in the ground truth. Recall, as an opposite, is a proportion of cor-
rect sub-tokens that appeared in the predicted sequence. Finally,
F1-score is a harmonic mean of these two values.

Although F1-score is a popular metric, it also has disadvan-
tages, e.g., it does not take into account the order of sub-tokens.
While for some method names (e.g., arraySort and sortArray) it
does not affect the meaning, in other cases it might alter it (e.g.,
implementationParser and parserImplementation). Recently,
Roy et al. [42] evaluated the applicability of multiple metrics from
the NLP domain in code summarization tasks. Specifically, the au-
thors evaluated METEOR [43], BLEU [44], Rouge [45], ChrF [46],
and BERTScore [47]. Judging by their results, ChrF [46] appears to
be the most robust metric for code summarization, so we employ it
in our study alongside the F1-score.

Model comparison. The answer to RQ2 requires pairwise com-
parisons of independent models trained and tested on the same
set of methods, but with data prepared by different parsers. To do
so, we run the paired bootstrap test [48] over the predictions of
two models. The test resamples the result pairs with replacement
multiple times and compares average results of the models on the

sampled examples. The probability of one model beating the other
is a ratio of how often the average score for this model is better
than for the other to the total number of runs.

5 RESULTS
We use the described evaluation scheme for per-parser analysis of
the produced ASTs and further comparison of two ML4SE models:
TreeLSTM [12] and Code2Seq [3] on the method name prediction
problem. We use a newly collected Java-norm dataset, leaving only
the methods successfully processed by all the studied parsers.

RQ1: Is there any statistical difference in the ASTs pro-

duced by different parsers?

In order to answer the first research question, for each tree, we

compute the following set of metrics:

‚Ä¢ tree size (TS) ‚Äî number of nodes in a tree;
‚Ä¢ tree depth (TD) ‚Äî number of nodes in a path from the root

of a tree to its deepest node;

‚Ä¢ branching factor (BF) ‚Äî mean number of children in non-

leaf vertices of a tree;

‚Ä¢ unique types (UTP) ‚Äî number of unique types of interme-
diate nodes used in an AST, lower values correspond to a
higher level of abstraction used in a parser as it can represent
the same code fragment in a more compact way;

‚Ä¢ unique tokens (UTK) ‚Äî number of unique sub-tokens in
AST leaves, lower values also reflect higher level of abstrac-
tion (e.g., whether parser keeps binary operators as tokens
or as node types).

The first three metrics aim to reflect the tree structure and esti-
mate how different the trees are size-wise. The latter two metrics
analyze the content and abstraction level of trees produced by dif-
ferent parsers. Table 2 provides quantitative information about the
computed metrics for each parser.

After computing the selected five metrics for trees built by each
parser, we apply the Student‚Äôs t-test [41] to determine whether the
distributions differ for each pair of parsers. We consider distribu-
tions to be significantly different if the corresponding p-value is
less than 0.01, i.e., we consider distributions different only if the
probability of a mistake is less than 1%. According to the conducted
statistical tests, all parser pairs produce trees different by at least
three metrics. Table 3 shows with respect to which metrics the
output of two parsers is similar for each pair of parsers.

Take away 1. By applying different parsers to the same
code fragment we get different trees both in terms of struc-
ture (i.e., tree size, depth, and branching) and in terms of
content (i.e., the number of unique types and tokens in the
trees). Although some parsers produce trees that are sim-
ilar by one or two metrics, for all parser pairs trees differ
significantly in various ways.

RQ2: How does the parser selection affect machine learn-

ing models that take into account tree structure of code?

Since RQ1 shows that different parsers produce trees with dif-
ferent characteristics, it is necessary to study whether it affects
the quality of models that utilize these trees. For that, we train
both models, TreeLSTM and Code2Seq, on the datasets produced

Conference‚Äô17, July 2017, Washington, DC, USA

Ilya Utkin, Egor Spirin, Egor Bogomolov, and Timofey Bryksin

Parser

ANTLR
JDT
JavaLang
JavaParser
PSI
Spoon
SrcML
Tree-Sitter

Tree metrics (median)
TS TD BF UTP UTK
56
31
36
28
93
109
42
60

32
14
19
13
32
15
17
25

24
19
19
17
29
28
19
27

3
2
2
2
2
2
2
3

8
6
7
6
9
8
8
8

Language
support
Any (grammar)
Java
Java
Java
Any (IntelliJ)
Java
Java/C/C++/C#
Any (grammar)

Python
API
+
-
+
-
-
-
-
+

SuperParser
speed (files/sec)
759
1045
78
938
30‚àó
150
313
117

Table 2: On the left-hand side: median values of the computed metrics for each parser. TS ‚Äì tree size, TD ‚Äì tree depth, BF ‚Äì
branching factor, UTP ‚Äì unique types, UTK ‚Äì unique tokens. On the right-hand side: parsers‚Äô features that can be considered
when building a data processing pipeline.
‚àóWe extract PSI trees with PSIMiner [34] rather than with SuperParser. PSIMiner is slower compared to SuperParser as it opens
each project in IntelliJ IDEA, which introduces a significant time overhead.

ANTLR
JDT
JavaLang
JavaParser
PSI
Spoon
SrcML
Tree-Sitter

ANTLR
-
-
-
-
UTP
TD
TD
TS

JDT
-
-
TS, UTK
TS, UTK
-
-
UTK
-

JavaLang
-
TS, UTK
-
UTK
-
-
TS, UTK
TD

JavaParser
-
TS, UTK
UTK
-
BF
-
UTK
-

PSI
UTP
-
-
BF
-
UTK
-
UTK

Spoon
TD
-
-
-
UTK
-
TD
UTK

SrcML
TD
UTK
TS, UTK
UTK
-
TD
-
-

Tree-Sitter
TS
-
TD
-
UTK
UTK
-
-

Table 3: Tree metrics that are similar for trees produced by each pair of parsers according to the Student‚Äôs t-test with ùëù-value
0.01. For example, trees built by ANTLR and Spoon are similar w.r.t. tree depth.

by each parser. To get reliable results, we train each model-parser
pair five times with different random seeds. Therefore, we train
and evaluate 80 models overall. Table 4 shows mean values and
variance of metrics for each model-parser pair.

The direct comparison already shows the difference between
parsers for both models. An accurate parser choice can increase F1-
score by up to 27% for TreeLSTM and 5.5% for Code2Seq compared
to the worst performing parser. For ChrF the results are similar,
with 31% and 8% increase for TreeLSTM and Code2Seq, respec-
tively. While the absolute differences might seem low, we argue
that they cannot be ignored in current ML4SE research. Nowadays,
most works compare the models by directly comparing metric val-
ues averaged over the testing dataset (average F1-score in the case
of method name prediction). For example, in the recent works on
method name prediction [5, 16], the differences of less than 2 per-
centage points in F1-score between different models and different
model variations are considered significant, while some of the com-
pared models heavily rely on code structure. Thus, the observed
differences in metrics depending on the parser are comparable to
the values considered significant in modern works.

In order to further ensure correctness of the results, we also
conduct paired bootstrap tests for the trained models with respect
to F1-score. For each pair of parsers and random seeds, we run their
pairwise comparison with 1,000 bootstrap resamples, 25,000 resam-
ples in total (5 random seeds for both parsers). We then estimate

the probability of one parser beating another as the percentage of
resamples where it has a higher average score. Figure 3a and Fig-
ure 3b provide the computed probabilities for both models and all
analyzed parsers. Given the current approach to model comparison
(i.e., direct comparison of averaged metrics over the testing dataset),
the values can be interpreted as the probability of the same model
with one parser showing better quality than with another one.

Firstly, the bootstrap results suggest that parsers indeed have an
impact on the model quality. Otherwise, all the pairwise winning
probabilities would be close to 50%. Secondly, for both models,
there exists a parser (JavaParser for TreeLSTM, JDT for Code2Seq)
that beats the others most of the time and can be treated as a
superior choice when working with this model. Also, there are
parsers for each model (e.g., ANTLR, JavaLang, and JDT in the case
of TreeLSTM) that show similar performance to each other and
can be used interchangeably depending on the existing limitations
(e.g., speed, ease of integration into the mining pipeline). Finally,
for the two studied models, the relative ordering of parsers differs.
While for TreeLSTM JavaParser is the best choice, and Spoon and
PSI show poor quality, for Code2Seq it is JDT which is the best, and
Spoon and Tree-sitter are falling behind the others.

Combining the changes in models‚Äô quality with the tree metrics
computed in RQ1 allows us to make assumptions about the nature
of the changes. We notice that shallower trees generally lead to
better model performance, which is intuitive since ML models may

Evaluating the Impact of Source Code Parsers on ML4SE Models

Conference‚Äô17, July 2017, Washington, DC, USA

Parser

ANTLR
JDT
JavaLang
JavaParser
PSI
Spoon
SrcML
Tree-Sitter

TreeLSTM

Code2Seq

Prec
36.0 ¬± 1.9
35.9 ¬± 2.2
36.4 ¬± 1.9
38.2 ¬± 2.0
35.6 ¬± 3.7
31.6 ¬± 1.9
36.1 ¬± 1.9
36.4 ¬± 1.4

Rec
25.5 ¬± 1.1
24.5 ¬± 1.3
24.7 ¬± 1.0
26.0 ¬± 1.4
22.9 ¬± 1.7
19.9 ¬± 1.7
24.1 ¬± 2.0
24.1 ¬± 1.0

F1
29.8 ¬± 1.3
29.1 ¬± 1.6
29.4 ¬± 1.0
30.9 ¬± 1.6
27.8 ¬± 2.3
24.3 ¬± 1.8
28.9 ¬± 2.0
29.0 ¬± 1.1

ChrF
24.8 ¬± 1.4
25.0 ¬± 1.2
25.1 ¬± 1.3
25.7 ¬± 1.8
23.3 ¬± 1.3
19.6 ¬± 1.2
24.3 ¬± 1.8
24.0 ¬± 0.3

Prec
50.2 ¬± 1.1
51.0 ¬± 0.6
50.7 ¬± 0.8
49.7 ¬± 0.9
50.3 ¬± 0.6
50.1 ¬± 0.8
50.3 ¬± 0.9
50.4 ¬± 0.5

Rec
32.7 ¬± 0.7
33.4 ¬± 0.5
33.1 ¬± 0.5
33.3 ¬± 0.3
32.9 ¬± 0.5
30.9 ¬± 0.5
32.7 ¬± 0.4
32.4 ¬± 0.4

F1
39.6 ¬± 0.4
40.3 ¬± 0.3
40.0 ¬± 0.1
39.9 ¬± 0.3
39.8 ¬± 0.4
38.2 ¬± 0.5
39.7 ¬± 0.5
39.4 ¬± 0.4

ChrF
31.3 ¬± 0.6
32.0 ¬± 0.4
31.9 ¬± 0.4
32.2 ¬± 0.1
31.7 ¬± 0.3
29.8 ¬± 0.3
31.5 ¬± 0.2
31.1 ¬± 0.4

Table 4: Quality of method name prediction for TreeLSTM and Code2Seq models on the data extracted with the studied parsers.

(a) Paired bootstrap for TreeLSTM model.

(b) Paired bootstrap for Code2Seq model.

Figure 3: Results of pairwise bootstrap tests for TreeLSTM and Code2Seq models. The numbers are probabilities of one model
beating another with random initialization and on a random subset of data. Colors are added for clarity.

struggle to pass information on long distances. Another observaion
is that the small size of the trees combined with a higher level of
their abstraction (fewer unique types and tokens) helps JavaParser,
JavaLang, and JDT consistently outperform the others.

Take away 2. The choice of a parser for data preprocessing
impacts the model‚Äôs quality. Although some parsers show
dominance over others, the optimal choice is specific to each
model. Without additional information about the model,
parsers that produce compact trees with higher level of
abstraction are more preferable.

RQ3: Alongside with the model quality, what else
researchers and practitioners should take into account when
choosing a parser for their data-processing pipelines?

For both models, there exist groups of parsers showing simi-
lar performance. When choosing between such parsers, aspects
other than raw model quality should be taken into consideration.
Depending on the task at hand, the size of the dataset, and target
programming languages, different parsers might become more or
less suitable.

When dealing with large-scale datasets, the parser‚Äôs speed might
become a bottleneck. Table 2 shows the average number of pro-
cessed examples per second for all the parsers. To measure this,
we run SuperParser multiple times for each parser with a task of
preparing data for the TreeLSTM model, for which parsing is the
most time-consuming stage of the pipeline. The fastest parsers con-
sidered in this study‚ÄîJDT and JavaParser‚Äîare more than 8 times
faster than JavaLang and more than 6 times faster than Tree-sitter.

(1)(2)(3)(4)(5)(6)(7)(8)(1) JavaParser‚Äî0.720.800.800.860.800.891.00(2) ANTLR0.28‚Äî0.630.610.690.680.791.00(3) JavaLang0.200.37‚Äî0.550.640.700.761.00(4) JDT0.200.390.45‚Äî0.550.550.741.00(5) Tree-Sitter0.140.310.360.45‚Äî0.590.671.00(6) SrcML0.200.320.300.450.41‚Äî0.641.00(7) PSI0.110.210.240.260.330.36‚Äî0.88(8) Spoon0.000.000.000.000.000.000.12‚Äî(1)(2)(3)(4)(5)(6)(7)(8)(1) JDT‚Äî0.780.770.810.860.910.991.00(2) JavaLang0.22‚Äî0.610.620.700.740.921.00(3) JavaParser0.230.39‚Äî0.570.570.580.801.00(4) PSI0.190.380.43‚Äî0.490.550.751.00(5) ANTLR0.140.300.430.51‚Äî0.540.741.00(6) SrcML0.090.260.420.450.46‚Äî0.681.00(7) Tree-Sitter0.010.080.200.250.260.32‚Äî1.00(8) Spoon0.000.000.000.000.000.000.00‚ÄîConference‚Äô17, July 2017, Washington, DC, USA

Ilya Utkin, Egor Spirin, Egor Bogomolov, and Timofey Bryksin

Another resource-related issue is the memory required to store
the resulting dataset. It might affect the speed of model training:
smaller datasets are faster to read from drive, and in some cases
can be loaded directly into RAM. Memory consumption directly
depends on the size of the produced trees. The Spoon and PSI parsers
generate 2 to 4 times larger trees compared to others. The TS column
in Table 2 provides average tree sizes for different parsers, which
allows to approximate the size of the processed dataset.

Support of multiple programming languages allows the whole
learning pipeline to be less dependent on a specific language and
thus be more extensible. Moreover, it is necessary for training
multi-language models [49]. Parser generators like ANTLR5 and
Tree-Sitter6 can support any language if the appropriate grammar
is available, making them the most flexible choice. In addition to
Java, SrcML also supports C, C++, and C#. While, PSI works for all
the languages supported by IntelliJ-based IDEs and their plug-ins,
extraction of PSI for each language requires manual work to extend
PSIMiner. Currently, PSIMiner supports Java and Kotlin. The rest
of the tools that we consider support only the Java language.

Finally, since most ML pipelines use Python as the main language,
Python bindings might be convenient to interact with the parser
directly. Out of the considered parsers, only ANTLR, Tree-Sitter,
and JavaLang provide the necessary bindings, while the other can
only be used with JVM.

Take away 3. In addition to the parser‚Äôs impact on the
model quality, there are other aspects that researchers and
practitioners might consider while selecting a parser: speed,
language support, ease of usage. The exact limitations on
the parser choice come from the studied problem, usage
scenarios, and the selected model architecture. In order
to choose a parser for data processing, researchers should
first identify parsers that are applicable in their situation,
and then treat the applicable parsers as a hyperparameter.
The final choice can be made via empirical validation or
by carefully studying features of trees produced by each
parser and the nuances of a particular ML model.

6 DISCUSSION AND FUTURE WORK
The conducted analysis of the impact parsers have on ML4SE mod-
els shows that it can lead to significant deviations in the models‚Äô
performance. For TreeLSTM, a model that directly works with ASTs,
the difference in F1-score can be as large as 27% depending on the
parser. For Code2Seq, a model that uses path-based representations
derived from ASTs, the effect is smaller, but it is still comparable to
the metric differences considered to be significant in recent works.
Judging by the results, researchers and practitioners should pay
closer attention to parser selection when designing their data pro-
cessing pipelines. Although some use cases imply strict limitations
on the used parser, in most cases there exist several parsers to

5ANTLR supported languages: https://github.com/antlr/grammars-v4
6Tree-Sitter supported languages: https://tree-sitter.github.io/tree-sitter/#available-
parsers

choose from. In such cases, the choice of a parser should be inter-
preted as one of hyperparameters when training a model, thus it
requires either clear reasoning or empirical validation.

In this work, we focused on the method name prediction task and
the Java language. However, the usage of structured code represen-
tations proved to be helpful in other tasks as well: variable misuse
prediction [8], clone detection [23], comment generation [50], etc.
Also, Java is not the only language that has a variety of parsers ‚Äî
it is also the case for C/C++, Python, PHP, and others. Extension of
this study to other tasks and languages is an important topic for
future work.

Most structured representations of source code utilize ASTs in
some way, but they are not limited to it. Researchers also used data-
flow graphs, control-flow graphs, program dependence graphs, and
even combined all of them into a single complex structure [4, 7]. As
it is the case with parsing, the process of building other structural
representations is also non-straightforward. Possible differences
in this process range from the exact set of dependencies included
in the graph to the specific tools used to extract them. We leave
careful analysis of the data preparation process for more complex
code representations and its impact on model quality for the future
work.

7 CONCLUSION
With this work, we present an analysis of how the selection of a
parsing tool for AST extraction can affect the quality of Machine
Learning models that rely on structured representations of code.
We demonstrate that the usage of different parsers leads to the dif-
ferences in AST characteristics, both in terms of structure (i.e., tree
size or depth) and content (i.e., number of different node types or
tokens). These differences further impact the quality of ML models.
For the TreeLSTM model that works directly with the trees, the
difference can reach up to 27% of F1-score and 31% of ChrF for
the method name prediction task. For Code2Seq, a model that uses
path-based representation derived from the AST, the difference is
less drastic, only about 5% of F1-score and 8% of ChrF, but it is
still significant.

In order to simplify experiments with different parsers, we de-
velop SuperParser, a tool that can run various parsers in the same
manner. In order to run experiments with different parsers, users
have to change a single value in the YAML configuration file.
The tool facilitates the end-to-end creation of machine learning
datasets from source code. SuperParser already supports 5 differ-
ent languages with 12 parser-language pairs and can be extended
to new parsers and languages. Our tool is publicly available at
https://doi.org/10.5281/zenodo.6366591.

We also publish Java-norm, a dataset similar to the popular Java-
small one: https://doi.org/10.5281/zenodo.6366599. Compared to
Java-small, the validation and testing parts of Java-norm contain
diverse projects making the evaluation more robust and representa-
tive. The usage of Java-norm does not require additional computa-
tional resources, as it has roughly the same size as Java-small, yet
it allows more robust comparison between models.

All in all, data preprocessing is one of the critical steps when ap-
plying machine learning algorithms. When working with structural
representations of source code (e.g., in the form of an AST), data

Evaluating the Impact of Source Code Parsers on ML4SE Models

Conference‚Äô17, July 2017, Washington, DC, USA

preprocessing becomes rather complex and requires the usage of
parsers or other external tools. To make such studies reproducible
while maximizing the achieved quality, it is important to properly
select a parsing tool and document the selection process in the
reports. We hope that future researchers will take our findings into
consideration and will provide more detailed information about the
data preparation process in their works.

REFERENCES
[1] T. Sharma, M. Kechagia, S. Georgiou, R. Tiwari, and F. Sarro, ‚ÄúA survey on machine
learning techniques for source code analysis,‚Äù arXiv preprint arXiv:2110.09610,
2021.

[2] M. D. Ernst, ‚ÄúNatural language is a programming language: Applying natural
language processing to software development,‚Äù in SNAPL 2017: the 2nd Summit oN
Advances in Programming Languages, Asilomar, CA, USA, May 2017, pp. 4:1‚Äì4:14.
[3] U. Alon, S. Brody, O. Levy, and E. Yahav, ‚Äúcode2seq: Generating sequences from
structured representations of code,‚Äù arXiv preprint arXiv:1808.01400, 2018.
[4] M. Allamanis, E. T. Barr, S. Ducousso, and Z. Gao, ‚ÄúTypilus: Neural type hints,‚Äù

in PLDI, 2020.

[5] D. Z√ºgner, T. Kirschstein, M. Catasta, J. Leskovec, and S. G√ºnnemann, ‚ÄúLanguage-
agnostic representation learning of source code from structure and context,‚Äù in
International Conference on Learning Representations (ICLR), 2021.

[6] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, ‚ÄúA general path-based representa-
tion for predicting program properties,‚Äù ACM SIGPLAN Notices, vol. 53, no. 4, pp.
404‚Äì419, 2018.

[7] M. Allamanis, M. Brockschmidt, and M. Khademi, ‚ÄúLearning to represent pro-

grams with graphs,‚Äù arXiv preprint arXiv:1711.00740, 2017.

[8] V. J. Hellendoorn, C. Sutton, R. Singh, P. Maniatis, and D. Bieber, ‚ÄúGlobal relational
models of source code,‚Äù in International Conference on Learning Representations,
2020. [Online]. Available: https://openreview.net/forum?id=B1lnbRNtwr

[9] G. Rusak, A. Al-Dujaili, and U.-M. O‚ÄôReilly, ‚ÄúAst-based deep learning for detecting
malicious powershell,‚Äù in Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security, 2018, pp. 2276‚Äì2278.

[10] M. Brunsfeld, P. Thomson, A. Hlynskyi, J. Vera, P. Turnbull, T. Clem, D. Creager,
A. Helwer, R. Rix, H. van Antwerpen, M. Davis, Ika, T.-A. Nguyen, S. Brunk,
N. Hasabnis, bfredl, M. Dong, V. Panteleev,
ikrima, S. Kalt, K. Lampe,
A. Pinkus, M. Schmitz, M. Krupcale, narpfel, S. Gallegos, V. Mart√≠, Edgar,
and G. Fraser, ‚Äútree-sitter/tree-sitter: v0.20.6,‚Äù Mar. 2022. [Online]. Available:
https://doi.org/10.5281/zenodo.6326492

[11] T. Parr, The Definitive ANTLR 4 Reference, 2nd ed. Pragmatic Bookshelf, 2013.
[12] K. S. Tai, R. Socher, and C. D. Manning, ‚ÄúImproved semantic representations from
tree-structured long short-term memory networks,‚Äù CoRR, vol. abs/1503.00075,
2015. [Online]. Available: http://arxiv.org/abs/1503.00075

[13] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural
Comput., vol. 9, no. 8, p. 1735‚Äì1780, nov 1997. [Online]. Available: https:
//doi.org/10.1162/neco.1997.9.8.1735

[14] A. R. Nagar, Code Search Using Code2Seq. University of California, Irvine, 2021.
[15] X. Zhang, Y. Lu, and K. Shi, ‚ÄúCb-path2vec: A cross block path based representation
for software defect prediction,‚Äù 2020 IEEE 6th International Conference on Computer
and Communications (ICCC), pp. 1961‚Äì1966, 2020.

[16] P. Fernandes, M. Allamanis, and M. Brockschmidt, ‚ÄúStructured neural
summarization,‚Äù in International Conference on Learning Representations, 2019.
[Online]. Available: https://openreview.net/forum?id=H1ersoRqtm

[17] V. Kovalenko, E. Bogomolov, T. Bryksin, and A. Bacchelli, ‚ÄúPathminer: a library
for mining of path-based representations of code,‚Äù in Proceedings of the 16th
International Conference on Mining Software Repositories.
IEEE Press, 2019, pp.
13‚Äì17.

[18] ‚ÄúProgram structure interface,‚Äù https://jetbrains.org/intellij/sdk/docs/basics/

architectural_overview/psi.html, [Online; accessed 17-March-2022].

[19] J. Arthanareeswaran, K. P. Tatavarthi, M. Palat, N. Gupta, and S. Sinha, ‚ÄúEclipse
Java Development tools,‚Äù https://www.eclipse.org/jdt/, [Online; accessed 17-
March-2022].

[20] D. van Bruggen, F. Tomassetti, R. Howell, M. Langkabel, N. Smith, A. Bosch,
M. Skoruppa, C. Maximilien, ThLeu, Panayiotis, S. K. (@skirsch79), Simon,
jean pierre L, A. Rou√©l, edefazio, D. Schipper,
J. Beleites, W. Tibackx,
Mathiponds, W. you want to know, R. Beckett, ptitjes, kotari4u, M. Wyrich,
R. Morais, M. Coene, bresai, Implex1v, and B. Haumacher, ‚Äújavaparser/javaparser:
Release javaparser- parent-3.16.1,‚Äù May 2020. [Online]. Available: https:
//doi.org/10.5281/zenodo.3842713

[21] X. Chen, C. Liu, and D. Song, ‚ÄúTree-to-tree neural networks for program trans-
Red Hook, NY, USA: Curran Associates Inc., 2018, p.

lation,‚Äù ser. NIPS‚Äô18.
2552‚Äì2562.

[22] Y. Shido, Y. Kobayashi, A. Yamamoto, A. Miyamoto, and T. Matsumura, ‚ÄúAutomatic
source code summarization with extended tree-lstm,‚Äù in 2019 International Joint

Conference on Neural Networks (IJCNN).

IEEE, 2019, pp. 1‚Äì8.

[23] H. Liang and L. Ai, ‚ÄúAst-path based compare-aggregate network for code clone
detection,‚Äù in 2021 International Joint Conference on Neural Networks (IJCNN).
IEEE, 2021, pp. 1‚Äì8.

[24] K. Shi, Y. Lu, J. Chang, and Z. Wei, ‚ÄúPathpair2vec: An ast path pair-based code
representation method for defect prediction,‚Äù Journal of Computer Languages,
vol. 59, p. 100979, 2020.

[25] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, ‚Äúcode2vec: Learning distributed
representations of code,‚Äù Proceedings of the ACM on Programming Languages,
vol. 3, no. POPL, pp. 1‚Äì29, 2019.

[26] J. Zhang, X. Wang, H. Zhang, H. Sun, and X. Liu, ‚ÄúRetrieval-based neural
source code summarization,‚Äù in Proceedings of the ACM/IEEE 42nd International
Conference on Software Engineering, ser. ICSE ‚Äô20. New York, NY, USA:
Association for Computing Machinery, 2020, p. 1385‚Äì1397. [Online]. Available:
https://doi.org/10.1145/3377811.3380383

[27] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, ‚ÄúConvolutional neural networks
over tree structures for programming language processing,‚Äù in Proceedings of the
Thirtieth AAAI Conference on Artificial Intelligence, ser. AAAI‚Äô16. AAAI Press,
2016, p. 1287‚Äì1293.

[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,
and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in Proceedings of the 31st Interna-
tional Conference on Neural Information Processing Systems, ser. NIPS‚Äô17. Red
Hook, NY, USA: Curran Associates Inc., 2017, p. 6000‚Äì6010.

[29] J. Falleri, F. Morandat, X. Blanc, M. Martinez, and M. Monperrus, ‚ÄúFine-
grained and accurate source code differencing,‚Äù in ACM/IEEE International
Conference on Automated Software Engineering, ASE ‚Äô14, Vasteras, Sweden
- September 15 - 19, 2014, 2014, pp. 313‚Äì324. [Online]. Available: http:
//doi.acm.org/10.1145/2642937.2642982

[30] P. Yin and G. Neubig, ‚ÄúA syntactic neural model for general-purpose code gener-

ation,‚Äù arXiv preprint arXiv:1704.01696, 2017.

[31] B. Wei, G. Li, X. Xia, Z. Fu, and Z. Jin, ‚ÄúCode generation as a dual
task of code summarization,‚Äù in NIPS Proceedings - Advances in Neural
Information Processing Systems 32 (NIPS 2019), ser. Advances in Neural
Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer,
F. d‚ÄôAlcheBuc, E. Fox, and R. Garnett, Eds., vol. 32. Neural Information
Processing Systems (NIPS), 2019, advances in Neural Information Processing
Systems 2019, NIPS 2019 ; Conference date: 08-12-2019 Through 14-12-
2019. [Online]. Available: https://nips.cc/Conferences/2019,https://papers.nips.
cc/book/advances-in-neural-information-processing-systems-32-2019

[32] R. Tairas and J. Gray, ‚ÄúPhoenix-based clone detection using suffix trees,‚Äù in
Proceedings of the 44th annual Southeast regional conference, 2006, pp. 679‚Äì684.
[33] C. Tunes, ‚Äújavalang: Pure python java parser and tools,‚Äù https://github.com/c2nes/

javalang, [Online; accessed 17-March-2022].

[34] E. Spirin, E. Bogomolov, V. Kovalenko, and T. Bryksin, ‚ÄúPsiminer: A tool for

mining rich abstract syntax trees from code,‚Äù 05 2021, pp. 13‚Äì17.

[35] R. Pawlak, M. Monperrus, N. Petitprez, C. Noguera, and L. Seinturier, ‚ÄúSpoon:
A Library for Implementing Analyses and Transformations of Java Source
Code,‚Äù Software: Practice and Experience, vol. 46, pp. 1155‚Äì1179, 2015. [Online].
Available: https://hal.archives-ouvertes.fr/hal-01078532/document

[36] J. Xuan, M. Martinez, F. Demarco, M. Clement, S. L. Marcote, T. Durieux,
D. Le Berre, and M. Monperrus, ‚ÄúNopol: Automatic repair of conditional state-
ment bugs in java programs,‚Äù IEEE Transactions on Software Engineering, vol. 43,
no. 1, pp. 34‚Äì55, 2016.

[37] M. Martinez and M. Monperrus, ‚ÄúAstor: A program repair library for java,‚Äù in
Proceedings of the 25th International Symposium on Software Testing and Analysis,
2016, pp. 441‚Äì444.

[38] F. Long, P. Amidon, and M. Rinard, ‚ÄúAutomatic inference of code transforms for
patch generation,‚Äù in Proceedings of the 2017 11th Joint Meeting on Foundations of
Software Engineering, 2017, pp. 727‚Äì739.

[39] M. L. Collard, M. J. Decker, and J. I. Maletic, ‚Äúsrcml: An infrastructure for the
exploration, analysis, and manipulation of source code: A tool demonstration,‚Äù
in 2013 IEEE International Conference on Software Maintenance.
IEEE, 2013, pp.
516‚Äì519.

[40] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton, ‚ÄúSuggesting accurate
method and class names,‚Äù in Proceedings of the 2015 10th Joint Meeting on
Foundations of Software Engineering, ser. ESEC/FSE 2015. New York, NY, USA:
Association for Computing Machinery, 2015, p. 38‚Äì49. [Online]. Available:
https://doi.org/10.1145/2786805.2786849

[41] Student, ‚ÄúThe probable error of a mean,‚Äù Biometrika, pp. 1‚Äì25, 1908.
[42] D. Roy, S. Fakhoury, and V. Arnaoudova, ‚ÄúReassessing automatic evaluation
metrics for code summarization tasks,‚Äù in Proceedings of the 29th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, 2021, pp. 1105‚Äì1116.

[43] S. Banerjee and A. Lavie, ‚ÄúMeteor: An automatic metric for mt evaluation with
improved correlation with human judgments,‚Äù in Proceedings of the acl workshop
on intrinsic and extrinsic evaluation measures for machine translation and/or
summarization, 2005, pp. 65‚Äì72.

Conference‚Äô17, July 2017, Washington, DC, USA

Ilya Utkin, Egor Spirin, Egor Bogomolov, and Timofey Bryksin

[44] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‚ÄúBleu: a method for automatic
evaluation of machine translation,‚Äù in Proceedings of the 40th annual meeting of
the Association for Computational Linguistics, 2002, pp. 311‚Äì318.

[45] C.-Y. Lin and F. Och, ‚ÄúLooking for a few good metrics: Rouge and its evaluation,‚Äù

in Ntcir workshop, 2004.

[46] M. Popoviƒá, ‚Äúchrf: character n-gram f-score for automatic mt evaluation,‚Äù in
Proceedings of the Tenth Workshop on Statistical Machine Translation, 2015, pp.
392‚Äì395.

[47] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, ‚ÄúBertscore: Evaluating

text generation with bert,‚Äù arXiv preprint arXiv:1904.09675, 2019.

[48] B. Efron and R. J. Tibshirani, An Introduction to the Bootstrap, ser. Monographs
on Statistics and Applied Probability. Boca Raton, Florida, USA: Chapman &

Hall/CRC, 1993, no. 57.

[49] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu,
D. Jiang, and M. Zhou, ‚ÄúCodeBERT: A pre-trained model for programming and
natural languages,‚Äù in Findings of the Association for Computational Linguistics:
EMNLP 2020. Online: Association for Computational Linguistics, Nov. 2020, pp.
1536‚Äì1547. [Online]. Available: https://aclanthology.org/2020.findings-emnlp.139
[50] B. Li, M. Yan, X. Xia, X. Hu, G. Li, and D. Lo, ‚ÄúDeepcommenter: a deep code
comment generation tool with hybrid lexical and syntactical information,‚Äù in
Proceedings of the 28th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering, 2020, pp.
1571‚Äì1575.

