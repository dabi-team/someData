A Two-phase Metamorphic Approach for Testing
Industrial Control Systems

Gaadha Sudheerbabu∗, Tanwir Ahmad∗, Filip Sebek‡, Dragos Truscan∗, J¨uri Vain∗† and Ivan Porres∗
∗ Dept. of Information Technology, ˚Abo Akademi University, Turku, Finland, Email: ﬁrstname.lastname@abo.ﬁ,
† High-assurance Software Laboratory, Tallinn Technical University, Tallinn, Estonia, Email: juri.vain@ttu.ee
‡ ABB, Sweden, Email: ﬁlip.sebek@se.abb.com

2
2
0
2

g
u
A
9
1

]
E
S
.
s
c
[

1
v
1
6
2
9
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—We elaborate on a metamorphic approach for testing
industrial control systems. The proposed approach consists of
two phases: an exploration phase in which we learn about fault
patterns of the system under test and an exploitation phase where
the observed fault patterns are used for targeted testing. Our
method extracts metamorphic relations and input space of the
system from its requirements. The seed input used for testing
is extracted from the execution logs of the system and used
to generate source tests and follow-up tests automatically. The
morphed input is constructed based on the seed input and reﬁned
using a set of constraints. The approach is exempliﬁed on a
position control system and the results show that it is effective
in discovering faults with an increased level of automation.

Index Terms—Metamorphic testing, industrial control systems

I. INTRODUCTION

The functional correctness of industrial software systems is
of utmost importance as a system failure may incur signiﬁcant
ﬁnancial or even human life losses. Testing of such industrial
systems are further complicated due to the lack of a test
oracle [1]. Metamorphic testing (MT) was introduced by Chen
et al. [2] as a solution to test systems when the expected
output or test oracle of the system under test (SUT) is not
available to compare the actual output of the SUT against
its expected output. In MT,
the behavioural or functional
properties of the system are deﬁned using generic relations
known as metamorphic relations (MRs) between different sets
of inputs and their expected outputs. These relations are used
to verify functional correctness instead of mapping speciﬁc
inputs to their expected outputs.

However, the recent surveys on MT highlight several ques-
tions remain open for further investigation: how to create the
MRs, how to deﬁne the follow-up test cases, and how to
automate different phases of the process. In this paper, we
propose a two-phase MT approach to circumvent the need for
a traditional pre-deﬁned test oracle: a) exploration: extracting
the MRs from system speciﬁcations, generating source test
cases automatically from real-execution data by analyzing data
logs, and generating follow-up test cases automatically from
source test cases, and b) exploitation: identifying fault patterns
via random test generation and exploring them in more detail
via guided test generation. In our approach the deﬁnition of
MRs is manual based on domain expert knowledge, but the test

This work has received funding from Horizon 2020 programme under grant

agreement No. 957212 - VeriDevOps project.

generation, execution, and verdict assignment are automatic.
In fact, studies have shown that manual testing with domain
expert involvement can be more effective than fully automated
testing [3]. We exemplify our approach on a position control
system that determines the position of a hanging load attached
to the hoisting frame of a crane.

II. OVERVIEW OF THE APPROACH

Metamorphic testing performs as follows [4]: a) identify
MRs based on system properties deﬁned in the software
speciﬁcation, b) generate a source test case passing the seed
input to the system, c) generate follow-up test cases from
the source test case based on MRs and execute them, and
d) compare the results of source and follow-up test cases if
they satisfy the MR.

An MR is composed of two parts: an input relation and
output relation [4]. An input relation represents the relation
between the inputs of source and follow-up test cases, whereas
an output relation represents the relation between the outputs
of the source and follow-up test cases. A source test case is the
ﬁrst set of tests performed using seed inputs. The seed inputs
are transformed into morphed inputs. The follow-up test cases
are performed using these morphed inputs. In addition, an
implication between the outputs of source and follow-up test
cases is needed to specify the impact of input transformations
on their corresponding outputs. Chen et al. [5] presented the
MT methodology and deﬁned MR as follows:

Deﬁnition II.1. (Metamorphic relation): Let f be a target
function or algorithm. An MR is a necessary property of f over
a sequence of two or more inputs (cid:104)x1, x2, ..., xn(cid:105) where n ≥ 2,
and their corresponding outputs (cid:104)f (x1), f (x2), ..., f (xn)(cid:105). It
can be expressed as a relation R ⊆ X n × Y n, where X n and
Y n are the Cartesian products of n input and n output spaces,
respectively.

We extend the above deﬁnition, by reﬁning R into Rin
and Rout, where the satisﬁability of MR output relation Rout
by outputs Ys and Ym presumes also that their corresponding
morphed inputs Xm satisfy respectively MR input relation
Rin. That is, given f (xi) = yi and f (xj) = yj∀(xi, xj), then
Rin(xi, xj) =⇒ Rout(yi, yj), where f denotes the function
that creates outputs (yi, yj) in response to inputs (xi, xj), Rin
is input MR and Rout is output MR.

 
 
 
 
 
 
load (see Figure 2). The two markers placed on the sides of
the hoisting frame are referred to as side markers. The top
marker is used to detect the tilt of load and to increase the
probability for the ﬁnd algorithm to identify the true markers.

For each set of markers,
the
PCS tries to identify the true
markers and to discard the mark-
ers corresponding to reﬂections.
The PCS produces two outputs:
a Boolean value found indicating
whether true markers is identi-
ﬁed and a vector of three integers
[Itm1, Itm2 , Itm3], indicating the
index in the input marker array of
the positional markers identiﬁed as
true markers. Whenever the PCS is not able to identify the true
markers consistently, the entire system can potentially move to
an unsafe state and requires human intervention. In the above
context, we deﬁne the output of the PCS as follows:

Fig. 2: Positional mark-
ers in PCS

Deﬁnition II.2. The output of the Position Control System,
f ({m1, m2, ..., mn}) is a pair (f ound, [Itm1, Itm2, Itm3 ]),
where mi and tmi are positional markers with two coordinates
xi and yi, {tm1, tm2, tm3} ⊆ {m1, m2, ..., mn}, f ound =
T RU E|F ALSE and [Itm1, Itm2, Itm3 ] is the vector of in-
dexes of true markers provided that f ound = T RU E.

B. Metamorphic relation

In our approach, we extract the MR from the requirements
of the SUT as follows: ”Assuming that the system is able
to classify correctly a set of markers received from camera
module in the absence of reﬂections (noise), the system should
be able to classify correctly the same inputs in the presence
of reﬂections”.

This can be formulated as the following metamorphic rela-

tion: f (Xs) ≡ f (Xs ∪ Xn).

C. Creating the seed input

2, tmi

1, tmi

In our approach, we choose the seed input as a series
of true marker triplets Xs = {s1, s2, . . . , sk}, where si =
{tmi
3}. We extract the seed input from previous
executions of the PCS by extracting those log entries that only
contain three positional markers and which were classiﬁed
correctly as true markers. The spatial continuity of the image
coordinates is validated via checking the distance between
consecutive image coordinate values against a predeﬁned
allowed range of movement. When the seed input data set
is extracted from the execution trace, we run an initial test
session against the SUT to conﬁrm that all the input marker
positions are classiﬁed correctly. In case execution logs are
not available, the seed input data can be collected from the
simulation environment of the PCS that is validated against a
real crane for the set of inputs the seed is extracted from.

Fig. 1: Overview of the metamorphic testing approach

Concretely, for a given sequence of inputs X c ⊆ X under
a constraint C, an MR R should hold for any correspond-
m), where
ing output of the system,
that
m ⊆ X c. Furthermore, we consider R to be of any of
X c
the types deﬁned in [6]: equivalence, equality, subset, disjoint,
complete, and difference.

s )Rf (X c

is f (X c

s , X c

Our MT approach is shown in Figure 1. It

is applied
using two phases: exploration phase and exploitation phase.
In the exploration phase, Xs, Xm are extracted/created from X
satisfying a set of constraints Cs, Cm speciﬁc to the SUT. Then
Xs, Xm are executed against the SUT and the corresponding
seed output Ys and respectively morphed output Ym are
collected and satisﬁability of Rout(Ys, Ym) is checked, where
Ys = f (Xs) and Ym = f (Xm).

From those pairs of seed and morphed inputs (Xsi, Xmi)
which fail the initial MR, we manually extract, in the ex-
ploitation phase, fault-inducing patterns of the input space.
Based on them, we deﬁne C (cid:48)
m as a more restrictive constraint
to be satisﬁed by morphed inputs X (cid:48)
m which we use to
verify the output metamorphic relation Rout(Ys, Y (cid:48)
m), where
Ys = f (Xs) and Y (cid:48)

m).
The novelty of our approach stands in the fact that C (cid:48)
m
allows us to deﬁne a reﬁned morphed input that tests the
system with more precision and effectiveness, by focusing the
testing on the parts of the input with a higher probability of
discovering faults.

m = f (X (cid:48)

A. Running Example

The ICS is a Position Control System (PCS) which deter-
mines the position of a hanging load using attached markers
on the hoisting frame. The PCS regularly receives up to 26
markers as [x,y] pixel coordinates from a camera module.
The input may contain three markers on the hoisting frame
attached to the load, as well as different light reﬂections in the
environment (water, rain, snow, dust, etc.) which the camera
ﬁlter was not able to remove. Only the markers corresponding
to the three markers placed on the hoisting frame carrying the
load are the true markers that determine the position of the

SUTRequirementsConstraint (Cs)Seed input (Xs)Output (Ys)MorphingTransformationConstraint (CM)Morphed input (Xm)Output (YM)violatedsatisfiedMR check(Rout)MorphingTransformationConstraint (C'M)Seed extractionMorphed input(X'm)Input space (X)Seed input (Xs)RinRoutMetamorphicrelationsRinRinOutput (Y'M)TestVerdict:FAILTestVerdict:PASSExploration  phaseFault patternsExploitation  phaseD. Creating the morphed input

In our case, the morphing transformation takes each sample
in the seed input Xs and adds markers corresponding to
reﬂections, which we denote as noise.

Deﬁnition II.3. (Noise): A series of noise markers corre-
sponding to environment reﬂections Xn =∧ {n1, n2, n3, ..., nj},
where ni ∈ X.

In the exploration phase, we use random generated noise
to perform an initial exploration of the SUT in order to collect
observations and identify fault patterns.

To this extent, we create random noise coordinate pairs of
marker vectors of different lengths ranging from 1 to 23. These
noise vectors are appended to the seed input one at a time. The
algorithm for generating morphed input generates random (x,
y) coordinates with a value in range [0,131072], which is the
size of the camera frame.

(Morphed input): A series of markers
Deﬁnition II.4.
Xm = {m1, m2, ..., mk}, where each sample mi =
{tmi
3, ..., ni
j} with j ≤ 23, is the combi-
nation of seed input markers Xs and noise markers Xn.

2, tmi

1, tmi

1, ni

3, ni

2, ni

In the exploitation phase, we analyze noise patterns in
the morphed input that caused the system to make incorrect
classiﬁcations. This led to the following observations: the set
of markers containing two or three noise markers having the
same geometrical pattern of true markers can trigger faulty
behavior of the system. Therefore, we reﬁne the morphed input
to a more constrained version of the input space to exploit the
above mentioned fault patterns.

3, ni
3, ni

1, ni
1, ni

Deﬁnition II.5. (Reﬁned Morphed input): A series of markers
samples X (cid:48)
m = {m1, m2, ..., mk}, where each sample mi =
2, tmi
1, tmi
{tmi
2} in the ﬁrst follow-up test and mi =
{tmi
3} in the second follow-up test. C (cid:48)
2, ni
2, tmi
1, tmi
m
is the restrictive constraint used to reﬁne the added noise to
two and three noise markers. The series X (cid:48)
m is the combination
of seed input markers Xs and restricted set of noise markers
X (cid:48)

n generated using the constraint C (cid:48)

m.

In order to automate the creation of the noise markers, we
create replicas of the true markers, thus obtaining a similar
geometrical pattern in the noise. For each sample of true
markers in the seed input we distribute the noise markers in a
rectangular grid pattern in the camera frame, in order to obtain
a uniform sampling of the input space.

In addition, each replica
of true marker pair placed
on the grid is rotated by
angles 450, 900, and 1350 to
distribute the noise markers
in a star like pattern (see
Figure 3). We note that, the
approach is completely au-
tomated and it allows us to
change the number of gen-

Fig. 3: Test data distribution for
the guided star approach

erated tests by changing the density of the grid and number
of rotations of the noise markers.

E. Test execution

The tests are generated in ofﬂine mode. Test execution
is performed using the Pytest testing framework [7] by
setting up an adapter to connect to the SUT. Open Platform
Communications Uniﬁed Architecture (OPC UA) [8] is used
as the adapter to connect
to the CODESYS development
environment where the PLC application program resides. The
test suite contains the functions to set up OPC server-client
connection. It also has a one-time setup to read the input from
an input ﬁle, send it to SUT, and collect the execution results
to verify if the MR deﬁned between the source and follow-up
test output holds.

III. EXPERIMENTS AND EVALUATION

For all experiments used in this section, we use a seed input
with 625 samples, each containing a sequence of three true
markers extracted from execution logs. Each entry in the seed
input is veriﬁed ﬁrst that it is classiﬁed correctly as the true
markers by the system.

Based on the outcomes of the executed follow up test cases
we can categorize the test cases as follows: true positive (TP)
– the system identiﬁes the actual positional markers as true
markers - expected behavior, false positive (FP) – the system
identiﬁes reﬂection markers as true markers - unexpected
behavior, false negative (FN) – the system fails to identify
the true markers even if they were present in the input -
unexpected behavior, and true negative (TN) – the system does
not identify true markers when the input does not contain true
markers. This is not applicable since in our approach since the
test input always contain true markers.

The TP classiﬁcation of true markers in the morphed output
satisﬁes the MR and counts as tests that do not fail. The failed
tests include FPs, which indicate an incorrect identiﬁcation
and FNs, which indicate missed identiﬁcation of true markers
placed in the ﬁrst three positions in the morphed input.

In the exploration phase,

the test generation algorithm
produces 625 × 10 × 23 = 143750 follow up tests. From
the total of 143 750 executed tests, 143 615 satisfy the
metamorphic relationship, while 135 do not. From the failed
tests, 39 selected the wrong combination of inputs as true
markers, whereas 96 could not ﬁnd true markers among the
inputs although they were present.

The geometric distribution of incorrectly classiﬁed data
points is shown in Figure 5. Further analysis of the failed
tests, provides us with the following observations. FP results
occurred when the input set contained either a set of two noise
markers resembling the pattern of true side markers or a set
of three noise markers resembling the geometrical pattern of
the true marker triplet. FN results occurred when the input set
contained either a number of markers greater than or equal to
6 or a set of two noise markers resembling the pattern of the
true side markers.

020000400006000080000100000120000020000400006000080000100000120000(a) Exploitation: FPs for markers=5

(b) Exploitation: FNs for markers=5

(c) Exploitation: FPs for markers=6

Fig. 4: Input distribution for failed tests in the exploitation phase

(a) morphed input w.r.t FP output

(b) morphed input w.r.t FN output

Fig. 5: Morphed input corresponding to incorrect classiﬁca-
tions in the exploration phase

In the exploitation phase, we ran two separate testing
sessions in which the reﬁned morphed input contains two and
three noise markers respectively, besides the true markers. In
both cases, the test generation algorithm produced 2400 reﬁned
morphed test inputs. These reﬁned morphed inputs are created
by replicating and rotating the seed input markers. It results
in 625 × 4 = 2500 noise markers and discarding the samples
that do not ﬁt in the 131072 x 131072 frame after the rotate
action. The results of the test execution are shown in Table I.
For the test session with ﬁve input markers, 7 FP and 74 FN
classiﬁcations are identiﬁed, whereas, for the subsequent test
with six input markers, no FN and 92 FP classiﬁcations are
identiﬁed. The distribution of the noise markers in the input
space for failed tests in the exploitation phase is shown in
Figure 4. The test execution results of the guided method
where the number of markers is 5 contain more FNs indicating
that the system is not identifying the true markers when a
replica of two side markers is added as noise. However, the
FP results of follow-up test where the number of markers
is 6 reveal that a replica of 3 true markers can trigger an
incorrect identiﬁcation and compromise the functional safety
of the system. It is also observed that a replica of two side
markers has low chances of causing FPs when compared to
the noise created with a replica of 3 true markers. Moreover,
only the noise markers corresponding to the exact replica
of true markers triggered the incorrect identiﬁcation of true
markers. In addition, we can observe that the noise markers
rotated by angles 450, 900, 1350 resulted in TP test cases,
where the system correctly identiﬁed the true markers despite

the presence of noise.

Table I also shows the corresponding Fault Detection Ratio
(FDR) [9] for each phase as the number of tests that found
a fault in the entire tests suite. As expected, the exploration
phase has a very low FDR due to the random test generation,
whereas in the exploitation phase, FDR has increased around
33 to 44 fold.

Method

Exploration

Exploitation

No. of
markers
4 - 26
5
6

No. of
tests
143750
2400
2400

TPs

FPs

FNs

FDR

143615
2319
2308

39
7
92

96
74
-

0.0009
0.03
0.04

TABLE I: Test execution results

IV. DISCUSSION AND CONCLUSIONS

In this work, metamorphic testing is effectively applied
to detect faulty behavior in industrial control systems. The
identiﬁcation of a metamorphic relation is done manually
based on the speciﬁcation of the system. A known challenge
in the identiﬁcation of MRs is the need for domain expertise
to assess the expected input and output behaviour of the
system. As future work, we plan to automate the identiﬁcation
of MR for an ICS from its speciﬁcation and to explore
the applicability of MT for fault localization and program
repair in industrial systems. It is also of interest to combine
metamorphic and mutation-based approaches for testing ICS
and to apply heuristic techniques for minimization of test
suites.

REFERENCES

[1] E. J. Weyuker, “On testing non-testable programs,” The Computer Jour-

nal, vol. 25, no. 4, pp. 465–470, 1982.

[2] T. Y. Chen et al., “Metamorphic testing: a new approach for generating

next test cases,” arXiv preprint arXiv:2002.12543, 2020.

[3] M. N. Zafar, W. Afzal, and E. Enoiu, “Evaluating system-level test genera-
tion for industrial software: A comparison between manual, combinatorial
and model-based testing,” in IEEE/ACM International Conference on
Automation of Software Test, AST@ICSE 2022, Pittsburgh, PA, USA, May
21-22, 2022, pp. 148–159, IEEE, 2022.

[4] H. Liu et al., “A new method for constructing metamorphic relations,” in
12th Intl. Conference on Quality Software, pp. 59–68, IEEE, 2012.
[5] T. Y. Chen et al., “Metamorphic testing: A review of challenges and
opportunities,” ACM Computing Surveys (CSUR), vol. 51, no. 1, pp. 1–
27, 2018.

020000400006000080000100000120000020000400006000080000100000120000020000400006000080000100000120000020000400006000080000100000120000020000400006000080000100000120000020000400006000080000100000120000020000400006000080000100000120000020000400006000080000100000120000020000400006000080000100000120000020000400006000080000100000120000[6] S. Segura et al., “Metamorphic testing of restful web APIs,” IEEE
Transactions on Software Engineering, vol. 44, no. 11, pp. 1083–1099,
2017.

[7] J. Hunt, “Pytest testing framework,” in Advanced Guide to Python 3

Programming, pp. 175–186, Springer, 2019.

[8] S.-H. Leitner and W. Mahnke, “OPC UA–service-oriented architecture
for industrial applications,” ABB Corporate Research Center, vol. 48,
no. 61-66, p. 22, 2006.

[9] S. Segura et al., “A survey on metamorphic testing,” IEEE Transactions

on software engineering, vol. 42, no. 9, pp. 805–824, 2016.

