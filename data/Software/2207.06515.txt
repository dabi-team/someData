Automated Cause Analysis of Latency Outliers
Using System-Level Dependency Graphs

Sneh Patel
sp18oo@brocku.ca
Brock University
St. Catharines, ON Canada
L2S 3A1

Brendan Park
bp18ul@brocku.ca
Brock University
St. Catharines, ON Canada
L2S 3A1

Naser Ezzati-Jivan
nezzati@brocku.ca
Brock University
St. Catharines, ON Canada
L2S 3A1

Quentin Fournier
quentin.fournier@polymtl.ca
Polytechnique Montr´eal
Montreal, QC Canada
H3T 1J4

2
2
0
2

l
u
J

3
1

]
F
P
.
s
c
[

1
v
5
1
5
6
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Detecting performance issues and identifying their
root causes in the runtime is a challenging task. Typically,
developers use methods such as logging and tracing to identify
bottlenecks. These solutions are, however, not ideal as they are
time-consuming and require manual effort. In this paper, we pro-
pose a method to automate the task of detecting latency outliers
using system-level traces and then comparing them to identify the
root cause(s). Our method makes use of dependency graphs to
show internal interactions between threads and system resources.
With these graphs, one can pinpoint where performance issues
occur. However, a single trace can be composed of a large number
of requests, each generating one graph. To automate the task of
identifying outliers within the dataset, we use machine learning
density-based models and statistical calculations such as Z-score.
Our evaluation shows an accuracy greater than 97% on outlier
detection, making them appropriate for in-production servers
and industry-level use cases.

Index Terms—Performance evaluation; latency outliers; outlier
detection; root cause analysis; execution tracing; dependency
graphs.

I. INTRODUCTION

Performance issues such as unexpected latency can neg-
atively affect a program. During the development phase,
debuggers and proﬁlers help detect and resolve potential per-
formance issues originating from bugs or poor design choices.
However, after the application has been released, new variables
that the developer may not have expected, such as different
workloads or a new piece of hardware, may cause performance
issues. Investigating latency issues of a released application
and identifying their potential root causes is known to be a
difﬁcult task. One method often used for performance issues
is execution trace-based runtime analysis.

Analyses are conducted either on-CPU or off-CPU de-
pending on the type of issue [1, 2]. When the analysis is
conducted on-CPU, any threads running on the CPU can be
analyzed. As a consequence, on-CPU analysis is only capable
of detecting issues that occur during the execution time on the
CPU. Such analysis is limited as it cannot detect performance
issues related to thread scheduling and thread interactions.
These performance anomalies can be detected with an off-
CPU analysis. Notably, off-CPU analysis can detect issues
related to hardware, I/O, network timers, and thread-waiting.

For instance, let us consider a Java program that performs nu-
merical computations with integers read from a local text ﬁle.
In this situation, the on-CPU analysis can detect performance
issues related to the calculation, while the off-CPU analysis
can detect performance issues associated with reading the ﬁle.
The proposed method in this paper is about analyzing latency
outliers using off-CPU analysis. We collect runtime execution
data by tracing the operating system kernel. We then convert
the collected trace data into dependency graphs to model the
internal structure of an execution. We base our latency and
root cause analyses on these graphs.

A trace represents all the functions and operations accessed
at
the layer traced, either the kernel or the user space,
during a program’s execution. Each time that a tracepoint1
is encountered during runtime, an event
is generated and
added to the trace. An event
is composed of a name, a
timestamp, and possibly many other arguments. For example,
the following event (thread1, syscall_open, file1,
cpu0, t1) corresponds to a ﬁle open system call, running on
core 0, called on thread 1 at time t1. The Linux kernel already
contains read-to-use tracepoints, making kernel-level tracing
straightforward. Furthermore, kernel trace data includes all the
interactions of a program with other threads/processes running
simultaneously on the system and the system resources ac-
cessed during the execution of a program. This makes kernel
tracing an excellent data source to investigate execution issues
such as performance and latency issues. However, given the
size of the collected trace data, they can be hard to analyze
and may introduce a high overhead to the system. Therefore,
there is a real need to introduce an efﬁcient and automatic
analysis tool for massive traces such as those collected at the
kernel level.

In this paper, we propose a method to meet that demand.
Our method automates the task of ﬁnding latency outliers in
system-level traces. Additionally, the method is able to identify
the root cause of latency issues by comparing individual
outliers with clusters of normal executions to ﬁnd shared
and distinct behaviours between them. Automating the task
of identifying outliers reduces the manual effort required
to analyze a trace dataset. To automate the task, we ﬁrst

1A tracepoint is a tracing macro added to the source code or binary code

©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works. DOI: 10.1109/QRS54544.2021.00054

 
 
 
 
 
 
ﬁlter out the parts of the trace that do not correspond to
the program. Using the ﬁltered trace data, we then construct
Waiting-Dependency Graphs (DepGraphs) [1], displaying the
internal interactions between threads and system resources.
Next, we use graph embedding techniques to convert
the
DepGraphs into ﬁxed-size vectors compatible with most off-
the-shelf machine learning algorithms. We use the vectors in
density-based clustering models to detect outliers. Finally, we
use our proposed merging and comparison algorithms to ﬁnd
the areas contributing to the latency issues. Our work in this
paper expands upon what was presented by Ezzati-Jivan et al.
[1]. The authors discuss converting system-level traces into
DepGraphs to reduce the effort required to analyze raw trace
data. In this paper, our main contribution is the automation
of analyzing DepGraphs for the outlier detection as well as
clustering them to use in discovering the root cause of the
outliers.

The remainder of this paper is organized as follows. Sec-
tion II reviews the related work. Section III explains our
proposed method for outlier detection and root cause analysis.
Section IV provides an evaluation regarding the automated
analysis along with a use case. Section V summarizes the
results and concludes this paper.

II. RELATED WORK

Performance is an essential consideration for many develop-
ers when designing their programs. For this reason, developers
often spend a signiﬁcant fraction of their time investigating
latency issues and optimizing their code. This section looks at
some of the existing methods designed to detect and analyze
performance issues.

The ﬁrst work is GAPP [3] which uses weighted criti-
cality measures to identify different serialization bottlenecks
in parallel Linux applications caused by an imbalance or
shared resource contention. GAPP can reveal a wide range of
bottleneck-related performance issues, and the authors show
how the extended Berkeley Packet Filter (eBPF) tracer can
make a lightweight-based proﬁler. When used in combination
with the bottleneck detector, it will pinpoint the lines of code
responsible for the bottleneck. However, proﬁlers are known to
be ineffective because they operate by averaging the metrics,
which may hide outliers [4].

Another method for bottleneck detection is using a causal
proﬁling technique. These techniques discover bottlenecks and
display the effects on the program were it to be optimized.
COZ [5] is a causal proﬁler that uses a virtual speed up
to perform causal proﬁling without actually optimizing the
program’s code. However, limitations to causal proﬁling do
exist. For instance, causal proﬁling only applies to user space
as it affects only the thread level. To address this issue, Ahn
et al. [6] introduced SCOZ, a system-wide causal proﬁler.
Nevertheless, SCOZ is also a proﬁler and thus ineffective when
it comes to outliers.

Similar to our proposed method, Inagaki et al. [7] uses
thread graphs to detect layered bottlenecks. They presented
a method to build a model that detects layered bottlenecks

by proﬁling threads and their dependencies in the target
system. This method is also capable of analyzing third-party
components. They detect layered bottlenecks by hierarchically
traversing the path with the most signiﬁcant threads in their
thread dependency graph. One limitation that remains in this
work is the manual effort required from the user, which our
method seeks to reduce.

Noble and Cook [8] introduces two new techniques for
graph-based anomaly detection. They present an algorithm
called Subdue that can ﬁnd repetitive patterns (substructures)
within graphs. For their ﬁrst approach, the authors use Subdue
to ﬁnd patterns within the dataset, and then any substructure
that is opposite of the patterns is declared to be an anomaly.
For their second approach, the authors separate the graph into
distinct structures and compare them to each other to ﬁnd
anomalous subgraphs. The limitation of these methods is that
Subdue can be very biased towards smaller graphs.

Lane and Brodley [9] propose machine learning algorithms
to detect anomalous behaviour within their program. Their
system learns a user proﬁle and subsequently applies it to de-
tect anomalous behaviours. The program compares the current
behaviours with the user proﬁles using a similarity measure.
To determine whether the current behaviour is abnormal, input
tokens are divided into ﬁxed-size segments and compared us-
ing a similarity measure. Using this technique, the authors aim
to detect intrusion attacks done by an outsider automatically.
Critical paths [10] can also be utilized to resolve perfor-
mance problems. A critical path can show the state of threads’
executions at different points of time to the user. A drawback
of critical paths is that they only show a limited view of the
execution rather than the complete process. For example, the
critical path of a thread that is waiting for another thread
to ﬁnish would not display the details of that other thread.
Dependency graphs, proposed by Ezzati-Jivan et al. [1] and
adapted in our work, can show all thread interactions and help
identify bottlenecks.

A common factor with existing tools for performance
anomaly detection is that they require some degree of manual
labour, making the analysis time-consuming and tedious. Our
proposed method uses Waiting-Dependency Graphs (Dep-
Graphs) to illustrate the interactions between different threads
and system resources (i.e., disk, CPU, network, etc.). Unlike
Ezzati-Jivan et al. [1], however, we aim to avoid manual
analysis of these DepGraphs. Instead, we want to automate the
process of detecting outliers within a dataset of DepGraphs.
To do so, we utilize machine learning algorithms over the
DepGraph data to identify outliers. After detecting outliers,
we can compare them with a group of normal DepGraphs to
identify the root causes of the anomalies.

III. METHODOLOGY

This section provides an in-depth explanation of the pro-
posed anomaly detection and root cause identiﬁcation method.
Using a kernel-level trace dataset, we extracted multiple re-
quests to convert them into DepGraphs. Each DepGraph shows
an overall view of the request’s internal behaviours, which can

then pinpoint latency issues. However, the analysis becomes
time-consuming as the number of DepGraphs increases. Our
proposed approach converts the DepGraphs into graph embed-
dings that are later used in machine learning clustering and
outlier detection algorithms to detect and investigate outliers
automatically. After identifying the outliers, we compare them
the exact
with clusters of normal executions to pinpoint
reasons behind the issues. In the following paragraphs, we
explain each step in more detail.

A. System-Level Data Collection

We use a low-overhead open-source Linux tracing tool
called the Linux Tracing Toolkit: next generation (LTTng [11])
to collect kernel-level
trace data from a Linux operating
system. The collected trace includes the full execution details
of all active threads of the system. A thread may handle several
tasks in its entire lifetime. We are, however, interested only in
some aspects of a group of active threads. We ﬁrst extract the
requests (e.g., web requests) handled by a web server thread
(or a group of similar threads, say Apache threads) from its
kernel execution trace data, and then we construct a DepGraph
for each request. This paper relies on the algorithm presented
by Ezzati-Jivan et al. [1] to extract the requests and construct
the DepGraphs from trace data. Although this paper focuses
on web requests, our method is generic enough to apply to any
execution (e.g., a button click, a compiler event, a graphical-
view rendering, or a network service).

B. Waiting-Dependency Graphs

A Waiting-Dependency Graph (DepGraph) [1] is a directed
graph whose nodes are labelled with the name of the thread,
system call, or other execution states and with their runtime
duration in milliseconds. In this graph, edges indicate the
percentage of time the source node spends waiting for the
destination node. DepGraphs show a visual representation
of a request’s internal execution breakdown. Including the
aggregated duration of a thread running on the CPU and the
aggregated duration of a thread waiting for a system resource
(e.g., disk, network, timer, CPU).

In previous works, developers would use critical paths to
observe how long each thread’s state takes [4]. A critical path
is a tool to display the execution states of a given thread at
any particular time. However, a critical path cannot generally
show why a thread is in a waiting state or how long a thread
has been waiting for a resource. One would need to do a
manual investigation to ﬁnd why a thread is in a waiting state.
If only a few threads exist investigating them is an easy task
to do. However, in a real-world scenario, there are usually a
large number of threads, making them tedious to investigate. In
contrast, DepGraphs display the overall breakdown of waiting
dependencies that cause latency issues. In essence, DepGraphs
are the aggregation of all interacting threads’ critical paths that
contribute to handling a request. Consider, for example, a web
server thread that is communicating with a database thread to
handle a web request. In this case, the DepGraph of the web
request is constructed by merging the critical paths of the two

threads. For more information regarding the construction of
DepGraphs we refer the reader to the original paper by Ezzati-
Jivan et al. [1].

DepGraphs allow ﬁnding latency issues and their root causes
by comparing the graphs of the different normal and abnormal
requests executions. To illustrate how DepGraphs can be used
to ﬁnd anomalies’ root cause, let us consider the example of a
web server that runs a PHP script to print all the table records
from a MySQL database. The script repeats this database
access three times. The command-line web client wget was
used to send two consecutive requests to the server. In this
example, one would expect the second call to be faster due to
client and server caching mechanisms. However, we observed
the opposite to be true. For this scenario, we have made four
DepGraphs (Figure 1): one for each client and server execu-
tion. Note that all edges with less than 3% have been removed
for readability since the edges with low values provide no sig-
niﬁcant latency to the execution. In Figure 1a, the DepGraph of
the client execution of the ﬁrst request shows that 83% of the
wget thread’s execution was spent waiting for the network. In
Figure 1b, the DepGraph of the ﬁrst request’s server execution,
we observe what occurred on the server during that time. Note
that although syscall_ftrucntate takes less than 3%
of the time, we preserved it for comparison with the second
request. Figure 1b shows that the majority of the server time
was spent waiting for the MySQL database to handle the
queries. For the second request, from the server’s DepGraph
shown in Figure 1d, we can see that the database requests only
take 700ms (compared to the 6298ms of the ﬁrst request). Such
a decrease in response time indicates that database ﬁles/queries
are (probably) cached in memory from the ﬁrst call, making
the second database call much more efﬁcient. Therefore, the
web/database server is not the bottleneck. The DepGraph of
the second request’s client execution (Figure 1c) explains
why the second request is slower than the ﬁrst request. The
latency is mainly due to a new state in the execution of the
wget client called ”Syscall ftruncate” where the wget waits
for the disk to write something while already busy serving
tasks from two other threads ([kworker/7:1H--307] and
[lttng-consumerd--17665]). This explains why the
total time of the second request has increased by almost three
times compared to the ﬁrst request.

As illustrated by the example, the analysis of DepGraphs,
although very useful, is manual and requires signiﬁcant human
labour to identify issues. In order to reduce the manual labour
behind analyzing these DepGraphs, we convert
them into
ﬁxed-size vectors and analyze them with machine learning
algorithms.

C. Graph2Vec

The proposed method is to analyze DepGraphs with ma-
chine learning algorithms automatically. Most off-the-shelf
machine learning algorithms require real-value vectors as
input;
thus, one must embed the graphs to use them as
input. Graph embedding is the approach of transforming
nodes, edges, and other graph features into a vector space.

(a) Client DepGraph of the ﬁrst request.

(b) Server DepGraph of the ﬁrst request.

(c) Client DepGraph of the second request.

(d) Server DepGraph of the second request.

Fig. 1: The Waiting-Dependency Graphs of two wget requests handled by PHP web server. (Top) The ﬁrst request behave as
expected. (Bottom) The second request is slower than the ﬁrst one, although MySQL cached the query.

For our proposed method, we used a technique introduced
by Narayanan et al. [12] called Graph2Vec. Given a set of
unidirectional graphs G = {G1...Gn}, Graph2Vec is able
to learn graph embeddings of any given size that contains
information pertaining to the graph’s topology. Given a dataset
of graphs, Graph2Vec attempts to explore all the nodes of
their sub-graph up to a certain degree. The technique follows
Doc2Vec [13] Skip-gram training process. It ﬁrst maps random
numerical values to the vector spaces, and then using the
Weisfeiler-Lehman (WL) relabeling strategy, they are trained
and reﬁned over several epochs.

To show an example of converting a DepGraph into a graph
embedding using Graph2Vec, we converted the DepGraph of
the previous example, shown in Figure 1d. Figure 2 displays
the input edges and nodes and the corresponding embedding
output. It is worth noting that similar graphs have similar
vector representations, and identical graphs have similar but
not
identical vector representations since embeddings are
randomly initialized. This shortcoming is not a concern for

our approach since the strong similarity is enough. Addition-
ally, Graph2Vec requires graphs with undirected edges, while
DepGraphs have directed edges. Therefore, to use Graph2Vec
with our dataset, we need to convert all the directed edges
into undirected edges, losing some information in the process.
Nonetheless, Graph2Vec is suitable for our method since it
preserves most of the relevant information.

D. Automatic Analysis

As explained in the previous section III-C, a graph em-
bedding is learned for each DepGraph in the kernel trace
dataset. Using outlier detection algorithms, we can automate
the process of ﬁnding outliers by clustering and analyzing
similar DepGraphs. After identifying the outliers, we use a
comparison algorithm to compare each outlier to a cluster of
normal executions to ﬁnd the differences, hence the cause of
the given outlier.

1) Outlier Detection: The deﬁnition of an outlier as pro-
vided by Hawkins [14] is: “an observation which deviates so

Syscall_ftruncate (42)Userspace (1046)[THREAD wget--1521] (10220)0%10%Syscall_select (8493)83%[network] (8478)100%[THREAD mysqld--1213] (6298)Userspace (6189)98%Userspace (241)[THREAD mysqld--21039] (172)[DISK] (5928)96%[kworker/7:1H--307] (5928)Syscall_poll (6511)97%3%100%[THREAD php--1445] (6908)3%94%[DISK] (22334)[lttng-consumerd--17665] (11287)51%[kworker/7:1H--307] (10519)47%[network] (3644)Userspace (1281)Syscall_sync_file_range (11287)100%[THREAD wget--1522] (28155)5%Syscall_select (3661)13%Syscall_ftruncate (22466)80%100%99%Userspace (216)[THREAD php--1445] (1936)11%Syscall_poll (1629)84%Syscall_sync_file_range (514)[THREAD mysqld--1214] (700)Userspace (649)93%43%[THREAD mysqld--1782] (890)55%[CPU] (98)11%Userspace (722)81%[DISK] (451)62%[lttng-consumerd--17665] (420)Syscall_sync_file_range (420)100%[DISK] (557)86%93%[lttng-consumerd--17665] (514)92%100%may provide inaccurate results with smaller datasets. However,
the sample size would not usually be an issue for our use
case since, for accurate Z-score calculations, the sample size
should be greater than 30. Z-score uses the mean and standard
deviation of the dataset to calculate its value. Outliers can
highly inﬂuence these values. Therefore, datasets with too
many outliers may also yield inaccurate results.

For the distance-based model, we used the k-nearest neigh-
bour (k-NN) algorithm. A data point is deﬁned as an outlier if
the distance between it and its k-nearest neighbour is greater
than a predeﬁned threshold. While the distance-based models
are easy to understand and implement, they are sensitive to
the distance metric.

Density-based models assume that outliers are points that
lie in a sparsely populated region of space. We consider
the Density-Based Spatial Clustering of Applications with
Noise (DBSCAN [16]) and Ordering Points To Identify the
Clustering Structure (OPTICS [17]) methods. Although these
methods were designed for clustering,
they are also able
to identify outliers as a byproduct. DBSCAN and OPTICS
work similarly to each other. Both algorithms require two
parameters: (cid:15) and m. (cid:15) is the maximum distance between
two data points for them to be neighbours, and m is the
minimum number of points required to create a cluster. The
algorithms start by selecting a random point x, then count
the points in its (cid:15)-neighbourhood. If the number of points is
greater than m, DBSCAN creates a new cluster with x and its
neighbours, whereas OPTICS creates an ordered list based on
the reach-ability distance by keeping a priority queue. If the
above condition is not satisﬁed, DBSCAN and OPTICS both
consider x to be an outlier. However, the neighbours of x are
considered and added to the cluster in DBSCAN, and OPTICS
will add it to its ordered list. After this process, OPTICS
and DBSCAN choose a new data point. When all data points
have been considered, DBSCAN and OPTICS terminate. It
is important to note that outliers declared by DBSCAN and
OPTICS are only points that lie in a low-density region.

Figure 4a and Figure 4b show a visualization of the Dep-
Graphs clustering with OPTICS and DBSCAN. From these
ﬁgures, we observe that both methods cluster points in a
similar manner, as they were able to get the same clusters
and outliers.

Our reasoning behind using DBSCAN and OPTICS is that
we can detect outliers with great accuracy and cluster the
rest of the dataset into likewise groups. The clusters created
by these algorithms are later used in Section III-D2. In
Section III-D2 we show our comparison method to compare
the outliers with the cluster of normal executions to ﬁnd the
difference between the two and possibly identify the root cause
of an outlier.

Evaluating the outlier detection is challenging since it is
an unsupervised problem. In the past, synthetically generated
datasets or a few rare aspects of a real dataset were often used
as proxies to evaluate unsupervised algorithms. Since external
criteria such as known labels have restricted requirements, one
might look for internal criteria for outlier validation. However,

Fig. 2: Converted the DepGraph shown in Figure 1d into an
embedding using Graph2Vec.

much from the other observations as to arouse suspicions that
it was generated by a different mechanism.”

Outliers may correspond to noise in the dataset, unusual
patterns or behaviours of interest, or anomalies such as latency,
intrusions, and bugs. In our paper, all the outliers detected
are latency-related issues. We consider a latency outlier to
be an execution/request with a high execution/response time.
Most of the requests in our dataset have a runtime lower
than 200ms (97.8% of the dataset). Any request that has a
runtime above 200ms is considered an outlier. Furthermore,
any request with unusual
internal nodes is considered an
outlier, even if the execution time is lower than 200ms. After
all, they represent a system resource, system call, or request
that has an unusual behaviour compared to the other requests
in the dataset. Using the outliers we are able to observe the
potential reason for the performance issue, since it displays the
internal threads/processes with high runtime or it might show
unnecessary threads/process that the request is using, making
it slow.

From our dataset, we show the DepGraph in Figure 3a as an
outlier compared to the DepGraph in Figure 3b. Figure 3a has
a runtime of 710ms and also contains elements that are unusual
compared to other DepGraphs in the dataset, such as thread
Xorg. Meanwhile, Figure 3b is a normal request since it only
has a runtime of 78ms, and all of the elements are similar to
other DepGraphs in the dataset. Later on, in Section III-D2,
we display the use of a comparison algorithm to compare the
outlier shown in Figure 3a with the other normal DepGraphs
to ﬁnd the differences between the two and ﬁnd the root cause
of the outlier.

We use three different models to detect outliers: Z-score,

distance-based model, and density-based model.

The Z-score approach indicates how many standard devi-
ations away a point lies from the sample’s mean value. We
decided to use Z-score because it is an effective method to
detect outliers on a Gaussian distribution [15] and because it
is easy to implement. The limitation of the Z-score is that it

012345678910111213Graph 1Edges: (0, 13)(1, 2)(2, 3)(3, 4)(4, 5)   (5, 12)(6, 11)(7, 8)(8, 9)   (9, 10)(10, 11)(11, 12)(12, 13)   Graph2Vec output: [ 0.01 -0.05](a) This DepGraph is considered to be an outlier because it contains novel elements compared to other DepGraphs in our dataset. Also it is
one of the only DepGraph with that big of a request time.

(b) This DepGraph is considered to be a normal request, since it has a runtime under 200ms and it contains similar elements as the other
clustered DepGraphs.

Fig. 3: Displays an example of a normal request and an outlier to differentiate between the two.

(a) DBSCAN

(b) OPTICS

Fig. 4: Results of two different clustering methods. The visualization is obtained by projecting the output of Graph2Vec in 2D
using t-SNE [18]. (a) DBSCAN is a clustering method that can produce outliers as a byproduct of clustering. It is not built
for outlier detection, however, it is often used for that. (b) OPTICS produces similar results to DBSCAN’s since they are both
density-based methods.

internal criteria are hardly ever used in outlier analysis because

they are known to be ﬂawed [19]. Internal criteria’s ﬂaw

sys:fcntl (2)No Stack (101)sys:getsockname (3)sys:read (8)sys:newfstat (2)sys:open (6)sys:newstat (5)sys:recvmsg (5)sys:setitimer (4)No Stack (227)sys:epoll_wait (11)sys:writev (11)sys:ioctl (319)[thread Xorg] (580)1%1%39%2%2%55%[waitcpu] (580)100%[thread apache2] (710)0%14%0%1%0%1%1%82%sys:fcntl (2)No Stack (58)sys:getsockname (1)sys:read (6)sys:newfstat (1)sys:open (3)sys:newstat (3)[thread apache2] (78)3%74%1%8%1%4%4%cluster 1 (654)cluster 2 (18)cluster 3 (6)outlier (19)cluster 1 (18)cluster 2 (654)cluster 3 (6)outlier (19)is that since outliers are rare by nature, a certain validity
measure might favour one outlier detection algorithm over
others. Therefore most of the validity measures used in outlier
analysis tend to be external measures.

Typically, unsupervised algorithms are evaluated with in-
ternal measures such as the value of a loss function on
the training set. Outlier detection algorithms may also be
evaluated with ground truths retrieved from synthetic or real
datasets. In that case, one may see the outlier detection task
as a supervised problem. Typically, soft methods return a
probability of belonging to the positive class, and a simple
decision threshold is used to make the classiﬁcation (outlier
or normal). Then, standard metrics such as accuracy, precision,
and recall may be computed.

An outlier detection algorithm’s primary goal is to declare
all positive outliers and ensure as few false positives and false
negatives as possible. In order to achieve this in a real-world
scenario, one may do a grid search on the decision threshold
value2 or a random search3.

The dataset used for this paper did not contain the ground
truths. As a solution, we considered slow requests as the only
outliers, although some fast requests could also contain un-
usual behaviours or patterns. The binary labels were generated
using a response time threshold set by an expert and used to
evaluate the different outlier detection methods. The threshold
was 200ms since the majority of the dataset was under 200ms.
Based on this threshold value 2.5% of the dataset was declared
as an outlier.

TABLE I: Accuracy, precision, recall of each outlier detection
method used.

# of outliers
Accuracy (%)
Precision (%)
Recall (%)
F1 (%)

DBSCAN
19
97.7
47.4
60
52.9

OPTICS
19
97.7
47.7
60
52.9

k-NN
17
97.1
35.3
40.0
37.5

Z-SCORE
20
98.1
55.0
73.3
62.9

Table I displays the accuracy, precision, recall, and f1
score of each outlier detection method considered (DB-
SCAN, OPTICS, Z-Score, and k-means). Each of these
two parameters ground truths and
metrics
predictions. While the ground truth remains the
the predictions are different
same for each method,
depending on the outlier detection method.

requires

The results obtained from this evaluation methodology
should be considered with caution since detected outliers may
correspond to anomalies unrelated to latency, in which case
we acknowledge the prediction to be erroneous. The accuracy
measures the ratio of correctly predicted samples, both normal
and outliers, while the precision and recall only consider the
outliers. The f1 score is a harmonic mean of precision and

2A grid search exhaustively considers all parameter combinations for the

set of values speciﬁed.

3A random search samples a given number of parameter combinations from

the distributions speciﬁed.

recall. The table allows us to determine which outlier detection
methods were successful in detecting slow requests.

2) Comparing DepGraphs: So far, using the proposed
outlier detection methods, we have been able to ﬁnd anomalies
and group similar DepGraphs. Now using these groups and
outliers, we would like to ﬁnd the root cause of these anoma-
lies. This is achieved by ﬁrst merging all the DepGraphs in
each cluster. By merging all the DepGraphs in a cluster, we
create a new DepGraph representing the entire cluster. We can
then use the comparison algorithm to discover the differences
between the outliers and the merged graphs.

Algorithm 1 Method used to merge the cluster

1: procedure MERGE(f ile)
2:

Data ← f ormatData(path, f unction(f ile))

(cid:46)
Collect all the executions and make sure, there are no
loose nodes

3:
4:
5:
6:
7:

8:

9:
10:
11:
12:

13:
14:
15:
16:
17:
18:

19:
20:
21:
22:
23:

24:
25:
26:
27:

28:

29:
30:
31:

max = dictionary()
min = dictionary()
count = dictionary()
size = dictionary()
for each execution in data do

for each node in the given execution do
the node if and only if its distinct else add the value
if nodePath is not in counts then

(cid:46) Add

count[nodeP ath] = 1

else

count[nodeP ath] = count[nodeP ath] + 1

end if
if nodePath is not in min then

min[nodeP ath] = nodeP ath[min]
else if nodePath[min] < min[nodePath] then
min[nodeP ath] = nodeP ath[min]

end if
if nodePath is not in max then

max[nodeP ath] = nodeP ath[max]
else if nodePath[max] > max[nodePath] then
max[nodeP ath] = nodeP ath[max]

end if
if nodePath is not in sizes then

sizes[nodeP ath] = nodeP ath[size]

else

sizes[nodeP ath] = sizes[nodeP ath] +

nodeP ath[size]
end if

end for

end for
CREATEGRAPH(size, count, min, max)
the new distinct nodes to make a new DepGraph

(cid:46) Pass on

32: end procedure

Algorithm 1 displays the method used to merge the clusters.
As input, it takes a ﬁle comprising of the DepGraphs to merge.
After retrieving the ﬁle, the algorithm formats the data by
extracting all the information of each execution and ensuring
no nodes without a parent exist. Then, the algorithm analyzes

each node from each execution (line 7). If the node is unseen,
the dictionary counts would add it; otherwise, it would
increase the count of that node, indicating that the node is
repeated (lines 9-13). The algorithm also records the minimum
and maximum values of all similar nodes. Therefore if a node’s
size is less than the current size, it will be updated as the new
minimum. Similarly, if a node’s size is greater than the current
size, it will be updated as the new maximum. If it is a distinct
node, its value becomes the maximum and the minimum (lines
14-23). Furthermore, the algorithm stores the sizes of each
distinct node. If the node already exists, it adds its value with
the new node’s value (lines 24-28). Using the size, count,
min, and max (line 31) dictionaries, the algorithm is able to
make a DepGraph that has the id and name of all distinct
nodes, the cumulative size of all repeated nodes, the number
of repeated nodes (count), the max and min values from all
the common nodes, and the path.

We then use the comparison algorithm shown in Algo-
rithm 2 to compare the merged graph (i.e., the representative
graph of the normal cluster) and the DepGraph of the outlier
request. As input, the algorithm takes two ﬁles, each con-
taining a DepGraph to compare. In our case, one DepGraph
represented the clusters, and the other represented the outlier.
After retrieving both ﬁles, the algorithm extracts all the nodes
from both executions and merged their data (counts, max,
min, and size) using a similar method shown in Algorithm 1.
Next, the algorithm gets the total number of repetitions for
each node from both executions (lines 12-22). Using the
totCount, the algorithm calculates the mean count of each
node in the ﬁrst execution and calculates the standard deviation
of the ﬁrst DepGraph (leftSds). Later it repeats the process
and calculates the mean count of each node in the second
execution (lines 23-30). With the two means, the algorithm can
ﬁnd the differences between the means, which is needed to ﬁnd
the similarities between the two DepGraphs. The algorithm
iterates over each difference in diffMeans and calculates
the difference between the standard deviation of the means. If
the two nodes are present in both the DepGraphs, the algorithm
uses getBoldness method to compare each node’s count.
Depending on the standard deviation, it can determine the
the
level of boldness. The higher the standard deviation,
higher the difference between the counts of the two nodes.
We use boldness to show the differences in the value of the
corresponding nodes of the given graphs. The getBoldness
method uses ﬁve different boldness levels. The larger the value
difference, the higher the boldness level of the edges. The
algorithm can also show distinct nodes that may only appear
in one of the two executions. If the node is only present in
the ﬁrst DepGraph, it will have a dashed edge, and if the node
is only present in the second DepGraph, it will have a dotted
edge. The resulting graph will depict the exact differences (i.e.,
value differences between corresponding nodes and existence
of a node in only one tree) between the two given DepGraphs,
useful in identifying the root cause(s) of the outlier request.

To display an example of using the merge and comparison
algorithms to ﬁnd root causes of anomalies, we will compare

the outlier shown in Figure 3a and the normal DepGraph in
Figure 3b. Using OPTICS, we can cluster the normal Dep-
Graph in cluster 2 and identify the outlier. Before comparing
the cluster with the outlier, we ﬁrst need to merge the entire
cluster into one DepGraph since the comparison algorithm can
only take two DepGraphs as input. Using the Algorithm 1, we
merged cluster 2 into a single representative DepGraph with
all distinct nodes. Figure 5 displays the ﬁnal DepGraph after
merging all 654 (normal) DepGraphs in the cluster. In this
ﬁgure, each node has a name, the total cumulative size of
all the same nodes, the maximum and minimum of all the
same nodes (i.e., the execution/run time of that node), and the
number of repeated nodes. For example, in the DepGraph, the
node thread apache2 shows a count of 654, which means
that 654 DepGraphs had the same node in their DepGraphs.
Out of the 654 nodes, the highest value was the runtime of
306ms, and the lowest value was the runtime of 60ms. The
node also shows a cumulative size of 69,791, which allows
computing the percentage of time spent by the thread in the
node. With the single merged DepGraph that represents cluster
2, we can now compare it with the outlier in Figure 3a using
the comparison Algorithm 2. Using the Algorithm 2, we get
the comparison graph shown in Figure 6.

The comparison graph shown in Figure 6 depicts all distinct
and common nodes from both of the DepGraphs. Nodes
with dotted edges are from the ﬁrst DepGraph (representing
cluster 2), and nodes with dashed edges are from the second
line
DepGraph (outlier). If an edge is a simple straight
(possibly with boldness level), it represents that the node is
present in both the DepGraphs. In Figure 6, we can observe
that most nodes are present in both DepGraphs. However,
waitcpu is only present in the second DepGraph (outlier).
By investigating the waitcpu, we can identify nodes (e.g.,
other active thread(s), which is the thread xorg here) that
take up the majority of the CPU time while the main thread is
waiting to get the CPU. In other words, this dash-lined node
from the difference graph shows that the CPU was utilized
by xorg thread, and consequently, it is not available for the
apache2 thread to handle the web request causing it to get
blocked and wait longer than usual to get the CPU. We can
conclude that this is one of the reasons why this request is an
outlier. There might be, however, some other reasons that can
be seen from the difference graph.

A. Computational Cost

IV. EVALUATION

1) Setup: The trace used for our example above was
collected using LTTng 2.11 on a workstation equipped with
32 GB of RAM and a quad-core Intel® Core™ i7-6700K
CPU @ 4.00 GHz. The operating system was Linux Ubuntu
16.04.6 LTS with the 64-bit kernel 4.15.0-62. The local hard
disk reference 7200 RPM WDC WD1003FZEX-0 stored the
trace data and trace analysis results.

2) Data: The trace data was collected using the LTTng
tracer and comprised 697 requests. The trace contains the
execution of an Apache web server handling a PHP web

Algorithm 2 Method to compare the merged cluster DepGraph and the outlier.

1: procedure MERGE(f ile1, f ile2)
2:
3:

data1 ← f ormatData(f ile1)
data2 ← f ormatData(f ile2)
totCount = dictionary()
means1 = dictionary()
means2 = dictionary()
lef tCounts = []
meanDif f Sd = dictionary()
dif f M eans = dictionary()
Merge the counts, max, min and size for the data1 and data2
for each node in execution1 do

(cid:46) For loop on line 7 in Merge algorithm 1

if node[path] in totCount then

totCount[node[path]] = totCount[node[path]] + node[count]

else totCount[node[path]] = node[count]
end if

end for
for each node in execution2 do

if node[path] in totCount then

totCount[node[path]] = totCount[node[path]] + node[count]

else totCount[node[path]] = node[count]
end if

end for

for node in execution1 do

(cid:46) mean of each node count is count/totalcount for that node

means1[node[path]] = node[count]/totCount[node[path]]
lef tCounts.append(node[count])

end for
lef tSds = standardDeviation(lef tCounts)
for node in execution2 do

means2[node[path]] = node[count]/totCount[node[path]]

end for
for mean in means1 do

if mean is in means2 then

dif f M eans[means] = means[mean] − means[mean]

end if

end for
for diff in diffMeans do

if leftSds != None then

meanDif f Sd[dif f ] = dif f M eans[dif f ]/lef tSds

else

meanDif f Sd[dif f ] = inf inity

end if

end for
for x in meanDiffsd do

boldness[x] = getBold(meanDif f Sd[x])

end for

CREATEGRAPH(M ergedmin, M ergedmax, M ergedcount, M ergedsize, boldness)

46:
47: end procedure

(cid:46) since the comparison graph has all nodes from both depgraphs it needs the merged data we got at line 10

4:
5:
6:
7:
8:
9:

10:
11:
12:
13:
14:
15:

16:
17:
18:
19:
20:
21:

22:

23:
24:
25:

26:
27:
28:
29:
30:
31:

32:
33:
34:
35:
36:
37:

38:
39:
40:
41:
42:
43:

44:
45:

Fig. 5: This DepGraph is a result of merging all the DepGraphs in cluster 2 from the OPTICS clustering method.

Fig. 6: This is a comparison graph that compares the cluster 2 from OPTICS and the outlier shown in Figure 3a

application. The internal execution of each request is reﬂected
in a distinct DepGraph. Therefore, there are 697 DepGraphs in
the dataset. Our observation shows that 97.8% of all requests
have a runtime between 0 and 200ms (considered as normal
executions), as shown in Figure 7. Therefore, for this dataset,
we consider any request and corresponding DepGraph that has
a runtime below 200ms to be a normal request and the rest to
be slow requests. Normal requests can still contain anomalies
that are unrelated to latency issues. More details of the dataset
and the constructed DepGraphs can be found in Ezzati-Jivan
et al. [1].

Fig. 7: Histogram of the request duration in ms.

3) Tracing and DepGraph Construction Cost: In the worst-
case scenario of tracing the Linux kernel where all kernel
tracepoints are enabled, which is not the case of our method,
the overhead imposed by tracing is 42%. In our method, when
collecting the trace to construct DepGraphs, only system calls
and events needed to extract the DepGraphs are enabled. Our
analyses show that the overhead imposed by the tracing never
exceeds 10.1%. After obtaining the trace, the DepGraphs were
created. When creating the DepGraphs, one needs to analyze
the trace, which is done ofﬂine after collecting it. Therefore
it does not have any overhead on the application and is not
considered in our time analysis.

4) Automated-Analysis: Table II displays the time taken for
various tasks in Section III-D. The table shows the time it
took to load the dataset, make the graph embeddings, and

run each outlier detection and clustering method. From that,
we observed that DBSCAN took the least amount of time to
cluster and detect outliers. The table also shows how long it
took to merge a cluster into one DepGraph and compare it with
an outlier. For this table, we measured the time taken to merge
cluster 2 and the time taken to compare it with the outlier
shown in Figure 3a. Here we notice that the majority of the
time was spent constructing the DepGraphs and embedding,
whereas everything else took less than a second to do.

The overhead imposed by our method is modest compared
to the cost of tracing and certainly negligible compared to the
time required by a human operator to investigate each request
manually.

B. Limitation

The proposed method described in this paper is heavily
reliant on Graph2Vec used to get our ﬁxed-size representation
of a trace. However, it has a few limitations. When training
the Skip-gram model to produce the embeddings, it is primar-
ily reliant on the embedding size and the hyper-parameters
wl_iterations and epochs. Therefore when we increase
the epochs and iterations, the time it takes to train the model
also increases. If a signiﬁcant portion of the graphs used to
train the model is large, that can also increase the time it
takes to train the model. Also, to train the model to embed, a
minimum number of graphs are needed. If there is less than
the minimum threshold, we run into a small sample problem,
resulting in overﬁtting.

Another limitation our proposed method has is that it can
only detect off-CPU issues and cannot detect any issues that
happen at
the on-CPU level. Since our proposed method
focuses on ﬁnding latency issues within dependency graphs,
this is not an issue. However, one cannot use our method to
detect issues within their code (on-CPU analysis). To do so,
we would need to get data from on-CPU proﬁlers. Since there
are no interactions between kernel space and user space, one
cannot get information about the user space functions from
the kernel level.

V. CONCLUSION AND FUTURE WORK

Our proposed method is able to automatically detect outliers
and their root causes from within a system-level trace data.
Using our method, we remove the need to manually examine
all the trace data and the abstract data (such the dependency

('[thread apache2]', '(69791)', 'Max 306', 'Min 60', 'Count 654')('sys fcntl', '(1285)', 'Max 4', 'Min 1', 'Count 654')2%('no stack', '(52205)', 'Max 245', 'Min 42', 'Count 654')75%('sys getsockname', '(1324)', 'Max 15', 'Min 1', 'Count 654')2%('sys read', '(5325)', 'Max 24', 'Min 5', 'Count 654')8%('sys newfstat', '(1203)', 'Max 14', 'Min 1', 'Count 654')2%('sys open', '(3407)', 'Max 21', 'Min 3', 'Count 654')5%('sys newstat', '(2995)', 'Max 23', 'Min 2', 'Count 654')4%('[thread apache2', '(70501)', 'Max 69791', 'Min 710', 'Count 710')('sys fcntl', '(1287)', 'Max 1285', 'Min 2', 'Count 2')('no stack', '(52306)', 'Max 52205', 'Min 101', 'Count 101')('sys getsockname', '(1327)', 'Max 1324', 'Min 3', 'Count 3')('sys read', '(5333)', 'Max 5325', 'Min 8', 'Count 8')('sys newfstat', '(1205)', 'Max 1203', 'Min 2', 'Count 2')('sys open', '(3413)', 'Max 3407', 'Min 6', 'Count 6')('sys newstat', '(3000)', 'Max 2995', 'Min 5', 'Count 5')('[waitcpu]', '(580)', 'Max 580', 'Min 580', 'Count 580')('[thread xorg]', '(580)', 'Max 580', 'Min 580', 'Count 580')('sys recvmsg', '(5)', 'Max 5', 'Min 5', 'Count 5')('sys setitimer', '(4)', 'Max 4', 'Min 4', 'Count 4')('no stack', '(227)', 'Max 227', 'Min 227', 'Count 227')('sys epoll_wait', '(11)', 'Max 11', 'Min 11', 'Count 11')('sys writev', '(11)', 'Max 11', 'Min 11', 'Count 11')('sys ioctl', '(319)', 'Max 319', 'Min 319', 'Count 319')0100200300400500600700800Request duration (ms)0100200300400500Number of requests24871151434TABLE II: Time taken by each task.

Data Processing

Cluster

Outlier Detection

Merging clusters

Compare outlier with cluster

Load DepGraphs
DepGraph Embeddings w/ Graph2Vec
DBSCAN
OPTICS
DBSCAN
OPTICS
k-NN
Z-Score
Load the cluster
Merge DepGraphs in cluster
Construct DepGraphs
Load the two DepGraphs
Merge both execution for details
Compare the two DepGraphs
Construct comparison graph

Time (s)
0.076
11.608
0.020
0.325
0.017
0.275
0.075
0.025
0.027
0.033
32.391
0.001
0.001
0.002
0.672

graphs) extracted from them. Furthermore, with our approach
to compare the declared outliers with the normal executions,
we can identify unusual activities contributing to the latency.
This method makes the process of analyzing the software ex-
ecution trace more efﬁcient for developers, as automating the
process reduces the time taken for debugging and gets the task
done to ﬁnd potential bottlenecks in the program. Additionally,
having an automated process removes the potential for human
error in the analysis. For future work, we would like to extend
our approach by reducing the graph embedding size while still
retaining the level of accuracy (or potentially increasing it). By
reducing the dimension of the vectors, we can train the models
faster while increasing the number of dependency graphs.

REFERENCES

[1] N. Ezzati-Jivan, Q. Fournier, M. R. Dagenais, and
A. Hamou-Lhadj, “Depgraph: Localizing performance
bottlenecks in multi-core applications using waiting de-
pendency graphs and software tracing,” in 2020 IEEE
20th International Working Conference on Source Code
Analysis and Manipulation (SCAM).
IEEE, 2020, pp.
149–159.

[2] F. Zhou, Y. Gan, S. Ma, and Y. Wang, “wperf: Generic
off-cpu analysis to identify bottleneck waiting events,”
in 13th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 18).
Carlsbad,
CA: USENIX Association, Oct. 2018, pp. 527–543.
[Online]. Available: https://www.usenix.org/conference/
osdi18/presentation/zhou

[3] R. Nair and T. Field, “Gapp: A fast proﬁler for detecting
serialization bottlenecks in parallel linux applications,” in
Proceedings of the ACM/SPEC International Conference
on Performance Engineering, 2020, pp. 257–264.

[4] Q. Fournier, N. Ezzati-jivan, D. Aloise, and M. R.
Dagenais, “Automatic cause detection of performance
problems in web applications,” in 2019 IEEE Interna-
tional Symposium on Software Reliability Engineering
Workshops (ISSREW).

IEEE, 2019, pp. 398–405.

[5] C. Curtsinger and E. D. Berger, “Coz: Finding code that
counts with causal proﬁling,” in Proceedings of the 25th
Symposium on Operating Systems Principles, 2015, pp.
184–197.

[6] M. Ahn, D. Kim, T. Nam, and J. Jeong, “Scoz: A system-
wide causal proﬁler for multicore systems,” Software:
Practice and Experience, 2020.

[7] T. Inagaki, Y. Ueda, T. Nakaike, and M. Ohara, “Proﬁle-
based detection of layered bottlenecks,” in Proceedings
the 2019 ACM/SPEC International Conference on
of
Performance Engineering, 2019, pp. 197–208.

[8] C. C. Noble and D. J. Cook, “Graph-based anomaly
detection,” in Proceedings of the Ninth ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, ser. KDD ’03. New York, NY,
USA: Association for Computing Machinery, 2003,
p. 631–636. [Online]. Available: https://doi.org/10.1145/
956750.956831

[9] T. Lane and C. E. Brodley, “An application of machine
learning to anomaly detection,” in Proceedings of the
20th National Information Systems Security Conference,
vol. 377. Baltimore, USA, 1997, pp. 366–380.

[10] F. Doray and M. Dagenais, “Diagnosing performance
variations by comparing multi-level execution traces,”
IEEE Transactions on Parallel and Distributed Systems,
vol. 28, no. 2, pp. 462–474, 2016.

[11] M. Desnoyers and M. R. Dagenais, “The lttng tracer:
A low impact performance and behavior monitor for
gnu/linux,” in OLS (Ottawa Linux Symposium) 2006,
2006, pp. 209–224.

[12] A. Narayanan, M. Chandramohan, R. Venkatesan,
L. Chen, Y. Liu, and S. Jaiswal, “graph2vec: Learning
distributed representations of graphs,” CoRR, vol.
abs/1707.05005, 2017. [Online]. Available: http://arxiv.
org/abs/1707.05005

[13] Q. V. Le and T. Mikolov, “Distributed representations of
sentences and documents,” CoRR, vol. abs/1405.4053,

2014. [Online]. Available: http://arxiv.org/abs/1405.4053
Springer,

[14] D. M. Hawkins, Identiﬁcation of outliers.

1980, vol. 11.
Santoyo,

[15] S.

“A brief

outlier
[Online]. Avail-
https://towardsdatascience.com/a-brief-overview-

detection techniques,” Nov. 2017.
able:
of-outlier-detection-techniques-1e0b2c19e561

overview of

[16] M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., “A
density-based algorithm for discovering clusters in large
spatial databases with noise.” in Kdd, vol. 96, no. 34,
1996, pp. 226–231.

[17] M. Ester, “Density-based clustering.” Data Clustering:
Algorithms and Applications, vol. 31, p. 111, 2013.
[18] L. van der Maaten and G. Hinton, “Visualizing data
using t-sne,” Journal of Machine Learning Research,
vol. 9, no. 86, pp. 2579–2605, 2008. [Online]. Available:
http://jmlr.org/papers/v9/vandermaaten08a.html

[19] C. C. Aggarwal, Data mining: the textbook.

Springer,

2015.

