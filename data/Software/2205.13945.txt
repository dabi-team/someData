2
2
0
2

y
a
M
7
2

]
E
S
.
s
c
[

1
v
5
4
9
3
1
.
5
0
2
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

1

Nighthawk: Fully Automated Localizing
UI Display Issues via Visual Understanding

Zhe Liu, Chunyang Chen, Junjie Wang, Yuekai Huang, Jun Hu, and Qing Wang

Abstract—Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they
can interact with each other. With the upgrading of mobile devices and the development of aesthetics, the visual effects of the GUI are
more and more attracting, and users pay more attention to the accessibility and usability of applications. However, such GUI complexity
posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text
overlap, component occlusion, missing image always occur during GUI rendering on different devices due to the software or hardware
compatibility. They negatively inﬂuence the app usability, resulting in poor user experience. To detect these issues, we propose a fully
automated approach, Nighthawk, based on deep learning for modelling visual information of the GUI screenshot. Nighthawk can
detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to ﬁx the bug. At
the same time, training the model needs a large amount of labeled buggy screenshots, which requires considerable manual effort to
prepare them. We therefore propose a heuristic-based training data auto-generation method to automatically generate the labeled
training data. The evaluation demonstrates that our Nighthawk can achieve average 0.84 precision and 0.84 recall in detecting UI
display issues, average 0.59 AP and 0.60 AR in localizing these issues. We also evaluate Nighthawk with popular Android apps on
Google Play and F-Droid, and successfully uncover 151 previously-undetected UI display issues with 75 of them being conﬁrmed or
ﬁxed so far.

Index Terms—UI display, Mobile App, UI testing, Deep Learning, Object Detection

(cid:70)

1 INTRODUCTION

G RAPHIC User Interface (GUI, also short for UI) plays

an important role ubiquitous in almost all modern
desktop software and mobile applications. It provides a
visual bridge between a software application and end users
through which they can interact with each other. Developers
design a UI that requires proper user interaction, informa-
tion architecture and visual effects of the UI. Therefore, a
good GUI design makes an application easy, practical and
efﬁcient to use, which signiﬁcantly affects the success of the
application and the loyalty of its users [1].

However, with the improvement of mobile device per-
formance and user’s aesthetic requirements for UI, more
and more fancy visual effects in GUI design such as in-
tensive media embedding, animation, light, ﬂoating and
shadows post a great challenge for developers in the imple-
mentation. Consequently, many display issues1 such as text
overlap, missing image, component occlusion as seen in Figure 1

• Zhe Liu is with the Laboratory for Internet Software Technologies, In-
stitute of Software Chinese Academy of Sciences, University of Chinese
Academy of Sciences, Beijing, China. E-mail: liuzhe181@mails.ucas.ac.cn
• Chunyang Chen is with the Monash University, Melbourne, Australia.

•

E-mail: Chunyang.chen@monash.edu
Junjie Wang(corresponding author) and Qing Wang(corresponding au-
thor) are with the Laboratory for Internet Software Technologies, State
Key Laboratory of Computer Sciences, Science & Technology on Integrated
Information System Laboratory, Institute of Software Chinese Academy of
Sciences, University of Chinese Academy of Sciences, Beijing, China. E-
mail: junjie@iscas.ac.cn, wq@iscas.ac.cn

• Yuekai Huang and Jun Hu are with the Laboratory for Internet Software
Technologies, Institute of Software Chinese Academy of Sciences, Univer-
sity of Chinese Academy of Sciences, Beijing, China.

Manuscript received April xx, xxxx; revised August xx, xxxx.

1. we call these bugs as UI display issues, and will interchangablely

use bug and issue in this paper.

always occur during the UI display process especially on
different mobile devices [2], [3].

In particular, we ﬁnd that most of those UI display issues
are caused by different system settings in different devices,
especially for Android, as there are more than 10 major
versions of Android OS running on 24,000+ distinct device
models with different screen resolutions [4]. Although the
software can still run along with these bugs, they nega-
tively inﬂuence the ﬂuent usage with the app, reduce the
accessibility and usability of the application, resulting in the
signiﬁcantly bad user experience and corresponding loss of
users. Therefore, this study is targeting at detecting those UI
display issues.

Fig. 1: Examples of UI display issues

At present, to ensure the correctness of UI display,
companies have to recruit many testers for app GUI testing
or leverage the crowdtesting. Although human testers can
spot these UI display issues, there are still two problems
with such mechanism. First, it requires signiﬁcant human

 
 
 
 
 
 
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

2

effort as testers have to manually explore tens of pages
by different interactive ways and also need to check the
UI display on different OS versions and devices with dif-
ferent resolution or screen size. Second, some errors in
the GUI display, especially relatively minor ones such as
text overlap, component occlusion, are often ignored by the
testers. To overcome those issues, some app development
teams adopt the Rapid Application Development (RAD) [5],
which focuses on developing applications rapidly through
frequent iterations and continuous feedback. They utilize
users’ feedback to reveal UI display issues, but it is a reactive
way for bug ﬁxing which may have already hurt users of the
app, resulting in the loss of market shares.

Therefore, in comparison with obtaining feedback from
users for reactive app UI assurance, we need a more proac-
tive mechanism which could check the UI display before the
app release, automatically spot the potential issues in the
GUI, and remind the developers to ﬁx issues if any. There
are many research works on automated GUI testing [6],
[7], [8], [9], [10], [11], [12], [13], [14], [15], [16] by dynam-
ically exploring different pages with random actions (e.g.,
clicking, scrolling, ﬁlling in the text) until triggering the
crash bugs or explicit exceptions. Some practical automated
testing tools like Monkey [17], [18], Dynodroid [19] are also
widely used in industry. However, these automated tools
can only spot critical crash bugs, rather than UI display
issues which cannot be captured by the system. In this work,
we propose a method to automatically detect GUI display
issues via visual understanding.

To have a preliminary understanding of the common
UI rendering issues, we ﬁrst carry out a pilot study on
10,330 non-duplicate screenshots from 562 mobile applica-
tion crowdtesting tasks to observe display issues in these
screenshots. Results show that a non-negligible portion
(43.2%) of screenshots are of display issues which can se-
riously impact the user experience, and degrade the repu-
tation of the applications. Besides, we also examine 1,432
screenshots from 200 random-chosen applications in the
commonly-used Rico dataset [20], and ﬁnd 8.8% apps hav-
ing UI display issues. The common categories of UI display
issues include component occlusion, text overlap, missing image,
null value and blurred screen. We use static analysis of XML
ﬁle to analyze 100 screenshots (50 buggy screenshots and 50
bug-free screenshots) including component occlusion, text
overlap and null value issues, since the missing image and
blurred screen issues cannot be detected by static analysis
of XML ﬁle. The results show that the precision of static
analysis method is 0.32, and the recall is 0.36. The result
shows that UI display issues cannot be completely detected
only by static analysis. Considering its popularity and lack
of support in current practice of automatic UI testing, it
would be valuable for classifying the screenshots with UI
display issues from the plenty of screenshots generated
during UI testing.

In our previous work, we proposed OwlEye [21], a UI
display issue detection and localization approach. It adopts
convolutional neural networks (CNN) to detect issues in
screenshots, and uses Grad-CAM visualizes the issue lo-
cation. However, OwlEye has two major limitations which
seriously effected the practicality of widely deploying this
method. First, the localized issue area by OwlEye is usually

too large, thus can hardly accurately guide the follow-up
bug ﬁxing, especially the automatic bug ﬁxing. Second, the
OwlEye relies on large number of GUIs with display issues,
and collecting the data is time-consuming.

To overcome the drawback of OwlEye and further im-
prove its performance, this paper propose an extension,
name Nighthawk2, to model the visual information by deep
learning to automatically detect and localize UI display
issues. This paper formulate display issue detection as an
object detection task. We adopt the Faster-RCNN model to
not only identify screenshots with UI display issues but
accurately point out the location of the visual issues within
the screenshot, which better helps developers and testers
debug their GUI code.

Training the model needs large amount of buggy screen-
shots, which requires considerable manual effort to pre-
pare them. We therefore propose a heuristic-based training
data auto-generation method to automatically generate the
buggy screenshots. This is done through localizing and
modifying the UI components related information (e.g., size
of the TextView) in the JSON ﬁle of the bug-free screen-
shots. Compared with our previous work, this method
takes into account more restrictions (the text size and more
components types) of the screenshots, so that the gener-
ated screenshots with issues are more realistic and diverse.
Through user experiments, our latest issue data generated
by Nighthawk is closer to the real issue data than the data
expansion method in OwlEye, and our data generated by
Nighthawk improves the performance of the model better
than the method in OwlEye.

We train the model on 64,000 generated screenshots from
30,000 Android apps, and evaluate its effectiveness on 1,600
screenshots from crowdtesting and 8,000 augmented screen-
shots. We ﬁrst evaluated the performance of our method in
bug detection. Compared with OwlEye and 13 other state-
of-the-art baselines, our Nighthawk can achieve more than
5% and 6% boost in recall and precision compared with
OwlEye, and at least 17% and 23% boost in recall and
precision compared with other baselines, resulting in 0.84
precision and 0.84 recall. We further compare its localization
results with OwlEye. The average precision and average
recall of Nighthawk are 55% and 56% higher than those
of OwlEye, with AP of 0.59 and AR of 0.60.

Apart from the accuracy of our Nighthawk, we also
evaluate the usefulness of our Nighthawk by applying it
in detecting the UI display issues in the real-world apps
from Google play and F-Droid. Among 1328 apps, we ﬁnd
that 151 of them are with UI display issues. We reported bug
reports to the development team and 75 are conﬁrmed and
ﬁxed by developers.

This paper is an extended version of our earlier
study [21]. The extension makes the following additional
contributions:

• We adopt the Faster-RCNN model to not only identify
buggy GUI screenshot, but accurately point out the
location of display issues within the screenshot, which

2. Our approach is named Nighthawk as it is like the nighthawk
to effectively spot and localize UI display issues. And our model
(catching small bugs at night like a nighthawk) can complement with
conventional automated GUI testing (diurnal like eagle) for ensuring
the robustness of the UI.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

3

Fig. 2: Examples of ﬁve categories of UI display issues

better helps developers and tester debug their GUI
code.

• To avoid the requirement of large-scale manual label-
ing, we propose a heuristic-based training data auto-
generation method to automatically generate diverse
and realistic screenshots with UI display issues from
bug-free UI screenshots.

• We carry out experiments on a large-scale dataset to
verify that the Nighthawk can automatically train
and detect UI display issues without manual anno-
tation with promising results. At the same time, we
also evaluate the issue localization performance of the
Nighthawk and the impact of the number of datasets
on the performance of the Nighthawk.

• We release the implementation of Nighthawk3, the de-
tailed experimental results, and the large-scale dataset
of app UIs with four kinds of issues and issue lo-
calization information, for other researchers’ follow-up
studies.

2 MOTIVATIONAL STUDY
In order to get a better understanding of the UI displaying
issues in real-world practice, we carry out a pilot study
to examine the prevalence of these issues. The pilot study
also explores what kinds of UI display issues exist, so as to
facilitate the design of our approach for detecting UIs with
display issues.

2.1 Data Collection

Our experimental dataset is collected from one of the
largest crowd-testing platforms4 in which crowd workers
are required to submit test reports after performing testing
tasks [22], [23], [24]. The dataset contains 562 Android
mobile application crowdtesting tasks between January 2015
and September 2016. These apps belong to different cate-
gories such as news, entertainment, medical, etc. In each
task, crowd workers submit hundreds of testing reports
which describe how the testing is conducted and what hap-
pened during the test, as well as accompanied screenshots of
the testing. The reason why we utilize this dataset is that it
includes both the UI screenshots and the corresponding bug
description which facilitates the searching and analysis of
UI display issues. This dataset contains 10,330 unique GUI
screenshots.

3. https://github.com/20200501/Nighthawk.
4. Baidu (baidu.com) is the largest Chinese search service provider.
Its crowdsourcing test platform (test.baidu.com) is also the largest ones
in China.

2.2 Categorizing UI Display Issues

For these GUI screenshots, the ﬁrst three authors individu-
ally check each of them manually with also its correspond-
ing description in the bug report. Only GUI screenshots
with the consensus from all three human markers are re-
garded as ones with display issues. A total of 4,470 GUI
screenshots are determined with UI display issues, which
accounts for 43.2% (4470/10330) in all screenshots. This
result indicates that the UI display issues account for a
non-negligible portion of mobile application bugs revealed
during crowdtesting and should be paid careful attention
for improving the software quality.

During the manual examination process, we notice that
there are different types of UI issues, a categorization of
these issues would facilitate the design and evaluation of
related approach. The ﬁrst and the third authors manually
check it following the open coding procedures [25]. We
analyze the issue screenshots and categorize the UI display
issues. In detail, each annotator carefully examines the issue
screenshots. We group similar codes into one category, and
the grouping process is iterative. Speciﬁcally, we constantly
move back and forth between the category. In the absence of
an agreement between the two authors, the second author
act as arbitrators to discuss and resolve the conﬂict. We
follow the procedure until all authors reach an agreement.
We classify those UI issues into ﬁve categories including
component occlusion, text overlap, missing image, null value and
blurred screen with details as follows:

Component occlusion (47%): As shown in Figure 2(a),
the textual information or component is occluded by other
components. It usually appears together with TextView or
EditText. The main reasons are as follows: the improper
setting of element’s height, or the adaptive issues triggered
when setting a larger-sized font.

Text overlap (21%): As shown in Figure 2(b), two pieces
of text are overlapped with each other. This might be caused
by the adaptive issues among different device models, e.g.,
when using a larger-sized font in a device model with small
screen might trigger this bug.

Note that, for text overlap category, two pieces of text
are mixed together; while for component occlusion, one
component covers part of the other component.

Missing image (25%): As shown in Figure 2(c), in the
icon position, the image is not showing as its design. The
possible reasons are as follows: wrong image path or layout
position, unsuccessful loading of the conﬁguration ﬁle due
to permissions, oversized image, network connection, code
logic, or picture errors, etc.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

4

NULL value (6%): As shown in Figure 2(d), the right
information is not displaying, instead NULL is showing in
corresponding area. This category of bugs usually occurs
with TextView. The main reasons are as follows: issues in
parameter setting or database reading, and the length of
text in TextView exceeding the threshold, etc.

Blurred screen (1%): As shown in Figure 2(e), the screen
is blurred. The reason for this bug might because the defects
in hardware, or the exclusion of hardware acceleration for
some CPU- or GPU- demanding functionalities.

To further validate the generality of our observations, we
also manually check 1,432 screenshots from 200 random-
chosen applications in Rico5 dataset [20] , which is a
commonly-used mobile application dataset with 66K UI
screenshots of Android Applications and we will further
introduce that dataset on Section 4. We ﬁnd that 18 UIs
from 16 apps (16/200 = 8.8% apps) are with UI display
issues. Note that number is highly underestimated, as the
collected UIs do not cover all pages of the applications, and
the applications are not fully tested on different devices with
different screen resolutions.

2.3 Why Visual Understanding in Detecting UI Display
Issues

These ﬁndings conﬁrm the severity of UI display issues, and
motivate us to design approach for automatically detecting
these GUI issues. One commonly-used practice for bug
detection in mobile apps is the program analysis, but it
may not be suitable in this scene. To apply the program
analysis, one need to instrument the target app, develop dif-
ferent rules for different types of UI display issues, rewrite
the code for different platforms (e.g., iOS, Android), and
customize their code to be compatible on different mobile
devices (e.g., Samsung, Huawei, etc) with different screen
resolution, which is extremely effort-consuming. Speciﬁ-
cally, it is not trivial to enumerate all display issues and
develop corresponding rules for detection.

Fig. 3: Examples of unable to detect and noise

We observed the screenshots of these 5 categories of
bugs and their corresponding JSON ﬁles, and found that

5. http://interactionmining.org/rico#quick-downloads

component occlusion, text overlap and null value can be
detected by analyzing JSON ﬁles to obtain component in-
formation(i.e., component coordinates, component text). We
try to use the static analysis of XML ﬁles to detect these three
categories of bugs as follows. For component occlusion,
we analyze the coordinates of all components, and regard
the components with intersection coordinates as bugs. For
text overlap, we analyze the coordinates of all text views,
and regard the components with intersection coordinates as
bugs. For null value, we analyze the text of the component,
and regard null text as a bug.

However, static analysis method has some limitations.
In the following cases, it is impossible to detect bugs by
analyzing JSON ﬁles. For component occlusion, as shown in
Figure 3(a), because the font size in the JSON ﬁle cannot be
obtained, the issue that the font is displayed incompletely
in EditText cannot be detected. As shown in Figure 3(b),
the toolbar, spinner and dialog will ﬂoat in front of the
component, which will cause noise to the detection. For text
overlap, there is still noise such as component occlusion.
For null value, there are more nulls in the text acquisition
process, most of which are due to the problems existing in
the process of getting JSON ﬁles, but there is no problem in
the actual UI display, which will add a lot of noise to the
detection of such bugs.

Taken in this sense, it is worthwhile developing a new
efﬁcient and general method for detecting UI display issues.
Inspired by the fact that these display issues can be spotted
by human eyes, we propose to identify these buggy screen-
shots with visual understanding technique which imitates
the human visual system. As the UI screenshots are easy to
fetch (either manually or automatically) and exert no sig-
niﬁcant difference across the apps from different platforms
or devices, our image-based approach are more ﬂexible and
easy to deploy.

3 ISSUES DETECTION AND LOCALIZATION AP-
PROACH
This paper proposes Nighthawk to automatically detect
and localize UI display issues in the screenshots of the
application under test, as shown in Figure 4. Given one UI
screenshot, our Nighthawk provides integrated detection
and localization services. Nighthawk can detect the screen-
shot related with UI display issues via visual understanding
and localize the detailed issue position by bounding boxes
on the UI screenshot for guiding developers to ﬁx the bug.
As the UI display issues can be spotted via the visual
information, we adopt the Faster-RCNN [26], which has
proven to be effective in object detection in computer vision
domain. Figure 4 shows the structure of our object detection
model which includes a feature extraction network (we use
ResNet50), a regional proposal network (RPN) module, and
an ROI pooling module.

Given the input UI screenshot, we convert it into a
certain image size with ﬁxed width and height as w × h,
and the image is normalized. Then the screenshot is input
into the convolution neural network ResNet50 [27]. The
Convolutional layer’s parameters consist of a set of learn-
able ﬁlters. The purpose of the convolutional operation is to
extract the different characteristics of the input (i.e., feature

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

5

Fig. 4: Overview of Nighthawk

extraction). After the convolutional layer, the screenshots
will be abstracted as the feature graph. However, with
the network depth increasing, accuracy gets saturated and
then degrades rapidly, it is easy to appear the vanishing
/ exploding gradients problem and degradation problem.
Because the gradient propagates back to the previous layer,
repeated multiplication may make the gradient inﬁnitesi-
mal. As a result, with the deeper layers of the network, its
performance tends to be saturated or even rapidly decline.
In order to solve this problem, ResNet50 introduces the con-
cept of residual error to solve this problem. ResNet50 solves
the degradation problem by introducing a deep residual
learning framework. Instead of making each layer directly
ﬁt a desired underlying mapping, it explicitly matches these
layers with a residual mapping.

After obtaining the feature map through ResNet50, we
input the feature map into Region Proposal Network (RPN)
module. The RPN takes the feature map (of any size) as
input and outputs a set of rectangular 3 object proposals,
each with an objectness score. Then, a 3x3 slide window
is used to traverse the whole feature map. In the process
of traversing, nine anchors are generated according to rate
and scale (1:2, 1:1, 2:1) in each window center. Then, full
connection is used to classify each anchor (foreground or
background) and preliminary bounding boxes expression.
Then the bounding box expression is used to modify the
anchors to obtain accurate proposals.

According to the feature map obtained by the feature
extraction module and proposal obtained by RPN module.
Input it into the ROI pooling layer to calculate the proposal
feature maps. Finally, the proposal feature maps are input
into the classiﬁcation module, and the speciﬁc category
(such as component occlusion, missing image, etc.) of each
proposal is calculated through the fully connected neural
networks (FC) and softmax layer, and the probability vector
is output. At the same time, the position offset of each
proposal is obtained by using bounding box expression
again, which is used to regression more accurate target
detection frame.

4 HEURISTIC-BASED
GENERATION

TRAINING

DATA

AUTO-

Training an object detection model for visual understand-
ing requires a large amount of input data. For example,
ResNet [28] model uses 128 million images from ImageNet
as training dataset for image classiﬁcation task. Similarly,
training our proposed Faster-RCNN for UI display issues
detection and localization requires abundant of screenshots
with UI display issues. However, there is so far no such type
of open dataset, and collecting the related buggy screenshots
is quite time- and effort-consuming. Different from image
classiﬁcation task, Faster-RCNN model not only needs to
determine whether there are bugs on the screenshots, but
also to mark the speciﬁc location of bugs on the screenshots.
This requires a large number of experienced testers to mark
it. At the same time, the approach in our previous work
OwlEye needs a certain number of screenshots with UI dis-
play issues, and collecting these issue screenshots requires
human annotation, since most screenshots are issue-free.
Therefore, we develop a heuristic-based training data auto-
generation method for generating UI screenshots with UI
display issues from bug-free UI images.

The data auto-generation is based on the Rico [20]
dataset which contains more than 66K unique screenshots
from 9.3K Android applications, as well as their accompa-
nied JSON ﬁle (i.e., detailed run-time view hierarchy of the
screenshot). According to our observation on Section 2, most
UI screenshots in this dataset are of no dispaly issues.

the algorithm ﬁrst

Algorithm 1 presents the heuristic-based training data
auto-generation algorithm. With the input screenshot and
its associated JSON ﬁle,
locates all
the TextView and ImageView, then randomly chooses a
TextView or ImageView depending on the generated cate-
gory. Based on the coordinates and size of the TextView/Im-
ageView, the algorithm then makes its copy and adjusts
its location or size following speciﬁc rules to generate the
screenshot with corresponding UI display issues (line 1-
12). Figure 5 demonstrates the illustrative examples of the
generated screenshots with UI display issues.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

6

Algorithm 1: Heuristic-based training data auto-
generation

Input: scr: screenshot without bugs;
json: associated JSON ﬁle;
category: category of generated UI display issue;
icon: pre-prepared image icon;
Output: genscr: generated screenshot with category

bug;

bbox: coordinates of bounding box;

1 Traverse json ﬁle to obtain all TextView &

ImageView & Button & EditText;
2 if category ==‘missing image’ then
Randomly choose an ImageView;
3
4 if category ==‘component occlusion’ then
5

Randomly choose a TextView / ImageView /
Button / EditText;

6 if category ==‘text overlap’ or ‘null value’ then
7

Randomly choose a TextView;

8 Obtain the coordinates of TextView / ImageView /
Button / EditText (x1,y1),(x2,y2); //coordinate of
upper left and lower right (If it is textview, get the
upper and lower left coordinates of text)
9 Calculate the width and height of TextView /

ImageView / Button / EditText(w, h) based on the
coordinates;

10 Obtain the text content of TextView (text);
11 Obtain the text font size of TextView (f s);
12 Obtain the background color of TextView /
ImageView / Button / EditText (bg);
13 if category ==‘component occlusion’ then
14

rand ← random.uniform(-1,1);
image.new((w, h × |rand|)),bg,f s);
if rand ≥ 0 then

//Occlude the upper part of component
genscr ← scr.paste(image, (x1,y1));

else

//Occlude the lower part of component
genscr ← scr.paste(image,
(x1,y2 + (h × rand)));

15

16

17

18

19

20 if category ==‘text overlap’ then
21

xrand ← random.uniform(−0.5 × w, 0.5 × w);
genscr ← scr.write([x2 − xrand, y1],text,f s);
//Get the coordinates of the overlap
x1,y1,x2,y2 ←getoverlap(genscr,f s);

22

23

24 if category ==‘missing image’ then
25

image.new((w, h),bg);
scr.paste(image,(x1,y1));
genscr ← scr.paste(icon, (x1 + 0.5 × w,
y1 + 0.5 × h));

26

27

28 if category ==‘null value’ then
image.new((w, h),bg);
29
scr.paste(image, (x1,y1));
genscr ← scr.write([x1,y1],“null”,f s);

31

30

32 bbox ← writexml(x1,y1,x2,y2);
33 return genscr, bbox;

Fig. 5: Examples of data auto-generation

Note that, among the ﬁve categories of UI display issues,
the category of blurred screen is difﬁcult to generate follow-
ing the above idea. In addition, the preliminary survey re-
sults show that the number of such bugs is small, accounting
for only 1% of the crowdtesting data (see Section 2.2). Hence,
we leave this category for future work. For the generation
methods of these four kinds of issues, we conducted a pilot
study. In detail, we select 500 issue screenshots of different
apps for each type of issue to summarize the characteristics
of issue screenshots, and the settings and parameters of the
auto-generation method is based on the summarized char-
acteristics. The general principle of the auto-generation is
that we randomly decide the occlusion/overlap offset or oc-
clusion/overlap region to generate diversiﬁed screenshots,
and we assume the large number of generated screenshots
in training dataset can mitigate the problem caused by too
large or too slight offset. We then present the detailed auto-
generation rules of the four categories.

Auto-generation for Component Occlusion Bug: When
this category of bug occurs, the textual information or com-
ponent is occluded by other components. In detail, the pilot
study reveals that it appears in TextView, EditText, Button or
ImageView, and the auto-generation is conducted on them
randomly. The pilot study also reveals that when occlusion
happens, the two involved components are usually with the
same width but different height. Therefore, we ﬁrst generate
a color block with the same width and background color as
the component but with a smaller height (i.e., randomly-
generated value), and then put it to cover part of the
component randomly (e.g., lower left part). The random
generated height is determined by random(−1, 1) guided
by the pilot study. For the determination of the color of the
occlusion area, we obtain the color of the upper left corner
and the upper right corner of the component, and take the
average as the color of the occlusion area. Finally, we get the
size of the component as the bounding box (line 13-19).

Auto-generation for Text Overlap Bug: The textual
contents are overlapped with each other, when this category
of bug occurs. To auto-generate this category of screenshots,
we ﬁrst get the height of the TextView, convert it to a font
size, and generate a piece of text with the same size and
content as the original TextView, and offset it slightly. The
threshold of offset is calculated by random number, that is,

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

7

random(−0.5 × w, 0.5 × w) which is observed in the pilot
study. Besides, through the pilot study, we determined that
the color of the font is also randomly selected (black, gray,
white). Finally, we use the overlapped part of the offset text
and the original text as the bounding box (line 20-23).

Auto-generation for Missing Image Bug: We notice that
when this category of bug occurs, an image icon would
show up to indicate that the area supposes to be an image.
We summarize 10 common icons from our pilot study. These
icons are quite different from real-world images, and these
issue image icons are rarely used in the UI of app (most
UI developers think these icons are issue icons), so the
detection results are mostly issue. To auto-generate this
category of screenshots, we ﬁrst download 10 frequently-
used image icons online, then cover the original image dis-
playing area with one random-chosen image icon and set its
background color as the color of its original image. Through
our real-world app detection in RQ4, we ﬁnd there are slight
different icons which can be detected by Nighthawk (indi-
cating the effectiveness of our propose approach), and donot
ﬁnd entirely different new icons (indicating the relatively
completeness of the training dataset.). Finally, we use the
region of ImageView as the bounding box. (line 24-27).

Auto-generation for NULL Value Bug: When this cate-
gory of bug occurs, NULL is displayed in the area where
supposes to be a piece of text. We ﬁrst get the height
of the text in TextView and convert it to font size. Then
we generate this category of screenshots by covering the
original TextView using a color block which shares the same
background color and adds NULL with the same font size
as the original text. Finally, use the text area of NULL as the
bounding box (line 28-31).

Note that, both component occlusion and text overlap in-
volves covering a TextView, the difference is that the former
one utilizes a color block to cover the TextView so that it
looks like a component blocks the text, while the latter one
employs a piece of text to cover the TextView to make it
look like the two pieces of text are overlapped with each
other. Another note is that, based on our observation on
the screenshots with UI display issues in Section 2, when
conducting the auto-generation, the TextView is covered
in the vertical direction in component occlusion, while it is
covered in the horizontal direction in text overlap.

5 EXPERIMENT DESIGN
5.1 Research Questions

• RQ1: (Issues Detection Performance) How effective is
our proposed Nighthawk in detecting UI display issues?
For RQ1, we ﬁrst present some general views of our
proposed approach for UI display issues detection and
the comparison with commonly-used baseline approaches
(details are in Section 5.3).
• RQ2: (Issues Localization Performance) How effective is
our proposed Nighthawk in localizing UI display issues?
For evaluating the performance of issues localization, we
compare it with our previous approach OwlEye, and detect
its accuracy through its average recall (AR) and average
precision (AP).
• RQ3: (Contribution of Data Auto-generation) What is
the contribution of the data auto-generation approach?

For RQ3, we evaluate the contribution of data auto-
generation. We ﬁrst examine the inﬂuence of auto-generated
training data size on the model performance, then experi-
mentally compare the issue detection performance between
the model with this upgraded auto-generation approach
and the model with the augmentation approach of OwlEye.
• RQ4: (Usefulness Evaluation) How does our proposed

Nighthawk work in real-world situations?

For RQ4, we integrate Nighthawk with DroidBot as a
fully automatic tool to collect the screenshots and detect
UI display issues, and then issue the detected bugs to the
development team.

5.2 Experimental Setup
As our Nighthawk is a fully automatic approach, we use
the heuristic-based training data auto-generation approach
in Section 4 to generate a large number of data. In detail,
we randomly download one or two screenshots from each
of the random-chosen 30,000 applications in Rico dataset,
and each screenshot would be utilized once for the data
auto-generation. In order to make the training data balanced
across categories, we used the same number of screenshots
as training data for each type of issues.

For the auto-generated 50,000 screenshots with UI dis-
play issues from Alg 1, we ﬁrst extract their features with
ORB feature extraction algorithm [29], rank them randomly,
compute the cosine similarity between a speciﬁc screen-
shot and each of its previous ones, and remove it when a
similarity value above 0.8 is observed. In this way, 40,000
screenshots with UI display issues from 50,000 screenshots
(each category has 10,000 screenshots) and equal number of
bug-free non-duplicate screenshots (from buggy screenshots
corresponding bug free versions), with a total of 80,000
screenshots are remained and added into the experimental
dataset.

TABLE 1: The number of 5 categories of buggy screenshots

Category

Component occlusion
Text overlap
Missing image
NULL value
Overall

Train

8000
8000
8000
8000
32000

Test
Crowd AutoGen

200
200
200
200
800

1000
1000
1000
1000
4000

Val

1000
1000
1000
1000
4000

In order to simulate the real-world application of our
proposed Nighthawk, we setup the experiment as follows.
For the 80,000 screenshots of the Rico dataset (the ratio of
positive and negative samples is 1:1), the 40,000 screenshots
with UI display issues are generated as positive samples
for the experiment, including 10,000 screenshots for each
of the four categories of bugs (Each category selects the
same number of negative samples). Set the training set,
testing set and validation set according to the ratio of 8:1:1.
According to the same experimental setup, always ensure
that the ratio of positive and negative samples is 1:1, 1,000
buggy screenshots were randomly selected as the testing set
(Only used in the issues detection performance by category
in Section 6.1), 1,000 buggy screenshots as the validation set,
and the remaining 8,000 buggy screenshots as the training
set.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

8

In addition, in order to understand the performance of
the Nighthawk in real-world dataset, the testing set also
includes the 1,600 screenshots (800 with UI display issues
and 800 without) from 300 crowdtesting apps (Note that in
order to compare with OwlEye, we use the same test dataset
in OwlEye.), we utilize the 400 screenshots (200 with UI
display issues and 200 without) 400 screenshots for each
category of bugs as testing set (Used in Section 6.1-6.2)
to evaluate the performance of Nighthawk. Considering
the long training time, we used the 3-fold cross-validation.
For simplicity, we present the average performance of the
experimental results.

Table 1 presents the distribution of screenshots in terms
of different categories. The model is trained in a NVIDIA
GeForce RTX 2060 GPU (16G memory) with 100 epochs for
about 8 hours.

5.3 Baselines
To further demonstrate the advantage of Nighthawk, we
compare it with 6 baselines utilizing both machine learning
and deep learning techniques. The 4 machine learning ap-
proaches ﬁrst extract visual features from the screenshots,
and employ machine learners for the classiﬁcation. The 2
deep learning approaches utilizes artiﬁcial neural network
directly on the screenshots for classiﬁcation. We ﬁrst present
the feature extraction approach used in machine learning
approaches.

SIFT [30]: Scale invariant feature transform (SIFT) is a
common feature extraction approach to detect and describe
local features in an image. It can extract the interesting
points on the object to generate the feature description of
the object, which is invariant to uniform scaling, orientation,
and illumination changes.

SURF [31]: Speed up robot features (SURF) is an im-
provement of SIFT. SURF uses an integer approximation
of the determinant of Hessian blob detector, which can be
computed with 3 integer operations using a precomputed
integral image.

ORB [29]: Oriented fast and rotated brief (ORB) is a fast
feature point extraction and description algorithm. Based
on the rapid binary descriptor ORB of brief, it has rotation
invariance and anti noise ability.

With these features, we apply four commonly-used ma-
chine learning approaches, i.e., Support Vector Machine
(SVM) [32], K-Nearest Neighbor (KNN) [33], Naive Bayes
(NB) [32] and Random Forests (RF) [34], for classifying the
screenshots with UI display issues.

MLP [35], [36]: Multilayer Perceptron (MLP) is a feed-
forward artiﬁcial neural network. The network structure is
divided into input layer, hidden layer and output layer.
Each node is a neuron that uses a nonlinear activation
function, e.g., corrected linear unit (ReLU). It is trained by
changing the connection weight according to the output
error compared with the ground truth. We used eight layers
of neural network, and we set the number of neurons in
each layer to 190, 190, 128, 128, 64, 64, 32 and 2, respectively.
OwlEye [21]: OwlEye builds on the Convolutional Neu-
ral Network (CNN) to identify the screenshots with UI
display issues, and utilizes Gradient weighted Class Acti-
vation Mapping (Grad-CAM) to localize the regions with
UI display issues.

5.4 Evaluation Metrics

In order to evaluate the issues detection performance of
our proposed approach in RQ1, we employ three eval-
uation metrics, i.e., precision, recall, F1-Score, which are
commonly-used in image classiﬁcation and pattern recog-
nition [37], [38]. For all the metrics, higher value leads to
better performance.

Precision and recall are often calculated by counting
true positions (TP), true negatives (TN), false positions
(FP), and false negatives (FN). In the issue detection task,
TP is the screenshot correctly predicted as buggy; FN is
the screenshot of the incorrectly predicted as buggy; TN
is the screenshot correctly predicted as normal; FP is the
screenshot incorrectly predicted as normal.

TABLE 2: Confusion matrix

Confusion matrix

Predicted

True fault
Not true fault

Observed

True fault
TP
FN

Not true fault
FP
TN

Precision is the proportion of screenshots that are cor-
rectly predicted as having UI display issues among all
screenshots predicted as buggy:

precision =

TP
TP + FP

(1)

Recall is the proportion of screenshots that are correctly
predicted as buggy among all screenshots that really have
UI display issues.

recall =

TP
TP + FN

(2)

F1-score (F-measure or F1) is the harmonic mean of
precision and recall, which combines both of the two metrics
above.

F1 − score =

2 × precision × recall
precision + recall

(3)

In Section 6.2, in order to evaluate the issues local-
ization performance of our proposed approach in RQ2,
we employ two evaluation metrics, i.e., average precision
(AP) and average recall (AR), which are commonly-used
in object detection [26]. The AP and AR can more accu-
rately and rigorously describe the localization performance
of the Nighthawk. For all metrics, the higher the value,
the better the performance. Among them, AP and AR are
similar to precision and recall in image classiﬁcation, but
the evaluation content is different. First, select the prediction
box whose conﬁdence score is greater than 0.5 [26], and
calculate the ratio IoU (intersection over union) of the
intersection and union of the prediction box and ground
truth box. The calculation approach is as follows: IoU =
the intersection of the predicted buggy region and the real
buggy region / the union of the predicted buggy region
and the real buggy region. The metric IoU can solve the
coverage problem. Then true positives (TP) is the number of
detection boxes with IoU ≥ 0.5. False positives (FP) is the
number of detection boxes with IoU <0.5, and the number
of redundant detection boxes detected in the same ground
truth box. False negatives (FN) is the number of ground
truth box not detected.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

6 RESULTS AND ANALYSIS
6.1 Issues Detection Performance (RQ1)

We ﬁrst present the issues detection performance of our
proposed Nighthawk, as well as the performance in terms
of four categories of UI display issues in the data-generation
dataset (Data-Gen) and the real-world dataset (Real-World)
in Table 3. In the data-generation dataset, with Nighthawk,
the average precision (P) is 0.843, indicating that an average
of 84.3% (837/993) of the screenshots which are predicted as
having UI display issues are truly buggy. The average recall
(R) is 0.837, indicating that an average of 83.7% (837/1000)
buggy screenshots can be found with Nighthawk. In the
real-world dataset, with Nighthawk, the average precision
is 0.826, indicating that an average of 82.6% (164/199) of
the screenshots which are predicted as having UI display
issues are truly buggy. The average recall is 0.821, indicat-
ing 82.1% (164/200) buggy screenshots can be found with
Nighthawk.

TABLE 3: Issues detection performance (RQ1)

Category

Component occlusion
Text overlap
Missing image
NULL value
Average

Data-Gen
R
0.751
0.813
0.879
0.904
0.837

P
0.768
0.851
0.870
0.881
0.843

F1
0.759
0.832
0.875
0.892
0.840

Real-World
R
0.735
0.790
0.870
0.890
0.821

P
0.750
0.810
0.866
0.878
0.826

F1
0.742
0.800
0.868
0.883
0.823

Although our Nighthawk is training on the data-
generation dataset, the average precision and recall in the
real-world dataset are only 0.017 and 0.016 lower than the
data-generation dataset, which further shows the effective-
ness of our Nighthawk.

We then shift our focus to the top half of Table 3, i.e.,
the performance in terms of each category of UI display
issues. All the four categories of UI display issues can be
detected with a relative high precision and recall, i.e., the
maximum precision and recall are 0.88 and 0.90, the mimi-
mum precision and recall are 0.77 and 0.75 respectively, The
category null value can be detected with the highest F1-score,
indicating both precision (0.88) and recall (0.90) achieve a
relatively high value. This might because screenshots with
null value bugs have relatively ﬁxed pattern and the buggy
area is relatively obvious, i.e., the screenshot as shown in
Section 2. In comparison, the category component occlusion
is recognized with the lowest F1-score, e.g., 0.77 precision
and 0.75 recall. This is due to the fact that the pattern of this
category is more diversiﬁed, and the buggy region is much
smaller, i.e., the occlusion area of component only accounts
for a mere of 10% of the component area.

We further analyze the screenshots which are wrongly
predicted as bug-free, with examples in Figure 6. One com-
mon shared by these screenshots is that the buggy area is too
tiny to be recognized even with human eye. Future work
will focus more on improving the detection performance
for these screenshots with attention mechanism and image
magniﬁcation.

6.1.1 Performance Comparison with Baselines.
Table 4 shows the performance comparison with the base-
lines. We ﬁrst compare the Nighthawk with the 5 baselines
in the top half of the table. We can see that our proposed

9

Fig. 6: Examples of bad case in issues detection (RQ1)

Nighthawk is better than the 5 baselines, i.e., 23.2% higher
in precision and 17.1% higher in recall compared with the
best baseline (MLP). This further indicates the effectiveness
of Nighthawk. Besides, it also implies that Nighthawk is
especially good at hunting for the buggy screenshots from
candidate ones. MLP achieves the highest precision and
recall among the baselines, indicating this deep learning
approach is better at identifying the buggy screenshots.

TABLE 4: Performance comparison with baselines (RQ1)

Method

SIFT-SVM
SIFT-KNN
SIFT-NB
SIFT-RF
SURF-SVM
SURF-KNN
SURF-NB
SURF-RF
ORB-SVM
ORB-KNN
ORB-NB
ORB-RF
MLP
OwlEye
Nighthawk

Data-Gen
R
0.518
0.538
0.562
0.563
0.545
0.558
0.590
0.585
0.549
0.561
0.590
0.586
0.666
0.778
0.837

P
0.525
0.537
0.564
0.556
0.551
0.560
0.586
0.583
0.556
0.564
0.586
0.584
0.611
0.790
0.843

F1
0.521
0.537
0.563
0.559
0.548
0.559
0.588
0.584
0.552
0.562
0.588
0.585
0.637
0.784
0.840

Real-World
R
0.495
0.513
0.509
0.511
0.514
0.528
0.535
0.535
0.516
0.530
0.540
0.544
0.539
0.743
0.821

P
0.500
0.512
0.518
0.511
0.517
0.526
0.543
0.529
0.519
0.526
0.547
0.535
0.547
0.764
0.826

F1
0.497
0.512
0.514
0.511
0.516
0.527
0.539
0.532
0.518
0.528
0.544
0.539
0.543
0.753
0.823

We compare the performance of the newly-designed
model and the old model in OwlEye with the same train-
ing and testing data, and results demonstrate that the
newly-designed model in Nighthawk achieves better per-
formance, i.e., 0.843 vs. 0.790 in precision, and 0.837 vs. 0.778
in recall. Our Nighthawk has a obvious improvement in
precision and recall, which may be due to the fact that the
object detection task provides a more accurate bounding box
for the UI display issues when training the model, which is
more convenient for the model to learn the corresponding
features. The training data of other baseline methods only
have category labels. Nighthawk performs well in issue
detection, and does not need to label data manually, so it
can better adapt to the different style of Android UI.

6.2 Issue Localization Performance (RQ2)

Figure 7 presents the examples of our issues localization
which highlights the buggy areas. Since the localization
result of OwlEye is in the form of heat map, in order to com-
pare its performance with Nighthawk, we use image bina-
rization to determine the bounding box of the highlighted
area of heat map, so that we can compare its performance
with the newly proposed approach.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

10

Fig. 8: Examples of bad case in issues localization (RQ2)

the indicated bug area is basically correct, which can also
provide corresponding presentation for developers.

Fig. 9: Examples of bad case in OwlEye localization (RQ2)

Then we compare the issues localization performance
of our Nighthawk with OwlEye. As shown in Table 5
our Nighthawk is obviously better than OwlEye, i.e., 55%
higher in AP and 56% higher in AR compared with the Owl-
Eye. We further analyze the bad case of issues localization
with OwlEye. As shown in Figure 9, the yellow one is the
prediction bounding box from OwlEye, and the red one is
the ground truth bounding box. The main reason is that
the visualization area of OwlEye is too large, which often
contains the ground truth bounding box, and the IoU area
is less than 0.5.

Fig. 10: Difference in localization of Nighthawk and Owl-
Eye

Please note that the worse performance of OwlEye in AP
and AR is due to the different evaluation criterion. In the
previous paper, we used manual evaluation to determine
the accuracy of UI display issue localization, in which the
participants are required to evaluate whether the localized
area by OwlEye has overlap with the actual issue area. By
comparison, the AP and AR indicators require the IoU area

Fig. 7: Examples of issues localization (RQ2)

Table 5 shows the issues location performance of our
proposed Nighthawk. In the real-world dataset, the av-
erage AP(average precision) and AR(average recall) of
Nighthawk are 0.589 and 0.601 respectively. Due to the
limitation of space, we only show the localization results in
the real-world dataset, and the trend in the data-generation
dataset is similar.

TABLE 5: Issues localization performance (RQ2)

Category

Component occlusion
Text overlap
Missing image
NULL value
Average

OwlEye

Nighthawk

AP
0.011
0.014
0.103
0.024
0.038

AR
0.017
0.012
0.121
0.028
0.045

AP
0.503
0.538
0.773
0.541
0.589

AR
0.547
0.567
0.765
0.523
0.601

We then shift our focus to the top half of Table 5, i.e., the
issues location performance in terms of each category of UI
display issues. All the four categories of UI display issues
can be detected with a high AP and AR, i.e., in the real-
world dataset the mimimum AP and AR are 0.503 and 0.523
respectively. In the real-world dataset, the category missing
image can be detected with the highest performance, indi-
cating both AP (0.773) and AR (0.765) achieve a relatively
high value. This might because screenshots with missing
image issues have large and relatively obvious buggy area. In
comparison, the category component occlusion is recognized
with the lowest performance, e.g., 0.503 AP and 0.547 AR.

This is due to the fact that the buggy region is much
smaller. As shown in Figure 8, the green one is the prediction
bounding box, and the red one is the ground truth bounding
box. The predicted bounding box area will be slightly larger
than the real-world bounding box, resulting in IoU area
less than 0.5, which will reduce the AP and AR. However,
as shown in Figure 8, although the predicted localization
result is judged to be wrong due to IoU less than 0.5,

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

11

to be more than 0.5, i.e., the highlighted issue area should
be at least 50% in common with the actual issue area. We
observe three cases in which the human evaluation comes
out with a high performance, yet the AP/AR suggests
a low localization accuracy. The ﬁrst is incomplete issue
detection as shown in Figure 10(a). We can see that there
are three issue areas, while OwlEye only highlights one of
them. By comparison, the newly-proposed Nighthawk can
detect all of them, and achieve better AP/AR than OwlEye.
The second is localization noise as shown in Figure 10(b).
OwlEye wrongly highlights another area in the lower left
part of the screenshot, and would obtain lower AP/AR than
Nighthawk. The third is the localization drift as shown in
Figure 10(b). The newly-proposed Nighthawk can perfectly
localize the issue area, i.e., the whole image region, while
the highlighted area by OwlEye does not hit the target
accurately.

6.3 Contribution of Data Auto-generation (RQ3)

We also conduct experiments to compare the issue detection
performance of our Nighthawk using different amounts of
training data. The testing set is manually labeled data from
the crowdtesting dataset, with 400 screenshots of each cat-
egory (half positive and half negative samples, see Section
5.2). As shown in Figure 11, we randomly extract 2000-9000
screenshots with bugs as positive samples in the training
set, and extract the same number of bug-free screenshots
as negative samples according to the same settings and
proportions to form the training set.

Figure 11 shows the performance of UI display issues
detection in terms of different amounts of training data, i.e.,
the number of negative samples is from 2000 to 9000.

Fig. 11: Result of different training data conﬁgurations (RQ3)

We can see that both precision and recall improve with
the increase of data volume, indicating the value of data
auto-generation for effective UI display issues detection.
Speciﬁcally, compared with the training data of 2000 buggy
screenshots, the improvement of 35% and 41% in average
precision and recall of 8000 buggy screenshot are observed
respectively. The larger improvement in recall and precision
indicates that, with more data, more screenshots with UI
display issues can be found. This might because the model
parameters of the object detection task are more than the
image classiﬁcation model, thus more training samples are
needed to train the model.

As shown in Figure 11, with the increase of the amount
of data, the improvement of model performance is grad-
ually decreasing. For example, the effect of 8000 buggy
screenshots and 9000 buggy screenshots almost remains un-
changed. At the same time, too much data will increase the

cost of training model, so we chose 8000 buggy screenshots
as our ﬁnal training data.

6.3.1 Data Auto-generation Performance

We investigate the contribution of data auto-generation by
comparing the issues detection performance of the data
auto-generation method in Nighthawk(Auto-DataGen)
with that of the data augmentation method in OwlEye
(DataAug) (details are in Section 5.2). We used the same
amount of generated data to train our Nighthawk to com-
plete the experiment.

TABLE 6: Contribution of data auto-generation (RQ4)

Category

Component occlusion
Text overlap
Missing image
NULL value
Average

F1

DataAug
P
R
0.672 0.605 0.636
0.716 0.645 0.679
0.785 0.765 0.774
0.711 0.795 0.803
0.746 0.703 0.723

Auto-DataGen
P
F1
R
0.750 0.735 0.742
0.810 0.790 0.800
0.866 0.870 0.868
0.878 0.890 0.883
0.826 0.821 0.823

As shown in Table 6, results show that, 8% and 12%
improvement are observed respectively for average preci-
sion and recall. Speciﬁcally, issues detection performance
in component occlusion category undergoes the largest im-
provement in F1-score. This might because the data auto-
generation method (DataGen(Nighthawk)) pays more at-
tention to the diversity of training screenshots and the gen-
eration of tiny region bugs (details are in Section 4). After us-
ing data auto-generation method (DataGen(Nighthawk)),
the diversity of the training screenshots signiﬁcantly im-
proves the performance.

6.4 Usefulness Evaluation (RQ4)
To further assess the usefulness of our Nighthawk, we
randomly sample 2,000 Android applications from F-Droid6
and 2,000 Android applications from Google play7, includ-
ing many new apps released on 2019 and 2020. Note that
none of these apps appear in our training dataset.

We use DroidBot, which is a commonly-used lightweight
Android test input generator [39], for exploring the mobile
apps and take the screenshot of each UI pages. Among the
4,000 collected apps, 70% (2785/4000) apps can be success-
fully run with Droidbot, and only 33% (1328/4000) of the
apps can be fetched with more than one screenshot, as they
require register or authenticate to explore more screenshots
which cannot be done by DroidBot. For the remaining 1328
apps, an average of eight screenshots are obtained for each
app. We then feed those screenshots to Nighthawk for
detecting if there are any UI display issues. Once a display
issue is spotted, we create a bug report by describing the
issue attached with buggy UI screenshot. Finally, we report
them to the app development team through issue reports or
emails.

Table 7 shows all bugs spotted by our Nighthawk, and
more detailed information of detected bugs can be seen in
our website3. For F-Droid applications, 82 UI display issues
are detected, among which 23 have been ﬁxed and another

6. http://f-droid.org/
7. http://play.google.com/store/apps

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

12

TABLE 7: Conﬁrmed or ﬁxed issues (RQ4)

APP Name

Category Download
Apps From Google Play

IssueId

Status

SHAREit
ShareMe
Perfect Piano
Music Player
Status Saver
Nimo TV
Nox security
DegooCloud
Proxynel
Secure VPN
Thunder VPN
Sweatcoin
ApowerMirror
PUB Gfx
MediaFire
Paytm
Playnimes Animes
Postegro
Deezer Player
Air China

Librera Reader
AdGuard
Kiwix
MTG Familiar
My Expenses
WiGL
Linphone
Transdroid
Tutanota
Onkyo
Aria2App
NewPipeLegacy
Noice
EasyRepost
Ahorcado
LessPass
SimpleLogin
DSAAssistent
Dagger
BetterUntis
ProExpense
TrailSense
CEToolbox
AppIntro
logger
Hauk
Shosetsu
Memory
DailyFinance
LeMondeRssr
Weather
OpenTracks-OSM
Yucata Envoy
ClassyShark3
VlcFreemote

500M+
Tools
500M+
Tools
50M+
Music
50M+
Music
50M+
Product
50M+
Enter
10M+
Tool
10M+
Tool
10M+
Tool
10M+
Tool
10M+
Tool
10M+
Health
5M+
Tool
5M+
Libraries
5M+
Product
1M+
Finance
1M+
Video
500K+
Commun
500K+
Music
Travel
100K+
Apps From F-droid
10M+
Reading
5M+
Secur
1M+
Books
500K+
Utilities
500K+
Finance
500K+
Connect
500K+
Commun
100K+
Tool
100K+
Commun
10K+
Music
10K+
Inter
8K+
Media
10K+
Media
10K+
Media
10K+
Game
5K+
Product
5K+
Internet
1K+
Game
1K+
Enter
1K+
Edu
1K+
Internet
500+
Navig
500+
Medical
500+
Image
500+
Connect
300+
Navig
100+
Reading
100+
Edu
50+
Finance
50+
Tool
50+
Tool
50+
Health
50+
Tool
50+
Tool
50+
Media

email
email
email
email
email
email
email
email
email
email
email
email
email
email
email
email
email
email
email
email

#652
#243
#2566
#512
#715
#460
#965
#542
#2527
#138
#140
#24
#458
#9
#3
#519
#11
#12
#12
#168
#23
#338
#4
#915
#92
#162
#91
#5
#23
#34
#1
#26
#3
#3
#24

ﬁxed
ﬁxed
conﬁrm
conﬁrm
ﬁxed
ﬁxed
ﬁxed
ﬁxed
conﬁrm
conﬁrm
ﬁxed
ﬁxed
conﬁrm
ﬁxed
conﬁrm
conﬁrm
ﬁxed
ﬁxed
ﬁxed
ﬁxed

ﬁxed
conﬁrm
conﬁrm
ﬁxed
ﬁxed
ﬁxed
conﬁrm
conﬁrm
conﬁrm
ﬁxed
conﬁrm
ﬁxed
ﬁxed
conﬁrm
ﬁxed
ﬁxed
ﬁxed
conﬁrm
ﬁxed
conﬁrm
ﬁxed
conﬁrm
conﬁrm
ﬁxed
ﬁxed
conﬁrm
ﬁxed
ﬁxed
ﬁxed
conﬁrm
ﬁxed
ﬁxed
conﬁrm
conﬁrm
conﬁrm

18 have been conﬁrmed by the developers. For Google Play,
69 UI display issues are detected, among which 25 have been
ﬁxed and another 9 have been conﬁrmed by the developers.
These ﬁxed or conﬁrmed bug reports further demonstrate
the effectiveness and usefulness of our proposed approach
in detecting UI display issues.

The results reveal that Nighthawk can not only cover
all the issues detected by OwlEye, but also ﬁnd 66 more
issues than OwlEye. The precision of Nighthawk is 81%
(151/186), which is 14%((81%-71%)/71%) higher than that
of OwlEye. As shown in the Figure 12, the Nighthawk can
detect more issues with small buggy area than OwlEye. In

Fig. 12: UI display issues detected by Nighthawk (RQ4)

addition, when developers conﬁrm the issue report submit-
ted by Nighthawk, they say that the UI display issue has a
great impact on the user experience and needs to be repaired
as soon as possible. These replies also prove the necessity of
Nighthawk to detect UI display issue.

7 DISCUSSION
Generality across platforms. Almost all the existing studies
of GUI bug detection [39], [40], [41] are designed for a
speciﬁc platform, e.g., Android, which limits its applica-
bility in real-world practice. In comparison, the primary
idea of our proposed Nighthawk is to detect UI display
issues from the screenshots generated when running the
applications. Since the screenshots from different platforms
(e.g., Android, iOS, Mac OS and Windows) exert almost no
difference, our approach can be generalized for UI display
issues detection in other platforms.

Fig. 13: Examples of different platforms

As shown in Figure 13, we have conducted a small scale
experiment for three other popular platform, i.e., iOS, Mac

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

13

OS and Windows, and experiment on 80 screenshots with
UI display issues collected in our daily-used applications.
Results show that our proposed Nighthawk can accurately
detect 86.3% (69/80) of the buggy screenshots. This further
demonstrates the generality of Nighthawk, and we will
conduct more thorough experiment in future.

Generality across languages. Another advantage of
Nighthawk is that it can be applied for UI display issues de-
tection in terms of different display languages of the appli-
cation. The testing data of the experiment for RQ1 contains
the screenshots in Chinese, while the experiment for RQ3
relates with the screenshots in English, which demonstrates
the generality of our approach across languages.

Fig. 14: Examples of different language settings on iPhone

As shown in Figure 14, we also collect 80 screenshots
with UI display issues in three other languages (i.e. German,
Japanese and Korean) from the applications in RQ3, and
run our approach for bug detection. Results show that our
proposed Nighthawk can accurately detect 87.5% (70/80)
of the buggy screenshots, which further demonstrates the
feasibility of Nighthawk.

Potential with more effective automatic testing tool.
Results in RQ3 have demonstrated the usefulness of
Nighthawk in real-world practice being integrated with
automatic testing tool as DroidBot. However, we have men-
tioned in Section 6.3 that some applications can not be run
with DroidBot, and some can only be fetched with one
screenshot due to the shortcoming of DroidBot, both of
which limit the full exploration of screenshots. If armed
with a more effective automatic testing tool, Nighthawk
should play a bigger role in detecting UI display issues in
real-world practice.

8 RELATED WORK
Mobile App GUI. GUI provides a visual bridge between
applications and users, and the quality of UI design is also
widely concerned. Therefore, many researchers are working
on assisting developers or designers in the GUI search [42],
[43], [44], [45], [46], [47], [48], [49] based on image features,
GUI code generation [50], [51], [52], [53], [54], [55], [56],
[57] based on computer vision techniques. Chen et al. [58]
introduce a novel approach based on a deep neural network
for encoding both the visual and textual information to
recover the missing tags for existing UI examples so that
they can be more easily found by text queries. Chen et
al. [59] studied the limitations and effective design of object
detection method based on deep learning in detecting UI
components. In addition, they designed a new top-down
strategy from coarse to ﬁne, and combined it with mature

GUI text deep learning model. Moran et al. [60] check if
the implemented GUI violates the original UI design by
comparing the images similarity with computer vision tech-
niques. A follow-up work by them [61] further detects and
summarizes GUI changes in evolving mobile applications.
Different from these works, our works are focusing on
detecting the GUI display issues to help improve the app
quality.

Automated GUI Testing. To ensure that GUI is work-
ing well, there are many static linting tools to ﬂag pro-
gramming errors, bugs, stylistic errors, and suspicious con-
structs [57], [62]. For example, Android Lint [63] reports
over 260 different types of Android bugs, including correct-
ness, performance, security, usability and accessibility [64].
StyleLint [65] helps developers avoid errors and enforce
conventions in styles. Different from static linting, automatic
GUI testing [6], [7], [8] dynamically explores GUIs of an
app. Several surveys [40], [41] compare different tools for
GUI testing for Android apps. Some testing works focus
on more speciﬁc UI issues such as UI rendering delays and
image loading. Gao et al. [66] and Li et al. [67] analyzed
the possible problems in UI rendering, and developed au-
tomatic approaches to detect them. Nayebi et al. [68] and
Holzinger et al. [69] found that different resolutions of the
mobile devices have brought challenges in Android app
design and implementation. Recently, deep learning based
techniques [70], [71] have been proposed for automatic GUI
testing. Unlike traditional GUI testing which explores the
GUIs by dynamic program analysis, they [70], [71] use
computer vision techniques to detect GUI components on
the screen to determine next actions.

The above mentioned GUI testing techniques focus on
functional testing, while our work is more about non-
functional testing i.e., GUI visual issues which will not cause
app crash, but negatively inﬂuence the app usability. The UI
display bugs detected by our approach are mainly caused by
the app compatibility [72], [73] due to the different devices
and Android versions. It is highly expensive and extremely
difﬁcult for the developers covering all the popular con-
texts when conducting testing. Besides, different from these
works based on static or dynamic code analysis, our work
only requires the screenshot as the input. Such characteristic
enables our light-weight computer vision based method,
and also makes our approach generalised to any platform
including Android, IOS, or IoT devices.

Web App Display Issues Detection. Because web apps
also run on devices with a variety of viewport widths,
there are some similarities between the mobile UI display
issues detection technique in this paper and those testing
approaches of web mobile apps. Kondratova and Gold-
farb [74] explored the UI design preferences of different
countries and cultures around the world. WebSee [75],
[76] and FieryEye [77] utilized computer vision and image
processing techniques to compare a browser rendered test
webpage with an oracle image to ﬁnd visual differences.
BROWSERBITE [78] combined image comparison methods
with machine learning techniques to detect cross-browser
differences in a web page. Mahajan et al. [79], [80] stud-
ied the automatic detection and repair of mobile friendly
problems in web pages. The XFix [81] repaired layout Cross
Browser Issues (XBIs) presentation failures arising from

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

14

the inconsistencies in the rendering of a website across
different browsers. Additionally, IFIXCC [82] repaired in-
ternationalization failures using search-based and cluster-
ing techniques, while CBRepair [83] addresses the same
types of failures but using constraint solving. Althomali et
al. [84] presented an automated approach that extracts the
responsive layout of two versions of a page and compares
them, alerting developers to the differences in layout that
they may wish to investigate further. Different from web
apps, Android apps have more types of UI display issues
(component occlusion, text overlap, missing image, null value,
blurred screen) and smaller issue area. Therefore, our work
adopts the method of computer vision for UI display issue
detection, and uses the data generation method to improve
the performance and robustness of the Nighthawk.

9 CONCLUSION

Improving the quality of mobile applications, especially in
a proactive way, is of great value and always encouraged.
This paper focuses on automatic detecting the UI display
issues from the screenshots generated during automatic
testing. The proposed Nighthawk is proven to be effective
in real-world practice, i.e., 75 conﬁrmed or ﬁxed previously-
undetected UI display issues from popular Android apps.
Nighthawk also achieves more than 17% and 23% boost in
recall and precision compared with the best baseline. As the
ﬁrst work of its kind, we also contribute to a systematical
investigation of UI display issues in real-world mobile apps,
as well as a large-scale dataset of app UIs with display issues
for follow-up studies.

In the future, we will keep improving our model for
better performance in the classiﬁcation. Apart from the
display issue detection, we will further locate the root cause
of these issues in our future work. Then we will develop
a set of tools for recommending patches to developers for
ﬁxing display bugs.

ACKNOWLEDGMENTS

This work is
supported by the National Key Re-
search and Development Program of China under grant
No.2018YFB1403400, National Natural Science Foundation
of China under Grant No. 62072442, No. 62002348, and
Youth Innovation Promotion Association Chinese Academy
of Sciences.

REFERENCES

[1] B.

J.

Jansen, “The graphical user
vol. 30, no. 2, p. 22–26, Apr. 1998.
https://doi.org/10.1145/279044.279051

interface,” SIGCHI Bull.,
[Online]. Available:

[2] Z. Liu, C. Chen,

crush: Assist manual gui
hint moves,” in CHI 2022, 2022.
//doi.org/10.1145/3491102.3501903

J. Wang, and Q. Wang, “Guided bug
android apps via
[Online]. Available: https:

testing of

[3] Y. Su, Z. Liu, C. Chen, J. Wang, and Q. Wang, “Owleyes-
online: a fully automated platform for detecting and localizing UI
display issues,” in ESEC/FSE ’21: 29th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering, Athens, Greece, August 23-28, 2021. ACM, 2021, pp.
1500–1504. [Online]. Available: https://doi.org/10.1145/3468264.
3473109

[4] L. Wei, Y. Liu, and S.-C. Cheung, “Taming android fragmentation:
Characterizing and detecting compatibility issues for android
apps,” in Proceedings of the 31st IEEE/ACM International Conference
on Automated Software Engineering, 2016, pp. 226–237.
J. Martin, Rapid Application Development. USA: Macmillan Pub-
lishing Co., Inc., 1991.

[5]

[6] N. Mirzaei, J. Garcia, H. Bagheri, A. Sadeghi, and S. Malek,
“Reducing combinatorics in gui testing of android applications,”
in Software Engineering (ICSE), 2016 IEEE/ACM 38th International
Conference on.

IEEE, 2016, pp. 559–570.

[7] Y.-M. Baek and D.-H. Bae, “Automated model-based android gui
testing using multi-level gui comparison criteria,” in Proceedings of
the 31st IEEE/ACM International Conference on Automated Software
Engineering. ACM, 2016, pp. 238–249.

[8] T. Su, G. Meng, Y. Chen, K. Wu, W. Yang, Y. Yao, G. Pu, Y. Liu,
and Z. Su, “Guided, stochastic model-based gui testing of android
apps,” in Proceedings of the 2017 11th Joint Meeting on Foundations
of Software Engineering. ACM, 2017, pp. 245–256.

[9] K. Moran, M. Linares-V´asquez, and D. Poshyvanyk, “Automated
gui
testing of android apps: From research to practice,” in
Proceedings of the 39th International Conference on Software Engineer-
ing Companion, ser. ICSE-C ’17.
IEEE Press, 2017, p. 505–506.
[Online]. Available: https://doi.org/10.1109/ICSE-C.2017.166
[10] A. M. Memon and M. B. Cohen, “Automated testing of gui appli-
cations: Models, tools, and controlling ﬂakiness,” in Proceedings of
the 2013 International Conference on Software Engineering, ser. ICSE
’13.

IEEE Press, 2013, p. 1479–1480.

[11] R. Coppola, M. Morisio, and M. Torchiano, “Scripted gui testing
of android apps: A study on diffusion, evolution and fragility,” in
Proceedings of the 13th International Conference on Predictive Models
and Data Analytics in Software Engineering, ser. PROMISE. New
York, NY, USA: Association for Computing Machinery, 2017,
p. 22–32. [Online]. Available: https://doi.org/10.1145/3127005.
3127008

[12] K. Moran, M. Linares-V´asquez, C. Bernal-C´ardenas, C. Vendome,
and D. Poshyvanyk,
for
automated testing of android applications,” in Proceedings of
the 39th International Conference on Software Engineering Companion,
ser. ICSE-C ’17.
IEEE Press, 2017, p. 15–18. [Online]. Available:
https://doi.org/10.1109/ICSE-C.2017.16

“Crashscope: A practical

tool

[13] L. Mariani, M. Pezz`e, and D. Zuddas, “Augusto: Exploiting
popular functionalities for the generation of semantic gui tests
with oracles,” in Proceedings of the 40th International Conference
on Software Engineering, ser. ICSE ’18. New York, NY, USA:
Association for Computing Machinery, 2018, p. 280–290. [Online].
Available: https://doi.org/10.1145/3180155.3180162

[14] G. Denaro, L. Guglielmo, L. Mariani, and O. Riganelli, “Gui testing
in production: Challenges and opportunities,” in Proceedings of
the Conference Companion of the 3rd International Conference on Art,
Science, and Engineering of Programming, ser. Programming ’19.
New York, NY, USA: Association for Computing Machinery, 2019.
[Online]. Available: https://doi.org/10.1145/3328433.3328452
[15] T. Gu, C. Sun, X. Ma, C. Cao, C. Xu, Y. Yao, Q. Zhang,
J. Lu, and Z. Su, “Practical gui testing of android applications
via model abstraction and reﬁnement,” in Proceedings of the
41st International Conference on Software Engineering, ser. ICSE ’19.
IEEE Press, 2019, p. 269–280.
[Online]. Available: https:
//doi.org/10.1109/ICSE.2019.00042

[16] Y. Ma, Y. Huang, Z. Hu, X. Xiao, and X. Liu, “Paladin: Automated
generation of reproducible test cases for android apps,” in
Proceedings of the 20th International Workshop on Mobile Computing
Systems and Applications, ser. HotMobile ’19. New York, NY, USA:
Association for Computing Machinery, 2019, p. 99–104. [Online].
Available: https://doi.org/10.1145/3301293.3302363
[17] A. Developers, “Ui/application exerciser monkey,” 2012.
[18] T. Wetzlmaier and R. Ramler, “Hybrid monkey testing: Enhancing
automated gui tests with random test generation,” in Proceedings of
the 8th ACM SIGSOFT International Workshop on Automated Software
Testing, ser. A-TEST 2017. New York, NY, USA: Association
for Computing Machinery, 2017, p. 5–10. [Online]. Available:
https://doi.org/10.1145/3121245.3121247

[19] A. Machiry, R. Tahiliani, and M. Naik, “Dynodroid: An
input generation system for android apps,” in Proceedings
of the 2013 9th Joint Meeting on Foundations of Software Engineering,
ser. ESEC/FSE 2013. New York, NY, USA: Association for
[Online]. Available:
Computing Machinery, 2013, p. 224–234.
https://doi.org/10.1145/2491411.2491450

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

15

[20] B. Deka, Z. Huang, C. Franzen, J. Hibschman, D. Afergan, Y. Li,
J. Nichols, and R. Kumar, “Rico: A mobile app dataset for building
data-driven design applications,” in Proceedings of the 30th Annual
Symposium on User Interface Software and Technology, ser. UIST ’17,
2017.

[21] Z. Liu, C. Chen, J. Wang, Y. Huang, J. Hu, and Q. Wang,
“Owl eyes: Spotting UI display issues via visual understanding,”
in 35th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2020, Melbourne, Australia, September 21-25, 2020.
IEEE, 2020, pp. 398–409. [Online]. Available: https://doi.org/10.
1145/3324884.3416547

[22] J. Wang, Y. Yang, R. Krishna, T. Menzies, and Q. Wang, “isense:
Completion-aware crowdtesting management,” in ICSE’2019,
2019, pp. 932–943.

[23] J. Wang, Y. Yang, S. Wang, Y. Hu, D. Wang, and Q. Wang, “Context-
aware in-process crowdworker recommendation,” ser. ICSE 2020,
2020.

[24] J. Wang, Y. Yang, S. Wang, C. Chen, D. Wang, and Q. Wang,
“Context-aware personalized crowdtesting task recommenda-
tion,” IEEE Transactions on Software Engineering, 2021.

[25] C. B. Seaman, “Qualitative methods in empirical studies of soft-
ware engineering,” IEEE Trans. Software Eng., vol. 25, no. 4, pp.
557–572, 1999.

[26] S. Ren, K. He, R. B. Girshick, and J. Sun, “Faster R-CNN: towards
real-time object detection with region proposal networks,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149,
2017. [Online]. Available: https://doi.org/10.1109/TPAMI.2016.
2577031

[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
for image recognition,” in 2016 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-
30, 2016.
IEEE Computer Society, 2016, pp. 770–778. [Online].
Available: https://doi.org/10.1109/CVPR.2016.90

[28] ——, “Deep residual learning for image recognition,” in 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Los Alamitos, CA, USA:
jun 2016,
pp. 770–778. [Online]. Available: https://doi.ieeecomputersociety.
org/10.1109/CVPR.2016.90

IEEE Computer Society,

[29] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An
efﬁcient alternative to sift or surf,” in Proceedings of the 2011
International Conference on Computer Vision, ser. ICCV ’11. USA:
IEEE Computer Society, 2011, p. 2564–2571. [Online]. Available:
https://doi.org/10.1109/ICCV.2011.6126544

[30] D. G. Lowe, “Distinctive image features from scale-invariant
keypoints,” Int. J. Comput. Vision, vol. 60, no. 2, p. 91–110,
Nov. 2004. [Online]. Available: https://doi.org/10.1023/B:VISI.
0000029664.99615.94

[31] H. Bay, T. Tuytelaars, and L. Van Gool, “Surf: Speeded up
robust features,” in Proceedings of the 9th European Conference on
Computer Vision - Volume Part I, ser. ECCV’06. Berlin, Heidelberg:
Springer-Verlag, 2006, p. 404–417.
[Online]. Available: https:
//doi.org/10.1007/11744023 32

[32] S. B. Kotsiantis, I. Zaharakis, and P. Pintelas, “Supervised machine
learning: A review of classiﬁcation techniques,” Emerging artiﬁcial
intelligence applications in computer engineering, vol. 160, pp. 3–24,
2007.

[33] A. Berson, S. Smith, and K. Thearling, “An overview of data
mining techniques,” Building Data Mining Application for CRM,
2004.

[34] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.

5–32, 2001.

[35] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT

press, 2016.

[36] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol.

521, no. 7553, pp. 436–444, 2015.

[37] S. Ma, Z. Xing, C. Chen, C. Chen, L. Qu, and G. Li, “Easy-
to-deploy api extraction by multi-level feature embedding and
transfer learning,” IEEE Transactions on Software Engineering, pp.
1–1, 2019.

[38] C. D. Manning, P. Raghavan, and H. Sch ¨utze, Introduction to

information retrieval. Cambridge university press, 2008.

[39] Y. Li, Z. Yang, Y. Guo, and X. Chen, “Droidbot: A lightweight
ui-guided test input generator for android,” in Proceedings of
the 39th International Conference on Software Engineering Companion,
IEEE Press, 2017, p. 23–26. [Online]. Available:
ser. ICSE-C ’17.
https://doi.org/10.1109/ICSE-C.2017.8

[40] S. Zein, N. Salleh, and J. Grundy, “A systematic mapping study
of mobile application testing techniques,” Journal of Systems and
Software, vol. 117, pp. 334–356, 2016.

[41] T. L¨ams¨a, “Comparison of gui testing tools for android applica-

tions,” 2017.

[42] S. P. Reiss, Y. Miao, and Q. Xin, “Seeking the user interface,”

Automated Software Engineering, vol. 25, no. 1, pp. 157–193, 2018.

[43] F. Behrang, S. P. Reiss, and A. Orso, “Guifetch: supporting app
design and development through gui search,” in Proceedings of
the 5th International Conference on Mobile Software Engineering and
Systems. ACM, 2018, pp. 236–246.

[44] J. Chen, C. Chen, Z. Xing, X. Xia, L. Zhu, J. Grundy, and
J. Wang, “Wireframe-based ui design search through image au-
toencoder,” ACM Transactions on Software Engineering and Method-
ology (TOSEM), vol. 29, no. 3, pp. 1–31, 2020.

[45] C. Chen, S. Feng, Z. Xing, L. Liu, S. Zhao, and J. Wang, “Gallery dc:
Design search and knowledge discovery through auto-created gui
component gallery,” Proceedings of the ACM on Human-Computer
Interaction, vol. 3, no. CSCW, pp. 1–22, 2019.

[46] D. Zhao, Z. Xing, C. Chen, X. Xia, and G. Li, “Actionnet: vision-
based workﬂow action recognition from programming screen-
casts,” in 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE).
IEEE, 2019, pp. 350–361.

[47] B. Yang, Z. Xing, X. Xia, C. Chen, D. Ye, and S. Li, “Uis-hunter:
Detecting ui design smells in android apps,” in 2021 IEEE/ACM
43rd International Conference on Software Engineering: Companion
Proceedings (ICSE-Companion).

IEEE, 2021, pp. 89–92.

[48] ——, “Don’t do that! hunting down visual design smells in
complex uis against design guidelines,” in 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE).
IEEE, 2021,
pp. 761–772.

[49] Q. Chen, C. Chen, S. Hassan, Z. Xing, X. Xia, and A. E. Hassan,
“How should i improve the ui of my app? a study of user reviews
of popular apps in the google play,” ACM Transactions on Software
Engineering and Methodology (TOSEM), vol. 30, no. 3, pp. 1–38,
2021.

[50] T. A. Nguyen and C. Csallner, “Reverse engineering mobile ap-
plication user interfaces with remaui (t),” in Automated Software
Engineering (ASE), 2015 30th IEEE/ACM International Conference on.
IEEE, 2015, pp. 248–259.

[51] C. Chen, T. Su, G. Meng, Z. Xing, and Y. Liu, “From ui design
image to gui skeleton: a neural machine translator to bootstrap
mobile gui implementation,” in Proceedings of the 40th International
Conference on Software Engineering. ACM, 2018, pp. 665–676.
[52] K. Moran, C. Bernal-C´ardenas, M. Curcio, R. Bonett, and
D. Poshyvanyk,
learning-based prototyping of
graphical user interfaces for mobile apps,” IEEE Trans. Software
Eng., vol. 46, no. 2, pp. 196–221, 2020.
[Online]. Available:
https://doi.org/10.1109/TSE.2018.2844788

“Machine

[53] S. Chen, L. Fan, C. Chen, T. Su, W. Li, Y. Liu, and L. Xu,
“Storydroid: Automated generation of storyboard for android
apps,” in 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE).
IEEE, 2019, pp. 596–607.

[54] S. Chen, L. Fan, C. Chen, M. Xue, Y. Liu, and L. Xu, “Gui-squatting
attack: Automated generation of android phishing apps,” IEEE
Transactions on Dependable and Secure Computing, 2019.

[55] S. Feng, S. Ma, J. Yu, C. Chen, T. Zhou, and Y. Zhen, “Auto-icon:
An automated code generation tool for icon designs assisting in
ui development,” in 26th International Conference on Intelligent User
Interfaces, 2021, pp. 59–69.

[56] T. Zhao, C. Chen, Y. Liu, and X. Zhu, “Guigan: Learning to gener-
ate gui designs using generative adversarial networks,” in 2021
IEEE/ACM 43rd International Conference on Software Engineering
(ICSE).

IEEE, 2021, pp. 748–760.

[57] D. Zhao, Z. Xing, C. Chen, X. Xu, L. Zhu, G. Li, and J. Wang,
“Seenomaly: vision-based linting of gui animation effects against
design-don’t guidelines,” in 2020 IEEE/ACM 42nd International
Conference on Software Engineering (ICSE).
IEEE, 2020, pp. 1286–
1297.

[58] C. Chunyang, F. Sidong, L. Zhengyang, X. Zhenchang, Z. Sheng-
dong, and L. Linda, “From lost to found: Discover missing ui
design semantics through recovering missing tags,” in Proceedings
of the ACM on Human-Computer Interaction, Volume. 4, No. CSCW,
November 2020, 2020.

[59] J. Chen, M. Xie, Z. Xing, C. Chen, X. Xu, L. Zhu, and
interface: old
G. Li, “Object detection for graphical user
fashioned or deep learning or a combination?” in ESEC/FSE

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

16

[78] T. Saar, M. Dumas, M. Kaljuve, and N. Semenenko, “Browserbite:
cross-browser testing via image processing,” Software: Practice and
Experience, vol. 46, no. 11, pp. 1459–1477, 2016.

[79] S. Mahajan, N. Abolhassani, P. McMinn, and W. G.

J.
Halfond, “Automated repair of mobile friendly problems in
web pages,” in Proceedings of the 40th International Conference
on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 -
June 03, 2018. ACM, 2018, pp. 140–150.
[Online]. Available:
https://doi.org/10.1145/3180155.3180262
[80] S. Mahajan, K. B. Gadde, A. Pasala,

J.
Halfond, “Detecting and localizing visual inconsistencies in web
applications,” in 23rd Asia-Paciﬁc Software Engineering Conference,
APSEC 2016, Hamilton, New Zealand, December 6-9, 2016.
IEEE
Computer Society, 2016, pp. 361–364.
[Online]. Available:
https://doi.org/10.1109/APSEC.2016.060

and W. G.

[81] S. Mahajan, A. Alameer, P. McMinn, and W. G. J. Halfond, “Xﬁx:
an automated tool for the repair of layout cross browser issues,”
in Proceedings of the 26th ACM SIGSOFT International Symposium
on Software Testing and Analysis, Santa Barbara, CA, USA, July 10 -
14, 2017, T. Bultan and K. Sen, Eds. ACM, 2017, pp. 368–371.
[Online]. Available: https://doi.org/10.1145/3092703.3098223
[82] S. Mahajan, A. Alameer, P. McMinn, and W. G. Halfond, “Effective
automated repair of internationalization presentation failures in
web applications using style similarity clustering and search-
based techniques,” Software Testing, Veriﬁcation and Reliability,
vol. 31, no. 1-2, p. e1746, 2021.

[83] A. Alameer, P. T. Chiou, and W. G. Halfond, “Efﬁciently repairing
internationalization presentation failures by solving layout con-
straints,” in 2019 12th IEEE Conference on Software Testing, Validation
and Veriﬁcation (ICST).

IEEE, 2019, pp. 172–182.

[84] I. Althomali, G. M. Kapfhammer, and P. McMinn, “Automatic
visual veriﬁcation of layout failures in responsively designed web
pages,” in 12th IEEE Conference on Software Testing, Validation and
Veriﬁcation, ICST 2019, Xi’an, China, April 22-27, 2019.
IEEE, 2019,
pp. 183–193. [Online]. Available: https://doi.org/10.1109/ICST.
2019.00027

’20: 28th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, Virtual Event,
USA, November 8-13, 2020. ACM, 2020, pp. 1202–1214. [Online].
Available: https://doi.org/10.1145/3368089.3409691

[60] K. Moran, B. Li, C. Bernal-C´ardenas, D. Jelf, and D. Poshyvanyk,
“Automated reporting of gui design violations for mobile apps,”
in Proceedings of the 40th International Conference on Software Engi-
neering. ACM, 2018, pp. 165–175.

[61] K. Moran, C. Watson, J. Hoskins, G. Purnell, and D. Poshyvanyk,
“Detecting and summarizing GUI changes in evolving mobile
apps,” in Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering, ASE 2018, Montpellier, France,
September 3-7, 2018, M. Huchard, C. K¨astner, and G. Fraser,
Eds. ACM, 2018, pp. 543–553.
[Online]. Available: https:
//doi.org/10.1145/3238147.3238203

[62] J. Chen, C. Chen, Z. Xing, X. Xu, L. Zhu, G. Li, and J. Wang,
“Unblind your apps: predicting natural-language labels for mobile
GUI components by deep learning,” in ICSE ’20: 42nd International
Conference on Software Engineering, Seoul, South Korea, 27 June - 19
July, 2020, G. Rothermel and D. Bae, Eds. ACM, 2020, pp. 322–334.
[Online]. Available: https://doi.org/10.1145/3377811.3380327

[63] http://tools.android.com/tips/lint, 2020.
[64] S. Chen, C. Chen, L. Fan, M. Fan, X. Zhan, and Y. Liu, “Accessible
or not an empirical investigation of android app accessibility,”
IEEE Transactions on Software Engineering, 2021.

[65] https://github.com/stylelint/stylelint, 2020.
[66] Y. Gao, Y. Luo, D. Chen, H. Huang, W. Dong, M. Xia, X. Liu, and
J. Bu, “Every pixel counts: Fine-grained ui rendering analysis for
mobile applications,” in IEEE INFOCOM 2017-IEEE Conference on
Computer Communications.

IEEE, 2017, pp. 1–9.

[67] W. Li, Y. Jiang, C. Xu, Y. Liu, X. Ma, and J. Lu, “Characterizing and
detecting inefﬁcient image displaying issues in android apps,”
in 26th IEEE International Conference on Software Analysis, Evolution
and Reengineering, SANER 2019, Hangzhou, China, February 24-27,
2019.
[Online]. Available: https:
//doi.org/10.1109/SANER.2019.8668030

IEEE, 2019, pp. 355–365.

[68] F. Nayebi, J.-M. Desharnais, and A. Abran, “The state of the art of
mobile application usability evaluation,” in 2012 25th IEEE Cana-
dian Conference on Electrical and Computer Engineering (CCECE).
IEEE, 2012, pp. 1–4.

[69] A. Holzinger, P. Treitler, and W. Slany, “Making apps useable on
multiple different mobile platforms: On interoperability for busi-
ness application development on smartphones,” in International
Conference on Availability, Reliability, and Security.
Springer, 2012,
pp. 176–189.

[70] T. D. White, G. Fraser, and G. J. Brown, “Improving random gui
testing with image-based widget detection,” in Proceedings of the
28th ACM SIGSOFT International Symposium on Software Testing and
Analysis. ACM, 2019, pp. 307–317.

[71] C. Degott, N. P. Borges Jr, and A. Zeller, “Learning user interface
element interactions,” in Proceedings of the 28th ACM SIGSOFT
International Symposium on Software Testing and Analysis. ACM,
2019, pp. 296–306.

[72] T. Zhang, J. Gao, J. Cheng, and T. Uehara, “Compatibility test-
ing service for mobile applications,” in 2015 IEEE Symposium on
Service-Oriented System Engineering.

IEEE, 2015, pp. 179–186.

[73] T. Ki, C. M. Park, K. Dantu, S. Y. Ko, and L. Ziarek, “Mimic: Ui
compatibility testing system for android apps,” in 2019 IEEE/ACM
41st International Conference on Software Engineering (ICSE).
IEEE,
2019, pp. 246–256.

[74] I. Kondratova and I. Goldfarb, “Culturally appropriate web user
interface design study: Research methodology and results,” in
Handbook of research on culturally-aware information technology: Per-
spectives and models.

IGI Global, 2011, pp. 316–336.

[75] S. Mahajan and W. G. Halfond, “Detection and localization of html
presentation failures using computer vision-based techniques,” in
2015 IEEE 8th International Conference on Software Testing, Veriﬁca-
tion and Validation (ICST).

IEEE, 2015, pp. 1–10.

[76] ——, “Websee: A tool for debugging html presentation failures,”
in 2015 IEEE 8th International Conference on Software Testing, Veriﬁ-
cation and Validation (ICST).

IEEE, 2015, pp. 1–8.

[77] S. Mahajan, B. Li, P. Behnamghader, and W. G. Halfond, “Using
visual symptoms for debugging presentation failures in web appli-
cations,” in 2016 IEEE International Conference on Software Testing,
Veriﬁcation and Validation (ICST).

IEEE, 2016, pp. 191–201.

