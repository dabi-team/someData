2
2
0
2

r
p
A
8
1

]
L
M

.
t
a
t
s
[

1
v
4
7
5
8
0
.
4
0
2
2
:
v
i
X
r
a

Adaptive Noisy Data Augmentation for Regularized Estimation
and Inference in Generalized Linear Models

Yinan Li1 and Fang Liu1∗
1 Department of Applied and Computational Mathematics and Statistics

University of Notre Dame, Notre Dame, IN 46556, U.S.A.

Abstract

We propose the AdaPtive Noise Augmentation (PANDA) procedure to regularize the
estimation and inference of generalized linear models (GLMs). PANDA iteratively op-
timizes the objective function given noise augmented data until convergence to obtain
the regularized model estimates. The augmented noises are designed to achieve various
regularization eﬀects, including l0, bridge (lasso and ridge included), elastic net, adaptive
lasso, and SCAD, as well as group lasso and fused ridge. We examine the tail bound
of the noise-augmented loss function and establish the almost sure convergence of the
noise-augmented loss function and its minimizer to the expected penalized loss function
and its minimizer, respectively. We derive the asymptotic distributions for the regular-
ized parameters, based on which, inferences can be obtained simultaneously with variable
selection. PANDA exhibits ensemble learning behaviors that help further decrease the
generalization error. Computationally, PANDA is easy to code, leveraging existing soft-
ware for implementing GLMs, without resorting to complicated optimization techniques.
We demonstrate the superior or similar performance of PANDA against the existing ap-
proaches of the same type of regularizers in simulated and real-life data. We show that
the inferences through PANDA achieve nominal or near-nominal coverage and are far
more eﬃcient compared to a popular existing post-selection procedure.

keywords: l0 penalty, augmented Fisher information, ensemble learning, noise injection
and augmentation, regularization and penalization, inference

Introduction

1
Regularization of generalized linear models (GLMs) to mitigate overﬁtting and conduct vari-
able selection is a well-studied topic. There exist a variety of regularizers, such as bridge
(Frank and Friedman, 1993), ridge (l2), lasso (l1) (Tibshirani, 1996), elastic net (Zou and
Hastie, 2005), SCAD (Fan and Li, 2001), adaptive lasso (Zou, 2006), group lasso (Yuan and
Lin, 2014), fused lasso (Tibshirani et al., 2005), sparse group lasso (SGL) (Simon et al., 2013),
among others. As for the inference for regression coeﬃcients in penalized GLMs, many exist-
ing approaches are post-election procedures, meaning the inference is initiated after variable
selection and oftentimes non-selected variables are assumed to be of no inferential interest and
no uncertainty quantiﬁcation are provided for the corresponding regression coeﬃcients (Leeb
and Pötscher, 2005; Leeb et al., 2006; Berk et al., 2013; Zhang and Zhang, 2013; Javanmard

∗Corresponding author email: fang.liu.131@nd.edu

1

 
 
 
 
 
 
and Montanari, 2014; Lockhart et al., 2014; Efron, 2014; Lee et al., 2016; Tibshirani et al.,
2016; Reid et al., 2017; Taylor and Tibshirani, 2017). Procedures for simultaneous variable
selection and inference do exist. Fan and Li (2001) provide simultaneous variance estimates for
the coeﬃcients of selected estimated variables (so non-zero coeﬃcient estimates). For linear
regression, (Zhang and Zhang, 2013; Javanmard and Montanari, 2014) provide simultaneous
variable selection with the lasso penalty and inference for both zero or non-zero coeﬃcient
estimates. Van de Geer et al. (2014) propose a procedure for constructing conﬁdence inter-
vals and hypothesis testing for a low-dimensional subset of a large parameter vector in the
high-dimensional GLM setting with convex loss functions with the lasso penalty.

Despite the extensiveness of the work on regularized variable selection in GLMs, there is still
room for improvement over the existing solutions. Two of these areas are the l0 regularization
and inference in regularized GLMs. Optimization with the l0 penalty is NP-hard. Dicker et al.
(2013) propose the seamless-l0 (SELO) penalty to approximate the l0 penalty and a coordinate
descent algorithm to obtain solutions in the context of the least-squares optimization and
p < n. SELO outperforms SCAD in model error and variable selection accuracy rate per
the empirical studies. Liu and Li (2016) propose an EM algorithm that approximates the
l0 regularized regression by solving a sequence of l2 optimizations. The method deals with
p > n, but is examined only in the least-squares setting and does not provide inferential
procedures. Regarding the inference for regularized GLMs, as mentioned above, the majority
of existing methods operate in a post-selection matter and thus focus on the inference for
selected variables only. Fan and Li (2001) and Dicker et al. (2013) provide standard errors for
the parameter estimates in non-convex optimization, and again for selected variables only.

We propose a novel general regularization framework, AdaPtive Noisy Data Augmentation
(PANDA), for GLMs that 1) achieves the l0 penalty in addition to all the above mentioned
existing penalty types, 2) obtains inference in regularized GLMs for both zero and non-zero
coeﬃcients, and 3) enjoys simple practical implementation that would greatly appeal to prac-
titioners. In brief, PANDA augments the original n observations with properly designed ne
noise terms to achieve the desired regularization eﬀects on model parameters. PANDA is
iterative and the variance terms of the augmented noise are adaptive to the most updated
parameter estimates until the algorithm converges. One requirement on ne is the augmented
data size n + ne > p so to allow for the ordinary least squares (OLS) or maximum likelihood
estimation (MLE) procedures to be applied without resorting to complicated optimization
algorithms. As such, PANDA is computationally straightforward and eﬃcient. PANDA is
also ﬂexible and general. By properly designing the variance of the augmented noise, PANDA
can achieve various regularization eﬀects, including lγ, for 0 ≤ γ ≤ 2 (including l0, lasso,
ridge as special cases), elastic net, SCAD, group lasso, and fused ridge. PANDA achieves
close-to-exact l0 regularization by promoting orthogonality between the coeﬃcients and the
augmented noise vector. When ne < p, PANDA shrinks exactly ne parameters towards 0
upon convergence. PANDA is more capable and more eﬃcient inferentially compared to exist-
ing inferential approaches for regularized GLMs. It conducts variable selection and provides
inference for coeﬃcients simultaneously, whether the coeﬃcients are estimated to be zero or
not. Our empirical results suggest the inference based on PANDA is valid and more eﬃcient
compared to some existing post-selection procedures. Finally, PANDA is theoretically justi-
ﬁed. We establish the Gaussian tail of the noise-augmented loss function and the almost sure

2

convergence to its expectation under some regularity conditions, providing theoretical justiﬁ-
cation for PANDA as a regularization technique and that the noise-augmented loss function
is trainable for practical implementation.

The optimizer calculated by PANDA from a GLM is similar to the local quadratic approx-
imation (LQA) technique (Tibshirani, 1996; Fan and Li, 2001), but with several important
diﬀerences. First, LQA cannot yield the l0 penalty while PANDA can achieve close-to-exact
l0 regularization; second, LQA relies on analytical work to approximate penalized loss func-
tion with a quadratic form, followed by the optimization of the quadratic function, whereas
PANDA only needs to augment the original data with noisy samples and then leverage existing
software to compute OLS/MLE from GLMs.

The rest of the paper is organized as follows. Sec 2 presents the PANDA algorithm and the reg-
ularization eﬀects it brings to GLMs. Sec 3 establishes the consistency on the noise-augmented
loss function and the regularized parameter estimates, presents the Fisher information of the
model parameters in augmented data, examines PANDA’s ensemble learning behavior, and
provides the asymptotic distributions for the parameter estimates via PANDA. Sec 4 demon-
strates the l0 penalty realized by PANDA, compares PANDA to a popular post-selection
inferential approach in statistical inferences for GLMs, and implements PANDA in simulated
and real-life studies to show its eﬀectiveness in regularizing GLM estimation. Sec 5 provides
some concluding remarks and oﬀers future research directions on PANDA.

2 Methodology

2.1 Noise Augmentation Scheme and Regularization Eﬀect in PANDA
Let Y be the outcome variable and X = (X1, . . . , Xp)T be the independent variables. GLM
is based on the assumption that the conditional distribution of Y given X comes from an
exponential family

p(Y |X) = exp (Y η − B(η) + h(Y )) ,

(1)

where η = θ0 + θX if the canonical link is used (e.g., the
identity link for Gaussian Y ; the logit link for Bernoulli Y ).
When p is large, regularization or penalty is often imposed
on θ when estimating θ.

PANDA regularizes the estimation of θ by ﬁrst augmenting
the observed data with a noisy data matrix. Fig 1 depicts
a schematic of data augmentation in PANDA, where the
augmented noise ei to y is ¯y, the sample average of y. For
logistic regression, ei ∼ Bern(ˆp), where ˆp is the sample pro-
portion of an event. The augmented data ex to x are drawn
from the Noise Generating Distributions (NGD), the vari-
ance term of which is function of θ and tuning parameters
λ.

ex ∼ N (0, V(θ; λ))

(2)

Proposition 1 (regularization eﬀects of PANDA for
GLM). Denote the loss function given the observed data

3

Figure 1: A schematic of the data
augmentation for GLM in PANDA
(for
ey,i ∼
Bern(ˆp), where ˆp is the sample pro-
portion of an event)

logistic regression,

(cid:16)

(cid:16)

i=1

h(yi)+

θ0+(cid:80)

(cid:17)
(x, y) by l(θ|x, y) = −(cid:80)n
jθjxij)
likelihood function), and that given the noise augmented data (˜x, ˜y)) by
θ0 + (cid:80)

j θj ˜xij
where θ = (θ0, {θj}j=1,...,p). The expectation of the Taylor series of lp around (cid:80)
the distribution of ex is

yi−j(θ0+(cid:80)

lp(θ|˜x, ˜y) = −

(cid:110)(cid:80)n+ne

θ0 + (cid:80)

h(˜yi)+

j θj ˜xij

jθjxij

−Bj

˜yi

i=1

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:16)

(cid:17)

(the negative log-

(cid:17)(cid:111)

,

(3)

j θjxij = 0 over

Eex(lp(θ|˜x, ˜y)) = l(θ|x, y) + P (θ), where

where C1 = 2−1B(cid:48)(cid:48)(θ0) and C = (cid:80)ne

P (θ) = ne

(cid:16)

C1

(cid:17)

(cid:80)

j θ2

j V(ej)

j V2(ej)(cid:1)(cid:17)
(cid:0)θ4
i=1 (h(ey,i)+ey,iθ0)+B(θ0) are constants independent of θ−0.

(cid:16)
ne

(cid:80)
j

+ C,

+O

(4)

The proof is given in Sec S.1 of the supplementary materials. The regularization eﬀect P (θ)
in Eqn (4) depends on the variance term of the NGD in Eqn (2) from which the augmented
noise to x is sampled. Eqns (5) to (8) list some examples of the NGD from which ej, the noise
term that augments xj for j = 1, . . . , p, is drawn and their expected regularization eﬀects.

bridge: ej ∼ N (cid:0)0, λ|θj|−γ(cid:1) for γ ∈ [0, 2]

including l0 when γ = 2, lasso when γ = 1, and ridge when γ = 0;

elastic net: ej ∼ N (cid:0)0, λ|θj|−1 + σ2(cid:1) ;
0, λ|θj|−1|ˆθj|−γ(cid:17)

adaptive lasso: ej ∼ N

(cid:16)

, where ˆθj is a consistent estimate for θj;

(5)

(6)

(7)

SCAD: ej

(cid:40)= 0 if |θj| > aneλ
(cid:16) λ
(cid:17)
|θj | − (a+1)
0,

∼ N

2a2ne

(cid:16)

1(0,neλ](|θj|)+ 1

(a−1)

(cid:16)aλ
|θj | − λ2ne

2θ2
j

− 2a2−1
2a2ne

(cid:17)

(cid:17)
1(neλ,aneλ](|θj|)

o.w.

,

where 1(l,u)(|θj|) = 1 if l < |θj| < u, 0 o.w.;

(8)
For regularizing a group of q parameters θq = (θ1, . . . , θq) simultaneously (e.g., genes on
the same pathway, binary dummy variables created from the same categorical attribute),
the NGDs in Eqns (9) and (11) can be used. Speciﬁcally, the group-lasso penalty sets all q
parameters in θq either at zero or nonzero simultaneously; and the fused-ridge and fused-lasso
penalties promote numerical similarity among θq.
(cid:18)

√

(cid:19)

group lasso: ej(l) ∼ N

0,

, where θ(l) = {θj(l)} for j = 1, . . . , ql; l = 1, . . . , g groups (9)

λ
ql
||θl||2

fused ridge: e = (e1, . . . , eq) ∼ N(q) (0, λ(TT(cid:48))) , where entries in T are

(10)

Tk,k = 1, Tk+1−k·1(k=q),k = −1; and 0 o.w.;

fused lasso: e = (e1, . . . , eq) ∼ N(q) (0, λ(TT(cid:48))) , where Tkk(cid:48) = λ|θj −θk(cid:48)|−1for k (cid:54)= k(cid:48).

(11)
The tuning parameters σ2 ≥ 0, λ > 0, 0 ≤ γ < 2, a > 2 in Eqn (5) and (11) can be
user-speciﬁed or chosen by a model selection criterion such as cross-validation (CV), AIC, or
BIC. The dispersion of the noise term varies by X in general. Xj associated with small |θj|
is augmented with more spread-out noises, and Xj with large |θj| is augmented with noises
concentrated around 0. The exceptions are the ridge (γ = 0 in Eqn (5)) and fused ridge
regularizations (Eqn (10)), where the variance term remains constant for θj for all j.
(cid:1)2,
(cid:0)˜yi −(cid:80)
For linear regression, the noise-augmented loss function is lp(θ|˜x, ˜y) = (cid:80)n+ne
and the penalty P (θ) realized by PANDA with diﬀerent types of NGD can be obtained in
closed form (Table 1) and are exact as suggested by the names. P (θ) in Table 1 can be

j ˜xijθj

i=1

4

easily derived based on the results in Sec S.1 of the supplementary materials. Compared to
the original SCAD in Fan and Li (2001) for linear regression, the SCAD penalty realized by
PANDA for |θj| < λne is not l1 as in the original SCAD but closer to a l0.5 penalty; the middle
segment penalty |θj| ∈ (λne, aneλ] is not exactly the same as the original SCAD either, but
it has the same functionality by shrinking |θj| ∈ (λne, |θj| < aλne] toward 0 and connecting
the two end segments to form an overall smooth penalty for θj; for |θj| > aλne, there is no
penalty as in the original SCAD.

Table 1: Close-formed penalty term in regularized linear regression (Gaussian Y ) via PANDA

NGD

lγ
EN
adaptive
SCAD

Eqn (5)
Eqn (6)
Eqn (7)
Eqn (8) (cid:80)p

P (θ) when Y is Gaussian
(cid:80)
(λne) (cid:80)p
(cid:80)
(λne) (cid:80)p
(cid:80)
(λne) (cid:80)p

j(cid:54)=k |θj|2−γ
j(cid:54)=k |θj| + (σ2ne) (cid:80)p
j(cid:54)=k |θj||ˆθj|−γ,

j=1

j=1

j=1

(cid:80)

j(cid:54)=k θ2
j

j=1

group lasso Eqn (9)

j=1

(cid:8)(neλ|θj| − (a + 1)θ2/(2a2ne)))·1|θj |≤λne+
(cid:0)aλne|θj|−(λne)2/2−θ2
ql||θl||2

√

(λne) (cid:80)g

l=1

j (1−1/(2a2))(cid:1) (a−1)−1·1|θj |∈(λne,aλne]

(cid:9)

When Y is non-Gaussian, the achieved regularization eﬀects P (θ) in Eqns (5) to (11) are
second-order approximate. For example, Table 2 lists the analytical form of P (θ) for the
lasso-type noise (γ = 1 in Eqn (5)). For all the regression types, in addition to the l1 penalty,
there is an additional big-O term on θ, which is arbitrarily small under some regularity
conditions (more details are provided in Sec 2.3).

Table 2: Expected penalty term in PANDA with lasso-type noise ej ∼ N (0, λ|θj|−γ)

P (θ)
λne
2
λne

Y
Bernoulli
Exponential
Poisson
Negative Binomial λne
2

λne

exp(θ0)
(1+exp(θ0))2

j |θj| + O(λ2ne||θ||2

(cid:80)
2 exp(θ0) (cid:80)
j |θj| + O(λ2ne||θ||2
2 exp(θ0) (cid:80)
j |θj| + O(λ2ne||θ||2
(cid:80)

j |θj| + O(λ2ne||θ||2

r exp(θ0)
(r+exp(θ0))

2) + C
2) + C

2) + C

2) + C (r is the # of failures)

For relatively small ne, especially when ne < p, PANDA promotes sparsity on θ by imposing
ne linear constraints on θ. Applying the second-order approximation at eT

lp(θ|˜x) = −

(cid:110)(cid:80)n+ne

(cid:16)

i=1

h(˜yi)+

(cid:16)

θ0 + (cid:80)

(cid:17)

(cid:17)

˜yi

−Bj

(cid:16)

θ0 + (cid:80)

≈ l(θ|x) + C1

(cid:80)ne
i=1

(cid:16)(cid:80)

j θjeij

j θj ˜xij
(cid:17)2

+ C,

i θ = 0, we have
j θj ˜xij

(cid:17)(cid:111)

(12)

where C1 and C the same as in Eqn (4). The regularization eﬀect obtained in Eqn (12)
with ﬁxed ne is diﬀerent from the regularization presented in Proposition 1 in the sense that
it takes eﬀect by promoting the orthogonality between θ and ex,i, i = 1 . . . , ne rather than
penalizing the individual parameters. The formal results are given in Proposition 2. The
proof is provided in Sec S.2 of the supplementary materials.

Proposition 2 (orthogonal regularization eﬀect of PANDA for GLM with ﬁxed ne).
With ﬁxed ne and the approximate loss function in Eqn (12), PANDA estimates θ by solving
(13)

ˆθ = arg minθ lp(θ|˜x) ≈ arg minθ(l(θ|x) + C1

(cid:0)eT

i θ(cid:1)2

(cid:80)ne
i=1

5

in each iteration, which is conceptually equivalent to the constrained optimization problem

(cid:80)ne

min l(θ|x) subject to
(cid:16)
i θ)2 ≤ (cid:80)ne
i=1
i=1(eT
i

∃ 0 < di < ((cid:80)ne

i=1(eT

(cid:17)2

eT
i

ˆθ

or equivalently,

ˆθ)2)1/2 such that |eT

i θ| ≤ di, for i = 1, . . . , ne.

(14)

(15)

ˆθ in Eqns (14) and (15) is the solution from Eqn (13). Proposition 2 suggests that the (uncon-
strained) optimization problem PANDA solves in each iteration is equivalent to a constrained
optimization problem with ne linear constraints on θ. When ne < p, the ne constraints in Eqn
(15) only aﬀect a subset of the p parameters. For the l0 penalty (γ = 2 in Eqn 5)) and when
λ is large, the constraints take eﬀect on exactly ne parameters. In other words, the following
two optimization problems are equivalent.

(cid:80)ne
i=1

(cid:0) (cid:80)

j θjeij

(cid:1)2(cid:9)

(16)

Problem 1: ¯θ = Ee(ˆθ) = Ee
Problem 2: ˆθ = arg minθ l(θ|x), subject to (cid:80)p

(cid:8)arg minθ(l(θ|x) + C1

j=1 1(θj (cid:54)= 0) = p − ne

(17)
Figure 2 plots the heat maps of the constrained region on θ = (θ1, θ2) as suggested by Eqn (14)
for ne = 1, 2 and 10, respectively when ˆθ = (0.01, 1) (the upper panel) and ˆθ = (0.01, 0.01)
(the bottom panel), with the l0 penalty. Speciﬁcally, each heat map is made of 50, 000 “dots”
uniformly distributed in the [2, 2] × [2, 2] solution region (for plotting purposes, we focus on
the region of [2, 2]2; in theory, the region can be as large as (−∞, ∞)2). The relative density
of a particular constraint on θ out the 5, 000 repeats is proportional to the grayness of the
dot. In the upper panel, with ne = 1, the chance of constraining θ1 at 0 is much higher than
at any non-zero values. As ne increases from 1 to 2 to 10, the constrained region for θ shrinks
(to 0 for θ1 and to within [−1, 1] for θ2). In the bottom panel, setting ne = 1 still lead a
substantial chance of getting non-zero θ. As ne increases to 2 and 10, the chance of θ = 0
drastically increases and is almost certain at ne = 10.

As ne further increases, the regularization eﬀect moves away from promoting the orthogonality
between ex and θ to focusing more on the individual parameter regularization, and eventually
converges to those in Proposition 1. In practice, ne can be pre-speciﬁed if users have prior
knowledge on the sparsity of θ, otherwise be regarded as a tuning parameter, chosen by the
CV procedure or an information criterion (AIC or BIC) for model selection.

2.2 Algorithmic Steps of PANDA
The practical implementation of PANDA starts with some initial values for θ. The estimates of
θ and the variance terms of the pre-speciﬁed NGD are updated iteratively until convergence.
The detailed steps are listed in Algorithm 1, along with some remarks on specifying the
algorithmic parameters and convergence criterion (Remarks 1 to 5).

Remark 1 (convergence criterion). We provide several choices to evaluate the convergence
of the PANDA algorithm. First, we may eyeball the trace plots of ¯l(t), which is often suﬃcient.
Second, we can apply a cutoﬀ value, say τ on the absolute percentage change on ¯l(t) from two
consecutive iterations: if |¯l(t+1) −¯l(t)|/¯l(t) < τ , then we may declare convergence. τ is supposed
to be close-to-0 upon convergence, but being arbitrarily close to 0 would be diﬃcult to achieve
given the ﬂuctuation around ¯l(t) with ﬁnite m or ne due to the randomness of the augmented
noises from iteration to iteration. Finally, we develop a formal statistical test for convergence

6

Figure 2: Heat maps of the l0 constraint through the orthogonal regularization in PANDA with ﬁxed
ne for 2-dimensional θ = (θ1, θ2)

based on ¯l(t); but the test should be used with caution as it tends to claim non-convergence.
The details of the test are provided in Sec S.7 of the supplementary materials.

Remark 2 (maximum iteration T ). T should be set at a number large enough so to
allow the algorithm to reach convergence within a reasonable time period. When ne is large,
we expect the algorithm to converge with a relatively small T (in the examples in Sec 4,
convergence is achieved for T ≤ 20 with a large ne). If ne is small, especially when PANDA
is used to realize the l0 regularization, T should be set a large number for convergence.

Remark 3 (m and r). In practical implement, ne, no matter how large, is still ﬁnite. In
addition, one might not want to set ne at a very large value as it will slow down the per-
iteration computation. With a ﬁnite ne, there is random ﬂuctuation around the loss function
and parameter estimates since each iteration is based on a diﬀerent set of ﬁnite samples, even
when the PANDA algorithm converges. To mitigate the random ﬂuctuation, we can take the
moving averages of the estimated parameters over multiple (m) iterations. The same rationale
applies to the banking of r estimates after convergence. In addition, taking the averages of the
estimates obtained from the multiple augmented data sets also leads to a small generalization
error due to the ensemble-learning type of eﬀect PANDA brings (see Sec 3.4 for more details).
In our empirical studies, r = O(10) seems to be suﬃcient.

Remark 4 (Bounding at τ0). The bounding at τ0 is necessary. Despite the fact that
estimates of zero-valued θ can get arbitrarily close to 0 (see Sec 3.1 for the almost sure
convergence of the minimizers in PANDA), being exactly 0 cannot be achieved computationally
in practice due to the numerical nature of PANDA. In addition, after the convergence of the

7

−2−1012−2−1012q1q2q^1 =0.01 , q^2 =1 , l =1 , g =2 , ne =1197193289384480576672768864960105611521247134314391535Counts−0.500.5−2−1012q1q2q^1 =0.01 , q^2 =1 , l =1 , g =2 , ne =215099148198247296345394443492541590640689738787Counts−2−1012−2−1012q1q2q^1 =0.01 , q^2 =1 , l =1 , g =2 , ne =1011092173264345426507588669751083119112991407151616241732Counts−2−1012−2−1012q1q2q^1 =0.01 , q^2 =0.01 , l =1 , g =2 , ne =11671331992653313974635295956617277938599259911057Counts−2−1012−2−1012q1q2q^1 =0.01 , q^2 =0.01 , l =1 , g =2 , ne =2162122182243304364424485546606666727788848908969Counts−2−1012−2−1012q1q2q^1 =0.01 , q^2 =0.01 , l =1 , g =2 , ne =10157112168224279335390446502557613668724780835891CountsAlgorithm 1 PANDA for GLM

p

1: Input:

1 , . . . , ¯θ(0)

initial parameter estimates ¯θ(0) = (cid:0)¯θ(0)

(cid:1); NGD; maximum iteration T ;
noisy data size ne and moving average (MA) window width m and number of banked
parameter estimates r after convergence; threshold τ0 .
2: Output: regularized parameter estimates ˆθ = (ˆθ1, . . . , ˆθp)
3: Centerize the observed independent variables x.
4: t ← 0; convergence ← 0
5: While t < T and convergence = 0
t ← t + 1
6:
7: Generate e(t)
8:

x from NGD N(cid:0)0, V(cid:0)¯θ(t−1)(cid:1)(cid:1) and set ey ≡ ¯y (∼Bern(ˆp) for Bernoulli Y ).

y , e(t)

x ) to obtain the augmented data (˜y(t), ˜x(t))

Combine (y, x) with (e(t)
Run GLM on (˜y(t), ˜x(t)) and obtain MLE ˆθ
ordinary least-squares estimates are obtained.
If t > m, calculate MA ¯θ(t) = m−1 (cid:80)t
l=t−m+1
loss function l(t) with ¯θ(t) plugged in Eqn (3).
Calculate the averaged loss function ¯l(t) = m−1(cid:80)t
Let convergence ← 1 if ¯l(t) satisﬁes one of the convergence criteria (Remark 1).

; otherwise ¯θ(t) = ˆθ

l=t−m+1 l(l).

ˆθ

(t)

(t)

(l)

12:
13: End While
14: Run Lines 6 and 9 above for additional m + r iterations, and record ¯θ(l) for l = t+ m +
(cid:1) for k = 1, . . . , p.

1, . . . , t+ m +r. Let ¯θ = (¯θ1, . . . , ¯θp), where ¯θj = (cid:0)¯θ(t+m+1)
j
¯θ(l)
j o.w.

15: Set ˆθj = 0 if max{|¯θj|} < τ0; and ˆθj = r−1(cid:80)t+m+r

, . . . , ¯θ(t+m+r)
j

l=t+m+1

. For linear regression with Gaussian Y ,

9:

10:

11:

. Calculate the

PANDA algorithm, there is still mild ﬂuctuation around the parameter estimates due to the
randomness of the augmented noise, especially when ne or m is not large. We suggest bounding
the absolute maximum of the estimates over a sequence of iterations as given in Algorithm 1,
which seems to be a robust criterion in the empirical studies in Sec 4.

Remark 5 (non-convex regularizers). PANDA optimizes a convex objective function in
each iteration of the GLM on the augmented data even when the targeted regularizer itself
is non-convex, such as the SCAD or l0. As such, PANDA does not run into the same type
of computational diﬃculties that gradient-based techniques often experience for non-convex
optimization (e.g., getting stuck in a local optimum). As a matter of fact, due to the stochastic
nature of PANDA from iteration to iteration, it can escape from a local optimum especially
if it is unstable, and lands at a more stable local optimum or even the global optimum. That
being said, the initial values used in PANDA would also aﬀect the ﬁnal solutions when the
targeted regularizer is non-convex.

2.3 ne vs. m
Upon convergence, the expected regularization in Proposition 1 can be realized either by
(cid:1)2 or by letting ne → ∞
(cid:80)ne
letting m → ∞ suggested by limm→∞ m−1 (cid:80)m
i=1
t=1
(cid:1)2 under the constraint neV(ej) = O(1) ∀ θj.
j e(t)
suggested by ne limne→∞ n−1
ik θj
e
The constraint neV(ej) = O(1) guarantees that injected noise e does not over-regularize

i −(cid:80)

i −(cid:80)

(cid:80)ne
i=1

j e(t)

(cid:0)e(t)

(cid:0)e(t)

ik θj

8

(cid:80)

j θ2

(cid:0)C1

or overwhelm the information about θ contained in the observed data x even when ne is
large. For example, V(ej) = λ|θj|−1 in the case of the lasso-type noise, and neλ can be
treated as one tuning parameter. The targeted regularization implied by the lower-order
j V(ej)(cid:1) in Eqn (4) can be approximated arbitrarily well as ne → ∞ with
term ne
neV (ej) = O(1). When m → ∞ and ne is ﬁxed, there exists, more or less, other type of
regularization on θ on top of the targeted regularization given that the higher-order term
j neV2(ej)(cid:1)(cid:1) in Eqn (4) does not disappear. If we also require neV (ej) = O(1) in the
O(cid:0) (cid:80)
j V(ej)(cid:1)(cid:1), then the high-order
j neV2(ej)(cid:1)(cid:1) = O(cid:0) (cid:80)
large m and small ne case, then O(cid:0) (cid:80)
term would also be ignorable if θ4

(cid:0)θ4
j V (ej) is small.

(cid:0)θ4

(cid:0)θ4

j

j

j

Figure 3 illustrates the diﬀerences between the realized regularization eﬀect P (θ), when the
targeted regularization is lasso (P (θ) = |θ|), by letting ne → ∞ (m is small) vs m → ∞
(ne is small) and its relationships with θ for several types of GLM (the regularization eﬀects
when Y follows an exponential distribution are similar to when Y is Poisson and the results
from the former are not provided). For ne → ∞ (λne = 1 ﬁxed at 1 and m = 50), the
realized penalty is identical to the targeted lasso in all four regression types, and is very close
lasso at ne = 100 except for some very mild random ﬂuctuation. The realized regularization
on θ at m → ∞ and small ne varies by regression type. When |θ| is small, the target
regularization is realized as the higher-order term that involves |θ| in Eqn (4) is ignorable. As
|θ| increases, the the higher-order term becomes less ignorable and regularization deviates from
lasso, except for linear regression where the higher-order term is analytically 0. Speciﬁcally,
the realized regularization is sub-linear for logistic and NB regression, and super-linear for
Poisson regression.

In summary, to achieve the expected regularization eﬀect in Proposition 1, one can set either
m or ne at a large number. Computationally, a large ne often requires less iterations even
when m is as small as 1. On the other hand, a very large ne slows down the computation per
iteration. Taken together, the actual time taken to reach convergence might not diﬀer that
much between the two cases. In some sense, the choices on ne and m more or less depends on
each other. If a large ne still results in noticeable ﬂuctuation around ˆθ, then a large m can be
used to speed up the convergence. For a small ne, a relatively large m should be used to yield
stable penalty. Fig 4 shows the parameter estimation trajectories of zero-valued regression
coeﬃcients across with λ in linear regression and Poisson regression on simulated data when
the lasso-type noise is used in PANDA There are 30 predictors (p = 30) and n = 100 in
each case. In the linear regression, the predictors were simulated from N(0, 1); in the Poisson
regression, the predictors were simulated from Unif(−0.3, 0.5). Out of the 30 coeﬃcients, 9
were set at 0, and the other 21 non-zero coeﬃcients ranged from 0.5 to 1. The estimation
trajectories for the 9 zero-valued parameters look very similar between large ne vs.
large m
in both regression settings.

If the targeted penalty is l0 (Proposition 2) and n > p, ne can be tuned within [1, p]. There
are other considerations regarding the choices of m and ne when using PANDA to obtain
inference on θ. More details are provided in Sec 3.3.

9

Figure 3: Realized regularization by PANDA in diﬀerent GLMs for the targeted penalty P (θ) = |θ|.

Figure 4: Estimation trajectories of zero-valued regression coeﬃcients across λ in linear regression
(left) and Poisson regression (right) with lasso-type noise in PANDA

3 Theoretical Properties and Statistical Inferences
In this section, we establish the almost sure (a. s.) convergence of the data augmented loss
function to its expectation and the a. s. convergence of the minimizer of the former to the
minimizer of the expected loss function as ne → ∞ or m → ∞ (Sec 3.1). We also examine
the Fisher information of the parameters in noise-augmented data (Sec 3.2) and statistical
inferences of the parameters via PANDA (Sec 3.3), and claim that PANDA exhibits ensemble
learning behavior (Sec 3.4).

10

1.833.665.49−0.2−0.10.00.10.2log(l)parameter estimatesne=100, m=1,500 ne=10,000, m=11.833.665.49−0.20.00.20.4log(l)ne=100, m=1,500 ne=10,000, m=1parameter estimates3.1 Almost sure convergence of noise augmented loss function and

its minimizer

Let ¯lp(θ|˜x, ˜y) denote the average loss function over m iterations of the PANDA algorithm upon
convergence. Theorem 1 presents the asymptotic properties of ¯lp(θ|˜x, ˜y) under two scenarios:
1) ne → ∞ while neV(ej) = O(1) for a given θj and m (≥ 1) is ﬁxed at a constant; 2) m → ∞
when ne(> p) takes a ﬁnite constant.

Theorem 1. (asymptotic properties of the noise-augmented loss function and its
minimizer for PANDA) Assume θ belongs to a compact set. Let lp(θ|x) = Ee(lp(θ|˜x, ˜y)).
1) If ne → ∞ while neV(ej) = O(1) for any given θj and m ≥ 1 is held at a constant, then

e C −1
n1/2
1

(cid:0)¯lp(θ|˜x, ˜y) − lp(θ|x)(cid:1) d−→ N (0, 1)

¯lp(θ|˜x, ˜y) a.s.−→ lp(θ|x) ne→∞−→ l(θ|x)+ P (θ) + C
¯lp(θ|˜x, ˜y) a.s.−→ arg inf

lp(θ|x),

θ

arg inf
θ

2) If m → ∞ while ne(> p) is ﬁxed, then

m1/2C −1

2

(cid:0)¯lp(θ|˜x, ˜y) − lp(θ|x)(cid:1) d−→ N (0, 1)

¯lp(θ|˜x, ˜y) a.s.−→ lp(θ|x) m→∞−→ l(θ|x) + P (θ) + C
¯lp(θ|˜x, ˜y) a.s.−→ arg inf

lp(θ|x).

θ

arg inf
θ

(18)

(19)

(20)

(21)

(22)

(23)

P (θ) in Eqns (19) and (22) is the same as deﬁned in Proposition 1. C1 and C2 are functions
of θ and take diﬀerent forms for diﬀerent types of Y .

√

√

ne and

The proof of Theorem 1 is provided in Sec S.3 of the supplementary materials. There are
two important takeaways. First, Theorem 1 states that ¯lp(θ|˜x, ˜y) follows a Gaussian distri-
bution at the rate of
m under the two scenarios, respectively, implying that the
augmented loss function in PANDA is trainable for practical implementation. The ﬂuctuation
of ¯lp(θ|˜x, ˜y) around its expected value due to noise augmentation is controlled and the tail
of the distribution of d = ¯lp(θ|˜x, ˜y) − lp(θ|x) decays to 0 exponentially fast in ne and m as
Pr(d > t) ≤ exp(−net2/2C 2) and Pr(d > t) ≤ exp(−mt2/2C 2) for any t > 0. Second, ¯lp(θ|˜x, ˜y)
converges a.s. to its expectation (the penalized loss function given the observed data (x, y)
with the targeted penalty term), guaranteeing that PANDA does what it is designed to do.

When there exists multicollinearity among X, the loss function minimized in PANDA has an
optimum region rather than a single optimum point. To examine the asymptotic properties in
this case, we deﬁne the optimum parameter set (Deﬁnition 1) and show that the parameters
learned by PANDA fall in the optimum parameter set asymptotically (Proposition 3).

Deﬁnition 1. (optimum parameter set) Let the expected loss function lp(θ|x) be a contin-
uous function in θ. The optimum set is deﬁned as Θ0 = (cid:8)θ0 ∈ Θ | lp(θ0|x) ≤ lp(θ|x), ∀ θ ∈ Θ(cid:9),
where Θ is the set containing all possible parameter values. The distance from θ ∈ Θ to Θ0
is deﬁned as d (cid:0)θ, Θ0(cid:1) = min
θ0∈Θ0

||θ − θ0||2.

Proposition 3. (consistency of parameter estimate in presence of multicollinearity)
Let ˆθ

¯lp(θ|˜x, ˜y). Given

(cid:12) → 0 as ne → ∞ (cid:84) neV(ej) = O(1) ∀j = 1, ..., p or m → ∞;

(24)

0
p = arg min
(cid:12)¯lp(θ|˜x, ˜y)−¯lp(θ|x)(cid:12)
(cid:12)

θ

sup
θ

11

and assume θ is compact, then Pr

(cid:18)

lim sup
m→∞ or ne→∞

d(cid:0)ˆθ

0

p, Θ0(cid:1) ≤ δ

(cid:19)

= 1 ∀ δ > 0.

The proof is given in Sec S.4 of the supplementary materials. Multicollinearity does not aﬀect
the convergence of the loss functions in PANDA; therefore, Eqn (24) holds per the proof of
Theorem 1.

3.2 Fisher Information in Noise Augmented Data
The augmented noise in PANDA brings endogenous information to observed data x to reg-
ularize the estimation of θ. The expected regularization can be achieved by letting (ne →
∞) ∩ (neV(ej) = O(1)). At the ﬁrst sight, it seems that a large amount of augmented noisy
data could potentially overshadow the information about the parameters in the observed data,
leading to over-regularization. We claim that this is not the case because of the constraint
neV(ej) = O(1) ∀ j.
In other words, ne combined with the tuning parameters from the
NGD variance term is treated as a single tuning parameter. For example, with the lasso-
type noise, neλ is treated one tuning parameter:
if ne is large, then λ takes a small value
so to keep neλ = O(1). Proposition 4 provides the theoretical justiﬁcation that, as long as
neV(ej) = O(1) for any given θj, the amount of regularization brought by the augmented data
to θj remains as constant even for ne → ∞. Proposition 4 is established in the context of
the bridge-type noise; the same conclusion can be obtained for other noise types in a similar
fashion. The proof is provided in Sec S.5 of the supplementary materials.

Proposition 4 (Fisher information in noise augmented data). The regularization on
the coeﬃcients θ in GLM introduced through the augmented bridge-type noise is proportional
to neλ|θ|−γ. Speciﬁcally, I˜x,˜y(θ), the Fisher information on θ contained in the augmented data
(˜x, ˜y) is the summation of Ix,y(θ), the Fisher information on θ contained in the observed data,
and Ie(θ), the amount of regularization on θ.

I˜x,˜y(θ) = Ix,y(θ) + (λne)B(cid:48)(cid:48)(θ0 + 0)Diag{|θ1|−γ, . . . , |θp|−γ} + O(cid:0)λn1/2

e

(cid:1)J,

(25)

where J is a p × p matrix with all elements at 1. The higher-order term O(cid:0)λn1/2
(cid:1) becomes
O(λ1/2) if λne = O(1) and is ignorable if λ is small. Eqn (25) suggests that the information
about θ does not increase with ne as along as λne|θ|−γ is kept at a constant. In addition, the
closer |θ| is to 0, the stronger the regularization the augmented information brings to θ.

e

3.3 Asymptotic Distribution of Regularized Parameters via PANDA
Proposition 5 presents the asymptotic distribution of the estimated ˆθ via PANDA, based
on which we can obtain statistical inferences for θ. The proof is given in Sec S.6 of the
supplementary materials.

(t)

Proposition 5 (asymptotic distribution of parameter estimates via PANDA). Let
ˆθ
for θ is denoted by ¯θ = r−1 (cid:80)r
n) ∀θ.
neV(e) = o(

denote the estimate of θ in iteration t of the PANDA algorithm. The ﬁnal estimate
from r ≥ 1 iterations after convergence. Assume

√

t=1

ˆθ

(t)

√

n(ˆθ
√

(t)

− θ) d→ N (0, Σ(t)) as n → ∞,

n(¯θ − θ) d→ N (cid:0)0, ¯Σ + Λ(cid:1) as n → ∞; r → ∞,

(26)

(27)

12

where Σ(t) = I˜x(t),˜y(θ)−1Ix,y(θ)I˜x(t),˜x(θ)−1 in iteration t, ¯Σ = r−1 (cid:80)r
is the between iteration variability of ˆθ
√

(t)

.

t=1 Σ(t), and Λ = V(ˆθ

(t)

)

n)). The asymptotic variance of ˆθ

n) takes diﬀerent forms for diﬀerent NGDs (e.g., for

The regularity condition neV(e) = o(
√
the bridge-type noise, it is λne = o(
involves the inverse
of I˜x(t),˜y(θ), which always exists given the augmented data. Eqn (27) suggests the overall
variance on ¯θ is the summation of two variance components, ¯Σ, the per-iteration variance
of ˆθ
. ¯Σ contains the unknown θ and can
be estimated by plugging in ˆθ
is not
accounted for. Λ can be estimated by the sample variance of ˆθ
over r iterations; that is,
− ¯θ(cid:1)T .
(r − 1)−1 (cid:80)r

, with the caveat that the uncertainty around ˆθ

, and Λ, the between-iteration variance of ˆθ

− ¯θ(cid:1)(cid:0)ˆθ

(cid:0)ˆθ

(t)

(t)

(t)

(t)

(t)

(t)

(t)

(t)

t=1

In the case of linear regression, the asymptotic distribution of ˆθ

(t)

in Eqn (26) becomes

√

n(ˆθ

(t)

− θ) d→ N

0, σ2(M (t))−1(xT x)(M (t))−1(cid:17)
(cid:16)

,

(28)

where M(t) = (xT x+nediag(V(e)). The asymptotic variance in Eqn (28) contains the unknown
(t)(cid:1)/(n − ν) with the
σ2, which can be estimated by ˆσ2 = SSE/(n − ν) = (cid:0)y − xˆθ
degree of freedom ν = tr(x(M (t))−1xT ). ˆσ2 converges to σ2χ2

(t)(cid:1)T (cid:0)y − xˆθ
n−ν in distribution.

When applying PANDA to obtain inference in GLMs, we should set ne at a small number and
m at a large number to achieve valid inference and targeted regularization eﬀect simultane-
ously. We recommend ne = o(n) as long as ne + n > p (e.g., one order of magnitude smaller
than n), especially when n is small. This is diﬀerent from when the main goal is variable selec-
tion (except for l0), regularized estimation, or prediction without uncertainty quantiﬁcation,
where a large ne can be used to achieve the targeted regularization eﬀect with fewer itera-
tions per Proposition 1. The reason is that a large ne (relative to n) tends to underestimate
¯Σ + Λ, the asymptotic variance of ¯θ, resulting in a lower-than-nominal coverage rate and an
inﬂated type I error rate. As mentioned above, ¯Σ = r−1 (cid:80)r
t=1 Σ(t) is estimated by plugging
in ˆθ(t) for t = 1, . . . , r upon convergence, pretending that it is the true parameter value and
thus ignoring the uncertainty around it. Though this issue exists regardless of whether a
large or a small ne is used, using a small ne helps to re-capture this lost variability with the
between-iteration variability Λ. Speciﬁcally, ˆθ(t) is a regularized estimate from minimizing
a loss function summed over the data component x and a penalty term, or equivalently, a
summation of the loss functions constructed from the data component x and the augmented
data component e in the context of PANDA. Instead of focusing on how ˆθ(t) changes with
sample data x, which is ﬁxed throughout iterations, we quantify how it changes with e. If a
large ne is used, the ignored sampling variability around ˆθ(t) can hardly be recovered through
Λ as it is close to 0, which is easy to understand as the realized regularization eﬀect with a
large ne is close to its expectation and it is almost like solving the same analytical constrained
optimization at every iteration, leading to very similar ˆθ(t) across iterations upon convergence.

3.4 Ensemble Learning Behavior of PANDA with Fixed ne
Ensemble learning methods combine multiple learners to achieve better predictive performance
than that from an individual learner. Let Y be the observed outcome and ¯Y be its prediction

13

i(E( ˆYi) − Y )(cid:1)2+(cid:80)

i E(cid:0) ˆYi−E( ˆYi)(cid:1)2+(cid:80)

from an ensemble method. Brown et al. (2005) suggest that the generalization error of the
ensemble method made of M learners, E( ¯Y − Y )2, can be decomposed as
M −2(cid:104)(cid:0) (cid:80)
where ˆYi refers to the prediction from the i-th learner in the ensemble for i = 1, . . . , M . The
success of ensemble methods, in part, can be attributed to the diversity term among the M
learners that is captured by the third term (covariance) in Eqn (29): as the diversity increase,
the covariance decrease, and the overall generalization error decreases. The diversity can be
achieved by perturbing the training data such as taking a subset of observation, or a subset
of attributes to train the learners.

j(cid:54)=i E(cid:0)( ˆYi − E( ˆYi))( ˆYj − E( ˆYj))(cid:1)(cid:105)

(29)

(cid:80)

,

i

We show that PANDA, in addition to achieving the targeted regularization eﬀects, also ex-
hibits some ensemble learning behavior with a ﬁxed ne, which may propel it to edge out the
existing constrained regularization approaches with smaller generalization error in prediction.
Intuitively, upon convergence, the ﬁnal estimates of θ are averages over the estimates trained
from diﬀerent sets of noise augmented data from r iterations, generating the diversity among
the learners needed for the ensemble learning.

Claim 1 (Ensemble learning behavior of PANDA with ﬁxed ne). Upon convergence,
the average estimates over the sets of parameter estimates from multiple iterations of PANDA
with a ﬁxed ne can be regarded an ensemble learner.
If the diversity brought by PANDA with a ﬁxed (small) ne and a large m surpasses the
increase in MSE (the sum of the ﬁrst two terms in Eqn (29)), PANDA would lead to a
smaller generalization error compared to the existing constrained optimization approaches for
penalized GLM regression that don’t promote diversity.

4 Numerical Examples

4.1 l0 Regularization via PANDA
We demonstrate the PANDA-l0 regularization in linear regression using the prostate cancer
dataset and in logistic regression using the kyphosis dataset (Tibshirani, 1996). The prostate
cancer dataset consists of 8 X’s and 97 observations. We standardized the X’s and centralized
Y prior to the application of the PANDA algorithm. The kyphosis dataset consists of 81
observations (64 : 17 for Y = 0 : 1). We included both the linear and the quadratic terms of
the three standardized X’s (X4, X5, X6 are the quadratic terms of X1, X2, X3, respectively),
in the logistic regression following Tibshirani (1996). We examine the regression coeﬃcient
estimation trajectories as ne increases from 1 to p and as λ increases while holding ne constant.
For comparison, we also run the lasso regression in each case via the R package glmnet.

The results are presented in Fig 5. Column A shows that the PANDA-l0 regularization shrinks
only ne coeﬃcients towards 0, leaving the other coeﬃcients unregularized, but lasso shrinks
all coeﬃcients simultaneously. The observations are consistent with Proposition 2 and Eqn
(17), which state the number of selected variables through PANDA-l0 is p − ne for ne < p.
In the logistic regression case, due to the high correlations (0.957, 0.969 and 0.974) between
the linear and the quadratic terms, the shrinkage occurs roughly around the same λ for a
ﬁxed ne for each linear+quadratic pair in the trajectory. The plots in column B examine the
eﬀect of λ on the estimation trajectory ﬁxing ne at p − p0 in PANDA-l0, where p0 is number

14

of variables selected by lasso (p0 = 3 in linear regression and p0 = 4 in logistic regression).
As λ increases, ne coeﬃcients shrinks to 0. Further increasing λ has no regularization eﬀect
on the remaining non-zero coeﬃcients, despite some minor ﬂuctuation around the non-zero
parameter estimates. The plots in column C are similar to column B but ne is ﬁxed at p. For
small λ, the estimation trajectories are similar to using ne < p as in column B; as λ continues
to increase, the non-zero coeﬃcients eventually get shrunk to 0, but in a diﬀerent manner than
lasso in the sense that its shrinkage process is not gradual but rather abruptly. For ne > p,
the estimation trajectories would be somewhere between column C and the lasso trajectories
(e.g. Fig 4), and eventually become the lasso trajectories as ne becomes very large.

A

B

C

Figure 5: Estimation trajectory in linear and logistic regression as ne changes (column A), λ changes
at ﬁxed ne < p (column B), and λ changes at ne = p (column C). The solid lines in column A are
from the PANDA-l0 regularization by varying ne from 1 to p and the dash lines represent the lasso
regression via R package glmnet with the smallest λ that yields p − ne non-zero estimates.

Inference for GLM parameters via PANDA

4.2
We investigate the inferential validity for GLM coeﬃcients based on the asymptotic distri-
butions in Proposition 5 via simulation studies. We examine Gaussian (σ2 = 1), Poisson,
Bernoulli, exponential (exp), and Negative Binomial (NB) (number of failure ﬁxed at r = 5)
outcomes with p = 30 in each case. For the Gaussian and NB outcomes, the predictors
were simulated from N(0, 1); for the Bernoulli, exp, and Poisson outcomes, the predictors
were simulated from Unif(−3, 3), Unif(−1, 2), and Unif(−0.3, 0.5), respectively. We examine
three sample size scenarios n = 50, 70, 100, with 500 repetitions in each simulation case. The
bridge-type noise is employed with γ = 1, ne = n, λne ∈ (1.5, 7) in logistic regression and
γ = 2, ne = 9, λ = n/10 in the other GLMs. The achieved regularization eﬀect is lasso in the
logistic regression and l0 in the other GLMs as ne = 9 is set at the number of zero coeﬃcients.
In each repetition, we calculate the 95% CIs for the 30 regression coeﬃcients (21 are non-

15

number of selected variables876543210−0.20.00.20.40.60.812357846tuning parameter  l1300.10.20.30.40.5−0.20.00.20.40.60.812357846tuning parameter  l1300.20.40.60.811.2−0.20.00.20.40.60.812357846number of selected variables6543210−1.5−1.0−0.50.00.51.01.5123456tuning parameter  l1400.10.20.30.40.5−1.5−1.0−0.50.00.51.01.5123456tuning parameter  l1400.20.40.60.811.2−1.5−0.50.00.51.01.5123456zero and 9 are zero) and examine the coverage probabilities (CP) and the CI widths. Tables
3 presents the results, benchmarked against the post-lasso inferential procedure (Lee et al.,
2016; Taylor and Tibshirani, 2017) implemented via the R package selectiveInference.

Table 3: Empirical CP and CI width via PANDA and post-selection procedures with lasso penalty

zero coeﬃcients (9)

non-zero coeﬃcients (21)

sample size

Gaussian
Bernoulli
Exp
Poisson
NB

50

70

100

PANDA
70

NA NA
NA NA

99.9
96.6
96.5
98.5
100

99.5
99.5
95.5
95.8
99.4

post selection
50
100
mean CP (%) among the 9 coeﬃcients
NA
98.2
NA
100
-
95.1
-
92.2
95.8
-
mean CI width among the 9 coeﬃcients
0.28
14.6
0.39
0.76
0.54

NA NA
NA NA

-
-
-

-
-
-

-
-
-

-
-
-

post selection

PANDA
70

50

70

100

93.2
75.2
-
-
-

92.3
65.9
-
-
-

96.5
88.4
99.5
87.5
95.4

97.7
92.6
94.4
94.1
99.9

50
100
mean CP (%) among the 21 coeﬃcients
94.2
91.4
82.9
97.3
-
87.1
-
87.0
83.1
-
mean CI width among the 21 coeﬃcients
0.91
24.8
1.07
1.28
1.19

29.6
22.0
-
-
-

2.01
10.6
-
-
-

0.15
1.32
0.23
0.44
0.28

NA
Gaussian
NA
Bernoulli
-
Exp
-
Poisson
-
NB
NA: Not Available. R Package selectiveInference does not provide inference for coeﬃcients whose estimates are 0.
In addition, it only produces CIs for linear and logistic regression with the lasso regularization. CIs obtained by
selectiveInference that have inﬁnite lower/upper bounds are excluded from the summary (4 ∼ 18%).

0.57
1.46
0.77
0.91
1.05

0.74
2.15
0.95
1.08
1.11

0.08
0.93
0.14
0.25
0.15

1.26
4.64
-
-
-

For true zero-valued coeﬃcients, PANDA maintains the nominal 95% coverage for all the
examined outcome types and sample sizes. The R selectiveInference package does not
provide inference for coeﬃcients whose estimates are 0 (that is, not selected by lasso in the
ﬁrst place). Among these 9 zero-valued coeﬃcients, lasso only selected some of them a few
times out of the 500 repetitions. When the true coeﬃcients are not 0, the CIs from PANDA
have better coverage with much narrower CIs than the post-selection procedure (except for
logistic regression at n = 50). The post-selection procedure experiences severe under-coverage
in the logistic regression for all n. We also examined the case of a larger ne (ne = 2n in
the logistic regression and ne = n for the other GLMs). There was some under-coverage (CP
≥∼ 90% for zero coeﬃcients; ≥∼ 80% for non-zero coeﬃcients), which improved as n increased.

4.3 Comparison with Existing Regularization in Linear and Logistic

Regression

To examine the regularization eﬀects by PANDA in linear and logistic regression, we use
In the linear
the same simulation setting as Examples 4.1 and 4.3 in Fan and Li (2001).
regression, Y = xβ + (cid:15), where βT = (3, 1.5, 0, 0, 2, 0, 0, 0) (p = 8), xj ∼ N (0, 1) for j = 1, . . . , p
with corr(xj, xj(cid:48)) = 0.5|j−j(cid:48)|, and (cid:15) ∼ N (0, σ2). Three sets of (n, σ) were examined: (40, 3),
(40, 1), and (60, 1). For the logistic regression, n was set at 200; Y ∼ Ber(cid:0)eXT β/(1 + eXT β)(cid:1),
where the ﬁrst six components of X were the same as those in linear regression and the last
two components x were drawn from Bernoulli(0.5) independently.

The medians of relative model error (MRME) and the number of correctly and incorrectly
identiﬁed zero coeﬃcients (out of 5) over 100 repetitions were obtained in each regression
case. The estimates from the ridge, lasso, adaptive lasso and EN regressions via the existing
approaches were obtained from R package glmnet and those from SCAD were from R package

16

ncvreg. We examine two scenarios of PANDA: large ne/small m and large ne/small m. The
speciﬁc values of ne and m, along with other PANDA algorithmic parameters are summarized
in Table ?? in the supplementary materials. The results are presented in Table 4 for the linear
regression and in Table 5 for the logistic regression. In summary, PANDA is either consistent
with or performs better (due to its additional ensemble behaviors) than existing approaches for
the same type of regularizer, per the MRME and the true 0/false 0 counts. The superiority
of PANDA is specially obvious in the logistic regression.
In general, PANDA-SCAD and
PANDA-l0 have the best performance.
Table 4: PANDA vs. Existing Approaches in Penalized Linear Regression with Various Regularizers

lasso

MRME

|ridge lasso adaptive EN SCAD l0
|
|
|
Existing |80.09 67.86
PANDA |80.06 67.70
|
Existing |94.60 68.03
PANDA |95.24 67.38
|
Existing |97.40 66.40
PANDA |97.62 66.22

68.24 72.79
68.31 72.50 78.99

69.45 44.42
68.40 44.87 45.11

67.92 44.91
67.02 44.82 44.77

68.47
67.18

68.32
63.58

68.34
61.48

ridge

lasso

adaptive
lasso

EN

SCAD

l0

# of correctly/incorrectly identiﬁed zero coeﬃcients

n = 40, σ = 3
0/0

2.78/0.04

2.80/0

2.66/0.03 3.59/0.09

0.01/0 2.37/0.01 2.69/0.01 2.50/0.01 4.01/0.17 3.83/0.13

n = 40, σ = 1
2.87/0
0/0
2.69/0
0.13/0
n = 60, σ = 1
2.61/0
0/0
2.55/0
0.19/0

2.83/0
3.07/0

2.56/0.03
2.62/0

4.72/0
4.91/0

2.66/0
3.06/0

2.55/0.03
2.43/0

4.96/0
5.00/0

4.86/0

5.00/0

Table 5: PANDA vs. Existing Approaches in Penalized Logistic Regression with Various Regularizers

ridge lasso adaptive EN SCAD l0

ridge

lasso

MRME

Existing
85.16 68.67
PANDA 76.50 61.15

67.96
58.60

69.71 48.14
62.14 34.87 37.66

lasso adaptive EN
lasso

SCAD

l0

# of correctly/incorrectly identiﬁed zero coeﬃcients
0.07/0 2.10/0 2.05/0 2.09/0
0.17/0 2.43/0 2.83/0 2.44/0 4.97/0.02

4.89/0

4.31/0

4.4 Sports Article Objectivity Data
We implemented PANDA in a real-life dataset that contains 1000 sports articles that are
labeled “objective” or “subjective”. The data set is available for download from the UCI
Machine Learning Repository (Rizk and Awad, 2018). There are 59 variables in the original
data. The independent variables X’s are the extracted features from the articles such as the
frequencies of diﬀerent types of words, (e.g., the objective and subjective SENTIWORDNET
scores, foreign words, subordinating preposition or conjunction) and frequencies of diﬀerent
types of punctuation (e.g., questions marks, exclamation marks), and text complexity score,
among others. After removing the redundant features (perfectly linear dependent variables)
and the highly imbalanced features (e.g, >99% in one category), and adjusting for the total
word counts, we kept 48 X’s plus Y (365 “subjective” and 635 “objective”). We split the 1000
cases into 800 training samples and 200 testing samples (100 subjective vs. 100 objective).

We learned the logistic regression parameters based on the 800 training samples and make
predictions for the 200 testing samples via the trained model. We run the logistic regression
with lasso, ridge, EN, and adaptive lasso penalties via the R package glmnet, and with the
SCAD penalty via the ncvreg package, and obtained the regularized regression with the same
types of penalty listed the above using PANDA. For the existing approaches, the 10-fold CV
was used for hyper-parameter tuning. For PANDA, we run 100 iterations with ne = 1000 and

17

m = 10. The algorithm converged after 10 ∼ 15 iterations, and the ﬁnal parameter estimates
were averaged over the last r = 20 iterations with τ0 = 0.01.

Table 6 presents the results on the MSE, classiﬁcation accuracy rate, and computational time.
Compared to the MLE from the non-regularized logistic regression, the prediction MSE and
the accuracy rate on the testing samples via PANDA are similar or slightly better with the
regularizers realized with the R packages glmnet and ncvreg. Speciﬁcally, the prediction MSE
decreases by ≈ 10%; the accuracy increases by 1.5% to 2% for the same regularizer types. The
number of zero coeﬃcients ranges about 10 to 20 (out of a total of 48), depending on which
regularizer is used per glmnet and ncvreg. PANDA took about 1.5 to 2 seconds to run 50
iterations. However, 25 iterations (costing 0.7 to 1 seconds) would also be suﬃcient for this
application. Suppose t values of tuning parameters are used in a K-fold cross-validation. The
total time including the hyper-parameter tuning would be around 0.7tK to tK seconds. Say
t = 10 and K = 10, then it will take about 1 to 1.5 mins for PANDA. This is signiﬁcantly
longer than the existing method, which is expected since PANDA involves random sampling
of data points and running GLM for every iteration.

Table 6: PANDA vs Existing Approaches in the Sports Article Objectivity Data
l0

adaptive lasso

SCAD

ridge

lasso

EN

penalty

Prediction MSE (0.1573 with MLE)

Existing
PANDA

0.1539
0.1312

0.1561
0.1260

0.1544
0.1277

0.1561
0.1280

0.1629
0.1340

0.1448

Accuracy Rate/Sensitivity/Speciﬁcity (%): 78.5/94/63 with MLE

78.5/95/62
Existing
PANDA 83.5/91/76

77.5/95/60
82.5/87/78

77.5/95/60
84.5/90/79

78/95/61
82/86/78

77/94/60
81.5/85/78

79.5/92/67

Existing
PANDA‡

0
1

# of zero-valued coeﬃcients (0 with MLE)
25
8
25
8

10
10

4
4

Computational Time (sec)||

Existing
PANDA
‡ hyperparameters were tuned to match the # of zero-valued coeﬃcients in existing methods.
|| V1.1.463 on PC (Intel Core i7-7660U CPU @ 2.50 GHz)

0.7 ∼ 0.8
0.3 ∼ 0.4 per 10 iterations

19

∼2.5

.

We also run PANDA using the same tuning parameters selected by the R packages for the
existing approaches for the same type of regularizer. PANDA performs better than the existing
approaches with smaller RMSE, slightly better accuracy rates, and doubled zero coeﬃcients
in most cases.

5 Discussion
PANDA is a regularization technique through noise augmentation. PANDA eﬀectively reg-
ularizes parameter estimation and allows valid inferences for GLMs, and displays ensemble
learning behavior in certain cases. We establish the Gaussian tail of the noise-augmented loss
function and the almost sure convergence to its expectation – a penalized loss function with
the targeted regularizer, providing the theoretical justiﬁcation for PANDA as a regularization
technique and that the noise-augmented loss function is trainable. For a pre-ﬁxed ne < p, we
show that PANDA is equivalent to imposing ne linear constraints on parameters and can lead
to the l0 regularization. PANDA is straightforward to implement. There is no need for sophis-

18

ticated optimization techniques as PANDA can leverage existing functions or procedures for
running GLMs in any statistical software. In terms of the computational time, large ne usually
leads to convergence with a small number of iterations, but the per-iteration computational
cost can be high. If PANDA is applied to yield the l0 regularization or to obtain inference in
GLMs on top of variable selection, a small ne with a relatively large m should be used.
The PANDA algorithm calculates ¯θ, the average of m minimizers of l(θ|˜x, ˜y) from the lat-
est m iterations, so to leverage the existing software for running GLM and to maintain its
computational advantage over the existing approaches that employ sophisticated optimization
techniques. Proposition 1 suggests the average of m noise-augmented loss function l(θ|˜x, ˜y)
yields a single minimizer ˆθ, the Monte Carlo version of Ee(lp(θ|˜x, ˜y) as m → ∞. We establish
in Corollary S.1 in the supplementary materials that ¯θ and ˆθ are ﬁrst-order equivalent for
large m and ne for PANDA in linear regression, We also present simulation results in the
linear regression and Poisson regression settings to illustrate the similarity between ¯θ and ˆθ.

For linear regression, the OLS estimator obtained from the noise-augmented data in each iter-
ation of the PANDA algorithm is a weighted ridge estimator on the observed data. Compared
to a regular ridge estimator, where the same constant is added to all the diagonal elements of
xT x, diﬀerent constants are used for diﬀerent diagonal elements in weighted ridge regression.
The formal results and the proof are provided in Sec S.9 of the supplementary materials.

PANDA and the noise augmentation technique, in general, can be extended to regularize
other types of learning problems such as undirected graphical models, where some of the
existing techniques are GLM-based. The realized l0 penalty by noise augmentation can be
used to regularize learning problems where such penalty is desired but hard to realize due
to computational constraints. Regarding the ensemble learning behavior of PANDA, it is
worthwhile to study further the underlying theory and run more empirical studies to quantify
the beneﬁts of the diversity term enabled by PANDA in generalization error reduction.

References

Berk, R., Brown, L., Buja, A., Zhang, K., Zhao, L., et al. (2013). Valid post-selection inference. The

Annals of Statistics, 41(2):802–837.

Brown, G., Wyatt, J. L., and Tino, P. (2005). Managing diversity in regression ensembles. Journal

of Machine Learning Research, 6:1621–2650.

Dicker, L., Huang, B., and Lin, X. (2013). Variable selection and estimation with the seamless-l0

penalty. Statistica Sinica, 23:929–962.

Efron, B. (2014). Estimation and accuracy after model selection. Journal of the American Statistical

Association, 109(507):991–1007.

Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle

properties. Journal of the American Statistical Association, 96(456):1348–1360.

Frank, I. E. and Friedman, J. H. (1993). A statistical view of some chemometrics regression tools.

Technometrics, 35:109–148.

Javanmard, A. and Montanari, A. (2014). Conﬁdence intervals and hypothesis testing for high-

dimensional regression. Journal of Machine Learning Research, 15:2869–2909.

Lee, J., Sun, D., Sun, Y., and Taylor, J. (2016). Exact post-selection inference, with application to

the lasso. The Annals of Statistics, 44(3):907–927.

19

Leeb, H. and Pötscher, B. M. (2005). Model selection and inference: Facts and ﬁction. Econometric

Theory, 21(1):21–59.

Leeb, H., Pötscher, B. M., et al. (2006). Can one estimate the conditional distribution of post-model-

selection estimators? The Annals of Statistics, 34(5):2554–2591.

Liu, Z. and Li, G. (2016). Eﬃcient regularized regression with l0 penalty for variable selection and

network construction. Computational and Mathematical Methods in Medicine, 3456153.

Lockhart, R., Taylor, J., Tibshirani, R. J., and Tibshirani, R. (2014). A signiﬁcance test for the lasso.

Annals of statistics, 42(2):413.

Reid, S., Taylor, J., and Tibshirani, R. (2017). Post-selection point and interval estimation of signal

sizes in gaussian samples. The Canadian Journal of Statistics, 45(2):128–148.

Rizk, Y. and Awad, M. (2018). Sports articles for objectivity analysis data set. https://archive.

ics.uci.edu/ml/datasets/Sports+articles+for+objectivity+analysis.

Simon, N., Friedman, J., Hastie, T., , and Tibshirani, R. (2013). SGL: Fit a GLM (or cox model)

with a combination of lasso and group lasso regularization. R package version 1.1.

Taylor, J. and Tibshirani, R. (2017). Post-selection inference for l1-penalized likelihood models. The

Canadian Journal of Statistics, 46(1):41–61.

Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society. Series B, 58(1):267–288.

Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Knight, K. (2005). Sparsity and smoothness

via the fused lasso. Journal of the Royal Statistical Society, Series B, 67(1):91–108.

Tibshirani, R. J., Taylor, J., Lockhart, R., and Tibshirani, R. (2016). Exact post-selection inference
for sequential regression procedures. Journal of the American Statistical Association, 111(154):600–
620.

Van de Geer, S., Bühlmann, P., Ritov, Y., and Dezeure, R. (2014). On asymptotically optimal
conﬁdence regions and tests for high-dimensional models. The Annals of Statistics, 42(3):1166–
1202.

Yuan, M. and Lin, Y. (2014). Model selection and estimation in regression with grouped variables.

Journal of the Royal Statistical Society, Series B, 1:685–693.

Zhang, C. and Zhang, S. S. (2013). Conﬁdence intervals for low-dimensional parameters in high-
dimensional linear models. Journal of the Royal Statistical Society Statistical Methodology Series
B, 76(1):217–242.

Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical

Association, 101(1):1418–1429.

Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of

the Royal Statistical Society, Series B, 62(2):301–320.

20

Supplementary Materials to
Adaptive Noisy Data Augmentation for Regularized Estimation and

Inference of Generalized Linear Models

Yinan Li, Fang Liu
Department of Applied and Computational Mathematics & Statistics
University of Notre Dame, Notre Dame, IN 46556, U.S.A.

S.1 Proof of Proposition 1
We take the Taylor expansion of lp(θ|˜x, ˜y), which is the negative log-likelihood, around ei = 0
for i = 1, . . . , ne, and evaluate its expectation over the distribution of ei.

lp(θ|˜x, ˜y)) = l(θ|x) + lp(θ|e) = l(θ|x)) +(cid:80)ne

=l(θ|x) −(cid:80)ne

(cid:16)

=l(θ|x)+(cid:80)ne

(cid:16)

(cid:17)

i=1

h(ei) + ei
i=1li(θ|ei)|ei=0−(cid:80)ne
− (cid:80)∞

− B

θ0+(cid:80)
j θjeij
(cid:110)
j (θjeij)−(cid:80)
(cid:80)
ei
j(θjeij)dB(d)(cid:16)
d=2(d!)−1(cid:80)
j(θjeij)+(cid:80)∞

(cid:104)
(B(cid:48)(θ0)−ei) (cid:80)

i=1

(cid:17)(cid:17)

j θjeij
j(θjeij)B(cid:48) (cid:16)
θ0+(cid:80)

j θjeij|eij=0
d=2(d!)−1B(d)(θ0)(cid:80)

j(θjeij)d(cid:105)

,

θ0+(cid:80)
(cid:17)(cid:111)

(cid:17)

j θjeij|eij=0

=l(θ|x)+C +(cid:80)ne

i=1

i=1 li(θ|ei)
θ0+(cid:80)

(cid:16)

where C = B(θ0) − (cid:80)ne
lp(θ|˜x, ˜y) over the distribution of eij ∼ N (0, V(ej)) is

i=1 (h(ei) + eiθ0), a constant independent of θ. The expectation of

Ee(lp(θ|˜x, ˜y)) = l(θ|x)+C +ne

= l(θ|x)+C +ne
(cid:17)
(cid:16)

(cid:80)

j θ2

j V(ej)

= l(θ|x) + ne

C1

jθ2

(cid:16)1
2B(cid:48)(cid:48)(θ0)(cid:80)
(cid:16)1
2B(cid:48)(cid:48)(θ0)(cid:80)
jθ2
(cid:16)
(cid:80)
ne

+C +O

j V(ej)

(cid:17)

+O

(cid:17)

j V(ej)
+O
j V 2(ej)(cid:1)(cid:17)
(cid:0)θ4

j

(cid:16)

(cid:16)
ne

ne

(cid:80)
j

j )(cid:1)(cid:17)
(cid:0)θ4
j E(e4
j V2(ej)(cid:1)(cid:17)
(cid:0)θ4
, where C1 = 2−1B(cid:48)(cid:48)(θ0).

(cid:80)
j

(1)

t=1

(cid:80)

(cid:80)ne

j θ2

j V(eij) = neC1

There are two ways to realize the expectation in Eqn (1) empirically. One way is to ap-
proximate lp(θ|e) by limm→∞ m−1 (cid:80)m
i=1 li(θ|e(t)
i ). The other way, under the constraint
neV(ej) = O(1), is to let ne → ∞, in which case the second term in Eqn (1) becomes
ijneV(ej)(cid:1). Between the two approaches,
(cid:80)ne
neC1
j neV2(ej))(cid:1)(cid:17)
(cid:0)θ4
letting ne → ∞ ∩ [neV(ej) = O(1)] also leads to the big-O term O
→ 0 in
Eqn (1); in other words, the second order Taylor approximation of Ee(lp(θ|˜x, ˜y)) is arbitrarily
close to Ee(lp(θ|˜x, ˜y)).
If θ0 = 0, then C1j = B(cid:48)(cid:48)(0) and Eqn (1) can be simpliﬁed to

j limne→∞ n−1
e

i=1 e2

(cid:16)(cid:80)

(cid:80)
j

(cid:0)θ2

j

l(θ|x) + C2

(cid:80)

j θ2

j (neV(ej))+C +O

j
(cid:16)
For linear regression, the expectation of lp(θ|˜x, ˜y) = (cid:80)n+ne
of noise e is

i=1

(cid:16)(cid:80)

j neV2(ej))(cid:1)(cid:17)
(cid:0)θ4
(cid:17)2

˜yi−(cid:80)

j ˜xijθj

.

(2)

over the distribution

Ee(lp(θ|˜x, ˜y)) = (cid:80)n

i=1

(cid:16)

xij −(cid:80)

(cid:17)2

j xijθj

+Ee

(cid:18)

(cid:80)ne
i=1

(cid:16)
ei−(cid:80)

j eijθj

(cid:17)2(cid:19)

(cid:16)

= (cid:80)n

i=1

xij − (cid:80)

j xijθj

(cid:17)2

= (cid:80)ne

i=1 Ee

(cid:16)(cid:80)

j eijθj

(cid:17)2

= l(θ|x) + ne

1

(cid:80)

j θ2

j V(eij).

(3)

(4)

S.2 Proof of Proposition 2
Deﬁne Optimization Problem 1 ˆθ = arg minθ f (θ) = arg minθ l(θ|x) + C1
to its convexity in θ, ˆθ can be solved directly from (cid:79)θf (ˆθ) = 0.
Deﬁne a constrained optimization Problem 2

(cid:80)ne
i=1

(cid:0)eT

i θ(cid:1)2 . Due

ˆθ = arg minθ l(θ|x),
s.t. (cid:80)ne
i θ)2 ≤ d,
i=1(eT

the corresponding Lagrangian for which is L(θ) = l(θ|x) + λL
KKT conditions are

(cid:16)(cid:80)ne

i=1

(cid:0)eT

i θ(cid:1)2) − d

(cid:17)

, and the

(5)

(cid:79)L(θ∗) = 0
λL ≥ 0

(cid:16)(cid:80)ne

i=1

λL

(cid:0)eT

i θ∗(cid:1)2 − d

(cid:17)

= 0.

(6)

For Problem 2 to have the same solution as Problem 1, that is, θ∗ = ˆθ, we set λL = C1 and
d = (cid:80)ne

. The constraint in Eqn (5) now becomes

(cid:17)2

ˆθ

(cid:16)

eT
i

i=1

i θ)2 ≤ (cid:80)ne
Given that ne noise data points are independent per the PANDA procedure, Eqn (7) can also
be regarded as ne linear constraints on θ

i=1(eT

(cid:80)ne

(7)

eT
i

i=1

ˆθ

(cid:16)

(cid:17)2

∃ 0 < di <

(cid:16)(cid:80)ne

i=1(eT
i

ˆθ)2(cid:17)1/2

: |eT

i θ| ≤ di, i = 1, . . . , ne

(8)

S.3 Proof of Theorem 1
We prove Theorem 1 for linear regression (Yi is Gaussian), Poisson regression, exponential
regression, negative binomial regression, and logistic regression (when Yi is Bernoulli), respec-
tively. WLOG, we use the bridge-type noise eij ∼ N (0, λ|θ|−γ) to demonstrate the proofs,
which can be easily extended to other types of noises. Prior to the proof of Theorem 1, we
state a theoretical result in Claim 2, on which the subsequent proofs rely on.

Claim 2. If lp(θ|˜x, ˜y) and lp(θ|x) are convex functions w.r.t. θ and share the same parameter
space θ, then

(cid:12)
(cid:12)
(cid:12)inf

θ

lp(θ|˜x, ˜y) − inf
θ

(cid:12)
(cid:12)
(cid:12) ≤ sup
lp(θ|x)

θ

|lp(θ|˜x, ˜y) − lp(θ|x)|

Proof : Since both inf
θ

lp(θ|˜x, ˜y) and inf
θ
a global optimum, denoted by ˆθ and ˜θ, respectively. Therefore,

lp(θ|x) are convex optimization problems, each has
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)inf
(cid:12)

lp(θ|˜x, ˜y)−inf
θ

lp(θ|x)

θ

=

(cid:12)
(cid:12)
(cid:12)lp(ˆθ|˜x, ˜y) − lp(˜θ|x)
(cid:12)
(cid:12)
(cid:12). Consider the following two scenarios,

i). if lp(ˆθ|˜x, ˜y) ≥ lp(˜θ|x), then lp(˜θ|˜x, ˜y) ≥ lp(ˆθ|˜x, ˜y) ≥ lp(˜θ|x) and

(cid:12)
(cid:12)
(cid:12) = lp(ˆθ|˜x, ˜y) − lp(˜θ|x) ≤ lp(˜θ|˜x, ˜y) − lp(˜θ|x) =
(cid:12)lp(ˆθ|˜x, ˜y) − lp(˜θ|x)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)lp(˜θ|˜x, ˜y) − lp(˜θ|x)
(cid:12)
(cid:12)
(cid:12)

2

ii). if lp(ˆθ|˜x, ˜y) < lp(˜θ|x), then lp(ˆθ|˜x, ˜y) < lp(˜θ|x) < lp(ˆθ|x) and
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)lp(ˆθ|˜x, ˜y) − lp(ˆθ|x)
(cid:12) = lp(˜θ|x) − lp(ˆθ|˜x, ˜y) ≤ lp(ˆθ|x) − lp(ˆθ|˜x, ˜y) =
(cid:12)lp(ˆθ|˜x, ˜y) − lp(˜θ|x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) .
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:16)(cid:12)
(cid:17)
(cid:12)lp(ˆθ|˜x, ˜y) − lp(ˆθ|x)
(cid:12)lp(˜θ|˜x, ˜y)1 −lp(˜θ|x)
(cid:12)lp(ˆθ|˜x, ˜y)−lp(˜θ|x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ max
All taken together,
(cid:12) ,
(cid:12)
≤ sup

|lp(θ|˜x, ˜y) − lp(θ|x)|.

θ

S.3.1 Linear regression

In this case the regularization eﬀects with ne → ∞ and m → ∞ are the same. The loss
function upon convergence is
¯lp(θ|˜x, ˜y) = (cid:80)n

+ m−1(cid:80)m

(cid:16)
ei−(cid:80)

(cid:16)
yi−(cid:80)
i=1

(cid:80)ne
i=1

j xijθj

j e(t)

ij θj

(cid:17)2

(cid:17)2

t=1

.

Since e(t)

ij = (cid:112)λ|θ|−γz(t)
¯lp(θ|˜x, ˜y) =l(θ|x)+ m−1(cid:80)m

ij , where z(t)

t=1

ij ∼ N (0, 1). Therefore,
λθ2
ij + 2 (cid:80)
j

(cid:80)ne
i=1

(cid:80)
j

(cid:18)

=l(θ|x)+m−1(cid:80)m

t=1

(cid:80)

(cid:16) λθ2
j
|θj |γ
j

(cid:80)ne

+ 2m−1 (cid:80)m

(cid:80)

t=1

j<k

(cid:80)ne

(cid:19)
ij z(t)

ik

i=1z(t)

.

|θj |γ z(t)2
(cid:17)
i=1z(t)2

ij

j<k

λθj θk
γ
|θj θk|
2

(cid:19)

ij

ik z(t)
z(t)
(cid:18)

λθj θk
γ
|θj θk|
2

i=1z(t)2

ij ∼ Γ (cid:0) ne
i=1z(t)

2 , 2(cid:1) and Γ (cid:0) ne
iv ∼ Γ (cid:0) ne
il z(t)

2 , 2(cid:1) ≈ N (ne, 2ne) = ne +(2ne)1/2N (0, 1) = ne +(2ne)1/2z1
Since (cid:80)ne
as ne → ∞; (cid:80)ne
e z2 as
ne → ∞, where z1 ∼ N (0, 1) and z2 ∼ N (0, 1). Therefore, the distribution of ¯lp(θ|˜x, ˜y) can
be approximated by
l(θ|x)+ (cid:80)

2 , 2(cid:1) ≈ N (0, 4ne) = 2n1/2

e N (0, 1) = 2n1/2

2 , 2(cid:1) − Γ (cid:0) ne

(9)

j neλ|θj|2−γ

(cid:32)

(cid:88)

+

j

neλ|θj|2−γ21/2n−1/2

e

(cid:32)

1
m

m
(cid:88)

t=1

(cid:33)(cid:33)

z(t)
1

(cid:88)

+

j<k

(cid:32)

(cid:32)
(cid:0)2n−1/2
(cid:1)

e

1
m

m
(cid:88)

(cid:33)(cid:33)

z(t)
2

= lp(θ|x)+ (mne)−1/2C1N (0, 1) where C1 = neλ

(cid:18) θ
|θ|
j neλ|θj|2−γ = Ee(lp(θ|˜x, ˜y)). Exactly the same Eqn (10) can be

where lp(θ|x) = l(θ|x)+ (cid:80)
obtained by letting m → ∞ rather than ne → ∞.
Per the strong law of large numbers (LLN), Eqn (10) suggests ¯lp(θ|˜x, ˜y) converges almost
surely to its mean for all θ ∈ θ as m → ∞ or ne → ∞ (with neλ = O(1)), assuming |θj| belongs
to a compact parameter space and is bounded by B. Consequently, sup

(cid:19)(cid:18) θ
|θ|

(cid:19)1/2

(10)

γ
2

γ
2

,

2

2

γ
2

λneθjθk
|θjθk|
(cid:12)
(cid:12)
(cid:18)
(cid:12)
(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)

t=1
(cid:19)T (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

θ

(cid:12)¯lp(θ|˜x, ˜y) − lp(θ|x)(cid:12)
(cid:12)
(cid:12)
a.s.
¯lp(θ|˜x, ˜y)
−→

a.s.
−→

lp(θ|x), and arg inf
θ

0 as m → ∞ or ne → ∞. Per Claim 2, inf
θ
arg inf
θ

lp(θ|x) due to the convexity of the loss function.

¯lp(θ|˜x, ˜y)

a.s.
−→ inf
θ

S.3.2 Poisson Regression

The averaged noise-augmented loss function over m iterations upon convergence is
(cid:32)

(cid:32)

(cid:33)(cid:33)

¯lp(θ|˜x, ˜y) = l(θ|x)−

m
(cid:88)

(cid:32)
ne(cid:88)
ei

(cid:88)

θ0+

(cid:33)
e(t)
ij θj

1
m

−log(ei!)−exp

θ0+

(cid:88)

e(t)
ij θj

(11)

t=1

i=1

j

j

3

=l(θ|x)−

1
m

m
(cid:88)

ei

(cid:88)

ne(cid:88)

θje(t)

ij +

t=1

j

1
m

m
(cid:88)

ne(cid:88)

t=1

i=1

(cid:32)

exp

θ0+

(cid:88)

θje(t)
ij

(cid:33)

+ C

i=1
(cid:32)√

λθj

γ
2

|θj|

(cid:33)

z(t)
ij

+

1
m

ne(cid:88)

i=1

m
(cid:88)

ne(cid:88)

t=1

i=1

j

(cid:32)

exp

θ0+

√

λθj

γ
2

|θj|

(cid:33)
z(t)
ij

(cid:88)

j

+C,

(12)

=l(θ|x) −

1
m

m
(cid:88)

(cid:88)

ei

t=1

j

=l(θ|x) + P (θ) + C,

where P (θ) refers to the boxed expression in Eqn (12), z(t)
i=1 yi that is
a constant, and C is a constant not related to θ. The regularizer P (θ) is diﬀerent for ne → ∞
vs m → ∞. We thus consider each case separately.

ij ∼ N (0, 1), ei ≡ n−1 (cid:80)n

Case 1: ne → ∞, neλ = O(1) and ﬁxed m
Assume m = 1 WLOG, then z(t)
that λ → 0, therefore, (cid:80)
around (cid:80)

|θj |
j θjzij = 0 to Eqn (12), as ne → ∞,
(cid:19)

λθj
γ
2

(cid:18) √

√

j

ij can be abbreviated as zij. ne → ∞ and λne = O(1) implies
zij → 0 in Eqn (12). Apply the second order Taylor expansion

λθj
γ
2

|θj |
(cid:18)

(cid:80)

¯lp(θ|˜x, ˜y) → l(θ|x)−ei

(cid:80)
j

(cid:80)ne

i=1 zij

+exp(θ0) (cid:80)ne

i=1

(cid:80)
j

√

λθj
γ
2

|θj |

zij

≈l(θ|x)+ 1

+ 1

i=1

2 exp(θ0) (cid:80)ne
(cid:16) λθ2
j
|θj |γ

j

2exp(θ0)(cid:80)
+ O (cid:0)n−1

e

(cid:1) C1(θ)N (1, 1) + C

√

λθj
γ
2

|θj |

j

(cid:19)2

zij

+ O (n−1

(cid:80)ne

(cid:17)
i=1 z2
ij

+exp(θ0)(cid:80)

j<k

e ) C1(θ)N (1, 1) + C
(cid:19)

(cid:18)

λθj θk
γ
|θj θk|
2

(cid:80)ne

i=1zijzik

(13)

(14)

2 exp(θ0)(cid:80)

e

→l(θ|x) + λne

k |θj|2−γ +O (n−0.5

)C2(θ)N (0, 1)+O (n−1

(15)
For Poisson regression, ei ≡ n−1 (cid:80)n
i=1 yi, the average of the observations in the outcome node
(the log of which estimates θ0) with the canonical log link function. In other words, when
ne → ∞ ei = exp(θ0); therefore, the second and third terms in Eqn (13) cancel out. C1(θ)
and C2(θ) are functions of θ and the standard deviations associated with the two asymptotic
normality terms in Eqn (15) which result from the summation over ne noise terms per the
CLT, and the C2(θ) term is the rate-limiting term and

e )C1(θ)N (1, 1)+C.

(cid:18)

(cid:19)(cid:18)

(cid:18)

(cid:12)
(cid:12)
2exp(2θ0)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|θ|1− γ

C2(θ) =

λne
2
2 exp(θ0)(cid:80)
Note that l(θ|x) + λne
k |θj|2−γ in Eqn (15) is lp(θ|x) = Ee(lp(θ|˜x, ˜y) per Proposition
1 and Appendix S.1. As ne → ∞ and λne = O(1), per the strong LLN and Eqn (15), ¯lp(θ|˜x, ˜y)
converges almost surely to lp(θ|x). Given the convexity of the loss function and per Claim 2,
arg inf
θ

¯lp(θ|˜x, ˜y) a.s.−→ arg inf

where λne = O(1).

lp(θ|x).

|θ|1− γ

(16)

θ

2

2

2

(cid:19)T (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

(cid:19)1/2

Case 2: m → ∞ and ﬁxed ne
The 2nd term in Eqn (12) is the summation of Gaussian variables, and the 3rd term follows
a log-normal distribution. Therefore, we can rewrite Eqn (12) as

¯lp(θ|˜x, ˜y) = l(θ|x)−ei

(cid:80)
j

√

√

λneθj
γ
m|θj |
2

N (0, 1) + m−1(cid:80)m

t=1

(cid:80)ne

i=1 LogN

(cid:16)

θ0, (cid:80)

j

λθ2
j
|θj |γ

(cid:17)

+ C.

(17)

4

Applying the CLT to Eqn (17) as m → ∞,

¯lp(θ|˜x, ˜y) → l(θ|x)− ei

(cid:80)
j






ne
m

+



exp

√

(cid:32)

(cid:88)

λθj

(cid:33)2

γ
2

|θj|

j

√

√

λneθj
γ
m|θj |
2


N (0, 1)



− 1

 exp

2θ0 +

(18)

(cid:32)

(cid:88)

j

1/2

√

λθj

γ
2

|θj|

(cid:33)2







N (0, 1) + C,

suggesting that ¯lp(θ|˜x, ˜y) follows a Gaussian distribution asymptotically. Per the strong LLN
as m → ∞, Eqn (18) converges almost surely to

Ee(lp(θ|˜x, ˜y)) = lp(θ|x) = l(θ|x)+P (θ)+C

= l(θ|x)+ne exp (θ0) exp


1

2

(cid:32)

λ

(cid:88)

|θj|1− γ

2

(cid:33)2

+C

k

(19)

for all θ ∈ Θ assuming Θ to be compact. Per claim 2, sup

(cid:12)¯lp(θ|˜x, ˜y) − lp(θ|x)(cid:12)
(cid:12)
(cid:12)

a.s.
−→ 0 as

θ

m → ∞, which leads to inf
θ

given the convexity of the loss function.

¯lp(θ|˜x, ˜y)

a.s.
−→ inf
θ

lp(θ|x) ⇒ arg inf
θ

¯lp(θ|˜x, ˜y)

a.s.
−→ arg inf
θ

lp(θ|x)

S.3.3 Exponential Regression

¯lp(θ|˜x, ˜y) = l(θ|x) − m−1(cid:80)m

The averaged noise-augmented loss function over m iterations upon convergence is
(cid:16)
(cid:80)ne
i=1

j e(t)
where ei = n−1 (cid:80)n
i=1 yi. The above loss function is equivalent to the loss function in Eqn (11)
in the PGM case except for the constant term that does not involve θ. Therefore, the proof
for PGM also applies in the case of EGM.

ij θj − ei exp

θ0 + (cid:80)

θ0 + (cid:80)

j e(t)

ij θj

(cid:17)(cid:17)

t=1

(cid:16)

,

S.3.4 Negative Binomial Regression

The averaged noise-augmented loss function over m iterations upon convergence is

¯lp(θ|˜x, ˜y) = l(θ|x) − 1

m

(cid:80)m

t=1

(cid:80)ne
i=1

(cid:18)

log

(cid:16) Γ(ei+r)rr
Γ(ei+1)Γ(r)

(cid:17)

(cid:80)

+ei

je(t)

ij θj

−(r+ei) log

(cid:16)
r+exp

(cid:16)
θ0+(cid:80)

je(t)

ij θj

(cid:17)(cid:17)(cid:19)

= l(θ|x) +C −

1
m

m
(cid:88)

ne(cid:88)
ei

(cid:88)

e(t)
ij θj +

1
m

t=1

i=1

= l(θ|x) +C −

1
m

m
(cid:88)
ei

(cid:88)

t=1

j

λθj

ne(cid:88)

γ
2

|θj|

i=1

j
(cid:32)√

m
(cid:88)

ne(cid:88)

(r+ei) log

(cid:32)
r+exp

(cid:32)

(cid:88)

θ0+

i=1

t=1
(cid:33)
z(t)
ij

+

(r + 1)log

(cid:32)
rexp

1
m

m
(cid:88)

ne(cid:88)

t=1

i=1

(cid:33)(cid:33)

e(t)
ij θj

(20)

(21)

j
(cid:32)

θ0+

√

λθj

γ
2

|θj|

(cid:88)

j

(cid:33)(cid:33)

z(t)
ij

(22)

= l(θ|x) + P (θ) + C = lp(θ|x) + C,

where P (θ) refers to the boxed expression in Eqn (22), z(t)
i=1 yi is a
constant, and C is a constant not related to θ. The regularizer P (θ) is diﬀerent for ne → ∞
vs m → ∞. We thus consider each case separately.

ij ∼ N (0, 1), ei ≡ n−1 (cid:80)n

Case 1: ne → ∞ and neλ = O(1) and ﬁxed m

5

Let m = 1 WLOG, thus z(t)
(cid:18)

implying λ → 0 and thus exp
around (cid:80)

j θjzij = 0 to Eqn (12), we have
ne(cid:88)

(cid:32)√

(cid:88)

λθj

¯lp(θ|˜x, ˜y) = l(θ|x)−ei

(cid:19)

(cid:80)
j

√

λθj
γ
2

|θj |

i=1
√

λθj

γ
2

|θj|
(cid:33)

γ
2

|θj|
(cid:32)

(cid:88)

j

ne(cid:88)

i=1

j

+

1
2

ne(cid:88)

i=1

(r + ei)r exp(θ0)
(r + exp(θ0))2
(cid:32)

λθ2
j
|θj|γ

→l(θ|x)+

1
(cid:88)
2
+ O (cid:0)n−1

r exp(θ0)
r+exp(θ0)
(cid:1) N (1, C1(θ)) +C
(cid:80)

e

j

r exp(θ0)
r+exp(θ0)

j

(cid:16) λθ2
j
|θj |γ

→l(θ|x)+ 1
2

ij can be abbreviated as zij. Since ne → ∞ and λne = O(1),

zij

→ 1. Applying the second order Taylor expansion

(cid:33)

zij

+

(r + ei) exp(θ0)
r + exp(θ0)

ne(cid:88)

(cid:88)

i=1

j

√

λθj

γ
2

|θj|

zij

(cid:33)2

zij

+ O (cid:0)n−1

e

(cid:1) N (1, C1(θ)) + C

(cid:32)

z2
ij

(cid:88)
+

k<l

r exp(θ0)
r+exp(θ0)

λθjθk
|θjθk|

γ
2

ne(cid:88)

(cid:33)
zije0il

i=1

(23)

(24)

(cid:80)ne

i=1 z2
ij

(cid:17)

+O(n−1

e ) N (1, C1(θ))+O(n−0.5

e

) C2(θ)N (0, 1)+C (25)

In NB regression, the logarithm of the average of the observations in the outcome node esti-
mates θ0 with the canonical log link function. In other words, when ne → ∞ ei = exp(θ0), and
r + exp(θ0) = r + ei; therefore, the second and third terms in Eqn (23) cancel out and the forth
term can be simplied as shown above. C1(θ) and C2(θ) are functions of θ and the standard
deviations associated with the two asymptotic normality terms in Eqn (25) that result from
the summation over ne noise terms per the CLT, and the C2(θ) term is the rate-limiting term
and

C2(θ) =

λne
2

(cid:18)

(cid:19)2(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:18) r exp(θ0)
2
r+exp(θ0)
(cid:17)

(cid:80)ne

(cid:16) λθ2
j
|θj |γ

(cid:19)(cid:18)

|θ|1− γ

2

|θ|1− γ

2

2

(cid:19)1/2

(cid:19)T (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

2

(26)

(cid:80)

r exp(θ0)
r+exp(θ0)

Note that l(θ|x) + 1
in Eqn (25) is lp(θ|x) = Ee(lp(θ|˜x, ˜y) per Propo-
2
sition 1 and Appendix S.1. As ne → ∞ and λne = O(1), per the strong LLN and Eqn (25),
¯lp(θ|˜x, ˜y) converges almost surely to lp(θ|x). Given the convexity of the loss function and
Claim 2,

i=1 z2
ij

j

arg inf
θ

lp(θ|˜x, ˜y) a.s.−→ arg inf

lp(θ|x).

θ

Case 2: m → ∞ and ﬁxed ne
The second term in Eqn (21) is the summation over Gaussian variables, therefore, the equation
can be written as

¯lp(θ|˜x, ˜y) = l(θ|x) −ei

(cid:80)
j

√

(cid:80)
j

√

= l(θ|x) −ei
(cid:16)(cid:80)

(cid:16)
r+exp

√

λneθj
m|θ(t−1)
j
√

λneθj
m|θ(t−1)
j
(cid:17)(cid:17)

N (0, 1)+ 1
m

(cid:80)m

t=1

(cid:80)ne

i=1 U (t)

i + C,

N (0, 1)+ ne
m

(cid:80)m

t=1U (t) + C,

γ
2

|

γ
2

|

i =(r + ei) log

where U (t)
i
same for all i = 1, . . . , ne. Applying the CLT to the U -term in Eqn (27) as m → ∞,
λneθj
γ
m|θj |
2
√

. The second equation holds because U (t)

N (0, 1) + neE (cid:0)U (t)(cid:1) + ne√

¯lp(θ|˜x, ˜y) → l(θ|x)− ei

m N (0, σU )

je(t)

(cid:80)
j

ij θj

√

√

= l(θ|x) + neE(U (t))− ei

(cid:80)
j

√

λneθj
γ
m|θj |
2

N (0, 1) + ne
√

σU

m N (0, 1),

(27)

is the

(28)

6

where σU is the standard deviation of U (t). Since log(r+exp(∗)) → max{log(r), ∗}, as ∗ → ±∞,
σU is a ﬁnite. Eqn (28) suggests that ¯lp(θ|˜x, ˜y) follows a Gaussian distribution as m → ∞.
Additionally, applying the strong LLN to Eqn (21), ¯lp(θ|˜x, ˜y) converges almost surely to its
mean lp(θ|x) = E(lp(θ|˜x, ˜y)) for all θ ∈ θ as m → ∞, assuming θ to be compact; that is,
¯lp(θ|˜x, ˜y) → lp(θ|x) + C = l(θ|x)+ neE(U (t)
a.s.
−→ 0 as m → ∞ ⇒ inf
θ

(cid:12)¯lp(θ|˜x, ˜y) − lp(θ|x)(cid:12)
(cid:12)
(cid:12)

It follows that sup

a.s.
−→ inf
θ

lp(θ|x) ⇒

lp(θ|˜x, ˜y)

i ) + C.

(29)

arg inf
θ

lp(θ|˜x, ˜y)

θ
a.s.
−→ arg inf
θ

lp(θ|x) given the convexity of the loss function.

S.3.5 Binomial Regression

The averaged noise-augmented loss function over m iterations upon convergence is

¯lp(θ|˜x, ˜y) = l(θ|x) −

1
m

(cid:32)

m
(cid:88)

ne(cid:88)

(cid:88)

e(t)
ij θj − log

ei

(cid:32)

(cid:32)

1 + exp

θ0 +

t=1

i=1

j

(cid:33)(cid:33)(cid:33)

,

(cid:88)

e(t)
ij θj

j

which is a special case of Eqn (20) when r = 1, and the proof for NBGM also applies to BGM.

S.4 Proof of Proposition 3

(m)
p

(ne)
p

(or ˆθ

In the case of multicollinearity, PANDA with sparsity regularization might experience diﬃculty
in learning minimizer ˆθ
) when ne( or m) → ∞. In such a case, we prove that
there exists (cid:15) > 0 and a sub-sequence [ne]i (or [m]i), such that letting θi
p
then d(cid:0)θi
Eqn (24), there exists a sub-sequence [i], such that,
(cid:18)
(cid:12)¯lp(θ|˜x, ˜y) − ¯lp(θ|x)(cid:12)
(cid:12)

p, Θ0(cid:1) > (cid:15), where Θ0 is the optimum parameter set. Denote li

(or ˆθ
),
p|˜x, ˜y), then by

∆= ˆθ
p = lp(θi

(cid:12) > δ

[ne]i
p

[m]i
p

(30)

Pr

(cid:19)

sup
θ

< k−1, k ∈ N.
(cid:16)ˆθ

∈ Θ, d

∗

∗

, Θ0(cid:17)

∗

≥ (cid:15), ˆθ

/∈ Θ0.

Since θ is compact, the sub-sequence [i] converges to a point ˆθ
On the other hand, for any θ ∈ Θ, we have

∗

¯lp(ˆθ

|˜x, ˜y) − ¯lp(θ|x) =(¯lp(ˆθ
+ (l[i]

|˜x, ˜y) − ¯lp(θ[i]
p (θ[i]
p ) − l[i]

∗

p |˜x, ˜y)) + (¯lp(θ[i]

p |˜x, ˜y) − l[i]

p (θ[i]

p ))

p (θ)) + (l[i]
p = ˆθ
θ[i]

∗

p (θ) − ¯lp(θ|x)).
, the ﬁrst term in the above equation

By the continuity of the loss function and lim
i→∞

is arbitrarily small with i → ∞; by equation (30), the second and forth terms are arbitrarily
small with i → ∞, and the third term is non-positive. Since θ ∈ Θ is arbitrary , we must
have ˆθ

∈ Θ0, which is a contradiction. The Proposition is proved.

∗

S.5 Proof of Proposition 4
WLOG, we derive the Fisher information with the bridge-type noise. The proofs for other
types of noise are similar. The Fisher information matrix I˜x,˜y(θ) on the augmented data is
obtained by taking the expectation of the negative second derivative of the noise-augmented
loss function in Eqn (3) over the distribution of data x and augmented noise e.

I˜x,˜y(θ) = Ex

(cid:0)xT B(cid:48)(cid:48)(x)x(cid:1) + Ee

(cid:0)eT

x B(cid:48)(cid:48)(ex)ex

(cid:1) = Ix,y(θ) + Ee

7

(cid:0)(cid:80)ne

i=1 eT

x,iB(cid:48)(cid:48)(eiθ)ex,i

(cid:1) ,

where B(cid:48)(cid:48)(x) = diag{B(cid:48)(cid:48)(x1θ), . . . , B(cid:48)(cid:48)(xnθ)} and B(ex) = diag{B(cid:48)(cid:48)(ex,1θ), . . . , B(cid:48)(cid:48)(ex,neθ)}.
Let λne = O(1) and V(ex,i) denote the covariance matrix of ex,i; take the second-order Taylor
expansion around ex,iθ = 0, we have

I˜x,˜y(θ) =Ix,y(θ) + neB(cid:48)(cid:48)(0)V(ex,i) + O(λn1/2

e

)Jp

=Ix,y(θ) + (λne)B(cid:48)(cid:48)(0)diag{|θj1|−γ, . . . , |θjp|−γ} + O(λn1/2

e

)Jp,

where Jp is a p × p matrix with all elements equal to 1.

S.6 Proof of Proposition 5
Given n−1/2l(cid:48)(θ|x) d→ N (0, I −1
1 (θ)), where l(cid:48)(θ|x) is the ﬁrst derivative of the negative log-
likelihood function given the observed data x and I1(θ) is the information matrix over one
observation. It follows that

n−1/2(l(cid:48)(θ|x) + l(cid:48)(θ|e)) = n−1/2l(cid:48)(θ|˜x, ˜y) d→ N (n−1/2l(cid:48)(θ|e), I1(θ))

(31)
where e is the augmented noise and l(cid:48)(θ|e) = (cid:80)ne
i=1 l(cid:48)(θ|ei). Let φ(e) = n−1/2l(cid:48)(θ|e) and it
expectation over the distribution of e can be worked out for diﬀerent types of noise. For ex-
ample, with the bridge-type noise, φ(e) = n−1/2l(cid:48)(θ|e) and Ee(φ) = λne√
n σ2sgn(θ0) for Gaussian
n sgn(θ0)+λ2ne
n sgn(θ0) + λ2ne√
outcome nodes, λne
n O(|θ0|)
√
for exponential and Poisson outcome nodes and λner
n O(|θ0|) for NB outcome
nodes. If λne = o(

n O(|θ0|) for Bernoulli outcome nodes, λne

n), then Ee(φ) → 0 as n → ∞.

2(r+1)n sgn(θ0)+ λ2ne√

√

√

8

2

Upon the convergence of the PANDA algorithm, the MLE of θ based on (˜x, ˜y) is the minimizer
ˆθj,e from solving l(cid:48)(ˆθj,e) = 0, its ﬁrst-order Taylor expansion around θ is l(cid:48)(ˆθe) ≈ l(cid:48)(θ|˜x, ˜y) +
l(cid:48)(cid:48)(θ|˜x, ˜y)(ˆθe − θ) = 0. Therefore, ˆθe − θ = −(l(cid:48)(cid:48)(θ|˜x, ˜y))−1l(cid:48)(θ|˜x, ˜y) and
=
−(n−1l(cid:48)(cid:48)(θ|˜x, ˜y))−1 (cid:0)n−1/2l(cid:48)(θ|˜x, ˜y)(cid:1), where l(cid:48)(cid:48)(θ|˜x, ˜y) is the Hessian matrix and l(cid:48)(cid:48)(θ|˜x, ˜y) →
Ip(θ) as n → ∞. Taken together with Eqn (31), assume λne = o(
n), by the Slutsky’s
theorem, as n → ∞
√
(cid:17)
(cid:16)ˆθe − θ)

= (n−1l(cid:48)(cid:48)(θ|˜x, ˜y))−1 (cid:0)n−1/2l(cid:48)(θ|˜x, ˜y)(cid:1) d→ N (cid:0)0, Ip(θ)−1I(θ)Ip(θ)−1(cid:1) ∆= N (0, Σe).
When the mean of m > 1 estimates over consecutive iteration are taken as the ﬁnal estimate
(t)
for θ, that is ¯θ = m−1 (cid:80)m
e , the variability among the m consecutive estimates will need
to be accounted for and be reﬂected in the variance of the ﬁnal estimate. It is easy to establish
this in the Bayesian framework. Speciﬁcally,

(cid:16)ˆθe − θ

√

√

t=1

ˆθ

(cid:17)

n

n

E(θ|x) = Ee(E(θ|˜x, ˜y)) = Ee(ˆθe) = m−1 (cid:80)m
V (θ|x) = Ee(V (θ|˜x, ˜y)) + Ve(E(θ|˜x, ˜y)) = Ee(Σe) + Ve(ˆθe) (cid:44) ¯Σ + Λ

(cid:44) ¯θ as m → ∞

t=1

ˆθ

(t)
e

= m−1 (cid:80)m

t=1 Σ(t)

e + (m − 1)−1 (cid:80)m

t=1

(t)

(cid:16)ˆθ

− ¯θ

(cid:17) (cid:16)ˆθ

(cid:17)(cid:48)

(t)

e − ¯θ

as m → ∞.

Per the large-sample Bayesian theory, the posterior mean and variance of θ given x are
asymptotically equivalent (n → ∞) to the MLE for θ and the inverse information matrix of
θ contained in x. In other words,
√

n(¯θ − θ) → N (cid:0)0, ¯Σ + Λ(cid:1) .
In the case of a ﬁnite m (as in practical application), V(θ|x) is estimated by ¯Σ + (1 + m−1)Λ

8

√

with the correction for the ﬁnite m. Applying Proposition 5 with lasso-type noise, we have

n(ˆθ − θ) → N (cid:0)n−1/2λnesgn(θ)M −1, σ2M −1(y(cid:48)x)M −1(cid:1) ,
where M = (y(cid:48)x + diag(λne|θ|−1)) and σ2 is the variance of the error term in the linear
regression, and is estimated by

ˆσ2 =SSE(n − ν)−1 = (n − ν)−1(xθ + (cid:15))(cid:48)(I − H)(xθ + (cid:15))

=(n − ν)−1(cid:15)(cid:48)(I − H)(cid:15) + (n − ν)−1 (θ(cid:48)x(cid:48)(I − Hxθ + 2θ(cid:48)x(cid:48)(I − H)(cid:15))

where H = x(y(cid:48)x + diag(λne|θ|−1))−1y(cid:48) and ν = trace(H).

S.7 A Formal Test on the Convergence of the PANDA Algorithm

When presenting the PANDA algorithm in Sec 2.2, we state that a formal statistical test can
be used to test convergence. This test is based on the assumption of ne → ∞ or m → ∞ and
should work well when either ne or m is large in practice. WLOG, we establish the test below
for ne → ∞; the procedure is similar for m → ∞ by replacing ne with m.

Theorem 1 shows that as ne → ∞, the distribution of the loss function in iteration t converges
to a Gaussian distribution (Eqn (18)). The asymptotic Gaussian distribution involves C1(θ),
which is unknown and can be estimated by plugging the ˆθ
from the current iteration t.
Speciﬁcally,

(t)

C (t)

1 = λne
2

(cid:32)

(cid:12)
(cid:12)
κ
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)ˆθ

(t)(cid:12)
(cid:12)ˆθ

(t)(cid:12)
(cid:12)

−γ/2(cid:17) (cid:16)ˆθ

(t)(cid:12)
(cid:12)ˆθ

(t)(cid:12)
(cid:12)

−γ/2(cid:17)T (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

2

(cid:33)1/2

,

where κ is a constant that depends on the type of Y (κ = 8 for Gaussian, 2 exp(2θ0)/(1 +
exp(2θ0))4 for Bernoulli, 2 exp(2θ0) for Poisson, 2 for Exponential, and 2r2 exp(2θ0)/(r +
exp(θ0))2 for NB; see Eqns (10), (16) and (26)). Let d(t) = ¯lp(˜x(t+1), ˜y) − ¯lp(˜x(t), ˜y) denote
the diﬀerence in the loss function from two consecutive iterations of the PANDA algorithm,
which is n−1/2
1 z(t)) per Eqn (18). If the PANDA algorithm converges, the
e
(t)
estimates ˆθ
≈ C (t)
1 and a nonzero d(t) is mostly
due to the randomness of the injected noise with an expected mean of 0; that is,
(cid:114)

(C (t+1)
1
stabilizes, so does C (t)

1 ; in other words, C (t+1)

z(t+1) − C (t)

1

z(t) = d(t)/

(cid:104)
1 + C (t+1)2
C (t)2

1

(cid:105)
.

n−1
e

(32)

Since z(t) is independent from z(t+1) (augmented noises are drawn independently across it-
eration). If |z(t)| > z1−α/2, then we may claim the PANDA algorithm has not converged at
iteration t at the signiﬁcance level of α.
The denominator in Eqn (32) assumes C (t)
are independent whey are likely to
1
positively correlated as both use the original data (x, y). With the under-estimated variance,
z(t) would be over-estimated, and convergence is likely to rejected more often than necessary.

and C (t+1)

1

S.8 Minimizer of Averaged noise-augmented Loss Function vs Av-

eraged minimizer of Noise-augmented Loss Functions

Per Proposition 1, one would take the average over m noise-augmented loss function l(Θ|x, e)
to yield a single minimizer ˆθ, which is the Monte Carlo version of Ee(lp(θ|x, e) as m →
∞. However, PANDA would lose its computational edge. To maintain the computational

9

advantage for PANDA, we instead calculate ¯θ, the average of m minimizers of l(Θ|x, e) from
the latest m iterations, which is the approach that the PANDA algorithm uses. We establish
in Corollary S.1 that ¯θ and ˆθ are equivalent under some regularity conditions. We also present
some numerical examples below to illustrate the similarity between ¯θ and ˆθ.

Corollary S.1 (First-order equivalence between minimizer of averaged noise-aug-
mented loss functions vs averaged minimizers of single noise-augmented loss func-
tions). The average ¯θ of m minimizers of the m perturbed loss functions upon convergence
is ﬁrst-order equivalent to the minimizer ˆθ of the averaged m noise-augmented loss functions
as m → ∞ or as ne → ∞ while V (θjne) = O(1). In addition, The higher-order diﬀerence
between ¯θ and ˆθ also approaches 0 as ne → ∞ while V (θjne) = O(1).

i=1 e(t)(cid:48)
(cid:16)(cid:80)ne

(cid:17)−1

i,x

i,x e(t)
i=1e(t)(cid:48)

x(cid:48)y,
(cid:17)
i,x e(t)
i,x
i=1 e(t)(cid:48)

i,x e(t)

Proof: WLOG, we work with the bridge-type noise. in this proof. The average of the mini-
mizers of the m loss functions is

¯θ = m−1(cid:80)m

t=1

(cid:16)

x(cid:48)x + (cid:80)ne

(33)

+ A(t) = diag(λne|θ|−γ) +
i,x from its mean. Let ¯A =

where eij ∼ N (0, λ|θj|−1). Let (cid:80)ne
¯A(t). A(t) can be regarded as the sample deviation of (cid:80)ne
m−1 (cid:80)m

i=1e(t)(cid:48)

i,x = E

i,x e(t)

t=1

t=1

(cid:80)ne
(cid:80)ne

¯A(t), the elements of which are
(cid:40) ¯A[j, j] = m−1 (cid:80)m
¯A[j, k] = m−1 (cid:80)m
t=1
ti ∼ N (0, 1) independently. Let S = (x(cid:48)x + diag(λne|θ|−1))−1.
where zti ∼ N (0, 1) and z(cid:48)
The Taylor expansion of the inverse of the sum of two matrices, assuming A(t) to be a small
increment, is (S−1 + A(t))−1 = S − SA(t)S + SA(t)SA(t)S + . . . Therefore, Eqn (33) becomes
¯θ = Sx(cid:48)y − S (cid:0) ¯A + O(λ2ne)(cid:1) Sx(cid:48)y.

ij − λne|θj|−1 ∼ λ|mθj|−1(χ2
ij e(t)

nem−nem)
(cid:80)ne

i=1 e(t)2
i=1 e(t)

∼ λ|θjθj|− 1

2 m−1(cid:80)m

i=1ztiz(cid:48)
ti

(34)

(35)

t=1

ij

,

On the other hand, the minimizer of the average of m loss functions is
(cid:16)
x(cid:48)x+diag(λne|θ|−1)+ ˆA

ˆθ =(cid:0)x(cid:48)x + (cid:80)nem

(cid:17)−1

x(cid:48)y,

(cid:1)−1x(cid:48)y =
i=1 ˆe(cid:48)
ijˆeij
(cid:17)
(cid:16) ˆA + O(λ2ne)

Sx(cid:48)y,

=Sx(cid:48)y − S

(36)

∼ λ

i=1 e2
i=1 eijeij

ij − λne|θj|−1 ∼ λ|mθj|−1(χ2

(cid:40) ˆA[j, j] = (cid:80)nem
ˆA[j, k] = (cid:80)nem

where eij ∼ N (0, λ|mθj|−1) for the sake of yielding the same regularization eﬀect as imposed
on ¯θ; and ˆA is deﬁned in a similar manner as ¯A, the elements of which are
nem − nem)
i=1 ziz(cid:48)
i

m |θjθk|− 1
i ∼ N (0, 1) independently. ¯A and ˆA in Eqn (34) and (37) follow the
where zi ∼ N (0, 1) and z(cid:48)
same distribution. The expected values of ¯A[j, j], ¯A[j, k], ˆA[j, j], and ˆA[j, k] are all equal to
zero; the variance of ¯A[j, j] and ˆA[j, j] is λ2|mθjk|−22nem = 2λ(λne)|θjk|−22/m, and that of
¯A[j, k] and ˆA[j, k] is λ2m−2|θjkθjl|−1nem = λ(λne)|θjk|−22/m. As m increases, both variance
terms shrink to 0. As ne increases while O(neλ) = 1, then both variance terms shrinks to 0 as
well. In other words, we expect ¯A and ˆA to be very similar. As such, ¯θ in Eqn (35) and ˆθ in
Eqn (36) are also very similar. In addition, as ne increases and λne = O(1), the higher-order
terms also goes to 0.
To ﬁrst illustrate the similarity between ¯θ and ˆθ, we simulated data (n = 30) from linear

2 (cid:80)nem

(37)

,

10

regression and a Poisson regression models, where the linear predictor is XT θ = X1 +0.75X2 +
0.5X3+0X4. X and the error in the linear regression was simulated from N(0, 1) independently.
The PANDA augmented noises e in both cases were drawn from N(0, λ2) with ne = 200. We
examined m = 30, 60, 90, 120 and λ2 = 0.25, 0.5, 1, 2, calculated ˆθ and ¯θ, and plotted their
diﬀerences in Figure S.1. The results show minimal diﬀerence between ˆθ and ¯θ.
linear regression

Poisson regression

Figure S.1: Diﬀerences between ¯θ and ˆθ in linear regression (top) and in Poisson regression (bottom)

S.9 PANDA in Each Iteration Realizes Weighted Ridge in Linear

Regression

Corollary S.2 (PANDA and weighted ridge regression). The OLS estimator in each
iteration of PANDA on the noise augmented data is equivalent to the weighted ridge estimator
ˆθ = (cid:0)xT x+eT e(cid:1)−1 xT y.
The proof is straightforward. Let ˜x = (x, ex)T .
estimator ˆθ = (˜xT ˜x)−1˜xT (y, 0) = (xT x+eT
x ex → neV(ex). For example, if the NGD is N(0,λ|θ|−γ
eT
and (λne) can be tuned as one single tuning parameter.

In each iteration of PANDA, the OLS
x ex)−1xy, leading to Corollary S.2. If ne → ∞, then
j );

j ), then neV(ex) = diag((neλ)|θ|−γ

11

q-q^l0.250.512m306090120306090120306090120306090120−0.003−0.002−0.0010.0000.0010.0020.003q1-q^1q2-q^2q3-q^3q4-q^4q-q^l0.250.512m306090120306090120306090120306090120−0.003−0.002−0.0010.0000.0010.0020.003q1-q^1q2-q^2q3-q^3q4-q^4