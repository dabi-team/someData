TREESKETCHNET: FROM SKETCH TO 3D TREE PARAMETERS
GENERATION

2
2
0
2

l
u
J

5
2

]

V
C
.
s
c
[

1
v
7
9
2
2
1
.
7
0
2
2
:
v
i
X
r
a

Gilda Manfredi, Nicola Capece, Ugo Erra, and Monica Gruosso
Department of Mathematics, Computer Science, and Economics
University of Basilicata
Potenza, Italy
{gilda.manfredi, nicola.capece, ugo.erra, monica.gruosso}@unibas.it

ABSTRACT

3D modeling of non-linear objects from stylized sketches is a challenge even for experts in computer
graphics. The extrapolation of objects parameters from a stylized sketch is a very complex and
cumbersome task. In the present study, we propose a broker system that mediates between the
modeler and the 3D modelling software and can transform a stylized sketch of a tree into a complete
3D model. The input sketches do not need to be accurate or detailed, and only need to represent
a rudimentary outline of the tree that the modeler wishes to 3D-model. Our approach is based on
a well-deﬁned Deep Neural Network (DNN) architecture, we called TreeSketchNet (TSN), based
on convolutions and able to generate Weber and Penn [1] parameters that can be interpreted by the
modelling software to generate a 3D model of a tree starting from a simple sketch. The training
dataset consists of synthetically-generated sketches that are associated with Weber-Penn parameters
generated by a dedicated Blender modelling software add-on. The accuracy of the proposed method
is demonstrated by testing the TSN with both synthetic and hand-made sketches. Finally, we provide
a qualitative analysis of our results, by evaluating the coherence of the predicted parameters with
several distinguishing features.

Keywords Computing methodologies · Computer graphics · Shape modeling · Image and video acquisition · 3D
imaging

1

Introduction

Manual mesh modeling of objects that are characterized by non-linear complex 3D structures remains a challenge
even for experts in computer graphics. Typically, objects such as trees are designed using procedural modelling [2],
which allows users to manipulate the speciﬁc parameters [3, 4, 5] that characterize them, avoiding direct editing of
their geometries. However, the complexity of the rules that affect these objects means that the set of parameters is
not only very large, but also non-linear. Consequently, their extraction is very complex. Drawing a target 3D object
is often easier and more intuitive than a manual reconstruction, or parametric modeling of its geometry. There are
several image-based approaches to reconstructing real objects as 3D models, e.g., those based on photogrammetry [6, 7]
and new technologies such as Depth Cameras [8, 9], LiDAR [10, 11], Laser Scanning [12] etc.. End-to-end and
GAN-based [13, 14] deep learning approaches are also used. Although there are several methods that use images to
obtain the ﬁnal 3D mesh [15, 16, 17, 18] many produce inaccurate results, with smoothed or very high poly meshes,
and wrongly-closed holes.

In this paper, we introduce a broker system that sits between 3D modelers and the specialized 3D modelling software
used to build 3D trees. Users only need to provide a hand-made (HM) sketch as input; then the broker provides
parameters that can be read by the 3D modelling software, and used to build a 3D Tree that is as similar as possible to
the sketch. Furthermore, our broker can provide the appropriate texture used to render the 3D tree. The key element
in our broker is a convolution based neural network that is trained using supervised learning paradigms, and is able
to learn parameter mappings and analyze the sketch based on a set of training data. As collecting a sufﬁciently large
number of HM sketches is a very expensive operation, we have developed a speciﬁc Blender add-on. This add-on,

 
 
 
 
 
 
Gilda Manfredi, Nicola Capece et al.

SHM

Rec.

GT

SHM

Rec.

GT

SHM

Reconstructed

SHM

Rec.

GT

SHM

Reconstructed

Figure 1: Reconstructed 3D trees are shown as images captured from different camera angles. The image on the left
shows a front-view maple tree reconstruction starting from a synthetic hand-made (SHM) input. The top row of the
middle column shows input, reconstruction and Ground Truth (GT) for the left-camera view of a palm, starting from the
SHM input. The middle row of the same column shows a front-camera view of a cherry tree, starting from the SHM
input. The bottom row of the middle column shows a right-camera view reconstruction of a bonsai, starting from SHM
input. Finally, the image in the right-hand column shows a left-camera view reconstruction of a pine tree, starting from
the SHM input.

called the Render Tree (RT) is based on the existing Blender Sapling Tree Gen 1. The beneﬁt of this approach is that it
makes it possible to create a consistent training dataset of synthetic hand-made (SHM) sketches, as reported in Figure 1
(e.g., the maple tree).

The choice of parameters is based on the Creation and Rendering of Realistic Trees reported in J. Weber and J. Penn [1].

Our experiment evaluated ﬁve tree types: maple, pine, bonsai, palm, and cherry. Four camera views were gathered
for each: front, back, left and right. In the ﬁnal step, we generated 250 randomly-controlled versions of each type of
tree, by varying the input parameters to the RT Blender add-on. This resulted in 5000 SHM. It should be noted that our
dataset could be extended to include other types of trees.

The main contributions of our paper can be summarized as:

1. An approach to quickly generate a training dataset consisting of synthetic and realistic sketches of 3D trees,

starting from randomly-controlled Weber-Penn parameters.

2. A speciﬁc DNN architecture with multiple outputs, based on the set of parameter values, and the training and

testing process.

3. The RT Blender add-on and the trained DNN are available online, and can be freely used for similar predictions:

https://github.com/Unibas3D/TreeSketchNet.

2 Related Work

Until recently, the best way to reconstruct an object in 3D relied upon the manual skills of a human modeler, and
modeling tools, such as well-known professional software suites that include Autodesk®3D Studio Max and Maya,
Cinema 4D (MAXON®), Houdini (SideFX®), and Blender. However, in recent years, and thanks to the rapid growth
of artiﬁcial intelligence technologies, 3D modeling has broken new ground, and has become accessible to non-expert
modelers. In this section, we report the state-of-the art in the domain of 3D mesh generation, starting from sketches or
images.

2.1

3D mesh generation

The use of new sensors such as LiDAR or Depth Cameras is very familiar to anyone who owns a latest-generation
smartphone. In this context, [19] propose an approach for the automatic reconstruction of 3D roofs, using airborne

1https://docs.blender.org/manual/en/latest/addons/add_curve/sapling.html

2

Gilda Manfredi, Nicola Capece et al.

LiDAR data and optical multi-view aerial imagery. [11] propose a computational framework for real-time 3D scene
reconstruction using single-photon data. The latter authors reconstructed complex outdoors scenes by acquiring LiDAR
data in broad daylight from distances up to 320 meters. A LiDAR-scanned point cloud has also been used to model
real-world trees, and [10] present a method to model plausible trees in ﬁne-grained detail from airborne LiDAR point
clouds. The tree model is reconstructed by ﬁrst segmenting a single tree point cloud, and then integrating trunk points
that complete the cloud, using a connected graph nearest neighbor search for each point. The constructed branch
skeletons are developed using a bottom-up greedy algorithm, then arranging the leaves. There are also other interesting
approaches that uses the tree point cloud, obtained by scanning real trees, as an input for a procedural modeling
algorithm ([20]) or for a DNN ([21]) to reconstruct a realistic 3D geometry of a tree. Many other studies used a depth
camera sensor. Examples include [8], who propose a 3D reconstruction approach using only one depth camera and two
mirrors, and [22, 23] who use a Kinect depth camera for 3D reconstruction. Laser scanning is another method that is
used for 3D reconstruction [12]. For example, [24] present an automated workﬂow that begins with tree detection from
mobile laser scanner point clouds and ends with single tree modelling. The aim is to model single trees for visualization
purposes in 3D city models.

More generally, as noted in [10], 3D tree modelling techniques can be classiﬁed into procedural [25], sketch [26, 27],
and image-based [28, 29] methods.

2.2 From images to 3D reconstruction

Digital photogrammetry using RGB images is one of the most popular techniques used to reconstruct 3D models.
However, software tools such as Reality Capture require the acquisition of a considerable number of images, with a
well-deﬁned methodology [30]. Several extensive, learning-based studies have tried to generate 3D objects starting
from simple, single RGB views [15, 31, 32, 33, 34]. [35] propose a deep implicit surface network to generate 3D
meshes from 2D images, by combining local and global image features to improve the accuracy of the distance ﬁeld
prediction. [36] propose a volumetric 3D generating network, based on a convolutional decoder. The latter approach
predicts the octree structure, and individual cell occupancy values provide high-resolution output even with limited
memory. Furthermore, it can generate shapes from a single image, along with a high-level representation of objects
and the overall scene. The generation of point clouds using deep learning has been widely explored. For example,[37]
investigate generative networks for 3D geometry based on a point cloud representation. The main focus of the latter
work is a well-designed pipeline that is used to infer point positions in the 3D frame from the input image, taking into
account the viewpoint. [38] propose a convolutional-recursive autoencoder architecture to retrieve cuboid, connectivity,
and symmetry aspects of objects using a single 3D image. The authors’ encoder is trained on a shape contour estimation
task and a decoder, and recursively decodes the cuboid structure. [39] investigate a deep learning approach based on
single and multiple views to reconstruct voxel 3D representations. The authors propose an encoder-decoder-based
framework called Pix2Vox that can be used for 3D reconstruction from real-world and synthetic images. One of the
recent contribution is provided by [40], that use a cGAN to extrapolate the 3D silouhette and the skeleton of a tree,
given a single tree image and some 2D strokes drawn by the user.

2.3 From sketches to 3D reconstruction

As reported in[41], the use of sketches to reconstruct a 3D model is intuitive for a human being. In this context,
deep learning approaches can be helpful in generating a 3D mesh using minimal data, such as a sketch, as input
information. [42] provide an encoder-decoder architecture to translate a 2D sketch into a 3D mesh. The method uses
latent parameters to represent and reﬁne a 3D mesh that can match the external contours of the sketch. [43] propose
a data-driven approach to learning that can reconstruct 3D shapes from one or more drawings. This Convolutional
Neural Network(CNN) based method predicts voxel grid occupancy from a line drawing, and outputs an initial 3D
reconstruction, while users can complete the drawing of the desired shape. Unsupervised learning is also used for 3D
object modeling. [44] propose a learning paradigm to reconstruct 3D objects from HM sketches that does not rely
on labeled HM sketch data during training. Their paradigm takes advantage of adaptation network training, notably
autoencoder and adversarial loss, and combines an unpaired 2D rendered image and an HM sketch in a shared latent
vector space. In a second step, nearest neighbors are retrieved from the embedded latent space, and each sketch in the
training set is used in a 3D Generative Adversarial Network. An interesting approach that is not based on deep learning
is proposed in [45]. The latter study presents a tool that can model complex free-form shapes by sketching sparse 2D
strokes. The proposed framework combines multi-view inputs to model a complex shape that can be occluded.

2.4 Procedural modeling

Rule-based methods [46, 47] can also take advantage of deep learning, and can overcome the need for user intervention
in the manipulation of a large number of parameters in rule sets, especially for a non-linear complex object such as

3

Gilda Manfredi, Nicola Capece et al.

a tree. Thus, [4] present a HM sketch approach to automatically compute a set of procedural model parameters that
can be used to generate 2D/3D shapes that resemble the initial input HM sketches. An interesting aspect of the latter
study is that it focuses on three procedural modeling rule sets, namely 3D containers (e.g., vases), 3D jewelry, and 2D
trees. The authors did not consider 3D tree shape generation due to the high complexity of the task. However, several
other approaches can overcome this problem by controlling the output of procedural modeling algorithms. Methods
include [48], who use a genetics-based algorithm, and [25], which is based on a Monte Carlo Markov Chain. There is an
interesting inverse procedural modeling approach, introduced by [49], that use deep learning to detect an L-system from
an input tree image. Another approach, provided by [50], use a combination of deep learning and procedural modeling
algorithms to extract a 3D tree model from a single image. This method is based on three DNNs that, starting form
an input photograph of a tree, mask out the tree, identify the tree type, and extract the tree Radial Bounding Volume
(RBV), respectively. The RBV is then used as an input for a procedural modeling algorithm that generates the 3D
model of the tree. To the best of our knowledge, our work is the ﬁrst study that can generate 3D tree shapes through the
prediction of deep learning parameters, starting from simple HM sketches.

3 Generation of 3D tree parameters

In this section we discuss the details of our approach, notably: (i) the dataset creation pipeline; (ii) the conﬁguration
of the TSN, and;(iii) the training procedure. The RT Blender add-on is a key element in our pipeline as it quickly
allows the generation of the training dataset, and the visualization of the 3D tree mesh. The latter is generated from
Weber-Penn parameters that are predicted from the TSN using SHM sketches, created by the add-on, as input.

3.1 The dataset creation pipeline

One of the weaknesses of supervised learning methods is ﬁnding a way to arrange a large set of labelled data, which
also have to be well-structured, to obtain the expected results [51, 52, 53]. Our RT Blender add-on overcomes this
drawback by creating, for each type, 250 3D tree meshes and storing parameters in a dedicated dictionary ﬁle for
each tree (see Details in our GitHub website [54]). We divide the Weber-Penn parameters into two subsets: ﬁxed and
unﬁxed. Fixed parameters take the same values for the same tree type, and unﬁxed parameters take randomly-controlled
values that vary the shape and detailed visual features of the tree. The TSN could be trained to understand the tree
type shown in the input sketch by adding a classiﬁcation branch. However, this would increase its complexity and,
consequently, degrade prediction performance. Moreover, adding a classiﬁcation branch is unnecessary because it is
possible to identify the tree type associated with the input sketch using ﬁxed parameters. This task was implemented
as a robust algorithm 1, and is discussed in Section 4. Starting from the pre-existing Sapling Tree Generator Blender
plugin, we manually deﬁne a dictionary of ﬁxed and unﬁxed parameters for each of the 5 tree types. Here, the aim is to
obtain consistent 3D tree meshes, in terms of visual features. The tree types were chosen in order to introduce as much
differentiation as possible with respect to their shape and visual features. The consistent mesh dictionaries produced by
our RT plugin were used as a starting point to generate the other 3D meshes, and their respective parameter dictionaries.
Unﬁxed parameters were randomly varied with respect to their order of magnitude, as shown in Table 1.

Table 1: Unﬁxed parameters and their range (minimum and maximum). The Sign parameter is binary.

Unﬁxed Parameters
Sign (binary)
Base Splits
Rotation Last Angle
Curvature
Back Curvature
Curvature Variation
Split Angle
Split Angle Variation
Rotate Angle
Rotate Angle Variation
Leaf Rotation
Leaf Down Angle Variation
Leaf Scale
Leaf Scale Variation
Down Angle Variation
Branch Rings

Max Values
1
ﬁxed parameter
360
360
360
360
360
360
360
360
360
360
∞
1
360
∞

Min Values
-1
0
-360
-360
-360
-360
-360
-360
-360
-360
-360
-360
0
0
-360
0

4

Gilda Manfredi, Nicola Capece et al.

In this way, we obtain 250 dictionaries for each tree type, which are used individually in our RT plugin to generate the
corresponding Blender Curve [55] object. This object is converted into two meshes: the skeleton (trunk and branches)
of the tree, excluding the foliage, and the foliage alone.

The obtained meshes are loaded into a well-designed Blender scene integrated with 4 cameras corresponding to the 4
tree views: front, back, left, and right as shown in Figures 2. In addition, our scene contains a directional light, without
shadows, to better-illuminate the tree. The global scene illumination consist of Blender path tracing and ambient
occlusion to increase rendering realism. The ﬁrst step in generating the SHM sketch is to add two black materials to the
tree, one for the trunk and one for the foliage, as shown in Figures 2a and 2b. The ﬁrst is a simple diffuse material with
zero roughness. The second consists of a simple leaf-shaped black texture, based on the tree type.

a

c

e

b

d

Figure 2: Generation of the SHM sketch. For each camera view, the tree’s skeleton and foliage are individually rendered
after applying the black and white materials, and after a set of pre-processing operations speciﬁc to the two types of
meshes. In the ﬁnal step, these renderings are mixed to obtain the ﬁnal SHM sketch.

The next step is to render the skeleton and the foliage meshes of the tree individually, with the previously-deﬁned black
materials. To obtain the skeleton edges, we deﬁne several steps as shown in Figure 2c: (i) the ﬁrst step is to slightly thin
the skeleton mesh to remove the thinnest branches; (ii) the second step consists of the following chain of compositing
operations [56]:

(a) remove the background using a 0.5 threshold value;

(b) apply the Intel® Open Image Denoise-based ﬁlter to delete the residual background artifact;

(c) apply the Sobel ﬁlter [57] to trace rough outlines;
(d) apply a color ramp that sets pixel values less than 0.9 to 0 and the remainder as 1, to highlight all branch edges;

(e) apply the inversion color ﬁlter to obtain a negative, with white background and dark edges;

(f) apply the erosion morphological operation to delete scattered points and produce the ﬁnal skeleton sketch, as

can be seen in Figure 2c.

Similarly, to obtain the foliage edges (see Figure 2d), we use a simpler compositing chain of operations consisting of:

5

Gilda Manfredi, Nicola Capece et al.

(a) apply the background removal ﬁlter using a 0.9 threshold value;

(b) apply the strong Gaussian ﬁlter to uniformalise and emphasize the shape of the foliage;

(c) apply a color ramp that sets values less than 0.01 to 0 pixel and the remainder 1, to include blurred foliage

edges;

(d) apply the Sobel ﬁlter to trace the edges;

(e) apply the same inversion color ﬁlter used for the skeleton sketch to obtain the foliage sketch, as shown in

Figure 2d.

The ﬁnal synthetic sketch is obtained by multiplying the skeleton and foliage images (the mixed sketch), as shown in
Figure 2e.

Maple Tree

Palm Tree

Pine Tree

Cherry Tree

Bonsai Tree

T
G

M
H
S

Figure 3: Sample datasets for each tree type. The ﬁrst row shows a camera view of the tree reconstructed using ground
truth (GT) parameters. The second row show SHM sketches corresponding to the GT views.

For each camera view in the scene, we generate an SHM sketch. The ﬁnal dataset therefore consists of 250 trees × 4
views × 5 tree-types = 5000 SHM sketches of trees (1000 sketches for each type [40]), together with their parameter
dictionaries and ground truth (GT) images. Resolution was 608 × 608, which is the maximum dimension supported by
our tested core net (see Section 5.2). This initial resolution can be resized based on the required input of each tested
TSN, including those chosen by us (see Section 3.2). The dataset is split into training and validation sets, with examples
of the 5 tree types, together with their GT parameters, and SHM sketches, as reported in Figure 3. The validation set
consists of a single viewpoint for each tree type, and the training set consists of the other three viewpoints [58]. A
speciﬁc test dataset was created separately, as described in Section 4.

3.2 The TSN architecture and training procedure

To address the regression problem, we deﬁne a speciﬁc TSN architecture based on EfﬁcientNet-B7 [59] as the core
net. This network was found to be the best core net for our architecture, after a series of empirical experiments with
other well-known networks, as reported in Section 5.2. EfﬁcientNet-B7 belongs to a family of networks created starting
from an initial network architecture, called EfﬁcientNet-B0, subsequently scaled up using the method proposed by [59].
This method aim to an improvement in performance by scaling uniformly the EfﬁcientNet-B0 width, depth and image
resolution. All the EfﬁcientNet architectures are characterized by a convolutional layer followed by 7 macro-blocks,
each containing Li Inverted Residual Blocks, sometimes called MBConv blocks[60]. EfﬁcientNet-B7 is 8.4× smaller,
6.1× faster, and much more accurate on ImageNet than the best existing CNN.

As we use a supervised learning paradigm, our TSN input consists of 224 × 224 reshaped sketches (see Sec. 3.1), and
the corresponding tree parameters are structured as a matrix that we call the target matrix. Each target matrix row
represents a single parameter, and column values are parameter values. Since the number of values of each parameter
item can be 4 or 1, our target matrix has a dimension of 4 × np, where np = 62 is the number of parameters (see Details
in our GitHub website [54]). To generate a consistent matrix, 1-valued parameters are repeated in each column. The
matrix contains all ﬁxed and unﬁxed parameters reported in our GitHub website [54], without consideration of pruning,
armature, and animation parameters, which are left as default values where they are mandatory. To avoid overﬁtting
and underﬁtting problems (see Sec. 3.2.1), based on their order of magnitude, we divide this target matrix into six
sub-matrices that represent the target for each ﬁnal TSN branch. The dimension of each sub-matrix is 4 × nt, where
nt ∈ [1; np], which represents the number of parameters for a speciﬁc order of magnitude (see details in Sec. 3.2.1).

6

Gilda Manfredi, Nicola Capece et al.

Figure 4: The architecture of our TSN, with input in the form of a 224 × 224 sketch. The initial block is a simple
convolutional layer. The following blocks contain Li MBConv blocks. The last part of the network is made up of six
branches, one for each parameter order of magnitude. Each branch has a fully-connected layer with linear activation
function and a reshape layer.

Our TSN was trained using Adam [61], with default parameters and 1e−5 as the initial Learning Rate and the Mean
Squared Error as the loss function. To assess performance accuracy we used (1 − RM SE) ∗ 100 (Root Mean Square
Error) during the training phase. Network weights were initialized using ImageNet [62, 63], and the batch size was set
at 8. Our TSN was trained using a NVIDIA Titan XP GPU with a 12 GB G5X frame buffer.

3.2.1 Overﬁtting and underﬁtting

The optimal dataset and the TSN conﬁguration were identiﬁed after several attempts, in which we addressed overﬁtting
and underﬁtting problems. Our ﬁrst, “toy”, approach used a basic VGG-16 [64] DNN as the core net that represents
608 × 608 rendered (sketches and GT) images. This conﬁguration was found to lead to general overﬁtting with higher
intensity in some branches and very high losses. We therefore inserted dropout layers [65] before the layers of each
output DNN branch with α = 0.2 for branches that were less affected by overﬁtting, and α = 0.5 for branches
that were more affected. High loss during training could be an indication of the vanishing gradient problem and,
consequently, underﬁtting [66, 67, 68]. To reduce this problem, we introduce a residual learning approach using skip
connections [69] that are placed among consecutive layers in the VGG-16 core net. Although this latter approach
resulted in acceptable performance, we used this experiment as a baseline for our ﬁnal conﬁguration. This conﬁguration
used a more recent version of EfﬁcientNet which is able to better-manage the problems described above. Four other
attempts used ResNet [69] (variant 50), AlexNet [70], Inception V3 [71] and CoAtNet [72] as core nets. ResNet
and Inception V3 are classic neural networks [73] that are used in many computer vision tasks, AlexNet was used in
comparable study with a similar sketch-to-mesh-parameter approach [4], while CoAtNet and EfﬁcientNet represent
the State-Of-The-Art (SOTA) for the image recognition. Section 5.2 reports quantitative performance comparisons
between VGG-16 with skip connections, and EfﬁcientNet-B7. Using VGG-16 as the core net, we experimented with
using the entire target matrix as the output of the DNN, with only one output branch. However, the results were very
poor, and the DNN was completely underﬁtted. We therefore deﬁned 6 DNN output branches, based on the order of
magnitude of parameter values. This was possible because the Weber-Penn Blender parameter dictionary consists of
heterogeneous data, as reported in our GitHub website [54]. Therefore, we were able to carry out conversions and resize
many of them, as a function of their order of magnitude. Speciﬁcally, we deﬁned several sub-matrices, one for each
DNN output branch, as shown in Figure 4. Discrete integers and string parameter values were parsed as ﬂoats; Boolean

7

Input (224 x 224)FC + LinearReshape (4 x 17)FC + LinearReshape (4 x 4)FC + LinearReshape (4 x 12)FC + LinearReshape (4 x 16)FC + LinearReshape (4 x 12)FC + LinearReshape (4 x 1)Conv, 3x3MBConv1, 3x3MBConv6, 3x3MBConv6, 5x5MBConv6, 3x3MBConv6, 5x5MBConv6, 5x5MBConv6, 3x3Gilda Manfredi, Nicola Capece et al.

t
u
p
n
I

d
e
t
c
u
r
t
s
n
o
c
e
R

h
t
u
r
T
d
n
u
o
r
G

Figure 5: The results of the SHM test set obtained from our RT add-on taken from the same camera view, for each
tree-type. The ﬁrst row shows the input SHM sketches; the second row shows the reconstructed 3D meshes using the
corresponding predicted parameters dictionaries; the last row shows the GT.

parameter values were parsed as integers; non-numeric string parameter values were parsed as ﬂoats using the Labeled
Encoding method (e.g., Leaf Shape described in our GitHub website [54]; and binary [−1; 1] parameters were converted
to [0; 1]. Although it is not possible to consider [− inf, inf] values, we normalized them and [−360, 360] values to a
[−1, 1] range using the Max Abs Scaling method. This scaled the data and preserved sparsity. Maximum values were
stored in normalization matrices and reused in the testing step. Tests of Inception V3 identiﬁed signiﬁcant overﬁtting on
[− inf, + inf] and [0, 1] DNN output branches, which were reduced by adding a L2 regularization [74] on the ﬁrst two
fully-connected layers, followed by a dropout layer with α = 0.2. We obtained little overﬁtting on the [−360, 360]
DNN output branch; in this case we only used a dropout layer with α = 0.5 to reduce the problem (see Figure 4).

4 Results

t
u
p
n
I

d
e
t
c
u
r
t
s
n
o
c
e
R

h
t
u
r
T
d
n
u
o
r
G

Figure 6: Results of TSN network given images with different camera views: in the left column the camera was right
tilted; in the middle was bottom placed and tilted upward; in the last column the camera was left tilted.

8

Gilda Manfredi, Nicola Capece et al.

Sketch

Skeleton Material

Mesh

Sketch

Skeleton Material

Mesh

Sketch

Skeleton Material

Mesh

h
t
u
r
T
d
n
u
o
r
G

n
o
i
t
c
i
d
e
r
P

h
t
u
r
T
d
n
u
o
r
G

n
o
i
t
c
i
d
e
r
P

h
t
u
r
T
d
n
u
o
r
G

n
o
i
t
c
i
d
e
r
P

Figure 7: This ﬁgure shows some examples of test sketches, the corresponding GT, and predictions. The skeletons
show that the trunk and branches correctly conform with the starting sketch. The reconstructed trees reported in the
“Material” column show the correspondence between the foliage and the shape proposed in the input sketch. The last
column shows simple reconstructed meshes without any material.

The tree sketch is an input to our TSN, which produces six 4 × nt sub-matrices containing predicted parameter
values (see Sec. 3.2). These sub-matrices, which need to be de-normalized, are multiplied from their corresponding
normalization matrices stored after the training phase. This operation makes it possible to adopt predicted TSN values as
valid Blender inputs. The sub-matrices that are obtained from a single sketch prediction are rearranged as a Blender-like
parameters dictionary. In particular, each sub-matrix column is associated with a dictionary key, representing a speciﬁc
parameter. When the expected parameter is represented by a single value rather than a vector of four elements, only the
ﬁrst value is taken. The dictionary is also used to identify the class of the predicted tree (see Algorithm 1), and assign
the correct textures, based on the tree type, to the 3D Blender mesh from the same dictionary. Speciﬁcally, Algorithm 1
takes two data structures as input. The ﬁrst is a well-designed dictionary-based data structure with characteristic
parameters CP that are organized for each tree type. Each element in cp ∈ CP contains three items of information:
the parameter name, its value, and the tree type. A parameter is deﬁned as cp for a speciﬁc tree type if it respects two
properties: (i) it is always present in all samples of that speciﬁc tree type, and; (ii) its value is in a speciﬁc range for that
tree type. The second input is P , which is a dictionary of predicted parameters. We compare each parameter p ∈ P with
the corresponding cp for each tree type using the parameter name, and verify that the value of p respects the following
condition:

p.value ∈ [cp.value − (cid:15), cp.value + (cid:15)]

(1)

where [cp.value − (cid:15), cp.value + (cid:15)] is the range of acceptable values for that tree type. Let eligibles be a dictionary
in which the keys are the tree types, and the values are counters (one for each tree type). If the predicted parameter
respects the condition (1), then this parameter is eligible for that tree type, and its counter is increased by one. Finally,
Algorithm 1 returns the tree type of the predicted dictionary based on the highest percentage of counters. The rearranged
predicted parameters dictionary is then loaded from Blender and interpreted using a load utility method in our RT
plugin. This utility makes it possible to generate the 3D tree mesh from the input dictionary, and assigns the correct
texture based on the tree type detected by Algorithm 1. The texture is chosen from a set of predeﬁned textures for
each tree type. To test our approach, an appropriate dataset was deﬁned based on our tree types. Figure 5 shows the

9

Gilda Manfredi, Nicola Capece et al.

Algorithm 1: Detecting the tree type from parameters
inputs :CP set of characteristic parameters for each tree-type

P dictionary of predicted parameters

outputs :t tree-type
foreach p ∈ P do

foreach cp ∈ CP do

if p.name == cp.name & p.value == cp.value ± (cid:15) then

eligibles[cp.type] + +

end

end

end
t = M axP ercentage(eligibles)

input SHM sketches similar to the training set that were generated by our RT add-on, the reconstructed 3D tree mesh,
and the GT. As Figure 5 shows, the tree structure of the test sketches was correctly predicted by our TSN, notably
with respect to, for example, the number of splits for each trunk segment, curve angles, the number and distribution of
branches, the ratio of the trunk to its height, and the overall height. As can be seen from Figures 5 , our results are
visually consistent with the input sketches, and are consistent with GT parameters with regard to SHM sketches. To
demonstrate the TSN generalization capability, we created also a test set containing sketch images rendered in different
views respect to the 4 views used for generating the training data. The results of this test, shown in Figures 6, conﬁrms
the generalization ability of TSN. Figure 7 shows some SHM sketches and the 3D meshes generated by Blender based
on the predicted parameters dictionaries. Figure 7 also shows the applied materials as a function of the tree type, and
the normal mapping of each 3D mesh.

5 Comparisons

To the best of our knowledge, this approach is the ﬁrst attempt to generate 3D tree shapes from well-deﬁned predicted
parameters. Consequently, we try to do our best to compare our results with other approaches. Inspired by [4], we
assessed and validated our approach in a user study based on HM sketches provided by 15 different users, as reported
in Sec. 5.1. In addition, we provide quantitative comparisons using different core nets, to validate our choice of
EfﬁcientNet-B7 as optimal (see Sec. 5.2 and Sec. 5.3). Finally, we provide an extensive qualitative analysis of our TSN
predicted parameters, as reported in Sec. 5.4.

5.1 Controlled experiment

To test our approach in a real-life context, we conducted a controlled experiment with 15 non-experts in drawing. Each
person was asked to provide us with a single sketch for each type of tree. The goal was to assess the conﬁdence level
of our TSN parameters prediction using HM sketches provided by participants. Participants were free to choose any
software package to draw their trees. For each participant we randomly selected 5 SHM sketches, one for each tree
type. The participants were asked to look at each SHM sketch for two minutes and then draw the tree sketch according
to their own style. In this way, the participants can understand what types of trees the system could handle without
being constrained too much on the drawings. Figure 8 shows some of the results of the experiment. In general, the TSN
performed well on most of the HM sketches provided by participants. Although there are some differences, notably the
cherry tree, the reconstructed 3D mesh is consistent. Moreover, the TSN was able to extract details contained in HM
sketches such as the Branch Rings (see Details in our GitHub website [54]) of the pine tree, the curvature of the trunk
of palm and bonsai trees, and the Base Splits (see Details in our GitHub website [54]) of the maple tree.

5.2 Core net testing

The choice of the EfﬁcientNet-B7 core can greatly inﬂuence the outcome of the approach. Therefore, we ran tests
with different core models, and assessed their performance. In particular, for each tested core model, we analyzed
the 1 − RM SE for each speciﬁc branch, and the overall average for all branches. The ﬁrst comparison was between
EfﬁcientNet-B7 and VGG-16 with skip connections (see Sec. 3.2.1). The second comparison was performed with
ResNet and Inception V3, which are used as core models in several computer vision tasks [71]. For ResNet, we used
variant 50 because its depth is sufﬁcient to extract the essential low-level features needed for our task. For the test
made with Inception V3 we used the ﬁrst 2 convolutional blocks, and the ﬁrst 5 Inception Modules, to reduce the

10

Gilda Manfredi, Nicola Capece et al.

t
u
p
n
I

d
e
t
c
u
r
t
s
n
o
c
e
R

Figure 8: Some results of the controlled experiments. The ﬁrst row shows the participants’ sketches with details. The
second row shows the reconstructed 3D trees with the same details. The effectiveness of our approach can be validated
empirically by comparing the sketches with the corresponding 3D reconstructions.

DNN complexity and consequently the overﬁtting problem caused by the overparametrization [75]. We also compared
EfﬁcientNet-B7 with AlexNet [70] to evaluate the validity of procedural modelling-based approaches for 3D mesh
generation that are proposed in [4]. For the sake of completeness, we ﬁnally compared EfﬁcientNet-B7 with another
SOTA network, called CoAtNet. Each test was performed using 30 sketches, of which 15 were SHM, and 15 were
HM. For each core model, we trained our architecture for 2k epochs, and the assessment was based on the following
factors: (i) possible overﬁtting of branches (low and high) based on the validation curve accuracy with respect to the
training curve accuracy; (ii) the accuracy of each branch, evaluated by observing the validation curve, and; (iii) the
generalization level, evaluated by observing the testing curve.

Table 2 shows the results of the comparison with the SHM-based test set. These sketches were obtained using our
pipeline, as described in Section 3.1. The SHM tests were run using all parameters and the prediction dictionaries
were compared with GT parameters using 1 − RM SE. EfﬁcientNet-B7 performs better with respect to the other core
models. However, CoAtNet does perform slightly better on the [−360, 360] branch. Inception V3 also performs well,
but there are no branches where Inception V3 outperforms EfﬁcientNet-B7. The overall performance suggest that
EfﬁcientNet-B7 perform better than the other networks.

Core Nets

[−inf, inf ] [−360, 360]

VGG-16
ResNet-50
AlexNet
Inception V3
CoAtNet
EfﬁcientNet-B7 (our)

0.8022
0.7885
0.7791
0.7957
0.8073
0.8093

0.9552
0.9568
0.9552
0.959
0.9607
0.9595

1-RMSE

[0, 1]
0.7988 0.9454
0.7803 0.937
0.7963 0.9688
0.802 0.9657
0.8047 0.9556
0.8129 0.9798

[0, inf ] [min, max] [−1, 1] Overall
0.9899 0.9115
0.9646 0.8975
0.9895 0.9113
0.988 0.9152
0.9848 0.915
0.9964 0.9253

0.9772
0.9579
0.9792
0.9811
0.9769
0.9936

Table 2: Core net comparisons of SHM sketches. Observations of overall 1 − RM SE suggests that EfﬁcientNet-B7
has an higher accuracy. However, CoAtNet performs slightly better on the [−360, 360] branch. The best values are
highlighted in bold.

For HM testing, we prepared some hand-made sketches inspired by those generated by the SHM (see Sec. 3.1 and
Figure 2) approach and considered their corresponding parameters as GT. The comparisons reported in Table 3 show that
both SOTA networks (Efﬁcientnet-B7 and CoAtNet) and Inception V3 achieved better results with respect to the other
three core models for HM sketches with respect to training sketches, demonstrating a higher level of generalization. In
particular, we can notice that EfﬁcientNet-B7 has an higher accuracy respect to CoAtNet and Inception V3. Furthermore,
a comparison of the results reported in Table 2 and Table 3, which have similar features, and an analysis of the training
report, identiﬁed that AlexNet is slightly overﬁtted on [0, inf ] and [min, max]. By applying the same analysis to the
other core models, we identiﬁed that for VGG-16 with skip connections, the branches [−inf, inf ] and [0, inf ] are
slightly overﬁtted, while for ResNet-50, Inception V3, EfﬁcientNet, and CoAtNet there is no evidence of overﬁtting.
However, EfﬁcientNet-B7 was selected as the best core model due to its higher accuracy compared to the other DNNs
and its generalization ability with HM sketches signiﬁcantly different from training data.

11

Gilda Manfredi, Nicola Capece et al.

Core Nets

[−inf, inf ] [−360, 360]

VGG-16
ResNet-50
AlexNet
Inception V3
CoAtNet
EfﬁcientNet-B7 (our)

0.7577
0.7585
0.75
0.7873
0.7712
0.7688

0.9485
0.9605
0.955
0.9618
0.9646
0.9628

1-RMSE

[0, 1]
0.7931 0.9176
0.8278 0.9356
0.8057 0.9136
0.8122 0.9607
0.8192 0.9455
0.8658 0.979

[0, inf ] [min, max] [−1, 1] Overall
0.9795 0.8879
0.9614 0.8993
0.975 0.8853
0.9862 0.9138
0.9815 0.9071
0.9957 0.9267

0.931
0.952
0.9124
0.9746
0.9608
0.9883

Table 3: The results of HM sketches clearly show the difference between the different core nets. Based on overall perfor-
mance, EfﬁcientNet-B7 has an higher accuracy. Despite the differences between hand-drawn sketches, EfﬁcientNet-B7
is demonstrated to have a greater ability to generalize. The best values are highlighted in bold.

5.3 Hausdorff Distance

Since in the previous section we provided a quantitative analysis of the DNNs performances in terms of accuracy, in
this section we asses the quality of the reconstructed 3D trees. For this purpose we used a metric called Hausdorff
distance [76]. In particular, we compare the distances calculated for EfﬁcientNet-B7, CoatNet, and Inception V3,
because they represent the best performing DNNs, as reported in the previous section. The Hausdorff distance represents
the maximum distance among a set of distances between two meshes. This set contains all minimum distances between
the vertices of the ﬁrst and second mesh. In our case, we calculate, for each DNN, the Hausdorff distance between
15 tree meshes of different types reconstructed from the DNN predictions and their ground-truth meshes. The 15
ground-truth trees are the same for all DNN to grant the results coherence. Table 4 shows, for each network, the average
of the calculated Hausdorff distances. From the results in Table 4, it can be seen that EfﬁcientNet-B7 is still the better
choice because its Hausdorff distance tends more to 0. It means that the meshes resulting from EfﬁcientNet-B7 are
more similar to their ground-truth than those resulting from Inception V3 or CoAtNet. Additionally, our approach
provides better results in calculating this metric than the SOTA [40].

Core Nets
Inception V3
CoAtNet
EfﬁcientNet-B7 (our)

Hausdorff Distance
0.047
0.041
0.031

Table 4: Hausdorff distances of the best performing DNNs. Based on the results, the meshes resulting from EfﬁcientNet-
B7 are more similar to their ground-truth. The best values are highlighted in bold.

5.4 Qualitative analysis

In this section, we provide an empirical analysis of our TSN predicted parameters using HM sketches for which GT
parameters were unknown. To do this, we manually extracted some easily-visible parameters for each tree type, based
on the input sketches, and compared them to TSN predictions.

Table 5 shows the input HM maple tree sketch, and some of its simple visual parameters. This table shows some of the
TSN’s predicted parameters and performance accuracy. In particular, the ﬁrst Vertical Attraction, highlighted in dark
orange, shows that the trunk (the ﬁrst branch level) is attracted upwards (positive sign) and the branches (second branch
level) are attracted downwards (negative sign) with a different attraction coefﬁcient, were correctly predicted by our
TSN. Trunk height indicates that about 30% of the maple tree height has no branches. The trunk in the sketch is also
split into two parts, conﬁrmed by the predicted Base Splits parameter, highlighted in green, which indicates one trunk
subdivision. The Branches parameter indicates the number of branches for each branch level. In the maple tree case,
there are 0 ﬁrst level branches because the trunk is not considered as a branch, and 54 second level branches. Curvature
and Back Curvature signs are positive, which means that the trunk and its tip are curved backward the front of the tree.
Second level branches and their tips are curved in the opposite direction to the trunk as shown in the related sketch. The
remaining parameters are also correctly predicted by our TSN, notably, the e.g., Levels parameters indicate that the
sketch is characterized by two branch levels.

Table 6 is a visual comparison of some simple parameters and their corresponding TSN-predicted values. In this case
Trunk Height indicates that only the 1% of the palm tree has no branches, but the Branch Distribution parameter gathers

12

Gilda Manfredi, Nicola Capece et al.

OUTPUT
[3.5, −1.82]

1

Vertical Attr.
Trunk Height 0.29
Base Splits
Branch Dist. 1.5
[0, 54]
Branches
[0.15, −12.4]
Curvature
[0.33, −3.14]
Back Curvat.
2
Levels
Branch Rings 0
ACCURACY ≈ 90%

Table 5: Simple visual parameters of the maple tree. The corresponding table reports TSN predicted values. Indices of
elements in array-structured parameters represent the branch level.

OUTPUT
[8.51, −0.2]

0

Vertical Attr.
Trunk Height 0.0
Base Splits
Branch Dist. 9.921
[0, 25]
Branches
[23.83, 51.3]
Curvature
[−22.87, 15.71]
Back Curvat.
2
Levels
Branch Rings 2
ACCURACY ≈ 90%

Table 6: An example of a palm tree SHM sketch with predicted parameters. The correctness of the prediction is mainly
demonstrated by the parameters Branch Distribution and Trunk Height, which indicate that the foliage is concentrated
towards the top of the tree.

all of the branches towards the top of the tree, and attenuates the Trunk Height effect. This aspect is also conﬁrmed
in the maple tree example 5, where these two parameters are balanced. In the sketch, there are no subdivisions of the
trunk, indicated by the Base Splits parameter. Since Curvature and Back Curvature trunk signs are discordant, the trunk
is S-shaped and curved backwards (positive sign for Curvature) and its tip (negative sign Back Curvature) is curved
forwards. The ﬁrst-level branches follow the curvature of the trunk. Finally, this example shows the Branch Rings
parameter, which indicates how many rings the branches are distributed on around the trunk.

The most evident parameter in Table 7 is Branch Rings, due to the pine tree structure. The sketch is characterized by
ﬁve rings and three branch levels, as can be seen in the corresponding table, notably the sign and number of Vertical
Attraction, Curvature, and Back Curvature parameters. The red arrows show the sign of the Curvature parameter, and
indicates that all branch levels are curved forwards. The tip of the last two branches levels follows the same direction of
curvature as their base, and the trunk tip is curved backwards with respect to its base, as can be seen from the Back
Curvature parameter. In this case, the branches of all levels are attracted upward as correctly predicted by our TSN.

The cherry tree shown in Table 8 is characterized by four branch Levels. The coefﬁcients of Curvature and Back
Curvature indicate the steepness of the curve. In this case, the ﬁrst is curved backwards, and the other branch levels are
curved in the opposite direction. The curvature of the tip of the ﬁrst three branch levels follows the curvature of the base
of the trunk, and the tips of the last branch levels follow their base curvature. In this case, the trunk is split into two, and
the branch attraction points upward for all levels. Finally, the Trunk Height indicates that about the 50% of the cherry
tree has no branches, which is consistent with the Branch Distribution parameter.

13

Gilda Manfredi, Nicola Capece et al.

OUTPUT

0

[2.0, 0.0, 0.45]

Vertical Attr.
Trunk Height 0.08
Base Splits
Branch Distr. 1.619
[0, 36, 7]
Branches
[−9.37, −36.12, −28.9]
Curvature
[10.9, −2.1, −1.31]
Back Curvat.
3
Levels
Branch Rings 5
ACCURACY ≈ 94%

Table 7: This ﬁgure and the corresponding table show that the number of Branch Rings is correctly predicted by our
system, as are other parameters.

OUTPUT

1

[0.0, 0.06, 0.2, 0.02]

Vertical Attr.
Trunk Height 0.49
Base Splits
Branch Dist. 1.965
[0, 39, 39, 10]
Branches
[18.1, −2.16, −1.4, −19.5]
Curvature
[22.6, 11.7, 3.1, −0.04]
Back Curvat.
4
Levels
Branch Rings 0

Table 8: An example of a cherry tree. In this case, the Base Splits parameter is 1, indicating that the main trunk is
divided into two sub-trunks.

ACCURACY ≈ 90%

The bonsai tree shown in Table 9 also have four branch Levels. The trunk of the bonsai has no splits, as indicated by

OUTPUT

Vertical Attr.

[0, 1, 0, 0]

Trunk Height

0.2

Base Splits

0

Branch Dist.

1.4807

Branches

Curvature

[0, 15, 15, 10]

[−62, 145, −1.4, −23]

Back Curvature [63, −136, −99, −0.6]

Levels

Branch Rings

4

0

ACCURACY ≈ 94%

Table 9: An example of the bonsai tree SHM sketch. The S-shape of the trunk and ﬁrst-level branches is indicated by
the discordant signs of Curvature and Back Curvature parameters.

the Base Splits parameter, and is also S-shaped, as showed by the discordant signs of the Curvature and Back Curvature
parameters. The ﬁrst-level branches are also S-shaped but are specular in their orientation to the trunk. Indeed, the
Curvature and Back Curvature of the other branches levels are all negative, so the bases and the tips of these branches

14

Gilda Manfredi, Nicola Capece et al.

are curved in the forward direction. In addition, the 20% of the cherry tree has no branches and all the branches levels
are attracted upward, as conﬁrmed by the Trunk Height and Vertical Attraction parameters respectively.

6 Conclusion and Future Work

Our proposed approach can predict parameters for 3D tree mesh generation using a DNN method. The main goal is to
automate procedural modeling, by introducing a broker system between the modeler and the modeling software used to
build 3D trees. The core of our broker consists of a DNN based on convolutions, that we call TreeSketchNet, which
is trained using supervised methods to learn the mapping between well-known Weber-Penn tree parameters and 2D
sketches of trees. As a large amount of data is needed for training and validation datasets, we developed a dedicated RT
Blender add-on. This add-on makes it possible to automate the generation of realistic SHM sketches starting from 3D
tree meshes that are generated a priori using a set of ﬁxed and unﬁxed randomly-controlled parameters. This approach
is implemented to overcome the problem of generating an expensive HM dataset of drawings. For our experimental
purpose, we consider 5 tree types (maple, pine, bonsai, palm, and cherry) and create corresponding sketches from front,
back, left, and right camera angles. We assessed our system and the obtained results using a controlled experiment.
Speciﬁcally, we asked participants to provide HM sketches based on reference SHM examples shown for two minutes,
and used them to test the system. The results were promising, and we believe may be a starting point for future
research. Our experiment also highlighted the high level of generalization, and validated the accuracy of our approach.
Furthermore, we experimented with several other core nets to identify the most suitable and performant option. Notably,
we tested AlexNet, which is widely-used in computer vision tasks, and won the ILSVRC competition [77], becoming
established as the state-of-the-art in the deep learning ﬁeld. AlexNet is the only DNN that has been used in a similar
sketch-to-mesh parameters approach [4]. However, it did not perform as well as Efﬁcientnet-B7, and was therefore
discarded. Finally, we provide a qualitative analysis of our results, speciﬁcally, a visual comparison of the predicted
parameters with their corresponding input sketches. Our results show that our procedural modelling-based approach is
better compared to image-to-mesh or voxel, with respect to rough and smooth surfaces, artifacts, holes, and deformed or
unnecessary polygons. As our results are promising, we plan to continue to investigate the sketch-to-3D mesh approach
based on procedural modelling and deep learning. In future work, we could expand the number of tree types and/ or
generate new 3D meshes by adapting our approach to other contexts where procedural modelling can be used, such as
vases, chairs, buildings, furniture, etc. [4]. Also in future work, we would like to deﬁne a method for texture generation,
which would avoid the use of the tree type identiﬁcation algorithm 1 that is used to select the most suitable texture from
a predeﬁned set. A possible method could be to consider colored input sketches, and extract the texture or color from
them.

6.1 Discussion and Limitations

To the best of our knowledge, our proposed approach is the ﬁrst to use deep learning to predict 3D tree mesh
parameters from sketches. Approaches that directly predict mesh using images as DNN input often present qualitative
problems [35, 39, 78], such as rough and smooth surfaces, artifacts, holes, and deformed and unnecessary polygons.
Furthermore, the 3D meshes that are generated from direct methods often poorly resemble the input RGB images or
sketches. Procedural modelling approaches to the generation of 3D meshes can overcome these problems [79, 80].
Another advantage is that the 3D mesh is always correct, as our robust RT Blender add-on correctly interprets the
parameters and uses them, avoiding artifacts and error generation. The latter is demonstrated by the predictive accuracy
of the parameters of our TSN, even in difﬁcult conditions, as reported in Section 5.1. Although procedural modelling
has only recently been explored in the ﬁeld of deep learning and artiﬁcial intelligence [80, 81, 82], there are, as yet, few
speciﬁc approaches that examine prediction parameters for 3D non-linear content generation, either plants, in general,
or trees [40, 50], in particular. Moreover, there are no approaches that use very rudimentary input data such as sketches.
The latter observations make it difﬁcult to compare our work with others [83]. However, we were able to compare our
baseline (see Section 5.2) with better-known core nets, to assess the effectiveness of our work. We also evaluated the
coherence of the predicted parameters with sketches provided as TSN input, through a visual qualitative analysis (see
Section 5.4) of some parameters that are easier to understand and visually identify. In addition, we compared the 3D
tree meshes generated from predicted parameters with ground-truth meshes using the Hausdorff distance, as reported in
Section 5.3. To test the robustness of our method, we conducted some extra experiments of challenging cases. In detail,
we tested our TSN with sketch images containing broken segments. Figure 9 shows two examples of tree sketches with
deleted parts. In the ﬁrst row of Figure 9 it can be seen that the reconstructed tree has less branches in the cropped
region of the sketch than the ground-truth. The row of Figure 9 shows the sketch of a pine with a missing ring. For
this reason, TSN predicts a pine with one less ring than the ground-truth. Figure 10 reports two outliers caused by the
poor TSN accuracy concerning the provided input sketches drawn by the experiment participants (see Section 5.1).
The ﬁrst row of Figure 10 shows a HM sketch of a maple tree with the relative 3D model. The sketch represents a tree

15

Gilda Manfredi, Nicola Capece et al.

with few branches drawn with a very thin line. As a result, the TSN predicts a maple tree with few branches, placing
leaves on them. For this reason, the crown of the output tree is barer than sketch one, while the predicted branches
are similar to the sketch ones. Another interesting behavior is shown in the second row of Figure 10. Here, the tree
branches are not well drawn, so the TSN tries to predict the tree shape based on the information provided by the trunk
and the crown. The result is a 3D mesh not very similar to the input sketch but consistent with the tree type and the
main visible elements.

Input

Reconstructed

Ground Truth

Figure 9: Results of TSN given sketches with missing regions.

Input HM

3D Mesh

Figure 10: Examples of outliers in the controlled experiment.

To avoid these outliers, we provided participants with some guidelines for drawing HM sketches. Although our work
provides a pipeline for the generation of 3D trees based on well-deﬁned parameters, one of the limitations is the small
number of tree types considered. However, this limitation could be easily overcome by considering more types in future
scenarios. Furthermore, the sketch must be considered as valid TSN input in terms of the closure of the foliage, stroke
thickness and color, the level of detail, and a white background.

7 Acknowledgments

The authors thank the NVIDIA’s Academic Research Team for providing the Titan Xp card under the Hardware
Donation Program. Furthermore, they wish to thank all of the participants in the controlled experiment for providing us
with the sketches.

8 Online Resources

We have shared our the RT Blender plugin, DNN source code, and the trained weights through a GitHub reposi-
tory: https://github.com/Unibas3D/TreeSketchNet. The authors are happy to share the training dataset upon
request by sending an email to them. Finally, we have included an illustrative video into the GitHub page.

16

Gilda Manfredi, Nicola Capece et al.

References

[1] J. Weber and J. Penn. Creation and rendering of realistic trees. In Proceedings of the 22nd Annual Conference
on Computer Graphics and Interactive Techniques, SIGGRAPH ’95, page 119–128, New York, NY, USA, 1995.
Association for Computing Machinery.

[2] K. Xie, F. Yan, A. Sharf, O. Deussen, H. Huang, and B. Chen. Tree modeling with real tree-parts examples. IEEE

Transactions on Visualization and Computer Graphics, 22(12):2608–2618, 2016.

[3] O. Deussen and B. Lintermann. Digital design of nature: computer generated plants and organics. Springer

Science & Business Media, Berlin, 2005.

[4] H. Huang, E. Kalogerakis, E. Yumer, and R. Mech. Shape synthesis from sketches via procedural models and
convolutional networks. IEEE Transactions on Visualization and Computer Graphics, 23(8):2003–2013, 2017.
[5] G. Nishida, I. Garcia-Dorado, D. G. Aliaga, B. Benes, and A. Bousseau. Interactive sketching of urban procedural

models. ACM Transactions on Graphics, 35(4):1 – 11, 2016.

[6] F. Remondino, S. Girardi, A. Rizzi, and L. Gonzo. 3d modeling of complex and detailed cultural heritage using

multi-resolution data. J. Comput. Cult. Herit., 2(1), July 2009.

[7] D. Gatziolis, J. F. Lienard, A. Vogs, and N. S. Strigul. 3d tree dimensionality assessment using photogrammetry

and small unmanned aerial vehicles. PLOS ONE, 10(9):1–21, 09 2015.

[8] T.-N. Nguyen, H.-H. Huynh, and J. Meunier. 3d reconstruction with time-of-ﬂight depth camera and multiple

mirrors. IEEE Access, 6:38106–38114, 2018.

[9] Q.-Y. Zhou and V. Koltun. Color map optimization for 3d reconstruction with consumer depth cameras. ACM

Trans. Graph., 33(4), July 2014.

[10] S. Hu, Z. Li, Z. Zhang, D. He, and M. Wimmer. Efﬁcient tree modeling from airborne lidar point clouds.

Computers & Graphics, 67:1–13, 2017.

[11] J. Tachella, Y. Altmann, N. Mellado, A. McCarthy, R. Tobin, G. S. Buller, J.-Y. Tourneret, and S. McLaughlin.
Real-time 3d reconstruction from single-photon lidar data using plug-and-play point cloud denoisers. Nature
communications, 10(1):1–6, 2019.

[12] X. Lu and X. Liu. Reconstruction of 3D Model Based on Laser Scanning, pages 317–332. Springer Berlin

Heidelberg, Berlin, Heidelberg, 2006.

[13] J. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenenbaum. Learning a probabilistic latent space of object
shapes via 3d generative-adversarial modeling. In Proceedings of the 30th International Conference on Neural
Information Processing Systems, NIPS’16, page 82–90, Red Hook, NY, USA, 2016. Curran Associates Inc.
[14] Q. Wan, Y. Li, H. Cui, and Z. Feng. 3d-mask-gan:unsupervised single-view 3d object reconstruction. In 2019 6th
International Conference on Behavioral, Economic and Socio-Cultural Computing (BESC), pages 1–6, Beijing,
China, 2019. IEEE.

[15] J. Lei, S. Sridhar, P. Guerrero, M. Sung, N. Mitra, and L. J. Guibas. Pix2surf: Learning parametric 3d surface
models of objects from images. In Proceedings of European Conference on Computer Vision (ECCV), Glasgow,
UK, August 2020. Springer International Publishing.

[16] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2mesh: Generating 3d mesh models from single
rgb images. In Proceedings of the European Conference on Computer Vision (ECCV), pages 52–67, Munich,
Germany, 2018. Springer.

[17] J. Pan, X. Han, W. Chen, J. Tang, and K. Jia. Deep mesh reconstruction from single rgb images via topology
modiﬁcation networks. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9963–
9972, Los Alamitos, CA, USA, nov 2019. IEEE Computer Society.

[18] A. Kanazawa, S. Tulsiani, A. A. Efros, and J. Malik. Learning category-speciﬁc mesh reconstruction from image
collections. In Proceedings of the European Conference on Computer Vision (ECCV), Glasgow, UK, September
2018. Springer.

[19] L. Cheng, L. Tong, Y. Chen, W. Zhang, J. Shan, Y. Liu, and M. Li. Integration of lidar data and optical multi-view
images for 3d reconstruction of building roofs. Optics and Lasers in Engineering, 51(4):493–502, 2013.
[20] J. Guo, S. Xu, D.-M. Yan, Z. Cheng, M. Jaeger, and X. Zhang. Realistic procedural plant modeling from multiple

view images. IEEE Transactions on Visualization and Computer Graphics, 26(2):1372–1384, 2020.

[21] Y. Liu, J. Guo, B. Benes, O. Deussen, X. Zhang, and H. Huang. Treepartnet: Neural decomposition of point

clouds for 3d tree reconstruction. ACM Trans. Graph., 40(6), dec 2021.

17

Gilda Manfredi, Nicola Capece et al.

[22] Sk. M. Haque, A. Chatterjee, and V. M. Govindu. High quality photometric reconstruction using a depth camera.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Columbus, OH,
USA, June 2014. Ieee.

[23] S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. Newcombe, P. Kohli, J. Shotton, S. Hodges, D. Freeman, and
A. Davison, A.and Fitzgibbon. Kinectfusion: Real-time 3d reconstruction and interaction using a moving depth
camera. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology, UIST
’11, page 559–568, New York, NY, USA, 2011. Association for Computing Machinery.

[24] M. Rutzinger, A. K. Pratihast, S. Oude Elberink, and G. Vosselman. Detection and modelling of 3d trees from

mobile laser scanning data. Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 38:520–525, 2010.

[25] O. Stava, S. Pirk, J. Kratt, B. Chen, R. M´zch, O. Deussen, and B. Benes. Inverse procedural modelling of trees.

Comput. Graph. Forum, 33(6):118–131, September 2014.

[26] J. Liu, X. Zhang, and H. Li. Sketch-based tree modeling by distribution control on planes. In Proceedings of the
9th ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in Industry, VRCAI ’10,
page 185–190, New York, NY, USA, 2010. Association for Computing Machinery.

[27] M. Okabe, S. Owada, and T. Igarashi. Interactive design of botanical trees using freehand sketches and example-
based editing. In ACM SIGGRAPH 2007 Courses, SIGGRAPH ’07, page 26–es, New York, NY, USA, 2007.
Association for Computing Machinery.

[28] G. Tan, P.and Zeng, J. Wang, S. B. Kang, and L. Quan. Image-based tree modeling. ACM Trans. Graph.,

26(3):87–es, July 2007.

[29] J. Kim and I.-K. Jeong. Single image–based 3d tree and growth models reconstruction. Etri Journal, 36(3):450–

459, 2014.

[30] K. Kingsland. Comparative analysis of digital photogrammetry software for cultural heritage. Digital Applications

in Archaeology and Cultural Heritage, 18:e00157, 2020.

[31] G. Yang, Y. Cui, S. Belongie, and B. Hariharan. Learning single-view 3d reconstruction with limited pose
supervision. In Proceedings of the European Conference on Computer Vision (ECCV), pages 86–101, Munich,
Germany, 2018. Springer.

[32] C. Häne, S. Tulsiani, and J. Malik. Hierarchical surface prediction for 3d object reconstruction.

In 2017

International Conference on 3D Vision (3DV), pages 412–420, Qingdao, China, 2017. IEEE.

[33] Y. Zhang, Z. Liu, T. Liu, B. Peng, and X. Li. Realpoint3d: An efﬁcient generation network for 3d object

reconstruction from a single image. IEEE Access, 7:57539–57549, 2019.

[34] C. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem. 3d-prnn: Generating shape primitives with recurrent neural
networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 900–909, Los Alamitos,
CA, USA, oct 2017. IEEE Computer Society.

[35] Q. Xu, W. Wang, R. Ceylan, D.and Mech, and U. Neumann. Disn: Deep implicit surface network for high-quality

single-view 3d reconstruction. 2019.

[36] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree generating networks: Efﬁcient convolutional architectures
for high-resolution 3d outputs. In Proceedings of the IEEE International Conference on Computer Vision, pages
2088–2096, Venice, Italy, 2017. Ieee.

[37] H. Fan, H. Su, and L. J. Guibas. A point set generation network for 3d object reconstruction from a single image.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 605–613, Honolulu, HI,
USA, 2017. Ieee.

[38] C. Niu, J. Li, and K. Xu. Im2struct: Recovering 3d shape structure from a single rgb image. In 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pages 4521–4529, Los Alamitos, CA, USA, jun
2018. IEEE Computer Society.

[39] H. Xie, H. Yao, X. Sun, S. Zhou, and S. Zhang. Pix2vox: Context-aware 3d reconstruction from single and
multi-view images. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2690–2698,
Los Alamitos, CA, USA, nov 2019. IEEE Computer Society.

[40] Z. Liu, K. Wu, J. Guo, Y. Wang, O. Deussen, and Z. Cheng. Single image tree reconstruction via adversarial

network. Graphical Models, 117:101–115, 2021.

[41] C. Ding and L. Liu. A survey of sketch based modeling systems. Frontiers of Computer Science, 10(6):985–999,

2016.

[42] B. Guillard, E. Remelli, P. Yvernay, and P. Fua. Sketch2mesh: Reconstructing and editing 3d shapes from sketches.

2021.

18

Gilda Manfredi, Nicola Capece et al.

[43] J. Delanoy, M. Aubry, P. Isola, A. A. Efros, and A. Bousseau. 3d sketching using multi-view deep volumetric
prediction. Proceedings of the ACM on Computer Graphics and Interactive Techniques, 1(1):1–22, 2018.
[44] L. Wang, C. Qian, J. Wang, and Y. Fang. Unsupervised learning of 3d model reconstruction from hand-drawn
sketches. In Proceedings of the 26th ACM International Conference on Multimedia, MM ’18, page 1820–1828,
New York, NY, USA, 2018. Association for Computing Machinery.

[45] C. Li, H. Pan, Y. Liu, X. Tong, A. Sheffer, and W. Wang. Bendsketch: Modeling freeform surfaces through 2d

sketching. ACM Trans. Graph., 36(4), July 2017.

[46] D. S. Ebert, F. K. Musgrave, D. Peachey, K. Perlin, and S. Worley. Texturing and Modeling: A Procedural

Approach. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 3rd edition, 2002.

[47] M. Schwarz and P. Müller. Advanced procedural modeling of architecture. ACM Trans. Graph., 34(4), July 2015.
[48] K. Haubenwallner, H.-P.r Seidel, and M. Steinberger. Shapegenetics: Using genetic algorithms for procedural

modeling. Comput. Graph. Forum, 36(2):213–223, May 2017.

[49] J. Guo, H. Jiang, B. Benes, O. Deussen, X. Zhang, D. Lischinski, and H. Huang. Inverse procedural modeling of

branching structures by inferring l-systems. ACM Trans. Graph., 39(5), jun 2020.

[50] B. Li, J. Kału˙zny, J. Klein, D. L. Michels, W. Pałubicki, B. Benes, and S. Pirk. Learning to reconstruct botanical

trees from single images. ACM Transactions on Graphics, 40(6):1 – 15, 2021.

[51] T. Fredriksson, D. I. Mattos, J. Bosch, and H. H. Olsson. Data labeling: An empirical investigation into industrial
challenges and mitigation strategies. In Maurizio Morisio, Marco Torchiano, and Andreas Jedlitschka, editors,
Product-Focused Software Process Improvement, pages 202–216, Cham, 2020. Springer International Publishing.

[52] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press, USA, 2018.
[53] X. Yang, Z. Song, I. King, and Z. Xu. A survey on deep semi-supervised learning. 2021.
[54] G. Manfredi, N. Capece, U. Erra, and M. Gruosso. Treesketchnet parameters details, 2021.
[55] J. F. Hughes. Computer graphics: principles and practice, 2014.
[56] T. Porter and T. Duff. Compositing digital images. SIGGRAPH Comput. Graph., 18(3):253–259, January 1984.
[57] N. Kanopoulos, N. Vasanthavada, and R. L. Baker. Design of an image edge detection ﬁlter using the sobel

operator. IEEE Journal of Solid-State Circuits, 23(2):358–367, 1988.

[58] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Inc., USA, 1995.
[59] M. Tan and Q. Le. EfﬁcientNet: Rethinking model scaling for convolutional neural networks. In Kamalika
Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pages 6105–6114, USA, 09–15 Jun 2019.
PMLR.

[60] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. Mobilenetv2: Inverted residuals and linear

bottlenecks. In CVPR, pages 4510–4520, Salt Lake City, UT, USA, 06 2018. IEEE.

[61] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. 2014.
[62] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li. Imagenet: A large-scale hierarchical image database. In

2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, Miami, FL, 2009. Ieee.

[63] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, USA, 2016.

http://www.

deeplearningbook.org.

[64] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.

In

International Conference on Learning Representations, San Diego, CA, USA, 2015. Ieee.

[65] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent

neural networks from overﬁtting. The journal of machine learning research, 15(1):1929–1958, 2014.

[66] J. F. Kolen and S. C. Kremer. Gradient Flow in Recurrent Nets: The Difﬁculty of Learning LongTerm Dependencies,

pages 237–243. Wiley-IEEE Press, USA, 2001.

[67] K. He and J. Sun. Convolutional neural networks at constrained time cost. In Proceedings of the IEEE conference

on computer vision and pattern recognition, pages 5353–5360, Boston, MA, USA, 2015. Ieee.

[68] N. Capece, F. Banterle, P. Cignoni, F. Ganovelli, R. Scopigno, and U. Erra. Deepﬂash: Turning a ﬂash selﬁe into a

studio portrait. Signal Processing: Image Communication, 77:28–39, 2019.

[69] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on

Computer Vision and Pattern Recognition (CVPR), pages 770–778, Las Vegas, NV, USA, 2016. Ieee.

19

Gilda Manfredi, Nicola Capece et al.

[70] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks.

Advances in neural information processing systems, 25:1097–1105, 2012.

[71] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer
vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826, Los
Alamitos, CA, USA, jun 2016. IEEE Computer Society.

[72] Z. Dai, H. Liu, Q. V. Le, and M. Tan. Coatnet: Marrying convolution and attention for all data sizes. In M. Ranzato,
A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information
Processing Systems, volume 34, pages 3965–3977, On-line, 2021. Curran Associates, Inc.

[73] A. Khan, A. Sohail, U. Zahoora, and A. S. Qureshi. A survey of the recent architectures of deep convolutional

neural networks. Artiﬁcial Intelligence Review, 53(8):5455–5516, 2020.

[74] A. Y. Ng. Feature selection, l1 vs. l2 regularization, and rotational invariance. In Proceedings of the Twenty-First
International Conference on Machine Learning, ICML ’04, page 78, New York, NY, USA, 2004. Association for
Computing Machinery.

[75] S. Salman and X. Liu. Overﬁtting mechanism and avoidance in deep neural networks. 2019.
[76] P. Cignoni, C. Rocchini, and R. Scopigno. Metro: Measuring error on simpliﬁed surfaces. Computer Graphics

Forum, 17(2):167–174, 1998.

[77] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
V Berg, and F.-F. Li. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer
Vision (IJCV), 115(3):211–252, 2015.

[78] G. Gkioxari, J. Johnson, and J. Malik. Mesh r-cnn. In 2019 IEEE/CVF International Conference on Computer

Vision (ICCV), pages 9784–9794, Seoul, Korea, 2019. Ieee.

[79] R. M. Smelik, T. Tutenel, R. Bidarra, and B. Benes. A survey on procedural modelling for virtual worlds.

Computer Graphics Forum, 33(6):31–50, 2014.

[80] J. Liu, S. Snodgrass, A. Khalifa, S. Risi, G. N. Yannakakis, and J. Togelius. Deep learning for procedural content

generation. Neural Computing and Applications, 33(1):19–37, 2021.

[81] K. Park, B. W. Mott, W. Min, K. E. Boyer, E. N. Wiebe, and J. C. Lester. Generating educational game levels with
multistep deep convolutional generative adversarial networks. In 2019 IEEE Conference on Games (CoG), pages
1–8, London, UK, 2019. Ieee.

[82] M. E. Yumer, R. Asente, P.and Mech, and L. B. Kara. Procedural modeling using autoencoder networks. In
Proceedings of the 28th Annual ACM Symposium on User Interface Software &; Technology, UIST ’15, page
109–118, New York, NY, USA, 2015. Association for Computing Machinery.

[83] T. Wang and S. Kurabayashi. Sketch2map: A game map design support system allowing quick hand sketch

prototyping. In 2020 IEEE Conference on Games (CoG), pages 596–599, Osaka, Japan, 2020. Ieee.

20

