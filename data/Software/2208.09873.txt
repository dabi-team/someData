Received: Added at production

Revised: Added at production

Accepted: Added at production

DOI: xxx/xxxx

ARTICLE TYPE

A failed proof can yield a useful test

2
2
0
2

p
e
S
4
1

]
E
S
.
s
c
[

3
v
3
7
8
9
0
.
8
0
2
2
:
v
i
X
r
a

Li Huang | Bertrand Meyer

Chair of Software Engineering,
Schaﬀhausen Institute of Technology,
Schaﬀhausen, Switzerland

Correspondence
*Li Huang, Chair of Software Engineering,
Schaﬀhausen Institute of Technology,
Rheinweg 9, 8200 Schaﬀhausen,
Switzerland. Email: li.huang@sit.org

A successful automated program proof is, in software veriﬁcation, the ultimate tri-

umph. In practice, however, the road to such success is paved with many failed proof

attempts. Unlike a failed test, which provides concrete evidence of an actual bug in

the program, a failed proof leaves the programmer in the dark. Can we instead learn

something useful from it?

The work reported here takes advantage of the rich internal information that some

automatic provers collect about the program when attempting a proof. If the proof

fails, the Proof2Test tool presented in this article uses the counterexample generated

by the prover (speciﬁcally, the SMT solver underlying the proof environment Boo-

gie, used in the AutoProof system to perform correctness proofs of contract-equipped

Eiﬀel programs) to produce a failed test, which provides the programmer with imme-

diately exploitable information to correct the program. The discussion presents the

Proof2Test tool and demonstrates the application of the ideas and tool to a collection

of representative examples.

KEYWORDS:
Program veriﬁcation, Eiﬀel, AutoProof, AutoTest, Counterexample

1

INTRODUCTION

Tests and proofs have long been considered the warring siblings of software veriﬁcation. They are characterized by dual beneﬁts
and limitations:

• Tests have, in their favor, their concreteness and relative ease of preparation. They face, however, a fundamental limitation:

successful tests, regardless of how many of them, do not guarantee the correctness of software.

• Proofs do hold that promise of correctness, when they succeed. In practice, however, the path towards a proof is arduous

and frustrating.

Can the two techniques help each other?

This article proposes a speciﬁc answer, exploiting the observation that failure has diﬀerent consequences for proofs and for

tests. Table 1 shows the diﬀerence.

In the table, - denotes the interesting cases, those in which we can conclusively deduce a useful property: proof succeeds
(great news, the program is correct), test fails (program is incorrect, we have found a bug, less triumphal news but still concrete
and useful). The other two cases, marked ,, are disappointing or of little practical value:

• A successful test does not tell us that the program is correct in the general case.

 
 
 
 
 
 
2

LI HUANG, BERTRAND MEYER

Proof

Test

Success

- Program is correct

, Program works for one more case

Failure , Inconclusive, don’t know what’s wrong

- Program is incorrect

TABLE 1 The diﬀerent meanings of failure and success for proofs and for tests

• A failed proof does not tell us whether the problem is with the program, the speciﬁcation, or the proof tool itself.

The case of a failed proof, the shaded bottom-left entry in the table, is particularly frustrating. While a mechanically-supported
program proof is the most exciting prospect in the search for program correctness, the daily practice of attempting to prove
programs is made of getting notices of proof failure and attempting to remove the cause for the failure. It is not so diﬀerent
psychologically from the practice of debugging a program, except that it is static, rather than dynamic, using only the program
text and no executions. Being static, it lacks the immediacy and concreteness of a test, which (when it fails) gives the programmer
direct examples of inputs that cause the program to miss its speciﬁcation. A failed proof just fails, giving you very little insight
as to what is actually wrong — without even guaranteeing that the program is at fault.

Such a guarantee of incorrectness does exist in the bottom-right case, a failed test. Failed tests are indeed precious, because
they exhibit important information about cases that the program misses. The downside is that the tester (a person, not a tool)
has to devise the failing test case, often a diﬃcult endeavor for a program that has already achieved a ﬁrst level of approximate
correctness. On the other hand, provers accumulate considerable knowledge about the program, including when they fail to prove
it. The work reported in this article consists of using that knowledge to go automatically from the disappointing bottom-left case
to the much more useful bottom-right case, a failed test, with its directly exploitable example of program incorrectness.

The tool presented in this article, Proof2Test1 automatically generates failing tests from failed proofs. The underlying prover
is AutoProof 1, 2, the veriﬁer for checking functional correctness of Eiﬀel 3 programs using axiomatic (Hoare-style) seman-
tics. The functional correctness of Eiﬀel programs is speciﬁed by contracts (such as pre- and postconditions, class and loop
invariants). When a proof fails, AutoProof’s back-end prover (the Z3 solver 4, used through Boogie 5, 6) will produce a coun-
terexample. Proof2Test exploits the information in the counterexample to generate a concrete test case. The generated test case
is directly exploitable and executable in AutoTest 7, 8, a testing tool for Eiﬀel programs based on run-time assertion checking.
Since AutoProof and AutoTest use the same contracts for program analysis and they are both integrated in the same develop-
ment environment (Eiﬀel IDE), executing the test produced by Proof2Test from the failed proof can reproduce an analogous test
failure in AutoTest: the execution will raise the run-time violation of the same failed contract. The general aim is to combine the
beneﬁts of both of the main types of veriﬁcation: static (proofs, performed by AutoProof) and dynamic (tests, through AutoTest).
The source code for Proof2Test and examples, as well as complementary material, are available in a GitHub repository2.
To give the reader a general understanding of the approach before introducing the theory, Section 2 illustrates an example
session of using Proof2Test. Section 3 introduces the fundamental technologies used in the Proof2Test veriﬁcation framework.
Section 4 describes the details of the implementation of Proof2Test. Section 5 demonstrates how Proof2Test helps diagnose
failed proofs. Section 6 evaluates the applicability of Proof2Test through a series of examples. After a review of related work in
Section 7, Section 8 concludes an analysis of limitations and ongoing work to overcome them.

2

AN EXAMPLE SESSION WITH PROOF2TEST

Before exploring the principles and technology behind the Proof2Test tool, we look at the practical use of the tool on a
representative example (Figure 1).

The intent of the max function in class MAX is to return into Result the maximum element of an integer array a of size
a.count. The two postcondition clauses in lines 26 and 27 (labeled result_is_maximum and result_in_array) specify this intent:
every element of the array should be less than or equal to Result; and at least one element should be equal to Result.

When we try to verify the max function using AutoProof, veriﬁcation fails and AutoProof returns the error message “Post-
condition result_is_maximum (line 26) might be violated”. Such a generic message tells us that the prover cannot establish the

1,
2https://github.com/huangl223/Proof2Test

LI HUANG, BERTRAND MEYER

3

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

class MAX
feature

max (a: SIMPLE_ARRAY [INTEGER]): INTEGER

require

local

do

a.count > 0

i: INTEGER

from

Result := a [1]; i := 2

invariant

2 ≤ i and i ≤ a.count + 1
∀ j: 1 |..|
∃ j: 1 |..|

(i − 1) | a [j] <= Result
(i − 1) | a [j] = Result

until

i = a.count

loop

if a [i] > Result then
Result := a [i]

end
i := i + 1

variant

a.count − i

end

ensure

result_is_maximum: ∀ j: 1 |..| a.count | a [j] <= Result
result_in_array: ∃ j: 1 |..| a.count | a [j] = Result

end

end

FIGURE 1 MAX is a class that ﬁnds the maximum element of an interger array a; the type of a is SIMPLE_ARRAY, an internal
class consisting of simple array features such as removing or appending array elements

postcondition, but does not enable us to ﬁnd out why. We are left with hypotheses, including: incorrect loop initialization (line
10); incorrect loop exit condition (line 16); incorrect loop body (lines 18 – 21); incorrect loop invariant (lines 12 – 14); or
limitations of the prover itself (in other words, it might be that the program is correct and AutoProof is unable to prove it).

What a programmer typically would like to see in such a case is not a general negative result, stating that the prover cannot
establish a property, but a concrete, speciﬁc result, showing that a certain input breaks the speciﬁcation. In other words, a failing
test. Producing such a test is the purpose of Proof2Test. Figure 2 shows the generated test, which includes the following sequence
of steps:

• Create an instance current_object of class MAX (line 7).

• Create an integer array a and ﬁll it with values 0 at position 1 and 8 at index 2 (lines 8 – 9).

• Call the erroneous function max on current_object with a as argument (line 10).

Running the test in AutoTest leads to a run-time failure where the same postcondition result_is_maximum is violated. To explore
the reason, we may step through the call current_object.max (a):

1. The precondition (line 5 in Figure 1) evaluates to True with a.count = 2.

2. After the loop initialization (line 10), Result = 0 and i = 2.

LI HUANG, BERTRAND MEYER

4

test_MAX_max_1
local

current_object: MAX
a: SIMPLE_ARRAY [INTEGER]
max_result: INTEGER

do

end

create current_object
create a.make (0)
a.force (0, 1); a.force (8, 2)
max_result := current_object.max (a)

1

2

3

4

5

6

7

8

9

10

11

FIGURE 2 A test case of max with input argument a[1] = 0, a[2] = 8

3. All the loop invariants (lines 12 – 14) are evaluated as True with i = 2, a.count = 2, a[1] = 0, a[2] = 8, Result = 0.

4. The exit condition of the loop (line 16) evaluates to True with a.count = 2 and i = 2, forcing the loop to terminate.

5. Postcondition result_is_maximum (line 26) evaluates to False with a [1] = 0, a [2] = 8 and Result = 0, which triggers

the debugger to report a run-time exception of postcondition violation.

Executing the generated test of Figure 1, shown above, reveals that the loop terminates too early, preventing the program from
getting to the actual maximum value, found at position 2 of the array a. To eliminate this error, it suﬃces to strengthen the exit
condition (line 16) to permit one more loop iteration: change i = a.count to i > a.count.

This example illustrates how tests and proofs can be complementary techniques: while a successful proof conclusively shows
that the program satisﬁes the given speciﬁcation, a failed proof does not by itself tell us what is wrong with the program; in that
case a test can bring the concrete evidence making it possible to proceed with the development process. The remaining sections
explain the technology behind this approach.

3

TECHNOLOGY STACK

Fundamental technologies used in Proof2Test include the Eiﬀel programming language and two analysis tools, AutoProof (static)
and AutoTest (dynamic).

Eiﬀel

3.1
The Eiﬀel object-oriented design and programming language 3, 9 natively supports the Design by Contract 10 methodology. An
Eiﬀel program consists of a set of classes. A class represents a set of run-time objects characterized by the features available on
them. Figure 3 shows an Eiﬀel class representing bank accounts, devised for demonstration purposes and intentionally seeded
with errors.

A class contains two kinds of features: attributes that represent data items associated with instances of the class,
such as balance and credit_limit; routines representing operations applicable to these instances, including make,
available_amount, deposit, withdraw, transfer. Routines are further divided into procedures (routines that have no
returned value) and functions (routines that return a value). Here, available_amount is a function returning an integer (denoted
by a special variable Result), and the other routines are procedures.

A class becomes a client of ACCOUNT by declaring one or more entities (attributes, local variables, formal arguments...) of

type ACCOUNT, as in a: ACCOUNT.

The value of an reference entity is void initially and can be attached to an object by using the create instruction or through
assignment of a non-void value. For example, create a.make(−100) creates a new object whose credit_limit is −100 and
attaches it to the entity a.

LI HUANG, BERTRAND MEYER

5

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

class ACCOUNT create

make
feature

make (limit: INTEGER)

34

35

36
-- Initialize with credit limit ‘limit’.
37

require

limit ≤ 0

do

balance := 0; credit_limit := limit

ensure

balance_set: balance = 0
credit_limit_set: credit_limit = limit

end

balance: INTEGER

-- Balance of this account.

credit_limit: INTEGER

-- Credit limit of this account.

available_amount: INTEGER

-- Amount available on this account.
do

Result := balance − credit_limit

end

deposit (amount: INTEGER)

-- Deposit ‘amount’ into this account.
do

balance := balance + amount

ensure

balance_increased: balance > old balance

end

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

withdraw (amount: INTEGER)

-- Withdraw ‘amount’ from this bank account.
require

amount ≥ 0
amount ≤ available_amount

do

balance := balance + amount

ensure

balance_set: balance = old balance − amount

end

transfer (amount: INTEGER; other: ACCOUNT)
-- Transfer ‘amount’ to ‘other’.
require

amount ≥ 0
amount ≤ available_amount

do

balance := balance − amount
other.deposit (amount)

ensure

withdrawal_made: balance = old balance −

amount

deposit_made: other.balance = old other.

balance + amount

end

invariant

available_amount ≥ 0

end

FIGURE 3 A class (with bugs) intended to implement the behavior of bank accounts

Programmers can specify the properties of classes and features by equipping them with contracts of the following kinds:

• Precondition (require): requirement that clients must satisfy whenever they call a routine; the precondition of make (line

6), for example, requires the value of limit to be negative or 0.

• Postcondition (ensure): property that the routine (the supplier) guarantees on routine exit, assuming the precondition and
termination; for example, the postcondition of deposit states that the value of balance at the exit of the routine will be
greater than its entry value (old balance).

• Class invariant (invariant): constraint that must be satisﬁed by instances of the class after object creation and after every
qualiﬁed call x.r (args) to a routine of the class; for example, the class invariant of ACCOUNT (line 58) constrains the value
of available_amount to be always non-negative.

• Loop invariant (invariant): property that the loop guarantees after initialization and every iteration; for example, the loop
invariant in max (lines 12 – 14 in Figure 1) speciﬁes what the loop has achieved (a part of the ﬁnal goal) at an intermediate
iteration.

6

LI HUANG, BERTRAND MEYER

• Loop variant (variant): an integer measure that remains non-negative and decreases at each loop iteration, ensuring that the

loop eventually terminates; the loop variant of the loop in max is a.count − i (line 23 in Figure 1).

Contracts embedded in the code allow programmers to open the way to both static analysis of the program, with AutoProof,

and dynamic analysis based on run-time assertion checking, with AutoTest.

AutoTest

3.2
AutoTest 7, 8 is an automatic contract-based testing tool: it uses the contracts present in Eiﬀel programs as test oracles and
monitors their validity during execution. A test case in AutoTest is an argument-less procedure which calls a “target routine” (the
routine to be tested) in a certain context (arguments and other objects). The context can be created manually, as in traditional
testing environments, but AutoTest can also automatically create it, using sophisticated algorithms. Figure 2 test_MAX_max_1
shows a test case with target routine max (part of the “target class” MAX). A test case usually consists of the following components:

• Declaration of input variables of the target routine.

• Creation of fresh objects for the declared variables.

• Instantiation of the objects with concrete values.

• Invocation of the target routine using the instantiated variables.

The execution of a test case in AutoTest performs run-time assertion checking for the target routine: AutoTest executes the target
routine through a qualiﬁed call 3 to the routine; during the execution of the routine call, AutoTest evaluates each of the contracts
in the routine; if any assertion is violated (evaluated to False), AutoTest terminates the execution and reports an exception of
type “contract violation”. The test leads to one of the following verdicts:

1. Passes (the execution of the test case satisﬁes all speciﬁcations).

2. Fails with a contract violation of the target routine.

3. Remains unresolved: the violation is not the target routine’s fault. The most important case in this category occurs when
AutoTest creates an input state that does not satisfy the target routine’s precondition. (AutoTest includes optimization
strategies to avoid this case, which teaches nothing about the correctness of the software, although it might suggest that
the routine’s precondition is too strict.)

AutoProof

3.3
AutoProof 1, 2 checks the correctness of Eiﬀel programs against their functional speciﬁcations (contracts). Veriﬁcation in
AutoProof covers a wide range of properties including whether:

• The postcondition of a routine holds at the time of call.

• The initialization of a loop ensures the invariant.

• That initialization leaves the variant non-negative.

• The body of a loop preserves the loop invariant (leaves it true if it was true before).

• That body leaves the variant non-negative (if it was non-negative before).

• It decreases the variant.

• A creation procedure (constructor) of a class produces an object that satisﬁes the class invariant.

• A qualiﬁed call x.f (.. .) ﬁnds the target object (the object attached to x) in a consistent state. AutoProof integrates two
mechanisms to verify class invariants: ownership 11 and semantic collaboration 12. A new approach 13, avoiding any need
for programmer annotations, is currently being implemented to replace them.

3As a reminder, a qualiﬁed call is of the form x.r(args), on a target object identiﬁed by x, as opposed to an unqualiﬁed call r(args) applying to the current object.

LI HUANG, BERTRAND MEYER

7

Boogie and Z3

3.4
When verifying an Eiﬀel program, AutoProof translates the input Eiﬀel program into a Boogie program, then relies on the
Boogie veriﬁer to transform this program into veriﬁcation conditions for the SMT solver (Z3). The underlying theory is Dijkstra’s
weakest-precondition calculus 14.

The veriﬁcation condition for a routine 𝑟 of body 𝑏, precondition 𝑃 and postcondition 𝑄, is of the form

𝑃 ⇐⇒ (𝑏 wp 𝑄)

[VC]

where 𝑏 wp 𝑄, per Dijkstra’s calculus, is the weakest precondition of 𝑏 for 𝑄, meaning the weakest possible assertion such
that 𝑏, started in a state satisfying 𝑃 , will terminate in a state satisfying 𝑄. The “weak/strong” terminology for logical formulas
refers to implication; more precisely, saying that 𝑃 ′ is weaker than or equal to 𝑃 (and 𝑃 stronger than to equal to 𝑃 ′) simply
expresses that 𝑃 ⇐⇒ 𝑃 ′. (Equivalently, if we identify 𝑃 and 𝑃 ′ with the set of states that satisfy the respective assertions, then
“𝑃 ′ is weaker than 𝑃 ” means 𝑃 ⊂ 𝑃 ′.) Then the veriﬁcation condition [VC] expresses that the precondition 𝑃 of the routine is
strong enough to guarantee that the routine’s execution will yield the postcondition 𝑄 (since 𝑃 is at least as strong as the weakest
possible assertion, 𝑏 wp 𝑄, guaranteeing this result).

If property [VC] is satisﬁed at the Boogie/Z3 level, the Eiﬀel routine is correct (with respect to 𝑃 and 𝑄).
The technique used by an SMT solver to prove a property such as [VC] is indirect: the solver tries to satisfy (the “S” in “SMT”)
a given logical formula — or to determine that such an assignment does not exist. So Z3 will work, instead of [VC], its negation
[NVC] as the formula to satisfy:

¬ (𝑃 ⇐⇒ (𝑏 wp 𝑄))

[NVC]

The proof succeeds if no variable assignment satisﬁes [NVC] (in other words, it is impossible to falsify the veriﬁcation
condition).

In the present work, we are interested in the case in which the proof does not succeed: the solver does ﬁnd a variable assignment
satisfying [NVC] and hence violating [VC]. The solver has authoritatively found that the program is buggy, by disproving [VC]:
it found a set of variable values that causes the routine not to produce a ﬁnal state satisfying 𝑄.

If the goal is not only to give the programmer a simple Succeeded/Failed notiﬁcation about the proof, but to help the program-
mer in the Failed case, it is important that such a disproof is constructive: it has actually produced a failing test case. That test
case is, however, buried in internal Z3 information and SMT-solving notation; it is the task of Proof2Test to extract the relevant
information and turn the counterexample into an actual test case that the programmer can understand, in terms of the original
programming language (Eiﬀel), and run. Section 4 will present the details of the counterexample and test generation process in
Proof2Test.

Listing 1 shows how the information appears internally (slightly simpliﬁed for the purposes of presentation). The notation
is the SMT-LIB format 15, used by Z3 and other SMT solvers. The goal is to express [NVC] on line 42, the previous lines
deﬁning the environment in the form of declarations of types or “sorts” (lines 1–2) , functions and constants4 (3 – 19), assertions
expressing typing properties (20 – 23) and assertions expressing veriﬁcation conditions (24 – 42). The ﬁnal line, check-sat,
directs the solver to check satisfaction of the conditions.

1
2
3
4
5
6
7
8
9
10
11
12
13

(declare−sort T@U 0)
(declare−sort T@T 0)
(declare−fun type (T@U) T@T)
(declare−fun type_of (T@U) T@U)
(declare−fun intType ( ) T@T)
(declare−fun TypeType ( ) T@T)
(declare−fun FieldType (T@T) T@T)
(declare−fun ACCOUNT ( ) T@U)
(declare−fun balance ( ) T@U)
(declare−fun credit_limit ( ) T@U)
(declare−fun Current ( ) T@U)
(declare−fun amount ( ) Int)
(declare−fun Heap@0 ( ) T@U)

4Constants are treated like functions with no arguments, declared, like other functions, in declare-fun clauses.

8

LI HUANG, BERTRAND MEYER

14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46

(declare−fun Heap@1 ( ) T@U)
(declare−fun Succ (T@U T@U) Bool)
(declare−fun value ( ) Int)
(declare−fun Select (T@U T@U T@U) T@U)
(declare−fun Store (T@U T@U T@U T@U) T@U)
(declare−fun fun.available_amount (T@U T@U) Int)
(assert (= (type ACCOUNT) TypeType))
(assert (= (type balance) (FieldType intType)))
(assert (= (type credit_limit) (FieldType intType)))
(assert (= (type_of Current) ACCOUNT))
(assert (let (

(available_amount (fun.available_amount Heap@0 Current))
(old_balance (Select Heap@0 Current balance))
(current_balance (Select Heap@1 Current balance))
(state_transition (Succ Heap@0 Heap@1))
(update_balance (= Heap@1 (Store Heap@0 Current balance value)))

)
(let (

(pre (and (≥ amount 0) (≤ amount available_amount)))
(wp ( ⇐⇒

(= value (+ old_balance amount))
(and

update_balance state_transition
(= current_balance (− old_balance amount))

)

)

)

)
(not ( ⇐⇒ pre wp))

)

)

)
(check−sat)

Listing 1: SMT encodings for verifying balance_set

The SMT-LIB syntax is parenthesis-based in the List style, meaning in particular that function application is in preﬁx form:
(f x) denotes the result of applying a function f to an argument x, which in more usual notation would be written f(x) . In
particular:

• ( ⇐⇒ x y) means x ⇐⇒ y (x implies y) in more usual notation.

• (let a b c) (to be read as “let a be deﬁned as b in c”) has the value of c, with every occurrence of a replaced by b.

The ﬁnal condition (not ( ⇐⇒ pre wp)) on line 42 (in more usual notation: ¬ (pre ⇐⇒ wp)) corresponds to [NVC], where
wp is the the weakest precondition. The preceding lines build up auxiliary elements culminating in this deﬁnition:

• Lines 1 – 2 deﬁne the basic sorts. “Sort” is to the modeling language, SMT-LIB, what “type” is to the programming
language being modeled; as a result, a sort can describe both values and types from that programming language. Two
fundamental sorts used in models generated by Boogie are T@U (line 1), describing values in the target programming
language, and T@T, describing types in that language. The 0 in both declarations indicate the arity (number of arguments,
here zero).

• Line 3 speciﬁes that the function type, which yields the type of a value, maps a value (T@U) to a type (T@T).

LI HUANG, BERTRAND MEYER

9

• The type_of function (line 4) relates an object reference to the object’s class. For example: Current5 is of type
ACCOUNT (line 23); amount, an integer argument of withdraw, is declared as an integer constant (line 12); balance and
credit_limit are two integer ﬁelds and deﬁned as instances of a composite type FieldType intType (lines 21 – 22).

• Modeling the execution semantics of an object-oriented program requires modeling the behavior of the heap (where the
objects are stored) through a sequence of constants preﬁxed with Heap@. In this example, there are two states during
execution of withdraw: before and after the assignment balance := balance + amount, represented by Heap@0 and
Heap@1 (lines 13 – 14). Succ (line 15) speciﬁes the relation between two successive states. The auxiliary variable value
(line 16) will contain the intermediate result of the assignment — the result of balance + amount (line 34).

• Select and Store (lines 17 – 18) are functions for retrieving and updating the values of data ﬁelds. Select H obj fd
will give the value in ﬁeld fd of object obj in heap H. Store H obj fd new is a new store obtained by replacing that
ﬁeld value by new.

• Line 32 deﬁnes the precondition pre, rephrased in SMT-LIB from the original Eiﬀel precondition clause (require).

• Lines 24 – 30 introduce auxiliary variables.

• Lines 33 – 40 deﬁne wp, as the weakest precondition of the routine balance_set for its given two-clause postcondition (36

– 37).

4

PROOF2TEST IMPLEMENTATION

Based on the technologies described in the previous section, Proof2Test automatically generates tests from failed proofs. This
section presents the overall workﬂow of Proof2Test, then the details of its implementation.

4.1 Overview
Figure 4 outlines the workﬂow of Proof2Test. The inputs are a Boogie program (ap.bpl), generated by AutoProof from the
input Eiﬀel program, and an SMT model ﬁle (ce.model), containing counterexamples from the Z3 solver. The output is a test
script (test.e) in the form of an Eiﬀel class. Proof2Test goes through three steps to construct a test case:

• 1(cid:13) Collect the relevant context information from the Boogie program, including names and types of the input arguments

of the failed routine 𝑟, as well as the attributes of the classes involved in 𝑟.

• 2(cid:13) Extract an input vector (a sequence of values of 𝑟’s input arguments) from a counterexample.

• 3(cid:13) Write a test case to the output ﬁle test.e based on the extracted context information and input vector.

FIGURE 4 Proof2Test workﬂow

5Current represents the active object in the current execution context, similar to this in Java.

10

LI HUANG, BERTRAND MEYER

A proof failure in AutoProof corresponds to a contract violation for a erroneous routine of a class, which results in a counterex-
ample. In the case of multiple proof failures, the resulting model ﬁle (ce.model) contains the same number of counterexamples.
Proof2Test parses each counterexample and generates test cases respectively.

Test generation

4.2
A counterexample is an execution trace (a sequence of program states) of the erroneous routine 𝑟, at the end of which the program
reaches a failed state (violating a contract element of 𝑟). Proof2Test parses the model to obtain the trace’s input vector, from
which it produces a test case for AutoTest (Section 3.2).

Extraction of input vector from counterexample

4.2.1
In the example of Figure 3, the routine withdraw is incorrect since its body does not ensure the postcondition balance_set (line
41). AutoProof consequently fails to prove the routine correct by obtaining, through Boogie, a counterexample from Z3. Figure
5 shows that counterexample (key parts only, slightly simpliﬁed).

1
2
3
4
5
6
7
8
9
10

amount →1
Current →T@U!val!18
Heap@0 →T@U!val!17
Heap@1 →T@U!val!22
ACCOUNT →T@U!val!6
balance →T@U!val!7
credit_limit →T@U!val!8
value →11
type_of →{

T@U!val!18 →T@U!val!6 }

11
12
13
14
15
16
17
18
19
20

Succ →{

T@U!val!17 T@U!val!22 →True }

Store →{

T@U!val!17 T@U!val!18 T@U!val!7 11→T@U!val!22 }

Select →{

T@U!val!17 T@U!val!18 T@U!val!7 →10
T@U!val!17 T@U!val!18 T@U!val!8 →(− 20)
T@U!val!22 T@U!val!18 T@U!val!7 →11}

fun.available_amount →{

T@U!val!17 T@U!val!18 →30}

FIGURE 5 Counterexample of the proof failure of balance_set

In this example the path to a contract violation goes through only two states, called Heap@0 and Heap@1 (as deﬁned in lines
3 and 4). The execution trace is not given explicitly but can be inferred from the diﬀerences between Heap@0 and Heap@1.
Formally, the counterexample in Figure 5 is a sequence of deﬁnitions of name-value associations, in the form name → value.
Basic values, other than integers, are of the form T@U!val!n denoting an abstract location n. For example, the value of amount
in the counterexample is 1 (line 1) and balance is stored at abstract location 7.

The initial and ﬁnal heaps, Heap@0 and Heap@1, are associated (lines 3 and 4) with locations 17 and 22. To infer the execution
trace, it suﬃces to look at the deﬁnitions starting with line 11, which show the diﬀerences between the contents of these two
heaps by referring to T@U!val!17 and T@U!val!22.

The preceding lines deﬁne variable values (for example, balance, credit_limit).
For a function f of 𝑛 arguments, the speciﬁcation of the function’s value in the counterexample takes the form

a1 a2

... a𝑛 → x

b1 b2

... b𝑛 → y

...

to mean that f(a1, a2,, ... a𝑛 ) = x etc. For example lines 15 to 18 give the state of both the initial heap (T@U!val!17) and the
ﬁnal heap (T@U!val!22) by specifying the Select function. (As seen in Section 3.4, Select (H, obj, fd) is the value in
ﬁeld fd of object obj in heap H.) This speciﬁcation yields the values, in either or both heaps, of the balance and credit_limit
ﬁelds (T@U!val!7 and T@U!val!8) for the Current object (T@U!val!18). Lines 13 and 14 deﬁne a state change, in the form
of an update of the Store function: change the balance ﬁeld (T@U!val!7) of the current object (T@U!val!18) in Heap@0

LI HUANG, BERTRAND MEYER

11

so that it will have the value 11 in Heap@1. In programming language terms this would be written just balance := 11 (where
balance denotes Current.balance).

To construct the input vector of withdraw from the model, it suﬃces to obtain the following information:

1. The initial state of the Current object (the target object of the qualiﬁed call), which includes the initial values of the two

data ﬁelds balance and credit_limit.

2. The value of the only argument amount.

On can obtain the value of amount directly (line 1), and the values of balance and credit_limit from their sym-
bolic addresses and the Select function (16 – 17). The resulting input vector for withdraw is: Current.balance = 10,
Current.credit_limit = − 20, amount = 1.

Construction of test cases based on input vectors

4.2.2
Once it has obtained the input vector, Proof2Test starts constructing a test case in the form of a qualiﬁed call, here
current_object.withdraw(amount). The input vector contains the values of current_object and argument amount
before the call.

1

2

3

4

5

6

7

8

9

test_ACCOUNT_withdraw_1

local

do

current_object: ACCOUNT; amount: INTEGER

create current_object.make(0)
{INTERNAL}.set_integer_field (balance, current_object, 10)
{INTERNAL}.set_integer_field (credit_limit, current_object, (− 20))
amount := 1
current_object.withdraw (amount)

10

end

FIGURE 6 Test case generated from the proof failure of withdraw

Figure 6 shows the generated test case, which includes four components:

1. Local declaration of target object (current_object) and input argument (amount) (line 3).

2. Creation of the target object (line 5).

3. Initialization of the target object and argument (lines 6 – 8). Since Eiﬀel is a strongly typed language, setting
individual ﬁelds of arbitrary objects requires using low-level library mechanisms from class INTERNAL, such as
set_integer_field.

4. Qualiﬁed call to withdraw (line 9).

The test generation process proceeds according to Algorithm 1. The result of the algorithm is the code of a test case t, which
includes three components: declaration clauses (t_d), creation clauses (t_c) and instructions instantiating variables (t_i).

The algorithm loops over all the input arguments (lines 7 – 15) of r and instantiates each of them according to their types:
arguments of primitive types (INTEGER, BOOLEAN, CHARACTER and REAL) are instantiated through direct assignments (line 10)
to the corresponding values in the input vector (for example, the integer argument amount is instantiated as amount := 1);
arguments of reference types are instantiated by applying the instantiate_reference procedure (see the details in Algorithm
2) based on the input vector. Finally, the algorithm builds a call of the target routine (lines 16 – 21) with the instantiated
arguments.

Algorithm 2 instantiates an object reference o based on the input vector v:

12

LI HUANG, BERTRAND MEYER

Algorithm 1 Generate a single test case based data collected from a counterexample

8:

9:

10:

11:

12:

13:

14:

18:

1: Input: target class c, target routine r, input vector v
2: Output: a test case t
3: t := name_of_test_case(c, r)
4: t_d := declare_variable(current_object)
5: t_c := create_reference(current_object)
6: across input arguments of r as a loop
7:

t_d := t_d + declare_variable(a)
if a is a variable of primitive type then
value := get_value(a, v)
t_i := t_i + assignment(a, value)

-- Construct the name for the test case

-- Declare and create current_object

-- Declare each argument

-- Get the value of a from v
-- Use assignments to instantiate primitive-type arguments

else

t_c := t_c + creation_of_reference(a)
t_i := t_i + instantiate_reference(a, v)

-- Create objects for reference arguments
-- Instantiate reference arguments according to v

end

15: end
16: if r is a function then
17:

res := name_of_result_variable(r)
t_d := t_d + declare_variable(res)

-- Construct a variable res to fetch r’s result

19: end
20: t := t + t_d + t_c + t_i
21: t := t + call_routine(current_object, r, res)

-- Combine the clauses of declaration, creation and instantiation
-- Call r with its arguments

• Checks whether o has an alias that has been instantiated earlier.

• If so, construct a direct assignment to the earliest alias (lines 4 – 5).

• If o is an object of a container type (such as ARRAY and SEQUENCE), instantiates each of its ﬁelds (lines 6 – 16).

• If it is instead an object of a non-container reference type, instantiate its ﬁelds transitively: assign to ﬁelds of primitive
types (such as balance and credit_limit) their corresponding values in the input vector (lines 20 – 21); for ﬁelds of
reference types, apply the procedure recursively (line 23).

5

COMBINING PROOFS AND TESTS: THE PROCESS

As noted in the introduction, tests and proofs have often been considered distinct, incompatible or competing approaches to
software veriﬁcation. The assumption behind the present work is that it is better to view them as complementary.

5.1 General description
The veriﬁcation process combining both approaches, thanks to AutoProof, AutoTest and Proof2Test, is the following:

• Step A: Attempt the veriﬁcation with AutoProof. If the veriﬁcation succeeds, the process stops. The remaining steps
assume the veriﬁcation failed. Figure 7 shows an example in which AutoProof is not able to prove some properties
(although it does succeed with some others).

• Step B: Run Proof2Test. The remaining steps assume Proof2Test is able to generate a failing test (if not, Proof2Test is not

helpful in this case).

• Step C: Run the test.

LI HUANG, BERTRAND MEYER

13

Algorithm 2 instantiate_reference: instantiate a variable of reference type

1: Input: object reference o, input vector v
2: Output: text t
3: if o has an instantiated alias then
4:

alias := get_alias_of_reference(o)
t := t + assignment(o, alias)
5:
6: elseif o is a variable of container type then
7:

size := get_size_of_container(o, v)
items := get_item_of_container(o, v)
from

i := 1

until

i ≤ size

loop

t := t + force_clause(o, items[i])
i := i + 1

end

8:

9:

10:

11:

12:

13:

14:

15:

16:

17: else
18:

across each ﬁeld of o as f loop

19:

20:

21:

22:

23:

24:

25:

if f is a variable of primitive type then
value := get_value(f, v)
t := set_field_clause(f, value)

else

t := instantiate_reference(f, v)

end

end

26: end

-- Construct an assignment o := alias

-- Get o’s size
-- Get o’s elements

-- Use force routine to insert each of o’s elements

-- Use set_type_field function to instantiate f

• Step D: use the test to attempt to correct the bug using standard testing and debugging techniques. The ﬁx can be a change

to the code or to the contract (or occasionally both).

• Repeat the process, attempting to prove the corrected code, until the proof succeeds.

FIGURE 7 Veriﬁcation result of ACCOUNT in AutoProof, with some properties proved and others not

Figure 8 illustrates the process.

14

LI HUANG, BERTRAND MEYER

FIGURE 8 Program veriﬁcation with the assistance of Proof2Test.

Running the veriﬁcation process: an example

5.2
Here is the application of the above process to the ACCOUNT class from Section 3.1.

Step A: run AugoProof on the class; Figure 7 already showed the result, which displays the AutoProof’s inability to establish
four postconditions: balance_increased in the deposit routine, balance_set in withdraw, withdrawal_made and deposit_made
in transfer. In this case, the model ﬁle (ce.model) contains four counterexample models, each of which corresponds to a
proof failure (postcondition violation).

Step B: use Proof2Test to generate test cases, shown in Figure 9, from the four counterexamples. In the order of proof failures:

• test_ACCOUNT_deposit_1 (lines 1 – 10) corresponds to deposit’s postcondition balance_increased; it ﬁrst instantiates
current_object by setting the values of the balance and credit_limit to 38 and −62 (lines 6 – 7) and then sets the
value of the input argument amount to −5 (line 8); ﬁnally it invokes a qualiﬁed call of deposit (the target routine) with
the instantiated argument (line 9).

• test_ACCOUNT_withdraw_1 (lines 12 – 21) corresponds to the proof failure of withdraw; similar to the test case of
deposit, it instantiates current_object’s two ﬁelds, balance and credit_limit, to 10 and −20, and sets the value
of amount to −1, followed by a call to withdraw.

• test_ACCOUNT_transfer_1 (lines 23 – 35) corresponds to the postcondition deposit_made of transfer; it ﬁrst instan-
tiates the state of current_object (lines 29 – 30), and then initializes the two arguments of transfer, amount (line
31) and other (line 32 – 33); it then calls transfer using the two arguments.

• test_ACCOUNT_transfer_2 (lines 37 – 47) corresponds to the proof failure of postcondition withdrawal_made
of transfer; it follows the same structure of test_ACCOUNT_transfer_1: it starts with the initialization of
current_object and arguments, followed by a call to transfer; additionally, as other and current_object are
aliases (they have the same symbolic value in the counterexample model), instead of instantiating other using the
set_type_field routines, it directly assign other with current_object (line 45).

Step C: exercise the test cases in AutoTest. Figure 10 shows the testing result: among the four

test cases,
test_ACCOUNT_transfer_1 passed and the other three test cases failed, raising the same postcondition violations as in the
veriﬁcation result (Figure 7).

Step D: to understand the causes of the contract violations, use the debugger from EiﬀelStudio to step through the three
failed test cases: test_ACCOUNT_deposit_1, test_ACCOUNT_withdraw_1 and test_ACCOUNT_transfer_2. For each test
case, move forward to the line of the respective call of the target routine and advance the execution of the target routine on one
instruction at a time to observe how the program state (values of variables) evolves. Figure 11 shows the execution traces of the
three target routines:

• deposit (Figure 11 (a)): initially the balance of the Current object is 38; after execution of the assignment, balance
is set to 33 (the sum of its previous value and amount; the ﬁnal state violates the postcondition balance_increased as
the value of balance (33) is smaller than its old value (38). The execution trace reveals a ﬂaw of deposit: depositing
negative amount is permitted. To correct this error, a precondition should be added to require that the value of the input
argument amount should be non-negative.

• withdraw (Figure 11 (b)): at the initial state, amount is 1, balance is 10, credit_limit is − 20, available_amount
(the diﬀerence between balance and credit limit) is 30; the two preconditions are satisﬁed; after executing the assign-
ment, balance is set to 11 (the sum of its previous value and amount); at the ﬁnal state, the value of old balance -

LI HUANG, BERTRAND MEYER

15

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

test_ACCOUNT_deposit_1

local

current_object: ACCOUNT; amount: INTEGER

do

end

create current_object.make (0)
{INTERNAL}.set_integer_field (balance, current_object, 38)
{INTERNAL}.set_integer_field (credit_limit, current_object, (−62))
amount := (−5)
current_object.deposit (amount)

test_ACCOUNT_withdraw_1

local

do

current_object: ACCOUNT; amount: INTEGER

create current_object.make (0)
{INTERNAL}.set_integer_field (balance, current_object, 10)
{INTERNAL}.set_integer_field (credit_limit, current_object, (− 20))
amount := 1
current_object.withdraw (amount)

end

test_ACCOUNT_transfer_1

local

current_object, other: ACCOUNT; amount: INTEGER

create current_object.make (0)
create other.make (0)
{INTERNAL}.set_integer_field (balance, current_object, (−2))
{INTERNAL}.set_integer_field (credit_limit, current_object, (−32))
amount := 6
{INTERNAL}.set_integer_field (balance, other, (−83))
{INTERNAL}.set_integer_field (credit_limit, other, (−83))
current_object.transfer (amount, other)

do

end

test_ACCOUNT_transfer_2

local

current_object, other: ACCOUNT; amount: INTEGER

do

end

create current_object.make (0)
{INTERNAL}.set_integer_field (balance, current_object, (−2))
{INTERNAL}.set_integer_field (credit_limit, current_object, (−53))
amount := 1
other := current_object
current_object.transfer (amount, other)

FIGURE 9 Test cases for ACCOUNT

16

LI HUANG, BERTRAND MEYER

FIGURE 10 Testing results in AutoTest: “PASS” means running the test satisﬁes all assertions during execution; “FAIL”
indicates the occurrence of a contract violation in the test run.

amount (9) is diﬀerent with the value of balance (11), which violates the postcondition. This indicates the apparent bug:
the routine implementation (addition operation) is inconsistent with what is expected in the postcondition (subtraction
operation). Replacing the “+” with “−” is enough to correct the bug.

• transfer (Figure 11 (c)): initially, amount = 1 and available_amount is 51, which satisﬁes the two preconditions; after
withdrawing amount from the Current account (balance := balance−amount), balance of the Current account is
set to −3; moreover, since Current and other are aliases (they refer to the same object), balance of the other account
is also set to −3; similarly, after depositing amount to the other, the balance in both other account and Current
account are updated to −2; at the end state, the postcondition withdral_made is not satisﬁed. The execution trace shows
a erroneous scenario where the Current account is transferring money to itself. To exclude this particular case from the
execution, we can add a precondition Current ≠ other to limit that other should be an account other than Current.

These failed tests become part of the regression test suite of the target class ACCOUNT.

6

EXPERIMENT AND EVALUATION

A preliminary evaluation of the usability of Proof2Test used the example programs of Table 2; some are adapted from problems
in software veriﬁcation competitions 16–18. Altogether, the experiment involved 40 editions of 9 Eiﬀel programs. It is meant to
enable measurement of the time for generating a single test case:

• Each edition results from injecting a single error into a correct program. Typical injected faults include switching between

+ and −, ≤ and <, > and ≥ and removing a clause from a loop invariant, a precondition or a postcondition.

• When verifying the modiﬁed program, AutoProof fails on a single assertion of one of the following ﬁve kinds: postcon-
dition violation, supplier precondition violation, failure to maintain a loop invariant, failure to decrease a loop variant,
failure to maintain the variant non-negative. Class invariant violations are more delicate and not handled in this version
of the work.

• From each such failure, Proof2Test generates one test case.

Each row in Table 2 corresponds to an experimental task performed on a program variant, including three procedures: 1)

veriﬁcation of the program in AutoProof; 2) generation of test case in Proof2Test; 3) exercising the test case in AutoTest. Size
𝑃
(the ﬁfth column) provide the number of lines of the program and counterexample model. The
(the second column) and Size
“Time (s)” column reports the performance of Proof2Test, in the form of the time in seconds for generating the test from the
model. The last column shows the results of running the corresponding tests in AutoTest.

𝑀

The examples include: 1) a HEATER class implementing a heater, adjusting it state (on or oﬀ) based on the current temperature
and user-deﬁned temperature; 2) the class ACCOUNT introduced in Section 3.1; 3) a class CLOCK implementing a clock counting

LI HUANG, BERTRAND MEYER

17

(a) Execution trace of deposit

(b) Execution of withdraw

(c) Execution of transfer

FIGURE 11 Debugging failed tests in EiﬀelStudio: each block displays a program state (a list of variables and their values)
after executing a designated statement (starting point of a blue line); the variable will be highlighted in red when its value is
changed after executing the statement.

seconds, minutes, and hours; 4) a class LAMP describing a lamp equipped with a switch (for switching the lamp on or oﬀ)
and a dimmer (for adjusting the light intensity of the lamp); 5) a class SUM_AND_MAX computing the maximum and the sum
of the elements in an array; 6) a class BINARY_SEARCH implementing binary search; 7) a class SQUARE_ROOT for calculating
two approximate square roots of a positive integer; 8) a class LINEAR_SEARCH implementing linear search; 9) the class MAX
presented in Section 2. Note that there can be multiple failures for one assertion, such as withdrawal_made in ACCOUNT.transfer
and hours_increased in CLOCK.increase_minutes. In these cases, the failures have diﬀerent causes, which result in diﬀerent
counterexample models and thus diﬀerent test cases.

Among the 40 total test runs, 27 runs failed and 13 runs passed. As noted earlier, even though the tests are generated from
the failed proofs, executing the tests will not necessarily lead to a contract violation. The reason the diﬀerence of semantics for
proofs and tests: proofs use modular semantics, using a routine postcondition or loop invariant and ignoring the implementation,
while testing (run-time assertion checking) applies non-modular semantics and directly goes through the implementation of
routines or loops without replacing them with speciﬁcations. In other words, a counterexample of a failed proof implies a failing
execution based on modular semantics, thus it’s not necessarily a failing execution in the perspective of non-modular semantics.
Therefore, in case of a successful test run, the proof failure might be due to the weakness in assisting speciﬁcations or veriﬁer
limitations. The latter case usually happens when proving complex properties with non-linear arithmetic, which rarely occurs in
practice for simple programs, thanks to the signiﬁcant progress of modern SMT solvers. Therefore, veriﬁcation engineers can
prioritize the speciﬁcation weakness as the reason of the failure and proceed to strengthen the speciﬁcation. In the experiment,
the 13 proof failures with passing tests are caused by too weak speciﬁcations.

7

RELATED WORK

While there exist several approaches 19–22 that make use of counterexamples to generate test cases, most of them were pro-
posed under the veriﬁcation framework of model checking of temporal logic speciﬁcations. We are only aware of one

18

LI HUANG, BERTRAND MEYER

TABLE 2 Test generation from failed proofs on a collection of examples

Class

HEATER

ACCOUNT

Size
𝑃
68

Routine
turn_on_oﬀ

97

withdraw

deposit
transfer

CLOCK

139

increase_hours
increase_minutes

increase_seconds

LAMP

78

turn_on_oﬀ

adjust_light

SUM_AND_MAX

44

sum_and_max

BINARY_SEARCH

51

binary_search

SQUARE_ROOT

43

square_root

LINEAR_SEARCH

MAX

33

39

linear_search

max_in_array

Total

597

14

Assertion label
heater_remains_oﬀ
heater_remains_on
balance_set
balance_non_negative
balance_increased
withdrawal_made
deposit_made
withdrawal_made
valid_hours
hours_increased
hours_increased
minutes_increased
valid_seconds
hours_increased
hours_unchanged
minutes_increased
lamp_is_turned_on
lamp_is_turned_oﬀ
from_high_to_low
from_high_to_low
result_is_maximum
partial_sum_and_max
max_so_far
result_is_maximum
sum_max_non_negative
not_in_upper_part
not_in_lower_part
–
present
not_in_lower_part
–
valid_result
result_so_far
present
–
max_so_far
result_is_maximum
result_in_array
result_is_maximum
–
40

Failure type
postcondition violation
postcondition violation
postcondition violation
postcondition violation
postcondition violation
postcondition violation
postcondition violation
postcondition violation
precondition violation
postcondition violation
postcondition violation
postcondition violation
precondition violation
postcondition violation
postcondition violation
postcondition violation
postcondition violation
postcondition violation
postcondition violation
postcondition violation
postcondition violation
invariant not maintained
invariant not maintained
postcondition violation
invariant not maintained
invariant not maintained
invariant not maintained
variant not decreased
postcondition violation
invariant violation on entry
variant not decreased
postcondition violation
invariant not maintained
postcondition violation
negative variant
invariant not maintained
postcondition violation
postcondition violation
postcondition violation
negative variant

Size
𝑀
649
645
578
579
573
610
637
608
571
603
651
624
573
647
649
633
654
661
636
622
1388
1171
1146
1360
1151
1097
1312
1306
1298
1188
621
723
644
967
967
1097
1065
1061
1075
1119
34159

Time (s)
1.74
0.92
0.71
0.74
1.22
0.75
1.05
0.82
0.80
0.86
1.13
0.81
1.20
0.83
0.78
0.78
0.83
1.19
0.83
0.72
1.04
1.01
1.00
1.51
1.33
1.25
0.85
1.48
0.82
1.25
0.73
0.71
0.71
0.83
1.24
0.87
0.87
0.91
0.93
0.88
38.9

Test
Fail
Fail
Fail
Fail
Fail
Pass
Pass
Fail
Fail
Fail
Pass
Pass
Fail
Pass
Pass
Pass
Fail
Fail
Fail
Fail
Pass
Fail
Fail
Fail
Fail
Fail
Fail
Fail
Pass
Fail
Fail
Pass
Fail
Pass
Fail
Fail
Pass
Pass
Fail
Fail
27/13

existing attempt 23, 24 of generating test cases from counterexamples in the context of Hoare-style veriﬁcation, which exploits
counterexamples produced by OpenJML 25 (the veriﬁcation tool for Java programs) to generate unit tests in JUnit 26 format.

In line with the objective of helping veriﬁcation engineers understand the reasons of proof failures, many approaches have
been proposed to provide a more user-friendly visualization of counterexample models: Claire et al. 6 developed the Boogie
Veriﬁcation Debugger (BVD), which can interpret a counterexample model as a static execution trace (a sequence of abstract
states). BVD has been applied in Dafny 27 and VCC 28. David et al. 29 transformed the models back into a counterexample trace
comprehensible at the original source code level (SPARK) and display the trace using comments. Similarly, Stoll 30 implemented
a tool that translates the models into programs understandable at the Viper source code level 31. Chakarov et al. 32 transformed
SMT models to a format close to the Dafny syntax. Instead of providing a static diagnosis trace, the Proof2Test approach is able

LI HUANG, BERTRAND MEYER

19

to present a “dynamic” trace, through execution of tests produced based on the models, which allows veriﬁcation engineers to
produce a program trace leading to a failing run-time state. Veriﬁcation engineers can even use a debugger to step through the
test, observing the faulty behaviors at their own pace. We ﬁnd this approach more appropriate since every software developer is
used to working with a debugger.

Another way of facilitating proof failures diagnosis is to generate of useful counterexamples: Polikarpova et al. 33 developed a
tool Boogaloo, which applied symbolic execution to generate counterexamples for failed proofs of Boogie programs. Boogaloo
displays the resulting counterexamples in the form of valuations of relevant variables, similar to the intermediate result of the
present work after the step “extraction of input data from the model” in Section 4. Likewise, Petiot et al. 34, 35 developed STADY
that produces failing tests for the failed assertions using symbolic execution techniques. Their approach is also referred to
testing-based counterexample synthesis: they ﬁrst translated the original C program into programs suitable for testing (run-time
assertion checking), and then applied symbolic execution to generate counterexamples (input of the failing tests) based on the
translated program. Unlike that approach, Proof2Test exploits the original programs, that are inherently amenable to run-time
assertion checking and thus requires no additional program transformation. Furthermore, our approach directly makes use of
the counterexample models produced by the underlying provers and no extra counterexample generation process is performed.
However, their approach is more ﬁne-grained than ours as they can distinguish between speciﬁcation weaknesses failures and
prover limitations failures.

Other facilities for diagnosing proof failures: Müller et al. 36 implemented a Visual Studio dynamic debugger plug-in for Spec#,
to reproduce a failing execution from the viewpoint of the prover. This approach creates a variation of the original program for
debugging, based on a modular veriﬁcation semantics: the eﬀect of the iteration of a loop is represented by the corresponding
values in the counterexample. Tschannen et al. 37 proposed a “two-step” approach to narrow down the reasons of proof failures:
it compares the proof failures with those of its variant where called functions are inlined and loops are unrolled, which allows
to discern failures caused by speciﬁcation weaknesses and failures resulting from inconsistency between code and contract.
However, inlining and unrolling are limited to a given number of nested calls and explicit iterations.

Some recent approaches share the present work’s vision of combining static and dynamic techniques to make veriﬁcation more
usable. Julian et al. 38 proposed a combination of AutoProof and AutoTest at a higher level, where the two tools were integrated
in the Eiﬀel IDE to avail the complementarity of proof and testing. On one hand, for those programming features currently not
supported by AutoProof (such as the code that relies on external precompiled C functions), testing can be used to check the
correctness of routines. On the other hand, proof can be used to analyze the code that can not be tested (e.g., deferred functions
that have no implementations). Collaborative veriﬁcation 39 is also based on the combination of testing and static veriﬁcation, and
on the explicit formalization of the restrictions of each tool used in the combination. The Proof2Test approach also complements
the limitations of proof techniques with testing, with a particular purpose to improve user experience in understanding proof
failures. However, it does not integrate the results of the two diﬀerent techniques.

8

CONCLUSION AND FUTURE WORK

The key assumption behind this work is that program proofs (static) and program tests (dynamic) are complementary rather
than exclusive approaches. Software veriﬁcation is a hard enough problem to justify taking advantage of all techniques that
help. Proofs bring the absolute certainties that tests lack, but are abstract and hard to get right; tests bring the concreteness
of counterexamples, immediately understandable to the programmer, and make it possible to use well-understood debugging
tools. By taking advantage of the seamless integration of AutoProof and AutoTest in the Eiﬀel method and the EiﬀelStudio
environment, Proof2Test attempts to leverage the beneﬁts of both.

From this basis, work is proceeding in various directions:

• Generate failing tests when the tests from Proof2Test are successful or non-executable.

• Make Proof2Test (currently a separate tool) a part of the EiﬀelStudio tool suite, at the same level of integration as

AutoProof and AutoTest.

• Extend the scope of the work to include the support for more intricate speciﬁcations, such as class invariants and assertions

involving ghost states.

• On the theoretical side, develop a detailed classiﬁcation of proof failures and possible ﬁx actions with correspondence to

their categories.

20

LI HUANG, BERTRAND MEYER

• On the empirical side, perform a systematic study of the beneﬁts of the proposed techniques for veriﬁcation non-experts.

As it stands, we believe that Proof2Test advances the prospect of an eﬀective approach to software veriﬁcation combining the
power of modern proving and testing techniques.

Acknowledgments Alexander Kogtenkov and Alexandr Naumchev made important contributions to the discussions leading to

this article. We are grateful to Filipp Mikoian and Manuel Oriol for useful discussions.

References

1. Tschannen J, Furia CA, Nordio M, and Polikarpova N. Autoproof: Auto-active Functional Veriﬁcation of Object-Oriented
Programs. In: International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS).
Springer; 2015. p. 566–580.

2. AutoProof;. Available from: http://comcom.csail.mit.edu/autoproof/.

3. Meyer B. Object-Oriented Software Construction. vol. 2. Prentice Hall; 1997.

4. De Moura L, and Bjørner N. Z3: An Eﬃcient SMT Solver. In: International Conference on Tools and Algorithms for the

Construction and Analysis of Systems (TACAS). Springer; 2008. p. 337–340.

5. Barnett M, Chang BYE, DeLine R, Jacobs B, and Leino KRM. Boogie: A Modular Reusable Veriﬁer for Object-Oriented
Programs. In: International Symposium on Formal Methods for Components and Objects. Springer; 2005. p. 364–387.

6. Le Goues C, Leino KRM, and Moskal M. The Boogie Veriﬁcation Debugger. In: International Conference on Software

Engineering and Formal Methods (SEFM). Springer; 2011. p. 407–414.

7. AutoTest;. Available from: https://www.eiﬀel.org/doc/eiﬀelstudio/Using_AutoTest.

8. Wei Y, Gebhardt S, Meyer B, and Oriol M. Satisfying Test Preconditions Through Guided Object Selection. In: International

Conference on Software Testing, Veriﬁcation and Validation (ICST). IEEE; 2010. p. 303–312.

9. Meyer B. Touch of Class: Learning to Program Well with Objects and Contracts. Springer; 2016.

10. Meyer B. Applying “Design by Contract”. Computer. 1992;25(10):40–51.

11. Leino KRM, and Müller P. Object Invariants in Dynamic Contexts.

In: European Conference on Object-Oriented

Programming (ECOOP). Springer; 2004. p. 491–515.

12. Polikarpova N, Tschannen J, Furia CA, and Meyer B. Flexible Invariants through Semantic Collaboration. In: International

Symposium on Formal Methods (FM). Springer; 2014. p. 514–530.

13. Meyer B, Arkadova A, and Kogtenkov A. The Concept of Class Invariant in Object-Oriented Programming. arXiv (preprint

of article submitted for publication). 2022;Available from: https://arxiv.org/abs/2109.06557.

14. Dijkstra EW. A Discipline of Programming. Prentice Hall; 1976.

15. Barrett C, Stump A, Tinelli C, et al. The SMT-LIB Standard: Version 2.0. In: International Workshop on Satisﬁability

Modulo Theories. vol. 13; 2010. p. 14.

16. Weide BW, Sitaraman M, Harton HK, Adcock B, Bucci P, Bronish D, et al. Incremental Benchmarks for Software Veriﬁ-
cation Tools and Techniques. In: Working Conference on Veriﬁed Software: Theories, Tools, and Experiments (VSTTE).
Springer; 2008. p. 84–98.

17. Bormer T, Brockschmidt M, Distefano D, et al. The COST IC0701 Veriﬁcation Competition. In: International Conference

on Formal Veriﬁcation of Object-Oriented Software (FoVeOO). Springer; 2011. p. 3–21.

18. Klebanov V, Müller P, , et al. The 1st Veriﬁed Software Competition: Experience Report. In: International Symposium on

Formal Methods (FM). Springer; 2011. p. 154–168.

LI HUANG, BERTRAND MEYER

21

19. Beyer D, Chlipala AJ, Henzinger TA, Jhala R, and Majumdar R. Generating Tests from Counterexamples. In: International

Conference on Software Engineering (ICSE). IEEE; 2004. p. 326–335.

20. Fantechi A, Gnesi S, and Maggiore A. Enhancing Test Coverage by Back-tracing Model-Checker counterexamples.

Electronic Notes in Theoretical Computer Science (ENTCS). 2005;116:199–211.

21. Black PE. Modeling and Marshaling: Making Tests from Model Checker Counterexamples. In: Digital Avionics Systems

Conference (DASC). vol. 1. IEEE; 2000. p. 1B3–1.

22. Beyer D, Dangl M, Lemberger T, and Tautschnig M. Tests from Witnesses. In: International Conference on Tests and

Proofs (TAP). Springer; 2018. p. 3–23.

23. Nilizadeh A, Calvo M, Leavens GT, and Cok DR. Generating Counterexamples in the Form of Unit Tests from Hoare-style
Veriﬁcation Attempts. In: International Conference on Formal Methods in Software Engineering (FormaliSE). IEEE; 2022.
p. 124–128.

24. Nilizadeh A, Calvo M, Leavens GT, and Le XBD. More Reliable Test Suites for Dynamic APR by Using Counterexamples.

In: International Symposium on Software Reliability Engineering (ISSRE). IEEE; 2021. p. 208 – 219.

25. Cok DR. JML and OpenJML for Java 16. In: International Workshop on Formal Techniques for Java-like Programs (FTfJP).

ACM; 2021. p. 65–67.

26. Cheon Y, and Leavens GT. A Simple and Practical Approach to Unit Testing: The JML and JUnit Way. In: European

Conference on Object-Oriented Programming (ECOOP). Springer; 2002. p. 231–255.

27. Leino KRM. Dafny: An Automatic Program Veriﬁer for Functional Correctness. In: International Conference on Logic for

Programming Artiﬁcial Intelligence and Reasoning (LPAR). Springer; 2010. p. 348–370.

28. Cohen E, Dahlweid M, Hillebrand M, Leinenbach D, Moskal M, Santen T, et al. VCC: A Practical System for Verifying
Concurrent C. In: International Conference on Theorem Proving in Higher Order Logics (TPHOLs). Springer; 2009. p.
23–42.

29. Hauzar D, Marché C, and Moy Y. Counterexamples from Proof Failures in SPARK.
Software Engineering and Formal Methods (SEFM). Springer; 2016. p. 215–233.

In: International Conference on

30. Stoll C. SMT Models for Veriﬁcation Debugging (Master thesis). ETH Zurich; 2019.

31. Müller P, Schwerhoﬀ M, and Summers AJ. Viper: A Veriﬁcation Infrastructure for Permission-based Reasoning.

In:
International Conference on Veriﬁcation, Model Checking, and Abstract Interpretation (VMCAI). Springer; 2016. p. 41–62.

32. Chakarov A, Fedchin A, Rakamarić Z, and Rungta N. Better Counterexamples for Dafny. In: International Conference on

Tools and Algorithms for the Construction and Analysis of Systems (TACAS). Springer; 2022. p. 404–411.

33. Polikarpova N, Furia CA, and West S. To Run What No One Has Run Before: Executing an Intermediate Veriﬁcation

Language. In: International Conference on Runtime Veriﬁcation (RV). Springer; 2013. p. 251–268.

34. Petiot G, Kosmatov N, Botella B, Giorgetti A, and Julliand J. How Testing Helps to Diagnose Proof Failures. Formal

Aspects of Computing (FAC). 2018;30(6):629–657.

35. Petiot G, Kosmatov N, Botella B, Giorgetti A, and Julliand J. Your Proof Fails? Testing Helps to Find the Reason. In:

International Conference on Tests and Proofs (TAP). Springer; 2016. p. 130–150.

36. Müller P, and Ruskiewicz JN. Using Debuggers to Understand Failed Veriﬁcation Attempts. In: International Symposium

on Formal Methods (FM). Springer; 2011. p. 73–87.

37. Tschannen J, Furia CA, Nordio M, and Meyer B. Program Checking with Less Hassle. In: Working Conference on Veriﬁed

Software: Theories, Tools, and Experiments. Springer; 2013. p. 149–169.

22

LI HUANG, BERTRAND MEYER

38. Tschannen J, Furia CA, Nordio M, and Meyer B. Usable Veriﬁcation of Object-Oriented Programs by Combining Static
and Dynamic Techniques. In: International Conference on Software Engineering and Formal Methods (SEFM). Springer;
2011. p. 382–398.

39. Christakis M, Müller P, and Wüstholz V. Collaborative Veriﬁcation and Testing with Explicit Assumptions. In: International

Symposium on Formal Methods (FM). Springer; 2012. p. 132–146.

