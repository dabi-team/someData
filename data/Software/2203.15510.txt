Achieving Guidance in Applied Machine Learning
through Software Engineering Techniques

Lars Reimann
G√ºnter Kniesel-W√ºnsche
reimann@cs.uni-bonn.de
gk@cs.uni-bonn.de
Smart Data Analytics, University of Bonn
Bonn, Germany

2
2
0
2

r
a

M
9
2

]
E
S
.
s
c
[

1
v
0
1
5
5
1
.
3
0
2
2
:
v
i
X
r
a

ABSTRACT
Development of machine learning (ML) applications is hard. Pro-
ducing successful applications requires, among others, being deeply
familiar with a variety of complex and quickly evolving application
programming interfaces (APIs). It is therefore critical to understand
what prevents developers from learning these APIs, using them
properly at development time, and understanding what went wrong
when it comes to debugging. We look at the (lack of) guidance that
currently used development environments and ML APIs provide
to developers of ML applications, contrast these with software en-
gineering best practices, and identify gaps in the current state of
the art. We show that current ML tools fall short of fulfilling some
basic software engineering gold standards and point out ways in
which software engineering concepts, tools and techniques need to
be extended and adapted to match the special needs of ML applica-
tion development. Our findings point out ample opportunities for
research on ML-specific software engineering.

CCS CONCEPTS
‚Ä¢ Software and its engineering ‚Üí Software verification and vali-
dation; Automated static analysis; Software libraries and repositories;
General programming languages; ‚Ä¢ Computing methodologies
‚Üí Machine learning;

KEYWORDS
machine learning, software engineering, guidance, usability, learn-
ability

ACM Reference Format:
Lars Reimann and G√ºnter Kniesel-W√ºnsche. 2020. Achieving Guidance
in Applied Machine Learning through Software Engineering Techniques.
In Companion Proceedings of the 4th International Conference on the Art,
Science, and Engineering of Programming (<Programming‚Äô20> Companion),
March 23‚Äì26, 2020, Porto, Portugal. ACM, New York, NY, USA, 6 pages.
https://doi.org/10.1145/3397537.3397552

1 INTRODUCTION
The success of deep learning and machine learning (ML) in general
in recent years is attributed to the availability of large datasets,

<Programming‚Äô20> Companion, March 23‚Äì26, 2020, Porto, Portugal
¬© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
This is the author‚Äôs version of the work. It is posted here for your personal use.
Not for redistribution. The definitive Version of Record was published in Companion
Proceedings of the 4th International Conference on the Art, Science, and Engineering
of Programming (<Programming‚Äô20> Companion), March 23‚Äì26, 2020, Porto, Portugal,
https://doi.org/10.1145/3397537.3397552.

more efficient algorithms, and specialized hardware [1]. Based on
the current hype, more and more companies try to integrate ML
into their products, leading to an ever increasing demand of data
scientists. Software developers who already know the targeted
application domain could fill this gap but typically lack a scientific
ML background. Therefore, they need guidance that helps them
correctly use ML.

We define guidance to encompass all means to facilitate or en-
force correct usage of a tool or an API or proper adherence to a
workflow. Guidance includes, on one hand, preventing, detecting,
explaining and fixing erroneous usage and, on the other hand, com-
municating best practices and helping apply them. In a traditional
software engineering (SE) context, guidance can help developers
when learning an API or the use of a tool, developing a program
using the API or tool, and debugging the program when something
went wrong.

This paper analyses how current ML tools and APIs fail to pro-
vide guidance to developers and presents ideas for improvement.
Sec. 2 reviews a typical workflow of ML application development,
points out differences to traditional software engineering, and elab-
orates the desirable guidance for each step of the ML workflow.
Sec. 3 reviews how the main APIs, languages, and development
environments currently used for ML application development fail
to provide guidance. Sec. 4 discusses how better guidance can be
offered by applying and adapting various SE concepts to the specific
requirements of ML application development.

2 ML WORKFLOWS ARE DIFFERENT
Due to space constraints, we focus without loss of generality on
supervised learning, the currently most widely-used ML workflow
[1]. The task in supervised learning is to learn a function ùêª on the
basis of some training data, which is a (typically very large) set
of examples containing features and labels. Given the features, ùêª
should ideally produce the labels. If the labels are from a continuous
set, we call it a regression problem, if the labels are from a discrete
set we call it a classification problem.

A traditional SE workflow involves activities such as require-
ment engineering, design, implementation, refactoring, and testing.
In an ML workflow, most of these activities are performed in a
quite specific way (e.g. Sec. 2.1 and 2.2), if at all1. Moreover, an
ML workflow involves additional essential activities, such as Data
Engineering (Sec. 2.3), Model Engineering (Sec. 2.4), and Model
Quality Engineering (Sec. 2.5).

1What, for instance, is the equivalent of refactoring in ML?

 
 
 
 
 
 
<Programming‚Äô20> Companion, March 23‚Äì26, 2020, Porto, Portugal

Lars Reimann and G√ºnter Kniesel-W√ºnsche

2.1 Evaluation-Focussed Requirements

Engineering

Requirements Engineering (RE) for ML reflects the fact that ML
development includes empirical evaluation of results as part of
the indispensable Model Quality Engineering activity (Sec. 2.5).
Accordingly, the definition of quality metrics and the required level
of quality for each metric is not an optional best practice but an
essential part of the workflow. One quality metric we might define
upfront, for instance, is that at least 80% accuracy must be achieved.
This gives as clear metric to assess the usefulness of a model and a
stopping criteria for the workflow.

Requirements Engineering Guidance. Desirable guidance for RE

for ML includes:

‚Ä¢ Automated suggestions of suitable quality metrics based on

problem (regression / classification) and data.

‚Ä¢ Automated checking if a comparable task has been solved

before and achieved the desired quality.

2.2 Test-Driven Development the ML Way
Before we can start learning, we must first ensure that a large
amount of data is available and split it into a

‚Ä¢ a training set that will be used for the actual learning / train-

ing (Sec. 2.4),

‚Ä¢ a validation set that will be used for assessing whether the
training results are good enough and to fine-tune the model2
(Sec. 2.5).

‚Ä¢ a test set, that will be used to get a last measure of how well
the model performs on real-world data before pushing the
model into production (Sec. 2.5).

Test-Driven Development Guidance. Desirable guidance for Test-

Driven Development in ML includes:

‚Ä¢ Automated suggestion whether to use cross-validation or a
specific validation set based on desired quality, training time
and the amount of data available.

‚Ä¢ Automated suggestion of what percentage of the data should

go into each of the sets.

‚Ä¢ Automated verification of best practices for a ‚Äògood‚Äô split of
the data. For instance, the distribution of labels and signifi-
cant features in the three sets should be as similar as possible
(stratified sampling [5]).

‚Ä¢ Automatically hiding the test set away until needed, so it
does not influence decisions in subsequent steps of the work-
flow like the choice of the model [1].

2.3 Data Engineering
Next, we must ensure that the available training data is of sufficient
quality. If not, we need to perform data engineering, which involves

‚Ä¢ handling of missing values,
‚Ä¢ turning categorical into numerical values,
‚Ä¢ normalization of numerical values,
‚Ä¢ assessing the statistical relevance of available features,
‚Ä¢ selecting the features used for learning, possibly combining

several features.

Data engineering is typically the most time consuming step in an
ML project but also the most important one: If the data used for
learning is of poor quality, so will be the learned function ùêª .

Data Engineering Guidance. Desirable guidance for data engi-

neering includes:

‚Ä¢ Automated detection of missing values and automated sug-
gestions for handling them, ideally specific to the type of
data or learning task at hand. For instance, for numeric val-
ues, missing values could be substituted by the mean of the
other values. This is not possible for a feature that represents
street names.

‚Ä¢ Automatic conversion of categorical into numerical values,
or at least suggestions for possible conversions, again taking
into account the specific data types and application seman-
tics. For instance, if categorical values can be ordered (e.g.
‚Äòpoor‚Äô, ‚Äòmiddle class‚Äô, ‚Äòrich‚Äô, ‚Äòbillionaire‚Äô), we can replace them
by an increasing sequence of numbers (label encoding). Oth-
erwise, we can perform a one-hot encoding, which maps each
different value to a sparse vector [5].

‚Ä¢ Automated normalization of numerical values.
‚Ä¢ Automated computation of the entropy of the distribution of
a feature or of any other measure for the statistical relevance
of the available features.

‚Ä¢ Automated suggestions for sensible feature combinations in

a specific application domain.

‚Ä¢ Ideally, any data processing step performed on the training
data, should be automatically applied also to the validation
and test set in order to avoid inconsistencies or tedious and
error-prone manual application of each step on each dataset.

2.4 Model Engineering
The next step in the ML workflow is the selection of an existing
ML API and the choice of a supported learning algorithm, such as
decision trees or neural networks (NNs). At first glance, this might
appear similar to the choice of proper algorithms and data structures
in SE. However, it was shown by Wolpert in [12] that lacking specific
assumptions no learning algorithm performs better than any other.
This makes it impossible to provide a general guideline to always
use, say NNs.

Choosing a learning algorithm instead requires significant back-
ground in ML and a careful consideration of the application goal3
and the properties of the data available after data engineering. Even
with this knowledge, though, the final choice of the proper learn-
ing algorithm is typically the outcome of a lot of experimentation,
involving several iterations of the entire ML development workflow.
Given a choice of an algorithm, a developer then writes a training
program and starts the learning process on the prepared training
data. Depending on the circumstances (hardware, data size, learning
algorithm, etc.) this step can take a few seconds or several days. Its
outcome is a model representing the learned function ùêª . It is worth
noting that, unlike in SE, the learned model is the final output of
the workflow, not the written program.

Model Engineering Guidance. Desirable guidance for model engi-

neering includes:

2Alternatively, we can use cross-validation [1].

3Do we need explainability?

Achieving Guidance in Applied Machine Learning

<Programming‚Äô20> Companion, March 23‚Äì26, 2020, Porto, Portugal

‚Ä¢ Automated suggestions of suitable learning algorithms based

on the available data and the application domain.

‚Ä¢ Well-documented APIs and all kinds of teaching and training

material that help quickly learn how to use them.

‚Ä¢ Automated checking of correct API use, ideally during pro-
gram development (static checking) but at least at runtime.
Given that a single training run can take hours and days
and the entire workflow involves many iterations, the impor-
tance of static checking cannot be overstated. Costs incurred
in statically detecting and fixing errors in the training pro-
gram that would be prohibitive in traditional SE appear small
in an ML context.

‚Ä¢ Help in understanding and fixing errors, ranging from sensi-
ble error messages, ideally enriched with suggestions how
to resolve the error, to automated quickfixes.

2.5 Model Quality Engineering
Next, the performance of the model is evaluated by running it
on the validation set (Sec. 2.2), to get a measure of how well the
model performs on data not seen during training. By measuring
the metrics established in the RE phase (Sec. 2.1) we check that the
model learned the characteristics of the training data well enough
(no underfitting) and did not simply learn the training data by heart
(no overfitting). The gathered metrics are used to fine-tune the
training program by setting different hyperparameters that control
the behaviour of the learning algorithm. For example, the maximum
number of nodes in a decision tree can be a hyperparameter. A
low value can lead to underfitting, while a high value can lead to
overfitting. With the improved settings, a new training is started
and the entire process is repeated from there. If the results are still
not satisfactory, one might go further back to select a different
learning algorithm (Sec. 2.4) or even rethink and enhance data
engineering (Sec. 2.3).

Finally, when the model has passed the validation stage, we use
the test set (Sec. 2.2) to get a last measure of how well the model
performs on unseen data before pushing the model into production.
We cannot use the validation set for this, since we selected the
model that works best on the validation set, which introduces a
risk of overfitting [5].

Model Quality Engineering Guidance. Desirable guidance for qual-

ity engineering includes:

‚Ä¢ Automated experiment management, that is, saving of the ùëõ
models that achieved the highest performance values during
validation along with the related training program and fully
data-engineered datasets (including training, validation and
test data) in order to support reproducibility of results.
‚Ä¢ Automated detection of overfitting and related suggestions
how to combat it, e.g. by inserting drop-out layers [7] in a
neural network.

‚Ä¢ Automated detection of underfitting and related suggestions
how to combat it, e.g. by choosing a more powerful learning
algorithm.

3 STATE OF THE ART
The highly explorative and iterative workflow described in Sec. 2 is
supported by high-level APIs for ML (Sec. 3.1), dynamic languages

(Sec. 3.2), and dynamic development environments (Sec. 3.3). In this
section, we review each of them with respect to the guidance they
provide.

3.1 Efficient, High-Level APIs
Based on a survey of ML APIs by Nguyen and others from 2019
[8] we investigated scikit-learn [10], a representative of an API for
‚Äòclassical‚Äô ML, as well as Keras [2] and PyTorch [9], which are APIs
for deep learning. These APIs provide high-level abstractions to
apply and adapt learning algorithms (model engineering), to ensure
sufficient prediction quality (model quality engineering) and even
to perform data engineering, thereby providing all functionality
needed to rapidly iterate the ML workflow from Sec. 2. With respect
to guidance, though, we identified several shortcomings:

Criticism: Hidden and Inconsistent Constraints in the API Documen-
tation. The broadness of these APIs makes it difficult to use them
correctly, especially since many constraints are hard to understand
from the documentation. By looking just at the documentation
of the support vector machine learning algorithm for classifica-
tion (SVC) in scikit-learn4 we can already identify four different
categories of constraints documented only in natural language or
documented inconsistently:

‚Ä¢ type constraints limit values of expressions in any context,
‚Ä¢ dependencies limit values depending on other values,
‚Ä¢ temporal constraints limit the order of operations,
‚Ä¢ execution context constraints express restrictions specific to
the used programming language or executions platform.

Figure 1: Type constraints (scikit-learn)

Hidden and inconsistent type constraints are illustrated in Fig. 1:
According to the first line of the documentation, the constructor
parameter kernel of the SVC must be of type string. However,
the rest of the text (see highlighting), reveals that only the five
string literals ‚Äòlinear‚Äô, ‚Äòpoly‚Äô, ‚Äòrbf‚Äô, ‚Äòsigmoid‚Äô and ‚Äòprecomputed‚Äô are
allowed and that we can additionally pass a callable, thus something
that is not a string. Reading on, we learn that the signature of the
callable is further restricted: It must have a single parameter that
must be an array of shape n_samples by n_samples.

Figure 2: Dependent API elements (scikit-learn)

A dependency constraint is shown in Fig. 2: The value of the
constructor parameter degree is only relevant if the constructor
parameter kernel has the value ‚Äòpoly‚Äô.

4https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html

<Programming‚Äô20> Companion, March 23‚Äì26, 2020, Porto, Portugal

Lars Reimann and G√ºnter Kniesel-W√ºnsche

Figure 3: Temporal constraint (scikit-learn)

A temporal constraint is shown in Fig. 3: We must set the attribute
probability to ‚ÄòTrue‚Äô prior to the first call of the fit method
(which starts the learning process).

Figure 4: Constraint of execution context (scikit-learn)

An execution context constraint is shown in Fig. 4: Verbose output

may not work in a multithreaded environment.

Criticism: Lack of Constraint Checking and Helpful Error Messages.
Let us now investigate what happens if we run a program that does
not comply with some of the restrictions shown above. We begin
with the simple program in List. 1. There, we create an SVC with the
invalid kernel ‚Äòline‚Äô (see Fig. 1) ‚Äî the programmer probably meant
‚Äôlinear‚Äô. But despite the wrong value, the code executes without
error. The programmer therefore adds more code to preprocess the
data and initiate training, which leads to the program state in List.
2. Executing this program throws the error shown in List. 3:

Listing 1: Incorrect, but throws no error (scikit-learn)

Listing 2: Throws a misleading error (scikit-learn)

Listing 3: Error thrown by program in List. 2 (scikit-learn)

There are multiple issues with this behaviour:

‚Ä¢ No precondition checking in API function. First, the error is
not thrown by the API at the place where the illegal value
was set, misleading the programmer to think that the first
two lines are correct.

‚Ä¢ Misleading, generic error messages. A generic error is thrown
by the runtime system when stumbling over the illegal value,
which is much too late. At this point, the error message and

shown stacktrace further mislead the programmer. Since
lines 3 and 4 contain Python lists that indeed do not contain
the string ‚Äòline‚Äô, the programmer could try to find a bug there.
Going back to line 2, which apparently executed flawlessly,
requires deep internal knowledge of the API, which is un-
likely for anybody except its authors. One could argue that
the string literal ‚Äòline‚Äô shows up in line 2, which could point
the programmer to the true issue. But note that the code
for data preprocessing is normally much longer than shown
here, so the line where the error originated might not be
visible in the context where it manifests itself.

‚Ä¢ No hint how to recover. Finally, once the programmer has
found the true bug, he must consult the documentation to
find the correct value since the generic error message does
not specify the valid inputs.

Criticism: No Checking of ML Best Practices. We only looked at
constraints of the API so far. For general ML best practices (Sec.
2), the situation is even more dire: For them none of the APIs pro-
vides any checking or feedback. In addition, due to the differences
between the APIs, each one would have to implement checking of
these best practices separately, even though they apply regardless
of the API that is used.

Criticism: No Static Checking. In either case, even the best docu-
mentation and ideal runtime error checking still requires developers
to wait for (possibly long) training runs just to find out that an error
occured. The effect is an annoying loop of editing, running, debug-
ging, fixing and re-running the training program in order to see if
the fix was successful ‚Äî the runtime acts like a test oracle. Ideally,
the developer would get guidance while he writes the program but
this is poorly supported in the programming languages used for
ML development, which brings us to the next point.

3.2 Languages
The above-mentioned study by Nguyen, et al. from 2019 [8] found
that Python is the most popular language for ML. This can be at-
tributed to its readability and to the fact that it is an interpreted lan-
guage and, thus, well suited for highly interactive and explorative
development. Moreover, Python has a vast ecosystem of scientific
APIs that are useful for ML. Being a general-purpose programming
language, Python provides the flexibility needed to implement new
learning algorithms and highly customized ML-workflows. How-
ever, much of this flexibility ‚Äî and therefore complexity of the
language ‚Äî is not needed if one is just using the functionality of an
API for data, model and quality engineering to accomplish standard
tasks.

Criticism: Static Checking is Hard. Because Python is such a
dynamic, general-purpose programming language, it is difficult to
statically check the constraints of an API. Since the 2015 release of
version 3.5, Python at least supports optional type hints, defined
in PEP 484 [11]. Still, the authors stress that ‚ÄúPython will remain
a dynamically typed language, and the authors have no desire to
ever make type hints mandatory, even by convention‚Äù. Thus, taking
advantage of type hints requires the use of an external tool such as

Achieving Guidance in Applied Machine Learning

<Programming‚Äô20> Companion, March 23‚Äì26, 2020, Porto, Portugal

the static type checker mypy5. However, of the three ML APIs we
observed, only the most recent one, PyTorch, includes type hints.
In addition, dependencies, temporal constraints, execution context
constraints and ML best practices are beyond the scope of type
hints. Even a simple constraint such as "the value must be a float
between 0 and 1" cannot be expressed.

The impact of static type checking was quantified (among others)
in a study by Fischer and Hanenberg from 2015 [4], where they
found the development time of a program in the statically typed
JavaScript-superset TypeScript6 to be significantly shorter than
in the dynamically typed JavaScript itself. We argue that static
checking of non-type errors has a similar positive effect.

3.3 Development Environments
Last but not least, notebooks for Python and other languages, e.g.
Jupyter Notebooks7, have become a popular learning and develop-
ment environment for ML and have recently been integrated into
the generic Python IDE PyCharm8.

In a notebook, a program can be split into cells that can be eval-
uated independently without deleting the results created by other
cells. Results, including complex visualizations, are shown directly
in the notebook next to the cell. It is possible to include text cells
between code cells for documentation purposes and to structure
the notebook. This feature also makes notebooks ideal for creating
executable teaching materials for novices. Due to their interactivity,
they are used in practice as a development environment for ML,
especially for data exploration. We claim that the popularity of
notebooks, even when used within PyCharm, points to the lack of
IDEs that are adapted to the needs of ML development.

Criticism: No Support for Data Engineering. Notebooks and generic
Python IDEs such as PyCharm (henceforth called altogether ‚ÄòML
tools‚Äô) provide no specific support for data engineering. There are
no built-in automated analyses of a dataset, as suggested in Sec. 2.3,
or visualizations of them. Developers must rely on external tools
or the functionality provided through APIs.

Criticism: No Support for Introspection. While developing a train-
ing program, developers need to inspect the current state of data
or models. There is no support for this in current ML tools. Instead,
developers need to call specific API functions that provide intro-
spection support. Keras, for example, has a summary method that
displays the layers of an NN, the shape of their outputs and the
number of trainable parameters, which is a good measure of how
long training is going to take and whether the model is likely to
overfit or underfit. Fig. 5 shows an example output of this method.
Although introspection is not conceptually part of the learning
program, we must mix the two concerns in the notebook environ-
ment. A dedicated ML IDE could provide a clearer separation of
concerns.

Criticism: No Experiment Management. For experiment manage-
ment developers must also invent their own solutions or choose
external tools. A dedicated IDE could fulfill this need instead and

5http://mypy-lang.org/
6https://www.typescriptlang.org
7https://jupyter.org/
8https://www.jetbrains.com/pycharm/

Figure 5: Output of the summary method (Keras)

also improve the discoverability of past experiments by automati-
cally comparing the current task and available data to a database,
thereby preventing duplicate work.

4 THE SIMPLE-ML APPROACH
The Simple-ML project9 is dedicated to the development of solutions
for the above-mentioned gaps in the state of the art. Its core ideas,
summarized in Fig. 6, are: (1) the development of a unified ML API,
(2) the development of a domain specific language (DSL) for ML, (3)
the development of a dedicated ML IDE, and (4) last but not least,
as a common basis for (2) and (3), the development of metamodels
of the ML domain, ML APIs, and application-specific datasets.

Figure 6: Our Simple-ML Approach

Towards a Unified ML API. In its first release, the unified ML API
will generalize the functionality of scikit-learn, Keras and PyTorch.
Development of an own API lets us (a) ensure the quality of its
documentation, and (b) ensure the existence of runtime checks
where no static checking is possible. This enables checking of com-
pliance with general, API-independent ML best practices once and
for all. The unified API will be based on adapters to the different
existing APIs and will shield the higher levels from their details. In
particular, it will serve as a unified target for code generation by
the compiler of the DSL.

A DSL with ML-aware Static Analysis Capabilities. The Simple-
ML DSL, will provide ML-specific abstractions as first-class lan-
guage elements, and will make them available via a textual and a
visual syntax. This will make it easy to learn by developers who
want to experiment with ML but are not experienced programmers.

9https://simple-ml.de/

<Programming‚Äô20> Companion, March 23‚Äì26, 2020, Porto, Portugal

Lars Reimann and G√ºnter Kniesel-W√ºnsche

static analysis, meta-modeling, and support by an IDE with an ad-
vanced GUI as key SE concepts, whose combined use could provide
the missing guidance. Finally, we sketched the Simple-ML project,
which will implement this solution in order make ML available as
a well-understood tool for software developers.

ACKNOWLEDGMENTS
This work was partially funded by the Federal Ministry of Education
and Research (BMBF), Germany under Simple-ML (01IS18054).

REFERENCES
[1] Francois Chollet. 2017. Deep Learning with Python (1st ed.). Manning Publications

Co., USA.

[2] Fran√ßois Chollet and Others. 2015. Keras. https://keras.io.
[3] Diego Esteves, Diego Moussallem, Ciro Baron Neto, Tommaso Soru, Ricardo
Usbeck, Markus Ackermann, and Jens Lehmann. 2015. MEX Vocabulary: A
Lightweight Interchange Format for Machine Learning Experiments. In Pro-
ceedings of the 11th International Conference on Semantic Systems (SEMANTICS
‚Äô15). Association for Computing Machinery, New York, NY, USA, 169‚Äî-176.
https://doi.org/10.1145/2814864.2814883

[4] Lars Fischer and Stefan Hanenberg. 2015. An empirical investigation of the effects
of type systems and code completion on API usability using TypeScript and
JavaScript in MS visual studio. In DLS 2015 - Proceedings of the 11th Symposium
on Dynamic Languages. Association for Computing Machinery, Inc, 154‚Äì167.
https://doi.org/10.1145/2816707.2816720

[5] Aurelien Geron. 2017. Hands-on machine learning with Scikit-Learn and Tensor-

Flow (1st ed.). O‚ÄôReilly.

[6] Simon Gottschalk, Nicolas Tempelmeier, G√ºnter Kniesel, Vasileios Iosifidis,
Besnik Fetahu, and Elena Demidova. 2019. Simple-ML: Towards a Framework for
Semantic Data Analytics Workflows. In Semantic Systems. The Power of AI and
Knowledge Graphs, Maribel Acosta, Philippe Cudr√©-Mauroux, Maria Maleshkova,
Tassilo Pellegrini, Harald Sack, and York Sure-Vetter (Eds.). Springer International
Publishing, Cham, 359‚Äì366.

[7] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of
feature detectors. CoRR abs/1207.0580 (2012). arXiv:1207.0580 http://arxiv.org/
abs/1207.0580

[8] Giang Nguyen, Stefan Dlugolinsky, Martin Bob√°k, Viet Tran, √Ålvaro L√≥pez Garc√≠a,
Ignacio Heredia, Peter Mal√≠k, and Ladislav Hluch√Ω. 2019. Machine Learning and
Deep Learning frameworks and libraries for large-scale data mining: a survey.
Artificial Intelligence Review 52, 1 (jun 2019), 77‚Äì124. https://doi.org/10.1007/
s10462-018-09679-z

[9] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-
maison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning
Library. In Advances in Neural Information Processing Systems 32, H Wallach,
H Larochelle, A Beygelzimer, F d Alch√©-Buc, E Fox, and R Garnett (Eds.). Curran
Associates, Inc., 8024‚Äì8035. http://papers.neurips.cc/paper/9015-pytorch-an-
imperative-style-high-performance-deep-learning-library.pdf

[10] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau,
Matthieu Brucher, Matthieu Perrot, and √âdouard Duchesnay. 2011. Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research 12 (oct 2011),
2825‚Äì2830. arXiv:1201.0490

[11] Guido van Rossum, Jukka Lehtosalo, and ≈Åukasz Langa. 2015. PEP 484 ‚Äì Type

Hints | Python.org. https://www.python.org/dev/peps/pep-0484

[12] David H. Wolpert. 1996. The Lack of a Priori Distinctions between Learning
Algorithms. Neural Computation 8, 7 (oct 1996), 1341‚Äì1390. https://doi.org/10.
1162/neco.1996.8.7.1341

In addition, the DSL will statically catch errors related to all the
types of constraints identified in Sec. 3.1.

Under the hood the DSL compiler will generate code for a general
purpose programming language such as Python, using the imple-
mented adapters to target an individual framework. This avoids
locking users into the Simple-ML DSL. If they eventually need the
flexibility of a general purpose programming language, they can
take the generated code and modify it as desired.

Using Ontologies to Store Metadata. ML-specific error detection
will be done based on metadata in the form of ontologies. We
will create ontologies that describe ML concepts, such as learning
algorithms and quality metrics, to capture information like the
hyperparameters of learning algorithms or whether a learning
algorithm and a quality metric are compatible. For this we consider
improving the MEX ontology [3].

The ML-specific metadata will be complemented by metadata
about available datasets. Each dataset will be associated with a
semantic description of its features [6]. For each feature, we will
store metainformation like its semantic category (e.g. the fact that
it represents the name of a person), or the associated unit of mea-
surement (e.g. whether a distance is measured in metres or miles).
We will also store general statistical properties of a feature, like
minimum and maximum values, standard deviation etc.

Using this metadata we can provide the desirable data engineer-
ing guidance identified in Sec. 2.3. For instance, we will be able to
quickly detect if data is normalized and normalize it automatically
if desired, or provide targeted suggestions for feature combinations
based on the semantic categories. For model engineering (Sec. 2.4)
we can also provide guidance, e.g. by suggesting suitable models
given the data and the task. Ontologies can also provide the techni-
cal basis for model quality engineering guidance (Sec. 2.5).

An IDE to Tie Everything Together. Further guidance will be pro-
vided by an ML-specific IDE that will support the entire ML work-
flow (Sec. 2) through GUI features. The error checking and fixing
capabilities of the DSL will be used to display errors and to pro-
vide quickfixes while the training program is being written. The
ontology-based metadata will be used for elaborate suggestions of
preprocessing steps during data engineering (Sec. 2.3), for sugges-
tions of suitable models during model engineering (Sec. 2.4), and for
experiment management and suggestions of model improvements
during quality engineering (Sec. 2.5). The GUI of the IDE will also
provide introspection (Sec. 3.3) capabilities, allowing us to eliminate
introspection from the user-visible part of the unified ML API, thus
making it easier to learn.

5 CONCLUSION
This paper introduced the concept of guidance as the means offered
by an API or tool to facilitate or enforce correct usage or to help
follow a predetermined workflow or established best practices.
Guidance speeds up learning, development and debugging.

We discussed key differences between ML and SE workflows,
derived the desirable ML-specific guidance and analyzed how exist-
ing APIs, languages and tools used for ML application development
fail to provide most of it. As a possible solution, we identified

