Boundary Adaptive Local Polynomial Conditional
Density Estimators ∗

Matias D. Cattaneo†

Michael Jansson‡

Rajita Chandak†
Xinwei Ma§

April 25, 2022

Abstract

We begin by introducing a class of conditional density estimators based on local
polynomial techniques. The estimators are automatically boundary adaptive and easy
to implement. We then study the (pointwise and) uniform statistical properties of
the estimators, oﬀering nonasymptotic characterizations of both probability concen-
tration and distributional approximation. In particular, we establish optimal uniform
convergence rate in probability and valid Gaussian distributional approximations for
the t-statistic process indexed over the data support. We also discuss implementation
issues such as consistent estimation of the covariance function of the Gaussian approx-
imation, optimal integrated mean squared error bandwidth selection, and valid robust
bias-corrected inference. We illustrate the applicability of our results by constructing
valid conﬁdence bands and hypothesis tests for both parametric speciﬁcation and shape
constraints, explicitly characterizing their nonasymptotic approximation probability er-
rors. A companion R software package implementing our main results is provided.

Keywords: Conditional distribution estimation, local polynomial methods, strong approxi-
mations, uniform inference, conﬁdence bands, speciﬁcation testing.

2
2
0
2

r
p
A
1
2

]
T
S
.
h
t
a
m

[

1
v
9
5
3
0
1
.
4
0
2
2
:
v
i
X
r
a

∗We thank Jianqing Fan, Jason Klusowski and William Underwood for comments. Cattaneo gratefully
acknowledges ﬁnancial support from the National Science Foundation through grant SES-1947805 and from
the National Institute of Health (R01 GM072611-16), and Jansson gratefully acknowledges ﬁnancial support
from the National Science Foundation through grant SES-1947662 and the research support of CREATES.
A companion R package is available at https://nppackages.github.io/lpcde/.

†Department of Operations Research and Financial Engineering, Princeton University.
‡Department of Economics, UC Berkeley and CREATES.
§Department of Economics, UC San Diego.

 
 
 
 
 
 
1

Introduction

,

y2, xT
2

y1, xT
1
q
p
X , where Y

Suppose that
on Y
be the conditional
cumulative distribution function (CDF) of yi given xi, important parameters of interest
in statistics, econometrics, and many other data science disciplines, are the conditional

is a random sample from a distribution supported
y

Rd are compact. Letting F

, . . . ,
q
R and X

yn, xT
n q

x
q
|

Ă

Ă

ˆ

p

p

p

probability density function (PDF) and derivatives thereof:

f pϑq

y

p

x
|

q “

1`ϑ

y1`ϑ F
B
B
f p0q

y

x
q
|

p

where, in particular, f

y

x
|

p

q “

y

,

x
q
|

p

N0

ϑ

P

0, 1, 2, . . .

“ t

,
u

is the conditional Lebesgue density of yi given xi.

Estimation and inference methodology for (conditional) PDFs has a long tradition in

statistics (e.g., Wand and Jones, 1995; Wasserman, 2006; Simonoﬀ, 2012; Scott, 2015, and
references therein). Unfortunately, without speciﬁc modiﬁcations, smoothing methods em-

ˆ

ploying kernel, series, or other local approximation techniques are invalid at or near boundary
points of Y
X . This problem complicates construction of estimation and inference proce-
dures that are valid uniformly over the support of the data. To address this challenge, we
introduce a boundary adaptive and closed-form nonparametric estimator of f pϑq
based
on local polynomial techniques (Fan and Gijbels, 1996) and provide an array of distributional
X . In particular, we
approximation results that are valid (pointwise and) uniformly over Y
obtain a uniformly valid stochastic linear representation for the estimator and develop uni-
form inference methods based on strong approximation techniques leading to, for example,
asymptotically valid conﬁdence bands and speciﬁcation testing methods for f pϑq
precise nonasymptotic characterization of their associated approximation errors.

x
q
|

x
q
|

with

ˆ

y

y

p

p

F

y

x
q
|

p

f pϑq

To motivate our proposed estimation approach, suppose ﬁrst that an estimator

F

is available. Then, a natural estimator of f pϑq

y

x
q
|

p

is the local polynomial estimator

y

x
q
|

p

of

p

n

F

y

x
|

p

q “

eT
1`ϑ

β

y

x
q
|

p

,

β

y

x
|

q “

argmin
uPRp`1

p

yi

x
|

p

q ´

yi

p
p

´

y

Kh

yi; y

,

p

q

1

p

p

ě

`

i“1
´
ÿ
p
p
T, el is the conformable
1!, u2
2!, . . . , up
1, u
where p
{
{
h for some kernel function K and some
h
y
l
1
p
q{
´
positive bandwidth h. By virtue of being based on a local (polynomial) smoothing approach
to approximating derivatives with respect to y, the estimator
is well deﬁned even
is not continuous in y. In this paper, we employ the following q-th order local
if

ϑ is the order of p
p
q “

-th unit vector, and Kh

q “ p
u
K
pp

p!
q
{

x
q
|

f pϑq

u; y

q{

`

F

u

y

y

p

q

p

2

Tu
¯

q

x
q
|

p

p

p

2

polynomial regression estimator of F

y

:

x
q
|

p

n

F

y

x
|

eT
0

γ

y

,

x
q
|

γ

y

x
|

argmin
uPRqd`1

yi

y

p

p

p

p

p

p

ď

q ´

q “

q “

i“1
ÿ

1p
`
where, using standard multi-index notation, q
u
p
q
lecting the ordered elements uν
ν
| ď
ν1
ν1, ν2, . . . , νd
Lb
b.

ν
| “
|
1, and
bd for some (multivariate) kernel function L and positive bandwidth

denotes the qd-dimensional vector col-
1 uν2
uν1
2
q
!
{p

ν! for 0
{
T, ν
u1, u2, . . . , ud

q, where uν
T, qd

νd for u
b
x
u
q{
q{

ď |
“ p

`
u; x

“
“ p

¨ ¨ ¨
q!d!

` ¨ ¨ ¨ `

uνd
d ,

“ p

q “

q ´

ν2

pp

´

`

L

d

q

p

q

q

xi

q
p

x
q

´

2

Tu

˘

Lb

xi; x
q

p

,

Our proposed estimator

is not only intuitive and easy to implement, but also
boundary adaptive for a possibly unknown support Y
X . These features follow directly
from its construction: unlike classical kernel-based conditional density estimators, which

x
q
|

ˆ

p

p

f pϑq

y

seek to approximate the conditional PDF indirectly (e.g., by constructing a ratio of two
unconditional kernel-based density estimators), our proposed estimator applies local polyno-
, which itself has automatic
mial techniques directly to the conditional CDF estimator

F

y

boundary carpentry. Furthermore, our approach oﬀers an easy way to construct higher-order
kernels to reduce misspeciﬁcation (or smoothing) bias via the choice of polynomial orders
p and q, while still retaining all its other appealing features. We discuss related literature
further below.

p

x
q
|

p

We present two main uniform results for our proposed estimator. (Pointwise results

x
q
|

p

are reported in the supplemental appendix.) First, we provide precise uniform probability
In
concentration bounds associated with a stochastic linear representation of

f pϑq

y

.

addition to being useful for the purposes of characterizing the distributional properties of
the conditional density estimator itself, the ﬁrst main result can be used to analyze multi-
enters as a preliminary step.
step estimation and inference procedures whenever

f pϑq

p

y

x
q
|

p

As a by-product of the development of the ﬁrst main result, we obtain a related class of
conditional density estimators, which may be of independent interest. For details, see the

p

supplemental appendix.

f pϑq

y

p

y

x
q
|

and indexed over Y

Our second main result employs the stochastic linear representation of

to estab-
lish a valid strong approximation for the standardized t-statistic stochastic process based on
f pϑq
X . This result is established using a powerful result due to Rio
(1994), which in turn builds on the celebrated Hungarian construction (Koml´os et al., 1975).
p
As is well known, t-statistic stochastic processes based on kernel-based nonparametric estima-
tors are not asymptotically tight and, as a consequence, do not converge weakly as a process
X (van der Vaart and Wellner, 1996; Gin´e and Nickl, 2016). Nevertheless,
indexed over Y
using strong approximations to such processes, it is possible to deduce distributional approx-

x
q
|

ˆ

ˆ

p

p

3

f pϑq

imations for functionals thereof (e.g., suprema of the process) employing anti-concentration
(Chernozhukov et al., 2014a). For example, combining these ideas, we obtain valid distri-
butional approximations for the suprema of the standardized t-statistic stochastic process
with approximation rates that are faster than those currently available
x
based on
q
|
in the literature for the case of d
1 (e.g., Remark 3.1(ii) in Chernozhukov et al. (2014b)).
Furthermore, our approach to inference builds on the Gaussian approximation for the entire
t-process, not just its suprema, thereby allowing for the study of other functionals of interest
with essentially no additional work.

“

p

y

p

In addition to our two main uniform estimation and distributional results, we discuss
several implementation results that are useful for practice. First, we present a covariance

function estimator for the Gaussian approximation and prove its uniform consistency. This
result enables us to estimate the statistical uncertainty underlying the Gaussian approxima-
tion for a feasible version of the t-statistic process. Second, we discuss optimal bandwidth
selection based on an asymptotic approximation to the integrated mean squared error (IMSE)
. This result allows us to implement our proposed estimator using
of the estimator

f pϑq

y

point estimation optimal data-driven bandwidth selection rules. Finally, we employ robust
bias correction (Calonico et al., 2018, 2022) to develop valid inference methods based on the
Gaussian approximation when using the estimated covariance function and IMSE-optimal

p

x
q
|

p

bandwidth rule.

We illustrate our theoretical and methodological results with three substantive applica-
tions: we construct valid conﬁdence bands for the unknown conditional density function and

derivatives thereof, and develop valid hypothesis testing procedures for parametric speciﬁ-
cation and shape constraints of f pϑq
. All these methods are data-driven and, in some
cases, optimal in terms of probability and/or distributional concentration, possibly up to
log
factors. We also present a small simulation study. All proofs are given in the supple-
mental appendix, which considers a more general setup and also oﬀers additional technical

x
q
|

n
q

y

p

p

and methodological results of potential independent interest. Last but not least, we provide
a general purpose R software package implementing the main results in this paper.

1.1 Related Literature

Our paper contributes to the literature on kernel-based conditional density estimation and
inference. See Hall et al. (1999), De Gooijer and Zerom (2003) and Hall et al. (2004) for

earlier reviews, and Wand and Jones (1995), Wasserman (2006), Simonoﬀ (2012) and Scott
(2015) for textbook introductions.

Traditional methods for conditional density estimation typically employ ratios of uncon-

4

ditional kernel density estimators, non-linear kernel-based derivative of distribution function
estimators, or local polynomial estimators based on some preliminary density-like approxi-
mation. In particular, in the leading special case of ϑ
0, the closest antecedent to our
proposed conditional density estimator is the local polynomial conditional density estimator
introduced by Fan et al. (1996). Unlike their proposal, our estimator is boundary adaptive
without requiring knowledge of the support Y. To further highlight the connections between
the estimators, notice that their estimator takes the form

“

fFYT

y

p

x
|

q “

eT
0 argmin
uPRqd `1

p

n

i“1
ÿ

`

yi; y

Kh

p

q ´

xi

q
p

x
q

´

2

Tu

˘

Lb

xi; x
q

p

,

where, by the way of motivation, Fan et al. (1996) note that if y belongs to the interior of
Y, then

lim
hÓ0

E
r

Kh

yi; y

xi

x

p

q|

“

s “

lim
hÓ0

Kh

u; y

f

u

x
q
|

p

du

“

f

y

.

x
q
|

p

q

p

żY
The displayed equality does not hold when y is a boundary point of Y, and for this reason
their estimator has poor bias properties when y is on (or near) the boundary of Y.

Our estimator of f

y

x
q
|

p

is similar to their estimator insofar as it can be interpreted as

f

y

p

x
|

q “

eT
0 argmin
uPRqd `1

where

p

n

Kh

i“1
ÿ

´

p

yi; y

xi

q
p

x
q

´

q ´

p

2

Tu
¯

Lb

xi; x
q

p

,

u; y

Kh

p

q “

eT
1 argmin
uPRp`1

u

1p

ď

yj

q ´

yj

p
p

y

q

´

2

Tu

Kh

yj, y

.

q

p

n

j“1
ÿ

`

In other words, our conditional density estimator

p

˘

can be interpreted as the estimator

proposed by Fan et al. (1996) but with a diﬀerent (data-driven) kernel function,
smoothing out the variable y. The implied kernel

satisﬁes

u; y

p
Kh

f

y

x
q
|

p

p

q

yi; y

Kh

p

,

q

p

Kh

u; y

f

u

x
q
|

p

q

du

“

p

eT
1 argmin
uPRp`1

żY

p

p
yj

F

p

n

j“1
ÿ

`

x
|

q ´

yj

p
p

y

q

´

2

Tu

Kh

yj, y

.

q

p

˘

Standard local polynomial reasoning therefore suggests that our estimator should enjoy good
bias properties even when y is on (or near) the boundary of Y. Indeed, our estimator oﬀers
automatic boundary carpentry, higher-order derivative estimation, and automatic higher-
order kernel constructions, among other features.

More generally, classical methods for conditional density estimation are not boundary
adaptive without speciﬁc modiﬁcations, and in some cases do not have a closed-form repre-

5

sentation. Boundary carpentry could be achieved by employing boundary-corrected kernels
in some cases, but such conditional density estimation methods do not appear to have been
considered in the literature before. Therefore, our ﬁrst contribution is to introduce a novel

automatic boundary adaptive, closed-form conditional density estimator, which does not re-
quire knowledge of the support of the data and is easy to implement and interpret. Our

proposed construction does not rely on boundary-corrected kernels explicitly nor does it ex-
ploit knowledge of the support of the data in its construction, but it rather builds on the idea
that automatic boundary-adaptive density estimators can be constructed using local poly-

nomial methods to smooth out the (discontinuous) distribution function (Cattaneo et al.,
, which
2020). We build on that idea to construct the conditional density estimator

f pϑq

y

does not directly estimate the ratio of unconditional PDFs, but rather forms the conditional
density estimator by using local polynomial methods applied to a conditional CDF estimator,
a conceptually and substantively distinct approach to estimation of the conditional PDF of
yi given xi.

p

x
q
|

p

1.2 Notation and Assumptions

To simplify the presentation, in the remainder of this paper we set L to be the product kernel
based on K; that is, L
K
. We also employ the same bandwidth,
p
b
0

p
h, in the construction of our proposed estimator, and assume q
throughout. General results are available in the supplemental appendix.

q ¨ ¨ ¨

q “

ud

u1

u2

K

K

ě

´

“

´

“

u

ϑ

p

1

q

q

p

p

Limits are taken with respect to the sample size tending to inﬁnity (i.e., n

). For
bn is bounded and an ÀP bn
{
bn is bounded in probability. Constants that do not depend on the sample
{

two non-negative sequences an and bn, an À bn means that an
means that an
size or the bandwidth will be denoted by c, c1, c2, etc.

Ñ 8

We also introduce the notation ÀTC, which not only provides an asymptotic order but
0, there

also controls the tail probability. To be precise, an ÀTC bn implies that for any c1
exists some c2 such that

ą

lim sup
nÑ8

nc1 P

an

c2bn

ě

.

ă 8

Finally, let X
following assumptions on the joint distribution and the kernel function.

1 , . . . , xT
xT
n q

T and Y

“ p

“ p

q

“
y1, . . . , yn

‰
T be the data matrices. We make the

Assumption 1 (DGP)
y1, xT
yn, xT
(i)
1
n q
p
1`d, and the joint Lebesgue density, f
0, 1
s

T, . . . ,

p

q

r

T is a random sample from a distribution supported on Y

X

“
, is continuous and bounded away from zero

ˆ

y, x
q

p

6

on Y
ˆ
(ii) f ppq
(iii)

X .
y
x
q
p
|
νf pϑq
y
p

B

exists and is continuous.
x
|

q{B

xν exists and is continuous for all

ν
|

| “

p

´

ϑ.

Assumption 2 (Kernel)
K is a symmetric, Lipschitz continuous PDF supported on

1, 1

.

s

r´

1.3 Paper Organization

f pϑq

y

x
q
|

uniformly over Y

X .
Section 2 ﬁrst presents a stochastic linear representation for
We then discuss the main theoretical properties of our proposed estimator, oﬀering precise
X .
concentration characterizations in probability and in distribution uniformly over Y
Section 3 deploys our theoretical results to three applications: construction of conﬁdence
bands, parametric speciﬁcation hypothesis testing, and shape constrained hypothesis testing
for f pϑq
. Section 4 reports a small simulation study employing our companion R package
(Cattaneo et al., 2022). Section 5 concludes. The supplemental appendix contains additional
results not included here to simplify the presentation: (i) boundary adaptive estimators for

x
q
|

ˆ

ˆ

p

y

p

p

the CDF and its derivatives with respect to x, (ii) a new class of estimators based on non-
random local smoothing that is less sensitive to “low” density regions, (iii) complete proofs,

(iv) details on bandwidth selection, (v) alternative covariance function estimators, and (vi)
other technical lemmas that may be of independent interest.

2 Main Results

This section presents four main theoretical results. First, we provide a stochastic linearization
of our estimator. Based on this representation, we obtain a uniform probability concentration

f pϑq

y

p

f pϑq

. Next, we obtain valid strong approximation results for the standardized
result for
x
q
|
t-process based on
. Finally, we develop a feasible distributional approximation
for the suprema of the t-process. To accomplish the latter, we obtain a uniform consistency
result for an estimator of the covariance function. The supplemental appendix discusses other
results under slightly weaker conditions: because our uniform results are
pointwise in

x
q
|

p

p

y

p

sharp, the only substantive diﬀerence is that in the pointwise results the log
be dropped.

terms can

n
q

p

y, x
q

p

7

2.1 Stochastic Linearization

Our proposed estimator can be written in closed-form as

f pϑq

y

p

x
|

q “

eT
1`ϑ

S´1
y

Ry,x

S´1
x e0,

where

Sy

p
Ry,x

“

“

p

p

P

´
yj

´

yi

´
h

y

´
h

y

T

,

¯
1
hd Q
´

¯

1
h

P

¯
1
h

p

p

p

n

1
n

T

xi

´
h

x

1
hd Q
´

¯

xi

´
h

q

´

x

T

,

¯

yj

,

q

ď

i“1
ÿ
yi

1p

Sx

“

xi

x

p
´
h

¯

n

p

1
n

´

i“1
ÿ
1
n2h1`ϑ

y

yi

n

´
h
n

j“1
ÿ

i“1
ÿ

with the deﬁnitions P

u

q
function into the basis. The matrices
respectively, where

q “

p

p

q

u

K

u

p
p

and Q
p
Sy and

u
, which absorb the kernel
q “
Sx are well approximated by Sy and Sx,

u
q

u
q

q
p

L
p

p

p

Sy

“

żY

p

´

u

y

´
h

u

´
h

1
h

P

´

¯

T

y

¯

dFy

u

,

q

p

Sx

“

żX

u

´
h

u

x

1
hd Q
´

¯

´
h

q

´

T

x

¯

dFx

,

u
q

p

with Fy and Fx denoting the CDFs of yi and xi, respectively. Obtaining and characterizing
a simple approximation to the matrix
Ry,x requires a little more care, but the end result can
be combined with the results for

Sx to obtain the following result for

f pϑq

y

:

x
q
|

p

Sy and
p

Lemma 1 (Stochastic Linearization)
p
Suppose Assumptions 1 and 2 hold. If h

p

Ñ

0 and if nh1`d

log

p

{

n

q Ñ 8

, then

p

sup
yPY,xPX ˇ
ˇ
ˇ
ˇ
ˇ

where

f pϑq

y

p

x
|

q ´

f pϑq

y

p

x
|

q ´

p

1
n

n

i“1
ÿ

K ˝
ϑ,h

`

yi, xi; y, x

ˇ
ˇ
˘
ˇ
ˇ
ˇ

ÀTC hp´ϑ

`

log

n
q

p
?n2h1`2ϑ`d`p2_dq

,

K ˝

ϑ,hp

a, b; y, x

q “

1
h1`ϑ eT

1`ϑS´1

y

a

u

F

u

b
q
|

p

q´

ď

1p

żY

´

u

y

´
h

1
h

P

´

¯

¯

dFy

u

p

q

1
hd Q

b

´
h

ˆ

T

x

˙

S´1
x e0.

f pϑq

y

The properties of

are thus governed by the properties of the stochastic linear
representation. In the supplemental appendix, we demonstrate important features of K ˝
ϑ,h,
such as boundedness and Lipschitz continuity, which will play a crucial role in our strong
approximation results. We also bound the uniform covering number for the class of func-

x
q
|

p

p

tions formed by varying the evaluation point. This uniform covering number result takes
into account the fact that the shape of K ˝
ϑ,h changes across diﬀerent evaluation points and

8

bandwidths, and is established using a generic result (namely, Lemma SA-3.6), which may
be of independent interest.

In the supplemental appendix, we also study an intermediate estimator (denoted

x
q
|
which has some distinctive features that may be of independent interest in some settings: its
construction employs a non-random weighting that is less sensitive to “low” density regions,
but it requires ex-ante knowledge of the support Y

X .

q

p

f pϑq

y

),

In the remainder of the paper, we use the representation established by Lemma 1 to study
. The lemma is also useful when studying the properties of multi-

the properties of

f pϑq

y

step nonparametric and semiparametric procedures employing conditional density estimators
as preliminary estimators, but to conserve space we do not discuss those applications here.

p

x
q
|

p

ˆ

2.2 Uniform Probability Concentration

The following theorem gives a uniform probability concentration result for

f pϑq

y

.

x
q
|

p

Theorem 1 (Probability Concentration)
Suppose Assumptions 1 and 2 hold. If h

0 and if nh1`d

Ñ

p
, then

log

p

{

n

q Ñ 8

sup
yPY,xPX

f pϑq

y

p

x
|

q ´

f pϑq

y

x
q
|

p

ÀTC hp´ϑ

n
log
nh1`d`2ϑ .
p
q

` c

ˇ
ˇ
ˇ p

ˇ
ˇ
ˇ

In the theorem, hp´ϑ stems from a bias term whose magnitude coincides with that of
the pointwise bias at interior evaluation points. As a consequence, the theorem implies

that the estimator is boundary adaptive even though the estimator can be implemented
X . The other term represents
without speciﬁc knowledge of the (compact) support Y
“noise”, whose magnitude is larger than its counterpart in Lemma 1. As a consequence, the
f pϑq

theorem implies that the estimation error
to n´1
yi, xi; y, x

x
|
whenever the bias is asymptotically negligible.

is asymptotically equivalent

x
q
|

f pϑq

q ´

ˆ

y

y

p

p

K ˝
ϑ,h

i

`

˘

ř

Section 3 characterizes the leading bias and variance constants and then uses these
to obtain (approximate) IMSE-optimal bandwidths. When doing so, we follow the local
ϑ to be even so that
polynomial regression literature (Fan and Gijbels, 1996) and require p
the leading bias term is easily characterized, but this condition is not required in Theorem 1.

´

p

By setting h

log

p

“ p

n

n
q

q{

1

1`d`2p , it follows from Theorem 1 that

sup
yPY,xPX

f pϑq

y

p

x
|

q ´

f pϑq

y

x
q
|

p

ˇ
ˇ
ˇ p

ÀTC

n
q

log
p
n

˙

ˆ

p´ϑ
1`d`2p

,

which matches the minimax optimal convergence rate established by Khas’minskii (1979).

ˇ
ˇ
ˇ

9

2.3 Strong Approximation

Next, we study the distributional properties of the process

Sϑ

p

y, x
q

p

: y

P

Y, x

X

q

P

, where

with

Sϑ

p

p

f pϑq

y

p

y, x

q “

x
q ´
|
Vϑ
p

f pϑq
p
y, x
q

y

p
x
,
q
|

(1)

ϑ,hp
Using elementary tools, Theorem SA-2.1 in the supplemental appendix obtains the

q “

“

‰

p

Vϑ

y, x

V

K ˝

yi, xi; y, x
q

.

following pointwise Gaussian approximation:

a

p

1
n

sup
uPR

P

Sϑ

y, x

p

u

u

Φ
p

q

´

q ď

À ?nh1`d`2p

n
log
q
p
?nh1`d

,

`

q

‰

u

ˇ
ˇ
ˇ

“
p

where Φ
p

denotes the standard Gaussian CDF.

As is well-known, however, the process

ˇ
ˇ
ˇ
Sϑ is not asymptotically tight and therefore
X
does not converge weakly to a Gaussian process in ℓ8
, the set of uniformly bounded
q
X equipped with the uniform norm (van der Vaart and Wellner,
real-valued functions on Y
1996; Gin´e and Nickl, 2016). To obtain a uniform distributional approximation, we use the
.
result of Rio (1994) and establish a strong approximation result for

Y, x

: y

ˆ

ˆ

X

Y

p

p

Sϑ

y, x
q

p

p

P

P

q

To state the result, deﬁne the covariance/correlation function

y, x, y1, x1

ρϑ

p

q “

Cϑ

y, x, y1, x1

Vϑ

p

a

p
y, x
q
a

q
y1, x1

Vϑ

p

p

,

q

where

y, x, y1, x1

Cϑ

p

E

1
n

q “

K ˝

ϑ,hp

yi, xi; y, x
q

K ˝

ϑ,hp

yi, xi; y1, x1

.

q

‰

Theorem 2 (Strong Approximation)
Suppose Assumptions 1 and 2 hold. If nh1`d`2p
exist two stochastic processes,

, then there
ϑ and Gϑ, in a possibly enlarged probability space, such that:
S1

q Ñ 8

log

Ñ

0 and if nh1`d

n

{

p

“

Sϑ and

S1
ϑ have the same distribution,

(i)
(ii) Gϑ is a centered Gaussian process with covariance function ρϑ, and
(iii)

p

p

p

y, x

q ´

Gϑ

y, x
q

p

sup
yPY,xPX

S1
ϑp

ˇ
ˇ
ˇp

ÀTC ?nh1`d`2p

ˇ
ˇ
ˇ

10

`

ˆ

log

1`d

n
p
q
nh1`d

1
2`2d

.

˙

The theorem provides a Gaussian approximation for the entire stochastic process

rather than for a particular functional thereof.

Sϑ

p

2.4 Covariance Estimation

Sϑ and the covariance function ρϑ depend on unknown features of
Because both the process
the underlying data generating process (namely, the covariance function Cϑ), Theorem 2 in
isolation cannot be used for inference. Equipped with a suitably accurate estimator of Cϑ,
on the other hand, Theorem 2 becomes immediately useful for inference. Accordingly, the
purpose of this subsection is to propose and study a natural estimator of Cϑ.

p

The covariance function Cϑ can be expressed as a functional of two unknowns, namely
y

conditional CDF of yi given xi and the marginal CDF of yi. Replacing F
yi
with

q
, respectively, we obtain the following natural

and Fy

x
q
|

n´1

and

y

y

y

p

p

n

F

x
q
|

Fy
i“1 1p
plug-in covariance function estimator:
p

q “

ř

p

y

p

p

ď

q

y, x, y1, x1

q “

Cϑ

p

p

1
n2

n

i“1
ÿ

K ˝
ϑ,h

yi, xi; y, x

K ˝
ϑ,h

´

x

¯

x

yi, xi; y1, x1
´

,

¯

where

K ˝
ϑ,h

a, b; y, x

´

¯

1
h1`ϑ eT

1`ϑ

S´1
y

“

1
n

«

n

j“1
ÿ

´

a

1p

ď

yj

q ´

F

yj

b
q
|

p

yj

y

´
h

1
h

P

´

¯

ﬀ

¯

1
hd Q

b

´
h

ˆ

T

x

˙

S´1
x e0.

p

x
p
The corresponding estimators of Vϑ and ρϑ are given by

p

Vϑ

p

y, x

q “

Cϑ

y, x, y, x
q

p

and

y, x, y1, x1

ρϑ

p

q “

Cϑ

y, x, y1, x1
p

Vϑ
p
p

p
y, x
q
b

q
y1, x1

q

Vϑ

p

p

,

b
respectively. The next lemma establishes a uniform probability concentration result for

p

p

p

Lemma 2 (Covariance Estimation)
Suppose Assumptions 1 and 2 hold. If h

0 and if nh1`d

Ñ

log

p

{

n

q Ñ 8

, then

sup
y,y1PY,x,x1PX

p

Cϑ

ˇ
ˇ
ˇp

y, x, y1, x1

Cϑ

p

q ´

y, x, y1, x1

ÀTC hp´ϑ´ 1

2

n
log
nh1`d .
p
q

` c

q

ˇ
ˇ
ˇ

11

Cϑ.

p

2.5 Suprema Approximation

Replacing Vϑ

y, x
q

p

with

Vϑ

y, x
q

p

in (1), we obtain

p

Tϑ

p

p

f pϑq

y

p

y, x

q “

x
|

p

f pϑq

y

x
q
|

.

p

q ´
Vϑ

y, x
q

p

b

p

Tϑ

By Theorem 2 and Lemma 2, the probability law of
can be
approximated by that of a centered Gaussian process with covariance function ρϑ, where
Tϑ admit feasible
the latter is well approximated by
distributional approximations. To illustrate this general phenomenon, the following theorem
p
Gϑ be a centered process
Tϑ
gives a result for the supremum of
whose law, conditionally on the data, is Gaussian with covariance function
ˇ
ˇp

ρϑ. As a consequence, functionals of

. To state the result, let

y, x
q

ρϑ.

: y

p

p

p

P

P

ˇ
ˇ

p

p

q

Y, x

X

Theorem 3 (Kolmogorov-Smirnov Distance: Suprema)
Suppose Assumptions 1 and 2 hold. If n log

h1`d`2p

0 and if nh1`d

Ñ

p
log

p

{

n

q Ñ 8

, then

sup
uPR

P

”

ˇ
ˇ
ˇ
ˇ

sup
yPY,xPX

Tϑ

y, x
q

p

u

ď

ı
ˇ
ˇ
h1`d`2p

ÀP

ˇ
ˇp
n log

p

a

n
q

sup
yPY,xPX

Gϑ

y, x
q

p

ď

log

2`2d

ˇ
ˇp
n
q
p
nh1`d

1
2`2d

ˇ
ˇ

˙

`

ˆ

u

X, Y

5

ıˇ
ˇ
ˇ
ˇ
ˇ
n
log
ˇ
ˇ
q
p
nh1`d

˙

1
4

.

n
q

p

´

P

”

`

ˆ

To compare the rate of distributional approximation with existing results, we follow the
literature and ignore the ﬁrst (smoothing bias) term. Then, the resulting rate takes the form

log

2`2d

n
p
q
nh1`d

1
2`2d

˙

`

ˆ

5

n
log
q
p
nh1`d

1
4

.

˙

ˆ

This rate matches what Chernozhukov et al. (2014b) obtained when d
3.1(ii)), but it is strictly faster when d

1.

“

2 (see their Remark

“

3 Applications

This section illustrates our theoretical and methodological results by means of three appli-

cations. Before turning to these applications, we discuss bandwidth selection, a necessary
step for implementation. It is customary to select the bandwidth by minimizing an approxi-
ϑ is even, we
mation to the IMSE of

. Employing Lemma 1 and assuming that p

f pϑq

y

´

x
q
|

p

p

12

show in the supplemental appendix that

E

f pϑq

” ĳYˆX ´

p

y

p

x
|

q ´

f pϑq

y

x
q
|

p

2

dydx

¯

ı

«

ĳYˆX

2

dydx

E

¯f pϑq

y

x
|

p

q ´

f pϑq

y

x
q
|

p

”´
h2p´2ϑBϑ

¯
ı
1
nh1`2ϑ`d Vϑ

2

y, x
q

p

`

«

ĳYˆX ˆ

dydx,

y, x
q

p

˙

where

with

Bϑ

Vϑ

p

p

cy,p`1

cx,ν

Ty

Tx

y, x

q “

f ppq

y

x
q
|

p

1`ϑS´1
eT

y cy,p`1

`

y, x

q “

f

y

x
q
|

p

y TyS´1

y e1`ϑ

1`ϑS´1
eT
´

¯´

0 S´1
eT

x cx,ν,

ν

y

xν f pϑq
B
|ν|“p´ϑ
B
ÿ
x TxS´1
0 S´1
eT

p

x e0

x
q
|

,

¯

1

u

1
u

`

p
p
1
ν!
´
min

p

!

´
x

q
´
h
¯
u1, u2
h

y

p`1 1
h
u

´
h
¯
ν 1
hd Q
´
y
1
h2 P

q ´

żY

żX

“

“

“

ĳYˆY

u

1
hd Q
´

x

´
h

Q

¯

´

u

´
h

“

żX

u

P

´
x

´
h

y

´
h

dFx

u

,

q

p

dFy

¯
u
q

p

,

´
x

T

¯

dFx

.

u
q

p

u1

y

¯
´
h

u2

´
h

T

y

¯

P

¯

´

dFy

dFy

u1

p

q

u2

,

q

p

(The supplemental appendix also discusses the case where p
general results.)

´

ϑ is odd and provides more

The bandwidth that minimizes the approximate IMSE is proportional to n´ 1
1`d`2p . Al-
though this bandwidth delivers point estimates that are approximately IMSE-optimal, a

non-vanishing bias will be present in their asymptotic distribution, complicating statistical
inference. To address this well-known problem, our construction of conﬁdence bands and test
statistics for parametric or shape restrictions employs robust bias correction (Calonico et al.,

2018, 2022): ﬁrst we construct an IMSE-optimal point estimator, and then we bias correct
the estimator and adjust the covariance function estimator accordingly to obtain a valid and

improved distributional approximation.

To make the robust bias-correction procedure precise, we augment the notation so that
it reﬂects the local polynomial order and the bandwidth used. For example, the conditional
density estimator using polynomial order p (and q
h (and b

1) and employing the bandwidth

h) is written as

´

“

´

ϑ

p

y

.

f pϑq
p

“

x; h
q
|

p

p

13

Application 1: Conﬁdence Bands

Conﬁdence bands can be constructed using the process

TCB

ϑ,p`1

p

y, x
q

p

: y

P

Y, x

X

q

P

, where

TCB

ϑ,p`1

p

y, x

q “

p

f pϑq
p`1

p

y

p

b

x; h‹
|
Vϑ,p`1

p
f pϑq

pq ´

p
y, x; h‹
pq

p

y

x
q
|

,

By Theorem 3, the distribution of
data) distribution of
ditionally on the data, is Gaussian with covariance function

ϑ,p`1 is well-approximated by the conditional (on the
Gϑ,p`1, the latter being a centered Gaussian process whose law, con-
. Accordingly,

p

ρϑ,p`1

TCB

p

y, x; h‹
pq

p

p

let

where

CBϑ,p`1

α

1

p

´

q “

”

f pϑq
p`1

y

x; h‹
|

pq ˘

p

cvCB

ϑ,p`1

α

p

q

p

p
y, x; h‹
pq

p

Vϑ,p`1

b

p

Y, x

: y

P

P

X

,

ı

cvCB

ϑ,p`1

α

p

q “

inf

u

R` : P

P

sup
yPY,xPX

Gϑ,p`1

y, x
q

p

ď

u

X, Y

1

α

.

ě

´

”
As the notation suggests, CBϑ,p`1

!

ˇ
ˇp
q

is a 100

ˇ
ˇ
1

p

α

q

´

α

1

p

´

ı

ˇ
ˇ
ˇ
% conﬁdence band. To be

)

speciﬁc, we have:

Theorem 4 (Conﬁdence Bands)
Suppose Assumptions 1 and 2 hold, f pp`1q
exists and is continuous for all

p

ν
|

| “

`

y

p
1

exists and is continuous, and

x
q
|
ϑ. Then
´

νf pϑq

y

p

x
|

q{B

xν

B

where rCB

n´ 1

1`d`2p

“

P

ˇ
ˇ
`

f pϑq

P
“
n´ 2p´2ϑ`1

4p1`d`2pq

CBϑ,p`1

α

1

p

´

q

´ p

1

α

q

´

À log

5
4

rCB,

n
q

p

‰
p1`d`2pqp1`dq .

p

n´

ˇ
ˇ

`
1

p

The conﬁdence band CBϑ,p`1

index set of the Gaussian process, the critical value cvϑ,p`1
ulation from a multivariate conditionally (on the data) Gaussian distribution. We illustrate
the performance of our proposed conﬁdence bands using simulated data in Section 4.

´

1

p

q

α

q

´

is easy to construct because, by discretizing the
can be computed by sim-

α

Application 2: Parametric Speciﬁcation Testing

Suppose the researcher postulates that the conditional density (or derivative) belongs to the
, where Γϑ is some parameter space. Abstracting away
parametric class
u
from the speciﬁcs of the estimation technique, we assume that the researcher also picks some

f pϑq
t

x; γ
|

Γϑ

: γ

P

y

p

q

14

estimator
in probability to some ¯γ

γ (e.g., maximum likelihood or minimum distance), which is assumed to converge
Γϑ. A natural statistic for the problem of testing

P

p

is

HPS

0 : f pϑq

vs.
1 : f pϑq

HPS

y

x; ¯γ
|

p

q “

f pϑq

y

x
q
|

p

for all

y, x

q P

p

Y

y

x; ¯γ
|

p

q ‰

f pϑq

y

x
q
|

p

for some

y, x

q P

p

ˆ

Y

X

X .

ˆ

sup
yPY,xPX

TPS

ϑ,p`1

y, x
q

p

,

TPS

ϑ,p`1

p

y, x

q “

ˇ
ˇp

ˇ
ˇ
Assuming the estimation error of
is given by cvCB

α

ϑ,p`1

p

p

. To be speciﬁc, we have:
p

q

f pϑq
p`1

y

p

x; h‹
|

pq ´

Vϑ,p`1

p

y

x;
|

f pϑq

p
y, x; h‹
pq

p

q

.

γ

p

b

p

γ is asymptotically negligible, a valid 100α% critical value

Theorem 5 (Parametric Speciﬁcation Testing)
Suppose Assumptions 1 and 2 hold, f pp`1q
exists and is continuous for all

p
1

p

y

x
q
|
ϑ. If
´

ν
|

| “

`

exists and is continuous, and

νf pϑq

y

p

x
|

q{B

xν

B

p´ϑ
1`d`2p

n

sup
yPY,xPX

f pϑq

γ

y

x;
|

p

q ´

f pϑq

y

x; ¯γ
|

q

p

ÀTC rCB,

ˇ
ˇ

p

ˇ
ˇ

then, under HPS
0 ,

P

sup
yPY,xPX |

TPS

ϑ,p`1

„

y, x

q| ą

p

cvCB

ϑ,p`1

α

p

´

q



p
Application 3: Testing Shape Restrictions

ˇ
ˇ
ˇ
ˇ

rCB.

n
q

p

α

À log

5
4

ˇ
ˇ
ˇ
ˇ

As a third application, suppose the researcher wants to test shape restrictions on f pϑq. Letting
cϑ be a pre-speciﬁed function, consider the problem of testing

HSR

0 : f pϑq

vs.
1 : f pϑq

HSR

y

p

x
|

q ď

cϑ

y

x
q
|

p

for all

y, x

q P

p

Y

y

p

x
|

q ą

cϑ

y

x
q
|

p

for some

y, x

q P

p

ˆ

Y

X

X .

ˆ

0 and if cϑ

is some (positive) constant value c, the testing problem
For example, if ϑ
refers to whether the conditional density exceeds c somewhere on its support. As another
0, then the testing problem refers to whether the
example,

1 and if cϑ

x
q
|

if ϑ

“

y

y

p

“

x
|

p

q “

15

conditional density is non-increasing in y for all values of x. More generally, the testing
problem above can be used to test for monotonicity, convexity, and other shape features of
the conditional density, possibly relative to the function cϑ

y

.

A natural testing procedure rejects HSR

0 whenever the test statistic

x
q
|

p

sup
yPY,xPX

TSR

ϑ,p`1

,

y, x
q

p

TSR

ϑ,p`1

p

y, x

q “

f pϑq
p`1

p

y

x; h‹
p
|
Vϑ,p`1

cϑ

y

pq ´
p
y, x; h‹
pq

p

x
q
|

b

p

exceeds a critical value of the form

cvSR

ϑ,p`1

α

p

q “

inf

u

!

R` : P

P

”

sup
yPY,xPX

Gϑ,p`1

p

y, x

q ď

u

X, Y

1

´

ě

α

.

)

ı

p

ˇ
ˇ
ˇ

Theorem 6 (Testing Shape Restriction)
Suppose Assumptions 1 and 2 hold, f pp`1q
x
exists and is continuous, and
q
|
ϑ. Then, under HSR
0 ,
exists and is continuous for all

p
1

p

y

ν
|

| “

`

´

νf pϑq

y

p

x
|

q{B

xν

B

P

sup
yPY,xPX

„

TSR

ϑ,p`1

p

y, x

q ą

cvSR

ϑ,p`1

α

p

´

q



p

ˇ
ˇ
ˇ
ˇ

4 Simulations

rCB.

n
q

p

α

À log

5
4

ˇ
ˇ
ˇ
ˇ

We illustrate the eﬀectiveness of our proposed methods with a Monte Carlo experiment.
Replication ﬁles, additional simulation results, and details of the companion R package,
lpcde, can be found at https://nppackages.github.io/lpcde/ and in our companion
software article (Cattaneo et al., 2022).
For the sake of simplicity, we set d

mean 0 joint normal distribution with variance 2 and covariance
We simulate 1000 data sets of sample size n
for the conditional PDF at three diﬀerent conditioning values: (a) interior (x
near-boundary (x
“
on 20 equally spaced points for y on
. Corresponding average rule-of-thumb (ROT)
bandwidth, bias, standard error, pointwise and uniform coverage, and average conﬁdence

1 and assume that x and y are simulated by a
2.
s
5000. Table 1 presents the simulation results
0), (b)
1). Point estimates are generated

0.8), and (c) at-boundary (x

0.1, truncated on

0, 1

1, 1

r´

“

“

´

“

“

s

r

interval/band width over the grid are presented in the table. The polynomial orders for
bandwidth selection and standard estimation (“WBC”) are p
1, while those for
robust bias-corrected estimation (“RBC”) are p
2. (Details of ROT bandwidth
selection and variance estimation are given in Sections SA-5.3 and SA-6 of the supplemental
appendix.)

3, q

2, q

“

“

“

“

16

p

hROT bias
0.09
0.09
0.10
0.10
0.10
0.10

WBC 0.38
RBC 0.38
WBC 0.35
RBC 0.35
WBC 0.38
RBC 0.38

0

0.8

1.0

x

x

x

“

“

“

Coverage

Average Width

Pointwise Uniform Pointwise Uniform

60.4
87.9
73.0
91.6
55.8
81.2

78.9
93.3
84.6
95.9
77.0
91.1

0.11
0.37
0.22
0.65
0.24
0.67

0.23
0.68
0.40
0.81
0.45
0.81

se
0.03
0.09
0.04
0.18
0.06
0.20

Table 1. Table of average uniform coverage for jointly normal data.
WBC: without bias-correction, RBC: robust bias-corrected.

The simulation results in Table 1 support our main theoretical results. First, robust

bias-correction leads to better performance of the inference procedures, both pointwise and
uniformly over Y. Second, our uniform distributional approximation leads to feasible conﬁ-
dence bands with good ﬁnite sample performance, when coupled with robust bias correction

methods.

p

y

p

“

“

fp

h
q

x,
|

2 and q

For example, for x

“
0.38, which leads to a point estimator

0.0, the averaged (across simulations) estimated MSE-optimal
h
with simulation
bandwidth choice is
bias 0.09 and standard error 0.03, with p
1. The associated pointwise
p
distribution theory leads to conﬁdence intervals with empirical coverage of about 60.4%
without robust bias correction, and 87.9% with robust bias correction (i.e., using p
3
instead of p
. Both are substantially below
the 95% nominal level because they are not uniform over Y. The feasible conﬁdence bands
are designed to address that issue: Table 1 shows that our proposed conﬁdence bands have
empirical coverage of 78.9% without robust bias correction, and 93.3% when robust bias
correction is employed. The latter approach is our recommended approach emerging from
Theorem 4, which is shown to perform well in all cases considered.

2) when constructing the t-statistic

y, x
q

Tϑ

“

“

´

“

p

p

p

p

5 Conclusion

We introduced a new boundary adaptive estimator of the conditional density and derivatives
thereof. This estimator is conceptually distinct from prior proposals in the literature, as

it relies on two (nested) local polynomial estimators. Our proposed estimation approach
has several appealing features, most notably automatic boundary carpentry. We provided

an array of uniform estimation and distributional results, including a valid uniform equiv-
alent kernel representation and uniform distributional approximations. Our methods are

17

applicable in data science settings either where the conditional density or its derivatives are
the main object of interest, or where they are preliminary estimands entering a multi-step
statistical procedure.

References

Calonico, S., Cattaneo, M. D., and Farrell, M. H. (2018). “On the Eﬀect of Bias Estimation

on Coverage Accuracy in Nonparametric Inference,” Journal of the American Statistical
Association, 113 (522), 767–779.

Calonico, S., Cattaneo, M. D., and Farrell, M. H. (2022). “Coverage Error Optimal Conﬁ-

dence Intervals for Local Polynomial Regression,” Bernoulli, forthcoming.

Cattaneo, M. D., Chandak, R., Jansson, M., and Ma, X. (2022). “lpcde: Local Polynomial

Conditional Density Estimation and Inference,” working paper.

Cattaneo, M. D., Jansson, M., and Ma, X. (2020). “Simple Local Polynomial Density Esti-

mators,” Journal of the American Statistical Association, 115 (531), 1449–1455.

Chernozhukov, V., Chetverikov, D., and Kato, K. (2014a). “Anti-Concentration and Honest,

Adaptive Conﬁdence Bands,” Annals of Statistics, 42 (5), 1787–1818.

Chernozhukov, V., Chetverikov, D., and Kato, K. (2014b). “Gaussian Approximation of

Suprema of Empirical Processes,” Annals of Statistics, 42 (4), 1564–1597.

De Gooijer, J. G. and Zerom, D. (2003). “On Conditional Density Estimation,” Statistica

Neerlandica, 57 (2), 159–176.

Fan, J. and Gijbels, I. (1996). Local Polynomial Modelling and Its Applications, New York:

Chapman & Hall/CRC.

Fan, J., Yao, Q., and Tong, H. (1996). “Estimation of Conditional Densities and Sensitivity

Measures in Nonlinear Dynamical Systems,” Biometrika, 83 (1), 189–206.

Gin´e, E. and Nickl, R. (2016). Mathematical Foundations of Inﬁnite-dimensional Statistical

Models, New York: Cambridge University Press.

Hall, P., Racine, J., and Li, Q. (2004). “Cross-Validation and the Estimation of Conditional
Probability Densities,” Journal of the American Statistical Association, 99 (468), 1015–

1026.

18

Hall, P., Wolﬀ, R. C., and Yao, Q. (1999). “Methods for Estimating a Conditional Distribu-

tion Function,” Journal of the American Statistical association, 94 (445), 154–163.

Khas’minskii, R. Z. (1979). “A Lower Bound on the Risks of Non-parametric Estimates of

Densities in the Uniform Metric,” Theory of Probability & Its Applications, 23 (4), 794–798.

Koml´os, J., Major, P., and Tusn´ady, G. (1975). “An Approximation of Partial Sums of

Independent RV’-s, and the sample DF. I,” Zeitschrift f¨ur Wahrscheinlichkeitstheorie und
verwandte Gebiete, 32 (1), 111–131.

Rio, E. (1994). “Local Invariance Principles and Their Application to Density Estimation,”

Probability Theory and Related Fields, 98 (1), 21–45.

Scott, D. W. (2015). Multivariate Density Estimation: Theory, Practice, and Visualization:

John Wiley & Sons.

Simonoﬀ, J. S. (2012). Smoothing Methods in Statistics: Springer.

van der Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence and Empirical Processes:

Springer.

Wand, M. and Jones, M. (1995). Kernel Smoothing: Chapman & Hall/CRC.

Wasserman, L. (2006). All of Nonparametric Statistics: Springer.

19

Supplemental Appendix to “Boundary Adaptive Local Polynomial
Conditional Density Estimators”∗

Matias D. Cattaneo†

Rajita Chandak∗

Michael Jansson‡

Xinwei Ma§

April 25, 2022

Abstract

This Supplemental Appendix contains general theoretical results encompassing those discussed
in the main paper, includes proofs of those general results, and discusses additional methodologi-
cal and technical results. A companion R package is available at https://nppackages.github.io/lpcde/.

2
2
0
2

r
p
A
1
2

]
T
S
.
h
t
a
m

[

1
v
9
5
3
0
1
.
4
0
2
2
:
v
i
X
r
a

∗Cattaneo gratefully acknowledges ﬁnancial support from the National Science Foundation through grant SES-
1947805 and from the National Institute of Health (R01 GM072611-16), and Jansson gratefully acknowledges ﬁnancial
support from the National Science Foundation through grant SES-1947662 and the research support of CREATES.

†Department of Operations Research and Financial Engineering, Princeton University.
‡Department of Economics, UC Berkeley and CREATES.
§Department of Economics, UC San Diego.

 
 
 
 
 
 
Contents

SA-1 Setup

SA-1.1 List of Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

SA-1.2 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

SA-1.3 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

SA-2 Pointwise Large-sample Properties

SA-3 Uniform Large-sample Properties

SA-4 Applications

3

4

6

8

8

12

17

SA-4.1 Conﬁdence Bands

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

SA-4.2 Parametric Speciﬁcation Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

SA-4.3 Testing Shape Restrictions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

SA-5 Bandwidth Selection

19

SA-5.1 Pointwise Asymptotic MSE Minimization . . . . . . . . . . . . . . . . . . . . . . 19

SA-5.2 Rule-of-thumb Bandwidth Selection . . . . . . . . . . . . . . . . . . . . . . . . . 22

SA-6 Alternative Variance Estimators

23

SA-6.1 V-statistic variance estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

SA-6.2 Asymptotic variance estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

SA-7 Technical Lemmas and Proofs

25

SA-7.1 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

SA-7.2 Proof of Lemma SA-2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

SA-7.3 Proof of Lemma SA-2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

SA-7.4 Proof of Lemma SA-2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

SA-7.5 Proof of Lemma SA-2.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

31

SA-7.6 Proof of Theorem SA-2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

SA-7.7 Omitted Details of Remark SA-2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . 35

SA-7.8 Proof of Lemma SA-3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

SA-7.9 Proof of Lemma SA-3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

SA-7.10 Proof of Lemma SA-3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

SA-7.11 Proof of Lemma SA-3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

41

41
SA-7.12 Proof of Lemma SA-3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
SA-7.13 Proof of Lemma SA-3.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

SA-7.14 Proof of Corollary SA-3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

SA-7.15 Proof of Theorem SA-3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

SA-7.16 Proof of Lemma SA-3.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

SA-7.17 Proof of Lemma SA-3.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

1

SA-7.18 Proof of Theorem SA-3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

SA-7.19 Proof of Theorem SA-4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

SA-7.20 Proof of Theorem SA-4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

SA-7.21 Proof of Theorem SA-4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

2

SA-1 Setup

Let xi

Rd and yi

= [0, 1]d and
= [0, 1], respectively. We are interested in estimating the conditional distribution function and

R be continuously distributed random variables supported on

X

∈

∈

Y
its derivatives:

θµ,ν(y, x) =

∂µ
∂yµ

∂ν
∂xν F (y

x),
|

N, and ν

where µ
along the y-direction, that is, we set ν = 0 and µ = ϑ + 1.)

∈

∈

Nd representing multi-indices. (In the main paper we only consider derivatives

To present our estimation strategy, we start from θ0,ν, the conditional distribution function and

its derivatives with respect to the conditioning variable, and apply the local polynomial method:

\∂ν
∂xν F (y

x) = eT
|

ν ˆγ(y, x),

ˆγ(y, x) = argmin
Rqd+1

γ

∈

n

i=1 (cid:16)
X

1(yi

y)

−

≤

q(xi

−

x)Tγ

Lh(xi; x),

2

(cid:17)

where, using standard multi-index notation, q(u) denotes the (qd + 1)-dimensional vector collecting
the ordered elements uν/ν! for 0
+ νd for
x)/h)/hd
, ud)T, ν = (ν1, ν2,
u = (u1, u2,
for some kernel function L, and eT

· · ·
ν is a basis vector extracting the corresponding estimate.

q, where uν = uν1
ν
2 · · ·
| ≤
, νd)T, and qd = (d+q)!/(q!d!)

uνd
d ,
= ν1 + ν2 +
1. Lh(u; x) = L((u

1 uν2

ν
|

≤ |

· · ·

· · ·

−

−

|

We can write the solution in closed form as

\∂ν
∂xν F (y

x) = eT
ν
|

1

ˆS−
x

1
n

n

i=1
X

1(yi

≤

y)q

(cid:16)

xi

x

−
h

where

Lh(xi; x)

,

!

(cid:17)

ˆSx =

1
n

n

q

i=1
X

(cid:16)

xi

x

−
h

xi

q

(cid:17)

(cid:16)

−
h

x

T

(cid:17)

Lh(xi; x).

To estimate θµ,ν, we further smooth via local polynomials along the y-direction:

ˆθµ,ν(y, x) = eT
µ

ˆβ(y, x),

In the above p(u) = (1, u, u2/2,
K((u

· · ·

−

ˆβ(y, x) = argmin
Rp+1

b

∈

n

ˆF (yi

i=1 (cid:16)
X

x)
|

−

p(yi

−

2

y)Tb

Kh(yi; y).

(cid:17)

, up/p!)T is the p-th order polynomial expansion, and Kh(u; y) =

y)/h)/h for some kernel function K. We can write the solution in closed-form as

ˆθµ,ν(y, x) = eT
µ

ˆS−
y

1

ˆRy,x ˆS−

1
x eν,

3

 
where

ˆSy =

ˆRy,x =

n

yi

p

1
n

i=1
X
1
n2hµ+

(cid:16)

ν

|

|

y

p

yi

(cid:17)
n

(cid:16)

−
h

−
h

n

1(yi

≤

j=1
X

i=1
X

y

T

(cid:17)

yj)p

Kh(yi; y),

and

yj

y

−
h

Kh(yj; y)q

(cid:17)

(cid:16)

xi

−
h

x

T

(cid:17)

Lh(xi; x).

(cid:16)

While in the above we considered local polynomial regressions along both the x- and y-directions,

it is also possible to employ a local smoothing technique. To be precise, let G be some function

such that the following Lebesgue-Stieltjes integration is well-deﬁned, then an alternative estimator

can be constructed as

ˇθµ,ν(y, x) = eT
µ

ˇβ(y, x),

which has the solution

ˇβ(y, x) = argmin
Rp+1

b

∈

ˆF (u
x)
|

Z (cid:16)

p(u

−

−

2

y)Tb
(cid:17)

Kh(u; y)dG(u),

ˇθµ,ν(y, x) = eT

µ S−
y

1

¯Ry,x ˆS−

1
x eν,

where

Sy =

p

u

y

−
h
n

p

(cid:17)

(cid:16)

u

−
h

1(yi

ν

|

|

i=1 (cid:18)ZY
X

(cid:16)

ZY

1
nhµ+

¯Ry,x =

y

T

Kh(u; y)dG(u),

and

(cid:17)

≤

u

y

−
h

u)p

(cid:16)

Kh(u; y)dG(u)

q

(cid:17)

(cid:19)

(cid:16)

xi

−
h

x

T

(cid:17)

Lh(xi; x).

SA-1.1 List of Notations

and h

Limits are taken with respect to the sample size tending to inﬁnity and the bandwidth shrink-
0). For two nonnegative sequences, an - bn implies that
ing to zero (i.e., n
is asymptotically bounded in probabil-
an/bn
lim supn
ity. We also adopt the small- and big-oh notation: an = OP(bn) is just an -P bn, and an = oP(bn)
means an/bn converges to zero in probability. Constants that do not depend on the sample size or
the bandwidth will be denoted by c, c1, c2, etc.

. Similarly, an -P bn means

→ ∞
<

an/bn
|

→∞ |

→

∞

|

|

We introduce another notation, OTC, which not only provides an asymptotic order but also
controls the tail probability. To be speciﬁc, an = OTC(bn) if for any c1 > 0, there exists some c2
such that

lim sup

n

→∞

nc1P [an

c2bn] <

.

∞

≥

Here the subscript, TC, stands for “tail control.”
n )T and Y = (y1,
Finally, let X = (xT
1 ,

, xT

· · ·

, yn)T be the data matrices.

· · ·

4

• F (y

x) and f (y
|

x): the conditional distribution and density functions of yi (at y) given xi = x.
|

The marginal distributions and densities are denoted by Fy, Fx, fy, and fx, respectively.

• y and x: the evaluation points.

•

X

= [0, 1]d and

Y

= [0, 1], the support of xi and yi, respectively.

• h: the bandwidth sequence.

• K: the kernel function, and L is the product kernel: L(u) = K(u1)K(u2)

K(ud).

· · ·

• p, q: polynomial expansions. For a univariate argument y, p(u) = (1, u, u2/2,

, up/p!)T,
and for a multivariate argument u, q(u) contains polynomials and interactions up to order q in
increasing order.

· · ·

• P and Q: deﬁned as p(
) and q(
), respectively.
)L(
)K(
·
·
·
·

• eµ and eν: standard basis vectors extracting the µ-th and ν-th element in the expansion of p

and q for univariate and multivariate arguments, respectively.

) the weighting function used in ˇθµ,ν, with its Lebesgue density denoted by g(
• G(
).
·
·

• Some matrices

p (u) P (u)T g(y + hu)du,

ˆSy =

1
nh

y

P

y

T

,

yi

−
h

Sy =

cy,ℓ =

Sx =

cx,m =

Y−y
h

Z

Y−y
h

Z

X −x
h

Z

X −x
h

Z

uℓ
ℓ!

P (u) g(y + hu)du,

ˆcy,ℓ =

1
nh

q (v) Q (v)T fx(x + hv)dv,

ˆSx =

1
nhd

vm
m!

Q (v) fx(x + hv)dv,

ˆcx,m =

1
nhd

Ty =

(u1 ∧

Y−y
h

Z Z

u2)P (u1) P (u2)T g(y + hu1)g(y + hu2)du1du2,

Tx =

X −x
h

Z

Q (v) Q (v)T fx(x + hv)dv,

ˆTx =

1
nhd

ˆRy,x =

1
n2h1+d+µ+

ν

|

|

¯Ry,x =

1
nh1+d+µ+

ν

|

|

n

n

j=1
X
n

i=1
X

i=1 (cid:18)ZY
X

yj

yj)P

(cid:16)
u

u)P

(cid:16)

−
h

y

−
h

1(yi

1(yi

≤

≤

5

n

yi

p

i=1
X
n

i=1
X
n

(cid:16)
1
ℓ!

q

i=1
X
n

(cid:16)

−
h

yi

(cid:17)
y

−
h

(cid:18)
xi

1
m!

x

−
h

xi

(cid:18)

i=1
X

(cid:16)
ℓ

(cid:19)

Q

(cid:17)

−
h

x

yi

P

(cid:18)
xi

−
h

y

T

,

(cid:19)
T

,

(cid:16)
x

(cid:17)

−
h

(cid:17)

xi

m

Q

(cid:19)

(cid:18)

x

−
h

,

(cid:19)

n

xi

Q

i=1
X
y

Q

(cid:18)

xi

(cid:17)

(cid:16)

−
h

x

−
h

dG(u)

Q

(cid:17)

(cid:19)

(cid:16)

xi

x

−
h

T

,

(cid:19)

Q

(cid:18)

x

(cid:19)

T

,

(cid:17)
xi

−
h

x

T

.

(cid:17)

• Equivalent kernels:

ˇK ◦µ,ν,h (a, b; y, x) =

1
hµ+

ν

|

|

eT
µ S−
y

1

ˆK ◦µ,ν,h (a, b; y, x) =

1
hµ+

ν

|

|

eT
µ

ˆS−
1
y 

1(a

yj)

−

≤

1(a

u)

−

≤

ˆF (u
b)
|

1
h

P

b)
|

(cid:18)
1
h

(cid:17)

(cid:17)
ˆF (yj

u

y

−
h

(cid:19)

dG(u)

yj

y

−
h

P

(cid:16)

1
hd Q

(cid:21)
1
hd Q

(cid:18)

(cid:20)ZY (cid:16)
n
1
n

j=1 (cid:16)
X

K ◦µ,ν,h (a, b; y, x) =

Kµ,ν,h (a, b; y, x) =

1
hµ+

1
hµ+

ν

|

|

ν

|

|

• Some rates:



eT
µ S−
y

1

(cid:20)ZY (cid:16)

eT
µ S−
y

1

1(a

(cid:20)ZY

1(a

u)

≤

b)
F (u
|

−

P

u)

1
h

≤

u

−
h

(cid:18)

u

1
h

P

(cid:18)

dG(u)

y

−
h

(cid:19)
1
hd Q

(cid:19)

(cid:21)

(cid:17)
y

(cid:18)
b

b

−
h

b

−
h

x

T

x

ˆS−
1
x eν ,

(cid:19)
T

ˆS−
1
x eν ,

(cid:19)

x

T

(cid:19)

S−

1
x eν ,



(cid:17)


dG(u)

b

−
h

(cid:18)

1
hd Q
(cid:21)
x

T

−
h

(cid:18)
1
x eν.

S−

(cid:19)

rB = hq+1

−|

ν

| + hp+1
−

µ,

rV =

1
ν

nhd+2
|

,

1

+2µ

−

|

r

rVE = hq+ 1

2 +

log n
nhd+1 ,

r

rSE =

log nrVE,

rSA =

p

SA-1.2 Overview

rBE =



logd+1 n

nhd+1

1
√nhd
1
√nhd+1

1
2d+2

.

!

if µ = 0, and θ0,0

= 0 or 1

if µ > 0, or θ0,0 = 0 or 1

,

In this subsection we provide an overview of the main results. Underlying assumptions and precise
statements of the lemmas and theorems will be given in later sections. First consider ˇθµ,ν(y, x),
and we employ a conditional expectation decomposition:

ˇθµ,ν(y, x) = h−

µ

−|

ν

|eT

µ S−
y

1

1
n

"

n

i=1 (cid:18)ZY
X

n

xi)
F (u
|

u

y

−
h

1
h

P

(cid:16)

dG(u)

(cid:17)

(cid:19)

xi

−
h

1
hd Q

(cid:16)

x

T

ˆS−
1
x eν

#

(cid:17)
xi

+ h−

µ

−|

ν

|eT

µ S−
y

1

1
n

"

1(yi

i=1 (cid:18)ZY (cid:16)
X

u)

−

≤

xi)
F (u
|

(cid:17)

u

y

−
h

1
h

P

(cid:16)

dG(u)

(cid:17)

(cid:19)

1
hd Q

(cid:16)

−
h

x

T

#

(cid:17)

ˆS−
1
x eν.

As we will show in Section SA-2, the ﬁrst term above consists of the centering of the estimator (i.e.,
the parameter of interest θµ,ν(y, x)) and the smoothing bias. The second term, on the other hand,
gives the asymptotic representation of the estimator. To be precise, we have

ˇθµ,ν(y, x)

−

θµ,ν(y, x) =

1
n

n

K ◦µ,ν,h (yi, xi; y, x)

i=1
X
+ OP

(cid:18)

hq+1

−|

ν

| + hp+1
−

µ +

Vµ,ν(y, x)

q

log n
√nhd

.

(cid:19)

6

6
 
As a result, we can focus on establishing properties of the the ﬁrst term, which provides an equivalent
kernel expression. Denote its variance by Vµ,ν(y, x). Then we show that the standardized process,

¯Sµ,ν(y, x) =

1

n

Vµ,ν(y, x)

K ◦µ,ν,h (yi, xi; y, x) ,

n

i=1
X

p

is approximately normally distributed both pointwise and uniformly for y

and x

. To be

even more precise, we establish a strong approximation result, meaning that there exists a copy
¯S′µ,ν(y, x), and a Gaussian process Gµ,ν(y, x) with the same covariance structure, such that

∈ Y

∈ X

sup
,x

y

∈Y

∈X (cid:12)
(cid:12)

¯S′µ,ν(y, x)

Gµ,ν(y, x)

= OP

−

(cid:12)
(cid:12)

1
2d+2

.

logd+1 n
nhd+1

!

Together with a feasible variance-covariance estimator, the strong approximation result not only

allows us to construct conﬁdence bands for the target parameter and test shape restrictions, but

also provides an explicit characterization of the coverage error probability for those procedures. On

nhd+2
|

ν

1

1

1

ν

≥

for µ

a related note, the (leading) variance of our estimator has the order
nhd+2µ+2
|
|−
nhd+1
to be
(cid:0)

for µ = 0, and
1. For example, setting µ = 1 and ν = 0, we have the leading variance

for conditional density estimation.

−
1
−
(cid:1)

µ is the order of the leading smoothing bias, and
Inside the remainder term, hq+1
(cid:0)
Vµ,ν(y, x)/(nhd) arises from the linearization step which replaces the random matrix ˆSx
(log n)
by its large-sample analogue Sx. It is worth mentioning that the order of the remainder term is
uniformly valid for y

, which is why an extra logarithmic factor is present.

| + hp+1
−

and x

p

−|

(cid:1)

(cid:0)

(cid:1)

−

ν

|

Now consider the other estimator, ˆθµ,ν(y, x). While it is not possible to take a conditional
expectation, we can still “center” the estimator with the conditional distribution function. That

∈ Y

∈ X

is,

ˆθµ,ν(y, x) = h−

µ

−|

ν

|eT
µ

ˆS−
1
y 

1
n2

n

n

i=1
X

j=1
X

F (yj

1
h

xi)
|

yj

y

−
h

1
hd Q

(cid:16)

(cid:17)

xi

−
h

P

(cid:16)



n

n

1(yi

i=1
X

j=1 (cid:16)
X

yj)

−

≤

F (yj

xi)
|

(cid:17)

1
h

P

(cid:16)

yj

y

−
h

(cid:17)

1
hd Q

(cid:16)

x

T

(cid:17)




xi

ˆS−
1
x eν

ˆS−
1
x eν .

−
h

x

T

(cid:17)





+ h−

µ

−|

ν

|eT
µ

ˆS−
1
y 

1
n2



As before, the ﬁrst term captures the target parameter and the smoothing bias. The analysis of the

second term is more involved. Besides the asymptotic linear representation term, it also consists of

a leave-in bias term (since the same observation is used twice) and a second order U-statistic. We

7

 
show that the following expansion holds uniformly for y

and x

:

∈ X

∈ Y

ˆθµ,ν (y, x)

−

θµ,ν(y, x) =

1
n

n

K ◦µ,ν,h (yi, xi; y, x)

i=1
X
+ OP

(cid:18)

hq+1

−|

ν

| + hp+1
−

µ +

Vµ,ν(y, x)

q

log n
√nhd

+

log n

√n2hd+2µ+2
|

ν

|

+1

.

(cid:19)

Here, the contribution of the U-statistic is represented by the order log n/(√n2hd+2µ+2
|
remainder term. Interestingly, this term is negligible compared to the standard error,
provided that log n/(nh2)

.

ν

+1) in the
|
Vµ,ν(y, x),

p

The above demonstrates that important large-sample properties of the local regression based
estimator, ˆθµ,ν(y, x) – such as pointwise and uniform normal approximation – stem from the equiv-
alent kernel representation. Here we note that the representation holds by setting G = Fy.
In
other words, ˆθµ,ν(y, x) is ﬁrst-order asymptotically equivalent to ˇθµ,ν(y, x) with the (infeasible)
local smoothing using the marginal distribution Fy.

→ ∞

SA-1.3 Assumptions

We make the following assumptions on the joint distribution, the kernel function, and the weighting

G.

Assumption SA-DGP (Data generating process)
(i)

n is a random sample from the joint distribution F supported on

i

yi, xi
{

}1
≤

≤

(ii) The joint density, f , is continuous and is bounded away from zero.
(iii) θ2,0 exists and is continuous.

= [0, 1]1+d.

Y × X

Assumption SA-K (Kernel)

The kernel function K is nonnegative, symmetric, supported on [

1, 1], Lipschitz continuous, and

−

integrates to one.

Assumption SA-W (Weighting function)

The weighting function G is continuously diﬀerentiable with a Lebesgue density denoted by g.

SA-2 Pointwise Large-sample Properties

We ﬁrst present several uniform convergence results which will be used later to establish pointwise

and uniform properties of our estimators.

Lemma SA-2.1 (Matrix convergence)

8

Let Assumptions SA-DGP, SA-K, and SA-W hold with h

0, nhd/ log n

→

→ ∞

, and G = Fy. Then

ˆSy

−

Sy

= OTC

log n
nh !

,

 r

sup
y

∈Y (cid:12)
(cid:12)
(cid:12)

sup
x

∈X (cid:12)
(cid:12)
(cid:12)

sup
x

(cid:12)
(cid:12)
(cid:12)
Sx

(cid:12)
(cid:12)
(cid:12)
Tx

ˆSx

−

ˆTx

−

= OTC

log n
nhd

,

!

 r

= OTC

log n
nhd

.

!

 r

, then

→ ∞

∈X (cid:12)
(cid:12)
(cid:12)
If in addition that nhd+1/ log n

(cid:12)
(cid:12)
(cid:12)

ˆcy,ℓ
|

−

cy,ℓ

|

= OTC

log n
nh !

,

 r

ˆcx,m
|

−

cx,m

|

= OTC

log n
nhd

,

!

 r

sup
y

∈Y

sup
x

∈X

sup
,x

y

∈Y

∈X (cid:12)
(cid:12)
(cid:12)

eT
µ S−
y

1

¯Ry,x

E

¯Ry,x

−

X
|

(cid:0)

(cid:2)

(cid:3)(cid:1) (cid:12)
(cid:12)
(cid:12)

= OTC (r1) , where r1 =




log n
nhd+2µ+2|ν|
log n
nhd+2µ+2|ν|−1

if µ = 0

if µ > 0

.

q

q

We now follow the decomposition in Section SA-1.2 and study the leading bias of our estimators.



Lemma SA-2.2 (Bias)

Let Assumptions SA-DGP, SA-K and SA-W hold with h
θµ′,ν′ exists and is continuous for all µ′ +

= max

→

ν′|
|

q + 1 + µ, p + 1 +
{

ν
|

|}

→ ∞
. Then

0 and nhd/ log n

. In addition,

n

eT
µ S−
y

1

1
nhµ+

ν

|

|

"

i=1 (cid:18)ZY
X

= θµ,ν(y, x) + Bµ,ν (y, x) + oP

xi)
F (u
|

1
h

u

P

−
h

y

dG(u)

(cid:16)

−|

ν

(cid:17)
| + hp+1
−

(cid:19)

,

µ

(cid:17)

hq+1
(cid:16)

xi

−
h

1
hd Q

(cid:16)

x

T

#

(cid:17)

ˆS−
1
x eν

where

Bµ,ν(y, x) = hq+1

−|

ν

|

Similarly,

θµ,m(y, x)cT

x,mS−

1
x eν

+hp+1
−

µ θp+1,ν(y, x)cT

y,p+1S−

1
y eµ

.

=q+1

m
X|
|

B(i),q+1(y, x)
{z

|

}

B(ii),p+1(y, x)
{z

|

}

eT
µ

ˆS−
1
y 

1
n2hµ+

ν

|

|

n

n

i=1
X

j=1
X

F (yj

1
h

xi)
|

= θµ,ν(y, x) + Bµ,ν(y, x) + oP



yj

P

y

−
h

1
hd Q

(cid:16)
| + hp+1
−

(cid:17)
µ

ˆS−
1
x eν

xi

−
h

(cid:16)

x

T

(cid:17)





ν

−|

hq+1
(cid:16)

.

(cid:17)

For future reference, we deﬁne the order of the leading bias as

rB = hq+1

−|

ν

| + hp+1
−

µ.

Remark SA-2.1 (Higher-order bias) Because the leading bias established in the lemma can

9

be exactly zero, one may need to extract higher-order terms for bandwidth selection:

Bµ,ν(y, x) = hq+1
+ hq+2

ν

−|

ν

−|

µB(ii),p+1(y, x)
|B(i),q+1(y, x) + hp+1
−
µB(ii),p+2(y, x) + hp+q+2
|B(i),q+2(y, x) + hp+2
−
−

ν

µ

−|

|B(iii),p+1,q+1(y, x),

where

B(i),q+2(y, x) =

=q+2

m
X|
|

θµ,m(y, x)cT

x,mS−

1
x eν,

B(ii),p+2(y, x) = θp+2,ν(y, x)cT

y,p+2S−

1
y eµ,

B(iii),p+1,q+1(y, x) = eT

µ S−

1

=q+1

m
X|
|

θp+1,m(y, x)cT

x,m

S−

1
x eν.



y cy,p+1 

ν

µ

−

−|

Note that the last term, hp+q+2

|B(iii),p+1,q+1(y, x), is present only if µ = p and

= q.

ν
|

|

(cid:4)

Next we study the leading variance of our estimator, deﬁned as

Vµ,ν(y, x) = V

1
n

"

n

i=1
X

K ◦µ,ν,h (yi, xi; y, x)

.

#

Lemma SA-2.3 (Variance)

Let Assumptions SA-DGP, SA-K and SA-W hold with h
(i) µ = 0 and θ0,0

= 0 or 1:

→

0 and nhd/ log n

. Then

→ ∞

V0,ν(y, x) =

θ0,0(y, x)(1

θ0,0(y, x))

eT
ν S−

x TxS−

1
x eν

1

−

1
nhd+2
|

ν

|

(cid:16)
(ii) µ = 0 and θ0,0 = 0 or 1: V0,ν(y, x) has the order
(iii) µ > 0:

1
nhd+2|ν|−1 .

+ O

1
nhd+2
|

(cid:18)

.

ν

1

|−

(cid:19)

(cid:17)

Vµ,ν(y, x) =

1
ν

|

nhd+2
|

+2µ

−

(cid:16)
For future reference, we will deﬁne

θ1,0(y, x)

1

eT
µ S−

y TyS−

1
y eµ

1

eT
ν S−

x TxS−

1
x eν

1

(cid:17)(cid:16)

1

nhd+2µ+2
|

ν

|−

2

(cid:18)

.

(cid:19)

+ O

(cid:17)

rV =

1
ν

nhd+2
|

.

1

+2µ

−

|

r

Remark SA-2.2 (Vanishing boundary variance when µ = 0) In case (ii), the true conditional

distribution function is 0 or 1, which is why the leading variance shrinks faster. We do not provide
(cid:4)

a formula as the leading variance in this case takes a complicated form.

Now, we propose two estimators for the variance that are valid for all three cases of Lemma

SA-2.3, and hence will be useful for establishing a self-normalized distributional approximation

10

6
later. Deﬁne

ˇVµ,ν(y, x) =

ˆVµ,ν(y, x) =

1
n2

1
n2

n

i=1
X
n

i=1
X

ˇK ◦µ,ν,h (yi, xi; y, x)2 ,

ˆK ◦µ,ν,h (yi, xi; y, x)2 .

Note that ˆVµ,ν(y, x) is simply the plug-in variance estimator for ˆθµ,ν(y, x) and ˇVµ,ν(y, x) is the
plug-in variance estimator for ˇθµ,ν(y, x). The next lemma provides pointwise convergence results
for the two variance estimators.

Lemma SA-2.4 (Variance estimation)

Let Assumptions SA-DGP, SA-K and SA-W hold with h
θ0,ν exists and is continuous for all
(i) µ = 0 and θ0,0

q + 1. Then

= 0 or 1:

ν
|

| ≤

0 and nhd+1/ log n

→

. In addition,

→ ∞

ˇV0,ν(y, x)

V0,ν(y, x)

−
V0,ν(y, x)

(ii) µ > 0, or θ0,0 = 0 or 1:

(cid:12)
(cid:12)
(cid:12)

= OP

hq+1 +

log n
nhd

.

!

r

(cid:12)
(cid:12)
(cid:12)

ˇVµ,ν(y, x)

Vµ,ν(y, x)

−
Vµ,ν(y, x)

= OP

hq+ 1

2 +

log n
nhd+1

.

!

r

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Let G = Fy, then the same conclusions hold for ˆVµ,ν(y, x).

Next, we study the large-sample distributional properties of the infeasible, standardized statis-

tic

¯Sµ,ν(y, x) =

1

n

Vµ,ν(y, x)

K ◦µ,ν,h (yi, xi; y, x) .

n

i=1
X

p

Note that this is equivalent to the scaled asymptotic linear representation of the estimator.

Theorem SA-2.1 (Asymptotic normality)

Let Assumptions SA-DGP, SA-K and SA-W hold with h

0. Then

→

sup
R
u

P

¯Sµ,ν(y, x)

u

≤

−

Φ(u)

= O (rBE) , where rBE =

∈

(cid:3)

(cid:2)

(cid:12)
(cid:12)
(cid:12)
While the theorem focuses on asymptotic normality of the infeasible t-statistic, ¯S◦µ,ν(y, x), we
show in the following remark that similar conclusions can be made for the t-statistics constructed
with the estimators, ˆθµ,ν(y, x) and ˇθµ,ν(y, x).



(cid:12)
(cid:12)
(cid:12)

1
√nhd
1
√nhd+1




if µ = 0, and θ0,0

= 0 or 1

.

if µ > 0, or if θ0,0 = 0 or 1

11

6
 
 
6
Remark SA-2.3 (Asymptotic normality of standardized statistics) We ﬁrst introduce the

statistic

ˇS◦µ,ν(y, x) =

ˇθµ,ν(y, x)

E

ˇθµ,ν(y, x)
X
|

−
Vµ,ν(y, x)
(cid:2)

,

(cid:3)

which is based on ˇθµ,ν(y, x).
(In the main paper we directly center all statistics at the target
parameter θµ,ν. For clarity, however, we will separate the discussion on distributional convergence
from the smoothing bias in this supplemental appendix. This is reﬂected by the superscript “circle.”)

p

By combining the results of Lemmas SA-2.1 and SA-2.3, we have

y

As a result,

ˇS◦µ,ν(y, x)

sup
,x

∈Y

∈X (cid:12)
(cid:12)

¯Sµ,ν(y, x)

= OTC

−

(cid:12)
(cid:12)

log n
√nhd

.

(cid:19)

(cid:18)

sup
R
u

∈

(cid:2)

(cid:12)
(cid:12)

P

ˇS◦µ,ν(y, x)

Φ(u)

= O

u

≤

−

(cid:3)

(cid:12)
(cid:12)

log n
√nhd

(cid:18)

+ rBE

.

(cid:19)

To present the pointwise distributional approximation result for the estimator ˆθµ,ν(y, x), we

deﬁne the following statistic

ˆS◦µ,ν(y, x) =

1

nhd+µ+

ν

|

|

Vµ,ν(y, x)

p

n

i=1
X

eT
µ

ˆS−
1
y 

1
n



n

j=1 h
X

1(yi

yj)

≤

xi

−
h

Q

(cid:18)

F (yj

xi)
|

i

1
h

P

(cid:16)

yj

y

−
h

T

ˆS−
1
x eν .

−

x

(cid:19)

(cid:17)





It is worth mentioning that ˆS◦µ,ν(y, x) is not exactly centered, that is, it is not mean zero. Never-
theless, by the results of Lemmas SA-2.1 and SA-2.3, and the concentration inequality for second

order U-statistics in Lemma SA-7.3, we have

sup
,x

y

∈Y

∈X (cid:12)
(cid:12)
(cid:12)

ˆS◦µ,ν(y, x)

−

ˇS◦µ,ν(y, x)
(cid:12)
(cid:12)
(cid:12)

= OTC

log n
√nh2

,

(cid:19)

(cid:18)

for some constants c1, c2, and c3. Then we can conclude that the coverage error satisﬁes

P

ˆS◦µ,ν(y, x)
h

≤

u

i

−

sup
R
u

∈

(cid:12)
(cid:12)
(cid:12)

Φ(u)

= O

(cid:12)
(cid:12)
(cid:12)

log n
√nhd
∨

2

(cid:18)

+ rBE

.

(cid:19)

(cid:4)

SA-3 Uniform Large-sample Properties

To conduct statistical inference on the entire function θµ,ν, such as constructing conﬁdence bands
or testing shape restrictions, we need uniform distributional approximations to our estimators. In

12

this section, we will consider large-sample properties of our estimator which hold uniformly on

= [0, 1]d+1. We ﬁrst establish the uniform convergence rate of our estimator.

Y × X
Lemma SA-3.1 (Uniform rate of convergence)

Let Assumptions SA-DGP, SA-K and SA-W hold with h
θµ′,ν′ exists and is continuous for all µ′ +
(i) µ = 0:

= max

ν′|
|

→

q + 1 + µ, p + 1 +
{

ν
|

|}

→ ∞

. Then

0 and nhd+1/ log n

. In addition,

sup
,x

∈Y

∈X (cid:12)
(cid:12)

ˇθ0,ν(y, x)

−

θ0,ν(y, x)

= OTC

hq+1

−|

ν

| + hp+1 +

(cid:12)
(cid:12)

log n
ν
nhd+2
|

;

| !

r

y

(ii) µ > 0:

ˇθµ,ν (y, x)

θµ,ν(y, x)

= OTC

hq+1

−|

ν

| + hp+1
−

µ +

−

log n
nhd+2µ+2
|

.

ν

1 !

|−

r

sup
,x

y

∈Y

∈X (cid:12)
(cid:12)

In the next lemma, we characterize the uniform convergence rate of the variance estimators

introduced in the previous section.

Lemma SA-3.2 (Uniform variance estimation)

Let Assumptions SA-DGP, SA-K and SA-W hold with h
θ0,ν exists and is continuous for all

q + 1. Then

ν
|
| ≤
Vµ,ν(y, x)

ˇVµ,ν(y, x)

−
Vµ,ν(y, x)

sup
,x

y

∈Y

∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)

Let G = Fy, then the same conclusions hold for ˆVµ,ν(y, x).

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0 and nhd+1/ log n

→

. In addition,

→ ∞

= OTC (rVE) , where rVE = hq+ 1

2 +

log n
nhd+1 .

r

Now, we introduce the Studentized processes for each of the estimators, ˆθµ,ν and ˇθµ,ν:

ˇT◦µ,ν (y, x) =

Vµ,ν(y, x)
ˇVµ,ν(y, x)

s

ˇS◦µ,ν(y, x),

ˆT◦µ,ν (y, x) =

Vµ,ν(y, x)
ˆVµ,ν(y, x)

s

ˆS◦µ,ν(y, x).

In the following lemma we study the error that arises from the Studentization of our estimators.

Lemma SA-3.3 (Studentization error)

Let Assumptions SA-DGP, SA-K and SA-W hold with h
θ0,ν exists and is continuous for all

q + 1. Then

ν
|

| ≤

0 and nhd+1/ log n

→

. In addition,

→ ∞

sup
,x

y

∈Y

∈X (cid:12)
(cid:12)

ˇT◦µ,ν (y, x)

−

ˇS◦µ,ν(y, x)
(cid:12)
(cid:12)

ˆS◦µ,ν(y, x).

−

The same holds for ˆT◦µ,ν (y, x)

= OTC (rSE) ,

where rSE =

log nrVE.

p

Our next goal is to establish a uniform normal approximation to the process ¯Sµ,ν(y, x). We

ﬁrst provide a few important properties of the equivalent kernel K ◦µ,ν,h(a, b; y, x).

13

 
 
Lemma SA-3.4 (Boundedness and compact support)

Let Assumptions SA-K and SA-W hold. Then
(i) Both Kµ,ν,h (a, b; y, x) and K ◦µ,ν,h (a, b; y, x) are bounded:

sup
a,b,y,x |

Kµ,ν,h (a, b; y, x)

+

|

K ◦µ,ν,h (a, b; y, x)
|

|

= O

h−

d

−

µ

−|

ν

|

.

(cid:16)

(cid:17)

(ii) For any y and x, Kµ,ν,h (
and all µ

1.

≥

; y, x) is supported within an h-neighborhood of (y, xT)T for all ν

,
·

·

Lemma SA-3.5 (Lipschitz continuity)
Let Assumptions SA-K and SA-W hold. Then
(i) Both Kµ,ν,h (a, b; y, x) and K ◦µ,ν,h (a, b; y, x) are Lipschitz continuous with respect to a and b:

sup
b′
b

−

a

−

a′

+

|

|

|

Kµ,ν,h (a, b; y, x)

>0,y,x (cid:12)
(cid:12)
(cid:12)

|

−

Kµ,ν,h (a′, b′; y, x)
(cid:12)
a′|
(cid:12)
(cid:12)
= O

a
|

−

+

+

K ◦µ,ν,h (a, b; y, x)
(cid:12)
b
(cid:12)
|
(cid:12)
h−

b′|

−
1

−|

−

−

µ

ν

d

.

|

K ◦µ,ν,h (a′, b′; y, x)

−

(cid:12)
(cid:12)
(cid:12)

(ii) Both Kµ,ν,h (a, b; y, x) and K ◦µ,ν,h (a, b; y, x) are Lipschitz continuous with respect to y and x:

(cid:16)

(cid:17)

sup
y′
x
+

|

|

a,b,

y

|

−

Kµ,ν,h (a, b; y, x)

x′

−

>0 (cid:12)
(cid:12)
(cid:12)

|

−

+

Kµ,ν (a, b; y′, x′)
(cid:12)
y′|
(cid:12)
(cid:12)
= O

y
|

+

−

K ◦µ,ν,h (a, b; y, x)
(cid:12)
x
(cid:12)
|
(cid:12)

x′|

K ◦µ,ν (a, b; y′, x′)

−

(cid:12)
(cid:12)
(cid:12)

−
h−

(cid:16)

1

d

µ

−

−

−|

ν

.

|

(cid:17)

Next, we prove a general result on the uniform covering number for function classes consisting

of kernels, which may be of independent interest. Importantly, we allow the kernels in the function

class to take diﬀerent shapes. This is crucial for our purpose as the implied kernel in our estimator
is boundary adaptive, and hence will take diﬀerent forms for interior and boundary evaluation

points.

Lemma SA-3.6 (Covering number)

Let

=

gz
{

·−
h
}
does not depend on h. Assume

∈

G

, z

[0, 1]d

z

(cid:0)

(cid:1)

be a class of functions, and h > 0. Let c be a generic constant that

(i) boundedness

sup
z

sup
z′

gz(z′)
|

| ≤

c

(ii) compact support

(iii) Lipschitz continuity

c, c]d,

[
−
gz(z′′)

| ≤

gz′′(z)

| ≤

z

∀
c
z′
|

ch−

))
supp(gz(
·

⊆

gz(z′)

sup
z |

gz′(z)

sup
z |

−

−

14

z′′

|

−
1

z′
|

z′′

.
|

−

Then, for any probability measure P , the L1(P )-covering number of the class

satisﬁes

G

, L1(P )
(cid:17)
where c′ is some constant that depends only on c and d.

(2c + 1)d+1ε,
(cid:16)

N

G

c′

1
εd+1 + 1,

≤

1, is clearly suboptimal for very
Remark SA-3.1 (On the covering number) This rate, ε−
small ε. The reason is that when we ﬁx h and consider how the covering number changes as
d, as in this case the class of functions is ﬁxed (c.f. Theorem 2.7.11 in
van der Vaart and Wellner 1996). Such suboptimality is introduced because we prefer a covering
number that depends only on ε (but not h). The result we derived performs better for moderate

0, the optimal rate is ε−

↓

ε

−

d

and large ε (relative to h).

Now consider how the above (a sharper result for moderate and large ε) manifests itself in our
proof. Take a ﬁxed ε. As the bandwidth shrinks to 0, we will be employing ﬁner partitions of [0, 1]d.
However, not all of the sets in the partition matter for bounding the covering number, because there
1 sets carrying a probability mass larger than ε. Given that the functions we consider
are at most ε−
have compact support, most of them become irrelevant in our calculation of the covering number.

Indeed, a function only makes a nontrivial contribution if its support intersects with some set in
d
the (very ﬁne) partition whose P -measure exceeds ε. Therefore, instead of considering all h−
1 term is introduced. (cid:4)
partitions, we only need to focus on ε−

1 of them, which is why an extra ε−

Corollary SA-3.1

Let Assumptions SA-K and SA-W hold, and µ

1. Then the function class,

≥

hd+µ+

|

ν

|K ◦µ,ν,h (

,
·

; y, x) : y
·

∈ Y

, x

∈ X

=

K

n

,

o

satisﬁes

N

ε,

sup
P

K

, L1(P)

(cid:0)

(cid:1)

c

1
εd+2 + 1,

≤

where the supremum is taken over all probability measures on [0, 1]d+1, and the constant c does not
depend on the bandwidth h.

Building on the properties of the equivalent kernel that we just established, we provide a

uniform normal approximation to ¯Sµ,ν(y, x) in the following lemma.

Theorem SA-3.1 (Strong approximation)

Let Assumptions SA-DGP, SA-K and SA-W hold with h

1. Deﬁne

µ

≥

0 and nhd+1/ log n

→

. Also assume

→ ∞

rSA =

logd+1 n
nhd+1

!

1
2d+2

.

15

 
Then there exist two centered processes, ¯S′µ,ν(y, x) and Gµ,ν(y, x), such that (i) ¯Sµ,ν(y, x) and
¯S′µ,ν(y, x) have the same distribution, (ii) Gµ,ν(y, x) is a Gaussian process and has the same co-
variance kernel as ¯Sµ,ν(y, x), and (iii) for some constants c1, c2, and c3,

sup
,x

y

¯S′µ,ν(y, x)

Gµ,ν(y, x)

= OTC (rSA) .

−

∈X (cid:12)
(cid:12)
and c3 can be made arbitrarily large by appropriate choices of c1.

∈Y

(cid:12)
(cid:12)

The Gaussian approximation in the above lemma is not feasible, as its covariance kernel de-

pends on unknowns. To be more precise, the covariance kernel takes the form

Cµ,ν (y, x, y′, x′) = Cov

¯Sµ,ν(y, x), ¯Sµ,ν (y′, x′)

=

Vµ,ν(y, x, y′, x′)
Vµ,ν(y, x)Vµ,ν (y′, x′)

,

(cid:3)

p

where

(cid:2)

1
n

Vµ,ν(y, x, y′, x′) =

Cov

K ◦µ,ν,h (yi, xi; y, x) , K ◦µ,ν,h

yi, xi; y′, x′

.

We consider two estimators of the covariance kernel

(cid:2)

(cid:0)

(cid:1)(cid:3)

ˇCµ,ν(y, x, y′, x′) =

ˇVµ,ν(y, x, y′, x′)
ˇVµ,ν(y, x)ˇVµ,ν (y′, x′)

,

q

ˆCµ,ν (y, x, y′, x′) =

ˆVµ,ν(y, x, y′, x′)
ˆVµ,ν(y, x)ˆVµ,ν (y′, x′)

,

q

and

ˇVµ,ν(y, x, y′, x′) =

ˆVµ,ν(y, x, y′, x′) =

1
n2

1
n2

n

i=1
X
n

i=1
X

ˇK ◦µ,ν,h (yi, xi; y, x) ˇK ◦µ,ν,h

yi, xi; y′, x′

ˆK ◦µ,ν,h (yi, xi; y, x) ˆK ◦µ,ν,h

(cid:0)

(cid:0)

(cid:1)

yi, xi; y′, x′

.

(cid:1)

Lemma SA-3.7 (Uniform consistency of the covariance estimator)

Let Assumptions SA-DGP, SA-K and SA-W hold with h
θ0,ν exists and is continuous for all

q + 1. Then

ν
|

| ≤

0 and nhd+1/ log n

→

. In addition,

→ ∞

sup

y,y′

,x,x′

∈Y

∈X (cid:12)
(cid:12)

ˇCµ,ν (y, x, y′, x′)

Cµ,ν(y, x, y′, x′)

= OTC (rVE) ,

−

(cid:12)
(cid:12)

where rVE is deﬁned in Lemma SA-3.2. Let G = Fy, then the same conclusion holds for ˆCµ,ν (y, x, y′, x′).

Lemma SA-3.8 (Gaussian comparison)

16

[Omitted long matching line]

P

sup
R (cid:12)
u
(cid:12)
∈
(cid:12)
(cid:12)
P
(cid:12)
sup
R (cid:12)
u
(cid:12)
∈
(cid:12)
(cid:12)
(cid:12)

sup
,x

∈Y

∈X

sup
,x

∈Y

∈X

"

y

"

y

ˇGµ,ν(y, x)
|

| ≤

ˆGµ,ν(y, x)
|

| ≤

Y, X

# −

Y, X

# −

P

P

"
y

"
y

sup
,x

∈Y

∈X

Gµ,ν(y, x)
|

| ≤

u

sup
,x

∈Y

∈X

Gµ,ν(y, x)
|

| ≤

u

= OP ((log n)√rVE) ,

= OP ((log n)√rVE) .

u

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Theorem SA-3.2 (Feasible normal approximation)

Let Assumptions SA-DGP, SA-K and SA-W hold with h
θ0,ν exists and is continuous for all

q + 1. Also assume µ

→

0 and nhd+1/ log n
1. Then

. In addition,

→ ∞

ν
|

| ≤

sup
,x

∈Y

∈X

sup
,x

∈Y

∈X

"
y

"
y

ˇT◦µ,ν (y, x)
|

| ≤

u
# −

ˆT◦µ,ν (y, x)
|

| ≤

u
# −

P

P

"

y

"

y

sup
,x

∈Y

∈X

sup
,x

∈Y

∈X

P

sup
R (cid:12)
u
(cid:12)
∈
(cid:12)
(cid:12)
P
(cid:12)
sup
R (cid:12)
u
(cid:12)
∈
(cid:12)
(cid:12)
(cid:12)

SA-4 Applications

ˇGµ,ν(y, x)
|

| ≤

ˆGµ,ν(y, x)
|

| ≤

u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

Y, X

Y, X

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= OP

log nrSA + (log n)√rVE

(cid:16)p

= OP

log nrSA + (log n)√rVE

(cid:16)p

,

.

(cid:17)

(cid:17)

SA-4.1 Conﬁdence Bands

A natural corollary of Theorem SA-3.2 is that one can employ critical values computed from
ˇGµ,ν(y, x) and ˆGµ,ν(y, x) to construct conﬁdence bands. To be very precise, deﬁne

ˇcvµ,ν(α) = inf

ˆcvµ,ν(α) = inf

u : P

(

"

y

u : P

(

"

y

sup
,x

∈Y

∈X

sup
,x

∈Y

∈X

ˇGµ,ν(y, x)
|

| ≤

u

ˆGµ,ν(y, x)
|

| ≤

u

Then level (1

−

α) conﬁdence bands can be constructed as

Y, X

# ≥

Y, X

# ≥

1

1

,

.

α

)

−

α

)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˇ
µ,ν(1
C

−

ˆ
µ,ν(1
C

−

α) =

ˇθµ,ν(y, x)

(cid:26)

α) =

ˆθµ,ν(y, x)

(cid:26)

±

±

ˇcvµ,ν(α)

ˇVµ,ν(y, x) : (y, x)

q

ˆcvµ,ν(α)

ˆVµ,ν(y, x) : (y, x)

q

∈ Y × X

∈ Y × X

,

,

(cid:27)

(cid:27)

whose coverage error is given in the following theorem.

Theorem SA-4.1 (Conﬁdence band)
Consider the setting of Theorem SA-3.2. In addition, θµ′,ν′ exists and is continuous for all µ′+

=

ν′|
|

17

max

q + 1 + µ, p + 1 +
{

ν
|

. Then

|}

P

θµ,ν(y, x)

ˇ
µ,ν(1
C

∈

−

α),

∀

(y, x)

∈ Y × X

≥

(cid:2)
θµ,ν(y, x)

P

h

ˆ
µ,ν(1
C

∈

−

α),

∀

(y, x)

∈ Y × X

(cid:3)

i

≥

α

α

1

1

−

−

−

−

O

O

log n

rSA +

(cid:18)

log n

rSA +

(cid:18)

(cid:18)

p

(cid:18)

p

rB
rV
rB
rV

(cid:19)

(cid:19)

+ (log n)√rVE

+ (log n)√rVE

,

.

(cid:19)

(cid:19)

SA-4.2 Parametric Speciﬁcation Testing

In applications, it is not uncommon to estimate conditional densities or higher-order derivatives

by specifying a parametric family of distributions. While such parametric restrictions may provide
reasonable approximations, it is still worthwhile to conduct speciﬁcation testing. To be speciﬁc,

assume the researcher postulates the following class

θµ,ν(y, x; γ) : γ

∈

n

Γµ,ν

,

o

where Γµ,ν is some compact parameter space. We abstract away from the speciﬁcs of the estimation
technique, and assume that the researcher also picks some estimator (maximum likelihood, mini-
mum distance, etc.) ˆγ. Under fairly mild conditions, the estimator will converge in probability to
some (possibly pseudo-true) parameter ¯γ in the parameter space Γµ,ν . As before, we will denote
the true parameter as θµ,ν(y, x), and consider the following competing hypotheses:

H0 : θµ,ν (y, x; ¯γ) = θµ,ν(y, x) vs. H1 : θµ,ν(y, x; ¯γ)

= θµ,ν(y, x).

The test statistics we employ takes the following form

ˇTPS(y, x) =

ˇθµ,ν(y, x)

θµ,ν(y, x; ˆγ)

−

ˇVµ,ν(y, x)

,

ˆTPS(y, x) =

ˆθµ,ν (y, x)

θµ,ν(y, x; ˆγ)

−

ˆVµ,ν(y, x)

.

q
Theorem SA-4.2 (Parametric speciﬁcation testing)
Consider the setting of Theorem SA-3.2. In addition, θµ′,ν′ exists and is continuous for all µ′+
max

. Assume the parametric estimate satisﬁes

q

q + 1 + µ, p + 1 +
{

ν
|

|}

=

ν′|
|

sup
,x

∈Y

∈X

y

θµ,ν(y, x; ˆγ)
|

θµ,ν (y, x; ¯γ)
|

−

= OTC (rPS) ,

for some rPS. Then under the null hypothesis,

P

P

"

y

"

y

sup
,x

∈Y

∈X

sup
,x

∈Y

∈X

ˇTPS(y, x)
|
|

> ˇcvµ,ν(α)

# ≤

ˆTPS(y, x)
|
|

> ˆcvµ,ν(α)

# ≤

α + O

log n

rSA +

(cid:18)

p

(cid:18)

α + O

log n

rSA +

(cid:18)

p

(cid:18)

rB + rPS
rV

rB + rPS
rV

(cid:19)

(cid:19)

+ (log n)√rVE

+ (log n)√rVE

,

.

(cid:19)

(cid:19)

18

6
SA-4.3 Testing Shape Restrictions

Now consider shape restrictions on the conditional density or its derivatives. Let c(y, x) be a

pre-speciﬁed function, and we study the following one-sided competing hypotheses.

H0 : θµ,ν(y, x)

≤

c(y, x) vs. H1 : θµ,ν (y, x) > c(y, x).

The statistic we employ takes the form

ˇTSR(y, x) =

ˇθµ,ν(y, x)

c(y, x)

−
ˇVµ,ν(y, x)

,

ˆTSR(y, x) =

ˆθµ,ν (y, x)

c(y, x)

.

−
ˆVµ,ν(y, x)

and we will reject the null hypothesis if the test statistic exceeds a critical value.

q

q

Theorem SA-4.3 (Shape restriction testing)
Consider the setting of Theorem SA-3.2. In addition, θµ′,ν′ exists and is continuous for all µ′+
. Then under the null hypothesis,
max

q + 1 + µ, p + 1 +
{

ν
|

|}

=

ν′|
|

P

P

"

y

"

y

sup
,x

∈Y

∈X

sup
,x

∈Y

∈X

ˇTSR(y, x) > ˇcvµ,ν (α)

# ≤

ˆTSR(y, x) > ˆcvµ,ν (α)

# ≤

α + O

log n

rSA +

(cid:18)

p

(cid:18)

α + O

log n

rSA +

(cid:18)

p

(cid:18)

rB + rPS
rV

rB + rPS
rV

(cid:19)

(cid:19)

+ (log n)√rVE

+ (log n)√rVE

,

.

(cid:19)

(cid:19)

SA-5 Bandwidth Selection

We assume throughout this section that µ > 0. Using the bias expression derived in Lemma SA-2.2,

and the leading variance is as characterized in Lemma SA-2.3, we can derive precise expressions

for bandwidth selection.

SA-5.1 Pointwise Asymptotic MSE Minimization

Following from Fan and Gijbels (1996), the pointwise MSE-optimal bandwidth is deﬁned as a min-

imizer of the following optimization problem

h∗p,q,µ,ν (y, x) = argmin

h>0

Vµ,ν(y, x) + B2

µ,ν(y, x)

(cid:2)
The solution to this equation gives an MSE-optimal bandwidth that depends on (i) the order of the

(cid:3)

polynomials, (ii) the order of the derivative to be estimated, and (iii) the position of the evaluation

point.

19

Case 1: q

ν

− |

|

= p

−

µ, odd

In this case, both the leading bias constants, B(i),q+1(y, x) and B(ii),p+1(y, x), are nonzero. There-
fore, the MSE-optimal bandwidth is

h∗p,q,µ,ν (y, x) = argmin

h>0 (cid:20)

1
ν

nhd+2
|

+2µ

−

|

1

=

"

(p + q + 2

−

ν
(d + 2
|
ν

µ

− |

|
)
|

Vµ,ν(y, x) + hp+q+2
−

µ

−|

ν

|

B(i),q+1(y, x) + B(ii),p+1(y, x)

2

1)Vµ,ν (y, x)

(cid:0)

+ 2µ

−

B(i),q+1(y, x) + B(ii),p+1(y, x)

1
d+p+q+|ν|+µ+1

.

1
n #

2

(cid:21)

(cid:1)

Case 2: q

ν

− |

|

= p

−

µ, even; either x or y is at or near the boundary

(cid:0)

(cid:1)

In this case, at least one of the leading bias constants, B(i),q+1(y, x) and B(ii),p+1(y, x), is nonzero.
Therefore, the MSE-optimal bandwidth is the same as in Case 1:

h∗p,q,µ,ν (y, x) = argmin

h>0 (cid:20)

1
ν

nhd+2
|

+2µ

−

|

1

=

"

(p + q + 2

−

ν
(d + 2
|
ν

µ

− |

|
)
|

Vµ,ν(y, x) + hp+q+2
−

µ

−|

ν

|

B(i),q+1(y, x) + B(ii),p+1(y, x)

2

1)Vµ,ν (y, x)

(cid:0)

+ 2µ

−

B(i),q+1(y, x) + B(ii),p+1(y, x)

1
d+p+q+|ν|+µ+1

.

1
n #

2

(cid:21)

(cid:1)

Case 3: q

ν

− |

|

= p

−

µ

= 0, even; both x and y are interior

(cid:0)

(cid:1)

In this case, both leading bias constants are zero. Therefore, the MSE-optimal bandwidth will

depend on higher-order bias terms:

h∗p,q,µ,ν (y, x) = argmin

h>0 (cid:20)

1
ν

nhd+2
|

+2µ

−

|

1

=

"

(p + q + 4

−

ν
(d + 2
|
ν

µ

− |

|
)
|

Vµ,ν(y, x) + hp+q+4
−

µ

−|

ν

|

B(i),q+2(y, x) + B(ii),p+2(y, x)

2

1)Vµ,ν (y, x)

(cid:0)

+ 2µ

−

B(i),q+2(y, x) + B(ii),p+2(y, x)

1
d+p+q+|ν|+µ+3

.

1
n #

2

(cid:21)

(cid:1)

Case 4: q

ν

− |

|

= p

−

µ = 0, even; both x and y are interior

(cid:0)

(cid:1)

As in Case 3, both leading bias constants are zero. The diﬀerence, however, is that the leading

bias will involve an extra term:

h∗p,q,µ,ν (y, x) = argmin

h>0 (cid:20)

1
ν

nhd+2
|

+2µ

−

|

1

=

"

4

(cid:0)

Vµ,ν(y, x) + h4

B(i),q+2(y, x) + B(ii),p+2(y, x) + B(iii),p+1,q+1(y, x)

2

−
B(i),q+2(y, x) + B(ii),p+2(y, x) + B(iii),p+1,q+1(y, x)

|

+ 2µ

1)Vµ,ν (y, x)

ν
(d + 2
|

(cid:0)

1
d+2|ν|+2µ+3

.

1
n #

2

(cid:21)

(cid:1)

(cid:1)

20

6
Case 5: q

ν

− |

|

< p

−

µ, q

ν

− |

|

odd

In this case, the leading bias will involve only one term:

h∗p,q,µ,ν(y, x) = argmin

1
ν

nhd+2
|

Vµ,ν(y, x) + h2q+2
−

2
|

ν

|B(i),q+1(y, x)2

+2µ

−

|

1

(cid:21)

h>0 (cid:20)
ν
(d + 2
|
(2q + 2

=

(cid:20)

+ 2µ
ν
2
|

1)Vµ,ν (y, x)
−
)B(i),q+1(y, x)2
|

|
−

1
d+2q+2µ+1

.

1
n

(cid:21)

Case 6: q

ν

= p

µ

1, q

ν

even; x is interior

−
In this case, the leading bias will involve two terms:

− |

− |

−

|

|

h∗p,q,µ,ν (y, x) = argmin

h>0 (cid:20)

1
ν

nhd+2
|

+2µ

−

|

1

=

"

(p + q + 3

−

ν
(d + 2
|
ν

µ

− |

|
)
|

Vµ,ν(y, x) + hp+q+3
−

µ

−|

ν

|

B(i),q+2(y, x) + B(ii),p+1(y, x)

2

1)Vµ,ν (y, x)

(cid:0)

+ 2µ

−

B(i),q+2(y, x) + B(ii),p+1(y, x)

1
d+p+q+µ+|ν|+2

.

1
n #

2

(cid:21)

(cid:1)

Case 7: q

ν

− |

|

< p

µ

−

−

1, q

ν

− |

|

(cid:0)
even; x is interior

(cid:1)

In this case, the leading bias will involve only one term:

h∗p,q,µ,ν(y, x) = argmin

nhd+2
|

+2µ

−

|

1

Vµ,ν(y, x) + h2q+4
−

2
|

ν

|B(i),q+2(y, x)2

h>0 (cid:20)
ν
(d + 2
|
(2q + 4

=

(cid:20)

+ 2µ
ν
2
|

1)Vµ,ν (y, x)
−
)B(i),q+2(y, x)2
|

|
−

1
d+2q+2µ+3

.

1
n

(cid:21)

Case 8: q

ν

> p

µ, p

µ odd

−
In this case, the leading bias will involve only one term:

− |

−

|

h∗p,q,µ,ν (y, x) = argmin

nhd+2
|

Vµ,ν(y, x) + h2p+2
−

2µB(ii),p+1(y, x)2

+2µ

−

|

1

h>0 (cid:20)
ν
(d + 2
|
(2p + 2

=

(cid:20)

1)Vµ,ν (y, x)
+ 2µ
2µ)B(ii),p+1(y, x)2

−

|
−

1
d+2p+2|ν|+1

.

1
n

(cid:21)

Case 9: q

ν

1 = p

µ, q

ν

even; y is interior

−
In this case, the leading bias will involve two terms:

− |

− |

| −

|

h∗p,q,µ,ν (y, x) = argmin

h>0 (cid:20)

1
ν

nhd+2
|

+2µ

−

|

1

=

"

(p + q + 3

−

ν
(d + 2
|
ν

µ

− |

|
)
|

Vµ,ν(y, x) + hp+q+3
−

µ

−|

ν

|

B(i),q+1(y, x) + B(ii),p+2(y, x)

2

1)Vµ,ν (y, x)

(cid:0)

+ 2µ

−

B(i),q+1(y, x) + B(ii),p+2(y, x)

1
d+p+q+µ+|ν|+2

.

1
n #

2

(cid:21)

(cid:1)

(cid:0)

21

(cid:1)

1
ν

1
ν

(cid:21)

(cid:21)

Case 10: q

ν

1 > p

µ, p

µ even; y is interior

−
In this case, the leading bias will involve only one term:

− |

| −

−

h∗p,q,µ,ν (y, x) = argmin

1
ν

nhd+2
|

Vµ,ν(y, x) + h2p+4
−

2µB(ii),p+2(y, x)2

+2µ

−

|

1

(cid:21)

h>0 (cid:20)
ν
(d + 2
|
(2p + 4

=

(cid:20)

1)Vµ,ν (y, x)
+ 2µ
2µ)B(ii),p+2(y, x)2

−

|
−

1
d+2p+2|ν|+3

.

1
n

(cid:21)

SA-5.2 Rule-of-thumb Bandwidth Selection

This section outlines the methodology that the companion R package, lpcde, uses to construct the
rule-of-thumb bandwidth selection.

The rule-of-thumb estimation uses the following assumptions in order to compute the optimal

bandwidth:

• the data is jointly normal,

• X and Y are independent, and,

• p

µ = q

ν
− |

|

−

are odd.

Using these assumptions, each of the terms in the formula given in Case 1 of Section SA-5.1 are

computed as follows:

1. The densities and relevant derivatives are evaluated based on the joint normal distribution

assumption.

2. Sy, Ty, Tx and Sx matrices are computed by plugging in for the range of the data, the

evaluation point, the respective marginal densities, and the kernel used.

3. Similarly, the cy and cx vectors are computed by using the range of the data, the evaluation

point, kernel function, and the respective marginal densities.

4. Bias and variance estimates are constructed using the relevant entries of the vectors and

matrices.

22

SA-6 Alternative Variance Estimators

SA-6.1 V-statistic variance estimator

We propose here an alternative variance estimator that is quick to implement in practice. We start

by ﬁrst observing that the estimator

θµ,ν(y, x) is a V-statistic. That is,

θµ,ν(y, x) =

b

=

1
n2

1
n2

i,j
X
n

i=1
X

where,

b
yj)eT
µ

S−

1
y P

1(yi

≤

b

a(yi, y)b(xi, x) +

(cid:18)
1
n2

=j
i
X

yj

y

−
h

QT

(cid:19)

(cid:18)

xi

x

−
h

S−

1
x eν

(cid:19)

b

1(yi

≤

yj)a(yj, y)b(xi, x),

(SA-6.1)

a(yj, y) = h1+µeT
µ

S−

1
y P

yj

y

−
h

,

(cid:19)

(cid:18)

b(xi, x) = hd+

|

ν

|eT
ν

S−

1
x Q

xi

x

−
h

.

(cid:19)

(cid:18)

) are scalar functions that are non-zero only for data points that are within
) and b(
Note that a(
·
·

h distance of the evaluation point. The second term in Equation SA-6.1 can now be symmetrized

b

b

and treated as a U-statistic. Applying the Hoeﬀding decomposition to the symmetrized version of

the second term and plugging it back into Equation SA-6.1, we get

E [a(yi, y)b(xi, x)] +

1

E[ui,j]

n

−
n

θµ,ν(y, x) =

b

+

1
n
1
n2

n

n

+

i=1
X
1
−
n

Wµ,ν(y, x)

(a(yi, y)b(xi, x)

−

E [a(yi, y)b(xi, x)]) +

n

1

−
n

Lµ,ν (y, x)

(SA-6.2)

yi, xi]
|

−

E [ui,j])

(ui,j

E [ui,j

yi, xi]
|

−

E [ui,j

yj, xj] + E [ui,j])
|

−

where

and

Lµ,ν(y, x) =

Wµ,ν(y, x) =

1
n

(cid:18)

2 (E [ui,j

1

i
X
n
−
2

(cid:19)

i<j
X

ui,j =

1
2

(1(yi

≤

yj)a(yj, y)b(xi, x) + 1(yj

yi)a(yi, y)b(xi, x))

≤

Dependence on polynomial orders is suppressed for notational simplicity. Since each of the terms

in Equation SA-6.1 are orthogonal, the variance of the estimator can be expressed as the sum of

the variance of each of the terms on the right hand side. Furthermore, we note that the ﬁrst three
terms and Wµ,ν(y, x) have higher-order variance. Thus, we only need to look at the variance of

23

6
Lµ,ν(y, x).

where we know

V (Lµ,ν(y, x)) = V

=

1
n

n

(E [ui,j

2
n

i=1
X
V (2E [ui,j

yi, xi]

|

−

E [ui,j])

!

yi, xi]

|

−

2E [ui,j])

2E [ui,j

|

yi, xi] =

1(yi

Z

u)a(u, y)dFyj |

≤

xi(u)b(xi, x) + F (yi

xi)a(yi, y)b(xi, x).
|

We can expand and simplify this to get

V (Lµ,ν(y, x)) = E

= E

"(cid:18)Z

(cid:20)Z Z
+

1(yi

u)a(u, y)dFyj |

≤

xi(u)b(xi, x) + F (yi

xi)a(yi, y)b(xi, x)
|

(cid:19)

2

#

1(yi

min

u, v
{

≤

)a(u, y)a(v, y)dFyj |
}

1(yi

u)a(u, y)dFyj |
≤
xi)a(yi, y)b(xi, x))2
|

Z
+ (F (yi

xi(u)F (yi

.

i

xi(v)b2(xi, x)

xi(u)dFyj |
xi)a(yi, y)b2(xi, x)
|

Note that this expression is identical to the variance expression derived in the proof of SA-2.3. This

leads to a natural alternative jackknife covariance estimator,

Cµ,ν(y, x, y′, x′) =

1
n

where

b

n

L(i),µ,ν (y, x)

L(i),µ,ν (y′, x′).

i=1
X

b

b

L(i),µ,ν (y, x) =

b

2

−

1

n

ui,j

=i (cid:16)
j
X

−

θµ,ν(y, x)

.

(cid:17)

b

In particular, note that if the two evaluation points are equivalent, we return the variance

estimator,

Cµ,ν(y, x, y, x)

Vµ,ν(y, x) =

≡

L2

(i),µ,ν (y, x)

1

−

1

n

n

i=1
X

b

We can similarly construct a jackknife covariance estimator that takes on the form This estimator
is implemented in the companion R package as the default variance-covariance matrix estimator.

b

b

24

 
6
SA-6.2 Asymptotic variance estimator

An alternative variance estimator is a sample version of the asymptotic variance derived in Lemma SA-2.3.

That is, each of the matrices in the formula are replaced with sample-analogs. That is,

(i) µ = 0:

V0,ν(y, x) =

1
nhd+2
|

ν

|

θ0,0(y, x)(1

−

θ0,0(y, x))

eT
ν

S−
x

1

Tx

S−

1
x eν

(cid:16)

b

b

b

(cid:17)

(ii) µ > 0:

b

b

b

Vµ,ν(y, x) =

1
ν

nhd+2
|

+2µ

−

|

1

θ1,0(y, x)

eT
µ

S−
y

1

Ty

S−

1
y eµ

eT
ν

S−
x

1

Tx

S−

1
x eν

.

b
b
Similarly the covariance can be estimated with the following expression,

b

b

b

b

b

(cid:16)

(cid:17)(cid:16)

(cid:17)

b

Cµ,ν(y, y′; x) =

1
ν

nhd+2
|

+2µ

−

|

1

b
where

θ1,0(y, x)

eT
µ

S−
y

1

Ty,y′

S−

1
y′ eµ

eT
ν

S−
x

1

Tx

S−

1
x eν

.

b

(cid:16)

b

b

b

(cid:17)(cid:16)

b

b

b

(cid:17)

Ty,y′ =

1
n2h2

b

n

i=1 Xj
X

=i (cid:18)

yj

yi

y
−
h ∧

−
h

y′

P

(cid:19)

(cid:18)

yi

y

−
h

yj

−
h

P

(cid:19)

(cid:18)

T

y′

(cid:19)

SA-7 Technical Lemmas and Proofs

SA-7.1 Technical Lemmas

Lemma SA-7.1 (Theorem 1.1 in Rio 1994)
Let z1, z2,
and d

be a class of functions from [0, 1]d to [

· · ·
2. Let

≥

G

1, 1], satisfying

−

, zn be iid random variables with continuous and strictly positive density on [0, 1]d,

N (ε,

sup
P

G

, L1(P ))

c1ε−

c2,

≤

where the supremum is taken over all probability measures on [0, 1]d, and c1 and c2 are constants
. In addition, assume the following measurability condition holds: there exists
that can depend on

a Suslin space

S

G

and a mapping F :

S → G

, such that (s, z)

7→

F(s, z) is measurable. Let

G

TV

= sup
g

sup
1 ([0,1]d) Z[0,1]d
∞
C∞1 ([0, 1]d) is the collection of inﬁnitely diﬀerentiable
where div is the divergence operator, and
functions with values in Rd, support included in [0, 1]d, and supremum norm bounded by 1. Then
on a possibly enlarged probability space, there exists a centered Gaussian process, G, indexed by
,

g(z)divφ(z)dz,

∈G

∈C

φ

G

25

6
such that (i) Cov[G(g), G(g′)] = Cov[g(zi), g′(zi)], and (ii) for any t

c3 log n,

≥

P

√n sup
g

∈G

"

B(g)
|

−

G(g)

| ≥

c3

d−1
d TV

n

q

t + c3t

log n

G

p

e−

t.

# ≤

In the above, B is the empirical process indexed by

:

G

B(g) =

1
√n

n

g(zi)

i=1 (cid:16)
X

−

,

E[g(zi)]
(cid:17)

and c3 is some constant that only depends on d, c1, and c2.

Lemma SA-7.2 (Corollary 5.1 in Chernozhukov, Chetverikov, Kato and Koike 2019)
Let z1, z2 ∈
respectively. Further assume that the diagonal elements in Ω1 are all one. Then

Rℓn be two mean-zero Gaussian random vectors with covariance matrices Ω1 and Ω2,

sup
Rℓn

A

⊆
A rectangular

P [z1 ∈
|

A]

P [z2 ∈

−

A]

| ≤

c

Ω1 −
k

Ω2k∞

log ℓn,

p

where

k · k∞

denotes the supremum norm, and c is an absolute constant.

Lemma SA-7.3 (Equation (3.5) in Gin´e, Lata la and Zinn 2000)

For a degenerate and decoupled second order U-statistic,

n
i,j=1,i

=j hij(xi, ˜xj), the following holds:

P

> t



≤

c exp

1
c

(−

min

t
D

,

"

t
B

(cid:18)

2
3

,

(cid:19)

(cid:18)

t
A

1
2

#)

(cid:19)

,

n

P

(cid:12)

(cid:12)
=j
Xi,j,i
(cid:12)
(cid:12)

(cid:12)
(cid:12)

uij(xi, ˜xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)


where c is some absolute constant, and A, B and D are any constants satisfying

A

≥

max
i,j
≤
≤

1

n

sup
u,v |

uij(u, v)
|

n

Euij(xi, v)2

n 

sup
v (cid:12)
i=1
(cid:12)
X
(cid:12)

(cid:12)
Euij(xi, ˜xj)2.
(cid:12)

B2

≥

max
i,j
≤
≤

1

n

D2

≥

Xi,j=1,i

=j

n

j=1
X

, sup
u (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Euij(u, ˜xj)2





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where

xi, 1
{
copy of

≤

xi, 1
{

≤

i

i

are independent random variables, and

˜xi, 1
{

i

n

}

≤

≤

is an independent

n

}
n

.
}

≤

≤

To apply the above lemma, an additional decoupling step is usually needed. Fortunately, the

decoupling step only introduces an extra constant, but will not aﬀect the order of the tail probability

bound. Formally,

Lemma SA-7.4 (de la Pe˜na and Montgomery-Smith 1995)

26

6
6
6
Consider the setting of Lemma SA-7.3. Then

n

P





(cid:12)
(cid:12)
=j
Xi,j,i
(cid:12)
(cid:12)
(cid:12)
(cid:12)

uij(xi, xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where c is an absolute constant.

> t

c

·

P

c




≤





n

=j
Xi,j,i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

uij(xi, ˜xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

> t

,





As a result, we will apply Lemma SA-7.3 without explicitly mentioning the decoupling step or

the extra constant it introduces.

Lemma SA-7.5 (Theorem 2.1 in Chernozhukov, Chetverikov and Kato 2014)
Let G be a centered and separable Gaussian process indexed by g
g

such that V[G(g)] = 1 for all
G(g)]. Then for all ε > 0,

almost surely. Deﬁne C

G(g) <

∈ G
= E[supg

. Assume supg

∈ G

∈G

∞

G

∈G

P

sup
R
u

∈

G(g)

u

−

sup
g

∈G

"(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

SA-7.2 Proof of Lemma SA-2.1

ε
# ≤

≤

4ε(C

G

+ 1).

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Part (i). We will prove the result for ˆSx. The same proof strategy applies to ˆcx,m, ˆSy, ˆcy,ℓ and ˆTx.

To start, note that

is compact, then for any ηn > 0, one can ﬁnd
1≤ℓ≤Mn Bℓ, where Bℓ := B(xℓ, ηn) is the Euclidean ball centered at xℓ with radius ηn. Deﬁne r =

xℓ : 1
{

Mn

≤

≤

X

}

ℓ

, such that

X ⊆
log n/(nhd).

∪
Then,

sup
x∈X

ˆSx
(cid:12)
(cid:12)
(cid:12)

Sx

−

max
1≤ℓ≤Mn

max
1≤ℓ≤Mn

≤

≤

(cid:12)
(cid:12)
(cid:12)

sup
x∈Bℓ (cid:12)
(cid:12)
ˆSxℓ −
(cid:12)

ˆSx

Sx

−

Sxℓ

(cid:12)
(cid:12)
(cid:12)
1≤ℓ≤Mn

+ sup

ˆSx

ˆSxℓ

−

sup
x∈Bℓ (cid:12)
(cid:12)
(II)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
}

(cid:12)
(cid:12)
(I)
(cid:12)

{z

(cid:12)
(cid:12)
(cid:12)
}

p

+ sup

1≤ℓ≤Mn

Sx

sup
x∈Bℓ |

.

Sxℓ |

−

(III)

Consider (III) ﬁrst. We will take ηn - h, then by the Lipschitz-h−1 continuity of Sx, the third term satisﬁes

|

{z

}

|

{z

|

(III) -

ηn
h

.

Similarly, the random matrix, ˆSx, is the average of Lipschitz-h−1−d continuous functions, which means

(II) -

ηn
h1+d .

Note that the above order is non-probabilistic.

Now consider the ﬁrst term. By employing the union bound, we have that, for any constant c1 > 0,

P [(I) > c1r]

Mn max

1≤ℓ≤Mn

≤

P

To proceed, we recall the formula of ˆSx:

ˆSxℓ −
h(cid:12)
(cid:12)
(cid:12)

Sxℓ

(cid:12)
(cid:12)
(cid:12)

.

> c1r
i

ˆSx =

1
nhd

n

q

i=1
X

(cid:16)

xi

x

−
h

Q

(cid:17)

(cid:16)

xi

−
h

T

x

.

(cid:17)

27

6
6
It is easy to show that the summands in the above satisﬁes

and

1
hd q

V

(cid:20)

(cid:16)

xi

x

−
h

Q

(cid:17)

(cid:16)

xi

−
h

T

x

≤

(cid:21)

(cid:17)

C ′h−d,

(cid:16)
where C ′ is a constant that does not depend on n, h or the evaluation point x. Therefore, we apply Bernstein’s
inequality, which gives

(cid:17)

(cid:17)

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
hd q

xi

−
h

x

Q

xi

−
h

T

x

C ′h−d,

Mn max

1≤ℓ≤Mn

P

ˆSxℓ −

Sxℓ

h(cid:12)
(cid:12)
(cid:12)

> c1r
i

≤

2 exp

= 2 exp

(cid:12)
(cid:12)
(cid:12)

1
2

1
2

−

(cid:26)

−

(cid:26)

1n2r2
c2
3 c1C ′h−dnr + log Mn

nC ′h−d + 1

c2
1 log n
3 c1C ′r + log Mn
C ′ + 1

.

(cid:27)

(cid:27)

To complete the proof, we note that Mn is at most polynomial in n as long as ηn is. Therefore, one can choose ηn
suﬃciently small so that both (II) and (III) become negligible relative to (I), and hence for some constants c1, c2,
and c3,

and c3 can be made arbitrarily large with appropriate choices of c1.

P

sup
x∈X

(cid:20)

ˆSx

Sx

−

> c1r
(cid:21)

≤

c2n−c3 ,

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Part (ii). Next consider eT

µ S−1
y

¯Ry,x

E[ ¯Ry,x

X]
|

, which takes the form

µ S−1
eT
y

¯Ry,x

E[ ¯Ry,x

X]
|

=

−

(cid:0)

(cid:1)

(cid:0)
1
nhµ+|ν|

It is straightforward to see that

−
n

µ S−1
eT
y

i=1
X

Z

(cid:1)

Y−y

1(yi

h (cid:16)

y + hu)

xi)
F (y + hu
|

−

≤

(cid:17)

P(u)g(y + hu)du

1
hd Q
(cid:16)

xi

−
h

x

T

.

(cid:17)

µ S−1
eT
y

Z

Y−y

h (cid:16)

1(yi

≤

y + hu)

xi)
F (y + hu
|

−

P(u)g(y + hu)du

(cid:17)

1
hd Q
(cid:16)

xi

−
h

x

T

(cid:17)

C ′h−d

≤

for some C ′ that holds uniformly for y

. We also have the following bound on the variance

and x

∈ X

∈ Y

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

µ S−1
eT
y

V

"

Z

1(yi

Y−y

h (cid:16)

y + hu)

xi)
F (y + hu
|

−

≤

(cid:17)

P(u)g(y + hu)du

1
hd Q
(cid:16)

xi

−
h

x

T

# ≤

(cid:17)

C ′

h−d

if µ = 0

h−d+1




if µ > 0.

.

Consider the ﬁrst case above (µ = 0). By a discretization
bound due to Bernstein’s inequality

(yℓ, xℓ) : 1
{

≤

ℓ

≤

Mn

of

}

Y × X

, we have the probabilistic



hµ+|ν| max
1≤ℓ≤Mn

P

(cid:20)

(cid:12)
(cid:12)
(cid:12)

eT
µ S−1

y ( ¯Ry,x

E[ ¯Ry,x

X])
|

−

> c1r
(cid:21)

≤

2 exp

= 2 exp

(cid:12)
(cid:12)
(cid:12)

1
2

1
2

−

(cid:26)

−

(cid:26)

nC ′h−d + 1

1n2r2
c2
3 c1C ′h−dnr + log Mn

c2
1 log n
3 c1C ′r + log Mn
C ′ + 1

,

(cid:27)

(cid:27)

provided that we set r =
log n/(nhd). Using arguments similar to those in part (i), it is straightforward to show
that Mn is at most polynomial in n, and the error from discretization can be ignored. This concludes the proof for
the µ = 0 case.

p

28

For µ > 0, we set r =

log n/(nhd−1), and the probabilistic bound takes the form

P

hµ+|ν| max
1≤ℓ≤Mn
(cid:20)

p
y ( ¯Ry,x
µ S−1
eT
(cid:12)
(cid:12)
(cid:12)

E[ ¯Ry,x

X])
|

−

(cid:12)
(cid:12)
(cid:12)

> c1r
(cid:21)

≤

2 exp

1
2

−

(cid:26)

1n2r2
c2

nC ′h−d+1 + 1

3 c1C ′h−dnr + log Mn

(cid:27)

= 2 exp

= 2 exp

1
2

1
2

−

−











log n
c2
1
hd−1
hd−1 + c1C′

3hd

C′

+ log Mn


log n
hd−1

q

c2
1 log n

C ′ + c1C′
3

log n
nhd+1

q


+ log Mn
.




This concludes the proof for the second case, where µ > 0.

SA-7.3 Proof of Lemma SA-2.2

The conditional expectation of ¯Ry,x in ˇθµ,ν is

n

i=1 (cid:20)ZY
X
n

1
nhµ+|ν|

E

"

=

1
nhµ+|ν|

1(yi

u)

1
h

P

≤

u

y

−
h

(cid:16)

(cid:17)

dG(u)

xi)
F (u
|

i=1 (cid:20)ZY
X

u

y

−
h

1
h

P

(cid:16)

(cid:17)

dG(u)

(cid:21)

1
hd Q

xi

−
h

T

x

X

#

(cid:16)
xi

(cid:21)
1
hd Q

(cid:16)

−
h

T

x

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

.

To proceed, we employ a Taylor expansion of the conditional distribution function to order s:

xi) =
F (u
|

Xℓ+|m|≤s

θℓ,m(y, x)

1
ℓ!m!

(u

−

y)ℓ(xi

−

m

x)

+ o

Then, the conditional expectation can be simpliﬁed as





Xℓ+|m|=s

u
|

−

y

ℓ
|

xi
|

−

x

m
|

.





1
n

=

=

n

i=1 (cid:20)ZY
X

Xℓ+|m|≤s

xi)
F (u
|

1
h

P

(cid:16)

θℓ,m(y, x)

(cid:20)ZY

u

y

−
h

1
ℓ!

(u

(cid:17)

−

dG(u)

1
hd Q

(cid:21)

y)ℓ 1
h

P

u

−
h

(cid:16)
y

xi

−
h

x

T

(cid:17)

dG(u)

1
n

(cid:21) "

(cid:17)

+ o





1
ℓ! |

u

y

ℓ 1
h
|

−

P

y

dG(u)

Xℓ+|m|=s (cid:20)ZY
hℓ+|m|θℓ,m(y, x)cy,ℓˆcT

(cid:16)
x,m + oP (hs) .

(cid:12)
(cid:12)
(cid:12)

(cid:17)(cid:12)
(cid:12)
(cid:12)

1
n

(cid:21) "

i=1
X

1
hd

1
m! |

xi

x

m
|

−

Q

(cid:12)
(cid:12)
(cid:12)

(cid:16)

u

−
h

1
hd

1
m!

(xi

−

m

x)

Q

n

i=1
X
n

xi

−
h

x

T

#

(cid:17)

xi

x

−
h

#
(cid:17)(cid:12)
(cid:12)

(cid:12)

(cid:16)

(cid:16)

Xℓ+|m|≤s

We note that

and

Therefore,

E[ˇθµ,ν

x1,
|

· · ·

S−1

y cy,ℓ = eℓ for all 0

ℓ

≤

≤

p,

ˆS−1

x ˆcx,m = em for all 0

m

≤ |

| ≤

q.

, xn] = θµ,ν (y, x) + hq+1−|ν|

θµ,m(y, x)ˆcT

x,m ˆS−1

x eν + hp+1−µθp+1,ν (y, x)cT

y,p+1S−1

y eµ

+ oP

(cid:16)

hq+1−|ν| + hp+1−µ

X|m|=q+1
.

(cid:17)

29

By Lemma SA-2.1, the second term on the right-hand side satisﬁes

hq+1−|ν|

X|m|=q+1

θµ,m(y, x)ˆcT

x,m ˆS−1

x eν = hq+1−|ν|

X|m|=q+1

θµ,m(y, x)cT

x,mS−1

x eν + OP

hq+1−|ν|

log n
nhd

,

!

r

which means we can denote the leading bias as

Bµ,ν (y, x) = hq+1−|ν|

θµ,m(y, x)cT

x,mS−1

x eν + hp+1−µθp+1,ν (y, x)cT

y,p+1S−1

y eµ.

X|m|=q+1
For the second claim of this lemma, we again consider a Taylor expansion

F (yj

xi) =
|

Xℓ+|m|≤s

θℓ,m(y, x)

1
ℓ!m!

(yj

−

y)ℓ(xi

−

m

x)

+ o





Xℓ+|m|=s

yj
|

−

y

ℓ
|

xi
|

−

x

m
|

.





eT
µ

ˆS−1
y

F (yj

h

xi)P
|

(cid:16)

yj

y

−
h

xi

−
h

P

(cid:17)i

(cid:16)

x

T

ˆS−1

x eν

(cid:17)

eT
µ

ˆS−1

y 

θℓ,m(y, x)

1
ℓ!m!

(yj

−

y)ℓ(xi

m

P

x)

−

P

yj
|

−

y

ℓ
|

xi
|

−

x

m
|

yj

−
h

Then

1
n2hd+1+µ+|ν|

=

1
n2hd+1+µ+|ν|

n

i,j=1
X
n

i,j=1
X

+ o

1
n2hd+1+µ+|ν| eT

µ



=θµ,ν (y, x) + hq+1−|ν|



Xℓ+|m|≤s

ˆS−1
y

n


Xℓ+|m|=s
i,j=1
X

θµ,m(y, x)cT

X|m|=q+1

SA-7.4 Proof of Lemma SA-2.3

Let c1 = S−1

y eµ and c2 = S−1

x eν.

yj

y

−
h

P

(cid:16)

y

xi

(cid:16)

x

P



(cid:17)


xi

−
h

−
h

x

T

ˆS−1

x eν

(cid:17)

ˆS−1

x eν




(cid:17)(cid:12)
(cid:12)

(cid:12)

(cid:12)
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
x eν + hp+1−µθp+1,ν(y, x)cT
y,p+1S−1

(cid:16)
y eµ + oP(hq+1−|ν| + hp+1−µ).



(cid:12)
(cid:12)
(cid:12)

(cid:16)

x,mS−1

V

1(yi

(cid:20)ZY
= E

h
V

(cid:20)

(cid:20) ZY

h

u)

−

≤

xi)
F (u
|

1(yi

u)

−

≤

P

1
h

cT
1
i
xi)
F (u
|

u

y

−
h

dG(u)

(cid:16)
cT
1
i

1
h

P

(cid:17)
u

(cid:16)

y

−
h

(cid:17)

1
hd Q

(cid:16)
dG(u)

xi

−
h

1
hd Q

x

T

c2

(cid:17)
xi

(cid:16)

−
h

= E

" Z Z

Y−y

h (cid:16)

F (y + h(u1

u2)

xi)
|

−

∧

F (y + hu1

xi)F (y + hu2
|

xi)
|

cT
2

1
hd Q

(cid:18)

(cid:16)

xi

x

−
h

2

#

(cid:17)(cid:19)

(cid:21)
x

T

c2

X

(cid:21)(cid:21)

(cid:17)
1 P (u1) cT
cT

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 P (u2) g(y + hu1)g(y + hu2)du1du2

(cid:17)

(*)

We make a further expansion:

F (y + h(u1

= F (y

xi)(1
|

−

F (y + hu1

u2)

∧
F (y

xi)
−
|
xi)) + h(u1
|

xi)F (y + hu2
|
u2)f (y

xi)
|

−

∧

xi)
|

h(u1 + u2)f (y

xi)F (y
|

xi) + O(h2).
|

Note that the remainder term, O(h2), holds uniformly for y
function is assumed to have bounded second derivative.

and xi

∈ Y

∈ X

since the conditional distribution

30

 
Therefore,

(*) =

1 cy,0cT
cT

y,0c1

(cid:16)

E

F (y

"

(cid:17)

xi)(1

|

F (y

xi))

|

−

cT
2

1
hd

Q

xi −
h

(cid:18)

(cid:18)

h

cT
1

y,0 + cy,0cT
y,1

cy,1cT
(cid:16)

c1

E

(cid:17)

i

xi)F (y

f (y

|

xi)

|

cT
2
(cid:20)

2

−

h
µ e0E

= eT

x

2

+ h

(cid:19)(cid:19)
1
hd

Q

#
xi −
h

(cid:18)

xi)

|

(cid:18)

cT
2

1
hd

Q

x

xi −
h

(cid:18)

2

(cid:19)(cid:19)

#

cT
1 Tyc1

E

f (y

(cid:16)
x

2

(cid:17)

+ O

(cid:19)(cid:21)

!

"

(cid:18)

1
hd−2

(cid:19)

F (y

"

xi)(1

|

F (y

xi))

|

−

cT
2

1
hd

Q

(cid:18)

(cid:18)

x

xi −
h

+ h

(cid:19)(cid:19)

#

µ S−1
eT

y TyS−1

y eµ

(cid:16)

E

f (y

xi)

|

"

(cid:18)

(cid:17)

cT
2

1
hd

Q

x

xi −
h

(cid:18)

2

+ O

(cid:19)(cid:19)

#

1
hd−2

.

(cid:19)

(cid:18)

To conclude the proof, we note that two scenarios can arise: µ = 0 and µ > 0. In the second case,

The ﬁrst case is more involved. If θ0,0

(*) =

1
hd−1 θ1,0

µ S−1
eT

y TyS−1

y eµ

ν S−1
eT

x TxS−1

x eν

+ O

1
hd−2

.

(cid:19)

(cid:18)

(cid:17)

(cid:16)
= 0, 1, then

(cid:17) (cid:16)

(*) =

1
hd θ0,0(1

−

θ0,0)

(cid:16)

ν S−1
eT

x TxS−1

x eν

+ OP

1
hd−1

.

(cid:19)

(cid:18)

(cid:17)

If θ0,0 = 0 or 1, then a further expansion is needed, which is why an extra h will be present in the leading variance.

SA-7.5 Proof of Lemma SA-2.4

SA-7.5.1 Consistency of ˇVµ,ν(y, x)

For the purposes of this proof, let c1 = S−1

y eµ, ˆc2 = ˆS−1

x eν, and c2 = S−1

x eν. To start, consider

1
n2h2d+2µ+2|ν|

=

1
n2h2d+2µ+2|ν |

2
n2h2d+2µ+2|ν |

−

n

i=1 (cid:20)ZY
X
n

(cid:16)

i=1 (cid:20)ZY
X
n

(cid:16)

1 (yi

u)

ˆF (u
xi)
|

−

≤

1
h

cT
1 P

u

−
h

(cid:16)

(cid:17)

1 (yi

u)

xi)
F (u
|

−

≤

1
h

cT
1 P

(cid:17)

(cid:17)
y

(cid:17)

u

−
h

(cid:16)
ˆF (u2

1 (yi

u1)

−

≤

i=1 Z Z
X

Y−y

h (cid:16)

F (u1

xi)
|

g(y + hu1)g(y + hu2)du1du2

y

dG(u)cT

2 Q

xi

−
h

(cid:16)

2

x

(cid:17)(cid:21)
x

2

dG(u)cT

2 Q

xi

−
h

(cid:17)(cid:21)

(cid:16)
1 P (u1) cT
cT

1 P (u2)

xi)
|

−

F (u2

xi)
|

(cid:17)

(cid:17)(cid:16)
cT
2 Q
h
cT
1 P

xi

−
h
y

x

2

(cid:17)i

dG(u)cT

2 Q

(cid:16)
u

−
h

(cid:16)

(cid:17)

xi

x

−
h

2

.

(cid:17)(cid:21)

(cid:16)

+

1
n2h2d+2µ+2|ν |

n

i=1 (cid:20)ZY
X

(cid:16)

ˆF (u
xi)
|

xi)
F (u
|

−

1
h

(cid:17)

(I)

(II)

(III)

First consider term (III). With the uniform convergence result for the estimated conditional distribution function,

it is clear that

(III)
|

|

-P

1
nhd+2µ+2|ν |

h2q+2 +

(cid:18)

log n
nhd

(cid:19)

-P




V0,ν (y, x)
Vµ,ν (y, x)

h2q+2 + log n
nhd
h2q+1 + log n
(cid:1)
nhd+1

(cid:0)

if µ = 0, and θ0,0

= 0 or 1

if µ > 0,

or θ0,0 = 0 or 1

.

Now we study term (I), which is clearly unbiased for Vµ,ν (y, x). Therefore, we compute its variance.



(cid:0)

(cid:1)

V [(I)] =

1
n3h4d+4µ+4|ν|

V

1
n3h4d+4µ+4|ν|

E

1
n3h4d+4µ+4|ν|

E

≤

=

"(cid:18)ZY

(cid:16)

1 (yi

u)

xi)
F (u
|

−

≤

(cid:17)

1 (yi

u)

xi)
F (u
|

−

≤

(cid:17)

"(cid:18)ZY

4

(cid:16)

1 (yi

uj )

F (uj

xi)
|

−

≤

"

j=1 (cid:20)ZY
Y

(cid:16)

31

1
h

1
h

cT
1 P

cT
1 P

u

(cid:16)

u

(cid:16)

−
h

−
h

1
h

cT
1 P

(cid:17)

(cid:16)

y

y

(cid:17)

(cid:17)
uj

dG(u)cT

2 Q

dG(u)cT

2 Q

xi

xi

x

x

−
h

−
h

2

#

(cid:17)(cid:19)
4

#

(cid:17)(cid:19)

(cid:16)

(cid:16)

y

−
h

(cid:17)

dG(uj)

cT
2 Q

(cid:21) h

(cid:16)

xi

−
h

x

4

(cid:17)i

.

#

 
6
6
With iterative expectation (by conditioning on xi), the above further reduces to

V [(I)] =

1

n3h3d+4µ+4|ν | θ0,0(1

θ0,0)(1

3θ0,0(1

θ0,0))

−

−

−

cT
1 cy,0
h

i

4

E

1
hd

(cid:20)

cT
2 R
h

(cid:16)

xi

−
h

x

4

+ O

(cid:21)

(cid:17)i

(cid:18)

h
n3h4d+4µ+4|ν|

.

(cid:19)

In other words,

Vµ,ν (y, x)

-P

1
n3h3d+4|ν|

q




h
n3h4d+4µ+4|ν|

if µ = 0, and θ0,0

= 0 or 1

if µ > 0,

or θ0,0 = 0 or 1

V0,ν (y, x)
Vµ,ν (y, x)

q

1
nhd

1
nhd+1

-




if µ = 0, and θ0,0

= 0 or 1

if µ > 0,

or θ0,0 = 0 or 1

.

Finally, we consider (II). Using the Cauchy-Schwartz inequality, we have



q

(I)

−

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

q



As a result,

(II)
|

|

-P

V0,ν (y, x)
Vµ,ν (y, x)

q

q






2

(II)
|
|

(I)

(III)
|

.

| · |

≤ |

h2q+2 + log n
nhd

h2q+1 + log n
nhd+1

if µ = 0, and θ0,0

= 0 or 1

if µ > 0,

or θ0,0 = 0 or 1

.

To conclude the proof for ˇVµ,ν (y, x), we note that replacing ˆc2 by c2 only leads to an additional multiplicative factor
1 + OP(1/√nhd). See Lemma SA-2.1.

SA-7.5.2 Consistency of ˆVµ,ν(y, x)

For the purposes of this proof, let ˆc1 = ˆS−1
following

y eµ, c1 = S−1

y eµ, ˆc2 = ˆS−1

x eν, and c2 = S−1

x eν. We ﬁrst consider the

1 (yi ≤
(cid:16)

n

Xj,k=1

n

1
n2h2d+2µ+2|ν|

=

1
n2h2d+2µ+2|ν|

n

n

Xi=1 (cid:20)ZY
1
n2



Xi=1

2
n2h2d+2µ+2|ν|

−

+

1
n2h2d+2µ+2|ν|

n

Xi=1

n

Xi=1











1
n2

1
n2

u)

ˆF (u

|

−

xi)

(cid:17)

1
h

cT
1 P

u

y

−
h

(cid:18)

(cid:19)

d ˆFy(u)cT

2 Q

x

xi −
h

(cid:18)

2

(cid:19)(cid:21)

(1 (yi ≤

yj)

F (yj|

xi)) (1 (yi ≤

−

yk)

F (yk|

−

xi))

1
h2

cT
1 P

y

yj −
h

(cid:18)

cT
1 P

(cid:19)

(cid:18)

y

yk −
h

cT
2 Q
(cid:20)

(cid:18)

x

xi −
h

(I)

(cid:19)





2

(cid:19)(cid:21)

2

ˆF (yj |

Xj,k=1 (cid:16)

xi)

F (yj|

−

xi)

(cid:17)

(1 (yi ≤

yk)

F (yk|

−

xi))

1
h2

cT
1 P

y

yj −
h

(cid:18)

cT
1 P

(cid:19)

(cid:18)

y

yk −
h

cT
2 Q
(cid:20)

(cid:18)

x

xi −
h

(II)

(cid:19)





(cid:19)(cid:21)

n

ˆF (yj |

Xj,k=1 (cid:16)

xi)

F (yj|

−

xi)

(cid:17) (cid:16)

ˆF (yk|

xi)

F (yk|

−

xi)

(cid:17)

1
h2

cT
1 P

y

yj −
h

(cid:18)

cT
1 P

(cid:19)

(cid:18)

y

yk −
h

cT
2 Q
(cid:20)

(cid:18)

x

xi −
h

2

.

(cid:19)(cid:21)

(III)

(cid:19)





By the uniform convergence rate of the estimated conditional distribution function, we have

(III)
|

|

-P

1
nhd+2µ+2|ν |

h2q+2 +

(cid:18)

log n
nhd

(cid:19)

-P




V0,ν (y, x)
Vµ,ν (y, x)

h2q+2 + log n
nhd
h2q+1 + log n
(cid:1)
nhd+1

(cid:0)

if µ = 0, and θ0,0

= 0 or 1

if µ > 0,

or θ0,0 = 0 or 1

.

Next we consider (II). Using the Cauchy-Schwartz inequality, we have



(cid:0)

(cid:1)

2

(II)
|
|

(I)

(III)
|

.

| · |

≤ |

32

6
6
6
6
Finally, consider term (I), which has the expansion

(1 (yi ≤
(cid:20)

yj )

F (yj |

xi)) (1 (yi ≤

−

yk)

F (yk|

−

xi)) cT

1 P

y

yj −
h

(cid:18)

cT
1 P

(cid:19)

(cid:18)

y

yk −
h

cT
2 Q

(cid:19)(cid:21) (cid:20)

(cid:18)

x

xi −
h

2

(cid:19)(cid:21)

(1 (yi ≤

yj)

F (yj|

−

xi)) (1

F (yi|

−

xi)) cT

1 P

y

yj −
h

(cid:18)

cT
1 P

(cid:19)

(cid:18)

y

yi −
h

cT
2 Q

(cid:19)(cid:21) (cid:20)

(cid:18)

x

xi −
h

2

(cid:19)(cid:21)

(1 (yi ≤

yj )

F (yj |

−

xi))2

cT
1 P
(cid:20)

(cid:18)

y

yj −
h

2

cT
2 Q

(cid:19)(cid:21)

(cid:20)

(cid:18)

x

xi −
h

2

(cid:19)(cid:21)

(1

F (yi|

−

xi))2

Xi=1

2

cT
1 P
(cid:20)

(cid:18)

y

yi −
h

(cid:19)(cid:21)

cT
2 Q
(cid:20)

(cid:18)

x

xi −
h

2

.

(cid:19)(cid:21)

(I.1)

(I.2)

(I.3)

(I.4)

(I) =

1
n4h2d+2µ+2|ν|+2

+

2
n4h2d+2µ+2|ν|+2

+

1
n4h2d+2µ+2|ν|+2

+

1
n4h2d+2µ+2|ν|+2

n

Xi,j,k=1
distinct

n

Xi,j=1
distinct

(cid:20)

n

Xi,j=1
distinct
n

Then,

(I.2)
|

| ≤

2
n4h2d+2µ+2|ν |+2

n

cT
1 P

i,j=1
distinct (cid:12)
X
(cid:12)
(cid:12)
n
cT
1 P

1
nh

i=1 (cid:12)
X
(cid:12)
(cid:12)

yj

y

−
h

(cid:16)

yi

y

−
h

(cid:16)

·

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

cT
1 P

yi

−
h

(cid:16)

n

1
nhd+1

i=1 (cid:12)
X
(cid:12)
(cid:12)

y

cT
2 Q

xi

−
h

(cid:16)

x

2

(cid:17)i

h

(cid:17)(cid:12)
(cid:12)
(cid:12)
cT
1 P

yi

y

−
h

(cid:16)

1 +

1
nh !  

1 +

r

r

1
n2hd+2µ+2|ν|

-

-

!

# "
(cid:17)(cid:12)
(cid:12)
(cid:12)

1
nhd+1

2
n2hd+2µ+2|ν|

≤

"

-P

1
n2hd+2µ+2|ν|

xi

−
h

cT
2 Q
h

(cid:16)

(cid:17)(cid:12)
(cid:12)
V0,ν (y, x) 1
(cid:12)
n
Vµ,ν (y, x) 1
nh




x

2

#

(cid:17)i
if µ = 0, and θ0,0

= 0 or 1

.

if µ > 0,

or θ0,0 = 0 or 1

Using similar techniques, one can show that



(I.3)
|

|

-P

V0,ν (y, x) 1
nh
Vµ,ν (y, x) 1
nh2




if µ = 0, and θ0,0

= 0 or 1

if µ > 0,

or θ0,0 = 0 or 1

and

(I.4)

-P

|

|

To streamline the remaining derivation, deﬁne



V0,ν (y, x) 1
n2h
Vµ,ν (y, x)
1
n2h2






if µ = 0, and θ0,0

= 0 or 1

if µ > 0,

or θ0,0 = 0 or 1

.

φj,i =

1
h

(1 (yi

yj)

−

≤

F (yj

xi)) cT
|

1 P

yj

y

−
h

(cid:16)

(cid:17)

, φi = E[φj,i

yi, xi], ψi =
|

cT
2 Q

h

(cid:16)

xi

x

−
h

2

.

(cid:17)i

Then

(I.1) =

1
n4h2d+2µ+2|ν |

=

1
n4h2d+2µ+2|ν |

φj,iφk,iψi

n

Xi,j,k=1
distinct
n

(φj,i

Xi,j,k=1
distinct

(I.1.1)

φi)(φk,i

−

−

φi)ψi

+

2 + O

(cid:18)

1
n

(cid:18)

(cid:19)(cid:19)

1
n3h2d+2µ+2|ν |

|

+

1 + O

(cid:18)

1
n

(cid:18)

(cid:19)(cid:19)

{z
1
n2h2d+2µ+2|ν|

n

}
.

φ2
i ψi

i=1
X

(I.1.3)

|

We have studied the term (I.1.3) in the proof for ˇVµ,ν (y, x). In particular,

|

{z

}

n

(φj,i

i,j=1
X
distinct

(I.1.2)

{z

φi)φiψi

−

}

−

(I.1.3)

(cid:12)
(cid:12)
(cid:12)

Vµ,ν (y, x)

-P

(cid:12)
(cid:12)
(cid:12)






V0,ν (y, x)
Vµ,ν (y, x)

q

1
nhd

1
nhd+1

if µ = 0, and θ0,0

= 0 or 1

if µ > 0,

or θ0,0 = 0 or 1

.

q

33

 
6
6
6
6
Term (I.1.1) is a mean zero third order U-statistic. Consider its variance

V [(I.1.1)] =

1
nhd+2µ+2|ν|

(cid:18)

(cid:19)

2

E 

1
n6h2d

n

n

(φj,i

Xi,j,k=1
distinct

Xi′,j′,k′=1

distinct





The above expectation is non-zero only in three scenarios: (j = j′, k = k′, i
(j = i′, k = k′, i = j′). Therefore,

φi)(φk,i

−

−

φi)(φj′,i′

−

φi′ )(φk′,i′

−

φi′ )ψiψi′ 

.

= i′), (j = j′, k = k′, i = i′) or





(I.1.1)
|

|

-P

1
nhd+2µ+2|ν|

(cid:18)
V0,ν (y, x) 1
nh
Vµ,ν (y, x) 1
nh2

-




1
nh

+

r

1
n3hd+2

-

!

(cid:18)

1
nhd+2µ+2|ν|

1
nh

(cid:19)

(cid:19)  

if µ = 0, and θ0,0

= 0 or 1

if µ > 0,

or θ0,0 = 0 or 1

.

Finally consider (I.1.2), which has a mean of zero. Its variance is



V [(I.1.2)] =

1
nhd+2µ+2|ν |

(cid:18)

1
nhd+2µ+2|ν |

=

(cid:18)

1
nhd+2µ+2|ν|

1
nhd+2µ+2|ν|

+

(cid:18)

-

(cid:18)

2

(cid:19)

2

(cid:19)

2

2

(cid:19)

(cid:19)

E 

1
n4h2d





E 

1
n4h2d





E



1
n4h2d



1
nh

(cid:18)

+

1
n2hd+1

.

(cid:19)

n

(φj,i

Xi,i′,j=1

distinct

n

(φj,i

i,j=1
X
distinct

−

n

n

(φj,i

i,j=1
X
distinct

Xi′,j′=1

distinct

φi)(φj′,i′

−

−

φi′ )φiψiφi′ ψi′ 





φi)(φj,i′

−

−

φi′ )φiψiφi′ ψi′ 




n

(φj,i

i,j=1
X
distinct

1
n4h2d

+ E






φi)(φi,j

φj )φiψiφjψj 

−

−




φi)2φ2

i ψ2

i 




In addition, an extra h factor emerges if µ > 0, or if θ0,0 = 0 or 1. As a result,

(I.1.2)

-P

|

|

V0,ν (y, x)
Vµ,ν (y, x)

1
nh

1
nh2

if µ = 0, and θ0,0

= 0 or 1

if µ > 0,

or θ0,0 = 0 or 1

.






q

q

To conclude the proof for ˆVµ,ν (y, x), we note that replacing ˆc1 by c1 and ˆc2 by c2 only leads to an additional
multiplicative factor 1 + OP(1/√nhd). See Lemma SA-2.1.

SA-7.6 Proof of Theorem SA-2.1

We will write

¯Sµ,ν (y, x) =

1
√n

n

hd+µ+|ν|K ◦

µ,ν,h (yi, xi; y, x)

.

i=1
X

V

r

hd+µ+|ν|K ◦
h

µ,ν,h (yi, xi; y, x)
i

Deﬁne c1 = S−1

y eµ and c2 = S−1

x eν.

34

6
6
6
To apply the Berry-Esseen theorem, we ﬁrst compute the third moment

hd+µ+|ν|K ◦

3

µ,ν,h (yi, xi; y, x)
(cid:12)
(cid:12)
(cid:12)
y + huj )

1(yi

≤

Z

Y−y

h (cid:16)

E

=E

3

(cid:20)(cid:12)
(cid:12)
(cid:12)
" 

j=1 (cid:12)
(cid:12)
Y
(cid:12)
(cid:12)
(cid:12)

The leading term in the above is simply

(cid:21)

−

F (y + huj

xi)
|

cT
1 P (uj ) dG(uj)
(cid:17)

cT
2 Q

xi

−
h

(cid:16)

.

#

x

3

(cid:17)(cid:12)
(cid:12)
(cid:12)

!

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

3

E

" 

j=1 (cid:12)
Z
(cid:12)
Y
(cid:12)
3
(cid:12)
cT
1 cy,0
(cid:12)

E

(cid:12)
(cid:12)
3
(cid:12)

E

cT
1 cy,0

=

(cid:12)
(cid:12)
(cid:12)

=

1(yi

y)

F (y

xi)
|

−

≤

Y−y

h (cid:16)

cT
1 P (uj ) dG(uj)
(cid:17)
3

xi

x

cT
2 Q

−
h

cT
2 Q

xi

−
h

(cid:16)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
!
(cid:12)
(cid:12)
3
(cid:12)
(cid:12)
(cid:17)(cid:12)
(cid:12)
(cid:12)
2θ0,0(y, x) + 1)

(cid:21)

x

3

#

(cid:17)(cid:12)
(cid:12)
(cid:12)

cT
2 Q

1(yi

y)

F (y

≤

−

xi)
|

(cid:20)(cid:12)
(cid:16)
(cid:12)
(cid:12)
θ0,0(y, x)(1
(cid:20)(cid:16)

−

(cid:12)
(cid:12)
θ0,0(y, x))(2θ0,0(y, x)2
(cid:12)

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:16)

−

(cid:17) (cid:12)
(cid:12)
(cid:12)
Note that the above will be exactly zero in cases (ii) and (iii) of Lemma SA-2.3.

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

xi

−
h

(cid:16)

x

3

= O(hd).

(cid:21)

(cid:17)(cid:12)
(cid:12)
(cid:12)

SA-7.7 Omitted Details of Remark SA-2.3

SA-7.7.1 Approximation and coverage error of ˇS◦µ,ν(y, x)

To start,

ˇS◦

µ,ν (y, x)

−

¯Sµ,ν (y, x)

=

nhµ+|ν|

1
Vµ,ν (y, x)

µ S−1
eT
y

n

i=1
X

(cid:20)ZY

h

1(yi

u)

xi)
F (u
|

−

≤

1
h

i

P

(cid:16)

u

y

−
h

(cid:17)

dG(u)

1
hd Q

(cid:21)

(cid:16)

xi

−
h

T

x

(cid:17)

(cid:16)

ˆS−1
x

−

S−1
x

eν.

(cid:17)

By allowing the constant c1 to take possibly diﬀerent values in each term, we have

p

P

sup
y∈Y,x∈X

(cid:20)

P

sup
x∈X

"

≤

ˇS◦

µ,ν (y, x)

−

¯Sµ,ν (y, x)

c1

≥

Sx

> c1

(cid:12)
(cid:12)
ˆSx

−

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
+ P

log n
nhd

#

r

log n
√nhd

(cid:21)

sup
y∈Y,x∈X

"

µ S−1
eT
y

¯Ry,x

¯Ry,x

E
−
Vµ,ν (y, x)

(cid:2)

(cid:0)

X
|

(cid:3)(cid:1)

(cid:12)
(cid:12)
(cid:12)

p

(cid:12)
(cid:12)
(cid:12)

> c1

log n

p

c2n−c3 ,

# ≤

where the conclusions follow from the uniform rates established in Lemma SA-2.1 and the variance calculations in
Lemma SA-2.3. Next, we consider the normal approximation error. Note that

P

¯Sµ,ν (y, x)

(cid:20)

c1

u

−

≤

log n
√nhd

−

(cid:21)

which means

c2n−c3

≤

P

ˇS◦

µ,ν (y, x)

(cid:2)

u

≤

≤

(cid:3)

P

¯Sµ,ν (y, x)
(cid:20)

≤

u + c1

log n
√nhd

(cid:21)

+ c2n−c3 ,

(cid:12)
(cid:12)
where rBE is deﬁned in Theorem SA-2.1.

(cid:2)

(cid:3)

(cid:12)
(cid:12)

P

ˇS◦

µ,ν (y, x)

sup
u∈R

u

≤

−

Φ(u)

-

log n
√nhd

+ rBE,

35

SA-7.7.2 Approximation and coverage error of ˆS◦µ,ν(y, x)

To begin with, we decompose the double sum into

yj

y

−
h

xi

−
h

Q

(cid:17)

(cid:16)

T

x

(cid:17)

n

i,j=1 h
X
n

i=1  
h
X
n

1
n2hd+1

=

1
n2hd+1

+

1
n2hd+1

1(yi

yj)

F (yj

xi)
|

−

≤

P

1

−

F (yi

xi)
|

yi

P

i

(cid:16)

−
h

(cid:16)

i
y

−

(cid:17)

ZY
h
yj

1(yi

yj)

F (yj

xi)
|

−

≤

i,j=1
X
distinct

h

P

i

(cid:16)

1(yi

u)

xi)
F (u
|

−

≤

u

y

−
h

P

i

(cid:16)

(cid:17)

dG(u)

Q

−
h

y

−

(cid:17)

1(yi

ZY

h

u)

xi)
F (u
|

−

≤

P

i

(cid:16)

(cid:16)
y

!

u

−
h

xi

−
h

T

x

(cid:17)

dG(u)

Q

(cid:17)

!

(cid:16)

+

1
nhd+1

n

1(yi

i=1 ZY
X

h

u)

xi)
F (u
|

−

≤

u

y

−
h

P

i

(cid:16)

(cid:17)

dG(u)Q

xi

−
h

T

x

,

(cid:17)

(cid:16)

where we set G = Fy. Term (I) represents the leave-in bias, and it is straightforward to show that

xi

−
h

(I)

T

x

(cid:17)

(II)

(III)

for some constants c1, c2, and c3. See Lemma SA-2.1 for the proof strategy.

P

sup
y∈Y,x∈X

"

(I)

> c1

1
n  

1 +

r

log n
nhd+1

!# ≤

c2n−c3 ,

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Term (II) is a degenerate U-statistic. Deﬁne

ui,j = cT
1

1(yi

h

yj)

F (yj

xi)
|

−

≤

yj

y

−
h

P

i

(cid:16)

−

(cid:17)

1(yi

ZY

h

u)

xi)
F (u
|

−

≤

u

y

−
h

P

i

(cid:16)

(cid:17)

dG(u)

Q

!

(cid:16)

xi

−
h

T

x

c2,

(cid:17)

where c1 and c2 are arbitrary (ﬁxed) vectors of conformable dimensions. Then with

A = C ′,

B2 = C ′nh,

D2 = C ′n2hd+1,

for some constant C ′, and

t = C(log n)√n2hd+1

for some large constant C, we apply Lemma SA-7.3, which gives (the value of C ′ may change for each line)

P



sup
y∈Y,x∈X



= C ′ exp

(cid:26)

As a result,

n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

i,j=1
X
distinct
√C
C ′ min

ui,j (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:20)



log n,

> t

≤

C ′ exp

1
C ′ min

t
√n2hd+1

,

(cid:20)

−

(cid:26)

t2/3
(nh)1/3 , t1/2

+ log n

(cid:21)

(cid:27)

(log n)2nhd

1
3

,

(log n)2n2hd+1

(cid:16)

(cid:17)

(cid:16)

1
4

(cid:21)

(cid:17)

+ log n

.

(cid:27)

for some constants c1, c2, and c3.

P

sup
y∈Y,x∈X

(cid:20)

(II)

> c1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

log n
√n2hd+1

≤

(cid:21)

c2n−c3 ,

We now collect the pieces. The diﬀerence between ˆS◦

µ,ν (y, x) and ˇS◦

µ,ν (y, x) is

ˇS◦

µ,ν (y, x)

ˆS◦

−

µ,ν (y, x)
1
Vµ,ν (y, x)

hµ+|ν|

=

eT
µ

ˆS−1
y

(I) + (II)

h

x eν +

ˆS−1
i

1
Vµ,ν (y, x)

eT
µ

ˆS−1

y −

S−1
y

(cid:16)

(cid:17) h

p

p

36

¯Ry,x

E

¯Ry,x

X
|

−

(cid:2)

ˆS−1

x eν,

(cid:3) i

 
 
and the conclusion follows from Lemmas SA-2.1 and SA-2.3.

SA-7.8 Proof of Lemma SA-3.1

Part (i) Convergence of ˇθµ,ν

−

θµ,ν . Recall that we have the following decomposition of our estimator

ˇθµ,ν

−

θµ,ν =

1
nh1+d+µ+|ν|

+

1
nh1+d+µ+|ν|

+

1
nh1+d+µ+|ν|

n

i=1 (cid:20)ZY
X
n

xi)eT
F (u
|

µ S−1

y P

u

−
h

(cid:16)

i=1 (cid:20)ZY
X
n

(cid:16)

i=1 (cid:20)ZY
X

(cid:16)

1(yi

u)

xi)
F (u
|

−

≤

1(yi

u)

xi)
F (u
|

−

≤

(cid:17)

(cid:17)

y

dG(u)

Q

xi

(cid:17)
µ S−1
eT

y P

µ S−1
eT

y P

(cid:16)
y

−
h

y

−
h

(cid:21)

u

u

(cid:16)

(cid:16)

−
h

x

T

ˆS−1

x eν

(cid:17)

θµ,ν

−

dG(u)

Q

(cid:21)

dG(u)

Q

(cid:21)

(cid:17)

(cid:17)

xi

xi

−
h

−
h

T

x

(cid:17)

T

x

(cid:17)

(cid:16)

(cid:16)

(I)

(II)

S−1

x eν

ˆS−1
x
(cid:16)

−

S−1
x

eν.

(cid:17)

(III)

(I) is simply the conditional bias, whose order is given in Lemma SA-2.2. The convergence rate of (II) can be easily
deduced from that of eT
in Lemma SA-2.1. Finally, it should be clear that (III) is negligible
relative to (II).

µ S−1
y

¯Ry,x

¯Ry,x

X
|

−

E

(cid:0)

(cid:2)

(cid:3)(cid:1)

Part (ii) Convergence of ˆθµ,ν

−

θµ,ν . This part follows from Remark SA-2.3.

SA-7.9 Proof of Lemma SA-3.2

SA-7.9.1 Uniform consistency of ˇVµ,ν(y, x)

For the purposes of this proof, let c1 = S−1

y eµ, ˆc2 = ˆS−1

x eν, and c2 = S−1

x eν. To start, consider

1
n2h2d+2µ+2|ν|

=

1
n2h2d+2µ+2|ν |

2
n2h2d+2µ+2|ν |

−

n

i=1 (cid:20)ZY
X
n

(cid:16)

i=1 (cid:20)ZY
X
n

(cid:16)

1 (yi

u)

ˆF (u
xi)
|

−

≤

1
h

cT
1 P

u

−
h

(cid:16)

(cid:17)

1 (yi

u)

xi)
F (u
|

−

≤

1
h

cT
1 P

(cid:17)

(cid:17)
y

(cid:17)

u

−
h

(cid:16)
ˆF (u2

1 (yi

u1)

−

≤

i=1 Z Z
X

Y−y

h (cid:16)

F (u1

xi)
|

g(y + hu1)g(y + hu2)du1du2

y

dG(u)cT

2 Q

xi

−
h

(cid:16)

2

x

(cid:17)(cid:21)
x

2

dG(u)cT

2 Q

xi

−
h

(cid:17)(cid:21)

(cid:16)
1 P (u1) cT
cT

1 P (u2)

xi)
|

−

F (u2

xi)
|

(cid:17)

(cid:17)(cid:16)
cT
2 Q
h
cT
1 P

xi

−
h
y

x

2

(cid:17)i

dG(u)cT

2 Q

(cid:16)
u

−
h

(cid:16)

(cid:17)

xi

x

−
h

2

.

(cid:17)(cid:21)

(cid:16)

+

1
n2h2d+2µ+2|ν |

n

i=1 (cid:20)ZY
X

(cid:16)

ˆF (u
xi)
|

xi)
F (u
|

−

1
h

(cid:17)

(I)

(II)

(III)

First consider term (I). Clearly this term is unbiased for Vµ,ν (y, x). In the proof of Lemma SA-2.4, we showed

that

V

"(cid:18)ZY

(cid:16)

1 (yi

u)

xi)
F (u
|

−

≤

1
h

cT
1 P

(cid:17)

Also note that

1 (yi

(cid:18)ZY

(cid:16)

u)

xi)
F (u
|

−

≤

(cid:16)

(cid:17)

u

y

−
h

(cid:17)

dG(u)cT

2 Q

xi

x

−
h

(cid:16)

2

# ≤

(cid:17)(cid:19)

hd

hd+1

C1 


if µ = 0

if µ > 0

.

1
h

cT
1 P

u

y

−
h

(cid:16)

(cid:17)

dG(u)cT

2 Q

xi

x

−
h

(cid:16)



2

C2.

≤

(cid:17)(cid:19)

In the above, the constants, C1 and C2, can be chosen to be independent of the evaluation point, the sample size, and
, and applying the union bound and Bernstein’s inequality,
the bandwidth. Then by a proper discretization of

Y × X

37

(I)

Vµ,ν (y, x)

−
Vµ,ν (y, x)

> c1r1

≤

(cid:21)

c2n−c3 ,

r1 =

log n
nhd
log n
nhd+1

if µ = 0

if µ > 0

,

q

q






for some constants c1, c2, and c3. In addition, c3 can be made arbitrarily large by appropriate choices of c1. See the
proof of Lemma SA-2.1 for an example of this proof strategy.

Next consider term (III). With the uniform convergence result for the estimated conditional distribution function,

one has

P

(cid:20)

sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)

it is clear that

P

(III)
Vµ,ν (y, x)

> c1r3

≤

(cid:21)

c2n−c3 ,

r3 = h2q+1 +

log n
nhd+1 .

Finally, we consider (II). Using the Cauchy-Schwartz inequality, we have

2

(II)
|
|

(I)

(III)
|

.

| · |

≤ |

As a result,

P

(II)
Vµ,ν (y, x)

≤

> c1r2

c2n−c3 ,

r2 = √1 + r1√r3.

(cid:21)
To conclude the proof for ˇVµ,ν (y, x), we note that replacing ˆc2 by c2 only leads to an additional multiplicative factor
1 + OP(

log n/(nhd)). See Lemma SA-2.1.

(cid:20)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
y∈Y,x∈X (cid:12)
(cid:20)
(cid:12)
(cid:12)
(cid:12)

sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)

p

SA-7.9.2 Uniform consistency of ˆVµ,ν(y, x)

For the purposes of this proof, let ˆc1 = ˆS−1

y eµ, c1 = S−1

y eµ, ˆc2 = ˆS−1

x eν, and c2 = S−1

x eν .

We consider the same decomposition used in the proof of Lemma SA-2.4:

1 (yi ≤
(cid:16)

n

Xj,k=1

n

1
n2h2d+2µ+2|ν|

=

1
n2h2d+2µ+2|ν|

n

n

Xi=1 (cid:20)ZY
1
n2



Xi=1

2
n2h2d+2µ+2|ν|

−

+

1
n2h2d+2µ+2|ν|

n

Xi=1

n

Xi=1











1
n2

1
n2

u)

ˆF (u

|

−

xi)

(cid:17)

1
h

cT
1 P

u

y

−
h

(cid:18)

(cid:19)

d ˆFy(u)cT

2 Q

x

xi −
h

(cid:18)

2

(cid:19)(cid:21)

(1 (yi ≤

yj)

F (yj|

xi)) (1 (yi ≤

−

yk)

F (yk|

−

xi))

1
h2

cT
1 P

y

yj −
h

(cid:18)

cT
1 P

(cid:19)

(cid:18)

y

yk −
h

cT
2 Q
(cid:20)

(cid:18)

x

xi −
h

(I)

(cid:19)





2

(cid:19)(cid:21)

2

ˆF (yj |

Xj,k=1 (cid:16)

xi)

F (yj|

−

xi)

(cid:17)

(1 (yi ≤

yk)

F (yk|

−

xi))

1
h2

cT
1 P

y

yj −
h

(cid:18)

cT
1 P

(cid:19)

(cid:18)

y

yk −
h

cT
2 Q
(cid:20)

(cid:18)

x

xi −
h

(II)

(cid:19)





(cid:19)(cid:21)

n

ˆF (yj |

Xj,k=1 (cid:16)

xi)

F (yj|

−

xi)

(cid:17) (cid:16)

ˆF (yk|

xi)

F (yk|

−

xi)

(cid:17)

1
h2

cT
1 P

y

yj −
h

(cid:18)

cT
1 P

(cid:19)

(cid:18)

y

yk −
h

cT
2 Q
(cid:20)

(cid:18)

x

xi −
h

2

.

(cid:19)(cid:21)

(III)

(cid:19)





By the uniform convergence rate for the estimated conditional distribution function, we have

(III)
Vµ,ν (y, x)

> c1r3

≤

(cid:21)

c2n−c3 ,

r3 = h2q+1 +

log n
nhd+1 .

P

sup
y∈Y,x∈X (cid:12)
(cid:20)
(cid:12)
(cid:12)
(cid:12)

Employing the Cauchy-Schwartz inequality gives

2

(II)
|
|

(I)

(III)
|

.

| · |

≤ |

As a result, a probabilistic order for term (II) follows that of terms (I) and (III).

38

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(I.1)

(I.2)

(I.3)

(I.4)

Finally, consider term (I), which has the expansion

(1 (yi ≤
(cid:20)

yj )

F (yj |

xi)) (1 (yi ≤

−

yk)

F (yk|

−

xi)) cT

1 P

y

yj −
h

(cid:18)

cT
1 P

(cid:19)

(cid:18)

y

yk −
h

cT
2 Q

(cid:19)(cid:21) (cid:20)

(cid:18)

x

xi −
h

2

(cid:19)(cid:21)

(1 (yi ≤

yj)

F (yj|

−

xi)) (1

F (yi|

−

xi)) cT

1 P

y

yj −
h

(cid:18)

cT
1 P

(cid:19)

(cid:18)

y

yi −
h

cT
2 Q

(cid:19)(cid:21) (cid:20)

(cid:18)

x

xi −
h

2

(cid:19)(cid:21)

n

Xi,j,k=1
distinct

n

Xi,j=1
distinct

(cid:20)

n

Xi,j=1
distinct
n

(I) =

1
n4h2d+2µ+2|ν|+2

+

2
n4h2d+2µ+2|ν|+2

+

1
n4h2d+2µ+2|ν|+2

+

1
n4h2d+2µ+2|ν|+2

Then,

(1 (yi ≤

yj )

F (yj |

−

xi))2

cT
1 P
(cid:20)

(cid:18)

y

yj −
h

2

cT
2 Q

(cid:19)(cid:21)

(cid:20)

(cid:18)

x

xi −
h

2

(cid:19)(cid:21)

(1

F (yi|

−

xi))2

Xi=1

2

cT
1 P
(cid:20)

(cid:18)

y

yi −
h

(cid:19)(cid:21)

cT
2 Q
(cid:20)

(cid:18)

x

xi −
h

2

.

(cid:19)(cid:21)

(I.2)
|

| ≤

1
n4h2d+2µ+2|ν|+2

1
nhd+2µ+2|ν |

(cid:19)

≤

(cid:18)

which means

n

cT
1 P

yj

y

−
h

(cid:16)

n

yi

cT
1 P

i=1 (cid:12)
X
(cid:12)
(cid:12)

(cid:16)

i,j=1
distinct (cid:12)
X
(cid:12)
(cid:12)

2
n "

1
nh

·

(cid:17)(cid:12)
(cid:12)
(cid:12)
y
−
h

cT
1 P
(cid:12)
(cid:12)
(cid:12)

# "
(cid:17)(cid:12)
(cid:12)
(cid:12)

yi

y

−
h

(cid:16)

xi

−
h

cT
2 Q
h

(cid:16)

x

2

(cid:17)i

(cid:17)(cid:12)
(cid:12)
(cid:12)

n

1
nhd+1

cT
1 P

yi

y

−
h

(cid:16)

(cid:17)(cid:12)
(cid:12)
(cid:12)

xi

−
h

cT
2 Q
h

(cid:16)

x

2

(cid:17)i

,

#

i=1 (cid:12)
X
(cid:12)
(cid:12)

sup
y∈Y,x∈X (cid:12)
(cid:20)
(cid:12)
(cid:12)
Using similar techniques, one can show that
(cid:12)

P

(I.2)
Vµ,ν (y, x)

c1

1
nh

≥

≤

(cid:21)

c2n−c3 .

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(I.3)
Vµ,ν (y, x)

c1

1
nh2

≥

≤

(cid:21)

c2n−c3 ,

and

P

P

(cid:20)

sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

To streamline the remaining derivation, deﬁne

(cid:20)

sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)

(I.4)
Vµ,ν (y, x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

c1

1
n2h2

≥

≤

(cid:21)

c2n−c3 .

φj,i =

1
h

(1 (yi

yj)

−

≤

F (yj

xi)) cT
|

1 P

yj

y

−
h

(cid:16)

(cid:17)

, φi = E[φj,i

yi, xi], ψi =
|

cT
2 Q

h

(cid:16)

xi

x

−
h

2

.

(cid:17)i

Then

(I.1) =

1
n4h2d+2µ+2|ν |

=

1
n4h2d+2µ+2|ν |

φj,iφk,iψi

n

Xi,j,k=1
distinct
n

(φj,i

Xi,j,k=1
distinct

(I.1.1)

φi)(φk,i

−

−

φi)ψi

+

2 + O

(cid:18)

1
n

(cid:18)

(cid:19)(cid:19)

1
n3h2d+2µ+2|ν |

|

+

1 + O

(cid:18)

1
n

(cid:18)

(cid:19)(cid:19)

{z
1
n2h2d+2µ+2|ν|

n

}
.

φ2
i ψi

i=1
X

(I.1.3)

|

By employing the same techniques in the proof for ˇVµ,ν (y, x), we have that

|

{z

}

n

(φj,i

i,j=1
X
distinct

(I.1.2)

{z

φi)φiψi

−

}

(I.1.3)

Vµ,ν (y, x)

−

Vµ,ν (y, x)

P

(cid:20)

sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)

c2n−c3 ,

r1 =

log n
nhd
log n
nhd+1

if µ = 0

if µ > 0

.

q

q






> c1r1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(cid:21)

39

Term (I.1.2) admits the following decomposition:

(I.1.2) =

1
nhd+2µ+2|ν|

n

−
n2

1

n

E

1
hd (φj,i

−

φi)φiψi

yj, xj

|

+

1
nhd+2µ+2|ν|

1
n2hd

j=1
X

h
(I.1.2.1)
n
{z

i,j=1
X
distinct

(cid:18)

(φj,i

−

φi)φiψi

−

(I.1.2.2)

(cid:12)
(cid:12)
(cid:12)

{z

i

}
E

h

1
hd (φj,i

−

φi)φiψi

yj, xj

.

(cid:12)
(cid:12)
(cid:12)

i(cid:19)

}

Using the same techniques of Lemmas SA-2.1 and SA-2.4, we have

|

µ = 0

µ > 0

P

P

"

"

sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)

(I.1.2.1)
Vµ,ν (y, x)

(I.1.2.1)
Vµ,ν (y, x)

> c1

> c1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

log n
nh # ≤

log n
nh2

# ≤

r

r

c2n−c3 ,

c2n−c3 .

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Term (I.1.2.2) is a degenerate second order U-statistic. We adopt Lemma SA-7.3, which implies (see Remark SA-2.3
and its proof for an example)

To handle term (I.1.1), ﬁrst consider the quantity φj,i

P

"

sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)

(I.1.2.2)
Vµ,ν (y, x)

log n
n2hd+3

> c1

r

# ≤

c2n−c3 .

φi, which takes the form

(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

max
i

(φj,i

φi)

−

1
n

n

j=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

sup
y′∈Y,x′∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

y′

1

n

1
nh

j=1 (cid:20)
X

(cid:0)

(cid:0)

Then it is straightforward to show that

yj

≤

−

F (yj

x′)
|

cT
1 P

(cid:1)

(cid:1)

yj

y

−
h

(cid:16)

−

Z

(cid:17)

y′

1

u

≤

x′)
F (u
|

−

cT
1 P

(cid:0)

(cid:0)

(cid:1)

(cid:1)

u

y

−
h

(cid:16)

(cid:17)

dG(u)

.

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

As a result,

P

max
i

"

1
n

n

j=1
X

sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(φj,i

φi)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

c1

≥

r

log n
nh # ≤

c2n−c3 .

To conclude the proof for ˆVµ,ν (y, x), we note that replacing ˆc1 by c1 and ˆc2 by c2 only leads to an additional

(I.1.1)
Vµ,ν (y, x)

> c1

log n
nh2

≤

(cid:21)

c2n−c3 .

P

sup
y∈Y,x∈X (cid:12)
(cid:20)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

multiplicative factor 1 + OP(

log n/nhd). See Lemma SA-2.1.

SA-7.10 Proof of Lemma SA-3.3

p

First consider ˇT◦

µ,ν (y, x). The diﬀerence between ˇT◦

µ,ν (y, x) and ˇS◦

µ,ν (y, x) is

ˇT◦

µ,ν (y, x)

−

ˇS◦

µ,ν (y, x) =

Vµ,ν (y, x)
ˇVµ,ν (y, x) −

1

!

 s

ˇS◦

µ,ν (y, x).

40

From Lemma SA-3.2, we have

P

"

To close the proof, it is straightforward to verify that

Vµ,ν (y, x)
ˇVµ,ν (y, x) −

sup
y∈Y,x∈X (cid:12)
s
(cid:12)
(cid:12)
(cid:12)
(cid:12)

> c1rVE

# ≤

c2n−c3 .

1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
which follows from the uniform convergence rate in Lemma SA-3.1. The same technique applies to the analysis of

(cid:12)
(cid:12)

p

P

(cid:20)

sup
y∈Y,x∈X

ˇS◦

µ,ν (y, x)

> c1

log n

c2n−c3 ,

≤

(cid:21)

ˆT◦

µ,ν (y, x)

−

ˆS◦

µ,ν (y, x).

SA-7.11 Proof of Lemma SA-3.4

We ﬁrst rewrite the kernel using change-of-variable

hd+µ+|ν|Kµ,ν,h (a, b; y, x) = eT

µ S−1
y

1(a

≤

"Z

Y−y
h

y + hv)P (v) g(y + hv)dv

b

−
h

Q

#

(cid:18)

T

x

(cid:19)

S−1

x eν.

Then it should be clear that the kernel is bounded. The same holds for hd+µ+|ν|K ◦

Next consider two cases. If (a

y)/h is larger than 1, then the integrand 1(a
1. Therefore, the kernel deﬁned above will be zero as well. For the case that (a

≤

−

P(v) is zero for v
can simply drop the indicator, as again P(v) will be zero for v

≥

1. Then the kernel becomes

≤ −

µ,ν,h (a, b; y, x).
y + hv)P (v) will be zero because
1, we

y)/h

−

≤ −

hd+µ+|ν|Kµ,ν,h (a, b; y, x) = eT

µ S−1
y

P (v) g(y + hv)dv

"Z

Y−y
h

b

−
h

Q

#

(cid:18)

T

x

(cid:19)

S−1

x eν,

a

1.

≤ −

Note that the matrix, Sy, can be written as

Sy =

Y−y
h

Z

P(v)p(v)Tg(y + hv)dv,

which means
that µ

1 and
R

≥

Y−y
h
a
|

−

/h
|

≥

P (v) g(y + hv)dv is simply the ﬁrst column of Sy. As a result, Kµ,ν,h (a, b; y, x) is zero provided
y

1.

As for the second argument, b, we note that Q((b

−

x)/h) is zero if b lies outside of a h-cube around x. This

concludes our proof.

SA-7.12 Proof of Lemma SA-3.5

We will consider hd+µ+|ν|Kµ,ν,h (a, b; y, x), which allows us to ignore the extra scaling factor. We ﬁrst rewrite the
kernel using change-of-variable

hd+µ+|ν|Kµ,ν,h (a, b; y, x) = eT

µ S−1
y

1(a

≤

"Z

Y−y
h

y + hv)P (v) g(y + hv)dv

b

−
h

Q

#

(cid:18)

T

x

(cid:19)

S−1

x eν,

then it should be obvious that it is Lipschitz-h−1 continuous with respect to b, as

hd+µ+|ν|

sup
a,y,x

Kµ,ν ,h (a, b; y, x)
|

Kµ,ν,h

a, b′; y, x

Q

b

−
h

(cid:18)

T

x

(cid:19)

Q

−

(cid:18)

b′

x

−
h

(cid:19)

because Q(

) is Lipschitz continuous.
·

≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|

(cid:1)

−

T

(cid:0)
eT
µ S−1
y

sup
a,y (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1(a

≤

"Z

Y−y
h

y + hv)P (v) g(y + hv)dv

S−1

x eν

- h−1,

sup
x

(cid:12)
(cid:12)

(cid:12)
(cid:12)

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

41

Next consider the direction a. Again, we have

hd+µ+|ν|

sup
b,y,x

Kµ,ν ,h (a, b; y, x)
|

−

Kµ,ν,h

a′, b; y, x

|

(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1(a

≤

y + hv)

− 1(a′

≤

Y−y

h (cid:16)

(cid:0)
y + hv)

(cid:17)

Y−y

h ∩[−1,1]∩h

a−y

h , a′ −y
h i

P (v) g(y + hv)dv

.

P (v) g(y + hv)dv

µ S−1
eT
y

sup
y

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
b,x (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

b

−
h

Q

(cid:18)

T

x

(cid:19)

S−1

x eν

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup

≤

- sup

Z

y (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
y (cid:12)
Z
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Therefore, the kernel is also Lipschitz-h−1 continuous with respect to a. The analysis of hd+µ+|ν|K ◦
similar.

µ,ν,h (a, b; y, x) is

Now we prove the second claim. First, it is not diﬃcult to show that Sy is Lipschitz continuous with respect to
y , as Sy is uniformly bounded

y, with the Lipschitz constant having the order 1/h. The same holds for its inverse, S−1
away from being singular. As a result,

Similarly, one can show that

Now consider the following diﬀerence

S−1

y −

S−1
y′

-

1
y
h |

y′

.
|

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

S−1
x

−

S−1
x′

-

1
h |

x

x′

.
|

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1(a

u)

1
h

≤

u

y

−
h

P

(cid:16)

(cid:16)

P

−

(cid:18)

(cid:17)

y′

u

−
h

(cid:19) (cid:17)

sup
a (cid:12)
ZY
(cid:12)
(cid:12)
(cid:12)

µ S−1
eT
y

sup
y

g(u)du
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

sup
b,x (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

b

−
h

Q

(cid:18)

T

x

(cid:19)

S−1

x eν

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

It is obvious that the kernel is Lipschitz-h−1 continuous in y and x. The analysis of hd+µ+|ν|K ◦

µ,ν,h (a, b; y, x) is

similar.

SA-7.13 Proof of Lemma SA-3.6

This proof is motivated by Lemma 4.1 in Rio (1994). Take ℓ =
intervals of equal length. This will lead to a partition
P -measure exceeds ε,

A

=

, and partition each coordinate [0, 1] into ℓ
1/h
⌋
⌊
of [0, 1]d. Next, consider sets whose
j
Aj : 1
{

ℓd

≤

≤

}

P,ε =

A

A

{

∈ A

: P [A] > ε

,
}

and their ch-enlargements

ch
P,ε =

A

Importantly, if z does not belong to any set in
with any set in

P,ε. In this case,

A

A

A + [

ch, ch]d : A

{

−

∈ A
ch
P,ε, it means the support of the function gz

P,ε

.
}

will not intersect

·−z
h

(cid:0)

(cid:1)

Deﬁne the complement of

P,ε as

A

gz

(cid:16)

Z (cid:12)
(cid:12)
(cid:12)

z

· −
h

(cid:17)(cid:12)
(cid:12)
(cid:12)
⊥
P,ε =

dP

≤

cP [h

·

supp(gz(

)) + z] .
·

supp(gz(

Then the set h
intersections between h
))+z+[
set h
·

supp(gz(

·

·

·
−

A

: P [A]

ε

,
}

{

A

≤
)) + z will be completely covered by sets in
·
supp(gz(
ℓ−1, ℓ−1]d, which is (2ch+ℓ−1)d. The Euclidean volume of each set in

⊥
P,ε. To determine the maximum number of
⊥
P,ε, it suﬃces to consider the Euclidean volume of the enlarged
⊥
P,ε is ℓ−d. Therefore,

)) + z and sets in
·

∈ A

A

A

A

42

the set h

supp(gz(

)) + z can intersect with at most
·

·

(2ch + ℓ−1)d
ℓ−d

= (2chℓ + 1)d

(2c + 1)d

≤

sets in

⊥
P,ε. As a result, we conclude that

A

gz

This leads to our ﬁrst result. Let Ach

P,ε =

z

dP

· −
h

≤

c (2c + 1)d ε.

(cid:16)

Z (cid:12)
(cid:12)
ch
(cid:12)
P,ε be the union of sets in
∪A

(cid:17)(cid:12)
(cid:12)
(cid:12)

ch
P,ε, and

A

then

G

1 =

gz

n

(cid:16)

z

· −
h

(cid:17)

: z

6∈

Ach
P,ε

,

o

As remark, we note that the function class

(cid:16)

1 changes with respect to h, ε, as well as the probability measure P .

N

(2c + 1)d+1ε,

1, L1(P )

= 1,

ε

∀

∈

(0, 1].

G

(cid:17)

Next, we consider some z which belongs to some set in

ℓ−1 + 2ch

≤

2(c + 1)h, because hℓ

0.5. As a result,

≥

G

ch
P,ε. Each set in

A

ch
P,ε is a cube with edge length

A

N

hε, Ach

P,ε,

(cid:16)

≤

| · |

(cid:17)

XA∈Ach

P,ε

N (hε, A,

)
| · |

≤

card(

ch
P,ε)

A

·

c′ 1

εd ≤

c′ 1
εd+1 .

Here, c′ is some ﬁxed number that only depends on c and d. Using the Lipschitz property, we have

z

· −
h

(cid:16)

gz′

· −
h

(cid:18)

−

(cid:17)

z′

gz

Z (cid:12)
(cid:12)
(cid:12)
(cid:12)

dP

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

≤

≤

gz

z

· −
h

Z (cid:12)
(cid:12)
(cid:12)
ch−1
(cid:12)

(cid:16)
z
|

−

ch−1

z
|

−

z′

z′

|

|

(cid:17)
+

gz

−

· −
h

(cid:18)

gz

Z (cid:12)
(cid:12)
+ ch−1
(cid:12)
(cid:12)

(cid:18)
z
|

−

z′

+

gz
(cid:12)
(cid:12)
(cid:12)
gz′
(cid:12)

(cid:19)(cid:12)
(cid:12)
z′
(cid:12)
(cid:12)
−
(cid:19)
2ch−1

· −
h
z′

| ≤

· −
h

(cid:18)

z′

(cid:19)

z′

gz′

−

· −
h

(cid:18)

dP

z′

dP

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

· −
h
z′

−

(cid:18)
z
|

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

.
|

Now deﬁne

then

2 =

G

G\G

1 =

gz

n

(cid:16)

z

· −
h

(cid:17)

: z

∈

Ach
P,ε

,

o

N

(2c + 1)d+1ε,

(cid:16)

2, L1(P )

G

N

≤

(cid:18)

(cid:17)

(2c + 1)d+1
2c

hε, Ach

P,ε,

c′ 1
εd+1 .

| · |

(cid:19)

≤

Combining previous results, we have

N

(2c + 1)d+1ε,

(cid:16)

, L1(P )

G

≤

(cid:17)

c′ 1
εd+1 + 1.

SA-7.14 Proof of Corollary SA-3.1

This corollary follows directly from Lemma SA-3.6 and the properties of K ◦

µ,ν,h given in Lemmas SA-3.4 and SA-3.5.

43

SA-7.15 Proof of Theorem SA-3.1

We will apply Lemma SA-7.1. To start, consider the process (i.e., without the additional scaling in ¯Sµ,ν (y, x))

˜Sµ,ν (y, x) =

1
√n

n

i=1
X

hd+µ+|ν|K ◦

µ,ν,h (yi, xi; y, x) ,

which is the empirical process indexed by the function class

=

K

hd+µ+|ν|K ◦
n

µ,ν,h (

,
·

; y, x) : y
·

∈ Y

, x

∈ X

.

o

From Lemma SA-3.4, the functions in the above class are uniformly bounded. Corollary SA-3.1 shows that the
function class above is of VC type, and the covering number does not depend on the bandwidth. The measurability
[0, 1]d+1, and the functions
condition required in Lemma SA-7.1 also holds, as our function class is indexed by (y, x)
in

are continuous in y and x.
Now the only missing ingredient is the total variation of the functions in

. First, note that the function
; y, x) is Lipschitz continuous with respect to the arguments, and the Lipschitz constant is of order
·

hd+µ+|ν|K ◦
h−1. Therefore, its total variation is bounded by

µ,ν,h (

,
·

K

K

∈

TV(y,x) = TV

hd+µ+|ν|K ◦

µ,ν,h (

-

1
h

vol

supp

K ◦

µ,ν,h (

,
·

; y, x)
·

,

,
·

; y, x)
·

(cid:17)

(cid:0)
(cid:0)
)) denotes the Euclidean volume of the support of K ◦
where vol (supp (
,
µ,ν,h (
·
·
above total variation is further bounded by

(cid:16)

; y, x). Thanks to Lemma SA-3.4, the
·

(cid:1)(cid:1)

which holds for all functions in

. That is,

K

TV(y,x) - hd,

TVK = sup

y∈Y,x∈X

TV(y,x) - hd.

Putting all pieces together, we conclude that there exists a centered Gaussian process, ˜Gµ,ν which has the same

covariance kernel as ˜Sµ,ν , such that

P

sup
y∈Y,x∈X

"

˜S′

µ,ν (y, x)

˜Gµ,ν (y, x)

−

c1

≥

 s

hd log n
1
d+1

n

+

log3 n

r

n !# ≤

c2n−c3 ,

where ˜S′

µ,ν (y, x) is a copy of ˜Sµ,ν (y, x). This concludes our proof.

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

SA-7.16 Proof of Lemma SA-3.7

Consider ˇCµ,ν (y, x, y′, x′). Note that we can decompose the diﬀerence into

ˇCµ,ν (y, x, y′, x′)

−

Cµ,ν (y, x, y′, x′) =

ˇVµ,ν (y, x, y′, x′)
ˇVµ,ν (y, x)ˇVµ,ν(y′, x′)

−

Vµ,ν (y, x, y′, x′)
Vµ,ν (y, x)Vµ,ν (y′, x′)

=

ˇVµ,ν (y, x, y′, x′)

Vµ,ν (y, x, y′, x′)

−
ˇVµ,ν (y, x)ˇVµ,ν (y′, x′)

q

(I)

q

+ Cµ,ν (y, x, y′, x′)

p

Vµ,ν (y, x)Vµ,ν (y′, x′)
ˇVµ,ν (y, x)ˇVµ,ν (y′, x′) −

1

.

!

 s

|

(II)

{z

}

The probabilistic order of the second term is given in Lemma SA-3.2.

|

{z

}

Using similar techniques as in the proof of Lemma SA-2.1 or SA-3.2, it is also straightforward to verify that

44

term (I) has the same order. That is,

P

"

sup
y,y′∈Y,x,x′∈X |

(I)
|

> c1rVE

# ≤

c2n−c3 .

SA-7.17 Proof of Lemma SA-3.8

Consider an ε discretization of
Gaussian vectors, z, ˇz

Y × X
RL, such that

, which is denoted by

ε =

A

(yℓ, xT
{

ℓ )T : 1

≤

ℓ

≤

L

. Then one can deﬁne two

}

∈
Cov[zℓ, zℓ′ ] = C(yℓ, xℓ, yℓ′ , xℓ′ ), Cov[ˇzℓ, ˇzℓ′

Data] = ˇC(yℓ, xℓ, yℓ′ , xℓ′ ).
|

Then we apply the Gaussian comparison result in Lemma SA-7.2 and the error rate in Lemma SA-3.7, which lead to

sup
u∈R

= sup
u∈R

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)

(cid:20)

sup
1≤ℓ≤L |

ˇzℓ

| ≤

Y, X

P

−

(cid:21)

sup
1≤ℓ≤L |

zℓ

(cid:20)

u

| ≤

sup
1≤ℓ≤L |

ˇGµ,ν (yℓ, xℓ)

| ≤

Y, X

P

(cid:20)

−

(cid:21)

Gµ,ν (yℓ, xℓ)

u

-P

| ≤

q
2 +

h

"

(cid:18)

log n
nhd+2

1
4

#

(cid:19)

log

1
ε

.

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
1≤ℓ≤L |

u
(cid:12)
(cid:12)
(cid:12)
(cid:12)

u
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Since ε only enters the above error bound logarithmically, one can choose ε = n−c for some c large enough, so that
the error that arises from discretization becomes negligible. The same applies to ˆGµ,ν (yℓ, xℓ).

SA-7.18 Proof of Theorem SA-3.2

First consider ˇT◦

µ,ν (y, x). Since

sup
y∈Y,x∈X |

¯Sµ,ν (y, x)

| −

sup
y∈Y,x∈X |

ˇT◦

µ,ν (y, x)

¯Sµ,ν (y, x)

−

| ≤

≤

then with Lemma SA-3.3,

sup
y∈Y,x∈X |

ˇT◦

µ,ν (y, x)

|

sup
y∈Y,x∈X |

¯Sµ,ν (y, x)

|

+ sup

y∈Y,x∈X |

ˇT◦

µ,ν (y, x)

¯Sµ,ν (y, x)

,
|

−

P

(cid:20)

sup
y∈Y,x∈X |

¯Sµ,ν (y, x)

u

−

| ≤

c1rSE

−

(cid:21)

c2n−c3

≤

≤

P

P

(cid:20)

(cid:20)

sup
y∈Y,x∈X |

ˇT◦

µ,ν (y, x)

| ≤

u

(cid:21)

sup
y∈Y,x∈X |

| ≤

¯Sµ,ν (y, x)

u + c1rSE

+ c2n−c3 .

(cid:21)
¯Sµ,ν (y, x) is negligible compared to rSE (see

In the above, we also used the fact that the diﬀerence ˇS◦
Remark SA-2.3).

µ,ν (y, x)

−

By applying Lemma SA-3.1,

P

(cid:20)

sup
y∈Y,x∈X |

Gµ,ν (y, x)

sup
y∈Y,x∈X |

ˇT◦

µ,ν (y, x)

| ≤

sup
y∈Y,x∈X |

Gµ,ν (y, x)

| ≤

u

(cid:21)
u + c1(rSE + rSA)

+ c2n−c3 .

(cid:21)

u

−

| ≤

c1(rSE + rSA)

c2n−c3

−

(cid:21)

≤

≤

P

P

(cid:20)

(cid:20)

45

Finally, we apply the Gaussian comparison result in Lemma SA-3.8, which implies that

P

(cid:20)

sup
y∈Y,x∈X |

Gµ,ν (y, x)

u

−

| ≤

c1(rSE + rSA)

P

(cid:20)

−

(cid:21)

sup
y∈Y,x∈X |

Gµ,ν (y, x)

| ≤

u

(cid:21)

−

c2n−c3

−

(log n)√rVE

-P P

-P P

(cid:20)

(cid:20)

As a result,

sup
y∈Y,x∈X |

ˇT◦

µ,ν (y, x)

| ≤

sup
y∈Y,x∈X |

Gµ,ν (y, x)

| ≤

u

P

−

sup
y∈Y,x∈X |

(cid:21)
(cid:20)
u + c1(rSE + rSA)

ˇGµ,ν (y, x)

| ≤

P

−

(cid:21)

(cid:20)

sup
y∈Y,x∈X |

Y, X

u
(cid:12)
(cid:12)
(cid:12)
Gµ,ν (y, x)
(cid:12)

(cid:21)

| ≤

u

+ c2n−c3 + (log n)√rVE.

(cid:21)

sup
u∈R

sup
y∈Y,x∈X |

ˇT◦

µ,ν (y, x)

P

(cid:20)

-P c2n−c3 + (log n)√rVE + sup
u∈R

(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

u

(cid:21)

−

(cid:20)

sup
y∈Y,x∈X |

| ≤

P

(cid:20)

sup
y∈Y,x∈X |

ˇGµ,ν y, x

Gµ,ν (y, x)

| ∈

| ≤

Y, X

u
(cid:12)
(cid:12)
(cid:12)
[u, u + c1(rSE + rSA)]
(cid:12)
(cid:21)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Finally, due to Lemma SA-7.5, we have

P

sup
u∈R

sup
y∈Y,x∈X |

Gµ,ν (y, x)

(cid:20)

| ∈

[u, u + c1(rSE + rSA)]
(cid:21)

-

log n(rSE + rSA).

p

SA-7.19 Proof of Theorem SA-4.1

Note that θµ,ν (y, x) falls into the conﬁdence band ˇ
C

µ,ν (1

−

α) if and only if

A suﬃcient condition would then be

sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˇθµ,ν (y, x)

θµ,ν (y, x)

−
ˇVµ,ν (y, x)

q

ˇcvµ,ν (α).

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
y∈Y,x∈X

ˇT◦

µ,ν (y, x)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

+ sup

y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

q

E[ˇθµ,ν (y, x)

θµ,ν (y, x)

X]
|
ˇVµ,ν (y, x)

−

ˇcvµ,ν (α).

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

The conclusion then follows from Theorem SA-3.2 and the bias calculation in Lemma SA-2.2. The same analysis
applies to ˆ
C

µ,ν (1

α).

−

SA-7.20 Proof of Theorem SA-4.2

To start, we decompose the test statistic into

ˇTPS(y, x) = ˇT◦

µ,ν (y, x) +

E[ˇθµ,ν (y, x)]

θµ,ν (y, x)

θµ,ν (y, x)

θµ,ν (y, x; ˆγ)

−
ˇVµ,ν (y, x)

+

−

ˇVµ,ν (y, x)

.

Then by the leading bias order in Lemma SA-2.2 and the leading variance order in Lemma SA-2.3, we have that

q

q

> c1

rB
rV

(1 + rVE)

c2n−c3 .



≤



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P





sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E[ˇθµ,ν (y, x)]

θµ,ν (y, x)

−
ˇVµ,ν (y, x)

q

46

Similarly, under the null hypothesis,

P





sup
y∈Y,x∈X (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

θµ,ν (y, x)

θµ,ν (y, x; ˆγ)

−

ˇVµ,ν (y, x)

q

> c1

rPS
rV

(1 + rVE)

c2n−c3 .



≤



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Then we have the following error bound

P

(cid:20)

sup
u∈R

-P

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
y∈Y,x∈X |

ˇTPS(y, x)

log n

rSE + rSA +

u

| ≤
(cid:21)
rB + rPS
rV

P

−

(cid:20)

sup
y∈Y,x∈X |

ˇGµ,ν (y, x)

| ≤

+ (log n)√rVE.

(cid:19)

Y, X

u
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

As a result,

p

(cid:18)

sup
y∈Y,x∈X |

ˇTPS(y, x)

|

> cvµ,ν (α)

P

(cid:20)

≤

(cid:21)

α + c

log n

rSE + rSA +

(cid:18)

p

(cid:18)

rB + rPS
rV

(cid:19)

+ (log n)√rVE

.

(cid:19)

The same strategy can be employed to establish results for ˆTPS(y, x).

SA-7.21 Proof of Theorem SA-4.3

The conclusion follows directly from Theorem SA-4.1.

(cid:4)

(cid:4)

47

References

Chernozhukov, V., Chetverikov, D., and Kato, K. (2014). “Gaussian Approximation of Suprema of

Empirical Processes,” Annals of Statistics, 42 (4), 1564–1597.

Chernozhukov, V., Chetverikov, D., Kato, K., and Koike, Y. (2019). “Improved Central Limit

Theorem and Bootstrap Approximations in High Dimensions,” arXiv:1912.10529.

Fan, J. and Gijbels, I. (1996). Local Polynomial Modelling and Its Applications, New York: Chap-

man & Hall/CRC.

Gin´e, E., Lata la, R., and Zinn, J. (2000). “Exponential and Moment Inequalities for U-statistics,”

In High Dimensional Probability II: Springer.

de la Pe˜na, V. H. and Montgomery-Smith, S. J. (1995). “Decoupling Inequalities for the Tail

Probabilities of Multivariate U-statistics,” Annals of Probability, 23 (2), 806–816.

Rio, E. (1994). “Local Invariance Principles and Their Application to Density Estimation,” Proba-

bility Theory and Related Fields, 98 (1), 21–45.

van der Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence and Empirical Processes:

Springer.

48

