Empirical Standards for Repository Mining
Tushar Sharma
tushar@dal.ca
Dalhousie University
Halifax, NS, Canada

Preetha Chatterjee
preetha.chatterjee@drexel.edu
Drexel University
Philadelphia, PA, USA

Paul Ralph
paulralph@dal.ca
Dalhousie University
Halifax, NS, Canada

2
2
0
2

r
a

M
9
2

]
L
D
.
s
c
[

1
v
0
5
9
5
1
.
3
0
2
2
:
v
i
X
r
a

ABSTRACT
The purpose of scholarly peer review is to evaluate the quality of
scientific manuscripts. However, study after study demonstrates
that peer review neither effectively nor reliably assesses research
quality. Empirical standards attempt to address this problem by
modelling a scientific community’s expectations for each kind of
empirical study conducted in that community. This should enhance
not only the quality of research but also the reliability and pre-
dictability of peer review, as scientists adopt the standards in both
their researcher and reviewer roles. However, these improvements
depend on the quality and adoption of the standards. This tutorial
will therefore present the empirical standard for mining software
repositories, both to communicate its contents and to get feedback
from the attendees. The tutorial will be organized into three parts:
(1) brief overview of the empirical standards project; (2) detailed
presentation of the repository mining standard; (3) discussion and
suggestions for improvement.

CCS CONCEPTS
• Software and its engineering → Software libraries and repos-
itories; Empirical software validation.

KEYWORDS
Mining software repositories, Empirical standards, scholarly peer
review

ACM Reference Format:
Preetha Chatterjee, Tushar Sharma, and Paul Ralph. 2022. Empirical Stan-
dards for Repository Mining. In Proceedings of The 2022 Mining Software
Repositories Conference (MSR 2022). ACM, New York, NY, USA, 2 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 OVERVIEW OF EMPIRICAL STANDARDS

PROJECT

The Empirical Standards project aims to improve review quality,
consistency, and predictability in the peer-review process for em-
pirical studies by creating brief public documents that model our
community’s expectations for empirical research [3]. Creating and
evolving these standards, and then reconstituting peer review pro-
cesses around them, should also improve research quality and con-
sensus.

The project started with the ACM SIGSOFT special initiative
to improve paper and peer review quality [4]. Two years and 50+

contributors later, Ralph et al. [2] released the standards and a pro-
totype reviewing tool [1]. The standards and review tools are avail-
able online.1 The quickest way to grasp what is meant by an “em-
pirical standard” is to go to the standards website and look at a
standard for a familiar methodology.

Critically, the standards are method-specific. The standard for
experiments is very different from the standard for case studies.
The standard for questionnaire surveys is very different from the
standard for simulations. Standards must be method-specific to fos-
ter diversity in research and avoid cross-paradigm criticism (e.g.
one should not criticize a case study for lack of generalizability
because that’s not what a case study is for [5]). However, the stan-
dards share a common format, including several sections:

• Application: how to determine whether this standard ap-

plies to a given manuscript

• Essential attributes: properties a manuscript must have to

be acceptable in any peer reviewed venue

• Desirable attributes: properties that may enhance the rigor
and quality of a manuscript but are not always necessary
• Extraordinary attributes: properties associated with award-

quality research

• Anti-patterns: common problems seen in this kind of study
• Invalid criticisms: critiques reviewers should not make about

this kind of study

• Suggested reading: references to helpful works about the

methodology

• Exemplars: published manuscripts that effectively demon-
strate some (not necessarily all) of the essential, desirable
or extraordinary attributes.

Each standard was initially developed by a small team of scien-
tists experienced in that method. Much care was taken to craft the
essential attributes, as these will determine whether a manuscript
is accepted for publication.

However, research methods constantly evolve along with asso-
ciated expectations for them; hence, this project aims to constantly
update the standards to foster and incorporate emerging expecta-
tions. An empirical standard is supposed to model, not set, a com-
munity’s expectations around rigor. We therefore encourage inter-
ested readers to suggest improvements to the standard by raising a
pull-request on the GitHub repository.2 and raising a pull-request.

2 MOTIVATION AND OBJECTIVES
To improve peer review and paper quality, the standards must be
adopted in several ways: researchers using the standards to design
studies and prepare manuscripts; reviewers using the standards to

MSR 2022, May, 2022, Pittsburgh, PA, USA
2022. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1https://acmsigsoft.github.io/EmpiricalStandards/docs/
2https://github.com/acmsigsoft/EmpiricalStandards

 
 
 
 
 
 
MSR 2022, May, 2022, Pittsburgh, PA, USA

Preetha Chatterjee, Tushar Sharma, and Paul Ralph

evaluate manuscripts; editors and program chairs using the stan-
dards to define quality for their venues. This motivates the twin
aims of our tutorial:

(1) to communicate the contents of the repository mining stan-
dard to attendees and help attendees understand how to ap-
ply the standards in their various roles;

(2) to hear the attendee’s feedback on the repository mining
standard, understand their challenges and concerns (if any),
and conceive potential improvements to the standard.

3 TUTORIAL FORMAT
The tutorial comprises three parts. It will begin with a brief overview
of the empirical standards project. During the overview, we will
explain the motivation for and goals of the empirical standards, as
well as how the repository mining standard was developed, and
the process for updating and improving the standards.

In the second part of the tutorial, we will explain how to inter-
pret and use the repository mining standard. We will explain its
main elements, as follows.
Application: The standard applies to software engineering stud-
ies that use automated techniques to extract data from large-
scale data repositories and quantitatively analyze the con-
tents mined from the repositories.

Essential Attributes: The standard specifies the minimum essen-
tial attributes that the study must explain. These attributes
include the description and justification of data sources, repos-
itory selection criteria, and the procedure of data extraction.
Desirable and Extraordinary Attributes: The standard summa-
rizes attributes that are not universally necessary but tend
to improve the quality of this kind of study. These attributes
include aspects related to supplementary material, hypoth-
esis testing, qualitative analysis of construct validity and
dataset quality. The standard also discusses extraordinary
attributes such as establishing causality among the studied
variables.

Anti-patterns: The standard discusses several anti-patterns that
a repository mining study must avoid, including limiting
analysis to quantitative description, convenience sampling
without good selection criteria, and presenting insufficient
details about the data processing steps.

Invalid Criticisms: The standard provides a list of common but
invalid criticisms including needing to include more reposi-
tories and expecting different sources of repositories or data
than those selected and justified in the study. Reviewers
should abstain from lodging such criticisms.

Suggested Reading: The standard lists a set of articles from the
community that provide comprehensive treatment to one or
more aspects included in the standard.

Exemplars: The standard include references to some good exam-

ple of software repository mining studies.

In the third part of the tutorial, the presenters will open the
floor to the attendees to provide feedback on the current standard.
Additionally, suggestions will be sought to improve its usefulness
and adoption by the research community. We hope that up to half
of the session can be dedicated to discussion and feedback.

4 SPEAKER BIOGRAPHIES
Preetha Chatterjee, Ph.D. (University of Delaware), is an Assis-
tant Professor at Drexel University. Her research interests are in
improving software engineers’ tools and environments through
empirical data analysis, natural language processing and machine
learning. She serves on the OC/PC for several conferences such as
ICSE, MSR, ICSME, and SANER.
Tushar Sharma, PhD (AUEB, Greece), MS (IIT-Madras, India), is an
assistant professor at Dalhousie University. His research interests
include software quality, refactoring, and applied machine learn-
ing for software engineering. He worked with Siemens Research
for more than nine years. He co-authored Refactoring for Software
Design Smells: Managing Technical Debt and two Oracle Java cer-
tification books. He has founded and developed Designite which
is a software design quality assessment tool used by many practi-
tioners and researchers worldwide. He is an IEEE Senior Member.

Paul Ralph, PhD (British Columbia), is an award-winning scien-
tist, author, consultant, and Professor of Software Engineering at
Dalhousie University. His research intersects software engineer-
ing, human-computer interaction, and project management. Paul
is the editor of the Software Engineering Empirical Standards.

ACKNOWLEDGMENTS
We would like to acknowledge the amazing support of everyone
who contributed to the Empirical Standards for Software Engineer-
ing Research. A complete list of contributors is available at:
https://acmsigsoft.github.io/EmpiricalStandards/people/

REFERENCES
[1] Arham Arshad, Taher Ghaleb, and Paul Ralph. 2021. Towards a More Structured
Peer Review Process with Empirical Standards. In Evaluation and Assessment in
Software Engineering. 353–358.

[2] Paul Ralph. 2021. ACM SIGSOFT empirical standards released. ACM SIGSOFT

Software Engineering Notes 46, 1 (2021), 19–19.

[3] Paul Ralph, Nauman bin Ali, Sebastian Baltes, Domenico Bianculli, Jessica Diaz,
Yvonne Dittrich, Neil Ernst, Michael Felderer, Robert Feldt, Antonio Filieri, et al.
2020. Empirical standards for software engineering research. arXiv preprint
arXiv:2010.03525 (2020).

[4] Paul Ralph and Romain Robbes. 2020. The ACM SIGSOFT Paper and Peer Review
Quality Initiative: Status Report. ACM SIGSOFT Software Engineering Notes 45, 2
(2020), 17–18.

[5] Klaas-Jan Stol and Brian Fitzgerald. 2018. The ABC of software engineering re-
search. ACM Transactions on Software Engineering and Methodology (TOSEM) 27,
3 (2018), 1–51.

