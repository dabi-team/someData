Reﬂections on the Evolution of Computer Science Education

Sreekrishnan Venkateswaran
Kyndryl Corporation
Bangalore, India
s.krishna@kyndryl.com

2
2
0
2

l
u
J

9

]

Y
C
.
s
c
[

1
v
3
1
7
4
0
.
8
0
2
2
:
v
i
X
r
a

ABSTRACT
Computer Science education has been evolving over the years to
reﬂect applied realities. Until about a decade ago, theory of com-
putation, algorithm design and system software dominated the
curricula. Most courses were considered core and were hence
mandatory; the programme structure did not allow much of a
choice or variety. This column analyses why this changed Circa
2010 when elective subjects across scores of topics become part of
mainstream education to reﬂect the on-going lateral acceleration
of Computer Science.

Fundamental discoveries in artiﬁcial intelligence, machine learn-
ing, virtualization and cloud computing are several decades old.
Many core theories in data science are centuries old. Yet their
leverage exploded only after Circa 2010, when the stage got set for
people-centric problem solving in massive scale. This was due in
part to the rush of innovative real-world applications that reached
the common man through the ubiquitous smart phone. AI/ML
modules arrived in popular programming languages; they could
be used to build and train models on powerful - yet aﬀordable -
compute on public clouds reachable through high-speed Internet
connectivity. Academia responded by adapting Computer Science
curricula to align it with the changing technology landscape.

The goal of this experiential piece is to trigger a lively discussion
on the past and future of Computer Science education.

1. THE EVOLUTION
When I was a student over a quarter of a century ago, Com-
puter Science (CS) education was about learning ﬁnite automata,
intractability, formal language theory, compilers and assemblers.
They taught us the Turing Test, the Halting Problem and Rus-
sell’s Paradox. We recursively negotiated the Eight Queens Puz-
zle and grappled with the Traveling Salesman Problem [2] in
our programming labs. We were asked to design compilers for
hypothetical programming languages for our semester projects.
Lazy evaluations and Greedy approaches were well-known algo-
rithmic paradigms; no one would misunderstand them to be hu-
man behavioural patterns! Getting industry-ready for a CS stu-
dent meant acquiring the ability to develop and debug Operating
System (OS) code and design look-ahead parsers. Campus in-
terview questions would entail designing fast-performing system
software algorithms or a scholarly comparison of programming
languages.

During my Master’s in CS, when our department purchased new
DEC Alpha1 systems for the computer lab, we had to cross-
compile the compiler before we could start indulging on our pro-

1Alpha is the name of the RISC server developed in the 1990s by
Digital Equipment Corporation (DEC)

gramming experiments. We were only starting to ﬁgure out what
the Internet was, so we neither had to attend courses on social
network analysis nor master the Web. And because the online
learning economy would not be born for another 15 years, we had
to pay attention to what our professors were expounding in class.

When I joined my ﬁrst job from the campus, I found high-quality
alignment between what we had been taught and the customer
projects that we were assigned to. We had to, for example, con-
tend with the Dining Philosopher’s Problem2 when doing ker-
nel programming, and apply graph theory to optimally traverse
network routing tables. We did ‘C’ programming to code Linux
device drivers, developed ﬁrmware for OS-less devices in assem-
bly language, and wrote VHDL3 code to optimize software par-
titioning between the hardware and the OS. My life revolved
around kernel debuggers, logic analysers and oscilloscopes as we
did software-hardware co-design for embedded systems. The com-
pany’s University Relations program leveraged campus industry
labs to test our network protocol code for interoperability with
systems built by other vendors.

Circa 2010: Sands started to shift. Software began to touch ev-
ery facet of life. Application development turned mainstream.
The demand for designing complex user-centric software started
outpacing requirements for engineering system software. API4
Economy [10] took root as a technology design point as the world
economy digitized across industries and sectors. Cloud comput-
ing, as-a-service consumption, and pay-as-you-go billing started
changing information technology. The demand for OS internals
programming, deeply embedded ﬁrmware, and system software
reduced and they gravitated to the niche.

Artiﬁcial Intelligence (AI) and Machine Learning (ML) had been
in existence for decades. Neural Networks were ﬁrst proposed
in the 1940s, virtualization began in the 1960s and early forms
of cloud computing came soon after. Core discoveries associated
with statistical models and distributions were even older. The bi-
nomial theorem dated back several centuries before it was gener-
alized in the 1700s; the Central Limit Theorem (CLT) and Bayes’
Theorem had also been known since the 1700s. The Poisson dis-
tribution had been published in 1837; the ﬁrst Logistic Regression
model had also been developed during this period.

But the time had ﬁnally arrived for their adoption to explode,

2An example scenario developed by Edsger Dijkstra to demon-
strate how to design concurrent algorithms
3Very high-speed integrated circuit Hardware Description Lan-
guage (VHDL) is used to specify the logic design of a conﬁgurable
digital system such as a Complex Programmable Logic Device
(CPLD)
4Application Programming Interface

 
 
 
 
 
 
thanks to the new-found ease of consumption. Most popular
programming languages implemented AI/ML libraries, and they
could be trained and delivered through aﬀordable processing power
on the cloud reachable through high-speed Internet connectivity.
Smart phones brought a profusion of AI/ML use cases to real-
world applications accessed by the common man.

People-centric problem solving needs, thus, far surpassed system-
centric demands. Scalability requirements hence expanded expo-
nentially. Abstraction for easy use and reuse of software also be-
came maximalist. A programmer needed to only know API speci-
ﬁcations in order to leverage the technology in question. Software
systems became integration-friendly by design, opening up new
possibilities.

Platform-as-a-Service (PaaS) and library modules came of age.
They masked the theory and the design of complex algorithms,
encapsulated the intrinsic, and enveloped it with an interface en-
gineered for easy use and portability. You no longer designed
code to concurrently search billions of distributed records in the
blink of an eye; instead, you used the best-available-ﬁt existing
service as your starting point. You did not have to write an ML
model when you needed to automatically track moving objects
from a live video feed in real-time; rather, you chose from exist-
ing models that your favourite programming language supported
as importable modules, trained the chosen model with datasets
that you could ﬁnd or buy, and tuned hyperparameters until you
reached acceptable performance levels.

Software that can generate software gave rise to the low-code/no-
code movement that sought to enable application development
with minimal coding. A user could visually produce certain types
of applications simply by dragging and dropping requirements or
components into a graphical interface.

2. ENSUING SHIFTS IN CS CURRICULA
All these signalled a paradigm shift in CS education. Universi-
ties started adjusting curricula to minimize the ensuing angle of
deviation between academics and industry.

Elective courses were scarce in the CS curricula of the 1990s.
When I did my Bachelors in CS during that period, we had 21
mandatory core CS courses supplemented by 2 electives. All elec-
tives were department/disciplinary; the concept of open (or inter-
departmental) electives did not exist. We could choose the 2 elec-
tives out of a list of 3: Image Processing, Computer Graphics and
Computer Architecture. Today, the same CS program [7] has re-
structured to require 14 mandatory core CS courses along with 9
electives. The electives can be chosen from a large basket of intra
and inter departmental courses that include Natural Language
Processing, Web Programming, Cloud Computing, Data Mining,
Machine Learning, Medical Imaging, Hashing Techniques for Big
Data, DNA Computing, Bioinformics, Computer Vision and Em-
bedded Systems.

Most electives specify a set of other courses as prerequisites, which
introduces dependencies by design. Only certain permutations of
courses can thus be chosen, hence only a predetermined set of pro-
gramme outcomes are possible. If you consider courses as nodes,
prerequisites generate edges between them and create a directed
graph; only a preplanned set of end nodes (or specializations)
exist.

versity education with the recent lateral expansion of Computer
Science.

Let us zoom in on Software Engineering. Two decades back, Soft-
ware Engineering was rarely part of the CS curriculum. My insti-
tute did not oﬀer it. But today, it is often a mandatory part of CS
curricula world-wide. The course content, by default, covers Agile
frameworks and iterative development. More modern courses on
software engineering include the twelve-factor methodology [1] for
building software-as-a-service applications and the microservices
architecture. They bring out the new world model that expounds
the deliver-fail-change-succeed cycle where the way to a success-
ful product is to bring out some features, test the market in a
calibrated manner that limits the impact radius of failure, and
expand iteratively.

Massive Open Online Courses (MOOC) further democratized soft-
ware skilling and demystiﬁed AI usage, though not the core in-
trinsic of the technology.

3. BUSINESS ALIGNMENT
Simultaneously, a new dimension entered CS education: the no-
tion of Return on Investment (RoI). Business was no longer the
bounded domain of management graduates. Engineers were now
expected to innovate with cost-awareness because of two reasons.
First, the pressure from stock markets in the new-age economy
and the explosion of start-up ecosystems seeking to quickly mon-
etize technology became considerable. Delivering in real-time and
performing longer-term research now had to progress at a ratio
that would keep investors happy. Second, with the ease of as-you-
go consumption of the cloud and the API Economy, it became easy
for spending to exceed budgets, and this called for cost-conscious
software engineering.

Moreover, digitization started touching and changing the core of
business models. The classic example is how App-based rideshar-
ing changed the business model of the cab industry with cloud, AI
and analytics. Disruption at the business model level extended
across industries, from ﬁnance and manufacturing to telecom and
retail.

4.
IS CORE CS RESEARCH DIMINISHING?
This is not to suggest that core CS research is diminishing in
It is growing indeed. It is not uncommon to see a
real terms.
blockchain engineer model failure-safety after say, the Byzantine
Generals Problem [6]. Or to listen to a distributed applications
programmer worry about the consistency and durability of a ﬁ-
nancial transaction. A chatbot developer has to indeed dwell on
modelling ACID5 in order to ensure data integrity while support-
ing massive concurrency. Some applications have to be designed
to operate in near planet-scale, which may require a prowess in
algorithm design.

The low-code/no-code movement might have taken oﬀ, but it
still needs Computer Scientists to design and write programs that
write programs!

In percentage terms, however, the quantum of user-centric engi-
neering and up-the-stack application programming seem to have
decisively overtaken core CS and system software development
functions today.

This evolution in structure and content largely holds in CS schools
across the world [8] [4] [9]. The end-result is the alignment of uni-

5Atomicity - Consistency - Isolation - Durability are commonly
abbreviated as ACID

5. CONCLUDING THOUGHTS
Any 35-year career cycle is unlikely to be monotonic. Some ﬂuctu-
ations and at least a couple of disruptions are near-certain. How-
ever, basic skills usually endure. Whether you code in Python or
JavaScript or C, you can connect your work to principles of pro-
gramming languages if you look one level deeper. Whether you
explore a new ML model or perform processing around an ex-
isting model to solve a real-world problem, Donald Knuth’s Art
of Computer Programming [5] is likely to be relevant. Dijkstra’s
shortest path algorithm [3] is as relevant to Google Maps and to
help chart ﬂight paths as it has been to network routing.

Charles Darwin theorized that it is not the strongest species that
survive, but the most adaptable ones. A combination of mastery
of basic skills and the propensity to adapt and transform will
help not just survive a long career in technology, but to thrive
and ﬂourish!

6. REFERENCES
[1] The twelve-factor app, andy wiggins.

https://12factor.net/.

[2] Thomas H. Cormen, Charles E. Leiserson, Ronald L.

Rivest, and Cliﬀord Stein. Introduction to Algorithms. The
MIT Press, 3rd edition, 2009.

[3] Dijkstra’s shortest path algorithm.

https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm.

[4] Bachelor of technology (btech) program in computer science
and engineering, indian institute of technology, kanpur.
https://www.cse.iitk.ac.in/pages/ProgramBTech.html.
[5] D. E. Knuth. The Art of Computer Programming. Addison

Wesley, 3rd edition, 1997.

[6] Leslie Lamport, Robert Shostak, and Marshall Pease. The
byzantine generals problem. ACM Trans. Program. Lang.
Syst., 4(3):382–401, jul 1982.

[7] Curriculum and syllabi for b.tech in computer science and
engineering, national institute of technology, calicut, india.
https://minerva.nitc.ac.in/sites/default/files/programs/syllabi/B2018-SyllabusDocument-FINAL-APRIL-MAY2018_new-min%281%29.pdf.

[8] Bachelor of computing (honours) in computer science,

national university of singapore.
https://www.comp.nus.edu.sg/programmes/ug/cs/curr/.

[9] Stanford university undergraduate

major in computer science, course and requirement overview.
https://cs.stanford.edu/degrees/undergrad/Requirements.shtml.

[10] Wei Tan, Yushun Fan, Ahmed Ghoneim, M. Anwar

Hossain, and Schahram Dustdar. From the service-oriented
architecture to the web api economy. IEEE Internet
Computing, 20(4):64–68, 2016.

