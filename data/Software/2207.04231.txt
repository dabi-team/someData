2
2
0
2

l
u
J

9

]

G
L
.
s
c
[

1
v
1
3
2
4
0
.
7
0
2
2
:
v
i
X
r
a

CEG4N: Counter-Example Guided Neural
Network Quantization Reﬁnement

Jo˜ao Batista P. Matos Jr.1, Iury Bessa1, Edoardo Manino2, Xidan Song2, and
Lucas C. Cordeiro2,1

1 Federal University of Amazonas, Manaus-AM, Brazil
jbpmj@icomp.ufam.edu.br and iurybessa@ufam.edu.br
2 Univeristy of Manchester, Machester, United Kingdom
{lucas.cordeiro,eduardo.manino,xidan.song}@manchester.ac.uk

Abstract. Neural networks are essential components of learning-based
software systems. However, their high compute, memory, and power re-
quirements make using them in low resources domains challenging. For
this reason, neural networks are often quantized before deployment. Ex-
isting quantization techniques tend to degrade the network accuracy.
We propose Counter-Example Guided Neural Network Quantization Re-
ﬁnement (CEG4N). This technique combines search-based quantization
and equivalence veriﬁcation: the former minimizes the computational re-
quirements, while the latter guarantees that the network’s output does
not change after quantization. We evaluate CEG4N on a diverse set of
benchmarks, including large and small networks. Our technique success-
fully quantizes the networks in our evaluation while producing models
with up to 72% better accuracy than state-of-the-art techniques.

Keywords: Robust Quantization, Neural Network Quantization · Neu-
ral Network Equivalence · Counter Example Guided Optimization

1

Introduction

Neural networks (NNs) are becoming essential in many applications such as
autonomous driving [6], security, medicine, and business [2]. However, current
state-of-the-art NNs often require substantial compute, memory, and power re-
sources, limiting their applicability [9].

In this respect, quantization techniques help reduce the network size and
its computational requirements [9,24,16]. Here, we focus on quantization tech-
niques, which aim at reducing the number of bits required to represent the neural
network weights [16]. A desirable quantization technique produces the smallest
neural network possible from the quantization perspective. However, at the same
time, quantization aﬀects the functional behavior of the resulting neural network
by making them more prone to erratic behavior due to loss of accuracy [18]. For
this reason, existing techniques monitor the degradation in the accuracy of the
quantized model with statistical measures deﬁned on the training set [16].

However, statistical accuracy measures do not capture the network’s vulner-
ability to malicious attacks. Indeed, there may exist some speciﬁc inputs for

 
 
 
 
 
 
2

Matos Jr. et al.

which the network performance degrades signiﬁcantly [19,27,3]. For this reason,
we reformulate the goal of guaranteeing the accuracy of a quantized model un-
der the notion of equivalence [12,17,11,20]. This formal property requires that
two neural network models both produce the same output for every input, thus
ensuring that the two networks are functionally equivalent [28,30].

We are the ﬁrst to explore the combination of quantization techniques and
equivalence checking in the present work. Doing so guarantees that the quantized
model is functionally equivalent to the original one. More speciﬁcally, our main
scientiﬁc contributions are the following:

– We model the equivalence quantization problem as an iterative optimization-

veriﬁcation cycle.

– We propose CEG4N, a counter-example guided neural network quantization

technique that provides formal guarantees of NN equivalence.

– We evaluate CEG4N on both large (ACAS Xu [23] and MNIST [26]) and

small (Iris [13] and Seeds [8]) benchmarks.

– We demonstrate that CEG4N can successfully quantize neural networks and
produce models with similar or better accuracy than a baseline state-of-the-
art quantization technique (up to 72% better accuracy).

2 Preliminaries

2.1 Neural Network

NNs are non-linear mapping functions f : I ⊂ Rn → O ⊂ Rm consisting of a
set of L linked layers, organized as a direct graph. Each layer l is connected with
the directly preceding layer l − 1, i.e., the output of the layer l − 1 is the input
of the layer l. Exceptions are the ﬁrst and last layers. The ﬁrst layer is just a
placeholder for the input for the NN while the last layer holds the NN function
mapping f . A layer l is composed by a matrix of weights Wl ∈ Rn×m and a bias
vector bl ∈ Rm.

The output of a layer is computed by performing the combination of an
aﬃne transformation, followed by the non-linear transformation on its input
xl ∈ Rn(see Eq. (1)). Formally, we can describe the function yl : Rn → Rm that
computes the output of a layer l as follows:

and the function that computes the activated output of a layer l as follows:

yl(xl) = Wl · xl + bl

yσ
l (xl) = σ(yl(xl))

(1)

(2)

where σ : Rm → Rm is the activation function. In other words, the output l
is the result of the activation function σ applied to the dot product between
weight and input, plus the bias. The most popular activation functions are:
namely, ReLU, sigmoid (Sigm), and the re-scaled version of the latter known as

CEG4N: Counter-Example Guided Neural Network Quantization Reﬁnement

3

hyperbolic tangent(TanH) [10]. We focus on the rectiﬁed linear unit activation
function ReLU = max {0, yl}.

Considering the above, let us denote the input of a NN with L layers as

x ∈ I , and f (x) ∈ O as the output; thus, we have that:

f (x) = σ (yL(σ (yL−1(...(σ (y1(x))))))))

(3)

2.2 Quantization

Quantization is the process of constraining high precision values (e.g., single-
precision ﬂoating-point values) to a ﬁnite range of lower precision values (e.g.,
a discrete set such as the integers) [1,16]. The quantization quality is usually
determined by a scalar n (the available number of bits) that deﬁnes the lower
and upper bounds of the ﬁnite range. Let us deﬁne quantization as a mapping
function Qn : Rm×p → Im×p, formulated as follow:

Q (n, A) = clip

(cid:18)(cid:22) A

(cid:25)

q(A, n)

, −2n−1, 2n−1 − 1

(cid:19)

(4)

where A ∈ Rm× p denotes the continuous value– notice that A can be a single
scalar, a vector, or a matrix; n denotes the number of bits for the quantization,
q(A, n) denotes a function that calculates the scaling factor for A in respect to a
number of bits n, and (cid:98)·(cid:101) denotes rounding to the nearest integer. Deﬁning the
scaling factor (see Eq. 5) is an important aspect of uniform quantization [22,25].
The scaling factor is essentially what divides a given range of real values
A into an arbitrary number of partitions. Thus, let us deﬁne a scaling factor
function by qn(A), a number of bits (bit-width) to be used for the quantization
by n, a clipping range by [α, β], the scaling factor can be deﬁned as follow:

q(A, n) =

β − α
2n − 1

(5)

The min/max of the signal are often used to determining the clipping range
values, i.e., α = min A and β = max A. But as we are using symmetric quan-
tization, the clipping values are deﬁned as α = β = max([| min A|, | max A|]).
In practice, the quantization process can produce an integer value that lies out-
side the range of [α, β]. To prevent that, the quantization process will have an
additional clip step.

Eq. (6) shows the corresponding de-quantization function, which computes
back the original ﬂoating-point value. However, we should note that the de-
quantization approximates the original ﬂoating-point value.

ˆA = q(A, 2)Q (n, A)

(6)

2.3 NN quantization

In this section, we discuss how a convolutional or fully-connected NN layer can
be quantized in the symmetric mode. Considering l to be any given layer in a

4

Matos Jr. et al.

NN, let us denote xl, Wl, and bl as the original ﬂoating-point input vector,
the original ﬂoating-point weight matrix, and the original ﬂoating-point bias
vector, respectively, of the layer l. And applying the de-quantization function
from Eq. (6), where, we assume that A = ˆA. Borrowing from notations used
in Sections 2.1 and 2.2. We can formalize the quantization of a NN layer l as
follows:

y1(xl) = Wl · xl + bl

≈ q(Wl, nl)Q (nl, Wl) · xl + q(bl, nl)Q (nl, bl)

(7)

Notice that the bias does not need to be re-scaled to match the scale of the
dot product. Since we consider maximum scaling factor between q(Wl, nl) and
q(bl, nl)), both the weight and the bias share the same scaling factor in Eq. (7).
With that in mind, the formalization of a NNf in Eq. (3) can be reused to
formalize a quantized NN as well.

2.4 NN Equivalence

Let F and T be two arbitrary NNs, and let I ∈ Rn be the common input space of
the two NNs and O ∈ Rm be their common output space. Thus, NN equivalence
veriﬁcation is the problem of proving that F and T , or more speciﬁcally, their
corresponding mathematical functions f : I → O, t : I → O are equivalent. In
essence, by proving the equivalence between two neural networks, one can prove
that both NNs produce the same outputs for the same set of inputs. Currently,
the literature reports the following deﬁnition of equivalence.

Deﬁnition 1 (Top-1-Equivalence
equivalent, if arg max f (x) = arg max t(x), for all x ∈ I.

[7,30]). Two NNs f and t are Top-1-

Let us formalise the notion of Top-1 Equivalence in ﬁrst-order logic. This is
necessary for the comprehension of the equivalence veriﬁcation explained in the
following sections of the paper. But ﬁrst, we formalize some essential assumptions
for the correctness of the equivalence properties.

Assumption 1 Let f (x) be the output of the NN F in real arithmetic (without
quantization). It is assumed that arg max f (x) = y such that x ∈ H.

Assumption 2 Let f q(x) be the output of the NN F in a quantized form. There
is set of numbers of bits N such that arg max f (x) = arg max f q(x) = y for all
x ∈ H.

Note that the quantization of the NN f that results in the NN f q(x) depends
on the number of bits N . Refer to Eq. (7) to understand the relationship between
N and f q.

An instance of a equivalence veriﬁcation is given by a conjunction of con-
straints on the input ψx(x), the output ψy(y) and the NNs f and f q. ψ(f, f q, x, y) =

CEG4N: Counter-Example Guided Neural Network Quantization Reﬁnement

5

ψx(x) → ψy(y). We denote ψy(y) the equivalence constraint. Let ¯x = x + ˆx such
that |x + ˆx|∞ ≤ (cid:15), consider ¯x ∈ H and y ∈ G. Taking from Deﬁnition 1, we have
that:

– ψx(x) is an equivalence property such that ψx(x) ↔ ¯x ∈ H
– ψy(y) is an equivalence property such that ψy(y) ↔ arg max f q(x) = y

Note that, to prove the equivalence of f and f q, one may prove that the
property ψ(f, f q, x, y) holds for any given x and y. This approach may not be
feasible. But proving that ψ(f, f q, x, y) does not hold for some x and y is a more
tractable approach. If we do so, we can provide a counter-example.

2.5 Veriﬁcation of NN properties

In this paper, we use the classic paradigm of SMT veriﬁcation. In this paradigm,
the property to check (e.g., equivalence) and the computational model (e.g., the
neural networks) are encoded as a ﬁrst-order logic formula, which is then checked
for satisﬁability. Moreover, to keep the problem decidable, SMT restricts the full
expressive power of ﬁrst-order logic to a decidable fragment.

SMT formulas can capture the complex relationship between variables, hold-
ing real, integer values and other data types. If it is possible to assign values to
such variables that a formula is evaluated as true, then the formula is said to
be satisﬁable. On the other hand, if it’s not possible to assign such values, the
formula is said to be unsatisﬁable.

Given a NN F and its mathematical function f , a set of safe input instances
H ∈ Rn, and a safe domain G ⊆ Om– both deﬁned as a set of constraints, safety
veriﬁcation is concerned with the question of whether there exist an instance
x ∈ H such that f (x) /∈ G. An instance of a safety veriﬁcation is given by a
conjunction of constraints on the input ψx(x), the output ψy(y) and the NN f .
ψ(f, x, y) = ψx(x) → ψy(y) is said to be satisﬁable if there exists some x ∈ H
such that f (x) returns y for the input x and ψ(f, x, y) does not hold.

3 Counter-Example Guided Neural Network

Quantization Reﬁnement (CEG4N)

We deﬁne robust quantization (RQ) to describe the problem of maximizing the
quantization of a NN while keeping the equivalence between the original model
and the quantized one (see Deﬁnition 2). Borrowing from the notations used in
Section 2, we formally deﬁne RC as follows.

Deﬁnition 2 (Robust Quantization). Let f be the reference NN and H ∈
Rn be a set of inputs instances. We deﬁne robust quantization as a process that
performs the quantization of f hence resulting in a quantized model f q such that
arg max f (x) ⇐⇒ arg max f q(x) ∀ x ∈ H.

6

Matos Jr. et al.

From the deﬁnition discussed in Section 2.4, we preserve the equivalence
between the mathematical functions f and f q associated with the NNs. In the
RC, we shift the focus from the original NN to the quantized NN, i.e., we assume
that f is safe (or robust) and use it as a reference to deﬁne the safety properties
we expect for f q. By checking the equivalence of f and f q, we can state that
f q is robust, and therefore, we achieve a robust quantization. In more details,
consider a NN f with L layers. The quantization of f assumes there is a set
N = {n1, n2, · · · , nL}, where nl ∈ N represents the number of bits that should
be used to quantize the l-th layer in f . In our robust quantization problem,
we obtain a sequence N for which each n ∈ N is minimized (e.g., one could
minimize the sum of all n ∈ N ) and the equality between f and f q is satisﬁed.

3.1 Robust quantization as a minimization problem

We consider the robust quantization of a NN as an iterative minimization prob-
lem. Each iteration is composed of two complementary sub-problems. First, we
need to minimize the quantization bit widths, that is, ﬁnding a candidate set
N . Second, we need to verify the equivalence property, that is, checking if a
NN quantized with the bit widths in N is equivalent to the original NN. If the
latter fails, we iteratively return to the minimization sub-problem with additional
information. More speciﬁcally, we formalize the ﬁrst optimization sub-problem
as follows.

Optimization sub-problem o:

Objective: N o = arg min
1,...,no
L

no

(cid:88)

nl

l∈Nl≤L

s.t:

arg max f (x) = arg max f q(x), ∀ x ∈ Ho
nl ≥ N ∀ nl ∈ N o
nl ≤ N ∀ nl ∈ N o

CE

(8)

where f is the mathematical function associated with the NN F and f q is the
quantized mathematical function associated with the NN F, Ho
CE is a set of
counter-examples available at iteration o. Consider N and N as the minimum and
the maximum bit width allowed to be used in the quantization; these parameters
are constant. N ensures two things, it gives an upper bound to the quantization
bit width, and provides a termination criteria, if a candidate N o such that
nl = N for every nl ∈ N o, the optimization is stopped because it reached
our Assumption 2. In particular, our Assumption 2 ensures the termination
of CEG4N, and it is build over the fact that there is a set of N for which
the quantization introduces a minimal amount of error to NN. In any case, if
CEG4N proposes a quantization solution equal to the N , this solution is veriﬁed
as well, and in case the veriﬁcation returns a counter-example, CEG4N ﬁnishes
with failure. Finally, note that Ho
CE is an iterative parameter, meaning its value
is updated at each iteration o. This is done based on the veriﬁcation sub-problem
(formalized below).

CEG4N: Counter-Example Guided Neural Network Quantization Reﬁnement

7

Veriﬁcation sub-problem o:

In the veriﬁcation sub-problem o, we check whether the N o generated by the
optimization sub-problem o satisﬁes the following equivalence property:

ψ(f, f q, x, y) = ψx(x) → ψy(y)

if ψx(x) → ψy(y) holds for the candidate N o, the optimization halts and N o is
declared as solution; otherwise, a new counter-example xCE is generated. Itera-
tion o+1 starts where iteration o stopped. That is, the optimization sub-problem
o + 1 receives as parameter a set of Ho+1

CE such that Ho+1

CE = Ho

CE ∪ xCE.

3.2 The CEG4N framework implementation

We propose CEG4N framework, which is a counterexample-guided optimization
approach to solve the robust quantization problem. In this approach, we consider
combining two main modules to solve the two sub-problems presented in Section
3.1: the optimization of the bit widths for the quantization and the veriﬁcation of
the NN equivalence. The ﬁrst module that solves the optimal bit width problem
roughly takes in a NN and generates quantized NN candidates. Then, the second
module takes in the candidates and veriﬁes their equivalence to the original
model.

Figure 1 illustrates the overall architecture of the CEG4N framework. It
also shows how each framework’s module interacts with the other and in what
sequence. The GA module is an instance of a Genetic Algorithm. The GA module
expects two main parameters, NN and a set of counter-examples H.
CE We can
also specify a maximum number of generations the algorithm is allowed to run
and lower and upper bounds to restrict the possible number of bits. Once the
GA module produces a candidate, that is, a sequence of bit widths, for each
layer of the neural network, CEG4N generates the C-Abstraction code for the
original model and the quantized candidate and then checks their equivalence.
Each check for this equivalence property is exported to a unique veriﬁcation test
case. Then, it triggers the execution of the veriﬁer for each veriﬁcation test case
and awaits the veriﬁer output. Here, Veriﬁer module is an instance of a formal
veriﬁer (i.e., a Bounded Model Checker (BMC), namely, ESBMC [15]). This step
is done sequentially, meaning each veriﬁcation is run once the last veriﬁcation
terminates.

Once all veriﬁcation test cases terminate, CEG4N collect and process all
outputs and checks whether any counter-example has been found. If so, it up-
dates the set of counter-examples H,
CE and triggers the GA module execution
again, thus initiating a new iteration of CEG4N. If no counter-example is found,
CEG4N considers the veriﬁcation successful and terminates the quantization
process outputting the found solution.

We work with two functional versions of the NN. The GA module works
with a functional NN written in Python, while the veriﬁer module works with
a functional version of the NN written in C. The two models are equivalent
since they share the same parameters; the python model loads the parameters

8

Matos Jr. et al.

Fig. 1. CEG4N architecture overview, highlighting the relationship between the main
modules, and their inputs and outputs.

to a framework built over Pytorch [29]. The C version loads the weights into a
framework designed and developed in C to work correctly with the veriﬁer idioms
and annotations. We provide more details regarding the C implementations of
the NNs in Section A.2.

4 Experimental Evaluation

This section describes our experimental setup and benchmarks, deﬁnes our ob-
jectives, and presents the results.

4.1 Description of the Benchmarks

We evaluate our methodology on a set of feedforward NN classiﬁcation models
extracted from the literature [10,23,26]. We chose these speciﬁc ones based on
their popularity in previous NN robustness and equivalence veriﬁcation studies
[30,10]. Additionally, we include a few other NN models to cover a broader range
of NN architectures (e.g., NN size, number of neurons).

CEG4N: Counter-Example Guided Neural Network Quantization Reﬁnement

9

ACAS Xu The airborne collision avoidance system for unmanned aircraft
ACAS Xu dataset [23] is derived from 8 speciﬁcations (features boundaries
and expected outputs). ACAS Xu features are sensor data indicating the speed,
present course of the aircraft, and the position and speed of any nearby intruder
aircraft. An ACAS Xu NN is expected to give appropriate navigation advi-
sories for a given input sensor data. The expected outputs indicate that either
the aircraft is clear-of-conﬂict, or it should take soft or hard turns to avoid the
collision. We evaluated CEG4N on 5 pre-trained NNs, each containing 8 lay-
ers and 300 ReLU nodes each. The pre-trained NNs were obtained from the
VNN-COMP2021 [5] benchmarks3

MNIST MNIST is a popular dataset [26] for image classiﬁcation. The dataset
contains 70,000 gray-scale images with uniform size of 28x28 pixels, where the
original pixel values from the integer range [0, 255] are rescaled to the ﬂoating-
point range [0, 1]. We evaluated CEG4N on two NNs with 2 layers, one with 10
ReLU nodes each and another with 25 and 10 ReLU nodes. The NNs followed
the architecture of models described by the work of Eleftheriadis et al. [10].

Seeds The Seeds dataset [8] consists of 210 samples of wheat grain belong-
ing to three diﬀerent species, namely Kama, Rosa and Canadian. The input
features are seven measurements of the wheat kernel geometry scaled between
[0,1]. We evaluated CEG4N on 2 NNs, containing 1 layer, one containing 15
ReLU nodes, and the other containing 2 ReLU nodes. Both NNs were trained
for the CEG4N evaluation.

Iris The Iris ﬂower dataset [13] consists of 50 samples from three species of Iris
ﬂower (Iris setosa, Iris virginica and Iris versicolor ). The dataset is a popular
benchmark in machine learning for classiﬁcation, and the data is composed of
records of real value measurements of the width and length of sepals and petals
of the ﬂowers. The data was scaled to [0,1]. We evaluated CEG4N on 2 NNs, one
of them containing 2 layers with 20 ReLU nodes and the other having only one
layer with 3 ReLU nodes. Both NNs were trained for the CEG4N evaluation.

4.2 Setup

Genetic Algorithm. As explained in Section 3.1, we quantize the NNs with
a NSGA-II Genetic Algorithm module. We set the upper and lower bounds for
the allowed bit widths to 2 and 52 in all experiments. The lower bound was
chosen because 2 is the ﬁrst valid integer that does not break our quantization
formulas. The upper bound was chosen to match the signiﬁcand of the double-
precision ﬂoat format IEEE 754-1985 [21]. The upper bound value could be
higher depending on the precision of weights parameters of the NN, as the scaling

3 The pre-trained weight for the ACAS Xu benchmarks can be found in the following

repository: https://github.com/stanleybak/vnncomp2021

10

Matos Jr. et al.

factor could lead the quantization to large integer values. However, as we wanted
the framework to work on every NN in our experimentation setup without further
steps, we restricted the clipping range to a comfortable number to avoid integer
overﬂow.

Furthermore, we allow the GA to run for 110 generations for each layer in the
NN. This number of generations was deﬁned after extensive preliminary tests,
which conﬁrmed that GA could reach the optimal solution in most cases (see
Table 3 in Appendix A.4). Lastly, we randomly select the initial set of counter-
examples H from the benchmark set of each case study. The samples in H do
not necessarily have to be counter-examples, and any valid concrete input can
be speciﬁed. Our choice is justiﬁed by the practical aspect of using samples from
the benchmark set.

Equivalence Properties. One input sample was selected for each output class
and used to deﬁne the equivalence properties. Due to the high dimensional num-
ber of the features in the MNIST study case, we proposed a diﬀerent approach
when specifying the equivalence properties for the equivalence veriﬁcation. We
considered three diﬀerent approaches: 1) one in which we considered all features
in the input domain; 2) another one in which we considered only a subset of 10
out of the 784 features in the input domain; 3) a last one in which we considered
only a subset of 4 out of the 784 features in the input domain. The subset of
features in cases 2 and 3 was randomly selected.

Availability of Data and Tools. Our experiments are based on a set of
publicly available benchmarks. All tools, benchmarks, and results of our evalua-
tion are available on a supplementary web page https://zenodo.org/record/
6791964.

4.3 Objectives

Considering the benchmarks given in Section 4.1, our evaluation has the following
two experimental goals:

EG1 (robustness) Show that the CEG4N framework can generate robust

quantized NNs.

EG2 (accuracy) Show that the quantized NNs do not have a signiﬁcant
drop in accuracy compared to other quantization techniques.

4.4 Results

In our ﬁrst set of experiments, we want to achieve our ﬁrst experimental goal
EG1. We want to show that our technique CEG4N can successfully generate

CEG4N: Counter-Example Guided Neural Network Quantization Reﬁnement

11

quantized NNs that are veriﬁably equivalent to the original NNs. As a sec-
ondary goal, we want to perform an empirical scalability study to help us eval-
uate the computational demands for quantizing and verifying the equivalence of
NNs models. Our ﬁndings are summarized in Table 1.

Table 1. Summary of the CEG4N executions, including the models, number of fea-
tures, the number of bits per layer, and the status.

Model Features Equivalence Properties Iterations

Bits

Status

iris 3
seeds 2
seeds 15
acasxu 1
acasxu 2
acasxu 3
acasxu 4
acasxu 5

mnist 10

mnist 25

4
7
7
5
5
5
5
5
5
10
784
5
10
784

3
3
3
6
7
7
7
7
10
10
10
10
10
10

1
1
1
1
1
1
1
1
1
1
0
1
1
0

4, 3
4, 3
4, 2
6, 8, 7, 7, 9, 7, 6

completed
completed
completed
completed
10, 9, 9, 9, 7, 7, 10 completed
completed
5, 9, 10, 7, 8, 8, 5
8, 9, 14, 9, 10, 10, 7 completed
6, 12, 8, 8, 10, 10, 10 completed
completed
completed
timeout
completed
completed
timeout

4, 3
4, 3
4, 3
3, 3
3, 3
3, 3

All the CEG4N runs that were completed successfully took only 1 iteration to
ﬁnd a solution. However, we observed that four of the CEG4N attempts to ﬁnd a
solution for MNIST models resulted in a timeout. We attribute this observation
to a mix of factors. First is the high number of features in the MNIST problem.
Second, the network’s overall architecture requires many arithmetic operations
to compute the model’s output. Finally, we also observed that it took only a few
minutes for CEG4Nto ﬁnd a solution to the Iris, Seeds, and Acas Xu benchmarks.
In contrast, on MNIST, it took hours to either ﬁnd a solution or fail with a
timeout.

These results answer our EG1: overall, these experiments show that
CEG4N can successfully produce robust quantized models. Although, one
should notice that for larger NNs models, scalability should be a point of
concern due to our veriﬁer stage.

In our second set of experiments, we want to achieve our second experimen-
tal goal EG2. We primarily want to understand the impact of the quantization
performed by CEG4N on the accuracy of the NNs compared to other quan-
tization techniques. Due to our research’s novelty, no existing techniques lend
themselves to a fair comparison. For this reason, we take a recent post-training
quantization technique called GPFQ [31] and modify it to our needs. GPFQ [31]

12

Matos Jr. et al.

is a greedy path-following quantization technique that also produces quantized
models with ﬂoating/double-precision values. It works by iterating over each
layer of the NN and quantizing each neuron sequentially. More speciﬁcally, a
greedy algorithm minimizes the error between the original neural output and
the quantized neuron.

Table 2 summarizes the accuracy of the models quantized using CEG4N and
GPFQ. Note that we do not report the accuracy of the Acas Xu models because
the original training and test datasets are not public.

Table 2. Comparison of Top-1 accuracy for NNs quantized using CEG4N and GPFQ

Model Method Ref Acc (%) Quant Acc (%) Acc Drop (%)

iris 3

seeds 2

seeds 15

mnist 10

mnist 25

CEG4N
GPFQ
CEG4N
GPFQ
CEG4N
GPFQ
CEG4N
GPFQ
CEG4N
GPFQ

93.33

88.09

90.04

91.98

93.68

83.33
23.33
85.71
64.28
85.71
40.47
86.7
91.29
92.57
92.59

10.0
70.0
2.38
23.81
4.33
49.57
5.28
0.69
1.11
1.09

Our ﬁndings show that the highest drops in accuracy happen on the Iris
benchmark (10% for CEG4N and 70% drop for GPFQ). In contrast, the lowest
drops in accuracy happen on mnist 25 for CEG4N and on mnist 10 for GPFQ.
Overall, the accuracy of models quantized with CEG4N are better on the Iris
and Seeds benchmarks, while the accuracy of models quantized with GPFQ are
better on the mnist benchmarks, but only by a small margin. Our understanding
is that GPFQ shows high drops in accuracy for smaller NNs because the number
of neurons in each layer is small. As GPFQ focuses on each neuron individually,
it may not be able to ﬁnd a good global quantization.

These results answer our EG2: overall, these experiments show that
CEG4N can successfully produce quantized models with superior or sim-
ilar accuracy to other state-of-the-art techniques.

4.5 Limitations

Although we showed in our evaluation that the CEG4N framework can generate
a quantized neural network while keeping the equivalence between the original
NN and the quantized NN, we note that the architecture of the NN used in the
evaluation does not fully reﬂect state-of-the-art NN architectures. The NNs used

CEG4N: Counter-Example Guided Neural Network Quantization Reﬁnement

13

in our evaluation have few layers and only hundreds of ReLU nodes, while state-
of-the-art NNs may have hundreds of layers and thousands of ReLU nodes. The
main bottleneck is state-of-the-art veriﬁcation algorithms, which currently do
not scale to large neural networks. As it is, our technique could only quantized
80% of the NN in our experimental evaluation.

In addition, the ﬁeld of research on NN equivalence is relatively new and
there is no well-established set of benchmarks that works in this ﬁeld could
beneﬁt from [10]. Furthermore, our work is the ﬁrst to propose a framework that
mixes NN quantization and NN equivalence veriﬁcation. There is no comparable
methodology in the literature we could compare our approach with.

5 Conclusion

We presented a new method for NN quantization, called CEG4N, a post-training
NN quantization technique that provides formal guarantees of NN equivalence.
This approach leverages a counter-example guided optimization technique, where
an optimization-based quantizer produces quantized model candidates. A state-
of-the-art C veriﬁer then checks these candidates to prove the equivalence of
the quantized candidates and the original models or refute that equivalence by
providing a counter-example. This counter-example is then passed back to the
quantized to guide it to search for a feasible candidate.

We evaluate the CEG4N method on four benchmarks, including large models
(ACAS Xu and MNIST) and smaller models (Iris and Seeds). We successfully
demonstrate the application of the CEG4N for NN quantization, where it could
successfully quantize the networks while producing models with up to 72% better
accuracy than state-of-the-art techniques. However, CEG4N can only handle a
restricted set of NNs models, and further work needs to scale the CEG4N appli-
cability on a broader set of NNs models (e.g., NNs models with a more signiﬁcant
number of layers and neurons and higher numbers of input features).

For future work, we could explore other quantization techniques, which are
not limited to search-based quantization and other promising equivalence veriﬁ-
cation techniques using a MILP approach [30] or an SMT-based approach [10].
Combining diﬀerent quantization and equivalence veriﬁcation techniques can en-
able CEG4N to achieve better scalability and quantization rates. Another inter-
esting future work relates to the possibility of mixing quantization approaches
that generate quantized models, which operate entirely on integer arithmetic;
this can potentially improve the veriﬁcation step scalability of the CEG4N.

Acknowledgment

The work is partially funded by EPSRC grant EP/T026995/1 entitled “EnnCore:
End-to-End Conceptual Guarding of Neural Architectures” under Security for
all in an AI-enabled society.

14

Matos Jr. et al.

References

1. Abate, A., Bessa, I., Cattaruzza, D., Cordeiro, L.C., David, C., Kesseli, P.,
Kroening, D.: Sound and automated synthesis of digital stabilizing controllers
for continuous plants. In: Frehse, G., Mitra, S. (eds.) Proceedings of the 20th
International Conference on Hybrid Systems: Computation and Control, HSCC
2017, Pittsburgh, PA, USA, April 18-20, 2017. pp. 197–206. ACM (2017). https:
//doi.org/10.1145/3049797.3049802

2. Abiodun, O.I.,

Jantan, A., Omolara, A.E., Dada, K.V., Mohamed,
in artiﬁcial neural network applica-
e00938 (2018). https://doi.org/https:
https://www.sciencedirect.

N.A., Arshad, H.: State-of-the-art
tions: A survey. Heliyon 4(11),
//doi.org/10.1016/j.heliyon.2018.e00938,
com/science/article/pii/S2405844018332067
neural

Introduction

3. Albarghouthi, A.:

to

network

veriﬁcation. ArXiv

abs/2109.10317 (2021)

4. Bai, J., Lu, F., Zhang, K., et al.: Onnx: Open neural network exchange. https:

//github.com/onnx/onnx (2019)

5. Bak, S., Liu, C., Johnson, T.: The second international veriﬁcation of neural net-

works competition (vnn-comp 2021): Summary and results (2021)

6. Bojarski, M., del Testa, D.W., Dworakowski, D., Firner, B., Flepp, B., Goyal, P.,
Jackel, L.D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., Zieba, K.:
End to end learning for self-driving cars. ArXiv abs/1604.07316 (2016)

7. B¨uning, M.K., Kern, P., Sinz, C.: Verifying equivalence properties of neural net-
works with relu activation functions. In: Simonis, H. (ed.) Principles and Practice
of Constraint Programming - 26th International Conference, CP 2020, Louvain-la-
Neuve, Belgium, September 7-11, 2020, Proceedings. Lecture Notes in Computer
Science, vol. 12333, pp. 868–884. Springer (2020). https://doi.org/10.1007/
978-3-030-58475-7_50, https://doi.org/10.1007/978-3-030-58475-7_50
8. Charytanowicz, M., Niewczas, J., Kulczycki, P., Kowalski, P.A., (cid:32)Lukasik, S., ˙Zak,
S.: Complete Gradient Clustering Algorithm for Features Analysis of X-Ray Im-
ages, pp. 15–24. Springer Berlin Heidelberg, Berlin, Heidelberg (2010)

9. Cheng, Y., Wang, D., Zhou, P., Zhang, T.: A survey of model compression and

acceleration for deep neural networks. ArXiv abs/1710.09282 (2017)

10. Eleftheriadis, C., Kekatos, N., Katsaros, P., Tripakis, S.: On neural network equiv-

alence checking using smt solvers. ArXiv abs/2203.11629 (2022)

11. Esser, S.K., Appuswamy, R., Merolla, P., Arthur, J.V., Modha, D.S.: Backpropa-
gation for energy-eﬃcient neuromorphic computing. In: Cortes, C., Lawrence, N.,
Lee, D., Sugiyama, M., Garnett, R. (eds.) Advances in Neural Information Pro-
cessing Systems. vol. 28. Curran Associates, Inc. (2015), https://proceedings.
neurips.cc/paper/2015/file/10a5ab2db37feedfdeaab192ead4ac0e-Paper.pdf
12. Farabet, C., LeCun, Y., Kavukcuoglu, K., Martini, B., Akselrod, P., Talay, S.,

Culurciello, E.: Large-scale fpga-based convolutional networks (2011)

13. Fisher, R.A.: The use of multiple measurements in taxonomic problems. Annals of

Eugenics 7, 179–188 (1936)

14. Gadelha, M.R., Menezes, R.S., Cordeiro, L.C.: ESBMC 6.1: automated test case
generation using bounded model checking. Int. J. Softw. Tools Technol. Transf.
23(6), 857–861 (2021). https://doi.org/10.1007/s10009-020-00571-2

15. Gadelha, M.R., Monteiro, F.R., Morse, J., Cordeiro, L.C., Fischer, B., Nicole, D.A.:
Esbmc 5.0: An industrial-strength c model checker. In: 2018 33rd IEEE/ACM
International Conference on Automated Software Engineering (ASE). pp. 888–891
(2018). https://doi.org/10.1145/3238147.3240481

CEG4N: Counter-Example Guided Neural Network Quantization Reﬁnement

15

16. Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M.W., Keutzer, K.: A
survey of quantization methods for eﬃcient neural network inference. ArXiv
abs/2103.13630 (2022)

17. Han, S., Pool, J., Tran, J., Dally, W.J.: Learning both weights and connections for

eﬃcient neural network. ArXiv abs/1506.02626 (2015)

18. Hooker, S., Courville, A.C., Dauphin, Y., Frome, A.: Selective brain damage: Mea-
suring the disparate impact of model pruning. ArXiv abs/1911.05248 (2019)
19. Huang, X., Kroening, D., Kwiatkowska, M., Ruan, W., Sun, Y., Thamo, E., Wu,
M., Yi, X.: Safety and trustworthiness of deep neural networks: A survey. ArXiv
abs/1812.08342 (2018)

20. Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y.: Quantized neu-
ral networks: Training neural networks with low precision weights and activations.
ArXiv abs/1609.07061 (2017)

21. IEEE: Ieee standard for ﬂoating-point arithmetic. IEEE Std 754-2019 (Revision
of IEEE 754-2008) pp. 1–84 (2019). https://doi.org/10.1109/IEEESTD.2019.
8766229

22. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A.G., Adam, H.,
Kalenichenko, D.: Quantization and training of neural networks for eﬃcient integer-
arithmetic-only inference. CoRR abs/1712.05877 (2017), http://arxiv.org/
abs/1712.05877

23. Julian, K.D., Lopez, J., Brush, J.S., Owen, M.P., Kochenderfer, M.J.: Policy com-
pression for aircraft collision avoidance systems. In: 2016 IEEE/AIAA 35th Digi-
tal Avionics Systems Conference (DASC). pp. 1–10 (2016). https://doi.org/10.
1109/DASC.2016.7778091

24. Kirchhoﬀer, H., Haase, P., Samek, W., M¨uller, K., Rezazadegan-Tavakoli, H.,
Cricri, F., Aksu, E., Hannuksela, M.M., Jiang, W., Wang, W., Liu, S., Jain, S.,
Hamidi-Rad, S., Racap´e, F., Bailer, W.: Overview of the neural network com-
pression and representation (nnr) standard. IEEE Transactions on Circuits and
Systems for Video Technology pp. 1–1 (2021). https://doi.org/10.1109/TCSVT.
2021.3095970

25. Krishnamoorthi, R.: Quantizing deep convolutional networks for eﬃcient infer-
ence: A whitepaper. CoRR abs/1806.08342 (2018), http://arxiv.org/abs/
1806.08342

26. LeCun, Y., Cortes, C.: The mnist database of handwritten digits (2005)
27. Liu, C., Arnon, T., Lazarus, C., Barrett, C.W., Kochenderfer, M.J.: Algorithms
for verifying deep neural networks. Found. Trends Optim. 4, 244–404 (2021)
28. Narodytska, N., Kasiviswanathan, S.P., Ryzhyk, L., Sagiv, S., Walsh, T.: Verifying

properties of binarized deep neural networks. In: AAAI (2018)

29. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai,
J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning
library. In: Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E.,
Garnett, R. (eds.) Advances in Neural Information Processing Systems 32, pp.
8024–8035. Curran Associates, Inc. (2019), http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf

30. Teuber, S., Buning, M.K., Kern, P., Sinz, C.: Geometric path enumera-
tion for equivalence veriﬁcation of neural networks. 2021 IEEE 33rd In-
Intelligence (ICTAI) (Nov
ternational Conference on Tools with Artiﬁcial

16

Matos Jr. et al.

2021). https://doi.org/10.1109/ictai52525.2021.00035, http://dx.doi.org/
10.1109/ICTAI52525.2021.00035

31. Zhang, J., Zhou, Y., Saab, R.: Post-training quantization for neural networks with

provable guarantees. arXiv preprint arXiv:2201.11113 (2022)

A Appendices

A.1 Implementation of NNs in Python.

The NNs were built and trained using the Pytorch library [29]. Weights of the
trained models were then exported to the ONNX [4] format, which can be inter-
preted by Pytorch and used to run predictions without any compromise in the
NNs performance.

A.2 Implementation of NNs abstract models in C.

In the present work, we use the C language to implement the abstract represen-
tation of the NNs. It allows us to explicitly model the NN operations in their
original and quantized forms and apply existing software veriﬁcation tools (e.g.,
ESBMC [14]). The operational C-abstraction models perform double-precision
arithmetic. Although, we must notice that the original and quantized only di-
verge on the precision of the weight and bias vectors that are embedded in the
abstractions code.

A.3 Encoding of Equivalence Properties

Suppose, a NN F , for which x ∈ H is a safe input and y ∈ G is the expected
output of f the input. We now show how one can specify the equivalence prop-
erties. For this example, consider that the function f can produce the outputs of
F in ﬂoating-point arithmetic, while f q produces the outputs of F in ﬁxed-point
arithmetic (i.e. quantization). First, the concrete NN input x is replaced by a
non-deterministic one, which is achieved using the command nondet ﬂoat from
the ESBMC.

Listing 1.1. Deﬁnition of concrete and symbolic input domain in EBMC.

f l o a t x0 = −1.0;
f l o a t x1 = 1 . 0 ;
f l o a t s 0 = n o n d e t f l o a t ( ) ;
f l o a t s 1 = n o n d e t f l o a t ( ) ;

Listing 1.2. Deﬁnition of input constraints in EBMC.

const f l o a t EPS = 0 . 5 ;

ESBMC assume ( x0 − EPS <= s 0 && s 0 <= x0 + EPS ) ;
ESBMC assume ( x1 − EPS <= s 1 && s 1 <= x1 + EPS ) ;

ESBMC assert ( f ( s0 ,

Listing 1.3. Deﬁnition of output constraints in EBMC.
s 1 ) == f q ( s0 ,

s 1 ) ) ;

CEG4N: Counter-Example Guided Neural Network Quantization Reﬁnement

17

A.4 Genetic Algorithm Parameters Deﬁnition

In Table 3, we report a summary of experiments conducted to tune the param-
eters of the Genetic Algorithm, more precisely, the number of generations. For
example, a NN with 2 layers would require a brute force algorithm to search
for 522 combinations of bits widths for the quantization. Similarly, a NN with
7 layers would require a brute force algorithm to search for 527 combinations
of bits widths. We conducted a set of experiments where we ran the GA one
hundred times with a diﬀerent number of generations options ranging from 50
to 1000. In addition, we ﬁxed the population size to 5. From our ﬁndings, the
GA needs about 100 to 110 generations per layer to ﬁnd the optimal bit width
solution for each run.

Table 3. Summary of experiments for tuning Genetic Algorithm Parameters.

Number of Layers Generations Population Percentage of optimal solutions

7
7
7
7
2
2
2
2

800
750
700
50
250
200
150
50

5
5
5
5
5
5
5
5

100
100
98
0
100
100
96
30

