2
2
0
2

y
a
M
0
2

]

M

I
.
h
p
-
o
r
t
s
a
[

1
v
7
6
2
0
1
.
5
0
2
2
:
v
i
X
r
a

Department: Head
Editor: Name, xxxx@email

Reproducibility of the First
Image of a Black Hole in the
Galaxy M87 from the Event
Horizon Telescope (EHT)
Collaboration

R. Patel∗, B. Roachell∗, S. Ca´ıno-Lores∗, R. Ketron∗, J. Leonard∗, N. Tan∗, K. Vahi†,
D. A. Brown‡, E. Deelman†, and M. Taufer∗
U. Tennessee - Knoxville∗, U. Southern California†, and Syracuse U.‡
Emails: ∗taufer@utk.edu, †deelman@isi.edu, ‡dabrown@syr.edu

Abstract—
This paper presents an interdisciplinary effort aiming to develop and share sustainable
knowledge necessary to analyze, understand, and use published scientiﬁc results to advance
reproducibility in multi-messenger astrophysics. Speciﬁcally, we target the breakthrough work
associated with the generation of the ﬁrst image of a black hole, called M87. The image was
computed by the Event Horizon Telescope Collaboration. Based on the artifacts made available
by EHT, we deliver documentation, code, and a computational environment to reproduce the ﬁrst
image of a black hole. Our deliverables support new discovery in multi-messenger astrophysics
by providing all the necessary tools for generalizing methods and ﬁndings from the EHT use
case. Challenges encountered during the reproducibility of EHT results are reported. The result
of our effort is an open-source, containerized software package that enables the public to
reproduce the ﬁrst image of a black hole in the galaxy M87.

containerized environments, workﬂows

Index Terms: Multi-messenger astrophysics,
black hole image, software documentation,

IT Professional

Published by the IEEE Computer Society

© 2019 IEEE

1

 
 
 
 
 
 
Department Head

Introduction

Developing reproducible analyses is a chal-
lenging aspect of scientiﬁc research. Few real-
world studies have been performed to provide
guidance on the necessary processes and prod-
ucts, especially in domains relying on scientiﬁc
computing. There reproducibility is limited by the
availability of data, software, platforms, and doc-
umentation. Consequently, despite a group’s best
efforts, other scientists attempting to reproduce an
analysis may ﬁnd that the necessary information
is incomplete.

We present an interdisciplinary effort to de-
velop and share sustainable knowledge necessary
to understand, reproduce, and reuse the published
scientiﬁc results of the Event Horizon Telescope
(EHT) project’s analysis of the black hole in
the center of the M87 galaxy [1]. Unlike our
previous reproduction of Advanced LIGO’s ob-
servations [2], none of the authors of this pa-
per was involved in the original EHT analysis.
Thus, our work builds exclusively on the several
papers describing the EHT project workﬂow [3],
data [4], [5], and software [6] that are available
online. Each EHT paper presents speciﬁc aspects
of the scientiﬁc discovery but a comprehensive
approach including documentation, software, and
environment to reproduce the published results of
the EHT project is still missing. To this end, this
paper follows rigorous reproducibility directions
and expands preliminary work presented in a
poster [7].

As part of our contributions, we investigate
the availability and integrity of the data used to
recreate the images of the M87 black hole. We
model the image processing workﬂow and study
its limitations in terms of software availability,
dependencies, conﬁguration, portability, and doc-
umentation. We rebuild the workﬂow’s software
stack to reproduce the published images; we use
the software stack for our analysis of discrep-
ancies between original and reproduced results.
We document each step in this process, starting
from a systematic assessment of the availability of
data, software, and documentation. We deliver a
collection of fully documented containers for data
validation and image reconstruction. Finally, we
compile guidelines to increase the reproducibility
of computational workﬂows in scientiﬁc projects.

Our work enhances the reproducibility and reach
of scientiﬁc projects like the EHT project, and
facilitates the engagement of the overall scien-
tiﬁc community, including postdocs and students,
regardless of the domain.

M87 Event Horizon Telescope (EHT)

The EHT project uses Very Long Baseline
Interferometery (VLBI) to link together eight
radio telescopes around the world to study the im-
mediate environment of a black hole with angular
resolution comparable to the size of the black
hole itself. In April 2019, the EHT Collaboration
published measurements of the properties of the
central radio source in M87 [8], including the
ﬁrst direct image of a black hole. The results,
that received world wide attention, revealed for
the ﬁrst time a bright ring formed as light bends
in the intense gravity around a black hole in the
galaxy M87. The black hole is 6.5 billion times
larger than the Sun.

The EHT project provides links to their cali-
brated data [5] published in CyVerse Data Com-
mons, a publication describing their data process-
ing and calibration [4], a link to the software used
in their imaging workﬂow [6], and a publication
describing the imaging workﬂow [3]. The EHT
Collaboration released both data products and
software, hosting them on third-party repositories.
This is a common approach for many NSF-
funded projects ranging in size from individual
investigators to international collaborations.

Characterization of the EHT Workﬂow

The EHT workﬂow comprises three key com-
ponents: the data collection, the data processing,
and image building (see Figure 1).

Data Collection. Eight telescopes in the EHT
network collect radio interferometry data on cer-
tain days at certain times that have permissible
weather conditions for all sites, allowing the gath-
ering of data from multiple angles and effectively
turning the Earth into one single giant telescope.
The EHT data used for the generation of the ﬁrst
M87 black hole images consists of spatiotemporal
data of visibility amplitudes collected over ﬁve
days in 2017 (i.e., April 5, 6, 7, 10, and 11). For
each day, collected raw data contains both high
and low telescope frequencies.

2

IT Professional

telescopes collecting radio
Figure 1: High level overview of the EHT project with its eight
interferometry data, its three workﬂow components including three pipelines for image building, and
an image of the M87 black hole extracted from Figure 3 in [8].

Raw Data Processing. Raw data is ﬁrst pieced
together by using the Earth’s geometry and
clock/delay model to obtain a common time ref-
erence and the pairwise correlation coefﬁcients
are computed. Then, the data is reduced to a
manageable size for use in source imaging and
model ﬁtting: data is fringe-ﬁtted, calibrated a
priori, and network calibrated. Fringe-ﬁtting is
performed using the EHT-HOPS Pipeline for Mil-
limeter VLBI Data Reduction [9]. Data undergoes
a priori calibration and network calibration in the
post-processing stage of the EHT-HOPS pipeline
to create .uvfits [4] ﬁles. The processed data
is stored in the First M87 EHT Results [5]
data repository in .csv, .txt, and .uvfits
formats and are available to the community. We
use this processed data for analysis as both raw
data is not open-access and processing scripts are
not open-source at the time of this reproducibility
study.

Image Building. To reduce biases and increase
trust in results, the EHT Collaboration uses three
independently-designed pipelines to generate the
black hole images. They are: the Difmap M87
Stokes I Imaging Pipeline (DIFMAP) [10], the
EHT-Imaging M87 Stokes I Imaging Pipeline
(EHT-Imaging) [11], and the Sparse Modeling
Imaging Library for Interferometry (SMILI) [12].
Each pipeline is based on different methods, algo-
rithms, and software libraries but uses the same
input data. While the code for each individual
pipeline is available as open-source software,
the repositories do not contain all of the scripts
for image post-processing and generation. Pro-
viding documentation for scientiﬁc software is

challenging and we ﬁnd that documentation for
packaging, installing, and running the pipelines
can be incomplete or or is unavailable for certain
parts of the analysis.

Table 1 lists the available, unavailable, and
incomplete data, scripts, code, and documentation
used by the EHT workﬂow and shared with the
community before our reproducibility study. To
succeed in our effort, we generated and made
available the missing components.

Validating the Data Integrity

A key aspect of any work when reproducing
scientiﬁc results is the validation of the data
integrity: the data used for the generation of the
original EHT images should match the data made
available to the community. The integrity of data
is often considered secondary but can compro-
mise any reproducibilty effort, as it was previ-
ously demonstrated by the author in [2]. Figure 1
in [3] characterizes the original data in terms of
telescope baselines (i.e., u-v coverages). Scripts to
compare the properties of the original data with
available data were not available. We generated
the missing Python scripts and integrated them
into a Jupyter notebook using standard Python
modules such as matplotlib, pandas, and
numpy.

Figure 2 shows the comparison between the
properties of the original data used in Figure 1
in [3] (set of sub-ﬁgures in Fig. 2(a)) and the
reproduced properties using the available data (set
of sub-ﬁgures in Fig. 2(b)). Top left plots in the
two sets represent the intra-site EHT interferome-
ter baselines (short baselines). The top right plots
represent the aggregate baseline coverage of the

May/June 2019

3

Department Head

Table 1: Availability of data, scripts, code, and documentation before our reproducibility study.
Available and incomplete components are linked to the paper presenting them; missing components
are marked as unavailable.

Data

Raw data
Processed data

Unavailable
Available [5]

Scripts

Unavailable
Raw data processing
Processed data validation Unavailable
Unavailable
Image post-processing
Unavailable
Figure generation

Pipelines

DIFMAP

EHT-Imaging

SMILI

Code
Packaging
Installation manual
User manual

Available [10]
Unavailable
Incomplete [10]
Incomplete [10]

Available [11] Available [12]
Unavailable
Incomplete [11]
Incomplete [11]

Unavailable
Incomplete [12]
Incomplete [12]

EHT array for all four days observed. The bottom
plots show the short and long baseline coverage
observed by each telescope set at high and low
frequencies each day. Qualitatively we can assess
the integrity of the data that we input to the
three pipelines (i.e., DIFMAP, EHT-imaging, and
SMILI). The only difference is the incomplete left
plot in Fig. 2(b) due to the fact that the analysis is
based on both the available processed EHT data
and the unavailable intra-ALMA data from the
Atacama Large Millimeter/submillimeter Array
(ALMA). This external dataset is not included in
the EHT Data Products; based on communica-
tions with the EHT Collaboration, the data is not
needed for the pipelines to be able to reproduce
the black hole images.

Rebuilding the EHT Software Stack

The three EHT pipelines that are part of
image building can be modeled in terms of their
functional modules (Figures 3(a), 4(a), and 5(a)).
Each pipeline comprises a parameter deﬁnition
module for users to establish workﬂow-speciﬁc
behaviour as well as data preparation and data
pre-calibration modules to pre-process the input
ﬁles that are fed to the core of each pipeline.
A module performing the image reconstruction
cycles runs the image reconstruction algorithm;
note how each pipeline uses a different number

of cycles. The output of each pipeline includes a
ﬁnal image and statistics module that is used for
qualitative and quantitative analysis of the recon-
structed results, respectively. In SMILI, the ﬁrst
two modules are inverted and a image evaluation
module for data visualization is available at the
end of the pipeline.

Although the three pipelines share similar
high-level steps, each of them has its own set
of auxiliary steps, dependencies, and implemen-
tation. Figures 3(b), 4(b), and 5(b)) show the
dependencies and software components of each
pipeline in relation to its functional modules.

DIFMAP (Figure 3) is written in C and uses
the CLEAN algorithm for image reconstruction
involving iterative deconvolution, paired with a
technique called “difference mapping.” EHT’s
DIFMAP script takes a ﬁle containing observation
data, a mask (set of cleaning windows) ﬁle that
deﬁnes areas of interest for the algorithm to
iterate upon, and ﬁve command-line arguments,
which have been provided in the EHT reposi-
tory [6]. After loading this ﬁle, the script initial-
izes values, reads the ﬁle specifying the mask, and
begins the pre-calibration phase, which involves
its ﬁrst cleaning and phase self-calibration. Af-
terwards, the image undergoes twenty rounds of
amplitude self-calibrations and cleanings, and this
is when image reconstruction occurs.

4

IT Professional

ing the ﬁnal image and an image summary ﬁle
containing various imaging parameters and data
related to the imaging process.

SMILI (Figure 5) is also written in Pyhton
and uses RML like EHT-Imaging. Prior to imag-
ing, SMILI also uses the EHTIM module in order
to use data sets pre-calibrated consistently with
the other workﬂows. After the pre-calibration
stage, the software generates data tables that are
used for the ﬁnal imaging process. Reconstruction
of an image begins with a circular Gaussian
with successive iterations relying on the image
generated from the previous iteration. There are
four stages of iterations with each stage perform-
ing three imaging cycles. Once completed, the
software outputs the ﬁnal image and packages the
input, pre-calibrated, and self-calibrated data ﬁles
for traceability.

Note that each pipeline has its own GitHub
repository [10], [11], [12]. The compilation of
each pipeline’s original code from the three EHT
repositories resulted in several errors. For exam-
ple, on a Power9 system we missed dependencies
and had to remove optimization compilation ﬂags
from the installation script to generate the exe-
cutable code successfully. In general none of the
three pipeline codes include a comprehensive list
of required software dependencies and libraries
used or their versions. We solved dependencies
manually by editing problematic scripts; we used
Spack, Anaconda, and Pip to install the latest
stable version of each necessary library. Once the
compilation was successfully completed, we ex-
perienced runtime errors with EHT-Imaging and
SMILI that we solved by correcting syntax issues
in part of the Python code. We could not ﬁnd
documentation on how to transform the grayscale
output of DIFMAP and SMILI into the colored
and formatted images from Figure 11 in [3]. We
solved this issue by utilizing the EHTIM module
for post-processing of grayscale output. In the
process of rebuilding the EHT software stack,
we documented the software packages used, their
dependencies, the compilation requirements, and
the execution processes for all three pipelines,
completing the unavailable or incomplete com-
ponents in Figure 1.

(a) Original

(b) Reproduced

Figure 2: Comparison of the properties embedded
in the original data used by the the EHT project
(from Figure 1 in [3]) (a) and the available data
used in our reproducibility study (b).

EHT-Imaging (Figure 4) uses the Regu-
larized Maximum Likelihood (RML) method
of image reconstruction and relies heavily on
the eht-imaging Python module (EHTIM)
to complete its processes. The EHTIM module
deﬁnes numerous classes to allow the loading,
simulation, and manipulation of VLBI data. By
leveraging the classes in this module, the EHT-
Imaging workﬂow loads both the low and high
band data ﬁles of a single day’s observations into
a data object and performs various data prepara-
tion and pre-calibration steps. The workﬂow then
moves to an imaging cycle with four iterations.
Each successive iteration relies directly on the
image generated in the previous iteration. After
image is output. The
four iterations,
pipeline also allows for optional outputs includ-

the ﬁnal

May/June 2019

5

Department Head

Figure 3: Detailed view of the DIFMAP pipeline and its implementation.

Figure 4: Detailed view of the EHT-Imaging pipeline and its implementation.

Figure 5: Detailed view of the SMILI pipeline and its implementation.

Packaging and Distribution

To support the portability of the ETH work-
ﬂow across different platforms, we created a
collection of four Docker containers that allows
users to reproduce two key results from the EHT
project: the characterization of available data (i.e.,
Figure 1 in [3]) and the ﬁnal EHT images of the
black hole in Figure 11 in [3]). A ﬁrst container
hosts the entire setting to reproduce the validation
of the data integrity; its includes the data tarball
from the EHT Data Product page along with our
Bash, Python, and Docker scripts. We developed
these scripts to automate the installation and
conﬁguration of the environment
in an easily
accessible and portable way. In order for users
to be completely satisﬁed with the validation of
the data integrity, we have incorporated a spare

tarball within the container for users to perform
the md5sum program on it
to compare with
md5sum of the data from the EHT Data Products
page. If both md5sum match, then users knows
that the data in the Data Products page has not
been modiﬁed in any way, and they can move on
with the validation by running the Python scripts
to reproduce the images of the black hole. The
other three containers are used to reproduce the
ﬁnal EHT images of the black hole. Each of the
EHT pipelines is packaged into an independent
container that automates their installation, de-
pendency setup, environment conﬁguration, and
execution. The containers include our own scripts
and auxiliary ﬁles for conducting the image post-
processing steps, which are not available in the
original EHT repository.

6

IT Professional

All four containers are publicly available in
a Docker Hub1. Additional documentation for
deploying and using these containers is available
in Github2, along with the scripts to generate the
ﬁgures reproduced in this paper. These materials
augment existing containers in the EHT Docker
Hub3 and the EHT repositories [6].

Reproducing EHT Images

We tested the containerized pipelines both on
commodity hardware (a laptop with Inter CPU)
and a Power9 cluster at the University of Ten-
nessee, Knoxville. Figure 6 compares our results:
Figure 6a shows the original images from Figure
11 in [3] and Figure 6b shows our reproduced
images using the containerized pipelines. The two
ﬁgures show that we can reproduce the M87
images for all three pipelines.

The images in Figure 6 provide us with a
qualitative comparison. Both sets of images look
visually similar in terms of shape and brightness,
and the similarity is consistent across pipelines.
To perform a quantitative analysis, we compare
the “closure” quantities reported in Table 5 in [3]
with those reported by our executions of the the
three pipelines. For each day and each pipeline,
we compare both the χ2
log CA quantities
computed across the top set of parameters [3].
For brevity, we only report the values with 0%
systematic uncertainty. We observe consistency
between the two sets of results with no per-
fect agreement for the EHT-Imaging and SMILI
pipelines. We also ﬁnd a larger difference be-
tween the original and reproduced values for the
DIFMAP pipeline:
this is consistent with the
discussion of the different time averaging used
in DIFMAP.

CP and χ2

Lessons Learned and Guidelines

We compile lessons learned and guidelines to
support the reproducibility of scientiﬁc projects
based on our experience and observations repro-
ducing the M87 black hole images from the EHT
project.

Data Availability. The unavailability of the
raw data made the direct validation of the pipeline

1https://hub.docker.com/repository/docker/

globalcomputinglab/reproducibility-eht

2https://github.com/TauferLab/Reproducibility EHT
3https://hub.docker.com/u/eventhorizontelescope

(a) Original

(b) Reproduced

Figure 6: Final images obtained from the original
publication from Figure 11 in [3] (a) and our
reproduced results (b).

input data unfeasible. As a proxy for data val-
idation, we reproduced Figure 1 in [3], as this
ﬁgure captures properties of the data telescope
frequency and coverage. We are able to reproduce
most of these properties except for the intra-site
EHT interferometer baselines (short baselines)
because the data from the intra-ALMA data) is
not available. While this was not
the case in
this study, any incomplete or missing dataset may
result in the users inability to fully verify the data
integrity, and can threaten the entire reproducibil-
ity process. Data size or ownership constraints
can be an obstacle to make raw data available
to the public. Under these circumstances, data
integrity mechanisms such as hashes ensure the
correctness of processed data when releasing the
raw data is not feasible. We add the additional
service to run an MD5 integrity check for the
pipeline input data as part of our EHT container
set to facilitate data integrity validation.

May/June 2019

7

Department Head

DIFMAP

EHT-Imaging

SMILI

Original

Reproduced

Original

Reproduced

Original

Reproduced

9.40 ± 3.35
4.99 ± 1.17

6.81(δ = 2.59)
35.12(δ = 30.13)

1.00 ± 0.13
0.97 ± 0.27

1.03(δ = 0.03)
1.38(δ = 0.41)

1.09 ± 0.21
1.11 ± 0.28

1.08(δ = 0.01)
1.29(δ = 0.18)

4.40 ± 1.45
3.49 ± 1.51

2.84(δ = 1.56)
24.43(δ = 20.94)

1.59 ± 0.16
1.00 ± 0.14

0.88(δ = 0.71)
1.21(δ = 0.21)

1.49 ± 0.23
1.22 ± 0.25

1.30(δ = 0.19)
1.62(δ = 0.40)

3.93 ± 2.10
1.57 ± 0.64

2.36(δ = 1.57)
22.74(δ = 21.17)

0.83 ± 0.10
1.30 ± 0.17

0.99(δ = 0.16)
0.82(δ = 0.48)

0.75 ± 0.14
1.11 ± 0.32

1.24(δ = 0.49)
0.86(δ = 0.25)

4.01 ± 1.67
3.33 ± 1.01

1.91(δ = 2.1)
61.32(δ = 57.99)

0.97 ± 0.10
0.99 ± 0.13

0.88(δ = 0.09)
0.85(δ = 0.14)

1.23 ± 0.28
1.14 ± 0.13

1.00(δ = 0.23)
1.04(δ = 0.1)

April 5
Top Set:

April 6
Top Set:

April 10
Top Set:

April 11
Top Set:

χ2
cp
χ2

logCA

χ2
cp
χ2

logCA

χ2
cp
χ2

logCA

χ2
cp
χ2

logCA

Table 2: Closure quantity χ2 values and statistics for top set images with 0% systematic uncertainty.
We compute the difference δ between the Top Set values in Table 5 in [3] and our reproduced values.
None of the values agree exactly, but our value is consistent with the spread reported in [3] for the
EHT-Imaging and SMILI pipelines.

Software Availability. Several pieces of soft-
ware were unavailable at different stages of the
EHT workﬂow, and for the three pipelines. The
raw data and corresponding software to process
the data are not available, neither are the scripts
to run the data validation. We developed those
scripts for data validation purposes. The code for
running the three pipelines is completely available
but
the image post-processing scripts are not,
which forced us to experiment with different
settings in order to obtain results comparable to
the original for each pipeline. Finally, the plotting
libraries used to reproduce the results in Figure
1 from [3] were insufﬁciently deﬁned. Thus, we
manually tuned our plotting scripts to obtain a
suitable plotting conﬁguration. The qualitative
differences between the original and reproduced
images can be the result of our manual tuning,
which indicates that just sharing the core for the
three pipelines is not sufﬁcient to reproduce the
original images of the black hole in the galaxy
M87. To support portability across platforms,
we generated four containers that allow users to
execute the data integrity validation and original
pipeline codes. We also enable execution of the
end-to-end workﬂow by providing all auxiliary
materials for the image post-processing, ﬁgure
generation, and result analysis.

Documentation Availability. In general, there
is insufﬁcient documentation on how to pack-

age, install, and execute the EHT pipelines, as
well as on how to perform both qualitative and
quantitative analyses of the results. This hinders
the overall reproducibility effort. For instance,
documentation is key to reproduce Table 5 in [3].
Regarding the pipelines, there is insufﬁcient infor-
mation about software dependencies and versions
used as well as ﬁle locations and their use.
Documenting the whole EHT workﬂow beyond
its image reconstruction components is crucial for
the successful reproducibility of the results. Our
documentation covering conﬁguration and use of
the entire EHT workﬂow was instrumental for the
success of our reproducibility study.

Software Packaging. The incomplete doc-
umentation resulted in installation, dependency,
and portability challenges. We manually edited
dependencies to allow installation and compila-
tion, and had to override installation instructions
that were resulting in unstable environments. We
found that by containerizing the workﬂow we
can hide these challenges from the end user,
simplifying the installation and deployment of the
EHT workﬂow.

Methods Description. Incomplete descrip-
tions of the results analysis process (e.g., the data
averaging time or the additional systematic error
budget added to the uncertainties) complicates
the reproducibility of the χ2 statistics in Table
5 in [3], as errors add up quickly. Conducting

8

IT Professional

an adequate quantitative assessment of the ﬁnal
results becomes very challenging under these cir-
cumstances. Members of the EHT Collaboration
highlighted in conversations with the authors of
this paper how a qualitative comparison of the
images is more interpretable.

Access to Final Results. The authors of [3]
did not release the output ﬁducial images, and
therefore we did not have a ﬁxed reference to use
for direct comparison of our reproduced images
and the original ones. This, in addition to partial
access to the data and incomplete description of
the methods used, prevents us from conducting a
complete validation of the reproduced images.

Access to Distributed Knowledge. The EHT
Collaboration made substantial investments to al-
low independent users to qualitative and quan-
titatively reproduce their results and ensure the
robustness of the EHT project. Nonetheless, we
found challenging to reproduce the original re-
sults without direct knowledge of the methods
and analyses, or the direct collaboration with
the authors of the studies. Our experience illus-
trates the general challenges that users external
to a project face when gathering knowledge on
data, code, and documentation that was originally
generated from multiple teams in a distributed
fashion. The effort of the EHT Collaboration to
remove biases by designing and deploying three
completely separate pipelines, while instrumental
for the trustworthiness of the project results, is
also an obstacle to the project reproducibilty.

Conclusions

In this paper we deliver our experience re-
producing the black hole images from the EHT
project, and report new guidance and practices
for building reproducible scientiﬁc research. Our
work complements the work of the EHT Collabo-
ration with supplemental data, scripts, documen-
tation, and a set of containers. Postdocs, graduate
and undergraduate students, and even high school
students can beneﬁt from accessing our data and
code, and using our documentation to reproduce
ﬁndings from the EHT project, learn about the
EHT funding, and ultimately get
involved in
STEM research. Our guidance and practices can
be incorporated more broadly by other scientiﬁc
workﬂows. The EHT project continues to be a
leader in reproducibility efforts and have provided

comprehensive data products for their recent ob-
servations of Sag A∗.

Assessing the level of detail required to cover
the vast knowledge developed in a project the size
of EHT is a complex task. Finding the balance
between the effort from original research teams
to enable reproducibility, and users attempting to
reproduce the results is still an open question.
Our experience with the EHT and LIGO projects
reveals an important and recurring issue in re-
producibility: challenges remain in disseminating
ﬁndings in a way that allows reproducibility of
results without direct interaction with the original
team that produced them.

ACKNOWLEDGMENT

This work was

supported in part by
NSF grants #2041977, #1941443, #2041901,
#1664162 and #2041878. We thank Kazunori
Akiyama, Lindy Blackburn, Chi-kwan Chan, and
Rebecca White for helpful discussions.

is a fourth-year undergraduate student
Ria Patel
majoring in Computer Science and minoring in Math-
ematics and Physics at the University of Tennessee,
Knoxville. She is an Undergraduate Research Assis-
tant in the Global Computing Lab under Dr. Taufer.

Brandan Roachell is an undergraduate student ma-
joring in Computer Science and minoring in Mathe-
matics at the University of Tennessee, Knoxville. He
is an Undergraduate Research Assistant in the Global
Computing Lab under Dr. Taufer.

Silvina Ca´ıno-Lores is a Research Assistant Pro-
fessor at the University of Tennessee, Knoxville. She
obtained her Ph.D. in Computer Science and Tech-
nology from Carlos III University of Madrid (Spain) in
2019. Her research interests include cloud comput-
ing, in-memory computing and storage, HPC scien-
tiﬁc simulations, and data-centric paradigms.

Ross Ketron is an undergraduate student in Com-
puter Science at
the University of Tennessee,
Knoxville and an Undergraduate Research Assistant
in the Global Computing Lab under Dr. Taufer.

Jacob Leonard is an undergraduate student
in
Computer Science at the University of Tennessee,
Knoxville and an Undergraduate Research Assistant
in the Global Computing Lab under Dr. Taufer.

May/June 2019

9

“First M87 Event Horizon Telescope Results. IV. Imag-

ing the Central Supermassive Black Hole,” The Astro-

physical Journal Letters, vol. 875, no. 1, p. L4, 2019.

4. ——, “First M87 Event Horizon Telescope Results. III.

Data Processing and Calibration,” The Astrophysical

Journal Letters, vol. 875, no. 1, p. L3, 2019.

5. The Event Horizon Telescope Collaboration, “First M87

EHT Results: Calibrated Data,” 2019, Accessed on

2022-04-30. [Online]. Available: https://datacommons.

cyverse.org/browse/iplant/home/shared/commons

repo/curated/EHTC FirstM87Results Apr2019

6. ——,

“First M87 EHT Results:

Imaging Pipelines,”

GitHub, 2019, Accessed on 2022-04-30.

[Online].

Available:

https://github.com/eventhorizontelescope/

2019-D01-02

7. R. Ketron, J. Leonard, B. Roachell, R. Patel, R. White,

S. Ca´ıno-Lores, N. Tan, P. Miles, K. Vahi, E. Deelman,

and M. Taufer, “A Case Study in Scientiﬁc Repro-

ducibility from the Event Horizon Telescope (EHT),” in

2021 IEEE 17th International Conference on eScience

(eScience).

IEEE, 2021, pp. 249–250.

8. K. Akiyama, A. Alberdi, W. Alef, K. Asada, and R. Azuly,

“First M87 Event Horizon Telescope Results. I. The

Shadow of the Supermassive Black Hole,” Astrophys.

J. Lett, vol. 875, no. 1, p. L1, 2019.

9. L. Blackburn, C. Chan, G. B. Crew, V. L. Fish, S. Is-

saoun, M. D. Johnson, M. Wielgus, K. Akiyama, J. Bar-

rett, and K. L. Bouman, “EHT-HOPS pipeline for millime-

ter VLBI data reduction,” The Astrophysical Journal, vol.

882, no. 1, p. 23, 2019.

10. M. C. Shepherd, “DIFMAP,” CalTech, 2021, Accessed

on 2022-05-02. [Online]. Available: https://sites.astro.

caltech.edu/∼mcs/difmap/index.html

11. A. Chael,

“EHTIM (EHT-Imaging),” GitHub, 2007,

Accessed on 2022-05-02.

[Online]. Available: https:

//github.com/achael/eht-imaging

12. SMILI Developer Team,

“SMILI,” GitHub, 2018,

Accessed

on

2022-05-02.

[Online].

Available:

https://github.com/astrosmili/smili

Department Head

Nigel Tan is an graduate student in Computer Sci-
ence at the University of Tennessee, Knoxville. His
research interests are in software portability across
heterogeneous platforms.

Karan Vahi is a Senior Computer Scientist at USC
Information Sciences Institute. Vahi received a M.S in
Computer Science in 2003 from University of South-
ern California. His research interests include scientiﬁc
workﬂows and distributed computing systems. Con-
tact him at vahi@isi.edu.

Duncan A. Brown is the Charles Brightman Pro-
fessor of Physics at Syracuse University. Dr. Brown
received a Ph.D. degree in physics from the University
of Wisconsin-Milwaukee in 2004. He was a member
of
the LIGO Scientiﬁc Collaboration from 1999 to
2018 and is a fellow of the American Physical Society.
His research is in gravitational-wave astronomy and
astrophysics, and the use of
large-scale scientiﬁc
workﬂows. Contact him at dabrown@syr.edu.

Ewa Deelman received her PhD in Computer Sci-
ence from the Rensselaer Polytechnic Institute. She
is a Research Director at USC/ISI and a Research
Professor at the USC Computer Science Department
and the lead the design and development of the Pe-
gasus Workﬂow Management software. Her research
explores the interplay between automation and the
management of scientiﬁc workﬂows that include re-
source provisioning and data management.

holds the Jack Dongarra Profes-
Michela Taufer
sorship in High Performance Computing within the
Department of Electrical Engineering and Computer
Science at the University of Tennessee, Knoxville. Dr.
Taufer received her Ph.D. in computer science from
the Swiss Federal Institute of Technology (ETH) in
2002. Her interdisciplinary research is at the inter-
section of computational sciences, high permanence
computing and data analytics.

REFERENCES

1. K. Akiyama, A. Alberdi, W. Alef, K. Asada, and R. Azuly,

“First M87 EHT Results. VI. The Shadow and Mass

of the Central Black Hole,” The Astrophysical Journal

Letters, vol. 875, no. 1, p. L6, 2019.

2. D. A. Brown, K. Vahi, M. Taufer, V. Welch, and E. Deel-

man, “Reproducing GW150914: The First Observa-

tion of Gravitational Waves From a Binary Black Hole

Merger,” Computing in Science & Engineering, vol. 23,

no. 2, pp. 73–82, 2021.

3. K. Akiyama, A. Alberdi, W. Alef, K. Asada, and R. Azuly,

10

IT Professional

