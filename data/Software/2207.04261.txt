2
2
0
2

l
u
J

9

]
L
M

.
t
a
t
s
[

1
v
1
6
2
4
0
.
7
0
2
2
:
v
i
X
r
a

Fuzzy Clustering by Hyperbolic Smoothing

Javier Trejos‡

David Masís∗

Esteban Segura†
Adilson Xavier§

July 12, 2022

Abstract

We propose a novel method for building fuzzy clusters of large data sets,
using a smoothing numerical approach. The usual sum-of-squares criterion is
relaxed so the search for good fuzzy partitions is made on a continuous space,
rather than a combinatorial space as in classical methods [8]. The smoothing
allows a conversion from a strongly non-diﬀerentiable problem into diﬀerentiable
subproblems of optimization without constraints of low dimension, by using a
diﬀerentiable function of inﬁnite class. For the implementation of the algorithm
we used the statistical software 𝑅 and the results obtained were compared to the
traditional fuzzy 𝐶–means method, proposed by Bezdek [1].

Keywords: clustering, fuzzy sets, numerical smoothing.

1 Introduction

Methods for making groups from data sets are usually based on the idea of disjoint
sets, such as the classical crisp clustering. The most well known are hierarchical
and 𝑘-means [8], whose resulting clusters are sets will no intersection. However,
this restriction may not be natural for some applications, where the condition for
some objects may be to belong to two or more clusters, rather than only one. Several
methods for constructing overlapping clusters have been proposed in the literature
[4, 5, 8]. Since Zadeh introduced the concept of fuzzy sets [17], the principle of
belonging to several clusters has been used in the sense of a degree of membership
to such clusters. In this direction, Bezdek [1] introduced a fuzzy clustering method
that became very popular since it solved the problem of representation of clusters
with centroids and the assignment of objects to clusters, by the minimization of
a well-stated numerical criterion. Several methods for fuzzy clustering have been
proposed in the literature; a survey of these methods can be found in [16].

∗Costa Rica Institute of Technology, Cartago, Costa Rica. E-Mail: dmasis@itcr.ac.cr
†CIMPA & School of Mathematics, University of Costa Rica, San José, Costa Rica. E-Mail: estebasegu-

raugalde@ucr.ac.cr

‡CIMPA & School of Mathematics, University of Costa Rica, San José, Costa Rica.

E-Mail:

javier.trejos@ucr.ac.cr

§Universidade Federal de Rio de Janeiro, Brazil. E-Mail: adilson.xavier@gmail.com

1

 
 
 
 
 
 
In this paper we propose a new fuzzy clustering method based on the numerical
principle of hyperbolic smoothing [15]. Fuzzy 𝐶-Means method is presented
in Section 2 and our proposed Hyperbolic Smoothing Fuzzy Clustering method in
Section 3. Comparative results between these two methods are presented in Section
4. Finally, Section 5 is devoted to the concluding remarks.

2 Fuzzy Clustering

The most well known method for fuzzy clustering is the original Bezdek’s 𝐶-means
method [1] and it is based on the same principles of 𝑘-means or dynamical clusters
[2], that is, iterations on two main steps: i) class representations by the optimization
of a numerical criterion, and ii) assignment to the closest class representative in
order to construct clusters; these iterations are made until a convergence is reached
to a local minimum of the overall quality criterion.

Let us introduce the notation that will be used and the numerical criterion for
optimization. Let X be a 𝑛 × 𝑝 data matrix containing 𝑝 numerical observations
over 𝑛 objects. We look for a 𝐾 × 𝑝 matrix G that represents centroids of 𝐾 clusters
of the 𝑛 objects and an 𝑛 × 𝐾 membership matrix with elements 𝜇𝑖𝑘 ∈ [0, 1], such
that the following criterion is minimized:

𝑊 (X, U, G) =

𝑛
∑︁

𝐾
∑︁

(𝜇𝑖𝑘 )𝑚 (cid:107)x𝑖 − g𝑘 (cid:107)2

subject to (cid:205)𝐾

𝜇𝑖𝑘 = 1, for all 𝑖 ∈ {1, 2, . . . , 𝑛}

𝑖=1

𝑘=1

𝑘=1
0 < (cid:205)𝑛
𝑖=1

𝜇𝑖𝑘 < 𝑛, for all 𝑘 ∈ {1, 2, . . . , 𝐾 },

(1)

where x𝑖 is the 𝑖-th row of X and g𝑘 is the 𝑘-th row of G, representing in R 𝑝 the
centroid of the 𝑘-th cluster.

The parameter 𝑚 ≠ 1 in (1) controls the fuzzyness of the clusters. According
to the literature [16], it is usual to take 𝑚 = 2, since greater values of 𝑚 tend to give
very low values of 𝜇𝑖𝑘 , tending to the usual crisp partitions such as in 𝑘-means.
We also assume that the number of clusters, 𝐾, is ﬁxed.

Minimization of (1) represents a non linear optimization problem with con-
straints, which can be solved using Lagrange multipliers as presented in [1]. The
solution, for each row of the centroids matrix, given a matrix U, is:

g𝑘 =

𝑛
∑︁

𝑖=1

(𝜇𝑖𝑘 )𝑚

x𝑖

(cid:44) 𝑛
∑︁

𝑖=1

(𝜇𝑖𝑘 )𝑚 .

The solution for the membership matrix, given a matrix centroids G, is [1]:

𝜇𝑖𝑘 =

(cid:32)

𝐾
∑︁

𝑗=1








||x𝑖 − g𝑘 ||2
||x𝑖 − g 𝑗 ||2

−1

.

(cid:33)1/(𝑚−1) 






(2)

(3)

The following pseudo-code shows the mains steps of Bezdek’s Fuzzy 𝐶-Means

method [1].

2

Bezdek’s Fuzzy c-Means (FCM) Algorithm

1. Initialize fuzzy membership matrix U = [𝜇𝑖𝑘 ]𝑛×𝐾
2. Compute centroids for fuzzy clusters according to (2)

3. Update membership matrix U according to (3)

4. If improvement in the criterion is less than a threshold, then stop; otherwise

go to Step 2.

Fuzzy 𝐶-Means method starts from an initial partition that is improved in each
iteration, according to (1), applying Steps 2 and 3 of the algorithm. It is clear that
this procedure may lead to local optima of (1) since iterative improvement in (2)
and (3) is made by a local search strategy.

3 Algorithm for Hyperbolic Smoothing Fuzzy
Clustering

For the clustering problem of the 𝑛 rows of data matrix X in 𝐾 clusters, we can
seek for the minimum distance between every x𝑖 and its class center g𝑘 :

𝑧2
𝑖 = min
g𝑘 ∈G

(cid:107)x𝑖 − g𝑘 (cid:107)2
2

where (cid:107) · (cid:107)2 is the Euclidean norm. The minimization can be stated as a sum-of-
squares:

min

𝑛
∑︁

𝑖=1

min
g𝑘 ∈G

(cid:107)x𝑖 − g𝑘 (cid:107)2

2 = min

𝑛
∑︁

𝑖=1

𝑧2
𝑖

leading to the following constrained problem:

min

𝑛
∑︁

𝑖=1

𝑧2
𝑖 subject to 𝑧𝑖 = min
g𝑘 ∈G

(cid:107)x𝑖 − g𝑘 (cid:107)2, with 𝑖 = 1, . . . , 𝑛.

3

This is equivalent to the following minimization problem:

min

𝑛
∑︁

𝑖=1

𝑧2
𝑖 subject to 𝑧𝑖 − (cid:107)x𝑖 − g𝑘 (cid:107)2 ≤ 0, with 𝑖 = 1, . . . , 𝑛 and 𝑘 = 1, . . . , 𝐾.

Considering the function: 𝜑(𝑦) = max(0, 𝑦), we obtain the problem:

min

𝑛
∑︁

𝑖=1

𝑧2
𝑖 subject to

𝐾
∑︁

𝑘=1

𝜑(𝑧𝑖 − (cid:107)x𝑖 − g𝑘 (cid:107)2) = 0 for 𝑖 = 1, . . . , 𝑛.

That problem can be re-stated as the following one:

min

𝑛
∑︁

𝑖=1

𝑧2
𝑖 subject to

𝐾
∑︁

𝑘=1

𝜑(𝑧𝑖 − (cid:107)x𝑖 − g𝑘 (cid:107)2) > 0, for 𝑖 = 1, . . . , 𝑛.

Given a perturbation 𝜖 > 0 it leads to the problem:

min

𝑛
∑︁

𝑖=1

𝑧2
𝑖 subject to

𝐾
∑︁

𝑘=1

𝜑(𝑧𝑖 − (cid:107)x𝑖 − g𝑘 (cid:107)2) ≥ 𝜖 for 𝑖 = 1, . . . , 𝑛.

It should be noted that function 𝜑 is not diﬀerentiable. Therefore, we will make
a smoothing procedure in order to formulate a diﬀerentiable function and pro-
ceed with a minimization by a numerical method. For that, consider the function:
, for all 𝑦 ∈ R, 𝜏 > 0, and the function: 𝜃 (x𝑖, g𝑘 , 𝛾) =
𝜓(𝑦, 𝜏) =
√︃(cid:205) 𝑝
(𝑥𝑖 𝑗 − 𝑔𝑘 𝑗 )2 + 𝛾2, for 𝛾 > 0. Hence, the minimization problem is trans-

𝑦2+𝜏2
2

𝑦+

√

𝑗=1
formed into:

min

𝑛
∑︁

𝑖=1

𝑧2
𝑖 subject to

𝐾
∑︁

𝑘=1

𝜓(𝑧𝑖 − 𝜃 (x𝑖, g𝑘 , 𝛾), 𝜏) ≥ 𝜖, for 𝑖 = 1, . . . , 𝑛.

Finally, according to the Karush–Kuhn–Tucker conditions [10, 11], all the

constraints are active and the ﬁnal formulation of the problem is:

min

𝑛
∑︁

𝑖=1

𝑧2
𝑖

subject to

ℎ𝑖 (𝑧𝑖, G) =

𝜖, 𝜏, 𝛾 > 0.

𝐾
∑︁

𝑘=1

𝜓(𝑧𝑖 − (cid:107)x𝑖 − g𝑘 (cid:107)2, 𝜏) − 𝜖 = 0, for 𝑖 = 1, . . . , 𝑛,

(4)
Considering (4), in [15] it was stated the Hyperbolic Smoothing Clustering Method
presented in the following algorithm.

Hyperbolic Smoothing Clustering Method (HSCM) Algorithm

1. Initialize cluster membership matrix U = [𝜇𝑖𝑘 ]𝑛×𝐾
2. Choose initial values: G0, 𝛾1, 𝜏1, 𝜖 1
3. Choose values: 0 < 𝜌1 < 1, 0 < 𝜌2 < 1, 0 < 𝜌3 < 1
4. Let 𝑙 = 1

4

5. Repeat steps 6 and 7 until a stop condition is reached:

6. Solve problem (P): min 𝑓 (G) =

𝑛
∑︁

𝑖 with 𝛾 = 𝛾𝑙, 𝜏 = 𝜏𝑙 y 𝜖 = 𝜖 𝑙, G𝑙−1
𝑧2

𝑖=1

𝑘=1

being the initial value and G𝑙 the obtained solution
7. Let 𝛾𝑙+1 = 𝜌1𝛾𝑙, 𝜏𝑙+1 = 𝜌2𝜏𝑙, 𝜖 𝑙+1 = 𝜌3𝜖 𝑙 y 𝑙 = 𝑙 + 1.
The most relevant task in the hyperbolic smoothing clustering method is ﬁnding
the zeroes of the function ℎ𝑖 (𝑧𝑖, G) = (cid:205)𝐾
𝜓(𝑧𝑖 − (cid:107)x𝑖 − g𝑘 (cid:107)2, 𝜏) − 𝜖 = 0 for
𝑖 = 1, . . . , 𝑛. In this paper, we used the Newton-Raphson method for ﬁnding these
zeroes [3], particularly the BFGS procedure [12]. Convergence of the Newton-
Raphson method was successful, mainly, thank to a good choice of initial solutions.
In our implementation, these initial approximations were generated by calculating
the minimum distance between the 𝑖-th object and the 𝑘-th centroid for a given
partition. Once the zeroes 𝑧𝑖 of the functions ℎ𝑖 are obtained, it is implemented
the hyperbolic smoothing. The ﬁnal solution for this method consists on solving a
ﬁnite number of optimization subproblems corresponding to problem (P) in Step
6 of the HSCM algorithm. Each one of these subproblems was solved with the R
routine optim [13], a useful tool for solving optimization problems in non linear
programming. As far as we know there is no closed solution for solving this step.
For the future, we can consider writing a program by our means, but for this paper
we are using this R routine.
Since we have that: (cid:205)𝐾

𝜓(𝑧𝑖 − 𝜃 (x𝑖, g𝑘 , 𝛾), 𝜏) = 𝜖, then each entry 𝜇𝑖𝑘 of
. It is worth to note that

the membership matrix is given by: 𝜇𝑖𝑘 =
fuzzyness is controlled by parameter 𝜖.

𝜓 (𝑧𝑖−𝑑𝑘 , 𝜏)
𝜖

𝑘=1

The following algorithm contains the main steps of the Hyperbolic Smoothing

Fuzzy Clustering (HSFC) method.

Hyperbolic Smoothing Fuzzy Clustering (HSFC) Algorithm

1. Set 𝜖 > 0
2. Choose initial values for: G0 (centroids matrix), 𝛾1, 𝜏1 y 𝑁 (maximum

number of iterations)

3. Choose values: 0 < 𝜌1 < 1, 0 < 𝜌2 < 1
4. Set 𝑙 = 1
5. While 𝑙 ≤ 𝑁:
6. Solve the problem (P): Minimize 𝑓 (G) = (cid:205)𝑛
𝑖=1

𝑖 con 𝛾 = 𝛾 (𝑙) y 𝜏 = 𝜏 (𝑙) ,
𝑧2

with an initial point G(𝑙−1) and G(𝑙) being the obtained solution

7. Set 𝛾 (𝑙+1) = 𝜌1𝛾 (𝑙) ,𝜏 (𝑙+1) = 𝜌2𝜏 (𝑙) , y 𝑙 = 𝑙 + 1
8. Set 𝜇𝑖𝑘 = 𝜓(𝑧𝑖 − 𝜃 (x𝑖, g𝑘 , 𝛾), 𝜏)/𝜖 para 𝑖 = 1, . . . , 𝑛 y 𝑘 = 1, . . . , 𝐾.

4 Comparative Results

Performance of the HSFC method was studied on a data table well known from the
literature, the Fisher’s iris [7] and 16 simulated data tables built from a semi-Monte
Carlo procedure [14].

5

Table 1: Minimum sum-of-squares (SS) reported for the Fisher’s iris data table with
HSFC and FCM, 𝐾 being the number of clusters, RI and ARI comparing both methods.
In bold best method.

Table

Fisher’s iris

𝐾

2
3
4

SS for HSFC

SS for FCM

ARI

152.348
78.85567
57.26934

152.3615
78.86733
57.26934

1
0.994
0.980

𝑖=1

𝑘=1

(cid:205)𝑛

For comparing FCM and HSFC, we used the implementation of FCM in R pack-
age fclust [6]. This comparison was made upon the within class sum-of-squares:
𝑊 (𝑃) = (cid:205)𝐾
𝜇𝑖𝑘 (cid:107)x𝑖 − g𝑘 (cid:107)2. Both methods were applied 50 times and the
best value of 𝑊 is reported. For simplicity here, for HSFC we used the following
parameters: 𝜌1 = 𝜌2 = 𝜌3 = 0.25, 𝜖 = 0.01 and 𝛾 = 𝜏 = 0.001 as initial values.
In Table 1 the results for Fisher’s iris are shown, in which case HSFC performs
slightly better. It contains the Adjusted Rand Index (ARI) [9] between HSFC and
the best FCM result among 100 runs; RI and ARI compare fuzzy membership
matrices crisped into hard partitions.

Simulated data tables were generated in a controlled experiment as in [14],
with random numbers following a Gaussian distribution. Factors of the experiment
were:

• The number of objects (with 2 levels, 𝑛 = 105 and 𝑛 = 525).
• The number of clusters (with levels 𝐾 = 3 and 𝐾 = 7).
• Cardinality (card) of clusters, with levels i) all with the same number of
objects (coded as card(=)), and ii) one large cluster with 50% of objects and
the rest with the same number (coded as card(≠)).

• Standard deviation of clusters, with levels i) all Gaussian random variables
with standard deviation (SD) equal to one (coded as SD(=)), and ii) one
cluster with SD=3 and the rest with SD=1 (coded as SD(≠)).

Table 2 contains codes for simulated data tables according to the codes we used.

Table 3 contains the minimum values of the sum-of-squares obtained for our
HSFC and Bezdek’s FCM methods; the best solution of 100 random applications
for FCM in presented and one run of HSFC. It also contains the ARI values for
comparing HSFC solution with that best solution of FCM. It can be seen that,
generally, HSFC method tends to obtain better results than FCM, with only few
exceptions. In 23 cases HSFC obtains better results, FCM is better in 5 cases, and
results are in same in 17 cases. However, ARI shows that partitions tend to be very
similar with both methods.

5 Concluding Remarks

In hyperbolic smoothing, parameters 𝜏, 𝛾 and 𝜖 tend to zero, so the constraints in
the subproblems make that problem (P) tends to solve (1). Parameter 𝜖 controls
the fuzzyness degree in clustering; the higher it is, the solution becomes more
and more fuzzy; the less it is, the clustering is more and more crisp. In order to

6

Table 2: Codes and characteristics of simulated data tables; 𝑛: number of objects, 𝐾:
number of clusters, card: cardinality, DS: standard deviation.

Table Characteristcs

Table Characteristcs

T1

T2

T3

T4

T5

T6

T7

T8

𝑛 = 525, 𝐾 = 3, card(=), SD(=)

𝑛 = 525, 𝐾 = 7, card(=), SD(=)

𝑛 = 105, 𝐾 = 3, card(=), SD(=)

𝑛 = 105, 𝐾 = 7, card(=), SD(=)

𝑛 = 525, 𝐾 = 3, card(=), SD(≠)

𝑛 = 525, 𝐾 = 7, card(=), SD(≠)

𝑛 = 105, 𝐾 = 3, card(=), SD(≠)

𝑛 = 105, 𝐾 = 7, card(=), SD(≠)

T9

T10

T11

T12

T13

T14

T15

T16

𝑛 = 525, 𝐾 = 3, card(≠),
DS(=)
𝑛 = 525, 𝐾 = 7, card(≠),
DS(=)
𝑛 = 105, 𝐾 = 3, card(≠),
DS(=)
𝑛 = 105, 𝐾 = 7, card(≠),
DS(=)
𝑛 = 525, 𝐾 = 3, card(≠),
DS(≠)
𝑛 = 525, 𝐾 = 7, card(≠),
DS(≠)
𝑛 = 105, 𝐾 = 3, card(≠),
DS(≠)
𝑛 = 105, 𝐾 = 7, card(≠),
DS(≠)

compare results and eﬃciency of the HSFC method, zeroes of functions ℎ𝑖 can
be obtained with any method for solving equations in one variable or a predeﬁned
routine. According to the results we obtained so far and the implementation of
the hyperbolic smoothing for fuzzy clustering, we can conclude that, generally,
the HSFC method has a slightly better performance than original Bezdek’s FCM
on small real and simulated data tables. Further research is required for testing
performance of HSFC method on very large data sets, with measures of eﬃciency,
quality of solutions and running time. We are also considering to study further
comparisons between HSFC and FCM with diﬀerent indices, and writing the
program for solving Step 6 in HSFC algorithm, that is the minimization of 𝑓 (𝐺),
by our means, instead of using the optim routine in R.

Acknowledgements

D. Masís acknowledges the School of Mathematics of the Costa Rica Institute
of Technology for their support; this work is part of his M.Sc. dissertation at the
University of Costa Rica. E. Segura and J. Trejos acknowledge the Research Center
for Pure and Applied Mathematics (CIMPA) of the University of Costa Rica for
their support. A.E. Xavier acknowledges the Federal University of Rio de Janeiro
and the Federal University of Juiz Fora for their support.

7

Table 3: Minimum sum-of-squares (SS) reported for HSFC and FCM methods on the
simulated data tables. Best method in bold.

Table

𝐾

SS for
HSFC

SS for
FCM

T9

T10

T11

T12

T13

T14

T15

T16

2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4

12524.31
9269.361
6298.47
5466.893
2977.58
2745.721
2969.247
1912.323
1401.394
1816.056
525.7118
477.0593
12804.03
8816.805
6293.774
16228.07
7255.113
6427.313
2616.286
1978.017
1526.895
2226.923
1232.074
982.7074

12524.31
9269.611
6298.368
5466.912
2977.58
2746.671
2969.32
1912.323
1401.394
1816.056
525.7118
477.2696
12805.05
8817.702
6293.951
16228.98
7255.423
6427.313
2616.943
1978.233
1526.953
2226.212
1232.124
982.9721

ARI

0.900
1
1
0.890
1
1
0.860
1
1
1
1
1
0.920
1
1
0.920
1
1
1
1
1
0.962
1
1

Table

𝐾

SS for
HSFC

SS for
FCM

T1

T2

T3

T4

T5

T6

T7

T8

2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4

7073.402
3146.119
2983.651
16987.19
11653.22
7776.855
3923.051
2917.13
2287.523
1720.365
569.3112
535.5491
15595.67
11724.93
8409.738
11877.96
8299.779
7212.611
4336.261
3041.076
2395.683
1767.43
1380.766
1215.302

7073.814
3146.119
2983.651
16987.71
11653.22
7777.396
3923.062
2917.13
2256.298
1720.374
569.3112
535.3541
15595.67
11725.28
8409.738
11877.96
8300.718
7213.725
4336.507
3041.076
2421.333
1767.43
1381.019
1211.235

ARI

0.780
1
1
0.764
1
1
0.763
0.754
0.993
0.992
1
1
0.910
1
0.984
0.970
1
1
0.955
1
1
1
1
1

8

References

[1] Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algo-

rithms. Plenum Press, New York (1981)

[2] H.-H. Bock: Origins and extensions of the k-means algorithm in cluster
analysis. Electronic Journ@l for History of Probability and Statistics 4 (2008)

[3] Burden, R., Faires, D.: Numerical analysis, 9th ed. Brooks/Cole, Paciﬁc

Grove (2011)

[4] Diday, E.: Orders and overlapping clusters by pyramids. In J.De Leeuw et al.

(eds.) Multidimensional Data Analysis, DSWO Press, Leiden (1986)

[5] Dunn, J.C.: A fuzzy relative of the ISODATA process and its use in detecting

compact, well separated clusters. J. Cybernetics 3, 32–57 (1974)

[6] Ferraro, M.B., Giordani, P., Seraﬁni, A.: fclust: An R Package for Fuzzy
Clustering. The R Journal 11(1): 198-210 (2019) doi: 10.32614/RJ-2019-
017

[7] Fisher, R.A.: The use of multiple measurements in taxonomic problems.

Annals of Eugenics 7: 179—188 (1936)

[8] Hartigan, J.A.: Clustering Algorithms. Wiley, New York, NY (1975)

[9] Hubert, L., Arabie, P.: Comparing partitions. Journal of Classiﬁcation 2(1),

193–218 (1985)

[10] Karush, W.: Minima of Functions of Several Variables with Inequalities
as Side Constraints. Master’s Thesis, Dept. of Mathematics, University of
Chicago, Chicago, Illinois (1939)

[11] Kuhn, H., Tucker, A.: Nonlinear programming, Proc. 2nd Berkeley Sym-
posium on Mathematical Statistics and Probability, University of California
Press, Berkeley, pp. 481-492 (1951)

[12] Li, D., Fukushima, M.: On the global convergence of the BFGS method
for nonconvex unconstrained optimization problems. SIAM J. Optim. 11,
1054–1064 (2001)

[13] R Core Team: R: A language and environment for statistical computing. R

Foundation for Statistical Computing, Vienna, Austria, (2021)

[14] Trejos, J., Villalobos, M.A.: Partitioning by particle swarm optimization.
In: Brito, P. Bertrand, P., Cucumel G., de Carvalho, F. (eds.) Selected
Contributions in Data Analysis and Classiﬁcation, pp. 235-244. Springer,
Berlin (2007)

[15] Xavier, A.: The hyperbolic smoothing clustering method, Pattern Recognit.

43, 731-737 (2010)

[16] Yang, M.S.: A survey of fuzzy clustering. Math. Comput. Modelling 18,

1–16 (1993)

[17] Zadeh, L.: Fuzzy sets, Information and Control 8, 38–33 (1965)

9

