2
2
0
2

l
u
J

9

]
L
M

.
t
a
t
s
[

1
v
1
6
2
4
0
.
7
0
2
2
:
v
i
X
r
a

Fuzzy Clustering by Hyperbolic Smoothing

Javier Trejosâ€¡

David MasÃ­sâˆ—

Esteban Seguraâ€ 
Adilson XavierÂ§

July 12, 2022

Abstract

We propose a novel method for building fuzzy clusters of large data sets,
using a smoothing numerical approach. The usual sum-of-squares criterion is
relaxed so the search for good fuzzy partitions is made on a continuous space,
rather than a combinatorial space as in classical methods [8]. The smoothing
allows a conversion from a strongly non-diï¬€erentiable problem into diï¬€erentiable
subproblems of optimization without constraints of low dimension, by using a
diï¬€erentiable function of inï¬nite class. For the implementation of the algorithm
we used the statistical software ğ‘… and the results obtained were compared to the
traditional fuzzy ğ¶â€“means method, proposed by Bezdek [1].

Keywords: clustering, fuzzy sets, numerical smoothing.

1 Introduction

Methods for making groups from data sets are usually based on the idea of disjoint
sets, such as the classical crisp clustering. The most well known are hierarchical
and ğ‘˜-means [8], whose resulting clusters are sets will no intersection. However,
this restriction may not be natural for some applications, where the condition for
some objects may be to belong to two or more clusters, rather than only one. Several
methods for constructing overlapping clusters have been proposed in the literature
[4, 5, 8]. Since Zadeh introduced the concept of fuzzy sets [17], the principle of
belonging to several clusters has been used in the sense of a degree of membership
to such clusters. In this direction, Bezdek [1] introduced a fuzzy clustering method
that became very popular since it solved the problem of representation of clusters
with centroids and the assignment of objects to clusters, by the minimization of
a well-stated numerical criterion. Several methods for fuzzy clustering have been
proposed in the literature; a survey of these methods can be found in [16].

âˆ—Costa Rica Institute of Technology, Cartago, Costa Rica. E-Mail: dmasis@itcr.ac.cr
â€ CIMPA & School of Mathematics, University of Costa Rica, San JosÃ©, Costa Rica. E-Mail: estebasegu-

raugalde@ucr.ac.cr

â€¡CIMPA & School of Mathematics, University of Costa Rica, San JosÃ©, Costa Rica.

E-Mail:

javier.trejos@ucr.ac.cr

Â§Universidade Federal de Rio de Janeiro, Brazil. E-Mail: adilson.xavier@gmail.com

1

 
 
 
 
 
 
In this paper we propose a new fuzzy clustering method based on the numerical
principle of hyperbolic smoothing [15]. Fuzzy ğ¶-Means method is presented
in Section 2 and our proposed Hyperbolic Smoothing Fuzzy Clustering method in
Section 3. Comparative results between these two methods are presented in Section
4. Finally, Section 5 is devoted to the concluding remarks.

2 Fuzzy Clustering

The most well known method for fuzzy clustering is the original Bezdekâ€™s ğ¶-means
method [1] and it is based on the same principles of ğ‘˜-means or dynamical clusters
[2], that is, iterations on two main steps: i) class representations by the optimization
of a numerical criterion, and ii) assignment to the closest class representative in
order to construct clusters; these iterations are made until a convergence is reached
to a local minimum of the overall quality criterion.

Let us introduce the notation that will be used and the numerical criterion for
optimization. Let X be a ğ‘› Ã— ğ‘ data matrix containing ğ‘ numerical observations
over ğ‘› objects. We look for a ğ¾ Ã— ğ‘ matrix G that represents centroids of ğ¾ clusters
of the ğ‘› objects and an ğ‘› Ã— ğ¾ membership matrix with elements ğœ‡ğ‘–ğ‘˜ âˆˆ [0, 1], such
that the following criterion is minimized:

ğ‘Š (X, U, G) =

ğ‘›
âˆ‘ï¸

ğ¾
âˆ‘ï¸

(ğœ‡ğ‘–ğ‘˜ )ğ‘š (cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2

subject to (cid:205)ğ¾

ğœ‡ğ‘–ğ‘˜ = 1, for all ğ‘– âˆˆ {1, 2, . . . , ğ‘›}

ğ‘–=1

ğ‘˜=1

ğ‘˜=1
0 < (cid:205)ğ‘›
ğ‘–=1

ğœ‡ğ‘–ğ‘˜ < ğ‘›, for all ğ‘˜ âˆˆ {1, 2, . . . , ğ¾ },

(1)

where xğ‘– is the ğ‘–-th row of X and gğ‘˜ is the ğ‘˜-th row of G, representing in R ğ‘ the
centroid of the ğ‘˜-th cluster.

The parameter ğ‘š â‰  1 in (1) controls the fuzzyness of the clusters. According
to the literature [16], it is usual to take ğ‘š = 2, since greater values of ğ‘š tend to give
very low values of ğœ‡ğ‘–ğ‘˜ , tending to the usual crisp partitions such as in ğ‘˜-means.
We also assume that the number of clusters, ğ¾, is ï¬xed.

Minimization of (1) represents a non linear optimization problem with con-
straints, which can be solved using Lagrange multipliers as presented in [1]. The
solution, for each row of the centroids matrix, given a matrix U, is:

gğ‘˜ =

ğ‘›
âˆ‘ï¸

ğ‘–=1

(ğœ‡ğ‘–ğ‘˜ )ğ‘š

xğ‘–

(cid:44) ğ‘›
âˆ‘ï¸

ğ‘–=1

(ğœ‡ğ‘–ğ‘˜ )ğ‘š .

The solution for the membership matrix, given a matrix centroids G, is [1]:

ğœ‡ğ‘–ğ‘˜ =

(cid:32)

ğ¾
âˆ‘ï¸

ğ‘—=1

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

||xğ‘– âˆ’ gğ‘˜ ||2
||xğ‘– âˆ’ g ğ‘— ||2

âˆ’1

.

(cid:33)1/(ğ‘šâˆ’1) ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

(2)

(3)

The following pseudo-code shows the mains steps of Bezdekâ€™s Fuzzy ğ¶-Means

method [1].

2

Bezdekâ€™s Fuzzy c-Means (FCM) Algorithm

1. Initialize fuzzy membership matrix U = [ğœ‡ğ‘–ğ‘˜ ]ğ‘›Ã—ğ¾
2. Compute centroids for fuzzy clusters according to (2)

3. Update membership matrix U according to (3)

4. If improvement in the criterion is less than a threshold, then stop; otherwise

go to Step 2.

Fuzzy ğ¶-Means method starts from an initial partition that is improved in each
iteration, according to (1), applying Steps 2 and 3 of the algorithm. It is clear that
this procedure may lead to local optima of (1) since iterative improvement in (2)
and (3) is made by a local search strategy.

3 Algorithm for Hyperbolic Smoothing Fuzzy
Clustering

For the clustering problem of the ğ‘› rows of data matrix X in ğ¾ clusters, we can
seek for the minimum distance between every xğ‘– and its class center gğ‘˜ :

ğ‘§2
ğ‘– = min
gğ‘˜ âˆˆG

(cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2
2

where (cid:107) Â· (cid:107)2 is the Euclidean norm. The minimization can be stated as a sum-of-
squares:

min

ğ‘›
âˆ‘ï¸

ğ‘–=1

min
gğ‘˜ âˆˆG

(cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2

2 = min

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘§2
ğ‘–

leading to the following constrained problem:

min

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘§2
ğ‘– subject to ğ‘§ğ‘– = min
gğ‘˜ âˆˆG

(cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2, with ğ‘– = 1, . . . , ğ‘›.

3

This is equivalent to the following minimization problem:

min

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘§2
ğ‘– subject to ğ‘§ğ‘– âˆ’ (cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2 â‰¤ 0, with ğ‘– = 1, . . . , ğ‘› and ğ‘˜ = 1, . . . , ğ¾.

Considering the function: ğœ‘(ğ‘¦) = max(0, ğ‘¦), we obtain the problem:

min

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘§2
ğ‘– subject to

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğœ‘(ğ‘§ğ‘– âˆ’ (cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2) = 0 for ğ‘– = 1, . . . , ğ‘›.

That problem can be re-stated as the following one:

min

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘§2
ğ‘– subject to

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğœ‘(ğ‘§ğ‘– âˆ’ (cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2) > 0, for ğ‘– = 1, . . . , ğ‘›.

Given a perturbation ğœ– > 0 it leads to the problem:

min

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘§2
ğ‘– subject to

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğœ‘(ğ‘§ğ‘– âˆ’ (cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2) â‰¥ ğœ– for ğ‘– = 1, . . . , ğ‘›.

It should be noted that function ğœ‘ is not diï¬€erentiable. Therefore, we will make
a smoothing procedure in order to formulate a diï¬€erentiable function and pro-
ceed with a minimization by a numerical method. For that, consider the function:
, for all ğ‘¦ âˆˆ R, ğœ > 0, and the function: ğœƒ (xğ‘–, gğ‘˜ , ğ›¾) =
ğœ“(ğ‘¦, ğœ) =
âˆšï¸ƒ(cid:205) ğ‘
(ğ‘¥ğ‘– ğ‘— âˆ’ ğ‘”ğ‘˜ ğ‘— )2 + ğ›¾2, for ğ›¾ > 0. Hence, the minimization problem is trans-

ğ‘¦2+ğœ2
2

ğ‘¦+

âˆš

ğ‘—=1
formed into:

min

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘§2
ğ‘– subject to

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğœ“(ğ‘§ğ‘– âˆ’ ğœƒ (xğ‘–, gğ‘˜ , ğ›¾), ğœ) â‰¥ ğœ–, for ğ‘– = 1, . . . , ğ‘›.

Finally, according to the Karushâ€“Kuhnâ€“Tucker conditions [10, 11], all the

constraints are active and the ï¬nal formulation of the problem is:

min

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘§2
ğ‘–

subject to

â„ğ‘– (ğ‘§ğ‘–, G) =

ğœ–, ğœ, ğ›¾ > 0.

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğœ“(ğ‘§ğ‘– âˆ’ (cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2, ğœ) âˆ’ ğœ– = 0, for ğ‘– = 1, . . . , ğ‘›,

(4)
Considering (4), in [15] it was stated the Hyperbolic Smoothing Clustering Method
presented in the following algorithm.

Hyperbolic Smoothing Clustering Method (HSCM) Algorithm

1. Initialize cluster membership matrix U = [ğœ‡ğ‘–ğ‘˜ ]ğ‘›Ã—ğ¾
2. Choose initial values: G0, ğ›¾1, ğœ1, ğœ– 1
3. Choose values: 0 < ğœŒ1 < 1, 0 < ğœŒ2 < 1, 0 < ğœŒ3 < 1
4. Let ğ‘™ = 1

4

5. Repeat steps 6 and 7 until a stop condition is reached:

6. Solve problem (P): min ğ‘“ (G) =

ğ‘›
âˆ‘ï¸

ğ‘– with ğ›¾ = ğ›¾ğ‘™, ğœ = ğœğ‘™ y ğœ– = ğœ– ğ‘™, Gğ‘™âˆ’1
ğ‘§2

ğ‘–=1

ğ‘˜=1

being the initial value and Gğ‘™ the obtained solution
7. Let ğ›¾ğ‘™+1 = ğœŒ1ğ›¾ğ‘™, ğœğ‘™+1 = ğœŒ2ğœğ‘™, ğœ– ğ‘™+1 = ğœŒ3ğœ– ğ‘™ y ğ‘™ = ğ‘™ + 1.
The most relevant task in the hyperbolic smoothing clustering method is ï¬nding
the zeroes of the function â„ğ‘– (ğ‘§ğ‘–, G) = (cid:205)ğ¾
ğœ“(ğ‘§ğ‘– âˆ’ (cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2, ğœ) âˆ’ ğœ– = 0 for
ğ‘– = 1, . . . , ğ‘›. In this paper, we used the Newton-Raphson method for ï¬nding these
zeroes [3], particularly the BFGS procedure [12]. Convergence of the Newton-
Raphson method was successful, mainly, thank to a good choice of initial solutions.
In our implementation, these initial approximations were generated by calculating
the minimum distance between the ğ‘–-th object and the ğ‘˜-th centroid for a given
partition. Once the zeroes ğ‘§ğ‘– of the functions â„ğ‘– are obtained, it is implemented
the hyperbolic smoothing. The ï¬nal solution for this method consists on solving a
ï¬nite number of optimization subproblems corresponding to problem (P) in Step
6 of the HSCM algorithm. Each one of these subproblems was solved with the R
routine optim [13], a useful tool for solving optimization problems in non linear
programming. As far as we know there is no closed solution for solving this step.
For the future, we can consider writing a program by our means, but for this paper
we are using this R routine.
Since we have that: (cid:205)ğ¾

ğœ“(ğ‘§ğ‘– âˆ’ ğœƒ (xğ‘–, gğ‘˜ , ğ›¾), ğœ) = ğœ–, then each entry ğœ‡ğ‘–ğ‘˜ of
. It is worth to note that

the membership matrix is given by: ğœ‡ğ‘–ğ‘˜ =
fuzzyness is controlled by parameter ğœ–.

ğœ“ (ğ‘§ğ‘–âˆ’ğ‘‘ğ‘˜ , ğœ)
ğœ–

ğ‘˜=1

The following algorithm contains the main steps of the Hyperbolic Smoothing

Fuzzy Clustering (HSFC) method.

Hyperbolic Smoothing Fuzzy Clustering (HSFC) Algorithm

1. Set ğœ– > 0
2. Choose initial values for: G0 (centroids matrix), ğ›¾1, ğœ1 y ğ‘ (maximum

number of iterations)

3. Choose values: 0 < ğœŒ1 < 1, 0 < ğœŒ2 < 1
4. Set ğ‘™ = 1
5. While ğ‘™ â‰¤ ğ‘:
6. Solve the problem (P): Minimize ğ‘“ (G) = (cid:205)ğ‘›
ğ‘–=1

ğ‘– con ğ›¾ = ğ›¾ (ğ‘™) y ğœ = ğœ (ğ‘™) ,
ğ‘§2

with an initial point G(ğ‘™âˆ’1) and G(ğ‘™) being the obtained solution

7. Set ğ›¾ (ğ‘™+1) = ğœŒ1ğ›¾ (ğ‘™) ,ğœ (ğ‘™+1) = ğœŒ2ğœ (ğ‘™) , y ğ‘™ = ğ‘™ + 1
8. Set ğœ‡ğ‘–ğ‘˜ = ğœ“(ğ‘§ğ‘– âˆ’ ğœƒ (xğ‘–, gğ‘˜ , ğ›¾), ğœ)/ğœ– para ğ‘– = 1, . . . , ğ‘› y ğ‘˜ = 1, . . . , ğ¾.

4 Comparative Results

Performance of the HSFC method was studied on a data table well known from the
literature, the Fisherâ€™s iris [7] and 16 simulated data tables built from a semi-Monte
Carlo procedure [14].

5

Table 1: Minimum sum-of-squares (SS) reported for the Fisherâ€™s iris data table with
HSFC and FCM, ğ¾ being the number of clusters, RI and ARI comparing both methods.
In bold best method.

Table

Fisherâ€™s iris

ğ¾

2
3
4

SS for HSFC

SS for FCM

ARI

152.348
78.85567
57.26934

152.3615
78.86733
57.26934

1
0.994
0.980

ğ‘–=1

ğ‘˜=1

(cid:205)ğ‘›

For comparing FCM and HSFC, we used the implementation of FCM in R pack-
age fclust [6]. This comparison was made upon the within class sum-of-squares:
ğ‘Š (ğ‘ƒ) = (cid:205)ğ¾
ğœ‡ğ‘–ğ‘˜ (cid:107)xğ‘– âˆ’ gğ‘˜ (cid:107)2. Both methods were applied 50 times and the
best value of ğ‘Š is reported. For simplicity here, for HSFC we used the following
parameters: ğœŒ1 = ğœŒ2 = ğœŒ3 = 0.25, ğœ– = 0.01 and ğ›¾ = ğœ = 0.001 as initial values.
In Table 1 the results for Fisherâ€™s iris are shown, in which case HSFC performs
slightly better. It contains the Adjusted Rand Index (ARI) [9] between HSFC and
the best FCM result among 100 runs; RI and ARI compare fuzzy membership
matrices crisped into hard partitions.

Simulated data tables were generated in a controlled experiment as in [14],
with random numbers following a Gaussian distribution. Factors of the experiment
were:

â€¢ The number of objects (with 2 levels, ğ‘› = 105 and ğ‘› = 525).
â€¢ The number of clusters (with levels ğ¾ = 3 and ğ¾ = 7).
â€¢ Cardinality (card) of clusters, with levels i) all with the same number of
objects (coded as card(=)), and ii) one large cluster with 50% of objects and
the rest with the same number (coded as card(â‰ )).

â€¢ Standard deviation of clusters, with levels i) all Gaussian random variables
with standard deviation (SD) equal to one (coded as SD(=)), and ii) one
cluster with SD=3 and the rest with SD=1 (coded as SD(â‰ )).

Table 2 contains codes for simulated data tables according to the codes we used.

Table 3 contains the minimum values of the sum-of-squares obtained for our
HSFC and Bezdekâ€™s FCM methods; the best solution of 100 random applications
for FCM in presented and one run of HSFC. It also contains the ARI values for
comparing HSFC solution with that best solution of FCM. It can be seen that,
generally, HSFC method tends to obtain better results than FCM, with only few
exceptions. In 23 cases HSFC obtains better results, FCM is better in 5 cases, and
results are in same in 17 cases. However, ARI shows that partitions tend to be very
similar with both methods.

5 Concluding Remarks

In hyperbolic smoothing, parameters ğœ, ğ›¾ and ğœ– tend to zero, so the constraints in
the subproblems make that problem (P) tends to solve (1). Parameter ğœ– controls
the fuzzyness degree in clustering; the higher it is, the solution becomes more
and more fuzzy; the less it is, the clustering is more and more crisp. In order to

6

Table 2: Codes and characteristics of simulated data tables; ğ‘›: number of objects, ğ¾:
number of clusters, card: cardinality, DS: standard deviation.

Table Characteristcs

Table Characteristcs

T1

T2

T3

T4

T5

T6

T7

T8

ğ‘› = 525, ğ¾ = 3, card(=), SD(=)

ğ‘› = 525, ğ¾ = 7, card(=), SD(=)

ğ‘› = 105, ğ¾ = 3, card(=), SD(=)

ğ‘› = 105, ğ¾ = 7, card(=), SD(=)

ğ‘› = 525, ğ¾ = 3, card(=), SD(â‰ )

ğ‘› = 525, ğ¾ = 7, card(=), SD(â‰ )

ğ‘› = 105, ğ¾ = 3, card(=), SD(â‰ )

ğ‘› = 105, ğ¾ = 7, card(=), SD(â‰ )

T9

T10

T11

T12

T13

T14

T15

T16

ğ‘› = 525, ğ¾ = 3, card(â‰ ),
DS(=)
ğ‘› = 525, ğ¾ = 7, card(â‰ ),
DS(=)
ğ‘› = 105, ğ¾ = 3, card(â‰ ),
DS(=)
ğ‘› = 105, ğ¾ = 7, card(â‰ ),
DS(=)
ğ‘› = 525, ğ¾ = 3, card(â‰ ),
DS(â‰ )
ğ‘› = 525, ğ¾ = 7, card(â‰ ),
DS(â‰ )
ğ‘› = 105, ğ¾ = 3, card(â‰ ),
DS(â‰ )
ğ‘› = 105, ğ¾ = 7, card(â‰ ),
DS(â‰ )

compare results and eï¬ƒciency of the HSFC method, zeroes of functions â„ğ‘– can
be obtained with any method for solving equations in one variable or a predeï¬ned
routine. According to the results we obtained so far and the implementation of
the hyperbolic smoothing for fuzzy clustering, we can conclude that, generally,
the HSFC method has a slightly better performance than original Bezdekâ€™s FCM
on small real and simulated data tables. Further research is required for testing
performance of HSFC method on very large data sets, with measures of eï¬ƒciency,
quality of solutions and running time. We are also considering to study further
comparisons between HSFC and FCM with diï¬€erent indices, and writing the
program for solving Step 6 in HSFC algorithm, that is the minimization of ğ‘“ (ğº),
by our means, instead of using the optim routine in R.

Acknowledgements

D. MasÃ­s acknowledges the School of Mathematics of the Costa Rica Institute
of Technology for their support; this work is part of his M.Sc. dissertation at the
University of Costa Rica. E. Segura and J. Trejos acknowledge the Research Center
for Pure and Applied Mathematics (CIMPA) of the University of Costa Rica for
their support. A.E. Xavier acknowledges the Federal University of Rio de Janeiro
and the Federal University of Juiz Fora for their support.

7

Table 3: Minimum sum-of-squares (SS) reported for HSFC and FCM methods on the
simulated data tables. Best method in bold.

Table

ğ¾

SS for
HSFC

SS for
FCM

T9

T10

T11

T12

T13

T14

T15

T16

2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4

12524.31
9269.361
6298.47
5466.893
2977.58
2745.721
2969.247
1912.323
1401.394
1816.056
525.7118
477.0593
12804.03
8816.805
6293.774
16228.07
7255.113
6427.313
2616.286
1978.017
1526.895
2226.923
1232.074
982.7074

12524.31
9269.611
6298.368
5466.912
2977.58
2746.671
2969.32
1912.323
1401.394
1816.056
525.7118
477.2696
12805.05
8817.702
6293.951
16228.98
7255.423
6427.313
2616.943
1978.233
1526.953
2226.212
1232.124
982.9721

ARI

0.900
1
1
0.890
1
1
0.860
1
1
1
1
1
0.920
1
1
0.920
1
1
1
1
1
0.962
1
1

Table

ğ¾

SS for
HSFC

SS for
FCM

T1

T2

T3

T4

T5

T6

T7

T8

2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4
2
3
4

7073.402
3146.119
2983.651
16987.19
11653.22
7776.855
3923.051
2917.13
2287.523
1720.365
569.3112
535.5491
15595.67
11724.93
8409.738
11877.96
8299.779
7212.611
4336.261
3041.076
2395.683
1767.43
1380.766
1215.302

7073.814
3146.119
2983.651
16987.71
11653.22
7777.396
3923.062
2917.13
2256.298
1720.374
569.3112
535.3541
15595.67
11725.28
8409.738
11877.96
8300.718
7213.725
4336.507
3041.076
2421.333
1767.43
1381.019
1211.235

ARI

0.780
1
1
0.764
1
1
0.763
0.754
0.993
0.992
1
1
0.910
1
0.984
0.970
1
1
0.955
1
1
1
1
1

8

References

[1] Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algo-

rithms. Plenum Press, New York (1981)

[2] H.-H. Bock: Origins and extensions of the k-means algorithm in cluster
analysis. Electronic Journ@l for History of Probability and Statistics 4 (2008)

[3] Burden, R., Faires, D.: Numerical analysis, 9th ed. Brooks/Cole, Paciï¬c

Grove (2011)

[4] Diday, E.: Orders and overlapping clusters by pyramids. In J.De Leeuw et al.

(eds.) Multidimensional Data Analysis, DSWO Press, Leiden (1986)

[5] Dunn, J.C.: A fuzzy relative of the ISODATA process and its use in detecting

compact, well separated clusters. J. Cybernetics 3, 32â€“57 (1974)

[6] Ferraro, M.B., Giordani, P., Seraï¬ni, A.: fclust: An R Package for Fuzzy
Clustering. The R Journal 11(1): 198-210 (2019) doi: 10.32614/RJ-2019-
017

[7] Fisher, R.A.: The use of multiple measurements in taxonomic problems.

Annals of Eugenics 7: 179â€”188 (1936)

[8] Hartigan, J.A.: Clustering Algorithms. Wiley, New York, NY (1975)

[9] Hubert, L., Arabie, P.: Comparing partitions. Journal of Classiï¬cation 2(1),

193â€“218 (1985)

[10] Karush, W.: Minima of Functions of Several Variables with Inequalities
as Side Constraints. Masterâ€™s Thesis, Dept. of Mathematics, University of
Chicago, Chicago, Illinois (1939)

[11] Kuhn, H., Tucker, A.: Nonlinear programming, Proc. 2nd Berkeley Sym-
posium on Mathematical Statistics and Probability, University of California
Press, Berkeley, pp. 481-492 (1951)

[12] Li, D., Fukushima, M.: On the global convergence of the BFGS method
for nonconvex unconstrained optimization problems. SIAM J. Optim. 11,
1054â€“1064 (2001)

[13] R Core Team: R: A language and environment for statistical computing. R

Foundation for Statistical Computing, Vienna, Austria, (2021)

[14] Trejos, J., Villalobos, M.A.: Partitioning by particle swarm optimization.
In: Brito, P. Bertrand, P., Cucumel G., de Carvalho, F. (eds.) Selected
Contributions in Data Analysis and Classiï¬cation, pp. 235-244. Springer,
Berlin (2007)

[15] Xavier, A.: The hyperbolic smoothing clustering method, Pattern Recognit.

43, 731-737 (2010)

[16] Yang, M.S.: A survey of fuzzy clustering. Math. Comput. Modelling 18,

1â€“16 (1993)

[17] Zadeh, L.: Fuzzy sets, Information and Control 8, 38â€“33 (1965)

9

