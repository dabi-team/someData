2
2
0
2

y
a
M
4
2

]

G
L
.
s
c
[

1
v
2
7
3
2
1
.
5
0
2
2
:
v
i
X
r
a

TorchNTK: A Library for Calculation of Neural Tangent
Kernels of PyTorch Models

Andrew Engel1, Zhichao Wang2, Anand D. Sarwate3, Sutanay Choudhury1,
and Tony Chiang1

1Paciﬁc Northwest National Laboratory, Richland, WA, 99354, USA
2University of California San Diego, La Jolla, CA 92093, USA
3Rutgers University - New Brunswick

Abstract

We introduce torchNTK, a python library to calculate the empirical neural tangent
kernel (NTK) of neural network models in the PyTorch framework. We provide an eﬃcient
method to calculate the NTK of multilayer perceptrons. We compare the explicit diﬀeren-
tiation implementation against autodiﬀerentiation implementations, which have the beneﬁt
of extending the utility of the library to any architecture supported by PyTorch, such as
convolutional networks. A feature of the library is that we expose the user to layerwise NTK
components, and show that in some regimes a layerwise calculation is more memory eﬃcient.
We conduct preliminary experiments to demonstrate use cases for the software and probe
the NTK. Our software can be installed from Github, here.

1

Introduction

Artiﬁcial neural networks (ANNs) give unprecedented results in machine learning tasks [17,
13, 28, 9, 29] though they continue to be little understood. Theoretical studies focus on the
equivalence between ANNs trained with (full-batch) gradient descent and kernel methods [14,
2, 3, 8]. Speciﬁcally, it was shown that in the inﬁnite-width limit neural networks trained by
gradient descent are equivalent to a kernel machine using the NTK [14]. It was then shown
that the necessary condition for the equivalence of a neural network model to kernel regression
was being in a so called ’lazy-training’ regime where the weights remain approximately static
[6]. This is equivalent to using a linearization of the neural network model about its initial
parameterization [6, 19]. Critics argue that being within this ’lazy-training’ regime gives
results in-consistent with phenomena that we observe, such as feature learning [31], and
that using the linearization of a model around its initialization can degrade performance

Corresponding Authors: Engel, A., andrew.engel@pnnl.gov and Chiang, T., tony.chiang@pnnl.gov

1

 
 
 
 
 
 
signiﬁcantly [6]. Following this, many teams evaluated the diﬀerence and similarities between
kernel regression using NTKs and neural networks across a variety of tasks [2, 18, 1], but
generally found that the performance diﬀerences depended on task and architecture [12].

Our team became interested in calculating these kernels for neural network models of
used in practice. A python library has been developed to dramatically increase the ease of
calculating both ﬁnite and inﬁnite width networks’ tangent kernels called neural-tangents
[23], but this library has a few limitations. First, it is built upon the jax framework [3]. Jax
is not currently natively supported on windows machines, and is still in pre-release. Jax has
not yet developed as large a user-base as other popular frameworks like TensorFlow [22] or
PyTorch [25] (see for example: [27]), and until a time where it does so, using neural-tangents
will require users to overcome the hurdle of mastering another deep learning framework. A
diﬀerent code-base that uses cupy [24] was built to eﬃciently calculate the inﬁnite convo-
lutional NTK [2], but not empirical NTKs. Therefore, a niche role in the community was
present to provide software to calculate the tangent-kernel within a framework that is widely
used in the ﬁeld, PyTorch.

We present torchNTK, a python library built on the PyTorch framework that calcu-
lates the empirical NTK using PyTorch autograd, and explicitly for multilayer perceptron
networks. We developed torchNTK to achieve our goals of studying the time-evolution of
the NTK for models that practical users of AI systems interface with. In the remainder of
this paper, we describe the explicit algorithms used to calculate the NTK, we benchmark
our software to compare implementations, detail an initial experiment to demonstrate our
software, and discuss plans for improving our software.

2 The Additive Components of the Neural Tangent

Kernel

Study of the NTK of ﬁnite-networks of large size trained on large number of datapoints has
been diﬃcult due to the sizes of the matrices involved. If a neural network parameterized by
θ and acts on a dataset X is denoted by fθ(X) then, the NTK matrix is the Gram matrix of
the Jacobian of the network [14] as follows:

J = (∇θfθ(X)), K N T K = J (cid:62)J.

(1)

For a dataset of size N and a network of size P , the Jacobian is a size P × N matrix.
Considering that deep learning is applied to problems that generally have many datapoints
and models have increased in size over time, the full Jacobian matrix is often too costly to
hold in memory on consumer workstations. As a concrete example, for a dataset of size 60,000
and a network with 100,000 parameters represented in fp32, the Jacobian is a 24 gigabyte
matrix, which is larger than the total available VRAM on most GPUs. Note, that these sizes
are typical for common toy problems like digit classiﬁcation but that modern architectures
might have 1e1 - 1e6 times the number of parameters [4, 13]. While the Jacobian is large
due to the number of parameters, the NTK is size N × N , and for modestly sized datasets
the NTK is more realistic to expect to hold in memory.

In the over-parameterized regime, we can lower peak memory requirements by transform-
ing the problem from holding a P × N matrix into holding many N × N matrices using a

2

layerwise approach. These additive components have already been pointed out directly in
works that derive algorithms for the calculation of the NTK [10, 2, 20], and can be most
explicitly represented as a sum over the layers L:

K NTK =

L
(cid:88)

l=0

(∇θlfθ)(X))(∇θlfθ(X))(cid:62).

(2)

We hypothesize that the additive components representing the layers contain more spe-
ciﬁc information about the operations they represent and may be getting ’lost’ in the full
NTK, though we leave demonstrating that to future work. For that reason, these compo-
nents are worthy of additional study, and in fact, there has been recent work on a spectral
analysis of these layerwise kernels [7].

We end this section by pointing out that in contemporary works on the weight matrices
[21], the Hessian [26, 30], and Fisher information matrices [15], have all examined a ’layerwise’
approach. These additive components of the NTK can be thought of as a natural extension
to compliment these modes of inquiry.

3 Algorithmic Details

TorchNTK is an accumulation of diﬀerent methods to calculate the NTK, which can be
broadly classiﬁed as either autograd or explicit diﬀerentiation. While autograd methods can
handle any model, the explicit diﬀerentiation technique was benchmarked to be much faster
on the MLP architectures that it is limited to. In the following sections we derive the formula
for MLP architectures used to recursively calculate the NTK:

3.1 Derivation of the NTK: MLP without bias

Consider the following to represent a neural network with parameters θ and X ∈ Rd0,n where
d0 is the dimensionality of the input vector, or equivalently is the width of the input layer
to our neural network. Let us ﬁrst consider neural networks composed of a series of matrix
multiplications, interrupted by non-linear activation functions. These networks are referred
to as multilayer perceptrons. Below, we also use the convention that Xl is the output of
layer l, that σ is some activation function, and that W is the weight matrix.

fθ(X) = w(cid:62) 1
√
dL

σ(W (cid:62)
L

1
(cid:112)dL−1

σ(...

1
√
d2

σ(W (cid:62)
2

1
√
d1

σ(W (cid:62)

1 X))))

Where we have adopted the practice of dividing by the square root of the width of each

layer which is necessary to place ourselves in the kernel parameterization.

Given that Sl is a matrix whose αth column is:

α = Dl
sl
α

W (cid:62)
l+1√
dl

Dl+1
α

W (cid:62)
l+2
(cid:112)dl+1

Dl+2

α ...

W (cid:62)
L
(cid:112)dL−1

DL
α

w
√
dL

,

where D is deﬁned by

Dk

α ≡ diag(σ(cid:48)(Wkxk−1

α ))

3

Then one can show (see Appendix G.2 of [10]):

K N T K = X (cid:62)

L XL +

L
(cid:88)

l=1

(S(cid:62)

l Sl) (cid:12) (X (cid:62)

l−1Xl−1)

The NTK is therefore a sum over components, each themselves being the product of a
co-variance matrix of features preceding the layer and a term related to the propagation of
gradients inside the network.

3.2 Derivation: MLP with bias

We extend these results to include a bias vector:

fθ(x) = w(cid:62) 1
√
dL

σ(W (cid:62)
L

1
(cid:112)dL−1

σ(...

1
√
d2

σ(W (cid:62)
2

1
√
d1

σ(W (cid:62)

1 x + B1) + B2)... + BL−1) + BL) + B

We need to update our terms as well, so that each layers output is now:

Xl =

1
√
dl

σ(W (cid:62)

l Xl−1 + Bl)

And update our deﬁnition of Dl:

Dl = diag(σ(cid:48)(W (cid:62)

l Xl−1 + Bl))

We can now derive the equation for the bias vectors’ contribution to the NTK for the
bias of any layer, Bl. Taking the series of gradients of each weight bearing tensor in the
operation reveal that the bias vectors also contribute components equal to the matrix S
described above times the element wise product with a matrix of all ones, Jn, which we
describe below.

∂fθ(x)
∂Bl

= w(cid:62) 1
√
dL

σ(cid:48)(W (cid:62)

L XL−1 + BL)W (cid:62)

L−1...σ(cid:48)(W (cid:62)

l Xl−1 + Bl)

∂Bl
∂Bl

Substituting in the deﬁnition for S taken above (with our new deﬁnition of D):

∂fθ(x)
∂Bl

= Sl

∂Bl
∂Bl

Considering that Bl is a matrix ∈ Rdl,n, and that Sl is the same for every element inside
that matrix, we can represent the computation for the entire matrix of bias parameters as
an element wise product with the matrix of ones, J. This matrix is also ∈ Rdl,n. We will
notate the ﬁrst dimension as a subscript and leave the second dimension understood. Thus,
Jdl is the matrix of all ones with shape Rdl,n. This allows us to write:

The NTK component from the bias at layer l is therefore:

∂fθ(x)
∂Bl

= Sl (cid:12) Jdl

4

The total NTK can therefore be expressed as:

K N T K
Bl

= (Sl (cid:12) Jdl)(cid:62)(Sl (cid:12) Jdl)

K N T K = X (cid:62)

L XL + J (cid:62)
dL

JdL +

L
(cid:88)

l=1

3.3 Autograd Algorithms

(S(cid:62)

l Sl) (cid:12) (X (cid:62)

l−1Xl−1) +

L
(cid:88)

l=1

(Sl (cid:12) Jdl)(cid:62)(Sl (cid:12) Jdl)

In addition to the algorithm described above that explicitly calculates the NTK for MLP
architectures, there are additional algorithms included in torchNTK that calculate the NTK
using autograd methods. Autograd methods, while slower than our explicit NTK calculation,
extend to other PyTorch architectures and largely work ’out-of-the-box’ reducing the user’s
margin of error:

1. The ﬁrst alternative makes a call to torch.autograd.functional.jacobian across the dataset,

stacks the resulting list of tensors from each datapoint, then simply constructs the NTK
as:

K N T K = ∇f (x, θ)(cid:62)∇f (x, θ)

2. A second alternative calls autograd on the model iteratively across each layer for each

datapoint, and was adapted from the work of [5]

3. A third alternative computes each row of the Jacobian vector product for each op-
eration, then outputs each operation to a dictionary. This represents our ’layerwise’
autograd method

4. With PyTorch 1.11, a new torch.vmap function was created to parallelize computations
across the batch dimension. One speciﬁc use case is to speed up the computation of
the Jacobian. This can also be applied to speed up the computation of the layerwise
autograd method and we have included it as a piece of experimental software.

4 Software Performance

In this section we detail the performance diﬀerences and trade-oﬀs between the various
algorithms for two classes of models: a MLP and a CNN, at two diﬀerent widths. All
algorithms were bench marked for their time to completion and maximum GPU memory
allocations for calculating the ﬁnal NTK of the same model for the same data on the same
hardware inside an IPython kernel. We tested the algorithms on a local computer cluster
equipped with an A-100 DGX node which we queried for 4 cpu cores and just one A-100
GPU. All algorithms were tested using GPU tensors and models, except the Full Jacobian
implementation, which is CPU only.

For each of the ’MLP’ benchmarks, we created a neural network represented by a Module
object with 4 fully-connected layers. Each hidden layer had a width of 100 neurons. The

5

input data was a vector of length 100 drawn from the standard normal distribution. The
network terminated into a single neuron. Each hidden layer used the tanh activation function,
while the output neuron was not routed through an activation function before calculating the
NTK. As is common in the NTK literature, we used the NTK parameterization by dividing
each layer’s output by the square root of the width of that layer. The weights between each
benchmark were kept the same using a random number generator seed, and were themselves
drawn from the standard normal distribution.

As a check of our claim that we expect layerwise computations to be more memory
eﬃcient in the deep and narrow regime, we also calculated a memory benchmark for a high
parameter MLP, called ’MLP-h’. This model has 8 layers, each hidden layer with a width
of 1000, an input shape of 1000, tanh activation functions, terminates into a single neuron
output. All memory benchmarks were calculated with the maximum allocated memory on
the GPU, (torch.cuda.max memory allocated ()).

N Datapoints

MLP Benchmark Time
Time [sec]
Full Jacobian Autograd L. Autograd L. Autograd w vmap Explicit Diﬀerentiation

10
100
1000
10000
30000
40000

3.3e-3
39e-3
3.71
359
OOM
OOM

5.95e-3
47.8e-3
467e-3
4.77
15.7
21.7

18.2e-3
174e-3
1.97
19.8
63.0
84.1

3.97e-3
10.3e-3
90.4e-3
869e-3
2.89
4.06

1.01e-3
0.98e-3
1.06e-3
9.03e-3
74.7e-3
OOM

Table 1: For a variety of number of datapoints, we ran our empirical NTK calculation algorithms
on the same MLP model and calculated the time until completion using the IPython magic function
% timeit and report the mean time here. For function calls longer than 10 seconds a single function
call’s time is reported, calculated using the diﬀerence in system clock time. When the algorithm
we called failed due to running out of system memory, we report ’OOM’ instead. Generally we
see that among autograd methods, the algorithms making use of DataLoader objects to feed the
calculation (Autograd and L. Autograd with vmap) are faster, with torch.vmap enabling greater
parallelization. With any appreciable amount of data, the method that calculates the full Jacobian
becomes appreciably slower, so much so, that for the next set of benchmarks on a larger model we
exclude the algorithm from analysis (Tables 3 and 4). Explicit Diﬀerentiation is faster than the
alternatives, owing this to the engineering made to parallelize the computation across the entire
dataset. With greater parallelization generally comes higher memory costs, which is explored
further in Table 2.

The results of these tables demonstrate that for very deep MLP networks, our explicit
diﬀerentiation technique is more memory eﬃcient and many times more time eﬃcient than
In addition, we show there is a regime of model architectures where
autograd methods.
layerwise computations are more memory eﬃcient than full Jacobian computation with Au-
tograd. Because the ’best choice’ of algorithm depends on the speciﬁc goal of the researcher,
we emphasize that individual researchers should benchmark their own architectures on their
own systems to make an informed decision about which NTK algorithm suits them. Another
key takeaway is that more eﬀort should be placed into developing and investigating highly
optimized explicit diﬀerentiation techniques for other model architectures. It is clear that

6

MLP Benchmark Memory

N Datapoints

10
100
1000
10000
30000
40000

Memory [Mb]
Autograd L. Autograd L. Autograd w vmap Explicit Diﬀerentiation
1.24
13.31
94.27
2471
21809
25731

1.61
12.89
132
2042
14514
25751

0.45
1.3
32.15
1659
14572
OOM

2.82
25.39
242
2416
7250
11233

Table 2: Each algorithm (Except for Full Jacobian which was implemented only on the CPU, so our
benchmark technique was not recorded) was run once on the GPU and the peak memory allocated
on the GPU device was recorded. If the device ran out of memory, then ’OOM’ was recorded instead.
There are a few factors that are coupled that determine the memory usage: including whether the
computation is batched over the number of data points, whether the computation parallelizes over
the number of datapoints, whether the computation is layerwise or not, the architecture, and the
number of datapoints. Due to this complexity, we suggest trying each method on a subset of the
data ﬁrst to search for a suitable choice.

N Datapoints

10
100
500
1000
10000
20000

MLP-h Benchmark Time
Time [sec]
Autograd L. Autograd L. Autograd w vmap Explicit Diﬀerentiation
30.8e-3
184e-3
1.3
2.29
OOM
OOM

70.5e-3
690e-3
3.38
6.89
OOM
OOM

2.43e-3
2.36e-3
2.08e-3
2.13e-3
44.3e-3
161e-3

25.3e-3
234e-3
1.31
OOM
OOM
OOM

Table 3: Given our belief that a layerwise computation will be memory eﬃcient in the deeper and
narrower regime, we re-ran our benchmarks on a deeper model for comparison. Each benchmark
was calculated using the IPython magic timieit function, with mean times reported here. If the
calculation took longer than 10 seconds, a single calculation was used with the time computed
from the diﬀerence in system clock times. Because MLP-h has many more parameters, the size
of the Jacobians needed to calculate the NTK more quickly saturate the available GPU memory.
Whenever the calculation used all the available GPU memory we replace the value with ’OOM’. We
see here that in the deeper regime, our layerwise approaches extend the calculation as we expected

7

N Datapoints

10
100
500
1000
10000
20000

MLP-h Benchmark Memory
Memory [Mb]
Autograd L. Autograd L. Autograd w vmap Explicit Diﬀerentiation
238
903
4160
8238
OOM
OOM

277
1314
6227
12378
OOM
OOM

158
101
175
284
4983
17899

717
5695
28101
OOM
OOM
OOM

Table 4: We also include memory benchmarks from each algorithm with increasing number of
datapoints, where we can see that in the deeper and narrower regime this model probes, and
especially with many datapoints, layerwise based calculations use less peak GPU memory than
non-layerwise approaches. Our Explicit Diﬀerentiation model extended the calculation of the ﬁnite
NTK to 10000 datapoints under 5GB of peak allocated memory, demonstrating that our code
extends the study of the ﬁnite NTK with many datapoints to modest workstations. There exist
many consumer level GPU choices with more than 5GB of memory.

tremendous beneﬁts exist in doing so: our MLP-h model benchmark shows a speed up of
over 1000x compared to the nearest autograd technique on 1000 datapoints. While tedious,
in scenarios where limited hardware is available or has a high cost, or where many of these
NTKs will need to be calculated (for instance, see our experiments in S5.2 below) the beneﬁts
of explicit diﬀerentiation can outweigh the up-front development costs. Finally, other neural
tangent libraries may beneﬁt in reducing peak memory use with a layerwise approach.

5 Experiments and Use Cases

5.1 Fisher Information Matrix

As pointed out in contemporary work on the Fisher Information Matrix, the NTK shares its
non-zero eigenvalues with its dual matrix [16, 15]:

F =

(cid:18) ∂fθ(x)
∂θ

(cid:19) (cid:18)∂fθ(x)

(cid:19)(cid:62)

∂θ

The Fisher information matrix (FIM) is of interest because at convergence with training
loss zero the Hessian of the mean squared loss function is equal to the FIM. In the following
equation t indexes the datapoint inside a dataset of size T, see equation 9 of [16].

H = F −

1
T

T
(cid:88)

t

(y(xt) − f (xt))∇θ∇θf (xt)

This makes the FIM useful in study the geometry of the loss landscape. Authors have
suggested studying the eigenvalues of the FIM to uncover what they refer to as ”pathological
sharpness” or the distance between the mean value of the eigenvalues of the FIM and the
maximum value of the FIM [16, 15]. Seeking models with low sharpness in the loss landscape

8

in the local neighborhood with respect to the parameters have been observed to improve
generalization [11], so it is plausible that the FIM provides correlative information on model
generalizability. In the layerwise setting, each operation of the neural network also can be
used to create a [pl x pl] layerwise Fisher information matrix. Given that we know pl from
our architecture, once we calculate the layerwise NTK we actually know the entire spectra
of these layerwise FIM.

5.2 Visualizing the NTK over training

In this experiment, we calculated the NTK and each layerwise NTK additive component
for every training step of an MLP trained by vanilla gradient descent to classify MNIST-2,
where we have randomly sampled handwritten digits of class 6 and 9. By collecting the NTK
at every training step, we can reconstruct a video of how the NTK changes that you can
view here.. A more detailed explanation of our experimental setup are available in appendix
B.

In the plots below, we have sorted our training data such that the ﬁrst 5000 indices are all
class 6 and the next 5000 indices are all class 9. This makes visualization more interpretable
and does not impact learning because our gradient updates are averaged over the entire
training dataset.

In Fig 1 we plot the initial and ﬁnal NTK matrix over training. The kernel has dis-
criminatory ability, meaning that the block of 6s have in general higher NTK value than
the block in the upper right and lower left quadrant. Note that as training progresses the
diagonal blocks become darker and the oﬀ-diagonal blocks become lighter, representing that
the NTK is capturing information about how the neural network is diﬀerentiating between
classes. This is consistent with the intuition that NTK represents a similarity score between
datapoints as measured by a dot product between the neural function’s gradients. One can
use this kernel to do binary classiﬁcation by computing the similarity of some training point
x with the dataset X. The kernel machine describing binary classiﬁcation is:

yi = sgn(

(cid:88)

wkYkK(xi, Xk))

k

Where the result is mapped onto -1,1. Because our training data is balanced, and for
simplicity, we set all wk to 1 as a quick approximation of the accuracy of a kernel machine
that could utilize each NTK. We compute these accuracies at the start and end of training
and report them in the table: 5

While not theoretically precise, it is possible that quantities of the ﬁnite NTK can give
insight to properties of the neural network, and in fact, there is preliminary evidence showing
that these ﬁnite kernels (non-linearly) correlate with the performance of their inﬁnite width
counterparts in CNNs, which in turn themselves correlate with the ANN’s performance
(compare table 2 and table 1 of [2]). Using this fact, one might be able to initiate a neural
architecture search by searching for architectures or parameterizations whose initial NTK
gives better performance.

9

Figure 1: The initial (left) and ﬁnal (right) NTK matrix values; the ﬁnal values were calculated
after training the neural network for 20k epochs on a binary classiﬁcation task. The dataset was
sorted before training with all indices 0-5k from MNIST class 6 and 5k-10k from MNIST class
9. Therefore, the darker squares in the top left and bottom right quadrant are expected given
similar datapoints are of the same class and that the NTK represents a similarity metric. This also
explains the high values along the diagonal, where the NTK similarity between the same datapoints
should be expected to be high. The full NTK shown in these images is constructed by summing the
additive components from the parameterized operations of the network, plotted in the appendices
as ﬁgures 2 through 5

Kernel/Method
layer 1
layer 2
layer 3
layer 4
NTK
Neural Network (train)
Neural Network (test)

initialization training end
98.37 +/- 0.04
95.0 +/- 0.2
98.54 +/- 0.04
96.5 +/- 0.1
98.91 +/- 0.03
98.0 +/- 0.1
97.2 +/- 0.1
96.2 +/- 0.1
98.96 +/- 0.03
98.56 +/- 0.04
98.81 +/- 0.02
51 +/- 1
97.83 +/- 0.05
50 +/- 1

Table 5: The test accuracies from a holdout dataset from the simpliﬁed kernel machine are shown
in the table before and after training. We note that all accuracies increased as the underlying
network specialized to our training task, and that the ﬁnal accuracy of the underlying ANN model
exceeded the performance of any of the kernels. Another interesting observation is that not all
layers have the same accuracy. While this is possibly explained between the layer 4 and layer 1
NTK as simply a matter of how many parameters are being used to measure the similarity, layer
2 and layer 3 have the exact same number of parameters, but still have a diﬀerent performance.
Future work will be conducted to systematically measure these performances from a variety of MLP
architectures using this software package.

10

6 Future Work

6.1 Future Improvements and Known Issues

We are releasing our software in alpha open-source with a pledge to continue to improve and
update our software. We welcome the contributions of the community and look forward to see
how other groups might use or be inspired by the software. There are speciﬁc improvements
to make the software complete that we brieﬂy touch on in this section.

Currently, each algorithm expects a single neuron output. This is a signiﬁcant limitation,
as common practice for even basic multi-task classiﬁcation would be to have a number of
output neurons representing each class. We believe that our autograd techniques could be
extended to multiple output neurons with additional eﬀort.

Motivated by our explicit derivative success in MLP, we could add additional deriva-
tives for other architectures.
Initial attempts at extending an explicit derivative to fully
convolutional networks became memory ineﬃcient by relying on large matrices to describe
derivatives of the convolution operation. However, additional eﬀort could be placed towards
the end of achieving a fast and memory eﬃcient form.

The software lacks multi-GPU support. Note that the neural-tangents library includes
native GPU parallelization. Multiple GPU support would be a large boon; it targets two
core issues with NTKs for larger models– memory constraints and time costs. Even the
calculation of the NTK for a modest multilayer perceptron on a subset of MNIST requires the
full memory of a single A100 GPU (see Table 2). This means for more common workstations
and consumer level GPUs researchers are still severely limited to small models and small
datasets.

7 Conclusion

This technical report has described the theoretical background and functional performance
of torchNTK. This software has the capability to eﬃciently calculate the tangent kernel for
MLP architectures in PyTorch, but through autograd methods we have extended the utility
to arbitrary architectures. This work is impactful because neural kernels are objects of
interest to the theoretical community, and with PyTorch support we can extend the number
of researchers who have access to compute them. Our software enables teams to more easily
calculate the kernels, which we hope will give way for further research and application.

A key takeaway from our work is that teams should consider the performance needs
to conduct their research and determine whether calculating the explicit derivative of the
network with respect to the parameters is worthwhile. We have shown that, at-least in
the cases of MLP architectures, explicit diﬀerentiation is more eﬃcient in both time and
memory than autograd methods. Furthermore; teams must evaluate honestly whether they
have access to the software expertise to implement the calculation they derive in the most
parallelized or eﬃcient manner. Converting the derived equations to eﬃcient code is a skill
set that should not be underestimated.

This software enables researchers to calculate the NTK in PyTorch faster than ever before
and exposes the user to what we have called the layerwise components of the NTK, each

11

representing a parameterized operation inside the neural network. Our future work will wield
this software package to explore the NTK and these components to search for use-cases for
practical A.I. end users and interpretation of such models.

References

[1] Sanjeev Arora et al. “Harnessing the Power of Inﬁnitely Wide Deep Nets on Small-data
Tasks”. In: arXiv e-prints, arXiv:1910.01663 (Oct. 2019), arXiv:1910.01663. arXiv:
1910.01663 [cs.LG].

[2] Sanjeev Arora et al. “On Exact Computation with an Inﬁnitely Wide Neural Net”.
In: arXiv e-prints, arXiv:1904.11955 (Apr. 2019), arXiv:1904.11955. arXiv: 1904.11955
[cs.LG].

[3] James Bradbury et al. JAX: composable transformations of Python+NumPy programs.

Version 0.2.5. 2018. url: http://github.com/google/jax.

[4] Tom B. Brown et al. “Language Models are Few-Shot Learners”. In: arXiv e-prints,

arXiv:2005.14165 (May 2020), arXiv:2005.14165. arXiv: 2005.14165 [cs.CL].

[5] Wuyang Chen, Xinyu Gong, and Zhangyang Wang. “Neural Architecture Search on
ImageNet in Four GPU Hours: A Theoretically Inspired Perspective”. In: arXiv e-
prints, arXiv:2102.11535 (Feb. 2021), arXiv:2102.11535. arXiv: 2102.11535 [cs.CV].

[6] Lenaic Chizat, Edouard Oyallon, and Francis Bach. “On Lazy Training in Diﬀerentiable
Programming”. In: arXiv e-prints, arXiv:1812.07956 (Dec. 2018), arXiv:1812.07956.
arXiv: 1812.07956 [math.OC].

[7] Yatin Dandi and Arthur Jacot. “Understanding Layer-wise Contributions in Deep Neu-
ral Networks through Spectral Analysis”. In: arXiv e-prints, arXiv:2111.03972 (Nov.
2021), arXiv:2111.03972. arXiv: 2111.03972 [cs.LG].

[8] Pedro Domingos. “Every Model Learned by Gradient Descent Is Approximately a
Kernel Machine”. In: arXiv e-prints, arXiv:2012.00152 (Nov. 2020), arXiv:2012.00152.
arXiv: 2012.00152 [cs.LG].

[9] Alexey Dosovitskiy et al. “An Image is Worth 16x16 Words: Transformers for Image

Recognition at Scale”. In: arXiv e-prints, arXiv:2010.11929 (Oct. 2020), arXiv:2010.11929.
arXiv: 2010.11929 [cs.CV].

[10] Zhou Fan and Zhichao Wang. “Spectra of the Conjugate Kernel and Neural Tangent
Kernel for linear-width neural networks”. In: arXiv e-prints, arXiv:2005.11879 (May
2020), arXiv:2005.11879. arXiv: 2005.11879 [stat.ML].

[11] Pierre Foret et al. “Sharpness-Aware Minimization for Eﬃciently Improving Gener-
alization”. In: arXiv e-prints, arXiv:2010.01412 (Oct. 2020), arXiv:2010.01412. arXiv:
2010.01412 [cs.LG].

[12] Mario Geiger et al. “Disentangling feature and lazy training in deep neural networks”.
In: Journal of Statistical Mechanics: Theory and Experiment 2020.11, 113301 (Nov.
2020), p. 113301. doi: 10.1088/1742-5468/abc4de. arXiv: 1906.08034 [cs.LG].

12

[13] Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: arXiv e-prints,

arXiv:1512.03385 (Dec. 2015), arXiv:1512.03385. arXiv: 1512.03385 [cs.CV].

[14] Arthur Jacot, Franck Gabriel, and Clement Hongler. “Neural Tangent Kernel: Conver-
gence and Generalization in Neural Networks”. In: arXiv e-prints, arXiv:1806.07572
(June 2018), arXiv:1806.07572. arXiv: 1806.07572 [cs.LG].

[15] Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. “Pathological spectra of the
Fisher information metric and its variants in deep neural networks”. In: arXiv e-prints,
arXiv:1910.05992 (Oct. 2019), arXiv:1910.05992. arXiv: 1910.05992 [stat.ML].

[16] Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. “The Normalization Method
for Alleviating Pathological Sharpness in Wide Neural Networks”. In: arXiv e-prints,
arXiv:1906.02926 (June 2019), arXiv:1906.02926. arXiv: 1906.02926 [stat.ML].

[17] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. “ImageNet Classiﬁcation with
Deep Convolutional Neural Networks”. In: Advances in Neural Information Processing
Systems. Ed. by F. Pereira et al. Vol. 25. Curran Associates, Inc., 2012. url: https:
/ / proceedings . neurips . cc / paper / 2012 / ﬁle / c399862d3b9d6b76c8436e924a68c45b -
Paper.pdf.

[18] Jaehoon Lee et al. “Finite Versus Inﬁnite Neural Networks: an Empirical Study”. In:
arXiv e-prints, arXiv:2007.15801 (July 2020), arXiv:2007.15801. arXiv: 2007 . 15801
[cs.LG].

[19] Jaehoon Lee et al. “Wide neural networks of any depth evolve as linear models un-
der gradient descent”. In: Journal of Statistical Mechanics: Theory and Experiment
2020.12, 124002 (Dec. 2020), p. 124002. doi: 10 . 1088 / 1742 - 5468 / abc62b. arXiv:
1902.06720 [stat.ML].

[20] Jaehoon Lee et al. “Wide neural networks of any depth evolve as linear models un-
der gradient descent”. In: Journal of Statistical Mechanics: Theory and Experiment
2020.12, 124002 (Dec. 2020), p. 124002. doi: 10 . 1088 / 1742 - 5468 / abc62b. arXiv:
1902.06720 [stat.ML].

[21] Charles H. Martin, Tongsu Serena Peng, and Michael W. Mahoney. “Predicting trends
in the quality of state-of-the-art neural networks without access to training or testing
data”. In: Nature Communications 12, 4122 (Jan. 2021), p. 4122. doi: 10.1038/s41467-
021-24025-8. arXiv: 2002.06716 [cs.LG].

[22] Martin Abadi et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Sys-
tems. Software available from tensorﬂow.org. 2015. url: https://www.tensorﬂow.org/.

[23] Roman Novak et al. “Neural Tangents: Fast and Easy Inﬁnite Neural Networks in
Python”. In: arXiv e-prints, arXiv:1912.02803 (Dec. 2019), arXiv:1912.02803. arXiv:
1912.02803 [stat.ML].

[24] Ryosuke Okuta et al. “CuPy: A NumPy-Compatible Library for NVIDIA GPU Calcu-
lations”. In: Proceedings of Workshop on Machine Learning Systems (LearningSys) in
The Thirty-ﬁrst Annual Conference on Neural Information Processing Systems (NIPS).
2017. url: http://learningsys.org/nips17/assets/papers/paper 16.pdf.

13

[25] Adam Paszke et al. “PyTorch: An Imperative Style, High-Performance Deep Learning
Library”. In: Advances in Neural Information Processing Systems 32. Ed. by H. Wal-
lach et al. Curran Associates, Inc., 2019, pp. 8024–8035. url: http://papers.neurips.cc/
paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.

[26] Adepu Ravi Sankar et al. “A Deeper Look at the Hessian Eigenspectrum of Deep Neural
Networks and its Applications to Regularization”. In: arXiv e-prints, arXiv:2012.03801
(Dec. 2020), arXiv:2012.03801. arXiv: 2012.03801 [cs.LG].

[27] State of Data Science and Machine Learning 2021. Oct. 2021. url: https : / / www .

kaggle.com/kaggle-survey-2021.

[28] Christian Szegedy et al. “Inception-v4, Inception-ResNet and the Impact of Residual

Connections on Learning”. In: arXiv e-prints, arXiv:1602.07261 (Feb. 2016), arXiv:1602.07261.
arXiv: 1602.07261 [cs.CV].

[29] Mingxing Tan and Quoc V. Le. “EﬃcientNetV2: Smaller Models and Faster Training”.
In: arXiv e-prints, arXiv:2104.00298 (Apr. 2021), arXiv:2104.00298. arXiv: 2104.00298
[cs.CV].

[30] Yikai Wu et al. “Dissecting Hessian: Understanding Common Structure of Hessian in
Neural Networks”. In: arXiv e-prints, arXiv:2010.04261 (Oct. 2020), arXiv:2010.04261.
arXiv: 2010.04261 [cs.LG].

[31] Greg Yang and Edward J. Hu. “Tensor Programs IV: Feature Learning in Inﬁnite-
Width Neural Networks”. In: Proceedings of the 38th International Conference on
Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of
Machine Learning Research. PMLR, 18–24 Jul 2021, pp. 11727–11737. url: https :
//proceedings.mlr.press/v139/yang21c.html.

14

A Layerwise NTK visualizations

Plotted below are the additive components of the NTK for the experiment described in
Section 5.2. Figure 1 described the evolution of the full NTK over training, here we describe
the evolution of the additive components of the NTK for the same experiment.

Figure 2: initial (left) and ﬁnal (right) layerwise NTK for layer 1, the ﬁrst dense layer. This layer
had 39200 parameters. While the contrast increased between the datapoints of diﬀerent classes
similar to the remaining layers, layer 1 is unique in that the location of horizontal and vertical bar
features shifts over training. We hypothesize this is a shift in what the NTK measures as an outlier

15

Figure 3: initial (left) and ﬁnal (right) layerwise NTK for layer 2, the second dense layer. Layer 2
has 2500 parameters. Generally, the notable behavior over the course of training was that in layer
2 the contrast increases between examples of MNIST class 6s and MNIST class 9s.

Figure 4: initial (left) and ﬁnal (right) layerwise NTK for layer 3, the third dense layer. Layer 3
has 2500 parameters. Generally, the notable behavior over the course of training was that in layer
3 the contrast increases between examples of MNIST class 6s and MNIST class 9s.

16

Figure 5: initial (left) and ﬁnal (right) layerwise NTK for layer 4, the fourth and ﬁnal dense layer.
Layer 4 has 50 parameters. Generally, the notable behavior over the course of training was that in
layer 4 the contrast increases between examples of MNIST class 6s and MNIST class 9s.

17

B Details of Experiment in S5.2

Our model is a four layer MLP with an input feature vector of size 784, each hidden layer
has a width of 50 neurons, and ends in a single neuron readout layer to facilitate binary
classiﬁcation. The NTK is calculated before the ﬁnal sigmoid activation, using explicit
diﬀerentiation. Sigmoid was chosen to map the network function onto a binary decision
between classes. The network was placed into the NTK parameterization by dividing each
hidden layer by the square root of the width of the layer. Weights were initialized from the
standard normal distribution, but biases were frozen at zero and not computed in the NTK.
Our training dataset of 5000 examples of MNIST 6s and and 5000 9s were ﬂattened to a
feature vector, placed into the range 0-1, and normalized. The training dataset was sorted
such that the ﬁrst 5000 indices were all label 6 and the last 5000 were all label 9. Since we
are using full-batch gradient descent sorting doesn’t aﬀect training, but makes visualizing
the NTK easier. The model was trained for 20,000 gradient descent steps with a learning
rate 1e-2. At this point training had saturated but had not converged to a training loss of
0. Because we wanted to capture the NTK at every single update step, we believed it was
prudent to stop training early at that point.

18

C Example Usage

Checkout the notebooks provided in the repository, especially, ”DemoMethods.ipynb” for an
overview of how to set the inputs for each individual algorithm; the notebook also demon-
strates the agreement between methods for a small example. Below, we include a snippet
that demonstrates the simplest and most general calculation of the layerwise NTK.

#Example Use of autograd_components_ntk
#NOTE! should work for arbitrary architectures so long
#As the architecture terminates to a single neuron

from torchntk.autograd import autograd_components_ntk

class FC(torch.nn.Module):

’’’
simple network
’’’
def __init__(self,):

super(FC, self).__init__()
self.d1 = torch.nn.Linear(100,100,bias=False)
self.d2 = torch.nn.Linear(100,100,bias=False)
self.d3 = torch.nn.Linear(100,1,bias=False)

def forward(self, x0):

x1 = 1/np.sqrt(100) * activation(self.d1(x0))
x2 = 1/np.sqrt(100) * activation(self.d2(x1))
x3 = self.d3(x2)
return x3

model = FC()

train_x = torch.empty((200,100),device=’cpu’).normal_(0,1)
#train_x has shape [batch_size, n_features]

y = model(train_x.to(device))

#NTK_components is a dictionary with keys that are named
#parameters from the model and values that are NTK
#created from those parameters’ gradients.

NTK_components = autograd_components_ntk(model,y[:,0])

#Therefore, to get the full NTK you simply sum over
#the value tensors in the dictionary
NTK_full = torch.sum(torch.stack(

[val for val in NTK_components.values()]
),dim=0)

19

