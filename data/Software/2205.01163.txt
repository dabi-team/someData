2
2
0
2

p
e
S
5

]
E
S
.
s
c
[

2
v
3
6
1
1
0
.
5
0
2
2
:
v
i
X
r
a

Paving the Way for Mature Secondary Research: The Seven
Types of Literature Review

Paul Ralph
Dalhousie University
Halifax, Canada
paulralph@dal.ca

Sebastian Baltes
University of Adelaide
Adelaide, Australia
sebastian.baltes@adelaide.edu.au

ABSTRACT
Confusion over diÔ¨Äerent kinds of secondary research, and their
divergent purposes, is undermining the eÔ¨Äectiveness and useful-
ness of secondary studies in software engineering. This short paper
therefore explains the diÔ¨Äerences between ad hoc review, case sur-
vey, critical review, meta-analysis (aka systematic literature review),
meta-synthesis (aka thematic analysis), rapid review and scoping re-
view (aka systematic mapping study). These deÔ¨Ånitions and asso-
ciated guidelines help researchers better select and describe their
literature reviews, while helping reviewers select more appropri-
ate evaluation criteria.

CCS CONCEPTS
‚Ä¢ General and reference ‚Üí Surveys and overviews.

KEYWORDS
Literature review, secondary research, systematic review

ACM Reference Format:
Paul Ralph and Sebastian Baltes. 2022. Paving the Way for Mature Sec-
ondary Research: The Seven Types of Literature Review. In Proceedings
of the 30th ACM Joint European Software Engineering Conference and Sym-
posium on the Foundations of Software Engineering (ESEC/FSE ‚Äô22), Novem-
ber 14‚Äì18, 2022, Singapore, Singapore. ACM, New York, NY, USA, 5 pages.
https://doi.org/10.1145/3540250.3560877

1 INTRODUCTION
Scholarship and scholarly writing can be stratiÔ¨Åed into primary,
secondary, and tertiary research.

Primary research involves making observations in the broadest
sense of collecting data about objects that are not themselves stud-
ies. Computing code metrics, administering questionnaires, inter-
viewing participants, counting bugs, collecting documents, down-
loading source code, and taking Ô¨Åeld notes while observing a ret-
rospective meeting are all primary research.

Secondary research involves analyzing, synthesizing and critiquing

primary studies. Ad hoc reviews, case surveys, critical reviews,
meta-analysis, meta-synthesis and scoping reviews are all types

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
the author(s) must be honored. Abstracting with credit is permitted. To copy other-
wise, or republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
¬© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9413-0/22/11. . . $15.00
https://doi.org/10.1145/3540250.3560877

of secondary research. Secondary research is central to evidence-
based practice [16] because (1) practitioners cannot read every study;
(2) important decisions typically should be made based on the bal-
ance of evidence rather than a single study; (3) in many Ô¨Åelds, indi-
vidual studies are too small to produce accurate estimates of pop-
ulation parameters (e.g. the strength of the relationship between
two variables). However, some secondary research degrades into
predominately descriptive ‚Äúpapers about papers‚Äù with limited scope
and usefulness.

Tertiary scholarship has two related meanings: (1) analyses of
groups of secondary studies (meta-reviews); (2) summaries or in-
dices of broad areas of scientiÔ¨Åc knowledge as found in textbooks,
encyclopedia entries, etc.

We focus on secondary research because confusion over diÔ¨Äer-
ent kinds of secondary research, and their divergent purposes, un-
dermines the eÔ¨Äectiveness and usefulness of secondary studies in
software engineering (SE), which motivates the following objec-
tive:

Objective: The purpose of this article is to compile, describe,
compare, and contrast the diÔ¨Äerent kinds of secondary research
commonly published in software engineering venues, to pave
the way for more mature secondary research.

2 REVIEW TYPES
2.1 Ad Hoc Reviews
An ad hoc literature review is simply a discussion of some litera-
ture, as contained in most research papers as part of a background
or related work section. Ad hoc reviews may develop theory [e.g.
27], or integrate a new theory into existing literature [cf. 31]. Ad
hoc reviews can support a position paper, or tertiary scholarship.
Ad hoc reviews use purposive sampling [3]; that is, researchers
purposefully select papers or studies that are useful, relevant, or
support their arguments. Ad hoc reviews are often appropriate,
for example, to support theory development, identify promising
research topics, or prepare for a comprehensive exam. However,
they suÔ¨Äer from important limitations: their unsystematic nature
introduces sampling bias and deÔ¨Åes replication. This may lead to
cherry-picking of evidence supporting authors‚Äô arguments [10].
Therefore, ad hoc reviews are inappropriate for supporting empirical
statements such as ‚Äúx causes y‚Äù, ‚Äúmost people/objects have property
P‚Äù, or ‚Äúprocess P has structure S or follows rules R‚Äù.

2.2 Systematic Reviews
The term systematic review has two meanings, as follows:

(1) a literature review that employs a systematic (hence the
name), replicable process of selecting primary studies for

 
 
 
 
 
 
ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore

Ralph and Baltes

Table 1: The Seven Types of Literature Review

Type

Systematic

Purpose

Primary Studies

Analysis

Approach

ad hoc
case survey
critical
meta-analysis
meta-synthesis
rapid
scoping

no
yes
yes
yes
yes
yes
yes

discuss
explain & predict
prescribe
explain & predict
explain
explain & predict
describe

any
qualitative
any
quantitative
qualitative
quantitative
any

any
quantitative
any
quantitative
qualitative
quantitative
both

discuss purposively-selected related work
test causal hypotheses by aggregating case study results
defend a position and make recommendations by analyzing a sample of papers
estimate eÔ¨Äect sizes by aggregating results of similar quantitative studies
synthesize the Ô¨Åndings of numerous studies using qualitative analysis
a meta-analytic review that compromises rigor for speed
describe an area of research and map studies into meaningful categories

inclusion (below referred to as systematic review), including
case surveys, critical reviews, meta-analyses, meta-syntheses,
and scoping reviews.

(2) a systematic review that uses meta-analysis of quantitative
studies (especially experiments) to assess the strength of the
evidence for speciÔ¨Åc, usually causal, propositions (below re-
ferred to as meta-analysis).

This double meaning is due to the history of systematic reviews.
The concept of a meta-analytic systematic review emerged from
health and medicine in the late 20th century [25, ch.2], when prac-
titioners could not stay current with accelerating research produc-
tion. When Chalmers [6] founded the Cochrane Library in 1993,
the medical community coalesced around using meta-analysis to
inform evidence-based practice. Now, systematic review is often
conÔ¨Çated with meta-analysis. However, other kinds of systematic
reviews have also been around for decades. Scoping reviews were
proposed no later than 2005 [1]. Meta-synthesis goes back at least
to 1996 [14]. Case surveys were proposed as far back as 1974 [18].
Systematic reviews begin by searching databases for primary
studies that meet pre-established criteria. Most types of system-
atic reviews seek to identify all the primary studies that meet the
selection criteria by applying various techniques to mitigate sam-
pling bias and publication bias including: ‚Äúbackward and forward
snowballing searches; checking proÔ¨Åles of proliÔ¨Åc authors in the
area; searching both formal databases (e.g. ACM Digital Library)
and indexes (e.g. Google Scholar); searching for relevant disser-
tations; searching pre-print servers (e.g. arXiv); soliciting unpub-
lished manuscripts through appropriate listservs or social media;
contacting known authors in the area‚Äù [28].

The way that these primary studies are analyzed determines the

type of systematic review.

2.2.1 Meta-analysis. An archetypal systematic review analyzes a
set of randomized controlled experiments with the same indepen-
dent and dependent variables.

Suppose, ten diÔ¨Äerent experiments randomized SE undergrads
into a control group (who complete tasks individually) and a treat-
ment group (who complete tasks in pairs); that is, individual vs.
pair programming. The dependent variable was the number of tasks
completed successfully. Each primary study reports the results of
an independent samples t-test including the mean and standard de-
viation for each group, the t-statistic, the p-value, Cohen‚Äôs ùê∑ (eÔ¨Äect
size) and the 95% conÔ¨Ådence interval for ùê∑.

Our aim, then, is to combine the results of these ten experiments
to estimate the eÔ¨Äect size of pair programming on eÔ¨Äectiveness.
Suppose that four studies found a negative eÔ¨Äect, three found no
signiÔ¨Åcant eÔ¨Äect, and three found a positive eÔ¨Äect. We DO NOT

proceed by vote counting; that is, conclude that the eÔ¨Äect is nega-
tive because four negative results outweigh three positive results.
Vote counting is invalid because primary studies can have wildly
diÔ¨Äerent sizes and quality levels. What if the studies that found pos-
itive eÔ¨Äects were much larger and more rigorous; while the studies
that found negative eÔ¨Äects were small and confounded? What if,
when the three studies without signiÔ¨Åcant results are aggregated,
together their results are signiÔ¨Åcant?

Instead of vote-counting, we apply meta-analysis [9]; that is, we
statistically aggregate primary study results into a global eÔ¨Äect size
estimate. This is often possible with the summary data reported in
papers, without the original datasets.

Meta-analysis can aggregate results from other kinds of (quan-
titative) methods as long as the studies have the same indepen-
dent and dependent variables, or overlapping sets of variables. The
more complicated the overlaps, the more complicated the meta-
analytic model. A comprehensive tutorial on statistical procedures
for meta-analysis is beyond the scope of this paper, but readily
available [4].

Meta-analytic reviews essentially have the same research ques-
tion as the studies being reviewed. The purpose of the meta-analysis
is to reach a more reliable, robust conclusion by aggregating all
available data, implying two important criteria for meta-analysis:
(1) researchers should go to great lengths to Ô¨Ånd ALL relevant stud-
ies; and (2) researchers must evaluate the quality of each primary
study and either exclude low quality studies or include study qual-
ity as a covariate in the meta-analytic model.

In summary, when scientists equate systematic reviews with
evidence-based practice, they usually mean meta-analytic reviews.
Meta analysis aggregates quantitative studies that investigate the
same or overlapping hypotheses. They do not simply describe exist-
ing research. Meta-analysis is rare in software engineering. While
there are several good examples [e.g. 12, 26, 29], quality meta-analysis
is dwarfed by superÔ¨Åcial scoping reviews [7].

2.2.2 Meta-synthesis. Meta-synthesis‚Äîaka thematic analysis, nar-
rative synthesis, meta-ethnography, and interpretive synthesis‚Äî
refers to a family of methods of aggregating qualitative studies
[14]. Meta-synthesis is approximately the constructivist analogue
of meta-analysis. After identifying the primary studies, the researcher
applies hermeneutical and dialectical analyses to understand each
primary study, translate them into each other, and construct an ac-
count of the body of research; for example, a theory of the central
phenomenon that unites the primary studies.

Meta-synthesis requires expertise in qualitative methods and
familiarity with the underlying philosophical assumptions. If one
does not know what ‚Äúhermeneutical and dialectical analyses‚Äù means,

Paving the Way for Mature Secondary Research: The Seven Types of Literature Review

ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore

one should not attempt meta-synthesis. Meta-synthesis is NOT
organizing papers into categories (as in scoping reviews). Meta-
synthesis is synthesizing a credible, nuanced account of a phenom-
enon from prior qualitative Ô¨Åndings.

In principle, meta-synthesis can be applied to both qualitative
and quantitative work. In practice, such combinations are philo-
sophically strained.

2.2.3 Case Survey (aka Case Meta-analysis). A case survey‚Äôs pri-
mary studies are (typically qualitative) case studies in the broadest
sense (i.e., a scholarly account of some events). Experience reports
and grey literature may or may not be included, depending on
the study‚Äôs purposes. Unlike meta-synthesis, however, a case sur-
vey transforms qualitative accounts into a quantitative dataset that
supports null-hypothesis testing. Case surveys share the philoso-
phy of meta-analysis (positivism), not meta-synthesis (construc-
tivism). They typically begin with a priori hypotheses and an a
priori coding scheme. The researcher reads each case and extracts
data into the coding scheme, often using simple dichotomous vari-
ables like ‚Äòdid the team have retrospective meetings? [yes/no]‚Äô or
‚Äòdoes the case mention coordination problems? [yes/no]‚Äô The re-
sulting dataset is often too sparse for regression modeling, so re-
searchers use simple bivariate correlations to test hypotheses [5].
Case surveys were proposed by the Rand Corporation as a ‚Äúway
to aggregate existing research‚Äù [18], quickly picked up by Yin [32],
and later elaborated in management [5, 17]. Now case meta-analysis
is widely used in management and information systems [15]. While
SE-specÔ¨Åc case survey guidelines are available [e.g. 19, 22], SE case
surveys remain rare. However, case surveys have been used to in-
vestigate strategic pivots at software start-ups [2] and how organi-
zations select component sourcing options [23]. Case surveys have
great potential in SE because case studies are so common.

2.2.4 Critical Reviews. A critical review analyzes a sample of pri-
mary studies to support an argument or critique.1 For example,
Stol et al. [31]‚Äôs critical review of the use of grounded theory in
SE criticizes method slurring; that is, claiming to have used a re-
search methodology that was not actually used to create illusory le-
gitimacy. Similarly, Baltes and Ralph‚Äôs critical review [3] criticizes
how SE researchers often overstate sample representativeness and
conÔ¨Çate random sampling with representative sampling. Indeed,
critical reviews in software engineering often investigate method-
ological topics, including how ethnography is reported [33] or how
qualitative research is synthesized [13].

Critical reviews diÔ¨Äer from case surveys and meta-analysis in
two important ways. First, meta-analytic reviews aggregate evi-
dence regarding causal relationships to generate evidence-based
recommendations, while critical reviews critically evaluate issues.
Critical reviews are not for supporting evidence-based practice, or
summarizing the evidence for a theory. Critical reviews are part
of the meta-scientiÔ¨Åc discourse; i.e., the conversations a scientiÔ¨Åc
community has internally about how it conducts research.

1The term critical review has been used diÔ¨Äerently elsewhere, but we focus on the
meaning in SE.

Second, for many critical reviews, including all relevant primary
studies is impossible and unnecessary. For example, a critical re-
view of adherence to the Introduction, Method, Results and Discus-
sion framework (IMRaD) framework [30] could include all SE pa-
pers ever written. Instead, a random sample of papers from a selec-
tion of top journals and conferences is suÔ¨Écient because critical
reviews do not assess causal claims so publication bias‚Äî‚Äúwhat if
signiÔ¨Åcant results were published but non-signiÔ¨Åcant results were
not?‚Äù‚Äîis irrelevant.

Analysis performed within a critical review can be quantitative,
qualitative or both. However, critical reviews typically adopt a crit-
ical stance; that is, they go beyond mere description and oÔ¨Äer spe-
ciÔ¨Åc critiques of the work being reviewed.

2.2.5
Scoping Reviews (aka Systematic Mapping Studies). What is
often called a ‚Äúsystematic mapping study‚Äù in SE [24] is usually
called a scoping review elsewhere (e.g. in health, medicine, and psy-
chology). The purpose of a scoping review is to understand the sta-
tus of research on a particular topic, typically by mapping primary
studies into categories (hence, ‚Äúmapping study‚Äù).

Scoping reviews are often primarily descriptive. They count the
number of studies on a topic. They often organize studies by re-
search method, subtopic, authors, geographical location, publica-
tion venue, etc. They often conclude that more research is needed
in this or that subtopic. For example, Mohanani et al. [20] mapped
primary studies according to which cognitive bias (e.g. conÔ¨Årma-
tion bias) they investigated and in which area of software devel-
opment (e.g. design, management) they investigated it, then called
for more research on debiasing (preventing or mitigating cognitive
biases).

The problem with scoping reviews is that they typically fail to

fulÔ¨Åll any of the core purposes of systematic reviews:

Meta-analysis and case surveys synthesize the results of many
studies to answer speciÔ¨Åc empirical (often causal) questions about
the world. Scoping reviews include a similar search but typically
do not provide suÔ¨Écient quantitative synthesis to answer impor-
tant empirical questions. Therefore, scoping reviews do not inform
evidence-based practice like meta-analyses and case surveys do.

Meta-synthesis involves deep, theory-oriented reinterpretation
of related qualitative studies. While scoping reviews often include
qualitative analysis (e.g. mapping or categorization), that analysis
is often too superÔ¨Åcial to generate novel, useful theories.

Critical reviews use a sample of papers to demonstrate an impor-
tant pattern for a scientiÔ¨Åc community‚Äôs internal discourse. While
scoping reviews often give recommendations regarding future re-
search, they focus on an empirical topic (e.g. cognitive biases in
SE); not a meta-scientiÔ¨Åc topic (e.g. construct validity); therefore,
they are not conÔ¨Ågured, from the outset, to deliver useful meta-
scientiÔ¨Åc critique.

In summary, scoping reviews begin like other kinds of system-
atic reviews, but stop short of synthesizing the data into aggregate
empirical results, theory, or meta-scientiÔ¨Åc critique. This is by def-
inition: if a scoping review applies a meta-analytic model to aggre-
gate primary study results, or applies hermeneutics and dialectics
to synthesize qualitative accounts, or develops an evidence-based
critique of a scientiÔ¨Åc practice, it is no longer a scoping review; it

ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore

Ralph and Baltes

is a meta-analysis, a case survey, a meta-synthesis, or a critical re-
view. Some authors therefore recommend a scoping review ‚Äúas a
precursor to a systematic review‚Äù [21].

2.2.6 Rapid Reviews. A rapid review is a meta-analysis that makes
methodological compromises to reduce completion time [8]. Ganann
et al. found many such compromises including restricting the lit-
erature search, truncating results, omitting techniques for over-
coming publication bias (e.g. reference snowballing); streamlining
screening and data extraction, and skipping quality assessment [8].
Rapid reviews are justiÔ¨Åed if and only if evidence is needed
to support imminent decisions, and waiting for a comprehensive
meta-analytic review would be harmful. These conditions occur in
health and medicine, for example, when an unprecedented viral
pandemic strikes. These conditions do not occur frequently in SE.
The term rapid review should not be used to legitimize bad system-
atic reviews where there is no pressing need for immediate results.
Moreover, SE-related topics rarely have so many primary studies
that it would take more than a year to complete a comprehensive
review.

3 DISCUSSION
Secondary research that synthesizes the results of a body of work
into robust, reliable Ô¨Åndings and practical ramiÔ¨Åcations is crucial
for evidence-based practice. Secondary research is the funnel that
concentrates the overwhelming Ô¨Çow of empirical results into some-
thing practitioners can digest and use. However, most secondary
research in SE contains no synthesis of Ô¨Åndings [7], because most
SE secondary studies are scoping reviews. Scoping reviews strug-
gle to get from a broad description of what is happening in a Ô¨Åeld
to speciÔ¨Åc recommendations for practitioners. While the purpose
of meta-analysis is to answer a speciÔ¨Åc empirical question by com-
bining the results of all of the studies that tested corresponding
hypotheses, scoping reviews just classify studies.

Meanwhile, SE researchers perceive inadequate numbers and
quality of primary studies as major barriers to good secondary
research [11]. Useful meta-analysis (and meta-synthesis) are only
possible given suÔ¨Éciently large bodies of similar quantitative (and
qualitative) studies. With their sparse data sets, case surveys need
many case studies.

3.1 Recommendations
Our primary recommendation is as follows:

Recommendation: Software engineering needs more meta-analysis,
case surveys, and meta-synthesis, but fewer scoping and rapid
reviews.

If the body of evidence is suÔ¨Écient for a meta-analysis, case
survey, or meta-synthesis, by all means, do one. Top venues should
welcome competent meta-analyses, case surveys, and meta-syntheses
as these approaches are central to evidence-based practice. In con-
trast, predominately descriptive scoping reviews are essentially works-
in-progress. They belong in workshops, poster sessions and short
paper tracks. If the literature does not support a meta-analysis, case
survey, or meta-synthesis, then a scoping review is unlikely to sub-
stantively contribute to the scholarly discourse. If the literature

does support some kind of synthesis, the scoping review is like a
pilot study for a more ambitious systematic review.

Rapid reviews similarly have little place in SE. The conditions
that justify a rapid review rarely occur in our Ô¨Åeld. Top venues
should expect rapid reviews to justify methodological compromises
and explain why results are needed immediately. The term ‚Äúrapid
review‚Äù should not be used to legitimize a bad meta-analysis.

The case for critical reviews is more complicated. SE does not
need more critical reviews per se; SE needs a more vivid, robust
meta-scientiÔ¨Åc discourse. Meta-science in SE has been hamstrung
by reviewers and venues insisting that every paper reports an em-
pirical study, regardless of whether its research question is empir-
ical. As long as this unwise position remains, we need critical re-
views to inject meta-science into our publications.

Since published guidelines for critical reviews are scarce, we will
state some recommendations. First, when elucidating a ubiquitous
problem (e.g. poor construct validity) citing speciÔ¨Åc examples of
papers making that mistake is unnecessary. The credibility gained
is not worth the costs to community cohesion, interpersonal rela-
tionships, and future collaborations. Authors should explain this
reasoning and reviewers should accept this compromise. Second,
reviewers should not misapply the ‚ÄúÔ¨Ånd ALL relevant studies‚Äù or
‚Äúexclude low quality studies‚Äù criteria to critical reviews.

Finally, authors should identify the kind of systematic review
they have done. Avoid using ‚Äúsystematic review‚Äù as a synonym
for a meta-analysis or a scoping review. In the broadest sense, any
review that uses a systematic process of identifying primary
studies is a systematic review.

3.2 Limitations
This is not an empirical paper. It is meta-science: part of our com-
munity‚Äôs internal discourse about what we are doing, why, and
how to do it. This paper is credible because it meets the expecta-
tions laid out in the Empirical Standard for Meta-science [28]: it
is useful because most SE researchers read or perform literature
reviews; it makes clear recommendations (Section 3.1); presents
coherent arguments; ‚Äúgoes beyond summarizing methodological
guidance from existing works... [and] provides insight speciÔ¨Åcally
for software engineering‚Äù. It is limited by excluding tertiary studies
and providing relatively brief descriptions of each type of review
to respect the venue‚Äôs page limit. Some reviewers of this paper ar-
gued that it did not Ô¨Åt the call-for-papers. This criticism underlines
the tendency in SE to abandon a meta-scientiÔ¨Åc discourse. We envi-
sion a future where a lively meta-scientiÔ¨Åc discourse is an essential
part of our research community.

4 CONCLUSION
Systematic review is often conÔ¨Çated with meta-analysis. We rec-
ommend using systematic review as the category of secondary re-
search in which primary studies are selected according to a sys-
tematic, replicable process; that is, the opposite of an ad hoc review.
Meta-analysis, case survey, meta-synthesis, critical review, scoping
review, and rapid review are all types of systematic review.

SE needs more meta-analysis, meta-synthesis, and case surveys.
These methods synthesize bodies of related work into actionable
recommendations for practitioners and are therefore crucial for

Paving the Way for Mature Secondary Research: The Seven Types of Literature Review

ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore

evidence-based practice. Scoping reviews, in contrast, rarely in-
clude enough synthesis to support evidence-based practice. We
consider scoping reviews a prelude to a more ambitious project.
If published at all, scoping reviews belong with other works-in-
progress in posters, workshops or short papers. Rapid reviews, mean-
while, are a valuable tool but SE phenomena rarely manifest the
conditions under which rapid reviews are justiÔ¨Åed. Finally, criti-
cal reviews have diÔ¨Äerent best practices and reviewers should stop
misapplying meta-analysis criteria to critical reviews.

We hope this paper helps researchers distinguish between dif-
ferent types of literature reviews‚Äîpaving the way for more mature
secondary research in software engineering.

REFERENCES
[1] Hilary Arksey and Lisa O‚ÄôMalley. 2005. Scoping studies: towards a methodolog-
ical framework. International Journal of Social Research Methodology 8, 1 (2005),
19‚Äì32.

[2] Sohaib Shahid Bajwa, Xiaofeng Wang, Anh Nguyen Duc, and Pekka Abrahams-
son. 2017. ‚ÄúFailures‚Äù to be celebrated: an analysis of major pivots of software
startups. Empirical Software Engineering 22, 5 (2017), 2373‚Äì2408.

[3] Sebastian Baltes and Paul Ralph. 2022. Sampling in software engineering re-
search: A critical review and guidelines. Empirical Software Engineering 27
(2022), article number 94.

[4] Michael Borenstein, Larry V Hedges, Julian PT Higgins, and Hannah R Rothstein.
2021. Introduction to Meta-Analysis. John Wiley & Sons, Chichester, GBR.
[5] RJ Bullock and Mark E Tubbs. 1987. The case meta-analysis method for OD.

Research in Organizational Change and Development 1 (1987), 171‚Äì228.

[6] Iain Chalmers. 1993. The Cochrane collaboration: preparing, maintaining, and
disseminating systematic reviews of the eÔ¨Äects of health care. Annals of the New
York Academy of Sciences 703, 1 (1993), 156‚Äì165.

[7] Daniela S Cruzes and Tore Dyb√•. 2011. Research synthesis in software engineer-
ing: A tertiary study. Information and Software Technology 53, 5 (2011), 440‚Äì455.
[8] Rebecca Ganann, Donna Ciliska, and Helen Thomas. 2010. Expediting system-
atic reviews: methods and implications of rapid reviews. Implementation Science
5, 1 (2010), 1‚Äì10.

[9] Gene V Glass. 1976. Primary, secondary, and meta-analysis of research. Educa-

tional Researcher 5, 10 (1976), 3‚Äì8.

[10] David Gough, Sandy Oliver, and James Thomas. 2017. An Introduction to System-

atic Reviews. Sage, London, GBR.

[11] Liliana Guzm√°n, Constanza Lampasona, Carolyn Seaman, and Dieter Rom-
bach. 2014. Survey on Research Synthesis in Software Engineering. In Pro-
ceedings of the 18th International Conference on Evaluation and Assessment in
Software Engineering (London, England, United Kingdom) (EASE ‚Äô14). Asso-
ciation for Computing Machinery, New York, NY, USA, Article 2, 10 pages.
https://doi.org/10.1145/2601248.2601273

[12] Jo E Hannay, Tore Dyb√•, Erik Arisholm, and Dag IK Sj√∏berg. 2009. The eÔ¨Äec-
tiveness of pair programming: A meta-analysis. Information and Software Tech-
nology 51, 7 (2009), 1110‚Äì1122.

[13] Xin Huang, He Zhang, Xin Zhou, Muhammad Ali Babar, and Song Yang. 2018.
Synthesizing Qualitative Research in Software Engineering: A Critical Review.
In Proceedings of the 40th International Conference on Software Engineering
(Gothenburg, Sweden) (ICSE ‚Äô18). Association for Computing Machinery, New
York, NY, USA, 1207‚Äì1218. https://doi.org/10.1145/3180155.3180235

[14] Louise A Jensen and Marion N Allen. 1996. Meta-synthesis of qualitative Ô¨Ånd-

ings. Qualitative Health Research 6, 4 (1996), 553‚Äì560.

[15] Marlen C Jurisch, Petra Wolf, and Helmut Krcmar. 2013. Using the case survey
method for synthesizing case study evidence in information systems research.
In Proceedings of the 19th Americas Conference on Information Systems (AMCIS).
Association for Information Systems, Chicago, USA. Article 5.

[16] Barbara A. Kitchenham, Tore Dyba, and Magne Jorgensen. 2004. Evidence-Based
Software Engineering. In Proceedings of the 26th International Conference on Soft-
ware Engineering (ICSE ‚Äô04). IEEE Computer Society, USA, 273‚Äì281.

[17] Rikard Larsson. 1993. Case survey methodology: Quantitative analysis of pat-
terns across case studies. Academy of Management Journal 36, 6 (1993), 1515‚Äì
1546.

[18] William A Lucas. 1974.

rience.
https://www.rand.org/pubs/reports/R1515.html

Technical Report 1515. Rand.

The case survey method: Aggregating case expe-
Retrieved Sept. 4, 2022 from

[19] Jorge Melegati and Xiaofeng Wang. 2020. Case Survey Studies in Software En-
gineering Research. In Proceedings of the 14th ACM / IEEE International Sym-
posium on Empirical Software Engineering and Measurement (ESEM) (Bari, Italy)
(ESEM ‚Äô20). Association for Computing Machinery, New York, NY, USA, Article
6, 12 pages. https://doi.org/10.1145/3382494.3410683

[20] Rahul Mohanani, IÔ¨Çaah Salman, Burak Turhan, Pilar Rodr√≠guez, and Paul Ralph.
2018. Cognitive biases in software engineering: a systematic mapping study.
IEEE Transactions on Software Engineering 46, 12 (2018), 1318‚Äì1339.

[21] Zachary Munn, Micah DJ Peters, Cindy Stern, Catalin Tufanaru, Alexa
McArthur, and Edoardo Aromataris. 2018. Systematic review or scoping review?
Guidance for authors when choosing between a systematic or scoping review
approach. BMC Medical Research Methodology 18, 1 (2018), 1‚Äì7.

[22] Kai Petersen. 2020. Guidelines for Case Survey Research in Software Engi-
neering. In Contemporary Empirical Methods in Software Engineering, Michael
Felderer and Guilherme Horta Travassos (Eds.). Springer International Publish-
ing, Cham, 63‚Äì92. https://doi.org/10.1007/978-3-030-32489-6_3

[23] Kai Petersen, Deepika Badampudi, Syed Muhammad Ali Shah, Krzysztof Wnuk,
Tony Gorschek, EÔ¨Å Papatheocharous, Jakob Axelsson, Severine Sentilles, Ivica
Crnkovic, and Antonio Cicchetti. 2017. Choosing component origins for soft-
ware intensive systems: In-house, COTS, OSS or outsourcing?‚ÄîA case survey.
IEEE Transactions on Software Engineering 44, 3 (2017), 237‚Äì261.

[24] Kai Petersen, Robert Feldt, Shahid Mujtaba, and Michael Mattsson. 2008. System-
atic Mapping Studies in Software Engineering. In Proceedings of the 12th Inter-
national Conference on Evaluation and Assessment in Software Engineering (Italy)
(EASE‚Äô08). BCS Learning & Development Ltd., Swindon, GBR, 68‚Äì77.

[25] Edward Purssell and Niall McCrae. 2020. How to Perform a Systematic Literature
Review: A Guide for Healthcare Researchers, Practitioners and Students. Springer
International Publishing, Cham. https://doi.org/10.1007/978-3-030-49672-2_2
[26] Yahya RaÔ¨Åque and Vojislav B Mi≈°iƒá. 2012. The eÔ¨Äects of test-driven develop-
ment on external quality and productivity: A meta-analysis. IEEE Transactions
on Software Engineering 39, 6 (2012), 835‚Äì856.

[27] Paul Ralph. 2018. Toward methodological guidelines for process theories and
taxonomies in software engineering. IEEE Transactions on Software Engineering
45, 7 (2018), 712‚Äì735.

[28] Paul Ralph, Sebastian Baltes, Domenico Bianculli, Yvonne Dittrich, Michael
Felderer, Robert Feldt, Antonio Filieri, Carlo Alberto Furia, Daniel Graziotin, Pin-
jia He, et al. 2020. ACM SIGSOFT Empirical Standards. arXiv: 2010.03525 [cs.SE]
(2020).

[29] Martin Shepperd, David Bowes, and Tracy Hall. 2014. Researcher bias: The use
of machine learning in software defect prediction. IEEE Transactions on Software
Engineering 40, 6 (2014), 603‚Äì616.

[30] Luciana B Sollaci and Mauricio G Pereira. 2004. The introduction, methods,
results, and discussion (IMRAD) structure: a Ô¨Åfty-year survey. Journal of the
Medical Library Association 92, 3 (2004), 364.

[31] Klaas-Jan Stol, Paul Ralph, and Brian Fitzgerald. 2016. Grounded Theory in
Software Engineering Research: A Critical Review and Guidelines. In Proceed-
ings of the 38th International Conference on Software Engineering (Austin, Texas)
(ICSE ‚Äô16). Association for Computing Machinery, New York, NY, USA, 120‚Äì131.
https://doi.org/10.1145/2884781.2884833

[32] Robert K. Yin and Karen A. Heald. 1975. Using the Case Survey Method to
Analyze Policy Studies. Administrative Science Quarterly 20, 3 (1975), 371‚Äì381.
http://www.jstor.org/stable/2391997

[33] He Zhang, Xin Huang, Xin Zhou, Huang Huang, and Muhammad Ali Babar. 2019.
Ethnographic Research in Software Engineering: A Critical Review and Check-
list. In Proceedings of the 2019 27th ACM Joint Meeting on European Software En-
gineering Conference and Symposium on the Foundations of Software Engineering
(Tallinn, Estonia) (ESEC/FSE 2019). Association for Computing Machinery, New
York, NY, USA, 659‚Äì670. https://doi.org/10.1145/3338906.3338976

