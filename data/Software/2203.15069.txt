©2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.

Leveraging Tactile Sensors for Low Latency Embedded
Smart Hands for Prosthetic and Robotic Applications

Xiaying Wang, Student Member, IEEE, Fabian Geiger, Vlad Niculescu, Michele Magno, Senior Member, IEEE,
Luca Benini, Fellow, IEEE

2
2
0
2

r
a

M
8
2

]

O
R
.
s
c
[

1
v
9
6
0
5
1
.
3
0
2
2
:
v
i
X
r
a

Abstract—Tactile sensing is a crucial perception mode for
robots and human amputees in need of controlling a prosthetic
device. Today robotic and prosthetic systems are still missing
the important feature of accurate tactile sensing. This lack is
mainly due to the fact that the existing tactile technologies have
limited spatial and temporal resolution and are either expensive
or not scalable. In this paper, we present the design and the
implementation of a hardware-software embedded system called
SmartHand. It is speciﬁcally designed to enable the acquisition
and the real-time processing of high-resolution tactile informa-
tion from a hand-shaped multi-sensor array for prosthetic and
robotic applications. During data collection, our system can
deliver a high throughput of 100 frames per second, which
is 13.7× higher than previous related work. This has allowed
the collection of a new tactile dataset consisting of 340,000
frames while interacting with 16 objects from everyday life
during ﬁve different sessions. Together with the empty hand, the
dataset presents a total of 17 classes. We propose a compact yet
accurate convolutional neural network that requires one order of
magnitude less memory and 15.6× fewer computations compared
to related work without degrading classiﬁcation accuracy. The
top-1 and top-3 cross-validation accuracies on the collected
dataset are respectively 98.86% and 99.83%. We further analyze
the inter-session variability and obtain the best top-3 leave-
one-out-validation accuracy of 77.84%. We deploy the trained
model on a high-performance ARM Cortex-M7 microcontroller
achieving an inference time of only 100 ms minimizing the
response latency. The overall measured power consumption is
505 mW. Finally, we fabricate a new control sensor and perform
additional experiments to provide analyses on sensor degradation
and slip detection. This work is a step forward in giving robotic
and prosthetic devices a sense of touch by demonstrating the
practicality of a smart embedded system that utilizes a scalable
tactile sensor with embedded tiny machine learning.

Index Terms—Embedded systems, convolutional neural net-
works, edge computing, tactile sensors, prosthetic hand, biomed-
ical applications

I. INTRODUCTION

Recent advances in electronics and microelectromechanical
system (MEMS) manufacturing have allowed the sophistica-
tion of sensors and robotic devices to grow by leaps and
bounds [1]–[3]. Robotic hands and prosthetic devices provide
robots with humans’ ability to interact with the surroundings
and restore lost abilities for amputees [3]–[6]. The preeminent
modality of choice for robotic devices that need to interact
with the environment is vision [7]–[9]. Optical sensors are

Manuscript received November 07, 2019; revised January 16, 2020; ac-

cepted February 10, 2020.

All authors are afﬁliated with the Department of Information Technology
and Electrical Engineering, ETH Zürich, Switzerland (Corresponding e-mail:
xiaywang@iis.ee.ethz.ch).

This project was supported by the Swiss Data Science Center PhD Fellow-

ship under grant ID P18-04.

performing well, are inexpensive, and mature computer vision
algorithms exist for every level of processing power. On the
other hand, there are tasks in which optical information is
insufﬁcient or leads to disproportionately complex control
strategies. One such task is the manipulation of arbitrary
objects with an articulated end-effector. Vision is required to
ﬁnd correctly the object and the position of the end-effector,
but once contact is made, the robot cannot tell if the object is
fragile or robust, if the end-effector has a good grip or if the
object will slip from its grasp. For this reason, a recent trend
is to develop non-visual technologies and methods to provide
a robot with those capabilities.

In nature, organisms have evolved a useful modality for
manipulating their surroundings: the touch. Science is often
inspired by nature, so in order to improve the ability of robots
and prosthetic devices to manipulate objects in the environ-
ment, tactile sensors have recently gained considerable atten-
tion. Even though much research has been conducted [10]–
[14], the task of developing a tactile sensor with a resolution
comparable to the human hand, with similar robustness as
well as ﬂexibility, remains an open challenge [15]. Object
grasping and manipulation are natural abilities for healthy
human beings, but
task for robotic
machines. Besides the sense of touch, humans can elaborate
sensory and perceptive inputs during object interaction. The
inertial properties of the object are immediately perceived,
and the corresponding adjustments are performed in real-time
during the manipulation. This is an essential ability that a
robotic machine has to be equipped with in order to optimally
interact with objects. In fact, the weight, the mass distribution,
and the inertial resistance are all factors to be considered
for efﬁcient and safe robotic motor planning. However, no
satisfying solution has yet been proposed due to the lack of
well-deﬁned sensing equipment that provides useful data for
estimating the inertial parameters [16].

is a very difﬁcult

it

State-of-the-art sensor solutions, from academia and in-
dustry, are typically hard to manufacture, expensive, and
lacking in temporal as well as spatial resolution [17]–[19].
The majority of the solutions mainly focus on replicating the
ﬁngers tips [3], [10], [20], [21] or have sensors covering only
a limited part of the hand palm [1]–[3], [22]–[24]. While
[25] presents a system with full hand coverage, they use a
small number of sensing elements (i.e., 16) and therefore only
prove the functionality of the system with seven objects.
Due to the high cost of the commercial solutions and the
small size of the most common solution usually covering only
a single ﬁnger, the access to tactile information for small
research groups or companies is still very limited and it is

 
 
 
 
 
 
not unleashing the potential of the research community [26].
In contrast, Sundaram and colleagues have proposed a full-
hand sensor using an affordable technology [15]. The authors
have designed and developed a scalable tactile glove (STAG)
enabling the full coverage of the human hand, providing
tactile information with high spatial resolution. The prelim-
inary work of the tactile glove includes a dataset collected
while interacting with 26 daily life objects for several minutes
each. A convolutional neural network (CNN) is subsequently
applied for classifying the objects and the empty hand, i.e., 27
classes. The top-1 and the top-3 accuracy results are around
74% and 89%, respectively. To be noted that the proposed
setup with ofﬂine sensor data processing based on several-
minute recordings introduces long latency, making it sub-
optimal for a real-world application where a prompt response
in the range of milliseconds is desired. Moreover, embedding
multi-sensors in small integrated systems is actively researched
thanks to its advantages for closed-loop control of the robotic
devices [27]. Weiner and colleagues [10] propose a ﬁngertip
system integrating several sensors that are miniaturized to ﬁt
in the very limited space of a ﬁngertip. Besides the integration
of sensors, a smart processing unit, e.g., microcontroller units
(MCUs), that can process the acquired sensor data at the edge
would enable a real-time response of the whole system. Table I
provides an in-depth comparison of the mentioned related
works in terms of hand coverage, number of sensors, detection
latency and classiﬁcation performance.

Machine learning and deep learning have been widely ap-
plied in medical ﬁeld. For instance, the authors in [28] propose
a method for detecting and eliminating the artifacts in the
photoplethysmographic measurements using artiﬁcial neural
networks (ANNs). Furthermore, [29] also takes advantage of
ANNs and presents a system that could assist the physician
in making a decision on the current state of the patient or
in predicting the future states. Recent developments in edge
processing units and tinyML [30] make it possible to bring
the data processing close to the sensors [31]. Along with
efforts in designing and deploying tiny machine learning and
deep learning models on the edge, nowadays edge devices
are becoming increasingly smart and can interpret sensor
data and translate it into actionable information in real-time,
for example gesture recognition using radar [32], [33] and
brain–machine interfaces using electroencephalograms [34]. A
drawback of low-power edge computing is that the resources
available on MCUs with milli-watt power consumption are
very limited making it impossible to deploy big and complex
models. Thus, a conscious design of tiny yet accurate models is
necessary, by taking into consideration the memory availability
and the computational capability of the underlying processor.
In this work, we propose a step towards equipping robotic
and prosthetic devices with a smart sense of touch by pre-
senting a smart embedded system, called Smarthand, capable
of reading and processing high-resolution tactile information
in real-time. A preliminary version of the Smarthand has
been presented in [35]. This paper signiﬁcantly extend the
previous work including a more extensive methodology and
new technical contributions. We present for the ﬁrst time a
novel read-out front-end to improve the acquisition of the

TABLE I: Comparison to related works. N/A: Not Applicable.
N/S: Not Speciﬁed.

2

[1]

[24]

[15]

[22]

[23]

[25] Ours

ﬁnger

ﬁnger tips
and palm

whole
hand

palm palm

whole
hand

whole
hand

96

150

yes

N/S

N/S

yes

64

548

64

15

N/A

N/S N/A N/A

yes

no

27 N/A

yes

4

16

40

yes

7

548

100

yes

17

90% N/A 84% 96%

99%

no N/A

no

N/S

yes

no

N/A

N/A

N/A

12-bit

N/S 10-bit

N/S

N/S

N/S

12-bit

Hand
coverage
# of sensing
elements
Latency
[ms]
Classiﬁcation
algorithm
# of classes
Classiﬁcation
accuracy
On-board
processing
Sampling
resolution

data in terms of energy and datarate. The experimental results
present more accurate and extensive evaluations on latency,
power, and accuracy of the proposed solution. Furthermore,
we fabricate a new control sensor and provide quantitative and
qualitative analyses on sensor degradation and slip evaluations.

The main contributions of this paper are as follows:
• We extend the SmartHand [35] with a novel front-end
able to acquire tactile and hand movement data with
a high throughput of 100 frames per second, which is
13.7× higher than [15], and a power consumption of
52 mW.

• We present a more extensive evaluation of the novel
CNN algorithm to classify 16 objects and the empty
hand considering the inter-session variability. We deploy
the model on a ARM Cortex-M7 MCU reaching a low
inference latency of 100 ms and a power consumption of
430 mW.

• We fabricate a new square sensor and perform controlled
experiments to assess quantitatively and qualitatively the
sensor degradation between sessions and slip behaviours.
Experimental results show that the average response of all
contact-frames diminishes by up to 40% for the sensor
glove and almost 30% for the control sensor.

• Experimental power measurements demonstrate that our
proposed solution consumes 505 mW in active mode and
185 µW in standby mode. Assuming a duty cycle of 10%
and a battery capacity of 1 Wh, our proposed SmartHand
can deliver an operating time of up to 20 hours per day.

We open-source release the dataset and the codes1.

II. BACKGROUND

This section provides related background knowledge for

understanding this paper.

A. Tactile Sensing

Tactile sensing requires that different tactile cues are trans-
duced to an electrically detectable signal. The force sensitivity
of a conductive polymer composite (CPC) [36] acts as a force

1https://iis.ee.ethz.ch/~datasets/smarthand/

sensitive resistor (FSR) when it is sandwiched between two
electrodes. The resistance of such a sensor sandwich consists
of two parts: a contact resistance and a polymer resistance. The
contact resistance depends on the type of electrodes used, their
shape and how they are bonded to the polymer. It is ideally
constant for a certain setup and does not drift or change over
time. Whereas, the polymer resistance transduces variations in
applied force to variations in resistance. The resistance value
and its behavior depend on the concentration of conductive
particles that is added to the fabrication of the CPC. The
resistance decreases with incremental applied stress if the
concentration is below the percolation threshold, while the
opposite is true above the threshold. An overview over the
physical properties of conductive polymer composites can be
found in [36].

B. Related Dataset

Sundaram and colleagues [15] have proposed for the ﬁrst
time a tactile glove with high spatial resolution covering the
full hand, called STAG, based on the concept of FSR using
a CPC. A total of 135,000 frames of tactile data has been
collected together with synchronized video footage in three
recording sessions, during each of which the sensor glove
is worn throughout the entire session. This dataset contains
several minutes of interaction with 26 different objects from
everyday life. Tactile frames are collected at a rate of about
7.3 Hz. All objects are manipulated in a repeatable way, mostly
pushing the sensor from the top onto the object lying on a
table. The frames corresponding to the actual touch of the
object are chosen by comparing the values with a threshold
found from an empty-hand dataset. This threshold value is
found for each taxel to account for any internal stress caused
by the sensor conformation. The authors have selected N
frames from the several-minute recordings by either random
selection or clustering to construct the input samples to a
neural network for object classiﬁcation. The dataset is publicly
available2 and we name it MIT-STAG dataset in this paper for
clarity.

III. METHODS
This section explains the proposed system architecture and
its operational modalities in Sec. III-A and Sec. III-B, re-
spectively, the recorded dataset in Sec. III-C, the proposed
neural network,
the validation methodologies, and its em-
bedded implementation in Sec. III-D. Finally, the additional
evaluations of the tactile sensor are explained in Sec. III-E.
The experimental results are reported and discussed in Sec. IV.

A. System Architecture

Fig. 1 depicts the block diagram and the architecture of
the designed Smarthand. It consists of a low cost resistive
tactile sensor based on a CPC as in [15], an analog front-end
(AFE) to read the tactile data, an inertial measurement unit
(IMU) to gain additional movement information, and an MCU
for system control and on-board signal processing. Fig. 2 and
Fig. 3 show the full system assembled and worn.

2http://stag.csail.mit.edu/

3

Fig. 1: The block diagram of the system.

Fig. 2: Fully assembled system from the top. The red PCB
on the back of the glove is the IMU. The readout circuit is
plugged onto the discovery board like a shield.

1) Tactile sensor: Following the fabrication procedures
described in [15], a tactile sensor with high spatial resolution
has been produced in our laboratory. The sensor is based on
the concept of FSR, as explained in Sec. II-A. It transduces
changes in applied force or pressure to changes in resistance
that can be read with a dedicated analog front-end. It consists
of a CPC, called Velostat, sandwiched between two orthog-
onal sets of electrodes, as shown in Fig. 4. The conductive
particles in Velostat are carbon black and their concentration
is below the percolation threshold, so its resistance decreases
in response to increasing normal forces. The sensor electrodes
are smooth stainless steel threads with a thickness of 0.25mm
and there are 32 electrodes running across each of the two
sides of the force sensitive ﬁlm forming a grid.

The function of the sensor does not depend on its shape,
thus it can be fabricated in arbitrary forms and sizes. To obtain
comparable results and capture useful data during object inter-
action, the sensor has been fabricated as similarly as possible
to [15]. Fig. 5 shows the ﬁnished sensor laminate before being
attached to a thin cotton glove. Because the active sensor area
is shaped like a hand, there are 548 physical crossings of
electrodes. However, to preserve the spatial relation between
taxels, or sensor points, and make it easier to work with the
sensor data, all 1024 available taxels are read. By arranging
the read values in a 32×32 matrix, an intuitive and natural
representation of the sensor can be constructed, as shown in
Fig. 6, and will be referred to as tactile frame, or simply frame.

4

Fig. 3: Fully assembled system from the bottom. The sensor
laminate is glued to the white cotton glove.

Fig. 5: Fabricated sensor before being glued to a glove.

Fig. 4: Sensor composition. Adhesive and low-density
polyethylene (LDPE) ﬁlm are required to bond the electrodes
to the force sensitive ﬁlm and protect it, respectively [15].

2) Readout circuit: Abstracting from its physical structure,
the tactile sensor can be drawn as a simple resistor network
with variable resistances, formed by the cross-talks between
the two orthogonal sets of 32 electrodes. Such an abstract
representation is shown in Fig. 7 framed with dashed line.
Each cross-talk can be seen as a simple resistor with variable
resistances depending on the pressure applied to the sensor
glove. The biggest challenge in accurately measuring each
resistor in this matrix conﬁguration is posed by the extensive
cross-talk between electrodes.

There are two main approaches for handling cross-talk. The
ﬁrst one makes sure that there is only one current path when
measuring the resistance between two electrodes, which is
through that same resistor [37]. Thus, it can also be described
isolation scheme. The second approach takes
as electrical
multiple measurements, including cross-talk, and ﬁnds the true
resistances by solving a set of linear equations [38]. The
advantage of this approach is that a minimum number of
components is required to realize the readout circuit. However,
is computationally more
it
expensive and slower than the ﬁrst approach. Since any MCU
should be able to efﬁciently use the proposed system, we have
decided to implement a variation of the electrical isolation
scheme for a fast readout.

is an iterative procedure so it

We base our implementation on the readout circuit used
in [15] and optimize it for low-power, speed, and accuracy by
using a more powerful MCU featuring a higher precision 12-
bit analog-to-digital converter (ADC). The working principle
is that one row is grounded, while all other rows and columns
are pulled up to a reference voltage. This ideally prevents

Fig. 6: Representation of the sensor values as a 32x32 matrix,
referred to as frame or tactile frame.

cross-talk between rows, and gives the opportunity to read
out all columns of the active row at the same time. The active
row and all columns form an inverting ampliﬁer conﬁguration,
with the FSR of the Velostat
in the forward path and a
simple resistor in the feedback path. The positive input of
the operational ampliﬁer is directly connected to the reference
voltage, resulting in the following equation for the output
voltage:

Vout = Vout ·

RF B + RF SR
RF SR

(1)

A block diagram of the sensor, its readout circuit, and the
inverting ampliﬁer conﬁguration between the active row and
a single column is shown in Fig. 7.

3) Movement sensor: We additionally integrate an IMU
for acquiring accelerometer and gyroscope data to collect
hand movements during object interaction. The used IMU
is MPU-9250 with a sampling frequency of 100 Hz and
a resolution of 16-bit for both the accelerometer and the
gyroscope.

4) Edge processing unit: The embedded system requires
an MCU with the ability to drive and read the tactile sensor,
as well as directly run neural network inferences with the
acquired data and take action based on the inference, e.g.,
actuating the motors of the robotic arm. For a fast development
of the prototype, we have chosen the STM32F769NI discovery
board, featuring an ARM Cortex-M7 core. With its 2 MB of
Flash memory, 532 kB of RAM, and a maximum clock speed
of 216 MHz, it is one of the most highly-performing MCU
in the low-power ARM Cortex-M family. It comes with the
software tool STM32CubeMX, from STMicroelectronics, em-
ployed for generating initialization code, compiling, and ﬂash-
ing. X-CUBE-AI is an expansion package for STM32CubeMx

Adhesive (0.13 mm)Force-sensitive film (0.1 mm)Conductive threadsStretchable LDPE film (~ 0.013 mm)5

Fig. 7: Block diagram of the sensor with its readout circuit.

that allows fast deployment of neural networks on STM32
MCUs.

B. Operational Modalities

The ﬁrmware of the system is implemented such that
three operational modalities are available for different use-
case scenarios. A basic timer peripheral on the MCU is used
to clock the applications.

a) Data collection: This modality is used for acquiring
a complete dataset. The system waits for a ‘start’ command,
given either by pressing a button on the discovery board
or sending the character ‘r’ via the universal asynchronous
receiver-transmitter (UART) protocol to the MCU. If either
of these two events is detected, the base timer is triggered
at a rate of 100 Hz, and the tactile along with IMU data is
collected. The external synchronous dynamic random-access
memory (SDRAM) on the discovery board was utilized as
intermediate storage for tactile data, while the IMU data is
sent directly after its acquisition. After a conﬁgurable number
of frames has been collected, the data acquisition stops and
immediately sends the content of the SDRAM to a connected
computer, and the system returns to the idle state waiting for
the next ‘start’ command. The SDRAM can store a maximum
of 4096 tactile frames, limiting one single interaction to a
maximum of about 40 seconds with a data collection rate of
100Hz. The acquisition frequency can be reduced if a longer
recording is desired.

b) Real-time data visualization: A graphical user inter-
face (GUI) designed using Python is implemented and can be
used to visualize the tactile sensor data on a computer screen
directly. Once the ‘start’ command is received, the base timer
is activated, and the tactile frames are collected at 10 Hz and
directly sent to the connected computer via UART. To stop
the visualization, either the button on the discovery board has
to be pressed again, or the character ‘p’ needs to be sent to
the system. The GUI displays each frame as a 32x32 matrix

Fig. 8: Snapshot from a demo video in full-system modality.

and provides two buttons, labeled ‘Run’ and ‘Pause’, which
respectively send ‘r’ or ‘p’ to the system, giving the option to
control the data visualization from the same interface.

c) Smarthand system: This modality leverages the full
system by not only reading tactile data but also processing
it real-time on the MCU using a deployed neural network.
The data acquisition starts after pressing the button on the
discovery board with the base timer conﬁgured to 8 Hz. The
sensor data is read and processed by the neural network on
the MCU. The system clock is conﬁgured to 216MHz to
maximize the inference speed. To evaluate the performance
of the SmartHand system, a second Python GUI is developed,
which displays the result of each network inference and its
corresponding input frame. To simultaneously process the
frame on-board and visualize it in real-time, a direct memory
access (DMA) controller is used for sensor data transmission.
It is to note that in this case, the connection to the computer
serves merely for the veriﬁcation purpose of a demo, the
system can fully function independent of any processing
engine other than the on-board MCU, and the output of the
neural network inference can be used in real-time for any
robotic or prosthetic control. Fig. 8 displays a snapshot of
a demo video in this application modality. The SmartHand is
worn like a glove, and the tactile data during the interaction
with a mug is acquired and classiﬁed immediately with the on-
board embedded CNN. The classiﬁcation output is displayed
in real-time on the screen.

C. ETHZ-STAG Dataset

Following the analogous procedures of fabrication, we cre-
ate our own STAG at ETH Zurich. A large dataset is collected
with the data collection modality described in Sec. III-B in
ﬁve recording sessions. Instead of synchronous video footage,
synchronous accelerometer and gyroscope data are collected
from an IMU. We select 16 objects that are easily found in our
daily life and are as similar as possible to the ones used in [15].
During every recording session, each object is manipulated for
40 seconds while continuously collecting tactile and IMU data
at a rate of 100 Hz. The manipulations are done in the same
manner as in [15] to increase repeatability between recording

6

Algorithm 1: Neural network training and validation
on our ETHZ-STAG dataset.
Data: Sensor data X
Result: Trained models
Nf olds = 7 ;
Nsessions = 5 ;
if Random split then

// 7 folds for CV
// 5 recording sessions

Shufﬂe X;
Split X into Nf olds training and validation sets;
for f from 1 to Nf olds do

Xtrain,f = training data from fold f ;
Xval,f = validation data from fold f ;
Initialize model mf ;
Train mf on Xtrain,f ;
Validate mf on Xval,f ;

end

end
if Inter-session then

for s from 1 to Nsessions do

Xval = data from session s for validation;
Xtrain = data from the remaining sessions for
training;
Initialize model ms;
Train ms on Xtrain,s;
Validate ms on Xval,s;

end

end

3) Training and validation: The model is trained using
PyTorch on GeForce GTX 1080 Ti GPUs with CUDA frame-
work. Two validation methodologies are employed: a) The
full dataset is randomly split, and seven-fold cross-validation
(CV) is performed. The classiﬁcation accuracy is reported as
the average of all folds. b) The dataset is split by session to
consider the inter-session variability. We perform the leave-
one-session-out CV, i.e., one session is kept as validation
set, while the remaining four are used for training. This
procedure is repeated ﬁve times for each of the ﬁve sessions.
The accuracy is reported as the average of all sessions. In
both cases, the model is trained for 60 epochs with cross-
entropy loss and Adam optimizer with a batch size of 32 and
a gradually decreasing learning rate as in [15] starting from
0.001. A pseudocode is shown in Algorithm 1 and the source
codes are open-source released.

4) Emdedded implementation: The X-CUBE-AI expansion
package for STM32CubeMx is used for deploying the trained
CNN on the MCU. For the embedded deployment, the model
is trained by leveraging the full dataset. It is then saved in
ONNX format and imported in STM32CubeMx. The tool
generates an application template code, to which the user code
to read the data can be added and the inference performed.

E. Sensor Evaluations

With the purpose of evaluating the sensor properties, we
perform two additional experiments. We fabricated a new
control sensor as a square 16×16 sensor array, as shown in

Fig. 9: Pictures of the 16 objects and the empty hand used for
the ETHZ-STAG dataset recording.

sessions, i.e., the objects are manipulated by mostly pushing
the sensor from the top onto the object lying on a table. The
valid frames of actual object contact are marked by comparing
to a threshold value for each taxel, as in [15]. We construct
these thresholds by ﬁnding the highest possible value that a
taxel can assume using more than 20,000 additional empty-
hand-frames of different hand poses without object contact.
Finally, a total of 340,000 frames are available for 17 classes
as shown in Fig. 9.

D. Neural Network

This section describes the model design, the training and
the evaluation of the neural network used for processing the
sensor data, and its embedded deployment on the MCU for
on-board processing.

1) Convolutional neural network design: Because of the
inherently spatial information present in the sensor, a CNN
architecture is chosen to process the tactile data. As in [15],
we select a model architecture based on ResNet-18 [39], and
redesign it by taking into consideration hardware resource
constraints. Fig. 10 depicts the proposed model architecture.
It uses multiple stages of convolutions, including two residual
blocks, to extract features from a single input frame. Finally, a
fully connected layer is used for the classiﬁcation. Compared
to [15], we restrict the number of input frames to a single
one directly reducing the latency. Another adaptation is a
four-fold reduction of convolution ﬁlters, i.e., 16 in the ﬁrst
convolutional
layers and 32 in the second residual block.
This decreases signiﬁcantly the inference speed thanks to the
reduced model complexity.

2) Sensor fusion: To include the IMU data, a simple multi-
layer perceptron (MLP) with one hidden layer consisting of
30 hidden units is added to extract the feature representations.
The output layer of the MLP consists of 3 neurons and is
subsequently concatenated to the features of the tactile data
before the ﬁnal fully connected layer for the classiﬁcation.
This method is motivated by literature [27], [40], [41], in
which the features of the individual sensors are ﬁrst extracted
and then concatenated. This additional branch featuring IMU
data can be added or removed depending on the application.

7

Fig. 10: The proposed CNN based on ResNet-18. The feature maps size is HxWxC, with H and W being respectively the
height and the width of the images and C the number of channels. k is the kernel size, s is the stride, and p is the dropout
rate. The sensor fusion can be added by concatenating the extracted IMU features at the Flatten layer. The ﬁnal model does
not include the IMU data as explained in Sec. IV-B.

Fig. 11, and used it in the following experiments to investigate
the physical properties of the sensor in a more controlled and
repeatable manner:

1) Sensor degradation: The objective of this experiment
was to determine, whether sensor degradation is caused by
excessively moving, stressing and folding the sensor laminate
— as is the case for the sensor glove — or if the degradation
simply happens over time, even for a stationary sensor. We
randomly choose an object (lotion) and place it on ﬁve
different locations of the sensor area, four times within one
week. This time frame is comparable to the last four recording
sessions with the sensor glove, enabling the comparison of
a strongly used sensor and a very lightly used sensor over
roughly the same time period.

2) Slip evaluations: An extremely useful ability for robotic
and prosthetic application is slip detection. Tactile sensors have
demonstrated the ability to detect object slip [42], and even slip
onset before the gripped object starts to move. Considering the
importance of this property, we examine the sensor’s reaction
to slip or object motion. In this experiment, the tactile sensor
was lying ﬂat on a table and an object was pulled across it at
constant speed, to simulate a slipping event. Fig. 11 illustrates
the experiment setup.

The fabrication of the new sensor is justiﬁed by the fol-
lowing reasons: a) The purpose of the Smarthand sensor is to
collect the dataset. After the dataset collection, the sensor’s
properties have been affected by the usage, hence, it is not
suitable for the additional evaluations. b) By fabricating a
new sensor in the regular shape of a square,
little strain
is introduced during the fabrication preserving the sensor
integrity yielding a more controlled experimental setup.

IV. RESULTS
In this section, we present the experimental results of object
classiﬁcation using our proposed system. Sec. IV-A presents
the performance of our CNN, its comparison to the previous
work on the MIT-STAG dataset [15], and the classiﬁcation
outcome on our dataset using the validation methodologies
explained in Sec. III-D. We discuss the sensor fusion approach
in Sec. IV-B and report the outcome of the embedded im-
plementation in terms of multiply-and-accumulate operations

Fig. 11: Square sensor used as control sensor and a simple
setup for simulating a slip event for a screwdriver.

]

%

[

y
c
a
r
u
c
c
A

100

80

60

40

20

0

0

1

2

Cit. [15]
Ours

7

8

9

4

3
6
Number of frames

5

Fig. 12: Comparison of the top-1 classiﬁcation accuracy of
our proposed model with the original network on MIT-STAG
dataset [15] using different numbers of input frames.

(MACCs), memory usage, and inference time on the selected
MCU in Sec. IV-C. The evaluations on the full system perfor-
mance is discussed in Sec. IV-D, while the additional sensor
evaluations are reported in Sec. IV-E. Finally, we discuss
future works in Sec. IV-F.

A. Neural network performance

First of all, we compare the accuracy of our proposed
model with the original network using the MIT-STAG dataset
comprising of 26 objects [15]. The authors randomly select

1
1
.
0
±
6
8
.
8
9

3
0
.
0
±
3
8
.
9
9

]

%

[

y
c
a
r
u
c
c
A

100

80

60

40

20

0

]

%

[

y
c
a
r
u
c
c
A

100

80

60

40

20

0

±
5
7
.
7
3

4
0
.
0
1

±
1
4
.
3
6

9
9
.
1
1

Top-1 Top-3

Top-1 Top-3

(a) Average 7-fold random split.

(b) Average inter-session.

Fig. 13: Classiﬁcation accuracy on ETHZ-STAG dataset.

Train loss
Train acc.

Val loss
Val. acc.

]

%

[

y
c
a
r
u
c
c
A

100

50

0

1.5

1

s
s
o
L

0.5

0

0
0

20
20

40
40

60
60

Fig. 14: Average CV learning curves on ETHZ-STAG dataset.

N frames from each recording of several minutes as the input
to the network. We reproduce their results with their CNN
and compare them with our proposed model trained on the
same dataset with the same training and validation methodol-
ogy [15]. As shown in Fig. 12, despite the signiﬁcantly smaller
size of our model, the accuracy is comparable, especially for
N = 8, where the accuracy drop is only 0.21%. However, N =
1 would introduce less computational burden for a real-time
application scenario yielding lower latency. Again, our model
performs comparably to the original one, with less than 2%
accuracy drop with N = 1.

The second step is to evaluate the model performance on
our dataset collected by the STAG fabricated in-house and
integrated into our proposed system. With our dataset, we
consider only N = 1 and take all the valid frames instead
of randomly selecting subsets of valid frames from each
recording. This reduces the several-minute latency of previous
work and enables the real-time response of the entire system.
Fig. 13a shows the results for the seven-fold cross-validation
with random splitting on the whole dataset without considering
inter-session variability. The top-1 and top-3 accuracy values,
i.e., the correct class is the one with the highest predicted
probability, and the correct class is among the ones with
the three highest probabilities, are respectively 98.86% and
99.83%. Fig. 14 depicts the learning curves averaged over
the 7 folds. We observe that both training and validation
accuracy and loss converge to a plateau after 60 epochs. This
demonstrates that the model is able to learn the full data
distribution.

We then proceed with inter-session validation,

the
model is trained on the data from four sessions and is validated
on the data from the remaining session. The top-1 and the
top-3 average accuracy over the ﬁve sessions are shown in
Fig. 13b. The drop in accuracy is expected due to the more

i.e.,

8

]

%

[

y
c
a
r
u
c
c
A

100

80

60

40

20

0

8
6
.
8
6

4
8
.
7
7

3
6
.
9
4

4
6
.
8
6

5
8
.
1
4

7
3
.
0
5

6
6
.
2
4

5
0
.
6
2

Top-1
Top-3

2
5
.
1
5

8
5
.
8
2

Sess. 1

Sess. 2

Sess. 3

Sess. 4

Sess. 5

(a) ETHZ-STAG dataset.

]

%

[

y
c
a
r
u
c
c
A

100

80

60

40

20

0

5
.
2
7

6
.
8
4

9
.
2
6

7
.
1
4

9
.
0
5

9
.
9
2

]

%

[

y
c
a
r
u
c
c
A

100

80

60

40

20

0

±
1
.
0
4

7
.
7

±
1
.
2
6

8
.
8

Sess. 1

Sess. 2

Sess. 3

Average

Top-1

Top-3

(b) MIT-STAG network and dataset.

Fig. 15: Inter-session accuracy. Top 1 and top 3 accuracy
when using one entire recording session for testing and the
remaining sessions for training.

challenging problem caused by the inter-session variability,
where the model does not generalize very well to the unseen
sessions. It is a common phenomenon observed in biomedical
applications [34]. In the following paragraphs, we analyze
more in depth the inter-session variability, which is likely
caused by the differences in object interaction and in the wear
of the glove between sessions. Another possible explanation
is the sensor degradation analyzed in Sec. IV-E.

In Fig. 15a, we unwrap the results from the inter-session
validation and demonstrate the accuracy on each session.
We can observe that
the performance with the leave-one-
session-out cross-validation strongly depends on the speciﬁc
session used as validation set. The best top-1 (49.63%) and
top-3 (77.84%) accuracy results are reached by using the
session 3 as validation set, while training the network on the
remaining sessions. The related work in [15] did not present
any inter-session validation results, however, for comparison,
we reproduce their network on the MIT-STAG dataset using
the same inter-session validation methodology used here. As
can be seen in Fig. 15b, similar results are obtained, proving
that the accuracy drop is not cause by our dataset and network,
but it is a result of the inter-session variability.

Next, we investigate the confusion matrices of different
sessions to gain a better understanding of the problems faced
by the machine learning model. The confusion matrix in
Fig. 16 reveals that there are some objects that are classiﬁed
well, while others are much ‘harder’ to classify. Additionally,
similar objects, such as a screwdriver and a pen, and objects
with a non-uniform shape, such as scissors and safety glasses,
are more likely to be confused with each other. This is intuitive
and can be largely attributed to the fact that recording sessions

9

Fig. 16: Confusion matrix when using recording session 3 as
test data. The rows are the true class and the columns are the
predicted ones.

Fig. 17: Confusion matrix when using recording session 1 as
test data. The rows are the true class and the columns are the
predicted ones.

are not perfectly repeatable, because of small differences in
interaction or in wearing the sensor glove. However,
this
explanation does not apply to all the sessions. Looking at
the confusion matrix of the worst performing session in
Fig. 17, some classes are classiﬁed well, but most others are
not much better than random guessing. The main source of
‘confusion’ does not seem to be similarity between objects,
and it is not obvious from the confusion matrix what the reason
for such a low accuracy is. Another possible explanation is
presented in Sec. IV-E with the additional evaluations on
sensor degradation.

too big for the selected MCU making it impossible to be
embedded onboard. Table II reports the comparison between
the two networks. Note that the inference time reported for the
original network is obtained by reducing the input convolution
ﬁlters from 64 to 54 since it is impossible to deploy the full
network due to memory constraints. Our model requires one
order of magnitude less memory, making it possible to be
deployed. The number of computations is reduced by 15.6×
in terms of MACCs, yielding more than 12× speedup for the
inference time. This effectively enables a system with real-
time response.

B. Sensor fusion

D. System performance

The IMU orientation in the form of Euler angles is directly
fed into the MLP, or only accelerometer or gyroscope data
is used. No conﬁguration has provided any improvement over
the model using only tactile data. The usage of IMU data in
this application scenario is not very meaningful, as all the
objects were mostly manipulated from the top. Hence, for
the ﬁnal evaluations presented in this paper, we exclude the
IMU sensor. In a real application scenario with robotic hands,
the information about end-effector orientation is crucial and
needs to be included at various stages of the processing loop.
Accordingly, the integration of the IMU is a step towards
a usable and practical system. We discuss future works in
Sec. IV-F that can potentially beneﬁt from the inclusion of
the IMU sensor.

C. Embedded implementation

We subsequently proceed with network deployment on
the MCU. Here the main advantage of our proposed model
becomes evident. In fact, the proposed network in [15] is

As explained in Sec. III-B, there are different applications
which have different processing throughput, according to how
their base timer was programmed. Table III summarizes the
throughput and system clock of each application. Compared
to the data collection in [15], our system has a throughput
of 100 frames per second (FPS), which is 13.7× higher (see
Table IV). The usable signal range was increased by adjusting
the analog circuit response, and additionally to increasing the
signal range, the ADC reference voltage was reduced from
5 V to 3.3 V. Thanks to the high data acquisition rate of the
analog front-end, signiﬁcantly more frames can be collected in
a shorter amount of time. In fact, ﬁve data collection sessions

TABLE II: Comparison of neural networks embedded on
STM32F769NI MCU using CUBE-AI.

Project

MACC

Flash

RAM Inference time

Cit. [15]
This work

73.3 M 2.77 MB
177 kB
4.7 M

196 kB
52 kB

>1.2 s
100 ms

were performed in this work, compared to the three sessions
of [15]. This allowed a more extensive analysis on the inter-
session variability.

Next, we present a detailed characterization of the system’s
power consumption, showing its feasibility and simultaneously
improvements. All power measure-
underscoring potential
ments were made while running the complete system applica-
tion, implementing data acquisition and neural network infer-
ence. This is the most relevant ﬁrmware for a possible real-
world application. The power characterization is performed
with a Keysight N6705C power analyzer. The subsystems are
the IMU, the MCU, the readout circuit, and the discovery
board excluding the MCU. With the exception of the discovery
board and MCU, the subsystems are fully independent physical
boards, making a distinction of the individual power domains
straightforward.

The IMU and MCU were supplied with 3.3V while the
discovery board and the readout circuit were supplied with
5V. Additionally to shorting all the negative outputs of the
power analyzer, the ground across all subsystems was shorted.
Also, the discovery board was only supplied to ensure proper
functioning of the application but will be excluded from the
analysis. The power consumption of the actively and continu-
ously running system is shown in Table V. The consumption of
the IMU (MPU-9250) agrees with the data sheet, considering
that all nine axes are active at full speed and that
is
supplied with 3.3V. The same is true for the microcontroller
(STM32F769NI), which runs at its maximum clock speed of
216MHz. Regarding the readout circuit, a power consumption
of 52mW is reasonable because it uses a low-dropout (LDO)
regulator to generate a reference voltage of 1.2V from an input
voltage of 5V.

it

We further characterize the low-power states of all com-
ponents. The system ﬁrst collects and processes data for ten
seconds and then puts all subsystems into a low-power state,
from which it can be woken at arbitrary moments by pressing
a button on the discovery board. Fig. 18, Fig. 19 and Fig. 20
display current
traces that were acquired with the power
analyzer during the low-power application. The periodicity
of the MCU and readout circuit trace in the active sections
clearly corresponds to the frequency of the processing timer.
Currently there is no possibility to completely turn off the
readout circuit. This leads to the observed behavior during the
standby phases in Fig. 20. It is caused by the fact that pins
of the MCU are left ﬂoating when it enters standby mode. In
future versions, a digital power switch can easily be added

TABLE III: Processing throughput.

Application

Data collection

GUI

Complete system

Throughput [Hz]
System clock [MHz]

100
144

10
144

8
216

TABLE IV: Comparison of our improved AFE based on [15].

Project

Throughput

Power

Resolution

Range

Cit. [15]
This work

7.3 fps
–
100 fps max. 52 mW

10 bit
12 bit

0.7 V
1.2 V

10

Fig. 18: Measured current of the IMU during a simulated low-
power application.

Fig. 19: Measured current of the MCU during a simulated
low-power application.

to the readout circuit to reduce the current consumption to a
minimum during standby. A conservative estimate is that such
a power switch will dissipate no more than 1µW.

By utilizing a special low-power accelerometer-only mode,
the IMU is not in complete standby but samples accelerometer
data at a conﬁgurable rate. Fig. 21 displays a closer look at the
standby section, revealing this sampling behavior. The output
data rate was conﬁgured to 31.25Hz which is readily recogniz-
able from the plot. A feature called wake-on-motion (WOM)
enables the IMU to send an interrupt if any accelerometer axis
exceeds a programmable threshold, which could be used in a
real application to wake the MCU if object contact is detected.
Measuring the power consumption during standby gives
the possibility of realistically predicting the lifetime of the
embedded system. A summary of relevant numbers is given
in Table V. Again, the measured current draw of IMU and
MCU was compared to the numbers given in the datasheet.
Both supply currents are slightly higher than speciﬁed. For
the inertial measurement unit this comes from the fact that
it is supplied with 3.3V, while the numbers in the datasheet
correspond to a supply voltage of 2.5V. In the case of the
microcontroller, the current draw is a bit more than twice the
speciﬁed value. Setting all pins of the MCU to analog before
entering standby mode will most likely reduce the current to
the value given in the datasheet.

We note the time when the system is in run mode and

11

To show that power optimization of the system is still
possible and advisable, a small experiment was conducted
by supplying the readout circuit with 4.5V instead of 5V.
The resulting power consumption of the readout circuit was
17mW — a 0.5V supply voltage reduction thus led to a three
times lower power consumption. The main reason for this
large inﬂuence of the supply voltage is the LDO regulator
that
is used to generate a reference voltage of 1.2V. By
decreasing the difference between input and output voltage of
the LDO regulator, the power dissipation in this component is
directly reduced. Furthermore, the power consumption of other
integrated circuits (ICs) on the printed circuit board (PCB) is
reduced.

E. Sensor evaluations

We did further evaluations using the fabricated control

sensor explained in Sec. III-E.

1) Sensor degradation: A possible cause for the large inter-
session variability, apart from the general difﬁculty of repeata-
bility, is that the response of the tactile data is in fact changing.
Indeed, analyzing the response of the sensor indicates that it is
weaker for every successive recording session. This behavior
is observed when plotting the average across all contact-frames
of each recording session and object class. An example of such
a plot is shown in Fig. 22, clearly displaying a weaker response
for later sessions. A more quantitative aspect is displayed in
Fig. 23, where we plot the average response of all available
contact-frames in each session with respect to the average
response of all contact-frames of session 1. The weakening of
the sensor response is monotonous and permanent and likely
plays a big role in the inter-session variability presented in
Fig. 15. We additionally observe that the highest inter-session
accuracy is obtained when using the session 3 as validation
set, as discussed in Sec. IV-A. Compared to the other sessions,
the data from session 3 present intermediate response. This
means that the model is trained using data with higher and
lower values and validated on intermediate values. This has
likely helped the model to generalize better on the session 3.
Finally, we compare the average response of the sensor
glove and the control sensor to see if the degradation is caused
by the extensive usage or if it is an intrinsic property of the
materials over time. Fig. 24 shows the relative mean response
compared to session 1 of the control sensor and the sensor
glove. We can see that the degradation is similar for both
sensor types. The only difference is that the response weakens

Fig. 20: Measured current of the readout circuit during a
simulated low-power application.

Fig. 21: Measured current of the IMU during a simulated low-
power application.

standby mode as tON and tOF F , respectively. Furthermore,
we deﬁne as duty-cycle the time spent in run mode divided
by the whole period, and therefore DC =
[43]. As
the system consumes PON = 505 mW during the run mode
and POF F = 0.185 mW during standby, the average power
during a whole period (i.e., tON + tOF F ) can be calculated
using Equation 2.

tON
tON +tOF F

Pavg = (1 − DC) · POF F + DC · PON

(2)

Assuming a duty cycle of 10% and an operating time of 20
hours per day, an energy of E = Pavg ·20h = (0.9·0.185mW +
0.1 · 505mW ) · 20h ≈ 1W h is required. A typical smart phone
battery has a capacity of more than 10Wh. This means that
the current prototype could last 20 hours on a battery with
only one tenth the size of a smartphone battery, even when
assuming the worst-case power consumption.

TABLE V: Power consumption.

(a) S1: 1.0 (b) S2: 0.74 (c) S3: 0.69 (d) S4: 0.58 (e) S5: 0.45

Subsystem

IMU

MCU

Readout circuit

Total

Supply voltage
Power run
Power standby

3.3V

3.3V
23mW 430mW
40µW
144µW

5V

–
52mW 505mW
1µW 185µW

Fig. 22: Average of all contact-frames from each recording
session for class 13 (screw driver). From (a) to (e) are
respectively session 1 to 5. The numbers represent the relative
mean response of each session compared to session 1.

e
s
n
o
p
s
e
r

n
a
e
m
e
v
i
t
a
l
e
R

1

0.8

0.6

0.4

0.2

0

0

1

2

3
Session

4

5

6

Fig. 23: Average response of all contact-frames in a recording
session with respect to the average response of all contact-
frames of session 1.

e
s
n
o
p
s
e
r

n
a
e
m
e
v
i
t
a
l
e
R

1

0.8

0.6

0.4

0.2

0

Square
Glove

0

1

2

3

4

5

Session

Fig. 24: Response degradation of the sensor glove and a square
control sensor.

slightly faster if the sensor is heavily used as in the case of the
glove. The decreasing response of the square sensor suggests
that the degradation is not only caused by extensive use, but
also simply by the combination of materials.

2) Slip evaluations: Thanks to the high frame rate of the
analog front-end and the high spatial resolution of the sensor,
a fast moving pressure spike is straightforward to capture. An
example frame sequence of such a case can be observed in
Fig. 25, where a screwdriver was pulled across the sensor
surface. It is much more difﬁcult if the pressure is evenly
distributed along the direction of movement. If, additionally
to being smooth and symmetrical, the moving object is longer
than the sensor is wide it becomes impossible to recognize any
movement by just inspecting the consecutive pressure frames.
This example is depicted in Fig. 26, where the ‘slipping’ object
is a full coke can.

F. Discussions

With this work, we demonstrate a low-latency smart hand
equipped with scalable tactile sensor and embedded machine
learning able to classify different daily objects in real-time.
Possible applications include the real-time control of robotic

12

or prosthetic hands or arms, where the embedded processing of
the sensor data and its classiﬁcation output can be immediately
used to program subsequent movements. We achieve up to
98.86% top-1 and 99.83% top-3 accuracies when splitting the
data from all sessions randomly. When a leave-one-session-out
validation is performed, the average top-1 and and top-3 inter-
session accuracy drops to 49.63% and 77.84%, respectively,
with the highest validation results being 49.63% top-1 and
77.84% top-3 when the data from the intermediate session is
used as validation set.

We perform additional sensor evaluations and show that
the degradation of the tactile sensor does not originate only
from repetitive usage, but is also an intrinsic property of the
combination of materials (electrodes and adhesive). Another
appropriate route of exploration is the continuous use and
examination of the sensor glove. The degradation in Fig. 23
slows down and shows a convergence to a stable behavior
with further use. If the sensor degradation effectively stops,
the analog front-end can easily be tuned to the ﬁnal stable
response and it is reasonable to expect large accuracy gains.
Finally, the dataset used in this work is collected by placing
the objects on a table and handling them mostly from the
top. This ﬁrst step of recognizing an object is important for
a robotic arm to decide how to better handle the object in
the following steps, for example, to lift it. In this subsequent
scenario, slip detection becomes relevant. Our experiments
on slip evaluations suggest that it is possible to observe the
slipping motion of an object using this sensor. In future work,
a new dataset can be collected, for example, while lifting or
moving the objects from one location to another, to evaluate
slip detection algorithms. Likewise, the utility of the IMU data
can be further assessed in these scenarios where the handling
of the objects is more variable. Finally, the proposed system
can be embedded on robotic hands for assessing its perfor-
mance on-ﬁeld, and the combination with other modalities
such as electromyography [4] can be explored.

V. CONCLUSION

This paper presents SmartHand, a smart embedded system
that is a step towards equipping robotic and prosthetic devices
with a sense of touch. Starting from replicating a state-of-
art
tactile sensor with high spatial resolution, SmartHand
focuses on maximizing the temporal resolution by reducing the
latency in the system response. This is made possible thanks
to an improved analog front-end and the real-time execution
of a compact deep learning model embedded close to the
sensor node. A working prototype of SmartHand is designed
and implemented to carry out solid experimental evaluations
that have shown an overall power consumption of 505 mW

Fig. 25: Section of a frame sequence where a screwdriver was
pulled from top to bottom across the sensor surface.

Fig. 26: Section of a frame sequence where a full coke can
was pulled from left to right across the sensor surface.

and 185 µW respectively in active and standby mode, and a
response latency of 100 ms achieving an inter-session accuracy
of 77.84% (top-3) in classifying 17 classes, i.e., 16 objects
and the empty hand. Further sensor evaluations demonstrate
the changing sensor response with repetitive usage and with
sliding objects, providing insights for future developments.

REFERENCES

[1] C.-P. Jiang, N. Zhao, G.-C. Shen, Z.-D. Lin, B. Yang, X.-L. Wang, and
J.-Q. Liu, “Tactile sensor array with tertiary leaf-vein structures and
position-encoded capacity,” IEEE Sensors Journal, vol. 21, no. 18, pp.
21 022–21 029, 2021.

[2] M. N. Saadatzi, J. R. Baptist, Z. Yang, and D. O. Popa, “Modeling
and fabrication of scalable tactile sensor arrays for ﬂexible robot skins,”
IEEE Sensors Journal, vol. 19, no. 17, pp. 7632–7643, 2019.

[3] L. Song, H. Zhu, Y. Zheng, M. Zhao, C. A. T. Tee, and F. Fang, “Bionic
compound eye-inspired high spatial and sensitive tactile sensor,” IEEE
Transactions on Instrumentation and Measurement, vol. 70, pp. 1–8,
2021.

[4] R. Meattini, S. Benatti, U. Scarcia, D. De Gregorio, L. Benini, and
C. Melchiorri, “An semg-based human–robot interface for robotic hands
using machine learning and synergies,” IEEE Transactions on Com-
ponents, Packaging and Manufacturing Technology, vol. 8, no. 7, pp.
1149–1158, 2018.

[5] R.-E. Precup, T.-A. Teban, A. Albu, A.-B. Borlea, I. A. Zamﬁrache, and
E. M. Petriu, “Evolving fuzzy models for prosthetic hand myoelectric-
based control,” IEEE Transactions on Instrumentation and Measurement,
vol. 69, no. 7, pp. 4625–4636, 2020.

[6] F. Santoni, A. De Angelis, I. Skog, A. Moschitta, and P. Carbone,
“Calibration and characterization of a magnetic positioning system using
a robotic arm,” IEEE Transactions on Instrumentation and Measurement,
vol. 68, no. 5, pp. 1494–1502, 2019.

[7] L. Bergamini, M. Sposato, M. Pellicciari, M. Peruzzini, S. Calderara,
and J. Schmidt, “Deep learning-based method for vision-guided robotic
grasping of unknown objects,” Advanced Engineering Informatics,
vol. 44, p. 101052, 2020.

[8] B. Cheng, W. Wu, D. Tao, S. Mei, T. Mao, and J. Cheng, “Random
cropping ensemble neural network for image classiﬁcation in a robotic
arm grasping system,” IEEE Transactions on Instrumentation and Mea-
surement, vol. 69, no. 9, pp. 6795–6806, 2020.

[9] S. Sahoo, M. Maheshwari, D. K. Pratihar, and S. Mukhopadhyay, “A
geometry recognition-based strategy for locomotion transitions early
prediction of prosthetic devices,” IEEE Transactions on Instrumentation
and Measurement, vol. 69, no. 4, pp. 1259–1267, 2019.

[10] P. Weiner, C. Neef, Y. Shibata, Y. Nakamura, and T. Asfour, “An
embedded, multi-modal sensor system for scalable robotic and prosthetic
hand ﬁngers,” Sensors, vol. 20, no. 1, 2020.

[11] L. Zou, C. Ge, Z. J. Wang, E. Cretu, and X. Li, “Novel

tactile
sensor technology and smart tactile sensing systems: A review,” Sensors
(Switzerland), vol. 17, 11 2017.

[12] Z. Kappassov, J. A. Corrales, and V. Perdereau, “Tactile sensing in
dexterous robot hands - review,” Robotics and Autonomous Systems,
vol. 74, pp. 195–220, 2015.

[13] F. Xu, X. Li, Y. Shi, L. Li, W. Wang, L. He, and R. Liu, “Recent
developments for ﬂexible pressure sensors: A review,” Micromachines,
vol. 9, 11 2018.

[14] D. Maddipatla, B. B. Narakathu, M. M. Ali, A. A. Chlaihawi, and M. Z.
Atashbar, “Development of a novel carbon nanotube based printed and
ﬂexible pressure sensor,” in 2017 IEEE Sensors Applications Symposium
(SAS), 2017, pp. 1–4.

[15] S. Sundaram, P. Kellnhofer, Y. Li, J. Y. Zhu, A. Torralba, and W. Ma-
tusik, “Learning the signatures of the human grasp using a scalable
tactile glove,” Nature, vol. 569, pp. 698–702, 5 2019.

[16] N. Mavrakis and R. Stolkin, “Estimation and exploitation of objects
’ inertial parameters in robotic grasping and manipulation: A survey,”
Robotics and Autonomous Systems, vol. 124, p. 103374, 2020.

[17] F. Yin, J. Yang, H. Peng, and W. Yuan, “Flexible and highly sensitive ar-
tiﬁcial electronic skin based on graphene/polyamide interlocking fabric,”
Journal of Materials Chemistry C, vol. 6, pp. 6840–6846, 2018.
[18] T. Zhang, L. Jiang, and H. Liu, “Design and functional evaluation of a
dexterous myoelectric hand prosthesis with biomimetic tactile sensor,”
IEEE Transactions on Neural Systems and Rehabilitation Engineering,
2018.

13

[19] M. Franceschi, L. Seminara, S. Dosen, M. Strbac, M. Valle, and
D. Farina, “A system for electrotactile feedback using electronic skin and
ﬂexible matrix electrodes: Experimental evaluation,” IEEE Transactions
on Haptics, vol. 10, 2017.

[20] S. Inc. (2021) Syntouch toccare haptics measurement system. [Online].

Available: https://syntouchinc.com/

[21] S.-h. Choi and K. Tahara, “Dexterous object manipulation by a multi-
ﬁngered robotic hand with visual-tactile ﬁngertip sensors,” ROBOMECH
Journal, vol. 7, 03 2020.

[22] M. Kang, J. Kim, B. Jang, Y. Chae, J.-H. Kim, and J.-H. Ahn,
“Graphene-based three dimensional capacitive touch sensor for wearable
electronics,” ACS Nano, vol. 11, 07 2017.

[23] M. Altamirano Cabrera, J. Heredia, and D. Tsetserukou, “Tactile percep-
tion of objects by the user’s palm for the development of multi-contact
wearable tactile displays,” in Haptics: Science, Technology, Applications,
I. Nisky, J. Hartcher-O’Brien, M. Wiertlewski, and J. Smeets, Eds.
Cham: Springer International Publishing, 2020, pp. 51–59.

[24] Y. Abbass, L. Seminara, M. Saleh, and M. Valle, “Novel wearable
tactile feedback system for post-stroke rehabilitation,” in 2021 IEEE
Biomedical Circuits and Systems Conference (BioCAS).
IEEE, 2021,
pp. 1–6.

[25] M. Zhu, Z. Sun, Z. Zhang, Q. Shi, T. He, H. Liu, T. Chen, and C. Lee,
“Haptic-feedback smart glove as a creative human-machine interface
(hmi) for virtual/augmented reality applications,” Science Advances,
vol. 6, no. 19, p. eaaz8693, 2020.

[26] Y. S. Narang, B. Sundaralingam, K. Van Wyk, A. Mousavian, and
D. Fox, “Interpreting and predicting tactile signals for the syntouch
biotac,” arXiv preprint arXiv:2101.05452, 2021.

[27] S. U. Yunas, A. Alharthi, and K. B. Ozanyan, “Multi-modality sensor
fusion for gait classiﬁcation using deep learning,” in 2020 IEEE Sensors
Applications Symposium (SAS), 2020, pp. 1–6.

[28] M. S. Roy, R. Gupta, J. K. Chandra, K. D. Sharma, and A. Taluk-
dar, “Improving photoplethysmographic measurements under motion
artifacts using artiﬁcial neural network for personal healthcare,” IEEE
Transactions on Instrumentation and Measurement, vol. 67, no. 12, pp.
2820–2829, 2018.

[29] A. Albu, R.-E. Precup, and T.-A. Teban, “Results and challenges of
artiﬁcial neural networks used for decision-making and control in med-
ical applications,” Facta Universitatis, Series: Mechanical Engineering,
vol. 17, no. 3, pp. 285–308, 2019.

[30] X. Wang, M. Magno, L. Cavigelli, and L. Benini, “Fann-on-mcu: An
open-source toolkit for energy-efﬁcient neural network inference at the
edge of the internet of things,” IEEE Internet of Things Journal, vol. 7,
no. 5, pp. 4403–4417, 2020.

[31] L. Zhao, J. Qian, F. Tian, R. Liu, B. Liua, S. Zhang, and M. Lu,
“A weighted discriminative extreme learning machine design for lung
cancer detection by an electronic nose system,” IEEE Transactions on
Instrumentation and Measurement, 2021.

[32] M. Scherer, M. Magno, J. Erb, P. Mayer, M. Eggimann, and L. Benini,
“Tinyradarnn: Combining spatial and temporal convolutional neural
networks for embedded gesture recognition with short range radars,”
IEEE Internet of Things Journal, pp. 1–1, 2021.

[33] T. Kang, K.-I. Oh, J.-J. Lee, B.-S. Park, W. Oh, and S.-E. Kim, “Mea-
surement and analysis of human body channel response for biometric
recognition,” IEEE Transactions on Instrumentation and Measurement,
vol. 70, pp. 1–12, 2021.

[34] X. Wang, M. Hersche, B. Tömekce, B. Kaya, M. Magno, and L. Benini,
“An accurate eegnet-based motor-imagery brain–computer interface for
low-power edge computing,” in 2020 IEEE international symposium on
medical measurements and applications (MeMeA).
IEEE, 2020, pp.
1–6.

[35] X. Wang, F. Geiger, V. Niculescu, M. Magno, and L. Benini, “Smart-
hand: Towards embedded smart hands for prosthetic and robotic appli-
cations,” in 2021 IEEE Sensors Applications Symposium (SAS).
IEEE,
2021, pp. 1–6.

[36] L. Paredes-Madrid, A. Matute, J. O. Bareño, C. A. Vargas, and E. I.
Velásquez, “Underlying physics of conductive polymer composites and
force sensing resistors (fsrs). a study on creep response and dynamic
loading,” Materials, vol. 10, 11 2017.

[37] J. S. Kim, D. Y. Kwon, and B. D. Choi, “High-accuracy, compact scan-
ning method and circuit for resistive sensor arrays,” Sensors (Switzer-
land), vol. 16, 1 2016.

[38] J. A. Hidalgo-López, R. Fernández-Ramos, J. Romero-Sánchez, J. F.
Martín-Canales, and F. J. Ríos-Gómez, “Improving accuracy in the
readout of resistive sensor arrays,” Journal of Sensors, vol. 2018, 2018.

14

[39] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016, pp. 770–778.

[40] M. Trumble, A. Gilbert, C. Malleson, A. Hilton, and J. Collomosse,
“Total capture: 3d human pose estimation fusing video and inertial
sensors,” in BMVC, 2017.

[41] C. Li, S. Wang, Y. Zhuang, and F. Yan, “Deep sensor fusion between
2d laser scanner and imu for mobile robot localization,” IEEE Sensors
Journal, pp. 1–1, 5 2019.

[42] B. Heyneman and M. R. Cutkosky, “Slip classiﬁcation for dynamic tac-
tile array sensors,” International Journal of Robotics Research, vol. 35,
pp. 404–421, 4 2016.

[43] P. Mayer, M. Magno, and L. Benini, “Smart power unit—mw-to-nw
power management and control for self-sustainable iot devices,” IEEE
Transactions on Power Electronics, vol. 36, no. 5, pp. 5700–5710, 2020.

Michele Magno is currently a Senior Researcher
and Lecturer at ETH Zurich, Switzerland, at the De-
partment of Information Technology and Electrical
Engineering (D-ITET). Since 2020, he is leading
the D-ITET center for project-based learning. He
received his master and Ph.D. degrees in electronic
engineering from the University of Bologna, Italy,
in 2004 and 2010, respectively. He is working in
ETH since 2013 and has become a visiting lec-
turer or professor in several universities, namely the
University of Nice Sophia, France, Enssat Lannion,
France, University of Bologna and Mid University Sweden. His current
research interests include smart sensing, low power machine learning, wireless
sensor networks, wearable devices, energy harvesting, low power management
techniques, and extension of the lifetime of batteries-operating devices. He has
authored more than 200 papers in international journals and conferences. Some
of his publications were awarded as best papers awards in IEEE conferences.

Xiaying Wang received her B.Sc. and M.Sc. de-
grees in biomedical engineering from Politecnico di
Milano, Italy and ETH Zürich, Switzerland in 2016
and 2018, respectively. She is currently pursuing a
Ph.D. degree at the Integrated Systems Laboratory at
ETH Zürich. Her research interests include biosignal
processing, low power embedded systems, energy-
efﬁcient smart sensors, brain—machine interfaces,
and machine learning on microcontrollers. She re-
ceived the excellent paper award at the IEEE Health-
com conference in 2018 and she won the Ph.D.

Fellowship funded by Swiss Data Science Center in 2019.

Fabian Geiger received his B.Sc. degree in Electri-
cal Engineering and his M.Sc. degree in Biomedical
Engineering from ETH Zürich, Switzerland in 2018
and 2020, respectively. He is currently working as a
design engineer in the ﬁelds hardware and ﬁrmware.
His interests include edge computing, smart sensor
nodes, machine learning on microcontrollers and
tinkering with new technologies.

Luca Benini is the Chair of Digital Circuits and
Systems at ETH Zürich and a Full Professor at
the University of Bologna. He has received a PhD
from Stanford University and has served as Chief
Architect for the Platform2012 in STMicroelectron-
ics, Grenoble. Dr. Benini’s research interests are in
energy-efﬁcient system and multi-core SoC design.
He is also active in the area of energy-efﬁcient smart
sensors and sensor networks. He has published more
than 1000 papers in peer-reviewed international jour-
nals and conferences, four books and several book
chapters. He is a Fellow of the ACM and of the IEEE and a member of the
Academia Europaea.

Vlad Niculescu received the Master’s degree in
Robotics, Systems, and Control
from the ETH
Zürich, in 2019. He is currently pursuing the Ph.D.
in Electrical Engineering within the Integrated Sys-
tems Laboratory in ETH Zürich. During the Bach-
elor and Master period, he competed in more than
ten international student competitions, and he was
the electrical lead of the student project Swissloop,
which won second place and the innovation award in
the SpaceX Hyperloop Pod Competition 2019. His
research is now focused on developing localization
and autonomous navigation algorithms that target ultra-low-power platforms
which can operate onboard nano-drones.

