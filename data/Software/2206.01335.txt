2
2
0
2

n
u
J

2
1

]
E
S
.
s
c
[

2
v
5
3
3
1
0
.
6
0
2
2
:
v
i
X
r
a

Code Generation Tools (Almost) for Free?
A Study of Few-Shot, Pre-Trained Language Models on Code

Patrick Barei√ü
University of Stuttgart
Germany

Marcelo d‚ÄôAmorim
Federal University of Pernambuco
Brasil

Beatriz Souza
Federal University of Pernambuco
Brasil

Michael Pradel
University of Stuttgart
Germany

ABSTRACT
Few-shot learning with large-scale, pre-trained language models is a
powerful way to answer questions about code, e.g., how to complete
a given code example, or even generate code snippets from scratch.
The success of these models raises the question whether they could
serve as a basis for building a wide range code generation tools.
Traditionally, such tools are built manually and separately for each
task. Instead, few-shot learning may allow to obtain different tools
from a single pre-trained language model by simply providing a
few examples or a natural language description of the expected
tool behavior. This paper studies to what extent a state-of-the-art,
pre-trained language model of code, Codex, may serve this purpose.
We consider three code manipulation and code generation tasks
targeted by a range of traditional tools: (i) code mutation; (ii) test
oracle generation from natural language documentation; and (iii)
test case generation. For each task, we compare few-shot learning to
a manually built tool. Our results show that the model-based tools
complement (code mutation), are on par (test oracle generation),
or even outperform their respective traditionally built tool (test
case generation), while imposing far less effort to develop them. By
comparing the effectiveness of different variants of the model-based
tools, we provide insights on how to design an appropriate input
(‚Äúprompt‚Äù) to the model and what influence the size of the model
has. For example, we find that providing a small natural language
description of the code generation task is an easy way to improve
predictions. Overall, we conclude that few-shot language models
are surprisingly effective, yet there is still more work to be done,
such as exploring more diverse ways of prompting and tackling
even more involved tasks.

1 INTRODUCTION
Various software engineering tools assist developers by generating
source code. One group of approaches reasons about existing code
and modifies it in a way suitable to achieve some goal. For example,
code mutation tools [33, 43] introduce mistakes to measure the
effectiveness of test suites, and automated program repair tools [37,
41] suggest how to fix programming mistakes. Another group of
approaches generates new code from scratch, given some existing
code that the new code is supposed to relate to. For example, test
case generators [17, 21, 42] automatically create tests that exercise
a given method under test, and code completion tools [14, 27, 47]
generate code that completes an existing code snippet in a suitable
way. Finally, a third group of code generation tools does not require
any existing code as an input, but instead generates new code

given some natural language artifact. For example, some approaches
generate test oracles based on informal API documentation [10,
11, 22], infer API usage protocols [57], or suggest missing type
annotations [39].

The traditional way of creating such code manipulation tools
is based on program analysis combined with various rules and
heuristics. Program analysis can, at least in principle, ensure that
the generated code is guaranteed to have certain properties, e.g.,
to be type-correct or to pass a given set of test cases. Hand-coded
rules and heuristics are typically required to enable a technique
to be effective and efficient on real-world software. More recently,
learning-based approaches have started to complement traditional
program analysis-based code generation tools [44]. Typically, these
approaches formulate the specific code generation task as a super-
vised learning problem, and require large amounts of training data
to obtain an effective machine learning model. A commonality of
both traditional program analyses and learning-based approaches
is that creating a new code generation tool involves significant
human effort. Even worse, this effort often must be repeated for
each new combination of a task to achieve and a programming
language to target.

A recent trend in the natural language processing (NLP) commu-
nity promises a form of ‚Äúgeneral intelligence‚Äù that remedies many
of the problems of building task-specific techniques: few-shot learn-
ing with large-scale, pre-trained language models [13], henceforth
abbreviated with FSLMs. These models are trained on huge amounts
of data without focusing on a specific downstream task. Instead,
the training is based on generic pseudo-tasks for which it is trivial
to obtain sufficient training data, e.g., predicting masked words or
whether two sentences belong together. Once trained, FSLMs are
effective at various question answering and text generation tasks,
e.g., reading comprehension, trivia quizzes, translation between
languages, and text completion [13].

Applying FSLMs to code is still a relatively sparsely explored
area. While recent work employs pre-training of models of code as a
means to reduce the amount of required training examples [3, 20, 24,
38], these approaches still fine-tune a model for a specific purpose
and hence require moderately large amounts of labeled training
examples. Noteworthy exceptions include GitHub‚Äôs Copilot code
completion system1, which is based on the Codex FSLM [15], and
the recently released, open-source PolyCoder model family [55].
While the results of these models are impressive, code comple-
tion is only one of many code generation tasks. Do the abilities of

1https://copilot.github.com/

1

 
 
 
 
 
 
Conference‚Äô17, July 2017, Washington, DC, USA

Patrick Barei√ü, Beatriz Souza, Marcelo d‚ÄôAmorim, and Michael Pradel

FSLMs generalize to other software engineering tasks that tradi-
tionally have been addressed by special-purpose code generation
techniques? In case of a positive answer, FSLMs offer the poten-
tial to obtain code generation tools (almost) for free, as an FSLM
gets trained once and can then be applied to many different tasks.
Despite this potential and the strong interest of the software en-
gineering community in automated code generation techniques,
there currently is no systematic study of the abilities of FSLMs on
such tasks.

This paper presents the first systematic study of FSLMs as the
key ingredient for creating code generation tools. We describe a
general framework for creating a code generation tool based on an
existing FSLM, apply it to three popular tasks that are representative
for different kinds of code generation problems, and compare the
FSLM-based approach against traditionally developed state-of-the-
art tools. Instantiating our framework for a specific code generation
tasks involves three steps. First, develop an extractor of code or
natural information to use in a query to the model. Second, design
a suitable prompt, i.e., a template of how to present the input to the
model, which then gets instantiated for each given example. Finally,
develop a lightweight post-processing module, which, e.g., removes
generated code that fails to compile. We argue that these steps are
lightweight compared to designing and implementing a traditional
program generation technique, as they leave the most challenging
parts of the tasks to the FSLM. As a result, the approach offers an
almost-for-free way of obtaining a code generation tool.

We instantiate these ideas for three code generation tasks: code
mutation, test oracle generation, and test case generation. These
tasks have received significant interest from the software engineer-
ing community, and hence, offer state-of-the-art tools to compare
against. The tasks also cover different levels of granularity of the
generated code, ranging from manipulating a few tokens in code
mutation to generating entire test cases. Finally, the selected tasks
are based on different kinds of input: code mutation and test case
generation are based on existing code, whereas test oracle genera-
tion is based on natural language documentation. Table 1 shows
two representative example outputs that FSLM-based tools produce
for each of these tasks. The examples follow the format ùë• | ==> ùë¶,
where ùë• and ùë¶ denote, respectively, the input and output of the
prompt for the given task.

For each task, we instantiate our general framework to create
an FSLM-based code generation tool and then apply the tool to
real-world software. We then systematically compare the results
produced by the FSLM-based tool against an existing, state-of-the-
art tool built specifically for the same purpose: the Major [34] code
mutation tool, the MeMo [11] test oracle extraction tool, and the
Randoop [42] test case generator. We measure the effectiveness
of each tool using metrics of success suitable for the task, e.g.,
code coverage for test case generation, and precision/recall w.r.t. a
ground truth for test oracle generation.

Our key findings include:

‚Ä¢ FSLM-based tools are similarly and sometimes even more
effective than existing, special-purpose tools. For example,
for oracle generation, we measure an F1 score of 0.59 and
0.60 for MeMo [11] and an FSLM-based tool, respectively. For

2

test generation, Randoop achieves 10% coverage, whereas a
simple FSLM-based tool achieves 14%.

‚Ä¢ FSLM-based and traditionally-developed tools often comple-
ment each other. For example, our FSLM-based code muta-
tion tool creates various mutants that Major cannot generate.
The complementary nature of the two kinds of tools shows
the potential of combining traditional and FSLM-based ap-
proaches. For example, combining Randoop-generated and
FSLM-generated test cases yields 16% coverage, i.e., it ex-
ceeds both approaches individually.

‚Ä¢ FSLM-based tool do not come completely for free. To be ef-
fective, they need specifically designed prompts and suitable
inputs extracted from the given code or natural language.
Yet, the effort required to create an FSLM-based tool is clearly
lower than that for building special-purpose code generation
tools from scratch.

In summary, this paper contributes the following:

‚Ä¢ The first systematic study of FSLM-based code generation

tools.

‚Ä¢ We are the first to address code mutation, test oracle genera-
tion, and test case generation in an end-to-end manner with
general-purpose FSLMs.

‚Ä¢ Insights that show the potential and challenges of building
FSLM-based code generation tools, providing guidance for
future work.

2 BACKGROUND
A generative language model is designed to predict the next token
given some previous tokens. For example, if such a model is given
the input ‚ÄúI am Barack Obama. I used to be the president of the
United States of‚Äù, such a language model might predict ‚ÄúAmerica‚Äù
as the next token. This can be used to generate text by repeatedly
sampling for the next token. When using such a model for down-
stream tasks that differ from the next token prediction objective,
the step of initial training is often referred to as pre-training.

A pre-trained model can be adapted to a specific downstream
task via fine-tuning, i.e., in additional training step based on labeled
data for the downstream task. A recently proposed alternative is
few-shot learning [13], which refers to the ability to perform a task
without any fine-tuning, but given only very few (typically, between
one and ten) examples as part of the query to the model. We utilize
generative language models as few-shot learners, which we refer
to as few-shot learning with large-scale, pre-trained language models
(FSLM). We use OpenAI‚Äôs Codex [15] model, which is trained on a
large set of GitHub projects. We access the model through its API.
Alternative generative models exist, e.g., GPT-NeoX [9].

The input provided to an FSLM is referred to as the prompt.
Prompts typically contain a few examples of inputs with their de-
sired outputs, followed by the input for which the model should
provide an answer. For the above example, a prompt could start by
giving a few example pairs of head of states and the correspond-
ing country, and then ‚ÄúBarack Obama‚Äù, to which the model might
respond with ‚ÄúUnited States‚Äù. Prompts are, in principle, unstruc-
tured text, and what exactly is provided in a prompt may strongly
influence the results. When querying an FSLM with a prompt, the
user can select the temperature, which intuitively speaking controls

A Study of Few-Shot, Pre-Trained Language Models on Code

Conference‚Äô17, July 2017, Washington, DC, USA

Table 1: Representative examples of results obtained with FSLM-based code generation tools.

Task

Example #1

Example #2

Code mutation

parsed = ( parsed + " 000000000 "). substring (0 , 9) ;
|== >
parsed = ( parsed + " 000000 "). substring (0 , 9) ;

ofSeconds ( divRem [0]. longValue () , divRem [1]. intValue () )
|==>
ofSeconds ( divRem [0]. longValue () / divRem [1]. intValue () ,

divRem [1]. intValue () )

The returned implementation is immutable , thread - safe and
Serializable . It is equivalent to tick ( system ( zone ) ,

Note for Java 7 and later : this method should be treated
as deprecated ; use the equivalent Long # compare method

Duration . ofSeconds (1) ).

instead .

Oracle generation

|==>
java . time . Clock . tick ( java . time . Clock . system ( zone ) ,

java . time . Duration . ofSeconds (1) )

IS EQUIVALENT TO
java . time . Clock . tickSeconds ( zone )

|==>
com . google . common . primitives . Longs . compare (a , b)
IS EQUIVALENT TO
java . lang . Long . compare (a , b)

public Vector1D scalarMultiply ( double a) {

public boolean contains ( Object o) {

Test generation

return new Vector1D (a * x); }

|==>
public void testScalarMultiply () {
Vector1D v1 = new Vector1D (1) ;
Vector1D v2 = new Vector1D (2) ;
Assert . assertEquals ( v1 . scalarMultiply (2) , v2 ); }

return indexOf ( o) >= 0; }

|==>
public void testContains () {

ArrayList < String > list = new ArrayList < String >() ;
list . add ("a");
list . add ("b");
list . add ("c");
Assert . assertTrue ( list . contains ("b"));
Assert . assertFalse ( list . contains ("d")); }

the creativity or randomness of the model‚Äôs responses. A higher
temperature means the model will generate more diverse responses,
but be less factual and precise. Repeatedly querying a model may
return different results, especially when using a higher temperature.

3 METHODOLOGY
Figure 1 shows a general framework for producing code generation
tools for a diverse set of tasks. The framework relies on a large-scale
language model pre-trained on code, such as Codex [15]. The input
to the framework is a textual representation of a software artifact,
e.g., source code or documentation. The output is a set of generated
code snippets, e.g., a modified version of the given source code, an
executable specification, or a test case. The framework is organized
in three main steps, which we briefly describe in the following.

(1) Instance extraction. The first step is responsible for ex-
tracting parts of a given software artifact that are relevant
for the code generation task. We refer to an extracted part
as an instance. For example, for code mutation, the instance
extraction takes in source code and extracts code lines for
which we want to generate mutants. The rationale for not
simply passing in the entire raw software artifact is two-fold.
First, FSLMs impose a maximum input size, e.g., 4,096 tokens
for the Codex model series. Second, larger inputs take longer
to process, i.e., the instance extraction reduces the overall
time to generate code.

(2) Prompt design. The second step to use our framework is
designing an effective prompt, which is perhaps the most
difficult part of creating an FSLM-based code generation
tool. The prompt contains (i) an instance, as extracted in
the previous step, and (ii) contextual information, such as
examples for addressing the code generation task and/or a
natural language description of the task. The prompts we
use in our study include a part that is invariant across all
instances (e.g., a natural language description of the task)

3

and a part that is instance-specific (e.g., the line of code to
mutate). Given a prompt for a specific instance, the approach
passes the prompt to the FSLM and then obtains a completion
of it.

(3) Post-processing. Finally, the third step is to post-process
the raw output produced by the model in order to obtain
the final code generation results. The post-processing may
filter the completions, e.g., to ensure that the generated code
compiles or to copy the predicted code into a task-specific
code template.

Sections 3.2, 3.3, and 3.4 describe the code generation tasks that

this paper focuses on according to the three above steps.

3.1 Research Questions
The overall goal of this study is to understand the strengths and
limitations of FSML-based code generation tools. To this end, we
investigate the following research questions.

RQ1. Accuracy: How accurate are the model‚Äôs predictions com-

pared to existing tools?

RQ2. Impact of Prompt: What kinds of prompts are most effec-

tive at producing accurate results?

RQ3. Impact of Model Size: How much does the size of the FSLM

influence the accuracy?

The motivation for RQ1 is that building traditional tools by hand
imposes significant human costs. Understanding to what extent a
single general-purpose language model could replace these tools
may help reducing the cost for creating new tools. The motivation
for RQ2 is that the prompt to query a pre-trained model is the main
‚Äúknob‚Äù to control the quality of the model‚Äôs predictions. Under-
standing what prompts are effective (or not) helps in making best
use of the existing models. Finally, the motivation for RQ3 is that
state-of-the-art language models are trained on huge datasets using
enormous computational resources. Understanding the impact of

Conference‚Äô17, July 2017, Washington, DC, USA

Patrick Barei√ü, Beatriz Souza, Marcelo d‚ÄôAmorim, and Michael Pradel

Figure 1: Overview of a general framework for generating code analysis tools using few-shot, pre-trained language models.

Generate mutations for the following snippets of code.
[[Code]]
long biasedExp = (longBits & DoubleConsts.EXP_BIT_MASK)¬ª

(DoubleConsts.SIGNIFICAND_WIDTH - 1);

[[Mutations]]
- longBits & DoubleConsts.EXP_BIT_MASK |==> longBits |

DoubleConsts.EXP_BIT_MASK

- longBits & DoubleConsts.EXP_BIT_MASK) ¬ª

(DoubleConsts.SIGNIFICAND_WIDTH - 1) |==> longBits &
DoubleConsts.EXP_BIT_MASK) ¬´ (DoubleConsts.SIGNIFICAND_WIDTH -
1)

- 1 |==> 0
- DoubleConsts.SIGNIFICAND_WIDTH - 1 |==>

DoubleConsts.SIGNIFICAND_WIDTH % 1

[[Code]]
...
(3 more examples)
...
[[Code]]
WeightMass mass = lhsDist.get(classVal);
[[Mutations]]
- classVal |==> classVal + 1
- classVal |==> 0

Figure 2: Prompt used for mutant generation. We shot the natural language
description in purple, any instance-specific parts of the prompt (e.g., general
examples, separators, etc.) in black, inputs that are specific to a concrete in-
stance (i.e., the part that changes when the instance changes) in blue, and
parts that the FSLM model generates in response to that input in green.

model size on the model‚Äôs effectiveness will help appropriately
allocate computational resources to train models.

3.2 Task 1: Code Mutation
We address our research questions by studying them on three popu-
lar code generation tasks. The first task is code mutation, a popular
technique to assess the quality of a test suite by estimating its ability
to detect injected faults. Code mutation modifies a given piece of
code by injecting a programming mistake. As a simple example, a
code mutation tool may change a comparison x > 5 into x < 5.

3.2.1 Baseline Tool. We study the effectiveness of an FSLM-based
code mutation tool by comparing it against Major [34], a popu-
lar code mutation tool for Java. Major applies different built-in
mutation operators and ensures that all created mutants compile.

Instance Extraction. To create an FSLM-based code mutation
3.2.2
tool, the first step is extracting code snippets to modify. Since muta-
tion operators typically are local code transformations, an instance
for this task consists of a single line of code. The instance extractor
takes a Java file as input and returns a list of lines of code that we
then try to mutate via the FSLM. For a fair comparison, we focus
our experiments on those lines where Major applies a mutation.

3.2.3 Prompt. To generate mutants via an FSLM, we design a
prompt that ask the model to modify one code line at a time. Fig-
ure 2 shows the default prompt for our study. The prompt contains
a brief natural language description of the task to perform, followed
by a short list of examples. To help the model understand the dif-
ferent sections of the prompt, we mark them, e.g., via brackets as
in ‚Äú[[Code]]‚Äù. Each example consists of the line of code to modify
(‚Äú[[Code]]‚Äù) and a few mutants to generate based on it (‚Äú[[Muta-
tions]]‚Äù). Since mutants are small, we ask the model to suggest
multiple mutants at once. Thus, the temperature can be set low for
consistent but not as diverse results. At the end, the prompt provides
the code line we wish to mutate, leaving the task of completing it
with suitable mutants to the model. For the example in Figure 2,
the model suggests a mutant that replaces the expression classVal
passed as parameter in the call lhsDist.get() with classVal + 1.

3.2.4 Post-processing. Once the model completes the prompt, we
post-process the raw completion using simple regular expressions
to extract the mutations suggested by the model. Because an FSLM
does not guarantee to produce code that is syntactically or semanti-
cally valid, we filter out any suggested mutants that do not compile.
All remaining code snippets are our final set of mutants generated
by the FSLM.

Table 2: Projects used in our study.

Project

Version # Classes

3.2.5 Benchmarks. As code
files to mutate, we ran-
domly select 32 classes
from the projects listed on
Table 2. The smallest class
has 19 lines of code, while
the longest has 1,346. In to-
tal, the classes have 6,747
lines of code. Across the
32 classes, our instance ex-
tractor yields 1,194 instances (lines of code) to generate mutants
for.

Colt
ElasticSearch
GWT
GraphStream
Guava
Hibernate
JDK
Commons Math
Weka

297
2,821
3,178
233
464
3,393
4,240
918
1,648

1.2.0
6.1.1
2.5.1
1.3
19.0
5.4.2
8
3.6.1
3.8.0

3.3 Task 2: Generating Oracles from Natural

Language Documentation

As a second code generation task, we consider the problem of gen-
erating test oracles from natural language documentation. This
task represents a class of tasks where an FSLM translates natural
language to code. Specifically, we focus on the task of extracting
metamorphic test oracles for API methods from Javadoc comments.
A metamorphic test oracle states that two inputs that are in a spe-
cific relationship are expected to lead to outputs that are in another

4

Huge code corpusLarge-scaletrainingPre-trainedlanguage modelInstanceextractionSource code,documentation,etc.PromptRaw completionsPost-processingCode generationresultsInstance321A Study of Few-Shot, Pre-Trained Language Models on Code

Conference‚Äô17, July 2017, Washington, DC, USA

### Signature
public static java.lang.String toString(java.lang.Object[] a)
### Comment
...
The value returned by this method is equal to the value that would
be returned by Arrays.asList(a).toString(), unless a is null, in
which case "null" is returned.
### Analysis
This method returns the same thing as the expression
Arrays.asList(a).toString(), therefore they are equivalent
(at least as long as a is not null).
### Equivalence
if (a != null) toString(a) <-> Arrays.asList(a).toString() ;
...
(3 more examples)
...
### Signature
public double norm2(cern.colt.matrix.DoubleMatrix1D x)
### Comment
Returns the two-norm (aka euclidean norm) of vector x;
equivalent to mult(x,x).
### Analysis
This method is equivalent to the expression mult(x,x).
### Equivalence
norm2(x) <-> mult(x,x);

Figure 3: The prompt used for oracle extraction.

known relationship [16]. In the context of testing API methods, this
typically means that some API usage is equivalent (or in some other
relationship) to some other API usage. As an example of an oracle
we aim to generate, consider this excerpt from the Javadoc of the
Array.toString method: ‚ÄúThe value returned by this method is equal
to the value that would be returned by Arrays.asList(a).toString(),
unless a is null, in which case null is returned.‚Äù.

The equivalence described in this documentation could be speci-
fied as an executable test oracle that states that Arrays.toString(a)
yields the same as Arrays.asList(a).toString() if a != null.

3.3.1 Baseline Tool. Extracting test oracles from natural language
documentation is an active area of research [10, 11, 22]. As a base-
line to compare an FSLM-based tool against, we use the recently
proposed MeMo [11]. MeMo extracts metamorphic test oracles
by first identifying natural language sentences that could contain
such oracles using simple heuristics, and then translating those
sentences into code. This translation, which is the most intricate
part of MeMo, decomposes the sentence using a dependency parser,
and then converts the parsed sentence into code based on a set
of hard-coded rules and heuristics. Because of the inherent im-
precision and diversity of natural language, the second step has
to cover many edge cases to be effective. Our study investigates
whether an FSLM-based tool could replace or complement this sec-
ond step of MeMo, i.e., replacing the hard-coded rules by queries
to a pre-trained model.

Instance Extraction. In the context of this task, we define an
3.3.2
instance to be a method whose description probably contains an
oracle. For a fair comparison with the baseline tool, and because
extracting such sentences is comparatively simple, we MeMo to
identify sentences that likely contain an oracle. We then pass the
entire comment containing such a sentence into our prompt, which
provides the FSLM with some context.

3.3.3 Prompt. We design the prompt to be a short list of examples
of what the model is supposed to achieve, as shown in Figure 3.
Each example consists of four parts: (1) The signature of the method
for which to generate an oracle (‚Äú### Signature‚Äù), (2) the natural
language description of the method‚Äôs behavior, as extracted from the
available Javadoc (‚Äú### Comment‚Äù), (3) A small section of natural
language explanation about how the equivalence manifests itself
in the example (‚Äú### Analysis‚Äù) (This part is motivated by the
observation that by letting the model explain its reasoning before
generating the result itself may increase its effectiveness [54].), and
(4) the Java code of the metamorphic oracle, which consists of a
conditional followed by two expressions separated by the symbol
<->, denoting ‚Äúequivalent to‚Äù (‚Äú### Equivalence‚Äù). After providing a
small number of such examples (four by default), we provide the
signature and comment of the instance we are interested in, and
then let the model complete the prompt by providing an analysis
and the oracle. For this task, the temperature is set to zero, as we
observe the model to produce too imprecise predictions otherwise.

3.3.4 Post-processing. Given the raw completion produced by the
model in response to our prompt, we extract the generated test
oracle. The extraction is based on simple regular expressions, e.g.,
anchored around the special <-> symbol. Next, we check whether
the predicted condition (if any) and the code snippets compile
properly. Finally, the approach expands names of classes, e.g., Math
to java.lang.Math, using JavaSymbolSolver.2

3.3.5 Benchmarks. To measure the effectiveness of the FSLM-based
tool, we use a ground truth dataset available from MeMo‚Äôs arti-
facts [11]. The dataset is based on 5,000 methods from nine open-
source Java projects, from which 299 metamorphic test oracles have
been manually extracted. The oracles are diverse and vary in length:
The natural language descriptions range between 3 and 500 words,
with a mean of 44.3. The code of the oracles ranges between 3 and
81 tokens, with a mean of 21.6.

3.4 Task 3: Test Case Generation
As the third code generation task, we consider the problem of gener-
ating unit tests. This task represents a class of tasks where the FSLM
generates a method, i.e., a larger portion of code compared with the
previous examples. Test case generation is a labor-intensive task in
software testing [6], and several techniques have been proposed to
automate unit test case generation [49].

3.4.1 Baseline Tool. There are many automatic test case generation
tools available. Randoop [42] and EvoSuite [21] are popular repre-
sentatives of such tools. We use Randoop in our study. To generate
test cases with Randoop, for each method under test, we invoke
its main class randoop.main.Main passing the gentests command
and the ‚Äìmethodlist=filename.txt and ‚Äìgenerated-limit=100 argu-
ments. The file filename.txt contains the method under test, as well
as helper methods it depends on. We select helper methods with a
minimum amount of dependencies to include. The generated-limit
argument defines the maximum number of test method candidates
generated internally. For a fair comparison, we let Randoop and

2http://javaparser.org/

5

Conference‚Äô17, July 2017, Washington, DC, USA

Patrick Barei√ü, Beatriz Souza, Marcelo d‚ÄôAmorim, and Michael Pradel

the FSLM generate the same number (100) of test cases per method
under test.

Suggest a test for a method with the DoubleArrayList

quantiles(DoubleArrayList percentages) signature.

Helper constructors and methods:

Instance Extraction. For unit test case generation, we con-
3.4.2
sider an instance to be a method under test. That is, the instance
extractor takes a Java class as its input, produces a list of public
methods, and randomly selects a method from the list to be tested.

3.4.3 Prompt. Figure 4 shows an example of the (default) prompt
that we use for unit test case generation. The prompt starts with
a brief natural language description of the task. Next, we provide
one example of the task. The reason for showing only one exam-
ple is that state-of-the-art FSLMs only support a bounded prompt
size. The example consists of three parts: (1) a list of helper meth-
ods to assist in the creation of values. (‚ÄúHelper constructors and
methods:‚Äù), (2) the method under test itself, and (3) a test case that
invokes the method under test. After the example, the instance,
consisting of the code of the method (as explained on Section 3.4.2
‚ÄúInstance Extraction‚Äù) is provided, leaving the task of generating
a test case to the model. Since the prompt contains only a single
example, selecting this example potentially has a large impact on
the generated test. Section 4.2 compares different strategies for
selecting the example, e.g., selecting another method under test
from the same class and selecting another method under test at
random. Because each query yields only one test case, we make
multiple queries while varying the temperature parameter from 0.0
to 0.9, in steps of 0.1. For each temperature, we make 10 queries.
This way, the model predicts a total of 100 test case candidates.

3.4.4 Post-processing. To post-process the raw completions of the
model, we inject each test case candidate into a template of a test
class, which contains the necessary scaffolding to yield an exe-
cutable test case. Similar to the previous tasks, we discard candi-
dates that do not compile. We also remove any duplicates that may
result from querying the model multiple times.

3.4.5 Benchmarks. As methods under test we use the 18 methods
that Table 5 shows. We select them by randomly identifying two
public methods from each of the 9 projects in Table 2.

4 RESULTS
This section presents answers to the three research questions posed
in Section 3.1. Section 5 discusses the results and their broader
impact.

4.1 RQ1: Accuracy
This section presents results on the accuracy of FSLM-based code
generation compared to traditionally built tools.

4.1.1 Code Mutation. Table 3 summarizes our results for the code
mutation task. Given the 1,194 instances extracted from 32 classes
(Section 3.2.5), our FSLM-based tool generates a total of 2,721 mu-
tants, whereas the baseline Major tool generates 2,810 mutants.
Because the model does not guarantee to generate valid code, only
62.5% of the FSLM-generated mutants are compilable, giving a total
of 1,701 usable mutants. On average, our tool changes 3.97 tokens
of the original code, which roughly equals the 4.28 tokens changed
by Major. Besides the raw amount of mutants generated, it is also
important to understand whether the generated mutants are useful.

DynamicBin1D()
DoubleArrayList()

Method: public synchronized double max() {

if (! isIncrementalStatValid) updateIncrementalStats();
return this.max;

}
Test: public static void testMax() {
double[] temp = new double[2];
temp[0] = 8.9;
temp[1] = 1;
DenseDoubleMatrix1D d1Double = new DenseDoubleMatrix1D(temp);
hep.aida.bin.DynamicBin1D d1ynamicBin =

cern.colt.matrix.doublealgo.Statistic.bin(d1Double);

double max = d1ynamicBin.max();
System.out.println("max="+ max);

}
‚Äî
Method:public DoubleArrayList quantiles(DoubleArrayList percentages)
{

return Descriptive.quantiles(sortedElements_unsafe(),percentages);

}
Test:public static void testQuantiles() {

double[] temp = new double[2];
temp[0] = 8.9;
temp[1] = 1;
DenseDoubleMatrix1D d1Double = new DenseDoubleMatrix1D(temp);
hep.aida.bin.DynamicBin1D d1ynamicBin =

cern.colt.matrix.doublealgo.Statistic.bin(d1Double);

DoubleArrayList quantiles = d1ynamicBin.quantiles(new
DoubleArrayList(new double[] 0.5,0.75));

System.out.println("quantiles="+ quantiles);

}

for the method DoubleArrayList
Figure 4: Test generation prompt
quantiles(DoubleArrayList percentages), declared in the class DynamicBin1D
from project Colt.

We address this question both quantitatively and qualitatively. As a
quantitative answer, we compute how many of the FSLM-generated
mutants exactly match one of the Major-generated mutants. We
observe an overlap of around 18% of the FSLM-generated mutants.
Under the assumption that Major-generated mutants are useful,
this means that at least 18% of the FSLM-generated mutants are
also useful.

As a qualitative answer, we manually inspect a random sample of
30 of the compilable mutants our tool generates. For each sampled
mutant, we carefully inspect the code and determine whether the
mutation changes the runtime behavior of the code, as opposed
to being an equivalent mutant that preserves the semantics of
the original code. The inspection shows that 90% of the mutants
certainly change the behavior, whereas the remaining 10% either
preserve the semantics or we could not clearly determine its effects.
To better understand the mutants generated by the model and
Major, we automatically classify them based on the kind of code
transformation. We distinguish four classes, as shown in the four
right-most columns of Table 3: (i) deleting a statement, (ii) replacing
one operator with another, (iii) replacing one value with another,
and (iv) some other transformation. The table shows that the distri-
bution of mutants that the FSLM and Major generate clearly differ:

6

A Study of Few-Shot, Pre-Trained Language Models on Code

Conference‚Äô17, July 2017, Washington, DC, USA

Table 3: Mutants generated by our FSLM-based tool and by Major [34].

Generated mutants

Kind of transformation

Total

Overlap w/ Major

Compilable

Delete statement

Replace operator

Replace value

Other

FSLM
FSLM (NL-only)
FSLM (Ex-only)
FSLM (Bad-ex)
FSLM (Small model)
Major

2,721
0
2,645
2,595
2,487
2,810

18.4%
-
17.7%
19.0%
15.4%
100.0%

62.5%
-
57.6%
64.5%
53.8%
100.0%

31.8%
-
35.2%
29.4%
30.0%
4.9%

9.0%
-
7.0%
9.1%
4.2%
48.6%

34.7%
-
36.3%
37.1%
33.1%
35.8%

24.5%
-
21.6%
24.4%
32.8%
0.0%

While Major mostly replaces operators and values, the model gen-
erates a more diverse set of mutants, suggesting that the two tools
complement each other.

Finally, we manually study another random sample of 30 mutants
produced by each tool to get qualitative insights into the differences
between the two tools. We make two interesting observations:

‚Ä¢ The FSLM model generates mutants that Major cannot gener-
ate based on its built-in mutation operators [34]. For example,
these FSLM-generated mutants include adding a constant to
an integer (e.g., turning nanos into nanos + 1) and changing
methods to semantically similar ones (e.g., turning Math.min
into Math.max).

‚Ä¢ A relatively large amount of the FSLM-generated mutants
(7/30=23%) replace an expression with null. While this yields
mutant that change the semantics, the high amount is still
surprising.

Overall, these results show that our FSLM-based tool, while not
generating exactly the same mutants as an existing tool, nonetheless
creates a large number of useful mutants with minimal effort.

4.1.2 Generating Oracles from Natural Language Documentation.
This section compares the accuracy of (metamorphic) test oracle
generators, namely, the state-of-the-art MeMo [11] and its FSLM-
based counterpart. To measure accuracy, we compare all generated
oracles against a ground truth consisting of 299 test oracles that
we wish to extract from the documentation of methods from the
projects listed on Table 2. Specifically, we measure precision (ùëÉùëü )
and recall (ùëÖùëí) as follows:
ùëÉùëü = # of correctly generated oracles

ùëÖùëí = # of correctly generated oracles
# of all ground truth oracles
In addition, we report the F1-score, defined as the harmonic
mean of precision and recall. Table 4 shows the results for each
of the studied libraries. Across all projects, the FSLM-based oracle
generator achieves an F1-score of 0.60, which slightly outperforms
MeMo‚Äôs F1-score of 0.59. Comparing precision and recall shows
that the model tends to generate oracles much more precisely, with
a precision of 0.82 instead of MeMo‚Äôs precision of 0.64.

# of all generated oracles

To understand the strengths and weakness of the two approaches,
we manually study some of the oracles. On the one hand, we in-
spect those oracles that the model predicts correctly while MeMo
misses them, which are nine oracles in total. Three of the nine
oracles are cases where there exist multiple oracles for a single
method, and the model discovers one, whereas MeMo discovers
the other. This highlights a limitation of our prompt design, which
enables the model to predict only one oracle per method. One could

7

Table 4: Effectiveness of test oracle generation.

Project

JDK
Colt
Guava
GWT
Graphstream
Apache commons
Hibernate
Elasticsearch
Weka

Total

FSLM

MeMo [11]

Precision Recall

F1 Precision Recall

F1

0.84
0.65
0.88
0.79
1.00
0.79
0.00
0.00
0.50

0.82

0.54 0.66
0.42 0.51
0.52 0.65
0.27 0.40
0.70 0.82
0.58 0.67
0.00 0.00
0.00 0.00
0.50 0.50

0.47 0.60

0.50
0.61
0.83
0.65
1.00
0.66
0.00
0.00
0.43

0.64

0.55 0.52
0.42 0.50
0.67 0.74
0.27 0.38
0.70 0.82
0.82 0.73
0.00 0.00
0.00 0.00
0.50 0.46

0.54 0.59

remedy this limitation by prompting for a list of oracles, similar
to the prompt for code mutations, or by querying the model mul-
tiple times with a higher temperature, similar to what we do for
test generation. The remaining six oracles are all related to MeMo
incorrectly capturing longer or nested pieces of code. For exam-
ple, the documentation ‚ÄúCalling getSource() is equivalent to calling
getSnapshot().getSource()‚Äù is translated by MeMo to an equiva-
lence between getSource() and getSnapshot(), which is incorrect.
In contrast, the model correctly predicts the equivalence between
getSource() and getSnapshot().getSource().

On the other hand, we also inspect the six instances where
the model misses an oracle that MeMo can predict. For two
of these oracles, the model ‚Äúinvents‚Äù code seemingly out of
thin air. For example, the documentation ‚ÄúThis is equivalent
to, but not necessarily implemented as, !(Float.isInfinite(value)
| | Float.isNaN(value)).‚Äù leads to an incorrect prediction of the
model saying that com.google.common.primitives.Float.isFinite
and java.lang.Float.isFinite are equivalent.

Overall, the FSLM-based oracle generator achieves results that
are on par, and even slightly better, than those of a state-of-the-art
tool based on a set hard-coded rules and heuristics.

4.1.3 Test Case Generation. Table 5 summarizes the results of gen-
erating test cases with our FSLM-based approach and with Ran-
doop [2] on 18 methods. The table reports the amount of compilable
tests (column ‚ÄúCT‚Äù), the average size of tests in number of lines of
code (column ‚ÄúTS‚Äù), and the line coverage that the tests achieve (col-
umn ‚ÄúLC‚Äù). We measure coverage using JaCoCo [1]. We notice from

Conference‚Äô17, July 2017, Washington, DC, USA

Patrick Barei√ü, Beatriz Souza, Marcelo d‚ÄôAmorim, and Michael Pradel

Table 5: Analysis of the test cases generated by our FSLM-based test generator and Randoop for the considered methods. The table presents (1) the number of
compilable test (CT) cases; (2) the average test size (TS) of the generated tests and (3) the line coverage (LC) achieved by them.

Project

Method

FSLM

Randoop

Combined

DoubleArrayList quantiles(DoubleArrayList percentages)
double moment(int k, double c)
String parent()
IndexRequest source(Map source, XContentType contentType)
boolean isClient()
UncaughtExceptionHandler getUncaughtExceptionHandler()

Colt
Colt
ElasticSearch
ElasticSearch
GWT
GWT
Graphstream boolean contains(Edge edge)
Graphstream boolean equals(Path p)
Guava
Guava
Hibernate
Hibernate
JDK
JDK
Math
Math
Weka
Weka

HashCode fromLong(long hash)
int writeBytesTo(byte[] dest, int offset, int maxLength)
Short toShort(Boolean value)
Boolean fromString(String string)
Object remove(int index)
boolean contains(Object o)
Vector1D scalarMultiply(double a)
double distanceSq(Vector1D p1, Vector1D p2)
AlgVector add(AlgVector other)
Instance getAsInstance(Instances model, Random random)

# CT

TS

LC

# CT

12
7
4
8
3
0
1
42
6
37
3
17
13
7
28
9
5
0

202

11
13
4
10
7
0
12
17
8
10
4
7
15
9
7
6
10
0

11

29%
17%
8%
13%
6%
0%
12%
31%
12%
32%
24%
22%
5%
3%
30%
23%
28%
0%

14%

29
49
1
66
1
1
49
44
28
50
3
35
70
44
58
51
47
56

682

TS

15
16
202
35
7
7
85
112
12
23
6
19
19
26
10
17
35
15

31

LC

26%
15%
5%
5%
6%
6%
9%
11%
7%
17%
20%
22%
3%
1%
24%
18%
17%
8%

10%

LC

39%
19%
8%
13%
6%
6%
13%
31%
12%
34%
24%
22%
5%
3%
32%
24%
28%
8%

16%

Total

these results that, overall, the model achieves higher code coverage
than Randoop (14% vs. 10%). This result is particularly remarkable
as Randoop generates more than three times the number of tests
the model generates (202 vs. 682 tests). Moreover, on average, the
size of the tests generated by the model are much smaller than the
tests generated by Randoop (11 vs. 31 lines).

On a closer analysis of the tests generated by each approach,
for each of the 18 methods, we can see that Randoop successfully
generates tests for all 18 methods under test. In contrast, the model
successfully generates tests for only 16 of them. More specifically,
(i) for 14 methods, the tests generated by the model achieve higher
coverage than the tests generated by Randoop; (ii) for two methods,
the tests generated by both approaches achieve the same cover-
age; (iii) for two methods, the tests generated by Randoop achieve
higher coverage than the tests generated by the model. These are
exactly the two methods for which the model fails to generate any
compilable tests.

These results provide initial evidence indicating that FSLM-based
tools can outperform state-of-the-art test generation tools. We also
calculate the coverage achieved by combining the tests generated
by both approaches. The results can be seen in the last column of Ta-
ble 5. Interestingly, the coverage achieved by the combination of the
tests (16%) is superior to the coverage achieved by the tests of each
approach individually. As an example, the coverage achieved by the
combination of the tests is considerably higher when considering
the quantiles method of the Colt project. In this case, individually,
the tests generated by the model achieve 29% line coverage and the
tests generated by Randoop achieve 26% line coverage. Combined,
the tests generated by both approaches achieve 39% line coverage.

Summary of RQ1: FSLM-based tools perform surprisingly well
across the different tasks, being on par or complimentary, and
for test generation even better, than the handcrafted tools we
compare against.

4.2 RQ2: Impact of Prompt
By default, our prompts contain both natural language task de-
scriptions and input-output examples. This section reports on the
impact of using different prompt variants. For each of the tasks, we
consider the following prompt variants: Only natural language de-
scription (NL-only); Only input-output examples (Ex-only); Poorly
chosen examples (Bad-ex).

4.2.1 Code Mutation. For mutant generation, NL-only means the
prompt includes only the natural language text at the top of Figure 2,
Ex-only means we keep everything but the NL description, and Bad-
ex means we include additional examples where our FSLM-based
tool should not generate mutants. For example, we add an import
statement as an example, but leave the mutants section empty. The
idea is to test how robust the model is to adversarial or poorly
chosen examples.

The middle rows in Table 3 show the results obtained with these
variants. Running NL-only does not produce promising results
since it is missing the guiding output format from the examples. We
attempt to "fix" the prompt by including more detailed descriptions
on how to format the output (i.e. we add "Return the result in the
format original |==> replacement as part of a list numbered using
‚Äô-‚Äô." to the prompt), but the output format remains inconsistent,
giving no results. This means examples play a large part in solving
this task using an FSLM. Looking at the results for Ex-only reveals

8

A Study of Few-Shot, Pre-Trained Language Models on Code

Conference‚Äô17, July 2017, Washington, DC, USA

Table 6: Line coverage achieved by the tests generated by our FSLM-based test
generator with different prompts and a smaller model.

Variant of the approach

Line coverage

FSLM
FSLM w/o example (NL-only)
FSLM w/o NL descr. (Ex-only)
FSLM w/ random example (Bad-ex)
FSLM w/ small model

14%
12%
9%
8%
10%

A likely explanation for this discrepancy across the tasks is that
the way natural language descriptions are used in the second task
differs from how it is used in the other two tasks (Section 3.3.3).
Consequently, to more uniformly compare the tasks, we also run an
experiment with a prompt where the natural language description
is in the form of a task description. This prompt yields an F1 score
of 0.54, i.e., substantially worse than our default prompt. These
results suggest that the quality of the examples are relatively more
important than the NL description for this task.

4.2.3 Test Case Generation. In this task, the method under test
and a list of helper constructors and methods is always provided
to the prompt. Therefore, for NL-only we remove the input-output
example, for Ex-only we remove the natural language description,
and for Bad-ex we provide a different input-output example, which
we randomly select from a different project.

Table 6 reports the results of these experiments. Overall, we can
see that, regarding line coverage, the default prompt achieves a
higher line coverage of 14%, followed by variation (NL-only) with
12% coverage, then variation (Ex-only) with 12% coverage, and
finally variation (Bad-ex) with only 8% coverage. These results
indicate that a natural language description can be even more im-
portant than an input-output example for test generation (12% vs.
9%). Moreover, an input-output example more related to the method
under test, i.e., from the same class in our case, can add more value
than a random example unrelated to the method under test (14% vs.
8%).

Summary of RQ2: Adding a brief natural language description
is an easy way to help (or at least not hurt) the FSLM-based
tools. Furthermore, we find that providing suitable examples is
crucial for the model to make effective predictions.

4.3 RQ3: Impact of Model Size
Training larger language models on more data often results in
performance improvements for downstream tasks [13]. By default,
we use the ‚ÄúDavinci‚Äù model of Codex, which currently is the largest
model offered via the OpenAI API. Since larger models come with
a hefty computational price [28], we also measure the impact of
using a smaller model. To this end, we repeat our experiments with
the ‚ÄúCushman‚Äù model of Codex, which is a derivative of a small
model trained by Chen et al. [15].

4.3.1 Code Mutation. The ‚ÄúFSLM w/ small model‚Äù row of Table 3
shows the impact of using a smaller model on code mutation. Sev-
eral metrics of success clearly drop, e.g., the total number of gen-
erated mutants (from 2,721 to 2,487) and the number of mutants

Figure 5: Oracle generation results. Comparison of prompt variations.

that less generated mutants compile, with a margin of 5%. This is
interesting as the textual description is only a single sentence in this
task and shows an easy way to improve performance over using a
prompt without it. Moreover, we observe the following behavior
for the Bad-ex variant of the prompt. The overlap with Major and
percentage of mutants that compile are actually slightly higher
than for our default approach. This is surprising in that a deliberate
attempt to worsen the predictions instead slightly improves the
results.

4.2.2 Generating Oracles from Natural Language Documentation.
For oracle generation, NL-only means we only add a natural lan-
guage description of the task and some information about format-
ting (e.g., "Extract equivalent pieces of code from the following
comments. Format the result as Code snippet A <-> Code snippet
B."). For Ex-only we remove the part of the prompt that describes
the task in NL (see purple text on Figure 3). This is different from
the style employed for mutant generation though, as in the oracle
extraction prompt the natural language description is part of each
example and not just a general task description. For Bad-ex, we
once again add examples designed to throw off the FSLM by in-
cluding examples that the model should not generate anything for.
For example, we add a method with comment ‚ÄúReturns the largest
value of the given array.‚Äù and leave the oracle section empty.

Figure 5 shows results of the FSLM for the oracle generation
task, when using different prompt variations. The accuracy is not
significantly affected by the different styles of prompt used, except
for NL-only. As for mutant generation, NL-only yields incorrectly
formatted responses, giving no usable results. Again, examples
appear necessary to be able to successfully use FSLMs for this task.
Considering the prompt variant where we remove NL description,
Ex-only, we observe that the difference in performance is negligible
compared to the default prompt, indicating that the helping text is
not as important as it was for mutation generation. Considering the
prompt variant Bad-ex, we observe that the use of bad examples
performs worse compared to other types of prompts. This indicates
that the quality of examples for this task is more important than
for mutant generation.

9

 0 H 0 R ) 6 / 0 ) 6 / 0  1 /  R Q O \  ) 6 / 0  ( [  R Q O \  ) 6 / 0  % D G  H [  ) 6 / 0  6 P D O O  P R G H O                             9 D O X H 3 U H F L V L R Q 5 H F D O O )   6 F R U HConference‚Äô17, July 2017, Washington, DC, USA

Patrick Barei√ü, Beatriz Souza, Marcelo d‚ÄôAmorim, and Michael Pradel

that compile (from 62.5% to 52.8%). These results show that using a
larger model is beneficial for this task.

4.3.2 Generating Oracles from Natural Language Documentation.
When running the test oracle generation using a smaller model, we
discover, surprisingly, that the results we obtain are nearly identical
to the larger model with an F1 score of 0.58 (as compared to 0.6).
Hence, it seems some tasks can be handled by smaller models almost
as well as with larger models.

4.3.3 Test Case Generation. For test case generation, we observe
a significant drop in effectiveness when using the smaller model
(Table 6). The line coverage drops to 10%, i.e., four percent points
less than with the larger model and about the same as with tests
generated by Randoop.

Summary of RQ3: Increasing the model size improves effec-
tiveness, or at least does not negatively affect it, for all three
code generation tasks. For one of the three tasks (oracle gen-
eration), the effect is small though. Given the computational
cost of large models, carefully selecting them for each task is
recommended.

5 DISCUSSION

Prompt design. Designing ‚Äúgood‚Äù prompts is central to the cre-
ation of FSLM-based tools. When answering RQ2, we observe that
examples are very important in prompt design and that natural
language descriptions are often helpful. There are, however, ques-
tions that remain to be evaluated, including (i) how to mine good
examples to create prompts, (ii) whether or not alternating through
examples is useful when the user queries the model multiple times,
and (iii) how sensible the prompts are to the data format.

Model size. Training large-scale models of code may easily cost
hundreds, or even millions, of dollars [28]. Additionally, these large-
scale models are hard to use due to their sheer size, or not being open
to the public in the first place. For our work, we find these models
to be effective, but obtaining the same results with an improved
smaller, open model would make the tools more accessible in the
long run.

Integrating FSLM-based and traditional tools. The conjunction of
low effort to create new code generation tools and the promising
results we obtain indicate that integrating FSLM-based tools with
existing tools can be helpful. For example, the results for the oracle
generation task (Table 4) show different precision-recall tradeoffs
of the two tools. Blending FSLM-based and traditional techniques
seems a promising direction to explore in the future.

Threats to Validity. We do not compare our results across differ-
ent models (except by size), potentially limiting the generalizability
of our findings. While we try to evaluate on a diverse set of tasks,
there are obviously many more code generation tasks not studied
here. The fact that the FSLM-based approach is able to provide
promising results on the first three tasks we study, gives at least
some indication about the potential for other tasks. Finally, we only
evaluated Java-based tools, i.e., our results might not generalize
beyond this language. Prior research shows that large-scale models
perform well across many differing languages [15].

10

6 RELATED WORK

Studies of neural models of code. As neural models of code be-
come more popular for a diverse set of tasks, many, similar to us,
have begun investigating the details of these models. This comes in
multiple forms, such as evaluating a series of similar models [55] or
models with the same architecture but differing size [15]. Another
approach is to apply a model of code to multiple downstream tasks
and compare its performance, e.g., by fine-tuning a transformer
model to perform tasks similar to the ones we explore in our re-
search [40]. What sets this paper apart is that (1) we investigate
few-shot learning, requiring less training data as compared to fine-
tuning, (2) we compare against commonly used traditional tools,
while others compare neural approaches against each other, and
(3) we target a different set of tasks.

Language models in software engineering. Degiovanni and Pa-
padakis [18] use a pre-trained language model for mutation testing
by masking one token at a time and asking the model to predict an
alternative, which is then considered a mutation. Instead, we study
using a generative model for end-to-end mutant generation, which
often changes multiple tokens at a time. Several papers [7, 15] study
language model-based code generation from short natural language
descriptions. In contrast to our work, there offer no comparison
to traditional tools and focus only on this single task. Jain et al.
[32] use generative language models for program synthesis given
a natural description of the desired functionality and some code
examples that are likely similar to the expected code. They propose
a ‚Äúcontext bank‚Äù of examples to provide in the prompt, which is an
idea one could also adapt for our tasks.

Generative language models in general. Since the introduction of
Transformers [51], generative language modeling has seen huge
progress. Large models, such as GPT-2 [46], shown generative lan-
guage models to perform well across different tasks when fine-
tuning them or in a few-shot setting [13, 50]. Predictions of future
performance promise that these models have the potential to even
further improve their abilities [31, 35]. While these models are eval-
uated on various tasks, we are not aware of any other systematic
study of few-shot models on different code generation tasks.

Neural software analysis. Our study is part of a larger stream of
work on neural models of software [44]. An important question is
how to embed code into a vector representation. Several approaches,
e.g., based on AST paths [5], control flow graphs [52], ASTs [56],
and a combination of token sequences and a graph representation
of code [29] have been proposed. The general-purpose generative
model used here does not explicitly embed code into a vector repre-
sentation, but instead relies on the ability of transformers [51] to rea-
son about long-range dependencies. Neural models of code address
a wide range of problems, e.g., code completion [4, 8, 36], type pre-
diction [26, 39, 45, 53], program repair [19, 25], code search [23, 48],
and making predictions about code changes [12, 30]. All the above
approaches address a specific problem with a model designed for
this problem. Instead, our work studies how successful a general-
purpose model is at competing with non-neural code manipulation
tools.

A Study of Few-Shot, Pre-Trained Language Models on Code

Conference‚Äô17, July 2017, Washington, DC, USA

7 CONCLUSIONS
This paper studies the strengths and limitations of few-shot, pre-
trained language models for three popular code generation tasks. By
systematically comparing the recently proposed Codex model [15]
against three traditionally built tools, we find that our model-based
tools complement, are on par with, or even exceed the baseline
tools. At the same time, creating a new FSLM-based tool based
on our methodology is relatively simple. While our study shows
promising results, we believe these are only first steps in applying
few-shot learning to software engineering problems.

REFERENCES
[1] 2022. JaCoCo Java Code Coverage Library (2022). https://www.jacoco.org/jacoco/

Accessed: 2022-05-02.

[2] 2022. Randoop: Automatic unit test generation for Java (2022). https://randoop.

github.io/randoop/ Accessed: 2022-05-02.

[3] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
2021. Unified Pre-training for Program Understanding and Generation. CoRR
abs/2103.06333 (2021). arXiv:2103.06333 https://arxiv.org/abs/2103.06333
[4] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating
Sequences from Structured Representations of Code. In 7th International Con-
ference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019. https://openreview.net/forum?id=H1gKYo09tX

[5] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: learning
distributed representations of code. Proc. ACM Program. Lang. 3, POPL (2019),
40:1‚Äì40:29. https://doi.org/10.1145/3290353

[6] Saswat Anand, Edmund K. Burke, Tsong Yueh Chen, John Clark, Myra B. Cohen,
Wolfgang Grieskamp, Mark Harman, Mary Jean Harrold, Phil McMinn, Antonia
Bertolino, J. Jenny Li, and Hong Zhu. 2013. An orchestrated survey of methodolo-
gies for automated software test case generation. Journal of Systems and Software
86, 8 (2013), 1978‚Äì2001. https://doi.org/10.1016/j.jss.2013.02.061

[7] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le,
and Charles Sutton. 2021. Program Synthesis with Large Language Models. CoRR
abs/2108.07732 (2021). arXiv:2108.07732 https://arxiv.org/abs/2108.07732
[8] Gareth Ari Aye and Gail E. Kaiser. 2020. Sequence Model Design for Code
Completion in the Modern IDE. CoRR abs/2004.05249 (2020). arXiv:2004.05249
https://arxiv.org/abs/2004.05249

[9] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022.
GPT-NeoX-20B: An Open-Source Autoregressive Language Model. arXiv preprint
arXiv:2204.06745 (2022).

[10] Arianna Blasi, Alberto Goffi, Konstantin Kuznetsov, Alessandra Gorla, Michael D.
Ernst, Mauro Pezz√®, and Sergio Delgado Castellanos. 2018. Translating code
comments to procedure specifications. In Proceedings of the 27th ACM SIGSOFT
International Symposium on Software Testing and Analysis, ISSTA 2018, Amsterdam,
The Netherlands, July 16-21, 2018, Frank Tip and Eric Bodden (Eds.). ACM, 242‚Äì253.
https://doi.org/10.1145/3213846.3213872

[11] Arianna Blasi, Alessandra Gorla, Michael D. Ernst, Mauro Pezz√®, and Antonio
Carzaniga. 2021. MeMo: Automatically identifying metamorphic relations in
Javadoc comments for test automation. Journal of Systems and Software (2021).
[12] Shaked Brody, Uri Alon, and Eran Yahav. 2020. A Structural Model for Contextual

Code Changes. In OOPSLA.

[13] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
In Advances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/
hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html

[14] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from examples
to improve code completion systems. In European Software Engineering Conference
and International Symposium on Foundations of Software Engineering (ESEC/FSE).
ACM, 213‚Äì222.

[15] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de
Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy
Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,

11

Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens
Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,
Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss,
Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,
Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike,
Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight,
Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario
Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Eval-
uating Large Language Models Trained on Code. CoRR abs/2107.03374 (2021).
arXiv:2107.03374 https://arxiv.org/abs/2107.03374

[16] Tsong Y Chen, Shing C Cheung, and Shiu Ming Yiu. 1998. Metamorphic testing: a
new approach for generating next test cases. Technical Report. Technical Report
HKUST-CS98-01, Department of Computer Science, Hong Kong.

[17] Christoph Csallner and Yannis Smaragdakis. 2004. JCrasher: an automatic ro-
bustness tester for Java. Software Prac. Experience 34, 11 (2004), 1025‚Äì1050.
[18] Renzo Degiovanni and Mike Papadakis. 2022. ùúáBERT: Mutation Testing using
Pre-Trained Language Models. CoRR abs/2203.03289 (2022). https://doi.org/10.
48550/arXiv.2203.03289 arXiv:2203.03289

[19] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang.
2020. Hoppity: Learning Graph Transformations to Detect and Fix Bugs in
Programs. In 8th International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.
net/forum?id=SJeqs6EFvB

[20] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of
the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20
November 2020 (Findings of ACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and
Yang Liu (Eds.). Association for Computational Linguistics, 1536‚Äì1547. https:
//doi.org/10.18653/v1/2020.findings-emnlp.139

[21] Gordon Fraser and Andrea Arcuri. 2011. EvoSuite: automatic test suite generation
for object-oriented software. In SIGSOFT/FSE‚Äô11 19th ACM SIGSOFT Symposium
on the Foundations of Software Engineering (FSE-19) and ESEC‚Äô11: 13th European
Software Engineering Conference (ESEC-13), Szeged, Hungary, September 5-9, 2011.
416‚Äì419.

[22] Alberto Goffi, Alessandra Gorla, Michael D. Ernst, and Mauro Pezz√®. 2016. Au-
tomatic generation of oracles for exceptional behaviors. In Proceedings of the
25th International Symposium on Software Testing and Analysis, ISSTA 2016, Saar-
br√ºcken, Germany, July 18-20, 2016, Andreas Zeller and Abhik Roychoudhury
(Eds.). ACM, 213‚Äì224. https://doi.org/10.1145/2931037.2931061

[23] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In
Proceedings of the 40th International Conference on Software Engineering, ICSE 2018,
Gothenburg, Sweden, May 27 - June 03, 2018, Michel Chaudron, Ivica Crnkovic,
Marsha Chechik, and Mark Harman (Eds.). ACM, 933‚Äì944. https://doi.org/10.
1145/3180155.3180167

[24] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with
Data Flow. In 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/
forum?id=jLoC4ez43PZ

[25] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish K. Shevade. 2017. DeepFix:
Fixing Common C Language Errors by Deep Learning. In Proceedings of the Thirty-
First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco,
California, USA, Satinder P. Singh and Shaul Markovitch (Eds.). AAAI Press,
1345‚Äì1351. http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603
[26] Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, and Miltiadis Allamanis. 2018.
Deep learning type inference. In Proceedings of the 2018 ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, No-
vember 04-09, 2018, Gary T. Leavens, Alessandro Garcia, and Corina S. Pasareanu
(Eds.). ACM, 152‚Äì162. https://doi.org/10.1145/3236024.3236051

[27] Vincent J. Hellendoorn, Sebastian Proksch, Harald C. Gall, and Alberto Bacchelli.
2019. When Code Completion Fails: a Case Study on Real-World Completions.
In ICSE.

[28] Vincent J. Hellendoorn and Anand Ashok Sawant. 2022. The growing cost
of deep learning for source code. Commun. ACM 65, 1 (2022), 31‚Äì33. https:
//doi.org/10.1145/3501261

[29] Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and
David Bieber. 2020. Global Relational Models of Source Code. In 8th International
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020. OpenReview.net. https://openreview.net/forum?id=B1lnbRNtwr
[30] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. CC2Vec: Dis-

tributed Representations of Code Changes. In ICSE.

[31] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, et al. 2022. Training Compute-Optimal Large Language

Conference‚Äô17, July 2017, Washington, DC, USA

Patrick Barei√ü, Beatriz Souza, Marcelo d‚ÄôAmorim, and Michael Pradel

[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le,
and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 (2022).

[55] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent J. Hellendoorn. 2022. A
Systematic Evaluation of Large Language Models of Code. CoRR abs/2202.13169
(2022). arXiv:2202.13169 https://arxiv.org/abs/2202.13169

[56] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong
Liu. 2019. A Novel Neural Source Code Representation based on Abstract Syntax
Tree. In ICSE.

[57] Hao Zhong, Lu Zhang, Tao Xie, and Hong Mei. 2009. Inferring Resource Specifi-
cations from Natural Language API Documentation. In International Conference
on Automated Software Engineering (ASE). 307‚Äì318.

Models. arXiv preprint arXiv:2203.15556 (2022).

[32] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh
Parthasarathy, Sriram Rajamani, and Rahul Sharma. 2022. Jigsaw: Large Language
Models meet Program Synthesis. In ICSE.

[33] Yue Jia and Mark Harman. 2010. An analysis and survey of the development of
mutation testing. IEEE transactions on software engineering 37, 5 (2010), 649‚Äì678.
[34] Ren√© Just. 2014. The major mutation framework: efficient and scalable mutation
analysis for Java. In International Symposium on Software Testing and Analysis,
ISSTA ‚Äô14, San Jose, CA, USA - July 21 - 26, 2014, Corina S. Pasareanu and Darko
Marinov (Eds.). ACM, 433‚Äì436. https://doi.org/10.1145/2610384.2628053
[35] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,
Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).
[36] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code Predic-
tion by Feeding Trees to Transformers. In 43rd IEEE/ACM International Conference
on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 150‚Äì162.
https://doi.org/10.1109/ICSE43902.2021.00026

[37] Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Automated
program repair. Commun. ACM 62, 12 (2019), 56‚Äì65. https://doi.org/10.1145/
3318162

[38] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-Task Learning based

Pre-Trained Language Model for Code Completion. In ASE.

[39] Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019. NL2Type: Inferring
JavaScript function types from natural language information. In Proceedings of
the 41st International Conference on Software Engineering, ICSE 2019, Montreal,
QC, Canada, May 25-31, 2019. 304‚Äì315. https://doi.org/10.1109/ICSE.2019.00045
[40] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader-Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks.
In 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021,
Madrid, Spain, 22-30 May 2021. IEEE, 336‚Äì347. https://doi.org/10.1109/ICSE43902.
2021.00041

[41] Martin Monperrus. 2018. Automatic software repair: a bibliography. ACM

Computing Surveys (CSUR) 51, 1 (2018), 1‚Äì24.

[42] Carlos Pacheco, Shuvendu K. Lahiri, Michael D. Ernst, and Thomas Ball. 2007.
Feedback-Directed Random Test Generation. In International Conference on Soft-
ware Engineering (ICSE). IEEE, 75‚Äì84.

[43] Mike Papadakis, Marinos Kintis, Jie Zhang, Yue Jia, Yves Le Traon, and Mark
Harman. 2019. Mutation testing advances: an analysis and survey. In Advances
in Computers. Vol. 112. Elsevier, 275‚Äì378.

[44] Michael Pradel and Satish Chandra. 2022. Neural software analysis. Commun.

ACM 65, 1 (2022), 86‚Äì96. https://doi.org/10.1145/3460348

[45] Michael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra. 2020. Type-
Writer: Neural Type Prediction with Search-based Validation. In ESEC/FSE ‚Äô20:
28th ACM Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, Virtual Event, USA, November 8-13, 2020.
209‚Äì220. https://doi.org/10.1145/3368089.3409715

[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
et al. 2019. Language models are unsupervised multitask learners. OpenAI blog
1, 8 (2019), 9.

[47] Veselin Raychev, Martin T. Vechev, and Eran Yahav. 2014. Code completion
with statistical language models. In ACM SIGPLAN Conference on Programming
Language Design and Implementation, PLDI ‚Äô14, Edinburgh, United Kingdom - June
09 - 11, 2014. 44.

[48] Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish
Chandra. 2018. Retrieval on source code: a neural code search. In Proceedings of the
2nd ACM SIGPLAN International Workshop on Machine Learning and Programming
Languages. ACM, 31‚Äì41.

[49] Domenico Serra, Giovanni Grano, Fabio Palomba, Filomena Ferrucci, Harald C.
Gall, and Alberto Bacchelli. 2019. On the Effectiveness of Manual and Automatic
Unit Test Generation: Ten Years Later. In 2019 IEEE/ACM 16th International
Conference on Mining Software Repositories (MSR). 121‚Äì125. https://doi.org/10.
1109/MSR.2019.00028

[50] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay
Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing
nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990
(2022).

[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In NIPS. 6000‚Äì6010. http://papers.nips.cc/paper/7181-attention-is-all-
you-need

[52] Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang. 2020. Learning Semantic
Program Embeddings with Graph Interval Neural Network. In OOPSLA.
[53] Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig. 2020. LambdaNet: Proba-
bilistic Type Inference using Graph Neural Networks. In 8th International Con-
ference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net. https://openreview.net/forum?id=Hkx6hANtwH

12

