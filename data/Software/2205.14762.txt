Rapid Regression Detection in Software Deployments through
Sequential Testing
Chris Sanden
Netflix, Inc
Los Gatos, CA, USA
csanden@netflix.com

Michael Lindon
Netflix, Inc
Los Gatos, CA, USA
mlindon@netflix.com

VachÃ© Shirikian
Netflix, Inc
Los Gatos, CA, USA
vache@netflix.com

2
2
0
2

n
u
J

2
2

]
E
S
.
s
c
[

2
v
2
6
7
4
1
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
The practice of continuous deployment has enabled companies to
reduce time-to-market by increasing the rate at which software
can be deployed. However, deploying more frequently bears the
risk that occasionally defective changes are released. For Internet
companies, this has the potential to degrade the user experience
and increase user abandonment. Therefore, quality control gates are
an important component of the software delivery process. These
are used to build confidence in the reliability of a release or change.
Towards this end, a common approach is to perform a canary test
to evaluate new software under production workloads. Detecting
defects as early as possible is necessary to reduce exposure and to
provide immediate feedback to the developer.

We present a statistical framework for rapidly detecting regres-
sions in software deployments. Our approach is based on sequential
tests of stochastic order and of equality in distribution. This enables
canary tests to be continuously monitored, permitting regressions
to be rapidly detected while strictly controlling the false detection
probability throughout. The utility of this approach is demonstrated
based on two case studies at Netflix.

CCS CONCEPTS
â€¢ Mathematics of computing â†’ Hypothesis testing and con-
fidence interval computation; â€¢ Software and its engineering
â†’ Software testing and debugging; Empirical software valida-
tion; Rapid application development.

KEYWORDS
A/B testing; regression detection; canary release; sequential testing;
experimentation; canary testing; software delivery; anytime-valid
inference; confidence sequences

1 INTRODUCTION
Many companies have invested in optimizing the software release
process to make developers more agile and permit new features to
reach users faster. Over the years, best practices have evolved to
continuous and progressive delivery models, where software can be
deployed at any time with a great deal of automation [10]. This has
resulted in rapid development cycles where changes are rolled out
constantly, dramatically reducing the time taken for new features to
reach end users. However, these changes pose an inherent risk as it
is inevitable that something will go wrong. For Internet companies
such as Netflix, this has the potential to degrade the user experience
and increase user abandonment [13].

While test suites can be successful in catching functional prob-
lems within internal test environments, performance regressions

may, at times, only appear under production workloads [6]. Fur-
thermore, it can be expensive to comprehensively test the quality
and performance of software via internal testing [25]. Maintaining
an integration and testing environment that fully replicates the pro-
duction environment is challenging and perhaps impossible. There
are likely to be many differences between the state of the test envi-
ronment and the state of the production environment. Therefore,
itâ€™s inevitable that software changes will make it past sophisticated
test suites, enter into the production environment, and degrade the
user experience. Quality gates [18] within the software delivery
process are therefore needed that are able to quickly and accurately
detect defective software under production workloads.

1.1 Regression-Driven Experiments
To minimize the risk of deploying defective software, engineering
practices have evolved towards regression-driven experimentation,
namely canary testing [17, 19]. This practice involves exposing a
change to a small group of users or traffic and studying its effects
under production workloads [10]. This approach reduces the nega-
tive impact of defective software by first validating the change on a
subset of the population. If no significant effects are observed, the
change is then rolled out to the entire population.

One common approach to canary testing is to treat it as a con-
trolled experiment [7, 22, 27]. In this approach, a small portion of
users or traffic are randomly assigned to one of two variants: con-
trol or treatment. The treatment represents the release candidate
and contains the change to be tested while the control is a copy
of the previous version. This approach is related to an A/B test
design found in online controlled experiments. The benefit of this
approach is that it establishes a causal relationship between the
software and a change in performance.

A major shortcoming of existing approaches is the reliance on
fixed-ğ‘› tests. These are statistical tests that provide statistical guar-
antees, such as type I (false detection) error control, when used
strictly once. Historically these were developed to possess optimal
statistical properties in applications where there was only one pos-
sible time to perform the analysis, such as when a statistician is
simply handed a dataset or when all observations arrive simultane-
ously. However, in a canary test, observations can arrive sequen-
tially which provide many opportunities for testing. Herein lies the
difficulty of using fixed-ğ‘› tests in sequential applications; the choice
of when to perform the test. Perform the test too early, and small
regressions may go undetected (a high type II error probability).
Perform the test too late and large regressions have been permitted
to cause harm. Ideally one seeks to detect large regressions as soon
as possible and end the test, preventing further experimental units
from being exposed. In contrast to fixed-ğ‘› methodology, a natural

 
 
 
 
 
 
form of scientific inquiry in sequential applications is to continue
to collect data until a hypothesis has been proven or disproven.

Without sufficient statistical tooling, an erroneous practice of
â€œpeekingâ€ has evolved where fixed-ğ‘› tests are repeatedly applied on
an accumulating set of data [11]. The problem with this practice
is a lack of control over type I errors. One wants to apply the
test frequently, to detect regressions as soon as possible, but the
probability of making a type I error with a fixed-ğ‘› test increases
with each usage [1]. We posit that canary tests should always have
a strict type I error guarantee. Otherwise, the experiment would be
unreliable, producing many false alerts which can increase manual
intervention and reduce delivery cadence.

This issue can be resolved by the development of modern sequen-
tial statistical tests, which are optimized for applications where
data arrives sequentially. The type I error is controlled no matter
how many times they are used, even after every datapoint if desired,
allowing experiments to be continuously monitored. This provides
the ability to perform optional stopping, stopping the experiment
in a data-dependent way as soon as a hypothesis is rejected i.e.
when a regression is detected. Towards this end, sequential tests
for difference-in-means of Gaussian random variables have already
been widely used for online A/B experiments [11, 12, 26]. However,
we argue that performing inference about means is too limited for
canary tests, for not all bugs or performance regressions can be
captured by differences in the mean alone, as the following example
demonstrates. Consider PlayDelay, an important key indicator of
the streaming quality at Netflix, which measures the time taken for
a title to start once the user has hit the play button. It is possible
for the mean of the control and treatment to be the same, but for
the tails of the distribution to be heavier in the treatment, result-
ing in an increased occurrence of extreme values. Large values of
PlayDelay, even if infrequent, are unacceptable and considered a
severe performance regression by our engineering teams. Differ-
ences in the mean are therefore too narrow in scope for defining
performance regressions.

1.2 Contributions
The contributions of this paper are to provide developers with a
new statistical framework for the rapid testing of rich hypotheses
in canary tests. We first extend the statistical definition of bugs and
performance regressions beyond simple differences in the mean,
and provide a decision-theoretic approach to reasoning about the
control and treatment. We propose a sequential canary testing
framework using sequential tests of equality and stochastic order-
ing between distributions to automate testing. These allow any
difference across the entire distribution, not just the mean, to be
tested in real-time and identified as soon as possible. This captures
a much broader class of bugs and performance regressions, while
providing strict type I error guarantees specified by the developer.
Our statistical methodology is based on the confidence sequences
and sequential tests of Howard and Ramdas [9]. We build upon
their tests by providing sequential ğ‘-values for testing equality
and stochastic ordering among distributions, a complementary
stopping rule for accepting approximately true null hypotheses,
and an upper bound on the time taken for the test to stop. The
contribution of sequential ğ‘-values is necessary if one wishes to

Michael Lindon, Chris Sanden, and VachÃ© Shirikian

measure the strength of evidence against the null hypothesis and
also if one wishes to use multiple testing corrections to control
the false discovery rate [12]. The complementary stopping rule
is necessary if one wishes the test to stop in finite time. Without
such a stopping rule the test is "open-ended", meaning it only stops
when the null is rejected. If the null is true, then with probability at
least 1 âˆ’ ğ›¼ the test would run indefinitely. In practise, developers
need the canary test to finish in a finite time. Our contribution
allows developer to stop the canary test and conclude with a high
degree of confidence that any difference, if it exists, is not practically
meaningful.

The complementary stopping rule also provides an upper bound
on the maximum number of samples required by the test to reject
the null or accept an approximate null at a user-specified tolerance
and confidence level. This gives the developer a meaningful max-
imum number of samples required for the canary test and helps
with planning. We also emphasize along the way that estimation is
just as valuable as testing in this context. While the hypothesis test-
ing component is useful for automating the logic that terminates
the experiment, we stress the use of â€œanytime-validâ€ confidence
sequences across all quantiles of each distribution. These can be
visualized at any time, without any need to be concerned about
peeking, and can be used by developers for anytime-valid insights
into what would otherwise be a black box. This gives a precise
description of the differences between control and treatment and
aids in developer learnings.

This paper is organized as follows. In Section 2 we present re-
lated work. Section 3 introduces the statistical methodology, first
by developing the mathematics for the fixed-ğ‘› case, and then gen-
eralizing it to work sequentially. Section 4 demonstrates the utility
of this approach based on two case studies from Netflix. In Section
5 we reflect on our learnings and conclude the paper in Section 6.

2 RELATED WORK
Canary testing systems [22, 27] have been proposed for developers
who wish to conduct regression-driven experiments. These sys-
tems are based on the design of a controlled experiment and rely
upon statistical tests that provide a type I error guarantee when
used exactly once. Due to this, these systems can suffer from the
erroneous practice of â€œpeekingâ€ if developers repeatedly apply the
statistical tests in an attempt to detect regressions early.

At Netflix, Kayenta [7] has been used extensively for canary
testing. Similar to [22, 27], this approach is based on the design of a
controlled experiment. To evaluate the outcome of the canary test,
Kayenta uses fixed-ğ‘› statistical tests such as the Mann-Whitney U
test. While these tests should be used strictly once we have found
that developers will perform the tests multiple times during an
experiment in an attempt to detect regressions early. This motivated
our investment into sequential testing.

The statistics literature on sequential testing dates back at least
to [23] with the introduction of the mixture sequential probability
ratio test (mSPRT). Introductory texts to the subject can be found in
[20, 24]. Johari et al. [11, 12] proposed an â€œanytime-validâ€ sequential
inference framework for differences in the means of Gaussian ran-
dom variables using the mSPRT to provide confidence sequences
and sequential ğ‘-values. In addition to being used in commercial

Rapid Regression Detection in Software Deployments through Sequential Testing

A/B testing software, [26] use this framework for managing the au-
tomated rollout of new software features. However, they formulate
performance regressions as differences in the mean which are too
narrow in scope to capture the breadth of potential problems.

Our approach is based on the results of Howard et al. [9]. The
authors provide confidence sequences that hold uniformly over
time across all quantiles of a distribution, which they use in the
appendix to construct sequential tests of equality in distribution
and stochastic dominance.

3 STATISTICAL METHODOLOGY
In this section, we first define a performance regression in statistical
terms. Then, we will describe the sequential test. For brevity, weâ€™ll
refer to the control and treatment simply as arms A and B of the
experiment.

3.1 Beyond Inference on the Mean
A canary testing system aims to identify bugs and performance
regressions by observing changes in the distribution of key metrics
between arms. There is a strong concern that small performance re-
gressions accumulate and substantially degrade the user experience
over time. Usually, there is a directional notion of â€œdesirableâ€ and
â€œundesirableâ€ changes in the distribution of a metric. Consider, for
example, PlayDelay. PlayDelay is the time taken, in milliseconds,
for a title to begin streaming after being requested by the user. A
shift in the distribution of PlayDelay toward larger values results
in a poorer streaming experience, whereas a shift toward smaller
values could be considered an improvement.

Many of the earlier works formulate performance regressions
as differences in the mean between arms A and B [26], with a
performance regression occurring if the mean shifts in the undesir-
able direction of that metric. We argue from a decision-theoretic
perspective that comparing means alone is insufficient to define
a performance regression. Consider two users, one with a fast in-
ternet connection, the other with a slow internet connection, and
consider the effect of PlayDelay on their satisfaction with the ser-
vice. The former user is less affected by increases in PlayDelay,
as the resulting values are likely still small and manageable, but
the latter user is more affected, as their values were already high
to begin with. As streaming performance is correlated with user
dissatisfaction, increases in PlayDelay increase the risk of the latter
user churning more than they do for the former. In other words,
increases in PlayDelay are not nearly as important for the average
user as they are for users already at risk, and increases in the mean
are not as important as increases in the tail of the distribution.

We have found the concept of stochastic ordering helpful in ele-
vating directional comparisons beyond comparisons of the mean.
Let ğ´ and ğµ denote single observations from arms A and B, respec-
tively. Let ğ¿ denote a loss function that defines the loss associated
with the value of an observation. A developer will then prefer arm
B over arm A if

ğ‘Ÿğ‘ âˆ’ ğ‘Ÿğ‘ = E[ğ¿(ğµ)] âˆ’ E[ğ¿(ğ´)] â‰¤ 0,

(1)

i.e. if the expected loss (risk) is lower for arm B than it is for A.
If the loss is linear, ğ¿(ğ‘¥) = ğ‘˜ğ‘¥, the arm with the smaller risk is
simply the arm with the smaller mean. As illustrated with PlayDelay,

however, many practical loss functions are not linear. Increases of
Î” in PlayDelay are worse for users with already large values i.e.
ğ¿(ğ‘ + Î”) âˆ’ ğ¿(ğ‘) > ğ¿(ğ‘ + Î”) âˆ’ ğ¿(ğ‘) for ğ‘ > ğ‘. This implies that
the loss function is not linear but convex, and clearly illustrates
why defining regressions in terms of the mean alone is insufficient.
Fortunately, ğ‘Ÿğ‘ â‰¤ ğ‘Ÿğ‘ can be guaranteed for all nondecreasing loss
functions ğ¿ if ğµ is stochastically less than or equal to ğ´.

Let ğ´ and ğµ be random variables with distributions ğ¹ğ‘ and ğ¹ğ‘ ,
respectively. ğµ is said to be stochastically less than or equal to ğ´,
written ğµ â‰¼ ğ´, if ğ¹ğ‘ (ğ‘¥) â‰¥ ğ¹ğ‘ (ğ‘¥) âˆ€ğ‘¥ âˆˆ X. If, in addition, ğ¹ğ‘ (ğ‘¥) >
ğ¹ğ‘ (ğ‘¥) for some ğ‘¥ âˆˆ X, then ğµ is said to be strictly stochastically
less than ğ´. This condition is also known as first-order stochastic
dominance. The practical significance behind stochastic ordering is
that ğµ â‰¼ ğ´ if and only if E[ğ¿(ğµ)] â‰¤ E[ğ¿(ğ´)] for all non-decreasing
loss functions ğ¿ [8].

The previous result is helpful because it removes the burden of
fully specifying a developerâ€™s subjective loss function ğ¿. While dif-
ferent developers may disagree about the exact functional form of
ğ¿, all agree that this function is non-decreasing because increases in
the distribution toward larger values are undesirable. If all develop-
ers agree that decreases in the distribution toward smaller values are
undesirable, then all agree that the loss function is non-increasing.
In the end, the exact values of ğ‘Ÿğ‘ and ğ‘Ÿğ‘ are not specifically of
interest. It is only relevant to know if ğ‘Ÿğ‘ â‰¤ ğ‘Ÿğ‘ and vice versa.

To give a concrete example, let us return to studying the dis-
tribution of PlayDelay in a canary test. Developers would like to
catch â€œincreasesâ€ in PlayDelay in the release candidate, while no
action is necessary for â€œdecreasesâ€. Therefore, our null hypothesis
is that observations from arm B are stochastically less than or equal
to observations from arm A, ğµ â‰¼ ğ´, while the alternative is the
complement of this, which we can summarize as

ğ»0 : ğ¹ğ‘ (ğ‘¥) â‰¥ ğ¹ğ‘ (ğ‘¥)
ğ»1 : ğ¹ğ‘ (ğ‘¥) < ğ¹ğ‘ (ğ‘¥)

âˆ€ğ‘¥ âˆˆ X,
for some ğ‘¥ âˆˆ X.

(2)

Note that the complement of the null hypothesis is not ğ´ â‰¼ ğµ as
stochastic ordering is only a partial ordering. If the null is indeed
rejected, it is the responsibility of the developer to use their judge-
ment. Metrics for which â€œdecreasesâ€ are bad can be tested similarly
by replacing â‰¥ and < with â‰¤ and > respectively.

In many cases, the release candidate is not expected to have an
effect on metrics at all. If the developer does not specify a direction,
or one is not available from the application domain, then we test
for any difference in the distribution

ğ»0 : ğ¹ğ‘ (ğ‘¥) = ğ¹ğ‘ (ğ‘¥)
ğ»1 : ğ¹ğ‘ (ğ‘¥) â‰  ğ¹ğ‘ (ğ‘¥)

âˆ€ğ‘¥ âˆˆ X,
for some ğ‘¥ âˆˆ X,

(3)

Although the decision for a canary test to succeed or fail is
formulated in terms of a hypothesis test, we stress that an equally
important objective of a canary test is estimation, that is, about
learning the quantile functions of arms A and B.

3.2 Fixed-ğ‘› Inference
To build up to our full inferential procedure, we first consider the
simpler fixed-ğ‘› case and then generalize it to the sequential case in
Section 3.3. The procedure is quite intuitive: estimate the distribu-
tion and/or quantile functions and then use these estimates to test

hypotheses. For example, one could test the null hypothesis ğ¹ğ‘ = ğ¹ğ‘
by asking if the confidence bands on ğ¹ğ‘ and ğ¹ğ‘ fail to intersect.

The confidence bands for the quantile functions are handled simi-
larly, and both are illustrated in 1.

Michael Lindon, Chris Sanden, and VachÃ© Shirikian

3.2.1 One-Sample Distribution Function Confidence Band. Con-
sider first the Dvoretzkyâ€“Kieferâ€“Wolfowitz (DKWM) inequality
[5, 16]

P[âˆ¥ğ¹ğ‘› âˆ’ ğ¹ âˆ¥âˆ > ğœ€] â‰¤ 2ğ‘’âˆ’2ğ‘›ğœ€2
(4)
where ğ¹ğ‘› (ğ‘¥) = 1
(cid:205)ğ‘›
ğ‘–=1 1[ğ‘¥ğ‘– â‰¤ ğ‘¥] is the empirical distribution func-
ğ‘›
tion and âˆ¥ Â· âˆ¥âˆ is the sup-norm. It follows directly that a confidence
band on the distribution function with coverage probability at least
1 âˆ’ ğ›¼ can be constructed with

,

P[ğ¹ ğ‘™

ğ‘› (ğ›¼, ğ‘¥) â‰¤ ğ¹ (ğ‘¥) â‰¤ ğ¹ğ‘¢

ğ‘› (ğ›¼, ğ‘¥) âˆ€ğ‘¥ âˆˆ X] â‰¥ 1 âˆ’ ğ›¼,

where

ğ¹ğ‘¢
ğ‘› (ğ›¼, ğ‘¥) = min(1, ğ¹ğ‘› (ğ‘¥) + ğœ–ğ‘› (ğ›¼)),
ğ¹ ğ‘™
ğ‘› (ğ›¼, ğ‘¥) = max(0, ğ¹ğ‘› (ğ‘¥) âˆ’ ğœ–ğ‘› (ğ›¼)),

ğœ–ğ‘› (ğ›¼) =

âˆšï¸„

log 2
ğ›¼
2ğ‘›

(for details see example 2.2.4. from [2]).

(5)

(6)

3.2.2 One-Sample Quantile Function Confidence Band. Alterna-
tively, one can construct a confidence band on the quantile function
instead, via
P[ğ‘„ğ‘™

ğ‘› (ğ›¼, ğ‘) âˆ€ğ‘ âˆˆ [0, 1]] â‰¥ 1 âˆ’ ğ›¼,

ğ‘› (ğ›¼, ğ‘) â‰¤ ğ‘„ (ğ‘) â‰¤ ğ‘„ğ‘¢

(7)

where

ğ‘„ğ‘¢
ğ‘„ğ‘™

ğ‘› (ğ›¼, ğ‘) = ğ‘„ğ‘› (ğ‘ + ğœ–ğ‘› (ğ›¼)),
ğ‘› (ğ›¼, ğ‘) = ğ‘„â†’
ğ‘› (ğ‘ âˆ’ ğœ–ğ‘› (ğ›¼)),
ğ‘„ğ‘› (ğ‘) = sup{ğ‘¥ âˆˆ X : ğ¹ğ‘› (ğ‘¥) â‰¤ ğ‘},
(cid:40)sup{ğ‘¥ âˆˆ X : ğ¹ğ‘› (ğ‘¥) < ğ‘}
inf {ğ‘¥ âˆˆ X}

ğ‘› (ğ‘) =

ğ‘„â†’

if ğ‘ > 0
otherwise.

ğ‘„ğ‘› (ğ‘) is the upper empirical quantile function (the right continuous
inverse of the empirical distribution function and equal to the âŒŠğ‘›ğ‘âŒ‹+
1 order statistic of the data). ğ‘„â†’
ğ‘› (ğ‘) is the lower quantile function
(the left continuous inverse of the empirical distribution function
and equal to the âŒˆğ‘›ğ‘âŒ‰ order statistic of the data), with the definition
extended to be inf {ğ‘¥ âˆˆ X} when ğ‘ â‰¤ 0.

3.2.3 Two-Sample Distribution/Quantile Function Confidence Bands.
Suppose one has two sets of i.i.d. samples resulting from an A/B
test. Let the distribution function, empirical distribution function,
sample size and sample from arm A be denoted ğ¹ğ‘, ğ¹ğ‘›ğ‘ , ğ‘›ğ‘ and
ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›ğ‘ respectively. Similarly let the distribution function,
empirical distribution function, sample size and sample from arm
B be denoted ğ¹ğ‘ , ğ¹ğ‘›ğ‘ ğ‘›ğ‘ and ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘›ğ‘ respectively. To obtain
confidence bands on the distribution functions of arm A and arm B
that hold simultaneously with probability at least 1 âˆ’ ğ›¼, then one
can apply a union bound, yielding

Figure 1: Empirical quantile (left) and distribution (right)
functions with 1 âˆ’ ğ›¼/2 confidence bands from equations (5)
and (7), based on ğ‘›ğ‘ = 300 and ğ‘›ğ‘ = 600 samples from
Normal(0, 2) and Normal(0, 0.25) distributions for arms A
and B respectively.

3.2.4 Confidence Sets on the Difference. Ultimately we would like
to test hypotheses about the difference between ğ¹ğ‘ and ğ¹ğ‘, and so
in this section we derive useful confidence statements about the
difference. This section provides a confidence band on the function
ğ‘‘ğ‘,ğ‘ := ğ¹ğ‘ âˆ’ ğ¹ğ‘ and a confidence interval for âˆ¥ğ‘‘ğ‘,ğ‘ âˆ¥âˆ. Figures
that complement figure 1 in visualizing the difference between
distribution functions are provided in Appendix A.

Proposition 3.1. Let the difference between distribution and em-
pirical distribution functions be denoted ğ‘‘ğ‘,ğ‘ := ğ¹ğ‘ âˆ’ ğ¹ğ‘ and ğ‘‘ğ‘›ğ‘,ğ‘›ğ‘ :=
ğ‘›ğ‘ , ğ¹ğ‘¢
ğ¹ğ‘›ğ‘ âˆ’ ğ¹ğ‘›ğ‘ , respectively, with ğ¹ ğ‘™
ğ‘›ğ‘ defined as in equation
(8), then
P[ğ‘‘ğ‘™

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) â‰¤ ğ‘‘ğ‘,ğ‘ (ğ‘¥) â‰¤ ğ‘‘ğ‘¢

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥)âˆ€ğ‘¥ âˆˆ X] â‰¥ 1 âˆ’ ğ›¼,

ğ‘›ğ‘ , ğ¹ğ‘¢

ğ‘›ğ‘ , ğ¹ ğ‘™

(9)

where

ğ‘‘ğ‘¢
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) = ğ¹ğ‘¢
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) = ğ¹ ğ‘™
ğ‘‘ğ‘™

ğ‘›ğ‘ (ğ›¼/2, ğ‘¥) âˆ’ ğ¹ ğ‘™
ğ‘›ğ‘ (ğ›¼/2, ğ‘¥) âˆ’ ğ¹ğ‘¢
The confidence band on ğ‘‘ğ‘,ğ‘ is an immediate consequence of the

ğ‘›ğ‘ (ğ›¼/2, ğ‘¥)
ğ‘›ğ‘ (ğ›¼/2, ğ‘¥).

confidence bands on ğ¹ğ‘ and ğ¹ğ‘ .

Corollary 3.2.

(cid:20)

(cid:20)

P

P

(cid:20)

(cid:20)

ğ‘‘ğ‘,ğ‘ (ğ‘¥) âˆˆ

sup
ğ‘¥ âˆˆX

sup
ğ‘¥ âˆˆX

ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥), sup
ğ‘¥ âˆˆX

ğ‘‘ğ‘¢
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥)

ğ‘‘ğ‘,ğ‘ (ğ‘¥) âˆˆ

inf
ğ‘¥ âˆˆX

inf
ğ‘¥ âˆˆX

ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥), inf
ğ‘¥ âˆˆX

ğ‘‘ğ‘¢
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥)

(cid:21) (cid:21)

(cid:21) (cid:21)

â‰¥ 1 âˆ’ ğ›¼

â‰¥ 1 âˆ’ ğ›¼

ğ‘›ğ‘ (ğ›¼/2, ğ‘¥) â‰¤ ğ¹ğ‘ (ğ‘¥) â‰¤ ğ¹ğ‘¢
P[ğ¹ ğ‘™
ğ¹ ğ‘™
ğ‘›ğ‘ (ğ›¼/2, ğ‘¥) â‰¤ ğ¹ğ‘ (ğ‘¥) â‰¤ ğ¹ğ‘¢
âˆ€ğ‘¥ âˆˆ X] â‰¥ 1 âˆ’ ğ›¼,

ğ‘›ğ‘ (ğ›¼/2, ğ‘¥)
ğ‘›ğ‘ (ğ›¼/2, ğ‘¥)

(8)

Corollary 3.3.

P (cid:2)âˆ¥ğ‘‘ğ‘,ğ‘ âˆ¥âˆ âˆˆ (cid:2)ğ‘™ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼), ğ‘¢ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼)(cid:3) (cid:3) â‰¥ 1 âˆ’ ğ›¼,

(10)

0.00.20.40.60.81.0Quantileâˆ’3âˆ’2âˆ’10123Îµna(Î±/2)Quna(Î±/2,p)Qna(p)Qlna(Î±/2,p)Qunb(Î±/2,p)Qnb(p)Qlnb(Î±/2,p)âˆ’3âˆ’2âˆ’101230.00.20.40.60.81.0Îµna(Î±/2)||Fnbâˆ’Fna||âˆFuna(Î±/2,x)Fna(x)Flna(Î±/2,x)Funb(Î±/2,x)Fnb(x)Flnb(Î±/2,x)Rapid Regression Detection in Software Deployments through Sequential Testing

where

ğ‘™ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼) = max

ğ‘¢ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼) = max

3.2.5 A Test of ğ´ â‰¼ ğµ.

(cid:18)(cid:12)
(cid:12)
inf
(cid:12)
ğ‘¥ âˆˆX
(cid:12)
(cid:18)(cid:12)
(cid:12)
inf
(cid:12)
ğ‘¥ âˆˆX
(cid:12)

ğ‘‘ğ‘¢
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥)

ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥)

ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥)

ğ‘‘ğ‘¢
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

,

(cid:12)
(cid:12)
sup
(cid:12)
(cid:12)
ğ‘¥ âˆˆX
(cid:12)
(cid:12)
sup
(cid:12)
(cid:12)
ğ‘¥ âˆˆX

(cid:19)

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Theorem 3.4. The null hypothesis ğ¹ğ‘ (ğ‘¥) â‰¥ ğ¹ğ‘ (ğ‘¥) âˆ€ğ‘¥ âˆˆ X can be

rejected at the ğ›¼-level if

sup ğ‘‘ğ‘™
ğ‘¥ âˆˆX

ğ‘›ğ‘,ğ‘›ğ‘

(ğ›¼, ğ‘¥) > 0

The ğ‘-value for this test, ğ‘ â‰¼

ğ‘›ğ‘,ğ‘›ğ‘ , is the root of

ğ‘“ (ğ›¼) = âˆ¥ğ‘‘+

ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ âˆ’ ğœ–ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼),
ğ‘›ğ‘,ğ‘›ğ‘ (ğ‘¥) = max(ğ‘‘ğ‘›ğ‘,ğ‘›ğ‘ (ğ‘¥), 0) and ğœ–ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼) = ğœ–ğ‘›ğ‘ (ğ›¼/2) +

(11)

where ğ‘‘+
ğœ–ğ‘›ğ‘ (ğ›¼/2).

The proof is provided in Appendix B. When ğ‘›ğ‘ = ğ‘›ğ‘ = ğ‘›, the

root of equation 25 can be computed analytically as

ğ‘ â‰¼
ğ‘›ğ‘,ğ‘›ğ‘ = 2ğ‘’âˆ’

ğ‘›âˆ¥ğ‘‘+

ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥
2

2
âˆ

.

(12)

When âˆ¥ğ‘‘+
ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ is large the ğ‘-value is small. When ğ‘›ğ‘ â‰  ğ‘›ğ‘ the
root of equation (11) must be found numerically. Some â€œbracketingâ€
root-finding algorithms require the specification of an interval
in which to seek the root, such as the bisection method [3]. To
construct lower and upper bounds on the ğ‘-value, define

ğ‘ğ‘› (ğ‘‘) := 4ğ‘’âˆ’ ğ‘›ğ‘‘2

2

.

(13)

The ğ‘-value is lower-bounded by ğ‘max(ğ‘›ğ‘,ğ‘›ğ‘ ) (âˆ¥ğ‘‘+
bounded by ğ‘min(ğ‘›ğ‘,ğ‘›ğ‘ ) (âˆ¥ğ‘‘+

ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ).

ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ), and upper-

Corollary 3.5. The null hypothesis ğ¹ğ‘ (ğ‘¥) â‰¤ ğ¹ğ‘ (ğ‘¥) âˆ€ğ‘¥ âˆˆ X

(ğ´ â‰½ ğµ) can be rejected at the ğ›¼-level if

inf
ğ‘¥ âˆˆX
The ğ‘-value for this test, ğ‘ â‰½

ğ‘‘ğ‘¢
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) < 0

ğ‘›ğ‘,ğ‘›ğ‘ , is the root of

(14)

ğ‘“ (ğ›¼) = âˆ¥ğ‘‘âˆ’
ğ‘›ğ‘,ğ‘›ğ‘ (ğ‘¥) = max(âˆ’ğ‘‘ğ‘›ğ‘,ğ‘›ğ‘ (ğ‘¥), 0).

ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ âˆ’ ğœ–ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼),

where ğ‘‘âˆ’
3.2.6 A Test of ğ´ ğ‘‘
= ğµ. The null hypothesis ğ´ ğ‘‘
= ğµ is the intersection
of the hypotheses ğ´ â‰¼ ğµ and ğ´ â‰½ ğµ i.e. ğ¹ğ‘ (ğ‘¥) â‰¤ ğ¹ğ‘ (ğ‘¥) and ğ¹ğ‘ (ğ‘¥) â‰¤
ğ¹ğ‘ (ğ‘¥) for all ğ‘¥ âˆˆ X. This means that the results of the previous sec-
tion can be re-purposed. The null hypothesis can be rejected at the
ğ›¼-level if sup ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) < 0. Similarly, the
ğ‘-value can be defined as ğ‘ğ‘›ğ‘,ğ‘›ğ‘ = min(ğ‘ â‰¼
ğ‘›ğ‘,ğ‘›ğ‘ ). When ğ‘›ğ‘ =
ğ‘›ğ‘ = ğ‘›, ğ‘ğ‘›ğ‘,ğ‘›ğ‘ = 4 exp(âˆ’ğ‘›/2âˆ¥ğ‘‘ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥2
âˆ), otherwise it is sandwiched
between ğ‘max(ğ‘›ğ‘,ğ‘›ğ‘ ) (âˆ¥ğ‘‘ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ) and ğ‘min(ğ‘›ğ‘,ğ‘›ğ‘ ) (âˆ¥ğ‘‘ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ).

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) > 0 or inf ğ‘‘ğ‘¢

To gain some intuition for the mathematics, it is useful to recog-

, ğ‘ â‰½

ğ‘›ğ‘,ğ‘›ğ‘

nize that the following are equivalent

â€¢ The 1 âˆ’ ğ›¼ lower confidence bound for âˆ¥ğ‘‘ğ‘,ğ‘ âˆ¥âˆ, ğ‘™ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼), is

greater than zero

â€¢ Either sup ğ‘‘ğ‘™
â€¢ The 1 âˆ’ ğ›¼ confidence band for the difference function ğ‘‘ğ‘,ğ‘

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) > 0 or inf ğ‘‘ğ‘¢

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) < 0

in equation (9) excludes 0 for some ğ‘¥ âˆˆ X

â€¢ The 1 âˆ’ ğ›¼/2 confidence bands for the distribution functions

in equation (8) fail to intersect for some ğ‘¥ âˆˆ X

â€¢ The 1 âˆ’ ğ›¼/2 confidence bands on the quantile functions fail

to intersect for some ğ‘ âˆˆ [0, 1]

These statements are visualized with the help of figures 1 and 6.

Sample Size Calculations. A sample size calculation can be
3.2.7
performed to obtain a 1 âˆ’ ğ›¼ confidence band for ğ‘‘ğ‘,ğ‘ of a desired
radius. When ğ‘›ğ‘ = ğ‘›ğ‘ = ğ‘› a radius of at most ğ‘Ÿ can be achieved
with

(cid:17)

log (cid:16) 4
ğ›¼
ğ‘Ÿ 2

ğ‘› = 2

.

(15)

A choice of ğ‘Ÿ can help reason if the difference is practically meaning-
ful. Suppose one considers differences between distribution func-
tions less than ğœ to be practically irrelevant. By choosing ğ‘Ÿ â‰¤ ğœ/2
the diameter of the confidence band is less than ğœ. If the confi-
dence band contains 0 for all ğ‘¥ âˆˆ X, then sup ğ‘‘ğ‘¢
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) â‰¤ ğœ and
inf ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ â‰¥ âˆ’ğœ, which implies ğ‘¢ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼) â‰¤ ğœ. One can conclude
with confidence 1 âˆ’ ğ›¼ that âˆ¥ğ‘‘ğ‘,ğ‘ âˆ¥âˆ â‰¤ ğœ.

3.3 Sequential Inference
The aforementioned coverage and type I error probabilities of the
statistical procedure constructed from the DKWM inequality hold
when the analysis is performed only once. This is an example of
a fixed-ğ‘› test which is appropriate when the analysis is to be per-
formed only once. This test is, however, not appropriate when data
arrives sequentially and the analysis is intended to be performed
continuously, as is the case with canary testing. To allow continuous
monitoring we seek a confidence sequence

P[ğ¹ ğ‘™

ğ‘› (ğ›¼, ğ‘¥) â‰¤ ğ¹ (ğ‘¥) â‰¤ ğ¹ğ‘¢

ğ‘› (ğ›¼, ğ‘¥) âˆ€ğ‘¥ âˆˆ X âˆ€ğ‘› âˆˆ N] â‰¥ 1 âˆ’ ğ›¼ .

(16)

This extends the confidence band in (5) to hold for all ğ‘› âˆˆ N. This,
in turn, allows the previous results for two samples in section 3.2 to
be extended for all ğ‘›ğ‘, ğ‘›ğ‘ âˆˆ N Ã— N. This can be achieved by using
elegant â€œdrop-inâ€ replacements for ğœ–ğ‘› (ğ›¼) in equation 6. Darling and
Robbins [4] propose using

ğœ–ğ‘› (ğ›¼) =

âˆšï¸‚ (ğ‘› + 1)(2 log ğ‘› âˆ’ log(ğ›¼ (ğ‘›â˜… âˆ’ 1)))
ğ‘›2

.

(17)

This provides an ğ›¼-level confidence sequences that holds for all
ğ‘› â‰¥ ğ‘›â˜…. Szorenyi et al. [21] provide a result that holds âˆ€ğ‘› âˆˆ N by
using

ğœ–ğ‘› (ğ›¼) =

âˆšï¸‚ 1
2ğ‘›
Their results follows directly from a union bound of the DKWM
ğ‘›=1 1/ğ‘›2 = ğœ‹ 2/6. Howard and
inequality in (4) over ğ‘› and using (cid:205)âˆ
Ramdas [9] obtain a tighter confidence sequence with the same
guarantee by using

ğœ‹ 2ğ‘›2
3ğ›¼

(18)

log

.

ğœ–ğ‘› (ğ›¼) = 0.85

âˆšï¸‚ log log(ğ‘’ğ‘›) + 0.8 log(1612/ğ›¼)
ğ‘›

.

(19)

The authors use these results to derive sequential tests of ğ¹ğ‘ (ğ‘¥) â‰¤
ğ¹ğ‘ (ğ‘¥) and ğ¹ğ‘ (ğ‘¥) = ğ¹ğ‘ (ğ‘¥) [4, 9]. We choose to work with equation
(19) moving forward. We complement these results by contributing
sequential ğ‘-values, stopping logic for accepting an approximately
true null hypothesis, and an upper bound on the number of obser-
vations required for the test to stop.

Sequential ğ‘-values. A sequential ğ‘-value satisfies the fol-

3.3.1
lowing

Pğ»0 [ğ‘ğ‘›ğ‘,ğ‘›ğ‘ â‰¤ ğ›¼ for some(ğ‘›ğ‘, ğ‘›ğ‘ ) âˆˆ N Ã— N] â‰¤ ğ›¼ .
ğ‘-values are often requested by developers as a measure of strength
against the null hypothesis and are also necessary inputs for proce-
dures controlling false discovery rate [12]. The sequential ğ‘-value
for testing ğ´ â‰¼ ğµ is equal to the root of (11), except that ğœ–ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼) is
replaced by its new definition in equation (19). When ğ‘›ğ‘ = ğ‘›ğ‘ = ğ‘›,
the sequential ğ‘-value is given by

(20)

ğ‘ â‰¼
ğ‘›ğ‘,ğ‘›ğ‘ =

3624
(cid:33)2

âˆ¥âˆ/2

,

âˆ’log log(ğ‘’ğ‘›)

(cid:32) âˆ¥ğ‘‘+

ğ‘›ğ‘,ğ‘›ğ‘
0.85

ğ‘›

ğ‘’
otherwise it is sandwiched between ğ‘max(ğ‘›ğ‘,ğ‘›ğ‘ ) (âˆ¥ğ‘‘+
ğ‘min(ğ‘›ğ‘,ğ‘›ğ‘ ) (âˆ¥ğ‘‘+

ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ) where

0.8

ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ) and

ğ‘ğ‘› (ğ‘‘) =

3624
(cid:17)2

âˆ’log log(ğ‘’ğ‘›)
0.8

,

ğ‘›

(cid:16) ğ‘‘/2
0.85

ğ‘’

and can be computed numerically via bracketed root finding al-
gorithms. Sequential ğ‘-values for testing ğ´ â‰½ ğµ and ğ´ ğ‘‘
= ğµ are
obtained by replacing âˆ¥ğ‘‘+
ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ with âˆ¥ğ‘‘âˆ’
ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ and âˆ¥ğ‘‘ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ,
respectively. Practically, this permits the analysis to be performed
as frequently as desired while maintaining coverage and type I error
guarantees. In particular, it permits the use of a data-dependent
stopping rule. The null hypothesis can be rejected at the ğ›¼ level as
soon as the corresponding sequential ğ‘-value falls below ğ›¼.

Observations are received in the experiment in no specific order
from arms A and B, they may even arrive at different rates. Let
ğ‘¡ = 1, 2, . . . be an enumeration of N Ã— N in the order in which they
occur. One can define a new sequential ğ‘-value by computing the
running minimum ğ‘ğ‘¡ = min(ğ‘ğ‘¡ âˆ’1, ğ‘ğ‘¡ ) with ğ‘0 = 1, which satisfies
Pğ»0 [ğ‘ğ‘¡ â‰¤ ğ›¼ for some ğ‘¡ âˆˆ N] â‰¤ ğ›¼ .
(21)
Confidence sequences on sup ğ‘‘ğ‘,ğ‘ (ğ‘¥), inf ğ‘‘ğ‘,ğ‘ (ğ‘¥) and âˆ¥ğ‘‘ğ‘,ğ‘ âˆ¥âˆ can
be constructed similarly, and are provided in Appendix D.

3.3.2 Accepting an Approximate Null Hypothesis. One can continue
to observe additional datapoints and tighten confidence bands on
quantile and distribution functions forever. The more datapoints
that are observed, the tighter these confidence bands become. For
this reason, any difference in these functions is guaranteed to be
revealed eventually i.e. the test is asymptotically power 1 [9].

In practice, developers want to stop the test in a finite time,
such as when they are satisfied that the distributions are â€œsimilar
enoughâ€. Let ğœ denote a subjective tolerance specified by the de-
veloper, such that any departure from the null hypothesis by less
than ğœ is practically irrelevant. We provide the developer a comple-
mentary stopping rule such that if the null is not already rejected,
one can at least conclude with confidence that the difference is less
than the desired tolerance.

For testing ğ¹ğ‘ (ğ‘¥) â‰¥ ğ¹ğ‘ (ğ‘¥) âˆ€ğ‘¥ âˆˆ X, we propose to stop when the
time-uniform version of the upper confidence bound on sup ğ‘‘ğ‘,ğ‘ (ğ‘¥),
sup ğ‘‘ğ‘¢
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥), in corollary 3.2 is less than ğœ. This ensures with
confidence at least 1 âˆ’ ğ›¼ that the null is approximately true i.e.
ğ¹ğ‘ (ğ‘¥) â‰¥ ğ¹ğ‘ (ğ‘¥) âˆ’ ğœ for all ğ‘¥ âˆˆ X. When testing ğ¹ğ‘ (ğ‘¥) â‰¤ ğ¹ğ‘ (ğ‘¥) âˆ€ğ‘¥ âˆˆ
X, stopping when inf ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) > âˆ’ğœ ensures with confidence at

Michael Lindon, Chris Sanden, and VachÃ© Shirikian

Table 1: Stopping Rules

Hypothesis

Reject

Approximate Accept

ğ´ â‰¼ ğµ
ğ´ â‰½ ğµ
ğ´ ğ‘‘

= ğµ

sup ğ‘‘ğ‘™
inf ğ‘‘ğ‘¢

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) > 0
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) < 0
ğ‘™ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼) > 0

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) < ğœ
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) > âˆ’ğœ

sup ğ‘‘ğ‘¢
inf ğ‘‘ğ‘™
ğ‘¢ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼) < ğœ

least 1 âˆ’ ğ›¼ that ğ¹ğ‘ (ğ‘¥) â‰¤ ğ¹ğ‘ (ğ‘¥) +ğœ for all ğ‘¥ âˆˆ X. Lastly, when testing
ğ¹ğ‘ = ğ¹ğ‘ , stopping when ğ‘¢ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼) < ğœ ensures with confidence at
least 1 âˆ’ ğ›¼ that |ğ¹ğ‘ (ğ‘¥) âˆ’ ğ¹ğ‘ (ğ‘¥)| â‰¤ ğœ for all ğ‘¥ âˆˆ X.

Stopping Rules. We provide a summary of the stopping rules
3.3.3
for each null hypothesis in table 1 (note that these definitions use
the time-uniform version of ğœ–ğ‘› (ğ›¼) in equation (19)). The probability
that the incorrect conclusion is drawn is at most ğ›¼. Similar "open-
ended" versions of these can be found in Howard and Ramdas [9,
Appendix B.2.]. The stopping logic for testing ğ´ â‰¼ ğµ is quite simple
when stated in plain English. With a slight abuse of mathematical
verbiage, ğ´ â‰¼ ğµ is rejected as soon as the confidence band on ğ‘‘ğ‘,ğ‘
is strictly positive for some ğ‘¥, and approximately accepted when it
is less than ğœ for all ğ‘¥. ğ´ ğ‘‘
= ğµ is rejected as soon as the confidence
band on ğ‘‘ğ‘,ğ‘ excludes zero for some ğ‘¥, and approximately accepted
when it is within âˆ’ğœ and ğœ for all ğ‘¥. Note that ğ‘™ğ‘›ğ‘,ğ‘›ğ‘ > 0 is simply
the logical OR of the rejection criteria of tests ğ´ â‰¼ ğµ and ğ´ â‰½ ğµ,
while ğ‘¢ğ‘›ğ‘,ğ‘›ğ‘ < ğœ is the logical AND of the acceptance criteria.

Alternatively, one neednâ€™t reject immediately. A developer can
continue observing datapoints until the confidence bands on the
quantile or distribution functions are precise enough to satisfy their
curiosity. In this case, the sequential ğ‘-value can be used as a final
measure of evidence against the null hypothesis. One can also obtain
a maximum number of observations required by the canary test
by considering the number required to give a confidence band on
ğ‘‘ğ‘,ğ‘ of desired radius, which allows the developer to reason about
meaningful differences, as discussed in section 3.2.7. When ğ‘›ğ‘ =
ğ‘›ğ‘ = ğ‘›, The number of observations required for a confidence band
of radius ğ‘Ÿ satisfies 2ğœ–ğ‘› (ğ›¼/2) = ğ‘Ÿ , which can be solved numerically
for ğ‘›.

3.4 Count Metrics
In general a datapoint ğ‘– from arm ğ‘˜ in a canary test is a 2-tuple
(ğ‘¥ğ‘˜ğ‘–, ğ‘¡ğ‘˜ğ‘– ) of a measurement ğ‘¥ğ‘˜ğ‘– and an arrival timestamp ğ‘¡ğ‘˜ğ‘– . The
measurement ğ‘¥ğ‘˜ğ‘– is often continuous, like PlayDelay in millisec-
onds, and ğ‘¡ğ‘˜ğ‘– is a timestamp corresponding to when the measure-
ment was taken or when it was received. In addition to testing
hypotheses about the distribution of the measurement, it is also
very relevant to test hypotheses about the timestamps. If the distri-
butions of the measurements are equal among arms, but datapoints
are received at a different rates, then this is also a cause for concern.
An extremely important example is when the metric corresponds
to a particular error. In this example the measurement is simply
an indicator that an error has occurred and the timestamp records
when the error occurred. It would be highly alarming if the release
candidate produced errors at a faster rate. Netflix carefully monitors
playback errors which indicate a failure to play a title. If arm B
experiences a higher volume of errors, then this is a strong signal

Rapid Regression Detection in Software Deployments through Sequential Testing

of a regression. A fixed-ğ‘› approach is simply to test whether the
number of errors produced in a window of time by arm B is greater
or less than arm A. For this reason we refer to these as count metrics.
The raw data for a count metric is simply an ordered sequence
of timestamps. Consequently, we model this as a one-dimensional
point process. Kuo and Yang [14] model the point process of soft-
ware failure times as a time-inhomogeneous Poisson point process,
or equivalently, the epochâ€™s of failure times as a time-inhomogeneous
exponential process. We would prefer, however, not to make such
strong assumptions. Instead, we model the sequence of timestamps
as a general renewal process.

A renewal process is a stochastic process in time with waiting
times between successive timestamps (epochs) drawn i.i.d. from
a renewal distribution. In contrast to the Poisson point process
assumption, no further assumptions are made about the renewal
distribution. Suppose the canary test begins at time ğ‘¡ = 0 and the
current time is T . During this time, a sequence of datapoints has
arrived in arm A at times ğ‘¡ğ‘1, ğ‘¡ğ‘2, . . . ğ‘¡ğ‘ğ‘›ğ‘ . Let ğºğ‘ be the renewal
distribution of arm A. The likelihood for ğºğ‘ given the observed
times is then

ğ‘ ((ğ‘¡ğ‘1, ğ‘¡ğ‘2, . . . , ğ‘¡ğ‘ğ‘›ğ‘ )|ğºğ‘) =

ğ‘›âˆ’1
(cid:214)

ğ‘”ğ‘ (ğ‘¡ğ‘ğ‘–+1 âˆ’ ğ‘¡ğ‘ğ‘– )

ğ‘–=1
ğ‘”ğ‘ (ğ‘¡ğ‘1)(1 âˆ’ ğºğ‘ (T âˆ’ ğ‘¡ğ‘ğ‘›ğ‘ ),

(22)

where ğ‘”ğ‘ is the Radonâ€“Nikodym derivative of ğºğ‘ with respect
to Lebesgue measure. Associated with each event ğ‘¡ğ‘ğ‘– can be an
additional measurement ğ‘¥ğ‘ğ‘– modelled as a random variable from the
distribution ğ¹ğ‘. Our proposal is to sequentially test for differences
in both the measurement distributions ğ¹ğ‘ and ğ¹ğ‘ as well as the
renewal distributions ğºğ‘ and ğºğ‘ . Testing for differences in the
renewal distribution is as simple as feeding the time differences
(ğ‘¡ğ‘2 âˆ’ ğ‘¡ğ‘1), (ğ‘¡ğ‘3 âˆ’ ğ‘¡ğ‘2), . . . for arms A and similarly for B into the
procedure described in section 3.3.

4 CASE STUDIES
In this section, we describe two case studies which demonstrate the
approach described above in the context of canary testing client
applications at Netflix. These tests are based on a controlled experi-
ment where a randomized group of users are assigned to receive the
release candidate while a control group receive the existing version
of the client application. These tests are configured to run until the
null is rejected or approximately accepted. Towards this end, our
approach was deployed as a quality gate in the existing software
delivery pipeline for the client applications under evaluation. These
quality gates were configured to alert developers when a regression
was detected with the release candidate.

4.1 Increase in PlayDelay
In the following case study, we show how the sequential methodol-
ogy successfully detected a performance regression in PlayDelay
and prevented the release candidate from reaching the production
environment. The data is taken from a canary test in which the
behavior of the client application was modified. The null hypothesis
was that observations of PlayDelay in the release candidate should
be stochastically less than or equal to observations in the existing
version (ğ»0 : ğ¹ğ‘ â‰¥ ğ¹ğ‘). Figure 2 shows the sequential ğ‘-value for

this hypothesis as a function of time since the beginning of the
canary test. The ğ‘-value falls below 0.01 after approximately 65
seconds.

Figure 2: Sequential ğ‘-value ğ‘ğ‘¡ from equation (21) for Play-
Delay.

Figure 3 shows that the 0.995 confidence bands for the quantile
functions of arms A and B fail to intersect for many quantiles. While
no quantiles are significantly lower in arm B, many are significantly
greater, revealing a small but significant increase in PlayDelay. This
is perhaps easier to see by considering the 0.99 confidence band on
the difference function in figure 8. The median value of PlayDelay
increased by at least 11 and at most 255 milliseconds in arm B,
while the 75-percentile increased by at least 51 and at most 635
milliseconds (based on a 0.99 confidence interval). If the developer
wishes to get more precise estimates and tighter confidence inter-
vals, they can continue the canary test without sacrificing coverage
guarantees due to the time-uniform nature of these confidence
sequences.

Figure 3: Empirical quantile (left) and distribution (right)
functions of PlayDelay with 0.995 confidence bands from
equations (5) and (7) at time t=100 seconds. Ticks omitted
for confidentiality.

020406080100Time Since Start [s]0.00.20.40.60.81.0Sequential p-valueÎ± = 0.010.00.20.40.60.81.0QuantileQuna(Î±/2,p)Qna(p)Qlna(Î±/2,p)Qunb(Î±/2,p)Qnb(p)Qlnb(Î±/2,p)Time0.00.20.40.60.81.0Funa(Î±/2,x)Fna(x)Flna(Î±/2,x)Funb(Î±/2,x)Fnb(x)Flnb(Î±/2,x)4.2 Drop in Successful Play Starts
While PlayDelay captures the time taken by successful playbacks,
it fails to capture unsuccessful playbacks (playback failures). To
complement PlayDelay, Netflix also closely monitors the number
of successful play starts. Simply, a Successful Play Start (SPS) event
is sent to the central logging system by the client application when-
ever a title begins streaming after the viewer hits play, logging the
successful start of the title. A significant drop in SPS events between
arms therefore implies a significant increase in playback failures,
and is an important metric for detecting software regressions. The
following case study uses real data from a canary test where a
software bug caused playback to fail for multiple devices.

The number of SPS events is an example of a count metric de-
scribed in section 3.4. Consequently, we look at the time-differences
between observed events (epochs) to study differences in the re-
newal distributions. In this example we test ğ¹ğ‘ = ğ¹ğ‘ , where ğ¹ğ‘–
denotes the renewal distribution for arm ğ‘–.

Figure 4: (Left Axis) Sequential ğ‘-value ğ‘ğ‘¡ from equation (21)
for SPS. (Right Axis) Cumulative counts of SPS events over
time for arms A and B. Ticks omitted for confidentiality.

Let the counting process ğ‘ğ‘– (ğ‘¡) be defined as the number of events
observed in the interval (0, ğ‘¡] for arm ğ‘–. These are shown on the
right axis of figure 4. Clearly there are fewer SPS events per unit
time for arm B running the newer client application, suggesting
a bug in the release candidate which prevents some users from
streaming.

The left axis of figure 4 shows the sequential ğ‘-value, which
falls below ğ›¼ = 0.01 in a mere 11.08 seconds. From a different
perspective, figure 7 shows the confidence sequence on âˆ¥ğ‘‘ğ‘,ğ‘ âˆ¥âˆ.
Note the sequential ğ‘-value falls below ğ›¼ = 0.01 at the same time
the confidence sequence excludes zero, when ğ‘™ğ‘¡ (0.01) > 0.

Figure 5 shows the empirical quantile and distribution functions
with 0.995 confidence bands at 15 seconds into the canary test. It is
clear that many quantiles are shifted toward larger values, implying
that the distribution of time-differences is shifted toward larger
values and that SPS events are arriving less frequently.

We regard the confidence sequences on the quantile and distri-
bution functions, as well as their differences, to be complementary
to the automated stopping logic. They allow the developer to be
kept in the loop as the canary test progresses and provide them

Michael Lindon, Chris Sanden, and VachÃ© Shirikian

insight into what would otherwise be a black-box system. Due to
the time-uniform guarantees these can be visualized at any time
without concern of peeking - the coverage guarantee holds for all
time.

Figure 5: Empirical quantile (left) and distribution (right)
functions of epochs of SPS with 0.995 confidence bands from
equations (5) and (7) at t=15 seconds. Ticks omitted for con-
fidentiality.

5 DISCUSSION
The case studies presented above demonstrate the utility of our ap-
proach in rapidly detecting regressions. In both cases, our approach
was successful in alerting developers to an issue within seconds.
This is in contrast to the existing approach, based on fixed-ğ‘› tests,
which would have taken upwards of 30 minutes or longer to detect
the regression. This would have exposed the treatment population
to a degraded experience for a prolonged period of time.

In practice, it can be tempting to apply fixed-ğ‘› tests for continu-
ous monitoring. In our own experience, we have observed devel-
opers repeatedly applying these tests to quickly detect regressions.
However, as demonstrated in Appendix E, the result of doing this is
beyond just theoretical, i.e., it can result in an unacceptable number
of false positives. This can erode trust in the canary test and impact
deployment velocity as developers search for sources of regressions
that do not exist.

For count metrics our methodology makes an assumption that
the distribution of time-differences between successive observa-
tions is stationary. We have observed this to be a reasonable as-
sumption for metrics with a high rate of events, as evinced in figure
4, because the run-time of the test is short. Count metrics for which
data arrives very slowly, such as SPS for a rarely used device, take
longer to reject or approximately accept the null. Therefore, over
the course of a long-running canary test, it is likely that the as-
sumption of a stationary renewal distribution is violated due to
time-varying usage patterns of the Netflix application. Despite this,
it is not clear to what extent the type I error guarantees of this
system would degrade, given that it affects both arms equally. An
alternative approach could be a sequential multinomial test to com-
pare the proportion of counts across arms, which remains valid
even in the presence of time-inhomogeneity [15].

02468101214Time Since Start [s]0.00.20.40.60.81.0Sequential p-valueÎ± = 0.01Na(t)Nb(t)0.00.20.40.60.81.0QuantileQuna(Î±/2,p)Qna(p)Qlna(Î±/2,p)Qunb(Î±/2,p)Qnb(p)Qlnb(Î±/2,p)Time0.00.20.40.60.81.0Funa(Î±/2,x)Fna(x)Flna(Î±/2,x)Funb(Î±/2,x)Fnb(x)Flnb(Î±/2,x)[14] Lynn Kuo and Tae Young Yang. 1996. Bayesian Computation for Nonhomoge-
neous Poisson Processes in Software Reliability. J. Amer. Statist. Assoc. 91, 434
(1996), 763â€“773. http://www.jstor.org/stable/2291671

[15] Michael Lindon and Alan Malek. 2020. Anytime-Valid Inference for Multinomial

Count Data. arXiv:2011.03567 [stat.ME]

[16] P. Massart. 1990. The Tight Constant in the Dvoretzky-Kiefer-Wolfowitz Inequal-

ity. The Annals of Probability 18, 3 (1990), 1269 â€“ 1283.

[17] Gerald Schermann. 2017. Continuous Experimentation for Software Developers.
In Proceedings of the 18th Doctoral Symposium of the 18th International Middleware
Conference (Las Vegas, Nevada) (Middleware â€™17). ACM, New York, NY, USA, 5â€“8.
[18] Gerald Schermann, JÃ¼rgen Cito, Philipp Leitner, and Harald C Gall. 2016. To-
wards quality gates in continuous delivery and deployment. In 2016 IEEE 24th
International Conference on Program Comprehension (ICPC). IEEE, 1â€“4.

[19] Gerald Schermann, JÃ¼rgen Cito, Philipp Leitner, Uwe Zdun, and Harald C. Gall.
2018. Weâ€™re doing it live: A multi-method empirical study on continuous experi-
mentation. Information and Software Technology 99 (2018), 41â€“57.

[20] David Siegmund. 1985.

Sequential Analysis: Tests and Confidence Intervals.

Springer.

[21] Balazs Szorenyi, Robert Busa-Fekete, Paul Weng, and Eyke HÃ¼llermeier. 2015.
Qualitative Multi-Armed Bandits: A Quantile-Based Approach. In Proceedings of
the 32nd International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 37). PMLR, Lille, France, 1660â€“1668.

[22] Alexander Tarvo, Peter F. Sweeney, Nick Mitchell, V.T. Rajan, Matthew Arnold,
and Ioana Baldini. 2015. CanaryAdvisor: A Statistical-Based Tool for Canary
Testing (Demo). In Proceedings of the 2015 International Symposium on Software
Testing and Analysis (Baltimore, MD, USA) (ISSTA 2015). ACM, New York, NY,
USA, 418â€“422.

[23] Abraham Wald. 1945. Sequential tests of statistical hypotheses. The annals of

mathematical statistics 16, 2 (1945), 117â€“186.

[24] Abraham Wald. 1947. Sequential Analysis. John Wiley & Sons, New York.
[25] Tong Xia, Sumit Bhardwaj, Pavel Dmitriev, and Aleksander Fabijan. 2019. Safe
Velocity: A Practical Guide to Software Deployment at Scale using Controlled
Rollout. In 2019 IEEE/ACM 41st International Conference on Software Engineering:
Software Engineering in Practice (Montreal, Quebec, Canada) (ICSE-SEIP â€™19). IEEE
Press, 11â€“20.

[26] Zhenyu Zhao, Mandie Liu, and Anirban Deb. 2018. Safely and Quickly Deploying
New Features with a Staged Rollout Framework Using Sequential Test and Adap-
tive Experimental Design. In 2018 3rd International Conference on Computational
Intelligence and Applications (ICCIA). IEEE Computer Society, Los Alamitos, CA,
USA, 59â€“70.

[27] Å tÄ›pÃ¡n DavidoviÄ and Betsy Beyer. 2018. Canary Analysis Service. Commun.

ACM 61, 5 (2018), 54â€“62.

Rapid Regression Detection in Software Deployments through Sequential Testing

6 CONCLUSION
This paper presents an approach to canary testing that has suc-
cessfully detected performance regressions and bugs in software
deployments at Netflix. The novel contributions of this approach
are the formulation of regressions in terms of stochastic orderings
and the sequential testing framework. Testing hypotheses of sto-
chastic order enables developers to assign an undesirable direction
to changes in a metric distribution beyond simply comparing the
means. When no direction is available or appropriate, any difference
in the distribution can be tested.

The sequential methodology permits canary tests to be continu-
ously monitored, while retaining strict type I error guarantees. This
allows automated stopping logic to be implemented for the canary
test, removing the burden on the developer to monitor the release
manually and freeing up time for them to return other tasks. The
end effect is that bugs and performance regressions are detected
rapidly, prevented from reaching the production environment and
subsequent experimental units are saved from a degraded experi-
ence. Near-immediate feedback is given to the developer, allowing
them to iterate quickly and remedy the issue.

ACKNOWLEDGMENTS
The authors would like to thank Minal Mishra, Toby Mao, Yanjun
Liu, and Martin Tingley for their contributions. The authors would
also like to thank the reviewers for their comments and suggestions.

REFERENCES
[1] P. Armitage, C. K. McPherson, and B. C. Rowe. 1969. Repeated Significance Tests
on Accumulating Data. Journal of the Royal Statistical Society. Series A (General)
132, 2 (1969), 235â€“244.

[2] Peter J. Bickel and Kjell A. Doksum. 1977. Mathematical statistics : basic ideas

and selected topics. Vol. 2. Holden-Day San Francisco. 240â€“241 pages.

[3] Samuel Daniel Conte and Carl W. De Boor. 1980. Elementary Numerical Analysis:
An Algorithmic Approach (3rd ed.). McGraw-Hill Higher Education. 74â€“75 pages.
[4] D. A. Darling and Herbert Robbins. 1968. SOME NONPARAMETRIC SEQUEN-
TIAL TESTS WITH POWER ONE. Proceedings of the National Academy of Sciences
61, 3 (1968), 804â€“809.

[5] A. Dvoretzky, J. Kiefer, and J. Wolfowitz. 1956. Asymptotic Minimax Character
of the Sample Distribution Function and of the Classical Multinomial Estimator.
The Annals of Mathematical Statistics 27, 3 (1956), 642 â€“ 669.

[6] King Chun Foo, Zhen Ming (Jack) Jiang, Bram Adams, Ahmed E. Hassan, Ying
Zou, and Parminder Flora. 2015. An Industrial Case Study on the Automated
Detection of Performance Regressions in Heterogeneous Environments. In 2015
IEEE/ACM 37th IEEE International Conference on Software Engineering (Florence,
Italy) (ICSE â€™15). IEEE Press, 159â€“168.

[7] Michael Graff and Chris Sanden. 2018. Automated Canary Analysis at Netflix
with Kayenta. https://medium.com/netflix-techblog/automated-canary-analysis-
at-netflix-with-kayenta-3260bc7acc69. Netflix Tech Blog.

[8] Josef Hadar and William R. Russell. 1969. Rules for Ordering Uncertain Prospects.

The American Economic Review 59, 1 (1969), 25â€“34.

[9] Steven R. Howard and Aaditya Ramdas. 2022. Sequential estimation of quantiles
with applications to A/B testing and best-arm identification. Bernoulli 28, 3 (2022),
1704 â€“ 1728. https://doi.org/10.3150/21-BEJ1388

[10] Jez Humble and David Farley. 2010. Continuous Delivery: Reliable Software Re-
leases through Build, Test, and Deployment Automation (1st ed.). Addison-Wesley
Professional.

[11] Ramesh Johari, Pete Koomen, Leonid Pekelis, and David Walsh. 2017. Peeking at
A/B Tests: Why It Matters, and What to Do about It. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(Halifax, NS, Canada) (KDD â€™17). ACM, New York, NY, USA, 1517â€“1525.
[12] Ramesh Johari, Leo Pekelis, and David J Walsh. 2015. Always Valid Inference:
Bringing Sequential Analysis to A/B Testing. arXiv:1512.04922 [math.ST]
[13] Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.
2013. Online Controlled Experiments at Large Scale. In Proceedings of the 19th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(Chicago, Illinois, USA) (KDD â€™13). ACM, New York, NY, USA, 1168â€“1176.

APPENDIX
A CONFIDENCE BAND ON |ğ‘‘ğ‘,ğ‘ |

Michael Lindon, Chris Sanden, and VachÃ© Shirikian

where ğœ–ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼) = ğœ–ğ‘›ğ‘ (ğ›¼/2) + ğœ–ğ‘›ğ‘ (ğ›¼/2). The intuition here comes
from finding the largest confidence band radius such that the test
still rejects. This critical value occurs when the confidence band
radius is equal to the largest positive value of ğ‘‘ğ‘›ğ‘,ğ‘›ğ‘ over the domain
X. For any ğ›¼ < ğ‘ğ‘›ğ‘,ğ‘›ğ‘ , sup ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) â‰¤ 0, which implies that
ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) â‰¤ 0 âˆ€ğ‘¥ âˆˆ X and the null hypothesis is not be rejected.

C ADDITIONAL FIGURES

Figure 6: Empirical difference (left) and absolute difference
(right) functions with 1âˆ’ğ›¼ confidence bands from equations
(9) and (23) using the same data as in figure 1.

The confidence band on ğ‘‘ğ‘,ğ‘ can be used to derive a confi-
dence band on |ğ‘‘ğ‘,ğ‘ |, this is illustrated in figure 6. Note that the
image of [ğ‘™, ğ‘¢] under ğ‘¥ â†’ |ğ‘¥ | is [0, max(|ğ‘™ |, |ğ‘¢ |)] if 0 âˆˆ [ğ‘™, ğ‘¢] or
[min(|ğ‘™ |, |ğ‘¢ |), max(|ğ‘™ |, |ğ‘¢ |)] otherwise. Define

ğ‘”(ğ‘¥, ğ‘¦) =

(cid:40)min(|ğ‘¥ |, |ğ‘¦|),
0,

if sign(ğ‘¥) = sign(ğ‘¦)
otherwise,

then

Figure 7: Confidence sequence on the sup-norm, as in equa-
tion (28), of the difference between renewal distribution
functions for SPS case study.

P[ğ‘”(ğ‘‘ğ‘¢

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥), ğ‘‘ğ‘™

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥)) â‰¤

|ğ‘‘ğ‘,ğ‘ (ğ‘¥)| â‰¤
max(|ğ‘‘ğ‘¢
âˆ€ğ‘¥ âˆˆ X] â‰¥ 1 âˆ’ ğ›¼,

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥)|, |ğ‘‘ğ‘™

ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥)|)

(23)

B PROOF OF THEOREM 3.4
The null hypothesis ğ¹ğ‘ (ğ‘¥) â‰¥ ğ¹ğ‘ (ğ‘¥) âˆ€ğ‘¥ âˆˆ X is equivalent to ğ‘‘ğ‘,ğ‘ (ğ‘¥) â‰¤
0 âˆ€ğ‘¥ âˆˆ X. It can be rejected at the ğ›¼-level if there exists an ğ‘¥ âˆˆ X
such that ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) > 0 i.e. when the lower bound on the confi-
dence interval of ğ¹ğ‘ (ğ‘¥) is greater than the upper bound on the confi-
dence interval of ğ¹ğ‘ (ğ‘¥). This is true if and only if sup{ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) :
ğ‘¥ âˆˆ X} > 0.

To obtain a ğ‘-value we seek the smallest such ğ›¼-level test that

rejects the null i.e.

ğ‘ â‰¼
ğ‘›ğ‘,ğ‘›ğ‘ = inf

ğ‘‘ğ‘™
ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼, ğ‘¥) > 0

(cid:26)
ğ›¼ âˆˆ [0, 1] : sup
ğ‘¥ âˆˆX
Let ğ‘‘+
ğ‘›ğ‘,ğ‘›ğ‘ (ğ‘¥) = max(ğ‘‘ğ‘›ğ‘,ğ‘›ğ‘ (ğ‘¥), 0) denote the positive part of the
empirical difference function. If there exists an ğ‘¥ âˆˆ X such that
ğ‘‘ğ‘›ğ‘,ğ‘›ğ‘ (ğ‘¥) > 0, this set is not empty, and ğ‘ â‰¼
is the root of the
following equation

ğ‘›ğ‘,ğ‘›ğ‘

(24)

(cid:27)

ğ‘“ (ğ›¼) = âˆ¥ğ‘‘+

ğ‘›ğ‘,ğ‘›ğ‘ âˆ¥âˆ âˆ’ ğœ–ğ‘›ğ‘,ğ‘›ğ‘ (ğ›¼)

(25)

P

ğ‘‘ğ‘,ğ‘ (ğ‘¥) âˆˆ

inf
ğ‘¥ âˆˆX

Figure 8: 0.99 confidence band on ğ‘„ğ‘ âˆ’ ğ‘„ğ‘ for PlayDelay at
time t=100 seconds.

D CONFIDENCE SEQUENCES ON âˆ¥ğ‘‘ğ‘,ğ‘ âˆ¥âˆ
One can define confidence sequences for sup ğ‘‘ğ‘,ğ‘ (ğ‘¥), inf ğ‘‘ğ‘,ğ‘ (ğ‘¥)
and âˆ¥ğ‘‘ğ‘,ğ‘ âˆ¥âˆ by taking running intersections, e.g.
(cid:20)

(cid:21) (cid:35)

(cid:34)

ğ‘‘ğ‘,ğ‘ (ğ‘¥) âˆˆ

P

sup
ğ‘¥ âˆˆX
(cid:34)

âˆ
(cid:217)

ğ‘¡ =1
âˆ
(cid:217)

ğ‘¡ =1

sup
ğ‘¥ âˆˆX

ğ‘‘ğ‘™
ğ‘¡ (ğ›¼, ğ‘¥), sup
ğ‘¥ âˆˆX

ğ‘‘ğ‘¢
ğ‘¡ (ğ›¼, ğ‘¥)

â‰¥ 1 âˆ’ ğ›¼

(26)

(cid:20)

inf
ğ‘¥ âˆˆX

ğ‘‘ğ‘™
ğ‘¡ (ğ›¼, ğ‘¥), inf
ğ‘¥ âˆˆX

ğ‘‘ğ‘¢
ğ‘¡ (ğ›¼, ğ‘¥)

(cid:21) (cid:35)

â‰¥ 1 âˆ’ ğ›¼

(27)

âˆ’3âˆ’2âˆ’10123âˆ’0.4âˆ’0.3âˆ’0.2âˆ’0.10.00.10.20.30.40.5||dna,nb||âˆduna,nb(Î±,x)dna,nb(x)dlna,nb(Î±,x)Ï„Zero Lineâˆ’3âˆ’2âˆ’10123âˆ’0.4âˆ’0.3âˆ’0.2âˆ’0.10.00.10.20.30.40.5una,nb(Î±)lna,nb(Î±)dna,nb(x)g(duna,nb(Î±,x),dlna,nb(Î±,x))max(|duna,nb(Î±,x)|,|dlna,nb(Î±,x)|)Ï„Zero Line02468101214Time Since Start [s]0.00.20.40.60.81.0una,nb(Î±)||dna,nb||âˆlna,nb(Î±)Ï„=0.050.00.20.40.60.81.0Quantileâˆ’400âˆ’2000200400millisecondsQunb(Î±/2,p)âˆ’Qlna(Î±/2,p)Qnb(p)âˆ’Qna(p)Qlnb(Î±/2,p)âˆ’Quna(Î±/2,p)Zero LineRapid Regression Detection in Software Deployments through Sequential Testing

Figure 9: Empirical distribution functions of the stopping
times based on 100 observations for tests configured at the
ğ›¼ = 0.05 level testing the null hypothesis ğ¹ğ‘ = ğ¹ğ‘ . (Left)
Under the null hypothesis: distributions for arms A and B
are Gamma(10, 10). (Right) Under the alternative hypothe-
sis: distributions for arms A and B are Gamma(10, 10) and
Gamma(10, 11) respectively.

(cid:34)

P

âˆ¥ğ‘‘ğ‘,ğ‘ âˆ¥âˆ âˆˆ

âˆ
(cid:217)

ğ‘¡ =1

(cid:35)

[ğ‘™ğ‘¡ (ğ›¼), ğ‘¢ğ‘¡ (ğ›¼)]

â‰¥ 1 âˆ’ ğ›¼ .

(28)

E SIMULATION STUDY
In this section, we present a simulation which demonstrates the
advantages of the sequential test compared to fixed-ğ‘› methods. To
illustrate the advantage of sequential over fixed-ğ‘› methods, we
examine the empirical type I error probabilities under continuous
monitoring, that is, performing a significance test after every data-
point. We generate 100 simulations where each simulation gener-
ates independent streams of i.i.d. Gamma(10, 10) random variables
for each arm. After every new pair of observations a significance
test configured at the ğ›¼ = 0.05 level is performed. If the ğ‘-value
is less than 0.05, the null hypothesis is rejected and the stopping
time is recorded, otherwise a new pair of observations is sampled
from arms A and B. The tests compared the fixed-ğ‘› Kolmogorov-
Smirnoff and Mann-Whitney tests to the sequential test in section
3.3. The random number generator is seeded consistently, so that
each stream of random variables is identical for each test. To ease
the computation, the simulations are terminated when 5000 obser-
vations are sampled from each arm.

The left of figure 9 clearly visualizes the problem with continuous
monitoring for the Kolmogorov-Smirnoff and Mann-Whitney tests,
which resulted in 64 and 57 false positives respectively. It also shows
that no type I errors resulted from the sequential procedure. This
can be explained by the fact that the sequential procedure must
control the type I error probability for all (ğ‘›ğ‘, ğ‘›ğ‘ ) âˆˆ N Ã— N, and so
one expects the type I error probability in a smaller, finite window
of time to be less than if it were to run indefinitely. The right of
figure 9 shows the empirical probability of correctly rejecting the
null by ğ‘› in simulations where the null is incorrect.

010002000300040005000n0.00.20.40.60.81.0Mann-WhitneyKolmogorovâ€“SmirnovHoward-Ramdasu=0.05010002000300040005000n0.00.20.40.60.81.0Mann-WhitneyKolmogorovâ€“SmirnovHoward-Ramdas