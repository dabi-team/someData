2
2
0
2

l
u
J

6
1

]

G
L
.
s
c
[

1
v
8
5
9
7
0
.
7
0
2
2
:
v
i
X
r
a

FASTML SCIENCE BENCHMARKS:
ACCELERATING REAL-TIME SCIENTIFIC EDGE MACHINE LEARNING

Javier Duarte * 1 Nhan Tran * 2 Ben Hawks 2 Christian Herwig 2
Jules Muhizi 3 Shvetank Prakash 3 Vijay Janapa Reddi 3

ABSTRACT
Applications of machine learning (ML) are growing by the day for many unique and challenging scientiﬁc
applications. However, a crucial challenge facing these applications is their need for ultra low-latency and
on-detector ML capabilities. Given the slowdown in Moore’s law and Dennard scaling, coupled with the rapid
advances in scientiﬁc instrumentation that is resulting in growing data rates, there is a need for ultra-fast ML at the
extreme edge. Fast ML at the edge is essential for reducing and ﬁltering scientiﬁc data in real-time to accelerate
science experimentation and enable more profound insights. To accelerate real-time scientiﬁc edge ML hardware
and software solutions, we need well-constrained benchmark tasks with enough speciﬁcations to be generically
applicable and accessible. These benchmarks can guide the design of future edge ML hardware for scientiﬁc
applications capable of meeting the nanosecond and microsecond level latency requirements. To this end, we
present an initial set of scientiﬁc ML benchmarks, covering a variety of ML and embedded system techniques.

1

INTRODUCTION

In pursuit of scientiﬁc discovery across many domains, ex-
periments are becoming exceedingly sophisticated to probe
physical systems at increasingly smaller spatial resolutions
and shorter timescales. These order of magnitude advance-
ments have led to explosions in both data volumes and rich-
ness, leaving domain scientists to develop novel methods to
handle growing data processing needs. Figure 1 shows the
volume of data (y-axis) that is generated in scientiﬁc appli-
cations such as those at the CERN Large Hadron Collider
(LHC) and in particle accelerator controls. They produce
tens of terabytes of data every second, as discussed below.

As scientiﬁc ecosystems snowball in their speed and scale,
new data processing and reduction paradigms need to be
integrated into the system-level design. The large volume
of data needs to be rapidly reduced to a sustainable level by
a real-time event ﬁlter system on whether the data should
be kept for further analysis or discarded. Fortunately, this
coincides with the rise of machine learning (ML), or the
use of algorithms that can learn directly from data. Recent
advancements demonstrate that ML architectures based on

*Equal contribution 1University of California San Diego, La
Jolla, CA, USA 2Fermi National Accelerator Laboratory, Batavia,
IL, USA 3Harvard University, Cambridge, MA, USA. Corre-
spondence to: Javier Duarte <jduarte@ucsd.edu>, Nhan Tran
<ntran@fnal.gov>, Vijay Janapa Reddi <vj@eecs.harvard.edu>.

Proceedings of the 5 th MLSys Conference, Santa Clara, CA, USA,
2022. Copyright 2022 by the author(s).

Figure 1. Reference latencies and streaming input data rates for
common benchmarks and those proposed in this paper. The hori-
zontal error bar represents the range of acceptable latencies for the
various domains, while the vertical error bar denotes the range of
streaming data rates typical for those domains. The real-time scien-
tiﬁc application domain, or the FastML Science domain, produces
a staggering volume of data and the inference latency requirements
are orders of magnitude far more stringent than they are for more
traditional consumer-facing applications and their benchmarks.

109107105103101101103105Reference latency [s]1021041061081010Streaming data rate [B/s]FastML ScienceJet ClassificationSensor Data CompressionBeam ControlMLPerf Tiny (IC)MLPerf Mobile (NLP) 
 
 
 
 
 
FastML Science Benchmarks

structured deep neural networks are versatile and capable of
solving a broad range of complex scientiﬁc problems. Fast
ML is leading to rapid advancements across many different
scientiﬁc domains (Carleo et al., 2019; Deiana et al., 2022).

However, a crucial challenge in applying ML to scientiﬁc
domains is the need for ultra-fast inference speeds on the or-
der of microseconds. Compared to traditional ML inference
latencies, the scientiﬁc application domain requires nearly
1000× faster inference performance, as shown in Figure 1.
Many of the consumer facing edge ML deployments like
MLPerf Mobile and Tiny are at least an order of magnitude
off from the scientiﬁc application latency requirements.

Moreover, while ML can power scientiﬁc advances that can
lead to future paradigm shifts in a broad range of scientiﬁc
domains, including particle physics, accelerator physics,
plasma physics, astronomy, neuroscience, chemistry, mate-
rial science, and biomedical engineering, each application
brings its own domain-speciﬁc knowledge and application-
speciﬁc constraints. Furthermore, there can be a broad range
of different challenges even within a given domain. This
leaves us with two primary types of challenges. The ﬁrst is
how can we deﬁne generically applicable ML benchmarks
tasks for bespoke domain problems which can attract inter-
est from a broad community of system and ML experts?
The second is how can we design benchmark tasks to satisfy
challenging system-level scientiﬁc requirements that can,
at its core, have common elements which overlap with a
number of system architectures? To address these issues, we
introduce a domain-speciﬁc benchmark for fast ML science.

In this paper, we address the aforementioned challenges by
deﬁning an initial set of scientiﬁc “edge” ML benchmarks.
These benchmarks strike a balance between a universality
common to many scientiﬁc ML applications while also be-
ing uniquely challenging to push designers to further the
state-of-the-art ML and system architecture beyond tradi-
tional industrial applications. We choose our benchmark
tasks to cover the three different areas of ML, also we se-
lect domain tasks that, while seeming quite speciﬁc, can be
generically mapped onto other scientiﬁc applications. In
generic terms, the initial benchmark tasks are the following:

• Supervised learning for physics event triggering:
feature extraction of point cloud inputs for rare data
classiﬁcation and ﬁltering.

• Unsupervised learning for lossy compression of
particle detector data: near-sensor image data com-
pression for custom sensor geometries using image
similarity metrics based on energy distribution proﬁles.

• Reinforcement learning for accelerator beam con-
trol: optimal real-time system controls based on time-
series data from multiple sensors.

With this ﬁrst suite of benchmarks, we provide suitable
coverage for scientiﬁc tasks that span a range of different
ML areas and include unique, meaningful, and challenging
system-level requirements that go beyond those from indus-
try. Future work would include ﬁnding areas not covered by
these tasks and broadening the benchmarks.

2 RELATED WORK

Table 1 compares characteristics of some of the existing
science-related benchmarks and initiatives to the real-time
scientiﬁc edge ML benchmarks we present in this work.
Most of the prior work included in the table has been for-
malized as an ofﬁcial benchmark, but a couple of related
initiatives remain as standalone datasets or competitions.

Among existing AI benchmarks, the community-driven
MLPerf benchmarks from MLCommons (Mattson et al.,
2020) are well-established. The benchmarks are run un-
der predeﬁned conditions and evaluate the performance of
training and inference for hardware, software, and services.
MLPerf regularly conducts new tests and adds new work-
loads to adapt to the latest industry trends and state of the art
in AI across various domains including high performance
computing (HPC) (Farrell et al., 2021), datacenter (MLCom-
mons, 2021a), edge (MLCommons, 2021b), mobile (Reddi
et al., 2020), and tiny (Banbury et al., 2021). Additionally,
BenchCouncil AIBench is a comprehensive AI benchmark
suite including AI Scenario, Training, Inference, Micro,
and Synthetic Benchmarks across datacenter, HPC, IoT and
edge (BenchCouncil, 2018). Other benchmarks have also
been developed by academia and industry. Additional ex-
amples of prior art and initiatives include AI Benchmark
(Ignatov et al., 2019), EEMBC MLMark (Torelli & Ban-
gale, 2019), AIMatrix (Alibaba, 2018), AIXPRT (Principled
Technologies, 2019), DeepBench (Baidu, 2017), TBD (Zhu
et al., 2018), Fathom (Adolf et al., 2016), RLBench (James
et al., 2020), and DAWNBench (Coleman et al., 2017).

However, scientiﬁc applications (i.e. cosmology, particle
physics, biology, clean energy, etc.) are innately distinct
from traditional industrial applications with respect to the
type and volume of data and the resulting model complex-
ity (Farrell et al., 2021). The MLCommons Science Work-
ing Group (MLCommons, 2020) has a suite of benchmarks
that focus on such scientiﬁc workloads including application
examples across several domains such as climate, materials,
medicine, and earthquakes. SciMLBench (Thiyagalingam
et al., 2021) from the Rutherford Appleton Laboratory is
another benchmark suite speciﬁcally focused on scientiﬁc
machine learning and aimed towards the “AI for Science”
domain. The suite currently contains three benchmarks that
represent problems taken from the material and environmen-
tal sciences. MLPerf HPC and AIBench HPCAI500 are
two more benchmarks that include scientiﬁc workloads. In

FastML Science Benchmarks

Real-Time
Formalized
Benchmark Workload(s) Computing Constraints

Scientiﬁc

Edge

FastML Science Benchmarks (this work)

SciMLBench (Thiyagalingam et al., 2021)
LHC New Physics Dataset (Govorkova et al., 2021)
MLPerf HPC (Farrell et al., 2021)
BenchCounil AIBench HPC (BenchCouncil, 2018)
MLCommons Science (MLCommons, 2020)
ITU Modulation Classiﬁcation (ITU, 2021)

(cid:88)

(cid:88)

×
(cid:88)
(cid:88)
(cid:88)

×

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

×

(cid:88)

(cid:88)
(cid:88)

×
×
×
(cid:88)

(cid:88)

×
(cid:88)

×
×
×
(cid:88)

Table 1. Comparison of existing machine learning benchmarks and related initiatives.

general, HPC is being leveraged by the scientiﬁc community
for accelerating scientiﬁc insights and discovery. MLPerf
HPC aims to systematically understand how scientiﬁc ap-
plications perform on diverse supercomputers, focusing on
the time to train for three representative scientiﬁc machine
learning applications with massive datasets (i.e. cosmology,
extreme weather analytics, and molecular dynamics). Sim-
ilarly, AIBench HPCAI500 also includes a benchmark on
extreme weather analytics. However, all of these existing
benchmarks fail to capture the unique constraints and de-
mands required by scientiﬁc “edge” computing in which the
processing must occur near the data source in real-time.

The LHC physics dataset for new physics detection (Gov-
orkova et al., 2021) is the only dataset to our knowledge
that retains such characteristics, requiring unsupervised de-
tection at 40 MHz. However, a related dataset for charged
particle tracking at the LHC, the throughput phase of the
TrackML Challenge (Amrouche et al., 2021) emphasized
balancing the accuracy of the solution with the speed
of inference. Similarly, one of the tasks in the Interna-
tional Telecommunication Union’s (ITU) ML in 5G Chal-
lenge (ITU, 2021) requires ultra-low latency for a different
application, namely modulation classiﬁcation in commu-
nication networks, which displays the generalizability of
real-time scientiﬁc edge ML benchmarks. Benchmarking
workloads of this nature can ultimately be a catalyst in en-
abling a diverse range of new solutions and applications.

3 BENCHMARKS

In this section, we ﬁrst introduce our benchmark design
philosophy to establish a suite of real-time scientiﬁc bench-
marks for edge machine learning. Next, we introduce the
tasks, models, datasets, and associated metrics that deﬁne
this initial suite. These benchmarks are in the initial stages
and the task list will continue to evolve over time.

3.1 Benchmark Design Philosophy

Our benchmarks tasks cover supervised learning, unsuper-
vised learning, and reinforcement learning and map generi-
cally onto other scientiﬁc applications. The benchmarks are
summarized in Table 2, which we explain in more detail in
the subsequent sections.

In each benchmark task, we are working at the edge, very
close to the data source. For each task, the input data has
been digitized1, but the analog signals from sensors are
digitized via custom ADCs that have a custom precision that
is tailored for the sensor technology. Scientiﬁc instrument
sensors often may have a non-standard precision and further,
may deal with inputs of varying quantization. As is often the
case in experimental design, we lay out system constraints
that will serve as guide rails for the various benchmark tasks.

Solutions for the scientiﬁc ML edge benchmarks must sat-
isfy the given requirements, and then the algorithm perfor-
mance is measured by a task-speciﬁc metric. Each bench-
mark has speciﬁc system-level constraints. They are all
latency-bound, with latency requirements ranging from hun-
dreds of nanoseconds to milliseconds. In some cases, the
algorithm latency and data arrival frequency (pipeline inter-
val) will not be the same. In one application, we also have
requirements for area and power.

In order to standardize measurement an implementation’s
performance of a given benchmark against a potentially di-
verse set of other hardware and technologies, a number of
related metrics should be captured and presented to more
accurately characterize a given aspect of a system. For exam-
ple, when measuring the latency of an implementation, both
the end-to-end latency and the initiation interval, or the time
required before a system is ready to accept an input sample
after it’s just received one, should be measured and pre-
sented as part of the results. To accurately support and mea-
sure various comprehensive metrics, careful implementation

1In even more extreme scenarios in the future, we may consider

analog inputs.

FastML Science Benchmarks

Dataset We allow for two types of input features. Sev-
eral previous related studies used expert features in a dense,
fully-connected network topology which we will describe
in the following section as our baseline model. There are 16
expert-designed features, traditionally standardized with a
standard scalar and represented with ﬁxed-point precision
with 16 total bits and six integer bits. In realistic systems,
the classiﬁcation task would also require the computation
of those expert features, but we do not need that here. How-
ever, to that end, we make available a more challenging
set of point cloud inputs which are the 100 most energetic
particles in the jet (with zero padding if there are less than
100 particles) (Pierini et al., 2020). Each particle is assumed
to be massless and has a feature set size of 3 corresponding
to the momentum vector. Models built from these input
particles can be more performant than the expert features
but could be more computationally expensive. Given that
the range of px, py, and pz for an individual particle is ap-
proximately contained within ±2 TeV, we adopt an input
integer quantization scheme of 16 total bits with a least sig-
niﬁcant bit corresponding to 0.0625 GeV and a total range
of ±2048 GeV.

Performance Metrics There are many metrics used in
the literature for this benchmark model. The two we will
focus on are (1) classiﬁcation accuracy and (2) FPR at TPR
of 50% for the signal being Z jet (Hawks et al., 2021).

Real-time System Constraints
In the planned upgrade of
the CMS trigger, the global correlator trigger would contain
jet tagging algorithms like the one described. This system
features a time-multiplexed design, such that information
from 6 consecutive proton beam crossings, which occur
even 25 ns, is processed simultaneously. For this system, a
latency of no more than 1 µs and the ability to accept new
inputs every 150 ns is required (CMS Collaboration, 2020).

Baseline Model and Implementation For our baseline
model and performance, we consider the 16-input dense,
fully-connected architecture ﬁrst presented in Ref. (Duarte
et al., 2018) and based on expert inputs. Because we con-
sider the model with expert inputs, a fully-connected archi-
tecture is selected, but other neural network architectures
may be more optimal, particularly if particle level inputs are
used.

The baseline model has an accuracy of 74.8% and the FPR
at TPR of 50% metric for the baseline model is 0.00129.
The baseline model implementation with the architecture
consists of 16 → 64 → 32 → 32 → 5 MLP with
quantization-aware training (QAT) to a homogeneous pre-
cision for the weights and biases of 6-bits as presented in
Ref. (Coelho et al., 2021). This model is synthesized for
a Virtex Ultrascale+ 9P FPGA with the following resource

Figure 2. The ﬁve jet type classes—light quark (q), gluon (g), top
quark (t), W, and Z boson—for the supervised learning benchmark
task. Figure adapted from (Moreno et al., 2020).

of a “host” system, which is responsible for administering
a benchmark, must also taken into account. Continuing
the example of measuring latency of an implementation,
the timing and method which inputs are presented to the
system under test must be capable of strictly obeying the
benchmark’s pipeline interval. A framework for performing
such measurements on varied hardware which we can adopt
is based on the MLPerf Tiny benchmark (Banbury et al.,
2021).

3.2 Supervised Learning: Jet Classiﬁcation

At the LHC, proton bunches collide at an extreme frequency
of 40 MHz, and data rates at the two multipurpose high en-
ergy particle physics experiments, CMS (Chatrchyan et al.,
2008) and ATLAS (Airapetian et al., 1999), are of the or-
der of hundreds of terabytes per second. With such high
data rates, the task of real-time processing to ﬁlter events
to reduce data rates to manageable levels for ofﬂine pro-
cessing is called triggering. The ﬁrst level of the trigger at
CMS (Sirunyan et al., 2020) and ATLAS (Aad et al., 2020)
is performed in FPGAs in custom electronics platforms
and have latency requirements at the microsecond scale; to
put this into proper perspective, it is worth noting that the
MLPerf inference tasks’ latency typically ranges between
10 to 100s of milliseconds.

While this task is speciﬁc to particle physics, ﬁltering data
to ﬁnd the most interesting subset in real-time to reduce data
transmission and volumes in big data scientiﬁc experiments
is a very common challenge. In this scenario, we are looking
for proton collisions with interesting energetic radiation
patterns called jets. In particular, we would like to identify
rare jet signatures originating from the W or Z boson or the
top quark (t) in contrast to more common signatures from
the lighter quarks (q) or gluons (g). Due to high ﬁdelity
simulations in particle physics experiments, this becomes a
supervised multi-classiﬁcation task. A schematic illustration
of the different types of jets is shown in Fig. 2.

q/g t→Wb→qqb h→bb W/Z→qqbq/g t→Wb→qqb h→bb W/Z→qqbq/g t→Wb→qqb h→bb W/Z→qqbq/g t→Wb→qqb h→bb W/Z→qqbq/g t→Wb→qqb h→bb W/Z→qqbq/g t→Wb→qqb h→bb W/Z→qqbFastML Science Benchmarks

compression variants, respectively.

Figure 3. Schematic of the compress-transmit-decompress pipeline
for the HGCal trigger data stream.

usage: 124 DSPs, 39782 LUTs, and 8128 FFs. The latency
is 55 ns at a 5 ns clock with a pipeline interval of 1 clock
cycle (5 ns).

3.3 Unsupervised Learning: Irregular Sensor Data

Compression

Processing of LHC events requires two parallel streams of
detector data to be transmitted from on-detector readout
chips. The ﬁrst is complete event data from all sensing ele-
ments, transmitted at the trigger accept rate of 100 kHz,
while the second is a compressed representation of the
same data at the full 40 MHz collision rate. This second,
lightweight representation is the basis for trigger decisions,
with a compression factor of O(400), ensuring parity be-
tween the bandwidth of each stream. The critical task is to
achieve this reduction in O(100 ns) with minimal impact on
downstream physics algorithms.

This task is common to many detector systems and directly
relevant to a host of on-device compression tasks for com-
plex sensor data. The scenario considers the CMS high-
granularity endcap calorimeter (HGCal), comprised of 6M
channels, each capturing a 5d (position, energy and time)
image of showering high-energy particles. HGCal is com-
prised of layers of hexagonal arrays, with a single particle
depositing energy into hundreds of individual sensors. Data
from each hexagonal array is compressed in an application-
speciﬁc integrated circuit (ASIC), with the encoded repre-
sentation transmitted off-detector and subsequently used to
recover the initial detector image as in Figure 3. A data-
driven approach is required to tune the parameters of the
compress and decompress algorithms to minimize differ-
ences between the original and decoded images.

Dataset Compression of the trigger data is accomplished
in multiple steps. First, nearest-neighbors within each hexag-
onal array are aggregated to form a set of 48 “trigger cells”
with energies represented as a custom 7b ﬂoat. These 48×7b
ﬂoating-point inputs are converted to 22b integers, summed,
and ﬁnally normalized to obtain 8b ﬁxed-point trigger cell
data. The sum is preserved as a 9b ﬂoat for transmission.
The benchmark task begins from this point, which must
capture this 48 × 8 = 384b representation in a budget
of either 144 or 48 bits, for the moderate—and extreme—

Real-time System Constraints The encoder must accept
new inputs at 40 MHz input rate and complete processing
within 100 ns. Furthermore, we must consider protection
against single-event effects due to the high-radiation envi-
ronment of the on-detector readout. As a rough ﬁrst order
guideline, on a low-power CMOS 65 nm technology node,
the algorithm area must not be greater than 4 mm2 while
drawing less than 100 mW.

Performance Metrics Performance is assessed by di-
rectly comparing the individual energies of each decoded
array of hexagonal sensors (i.e. 48 trigger cells) to the
original image of normalized inputs. The energy mover’s
distance (Komiske et al., 2019) (EMD) is used to compare
the reconstructed radiation patterns, giving smaller penal-
ties for misreconstructed energies that are close-by to the
original deposit.

Baseline Model A convolutional neutral network (CNN)
autoencoder architecture is used to perform the compress-
and-decompress task because the sensor data is image-like,
though the hexagonal geometry provides a unique challenge.
Normalized sensor data is re-arranged and fed to a CNN
with one convolutional and one dense layer with 6b weights,
leading to a maximum of 16×9b outputs, saturating the
144b bandwidth (Di Guglielmo et al., 2021). Data is de-
compressed by a second CNN with inverted architecture,
whose outputs are multiplied by the energy sum to recover
the original input image.

The compression logic is implemented in an “encoder ASIC”
using a low-power CMOS process with 65 nm feature size.
The total latency for this circuit is 25 ns and is estimated to
draw 60 mW in simulation.

3.4 Reinforcement Learning: Beam Control

Intense and energetic particle beams are used for various
applications, from materials discovery to studying nuclear
matter to fundamental particle physics, and even cancer ther-
apy. Controlling precise particle beams, such as those at the
Department of Energy User Facilities, requires intelligent
algorithms running at the edge in low-latency, real-time sys-
tems to steer particles traversing miles of beamline at nearly
the speed of light.

A dataset has been developed (Kafkes & St John, 2021) for
studying how to control the bending magnet ramping rate
of power supplies (St. John et al., 2021) in the rapidly cy-
cling Booster synchrotron ring (Hubbard et al., 1973) at the
Fermilab Accelerator Complex. This is illustrated diagram-
matically in Figure 4. The power supply control signals are
provided at 15 Hz. This beam controls application can be

FastML Science Benchmarks

Type

Benchmark

Input
Precision

Pipeline Real-time
Latency

Rate

Misc. Req.

Baseline Model
Parameters

Supervised Learning
Unsupervised Learning
Reinforcement Learning

Jet Classiﬁcation
Sensor Data Compression
Beam Control

16b
9b
32b

150 ns
25 ns
5 ms

1 µs
100 ns
5 ms

-
area, power (65 nm)
-

4,389
2,288
34,695

Table 2. Summary of constraints for three benchmark tasks and number of parameters for the benchmark baseline models.

Baseline Model(s) There are two models involved in this
benchmark task: (1) the surrogate model for the Booster
accelerator and (2) the online agent, which is correcting the
reference magnet power supplies in real-time. The surrogate
model is ﬁxed in this benchmark task and plays the role of
the environment in this reinforcement learning task. The
long short-term memory (LSTM) (Hochreiter & Schmidhu-
ber, 1997) surrogate model inputs are the previous 150-time
steps of the top 5 causal variables—variables related to
the synchrotron and downstream accelerator currents and
current errors concerning reference. The model has approxi-
mately 1.5 million parameters.

The benchmark online agent running in the Arria10 system-
on-chip (SoC) is a multilayer perceptron taking the ﬁve
input parameters, has three hidden layers (128, 128, 128)
and approximately 35,000 parameters. The architecture is a
fully-connected neural network because the 5 inputs are al-
ready selected expert variables. The deep Q-network (Mnih
et al., 2013; 2015) has 7 discrete outputs and maximizes the
reward, R, deﬁned above. The reward metric is measured as
a function of RL episode and is presented in Fig. 7 (bottom)
of (St. John et al., 2021).

The benchmark model weights and biases are quantized to
20 total bits in a ﬁxed-point representation in hardware. The
lowest latency implementation of the model is implemented
for an Intel Arria10 SoC with a resource usage of 53 DSPs,
238 BRAMs, 672 MLABs, 43.3 kALMs, 92.6 kFFs. The
algorithm has a latency of 3.9 µs.

4 DISCUSSION AND OUTLOOK

This position paper highlights both the need and challenges
for developing machine learning (ML) benchmarks for edge
applications in science. Given the demise of Moore’s law
and Dennard scaling (Dennard et al., 1974; Esmaeilzadeh
et al., 2011) and advances in scientiﬁc instrumentation re-
sulting in rapidly growing data rates, edge computing is
becoming exceedingly crucial for reducing and ﬁltering
scientiﬁc data in real-time to accelerate science experimen-
tation and enable more profound insights. There are chal-
lenges in building well-constrained benchmark tasks with
enough speciﬁcation to be generically applicable and ac-
cessible simultaneously. However, we can use these edge
applications in extreme data processing environments to ad-
vance many scientiﬁc domains and enable the development

Figure 4. Synchrotron magnet power supply control system for the
Fermilab Booster Ring, adapted from (St. John et al., 2021)

framed as a reinforcement learning benchmark task. Be-
cause an accurate and reliable simulation of the synchrotron
is not possible from ﬁrst principles, a “virtual” accelerator
complex surrogate model has been developed to emulate
the actual physical system. This surrogate model will serve
as the environment with which our reinforcement learning
benchmark interacts.

Dataset A Booster synchrotron power supply regulation
dataset provides cycle-by-cycle time series of readings and
settings from the most relevant devices available in the Fer-
milab control system. This data was drawn from the time
series of a select subset of the roughly 200,000 entries that
populate the device database of the accelerator control net-
work. Data was sampled at 15 Hz for 54 devices pertaining
to the system’s regulation. Because of how data is trans-
mitted and communicated, inputs are 32-bit ﬂoating-point
numbers, but the sensor source’s precision is, in many cases,
less.

Real-time System Constraints The Booster ramping cy-
cle rate is 15 Hz, which sets the control loop’s time scale.
We deﬁne the algorithm latency requirement as 5 ms for this
benchmark due to data movement latency.

Performance Metrics The primary performance metric
in this reference benchmark is the reward, R, deﬁned as the
negative of the error with respect to the reference expected
current in the Booster, R = −|∆Imin|.

Power supply control systemProgrammable logicBoosterSynchrotronControl signalEnvironment feedbackBending magnet in-situ measurementof state-of-the-art tools.

Workload Characterization (IISWC), pp. 1. IEEE, 2016.

FastML Science Benchmarks

In devising scientiﬁc ML edge benchmarks tasks, we aim to
cover machine learning and embedded system techniques.
We choose three applications that span supervised, unsuper-
vised, and reinforcement learning. The system constraints
also span a wide range of latency requirements—from hun-
dreds of nanoseconds to milliseconds where technologies
vary from ASIC to FPGA to more relaxed constraints with
the freedom to choose system architecture. Finally, there is
also a variation in input data representations from expert-
level inputs to image data, point cloud data, and time-series
data. A summary of the proposed scientiﬁc edge ML bench-
marks are presented in Table 2 including the input, latency,
and pipeline interval constraints of the benchmark applica-
tions. We also provide a prototype code repository for these
benchmarks (Muhizi & Duarte, 2022).

5 CONCLUSION

In this work, we present an initial set of scientiﬁc machine
learning benchmarks that are speciﬁcally geared towards
real-time scientiﬁc edge machine learning needs. Our goal is
to ﬁrst and foremost identify and provide a suite of datasets
and a code repository for these various edge ML tasks. We
will collect and document different solutions and, based on
interest, extend this work. In particular, there is potential
to provide more domain applications such as for astronomy,
neuroscience, and microscopy that could attract a broader
set of use-cases.

ACKNOWLEDGMENTS

JD is supported by the US Department of Energy (DOE)
Award Nos. DE-SC0021187, DE-SC0021396 and the Na-
tional Science Foundation (NSF) A3D3 Institute under Co-
operative Agreement OAC-2117997. BH, CH, JM, and NT
are supported by Fermi Research Alliance, LLC under Con-
tract No. DE-AC02-07CH11359 with the U.S. Department
of Energy (DOE), Ofﬁce of Science, Ofﬁce of High Energy
Physics, the DOE Early Career Research program under
Award No. DE-0000247070, and the DOE, Ofﬁce of Sci-
ence, Ofﬁce of Advanced Scientiﬁc Computing Research
under Award No. DE-FOA-0002501.

REFERENCES

Aad, G. et al.

Operation of

the ATLAS trigger
JINST, 15:P10004, 2020,

system in Run 2.
doi:10.1088/1748-0221/15/10/P10004,
arXiv:2007.12539.

Adolf, R., Rama, S., Reagen, B., Wei, G.-Y., and Brooks, D.
Fathom: Reference workloads for modern deep learning
In 2016 IEEE International Symposium on
methods.

Airapetian, A. et al. ATLAS: Detector and physics per-
formance technical design report. Volume 1. ATLAS
Technical Design Report CERN-LHCC-99-14, ATLAS-
TDR-14, 5 1999. URL https://cds.cern.ch/
record/391176.

Alibaba. AI matrix, 2018. URL https://aimatrix.

ai/en-us. Accessed: 10 January 2022.

Amrouche, S. et al. The Tracking Machine Learning chal-
lenge: Throughput phase. Submitted to Comput. Softw.
Big Sci., 5 2021, arXiv:2105.01160.

Baidu. DeepBench: Benchmarking deep learning operations
on different hardware, 2017. URL https://github.
com/baidu-research/DeepBench. Accessed:
17 January 2022.

Banbury, C., Reddi, V. J., Torelli, P., Holleman, J.,
Jeffries, N., Kiraly, C., Montino, P., Kanter, D.,
Ahmed, S., Pau, D., Thakker, U., Torrini, A., War-
den, P., Cordaro, J., Guglielmo, G. D., Duarte, J.,
Gibellini, S., Parekh, V., Tran, H., Tran, N., Wenxu,
N., and Xuesong, X. MLPerf Tiny benchmark.
In
Proceedings of
the Neural Information Processing
Systems Track on Datasets and Benchmarks, volume 1,
12 2021, arXiv:2106.07597.
URL https:
//datasets-benchmarks-proceedings.
neurips.cc/paper/2021/hash/
da4fb5c6e93e74d3df8527599fa62642-
Abstract-round1.html.

BenchCouncil. Aibench, 2018. URL https://www.
benchcouncil.org/aibench. Accessed: 10 Jan-
uary 2022.

Carleo, G., Cirac,

I., Cranmer, K., Daudet, L.,
Schuld, M., Tishby, N., Vogt-Maranto, L., and
Zdeborov´a, L. Machine learning and the physi-
Rev. Mod. Phys., 91:045002, 2019,
cal sciences.
doi:10.1103/RevModPhys.91.045002,
arXiv:1903.10563.

Chatrchyan, S.

al.
the CERN LHC.

et

at
doi:10.1088/1748-0221/3/08/S08004.

The CMS Experiment
JINST, 3:S08004, 2008,

CMS Collaboration. The Phase-2 upgrade of the CMS
Level-1 trigger. CMS Technical Design Report CERN-
LHCC-2020-004. CMS-TDR-021, 2020. URL https:
//cds.cern.ch/record/2714892.

Coelho, C. N., Kuusela, A., Li, S., Zhuang, H., Aarrestad,
T., Loncar, V., Ngadiuba, J., Pierini, M., Pol, A. A.,
and Summers, S. Automatic heterogeneous quantiza-
tion of deep neural networks for low-latency inference on

FastML Science Benchmarks

the edge for particle detectors. Nat. Mach. Intell., 3(8):
675, 2021, doi:10.1038/s42256-021-00356-5,
arXiv:2006.10159.

Coleman, C., Narayanan, D., Kang, D., Zhao, T., Zhang, J.,
Nardi, L., Bailis, P., Olukotun, K., R´e, C., and Zaharia,
M. Dawnbench: An end-to-end deep learning benchmark
and competition. Training, 100(101):102, 2017.

Deiana, A. M., Tran, N., Agar, J., Blott, M., Guglielmo,
G. D., Duarte, J., Harris, P. C., Hauck, S., Liu, M.,
Neubauer, M. S., Ngadiuba, J., Memik, S. O., Pierini,
M., Aarrestad, T., B¨ahr, S., Becker, J., Berthold, A.,
Bonventre, R. J., Bravo, T. E. M., Diefenthaler, M.,
Dong, Z., Fritzsche, N., Gholami, A., Govorkova, E.,
Hazelwood, K. J., Herwig, C., Khan, B., Kim, S.,
Klijnsma, T., Liu, Y., Lo, K. H., Nguyen, T., Pezzullo, G.,
Rasoulinezhad, S., Rivera, R. A., Scholberg, K., Selig, J.,
Sen, S., Strukov, D., Tang, W., Thais, S., Unger, K. L.,
Vilalta, R., Krosigk, B., Warburton, T. K., Flechas, M. A.,
Aportela, A., Calvet, T., Cristella, L., Diaz, D., Doglioni,
C., Galati, M. D., Khoda, E. E., Fahim, F., Giri, D.,
Hawks, B., Hoang, D., Holzman, B., Hsu, S., Jindariani,
S., Johnson, I., Kansal, R., Kastner, R., Katsavounidis, E.,
Krupa, J., Li, P., Madireddy, S., Marx, E., McCormack,
P., Meza, A., Mitrevski, J., Mohammed, M. A., Mokhtar,
F., Moreno, E., Nagu, S., Narayan, R., Palladino, N., Que,
Z., Park, S. E., Ramamoorthy, S., Rankin, D., Rothman,
S., Sharma, A., Summers, S., Vischia, P., Vlimant, J.,
and Weng, O. Applications and techniques for fast
machine learning in science. Front. Big Data, 5:787421,
doi:10.3389/fdata.2022.787421,
2022,
arXiv:2110.13041.

Dennard, R. H., Gaensslen, F. H., Yu, H., Rideout,
V. L., Bassous, E., and LeBlanc, A. R. Design of
ion-implanted MOSFET’s with very small physical di-
IEEE J. Solid-State Circuits, 9:256, 1974,
mensions.
doi:10.1109/JSSC.1974.1050511.

Di Guglielmo, G. et al.

A reconﬁgurable neural
network ASIC for detector front-end data compres-
IEEE Trans. Nucl. Sci., 68:
sion at the HL-LHC.
2179, 2021, doi:10.1109/TNS.2021.3087100,
arXiv:2105.01683.

Duarte, J. et al. Fast inference of deep neural networks in
FPGAs for particle physics. JINST, 13(07):P07027, 2018,
doi:10.1088/1748-0221/13/07/P07027,
arXiv:1804.06913.

Esmaeilzadeh, H., Blem, E., St. Amant, R., Sankaralingam,
K., and Burger, D. Dark silicon and the end of multi-
core scaling. In Proceedings of the 38th Annual Inter-
national Symposium on Computer Architecture, pp. 365,
New York, NY, USA, 2011. ACM. ISBN 9781450304726,
doi:10.1145/2000064.2000108.

Farrell, S., Emani, M., Balma, J., Drescher, L., Drozd, A.,
Fink, A., Fox, G., Kanter, D., Kurth, T., Mattson, P., et al.
MLPerf HPC: A Holistic Benchmark Suite for Scientiﬁc
Machine Learning on HPC Systems. In 2021 IEEE/ACM
Workshop on Machine Learning in High Performance
Computing Environments (MLHPC), pp. 33. IEEE, 2021,
arXiv:2110.11466.

Govorkova, E., Puljak, E., Aarrestad, T., Pierini, M.,
Wo´zniak, K. A., and Ngadiuba, J. LHC physics dataset
for unsupervised New Physics detection at 40 MHz. Sub-
mitted to Sci. Data., 7 2021, arXiv:2107.02157.

Hawks, B., Duarte, J., Fraser, N. J., Pappalardo, A., Tran, N.,
and Umuroglu, Y. Ps and qs: Quantization-aware pruning
for efﬁcient low latency neural network inference. Front.
AI, 4, 2021, doi:10.3389/frai.2021.676564,
arXiv:2102.11289.

Hochreiter, S. and Schmidhuber,

term memory.
doi:10.1162/neco.1997.9.8.1735.

Long short-
J.
Neural Comput., 9(8):1735, 1997,

Hubbard, E. et al. Booster Synchrotron. Fermilab technical

memo, 1 1973.

Ignatov, A., Timofte, R., Kulik, A., Yang, S., Wang, K.,
Baum, F., Wu, M., Xu, L., and Van Gool, L. Ai bench-
mark: All about deep learning on smartphones in 2019. In
2019 IEEE/CVF International Conference on Computer
Vision Workshop (ICCVW), pp. 3617. IEEE, 2019.

ITU.

AI/ML in 5G challenge:

applying machine
learning in communication networks, 2021. URL
https://challenge.aiforgood.itu.int/
match/matchitem/34. Accessed: 17 January 2022.

James, S., Ma, Z., Arrojo, D. R., and Davison, A. J. Rlbench:
The robot learning benchmark & learning environment.
IEEE Robotics and Automation Letters, 5(2):3019, 2020.

Kafkes, D. and St John, J. BOOSTR: A dataset for
accelerator control systems. Data, 6(4):42, 2021,
arXiv:2101.08359.

Komiske, P. T., Metodiev, E. M., and Thaler, J. Metric space
of collider events. Phys. Rev. Lett., 123:041801, 2019,
doi:10.1103/PhysRevLett.123.041801,
arXiv:1902.02346.

Mattson, P., Reddi, V. J., Cheng, C., Coleman, C., Di-
amos, G., Kanter, D., Micikevicius, P., Patterson, D.,
Schmuelling, G., Tang, H., et al. MLPerf: An industry
standard benchmark suite for machine learning perfor-
mance. IEEE Micro, 40(2):8, 2020.

MLCommons. Science working group, 2020. URL https:
//mlcommons.org/en/groups/research-
science/. Accessed: 17 January 2022.

FastML Science Benchmarks

MLCommons.

Inference datacenter v1.1, 2021a. URL

https://mlcommons.org/en/inference-
datacenter-11/.

Thiyagalingam, J., Shankar, M., Fox, G., and Hey,
2021,

T. Scientiﬁc machine learning benchmarks.
arXiv:2110.12773.

Torelli, P. and Bangale, M. Measuring inference perfor-
mance of machine-learning frameworks on edge-class
devices with the MLMark benchmark. White Pa-
URL https://www.eembc.org/
per, 2019.
techlit/articles/MLMARK-WHITEPAPER-
FINAL-1.pdf. Accessed: 10 January 2022.

Zhu, H., Akrout, M., Zheng, B., Pelegris, A., Jayarajan,
A., Phanishayee, A., Schroeder, B., and Pekhimenko,
G. Benchmarking and analyzing deep neural network
In 2018 IEEE International Symposium on
training.
Workload Characterization (IISWC), pp. 88. IEEE, 2018,
arXiv:1803.06905.

MLCommons.

Inference edge v1.1, 2021b.

URL

https://mlcommons.org/en/inference-
edge-11/.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing
Atari with deep reinforcement learning. In Deep Learning
Workshop NIPS 2013, 2013, arXiv:1312.5602.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M. A., Fid-
jeland, A., Ostrovski, G., Petersen, S., Beattie, C., Sadik,
A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D.,
Legg, S., and Hassabis, D. Human-level control through
deep reinforcement learning. Nature, 518:529, 2015.

Moreno, E. A., Cerri, O., Duarte, J. M., Newman,
H. B., Nguyen, T. Q., Periwal, A., Pierini, M.,
Serikova, A., Spiropulu, M., and Vlimant,
J.-R.
identiﬁcation algorithm based on
JEDI-net:
interaction networks. Eur. Phys. J. C, 80:58, 01 2020,
doi:10.1140/epjc/s10052-020-7608-4,
arXiv:1908.05318.

a jet

Muhizi, J. and Duarte, J.

fastmachinelearning/fastml-
science, 2022, doi:10.5281/zenodo.5866587.
https://github.com/
URL
fastmachinelearning/fastml-science.

Pierini, M., Duarte, J. M., Tran, N., and Freytsis,
hls4ml LHC jet dataset (100 particles), 2020,

M.
doi:10.5281/zenodo.3602254.

Technologies.
Principled
nity
2019.
www.principledtechnologies.com/
benchmarkxprt/aixprt.

preview,

Aixprt

URL

commu-
https://

Reddi, V. J., Kanter, D., Mattson, P., Duke, J., Nguyen,
T., Chukka, R., Shiring, K., Tan, K.-S., Charlebois, M.,
Chou, W., et al. MLPerf Mobile Inference Benchmark.
2020, arXiv:2012.02328.

Sirunyan, A. M. et al.

Level-1
√

in
s = 13 TeV.

trigger

proton-proton

Performance of the CMS
at
2020,

JINST, 15:P10017,

collisions

doi:10.1088/1748-0221/15/10/P10017,
arXiv:2006.10165.

St. John, J. et al.

Real-time artiﬁcial

intelligence
the Fermilab
for accelerator control: A study at
Booster. Phys. Rev. Accel. Beams, 24(10):104601, 2021,
doi:10.1103/PhysRevAccelBeams.24.104601,
arXiv:2011.07371.

