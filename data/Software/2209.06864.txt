2
2
0
2

p
e
S
4
1

]
h
p
-
t
n
a
u
q
[

1
v
4
6
8
6
0
.
9
0
2
2
:
v
i
X
r
a

Experimental benchmarking of an automated deterministic error suppression
workﬂow for quantum algorithms

Pranav S. Mundada,∗ Aaron Barbosa,∗ Smarak Maity, T. M. Stace, Thomas Merkh, Felicity
Nielson, Andre R. R. Carvalho, Michael Hush, Michael J. Biercuk,† and Yuval Baum
Q-CTRL, Sydney, NSW Australia & Los Angeles, CA USA & Berlin, Germany
(Dated: September 16, 2022)

Excitement about the promise of quantum computers is tempered by the reality that the hard-
ware remains exceptionally fragile and error-prone, forming a bottleneck in the development of novel
applications. In this manuscript we describe and experimentally test a fully autonomous workﬂow
designed to deterministically suppress errors in quantum algorithms from the gate level through to
circuit execution and measurement. We introduce the key elements of this workﬂow, delivered as a
software package called Fire Opal, and survey the underlying physical concepts: error-aware com-
pilation; automated system-wide gate optimization; automated dynamical decoupling embedding
for circuit-level error cancellation; and calibration-eﬃcient measurement-error mitigation. We then
present a comprehensive suite of performance benchmarks executed on IBM hardware, demonstrat-
ing up to > 1000× improvement over the best alternative expert-conﬁgured techniques available
in the open literature. Benchmarking includes experiments using up to 16 qubit systems execut-
ing: Bernstein Vazirani, Quantum Fourier Transform, Grover’s Search, QAOA, VQE, Syndrome
extraction on a ﬁve-qubit Quantum Error Correction code, and Quantum Volume. Experiments
reveal a strong contribution of Non-Markovian errors to baseline algorithmic performance; in all
cases the deterministic error-suppression workﬂow delivers the highest performance and approaches
incoherent error bounds without the need for any additional sampling or randomization overhead,
while maintaining compatibility with all additional probabilistic error suppression techniques.

I.

INTRODUCTION

Large-scale fault-tolerant quantum computers are
likely to enable new solutions for problems known to
be hard for classical computers. The usefulness of a
quantum computer is ultimately determined by its abil-
ity to successfully implement meaningful and relevant
quantum algorithms in reasonable times and with rea-
sonable resources. While in recent years demonstrations
by Google [1] and the Chinese Academy of Sciences [2]
showed ﬁrst steps toward quantum advantage, a reliable
implementation of generic quantum algorithms is still out
of reach. Demonstrating the ability to run mid scale al-
gorithms on NISQ devices with high success probability
is an essential step in the route to quantum advantage in
the near future.

In augmenting the performance of NISQ-era algo-
rithms, much attention has focused on performance op-
timization at the level of individual quantum logic gates,
pushing them towards nominal fault-tolerance thresholds
[3–12]. However, experimental demonstrations [13, 14]
have identiﬁed a gap between the algorithmic perfor-
mance projected from gate-level proxy measures such as
randomized benchmarking and the actual performance
achieved on real hardware. This reﬂects the fact that the
quality of a quantum algorithm is aﬀected not only by the
quality of the individual constituent components (qubits,
gates, measurements), but also by the interplay of global

∗ These two authors contributed equally
† Also ARC Centre for Engineered Quantum Systems, The Uni-

versity of Sydney, NSW Australia

device and algorithmic properties such as device topol-
ogy, multi-qubit noise correlations, and circuit structures.
In order to better capture this phenomenology, a vari-
ety of system-level benchmarking strategies have been
devised, including the creation of more comprehensive
system-level metrics such as Quantum Volume [15], and
the development of algorithmic benchmarking test suites
[16].

In this work we introduce an automated workﬂow for
deterministic error suppression [17]
in quantum algo-
rithms, and present a wide range of performance bench-
marks highlighting how algorithms with varying charac-
teristics can be augmented through this tooling. We per-
form experimental demonstrations using a software pack-
age called Fire Opal on commercial hardware from IBM
and demonstrate > 1000× improvement over the best
expert-conﬁgured implementations using tools available
from the respective platforms. In our analyses we demon-
strate that there are typically substantial performance
gaps between the performance predicted from proxy mea-
sures such as gate-level randomized benchmarking and
realized algorithmic performance; we ascribe these to
non-Markovian error sources which are eﬃciently sup-
pressed using deterministic error suppression. Across de-
vices, topologies, and algorithms we show that through
a holistic view of error suppression from the gate level
to the circuit level we can approach incoherent error lim-
its.
Importantly, these techniques work with no addi-
tional user overhead in circuit execution, in contrast with
sampling techniques which involve many repeated exe-
cutions of each circuit [18–21]. We present results for
comprehensive benchmarking including the following al-
gorithms exhibiting widely varying characteristics (con-

 
 
 
 
 
 
nectivity, circuit depth, etc): Bernstein Vazirani, Quan-
tum Fourier Transform, Grover Search, Quantum Ap-
proximate Optimization Algorithm (QAOA), Variational
Quantum Eigensolver (VQE), Quantum Error Correc-
tion, and Quantum Volume.

We begin with an overview of typical hardware per-
formance and link to key error channels observed on
NISQ-era devices, many of which deviate substantially
from the assumption of Markovian errors. We then intro-
duce the key elements of an automated, deterministic er-
ror suppression pipeline designed to maximally suppress
hardware errors to incoherent limits. This is followed by
full reporting of performance benchmarking with quan-
titative analysis of observed performance enhancement
across a range of test algorithms. We conclude with a
brief discussion and future outlook.

II. BACKGROUND AND CONTEXT

A. Performance of algorithms on typical hardware

Quantum algorithms fail when executed on faulty
This simple phenomenology is routinely
hardware.
observed in both commercial and laboratory settings
through a variety of proxy measures such as circuit suc-
cess probability, or the overlap of the measured output
distribution relative to an ideal output. Ultimately this
phenomenology is linked to errors in the underlying hard-
ware elements.

Most hardware backends provide tabulated data for
the error rate associated with each gate in an algorithm,
as measured using device-level characterization protocols
such as randomized benchmarking, cycle benchmarking
and gateset tomography [22–28]. These protocols return
average proxy measures which characterize the ﬁdelity of
single and multiqubit gates, the building blocks of quan-
tum algorithms. With this information one may calculate
the expected performance of the algorithm by assigning
the backend-provided error rates to each operation (and
qubit) in the compiled machine instructions.

Algorithmic-performance estimates derived from these
proxy measures routinely overestimate actual hardware
utility [13, 14, 29]. This leads to a simple but fundamen-
tal observation: proxy measures for constituent opera-
tions are in isolation generally poor predictors for system-
level performance. As shown later, the presence of noise
and error correlations in space and time dominate this
discrepancy, and open opportunities for the deployment
of new techniques for performance enhancement leverag-
ing concepts from quantum coherent control.

B. Review of errors sources in near term quantum
computers

Overall, understanding and predicting algorithm-level
performance requires consideration of a much wider va-

2

riety of error processes than typically discussed in theo-
retical literature. In this subsection we review the most
common realistic error sources encountered in hardware,
including Non-Markovian (correlated or coherent) errors
which violate the assumption that errors are statistically
independent.

Single qubit coherence limits (T1/T2): The quantum
state of a single qubit can be represented by a complex
superposition of its internal states, A|0(cid:105) + B exp{iφ}|1(cid:105)
with A2 + B2 ≤ 1. While quantum gates intentionally
steer quantum states towards a desired target, quantum
noise and unwanted couplings may steer quantum states
as well. For example, noise processes may arise due to
energy loss to the environment, in which a qubit state
|1(cid:105) decays to |0(cid:105). This process is characterized by a typ-
ical time scale know as T1 and represents a stochastic
process producing statistically independent errors. In an-
other process called dephasing, the phase φ is randomized
through interaction with the environment (for instance
ﬂuctuating magnetic ﬁelds). The underlying processes
which cause dephasing are often characterized as being
stochastic, but in reality tend to exhibit strong temporal
correlations which result in coherent errors. This process
is characterized by a typical time scale know as T2 which
may be much shorter than T1. This is the ﬁrst canoni-
cally Non-Markovian error process typically encountered
in real hardware.

Calibration and gate errors: Universal quantum com-
puting can be achieved using a small and ﬁnite “universal
gate set” composed of single-qubit rotations and a mul-
tiqubit coupling mechanism. Both single-qubit and two-
qubit gates are typically realized by irradiating the qubits
with shaped wave-packets (pulses). During each oper-
ation, incoherent T1 decay processes contribute a gate
error approximately proportional to the ratio of τg/T1,
with τg the gate duration.

Additional Non-Markovian errors can arise from im-
perfect execution of the gate itself. The ideal gate im-
plementation has speciﬁc ideal pulse parameters (ampli-
tude, duration, frequency, waveform) and any deviation
of these parameters causes gate errors. Such deviation
can arise due to an imperfect calibration process (e.g.
identifying the qubit’s native frequency), slow variations
in system parameters, or signal distortions in generation
or transmission. Simultaneously, individual gate oper-
ations can experience Non-Markovian processes such as
leakage out of the qubit subspace induced by the driving
control signal’s spectral content. All of these processes
produce errors with long-time correlations across many
gates in a circuit.

Circuit level crosstalk : In addition to errors impact-
ing individual qubits, errors may also arise due to un-
wanted coherent coupling or semiclassical driving be-
tween nearby qubits, resulting in another substantive
form of Non-Markovian error. These phenomena are
known as crosstalk which may arise from a wide range
of physical mechanisms. Quantum crosstalk emerges due
to spurious coupling between qubits via the mechanism of

3

FIG. 1. The Fire Opal automated error-suppressing workﬂow for quantum algorithms. Each step (described in the main text)
is executed autonomously with no user conﬁguration, given an arbitrary input circuit and access to a supported quantum
hardware backend. Measurement error mitigation is performed after the hardware backend returns raw results, and the output
is returned to the user.

device fabrication or operation. The most common spu-
rious coupling in superconducting qubits is an unwanted
ZZ coupling; in this process, a qubit acquires a deter-
ministic phase shift which depends both on its state and
a nearby qubit state. Classical crosstalk emerges due to
unwanted driving of a qubit by control signals applied to
neighboring devices. Even when the drive on a neighbor-
ing device is oﬀ resonant, weak coupling over long cir-
cuits can introduce unwanted rotation errors. This phe-
nomenon may actually appear nonlocal across a device
because of the relatively long wavelength of microwave
control signals employed in many architectures, and the
challenges of microwave hygiene at the chip scale. Both
forms of crosstalk can produce substantial errors on idling
qubits during the execution of a circuit; classical crosstalk
can also cause challenges in the implementation of par-
allel temporally synchronous gates within a circuit.

Measurement errors: Measurement in quantum com-
puters involves a process of interaction with the quan-
tum device causing a probabilistic collapse of the sys-
tem’s superposition state to one of the measurement ba-
sis states. This physical process is itself susceptible to
error from the device-level interrogation procedure. For
instance, the microwave radiation used in reading out su-
perconducting circuits can induce unwanted transitions
between states, or the process of interpreting a (typically
analog) readout signal such as the phase of reﬂected mi-
crowaves from a resonator can lead to to imperfections
in assigning a |0(cid:105) on |1(cid:105) state. These individual read-
out errors grow exponentially with qubit count and are
often spatially correlated, adding substantial complexity
in disambiguating between various output bitstrings at
the conclusion of a computation.

III. AUTOMATED WORKFLOW FOR
DETERMINISTIC ERROR SUPPRESSION

We introduce an eﬃcient protocol for error suppression
that is designed to enhance the performance of quantum
algorithms on quantum hardware. Our strategy is based
on purely deterministic approaches that reduce errors
without the use of sampling or randomization methods;

i.e. they do not require additional execution overhead in
repetition in order to attain error reduction.

In this workﬂow the user provides an algorithm spec-
iﬁed in the QASM intermediate representation [30] and
our protocol executes a series of deterministic error sup-
pression strategies to augment the machine instructions
prior to execution. Additional postprocessing is then em-
ployed in order to improve readout eﬃciency.

In a separate manuscript we describe the detailed
methodologies in use, but here provide a brief summary
of the error-reduction pipeline, which is schematically vi-
sualized in Fig. 1.

1. Depth reduction and logical transpilation – A se-
quence of compiler passes is used to mathemati-
cally reduce the depth (gate count) of the quantum
circuit and the logical operations in the circuit are
mapped to the native gates available on the hard-
ware. This step is executed in runtime on user ini-
tiation of the pipeline.

2. Error-aware hardware mapping – Error-aware com-
pilation is used to best select the appropriate subset
and logical assignment of qubits on a device. These
additional passes account for device topology, tab-
ulated gate errors, parallel-gate crosstalk, etc. This
step is executed in runtime on user initiation of the
pipeline.

3. Eliminate circuit crosstalk – Dynamical decou-
pling (DD) sequences are incorporated to mitigate
various idling errors including dephasing and ZZ
crosstalk at the algorithmic level. The underlying
pulse sequences are optimized for the speciﬁcations
of a particular backend via an automatic “ranking”
routine. The ranking routine is context-aware, ap-
plying optimal sequences to each qubit’s idle period
with knowledge of single-qubit gate errors as well
as the gate sequences applied to neighboring qubits
in order to account for both quantum and classical
(single-qubit-gate) crosstalk. This step is executed
in runtime on user initiation of the pipeline.

4. Optimized gate replacement – Backend error rates
and device coherence times are consumed in order

to identify gates with substantial headroom above
incoherent (T1) limits. Parallelized AI-powered op-
timizers then automatically perform an eﬃcient
analog-level gate optimization. The process in-
volves automated parsing of the device topology
to ensure parallel gate optimizations do not share
qubits; multiple parallelized steps are executed in
series to ensure all relevant single and or multi-
qubit gates are optimized. These routines are per-
formed in the background and are not part of the
user workﬂow. The output of this optimization is
a lookup table of new analog-layer gate deﬁnitions
that are called at runtime in deﬁning the overall
circuit.

5. Run experiment. The user circuit is conﬁgured for
execution in an appropriate language supporting
both gate timing and analog-layer waveform con-
trols such as qiskit pulse, pyquil, or AWS braket
[31–33]. The circuit is then executed on the hard-
ware backend over a speciﬁed number of shots and
results returned.
In this manuscript we focus on
demonstrations employing IBM hardware with cor-
responding choice of low-level language.

6. Measurement-error mitigation – A ﬁnal postpro-
cessing step is performed on all shots returned
from the backend. An initial AI-driven calibra-
tion routine identiﬁes measurement-error processes,
and this calibration data is stored. In runtime, the
calibration data and measurement results are com-
bined in order to estimate the probability distribu-
tion of the hardware’s output bit strings and an as-
sociated conﬁdence interval. This process scales as
O(1) with qubit count, providing an eﬃcient mech-
anism to capture correlations with low calibration
overhead and fast processing. This step is executed
in runtime with no additional overhead on the part
of the user.

This error-suppressing pipeline is applicable to any
quantum computing architecture, and is not speciﬁc to
a particular qubit type; hardware-speciﬁc elements are
captured by the various front-end and back-end compiler
stages.
Importantly, it can be executed independently
in NISQ-era algorithms and also in the augmentation
of fault-tolerant quantum error correction routines as
demonstrated below. Further context on existing strate-
gies for error management in NISQ era devices is pro-
vided in Appendix A [18–21, 34–40].

In practice, this workﬂow is “zeroconﬁg” requiring no
user intervention, conﬁguration, or coding. This stands
in contrast to other techniques where substantial cod-
ing and conﬁguration is required by a user to execute
or combine various error-suppression strategies (see Ap-
pendix A). All calibration and optimization steps for
measurement-error mitigation and automated gate op-
timization are executed in advance and results stored
for on-demand algorithmic execution. Other protocols

4

use runtime decision-making to select strategies based on
the updated backend hardware parameters. In practice,
a user inputs a QASM deﬁnition of a circuit; execution
and result-return is conducted with a single command.

IV. ALGORITHMIC BENCHMARKING WITH
DETERMINISTIC ERROR SUPPRESSION

A. Benchmarking settings

We now present a series of experimental demon-
strations of algorithmic benchmarking using the error-
suppressing pipeline described above.
In order to un-
derstand performance advantages under widely varying
settings – locality, circuit depth and width, circuit den-
sity – we select a comprehensive suite of benchmarks, fol-
lowing previous publications on algortihmic benchmark-
ing, augmented with protocols including quantum error
correction and Quantum Volume. An overview of the
selected benchmarks, their characteristics, and the un-
derlying evaluation metrics is presented in Table I.

We benchmark our methods against two software con-
ﬁgurations and execute on several diﬀerent IBM backends
(See Table II). All algorithms are conﬁgured in Qiskit
and then compiled to hardware instructions using the
two alternative methods. The ﬁrst method uses the de-
fault settings deﬁned by the hardware provider and in-
cludes the most aggressive compilation scheme the plat-
form provides. Most common users do not alter or add
to the default settings. The second method we bench-
mark against uses the best expert-conﬁgured implemen-
tations of tools for error suppression available from the
respective platforms; such available methods include in-
terleaved dynamic decoupling, extra compilation strate-
gies, and measurement-error [41, 42], which can be added
by well informed users. We refer to the second method
as “expert settings” as conﬁguration requires a user to
be knowledgeable about both the underlying strategy
and how to conﬁgure the technique in circuit execution.
When performing comparisons we ensure all conﬁgura-
tions (Default, Expert, and Q-CTRL) are executed back-
to-back in the same session in order to minimize the im-
pact of daily parameter variations known to occur on
these devices [43]. Full details of the relevant bench-
marking conﬁgurations are presented in table II.

B. Deterministic algorithms

Deterministic algorithms are designed to perform a
speciﬁc task. Given an initial state, their ideal outcome
if fully deterministic. Such algorithms includes, among
others, the Bernstein-Vazirani (BV) algorithm, the quan-
tum Fourier transform (QFT), Grover search, Shor’s al-
gorithm, Hamiltonian evolution and more. As some of
these algorithms require thousands of entangling gates
even for small number of qubits, we focus our attention

Algorithm

Characteristics

Evaluation metric

Bernstein-
Vazirani (BV)

Shallow and Sequential - circuit depth
scales linearly with circuit width (for
full connectivity) and native entangling
gates do not overlap.

Success probability of returning the
all
‘1’ state. This selection con-
stitutes a worst-case scenario us-
ing the most challenging (state-
dependent) oracle.

5

Performance
enhancement
> 1000× improvement in
success probability, up to 16
qubits.

Quantum Fourier
Transform
(QFT)

Grover Search

Quantum
Approximate
Optimization
Algorithm

Variational
Quantum
Eigensolver

Intermediate depth and partially
sequential - circuit depth scales
quadratically with circuit width (for full
connectivity). Parts of the algorithm
can be parallelized.
High depth, scaling depends on the
implementation of the multi-CX gate.
The depth can be reduced by allowing
the addition of ancilla qubits.
Depth and density depend on the
speciﬁc problem. Problem under test is
a ﬁve qubit max-cut type Hamiltonian
with depth and density similar to the
QFT class.

Depth and density depend on the
speciﬁc ansatz. Problem under test uses
four and six qubit Ry ansatzes with
linear entanglement. Each round of the
ansatz has depth and density in the BV
class.

Success probability averaged over
16-32 initial states (chosen to be in-
verse QFT of single bitstrings).

7× (3×) over default (ex-
pert), up to 7 qubits.

Selectivity and distance (circuit in-
ﬁdelity) to the ideal target proba-
bility distribution over 32 diﬀerent
5Q target states.
Structural
similarity metric be-
tween the ideal and measured cost
landscapes.

8× improvement in circuit
ﬁdelity; transform selectiv-
ity (S < 0 → S > 1) for all
target states, up to 5 qubits.
28× improvement
in the
Structural similarity metric.

Ground state energy accuracy for
BeH2 molecule and deviation from
the ideal set of Pauli expectation
values using the Pearson distance.

5× error reduction in the
mean ground state energy
prediction,
and 16× im-
provement in the Pearson
distance of
the measured
Pauli expectation values.

4× improved error detection
in the repetition code and
3.3× for the full ﬁve qubit
code (total 9 qubits).
QV increased from 32 to 64.

Quantum Error
Correction codes

Depth in the QFT class. Due to device
topology, the encoding block and each
of syndrome inference blocks are highly
sequential.

Agreement between syndrome read-
out and errors inferred from direct
data qubits readout.

Quantum Volume Intermediate depth (similar to QFT)
with maximal density. Operation are
maximally parallelized.

Mean heavy output (HO) probabil-
ity, averaged over 300 random cir-
cuits sampled with 1000 shots each.

TABLE I. Benchmark algorithms. The algorithms chosen diﬀer in their depth to width ratio, operations density, duration of
idling periods and the sparseness of the ideal output distribution. These diﬀerent characteristics pose diﬀerent challenges and
sensitivities to errors.

on the BV and QFT algorithms along with an optimized
version of a ﬁve-qubit Grover’s search.

1. Bernstein Vazirani

An N -qubits BV algorithm has a deterministic answer
in the form of a single (N − 1)-qubits bitstring. The
answer (target) bitstring is determined by the choice of
oracle circuit. We choose the oracle circuit in such a way
that the answer is given by the all ’1’ bitstring. This or-
acle is the most challenging one among the possible BV
oracles as its circuit includes the highest number of en-
tangling gates, hence, the results we obtain can be viewed
as worst performance among all possible oracles. As the
ideal answer is a single bitstring, we use success proba-
bility (SP) as a quality measure, i.e., the probability to
measure the correct bitstring.

Typical comparative performance for BV is illustrated
in Fig. 2. For both default and expert setting, as the
number of qubits grow, the likelihood that the algo-
rithm returns the correct answer diminishes. By the
time the algorithm integrates just nine qubits, the like-
lihood of success for the default conﬁguration is less
than approximately one percent. For 16 qubits, in this
demonstration, the base hardware does not return the
correct answer over 32,000 attempts. Adding the de-
terministic error-suppressing pipeline delivers substan-
tial beneﬁts for all circuit widths; as the width increases,
the relative performance advantage increases with qubit
count. In this demonstration at 16 qubit circuit-width
the Q-CTRL pipeline delivers success probability ap-
proximately ∼ 20%, with daily performance ﬂuctuations
bringing this occasionally near 40%.
In all trials exe-
cuted, the Q-CTRL error-suppressing pipeline always de-
livers the best performance for all circuit widths.

Component
Compilation
method

Default
Qiskit level 3 Sabre compiler,
best out of four seeds.

Quantum Logic
Gates
Error mitigation
strategies

Default

None

Expert
Qiskit level 3 Sabre compiler com-
bined with Qiskit mapomatic func-
tion for layout selection.
Default

Qiskit native dynamical decoupling
and Qiskit measurement-error miti-
gation (“complete” for circuits with
≤ 7 qubits and “M3” > 7 qubits).

6

Q-CTRL
Q-CTRL compilation passes and
layout-selection optimization.

optimal

Q-CTRL autonomously optimized
two-qubit gates.
Q-CTRL
context-
crosstalk-
aware
suppressing dynamical decoupling
sequences and Q-CTRL scalable
measurement-error mitigation.

embedding

of

TABLE II. Benchmarking conﬁgurations as implemented on the IBM backend. Devices used: Lagos, Jakarta and Guadalupe.

FIG. 2. Success probability of a BV algorithm as a function of the number of qubits. Default, expert, and Q-CTRL settings
are described in Table II. Associating the mode of the distribution with the correct answer, the Q-CTRL pipeline returns the
correct answer for all system sizes while the other methods fail to return the correct answer beyond 8 qubits (default) and
beyond 10 qubits (expert).

Further detail is revealed when examining these re-
sults on a logarithmic scale and comparing the achieved
success probability to that predicted using simple proxy
error measures (Fig. 3). The ﬁrst proxy measure uses
backend-provided gate-error rates in order to estimate
the circuit ﬁdelity given the count of various gate opera-
tions (Gate-error limit), assuming gate imperfections are
the only source of error. A second proxy measure is esti-
mated assuming performance is bounded by irreversible
T1 decay processes occurring throughout the execution
of the circuit. The two estimates are similar, but the T1
limit in this case is slightly lower due to the presence of
idle periods which are not properly accounted assuming
only gate errors.

We observe, consistent with Refs [13, 14], that the
achieved performance using the default and expert set-
tings is up to ﬁve orders of magnitude lower than the
bounds derived from simple proxy measures. This ob-
servation highlights the importance of correlated error
processes that are not typically captured eﬀectively by

considering independent-gate-error models. By contrast,
the Q-CTRL pipeline delivers performance within a fac-
tor of order unity of the two proxy bounds. Additional
observations show that over an eight month period the re-
sults from the Q-CTRL pipeline remain constant within
a factor of order unity, while for any particular circuit
width the performance of the default and expert con-
ﬁgurations can vary up to over an order of magnitude
(see App. B). There remains residual opportunity for im-
provement, but these comparisons give strong evidence
that deterministic error suppression can eﬀectively sta-
bilize against correlated error processes that otherwise
limit algorithmic performance.

2. Quantum Fourier Transform

The Quantum Fourier Transform represents a mapping
between quantum states in the Hilbert space. In order
to eﬃciently estimate its performance without the need

7

FIG. 4. The mean success probability of a QFT circuit as
a function of the system size, over 8 diﬀerent initial states.
Absolute performance is lower than for BV due to the deeper
circuit structure.

3. Grover’s Search

The last deterministic algorithm we benchmark is
Grover’s search, whose solution contains a predeﬁned set
of bitstrings (we focus on the case where the solution set
includes a single bitstring). The solution of the search
is encoded as the mode of the output distribution; we
compare the measured distribution over all possible bit-
strings to the ideal distribution in order to determine
success probability (see Table I). We quantify the agree-
ment between the ideal and the measured distributions
using circuit inﬁdelity, I = 1 − (1 − H 2)2, with H be-
ing the Hellinger distance between the two probability
distributions.

(cid:17)

In order to quantify the usefulness of the search, we de-
(cid:16) pt
ﬁne the selectivity, S = log2
, where pt is the proba-
pn
bility of measuring the target bitstring of the search and
pn is the probability of measuring the most probable bit-
string which is not the target of the search. The number
of shots needed in order to guarantee the correct solu-
tion with high conﬁdence level scales as p−1
t S−2, i.e., pt
alone is not a suﬃcient measure of quality of a Grover’s
search or any algorithm that encodes the solution in the
mode of the output distribution. Eﬀectively, this metric
enables a quantitative measure for the algorithm’s ability
to discriminate the target state from a noisy background
distribution.

Large positive values of S indicate that the correct an-
swer can be obtained consistently and eﬃciently (Strong
correct regime), i.e., by sampling the algorithm a small
number of times (in the limit S → ∞ a single shot is
guaranteed to return the correct answer). Low, yet pos-
itive, values of S ≤ 1 allow ﬁnding the correct answer
consistently but require additional sampling overhead in

FIG. 3. Success probability of the BV algorithm for the diﬀer-
ent settings as appeared in Fig. 2, together with two projected
performance bounds which assume the existence of either un-
correlated gate errors or T1 processes only. The gate-error
limit is calculated by a simple multiplication of the ﬁdelities
derived from the backend for all single and two-qubit gates
in the circuits. The circuit T1 limit is calculated by a multi-
plication of the T1-limited ﬁdelity of each participating qubit,
(cid:16)
calculated as (cid:81)
−Ti/2T (i)
; here Ti is the total active
time of qubit i (from ﬁrst operation until measurement).

i exp

(cid:17)

1

for full multiqubit state tomography, we initialize a cir-
cuit to a state which is an inverse Fourier transform of a
single bitstring. Such states can be generated with high
ﬁdelity by a single layer of Hadamard gates followed by
a single layer of virtual Z rotations; these still form a full
superposition of all possible bitstrings despite the simple
initialization process. After execution of the QFT algo-
rithm on these initial states, we measure the probability
of ﬁnding a speciﬁc bitstring as the output of the algo-
rithm. We repeat this procedure for diﬀerent initial state
and average the success probability achieved for each tar-
get output bitstring.

As in the BV demonstrations, we observe rapid degra-
dation of measured success probability with circuit width
for the default and expert settings when executed on
hardware (Fig. 4). In this case the greater circuit depth
used for QFT results in a more rapid decay than BV. The
addition of the Q-CTRL error-suppressing pipeline shows
a consistent improvement over both the expert settings
(over 3×) and default settings (7×). These enhancements
are larger than achieved for BV at similar circuit widths.
We have not performed experiments beyond 7 qubits for
QFT, yet, we expect similar growth in performance en-
hancement as achieved in BV.

8

FIG. 5. (a-c) The output probability distributions over all possible 5Q bitstrings after two iterations of a Grover search algorithm
with a speciﬁc oracle (00101 as a solution). (a) is the ideal expected distribution as was calculated using a noiseless simulator,
and (b-c) are probabilities obtained using default settings (black) and using Q-CTRL pipeline (purple). The resemblance
between the measured and ideal distributions is captured by the circuit inﬁdelity. The distribution obtained by using Q-CTRL
tools is 8× closer to the ideal results. Similar improvement is observed for all target states. (d) Selectivity calculated for all
target bitstrings, demonstrating S > 1 for all possible 5 qubit states using the Q-CTRL pipeline compared to S < 0 using the
default settings.

order to yield the correct answer (Weak correct regime).
For S ≤ 0, independent of the number of samples, the
algorithm will consistently yield an incorrect answer (in-
correct regime).

We present the results of a ﬁve-qubit Grover’s bench-
mark in Fig. 5. We implement an optimized construction
of the search algorithm (which uses two additional ancilla
qubits as presented in [44]), in order to overcome the ex-
treme circuit depth required in a device with weak con-
nectivity. We restrict ourselves to implementing at most
two iterations of the search protocol, approximately con-
√
sistent with the expected
N iteration for an N -qubit
search required to guarantee convergence in the ﬁve-qubit
case we test.

In this benchmark, we consistently observe a binary
distinction where the default implementation fails to
yield the correct answer while the Q-CTRL pipeline cor-
rectly identiﬁes the target. In our observations Default
and expert conﬁgurations were qualitatively similar in
their inability to produce a correct outcome from the
search. Examining more closely, a calculation of the se-
lectivity for all ﬁve-qubit target bitstrings (Fig. 5d) in-
dicates that the default settings consistently result in
S < 0, while with the Q-CTRL error-suppressing pipeline
S > 1 in all cases.

C. Hybrid algorithms

Hybrid algorithms such as VQE and QAOA are in-
creasingly popular due to their utility in a range of opti-

mization problems and the potential to deliver quantum
advantage in the NISQ era. They have been applied to
a wide range of tasks from chemistry to transport and
ﬁnancial optimization. Despite their popularity they are
more challenging to benchmark than their determinis-
tic cousins due to their hybrid quantum-classical struc-
ture. The achieved performance in this case depends not
only on the quantum processor performance, but also
the choice of ansatz, selected level of approximation, the
number of circuit evaluations available, and the ability of
the extrinsic classical optimization algorithm to navigate
the quantum search space.

We isolate QPU performance, and explore the ability of
our error-suppressing pipeline to improve the execution
of the underlying quantum circuits in a single, arbitrary
QAOA or VQE execution.

1. QAOA

We choose a typical problem Hamiltonian (Hp) for
QAOA, containing all single and pairwise Z terms with
non uniform coeﬃcients [45], and ﬁxing the approxima-
tion level (p). We establish baseline performance by
calculating the cost function (expectation value of Hp)
for large choices of parameters using an ideal (noise-free)
simulator. We then execute the same circuits on hard-
ware, directly evaluating the output for the same range
of {γ, β} QAOA parameters and compare the measured
and the ideal cost landscape; in the limit of perfect QPU-
hardware execution these two landscapes should be iden-

tical.

In order to quantitatively evaluate the similarity of
the cost landscapes we use a structural similarity met-
ric (SSIM) which is widely used in image analysis [46].
This metric compares two images in a manner known to
be more robust to high-spatial-frequency noise than typ-
ical variance calculations. A score of one corresponds to
ideal matching, and in this case implies that the ideal
and measured cost landscapes are identical.

As an example we calculate the SSIM comparing the
default hardware implementation against the ideal land-
indicating low
scape, and demonstrate SSIM∼ 0.03,
structural similarity to the ideal landscape (Fig. 6). In
the presence of realistic, strongly correlated hardware er-
rors the QAOA landscape exhibits areas of relatively bar-
ren plateaus with respect to the ideal landscape. Adding
the Q-CTRL pipeline increases the SSIM by 28× to
SSIM∼ 0.84. This demonstrated ability of determinis-
tic error suppression at the hardware level to improve
QPU performance in QAOA stands in contrast to sam-
pling routines postulated to be unable to improve the
inﬂuence of barren plateaus [47].

2. VQE

The variational quantum eigensolver (VQE) uses the
variational principle to compute the ground state energy
of a Hamiltonian. VQE does so through the use of a
parameterized circuit with a ﬁxed form known as a vari-
ational form or an Ansatz.

We benchmark the performance of VQE by utilizing it
to compute the ground state energy of Beryllium hydride
(BeH2) molecule [48], which can be well described (with
a minimal loss of precision) by considering four interact-
ing atomic orbitals and a spin degree of freedom, which
in turn, can be mapped to a 6-qubit problem. Such a
mapping converts the Hamiltonian of the molecule into
a sum of multi-qubit Pauli operators over the 6 qubits.
Further approximations can be made (freezing additional
degrees of freedom) leading to a 4-qubit representation
of the molecule’s Hamiltonian. Both the 4- and 6-qubit
representations of the Hamiltonian are taken from the
Qiskit-nature library [49].

We generate trial wavefunctions using the two-local
ansatz with two repetitions, in which Ry gates are used
to generate rotations and CNOT gates are used to gener-
ate entanglement. This variational form is known as the
Ry ansatz with linear entanglement, and it has 18 free
parameters [50].

We evaluate the ground state energy of the BeH2
molecule for diﬀerent bond distances ranging from 0.5˚A
to 5˚A. For each setting we optimize the parameters by
running the algorithm on an ideal simulator, and then
use the optimized parameters to compute the expecta-
tion of the Hamiltonian on real QPU hardware. We col-
lect 8 × 103 samples for each quantum circuit we run on
the real QPU.

9

FIG. 6. The cost (expectation of H) landscape of a p = 1
ﬁve qubits QAOA. Here, we compare the ideal landscape
(top), the experimentally obtained landscapes using the de-
fault settings (middle) and using the Q-CTRL pipeline (bot-
tom). Structural similarity relative to the ideal landscape is
calculated and shown for each setting (see text).

Fig. 7 shows the computed ground state energy of the
BeH2 molecule as a function of the Be-H bond distance
for the 6-qubit ansatz. The dotted line displays the result
from an ideal simulator, and the individual points rep-
resent the results computed on real hardware with and
without our pipeline; inclusion of the Q-CTRL error sup-
pressing pipeline shows substantially better agreement
between the estimated ground state energy and the ideal
one. The mean energy deviation from the ideal value ob-
tained using the Q-CTRL pipeline is about 5× smaller
compared to the hardware default for the 6-qubit ansatz.
Similar experiment with the 4-qubit ansatz (not shown)
yields an energy deviation about 3.5× smaller with the
Q-CTRL pipeline.

Each data point in Fig. 7 (a) is obtained by a weighted

10

We can further analyze the deviation from the ideal
expectation values. As apparent, with default and ex-
pert settings, when the ideal expectation value is near
zero, the deviation from the measured values is relatively
small. This eﬀect is expected, as random errors typically
yield ﬂat probability distributions that are characterized
by small expectation values. In stark contrast, probabil-
ity distributions that lead to extremal expectation val-
ues (near ±1), are unlikely to be generated by random
noise. Indeed, both with default and expert settings, the
deviation in expectation values near the extremal val-
ues grows dramatically as a function of the expectation
value. With Q-CTRL settings, the deviation curve is ap-
proximately ﬂat indicating that the expectation values
are consistently originating from the correct probability
distributions.

Statistical methods, such as zero-noise extrapolation,
Pauli twirling and random compilation (see Appendix A)
can follow our deterministic pipeline to further improve
the accuracy of the expectation values by reducing the
eﬀect of Markovian errors at the price of extensive over-
head.

D. Quantum Error Correction

Quantum Error Correction (QEC) is a critical task
that will be required for full quantum computation. At
its heart, QEC depends on an encoding, which redun-
dantly maps k logical qubits into n > k physical qubits,
and n − k stabiliser generators, which provide partial
information about hidden errors that may occur. Collec-
tively, measurements of the stabilisers produce syndrome
data, and a decoding algorithm, processes the syndrome
to provide a likely correction operator that has a high
probability of correcting the error [51].

Diﬀerent QEC codes have diﬀerent performance char-
acteristics, such as threshold [52–55], geometry [56], spar-
sity [57], rate, and decoder complexity [58, 59], all of
which aﬀect how eﬀectively logical information can be
stored. However, the basic physical processes are very
similar across all stabiliser codes: ancillary qubits are
entangled with the data qubits using circuits of one- and
two-qubit gates to eﬀect the few- or many-body stabiliser
measurements that provide the error syndrome.

At a low level, QEC is a form of hybrid quantum al-
gorithm that generates ‘mid-circuit’ measurement results
(i.e. syndrome data) used in a classical decoding process
to infer and correct for errors during the course of a logi-
cal computation. From this perspective, the algorithmic
protocol constituting all key steps is amenable to circuit-
level performance optimization using the tools described
in Sec. III.

We discuss two error-correcting codes: the 5-qubit rep-
etition code which is capable of correcting up to two
bit-ﬂip errors, and the 5-qubit quantum error correction
code, which is capable of correcting any single qubit er-
ror. For each, we encode a logical qubit into the phys-

FIG. 7. VQE performance with diﬀerent workﬂows. (a) Com-
puted ground state energy of the BeH2 molecule as a function
of the Be-H bond distance for a 6-qubit ansatz. The dotted
line displays results from an ideal simulator, and the individ-
ual points represent the results computed on real hardware
with diﬀerent settings.
(b) Deviation from expected Pauli
measurement as a function of the ideal value. Perfect agree-
ment between theory and experiment would correspond to all
points on the zero dashed line. To quantify the agreement,
we calculate the Pearson distance, Pd for the three methods
- Default: 0.177, Expert: 0.042, Q-CTRL: 0.011

sum of diﬀerent Pauli expectation values. In Fig. 7 (b),
we show all the measured expectation values across dif-
ferent bond distances, and compare them to their ideal
values. We quantify the agreement between the ideal and
measured data using the Pearson distance, Pd = 1 − Pc,
with Pc being the standard Pearson correlation coeﬃ-
cient. A value of Pd = 0 corresponds to perfect correla-
tion where the deviation from the ideal value is zero for
all points, while Pd = 1 corresponds to a random data
where the measured data is a random number between
−1 to 1 independently of the ideal value. For six qubits,
the agreement between the calculated expectation val-
ues and the ideal values is 16× higher when using the
Q-CTRL pipeline (4× higher with respect to the expert
settings).

11

FIG. 8. Performance of QEC using the Q-CTRL pipeline. (a-d) Benchmarking data for the ﬁve-qubit repetition code. (a-b)
joint probability distribution of the measured and expected syndromes for the diﬀerent settings after an error was injected to
qubit 2. The syndrome bit values “0” or “1” correspond to the measurement outcomes “+1” and “−1” for each of the stabilizer
generators in the ordered set, G, in Eq. 1. (c) Detection success as a function of the injected error for the default and Q-CTRL
pipelines using color coding consistent with previous graphs. Dashed line denotes the randomness threshold, which represent
the success achieved by a random chance. (d) Mean detection success averaged over the six bit-error-location possibilities,
comparing the two protocols. Arrow indicates quantitative performance enhancement, calculated with respect to the dashed
line. (e-f) Data on full ﬁve-qubit QEC. (e) Detection success for each of the four stabilizers measured for the diﬀerent pipelines.
As before, the dashed line is the random chance value. (f) Overall mean (over injected errors) detection success incorporating
data averaged over all four stabilizers simultaneously with net performance enhancement with respect to the random value
represented.

ical data qubits using a standard encoding circuit, in-
ject an artiﬁcial error into the encoded qubit, and then
perform syndrome measurements on ancilla qubits to in-
directly read out the syndrome. Because of the limita-
tions in mid-circuit measurement and conditional logic
in the hardware interface we used, we were not able to
perform real-time feedback for correction. Instead focus
on the performance of a single iteration of the syndrome
extraction protocol. We execute this process using both
a default conﬁguration and Q-CTRL’s automatic error
suppression pipeline, similar to the other benchmarking
algorithms presented above.

We ﬁrst implement a circuit for encoding a logical XL
state of the 5-qubit repetition code [51], followed by one
round of ancilla-assisted syndrome readout, and termi-
nated by direct single-qubit measurements on all data

and ancilla qubits; this circuit uses a total of nine qubits.
Because this code is classical, all the stabilisers, which
are generated by the stabilizer generators in the ordered
set:

G = {Z0Z1, Z1Z2, Z2Z3, Z4Z4},

(1)

which commute pointwise. As a result the syndrome
can be found either using ancilla qubits to give the mea-
sured syndrome, or determined directly using single-qubit
measurements on the data qubits to give an expected
syndrome. Comparing the measured and expected syn-
drome, which ideally should agree, gives an estimate of
the eﬃcacy of the measurement.

In Fig. 8(a)-(b) we apply an artiﬁcial bit-ﬂip error on
q2, and measure the joint probability distribution of the
measured and expected syndromes; perfect agreement of

these distributions would yield only diagonal elements
matching the location of any induced error. Ideally, this
error would generate the syndrome ”0110”, and the prob-
ability of both measured and expected syndromes would
be concentrated at this diagonal element. Imperfections
manifest as oﬀ-diagonal elements with nonzero probabil-
ity. Comparing the default and Q-CTRL pipelines, we
see that the default conﬁguration yields a joint proba-
bility distribution with high oﬀ-diagonal weight relative
to the diagonal. This indicates that the syndrome ex-
traction process fails more often than it correctly identi-
ﬁes the error location. By contrast, using the Q-CTRL
protocol the joint-distribution is much more constrained
to the correct diagonal element at ”0110”,
indicating
consistency between measured and expected syndromes,
given the applied error. The error-detection success is
measured as the trace over the joint probability distri-
bution for each injected-error location in Fig. 8(c) and
consistently shows higher performance for the Q-CTRL
pipeline. We make quantitative comparisons between the
two approaches by subtracting the contribution of ran-
dom chance (1/16) from the detection success metric and
calculating the performance ratio; we observe up to 4×
improvement for error-detection success with a single bit-
ﬂip error and an average of 2.4× improvement over the
ﬁve diﬀerent error locations (Fig. 8(d)).

We next consider the performance of the 5-qubit QEC
code, implemented using the encoding circuit from [60].
In this case stabilizers do not commute pointwise, and as
a result it is not possible to determine the full syndrome
directly from single-qubit measurements at the end of the
circuit. Instead, we can choose one of the stabilizer gener-
ators to measure pointwise, to extract the expected syn-
drome. For concreteness we consider the 5-qubit [5, 1, 3]
code which involves ﬁve data and four ancilla qubits
used for syndrome measurement; the stabiliser generators
are {XZZXI, IXZZX, XIXZZ, ZXIXZ}, which mu-
tually commute but mix X and Z operators. We choose
one of the generators to determine from single-qubit end-
circuit measurements on the data qubits, asking whether
the result agrees with the corresponding syndrome bit
measured using ancilla qubits. Repeating this for diﬀer-
ent choices of the bit-wise stabilizer measurements, we
build up statistics for the success probability.

Fig. 8(e) shows the probability that a given direct
measurement of a stabilizer generator matches the corre-
sponding ancilla-measured syndrome bit for each of the
four stabilizer generators, given all 15 possible single-
qubit errors applied artiﬁcially. A probability of 50%
on this scale corresponds to completely random, uncor-
related outcomes; 100% indicates that the measured syn-
drome bit and the directly determined stabilizer bit al-
ways agree. Results using the Q-CTRL protocol are con-
sistently higher than the default pipeline, and all give
an enhanced likelihood of error identiﬁcation using the
Q-CTRL pipeline. By contrast, two stabilizer measure-
ments performed with default conﬁgurations are consis-
tent with random chance. The average detection suc-

12

cess per stabilizer above the randomness threshold (over
the four stabilizers) is ∼ 2.5× higher using the Q-CTRL
pipeline and the overall detection success (of all four sta-
bilizers simultaneously) is ∼ 3.3× higher using the Q-
CTRL pipeline (Fig. 8(f)).

We note that the hardware layout on which we imple-
ment these codes was optimized for the ‘heavy-hexagon’
QECC [61], and was not optimal for the small demon-
stration codes we used. This contributes to the relatively
low absolute performance of syndrome measurement ob-
tained in our demonstrations. Nonetheless, we are able
to compare the impact of hardware and circuit level im-
perfections on the execution of the key algorithmic steps
of QEC with our tests.

E. Quantum Volume

Quantum Volume (QV) is a single metric that quanti-
ﬁes the largest random circuit of equal width and depth
that a quantum computer can successfully implement.
Higher quality quantum computing systems are expected
to have higher quantum volumes. Estimating quantum
volume for hardware involves a complex protocol in which
a wide range of random circuits are executed [15]. The
ideal output distribution of each circuit is calculated us-
ing a noiseless simulation, and the heavy output set (all
the bitstring with higher probability than the median
bitstring probability) is extracted. The circuits are then
executed and sampled repeatedly in order to estimate
the probability to measure a bitstring that belongs to
the ideal set of heavy outputs (HO). We refer to this
probability as the HO probability.

For each number of qubits , the HO probability is cal-
culated and averaged over many random instances of the
the quantum volume circuits. Any mean HO value above
2/3 may be considered as a success given that either the
standard deviation is suﬃciently small or the mean value
saturates and is unaﬀected by the addition of further cir-
cuits; the ideal value in the limit of a large number of
random circuits is ∼ 0.85. The quantum volume is then
deﬁned as 2Nmax where Nmax is the largest number of
qubits used in a circuit that passes the above test.

We perform a QV analysis on a 16 qubit IBM device
with reported QV of 32 (QV32) using both the expert
settings and the Q-CTRL pipeline (as QV is a single-
shot measure, we omit the post-processing measurement-
error-mitigation component of the pipeline). We generate
300 random instances QV circuits (using the QV method
in Qiskit) and execute each with 1000 shots. The exe-
cution of QV circuits is computationally intense and due
to access constraints we elect to restrict our analysis to a
moderate circuit count using a bootstrapping analysis of
the mean cumulative heavy output probability for each
value of circuit width.

As seen in Fig. 9a, the heavy output (HO) probabil-
ity obtained using the Q-CTRL pipeline are consistently
closer to the ideal 0.85 value. The Q-CTRL data is

13

cus on deterministic techniques, leveraging physics-based
knowledge of the dominant error processes gleaned by
system identiﬁcation experiments.

The results presented above indicate that an au-
tonomous pipeline leveraging deterministic error suppres-
sion strategies from the gate through to the compiler level
can deliver large performance advantages across a range
of algorithmic benchmarks and system-level proxy mea-
sures. Experiments on superconducting quantum com-
puters with up to 16 qubits demonstrate up to > 1000×
enhancement of algorithmic success on shallow-depth al-
gorithms relative to default conﬁgurations. As a corollary
we observed signatures of strong non-Markovian errors
in these circuits which were eﬃciently suppressed near
bounds set by T1 processes via deterministic error sup-
pression. Beneﬁts up to > 100× enhancement persisted
when compared against algorithmic execution including
multiple expert-conﬁgured tools for error mitigation and
suppression. In high-depth circuits we observe up to 7×
enhancement, again approaching limits imposed by inco-
herent errors. Our benchmarks also reveal the ability to
improve the structural similarity of the experimental cost
landscape in QAOA relative to an ideal-noise free land-
scape by 28×. Combining this open-loop deterministic
error-reducing pipeline with the execution of quantum er-
ror correction increases the average success of syndrome
identiﬁcation of an engineered error by 2.3×. Finally, we
demonstrate enhancement of Quantum Volume on a 16
qubit device from 32 → 64.

The approach we have demonstrated here, and the
workﬂow encapsulated in the Fire Opal software, pos-
sesses multiple advantages relative to existing strategies
for error reduction in quantum computers. First, the un-
derlying error-suppression techniques used in algorithmic
execution are deterministic, requiring zero user overhead
in terms of additional sampling or randomization; this
saves both wall-clock time and user-cost when executing
on hardware. Deterministic techniques for coherent aver-
aging leveraging physics-based knowledge of the underly-
ing hardware-noise sources are known to deliver superior
performance relative to randomization strategies and re-
quire zero repetition, at the cost of including additional
single-qubit gates (or more complex gate waveforms) in
circuit execution. Nonetheless, for NISQ-era algorithms
deterministic strategies are completely compatible with
additional layers of sampling-based approaches such as
zero-noise extrapolation or randomized compiling; deter-
ministic error-suppression techniques must only consti-
tute the ﬁnal compiler pass in order to preserve the rel-
evant physical frames for coherent averaging.

Next, the Fire Opal toolchain requires zero user con-
ﬁguration and is executed with a single command, obvi-
ating the need to manually include additional codeblocks
in circuit execution. By contrast, existing tools typically
require substantial conﬁguration, making them inappro-
priate for users without detailed knowledge of the under-
lying physical mechanisms for error suppression. Finally,
as our demonstrations validate, this strategy is applica-

FIG. 9. A quantum volume (QV) experiment on a 16 qubit
IBM device with reported QV of 32. In the experiment, we
generate 300 random circuit instances and sample each 1000
times. We then calculate the mean heavy output (HO) prob-
ability over the 300 random circuits. (a) shows the mean HO
probability vs. the number of qubits as obtained using both
expert and Q-CTRL settings. (b) shows the cumulative sum
and standard error as extracted from bootstrapping the 300
data point obtained in the 6Q experiment.

consistent with QV64 and is within ∼ 1% of achieving
QV128, while the the expert settings yield results con-
sistent with QV32 as reported for this hardware system.
Examining the cumulative mean HO for the 6 qubit case
(Fig. 9b), conﬁrms that the value we measure is saturated
to a value above the success threshold. Here shading in-
dicates the range of mean values achieved for diﬀerent
random orderings of the trial averaging process in order
to avoid systematic biases in the analysis.

V. DISCUSSION AND OUTLOOK

At present, the gap between anticipated performance
extrapolated from proxy-measures such as gate-level ran-
domized benchmarking and actual algorithmic perfor-
mance leaves substantial room for ongoing improve-
In our observations this gap arises frequently
ment.
from circuit-level error processes including classical and
quantum crosstalk that require dedicated error reduction
strategies. Our approach to this challenge has been to fo-

ble to both NISQ-era and fault-tolerant algorithmic ex-
ecution, as the open-loop error-suppression strategies in
use are fully compatible with execution of quantum error
correction protocols. By virtue of having no requirement
of additional sampling overhead, it is possible to deter-
ministically suppress errors occurring during the QEC
encoding and syndrome measurement processes in order
to improve the likelihood of correctly identifying errors.
Achieving suﬃcient absolute algorithmic performance
for practical applications will necessitate ongoing ad-
vances in hardware capability. This primarily includes
enhanced incoherent lifetimes (T1) which currently pose
restrictive bounds on both individual gate error rates and
overall algorithmic performance for deep circuits. An
empirical observation in our demonstrations is that the
magnitude of performance improvement due to applica-
tion of the error-suppression pipeline grows exponentially
with qubit count. We are therefore conﬁdent that as base
hardware systems grow in scale and improve in perfor-
mance that these approaches to deterministic error sup-
pression in algorithmic execution will continue to yield
fruit.

ACKNOWLEDGMENTS

The authors are grateful to all other colleagues at Q-
CTRL whose technical, product engineering, and design
work has supported the results presented in this paper.
The authors acknowledge with gratitude IBM research
for providing access to the hardware used in these exper-
iments.

Appendix A: Summary of approaches to error
management in NISQ era devices

In advance of the achievement of full-scale quantum er-
ror correction beyond the performance breakeven point, a
range of techniques have been developed for the manage-
ment of error in NISQ era hardware. These approaches
utilize diﬀerent physical mechanisms but share the com-
monality of being open-loop in that they are designed to
reduce errors without using measurement feedback.

• Circuit depth reduction (compilation) [62–65]: The
dominant approach to algorithmic performance
enhancement is to employ mathematical
identi-
ties to simplify quantum circuits, reducing gate
counts and the duration of an executed circuit.
Challenges: typical compiler outputs are stochas-
tic, requiring multiple passes to deliver a high-
performance circuit; run-to-run ﬂuctuations in cir-
cuit characteristics and performance.

• Circuit layout optimization (compilation) [66–68]:
Due to the inhomogeneity of errors in typical de-
vices, a compilation strategy may be “noise aware”
in selecting a layout or qubit assignment which

14

avoids faulty devices or minimizes use of known
error-prone gate operations.

• Dynamic decoupling for dephasing suppression [34–
37]: The physics of coherent averaging can be used
to reduce dephasing on idling qubits in quantum al-
gorithms via the addition of refocusing single-qubit
pulses to the circuit. Challenges: DD sequences are
generally designed for single-qubit coherence, with-
out consideration of circuit-level contextuality.

• Error-robust gate design: Quantum logic gates may
be designed with echo-like physics exploiting coher-
ent averaging to cancel Hamiltonian noise terms
during gate execution [9, 10, 69–71]. Challenges:
individual gate-level optimization alone does not
address circuit-level and contextual errors.

• Randomized compiling [18, 19, 38–40, 72]: A circuit
can be “decorated” with a randomly selected trans-
formation applied by injected single qubit gates
throughout the circuit. This transformation may
be tracked such that it requires no additional oper-
ations, but requires averaging over many diﬀerent
randomizations of the circuit. A related approach
called Probabilistic error cancellation [20] involves
using random sampling to create a noise map which
can then be deployed to invert the process in algo-
rithmic execution. Challenges: already for small
numbers of qubits, requires user overhead of 10-
20× circuit repetition in order to attain noise sup-
pression, e.g., in Ref [19] 50-100 randomizations per
circuit were used in case of 4Q QFT.

• Zero-noise extrapolation [21, 38, 39, 73]:

In at-
tempting to improve the estimation of a quan-
tum computer’s output under noisy evolution it is
possible to deliberately add noise in a systematic
way and subsequently extrapolate to the expected
performance in the absence of noise. Challenges:
This approach requires repeated execution of the

FIG. 10. Success probability (on a logarithmic scale) of the
BV algorithm on three diﬀerent dates (all diﬀerent than the
plot in the main text). Data presented on a log scale.

same circuit with additional noise channels (e.g.
extended-duration gates).

Appendix B: Stability of performance enhancement
over time

As has been documented elsewhere [43], device perfor-
mance varies both within a day and over longer time peri-

15

ods; accordingly the quality improvements demonstrated
in the data shown in the main text are representative
rather than absolute and static.

In Fig. 10 we show the measured success probability of
the Bernstein-Vazirani algorithm on three diﬀerent dates
(all diﬀerent than for the data presented in the main text
(Fig 2). The Q-CTRL performance is consistent over all
data sets while for a particular circuit width the expert
conﬁguration can vary up to an order of magnitude over
diﬀerent days.

[1] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C.
Bardin, R. Barends, R. Biswas, S. Boixo, F. G. S. L.
Brandao, D. A. Buell, B. Burkett, Y. Chen, Z. Chen,
B. Chiaro, R. Collins, W. Courtney, A. Dunsworth,
E. Farhi, B. Foxen, A. Fowler, C. Gidney, M. Giustina,
R. Graﬀ, K. Guerin, S. Habegger, M. P. Harrigan,
M. J. Hartmann, A. Ho, M. Hoﬀmann, T. Huang,
T. S. Humble, S. V. Isakov, E. Jeﬀrey, Z. Jiang,
D. Kafri, K. Kechedzhi, J. Kelly, P. V. Klimov, S. Knysh,
A. Korotkov, F. Kostritsa, D. Landhuis, M. Lind-
mark, E. Lucero, D. Lyakh, S. Mandr ˜A , J. R. Mc-
Clean, M. McEwen, A. Megrant, X. Mi, K. Michielsen,
M. Mohseni, J. Mutus, O. Naaman, M. Neeley, C. Neill,
M. Y. Niu, E. Ostby, A. Petukhov, J. C. Platt, C. Quin-
tana, E. G. Rieﬀel, P. Roushan, N. C. Rubin, D. Sank,
K. J. Satzinger, V. Smelyanskiy, K. J. Sung, M. D. Tre-
vithick, A. Vainsencher, B. Villalonga, T. White, Z. J.
Yao, P. Yeh, A. Zalcman, H. Neven, and J. M. Martinis,
Nature 574, 505 (2019).

[2] H.-S. Zhong, H. Wang, Y.-H. Deng, M.-C. Chen, L.-C.
Peng, Y.-H. Luo, J. Qin, D. Wu, X. Ding, Y. Hu, P. Hu,
X.-Y. Yang, W.-J. Zhang, H. Li, Y. Li, X. Jiang, L. Gan,
G. Yang, L. You, Z. Wang, L. Li, N.-L. Liu, C.-Y. Lu,
and J.-W. Pan, Science 370, 1460 (2020).

[3] F. Motzoi, J. M. Gambetta, P. Rebentrost, and F. K.

Wilhelm, Phys. Rev. Lett. 103, 110501 (2009).

[4] Y. Baum, M. Amico, S. Howell, M. Hush, M. Liuzzi,
P. Mundada, T. Merkh, A. R. Carvalho, and M. J. Bier-
cuk, PRX Quantum 2, 040324 (2021).

[5] M. Y. Niu, S. Boixo, V. N. Smelyanskiy, and H. Neven,

npj Quantum Information 5, 33 (2019).

[6] Z. An and D. L. Zhou, EPL (Europhysics Letters) 126,

60002 (2019).

[12] N. Wittler, F. Roy, K. Pack, M. Werninghaus, A. S. Roy,
D. J. Egger, S. Filipp, F. K. Wilhelm, and S. Machnes,
Phys. Rev. Applied 15, 034080 (2021).

[13] T. Proctor, K. Rudinger, K. Young, M. Sarovar, and
R. Blume-Kohout, Phys. Rev. Lett. 119, 130502 (2017).
[14] T. Proctor, K. Rudinger, K. Young, E. Nielsen, and

R. Blume-Kohout, Nature Physics 18, 75 (2022).

[15] A. W. Cross, L. S. Bishop, S. Sheldon, P. D. Nation, and
J. M. Gambetta, Phys. Rev. A 100, 032328 (2019).
[16] T. Lubinski, S. Johri, P. Varosy, J. Coleman, L. Zhao,
J. Necaise, C. H. Baldwin, K. Mayer, and T. Proctor,
Application-oriented performance benchmarks for quan-
tum computing (2021).

[17] Y. Baum, P. S. Mundada, H. Michael, A. Barbosa,
A. Carvalho, and M. Biercuk, Error robust quantum com-
piler (2022), Australian patent ﬁling 2022900597.

[18] J. J. Wallman and J. Emerson, Phys. Rev. A 94, 052325

(2016).

[19] A. Hashim, R. K. Naik, A. Morvan, J.-L. Ville,
B. Mitchell, J. M. Kreikebaum, M. Davis, E. Smith,
C. Iancu, K. P. O’Brien, I. Hincks, J. J. Wallman,
J. Emerson, and I. Siddiqi, Phys. Rev. X 11, 041039
(2021).

[20] E. v. d. Berg, Z. K. Minev, A. Kandala, and K. Temme,
Probabilistic error cancellation with sparse pauli-lindblad
models on noisy quantum processors (2022).

[21] A. Kandala, K. Temme, A. D. C´orcoles, A. Mezzacapo,
J. M. Chow, and J. M. Gambetta, Nature 567, 491
(2019).

[22] E. Knill, D. Leibfried, R. Reichle, J. Britton, R. B.
Blakestad, J. D. Jost, C. Langer, R. Ozeri, S. Seidelin,
and D. J. Wineland, Phys. Rev. A 77, 012307 (2008).
[23] E. Magesan, J. M. Gambetta, and J. Emerson, Phys.

[7] M. Y. Niu, S. Boixo, V. N. Smelyanskiy, and H. Neven,

Rev. Lett. 106, 180504 (2011).

npj Quantum Information 5, 33 (2019).

[8] R. W. Heeres, P. Reinhold, N. Ofek, L. Frunzio, L. Jiang,
M. H. Devoret, and R. J. Schoelkopf, Nature Communi-
cations 8, 94 (2017).

[9] A. R. R. Carvalho, H. Ball, M. J. Biercuk, M. R. Hush,
and F. Thomsen, Error-robust quantum logic optimiza-
tion using a cloud quantum computer interface (2020),
arXiv:2010.08057 [quant-ph].

[10] A. Soare, H. Ball, D. Hayes, J. Sastrawan, M. Jarratt,
J. McLoughlin, X. Zhen, T. Green, and M. Biercuk, Na-
ture Physics 10, 825 (2014).

[11] M. Werninghaus, D. J. Egger, F. Roy, S. Machnes, F. K.
Wilhelm, and S. Filipp, Leakage reduction in fast su-
perconducting qubit gates via optimal control (2020),
arXiv:2003.05952 [quant-ph].

[24] R. Blume-Kohout, J. K. Gamble, E. Nielsen, J. Mizrahi,
J. D. Sterk, and P. Maunz, Nature Physics (2013).
[25] A. Erhard, J. J. Wallman, L. Postler, M. Meth,
R. Stricker, E. A. Martinez, P. Schindler, T. Monz,
J. Emerson, and R. Blatt, Nature Communications 10,
5347 (2019).

[26] A. D. C´orcoles, J. M. Gambetta, J. M. Chow, J. A.
Smolin, M. Ware, J. Strand, B. L. T. Plourde, and
M. Steﬀen, Phys. Rev. A 87, 030301 (2013).

[27] T. Xia, M. Lichtman, K. Maller, A. W. Carr, M. J. Pi-
otrowicz, L. Isenhower, and M. Saﬀman, Phys. Rev. Lett.
114, 100503 (2015).

[28] R. Barends, J. Kelly, A. Veitia, A. Megrant, A. G.
Fowler, B. Campbell, Y. Chen, Z. Chen, B. Chiaro,
A. Dunsworth, I.-C. Hoi, E. Jeﬀrey, C. Neill, P. J. J.

16

O’Malley, J. Mutus, C. Quintana, P. Roushan, D. Sank,
J. Wenner, T. C. White, A. N. Korotkov, A. N. Cleland,
and J. M. Martinis, Phys. Rev. A 90, 030303 (2014).
[29] S. Mavadia, C. L. Edmunds, C. Hempel, H. Ball, F. Roy,
T. M. Stace, and M. J. Biercuk, npj Quantum Informa-
tion 4, 7 (2018).

2000).

[52] E. Dennis, A. Kitaev, A. Landahl, and J. Preskill, Jour-

nal of Mathematical Physics 43, 4452 (2002).

[53] T. M. Stace and S. D. Barrett, Phys. Rev. A 81, 022317

(2010).

[54] F. H. E. Watson and S. D. Barrett, New Journal of

[30] A. W. Cross, L. S. Bishop, J. A. Smolin, and J. M. Gam-

Physics 16, 093045 (2014).

betta, Open quantum assembly language (2017).

[31] IBM, Ibm quantum (accessed: 5-Jan-2021).
[32] Amazon, Amazon braket (accessed: 5-Jan-2021).
[33] Rigetti, Rigetti computing (accessed: 5-Jan-2021).
[34] V. Tripathi, H. Chen, M. Khezri, K.-W. Yip, E. M.
Levenson-Falk, and D. A. Lidar, Suppression of crosstalk
in superconducting qubits using dynamical decoupling
(2022), arXiv:2108.04530 [quant-ph].

[55] D. K. Tuckett, A. S. Darmawan, C. T. Chubb, S. Bravyi,
S. D. Bartlett, and S. T. Flammia, Phys. Rev. X 9,
041031 (2019).

[56] R. Raussendorf and J. Harrington, Phys. Rev. Lett. 98,

190504 (2007).

[57] J.-P. Tillich and G. Z´emor, IEEE Transactions on Infor-

mation Theory 60, 1193 (2013).

[58] A. Bolt, G. Duclos-Cianci, D. Poulin, and T. M. Stace,

[35] L. F. Santos and L. Viola, Phys. Rev. A 72, 062303

Phys. Rev. Lett. 117, 070501 (2016).

(2005).

[59] T. Farrelly, R. J. Harris, N. A. McMahon, and T. M.

[36] P. Sekatski, M. Skotiniotis, and W. D¨ur, New Journal of

Stace, Phys. Rev. Lett. 127, 040507 (2021).

Physics 18, 073034 (2016).

[60] X. Xu, S. C. Benjamin, and X. Yuan, Phys. Rev. Applied

[37] B. Pokharel, N. Anand, B. Fortman, and D. A. Lidar,

15, 034068 (2021).

Phys. Rev. Lett. 121, 220502 (2018).

[38] K. Temme, S. Bravyi, and J. M. Gambetta, Phys. Rev.

Lett. 119, 180509 (2017).

[39] Y. Li and S. C. Benjamin, Phys. Rev. X 7, 021050 (2017).
[40] Z. Cai and S. C. Benjamin, Scientiﬁc Reports 9, 11281

(2019).

[41] S. Bravyi, S. Sheldon, A. Kandala, D. C. Mckay, and
J. M. Gambetta, Phys. Rev. A 103, 042605 (2021).
[42] P. D. Nation, H. Kang, N. Sundaresan, and J. M. Gam-

[61] C. Chamberland, G. Zhu, T. J. Yoder, J. B. Hertzberg,
and A. W. Cross, Phys. Rev. X 10, 011022 (2020).
[62] S. Sivarajah, S. Dilkes, A. Cowtan, W. Simmons, A. Edg-
ington, and R. Duncan, Quantum Science and Technol-
ogy 6, 014003 (2020).

[63] G. Li, Y. Ding, and Y. Xie, Tackling the qubit mapping

problem for nisq-era quantum devices (2018).

[64] T. Jones and S. C. Benjamin, Quantum 6, 628 (2022).
[65] M. Maronese, L. Moro, L. Rocutto, and E. Prati, Quan-

betta, PRX Quantum 2, 040326 (2021).

tum compiling (2021).

[43] A. R. R. Carvalho, H. Ball, M. J. Biercuk, M. R. Hush,
and F. Thomsen, Phys. Rev. Applied 15, 064054 (2021).
[44] M. Bria´nski, J. Gwinner, V. Hlembotskyi, W. Jarnicki,
S. Pli´s, and A. Szady, Phys. Rev. A 103, 062425 (2021).
[45] M. X. Goemans and D. P. Williamson, J. ACM 42,

1115–1145 (1995).

[46] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, IEEE
Transactions on Image Processing 13, 600 (2004).
[47] S. Wang, E. Fontana, M. Cerezo, K. Sharma, A. Sone,
L. Cincio, and P. J. Coles, Nature Communications 12,
6961 (2021).

[48] A. Kandala, A. Mezzacapo, K. Temme, M. Takita,
M. Brink, J. M. Chow, and J. M. Gambetta, Nature 549,
242 (2017).

[49] M. S. A. et al., Qiskit: An open-source framework for

quantum computing (2021).

[50] J. Tilly, H. Chen, S. Cao, D. Picozzi, K. Setia, Y. Li,
E. Grant, L. Wossnig, I. Rungger, G. H. Booth, and
J. Tennyson, The variational quantum eigensolver: a re-
view of methods and best practices (2021).

[51] M. A. Nielsen and I. L. Chuang, Quantum Computation
and Quantum Information (Cambridge University Press,

[66] M. Pedram and A. Shafaei, IEEE Circuits and Systems

Magazine 16, 62 (2016).

[67] A. Paler, L. M. Sasu, A. Florea, and R. Andonie, Machine
learning optimization of quantum circuit layouts (2020).
[68] P. Murali, J. M. Baker, A. J. Abhari, F. T. Chong,
and M. Martonosi, Noise-adaptive compiler mappings for
noisy intermediate-scale quantum computers (2019).
[69] S. Sheldon, E. Magesan, J. M. Chow, and J. M. Gam-

betta, Phys. Rev. A 93, 060302 (2016).

[70] N. Sundaresan, I. Lauer, E. Pritchett, E. Magesan, P. Ju-
rcevic, and J. M. Gambetta, PRX Quantum 1, 020318
(2020).

[71] A. Patterson, J. Rahamim, T. Tsunoda, P. Spring, S. Je-
bari, K. Ratter, M. Mergenthaler, G. Tancredi, B. Vlas-
takis, M. Esposito, and P. Leek, Phys. Rev. Applied 12,
064013 (2019).

[72] S. Endo, S. C. Benjamin, and Y. Li, Phys. Rev. X 8,

031027 (2018).

[73] T. Giurgica-Tiron, Y. Hindy, R. LaRose, A. Mari,
and W. J. Zeng, 2020 IEEE International Conference
on Quantum Computing and Engineering (QCE) , 306
(2020).

