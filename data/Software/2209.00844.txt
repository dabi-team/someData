Noname manuscript No.
(will be inserted by the editor)

Developer Discussion Topics on the Adoption and Barriers
of Low Code Software Development Platforms

Md Abdullah Al Alamin, Gias Uddin, Sanjay
Malakar, Sadia Afroz, Tameem Haider,
Anindya Iqbal

2
2
0
2

p
e
S
2

]
E
S
.
s
c
[

1
v
4
4
8
0
0
.
9
0
2
2
:
v
i
X
r
a

Abstract Low-code software development (LCSD) is an emerging approach to
democratize application development for software practitioners from diverse back-
grounds. LCSD platforms promote rapid application development with a drag-
and-drop interface and minimal programming by hand. As it is a relatively new
paradigm, it is vital to study developers’ diﬃculties when adopting LCSD plat-
forms. Software engineers frequently use the online developer forum Stack Overﬂow
(SO) to seek assistance with technical issues. We observe a growing body of LCSD-
related posts in SO. This paper presents an empirical study of around 33K SO
posts (questions + accepted answers) containing discussions of 38 popular LCSD
platforms. We use Topic Modeling to determine the topics discussed in those posts.
Additionally, we examine how these topics are spread across the various phases of
the agile software development life cycle (SDLC) and which part of LCSD is the
most popular and challenging. Our study oﬀers several interesting ﬁndings. First,
we ﬁnd 40 LCSD topics that we group into ﬁve categories: Application Customiza-
tion, Database and File Management, Platform Adoption, Platform Maintenance,
and Third-party API Integration. Second, while the Application Customization
(30%) and Data Storage (25%) topic categories are the most common, inquiries
relating to several other categories (e.g., the Platform Adoption topic category)
have gained considerable attention in recent years. Third, all topic categories are
evolving rapidly, especially during the Covid-19 pandemic. Fourth, the How-type
questions are prevalent in all topics, but the What-type and Why-type (i.e., detail
information for clariﬁcation) questions are more prevalent in the Platform Adop-
tion and Platform Maintenance category. Fifth, LCSD practitioners ﬁnd topics re-
lated to Platform Query the most popular, while topics related to Message Queue
and Library Dependency Management as the most diﬃcult to get accepted an-
swers to. Sixth, the Why-type and What-type questions and Agile Maintenance
and Deployment phase are the most challenging among practitioners. The ﬁndings
of this study have implications for all three LCSD stakeholders: LCSD platform
vendors, LCSD developers/practitioners, Researchers, and Educators. Researchers

Md Abdullah Al Alamin (corresponding author) and Gias Uddin
DISA Lab, University of Calgary, E-mail: mdabdullahal.alamin, gias.uddin@ucalgary.ca
Sanjay Malakar, Sadia Afroz, Tameem Bin Haider, and Anindya Iqbal
Bangladesh University of Engineering & Technology

 
 
 
 
 
 
2

Alamin et al.

and LCSD platform vendors can collaborate to improve diﬀerent aspects of LCSD,
such as better tutorial-based documentation, testing, and DevOps support.

Keywords Low-Code Software Development, Empirical Study, Stack Overﬂow

1 Introduction

There is a massive shortage of skilled software developers in this age of digital-
ization. According to Gartner, the demand for IT professionals will be multiple
times more than supply [109, 128]. To make matters worse, training and hiring
new software developers are very expensive in this rapidly evolving world. LCSD
aims to address this issue by democratizing software development to domain ex-
perts and accelerating the development and deployment process. It tries to bridge
the gap between the system requirement and the developer constraints, which is
a common reason for long development times in complex business applications.

LCSD is a novel paradigm for developing software applications with mini-
mal hand-coding through visual programming, a graphical user interface, and
model-driven design. LCSD embodies End User Software Programming [81] by de-
mocratizing application development to software practitioners from diverse back-
grounds [39]. It combines various approaches such as visual modeling, rapid app
development, model-driven development, cloud computing, and automatic code
generation. Low-code development tools enable the development of production-
ready apps with less coding by facilitating automatic code generation. Addition-
ally, LCSD platforms also provide more ﬂexibility and agility, faster development
time that allows responding quickly to market needs, less bug ﬁxing, less de-
ployment eﬀort, and easier maintenance [39, 100]. These platforms are used to
develop high-performance database-driven mobile and online applications for var-
ious purposes. As a result, low-code development is rapidly growing in popularity.
According to Forrester, the LCSD platform market is estimated to reach $21 bil-
lion by 2022. By 2024, over 65% of big companies will utilize LCSD systems to
some extent, according to a Gartner report [130].

To date, there are more than 400 LCSD platforms [110], oﬀered by almost all
major companies like Google [47] and Salesforce [101]. Naturally, LCSD has some
unique challenges [100]. Wrong choice of LCSD application/platforms may cause
a waste of time and resources. There is also concern about the security/scalability
of LCSD applications [59]. With interests in LCSD growing, we observe discus-
sions about LCSD platforms are becoming prevalent in online developer forums
like Stack Overﬂow (SO). SO is a large online technical Q&A site with around
120 million posts and 12 million registered users [79]. Several research has been
conducted to analyze SO posts (e.g., IoT [122], big data [16], blockchain [127] con-
currency [4], , microservices [18]). The studies, however, did not analyze discussions
about LCSD platforms in SO.

In 2021, we conducted an empirical study [6] by analyzing 4,785 posts (3,597
questions + 1,118 accepted answers) from SO that contained discussion about nine
LCSD platforms. The study oﬀered, for the ﬁrst time, an overview of the challenges
software developers face while using LCSD platforms. However, to date, there
are over 400 LCSD platforms and we observed discussions about many of those
platforms in SO. Therefore, it was important that we revisit our empirical study

Topics of Developer Discussions of the LCSD Platforms

3

with a larger dataset of discussions about LCSD platforms in SO. In addition,
given that the previous empirical study was a conference paper, the analysis was
not as in-depth as we could have provided due to space limitations. Therefore,
a larger-scale empirical study of the challenges developers face to adopt and use
the LCSD platforms was warranted. Such insights can complement our previous
empirical study [6] as well as the existing LCSD literature – which so far has
mainly used surveys or controlled studies to understand the needs of low-code
practitioners [7, 44, 59, 60].

Speciﬁcally, in this paper, we present an empirical study of 33.7K SO posts
relating to the top 38 LCSD platforms (according to Gartner [45]) at the time of
our analysis to ascertain the interest and challenges of LCSD practitioners. We
answer ﬁve research questions by analyzing the dataset.

RQ1. What topics do LCSD practitioners discuss? Given that LCSD is a novel
paradigm, it is vital to study the types of topics discussed by LCSD practitioners
on a technical Q&A platform such as SO. As a result, we use the topic modelling
method LDA [27] on our 33.7K post dataset. We ﬁnd a total of 40 LCSD topics
grouped into ﬁve categories: Application Customization (30% Questions, 11 Top-
ics), Data Storage (25% Questions, 9 Topics), Platform Adoption (20% Questions,
9 Topics), Platform Maintenance (14% Questions, 6 Topics), and Third-Party In-
tegration (12% Questions, 5 Topics). Around 34% of questions are particular to
the many supported capabilities of LCSD platforms, while the remaining 66% are
regarding development activities, namely application customization. This is be-
cause the LCSD platform’s features are naturally oriented around a graphical user
interface (GUI) in a drag-and-drop environment. As a result, any customization
of such features that are not native to the LCSD platforms becomes diﬃcult.

RQ2. How do the LCSD topics evolve over time? We elaborate on our ﬁnd-
ings from RQ1 by examining how the observed LCSD topics evolved in SO over
time. We conduct an in-depth analysis of LCSD-related discussions from 2008 to
mid-2021 in SO. We discover that since 2012, discussion about LCSD has piqued
community interest, which has increased signiﬁcantly throughout the pandemic,
i.e., since 2020. In recent years, Platform Adoption-related discussions have ac-
quired more traction than original application customization or database query-
related discussions. Future research and LCSD platform vendors should support
emerging topics such as Library Dependency Management, External Web Request
Processing, Platform Infrastructure API, and Data Migration.

RQ3. What types of questions are asked across the observed topic cate-
gories? From RQ1, we ﬁnd some of the unique challenges for LCSD practitioners
regarding Customization, Data Storage on the completely managed cloud plat-
forms. This motivates us to explore further to understand more of those chal-
lenges. For instance, we want to understand if practitioners mostly ask about
diﬀerent solution approaches (i.e., How-type) or further explanation clariﬁcation
type (Why/What-type). Following previous studies[1, 122], we manually anno-
tated a statistically signiﬁcant number of posts (e.g., 471 Questions) into four
categories. We ﬁnd that How-type (57%) is the most common form of inquiry
across all ﬁve topic categories, followed by What-type (18%), Why-type (14%),
and Other-type (12%) questions. Most of the How-type questions are application
implementation-related, and most of the What-type and Why-type Questions are

4

Alamin et al.

server conﬁguration and troubleshooting related. According to our ﬁndings, proper
documentation and tutorials might signiﬁcantly reduce these challenges.

RQ4. How are the observed topic categories discussed across SDLC phases?
Our ﬁndings from the previous research questions examined the practitioners’
challenges on LCSD platforms and their evolution. The acceptance of this emerg-
ing technology depends largely on eﬀective adoption into the various stages of a
software development life cycle (SDLC). So, following our previous study [6] we
manually annotate statistically signiﬁcant samples (e.g., 471 Questions) into six
agile SDLC stages. We ﬁnd that the Implementation (65%) is the most prominent
phase in terms of the number of questions, followed by Application Design (17%)
and Requirement Analysis & Planning (9.1%).

RQ5.What LCSD topics are the most diﬃcult to answer? LCSD practition-
ers face many diﬀerent challenges to understand diﬀerent features of the cloud
platform, server conﬁguration. LCSD vendors aim to provide support from re-
quirement gathering to deployment and maintenance, but practitioners still strug-
gle with customization, data management, and cloud conﬁguration. We ﬁnd that,
while the topic of application customization and the Implementation-SDLC are
the most prevalent, Platform Adoption topic category and the Deployment-SDLC
and Maintenance-SDLC as the most popular and hardest to get accepted answers.
This paper extends our previous paper [6] along two major dimensions: the

data used and the results reported. We oﬀer details about the extensions below.

1. Data (see Section 3). The dataset in this study is signiﬁcantly larger and

more diverse than our previous paper as follows.

– Size. The size of the SO dataset in this paper is almost seven times bigger
than the dataset used in our previous paper. This study analyses 33766
posts (26763 Questions + 11355 Accepted Answers). Our prior paper ex-
amined 4,785 posts (3597 Q + 1188 A).

– Time. This study analyzesLCSD-related discussions in SO between July
2008 to May 2021, while the previous study analyzed the discussions be-
tween July 2008 to May 2020.

– LCSD Platforms. This study analyzes 64 LCSD-related tags which contain
38 LCSD platforms, while the previous study analyzed 19 SO tags related
to 9 LCSD platforms.

2. Empirical Study (see Section 4). This paper considerably enhances our un-

derstanding of LCSD platforms over our previous paper [6] as follows.

– Research Questions (RQ). We have answered ﬁve research questions
(RQ2, RQ3) in this paper compared to three RQs in our previous paper
(RQ1, RQ4, RQ5). The two new RQs oﬀer insights on the type of LCSD
questions asked and the evolution of the LCSD topics. Our revision of the
previous three RQs provided several new results as follows.

– LCSD Topics. In this study, we found 40 topics organized into ﬁve high-
level categories. We found 13 topics organized into four high-level categories
in our previous paper. While we found all the previous 13 topics, we also
found 27 new LCSD topics. This study found Platform Maintenance as a
new high-level topic category (see Section 4.1).

– Finer-Grained Analysis. Due to our use of more data, we ﬁnd better re-
sults from our topic modeling. For example, some topics from our previous
studies are broken down into more informative/coherent topics. For exam-

Topics of Developer Discussions of the LCSD Platforms

5

ple, Client-Server Communication and IO from Platform Adoption topic
category became topics Web Service Communication and Message Queue
under Asynchronous Service to Service Communication sub-sub-category in
this study as those topics contained more coherent discussions. Similarly, we
have expanded our understanding of software development lifecycle phases
(SDLC) around the new 40 topics (see Section 4.4).

– Topic Evolution. Our new RQ2 analyzes the evolution of the observed
LCSD topics in Section 4.2. We further discuss the prevalence and evolu-
tion of the topics across the top 10 LCSD platforms in our dataset (see
Section 5.4).

– Question Type. Our new RQ3 oﬀers insights into the type of questions

asked across the observed LCSD topics (see Section 4.3).

– Popularity vs Diﬃculty. In addition to analyzing the popularity and dif-
ﬁculty of all 40 topics in Section 4.5, we also oﬀer the following new in-
sights. (a) Following a recent study [122], we report the popularity and the
diﬃculty using two fused metrics (see Section 4.5). (b) We report the pop-
ularity and diﬃculty of the LCSD question types and SDLC phases (see
Section 5.6)

3. Related Work. We have expanded our literature review with a comparison of
key metrics around our observed LCSD topics against those previously reported
for other domains while using the SO data (see Section 7.2).

Our study ﬁndings can enhance our understanding of the developers’ struggle
while using LCSD platforms. The ﬁndings would help the research community
and platform vendors better focus on the speciﬁc LCSD areas. The practitioners
can prepare for diﬃcult areas. LCSD platforms can design more eﬀective and
usable tools. All stakeholders can collaborate to provide enhanced documentation
assistance. The LCSD vendors can support increased customization of the LCSD
middleware and UI to make the provided functionalities more usable.

Replication Package: The code and data are shared in https://github.com/al-
alamin/LCSD_challenge_EMSE

2 Background

This section aims to provide a high-level overview of LCSD development, as well
as some of the relevant technologies and research that have shaped this industry.
We hope that this will serve as a resource for future researchers (particularly
those interested in the underlying technologies)) and practitioners to learn and
contribute more to this emerging new ﬁeld.

Low-code Software Application. To cater to the demand of the competitive
market, business organizations often need to quickly develop and deliver customer-
facing applications. LCSD platform allows the quick translation of the business
requirement into a usable software application. It also enables citizen developers
of varying levels of software development experience to develop applications using
visual tools to design the user interface in a drag-and-drop manner and deploy
them easily [68]. LCSD is inspired by the model-driven software principle where
abstract representations of the knowledge and activities drive the development,
rather than focusing on algorithmic computation [100]. LCSD platforms aim to

6

Alamin et al.

Fig. 1: Agile methodologies in traditional vs LCSD development

abstract away the complexity of testing, deployment, and maintenance that we
observe in traditional software development. Some of the most popular low-code
platforms are Appian [12], Google App Maker [47], Microsoft Powerapps [88], and
Salesforce Lightning [101].

Technologies that Shaped LCSD. Model-driven Software Engineering (MDSE)
ﬁeld proposes the adoption of domain-speciﬁc modeling practices [29]. Low-code
platforms adopt model-driven engineering (MDE) principles as their core that
has been applied in several engineering disciplines for the purpose of automation,
analysis, user interface design [28, 30, 85] and abstraction possibilities enabled by
the adoption of modelling and meta modeling [20]. Besides, End-User Development
(EUD) is a set of methods, techniques, and tools that allow users of software
systems, who are mainly non-professional software developers, at some point to
create, modify or extend a software artifact [42, 82]. EUD for GUIs can be a good
example of its usage [36]. Scratch [94], Bloqqi [43], EUD-MARS [5], App Inventor
[129], AppSheet [46] are such “low-code/no-code” application development tools
that oﬀer visual drag-and-drop facilities. Similarly, there are several other research
areas within the domains of HCI [102] and Software engineering, such as Visual
Programming [31], Programming by example [49], End users programming [75],
domain speciﬁc language [73, 124], trigger action programming [123] that aim to
enhance the technologies underlying low-code software development. Thus, gaining
a better knowledge of the problems associated with low-code platforms through
developer discussion would be extremely beneﬁcial for further improving these
studies.

Development Phases of an LCSD Application. A typical LCSD application
can be built in two ways [100]: 1. “UI to Data Design”, where developers create UI
and then connect the UI to necessary data sources, or 2. “Data to UI” where the
design of the data model is followed by the design of the user interfaces. In both ap-
proaches, application logic is implemented, and then third-party services and APIs

Topics of Developer Discussions of the LCSD Platforms

7

are integrated. APIs are interfaces to reusable software libraries [95]. A major mo-
tivation behind LCSD is to build applications, get reviews from the users, and
incorporate those changes quickly [128]. Some software development approaches
are quite popular and supported by diﬀerent LCSD platforms, such as Iterative
software development [21] which is based on the iterative development of the appli-
cation. In this way, every step is cyclically repeated one after another. In practice,
this is very helpful because it allows developing and improving the application
gradually. Another approach can be Rapid application development (RAD) [25]
is a software development methodology that promotes the rapid release of a soft-
ware prototype. It is an agile approach and aims to utilize user feedback from the
prototype to deliver a better product. Another popular methodology is the agile
development methodology [24] which is a collection of approaches and practices
that promote the evolution of software development through collaboration among
cross-functional teams.

Diﬀerent LCSD teams may adopt diﬀerent SDLC approaches. However, we fo-
cus mostly on Agile methodology for this study because Agile and LCSD can go
hand in hand because the fundamental principle and objective are customer sat-
isfaction and continuous incremental delivery. Traditional software development
teams widely use agile, which also provides the generalizability for other method-
ologies. So, in this study, we map agile software development life cycle phases
with LCSD methodologies. The inner circle of Figure 1 shows the important de-
velopment phases of an LCSD application, as outlined in [100]. The outer circle of
Figure 1 shows the phases in a traditional agile software development environment.
As LCSD platforms take care of many application development challenges, some
of the agile application development phases have shorter time/execution spans in
LCSD than traditional software development.

3 Study Data Collection and Topic Modeling

In this Section, we discuss our data collection process to ﬁnd LCSD related posts
(Section 3.1). We then discuss the details about our pre-processing and topic
modeling steps on the selected posts (Section 3.2).

3.1 Data Collection

We collect LCSD related SO posts in three steps: (1) Download SO data dump,
(2) Identify LCSD related tag list, and (3) Extract LCSD related posts from the
data dump based on our selected tag list. We describe the steps below.

Step 1: Download SO data dump. For this study, we used the most popular Q&A
site, Stack Overﬂow (SO), where developers from diverse background discuss about
various software and hardware related issues [79]. For this study, We downloaded
SO data dump [40] of May 2021 which was the latest dataset available during the
starting of this study. We used the contents of “Post.xml” ﬁle, which contained
information about each post like the post’s unique ID, type (Question or Answer),
title, body, associated tags, creation date, view-count, etc. Our data dump included
discussion of 12 years from July 2008 to July 2021 and contained around 53,086,327
posts. Out of them, 21,286,478 (i.e., 40.1%) are questions, 31,799,849 (i.e., 59.9%)

8

Alamin et al.

.
t
s
i
l
g
a
t

l
a
i
t
i

n

i

r
u
o
m
o
r
f

s
e
u

l
a
v

V

d
n
a

u

t
n
e
r
e
ﬀ
d

i

n
o

d
e
s
a
b

s
g
a
t

t
n
a
v
e
l
e
r

s
v

d
e
d
n
e
m
m
o
c
e
r

d
e
t
a
l
e
r
D
S
C
L

f
o

n
o
i
t
u
b
i
r
t
s
i
D

:
2

.
g
i
F

Topics of Developer Discussions of the LCSD Platforms

9

are answers, and 51.5% questions had accepted answers. Around 12 million users
from all over the world participated in the discussions.

Each SO post contains 19 attributes, and some of the relevant are: (1) Post’s
body with code snippets, (2) Post’s Id, creation and modiﬁcation time, (3) Post’s
view count, favorite count, score, (4) User Id of the creator, (5) Accepted answer
Id and a list of 0 to 5 tags.

Step 2: Identify low-code tags. We need to identify the tags that are related
to LCSD in order to extract low-code related posts from SO discussions. To ﬁnd
relevant tags, we followed a similar procedure used in prior work [1, 4, 6, 65, 122,
127]. At Step 1, we identify the initial low-code related tags and call them Tinit.
At Step 2, we ﬁnalize our low-code tag list following related work [16, 133]. Our
ﬁnal tag list Tf inal contains 64 tags from the top 38 LCSD platforms. We discuss
each step in details below.

(1) Identifying Initial low-code tags. The SO posts do not have tags like “low-
code” or “lowcode”. Instead, we ﬁnd that low-code developers use an LCSD plat-
form name as a tag, e.g., “appmaker” for Google Appmaker [47]. Hence, to ﬁnd
relevant tags, ﬁrst, we compile a list of top LCSD platforms by analyzing a list of
platforms that are considered as the market leaders in Gartner [125], Forrester [99],
related research work [100], and other online resources like PC magazine [83].
Our compiled list contained 137 LCSD platforms, including all of our previous
nine platforms from previous study [6]. Then for each of the LCSD platforms,
we manually searched for the SO tags in SO. For example, we search for Oracle
Apex via SO search engine and ﬁnd a list of SO posts. We build a potential list
of tags related with this platform based on manual inspection, such as “oracle”
and “oracle-apex”. Then, manually examine the metadata associated with each of
these tags 1. For example, “oracle-apex” tag’s metadata says “Oracle Application
Express (Oracle APEX) is a rapid Web application development tool that lets
you share data and create applications. Using only a Web browser and limited
programming experience, you can develop and deploy applications that are fast
and secure.” and “oracle” tag’s metadata says “Oracle Database is a Multi-Model
Database Management System created by Oracle Corporation. Do NOT use this
tag for other products owned by Oracle, such as Java and MySQL.”. Therefore,
we select the “oracle-apex” tag for Oracle Apex platform. Not all LCSD platforms
have associated SO tags; thus, they were excluded. For example, OneBlink [77]
low-code platform there is no associated SO tags and thus we exclude this from
our list. In order to better understand the evolution of this domain, we excluded
discontinued LCSD platforms. For example, In Jan 2020, Google announced that
they would no longer release new features for Google App Maker and discontinue
it by 2021 [48] and so we excluded this platform from our list. Finally, we found
38 relevant SO tags from 38 platforms. The ﬁfth and the ﬁrst author participated
in this step, and the complete list of the platforms and tags are available in our
replication package.

So, our initial list contains 38 LCSD platforms such as: Zoho Creator [136],
Salesforce [101], Quickbase [89], Outsystems [89], Mendix [72], Vinyl [126], Ap-
pian [12], and Microsoft Powerapps [88]. We thus focus on the discussions of the
above 38 LCSD platforms in SO. We ﬁnd one tag per LCSD platform as the name

1 https://meta.stackexchange.com/tags

10

Alamin et al.

of the platform (e.g., “powerapps” for Microsoft Powerapps platform). Thus, We
refer to these 38 tags as Tinit.

(2) Finalizing low-code related tags. Intuitively, there might be more variations
to tags of 38 LCSD platforms other than those in Tinit. We use heuristics from
previous related works [6, 16, 133] to ﬁnd other relevant tags. First, we denote
our entire SO dump data as Qall. Second, we extract all the questions Q that
contain any tag from Tinit. Third, we create a candidate tag list Tcandidate using
all the tags found in questions Q. Fourth, we select signiﬁcantly relevant tags from
Tcandidate for our LCSD discussions. Following related works [16, 122, 133], we
compute signiﬁcance and relevance for each tag t in Tcandidate with respect to
Q (our extracted questions that has Tinit tag) and Qall (i.e., our data dump) as
follows,

(Signif icance) Stag =

# of ques. with the tag t in Q
# of ques. with the tag t in Qall

(Relevance) Rtag =

# of questions with tag t in Q
# of questions in Q

A tag t is signiﬁcantly relevant to LCSD if the Stag and Rtag are higher than
a threshold value. We experimented with a wide range of values of Stag = {0.05,
0.10, 0.15, 0.20, 0.25, 0.30, 0.35} and Rtag = {0.001, 0.005, 0.010, 0.015, 0.020,
0.025, 0.03}. Figure 2 shows the total number of recommended vs relevant tags
from our 49 experiments. It shows that as we increase Stag and Rtag the total
number of recommend tags decreases. For example, we ﬁnd that for Stag=.05
and Rtag = 0.001 the total number of recommended tags is 61 which is highest.
However, not all of the suggested tags are LCSD-related. For instance, according
to our signiﬁcance and relevance analysis, tags such as “oracle-xe”, “ems”, “aura-
framework”, “power-automate” etc are frequently correlated with other LCSD
platform tags, although they do not contain low-code-related discussions. After
manually analysing these 61 tags we ﬁnd that 26 tags are relevant to LCSD-related
discussions. So, for the lowest Stag = 0.3 and Rtag = 0.001 we ﬁnd 26 additional
LCSD-related tags. These values are consistent with related work [4, 6, 16, 122].
The ﬁnal tag list Tf inal contains 64 signiﬁcantly relevant tags. So, after combining
with out initial taglist, i.e., Tinit, our ﬁnal tag list Tf inal contains 64 signiﬁcantly
relevant LCSD-related tags which are:

simple-salesforce,

salesforce-service-cloud,

{ apex-code, lotus-notes, domino-designer-eclipse, visualforce, salesforce-chatter,
salesforce-ios-sdk, apex-
apex,
trigger, oracle-apex-5, salesforce-lightning, salesforce-communities, oracle-
apex-5.1, servicenow-rest-api, powerapps-formula, salesforce-marketing-cloud,
powerapps-selected-items,
powerapps-collection,
powerapps-canvas, oracle-apex-18.2, lwc, salesforce-development, oracle-apex-
19.1, oracle-apex-19.2, outsystems, appian, quickbase, powerapps, oracle-apex,
salesforce, zoho, mendix, servicenow, pega, retool, vinyl, kissﬂow, bizagi,
neutrinos-platform, rad, joget, ﬁlemaker, boomi, opentext, tibco, webmethods,
conductor, temenos-quantum, shoutem, oracle-cloud-infrastructure, amazon-
honeycode, convertigo, lotus-domino, genero, genesis, gramex, processmaker,
orocrm, slingr, unqork, uniface, structr}

powerapps-modeldriven,

Topics of Developer Discussions of the LCSD Platforms

11

Step 3: Extracting low-code related posts. An SO question can have at most
ﬁve tags, and we consider a question as low-code related question if at least one of
its tag is in our chosen tag list Tf inal. Based on our Tf inal tag set, we found a total
of 27,880 questions from our data dump. SO has a score-based system (up-vote and
down-vote) to ensure the questions are in proper language with necessary infor-
mation (code samples and error messages), not repeated, oﬀ-topic or incorrectly
tagged. Here is an example for a question with score “-4” where a practitioner is
making an API related query in Powerapps(Q61147923)2 platform. However, it is
not clear what the practitioner is asking as the question is poorly written and with-
out any clear example. So, in order to ensure good quality discussions, we excluded
questions that had a negative score. Following previous research [16, 19, 98, 122],
we also excluded unaccepted answers and only considered accepted answers for
our dataset. Hence, our ﬁnal dataset B contained 37,766 posts containing 67.4%
Questions (i.e., 26,763) and 32.6% Accepted Answers (i.e., 11,010).

To ensure that our ﬁnal taglist Tf inal comprises discussions relating to low-
code software development, we randomly select 96 questions from our dataset that
are statistically signiﬁcant with a 95% conﬁdence level and 10 conﬁdence inter-
val. First and third authors contributed to this manual analysis, and after manual
analysis, we found that 98% of questions from our selected taglist contain low-code
platform-related discussion, with only two questions containing discussion that is
not particularly related to low-code platforms. For instance, question Q59402662
includes the tag “appian”, yet the question body describes only about a MySQL
database performance-related issue on the Azure platform. Similarly, the ques-
tion Q19289762 contains the “apex-code” tag, but exclusively discusses AWS cloud
authentication signature-related issues in its problem description.

3.2 Topic Modeling

We produce LCSD topics from our extracted posts in three steps: (1) Preprocess
the posts, (2) Find optimal number of topics, and (3) Generate topics. We discuss
the steps below.

Step 1. Preprocess the posts. For each post text, we remove noise following
related works [1, 16, 19]. First, we remove the code snippets from the body,
which is inside <code></code> tag, HTML tags such as (<p></p>, <a></a>,
<li></li> etc), and URLs. Then we remove the stop words such as “the”, “is”,
“are”, punctuation marks, numbers, non-alphabetical characters using the stop
word list from MALLET [70], NLTK [66], and our custom low-code speciﬁc (i.e.,
LCSD platform names) stop word list. We remove the platform’s name from the
dataset since, based on our experiments with LDA topic modeling for this study
and our past work [6], the resultant topics sometimes tend to cluster around LCSD
platforms rather than the technical challenges discussed. Thus, we remove the
LCSD platform names from our dataset. After this, we use porter stemmer [90]
to get the stemmed representations of the words e.g., “wait”, “waits”, “waiting”,
and “waited” - all of which are stemmed to base form “wait”.

Step 2. Finding the optimal number of topics. After the prepossessing, we
use Latent Dirichlet Allocation [27] and the MALLET tool [70] to ﬁnd out the

2 Qi and Ai denote a question Q or answer A in SO with an ID i

12

Alamin et al.

LCSD-related topics in SO discussions. We follow similar studies in Software en-
gineering research using topic modeling [1, 13, 15, 16, 133]. Our goal is to ﬁnd
the optimal number of topics K for our dataset B so that the coherence score,
i.e., encapsulation of underlying topics, is high. We use Gensim package [92] to
determine the coherence score following previous works [96, 113]. We experiment
with diﬀerent values of K that range from {5, 10, 15, 20, 25, 30, 35, 40, 45, 50,
55, 60, 65, 70} and for each value, we run MALLET LDA on our dataset for 1000
iterations [16]. Then we observe how the coherence score is changing with respect
to K. We pick the topic model with the highest coherence score. Choosing the
right value of K is important because, for smaller values of K, multiple real-world
concepts merge, and for a large value of K, a topic breaks down. For example, in
our result, the highest coherence score 0.50 for K = 45 and K = 40. The ﬁrst,
third, fourth, and ﬁfth authors participate in the analysis and we choose K =
45 as it captures our underlying topics better. MALLET also uses two hyper-
parameters, α and β, to distribute words and posts across the topics. Following
the previous works [4, 6, 16, 17, 98, 122], we use the standard values 50/K and
0.01 for hyper-parameters α and β in our experiment.

Step 3. Generating topics. Topic modeling is a method of extracting a set of
topics by analysing a collection of documents without any predeﬁned taxonomy.
Each document has a probability distribution of topics, and every topic has a
probability distribution of a set of related words [19]. We produced 45 topics using
the above LDA conﬁguration on our dataset B. Each topic model oﬀers a list of
top N words and a list of M posts associated with the topic. In our settings, a
topic consists of 30 most frequently co-related words, which represent a concept.
Each post had a correlation score between 0 to 1, and following the previous
work [6, 122, 127], we assign a document with a topic that it correlates most.

4 Empirical Study

We report the results of an empirical study by answering the following ﬁve research
questions (RQ) based on our analysis of LCSD topics in our dataset.

RQ1. What topics do LCSD practitioners discuss? (Section 4.1)
RQ2. How do the LCSD topics evolve over time in SO? (Section 4.2)
RQ3. What types of questions are asked across the observed topic categories?

(Section 4.3)

RQ4. How are the observed topic categories discussed across SDLC phases? (Sec-

tion 4.4)

RQ5. What LCSD topics are the most diﬃcult to answer? (Section 4.5)

The ﬁrst two research questions (RQ1, RQ2) provide insights about what topics
practitioners discuss in SO and how these topics have evolved over time. The
third and fourth research questions (RQ3, RQ4) explore the types of questions in
these topics and they aﬀected diﬀerent SDLC phases. At the end, we discuss the
popularity and diﬃculty of the LCSD topics in the last research question (RQ5).

Topics of Developer Discussions of the LCSD Platforms

13

4.1 What topics are discussed about LCSD in Stack Overﬂow? (RQ1)

4.1.1 Motivation

The increased popularity of LCSD as a ﬂexible and straightforward approach helps
develop practical business applications. The challenges of LCSD are yet to be
studied as this is a new approach to software development. SO is an established
source of knowledge repository to systematically study the real-world challenges
that the practitioners face. An understanding of the LCSD topics in SO developer
discussions will help LCSD platform providers and researchers to have a better
understanding of the underlying prevalent issues, which can then help guide eﬀorts
to improve the quality of LCSD platforms.

4.1.2 Approach

We applied LDA topic modeling to our LCSD-related discussion in SO. We get 45
low-code related topics from our LDA topic modeling, as discussed in Section 3.
We use card sorting [41] to label these topics following previous works [1, 4, 16, 98,
133]. In open card sorting, there is no predeﬁned list of labels. Following related
works [1, 6, 16, 122], we label each topic by analyzing the top 30 words for the
topic and a random sample of at least 20 questions that are assigned to the topic.
Four of the authors participated in the labeling process in group sessions (ﬁrst,
third to ﬁfth). Each author assigns a label to each topic and discusses it until there
is an agreement. The authors reached an agreement after around 10 iterations of
meetings over Skype and email and labeled the 45 topics from the LDA output.

After this initial labeling, we merged a few topics because they contained sim-
ilar discussions with diﬀerent vocabularies. For example, we merged topic 36 and
43 into Dynamic form controller because both topics contained discussions related
to forms with a predeﬁned list of values, dynamically changing the ﬁelds (or op-
tions) of forms values based on users’ actions or previous selections. Similarly, we
merged topic 2 and 19 to DB Setup & Migration. In the end, we obtained 40
distinct LCSD-related topics.

After the labeling of the topics, we revisited the labels in an attempt to ﬁnd
any clusters/groups among the topics. For example, Date & Time Manipulation,
Formatted Data Parsing, and Pattern Matching topics are related, and thus, they
are grouped under the General Programming category. We repeated this pro-
cess multiple times to ﬁnd increasingly higher categories. For example, we found
another category called Dynamic Content which contained two topics Dynamic
Data Binding and Dynamic Data Filtering. We then put these two categories un-
der called Business Logic Implementation. This higher abstraction helped us to
place other topics related to implementing business logic under this category. Fol-
lowing a similar strategy, we put this Business logic implementation under the
Customization category, which discussed customizing applications. For example,
under Customization, there were a category called UI which contained Dynamic
Layout, and Script category, which contained topics such as Dynamic Page Lay-
out, Dialog Box Manipulation, Window Style Manipulation, and Dynamic Form
Controller. The entire process of creating this hierarchy of topic categories took
multiple iterations and revisions. We created a coding guideline for creating the

14

Alamin et al.

Data Storage
(25% Questions in 9 Topics)

Customization
(30% Questions in 11 Topics)

Platform Adoption
(20% Questions in 9 Topics)

Third-Party Integration
(12% Questions in 5 Topics)

Platform Maintenance
(14% Questions in 6 Topics)

Fig. 3: Distribution of Questions and Topics per Topic Category

taxonomy of topics to ensure consistency and reproducibility. We share the coding
guide with our replication package.

4.1.3 Results

After labeling and merging, we ﬁnd 40 LCSD-related topics. Then after grouping
these topics into higher categories, we ﬁnd ﬁve high-level categories: (1) Cus-
tomization, (2) Data Storage, (3) Platform Adoption, (4) Platform Maintenance,
and (5) Third-Party Integration . Figure 3 shows the distribution of topics and
questions into these ﬁve categories. Among these categories, Customization has the
highest coverage of questions and topics (30% Questions in 11 Topics), followed
by Data Storage (25% Questions in 9 Topics), Platform Adoption (20% Questions
in 9 Topics), Platform Maintenance (14% Questions in 6 Topics), Third-Party
Integration (12% Questions in 5 Topics).

Figure 4 shows the 40 LCSD topics sorted by numbers of posts. A post means
an LCSD-related question or an accepted answer in our case. As discussed in
Section 3.1, our dataset has total 37,773 posts containing 26,763 questions and
11,010 accepted answers. The topic with the highest number of posts is placed
on top of the list. On average, each topic has 944 posts (question + accepted
answer). The topic Window Style Manipulation has the highest number of posts
(6.3%) with Questions 5.9% of total questions and 7.2% total accepted answers.
On average, each topic has around 669 questions.

Figure 5 provides a taxonomy of 40 LCSD related topics into ﬁve categories.
The topics are organized in descending order of the number of questions. For ex-
ample, the Customization category has the highest number of questions, followed
by Data Storage. Each category may have some sub-categories of topics. For ex-
ample, the Customization category has two sub-categories: UI and Business Logic.
The topics under each sub-category can further be grouped into multiple sub-sub-
categories. For example, the UI sub-category has 4 topics grouped into Script and
Dynamic Layout sub-sub-categories. Each sub-category, sub-sub-categories, and
topics are also presented in descending order of the number of questions.
We discuss the ﬁve categories and the 40 topics in detail below.

Customization Topic Category. Customization is the largest topic category in
terms of the number of topics and percentage of questions. Out of the 40 topics, 11
topics belong to the Customization category, with around 30% of questions in our
dataset. These topics contain discussions about implementing business logic, cus-
tomizing UI, input and form data validation, general programming-related query

Topics of Developer Discussions of the LCSD Platforms

15

Fig. 4: Distribution of questions by low-code topics by total number of posts
(Number of Questions + Number of Accepted Answers)

to implement some features, etc. This category has two sub-categories: (1) UI
contains discussion about customizing the UI, dynamically changing window com-
ponents, interactive dialog boxes, and (2) Business Logic contains discussion about
diﬀerent programming customization-related queries, dynamically binding UI ele-
ments to backend data.

• UI Sub-Category contains 15% questions and four topics divided in two sub-
sub-categories: (1) Script contains discussion about manipulation of text widgets,
formatting components, and (2) Dynamic Layout is about hiding and moving
components, showing popups.

The Script sub-sub-category contains 10.4% questions and has two topics:
(1) Topic Window Style Manipulation (5.9%) concerns about manipulating the
style of the HTML documents such as adding/removing margins/padding (e.g.,
Q36503030), adding links, manipulating navigation bar and embedded views (e.g.,
Q30453620). (2) Topic Dynamic Form Controller (4.5%) are about dynamic form, i.e.,

1.3%1.3%1.4%1.4%1.5%1.6%1.6%1.6%1.8%1.9%1.9%1.9%1.9%1.9%2%2%2%2.2%2.2%2.3%2.4%2.5%2.5%2.5%2.6%2.7%2.7%2.7%2.8%2.9%3%3.1%3.2%3.2%3.4%3.5%3.9%4%4.3%6.3%1.5%1.6%1.5%1.6%1.7%1.5%1.7%1.9%1.6%1.5%1.7%2%1.8%2.1%2.1%1.9%2.4%2.1%2.7%2.6%1.7%2.8%2.8%1.8%2.9%2.8%2.6%2.5%3.2%3.1%3.7%2.2%3.6%1.5%2.6%3.9%4.4%4.5%4.1%5.9%0.7%0.6%1.1%1.1%0.9%1.9%1.5%1%2.4%2.8%2.5%1.8%2.2%1.6%1.9%2.4%1.1%2.4%1%1.6%4%1.7%1.9%4.3%1.7%2.3%2.9%3.4%1.6%2.5%1.1%5.3%2%7.3%5.2%2.7%2.7%2.8%4.9%7.2%05001000150020002500eSignatureGraph/ChartMessage QueueSemi-Structured Data Proc.Date-based filteringDate & Time ManipulationWeb-Service CommunicationApp DeploymentConditional BPMNAsynchronous Batch JobsUpgradation & CompatibilityDialog Box ManipulationHosting Config. & SEOLibrary Dependency MngmtTestingDynamic Data FilteringEmail ProcessingUser Role ManagementSQL Sysntax ErrorEntity Relationship MgmtPlatform Related QueryData Security & ReplicationInteractive ReportPattern MatchingDynamic Page LayoutFetch & Process API ResponseFile ManagementSQL CRUDPlatform Infrastructure APIDynamic Event HandlingExternal Web Req ProcessingFormatted Data ParsingAuthentication & AuthorizationMisc. SWE DiscussionDynamic Data BindingDB Stored ProcedureDB Setup & MigrationDynamic Form ControllerBuild Config. ManagementWindow Style Manipulation%Questions%Accepted Answers%Total Posts16

Alamin et al.

Fig. 5: A taxonomy of LCSD topics with sub-categories.

forms with predeﬁned list of values (e.g., Q64373454), multi select content (e.g.,
Q39318510), changing forms option based on previous selection (e.g., Q43725028).
The Dynamic Layout sub-sub-category covers 4.9% questions and has two top-
ics: (1) Topic Dynamic Page Layout (2.9%) contains discussion about UI (i.e. page)
customization (e.g., Q65964413), pagination in Q4536018, hiding or moving element
based on some user action or an event (e.g., Q13231072)., (2) Topic Dialog Box Ma-
nipulation (2.0%) is about manipulating dialog box (e.g., pop up/ modals) such as
hiding them in Q49804455, close them in Q55513527, show popup, refresh web-page
(e.g., Q60606986, Q21701437).

Customization 30.0%DBMS20.6%Business Logic 14.7%UI 15.3%Schema6.5%SQL6.9%DB Stored Procedure 3.9%Date-based filtering 1.7%File Management 2.6%Entity Relationship Mgmt 2.6%Script 10.4%Dynamic Layout 4.9%Programming 5.5%Window Style Manipulation 5.9%Dynamic Form Controller 4.5%Dynamic Page Layout 2.9%Dialog Box Manipulation 2.0%Formatted Data Parsing 2.2%Pattern Matching 1.8%Date & Time Manipulation 1.5%SQL Syntax Error 2.7%Conditional BPMN 1.6%Configuration7.2%Data Security & Replication 2.8%DB Setup & Migration 4.4%SQL CRUD 2.5%Platform Adoption19.7%Documentation9.7%File4.2%Data Visualization 4.4%User Role Management 2.1%Platform Related Query 1.7%Misc. SWE Discussion 1.5%Interactive Report 2.8%Graph/Chart 1.6%Features 3.8%Platform Infrastructure API 3.2%Web-Service Communication 1.7%Authentication & Authorization 3.6%Async S2S Comm. 3.2%Message Queue 1.5%Build Config. Management 4.1%Semi-Structured Data Proc. 1.6%Platform Maintenance13.5%REST API3.6%Architecture6.4%Dependency Resolution 6.2%Testing 2.1%Hosting Config. & SEO 1.8%Library Dependency Mngmt 2.1%App Deployment 1.9%Server Config1.8%Monitoring3.4%Fetch & Process API Response 2.8%External Web Req Processing 3.7%Asynchronous Batch Jobs 1.5%Email Processing 2.4%Third-Party Integration12.1%Upgradation & Compatibility 1.7%eSignature 1.5%LowCode Topics with Categories and SubcategoriesData Storage24.8%Configuration8.0%CI/CD 5.5%REST API6.5%Plugins5.6%Dynamic Content 7.6%Dynamic Data Binding 2.6%Dynamic Data Filtering 1.9%Dynamic Event Handling 3.1%Topics of Developer Discussions of the LCSD Platforms

17

• Business Logic Sub-Category contains 14.7% questions and 7 topics in
two sub-sub-categories: (1) Programming is about discussion related to diﬀer-
ent programming-related questions and data access, and (2) Dynamic Content is
about discussions related to dynamically querying data from diﬀerent data sources,
dynamically changing the web-page content. The Business Logic sub-category con-
tains one topic Conditional BPMN that does not belong to any sub-sub-category.
Topic Conditional BPMN (1.6%) contains LCSD platform’s application customiza-
tion related discussions on Business Process Model and Notation (BPMN) (e.g.
Q38265464) and conditional logic features (e.g., Q66335289, Q65838553).

The Dynamic Content sub-sub-category contains 7.6% of questions and has
three topics: (1) Topic Dynamic Event Handling (3.1%) discusses about diﬀerent
JavaScript related issues such as JavaScript feature not working (e.g., “JavaScript
promise is not working” in Q65550370), browser compatibility, JS event initializa-
tion issue (e.g., Q64507615) etc. (2) Topic Dynamic Data Binding (2.6%) is about
discussions related to the design of forms with predeﬁned values (e.g., Q45051098),
the implementation of multi-select, customized drop-down values, form validation
(e.g., Q51115573), changing content of one ﬁeld based on the value of other ﬁeld
in Q47652192. (3) Topic Dynamic Data Filtering (1.9%) contains business logic cus-
tomization related discussion based on advanced ﬁltering criteria and querying
multiple tables. (e.g., “Find Records Based on the Contents of Records in a Re-
lated Table?” in Q20665253 and “ﬁnd the latest record grouped by name in layout”
in Q38128584).

The Programming sub-sub-category contains 5.5% of questions and has three
topics: (1) Topic Formatted Data Parsing (2.2%) is about programming related dis-
cussion on parsing formatted data, i.e., JSON (e.g., Q50184058, Q44803257), XML
(e.g., Q13785513), array of objects (e.g., Q66744874) etc. (2) Topic Pattern Match-
ing (1.8%) topic concerns programming related discussions about searching and
modifying strings by pattern matching using regular expression (e.g., “How do I
search for an Exact phrase” in Q51258323, “Regex pattern to replace html in a
given text string” in Q48251198). (3) Topic Date & Time Manipulation (1.5%) con-
tains programming discussions about date-time manipulation like conversion of
formatted string from data-time in Q51714301, calculation of diﬀerence between
date-time (e.g., Q59230493) and timezone, time conversion (e.g., “How to convert
a Date value to unix timestamp?” in Q60601201).

Data Storage Topic Category. Data Storage Category is the second largest topic
category with a total of 9 topics and around 25% of the questions of our dataset.
This topic category contains discussions on database management and ﬁle storage.
It contains two sub-categories: (1) DBMS is about discussion related to database
setup, migration, DB query, (2) File concerns storing and retrieving ﬁles (i.e.,
images, CSV ﬁles, etc.).

• DBMS Sub-Category contains around around 20.6% questions with seven top-
ics under three sub-sub-categories: (1) Conﬁguration contains discussions about
database setup, database connection, DB data security, (2) SQL contains discus-
sion about SQL query, (3) Schema is about database schema design (i.e., Primary
key, foreign key design), diﬀerent stored procedure.

Conﬁguration sub-sub-category contains 7.2% questions and two topics:
(1) Topic DB Setup & Migration (4.4%) topic is about connecting applications
to diﬀerent vendor databases (i.e., MySQL, Postgres, Oracle etc.) (e.g., “is ODBC

18

Alamin et al.

Firebird connection possible?” in Q28251836), DB users (e.g., Q58815776), issues
about diﬀerent database versions, data migration to LCSD platform (e.g., “How
to add External data source into MySQL?” in Q28251836 or Q22626970). (2) Topic
Data Security & Replication (2.8%) topic concerns about discussion related to data
security (e.g., encryption and decryption in Q1567252), accessing stored of database
ﬁle (e.g., Q5730482), data backup or replication (e.g., Q10997987) etc. SQL sub-sub-
category contains 6.9% questions and three topics: (1) Topic SQL Syntax Error
(2.7%) discusses about errors in syntax in diﬀerent SQL query and stored pro-
cedure. For example, there are questions such as “I’m getting error while creat-
ing a procedure in pl/sql” in Q63990287, “SQL parsing fails, not sure what the
issue is?” in Q8298486. (2) Topic SQL CRUD (2.5%) is about database Create,
read, update and delete (i.e., CRUD) related queries (e.g., Q22712624), and ad-
vanced queries too, such as inner join, nested join, aggregate (e.g., “SQL Query:
JOIN Three tables then Not showing the results after joining the 3rd table” in
Q22712624). This topic also contains discussion about Object query language, which
is a high-level wrapper over SQL in Q64812548. (3) Topic Date-based ﬁltering (1.7%)
contains database query related discussion specially for date-time based ﬁlter-
ing (e.g., Q52389335), i.e., monthly/quarterly query, time-based data grouping etc.
For example, there are questions such as How to count total amount of value
by day (e.g., Q65142062). Schema sub-sub-category contains 6.5% questions and
two topics: (1) Topic DB Stored Procedure (3.9%) is about database schema and
advanced database related discussion on stored procedure (e.g., support of trig-
gers in LCSD platform, Q11799577, Q37810803). (2) Topic Entity Relationship Mgmt
(2.6%) concerns about discussion on advanced database schema design (e.g., “How
to automatically insert foreign key into table after submit in [LCSD platform]”,
Q66968187) and database discussion to automatically update database (e.g., “Auto
increment item in Oracle APEX” in Q61961348).

• File Sub-Category contains 4.2% questions with two topics: (1) Topic File
Management (2.6%) contains discussion of ﬁle management, i.e., storing and pro-
cessing ﬁles, renaming it in Q56414466, converting ﬁles from one format to another
in Q45796962, handling image ﬁles (e.g, Q34203211). (2) Topic Semi-Structured Data
Proc. (1.6%) is about diﬀerent programming related discussion on processing, mod-
ifying and storing semi-structured data ﬁles, i.e., XML, CSV ﬁles. For example,
there are questions such as Fetch CSV ﬁle columns dynamically Using [platform]
package in Q66310875.

Platform Adoption Topic Category. A total of nine topics belong to the Plat-
form Adoption category with around 20% questions. The nine topics belong to
three sub-categories: (1) Documentation contains LCSD platform’s feature-related
discussions and how to use those features, (2) Architecture concerns about what
type of software development architecture (e.g., client-server communication) is
supported by the LCSD platforms, (3) REST API contains LCSD platform’s
RESTful APIs.

• Documentation Sub-Category contains 9.7% questions and ﬁve topics. Four
of the topics fall under two sub-sub categories: (1) Data Visualization contains
discussion related to interactive reports and graphs, (2) Features is about LCSD
platform provides features such as user’s role management, support on SDLC man-
agement. Topic Misc. SWE Discussion (1.5%) concerns about discussions related
to general software engineering such as Unix Threading (e.g., Q30530873), Object-

Topics of Developer Discussions of the LCSD Platforms

19

oriented programming (e.g., Q314241), auto scaling, ambiguous documentation in
Q10348746.

Data Visualization sub-sub-category contains 4.4% questions and has two
topics: (1) Topic Interactive Report (2.8%) is about data visualization and inter-
active data reports. It contains developers’ discussions about how they can use
diﬀerent platform features for customized reports. For example, “using jquery
hide column heading when no data in column in interactive report in [platform]”
in Q53294659. (2) Topic Graph/Chart (1.6%) discusses about platform’s support
and documentation request to draw diﬀerent graphs (e.g., Q41257691) and charts
using stored data. For example, “How to overlay a line plot over a bar graph in
[platform]?” in Q28727869 Features sub-sub-category contains 3.8% questions and
has two topics: (1) Topic User Role Management (2.1%) contains discussion about
diﬀerent user role management features (i.e., administrators and regulators) pro-
vided by the LCSD platforms. It discusses about user’s proﬁle management (e.g.,
Q66853056), user group and access-level (e.g., Q35457840). (2) Topic Platform Related
Query (1.7%) contains general discussion about LCSD platforms such as compari-
son of features between diﬀerent platforms (e.g., “How is [platform A] better than
[platform B] in BPM?” in Q39127918), Agile and RAD development support (e.g.,
Q2512396), performance of a speciﬁc feature of a platform in Q6068882.

• Architecture Sub-Category contains 6.4% questions and two topics and one
sub-sub category: (1) Async S2S Comm contains discussion related to distributed
applications with service to service communication. Topic Platform Infrastructure
API (3.2%) contains cloud-based REST API from the LCSD platforms to conﬁgure
and utilize diﬀerent platform features, connect to other services or data sources
via connectors or cloud REST APIs. For example, the questions are about how
[platform] apps portal integration with [DB] On-premise in Q63688934, “Change
Shape OCI instance with Ansible” in Q60511836.

Async S2S Comm sub-sub-category contains 3.2% questions and has two
topics: (1) Topic Web-Service Communication (1.7%) contains discussions about
micro-service architecture, service to service communication via web service de-
scription language (e.g., Q16278661, Q2567466), HTTP REST message in Q58689313,
Windows Communication Foundation (e.g., Q36849686). (2) Topic Message Queue
(1.5%) is about discussion about diﬀerent asynchronous service-to-service data
exchange mechanisms such as using a message queue. It generally contains discus-
sions about micro-service design patterns and producer and consumer mechanisms
(e.g., Q41640881) for data exchange. For example, “How to know who is connected
to a [Platform] EMS Queue” in Q66999418, Q56334001.

REST API sub-category contains 3.6% questions and has one topics: (1) Topic
Authentication & Authorization (3.6%) contains discussion about LCSD platforms
support on diﬀerent standard authentication and authorization protocol such as
OAuth2 (e.g., Q30475542), SAML (e.g., Q23624206), access token (e.g., “access token
in android [Platform] sdk” in Q32943204).

Platform Maintenance Topic Category. We ﬁnd 6 topics and 13.5% questions
in Platform Maintenance category. It has two sub-categories: (1) Conﬁguration
contains discussion on LCSD platforms library and build conﬁguration, (2) CI/CD.
is about discussion related to DevOps tasks such as continuous integration and
continuous delivery, testing etc.

20

Alamin et al.

• Conﬁguration Sub-Category contains 8.0% questions and three topics un-
der two sub-sub categories: (1) Dependency Resolution is about LCSD platforms
server’s library dependency management, (2) Server Conﬁguration is about LCSD
platform’s servers conﬁguration and hosting settings such as SSL conﬁguration.

Dependency Resolution sub-sub-category contains 6.2% questions and has
two topics: (1) Topic Build Conﬁg. Management (4.1%) contains discussion
about system build conﬁguration and external library management-related is-
sues in Q48727452. This topic also contains discussion about compilation failure
(e.g., Q29243987), library dependency, build path not conﬁgured properly (e.g.,
Q57015131) etc. (2) Topic Library Dependency Mngmt (2.1%) is about the library
and dependencies of the system (e.g. Q23471590, Q62872836), server conﬁguration,
diﬀerent library version compatibility issues in Q60050869. Server Conﬁg. sub-sub-
category contains 1.8% questions and has one topic: (1) Topic Hosting Conﬁg. &
SEO (1.8%) is about discussions about LCSD platforms support on server conﬁg-
uration, i.e., conﬁguring SSL certiﬁcate (e.g., “[platform] client ignoring expired
certiﬁcate” in Q55044903), LCSD platform’s support on making the application
accessible and index-able (e.g., Q34860991).
• CI/CD Sub-Category contains 5.5% questions and three topics. Two of the top-
ics fall under one sub-sub category:(1) Monitoring is about the discussion on mon-
itoring the deployed applications and scheduled job status. Topic Testing (2.1%)
contains discussions about LCSD platforms support on testing and test coverage.
For example, “How to know overall code coverage of multiple test classes?” in
Q67724447, How to write a test class as in Q50586482.

Monitoring sub-sub-category contains 3.4% questions and has two topics:
(1) Topic App Deployment (1.9%) discusses about the LCSD platform’s CI/CD
features such as incrementally updating the application code in Q39045129, deploy-
ment packages as in Q4813597, monitoring the changes in the application code (e.g.,
Q61938011). (2) Topic Asynchronous Batch Jobs (1.5%) contains discussions about
LCSD platforms’ support for monitoring and scheduling asynchronous batch jobs
and scheduled tasks. For example, “How to get your failing batch records?” in
Q11068830, “How can I schedule apex to run every 30 seconds?” in Q17143633.
Third-Party Integration Topic Category is smallest topic category based on
number of questions (12.1% questions). It has ﬁve Topics. Four of its topics fall un-
der two sub-categories: (1) REST API contains discussion related to RestFul API
communication with third-party services, (2) Plugins is about discussion about
external plugins and APIs that are supported by the LCSD platforms.

• REST API Sub-Category contains 6.5% questions and two topics: (1) Topic
External Web Req Processing (3.7%) contains discussion about integrating 3rd party
REST APIs, processing and parsing external requests such as “Connect to [Plat-
form] REST API with [Service] data integration” in Q51865601, Q46033973. (2) Topic
Fetch & Process API Response (2.8%) contains discussions about making HTTP re-
quest to remote servers (e.g., “REST http post method - what does -d mean in a
curl?” in Q65877037), analyzing and processing the response, handling web security
issues (e.g., CORS policies in Q60270574).
• Plugins Sub-Category contains 5.6% questions and three topics: (1) Topic
Email Processing (2.4%) discusses about processing automating emailing in
Q65626477, sending formatted HTML email (e.g, “Send HTML email using [plat-
form]” in Q41546887), forwarding emails in Q18234790 etc. (2) Topic Upgradation &

Topics of Developer Discussions of the LCSD Platforms

21

Compatibility (1.7%) contains discussion about application version migration as in
Q16894245, upgradation and compatibility issues of diﬀerent plugins used in low-
code applications (e.g., Q18231293, Q49017642). (3) Topic eSignature (1.5%) contains
discussion about diﬀerent issues and customization for electronic signature of doc-
uments, i.e., docusign about collecting user’s agreement/permission for sales or
account opening. For example, there are questions such as “Auto Add Document
to DocuSign [Platform] Using Custom Button” in Q34804072, Q27512874.

RQ1. What topics are discussed about LCSD in SO? We found 40 Topics
organized into ﬁve high-level categories. The Customization category (30%) has
the highest number of questions, followed by Data Storage (25%), Platform
Adoption (20%), Platform Maintenance (14%), and Third-Party Integration
(12%). Window Style Manipulation from the Customization category has the
highest number of questions (5.9%) followed by build Conﬁguration (4.1%)
Management from the Platform Maintenance category. Our studies reveal that
low-code practitioners struggle with RESTful API Integration, conﬁguration
and maintenance of the platforms. We also observed that proper
documentation could have mitigated these challenges to a great extent.

4.2 How do the LCSD topics evolve over time? (RQ2)

4.2.1 Motivation

Our analysis of RQ1 ﬁnds that LCSD topics are diverse. For example, the Cus-
tomization topic category contains discussions about developing and customizing
the application, and Platform Adoption and Platform Maintenance topic contains
discussions related to diﬀerent features provided by the LCSD platform providers.
The platforms for LCSD continue to evolve, as do the underlying topics and ques-
tion types. We study the evolution of these topics and question types to understand
better the evolution and adoption of LCSD development and its community. This
analysis will provide valuable insights into the LCSD community and help identify
if any topic needs special attention.

4.2.2 Approach

Following related studies [122], we study the absolute and relative impacts of each
of our observed ﬁve LCSD topic categories as follows.

Topic Absolute Impact. We apply LDA topic for our corpus C and get K
topics (t1, t2,........,tk). The absolute impact metric for a topic tk in a month (m)
is deﬁned as:

Impactabsolute(tk; m) =

D(m)
(cid:88)

pi=1

θ(pi; tk)

(1)

Here the D(m) is the total number of SO posts in the month m and θ(pi; tk)
denotes the possibility for a post (pi) belonging to a topic tk.

From our topic modeling, we found 40 topics that were categorized into ﬁve
high topic categories, i.e., Customization, Data Storage, Platform Adoption, Platform

22

Alamin et al.

Maintenance, Third-Party Integration. Now, we further reﬁne the equation for abso-
lute impact for LCSD topics to ﬁnd absolute impact metrics for a topic category
(T Cj) for a month m as follows:

Impactabsolute(T Cj; m) =

T Cj
(cid:88)

tk

Impactabsolute(tk; m), 0 < j < T C

(2)

Topic Relative Impact. Related impact metric signiﬁes the proportion of
posts for a particular LCSD topic tk relative to all the posts in our corpus C
for a particular month m. Following related studies [122], we compute the related
impact metrics for LCSD topics. We use the following equation to compute the
metric for a topic tk in a month m as follows:

Impactrelative(tk, m) =

1
|D(m)|

θ
(cid:88)

(pi; tk), 1 ≤ i ≤ C

(3)

pi=1

Here D(m) denotes the total number of posts that belongs to a topic tk for a
particular month m. Here δ denotes the probability of a particular post pi for our
Corpus C belonging to a particular topic tk.

Similar to the absolute impact, we reﬁne the equation to compute the relative

impact on LCSD topic categories as follows:

Impactrelative(T C; m) =

T C
(cid:88)

tk

Impactrelative(tk; m)

(4)

Here T C donates one of our ﬁve topic categories and topics that belong to each

topic category.

4.2.3 Result

Figure 6 depicts the progression of overall LCSD-related conversation using the
absolution impact equation from our extracted dataset between 2008 to 2021.
Additionally, it demonstrates that the peaks of LCSD-related talks occurred in
mid-2020 (i.e., around 400 questions per month). It also reveals that LCSD-related
discussion has been gaining popularity since 2012. In the section below, we provide
a more extensive explanation for the minor spikes in the Figure 6.

We observe that in the early days (i.e., 2009), Platform Adoption along with
Data Storage topic category has more questions compared to Customization. Cus-
tomization topic category starts to get a dominant position from mid (i.e., August)
of 2011 over Platform Adoption, and it remains the dominant topic till the end
of 2021. The number of questions in the Customization topic category gradually
increased over time, from August 2011 (23) to March 2012 (81) to May 2020
(99). Data Storage topic category brieﬂy exceeds Customization Category during
August 2013, but it mostly holds a dominant position other times compared to
Platform Adoption topic category. On the other hand, Platform Maintenance and
Third-Party Integration exhibits very similar evolution over the whole time.

We further look into the Figure 7 and see there are mainly two signiﬁcant
upticks in the number of posts about LCSD. The ﬁrst one is between August 2011

Topics of Developer Discussions of the LCSD Platforms

23

Fig. 6: The evolution of overall LCSD-related discussions over time.

Fig. 7: The absolute impact for LCSD topic categories over time.

to May 2012, when there is a sharp increase in the number of questions for almost
all topic categories, especially for Customization and Data Storage Category. By
this Salesforce[101] LCSD platform-related discussion becomes quite popular in
SO, and around that time, it ranks very high as a CRM platform. The second in-
crease in posts is between February 2020 and August 2020. During this time of the
Covid19 pandemic, many businesses began to operate online and there is a signiﬁ-
cant uptick in the number of questions in the Customization category, followed by

24

Alamin et al.

Fig. 8: The relative impact for LCSD topic categories over time.

Data Storage and Platform Adoption Moreover, there is an uptick in the number
of questions on building a simple app to collect and report data, especially the
Salesforce platform. There is an increase in the Platform Adoption topic category
between mid-2016 to mid-2017. During this time Oracle Apex platform released
version 5.0, and there is an uptick of questions regarding diﬀerent features such as
interactive grid in Q43316233, drag and drop layout in Q45818292. Now we provide
a detailed analysis of each of the ﬁve topic categories.

Customization This is the biggest topic category with 11 topics. From 2008
to mid of 2011, all of these topics evolve homogeneously. From the mid of 2011 to
the ﬁrst Quartile of 2012, Dynamic Page Layout topic becomes dominant. “How
to get ﬁelds in page layout” in Q7256190, issues with page layout in diﬀerent LCSD
platforms (e.g., Q7421985). From the end of 2012 to 2017, Window Style Manipu-
lation topic remains most dominant. “Passing values from child window to parent
window which is on another domain?” in Q16463602, view related issues Q15715645.
From the end of 2017 to the end, our dataset Dynamic Form Controller topic
remains the most dominant.

Data Storage Category From mid-2015, Database Setup & Migration topic
becomes the most dominant topic in this category and has some high spikes dur-
ing the pandemic and mid of 2017. For instance, there are queries like “Using
Jenkins for OCI database migration” in Q62217796 and “Almost all the cloud ser-
vice providers have 99.95% of data available on the cloud. What will happen if
the whole region sinks in an earthquake?” in Q62102679. Since 2017 DevOps and
database “Domino Xpage database building automation or continuous integration
using Jenkins with maven.” in Q43092239. From mid-2011 to mid-2014, DB Stored
Procedure topic remains dominant. “Oracle APEX: Call stored procedure from
javascript” in Q20501834.

Platform Adoption Category From 2008 to mid-2011, Platform Adoption
related topics were the most dominant (e.g., “Suggested platform/tools for rapid

Topics of Developer Discussions of the LCSD Platforms

25

game development and game prototyping” in Q312357). Between mid-2011 to mid-
2017, Authentication & Authorization topic becomes dominant (e.g., “Can I imple-
ment my own authentication process in force.com or it is against terms of service?”
Q13059568, Q13034866). Since the end of 2017, Platform Infrastructure API remains
the most dominant. So, practitioners ask queries like “VirtualBox VM changes
MAC address after imported to Oracle Cloud Infrastructure” in Q61501108 and
“How to send a classic report as mail body in oracle Apex 19.2” in Q59693984,
report layout (Q59833909, Q59752159).

Platform Maintenance Topic Category From 2008 to mid-2019, the Build
Conﬁguration Management topic remains the most dominant topic. It has some
high spikes in the number of questions during the beginning of 2012 and the
ﬁrst quartile of 2014. Build error Q21720165, Q21326163, build projects automati-
cally Q21758244. From mid-2019, Library Dependency Management topic-related
questions became popular (e.g., library-related issues (e.g., Q62825046, Q61100705),
library not foundQ61911916).

Third-Party Integration Topic Category. The ﬁve topics from this category
evolve simultaneously. From the beginning of 2015, the External Web Request Pro-
cessing topic has become more dominant than other topics with a slight margin.
External Web Request Processing and Fetch & Process API response, E-signature
topics become dominant during the pandemic with queries such as platform sup-
port on e-signature Q62417381 and etc.

In Figure 8, we now provide more insight into the evolution of LCSD topic
categories. It conﬁrms the ﬁndings presented in Figure 7 and adds some previ-
ously unknown insights. For instance, in the last quartile of 2009, it is apparent
that Data Storage is the most popular Topic Category. According to the absolute
impact metric, all ﬁve themes are increasing monotonically. The relative impact
measure, on the other hand, indicates that the Customization, Platform Main-
tenance, and Third-Party Integration Topic group evolves in a nearly identical
manner. However, this Figure demonstrates that, beginning in 2016, Platform
Adoption-related conversation increased and eventually surpassed Data Storage-
related discussion. This in-depth examination of evolution is signiﬁcant because it
demonstrates that, while Data Storage Topics are the second-largest Topic cate-
gory, Platform Adoption-related queries are evolving rapidly and require further
attention from platform vendors.

RQ2. How does the LCSD-related discussion evolve? Since 2012,
LCSD-related talks in SO have grown in popularity, and this trend has
accelerated since 2020. Initially, the Customization and Data Storage Topic
Categories dominated, but in recent years, Platform Adoption-related inquiries
have grown in popularity.

4.3 What types of questions are asked across the observed topic categories?
(RQ3)

4.3.1 Motivation

This research question aims to provide a deeper understanding of LCSD-related
topics based on the types of questions asked about the LCSD platforms in SO.

26

Alamin et al.

For example, “what” types of questions denote that developers are not sure about
some speciﬁc characteristics of LCSD platforms, while “how” types of questions
denote that they do not know how to solve a problem using an LCSD platform.
Intuitively, the prevalence of “what” types of questions would denote that the
LCSD platforms need to better inform the services they oﬀer, while the preva-
lence of “how” type of questions would denote that the LCSD platforms need
to have better documentation so that developers can learn easily on how to use
those. Initially, in 2011 Treude et al. [111] investigated diﬀerent types of questions
on stack overﬂow. Later Rosen et al. [97] conducts an empirical study like ours on
Mobile developers’ discussion in stack overﬂow with these four types of questions.
Later, very similar studies on chatbot development [1] and IoT developers’ discus-
sion on Stack overﬂow [122] also explore this research question to provide more
insights about speciﬁc domains and complement the ﬁndings of topic modeling.

4.3.2 Approach

In order to understand what-type of questions are discussed in SO by the
LCSD practitioners, we take a statistically signiﬁcant sample from our extracted
dataset and then manually analyze each question and label them into one of
four types: How-type, Why-type, What-type, Others-type following related stud-
ies [1, 97, 122]. So, our approach is divided into two steps: Step 1. We generate a
statistically signiﬁcant sample size, Step 2. we manually analyze and label them.

Step 1. Generate Sample. As discussed in Section 3.1 our ﬁnal dataset has 26,763
questions. A statistically signiﬁcant sample with a 95% conﬁdence level and ﬁve
conﬁdence intervals would be at least 379 random questions, and a 10 conﬁdence
interval would have a sample size of 96 questions. A random sample represents
a representative for the entire dataset, and thus this could miss questions from
the subset of questions that may belong to smaller topic categories. For example,
as discussed in RQ1, we have 40 topics organized into ﬁve categories. As random
sampling is not uniform across the topic categories, it might miss important ques-
tions from smaller topic categories such as Third-Party Integration. Therefore,
following previous empirical studies [1, 122], we draw a statistically signiﬁcant
random sample from each of the ﬁve topic categories. Speciﬁcally, we draw the
distribution of questions in our sample from each of the 5 topic categories with a
95% conﬁdence level and ten conﬁdence intervals. The sample is drawn as follows:
95 questions from the Customization category (total question 8014), 95 questions
from the Data Storage category (total question 6610), 94 questions from Platform
Adoption category (total question 5285), 94 questions from Platform Maintenance
category (total question 3607), 93 questions from Third-Party Integration category
(total question 3247). In summary, we sampled a total of 471 questions.

Step 2. Label Question Types. We analyze and label each question from our
samples into the following four categories. The categories and the coding guides
follow previous research [1, 98, 122]

– How-type post contains a discussion about the implementation details of a
technical task [122]. The questions primarily focus on the steps required to
solve certain issues or complete certain tasks (e.g., “How to create a submit
button template in Oracle APEX?” in Q1730566).

Topics of Developer Discussions of the LCSD Platforms

27

How-type
(55.6%)

What-type
(17.9%)

Other-type
(12.5%)

Why-type
(14%)

Fig. 9: Distribution of diﬀerent question Types

– Why-type post is about troubleshooting and attempting to determine the
cause/reason for a behavior. These questions help practitioners understand
the problem-solving or debugging approach, e.g., in Q25176669, a user is trying
to ﬁnd out why an SSL server certiﬁcate is rejected.

– What-type question asks for more information about a particular architec-
ture/event. The practitioners ask for more information that helps them to
make informed decisions. For example, in Q11608661 a practitioner is asking for
detailed information about the Oracle Apex platform’s secure cookies.

– Other-type question do not fall into any of the above three categories, e.g.,

“Initiating Salesforce API in Google App Script” in Q66317111

Three authors (ﬁrst, third and fourth) participated together in the labeling
process. We assessed our level of agreement using Cohen kappa[71]. The disagree-
ment and annotation diﬃculties were resolved by discussing with the ﬁrst author.
In general, the authors achieved a substantial agreement (k > 0.6) on the 471 ques-
tions classiﬁed. Our coding guidelines and the ﬁnal annotated dataset are available
in our replication package.

4.3.3 Result

Table 1 shows the percentage of type of questions across our ﬁve LCSD topic
categories. Similar to related studies [1, 98, 122], during our labeling process, we
observed that some of the questions can have multiple labels, e.g., What-type and
How-type. For example, “How can I get jQuery post to work with a [platform]
WebToLead” in Q2339550 discusses making Ajax request using jQuery where the
practitioner is getting an error response. At the same time, the practitioner is
further querying some detailed information on how Ajax requests. Therefore, the
sum of percentages of question types reported in the result section is more than
471 classiﬁed posts. We now discuss each question type with examples.

How-type. Around 57% of our annotated questions fall under this type. This
type of question is most prevalent in the topic categories Third-Party Integration
(61%) followed by Data Storage (60%), Platform Adoption (57%), Customization
(51%), Platform Maintenance (49%). This high prevalence is not surprising, given
that SO is a Q&A platform and the LCSD practitioners ask many questions about
how to implement certain features or debug an issue. Additionally this also sig-
niﬁes that LCSD practitioners are asking a lot questions while integrating with

28

Alamin et al.

Table 1: Types of questions across the LCSD topic categories

Topic Category

How What Why Other

Customization
Data Storage
Platform Adoption
Platform Maintenance
Third-Party Integration

51.0% 18.4% 17.3%
59.8% 13.4% 17.5%
57.3% 19.8%
9.4%
49.0% 20.4% 17.3%
8.2%
61.2% 17.3%

13.3%
9.3%
13.5%
13.3%
13.3%

Overall

55.6% 17.9% 14.0%

12.5%

third-party library (e.g., Q62825046) and plugins (e.g., Q61455233) and managing
the data with a database management system (e.g., Q38111768) or ﬁle storage (e.g.,
Q63284305). To explain further we ﬁnd questions regarding implementing encryp-
tion, e.g., Q2220076, or Making HTTP POST request, e.g., Q32736416, or debugging
a script, e.g., Q45619586, or implement a feature, e.g., Q28990848 etc.

What-type. This is the second biggest question type with 18% of all annotated
questions. This type of question is the most dominant in the topic categories Plat-
form Maintenance (20.5%), Platform Adoption (20%), and Customization (18%).
This type of question can be associated with How-type questions, where the practi-
tioners require further information to implement certain features. For instance, in
this question, a practitioner is querying about “How to implement circular cascade
select-lists” in Q60676786. The questions in this category signify that practitioners
fail to ﬁnd relevant information from the oﬃcial documentation (e.g., Q9377042)
sources. Therefore, as this type of question is prevalent in Platform Maintenance
and Platform Adoption category, LCSD platform providers might focus more on
improving their resources. As an example, we ﬁnd questions on JavaScript events
not working correctly, e.g., Q51564154, roll back changes, e.g., Q11156810, designing
workﬂow, e.g., Q11156810.

Why-type. This is the third most prevalent question type category, with 14%
of all annotated questions. This type of question is the most prevalent in the topic
categories Customization, Data Storage, and Platform Maintenance with around
17% questions. These questions are mostly related to troubleshooting like when
LCSD practitioners implement particular features or deploy an application. For
instance, e.g., “Why does this error happen in [Platform]?” in Q48818859, “Why
isn’t the document going into edit mode” in Q51660117, “Not able to launch Android
Hybrid app using [Platform] Mobile SDK” in Q20417235, “Java code running twice”
in Q17147921.

Other-type. Around 14% of our annotated questions fall under this type. The
questions are almost uniformly distributed across the ﬁve topic categories. The
questions contain general problems, e.g., “UTF-8 character in attachment name”
in Q22808965 or “Domino Server is installed on Unix or Windows?” in Q10796638.
Some of the questions in this type also contain multiple/ambiguous questions (e.g.,
Q27896327). For example, How to test an application?, which library is better? etc.

Topics of Developer Discussions of the LCSD Platforms

29

RQ3. What types of questions are asked across the observed topic
categories? How-type (57%) questions are the most prevalent across all ﬁve
topic categories, followed by What-type (18%), Why-type (14%), and
Other-type (12%) questions. Practitioners in the Customization and Platform
Maintenance topic categories are more interested in troubleshooting, i.e.
(Why-type, What-type). Practitioners generally ask more implementation
questions (i.e., How-type) in the Third-Party Integration Category.
Practitioners in the Data Storage topic category are interested in designing
databases (i.e., How-type) and troubleshooting (i.e., What-type, Why-type).
This indicates the necessity for a more robust community for troubleshooting
and debugging issues.

4.4 How are the observed topic categories discussed across SDLC phases? (RQ4)

4.4.1 Motivation

As we observed the prevalence and evolution of diverse LCSD topics in SO, we
also ﬁnd that the topics contain diﬀerent types of questions. This diversity may
indicate that the topics correspond to the diﬀerent SDLC phases that are used
to develop low code software development (see Section 2 for an overview of the
LCSD phases). For example, intuitively What-type of questions may be due to the
clariﬁcation of a low code software requirements during its design phases, which
questions/topics related to troubleshooting of issues may be asked during the
development, deployment, and maintenance phase. Therefore, an understanding
of the SDLC phases in the LCSD questions in SO may oﬀer us an idea about the
prevalence of those SDLC phases across our observed LCSD topics in SO. This
understanding may help the LCSD practitioners to determine how SO can be used
during low code software development across the various SDLC phases.

4.4.2 Approach

In order to understand the distribution of LCSD topics across agile SDLC phases,
we collect a statistically signiﬁcant sample of questions from our extracted dataset
D into one of the six Agile software development methodology [24] phases: (1) Re-
quirement Analysis & Planning, (2) Application Design, (3) Implementation,
(4) Testing, (5) Deployment, and (6) Maintenance. First, we generate a statis-
tically signiﬁcant sample size. We use the same set of randomly selected (i.e., 471)
posts that we produced during RQ3 (see Section 4.3). So, we take a statistically
signiﬁcant stratiﬁed random sample for each topic category in our dataset with
95% conﬁdence level and 10 conﬁdence interval to ensure that we have a repre-
sentative sample from each topic category [1, 122]. We manually annotate each
question post with one/more SDLC phases.

We followed the same annotation strategy to label SDLC phased as we did for
RQ3 (see Section 4.3.2). Each question was labeled by at least two authors (sec-
ond and third/fourth) after extensive group discussion on formalizing annotation
guidelines and participating in joint sessions. We ﬁnd our level of agreement using
Cohen kappa [6, 71]. The authors generally achieved a substantial agreement (k

30

Implementation 65%

Alamin et al.

Testing 2.7%
Maintenance 2.8%
Deployment 3.3%

Requirement Analysis & Planning 9.1%

Application Design 17%

Fig. 10: Distribution of questions (Q) per SDLC phase

> 0.70). For example, a new practitioner is tasked with ﬁnding the right LCSD
platform during the planning stage of his/her LCSD application. The practitioner
queries, “Are there any serious pitfalls to Outsystems Agile Platform?” (Q3016015).
We thus assign the SDLC phase as “Requirement Analysis & Planning”. Another
question asks, “Google App Maker app not working after deploy” (Q42506938). We
label the SDLC phase as “Deployment”. For some questions, it involved signiﬁcant
manual assessment to assign appropriate SDLC phase, e.g., Requirement Analysis
& Planning phase vs Application Design and Application Design vs Implemen-
tation phase. As such, we developed a detailed annotation/coding guide to help
us with the manual assessment. This annotation guide was constantly updated
during our study to ensure that the guide remained useful with all relevant in-
structions. For example, one of the questions that helped reﬁne our annotation
guide is the question noted by the respected reviewer, i.e., “Can AppMaker be
utilized with SQL Server?” in Q55220499. The user in this question wants to know
if Google App Maker and SQL Server databases can be connected. This question
was categorized as Application design phase. Based on this, according to our anno-
tation guideline, this question can be labelled as Requirement Analysis & Planning
phase too. However, after discussion, the ﬁrst and third authors agreed to label it
as Application Design phase because from the problem description, it seems the
question mainly focuses on connecting the application to a custom data storage.
As this question focuses on data source design, which is often explored during the
Application Design phase, we concluded that it should be labeled as such. The
labeling of each question to determine the precise SDLC phases was conducted by
several co-authors in joint discussion sessions spanning over 80 person-hours.

4.4.3 Results

Figure 10 shows the distribution of our LCSD questions into six agile SDLC phase.
We ﬁnd that the Implementation phase has 65% of our 471 annotated questions,
followed by Application Design (17%), Requirement Analysis & Planning (9.1%).
It is not surprising that the Implementation phase has so many questions because

Topics of Developer Discussions of the LCSD Platforms

31

SO is a technical Q&A platform and practitioners use it mostly to ﬁnd issues when
trying to implement some feature. Though the percentage of questions is not very
high (e.g., between 2-3%), we also ﬁnd practitioners ask questions regarding Test-
ing and Deployment phases too (e.g., “Automated Testing for Oracle [Platform]
Web Application” in Q1764497). This analysis highlights that LCSD practitioners
ask questions regarding the feasibility analysis of a feature to make design deci-
sions to implement the feature to deployment. We provide an overview of the types
of questions asked during these six SDLC phases.

Requirement Analysis & Planning (43, 9.1%). Requirement analysis is the
ﬁrst and most important stage of software development because the application
largely depends on this. Requirement analysis is a process to develop software
according to the users’ needs. In agile software development methodology, fea-
tures are implemented incrementally, and requirement and feasibility analysis are
crucial in implementing a new feature. During this phase, the operational factors
are considered, and the feasibility, time-frame, potential complexity, and reliabil-
ity. Requirement management tools are typically included with LCSD systems,
allowing developers to collect data, modify checklists, and import user stories into
sprint plans. Throughout this stage, developers tend to ask questions regarding
the platform’s features (e.g., “Does Mendix generates a source code in any par-
ticular language, which can be edited and reused?” in Q53043346), learning curve
(e.g., Q55304547, Q45631057), and the LCSD platform’s support for faster applica-
tion development (e.g., Q28983651), general deployment/maintenance support (e.g.,
Q50460088) in order to select the best platform for their needs. For example, in this
popular question, a new practitioner is asking for some drawbacks on some poten-
tial pitfalls for a particular LCSD platform, e.g., ”Are there any serious pitfalls to
[Platform] Agile Platform?” (Q3016015). A developer from that platform provider
suggests using the platform to build an application and decide for himself as it is
hard to deﬁne what someone might consider a pitfall. In another question, a prac-
titioner is asking if it is possible to integrate Selenium with an LCSD platform
(e.g., Q52010004)

Application Design (80, 17%). The design speciﬁcation is created in this step
based on the application’s needs. The application architecture (e.g., Q53820097),
modularity, and extensibility are all reviewed and approved by all critical stake-
holders. The LCSD developers face challenges regarding data storage design, drag
and drop UI design, connecting on-premise data-sources with the LCSD platform
(e.g., “Can AppMaker be used with SQL Server” (Q55220499)), data migration to
LCSD platform (Q46421271), following best practices (e.g, “Salesforce Best Practice
To Minimize Data Storage Size” in Q14073151), designing a responsive web page
(e.g., (Q52744026)).

Implementation (306, 65%). The actual application development begins at
this phase. LCSD developers confront a variety of obstacles when they try to cus-
tomize the application (i.e., personalize UI (e.g, Q6454308), implement business
logic (e.g, Q40472354)), integrate third-party plugins(e.g, Q46538734), debug (e.g,
Q35898112) and test the implemented functionality. For example, LCSD practition-
ers ask customization questions such as How can they change the timezone in a
platform in Q47731051, customizing UI in Q40159662. Many of these challenges arise
from incomplete or incorrect documentation. In Q34510911, an LCSD developer
asks for sample code to convert a web page to a PDF. The oﬃcial documentation
is not suﬃcient enough for entry-level practitioners.

32

Alamin et al.

Table 2: Distribution (frequency) of LCSD topics per SDLC phase. Each colored
bar denotes a phase (Black = Requirement Analysis, Green = Application De-
sign, Magenta = Implementation, Red = Testing, Blue = Deployment, Orange =
Maintenance)

Topics

Development Phases Found in #Questions

Customization (95)
Data Storage (95)
Platform Adoption (94)
Platform Maintenance (94)
Third-Party Integration (93)

5
6

17
16

17

9

17

11
4

21

71
70
51

46

10

68

1

1

1
12

2

2

1
1
4
6
1

Testing (13, 2.7%). LCSD testing diﬀers from standard software testing in
some fundamental ways. In LCSD development, many of the features are im-
plemented using graphical interfaces, and they are provided and tested by the
LCSD platform providers. As a result, unit testing is less important compared
to traditional software development. In LCSD approach practitioners face dif-
ﬁculties to lack of documentation of testing approach in LCSD platform (e.g,
“How to bypass login for unit-testing [Platform]?” in Q54432666), test coverage
(e.g, Q54899980, Q57755398), automated testing (e.g, “[Platform] 20.1 automated
testing” Q63594106), testing browser compatibility (e.g, Q), troubleshooting errors
while running tests (e.g, Q47254010) etc.

Deployment (16, 3.3%). At this phase, the feature of the application needs
to be deployed for the targeted users. One of the goals of LCSD development is to
handle many of the complexities of the deployment and maintenance phase. Many
LCSD platform providers provide advanced Application Life-Cycle Management
tools to deploy and maintain the staging (i.e., testing) and the production server
(e.g., Q65124133). However, LCSD practitioners still face many challenges regard-
ing deployment conﬁguration issues (Q46369742), Domain name conﬁguration (e.g.,
DNS conﬁguration (e.g, Q65678735), SSL Conﬁguration (e.g, Q67186273)), accessi-
bility issues such as with public URL (Q44136328, Q53884162)) etc. For example,
in this post, a practitioner is having deployment issues (e.g., “[Platform] app not
working after deployment” (Q42506938)). A community member provides a detailed
description of how to accomplish this in the answer, highlighting the lack of Oﬃcial
Documentation for such a critical use-case. There are a few questions concerning
delivering an app with a custom URL or domain name (for example, ”How to
make friendly custom URL for deployed app” in Q47194231). It was challenging in
this scenario because the platform did not have native support.

Maintenance (13, 2.8%). At this phase, the LCSD application is deployed
and requires ongoing maintenance. Sometimes new software development life cy-
cle is agile (i.e., incremental) because new issues are reported that were previously
undiscovered and request new features from the users. LCSD practitioners face
some problems at this phase, such as event monitoring (e.g, Q64322219), collabora-
tion and developers role management (e.g, “Role based hierarchy in report access”
in Q10436719 or Q52762374), and application reuse (e.g, Q64276891), application ver-
sion, i.e., “Do I have the latest version of an [Platform] component?” in Q45209796
or Q52762374, etc.

Topics of Developer Discussions of the LCSD Platforms

33

Table 3: Types of questions across the Software development life cycle phases

SDLC phase

How What Why Other

Requirement Analysis & Planning(9%)
Application Design(17%)
Implementation(65%)
Testing(3%)
Deployment(3%)
Maintenance(3%)

28%
75%
59%
62%
31%
46%

35%
16%
17%
15%
25%
15%

7%
4%
16%
15%
38%
31%

30%
10%
11%
8%
12%
15%

Topic Categories in different SDLC phases. We ﬁnd that for all ﬁve topic
categories, LCSD practitioners need some community support from planning to
debugging to deployment (e.g., “How does one deploy after building on [platform]”
in Q3952481). We report how LCSD topics and diﬀerent types of questions are dis-
tributed across six SDLC phases. Table 2 shows the distribution of SDLC phases
for each topic category. Our analysis shows that for the Customization topic Cat-
egory, most questions are asked during the Implementation (75%) and Design
(18%) phases. The most dominant SDLC phase, i.e., the Implementation phase,
is most prevalent in Customization (75%), Data Storage (74%), and Third-Party
Integration (73%). Requirement Analysis phase is dominant in Platform Adop-
tion (18%) and Platform Maintenance (12%) topic categories where practitioners
ask questions like “Disadvantages of the [platform]” in Q1664503. Similarly, ques-
tion in Platform Maintenance topic category is also prevalent in Testing (11%),
deployment (13%), and Maintenance (6%) SDLC stage.

Types of questions in different SDLC phases. We report the distribution of
question types across SDLC phases in Table 3. It shows that for Requirement
Analysis & Planning phase, most questions (35%) belong to What-type. This
insight signiﬁes that at this phase, practitioners are making inquiries about feature
details (e.g., Q9577099). In the Application Design, Implementation, and testing
phase, most of the questions belong to How-type, i.e., practitioners are querying
about how they can implement a particular feature (e.g., Q13933003) or test it (e.g.,
Q9594709). At the Deployment phase most prominent is Why-type (38%) followed
by How-type(31%). We can see a similar pattern for the Maintenance phase, where
the most signiﬁcant question type is How-type (46%) followed by Why-type (31%).
We see this pattern because, at the Deployment and Maintenance phase, most of
the questions belong to some server conﬁguration error (e.g., Q4497228) and the
practitioners’ inquiry about how they can set up speciﬁc server settings (e.g.,
Q8148247). Similarly, we ﬁnd that What-type questions are more prevalent during
Requirement Analysis and Deployment phases.

34

Alamin et al.

RQ4. How are the observed topic categories discussed across SDLC
phases? Among six agile SDLC phases, the Implementation phase is the most
prevalent (65% questions), followed by Application Design (17%), Requirement
Analysis & Planning (9.1%), Deployment (3.3%), Maintenance (2.8%) and
Testing (2.7%). The Implementation Phase is most prevalent in all of the ﬁve
topic categories and four question types. During Requirement Analysis,
Testing, and Deployment phases, Platform Adoption and Platform
Maintenance topic categories are more dominant. The How-type question is
most popular in the Application Design phase, the what-type question is
prevalent in the Requirement Analysis and Planning phase, and the why-type
question is prevalent in the Deployment and Requirement Analysis phases.

4.5 What LCSD topics are the most diﬃcult to get an accepted answer? (RQ5)

4.5.1 Motivation

After reviewing LCSD-related topics and discussions in the agile SDLC stages, we
discovered that LCSD practitioners encounter generic software development prob-
lems and particular challenges speciﬁc to LCSD platforms (e.g., Platform Adop-
tion, Platform Maintenance). Some posts come up repeatedly, and some have a
lot of community participation (i.e., answers, comments, up-votes). As a result,
not all topics and SDLC phases are equally diﬃcult to get a solution. A thorough
examination of the complexity and popularity of the practitioners’ conversation
might yield valuable information about how to prioritize research and community
support. For example, LCSD platform providers and academics can take the re-
quired measures to make the architecture, design, features, and tools of LCSD
platforms more useable for practitioners, particularly newbies.

4.5.2 Approach

We compute the diﬃculty of getting an accepted answer for a group of questions
using two metrics for each question in that group (1) Percentage of questions
without an accepted answer, (2) Average median time needed to get an accepted
answer. In the same way, we use the following three popularity metrics to calculate
popularity of that topic in the SO community: (1) Average number of views,
(2) Average number of favorites (i.e., for each question number of users marked as
favorite), (3) Average score.

The ﬁve metrics are standard features of a SO question, and many other related
studies [1, 4, 6, 16, 122] have used them to analyze the popularity and diﬃculty of
getting a solution for a question. In SO, one question can have multiple answers,
and The user who posted the question has the option of marking it as accepted.
Hence, the accepted answer is considered correct or sound quality. So, the absence
of an accepted answer may indicate the user did not ﬁnd a helpful, appropriate
answer. The quality of the question (i.e., problem description) might be one reason
for not getting an acceptable answer. However, the SO community collaboratively
edits and improves the posts. Therefore, the lack of an accepted answer most likely
indicates that the SO community ﬁnds those questions challenging to answer. The

Topics of Developer Discussions of the LCSD Platforms

35

success and usefulness of a crowd-sourced platform such as SO depends on the
community members to quickly provide relevant, helpful correct information. In
SO, the median time to get an answer is around 21 minutes only [122], but a
complicated or domain-speciﬁc question may necessitate additional time to receive
an accepted answer.

It can be non-trivial to assess the popularity and diﬃculty of getting an ac-
cepted answer for the topics using multiple metrics. We thus compute two fused
metrics following related works [122]. We describe the two fused metrics below.

Fused Popularity Metrics. First, we compute the popularity metrics for each
of the 40 LCSD topics. However, the average view counts can be in the range
of hundreds, average scores, and average favorite count between 0-3. Therefore,
following related study [122] we normalize the values of the metrics by dividing
the metrics by the average of the metric values of all the groups (e.g., for topics K
= 40). Thus, we create three new normalized popularity metrics for each topic. For
example the normalized metrics for a group i for all the K groups can be V iewNi,
F avoriteNi, ScoreNi (e.g., for LCSD topics K = 40). Finally, We calculate the
fused popularity F usedPi of a group i by taking the average of the three normalized
metric values.

V iewNi =

(cid:80)K

V iewi
j=1 V iewj
K

F avoriteNi =

(cid:80)K

ScoreNi =

F avoritei
j=1 F avoritej
K
Scorei
j=1 Scorej
K

(cid:80)K

F usedPi =

V iewNi + F avoriteNi + ScoreNi
3

Fused Diﬃculty Metrics. Similar to popularity metrics, we ﬁrst compute the
diﬃculty metrics for each topic. Then we normalize the metric values by dividing
them by the average of the metric value across all groups (e.g., 40 for LCSD
topics). Thus we, create two new normalized metrics for a given topic i. Finally,
We calculate the fused diﬃculty metric F usedDi of topic i by taking the average
of the normalized metric values.

P ctQuesW OAccAnsNi =

M edHrsT oGetAccAnsNi =

P ctQW oAcceptedAnsweri
(cid:80)K
j=1 P ctQW oAcceptedAnswerj
K
M edHrsT oGetAccAnsi
(cid:80)K
j=1 M edHrsT oGetAccAnsj
K

F usedDi =

P ctQuesW OAccAnsNi + M edHrsT oGetAccAnsNi
2

In addition to this, we also aim to determine the correlation between the dif-
ﬁculty and the popularity of the topics. We use the Kendall Tau correlation mea-
sure [56] to ﬁnd the correlation between topic popularity and topic diﬃculty. Unlike
Mann-Whitney correlation[61], it is not susceptible to outliers in the data. We can
not provide the evolution of popularity and diﬃculty for these topics because SO
does not provide the data across a time series for all metrics such as view count,
score, etc. However, asLCSD-related topics are showing increasing trends in recent
times, our analysis is valid for recent times.

(5)

(6)

(7)

(8)

(9)

(10)

(11)

36

Alamin et al.

Fig. 11: The popularity vs. diﬃculty of getting an accepted answer for LCSD Topic
categories.

4.5.3 Results

In Figure 11 we present an overview of the ﬁve high-level topic categories and
their popularity and diﬃculty to get an accepted answer. In the Figure, the bubble
size represents the number of questions in that category. The Figure shows that
Platform Adoption is the most popular and challenging topic category to get an
accepted answer, followed by Customization, Data Storage, Platform Maintenance,
and Third-Party Integration. We can also see that three topic categories, Platform
Maintenance, Data Storage, and Customization, are almost similar in terms of
diﬃculty to get a solution. From our analysis, we ﬁnd that practitioners ﬁnd
the Third-Party Integration topic category relatively less diﬃcult because many
questions in this category are also relevant to traditional software development
(e.g., integrating Google Maps in Q63457325 and Q1258834) and thus easier to get
community support. Similarly, we ﬁnd that questions in the Platform Adoption
topic category are quite speciﬁc to particular LCSD platforms and thus sometimes
have less community support to ﬁnd an acceptable answer quickly.

Topic Popularity. For each of the 40 topics, Table 4 shows three popularity
metrics: Average number of 1. Views, 2. Favorites, 3. Scores. It also contains the
combined popularity metrics (i.e., FusedP) that are based on the above three met-
rics and using the Equation 8. In the Table, the topics are presented in descending
order based on the FusedP popularity metric.

Platform Related Query topic from the Platform Adoption Category has the
highest FusedP score. It also has the highest average favorite count (e.g., 0.90)
and highest average score (e.g., 2.60). 1.7% of total questions. This topic contains
discussion about LCSD platforms features of diﬀerent platforms, software devel-
opment methodologies such as Agile and RAD development. The topic Message
Queue under Platform Adoption category has the second highest FusedP value.
This topic is about diﬀerent asynchronous service-to-service data exchange mech-
anisms such as using a message queue. It generally contains discussions about
popular micro-service design patterns. The topic Dynamic Page Layout under
Customization categories is the third most popular topic and it has the highest

CustomizationData StoragePlatform AdoptionPlatform MaintenanceThird-Party Integration0246810121402468101214Difficutly of Topic Category (Combined difficulty metric)Popularity of Topic Category (Combined popularity metric)Topics of Developer Discussions of the LCSD Platforms

37

Table 4: Popularity for getting an accepted answer for LCSD topics

Topic

Category

FusedP #View #Favorite #Score

Platform Related Query
Message Queue
Dynamic Page Layout
Build Conﬁg. Management
Pattern Matching
SQL CRUD
Web-Service Communication
Misc. SWE Discussion
Interactive Report
Fetch & Process API Response
Data Security & Replication
Hosting Conﬁg. & SEO
Asynchronous Batch Jobs
Authentication & Authorization
Library Dependency Mngmt
Email Processing
Formatted Data Parsing
File Management
DB Setup & Migration
Testing
Date & Time Manipulation
DB Stored Procedure
Dynamic Data Binding
External Web Req Processing
App Deployment
Semi-Structured Data Proc.
Upgradation & Compatibility
Dialog Box Manipulation
Dynamic Event Handling
Dynamic Form Controller
Conditional BPMN
SQL Sysntax Error
Graph/Chart
User Role Management
Window Style Manipulation
Entity Relationship Mgmt
Dynamic Data Filtering
Date-based ﬁltering
Platform Infrastructure API
eSignature

Platform Adoption
Platform Adoption
Customization
Platform Maintenance
Customization
Data Storage
Platform Adoption
Platform Adoption
Platform Adoption
Third-Party Integration
Data Storage
Platform Maintenance
Platform Maintenance
Platform Adoption
Platform Maintenance
Third-Party Integration
Customization
Data Storage
Data Storage
Platform Maintenance
Customization
Data Storage
Customization
Third-Party Integration
Platform Maintenance
Data Storage
Third-Party Integration
Customization
Customization
Customization
Customization
Data Storage
Platform Adoption
Platform Adoption
Customization
Data Storage
Customization
Data Storage
Platform Adoption
Third-Party Integration

3.45
1.49
1.31
1.22
1.17
1.15
1.15
1.13
1.12
1.11
1.1
1.09
1.06
1.05
1.05
1.05
1
0.98
0.97
0.96
0.94
0.94
0.92
0.91
0.89
0.88
0.88
0.79
0.77
0.76
0.75
0.75
0.75
0.73
0.71
0.71
0.66
0.57
0.56
0.52

2229.9
1671.7
2447.2
1522.1
1539.5
1615.7
1454.9
1193.4
1680.8
1277.8
1411.7
1377.9
1245.7
1057.6
1230.6
1600
1607.8
1302.6
1282.9
1810.5
1720.5
1715.5
1810.5
831.8
757.3
1096.5
559.1
1479.4
1245.1
1382.1
1335.1
1343
1156.3
1058.3
1000
1178
1148.1
781.9
577.2
574.9

0.9
0.3
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.1
0.2
0.2
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1

2.6
1.1
0.7
1
0.9
0.8
0.9
1
0.7
0.9
0.8
0.8
0.8
0.9
0.8
0.6
0.9
0.6
0.6
0.7
0.7
0.7
0.6
0.7
0.7
0.5
0.8
0.5
0.6
0.5
0.5
0.5
0.6
0.6
0.6
0.5
0.4
0.4
0.5
0.4

average view count (e.g., 2447.2). The posts under this topic discuss about UI
(i.e. page) customization, hiding or moving elements based on some user action or
an event (e.g., disable a button for dynamic action in Q8640964. The eSignature
topic from Third-Party Integration is the least popular with only 1.15% of total
questions, a fused value of 0.52. It has the lowest favorite and score count. This
contains discussion about diﬀerent issues and customization for electronic signa-
ture of documents, i.e., docusign about collecting user’s agreement/permission for
sales or account opening. This topic is not that much popular and easy to get
an accepted answer because this requirement is not generalized and not all the
low-code application requires this.

Topic Diﬃculty. In Table 5 we present the two diﬃculty metrics: for all the
questions in a topic 1. Percentage of questions without accepted answers, 2. Me-
dian hours to get accepted answer. Similar to topic popularity, we also report the
combined topic diﬃculty metrics (e.g., FusedD) using the Equation 11 and the
above two diﬃculty metrics. The topics in Table 5 are presented in descending
order based on the FusedD value.

38

Alamin et al.

Table 5: Diﬃculty for getting an accepted answer for LCSD topics

Topic

Category

FusedD Med. Hrs To Acc. Ques. W/O Acc.

Message Queue
Library Dependency Mngmt
Web-Service Communication
Authentication & Authorization
Platform Infrastructure API
Fetch & Process API Response
External Web Req Processing
App Deployment
Hosting Conﬁg. & SEO
eSignature
File Management
Asynchronous Batch Jobs
Dynamic Page Layout
User Role Management
Graph/Chart
Platform Related Query
DB Stored Procedure
DB Setup & Migration
Dynamic Form Controller
Testing
Conditional BPMN
Build Conﬁg. Management
Dynamic Event Handling
Semi-Structured Data Proc.
Email Processing
Dialog Box Manipulation
Interactive Report
Dynamic Data Binding
Dynamic Data Filtering
Misc. SWE Discussion
Data Security & Replication
Upgradation & Compatibility
Date-based ﬁltering
SQL Syntax Error
Entity Relationship Mgmt
Formatted Data Parsing
Date & Time Manipulation
Window Style Manipulation
Pattern Matching
SQL CRUD

Platform Adoption
Platform Maintenance
Platform Adoption
Platform Adoption
Platform Adoption
Third-Party Integration
Third-Party Integration
Platform Maintenance
Platform Maintenance
Third-Party Integration
Data Storage
Platform Maintenance
Customization
Platform Adoption
Platform Adoption
Platform Adoption
Data Storage
Data Storage
Customization
Platform Maintenance
Customization
Platform Maintenance
Customization
Data Storage
Third-Party Integration
Customization
Platform Adoption
Customization
Customization
Platform Adoption
Data Storage
Third-Party Integration
Data Storage
Data Storage
Data Storage
Customization
Customization
Customization
Customization
Data Storage

1.86
1.78
1.76
1.67
1.62
1.6
1.37
1.35
1.35
1.32
1.24
1.19
1.15
1.03
0.99
0.99
0.97
0.95
0.92
0.92
0.85
0.85
0.84
0.82
0.8
0.79
0.79
0.77
0.71
0.71
0.66
0.66
0.65
0.62
0.62
0.61
0.59
0.58
0.52
0.50

21.4
18.8
19.8
17.4
16.3
16.8
13.1
12.1
12.6
11.4
11.4
10.8
10.1
8.3
6.6
8.4
7.7
6.9
6.3
6.7
5.7
6.4
4.2
4.6
4.7
4.8
5
5.1
4.8
4.8
3.5
4.1
2
1.7
2.2
3.1
2.5
2.3
1.8
1.7

61
70
60
68
70
64
64
70
65
71
62
60
61
60
68
54
57
60
61
59
58
53
68
63
59
57
56
53
48
48
52
48
61
60
57
49
51
51
48
46

Table 6: Correlation between the topic popularity and diﬃculty

coeﬃcient/p-value

View Favorites

Score

% Ques. w/o acc. ans
Med. Hrs to acc. ans

-0.33/0.01
-0.05/0.65

0.02/0.88
0.30/0.02

-0.17/0.15
0.22/0.05

Topic Message Queue under Platform Adoption category is the most diﬃcult
topic to get an accepted answer in terms of FusedD value. Most median hours
to get accepted answers (21). This topic contains discussion about general micro-
service architecture (i.e., producer and consumer) and well as LCSD platform-
speciﬁc support for these architectures. This is why this topic is also second most
popular topic. Library Dependency Mngmt topic from Platform Maintenance is
the second most diﬃcult topic
to get an accepted answer. Around 70% of its
questions do not have any accepted answers. This topic concerns diﬀerent trou-
bleshooting issues about library and decencies of the system, server conﬁguration,
diﬀerent library version compatibility issues. Web-Service Communication topic
from Platform Adoption is the third most diﬃcult topic. It has a long median
wait time (around 20 hours) to get an accepted answer. This topic contains discus-

Topics of Developer Discussions of the LCSD Platforms

39

sions about service-to-service communication via web service description language,
HTTP REST message, and Windows Communication Foundation.

The topics that contain discussion about general software development (not
speciﬁc to LCSD platforms) are the least diﬃcult topics to get an accepted an-
swer. For example, topic SQL CRUD under Data Storage category is the least
diﬃcult topic in terms of FusedD value (e.g., 0.5). This contains database CRUD
related queries, and advanced queries too, such as inner join, nested join, aggregate.
This also contains discussion about Object query language, which is a high-level
wrapper over SQL. Topic SQL CRUD and SQL Syntax Error from the Data Stor-
age category are two of the least diﬃcult topics in terms of median hours to get
accepted answers. Topic Pattern Matching and SQL CRUD are two of the least
diﬃcult topics in terms of questions without accepted answers.

Alternatively, topics that are speciﬁc to LCSD platforms are the most diﬃcult
topics. Four out of ﬁve most diﬃcult topic belongs to Platform Adoption Cate-
gories. These questions can be popular as well as diﬃcult. For example, LCSD-
related Third-Party Integration related topic eSignature is the least popular topic
from Table 4, is the most diﬃcult topic in terms of questions without accepted
answers (71%). Topic Platform Related Query is in the mid-range in terms of
diﬃculty but most popular to get an accepted answer.

Correlation between Topic Diﬃculty and Popularity. Here we want to
explore if there is any positive or negative relationship between topic popularity
and diﬃculty. For example, Message Queue is the most diﬃcult and, at the same
time second most popular topic to get an accepted answer in terms of FusedD
and FusedP metrics. Platform Related Query is the most popular but mid-range
diﬃcult topic.

Table 6 shows six correlation measures between topic diﬃculty and popularity
in Table 4 and 5. Three out of six correlation coeﬃcients are negative, and the other
three are positive and they are not statistically signiﬁcant with a 95% conﬁdence
level. Therefore, we can not say the most popular topic is the least diﬃcult to get
an accepted answer and vice versa. Nonetheless, LCSD platform provides could
use this insight to take necessary steps. Most popular topics should have an easy
to access-able answer (i.e., least diﬃcult).

RQ5. What LCSD topics are the most diﬃcult to answer? Platform
Adoption is the most popular and challenging topic category, followed by
Customization, Data Storage, Platform Maintenance, and Third-Party
Integration. We also ﬁnd that LCSD practitioners ﬁnd Software Deployment
and Maintenance phase most popular and diﬃcult and Testing phase to be
least diﬃcult to an accepted answer. This indicates that LCSD platform
providers should provide additional support to enable low-code practitioners
understand and utilize the platform’s features.

5 Discussions

During our analysis, we observed that several LCSD platforms are more popular
across the topics than other platforms. We analyze our ﬁndings of LCSD topics
across the top 10 most prevalent LCSD platforms in the dataset (Section 5.4).
Finally, we discuss the implications of our study ﬁndings in Section 5.6.

40

Alamin et al.

5.1 Issues with not accepted answers or posts with negative score.

In this paper, for topic modeling we used questions and accepted answers only. We
did not consider the posts with negative score too because of the following observa-
tions. (1) Many other similar empirical studies on Topic modeling on SO posts also
considered the questions and accepted answers only, e.g., IoT developers discus-
sions in SO [122], big data related discussion [16], concurrency related topics [4],
mobile app development [97]. (2) A signiﬁcant number of studies [14, 86, 93, 132]
report quality of questions and unaccepted answers in SO are questionable and
therefore it is quite a standard practice for SE researchers to consider the ac-
cepted answers in SO only. For example, In Q7504057 (Fig 12) a user asks question
about python code/package to connect and retrieve data from Salesforce. The ac-
cepted answer A7504244 provides a relevant python code snippet the unaccepted
answer A34055640 provide resource link for a command line tool which may be
relevant but exactly not what the user asked for. (3) Negative scored questions
are typically incorrectly tagged (e.g., Q4862071, Q21377026, Q37371712), duplicates
(e.g., Q12282151, Q48121405), lack a detailed problem description (e.g., Q25691340,
Q1974480, Q50666660), lack correct formatting (e.g., Q32208310). For instance, in
Q51635004 (Fig 12) a user inquires about an error encountered when attempting
to contact a Zoho API. However, crucial important information such as an issue
code or error message is lacking from the question description. In Q4862071, an
inexperienced user inadvertently tagged a question about the Oracle Apex plat-
form with the Salesforce tag. We, therefore, choose not to include questions with
a negative score or unaccepted answers. We also provide potentially missing out
some insights for this choice in the threats to validity section (Section 6).

(a) A question with negative score

(b) A question with irreverent unaccepted answer

Fig. 12: SO questions with a negative score or unaccepted SO answer.

Topics of Developer Discussions of the LCSD Platforms

41

5.2 Discontinued low-code platforms and future trends.

From our analysis on Section 4.2 we see the evolution of LCSD platforms, especially
from 2012. According to our data, we can see the discontinuation of some low-code
platforms but they are usually soon replaced by new low-code/no-code services.
For example, In Jan 2020, Google announced the discontinuation of Google App
Maker [47] by 2021 [48]. But, shortly thereafter, Google announced a “no-code”
platform called “AppSheet” [46] and promoted their fully managed serverless plat-
form called AppEngine [11] to create web application promoting low-code ap-
proach. Microsoft and Amazon are also competing for superior low-code/no-code
platforms with the emergence of new low-code service platforms such as Microsoft
Power FX [74], Amazon Honeycode [51], AWS Amplify Studio [9]. The low-code
approach is attracting increasing interest from traditional businesses, particularly
during the pandemic [80].

5.3 LDA parameter Analysis.

In this study, we applied LDA topic modelling, which employs Dirichlet distri-
bution, to identify practitioners’ discussions on low-code. As described in details
in Section 3.2, we followed the industry standard to conﬁgure the parameters
and hyperparameters and also followed the industry recommendation to manually
annotate the topics as described in Section 4.1 in order to avoid sub-optimal solu-
tions [38]. Following similar studies [1, 50, 122] we use the use the coherence score
of of each model for diﬀerent values of K. However, since LDA itself is probabilistic
in nature [3] and can produce diﬀerent results diﬀerent runs on the same low-code
dataset. In order to mintage this problem, we run our LDA model three times and
compare the optimal number of topics. Fig. 13 shows the result of diﬀerent coher-
ence score for diﬀerent values of K. Moreover, we can see after reaching highest
coherence values for K = 45 the overall coherence score decreases as the value of
K increases.

Fig. 13: The diﬀerent coherence values for varied K on diﬀerent runs.

42

Alamin et al.

Fig. 14: The evolution of top ten LCSD platforms discussions over time.

5.4 The Prevalence & Evolution of Top Ten LCSD platforms

Our analysis of the evolution of topic categories (see Section 4.2) shows that there
is an overall increase in the number of new questions across the topics in SO. Our
SO dataset is created by taking into account the LCSD platforms. In Figure 14,
we show how the 10 LCSD platforms evolve in our SO dataset over the past
decade based on the number of new questions. Salesforce[101] is the biggest and
one of the oldest LCSD platforms (released in 1999) in our dataset with around
30% of all questions followed by Lotus Software[67], Oracle Apex [10], Microsoft
powerapps[88]. Among these platforms, IBM Lotus Software was quite popular
during the 2014s and gradually lost its popularity, and IBM ﬁnally sold it in 2018.
Salesforce platform has been the most popular platform in terms of SO discussions
since 2012. Our graph shows that these other three platforms, especially Microsoft
Powerapps, are gaining lots of attention during the pandemic, i.e., early 2020.

We provide more context for these platforms in Figure 15 by illustrating the dis-
tribution of our observed ﬁve topic categories across the top ten LCSD platforms.
We can see that Powerapps have the most number of queries in the Customization
and Platform Adoption category. This happens because Powerapps is a relatively
new LCSD platform (released in 2016) and it is gaining more and more attention
from the community, and thus there are more queries such as business logic im-
plementation (i.e., Q61685582), connect Powerapps to a database in Q61611950, user
permission (i.e., Q61838119). We can also see that older platforms such as Salesforce
and Oracle Apex have more queries regarding Platform Maintenance, Third-Party
Integration topic category. Practitioners ask many diﬀerent questions regarding
these platforms such as deployment-related (e.g., “Deploying a salesforce.com ﬂex
app without visualforce” in Q6614226), third-party API integration (e.g., “Google
Map Integrated with Salesforce is Blank” in Q9028682), maintenance deployment
(e.g., “Salesforce deployment error because of test class failure” in Q9171945), in-

Topics of Developer Discussions of the LCSD Platforms

43

Fig. 15: The distribution of topic categories across top ten LCSD platforms.

Fig. 16: The percentage of questions distributed across ﬁve topic categories for the
top ten LCSD platforms versus the rest of the platforms.

teractive report in Q9700660, customization with JSON Q9833992, “what is dash-
board?” in Q10911269 and Oracle Apex how to use platform in Q9438695. Platform
Adoption is a prevalent topic category in the Powerapps, ServiceNow, and Tibco
platforms. We also notice that the Data Storage category is quite popular in File-
maker and Lotus Software. Interestingly we see that Zoho Creator LCSD platform
around 60% questions belong to Third-party API integration (especially email
conﬁguration Q48865565). This data sheds light on the popular discussion topics of
more recent and earlier LCSD platforms.

44

Alamin et al.

5.5 The Case of “aggregating” data across multiple LCSD platforms.

In this study, we analysed 38 LCSD platforms and these platforms have distinct
characteristics and challenges. Our goal is to oﬀer researchers and practitioners a
comprehensive overview of the LCSD domain as a whole, as opposed to focusing on
a single LCSD platform. Hence, we integrated the data from all of these platforms.
For instance,Fig. 14 demonstrates that some of the most popular platforms such
as Salesforce, Oracle Apex, and Microsoft Powerapps have more questions in SO
than other LCSD platforms. Fig. 15 demonstrates that questions across these
platforms over diﬀerent topic categories diﬀers slightly. However, Fig. 16 shows that
questions for Application Customization and Platform Maintenance topic category
for top ten platforms vs others remain about the same at around 30% and 13%
respectively. Popular platforms have more questions related to Third Party API
integration (15%) than others (4%). The top ten platforms have relatively fewer
questions (15%) in Data Storage (23% vs 29%) and Platform Adoption (19% vs
24%) Category compared to other platforms. Overall, we found that the observed
topics are found across all the platforms that we studied. Given the popularity of
some platforms over others, it is understandable that those platforms are discussed
are more and as such some platforms can have more coverage (in terms of number of
questions) in a topic over other platforms. However, the prevalence of all platforms
across each topic shows that the topics are generally well-represented across the
platforms.

5.6 Implications

In Table 7, we summarize the core ﬁndings of our study and provide recommenda-
tions for each ﬁndings. The ﬁndings from our study can guide the following three
stakeholders: (1) LCSD platform Providers to improve the documentation, deploy-
ment, and maintenance support, (2) LCSD Practitioners/Developers to gain a bet-
ter understanding of the trade-oﬀs between rapid development and customization
constraints, (3) Community of LCSD Researchers & Educators to have a deeper
understanding of the signiﬁcant challenges facing the broader research area to
make software development more accessible. We discuss the implications below.

In this empirical study, we infer implications and recommendations based on
our observation of practitioners’ discussion in SO. So further validation from the
developers’ survey can provide more insight. However, the diversity of the low-
code platforms and topics makes it non-trivial to design a proper survey with
representative sample of LCSD practitioners. Therefore, the ﬁndings can be used
to design multiple LCSD related surveys focusing on diﬀerent low-code topics and
platforms.

LCSD Platform Vendors. In order to better understand the issues of LCSD,
we present a bubble chart with diﬃculty and popularity of diﬀerent aspects of
LCSD such as Topic Category in Figure 11, Types of questions in Figure 19 and
agile SDLC phases in Figure 18. These ﬁndings coupled with the evolution of
LCSD platforms (14) and discussions (7) shows that Customization and Data
Storage related queries are more prevalent, with the majority of these queries
occurring during Implementation agile SDLC stage. However, one of our interesting
ﬁndings is Platform Adoption related queries are increasing in popularity. LCSD

Topics of Developer Discussions of the LCSD Platforms

45

.
s
c
i

p
o
t

d
e
t
a
l
e
r
-
D
S
C
L

0
4

r
o
f

t
r
a
h
c

e
l
b
b
u
b

y
t
l
u
c
ﬃ
i
d

.
s
v

y
t
i
r
a
l
u
p
o
p

e
h
T

:
7
1

.
g
i
F

Interactive ReportDB Setup & MigrationWindow Style ManipulationDynamic Form ControllerBuild Config. ManagementFormatted Data ParsingDate & Time …SQL CRUDPattern MatchingDynamic Event HandlingDynamic Page LayoutConditional BPMNDB Stored ProcedureSQL Sysntax ErrorEntity Relationship MgmtDynamic Data FilteringDate-based filteringSemi-Structured Data Proc.Authentication & AuthorizationFile ManagementMessage QueueFetch & Process API ResponseExternal Web Req ProcessingWeb-Service CommunicationPlatform Infrastructure APIGraph/ChartUser Role ManagementPlatform Related QueryMisc. SWE DiscussionAsynchronous Batch JobsApp DeploymentTestingLibrary Dependency MngmtHosting Config. & SEOeSignatureUpgradation & CompatibilityEmail Processing0.340.540.740.941.141.341.541.741.940.430.630.831.031.231.431.631.83Difficulty of SDLC Phase (Combined diffuculty metric)Popularity of SDLC Phase (Combined popularity metric)46

Alamin et al.

Table 7: The summary of our core ﬁndings and recommendations

RQ Theme

Core Findings

Recommendations

1

2

3

4

5

Platform
Customiza-
tion

We identiﬁed 40 LCSD topics that
fall into ﬁve categories. Customiza-
tion of platform features continues
to be the most discussed challenge,
referring to the diﬃculty developers
face when seeking to adapt a given
feature to a particular situation.

The diﬃculty with customization can be
observed when the LCSD platforms do
not oﬀer anyway to change the default in-
terface/functionality. The platforms may
oﬀer coding interface where individual
components (e.g., a button) can be cus-
tomized by writing code.

Inte-

API
gration

Around 12% of the topics discuss
challenges related to the integration
of third party apps into an LCSD
platform like the integration of REST
services.

Platform
Adoption

Topics related to customization and
platform adoption are being dis-
cussed in recent years, because devel-
opers are increasingly asking for more
documentation and other supporting
features as the platforms are being
adopted in real-world scenarios.

How to Use More than 50% of the questions are
How-type, showing the needs for bet-
ter learning resources.

The LCSD platforms may introduce spe-
cialized and programmable interfaces to
support the call of a REST service and
to be able to process the service output
into a format that can be usable within
the platform.

While the oﬃcial documentation re-
1
sources can be further improved by the
LCSD vendor, automated tools can be in-
vestigated to generate documentation by
learning of existing adoption of the plat-
forms.

The oﬃcial documentation of implemen-
tation can be further enhanced by pro-
cessing the crowd-shared knowledge of the
usage discussions of the platforms (e.g.,
from Stack Overﬂow)

Server
Setup

What and Why-type questions are
most dominant for server setup re-
lated.

LCSD platform should provide provide
better visualizer and debugger for the
practitioners to improve troubleshooting.

Development
Eﬀort

Implementation is the most domi-
nant SDLC phase (65%). Interest-
ingly we also around one third of
the are related to Application Design
(17%) and Requirement Analysis &
Planning (9%)

Deployment
vs Mainte-
nance

Messaging

to

related

Questions
the
deployment-SDLC and Maintenance-
SDLC as
the most popular and
hardest to get accepted answers.

both

The topic message queue from plat-
form adoption is the most diﬃcult to
get an accepted answer. This topic
contains discussion about the adop-
tion of a general micro-service archi-
tecture within LCSD platforms

LCSD platform providers should take nec-
essary steps to provide better community
support in SO to address these challenges.
Practitioners should also be aware LCSD
platforms can improve the development
of traditional software development team
but they are not yet panacea for all prob-
lems.

Platform providers should provide bet-
ter level of abstraction for cloud manage-
ment, application deployment and moni-
toring. Educators can provide better re-
sources to learn about cloud platforms.

LCSD platforms can improve the adop-
tion of micro-service architecture and the
communication between diﬀerent the mi-
croservices with better message queuing

practitioners ﬁnd LCSD platform infrastructure and server conﬁguration-related
quires tough and popular during the Deployment and Maintenance phase. The top
ﬁve most challenging topics belong to Platform Adoption and Maintenance topic
category.

Many new practitioners make queries regarding LCSD platforms, learning re-
sources, basic application and UI customization, and how to get started with this
new emerging technology. Figure 17 shows that Platform Related Query topic is
the most popular among LCSD practitioners. We ﬁnd that Documentation related

Topics of Developer Discussions of the LCSD Platforms

47

Fig. 18: The popularity vs. diﬃculty bubble chart for low-code software develop-
ment life cycle phases

queries are both top-rated and challenging. Our ﬁndings also suggest that many
practitioners still face challenges during testing, especially with third-party test-
ing tools like JUnit (in Q9811992) and troubleshooting. Consequently, many of the
questions on this topic remain unanswered. It reveals that to ensure smooth adop-
tion of the LCSD platforms, providers should provide better and more eﬀective
documentation and learning resources to reduce entry-level barriers and smooth
out the learning curve.

LCSD Practitioners/Developers. Gartner [45] estimates that by 2022, more
than half of the organizations will adapt LCSD to some extent. Additionally, our
analysis reveals a rising trend for LCSD approaches, particularly during Covid-19
pandemic (Fig. 6). We can also see that new LCSD platforms such as Microsoft
Powerapps are gaining many developers’ attention. LCSD platform enables prac-
titioners with diverse experience to contribute to the development process even
without a software development background. However, our ﬁnding shows that
practitioners ﬁnd debugging, application accessibility, and documentation chal-
lenges. Hence, the practitioners should take the necessary steps to understand
the tradeoﬀs of LCSD platforms’ features deeply. The project manager should
adopt speciﬁc strategies to customize, debug, and test the application. For ex-
ample, many practitioners struggle with general Third-Party API integration and
database design and query. We ﬁnd that DevOps-related tasks such as CI/CD,
Server conﬁguration, and monitoring-related queries are most challenging to the
practitioners. So, a well-functioning LCSD team should allocate time and resources
to them. It provides valuable insights for project managers to manage resources
better (i.e., human resources and development time).

Figure 18 shows that Maintenance is the most popular development phase, fol-
lowed by Deployment, and Testing is the least popular SDLC phase. Similarly, the
ﬁgure also shows that questions asked in deployment phase are the most diﬃcult
followed by Maintenance. Implementation, Requirement analysis and planning,

48

Alamin et al.

Application design phase are in the middle range in terms of popularity and dif-
ﬁculty spectrum. Thus, our analysis indicates that LCSD practitioners face more
broad and complex application maintenance and deployment-related challenges,
on which LCSD platform vendors should concentrate their eﬀorts. This ﬁnding
can inﬂuence the decision-making process of LCSD developers and practitioners
like prioritizing their eﬀorts during the design, development, and deployment of
software that uses LCSD platforms. For example, if suﬃcient support or tools are
not available for scalable usage and deployment of an LCSD platform, developers
may look for alternatives that have better deployment and maintenance support.
One fundamental shortcoming of LCSD platforms is that their abstraction
and feature limitations can make customization and debugging extremely diﬃ-
cult. Additionally, managed cloud platforms make data management and deploy-
ability more challenging [69, 100]. The ﬁndings in this study help to present some
strengths and limitations of the overall LCSD paradigm, which complements the
ﬁndings of other studies [2, 8, 69, 76, 100, 128]. The analysis could assist LCSD
teams in selecting the appropriate LCSD platforms, which is critical for future
success.

LCSD Researchers & Educators. The ﬁndings of this study have many
implications for researchers and educators of LCSD platforms and the border
research community to improve the software development process. We discover
that What-type and How-type questions are popular among LCSD practition-
ers. They also ﬁnd them challenging because of adequate usable documentation.
Thus, practitioners ask questions about certain limits or how to implement cer-
tain features, and in the accepted answer, some other user simply points to the
oﬃcial documentation page (e.g., “Domino Data Service API Documentation” in
Q59739877 and Q5806293). Many of the challenges faced by low-code petitioners are
similar to traditional software developers. So, researchers from border software
engineering domain can contribute to improving aspects such as improving doc-
umentation [23, 26, 57], improving API description usage [119, 120] and make it
more accessible to general practitioners. In the Customization and Data Storage
topic category, we ﬁnd practitioners asking help in generic programming queries,
database design, and ﬁle management. So, research on those topics will also help
the adoption of LCSD. Some LCSD platforms provide great in-build support for
unit and functional testing. However, we ﬁnd around 2.1% of questions belong to
Testing topic. Most of these LCSD platforms heavily rely on cloud computing, and
thus research improvement of server conﬁguration and library management, i.e.,
DevOps [134] in general, will aid in better platforms. On the other hand, educa-
tors can focus their eﬀorts on making the learning resources on Automatic testing,
Server conﬁg. and DevOps practices such as CI/CD more accessible to the citizen
developers.

Figure 19 shows What-type of posts are most popular, followed by Why-type,
How-type, and Others-type. Additionally, it demonstrates that the most challeng-
ing question type is Why-type, followed by What-type, How-type, and others. So,
although How-type questions are most dominant, the details types (i.e., Why-
type, What-type) of questions are more popular and challenging. This analysis
implies that LCSD practitioners have a harder time ﬁnding detailed information
regarding diﬀerent platform features. As a result, LCSD platform providers should
improve their documentation. Intuitively, How-type questions can be answered
with better documentation for LCSD platforms. Given the oﬃcial API documen-

Topics of Developer Discussions of the LCSD Platforms

49

Fig. 19: The popularity vs. diﬃculty of diﬀerent types of LCSD-related questions

tation can often be incomplete and obsolete [58, 121] and given that our research
shows that LCSD developers use SO to ask questions about various topics, LCSD
researchers can develop techniques and tools to automatically improve the doc-
umentation of LCSD platforms by analyzing SO questions and answers. Indeed,
existing research eﬀorts show that solutions posted in SO can be used to improve
API documentation by supporting diverse development tasks and programming
languages [32, 112, 114, 115, 116, 117, 118].

We ﬁnd that the LCSD paradigm’s challenges can be diﬀerent from tradi-
tional software development [100]. Simultaneously, researchers can study how to
provide better tools for practitioners to customize the application. Security is an
open research opportunity for such platforms as a security vulnerability in such
platforms or frameworks could compromise millions of applications and users [64].
Researchers can develop better testing approaches to ensure faster development
and dependability. Educators can also beneﬁt from the results presented in Ta-
ble 4, 4 and Figure 17 to prioritize their focus on diﬀerent topics such as Library
Dependency Mngmt, Web-Service Communication, Asynchronous Batch Jobs, Testing,
Dynamic Form Controller.

6 Threats to Validity

Internal validity threats, in our study, relate to the authors’ bias while conducting
the analysis as we have manually labeled the topics. We mitigate the bias in our
manual labeling of topics, types of questions, and LCSD phases by consulting the
labels among multiple authors and resolving any conﬂicts via discussion. Four of
the authors actively participated in the labelling process. The ﬁrst author reviewed
the ﬁnal labels and reﬁned the labels by consulting with the second author.

Construct Validity threats relate to the errors that may occur in data collection,
like identifying relevant LCSD tags. To mitigate this, we created our initial list of

50

Alamin et al.

tags, as stated in Section 3, by analyzing the posts in SO related to the leading
LCSD platforms. Then we expanded our tag list using state-of-art approach [1,
4, 16, 98]. Another potential threat is the topic modeling technique, where we
choose K = 45 as the optimal number of topics for our dataset B. This optimal
number of topics has a direct impact on the output of LDA. We experimented
with diﬀerent values of K following related works [1, 16]. We used the coherence
score and manual examination to ﬁnd K’s optimal value that gives us the most
relevant and generalized low-code related topics [1, 6, 122].

External Validity threats relate to the generalizability of our ﬁndings. Our study
is based on data from developers’ discussions on SO. However, there are other
forums LCSD developers may use to discuss. We only considered questions and
accepted answers in our topic modeling. We also had the option of choosing the
best answer. In SO, the accepted answer and best answer may be diﬀerent. Ac-
cepted answer is the one approved by the questioner while the best answer is voted
by all the viewers. as discussed in Section 5.1 it is quite diﬃcult to detect if an an-
swer is relevant to the question or not. Thus We chose the accepted answer in this
study because we believe that the questioner is the best judge of whether the an-
swer solves the problem or not. Even without the unaccepted answers, our dataset
contains around 38K posts (27K questions + 11K accepted answers). This also
conforms with previous works [1, 6, 122, 132]. Some novice practitioners post du-
plicate questions, assign incorrect tags, and provide inadequate descriptions, which
receives an overall negative score from the community. To ensure that topics con-
tain relevant discussion of high quality, we only use posts with non-negative scores.
Nevertheless, we believe using SO’s data provides us with generalizability because
SO is a widely used Q&A platform for developers. However, we also believe this
study can be complemented by including the best answers to the questions in SO,
as we discussed earlier, including discussions from other forums, surveying, and
interviewing low-code developers.

7 Related Work

We previously published a paper at the MSR 2021 based on an empirical study
of LCSD topics in SO (see [6]). We compared the ﬁndings of this paper against
our previous paper in Section 1. Other related work can broadly be divided into
two categories: SE (Software Engineering) research on/using (1) low code software
development (Section 7.1), and (2) topic modeling (Section 7.2).

7.1 Research on Low Code Software Development and Methodologies

LCSD is a new technology, with only a handful of research papers published in this
ﬁeld. Some research has been conducted on the potential applications of this devel-
oping technology in various software applications [44] or for automating business
process in manufacturing [128], healthcare [76, 131], Digital transformation [84],
Industrial engineering education[2], IoT systems using LCSD [53]. Sipio et al. [39]
present the beneﬁts and future potential of LCSD by sharing their experience of
building a custom recommendation system in the LCSD platform. Kourouklidis et

Topics of Developer Discussions of the LCSD Platforms

51

al. [60] discuss the low-code solution to monitor the machine learning model’s per-
formance. Sahay et al. [100] survey LCDP and compare diﬀerent LCDPs based on
their helpful features and functionalities. Khorram et al. [59] analyse commercial
LCSD platforms and present a list of features and testing challenges. Zhuang et
al. [135] created a low-code platform called EasyFL where researchers and educa-
tors can easily build systems for privacy-preserving distributed learning method.
Ihirwe et al. [54] analyse 16 LCSD platforms and identiﬁes what IoT application-
related features and services each platform provides. All of these studies compare
a single LCSD platform and its support and limitations for various sorts of appli-
cations [7], rather than taking a holistic picture of the diﬃculties that the broader
community faces.

There are also some studies where researchers proposed diﬀerent techniques to
improve LCSD platform such as Overeem et al. [78] on LCSD platform’s impact
analysis, Jacinto et al. [55] improve testing for LCSD platforms.

Additionally, there are some studies that describe the diﬃculties faced by
LCSD practitioners. The main research methodology and objectives of these stud-
ies, however, are signiﬁcantly diﬀerent from this study. Yajing et al. [69] analyse
the LCSD platform’s characteristics including programming languages used, ma-
jor implementation units, supporting technologies, applications being developed,
domains, etc., along with the beneﬁts, limitations, and challenges by collecting rel-
evant posts from SO and Reddit. In this study, we use tag-based approach to ﬁnd
relevant LCSD-related posts which is much more reliable than text-based search-
ing. Furthermore, the SO related discussion used in this study is signiﬁcantly larger
and our research objective about LCSD platforms challenges are quite diﬀerent.
Timothy et al. [62] discuss experiences with several low-code platforms and provide
recommendations focusing on low-code platforms enabling scaling, understandabil-
ity, documentability, testability, vendor-independence, and the overall user expe-
rience for developers as end-users who do some development. Danial et al. [37]
and ALSAADI et al. [8] Surveyed on factors hindering the widespread adaptation
of LCSD by interviewing LCSD developers or conducting a survey. To the best
of our knowledge, ours is the ﬁrst empirical study of LCSD platforms based on
developers’ discussions from Stack Overﬂow, and hence our ﬁndings complement
those of other studies.

7.2 Topic Modeling in Software Engineering

Our motivation to use topic modeling to understand LCSD discussions stems from
existing research in software engineering that shows that topics generated from tex-
tual contents can be a good approximation of the underlying themes [34, 104, 105].
Topic models are used recently to understand software logging [63] and previously
for diverse other tasks, such as concept and feature location [35, 87], traceability
linking (e.g., bug) [15, 91], to understand software and source code history evo-
lution [52, 106, 107], to facilitate code search by categorizing software [108], to
refactor software code base [22], as well as to explain software defect [33], and
various software maintenance tasks [103, 104]. The SO posts are subject to several
studies on various aspects of software development using topic modeling, such as
what developers are discussing in general [19] or about a particular aspect, e.g.,
concurrency [4], big data [16], chatbot [1].

52

Alamin et al.

Table 8: Comparing the popularity and diﬃculty metrics of diﬀerent domains

Type Metrics

LCSD

IoT Big Data Chatbot

Security Mobile Concurrency

# Posts
Avg View
Avg Favorite
Avg Score

% W/o Acc. Ans
Med Hrs to Acc.

33,766
1330.6
1.2
0.7

41%
5.7

53,173
1,320.3
1.5
0.8

64%
2.9

125,671
1,560.4
1.9
1.4

60.3%
3.3

3,890
512.4
1.6
0.7

67.7%
14.8

94,541
2,461.1
3.8
2.7

48.2%
0.9

1,604,483
2,300.0
2.8
2.1

52%
0.7

P

D

245,541
1,641
0.8
2.5

43.8%
0.7

Table 9: Comparative analysis of question types across diﬀerent domains

Question Type How What Why Others

LCSD
IoT
Chatbot

14%
55.6% 17.9%
47.3% 37.9%
20%
61.8% 11.7% 25.4%

12.5%
8.3%
1.2%

In particular, SO posts have been used in various studies where the researchers
analysed topics for that particular domains. For instance, SO posts has been used
to study developers challenges in IoT [122], big data [16], chatbots [1] and so on.
The distributions and the nature of these posts diﬀers. As SO is arguably the most
popular public forum for developers, the analysis of these domains’ characteristics
may help us identify the SO community better. Therefore, a systematic analysis
of these domains is interesting. Following related studies[122], we use six metrics
in this study: (1) Total #posts, (2) Avg views, (3) Avg favorite, (4) Avg score,
(5) Percentage of questions without accepted answers, (6) Median hours to get
accepted answers per domain. The ﬁrst four metrics are popularity metrics and
the last two are diﬃculty metrics.

In this study, we do not replicate the ﬁndings of the original study in our
dataset. Rather we only report the ﬁndings from the original study. So following
related work [122], we compared our LCSD-related discussions with other ﬁve
domains: IoT [122],big data[16], security [133], mobile apps [97], chatbots [1] and
concurrency [4].

Table 8 provides an overview of the seven metrics. We can see that it has a
greater number of SO posts than chatbot domains but fewer than the other ﬁve
domains. There are two other studies on Blockchain [127] and deep learning [50]
where the total number of posts are 32,375 and 25,887, respectively. However,
these two studies did not report the other metrics, so they are excluded from the
Table. Although the LCSD-related discussion may have fewer posts than these
other domains, as discussed in RQ3, this number is increasing rapidly.

We can also observe that the LCSD domain shows similarities with IoT, Con-
currency domain in terms of Avg. View count. Security and Mobile domain seem
most popular in terms of Avg. Favourite count and LCSD rank lower in this met-
ric. LCSD domains most resemble with IoT domain in terms of Avg. View, Avg.
Favourite and Avg. Score. In terms of diﬃculty metrics percentage of posts without
accepted answers, LCSD domain ranks lowest, which is good. However, it takes
much longer to get accepted answers( 0.7 hours for Mobile, 2.9 for IoT). Only
the chatbot domain requires more time to get accepted answers (i.e., 5.3 hours in
LCSD vs 14.8 hours in chatbot).

Topics of Developer Discussions of the LCSD Platforms

53

We further discuss the LCSD, IoT, and Chatbot domains with the distribu-
tion of diﬀerent types of questions in Table 9. We ﬁnd that LCSD domain is in
the middle of IoT and Chatbot domains in terms of How-type (57%) compared
to IoT (47%) and chatbot (62%). This signiﬁes that LCSD domain practition-
ers ask more about implementation-related questions. In the Why-type question
percentage of LCSD-related domains is lowest (14%) compared with IoT (20%)
and chatbot (25%). This suggests that practitioners in the LCSD domain enquire
about relatively modest troubleshooting issues. For What-type, we notice that the
IoT domain dominates with 38% of questions, compared to the LCSD domain’s
12%. Practitioners of LCSD are less inquisitive about domain architecture and
technologies compared to IoT domain. As a result of these analyses, we can see
that LCSD domain practitioners exhibit several traits that distinguish them from
practitioners in other domains, which LCSD vendors and educators should take
into account.

8 Conclusions

Low Code Software Development (LCSD) is a novel paradigm for developing soft-
ware applications utilizing visual programming with minimum hand-coding. We
present an empirical study that provides insights into the types of discussions low-
code developers discuss in Stack Overﬂow (SO). We ﬁnd 40 low-code topics in
our dataset of 33.7K SO posts (question + accepted answers). We collected these
posts based on 64 SO tags belonging to the popular 38 LCSD platforms. We cat-
egorize them into ﬁve high-level groups, namely Application Customization (30%
Questions, 11 Topics), Data Storage (25% Questions, 9 Topics), Platform Adop-
tion (20% Questions, 9 Topics), Platform Maintenance (14% Questions, 6 Topics),
and Third-Party Integration (12% Questions, 5 Topics). We ﬁnd that the Plat-
form Adoption topic category has gained popularity recently. Platform Related
Query and Message Queue topics from this category are the most popular. On the
other hand, We also ﬁnd that practitioners ﬁnd Platform Adoption and Mainte-
nance related topics most challenging. How-type questions are the most common,
but our research reveals that practitioners ﬁnd what-type and why-type questions
more diﬃcult. Despite extensive support for testing, deployment, and maintenance,
our analysis shows that server conﬁguration, data migration, and system module
upgrading-related queries are widespread and complex to LCSD practitioners. De-
spite signiﬁcant testing, deployment, and maintenance support, our analysis ﬁnds
numerous and complex queries regarding server conﬁguration, data migration, and
system module updating. Our analysis ﬁnds that better tutorial-based documenta-
tion can help solve many of these problems. We also ﬁnd that during the Covid-19
pandemic, LCSD platforms were very popular with developers, especially when
it came to dynamic form-based applications. We hope that all of these ﬁndings
will help various LCSD stakeholders (e.g., LCSD platform vendors, practitioners,
SE researchers) to take necessary actions to address the various LCSD challenges.
Since the growth indicates that this technology is likely to be widely adopted
by various companies for their internal and customer-facing applications, plat-
form providers should address the prevailing developers’ challenges. Our future
work will focus on (1) getting developers’ feedback on our study ﬁndings based

54

Alamin et al.

on surveys and developer interviews, and (2) developing tools and techniques to
automatically address the challenges in the LCSD platforms that we observed.

References

1. A. Abdellatif, D. Costa, K. Badran, R. Abdalkareem, and E. Shihab. Challenges in
chatbot development: A study of stack overﬂow posts. In Proceedings of the 17th In-
ternational Conference on Mining Software Repositories, MSR ’20, page 174–185, New
York, NY, USA, 2020. Association for Computing Machinery.

2. B. Adrian, S. Hinrichsen, and A. Nikolenko. App development via low-code programming
In International Conference on

as part of modern industrial engineering education.
Applied Human Factors and Ergonomics, pages 45–51. Springer, 2020.

3. A. Agrawal, W. Fu, and T. Menzies. What is wrong with topic modeling? and how to
Information and Software Technology,

ﬁx it using search-based software engineering.
98:74–88, 2018.

4. S. Ahmed and M. Bagherzadeh. What do concurrency developers ask about? a large-
scale study using stack overﬂow. In Proceedings of the 12th ACM/IEEE International
Symposium on Empirical Software Engineering and Measurement, ESEM ’18, New York,
NY, USA, 2018. Association for Computing Machinery.

5. P. A. Akiki, P. A. Akiki, A. K. Bandara, and Y. Yu. Eud-mars: End-user development
of model-driven adaptive robotics software systems. Science of Computer Programming,
200:102534, 2020.

6. M. A. A. Alamin, S. Malakar, G. Uddin, S. Afroz, T. B. Haider, and A. Iqbal. An
empirical study of developer discussions on low-code software development challenges.
In 2021 IEEE/ACM 18th International Conference on Mining Software Repositories
(MSR), pages 46–57. IEEE, 2021.

7. A. N. Alonso, J. Abreu, D. Nunes, A. Vieira, L. Santos, T. Soares, and J. Pereira. Towards
a polyglot data access layer for a low-code application development platform. arXiv
preprint arXiv:2004.13495, 2020.

8. H. A. ALSAADI, D. T. RADAIN, M. M. ALZAHRANI, W. F. ALSHAMMARI,
D. ALAHMADI, and B. FAKIEH. Factors that aﬀect the utilization of low-code de-
velopment platforms: survey study. Romanian Journal of Information Technology and
Automatic Control, 31(3):123–140, 2021.

9. AWS Amplify Studio overview. Available: https://aws.amazon.com/amplify/studio/.

[Online; accessed 5-January-2022].

10. Oracle Apex Platform. Available: https://apex.oracle.com/.

[Online; accessed 5-

January-2022].

11. App Engine: a fully managed, serverless platform for developing and hosting web applica-
tions at scale. Available: https://cloud.google.com/appengine/docs. [Online; accessed
13-December-2021].

12. Appian platform overview. Available: https://www.appian.com/.

[Online; accessed 5-

January-2022].

13. R. Arun, V. Suresh, C. V. Madhavan, and M. N. Murthy. On ﬁnding the natural number
of topics with latent dirichlet allocation: Some observations. In Paciﬁc-Asia conference
on knowledge discovery and data mining, pages 391–402. Springer, 2010.

14. M. Asaduzzaman, A. S. Mashiyat, C. K. Roy, and K. A. Schneider. Answering questions
In 2013 10th Working Conference on

about unanswered questions of stack overﬂow.
Mining Software Repositories (MSR), pages 97–100. IEEE, 2013.

15. H. U. Asuncion, A. U. Asuncion, and R. N. Taylor. Software traceability with topic
modeling. In 2010 ACM/IEEE 32nd International Conference on Software Engineering,
volume 1, pages 95–104. IEEE, 2010.

16. M. Bagherzadeh and R. Khatchadourian. Going big: A large-scale study on what big
data developers ask.
In Proceedings of the 2019 27th ACM Joint Meeting on Euro-
pean Software Engineering Conference and Symposium on the Foundations of Software
Engineering, ESEC/FSE 2019, pages 432–442, New York, NY, USA, 2019. ACM.

17. K. Bajaj, K. Pattabiraman, and A. Mesbah. Mining questions asked by web developers.
In Proceedings of the 11th Working Conference on Mining Software Repositories, pages
112–121, 2014.

Topics of Developer Discussions of the LCSD Platforms

55

18. A. Bandeira, C. A. Medeiros, M. Paixao, and P. H. Maia. We need to talk about mi-
croservices: an analysis from the discussions on stackoverﬂow. In 2019 IEEE/ACM 16th
International Conference on Mining Software Repositories (MSR), pages 255–259. IEEE,
2019.

19. A. Barua, S. W. Thomas, and A. E. Hassan. What are developers talking about? an analy-
sis of topics and trends in stack overﬂow. Empirical Software Engineering, 19(3):619–654,
2014.

20. F. Basciani, L. Iovino, A. Pierantonio, et al. Mdeforge: an extensible web-based modeling
platform. In 2nd International Workshop on Model-Driven Engineering on and for the
Cloud, CloudMDE 2014, Co-located with the 17th International Conference on Model
Driven Engineering Languages and Systems, MoDELS 2014, volume 1242, pages 66–75.
CEUR-WS, 2014.

21. V. R. Basil and A. J. Turner. Iterative enhancement: A practical technique for software

development. IEEE Transactions on Software Engineering, (4):390–396, 1975.

22. G. Bavota, R. Oliveto, M. Gethers, D. Poshyvanyk, and A. D. Lucia. Methodbook:
Recommending move method refactorings via relational topic models. IEEE Transactions
on Software Engineering, 40(7):671–694, 2014.

23. J. Bayer and D. Muthig. A view-based approach for improving software documentation
practices. In 13th Annual IEEE International Symposium and Workshop on Engineering
of Computer-Based Systems (ECBS’06), pages 10–pp. IEEE, 2006.

24. K. Beck, M. Beedle, A. Van Bennekum, A. Cockburn, W. Cunningham, M. Fowler,
J. Grenning, J. Highsmith, A. Hunt, R. Jeﬀries, et al. Manifesto for agile software
development. 2001.

25. P. Beynon-Davies, C. Carne, H. Mackay, and D. Tudhope. Rapid application development
(rad): an empirical review. European Journal of Information Systems, 8(3):211–223,
1999.

26. J. M. Bhat, M. Gupta, and S. N. Murthy. Overcoming requirements engineering chal-

lenges: Lessons from oﬀshore outsourcing. IEEE software, 23(5):38–44, 2006.

27. D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of Machine

Learning Research, 3(4-5):993–1022, 2003.

28. G. Botterweck. A model-driven approach to the engineering of multiple user interfaces. In
International Conference on Model Driven Engineering Languages and Systems, pages
106–115. Springer, 2006.

29. M. Brambilla, J. Cabot, and M. Wimmer. Model-driven software engineering in practice.

Synthesis lectures on software engineering, 3(1):1–207, 2017.

30. M. Brambilla, E. Umuhoza, and R. Acerbis. Model-driven development of user interfaces
for iot systems via domain-speciﬁc components and patterns. Journal of Internet Services
and Applications, 8(1):1–21, 2017.

31. M. M. Burnett and D. W. McIntyre. Visual programming. COMPUTER-LOS

ALAMITOS-, 28:14–14, 1995.

32. P. Chakraborty, R. Shahriyar, A. Iqbal, and G. Uddin. How do developers discuss and
support new programming languages in technical q&a site? an empirical study of go,
swift, and rust in stack overﬂow. Information and Software Technology (IST), page 19,
2021.

33. T.-H. Chen, S. W. Thomas, M. Nagappan, and A. E. Hassan. Explaining software defects
using topic models. In 9th working conference on mining software repositories, pages
189–198, 2012.

34. T.-H. P. Chen, S. W. Thomas, and A. E. Hassan. A survey on the use of topic models
when mining software repositories. Empirical Software Engineering, 21(5):1843–1919,
2016.

35. B. Cleary, C. Exton, J. Buckley, and M. English. An empirical analysis of information re-
trieval based concept location techniques in software comprehension. Empirical Software
Engineering, 14:93–130, 2009.

36. M. F. Costabile, D. Fogli, P. Mussio, and A. Piccinno. Visual interactive systems for end-
user development: a model-based design methodology. IEEE transactions on systems,
man, and cybernetics-part a: systems and humans, 37(6):1029–1046, 2007.

37. D. Dahlberg. Developer experience of a low-code platform: An exploratory study, 2020.
38. A. De Lucia, M. Di Penta, R. Oliveto, A. Panichella, and S. Panichella. Labeling source
code with information retrieval methods: an empirical study. Empirical Software Engi-
neering, 19(5):1383–1420, 2014.

56

Alamin et al.

39. C. Di Sipio, D. Di Ruscio, and P. T. Nguyen. Democratizing the development of recom-
mender systems by means of low-code platforms. In Proceedings of the 23rd ACM/IEEE
International Conference on Model Driven Engineering Languages and Systems: Com-
panion Proceedings, pages 1–9, 2020.

40. S. Exchange. Stack exchange data dump . Available: https://archive.org/details/

stackexchange, 2020. [Online; accessed 5-January-2022].

41. S. Fincher and J. Tenenberg. Making sense of card sorting data. Expert Systems, 22(3):89–

93, 2005.

42. G. Fischer, E. Giaccardi, Y. Ye, A. G. Sutcliﬀe, and N. Mehandjiev. Meta-design: a

manifesto for end-user development. Communications of the ACM, 47(9):33–37, 2004.

43. N. Fors. The Design and Implementation of Bloqqi-A Feature-Based Diagram Program-

ming Language. PhD thesis, Lund University, 2016.

44. M. Fryling. Low code app development. J. Comput. Sci. Coll., 34(6):119, Apr. 2019.
45. Enterprise Low-Code Application Platforms (LCAP) Reviews and Ratings. Avail-
able: https://www.gartner.com/reviews/market/enterprise-low-code-application-
platform. [Online; accessed 5-January-2022].

46. AppSheet, Low-code application development. Available: https://www.appsheet.com.

[Online; accessed 13-December-2021].

47. Google App Maker platform overview. Available: https://developers.google.com/

appmaker. [Online; accessed 5-January-2022].

48. Google App Maker will be shut down on January 19, 2021. https://workspaceupdates.

googleblog.com/2020/01/app-maker-update.html. [Online; accessed 5-January-2022].

49. D. C. Halbert. Programming by example. PhD thesis, University of California, Berkeley,

1984.

50. J. Han, E. Shihab, Z. Wan, S. Deng, and X. Xia. What do programmers discuss about
deep learning frameworks. Empirical Software Engineering, 25(4):2694–2747, 2020.

51. Amazon Honeycode platform overview. Available: https://www.honeycode.aws/.

[On-

line; accessed 5-January-2022].

52. J. Hu, X. Sun, D. Lo, and B. Li. Modeling the evolution of development topics using
dynamic topic models. In IEEE 22nd International Conference on Software Analysis,
Evolution, and Reengineering, pages 3–12, 2015.

53. F. Ihirwe, D. Di Ruscio, S. Mazzini, P. Pierini, and A. Pierantonio. Low-code engineering
for internet of things: A state of research. In Proceedings of the 23rd ACM/IEEE Inter-
national Conference on Model Driven Engineering Languages and Systems: Companion
Proceedings, pages 1–8, 2020.

54. F. Ihirwe, D. Di Ruscio, S. Mazzini, P. Pierini, and A. Pierantonio. Low-code engineering
for internet of things: A state of research. In Proceedings of the 23rd ACM/IEEE Inter-
national Conference on Model Driven Engineering Languages and Systems: Companion
Proceedings, MODELS ’20, New York, NY, USA, 2020. Association for Computing Ma-
chinery.

55. A. Jacinto, M. Louren¸co, and C. Ferreira. Test mocks for low-code applications built
with outsystems. In Proceedings of the 23rd ACM/IEEE International Conference on
Model Driven Engineering Languages and Systems: Companion Proceedings, pages 1–5,
2020.

56. M. G. Kendall. A new measure of rank correlation. Biometrika, 30(1):81–93, 1938.
57. J. Y. Khan, M. T. I. Khondaker, G. Uddin, and A. Iqbal. Automatic detection of ﬁve
api documentation smells: Practitioners’ perspectives. In 2021 IEEE International Con-
ference on Software Analysis, Evolution and Reengineering (SANER), pages 318–329.
IEEE, 2021.

58. J. Y. Khan, M. T. I. Khondaker, G. Uddin, and A. Iqbal. Automatic detection of ﬁve api
documentation smells: Practitioners’ perspectives. In IEEE International Conference on
Software Analysis, Evolution and Reengineering (SANER), page 12, 2021.

59. F. Khorram, J.-M. Mottu, and G. Suny´e. Challenges & opportunities in low-code test-
ing. In Proceedings of the 23rd ACM/IEEE International Conference on Model Driven
Engineering Languages and Systems: Companion Proceedings, MODELS ’20, New York,
NY, USA, 2020. Association for Computing Machinery.

60. P. Kourouklidis, D. Kolovos, N. Matragkas, and J. Noppen. Towards a low-code so-
lution for monitoring machine learning model performance. In Proceedings of the 23rd
ACM/IEEE International Conference on Model Driven Engineering Languages and Sys-
tems: Companion Proceedings, pages 1–8, 2020.

Topics of Developer Discussions of the LCSD Platforms

57

61. W. H. Kruskal. Historical notes on the wilcoxon unpaired two-sample test. Journal of

the American Statistical Association, 52(279):356–360, 1957.

62. T. C. Lethbridge. Low-code is often high-code, so we must design low-code platforms to
enable proper software engineering. In International Symposium on Leveraging Applica-
tions of Formal Methods, pages 202–212. Springer, 2021.

63. H. Li, T.-H. P. Chen, W. Shang, and A. E. Hassan. Studying software logging using topic

models. Empirical Software Engineering, 23:2655–2694, 2018.

64. G. Lin, S. Wen, Q.-L. Han, J. Zhang, and Y. Xiang. Software vulnerability detection
using deep neural networks: a survey. Proceedings of the IEEE, 108(10):1825–1848, 2020.
65. M. Linares-V´asquez, B. Dit, and D. Poshyvanyk. An exploratory analysis of mobile
development issues using stack overﬂow. In 2013 10th Working Conference on Mining
Software Repositories (MSR), pages 93–96. IEEE, 2013.

66. E. Loper and S. Bird. Nltk: the natural language toolkit. arXiv preprint cs/0205028,

2002.

67. IBM Lotus software. Available: https://help.hcltechsw.com/.

[Online; accessed 5-

January-2022].

68. Low-code development platform . Available: https://en.wikipedia.org/wiki/Low-

code_development_platform. [Online; accessed 5-January-2022].

69. Y. Luo, P. Liang, C. Wang, M. Shahin, and J. Zhan. Characteristics and challenges
of low-code development: The practitioners’ perspective.
In Proceedings of the 15th
ACM/IEEE International Symposium on Empirical Software Engineering and Measure-
ment (ESEM), pages 1–11, 2021.

70. A. K. McCallum. Mallet: A machine learning for language toolkit. http://mallet. cs.

umass. edu, 2002.

71. M. L. McHugh. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276–

282, 2012.

72. Mendix platform overview. Available: https://www.mendix.com/.

[Online; accessed 5-

January-2022].

73. M. Mernik, J. Heering, and A. M. Sloane. When and how to develop domain-speciﬁc

languages. ACM computing surveys (CSUR), 37(4):316–344, 2005.

74. Microsoft Power FX. Available: https://docs.microsoft.com/en-us/power-platform/

power-fx/overview. [Online; accessed 13-December-2021].

75. B. A. Myers, A. J. Ko, and M. M. Burnett.

Invited research overview: end-user pro-
gramming. In CHI’06 extended abstracts on Human factors in computing systems, pages
75–80, 2006.

76. C. Ness and M. E. Hansen. Potential of low-code in the healthcare sector: an exploratory
study of the potential of low-code development in the healthcare sector in norway. Mas-
ter’s thesis, 2019.

77. OneBlink platform overview. Available: https://www.oneblink.io/.

[Online; accessed

5-January-2022].

78. M. Overeem and S. Jansen. Proposing a framework for impact analysis for low-code devel-
opment platforms. In 2021 ACM/IEEE International Conference on Model Driven En-
gineering Languages and Systems Companion (MODELS-C), pages 88–97. IEEE, 2021.
79. S. Overﬂow. Stack Overﬂow Questions. https://stackoverflow.com/questions/, 2020.

Last accessed on 14 November 2020.

80. Programming Gains Speed As Developers Turn to Low-Code During the Pan-
demic.
Available: https://www.designnews.com/automation/programming-gains-
speed-developers-turn-low-code-during-pandemic. [Online; accessed 5-August-2022].
81. J. Pane and B. Myers. More Natural Programming Languages and Environments, pages

31–50. Springer, 10 2006.

82. F. Patern`o. End user development: Survey of an emerging ﬁeld for empowering people.

International Scholarly Research Notices, 2013, 2013.

83. The Best Low-Code Development Platforms. Available: https://www.pcmag.com/picks/

the-best-low-code-development-platforms. [Online; accessed 5-January-2022].

84. V. S. Phalake and S. D. Joshi. Low code development platform for digital transforma-
tion. In Information and Communication Technology for Competitive Strategies (ICTCS
2020), pages 689–697. Springer, 2021.

85. A. Pleuss, S. Wollny, and G. Botterweck. Model-driven development and evolution of
customized user interfaces. In Proceedings of the 5th ACM SIGCHI symposium on En-
gineering interactive computing systems, pages 13–22, 2013.

58

Alamin et al.

86. L. Ponzanelli, A. Mocci, A. Bacchelli, M. Lanza, and D. Fullerton. Improving low qual-
ity stack overﬂow post detection. In 2014 IEEE international conference on software
maintenance and evolution, pages 541–544. IEEE, 2014.

87. D. Poshyvanyk, Y.-G. Gu´eh´eneuc, A. Marcus, G. Antoniol, and V. T. Rajlich. Fea-
ture location using probabilistic ranking of methods based on execution scenarios and
information retrieval. IEEE Transactions on Software Engineering, 33(6):420–432, 2007.
88. Microsoft power apps platform overview. Available: https://powerapps.microsoft.com/

en-us/. [Online; accessed 5-January-2022].

89. Quickbase platform overview.

Available: https://www.quickbase.com/product/

product-overview. [Online; accessed 5-January-2022].

90. C. Ramasubramanian and R. Ramya. Eﬀective pre-processing activities in text min-
International Journal of Advanced

ing using improved porter’s stemming algorithm.
Research in Computer and Communication Engineering, 2(12):4536–4538, 2013.

91. S. Rao and A. C. Kak. Retrieval from software libraries for bug localization: a comparative
In 8th Working Conference on Mining

study of generic and composite text models.
Software Repositories, page 43–52, 2011.

92. R. Rehurek and P. Sojka. Software framework for topic modelling with large corpora. In
In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks.
Citeseer, 2010.

93. X. Ren, Z. Xing, X. Xia, G. Li, and J. Sun. Discovering, explaining and summarizing
controversial discussions in community q&a sites. In 2019 34th IEEE/ACM International
Conference on Automated Software Engineering (ASE), pages 151–162. IEEE, 2019.
94. M. Resnick, J. Maloney, A. Monroy-Hern´andez, N. Rusk, E. Eastmond, K. Brennan,
A. Millner, E. Rosenbaum, J. Silver, B. Silverman, et al. Scratch: programming for all.
Communications of the ACM, 52(11):60–67, 2009.

95. M. P. Robillard, E. Bodden, D. Kawrykow, M. Mezini, and T. Ratchford. Automated
API property inference techniques. IEEE Transactions on Software Engineering, page 28,
2012.

96. M. R¨oder, A. Both, and A. Hinneburg. Exploring the space of topic coherence measures.
In Proceedings of the eighth ACM international conference on Web search and data
mining, pages 399–408, 2015.

97. C. Rosen and E. Shihab. What are mobile developers asking about? a large scale study

using stack overﬂow. Empirical Software Engineering, page 33, 2015.

98. C. Rosen and E. Shihab. What are mobile developers asking about? a large scale study

using stack overﬂow. Empirical Software Engineering, 21(3):1192–1223, 2016.

99. J. R. Rymer, R. Koplowitz, and S. A. Leaders. The forrester wave(tm) low-code devel-

opment platforms for ad&d professionals, q1 2019. 2019.

100. A. Sahay, A. Indamutsa, D. Di Ruscio, and A. Pierantonio. Supporting the understanding
and comparison of low-code development platforms. In 2020 46th Euromicro Conference
on Software Engineering and Advanced Applications (SEAA), pages 171–178. IEEE,
2020.

101. Salesforce platform overview. Available: https://www.salesforce.com/in/?ir=1.

[On-

line; accessed 5-January-2022].

102. G. Sinha, R. Shahi, and M. Shankar. Human computer interaction. In 2010 3rd Interna-
tional Conference on Emerging Trends in Engineering and Technology, pages 1–4. IEEE,
2010.

103. X. Sun, B. Li, H. Leung, B. Li, and Y. Li. Msr4sm: Using topic models to eﬀectively
mining software repositories for software maintenance tasks. Information and Software
Technology, 66:671–694, 2015.

104. X. Sun, B. Li, Y. Li, and Y. Chen. What information in software historical repositories
do we need to support software maintenance tasks? an approach based on topic model.
Computer and Information Science, pages 22–37, 2015.

105. X. Sun, X. Liu, B. Li, Y. Duan, H. Yang, and J. Hu. Exploring topic models in software
engineering data analysis: A survey. In 17th IEEE/ACIS International Conference on
Software Engineering, Artiﬁcial Intelligence, Networking and Parallel/Distributed Com-
puting, pages 357–362, 2016.

106. S. W. Thomas, B. Adams, A. E. Hassan, and D. Blostein. Modeling the evolution of topics
in source code histories. In 8th working conference on mining software repositories, pages
173–182, 2011.

Topics of Developer Discussions of the LCSD Platforms

59

107. S. W. Thomas, B. Adams, A. E. Hassan, and D. Blostein. Studying software evolution

using topic models. Science of Computer Programming, 80(B):457–479, 2014.

108. K. Tian, M. Revelle, and D. Poshyvanyk. Using latent dirichlet allocation for automatic
categorization of software. In 6th international working conference on mining software
repositories, pages 163–166, 2009.

109. C. Torres. Demand for programmers hits full boil as us job market simmers. Bloomberg.

Com, 2018.

110. How many Low-Code/No-Code platforms are out there? Available: https://www.
spreadsheetweb.com/how-many-low-code-no-code-platforms-are-out-there/.
[On-
line; accessed 5-August-2022].

111. C. Treude, O. Barzilay, and M.-A. Storey. How do programmers ask and answer questions
on the web?(nier track). In Proceedings of the 33rd international conference on software
engineering, pages 804–807, 2011.

112. G. Uddin, O. Baysal, L. Guerroj, and F. Khomh. Understanding how and why developers
IEEE Transactions on Software Engineering,

seek and analyze api related opinions.
page 40, 2019.

113. G. Uddin and F. Khomh. Automatic summarization of api reviews.

In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE), pages
159–170. IEEE, 2017.

114. G. Uddin and F. Khomh. Automatic summarization of API reviews.

In Proc. 32nd
IEEE/ACM International Conference on Automated Software Engineering, page 12,
2017.

115. G. Uddin and F. Khomh. Opiner: A search and summarization engine for API reviews. In
Proc. 32nd IEEE/ACM International Conference on Automated Software Engineering,
page 6, 2017.

116. G. Uddin and F. Khomh. Automatic opinion mining from API reviews from stack over-

ﬂow. IEEE Transactions on Software Engineering, page 35, 2019.

117. G. Uddin, F. Khomh, and C. K. Roy. Automatic api usage scenario documentation
from technical q&a sites. ACM Transactions on Software Engineering and Methodology,
page 43, 2020.

118. G. Uddin, F. Khomh, and C. K. Roy. Automatic mining of api usage scenarios from

stack overﬂow. Information and Software Technology (IST), page 16, 2020.

119. G. Uddin, F. Khomh, and C. K. Roy. Automatic api usage scenario documentation
from technical q&a sites. ACM Transactions on Software Engineering and Methodology
(TOSEM), 30(3):1–45, 2021.

120. G. Uddin and M. P. Robillard. How api documentation fails. Ieee software, 32(4):68–75,

2015.

121. G. Uddin and M. P. Robillard. How api documentation fails. IEEE Softawre, 32(4):76–83,

2015.

122. G. Uddin, F. Sabir, Y.-G. Gu´eh´eneuc, O. Alam, and F. Khomh. An empirical study of
iot topics in iot developer discussions on stack overﬂow. Empirical Software Engineering,
26, 11 2021.

123. B. Ur, E. McManus, M. Pak Yong Ho, and M. L. Littman. Practical trigger-action
programming in the smart home. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, pages 803–812, 2014.

124. A. Van Deursen, P. Klint, and J. Visser. Domain-speciﬁc languages: An annotated bib-

liography. ACM Sigplan Notices, 35(6):26–36, 2000.

125. P. Vincent, K. Lijima, M. Driver, J. Wong, and Y. Natis. Magic quadrant for enterprise

low-code application platforms. Retrieved December, 18:2019, 2019.

126. Vinyl platform overview. Available: https://zudy.com/.

[Online; accessed 5-January-

2022].

127. Z. Wan, X. Xia, and A. E. Hassan. What is discussed about blockchain? a case study
on the use of balanced lda and the reference architecture of a domain to capture online
discussions about blockchain platforms across the stack exchange communities. IEEE
Transactions on Software Engineering, 2019.

128. R. Waszkowski. Low-code platform for automating business processes in manufacturing.

IFAC-PapersOnLine, 52:376–381, 01 2019.

129. D. Wolber. App inventor and real-world motivation. In Proceedings of the 42nd ACM

technical symposium on Computer science education, pages 601–606, 2011.

60

Alamin et al.

130. J. Wong, M. Driver, and P. Vincent. Low-code development technologies evaluation

guide, 2019.

131. M. Woo. The rise of no/low code software development—no experience needed? Engi-

neering (Beijing, China), 6(9):960, 2020.

132. D. Yang, A. Hussain, and C. V. Lopes. From query to usable code: an analysis of
stack overﬂow code snippets. In 2016 IEEE/ACM 13th Working Conference on Mining
Software Repositories (MSR), pages 391–401. IEEE, 2016.

133. X.-L. Yang, D. Lo, X. Xia, Z.-Y. Wan, and J.-L. Sun. What security questions do
developers ask? a large-scale study of stack overﬂow posts. Journal of Computer Science
and Technology, 31(5):910–924, 2016.

134. L. Zhu, L. Bass, and G. Champlin-Scharﬀ. Devops and its practices. IEEE Software,

33(3):32–34, 2016.

135. W. Zhuang, X. Gan, Y. Wen, and S. Zhang. Easyﬂ: A low-code federated learning

platform for dummies. arXiv preprint arXiv:2105.07603, 2021.

136. Zoho Creator platform overview. Available: https://www.zoho.com/creator/.

[Online;

accessed 5-January-2022].

