2
2
0
2

l
u
J

9
2

]

M
M

.
s
c
[

1
v
4
3
5
4
1
.
7
0
2
2
:
v
i
X
r
a

ACM Multimedia Grand Challenge on Detecting Cheapfakes

Shivangi Aneja
Technical University of Munich

Cise Midoglu
SimulaMet

Duc-Tien Dang-Nguyen
University of Bergen
Kristiania University College

Sohail Ahmed Khan
University of Bergen

Michael Riegler
SimulaMet

P√•l Halvorsen
SimulaMet

Chris Bregler
Google AI

Balu Adsumilli
YouTube

Figure 1: Deepfakes (left) are defined as falsified media created using sophisticated AI-based media manipulation tools and
techniques. Cheapfakes (right) include falsified media created with/without contemporary non-AI based editing tools which
are easily accessible. Photoshopping tools can be used to tamper with images. Videos can be sped up or slowed down to change
the intent or misrepresent the person in the video. Re-contextualizing includes associating falsified or unrelated claims with a
genuine image to misrepresent events or persons. This challenge is focused on detecting re-contextualized cheapfakes. Image
sources: [6, 12, 13, 22, 27]

ABSTRACT
Cheapfake is a recently coined term that encompasses non-AI
(‚Äúcheap‚Äù) manipulations of multimedia content. Cheapfakes are
known to be more prevalent than deepfakes. Cheapfake media can
be created using editing software for image/video manipulations, or
even without using any software, by simply altering the context of
an image/video by sharing the media alongside misleading claims.
This alteration of context is referred to as out-of-context (OOC)
misuse of media. OOC media is much harder to detect than fake
media, since the images and videos are not tampered. In this chal-
lenge, we focus on detecting OOC images, and more specifically
the misuse of real photographs with conflicting image captions in
news items. The aim of this challenge is to develop and benchmark
models that can be used to detect whether given samples (news
image and associated captions) are OOC, based on the recently
compiled COSMOS dataset.

KEYWORDS
cheapfakes, misinformation, news, out-of-context misuse,
re-contextualized media

1 INTRODUCTION
The last decade has seen a surge in the use of social media platforms
as a means of consuming news. In a recent study [25], Forbes reports
that social media giant Facebook leads this trend with 36% of its
customers using the platform for consuming news. Social media
platforms come with a freedom for users to upload and share posts,
which has led to the proliferation of fake media on these platforms.
Fake media (including audio, images, videos, and text) circu-
lated on social media platforms can be broadly grouped into two
major categories: deepfakes and cheapfakes, as shown in Figure 1.
Deepfakes are falsified media, most commonly facial videos cre-
ated using sophisticated AI-based media manipulation tools and
techniques. Several deepfake detection methods [1, 2, 5, 10, 11, 15,
16, 20, 22, 24, 28] are in place to monitor and regulate the spread
of deepfake videos. Cheapfake is a recently coined general term

1

 
 
 
 
 
 
ACMMM‚Äô22, Lisbon, Portugal

Aneja et al.

Figure 2: Each image in the dataset is accompanied by one or two captions that the image was circulated together with on
the Internet. On the left, one of the two captions is misleading with an alteration of context, indicating out-of-context (OOC)
misuse. On the right, none of the two captions are misleading, hence not-out-of-context (NOOC). Image source: [3]

that encompasses non-AI (‚Äúcheap‚Äù) manipulations of multimedia
content, created without using deep learning methods. Although a
lot of attention has been paid to the creation, detection, and misuse
of deepfakes in the last years, cheapfakes are actually known to be
more prevalent than deepfakes [7, 18, 23].

Cheapfakes are created with or without contemporary editing
tools which are non-AI based and are easily accessible. Image ma-
nipulations, speeding/slowing of videos, and deliberate alteration
of the context of the multimedia asset in, e.g., news captions, by
sharing the media alongside misleading claims, are some of the
methods that are currently in use (see Figure 1). The latter is re-
ferred to as out-of-context (OOC) misuse of media. OOC media are
much harder to detect than fake media, since the images and/or
videos are not tampered. We refer readers to the report by Paris et
al. [21] for an overview of different types of cheapfakes surfacing
the Internet.

Depending on the type of cheapfakes, different detection tools
can be used. Methods to detect image manipulations such as photo-
shopping and image splicing have been investigated [8, 9, 14, 26].
Re-contextualization or OOC misuse, which include associating
falsified or unrelated claims with a genuine image in order to mis-
represent events or persons is, however, relatively niche and unex-
plored. Very recently, Aneja et al. [3] introduced this task, provided
a dataset of real-world news posts called COSMOS, and proposed a
method for detecting cheapfakes, which was benchmarked using
the COSMOS dataset.

In this challenge, we focus on detecting OOC images, and more
specifically the misuse of real photographs with conflicting image
captions, in news items. The aim of this challenge is to develop
and benchmark models that can be used to detect whether given
samples (news image and associated captions) are OOC, based on a
version of the COSMOS dataset.

2 CHALLENGE TASKS
Task 1: Identification of Conflicting
Image-Caption Triplets
An image serves as evidence of the event described by a news
caption. If two captions associated with an image are valid, then
they should describe the same event. If they align with the same
object(s) in the image, then they should be broadly conveying the
same information. Based on these patterns, we define out-of-context
(OOC) use of an image as presenting the image as an evidence of
untrue and/or unrelated event(s). If the two captions refer to same
object(s) in the image, but are semantically different, i.e., associate
the same subject to different events, this indicates OOC use of
the image. However, if the captions correspond to the same event,
irrespective of the object(s) the captions describe, this is defined as
not-out-of-context (NOOC) use of the image. See Figure 2 for more
details.

In this task, the participants are asked to come up with methods
to detect conflicting image-caption triplets, which indicates miscon-
textualization. More specifically, given <Image,Caption1,Caption2>
triplets as input, the proposed model should predict corresponding
class labels (OOC or NOOC). The end goal for this task is not to
identify which of the two captions is true/false, but rather to de-
tect the existence of miscontextualization. This kind of a setup is
considered particularly useful for assisting fact checkers, as high-
lighting conflicting image-caption triplets allows them to narrow
down their search space.

Task 2: Fake Caption Detection
A NOOC scenario from Task 1 makes no conclusions regarding the
veracity of the statements. In a practical scenario, multiple captions
might not be available for a given image. In such a scenario, the
task boils down to figuring out whether a given caption linked to
the image is genuine or not. We argue that this is a challenging
task, even for human moderators, without prior knowledge about

2

ACM Multimedia Grand Challenge on Detecting Cheapfakes

ACMMM‚Äô22, Lisbon, Portugal

the image origin. Luo et al. [17] verified this claim with a study on
human evaluators who were instructed not to use search engines,
where the average human accuracy was around 65%.

In this task, the participants are asked to come up with methods
to determine whether a given <Image,Caption> pair is genuine
(real) or falsely generated (fake). Since our dataset only contains
real, non-photoshopped images, it is suitable for a practical use
case and challenging at the same time.

3 DATASET
Aneja et al. [3] have created COSMOS, a large-scale dataset of
around 200ùêæ images which have been matched with 450ùêæ textual
captions from different news websites, blogs, and social media posts.
Figure 3 presents the category distribution of the images in the
dataset, which were collected from a wide variety of articles with a
special focus on topics where misinformation spread is prominent.

Figure 3: Distribution of the images in the COSMOS dataset
per category.

For this challenge, a part of the COSMOS dataset [3] is sampled
and assigned as the public dataset. The public dataset, consisting of
the training, validation and public test splits, is provided openly to
participants for training and testing their algorithms. The remaining
part of the COSMOS dataset is augmented with new samples and
modified to create the hidden test splits, which are not made publicly
available, and will be used by the challenge organizers to evaluate
the submissions. Details of the challenge dataset are summarized
in Table 1.

Table 1: Challenge dataset statistics.

Task

Split

# Images

# Captions Context An-

‚Ä¢ img_local_path: Source path for the image in the dataset

directory.

‚Ä¢ articles: List of dictionaries containing metadata for every

caption associated with the image.

‚Ä¢ caption: Original caption scraped from the news website.
‚Ä¢ article_url: Link to the website from where the image

and caption were scraped.

‚Ä¢ caption_modified: Modified caption after applying Spacy
NER2. Authors in [3] use this caption as an input to their
model during experiments.

‚Ä¢ entity_list: List of mappings between the modified named
entities in caption with the corresponding hypernyms.
‚Ä¢ maskrcnn_bboxes: List of detected bounding boxes corre-
sponding to the image. (x1,y1) refers to the start vertex of the
rectangle and (x2, y2) refers to end vertex of the rectangle.
Note that for detecting bounding boxes, the authors in [3]
use the Detectron2 pretrained model3 available under4. They
detect up to 10 bounding boxes per image.

3.2 Test Splits
We provide separate test splits for evaluating the performance of
the two tasks.

Task 1 test split: The public test split is provided as a JSON
formatted text file called test.json, and has the structure described
in3.1. The hidden test split is structurally identical to the public test
split. The attributes in test.json are as follows.

‚Ä¢ img_local_path: Source path for the image in the dataset

directory.

‚Ä¢ caption1: First caption associated with the image.
‚Ä¢ caption1_modified: Modified caption1 after applying Spacy

NER.

‚Ä¢ caption1_entities: List of mappings between the mod-
ified named entities in caption1 with the corresponding
hypernyms.

‚Ä¢ caption2: Second caption associated with the image.
‚Ä¢ caption2_modified: Modified caption2 after applying Spacy

NER.

‚Ä¢ caption2_entities: List of mappings between the mod-
ified named entities in caption2 with the corresponding
hypernyms.

‚Ä¢ article_url: Link to the website from where the image

notation

and caption were scraped.

Training
Validation
Public Test
Public Test

161752
41006
1000
100

Task 1
Task 2

360749 No
90036 No
Yes
2000
Yes
100

3.1 Training and Validation Splits
We provide common training and validation splits for both chal-
lenge tasks. The training and validation splits are provided as
JavaScript Object Notation (JSON) formatted text files called train.json
and val.json, where each data sample is stored as a dictionary 1.
The attributes in train.json and val.json are as follows.

1https://github.com/shivangi-aneja/COSMOS

3

‚Ä¢ label: Class label indicating whether the two captions are
out-of-context with respect to the image, where 1=out-of-
context (OOC) and 0=not-out-of-context (NOOC).

‚Ä¢ maskrcnn_bboxes: List of detected bounding boxes corre-
sponding to the image. (x1,y1) refers to the start vertex of
the rectangle and (x2, y2) refers to the end vertex of the
rectangle.

2https://spacy.io/api/entityrecognizer
3https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md
4https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-
Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml

ACMMM‚Äô22, Lisbon, Portugal

Aneja et al.

Task 2 test split: The public test split is provided as a JSON
formatted text file called task_2.json. The hidden test split is struc-
turally identical to the public test split. The attributes in task_2.json
are as follows.

‚Ä¢ img_local_path: Source path for the image in the dataset

directory.

‚Ä¢ caption: Text caption associated with the image.
‚Ä¢ genuine: Classification label, where 0=out-of-context (OOC)

and 1=not-out-of-context (NOOC).

4 EVALUATION CRITERIA
There are two considerations in the fulfillment of the above de-
scribed tasks. Participant models will be evaluated and ranked ac-
cording to two aggregate scores representing these considerations,
each composed of 5 and 3 metrics respectively.

Effectiveness: The first goal is to achieve high detection perfor-
mance. This speaks to effectiveness, which will be evaluated based
on accuracy, precision, recall, F1-score, and Matthews correlation
coefficient (MCC). Participants are asked to calculate these 5 metrics
for their model and include the values in their submission.

Efficiency: In certain scenarios, having an idea about the poten-
tial misuse of images in real-time and with minimal resources can
be more important than the detection performance itself. We take
this aspect into consideration by introducing an additional goal:
having low latency and low complexity. This speaks to efficiency,
which will be evaluated based on latency5, number of parameters6,
and model size7. Participants are asked to calculate these 3 metrics
for their model and include the values in their submission.

5 ADMINISTRATIVE DETAILS
A challenge website8 has been setup with a commitment to be main-
tained at least for the next 3 years. Along with the official website,
a Google Group9 and a GitHub repository10 have been established
to support prospective participants. Interested participants can find
the previously asked questions and join interactive discussions on
these platforms.

6 CONCLUSION AND OUTLOOK
The ACM Multimedia Grand Challenge on Detecting Cheapfakes
addresses the relatively new but prominent problem of detecting
cheapfake multimedia content in news. More specifically, it focuses
on the OOC misuse of images in news items. As emphasized in
Section 1, this is a relatively novel area of research, in comparison to
deepfakes, and is distinguishable from the wider field of fake news
in the following way: the term ‚Äúfake news‚Äù traditionally refers to
the use of either fake multimedia content or false captions, whereas
OOC misuse refers to the scenario where the multimedia content
in the news item (images in this case) is decidedly not fake, and
the captions may or may not be false. Rather, the misinformation
results from the OOC combination of the two.

5Average, i.e., arithmetic mean of the runtime per sample, calculated over all samples
in the public test split (ms).
6Number of trainable parameters in the model (million).
7Storage size for the model (MB).
8https://detecting-cheapfakes.github.io/
9https://groups.google.com/g/grandchallenge-cheapfakes
10https://github.com/detecting-cheapfakes/detecting-cheapfakes-code

4

With this challenge, we firstly aim to motivate researchers to
develop different methods for addressing this particular problem,
and to benchmark a number of proposed models for detecting the
OOC misuse of real photographs in news items. Secondly, through
the dissection and analysis of the COSMOS dataset, we aim to
encourage the in-depth understanding, and later generation of
(further), supervised datasets for the above described challenges.
Algorithmic benchmarking is an efficient approach to analyze the
results from different detection methods, but on top of evaluating
the models themselves, the comparison of different approaches can
help identify the potentials as well as the shortcomings of existing
open datasets.

There is a growing interest from the scientific community to-
wards addressing the problem of misinformation in general, and
towards the detection of deepfakes in particular. We hope that
this challenge can increase awareness regarding the prominence of
cheapfakes as well, and that in the future, the methods presented
within this context can evolve into systems that support researchers,
regulatory bodies, news consumers, and the general public in their
search for a safe and truthful information ecosystem.

The first edition of this challenge has been organized within the
ACM Multimedia Systems (MMSys) Conference as the ‚ÄúMMSys‚Äô21
Grand Challenge on Detecting Cheapfakes‚Äù [4, 19]. The challenge
will be sustained over the next years, as we believe that it is specific
enough to be relevant and timely, as well as generic enough to
evolve over time according to different needs. For instance, with
the rising importance of synthetic data for research and training
purposes, a possible future task could be the generation of fake
captions.

REFERENCES
[1] D. Afchar, V. Nozick, J. Yamagishi, and I. Echizen. 2018. MesoNet: a Compact Facial

Video Forgery Detection Network. https://doi.org/10.1109/wifs.2018.8630761

[2] S. Agarwal, H. Farid, Y. Gu, M. He, K. Nagano, and H. Li. 2019. Protecting World

Leaders Against Deep Fakes. In CVPR Workshops.

[3] S. Aneja, C. Bregler, and M. Nie√üner. 2021. COSMOS: Catching Out-of-Context
Misinformation with Self-Supervised Learning. arXiv:2101.06278 [cs.CV]
[4] S. Aneja, C. Midoglu, D. Dang-Nguyen, M. A. Riegler, P. Halvorsen, Ma. Niessner,
B. Adsumilli, and C. Bregler. 2021. MMSys‚Äô21 Grand Challenge on Detecting
Cheapfakes. arXiv:2107.05297 [cs.MM]

[5] S. Aneja and M. Nie√üner. 2020. Generalized Zero and Few-Shot Transfer for

Facial Forgery Detection. arXiv:2006.11863 [cs.CV]

[6] Boredpanda. 2019. 30 Fake Viral Photos People Believed Were Real. https://www.

boredpanda.com/fake-news-photos-viral-photoshop/

[7] J. S. Brennen, F. M. Simon, P. N. Howard, and R. K. Nielsen. 2020. Types, Sources,
and Claims of COVID-19 Misinformation. http://www.primaonline.it/wp-content/
uploads/2020/04/COVID-19_reuters.pdf

[8] C. Chen, S. McCloskey, and J. Yu. 2017. Image Splicing Detection via Camera
Response Function Analysis. 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2017), 1876‚Äì1885.

[9] D. Cozzolino, G. Poggi, and L. Verdoliva. 2015. Splicebuster: A new blind image
splicing detector. 2015 IEEE International Workshop on Information Forensics and
Security (WIFS) (2015), 1‚Äì6.

[10] D. Cozzolino, A. R√∂ssler, J. Thies, M. Nie√üner, and L. Verdoliva. 2020. ID-Reveal:

Identity-aware DeepFake Video Detection. arXiv:2012.02512 [cs.CV]

[11] D. Cozzolino, J. Thies, A. R√∂ssler, C. Riess, M. Nie√üner, and L. Verdoliva. 2018.
Forensictransfer: Weakly-supervised domain adaptation for forgery detection.
arXiv preprint arXiv:1812.02510 (2018).

[12] D. Evon. 2020. Is This Obama, Fauci, and Gates at a Wuhan Lab in 2015? Re-
trieved July 13, 2020 from https://www.snopes.com/fact-check/obama-fauci-
gates-wuhan-lab/

[13] The White House. 2014.
the Vac-
https:
cine Research Center at
//obamawhitehouse.archives.gov/photos-and-video/photo/2014/12/president-
obama-tours-lab-vaccine-research-center-national-institutes

President Obama tours a lab at

Institutes of Health.

the National

ACM Multimedia Grand Challenge on Detecting Cheapfakes

ACMMM‚Äô22, Lisbon, Portugal

[14] M. Huh, A. Liu, A. Owens, and A. A. Efros. 2018. Fighting Fake News: Image
Splice Detection via Learned Self-Consistency. arXiv:1805.04096 [cs.CV]
[15] L. Li, J. Bao, T. Zhang, H. Yang, D. Chen, F. Wen, and B. Guo. 2020. Face x-ray for
more general face forgery detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 5001‚Äì5010.

[16] Y. Li and S. Lyu. 2018. Exposing DeepFake Videos By Detecting Face Warping

Artifacts. arXiv:1811.00656 [cs.CV]

[17] G. Luo, T. Darrell, and A. Rohrbach. 2021. NewsCLIPpings: Automatic Generation

of Out-of-Context Multimodal Media. arXiv:2104.05893 [cs.CV]

[18] Joel Luther. 2022. MediaReview: A next step in solving the misinforma-
tion crisis. https://reporterslab.org/mediareview-a-next-step-in-solving-the-
misinformation-crisis/

[22] A. R√∂ssler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nie√üner.
FaceForensics++: Learning to Detect Manipulated Facial Images.

2019.
arXiv:1901.08971 [cs.CV]

[23] N. Schick. 2020. Don‚Äôt underestimate the cheapfake. Retrieved Dec 22, 2020
from https://www.technologyreview.com/2020/12/22/1015442/cheapfakes-more-
political-damage-2020-election-than-deepfakes/

[24] L. Verdoliva. 2020. Media Forensics and DeepFakes: An Overview. IEEE Journal
of Selected Topics in Signal Processing 14, 5 (2020), 910‚Äì932. https://doi.org/10.
1109/JSTSP.2020.3002101

[25] M. Vorhaus. 2020. People Increasingly Turn To Social Media For News. Re-
trieved June 24, 2020 from https://www.forbes.com/sites/mikevorhaus/2020/06/
24/people-increasingly-turn-to-social-media-for-news/

[19] C. Midoglu and S. Aneja. 2021. Grand Challenge on Detecting Cheapfakes. https:

[26] S. Wang, O. Wang, A. Owens, R. Zhang, and A. A. Efros. 2019. Detecting Photo-

//2021.acmmmsys.org/cheapfake_challenge.php

shopped Faces by Scripting Photoshop. In ICCV.

[20] H. H. Nguyen, J. Yamagishi, and I. Echizen. 2019. Capsule-forensics: Using
Capsule Networks to Detect Forged Images and Videos. https://doi.org/10.1109/
icassp.2019.8682602

[21] B. Paris and J. Donovan. 2019. Deepfakes and cheapfakes: The manipulation of
audio and visual evidence. https://datasociety.net/wp-content/uploads/2019/09/
DataSociety_Deepfakes_Cheap_Fakes.pdf

[27] J. Waterson. 2019. Facebook refuses to delete fake Pelosi video spread by Trump
supporters. https://www.theguardian.com/technology/2019/may/24/facebook-
leaves-fake-nancy-pelosi-video-on-site

[28] X. Yang, Y. Li, and S. Lyu. 2019. Exposing Deep Fakes Using Inconsistent Head
Poses. ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) (2019). https://doi.org/10.1109/icassp.2019.8683164

5

