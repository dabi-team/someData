2
2
0
2

n
u
J

8

]

A
N
.
h
t
a
m

[

1
v
8
6
7
3
0
.
6
0
2
2
:
v
i
X
r
a

Thick-restarted joint Lanczos bidiagonalization for the GSVD∗

Fernando Alvarruiz†

Carmen Campos‡

Jose E. Roman§

June 9, 2022

Abstract

The computation of the partial generalized singular value decomposition (GSVD) of large-
scale matrix pairs can be approached by means of iterative methods based on expanding
subspaces, particularly Krylov subspaces. We consider the joint Lanczos bidiagonalization
method, and analyze the feasibility of adapting the thick restart technique that is being
used successfully in the context of other linear algebra problems. Numerical experiments
illustrate the eﬀectiveness of the proposed method. We also compare the new method with
an alternative solution via equivalent eigenvalue problems, considering accuracy as well as
computational performance. The analysis is done using a parallel implementation in the
SLEPc library.

1 Introduction

The generalized singular value decomposition (GSVD) of two matrices was introduced by Van
Loan [21], with subsequent additional developments by Paige and Saunders [22]. Given two real
is given by
matrices A and B with the same number of columns, the GSVD of the pair

A, B
{

}

U T

A AG = C,

U T

B BG = S,

(1)

where UA, UB are orthogonal matrices, G is a square nonsingular matrix, and C, S are diagonal
matrices in the simplest case. A more precise deﬁnition of the problem will be given in section 2.1.
This factorization, as an extension of the usual singular value decomposition (SVD), is ﬁnding
its way in an increasing number of applications, for instance in the solution of constrained least
squares problems [11], discrete ill-posed problems via Tikhonov regularization [20, 19], as well
as in many other contexts [9].

The problem of computing the GSVD of a small dense matrix pair is well understood, and
a robust implementation is available in LAPACK [1, §2.3.5.3]. However, the case of large sparse
matrix pairs is still the subject of active research. Normally, the computation of the large-scale
GSVD is addressed by means of iterative methods based on expanding subspaces. This is the
case of the Jacobi–Davidson method proposed by Hochstenbach [15]. In this paper, we focus on
Krylov methods, which still need to incorporate a great deal of the knowledge and innovation
that has been successfully applied in similar linear algebra problems such as the SVD. One
example of such is an eﬀective restart mechanism, which is the main focus of this paper. The

∗This work was supported by the Spanish Agencia Estatal de Investigaci´on under grant PID2019-107379RB-I00

/ AEI / 10.13039/501100011033.

†D. Sistemes

Inform`atics

i Computaci´o, Universitat Polit`ecnica de Val`encia, Val`encia,

Spain

(fbermejo@dsic.upv.es).

‡D. Did´actica de la Matem´atica, Universitat de Val`encia, Val`encia, Spain (carmen.campos-gonzalez@uv.es).
§D. Sistemes
Spain

i Computaci´o, Universitat Polit`ecnica de Val`encia, Val`encia,

Inform`atics

(jroman@dsic.upv.es).

1

 
 
 
 
 
 
ultimate goal is to provide reliable software implementations of the methods that are readily
available to users from the ﬁeld of scientiﬁc computing or any other discipline that requires
computing a GSVD. In our case, the developments presented in this paper are included in one
of the solvers of SLEPc, the Scalable Library for Eigenvalue Problem Computations [13].

Zha [27] was the ﬁrst to propose a Lanczos method to be used for the GSVD. His method
relies on a joint bidiagonalization of the two matrices, A and B, in a similar way as Lanczos
methods for the SVD have a single bidiagonalization in their foundation. The joint bidiago-
nalization in Zha’s method results in two small-sized upper bidiagonal matrices. Later, Kilmer
and coauthors [20] proposed a variant in which one of the two bidiagonal matrices generated by
the joint bidiagonalization is lower instead of upper. The latter has since then been the most
popular approach, and we focus mainly on that variant for our developments.

As in any Lanczos method, a ﬁnite-precision arithmetic implementation suﬀers from various
numerical pitfalls, such as loss of orthogonality in the generated Krylov basis vectors. Jia and
Li [18] have studied numerical error in the context of joint bidiagonalization for the GSVD,
showing that loss of orthogonality can be prevented by simply enforcing semiorthogonality of
Lanczos vectors, e.g., with a partial reorthogonalization technique [17], as is done in Lanczos
methods for other linear algebra problems. Either in the case that a semiorthogonality scheme is
pursued, or a full reorthogonalization approach is followed as we do to avoid loss of orthogonality,
a consequence is that all Lanczos vectors must be kept throughout the computation, with the
consequent increase in storage and computational costs.

Thick restart is an eﬀective mechanism to keep the size of the Krylov basis bounded, that
has been applied to Lanczos methods in many diﬀerent contexts such as the symmetric eigen-
problem [26] or the SVD [2, 14]. Compared to explicit restart, the thick restart scheme is much
more eﬀective because it compresses the currently available Krylov subspace into another Krylov
subspace of smaller dimension (not a single vector), that retains the wanted spectral information
while purging the components associated with unwanted eigenvalues (or singular values). The
thick restart methodology can be summarized in two stages.
In the ﬁrst stage, one or more
Lanczos recurrences are used to build one or more sets of Lanczos vectors, in a way that the
small-sized problem resulting from the projection of the original problem retains the properties
of the original problem (structure preservation). For instance, for symmetric-indeﬁnite matrix
pencils it is possible to employ a pseudo-Lanczos recurrence that results in a symmetric-indeﬁnite
projected problem [6]. In the second stage, the built factorization is truncated to a smaller size
decomposition, in a way that it is feasible to extend it again using the same Lanczos recurrences.
In this paper, we work out the details that are needed for thick-restarting the Lanczos
recurrences associated with the joint bidiagonalization for the GSVD, so that the projected
problem is a small-size GSVD, and this structure is preserved whenever the restart truncates
the involved decompositions and they are extended again.

The rest of the paper is organized as follows. Section 2 presents all the background material
that is required for section 3, which presents the new developments related to thick restart for the
GSVD. In section 4 we provide a few details of how the proposed method has been implemented
in the SLEPc library. Section 5 illustrates how the solver performs when applied to several test
problems. Finally, we wrap up with some concluding remarks in section 6. Throughout the
paper, the presentation is done for real matrices, although the extension to the complex case is
straightforward. In fact, our implemented solver supports both real and complex arithmetic.

2 Background

In this section, we review a number of concepts that are required for the developments of
subsequent sections. Many of the concepts are also discussed for the case of the SVD, aiming

2

at facilitating the understanding of the GSVD case, which is more involved.

2.1 The SVD and the GSVD

Recall that the (standard) SVD of a matrix A

Rm

×

∈
A = U ΣV T ,

n is written as

(2)

Rm

×

where U = [u1, . . . , um]
∈
Rm
n is a diagonal matrix with real non-negative diagonal entries Σii = σi, i = 1, . . . , min
.
}
The vectors ui and vi are called the left and right singular vectors, respectively, and the σi are
the singular values.

n are orthogonal matrices, and Σ
m, n
{

m and V = [v1, . . . , vn]

∈

∈

×

×

Rn

It is customary to write the decomposition in a way that the singular values are sorted in
σr > σr+1 = . . . = σn = 0, where r = rank(A). We can

non-increasing order, σ1 ≥
write the decomposition (2) as a sum of outer product matrices,

σ2 ≥

. . .

≥

A =

r

Xi=1

σiuivT
i .

(3)

It is well known that if only k < r terms in (3) are considered, the resulting matrix is the best
rank-k approximation of matrix A, in the least squares sense. This so called truncated SVD of
A is what one can usually aﬀord to compute in the large-scale, sparse case. More generally, we
will consider the case where the k < r terms taken in (3) correspond to either the largest or the
smallest singular values, and we will refer to this decomposition as the partial SVD.

Now consider two matrices with the same column dimension, A

GSVD (1) can also be written as

∈

A = UACG−

1,

B = UBSG−

1,

Rm

×

n and B

∈

Rp

n. The

×

(4)

Rp

Rm

×

p orthogonal and G

m and UB ∈

n nonsingular. For our purpose, we
with UA ∈
can think of C and S as being diagonal matrices, but in the general case this needs a detailed
discussion that we summarize below. In (1) and (4), we are assuming that the pair
is
regular, which means that the matrix obtained by stacking A and B has full column rank and
hence the triangular factor of its QR decomposition is nonsingular1,

A, B
{

∈

}

×

×

Rn

Z :=

A
B(cid:21)

(cid:20)

= QR =

QA
QB(cid:21)

(cid:20)

R,

(5)

Rn

where R
structure of C and S is

∈

×

n is upper triangular and Q

R(m+p)

×

n has orthonormal columns. Then the

∈

q
Iq

C = 

ℓ n

q

ℓ

−

−

ˆC

q

ℓ



n

q

ℓ

−

−

q
O

S = 

ℓ

ˆS

n+q

−

p

ℓ



(6)

O

m

q

ℓ

In

q

ℓ

n

q

ℓ






where ˆC and ˆS are square diagonal matrices, Iq and In
ℓ are identity matrices of the indicated
size, and O represents a rectangular block of zeros with the appropriate dimensions. Writing
cq+ℓ > 0, and ˆS = diag(sq+1, . . . , sq+ℓ) with 0 <
ˆC = diag(cq+1, . . . , cq+ℓ) with cq+1 ≥ · · · ≥



−

−

−

−

−

−

−

−

q

1If the pair {A, B} is not regular, a rank-revealing decomposition should be used instead of the QR [22].

3

U T
A

U T
B

A

B

G

=

=

G

C

S

Figure 1: Scheme of the GSVD of two matrices A and B, for the case m > n and p < n.

sq+ℓ, we have that c2
sq+1 ≤ · · · ≤
values σ(A, B) are the ratios of these quantities,

i + s2

i = 1, i = q + 1, . . . , q + ℓ, and the generalized singular

, . . . ,

, cq+1/sq+1, . . . , cq+ℓ/sq+ℓ

, 0, . . . , 0

.

∞

(7)

∞

q
{z

−

}

|

}

|

ℓ
q
n
−
| {z }

ℓ
{z
In order to better understand the structure of C and S, it is helpful to note that the zero blocks
in (6) may have zero rows or columns, and also that the number of nonzero rows of C is equal to
rank(A) and similarly for S with respect to rank(B). For instance, Figure 1 shows an example
of GSVD for the case that m > n and p < n. Assuming that A and B have full column and row
ranks, respectively, we have that there are q = n

p inﬁnite generalized singular values.

−
In order to simplify the presentation, in the sequel we will suppose that there are no inﬁnite
or zero generalized singular values, that is, q = 0 and ℓ = n. In section 3.3 we will discuss what
happens when our method is applied to problems with inﬁnite or zero σ’s, such as when either
A or B have less rows than columns.

As in the case of the SVD, for large-scale problems we will consider a partial GSVD, that is,
we employ methods that compute approximations of k quadruples (σi, uA
i , gi) corresponding
to either the largest or the smallest generalized singular values σi. We call uA
i the left
generalized singular vectors, while gi are the right generalized singular vectors. Note that if σi
are the generalized singular values of
are the generalized singular values of
.
B, A
}
{

, then σ−
i
}

i and uB

A, B
{

i , uB

1

2.2 Equivalent eigenvalue problems

The solution of the two problems presented in the previous section can be approached by for-
mulating a related eigenvalue problem. More precisely, there are two possible strategies, that
we will refer to as cross and cyclic. We start by discussing this in the context of the SVD and
then extend it to the GSVD.

The SVD relation (2) can be written as AV = U Σ or as AT U = V ΣT . Suppose that m

then equating the columns we have

Avi = uiσi,
AT ui = viσi,
AT ui = 0,

i = 1, . . . , n,

i = 1, . . . , n,

i = n + 1, . . . , m.

Premultiplying (8) by AT and using (9) results in the relation

AT Avi = σ2

i vi,

4

n,

≥

(8)

(9)

(10)

(11)

that is, the vi are the eigenvectors of the symmetric matrix AT A corresponding to eigenvalues σ2
i .
If the corresponding left singular vectors are also required, they can be computed as ui = 1
σi Avi
from (8). Alternatively, it is possible to compute the left vectors ﬁrst, via

AAT ui = σ2

i ui,

(12)

and then the right ones as vi = 1
zero eigenvalues. In practice, one would generally use (11) if m
call this approach the cross product eigenproblem.

σi AT ui, but care must be taken that (12) has at least m

n
n and (12) otherwise. We will

−

≥

The second strategy is the cyclic eigenproblem. Consider the symmetric matrix of order

m + n

H(A) =

0 A
AT
0 (cid:21)

(cid:20)

,

(13)

±

σi, i = 1, . . . , r, together with m + n

2r zero eigenvalues, where r =
that has eigenvalues
. Hence we can extract
rank(A). The normalized eigenvectors corresponding to
the singular triplets (σi, ui, vi) of A directly from the eigenpairs of H(A). Note that in this case
the singular values are not squared, so the computed smallest singular values will not suﬀer from
severe loss of accuracy as in the cross product approach. The drawback in this case is that small
eigenvalues are located in the interior of the spectrum.

−
σi are 1
√2

ui
±
vi

±

(cid:3)

(cid:2)

The cross and cyclic schemes can also be applied to the GSVD (1). The columns of G satisfy

i AT Agi = c2
s2

i BT Bgi,

(14)

so solving a symmetric-deﬁnite generalized eigenvalue problem for the pencil (AT A, BT B) pro-
vides us with the generalized singular values and right generalized singular vectors. From gi we
can compute uA
si Bgi. This is the analog of the cross product eigenprob-
lem for the SVD (11). It has the same concerns regarding the loss of accuracy in the smallest
generalized singular values, but in this case one may consider computing these values as the
reciprocals of the largest eigenvalues of the reversed pencil (BT B, AT A).

ci Agi and uB

i = 1

i = 1

Likewise, the formulation that is analogous to the eigenproblem associated with the cyclic
matrix (13) is to solve the symmetric-deﬁnite generalized eigenvalue problem deﬁned by any of
the matrix pencils

0 A
AT
0 (cid:21)

,

0
I
0 BT B(cid:21)(cid:19)

(cid:20)

,

(cid:18)(cid:20)

or

0 B
BT

0 (cid:21)

,

0
I
0 AT A(cid:21)(cid:19)

(cid:20)

,

(cid:18)(cid:20)

(15)

±

1
√2 h

uA
i
gi/si i

±

σi,
of dimensions m + n and p + n, respectively. The nonzero eigenvalues of the ﬁrst pencil are
while those of the second pencil are
, so the latter is likely to be preferred when the smallest
generalized singular values are required. The generalized singular vectors can be obtained from
, or
the eigenvectors corresponding to nonzero eigenvalues

σi of the ﬁrst pencil,

σ−
i

±

±

1

the second pencil, 1
ﬁnite precision computations, based on the conditioning of A and B.

√2 h

±

. See [16] for considerations on which of the two pencils is best in

uB
i
gi/ci i

Note that the symmetric-deﬁnite generalized eigenproblems discussed in this section may
in fact be semi-deﬁnite, e.g., if BT B is singular because p < n. This may cause numerical
diﬃculties when solving the problem via the cross and cyclic approaches.

2.3 SVD via Lanczos bidiagonalization

In this section we give an overview of the computation of the partial SVD by means of Lanczos
recurrences. Additional details can be found, e.g., in [14]. Without loss of generality, we will
assume that A is tall, i.e., m

n.

≥

5

In the same way that the solution of the cross product eigenproblem for AT A can be ap-
proached by ﬁrst computing an orthogonal similarity transformation to tridiagonal form, bidi-
agonalization methods for the SVD rely on an orthogonal reduction to bidiagonal form,

A = PnJnQT
n ,
n having orthonormal columns and Jn ∈

Rn

×

n Jn is tridiagonal with eigenvalues σ2

Rn
n being upper
i , that is, Jn has the same singular

×

(16)

Rm

with Pn ∈
bidiagonal, so that J T
values as A.

×

n and Qn ∈

In a Lanczos method, we compute a partial bidiagonalization instead of the full one. From (16)
n , and equating the ﬁrst k < n

we can establish the two equalities AQn = PnJn and AT Pn = QnJ T
columns we obtain the Lanczos relations

AQk = PkJk,
AT Pk = QkJ T

k + βkqk+1eT
k ,

(17)

(18)

where Jk denotes the k

×

k leading principal submatrix of Jn, and we have used the notation

α1 β1

α2 β2
. . .

Jk =










. . .
αk

−

1 βk
−
αk

1

.










(19)

Equating the jth column of (17)-(18) gives the familiar double Lanczos recurrence that is shown
in algorithmic form in Algorithm 1.

Rn, number of steps k.

∈

∈

ALGORITHM 1: Lanczos bidiagonalization
Rm×n, unit-norm vector q1
Input: Matrix A
Output: Partial bidiagonalization (17)-(18).
1: Set β0 = 0
2: for j = 1, 2, . . . , k do
3:
4:
5:
6:
7: end for

pj = Aqj
−
Normalize: αj =
qj+1 = AT pj
Normalize: βj =

βj−1pj−1
pj
k
αjqj

2, qj+1 = qj+1/βj

2, pj = pj/αj

qj+1

−

k

k

k

It is possible to establish an equivalence between the output of Algorithm 1 and the quantities
computed by the Lanczos recurrence for tridiagonalizing the cross product matrix AT A, see for
In particular, the right Lanczos vectors qj computed by Algorithm 1 form an
instance [14].
Kk(AT A, q1). Similarly, the left Lanczos vectors pj
orthonormal basis of the Krylov subspace
Kk(AAT , Aq1). Finally, there is also an equivalence with the Lanczos
span the Krylov subspace
1 ]T
tridiagonalization associated with the cyclic matrix (13), provided that the initial vector [0T qT
is used.

From the discussion above, it is clear that Ritz approximations of the singular triplets of A
can be obtained. After k Lanczos steps, the Ritz values ˜σi (approximate singular values of A)
are computed as the singular values of Jk, and the Ritz vectors are ˜ui = Pkxi and ˜vi = Qkyi,
where xi and yi are the left and right singular vectors of Jk, respectively.

6

2.4 Residual and stopping criterion

As the number of Lanczos steps increase, the Ritz approximations become increasingly accurate.
A criterion is required to determine when a certain approximate singular triplet (˜σi, ˜ui, ˜vi) can
be declared converged. And for this, we need to deﬁne a residual.

Due to the equivalence discussed in section 2.2, we can deﬁne the residual in terms of an
equivalent eigenproblem. In the case of the cross product eigenproblem, (11) or (12), only one
of the singular vectors would appear, so it is better to deﬁne the residual vector in terms of
eigenproblem associated with the cyclic matrix (13), whose norm is

qk

rSVD
i
k

k2 =

A˜vi −

˜σi ˜uik
This residual norm can be used in a posteriori error bounds. In particular, there exists a singular
value σi of A such that

k2 [3].
In the context of Lanczos, the residual (20) can be computed cheaply as follows.

If the
Lanczos relations (17) and (18) are multiplied on the right respectively by yi and xi, the right
and left singular vectors of Jk, then

˜σi| ≤ k

σi −
|

AT ˜ui −
k

2
2.
˜σi˜vik

rSVD
i

2
2 +

(20)

A˜vi = ˜σi ˜ui,

AT ˜ui = ˜σi˜vi + βkqk+1eT

k xi,

and therefore

k2 = βk|
The residual estimate (21) can be used in the stopping criterion in practical implementations of
Lanczos bidiagonalization.

(21)

rSVD
i
k

eT
.
k xi|

2.5 Thick restart for the SVD

As in any Lanczos process, when run in ﬁnite precision arithmetic, loss of orthogonality among
Lanczos vectors occurs in Algorithm 1 whenever the Ritz values start to converge. The sim-
plest cure is full reorthogonalization, which we consider in this work. This solution would be
implemented by replacing line 3 of Algorithm 1 with

pj = orthog(Aqj, Pj

1)

−

that applies Gram-Schmidt to explicitly orthogonalize vector Aqj against the columns of Pj
and similarly for line 5.

1,

−

Full reorthogonalization requires keeping all previously computed (left and right) Lanczos
vectors, and this justiﬁes the need of a restart technique that limits the number of vectors, not
only due to storage requirements but also because the computational cost of full reorthogonal-
ization is proportional to the number of involved vectors.

Thick restart is very eﬀective compared to explicit restart, because instead of explicitly
building a new initial vector to rerun the algorithm, it keeps a smaller dimensional subspace
that retains the most relevant spectral information computed so far. The computation is thus a
sequence of subspace expansions and contractions, until the subspace contains enough converged
solutions. The key is to purge unwanted components during the contraction, which is beneﬁcial
for overall convergence.

Suppose we have built the Lanczos relations (17)-(18) of order k and we want to compress to
size r < k. We start by transforming the decomposition in a way that approximate singular val-
ues and the residual norms (21) appear explicitly in the equations, as follows. First, compute the

7

1
+
k
q

1
+
k
q

1
+
k
q

1
+
k
q

1
+
k
q

J T
k

˜J T
k

˜J T
k

˜J T
r

Qk

˜Vk

˜Vr

˜Vr

˜Vr

(1)

(2)

(3)

(4)

(5)

Figure 2: Illustration of the ﬁve steps of thick restart in SVD: (1) initial Lanczos factorization of
order k, (2) solve projected problem, (3) sort and check convergence, (4) truncate to factorization
of order r, and (5) extend to a factorization of order k.

SVD of the bidiagonal matrix, Jk = Xk ˜ΣkY T
of (17)-(18) by Yk and Xk, respectively, results in

k , with ˜Σk = diag(˜σ1, . . . , ˜σk). Post-multiplication

A ˜Vk = ˜Uk ˜Σk,

AT ˜Uk = ˜Vk ˜Σk + βkqk+1eT

k Xk,

(22)

(23)

where ˜Uk = PkXk and ˜Vk = QkYk, whose columns are the left and right Ritz vectors. This
would be an exact partial SVD of A if it were not for the second summand of the right-hand
side of (23), which is related to the residual norms of the k Ritz approximations. Equation (23)
can also be written as

AT ˜Uk =

˜Vk

qk+1

˜J T
k ,

˜Jk =

(cid:2)

(cid:3)

˜σ1

˜σ2








. . .

ρ1
ρ2
...
˜σk ρk

,








(24)

where ρi := βkeT
sented graphically in Figure 2 (step 2).

k xi. Note that

ρi|
|

is equal to the residual norm (21). Equation (24) is repre-

Due to the fact that ˜Σk in (22)-(23) is diagonal, it is possible to truncate this decomposition
at any size r. For this, we must permute it so that the leading principal submatrix of ˜Σk
contains the approximations of the wanted singular values (typically the largest or smallest
ones). Figure 2 (step 3) shows in dark gray the part of the decomposition that will be discarded.
Then the new decomposition after truncation is

A ˜Vr = ˜Ur ˜Σr,

AT ˜Ur = ˜Vr ˜Σr + qk+1bT
r ,

r = [ρ1, . . . , ρr], see Figure 2 (step 4).

where bT
It only remains to extend the decomposition
by running a modiﬁed version of Algorithm 1 that starts the loop at j = r + 1. Note that
the ﬁrst newly generated left Lanczos vector is computed as pr+1 = orthog(Aqk+1, ˜Ur), whose
orthogonalization coeﬃcients are precisely the ρi’s. When the algorithm stops at iteration j = k,
the new Jk matrix has the shape depicted in Figure 2 (step 5), a bidiagonal except for the leading
part that has an arrowhead form.

8

2.6 GSVD via joint Lanczos bidiagonalization

We now turn our attention to the GSVD, and consider Lanczos methods that compute a joint
bidiagonalization of the two matrices, A and B. In this context, the stacked matrix Z (5) is
relevant and will appear in the algorithms. Also, the matrices QA and QB in (5), whose row
dimensions are the same as A and B, respectively, will be used for deriving the methods, but
need not be formed explicitly as justiﬁed later.

The matrices C and S (6) are related to the CS decomposition [11, §2.6.4] of

, that
is, the values ci and si are related to the singular values of QA and QB, respectively. Let the
CS decomposition of

QA, QB}
{

be

QA, QB}
{

QA = UACW T , QB = UBSW T ,

(25)

n is also orthogonal. Note that the ﬁrst
where UA, UB are the same as in (4), and W
∈
equation in (25) can be seen as the conventional SVD of QA, with the singular values sorted
in non-increasing order, while the second equation is an SVD-like relation where the singular
values appear in non-decreasing order with the largest value at the bottom-right corner of S.

×

Rn

If Z has full column rank, the GSVD decomposition of

A, B
{
1, B = UBSG−

1,

is given by

}

(26)

A = UACG−

where G = R−

1W with R as deﬁned in (5).
An approach to compute the CS decomposition of

of both matrices, i.e., a decomposition of the form

QA, QB}
{

is to perform a bidiagonalization

QA = U JV T , QB =

U

JV T ,

J (upper or lower) bidiagonal matrices such that the stacked matrix

b

b

(27)

has orthonormal

J
bJ

h

i

J = I. Then, the problem is reduced to the CS decomposition of the

with J,
J T
columns, that is, J T J +
b
,
J
J,
bidiagonal matrices
b
}
{
b

b

J = XCY T ,

J =

XSY T .

Substituting in (27) we get the CS decomposition (25) with UA = U X, UB =

X and W = V Y .
b
As mentioned in section 2.1, if A, B are very large and sparse matrices, the interest is
normally to compute a partial decomposition, i.e., a few (extreme) generalized singular values
and vectors.
In that case, a partial bidiagonalization of matrices QA and QB is done using
Lanczos recurrences, as described next.

U

b

b

b

Zha [27] presented an algorithm for the joint bidiagonalization of QA and QB, in which both
matrices are reduced to upper bidiagonal form without explicitly computing QA or QB. Later,
Kilmer et al. [20] proposed a variation of the joint bidiagonalization where QA and QB are
transformed to lower and upper bidiagonal forms, respectively. To keep the presentation short,
we focus on the latter variant from now on, although our solver also includes an implementation
of Zha’s variant.

Although not computing QA or QB explicitly, Kilmer’s joint bidiagonalization is based on
the application of the lower and upper Lanczos bidiagonalization algorithms in [23] to QA and
QB, respectively, yielding

QAVk = Uk+1Jk,

QB

Vk =

Uk

Jk,

b

b

b

QT

k + αk+1vk+1eT
AUk+1 = VkJ T
k + ˆβk ˆvk+1eT
J T
QT
k ,
Uk =
B
b

Vk

b

b

k+1,

(28)

(29)

9

with the column-orthonormal matrices Uk+1 = [u1, u2, . . . , uk+1], Vk = [v1, v2, . . . , vk],
[ˆu1, ˆu2, . . . , ˆuk] and

Vk = [ˆv1, ˆv2, . . . , ˆvk] and the lower and upper bidiagonal matrices

Uk =

b

Jk =

b
α1
β2 α2
. . .










. . .
βk

∈










αk
βk+1

R(k+1)

×

k,

Jk =

b

ˆα1

ˆβ1
ˆα2









. . .
. . .









1

ˆβk
−
ˆαk

Rk

k.

×

∈

Note that the bottom Lanczos relations (29) are analogous to those used for the bidiagonalization
in SVD (17)-(18). However, the top Lanczos relations (28) diﬀer in that the associated bidiagonal
matrix Jk is lower bidiagonal with one more row than columns, and the basis of left Lanczos
vectors Uk+1 contains one more vector than in the other case. The reason is that the lower
bidiagonalization algorithm in [23] starts the recurrence with the left vectors instead of the right
ones.

Zha’s method generates two upper bidiagonal matrices by applying Algorithm 1 to both
QA and QB with the same initial vector. In contrast, Kilmer’s method uses the two types of
bidiagonalizations and connects them by using the ﬁrst right Lanczos vector v1 generated in (28)
as initial vector ˆv1 in (29).

It can be shown [27, 20] that if v1 = ˆv1, the joint bidiagonalization given by (28)-(29) veriﬁes

ˆvi = (

−

1)i

1vi,

−

ˆαi ˆβi = αi+1βi+1,

i = 1, 2, . . . .

Thus, (28)-(29) can be rewritten as

QAVk = Uk+1Jk,

QBVk =

Uk ˇJk,

QT

AUk+1 = VkJ T
Uk = Vk ˇJ T
QT
B

k + αk+1vk+1eT
k + ˇβkvk+1eT
k ,

k+1,

(30)

(31)

(32)

where ˇJk =

JkD, D = diag(1,
Taking into account that QA =

1)k
−
Im 0
on the right side of (31)-(32) by Q, we get
(cid:3)

1, . . . , (

−

b

(cid:2)

b

b
1) and ˇβk = (
−
Q, QB =

−
0 Ip

1)k ˆβk.

(cid:2)

(cid:3)

Q, and premultiplying equalities

QVk = Uk+1Jk,

Im 0
(cid:3)

(cid:2)

0 Ip

QVk =

Uk ˇJk,

(cid:2)

(cid:3)

b

or, deﬁning

Vk = QVk,

e
Im 0

Vk = Uk+1Jk,

Uk+1

QQT

(cid:20)

QQT

0 (cid:21)
0
Uk(cid:21)

(cid:20)

b

= QVkJ T

k + αk+1Qvk+1eT

k+1,

= QVk ˇJ T

k + ˇβkQvk+1eT
k ,

e
Vk =

Uk ˇJk,

QQT

(cid:2)

(cid:2)

0 Ip

(cid:3)

(cid:3)

e

b

QQT

(cid:20)

Uk+1

0 (cid:21)
0
Uk(cid:21)

(cid:20)

b

e

=

=

VkJ T

k + αk+1˜vk+1eT

k+1,

e
Vk ˇJ T

k + ˇβk ˜vk+1eT
k .

(33)

(34)

The joint bidiagonalization process in [20] uses the two Lanczos relations in (33) and the
Vk, Jk, ˇJk. Recall that the vectors uj, ˆuj, ˜vj
ﬁrst one in (34) to compute matrices Uk+1,
Uk,
have lengths m, p and m + p, respectively. Algorithm 2 gives the details of Kilmer’s joint bidiag-
onalization, where in lines 2 and 9 expand(A, B, uj+1) denotes the operation that generates new
Krylov directions from the A and B matrices as follows. Each step j of the joint bidiagonaliza-
tion requires computing QQT ˜uj+1, where ˜uj+1 =

. Note that this is the orthogonal

b

e

T

uT
j+1, 0
i

h

10

projection of ˜uj+1 onto the column space of Z, which means that QQT ˜uj+1 = Zxj+1, where
xj+1 is the solution of the least squares problem

xj+1 = arg min
Rn k

x

Zx

∈

.
˜uj+1k

−

(35)

The expand(A, B, uj+1) operation ﬁrst computes the solution of the least squares problem (35)
by padding uj+1 below with p zeros, and then performs an additional multiplication by Z. If Z
is a large sparse matrix, the least squares problem is solved by means of an iterative solver such
as the LSQR algorithm [23]. Thus, the bidiagonalization process does not need to compute the
QR factorization of Z.

Rm, number of steps k.

∈

∈

∈

2, ˜v1 = ˜v1/α1

Rp×n, unit-norm vector u1

ALGORITHM 2: Lower-upper joint Lanczos bidiagonalization [20]
Rm×n, B
Input: Matrices A
Output: Partial joint bidiagonalization (33)-(34).
1: Set ˆβ0 = 0
2: ˜v1 = expand(A, B, u1)
3: Normalize: α1 =
˜v1
k
4: for j = 1, 2, . . . , k do
5:
6:
7:
8:
9:
10:
11:
12: end for

ˆuj = (
Normalize: ˆαj =
k
uj+1 = [Im, 0] ˜vj
−
Normalize: βj+1 =
k
˜vj+1 = expand(A, B, uj+1)
Normalize: αj+1 =
k
ˆβj = (αj+1βj+1)/ ˆαj

k
1)j−1 [0, Ip] ˜vj
ˆuj
k
αj uj

ˆβj−1 ˆuj−1
−
2, ˆuj = ˆuj/ ˆαj

−
2, ˜vj+1 = ˜vj+1/αj+1

2, uj+1 = uj+1/βj+1

βj+1˜vj

uj+1

˜vj+1

−

k

k

After running Algorithm 2 we get matrices Uk+1,

Vk,
satisfy equations (31)-(32). On the other hand, as pointed out in [20], it can be shown that
k Jk + ˇJ T
J T

ˇJk = Ik, which means that the matrix pair

admits a CS decomposition

Vk, Jk, ˇJk that, taking Vk = QT

Uk,

e

k

b
e
Jk, ˇJk}
{
XSY T ,

ˇJk =

Jk = X

C
0 (cid:21)

(cid:20)

Y T ,

where X
matrices.

∈

R(k+1)

×

(k+1),

X, Y

Rk

×

∈

b

k are orthogonal matrices, and C, S are k

k diagonal

×

Using the CS decomposition (36), the Lanczos relations (31)-(32) become

b

QAVkY = Uk+1X

QBVkY =

Uk

XS,

C
0 (cid:21)

(cid:20)

,

QT

AUk+1X = VkY

+ αk+1vk+1eT

k+1X,

C 0
(cid:3)
(cid:2)
X = VkY S + ˇβkvk+1eT
k

X.

QT
B

Uk

b

b

Taking wi, uA
ci, si, as the ith diagonal elements of C, S, we have

i , xi, ˆxi as the ith columns of VkY , Uk+1X,

i , uB

b

b

b
X, X and

Uk

X, respectively, and

QAwi = ciuA
i ,
QBwi = siuB
i ,

b

b
i = ciwi + αk+1vk+1eT
i = siwi + ˇβkvk+1eT
k ˆxi,

QT
QT

AuA
BuB

b
k+1xi,

for i = 1, 2, . . . , k. Thus, uA
A and B, respectively, while the approximate right singular vectors gi = R−
as the solution to the least squares problem Zgi = ˜wi, where ˜wi is the ith column of

i , are approximations of the left generalized singular vectors for
1wi can be computed

i , uB

(36)

(37)

(38)

(39)

(40)

11

VkY .

e

3 Thick restart for the GSVD

With all the ingredients presented in the previous section, we are now able to adapt the thick
restart technique of section 2.5 to the case of Kilmer’s joint bidiagonalization for the GSVD. As
was done for the SVD, the goal is to successively compress and expand the decomposition until
the working Lanczos bases contain suﬃciently good approximations to the wanted solutions.
The compression is carried out by transforming the decomposition computed by Algorithm 2 to
a form where the generalized singular values (or, more precisely, the ci and si values) appear
explicitly, and then truncating it in a way that keeps the most relevant part. This requires
computing a small-sized GSVD (or CS decomposition), as in (36). Once the decomposition is
truncated, it should be possible to extend it again by a slightly modiﬁed variant of Algorithm 2
whose loop starts at the current size after truncation.

3.1 Restarting Kilmer’s joint bidiagonalization

Substituting (36) in (33)-(34), we have

Im 0

VkY = Uk+1X

(cid:2)

(cid:2)

0 Ip

(cid:3)

(cid:3)

e
VkY =

Uk

XS,

e

b

b

C
0 (cid:21)

(cid:20)

,

QQT

Uk+1X
0

(cid:20)

(cid:21)

=

VkY

+ αk+1˜vk+1eT

k+1X,

C 0
(cid:3)

(cid:2)

e
VkY S + ˇβk ˜vk+1eT
k

X.

b

QQT

0
X(cid:21)
Uk

(cid:20)

=

e

b

b

In order to truncate the equations above, we deﬁne Yr and

This decomposition is similar to (37)-(38), but expressed in terms of the

Vk basis instead of Vk.
Xr as the matrices formed by
taking the ﬁrst r columns of Y and
r
submatrix of C and S, and Xr+1 = [x1, x2 . . . , xr, xk+1], where xj is column j of X. We can
then write

X, respectively, Cr and Sr as the leading principal r

×

b

b

e

Im 0
(cid:3)

(cid:2)

0 Ip

VkYr = Uk+1Xr+1 (cid:20)
e
VkYr =

XrSr,

Uk

(cid:2)

or

(cid:3)

e

b

b

Cr
0 (cid:21)

, QQT

Uk+1Xr+1
0

(cid:20)

(cid:21)

=

VkYr

Cr 0

+ αk+1˜vk+1eT

k+1Xr+1,

QQT

0
Xr(cid:21)
Uk

(cid:20)

=

(cid:2)

(cid:3)

e
VkYrSr + ˇβk ˜vk+1eT
k

e

Xr,

b

Im 0
(cid:3)

(cid:2)

0 Ip

(cid:2)

(cid:3)

Cr
0 (cid:21)

,

V ′r = U ′r+1 (cid:20)
e
V ′r =

U ′rSr,

e

b

b

b

QQT

(cid:20)

QQT

U ′r+1
0 (cid:21)
0
U ′r(cid:21)

(cid:20)

b

+ ˜vk+1bT

r+1,

=

V ′r

Cr 0
(cid:3)

(cid:2)

=

e
V ′r Sr + ˜vk+1ˆbT
r ,
e

(41)

(42)

X T
U ′r =
b
b

r ek, while the orthonormal vector bases have been

r+1ek+1 and ˆbr = ˇβk
where br+1 = αk+1X T
updated according to U ′r+1 = Uk+1Xr+1,

V ′r =

Xr and

the next vector of the

Uk
The bidiagonalization process continues with the updated vector bases and taking ˜vk+1 as
e
V basis. That is, Algorithm 2 is run with the loop starting at j = r + 1.
The process is illustrated graphically in Figure 3, in a similar way as was done for the SVD in
Figure 2. This time, the pictures show the right Lanczos basis
Vk, together with both bidiagonals
Jk and ˇJk, and depict how these matrices evolve when the compression and extension are carried
out. In the last panel (step 5), we can see how after restart both the upper and lower bidiagonals
have the leading part with an arrowhead shape, with the spike pointing to the left in both cases.

VkYr.

e

e

e

b

b

12

1
+
k
˜v

1
+
k
˜v

1
+
k
˜v

1
+
k
˜v

1
+
k
˜v

J T
k

ˇJ T
k

V ′k

e

Ck

Sk

˜V ′r

Vk

e

Ck

Sk

˜V ′r

Cr

Sr

˜V ′r

(1)

(2)

(3)

(4)

(5)

Figure 3: Illustration of the ﬁve steps of thick restart in GSVD: (1) initial Lanczos factorization of
order k, (2) solve projected problem, (3) sort and check convergence, (4) truncate to factorization
of order r, and (5) extend to a factorization of order k.

3.2 Residual estimates

i , ˜uB

We now discuss a convergence criterion to be used in the computation of the partial GSVD
with the method of section 3.1 to decide when an approximate generalized singular quadruple
(˜σi, ˜uA
i , ˜gi), with ˜σi = ˜ci/˜si, can be considered converged. In the remainder of this section
we will omit the tildes to simplify the notation. We must derive formulas for the estimates of
residual norms, using the information obtained during the bidiagonalization, without expensive
additional computation. The iteration stops when the number of converged solutions reaches
the number requested by the user.

As in the case of the SVD (see section 2.4), there are two possible alternatives to deﬁne the
residual associated to an approximate solution, in terms of either the cross product eigenprob-
lem (14) or the cyclic eigenproblem (15),

rCROSS
i

= s2

rCYCLIC,A
i

=

rCYCLIC,B
i

=

AT uA

i BT Bgi,
c2
i AT Agi −
σiuA
Agi/si −
i
σiBT Bgi/si(cid:21)
i −
1
i uB
σ−
Bgi/ci −
i
1
i AT Agi/ci(cid:21)
σ−
i −

BT uB

,

(cid:20)

(cid:20)

(43)

(44)

(45)

.

The residual norm

rCROSS
i

2 was used by Zha [27], who showed, in the context of joint

bidiagonalization with both bidiagonal matrices in upper form, that

2 ≤
(cid:13)
(cid:13)
where yi is the ith right singular vector of the bidiagonal matrix Jk resulting from a k-step joint
In the case of joint bidiagonalization with lower-upper bidiagonal
bidiagonalization process.
forms, the above inequality should be modiﬁed slightly,

(cid:13)
(cid:13)

−

(cid:12)
(cid:12)

i BT B)gi
c2

αkβk

R
k

k2 ,

eT
k yi
(cid:12)
(cid:12)

(s2
(cid:13)
(cid:13)

i AT A

i BT B)gi
c2

αk+1βk+1

2 ≤
The matrix R above is the triangular factor of the QR decomposition (5), so its 2-norm is not
readily available, but it can be bounded by √m + p max

(cid:13)
(cid:13)

−

A

(cid:12)
(cid:12)

k

,

The residual rCROSS

rCYCLIC,A
i
method [15]. It refers to uA

(43) does not account for errors in left vectors uA

i , so the residual
(44) may be more appropriate. This is the residual employed in the Jacobi-Davidson
(45) stemming from the second

i . Similarly, the residual rCYCLIC,B

i , uB

i

i

{k

k∞

B
k

.
k∞}

eT
k yi
(cid:12)
(cid:12)

R

k2 .

(46)

13

(cid:13)
(cid:13)

(cid:13)
(cid:13)
i AT A

(s2

(47)

(48)

(49)

(50)

(51)

(52)

cyclic eigenproblem in (15) refers to uB
combines the two cyclic residuals so that both uA
From the deﬁnition of the GSVD (26) we have

i . In the following, we deﬁne a convergence criterion that

i and uB

i are taken into consideration.

Agi = ciuA
i ,
Bgi = siuB
i ,

siAT uA

i = ciBT uB
i .

Using (48) in (49), and in the same way using (47) in (49), we get

i AT uA
s2
i BT uB
c2

i = ciBT Bgi,
i = siAT Agi,

and we deﬁne a residual that accounts for these two relations, whose norm is

We can derive bounds for this residual from the quantities computed in the joint bidiagonal-

s2
i AT uA

i −

ciBT Bgi

c2
i BT uB

i −

siAT Agi

2
2 +

(cid:13)
(cid:13)

(cid:13)
(cid:13)

2
2.
(cid:13)
(cid:13)

rGSVD
q(cid:13)
(cid:13)
(cid:13)
(cid:13)
ization. From (39)-(40) we have

2 =

(cid:13)
(cid:13)

Agi = ciuA
i ,
Bgi = siuB
i ,

AT uA
BT uB

i = RT
i = RT

ciRgi + αk+1vk+1eT
(cid:0)
siRgi + ˇβkvk+1eT
k ˆxi
(cid:0)

(cid:1)

k+1xi
.

,

(cid:1)

(53)

(54)

Using (53) and taking into account that c2

i + s2

i AT uA
s2
(cid:13)
(cid:13)

i −

ciBT Bgi

2 =
(cid:13)
=
(cid:13)

with the bound

AT uA
(cid:13)
AT uA
(cid:13)
(cid:13)
(cid:13)

i −
i −

i = 1, the residual norm for (50) is
ciZ T Zgi
ciRT Rgi

k+1xi

2
2 =

αk+1RT vk+1eT
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)

i AT uA
s2

k2 .
In fact, this is a bound for s2
for the cyclic residual (44), since the upper part of
i k
that residual is exactly zero due to the left relation in (53). Analogously, the residual associated
to (51) veriﬁes

i −
rCYCLIC,A
i

αk+1

R
k

2 ≤

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:12)
(cid:12)

eT
k+1xi
(cid:12)
(cid:12)

ciBT Bgi

k

which is related to c2
i k

(cid:13)
(cid:13)
rCYCLIC,B
i

i BT uB
c2

i −

siAT Agi

ˇβkeT
(cid:12)
(cid:12)
. Combining the previous two bounds,
k

2 ≤
(cid:13)
(cid:13)

k2 ,

R
k

k ˆxi

(cid:12)
(cid:12)

2 ≤ q(cid:0)
(cid:13)
(cid:13)
rGSVD
It is interesting to see that

(cid:13)
(cid:13)

rGSVD

αk+1eT

k+1xi

2 +
(cid:1)

k ˆxi

ˇβkeT
(cid:0)

(cid:1)

2

R

k2 .

k

(55)

2 is equal to the residual norm of (49). Using (47)

and (48) we have

i AT uA
s2
i BT uB
c2

i −
i −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

and

(cid:13)
(cid:13)
ciBT Bgi
2 =
(cid:13)
siAT Agi
2 =
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
i AT uA
s2
(cid:13)
i BT uB
c2
(cid:13)
(cid:13)
(cid:13)
rGSVD

i −
i −

cisiBT uB
i
siciAT uA
i

2 = si
(cid:13)
2 = ci
(cid:13)
(cid:13)
(cid:13)
ciBT uB
i

siAT uA
(cid:13)
ciBT uB
(cid:13)
(cid:13)
(cid:13)
2 .

i −
i −

ciBT uB
i
siAT uA
i

2 ,
2 ,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2 =
(cid:13)
(cid:13)

siAT uA
(cid:13)
(cid:13)

i −

(cid:13)
(cid:13)

As a summary, our criterion to accept a computed generalized singular quadruple as con-

< tol,
verged during the restart of the joint bidiagonalization is
where tol is the user-deﬁned tolerance. According to (55), this can be considered a criterion
relative to the norm of the problem matrices.

k+1xi

q(cid:0)

k ˆxi

(cid:1)

2 +
(cid:1)

ˇβkeT
(cid:0)

2

(cid:13)
(cid:13)
αk+1eT

14

3.3 Generalized singular values equal to zero or inﬁnity

Remember from (7) that some of the generalized singular values can be 0 or
. These values
represent a problem for the joint bidiagonalization algorithm, and should be avoided. Given the
CS decomposition (25), and taking into account (6), the subset of generalized singular values
corresponding to ˆC, ˆS, satisfy
QAwi = ciuA

i = ciwi QBwi = siuB

i , QT

i , QT

BuB

AuA

(56)

∞

i = siwi.
. A generalized singular value of

The rest of the generalized singular values are 0 or
corresponds to vectors wi, uA

i satisfying
QAwi = uA
i = wi, QBwi = 0,
and a zero generalized singular value corresponds to vectors wi, uB

i , QT

AuA

∞

QAwi = 0, QBwi = uB

i , QT

BuB

i = wi.

Finally, there can be a number of left singular vectors for which

QT

BuB

i = 0

or

i satisfying

∞

(57)

(58)

(59)

QT

AuA

i = 0.
The joint bidiagonalization algorithm should avoid converging to any of the generalized
singular vectors in (57)-(60). In exact arithmetic, if the start vector ui of the algorithm is a
linear combination of the generalized singular vectors uA
i of (56), all the generated vectors vi, ui
and ˆui will also be combinations of the vectors wi, uA
i and uB
i , respectively, of (56). Such an
initial vector u1 can be generated with the following steps:

(60)

1. Start with a random vector ˆu0 of p elements. That vector will be a combination of the

singular vectors uB

i of (56), (58) and (59).

2. Compute ˜v0 = QQT

0
ˆu0(cid:21)

(cid:20)

. We have that ˜v0 = Qv0, with v0 = QT

B ˆu0. Because of (59),

vector v0 will be a linear combination of vectors wi of only (56) and (58).

3. Compute u1 as the result of normalizing

˜v0. Then u1 = QAv0 and, because

QAwi = 0 in (58), vector u1 will be a linear combination only of vectors uA

(cid:2)

i of (56).

Im 0
(cid:3)

Unfortunately, when working in ﬁnite precision, rounding errors may cause components of
the unwanted singular vectors to appear even if a suitable start vector is used. The algorithm
eﬀectively avoids components of the generalized singular vectors from (58), related to the null
space of QA, which comes from the fact that, according to (33), computation of each vector ˜vi
starts by forming the vector QQT

= Qx, where x is a vector in the range space of QT

A, thus

ui
0 (cid:21)

(cid:20)
orthogonal to the null space of QA.

However, the same is not true for components of vectors from (57) or (60). In particular, if
the null space of QB is non-trivial, such as when p < n, unwanted components of the singular
vectors of (57) may appear when computing the largest generalized singular values. Similarly, if
the null space of QT
A is non-trivial, such as when m > n, unwanted components of the singular
vectors of (60) may appear when computing either the largest or the smallest generalized singular
values.

We mitigate both problems by checking the dimensions an interchanging matrices if neces-
n, we
instead. If looking for the smallest values, m > n and

sary. In particular, if the largest generalized singular values of
compute the smallest values of
p

B, A
}
{
n, we compute the largest values of

are sought and p

A, B
{

≤

}

≤

.
B, A
}
{

15

3.4 Using a scale factor

During the development of the solver, we have noticed that scaling one of the matrices may
result in a signiﬁcant improvement of convergence. When the user speciﬁes a scale factor γ,
the method is applied to the pair
. As a consequence, the algorithm
instead of
A, γB
}
{
works with a modiﬁed coeﬃcient matrix for the least squares problem,

A, B
{

}

Zγ :=

A
γB(cid:21)

(cid:20)

=

QA,γ
QB,γ(cid:21)

(cid:20)

Rγ,

where QA,γ and QB,γ are diﬀerent from those obtained without scaling. The results of section 5
will illustrate how large is the impact of scaling on performance. An analysis of why this is the
case is out of the scope of this paper.

If we use (ci, si, uA

i , uB

i , gi) to denote the solution for the pair

the relations for the scaled problem as

A, B
{

}

as in (4), we can write

A(ωigi) = (ωici)uA
i ,

γB(ωigi) = (γωisi)uB
i ,

(61)

for certain weights ωi, so after solving (61) we can retrieve the solution of the original problem (4)
by multiplying the generalized singular values σi by γ and also multiplying the gi vectors by
ω−
i

, which can be obtained from the relation

1

i c2
ω2

i + γ2ω2

i s2

i = 1, ω−

1
i =

c2
i + γ2s2
i ,

q

where ci, si, corresponding to the original problem, can be computed from σi.

When computing the largest generalized singular values, a good choice for the scale factor
is a value similar to σ1, but of course this value is not known a priori. In cases where no good
initial guess of σ1 is available, we have devised a dynamic scaling mechanism, in which the user
speciﬁes a threshold γ0 so that, at the time of restart, the solver checks whether ˜σ1 > γ0, where
˜σ1 is the current approximation to the largest generalized singular value, and if this condition
is met then the factorization is thrown away and started from scratch by using a scale factor
γ = ˜σ1. This scale factor is cumulative for several restarts in case γ0 is small, and still the
wasted computation usually pays oﬀ compared to a run without scaling.

3.5 One-sided orthogonalization

When the thick restart technique was introduced for the SVD in section 2.5, we pointed out
that the loss of orthogonality among Lanczos vectors can be avoided by means of full or-
thogonalization, that is, enforcing the full orthogonality of both left and right Lanczos bases
via explicit orthogonalization.
In practice, lines 3 and 5 of Algorithm 1 are replaced with
1) and qj+1 = orthog(AT pj, Qj). However, Simon and Zha [25] showed
pj = orthog(Aqj, Pj
that it is not necessary to explicitly orthogonalize both bases, and it is enough with orthogo-
nalizing one of them since the level of orthogonality of left and right Lanczos vectors go hand
in hand. This technique is called one-sided orthogonalization, and is implemented in SLEPc’s
thick-restart Lanczos solver for the SVD [14], reducing the cost of each iteration signiﬁcantly.

−

The one-sided orthogonalization technique can also be applied to the GSVD case. In Kilmer’s
joint bidiagonalization (Algorithm 2), lines 5, 7, and 9 involve a full orthogonalization that
would be implemented with a call to orthog(). However, two of these three orthogonalizations
can be avoided, that is, instead of explicitly orthogonalizing against the full basis, just a local
In particular, we have found that it is
orthogonalization with the previous vector is done.
enough to explicitly orthogonalize the Uk+1 basis (line 7). This saves about two thirds of the
orthogonalization cost, as will be illustrated in the computational results of section 5.

16

One-sided orthogonalization in the context of joint Lanczos bidiagonalization is discussed
in [18], where the authors show that it is suﬃcient to explicitly orthogonalize Uk+1 and Vk,
provided that the bidiagonal matrix Jk does not become too ill-conditioned. Note that our one-
sided orthogonalization strategy is more aggressive, as it orthogonalizes just one basis instead
of two. That is why in our solver one-sided orthogonalization is an option that the user can
activate, but by default full orthogonalization of the three bases is carried out. However, in our
tests we have not found any situation where the one-sided variant leads to large residual norms
or a bad level of orthogonality of the computed bases.

4 Details of the implementation in SLEPc

We now discuss a few relevant details of our implementation, with which the results shown in
section 5 have been obtained. The implementation is included in one of the solvers of SLEPc,
the Scalable Library for Eigenvalue Problem Computations [13]. We ﬁrst give an overview of
this library, before going into the details.

SLEPc is a parallel library intended for solving large-scale eigenvalue problems, mainly by
iterative methods, that is, in cases where only part of the spectrum is required. It consists of
several modules, each one for a diﬀerent type of problem, including linear eigenproblems, polyno-
mial eigenproblems, general nonlinear eigenproblems, and singular-value-type decompositions.
The latter module is called SVD and is the one relevant for this work.

SLEPc can be seen as an extension of PETSc, the Portable, Extensible Toolkit for Scientiﬁc
Computation [4]. PETSc provides a lot of functionality related to data structures and solvers
useful for large-scale scientiﬁc applications modeled by partial diﬀerential equations. Apart from
the basic data objects for working with matrices and vectors, the only PETSc modules that are
relevant for this work are those implementing iterative methods for linear systems of equations
(KSP) and preconditioners (PC).

The model of parallelism of PETSc/SLEPc is message-passing (MPI), where every object is
created with respect to an MPI communicator, implying that certain operations are carried out
collectively by all processes belonging to that communicator. A second level of parallelism is also
available to further accelerate the local computations either via CPU threads or by launching
kernels to a GPU, but we do not consider them in this paper.

The solver modules are essentially a collection of solvers with a common user interface,
where the user can choose the solver to be employed. This selection can be done at run time
via command-line options, which confers a lot of ﬂexibility by allowing also to specify solver
parameters such as the dimension of the subspace, the tolerance, and many more. In the case
of the SVD module, it contains a number of solvers, from which we highlight the following:

• cross. This solver originally contained a gateway for computing the SVD via the linear
eigenvalue problem module, in particular with the equivalent cross product matrix eigen-
problem, (11) or (12). In this work, we have extended this for the GSVD via the equivalent
eigenproblem (14).

• cyclic. Similarly to the previous one, this solver had code for the SVD formulated via
the cyclic eigenproblem (13), and we have extended it for the GSVD using the equivalent
cyclic eigenproblems (15).

• trlanczos. This is the thick-restart Lanczos solver. The implementation for the SVD
is described in [24]. The extension to the GSVD, following the methodology detailed in
section 3, constitutes the main contribution of this paper.

17

At the core of the thick-restart Lanczos solver is the lower-upper2 joint bidiagonalization of
Algorithm 2. One of the main operations in this algorithm is the orthogonalization of vectors,
which in SLEPc is carried out with an iterated classical Gram-Schmidt variant that is both
eﬃcient in parallel and numerically stable [12]. Another notable operation is the expansion of
the
V Lanczos basis, which implies the solution of the least squares problem (35). This is done
using PETSc’s functionality, as described next.

e
PETSc allows combining iterative methods from KSP with preconditioners from PC, for in-
stance gmres and jacobi. Direct linear solvers can be used with a special KSP that means
precondition only, e.g., preonly and lu. There are a limited number of instances of KSP and PC
that can handle rectangular matrices, so the possibilities for least squares problems are:

• A purely iterative method, i.e., without preconditioning, via the KSP solver lsqr that
implements the LSQR method [23], or the cgls method, which is equivalent to applying
the conjugate gradients on the normal equations.

• A direct method, with preonly and qr. This requires installing PETSc together with

SuiteSparse, which implements a sparse QR factorization [7].

• A combination of the two, i.e., lsqr and qr. Since qr is a direct method, the lsqr method
will ﬁnish in just one iteration. Note that LSQR requires that the preconditioner is built for
the normal equations matrix Z T Z, which means that in the case of qr the preconditioner
application will consist in a forward triangular solve followed by a backward triangular
solve with R, the computed triangular factor. Even though the number of iterations is
one, due to how the method is implemented the number of preconditioner applications
is two, but still this is usually cheaper than preonly with qr because the latter needs
applying the orthogonal factor of the QR decomposition, which is avoided with lsqr.

We will consider lsqr without preconditioner and with the qr preconditioner. The latter
case requires that matrix Z is built explicitly, by stacking the user matrices A and B, while the
unpreconditioned lsqr can operate with A and B independently. Hence, we have added a user
parameter explicitmatrix that can be turned on if necessary. This option applies also to the
cross and cyclic solvers to explicitly build the matrices of (14) and (15), which is necessary
in the case that a direct linear solver is to be used in the solution of the eigenproblem, e.g., to
factorize matrix BT B.

Apart from the joint bidiagonalization of Algorithm 2, the main ingredient of the solver is
the thick restart discussed in section 3. In SLEPc, the small-sized dense projected problem is
handled within an auxiliary object called DS, with operations to solve the projected problem, sort
the computed solution according to a certain criterion, and truncate it to a smaller size, among
other. For the GSVD, we have implemented the computation of the CS decomposition (36)
using LAPACK’s subroutine GGSVD3, and the truncation operation that manages the arrowhead
shapes in Figure 3 using a compact storage.

A common parameter in all SLEPc solvers is ncv, the number of column vectors, that is, the
maximum allowed dimension of the subspaces. In SVD-type computations, the user can also
choose whether to compute the largest (default) or smallest (generalized) singular values and
vectors. The user interface of the GSVD thick-restart solver includes two additional parameters:
the scale factor, discussed in section 3.4, and the restart parameter, indicating the proportion
of basis vectors that must be kept after restart (the default is 50%).

A ﬁnal comment is about locking. The absolute value of the spikes of arrows in Figure 3
are precisely the residual bounds used in the convergence criterion (55). As soon as the gener-

2

We have also implemented the upper-upper variant (Zha’s method), including thick restart, but we do not

consider this here to keep the presentation short.

18

Table 1: Description of the test problems used in the computational experiments: number of
rows m and columns n of A, number of rows p of B, number of requested generalized singular
values nsv, whether largest or smallest generalized singular values are wanted, and the ﬁrst
computed value.
name
diagonal
invinterp
hardesty2
3elt
gemat12
onetone1

which
largest
largest
largest
largest
largest
smallest σn = 3.44

ﬁrst value
σ1 = 0.57735
σ1 = 6407.9
σ1 = 53990.8
σ1 = 8727.4
σ1 = 47738

p
500000
131072
303645
4721
4930
36058

m
500000
65536
626256
4720
4929
36057

n
500000
65536
303645
4720
4929
36057

nsv
20
20
20
5
5
5

10−

4

·

alized singular quadruples converge, the leading values of these spikes become small (below the
tolerance). Then one could set these values to zero explicitly, with the consequent decoupling
from the rest of the bidiagonalization. This is called locking, a form of deﬂation in which the
converged solutions are used in the orthogonalization but are excluded from the rest of opera-
tions. In our solver, locking is active by default, but the user can deactivate it so that the full
bidiagonalization is updated at restart (including converged vectors). All results discussed in
next section use locking.

5 Computational results

In this section we present results of some computational experiments to assess the performance of
the solvers in terms of accuracy, convergence, and parallel scalability. The executions have been
carried out on the Tirant III computer, which consists of 336 computing nodes, each of them
with two Intel Xeon SandyBridge E5-2670 processors (16 cores each) running at 2.6 GHz with 32
GB of memory, connected with an Inﬁniband network. We allocated 4 MPI processes per node
at most. The presented results correspond to SLEPc version 3.18, together with PETSc 3.18,
SuiteSparse 5.12 and MUMPS 5.5. All software has been compiled with Intel C and Fortran
compilers (version 18) and Intel MPI.

Table 1 lists the problems used in the experiments, summarizing some properties and pa-

rameters. Here is a short description of the problems:

• The diagonal problem is taken from [27, Example 1], but with larger dimension. The
problem matrices are computed as A = CD and B = SD, where C = diag(ci) with
+ ri where ri is a
ci = (n
random number uniformly distributed on [0, 1].

C 2, and D = diag(di) with di =

i + 1)/2n, S = √I

4i/n
⌈

−

−

⌉

• The invinterp case corresponds to the inverse interpolation problem of IR Tools [10, §3.2],
intended to test iterative regularization methods for linear inverse problems. The GSVD
can be used in this context.
In this case, the A matrix comes from the interpolation
relations of n points at random locations with respect to a 2D regular grid of side √n, and
B is the regularization matrix. In our case, for matrix B we use the last case listed in [10,
§4.2] (2D Laplacian with zero derivative enforced on one of the boundaries) except that
we do not compute the compact QR decomposition of this matrix.

• In the 3elt, gemat12 and onetone1 problems, the A matrix is the matrix with the same name
taken from the SuiteSparse matrix collection [8], while the B matrix is a bidiagonal matrix
1 and 1 on the subdiagonal and diagonal,
with one more row than columns and values

−

19

Table 2: Computational results for the ﬁrst three test cases, solved sequentially and in parallel
(with 16 MPI processes), using a sparse QR factorization or the LSQR iterative method for the
least squares solves, respectively. We show the scale factor γ, the number of outer iterations of
thick-restart Lanczos (iters), the total number of iterations of the linear solver, the execution
time in seconds, and the maximum error of all computed solutions.

test problem LS solver
diagonal
invinterp
hardesty2
diagonal
invinterp

QR
QR
QR
LSQR
LSQR

processes
1
1
1
16
16

γ
1
50
1000
1
50

iters
763
17
3
818
16

linear its
12479
326
102
701867
767359

time
2847
37
81
489
518

6
3
6
6
4

error
10−
10−
10−
10−
10−

·
·
·
·
·

9

10

11

9

7

respectively. The hardesty2 matrix also belongs to the SuiteSparse matrix collection, but
instead of using it with a bidiagonal matrix, we take the bottom square block as matrix
B and the remaining top rows as matrix A.

All results in this section are obtained with the explicitmatrix ﬂag set, except for the
diagonal, 3elt, gemat12 and onetone1 problems when using LSQR. Table 2 illustrates the per-
formance of our solver with the ﬁrst three test cases. In sequential runs we use the sparse QR
factorization to solve the least squares problems, but in parallel we have to resort to LSQR,
which may need a lot of iterations (in fact, for the hardesty2 problem the maximum number
of iterations is exceeded). From the table, we can see that the invinterp problem needs more
time in parallel than sequentially, because LSQR is slow (this is also related to the scale factor
as discussed below). Still, parallelism allows solving very large problems where computing a
sequential sparse QR is not viable.

(cid:13)
(cid:13)

k∞

rGSVD
(cid:13)
(cid:13)

To assess the accuracy of the computed generalized singular quadruples, we use the residual
norm (52) relative to the norm of the stacked matrix (5),
. The default tolerance
Z
2 /
k
8, and we see in Table 2 that the error is below the
for the thick-restart Lanczos solver is 10−
tolerance in all cases except for invinterp with LSQR. This can be ﬁxed by requesting a more
10), otherwise a
stringent tolerance for the LSQR stopping criterion (which we have set to 10−
bad accuracy of the inner iteration prevents attaining the requested accuracy in the outer one.
In the case of a sparse QR factorization, the value of the total number of iterations of the
linear solver in Table 2 can be interpreted as the number of least-squares problems that need
to be solved. To understand this value, we have to take into account that the default value
of ncv (number of column vectors) is equal to max
nsv, 10
, that is, 40 in the case of the
}
test problems analyzed in Table 2. In addition to the least-squares problems required during
1wi
the Lanczos iteration, the postprocessing to obtain the ﬁnal right singular vectors gi = R−
requires additional least squares solves.

2
{

·

The scale factor discussed in section 3.4 may have a signiﬁcant impact on the performance
of the thick-restart Lanczos solver, as it has an inﬂuence on the number of Lanczos iterations
as well as the number of iterations needed by LSQR. Figure 4 compares the execution time of
the last problems in Table 1 when the scale factor is changed. The tolerance used for the LSQR
12. We can draw several conclusions. First, in all these problems scaling is
in this case is 10−
beneﬁcial, and execution time is reduced whenever the scale factor γ is increased, up to a point
where it stabilizes. This is also true, but with the reciprocal of γ, if the smallest generalized
singular values are computed (onetone1 problem). Second, in these problems it is much cheaper
to use a sparse QR factorization for the least squares problems, compared to solving them with
LSQR, as the latter may require many iterations (it does not converge in the onetone1 problem).
Still, LSQR is currently necessary for parallel runs, and in Figure 5 we study how the number

20

3elt qr
3elt lsqr
gemat12 qr
gemat12 lsqr
onetone1 qr (1/γ)

]
s
[

e
m
T

i

104

103

102

101

100

10−1

101

102

103

104

Figure 4: Execution time (in seconds) with diﬀerent scale factors.

s
r
e
t
i

R
Q
S
L

103

102

3elt lsqr
gemat12 lsqr

101

102

103

104

Figure 5: Average number of LSQR iterations per least squares solve for 3elt and gemat12 with
diﬀerent scale factors.

of iterations of LSQR changes with respect to the scale factor. As the scale factor increases, the
LSQR method has more diﬃculties to converge, but still the overall time goes down (although
relatively less compared to the sparse QR cases).

Figure 6 analyzes the performance of the dynamic scaling mechanism described in section 3.4.
As we increase the value for the dynamic scale threshold γ0, we reduce the number of re-
scale operations but increase the amount of discarded iterations (when re-scaling, the previous
iterations are discarded). The cost of each re-scale operation is not negligible in this case,
because we are using sparse QR factorization, which has to be computed after each re-scale. As
a result, the best execution time is achieved at intermediate values of the threshold (10 or 20).
Figure 7 shows the execution time of the thick-restart Lanczos solver compared with the cross
and cyclic solvers discussed in section 2.2. In all cases we use a direct method for the linear
solves (SuiteSparse for Lanczos and MUMPS for cross and cyclic). There is no clear winner,
but take into account that in Lanczos we are using the scale factor γ that gave the smallest
time in our tests for each problem, otherwise the Lanczos solver is not competitive in general.
However, it is worth noting that for the hardesty2 problem, the Lanczos solver is the only one
that provides a suitable solution in terms of the residual. In this case, the matrix BT B in the
cross and cyclic methods is singular and, because the generalized symmetric-deﬁnite Lanczos
eigensolver fails, one has to solve the equivalent eigenproblem as non-symmetric, resulting in
11). In the
much lower accuracy (
other cases, the error of the computed solutions is similar in all solvers.

4), compared to the Lanczos GSVD solver (

10−

10−

≈

≈

1

6

·

·

We conclude this section by analyzing parallel performance. Figure 8 plots the parallel
execution time of the Lanczos solver for the diagonal problem with up to 64 MPI processes. We
can see that the scaling is very good, close to the ideal one. The ﬁgure shows the one-sided

21

(a) Num re-scales

(b) Outer iters

(c) Time [s]

5

4

3

2

1

102

101

101

100

100

101

102

100
3elt

101
gemat12

102
onetone1

100

101

102

Figure 6: Dynamic scale results for diﬀerent thresholds. (a) Number of re-scales. (b) Number
of discarded iterations. (c) Execution time.

trlanczos
cross
cyclic

]
s
[

e
m
T

i

103

102

101

100

1

10−

invinterp

hardesty2

3elt

ge m at12

onetone1

Figure 7: Comparing the thick-restart Lanczos solver (using the best scale parameter γ and
sparse QR for the least squares problems) with the cross and cyclic solvers (using MUMPS for
the linear solves).

22

diagonal - total time

diagonal - orthogonalization only

104

]
s
[

e
m
T

i

103

102

103

102

101

1

2

4

8

16

32

64

1

2

4

8

16

32

64

twoside

oneside

ideal scaling

Figure 8: Execution time (in seconds) with up to 64 MPI processes for the diagonal problem,
solved with thick-restart Lanczos using one-sided or two-sided orthogonalization. The left plot
shows the total time, while the right one accounts only for the time of orthogonalization.

103.4

103.2

103

102.8

102.6

]
s
[

e
m
T

i

Figure 9: Execution time (in seconds) with up to 64 MPI processes for the invinterp problem.

1

2

4

8

16

32

64

23

variant together with the default one (two-sided). The one-sided variant is always faster, but the
diﬀerence is not too signiﬁcant because orthogonalization amounts to only a modest percentage
of the overall cost. To better appreciate the gain of one-sided orthogonalization, the right panel
of Figure 8 shows only the time of the orthogonalization, with a factor of about 2.5 improvement
of the one-sided scheme with respect to the default.

The diagonal problem is very favorable for parallel computing, as the matrix-vector product is
trivially parallelizable. On the other extreme, the invinterp problem has a very disadvantageous
sparsity pattern (nonzeros located at random positions), so the speedup shown in Figure 9 is
good up to 16 MPI processes, but gets ruined afterwards.

6 Concluding remarks

We have developed a thick restart mechanism for the joint Lanczos bidiagonalization to compute
a partial GSVD of a large-scale matrix pair. This is a very important piece to make the solver
usable in the context of real applications. We have developed a fully-ﬂedged implementation
in the SLEPc library, that in addition to the restart, includes other interesting features such
as one-sided orthogonalization, scaling, or locking. The solver is very ﬂexible, in the sense that
the user can indicate at run time whether the associated least squares problems must be solved
with a direct or iterative method, as well as specify many other settings such as the dimension
of the Krylov subspace or the scale factor.

We have conducted a number of computational experiments, showing that our solver is
numerically robust, computationally eﬃcient and scalable in parallel runs. If an appropriate
scale factor is used, the performance of Lanczos method is on a par with that of the cross and
cyclic solvers for the GSVD, which we have also developed during this work.

In parallel executions for large-scale problems, the only possibility at the moment for the
least squares problem is to use LSQR without preconditioner, since PETSc does not currently
provide parallel preconditioners for this case. This represents a severe performance bottleneck,
especially if a large scale factor is used, as this hinders convergence of LSQR. As a future work,
we could consider implementing a parallel preconditioner for least squares [5].

Acknowledgements The computational experiments of section 5 were carried out on the
supercomputer Tirant III belonging to Universitat de Val`encia. Funding for open access charge:
CRUE-Universitat Polit`ecnica de Val`encia.

References

[1] E. Anderson, Z. Bai, C. Bischof, L. S. Blackford, J. Demmel, J. Dongarra, J. Du Croz,
A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen. LAPACK Users’ Guide.
Society for Industrial and Applied Mathematics, Philadelphia, PA, third edition, 1999.

[2] James Baglama and Lothar Reichel. Augmented implicitly restarted Lanczos bidiagonal-

ization methods. SIAM J. Sci. Comput., 27(1):19–42, 2005.

[3] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der Vorst, editors. Templates for the
Solution of Algebraic Eigenvalue Problems: A Practical Guide. Society for Industrial and
Applied Mathematics, Philadelphia, PA, 2000.

[4] S. Balay, S. Abhyankar, M. F. Adams, J. Brown S. Benson, P. Brune, K. Buschelman,
E. M. Constantinescu, L. Dalcin, A. Dener, V. Eijkhout, W. D. Gropp, V. Hapla, T. Isaac,
P. Jolivet, D. Karpeev, D. Kaushik, M. G. Knepley, F. Kong, S. Kruger, D. A. May,

24

L. Curfman McInnes, R. Tran Mills, L. Mitchell, T. Munson, J. E. Roman, K. Rupp,
P. Sanan, J. Sarich, B. F. Smith, S. Zampini, H. Zhang, H. Zhang, and J. Zhang. PETSc
users manual. Technical Report ANL-95/11 - Revision 3.18, Argonne National Laboratory,
2022.

[5] R. Bru, J. Mar´ın, J. Mas, and M. T˚uma. Preconditioned iterative methods for solving

linear least squares problems. SIAM J. Sci. Comput., 36(4):A2002–A2022, 2014.

[6] C. Campos and J. E. Roman. Restarted Q-Arnoldi-type methods exploiting symmetry in

quadratic eigenvalue problems. BIT, 56(4):1213–1236, 2016.

[7] T. A. Davis. Algorithm 915, SuiteSparseQR: Multifrontal multithreaded rank-revealing

sparse QR factorization. ACM Trans. Math. Software, 38(1):1–22, 2011.

[8] T. A. Davis and Y. Hu. The University of Florida Sparse Matrix Collection. ACM Trans.

Math. Software, 38(1):1:1–1:25, 2011.

[9] A. Edelman and Y. Wang. The GSVD: Where are the ellipses?, matrix trigonometry, and

more. SIAM J. Matrix Anal. Appl., 41(4):1826–1856, 2020.

[10] S. Gazzola, P. C. Hansen, and J. G. Nagy.

IR Tools: a MATLAB package of iterative
regularization methods and large-scale test problems. Numer. Algorithms, 81(3):773–811,
2018.

[11] G. H. Golub and C. F. van Loan. Matrix Computations. The Johns Hopkins University

Press, Baltimore, MD, third edition, 1996.

[12] V. Hernandez, J. E. Roman, and A. Tomas. Parallel Arnoldi eigensolvers with enhanced
scalability via global communications rearrangement. Parallel Comput., 33(7–8):521–540,
2007.

[13] V. Hernandez, J. E. Roman, and V. Vidal. SLEPc: A scalable and ﬂexible toolkit for the

solution of eigenvalue problems. ACM Trans. Math. Software, 31(3):351–362, 2005.

[14] Vicente Hernandez, Jose E. Roman, and Andres Tomas. A robust and eﬃcient parallel
SVD solver based on restarted Lanczos bidiagonalization. Electron. Trans. Numer. Anal.,
31:68–85, 2008.

[15] M. E. Hochstenbach. A Jacobi–Davidson type method for the generalized singular value

problem. Linear Algebra Appl., 431(3-4):471–487, 2009.

[16] J. Huang and Z. Jia. On choices of formulations of computing the generalized singular value

decomposition of a large matrix pair. Numer. Algorithms, 87(2):689–718, 2020.

[17] Z. Jia and H. Li. The joint bidiagonalization process with partial reorthogonalization.

Numer. Algorithms, 88(2):965–992, 2021.

[18] Z. Jia and H. Li. The joint bidiagonalization method for large GSVD computations in ﬁnite

precision. arXiv:1912.08505 : retrieved 11 Feb 2022, 2022.

[19] Z. Jia and Y. Yang. A joint bidiagonalization based iterative algorithm for large scale

general-form Tikhonov regularization. App. Numer. Math., 157:159–177, 2020.

[20] M. E. Kilmer, P. C. Hansen, and M. I. Espa˜nol. A projection-based approach to general-

form Tikhonov regularization. SIAM J. Sci. Comput., 29(1):315–330, 2007.

25

[21] C. F. Van Loan. Generalizing the singular value decomposition. SIAM J. Numer. Anal.,

13(1):76–83, 1976.

[22] C. C. Paige and M. A. Saunders. Towards a generalized singular value decomposition.

SIAM J. Numer. Anal., 18(3):398–405, 1981.

[23] C. C. Paige and M. A. Saunders. LSQR: an algorithm for sparse linear equations and sparse

least squares. ACM Trans. Math. Software, 8(1):43–71, 1982.

[24] J. E. Roman, C. Campos, L. Dalcin, E. Romero, and A. Tomas. SLEPc users manual. Tech-
nical Report DSIC-II/24/02–Revision 3.18, D. Sistemes Inform`atics i Computaci´o, Univer-
sitat Polit`ecnica de Val`encia, 2022.

[25] Horst D. Simon and Hongyuan Zha. Low-rank matrix approximation using the Lanczos
bidiagonalization process with applications. SIAM J. Sci. Comput., 21(6):2257–2274, 2000.

[26] Kesheng Wu and Horst Simon. Thick-restart Lanczos method for large symmetric eigenvalue

problems. SIAM J. Matrix Anal. Appl., 22(2):602–616, 2000.

[27] H. Zha. Computing the generalized singular values/vectors of large sparse or structured

matrix pairs. Numer. Math., 72(3):391–417, 1996.

26

