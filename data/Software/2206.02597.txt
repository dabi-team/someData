2
2
0
2

n
u
J

6

]

O
R
.
s
c
[

1
v
7
9
5
2
0
.
6
0
2
2
:
v
i
X
r
a

No GPU? No problem: an ultra fast 3D detection of road
users with a simple proposal generator and energy-based
out-of-distribution PointNets

Alvari Sepp¨anen, Eerik Alamikkotervo, Risto Ojala, Giacomo Dario, Kari Tammi

June 7, 2022

Abstract

This paper presents a novel architecture for point cloud road user detection, which is based
on a classical point cloud proposal generator approach, that utilizes simple geometrical rules.
New methods are coupled with this technique to achieve extremely small computational re-
quirement, and mAP that is comparable to the state-of-the-art. The idea is to speciﬁcally
exploit geometrical rules in hopes of faster performance. The typical downsides of this ap-
proach, e.g. global context loss, are tackled in this paper, and solutions are presented. This
approach allows real-time performance on a single core CPU, which is not the case with end-
to-end solutions presented in the state-of-the-art. We have evaluated the performance of the
method with the public KITTI dataset, and with our own annotated dataset collected with a
small mobile robot platform. Moreover, we also present a novel ground segmentation method,
which is evaluated with the public SemanticKITTI dataset.

1

Introduction

Road user detection is an important task for an autonomous mobile robot, and it plays a key
in obstacle avoidance and route planning tasks. Sensors that produce point clouds are
role e.g.
useful due to the rich 3D information that they provide. Therefore, they are common in mobile
robot applications. These sensors are e.g. LiDAR and depth camera, or even monocular camera,
thanks to impressive advancements in image processing methods. As road user detection is a safety
critical task, low latency and high mAP are desired. Earnest eﬀorts have been directed towards
the development of methods that achieve high mAP. However, signiﬁcantly less eﬀort has been
directed towards low computational approaches. Example beneﬁts of an algorithm that has low
computational requirement in mobile robot applications are:

• less energy consumption results in longer operation time,

• higher FPS increases safety,

• no need for an expensive processing unit,

• a GPU is not a requirement, which results in lighter weight, which is crucial especially for

aerial robots,

• more processing resources can be allocated for other functions of the robot.

This paper takes a drastically diﬀerent approach for 3D road user detection to achieve low com-
putational requirement. Unlike most method is the literature, our approach discards information,
while increasing algorithmic complexity. In other words, the ﬁrst pieces of the algorithm are simple
to quickly discard substantial amounts of irrelevant data. As the amount of data decreases, the
complexity of the algorithm increases. We implemented this idea by utilizing a classical proposal
generator and energy-based out-of-distribution PointNets. With this approach, we can achieve im-
pressive results in terms of FPS, and comparable AP in 3D pedestrian detection to state-of-the-art.
Figure 1 shows the comparison to the state-of-the-art in terms of 3D pedestrian detection AP on
KITTI dataset and FPS.

1

 
 
 
 
 
 
Figure 1: Our method compared to the state-of-the-art.

1.1 Related work

Point cloud object detection is a well-established ﬁeld. The detection methods are mainly divided
into voxel, range image, and bird’s eye view methods. VoxelNet [1] partitions the point cloud into
voxel partitions. Points within a voxel partition are encoded into a vector representation character-
izing the shape information. 3D convolution is performed on the voxels to predict bounding boxes
and classes. SECOND [2] improved the accuracy and reduced computational load of VoxelNet by
preprocessing the point cloud by dropping voxels that include no points.

Authors of [3, 4, 5, 6] use range image –based method. A range image is a projection of the
point cloud on a spherical surface. The beneﬁt of using projection is to preserve the information
of neighboring measurements. However, the authors claim that some of the information is lost
during the projection. The authors use 2D convolutional neural network to make predictions for
bounding boxes and classes.

Third popular approach is to use bird’s eye view pseudo image as input for the detection
algorithm [7, 8, 9, 10]. PointPillars [7] construct pillar-like features from the point cloud with neural
network and forms a bird’s eye view pseudo image. Then a 2D convolutional neural network is
applied to this image, which makes the predictions. Bird’s eye view is a convenient representation
of the point cloud because objects are rarely stacked on top of each other in, for example, outdoor
driving scenarios, given an optimal vertical ﬁeld-of-view of the sensor.

Li et al.

[11] focused on the proposal classiﬁcation with limited processing resources. They
presented a method where the proposals are generated by removing the points belonging to the
ground and then clustering the remaining points. The proposals are classiﬁed using a neural
network. They achieved impressive results in terms of recall given the computational limitation,
even comparable to the state-of-the-art.

1.2 Scientiﬁc contribution

The literature has a large amount of point cloud object detection methods that are tested for road
user detection. However, most of them require a powerful GPU unit to run in real-time, which
some mobile robots do not have due to e.g. power, cost, weight, or size restrictions. Therefore, we
revisit a classical approach where object proposals are generated with a simple algorithm, which
runs in real-time on a standard CPU and introduce novel methods for making the detections from
the simple proposals, while preserving the light computational requirement. These novel methods
are the contributions of this paper, and they are summarized in descending order below.

1. A novel architecture for 3D detection of road users on point cloud data, which include fol-

lowing subsections:

2. the implementation of an energy-based out-of-distribution detection and in-distribution multi-

class classiﬁcation neural network for point cloud object proposals in traﬃc scenarios,

3. the extension of the conventional classiﬁcation of proposals with a proposal voxel location

encoder, which improves the accuracy of the model by a signiﬁcant margin,

4. study on PointNet critical and upper bound point sets of road users in classiﬁcation and
bounding box tasks and how they can be used for improving the out-of-distribution detection,
and

2

05101520FPS(CPU)01020304050603D pedestrian AP KITTIOursVPFNet (KITTI top1)3DSSD (IEEE/CVF 2020)IA-SSD (CVPR 2022)5. a ground segmentation method, which outperforms competitive methods in literature. We
present simple convolutional ﬁlters for the sampling of ground points for the plane ﬁt. This
simple but eﬀective sampling method does not exist in the literature to the best of authors
knowledge.

1.3 Paper structure

First, we present the methodology in detail, then the implementation details and experimental
setup, then, the results on public and private datasets. We also present some ablation studies,
qualitative results, and discussion. Finally, we conclude the paper and propose future research
directions.

2 Methods

A general schematic of the proposed architecture is presented in Figure 2. The basic principle
is to generate simple proposals and utilize eﬀective classiﬁers that diﬀerentiate between the in-
distribution (ID) proposals and the out-of-distribution (OOD) proposals. This is implemented
using novel energy-based OOD PointNets. The ﬁrst PointNet predicts the class probability vector
and OOD/ID energy score for the proposals, and the ﬁrst batch of OODs are discarded. Then,
3D bounding boxes are predicted to the remaining samples with an alternative PointNet, which
also predicts ID/OOD energy score for the samples. Moreover, a novel proposal voxel location
encoder (PVLE) is utilized to preserve information that would otherwise be lost in the proposal
normalization process. This encoder attempts to increase the accuracy of the neural networks.
The increased accuracy will prove that proposal location information has value in the proposal
classiﬁcation and bounding box estimation tasks. To summarize, the architecture aims to achieve
low computational requirement by eﬀective information discarding in hierarchical manner, while
preserving the useful information regarding to the road user detection task.

Figure 2: The proposed architecture. The input point cloud is organized by mapping Π :
Rn×3 (cid:55)→ Rsh×sw×3. Then, the ground segmentation coupled with a clustering algorithm generate
simple proposals, that are fed into the neural networks. An ID pass-through module discards the
proposals that do not belong to the ID samples, which enables low computational requirement for
the box estimation network. Finally, a second ID pass-through module discards boxes that do not
belong to the ID samples. The output of the pipeline are 3D bounding boxes and class probabilities
for the objects of interest.

2.1 Ordered point cloud representation

Points from a typical LiDAR sensor p = (cid:104)x, y, z(cid:105) are mapped Π : Rn×3 (cid:55)→ Rsh×sw×3 to spherical
coordinates, and ﬁnally to image coordinates, as deﬁned by

(cid:34)

(cid:21)

(cid:20)pu
pv

=

1/2(1 − tan−1(yx−1)π−1)sw

(1 − (sin−1(z(cid:112)x2 + y2 + z2

−1

) + fup)f −1)sh

(cid:35)

(1)

3

d) PVL-encoder     R-NetID pass-through moduleOODsdropout     a) Ground segmentation b) Clustering,   normalization EncoderFeaturesT-NetFCsc) Center to voxel EncoderFeaturesFCsEncoderFeat.ID pass-through moduleBox flowPoints flowGeometric affine modulee) OOD detection and ID classificationf) OOD/ID box estimationwhere (u, v) are image coordinates, (sh, sw) are the height and width of the desired projection
image representation, f is the total vertical ﬁeld-of-view of the sensor, and fup is the vertical ﬁeld-
of-view spanning upwards from the horizontal origin plane. The resulted list of image coordinates
is used to construct a (x, y, z)-channel image, which is the input for the next stage of the proposed
architecture.

2.2 Ground segmentation

As an additional contribution, we introduce a novel ground segmentation method, which is ultra
fast and more accurate than the methods existing in the literature. Our ground segmentation
combines a novel point sampling with the well proven RANSAC plane ﬁtting method. Figure 3
shows a graphical presentation of the method.

Figure 3: An illustration of the ground segmentation method. On the left, points that passed the
ﬁlter are indicated by green, and points that are between the dashed lines are segmented ground
points. On the right, points surrounded by ellipses are considered by Sv and Su ﬁlters when
computing the point marked with a star. A plane is ﬁtted to each sector of points. Note that here
R =

X 2 + Y 2 i.e. the distance to the Z-axis.

√

Ground segmentation is done to diﬀerentiate between the ground and the object proposals. It
is essential to segment the ground in the beginning of the pipeline for following reasons: a) reduce
amount of points to process, b) remove points that are invalid and not considered in later stages
of the pipeline, and c) improve the performance of the clustering algorithm, as points in some
clusters are close to the points of the ground plane.

We sample the potential ground plane points with two convolutional Sobel inspired ﬁlters [12].

These kernel ﬁlters are formulated as follows

Sv =

(cid:21)

(cid:20) 2
1
−2 −1

Su = (cid:2)1

2 −2 −1(cid:3) .

(2)

The ﬁlters are discrete diﬀerentiation operators that operate on the point cloud projection image.
Sv yields an approximation of vertical derivative on the projection surface, and Su gives us a
horizontal approximation. The ﬁrst term of Sv, and second term of Su are the centers of the ﬁlters,
respectively. The ﬁlters incorporate information of multiple neighboring points with Gaussian
function, where points closer to center have higher eﬀect, because simply computing the derivatives
with subtraction of a single neighboring point is not viable due to noise in a typical LiDAR
measurement. In a typical LiDAR sensor, the horizontal resolution is signiﬁcantly higher than the
vertical resolution, therefore, the shape of Su is 1x4 which means that it does not consider points on
neighboring rows, which are signiﬁcantly more distant compared to points on neighboring columns.
This way, ﬁlter is able to capture the approximation of local derivative more accurately. LiDAR
point cloud pattern on a planar ground is visualized on the right side of Figure 3. Convolutions
are run on the range matrix

X 2 + Y 2 = R ∈ Rsh×sw and height matrix Z ∈ Rsh×sw .

√

Fy =

∆Z
∆R

=

Sv ∗ Z
Sv ∗ R

, R(r, c) (cid:54)= 0

Fx = Su ∗ R

(3)

where ∗ denotes the 2-dimensional convolution operation. Matricies Fy and Fx denote an ap-
proximation of point-wise normal, as ﬁlters produce derivatives. This method for approximating
point-wise normal requires only a small amount of computation, still preserving satisfactory accu-
racy. We apply a threshold to Fy and Fx, which gives us a sample mask Fmask of ground points,
which will be the input for the RANSAC algorithm:

4

Planes fitted to sectors SensorFitted plane(cid:40)

Fmask (r, c) =

if − fth1 ≤ Fy(r, c) ≤ fth1, and − fth2 ≤ Fx(r, c) ≤ fth2

1,
0, otherwise.

(4)

Then, an element-wise matrix multiplication transformation into 3 × n matrix is carried out to
compute the sample of ground points Gsamples ∈ Rn×3 for the RANSAC algorithm:

Gsamples = Fmask (cid:12) (cid:104)X, Y , Z(cid:105) × 1T

(5)

where (cid:104)X, Y , Z(cid:105) ∈ Rsh×sw×3 contain the Cartesian coordinates of each point in the point cloud,
and (cid:12) indicates Hadamard product. Only the sampled points are considered as the inliers for the
RANSAC algorithm, thus only handful of iterations are needed, compared to case where the inliers
are searched from the entire point cloud. Again, saving a lot of computation. RANSAC (Gsamples ) =
(cid:104)a1, a2, a3, a4(cid:105) gives the parameters of the detected plane. The distances D ∈ Rsh×sw between
points and the plane is computed using the following equation:

D =

a1X + a2Y + a3Z + a4
1 + a2

2 + a2
3

(cid:112)a2

.

Finally, a threshold operation deﬁnes all points belonging to the ground Gmask ∈ Zsh×sw

2

:

Gmask (r, c) =

(cid:40)

if |D(r, c)| < pth

1,
0, otherwise.

(6)

(7)

2.3 Point cloud clustering

We are using the depth cluster method [13], mainly because it is fast (full scan from a 64 channel
LiDAR in 10 ms with a single core of a 2.0 GHz CPU, reported in [13]). However, our architecture
does not have constrains that would prevent the usage of other standard clustering algorithms
such as [14, 15, 16, 17, 18]. We picked the depth cluster method simply because it runs the fastest
on our device. The depth cluster algorithm computes the angle between neighboring points, and
if this angle satisﬁes a threshold, the point in hand is assigned under the label of the currently
computed cluster. A breath-ﬁrst search (BFS) is implemented to add points to the current cluster.
Completed BFS indicates a completion of a cluster. The algorithm is fast since is takes advantage
of the order of the point cloud, which means that ﬁnding the neighboring points is convenient.
Unfortunately, organized point clouds have holes, which are caused by the missing reﬂections of
the laser. Therefore, algorithm searches points not only from instant neighbors but from a region
of few points.

2.4 Road user detection from the simple proposals

This paper presents a novel energy-based OOD classiﬁcation and ID multi-class classiﬁcation and
3D bounding box estimation networks for point clusters in traﬃc scenarios. The proposed neural
network architectures build on the works of the creators of PointNet, Qi et al.
[19, 20], apply-
ing some application speciﬁc modiﬁcations in order to reduce computational requirement. These
modiﬁcations include the concatenation of cluster voxel position encoder features, to leverage on
the observation angle and distance, and simpliﬁed architecture in the main encoder as well as in
the fully connected layers. The main modiﬁcation is the implementation of an energy-based OOD
learning objective to mitigate the false positive rate, since our proposal generator is simple, re-
sulting in vast amounts of OOD instances. Moreover, the amount of safety critical classes present
in typical traﬃc scenarios is lower than the total amount of classes in the original PointNet [19],
which enables major simpliﬁcation of the networks, which results in the decrease in computational
complexity. Lastly, we discovered that the respective critical and the upper bound points sets for
classiﬁer and box estimation networks are diﬀerent for same cluster of points. We take advantage
of this phenomenon by implementing two separate ID pass-through modules to the pipeline for
more eﬀective OOD/ID separation.

5

2.4.1 Multi-task learning with an energy-based loss function

The basic idea of an energy-based function is to map each point of the input space to a non-
probabilistic scalar called energy E(x; f ) : RD (cid:55)→ R [21]. In our application, the output vectors of
the classiﬁer and bounding box estimator networks are mapped into their respective energy scalars,
that represent the ”in-distributioness” of a cluster of points for a given task. Method presented
here is based on Liu et al. [22], where modiﬁed version of the Helmholtz free energy from statistical
mechanics is used as E(x; f ) [23].

E(x; f ) = −T · log

K
(cid:88)

i

efi(x)/T

(8)

where x denotes the logits of a neural network, T temperature scalar, K number of logits, and f
a neural network. The energy-based training objective for the classiﬁer is same as in [22].

min
θ

E

(x,y)∼Dtrain

ID

(− log ·Fy(x)) + λ · Lenergy

where energy loss is computed as

Lenergy = E

(xID ,y)∼Dtrain

+E

xOoD∼Dtrain
OoD

ID

((E(xID ; f ) − mID )+)2
((mOoD − E(xOoD ; f ))+)2

(9)

(10)

where xID is an ID sample from KITTI training split (point inside a ground truth bounding box),
and xOOD is an OOD sample from the auxiliary OOD dataset. mID and mOOD are the means
of ID and OOD energies, respectively. The training objective is diﬀerent for the box estimation
network. Loss function for the box estimation network

Lbox = Lc1 −reg + Lc2 −reg + Lh−cls + Lh−reg +
Ls−cls + Ls−reg + γLcorner + λLbox −energy

where corner loss is computed as

Lcorner =

NS
(cid:88)

NH
(cid:88)

i=1

j=1

δij min

(cid:40) 8

(cid:88)

k=1

||P ij

k − P ∗

k ||,

(cid:41)

||P ij

k − P ∗∗
k ||

8
(cid:88)

i=1

(11)

(12)

box estimator energy Lbox −energy is computed with the Equation (10), where logits are deﬁned as
the heading and size class logits from the output vector fb(x).

2.4.2

ID pass-through modules

During inference time, energy score for each sample is computed from logits of the PointNets using
equation (8). This is straightforward with the classiﬁer PointNet, since logits fc(x) are just the class
probabilities. However, the output of the box estimation network fb(x) is a structure of heading
and size class probabilities, and residual heading, size, and center predictions. Consequently, we
have to ﬁnd optimal way of using this special vector. We found experimentally that the heading
and the size class provide the best measure for ID/OOD separation. We also discovered that the
critical and upper bound point sets for an input x are signiﬁcantly diﬀerent in the classiﬁcation
and box estimation tasks, respectively. Therefore, we implemented two separate ID pass-thorough
modules, to have more eﬀective ID/OOD separation.

6

Figure 4: The heading (right) and size (left) class vectors for a pre-trained box estimation network.

The ID/OOD classiﬁcation is computed with a threshold γ, which is a chosen to classify 95%

of the ID samples correctly.

p(x; γc, γb, fc, fb) =

(cid:40)

if Ec(x; fc) < γc, and Eb(x; fb) < γb

in,
out, otherwise.

(13)

2.5 Proposal voxel location encoding

Proposals, i.e. point clusters, are normalized before the classiﬁer because the performance is better
with canonical inputs [19]. However, location information (observation angle and distance) is lost
during this process. To make use of this information, we propose a proposal voxel location encoder
(PVLE) module, which aims to improve the point cloud proposal classiﬁcation and 3D bounding
box estimation tasks. The module processes proposal mean point voxel coordinates and outputs
learned features.

The intuition behind this method is that the observation angle and the distance of an ID
proposal carry useful information, since proposal shape, in our application, is never dominated by
a concave feature, and the density is some function of down sampling and 1/r2
m),
where subscript m denotes a mean point of a proposal, and r denotes the Euclidean distance to
the origin. The coordinates of the mean point of given proposal cluster is ﬁrst voxelized and then
encoded into a small feature vector, which is concatenated with the global features of the classiﬁer
and bounding box networks as well as the R-Net and T-Net (small spatial transformer networks).
This is done so that the networks can leverage on the observation angle and distance of a given
proposal. The design of this encoder is inspired by the ﬁrst encoder layer of the PointNet (MLP:
3-64). We use a standard approach of utilizing a fully connected layer, which produces the output
vector of the encoder (FC: 64-32). We utilize similar approach to the vanilla PointNet because
voxel coordinate input has the same shape as a point input in the vanilla PointNet.

m = 1/(x2

m+y2

m+z2

The proposal mean point is voxelized in spherical coordinates. We chose this system because it
is the natural system for sensors, e.g. LiDAR, which emits lasers from a single point. Note that the
point density follows the inverse-square law d ∝ 1/r2, voxel volume in spherical coordinate system
compensates this density variation, since voxel volume follows square law v ∝ r2. This results in
similar magnitude of points in each voxel. Another reason for this coordinate system is that given
an uneven ground and occasional sensor pivot, proposals shift to diﬀerent voxel coordinates, by
utilizing spherical coordinates, it is more robust in this situation.

3 Experiments

Experiments are conducted in three datasets. KITTI [24] and SemanticKITTI [25] are used to
measure the accuracy of the 3D object detection and ground segmentation, respectively. The
detection algorithm is trained on KITTI training set. Moreover, detection accuracy is also measured
on the DBot dataset, which is our own annotated data, collected with a compact mobile robot
platform, which is equiped with a Velodyne VLP-16 LiDAR. In-depth analysis is carried out
to validate our design choices. Lastly, qualitative results and discussion of the strengths and
weaknesses of our methods is presented in section 3.5.

7

20246810Negative energy0.000.050.100.150.200.250.300.35FrequencyIn-distributionOut-of-distribution0.02.55.07.510.012.515.0Negative energy0.000.050.100.150.200.25FrequencyIn-distributionOut-of-distribution3.1

Implementation details

Inference time is benchmarked with a 4.0 GHz CPU. Training is done with a NVidia GTX 1060
GPU. We utilized following languages and libraries: PyTorch, Python, Cython, pyBind11, C++,
NumPy, SciPy. Both classiﬁer and box estimator are trained for 200 epochs with a learning rate
of 0.001, betas are 0.9 and 0.999, and we use the Adam optimizer [26]. The resolution of the voxel
grid is: azimuth 10◦, elevation 10◦, and range 1 m.

3.2 Overall performance

For the comparison, we chose the best method for 3D pedestrian detection of the oﬃcial KITTI
benchmark: VPFNet [27], the best for 3D car detection: SFD [28], and state-of-the-art high FPS
methods: IA-SSD [29] and 3DSSD [30]. The preformance on KITTI val split is presented in Table
1.

Method

Table 1: 3D detection AP on KITTI val set.
Cars

Pedestrians
Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard

Cyclists

VPFNet[31]
SFD [28]
IA-SSD [29]
3DSSD [30]

Ours

88.51
91.73
88.34
88.36

49.8

80.97
84.76
80.13
79.57

51.2

76.74
77.92
75.04
74.55

54.65
-
46.51
54.64

47.9

43.5

48.36
-
39.03
44.27

37.1

44.98
-
35.60
40.23

77.64
-
78.35
82.48

36.6

62.8

64.10
-
61.94
64.10

47.1

58.00
-
55.70
56.90

47.2

Our method achieves similar AP on pedestrian and cyclist detection to other methods, which is
impressive taking into account the simplicity of our approach. Table 2 compares the performance
in terms of FPS and mAP. Our method is the only one that achieves real-time performance on a
CPU, and on an edge computing device.

Method

mAP

FPS (GPU) FPS(Jetson Nano) FPS (CPU)

Table 2: KITTI: mAP vs FPS.

VPFNet [31]
SFD [28]
IA-SSD [29]
3DSSD [30]

65.99
84.80 (Cars only)
62.29
65.01

10.0
10.2
83.0
26.3

Ours

47.02

120.1

4.2
5.1
8.5
5.3

90.0

0.4
1.1
3.5
1.2

15.2

Figure 5 illustrates the separability between ID and OOD samples. Plot includes 1000 samples
from IDs and OODs, respectively. Note that true partitions of ID and OOD are approximately
6% and 94%, respectively, in the validation split. Energy distribution of cars is much narrower
compered to other classes because the amount of car samples in the training set is signiﬁcantly
larger compared to other classes. This allows the network to learn the diﬀerence between cars and
OODs better.

8

Figure 5: Histogram of 1000 ID and 1000 OOD samples showcases separability.

3.3 Ablation study

Table 3.3 illustrates the contributions of the modules to the mAP and FPS(CPU). Modules improve
the mAP signiﬁcantly while decrease in FPS in negligible.

Table 3: Ablations of diﬀerent modules and their contribution to the mAP and the FPS. Abbre-
viations: PVLE –proposal voxel location encoder, IDPTM –ID pass-through module

PVLE(cls) PVLE(box)

1st IDPTM 2nd IDPTM mAP FPS (CPU)

-
(cid:88)
(cid:88)
-
-
(cid:88)
(cid:88)

-
-
(cid:88)
-
-
(cid:88)
(cid:88)

-
-
-
(cid:88)
(cid:88)
(cid:88)
(cid:88)

-
-
-
-
(cid:88)
-
(cid:88)

38.54
39.88
42.34
41.32
43.05
45.13
47.02

16.2
15.6
15.5
15.6
15.4
15.3
15.2

3.4 Ground segmentation

Results of a performance tests are summarized on Table 4. Our ground segmentation method is
compared to frequent and state-of-the-art methods in the literature, and it is clearly performing
better in terms of computational requirement, accuracy, and IoU. This is due to our eﬀective
sampling method, which reduces the iterations needed in the RANSAC function.

Table 4: Comparison of ground segmentation methods. Our results are achieved with a single core
of a 4 GHz CPU, and 32 even sectors on a 360◦ scan, * is for GPU, - not reported, bold is best.

Method

Dataset

Scans Time (s) Precision Recall Accuracy

HD [32]
LF [32]
GPF [32]
GPF-Opti [32]
GPF-RANSAC [32]
Hybrid-reg [33]
CNN-method [34]
CRF-method [35]
GndNet [36]

semKITTI
semKITTI
semKITTI
semKITTI
semKITTI
KITTI
Custom
semKITTI
semKITTI

23201
23201
23201
23201
23201
5
252
3040
3040

0.306
0.658
31.713
0.207
0.028
0.888
0.139
0,147
0.0180*

0.47
0.38
0.67
0.66
0.65
-
0.93
0.80
0.84

Ours

semKITTI

23201

0.00025

0.93

0.95
0.77
0.63
0.59
0.88
-
0.99
-
0.99

0.97

IoU

0.45
0.34
0.45
0.43
0.74
-
-
0.78
0.84

-
-
-
-
-
0.88
-
-
-

0.95

0.87

3.5 Qualitative results and discussion

Simple proposal generator reduces the computational requirement signiﬁcantly, while still achieving
good mAP. Simple proposal generator has another beneﬁt too. It allows data streaming i.e. data

9

20246Negative energy0.00.20.40.60.81.01.21.4FrequencyCarsPedestriansCyclistsOut-of-distributionThresholdcan be processed in small sectors, which means that processing can be started earlier compared to
full scan approaches. This will decrease the latency of the detection. The limitation of a simple
proposal generator is cluster fusion when objects are close to each other. PointNets work well as in
road user classiﬁcation and bounding box estimation tasks. They can be leveraged by the proposed
PVLE and energy-based OOD modules, without adding signiﬁcant amount of computation. Some
of the detections are visualized on Figure 6.

Figure 6: Example detections (zoom for better detail).

4 Conclusions

This paper presented a novel architecture for 3D road user detection task. An impressive 15.2
FPS was achieved with a CPU only implementation while having 47 mAP on KITTI dataset. The
architecture is based on a simple proposal generator and OOD PointNets, which leverage from a
novel PVLE-module. In the future, other OOD detection methods could be studied in the 3D road
user detection task.

References

[1] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object
detection. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 4490–4499, 2018.

[2] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection.

Sensors, 18(10):3337, 2018.

[3] Lue Fan, Xuan Xiong, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang. Rangedet:

In
defense of range view for lidar-based 3d object detection. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 2918–2927, 2021.

[4] Yuning Chai, Pei Sun, Jiquan Ngiam, Weiyue Wang, Benjamin Caine, Vijay Vasudevan, Xiao
Zhang, and Dragomir Anguelov. To the point: Eﬃcient 3d object detection in the range image
with graph convolution kernels. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 16000–16009, 2021.

[5] Zhidong Liang, Ming Zhang, Zehan Zhang, Xian Zhao, and Shiliang Pu. Rangercnn: To-
wards fast and accurate 3d object detection with range image representation. arXiv preprint
arXiv:2009.00206, 2020.

[6] Gregory P Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez, and Carl K Wellington.
Lasernet: An eﬃcient probabilistic 3d object detector for autonomous driving. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition, pages 12677–12686,
2019.

[7] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom.
Pointpillars: Fast encoders for object detection from point clouds.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12697–12705,
2019.

[8] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-time 3d object detection from point
clouds. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,
pages 7652–7660, 2018.

[9] Wu Zheng, Weiliang Tang, Li Jiang, and Chi-Wing Fu. Se-ssd: Self-ensembling single-stage
object detector from point cloud. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 14494–14503, 2021.

10

[10] Wu Zheng, Weiliang Tang, Sijin Chen, Li Jiang, and Chi-Wing Fu. Cia-ssd: Conﬁdent iou-
aware single-stage object detector from point cloud. arXiv preprint arXiv:2012.03015, 2020.

[11] Xuesong Li, Jose Guivant, and Subhan Khan. Real-time 3d object proposal generation
and classiﬁcation using limited processing resources. Robotics and Autonomous Systems,
130:103557, 2020.

[12] Irwin Sobel. An isotropic 3x3 image gradient operator. Presentation at Stanford A.I. Project

1968, 02 2014.

[13] Igor Bogoslavskyi and Cyrill Stachniss. Fast range image-based segmentation of sparse 3d
laser scans for online operation. In 2016 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pages 163–169. IEEE, 2016.

[14] Yiming Zhao, Xiao Zhang, and Xinming Huang. A technical survey and evaluation of tradi-
tional point cloud clustering methods for lidar panoptic segmentation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 2464–2473, 2021.

[15] Radu Bogdan Rusu. Semantic 3d object maps for everyday manipulation in human living

environments. KI-K¨unstliche Intelligenz, 24(4):345–348, 2010.

[16] Radu Bogdan Rusu and Steve Cousins. 3d is here: Point cloud library (pcl). In 2011 IEEE

international conference on robotics and automation, pages 1–4. IEEE, 2011.

[17] Jeremie Papon, Alexey Abramov, Markus Schoeler, and Florentin Worgotter. Voxel cloud
connectivity segmentation-supervoxels for point clouds. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 2027–2034, 2013.

[18] Dimitris Zermas, Izzat Izzat, and Nikolaos Papanikolopoulos. Fast segmentation of 3d point
clouds: A paradigm on lidar data for autonomous vehicle applications. In 2017 IEEE Inter-
national Conference on Robotics and Automation (ICRA), pages 5067–5073. IEEE, 2017.

[19] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on
point sets for 3d classiﬁcation and segmentation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 652–660, 2017.

[20] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum pointnets
for 3d object detection from rgb-d data. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 918–927, 2018.

[21] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-

based learning. Predicting structured data, 1(0), 2006.

[22] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution
detection. Advances in Neural Information Processing Systems, 33:21464–21475, 2020.

[23] Geoﬀrey E Hinton and Richard Zemel. Autoencoders, minimum description length and

helmholtz free energy. Advances in neural information processing systems, 6, 1993.

[24] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving?
the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern
recognition, pages 3354–3361. IEEE, 2012.

[25] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, J. Gall, and C. Stachniss. To-
wards 3D LiDAR-based semantic scene understanding of 3D point cloud sequences: The
SemanticKITTI Dataset. The International Journal on Robotics Research, 40(8-9):959–967,
2021.

[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980, 2014.

[27] Hanqi Zhu, Jiajun Deng, Yu Zhang, Jianmin Ji, Qiuyu Mao, Houqiang Li, and Yanyong
Zhang. Vpfnet: Improving 3d object detection with virtual point based lidar and stereo data
fusion. arXiv preprint arXiv:2111.14382, 2021.

[28] Xiaopei Wu, Liang Peng, Honghui Yang, Liang Xie, Chenxi Huang, Chengqi Deng, Haifeng
Liu, and Deng Cai. Sparse fuse dense: Towards high quality 3d detection with depth comple-
tion. Accepted to CVPR 2022, arXiv preprint arXiv:2203.09780, 2022.

11

[29] Yifan Zhang, Qingyong Hu, Guoquan Xu, Yanxin Ma, Jianwei Wan, and Yulan Guo. Not
all points are equal: Learning highly eﬃcient point-based detectors for 3d lidar point clouds.
Accepted to CVPR 2022, arXiv preprint arXiv:2203.11139, 2022.

[30] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd: Point-based 3d single stage ob-
ject detector. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 11040–11048, 2020.

[31] Chia-Hung Wang, Hsueh-Wei Chen, and Li-Chen Fu. Vpfnet: Voxel-pixel fusion network for

multi-class 3d object detection. arXiv preprint arXiv:2111.00966, 2021.

[32] Zhenchao Ouyang, Xiaoyun Dong, Jiahe Cui, Jianwei Niu, and Mohsen Guizani. Pv-enconet:
Fast object detection based on colored point cloud. IEEE Transactions on Intelligent Trans-
portation Systems, 2021.

[33] Kaiqi Liu, Wenguang Wang, Ratnasingham Tharmarasa, Jun Wang, and Yan Zuo. Ground
surface ﬁltering of 3d point clouds based on hybrid regression technique. IEEE Access, 7:23270–
23284, 2019.

[34] Martin Velas, Michal Spanel, Michal Hradis, and Adam Herout. Cnn for very fast ground
segmentation in velodyne lidar data. In 2018 IEEE International Conference on Autonomous
Robot Systems and Competitions (ICARSC), pages 97–103. IEEE, 2018.

[35] Lukas Rummelhard, Anshul Paigwar, Amaury N`egre, and Christian Laugier. Ground esti-
mation and point cloud segmentation using spatiotemporal conditional random ﬁeld. In 2017
IEEE Intelligent Vehicles Symposium (IV), pages 1105–1110. IEEE, 2017.

[36] Anshul Paigwar, ¨Ozg¨ur Erkent, David Sierra-Gonzalez, and Christian Laugier. Gndnet: Fast
ground plane estimation and point cloud segmentation for autonomous vehicles.
In 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2150–
2156. IEEE, 2020.

12

