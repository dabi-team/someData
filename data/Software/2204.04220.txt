2
2
0
2

r
p
A
8

]

G
L
.
s
c
[

1
v
0
2
2
4
0
.
4
0
2
2
:
v
i
X
r
a

Characterizing and Understanding the Behavior of Quantized
Models for Reliable Deployment

Qiang Hu
SnT, University of Luxembourg
Luxembourg

Yuejun Guo
SnT, University of Luxembourg
Luxembourg

Maxime Cordy
SnT, University of Luxembourg
Luxembourg

Xiaofei Xie
Singapore Management University
Singapore

Wei Ma
SnT, University of Luxembourg
Luxembourg

Mike Papadakis
SnT, University of Luxembourg
Luxembourg

Yves Le Traon
SnT, University of Luxembourg
Luxembourg

ABSTRACT
Deep Neural Networks (DNNs) have gained considerable attention
in the past decades due to their astounding performance in different
applications, such as natural language modeling, self-driving assis-
tance, and source code understanding. With rapid exploration, more
and more complex DNN architectures have been proposed along
with huge pre-trained model parameters. The common way to use
such DNN models in user-friendly devices (e.g., mobile phones)
is to perform model compression before deployment. However,
recent research has demonstrated that model compression, e.g.,
model quantization, yields accuracy degradation as well as outputs
disagreements when tested on unseen data. Since the unseen data
always include distribution shifts and often appear in the wild, the
quality and reliability of quantized models are not ensured. In this
paper, we conduct a comprehensive study to characterize and help
users understand the behaviors of quantized models. Our study
considers 4 datasets spanning from image to text, 8 DNN architec-
tures including feed-forward neural networks and recurrent neural
networks, and 42 shifted sets with both synthetic and natural distri-
bution shifts. The results reveal that 1) data with distribution shifts
happen more disagreements than without. 2) Quantization-aware
training can produce more stable models than standard, adversar-
ial, and Mixup training. 3) Disagreements often have closer top-1
and top-2 output probabilities, and ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› is a better indicator
than the other uncertainty metrics to distinguish disagreements.
4) Retraining with disagreements has limited efficiency in remov-
ing disagreements. We opensource our code and models as a new
benchmark for further studying the quantized models.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conferenceâ€™17, July 2017, Washington, DC, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

CCS CONCEPTS
â€¢ Computer systems organization â†’ Embedded systems; Re-
dundancy; Robotics; â€¢ Networks â†’ Network reliability.

KEYWORDS
datasets, neural networks, gaze detection, text tagging

ACM Reference Format:
Qiang Hu, Yuejun Guo, Maxime Cordy, Xiaofei Xie, Wei Ma, Mike Pa-
padakis, and Yves Le Traon. 2022. Characterizing and Understanding the
Behavior of Quantized Models for Reliable Deployment. In Proceedings
of ACM Conference (Conferenceâ€™17). ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Thanks to the massively available data released and powerful hard-
ware devices supported, Deep Learning (DL) gains considerable
attention and achieves even better performance than humans on
some tasks [54]. As the backbone of DL systems, Deep Neural Net-
works (DNNs) follow the data-driven paradigm to learn knowledge
from the labeled data automatically and make predictions for in-
coming unlabelled ones. Inspired by the usage of DNNs for natural
language processing, researchers also employ DNNs for source
code-related tasks, e.g., code summarization [4] and problem classi-
fication [47]. Correspondingly, the behavior, quality, and security of
DNNs are also concerned by the software engineering community.
A factor that limits the application of DNNs is that DNNs are
large and require strong computing resources. For example, the
famous language prediction model GPT-3 [7] has 175 billion pa-
rameters, which is hard to be deployed in our daily used devices.
For code tasks, the recently released model GraphCodeBERT [20]
occupies 124M of storage memory, which is also difficult to be
plugged in the generally used IDEs. Furthermore, with the rapid
research progress, more and more complex DNNs are developed,
which makes the DNN deployment even more challenging.

To solve this deployment issue, instead of directly transferring
DNNs to devices, one typical process is to reduce the size of DNN
models by model compression for lighter and easier deployment.
There are different ways to perform model compression, e.g., model
pruning which removes useless parameters from the model, and

 
 
 
 
 
 
Conferenceâ€™17, July 2017, Washington, DC, USA

Hu and Guo, et al.

model quantization which degrades float-level parameters to lower-
level parameters (integer-level). In general, the compression process
is of great importance and should preserve the performance of
original models as much as possible. The reason is that after a model
is compressed, it is hard to change it when unexpected problems
occur. For example, retraining a model deployed on a mobile device
is impractical because this model is packaged.

Unfortunately, recent research has revealed two problems of
model compression. First, [22] shows that a compressed model
could have a big accuracy difference (more than 5%) compared to
its original model. Second [57, 60] demonstrate that it is common to
find inputs that trigger different predictions by a compressed model
and its original model. As these study reveal, it remains unclear to
what extent model compression preserves prediction performance
and under which conditions. The existing literature currently lacks
a detailed assessment of these conditions and this lack, in turn,
impedes the reliable application of compression techniques.

In this paper, we fill this gap and empirically characterize the
behavior of compressed models under various experimental set-
tings in order to better understand the limitations of compression
techniques. We specifically consider quantization as this approach
is mostly applied in practice [9]. We focus our study on the DL
models quantized by TensorFLowLite [3] and CoreML [55] which
are widely adopted in the industry. For example, Google uses Ten-
sorFLowLite for model deployment on Android devices and Apple
applies CoreML for IOS devices. In total, our experimental settings
include 4 datasets ranging from image to text, 8 different DNNs
including both Feed-forward Neural Networks (FNNs) and Recur-
rent Neural Networks (RNNs), 42 different sets with both synthetic
and natural distribution shifts. With this material, we explore four
research questions that existing studies have overlooked:

RQ1: How do quantized models react to distribution shifts?
Real applications of DL systems often witness data distributions
shifts â€“ changes in data distribution that typically cause drops in
model performance [35]. Given the practical predominance of this
phenomenon, research [5, 12, 30] has emphasized the need to con-
sider distribution shift when evaluating DL models. We, therefore,
study the impact of model quantization in the case of distribution
shifts. We evaluate the quantized models against two types of dis-
tribution shift datasets: synthetic (based on image transformations)
and natural (reported in the literature). We compare the original and
the quantized models in terms of accuracy difference and predicted
label differences, i.e. disagreements.

RQ2: How does the training strategy influence the behav-
ior of quantized models? We explore the influence of different
training strategies: standard training which is the basic way to
prepare pretrained model, quantization-aware training [33] which
is specifically designed for model quantization, adversarial training
[19] and mixup training [61], which are the commonly used data
augmentation training strategies. We apply each strategy to train
original models and then quantize these models. We compare the
pairs of models in terms of accuracy difference and disagreements.
RQ3: What are the characteristics of the data on which
original and quantized models disagree? We aim to find dis-
criminating factors that can help identify the disagreement inputs.
In particular, we investigate whether the most uncertain data are

the most likely to produce disagreements. Based on different un-
certainty metrics, we train simple classifiers based on logistic re-
gression and evaluate their capabilities to predict disagreements.

RQ4: Can model retraining reduce disagreements? We in-
vestigate whether retraining â€“ a common approach to improve DL
models â€“ can efficiently fix disagreements. Specifically, we explore
whether retraining the original model (for additional epochs) with
disagreement inputs can help preserve the knowledge of these in-
puts through the quantization process, and make the quantized
model classify these inputs correctly.

In summary, the main novel contributions of this paper are:
â€¢ We show that synthetic distribution shift has a significant impact
on quantized models; it increases the accuracy change by up to
3.03% and the percentage of disagreements by 5.28%.

â€¢ We empirically confirm that quantization-aware training is the
best method to alleviate performance loss and disagreements
after quantization, including when distribution shifts occur.
â€¢ We demonstrate that data uncertainty â€“ as captured by the Mar-
gin metric â€“ is a suitable factor to discriminate disagreement
data. A simple classifier based on Margin reaches an AUC-ROC
of 0.63 to 0.97.

â€¢ We illustrate that retraining on disagreement inputs does not
decrease the total level of disagreements between original and
quantized models, because it has the side effect of introducing
new disagreements.

We release all our code, models (before and after quantization)
and benchmarks in order to support future research studying and
improving model quantization1.

2 BACKGROUND
2.1 Deep Learning
Deep learning [18] is a machine learning technique that uses inter-
mediate layers to progressively obtain knowledge from raw data,
and deep neural networks form the backbone of deep learning. A
typical deep neural network consists of an input layer, several hid-
den layers, and an output layer. Each layer includes neurons that
mimic the neurons in human brains and undertake specific com-
putations, such as sigmoid and rectifier. The connections between
successive layers establish the data flow. In brief, training a deep
neural network is to tune the parameters (importance of neurons)
of the connections, and testing is to ensure accuracy and reliability
during deployment in real-world applications.

2.2 Model Quantization
Model quantization is an optimization technique that aims at trans-
forming the higher-bit level weights to lower-bit level weights, e.g.,
from float32 weights to 8-bit integer weights, to reduce the size of
the model for an easy model deployment. Multiple quantization
approaches [32, 33, 38, 53] have been proposed given its impor-
tance in DL-based engineering. An important part of quantization
methods is the mapping between the two parts of weights. This
mapping can be constructed by using a simple linear function to
find the scale for two levels of weights, or by different clustering

1https://github.com/Anony4paper/quan_study

Characterizing and Understanding the Behavior of Quantized Models for Reliable Deployment

Conferenceâ€™17, July 2017, Washington, DC, USA

metrics (e.g., k-means cluster used in CoreML) to find the lookup
table quantization of weights.

Figure 1: An example of weights quantization. Each weight
in Float32 format is converted into Int8.

Figure 1 gives an example of the basic linear quantization. We as-
sume the left side is the float32-level weights, and the range of these
weights is [-1, 1]. We plan to convert the float weights to 8-bit inte-
ger weights ranging in [-128, 127]. Thus, the ğ‘ ğ‘ğ‘ğ‘™ğ‘’ here is 128 and
the quantized weights is calculated by ğ‘…ğ‘œğ‘¢ğ‘›ğ‘‘ (ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘ ğ‘“ ğ‘™ğ‘œğ‘ğ‘¡ /ğ‘ ğ‘ğ‘ğ‘™ğ‘’).
Generally, to further reduce the memory usage of quantized models,
the quantization only keeps positive weights.

2.3 Distribution Shift
Distribution shift refers to the change of data distribution in the
test dataset compared to the training dataset. Generally, benchmark
datasets [36, 44] are designed to include training and test data fol-
lowing the same distribution. However, in real-world deployments,
the test data can be from the same or a different distribution, which
raises the security concern [5]. Generally speaking, there are two
types of distribution shifts, synthetic and natural [35].

Synthetic distribution shift considers possible perturbations in
the real world. In addition, concerning the severity of corruption,
data can have various levels of noise, which covers many different
situations. As a result, synthetic distribution shift is always taken
as a starting point to evaluate the performance of a DNN under
different settings. A wide range of visual corruptions has been
developed in the image domain [24, 46]. For example, adding motion
blur into an image can mimic the scenario of a moving object, and
inserting the fog effect can simulate the condition of foggy weather.
Differ from synthetic shift, natural distribution shift comes from
natural variations in datasets. For instance, in the widely used
text dataset, IMDb [44], data (movies reviews) are collected from
IMDb. When testing, the reviews can be from another movie review
website or from a different customer groups.

3 OVERVIEW
3.1 Study Design
Figure 2 gives an overview of our study. Following the common
DL systems development process, we prepare the original model
DNN by standard model training (Section 3.4) using the collected
datasets. Then, we use quantization techniques (e.g., TensorflowLite,
CoreML) to compress the model and prepare the optimized model
DNNâ€™ for further deployment. Afterward, to study whether the
quantization is reliable or not, we prepare two types of test data,
the ID test set and OOD test set. Remark that the ID test set is
the original test data from each dataset, which is in distribution
compared to the training data. The OOD test set is the data with

distribution shifts. We compare the performance of the original
DNN and compressed DNNâ€™ on these two types of test sets and
check the differences to answer RQ1. In our study, we consider two
types of distribution shifts, synthetic and natural.

In the development phase, in addition to standard training, other
training strategies can be used to prepare pre-trained models. Thus,
it is essential to explore the potential factor that could influence
the behaviors of quantized models â€“ training strategy. We utilize
3 additional training strategies to train the original models and
then analyze the behaviors of the quantized model to answer RQ2.
Specifically, we include quantization-aware training [33], which is
specifically designed for solving the problem of accuracy decline
after quantization, adversarial training [19] and Mixup training
[61] which aim to improve the generalization of a DNN model.

After analyzing the behaviors of quantized models, we obtain
multiple models that are waiting for repair with their disagreements.
Before trying to remove the disagreements and repair the quantized
models, the first step should be to investigate the properties of the
data that cause disagreements between DNN and DNNâ€™. We utilize
the uncertainty metric as an indicator to check if it can represent
the properties of disagreements and answer RQ3. Specifically, for
each test dataset (ID/OOD) and model, we collect all the disagree-
ments that have at least once been predicted differently by the
original model and the quantized model. Then, we randomly select
the same number as the disagreements of normal inputs where the
predictions before and after quantization are consistent. Afterward,
we obtain the output probabilities of these two (disagreements and
normal inputs) sets and calculate their uncertainty scores by differ-
ent uncertainty metrics as the input data of the logistic regression
classifier. We assign the label of disagreement and normal input as
1 and 0, respectively. We then combine and shuffle the two sets and
split them into training data and test data following the ratio 9:1.
Finally, we train the classifier using the training data and calculate
the AUC-ROC score of the classifiers using the prediction of test
data with a threshold of 95%. The AUC-ROC score is used to de-
termine the best uncertainty metric that is discriminative between
disagreements and normal inputs significantly.

Finally, we make the first step to repair the quantized model
for reliable model deployment. We verify if model retraining is
helping to alleviate disagreements to answer RQ4. Model retraining
is the most straightforward and commonly used method during
deployment to specifically let a pre-trained model work on unlearnt
features [27]. However, its effectiveness on model quantization is
uncovered. After retraining, we follow the same procedure as RQ1
to produce the quantized model and check if the disagreements
decreased. Remarkably, we consider both the existing and newly
generated disagreements.

3.2 Datasets and Models
Table 1 presents the details of datasets and models. In this study,
we consider 4 widely studied datasets over image and text domains.
For each dataset, we build two different models. More specifically,
MNIST [36] is a gray-scale image dataset containing digit numbers
from 0 to 9. We train LeNet-1 and LeNet-5 from the LeNet [36]
family. CIFAR-10 contains color images of airplanes and birds. For
this dataset, we build two models, Network in network (NiN) [40]

0.2-0.10.30.40.2-0.5-0.20.60.326-1338512664-267738QuantizationFloat32Int8Conferenceâ€™17, July 2017, Washington, DC, USA

Hu and Guo, et al.

Figure 2: Overview of the experimental design.

Table 1: Details of datasets and DNNs

Dataset

MNIST

CIFAR-10

iWildCam

IMDb

DNN
LeNet-1
LeNet-5
ResNet20
NiN
ResNet-50
Densenet-121
LSTM
GRU

Classes Training

ID Test Accuracy (%)

10

10

60000

10000

50000

10000

182

129809

8154

2

5000

5000

98.62
98.87
87.44
88.27
75.78
76.01
83.78
83.14

OOD Test
MNIST-C
(Synthetic)
CIFAR-10-C
(Synthetic)
Camera Traps
(Natural)
CR, Yelp
(Natural)

except 8-bit integer quantization for RNNs [1]. In our experiments,
we only apply 16-bit float quantization for IMDb-related models.
CoreML [55] is an Apple framework that converts models from
third-party frameworks (e.g., TensorFlow and Pytorch) to Mlmodel.
Mlmodel is a specific deep learning model format for IOS platforms.
CoreML also provides post-training quantization interfaces to com-
press models. Differ from TensorflowLite, CoreML supports all bits
level quantization for all types of DNNs.

and ResNet-20 [23]. iWildCam is a dataset from the distribution
shift benchmark Wilds [35]. It consists of color images of different
animals, e.g., cow, wild horse, and giraffe. We follow the recommen-
dation of the benchmark to build ResNet-50 [23] for iWildCam and
add one more model, DenseNet-121 [31], in our study. IMDb [44]
is a text dataset collected from the popular movie review website
IMDb. This dataset is mainly used for sentiment analysis, i.e., the
reviewer holds a positive or negative opinion in a movie. We build
two well-known RNN models, LSTM [26] and GRU [11], for IMDb.
Test data with distribution shift. For synthetic distribution
shift, we test on MNIST and CIFAR-10 with benchmark datasets
MNIST-C [46] and CIFAR-10-C [24], respectively. Both benchmarks
include several groups of noisy images synthesized by different
image transformation methods, e.g., image rotation and image scale.
MNIST-C contains 16 types of transformations and CIFAR-10-C
has 19 types. For natural distribution shift, we test on iWildCam
and IMDb using the Wilds benchmark. The distribution shift comes
from the change of camera traps in iWildCam and the difference in
websites and customers in IMDb.

3.3 Quantization Techniques
TensorflowLite [3] is a component of the deep learning framework
â€“ TensorFlow, which is developed and maintained by Google. It
provides interfaces to covert TensorFlow models into Lite models to
promote the deployment in different low-computing devices, such
as Android mobile phones. Currently, TensorFLowLite supports
both 8-bit integer and 16-bit float quantizations for most DNNs

3.4 Training Strategies
In addition to standard training, we consider three representative
training strategies from different perspectives, quantization-aware
[33], adversarial [19], and Mixup [61].

Standard training is the baseline to evaluate the other training
strategies. In this setting, we train the model without any modifica-
tion in the model (e.g., quantization-aware) or data (e.g., Mixup).

Quantization-aware training is designed by the TensorFlow
group, which is used for preserving the accuracy of the model after
post-training quantization in the training process. It simulates the
quantization effects in the forward pass of training. Namely, during
training, the parameters of the model will be updated by both the
normal operations and the injected quantization operations. In this
way, the trained model can learn the knowledge for quantization.
Adversarial training is one of the most effective defenses for
promoting model robustness by adversarially data augmentation.
Compared to standard training, adversarial examples crafted from
raw inputs are fed to train the model during each epoch. As a result,
the training dataset is augmented successively.

Mixup training is a data augmentation technique that gener-
ates new samples by weighted combinations of random training
data and their labels. It has been empirically proved to be effective
in improving the generalization of DNNs and has several variants,
such as AugMix [25]. In this paper, we consider the original Mixup.

TrainDNNTrainingSetIDTestSetOutputQuantizationDNNâ€™TestTestShiftOODTestSetOutputDiff?DisagreementSetRetrainAnalysisReportRQ4:Effectiveness of model retrainingStandardQuantization-awareAdversarialMixupTensorFlowLiteCoreMLMotion blurFogCustomerEtc.RQ3:Characteristic of disagreementsRQ1:Behavior of quantization modelsRQ2:Influence of training strategyDevelopmentDeploymentResearchDirectionsCharacterizing and Understanding the Behavior of Quantized Models for Reliable Deployment

Conferenceâ€™17, July 2017, Washington, DC, USA

3.5 Evaluation Measures
We consider both the accuracy and disagreement to evaluate
the performance of DNNs, and use AUC-ROC to evaluate the
performance of logistic regression classifiers.

Accuracy is the basic criterion to quantify the quality of a DNN

model, which refers to the ratio of correct predictions.

Number of disagreements is defined in [60] to characterize
the difference between two DNNs. A disagreement is an input that
triggers different outputs by the original model and its quantized
version. By measuring the number of disagreements in the test data,
one can observe the modelâ€™s behavior change after quantization.
Area Under the Receiver Operating Characteristic Curve
(AUC-ROC) [13] is a threshold-independent performance evalu-
ation metric. In RQ3, we utilize AUC-ROC score to measure the
performance of the trained logistic regression classifiers.

3.6 Uncertainty Metrics
In RQ3, we utilize uncertainty metrics to estimate the characteris-
tics of the disagreement inputs. Following previous studies [28, 43],
we select 4 commonly used output-based uncertainty metrics in
our study. Given a classification task, let ğ·ğ‘ ğ‘ be a ğ¶-class model
and ğ‘¥ be an input. ğ‘ğ‘– (ğ‘¥) denotes the predicted probability of ğ‘¥
belonging to the ğ‘–th class, 0 â‰¤ ğ‘– â‰¤ ğ¶. Entropy score [52] quan-
tifies the uncertainty of ğ‘¥ by Shannon entropy: ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ (ğ‘¥) = -
(cid:205)ğ¶
ğ‘ğ‘– (ğ‘¥) log ğ‘ğ‘– (ğ‘¥). Gini [14] score is calculated as: ğºğ‘–ğ‘›ğ‘– (ğ‘¥) =
ğ‘–=1
1 âˆ’ (cid:205)ğ¶
ğ‘–=1 (ğ‘ğ‘– (ğ‘¥))2. ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› [58] score is based on the top-2 pre-
diction probabilities: ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘›(ğ‘¥) = ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› (ğ‘¥) = ğ‘ğ‘˜ (ğ‘¥) âˆ’ ğ‘ ğ‘— (ğ‘¥),
(ğ‘ğ‘– (ğ‘¥)). Least Con-
(ğ‘ğ‘– (ğ‘¥)) and ğ‘— = arg max
where ğ‘˜ = arg max
ğ‘–={1:ğ¶ }/ğ‘˜

ğ‘–=1:ğ¶

fidence (LC) [51] score is the difference between the most con-
fident prediction and 100% confidence. ğ¿ğ¶ (ğ‘¥) = 1 - ğ‘ğ‘˜ (ğ‘¥), where
ğ‘˜ = arg max

(ğ‘ğ‘– (ğ‘¥)).

ğ‘–=1:ğ¶

4 CONFIGURATION
Environments. We undertake model training and retraining on an
NVIDIA Tesla V100 16G SXM2 GPU. For the TensorFlowLite model
evaluation, we run experiments on a 2.6 GHz Intel Xeon Gold 6132
CPU. For the CoreML model evaluation, we conduct experiments
on a MacBook Pro laptop with macOS Big Sur 11.0.1 with a 2GHz
GHz QuadCore Intel Core i5 CPU with 16GB RAM.

Quantization. We apply the interfaces provided by Tensor-
FLowLite and CoreML to accomplish post-training model quantiza-
tion. For IMDb-related models, we only apply 16-bit float quantiza-
tion by TensorFlowLite and utilize both 8-bit interger and 16-bit
float quantization by CoreML. For other models, we conduct 8-bit
integer and 16-bit float quantization using both techniques.

Model training. For the quantization-aware training, we mask
layers (e.g., BatchNormalization layer) that are not supported by
the current TensorFlow framework. In addition, since TensorFlow
does not support RNNs [2], we skip IMDb-related models in this
experiment. Regarding the adversarial training, we employ the com-
monly used PGD-based [45] adversarial training for image datasets,
and PWWS-based [48] adversarial training for text datasets. Con-
cerning the Mixup training, we follow the recommendation by the
original paper to set the mixup parameter ğ›¼ as 0.2.

Model retraining. Following the same setting from the empiri-
cal study of model retraining [27], we add all disagreements into
original training data to train the pre-trained model with additional
several epochs (5 epochs for MNIST, IMDb, and iWildsCam, 10
epochs for CIFAR-10).

All the detailed configurations can be found at our project site 1.

5 EXPERIMENTAL RESULTS
In this section, we report the experimental results to answer each
research question. Meanwhile, we highlight our novel findings.

5.1 RQ1: Behavior of Quantized Models
Table 2 presents the results of the behaviors of quantized models
on ID test data and OOD test data with synthetic distribution shifts.
Concerning the accuracy change, the accuracy is supposed to de-
grade due to the loss of information during quantization, which
is also demonstrated by the existing studies [22, 28]. Surprisingly,
the results also show almost 30% of (86 out of 292) opposite cases
where quantized models hold higher accuracy than their original
models. Particularly, in the case of ğ‘…ğ‘’ğ‘ ğ‘ ğ‘’ğ‘¡20, ğ‘ğ‘œğ‘œğ‘š_ğ‘ğ‘™ğ‘¢ğ‘Ÿ , the quan-
tized model has an improvement of 1.98%. On the other hand, this
phenomenon also happens to the natural distribution shift (10 out
of 16 cases in Table 3). Regarding shifted data as natural adversarial
examples, our finding confirms the conclusion from a recent re-
search [15] that the quantization process can be useful to promote
the modelâ€™s adversarial robustness. In addition, the distribution
shift can lead to larger change and should be taken into account
during deployment. For example, in MNIST, TF-8, the quantized
model has an accuracy change of 0.04% on ID test data but 0.78
under the Fog shift (Table 2). And comparing the ID and OOD test
sets, we found the synthetic distribution shift can increase the ac-
curacy change by up to 3.03% (ResNet20-Gaussian_noise-CM-8).
Finding 1: Post-training model quantization does not always harm
the accuracy of original models. Distribution shift can cause a large
accuracy change compared to testing on ID data.

Concerning the disagreement, even if the quantized model main-
tains accuracy, there may exist disagreements. For example, in the
case of LeNet1, CM-8, the accuracy change is 0, but the number
of disagreements is 6. Even worse, in DenseNet-121, CM-16, 216
disagreements appear without any accuracy change. This calls for
the attention that the behaviors of quantized models can not be
exactly reflected by only comparing the test accuracy. Thus, during
deployment, using accuracy only to evaluate the quality and relia-
bility of quantized DNNs is insufficient. Finding 2: Disagreements
may exist even if quantized models maintain the accuracy.

Moreover, comparing the number of disagreements from the ID
test data and OOD test data, we observe that the distribution shift
tends to lead to more disagreements. In 82% cases (241 of 294), the
number of disagreements from OOD test data is greater than from
ID test data, the difference can be by up to 5.28% (LeNet1, Fog, TF-8).
However, after the model has been deployed and used in the wild,
test data are more likely to have distribution shifts which raises a
big concern that model quantization may bring unexcepted errors.
Finding 3: Model quantization is sensitive to the distribution shift
where more disagreements happen.

Conferenceâ€™17, July 2017, Washington, DC, USA

Hu and Guo, et al.

Table 2: Behavior of quantized models under synthetic dis-
tribution shift. Non-highlighted value: accuracy change (%),
highlighted value: number of disagreements. A low value
indicates a small difference between the original and quan-
tized models. ID refers to the ID test data and the others are
OOD test data. TF: TensorFlowLite. CM: CoreML. |Average|:
the average of absolute changes.

Test Data

ID
Brightness
Canny_edges
Dotted_line
Fog
Glass_blur
Identity
Impulse_noise
Motion_blur
Rotate
Scale
Shear
Shot_noise
Spatter
Stripe
Translate
Zigzag
|Average|

LeNet1

MNIST

TF-8

TF-16

CM-8

LeNet5

TF-8

TF-16

-0.04
0.51
0.77
-0.21
0.78
-0.05
-0.04
-0.23
0.14
-0.07
-0.28
0
-0.06
-0.05
-0.42
-0.18
0.03
0.23

14
172
172
38
542
41
14
77
79
62
102
22
26
31
113
159
88
103

-0.04
0.28
0.5
-0.06
0.11
-0.04
-0.04
0.06
0.08
-0.03
-0.15
0
-0.02
0.06
-0.1
-0.08
-0.03
0.10

9
67
86
24
133
18
9
28
29
30
43
8
11
13
70
70
41
41

CM-8
0
-0.04
0.02
-0.03
-0.17
-0.05
0
-0.11
-0.07
0
-0.04
0.01
0
0.02
0.18
-0.04
-0.01
0.05

2
53
51
8
112
10
2
33
20
13
29
11
10
6
103
67
34
33

TF-8

TF-16

CM-8

NiN

CM-16
0
0
7
0.01
4
-0.01
0
0
6
0
0
0
0
0
4
-0.04
1
0.01
2
-0.02
0
0
2
0.01
0
0
0
0
6
-0.04
4
0
7
-0.04
3
0.01

0.02
-0.27
0.16
-0.01
0.31
0.09
0.02
-0.02
0.18
-0.11
-0.05
0
0.06
0.04
-0.03
0.15
-0.06
0.09
CIFAR-10

4
175
73
26
321
44
4
50
59
30
53
22
16
14
88
135
66
69

0.01
-0.56
-0.06
-0.06
-0.43
-0.1
0.01
-0.13
-0.11
-0.09
-0.02
-0.03
-0.01
-0.02
-0.03
-0.05
-0.17
0.11

3
100
35
13
112
23
3
35
23
18
22
11
7
10
36
64
34
32

0.01
-0.79
-0.01
-0.04
-0.6
-0.03
0.01
0.01
-0.01
-0.05
-0.02
-0.01
-0.03
0
-0.19
0
-0.08
0.11

1
100
17
12
120
17
1
23
17
10
8
5
6
8
53
47
30
28

ID
-0.95
Brightness
-0.02
Contrast
0.04
Defocus_blur
-0.1
Elastic_transform 0.03
Fog
-0.02
Frost
0.02
Gaussian_blur
-0.05
Gaussian_noise
0.06
Glass_blur
-0.36
Impulse_noise
-0.28
Jpeg_compression -0.26
Motion_blur
Pixelate
Saturate
Shot_noise
Snow
Spatter
Speckle_noise
Zoom_blur
|Average|

0
0.03
-0.13
-0.06
-0.14
-1.06
-1.89
0.6
0.31

514
70
78
51
107
57
81
56
96
164
108
82
87
84
89
105
698
604
631
883
232

-0.1
-0.01
-0.06
-0.05
0.02
0.02
-0.05
-0.06
-0.03
-0.42
-0.34
-0.15
-0.06
0
0
-0.08
-0.03
-0.02
-0.12
-0.07
0.08

24
50
42
34
65
29
53
29
57
122
86
55
43
49
49
60
67
43
49
56
53

0.03
0.03
-0.06
-0.06
0.06
-0.06
0
-0.04
0.06
-0.02
-0.11
-0.21
0.14
-0.06
-0.05
-0.09
-0.02
0.08
-0.06
0.1
0.07

45
44
48
37
70
37
50
36
65
90
82
57
67
53
49
69
66
52
65
77
58

CM-16
7
0.02
9
0.01
6
-0.04
5
-0.04
10
0.01
6
-0.03
10
0
5
-0.02
8
-0.06
20
-0.08
18
-0.05
12
-0.05
13
-0.05
6
-0.01
8
-0.02
8
-0.02
5
-0.02
4
0.02
5
0.01
12
-0.04
9
0.03

ResNet20

TF-8

TF-16

CM-8

0.04
-0.02
-0.12
-0.21
0.02
-0.07
-0.28
-0.18
-0.35
-0.09
-0.12
-0.15
0.46
0.08
-0.16
-0.18
-0.56
-0.29
-0.34
0.25
0.20

456
190
250
205
342
236
260
207
343
559
275
259
336
251
262
302
255
228
269
415
295

-0.4
0.1
0.04
0.01
0
0.01
0.05
0.05
0.06
0.12
0.04
-0.08
-0.04
0.05
-0.02
0.03
0.04
0.06
-0.01
-0.04
0.06

54
51
49
53
84
45
68
60
106
168
72
75
83
61
66
79
68
64
70
125
75

0.36
-0.08
0.02
0.02
0.83
-0.07
-0.76
0.04
-2.67
-1.56
-1.1
-0.94
1.69
-0.32
0.11
-2.01
-0.84
-0.38
-1.98
1.98
0.89

181
170
187
175
302
212
317
161
499
754
329
315
325
239
238
409
291
239
398
392
307

CM-16
0
0
6
0.03
2
0.02
0
0
8
0.01
0
0
0
0
3
-0.03
1
0.01
0
0
0
0
0
0
0
0
1
-0.01
1
0
2
-0.01
1
-0.01
1
0.01

CM-16
7
0.03
5
-0.03
10
-0.06
15
-0.03
18
-0.05
16
0
14
0.01
12
0.03
29
-0.15
57
-0.14
20
-0.01
21
-0.06
21
-0.02
10
-0.02
22
-0.01
23
-0.12
23
-0.08
15
-0.01
13
-0.04
24
0.01
19
0.05

predict 188 more data than TensorFlowLite-quantized model, which
is a considerable difference. In 8-bit integer quantization, CoreML
can still outperform TensorFlowLite in most cases (4 out of 6). Ad-
ditionally, we found an extreme case (iWildCam, DensetNet-121)
where the accuracy of quantized models by both techniques drops
a lot. This finding raises the concern that both quantization tools
have room for improvement and require a thorough test. On the
other hand, concerning the number of disagreements, the models
quantized by CoreML have fewer disagreement inputs than those
by TensorFlowLite in most cases (13 out of 14). Finding 4: In post-
training model quantization, compared to TensorFlowLite, CoreML
maintains the accuracy better as well as causes fewer disagree-
ments.

Answer to RQ1: Under synthetic distribution shift, the accuracy
change and number of disagreements between the original and
quantized models increase by up to 3.03% and 5.28%. Regardless
of the dataset, DNN, and distribution shift, CoreML keeps the
behaviors of original DNNs better than TensorFlowLite during
deployment.

5.2 RQ2: Influence of Training Strategy
In this section, we explore how different training strategies influ-
ence the behaviors of quantized models. Due to the space limita-
tion, we only report the results of one model from each dataset
(MNIST-LeNet5, CIFAR-10-ResNet20, IMDb-LSTM, and iWildsCam-
ResNet50). The whole results are available at our project site.

Table 3: Behavior of quantized models under natural dis-
tribution shift. Non-highlighted value: accuracy change (%),
highlighted value: number of disagreements. A low value
indicates a small difference between the original and quan-
tized models. ID refers to the ID test data and the others are
OOD test data. TF: TensorFlowLite. CM: CoreML. |Average|:
the average of absolute changes.

Test Data

DenseNet-121

TF-8

TF-16

CM-8

ID
OOD
|Average|

-18.96
-10.91
14.94

2830
14279
8555

-0.12
-0.42
0.27

167
1105
636

-8.34
-5.18
6.76

2035
11095
6565

LSTM

TF-8

TF-16

CM-8

ID
CR
Yelp
|Average|

-
-
-
-

-
-
-
-

-0.08
-0.04
-0.12
0.08

8
8
14
10

0
-0.06
0.02
0.03

6
9
7
7

iWildCam

CM-16
34
216
125

0.04
0
0.02
IMDb

CM-16
0
0
0
0
0
0
0
0.00

ResNet50

TF-8

TF-16

CM-8

CM-16

0
1.09
0.55

326
2158
1242

-0.11
0.6
0.35

187
1270
729

-0.21
-0.33
0.27

226
1811
1019

0.06
-0.01
0.04

16
128
72

GRU

TF-8

TF-16

CM-8

-
-
-
-

-
-
-
-

-0.06
0.28
0.06
0.13

3
30
9
14

0.04
0.2
0.08
0.11

2
24
8
11

CM-16
0
0
1
-0.02
0
0
0
0.01

Next, we compare the two quantization techniques concerning
the accuracy change. On average, regardless of the dataset, DNN,
and quantization level, CoreML produces more stable quantized
models (smaller change) than TensorFlowLite in most cases (12
out of 14). Concretely, in 16-bit float quantization, CoreML always
outperforms TensorFlowLite. Take iWildCam, DenseNet-121 as an
example, in 16-bit level quantization, the average accuracy change
is 0.27% by TensorFlowLite but only 0.02% by CoreML. This differ-
ence 0.25% could cause the CoreML-quantized model to correctly

(a) MNIST

(b) CIFAR-10

(c) IMDb

(d) iWildCam

Figure 3: Accuracy (%) of models (before quantization)
trained by different training strategies. ID represents the ac-
curacy on ID test datasets, and the others are on OOD test
datasets. Stan: standard training. QA: quantization-aware
training. Adv: adversarial training. Mixup: Mixup training.

IDbrightnesscanny_edgesdotted_linefogglass_bluridentityimpulse_noisemotion_blurrotatescaleshearshot_noisespatterstripetranslatezigzag406080StanQAAdvMixupIDbrightnesscontrastdefocus_blurelastic_transformfogfrostgaussian_blurgaussian_noiseglass_blurimpulse_noisejpeg_compressionmotion_blurpixelatesaturateshot_noisesnowspatterspeckle_noisezoom_blur406080StanQAAdvMixupIDCRYelp6080StanAdvMixupIDOOD010203040506070StanQAAdvMixupCharacterizing and Understanding the Behavior of Quantized Models for Reliable Deployment

Conferenceâ€™17, July 2017, Washington, DC, USA

(a) MNIST, TensorFlowLite

(b) MNIST, CoreML

(c) CIFAR-10, TensorFlowLite

(d) CIFAR-10, CoreML

(e) IMDb, TensorFlowLite

(f) IMDb, CoreML

(g) iWildCam, TensorFlowLite

(h) iWildCam, CoreML

Figure 4: The disagreement change of models trained by different training strategies compared to by standard training. ID: ID
test datasets, and the others are OOD test datasets. QA: quantization-aware training. Adv: adversarial training. Mixup: Mixup
training. ğ‘¦âˆ’axis: the difference of the number of disagreements between a training strategy and the standard training.

First, we evaluate the performance of each training strategy
concerning the distribution shift before model quantization. Figure
3 shows the results. Under synthetic distribution shift, for MNIST,
there are 12, 5, 6 cases out of 17 that using quantization-aware,
adversarial, and Mixup training, respectively, improve the accuracy
compared to using standard training. While the result for CIFAR-10
changes to 5, 8, and 12 cases of 20 correspondingly. We conclude that
none of these three training strategies can consistently deal with the
issue of accuracy degradation under synthetic distribution shifts. On
the other hand, under natural distribution shift, interestingly, when
performing adversarial training for IMDb models, the accuracy
of models on both distribution shifted datasets (ğ¶ğ‘… and ğ‘Œğ‘’ğ‘™ğ‘) has
been improved. We conjecture that the features of text adversarial
examples are more likely to appear in the OOD test dataset. For
example, the original sentence "a wonderful...are terribly well done
and its adversarial sentence "a wonderful...are terribly considerably
perform" only have two-word difference, but the model predicts
differently. We found that the words considerably and perform are
both in the vocabulary of OOD data. For iWildCam, only the Mixup
training can improve the accuracy of models on shifted data. Find-
ing 5: Concerning the accuracy, none of the three (quantization-
aware, adversarial, Mixup) training strategies has a significant ad-
vantage over standard training before quantization.

Second, we check the accuracy change of each model trained
by different training strategies after quantization. Table 4 presents
the results of the average accuracy change of all test datasets of
each model. Compared to standard training, the quantized models
by using the quantization-aware training are more stable where
the accuracy change in most cases (10 out of 12) is the same as or
smaller. For example, in CIFAR-10, CM-8, by standard training, the
quantized model has an average of 0.89% difference compared to
its original model. However, by quantization-aware training, the

Table 4: Average accuracy change (%) of models by quantiza-
tion. The average is of 17, 20, 3, and 2 test datasets of MNIST,
CIFAR-10, IMDb, and iWildCam, respectively. A low average
value indicates a small difference between the original and
quantized models. Highlighted values indicate that the ac-
curacy change by the corresponding training strategy is the
same as or smaller than by standard training. TF: Tensor-
FlowLite. CM: CoreML. Baseline: standard training.

Dataset

Quantization

Training Strategy
Standard Quantization-aware Adversarial Mixup

MNIST

CIFAR-10

IMDb

iWildCam

TF-8
TF-16
CM-8
CM-16
TF-8
TF-16
CM-8
CM-16
TF-16
CM-8
CM-16
TF-8
TF-16
CM-8
CM-16

0.09
0.11
0.11
0.01
0.20
0.06
0.89
0.05
0.08
0.03
0.01
0.55
0.35
0.27
0.04

0.06
0.01
0.06
0.01
0.77
0.06
0.05
0.03
-
-
-
0.47
0.05
0.19
0.11

0.36
0.09
0.11
0.01
1.02
0.21
0.20
0.16
0.03
0.03
0.01
0.24
0.10
0.04
0.04

0.53
0.24
0.09
0.02
1.48
0.15
0.12
0.04
0.05
0.03
0.01
0.29
0.07
0.62
0.06

difference can decline to only 0.05%. By contrast, both adversarial
and Mixup training can result in more stable (11 out of 15, 8 out
of 15 cases) quantized models than standard training but not as
well as quantization-aware training. In short, quantization-aware
training outperforms adversarial and Mixup training concerning
minimizing the accuracy change during deployment.

In addition, similar to the findings in RQ1, we observe that under
synthetic distribution shift (MNIST and CIFAR-10), most (7 out
of 8) of the accuracy change improvements happen in the models
quantized by TensorFlowLite. And for the data with natural dis-
tribution shifts, the accuracy change increase only happens in the
models quantized by CoreML. Finding 6: In terms of accuracy
change, quantization-aware training produces more stable models

IDbrightnesscanny_edgesdotted_linefogglass_bluridentityimpulse_noisemotion_blurrotatescaleshearshot_noisespatterstripetranslatezigzag02004006008001000Disagreement ChangeQAAdvMixupIDbrightnesscanny_edgesdotted_linefogglass_bluridentityimpulse_noisemotion_blurrotatescaleshearshot_noisespatterstripetranslatezigzag0100200300400Disagreement ChangeQAAdvMixupIDbrightnesscontrastdefocus_blurelastic_transformfogfrostgaussian_blurgaussian_noiseglass_blurimpulse_noisejpeg_compressionmotion_blurpixelatesaturateshot_noisesnowspatterspeckle_noisezoom_blur3002001000100Disagreement ChangeQAAdvMixupIDbrightnesscontrastdefocus_blurelastic_transformfogfrostgaussian_blurgaussian_noiseglass_blurimpulse_noisejpeg_compressionmotion_blurpixelatesaturateshot_noisesnowspatterspeckle_noisezoom_blur7006005004003002001000Disagreement ChangeQAAdvMixupIDCRYelp642024Disagreement ChangeAdvMixupIDCRYelp76543210Disagreement ChangeAdvMixupIDOOD200017501500125010007505002500Disagreement ChangeQAAdvMixupIDOOD1400120010008006004002000Disagreement ChangeQAAdvMixupConferenceâ€™17, July 2017, Washington, DC, USA

Hu and Guo, et al.

than standard, adversarial, and Mixup training. During deployment,
TensorFlowLite is more suitable to deal with natural distribution
shift, while CoreML performs better for synthetic distribution shift.
Finally, we check the disagreements that occur during model
quantization. Figure 4 shows disagreement change of models trained
by different strategies compared to by the standard training. Given
all OOD test datasets, the quantization-aware equipped with Ten-
sorFlowLite can efficiently decrease the number of disagreements.
Under synthetic distribution shift only, after TensorFlowLite quan-
tization, the models trained by Mixup training happen more dis-
agreements. On the other hand, under natural distribution shift, all
these tree training strategies are useful to reduce disagreements
(negative disagreement change in Figures 4(e) - 4(h)) regardless
of the quantization technique. Finding 7: Under synthetic dis-
tribution shift, quantization-aware training is useful to remove
disagreements for TensorFlowLite-quantized models. While under
natural distribution shift, all three training strategies are efficient
to reduce disagreements.

Answer to RQ2: Generally, quantization-aware training can
produce more stable models with small accuracy changes and
fewer disagreements after model quantization. For data with
natural distribution shifts, both quantization-aware training and
basic data augmentation training (adversarial training and Mixup
training) can reduce the disagreements.

5.3 RQ3: Characteristic of Disagreements
To understand which data are likely to cause disagreements during
quantization, we explore the data properties based on the output
uncertainty. The intuition is that the disagreements are those close
to the decision boundary of the model [60]. Concretely, after quan-
tization, the decision boundary of a model may slightly move due
to the precision of parameter change. As a result, the data that are
close to the boundary might cross over the boundary and cause
disagreements. Generally, those data are uncertain to the model.
Many uncertainty metrics have been developed but which one can
be used to more precisely distinguish the disagreements and normal
inputs is unclear. In our study, we consider four (Entropy, Margin,
Gini, Least Confidence) widely used uncertainty metrics only based
on the output of the model to determine the best one to present the
property of disagreements.

Figure 5 gives an example (CIFAR-10, ResNet20) of the distribu-
tion of uncertainty scores of the disagreements and normal inputs.
First of all, regardless of the uncertainty metric, the result confirms
that disagreements are more uncertain for a model than normal
inputs as they usually have higher (lower in Margin) uncertainty
scores. Take the least confidence as an example, most normal in-
puts have LC scores near 0. According to the definition of LC (in
Equation (??)), the result demonstrates that the model is confident
(with almost 100%) in the top-1 predictions for these inputs. In
detail, the number of inputs having LC scores in the ranges of [0,
0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], and (0.8, 1] are 462, 31, 7, 0, and 0
respectively. In contrast, for the disagreement inputs, most of them
have high uncertain scores. Specifically, the number of inputs that
the LC scores in the ranges of [0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8],
and (0.8, 1] are 110, 175, 225, 26, and 0 respectively. Finding 8:

(a) Entropy

(b) Margin

(c) Gini

(d) Least Confidence

Figure 5: An example (CIFAR-10, ResNet20, ID test data) of
the distributions of output uncertainty scores. Red: disagree-
ments. Blue: normal inputs.

Output-based uncertainty is a promising indicator to distinguish
disagreements and normal inputs.

Table 5 presents the AUC-ROC scores of the classifiers. Regard-
less of the dataset, DNN, and training strategy, in most cases (27
out of 30), the classifiers trained by ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› score have greater
AUC-ROC scores than other classifiers, which means that the dis-
agreement inputs and normal inputs have a bigger difference based
on the ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› score. Specifically, in 23 (out of 30) cases, the classi-
fiers trained using ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› score as the training data have greater
than 90% AUC-ROC scores, which indicates the classifiers are useful
to distinguish the normal inputs and disagreements. Besides, most
IMDb classifiers have 100% AUC-ROC scores, the perfect results
could come from the limited number of disagreements but can still
prove the output-based uncertainty score is a promising indicator
to represent the property of disagreements.

Figure 5 also shows a few disagreements where the model has
high confidence. We call them extreme disagreements. We uti-
lize the ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› score to set the threshold and analyze how many
extreme disagreements exist and where do they come from. Con-
cretely, we define the disagreements with ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› > 0.95 as extreme.
We observe that there are 3, 226, 0, and 9 extreme disagreements
in MNIST, CIFAR-10, IMDb, and iWildsCam, respectively. Interest-
ingly, all the extreme disagreements come from the disagreements
between TensorFlowLite-8bit quantized model and the original
model, which means this quantization moves the decision bound-
ary a lot in some areas. A deeper analysis could be an interesting
research direction. Finding 9: Extreme disagreements where the
original model has high prediction confidences only come from
TensorFlowLite-8bit quantized models.

02004006008001000Data0.00.51.01.52.0Entropy score02004006008001000Data0.00.20.40.60.81.0Margin score02004006008001000Data0.00.10.20.30.40.50.60.70.8Gini score02004006008001000Data0.00.10.20.30.40.50.60.7LC scoreCharacterizing and Understanding the Behavior of Quantized Models for Reliable Deployment

Conferenceâ€™17, July 2017, Washington, DC, USA

Table 5: ğ´ğ‘ˆğ¶ âˆ’ğ‘…ğ‘‚ğ¶ score of the logistic regression classifiers
trained by using different uncertainty scores. A high value
indicates a significant difference between two sets. The best
results among four uncertainty metrics are highlighted.

Table 6: Number of disagreements before and after model re-
training. In total: disagreements in a test dataset regardless
of the quantization technique. Values in brackets are the dif-
ference. Stubborn: disagreements cannot be removed by re-
training. New: disagreements appearing after retraining.

Dataset

DNN

Training Strategy

MNIST

CIFAR10

IMDb

iWildCam

Lenet1

Lenet5

ResNet20

NiN

LSTM

GRU

Densenet

Resnet50

Standard
Quantization-aware
Adversarial
Mixup
Average
Standard
Quantization-aware
Adversarial
Mixup
Average
Standard
Quantization-aware
Adversarial
Mixup
Average
Standard
Quantization-aware
Adversarial
Mixup
Average
Standard
Adversarial
Mixup
Average
Standard
Adversarial
Mixup
Average
Standard
Quantization-aware
Adversarial
Mixup
Average
Standard
Quantization-aware
Adversarial
Mixup
Average

Uncertainty Measure

Entropy Gini Margin
85.00
95.20
73.58
84.34
84.53
89.49
83.3
76.47
72.00
80.32
93.58
95.62
94.01
90.31
93.38
94.88
85.74
94.96
89.98
91.39
100
83.33
100
94.44
100
50.00
100
83.33
85.00
75.73
62.04
79.98
75.90
93.41
91.60
85.81
89.68
90.13

83.67
95.81
71.41
79.74
82.66
86.79
80.39
72.02
71.53
77.68
95.42
95.29
92.63
87.03
92.59
93.36
85.23
93.79
88.59
90.24
100
100
100
100
100
100
100
100
78.67
75.64
61.71
82.57
74.65
87.00
89.71
88.54
87.73
88.25

94.76
97.45
96.51
94.06
95.70
97.36
94.82
96.78
89.42
94.60
94.54
96.52
97.05
95.28
95.85
96.2
87.47
96.25
93.15
93.27
100
100
100
100
100
100
100
100
85.60
76.51
63.35
82.46
76.98
95.95
97.36
96.77
96.23
96.58

LC
89.78
96.61
82.49
89.76
89.66
94.49
88.48
85.09
78.37
86.61
94.28
96.11
96.04
93.27
94.93
95.65
86.31
95.64
91.85
92.36
100
100
100
100
100
100
100
100
85.83
76.18
62.28
80.98
76.32
94.44
94.79
89.88
93.10
93.05

MNIST
TensorFlowLite-8
TensorFlowLite-16
CoreML-8
CoreML-16
In total
Stubborn
New
CIFAR-10
TensorFlowLite-8
TensorFlowLite-16
CoreML-8
CoreML-16
In Total
Stubborn
New
IMDb
TensorFlowLite-16
CoreML-8
CoreML-16
In Total
Stubborn
New
iWildCam
TensorFlowLite-8
TensorFlowLite-16
CoreML-8
CoreML-16
In Total
Stubborn
New

Before

After

Before

After

LeNet1

LeNet5

14
9
2
0
15

514
24
45
7
540

8
6
0
13

7(-7)
4(-5)
8(+6)
0(0)
16(+1)

1
15
NiN

371(-143)
26(+2)
31(-14)
4(-3)
401(-139)

47
354
LSTM

7(-1)
2(-4)
0(0)
8(-5)

4
3
1
0
6

3(-1)
1(-2)
1(0)
0(0)
4(-2)

0
4
ResNet20

456
54
181
7
536

3
2
0
5

439(-17)
49(-5)
56(-125)
13(+6)
480(-56)

100
380
GRU

1(-2)
0(-2)
0(0)
1(-4)

0
8
DenseNet

0
1
ResNet50

2830
167
2035
34
3834

3319(+489)
12(-155)
7(-2028)
2(-32)
3324 (-510)

326
187
226
16
469

373(+17)
143(-44)
101(-125)
27(+11)
462 (-7)

2230
1094

24
438

Answer to RQ3: Most disagreements have closer top-1 and top-
2 output probabilities (i.e., smaller ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› score) than normal
inputs. Compared to Entropy, Gini and Least Confidence, Margin
is a better metric to distinguish disagreements and normal inputs.

5.4 RQ4: Effectiveness of Retraining
In RQ3, we observe that the disagreements are data where the
model has low confidence in the prediction. We investigate if model
retraining, an efficient method to improve confidence, can ensure a
stable compressed model during quantization.

Table 6 presents the number of disagreements from the ID test
data before and after model retraining. In most cases (18 out of 26
cases that have disagreements before retraining), the number of
disagreements decreases after model retraining. However, surpris-
ingly, there are some exceptions that the disagreements increase.
For example, in MNIST, LeNet1, CoreML-8, 6 more disagreements
appear after retraining. Finding 10: Retraining the model using
disagreements cannot always remove the disagreements.

In addition, we study whether the disagreements are really re-
moved by model retraining. To this end, we compare if the dis-
agreements maintain the same after retraining. For simplicity, we

(a) MNIST-LeNet1

(b) CIFAR-10-ResNet20

Figure 6: Examples of two stubborn disagreements. MNIST:
predicted label before retraining: 1, 100% confidence, after:
8, 100% confidence. CIFAR-10: prediction before retraining:
cat, 59% confidence, after: deer, 92% confidence.

define the stubborn disagreement as the disagreement appearing
both before and after retraining, and new disagreement as the dis-
agreement introduced by retraining. Figure 6 gives two examples of
stubborn disagreements. For the MNIST image, the model predicts
the digital number as 0 or 9, while the true label is 8. For the CIFAR-
10 image, the model hesitates to predict the animal to be a cat before
retraining, and raises the confidence of this wrong prediction after
retraining, while the true label is deer. Besides, we observe that the
average ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› score of all the stubborn disagreements before and

05101520250510152025051015202530051015202530Conferenceâ€™17, July 2017, Washington, DC, USA

Hu and Guo, et al.

after retraining are 0.40 and 0.56, respectively. That means although
models become more confident with these stubborn disagreements
after retraining, their uncertainty is still high. In Table 6, regardless
of the quantization technique, only a few stubborn disagreements
remain after retraining. For example, in CIFAR-10, NiN, only 47 (of
540) disagreements are left. However, model retraining introduces
new disagreements which have the same size as without retraining.
For example, in iWildCam, ResNet50, through retraining, only 24
stubborn disagreements are left and all the other 445 are efficiently
removed, but meanwhile, 438 new disagreements appear. Finding
11: Through model retraining, only a few stubborn disagreements
remain but a similar size of new disagreements are introduced.

Answer to RQ4: Retraining fails to reduce the total number
of disagreements. Though it manages to remove some existing
disagreements, it introduces as many new ones.

6 DISCUSSION
6.1 Quantized Model Repair
We have verified that model retraining, the most common strategy
to enhance performance, has limited functionality in removing dis-
agreements. How to solve this issue is still an open problem. Based
on our investigation, the disagreements are mainly the data with
small ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› scores by quantized models. Therefore, the main chal-
lenge is how to improve the confidence of the data. We provide two
potential solutions. 1) Online monitoring. Before quantization,
training multiple models to perform prediction can also improve
confidence [6]. Concretely, we can divide data into different groups
based on their ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› scores. For each group of data, a model
is trained and quantized. 2) Offline repair. After quantization,
building an ensemble model to perform prediction instead of the
quantized model. Ensemble learning [37, 50] has been proved to
effectively improve the predictive performance of a single model by
taking weighted average confidence from multiple models. How-
ever, both solutions will increase the storage size since more models
are required. As a result, there is a trade-off between fewer disagree-
ments and efficient model quantization. Thus, designing a robust
quantization method is still an ongoing and important direction.

6.2 Threats to Validity
First, the threats to validity come from the selected datasets and
models. Regarding the datasets, we consider both image and text
classification tasks and include OOD benchmark datasets with both
synthetic and natural distribution shifts. All the datasets are widely
used in previous studies. As for the models, we cover two types
of DNN architectures, feed-forward neural network, e.g., ResNet,
and recurrent neural network, e.g., LSTM. In addition, we take into
account the model complexity and apply both simple and complex
ones, such as LeNet1 and ResNet50. For each dataset, we employ
two different models to eliminate the influence of selected models.
An interesting research direction is to repeat our experiments on
other tasks, such as the regression task.

Second, the training strategies and uncertainty metrics could
be other threats to validity. For the training strategies, among all
possible choices, we include the four most representative and com-
mon ones. Standard training is the most basic training procedure

and should be taken as the baseline. Quantization-aware training
is specifically designed for quantization. Mixup training is the first
and basic data augmentation approach to improve the generaliza-
tion of DNNs over different distribution shifts. Adversarial training
is one of the most effective techniques to promote model robust-
ness/generalization. For the uncertainty metrics, we tend to select
metrics that require as few configurations as possible. The four
metrics included in this work are all solely based on the output
probabilities. This is to avoid the impact of uncontrollable factors.
For example, the dropout-based uncertainty metric [16] needs to
consider where to put the dropout layer and the dropout ratio.

7 RELATED WORK
7.1 Deep Learning Testing
As a critical phase in the software development life cycle [41], deep
learning testing ensures the functionality of DL-based systems
during deployment. Multiple testing methods have been proposed
in recent years [17, 29, 34, 56, 62]. For example, from the perspective
of deep learning models, Pei et al. proposed DeepXplore which
borrows the idea from code coverage and defines neuron coverage
to measure if the test set is enough or not. Later on, DeepGauge [42]
defines some new coverage metrics, e.g., k-multisection Neuron
Coverage and Neuron Boundary Coverage, and demonstrates their
effectiveness compared to the basic neuron coverage. From the
perspective of test data, several test generation [12, 21, 49, 59] and
test selection [8, 14, 39] approaches have been proposed. Gao et
al. proposed SENSEI [17] which utilizes genetic search to find the
best image transformation methods (e.g., image rotate) to generate
the suitable data for training a more robust model. Chen et al.
proposed PACE [8] which uses clustering methods and MMD-critic
algorithm to select a small size of test data to estimate the accuracy
of the model. However, all of these works test the model before
quantization, while our study mainly focuses on the analysis of the
difference between the models before and after quantization.

There are two studies closely related to our work [57, 60]. Both
of them generate test inputs that have a different output between
the original and compressed models. However, these works did not
1) study the properties of such disagreements; 2) try to solve the
disagreements; 3) consider natural distribution shift, all of which
are considered in our work.

7.2 Empirical Study for Deep Learning Systems
Empirical software engineering is one general way to practical
analyze software systems. In recent years, multiple empirical studies
for deep learning systems have been conducted to help understand
such complex systems.

The empirical study by Zhang et al. [63] pointed out that model
migration is one of the top-three common programming issues in
developing deep learning applications. Noticing the lack of bench-
mark understanding of the migration and quantization, Guo et
al. [22] investigated, for deployment process, the performance of
trained models when migrated/quantized to real mobile and web
browsers. They focus on the impacts of the deployment process
on prediction accuracy, time cost, and memory consumption. In
addition to the accuracy, we further evaluate the robustness of a

Characterizing and Understanding the Behavior of Quantized Models for Reliable Deployment

Conferenceâ€™17, July 2017, Washington, DC, USA

model, especially considering the synthetic and natural distribu-
tion shifts in the test data. Chen et al. [10] studied the faults when
deploying deep learning models on mobile devices. Especially, they
apply TensorFlowLite and CoreML in the deployment, which is also
considered in our study. The difference with our study is that their
empirical study explores the failures related to data preparation
(datatype error), memory issue, dependency resolution error, and
so on, while our study focuses on the differential behavior during
deployment and retraining. Hu et al. [28] verified that model quan-
tization has opposite impacts over different tasks in the setting of
active learning. For example, after quantization, the model is less
accurate in the image classification task while exhibiting better
performance in the text classification task. In our study, since the
labels of all data are available, we apply standard training instead
of active learning.

8 CONCLUSION
In this paper, we conducted a systematically study to characterize
and help people understand the behaviors of quantized models
under different data distributions. Our results reveal that there are
more disagreement inputs in data with distribution shift than in the
original test data. Quantization-aware training is a useful training
strategy to produce a model that has fewer disagreements after
quantization. The disagreements are those data that have high un-
certainty scores, and the ğ‘€ğ‘ğ‘Ÿğ‘”ğ‘–ğ‘› score is a more effective indicator
to distinguish the normal inputs and disagreements. More impor-
tantly, we also demonstrated that the commonly used approach â€“
retraining the model with disagreements has limited usefulness to
remove the disagreements and repair quantized models. Based on
our findings, we provide two future research directions to solve the
disagreement issue. To support further research, we released our
code, models (before and after quantization) to be a new benchmark
for studying the quantization problem.

REFERENCES
[1] 2022. https://github.com/tensorflow/tensorflow/issues/35194
[2] 2022. https://github.com/tensorflow/tensorflow/issues/25563
[3] MartÃ­n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
2016. Tensorflow: A system for large-scale machine learning. In 12th {USENIX}
symposium on operating systems design and implementation ({OSDI} 16). 265â€“283.
[4] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing distributed representations of code. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 1â€“29.

[5] David Berend, Xiaofei Xie, Lei Ma, Lingjun Zhou, Yang Liu, Chi Xu, and Jianjun
Zhao. 2020. Cats are not fish: Deep learning testing calls for out-of-distribution
awareness. In Proceedings of the 35th IEEE/ACM International Conference on
Automated Software Engineering. 1041â€“1052.

[6] Pavol Bielik and Martin Vechev. 2020. Adversarial Robustness for Code. In
Proceedings of the 37th International Conference on Machine Learning (Proceedings
of Machine Learning Research, Vol. 119), Hal DaumÃ© III and Aarti Singh (Eds.).
PMLR, 896â€“907. https://proceedings.mlr.press/v119/bielik20a.html

[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165 (2020).

[8] Junjie Chen, Zhuo Wu, Zan Wang, Hanmo You, Lingming Zhang, and Ming Yan.
2020. Practical accuracy estimation for efficient deep neural network testing.
ACM Transactions on Software Engineering and Methodology (TOSEM) 29, 4 (2020),
1â€“35.

[9] Zhenpeng Chen, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Tao Xie, and Xuanzhe
Liu. 2020. A comprehensive study on challenges in deploying deep learning based
software. In Proceedings of the 28th ACM Joint Meeting on European Software

Engineering Conference and Symposium on the Foundations of Software Engineering.
750â€“762.

[10] Zhenpeng Chen, Huihan Yao, Yiling Lou, Yanbin Cao, Yuanqiang Liu, Haoyu
Wang, and Xuanzhe Liu. 2021. An Empirical Study on Deployment Faults of
Deep Learning Based Mobile Applications. 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE) (2021), 674â€“685.

[11] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).

[12] Swaroopa Dola, Matthew B Dwyer, and Mary Lou Soffa. 2021. Distribution-
aware testing of neural networks using generative models. In 2021 IEEE/ACM
43rd International Conference on Software Engineering (ICSE). IEEE, 226â€“237.
[13] Tom Fawcett. 2006. An introduction to ROC analysis. Pattern recognition letters

27, 8 (2006), 861â€“874.

[14] Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and Zhenyu
Chen. 2020. Deepgini: prioritizing massive tests to enhance the robustness of
deep neural networks. In Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis. 177â€“188.

[15] Yonggan Fu, Qixuan Yu, Meng Li, Vikas Chandra, and Yingyan Lin. 2021. Double-
Win Quant: Aggressively Winning Robustness of Quantized Deep Neural Net-
works via Random Precision Training and Inference. In Proceedings of the 38th
International Conference on Machine Learning (Proceedings of Machine Learn-
ing Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 3492â€“3504.
https://proceedings.mlr.press/v139/fu21c.html

[16] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation:
Representing model uncertainty in deep learning. In international conference on
machine learning. PMLR, 1050â€“1059.

[17] Xiang Gao, Ripon K Saha, Mukul R Prasad, and Abhik Roychoudhury. 2020. Fuzz
testing based data augmentation to improve robustness of deep neural networks.
In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE).
IEEE, 1147â€“1158.

[18] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT

press.

[19] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and

harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).

[20] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcodebert:
Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366
(2020).

[21] Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, and Jiaguang Sun. 2018. Dlfuzz: Dif-
ferential fuzzing testing of deep learning systems. In Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 739â€“743.

[22] Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao Liu, Yang Liu,
Jianjun Zhao, and Xiaohong Li. 2019. An empirical study towards characterizing
deep learning development and deployment across different frameworks and
platforms. In 2019 34th IEEE/ACM International Conference on Automated Software
Engineering (ASE). IEEE, 810â€“822.

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). 770â€“778. https://doi.org/10.1109/CVPR.2016.90
[24] Dan Hendrycks and Thomas Dietterich. 2019. Benchmarking neural net-
work robustness to common corruptions and perturbations. arXiv preprint
arXiv:1903.12261 (2019).

[25] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and
Balaji Lakshminarayanan. 2019. Augmix: A simple data processing method to
improve robustness and uncertainty. arXiv preprint arXiv:1912.02781 (2019).
[26] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural

computation 9, 8 (1997), 1735â€“1780.

[27] Qiang Hu, Yuejun Guo, Maxime Cordy, Xiaofei Xie, Lei Ma, Mike Papadakis,
and Yves Le Traon. 2022. An empirical study on data distribution-aware test
selection for deep learning enhancement (In press). ACM Transactions on Software
Engineering and Methodology (TOSEM) (2022). https://orbilu.uni.lu/handle/10993/
50265

[28] Qiang Hu, Yuejun Guo, Maxime Cordy, Xiaofei Xie, Wei Ma, Mike Papadakis, and
Yves Le Traon. 2021. Towards Exploring the Limitations of Active Learning: An
Empirical Study. In The 36th IEEE/ACM International Conference on Automated
Software Engineering.

[29] Qiang Hu, Lei Ma, Xiaofei Xie, Bing Yu, Yang Liu, and Jianjun Zhao. 2019. Deep-
mutation++: A mutation testing framework for deep learning systems. In 2019
34th IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 1158â€“1161.

[30] Rui Hu, Jitao Sang, Jinqiang Wang, and Chaoquan Jiang. 2021. Understanding
and testing generalization of deep networks on out-of-distribution data. arXiv
preprint arXiv:2111.09190 (2021).

[31] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
2017. Densely connected convolutional networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 4700â€“4708.

Conferenceâ€™17, July 2017, Washington, DC, USA

Hu and Guo, et al.

40th international conference on software engineering. 303â€“314.

[57] Yongqiang Tian, Wuqi Zhang, Ming Wen, Shing-Chi Cheung, Chengnian Sun,
Shiqing Ma, and Yu Jiang. 2021. Fast Test Input Generation for Finding Deviated
Behaviors in Compressed Deep Neural Network. arXiv preprint arXiv:2112.02819
(2021).

[58] Dan Wang and Yi Shang. 2014. A new active labeling method for deep learning.
In 2014 International joint conference on neural networks (IJCNN). IEEE, 112â€“119.
[59] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun
Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. Deephunter: a coverage-guided
fuzz testing framework for deep neural networks. In Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis. 146â€“157.
[60] Xiaofei Xie, Lei Ma, Haijun Wang, Yuekang Li, Yang Liu, and Xiaohong Li. 2019.
DiffChaser: Detecting Disagreements for Deep Neural Networks.. In IJCAI. 5772â€“
5778.

[61] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2017.
mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412
(2017).

[62] Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020. Machine learning testing:
IEEE Transactions on Software Engineering

Survey, landscapes and horizons.
(2020).

[63] Tianyi Zhang, Cuiyun Gao, Lei Ma, Michael R. Lyu, and Miryung Kim. 2019. An
Empirical Study of Common Challenges in Developing Deep Learning Applica-
tions. 2019 IEEE 30th International Symposium on Software Reliability Engineering
(ISSRE) (2019), 104â€“115.

[32] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. 2021.
Accurate Post Training Quantization With Small Calibration Sets. In Proceedings
of the 38th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 4466â€“
4475. https://proceedings.mlr.press/v139/hubara21a.html

[33] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew
Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and
training of neural networks for efficient integer-arithmetic-only inference. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
2704â€“2713.

[34] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testing using surprise adequacy. In 2019 IEEE/ACM 41st International Conference
on Software Engineering (ICSE). IEEE, 1039â€“1049.

[35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin
Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas
Phillips, Irena Gao, et al. 2021. Wilds: A benchmark of in-the-wild distribution
shifts. In International Conference on Machine Learning. PMLR, 5637â€“5664.
[36] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278â€“
2324.

[37] Leijun Li, Qinghua Hu, Xiangqian Wu, and Daren Yu. 2014. Exploration of
classification confidence in ensemble learning. Pattern Recognition 47, 9 (2014),
3120â€“3131. https://doi.org/10.1016/j.patcog.2014.03.021

[38] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei
Wang, and Shi Gu. 2021. Brecq: Pushing the limit of post-training quantization
by block reconstruction. arXiv preprint arXiv:2102.05426 (2021).

[39] Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, and Jian LÃ¼. 2019.
Boosting operational dnn testing efficiency through conditioning. In Proceedings
of the 2019 27th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering. 499â€“509.

[40] Min Lin, Qiang Chen, and Shuicheng Yan. 2013. Network in network. arXiv

preprint arXiv:1312.4400 (2013).

[41] Lei Ma, Felix Juefei-Xu, Minhui Xue, Qiang Hu, Sen Chen, Bo Li, Yang Liu, Jianjun
Zhao, Jianxiong Yin, and Simon See. 2018. Secure deep learning engineering: A
software quality assurance perspective. arXiv preprint arXiv:1810.04538 (2018).
[42] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yang Chen, Ting Su, Li Li, Yang Liu, et al. 2018. Deepgauge: Multi-granularity
testing criteria for deep learning systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering. 120â€“131.

[43] Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, and Yves Le Traon.
2021. Test selection for deep learning systems. ACM Transactions on Software
Engineering and Methodology (TOSEM) 30, 2 (2021), 1â€“22.

[44] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng,
and Christopher Potts. 2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th annual meeting of the association for computational
linguistics: Human language technologies. 142â€“150.

[45] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards deep learning models resistant to adversarial attacks.
arXiv preprint arXiv:1706.06083 (2017).

[46] Norman Mu and Justin Gilmer. 2019. Mnist-c: A robustness benchmark for

computer vision. arXiv preprint arXiv:1906.02337 (2019).

[47] Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi,
Vladmir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, et al.
2021. Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity
of Coding Tasks. arXiv preprint arXiv:2105.12655 (2021).

[48] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. 2019. Generating natu-
ral language adversarial examples through probability weighted word saliency.
In Proceedings of the 57th annual meeting of the association for computational
linguistics. 1085â€“1097.

[49] Vincenzo Riccio, Nargiz Humbatova, Gunel Jahangirova, and Paolo Tonella. 2021.
DeepMetis: Augmenting a Deep Learning Test Set to Increase its Mutation Score.
arXiv preprint arXiv:2109.07514 (2021).

[50] Omer Sagi and Lior Rokach. 2018. Ensemble learning: a survey. WIREs Data
Mining and Knowledge Discovery 8, 4 (2018), e1249. https://doi.org/10.1002/widm.
1249 arXiv:https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1249

[51] Burr Settles. 2009. Active learning literature survey. (2009).
[52] Claude Elwood Shannon. 1948. A mathematical theory of communication. The

Bell system technical journal 27, 3 (1948), 379â€“423.

[53] Gil Shomron, Freddy Gabbay, Samer Kurzum, and Uri Weiser. 2021. Post-Training

Sparsity-Aware Quantization. arXiv preprint arXiv:2105.11010 (2021).

[54] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George
Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural
networks and tree search. nature 529, 7587 (2016), 484â€“489.

[55] Mohit Thakkar. 2019. Beginning machine learning in ios: CoreML framework (1st

ed.). APress.

[56] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Automated
testing of deep-neural-network-driven autonomous cars. In Proceedings of the

