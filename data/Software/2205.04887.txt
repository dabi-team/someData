2
2
0
2

y
a
M
4
1

]

G
L
.
s
c
[

2
v
7
8
8
4
0
.
5
0
2
2
:
v
i
X
r
a

Search-Based Testing of Reinforcement Learning ∗
Martin Tappler1,3 † , Filip Cano C´ordoba2 † , Bernhard K. Aichernig1,3 , Bettina K¨onighofer2,4 †
1Institute of Software Technology, Graz University of Technology
2Institute of Applied Information Processing and Communications, Graz University of Technology
3TU Graz-SAL DES Lab,Silicon Austria Labs, Graz, Austria
4Lamarr Security Research
martin.tappler@ist.tugraz.at, ﬁlip.cano@iaik.tugraz.at, aichernig@ist.tugraz.at,
bettina.koenighofer@lamarr.at

Figure 1: Super Mario Bros. Up: Reference Trace and Boundary States. Down: Reference Trace and Fuzz Traces.

Abstract

Evaluation of deep reinforcement learning (RL)
is inherently challenging. Especially the opaque-
ness of learned policies and the stochastic nature of
both agents and environments make testing the be-
havior of deep RL agents difﬁcult. We present a
search-based testing framework that enables a wide
range of novel analysis capabilities for evaluating
the safety and performance of deep RL agents. For
safety testing, our framework utilizes a search algo-
rithm that searches for a reference trace that solves
the RL task. The backtracking states of the search,
called boundary states, pose safety-critical situa-
tions. We create safety test-suites that evaluate how
well the RL agent escapes safety-critical situations
near these boundary states. For robust performance
testing, we create a diverse set of traces via fuzz
testing. These fuzz traces are used to bring the
agent into a wide variety of potentially unknown
states from which the average performance of the
agent is compared to the average performance of
the fuzz traces. We apply our search-based testing
approach on RL for Nintendo’s Super Mario Bros.

1 Introduction
In reinforcement learning (RL) [Sutton and Barto, 1998], an
agent aims to maximize the total amount of reward through
trial-and-error via interactions with an unknown environment.
Recently, RL algorithms achieved stunning results in playing

∗To appear in Proceedings of IJCAI-ECAI 2022,
†Contact author.

(ijcai.org).

video games and complex board games [Schrittwieser et al.,
2020].

To achieve a broad acceptance and enlarge the application
areas of learned controllers, there is the urgent need to re-
liably evaluate trained RL agents. When evaluating trained
agents, two fundamental questions need to be answered: (Q1)
Does the trained deep RL agent circumvent safety viola-
tions? (Q2) Does the trained deep RL agent perform well from
a wide variety of states? Testing deep RL agents is notori-
ously difﬁcult. The ﬁrst challenge arises from the environ-
ment, which is is often not fully known and has an immense
state space, combined with the byzantine complexity of the
agent’s model, and the lack of determinism of both the agent
and the environment. Secondly, to evaluate the performance
of a trained agent’s policy, an estimation of the performance
of the optimal policy is needed.

To address these challenges, we transfer well-established
search-based concepts from software testing into the RL-
setting. Search algorithms like backtracking-based depth-
ﬁrst search (DFS) are standard to ﬁnd valid and invalid pro-
gram executions. Fuzz testing refers to automated software
testing techniques that generate interesting test cases with the
goal to expose corner cases that have not been properly dealt
with in the program under test. In this work, we propose a
search-based testing framework to reliably evaluate trained
RL agents to answer the questions Q1 and Q2. Our testing
framework comprises four steps:
Step 1: Search for reference trace and boundary states.
In the ﬁrst step, we use a DFS algorithm to search for a refer-
ence trace that solves the RL task by sampling the black-box
environment. This idea is motivated by the experience from
AI competitions, like the Mario AI and the NetHack Chal-
lenges [Karakovskiy and Togelius, 2012; K¨uttler et al., 2020],

 
 
 
 
 
 
where best performers are symbolic agents, providing a refer-
ence solution of the task faster than neural-based agents. Fur-
thermore, since the DFS algorithm backtracks when reaching
an unsafe state in the environment, the search reveals safety-
critical situations that we call boundary states.
Step 2: Testing for safety. To answer Q1, our testing frame-
work computes safety test-suites that bring the agent into
safety-critical situations near the boundary states. Based on
the ability of the agent to succeed in these safety-critical sit-
uations we can evaluate the safety of agents. A safe agent
should not violate safety regardless of the situation it faces.
Step 3: Generation of fuzz traces. As a basis for perfor-
mance testing, our testing framework applies a search-based
fuzzing method to compute a diverse set of traces from the ref-
erence trace (Step 1) aiming for traces that gain high rewards
and cover large parts of the state space.
Step 4: Testing for performance. To answer Q2, we create
performance test-suites from the fuzz traces to bring the agent
into a diverse set of states within the environment. As perfor-
mance metric we propose to point-wise compare the averaged
performance gained by executing the agent’s policy with the
averaged performance gained by executing the fuzz traces.

Our approach is very general and can be adapted to sev-
eral application areas.
In settings where initial traces are
given, for example, stemming from demonstrations by hu-
mans, such traces can be used as a basis for fuzzing. Our
approach only needs to be able to sample the environment as
an oracle. Even in the case of partial observability, our testing
framework can be successfully applied. This is the case since
we only need the information on whether a trace successfully
completed the task to be learned, partially completed the task,
or whether it violated safety. Exact state information is not
required. Fuzzing has been applied to test complex software
systems, like operating system kernels, communication pro-
tocols, and parsers for programming lanuages [Man`es et al.,
2021]. Hence, it offers scalability to large environments.

In our case study, we apply our framework to test the safety
and performance of a set of deep RL agents trained to play
Super Mario Bros. Fig. 1 shows the reference trace (red) and
boundary states (white points) computed in Step 1 and the
fuzz traces (yellow) from Step 3, computed in our case study.
Since we consider the environment (as well as the trained
agent, where a learned policy may need to break ties) to be
probabilistic, we execute every test case a number of times
and present the averaged results.
Related Work. While RL has proven successful in solving
many complex tasks [Silver et al., 2016] and often outper-
forms classical controllers [Kiran et al., 2021], safety con-
cerns prevent learned controllers from being widely used in
safety-critical applications. The research on safe RL targets to
guarantee safety during the training and the execution phase
of the RL agent [Garcıa and Fern´andez, 2015]. Safe RL has
attracted a lot of attention in the formal methods commu-
nity, culminating in a growing body of work on the veriﬁ-
cation of trained networks [Ehlers, 2017; Pathak et al., 2017;
Corsi et al., 2021]. However, all of these approaches suf-
fer from scalability issues and are not yet able to verify
industrial-size deep neural networks. An alternative line of
research aims to enforce safe operation of an RL agent during

runtime, using techniques from runtime monitoring and en-
forcement [Alshiekh et al., 2018; Pranger et al., 2021]. These
methods typically require a complete and faithful model
of the environment dynamics, which is often not available.
While a large amount of work on ofﬂine and runtime veriﬁ-
cation of RL agents exists, studying suitable testing methods
for RL has attracted less attention.

The development of RL algorithms has greatly beneﬁted
from benchmark environments for performance evaluation,
including the Arcade Learning Environment [Bellemare et
al., 2013], and OpenAI Gym [Brockman et al., 2016], Deep-
mind Control Suite [Tassa et al., 2018], to name a few. Safe-
tyGym [Achiam and Amodei, 2019] was especially designed
to evaluate the safety of RL algorithms during exploration.
Most work on testing for RL evaluates the aggregate per-
formance by comparing the mean and median scores across
tasks. Recently, testing metrics addressing the statistical un-
certainty in such point estimates have been proposed [Agar-
wal et al., 2021]. We extend previous work by propos-
ing search-based testing tailored toward (deep) RL. We use
search-based methods to automatically create safety-critical
test-cases and test cases for robust performance testing.

RL has been proposed for software testing and in particular
also for fuzz testing [B¨ottinger et al., 2018; Wang et al., 2021;
Scott et al., 2021; Drozd and Wagner, 2018].
In contrast,
we propose a novel search-based testing framework includ-
ing fuzzing to test RL agents. Fuzzing has been applied to
efﬁciently solve complex tasks [Aschermann et al., 2020;
Schumilo et al., 2022]. We perform a backtracking-based
search to efﬁciently solve the task, while fuzzing serves to
cover a large portion of the state space. Related is also
the work from Trujillo et al. [Trujillo et al., 2020] which
analyzes the adequacy of neuron coverage for testing deep
RL, whereas our adequacy criteria are inspired by traditional
boundary value and combinatorial testing.

We used our testing framework to evaluate trained deep-
Q learning agents, agents that internally use deep neural net-
works to approximate the Q-function. Recent years have seen
a surge in works on testing deep neural networks. Techniques,
like DeepTest [Tian et al., 2018], DeepXplore [Pei et al.,
2019], and DeepRoad [Zhang et al., 2018], are orthogonal to
our proposed framework. While we focus on the stateful reac-
tive nature of RL agents, viewing them as a whole, these tech-
niques are used to test sensor-related aspects of autonomous
agents and ﬁnd applications in particular in image processing.
Furthermore, we may consider taking into account neural-
network-speciﬁc testing criteria [Ma et al., 2018]. However,
doubts about the adequacy of neuron coverage and related cri-
teria have been raised recently [Harel-Canada et al., 2020].
Hence, more research is necessary in this area, as has been
pointed out by Trujillo et al. [Trujillo et al., 2020].

Outline. The remainder of the paper is structured as follows.
In Sec. 2, we give the background and notation. In the Sec. 3
to 6 we present and discuss in detail Step 1 - Step 4 of our
testing framework. We present a detailed case study in Sec. 7.

2 Preliminaries

A Markov decision process (MDP) M = (S, s0, A, P, R)
is a tuple with a ﬁnite set S of states including initial state s0,
a ﬁnite set A = {a1 . . . , an} of actions, and a probabilistic
transition function P : S × A × S → [0, 1], and an immediate
reward function R : S × A × S → R. For all s ∈ S the
available actions are A(s) = {a ∈ A | ∃s(cid:48), P(s, a, s(cid:48)) (cid:54)=
0} and we assume |A(s)| ≥ 1. A memoryless deterministic
policy π : S → A is a function over action given states. The
set of all memoryless deterministic policies is denoted by Π.
An MDP with terminal states is an MDP M with a
set of terminal states ST ⊆ S in which the MDP termi-
nates, i.e., the execution of a policy π on M yields a trace
execπ(π, s0) = (cid:104)s0, a1, r1, s1, . . . , rn, sn(cid:105) with only sn be-
ing a state in ST . ST consists of two types of states: goal-
states SG ⊆ ST representing states in which the task to be
learned was accomplished by reaching them, and undesired
unsafe-states SU ⊆ ST . A safety violation occurs whenever
a state in SU is entered. We deﬁne the set of bad-states SB
as all states that almost-surely lead to an unsafe state in SU ,
i.e., a state sB ∈ S is in SB, if applying any policy π ∈ Π
starting in sB leads to a state in SU with probability 1. The
set of boundary-states SBO is deﬁned as the set of not bad
states with successor states within the bad states, i.e., a state
sBO ∈ S is in SBO if sBO (cid:54)∈ SB and there exists a state
s ∈ SB and an action a ∈ A with P(sBO, a, s) > 0.

We consider reinforcement learning (RL) in which an
agent learns a task through trial-and-error via interactions
with an unknown environment modeled by a MDP M =
(S, s0, A, P, R) with terminal states ST ∈ S. At each step t,
the agent receives an observation st. It then chooses an action
at+1 ∈ A. The environment then moves to a state st+1 with
probability P(st, at+1, st+1). The reward is determined with
rt+1 = R(st, at+1, st+1). If the environment enters a termi-
nal state in ST , the training episode ends. The time step the
episode ends is denoted by tend. The return ret = Σtend
t=1 γtrt
is the cumulative future discounted reward per episode, using
the discount factor γ ∈ [0, 1]. The objective of the agent is
to learn an optimal policy π∗ : S → A that maximizes the
expectation of the return, i.e., maxπ∈Π Eπ(ret). The accu-
mulated reward per episode is R = Σtend

t=1 rt.

Traces. A trace τ = (cid:104)s0, a1, r1, s1, . . . , an, rn, sn(cid:105) is the
state-action-reward sequence induced by a policy during an
episode starting with the initial state s0. We denote a set of
traces with T . Given a trace τ = (cid:104)s0, a1, r1, s1 . . . rn, sn(cid:105),
we use τ [i] to denote the ith state of τ (si = τ [i]), τ −i to
denote the preﬁx of τ (τ −i consists of all entries from τ from
position 0 to i) and we denote the trace τ +i to be the sufﬁx
of τ (τ +i consists of all entries from τ from position i to
n). Given a trace τ = (cid:104)s0, a1, r1, s1 . . . rn, sn(cid:105), we denote
|τ | = n to be the length of the trace. We denote the ﬁrst
appearance of state s in trace τ by d(τ, s) (if d(τ, s) = i
then τ [i] = s). We call the action sequence resulting from
omitting the states and rewards from τ an action trace τA =
(cid:104)a1, a2, . . . an(cid:105). τA[i] gives the ith action, i.e., ai = τA[i].
Executing τA on M from s0 yields a trace execτ (τA, s0) =
(cid:104)s0, a1, r1, s1 . . . rn, sn(cid:105) with n = |τA|.

Algorithm 1: Search for Reference Trace τref
input : MDP M = (S, s0, A, P, R), repetitions rep
output: τref , S (cid:48)

BO

BO ← ∅;

sprev ← s0;
for i ∈ 1, . . . , |VA| do

1 VS ← [s0]; VA ← [ ]; Explored ← ∅; success ← false;
2 τref ← [s0]; S (cid:48)
3 DFS(s0);
4 if success then
5
6
7
8
9
10
11
12

r ← R(sprev, a, s);
Push(τref , (cid:104)a, r, s(cid:105));
sprev ← s;
if VS [i + 2] ∈ Explored then

a, s ← VA[i], VS [i + 1];
if s /∈ Explored then

/* next state is a backtracking point */
BO ← S (cid:48)
S (cid:48)

BO ∪ {s};

if s ∈ SU then

13
14 Function DFS(s):
15
16
17
18
19
20
21

for a ∈ A do

Explored ← Explored ∪ {s}; return;

if s ∈ SG or success then

success ← true; return;

repeat rep times

Sample s(cid:48) from P(s, a);
if s(cid:48) /∈ VS then

22
23

24
25

Push(VA, a) ; Push(VS , s);
DFS(s(cid:48));

if ¬success then Explored ← Explored ∪ {s};

3 Step 1 - Search for Reference Trace and

Boundary States

The ﬁrst step of our testing framework is to perform a search
for a reference trace τref that performs the tasks to be learned
by the RL agent (not necessarily in the optimal way) and to
detect boundary-states S (cid:48)
B0 ⊆ SB0 along the reference trace.
We propose to compute τref using a backtracking-based,
depth-ﬁrst search (DFS) by sampling the MDP M. For the
DFS, we abstract away stochastic behavior of M by explor-
ing all possible behaviors in visited states by repeating ac-
tions sufﬁciently often [Khalili and Tacchella, 2014]. Assum-
ing that p = P(s, a, s(cid:48)) is the smallest transition probability
greater 0 for any s, s(cid:48) ∈ S and a ∈ A in M, we compute
the number of repetitions rep required to match a conﬁdence
level c via rep(c, p) = log(1 − c)/log(1 − p). This ensures
observing all possible states with a probability of at least c.

Example. Assume that p = 0.1 is the smallest probability
> 0 in M. To achieve a conﬁdence level of 90% that the
search visited any reachable state, the DFS has to perform
rep(0.9, 0.1) = 22 repetitions of any action in any state.

Algorithm 1 gives the pseudo code of our search algorithm
to compute τref ∈ T and a set of boundary-states S (cid:48)
B0 ⊆ SB0.
The list VS stores states that have already been visited, and
VA stores the executed actions leading to the corresponding
states in VS. Every time the search visits an unsafe state
the algorithm backtracks. A non-terminal state s is added
to Explored if the DFS backtracked to s from all successor
states. By tracking visited states in VS , we ensure that we do
not explore a state twice along the same trace. That is, we
use VS to detect cycles. When visiting a goal state, DFS(s0)
terminates successfully. In this case, τref is built from the set
of visited states that were not part of a backtracking branch of
the search, i.e., s ∈ τS if s ∈ VS and s (cid:54)∈ Explored, with cor-

Figure 2: Run of the Search Algorithm

BO.

responding actions in VA. States s ∈ τref that have successor
states s(cid:48) ∈ Explored are boundary states, i.e., s ∈ S (cid:48)

Example. Figure 2 shows parts of an MDP M that was
explored during a run of our search algorithm. Found un-
safe states are marked red. After visiting s10 ∈ SG (green
the search function DFS(s0) returns with VS =
circle),
[s0, . . . , s10], VA = [a, a, b, a, b, b, a, a, b, b] and Explored =
{s2, s3, s4, s5, s8, s9}. The reference trace (omitting re-
wards) is τref = {s0, a, s1, b, s6, a, s7, b, s10} and the subset
of boundary states is S (cid:48)

BO = {s1, s7} (blue circles).

Optimizing Search. Proper abstractions of the state space
may be used to merge similar states, thereby pruning the
search space and enabling to ﬁnd cycles in the abstract state
space via the DFS. Detecting cycles speeds up the search
since the DFS backtracks when ﬁnding an edge to an already
visited state. An example for such an abstraction is omitting
the execution time in the state space to merge states.

4 Step 2 - Testing for Safety
Based on τref and S (cid:48)
BO searched for in Step 1, we propose
several test suites to identify weak points of a policy with
a high frequency of fail verdicts, i.e., safety violations. Af-
ter discussing suitable test suites, we discuss how to execute
them to test the safety of RL agents.
Simple Boundary Test Suite. We use the boundary states
S (cid:48)
BO in τref for boundary value testing [Pezz`e and Young,
2007].We compute a simple test suite that consists of all pre-
ﬁxes of τref that end in a boundary state. From these traces,
we use the action traces to bring the RL agent into safety-
critical situations and to test its behavior regarding safety.

Formally, let DB be the sequence of depths of the bound-
BO : d(τ, sBO) ∈

ary states S (cid:48)
DB. Using DB, we compute a set of traces T by

BO in τref , i.e., for any sBO ∈ S (cid:48)

T = {τ −DB[i]
ref

| 1 ≤ i ≤ |DB|}.

Omitting states and rewards from the traces in T results in
a set of action traces that form a simple boundary test suite
called ST . We say the action trace τ DB[i]
A,ref ∈ ST is the test
case for the ith boundary state in S (cid:48)
Test Suites using Boundary Intervals. Boundary value
testing checks not only boundary values, but also inputs
slightly off of the boundary [Pezz`e and Young, 2007]. To
transfer this concept into RL testing, we introduce boundary
intervals to test at additional states near the boundary.

BO.

In contrast to boundary testing of conventional software,
our test cases stay in states traversed by τref . This choice
is motivated by the deﬁnition of a boundary state: a state
with successor states that necessarily lead to an unsafe state.

Bringing the RL agent in such a losing position will not pro-
vide additional insight concerning the learned safety objec-
tive, since the agent has no other choice than to violate safety.
However, testing states of τref within an offset of boundary
states provides insights into how well the RL agent escapes
safety-critical situations. Given a simple test suite ST and
an interval-size is, we create an interval test suite IT (is) by
adding additional test cases to ST , such that

IT (is) = {τ DB[i]+oﬀ

A,ref

| τ DB[i]

A,ref ∈ ST, −is ≤ oﬀ ≤ is},

is the reference action-trace. The test case
tests the agent at boundary state i with offset oﬀ .

where τA,ref
τ DB[i]+oﬀ
A,ref
Test Suites using Action Coverage. Combinatorial testing
covers issues resulting from combinations of input values.
We adapt this concept by creating test suites that cover com-
binations of actions near boundary states, i.e., the test suite
evaluates which actions cause unsafe behavior in boundary
regions. Given the reference action-trace τA,ref , a simple test
suite ST , and a k ≥ 1, we generate a k-wise action-coverage
test suite AC(k) by creating |A|k test cases for every test case
in ST covering all k-wise combinations of actions at the kth
predecessor of a boundary state. The test suite is given by

AC(k) = {τ DB[i]−k

A,ref

· ac | τ DB[i]

A,ref ∈ ST, ac ∈ Ak}.

Test-case Execution & Verdict. To test the behavior of an
agent regarding safety, we use a safety test-suite to bring the
agent in safety critical situations. A single test-case execution
takes an action-trace τA, an inital state s0 and a test length l as
parameters. To test an RL agent using τA, we ﬁrst execute τA,
yielding a trace exec(τA) = (cid:104)s0, a1, r1, s1, . . . , an, rn, sn(cid:105).
A test case τA is invalid if exec(τA) consistently visits a ter-
minal state in ST when executed repeatedly.

Starting from sn, we pick the next l actions according to
the policy of the agent. Note that l should be chosen large
enough to evaluate the behavior of the agent regarding safety.
Therefore, it should be considerably larger than the shortest
path to the next unsafe state in SU . After performing l steps
of the agent’s policy, we evaluate the test case. A test can fail
or pass: A test fails, if starting from sn the agent reaches an
unsafe state in SU within l steps. Otherwise, the test passes.
To execute a test suite T , we perform every test case of T n
times. During that, we compute the relative frequency of fail
verdicts resulting from executing each individual test case.

5 Step 3 - Generation of Fuzz Traces
Our testing framework evaluates the performance of RL
agents using fuzz traces. The traces are used to compare
gained rewards as well as to bring the agent in a variety of
states and to evaluate the performance from these onward. In
this section, we discuss the fuzz-trace generation for perfor-
mance testing. For this purpose, we propose a search-based
fuzzing method [Zeller et al., 2021] based on genetic algo-
rithms. The goal is to ﬁnd action traces that (1) cover a large
portion of the state space while (2) accomplishing the task to
be learned by the RL agent.
Overview of Computation of Fuzz Traces. Given the ref-
erence trace τref that solves the RL task (i.e., sn ∈ SG) and

parameter values for the number of generations g and the pop-
ulation size p, the fuzz traces are computed as follows:
1. Initialize T0, the trace population: T0 := {τA,ref }.
2. For i = 1 to g generations do:

(a) Create p action traces (called offspring) from Ti−1

to yield a new population Ti of size p by:
• either mutating a single parent trace from Ti−1,
• or through crossover of two parents from Ti−1

with a speciﬁed crossover probability.
(b) Evaluate the ﬁtness of every offspring trace in Ti.
3. Return Tﬁt containing the ﬁttest trace of each generation
The ﬁtness of a trace is deﬁned in terms of state-space cov-
erage and the degree to which the RL task is solved. The
computation of the fuzz traces searches iteratively for traces
with a high ﬁtness by choosing parent traces with a proba-
bility proportional to their ﬁtness. To promote diversity, we
favor mutation over crossover, by setting the crossover prob-
ability to a value < 0.5. The set of the ﬁttest traces Tﬁt will
be used in Step 4 for performance testing. Using the single
ﬁttest trace from every generation helps enforcing variety.
Fitness Computation. We propose a ﬁtness function espe-
cially suited for testing RL agents. For an action trace τA, the
ﬁtness F (τA) is the weighted sum of three normalized terms:
• The positive-reward term rpos(τA, s0) is the normalized

positive reward gained in execτ (τA, s0).

• The negative-reward term rneg(τA, s0) is the normal-
ized inverted negative reward gained in execτ (τA, s0).
• The coverage ﬁtness-term f c(τA, s0) describes the num-
ber of newly visited states by execτ (τA, s0), normalized
by dividing by the maximum number of newly visited
states by any action traces in the current population.
Positive rewards correspond to the degree as to which the
RL task is solved by τA. Negative rewards often corre-
spond to the time required to solve the RL task. Hence if
τA solves the task fast, it would be assigned a small nega-
tive reward. We invert the negative reward to have only pos-
itive ﬁtness terms. We normalize rpos/rneg by dividing it by
the highest r(cid:48)
neg in the current generation. The coverage
ﬁtness-term depends on all states visited in previous popula-
tions. Assume that the current generation is i. Let Covppop
be the set of all states visited by the previous populations
(cid:83)
j<i Tj and let Cov(τA) be the visited states when execut-
ing an action trace τA, i.e., Cov(τA) = (cid:83)
k≤n{sk}, where
execτ (τA, s0) = (cid:104)s0, a0, r0, s1, . . . , sn(cid:105).

pos/r(cid:48)

The coverage ﬁtness-term f c(τA) is then given by

f c(τA) =

|Covppop \ Cov(τA)|
A∈Ti |Covppop \ Cov(τ (cid:48)

A)|

maxτ (cid:48)

.

Algorithm 2: Performance Testing with Fuzz Traces
input : M = (S, s0, A, P, R), policy π, fuzz traces Tﬁt , # episodes nep
output: Avg. accumulated rewards of the agent Ra and the fuzz traces Rt
1 return Rt ← EvalTraces(Tﬁt , s0, nep), Ra ← EvalAgent(π, s0, nep);
2 Function EvalTraces(Tﬁt , s0, nep):
3
4
5
6

τi ← execτ (τA, s0) = (cid:104)s0, a1, r1 . . . sn(cid:105);
Rt,τA ,i ← Σn
return Rt = (ΣτA ∈Tﬁt Σ

nep
i=1 Rt,τA ,i)/(nep · |Tﬁt |)

k=1rk with rk ∈ τi

for i ← 1 to nep do

for τA ∈ Tﬁt do

7
8 Function EvalAgent(π, s0, nep):
for i ← 0 to nep do
9
10
11

τi ← execπ(π, s0) = (cid:104)s0, a1, r1 . . . sn(cid:105) with sn ∈ ST ;
Ra,i ← Σn
return Ra = (Σ

k=1rk with rk ∈ τi;
nep
i=1 Ra,i)/nep

Algorithm 3: Robust Performance Testing
input : M = (S, s0, A, P, R), policy π, fuzz traces Tﬁt , # tests ntest,

# episodes nep, step width w

output: Avg. accumulated rewards Rpl

t and Rpl

a

1 pl ← w;
2 repeat
3
4

for i ← 1 to ntest do

A , s0) = (cid:104)s0 . . . spl(cid:105);

τA ← random action trace ∈ Tﬁt ;
τ −pl ← execτ (τ −pl
R− ← Σpl
Rpl
Rpl

t=1rt with rt ∈ τ −pl;
t,i ← R−+EvalTraces({τ pl+
a,i ← R−+EvalAgent(π, spl, nep);
t,i)/ntest;
a,i)/ntest;

i=1 Rpl
i=1 Rpl

A }, spl, nep);

t ← (Σntest
a ← (Σntest

Rpl
Rpl
pl ← pl + w;

11
12 until |{τA ∈ Tﬁt : |τA| ≥ pl}| < ntest // too few traces of length pl;
13 return (cid:83)
pl{pl (cid:55)→ Rpl
a }

pl{pl (cid:55)→ Rpl

t }, (cid:83)

12

5

6

7

8

9

10

To create an offspring trace, we uniformly select a random
crossover point i ∈ {1, . . . , min(|τA,1|, |τA,2|) − 1}. The
offspring is the concatenation of τ −i
A,2. For mutation,
we repeatedly apply mutation operators. Given a parent trace
τA and a parameter ms deﬁning the potential effect size of a
mutation, we create an offspring τ (cid:48)

A,1 and τ +i

A as following:

1. Uniformly sample x ∈ {1, . . . , ms}.
2. Chose a mutation operator parametrized with x and per-
form the mutation on τA to create an action trace τ (cid:48)
A.
3. Stop with a probability pmstop ∈ (0, 1] and return τ (cid:48)
A.

Otherwise, set τA ← τ (cid:48)

A and continue with Step 1.

The applied mutation operators are (1) Insert, (2) Remove
(3) Change, and (4) Append. Each performs its eponymous
operation on an action sequence of length x at a randomly
chosen index in the parent trace, except for Append.

f c(τA) is a normalized value ≤ 1 and changes during fuzzing
as more states are covered. The ﬁtness F (τA) is given by
F (τA) = λcovf c(τA)+λposrpos(τA)+λneg(1−|rneg(τA)|),
where the factors λj are weights and rpos(τA) and rneg(τA)
are normalized rewards gained when executing τA.

Mutation & Crossover. To generate a new action trace,we
perform either a crossover of two parent traces or mutate a
single parent trace. For crossover, we create a new offspring
trace splitting two parent traces and concatenating the result-
ing subtraces. Let τA,1 and τA,2 be the parent action-traces.

6 Step 4 - Testing for Performance

In the ﬁnal step, we evaluate the performance of trained RL
agents. The evaluation compares the accumulated reward
gained from applying the agent’s policy with the accumulated
reward gained by executing fuzzed traces. Especially in RL
settings where the maximal expected reward is unknown, the
rewards gained by the fuzz traces serve as a benchmark for the
agents’ performance. Furthermore, the fuzz traces are used to
test the agents’ performance from a diverse set of states.
Performance Testing. Simple performance testing starts in

a ﬁxed initial state and compares the average accumulated
reward of the agent with the average accumulated reward re-
sulting from the execution of fuzz traces. Given the policy π
of an agent under test, the fuzz traces Tﬁt , an initial state s0,
and a number of episodes nep, Algo. 2 returns the averaged
accumulated rewards of the agent Ra and the fuzz traces Rt.
Robust Performance Testing. Robust performance testing
targets checking the robustness of learned policies in poten-
tially unknown situations. For this purpose, we use the fuzz
traces to bring the agent into a diverse set of states and apply
the policy of the agent from these states onward until a termi-
nal state is reached. To cover states close to the initial state as
well as close to the goal, we actually use fuzz trace preﬁxes of
increasing length. The averaged accumulated rewards of the
agent traces and the fuzz traces serve as performance metric.
Let π be the policy of the RL agent under test, Tﬁt be the
fuzz traces, s0 be an initial state, w be a step width to increase
the fuzz trace preﬁx-length, and ntest and nep be numbers
of tests and episodes, Algo. 3 implements our robust perfor-
mance testing approach. Starting from preﬁx length pl = w,
for each pl the amount of executed tests is ntest. To do so,
we ﬁrst select a random trace (line 4) and execute its preﬁx
of length pl to arrive at a state spl. From spl, we compute the
accumulated reward of the fuzz trace (line 6) and of the agent
averaged over nep episodes (line 8) and add the accumulated
reward of the common preﬁx R− to both. We average the
accumulated rewards over all tests for each pl individiually
(lines 9 and 10) and ﬁnally return all results in Line 13.

7 Experimental Evaluation

We evaluate our testing framework on trained deep RL agents
for the video game Super Mario Bros., trained with varying
numbers of training episodes and different action sets.
Setup for RL. We use a third-party RL agent that operates
in the OpenAI gym environment and uses double deep Q-
Networks [Feng et al., 2020]. Details on the learning param-
eters along with more experiments and the source code are in-
cluded in the technical appendix. To evaluate different agents,
we test agents at varying stages of learning having three dif-
ferent action sets: (1) 2-moves: fast running and jumping to
the right. (2) basic: 2-moves plus slow running to the right
and left. (3) right-complex: basic without running left, but ac-
tions for pressing up and down, resulting in the largest action
set. Unless otherwise noted, we present results from training
for 80k episodes. We stopped training at this point since we
observed only little improvement from 40k to 80k episodes,
which is also twice as long as suggested [Feng et al., 2020].
Setup for Search and Fuzzing. The search for the refer-
ence traces uses the 2-moves actions, which are sufﬁcient to
complete most levels in Super Mario Bros. We compute
fuzz traces for each action set of the different RL agents.
We fuzzed for 50 generations with a population of 50, used
a mutation stop-probability pmstop = 0.2 with effect size
ms = 15 and ﬁtness weights λcov = 2, λpos = 1.5, and
λneg = 1, to focus on exploration. With a crossover proba-
bility of 0.25, we mostly rely on mutations. The search and
fuzzing were performed in a standard laptop in a few minutes
and a few hours, respectively. Compared to the training that

simple

interval (is = 1)

action coverage (k = 1)

action coverage (k = 2)

y
c
n
e
u
q
e
r
F
t
c
i
d
r
e
V

l
i
a
F

0.6

0.4

0.2

0

2-moves

right-complex

basic

Figure 3: Safety Testing: Relative frequency of fail verdicts

simple
interval (is = 1)
action coverage (k = 1)
action coverage (k = 2)

y
c
n
e
u
q
e
r
F
t
c
i
d
r
e
V

l
i
a
F

1

0.5

0

0

2

4

6

8

10

12

Boundary State

Figure 4: Safety Testing of the right-complex agent: Relative fre-
quencies of fail verdicts at boundary states

took several days on a dedicated cluster, the computational
effort for testing is relatively low. For safety testing, we use
10 repetitions and a test length of l = 40 and for performance
testing we use ntest = nep = 10 and step width w = 20.

Safety Testing. Fig. (3) shows the relative number of
fail verdicts averaged over all tests at all boundary states for
agents with different action sets. For any agent, all test suites
found safety violations. For instance, the simple test suite
produces fail verdicts in about 38% of the cases when test-
ing the right-complex RL agent. The agent with 2-moves is
tested to be the safest agent, failing only 10% of test cases of
the simple test suite. For right-complex, the least safe agent,
Fig. (4) depicts the relative number of fail verdicts distributed
over the boundary states, when executing all test suites. Note
that the results are affected by stochasticity and they are nor-
malized, so that all results are within [0, 1] even though the
extended test suites perform more tests. We can see that the
early boundary states that are explored the most cause the
least issues. Furthermore, we observe that the boundary inter-
val test suite ﬁnds safety violations not detected by the simple
test suite, e.g., at the boundary states 3 and 4.
Robust Performance Testing. We perform robust perfor-
mance testing on all agents trained for 20k and 80k episodes,
respectively. Fig. (5) shows the average accumulated rewards
(y-axis) gained by the agents and the fuzz traces when per-
forming fuzz trace preﬁxes of the length given by the x-axis.
It can be seen that initially only the well-trained 2-moves
agent surpasses the performance benchmark set by the fuzz
traces. Training for 20k episodes is not enough for any agent
to achieve rewards close to the fuzz traces and the basic agent
does not improve much with more training. Hence, a larger

2-moves (20k)
right-comp (80k)
fuzz traces

2-moves (80k)
basic (20k)

right-comp (20k)
basic (80k)

n
r
u
t
e
R
e
g
a
r
e
v
A

3,000

2,000

1,000

40

80

120

160

200

240

280

320

360

Fuzz Trace Preﬁx Length pl

Figure 5: Robust Performance Testing: Average accumulated re-
wards of fuzz traces and the agents trained for 20k and 80k
episodes.

Safety Violations

Average Return

y
c
n
e
u
q
e
r
F
t
c
i
d
r
e
V

l
i
a
F
e
v
i
t
a
l
e
R

0.8

0.6

0.4

0.2

0

400

300

200

100

0

n
r
u
t
e
R
e
g
a
r
e
v
A

2-moves
(20k)

right-c
(20k)

basic
(20k)

2-moves
(80k)

right-c
(80k)

basic
(80k)

Figure 6: Average frequency of safety violations and average ac-
cumulated rewards during testing with the simple test suite.

action space may hurt robustness. The ability to move left of
the basic agent also increases the state space of the underlying
MDP, since only moving right induces a DAG-like structure.
This explains the poor performance of the basic agents.
Relationship between Safety and Performance. Finally, we
investigate whether performance expressed via rewards im-
plies safety. Fig. (6) shows the average number of safety vi-
olations and the average accumulated rewards gained during
testing with a simple test suite with all agents trained for 20k
and 80k episodes, respectively. Comparing the agents with
low amount of training, the safest agent (basic) is also the
one that gains the lowest reward. For well-trained agents, the
safest agent (2-moves) also receives the most reward. The
large negative reward assigned to safety violations (losing a
life) may not be sufﬁcient to enforce safe behavior of right-
complex. Hence, our testing method may point to issues in re-
ward function design. However, computing the Pearson cor-
relation coefﬁcient between fail verdict frequency and mean
accumulated reward for all agents at four stages of training
with all test suites reveals a moderate negative correlation of
−0.7, thus high reward often implies low fail frequency.

8 Concluding Remarks
We present a search-based testing framework for safety and
robust-performance testing of RL agents. For safety test-
ing, we apply backtracking-based DFS to identify relevant
states and adapt test-adequacy criteria from boundary value
and combinatorial testing. For performance testing, we apply
genetic-algorithm-based fuzzing starting from a seed trace
found by the DFS. We show both testing methodologies on

an off-the-shelf deep RL agent for playing Super Mario Bros,
where we ﬁnd safety violations of well-trained agents and an-
alyze their performance and robustness. To the best of our
knowledge, we propose one of the ﬁrst testing frameworks
tailored toward RL. For future work, we will instantiate our
framework for more RL tasks, where solutions can be found
through search and other domain-speciﬁc approaches. Fur-
thermore, we plan to investigate different fuzzing approaches
like fuzzing on the policy level rather than on the trace level.
Acknowledgments. This work has been supported by the
”University SAL Labs” initiative of Silicon Austria Labs
(SAL) and its Austrian partner universities for applied fun-
damental research for electronic based systems. We would
like to acknowledge the use of HPC resources provided
by the ZID of Graz University of Technology. Addition-
ally, this project has received funding from the European
Union’s Horizon 2020 research and innovation programme
under grant agreement N◦ 956123 - FOCETA. We also thank
Vedad Hadˇzi´c for his help in the initial development of the
depth-ﬁrst search of the reference trace.

References
[Achiam and Amodei, 2019] Joshua Achiam and Dario
Benchmarking safe exploration in deep

Amodei.
reinforcement learning. Preprint. Under review, 2019.
[Agarwal et al., 2021] Rishabh Agarwal, Max Schwarzer,
Pablo Samuel Castro, Aaron C Courville, and Marc Belle-
mare. Deep reinforcement learning at the edge of the sta-
tistical precipice. In NeurIPS, volume 34, 2021.

[Alshiekh et al., 2018] Mohammed Alshiekh,

Roderick
Bloem, R¨udiger Ehlers, Bettina K¨onighofer, Scott
Niekum, and Ufuk Topcu. Safe reinforcement learning
via shielding. In AAAI-18, pages 2669–2678, 2018.

[Aschermann et al., 2020] Cornelius Aschermann, Sergej
Schumilo, Ali Abbasi, and Thorsten Holz. Ijon: Explor-
ing deep state spaces via fuzzing. In IEEE SP 2020, pages
1597–1612, 2020.

[Bellemare et al., 2013] Marc G. Bellemare, Yavar Naddaf,
Joel Veness, and Michael Bowling. The arcade learning
environment: An evaluation platform for general agents.
J. Artif. Intell. Res., 47:253–279, 2013.

[B¨ottinger et al., 2018] Konstantin B¨ottinger, Patrice Gode-
froid, and Rishabh Singh. Deep reinforcement fuzzing. In
IEEE SP Workshops 2018, pages 116–122, 2018.

[Brockman et al., 2016] Greg Brockman, Vicki Cheung,
Ludwig Pettersson, Jonas Schneider, John Schulman, Jie
arXiv
Tang, and Wojciech Zaremba. OpenAI gym.
preprint arXiv:1606.01540, 2016.

[Corsi et al., 2021] Davide Corsi, Enrico Marchesini, and
Alessandro Farinelli. Formal veriﬁcation of neural net-
works for safety-critical tasks in deep reinforcement learn-
In UAI, volume 161 of Proceedings of Machine
ing.
Learning Research, pages 333–343, 2021.

[Drozd and Wagner, 2018] William Drozd and Michael D.
FuzzerGym: A competitive framework for

Wagner.
fuzzing and learning. CoRR, abs/1807.07490, 2018.

[Ehlers, 2017] R¨udiger Ehlers. Formal veriﬁcation of piece-
wise linear feed-forward neural networks. In ATVA 2017,
volume 10482 of LNCS, pages 269–286, 2017.

[Feng et al., 2020] Yuansong Feng, Suraj Subramanian,
Howard Wang, and Steven Guo. Train a mario-playing RL
agent. https://pytorch.org/tutorials/intermediate/mario rl
tutorial.html, 2020. A PyTorch tutorial [online], accessed:
2022, January 07.

[Garcıa and Fern´andez, 2015] Javier Garcıa and Fernando
Fern´andez. A comprehensive survey on safe reinforce-
ment learning. Journal of Machine Learning Research,
16(1):1437–1480, 2015.

[Harel-Canada et al., 2020] Fabrice Harel-Canada, Lingx-
iao Wang, Muhammad Ali Gulzar, Quanquan Gu, and
Miryung Kim. Is neuron coverage a meaningful measure
In ESEC/FSE, pages
for testing deep neural networks?
851–862. ACM, 2020.

[Karakovskiy and Togelius, 2012] Sergey Karakovskiy and
Julian Togelius. The mario AI benchmark and competi-
tions. IEEE T-CIAIG, 4(1):55–67, 2012.

[Khalili and Tacchella, 2014] Ali Khalili and Armando Tac-
chella. Learning nondeterministic Mealy machines.
In
ICGI 2014, volume 34 of JMLR Workshop and Conference
Proceedings, pages 109–123, 2014.

[Kiran et al., 2021] B Ravi Kiran, Ibrahim Sobh, Victor Tal-
paert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yo-
gamani, and Patrick P´erez. Deep reinforcement learning
for autonomous driving: A survey. IEEE Transactions on
Intelligent Transportation Systems, 2021.

[K¨uttler et al., 2020] Heinrich K¨uttler, Nantas Nardelli,
Alexander H. Miller, Roberta Raileanu, Marco Selvatici,
Edward Grefenstette, and Tim Rockt¨aschel. The NetHack
Learning Environment. In NeurIPS 2020, 2020.

[Ma et al., 2018] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang,
Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su,
Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. Deep-
Gauge: multi-granularity testing criteria for deep learning
systems. In ASE, pages 120–131. ACM, 2018.

[Man`es et al., 2021] Valentin J. M. Man`es, HyungSeok Han,
Choongwoo Han, Sang Kil Cha, Manuel Egele, Edward J.
Schwartz, and Maverick Woo. The art, science, and engi-
neering of fuzzing: A survey. IEEE Trans. Software Eng.,
47(11):2312–2331, 2021.

[Pathak et al., 2017] Shashank Pathak, Luca Pulina, and Ar-
mando Tacchella. Veriﬁcation and repair of control poli-
cies for safe reinforcement learning. Applied Intelligence,
48:886–908, 2017.

[Pei et al., 2019] Kexin Pei, Yinzhi Cao, Junfeng Yang, and
Suman Jana. DeepXplore: automated whitebox testing of
deep learning systems. Commun. ACM, 62(11):137–145,
2019.

[Pezz`e and Young, 2007] Mauro Pezz`e and Michal Young.
Software testing and analysis - process, principles and
techniques. Wiley, 2007.

[Pranger et al., 2021] Stefan Pranger, Bettina K¨onighofer,
Martin Tappler, Martin Deixelberger, Nils Jansen, and
Roderick Bloem. Adaptive shielding under uncertainty.
In ACC, pages 3467–3474, 2021.

[Schrittwieser et al., 2020] Julian Schrittwieser,

Ioannis
Antonoglou, Thomas Hubert, Karen Simonyan, Laurent
Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart,
Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model.
Nature, 588(7839):604–609, 2020.

[Schumilo et al., 2022] Sergej Schumilo, Cornelius Ascher-
mann, Andrea Jemmett, Ali Abbasi, and Thorsten Holz.
Nyx-net: network fuzzing with incremental snapshots. In
EuroSys, pages 166–180, 2022.

[Scott et al., 2021] Joseph Scott, Trishal Sudula, Hammad
Rehman, Federico Mora, and Vijay Ganesh. Bandit-
fuzz: Fuzzing SMT solvers with multi-agent reinforce-
ment learning. In FM 2021, volume 13047 of LNCS, pages
103–121, 2021.

[Silver et al., 2016] David Silver, Aja Huang, Chris J Maddi-
son, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. Mastering the game of
Go with deep neural networks and tree search. Nature,
529(7587):484, 2016.

[Sutton and Barto, 1998] Richard S. Sutton and Andrew G.
Barto. Reinforcement learning - an introduction. Adaptive
computation and machine learning. MIT Press, 1998.
[Tassa et al., 2018] Yuval Tassa, Yotam Doron, Alistair
Muldal, Tom Erez, Yazhe Li, Diego de Las Casas,
David Budden, Abbas Abdolmaleki, Josh Merel, Andrew
Lefrancq, et al. DeepMind control suite. arXiv preprint
arXiv:1801.00690, 2018.

[Tian et al., 2018] Yuchi Tian, Kexin Pei, Suman Jana, and
Baishakhi Ray. DeepTest: automated testing of deep-
In ICSE, pages
neural-network-driven autonomous cars.
303–314. ACM, 2018.

[Trujillo et al., 2020] Miller Trujillo, Mario Linares-
V´asquez, Camilo Escobar-Vel´asquez, Ivana Dusparic, and
Nicol´as Cardozo. Does neuron coverage matter for deep
In ICSE
reinforcement learning?: A preliminary study.
’20 Workshops, pages 215–220, 2020.

[Wang et al., 2021] Daimeng Wang, Zheng Zhang, Hang
Zhang, Zhiyun Qian, Srikanth V. Krishnamurthy, and
Nael B. Abu-Ghazaleh. Syzvegas: Beating kernel fuzzing
In USENIX Security
odds with reinforcement learning.
2021, pages 2741–2758, 2021.

[Zeller et al., 2021] Andreas Zeller, Rahul Gopinath, Marcel
B¨ohme, Gordon Fraser, and Christian Holler. The Fuzzing
Book. CISPA Helmholtz Center for Information Security,
2021. accessed: 2022, January 07.

[Zhang et al., 2018] Mengshi Zhang, Yuqun Zhang, Ling-
ming Zhang, Cong Liu, and Sarfraz Khurshid. Deep-
Road: Gan-based metamorphic testing and input valida-
tion framework for autonomous driving systems. In ASE,
pages 132–142. ACM, 2018.

A Conﬁguration for Deep RL
The RL agents that we test are based on a PyTorch tuto-
rial [Feng et al., 2020] and use Double Q-learning to play Su-
per Mario Bros. The Deep Q networks consist of a sequence
of three convolutional+ReLU layers and two fully connected
linear layers. The input for the ﬁrst convolutional layer is
a stack of 4 transformed images showing the video game
screen in 4 consecutive frames. The transformation consists
of a grey-scale conversion and downsampling from a image
of size 240 by 256 to a square image with 84 × 84 pixels.
Stacking 4 such images enables the agent to detect motion,
since it is not possible to, e.g., detect whether Mario is jump-
ing or landing based on a single image. The output of the
ﬁnal layer is the Q value of an action in a state consisting of
4 stacked, transformed images.

In the regular, non-sparse reward scheme (see below for
results on sparse rewards), the agent receives positive reward
proportional to the distance travelled in positive horizontal
direction (up to +5). The agent receives negative reward for
the time spent in the level. Additionally, it receives a large
constant negative reward for dying, which is 5 times as large
as the maximal positive reward of a single state-action pair
(i.e., −25).

The parameters for learning are set as follows for all train-

ing runs:

• Discount factor: γ = 0.9.
• Exploration rate decay: εdecay = 0.9999995, the multi-
plicative factor with which the exploration rate ε decays
in the ε-greedy exploration.
• Minimum exploration rate:

εmin = 0.1, the smallest

exploration rate during training.

• Burn-in:

burn = 100, the number of experiences be-

fore training starts.

• Batch size:

batch = 20, the number of experiences

sampled from memory at once for learning.

• Memory size: mem = 30000, speciﬁes that up to mem
experiences are stored in memory, from which we sam-
ple for learning.
• Learn Interval:

li = 3, the interval between updates of

the online Q-network in number of experiences.

• Synchronization Interval: si = 1000, the interval be-
tween synchronization of the online Q-network and the
target Q-network in number of experiences.

We ﬁne-tuned the parameters until the learning perfor-
mance was satisfactory, and then maintained the parameters
unchanged for the training of different agents. The difference
between different trained agents are in action set (2 moves,
basic, right-complex), training episodes (20k, 40k and 80k)
and reward scheme (sparse or dense).

B Additional Experimental Results
B.1 Testing Agents Trained with Sparse Rewards
In addition to the agents evaluated in Sec. 7, we evaluated
agents trained using a sparse reward function. The sparse
reward scheme provides reward less often for progressing in
the level. More concretely, it provides rewards when com-
pleting a segment of the level (+10 at each of ﬁve segments),
for defeating enemies (+1), and for getting power-ups (+10).

Since the maximum reward under this scheme is lower, neg-
ative reward from losing lives (−25) should have a larger im-
pact. The intuition being that trained agents have a larger
incentive for safe behavior (gaining power-ups) and avoiding
unsafe behavior.
Safety testing with Sparse Rewards. Unfortunately, we did
not observe the desired results. Fig. (7) shows the relative
frequency of fail verdicts observed during testing after train-
ing for 20k episodes. Fig. (8) shows the same for the normal
reward scheme after training for 20k episodes (as opposed to
80k reported in Fig. (3)) as a reference. Except in one case,
testing the right-complex with the simple test suite, the sparse
rewards lead to worse behavior w.r.t. safety, despite the fact
that the punishment for unsafe behavior is larger.

simple

environment (e = 1)

action coverage (k = 1)

action coverage (k = 2)

y
c
n
e
u
q
e
r
F
t
c
i
d
r
e
V

l
i
a
F

1

0.8

0.6

0.4

0.2

0

2-moves

right-complex

basic

Figure 7: Safety Testing: Relative frequency of fail verdicts under
the sparse reward scheme after training for 20k episodes.

simple

interval (is = 1)

action coverage (k = 1)

action coverage (k = 2)

y
c
n
e
u
q
e
r
F

t
c
i
d
r
e
V

l
i
a
F

1

0.8

0.6

0.4

0.2

0

2-moves

right-complex

basic

Figure 8: Safety Testing: Relative frequency of fail verdicts under
the normal reward scheme after training for 20k episodes.

Analyzing the behavior in more detail, we see an interest-
ing effect, though. Fig. 9 shows the fail verdicts frequencies
at the boundary states from safety testing the right-complex
agent with sparse reward. While the agent trained with nor-
mal rewards (Fig. (4)) showed unsafe behavior at boundary
state 12, training with sparse reward led to safe behavior at
this state. At most other states, especially at the beginning,
sparse rewards were detrimental to safety.

Fig. 9 includes an effect of stochasticity that we want to
brieﬂy explain. At boundary state 11, the simple test suite
ﬁnds a non-zero amount of issues and the environment test
suite does not ﬁnd any issues, even though the latter includes
the former. This can be explained by stochastic behavior,

simple
interval (is = 1)
action coverage (k = 1)
action coverage (k = 2)

y
c
n
e
u
q
e
r
F
t
c
i
d
r
e
V

l
i
a
F

1.5

1

0.5

0

Safety Violations

Average Return

y
c
n
e
u
q
e
r
F
t
c
i
d
r
e
V

l
i
a
F
e
v
i
t
a
l
e
R

0.8

0.6

0.4

0.2

0

400

300

200

100

0

n
r
u
t
e
R
e
g
a
r
e
v
A

0

2

4

6

8

10

12

Boundary State

Figure 9: Safety Testing of the right-complex agent with sparse re-
wards: Relative frequencies of fail verdicts at boundary states.

2-moves

right-c

basic

Figure 11: Average frequency of safety violations (fails verdicts)
and average accumulated reward during testing of the sparse-reward
agents with the simple test suite.

2-moves (20k)
right-comp. (40k)
fuzz traces

2-moves (40k)
basic (20k)

right-comp. (20k)
basic (40k)

n
r
u
t
e
R
e
g
a
r
e
v
A

3,000

2,000

1,000

y
c
n
e
u
q
e
r
F
t
c
i
d
r
e
V

l
i
a
F

1

0.8

0.6

0.4

0.2

0

simple

interval (is = 1)

action coverage (k = 1)

action coverage (k = 2)

40

80

120

160

200

240

280

320

360

Fuzz Trace Preﬁx Length pl

Figure 12: Safety Testing: Relative frequency of fail verdicts when
testing agents trained to complete level 1-2 after training for 80k
episodes.

2-moves

right-complex

basic

Figure 10: Performance testing of agents trained with sparse re-
wards.

causing one execution of the test case at boundary state 11
to ﬁnd an issue that was not detected while executing the en-
vironment test suite.
Performance testing with Sparse Rewards. A possible ex-
planation for unsafe behavior may simply be that training
with sparse rewards worked less well, resulting in worse-
performing agents. Fig. 10 shows the results of performance
testing these agents after 20k and 40k of training, respec-
tively. In the performance testing, we apply the normal re-
ward scheme to enable a better comparison with the results
present in Sect. 7.
In both cases, the agents are trained to
solve the same task, thus well-trained agents should perform
well with either reward scheme. None of the agents’ perfor-
mance comes close to the reward gained by the fuzz traces,
except for preﬁx lengths greater than 280, i.e., very late in the
Super Mario Bros. level. Since we observed that the perfor-
mance difference between training for 20k and 40k is con-
sistently relatively low, we refrained from training the agents
with sparse rewards for the full 80k episodes used for the nor-
mal rewards.
Comparing Safety and Performance. Comparing safety
and performance during safety testing in Fig. 11, it can be
seen that the safest agents receives the highest reward and the
second and third safest receive the second-highest and third-
highest rewards.

B.2 Testing Agent Trained for 1-2
In addition to agents trained to complete the ﬁrst level of Su-
per Mario Bros., we also tested agents trained to complete
the second level, which is called 1-2. For this case study, we
again used the normal reward scheme.
Safety Testing. Fig. 12 shows a summary of the results
of safety testing RL agents that have been trained for 80k
episodes. The execution of the second action coverage test
suite with k = 2 on the right-complex agent unfortunately
did not ﬁnish in time for the submission. It can be seen that
the agents performed considerably worse w.r.t. safety than the
agents trained for the ﬁrst level. Looking at the detailed re-
sults for the 2-moves in Fig. 13 presents a possible expla-
nation. The level contains more boundary states, i.e., more
safety-critical situations that the agents need to deal with.
There are several states, where the 2-moves agent, despite be-
ing the safest, has a relative fail frequency close to 1.
Performance Testing. Fig. 14 shows the results from ro-
bust performance testing. The agents do not achieve the per-
formance benchmark set by the fuzz testing until very late
in the level, similarly to the sparse reward scheme. At the
stage where the agents’ performance results are close to the
fuzz trace performance, several safety-critical situations have
been already passed by the fuzz trace preﬁx. In contrast to
the ﬁrst level, the basic agent performs similarly well as the
others and increased training does not help performance; the
2-moves trained for 40k episodes even performs better than

simple
interval (is = 1)
action coverage (k = 1)
action coverage (k = 2)

y
c
n
e
u
q
e
r
F
t
c
i
d
r
e
V

l
i
a
F

1.5

1

0.5

0

0

5

10

15

Boundary State

Figure 13: Safety Testing of the 2-moves agent trained to complete
level 1-2: Relative frequencies of fail verdicts at boundary states.

2-moves (80k)
right-comp (40k)
fuzz traces

2-moves (40k)
basic (80k)

right-comp (80k)
basic (40k)

3,000

2,000

1,000

n
r
u
t
e
R
e
g
a
r
e
v
A

40

80

120 160 200 240 280 320 360 400

Fuzz Trace Preﬁx Length pl

Figure 14: Performance testing of agents trained to complete level
1-2.

Safety Violations

Average Return

y
c
n
e
u
q
e
r
F

t
c
i
d
r
e
V

l
i
a
F
e
v
i
t
a
l
e
R

0.8

0.6

0.4

0.2

0

400

300

200

100

0

n
r
u
t
e
R
e
g
a
r
e
v
A

2-moves

right-c

basic

Figure 15: Average frequency of safety violations (fails verdicts)
and average accumulated reward during testing of agents trained to
complete level 1-2 with the simple test suite.

when trained for 80k episodes.
Comparing Safety and Performance. Finally, we want to
compare safety and performance during safety testing for
these agents as well. Fig. 15 shows the average accumulated
rewards and the relative fail frequency during safety testing
with the simple test suite. As for the ﬁrst level, the 2-moves
agents is the safest and achieves the highest reward. Apart
from that, the difference between the other two agents is rel-
atively small. This shows that the small action space enables
considerably better training.

