Automatic Classiﬁcation of Bug Reports Based
on Multiple Text Information and Reports’
Intention

Fanqi Meng, Xuesong Wang(B), Jingdong Wang(B), and Peifang Wang

School of Computer, Northeast Electric Power University, Jilin City, Jilin, China
{mengfanqi,wangjingdong}@neepu.edu.cn, wxs970705@163.com

Abstract. With the rapid growth of software scale and complexity, a large number
of bug reports are submitted to the bug tracking system. In order to speed up defect
repair, these reports need to be accurately classiﬁed so that they can be sent to the
appropriate developers. However, the existing classiﬁcation methods only use the
text information of the bug report, which leads to their low performance. To solve
the above problems, this paper proposes a new automatic classiﬁcation method of
bug reports. The innovation is that when categorizing bug reports, in addition to
using the text information of the report, the intention of the report (i.e. “suggestion”
or “explanation”) is also considered, thereby improving the performance of the
classiﬁcation. First, we collect bug reports from four ecosystems (Apache, Eclipse,
Gentoo, Mozilla) and manually annotate them to construct an experimental data
set. Then, we use Natural Language Processing technology to preprocess the data.
On this basis, BERT and TF-IDF are used to extract the features of the intention and
the multiple text information. Finally, the features are used to train the classiﬁers.
The experimental result on ﬁve classiﬁers (including K-Nearest Neighbor, Naive
Bayes, Logistic Regression, Support Vector Machine and Random Forest) show
that our proposed method achieves better performance and its F-Measure achieves
from 87.3% to 95.5% .

Keywords: Automatic classiﬁcation · Bug report · Defect repair · Report
intention

1 Introduction

Defect repair has an important impact on software quality assurance. It is the main
activity in the later maintenance phase of software engineering. In recent years, with the
vigorous development of the software engineering industry, the architecture complexity
and code capacity of software systems have reached a new level that makes it difﬁcult
for developers to understand and manage [1]. This trend leads to a large number of bugs
inevitably generated in the development process of software systems. To ﬁx these bugs,
developers must check the bug report [2]. The bug report describes the defects of the
software system in the form of text, which contains multiple tags such as ID, Reporter,
Summary, etc., as shown in Fig. 1. In the past, managers classiﬁed bug reports based on

© Springer Nature Switzerland AG 2022
Y. Aït-Ameur and F. Cr˘aciun (Eds.): TASE 2022, LNCS 13299, pp. 131–147, 2022.
https://doi.org/10.1007/978-3-031-10363-6_9

132

F. Meng et al.

tags so that they could assign the reports to appropriate developers to accomplish bug
ﬁxes. However, this is a very time-consuming task as there are too many bug reports to
check manually. Moreover, due to the different experience and knowledge background
of the reporter, the report submitted in the Bug Tracking System may have incorrect
tags [3]. These wrong tags will cause the bug report to not be correctly assigned to the
appropriate developers, thereby increasing the difﬁculty of defect repair [4, 5]. In order
to reduce this impact and accelerate the speed of defect repair, the software engineering
industry needs accurate and automated classiﬁcation methods for bug reports.

Fig. 1. Several examples of bug report from Bugzilla

In recent years, many researchers have explored the automatic classiﬁcation of bug
reports. Among them, Antoniol et al. [6] classiﬁed bug reports via text mining technology.
It proved that automatically classify reports into bug and other types through training
models is effective and feasible. Zhou et al. [7] proposed a hybrid method combining
text mining and data mining techniques to determine whether a new bug report is a
real bug. This method also considers the structured information of the report on the
basis of purely mining text description [5] (Such as severity and priority). Lamkanfj
et al. [8] adopted machine learning technology to classify bug reports into severity
and non-severity. Similarly, Tian et al. [9] proposed a nearest neighbor solution based on
information retrieval to predict the severity of the bug report. They focused on predicting
the ﬁve severity levels of the report, namely: Blocker, Critical, Major, Minor and Trivial.
In addition, some scholars are concerned about the quality of bug reports [10] and the
imbalance of data sets [11, 12] and other issues.

Reporters submit reports with clear intentions. After reading the summary of a large
number of open source software bug reports, we found that the intention of the summary
text content can be classiﬁed into two types: explanation or suggestion. However, there
is no intention label in the Bug Tracking System. A large number of existing studies
fail to consider the intention of the report when classifying bug reports, which lead
to lower performance of their methods. Considering that these intentions will affect the
classiﬁcation of reports, the method in this paper incorporates the intentions of the report.
Among them, the explanation refers to the description of the defect, such as a problem
or the cause of the problem in a certain location, and the suggestion refers to a solution
to the defect, such as how to deal with a certain problem. Table 1 shows real examples
of software bug reports in four different ecosystems and their intention.

Automatic Classiﬁcation of Bug Reports

133

Table 1. Bug reports with intention tags

Ecosystem Bug ID

Summary

Intention

Apache

Eclipse

Gentoo

Mozilla

63099

Regression in JMeter 5.0 due to ﬁx of Bug 62478

Explanation

82281

logical structures table should sort on name

Suggestion

76636

Kernel module dvb-ttpci does not ﬁnd its ﬁrmware

Explanation

277324

Copy XML doesn’t work on #document nodes

Explanation

To sum up, the contributions of this paper are as follows:

(1) A new bug report classiﬁcation method is proposed. Based on the text ﬁeld clas-
siﬁcation, the intention of the report is additionally considered, and the report is
classiﬁed as bug and no-bug.

(2) An automatic classiﬁcation model was constructed based on the proposed method,
and the classiﬁcation performance of ﬁve classiﬁers (K-NN, NB, LR, SVM, RF)
was observed. To measure the performance, the Accuracy, Precision, Recall, and
F-measure are calculated.

(3) A dataset containing the intention and type of report that can be used by researchers

to further explore the automatic classiﬁcation of bug reports.

The rest of this paper is as follows: The Sect. 2 introduces the related work of bug
report classiﬁcation. The Sect. 3 introduces the proposed method. The Sect. 4 shows the
experiment. The Sect. 5 discusses the experiment. Finally, the Sect. 6 summarizes the
full text and puts forward insights on future work.

2 Related Work

Bug report classiﬁcation helps developers understand and ﬁx software defects. Due to the
skyrocketing number of bug reports, manual classiﬁcation has become time-consuming
and laborious. For a long time, researchers have been exploring how to implement
automatic classiﬁcation of bug reports [18]. This section will summarize some existing
research work.

The earliest bug classiﬁcation method is Orthogonal Defect Classiﬁcation (ODC)
proposed by IBM’s Chillarege et al. [19] in 1992. It is a method between qualitative
analysis and quantitative analysis, including 13 categories (such as functions, interfaces,
documents, etc.). In 2008, Antoniol et al. [6] proposed an automatic classiﬁcation method
for bug reports, using vector space technology to extract features, and training Decision
Trees (DT), NB and LR classiﬁers to judge whether the report is a bug. The results
show that the classiﬁcation accuracy on Mozila, Eclipse and JBoss projects can reach
77% to 82%. In 2013, Pingclasai et al. [20] proposed a classiﬁcation method to identify
the authenticity of bugs. They adopted the topic model of Latent Dirichlet Allocation
(LDA) combined with NB and Linear Logistic Regression (LLR) classiﬁers, and the
accuracy of the three projects of HTTP-Client, Jackrabbit and Lucene reached 66%

134

F. Meng et al.

to 76%, 65% to 77% and 71% to 82%. Similarly, kukkar et al. [13] applied a hybrid
method to identify whether the report is a bug or non-bug in 2019, which integrates
TM, NLP and ML technologies. They observed the performance of Term Frequency-
Inverse Document Frequency (TF-IDF) and feature selection and K-NN classiﬁers on
ﬁve different data sets (Mozilla, Eclipse, JBoss, Firefox, OpenFOAM). Experiments
show that the performance of the K-NN classiﬁer varies with different data sets, and its
F-measure is 78% to 96%.

In addition, there are also researchers who classify the severity of bug reports. Men-
zies et al. [21] presented a new automated method called SERVERS in 2008. This
method uses TF-IDF, InfoGain and Rule Learning technology to divide the severity of
bug reports into 5 categories, from the most severe to the most insensitive. In 2011,
Sari et al. [22] applied the InfoGain method to ﬁlter out 5 valid attributes from the 14
attributes reported in the bug report for severe and non-serious classiﬁcation. These 5
attributes are component, qa_contact, summary, cc_list, and product. Their combination
can achieve 99.83% accuracy on the SVM model. In 2016, Zhang et al. [23] improved
the REP (i.e. REP theme) and K-NN algorithm to search for historical bug reports simi-
lar to new bugs, further extracted their features to predict the severity, and classiﬁed the
bug reports into Blocker, Trivial, Critical, Minor, and Major. The results show that their
proposed method can effectively improve the accuracy of the prediction of the severity
of bug reports. In 2019, Kukkar et al. [24] believed that traditional Machine Learning
classiﬁers could not capture some potentially important features of bug reports, so they
proposed a classiﬁcation method based on Deep Learning. The model uses the N-gram
algorithm and Convolutional Neural Network (CNN) and Random Forest with Boost-
ing to solve the multi-level severity prediction problem of bug reports. Their work has
achieved good results, with an average accuracy rate of 96.34% in the ﬁve open source
projects.

Not only limited to bug or severity, but also researchers have proposed different clas-
siﬁcation models from other perspectives. Du et al. [25] developed an automatic classiﬁ-
cation framework based on word2vec in 2017, which classiﬁed bug reports into different
fault trigger categories from four granularities, including Bug/Non-Bug, BOH/MAN,
ARB/NAM, and NAM/ARB. In 2014, Tan et al. [26] believed that semantic, security
and concurrency problems are strongly related to software systems. Based on the above
assumptions, they studied the distribution of these three types in projects such as Apache,
Mozilla and Linux, and automatically classiﬁed bug reports into the above three types
through machine learning technology. The average F-measure is about 70%. Recently,
Catolino et al. [27] deﬁned a new bug report classiﬁcation pattern in 2019, includ-
ing 9 defect types (Conﬁguration issue, Network issue, Database-related issue, GUI-
related issue, Performance issue, Permission/deprecation issue, Security issue, Program
anomaly issue, Test code-related issue). Compared with Tan et al. [26], the method of
Catolino et al. can provide a clearer and comprehensive overview of the types of bug
reports. At the same time, the automatic model they built also achieved higher F-Measure
and AUC-ROC (64% and 74%).

It can be seen from related work that many researchers have achieved good results in
the automatic classiﬁcation of bug reports. On the basis of existing research, the focus of
this article is to add a new factor, that is, the intention of the report, when implementing

Automatic Classiﬁcation of Bug Reports

135

the automatic classiﬁcation of bug reports. We believe that increasing this factor will
improve classiﬁcation performance.

3 Methodology

This section details the proposed classiﬁcation method for bug reports. The framework is
shown in Fig. 2. First, we collect and manually mark bug reports in the open repository,
and then perform preprocessing steps on them. Then, we use BERT and TF-IDF methods
to extract features. And the text feature and frequency feature are merged and normalized.
Next, we input the extracted features into ﬁve classiﬁers (including K-NN, NB, LR, SVM
and RF). Finally, we categorize bug reports into bug and non-bug.

Training stage
Training stage

Bug reports

Preprocessing
Preprocessing

Normalization

Tokenization
Tokenization

Stop word removal
Stop word removal

Stemming
Stemming

 Feature Extraction
 Feature Extraction

Summary
Summary

BERT
BERT

Product
Product

Component
Component

Reporter
Reporter

Severity
Severity

Intention
Intention

Generate
Generate
feature matrix
feature matrix
and
and
Normaliza(cid:2)on
Normaliza(cid:2)on

TF-IDF
TF-IDF

 Classification  stage
 Classification  stage

New bug report
New bug report

Classification 
Classification
model
model

Fig. 2. Framework of our approach

Bugzilla repository
Bugzilla repository

3.1 Preprocessing

Classifier
Classifier

BugBug

Non-Bug
Non-Bug

This experiment uses manually marked bug reports as the experimental data set. The data
is input in CSV ﬁle format, and text preprocessing steps are performed on the summary
ﬁeld, including normalization, tokenization, stop word removal, and stemming.

(1) Normalization: Its task is to unify all words and letters in the data into lowercase.
(2) Tokenization: Its task is to delete numbers, symbols, and punctuation. In this
experiment, spaces are used to replace punctuation and numbers are deleted.
(3) Stop word removal: Its task is to delete common words that do not carry speciﬁc
context-related information, thereby improving the classiﬁcation performance of
the model.

(4) Stemming: Its task is to remove the afﬁxes of words and extract the main part to

reduce the redundancy of text data.

136

F. Meng et al.

3.2 Feature Extraction

After the preprocessing step, we use BERT to extract the text features of the summary
ﬁeld. The BERT model is a pre-training model proposed by Google [28], which can
learn dynamic context word vectors and more comprehensively capture the features of
word meaning, word position and sentence meaning. In this experiment, the output of the
penultimate layer of the BERT model is used as the feature score. For ﬁelds other than the
summary (i.e. product, component, reporter, severity, intention), the score is calculated
using the TF-IDF algorithm. The TF-IDF algorithm can indicate the importance of the
ﬁeld in the document, which helps to increase the classiﬁcation ability of the model.
Finally, the text feature scores and frequency feature scores are spliced and fused to
generate a feature matrix, and normalized. The steps are shown in Table 2.

Table 2. The steps of feature extraction

3.3 Classiﬁer

In order to ﬁnd the most suitable classiﬁer for the proposed method, we input the extracted
features into ﬁve classiﬁers for training respectively, and observe the performance of each
classiﬁer. These classiﬁers include K-NN, NB, LR, SVM and RF.

Automatic Classiﬁcation of Bug Reports

137

3.3.1 K-Nearest Neighbor

K-Nearest Neighbor is a supervised classiﬁcation algorithm based on distance, which is
often used in the ﬁeld of data mining. The core idea is: if most of the k nearest neighbors
of a sample in the feature space belongs to a certain category, the sample also belongs to
this category and has the characteristics of the samples in this category. That is to say, for
a given test sample and a way based on a certain distance measurement, the classiﬁcation
result of the current sample is predicted through the closest K training samples.

Suppose there is a training data set T = {(x1, y1), (x2, y2), …, (xN, yN)}, where xi
is the feature vector of the sample, and y = {C1, C2, …, Ck} is sample category, i = 1,
2, …, N. According to the selected distance metric, ﬁnd the K nearest neighbors to x in
the training set T, covering the x neighborhood Nk(x) of these K points. According to
the index that measures the similarity between samples, the nearest K known samples
of each unknown category sample are searched out to form a cluster. The voting method
is used in the neighborhood to vote on the searched known samples, that is, the label
category with the most occurrences among the K samples is selected to determine the
category y of x:

y = arg max

(cid:2)

(cid:3)
yi = cj

I

(cid:4)

xi∈Ni(x)

(1)

In Eq. (1), i = 1, 2, …, N, j = 1, 2, …, K. Where I is an indicator function, and when

yi = cj, I is 1, otherwise it is 0.

3.3.2 Naive Bayes

Naive Bayes classiﬁer is a classiﬁcation technique based on Bayes’ theorem. It requires
that each feature used for classiﬁcation is independent and does not affect each other.
The core idea is to calculate the category probability of each sample, and the category
with the largest probability value is used as the ﬁnal classiﬁcation of the sample. Suppose
there is a training data set, in order to calculate the probability that the sample y classiﬁed
as x. According to Bayes’ theorem:

p(xi|y1, y2, · · · , yn) =

p(xi)
p(y1, y2, · · · yn)

n(cid:5)

k=1

p(yk |xi)

(2)

where p(xi) and p(y) represent the a priori probabilities of category xi and sample y,
respectively. p(y| xi) represents the possibility that category xi is sample y, and p(xi|
y) represents the possibility that sample y is category xi. Usually, when we deal with
classiﬁcation problems, the sample contains multiple features, which can be expressed
y = (y1, y2, …, yn). When each feature is independent of each other, it can be known
from (2):

p(xi|y1, y2, · · · , yn) =

p(xi)
p(y1, y2, · · · yn)

n(cid:5)

k=1

p(yk |xi)

(3)

138

F. Meng et al.

Regarding p(xi) and p(y1, y2, …, yn) as constants, after simplifying (3), we can get:

xc = arg max

n(cid:5)

k=1

p(yk |xi)

(4)

where yi is the feature of the data, and xc is the classiﬁcation result of the sample. In our
experiment, yi is the feature of the bug report represented by the vector, and the result
of xc has two types, including bug and non-bug.

3.3.3 Support Vector Machine

Support Vector Machine is a classiﬁer based on statistical learning VC dimension and
structural risk minimum theory. It ﬁnds a balance between classiﬁcation ability (no error
classiﬁcation for any sample) and model complexity (classiﬁcation accuracy of a speciﬁc
sample) based on limited information, with the purpose of making the classiﬁer get the
best generalization ability. Suppose there is a linear sample set (xi, yi), i = 1, 2, …, n,
x ∈ R2, y is the category label and y ∈ {−1, 1}. The linear discriminant function in
d-dimensional space is:

g(x) = ω · x + b

(5)

If the linear classiﬁcation line can accurately separate the two types of samples, the

following conditions should be met:

yi = 1 ⇔ g(xi) = ω · xi + b ≥ 1

yi = −1 ⇔ g(xi) = ω · xi + b ≤ −1

Simplify (6) and (7) to get:

yi(ω · xi + b) ≥ 1

(6)

(7)

(8)

At this time, the classiﬁcation interval is equal to 2/(cid:6)ω(cid:6). When the condition yi(ω·xi
+ b) ≥ 1 is satisﬁed, the minimum value of φ(ω) = (ω·ω)/2 needs to be found. Apply
Lagrange multiplier and satisfy Kuhn-Tucker conditions:

(cid:6)
(cid:7)
yi(ω · xi + b) − 1

αi

= 0

Finally, the optimal classiﬁcation function is obtained:

f (x) = sgn

(cid:3)(cid:6)

ω∗ · x

(cid:4)

(cid:7)

+ b∗

(cid:8)

= sgn

k(cid:2)

i=1

(cid:9)

i yi(xi · x) + b∗
a∗

(9)

(10)

where αi*, b* is the parameter to determine the optimal hyperplane, and (xi·x) is the
inner product of the two vectors.

Automatic Classiﬁcation of Bug Reports

139

3.3.4 Logistic Regression

Logistic Regression, also known as logistic regression analysis, is a generalized linear
regression classiﬁcation model, which is often used in the ﬁeld of data mining. Logistic
regression is essentially a binary classiﬁcation problem, and its dependent variable Y
has two values {0, 1}. The formula of the multiple logistic regression classiﬁer is as
follows:

π (X1, X2, . . . , Xn) = eY0+Y1·X1+···+Yn·Xn
1 + eY0+Y1·X1+···+Yn·Xn

(11)

where Xi is a vector describing the features of the data, and 1 ≥ π ≥ 0 is the value on the
logistic regression curve. In order to achieve classiﬁcation, it is also necessary to set a
threshold. For example, the threshold value in the model is 0.5, and x represents the text
feature and frequency feature extracted from the bug report. When π > 0.5, the report
is classiﬁed as bug; when π < 0.5„ the report is classiﬁed as non-bug.

3.3.5 Random Forest

Random Forest is a classiﬁcation algorithm based on ensemble learning method, and its
basic unit is decision tree. It includes “random” and “forest” parts. “Forest” means that
the classiﬁer consists of many trees, and it is based on ensemble learning theory. Random
includes two aspects: one is for the training process. In order to ensure that all samples
have a chance to be drawn once, the classiﬁer randomly selects a training sample set, and
the data used in each round of training is randomly selected from the original sample set
with replacement. The other is for feature selection. Assuming that the original data has
M features, S number of features are randomly selected from M features as candidate
features of the training tree. After the training samples and features are determined,
a decision tree is constructed on each training sample to get the prediction result. N
samples can get N prediction models, and then use the model to predict the test samples,
so that each sample can get N prediction results, and ﬁnally determine the ﬁnal result
through a simple majority voting principle. The formula of the model is as follows:

H (S) = argMax

n(cid:2)

i

I (hi(Si) = Y )

(12)

where hi(Si) is a single decision tree, Y is the prediction result, and I is an indicator
function.

4 Experiment

This experiment divides the training data and test data by 8:2, and extracts the features
of the report summary ﬁeld, other ﬁelds (product, component, reporter, severity), and
intention, respectively. In order to ﬁnd the most suitable classiﬁer for the proposed
method, we superimpose and fuse these three features in turn, and input them into ﬁve
different machine learning classiﬁers (K-NN, NB, SVM, LR, RF) for experiments.

140

F. Meng et al.

The experiments solved the following research questions:
RQ.1 Does adding the intention of the report improve the accuracy of the automatic

classiﬁcation for bug reports?

RQ.2 How about the performance of our proposed method on ﬁve different

classiﬁers?

4.1 Dataset

In this study, we collected 2,230 bug reports from four ecosystems in the Bugzilla
repository, respectively from Apache [14], Eclipse [15], Gentoo [16] and Mozilla [17].
Speciﬁcally, we select the reports whose status is “RESOLVED” or “VERIFIED” and
the resolution is “FIXED”. And extract their product, component, reporter, severity,
and summary tags. On this basis, we manually marked the types and intention of these
reports, and their type information statistics are shown in Table 3.

Ecosystem

Apache

Eclipse

Gentoo

Mozilla

Table 3. Type statistics of our dataset

Total

446

658

511

615

Bug

296

419

294

425

Non-Bug

150

239

217

190

4.2 Evaluation Metrics

To measure the performance of classiﬁcation, we use Accuracy, Precision, Recall and
F-Measure. Their deﬁnitions are as follows:

Accuracy =

TP + TN
TP + TN + FP + FN

Precision =

TP
TP + FP

Recall =

TP
TP + TN

F − measure = 2 × Precision × Recall
Precision + Recall

(13)

(14)

(15)

(16)

where TP is the number of true positives, TN is the number of true negatives, FP is the
number of false positives, and FN is the number of false negatives. In order to deal with
the randomness caused by different data splits, ten-fold cross-validation is used to obtain
the average value of evaluation metrics to measure the performance of classiﬁcation.

4.3 Results

Automatic Classiﬁcation of Bug Reports

141

RQ1. Does Adding the Intention of the Report Improve the Accuracy of the Auto-
matic Classiﬁcation for Bug Reports?
We use three types of features that are sequentially fused and superimposed to train the
classiﬁer, and the average accuracy of the ten-fold cross-validation is shown in Table 4.
Text represents the textual feature of the summary, Freq represents the word frequency
feature of other ﬁelds (product, component, reporter, severity), and Intention represents
the feature of the intention of the bug report. The unit of the values in the table is the
percentage system. Text+Freq+Intention is the method we put forward.

Table 4. Average accuracy of all datasets

Ecosystem

Features

Apache

Text

Text+Freq

Text+Freq+Intention

Eclipse

Text

Text+Freq

Text+Freq+Intention

Gentoo

Text

Text+Freq

Text+Freq+Intention

Mozilla

Text

Text+Freq

Text+Freq+Intention

Classiﬁer

K-NN

60.5

70.6

90.4

61.5

66.4

83.9

67.7

83.2

91.8

65.2

75.3

89.9

NB

65.5

80.0

89.2

63.7

66.1

84.0

61.8

73.6

85.1

66.8

70.4

87.5

LR

65.6

70.9

90.8

65.0

65.2

84.8

57.3

71.2

86.1

65.0

72.0

87.8

SVM

66.4

70.9

91.0

64.6

64.4

84.8

62.8

72.8

87.7

69.4

72.3

88.0

RF

63.0

85.7

91.7

61.0

73.1

84.8

67.3

87.3

94.5

67.5

78.2

87.8

RQ2. How About the Performance of Our Proposed Method on Five Different
Classiﬁers?
The performance of our proposed method, which combines text, frequency and intention
features (Text+Freq+Intention), on the ﬁve classiﬁers is shown in Figs. 3, 4, 5, 6 and 7.
The x-axis represents the source of the data, and the y-axis represents the average value
of the ten-fold cross-validation.

142

F. Meng et al.

Precision(%)

Recall(%)

F-Measure(%)

95.2

92.5

90.2

88.8

87.5

86.4

94.9

93

91.3

93.2

92.3

92.6

Apache

Eclipse

Gentoo

Mozilla

Fig. 3. The performance of all data on the K-NN classiﬁer

Precision(%)

Recall(%)

F-Measure(%)

95 

91.7

88.9

88.5

86.4

87.3

83.2

93.2

87.8

93.3

90.6

88.3

Apache

Eclipse

Gentoo

Mozilla

Fig. 4. The performance of all data on the NB classiﬁer

Precision(%)

Recall(%)

F-Measure(%)

96.9

93

89.5

88.4

87.8

88

85.2

95.2

89.9

93.6

91

88.7

Apache

Eclipse

Gentoo

Mozilla

Fig. 5. The performance of all data on the SVM classiﬁer

Precision(%)

Recall(%)

F-Measure(%)

96.5

92.8

89.5

88.4

87.8

88

82.8

95.9

88.8

92.7

89.4

91

Apache

Eclipse

Gentoo

Mozilla

Fig. 6. The performance of all data on the LR classiﬁer

Automatic Classiﬁcation of Bug Reports

143

Precision(%)

Recall(%)

F-Measure(%)

94.4

93.2

93.8

89.7

88.2

86.8

96.2

95.5

94.8

92.8

90.1

91

Apache

Eclipse

Gentoo

Mozilla

Fig. 7. The performance of all data on the RF classiﬁer

5 Discussion

5.1 Experiment Analysis

Performance of our method: we combine the proposed approach with ﬁve different
machine learning classiﬁers, and conduct experiments on the datasets of Apache, Eclipse,
Gentoo, and Mozilla. Table 4 shows the average Accuracy of all data sets on different
classiﬁers. Among them, the Apache data set has a maximum of 91.7%, the Eclipse
data set has a maximum of 84.8%, the Gentoo data set has a maximum of 94.5%, and
the Mozilla data set has a maximum of 89.9%. As can be seen from Table 4, compared
with considering the text ﬁeld of the report alone, after adding the intention factor of
the report we proposed, the accuracy of the data sets of the four ecosystems on the ﬁve
classiﬁers has been signiﬁcantly improved. To explore why adding the binary feature
of reporting intention can improve classiﬁcation performance, we made statistics on the
distribution of intention features and their correlation with labels (i.e. bug or non-bug)
in the experimental dataset. The results are shown in Table 5.

Table 5. Distribution statistics of intention features for all data

Ecosystem

Type

Intention distribution

Explanation

Suggestion

Apache

Eclipse

Gentoo

Mozilla

Bug

Non-bug

Bug

Non-bug

Bug

Non-bug

Bug

Non-bug

265

9

368

49

284

66

373

23

31

141

51

190

10

151

52

167

From the above table, it can be concluded that there is an imbalance in the distribution
of intention features in the dataset. Across the four ecosystems, bug reports included

144

F. Meng et al.

more explanations than non-bug reports, and non-bug reports included more suggestions.
The imbalanced distribution of binary features can make the classiﬁer more sensitive to
different labels during training. Therefore, adding a binary feature of reported intention
can improve classiﬁcation performance.

In addition, in order to test the scalability of our method and select a classiﬁer suitable
for it, we also tested the performance of our approach on K-NN, NB, LF, SVM and RF
classiﬁers. Figures 3, 4, 5, 6 and 7 shows the evaluation index value of each classiﬁer. The
results show that our proposed method has achieved good results in Precision, Recall and
F-measure on ﬁve classiﬁers. In all data sets, the Precision reached 82.8% to 96.9%, the
Recall reached 86.4% to 96.2%, and the F-measure reached 87.3% to 95.5%. Among the
ﬁve classiﬁers, the comprehensive performance of Random Forest is better than other
classiﬁers, and the performance of each classiﬁer will change with the different data
sets. Although the performance of our method changes with the classiﬁer, the overall
result is still a good level. This also means that when using our method to conduct a bug
report classiﬁcation, you can adjust the classiﬁer according to different ecosystems and
task requirements to achieve the best results. We believe our method is scalable.

Some of our thoughts:

(2)

(1) Bugs are harder to understand than non-bugs. Therefore, when faced with bug-
type defects, only a few reporters can provide solutions, and most reporters
can only describe the problem. This results in most reports where the intention
is “explanations” are bugs, and those where the intention is “suggestions” are
non-bugs.
In the Bug Tracking System of open source software, the reporters are not only
software developers and testers, but also a large number of users. The reporters who
explain the defects are mostly users, and the reporters who can make suggestions
for the defects are mostly software developers and testers. Because developers
and testers have richer experience and knowledge than users, they have a better
understanding of the code and architecture of the program, and can give advice on
complex defects.

(3) The intention labels in this study are manually labeled, and this work seems to
increase the training time of the automatic classiﬁcation model, but we are to verify
that the proposed method can improve the performance of automatic classiﬁcation
of bug reports. We think it is possible to add the label of reporting intention to the
Bug Tracking System, so that reporters can explain their intentions when submit-
ting reports, which can greatly reduce the time for manual labeling during model
training. It is much easier for reporters to state their intentions than to judge whether
a report is a bug.

5.2 Threats to Validity

In this part, we identiﬁed the following threats that may exist in this study.

Internal threat: The bug report tags of most open repositories contain errors. In order
to avoid incorrect labeling to affect the performance of the model, the data set of this
experiment is constructed by our manual labeling based on the data in the Bugzilla
repository. Although we have ﬂagged bugs or non-bugs in accordance with the rules

Automatic Classiﬁcation of Bug Reports

145

proposed in the existing literature [29], due to differences in experience and knowledge
background, there may be ﬂagging errors that affect the performance of the model.

External threat: This research focuses on the 2,230 bug reports of the four ecosys-
tems (Apache, Eclipse, Gentoo, Mozilla) to classify bugs or non-bugs. However, the
performance of our method on other ecosystems is unknown, that is, the performance of
our bug report classiﬁcation model on data from other software systems may be higher
or lower than the results of our experiments.

6 Conclusion and Future Work

In this study, we propose a new automatic classiﬁcation approach for bug reports, that
is, to increase the intention of the report based on the text information of the report. Our
approach combines Text Mining, Natural Language Processing and Machine Learning
technologies. We ﬁrst collected 2,230 reports from the four ecosystems (Apache, Eclipse,
Gentoo, Mozilla) in the bug repository, and manually marked their types and intention,
with the goal of constructing the data set required for the research. Then, we perform
preprocessing steps on the data, extract the text features of the report summary ﬁeld
and the word frequency features of other ﬁelds, and add the intention features of the
report. Next, we superimpose these features and input them into ﬁve classiﬁers (K-NN,
NB, SVM, LF, RF). Finally, we classify bug reports into bugs and non-bugs. The results
show that, compared with simply extracting text information features for classiﬁcation,
adding the intention features of the report we proposed can signiﬁcantly improve the
performance of bug report classiﬁcation. In the future, we will verify the proposed
approach on more open source ecosystems, and combine Deep Learning technology to
improve the performance of automatic classiﬁcation of bug reports.

Acknowledgment. This work is supported by the Science and Technology Research Project of
the Jilin Provincial Department of Education, “Research on Overtime Risk Assessment and Early
Warning Technology of Industrial Control Code” (No. JJKH20210097KJ).

References

1. Meng, F., Cheng, W., Wang, J.: Semi-supervised software defect prediction model based on

tri-training. KSII Trans. Internet Inf. Syst. 15(11), 4028–4042 (2021)

2. Guo, S., Chen, R., Li, H.: Using knowledge transfer and rough set to predict the severity of

android test reports via text mining. Symmetry 9(8), 144–161 (2017)

3. Yang, G., Min, K., Lee, J.W.: Applying topic modeling and similarity for predicting bug

severity in cross projects. KSII Trans. Internet Inf. Syst. 13(3), 1583–1589 (2019)

4. Kim, S., Zhang, H., Wu, R., Gong, L.: Dealing with noise in defect prediction. In: 2011 33rd

International Conference on Software Engineering (ICSE), pp. 481–490. ACM (2011)

5. Kochhar, P.S., Le, T.D.B., Lo, D.: Dealing with noise in defect prediction. In: 2014 11th
Working Conference on Mining Software Repositories (MSR), pp. 296–299. IEEE (2014)
6. Antoniol, G., Ayari, K., Di, P.M., Khomh, F., Guéhéneuc, Y.G.: Is it a bug or an enhancement?
A text-based approach to classify change requests. In: 2008 Conference of the Centre for
Advanced Studies on Collaborative Research: Meeting of Minds, pp. 304–318 (2008)

146

F. Meng et al.

7. Zhou, Y., Tong, Y., Gu, R., Gall, H.: Combining text mining and data mining for bug report

classiﬁcation. J. Softw.: Evol. Process 28(3), 150–176 (2016)

8. Lamkanﬁ, A., Demeyer, S., Giger, E., Goethals, B.: Predicting the severity of a reported
bug. In: 2010 7th IEEE/ACM Working Conference on Mining Software Repositories (MSR),
pp. 1–10. IEEE (2010)

9. Tian, Y., Lo, D., Sun, C.: Information retrieval based nearest neighbor classiﬁcation for ﬁne-
grained bug severity prediction. In: 2012 19th Working Conference on Reverse Engineering,
pp. 215–224 (2012)

10. Feng, Y., Chen, Z., Jones, J., Fang, C., Xu, B.: Test report prioritization to assist crowdsourced
testing. In: 2015 10th Joint Meeting on Foundations of Software Engineering, pp. 225–236
(2015)

11. Zhang, T., Chen, Y., Yang, X., Zhu, H.: Approach of bug reports classiﬁcation based on cost

extreme learning machine. J. Softw. 30(5), 1386–1406 (2019)

12. Yang, X.L., Lo, D., Xia, X., Huang, Q., Sun, J.L.: High-impact bug report identiﬁcation with

imbalanced learning strategies. J. Comput. Sci. Technol. 32(1), 181–198 (2017)

13. Kukkar, A., Mohana, R.: A supervised bug report classiﬁcation with incorporate and textual

ﬁeld knowledge. Proc. Comput. Sci. 132, 352–361 (2018)

14. http://bz.apache.org
15. http://bugs.eclipse.org
16. http://bugs.gentoo.org
17. http://bugzilla.mozilla.org
18. Zhang, T., Jiang, H., Luo, X., Chen, A.T.: A literature review of research in bug resolution:

tasks, challenges and future directions. Comput. J. 59(5), 741–773 (2016)

19. Chillarege, R., et al.: Orthogonal defect classiﬁcation-a concept for in-process measurements.

IEEE Trans. Softw. Eng. 18(11), 943–956 (1992)

20. Pingclasai, N., Hata, H., Matsumoto, K.I.: Classifying bug reports to bugs and other requests
using topic modelling. In: 2013 20th Asia-Paciﬁc Software Engineering Conference (APSEC),
vol. 2, pp. 13–18 (2011)

21. Menzies, T., Marcus, A.: Automated severity assessment of software defect reports. In: 2008
IEEE International Conference on Software Maintenance (ICSM), pp. 346–355. IEEE (2008)
22. Sari, G.I.P., Siahaan, D.O.: An attribute selection for severity level determination accord-
ing to the support vector machine classiﬁcation result. In: 1st International Conference on
Information Systems for Business Competitiveness (ICISBC) (2012)

23. Zhang, T., Chen, J., Yang, G., Lee, B., Luo, X.: Towards more accurate severity prediction

and ﬁxer recommendation of software bugs. J. Syst. Softw. 177(10), 166–184 (2016)

24. Kukkar, A., Mohana, R., Nayyar, A., Kim, J., Kang, B.G., Chilamkurti, N.: A novel deep-
learning-based bug severity classiﬁcation technique using convolutional neural networks and
random forest with boosting. Sensors 19(13), 2943–2964 (2019)

25. Du, X., Zheng, Z., Xiao, G., Yin, B.: The automatic classiﬁcation of fault trigger based
bug report. In: 2017 IEEE International Symposium on Software Reliability Engineering
Workshops (ISSREW), pp. 259–265. IEEE (2017)

26. Tan, L., Liu, C., Li, Z., Wang, X., Zhou, Y., Zhai, C.: Bug characteristics in open source
software. Empir. Softw. Eng. 19(6), 1665–1705 (2013). https://doi.org/10.1007/s10664-013-
9258-8

27. Catolino, G., Palomba, F., Zaidman, A., Ferrucci, F.: Not all bugs are the same: understanding,

characterizing, and classifying bug types. J. Syst. Softw. 152(10), 165–181 (2019)

Automatic Classiﬁcation of Bug Reports

147

28. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional

transformers for language understanding. arXiv:1810.04805 (2018)

29. Herzig, K., Just, S., Zeller, A.: It’s not a bug, it’s a feature: how misclassiﬁcation impacts
bug prediction. In: 2013 35th International Conference on Software Engineering (ICSE),
pp. 392–401. ACM (2013)

