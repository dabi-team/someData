2
2
0
2

l
u
J

8
1

]
E
S
.
s
c
[

1
v
6
6
4
8
0
.
7
0
2
2
:
v
i
X
r
a

What does Transformer learn about source code?

Kechi Zhang
Peking University∗
zhangkechi@pku.edu.cn

Ge Li
Peking University∗
lige@pku.edu.cn

Zhi Jin
Peking University∗
zhijin@pku.edu.cn

Abstract

In the ﬁeld of source code processing, the transformer-based representation models
have shown great powerfulness and have achieved state-of-the-art (SOTA) perfor-
mance in many tasks. Although the transformer models process the sequential
source code, pieces of evidence show that they may capture the structural infor-
mation (e.g., in the syntax tree, data ﬂow, control ﬂow, etc.) as well. We propose
the aggregated attention score, a method to investigate the structural information
learned by the transformer. We also put forward the aggregated attention graph,
a new way to extract program graphs from the pre-trained models automatically.
We measure our methods from multiple perspectives. Furthermore, based on our
empirical ﬁndings, we use the automatically extracted graphs to replace those inge-
nious manual designed graphs in the Variable Misuse task. Experimental results
show that the semantic graphs we extracted automatically are greatly meaningful
and effective, which provide a new perspective for us to understand and use the
information contained in the model.

1

Introduction

Since it was proposed, the Transformer profoundly impacts on nature language processing due to
its ability of modeling long-range dependencies and its self-attention mechanism. The last several
years have witnessed that transformer-based models, such as BERT, can handle the downstream
tasks with sequences. Moreover, there are also many studies on why transformer-based models can
represent sequences well. [Lin et al., 2019] shows that BERT representations are hierarchical rather
than linear. [Wu et al., 2020] also shows that we can extract the global syntactic information from
the transformer-based encoder. It indicates that the transformer-based models can learn about the
structural information about the natural language while training on the downstream tasks.

Transformer-based models are also wildly applied in the code representation ﬁeld, such as some
pre-trained models, including CodeBert ([Feng et al., 2020]) and GraphCodeBert ([Guo et al., 2020]).
Because of its complex logical structure (such as the syntax structure and control or data ﬂow) and
unique keyword distribution (including some programming idioms), the programming language is
quite different from the nature language. There are more stringent rules to constrain the programming
languages, while the natural language is ﬂexible. Because of this strong structural information, in most
cases, the additional structural information can effectively enhance the performance of the models
in downstream code intelligence tasks. It raises an emerging question for us that what Transformer
learns about source code. However, most previous model explanation methods in natural language
fail on revealing the special structural information of the source code learned by Transformer. It is
important to propose an interpretation method suitable for program language. Solving this question
can guide us to use the transformer-based models to represent the code effectively and help us explore
the ability of models to learn structural information.

∗Also with Key Laboratory of High Conﬁdence Software Technologies (Peking University), Ministry of

Education, China

Preprint. Under review.

 
 
 
 
 
 
Our experiments are based on these thinkings. We propose the aggregated attention score, a new
method to capture the structural information learned by the transformer-based model. We analyze
the aggregated attention score of each layer and summarize an effective way to extract structural
information. Considering that the process of the existing manual construction of program repre-
sentation graph is very complex, sometimes it needs to deﬁne many rules and use speciﬁc tools,
we also propose the aggregated attention graph, a method to automatically extract the program
representation graph from pre-trained transformer-based models like CodeBert and GraphCodeBert
with the Chu-Liu/Edmonds (CLE) algorithm ([Chu and Liu, 1965]) and attention head information.
We evaluate the obtained graph quantitatively and apply it to a speciﬁc downstream task. The results
not only show that automatically extracted graphs can maintain a comparable performance compared
with the ingenious manual designed graph, but also proves the effectiveness of our analysis method.
Through these experiments, We provide a new perspective to understand and utilize the structural
information Transformer learns about source code.

The contributions of our work are as follows:

• We introduce a new method, the aggregated attention score, to capture the structural infor-
mation learned by the transformer-based model. We take CodeBERT and GraphCodeBert
as examples.

• We also propose the aggregated attention graph, an effective way to automatically extract the
program representation graph from pre-trained transformer-based models. We put forward
some methods to measure the obtained graphs in terms of quantity and show the rich
semantic information in the graphs.

• We apply the aggregated attention graph to the Variable Misuse task with the state-of-
the-art model, Graph Relational Embedding Attention Transformers (GREAT for short)
([Hellendoorn et al., 2020]). The result shows that our work can maintain a comparable
performance with smaller graphs compared with the manual designed graphs, proving the
correctness of our analysis method. It offers an insight to understand and utilize the structual
information learned by Transformer about source code.

2 Background

2.1 Transformer

Transformer ([Vaswani et al., 2017]) is a model architecture relying on the attention mechanism.
(cid:3). The
Given input tokens {xi}|x|
stacked L-layer Transformer computes the ﬁnal output via X l = T ransf ormerl(X l−1), l ∈ [1, L].
The core component of a Transformer block is multi-head self-attention. The h-th self-attention head
is described as:

i=1, we pack their word embeddings to a matrix X 0 = (cid:2)x1, . . . , x|x|

Qh = XW Q

h , K = XW K

Ah = sof tmax

h , V = XW V
h
(cid:19)

(cid:18) QhK (cid:62)
h√
dk

Hh = AttentionHead (X) = AhVh

(1)

(2)

(3)

where Q, K ∈ Rn×dk , V ∈ Rn×dv , and the score Ai,j indicates how much attention token xi puts on
xj. There are usually multiple attention heads in a Transformer block. The attention heads follow the
same computation despite using different parameters. Let |h| denote the number of attention heads
in each layer, the output of multi-head attention is given by M ultiHead(X) = (cid:2)H1, . . . , H|h|
(cid:3) W o,
where W o ∈ R|h|dv×dx , [. . .] means concatenation, and Hi is computed as in Equation (3).

2.2 CodeBERT

In our experiments, we employ the CodeBERT ([Feng et al., 2020]). CodeBERT is the ﬁrst large
NL-PL pre-trained model. It is a bimodal pre-trained model based on Transformer with 12 layers,
768 dimensional hidden states, and 12 attention heads for programming language (PL) and natural
language (NL) by masked language modeling and replaced token detection to support text-code tasks
such as code search.

2

2.3 GraphCodeBERT

In our experiments, we also employ the GraphCodeBert model ([Guo et al., 2020]). GraphCodeBERT
is a Transformer-based pre-trained model for programming language that incorporates data ﬂow
information in the graph representation of variables in the code. The data ﬂow graph encodes the
structure of variables based on “where-the-value-comes-from” from the AST parse. The pre-trained
model is jointly trained on the code, the natural language comment of the code, and the data ﬂow
graph of the code. GraphCodeBERT includes 12 layers of Transformer with 768-dimensional hidden
states and 12 attention heads. With additional structure information added to the attention score,
GraphCodeBert can deliver improvements on Natural Language Code Search, Code Clone Detection,
Code Translation, and Code Reﬁnement.
Both the CodeBert model and the GraphCodeBert we use are from the ofﬁcial repository2, which use
the huggingface/transformers framework.

3 Methods: Aggregated Attention Score and Graph

Aggregated Attention Score
[Lin et al., 2019] shows that BERT representations are hierarchical
rather than linear. There is something akin to the syntactic tree structure in addition to the word order
information. It suggests that we can extract syntactic information of the input sequence from the
transformer-based model. Due to the powerful multi-head self-attention mechanism, we have designed
a method to extract information from the attention weights generated by Transformer. CodeBERT
consist of 12 transformer layers, and the number of attention heads in each layer |H| = 12. In order
to aggregate attention scores among all attention heads, we deﬁne the aggregated attention score,
which takes the maximum over different heads for each layer:

AggregatedAttnl[i, j] =

H
max
h=1

hh
l [i, j]

(4)

where the score hh
head in the l − th layer.

l [i, j] represents how much attention token xi puts on xj in the h-th self-attention

Figure 1: Illustration of the Aggregated Attention Graph. The maximum spanning graph is extracted
from the aggregated attention score, with edge types from attention head information. All reﬂexive
edges are removed.

Aggregated Attention Graph In order to make the matrix sparse and extract hierarchical structure
information, we regard the aggregated attention score as the edge weight between tokens and apply the
Chu-Liu-Edmonds algorithm to ﬁnd the maximum spanning tree. Then we can get the aggregated
attention graph, a syntactic graph learned by each layer in the transformer-based model.

Furthermore, through our analytical experiments in section 4.3, we ﬁnd that different attention heads
have a different semantic preference when extracting information. It provides natural edge type

2https://github.com/microsoft/codebert

3

(cid:28635)(cid:28635)(cid:28664)(cid:28660)(cid:28663)(cid:28595)(cid:28611)(cid:28635)(cid:28664)(cid:28660)(cid:28663)(cid:28595)(cid:28612)(cid:28635)(cid:28664)(cid:28660)(cid:28663)(cid:28595)(cid:28613)(cid:28580)(cid:28581)(cid:28582)(cid:28635)(cid:28664)(cid:28660)(cid:28663)(cid:28595)(cid:28612)(cid:28635)(cid:28664)(cid:28660)(cid:28663)(cid:28595)(cid:28613)information for us while constructing semantic graphs automatically without any manual deﬁnition
rules. Finally, we can extract a programming representation graph with abundant edge types which
contain rich semantic information. The whole process of the automatic program graph extraction is
shown in Figure 1.

4 Analytical Experiments

We employ the CodeBERT model and GraphCodeBERT model and conduct several analytical
experiments. In order to ﬁnd the most suitable model for program graph extraction, we also consider
the effect of ﬁne-tuning on the pre-trained transformer-based models. We ﬁne-tune models in Code
Summarization task ([Iyer et al., 2016]) using the Python dataset from CodeSearchNet dataset
([Husain et al., 2019, Lu et al., 2021]) guided by the ofﬁcial repository.3 The objective of the Code
Summarization task is to generate the natural language comment for a code, which requires reasoning
on the semantics of complete code snippets.

Based on these original or ﬁne-tuned pre-trained models, we analyze the aggregated attention score
from different layers and ﬁnd that the structural information can be extracted from the deepest layer.
We use the proposed graph extraction method to obtain the representation graph from attention heads
and try to evaluate the graph quantitatively. We also ﬁnd that different attention heads have different
semantic preferences, which provides nature edge types for our proposed extraction method.

All the input sample code comes from the CodeSearchNet dataset. For the convenience of experiments,
we choose the part of Python because it accounts for the largest proportion of the six languages, and
there are many parsing utilities for Python source codes.

4.1 Attention Patterns Analysis between layers

Using the Equation(4), we calculate the aggregated attention score for each layer given an input
sequence. In ﬁgure 2 we show the aggregated attention scores and their corresponding maximum
spanning tree between different layers given an example input in CodeBERT. We notice that similar
ﬁndings can be observed in GraphCodeBERT. As the layers deepen, the aggregated attention scores
change from diagonal patterns to heterogeneous patterns. The diagonal pattern means the attention
weights focus on neighbor words. In contrast, the heterogeneous pattern means the attention weights
become more focused on the semantically related words but far apart. It stands to reason that the
lower layers have the most information about linear word order, and the higher layers may have more
information about semantic knowledge and task-speciﬁc knowledge.

Figure 2: Illustration of the aggregated attention scores and the maximum spanning tree from different
layers in CodeBERT. As the layers deepen, the aggregated attention scores change from diagonal
pattern to heterogeneous pattern.

It hints that we can extract syntactic information from the attention heads in deep layers. Figure 2
also shows that the attention score matrix is too dense to analyze, especially in deep layers. Using
the maximum spanning tree algorithm is an effective method for extracting features from aggregated
attention scores more clearly.

3https://github.com/microsoft/CodeXGLUE

4

4.2 Effectiveness Analysis of Graphs

Based on our ﬁnding in section 4.1, we use the deepest aggregated attention scores to extract semantic
knowledge. Figure 3(a) displays a aggregated attention graph induced from the aggregated attention
score in the last layer in CodeBERT with the maximum spanning tree algorithm. We can ﬁnd that a
lot of important structural relations, including control ﬂows and data ﬂows, can be extracted from the
attention scores, including the relation between keywords def and return, the relation between the
two sides of an assignment statement, the relation between the deﬁnition of a variable and its usage,
etc. It suggests that transformer-based models can learn complex structural information from the
source code through the self-attention mechanism.

(a) An example of the aggregated attention graph

(b) Corresponding concrete syntax tree(CST)

Figure 3: Illustration of (a): an example of the aggregated attention graph. Each line represents
an edge between two tokens. We ﬁlter out some edges with obvious semantic features and mark
them with blue lines. (b):Corresponding concrete syntax tree(CST). Some nodes are simpliﬁed. The
intermediate nodes between def and return are displayed in blue dotted borders.

In order to provide a shred of subjective evidence for quantitative evaluation, we compare the
aggregated attention graphs with the concrete syntax tree. We deﬁne the distance between tokens in
the concrete syntax tree as follows: given two tokens, the tree distance is the number of intermediate
nodes on the shortest path between them. For example, the tree distance between def and return
in Figure 3(b) is 3 where all the intermediate nodes are displayed in blue dotted borders. A short
tree distance means that tokens are semantically closely related. Take the ﬁne-tuned CodeBERT as
an example; we show our statistical results of the tree distance in Figure 4. We conduct the same
statistical experiments on original CodeBERT and GraphCodeBERT models and have observed
similar ﬁndings.

(a) Distribution of the tree distance

(b) Relationship between tree distance and
sequence distance

Figure 4: Illustration of (a): the distribution of the distance in the concrete syntax tree for our
extracted tree in ﬁne-tuned CodeBERT, and (b): the relationship between the tree distance and the
sequence distance in ﬁne-tuned CodeBERT. The deep color indicates a high frequency corresponding
to the point.

Considering that neighbor words also lead to short tree distances, which are less meaningful than
those that are far apart but closely related semantically, we also count the distance on the sequence.

5

defcopy(self,x=None,y=None):returnself.deepcopy(x=x,y=y)modulefunction_definitiondefcopyparameters:blockreturn_statementreturn……05101520treedistance0.0000.0250.0500.0750.1000.1250.1500.175frequencyFigure 4(b) shows that no matter how far two tokens are in the sequence, we can still extract a relation
from the attention scores if they are very close in the concrete syntax tree. This situation often
happens in programming languages. For example, given a simple source code with keywords if and
else, these two keywords may be far apart in sequence but close in the concrete syntax tree. By the
way, this kind of situation appears less in natural language, especially in a simple sentence.

(a) Distribution in original CodeBERT

(b) Distribution in ﬁne-tuned CodeBERT

Figure 5: Illustration of (a)(b): last common parent node type distribution in original CodeBERT and
ﬁne-tuned CodeBERT. We show the 10 most frequent parent node types in the ﬁgure.

We also ﬁnd the differences between original models and ﬁne-tuned models. By getting the last
common parent node in concrete syntax trees, We explore the factual semantic information in the
edge of the aggregated attention graph. Statistical results are shown in Figure 5. We ﬁnd that the
original models prefer to capture common semantic relations across different programming languages,
including the relations between adjacent parameters or arguments. The ﬁne-tuned models prefer to
capture some language-speciﬁc structural information. For example, models ﬁne-tuned in Python
dataset are sensitive to the relation about the language-speciﬁc function deﬁnition forms, such as def
and return. We sample 1000 source code examples and ﬁnd 122 def-return edges in graphs extracted
from ﬁne-tuned CodeBERT. But for the original CodeBERT we only ﬁnd 9 def-return edges. This
ﬁnding proves that ﬁne-tuning on speciﬁc programming language can help pre-trained models learn
the language-speciﬁc structural information, which is instructive for extracting graphs automatically
from transformer-based models.

4.3 Head-speciﬁc Relation Analysis

Besides, we analyze the relationship extracted from different attention heads. We sample 1000 source
code examples and extract their syntactic graph from the aggregated attention scores in the last layer.
Specially we collect 1826 edges connecting word self in different positions and 122 edges connecting
word def and word return. We count the attention head to which each edge belongs, and statistical
results are shown in Figure 6.

(a) self-self edge

(b) def-return edge

Figure 6: The distribution of two different edges on attention heads in ﬁne-tuned CodeBERT

The self-self edge is closely related to data ﬂow and appears more in the 1st head, the 6th head,
and 10th head. Correspondingly, the def-return edge is closely related to the code structure and

6

0.000.020.040.060.080.100.120.14frequencydefault_parametercomparison_operatorsubscriptassignmentcallmoduleattributefunction_definitionparametersargument_listparentnodetype0.000.050.100.150.200.25frequencysubscriptdefault_parametercomparison_operatorargument_listassignmentcallattributeparametersfunction_definitionmodule123456789101112n-thhead0.000.050.100.150.200.250.300.350.40frequency123456789101112n-thhead0.000.050.100.150.200.250.300.35frequencyappears more in the 10th head and the 12th head. Similar ﬁndings have been found in other models,
conﬁrming that different attention heads have a different semantic preference when extracting
information. It also proves the effectiveness of the multi-head attention mechanism when extracting
complex structural information.

5 Applications: VarMisuse

Our analytical experiments ﬁnd that it is possible to automatically extract representation graphs
from pre-trained transformer-based models. We also ﬁnd that ﬁne-tuning can help the model learn
language-speciﬁc structural information better.

In order to further prove the effectiveness and correctness of our methods, we choose the Variable
Misuse task ([Allamanis et al., 2018, Vasic et al., 2019]) as the application experiment of our proposed
method. The state-of-the-art model is the Graph Relational Embedding Attention Transformer
(GREAT for short) ([Hellendoorn et al., 2020]), which uses the program representation graph as
inputs. The dataset for the Variable Misuse task used by the GREAT model does not release the
construction tools, making it hard to use on other datasets. In our application experiments, we will
replace the original input graphs with our aggregated attention graphs.

5.1 Experimental Setup

The VarMisuse Task Detecting variable misuses in code is a task that requires understanding and
reasoning about program semantics. We follow the previous work ([Allamanis et al., 2018, Vasic
et al., 2019]) and focus on the localization-and-repair task. The task is deﬁned as follows: Given a
function, predict two pointers into the function’s tokens, one pointer for the location of a variable use
containing the wrong variable (or a special no-bug location), and one pointer for any occurrence of
the correct variable that should be used at the wrong location instead.

Model
[Hellendoorn et al., 2020] has proposed the Graph Relational Embedding Attention Trans-
formers(GREAT for short), which bias traditional Transformers with relational information from
graph edge types, and have achieved good performance with their ingenious design of graph repre-
sentation of code. The graph includes control ﬂow edges, data ﬂow edges, semantic edges, syntactic
edges, which takes many human resources to deﬁne and implement these rules.
In the experiments, the GREAT model we use is from the ofﬁcial repository.4 The number of layers
is 6, and the attention dimension is set to 256. All the experiments are conducted on two Tesla V100
GPUs.

Metrics We measure four accuracy metrics in the experiments to fully analyze the performance
of different input graphs trained with the GREAT model with the same parameter settings. All
accuracies are reported as: joint localization & repair accuracy (the key indicator, for buggy samples,
how often the model correctly localizes and repairs the bug), bug-free classiﬁcation accuracy (whether
the model correctly identiﬁes the method as (non-)buggy), bug-localization accuracy (whether the
model correctly identiﬁes the bug’s location for buggy samples) and repair accuracy (whether the
model points to the correct variable to repair the bug).

Dataset This dataset provided by the ofﬁcial repository 5 is generated synthetically from the corpus
of Python code in the ETH Py150 Open dataset ([Raychev et al., 2016]). While the construction
tools are not provided, the dataset provides the source tokens, which can be utilized by our automatic
program graph extraction method.

5.2

Input Graph Analysis

Based on our ﬁnding that ﬁne-tuning on speciﬁc programming language can help pre-trained models
learn the language-speciﬁc structural information, we use the ﬁne-tuned CodeBERT and GraphCode-
BERT in Code Summarization task with Python dataset in our experiments. We use the aggregated

4https://github.com/VHellendoorn/ICLR20-Great
5https://github.com/google-research-datasets/great

7

attention graphs extracted from these models to replace the original rule-based graphs, respectively.
We also notice that there are some special symbols for representing formats such as #NEWLINE# that
are not in the vocabulary of CodeBERT. We replace them with <mask> while extracting semantic
graphs and do not record the edges on them, which makes our graph smaller and more reasonable.
Because the graph is too small and the aggregated attention scores in the deep layer may lose infor-
mation about linear word order as described in section 4.1, we add sequence edges as a new edge
type in our graph.

In addition, we also use a sequence structure graph, which only connects adjacent words, as our
baseline. Statistics of these graphs are listed in table 1, and the statistics of edge types include the
reverse edges.

Table 1: Statistics of our experiment
graphs

Figure 7: The proportion of edge types in original
graphs extracted from CodeBERT

Edge
types

22
2

Origin
Sequence
Ours
CodeBERT
26
GraphCodeBERT 26

Avg.
edges

193.3
114.9

185.4
183.2

We also measure the degree of coincidence between our constructed graphs and the original graphs.
There are 22 edge types in original graphs, including semantic edges, syntactic edges, control-ﬂow
edges, and data ﬂow edges. All these edges are generated by ingenious manual design. We analyze
the automatically extracted graph and explore the proportion of each edge type in the original graph
in Figure 7. We ﬁnd that except for NEXT_SYNTAX edge, there are many syntactic edges (FIELD,
LAST_LEXICAL_USE) and data ﬂow edges (LAST_READ, LAST_WRITE) that can be extracted from
the pre-trained models automatically.

The statistical results mean that without any artiﬁcial parsing rules when constructing graphs, our
graphs are still quite similar to the ingenious manual design. Moreover, we further explore the unique
edges in our graphs and ﬁnd some edges connected to brackets and other symbols. These edges can
not be well explained in current artiﬁcial rules, but their effectiveness is still worth exploring in future
work.

It also hints that the self-attention mechanism can well handle some implicit syntactic and data ﬂow
relationships. However, it is still limited to complex relationships such as control ﬂows. In the
Variable Misuse task, it is obvious that the data ﬂow relations play an important role in the model’s
performance, which is consistent with the feature of the graph we constructed.

5.3 Results

The experiment results are shown in table 2. We are surprised to ﬁnd that even though the CodeBERT
model and the GraphCodeBERT model we used to extract relations are only ﬁne-tuned on the Code
Summarization task, the graphs we extracted are still very effective and meaningful. Although
the ﬁnal result drops slightly compared with the original graphs, we can still conclude that the
aggregated attention graph we extracted contains a lot of real syntax information and can be utilized
in downstream tasks. The advantage of our graph is that it is smaller and needs no manual design.

We also notice that with the same training steps, the graphs from GraphCodeBERT slightly outperform
those from CodeBERT. We think that using the code structure information during pre-training is
essential, but the ﬁne-tuning process may weaken this advantage. However, unfortunately, because
the original pre-trained model is relatively insensitive to the language-speciﬁc structural information,
it is hard for us to explore it further.

The positive results on the VarMisuse task prove our point that transformer-based models can
extract rich syntactic information with the self-attention mechanism, and this syntactic information
is available in downstream tasks. Our method of automatically extracting program graphs from the

8

0.00.20.40.60.81.0proportionenum_RETURNS_TOenum_FORMAL_ARG_NAMEenum_CFG_NEXTenum_COMPUTED_FROMenum_CALLSenum_LAST_LEXICAL_USEenum_LAST_WRITEenum_LAST_READenum_FIELDenum_NEXT_SYNTAXedgetypesinoriginalgraphsTable 2: Test-set results of our experiment graphs with GREAT model on VarMisuse task

origin
sequence
ours
from CodeBERT
77.33%
from GraphCodeBERT 77.81%

Loc & Rep Acc Class. Acc Loc Acc Rep Acc
86.16% 85.62%
78.18%
79.09% 81.77%
72.12%

90.36%
91.99%

88.98%
90.38%

85.77% 84.38%
85.85% 85.37%

aggregated attention score is effective. It provides a new perspective for us to understand and use the
information contained in the model. Furthermore, we can continue to mine the useful information in
attention scores, especially those relationships that cannot be well explained in current artiﬁcial rules.

6 Related Work

There has been substantial research investigating what pre-trained transformer models have learned
about nature languages’ structures.

[Lin et al., 2019] shows that BERT representations are hierarchical rather than linear. [Kovaleva
et al., 2019] proposes ﬁve attention patterns and ﬁnds the "heterogeneous" attention pattern could
potentially be linguistically interpretable. [Clark et al., 2019] lists many examples to explore attention
scores in BERT ([Devlin et al., 2019]) and thinks that it is possible to extract parse trees from BERT
heads, and several studies focused on it. [Jawahar et al., 2019] includes a brief illustration of a
dependency tree extracted directly from self-attention weights. [Htut et al., 2019] uses two simple
dependency relation extraction methods and ﬁnds neither of their analysis methods supports the
existence of generalist heads that can perform holistic parsing. [Marecek and Rosa, 2019] proposes a
transparent deterministic method of quantifying the amount of syntactic information present in the
self-attentions. [Hao et al., 2020] proposes a self-attention attribution method (ATTATTR) based on
integrated gradient to interpret the information interactions inside Transformer.

Recently there are some work argues that self-attention distributions are not directly interpretable in
nature language ([Serrano and Smith, 2019, Jain and Wallace, 2019, Brunner et al., 2020]). [Rogers
et al., 2020] reviews the current state of knowledge about how BERT works and hold the opinion
that the syntactic structure of nature language is not directly encoded in self-attention weights. Many
recent approaches use the feature representations ([Hewitt and Manning, 2019, Reif et al., 2019,
Rosa and Marecek, 2019]) generated by a pre-trained model and train a small model to perform a
supervised task (e.g., dependency labeling). Most recently, [Wu et al., 2020] shows that we could use
encoder representations instead of attention scores to recover syntactic trees of the nature language
from BERT. We have tried this method and found that it does not improve extracting deep semantic
information of source code.

We hypothesize that there are more complex logical structures in the source code. For example, in
source code, there may be two words that are far apart but closely related semantically, such as if
and else. However, it is less likely to happen in natural language, especially in a simple sentence.
Because of these structural differences, many methods of interpreting natural language models are
not ideally suited to programming languages. The interaction between words may be more effective
for information extraction in source code, just like the key-value attention mechanism.

7 Conclusion

In this paper, we try to ﬁgure out what transformer-based models learn about source code by proposing
a new method, the aggregated attention score. Based on our method, we put forward the aggregated
attention graph, a new way to automatically extract representation graphs from pre-trained models.
We measure our methods from multiple perspectives and apply our methods on VarMisuse task. The
results also prove the correctness of our methods and analytical experiments.

In the future, we plan to further explore other semantic information extracted from transformer-based
models. We would also rethink the implicit information in pre-trained transformer models and
propose new models to fully utilize these task-speciﬁc features in different code intelligence tasks.

9

References

Yongjie Lin, Yi Chern Tan, and Robert Frank. Open sesame: Getting inside bert’s linguistic

knowledge. CoRR, abs/1906.01698, 2019. URL http://arxiv.org/abs/1906.01698.

Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing for
analyzing and interpreting BERT. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4166–4176. Association for Computational
Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.383. URL https://doi.org/10.18653/
v1/2020.acl-main.383.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, and Ming Zhou. Codebert: A pre-trained model for programming
and natural languages. In Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP
2020, Online Event, 16-20 November 2020, pages 1536–1547. Association for Computational
Linguistics, 2020. doi: 10.18653/v1/2020.ﬁndings-emnlp.139. URL https://doi.org/10.
18653/v1/2020.findings-emnlp.139.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn
Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. Graphcodebert: Pre-training code
representations with data ﬂow. CoRR, abs/2009.08366, 2020. URL https://arxiv.org/abs/
2009.08366.

Y. J. Chu and T. H. Liu. On the shortest arborescence of a directed graph. Science Sinica, 14, 1965.

Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global
relational models of source code. In 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:
//openreview.net/forum?id=B1lnbRNtwr.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.
In Isabelle Guyon, Ulrike
von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Ro-
man Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Confer-
ence on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, pages 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Summarizing source code
using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long
Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/p16-1195. URL
https://doi.org/10.18653/v1/p16-1195.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
arXiv preprint

CodeSearchNet challenge: Evaluating the state of semantic code search.
arXiv:1909.09436, 2019.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,
Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu
Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding
and generation. CoRR, abs/2102.04664, 2021.

Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs
with graphs. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL
https://openreview.net/forum?id=BJOFETxR-.

10

Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. Neural program
repair by jointly learning to localize and repair. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL
https://openreview.net/forum?id=ByloJ20qtm.

Veselin Raychev, Pavol Bielik, and Martin T. Vechev. Probabilistic model for code with decision
trees. In Eelco Visser and Yannis Smaragdakis, editors, Proceedings of the 2016 ACM SIGPLAN
International Conference on Object-Oriented Programming, Systems, Languages, and Applications,
OOPSLA 2016, part of SPLASH 2016, Amsterdam, The Netherlands, October 30 - November 4,
2016, pages 731–747. ACM, 2016. doi: 10.1145/2983990.2984041. URL https://doi.org/
10.1145/2983990.2984041.

Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets
of BERT. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China,
November 3-7, 2019, pages 4364–4373. Association for Computational Linguistics, 2019. doi:
10.18653/v1/D19-1445. URL https://doi.org/10.18653/v1/D19-1445.

Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look
at? an analysis of bert’s attention. CoRR, abs/1906.04341, 2019. URL http://arxiv.org/abs/
1906.04341.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of
deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and
Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–
4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL
https://doi.org/10.18653/v1/n19-1423.

Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. What does BERT learn about the structure of
language? In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the
57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers, pages 3651–3657. Association for Computational Lin-
guistics, 2019. doi: 10.18653/v1/p19-1356. URL https://doi.org/10.18653/v1/p19-1356.

Phu Mon Htut, Jason Phang, Shikha Bordia, and Samuel R. Bowman. Do attention heads in BERT
track syntactic dependencies? CoRR, abs/1911.12246, 2019. URL http://arxiv.org/abs/
1911.12246.

David Marecek and Rudolf Rosa. From balustrades to pierre vinken: Looking for syntax in trans-
former self-attentions. CoRR, abs/1906.01958, 2019. URL http://arxiv.org/abs/1906.
01958.

Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information
interactions inside transformer. CoRR, abs/2004.11207, 2020. URL https://arxiv.org/abs/
2004.11207.

Soﬁa Serrano and Noah A. Smith. Is attention interpretable? In Anna Korhonen, David R. Traum, and
Lluís Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational
Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages
2931–2951. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1282. URL
https://doi.org/10.18653/v1/p19-1282.

Sarthak Jain and Byron C. Wallace. Attention is not explanation. In Jill Burstein, Christy Doran,
and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 3543–
3556. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1357. URL
https://doi.org/10.18653/v1/n19-1357.

11

Gino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger
Wattenhofer. On identiﬁability in transformers. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
URL https://openreview.net/forum?id=BJg1f6EFDB.

Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know
about how BERT works. Trans. Assoc. Comput. Linguistics, 8:842–866, 2020. URL https:
//transacl.org/ojs/index.php/tacl/article/view/2257.

John Hewitt and Christopher D. Manning. A structural probe for ﬁnding syntax in word represen-
tations. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,
Volume 1 (Long and Short Papers), pages 4129–4138. Association for Computational Linguistics,
2019. doi: 10.18653/v1/n19-1419. URL https://doi.org/10.18653/v1/n19-1419.

Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B. Viégas, Andy Coenen, Adam Pearce,
and Been Kim. Visualizing and measuring the geometry of BERT.
In Hanna M. Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett,
editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, pages 8592–8600, 2019. URL https://proceedings.neurips.cc/paper/2019/
hash/159c1ffe5b61b41b3c4d8f4c2150f6c4-Abstract.html.

Rudolf Rosa and David Marecek. Inducing syntactic trees from BERT representations. CoRR,

abs/1906.11511, 2019. URL http://arxiv.org/abs/1906.11511.

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes]
(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-

mental results (either in the supplemental material or as a URL)? [Yes]

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes] See Section 5.1.

(c) Did you report error bars (e.g., with respect to the random seed after running experi-

ments multiple times)? [N/A]

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.1.

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [N/A]
(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]

(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [N/A]

12

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [N/A]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

13

