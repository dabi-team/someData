Neural Augmented Min-Sum Decoding of Short
Block Codes for Fading Channels

Sravan Kumar Ankireddy, Hyeji Kim
Department of Electrical and Computer Engineering
University of Texas at Austin

2
2
0
2

y
a
M
1
2

]
T
I
.
s
c
[

1
v
4
8
6
0
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—In the decoding of linear block codes, it was shown
that noticeable gains in terms of bit error rate can be achieved
by introducing learnable parameters to the Belief Propagation
(BP) decoder. Despite the success of these methods, there are
two key open problems. The ﬁrst is the lack of analysis for
channels other than AWGN. The second is the interpretation
of the weights learned and their effect on the reliability of the
BP decoder. In this work, we aim to bridge this gap by looking
at non-AWGN channels such as Extended Typical Urban (ETU)
channel. We study the effect of entangling the weights and how
the performance holds across different channel settings for the
min-sum version of BP decoder. We show that while entanglement
has little degradation in the AWGN channel, a signiﬁcant loss
is observed in more complex channels. We also provide insights
into the weights learned and their connection to the structure of
the underlying code. Finally, we evaluate our algorithm on the
over-the-air channels using Software Deﬁned Radios.

I. INTRODUCTION

Short length block codes are an attractive choice for use
cases with stringent latency requirements [1]. However, they
suffer from poor error correction performance. One key reason
for this is the presence of short cycles in the Tanner graph of
the code, making their decoding sub-optimal. Unfortunately at
such short lengths, codes designed without cycles have worse
error correction performance [2]. Additionally, when fading
channels are considered, the Log Likelihood Ratio (LLR)
computation suffers from errors because of imperfect Chan-
nel State Information (CSI) and equalization, which makes
the Belief Propagation (BP) decoder sub-optimal [3]. Hence
modiﬁcations to the traditional BP decoder are necessary to
improve the performance.

In [4],

the authors demonstrate the advantage of using
neural networks for improving the BP decoder by placing
multiplicative weights along the edges of the Tanner graph
structure and unrolling the iterations, resulting in a neural
BP decoder. Since then, there have been many works that
explored the possibility of reducing the complexity of the
neural BP decoders. In [5], the authors show that instead
of using multiplicative weights, comparable gains can be
achieved by using offset factors combined with min-sum
approximation, which reduces the implementation cost. Later
in [6], the authors explore the possibility of reducing the
number of weights needed by entangling them across iterations
in a recurrent fashion and show that the performance is still
comparable to the fully parameterized model. More recently,

This project was sponsored by the Ofﬁce of Naval Research

the authors in [7] propose entangling the weights not only
across iterations but also across the edges. To compensate for
the performance loss, a shallow neural network referred to as
Parameter Adapter Network (PAN), is used to select different
weights for different SNRs regions resulting in a performance
very close to [6].

Despite the success of these works,

there are two key
open problems that we answer in this work. The ﬁrst is the
trade-offs associated with entanglement, which is limited to
only AWGN channels in [4]–[7]. We consider non-AWGN
channels such as ETU and show that the trade-offs are quite
different from AWGN. The second is the interpretation of the
learned weights. While it has been conjectured that the use of
normalization or offsets to modify the beliefs helps mitigate
the detrimental effects of short cycles [4], [5], there is no
supporting evidence. We present empirical evidence that the
weights learned are directly related to the short cycles present
in the code and investigate how they help in improving the
reliability of the posterior LLRs.

Our main contributions can be summarized are as follows:
• Complexity Trade-off: We explore the complexity vs
performance trade-off for neural augmented min-sum de-
coders across various channels. We show that we beneﬁt
signiﬁcantly by having more number of weights for a
complex channel while fewer weights are sufﬁcient for
a simple channel such as AWGN and thus the choice of
parameters can be made based on the channel conditions.
• Robustness and adaptivity: We show that the learned
weights are fairly robust to channel variations i.e, the
decoder trained on one channel still outperforms the
classical decoders on other channels. We further show
that these neural decoders can be quickly adapted to
new channels with minimal additional training, to achieve
additional gains.

• Over-the-air channels: We evaluate neural min-sum de-
coders for over-the-air channels using USRPs. We show
that that the fully parameterized min-sum decoder out-
performs the entangled variant.

• Interpretation: We provide insights into the learned de-
coder by establishing a connection between the number
of cycles in the Tanner graph and the weights learned. We
also show that the learned weights reduce the correlation
between the beliefs propagated, which is a desirable
property since more independent beliefs result in more
reliable estimation of the posterior LLRs [8].

 
 
 
 
 
 
II. SYSTEM MODEL

In this work, we consider linear block codes. A (N, K)
block code maps a message of length K to a codeword of
length N and is uniquely described by its parity check matrix
H of dimensions (N − K) × N , where the rate of the code
is R = K/N. The linear block code can be also represented
using a bipartite graph, known as Tanner graph, which can be
constructed using its parity check matrix H. The Tanner graph
consists of two types of nodes. We refer to them as Check
Nodes (CN) that represent the parity check equations and
Variable Nodes (VN) that represent the symbols in codeword.
There is an edge present between a check node c and variable
node v if H(c, v) = 1.

We consider a system with Binary Phase Shift Keying
transmits a random vector X ∈
(BPSK) modulation that
{−1, 1}N , where N is the code length. The generated vector
is then passed through a channel to receive Y as follows

Y = Channel(X) + W,

where Channel(X) applies the effect of channel on the transmit
vector X and W is the noise at the receiver. As a special case,
when the channel is AWGN the LLR at the receiver for vth
symbol can be calculated as

lv =

p(Yv = yv|Xv = −1)
p(Yv = yv|Xv = 1)

=

2yv
σ2 ,

where Var(W ) = σ2 is the noise variance.

Apart from the AWGN channel, we also consider multi-
path fading propagation channels from the 3GPP speciﬁcations
[9], [10]. These channels result in interference and distor-
tion at the receiver because of the multi-path components
and also the mobility of the users. Speciﬁcally, we consider
multi-path fading ETU channel, which has high delay spread
and hence hard to achieve perfect equalization. We use the
MATLAB LTE Toolbox to simulate the transmission and
receiving of data. Finally, we also test the decoder on the
real-world data by transmitting and receiving over the air
using a USRP N200 SDR setup in non line of sight (NLOS)
multi-path environment. More comprehensive results for other
channels and the code for the algorithm can be found at:
https://github.com/sravan-ankireddy/nams

III. BACKGROUND

In this section, we brieﬂy review the BP decoding and
existing classical and data driven methods for its improvement.
We then look at the min-sum variant of the BP decoder and the
corresponding methods for its improvement. We also introduce
and review the baseline algorithms that we will be comparing
against.

A. Neural Belief Propagation decoder

The BP decoder is an iterative soft-in soft-out decoder that
operates on the Tanner graph structure of the linear block
codes to compute the posterior LLRs of the received vector.
We also refer to these messages as beliefs. In each iteration,
the check nodes and the variable nodes process the information

to update the beliefs passed along the edge. Operating in such
an iterative fashion allows for incremental improvement in the
estimated LLRs, converging towards the right codeword that
is obtained by binary quantization after the ﬁnal iteration.
During the ﬁrst half of the iteration, at the VN v,
the
received channel LLR lv is combined with the rest of the
incoming beliefs µt−1
c′,v from check node to calculate a new
updated belief,
to be passed to the check nodes in next
iteration. Hence, the message from VN v to CN c at iteration
t can be computed as

µt
v,c = lv +

µt−1
c′,v ,

(1)

Xc′∈N (v)\c

where N (v) \ c is the set of all check nodes connected to
variable node v except c.

During the latter half of the iteration, at the CN c, the
message from the CN to any VN is calculated based on the
criterion that the incoming beliefs µt
v′,c at any check node
should always satisfy the parity constraint. Speciﬁcally, the
message from CN c to VN v at iteration t is given by

µt

v′,c
2 !


c,v = 2 tanh−1
µt



tanh

,

(2)

Yv′∈M(c)\v
where M (c) \ v is the set of all variable nodes connected



to check node c except v.

Finally, at

the end of iteration t, we combine all

the

incoming beliefs to estimate the posterior belief as

ot
v = lv +

µt
c′,v .

(3)

Xc′∈N (v)
In [4], it is shown that the decoding performance of BP
can be improved by introducing trainable weights along the
edges of the Tanner graph, resulting in a Weighted Belief
Propagation (WBP), at the VN as

v,c = wt
µt

vlv +

Xc′∈N (v)\c

B. Neural Min-Sum decoding

wt

c′,vµt

c′,v .

(4)

To reduce the computational complexity of BP decoding, a
hardware friendly variant known as min-sum approximation is
used in practice. This simpliﬁes (2) to

µt
c,v = min

v′∈M(c)\v

(|µt

v′,c|)

sign

µt
v′,c

.

(5)

Yv′∈M(c)\v

(cid:0)

(cid:1)

While the min-sum approximation simpliﬁes the computation,
it also comes with a loss in performance. It is readily shown
in [11] that min-sum approximation is always greater than
the true LLR from BP. To correct for this,
two popular
techniques have been introduced [11]–[13]. The ﬁrst is to use
a normalization factor α to scale the beliefs appropriately as

µt
c,v = α

min
v′∈M(c)\v

(cid:18)

(|µt

v′,c|)

sign

µt
v′,c

.

(6)

(cid:19) Yv′∈M(c)\v

(cid:0)

(cid:1)

 
Another approach is to use an offset factor which reduces

the beliefs at the CN by β, while maintaining the sign, as

such as ETU and the over-the-air channels, beyond AWGN
channels.

min
v′∈M(c)\v

(cid:18)

µt
c,v = max

(|µt

v′,c|) − β, 0

sign

µt
v′,c

.

(cid:19) Yv′∈M(c)\v

(cid:0)

(cid:1)
(7)
While these methods have been shown to improve the error
correction performance of min-sum algorithm empirically,
there has been no analytical solution to calculate the optimal
normalization or offset parameters. One of the key reasons for
this is the difﬁculty in modeling the posterior probabilities.
The authors in [11] use Monte Carlo simulations to select the
best normalization factors and also remark the difﬁculty in the
theoretical analysis beyond ﬁrst iteration.
Similar to the neural BP decoder,

the neural min-sum
decoder can be formulated by using different normalization
or offset factors along different edges [5], [6]. The equations
(6) and (7) can be modiﬁed respectively to produce Neural
Normalized Min-Sum (NNMS) and Neural Offset Min-Sum
(NOMS) algorithms, given by

µt
c,v = αt

v′,c

min
v′∈M(c)\v

(cid:18)

(|µt

v′,c|)

sign

µt
v′,c

,

µt
c,v = max

min
v′∈M(c)\v

(cid:18)

(cid:19) Yv′∈M(c)\v

(|µt

v′,c|) − βt

v′,c, 0

sign

µt
v′,c

(cid:1)

(8)

(9)

(cid:0)

(cid:19)
,

Yv′∈M(c)\v
respectively, where the coefﬁcients αt
trainable weights.

(cid:1)
(cid:0)
v′,c and βt
v′,c denote

The drawback of using neural network based decoders is
the increase in storage and computational complexity. Hence,
it is important to study the gains achieved and explore possible
simpliﬁcations to the neural decoder with little to no trade-off
in performance.

C. Reducing the complexity via entangling the weights

To reduce the storage complexity of WBP, the authors in
[6] propose entangling the weights across iterations. Further,
in [7], the authors propose entangling weights not only across
iterations but across edges as well. However, this results in a
loss of performance, which is compensated using a parameter
adapter network that selects different weights for different
SNR regions, known as BP-PAN. Similary, for the neural min-
sum decoders, the authors in [6], [7] conclude that entangling
the weights across iterations or/and edges only comes with a
negligible loss for AWGN channels.

IV. NEURAL AUGMENTED MIN-SUM DECODING

In this section, we propose combining the augmentation of
normalization and offsets to form a more generalized neural
min-sum decoder. We refer to this as Neural Augmented
Min-Sum (NAMS) decoder. The modiﬁed CN update rule at
iteration t is given by

µt
c,v = αt

v′,c

max

(cid:18)

(cid:18)

min
v′∈M(c)\v

(|µt

v′,c|) − βt

v′,c, 0

(cid:19)(cid:19)
.

µt
v′,c

sign

(10)

Yv′∈M(c)\v

(cid:0)

(cid:1)

We consider the min-sum approximation and limit
the
modiﬁcations to only CN updates to keep the complexity low.
Entanglement. We consider three variants of NAMS with

different levels of entanglement.

1) NAMS-full: Different weights for all edges across all

iterations as seen in (10).

2) NAMS-relaxed: Entangled weights across the iterations.

µt
c,v = αv′,c

max

(cid:18)

min
v′∈M(c)\v

(cid:18)

(|µt

v′,c|) − βv′,c, 0

(cid:19)(cid:19)
.

µt
v′,c

sign

Yv′∈M(c)\v

(cid:0)

(cid:1)

3) NAMS-simple: Entangled weights across the iterations

and edges.

µt
c,v = α

max

(cid:18)

min
v′∈M(c)\v

(cid:18)

(|µt

v′,c|) − β, 0

(cid:19)(cid:19)
.

µt
v′,c

sign

Yv′∈M(c)\v

(cid:0)

(cid:1)

Loss function and Training. We use the binary cross
entropy with logits loss function to measure the difference
between the soft output of the each layer and the transmitted
codeword and take the sum of losses across each layer to
calculate the multiloss term [6] as

L(o, x) = −

1
N

T

N

t=1
X

v=1
X

xv log(ot

v) + (1 − xv) log(1 − ot

v),

D. Non-AWGN channels

In channels with multi-path and Doppler components, the
received LLRs are more unreliable compared to AWGN
channel, which makes the decoding harder. Hence, whether
the observation in [6], [7] that entangling the weights lead
to negligible reliability remains the same for non-AWGN
channels remains an open problem. In the following, we
study the effect of weight entanglements for various channels,

where ot

v is the output for symbol v at iteration t of NAMS

and x is the transmitted codeword.

To keep the comparison fair, we use the same amount of
training data as [5]. Each mini-batch consists of 10 codewords
per SNR point in the training region and Stochastic Gradient
Descent (SGD) method is used to learn the weights. We use
the Adam optimizer with a initial learning rate of 0.005 and
use cosine annealing learning rate scheduler [14].

(a) AWGN

(b) ETU

Fig. 1: BER for BCH(63,36) : On the left we see that entanglement of weights has almost no effect on performance for AWGN channel.
However, for ETU channel on the right, NAMS-full outperforms NAMS-simple by more than 2 dB at a BER of 10−3.

V. RESULTS

We evaluate the error correction performance of different
neural decoders and study the trade-off associated with entan-
glement for various channels. We also look at the robustness of
the learned decoders by testing across channels. Additionally,
we also study the adaptability of the learned decoders via ad-
ditional training on the new channel. Finally, we demonstrate
the performance of our NAMS decoder on practical data using
USRP N210 SDR transmitter and receiver setup.

A. Entanglement vs channels

In Fig. 1 we plot the the Bit Error Rate (BER) performance
of three variants of the NAMS decoder with varying degrees
of entanglement. We choose BCH(63,36) speciﬁcally to enable
direct comparison against the existing neural decoders [4]–
[7]. We can see from Fig. 1(a) that the performance is almost
indistinguishable across the variants for AWGN channel. In
Fig. 1(b), we perform the same experiment for ETU channel.
For this simulation, we estimate the channel at the receiver
using the pilots of OFDM symbols and use the MMSE equal-
izer from LTE MATLAB toolbox to perform equalization. We
can see from Fig. 1(b) that even after equalization, the choice
of entanglement has signiﬁcant impact on the error correction
performance. At a moderate BER of 10−3, the NAMS-full
outperforms NAMS-simple by more than 2 dB.

AWGN vs. ETU. One of the key differences between the
AWGN and ETU channels is that the multi-path components
in the ETU channel create a correlation between adjacent
received symbols. To understand the impact of this better,
we compute the expected value of correlation coefﬁcient
between the consecutive received symbols post equalization
i.e, E[ρlv ,lv+1] where lv is LLR of the vth received symbol.
From this, we see that
the correlation between adjacent
symbols is much higher for ETU channel. For the setting
in Fig. 1, we observed an expected correlation coefﬁcient of
0.02 and 0.15 between adjacent symbols for AWGN and ETU
channels respectively. We conjecture that through training, the

decoder learns to compensate for this imperfect equalization
and improve the error correction performance.

Fig. 2: BER for BCH(63,36) ETU channel: NAMS outperforms
NOMS and NNMS by 0.1-0.3 dB. Additionally, NAMS-full is close
to WBP but with lower complexity.

B. Comparison with existing neural decoders

For a complete comparison, we ﬁrst compare NAMS-
full against the neural BP decoders WBP [4] and BP-PAN
[7]. Fig. 2 shows that with similar number of weights, the
performance of NAMS-full is comparable to that of WBP
decoder but with much lower complexity because of the min-
sum approximation. Additionally, we also see that NAMS-full
outperforms BP-PAN by more than 1dB, by taking advantage
of the large number of weights. 1

In Fig. 2, we compare the performance of fully parame-
terized NAMS against existing neural min-sum decoders for

1We note that the results in Fig. 2 are based on our PyTorch implemen-
tation. When the simulations are ran using the publicly available code for
NOMS and NNMS, the performance was poor and hence we use our own
implementation of these algorithms with the hyper-parameters optimized for
the ETU channels.

10-1

10-2

R
E
B

10-3

10-4

BP
Min-Sum
BP-PAN trained on AWGN
WBP trained on AWGN
NOMS-full trained on AWGN
NNMS-full trained on AWGN
NAMS-full trained on AWGN

9

10

11

12

13

14

Eb / N0

15

16

17

18

Fig. 3: BER for BCH(63,36) ETU channel: The neural min-sum
family is more robust than BP-PAN. Among the neural min-sum
family, NAMS-full is up to 0.5 dB more robust compared to NOMS-
full and NNMS-full.

the ETU channels. Fig. 2 shows that NAMS-full outperforms
NOMS [5] and NNMS [6] by 0.2-0.3 dB.

Robustness. We test the robustness by taking a decoder
trained on AWGN channel and testing on ETU channel. Fig. 3
shows that when the channel changes from AWGN to ETU, the
NAMS-full decoder still outperforms the original BP decoder.
Additionally, the degradation in performance because of the
change of channel is lower for NAMS-full and is up to 0.4
dB more robust compared to other min-sum decoders. It is
also interesting to note that the performance of BP-PAN [7]
degrades signiﬁcantly when the decoder is directly reused on
ETU channel. This is mainly because the weight selection by
PAN is heavily dependent on the SNR which does not translate
directly when the channel is changed. This shows that while
BP-PAN [7] learns very well on a simple channel with very
few parameters, NAMS takes advantage of the larger number
of weights and generalizes well, making it more robust.

Adaptivity. Given the dynamic nature of channel condi-
tions experienced in practical wireless channels, we would like
our decoder to be able to adapt to new channels quickly. To
this end, we train our existing decoder, that was trained on
AWGN channel, on minimal (10 − 15%) amount of data from
ETU channel, and observe that we recover the reliability of
matched training (i.e., training on the ETU channel), showing
that the NAMS decoder has good adaptive capabilities.2

C. Robustness and adaptivity to new channels

While using the same channel conditions for training and
testing gives the best performance [15], we also desire the
learned decoder to be (a) reasonably robust to any changes in
channel conditions and (b) quickly adaptable to new channels.

D. Over The Air channels

Apart from learning on the simulated data, we train and
test the NAMS decoder on a real multi-path fading envi-
ronment by using two USRP N200 series RF-transceivers to

2Complete results available at github repository.

communicate over the air. We generate random data, encode
it using BCH(63,36) code and modulate using BPSK. We add
a synchronization preamble and transmit over the channel. At
the receiver, we capture the frames, correct for frequency offset
and perform channel equalization. We then demodulate the
data to estimate the LLR and feed it to the NAMS decoder
for inference. Fig. 4 shows that for a given power level, for
example 3 dBm, NAMS-full corrects up to 2.88× more errors
comapred to NAMS-simple, which has only two weights.

!

R
E
B

10-1

10-2

10-3

10-4

10-5

10-6

1

BP
Min-Sum
NAMS-simple
NAMS-relaxed
NAMS-full

1.5

2

2.5
Eb / N0

3

3.5

4

Fig. 4: BER for BCH(63,36) OTA channel: at 3 dBm, NAMS-full
corrects 2.88 × more errors compared to NAMS-simple.

VI. INTERPRETATION

While neural decoders achieve some impressive gains, it is
natural to wonder about the reason for these gains. In [4],
[6], [7], it has been conjectured that the weights mitigate the
effect of cycles and thus improve the performance. While this
is an intuitively sound explanation, the evidence for the same is
lacking. We provide a ﬁrst empirical evidence that the weights
are closely associated with the cycles in the graph.

For ease of analysis, we consider the NNMS-full version of
the neural min-sum decoder, which only has the normalization
factors. We ﬁrst study the relation between the weights learned
and the cycles in the graph.

Connection to cycles. We look at three different rates of
length 63 BCH codes, which have different number of length-
4 cycles. For each code, we measure the average weight across
all edges and iterations. Given the degradation associated with
the cycles, we would expect the learned weights to attenuate
more in presence of higher number of cycles. From Table I,
we can see that this holds true, with BCH(63,30) having the
highest number of cycles and hence the least average weight,
while the default weight in traditional min-sum decoder would
be 1. This shows that the weights being learned are indeed
commensurate with the cycles present.

Since these cycles lead to correlation between the incoming
beliefs at the variable nodes, we proceed to empirically esti-
mate and compare these correlations to better understand the
effect of the weights.

Correlation. We consider the AWGN channel for this
analysis, where the correlation between incoming beliefs at

[2] W. Ryan and S. Lin, Channel codes: classical and modern. Cambridge

university press, 2009.

[3] R. Yazdani and M. Ardakani, “Linear llr approximation for iterative
decoding on wireless channels,” IEEE Transactions on Communications,
vol. 57, no. 11, pp. 3278–3287, 2009.

[4] E. Nachmani, Y. Be’ery, and D. Burshtein, “Learning to decode linear
codes using deep learning,” in 2016 54th Annual Allerton Conference on
Communication, Control, and Computing (Allerton), 2016, pp. 341–346.
[5] L. Lugosch and W. J. Gross, “Neural offset min-sum decoding,” in 2017
IEEE International Symposium on Information Theory (ISIT), 2017, pp.
1361–1365.

[6] E. Nachmani, E. Marciano, L. Lugosch, W. J. Gross, D. Burshtein, and
Y. Be’ery, “Deep learning methods for improved decoding of linear
codes,” IEEE Journal of Selected Topics in Signal Processing, vol. 12,
no. 1, pp. 119–131, 2018.

[7] M. Lian, F. Carpi, C. Häger, and H. D. Pﬁster, “Learned belief-
propagation decoding with simple scaling and snr adaptation,” in 2019
IEEE International Symposium on Information Theory (ISIT), 2019, pp.
161–165.

[8] M. Yazdani, S. Hemati, and A. Banihashemi, “Improving belief prop-
agation on graphs with cycles,” IEEE Communications Letters, vol. 8,
no. 1, pp. 57–59, 2004.

[9] “3gpp ts 36.101. evolved universal terrestrial radio access (e-utra); user
equipment (ue) radio transmission and reception.” in 3rd Generation
Partnership Project; Technical Speciﬁcation Group Radio Access Net-
work.

[10] “3gpp ts 36.104. evolved universal terrestrial radio access (e-utra); base
station (bs) radio transmission and reception,” in 3rd Generation Part-
nership Project; Technical Speciﬁcation Group Radio Access Network.
[11] J. Chen and M. Fossorier, “Near optimum universal belief propagation
based decoding of low-density parity check codes,” IEEE Transactions
on Communications, vol. 50, no. 3, pp. 406–414, 2002.

[12] J. Chen, A. Dholakia, E. Eleftheriou, M. Fossorier, and X.-Y. Hu,
“Reduced-complexity decoding of ldpc codes,” IEEE Transactions on
Communications, vol. 53, no. 8, pp. 1288–1299, 2005.

[13] J. Chen, R. Tanner, C. Jones, and Y. Li, “Improved min-sum decoding
ldpc codes,” in Proceedings. International
algorithms for irregular
Symposium on Information Theory, 2005. ISIT 2005., 2005, pp. 449–
453.

[14] I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient descent with

warm restarts,” arXiv preprint arXiv:1608.03983, 2016.

[15] Y. He, J. Zhang, S. Jin, C.-K. Wen, and G. Y. Li, “Model-driven dnn
decoder for turbo codes: Design, simulation, and experimental results,”
IEEE Transactions on Communications, vol. 68, no. 10, pp. 6127–6140,
2020.

[16] H. El Gamal and A. Hammons, “Analyzing the turbo decoder using
the gaussian approximation,” IEEE Transactions on Information Theory,
vol. 47, no. 2, pp. 671–686, 2001.

Code

Length-4 cycles

BCH (63,30)
BCH (63,36)
BCH (63,57)

10122
5909
1800

Average weight

AWGN

ETU

0.2632
0.2987
0.3990

0.4079
0.4318
0.6583

TABLE I: Number of short cycles present vs the weights learned.
Weights are inversely proportional to number of cycles indicating
that the weights correct more when number of cycles is high.

nodes is because of the cycles in Tanner graph. To ﬁnd if
the learned weights mitigate this effect, we measure and com-
pare the expected pairwise correlation coefﬁcient between the
incoming messages at the variable nodes i.e, E[ρµv,ci ,µv,cj ],
where ci, cj ∈ M (v) and i 6= j and the expectation is over
the message and the channel. We observe from Fig. 5 that the
correlation is reduced considerably across all the bit positions,
which supports our conjecture that the weights are improving
the reliability of the posterior probabilities by reducing the
correlation.

Fig. 5: BCH(63,36) AWGN channel at SNR 4dB. The expected
pairwise correlation between incoming messages at variable nodes
is reduced across all positions.

VII. CONCLUSION AND REMARKS

In this work we provided a generalized framework for
augmenting the min-sum variant of BP decoder for linear
block codes. Through our simulations, we show that having
more weights in the decoder is more advantageous in presence
of complex channels. We also provide empirical evidence
showing relation between the underlying code structure and
the weights learned; we show that the weights learned are
attenuating the effect of these cycles and improve the relia-
bility of the posterior LLRs. As a future direction, we would
like to understand more about the connections between the
choice of hyper parameters and the structure of the code, that
can help in faster learning of the weights. We also believe
that a faster adaptation to newer channels is possible by using
meta learning techniques. On the theoretical front, it would
be interesting to analyze the performance of neural decoders
using Gaussian approximation [16].

REFERENCES

[1] CCSDS and N.A.S.A, “Recommendation for space data system stan-
dards, ccsds 231.0-b-4,” https://public.ccsds.org/Pubs/231x0b4e0.pdf ,
2021.

