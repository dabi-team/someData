Quality issues in Machine Learning Software
Systems

Pierre-Olivier Cˆot´e, Amin Nikanjam, Rached Bouchoucha, Foutse Khomh
SWAT Lab., Polytechnique Montr´eal, Qu´ebec, Canada
{pierre-olivier.cote,amin.nikanjam,rached.bouchoucha,foutse.khomh}@polymtl.ca

2
2
0
2

g
u
A
2
2

]
E
S
.
s
c
[

2
v
2
8
9
8
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Context: An increasing demand is observed in var-
ious domains to employ Machine Learning (ML) for solving
complex problems. ML models are implemented as software
components and deployed in Machine Learning Software Systems
(MLSSs). Problem: There is a strong need for ensuring the
serving quality of MLSSs. False or poor decisions of such systems
can lead to malfunction of other systems, signiﬁcant ﬁnancial
losses, or even threat to human life. The quality assurance of
MLSSs is considered as a challenging task and currently is a
hot research topic. Moreover, it is important to cover all various
aspects of the quality in MLSSs. Objective: This paper aims to
investigate the characteristics of real quality issues in MLSSs
from the viewpoint of practitioners. This empirical study aims
to identify a catalog of bad-practices related to poor quality
in MLSSs. Method: We plan to conduct a set of interviews
with practitioners/experts, believing that interviews are the best
method to retrieve their experience and practices when dealing
with quality issues. We expect that the catalog of issues developed
at this step will also help us later to identify the severity, root
causes, and possible remedy for quality issues of MLSSs, allowing
us to develop efﬁcient quality assurance tools for ML models and
MLSSs.

Index Terms—Machine Learning based Software Systems,

Quality Assurance, Quality issues, Interview.

I. INTRODUCTION

Nowadays, Machine Learning Software Systems (MLSSs)
have become a part of our daily life (e.g., recommendation
systems, speech recognition, face detection). An increasing
demand is observed in various companies to employ Ma-
chine Learning (ML) for solving problems in their business.
Typically, a MLSS receives data as input and employs ML
models to make intelligent decisions automatically based on
learned patterns, associations and knowledge from data [1].
Therefore, ML models are implemented as software compo-
nents integrated to other subsystems in MLSSs, and like other
software systems, quality assurance is necessary. According to
the growing importance of MLSSs in today’s world, there is
a strong need for ensuring their serving quality. False or poor
decisions of such systems can lead to malfunction of other
systems, signiﬁcant ﬁnancial losses or even threat to human
life [2].

The quality assessment of MLSSs is regarded as a chal-
lenging task [3] and currently is a hot research topic [4],
[5]. Recently some research work on the quality of MLSSs is
suggested to cover all different aspects of the quality [6], [7],
i.e., not only their prediction accuracy. In this paper, we, with
our industrial partner, plan to investigate the characteristics

of real-world quality issues in MLSSs from the viewpoint of
practitioners identifying a list of bad-practices related to poor
systems/models quality. This is a requirement of comprehen-
sive quality assessment of MLSSs, as Zhang et al. already
acknowledged the lack of such empirical study and asserted
that conducting empirical studies on the prevalence of poor
models among deployed ML models should be interesting
[8]. This study will cover all relevant quality factors like
performance (accuracy), robustness, explainability, scalability,
hardware demand and model complexity. We plan to conduct
a set of interviews with practitioners/experts, believing that
interviews are the best method to retrieve their experience and
practices when dealing with quality issues. We expect that
the catalog of issues developed at this step will also help us
later to identify the severity, root causes and possible remedy
for quality issues of MLSSs, allowing us to develop efﬁcient
quality assurance tools for ML models and MLSSs. We present
in the following the proposed methodology to achieve our
objectives.

II. RELATED WORK

In 2015, Sculley et al. shared through a seminal work, the
challenges that Google faced while building, deploying, and
maintaining ML models [9]. Following this work, many other
studies tried to characterize quality issues in MLSSs [10]–
[17]. Van Oort et al. mined open-source GitHub repositories
to aggregate a list of the most common maintenance-related
modiﬁcations in Deep Learning (DL) projects [12]. From this
list, they extracted 5 code smells in DL systems. Furthermore,
they measured how prevalent and problematic the code smells
are from the point of view of practitioners using a survey.
Similarly, Dilhara et al. [18] mined 26 open-source MLSSs
and identiﬁed 14 refactoring and 7 new technical debt (TD)
categories speciﬁc to ML. Instead of ﬁnding new quality
issues, Alahdab et al. shared with the scientiﬁc community
how TD types appear in the early phases of an industrial
DL project [11]. Facing the increasing number of papers
regarding technical debt and code smells in MLSSs, Bogner
et al. attempted to aggregate the knowledge on these topics
by performing a systematic mapping study that presented 4
new types of technical debt, 72 antipatterns along with 46
solutions [12]. A similar work has been done by Washizaki
et al. but by consulting grey literature as well [10]. Other
works indirectly contributed toward increasing the knowledge
on quality issues in MLSSs, such as the study of Nahar

 
 
 
 
 
 
et al. [17], which looked at collaboration challenges while
building ML systems. In reaction to the growing concern of
quality issues in MLSSs, researchers developed tools such
as the DataLinter [19] or the data validation component of
TFX [5] to automate quality assurance processes. Another
group of researchers adopted a different approach and instead
shared a checklist of tests to assess the production readiness of
MLSSs [7]. In a similar fashion, a process model is proposed
by Studer et al. for the development of ML applications
with a quality assurance methodology [6]. While there is an
increasingly growing number of studies on quality issues in
MLSSs, a signiﬁcant portion of them is produced by large
software enterprises such as Google or Meta, which limits the
generalization of the ﬁndings [12]. We believe that there is a
need for a study presenting the quality issues of MLSSs that
are encountered by practitioners from different backgrounds
and company sizes.

III. STUDY DESIGN

A. Objectives and Research Questions

The goal of this study is to provide a detailed analysis
of quality issues (including data and model) in MLSSs. We
believe that interviewing people who really experienced these
issues is an effective way to gain that knowledge. Thus, we
will proceed this way and we deﬁne the following Research
Questions (RQ):

RQ1: What are the quality issues encountered by prac-
titioners when building MLSSs and which ones
are the most prevalent? For future works to solve
quality issues, they must know the issues that exist.
While there is some literature covering some quality
issues [6], [9], [17], we believe that a lot is still
unknown. In this study, we aim to share with the
research community the quality challenges encoun-
tered by practitioners when building ML systems.
This includes issues related to data, model, and other
components in MLSSs.

RQ2: What are the root causes, symptoms, and con-
sequences of quality issues? To understand quality
issues in ML systems, it is not sufﬁcient to know
they exist, but also their root causes, symptoms and
potential consequences. We hope that the answer to
that question will guide future work towards solving
the most pressing issues.

RQ3: How are the quality issues currently handled
by the practitioners? Once quality issues have
been detected (e.g., in data or model), we expect
practitioners to have put in place mechanisms to
mitigate or at least attenuate their consequences. We
are interested in understanding the current mitigation
approaches implemented in the industry.

RQ4: In the case of data, which data types and collec-
tion processes are the most challenging in terms
of data quality? The training data ingested by ML
models comes in many forms. Face recognition sys-
tems use images, while stock prediction applications

can use numbers/amounts (like time-series data) or
even text. Each data type comes with its own data
quality challenges when training ML models. By
answering this RQ, future research will be guided
towards the most pressing data quality challenges
faced by practitioners. Authors in [20] mention that
data collection processes, the process of gathering
data, may affect the quality of data. For example,
one could choose to train her/his model on public
datasets, or manually acquire more data with data
collectors. Each of these processes have different
challenges which may lead to different data quality
issues. In this study, we want to identify the data
collection processes that are most prone to data
quality issues.

RQ5: What are the challenges of data quality assurance
during model evolution? Many MLSSs encounter
rapidly changing/non-stationary data, adversarial in-
put, or difference in data distribution (concept drift,
e.g., content recommendation systems and ﬁnancial
ML applications). Hence, the quality of the deployed
model may be decreasing over time and conse-
quently affects the performance of the whole system.
Therefore, the robustness and the accuracy of the
model’s predictions must be assessed frequently in
production. Actively monitoring the quality of the
deployed model in production is crucial to detect
performance degradation and model staleness.

B. Industrial Partner

MoovAI1 is a Montreal-based company in Canada, which is
active in developing AI/ML quality assurance solutions to ad-
dress practical needs in the various businesses. MoovAI’s ex-
perts guide their customers (i.e., companies) to take advantage
of these cutting-edge technologies, regardless of their level of
maturity in data science. Nowadays, various sectors in industry
are and will be developing ML models in their systems, e.g.
energy sector, ﬁnancial sector, supply chain recommendations,
medical diagnosis and treatment. As ML-based technologies
become more widespread, the quality demands of ML become
more important. Poor quality models are a potential barrier in
exploiting ML in real-world applications and currently more
companies request for qualiﬁed ML-systems [21]. A study
showed that 87% of ML proof of concepts have never come
to production [22], usually due to lack of enough quality.
Currently MoovAI is using a model validation tool
to
validate ML models prior to deployment in the real world
[23]. This tool includes preliminary validation methods to
assess the overall quality of a ML model. The tool evaluates
accuracy, stability, biases and sensitivity of ML models. Now,
MoovAI aims to push their tool forward to develop advanced
methods for ensuring the quality of not only the ML models
but also ML-based systems (i.e., software systems containing
ML components) and to develop a stand-alone model vali-

1https://moov.ai/en/

dation platform. Experts at MoovAI now are looking for a
comprehensive quality evaluation tool to assess and monitor
the quality of ML models during their whole life cycle; from
data collection, to development, deployment, and maintenance.

C. Participants

We plan to interview at least 50 participants, which is more
than previous similar studies [24], [25]. The interviews will
occur in two rounds. In the ﬁrst round of the study, we
will interview the employees of our industrial partner (i.e.,
MoovAI). They are Data Scientists, Machine Learning Engi-
neers, Data Engineers and Project Managers of ML projects.
They have worked on many different ML projects for different
clients. Thus, we think that they are able to provide a global
view of quality issues encountered in the industry. We expect
to be able to conduct at least 25 interviews with MoovAI’s
practitioners. In the second phase of the study, we plan to
interview practitioners from other companies. We will adopt
4 strategies to recruit participants.

• Personal contacts: We will start recruiting people for
interviews through our personal contacts, since it usually
has a higher response rate than cold emails. We will
contact industrial partners in our network. For example,
we plan to reach out to companies who are partners
in Software Engineering for Artiﬁcial Intelligence1, a
training program co-created by our lab which includes
companies such as IBM, Ericsson, and Cisco. We will
also use LinkedIn2 to ﬁnd qualiﬁed experts that may have
relevant expertise for this project. Using the results from a
similar study [24], we expect at least 5 positive responses.
• Q&A websites: Questions and Answers websites are plat-
forms on which a lot of knowledge is shared. Following
similar previous work [24], we will search for practi-
tioners with meaningful experience on ML willing to be
interviewed on quality issues in ML systems. We chose
to search on Stack Overﬂow3 and Data Science Stack
Exchange4, because both these websites are signiﬁcantly
used by the ML community for question answering.
to the top askers and answerers5
We will reach out
since they have shown signiﬁcant involvement in the ML
community and most likely have expertise. In order to
ﬁnd them, we will search on two platforms: 1) Data
Science Stack Exchange, on which we will simply look
for the top answerers and askers of the Q&A platform on
any topic, and 2) Stack Overﬂow, for which a different
strategy must be adopted since the website also holds
questions regarding Software Engineering in general.
Thus, we will search for the top askers and answerers
for topics related to the subject of quality issues in ML
systems, using tags. Similar to [24], we selected the tags

1https://se4ai.org/
2https://www.linkedin.com/
3https://stackoverﬂow.com/
4https://datascience.stackexchange.com/
5The words askers and answerers are part of the terminology used by

the Q&A websites to describe people asking and answering questions

data-cleaning, dataset, machine-learning and artiﬁcial-
intelligence. We will search for practitioners on Data
Science Stack Exchange and on Stack Overﬂow using
the 4 tags we have described (1 for Data Science Stack
Exchange + 4 tags on Stack Overﬂow, 5 in total). Top
users are ranked in two categories: ’Last 30 Days’ and
’All Time’; we will pick the ﬁrst 10 users from both. In
total, we will send 100 emails. Using the results from a
similar study [24], we expect to meet 10 interviewees.
• Social networks: Social networks are platforms in which
users may engage conversations on a wide range of
topics. Some of them host discussions on ML and Ar-
tiﬁcial Intelligence in general. We believe users with
important ML expertise can be found on these websites.
We are planning to post an invitation for interviews on
the deep learning and ML communities of Reddit6 similar
to our previous works [26]. We expect at least 5 positive
responses.

• Freelance platforms: Following previous work [24], we
plan to use freelance platforms to ﬁnd practitioners with
ML expertise if all the other techniques have been used
and we have not reached theoretical saturation yet [27].
We will follow the methodology of similar studies like
[24] to select the candidates for interviews.

D. Interview Process

Since the subject of quality issues is not mature and has
still room for research work, we will follow a research
procedure suited for exploratory work, Straussian Grounded
Theory [28]. It is a research method in which data collection
and data analysis are executed in an iterative manner until
a new theory emerges from the data. As opposed to many
deductive approaches where a theory is ﬁrst conceptualized
then tested through experiments, Grounded Theory goes the
opposite way by inductively generating a theory from the
data. The knowledge gathered from data using open and axial
coding should guide the sampling process, a concept named
theoretical sampling. Data collection stops when theoretical
saturation is met: when the understanding of the subject is
complete and new data does not invalidate the emerging theory.
Grounded Theory is often used when little is known about a
phenomenon, because of its ﬂexibility and its aptitude for the
discovery of unknown concepts. It has been used in similar
studies, such as [17]. Because we are doing exploratory work,
the interviews have to be structured in a way that allows the
interviewee to share knowledge we might not be aware of. For
this reason, we will be conducting semi-structured interviews
similar to [24]. We devised an interview guide that to help the
interviewer cover every relevant topic. It is composed mostly
of open-ended questions, to allow the interviewee direct us
towards interesting information. It is the responsibility of the
interviewer to ask follow-up questions when the respondent
touches upon a subject relevant to the study. To avoid the inter-
viewer being overwhelmed with his tasks and having difﬁculty

6https://www.reddit.com/

asking the right follow up questions, each interview will be
conducted by two persons. This follows the recommendation
of previous works which show that interviewees share more
information when two interviewers are present rather than one
[29]. The second interviewer will be tasked with helping the
primary one to ask follow-up questions and to transcribe the
interview with the support of the tool Descript1. This tool is
an automated speech recognition tool that can transform the
audio stream of an online meeting into text. The role of the
second interviewer will simply be to correct the mistakes of
the transcription of the tool.

The interview guide will be subject to change as our knowl-
edge on the topic grows, since we are following Grounded
Theory. During its writing, we will consider quality issues
elicited by other studies [7], [9], [10], [12], [25]. We also took
care of asking questions that have potential to cover different
quality dimensions as deﬁned by [30]. We will complete our
interview guide with questions drawn from a similar study
[24]. In order to assess the quality of our interview guide, we
will conduct a pilot of our study. We will purposefully select
5 participants with diverse experience (i.e. Data scientists,
Data/ML Engineers and AI project managers). Doing so, we
will be able to verify that our questions are unambiguous,
precise and able to answer our research questions effectively.
A few days prior to the interview, we will share a summary
of the content interview with the participants so they can be
familiar with the objectives of the study.

Interviews will take place in English or French, depending
on the preference of the interviewee. We will start by giving
a brief overview of the project followed by a quick round
of introductions. Then we will ask participants for some
background information regarding his experience in ML. The
interview will ofﬁcially start with a general question: ”What
are the main quality issues you have encountered with your
data/model so far”? By asking an open-ended question, we are
allowing the interviewee to share experiences he is the most
conﬁdent to talk about. Then we will probe the interviewee’s
experience in an attempt to discover quality issues. We will
cover each phase of a ML workﬂow as described by [13].
in order to exhaustively search for situations where quality
issues might occur. As a difference, we do not include the
model requirement phase in our study and chose to merge
data cleaning with feature engineering and data labeling with
data collection for conciseness.

• Data collection: We ask for experiences with different
data collection processes: data collectors, automatically
generated data, public datasets, external services (e.g. a
weather API), or predictions of another model (effectively
creating cascading models). For each one of them, we
search for issues the interviewee may have experienced
collecting the data along with solutions they put in place.
• Data preparation: Notably, we ask about pain points
when preparing data for ML and for tools to automate the

1https://www.descript.com/transcription

process. We consider any challenge related to the cleaning
and transformation of the data.

• Model evaluation: We probe for potential problems that
the interviewee experienced when evaluating the quality
of its model. For example, we ask if the respondent
ever faced a situation where the model performed poorly
on some group of people, potentially leading to fairness
issues.

• Model deployment: We gather general information about
the process by which models are put into production.
Then, we search for issues encountered at this step. For
example, one question is: “Did you ever deploy a model
that performed well locally but poorly once deployed?”.
• Model maintenance: We ask the interviewee how he
ensures that the quality of its models remains the same
after deployment. We speciﬁcally ask for past instances
of model staleness and how it has been handled.

At the end of every section, we ask for any other issue that the
interviewee may have at this step of the workﬂow in case we
missed out on something. We will conclude the interview with
the open question: ”In your opinion, what is the most pressing
quality issue researchers should work on in an attempt to solve
the problem?”. The answer to that question might provide
interesting future work directions and follows Harvard’s best
practices for qualitative interviews2. The interview guide is
available online 3.

Prior to starting the interview, we will ask to the respondent
for the permission to register and share the transcription. In
order to follow ethical guidelines, we validated our research
project with Research Ethics Committee at Polytechnique
Montr´eal4 and got their approval. We will anonymize the
respondent of the interviews in the transcript in order to respect
their privacy.

E. Questionnaire

After interviews have been conducted and analyzed, we
expect to have a set of potential quality issues. In order to
validate the quality of our ﬁndings, we will investigate their
prevalence in real MLSSs. Owners of such systems will be
contacted and asked to answer a questionnaire where quality
issues are presented. The questionnaire will be built using
Google Forms5. We will reach out to owners of MLSSs by
contacting MoovAI’s clients. We expect to have answers from
at least 20 respondents from 4 different companies. Because
we are aware that they might not have a profound under-
standing of ML, the form will describe the symptoms of the
potential quality issues including their technical description.
The respondents will evaluate on a Likert scale [31] how often
this kind of problem happens in their experience. In the case
of a positive response, they will be invited to share in more
detail about the issues that they experienced and to describe
their consequences in MLSSs.

2https://sociology.fas.harvard.edu/ﬁles/sociology/ﬁles/interview strategies.pdf
3https://github.com/poclecoqq/quality issues in MLSSs
4https://www.polymtl.ca/recherche/la-recherche-polytechnique/exigences-deontologiques/travaux-de-recherche-avec-des-etres-humains
5https://www.google.ca/forms/about/

F. Analysis Plan

For all the research questions except RQ4, we will use the
coding techniques from Straussian Grounded Theory [32] to
extract knowledge from data. When an interview transcript
is coded for the ﬁrst time, we will use open coding [33] to
break it into discrete parts. These codes will be reorganized
and grouped into categories through a step of axial coding.
This process allows the researchers to analyze its data on a
higher level so to have a better understanding of it. In the
ﬁnal rounds of coding, the central theme around which all
categories relate can be established in the ﬁnal procedure
of selective coding. While the current document has been
written such as the data collection and analysis plan are
separated, it is important to understand that these two steps
will happen in an iterative manner. We expect to write down
memos of preliminary categories throughout the process of
data collection and analysis. They will contribute to the rise
of our theory through phases of memo sorting.

For RQ4, because the question is less open-ended, we will
adopt a simpler coding strategy. The codes will be the data
set quality dimensions deﬁned in [30] and the data collection
processes we will encounter in our interviews. We will sum the
codes throughout the interviews and we will take into account
duplicates: when two interviewees are on the same team and
share the same problem.

In order to measure the most prevalent quality issues and
effectively answer RQ1, we will average the results (that were
on a Likert scale) from the questionnaire.

To help us code the transcripts, we will use Delve qualitative
analysis tool1, because the researchers are familiar with it and
it is easy to use. Delve is a computer-assisted qualitative data
analysis software (CADQAS) that provides simple interfaces
to code and to analyze data. In order to ensure the quality of
the analysis, each document will be coded by two researchers.
In case of a disagreement in codes, a third researcher will play
the role of moderator and will select the ﬁnal code for a text
segment. This process will be helpful for the construction of
a shared understanding of the data.

On completion of the study, we will share with the public
a replication package. This package will contain the interview
guide, the anonymized transcription of the interviews and any
discussion or analysis between the contributors that could help
replicating the study.

G. Threats to validity

Using some of the validity threats described in [34], we
will divide the analysis of the limitations of the study into
four categories: internal validity, external validity, construct
validity and conclusion limitations. The former refers to the
degree of conﬁdence that the ﬁndings are trustworthy. For
example, confounding factors or a lack of scientiﬁc rigor could
hinder the quality of the results. In our case, we identiﬁed
there is a
four potential threats to internal validity. First,
risk of some quality issues being over-represented. In fact,

we are conducting semi-structured interviews; some of the
conversations with the interviewee will be improvised. Thus,
it is possible that the ﬁndings are tainted by the precon-
ception of the interviewer about the potential quality issues
in MLSSs. For example, there could be more emphasis on
explainability issues in the interview, which may lead to an
over-representation of these issues in our ﬁndings. Second, it
is possible that the questions asked when the interviews are
in English are not exactly the same as the ones asked when
the interviews are in French. While our primary interviewers
have a good understanding of both languages, their choice
of words might convey slightly different meanings, leading
to bias. Third, the coding of the interview may be prone
to researcher bias, because it partially relies on subjective
interpretation. To mitigate this bias, each interview will be
coded by two researchers, and inconsistencies in the codes will
be resolved by a third researcher. Fourth, there is a risk that our
ﬁndings for RQ2 are inaccurate because of our data collection
method. Asking practitioners about the root cause of quality
issues may lead to misunderstandings or even misinformation
in case they do not want to admit their mistakes. To mitigate
this issue, we will cross our ﬁndings with MoovAI knowledge
on similar cases.
For external validity, we identiﬁed two potential ways that
our sample of practitioners could be unrepresentative of the
industry. First, we expect a majority of the interviewees to
be practitioners from Moov.AI. Their clients are companies of
medium to large size. Thus, our results may not reﬂect the
reality of very small companies, such as startups, or larger
software enterprises, such as Google, Microsoft, and the likes.
Second, because the study follows Grounded Theory and we
are doing theoretical sampling [27], it is possible that we end
up focusing on some slice of the population and leaving others
understudied.
About construct validity, limitations of our approach could
come from keywords (tags) used for ﬁnding candidates over
Q&A websites and social media. For keywords, we used data-
cleaning, dataset, machine-learning and artiﬁcial-intelligence
which are sufﬁciently general to match a lot of users and is
more likely to generate False Positive (users that we would
end up ignoring) rather than False Negative.
Conclusion limitations can be of potential wrongly understood
issues, missing issues, and the replicability of the study. We
believe that our sources of information are sound and various
enough to be representative, and not misleading us in our
conclusions. At last, we provided a replication package2 to
allow for the reproducibility of our results, and also for other
researchers to build on our study.

ACKNOWLEDGMENT

This work is partly funded by the Natural Sciences and En-
gineering Research Council of Canada (NSERC), PROMPT,
and Les Technologies MoovAI Inc.

1https://delvetool.com/

2https://github.com/poclecoqq/quality issues in MLSSs

REFERENCES

[1] D. Marijan, A. Gotlieb, and M. K. Ahuja, “Challenges of testing machine
learning based systems,” in 2019 IEEE International Conference On
Artiﬁcial Intelligence Testing (AITest), pp. 101–102, IEEE, 2019.
[2] H. Foidl and M. Felderer, “Risk-based data validation in machine
learning-based software systems,” in proceedings of
the 3rd ACM
SIGSOFT international workshop on machine learning techniques for
software quality evaluation, pp. 13–18, 2019.

[3] H. B. Braiek and F. Khomh, “On testing machine learning programs,”

Journal of Systems and Software, vol. 164, p. 110542, 2020.

[4] F. Khomh, B. Adams, J. Cheng, M. Fokaefs, and G. Antoniol, “Software
engineering for machine-learning applications: The road ahead,” IEEE
Software, vol. 35, no. 5, pp. 81–84, 2018.

[5] E. Breck, N. Polyzotis, S. Roy, S. Whang, and M. Zinkevich, “Data

validation for machine learning.,” in MLSys, 2019.

[6] S. Studer, T. B. Bui, C. Drescher, A. Hanuschkin, L. Winkler, S. Peters,
and K.-R. M¨uller, “Towards crisp-ml (q): a machine learning process
model with quality assurance methodology,” Machine Learning and
Knowledge Extraction, vol. 3, no. 2, pp. 392–413, 2021.

[7] E. Breck, S. Cai, E. Nielsen, M. Salib, and D. Sculley, “The ml test score:
A rubric for ml production readiness and technical debt reduction,” in
2017 IEEE International Conference on Big Data (Big Data), pp. 1123–
1132, IEEE, 2017.

[8] J. M. Zhang, M. Harman, L. Ma, and Y. Liu, “Machine learning test-
ing: Survey, landscapes and horizons,” IEEE Transactions on Software
Engineering, 2020.

[9] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,
V. Chaudhary, M. Young, J.-F. Crespo, and D. Dennison, “Hidden
technical debt in machine learning systems,” in Advances in Neural
Information Processing Systems (C. Cortes, N. Lawrence, D. Lee,
M. Sugiyama, and R. Garnett, eds.), vol. 28, Curran Associates, Inc.,
2015.

[10] H. Washizaki, H. Uchida, F. Khomh, and Y.-G. Gu´eh´eneuc, “Studying
software engineering patterns for designing machine learning systems,”
in 2019 10th International Workshop on Empirical Software Engineering
in Practice (IWESEP), pp. 49–495, IEEE, 2019.

[11] M. Alahdab and G. C¸ alıklı, “Empirical analysis of hidden technical debt
patterns in machine learning software,” in International Conference on
Product-Focused Software Process Improvement, pp. 195–202, Springer,
2019.

[12] J. Bogner, R. Verdecchia, and I. Gerostathopoulos, “Characterizing tech-
nical debt and antipatterns in ai-based systems: A systematic mapping
study,” in 2021 IEEE/ACM International Conference on Technical Debt
(TechDebt), pp. 64–73, IEEE, 2021.

[13] S. Amershi, A. Begel, C. Bird, R. DeLine, H. Gall, E. Kamar, N. Na-
gappan, B. Nushi, and T. Zimmermann, “Software engineering for
machine learning: A case study,” in 2019 IEEE/ACM 41st International
Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP), pp. 291–300, IEEE, 2019.

[14] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,
V. Chaudhary, and M. Young, “Machine learning: The high interest
credit card of technical debt,” 2014.

[15] B. van Oort, L. Cruz, M. Aniche, and A. van Deursen, “The prevalence
of code smells in machine learning projects,” in 2021 IEEE/ACM 1st
Workshop on AI Engineering-Software Engineering for AI (WAIN),
pp. 1–8, IEEE, 2021.

[16] Y. Tang, R. Khatchadourian, M. Bagherzadeh, R. Singh, A. Stewart,
and A. Raja, “An empirical study of refactorings and technical debt
in machine learning systems,” in 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE), pp. 238–250, IEEE, 2021.
[17] N. Nahar, S. Zhou, G. Lewis, and C. K¨astner, “Collaboration challenges
in building ml-enabled systems: Communication, documentation, engi-
neering, and process,” Organization, vol. 1, no. 2, p. 3, 2022.

[18] M. Dilhara, A. Ketkar, and D. Dig, “Understanding software-2.0: A
study of machine learning library usage and evolution,” ACM Trans-
actions on Software Engineering and Methodology (TOSEM), vol. 30,
no. 4, pp. 1–42, 2021.

[19] N. Hynes, D. Sculley, and M. Terry, “The data linter: Lightweight,
automated sanity checking for ml data sets,” in NIPS MLSys Workshop,
vol. 1, 2017.

[20] S. E. Whang, Y. Roh, H. Song, and J.-G. Lee, “Data collection and
quality challenges in deep learning: A data-centric ai perspective,” arXiv
preprint arXiv:2112.06409, 2021.

[21] D. Sato, A. Wider, and C. Windheuser, “Continuous delivery for machine

learning,” 2019.

[22] S. Azimi and C. Pahl, “Root cause analysis and remediation for quality
and value improvement in machine learning driven information models.,”
in ICEIS (1), pp. 656–665, 2020.

[23] O. Blais, “Validate and monitor your machine learning models,” 2020.
[24] N. Humbatova, G. Jahangirova, G. Bavota, V. Riccio, A. Stocco,
and P. Tonella, “Taxonomy of real faults in deep learning systems,”
in Proceedings of the ACM/IEEE 42nd International Conference on
Software Engineering, pp. 1110–1121, 2020.

[25] A. Serban and J. Visser, “An empirical study of software architecture

for machine learning,” arXiv preprint arXiv:2105.12422, 2021.

[26] A. Nikanjam and F. Khomh, “Design smells in deep learning programs:
An empirical study,” in 2021 IEEE International Conference on Software
Maintenance and Evolution (ICSME), pp. 332–342, IEEE, 2021.
[27] K.-J. Stol, P. Ralph, and B. Fitzgerald, “Grounded theory in software
engineering research: a critical review and guidelines,” in Proceedings
of the 38th International Conference on Software Engineering, pp. 120–
131, 2016.

[28] A. Strauss, J. Corbin, and J. Corbin, Basics of Qualitative Research:
Techniques and Procedures for Developing Grounded Theory. SAGE
Publications, 1998.

[29] S. Hove and B. Anda, “Experiences from conducting semi-structured
interviews in empirical software engineering research,” in 11th IEEE
International Software Metrics Symposium (METRICS’05), pp. 10 pp.–
23, 2005.

[30] C. Cappi, C. Chapdelaine, L. Gardes, E. Jenn, B. Lefevre, S. Picard,
and T. Soumarmon, “Dataset deﬁnition standard (dds),” arXiv preprint
arXiv:2101.03020, 2021.

[31] A. Joshi, S. Kale, S. Chandel, and D. K. Pal, “Likert scale: Explored
and explained,” British journal of applied science & technology, vol. 7,
no. 4, p. 396, 2015.

[32] A. Strauss and J. Corbin, “Grounded theory methodology: An

overview.,” 1994.

[33] C. B. Seaman, “Qualitative methods in empirical studies of software
engineering,” IEEE Transactions on software engineering, vol. 25, no. 4,
pp. 557–572, 1999.

[34] R. Feldt and A. Magazinius, “Validity threats in empirical software

engineering research-an initial survey.,” in Seke, pp. 374–379, 2010.

