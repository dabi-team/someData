2
2
0
2

l
u
J

2
2

]

G
L
.
s
c
[

1
v
0
8
2
1
1
.
7
0
2
2
:
v
i
X
r
a

PANGU-CODER: Program Synthesis with
Function-Level Language Modeling

TECHNICAL REPORT

Fenia Christopoulou1∗
Yinpeng Guo1∗
Hao Yu2 Li Yan2 Pingyi Zhou1 Xin Wang1 Yuchi Ma2†

Gerasimos Lampouras1∗

Zhongqi Li2∗

Qi Zhang2∗

Milan Gritta1∗

Meng Xiao1 Bo Shen2 Lin Li2
Ignacio Iacobacci1†

Guchun Zhang1∗

Yasheng Wang1†

Guangtai Liang2
Qianxiang Wang2 Qun Liu1

Jiansheng Wei1 Xin Jiang1

1Huawei Noah’s Ark Lab
2Huawei Cloud

Abstract

We present PANGU-CODER, a pretrained decoder-only language model adopting
the PANGU-α architecture for text-to-code generation, i.e. the synthesis of pro-
gramming language solutions given a natural language problem description. We
train PANGU-CODER using a two-stage strategy: the ﬁrst stage employs Causal
Language Modelling (CLM) to pre-train on raw programming language data, while
the second stage uses a combination of Causal Language Modelling and Masked
Language Modelling (MLM) training objectives that focus on the downstream task
of text-to-code generation and train on loosely curated pairs of natural language
program deﬁnitions and code functions. Finally, we discuss PANGU-CODER-FT,
which is ﬁne-tuned on a combination of competitive programming problems and
code with continuous integration tests. We evaluate PANGU-CODER with a fo-
cus on whether it generates functionally correct programs and demonstrate that
it achieves equivalent or better performance than similarly sized models, such as
CodeX [16], while attending a smaller context window and training on less data.

1

Introduction

Increasingly more large pre-trained language models [56, 23, 58, 32] based on the transformer [66]
architecture have been proposed and shown to achieve state-of-the-art results on a variety of Natural
Language Processing (NLP) tasks. Lately, such models have been adapted to more speciﬁc language
domains, e.g. to the Biomedical [37, 46, 9], Legal [14], Cyber Security [2], and Finance [6] domains,
while simultaneously expanding to include signals from modalities other than natural language, e.g.
Vision [65, 18, 15, 13, 64, 25], Proteins [11], Time Series [71, 72, 55] and Code [42, 70, 26, 31, 54].

In this work, we focus on pre-trained language models speciﬁcally created for text-to-code generation,
i.e. the task of program synthesis from Natural Language (NL) descriptions (e.g. problem deﬁnitions
or docstrings). While some of the proposed models for this task adapt the encoder-decoder architecture
[47], most are trained as decoder-only transformer models [16, 51, 27]. The encoder-decoder
architecture requires a clear distinction and association between input and output. In contrast, a single
component (encoder-only or decoder-only) architecture treats inputs as a continuous sequence, and as
such is well-suited for training over vast amounts of raw data. Regardless of architecture, most recent

∗Equal contributions
†Corresponding authors: {mayuchi1,ignacio.iacobacci,wangyasheng}@huawei.com

 
 
 
 
 
 
work is further trained on large amounts of code data retrieved from GitHub3, Stack Exchange4 and
other sources, with some being initialized from existing language models that were pre-trained on NL
exclusively (e.g. initialised from GPT [12] or BERT [23]).

We present PANGU-CODER, a pre-trained language model for text-to-code generation. PANGU-
CODER follows the PANGU-α architecture (see Figure 1) introduced by Zeng et al. [73], which
consists of a uni-directional decoder-only transformer with an extra query layer stacked on top. At
each time-step t, the query layer attends to the positional embedding t + 1 to determine the next token.
While PANGU-α was proposed to handle both English and Chinese NL text, PANGU-CODER is
currently focused on text-to-code generation from exclusively English prompts. PANGU-CODER only
supports Python outputs at the moment, but the model is easily extendable to other languages.

We train PANGU-CODER using a two-stage strategy, with the ﬁrst stage acting as pre-training on
unsupervised raw programming language data. Natural language is included during this stage in the
form of docstrings or inline comments, where available. To take full advantage of the raw data, we
follow the training regime of existing decoder-only models and employ regular Causal Language
Modeling (CLM) [56] while treating all data as a continuous sequence. The second stage of training
is designed to focus on the downstream task of text-to-code generation and take advantage of the
fact that it comprises distinct source and target sequences, i.e. the input NL problem deﬁnition and
output code. As such, the second stage is focused exclusively on aligned pairs of NL and code. We
experiment with various training objectives, including combinations of Causal and Masked Language
Modelling (MLM) objectives, drawing inspiration from the training regimes of encoder-decoder
architectures. Through this two-stage approach, our model is able to learn how general code structures
relate to natural language through raw data in the ﬁrst stage, and subsequently focus on how to best
generate the correct output code given the NL input during the second stage. Finally, we also
ﬁne-tune PANGU-CODER using data that is more closely related to the target domain (Section 4).
We distinguish our stage-2 training from ﬁne-tuning by the nature of the data used, i.e. extracted
from possibly unaligned or noisy online sources vs. data more suited for text-to-code generation, e.g.
retrieved from programming contests.

The remainder of this technical report is organized as follows. Section 2 deﬁnes the task of text-
to-code generation formally. In Section 3, we detail our training data, methodology and present
zero-shot results and analysis. Similarly, Section 4 presents our ﬁne-tuning methodology, data and
results. Finally, Section 5 discusses related work.

2 Task Deﬁnition

PROBLEM DESCRIPTION

Return a greatest common divisor of two integers a and b

FUNCTION SIGNATURE

def greatest_common_divisor(a: int, b: int) -> int:

UNIT TESTS

greatest_common_divisor(3, 5) = 1
greatest_common_divisor(25, 15) = 5

FUNCTION BODY

a, b = b, a % b

while b:

return a

Table 1: Examples taken from the HumanEval dataset.

The PANGU-CODER model we present in this report is speciﬁcally created for the task of text-to-code
generation, i.e. to synthesize running code that successfully solves the problem described by an NL
description. Table 1 shows an example from the HumanEval dataset [16], which is typical for the
task; the input consists of an NL description of the problem, followed by the function signature of the
solution (the function argument types and the expected return type may or may not be included). In
addition, the problems may be accompanied by some unit tests, in the form of function calls with

3github.com
4stackoverﬂow.com

2

Figure 1: Schematic of the PANGU-α architecture.

MODEL

PANGU-CODER 317 M
PANGU-CODER 2.6 B

# LAYERS HIDDEN SIZE

(L)

24
32

(d)

1,024
2,560

FFN size
(dff)

# HEADS
(Nh)

CONTEXT SIZE
(nCNTX)

VOCAB
(nVOCAB)

4,096
10,240

16
32

1,024
1,024

41,865
41,865

Table 2: PANGU-CODER model sizes and conﬁgurations.

speciﬁc inputs and the corresponding expected outputs, e.g. greatest_common_divisor(3, 5)
= 1. The model is then asked to produce the body of the code, and is evaluated against a number of
held-out unit tests to ensure that the problem is properly solved.

3 Training Methodology

As mentioned in the introduction, PANGU-CODER uses PANGU-α [73] as its underlying architecture.
PANGU-α was ﬁrst developed to investigate the effect of large-scale pre-trained language models on
Chinese NLP tasks. Its architecture (see Figure 1) was designed for scaling to hundreds of billions of
parameters, and was implemented in MindSpore Auto-parallel5 to allow for training parallelization
across a cluster of 2,048 Ascend 910 AI processors6. The current version of PANGU-CODER model
is implemented in Pytorch [53] and its training was done on Nvidia V100 GPU cards.

Similarly to GPT, PANGU-α is a uni-directional autoregressive decoder-only transformer with an
additional attention layer on top, where an embedding pn ∈ Rd indicating the next token position is
used as the query vector in the attention mechanism.

The attention weights in the extra layer are computed as follows:

h H (cid:62)
L ,
(1)
where W q
h ∈ Rd×d/Nh are projection matrices, and HL ∈ RV ×d corresponds to the token
representations obtained from the top transformer layer, with h representing the index of the attention
head, d being the hidden dimension, Nh the number of attention heads, and V the vocabulary size.

αh = pn W q

h , W k

h W k(cid:62)

For Chinese NLP tasks, multiple sized models were trained up to 200B parameters; for programming
language modeling, we test the following model conﬁgurations, as shown in Table 2. In the remainder
of this section, we discuss the data used for training and evaluation, and show how we use the PANGU-
α model, together with its accompanying tokenizer and vocabulary, to train it on code-speciﬁc data
using various strategies. The section concludes with an analysis of the effect of different decoding
strategies on the models’ zero-shot performance.

5https://www.mindspore.cn/en
6https://e.huawei.com/en/products/servers/ascend

3

Returna\n<eoc><descr>N-1210TokenPositionTransformer LayersQuery LayerN++++...Figure 2: Example of the Abstract Syntax Tree of an extracted function.

3.1 Data

Collection The initial dataset was collected through GHTorrent [29], an online tool that collects
and stores Github public event metadata. In an effort to make our pre-training data comparable to
previous work, we focused exclusively on Github repositories established before May 2021. The
download process occurred in between March 2022 and May 2022 using a 280 nodes Spark cluster.

For model pre-training, about 65 million Python ﬁles were extracted, totaling a size of 380 GB. All
ﬁles were processed on a Spark cluster, and a rowKey of MD5 (generated from each ﬁle’s content)
was used as the unique index of the NoSQL database to remove duplicate ﬁles. After this we ended
up with about 40 million ﬁles, and the total size was reduced to 185 GB. To establish data quality, we
only kept the collected ﬁles that met the following criteria:

a) the ﬁle size is under 1MB;
b) the code is Python3 compatible, as determined through its Abstract Syntactic Tree (AST);
c) there are fewer than 100 characters per line on average;
d) and there are fewer than 1,000 characters in any single line.

This process further reduced the number of data to about 147 GB in size.

Pre-processing To format the data for text-to-code generation, as described in Section 2, we need
to extract the problem descriptions, the corresponding function signatures and bodies. We apply
AST parsing on each of the remaining ﬁles and extract functions and corresponding docstrings7 (see
Figure 2). A docstring is a snippet of natural language text that immediately follows the deﬁnition
of a function or class, usually enclosed with the multi-line annotation in Python ("""), as shown
below. Exploiting the AST’s structure, the docstring node is located and the text is extracted. We then
collapse the AST structure, ignoring the docstring node, to re-obtain the function signature and body.
The beneﬁt of this process is that the code is well formatted with all meaningless empty lines and
spaces removed. Finally, we apply the de-duplication process to the extracted function bodies as well.

For training, we use these docstrings in lieu of NL problem descriptions, as we consider them to be
the closest readily available alternative.

def _forward(self, X):

"""
Perform one forward pass of the model and return the loss
"""
loss = self.model(X)
return loss

7While we also extract and train on classes, we will only refer to functions as the process is identical.

4

DocstringReturn ValueFunction BodyWe create our training samples using two data formats, depending on whether a corresponding
docstring is available for the relevant function body. When a docstring exists, our training samples
are constructed as <descr> docstring <python> code <eoc>:

<descr> Perform one forward pass of the model and return the loss <python>\n def
_forward(self, X):\n

loss = self.model(X)\n

return loss <eoc>

When there is no docstring available, we omit the <descr> segment and assemble the samples as
<python> code <eoc>:

<python>\n def _forward(self, X):\n

loss = self.model(X)\n

return loss <eoc>

Here, the <descr> symbol marks the beginning of the problem description, <python> indicates
the beginning of the code solution, while the <eoc> symbol (end-of-code) marks the end of the
docstring-code or code-only instance. During the ﬁrst stage of training, we use both data formats
(code-only and docstring-code), while only the docstring-code format is used for the second stage.

Evaluation In order to evaluate our models, we make use of two of the most commonly used
datasets for evaluating the functional correctness of generated programs, namely HumanEval [16] and
the Mostly Basic Programming Problems [7, MBPP] datasets. HumanEval8 contains 164 handcrafted
Python problems accompanied by a set of held-out unit tests (average of 7.7 unit tests per problem),
all of which must pass in order to count as a successful solution. We note that HumanEval may also
provide some unit tests as a part of the problem description, but they are distinct from the unit tests
used for evaluation. HumanEval is a good estimator of zero-shot model performance as its handcrafted
problem descriptions ensure little to no overlap with any pre-training data that are automatically
extracted from online resources. In a similar vein, MBPP is comprised of 974 programming problems
(474 train, 500 test) designed to be solved by entry-level Python programmers.

In order to estimate model performance, we sample n programs/solutions per
Evaluation Metrics
problem and calculate pass@k for k = [1, 10, 100]. To avoid issues of high variance, we use the
unbiased estimator of pass@k introduced by Chen et al. [16]. In all the experiments we use a sample
size n = 200, unless otherwise stated.

An issue we observed in preliminary evaluation arises from the fact that the pre-training and ﬁne-
tuning data are focused exclusively on function signatures, bodies and their preceding docstring.
As such, the code generated by our models often misses the required dependencies. To accurately
estimate performance, whenever there is a missing dependency, the solution is re-evaluated with the
missing library automatically imported before the function call, essentially simulating the auto-import
function of many integrated development environments (i.e. IDEs).

3.2 Training Stages and Objectives

We train PANGU-CODER using a 2-stage process, as detailed below. Formally, we denote the docstring
and code output as a single sequence of tokens X = XD + XC = {d1, ..., dND , c1, ..., cNC }, with
XD and XC the tokens corresponding to the docstring and code, respectively.

3.2.1 Stage-1 Training

During the ﬁrst stage of training, the PANGU-CODER model is initialised with random weights and
trained on Python code via Causal Language Modeling (CLM), also known as next token prediction,
using the formats introduced in Section 3.1.

LCLM(X) = LCLM(XD, XC) = −

1
N

N
(cid:88)

n=1

log p(xn|xi≤n−1; θ),

(2)

where N = ND + NC indicates the total number of tokens in the input sequence X (with ND,
NC being the number of docstring and code tokens, respectively) and θ corresponds to the model
parameters. In the following equations θ is omitted for brevity.

8https://github.com/openai/human-eval

5

(a) Stage-1 instance formation, where all available data are
concatenated and split at a given length.

(b) Stage-2 instance formation, with each
docstring-code pair fed separately to the model.

Figure 3: Input formats during stage-1 and stage-2 training.

In order to formulate the model input, we concatenate all EA training examples as a single sequence
(both code-only and docstring-code examples) and generate training instances by splitting the
concatenated sequence into IA ≤ |EA| chunks of 1, 024 subwords, including the inserted special
tokens, as shown in Figure 3a. The model is then trained for 188 billion tokens in total.

3.2.2 Stage-2 Training

For the second stage, we form the model inputs by exclusively considering |EB| ≤ |EA| docstring-
code examples and treating each as a single training instance IB = |EB|, as shown in Figure 3b.
To further reduce noise in the data, we removed edge cases where the docstring was shorter than
19 words, the function body longer than 400, or where their length ratio was greater than 32; these
values were empirically determined through observation of the curated datasets’ statistics.

We explore several objectives for stage-2 model training on Python code, each consisting of mixtures
of joint losses, focusing independently on the docstring and code subsequences. The combinations
are primarily motivated by the shift in focus to the downstream task of text-to-code generation during
stage-2. We present the individual losses below:

Code-CLM: Causal Language Modeling on Code This loss is computed by applying CLM
exclusively on the code subsequence, hence it is named CODE-CLM. Enforcing this objective during
pre-training brings us closer to the target objective of the downstream task.

LCODE-CLM(X) = −

1
NC

NC(cid:88)

n=1

log p(cn|ci≤n−1, d1, ..., dND ),

(3)

As shown in Equation (3), and depicted in Figure 4, each code token ci is predicted based on all
previous tokens, including the tokens of docstring d ∈ XD.

Figure 4: CODE-CLM: Causal Language Modeling over code-only tokens.

Docstr-MLM: Masked Language Modeling on Docstring Since the down-stream task is not
reliant on next word prediction for the docstring, this loss calculates standard Masked Language Mod-
eling (MLM) exclusively on the docstring (DOCSTR-MLM) as depicted in Figure 5a. Speciﬁcally, a
few random M < ND tokens in the docstring are replaced with a mask, a random token or the same
token with 0.8/0.1/0.1 chance, respectively, similar to Devlin et al. [23].

In contrast to models that perform MLM in a bidirectional fashion, using a single decoder network
(namely preﬁx LMs) [24, 8, 31], in our case we do not change the underlying attention mechanism of
the model. As a result, the masked docstring tokens are predicted by only attending to previous ones.

6

Docstr-Code 2Docstr-Code 2Code 4...Code |EA|-1Instance 1Instance 2Instance IADocstr-Code |EA|Code 3Docstr-Code 1Instance 1Instance 2Instance IBDocstr-Code 1...Docstr-Code 2Docstr-Code |EB|..._loss_pass_<python>\n__def__XPanGu-Coder...)(__def__X)<eoc>_one_forward<descr>_Perform...Finally, we need to underline that in this objective, we shift from next token prediction to same
(masked) token prediction solely on the docstring.

LDOCSTR-MLM(XD) = −

1
M

M
(cid:88)

m=1,m∈ND

log p(dm|di≤m)

(4)

Docstr-MCLM: Masked Causal Language Modeling on Docstring Similarly to the previous
one, in this loss we randomly mask tokens in the docstring, but instead of predicting the masked
tokens, we employ causal language modeling on the masked docstring. To make the task easier
and the training more efﬁcient, we calculate the docstring loss over a few randomly selected tokens
J < ND, with probability 0.15. We name this loss DOCSTR-MCLM as the input docstring includes
masked tokens while computing a causal language modeling loss on it. An example of the loss is
shown in Figure 5b.

LDOCSTR-MCLM(XD) = −

1
J

J
(cid:88)

j=1,j∈ND

log p(dj|di≤j−1)

(5)

where J corresponds to a number of random tokens in the docstring, selected for prediction and di is
the i-th docstring token.

(a) DOCSTR-MLM: Masked Language Modeling on
docstring.

(b) DOCSTR-MCLM: Masked Causal Language mod-
eling on docstring.

Figure 5: Masked Language Modeling modiﬁcations applied on the docstring.

Training objectives
In Table 3, we summarise the objectives and the corresponding joint losses
with which PANGU-CODER is trained. The ﬁrst objective uses exclusively the CODE-CLM loss and
no loss is calculated over the docstring. As PANGU-CODER is meant to generate only code, since
the docstring of the problem will be provided as input, it is preferable to prioritise the next token
prediction loss of the code subsequence over the docstring. In the next two objectives, we re-introduce
different computations of the loss on the docstring part of the sequence. Typically, in preﬁx LMs both
input segments (docstring and code) are masked but only tokens in the second segment (code) are
predicted. Here, instead, we apply masking and prediction on the ﬁrst segment, while on the second
we predict tokens one-by-one without any masking.

OBJECTIVE

LOSS

Code-CLM
Docstr-MCLM + Code-CLM L = LDOCSTR-MCLM(XD) + LCODE-CLM(X)
L = LDOCSTR-MLM(XD) + LCODE-CLM(X)
Docstr-MLM + Code-CLM

L = LCODE-CLM(X)

Table 3: Pre-training losses for different objectives.

3.3 Tokenization

We inherit the tokenization model and corresponding vocabulary from PANGU-α, which was built
using SentencePiece [44] and contains 70K subtokens. However, the original PANGU-α vocabulary
supports both Latin and Chinese characters; since there is no need for the latter in the applied
datasets, our model’s vocabulary consists only of 42K subtokens. After the input sequences are

7

_Perform_passPanGu-Coder...<descr>_loss<mask><mask>_<python>_one_forwardPanGu-Coder..._loss<mask><mask>_<python>_one_forward_pass_oneconstructed using the data formats described in Section 3.1, we apply the tokenization model to split
each sequence into subtokens as the sole pre-processing step before training or testing.

When the same tokenization strategy is used for the whole sequence, some embeddings may be
shared between the docstring and code subsequences. In Stage-1 training, the embeddings are shared
across docstring and code, but in Stage-2 training we experimented with both sharing and separating
the embeddings used in each subsequence.

3.4 Pre-training Results and Analysis

Training details We train models with a batch size of 256, using the Adam optimizer [43] with
β1 = 0.9, β2 = 0.95 and weight decay of 0.01. We employ a cosine decay scheduler with maximum
and minimum learning rates 1e-5 and 5e-6, respectively. The gradients are clipped at 3.0 during
stage-1 and 1.0 during stage-2. For stage-1 training, the model is initialised with random weights
and trained from scratch with a warmup ratio of 1% for 188 billion tokens in total. For stage-2, we
initialise the model from a checkpoint obtained during stage-1, reset the optimizer, keep the same
hyper-parameters and continue training for a maximum of 1M steps, corresponding to 42 billion
tokens.

Generation As our models are based on the PANGU-α architecture, we follow the standard
decoding / generation process used for auto-regressive language models. Generation adopts a prompt
that is similar to the data format used during the pre-training stages (c.f. Section 3.1), with only the
function signature provided and not the body, i.e. <descr> docstring <python> signature.
Due to the nature of our applied tasks, we assume that there is always a docstring available during
generation. We remove any superﬂuous whitespaces and line breaks from the problem descriptions.

<descr> Perform one forward pass of the model and return the loss <python>\n def
_forward(self, X):\n

Formally, the prompt is a sequence of tokens P = PD + PS = {d1, ..., dND , s1, ..., sNS } where
|ND|, |NS| are the number of tokens in the docstring and signature subsequences. The model then
generates a continuation C (cid:48) of the prompt in a left-to-right manner, decoding one token at a time
while attending on the previous tokens in the sequence. The generation continues until either a special
end token (i.e. the <eoc> token) or a maximum sequence length is reached.

C (cid:48)(xt) = PANGU-CODER (d1, ..., dND , s1, ..., sNS , c(cid:48)

0, ..., c(cid:48)

t−1).

(6)

As is standard in such models, the decoding can be performed through a number of stochastic
strategies. Here we use temperature scaling (t) and nucleus sampling (p) [36], and following previous
work, we report results from the best settings in each conﬁguration. We analyse the effect of different
decoding strategies and prompt variations in the later subsections.

3.4.1 Zero-shot Results

Tables 4 and 5 summarise our best results on the HumanEval and MBPP datasets respectively. For
the 317M parameter model, we use p = 0.4, t = 0.2 for pass@1, and p = 0.8, t = 0.95 for pass@10
and pass@100. For the 2.6B model, we use p = 0.4, t = 0.2 for pass@1, and p = 0.8, t = 0.65 for
pass@10 and pass@100.

Table 4 reports a comparison of existing models, as well as our proposed PANGU-CODER on the
HumanEval dataset, along with accompanied model and data sizes, the contextual window allowed
(cCNTX), vocabulary size (nVOCAB) and number of tokens the models were trained for.

For CODEX [16], the total data size and number of trained tokens is calculated by considering the
initial training of GPT-3 [12] for 300 billion tokens on a collection of data equivalent to 570GB of
compressed plain text. Similarly, for CODEGEN [51], the dataset size was computed by accumulat-
ing The Pile [28] and BigQuery10 for CODEGEN-MULTI and combining The Pile, BigQuery and
BigPython [51] for CODEGEN-MONO, as models were trained sequentially on the three datasets.
In order to calculate the number of training tokens for the CODEGEN models we assume the batch
size reported in the paper corresponds to tokens instead of instances. Information about GPT-NEO

10https://cloud.google.com/bigquery/public-data/

8

MODEL

SIZE

nCNTX

nVOCAB

DATA
(GB)

TRAIN
TOKENS

HUMANEVAL (%)
PASS@10

PASS@100

PASS@1

GPT-NEO [10]

125 M 2,048

50 K

825

300 M 4,096
CODEX [16]
ALPHACODE [47]
302 M 2,304
CODEGEN MULTI [51] 350 M 2,048
CODEGEN MONO [51] 350 M 2,048
PANGU-CODER
317 M 1,024

CODEX
ALPHACODE

ALPHACODE
GPT-NEO

CODEX
PANGU-CODER
CODEGEN MULTI
CODEGEN MONO
GPT-NEO

GPT-J [67]
CODEGEN MULTI
CODEGEN MONO
INCODER [27]

679 M 4,096
685 M 2,304

1.1 B 2,304
1.3 B 2,048

2.5 B 4,096
2.6 B 1,024
2.7 B 2,048
2.7 B 2,048
2.7 B 2,048

6 B 2,048
6.1 B 2,048
6.1 B 2,048
6.7 B 2,048

50 K
8 K

729
715
50 K 1,595
50 K 1,812
147
42 K

50 K
8 K

8 K
50 K

729
715

715
825

729
50 K
42 K
147
50 K 1,595
50 K 1,812
825
50 K

50 K
825
50 K 1,595
50 K 1,812
216

27.6 K

300 B

400 B
-
250 B
325 B
211 B

400 B
-

-
380 B

400 B
387 B
500 B
650 B
420 B

402 B
1 T
1.3 T
52 B

0.75

13.17
11.60
6.67
12.76
17.07

16.22
14.20

17.10
4.79

21.36
23.78
14.51
23.70
6.41

11.62
18.20
26.13
15.20

1.88

20.37
18.80
10.61
23.11
24.05

25.70
24.40

28.20
7.47

35.42
35.36
24.67
36.64
11.27

15.74
28.70
42.29
27.80

2.97

36.27
31.80
16.84
35.19
34.55

40.95
38.80

45.30
16.30

59.50
51.24
38.56
57.01
21.37

27.74
44.90
65.82
47.00

Table 4: Pass@k rates on the HumanEval dataset, among various models. Sizes are reported in
thousands (K), millions (M), billions (B) and trillions (T).9

MODEL

SIZE

nCNTX

nVOCAB

DATA
(GB)

TRAIN
TOKENS

PASS@1

MBPP (%)
PASS@10

PASS@100

INCODER [27]

6.7 B 2,048

22.6 K

PANGU-CODER

317 M 1,024
2.6 B 1,024

42 K
42 K

216

147
147

52 B

211 B
387 B

19.40

16.20
23.00

-

34.39
43.60

-

53.74
59.64

Table 5: Pass@k rates on the MBPP dataset.

and GPT-J was obtained via the model cards available11. For INCODER, the vocabulary size was
calculated as 55% of GPT-2 vocabulary, based on Fried et al. [27]. For the rest of the models, explicit
information was provided in the corresponding papers. For all models, pass@k rates are computed
with 200 samples, except for ALPHACODE where the reported rates used 1, 000 samples.12

PANGU-CODER results in the best performance in the 300M family of models for pass@1 and
pass@10. For pass@100, PANGU-CODER performs lower than CODEGEN-MONO and CODEX, but
the latter has been trained on a 2x and 4x larger input context respectively, and for at least four times
more data and more tokens. Looking at the 2.6B models family, PANGU-CODER again achieves
the best pass@1 performance. On the other hand, PANGU-CODER underperforms compared to
CODEGEN-MONO and CODEX on pass@10 and pass@100, but similarly to the 300M family of
models, these two have been trained with a larger context, on more data, and for more tokens.

Regarding the MBPP dataset, most other models do not report zero-shot results on it, with only
INCODER reporting pass@1. PANGU-CODER 2.6B outperforms INCODER even though it is less
than half its size (2.6B vs 6.7B parameters). Furthermore, even though we are not able to make an
apples-to-apples comparison with the PANGU-CODER 317M model due to their size difference, it is
interesting to note that it is only 3.2 points below INCODER on pass@1.

11https://huggingface.co/EleutherAI
12We include the decoder-only baseline presented by ALPHACODE, and not the encoder-decoder model, as
HumanEval results are only reported on the former. The number of train tokens of this baseline are not reported.
12We did not include even larger scale models, since they would not be directly comparable with this work.

9

Overall we observe, and this is further conﬁrmed through analysis in later sections, that PANGU-
CODER performs better for lower k values. We speculate that the gap in performance for pass@100
is a result of the small context size PANGU-CODER has been trained on, that prevents the model from
learning to generate long solutions, or solutions that have to attend over quite long descriptions.

3.4.2 Impact of Different Decoding Strategies

Figure 6 shows a study on the effect of temperature scaling and nucleus sampling on the best
checkpoint of PANGU-CODER trained via the CODE-CLM training objective. This study is focused
on the HumanEval dataset and we gather n = 50 samples per problem to calculate pass@1 and
pass@10. We observe similar ﬁndings as observed by Chen et al. [16], with different decoding
strategies being better suited for different values of k in pass@k, e.g. higher precision (low p and
temperature) is required for low values of k while higher recall (high p and temperature) improves
larger k pass ratios.

An interesting observation is that the hyper-parameter values that are optimal for pass@1 performance
are essentially equivalent to performing greedy decoding. As such, we use greedy decoding to evaluate
performance of various checkpoints over training as it is much more efﬁcient.

(a) PASS@1

(b) PASS@10

Figure 6: Effect of temperature scaling and nucleus sampling on pass@1 and pass@10 on HumanEval.

3.4.3 Impact of Different Prompting Strategies

As shown in Table 1, the HumanEval dataset problems also provide some input unit tests, and the
provided function signatures may contain argument and return value typing. Speciﬁcally, unit tests are
available for 136 of 164 problems, while function type declarations are present in 30 of 164 problems.
We note that we are exclusively examining the unit tests that are available as part of HumanEval’s
prompt and not the held-out unit tests used for evaluation of the generated solutions. By default, the
dataset prompt is formed to incorporate this additional information as shown in the example below.

<descr> Return a greatest common divisor of two integers a and b. \n >>>
greatest_common_divisor(3, 5) \n 1 \n >>> greatest_common_divisor(25, 15) \n 5
<python>\n def greatest_common_divisor(a: int, b: int) -> int:\n

However, such information is usually absent in data extracted from online code repositories, such as
those PANGU-CODER is trained on. To bring the HumanEval prompts closer to the format the model
was exposed to during training, we explored some variations by removing the unit tests and/or typing.
Both removals are performed automatically via simple heuristics.

<descr> Return a greatest common divisor of two integers a and b. <python>\n def
greatest_common_divisor(a, b)\n

Table 6 shows pass@k results with different prompt variations on the best checkpoint of PANGU-
CODER trained via the CODE-CLM training objective. We observe some improvement on pass@10
performance when excluding both unit-tests and typing from the prompt, but no consistently signiﬁ-
cant differences altogether. Similar behavior is observed on our ﬁne-tuned models (Section 4.1).

10

1.21.110.90.80.70.60.50.40.30.20.1Temperature10.90.80.70.60.50.40.30.20.1Nucleus Sampling p3.685.307.138.9610.3811.3912.3813.3514.0115.2015.5115.787.098.709.8310.9412.2013.0213.7013.9114.4415.3915.4115.848.8210.3211.6812.3913.2313.6713.8414.1315.1315.1715.5616.2410.5411.6812.5513.2413.6114.0014.2914.8914.9115.6315.9016.4311.8812.8213.2013.7414.2814.2214.7415.0716.0916.2016.3916.6513.0413.7614.2614.4615.1815.6616.2816.5416.7217.0717.0717.0714.1714.6714.9615.3915.5716.4816.7016.8817.0717.0717.0717.0715.2215.6215.7116.2716.6816.8316.8417.0717.0717.0717.0717.0716.1616.3716.8316.8316.8317.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.07468101214161.21.110.90.80.70.60.50.40.30.20.1Temperature10.90.80.70.60.50.40.30.20.1Nucleus Sampling p13.3415.9519.5820.9921.6222.0522.6122.9522.6622.2020.9718.2217.7421.1121.9921.8522.6122.5523.1922.4822.1721.2319.6217.6820.0422.0922.5222.0022.7122.9222.8421.3021.6119.6718.1417.6821.7122.7622.5122.7722.2921.9521.7921.3819.6818.7517.6817.6822.3722.9823.0422.4022.0920.9420.3418.8118.8217.6817.6817.6822.5522.4222.0321.2020.8419.9718.8718.8617.6817.0717.0717.0722.1221.5920.3119.4318.8718.2717.6817.0717.0717.0717.0717.0720.3219.4418.2618.2717.6817.0717.0717.0717.0717.0717.0717.0718.2617.6817.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.0717.071416182022Unit-Tests Typing

PASS@1

PASS@10

PASS@100

(cid:51)
(cid:51)
(cid:55)
(cid:55)

(cid:51)
(cid:55)
(cid:51)
(cid:55)

17.07
15.24
17.07
16.46

23.02
23.05
24.01
24.05

34.36
34.21
33.29
33.08

Table 6: Effect of including unit tests and typing in the prompt of the HumanEval dataset.

3.4.4 Impact of Pre-training Strategies

We also test the effect that each pre-training objective has on the model performance for the Hu-
manEval dataset in Table 7.

OBJECTIVE

PASS@1

PASS@10

PASS@100

Stage-1 CLM

Stage-2

CODE-CLM
DOCSTR-MLM + CODE-CLM
DOCSTR-MCLM + CODE-CLM

10.37

17.07
12.80
12.19

13.95

24.05
15.27
16.48

25.32

34.55
24.16
24.92

Table 7: Comparison of different training strategies on the HumanEval dataset for the 317M model.

We observe that objectives that calculate a loss on the docstring (i.e., DOCSTR-MCLM and DOCSTR-
MLM) perform poorly compared to training the model with loss only on the code-tokens (CODE-
CLM). We attribute this phenomenon to the fact that in these objectives, the docstring token em-
beddings receive a joint update via backpropagation combining the loss on the code tokens and the
loss on the docstring tokens. As a result, the models try to optimize at the same time two conﬂicting
objectives, i.e. updates that lead to better docstring generation/understanding and updates leading to
better code generation. On the contrary, when there is no loss computed over the docstring, the token
embeddings are updated solely based on what will result in better code generation and consequently
give higher performance. Compared with stage-1, all objectives manage to improve during stage-2
across pass@1 and pass@10, while those using docstring-losses seem to hurt pass@100. Generally,
we observe that all objectives lead to higher gains for lower k values of pass@k. This implies that
the objectives overﬁt on single correct solutions per problem, leading to a prioritization of precision
over recall.

(a) CODE-CLM

(b) DOCSTR-MLM

(c) DOCSTR-MCLM

Figure 7: Validation set Docstring and Code causal losses when employing different objectives on
the 317M model.

To further analyse the impact of the objectives on the two subsequences, we plot the causal loss
(i.e., next token prediction) on the docstring (docstring loss) and code tokens (code loss) separately
for each of these objectives as shown in Figure 7. We opt to examine the CLM loss rather than
any alternatives, to determine whether there is any catastrophic forgetting happening as a result of
abandoning the CLM loss on docstring as it was used in stage-1.

The CODE-CLM objective achieves lower code loss compared to the other objectives (0.46 versus
0.53), further supporting its improvements. Interestingly, the code loss on the other objectives
also decreases but the conﬂicting objectives do not allow it to achieve CODE-CLM performance.
Regarding docstring loss, we observe that for CODE-CLM and DOCSTR-MLM the loss keeps

11

0K200K400K600K10.811.011.211.411.6Docstring Loss0.460.480.500.520.54Code Loss0K200K400K600K1112131415Docstring Loss0.530.540.550.560.57Code Loss0K200K400K600K3.84.04.24.44.64.8Docstring Loss0.530.540.550.560.57Code Lossincreasing over time, while for the DOCSTR-MCLM objective, both losses decrease over the course
of training. This makes sense, intuitively, as both CODE-CLM and DOCSTR-MLM abandon CLM
on docstring. For DOCSTR-MCLM, even though the loss on the docstring is decreased, this does
not lead to lower code loss and consequently fails to increase performance, indicating an interesting
direction of how we can best simultaneously encode natural language and programming language.

3.4.5 Impact of Shared Embeddings

We check the impact of having separate embeddings between the code and the docstring on our
best objective CODE-CLM, as discussed in Section 3.3. Table 8 illustrates an increasing drop in
performance for different values of k, with −1.2 percentage points on pass@1, −2 on pass@10 and
−2.7 on pass@100, when the token embeddings between code and docstring are shared. This ﬁnding
indicates that keeping separate embeddings between the two inputs can be beneﬁcial for the task.

DOCSTR-CODE EMBEDDINGS

PASS@1

PASS@10

PASS@100

separate
shared

17.07
15.85

24.05
22.01

34.55
31.82

Table 8: Comparison between different tokenizations between docstring and code for the CODE-CLM
objective on the HumanEval dataset for the 317M model.

Enabling shared embeddings enforces an explicit connection between docstring and code tokens in
terms of their semantics. Although this can be true for some natural language-like programming
languages, as in our case Python, keeping separate embeddings can assist learning of docstring token
representations for better code generation.

4 Fine-Tuning Methodology

In this section, we evaluate a version of PANGU-CODER that was ﬁne-tuned on a combination of
competitive programming problems and code from continuous integration tests. The model, called
PANGU-CODER-FT, allows us to estimate the improvement in problem solve rates given additional
data from a more similar domain / distribution, which we describe in the following paragraphs.

Competitive Programming Data The following two datasets provide dozens of successful code
completions for each problem. Therefore, we can upsample each dataset by pairing up to ﬁve
solutions with each problem description to avail the model of different correct solutions for the
same problem speciﬁcation. APPS [34] was proposed to benchmark the programming capability
of code-generating models. It includes 10,000 programming tasks (5,000 train, 5,000 test) that
require code generation / completion given a detailed problem description13. Program arguments are
provided from standard input as well as function calls, sometimes embedded inside the class. We use
the train and test set programs, which we ﬁlter for the maximum input length of the model (1,024),
retaining 43K examples. Code Contests (CC) is a similar dataset [47] for training and evaluation of
competition-level code generation14, containing over 13K programming problems. After upsampling
and length ﬁltering, we retain 18K instances for ﬁne-tuning.

Continuous Integration Data
In addition to the competitive programming data, we also gathered
a dataset consisting of correctly-implemented functions from open source projects. Following recent
work on evaluating language models trained on code [16], we considered public Python repositories
on GitHub using Travis15 or Tox16 as their continuous integration (CI) system(s). First, we reproduced
the CI environment of each project locally in docker instances, then injected a tracing function in the
environment, which would capture the input, output, invocation location and context code for each
function invoked during integration testing. After that, the CI script was triggered and the data of
invoked functions were recorded into a database. We further clustered the collected data into 4 groups,

13https://github.com/hendrycks/apps
14https://github.com/deepmind/code_contests
15https://www.travis-ci.com/
16https://tox.wiki/en/latest/

12

according to the external contextual information (dependencies) that a function requires to run. That
is, functions depending on Python-builtin objects, Python-standard libraries, Pypi-public libraries,
and the residing class. We expect this variety could enable the generation of different function types,
e.g., self-contained, module, member or class functions. After ﬁltering for maximum model input
length, 49K examples are retained for ﬁne-tuning.

Training Details We ﬁne-tune PANGU-CODER-FT for 5 epochs on the aforementioned data, using
the CODE-CLM objective. We decrease the batch size to 32 and use a linear decay learning rate
scheduler. All other model training and program generation settings follow the Stage-2 protocol
(Section 3.4).

4.1 Fine-tuning Results

PANGU-CODER-FT clearly beneﬁts from additional ﬁne-tuning on data that is closer to the target
distribution, i.e. short programming tasks solving competitive and technical interview questions
(Table 9). The pass rate improvement is more pronounced for MBPP, which may be due to the lower
difﬁculty of problems (relative to HumanEval). Recall that the B in MBPP stands for ’basic’. In
the following paragraphs, we discuss the importance of appropriate ﬁne-tuning data and provide
additional methods for boosting the pass rates of solutions generated by PANGU-CODER(-FT).

4.1.1 Impact of In-domain Data

MBPP and HumanEval provide a challenging zero-shot evaluation of models designed to generate
functionally correct programs. MBPP17 additionally provides a small training/prompting set of
474 instances, which we can use to contrast the rate of improvement between out-of-domain and
in-domain data. Fine-tuning PANGU-CODER on just a few hundred relevant problems increases
pass@k almost as much as tens of thousands of correctly implemented but mostly out-of-domain
examples (see Table 9). Models therefore need to acquire a wide spectrum of problem solving
and programming knowledge to perform well on these datasets. Note that despite their superﬁcial
similarity, MBPP ﬁne-tuning provides no beneﬁt to HumanEval (Table 9).

MODELS

PASS@1

MBPP (%)
PASS@10

PASS@100

PASS@1

HUMANEVAL (%)
PASS@10

PASS@100

PANGU-CODER
PANGU-CODER-FT
PANGU-CODER-MBPP

16.20
24.60
25.40

34.39
44.19
43.32

53.74
63.07
60.03

17.07
19.50
15.24

24.05
25.96
22.73

34.55
40.80
32.65

Table 9: PANGU-CODER-FT (ﬁne-tuned with competitive and continuous integration data), PANGU-
CODER-MBPP (ﬁne-tuned on MBPP train set only), PANGU-CODER(trained on open-source data).

4.1.2 Impact of Unit Tests

HumanEval provides input/output pairs in the problem description that can be used to verify the
correctness of a solution, e.g. "remove_letters(PHP, P) == H". MBPP problem descriptions
can be augmented with test cases provided for evaluation in order to investigate whether the model
beneﬁts from the additional insights. Table 10 shows the pass rates for models trained/ﬁne-tuned
(TRAIN) with and without unit tests as well as programs generated (SAMPLE) with and without
asserts. We observe that for MBPP, the pass rates substantially increase with unit tests in the prompt,
particularly when this is provided during sampling as well. This is not the case for HumanEval,
however, possibly because the problem descriptions are longer, more detailed and less formulaic.
Similar observations were made during the zero-shot analysis in Section 3.4.3. A more effective way
of improving HumanEval scores is to use the same unit tests to ﬁlter out dysfunctional programs with
our proposed methods, outlined in section 4.1.3.

4.1.3 Filtering Generated Programs

In order to continue to improve PANGU-CODER’s pass rates, we can follow the program generation
stage with an additional post-processing step, e.g. ﬁltering on unit tests, declared function types and

17https://github.com/google-research/google-research/tree/master/mbpp

13

MODEL

TRAIN

SAMPLE

PASS@1

MBPP (%)
PASS@10

PASS@100

HUMANEVAL (%)
PASS@10

PASS@100

PASS@1

PANGU-CODER
PANGU-CODER

+ MBPP-train
+ MBPP-train
+ MBPP-train
+ MBPP-train

N/A
N/A
(cid:55)
(cid:55)
(cid:51)
(cid:51)

(cid:51)
(cid:55)

(cid:55)
(cid:51)
(cid:55)
(cid:51)

16.20
17.40

20.00
23.60
21.00
25.40

34.39
34.29

37.70
40.91
36.44
43.32

53.74
50.91

53.04
58.91
51.92
60.03

17.07
16.46

15.85
13.41
16.46
15.24

24.05
23.38

24.11
23.52
23.79
22.73

34.55
33.58

33.66
33.44
31.50
32.65

Table 10: PANGU-CODER trained and/or solutions generated with and without unit tests.

checking for syntactic correctness. The solutions (n = 200) to each HumanEval problem are ﬁltered
before being evaluated on the held-out unit tests. Not all problems have embedded/parsable unit tests
or have declared all function arguments and return types. Therefore, we evaluate the effectiveness
of our ﬁltering methods on problem subsets that have the speciﬁc ﬁlter available, i.e. 136 of 164
with unit tests and 30 of 164 with full type declarations (all 164 problems can be checked for syntax
errors). This gives us a baseline score when no ﬁltering is applied. The number of problems with
each ﬁlter and the corresponding pass rates are shown in Table 11.

FILTERING METHOD

PANGU-CODER

PASS@1

PASS@10

PANGU-CODER-FT
PASS10
PASS@1

UNIT TESTING

TYPING

INVALID SYNTAX

BASE
FILTER

BASE
FILTER

BASE
FILTER

14.53
35.48

25.16
26.99

12.05
12.06

-
-

50.65
52.00

23.27
23.27

16.67
41.52

30.45
31.30

13.85
13.86

-
-

54.61
55.72

25.40
25.44

Table 11: HumanEval pass@k with/out ﬁltering (for each problem subset). Unit tests are available
for 136/164 problems, argument/return types for 30/164 problems and syntax check for all problems.

Unit Tests
In many real-world scenarios, the description of a problem can include example function
input(s) and output(s), which we can use to ﬁlter out unsuccessful solutions, potentially increasing the
overall pass rate. In addition to the held-out tests, HumanEval typically provides unit tests embedded
in the description, expressed in a natural way and usually following a simple pattern. A basic regular
expression (to avoid overﬁtting) is used to extract the majority (136/164) of the unit tests. Table 11
shows a signiﬁcantly improved pass rate after this ﬁltering step. Only the pass@1 score is available
with this ﬁlter, as many problems fail to reach at least 10 solutions per problem, once ﬁltered. The
mean/median number of solutions after ﬁltering is 115/155, an average reduction of around 40-45%.

Typing In other scenarios, we may be provided with function argument types and the expected
return type. In such cases, we can generate plausible function inputs, execute the sampled program
and evaluate the type of the return value. If the program returns the expected type, it will be included
for the ﬁnal pass@k evaluation. As the generated programs are executed, we implicitly verify the
syntactic and semantic correctness, too. Relying only on checking the return type will inevitably
lead to false positives as programs often return the correct type, e.g. a ﬂoat or a list of ﬂoats but with
the wrong value(s). However, even this simple ﬁlter can improve the ﬁnal pass rates (Table 11) and
reduce the number of programs by around 25% (mean/median 150/152 after ﬁltering).

Invalid Syntax In the absence of unit tests and input/output type declarations retrieved from the
description, ﬁltering out syntactically invalid solutions can be a fast and simple method for eliminating
incorrect problem solutions. With that in mind, Table 11 shows that PANGU-CODER is generating
syntactically well-formed solutions almost all the time hence we ﬁlter < 1 out of 200 problem
solutions (mean/median after ﬁltering 199/200), leaving the pass rates unchanged.

14

(a) PASS@1

(b) PASS@10

(c) PASS@100

Figure 8: The pass rates as we increase the ﬁne-tuning data. Each time, 10K nearest examples are
added. The red line shows the pass@k when PANGU-CODER is ﬁne-tuned with all 120K examples.

4.2 Data Selection with Few-Shot Similarity

Transformer-based pretrained language models continue to rapidly increase in size [70, 47, 19] hence
it is desirable to reduce the compute requirements while maintaining good performance, whenever
possible. To this end, we simulate a scenario, in which we possess a small sample of programming
problems (possibly written from scratch) and a much larger dataset of heterogeneous training data.
We then experiment with choosing training examples from the much larger dataset based on their
similarity to a centroid embedding that represents our ’few-shot’ sample. We randomly select
10 problems (description and code) from HumanEval and 10 from MBPP (to avoid overﬁtting to
HumanEval) as our sample, denoted P = [p0, p1, .., p20] in Equation 7. The centroid embedding pemb
is obtained using a version of CodeBERT [26], which has been further trained on CodeSearchNet
[38] using the Replaced Token Detection loss [20]. The model is denoted by f in Equations 7 and 8.

pemb =

1
|P |

P
(cid:88)

i=1

f (pi)

distance(xi) = M SE (f (xi), pemb)

(7)

(8)

A training example xi is ranked/chosen by its Mean Squared Error (MSE) distance (lowest ﬁrst)
from pemb. PANGU-CODER is then ﬁne-tuned for ﬁve epochs with training data incremented by 10K,
up to 60K examples, which is 50% of the total data. The pass@k are shown in Figure 8.

Perhaps unsurprisingly, the competitive programming share of the ﬁne-tuning data increases from
46.4% (in the full 120K dataset) to 68.7% after our similarity-based selection. We observe that for all
pass@k, the full (dataset) score is reached or exceeded by ﬁne-tuning on 25% - 50% of the larger
dataset (see Figure 8). Using this method, we are able to reduce model compute requirements while
maintaining (even surpassing) the original pass rate. If we wish to ﬁne-tune PANGU-CODER on some
target data distribution in the future, we require a small “few-shot” sample of problems (possibly
written from scratch) to effectively subsample our larger training dataset to achieve a good pass rate
while reducing computational resources.

5 Related Work

Recently, there has been an increasing interest in applying deep learning methods for NLP to code
understanding and generation tasks. Among others, this is evidenced by the introduction of new NLP
venues, e.g. Natural Language Processing Advancements for Software Engineering (NLPaSE) [1],
which had its second installment at APSEC 2021 and the 2nd International Workshop on Software
Engineering Automation: A Natural Language Perspective (NLP-SEA 2021) [5] at ASE 2021. Other
new venues include the 1st Workshop on Natural Language Processing for Programming (NLP4Prog
2021) at ACL 2021 [45], the Deep Learning for Code (DL4C) Workshop at ICLR 2022 [62] and
the 1st International Workshop on Natural Language-based Software Engineering (NLBSE 2022),
co-located with ICSE 2022 [63].

5.1 Pre-trained Language Models for Programming Language

From an NLP research perspective, the recent advancements in code understanding and generation
have been mostly focused on revisiting proven effective natural language understanding (NLU)

15

10K20K30K40K50K60KTrain Data Size15.015.516.016.5Pass Rate10K20K30K40K50K60KTrain Data Size24.525.025.526.0Pass Rate10K20K30K40K50K60KTrain Data Size37383940Pass Rateand generation (NLG) methods. CodeBERT [26], for instance, was trained using a combination of
Masked Language Modeling inspired by [23, BERT] and Replaced Token Detection [20, ELECTRA].
CodeT5 [70] and PYMT5 [22] were built on top of [58, T5] while UniXcoder [31] was based on
UniLM [24]. The most notable works in the area are discussed next. We distinguish between models
focused on code understanding, where the goal is to learn contextual representations of source code,
and code generation, where the aim is to translate between different programming and / or natural
languages as well as to perform code completion and / or code repair.

Code Understanding Research on this topic started with the seminal work of Alon et al. [4] who
introduced code2vec[4], a neural model for representing snippets of code as continuous distributed
vectors, extending the idea from Mikolov et al. [49, word2vec]. Inspired by the introduction of
BERT [23], Kanade et al. [42] proposed CuBERT (Code Understanding BERT), a natural adaptation
of the model trained on source code. CodeBERT [26] instead, used both source code and Natural
Language (NL) with a discriminative objective, inspired by [21, ELECTRA]. The model receives a
masked NL-code pair and the generator then predicts the masked tokens (the prediction might be
different from the original token). Subsequently, CodeBERT is trained to predict which tokens were
replaced by the generator. Guo et al. [30] noted that previous pretrained models treat code snippets as
sequences of tokens while ignoring the inherent structure of code. They presented GraphCodeBERT,
which showed that incorporating the data ﬂow, i.e. a semantic-level structure of code extracted from
the abstract syntax tree (AST), leads to richer code representations. Jiang et al. [41, TREEBert]
instead used the actual AST together with code snippets.

SynCoBERT [68] trained the model with natural language, code and ASTs, but updated the training
objective to include identiﬁer prediction and AST Edge Prediction. In addition, Contrastive Learn-
ing [33, 17] has been used as a training regime to help associate snippets of code with syntactically
diverse but equivalent programs as well as with corresponding natural language descriptions [40, 50,
ContraCode] or with different modalities [69, Code-MVP].

Code Generation Earlier approaches for code generation were mostly limited to simpliﬁed ver-
sions of code completion or code translation tasks [35, 59]. However, recent work has been proposed
to tackle more complex problems, e.g. text-to-code generation (program synthesis) and code sum-
marization. Lu et al. [48] introduced CodeGPT, an adaptation of GPT [57] applied to code, as a
baseline for the CodeXGLUE benchmark. TransCoder [61] presented a pre-trained language model
speciﬁcally focused on unsupervised translation of code between different programming languages.
PLBART [3] employed a denoising objective over code and natural language via token masking,
token deletion, and token inﬁlling as noising strategies. CoText [54] was build on top of T5 with a
special focus on multi-task learning over multiple programming languages. Another model variant
was introduced by Fried et al. [27, InCoder]. Instead of training to perform code/text generation
in a single left-to-right pass, InCoder is also able to edit existing/partial programs via an inﬁlling
training objective. The training regime involved randomly replacing spans of code/comments with a
placeholder and asking the model to generate the replaced lines.

More recently, a number of large pretrained language models have been proposed, primarily focused
on the task of text-to-functional-code generation. Chen et al. [16] introduced CodeX [16], a set of
GPT-based language models trained on publicly available code from GitHub, up to 12B parameters in
size. In addition, the authors ﬁne-tuned their models using a set of training problems from competitive
programming websites and repositories with continuous integration for better program synthesis,
called Codex-S. Codex-D was ﬁne-tuned on the reverse task of generating the program description
given a particular function/code.

Li et al. [47] introduced AlphaCode, a set of sequence-to-sequence models with up to 41B parameters,
trained on data from programming competitions, e.g. Codeforces18 (similar to Codex-S) as well
as GitHub code in several programming languages. AlphaCode produces problem solutions by
overgeneration-and-ranking, i.e. the model samples multiple solutions and uses ﬁltering and clustering
to determine the best ones. Finally, CodeGen [51] was proposed as a conversational text-to-code
approach using large language models with sizes of up to 16B parameters. CodeGen-NL was trained
on The Pile [28], which contains around 6GB of Python code. CodeGen-Multi was then further
trained on BigQuery, which includes data from 6 different programming languages (C, C++, Go,

18https://codeforces.com/

16

Java, JavaScript and Python). A third model, CodeGen-Mono, builds on top of CodeGen-Multi and
was additionally trained on Python-only data.

5.2 Code Datasets and Evaluation

There is a number of datasets proposed for code understanding and generation tasks, many of which
have recently been included in the CodeXGLUE benchmark [48]. CodeXGLUE comprises 14
datasets across 11 tasks, including clone detection, defect detection, cloze test and code search
for understanding as well as code completion, code translation, code summarization, code repair,
code reﬁnement, document translation and text-to-code generation. Among these datasets are
CONCODE [39], a Java-based dataset with more than 100,000 examples of Java classes built
from public Github projects and containing environment information along with Javadocs and the
corresponding code. CodeSearchNet [38] is a benchmark speciﬁcally designed to evaluate systems
on semantic code search, i.e. a code retrieval task based on natural language queries.

The above tasks have mostly adopted NLP evaluation metrics, for example, text-to-code generation
still widely uses CodeBLUE [60] and exact match of the outputs. CodeBLUE is an extension of
the standard n-gram overlap metric BLEU [52], designed to improve the evaluation of generated
code. More recently, evaluation datasets and metrics started focusing on the functional correctness
of generated programs. Chen et al. [16, Codex] introduced the HumanEval dataset (c.f. Section3.1)
and Austin et al. [7] introduced the Mostly Basic Programming Problems [7, MBPP] dataset. Both
datasets are relatively small, comprising of 164 and 974 instances where descriptions of programming
problems and their corresponding coding solutions are included. These problems are then evaluated
in terms of their behavior, instead of being treated as natural language (either syntantically or
semantically). Speciﬁcally, the datasets provide unit tests that can be used to evaluate how close
outputs are to what is expected. The main difference between HumanEval and MBPP is that the former
does not include a training set as it was meant to be used for zero-shot evaluation. Other datasets,
such as APPS [34], containing 10,000 problems and Code Contests [47, CC] (13,610 problems) (c.f.
Section 4), leverage data from existing online coding platforms, resulting in a signiﬁcantly larger
dataset size but also more noise.

6 Conclusions

In this report, we presented PANGU-CODER, a pre-trained language model for the task of text-to-code
generation. PANGU-CODER is based on the Pangu-α architecture and was initially pre-trained on
raw natural language and programming data using the CLM objective, and subsequently trained
speciﬁcally on pairs of docstrings and functions using combinations of CODE-CLM, DOCSTR-
MLM, and DOCSTR-MCLM training objectives. Zero-shot evaluation of PANGU-CODER on the
HumanEval and MBPP datasets, designed to measure whether outputs comprise functionally correct
programs, shows that this training can help reach equivalent or better performance than similarly
sized models while using a smaller context window and less training data. Further analysis examined
the impact of various decoding and sampling strategies, training objectives, and embedding sharing.

PANGU-CODER-FT demonstrated that the scores of the base model can be improved at a faster rate by
curating data more closely related to our target task as the model is quite sensitive to mismatches and
shifts in ﬁne-tuning data distributions. If we possess a small set of example problems from the target
distribution, this sensitivity can be somewhat mitigated by choosing a subset of our ﬁne-tuning data
based on the similarity to a centroid embedding. We also evaluated various post-processing methods,
some of which allowed us to signiﬁcantly improve pass rates by ﬁltering out failing programs.

7 Acknowledgements

The authors thank Wei Zhang, Jun Yao, Qian Zhao, Feng Xu, Zongwei Tan for their great support to
this work. The authors also thank Philip John Gorinski for his feedback on an early version of this
report.

17

References

[1] NLPaSE 2021 Organization.

In 2021 28th Asia-Paciﬁc Software Engineering Conference
Workshops (APSEC Workshops), pages 13–13, 2021. doi: 10.1109/APSECW53869.2021.00008.

[2] Ehsan Aghaei, Xi Niu, Waseem Shadid, and Ehab Al-Shaer. Language model for text analytic

in cybersecurity, 2022. URL https://arxiv.org/abs/2204.02685.

[3] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Uniﬁed pre-training for
program understanding and generation. In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, pages 2655–2668, Online, June 2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.naacl-main.211. URL https://aclanthology.org/2021.naacl-main.
211.

[4] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. Code2vec: Learning distributed
representations of code. 3(POPL), jan 2019. doi: 10.1145/3290353. URL https://doi.org/
10.1145/3290353.

[5] Sajid Anwar, Mehrdad Saadatmand, Abdul Rauf, Muhammad Ramzan, and Imran Razzak. 2nd
International Workshop on Software Engineering Automation: A Natural Language Perspective
(NLP-SEA 2021) at ASE 2021. http://nlpsea.bsoft.pk/, 2022. [Online; accessed 9-June-
2022].

[6] Dogu Araci. Finbert: Financial sentiment analysis with pre-trained language models. CoRR,

abs/1908.10063, 2019. URL http://arxiv.org/abs/1908.10063.

[7] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David
Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large
language models. arXiv preprint arXiv:2108.07732, 2021.

[8] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jian-
feng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen Hon. UniLMv2: Pseudo-masked
language models for uniﬁed language model pre-training. In Hal Daumé III and Aarti Singh,
editors, Proceedings of the 37th International Conference on Machine Learning, volume 119
of Proceedings of Machine Learning Research, pages 642–652. PMLR, 13–18 Jul 2020. URL
https://proceedings.mlr.press/v119/bao20a.html.

[9] Iz Beltagy, Arman Cohan, and Kyle Lo. Scibert: Pretrained contextualized embeddings for
scientiﬁc text. CoRR, abs/1903.10676, 2019. URL http://arxiv.org/abs/1903.10676.

[10] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large
Scale Autoregressive Language Modeling with Mesh-Tensorﬂow, March 2021. URL https:
//doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these
metadata.

[11] Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial. ProteinBERT:
A universal deep-learning model of protein sequence and function. Bioinformatics, 38(8):
2102–2110, 2022.

[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020.

[13] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on
Computer Vision, pages 213–229. Springer, 2020.

[14] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion An-
droutsopoulos. LEGAL-BERT: The muppets straight out of law school. In Findings of the
Association for Computational Linguistics: EMNLP 2020, pages 2898–2904, Online, November
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.261.
URL https://aclanthology.org/2020.findings-emnlp.261.

18

[15] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya
Sutskever. Generative pretraining from pixels. In Hal Daumé III and Aarti Singh, editors,
Proceedings of the 37th International Conference on Machine Learning, volume 119 of Pro-
ceedings of Machine Learning Research, pages 1691–1703. PMLR, 13–18 Jul 2020. URL
https://proceedings.mlr.press/v119/chen20s.html.

[16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul
Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke
Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad
Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex
Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant
Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie
Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374,
2021. URL https://arxiv.org/abs/2107.03374.

[17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In Proceedings of the 37th International
Conference on Machine Learning, ICML’20. JMLR.org, 2020.

[18] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform
resnets without pre-training or strong data augmentations. In International Conference on Learn-
ing Representations, 2022. URL https://openreview.net/forum?id=LtKcMgGOeLt.

[19] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[20] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-
training text encoders as discriminators rather than generators. In International Conference on
Learning Representations, 2020. URL https://openreview.net/forum?id=r1xMH1BtvB.

[21] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA:
In ICLR, 2020. URL

Pre-training text encoders as discriminators rather than generators.
https://openreview.net/pdf?id=r1xMH1BtvB.

[22] Colin B Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan.
PyMT5: Multi-mode translation of natural language and python code with transformers. arXiv
preprint arXiv:2010.03150, 2020.

[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://aclanthology.org/N19-1423.

[24] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming
Zhou, and Hsiao-Wuen Hon. Uniﬁed Language Model Pre-Training for Natural Language
Understanding and Generation. Curran Associates Inc., Red Hook, NY, USA, 2019.

[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=YicbFdNTTy.

[26] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Lin-
jun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained

19

In Findings of the Association for Com-
model for programming and natural languages.
putational Linguistics: EMNLP 2020, pages 1536–1547, Online, November 2020. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.139. URL
https://aclanthology.org/2020.findings-emnlp.139.

[27] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,
Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code inﬁlling
and synthesis. arXiv preprint arXiv:2204.05999, 2022.

[28] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:
An 800gb dataset of diverse text for language modeling. CoRR, abs/2101.00027, 2021. URL
https://arxiv.org/abs/2101.00027.

[29] Georgios Gousios. The ghtorrent dataset and tool suite. In Proceedings of the 10th Working
Conference on Mining Software Repositories, MSR ’13, pages 233–236, Piscataway, NJ, USA,
2013. IEEE Press. ISBN 978-1-4673-2936-1. URL http://dl.acm.org/citation.cfm?
id=2487085.2487132.

[30] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn
Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. Graphcode{bert}: Pre-training
code representations with data ﬂow. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=jLoC4ez43PZ.

[31] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. UniXcoder: Uniﬁed
cross-modal pre-training for code representation. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7212–7225,
Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.
acl-long.499. URL https://aclanthology.org/2022.acl-long.499.

[32] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang
Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu,
Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu.
Pre-trained models: Past, present and future. AI Open, 2021. ISSN 2666-6510. doi: https://
doi.org/10.1016/j.aiopen.2021.08.002. URL https://www.sciencedirect.com/science/
article/pii/S2666651021000231.

[33] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast
for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 9726–9735, 2020. doi: 10.1109/CVPR42600.
2020.00975.

[34] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo,
Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding
challenge competence with apps. In J. Vanschoren and S. Yeung, editors, Proceedings of
the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1,
2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/
file/c24cd76e1ce41366a4bbe8a49b02a028-Paper-round2.pdf.

[35] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the
naturalness of software. ICSE ’12, page 837–847. IEEE Press, 2012. ISBN 9781467310673.

[36] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural
In ICLR, 2020. URL http://dblp.uni-trier.de/db/conf/iclr/

text degeneration.
iclr2020.html#HoltzmanBDFC20.

[37] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and
predicting hospital readmission. CoRR, abs/1904.05342, 2019. URL http://arxiv.org/
abs/1904.05342.

[38] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
Codesearchnet challenge: Evaluating the state of semantic code search. CoRR, abs/1909.09436,
2019. URL http://arxiv.org/abs/1909.09436.

20

[39] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to
code in programmatic context. In Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 1643–1652, Brussels, Belgium, October-November
2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1192. URL https:
//aclanthology.org/D18-1192.

[40] Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez, and Ion Stoica. Con-
trastive code representation learning. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, pages 5954–5971, Online and Punta Cana, Domini-
can Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/
2021.emnlp-main.482. URL https://aclanthology.org/2021.emnlp-main.482.

[41] Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu. Treebert: A tree-based pre-trained
model for programming language. In Uncertainty in Artiﬁcial Intelligence, pages 54–63. PMLR,
2021.

[42] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Pre-trained contextual
embedding of source code, 2020. URL https://openreview.net/forum?id=rygoURNYvS.

[43] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1412.6980.

[44] Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–
71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012.

[45] Royi Lachmy, Ziyu Yao, Greg Durrett, Milos Gligoric, Junyi Jessy Li, Ray Mooney, Graham
Neubig, Yu Su, Huan Sun, and Reut Tsarfaty, editors. Proceedings of the 1st Workshop on Natu-
ral Language Processing for Programming (NLP4Prog 2021), Online, August 2021. Association
for Computational Linguistics. URL https://aclanthology.org/2021.nlp4prog-1.0.

[46] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and
Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical
text mining. Bioinformatics, 36(4):1234–1240, 2020.

[47] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond,
Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy,
Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl,
Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,
Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level
Code Generation with AlphaCode, 2022. URL https://arxiv.org/abs/2203.07814.

[48] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,
Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,
Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code
understanding and generation. CoRR, abs/2102.04664, 2021.

[49] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word

representations in vector space. arXiv preprint arXiv:1301.3781, 2013.

[50] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek,
Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav
Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr,
Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne
Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive pre-training.
CoRR, abs/2201.10005, 2022. URL https://arxiv.org/abs/2201.10005.

21

[51] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,
and Caiming Xiong. A conversational paradigm for program synthesis. arXiv preprint
arXiv:2203.13474, 2022.

[52] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July 2002.
Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https:
//aclanthology.org/P02-1040.

[53] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. Advances in neural information processing
systems, 32, 2019.

[54] Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Annibal, Alec Peltekian, and Yanfang
Ye. CoTexT: Multi-task learning with code-text transformer. In Proceedings of the 1st Workshop
on Natural Language Processing for Programming (NLP4Prog 2021), pages 40–47, Online,
August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.nlp4prog-1.5.
URL https://aclanthology.org/2021.nlp4prog-1.5.

[55] Jiahao Qin and Lu Zong. TS-BERT: A fusion model for pre-trainning time series-text represen-

tations, 2022. URL https://openreview.net/forum?id=Fia60I79-4B.

[56] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language

understanding by generative pre-training. 2018.

[57] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language

models are unsupervised multitask learners. 2019.

[58] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL
http://jmlr.org/papers/v21/20-074.html.

[59] Veselin Raychev, Martin Vechev, and Eran Yahav. Code completion with statistical language
models. In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language
Design and Implementation, pages 419–428, 2014.

[60] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming
Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code
synthesis. CoRR, abs/2009.10297, 2020. URL https://arxiv.org/abs/2009.10297.

[61] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsu-
pervised translation of programming languages. Advances in Neural Information Processing
Systems, 33:20601–20611, 2020.

[62] Torsten Scholak, Gabriel Orlanski, Disha Shrivastava, Arun Raja, Dzmitry Bahdanau, and
Jonathan Herzig. Deep Learning For Code (DL4C) Workshop at ICLR 2022. https://dl4c.
github.io/, 2022. [Online; accessed 9-June-2022].

[63] Andrea Di Sorbo, Sebastiano Panichella, Oscar Chaparro, Rafael Kallis, and Yang Song. The
1st Intl. Workshop on Natural Language-based Software Engineering Co-located with ICSE
2022. https://nlbse2022.github.io/, 2022. [Online; accessed 20-June-2022].

[64] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles,
and Herve Jegou. Training data-efﬁcient image transformers & distillation through attention.
In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages
10347–10357. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/
touvron21a.html.

22

[65] Trieu H. Trinh, Minh-Thang Luong, and Quoc V. Le. Selﬁe: Self-supervised pretraining
for image embedding. CoRR, abs/1906.02940, 2019. URL http://arxiv.org/abs/1906.
02940.

[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information
Processing Systems, 30, 2017.

[67] Ben Wang. Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Lan-
guage Model with JAX. https://github.com/kingoflolz/mesh-transformer-jax,
May 2021.

[68] Xin Wang, Yasheng Wang, Pingyi Zhou, Fei Mi, Meng Xiao, Yadao Wang, Li Li, Xiao Liu, Hao
Wu, Jin Liu, and Xin Jiang. CLSEBERT: contrastive learning for syntax enhanced code pre-
trained model. CoRR, abs/2108.04556, 2021. URL https://arxiv.org/abs/2108.04556.

[69] Xin Wang, Yasheng Wang, Yao Wan, Jiawei Wang, Pingyi Zhou, Li Li, Hao Wu, and Jin Liu.
Code-mvp: Learning to represent source code from multiple views with contrastive pre-training,
2022. URL https://arxiv.org/abs/2205.02029.

[70] Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven C.H. Hoi. CodeT5: Identiﬁer-aware uniﬁed
pre-trained encoder-decoder models for code understanding and generation. In Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8696–8708,
Online and Punta Cana, Dominican Republic, November 2021. Association for Computational
Linguistics. doi: 10.18653/v1/2021.emnlp-main.685. URL https://aclanthology.org/
2021.emnlp-main.685.

[71] Neo Wu, Bradley Green, Xue Ben, and Shawn O’Banion. Deep transformer models for
time series forecasting: The inﬂuenza prevalence case. CoRR, abs/2001.08317, 2020. URL
https://arxiv.org/abs/2001.08317.

[72] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang.
Connecting the dots: Multivariate time series forecasting with graph neural networks.
In
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, KDD ’20, page 753–763, New York, NY, USA, 2020. Association for Computing
Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3403118. URL https://doi.org/
10.1145/3394486.3403118.

[73] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang
Yang, Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang,
Jun Wang, Jianfeng Yu, Qi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao Tao, Dasen Yan,
Zexuan Yi, Fang Peng, Fangqing Jiang, Han Zhang, Lingfeng Deng, Yehong Zhang, Zhe
Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo, Shanzhi Gu, Gaojun Fan, Yaowei Wang,
Xuefeng Jin, Qun Liu, and Yonghong Tian. Pangu-α: Large-scale autoregressive pretrained
chinese language models with auto-parallel computation. CoRR, abs/2104.12369, 2021. URL
https://arxiv.org/abs/2104.12369.

23

A Qualitative Analysis of Example Outputs

We show some examples of code solutions generated by the different versions of PANGU-CODER after
stage-1 pre-training (c.f. Section 3.2.1), after stage-2 training with the CODE-CLM objective (c.f.
Section 3.2.2), and after ﬁne-tuning (c.f. Section 4).

In this ﬁrst example from HumanEval in Table 12, we can see that both Stage-1 and Stage-2 models
are attempting to generate solutions, but only the ﬁne-tuned model produces a correct one.

PROMPT

Return a greatest common divisor of two integers a and b.
>>> greatest_common_divisor(3, 5)
1
>>> greatest_common_divisor(25, 15)
5

def greatest_common_divisor(a: int, b: int) -> int:

STAGE-1

(cid:55)

if a == 0:

return b

if b == 0:

return a

return greatest_common_divisor(a // b, b // a)

def greatest_common_divisor(a: int, b: int) -> int:

STAGE-2

(cid:55)

if a == 0 or b == 0:

return 0

else:

return a if a > b else b

def greatest_common_divisor(a: int, b: int) -> int:

FT

(cid:51)

if a > b:

a, b = b, a

while b!= 0:

a, b = b, a % b

return a

Table 12: Example 1 from Human Eval

In the second example from HumanEval in Table 13, we observe a similar pattern with Stage-1 and
Stage-2 failing to produce a solution, but the ﬁne-tuned succeeds. Interestingly, the Stage-2 model
did not even attempt to provide a solution. Given the complexity of the problem, we can assume that
the ﬁne-tuned version was able to solve it because it has encountered it (or one quite similar to it) in
the ﬁne-tuning data.

In the third and ﬁnal example from HumanEval in Table 14, Stage-2 managed to ﬁnd a correct
solution but Stage-1 and ﬁne-tuned fails. The ﬁne-tuned version actually generates its own docstring
that is very similar with the prompt, indicating that it must have encountered the problem in the
ﬁne-tuning data as well. However, the subsequent generated solution is not adequate to solve the
problem.

We observe similar behavior in Tables 15 and 16 which show two corresponding examples for the
MBPP dataset.

24

PROMPT

STAGE-1

STAGE-2

FT

(cid:55)

(cid:55)

(cid:51)

In this Kata, you have to sort an array of non-negative
integers according to number of ones in their binary
representation in ascending order. For similar number
of ones, sort based on decimal value.

It must be implemented like this:
>>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]
>>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]
>>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]

def sort_array(arr):

return sorted(arr)

def sort_array(arr):

pass

def sort_array(arr):

return sorted(arr, key=lambda x: (bin(x).count("1"), x))

Table 13: Example 2 from Human Eval

PROMPT

Given an array of integers nums, find the minimum sum of
any non-empty sub-array of nums.

STAGE-1

(cid:55)

STAGE-2

(cid:51)

def minSubArraySum(nums):
if len(nums) == 0:

return 0
min_sum = nums[0]
for i in range(1, len(nums)):
if nums[i] < min_sum:
min_sum = nums[i]

return min_sum

def minSubArraySum(nums):

if not nums:
return 0
min_sum = nums[0]
current_sum = nums[0]
for i in range(1, len(nums)):

FT

(cid:55)

current_sum = min(current_sum + nums[i], nums[i])
min_sum = min(min_sum, current_sum)

return min_sum

def minSubArraySum(nums):

"""
Given an array of integers nums, find the minimum sum
of any non-empty sub-array of nums.
Example minSubArraySum([2, 3, 4, 1, 2, 4]) == 1
minSubArraySum([-1, -2, -3]) == -6
"""
min_sum = nums[0]
for i in range(1, len(nums)):

min_sum = min(min_sum + nums[i], nums[i])

return min_sum

Table 14: Example 3 from HumanEval

25

PROMPT

STAGE-1

STAGE-2

FT

(cid:55)

(cid:55)

(cid:51)

Write a function to find whether a given array of
integers contains any duplicate element.
Your code should pass these tests:
assert test_duplicate(([1,2,3,4,5]))==False
assert test_duplicate(([1,2,3,4, 4]))==True
assert test_duplicate([1,1,2,2,3,3,4,4,5])==True

def test_duplicate(arraynums):

assert test_duplicate(arraynums) == False

def test_duplicate(arraynums):

for i in arraynums:

if i in arraynums:
return True

return False

def test_duplicate(arraynums):

return len(arraynums)!= len(set(arraynums))

Table 15: Example 1 from MBPP

26

PROMPT

Write a function to calculate the harmonic sum of n-1.
Your code should pass these tests:
assert harmonic_sum(7) == 2.5928571428571425
assert harmonic_sum(4) == 2.083333333333333
assert harmonic_sum(19) == 3.547739657143682

STAGE-1

(cid:55)

def harmonic_sum(n):
assert n > 0
assert n <= 1
assert n % 2 == 0
assert n > 1
assert n % 3 == 0
assert n > 2
assert n % 5 == 0
assert n > 3
assert n % 7 == 0
assert n > 4
assert n % 11 == 0
assert n > 5
assert n % 13 == 0
assert n > 6
assert n % 15 == 0
assert n > 7
assert n % 17 == 0
assert n > 8
assert n % 19 == 0
assert n > 9
assert n % 23 == 0
assert n > 10
assert n % 29 == 0
assert n > 11
assert n % 31 == 0
assert n > 12
assert n % 41 == 0
assert n > 13
assert n % 43 == n
assert n > 14

STAGE-2

FT

(cid:51)

(cid:55)

def harmonic_sum(n):

return sum([i for i in range(1, n + 1)])

def harmonic_sum(n):

return (n * (n + 1)) / 2

Table 16: Example 2 from MBPP

27

