Programming Data Structures for Large-Scale
Desktop Simulations of Complex Systems*

Patrik Christen
Institute for Information Systems
FHNW
Olten, Switzerland
patrik.christen@fhnw.ch

2
2
0
2

l
u
J

9
2

]
S
D
.
s
c
[

2
v
7
3
8
4
0
.
5
0
2
2
:
v
i
X
r
a

Abstract—The investigation of complex systems requires
running large-scale simulations over many temporal iterations.
It is therefore important to provide efﬁcient implementations.
The present study borrows philosophical concepts from Gilbert
Simondon to identify data structures and algorithms that have
the biggest impact on running time and memory usage. These
are the entity e-tuple E and the intertwined update function
φ. Focusing on implementing data structures in C#, E is
implemented as a list of objects according to current software
engineering practice and as an array of pointers according to
theoretical considerations. Cellular automaton simulations with
109 entities over one iteration reveal that the object-list with
dynamic typing and multi-state readiness has a drastic effect
on running time and memory usage, especially dynamic typing
as it has a big impact on the evolution time. Pointer-arrays are
possible to implement in C# and are more running time and
memory efﬁcient as compared to the object-list implementation,
however, they are cumbersome to implement. In conclusion,
avoiding dynamic typing in object-list based implementations
or using pointer-arrays gives evolution times that are acceptable
in practice, even on desktop computers.

Index Terms—allagmatic method, complex systems, data
structures, large-scale modelling and simulation, programming

I. INTRODUCTION

Studying complex systems is as fascinating as it is crucial.
Fascinating because new and beautiful things can appear out
of nowhere such as the intricate pattern generated by a cellu-
lar automaton updating its states according to Wolfram’s rule
110 [1]. And crucial because we face enormous problems
such as climate change and socio-economic instability, where
a complex systems perspective and methodology promise
a potential way forward [2]. Such complex systems are
typically characterised by a large number of elements or
entities that interact with each other over time, where the
interactions are speciﬁc to the entities and change over time
[3]. The co-evolution of entity states and entity interactions
makes complex systems difﬁcult if not impossible to treat
analytically, since this would lead to a system of dynamical
equations dynamically coupled to their boundary conditions
[3]. A more natural way to determine the dynamics of a
complex system is to specify an algorithm consisting of
rules on how entities and their interactions update over time
[3], which is nicely illustrated with cellular automata. The
algorithmic approach allows us to simulate complex systems
and run experiments for hypothesis testing. In contrast to
the analytical approach where equations are solved and

This work was supported by the Hasler Foundation under grant No.

21017.

results can be directly calculated for any point in time, the
algorithmic approach requires the calculation of entity states
in every time iteration over the simulated time. It is not
possible to jump ahead in time, which makes this approach
computationally expensive, especially because complex sys-
tems are usually composed of a large number of entities and
a series of experiments are run to study their behaviour in
response to certain changes.

The programming of efﬁcient algorithms and data struc-
tures for modelling complex systems is therefore essential.
There is, however, no standardised method for modelling
complex systems which makes this task a challenge. We
recently developed the so-called allagmatic method [4]–[6],
which is a general approach for modelling and simulating
complex systems including the deﬁnition of data structures
and algorithms. It is inspired by philosophical concepts such
as Gilbert Simondon’s structure and operation [7], [8], and
Alfred North Whitehead’s adaptation and control [9], [10].
These concepts are implemented as a system metamodel
with program code that not only allows the concepts to be
deﬁned precisely, but also allows them to be executed or
run in relation to other concepts. The allagmatic method
proved to be useful to generate concrete computer models
such as cellular automata and artiﬁcial neural networks [4],
[6] as well as to guide automatic programming by providing
abstract model building blocks from which systems are
composed of [11]. These model building blocks can be put
together in an automatic way by concretising them into
more and more concrete models of complex systems. With
self-modifying code, on the other hand, it allows controlled
modiﬁcation of speciﬁc code regions [12]–[14].

It is common to use C++ for efﬁcient implementations
and to run the code on a supercomputer. The allagmatic
method was ﬁrst developed in C++ using classes and generic
programming (template metaprogramming) [15] to describe
the system metamodel [6]. In later studies on self-modifying
code, the system metamodel was reimplemented in C# [12]–
[14], because it allows compiling and running code as an
object, provides reﬂection capabilities, and generally comes
with a vast API. Compiling and running code as an object
in C# is faster than writing code into a ﬁle, call the system
to compile and then run the executable in C++. Reﬂection is
rather limited if not missing in C++ while C# provides re-
spective classes. Although it is possible, self-modifying code
and reﬂection are not welcome concepts to be implemented
on supercomputers, where you want to have full control over
the code and C# might only run in cloud systems and not

 
 
 
 
 
 
on supercomputers. Both, cloud systems and supercomputers,
are usually designed with nodes that consist of little memory
and one is required to parallelise the code. Since complex
systems are composed of a large number of elements, this
can be an issue, especially if one wants to create prototypes
and not invest too much time into parallel implementations.
Desktop computers have become fast too and, most relevant
for this application, they provide large memory. In addition,
desktop computers are also an interesting alternative when
it comes to sensitive data allowing computation on-site such
as in a hospital or in a military application without any data
transfer over the internet.

The allagmatic method was developed with a focus on
modelling and simulating complex systems according to
philosophical concepts, however, the respective data struc-
tures and algorithms were not optimally implemented with
respect to running time and memory usage. In the present
study, an implementation based on objects and another based
on pointers are described and compared in terms of running
time and memory usage on a desktop computer. In the ﬁrst
section of the paper, I introduce the allagmatic method and
then use it to abstractly deﬁne data structures and algorithms
of complex systems in the second section. The third section
deals with the programming and thus speciﬁc implementation
of the deﬁned data structures and algorithms of the second
section. The fourth section describes large-scale experiments
and presents the results of their running time and memory
usage on a desktop computer. Finally, the differences in
performance are discussed and an outlook on some of the
next steps is provided.

II. THE ALLAGMATIC METHOD

The French philosopher Gilbert Simondon describes ob-
jects and processes (or, more generally, systems) in terms of
structures and operations [7], [8]. Structures capture the static
or spatial side of a system while operations capture their
dynamic or temporal side. Thereby, Simondon emphasises
that structures and operations are tightly intertwined and
should therefore not be studied in isolation. This means
that every system can be described from a structural and
operational perspective and it also means that each operation
must have a respective structure to operate on and vice versa.
The philosophy of Alfred North Whitehead is compatible
with Simondon’s system perspective and adds to it important
concepts such as entity, adaptation, and control [9], [10].
Whitehead describes systems in terms of entities that con-
stantly interact with each other forming higher level systems
(or in his terminology societies and nex¯us). This allows him
to clearly deﬁne deep concepts including adaptation, control,
and complexity.

The so-called allagmatic method was initially developed
based on Simondon’s structure and operation [6], and later
extended with Whitehead’s adaptation and control [4], [5].
These concepts enable the modelling and simulation of sys-
tems and especially of complex systems. The method consists
of a system metamodel that describes the modelled system at
the most abstract level in the so-called virtual regime. At this
point the system is virtual and deﬁned in terms of its abstract
structures and operations. These structures and operations are
then concretised in the so-called metastable regime. Here,

details such as the size of the system and speciﬁc operations
are deﬁned. Once a concrete model of a system is created, it
can be executed or run in the so-called actual regime. This
is where the simulation is run. There are also feedback loops
between the regimes describing higher level concepts such
as adaptation. In adaptation, the method cycles between the
metastable and actual regimes [4], [5].

The system metamodel of the allagmatic method describes
a (complex) system as a network of entities that change their
states over time in response to certain update rules and states
of connected entities in the network, here called the milieu.
Adaptation means changing these update rules or milieus.
We can formally deﬁne a system model SM (which also
consists of the system metamodel) with a tuple of structures
S and operations O speciﬁc to complex systems as follows:

Deﬁnition 2.1:

SM := (E, Q, M, U, A, P, . . . , ˆss, φ, ψ, . . . , ˆoo),

(1)

where E is an entity e-tuple describing the entity states, Q is
a set describing the possible entity states, M is a milieu e-
tuple describing the connected entities for each entity, U is an
update rules u-tuple describing the dynamics of entity states,
A is an adaptation rule a-tuple describing the adaptation of
the update rules U taking into account the adaptation end
described with the p-tuple P, ˆsi are any further structures,
φ is the update function executing the update rules U on the
entity states E, ψ is the adaptation function executing the
adaptation rules A on the update rules U and milieus M,
and ˆoj are any further operations.

III. DATA STRUCTURES OF COMPLEX SYSTEMS

We now use the allagmatic method to deﬁne the essential
data structures of complex systems. Simondon’s structures
can be seen as data structures and operations as algorithms
in the way they are classically described in textbooks on
algorithms and data structures [16]. It is interesting to note
at this point that also in the ﬁeld of algorithms and data
structures one should study the two not in isolation but in
relation to each other. So Simondon has already foreseen
this.

If we are guided by the allagmatic method, we can see that
at the core there are the entity states E updated by the update
function φ and it is mostly them that determine the running
time and memory usage. The milieu M is also important
as it determines the network structure of the system. The
other structures and operations are of course important as
well, however, not with respect to running time and memory
usage because these structures are rather small and these
operations are executed on the small structures and only a
few times. In contrast, the entity state e-tuple E holds the
states of every entity and has a size of e, which is generally
large in complex systems. The update function φ operates
on every entity and updates its state in every iteration over
time.

Further, we can apply the theory of algorithms and data
structures [16] to this core data structure and algorithm.
The network structure described with E and M can be well
captured with a graph, and graphs, on the other hand, can be
well represented by adjacency lists or adjacency matrices.
Since in our case the total number of entities e is much

bigger than the number of entities in a milieu of a particular
entity m, an adjacency list is favourable in terms of space-
efﬁciency. Each entity is updated by φ and states of the
entities in the milieu can be easily accessed via index. If
the milieu M changes, the references have to be updated
as well. In case a new entity is added to the system, one
does not have to rearrange E and M, it can be added to the
end of the adjacency list. It is therefore favourable in terms
of time-efﬁciency to base the adjacency list on an array of
arrays. Thereby, the array represents the index of E and the
arrays within it represent M.

IV. PROGRAMMING OF DATA STRUCTURES

The adjacency list and the update function operating on it
were implemented here in two different ways: ﬁrst, according
to standard software engineering practice and second, an
optimised version with the theory of algorithms and data
structures in mind. Both versions were implemented in C#
bearing in mind that most applications will require concepts
such as reﬂection that would be difﬁcult or cumbersome to
implement in C++. Please consult Skeet’s book C# in Depth
[17] or the .NET Documentation [18] for more details on the
capabilities of C#.

The ﬁrst version of the implementation is in accordance
with standard software engineering practice. In the case of
C#, this means object-oriented programming and making
use of data containers provided by the API. An entity is
implemented with a class Entity consisting of three ﬁelds:
ﬁrst, a ﬁeld storing the state of the entity. Second, a ﬁeld
storing the updated state of the entity. This is required
because one cannot overwrite the state directly during the
update since the current state might still be used by another
entity. And third, a ﬁeld storing the milieu. The ﬁrst two
ﬁelds are of the same data type,
in the example listing
below they are of type dynamic. This data type was chosen
because according to the allagmatic method, entity states
are concretised. In addition, boolean type was also used for
comparison as well as considering single-state and multi-
state entities, each of them with boolean type and dynamic
type. Multi-state means that instead of a primitive data type,
a list has been used to allow storing of multiple states per
entity. Only one state was implemented, thus it is rather
multi-state ready. The milieu is an array of references to
entity objects. In C#, arrays of objects are provided by the
List class [19]. It is an array and should not be confused
with a list such as a linked list.
p u b l i c
{

c l a s s E n t i t y

p r i v a t e dynamic
p r i v a t e dynamic n e x t S t a t e ;
p r i v a t e L i s t <E n t i t y > m i l i e u ;

s t a t e ;

}

In the main program, another array is created to store the
entities. The List class is used since objects are stored in
an array. The array is ﬁrst declared and then objects are
added to it with the method Add in a ﬁrst for-loop and the
milieu is generated with the method GenerateMilieu in
a second for-loop as shown in the listing below. First and
last entities in the array are treated separately outside the
for-loop deﬁning certain boundary conditions. It implements
the structures E and M.

L i s t <E n t i t y > e n t i t i e s = new L i s t <E n t i t y > ( ) ;
f o r
{

i <n u m b e r O f E n t i t i e s ;

i ++)

i = 0 ;

( i n t

E n t i t y e n t i t y = new E n t i t y ( ) ;
e n t i t i e s . Add ( e n t i t y ) ;

}
f o r
{

( i n t

i = 1 ;

i <n u m b e r O f E n t i t i e s − 1 ;

i ++)

e n t i t i e s [ i ] . G e n e r a t e M i l i e u ( e n t i t i e s ,

i ) ;

}

The Entity class also consists of a method implementing
the update function φ. It updates the states of each entity
in each time iteration according to predeﬁned update rules
implemented with if-statements in a ﬁrst for-loop. Once the
states of the next iteration are determined and written to the
ﬁeld nextState, a second for-loop is used to overwrite the
ﬁeld value of state with the ﬁeld value of nextState
for each entity. This is done in every iteration over time in an
outer for-loop. The listing below shows the three for-loops
in the main program.

( i n t

i = 0 ;

i <n u m b e r O f U p d a t e I t e r a t i o n s ;

i ++)

f o r
{

( i n t

j = 0 ;

j <n u m b e r O f E n t i t i e s ;

j ++)

f o r
{

e n t i t i e s [ j ] . U p d a t e F u n c t i o n ( ) ;

}
f o r
{

( i n t

j = 0 ;

j <n u m b e r O f E n t i t i e s ;

j ++)

e n t i t i e s [ j ] . U p d a t e F i e l d s ( ) ;

}

}

The second version of the implementation is optimised
according to the theory of algorithms and data structures
and speciﬁc to C#. That means this version tries to directly
implement the theoretical concepts, i.e. references and ar-
rays, and it tries to avoid programming concepts known to
increase running time or memory usage. From theory [16]
we already know from the ﬁrst version that an adjacency list
implemented with an array of references pointing to other
arrays is most suitable for the update function. In the ﬁrst
version, this was implemented with an array (speciﬁcally
a list in C#) of references pointing to objects, and these
objects aggregate entity states and milieus. The second
implementation is closer to the idea of having an array of
references to other arrays by implementing basic arrays and
pointers. It does not involve objects but actual pointers as
used in C++. It thus avoids objects, which take time to set
up by the constructor and maintain in memory, especially
memory allocation and garbage collection. This is also the
reason why it has been suggested to reuse allocated memory
rather than newly allocate it.

The entity states and thus E are directly stored in a
basic array as shown in the listing below. By basic array,
I mean an array of primitive data types, without dynamic
resizing, and without dynamic data type. It is not to be
confused with the List class that provides an array that can
hold objects and dynamically adapt its size. Note that both,
dynamic resizing and dynamic data type, are not possible
to implement with pointers in C#. There are two arrays
entity_state and entity_nextState in the main
program holding the current and next states for each entity.
A two-dimensional basic array [20] is used for the multi-

state ready implementation. And there is an array of arrays
milieu to describe the milieu M. Also the third array is
a basic array, however, it is a pointer array and a so-called
jagged array [21]. A jagged array is an array of arrays, where
the contained arrays can be of different sizes.

b o o l [ ]

e n t i t y s t a t e =

b o o l [ ]

new b o o l [ n u m b e r O f E n t i t i e s ] ;
e n t i t y n e x t S t a t e =
new b o o l [ n u m b e r O f E n t i t i e s ] ;

b o o l * [ ] [ ] m i l i e u =

new b o o l * [ n u m b e r O f E n t i t i e s ] [ ] ;
While creating and using pointers in C++ is straight-
forward, it is not so in languages with automatic memory
management such as in C#. Memory allocated in C++ stays
at the physical memory location and thus also at the address
as long as the programmer does not instruct the program
otherwise. Memory allocated in C#, in contrast, might be
optimised at runtime, which also includes changing the
physical memory location and thus the address. Pointers
are variables that hold memory addresses and thus rely on
allocated memory to stay in the physical memory location.
C# provides the fixed statement [22] to pin or ﬁx the
allocated memory to its location. It prevents the garbage
collector from moving allocated memory to another location
[22]. Pointers in C# are only possible with the fixed
statement and within an unsafe context [23]. Here, the
main method is deﬁned in unsafe context to make it possible
to use pointers. Pointers are used in the milieu array.
The milieu of an entity is implemented by pointing to the
neighbouring entities in the entity states array. This means
that the memory address of individual elements of the entity
states array entity_state are stored in the milieu
array. For each entity, a pointer is created for each of its
neighbours in a fixed statement, and then a pointer array
is created to store these pointers as shown in the follwoing
listing.

( i n t

i = 1 ;

i <n u m b e r O f E n t i t i e s − 1 ;

i ++)

f o r
{

f i x e d ( b o o l * p o i n t e r T o N e i g h b o u r 1 =

& e n t i t y s t a t e [ i − 1 ] )

f i x e d ( b o o l * p o i n t e r T o N e i g h b o u r 2 =

& e n t i t y s t a t e [ i + 1 ] )

{

}

m i l i e u [ i ] = new b o o l * [ 2 ] ;
m i l i e u [ i ] [ 0 ] = p o i n t e r T o N e i g h b o u r 1 ;
m i l i e u [ i ] [ 1 ] = p o i n t e r T o N e i g h b o u r 2 ;

{

}

}

While the use of objects is avoided, it is still possible to
use methods. To run the update function φ, a static method
was implemented in the Entity class as shown in the
listing below. It updates entity states according to predeﬁned
update rules implemented in the same way as in the ﬁrst
implementation. Besides the method being static, it has to
be deﬁned unsafe to use pointers as parameters. Also in
this case, entity states are ﬁrst determined and stored in
entity_nextState for each entity in a for-loop. Once
determined, another for-loop is used through all the entities
to overwrite the values in entity_state with the new
value stored in entity_nextState.

0000000000000001000000000000000
0000000000000011100000000000000
0000000000000110010000000000000
0000000000001101111000000000000
0000000000011001000100000000000
0000000000110111101110000000000
0000000001100100001001000000000
0000000011011110011111100000000
0000000110010001110000010000000
0000001101111011001000111000000
0000011001000010111101100100000
0000110111100110100001011110000
0001100100011100110011010001000
0011011110110011101110011011100
0110010000101110001001110010010
1101111001101001011111001111111

Fig. 1. Veriﬁcation output of running the cellular automaton implementa-
tions of Wolfram’s rule 30 with 31 entities and for 16 update iterations.

( i n t

i = 0 ;

i <n u m b e r O f U p d a t e I t e r a t i o n s ;

i ++)

f o r
{

E n t i t y . P o i n t e r A r r a y U p d a t e F u n c t i o n

( e n t i t y s t a t e ,

e n t i t y n e x t S t a t e , m i l i e u ) ;

}

V. LARGE-SCALE EXPERIMENTS AND RESULTS

The object-list and pointer-array based implementations
are compared by running a large-scale experiment with a
system size of 109 entities. Running time is measured using
the Stopwatch class [24] and memory usage is measured
using the top command of the Terminal application of
MacOS X. Experiments are run on a desktop computer
(Apple Mac Pro) simulating a large elementary cellular
automaton with Wolfram’s rule 30 [1]. Initially, the state
of each entity is set false, with the exception of the array
element at index 109/2, which is set true. The different
implementations are compared by measuring the running
time to set up the arrays, which is here called development
time, and to run one iteration of the cellular automaton,
which is here called evolution time. Memory usage is given
as the maximum memory assigned to the process running
the program.

Before running the large-scale simulation of the cellular
automaton for the different implementations, they were tested
on a much smaller scale of 31 entities and run for 16 iter-
ations. The expected pattern for Wolfram’s rule 30 emerged
(Fig. 1), verifying the implementations.

Running time and memory usage for 109 entities and one
update iteration are shown in Tab. I. The object-list develop-
ment time is mostly affected by the combination of dynamic
type and multi-state ready implementation. Dynamic type
and single-state implementation as well as static typing take
much less time. The situation is somewhat different for
object-list evolution. There it is clearly the dynamic type that
affects the running time the most. In terms of memory usage,
it is interesting to note that both, dynamic typing and multi-
state readiness, increase memory usage. The pointer-array
development time is not affected by multi-state readiness,
however the evolution time is doubling the required running

TABLE I
RUNNING TIME [HH:MM:SS] AND MEMORY USAGE [GB]
FOR 109 ENTITIES AND ONE UPDATE ITERATION

Boolean Type
Multi-
State
00:24:49

Single-
State
00:09:07

Dynamic Type
Multi-
State
03:53:16

Single-
State
00:17:17

00:01:27

00:01:26

-

-

00:00:25

00:00:58

00:42:44

01:08:32

00:00:07

00:00:14

-

119

47

249

47

196

-

-

465

-

Object-List
Development
Time
Pointer-Array
Development
Time
Object-List
Evolution Time
Pointer-Array
Evolution Time
Object-List
Memory Usage
Pointer-Array
Memory Usage

time. Dynamic typing is not possible with pointers and thus
no comparison can be made. Memory usage is also not
affected by multi-state readiness.

VI. DISCUSSION AND CONCLUSION

Data structures and algorithms for simulating complex
systems are successfully deﬁned with the allgmatic method
[4]–[6]. It provides abstract descriptions and formalisms for
structures capturing the spatial dimension and operations
capturing the temporal dimension of complex systems ac-
cording to the philosophy of Gilbert Simondon [7], [8].
Structures and operations can directly be understood as data
structures and algorithms, respectively, which makes Simon-
don’s philosophy relevant in theoretical computer science and
programming.

The entities e-tuple E and the intertwined update function
φ are most relevant with respect to running time and memory
usage. The present study focuses on the implementation
of E and presents different options for implementing it in
C#. One implementation is according to the current practice
in software engineering following object-oriented program-
ming. It implements a list of entity objects, which allows
most of the modern features such as dynamic resizing and
dynamic typing. This greatly increases running time and
memory usage, especially if one combines dynamic typing
and lists (Tab. I). Dynamic typing should be avoided since
it not only increases development time, it also substantially
increases evolution time, which is most important because
many iterations usually need to be run. The pointer-array is
the most time and memory efﬁcient implementation although
much more cumbersome to implement. Multi-dimensional
and jagged arrays are implemented as basic arrays refer-
encing other arrays, which seems to avoid producing any
overhead since multi-state readiness does not affect running
time or memory usage.

As long as the memory usage is not exceeding the memory
capacity of the computer, it is expected to get similar results
using different computers but the same operating system.
With at least 47 GB memory usage per iteration, the memory
capacity can be exceeded in only a few iterations if the
simulation results of each iteration are stored. The storing
of simulation results remains to be investigated and an

efﬁcient algorithm implemented. Running the simulations on
a different operating system might lead to different results.
Some processes related to memory management could affect
development and evolution time.

The current study also shows that modern desktop comput-
ers are able to run relatively large systems with 109 entities
and over many iterations in time. It can thus be concluded
that avoiding dynamic typing in object-list based implemen-
tations or using pointer-arrays gives evolution times that are
acceptable in practice, even on desktop computers. It can
also be concluded that C# allows the combination of efﬁcient
tools like pointers and basic arrays with more convenient and
modern tools as well as a rich API.

Many questions, however, remain unanswered, for ex-
ample, regarding storing simulation results and the update
function itself. Future studies might look into parallel ﬁle
writing and computing as well as other optimisations of
storing large data sets and algorithms of the update function.

REFERENCES

[1] S. Wolfram, A New Kind of Science. Champaign, IL: Wolfram Media,

2002.

[2] S. Thurner, Die Zerbrechlichkeit der Welt. Wien: edition a, 2020.
[3] S. Thurner, R. Hanel, and P. Klimek, Introduction to the Theory of
Complex Systems. New York, NY: Oxford University Press, 2018.
[4] O. Del Fabbro and P. Christen, “Philosophy-Guided Modelling and
Implementation of Adaptation and Control in Complex Systems,” in
IEEE World Congress On Computational Intelligence (IEEE WCCI),
2022. arXiv:2009.00110 [cs.NE].

[5] P. Christen and O. Del Fabbro, “Philosophy-Guided Mathematical For-
malism for Complex Systems Modelling,” in 2022 IEEE International
Conference on Systems, Man and Cybernetics (IEEE SMC), 2022.
arXiv:2005.01192 [cs.NE].

[6] P. Christen and O. Del Fabbro, “Cybernetical Concepts for Cellular
Automaton and Artiﬁcial Neural Network Modelling and Implemen-
tation,” in 2019 IEEE International Conference on Systems, Man and
Cybernetics (IEEE SMC), pp. 4124–4130, 2019. arXiv:2001.02037
[cs.OH].

[7] O. Del Fabbro, Philosophieren mit Objekten: Gilbert Simondons
prozessuale Individuationsontologie. Frankfurt and New York: Cam-
pus Verlag, 2021.

[8] G. Simondon, Individuation in Light of Notions of Form and Infor-
mation (T. Adkins, trans.), vol. I & II. Minneapolis: University of
Minnesota Press, 2020.

[9] D. Debaise, Nature as Event: The Lure of the Possible (M. Halewood,

trans.). Durham and London: Duke University Press, 2017.

[10] A. N. Whitehead, Process and Reality: An Essay in Cosmology (D.
R. Griﬁn and D. W. Sherburne, eds.). New York, NY: Free Press,
corrected ed., 1978.

[11] P. Christen and O. Del Fabbro, “Automatic programming of cellular
automata and artiﬁcial neural networks guided by philosophy,” in New
Trends in Business Information Systems and Technology (R. Dorn-
berger, ed.), vol. 294 of Studies in Systems, Decision and Control,
pp. 131–146, Cham: Springer, 2021. arXiv:1905.04232 [cs.AI].
[12] P. Christen, “Curb Your Self-Modifying Code,” in 2022 IEEE Inter-
national Conference on Systems, Man and Cybernetics (IEEE SMC),
2022. arXiv:2202.13830 [cs.SE].

[13] P. Christen, “Self-Modifying Code in Open-Ended Evolutionary Sys-

tems,” 2022. arXiv:2201.06858 [cs.NE].

[14] P. Christen, “Modelling and Implementing Open-Ended Evolution-
ary Systems,” in The Fourth Workshop on Open-Ended Evolution
(OEE4), The 2021 Conference on Artiﬁcial Life (ALife), 2021.
arXiv:2201.06858v1 [cs.NE].

[15] A. Alexandrescu, Modern C++ Design: Generic Programming and
Design Patterns Applied. Upper Saddle River, NJ: Addison-Wesley,
2001.

[16] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction

to Algorithms. Cambridge, MA: MIT Press, 4th ed., 2022.
[17] J. Skeet, C# in Depth. Shelter Island, NY: Manning, 2019.
[18] The

Documentation,

.NET

“C#

C#

https://docs.microsoft.com/en-gb/dotnet/csharp/, 2022.
April 2022].

documentation.”
[Accessed 19

“Multidimen-
Guide).”

“Jagged
Guide).”

[19] The

.NET

C#

Documentation,

“List<T>

Class.”

https://docs.microsoft.com/en-gb/dotnet/api/system.collections.generic.list-1?view=net-6.0,
2022. [Accessed 19 April 2022].

[20] The

.NET

C#

Documentation,

Arrays

sional
https://docs.microsoft.com/en-gb/dotnet/csharp/programming-guide/arrays/multidimensional-arrays,
2022. [Accessed 25 April 2022].

Programming

(C#

[21] The

.NET

C#

Documentation,

Arrays
https://docs.microsoft.com/en-gb/dotnet/csharp/programming-guide/arrays/jagged-arrays,
2022. [Accessed 20 April 2022].

Programming

(C#

[22] The .NET C# Documentation, “ﬁxed Statement (C# Reference).”

https://docs.microsoft.com/en-gb/dotnet/csharp/language-reference/keywords/ﬁxed-statement,
2022. [Accessed 20 April 2022].

[23] The

.NET C# Documentation,

“unsafe

(C# Reference).”

https://docs.microsoft.com/en-gb/dotnet/csharp/language-reference/keywords/unsafe,
2022. [Accessed 20 April 2022].

[24] The

.NET

C#

Documentation,

“Stopwatch

Class.”

https://docs.microsoft.com/en-gb/dotnet/api/system.diagnostics.stopwatch?view=net-6.0,
2022. [Accessed 20 April 2022].

