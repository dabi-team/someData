Forecast combinations: an over 50-year review

Xiaoqian Wang*, Rob J Hyndman†, Feng Li‡, Yanfei Kang§

26 September 2022

Abstract

Forecast combinations have ﬂourished remarkably in the forecasting community and, in recent
years, have become part of the mainstream of forecasting research and activities. Combining

multiple forecasts produced from single (target) series is now widely used to improve accuracy

through the integration of information gleaned from different sources, thereby mitigating the risk of

identifying a single “best” forecast. Combination schemes have evolved from simple combination

methods without estimation, to sophisticated methods involving time-varying weights, nonlinear

combinations, correlations among components, and cross-learning. They include combining point

forecasts and combining probabilistic forecasts. This paper provides an up-to-date review of the

extensive literature on forecast combinations, together with reference to available open-source

software implementations. We discuss the potential and limitations of various methods and

highlight how these ideas have developed over time. Some important issues concerning the utility

of forecast combinations are also surveyed. Finally, we conclude with current research gaps and

potential insights for future research.

Keywords: Combination forecast; Cross learning; Forecast combination puzzle; Forecast ensembles;

Model averaging; Open-source software; Pooling; Probabilistic forecasts; Quantile forecasts.

2
2
0
2

p
e
S
3
2

]
E
M

.
t
a
t
s
[

2
v
6
1
2
4
0
.
5
0
2
2
:
v
i
X
r
a

*School of Economics and Management, Beihang University, Beijing 100191, China. E-mail: xiaoqianwang@buaa.edu.cn.
†Department of Econometrics & Business Statistics, Monash University, Clayton VIC 3800, Australia. E-mail:

rob.hyndman@monash.edu.

‡School of Statistics and Mathematics, Central University of Finance and Economics, Beijing 102206, China. E-mail:

feng.li@cufe.edu.cn.

§Author for correspondence. School of Economics and Management, Beihang University, Beijing 100191, China. E-mail:

yanfeikang@buaa.edu.cn

1

 
 
 
 
 
 
1

Introduction

The idea of combining multiple individual forecasts dates back at least to Francis Galton, who in 1906

visited an ox-weight-judging competition and observed that the average of 787 estimates of an ox’s

weight was remarkably close to the ox’s actual weight; see Surowiecki (2005) for more details of the

story. About sixty years later, the famous work of Bates and Granger (1969) popularized the idea and

spawned a rich literature on forecast combinations. More than ﬁfty years have passed since Bates and

Granger’s (1969) seminal work, and it is now well established that forecast combinations are beneﬁcial,

offering substantially improved forecasts on average relative to constituent models; see Clemen (1989)

and Timmermann (2006) for extensive earlier literature reviews.

In this paper, we aim to present an up-to-date modern review of the literature on forecast combina-

tions over the past ﬁve decades. We cover a wide variety of forecast combination methods for both

point forecasts and probabilistic forecasts, contrasting them and highlighting how various related

ideas have developed in parallel.

Combining multiple forecasts derived from numerous forecasting methods is often a better ap-

proach than identifying a single “best forecast”. These are usually called “combination forecasts” or

“ensemble forecasts” in different domains. Observed time series data are unlikely to be generated by

a simple process speciﬁed with a speciﬁc functional form because of the possibility of time-varying

trends, seasonality changes, structural breaks, and the complexity of real data generating processes

(Clements and Hendry, 1998). Thus, selecting a single “best model” to approximate the unknown

underlying data generating process may be misleading, and is subject to at least three sources of

uncertainty: data uncertainty, parameter uncertainty, and model uncertainty (Petropoulos et al., 2018a;

Kourentzes et al., 2019). Given these challenges, it is often better to combine multiple forecasts to

incorporate multiple drivers of the data generating process and mitigate uncertainties regarding

model form and parameter speciﬁcation.

Potential explanations for the strong performance of forecast combinations are manifold. First, the

combination is likely to improve forecasting performance when multiple forecasts to be combined

incorporate partial (but incompletely overlapping) information. Second, structural breaks are a

common motivation for combining forecasts from different models (Timmermann, 2006). In the

presence of structural breaks and other instabilities, combining forecasts from models with different

degrees of misspeciﬁcation and adaptability can mitigate the problem, and helps explains the empirical

success of forecast combinations. See, e.g., Rossi (2013) and Rossi (2021) for an extensive discussion

on forecast combinations in the presence of instabilities. Indeed, one can consider the competing

forecasts as a form of intercept correction relative to a baseline forecast, providing potential gains

in forecast accuracy if there are either structural breaks or deterministic misspeciﬁcations (Hendry

and Clements, 2004). Finally, Hendry and Clements (2004) noted that forecast combination can be

viewed as an application of Stein-James shrinkage estimation (Judge and Bock, 1978). Speciﬁcally, if

the unknown future value is considered as a “meta-parameter” of which all the individual forecasts

are estimates, then averaging has the potential to provide an improved estimate.

In light of their superiority, forecast combinations have appeared in a wide range of applications

such as retail (Ma and Fildes, 2021), energy (Xie and Hong, 2016), economics (Aastveit et al., 2019),

2

and epidemiology (Ray et al., 2022). Among all published forecasting papers included in the Web of

Science, the proportion of papers concerning forecast combinations has been trending upward over

the past 50 years, reaching 13.80% in 2021, as shown in Figure 1. As a consequence, it is timely and

necessary to review the extant literature on this topic.

Figure 1: The proportion of papers that concern forecast combinations among all published
forecasting papers included in the Web of Science databases during the publication year range
1969–2021. Speciﬁcally, we use the search query TS = (forecast*) to ﬁnd all forecasting papers,
and to ﬁnd papers concerning forecast combinations we use TS = ((forecast* NEAR/5 combin*)
OR (forecast* NEAR/5 ensemble*) OR (forecast* NEAR/5 averag*) OR (forecast* NEAR/5
aggregat*) OR (forecast* NEAR/5 pool*) OR (forecast* AND ((model* NEAR/5 combin*) OR
(model* NEAR/5 ensemble*) OR (model* NEAR/5 averag*) OR (model* NEAR/5 aggregat*) OR
(model* NEAR/5 pool*)))).

The gains from forecast combinations rely on not only the quality of the individual forecasts to be

combined, but the estimation of the combination weights assigned to each forecast (Timmermann, 2006;

Cang and Yu, 2014). Numerous studies have been devoted to discussing critical issues concerning the

constitution of the model pool and the selection of the optimal model subset, including but not limited

to the accuracy, diversity, and robustness of individual models (Batchelor and Dua, 1995; Mannes

et al., 2014; Thomson et al., 2019; Lichtendahl and Winkler, 2020; Kang et al., 2021). On the other hand,

combination schemes vary across studies and have evolved from simple combination methods that

avoid weight estimation (e.g., Clemen and Winkler, 1986; Palm and Zellner, 1992; Genre et al., 2013;

Grushka-Cockayne et al., 2017a; Petropoulos and Svetunkov, 2020) to sophisticated methods that

tailor weights for different individual models (e.g., Bates and Granger, 1969; Newbold and Granger,

1974; Kolassa, 2011; Li et al., 2020; Montero-Manso et al., 2020; Kang et al., 2021; Wang et al., 2022b).

Accordingly, forecast combinations can be linear or nonlinear, static or time-varying, series-speciﬁc or

cross-learning, and ignore or cover correlations among individual forecasts. Despite the diverse set

of forecast combination schemes, forecasters still have little guidance on how to solve the “forecast

combination puzzle” (Stock and Watson, 2004; Smith and Wallis, 2009; Claeskens et al., 2016; Chan

and Pauwels, 2018) — simple averaging often empirically dominates sophisticated weighting schemes

that should (asymptotically) be superior.

3

05101519691975198019851990199520002005201020152021Publication yearProportion (%)Initial work on forecast combinations after the seminal work of Bates and Granger (1969) focused

on dealing with point forecasts (see, for example, Clemen, 1989; Timmermann, 2006). In recent

years considerable attention has moved towards the use of probabilistic forecasts (e.g., Hall and

Mitchell, 2007; Gneiting and Ranjan, 2013; Kapetanios et al., 2015; Martin et al., 2021) as they enable a

rich assessment of forecast uncertainties. When working with probabilistic forecasts, issues such as

diversity among individual forecasts can be more complex and less understood than combining point

forecasts (Ranjan and Gneiting, 2010), and additional issues such as calibration and sharpness need to

be considered when assessing or selecting a combination scheme (Gneiting et al., 2007). Additionally,

probabilistic forecasts can be elicited in different forms (i.e., density forecasts, quantiles, prediction

intervals, etc.), and the resulting combinations may have different properties such as calibration,

sharpness, and shape; see Lichtendahl et al. (2013) for further analytical details.

We should clarify that we take the individual forecasts to be combined as given, and we do not dis-

cuss how the forecasts themselves are generated. We focus our attention on combinations of multiple

forecasts derived from separate and non-interfering models for a given time series. Nevertheless, the

literature involves at least two other types of combinations that are not covered in the present review.

The ﬁrst is the case of generating multiple series from the single (target) series, forecasting each of the

generated series independently, and then combining the outcomes. Such data manipulation extracts

more information from the target time series, which, in turn, can be used to enhance the forecasting

performance. Petropoulos and Spiliotis (2021) referred to this category of forecast combinations

generally as “wisdom of the data” and provided an overview of approaches in this category. In this

particular context, the combination methods reviewed in this paper can function as tools to aggregate

(or combine) the forecasts computed from different perspectives of the same data. The second type

of forecast combination we do not cover is forecast reconciliation for hierarchical time series, which

has developed over the past ten years since the pioneering work of Hyndman et al. (2011). Forecast

reconciliation involves reconciling forecasts across the hierarchy to ensure that the forecasts sum

appropriately across the levels of the hierarchy, and hence is a type of forecast combination.

We note that forecast combination and model averaging are sometimes used without distinction in

the literature. The two terms overlap, but their focuses are different. “Model averaging” is a general

term allowing for model uncertainty, particularly in parameter estimation, which can lead to better

estimates and more reliable forecasts and prediction intervals than model selection (selecting a single

best model). Several approaches to model averaging have been developed in statistics, econometrics,

and machine learning. Two main strands can be identiﬁed: frequentist approaches (e.g., Fletcher, 2018)

and Bayesian approaches (e.g. Steel, 2020). “Forecast combination” is a more focused terminology

describing the combination of forecasts to generate a better forecast; the component forecasts could

be outcomes from model averaging, individual models, or expert forecasts, for example. As with

model averaging, weights can be used to combine the component forecasts. Unlike model averaging,

however, forecast combination also has some underlying assumptions on the forecasts to ensure that

the forecast combinations are unbiased or optimal.

This paper aims to contribute a broad perspective and historical overview of the main develop-

ments in forecast combinations. The paper is organized into two main sections on point forecast

combinations (Section 2) and probabilistic forecast combinations (Section 3). Section 4 concludes the

paper and identiﬁes possible future developments in the future.

4

2 Point forecast combinations

2.1 Simple point forecast combinations

A considerable literature has accumulated over the years regarding how individual forecasts are

combined, with the unanimous conclusion that simple combination schemes are hard to beat (Kang,

1986; Clemen, 1989; Fischer and Harvey, 1999; Stock and Watson, 2004; Lichtendahl and Winkler,

2020). That is, equally weighted averages, which ignore past information regarding the precision of

individual forecasts and correlations between forecast errors, work reasonably well compared to more

sophisticated combination schemes.

The vast majority of studies on combining multiple forecasts have dealt with point forecasting, even

though point forecasts (without associated measures of uncertainty) provide insufﬁcient information

for decision-making. The simple arithmetic average of forecasts based on equal weights stands out

as the most popular and surprisingly robust combination rule (see Bunn, 1985; Clemen and Winkler,

1986; Stock and Watson, 2003; Genre et al., 2013), and can be effortlessly implemented.

An early example of an equally weighted combination is from the M-competition, the ﬁrst fore-

casting competition run by Spyros Makridakis and Mich`ele Hibon, involving 1001 time series; see

Makridakis et al. (1982) and Hyndman (2020) for more details of the competition. Makridakis et al.

(1982) reported that the simple average outperformed the individual forecasting models. Clemen (1989)

provided an extensive bibliographical review of the early work on the combination of forecasts, and

then addressed the issue that the arithmetic means often dominate more reﬁned forecast combinations.

Makridakis and Winkler (1983) concluded empirically that a larger number of individual methods

included in the simple average scheme would help improve the accuracy of combined forecasts and

reduce the variability associated with the selection of methods. Palm and Zellner (1992) concisely

summarized the advantages of adopting simple averaging into three aspects: (i) combination weights

are equal and do not have to be estimated; (ii) simple averaging signiﬁcantly reduces variance and

bias by averaging out individual bias in many cases; and (iii) simple averaging should be considered

when the uncertainty of weight estimation is taken into account. Additionally, Timmermann (2006)

pointed out that the outstanding average performance of simple averaging depends strongly on model

instability and the ratio of forecast error variances associated with different forecasting models.

More attention has been given to other strategies, including using the median and mode, as well

as trimmed and winsorized means (e.g., Chan et al., 1999; Stock and Watson, 2004; Genre et al., 2013;

Jose et al., 2014; Grushka-Cockayne et al., 2017a), due to their robustness in the sense of being less

sensitive to extreme forecasts than a simple average (Lichtendahl and Winkler, 2020). For example,

the early work of Galton (1907b) observed that the “middlemost” of 787 estimates of an ox’s weight

is within nine pounds of the ox’s actual weight, and thus advocated for the median forecast as the

“vox populi” (Galton, 1907a). However, there is little consensus in the literature on whether the mean

or the median of individual forecasts performs better in terms of point forecasting (Kolassa, 2011).

Speciﬁcally, McNees (1992) found no signiﬁcant difference between the mean and the median, while

the results of Stock and Watson (2004) supported the mean and Agnew (1985) and Galton (1907b)

recommended the median. Jose and Winkler (2008) studied the forecasting performance of the mean

and median, as well as the trimmed and winsorized means. Their results suggested that the trimmed

5

and winsorized means are appealing, particularly when there is a high level of variability among the

individual forecasts, because of their simplicity and robust performance. Kourentzes et al. (2014a)

compared empirically the mean, mode and median combination operators based on kernel density

estimation, and found that the three operators deal with outlying extreme values differently, with the

mean being the most sensitive and the mode operator the least. Based on these experimental results,

they recommended further investigation of the use of the mode and median operators, which have

been largely overlooked in the relevant literature.

Compared to various complicated combination approaches and machine learning algorithms,

simple combinations seem outdated and uncompetitive in the big data era. However, the results from

the recent M4 competition (Makridakis et al., 2020a) showed that simple combinations continue to

achieve relatively good forecasting performance and are still competitive. Speciﬁcally, a simple equal-

weight combination ranked the third for yearly time series (Shaub, 2019) and a median combination

of four simple forecasting models achieved the sixth place for point forecasting (Petropoulos and

Svetunkov, 2020). Genre et al. (2013) encompassed a variety of combination methods in the case of

forecasting GDP growth and the unemployment rate. They found that the simple average sets a tough

benchmark, with few combination schemes outperforming it. Moreover, simple combinations have a

lower computational burden and can be implemented more efﬁciently than alternatives. Therefore,

simple combination rules have been consistently the choice of many researchers and practitioners, and

provide a challenging benchmark to measure the effectiveness of the newly proposed weighted forecast

combination algorithms (e.g., Makridakis and Hibon, 2000; Stock and Watson, 2004; Makridakis et al.,

2020a; Montero-Manso et al., 2020; Kang et al., 2020a; Wang et al., 2022b).

Despite the ease of implementing simple combination schemes, their success still depends largely

on the choice of the forecasts to be combined. Intuitively, we prefer that the component forecasts fall on

opposite sides of the truth (the realization) (Bates and Granger, 1969; Larrick and Soll, 2006), so that the

forecast errors tend to cancel each other out. However, this rarely occurs in practice, as the component

forecasts are usually trained based on overlapping information sets and use similar forecasting

methods. If all component forecasts are established similarly based on the same, or highly overlapping

sets of information, forecast combinations are unlikely to be beneﬁcial for the improvement of forecast

accuracy. Mannes et al. (2014) and Lichtendahl and Winkler (2020) emphasized two critical issues

concerning the performance of simple combination rules: one for the level of accuracy (or expertise) of

the forecasts in the pool and another for diversity among individual forecasts. Involving forecasts with

low accuracy in the pool can decrease the combination performance. Additionally, a high degree of

diversity among component models facilitates the achievement of the best possible forecast accuracy

from simple combinations (Thomson et al., 2019). In conclusion, simple, easy-to-use combination rules

can provide good and robust forecasting performance, especially when properly considering issues

such as the accuracy and diversity of the individual forecasts to be combined.

2.2 Linear combinations

Despite the simplicity and performance of simple combination rules, it makes sense to assign

greater weight to the most accurate forecast methods. But how to choose those weights? The

problem of point forecast combinations can be deﬁned as seeking a one-dimensional aggregator

6

that integrates an N-dimensional vector of h-step-ahead forecasts involving the information up
to time T, ˆyT+h|T = (cid:0) ˆyT+h|T,1, ˆyT+h|T,2, . . . , ˆyT+h|T,N
, into a single combined h-step-ahead forecast
(cid:1), where N is the number of forecasts to be combined and wT+h|T is an
˜yT+h|T = C (cid:0) ˆyT+h|T; wT+h|T
N-dimensional vector of combining weights. The class of combination methods represented by the

(cid:1)(cid:48)

mapping, C, comprises linear and nonlinear combinations, as well as series-speciﬁc and cross-learning

combinations. Additionally, the combination weights can be static or time-varying along the forecast-

ing horizon. Below we discuss in detail the use of various approaches to determining combination

weights associated with individual forecasts.

Typically, the combined forecast is constructed as a linear combination of the individual forecasts,

which can be written as

˜yT+h|T = w(cid:48)

T+h|T ˆyT+h|T,

where wT+h|T = (cid:0)wT+h|T,1, . . . , wT+h|T,N
assigned to N individual forecasts.

(cid:1)(cid:48)

is an N-dimensional vector of linear combination weights

Optimal weights

The seminal work of Bates and Granger (1969) proposed a method to ﬁnd the so-called “optimal”

weights by minimizing the variance of the combined forecast error, and discussed only combinations

of pairs of forecasts. Newbold and Granger (1974) then extended the method to combinations of

more than two forecasts. Speciﬁcally, if the individual forecasts are unbiased and their error variances

are consistent over time, then the combined forecast obtained by a linear combination will also be
unbiased. Differentiating with respect to wT+h|T and solving the ﬁrst order condition, the variance of
the combined forecast error is minimized by taking

wopt

T+h|T

=

Σ−1

T+h|T1
T+h|T1

1(cid:48)Σ−1

,

(1)

where Σ
T+h|T is the N × N covariance matrix of the h-step forecast errors and 1 is an N-dimensional
unit vector. This is implemented, for example, in the R package ForecastComb (Weiss et al., 2018). In
practice, the elements of the covariance matrix Σ
T+h|T are usually unknown and need to be estimated.

It follows that if wT+h|T is determined by Equation (1), one can identify a combined forecast ˜yT+h|T
with no greater error variance than the minimum error variance of all individual forecasts. The fact

was further explored by Timmermann (2006) to illustrate the diversiﬁcation gains offered by forecast

combinations, by simply considering combinations of pairs of forecasts. Under mean squared error

(MSE) loss, Timmermann (2006) characterized the general solution of the optimal linear combination
weights by assuming a joint Gaussian distribution of the outcome yT+h and available forecasts ˆyT+h|T.

The loss assumed in Bates and Granger (1969) and Newbold and Granger (1974) is quadratic and

symmetric. Elliott and Timmermann (2004) examined forecast combinations under more general

loss functions accounting for asymmetries as well as skewed forecast error distributions. They

demonstrated that the optimal combination weights strongly depend on the degree of asymmetry in

the loss function and skewness in the underlying forecast error distributions. Subsequently, Patton

7

and Timmermann (2007) demonstrated that the properties of optimal forecasts established under MSE

loss are not generally robust under more general assumptions about the loss function. In addition,

the properties of optimal forecasts were generalized to consider asymmetric loss and nonlinear data

generating processes.

Regression-based weights

The seminal work by Granger and Ramanathan (1984) provided an important impetus for approximat-

ing the “optimal” weights under a linear regression framework. They recommended the strategy that

the combination weights can be estimated by ordinary least squares (OLS) in regression models having

the vector of past observations as the response variable and the matrix of past individual forecasts

as the predictor variables. Three alternative approaches imposing various possible restrictions were

considered

T+h|T ˆyT+h|T + ε T+h,
T+h|T ˆyT+h|T + ε T+h,

yT+h = w(cid:48)
yT+h = w(cid:48)
yT+h = wT+h|T,0 + w(cid:48)

T+h|T ˆyT+h|T + ε T+h.

s.t. w(cid:48)1 = 1,

(2)

(3)

(4)

The R package ForecastComb (Weiss et al., 2018) provides the corresponding implementations. The

constrained OLS estimation of the regression in Equation (2), in which the constant is omitted and the

weights are constrained to sum to one, yields results identical to the “optimal” weights proposed by

Bates and Granger (1969). Granger and Ramanathan (1984) further suggested that the unrestricted OLS

regression in Equation (4), which allows for a constant term and does not require the weights to sum

to one, is superior to the popular “optimal” method regardless of whether the constituent forecasts

are biased. However, De Menezes et al. (2000) argue that when using the unrestricted regression, one

needs to consider the stationarity of the series being forecast, the possible presence of serial correlation

in forecast errors (see also Diebold, 1988; Coulson and Robins, 1993), and the issue of multicollinearity.

Generalizations of the combination regressions have been considered in a large body of literature.

Diebold (1988) exploited serial correlated errors in the least squares framework by characterizing the

combined forecast errors as autoregressive moving average (ARMA) processes, leading to improved

combined forecasts. Gunter (1992) and Aksu and Gunter (1992) provided an empirical analysis

to compare the performance of various combination strategies, including the simple average, the

unrestricted OLS regression, the restricted OLS regression where the weights are constrained to sum

to unity, and the restricted OLS regression where the weights are constrained to be nonnegative. The

results revealed that constraining weights to be nonnegative is at least as robust and accurate as the

simple average and yields superior results compared to other combinations based on a regression

framework. Conﬂitti et al. (2015) addressed the problem of determining the combination weights by

imposing both restrictions (that the weights should be nonnegative and sum to one), which turns out

to be a special case of a LASSO regression. Coulson and Robins (1993) found that allowing a lagged

dependent variable in forecast combination regressions can achieve improved performance. Instead

of using the quadratic loss function, Nowotarski et al. (2014) applied the absolute loss function in

8

the unrestricted regression, also implemented in the ForecastComb package for R, to yield the least

absolute deviation regression which is more robust to outliers than OLS combinations.

Forecast combinations using changing weights have also been developed to solve various types

of structural changes in constituent forecasts. For instance, Diebold and Pauly (1987) explored

rolling weighted least squares as well as time-varying parameter techniques in the basic regression

framework, including both deterministic and stochastic time-varying parameters. Speciﬁcally, the

combination weights are either described as deterministic nonlinear (polynomial) functions of time

or allowed to involve random variation. They showed, via numerical examples based on various

types of structural change in the constituent forecasts, that time-varying weights substantially help

in improving forecasting ability in the presence of instabilities. Deutsch et al. (1994) allowed the

combination weights to evolve immediately or smoothly using switching regression models and

smooth transition regression models. Terui and Dijk (2002) generalized the regression method by

incorporating time-varying coefﬁcients that are assumed to follow a random walk process. The

generalized model can be interpreted as a state space model and then estimated using Kalman ﬁlter

updating. Following the spirit of Terui and Dijk (2002), Raftery et al. (2010) achieved an accelerated

inference process by using forgetting factors in the recursive Kalman ﬁlter updating.

Researchers have also worked on including many forecasts in a regression framework to take

advantage of many models. However, Chan et al. (1999) examined a wide range of combination

methods and showed that OLS combinations have very poor performance when N (the number of

forecasts to be combined) is very large. Factor methods are a common way of condensing information

when modeling and forecasting. They have also been used explicitly in forecast combination settings,
and are especially attractive when the number of forecasts to be combined is very large (N > T);
see, e.g., Chan et al. (1999) for a dynamic factor model framework for forecast combinations. The

common factors in approximate dynamic factor models can be estimated by principal components

(Stock and Watson, 1999). Principal components regression (PCR) is typically motivated as an ad hoc

tool for the solution of multicollinearity. Chan et al. (1999) and Stock and Watson (2004) explicitly

applied PCR to forecast combinations, resulting in a two-step procedure. The ﬁrst step extracts the

principal components, while the second step produces the ﬁnal forecasts utilizing OLS regression. The

superiority of PCR over OLS combinations was also supported by Rapach and Strauss (2008) and

Poncela et al. (2011). In turn, these methods relate to the question of whether one should forecast with

variables (competing point forecasts in our paper’s context), factors (extracted from the N competing

forecasts), or both; see, e.g., Castle et al. (2013) for a detailed discussion.

In large N cases, given the estimation problems that arise when N > T, researchers also frequently
relate forecast combinations to shrinkage-type approaches (whether frequentist or Bayesian) that
facilitate estimation of the forecast combination regression even when N > T; e.g. see Stock and
Watson (2004). Diebold and Shin (2019) considered methods for selection and shrinkage in regression-

based forecast combinations to address the estimation problem. They shed light on how machine

learning can be used to optimally combine a large set of forecasts by introducing a LASSO-based

procedure that consists of two steps. The ﬁrst step involves setting some combination weights to zero

using LASSO, and the second step shrinks the combination weights of the survivors toward equal

weights. Additionally, Aiolﬁ and Timmermann (2006) argued in favor of clustering the individual

forecasts using the k-means clustering algorithm based on their historical performance. For each

9

cluster, a pooled (average) forecast is computed, which precedes the estimation of combination weights

for the constructed clusters.

Performance-based weights

Estimation errors in the “optimal” weights and regression-based weights tend to be particularly large
due to difﬁculties in properly estimating the covariance matrix Σ
T+h|T, especially in situations with
many forecasts to combine. Instead, Bates and Granger (1969) suggested weighing the constituent

forecasts in inverse proportion to their historical performance, ignoring mutual dependence. In

follow-up studies, Newbold and Granger (1974) and Winkler and Makridakis (1983) generalized this

idea in the sense of considering more time series, more individual forecasts, and multiple forecast

horizons. Their extensive results demonstrated that combinations ignoring correlations are more

successful than those attempting to take account of correlations, and consequently reconﬁrmed Bates

and Granger’s (1969) argument that correlations can be poorly estimated in practice and should be

ignored when calculating combination weights.

Let eT+h|T = 1yT+h − ˆyT+h|T be the N-dimensional vector of h-step forecast errors computed from
the individual forecasts. Then the ﬁve procedures suggested in Bates and Granger (1969) for estimating
the combination weights when Σ

T+h|T is unknown, are extended to the general case as follows:

(5)

(6)

(cid:16)

∑T

t=T−ν+1 e2
(cid:16)

t|t−h,i

∑T

t=T−ν+1 e2

t|t−h,j

(cid:17)−1

(cid:17)−1 ;

wbg1

T+h|T,i

=

wbg2

T+h|T

=

∑N

j=1
ˆΣ−1

T+h|T1
T+h|T1

1(cid:48) ˆΣ−1

, where

( ˆΣ

T+h|T)i,j = ν−1

et|t−h,iet|t−h,j;

T
∑
t=T−ν+1
(cid:17)−1

t|t−h,i

(cid:16)

∑T

t=T−ν+1 e2
(cid:16)

∑N

j=1

∑T

t=T−ν+1 e2

t|t−h,j

wbg3

T+h|T,i

= α ˆwT+h−1|T−1,i + (1 − α)

(cid:17)−1 , where

0 < α < 1;

(7)

(cid:16)

∑N

j=1
ˆΣ−1

wbg4

T+h|T,i

=

wbg5

T+h|T

=

∑T

t=1 γte2
(cid:16)

t|t−h,i

∑T

t=1 γte2

t|t−h,j

(cid:17)−1

(cid:17)−1 , where γ ≥ 1;

T+h|T1
T+h|T1

1(cid:48) ˆΣ−1

, where

( ˆΣ

T+h|T)i,j =

∑T

t=1 γtet|t−h,iet|t−h,j
t=1 γt

∑T

(8)

(9)

and γ ≥ 1.

These weighting schemes differ in the factors, as well as the choice of the parameters, ν, α, and γ.
Correlations across forecast errors are either ignored by treating the covariance matrix Σ
T+h|T as a
diagonal matrix or estimated via the usual sample estimator (which may lead to quite unstable esti-
mates of Σ
T+h|T given highly correlated forecast errors). Some estimation schemes suggest computing
or updating the relative performance of individual forecasts over rolling windows of the most recent ν

observations, while others base the weights on exponential discounting with higher values of γ giving

larger weights to more recent observations. Consequently, these weighting schemes are well adapted

to allow a non-stationary relationship between the individual forecasting procedures over time (New-

bold and Granger, 1974). However, they tend to increase the variance of the parameter estimates and

work quite poorly if the data generating process is truly covariance stationary (Timmermann, 2006).

10

A broader set of combination weights based on the relative performance of individual forecasting

techniques has been developed and examined in a series of studies. For example, Stock and Watson

(1998) generalized the rolling window scheme in Equation (5) in the sense that the weights on the
individual forecasts are inversely proportional to the kth power of their MSE. The weights with k = 0
correspond to assigning equal weights to all forecasts, while more weights are placed on the best
performing forecasts by considering k ≥ 1. Other forms of forecast error measures, such as the root
mean squared error (RMSE) and the symmetric mean absolute percentage error (sMAPE), have also

been considered to lead to performance-based combination weights (e.g., Nowotarski et al., 2014;

Pawlikowski and Chorowska, 2020). A weighting scheme with the weights depending inversely

on the exponentially discounted errors was proposed by Stock and Watson (2004) as an upgraded

version of the scheme in Equation (8), and was used in several subsequent studies (e.g., Clark and

McCracken, 2010; Genre et al., 2013) to achieve gains from combining forecasts. The pseudo out-of-

sample performance used in these weighting schemes is commonly computed based on rolling or

recursive (expanding) windows (e.g., Stock and Watson, 1998; Clark and McCracken, 2010; Genre et al.,

2013). It is natural to adopt rolling windows in estimating the weights to deal with structural changes,

but the window length should not be too short without the estimates of the weights becoming too

noisy (Baumeister and Kilian, 2015).

Compared to constructing the weights directly using historical forecast errors, a new form of

combinations that is more robust and less sensitive to outliers was introduced based on the “ranking”

of individual forecasts. Again this kind of combination ignores correlations among forecast errors.

The simplest and most commonly used method in the class is to use the median forecast as the output.

Aiolﬁ and Timmermann (2006) constructed the weights proportional to the inverse of performance

ranks (sorted according to increasing order of forecast errors), which were later employed by Andrawis
et al. (2011) for tourism demand forecasting. The R package ForecastComb (Weiss et al., 2018) provides

tools for rank-based combinations. Another weighting scheme which attaches a weight proportional
to exp(β(N + 1 − i)) to the ith ordered constituent forecast was adopted in Yao and Islam (2008) and
Donate et al. (2013) to combine forecasts obtained from artiﬁcial neural networks (ANNs), where β is

a scaling factor. However, as mentioned by Andrawis et al. (2011), this class of combination methods

limits the weights to only a discrete set of possible values.

Criteria-based weights

Information criteria, such as Akaike’s information criterion (AIC, Akaike, 1974), the corrected Akaike

information criterion (AICc, Sugiura, 1978), and the Bayesian information criterion (BIC, Schwarz,

1978), are often used for model selection in forecasting. However, choosing a single model out of the

candidate model pool may be misleading because of the information loss from the alternative models.

An alternative approach proposed by Burnham and Anderson (2002) is to combine multiple models

based on information criteria to mitigate the risk of selecting a single model. It is also worth mentioning
that the R packages MuMIn (Barto ´n, 2022) and mmSAR (Guilhaumon, 2019) have been developed

to perform model selection and multimodel averaging based on the use of information-theoretic

approaches introduced by Burnham and Anderson (2002).

11

One such common approach is using Akaike weights. Speciﬁcally, in light of the fact that AIC

estimates the Kullback-Leibler distance (Kullback and Leibler, 1951) between a model and the true

data generating process, differences in the AIC can be used to weight different models, providing a

measure of the evidence for supporting a given model relative to other constituent models. Given N

individual models, the Akaike weight of model i can be derived as:

waic

T+h|T,i =

∑N
∆AICi = AICi − min

exp(−0.5∆AICi)
k=1 exp (−0.5∆AICk)
AIC(k).

,

k∈{1,2,··· ,N}

where

Akaike weights calculated in this manner can be interpreted as the probability that a given model

performs best at approximating the unknown data generating process, given the model set and the

available and historical data (Kolassa, 2011). Similar weights from AICc, BIC, and other variants with

different penalties, can be derived analogously.

The outstanding performance of weighted combinations based on information criteria has been

supported in several studies. For instance, Kolassa (2011) used weights derived from AIC, AICc

and BIC to combine exponential smoothing forecasts, and obtained superior accuracy over selecting

a model using the same information criteria. A similar strategy was adopted by Petropoulos et al.

(2018a) to separately explore the beneﬁts of bootstrap aggregation (bagging) for time series forecasting.

Additionally, an empirical study by Petropoulos et al. (2018b) showed that a weighted combination

based on AIC improves the performance of the statistical benchmark they used.

Bayesian weights

Some effort has been directed towards Bayesian approaches to updating forecast combination weights

in the face of new information gleaned from various sources. Recall that obtaining reliable estimates of
the covariance matrix Σ (the time and horizon subscripts are dropped for simplicity) of forecast errors
is a major challenge in practice regardless of whether correlations among forecast errors are ignored

or not. With this in mind, Bunn (1975) suggested the idea of Bayesian combinations on the basis of

the probability of each forecasting model performing the best on any given occasion. Considering

the beta and the Dirichlet distributions as the conjugate priors for the binomial and multinomial

processes respectively, the suggested non-parametric method performs well when there is relatively

little past data by means of attaching prior subjective probabilities to individual forecasts (Bunn, 1985;
De Menezes et al., 2000). ¨Oller (1978) presented another approach to using subjective probability in
a Bayesian updating scheme based on the self-scoring weights proportional to the evaluation of the

expert’s forecasting ability.

A different strand of research has also advocated the incorporation of prior information into the

estimation of combination weights, but with the weights being shrunk toward some prior mean under

a regression-based combination framework (Newbold and Harvey, 2004). Assuming that the vector of

forecast errors is normally distributed, Clemen and Winkler (1986) developed a Bayesian approach
with the conjugate prior for Σ represented by an inverted Wishart distribution with covariance matrix
Σ0 and scalar degrees of freedom ν0. Again we drop time and horizon subscripts for simplicity. If
the last T observations are used to estimate Σ, the combination weights derived from the posterior

12

distribution for Σ are

wcw =

Σ∗1
1(cid:48)Σ∗1

,

0 + T ˆΣ−1(cid:1)/(ν0 + T) is the precision matrix and ˆΣ is the sample covariance matrix.
where Σ∗ = (cid:0)ν0Σ−1
Compared to estimating Σ using the standard sample covariance estimator or treating it as a diagonal
matrix, the proposed approach provides a more stable estimation and allows for correlations between

methods. The subsequent work by Diebold and Pauly (1990) allowed the incorporation of the standard

normal-gamma conjugate prior by considering a normal regression-based combination

y = ˆYw + ε,

ε ∼ N (cid:0)0, σ2 I(cid:1) ,

where y and ε are T-dimensional vectors of historical data and residuals, respectively, and ˆY is the
T × N matrix of constituent one-step forecasts. This approach results in estimated combination
weights that can be viewed as a matrix-weighted average of those for the two polar cases (least squares

and prior weights), and it can provide a rational transition between the subjective and data-based

estimation of the combination weights. In light of the fact that Bayesian approaches have been

mostly employed to construct combinations of probability forecasts, we will elaborate on other newly

developed methods of determining combination weights from a foundational Bayesian perspective in

Section 3.7.

2.3 Nonlinear combinations

Linear combination approaches implicitly assume a linear dependence between constituent forecasts

and the variable of interest (Donaldson and Kamstra, 1996; Freitas and Rodrigues, 2006), and may not

result in the best forecast (Shi et al., 1999), especially if the individual forecasts come from nonlinear

models or if the true relationship between combination members and the best forecast is characterized

by nonlinear systems (Babikir and Mwambi, 2016). In such cases, it is natural to relax the linearity

assumption and consider nonlinear combination schemes of higher complexity; these have received

very limited research attention so far.

As Timmermann (2006) identiﬁed, two types of nonlinearities can be incorporated in forecast

combinations. One involves nonlinear functions of the individual forecasts, but with the unknown

parameters of the combination weights given in the linear form. The other allows a more general

combination with nonlinearities directly considered in the combination parameters. Neural networks

are often employed to estimate the nonlinear mapping because they offer the potential of learning the

underlying nonlinear relationship between the future outcome and individual forecasts. The design

of a neural network model is nevertheless time-consuming, and sometimes leads to overﬁtting and

poor forecasting performance as more parameters need to be estimated.

13

Donaldson and Kamstra (1996) used ANNs to obtain the combined forecasts ˜yT+h|T by the follow-

ing form

˜yT+h|T = β0 +

k
∑
j=1

βj ˆyT+h|T,j +

p
∑
i=1

δig(zT+h|Tγi),

(cid:32)

g(zT+h|Tγi) =

1 + exp

(cid:26)

(cid:16)

−

γ0,i +

N
∑
j=1

γ1,jzT+h|T,j

(cid:17)(cid:27)(cid:33)−1

,

(10)

(11)

where zT+h|T,j = ( ˆyT+h|T,j − ¯y)/ ˆσ, ¯y and ˆσ denote the in-sample mean and in-sample standard
deviation respectively, k ∈ {0, N}, and p ∈ {0, 1, 2, 3}. This approach permits special cases of both
purely linear combinations (k = N and p = 0) and nonlinear combinations (k = 0 and p (cid:54)= 0).
Building on this, Harrald and Kamstra (1997) proposed to evolve ANNs and demonstrated their

utility, but only using a single time series. Krasnopolsky and Lin (2012) and Babikir and Mwambi

(2016) employed neural network approaches with various activation functions to approximate the

nonlinear dependence of individual forecasts and achieve nonlinear mapping, resulting in variants of

Equation (10). The empirical results of nonlinear combinations from these studies generally dominate

those from traditional linear combination strategies, such as simple average, OLS weights, and

performance-based weights. However, the empirical evidence provided is based on fewer than ten

time series, possibly hand-picked to lead to this result. Additionally, these nonlinear combination

methods suffer from other drawbacks including the neglect of correlations among forecast errors, the

instability of parameter estimation, and the multicollinearity caused by the overlap in the information

sets used to produce individual forecasts. Thus, the performance of nonlinear combinations relative to

linear combinations needs further investigation.

Some researchers have sought to construct nonlinear combinations via the inclusion of an addi-

tional nonlinear term to cope with the case where the individual forecast errors are correlated. The

combination mechanism can be generalized to the following form

˜yT+h|T = β0 +

N
∑
j=1

βj ˆyT+h|T,j +

N
∑

i,j=1
i<j

πijvij,

where vij is some nonlinear combination of forecasts i and j. In this way, the general framework for
linear combinations is extended to deal with nonlinearities.

For example, Freitas and Rodrigues (2006) deﬁned vij as the product of individual forecasts from
different models, vij = ˆyT+h|T,i ˆyT+h|T,j, while Adhikari and Agrawal (2012) took into account the linear
correlations among the forecast pairs by including the term, vij = ( ˆyT+h|T,i − ¯yi)( ˆyT+h|T,j − ¯yj)/(σiσj)2,
where ¯yi and σi are the mean and standard deviation of the ith model, respectively. Moreover,
(cid:1), where ˆzi denotes the
Adhikari (2015) deﬁned the nonlinear term using vij = (cid:0) ˆzi − mij ˆzj
standardized ith individual forecast using the mean ¯yi and standard deviation σi, and the term mij
denotes the degree of mutual dependency between the ith and jth individual forecasts.

(cid:1) (cid:0) ˆzj − mji ˆzi

Clearly, combining forecasts nonlinearly requires further research. In particular, the forecasting

performance of the various proposed nonlinear combination schemes should be properly investigated

with a large, diverse collection of time series datasets along with appropriate statistical inference.

14

There is also a need to develop nonlinear combination approaches that take account of correlations

across forecast errors and the multicollinearity of forecasts.

2.4 Combining by learning

Stacked generalization (stacking, Wolpert, 1992) provides a strategy to adaptively combine the avail-

able forecasting models. Stacking is frequently employed on a wide variety of classiﬁcation tasks

(Zhou, 2012); in the time series forecast context, it uses the concept of meta-learning to boost forecasting

accuracy beyond that achieved by any of the individual models. Stacking is a general framework that

comprises at least two levels. The ﬁrst level involves training the individual forecasting models using

the original data, while the second and any subsequent levels utilize an additional “meta-model”,

using the prior level forecasts as inputs to form a set of forecasts. Thus, the stacking approach to

forecast combinations weights individual forecasts adaptively using meta-learning processes.

There are many ways to implement the stacking strategy. Its primary implementation is to combine

individual models in a series-by-series fashion. Individual forecasting models in the method pool are

trained using only data of the single series they are going to forecast, while their forecast outputs are

subsequently fed to a meta-model tailored for the target series to calculate the combined forecasts.

This means that n meta-models are required for n separate time series data. Unsurprisingly, regression-

based weight combinations discussed in Section 2.2 (e.g., Granger and Ramanathan, 1984; Gunter,

1992) fall into this category and can be viewed as the most simple, common learning algorithm used

in stacking. Instead of applying multiple linear regressions, Moon et al. (2020) suggested a PCR model

as the meta-model predominantly due to its desirable characteristics such as dimensionality reduction

and avoidance of multicollinearity between the input forecasts of individual models. Similarly,

LASSO regression, ANN, wavelet neural network (WNN), and support vector regression (SVR) can be

conducted in a series-by-series fashion to achieve the same goal (e.g., Donaldson and Kamstra, 1996;

Conﬂitti et al., 2015; Ribeiro et al., 2019; Ribeiro and Santos Coelho, 2020). One could use an expanding

or rolling window method to ensure that enough individual forecasts are generated for the training

of meta-models. Time series cross-validation, also known as “evaluation on a rolling forecasting

origin” (Hyndman and Athanasopoulos, 2021), is also recommended in the training procedures

for both individual models and meta-models to help with the parameter estimation. Nevertheless,

stacking approaches implemented in a series-by-series fashion still suffer from some limitations such

as requiring a long computation time and long time series, and inefﬁciently using the training data.

An alternative way to perform the stacking strategy sheds some light on the potential of cross-

learning. Speciﬁcally, the meta-model is trained using information derived from multiple time series

rather than employing only a single series, thus various patterns can be captured along different

series. The M4 competition organized by Spyros Makridakis et al. (2020a), comprising 100, 000 time

series, recognized the beneﬁts of cross-learning in the sense that the top three performing methods

of the competition utilize the information across the whole dataset rather than a single series. Cross-

learning can therefore be identiﬁed as a promising strategy to boost forecasting accuracy, at least when

appropriate strategies for extracting information from large, diverse time series datasets are adopted

(Kang et al., 2020c; Semenoglou et al., 2020). Zhao and Feng (2020) trained a neural network model

across the M4 competition dataset to learn how to combine individual models in the method pool.

15

They adopted the temporal holdout strategy to generate the training dataset and utilized only the

out-of-sample forecasts produced by standard individual models as the input for the neural network

model.

An increasing stream of studies has shown that time series features characterizing each series in a

dataset, provide valuable information for forecast combinations in a cross-learning fashion, leading to

an extension of stacking. Numerous software packages have been developed for time series feature
extraction, including the R packages feasts (O’Hara-Wild et al., 2021) and tsfeatures (Hyndman et al.,
2019), the Python packages Kats (Facebook’s Infrastructure Data Science team, 2021), tsfresh (Christ
et al., 2018) and TSFEL (Barandas et al., 2020), the Matlab package hctsa (Fulcher and Jones, 2017), and
the C-coded package catch22 (Lubba et al., 2019). These sets of time series features were empirically

evaluated by Henderson and Fulcher (2021).

The pioneering work by Collopy and Armstrong (1992) developed a rule base consisting of 99 rules

to combine forecasts from four statistical models using 18 time series features. Petropoulos et al. (2014)

identiﬁed the main determinants of forecasting accuracy through an empirical study involving 14

forecasting models and seven time series features. The ﬁndings can provide useful information for fore-

cast combinations. More recently, Montero-Manso et al. (2020) introduced a Feature-based FORecast
Model Averaging (FFORMA) approach available in the R package M4metalearning (Montero-Manso,
2019), which employs 42 statistical features (implemented using the R package tsfeatures) to estimate

the optimal weights for combining nine different traditional models trained per series based on an

XGBoost model. The FFORMA method reported the second-best forecasting accuracy in the M4

competition. Additionally, Ma and Fildes (2021) highlighted the potential of convolutional neural

networks as a meta-model to link the learnt features with a set of combination weights. Li et al. (2020)

extracted time series features automatically with the idea of time series imaging, then these features

were used for forecast combinations. Gastinger et al. (2021) demonstrated the value of a collection

of combination methods on a large and diverse amount of time series from the M3 (Makridakis and
Hibon, 2000), M4, M5 (Makridakis et al., 2022) datasets and FRED datasets1. In light of the fact that
it is not clear which combination strategy should be selected, they introduced a meta-learning step

to select a promising subset of combination methods for a newly given dataset based on extracted

features.

In addition to the time series features extracted from the historical data, it is crucial to look at the

diversity of the individual model pool in the context of forecast combinations (Batchelor and Dua,

1995; Thomson et al., 2019; Atiya, 2020; Lichtendahl and Winkler, 2020). An increase in diversity

among forecasting models has the potential to improve the accuracy of their combination. In this

respect, features measuring the diversity of the method pool should be included in the feature pool

to provide additional information possibly relevant to combining models. Lemke and Gabrys (2010)

calculated six diversity features and created an extensive feature pool describing both the time series

and the individual method pool. Three meta-learning algorithms were implemented to link knowledge

on the performance of individual models with the extracted features, and to improve forecasting

performance. Kang et al. (2021) utilized a group of features only measuring the diversity across

the candidate forecasts to construct a forecast combination model mapping the diversity matrix to

1The FRED (Federal Reserve Economic Data) dataset is openly available at https://fred.stlouisfed.org.

16

the forecast errors. The proposed approach yielded comparable forecasting performance with the

top-performing methods in the M4 competition.

As expected, the implementations of stacking in a cross-learning manner also come with their

limitations. The ﬁrst limitation is the requirement for a large, diverse time series dataset to enable

meaningful training outcomes. This issue can be addressed by simulating series on the basis of some
assumed data generating processes (Talagala et al., 2018) (implemented using the R package forecast,

Hyndman et al., 2021), or by generating time series with diverse and controllable characteristics
(Kang et al., 2020a) (implemented in the R package gratis, Kang et al., 2020b). Moreover, given the

considerable literature on feature identiﬁcation and feature engineering (e.g., Wang et al., 2009; Kang

et al., 2017; Lemke and Gabrys, 2010; Montero-Manso et al., 2020; Li et al., 2020), the feature-based

forecast combination methods naturally raise some issues yet to receive much research attention

including how to design an appropriate feature pool in order to achieve the best out of such methods,

and what is the best loss function for the meta-model.

It is also worth mentioning that many neural network models rely on a model combination

strategy, namely “ensembling” (see, e.g., Caruana et al., 2004, a popular work in the machine learning

context), that is applied internally to improve the overall forecasting performance. Due to the weak

learning process in deep learning models, the overall forecasting results heavily depend on the

combination of each forecasting result. They diversify the individual forecast via (1) varying the

training data, (2) varying the model pool, and (3) varying the evaluation metric. For example, the

N-BEATS model (Oreshkin et al., 2019) utilized different strategies to diversify the forecasting results.

For each forecasting horizon, individual models are trained with six window lengths. It also used

three metrics sMAPE, MASE and MAPE to validate each model. In the end, a variety of models are

used to make the median ensemble for results on the test set. One may refer to Ganaie et al. (2022) for

a general view of deep learning ensembles.

2.5 Which forecasts should be combined?

Including forecast methods with poor accuracy degrades the performance of the forecast combi-

nation. One prefers to exclude component forecasts that perform poorly and to combine only the

top performers. In judgmental forecasting, Mannes et al. (2014) highlighted the importance of the

crowd’s mean level of accuracy (expertise). They argued that the mean level of expertise sets a ﬂoor

on the performance of combining. The gains in accuracy from selecting top-performing forecasts for

combination have been investigated and conﬁrmed by a stream of articles such as Budescu and Chen

(2015) and Kourentzes et al. (2019). Lichtendahl and Winkler (2020) emphasized that the variance of

accuracy across time series, which provides an indication of the accuracy risk, exerts a great inﬂuence

on the performance of the combined forecasts. They suggested balancing the trade-offs between

the average accuracy and the variance of accuracy when choosing component models from a set of

available models.

Another key issue is diversity. Diversity among the individual forecasts is often recognized as one

of the elements required for accurate forecast combination (Batchelor and Dua, 1995; Brown et al.,

2005; Thomson et al., 2019). Atiya (2020) utilized the bias-variance decomposition of MSE to study

the effects of forecast combinations and conﬁrmed that an increase in diversity among the individual

17

forecasts is responsible for the error reduction achieved in combined forecasts. Diversity among

individual forecasts is frequently measured in terms of correlations among their forecast errors, with

lower correlations indicating a higher degree of diversity. The distance of top-performing clusters

introduced by Lemke and Gabrys (2010), where a k-means clustering algorithm is applied to construct

clusters, and a measure of coherence proposed by Thomson et al. (2019) are also considered as other

measures to reﬂect the degree of diversity among forecasts.

In an analysis of a winner-take-all forecasting competition, Lichtendahl Jr et al. (2013) found that the

optimal strategy for reporting forecasts is to exaggerate the forecasters’ own private information and

down-weight any common information. This exaggeration results in gains in the accuracy of the simple

average by amplifying the diversity of the individual forecasts. The gains were conﬁrmed by Grushka-

Cockayne et al. (2017a), who looked more closely at the impact of private-signal exaggeration on

forecast combinations, which translates into averaging forecasts that are overﬁtted and overconﬁdent.

Ideally, we would choose independent forecasts to amplify the diversity of the component forecasts

when forming a combination. However, the available individual forecasts are often produced based

on similar training, similar models and overlapping information sets, leading to highly positively cor-

related forecast errors. Including forecasts that have highly correlated forecast errors in a combination

creates redundancy and may result in unstable weights, especially in the class of regression-based

combinations (see Section 2.2). In this respect, using different types of forecasting models (e.g., statisti-

cal, machine learning, and judgmental), or different sources of information (e.g., exogenous variables),

can help improve diversity (Atiya, 2020). The results of the M4 competition reconﬁrmed the beneﬁts

of combinations of statistical and machine learning models (Makridakis et al., 2020a).

It is often suggested that a subset of individual forecasts be combined, rather than the full set of

forecasts, as there are decreasing returns to adding additional forecasts (Armstrong, 2001; Zhou et al.,

2002; Hibon and Evgeniou, 2005; Geweke and Amisano, 2011; Diebold and Shin, 2019; Lichtendahl

and Winkler, 2020). Simply put, many could be better than all. In this regard, given a method pool with

many forecasting models available, one can consider an additional step ahead of combining: subset

selection. Instead of using all available forecasts in a combination, the step aims to eliminate some

forecasts from the combination and select only a subset of the available forecasts.

The most common technique of subset selection is to include only the most accurate methods in

the combination, discarding the worst-performing individual forecasts (e.g., Granger and Jeon, 2004).

Mannes et al. (2014) investigated the gains in accuracy from this select-crowd strategy. Kourentzes

et al. (2019) proposed a heuristic, where we exclude component forecasts that show a sharp drop in

performance by using the outlier detection methods in boxplots. Their empirical results over four

diverse datasets showed that this subset selection approach outperforms selecting a single forecast or

combining all available forecasts. Nonetheless, the approach may suffer from a lack of diversity when

formulating appropriate pools.

Early studies considering diversity used forecast encompassing tests for combining forecasts (e.g.,

Kıs¸ınbay, 2010; Costantini and Pappalardo, 2010). The forecast encompassing literature ties in very

closely with forecast combinations. Several forecast encompassing tests have been developed to test

whether one forecast (or a set of forecasts) encompasses all information contained in another forecast

(or another set of forecasts); see, e.g., Chong and Hendry (1986) and Harvey et al. (1998). A classical

18

argument suggests that when ﬁxed weights are used (as in an average), only non-encompassed

individual models are worth combining (Diebold, 1989). However, Hendry and Clements (2004)

provided a counter example in processes subject to location shifts where previously encompassed

models may later dominate, while the earlier dominant model may later fail badly.

The diversity of an available forecast pool has occasionally been explicitly considered for subset

selection. Cang and Yu (2014) proposed an optimal subset selection algorithm for forecast combinations

based on mutual information, which takes account of diversity among different forecasts. More

recently, Lichtendahl and Winkler (2020) developed a subset selection approach comprising two

screens: one screen for removing individual models that perform worse than the Na¨ıve2 benchmark,

and another for excluding pairs of models with highly correlated forecast errors. In this way, both

accuracy and diversity issues are addressed when forming a combination.

Subset selection techniques take advantage of allowing many forecasts to be considered when

combining, reducing weight estimation errors and improving computational efﬁciency. However,

subset selection has received scant attention in the context of forecast combinations, and it is mainly

focused on trimming based on the principles of expertise. Therefore, automatic selection techniques

considering both expertise and diversity merit further attention and development.

One approach is to note that subset selection is equivalent to assigning zero weights to some

individual forecasts, which could be determined either statistically or judgmentally. Diebold and Shin

(2019) focused on weights that solve a penalized estimation problem. Speciﬁcally, they proposed a

two-step LASSO-based procedure that selects a subset of forecasts to combine in the ﬁrst step, and

shrinks the weights of the selected candidates toward equality. An alternative idea can be using

a pre-set threshold to select individual models with weights greater than the threshold to join the

subsequent combination; see, e.g., Zhou et al. (2002) and Wang et al. (2022b). Of course, there is no

guarantee that the zero weight over the training period will also be zero over the forecast horizon.

Hence, time-varying subset selection is certainly one solution to this problem and can be achieved by

applying a pre-set threshold to forecast combinations with time-varying weights (Li et al., 2022).

2.6 Forecast combination puzzle

Despite the explosion of a variety of popular and sophisticated combination methods, empirical

evidence and extensive simulations repeatedly show that the simple average with equal weights often

outperforms more complicated weighting schemes. This somewhat surprising result has occupied

a very large literature, including the early studies by Stock and Watson (1998), Stock and Watson

(2003), and Stock and Watson (2004), the series of Makridakis competitions (Makridakis et al., 1982;

Makridakis and Hibon, 2000; Makridakis et al., 2020a), and also the more recent articles by Blanc

and Setzer (2016) and Blanc and Setzer (2020), etc. Clemen (1989) surveyed the early combination

studies and raised a variety of issues that remain to be addressed, one of which is “What is the

explanation for the robustness of the simple average of forecasts?” In a recent study, Gastinger et al.

(2021) investigated the forecasting performance of a collection of combination methods on many time

series from diverse sources and found that the winning combination methods differ for the different

data sources, while the simple average strategies show, on average, more gains in improving accuracy

than other more complex methods. Stock and Watson (2004) coined the term “forecast combination

19

puzzle” for the phenomenon — theoretically sophisticated weighting schemes should provide more

beneﬁts than the simple average from forecast combination, while empirically the simple average has

been continuously found to dominate more complicated approaches to combining forecasts.

Most explanations of why simple averaging might dominate complex combinations in practice

have centered on the errors that arise when estimating the combination weights. For example, Tim-

mermann (2006) noted that the success of simple combinations is due to the increased parameter

estimation error with weighted combinations — simple combination schemes do not require esti-

mating combination parameters, such as weights based on forecast errors. Smith and Wallis (2009)

demonstrated that the simple average is expected to overshadow the weighted average in a situation

where the weights are theoretically equivalent. The results from simulations and an empirical study

showed the estimation cost of weighted averages when the optimal weights are close to equality, thus

providing an empirical explanation of the puzzle. Later, Claeskens et al. (2016) provided a theoretical

explanation for these empirical results. Taking the estimation of “optimal” weights (see Section 2.2)

into account, Claeskens et al. (2016) considered random weights rather than ﬁxed weights during

the optimality derivation and showed that, in this case, the forecast combination may introduce

biases in combinations of unbiased component forecasts and the variance of the forecast combination

may be larger than in the ﬁxed-weight case, such as the simple average. More recently, Chan and

Pauwels (2018) proposed a framework to study the theoretical properties of forecast combinations. The

proposed framework veriﬁed the estimation error explanation of the “forecast combination puzzle”

and, more crucially, provided additional insights into the puzzle. Speciﬁcally, the mean squared

forecast error (MSFE) can be considered as a variance estimator of the forecast errors which may not be

consistent, leading to biased results with different weighting schemes based on a simple comparison

of MSFE values. Blanc and Setzer (2020) explained why, in practice, equal weights are often a good

choice using the tradeoff between bias (reﬂecting the error resulting from underﬁtting training data

when choosing equal weights) and variance (quantifying the error resulting from the uncertainty

when estimating other weights).

Explaining the puzzle using estimation error requires a hypothesis that potential gains from the

“optimal” combination are not too large so that estimation error overwhelms the gains. Special cases,

such as where the covariance matrix of the forecast errors has equal variances on the diagonal, and all

off-diagonal covariances are equal to a constant, are illustrated by Timmermann (2006) and Hsiao and

Wan (2014) to arrive at equivalence between the simple average and the “optimal” combination. Elliott

(2011) characterized the potential bounds on the size of gains from the “optimal” weights over the

equal weights and illustrated that these gains are often too small to balance estimation error, providing

a supplementary explanation of the puzzle for the explanation of large estimation error.

Rather than focusing on the impact of combination weight estimation, Zischke et al. (2022) instead

explored the impact of sampling variability in forecast combinations. They demonstrated that,

asymptotically, the sampling variability in the performance of the combination forecast is driven

entirely by the variability arising from the estimation of the constituent models, and combination

weight estimation imparts no bias or variability to the performance of forecast combinations, which

lies in opposition to the ﬁnding of Claeskens et al. (2016). These ﬁndings imply that, when the

combination weights are theoretically equivalent, there will be little performance difference between a

20

sophisticated forecast combination and an equally weighted combination, providing new insights into

the “forecast combination puzzle”.

The examination and explanation of the “forecast combination puzzle” can provide decision

makers with the following guidelines to identify which combination method to choose in speciﬁc

forecasting problems.

• Estimation errors are identiﬁed as “ﬁnite-sample estimation effects” in Smith and Wallis (2009),

which suggests that an insufﬁciently small sample size may be unable to provide robust weight

estimates. Thus, if one has access to limited historical data, the simple average or estimated

weights with covariances between forecast errors being neglected are recommended. In addition,

alternative simple combination operators such as trimmed and winsorized means can be adopted

to eliminate extreme forecasts, and thus, offer more robust estimates than the simple average.

• Structural changes, which may cause different weight estimates in the training and evaluation

samples, tend to impact sophisticated combination approaches more than the simple average.

This case makes the simple average the better choice. The forecast combinations using changing

weights can also be considered as a means to cope with structural changes, as suggested in

Diebold and Pauly (1987) and Deutsch et al. (1994).

• If one has access to many component forecasts, the PCR and the clustering strategy (for details,

see Section 2.2) might be useful to diminish estimation errors and solve the multicollinearity

problem by reducing the number of parameters need to be estimated.

• Involving time series features (see Section 2.4) and diverse individual forecasts (see Section 2.5)

in the process of weight estimation can enlarge the gains of forecast combinations, providing a

possible way to untangle the “forecast combination puzzle”.

In summary, forecasters are encouraged to analyze the data prior to identifying the combination

strategy and to choose combination rules tailored to speciﬁc forecasting problems.

3 Probabilistic forecast combinations

3.1 Probabilistic forecasts

In recent years, probabilistic forecasts have received increasing attention. For example, the recent

Makridakis competitions, the M4 and the M5 Uncertainty (Makridakis et al., 2020b) competitions,

encouraged participants to provide probabilistic forecasts of different types as well as point forecasts.

Probabilistic forecasts are appealing for enabling optimal decision-making with better understanding

of uncertainties and the resulting risks. A brief survey of extensive applications of probabilistic

forecasting was offered by Gneiting and Katzfuss (2014).

Probabilistic forecasts can be reported in various forms including density forecasts, distribution

forecasts, quantiles, and prediction intervals, and how to combine them can vary. For example,

although a quantile forecast is the inverse of the corresponding forecast represented by the cumulative

distribution function, the combined quantile forecast and the combined probability forecast may

21

not be equivalent. Simple examples of averaging quantiles and probabilities with equal weights are

provided by Lichtendahl et al. (2013).

Interval forecasts form a crucial special case and are often constructed using quantile forecasts

where the endpoints are speciﬁc quantiles of a forecast distribution. For example, the lower and upper
endpoints of a central (1 − α) × 100% prediction interval can be deﬁned via the quantiles at levels α/2
and 1 − α/2.

As with point forecasts, combining multiple probabilistic forecasts allows for diverse information

sets and different types of forecasting models, as well as the mitigation of potential misspeciﬁcations

derived from a single model. Empirical studies suggest that the relative performance of different

models often varies over time due to structural instabilities in the unknown data generating process

(e.g., Billio et al., 2013). Thus, there has been a growing interest in bringing together multiple

probabilistic forecasts to produce combined forecasts that integrate information from separate sources.

3.2 Scoring rules

Decision makers mainly focus on accuracy when combining point forecasts, while other measures such

as calibration and sharpness need to be considered when working with combinations of probabilistic

forecasts (Gneiting et al., 2007; Gneiting and Raftery, 2007; Lahiri et al., 2015). Calibration concerns

the statistical consistency between the probabilistic forecasts and the corresponding realizations, thus

serving as a joint property of forecasts and observations. In practice, a probability integral transform

(PIT) histogram is commonly employed informally as a diagnostic tool to assess the calibration of

probability forecasts regardless of whether they are continuous (Dawid, 1984; Diebold et al., 1998) or

discrete (Gneiting and Ranjan, 2013): A uniform histogram indicates a probabilistically calibrated

forecast. Sharpness refers to the concentration of probabilistic forecasts, and thus serves as a property

of the forecasts only; the sharper a forecast is, the better it is. Sharpness is easily comprehended when

considering prediction intervals: the sharper the forecasts, the narrower the intervals. In the case of

probability forecasts, sharpness can be assessed in terms of the width of central prediction intervals.

For more thorough deﬁnitions and diagnostic tools of calibration and sharpness, we refer to Gneiting

and Katzfuss (2014).

According to Gneiting et al. (2007), the intent of probabilistic forecasting is to maximize the sharpness

of the forecast distributions subject to calibration based on the available information set. In this light,

scoring rules that reward both calibration and sharpness are appealing in the sense of providing

summary measures for the quality of probabilistic forecasts, with a higher score indicating a better

forecast. For a probabilistic forecast F, a scoring rule is proper if it satisﬁes the condition that the
expected score for an observation drawn from distribution G is maximized when F = G. It is strictly
proper if the maximum is unique. Gneiting and Raftery (2007) provides an excellent review and

discussion on a diverse collection of proper scoring rules for probabilistic forecasts.

The schemes for combining multiple probabilistic forecasts have evolved from a simple distribution

mixture to more sophisticated combinations accounting for correlations between distributions. Which

type of strategy one might choose to use depends largely on the computational burden, and the overall

performance of the combined forecasts with regard to accuracy, calibration, and sharpness.

22

3.3 Linear pooling

Probability forecasts strive to predict the probability distribution of quantities or events of interest.

In line with the notations in previous sections, here we consider N individual forecasts speciﬁed
as cumulative probability distributions of a random variable Y at time T + h, denoted Fi(yT+h|IT),
i = 1, . . . , N, using the information available up to time T, IT. One popular approach is to directly take
a mixture distribution of these N individual probability forecasts with estimated weights, neglecting

correlations between these individual components. This approach is commonly referred to as the

“linear opinion pool” in the literature on combining experts’ subjective probability distributions, dating

back at least to Stone (1961). The linear pool of probability forecasts is deﬁned as the ﬁnite mixture

˜F(yT+h|IT) =

N
∑
i=1

wT+h|T,iFi(yT+h|IT),

(12)

where wT+h|T,i is the weight assigned to the ith probability forecast. These weights are often set to
be non-negative and sum to one to guarantee that the pooled forecast preserves properties of both

non-negativity and integrating to one. The pooled probability forecast satisﬁes numerous properties

such as the unanimity property (if all individual forecasters agree on a probability then the pooled

forecast agrees also); see Clemen and Winkler (1999) for more details.

Linear pooling of probability forecasts allows us to accommodate skewness and kurtosis (fat tails),

and also multi-modality, even under normal distributions of individual forecasts; see Wallis (2005)

and Hall and Mitchell (2007) for further discussion on this point.

Deﬁne µi and σ2

i as the mean and variance of the ith component forecast distribution and drop
the time and horizon subscripts for simplicity. Then the linear combined probability forecast has the

mean and variance,

˜µ =

wiµi,

N
∑
i=1
N
∑
i=1

and

˜σ2 =

wiσ2

i +

N
∑
i=1

wi (µi − ˜µ)2 .

(13)

(14)

Note that the mean of the combination distribution is equivalent to the linear combination of the

individual means. Thus, the associated combination point forecast is consistent with the linear

combination point forecast.

However, the variance of the combination distribution is larger than the linear combination of the

individual variances when the individual means differ. Consequently, the common strategy of seeking

diverse forecasts may harm the probabilistic forecast, while helping the point forecast; see Ranjan and

Gneiting (2010) for a theoretical illustration and simulation study. Simply put, as the diversity among

individual probability forecasts increases, the mixed forecast will lose sharpness and may become

under-conﬁdent because of the spread driven by the disagreement between the individual probability

forecasts (Hora, 2004; Wallis, 2005; Ranjan and Gneiting, 2010).

Even in the ideal case in which individual forecasts are well calibrated, the resulting linear pooling

combination may be poorly calibrated. Theoretical aspects of this ﬁnding and properties of linear

23

pools of probability forecasts have been further studied in Hora (2004), Ranjan and Gneiting (2010),

and Lichtendahl et al. (2013).

On the other hand, Hora (2004) demonstrated, both from theoretical and empirical aspects, that

linear pooling may work to provide better calibrated forecasts than the individual distributions when

individual forecasts tend to be overconﬁdent. This ﬁnding helps to account for the success of linear

pooling in varied applications. Jose et al. (2014) highlighted that if the experts are overconﬁdent but

have low diversity, the linear pool may remain overconﬁdent. Lichtendahl et al. (2013) identiﬁed three

factors that manipulate the calibration of the probability forecast derived from linear pooling: (i) the

number of constituent forecasts, (ii) the degree to which the constituent forecasts are overconﬁdent,

and (iii) the degree of the constituents’ disagreement on the location (e.g., mean) of the distribution.

In principle, probability forecasts can be recalibrated before or after the pooling to correct for

miscalibration (Turner et al., 2014). However, it is challenging to appraise the degree of miscalibration,

which may vary considerably among different forecasts and over time, and therefore to recalibrate

accordingly. Some effort has been directed toward the development of alternative combination

methods to address the calibration issue. For example, Jose et al. (2014) suggested the “trimmed

opinion pool”, which trims away some individual forecasts from the “linear opinion pool” before

mixing the component forecasts. Speciﬁcally, exterior trimming that trims away forecasts with

low or high means values serves as a way to address under-conﬁdence by decreasing the variance.

Conversely, interior trimming that trims away forecasts with moderate means is suggested to mitigate

overconﬁdence via increasing the variance. The improvement in forecasting performance offered by

trimming was conﬁrmed by Grushka-Cockayne et al. (2017a) at a more foundational level.

Some researchers prefer nonlinear alternatives, including a generalized linear pool, the spread-

adjusted linear pool, and the beta-transformed linear pool, in terms of delivering better calibrated

combined probability forecasts; these are discussed in Section 3.5. Instead of mixing probability

forecasts mentioned above, Lichtendahl et al. (2013) recommended averaging quantile forecasts (see

Section 3.9) based on the supportive results both theoretically and empirically.

The key practical issue determining the success (or failure) of linear pooling is how the weights

for the individual probability forecasts in the ﬁnite mixture should be estimated. As with point

forecast combinations, equal weights are worthy of consideration, while determining optimal weights

is particularly challenging in the case of having access to probability forecasts with limited historical

data.

Linear pooling with equal weights is easy to understand and implement, commonly yielding

robust and stable outcomes. For reviews, see, e.g., Wallis (2005) and O’Hagan et al. (2006). A leading

example is the survey of professional forecasters (SPF) in the US, which publishes mixed probability

forecasts (in the form of histograms) for inﬂation and GDP growth using equal weights. As the

experience of combining point forecasts has taught us, the equally weighted approach often turns out

to be hard to beat. An important reason is that it avoids parameter estimation error that often exists in

weighted approaches; see Section 2.6 for more details and illustrations.

Motivated by the “optimal” weights obtained in point forecast combinations by minimizing the

MSE loss (see Section 2.2), Hall and Mitchell (2007) proposed obtaining the set of weights by mini-

mizing the Kullback-Leibler information criterion (KLIC) distance between the combined probability

24

forecast density ˜f (yτ+h|Iτ) and the true (but unknown) probability density f (yτ+h), τ = 1, . . . , T. The
KLIC distance is deﬁned as

(cid:90)

KLIC =

f (yτ+h) log

(cid:27)

(cid:26) f (yτ+h)
˜f (yτ+h|Iτ)

dyτ+h = E (cid:2)log f (yτ+h) − log ˜f (yτ+h|Iτ)(cid:3) .

Under the asymptotic assumption that the number of observations T grows to inﬁnity, the problem

of minimizing the KLIC distance reduces to the maximization of the average logarithmic score of the
combined probability forecast. Therefore, the optimal weight vector wT+h|T is given by

wT+h|T = argmax

w

1
T − h

T−h
∑
t=1

log ˜f (yt+h|It),

(15)

where wT+h|T = (cid:0)wT+h|T,1, . . . , wT+h|T,N
to estimate the unknown true probability distribution, and therefore simpliﬁes the weight estimation

. The use of the logarithmic scoring rule eliminates the need

(cid:1)(cid:48)

for the component forecasts. This was followed by Pauwels and Vasnev (2016) documenting the

properties of the optimal weights in Equation (15), centering on the asymptotic assumption used

by Hall and Mitchell (2007). Their simulations and empirical results indicated that the combination

with optimal weights is inferior for small T, while it is valid in minimizing the KLIC distance when

T is sufﬁciently large. Therefore, a sufﬁcient training sample is recommended when solving the

optimization problem.

Following in the footsteps of Hall and Mitchell (2007), many extensions and reﬁnements of the

combination strategy have been suggested. Conﬂitti et al. (2015) devised a simple iterative algorithm

to compute the optimal weights in Equation (15). The algorithm scales well with the dimension N, and

hence enables the combination of many individual probability forecasts. Geweke and Amisano (2011)

provided a Bayesian perspective on an optimal linear pool, and provided a theoretical justiﬁcation for

the use of optimal weights. Li et al. (2022) conducted time-varying weights based on time-varying

features from historical information, where the weights in the forecast combination were estimated via

Bayesian logarithmic predictive scores. Jore et al. (2010) put forward an exponential weighting scheme

based on the recursive weights constructed using the relative past performance of each individual

probability forecast in terms of the logarithmic score. In contrast to the optimal opinion pool based on

the weights in Equation (15), in this case, the logarithmic score of the combined probability forecast

is not necessarily maximized. The logarithmic scoring rule is appealing as it intuitively assigns a

higher weight to a component forecast that better ﬁts the realized value. On the other hand, forecast

combinations with weights optimized by minimizing the continuously ranked probability score (CRPS,

Gneiting and Raftery, 2007), which is a strictly proper scoring rule for distribution forecasts, have been

considered in some research, see, e.g, Raftery et al. (2005), Thorey et al. (2017), and Thorey et al. (2018).

Furthermore, some special treatments were given to accommodate probability forecast combi-

nations in applications such as energy forecasting, retail forecasting, and economic forecasting. For

instance, Opschoor et al. (2017) extended the idea of optimal combinations but estimated optimal

weights by either maximizing the censored likelihood scoring rule (Diks et al., 2011) or minimizing

a weighted version of the CRPS, allowing forecasters to limit themselves to a speciﬁc region of the

target distribution. For example, we are more likely to be interested in avoiding out-of-stocks when

working with retail forecasting. The tail of the distribution is also the main feature of interest when

25

measuring downside risk in equity markets. Additionally, Zischke et al. (2022) showed that when

forecasting during times of high volatility, forecast combinations produced by optimizing according to

the censored likelihood scoring rule always lead to a better out-of-sample performance than “optimal”

forecast combinations with weights optimized using the logarithmic score, which lends support to the

use of a scoring rule that prioritizes accurate forecasts in a speciﬁc region. Diebold et al. (2022) instead

constructed regularized mixtures of density forecasts using a variety of objectives and regularization

penalties. The optimal regularization tends to spread probability mass from the center into both tails

of the distribution, correcting for overconﬁdence and adjusting kurtosis. Besides, Pauwels et al. (2020)

proposed an approach to computing the optimal weights by maximizing the average logarithmic

score subject to additional higher moments restrictions. Through constrained optimization, the com-

bined probability forecast can preserve speciﬁc characteristics of the distribution, such as fat tails or

asymmetry. Martin et al. (2021) looked at mode misspeciﬁcation, and showed via simulation and

empirical results that score-speciﬁc optimization of linear pooling weights does not always achieve

improvements in forecasting accuracy.

3.4 Bayesian model averaging

Bayesian model averaging (BMA) provides an alternative means of mixing individual probability

forecasts with respect to their posterior model probabilities. BMA offers a conceptually elegant

and logically coherent solution to the issue of accounting for model uncertainty (see, e.g., Leamer,

1978; Draper, 1995; Raftery et al., 1997; Garratt et al., 2003). Under this approach, the posterior

probability forecast is computed by mixing a set of individual probability forecasts distributions,
Fi(yT+h|IT) = F(yT+h|IT, Mi), from model Mi, and can be given as

˜F(yT+h|IT) =

N
∑
i=1

P(Mi|IT)F(yT+h|IT, Mi),

(16)

where P(Mi|IT) is the posterior probability of model Mi. The decision makers update the prior
probability of model Mi being the true model, P(Mi), via Bayes’ Theorem to compute the posterior
probability

where

P(Mi|IT) =

P(Mi)P(IT|Mi)
i=1 P(Mi)P(IT|Mi)

∑N

,

P(IT|Mi) =

(cid:90)

θi

P (θi|Mi) P (IT|Mi, θi) dθi

(17)

(18)

is the marginal likelihood of model Mi, P (θi|Mi) is the prior on the unknown parameters θi conditional
on model Mi, and P (IT|Mi, θi) is the likelihood function of model Mi. See, e.g., Koop (2003) for
textbook illustrations of BMA.

BMA in Equation (16) can be viewed as a form of linear pooling of individual probability forecasts

(12), weighted by their posterior model probabilities given in Equation (17). Note that the weights

characterized by posterior probabilities do not account for correlations among individual probability

26

forecasts. The approach provides a general way to deal with model uncertainty and does not necessar-

ily require the use of conjugate families of distributions. The BMA procedure is consistent in the sense
that the posterior probability in Equation (17) indicates the probability that model Mi is the best under
the KLIC measure distance and shows how well the model ﬁts the observations (Fern´andez-Villaverde

and Rubio-Ramırez, 2004; Raftery et al., 2005; Wright, 2008).

While theoretically attractive, BMA suffers from three major challenges when implemented in

practice. One is how to properly specify the model space of interest to avoid model incompleteness. It

is often impractical to cover the complete set of models when the number of possible models is large

or their structures are complex. This difﬁculty can mostly be resolved via the selection of a subset

of models that are supported by the data or through stochastic search algorithms over the model

space; see Hoeting et al. (1999) and Koop and Potter (2003) for more details on model search strategies.

A second well-known challenge relates to the elicitation of two types of priors (on parameters and

models) for many models of interest (Moral-Benito, 2015; Aastveit et al., 2019). Another practical

concern lies in the computation of the integrals in Equation (18). The integrals that are required for

the derivation of the marginal likelihood may be analytically intractable in many cases, except for

the generalized linear regression models with conjugate priors. The Laplace method as well as the

Markov chain Monte Carlo (MCMC) methods are therefore frequently used to provide an excellent
approximation to P(IT|Mi); see, e.g., Hoeting et al. (1999) and Bassetti et al. (2020) for discussions of
these approximations.

One drawback of the BMA approach is the implicit assumption that the true model is included

in the model space to be considered (Wright, 2008). Under this assumption, when the sample size

tends to inﬁnity, the posterior probabilities converge to zero, except for one which converges to unity.

Thus, BMA reduces to model selection for large sample size, with the best model (which is the true

model if that exists, but is still well-deﬁned if none of the models is in fact true) receiving a weight

very close to one; see Geweke and Amisano (2010) for an empirical demonstration. In this regard, the

combined forecast derived from BMA may be misspeciﬁed when the model space is incomplete (i.e.

all models under consideration are incorrect), arising the issue of model incompleteness. Recently,

Yao et al. (2018) took the idea of stacking from the literature on point forecast combinations (see

Section 2.4) and generalized it to the combinations of forecast distributions in the Bayesian setting,

which can essentially be regarded as a minor tweak on BMA. However, as the critique given at the

end of Yao et al. (2018) says, averaging distribution functions may be inferior to averaging quantiles

(see Section 3.9), especially when the combination problem is more like an information aggregation

problem rather than a BMA problem, and BMA (or minor tweaks on it) does not seem like the right

framework since we are almost always in a world where there is no true model.

In contrast, optimal weights as deﬁned in Equation (15) do not suffer from the issue of model

incompleteness because the weights need not converge to zero or unity regardless of whether the

component models are correct or not, which allows for a convex combination (rather than a selection)

of the individual probability forecasts distributions. In a binary-event context, Lichtendahl Jr et al.

(2022) introduced a new class of Bayesian combinations in which stacking is used to form the approach

by aggregating the probabilities provided by the experts or models. But it should not be confused

with BMA and the approach developed by Yao et al. (2018), since it does not have to assume, as

BMA does, that one of the models being combined is the true model, and its setting is information

27

aggregation rather than model selection. Lichtendahl Jr et al. (2022) showed that extremizing (i.e.

shifting the average probability closer to its nearest extreme, see, e.g., Satop¨a¨a et al., 2016) is not always

appropriate when combining binary-event forecasts.

The other drawback of the BMA approach may be related to the ﬁxed probabilities assigned to

component models, as documented in Aastveit et al. (2019). The uncertainty of the weights is ignored

in this case, leading to unstable combined forecasts in a forecasting environment characterized by

large instability and structural changes in the forecast performance of the individual models. Thus,

it is plausible to let the pooling weights evolve over time. Raftery et al. (2010) developed a model

combination strategy for doing dynamic model averaging (DMA) which allows for the forecasting

model as well as the coefﬁcients in each model to evolve over time. Considering multiple models, the
goal of DMA is to calculate the probabilities that the process is governed by model Mi for i = 1, . . . , N
at time T + 1, given the information available up to time T, and average forecasts across individual
models using these probabilities. When the forecasting model and model parameters do not change,

DMA reduces to a recursive implementation of standard BMA. The strategy advocated by Raftery

et al. (2010) can also be used for dynamic model selection (DMS), where a single model with the

highest probability is selected and used to forecast. Note that these calculated probabilities will vary

over time and, thus, different forecasting models hold at each point in time. Such speciﬁcations are of

particular interest in economics, see, e.g., Koop and Korobilis (2012) and Del Negro et al. (2016) for

notable macroeconomic applications. One contribution of Raftery et al. (2010) is that a forgetting factor

is used to develop a computationally efﬁcient recursive algorithm that allows for fast calculation of

the required probabilities when model uncertainty and the number of models considered are large.

3.5 Nonlinear pooling

Despite their simplicity and popularity, the classical linear pooling methods have several shortcomings,

such as the calibration problem discussed previously. A linear pooling of probability forecasts increases

the variance of the forecasts and may result in a suboptimal solution, lacking both calibration and

sharpness. To address these shortcomings, several nonlinear alternatives to linear pooling methods

have been developed for recalibration purposes.

Motivated by the seminal work of Dawid et al. (1995), Gneiting and Ranjan (2013) developed

the generalized linear pool (GLP) to incorporate a parametric family of combination formulas. Let
Fi(yT+h|IT) denote the cdf of the probability forecast (i = 1, . . . , N), and ˜F(yT+h|IT) denote the cdf of
the combined forecast. The generalized pooling scheme takes the following form

˜F(yT+h|IT) = g−1

wT+h|T,ig(cid:0)Fi(yT+h|IT)(cid:1)

(cid:19)

,

(cid:18) N
∑
i=1

where wT+h|T,1, . . . , wT+h|T,N are nonnegative weights that sum to one, and g denotes a continuous
and strictly monotonic function with the inverse g−1. The linear, harmonic and logarithmic (geometric)
pools become special cases of the GLP for g(x) = x, g(x) = 1/x and g(x) = log(x), respectively.
Gneiting and Ranjan (2013) highlighted that the generalized pooling strategy may fail to be sufﬁciently

ﬂexibly dispersive for calibration.

28

As a result, they also proposed the spread-adjusted linear pool (SLP) to allow one to address the
i (yT+h − ηi|IT) and
i (yT+h − ηi|IT), where ηi is the unique median of Fi(yT+h|IT). Then the SLP has the

calibration problem. Deﬁne F0
fi(yT+h|IT) = f 0
combined cdf and the corresponding density,

i and corresponding density f 0

i via Fi(yT+h|IT) = F0

˜F(yT+h|IT) =

N
∑
i=1

wT+h|T,iF0
i

˜f (yT+h|IT) =

1
c

N
∑
i=1

wT+h|T,i f 0
i

(cid:12)
(cid:18) yT+h − ηi
(cid:12)
(cid:12)
c
(cid:12)
(cid:18) yT+h − ηi
c

(cid:19)

IT

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

and

IT

respectively, where wT+h|T,1, . . . , wT+h|T,N are nonnegative weights with ∑N
i=1 wT+h|T,i = 1, and c is a
strictly positive spread adjustment parameter. The traditional linear pool arises as a special case for
c = 1. A value of c < 1 is suggested for neutrally conﬁdent or underconﬁdent component forecasts,
while a value c ≥ 1 is suggested for overconﬁdent components. Moreover, one can introduce spread
adjustment parameters varying with the components in case the degrees of miscalibration of the

components differ substantially.

The cumulative beta distribution is widely employed for recalibration because of the ﬂexibility of

its shape (see, e.g., Graham, 1996). Ranjan and Gneiting (2010) introduced a beta-transformed linear

pool (BLP) that merges the traditional linear pool with a beta transform to achieve calibration. The

BLP takes the form

˜F(yT+h|IT) = Bα,β

wT+h|T,iFi(yT+h|IT)

(cid:19)

,

(cid:18) N
∑
i=1

where wT+h|T,1, . . . , wT+h|T,N are nonnegative weights that sum to one, and Bα,β is the cdf of the
beta distribution with the shape parameters α > 0 and β > 0. Full generality of the BLP enables
an asymmetric beta-transformation on the basis of the linear pooling of probability forecasts. In

its most simplistic case, the BLP approach nests the traditional linear pool, under the restriction
α = β = 1. The beta-transformation tunes up a linear pooled probability forecast if it is larger than
0.5 and tunes it down otherwise when imposing the constraint α = β ≥ 1. The approach can be
used to combine probability forecasts from both calibrated and uncalibrated sources. The estimates

of the beta-transformation along with the mixture weights for linear pooling can be obtained by

maximum likelihood, as suggested by Ranjan and Gneiting (2010). Recent work by Lahiri et al. (2015)

demonstrated the superiority of the BLP approach, based on identifying the most valuable individual

forecasts by a Welch-type test, over the equally weighted approach with respect to calibration and

sharpness.

To achieve improved calibration properties, Bassetti et al. (2018) proposed a Bayesian nonparamet-

ric approach, which is based on Gibbs and slice sampling, to realize the calibration and combination

of probability forecasts by introducing an additional beta mixture in the BLP method. The resulting

predictive cdf is

˜F(yT+h|IT) =

K
∑
k=1

ωkBαk,βk

(cid:18) N
∑
i=1

wT+h|T,kiFi(yT+h|IT)

(cid:19)

,

29

where ω1, . . . , ωK denote beta mixture weights. The proposed approach enables one to treat the
parameter K as bounded or unbounded and it reduces to the BLP for K = 1. The Bayesian inference
approach achieved a compromise between parsimony and ﬂexibility and produced well calibrated

and accurate forecasts in their simulations and the empirical examples, outperforming the linear pool

substantially.

The essence of these nonlinear pooling methods is to perform various transformations, that may

be nonlinear, to either the component forecasts or the linearly pooled forecasts, in order to restore

calibration and sharpness. Kapetanios et al. (2015) generalized the literature by incorporating the

dependence of the mixture weights on the variable one is trying to forecast, allowing the weights

themselves to introduce the nonlinearities and thus leading to outcome-dependent density pooling.

Clearly, the forecast performance of nonlinear pooling approaches largely depends on diverse factors,

including the features of the target data, mixture component models, and training periods, and thereby

deserves further research. This is in agreement with Baran and Lerch (2018) who investigated the

performance of state-of-the-art forecast combination methods through case studies and found no

substantial differences in forecast performance between the simple linear pool and the theoretically

superior but cumbersome nonlinear pooling approaches.

3.6 Meteorological ensembles

The term “combination” and “ensemble” are often used interchangeably in the literature on forecast

combinations. However, “ensemble” was originally developed in the meteorological literature in a

distinct way from the combinations of probabilistic forecasts that we have discussed so far.

Instead of combining multiple probabilistic forecasts available for forecasters, as with the ap-

proaches reviewed in preceding sections, an ensemble weather forecast is constructed from a set of

point forecasts of the same weather quantity of interest, based on perturbed initial atmospheric states

(e.g., Maqsood et al., 2004; Gneiting and Raftery, 2005) and/or different model formulations (e.g.,

Buizza et al., 1999; Buizza et al., 2005). In this light, two major sources of forecast uncertainty, initial

condition uncertainty resulting from the chaotic nature of the atmosphere, and model uncertainty

arising from imperfect numerical models, are addressed (Lorenz, 1963; Weigel et al., 2008; Baran, 2014).

This enables a measure of uncertainty to be attached and makes an ensemble weather forecast more

valuable than a single “deterministic” forecast, providing an inherently probabilistic assessment.

A meteorological ensemble forecast is a probabilistic forecast in the sense described here, assuming

that there is no inherent uncertainty other than that contained in the initial conditions and the model

formulation. In contrast, most statistical forecasting methods include an important additional source

of uncertainty due to random noise innovations, but do not usually include uncertainty due to initial

conditions. The distinction is important enough, and the literature sufﬁciently distinct, that we have

chosen to discuss meteorological ensemble forecasts in this separate section.

It has been demonstrated that the raw meteorological ensemble forecasts typically present system-

atic errors regarding bias (Atger, 2003; Mass, 2003) and dispersion (Buizza et al., 2005; Sloughter et al.,

2010), with a tendency for the truth frequently falling outside of the range of the ensemble. Various

statistical postprocessing methods have been introduced accordingly, with the aim of improving the

30

forecast quality, to correct these errors by estimating representable relationships between the response

variable of interest and predictors. Most postprocessing methods can be categorized into two groups:

parametric approaches with distribution-based assumptions, such as ensemble model output statistics

(EMOS, Gneiting et al., 2005) models and BMA (Raftery et al., 2005), and nonparametric approaches

with distribution-free assumptions, such as the analog-based method (e.g., Delle Monache et al., 2013)

and the quantile regression forest (Taillardat et al., 2019). See Vannitsem et al. (2021) for a recent review

of statistical postprocessing methods as well as their potential and challenges.

Recently, the community of weather forecasting is starting to explore the potentials of machine

learning techniques, especially in the context of ensemble forecasting, in the sense of including

arbitrary predictors and accounting for nonlinear dynamics of the Earth system that are not captured

by existing numerical models (Dueben et al., 2021). One use of machine learning techniques is to

complement ensemble NWP (numerical weather prediction, see, e.g., Bauer et al., 2015; Benjamin et al.,

2018, for a summary of its revolution) forecasts using an additive postprocessing step for correction

of ensemble bias and spread (Rasp and Lerch, 2018; Scher and Messori, 2018; Gr ¨onquist et al., 2021).

Machine learning techniques, such as neural networks, have also been used as data-driven forecast

tools, an alternative to NWP models based on the physical laws governing the atmosphere, to generate

base forecasts. These techniques lead to improved computational efﬁciency in creating ensemble

forecasts with much larger ensemble sizes (Dueben and Bauer, 2018; Scher, 2018; Rasp and Thuerey,

2021; Scher and Messori, 2021).

3.7 Combinations constructed via Bayes’ Theorem

Pooling approaches, elaborated in Sections 3.3–3.5, pool/mix multiple probability forecasts with

equal weights, weights evaluated using various scoring rules, or posterior probabilities sequentially

updated via Bayes’ Theorem. They inherently neglect correlations among the component probability

forecasts. Forecasts derived from different sources, nevertheless, are likely to share the same data,

overlapping information, similar forecasting models, and common training processes. Thus, some

sort of dependence among individual probability forecasts is extremely likely, and such dependence

can have a serious impact on the aggregated distributions. In this section, we review an alternative

class of combination techniques, in which dependence among component probability forecasts can be

incorporated. The major feature, that makes this class of combinations difﬁcult, lies in how to model

the dependence among individual distributions in order to achieve good performance.

The extensive literature on probability forecast combinations considering correlations among

individual distributions has, for the most part, been driven from a foundational Bayesian perspective

and originated in agent opinion analysis theory, free from the time series context, dating back at least

to the pioneering work of Winkler (1968). We remark that in pooling techniques, the contribution of

each individual probability forecast to the ﬁnal aggregated probability forecast is measured explicitly

via weights, whereas it is not speciﬁed by a speciﬁc form in the Bayesian combination techniques we

discuss in this section.

Early work in the Bayesian vein focused on a Bayesian paradigm developed by Morris (1974)

and Morris (1977) in which a decision maker views available probability forecasts from various

sources simply as data, and updates his/her prior distribution by means of Bayes’ Theorem. At time

31

T, the decision maker aims to forecast yT+h and receives current h-step-ahead probability forecasts
HT+h = { f1(yT+h|IT), . . . , fN(yT+h|IT)} from the set of models. The posterior probability forecast of
yT+h is then

˜f (yT+h|IT, HT+h) ∝ p (yT+h|IT) fN (HT+h|yT+h, IT) ,

(19)

where p (yT+h|IT) denotes the decision maker’s prior probability for yT+h given the available infor-
mation IT, and fN (HT+h|yT+h, IT) denotes the joint likelihood function derived from the individual
distributions.

The problem of eliciting the posterior probability forecast in Equation (19) is therefore broken down

into the problem of specifying the prior distribution and assessing the form of the joint distribution, or

likelihood, derived from the component probability forecasts. A ﬂat (possibly improper) prior is often

considered in the literature because: (i) it is reasonable to assume that everything the decision maker

knows is integrated into the individual distributions; and (ii) if not, the extra knowledge from the

decision maker can be incorporated in the likelihood as an additional individual distribution; see, e.g.,

Winkler (1968), Clemen and Winkler (1993), Clemen and Winkler (1985), and Jouini and Clemen (1996).

Thus, the application of Bayes’ Theorem presents the most taxing difﬁculties in delicately specifying

the likelihood function, which requires consideration of the bias and precision of the individual

distributions as well as their dependence (Hall and Mitchell, 2007).

One line of research has considered specifying the likelihood as a joint distribution of forecast errors,

and supported the use of the correlation between individuals’ forecast errors in an effort to represent

the dependence among individual distributions. Emphasis has been placed on making the likelihood

computation tractable by adopting certain distributional assumptions. For example, Winkler (1981)

assessed the likelihood as a multivariate normal distribution. Restricting the focus to individual

models with unbiased forecasts, he derived tractable expressions for the posterior probability forecast,
a normal distribution with mean ˜µ = 1(cid:48)Σ−1µ/1(cid:48)Σ−11 and variance ˜σ2 = 1/1(cid:48)Σ−11, where 1 is an
N-dimensional unit vector, µ is an N-dimensional vector of individuals’ mean, and Σ is a known
covariance matrix of forecast errors. The mean of the posterior probability forecast is essentially a linear
combination of the individuals’ means with weights 1(cid:48)Σ−1/1(cid:48)Σ1 identical to the “optimal” weights
proposed by Bates and Granger (1969) and the weights derived from the constrained regression

proposed by Granger and Ramanathan (1984) (see Section 2.2), while the latter two approaches
do not require normality. The estimation of Σ, therefore, becomes crucial when Σ is unknown.
Winkler (1981) suggested estimating Σ from data and using an inverted Wishart distribution as a
prior for Σ. The procedure is computationally intensive when the number of individual distributions
to combine increases; see Hall and Mitchell (2007) for more discussion of the covariance matrix

estimation. Following Winkler (1981), Palm and Zellner (1992) extended the approach to allow for

biased individual forecasts, providing a complete solution to the forecast combination problem that

takes into account the joint distribution of forecast errors from the individual models.

Jouini and Clemen (1996) took a different perspective and looked at the likelihood function derived

from a copula-based joint distribution, in which dependence among individual distributions is

encoded into the copula. The procedure is appealing in the sense of being able to deal with individual

forecasts with arbitrary distributions. A recent study by Wilson (2017) gave an expert judgment

32

study to assess the practical signiﬁcance of the individuals’ dependency by comparing the Bayesian

combination methods, developed by Winkler (1981) and Jouini and Clemen (1996), and common

pooling methods.

3.8 Combinations constructed via integration

A fully speciﬁed Bayesian model is difﬁcult to conceptualize, especially in a setting where biases and

miscalibration of individual distributions (and critically, dependencies among them) are time-varying.

In this light, the probability forecast combination method of McAlinn and West (2019) may be helpful.

They adapted and extended the basic Bayesian predictive synthesis (BPS) framework developed in

agent opinion analysis (see, e.g., Genest and Schervish, 1985; West and Crosse, 1992; West, 1992) to

sequential forecasting in time series. In the dynamic extension of BPS model, the posterior probability

forecast takes the form

˜f (yT+h|IT, H1:T+h) =

(cid:90)

α (yT+h|xT+h) ∏
i=1:N

xT+h

fi (xT+h,i|IT) dxT+h,

where, to use our earlier notation, H1:T+h denotes the full set of individual probability forecasts
available for the decision maker up to forecast origin T, xT+h = xT+h,1:N is an N-dimensional vector
of latent variables at time T + h, and α (yT+h|xT+h) is a conditional distribution for yT+h given xT+h
deﬁning the synthesis function.

Instead of constructing Bayesian combinations by multiplying a likelihood by a prior, the dynamic

BPS method follows a subclass of Bayesian updating rules, i.e. updating by integration, in the

form of latent factor models. Information about biases, miscalibration, and dependencies among

the individual distributions can then be incorporated directly through speciﬁcation of the synthesis

function. Speciﬁcally, McAlinn and West (2019) developed a time-varying (non-convex/nonlinear)

combination of probability forecasts by deﬁning a normally distributed synthesis function

α (yT+h|xT+h) = N (cid:0)yT+h|A(cid:48)

T+hθT+h, vT+h

(cid:1)

with AT+h = (cid:0)1, x(cid:48)
model time evolution of these parameter processes, which is deﬁned as

and θT+h = (θT+h,0, θT+h,1, . . . , θT+h,N)(cid:48)

T+h

(cid:1)(cid:48)

. A dynamic linear model is built to

T+hθT+h + νT+h,

yT+h = A(cid:48)
θT+h = θT+h−1 + ωT+h, ωT+h ∼ N (0, vT+hWT+h) ,

νT+h ∼ N (0, vT+h) ,

where θT+h evolves in time according to a normal random walk with innovations variance matrix
vT+hWT+h, and vT+h identiﬁes the residual variance in forecasting yT+h given past information and the
set of agent forecast distributions. It is suggested that BPS models be customized speciﬁc to the forecast

horizon, as a forecasting model may provide different forecast performances at different forecast

horizons. MCMC methods are required for this posterior inference and, accordingly, dependencies

among agents are involved in sequentially updated estimates of the BPS parameters. Their results

of forecasting a quarterly series of inﬂation rates showed that the proposed dynamic BPS model

signiﬁcantly outperforms benchmark methods, such as the pooling and BMA combination techniques

33

described in Sections 3.3 and 3.4. This also held true in a multivariate extension studied by McAlinn

et al. (2020). McAlinn and West (2019) also showed that the dynamic BPS framework encompasses

many existing combination methods, including linear pooling and BMA methods, by specifying

different forms of the BPS synthesis function.

3.9 Quantile forecast combinations

Probabilistic forecasts can also be elicited in the form of quantiles, which are the inverse of the

corresponding probability forecasts characterized by the cumulative distribution functions (cdfs).

Quantile combinations involve averaging the individuals’ quantile functions rather than their inverses

as in linear pooling (see Section 3.3). In other words, quantile combinations entail horizontally

averaging of the individuals’ cdfs while linear pooling entails vertically averaging (Lichtendahl et al.,

2013).

The standard combination strategy for quantiles is to allocate identical weights over all quantile levels
for each individual model. For i = 1, . . . , N, let Fi(yT+h|IT) denote the individual cdf with correspond-
ing probability density function given by fi(yT+h|IT), and let QT+h|T,i(τ) = F−1
(τ) denote the
corresponding quantile function. Quantile averaging is then given by

T+h|T,i

˜QT+h|T(τ) =

N
∑
i=1

wT+h|T,iQT+h|T,i(τ),

0 < τ ≤ 1,

(20)

where the weight wT+h|T,i ≥ 0 such that ∑N
to as Vincentization (Vincent, 1912).

i=1 wT+h|T,i = 1. This combination strategy is also referred

Interestingly, unlike linear pooling in Equation (12), if individual distributions belong to the

same location-scale family (such as normal, Logistic, Cauchy, etc.), then quantile averaging yields

a combined distribution from the same family, with parameters given by weighted averages of the

individuals’ parameters (Ratcliff, 1979; Thomas and Ross, 1980). Consequently, quantile averaging

of normal distributions is always uni-modal (and normal), while linear pooling in general may

be multi-modal. Moreover, quantile averaging and linear pooling share the same mean, while

quantile averaging tends to be sharper and more conﬁdent due to the additional spread driven by the

disagreement on the mean in linear pooling.

Is it better to average quantiles (as in quantile averaging) or average probabilities (as in linear

pooling)? By restricting themselves to simple averages, Lichtendahl et al. (2013) and Busetti (2017) the-

oretically and empirically compared the properties of these two combination strategies and suggested

that quantile averaging seems overall a preferable and viable approach. Lichtendahl et al. (2013) at-

tributed this, in part, to the fact that the average probability forecast is in general underconﬁdent while

the average quantile forecast is always sharper. Even when individual forecasts agree on the location

and the average probability forecast is overconﬁdent, the more overconﬁdent average quantile forecast

still offers the possibility of forecast improvements due to its shape properties, speciﬁcally a higher

density in the shoulders and a lower density in the tails. Busetti (2017) reconﬁrmed the argument

and demonstrated that quantile averaging performs better than both linear pooling and logarithmic

pooling when combining individual forecast distributions with large biases. Taken together, it may be

34

useful to incorporate quantile averaging and probability averaging so that additional insight may be

provided. Accordingly, three simple methods were suggested by Lichtendahl et al. (2013) to blend

these two different combination strategies.

Rather than assuming that the entire individual quantile functions are available, one is often

provided with a collection of quantiles corresponding to an equidistant dense grid of probabilities
T ⊆ [0, 1], leading to a loss of information compared with consideration of the whole distribution. For
instance, the 0.05, 0.25, 0.50, 0.75 and 0.95 quantiles are often elicited in practice, and another popular
choice contains quantiles on all percentiles, i.e. T = (0.01, 0.02, . . . , 0.99). Quantile combination in
this case turns out to be a special case of the general aggregation rule in Equation (20). For each

quantile level, equally weighing the quantiles across all individual models is simple and quite robust,

yielding improved forecast skill relative to the individual models, and competitive performance

relative to a variety of more sophisticated combination strategies (e.g., Busetti, 2017; Smyl and Hua,

2019; Ray et al., 2022). In cases where sufﬁcient past data is available, some effort has been directed

toward the determination of combination weights via a cross-validation framework to reﬂect the past

out-of-sample performance of the different models and to improve the utility of combinations. Scoring

rules for quantile forecast evaluation can be harnessed for weight construction (Gneiting and Raftery,

2007; Grushka-Cockayne et al., 2017b; Trapero et al., 2019).

One line of research has looked at tailoring the individual weights for different quantile levels, i.e., a
separate weight is allocated for each individual model and each quantile level, by replacing wT+h|T,i
with wT+h|T,i(τ) in Equation (20). For example, individual quantiles can be weighted by the reciprocal
of the value of the pinball loss function (also referred to as the quantile loss) (Wang et al., 2019; Zhang

et al., 2020; Browell et al., 2020). This ﬂexible strategy enables the combination to accommodate the

fact that individual forecasting models may have varying performances at different quantile levels.

However, the number of weights to be learned scales with the number of quantile levels considered,

which makes it challenging to achieve forecast improvements. Computationally intensive techniques,

such as grid search and linear programming (LP), are applied for weight estimation, which are hardly

scalable to large datasets. What’s more, the datasets involved in their empirical studies are not large

enough to demonstrate the potential beneﬁts of estimating such a large number of weights.

As has been discussed previously in the literature on point forecast combinations, the error in

the estimation of optimal weights often exacerbates out-of-sample combined forecasts. The issue is

even more problematic when it comes to quantile combinations because it is a much more challenging

task to estimate combination weights for a collection of quantiles, especially in the tails of forecast

distributions, than merely for point forecasts deﬁned by the means of distributions. For example, Ray

et al. (2022) failed to empirically demonstrate the utility of weighting different quantiles separately

by optimizing the weighted interval score (WIS, Bracher et al., 2021) of the corresponding combined

result. On the other hand, a promising line of research by Fakoor et al. (2021), in the context of quantile

regression rather than time series forecasting, produced superior estimates using an aggregation

strategy of greater ﬂexibility than previously introduced. Separate weights depend on the features of

individual models, and (pairs of) quantile levels. They used a scalable stochastic gradient descent

(SGD) algorithm with a monotone operator to solve the weight optimization problem and obviate

the need of non-crossing constraints. More recently, Berrisch and Ziel (2021) introduced a new

weighting method that allows the individual forecasters to perform differently over time and within the

35

distribution. This was the ﬁrst to consider methods that aggregate across quantiles for optimal CRPS-

based combinations by using pointwise evaluation across the pinball loss. They also demonstrated

the optimal convergence properties, as well as the potential for performance improvements, by

considering pointwise optimization of the weight functions.

An independent line of research has looked at model-free combination heuristics, which frequently

serve as benchmarks to measure the effectiveness of newly developed combination strategies. From

a statistical point of view, these heuristics involve pooling together Q quantiles derived from N

individual forecast distributions that are assumed to have the same values of characteristics (e.g.,

the quantile function at different quantile levels) and using the stacked larger pool to draw more

precise estimates of those characteristics without training. Wang et al. (2019) formally introduced
na¨ıve sorting and median-based sorting methods, in which a total of N × Q quantiles are stacked and
sorted by ascending order to pick the ﬁrst and median values respectively in consecutive blocks of N

forecasts. Naturally, averaging blocks of N forecasts is also another option.

While aggregating quantiles tightly connects with aggregating probability distributions, there

has been little theoretical work in this area compared to combinations of probability forecasts which

have seen considerable theoretical advances. One exception is Lichtendahl et al. (2013) who looked

at the statistical properties of the simple average of probability forecasts and the ability to beneﬁt

from averaging. The choice of the combination weights has only been explored empirically, mainly

in the context of energy forecasting (e.g., Wang et al., 2019; Browell et al., 2020) and epidemiological

forecasting (e.g., Ray et al., 2022). Some of these proposals appear practical and beneﬁcial, while some

of them appear less useful. Further research is required to explore their utility.

Quantile crossing is a well-known problem caused by the lack of monotonicity in quantile esti-

mates, which may arise when different combination weights are utilized for different quantile levels.

Obviously, some model-free heuristics, such as the median, are also included in such cases. Quantile

crossing can be avoided by, for example, (i) integrating the aggregation problems for individual

models into one optimization problem subject to more non-crossing constraints (e.g., Bondell et al.,

2010; Fakoor et al., 2021), and (ii) conducting na¨ıve rearrangement after all the combined quantiles

are obtained (e.g., Chernozhukov et al., 2010; Berrisch and Ziel, 2021). The rearrangement operation,

though simple, is frequently recommended in practice since it will never deteriorate the forecasting

performance in terms of the pinball loss (Chernozhukov et al., 2010).

Interval forecasts form a crucial special case of quantile forecasts, which makes the preceding

combination approaches for quantile forecasts naturally apply to interval forecasts as well. When

forming combinations of interval forecasts, attention should be paid to the fact that the combined

interval forecasts are not guaranteed to provide target coverage rates (Wallis, 2005; Timmermann,

2006; Grushka-Cockayne and Jose, 2020). As a result, when evaluating the combined interval forecasts,

proper scoring approaches that take into account both width and coverage are appealing, and can

serve as objective functions to determine combination weights; see, e.g., Gneiting and Raftery (2007)

and Jose and Winkler (2009).

For interval forecasts, six heuristics have been outlined: (1) simple average, (2) median, (3) enve-

lope, (4) interior trimming, (5) exterior trimming, and (6) probability averaging of endpoints (Park

and Budescu, 2015; Gaba et al., 2017). These six heuristics are virtually free of computational costs and

36

have subsequently been promoted by recent research due to their robustness and beneﬁts in different

scenarios for addressing underconﬁdence/overconﬁdence; e.g., Smyl and Hua (2019), Petropoulos

and Svetunkov (2020), and Grushka-Cockayne and Jose (2020). They can easily be extended to address

the combinations of quantiles by aggregating individual quantiles in several ways for each quantile

level.

Determining combination weights for interval forecasts is evidently easier to implement than

quantile forecasts, since one only has to consider two quantiles. For example, by assuming the

intervals to be symmetric around the point forecast, Montero-Manso et al. (2020) used the combined

point forecast produced by a feature-based meta-learner as the center of the combined interval and

generated the radius as a linear combination of the individual radii with the goal of minimizing

the MSIS (mean scaled interval score, Gneiting and Raftery, 2007) of the interval. The approach

achieved the second position in the M4 competition with 100, 000 time series involved. Subsequent

work by Wang et al. (2022b) introduced a feature-based weight determination approach to directly

combine lower and upper bounds of individual interval forecasts, leading to signiﬁcant performance

improvements compared to individual forecasts and the simple average.

4 Conclusions and a look to the future

Forecasting plays an indispensable role in decision-making, where success depends heavily on the

accuracy of the available forecasts. Even with a small increase in accuracy, remarkable gains may be

achieved in activities such as management planning and strategy setting (Makridakis, 1996; Syntetos

et al., 2009). In this regard, forecast combinations provide an easy path to improving forecast accuracy

by integrating the available information used in individual forecasts.

In this review, our goal has been to show not only how forecast combinations have evolved

over time, but also to identify the potential and limitations of various methods, and to highlight the

areas needing further research. Forecast combinations can be model-free or model-ﬁtting, linear or

nonlinear, static or time-varying, series-speciﬁc or cross-learning, and frequentist or Bayesian. The

toolbox of combination methods has grown in size and sophistication, each with its own merits. Which

combination method to choose depends on several factors such as the form of forecasts (point forecasts,

probabilistic forecasts, quantiles, etc.), the quality and size of the model pool, the information available,

and the speciﬁc forecasting problems. There is no clear consensus on which forecast combination

method can be expected to perform best in a speciﬁc setting. Based on this review, we summarise some

of the current research gaps and potential insights for future research in the following paragraphs.

Continuing to examine simple averaging. Over ﬁfty years after Bates and Granger’s (1969) pio-

neering work on forecast combinations, it is amazing that, in empirical studies, simple averaging still

repeatedly dominates sophisticated weighted combinations which are theoretically preferred, posing

a tough benchmark to beat. Although it is well known that the “forecast combination puzzle” stems

from the unstable estimates of combination weights, researchers still lack comprehensive quantitative

decision guidance on when to choose a simple averaging strategy over more complex strategies.

One exception is Blanc and Setzer (2016) who merely looked at the combination of two individual

forecasts and proposed decision rules to decide when to choose simple averaging over the “optimal”

37

weights introduced by Bates and Granger (1969). In addition, the examination of simple averaging in

the context of probabilistic forecast combinations deserves further attention and development, both

theoretical and empirical.

Keeping combinations sophisticatedly simple. Forecasting models and forecast combination meth-

ods have grown swiftly in both size and sophistication. Nevertheless, empirical results are ambiguous

and there is no coherent evidence that complexity systematically improves forecast accuracy; see,

e.g., Green and Armstrong (2015) for a review comparing simple and complex forecasting methods.

Following Zellner (2001), we suggest the blooming of sophisticatedly simple combinations to balance

the tradeoff between the beneﬁts of tailoring weights for different individual models and the instability

of learned weights in sophisticated weighting schemes. Additionally, it is strongly recommended that

a detailed analysis is required to explore in depth how and why various sophisticated combination

strategies work and, thus, provide more insights into which combination method to choose in a

particular situation; Petropoulos et al. (2018a) provided a good example of this kind of work.

Obtaining statistical inference for the combination forecasts. The “forecast combination puzzle”

revolves primarily around the question of choosing ﬁxed simple weights or random “optimal” ones. A

related aspect to the puzzle, but somehow different from it, is that the randomness of the combination

weights (and in particular the correlation with the forecasts) makes it difﬁcult to perform statistical

inference for the weighted combined forecast. A standard error is nontrivial to obtain in most cases,

let alone the sampling distribution. Getting the combined forecast is one aspect, what to do with it

in a statistical sense is another aspect. Therefore, future studies on the randomness of combination

weights and the statistical inference for the combined forecasts would be of interest.

Selecting forecasts to be combined. The pool of individual forecasts lays the foundation for the

utility of forecast combinations. These forecasts may come from statistical or machine learning models,

and may be based on observed data or be elicited from experts. Empirical evidence suggests that the

future lies in the combination of statistical and machine learning generated forecasts, as well as the

incorporation of human judgment (Petropoulos et al., 2018b; Makridakis et al., 2020a; Petropoulos

et al., 2022). Given a large number of available forecasts, selecting a subset of combinations becomes

particularly pivotal for the purpose of improving forecast skill and reducing computational costs. A

variety of crucial issues in the selection of forecasts have to be addressed, such as accuracy, robustness,

diversity, and calibration when considering probabilistic forecasts (Lichtendahl and Winkler, 2020;

Wang et al., 2022a). However, most of the existing algorithms perform ad hoc selections and lack

statistical rationale (Kourentzes et al., 2019). Therefore, further attention should be paid to the

development of empirical guidelines and quantitative metrics to help forecasters in selecting forecasts

prior to combinations. Since a zero weight in the past does not indicate a zero weight in the future,

time-varying subset selection for forecast combination would also be an interesting research direction.

Advancing the theory of nonlinear combinations. While the beneﬁts of linear combinations of

multiple forecasts are well appreciated in the forecasting literature, less attention has been paid to

nonlinear combination schemes to model nonlinear dependencies among individual forecasts possibly

due to the lack of theoretical foundations and poor records of success; see Timmermann (2006) for a

brief review of the related literature. Nonlinearities are currently addressed with the use of neural

networks or an additional nonlinear term in the combination equation. However, the limited evidence

on the beneﬁts of involving nonlinear combinations is mostly derived from only a few time series and

38

is not entirely unconvincing. As a consequence, we expect more theoretical and empirical work in this

area in the near future.

Focusing more on probabilistic forecast combinations. In probabilistic forecast combinations,

linear pooling and quantile averaging suggest two different ways of thinking — linear pooling entails

vertically averaging the individuals’ cdfs while quantile aggregation entails horizontally averaging.

Accordingly, their combined forecasts hold different properties and beneﬁt differently from the combi-

nation. For example, the shape-preserving property of quantile averaging may be appealing in certain

settings (Lichtendahl et al., 2013). Over the past decade, linear pooling has attracted considerable

attention, achieving appreciable advancements both theoretically and empirically. Quantile averaging,

however, has not received much attention, especially in the theoretical realm. Furthermore, when

tailoring combination weights for different quantile levels, the instability of the estimated weights

is especially problematic since a lot of parameters have to be estimated. This issue is likely to harm

the calibration and sharpness of the out-of-sample combined forecasts, making quantile averaging

a challenging task. Taken together, we expect combinations of quantiles to be an important area of

research in the future.

Discussing if, how, and when it is helpful to interpret combination weights. In probability forecast

combinations, some combination approaches have the property that poorly performing forecasts will

almost surely be rejected in favor of the best one as the sample size tends to inﬁnity. For example, BMA

reduces to model selection for a large sample size, with the best model receiving a weight very close to

one. See Section 3.4 for more detailed discussions. However, it is sometimes found that individually

“bad” forecasts may be still helpful in combinations (e.g., Geweke and Amisano, 2011). In this case,

one does not want to zero-weight these bad forecasts (in the limit, as the sample size goes to inﬁnity).

This relates to the question of if, how, and when it is helpful to interpret combination weights, another

future research direction worth exploring.

Taking account of correlations among individual forecasts. Some sort of correlations among

individual forecasts are expected as they are likely to share the same data, overlapping information,

similar forecasting models, and a common training process. Such correlations can be critical and have

a serious impact on the utility of forecast combinations (De Menezes et al., 2000). An extensive body of

literature on point forecast combinations has attempted to account for correlations in terms of weight

estimation, despite the fact that these correlations can be poorly estimated. Despite the existence

of such correlations, the literature on probabilistic forecast combinations has paid scant attention

to addressing them; they are primarily addressed from a Bayesian perspective (e.g., Winkler, 1981;

McAlinn and West, 2019). Therefore, another interesting path for further research would be to take

more into account of correlations among individual forecasts in weighting schemes for probabilistic

forecast combinations.

Cross-learning and feature engineering. Instead of combinations in a series-by-series fashion,

numerous studies have conﬁrmed the beneﬁcial usage of information from multiple series to study

common patterns among series, thereby facilitating the determination of combination weights and

exploiting the beneﬁts of cross-learning. The evidence of the potential of cross-learning has largely

come from competitions (e.g., Makridakis et al., 2020a; Makridakis et al., 2022) and empirical studies

(e.g., Ma and Fildes, 2021). Moreover, access to feature engineering can lead to improved forecasting

performance, providing valuable information for forecast combinations in a cross-learning fashion

39

(Montero-Manso et al., 2020; Kang et al., 2021). In this regard, we believe that further research needs

to be done on feature engineering for time series data to unlock the potential of cross-learning.

Encouraging researchers to contribute open-source software and datasets. In this paper, we list

some open-source packages linking to the developed approaches for forecast combinations (e.g., fable,
ForecastComb, and forecastHybrid packages for R), time series features (e.g., feasts and tsfeatures
packages for R and tsfresh and Kats packages for Python), and time series generations (e.g., forecast
and gratis packages for R). We emphasize that open-source research software is a pathway to impact.

Recent decades have witnessed a dramatically accelerating pace in the advancements in computing.

As a consequence, it is time to promote the idea of researchers producing open-source software

that provides evidence and support behind all the statements that are made. Publicly releasing new

software beneﬁts researchers and end users. It reduces research costs, allows for quick implementation,

helps people modify the existing software and adapts it to other research ideas. We also encourage

researchers to contribute open-source datasets because of the beneﬁts of investigating and comparing

the performance of newly developed methods; see, e.g., Godahewa et al. (2021a) and Godahewa et al.

(2021b) for a time series forecasting archive containing 20 publicly available time series datasets from

different domains.

In this paper, we take the multiple forecasts to be combined essentially as given and limit ourselves

to combinations of forecasts derived from separate models for a given series. These separate models

can be identiﬁed with different model forms and/or the same model form with different parameters.

However, we highlight that there are other types of forecast combinations in the forecasting litera-

ture. For example, one type of approach involves constructing replicas of the original time series

through various manipulations of local curvatures, frequency transformation or bootstrapping, and

subsequently multiple forecasts are produced to form the ﬁnal combined forecasts, leading to a wide

variety of approaches such as the theta method (Assimakopoulos and Nikolopoulos, 2000), temporal

aggregation (e.g., Kourentzes et al., 2014b; Kourentzes and Petropoulos, 2016; Kourentzes et al.,

2017), bagging (e.g., Bergmeir et al., 2016; Petropoulos et al., 2018a), and structural combination (e.g.,

Rendon-Sanchez and De Menezes, 2019). Another approach involves forming a hierarchical structure

using multiple time series that are structurally connected based on geographic or logical reasons

and reconciling multiple forecasts across the hierarchy, leading to various hierarchical aggregation

methods (e.g., Hyndman et al., 2011; Wickramasuriya et al., 2019; Ben Taieb et al., 2021; Hollyman

et al., 2021).

Acknowledgments

We thank Adrian Raftery, Casey Lichtendahl, Yael Grushka-Cockayne, Fotios Petropoulos and other

experts in this area for providing very helpful feedback on an earlier version of this paper. We thank

the editors and two anonymous reviewers for their helpful comments and suggestions that improved

the paper.

40

References

Aastveit, KA, Mitchell, J, Ravazzolo, F, and Dijk, HK van (2019). The evolution of forecast density

combinations in economics. DOI: 10.1093/acrefore/9780190625979.013.381.

Adhikari, R (2015). A mutual association based nonlinear ensemble mechanism for time series fore-

casting. Applied Intelligence 43(2), 233–250. DOI: 10.1007/s10489-014-0641-y.

Adhikari, R and Agrawal, RK (2012). A novel weighted ensemble technique for time series forecasting.

In: Advances in Knowledge Discovery and Data Mining. Berlin, Heidelberg: Springer Berlin Heidelberg,
pp.38–49. DOI: 10.1007/978-3-642-30217-6_4.

Agnew, CE (1985). Bayesian consensus forecasts of macroeconomic variables. Journal of Forecasting

4(4), 363–376. DOI: 10.1002/for.3980040405.

Aiolﬁ, M and Timmermann, A (2006). Persistence in forecasting performance and conditional combi-
nation strategies. Journal of Econometrics 135(1), 31–53. DOI: 10.1016/j.jeconom.2005.07.015.

Akaike, H (1974). A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic

Control 19(6), 716–723. DOI: 10.1109/TAC.1974.1100705.

Aksu, C and Gunter, SI (1992). An empirical analysis of the accuracy of SA, OLS, ERLS and NRLS
combination forecasts. International Journal of Forecasting 8(1), 27–43. DOI: 10.1016/0169-2070(92)
90005-T.

Andrawis, RR, Atiya, AF, and El-Shishiny, H (2011). Combination of long term and short term forecasts,

with application to tourism demand forecasting. International Journal of Forecasting 27(3), 870–886.
DOI: 10.1016/j.ijforecast.2010.05.019.

Armstrong, JS (2001). “Combining Forecasts”. In: Principles of Forecasting: A Handbook for Researchers
and Practitioners. Boston, MA: Springer, pp.417–439. DOI: 10.1007/978-0-306-47630-3_19.

Assimakopoulos, V and Nikolopoulos, K (2000). The theta model: a decomposition approach to
forecasting. International Journal of Forecasting 16(4), 521–530. DOI: 10.1016/S0169-2070(00)00066-
2.

Atger, F (2003). Spatial and Interannual Variability of the Reliability of Ensemble-Based Probabilistic
Forecasts: Consequences for Calibration. Monthly Weather Review 131(8), 1509–1523. DOI: 10.1175/
/1520-0493(2003)131<1509:SAIVOT>2.0.CO;2.

Atiya, AF (2020). Why does forecast combination work so well? International Journal of Forecasting 36(1),

197–200. DOI: 10.1016/j.ijforecast.2019.03.010.

Babikir, A and Mwambi, H (2016). Evaluating the combined forecasts of the dynamic factor model

and the artiﬁcial neural network model using linear and nonlinear combining methods. Empirical
Economics 51(4), 1541–1556. DOI: 10.1007/s00181-015-1049-1.

Baran, S (2014). Probabilistic wind speed forecasting using Bayesian model averaging with truncated
normal components. Computational Statistics & Data Analysis 75, 227–238. DOI: 10.1016/j.csda.
2014.02.013.

Baran, S and Lerch, S (2018). Combining predictive distributions for the statistical post-processing of
ensemble forecasts. International Journal of Forecasting 34(3), 477–496. DOI: 10.1016/j.ijforecast.
2018.01.005.

Barandas, M, Folgado, D, Fernandes, L, Santos, S, Abreu, M, Bota, P, Liu, H, Schultz, T, and Gamboa,
H (2020). TSFEL: Time Series Feature Extraction Library. SoftwareX 11, 100456. DOI: 10.1016/j.
softx.2020.100456.

41

Barto ´n, K (2022). MuMIn: Multi-Model Inference. R package version 1.47.1. https://CRAN.R-project.

org/package=MuMIn.

Bassetti, F, Casarin, R, and Ravazzolo, F (2018). Bayesian Nonparametric Calibration and Combination

of Predictive Distributions. Journal of the American Statistical Association 113(522), 675–685. DOI:
10.1080/01621459.2016.1273117.

Bassetti, F, Casarin, R, and Ravazzolo, F (2020). “Density Forecasting”. In: Macroeconomic Forecasting in

the Era of Big Data: Theory and Practice. Ed. by P Fuleky. Cham: Springer International Publishing,
pp.465–494. DOI: 10.1007/978-3-030-31150-6_15.

Batchelor, R and Dua, P (1995). Forecaster diversity and the beneﬁts of combining forecasts. Manage-

ment Science 41(1), 68–75. DOI: 10.1287/mnsc.41.1.68.

Bates, JM and Granger, CWJ (1969). The combination of forecasts. Journal of the Operational Research

Society 20(4), 451–468. DOI: 10.1057/jors.1969.103.

Bauer, P, Thorpe, A, and Brunet, G (2015). The quiet revolution of numerical weather prediction.

Nature 525(7567), 47–55. DOI: 10.1038/nature14956.

Baumeister, C and Kilian, L (2015). Forecasting the Real Price of Oil in a Changing World: A Forecast
Combination Approach. Journal of Business & Economic Statistics 33(3), 338–351. DOI: 10.1080/
07350015.2014.949342.

Ben Taieb, S, Taylor, JW, and Hyndman, RJ (2021). Hierarchical Probabilistic Forecasting of Electricity

Demand With Smart Meter Data. Journal of the American Statistical Association 116(533), 27–43. DOI:
10.1080/01621459.2020.1736081.

Benjamin, SG, Brown, JM, Brunet, G, Lynch, P, Saito, K, and Schlatter, TW (2018). 100 years of

progress in forecasting and NWP applications. Meteorological Monographs 59, 13.1–13.67. DOI:
10.1175/AMSMONOGRAPHS-D-18-0020.1.

Bergmeir, C, Hyndman, RJ, and Ben´ıtez, JM (2016). Bagging exponential smoothing methods using

STL decomposition and Box–Cox transformation. International Journal of Forecasting 32(2), 303–312.
DOI: 10.1016/j.ijforecast.2015.07.002.

Berrisch, J and Ziel, F (2021). CRPS learning. Journal of Econometrics. DOI: 10.1016/j.jeconom.2021.

11.008.

Billio, M, Casarin, R, Ravazzolo, F, and Dijk, HK van (2013). Time-varying combinations of predictive
densities using nonlinear ﬁltering. Journal of Econometrics 177(2), 213–232. DOI: 10.1016/j.jeconom.
2013.04.009.

Blanc, SM and Setzer, T (2016). When to choose the simple average in forecast combination. Journal of

Business Research 69(10), 3951–3962. DOI: 10.1016/j.jbusres.2016.05.013.

Blanc, SM and Setzer, T (2020). Bias–Variance Trade-Off and Shrinkage of Weights in Forecast Combi-

nation. Management Science 66(12), 5720–5737. DOI: 10.1287/mnsc.2019.3476.

Bondell, HD, Reich, BJ, and Wang, H (2010). Noncrossing quantile regression curve estimation.

Biometrika 97(4), 825–838. DOI: 10.1093/biomet/asq048.

Bracher, J, Ray, EL, Gneiting, T, and Reich, NG (2021). Evaluating epidemic forecasts in an interval

format. PLoS Computational Biology 17(2), e1008618. DOI: 10.1371/journal.pcbi.1008618.

Browell, J, Gilbert, C, Tawn, R, and May, L (2020). Quantile Combination for the EEM20 Wind Power

Forecasting Competition. In: 2020 17th International Conference on the European Energy Market (EEM).
ieeexplore.ieee.org, pp.1–6. DOI: 10.1109/EEM49802.2020.9221942.

42

Brown, G, Wyatt, J, Harris, R, and Yao, X (2005). Diversity creation methods: a survey and categorisa-

tion. Information Fusion 6(1), 5–20. DOI: 10.1016/j.inffus.2004.04.004.

Budescu, DV and Chen, E (2015). Identifying expertise to extract the wisdom of crowds. Management

Science 61(2), 267–280. DOI: 10.1287/mnsc.2014.1909.

Buizza, R, Houtekamer, PL, Pellerin, G, Toth, Z, Zhu, Y, and Wei, M (2005). A Comparison of the

ECMWF, MSC, and NCEP Global Ensemble Prediction Systems. Monthly Weather Review 133(5),
1076–1097. DOI: 10.1175/MWR2905.1.

Buizza, R, Milleer, M, and Palmer, TN (1999). Stochastic representation of model uncertainties in the

ECMWF ensemble prediction system. Quarterly Journal of the Royal Meteorological Society 125(560),
2887–2908. DOI: 10.1002/qj.49712556006.

Bunn, DW (1975). A Bayesian approach to the linear combination of forecasts. Journal of the Operational

Research Society 26, 325–329. DOI: 10.1057/jors.1975.67.

Bunn, DW (1985). Statistical efﬁciency in the linear combination of forecasts. International Journal of

Forecasting 1(2), 151–163. DOI: 10.1016/0169-2070(85)90020-2.

Burnham, KP and Anderson, DR (2002). Model selection and multi-model inference: A practical information-

theoretic approach. 2nd. Springer New York, NY. DOI: 10.1007/b97636.

Busetti, F (2017). Quantile aggregation of density forecasts. Oxford Bulletin of Economics and Statistics

79(4), 495–512. DOI: 10.1111/obes.12163.

Cang, S and Yu, H (2014). A combination selection algorithm on forecasting. European Journal of

Operational Research 234(1), 127–139. DOI: 10.1016/j.ejor.2013.08.045.

Caruana, R, Niculescu-Mizil, A, Crew, G, and Ksikes, A (2004). Ensemble selection from libraries of

models. In: Proceedings of the Twenty-ﬁrst International Conference on Machine learning. Banff, Alberta,
Canada: Association for Computing Machinery, pp.18. DOI: 10.1145/1015330.1015432.

Castle, JL, Clements, MP, and Hendry, DF (2013). Forecasting by factors, by variables, by both or

neither? Journal of Econometrics 177(2), 305–319. DOI: 10.1016/j.jeconom.2013.04.015.

Chan, F and Pauwels, LL (2018). Some theoretical results on forecast combinations. International Journal

of Forecasting 34(1), 64–74. DOI: 10.1016/j.ijforecast.2017.08.005.

Chan, YL, Stock, JH, and Watson, MW (1999). A dynamic factor model framework for forecast

combination. Spanish Economic Review 1(2), 91–121. DOI: 10.1007/s101080050005.

Chernozhukov, V, Fern´andez-Val, I, and Galichon, A (2010). Quantile and probability curves without
crossing. Econometrica: Journal of the Econometric Society 78(3), 1093–1125. DOI: 10.3982/ecta7880.

Chong, YY and Hendry, DF (1986). Econometric evaluation of linear macro-economic models. The

Review of Economic Studies 53(4), 671–690. DOI: 10.2307/2297611.

Christ, M, Braun, N, Neuffer, J, and Kempa-Liehr, AW (2018). Time Series Feature Extraction on

basis of Scalable Hypothesis tests (tsfresh – A Python package). Neurocomputing 307, 72–77. DOI:
10.1016/j.neucom.2018.03.067.

Claeskens, G, Magnus, JR, Vasnev, AL, and Wang, W (2016). The forecast combination puzzle: A
simple theoretical explanation. International Journal of Forecasting 32(3), 754–762. DOI: 10.1016/j.
ijforecast.2015.12.005.

Clark, TE and McCracken, MW (2010). Averaging forecasts from VARs with uncertain instabilities.

Journal of Applied Econometrics 25(1), 5–29. DOI: 10.1002/jae.1127.

Clemen, RT (1989). Combining forecasts: A review and annotated bibliography. International Journal of

Forecasting 5(4), 559–583. DOI: 10.1016/0169-2070(89)90012-5.

43

Clemen, RT and Winkler, RL (1985). Limits for the Precision and Value of Information from Dependent

Sources. Operations Research 33(2), 427–442. DOI: 10.1287/opre.33.2.427.

Clemen, RT and Winkler, RL (1986). Combining Economic Forecasts. Journal of Business & Economic

Statistics 4(1), 39–46. DOI: 10.1080/07350015.1986.10509492.

Clemen, RT and Winkler, RL (1993). Aggregating Point Estimates: A Flexible Modeling Approach.

Management Science 39(4), 501–515. DOI: 10.1287/mnsc.39.4.501.

Clemen, RT and Winkler, RL (1999). Combining Probability Distributions From Experts in Risk

Analysis. Risk Analysis 19(2), 187–203. DOI: 10.1023/A:1006917509560.

Clements, M and Hendry, D (1998). Forecasting Economic Time Series. Cambridge University Press. DOI:

10.1017/CBO9780511599286.

Collopy, F and Armstrong, JS (1992). Rule-based forecasting: Development and validation of an expert

systems approach to combining time series extrapolations. Management Science 38(10), 1394–1414.
DOI: 10.1287/mnsc.38.10.1394.

Conﬂitti, C, De Mol, C, and Giannone, D (2015). Optimal combination of survey forecasts. International

Journal of Forecasting 31(4), 1096–1103. DOI: 10.1016/j.ijforecast.2015.03.009.

Costantini, M and Pappalardo, C (2010). A hierarchical procedure for the combination of forecasts.
International Journal of Forecasting 26(4), 725–743. DOI: 10.1016/j.ijforecast.2009.09.006.

Coulson, NE and Robins, RP (1993). Forecast combination in a dynamic setting. Journal of Forecasting

12(1), 63–67. DOI: 10.1002/for.3980120106.

Dawid, AP, DeGroot, MH, Mortera, J, Cooke, R, French, S, Genest, C, Schervish, MJ, Lindley, DV,

McConway, KJ, and Winkler, RL (1995). Coherent combination of experts’ opinions. Test 4, 263–313.
DOI: 10.1007/BF02562628.

Dawid, AP (1984). Present position and potential developments: Some personal views statistical theory

the prequential approach. Journal of the Royal Statistical Society: Series A (General) 147(2), 278–290.
DOI: 10.2307/2981683.

De Menezes, LM, Bunn, DW, and Taylor, JW (2000). Review of guidelines for the use of combined
forecasts. European Journal of Operational Research 120(1), 190–204. DOI: 10.1016/S0377-2217(98)
00380-4.

Del Negro, M, Hasegawa, RB, and Schorfheide, F (2016). Dynamic prediction pools: An investigation

of ﬁnancial frictions and forecasting performance. Journal of Econometrics 192(2), 391–405. DOI:
10.1016/j.jeconom.2016.02.006.

Delle Monache, L, Eckel, FA, Rife, DL, Nagarajan, B, and Searight, K (2013). Probabilistic weather
prediction with an analog ensemble. Monthly Weather Review 141(10), 3498–3516. DOI: 10.1175/mwr-
d-12-00281.1.

Deutsch, M, Granger, CWJ, and Ter¨asvirta, T (1994). The combination of forecasts using changing
weights. International Journal of Forecasting 10(1), 47–57. DOI: 10.1016/0169-2070(94)90049-3.

Diebold, FX (1988). Serial Correlation and the Combination of Forecasts. Journal of Business & Economic

Statistics 6(1), 105–111. DOI: 10.1080/07350015.1988.10509642.

Diebold, FX (1989). Forecast combination and encompassing: Reconciling two divergent literatures.

International Journal of Forecasting 5(4), 589–592. DOI: 10.1016/0169-2070(89)90014-9.

Diebold, FX, Gunther, TA, and Tay, AS (1998). Evaluating Density Forecasts with Applications to
Financial Risk Management. International Economic Review 39(4), 863–883. DOI: 10.2307/2527342.

44

Diebold, FX and Pauly, P (1987). Structural change and the combination of forecasts. Journal of Forecast-

ing 6(1), 21–40. DOI: 10.1002/for.3980060103.

Diebold, FX and Pauly, P (1990). The use of prior information in forecast combination. International

Journal of Forecasting 6(4), 503–508. DOI: 10.1016/0169-2070(90)90028-A.

Diebold, FX and Shin, M (2019). Machine learning for regularized survey forecast combination:

Partially-egalitarian LASSO and its derivatives. International Journal of Forecasting 35(4), 1679–1691.
DOI: 10.1016/j.ijforecast.2018.09.006.

Diebold, FX, Shin, M, and Zhang, B (2022). On the aggregation of probability assessments: Regularized

mixtures of predictive densities for Eurozone inﬂation and real interest rates. Journal of Econometrics.
DOI: 10.1016/j.jeconom.2022.06.008.

Diks, C, Panchenko, V, and Dijk, D van (2011). Likelihood-based scoring rules for comparing density
forecasts in tails. Journal of Econometrics 163(2), 215–230. DOI: 10.1016/j.jeconom.2011.04.001.

Donaldson, RG and Kamstra, M (1996). Forecast combining with neural networks. Journal of Forecasting
15(1), 49–61. DOI: 10.1002/(SICI)1099-131X(199601)15:1<49::AID-FOR604>3.0.CO;2-2.

Donate, JP, Cortez, P, Sanchez, GG, and De Miguel, AS (2013). Time series forecasting using a weighted

cross-validation evolutionary artiﬁcial neural network ensemble. Neurocomputing 109, 27–32. DOI:
10.1016/j.neucom.2012.02.053.

Draper, D (1995). Assessment and propagation of model uncertainty. Journal of the Royal Statistical

Society: Series B (Methodological) 57(1), 45–70. DOI: 10.1111/j.2517-6161.1995.tb02015.x.

Dueben, PD and Bauer, P (2018). Challenges and design choices for global weather and climate models
based on machine learning. Geoscientiﬁc Model Development 11(10), 3999–4009. DOI: 10.5194/gmd-
11-3999-2018.

Dueben, PD, Bauer, P, and Adams, S (2021). Deep learning to improve weather predictions. DOI: 10.1002/

9781119646181.ch14.

Elliott, G (2011). Averaging and the optimal combination of forecasts. University of California, San Diego.

Elliott, G and Timmermann, A (2004). Optimal forecast combinations under general loss functions and
forecast error distributions. Journal of Econometrics 122(1), 47–79. DOI: 10.1016/j.jeconom.2003.
10.019.

Facebook’s Infrastructure Data Science team (2021). Kats. Python package version 0.1.0. https://

facebookresearch.github.io/Kats/.

Fakoor, R, Kim, T, Mueller, J, Smola, A, and Tibshirani, RJ (2021). Flexible Model Aggregation for Quantile

Regression. DOI: 10.48550/ARXIV.2103.00083.

Fern´andez-Villaverde, J and Rubio-Ramırez, JF (2004). Comparing dynamic equilibrium models to
data: a Bayesian approach. Journal of Econometrics 123(1), 153–187. DOI: 10.1016/j.jeconom.2003.
10.031.

Fischer, I and Harvey, N (1999). Combining forecasts: What information do judges need to outperform
the simple average? International Journal of Forecasting 15(3), 227–246. DOI: 10 . 1016 / S0169 -
2070(98)00073-9.

Fletcher, D (2018). Model Averaging. Berlin: Springer. DOI: 10.1007/978-3-662-58541-2.

Freitas, PSA and Rodrigues, AJL (2006). Model combination in neural-based forecasting. European

Journal of Operational Research 173(3), 801–814. DOI: 10.1016/j.ejor.2005.06.057.

45

Fulcher, BD and Jones, NS (2017). hctsa: A Computational Framework for Automated Time-Series
Phenotyping Using Massive Feature Extraction. Cell Systems 5(5), 527–531.e3. DOI: 10.1016/j.
cels.2017.10.001.

Gaba, A, Tsetlin, I, and Winkler, RL (2017). Combining Interval Forecasts. Decision Analysis 14(1), 1–20.

DOI: 10.1287/deca.2016.0340.

Galton, F (1907a). One vote, one value. Nature 75, 414. DOI: 10.1038/075414a0.
Galton, F (1907b). Vox populi. Nature 75, 450–451. DOI: 10.1038/075450a0.

Ganaie, MA, Hu, M, Malik, AK, Tanveer, M, and Suganthan, PN (2022). Ensemble deep learning: A

review. DOI: 10.1016/j.engappai.2022.105151.

Garratt, A, Lee, K, Pesaran, MH, and Shin, Y (2003). Forecast uncertainties in macroeconomic modeling.
Journal of the American Statistical Association 98(464), 829–838. DOI: 10.1198/016214503000000765.

Gastinger, J, Nicolas, S, Stepi´c, D, Schmidt, M, and Sch ¨ulke, A (2021). A study on Ensemble Learning

for Time Series Forecasting and the need for Meta-Learning. In: 2021 International Joint Conference
on Neural Networks (IJCNN), pp.1–8. DOI: 10.1109/IJCNN52387.2021.9533378.

Genest, C and Schervish, MJ (1985). Modeling Expert Judgments for Bayesian Updating. Annals of

Statistics 13(3), 1198–1212. DOI: 10.1214/aos/1176349664.

Genre, V, Kenny, G, Meyler, A, and Timmermann, A (2013). Combining expert forecasts: Can anything
beat the simple average? International Journal of Forecasting 29(1), 108–121. DOI: 10 . 1016 / j .
ijforecast.2012.06.004.

Geweke, J and Amisano, G (2010). Comparing and evaluating Bayesian predictive distributions of
asset returns. International Journal of Forecasting 26(2), 216–230. DOI: 10.1016/j.ijforecast.2009.
10.007.

Geweke, J and Amisano, G (2011). Optimal prediction pools. Journal of Econometrics 164(1), 130–141.

DOI: 10.1016/j.jeconom.2011.02.017.

Gneiting, T, Balabdaoui, F, and Raftery, AE (2007). Probabilistic forecasts, calibration and sharpness.
Journal of the Royal Statistical Society: Series B (Statistical Methodology) 69(2), 243–268. DOI: 10.1111/
j.1467-9868.2007.00587.x.

Gneiting, T and Katzfuss, M (2014). Probabilistic Forecasting. Annual Review of Statistics and Its

Application 1(1), 125–151. DOI: 10.1146/annurev-statistics-062713-085831.

Gneiting, T and Raftery, AE (2005). Weather forecasting with ensemble methods. Science 310(5746),

248–249. DOI: 10.1126/science.1115255.

Gneiting, T and Raftery, AE (2007). Strictly Proper Scoring Rules, Prediction, and Estimation. Journal of

the American Statistical Association 102(477), 359–378. DOI: 10.1198/016214506000001437.

Gneiting, T, Raftery, AE, Westveld, AH, and Goldman, T (2005). Calibrated Probabilistic Forecasting

Using Ensemble Model Output Statistics and Minimum CRPS Estimation. Monthly Weather Review
133(5), 1098–1118. DOI: 10.1175/MWR2904.1.

Gneiting, T and Ranjan, R (2013). Combining predictive distributions. Electronic Journal of Statistics 7,

1747–1782. DOI: 10.1214/13-EJS823.

Godahewa, R, Bergmeir, C, Webb, GI, Hyndman, RJ, and Montero-Manso, P (2021a). Monash Time

Series Forecasting Archive. In: Neural Information Processing Systems Track on Datasets and Benchmarks.

Forthcoming.

Godahewa, R, Bergmeir, C, Webb, GI, Hyndman, RJ, and Montero-Manso, P (2021b). Monash Time

Series Forecasting Repository. https://forecastingdata.org/.

46

Graham, JR (1996). Is a Group of Economists Better Than One? Than None? Journal of Business 69(2),

193–232.

Granger, CWJ and Jeon, Y (2004). Thick modeling. Economic Modelling 21(2), 323–343. DOI: 10.1016/

S0264-9993(03)00017-8.

Granger, CWJ and Ramanathan, R (1984). Improved methods of combining forecasts. Journal of

Forecasting 3(2), 197–204. DOI: 10.1002/for.3980030207.

Green, KC and Armstrong, JS (2015). Simple versus complex forecasting: The evidence. Journal of

Business Research 68(8), 1678–1685. DOI: 10.1016/j.jbusres.2015.03.026.

Gr ¨onquist, P, Yao, C, Ben-Nun, T, Dryden, N, Dueben, P, Li, S, and Hoeﬂer, T (2021). Deep learning

for post-processing ensemble weather forecasts. Philosophical Transactions of the Royal Society A
379(2194), 20200092. DOI: 10.1098/rsta.2020.0092.

Grushka-Cockayne, Y and Jose, VRR (2020). Combining prediction intervals in the M4 competition.
International Journal of Forecasting 36(1), 178–185. DOI: 10.1016/j.ijforecast.2019.04.015.

Grushka-Cockayne, Y, Jose, VRR, and Lichtendahl Jr, KC (2017a). Ensembles of Overﬁt and Overconﬁ-

dent Forecasts. Management Science 63(4), 1110–1130. DOI: 10.1287/mnsc.2015.2389.

Grushka-Cockayne, Y, Lichtendahl Jr, KC, Jose, VRR, and Winkler, RL (2017b). Quantile evaluation,

sensitivity to bracketing, and sharing business payoffs. Operations Research 65(3), 712–728. DOI:
10.1287/opre.2017.1588.

Guilhaumon, F (2019). mmSAR: multimodel Species-Area Relationships. R package version 1.0. http:

//mmsar.r-forge.r-project.org/.

Gunter, SI (1992). Nonnegativity restricted least squares combinations. International Journal of Forecast-

ing 8(1), 45–59. DOI: 10.1016/0169-2070(92)90006-U.

Hall, SG and Mitchell, J (2007). Combining density forecasts. International Journal of Forecasting 23(1),

1–13. DOI: 10.1016/j.ijforecast.2006.08.001.

Harrald, PG and Kamstra, M (1997). Evolving artiﬁcial neural networks to combine ﬁnancial forecasts.

IEEE Transactions on Evolutionary Computation 1(1), 40–52. DOI: 10.1109/4235.585891.

Harvey, DI, Leybourne, SJ, and Newbold, P (1998). Tests for forecast encompassing. Journal of Business

& Economic Statistics 16(2), 254–259. DOI: 10.1080/07350015.1998.10524759.

Henderson, T and Fulcher, BD (2021). An empirical evaluation of time-series feature sets. In: 2021
International Conference on Data Mining Workshops (ICDMW). IEEE, pp.1032–1038. DOI: 10.1109/
ICDMW53433.2021.00134.

Hendry, DF and Clements, MP (2004). Pooling of forecasts. Econometrics Journal 7(1), 1–31.

Hibon, M and Evgeniou, T (2005). To combine or not to combine: selecting among forecasts and their
combinations. International Journal of Forecasting 21(1), 15–24. DOI: 10.1016/j.ijforecast.2004.
05.002.

Hoeting, JA, Madigan, D, Raftery, AE, and Volinsky, CT (1999). Bayesian model averaging: a tutorial.

Statistical Science 14(4), 382–417. DOI: 10.1214/ss/1009212519.

Hollyman, R, Petropoulos, F, and Tipping, ME (2021). Understanding forecast reconciliation. European

Journal of Operational Research 294(1), 149–160. DOI: 10.1016/j.ejor.2021.01.017.

Hora, SC (2004). Probability Judgments for Continuous Quantities: Linear Combinations and Calibra-

tion. Management Science 50(5), 597–604. DOI: 10.1287/mnsc.1040.0205.

Hsiao, C and Wan, SK (2014). Is there an optimal forecast combination? Journal of Econometrics 178,

294–309. DOI: 10.1016/j.jeconom.2013.11.003.

47

Hyndman, RJ (2020). A brief history of forecasting competitions. International Journal of Forecasting

36(1), 7–14. DOI: 10.1016/j.ijforecast.2019.03.015.

Hyndman, RJ, Ahmed, RA, Athanasopoulos, G, and Shang, HL (2011). Optimal combination forecasts
for hierarchical time series. Computational Statistics & Data Analysis 55(9), 2579–2589. DOI: 10.1016/
j.csda.2011.03.006.

Hyndman, RJ and Athanasopoulos, G (2021). Forecasting: principles and practice. 3rd. OTexts: Melbourne,

Australia.

Hyndman, RJ, Athanasopoulos, G, Bergmeir, C, Caceres, G, Chhay, L, O’Hara-Wild, M, Petropoulos, F,

Razbash, S, Wang, E, and Yasmeen, F (2021). forecast: Forecasting functions for time series and linear
models. R package version 8.15. https://pkg.robjhyndman.com/forecast/.

Hyndman, RJ, Kang, Y, Montero-Manso, P, Talagala, TS, Wang, E, Yang, Y, O’Hara-Wild, M, Ben

Taieb, S, Hanqing, C, Lake, DK, Laptev, N, and Moorman, JR (2019). tsfeatures: Time Series Feature
Extraction. R package version 1.0.2. https://CRAN.R-project.org/package=tsfeatures.

Jore, AS, Mitchell, J, and Vahey, SP (2010). Combining forecast densities from VARs with uncertain

instabilities. Journal of Applied Economics 25(4), 621–634. DOI: 10.1002/jae.1162.

Jose, VRR, Grushka-Cockayne, Y, and Lichtendahl Jr, KC (2014). Trimmed Opinion Pools and the
Crowd’s Calibration Problem. Management Science 60(2), 463–475. DOI: 10.1287/mnsc.2013.1781.

Jose, VRR and Winkler, RL (2008). Simple robust averages of forecasts: Some empirical results. Interna-

tional Journal of Forecasting 24(1), 163–169. DOI: 10.1016/j.ijforecast.2007.06.001.

Jose, VRR and Winkler, RL (2009). Evaluating Quantile Assessments. Operations Research 57(5), 1287–

1297. DOI: 10.1287/opre.1080.0665.

Jouini, MN and Clemen, RT (1996). Copula Models for Aggregating Expert Opinions. Operations

Research 44(3), 444–457. DOI: 10.1287/opre.44.3.444.

Judge, GG and Bock, ME (1978). The statistical implications of pre-test and Stein-rule estimators in econo-

metrics. Amsterdam: North Holland.

Kang, H (1986). Unstable Weights in the Combination of Forecasts. Management Science 32(6), 683–695.

DOI: 10.1287/mnsc.32.6.683.

Kang, Y, Cao, W, Petropoulos, F, and Li, F (2021). Forecast with forecasts: Diversity matters. European

Journal of Operational Research. DOI: 10.1016/j.ejor.2021.10.024.

Kang, Y, Hyndman, RJ, and Li, F (2020a). GRATIS: GeneRAting TIme Series with diverse and control-
lable characteristics. Statistical Analysis and Data Mining 13(4), 354–376. DOI: 10.1002/sam.11461.

Kang, Y, Hyndman, RJ, and Smith-Miles, K (2017). Visualising forecasting algorithm performance
using time series instance spaces. International Journal of Forecasting 33(2), 345–358. DOI: 10.1016/j.
ijforecast.2016.09.004.

Kang, Y, Li, F, Hyndman, RJ, O’Hara-Wild, M, and Zhao, B (2020b). gratis: GeneRAting TIme Series
with diverse and controllable characteristics. R package version 0.2-1. https://CRAN.R-project.org/
package=gratis.

Kang, Y, Spiliotis, E, Petropoulos, F, Athiniotis, N, Li, F, and Assimakopoulos, V (2020c). D´ej`a vu: A

data-centric forecasting approach through time series cross-similarity. Journal of Business Research.
DOI: 10.1016/j.jbusres.2020.10.051.

Kapetanios, G, Mitchell, J, Price, S, and Fawcett, N (2015). Generalised density forecast combinations.

Journal of Econometrics 188(1), 150–165. DOI: 10.1016/j.jeconom.2015.02.047.

48

Kıs¸ınbay, T (2010). The use of encompassing tests for forecast combinations. Journal of Forecasting 29(8),

715–727. DOI: 10.1002/for.1170.

Kolassa, S (2011). Combining exponential smoothing forecasts using Akaike weights. International

Journal of Forecasting 27(2), 238–251. DOI: 10.1016/j.ijforecast.2010.04.006.

Koop, G (2003). Bayesian Econometrics. Wiley.

Koop, G and Korobilis, D (2012). Forecasting inﬂation using dynamic model averaging. International

Economic Review 53(3), 867–886. DOI: 10.1111/j.1468-2354.2012.00704.x.

Koop, G and Potter, S (2003). Forecasting in large macroeconomic panels using Bayesian model

averaging. FRB NY Staff Report No. 163. DOI: 10.2139/ssrn.892860.

Kourentzes, N, Barrow, D, and Petropoulos, F (2019). Another look at forecast selection and combi-

nation: Evidence from forecast pooling. International Journal of Production Economics 209(February
2018), 226–235. DOI: 10.1016/j.ijpe.2018.05.019.

Kourentzes, N, Barrow, DK, and Crone, SF (2014a). Neural network ensemble operators for time series
forecasting. Expert Systems with Applications 41(9), 4235–4244. DOI: 10.1016/j.eswa.2013.12.011.

Kourentzes, N and Petropoulos, F (2016). Forecasting with multivariate temporal aggregation: The

case of promotional modelling. International Journal of Production Economics 181, 145–153. DOI:
10.1016/j.ijpe.2015.09.011.

Kourentzes, N, Petropoulos, F, and Trapero, JR (2014b). Improving forecasting by estimating time

series structural components across multiple frequencies. International Journal of Forecasting 30(2),
291–302. DOI: 10.1016/j.ijforecast.2013.09.006.

Kourentzes, N, Rostami-Tabar, B, and Barrow, DK (2017). Demand forecasting by temporal aggregation:
Using optimal or multiple aggregation levels? Journal of Business Research 78, 1–9. DOI: 10.1016/j.
jbusres.2017.04.016.

Krasnopolsky, VM and Lin, Y (2012). A neural network nonlinear multimodel ensemble to improve
precipitation forecasts over continental US. Advances in Meteorology 2012. DOI: 10.1155/2012/
649450.

Kullback, S and Leibler, RA (1951). On information and sufﬁciency. Annals of Mathematical Statistics

22(1).

Lahiri, K, Peng, H, and Zhao, Y (2015). Testing the Value of Probability Forecasts for Calibrated
Combining. International Journal of Forecasting 31(1), 113–129. DOI: 10.1016/j.ijforecast.2014.
03.005.

Larrick, RP and Soll, JB (2006). Intuitions about combining opinions: Misappreciation of the averaging

principle. Management Science 52(1), 111–127. DOI: 10.1287/mnsc.1050.0459.

Leamer, EE (1978). Speciﬁcation Searches: Ad Hoc Inference with Nonexperimental Data. Wiley.

Lemke, C and Gabrys, B (2010). Meta-learning for time series forecasting and forecast combination.

Neurocomputing 73(10), 2006–2016. DOI: 10.1016/j.neucom.2009.09.020.

Li, L, Kang, Y, and Li, F (2022). Bayesian forecast combination using time-varying features. International

Journal of Forecasting. DOI: 10.1016/j.ijforecast.2022.06.002.

Li, X, Kang, Y, and Li, F (2020). Forecasting with time series imaging. Expert Systems with Applications

160(113680), 113680. DOI: 10.1016/j.eswa.2020.113680.

Lichtendahl Jr, KC, Grushka-Cockayne, Y, and Winkler, RL (2013). Is it better to average probabilities

or quantiles? Management Science 59(7), 1594–1611. DOI: 10.1287/mnsc.1120.1667.

49

Lichtendahl Jr, KC and Winkler, RL (2020). Why do some combinations perform better than others?
International Journal of Forecasting 36(1), 142–149. DOI: 10.1016/j.ijforecast.2019.03.027.

Lichtendahl Jr, KC, Grushka-Cockayne, Y, Jose, VR, and Winkler, RL (2022). Extremizing and Antiex-
tremizing in Bayesian Ensembles of Binary-Event Forecasts. Operations Research. DOI: 10.1287/
opre.2021.2176.

Lichtendahl Jr, KC, Grushka-Cockayne, Y, and Pfeifer, PE (2013). The wisdom of competitive crowds.

Operations Research 61(6), 1383–1398. DOI: 10.1287/opre.2013.1213.

Lorenz, EN (1963). Deterministic Nonperiodic Flow. Journal of Atmospheric Sciences 20(2), 130–141. DOI:

10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2.

Lubba, CH, Sethi, SS, Knaute, P, Schultz, SR, Fulcher, BD, and Jones, NS (2019). catch22: CAnonical
Time-series CHaracteristics. Data Mining and Knowledge Discovery 33(6), 1821–1852. DOI: 10.1007/
s10618-019-00647-x.

Ma, S and Fildes, R (2021). Retail sales forecasting with meta-learning. European Journal of Operational

Research 288(1), 111–128. DOI: 10.1016/j.ejor.2020.05.038.

Makridakis, S, Spiliotis, E, and Assimakopoulos, V (2022). The M5 accuracy competition: Results,
ﬁndings and conclusions. International Journal of Forecasting. DOI: 10.1016/j.ijforecast.2021.
11.013.

Makridakis, S (1996). Forecasting: its role and value for planning and strategy. International Journal of

Forecasting 12(4), 513–537. DOI: 10.1016/S0169-2070(96)00677-2.

Makridakis, S, Andersen, A, Carbone, R, Fildes, R, Hibon, M, Lewandowski, R, Newton, J, Parzen, E,

and Winkler, R (1982). The accuracy of extrapolation (time series) methods: Results of a forecasting
competition. Journal of Forecasting 1(2), 111–153. DOI: 10.1002/for.3980010202.

Makridakis, S and Hibon, M (2000). The M3-Competition: results, conclusions and implications.

International Journal of Forecasting 16(4), 451–476. DOI: 10.1016/S0169-2070(00)00057-1.

Makridakis, S, Spiliotis, E, and Assimakopoulos, V (2020a). The M4 Competition: 100,000 time series
and 61 forecasting methods. International Journal of Forecasting 36(1), 54–74. DOI: 10 . 1016 / j .
ijforecast.2019.04.014.

Makridakis, S, Spiliotis, E, Assimakopoulos, V, Chen, Z, Gaba, A, Tsetlin, I, and Winkler, RL (2020b).

The M5 Uncertainty competition: Results, ﬁndings and conclusions. International Journal of Forecast-
ing, 1–24. DOI: 10.1016/j.ijforecast.2021.10.009.

Makridakis, S and Winkler, RL (1983). Averages of Forecasts: Some Empirical Results. DOI: 10.1287/mnsc.

29.9.987.

Mannes, AE, Soll, JB, and Larrick, RP (2014). The wisdom of select crowds. Journal of Personality and

Social Psychology 107(2), 276–299. DOI: 10.1037/a0036677.

Maqsood, I, Khan, MR, and Abraham, A (2004). An ensemble of neural networks for weather forecast-

ing. Neural Computing & Applications 13(2), 112–122. DOI: 10.1007/s00521-004-0413-4.

Martin, GM, Loaiza-Maya, R, Maneesoonthorn, W, Frazier, DT, and Ram´ırez-Hassan, A (2021).

Optimal probabilistic forecasts: When do they work? International Journal of Forecasting. DOI:
10.1016/j.ijforecast.2021.05.008.

Mass, CF (2003). IFPS and the Future of the National Weather Service. Weather and Forecasting 18(1),

75–79. DOI: 10.1175/1520-0434(2003)018<0075:IATFOT>2.0.CO;2.

50

McAlinn, K, Aastveit, KA, Nakajima, J, and West, M (2020). Multivariate Bayesian Predictive Synthesis

in Macroeconomic Forecasting. Journal of the American Statistical Association 115(531), 1092–1110.
DOI: 10.1080/01621459.2019.1660171.

McAlinn, K and West, M (2019). Dynamic Bayesian predictive synthesis in time series forecasting.

Journal of Econometrics 210(1), 155–169. DOI: 10.1016/j.jeconom.2018.11.010.

McNees, SK (1992). The uses and abuses of ‘consensus’ forecasts. Journal of Forecasting 11(8), 703–710.

DOI: 10.1002/for.3980110807.

Montero-Manso, P (2019). M4metalearning: Metalearning Tools For Time Series Forecasting. R package

version 0.0.0.9000. https://github.com/robjhyndman/M4metalearning.

Montero-Manso, P, Athanasopoulos, G, Hyndman, RJ, and Talagala, TS (2020). FFORMA: Feature-
based forecast model averaging. International Journal of Forecasting 36(1), 86–92. DOI: 10.1016/j.
ijforecast.2019.02.011.

Moon, J, Jung, S, Rew, J, Rho, S, and Hwang, E (2020). Combination of short-term load forecasting
models based on a stacking ensemble approach. Energy and Buildings 216, 109921. DOI: 10.1016/j.
enbuild.2020.109921.

Moral-Benito, E (2015). Model averaging in economics: An overview. Journal of Economic Surveys 29(1),

46–75. DOI: 10.1111/joes.12044.

Morris, PA (1974). Decision analysis expert use. Management Science 20(9). DOI: 10.1287/mnsc.20.9.

1233.

Morris, PA (1977). Combining expert judgments: A Bayesian approach. Management Science 23(7),

667–787. DOI: 10.1287/mnsc.23.7.679.

Newbold, P and Granger, CWJ (1974). Experience with forecasting univariate time series and the

combination of forecasts. Journal of the Royal Statistical Society: Series A (General) 137(2), 131–146.
DOI: 10.2307/2344546.

Newbold, P and Harvey, DI (2004). “Forecast combination and encompassing”. In: A Companion to

Economic Forecasting. Ed. by MP Clements and DF Hendry. Blackwell Publishing. Chap. 12. DOI:
10.1002/9780470996430.ch12.

Nowotarski, J, Raviv, E, Tr ¨uck, S, and Weron, R (2014). An empirical comparison of alternative schemes
for combining electricity spot price forecasts. Energy Economics 46, 395–412. DOI: 10.1016/j.eneco.
2014.07.014.

O’Hagan, A, Buck, CE, Daneshkhah, A, Richard Eiser, J, Garthwaite, PH, Jenkinson, DJ, Oakley, JE,

and Rakow, T (2006). Uncertain Judgements: Eliciting Experts’ Probabilities. John Wiley & Sons. DOI:
10.1002/0470033312.

O’Hara-Wild, M, Hyndman, RJ, Wang, E, Cook, D, Talagala, TS, and Chhay, L (2021). feasts: Feature
Extraction and Statistics for Time Series. R package version 0.2.2. https://CRAN.R-project.org/
package=feasts.

¨Oller, LE (1978). A Method for Pooling Forecasts. Journal of the Operational Research Society 29(1), 55–63.

DOI: 10.1057/jors.1978.8.

Opschoor, A, Dijk, D van, and Wel, M van der (2017). Combining density forecasts using focused

scoring rules. Journal of Applied Economics 32(7), 1298–1313. DOI: 10.1002/jae.2575.

Oreshkin, BN, Carpov, D, Chapados, N, and Bengio, Y (2019). N-BEATS: Neural basis expansion analysis

for interpretable time series forecasting. DOI: 10.48550/ARXIV.1905.10437.

51

Palm, FC and Zellner, A (1992). To combine or not to combine? issues of combining forecasts. Journal

of Forecasting 11(8), 687–701. DOI: 10.1002/for.3980110806.

Park, S and Budescu, DV (2015). Aggregating multiple probability intervals to improve calibration.

Judgment and Decision Making 10(2), 130–143.

Patton, AJ and Timmermann, A (2007). Properties of optimal forecasts under asymmetric loss and
nonlinearity. Journal of Econometrics 140(2), 884–918. DOI: 10.1016/j.jeconom.2006.07.018.

Pauwels, LL, Radchenko, P, and Vasnev, AL (2020). Higher moment constraints for predictive density

combination. Working Paper 2020-45. CAMA. DOI: 10.2139/ssrn.3593124.

Pauwels, LL and Vasnev, AL (2016). A note on the estimation of optimal weights for density forecast
combinations. International Journal of Forecasting 32(2), 391–397. DOI: 10.1016/j.ijforecast.2015.
09.002.

Pawlikowski, M and Chorowska, A (2020). Weighted ensemble of statistical models. International

Journal of Forecasting 36(1), 93–97. DOI: 10.1016/j.ijforecast.2019.03.019.

Petropoulos, F, Apiletti, D, Assimakopoulos, V, Babai, MZ, Barrow, DK, Ben Taieb, S, Bergmeir, C,

Bessa, RJ, Bijak, J, Boylan, JE, Browell, J, Carnevale, C, Castle, JL, Cirillo, P, Clements, MP, Cordeiro,

C, Oliveira, FLC, Baets, SD, Dokumentov, A, Ellison, J, Fiszeder, P, Franses, PH, Frazier, DT,

Gilliland, M, G ¨on ¨ul, MS, Goodwin, P, Grossi, L, Grushka-Cockayne, Y, Guidolin, M, Guidolin,

M, Gunter, U, Guo, X, Guseo, R, Harvey, N, Hendry, DF, Hollyman, R, Januschowski, T, Jeon, J,

Jose, VRR, Kang, Y, Koehler, AB, Kolassa, S, Kourentzes, N, Leva, S, Li, F, Litsiou, K, Makridakis,
S, Martin, GM, Martinez, AB, Meeran, S, Modis, T, Nikolopoulos, K, ¨Onkal, D, Paccagnini, A,
Panagiotelis, A, Panapakidis, I, Pav´ıa, JM, Pedio, M, Pedregal, DJ, Pinson, P, Ramos, P, Rapach,

DE, Reade, JJ, Rostami-Tabar, B, Rubaszek, M, Sermpinis, G, Shang, HL, Spiliotis, E, Syntetos,

AA, Talagala, PD, Talagala, TS, Tashman, L, Thomakos, D, Thorarinsdottir, T, Todini, E, Arenas,

JRT, Wang, X, Winkler, RL, Yusupova, A, and Ziel, F (2022). Forecasting: theory and practice.
International Journal of Forecasting 38(3), 705–871. DOI: 10.1016/j.ijforecast.2021.11.001.

Petropoulos, F, Hyndman, RJ, and Bergmeir, C (2018a). Exploring the sources of uncertainty: Why does

bagging for time series forecasting work? European Journal of Operational Research 268(2), 545–554.
DOI: 10.1016/j.ejor.2018.01.045.

Petropoulos, F, Kourentzes, N, Nikolopoulos, K, and Siemsen, E (2018b). Judgmental selection of
forecasting models. Journal of Operations Management 60, 34–46. DOI: 10.1016/j.jom.2018.05.005.

Petropoulos, F, Makridakis, S, Assimakopoulos, V, and Nikolopoulos, K (2014). ’Horses for Courses’
in demand forecasting. European Journal of Operational Research 237(1), 152–163. DOI: 10.1016/j.
ejor.2014.02.036.

Petropoulos, F and Spiliotis, E (2021). The wisdom of the data: Getting the most out of univariate time

series forecasting. Forecasting 3(3), 478–497. DOI: doi.org/10.3390/forecast3030029.

Petropoulos, F and Svetunkov, I (2020). A simple combination of univariate models. International

Journal of Forecasting 36(1), 110–115. DOI: 10.1016/j.ijforecast.2019.01.006.

Poncela, P, Rodr´ıguez, J, S´anchez-Mangas, R, and Senra, E (2011). Forecast combination through
dimension reduction techniques. International Journal of Forecasting 27(2), 224–237. DOI: 10.1016/j.
ijforecast.2010.01.012.

Raftery, AE, Gneiting, T, Balabdaoui, F, and Polakowski, M (2005). Using Bayesian model averaging to
calibrate forecast ensembles. Monthly Weather Review 133(5), 1155–1174. DOI: 10.1175/mwr2906.1.

52

Raftery, AE, K´arn ´y, M, and Ettler, P (2010). Online Prediction Under Model Uncertainty via Dynamic
Model Averaging: Application to a Cold Rolling Mill. Technometrics 52(1), 52–66. DOI: 10.1198/
TECH.2009.08104.

Raftery, AE, Madigan, D, and Hoeting, JA (1997). Bayesian Model Averaging for Linear Regression
Models. Journal of the American Statistical Association 92(437), 179–191. DOI: 10.1080/01621459.
1997.10473615.

Ranjan, R and Gneiting, T (2010). Combining probability forecasts. Journal of the Royal Statistical Society:

Series B (Statistical Methodology) 72(1), 71–91. DOI: 10.1111/j.1467-9868.2009.00726.x.

Rapach, DE and Strauss, JK (2008). Forecasting US employment growth using forecast combining

methods. Journal of Forecasting 27(1), 75–93. DOI: 10.1002/for.1051.

Rasp, S and Lerch, S (2018). Neural Networks for Postprocessing Ensemble Weather Forecasts. Monthly

Weather Review 146(11), 3885–3900. DOI: 10.1175/MWR-D-18-0187.1.

Rasp, S and Thuerey, N (2021). Data-driven medium-range weather prediction with a resnet pretrained

on climate simulations: A new model for WeatherBench. Journal of Advances in Modeling Earth
Systems 13(2). DOI: 10.1029/2020ms002405.

Ratcliff, R (1979). Group reaction time distributions and an analysis of distribution statistics. Psycholog-

ical Bulletin 86(3), 446–461.

Ray, EL, Brooks, LC, Bien, J, Biggerstaff, M, Bosse, NI, Bracher, J, Cramer, EY, Funk, S, Gerding, A,

Johansson, MA, Rumack, A, Wang, Y, Zorn, M, Tibshirani, RJ, and Reich, NG (2022). Comparing

trained and untrained probabilistic ensemble forecasts of COVID-19 cases and deaths in the United States.
DOI: 10.48550/ARXIV.2201.12387.

Rendon-Sanchez, JF and De Menezes, LM (2019). Structural combination of seasonal exponential

smoothing forecasts applied to load forecasting. European Journal of Operational Research 275(3),
916–924. DOI: 10.1016/j.ejor.2018.12.013.

Ribeiro, GT, Mariani, VC, and Coelho, LdS (2019). Enhanced ensemble structures using wavelet neural

networks applied to short-term load forecasting. Engineering Applications of Artiﬁcial Intelligence 82,
272–281. DOI: 10.1016/j.engappai.2019.03.012.

Ribeiro, MHDM and Santos Coelho, L dos (2020). Ensemble approach based on bagging, boosting and

stacking for short-term prediction in agribusiness time series. Applied Soft Computing 86, 105837.
DOI: 10.1016/j.asoc.2019.105837.

Rossi, B (2013). “Chapter 21 - Advances in Forecasting under Instability”. In: Handbook of Economic

Forecasting. Ed. by G Elliott and A Timmermann. Vol. 2. Handbook of Economic Forecasting.
Elsevier, pp.1203–1324. DOI: https://doi.org/10.1016/B978-0-444-62731-5.00021-X.

Rossi, B (2021). Forecasting in the Presence of Instabilities: How We Know Whether Models Predict
Well and How to Improve Them. Journal of Economic Literature 59(4), 1135–90. DOI: 10.1257/jel.
20201479.

Satop¨a¨a, VA, Pemantle, R, and Ungar, LH (2016). Modeling probability forecasts via information

diversity. Journal of the American Statistical Association 111(516), 1623–1633.

Scher, S (2018). Toward data-driven weather and climate forecasting: Approximating a simple general

circulation model with deep learning. Geophysical Research Letters 45(22), 12, 616–12, 622. DOI:
10.1029/2018gl080704.

Scher, S and Messori, G (2018). Predicting weather forecast uncertainty with machine learning. Quar-

terly Journal of the Royal Meteorological Society 144(717), 2830–2841. DOI: 10.1002/qj.3410.

53

Scher, S and Messori, G (2021). Ensemble methods for neural network-based weather forecasts. Journal

of Advances in Modeling Earth Systems 13(2). DOI: 10.1029/2020ms002331.

Schwarz, G (1978). Estimating the Dimension of a Model. Annals of Statistics 6(2), 461–464. DOI:

10.1214/aos/1176344136.

Semenoglou, AA, Spiliotis, E, Makridakis, S, and Assimakopoulos, V (2020). Investigating the accuracy

of cross-learning time series forecasting methods. International Journal of Forecasting 37(3), 1072–
1084. DOI: 10.1016/j.ijforecast.2020.11.009.

Shaub, D (2019). Fast and accurate yearly time series forecasting with forecast combinations. Interna-

tional Journal of Forecasting. DOI: 10.1016/j.ijforecast.2019.03.032.

Shi, SM, Da Xu, L, and Liu, B (1999). Improving the accuracy of nonlinear combined forecasting using
neural networks. Expert Systems with Applications 16(1), 49–54. DOI: 10.1016/S0957- 4174(98)
00030-X.

Sloughter, JM, Gneiting, T, and Raftery, AE (2010). Probabilistic Wind Speed Forecasting Using

Ensembles and Bayesian Model Averaging. Journal of the American Statistical Association 105(489),
25–35. DOI: 10.1198/jasa.2009.ap08615.

Smith, J and Wallis, KF (2009). A simple explanation of the forecast combination puzzle. Oxford Bulletin

of Economics and Statistics 71(3), 331–355. DOI: 10.1111/j.1468-0084.2008.00541.x.

Smyl, S and Hua, NG (2019). Machine learning methods for GEFCom2017 probabilistic load orecasting.
International Journal of Forecasting 35(4), 1424–1431. DOI: 10.1016/j.ijforecast.2019.02.002.

Steel, MF (2020). Model averaging and its use in economics. Journal of Economic Literature 58(3), 644–719.

DOI: 10.1257/jel.20191385.

Stock, JH and Watson, MW (1998). A Comparison of Linear and Nonlinear Univariate Models for Forecasting

Macroeconomic Time Series. Working Paper 6607. National Bureau of Economic Research. DOI:
10.3386/w6607.

Stock, JH and Watson, MW (1999). Forecasting inﬂation. Journal of Monetary Economics 44(2), 293–335.

DOI: 10.1016/S0304-3932(99)00027-6.

Stock, JH and Watson, MW (2003). How Did Leading Indicator Forecasts Perform during the 2001

Recession? FRB Richmond Economic Quarterly 89(3), 71–90.

Stock, JH and Watson, MW (2004). Combination forecasts of output growth in a seven-country data

set. Journal of Forecasting 23(6), 405–430. DOI: 10.1002/for.928.

Stone, M (1961). The Opinion Pool. Annals of Mathematical Statistics 32(4), 1339–1342.

Sugiura, N (1978). Further analysts of the data by akaike’s information criterion and the ﬁnite correc-

tions: Further analysts of the data by akaike’s. Communications in Statistics-Theory and Methods. DOI:
10.1080/03610927808827599.

Surowiecki, J (2005). The wisdom of crowds. Anchor.

Syntetos, AA, Boylan, JE, and Disney, SM (2009). Forecasting for inventory planning: a 50-year review.

Journal of the Operational Research Society 60(1), S149–S160. DOI: 10.1057/jors.2008.173.

Taillardat, M, Foug`eres, AL, Naveau, P, and Mestre, O (2019). Forest-Based and Semiparametric

Methods for the Postprocessing of Rainfall Ensemble Forecasting. Weather and Forecasting 34(3),
617–634. DOI: 10.1175/WAF-D-18-0149.1.

Talagala, TS, Hyndman, RJ, and Athanasopoulos, G (2018). Meta-learning how to forecast time series.

Monash Econometrics and Business Statistics Working Papers 6, 18.

54

Terui, N and Dijk, HK van (2002). Combined forecasts from linear and nonlinear time series models.

International Journal of Forecasting 18(3), 421–438. DOI: 10.1016/S0169-2070(01)00120-0.

Thomas, EAC and Ross, BH (1980). On appropriate procedures for combining probability distributions
within the same family. Journal of Mathematical Psychology 21(2), 136–152. DOI: 10.1016/0022-
2496(80)90003-6.

Thomson, ME, Pollock, AC, ¨Onkal, D, and G ¨on ¨ul, MS (2019). Combining forecasts: Performance and
coherence. International Journal of Forecasting 35(2), 474–484. DOI: 10.1016/j.ijforecast.2018.
10.006.

Thorey, J, Chaussin, C, and Mallet, V (2018). Ensemble forecast of photovoltaic power with online
CRPS learning. International Journal of Forecasting 34(4), 762–773. DOI: 10.1016/j.ijforecast.
2018.05.007.

Thorey, J, Mallet, V, and Baudin, P (2017). Online learning with the Continuous Ranked Probability

Score for ensemble forecasting. Quarterly Journal of the Royal Meteorological Society 143(702), 521–529.
DOI: 10.1002/qj.2940.

Timmermann, A (2006). “Forecast Combinations”. In: Handbook of Economic Forecasting. Ed. by G Elliott,
CWJ Granger, and A Timmermann. Vol. 1. Elsevier. Chap. 4, pp.135–196. DOI: 10.1016/S1574-
0706(05)01004-9.

Trapero, JR, Card ´os, M, and Kourentzes, N (2019). Quantile forecast optimal combination to en-
hance safety stock estimation. International Journal of Forecasting 35(1), 239–250. DOI: 10.1016/j.
ijforecast.2018.05.009.

Turner, BM, Steyvers, M, Merkle, EC, Budescu, DV, and Wallsten, TS (2014). Forecast aggregation via

recalibration. Machine Learning 95(3), 261–289. DOI: 10.1007/s10994-013-5401-4.

Vannitsem, S, Bremnes, JB, Demaeyer, J, Evans, GR, Flowerdew, J, Hemri, S, Lerch, S, Roberts, N,

Theis, S, Atencia, A, Ben Bouall`egue, Z, Bhend, J, Dabernig, M, De Cruz, L, Hieta, L, Mestre, O,

Moret, L, Plenkovi´c, IO, Schmeits, M, Taillardat, M, Van den Bergh, J, Van Schaeybroeck, B, Whan,

K, and Ylhaisi, J (2021). Statistical Postprocessing for Weather Forecasts: Review, Challenges, and

Avenues in a Big Data World. Bulletin of the American Meteorological Society 102(3), E681–E699. DOI:
10.1175/BAMS-D-19-0308.1.

Vincent, SB (1912). The Functions of the Vibrissae in the Behavior of the White Rat. Kessinger Publishing.

Wallis, KF (2005). Combining density and interval forecasts: A modest proposal. Oxford Bulletin of

Economics and Statistics 67(s1), 983–994. DOI: 10.1111/j.1468-0084.2005.00148.x.

Wang, X, Kang, Y, and Li, F (2022a). Another look at forecast trimming for combinations: robustness,

accuracy and diversity. arXiv preprint arXiv:2208.00139.

Wang, X, Kang, Y, Petropoulos, F, and Li, F (2022b). The uncertainty estimation of feature-based
forecast combinations. Journal of the Operational Research Society 73(5), 979–993. DOI: 10.1080/
01605682.2021.1880297.

Wang, X, Smith-Miles, K, and Hyndman, RJ (2009). Rule induction for forecasting method selection:

Meta-learning the characteristics of univariate time series. Neurocomputing 72(10), 2581–2594. DOI:
10.1016/j.neucom.2008.10.017.

Wang, Y, Zhang, N, Tan, Y, Hong, T, Kirschen, DS, and Kang, C (2019). Combining Probabilistic Load
Forecasts. IEEE Transactions on Smart Grid 10(4), 3664–3674. DOI: 10.1109/TSG.2018.2833869.

55

Weigel, AP, Liniger, MA, and Appenzeller, C (2008). Can multi-model combination really enhance

the prediction skill of probabilistic ensemble forecasts? Quarterly Journal of the Royal Meteorological
Society 134(630), 241–260. DOI: 10.1002/qj.210.

Weiss, CE, Roetzer, GR, and Raviv, E (2018). ForecastComb: Forecast Combination Methods. R package

version 1.3.1. https://CRAN.R-project.org/package=ForecastComb.

West, M (1992). Modelling agent forecast distributions. Journal of the Royal Statistical Society: Series B

(Methodological) 54(2), 553–567. DOI: 10.1111/j.2517-6161.1992.tb01896.x.

West, M and Crosse, J (1992). Modelling probabilistic agent opinion. Journal of the Royal Statistical
Society: Series B (Methodological) 54(1), 285–299. DOI: 10.1111/j.2517-6161.1992.tb01882.x.

Wickramasuriya, SL, Athanasopoulos, G, and Hyndman, RJ (2019). Optimal Forecast Reconciliation

for Hierarchical and Grouped Time Series Through Trace Minimization. Journal of the American
Statistical Association 114(526), 804–819. DOI: 10.1080/01621459.2018.1448825.

Wilson, KJ (2017). An investigation of dependence in expert judgement studies with multiple experts.
International Journal of Forecasting 33(1), 325–336. DOI: 10.1016/j.ijforecast.2015.11.014.

Winkler, RL (1968). The Consensus of Subjective Probability Distributions. Management Science 15(2),

B–61–B–75. DOI: 10.1287/mnsc.15.2.B61.

Winkler, RL (1981). Combining Probability Distributions from Dependent Information Sources. Man-

agement Science 27(4), 479–488. DOI: 10.1287/mnsc.27.4.479.

Winkler, RL and Makridakis, S (1983). The combination of forecasts. Journal of the Royal Statistical

Society: Series A (General) 146(2), 150–157. DOI: 10.2307/2982011.

Wolpert, DH (1992). Stacked Generalization. Neural Networks 5(2), 241–259. DOI: 10.1016/S0893-

6080(05)80023-1.

Wright, JH (2008). Bayesian Model Averaging and exchange rate forecasts. Journal of Econometrics

146(2), 329–341. DOI: 10.1016/j.jeconom.2008.08.012.

Xie, J and Hong, T (2016). GEFCom2014 probabilistic electric load forecasting: An integrated solution

with forecast combination and residual simulation. International Journal of Forecasting 32(3), 1012–
1016. DOI: 10.1016/j.ijforecast.2015.11.005.

Yao, X and Islam, MM (2008). Evolving artiﬁcial neural network ensembles. IEEE Computational

Intelligence Magazine 3(1), 31–42. DOI: 10.1109/MCI.2007.913386.

Yao, Y, Vehtari, A, Simpson, D, and Gelman, A (2018). Using stacking to average Bayesian predictive
distributions (with discussion). Bayesian Analysis 13(3), 917–1007. DOI: 10.1214/17-BA1091.

Zellner, A (2001). “Keep it sophisticatedly simple”. In: Simplicity, Inference and Modelling. Ed. by A

Zellner, HA Keuzenkamp, and M McAleer. Cambridge: Cambridge University Press, pp. 242–262.

Zhang, S, Wang, Y, Zhang, Y, Wang, D, and Zhang, N (2020). Load probability density forecasting
by transforming and combining quantile forecasts. Applied Energy 277, 115600. DOI: 10.1016/j.
apenergy.2020.115600.

Zhao, S and Feng, Y (2020). For2For: Learning to forecast from forecasts. DOI: 10.48550/ARXIV.2001.

04601.

Zhou, ZH (2012). Ensemble methods: foundations and algorithms. CRC press.

Zhou, ZH, Wu, J, and Tang, W (2002). Ensembling neural networks: Many could be better than all.

Artiﬁcial Intelligence 137(1), 239–263. DOI: 10.1016/S0004-3702(02)00190-X.

Zischke, R, Martin, GM, Frazier, DT, and Poskitt, D (2022). The Impact of Sampling Variability on Estimated

Combinations of Distributional Forecasts. DOI: 10.48550/ARXIV.2206.02376.

56

