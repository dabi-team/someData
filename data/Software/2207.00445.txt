Using Neural Networks for Novelty-based Test Selection to
Accelerate Functional Coverage Closure

Xuan Zheng
Trustworthy Systems Laboratory
Department of Computer Science
University of Bristol, UK
dq18619@bristol.ac.uk

Kerstin Eder
Trustworthy Systems Laboratory
Department of Computer Science
University of Bristol, UK
kerstin.eder@bristol.ac.uk

Tim Blackmore
Infineon Technologies
Bristol, UK
tim.blackmore@infineon.com

2
2
0
2

l
u
J

7

]
E
S
.
s
c
[

2
v
5
4
4
0
0
.
7
0
2
2
:
v
i
X
r
a

ABSTRACT
Machine learning (ML) has been used to accelerate the closure of
functional coverage in simulation-based verification. A supervised
ML algorithm, as a prevalent option in the previous work, is used
to bias the test generation or filter the generated tests. However,
for missing coverage events, these algorithms lack the positive
examples to learn from in the training phase. Therefore, the tests
generated or filtered by the algorithms cannot effectively fill the
coverage holes. This is more severe when verifying large-scale
design because the coverage space is larger and the functionalities
are more complex. This paper presents a configurable framework of
test selection based on neural networks (NN), which can achieve a
similar coverage gain as random simulation with far less simulation
effort under three configurations of the framework. Moreover, the
performance of the framework is not limited by the number of
coverage events being hit. A commercial signal processing unit
is used in the experiment to demonstrate the effectiveness of the
framework. Compared to the random simulation, the framework
can reduce up to 53.74% of simulation time to reach 99% coverage
level.

KEYWORDS
Simulation-Based Verification, Functional Coverage, Novelty De-
tection, Deep Learning

ACM Reference Format:
Xuan Zheng, Kerstin Eder, and Tim Blackmore. 2022. Using Neural Networks
for Novelty-based Test Selection to Accelerate Functional Coverage Closure.
In Proceedings of ACM Conference (Conferenceâ€™17). ACM, New York, NY,
USA, 7 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Simulation-based verification is a vital technique that is used to
gain confidence in the functional correctness of digital designs.
Traditionally, the quality of a generated test is measured by various
coverage metrics obtained during simulation. Functional coverage
records whether the specified functionality has been executed by
simulated tests. In practice, functional coverage closure requires

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conferenceâ€™17, July 2017, Washington, DC, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

the generation and simulation of many tests. Although the test
generation is cheap, the simulation is expensive. In addition, the
coverage gain brought by the test simulation sharply decreases
along with the progress of the verification. These barriers slow
down the development of digital design and become more severe
because of the increasing complexity of modern digital designs.

In recent years, ML is reported to effectively automate Cover-
age Directed Test Generation (CDG) and accelerate the closure
of functional coverage [9]. Although the effectiveness of the ML-
CDG framework has been demonstrated in previous works, several
knowledge gaps still exist that prevent ML models to accelerate
simulation-based verification for designs of commercial complexity.
1) Gap 1: Insufficient information for learning the correlation

between tests and the un-covered coverage events.

Most of the previous work adopts supervised learning algorithms
that empirically learn to establish the loop between coverage analy-
sis and test generation, i.e. the logic that would automatically alter
the next tests to be run to target the coverage holes left by previous
simulation rounds [9]. A shortfall affecting the training phase is
that for the un-covered coverage events, there are no simulated
tests for the algorithms to use as positive examples in order to
learn the logic correlation between tests and the un-covered events.
Therefore, the supervised algorithms cannot effectively bias test
generation or filter out generated tests to target the coverage holes
in the next simulation round. Rarely covered events are also affected
in a similar way. To overcome this shortfall, syntax-based distance
metrics have been used to identify coverage events that have been
hit in syntactically close proximity to coverage holes [5]. However,
syntactic proximity, e.g. based on the Hamming distance between
coverage tuples, is no guarantee for semantic proximity.

2) Gap 2: The scalability of ML algorithms to accelerate the
closure of functional coverage for the large-scale designs has not
been sufficiently often demonstrated [9, 11].

The functionality of small designs tends to be simple and func-
tional coverage events are highly correlated to each other, which
alleviates the severity of Gap 1. For instance, if the coverage events
A and B are correlated, then tests that can hit A are also very likely
to hit B. Suppose A has already been covered and B is uncovered,
the ML algorithms can still bias the test generator to produce the
tests that can hit B by only utilizing the learned relation between
tests and A.

On the other hand, Novelty Detection (ND) has also been in-
troduced to simulation-based verification to accelerate coverage
closure [2, 3, 7, 11]. This approach is based on the hypothesis that
dissimilar tests hit dissimilar functional coverage events. Therefore,
simulating dissimilar tests is assumed to increase coverage more

 
 
 
 
 
 
Conferenceâ€™17, July 2017, Washington, DC, USA

Xuan Zheng, Kerstin Eder, and Tim Blackmore

effectively than the random simulation of tests [7]. In [2, 7] and [3],
a One-Class Support Vector Machine (OCSVM) is adopted to per-
form ND where the dissimilarity between tests can be measured by
different kernel-induced distance metrics. Constructing such a dis-
tance metric depends on how tests are presented to the ML model,
i.e., how test vectors are encoded. This restricts the portability of
using the OCSVM techniques across different verification projects.
Nevertheless, ND can also be implemented by an Autoencoder
that does not require a user-defined distance metric to measure the
novelty between tests [11]. In [11], an autoencoder is trained to
reconstruct the simulated tests and future tests with high recon-
struction errors are deemed novel. As an unsupervised learning
technique, it has the prominent merit that the training and predic-
tion phases are independent of the coverage space and only the
test information is required. Another work demonstrates that for
the NN with ReLu hidden layers, the values of hidden neurons cor-
responding to the novel input data reside in the corner-activation
regions that have not been activated before [8]. We assume that the
corner-activation region is the sparse region of the hidden neuron
and the density in the input space can be projected onto the neuron
output space. Section 4 confirms this assumption and shows that
the novel tests can be identified by analyzing their associated values
of the hidden neurons.

Inspired by the approaches of test selection and the prominent
characteristics of NN discussed above, this paper presents a Neural
Network Based Novel Test Selector (NNBNTS), of which the input
is test features, and the output space can be configured to represent
different definitions of novelty. NNBNTS selects the tests generated
from the Constraint Random Test Generator (CRTG) to ensure novel
tests are prioritized during simulation. Three configurations of the
output space are given in the paper. The first configuration is to set
the NNBNTS to reconstruct the input test vector and the output
space is the reconstructed input space, i.e. an autoencoder. The
tests with high reconstruction errors are regarded as the novel. The
second configuration is to set the NNBNTS to predict the correlation
between the test feature space and the coverage space. However,
instead of biasing the test generator or filtering the generated tests
according to the prediction result, the novelty score calculated from
the neuron outputs is assigned to each test [6, 8]. Unlike the two
aforementioned configurations that only calculate the novelty score
in the test feature space, the output space in the third configuration
directly generates a non-linear score to indicate the novelty degree
of the generated tests in the coverage space formed by the simulated
tests. From the experimental results, NNBNTS can accelerate the
closure of functional coverage for a particular commercial design
compared to random test simulation. Although the configuration of
the output space should be set before the training and deployment
phase, an explicit distance metric is not required, enhancing the
transferability of NNBNTS between verification projects. Moreover,
the NNBNTS is easy to implement and maintain because it does
not bias CSTG or modify generated tests.

The paper is structured as follows. Section 2 reviews and com-
pares the related work to unveil the research questions yet to solve
and illustrate the inspirations for this paper. Section 3 illustrates
the hypothesis that simulating novel tests can enrich the cumula-
tive coverage gain. The principle of novel test selection and the

Figure 1: Coverage-Driven Verification automated by ML

implementation of NNBNTS are also included in this section. Sec-
tion 4 presents and discusses the experimental results of NNBNTS
in the particular simulation-based verification environment of a
commercial design. Section 5 concludes the paper and discusses the
future works.

2 PREVIOUS WORK
Previous work has embedded ML algorithms in CDG to automati-
cally accelerate the coverage closure using fewer tests than random
simulation [9]. During the training phase, ML empirically learns the
correlation between the constraints used to generate the simulated
tests, the simulated tests (two inputs of ML) and the corresponding
coverage data (output of ML) until the satisfying prediction perfor-
mance of ML is achieved. Afterwards, the trained ML can be either
used to bias the test generation or filter generated tests with the
aim to fill coverage holes or increase existing coverage. Retraining
is necessary if the prediction quality decreases or once a coverage
hole has been covered, leading to an iterative loop of training and
prediction, as shown in Figure 1.

By contrast, ND has been shown to effectively accelerate cover-
age closure by arranging the simulation order of generated tests [2,
3, 7, 11]. It is assumed that tests dissimilar to those already simulated
will execute functionality not yet covered, with the expectation to
detect potential bugs. Existing ND approaches repeatedly select
a batch of un-simulated tests for simulation, these tests are most
dissimilar to the already simulated tests. The more dissimilar a
test is, the higher its novelty score. In the literature, ND is mostly
implemented by unsupervised learning algorithms, which means
the dissimilarity is calculated in the test feature space according
to a distance metric, and the training and prediction phases are
de-coupled from the coverage space. Conducting ND only in the
test feature space is a distinguishing characteristic that makes the
ND-based approaches not suffer from the lack of sufficiently many
examples for un-hit and rarely-hit coverage events, as addressed in
Gap 1.

In [2, 3, 7], ND is implemented by a One-Class Support Vector
Machine (OCSVM) that requires designating a distance metric to
measure the dissimilarity between tests. Nevertheless, no common
distance metric can accurately capture the dissimilarity in different
schemes of test encoding. In [7], every two dimensions of the feature
vectors correspond to a binary bit in the test, with the unknown
state X also being considered. For this test encoding scheme, both

Using Neural Networks for Novelty-based Test Selection to Accelerate Functional Coverage Closure

Conferenceâ€™17, July 2017, Washington, DC, USA

Figure 2: Similarity in Test Feature Space and Coverage
Space

Gaussian and Dot-Product kernel functions can correctly capture
the novelty in the dataset. However, if a test vector consists of
continuous values instead of discrete binary bits, the Dot-Product
function cannot recognize the dissimilar tests in the test feature
space. Besides, in [2], each feature of the test vector is the graph-
based distance from another test vector, for which the Gaussian
function cannot express the dissimilarity between the test vectors.
On the contrary, an autoencoder can also be used to select novel
tests. This only requires each test to be encoded as a test feature
vector without designating an explicit distance metric [11]. Each fea-
ture in the test vector represents the value in each register that con-
figures the functionality of the design. The autoencoder is trained
to reproduce the simulated test vectors and the novelty score is
defined as the mean squared difference between the input and repro-
duced test vectors. A higher novelty score implies a lower number
of similar tests in the training set (simulated tests) and thus the NN
cannot accurately reconstruct the vector. The case study is based
on the signal processing unit of the commercial design with about
6,000 white-box coverage events. The coverage events are internal
signals, which are hard to hit. Therefore, the case study demon-
strates the scalability of using the autoencoder to select novel tests
to accelerate the closure of functional coverage for a large-scale
design.

A NN is also used in [6] as a test selector to accelerate the cov-
erage closure. The NN is trained to infer the empirical correlation
from the test feature space to the coverage space. Unlike other ap-
proaches that rely on the prediction result to bias the test generator,
simulating tests with low-confidence prediction is reported to effec-
tively hit the coverage holes, though the rationale is not presented.
The authors of [8] associate novelty in the input space with the
output value of hidden neurons. Novel data samples are regarded
to have outlier values for the neuron outputs. Inspired by [8], we
assume that the density in the input space can also be projected
onto the neuron output space. Furthermore, we also assume that
the low-confidence outputs in [6] reside in the sparse region of the
output-layer neurons for the trained logistic regression model.

3 METHODOLOGY
3.1 Hypothesis
The principle of the novel test selector is based on the hypothesis
that dissimilar tests exercise dissimilar functionality of the design.
This can be evidenced by more coverage events being hit by a group
of dissimilar tests than a group of similar tests [7].

As it is shown in Figure 2, tests are characterized by various fea-
tures, e.g. these features can be the values driven to configuration
registers (configuring the functionalities of the design), sequences

Figure 3: Novel Test Selection Framework

driven to the design, instructions executed on the processor and
so on. Each test in the feature space is represented as a test vector.
The dissimilarity between test vectors in the feature space can be
correspondingly projected onto the coverage space [7]. This means
the more dissimilar two test vectors are, the more cumulative cover-
age they can jointly achieve. Therefore, if an effective test selection
scheme can identify dissimilar tests before simulation, faster clo-
sure of functional coverage can be achieved. Moreover, a mature
test selection scheme from the industrial perspective should satisfy
two prerequisites. First, the measure of dissimilarity between tests
needs to be general enough so that algorithms can select dissimilar
tests across the projects. Second, the implementation and operation
schemes must be highly automatic to minimise engineering efforts.
In the ML area, comparing the difference between data samples
is known as anomaly detection[1]. Novelty detection and outlier
detection are implementations of anomaly detection. They mainly
differ in the training data and prediction data. In outlier detection,
the training data samples contain dissimilar data samples and the
objective of the ML algorithm is to recognize them (outliers). In
contrast, in novelty detection the training data samples are deemed
to only consist of similar samples and the ML algorithm detects
whether a new sample is novel compared to the training set.

3.2 Test Selection Framework
Based on our hypothesis, a framework for novel test selection
for simulation-based verification is proposed in this section, as
shown in Figure 3. First, un-simulated tests from CRTG are stored
in the generated tests pool before test selection. The test selector
was previously trained with a small number of simulated tests
before being embedded into the verification environment. After
the initial training, the pre-trained test selector is in the selection
phase, in which the objective is to generate the novelty score for
each test. Afterwards a batch of the most novel tests relative to
the simulated tests is selected. Simulating a new batch of tests
dynamically changes the definition of test novelty. To obtain the
optimum performance of the test selector, retraining it with all
the simulated tests after each simulation round is necessary before
selecting a new batch of novel tests. The above process repeats
until either a termination criterion is reached, or all the generated

Conferenceâ€™17, July 2017, Washington, DC, USA

Xuan Zheng, Kerstin Eder, and Tim Blackmore

tests are simulated. The coverage information of simulated tests
can also be logged in the simulated tests pool to form the training
set if it is required by NNBNTS.

3.3 Construction of Test Selector
NNBNTS is based on a feedforward NN that is known as Artificial
Neural Network. The test vector set is driven as the input to the NN
and therefore the number of input space dimensions matches the
number of features used to characterize a test vector. The neurons
of adjacent layers are fully connected. The user-defined parame-
ters for the NN include the number of neurons in each layer, the
number of layers, and the activation functions, which deterministi-
cally influence the performance of the NN. However, tuning these
parameters is not in the scope of this paper. Instead, configuring
the prediction space and the associated novelty function is one of
the main topics of this paper. Herein, three configurations of the
prediction space are described that can be categorized as Density
NNBNTS, Autoencoder NNBNTS and Coverage-Novelty NNBNTS.
For Density NNBNTS, each neuron in the prediction space is
configured to output the probability of a coverage event being hit by
the input test, i.e. it is a logistic regression model that estimates the
correlation between the test space and coverage space. We assume
that the novelty in the input space of the NN can be projected
onto the output space in each subsequent hidden layer. This idea is
inspired by [8], in which the distance between the input data and
the training set is equal to the number of hidden neurons in the
corner-activation areas. The corner-activation areas are ranges of
values that have never occurred in the training set and are reached
by new input when the NN is deployed. Therefore, it also implies the
corner-activation areas are low-density regions. The novelty score
for an un-simulated test in the input space is the sum of the score
components calculated based on the K-nearest neighbours (KNN)
of each hidden-neuron value. The search scope for KNN is within
the set of the neuron values for the simulated tests. We assume
that a positive correlation exists between the summed distances
from the output value of a hidden neuron to its KNN and the local
sparsity of that neuron. This is formally illustrated in Algorithm 1.
Suppose we input an un-simulated test T to the NN and N is
the novelty score for T. ğ‘‚ğ‘¢ğ‘  is the list of output values of all the
hidden neurons for T. ğ‘‚ğ‘  is the list of output values of all the
hidden neurons for the simulated tests (training set). The index
of ğ‘‚ğ‘  is the index of hidden neurons. For instance, ğ‘‚ğ‘  [0] is the
value set that contains outputs of hidden neuron 0 for the simulated
tests. N_K is the set of values in ğ‘‚ğ‘  [ğ‘–] which are KNN to ğ‘‚ğ‘¢ğ‘  [ğ‘–] in
Euclidean space. The sum of Euclidean distances from ğ‘‚ğ‘¢ğ‘  [ğ‘–] to its
KNN is the score component ğ‘› obtained from hidden neuron ğ‘–. N is
defined as the sum of ğ‘› for each hidden neuron. Compared to [8],
Density NNBNTS provides a more detailed approach to comparing
the novelty difference between the input samples rather than just
recognizing the novel inputs.

For the hidden layers in Density NNBNTS, a Leaky ReLu func-
tion is adopted as the activation function rather than the ReLu
function used in [8] for two reasons. First, a ReLu neuron could
never be activated after a large negative gradient flowing through
the neuron, which causes the neuron to always output zero during
the feedforward and backward propagation [12]. Second, the output

of a ReLu neuron corresponding to all the negative inputs is zero,
which causes the loss of density information from the negative ter-
ritory in the input space of the neuron. By contrast, the minimum
value that Leaky ReLu can represent is negative infinity.

Algorithm 1 Calculating a Novelty Score for an Un-simulated Test
in Density NNBNTS

1: ğ‘ â† 0
2: for < ğ‘– in range(0,len(ğ‘‚ğ‘  )> do
Find N_K of ğ‘‚ğ‘¢ğ‘  [ğ‘–] in ğ‘‚ğ‘  [ğ‘–]
3:
for < ğ‘— in range(0,len(ğ‘ _ğ¾)> do

4:

5:

6:

ğ‘› â† ğ· [ğ‘‚ğ‘¢ğ‘  [ğ‘–], ğ‘ _ğ¾ [ ğ‘—]]
ğ‘ â† ğ‘ + ğ‘›

end for

7:
8: end for

Although the Density NNBNTS empirically learns the correlation
between the test feature space and the coverage space, the test
selection is not based on the prediction result of whether an input
test can hit the coverage holes. Gap 1 discussed in Section 1 is subtly
circumvented by acquiring the novelty score of the input data in
the output space of hidden neurons, transferring the prediction task
to the ND task.

On the other hand, NNBNTS can also be configured to compress
input test vectors into lower dimensions and then reconstruct test
vectors from the compressed dimensions. Such a configuration of
a NN is known as Autoencoder in the deep learning area and it
is termed Autoencoder NNBNTS in this paper. The mean squared
difference that expresses the reconstruction error is regarded as the
novelty score as shown in Equation 1.

ğ‘ =

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘—=1

(ğ¼ ğ‘— âˆ’ ğ‘‚ ğ‘— )2

(1)

For an input test vector, ğ¼ ğ‘— is the value of the ğ‘—th feature, ğ‘‚ ğ‘—
is the jth reconstructed feature in the output space and ğ‘› is the
total number of features. In the deployment phase, the higher the
reconstruction error is, the more novel the input test vector is. The
layout of Autoencoder NNBNTS adheres to the constraint that the
number of neurons per layer decreases from the input layer to
the middle layer (encoder part) and then increases to decode the
compressed information back to the original dimensions (decoder
part).

For Autoencoder and Density NNBNTS, the coverage informa-
tion of simulated tests is not utilized to construct the novelty score
function though in the latter approach it is used to train the NN.
We propose an innovative configuration of the prediction space, in
which a single neuron directly estimates how novel an input test is
in the coverage space constructed by the simulated tests. This is a
supervised learning algorithm called Coverage Novelty NNBNTS.
We assume that for a simulated test, if a coverage event hit by the
test is also frequently hit by other simulated tests, then the test is
very similar to the other tests in that coverage-event dimension
(an entry of ğ¶ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦ in Algorithm 2). The overall dissimilarity of
a simulated test in the coverage space is the sum of the dissimi-
larity in each coverage dimension. This is formally illustrated in

Using Neural Networks for Novelty-based Test Selection to Accelerate Functional Coverage Closure

Conferenceâ€™17, July 2017, Washington, DC, USA

Algorithm 2 where N is the coverage-novelty score for a simulated
test and ğ‘›ğ‘ğ‘– is the score component calculated from each coverage
event.

Algorithm 2 Generating a Training Label for a Simulated Test in
Coverage-Novelty NNBNTS

1: ğ‘ â† 0
2: for < ğ‘– in range(0,len(ğ¶ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦)> do
3:

if ğ¶ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦ [ğ‘–] = 0 then

ğ‘›ğ‘ğ‘– â† 0

else

ğ‘›ğ‘ğ‘– â†

1

âˆš
3
2

ğ¶ğ»ğ‘–ğ‘¡ğ‘‡ ğ‘–ğ‘šğ‘’ğ‘  [ğ‘– ]

4:

5:

6:

7:

end if
ğ‘ â† ğ‘ + ğ‘›ğ‘ğ‘–

8:
9: end for

In Algorithm 2, ğ¶ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦ is the vector that records whether each
coverage event has been hit by the test requiring the training label
and ğ¶ğ»ğ‘–ğ‘¡ğ‘‡ ğ‘–ğ‘šğ‘’ğ‘  is the vector that records the number of times each
coverage event has been hit by all the simulated tests. For example,
ğ¶ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘¦ [0] = 1 represents the coverage event 0 is hit by the test and
ğ¶ğ»ğ‘–ğ‘¡ğ‘‡ ğ‘–ğ‘šğ‘’ğ‘  [1] = 20 means twenty simulated tests hit the coverage
event 1. ğ‘›ğ‘ğ‘– is 0 if the test requiring the label does not hit coverage
event ğ‘–. N is the sum of ğ‘›ğ‘ğ‘– over all the coverage events. The denom-
in the else branch distributes more scores to the
inator

1
ğ¶ğ»ğ‘–ğ‘¡ğ‘‡ ğ‘–ğ‘šğ‘’ğ‘ 

âˆš
3
2

rarely-hit coverage events of a simulated test. Thus, the situation
where the test that activates few infrequently hit coverage events
is assigned a lower N than the test that activates many frequently
hit coverage events can be easily mitigated. The labeling algorithm
only includes the coverage information of simulated tests and thus
does not require the test information for the un-hit coverage events.
The trained NN then is used to predict how novel an un-simulated
test is with respect to the coverage space of the simulated tests.

4 EXPERIMENTAL EVALUATION
4.1 The experiment background
In this study, the design under verification is the Signal Processing
Unit (SPU) of the Advanced Driver Assistance Subsystem (ADAS)
in Infineonâ€™s Aurix family of microcontrollers. It is the same design
used in [11]. The SPU processes RADAR data received from sensors
in the vehicle. Besides, the processing is highly configurable via a
large number of configuration registers. Thus, the SPU is a complex,
safety-critical and highly configurable IP.

Simulation-based and formal methods are used to verify the SPU.
The simulation-based verification is metric-driven, constrained-
random verification with the test generation and the simulation
environment being decoupled. A generated test consists of a con-
figuration of the SPU, stored in a database, and the RADAR data
to be processed. Only the configuration is used as test features,
contributing to 290 raw features. Moreover, the features are engi-
neered, for example to condense large, data-centric configuration
fields, leaving a total of 265 final features.

Afterwards, the engineered features are standardized with the
library in Scikit-Learn [10] before being presented to NNBNTS. For
the functional coverage model, there are 8409 white-box functional

coverage events. The real-life project requires 6 months efforts from
the verification team and simulating about 2 million constrained
random tests to achieve the coverage closure (100% coverage). For
the experiment, 3076 tests that give 100% functional coverage are
saved from the completed project. In addition, 82335 tests are pro-
duced by a constrained random test generator (i.e. as random as
possible within the constraints of a valid test), giving a total of
85411 tests. The coverage information of these tests is also known
prior to the experiments.

NNBNTS of all the prediction spaces are constructed by using
Keras [4]. For each NNBNTS in the experiment, a proportionate
structure is configured. The configuration guideline is to keep the
number of neurons in each hidden layer to be either twice, half
of, or the same as the number in the previous layer. The following
vectors illustrate the number of layers and the number of neurons
in each layer for the NNBNTS:

Autoencoder-NNBNTS: [265, 128, 64, 128, 265]
Density-NNBNTS: [265, 512, 512, 265, 128, 50]
Coverage-Novelty-NNBNTS: [265, 512, 265, 128, 64, 1]

Each element from the left of the vector to the right represents
the number of neurons in each layer starting from the input layer.
For instance, the Autoencoder-NNBNTS vector represents a NN
with five layers and the input and output layers respectively have
265 and 265 neurons. The first hidden layer has 128 neurons, with
another two hidden layers following it. The output space of Density-
NNBNTS consists of the neurons only corresponding to the most-hit
fifty coverage events. By reducing the number of neurons in the
output space, the number of neurons in the hidden layers can corre-
spondingly be reduced. Thus, the expense of calculating the novelty
score for the Density NNBNTS can also be reduced. However, the
option of the most-hit fifty coverage events is not the result of fine-
tuning for the optimum performance of the NNBNTS. The N_K in
Algorithm 1 is set to 15 and only the last three hidden layers are
included in the calculation of the novelty score. The rationale to
chose these two parameters is to reduce the training expense. They
are also not the result of fine-tuning.

4.2 Experimental Results
All the NNBNTS are initialized with 500 simulated tests before
the experiment. Afterwards, the 1000 most novel tests are selected
to simulate in each selection round. Retraining occurs after the
simulation of the selected tests and all the simulated tests and the
associated coverage information (if necessary) form the retraining
set. Table 1 shows the performance of random test simulation (RD)
compared to Density (DS), Coverage-Novelty (CN) and Autoencoder
(AE) NNBNTS. The first column represents the five functional cov-
erage goals, with the top part showing the number of tests required
to reach the respective coverage goal using a certain test selection
technique and the bottom part showing the corresponding savings
compared to random simulation. In terms of achieving coverage
goals from 95% to 99.5%, NNBNTS with all the configured prediction
spaces can noticeably accelerate the closure of functional coverage
compared to random simulation used in traditional CRV. In this

Conferenceâ€™17, July 2017, Washington, DC, USA

Xuan Zheng, Kerstin Eder, and Tim Blackmore

Table 1: Result of Using NNBNTS to improve Verification Ef-
ficiency

Coverage Goals
95%
97%
99%
99.5%
99.95%

RD
17888
25504
48641
56590
82665

DS
6262
12703
36246
49176
75318

CN
5242
8579
22503
32969
63991

AE
4867
9153
27599
38304
62050

Savings (vs. Random)
95%
97%
99%
99.5%
99.95%

DS

AE

CN
64.99% 70.70% 72.80%
50.19% 66.36% 64.11%
25.48% 53.74% 43.26%
13.10% 41.74% 32.31%
22.59% 24.94%
8.89%

experiment, the 99% coverage goal is the point where the correla-
tion between the test features and the remaining coverage holes
becomes extremely complicated. At this point, directed tests crafted
by experienced verification engineers are also simulated along the
use of NNBNTS to target the remaining coverage holes (around
80 coverage events for our example), though the latter approach is
still obviously better than the random simulation.

Among the results of NNBNTS, the performance of Autoencoder
and Coverage-Novelty NNBNTS remains relatively stable through-
out the verification. As shown in Table 1, in terms of achieving the
99.95% coverage goal, Autoencoder NNBNTS can save 24.94% of sim-
ulated tests relative to random simulation while Coverage-Novelty
NNBNTS can lower the number of simulated tests by 22.59%. Al-
though the general performance of Density NNBNTS is not as
high as the other two NNBNTS, it is still better than the random
simulation. The computational expense for simulating a test in a
large-scale design is very high. Specifically, the average simulation
time for each test is 2 hours in the experiment. Thus, even a minor
saving in the number of simulated tests can significantly reduce
the simulation time after considering the extra costs brought by
the computation time of NNBNTS and the feature engineering.

A more detailed comparison between the performance of the
test selection techniques is shown in Figure 4, which depicts the
cumulative coverage along with the progress of test simulation.
NNBNTS can select more efficient tests during the verification
process and thus continuously achieve similar coverage with far
lower simulation effort. This also proves that the reduction in the
number of simulated tests required to achieve the five coverage
goals in Table 1 is not coincidental.

5 CONCLUSION
This paper proposes a novel test selector based on NN to achieve
faster closure of functional coverage than the random test simu-
lation in the simulation-based verification environment. We have
considered three configurations of the test selector. The first con-
figuration is based on Autoencoder while the second configuration
utilizes the novelty in the output space of hidden neurons to reflect
the novelty in the input space. The third configuration calculates
the novelty scores for the input tests in the coverage space of the
simulated tests. However, the configuration of the output space is

Figure 4: Result of Using NNBNTS to improve Verification
Efficiency - Line Chart

not limited to the three modes illustrated in this paper. In the ex-
periments, we demonstrate that the test selector can select the tests
bringing more cumulative coverage than the random simulation in
the verification environment for the commercial SPU.

The input features of the NNBNTS in the experiments are de-
rived from the static configuration of the SPU. For a future design,
including the transactions dynamically driven to the design in the
input feature space is our next step. By doing so, we hope the
NNBNTS can capture the temporal logic between the transactions.
Furthermore, filtering irrelevant features for NNBNTS is another
un-researched and challenging area.

REFERENCES
[1] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM

computing surveys (CSUR), 41(3):1â€“58, 2009.

[2] P.-H. Chang, D. Drmanac, and L.-C. Wang. Online selection of effective func-
tional test programs based on novelty detection. In 2010 IEEE/ACM International
Conference on Computer-Aided Design (ICCAD), pages 762â€“769. IEEE, 2010.
[3] W. Chen, N. Sumikawa, L.-C. Wang, J. Bhadra, X. Feng, and M. S. Abadir. Novel
test detection to improve simulation efficiencyâ€”a commercial experiment. In
2012 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),
pages 101â€“108. IEEE, 2012.

[4] F. Chollet et al. Keras. https://keras.io, 2015.
[5] K. Eder, P. Flach, and H.-W. Hsueh. Towards automating simulation-based design
verification using ilp. In S. Muggleton, R. Otero, and A. Tamaddoni-Nezhad,
editors, Inductive Logic Programming, pages 154â€“168, Berlin, Heidelberg, 2007.
Springer Berlin Heidelberg.

[6] S. Gogri, J. Hu, A. Tyagi, M. Quinn, S. Ramachandran, F. Batool, and A. Jagadeesh.
Machine learning-guided stimulus generation for functional verification.
In
Design and Verification conference 2020, virtual conference, 2020.

[7] O. Guzey, L.-C. Wang, J. Levitt, and H. Foster. Functional test selection based on
unsupervised support vector analysis. In 2008 45th ACM/IEEE Design Automation
Conference, pages 262â€“267. IEEE, 2008.

[8] D. Hond, H. Asgari, and D. Jeffery. Verifying artificial neural network classifier
performance using dataset dissimilarity measures. In 2020 19th IEEE International
Conference on Machine Learning and Applications (ICMLA), pages 115â€“121, 2020.
[9] C. Ioannides and K. I. Eder. Coverage-directed test generation automated by
machine learningâ€“a review. ACM Transactions on Design Automation of Electronic
Systems (TODAES), 17(1):1â€“21, 2012.

[10] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Courna-
peau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research, 12:2825â€“2830, 2011.

[11] R. H. T. Blackmore and S. Schaal. Proceedings of the 2021 design and verification

conference (virtual), 2021.

[12] J. Xu, Z. Li, B. Du, M. Zhang, and J. Liu. Reluplex made more practical: Leaky
relu. In 2020 IEEE Symposium on Computers and Communications (ISCC), pages

Using Neural Networks for Novelty-based Test Selection to Accelerate Functional Coverage Closure

Conferenceâ€™17, July 2017, Washington, DC, USA

1â€“7. IEEE, 2020.

