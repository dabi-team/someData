Towards making the most of NLP-based device
mapping optimization for OpenCL kernels

Petros Vavaroutsos, Ioannis Oroutzoglou, Dimosthenis Masouros, Dimitrios Soudris
School of Electrical and Computer Engineering, National Technical University of Athens, Greece
Emails: {petrosvav, ioroutzoglou, demo.masouros, dsoudris}@microlab.ntua.gr

2
2
0
2

g
u
A
0
3

]

G
L
.
s
c
[

1
v
4
2
1
4
1
.
8
0
2
2
:
v
i
X
r
a

Abstract—Nowadays, we are living in an era of extreme device
heterogeneity. Despite the high variety of conventional CPU
architectures, accelerator devices, such as GPUs and FPGAs, also
appear in the foreground exploding the pool of available solutions
to execute applications. However, choosing the appropriate device
per application needs is an extremely challenging task due
to the abstract relationship between hardware and software.
Automatic optimization algorithms that are accurate are required
to cope with the complexity and variety of current hardware and
software. Optimal execution has always relied on time-consuming
trial and error approaches. Machine learning (ML) and Natural
Language Processing (NLP) has ﬂourished over the last decade
with research focusing on deep architectures. In this context, the
use of natural language processing techniques to source code in
order to conduct autotuning tasks is an emerging ﬁeld of study.
In this paper, we extend the work of Cummins et al., namely
Deeptune, that tackles the problem of optimal device selection
(CPU or GPU) for accelerated OpenCL kernels. We identify three
major limitations of Deeptune and, based on these, we propose
four different DNN models that provide enhanced contextual
information of source codes. Experimental results show that our
proposed methodology surpasses that of Cummins et al. work,
providing up to 4% improvement in prediction accuracy.

Index Terms—Machine Learning, Natural Language Process-

ing, OpenCL, Heterogeneous Systems

I. INTRODUCTION

The ever-increasing amount of data generated and shared
by enterprises, industrial and non-proﬁt sectors, and scientiﬁc
research has resulted in an unprecedented increase in the size
and volume of data-intensive jobs [2]. In order to meet the new
computational demands of the big data, both hardware and
software undergo major changes. Additionally, new latency
and power constraints require approaches to solve number
of different application and compiler optimization problems.
However, their large design decision space to explore, makes
tuning applications an even more difﬁcult and time consuming
procedure, making the development of tuning heuristics more
urgent than ever. Furthermore, contemporary compilers and
runtime environments, featuring already hand-coded heuris-
tics, performing this decision making, make their program’s
performances contingent upon their quality.

Therefore, in order to make heuristic construction more
efﬁcient and inexpensive, automation tools have to be imported
in this procedure. In this regard, machine learning techniques
have been deployed in order to automate the process of

978-1-6654-8356-8/22/$31.00 ©2022 IEEE

selecting the best optimizations [4]. Prediction models using
different classes of machine learning are trained through
applications’ representative features in order to correlate them
with their optimal versions. Features can be any important
quantiﬁable properties of applications, static or dynamic, and
the choice of the most appropriate features constitutes an extra
optimization problem for the designers as well.

Even though machine learning has a proven contribution in
automated tuning heuristics [10], [14], its success is contingent
on the quality of the extracted features, which is frequently
achieved through a combination of domain expertise and trial
and error. In order to avoid the extraction of non appropriate
tuning models, humans are
features and ﬁnally inefﬁcient
needed to be removed from the loop. Latest publications [5],
[8] focus their work on characterizing and tuning applications
without using any code feature. Natural Language Processing
(NLP) methods are deployed to extract automatically and
internally code’s text features and feed them to the predic-
tive model. Therefore, NLP models are now able to extract
feature representations from source codes automatically and
afterwards, other learning systems could employ these learnt
feature representations as inputs to predict various down
stream tasks.

Among the dozens of optimizations, lately, most of them
are focused on the accelerators devices (such as GPUs and
FPGAs) deployed to satisfy the even more demanding per-
formance and power constraints from the edge to cloud con-
tinuum. One of the most effective and popular one, concerns
the optimal heterogeneous device mapping optimization for
OpenCL written applications. More speciﬁcally, this optimiza-
tions refers to the selection of either CPU or GPU device for
the most efﬁcient execution of OpenCL kernels in terms of
performance.

In this work, we extend DeepTune [8], one of the most inﬂu-
ential works on machine-learning based auto-tuning method-
ologies without any hand engineered feature extractors, at-
tempting to solve the heterogeneous device mapping optimiza-
tion. We achieve to further improve its effectiveness and ﬁnally
provide more efﬁcient auto-tuning methods without any need
for feature engineering. Our proposed work outperforms Deep-
Tune by providing up to 4.12%, with average 2.65% higher
prediction accuracy for the optimal device mapping selection.
The rest of paper is structured as follows: Section II introduces
the related work already published concerning the automation
of applications tuning methods. Section III describes the

 
 
 
 
 
 
overview of DeepTune’s baseline implementation while in
Section IV we present our proposed improved methodology.
The experiments and the ﬁndings are described in Section V
and ﬁnally, in Section VI we draw our conclusions.

II. RELATED WORK

There have been numerous works conducting with the goal
of optimizing source code using machine learning models in
order to automate tuning procedure [3]. Back in the early 2008,
Agakov et al. [1] were the ﬁrst to use a machine learning based
predictive model to speed up iterative optimization while, in
the same year, Cavazos et al. [6] applied logistic regression
models to automatically select the best optimization for user’s
applications. Later, Liu et al. [13] turning to accelerators, used
regression trees to optimize CUDA kernels, while Cummins
et al. [7] applied classiﬁers to select the optimal workgroup
size of OpenCL compute kernels. Finally, Magni et al. [14]
were the ﬁrst to address the coarsening optimizations through
Neural Networks cascade models.

Even though the above works managed to provide sufﬁcient
results based on hand crafted program features by developers,
heuristics needed to take humans out of the loop. Building
auto-tuning models without feature engineering can provide
faster, cheaper and more independent heuristics achieving to
discover more optimal tuning solutions without any human
guidance. Cummins et al. [8] where the ﬁrst to introduce their
work in that direction, with deep neural networks to replace
any hand-picked or even compiler IR based automatically
extracted features. Surprisingly, their approach matched or
surpassed the predictive models using hand-crafted features,
proving that deep learning can select more representative
and sufﬁcient features than even experts engineers. In the
same direction, Ben-Nun et al.
[5] managed to apply NLP
techniques on the Intermediate Representations (IR) of ap-
plications in order to take the feature extraction out of the
loop to support a wider range of programming languages.
Their experimental results were encouraging, disclosing that
the hand-coded features, are not, but an obstacle in the building
of the auto-tuning heuristics.

III. DEEPTUNE OVERVIEW

In this paper, we base our research in the work of [8].
Cummins et. al present an end to end architecture that ac-
cepts OpenCL kernels as input and decides on the optimal
device that those kernels should execute, CPU or GPU. Their
proposed methodology consists of two phases: i) a kernel
pre-processing stage, that transforms the OpenCL kernels to
a machine-learning friendly interprentetation and ii) a DNN-
based feature extraction phase, that receives the transformed
code and based on its features identiﬁes the more performance
efﬁcient device mapping for the respective kernel. Figure 1
shows an overview of the kernel transformation ﬂow, as well as
the basic layers, used as building blocks for feature extraction
used in [8]. Next, we brieﬂy describe the basic steps of the
work of Cummins et. al, henceforth referred to as Deeptune.

A. Source Rewriter

The ﬁrst stage of Deeptune consists of a source rewriter 1 .
Its purpose is to reconstruct a given OpenCL kernel to a reﬁned
version that eliminates semantically irrelevant
information.
This reﬁned version can then be more easily processed in an
automated manner. Speciﬁcally, this component accepts hand
written code and it performs the following three actions i)
removes comments and erase unnecessary spacing ii) rewrites
function names using an increasing alphabetical order of
capital letters [A-Z] and iii) rewrites the codes variable using
an increasing alphabetical order of lowercase letters [a-z].

B. Vocabulary Composition

The second step of Deeptune’s methodology is the vocab-
ulary composition 2 . This step concerns the construction
of a vocabulary that maps code related deﬁnitions (e.g.,
__kernel, int) and punctuation marks (e.g., parentheses,
semicolons) to a set of integer representations. This vocabulary
is mandatory, since machine-learning models receive as input
numeric values and are not able to process source code
directly. To build the corpus vocabulary, this step considers
all the possible words and symbols appeared over all the ex-
amined OpenCL kernels and performs word level tokenization,
which maps each token to a unique integer identiﬁer, called
token id.

C. Sequence Encoder

The last step of the pre-processing phase is the sequence
encoder 3 . This component combines the processed OpenCL
kernel (output of step 1 ) with the corpus vocabulary (output
of step 2 ) and provides an integer sequence that corresponds
to the respective input code.

D. DNN-based feature extraction

As mentioned before, Deeptune utilizes a DNN-based ap-
proach to automate the process of feature extraction, thus
eliminating completely the need for hand-crafted solutions.
The proposed DNN architecture consists of three major layers,
that form the building blocks of their model ( 4 –top). First,
an Embedding layer acts as a language model that receives
as input the tokenized source code sequence and converts
them to embedding vectors of dimension D = 64. Next, the
embedded vector is passed to a block of two Long Short-
Term Memory [12] layers, that is responsible for exposing
information on the input vector. Last,
one-way sequential
DeepTune’s model ﬁnal component
is a non-linear block,
that is responsible for exposing non-linearities in the data.
It consists of two fully connected artiﬁcial neural network
layers, where the initial layer is composed of 32 neurons. Each
possible heuristic decision is represented by a single neuron in
the second layer. The model’s conﬁdence that the associated
choice is right is represented by the activation of each neuron
in the output layer. Taking the output layer’s argmax yields
the decision with the highest activation.

Fig. 1: High-level overview of the end-to-end optimal device mapping methodology for OpenCL kernels. The kernel-
preprocessing phase transforms source codes to machine learning interpretable format and the feature extraction, based on
the optimization problem, it generates highly descriptive characteristics for each kernel. 4 -top shows the feature extraction
approach of Deeptune [8] and 4 -bottom our proposed building blocks.

E. Deeptune’s Limitations & Motivation

While the work of [8] is revolutionary in automating the fea-
ture extraction process of GPU code, we argue that it neglects
important aspects and inherent characteristics of the structure
and nature of modern programming languages, which,
if
approached properly, could provide useful insights. Moreover,
novel advancements in the domain of machine learning and
NLP [5], [9], [18], can provide more representative and/or
diverse features, unveiling additional hidden patterns in the
underlying data. Speciﬁcally, we pinpoint
three important
remarks that Deeptune disregards and which also form the
motivation of our work:

R1) Relation between adjacent tokens: The structure
of a programming language is very well deﬁned, with syn-
tactical rules. The syntax of a programming language is a
collection of rules deﬁning the combinations of symbols that
constitute appropriately organized statements or expressions
in that language. Deeptune neglects that structure, by treating
tokens sequentially, thus it makes no consideration for the
premise that adjacent tokens provide additional context for
code comprehension. In the example below, the model will
assign a high probability to the right token, which is int,
based on the structure of the variable’s a declaration.

1 int main(){
2

int a = 10;
___ b = a + 1;

3
4 }

Listing 1: Example code that highlights the dependence of
adjacent tokens

R2) Preserve past and future information: While Deep-
tune examines hidden relationships in the sequential data
through the LSTM layers, it only acquires knowledge exclu-
sively from the input’s subsequent pass, since unidirectional
LSTMs only preserve inputs that has already passed through it
using the hidden state. However, the typical ﬂow of imperative
programming reveals bidirectional inter-dependencies, from
the beginning of the code to its end and vice-versa. A typical
example is the following:

1 float sqrt(int a);
2 int main(){
3

int x = sqrt(2);
// several lines of code

4
5 }
6 float sqrt(int a){
7
8 }

// implementation

Listing 2: Example code that depicts future dependencies

where the call of a function is several lines afterwards than
it’s implementation.

R3) Signiﬁcance-aware feature processing: Last, Deep-
tune treats all the input features equally, even if part of it
is less signiﬁcant. However, it is apparent that not all source
code semantics provide proportionate contextual information.
For example, a variable declaration (e.g., int) is not as critical
as the deﬁnition of a loop (e.g., for). Therefore, it is of great
importance to be able to distinguish more valuable features
from the less signiﬁcant ones.

IV. PROPOSED METHODOLOGY

This section describes the core concepts of our methodology
for optimal device mapping of OpenCL kernels between CPU
and GPU devices based on NLP techniques. The rationale
behind the proposed processing ﬂow is driven by the remarks
(R1-R3) drawn in Section III-E. Speciﬁcally, similar to [8], our
methodology consists of two phases, the kernel pre-processing
and the feature extraction, as depicted in Fig. 1. However,
it extends the latter to account for additional contextual
information on the examined kernels.

A. Kernel pre-processing

The kernel pre-processing phase of our methodology is
to the one proposed in Deeptune. We replicate
identical
the pre-processing components described in Sections III-A
to III-C, which correspond to steps 1 - 3 of Fig. 1.

B. Building blocks for feature extraction

We give special emphasis on the building blocks used to
design the DNN model that performs the device mapping
classiﬁcation task ( 4 ). Compared to Deeptune, we incorporate

__kernel void matrixMul(__global float* C,  __global float* A,  __global float* B,  int wA, int wB){    int tx = get_global_id(0);    int ty = get_global_id(1);    // value stores the element that is    // computed by the thread    float value = 0;    for (int k = 0; k < wA; ++k)    {        float elementA = A[ty * wA + k];        float elementB = B[k * wB + tx];        value += elementA * elementB;    }    // Write the matrix to device memory     // each thread writes one element    C[ty * wA + tx] = value;}Initial OpenCL Kernel__kernel void A(__global float* a,   __global float* b,   __global float* c,   int d,   int e){   int f = get_global_id(0);     int g = get_global_id(1);     float h = 0;     for (int i = 0; i < d; ++i)     {       float j = b[g * d + i];           float k = c[i * e + f];           h += j * k;   }   a[g * d + f] = h;}Processed OpenCL Kernel'(': 0')': 1'*': 2'+': 3',': 4'0': 5';': 6'<': 7'=': 8'A': 9'[': 10']': 11 Vocabulary'__global': 12 '__kernel': 13'a': 14'b': 15'c': 16'd': 17'e': 18'f': 19' oat': 20' oat*': 21for': 22'g': 23'get_global_id(0)':24 'get_global_id(1)':25'h': 26'i': 27'int': 28'j': 29'k': 30'void': 31'{': 32'}': 3313 31 9 0 12 21 14 4 12 21 15 4 1221 16 4 28 17 4 28 18 1 32 28 19 824 6 28 23 8 25 6 20 26 8 5 6 22 028 27 8 5 6 27 7 17 6 3 3 27 1 3220 29 8 15 10 23 2 17 3 27 11 6 2030 8 16 10 27 2 18 3 19 11 6 26 3 829 2 30 6 33 14 10 23 2 17 3 19 118 26 6 33 Source code to encoded SequenceVocabulary  Composition Map every possible word / symbol from a set of kernels to an integer valueSource RewriterRemove comments Rename functions Rename variables 12Sequence EncoderTranslate processed OpenCLkernel using the vocabulary 3OpenCL KernelsSource codesKernel pre-processingBuilding blocks of DNN-based Feature extraction Embedding Convertencoding toembeddedvectorsLSTMOnedirection  ow featureextractionNon-LinearBlock Expose nonlinearcorrelationsDeeptune [8]Embedding Convertencoding toembeddedvectorsCNN Combine &Extractinformation fromadjacent tokensBi-LSTM Twodirections ow featureextractionAttention Highlight the importanceof di erent featuresNon-Linear Block Expose non linearcorrelationsProposed4(a) Deeptune [8]

(b) Deeptune-CNN

(c) Deeptune-BiLSTM

(d) Deeptune-Attention

(e) Hybrid Architecture

Fig. 2: Examined DNN architectures

three additional processing layers, that aim to tackle Deep-
tune’s limitations presented above.

First, we extend DeepTune by adding a CNN layer, that re-
ceives as input the output of the embedding layer. This method
takes advantage of the inherent structure of a textual label
by identifying common attributes shared by multiple words
and dividing them into three equal parts called trigrams [15].
Trigrams, we believe, are an effective representation of pro-
gramming syntax structure and helps mitigating the gap of
remark (R1). It is composed of 2 layers: one 1-d convolutional
layer, with 64 ﬁlters, kernel size of 3 and striding step 1,
and one max pooling layer of size 4. The embedding layer
transmits the words to the convolutional layers in the form of
sentences (in our case code instructions). Convolution layer
convolve the input using pooling layers; pooling layers aid in
reducing the representation of input phrases, input parameters,
computation, and overﬁtting in the network.

For preserving both past and future information on the
input data (remark R2), we employ bidirectional LSTM layers.
Speciﬁcally, Bi-LSTMs are able to obtain knowledge about
the sequence in both directions, backwards (future to past)
and forwards (past to future) and recognize long-term depen-
dencies [16]. The core idea, is to split the state neurons of
a regular LSTM in a part that is responsible for the positive
time direction (forward states) and a part for the negative time
direction (backward states). Outputs from forward states are
not connected to inputs of backward states, and vice versa.
Then the hidden states of the two LSTMs are combined to
ﬁnd the hidden state for each time point. It is proven to be
more accurate than the traditional LSTM networks [17], but
in trade off signiﬁcant training time. In our case, bidirectional
keeps information from both directions, making it easier for
the network to understand long-term code dependencies, such
as function declarations.

ﬁeld of NLP. namely Attention layers [18]. The attention
block highlights the importance of different features that are
highly correlated with classiﬁcation, by assigning weights to
features, extracting the contextual information. Attention can
be proven to be useful in the case of programming code
modeling, emphasizing more critical points, such as branches,
loops etc., against less important, such as variable declarations,
increments etc.

C. Examined DNN architectures

To understand the impact of each additional layer on the
overall accuracy of the device mapping problem, we design
different DNN architectures with diverse combinations of the
aforementioned building blocks, as shown in Fig. 2. These
architectures extend the baseline model of Deeptune (Fig. 2a)
in the following directions:

thus,

• Deeptune-CNN: Includes a CNN layer right after the
Embedding,
introducing the aspect of trigrams
(Fig. 2b). This model examines the impact of feature
extraction from adjacent tokens in the source code (R1).
• Deeptune-BiLSTM: We replace the two unidirectional
LSTM layers of Deeptune, with a bidirectional one
(Fig. 2c). Through this model, we explore the effect of
preserving past and future information in the code (R2).
• Deeptune-Attention: This architecture introduces an At-
tention layer right after the LSTM, which identiﬁes
important features in the input. With this model, we
explore whether a signiﬁcance analysis on the features
of the input affect the efﬁciency of the prediction (R3).
• Hybrid Architecture: Final, the hybrid architecture com-
bines all the aforementioned techniques and examines
their aggregated impact on the overall accuracy of the
model.

V. EVALUATION

Last, we tackle the problem of signiﬁcance-aware feature
processing (remark R3) by exploiting a novel solution in the

We evaluate our proposed methodology and compare it
directly with Deeptune by examining the accuracy of each

LSTMInputEmbeddingLSTMBatchNormNon-LinearNon-LinearPrediction(1024,0)(1024,64)(1024,64)(2,)(1024,0)(1024,64)(255,64)(2,)LSTMInputEmbeddingLSTMBatchNormNon-LinearNon-LinearPredictionCNNMaxPool (4)(1024,64)(255,64)(1024,0)(1024,64)(1024,128)(2,)InputEmbeddingBi-LSTMBatchNormNon-LinearNon-LinearPrediction(1024,0)(1024,64)(1024,64)BatchNormNon-LinearNon-LinearPrediction(2,)AttentionLSTMInputEmbeddingLSTM(1024,128)(1024,0)(1024,64)(255,128)(2,)Bi-LSTMInputEmbeddingBatchNormNon-LinearNon-LinearPredictionCNNMaxPool (4)(1024,64)(255,64)Attention(255,128)Platform
Intel Core i7-3820
AMD Tahiti 7970
NVIDIA GTX 970

Frequency Memory
3.6 GHz
1000 MHz
1050 MHz

8 GB
3 GB
4 GB

Driver
AMD 1526.3
AMD 1526.3
NVIDIA 361.42

TABLE I: Platform Details

one of the DNN architectures presented in Section IV-C.

A. Examined Dataset

We base our evaluation on the dataset of [8] in order
to have accurate comparison between the different model
architectures. The dataset contains preprocessed and tokenized
OpenCL kernels, from 7 different benchmark suites. Addition-
ally, it contains the execution times for each kernel on two
GPU devices, the AMD Tahiti 7970 and the NVIDIA GTX
970, as well as on an Intel Core i7-3820 CPU. Table I contains
details about the CPU-GPU platforms.

The prediction target is the platform in which the execution
time is lower. More precisely, when we examine the AMD
GPU and Intel CPU cases, the target is [1, 0] if the kernel runs
faster on the GPU and [0, 1] if the kernel runs faster on the
CPU. This is also referred to as one-hot encoding. Likewise,
for the NVIDIA GPU and the Intel CPU case.

B. Experimental Setup

We use stratiﬁed 10-fold cross-validation to evaluate the
predictive quality of each model. Each program is randomly
assigned to one of ten equal-sized sets; the sets are balanced
to ensure a consistent proportion of samples from each class
across the whole set. A model is trained on all but one of
the sets’ programs and then tested on the programs from the
unseen set. This procedure is done for each of the ten sets
in order to provide a comprehensive prediction for the entire
dataset.

All models were implemented in Python using Tensorﬂow1
and Keras2 backends. To ensure the most accurate comparison,
we seed our layers with the same number, in order to be
initialized with the same weights. The maximum sequence
length was set to 1024 and the learning rate at 10−3. We used
categorical cross entropy as loss function, a batch size of 64
and trained for 50 epochs. As optimizer, we used the adaptive
learning rate optimization algorithm, Adam. The experiments
were carried out on an NVIDIA Tesla V100 GPU. Finally, we
used TensorBoard3 to measure and visualize parameters like
loss and accuracy.

C. Experimental Results

The average accuracy of each model, as measured in a 10-
fold test set, is shown in Table II, where the best results are
printed in bold font.

Baseline architecture: The baseline architecture is Deep-
tune, the model suggested in [8]. For the shake of fair com-
parison, we have retrained the model using the steps of Sec.

1Tensorﬂow: https://www.tensorﬂow.org/
2Keras: https://keras.io/
3TensorBoard: https://www.tensorﬂow.org/tensorboard/

Fig. 3: Train and validation loss of the Hybrid Architecture
model.

Fig. 4: Train and validation accuracy of the Hybrid Architec-
ture model.

V-B, rather than hard copying the results from [8]. We observe
that Deeptune achieves an average accuracy of 81.76%, with
better results on the AMD platform with 83.23%, while in the
NVIDIA device it scored 80.29%.

Impact of Trigrams (R1): We observe that the notion of
trigrams helps the model’s code comprehension capabilities.
Speciﬁcally, the Deeptune-CNN model already outperforms
the baseline by 2.65% on AMD platform and 1.18% on
average, but performs slightly worse on the NVIDIA plat-
form (0.28% accuracy drop), which, however, is statistically
insigniﬁcant. We also observe that the introduction of the CNN
layer provides the greatest accuracy increment in the case of
AMD GPU compared to that of Bi-LSTM and Attention.

Impact of preserving past and future information (R2):
The Deeptune-BiLSTM model, performs better in both devices
compared to Deeptune. More speciﬁcally the model scored
84.85% on the AMD device and 80.88% on the NVIDIA,
resulting to an average precision of 82.87%, that is 1.11%
improvement
than the baseline. Moreover, we notice that
similar to the case of trigrams, the accuracy increment is

01020304050Epochs0.20.30.40.50.60.70.80.9Losstrain_lossval_loss01020304050Epochs0.50.60.70.80.9Accuracytrain_accuracyval_accuracyDeepTune [8]
DeepTune-CNN
DeepTune-bilstm
DeepTune-Attention
CNN-BiLSTM-Attention

AMD Tahiti 7970
83.23%
85.88%
84.85%
83.91%
87.35%

NVIDIA GTX 970
80.29%
80.01%
80.88%
81.03%
81.47%

Average
81.76%
82.94%
82.87%
82.50%
84.41%

TABLE II: Accuracy per examined DNN architecture (as described in Sec. IV-C)

lower for the NVIDIA case.

Impact of feature signiﬁcance (R3): The addition of an
attention layer in DeepTune-Attention model, also outperforms
the baseline in both devices, scoring 83.91% on the AMD
platform and 81.03% on the NVIDIA, leading to 82.50% on
average. It appears to give the best results, so far, on the
NVIDIA platform, that is 0.73% higher than the baseline.

Hybrid Architecture: Last, our proposed hybrid model,
CNN-BiLSTM-Attention, outperforms the baseline model and
all other models. This reveals that by combining all of our
proposed feature processing steps, we can obtain much greater
contextual information from the processed kernel and, thus,
maximize the accuracy of our model. Speciﬁcally, the hybrid
architecture scored 87.35% on the AMD platform and 81.47%
on the NVIDIA device, that is 4.12% and 1.18% higher than
the baseline, respectively, resulting in an average accuracy
increase of 2.65%, that is 84.41%.

For the shake of completeness, we also report the loss
and accuracy curves over the training phase, for the Hybrid
Architecture. Figures 3 and 4 show the respective results,
where each curve corresponds to the average loss and accuracy
over the 10-fold cross train and validation sets. As can be
seen, the validation loss decreases until epoch 40, at which
point it remains pretty constant. Because the training loss
continues to decrease, we stop training until epoch 50 to avoid
overﬁtting [11].

VI. CONCLUSION

In the era of extreme device heterogeneity, choosing the
most appropriate device for application execution forms a
really challenging problem. In this paper, we build upon
the work of Cummins et al. [8], and pinpoint some of its
limitations. Then, we examine the impact of additional feature
extraction approaches for improving the accuracy of optimal
device mapping prediction for OpenCL kernels. On average,
our suggested model, namely CNN-BiLSTM-Attention, out-
performs the baseline model, surpassing it on both prediction
challenges. We achieve 4.12% higher prediction accuracy than
the baseline on the AMD platform and 1.18% higher on the
NVIDIA platform.

In future work, we will expand our method to source
code modeling by using transformers to do unsupervised
training on unlabeled C code in order to enhance programming
language understanding [9]; continue our investigation into the
optimization of CUDA kernels; and train models to aid in the
development of energy-efﬁcient embedded devices.

ACKNOWLEDGMENT

This work is partially funded by the EU Horizon 2020
research and innovation programme, under project EVOLVE,
grant agreement No 825061.

REFERENCES

[1] F. Agakov, E. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. F. O’Boyle,
J. Thomson, M. Toussaint, and C. K. Williams, “Using machine learning
to focus iterative optimization,” in International Symposium on Code
Generation and Optimization (CGO’06).

IEEE, 2006, pp. 11–pp.

[2] R. Agarwal and V. Dhar, “Big data, data science, and analytics: The

opportunity and challenge for is research,” pp. 443–448, 2014.

[3] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton, “A survey
of machine learning for big code and naturalness,” ACM Computing
Surveys (CSUR), vol. 51, no. 4, pp. 1–37, 2018.

[4] A. H. Ashouri, W. Killian, J. Cavazos, G. Palermo, and C. Silvano,
“A survey on compiler autotuning using machine learning,” ACM
Computing Surveys (CSUR), vol. 51, no. 5, pp. 1–42, 2018.

[5] T. Ben-Nun, A. S. Jakobovits, and T. Hoeﬂer, “Neural code comprehen-
sion: A learnable representation of code semantics,” Advances in Neural
Information Processing Systems, vol. 31, 2018.

[6] J. Cavazos and M. F. O’boyle, “Method-speciﬁc dynamic compilation
using logistic regression,” ACM SIGPLAN Notices, vol. 41, no. 10, pp.
229–240, 2006.

[7] C. Cummins, P. Petoumenos, M. Steuwer, and H. Leather, “Auto-
tuning opencl workgroup size for stencil patterns,” arXiv preprint
arXiv:1511.02490, 2015.

[8] C. Cummins, P. Petoumenos, Z. Wang, and H. Leather, “End-to-end deep
learning of optimization heuristics,” in 2017 26th International Con-
ference on Parallel Architectures and Compilation Techniques (PACT).
IEEE, 2017, pp. 219–232.

[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.

[10] D. Grewe, Z. Wang, and M. F. O’Boyle, “Portable mapping of data
parallel programs to opencl for heterogeneous systems,” in Proceedings
of the 2013 IEEE/ACM International Symposium on Code Generation
and Optimization (CGO).

IEEE, 2013, pp. 1–10.

[11] D. M. Hawkins, “The problem of overﬁtting,” Journal of chemical
information and computer sciences, vol. 44, no. 1, pp. 1–12, 2004.
[12] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[13] Y. Liu, E. Z. Zhang, and X. Shen, “A cross-input adaptive framework
for gpu program optimizations,” in 2009 IEEE International Symposium
on Parallel & Distributed Processing.

IEEE, 2009, pp. 1–10.

[14] A. Magni, C. Dubach, and M. O’Boyle, “Automatic optimization of
thread-coarsening for graphics processors,” in Proceedings of the 23rd
international conference on Parallel architectures and compilation,
2014, pp. 455–466.

[15] A. Poznanski and L. Wolf, “Cnn-n-gram for handwriting word recog-
nition,” in Proceedings of the IEEE conference on computer vision and
pattern recognition, 2016, pp. 2305–2314.

[16] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural net-
works,” IEEE transactions on Signal Processing, vol. 45, no. 11, pp.
2673–2681, 1997.

[17] S. Siami-Namini, N. Tavakoli, and A. S. Namin, “The performance of
lstm and bilstm in forecasting time series,” in 2019 IEEE International
Conference on Big Data (Big Data).

IEEE, 2019, pp. 3285–3292.

[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems, vol. 30, 2017.

