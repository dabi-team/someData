Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

MammoDL: Mammographic Breast Density Estimation using Federated 
Learning 
Ramya Muthukrishnan1, Angelina Heyler2, Keshava Katti2, Sarthak Pati3,4,5, Walter 
Mankowski3,5, Aprupa Alahari1, Michael Sanborn2, Emily F. Conant5, Pratik Chaudhari1, 
Despina Kontos3,5*, Spyridon Bakas3,4,5*.  

Affiliations: 

1Department of Computer and Information Science, University of Pennsylvania, 
Philadelphia, PA, USA.  
2Department of Electrical and Systems Engineering, University of Pennsylvania, 
Philadelphia, PA, USA. 
3Center for Biomedical Image Computing and Analytics (CBICA), University of 
Pennsylvania, Philadelphia, PA, USA.  
4Department of Pathology & Laboratory Medicine, Perelman School of Medicine, University 
of Pennsylvania, Philadelphia, PA, USA.  
5Department of Radiology, Perelman School of Medicine, University of Pennsylvania, 
Philadelphia, PA, USA. 

*Corresponding authors. Email: despina.kontos@pennmedicine.upenn.edu, 
sbakas@upenn.edu. 

1 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

Abstract:  Assessing  breast  cancer  risk  from  imaging  remains  a  subjective  process,  in  which 
radiologists  employ  simple  computer  aided  detection  (CAD)  systems  or  qualitative  visual 
assessment to estimate breast percent density (PD). Machine learning (ML) models have become 
the most promising way to quantify breast cancer risk for early, accurate, and equitable diagnoses, 
but training such models in medical research is often restricted to small, single-institution data. 
Since  patient  demographics  and  imaging  characteristics  may  vary  considerably  across  imaging 
sites,  models  trained  on  single-institution  data  tend  not  to  generalize  well.  In  response  to  this 
problem, MammoDL is proposed, an open-source software tool that leverages a U-Net architecture 
to accurately estimate breast PD and complexity from mammography. With the Open Federated 
Learning  (OpenFL)  library,  this  solution  enables  secure  training  on  datasets  across  multiple 
institutions. MammoDL is a leaner, more flexible model than its predecessors, boasting improved 
generalization due to federation-enabled training on larger, more representative datasets.  

One-Sentence Summary: MammoDL uses the U-Net deep learning architecture to quantitatively 
assess  breast  tissue  density  and  complexity  from  mammograms  and  enables  a  data  privacy 
approach by using federated learning.  

Keywords:  Breast  Cancer  Risk,  Mammography,  Breast  Density,  Deep  Learning,  Machine 
Learning, Federated Learning, OpenFL. 

2 

 
 
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

1.  Introduction       

Breast cancer remains the most frequent cancer among women, with about 1 in 8 women 
in  the United States developing breast cancer over the course of  her lifetime [1].  Furthermore, 
breast cancer is the most common cause of cancer-related death for women worldwide [2]. The 
goal  of  mammographic  screening  is  to  reduce  the  mortality  rate  of  breast  cancer.  As  such, 
mammography  has  become  the  most  standard  and  reliable  method  for  breast  screening  today. 
Randomized trials and incidence-based mortality studies show a notable decrease in breast cancer 
mortality due to participation in mammography screenings [3-5]. In particular, mammography is 
highly effective in identifying breast cancers before they become fatal [6]. While mammography 
may be widely considered as the gold standard of breast screening, it suffers from relatively poor 
sensitivities ranging from 75% to 85%, with the lowest sensitivity in detecting cancers in women 
with  the densest  breast  tissue [7].  Mammography screenings consist  of two-view (mediolateral 
oblique  (MLO)  and  craniocaudal  (CC))  bilateral  examinations  captured  as  full-field  digital 
mammography (FFDM) images. In the United States, a large portion of screenings are also with 
digital breast tomosynthesis (DBT), which capture a 3-dimensional image of the breast, but this 
technique still suffers from diminished sensitivity as breast density increases. Radiologists then 
perform a visual grading of breast density based on the American College of Radiology’s Breast 
Imaging Reporting and Data Systems (BI-RADS). Some radiologists employ simple computer-
aided detection (CAD) systems, which generally present limited improvements [8], especially in 
comparison to their machine learning (ML) counterparts. 

Breast  density not  only  limits  the sensitivity  of mammographic screenings but  is also a 
major risk factor for breast cancer [9]. Over 43% of women in the United States between the age 
of 40 to 74 have heterogeneously or extremely dense breasts [10]. The most frequent method to 
grade breast density is subjective, using the BI-RADS classification of breast density based on 
mammographic images [10-12]. This method does not provide a continuous, real-valued percent 
density (PD), which would help a radiologist better monitor changes in a patient’s breast density 
that point to heightened breast cancer risk, but rather classifies the breast into one of four density 
categories.   

Current  efforts  to  automate  quantitative  breast  density  estimates  from  mammographic 
images  come  in  the  form  of  commercially  available  software  and  research  tools.  The  semi-
automated thresholding tool Cumulus remains the current gold-standard area-based breast density 
estimation  method  for  breast  cancer  screenings.  Commercial  software  for  volumetric  breast 
composition  measurement,  like  Quantra  [13]  and  Volpara  [14],  show  strong  association  with 
Cumulus [15] but with several key limitations. These tools determine breast density from x-ray 
beam interaction models, which make underlying assumptions on metadata to simplify estimates 
that  can  lead  to  inaccurate  results  [9],  especially  if  some  of  the  prerequisite  metadata  is  not 
available.  Such  commercial  software  may  be  expensive  and  may  suffers  from  limited 
interpretability, as the tools do not output a spatial map delineating the dense tissue from the non-
dense tissue in the mammogram.  Research tools [16-28] come with their own set of limitations. 
First, with the exception of LIBRA, such tools are not freely available, which not only decreases 
their  likelihood  of  adoption  but  also  the  ability  to  benchmark  their  performance  with  other 
available PD estimation methods. Second, these research tools have been trained on small, single-
institution datasets [9], a consequence of the data silo problem that plagues the medical imaging 
field. 

Upon  the  advent  of  deep  learning  (DL),  convolutional  neural  networks  (CNN)  have 
become  the  workhorse  for  fully  automated  mammographic  density  estimation  tools  [8].  Such 
algorithms rely on sufficiently large and diverse datasets for training, but these datasets  can be 
tremendously difficult to obtain in the medical field. Furthermore, single-institution datasets are 

3 

 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

not sufficient to provide a representative sample for model training and can lead to low-accuracy 
generalization  [29].  In  addition,  multi-institutional  collaborations  that  employ  centrally  shared 
patient data present numerous privacy and ownership concerns. The solution to these concerns is 
a novel paradigm for multi-site collaboration called federated learning [29], which allows model 
training to leverage data across multiple decentralized institutions without sharing data between 
institutions.  Instead,  the  training  process  is  distributed  to  each  of  the  data  owners  and  then 
aggregated into a single model, which not only addresses access rights and privacy concerns from 
multi-institutional  collaborations  but  vastly  increases  the  generalizability  of  DL-based  medical 
imaging models as compared to collaborative data sharing (CDS) [29].  

This work presents MammoDL, a breast PD estimation tool that builds upon Deep-LIBRA 
[9],  a  DL-based  improvement  to  the  Laboratory  for  Individualized  Breast  Radiodensity 
Assessment (LIBRA) tool [16]. MammoDL utilizes 2 deep learning models (U-Nets) to separately 
segment the breast and dense tissue regions from the mammogram in order to accurately estimate 
PD. In contrast to Deep-LIBRA, which uses separate U-Nets to remove the background and to 
remove the pectoralis muscle, MammoDL uses one U-Net to do both by directly segmenting the 
breast  in  the  mammogram.  Furthermore,  while  Deep-LIBRA  segments  the  dense  tissue  using 
traditional ML modeling, particularly a support vector machine (SVM), MammoDL replaces this 
step with deep learning by employing a U-Net to perform this segmentation. Thus, MammoDL is 
a  fully  DL-based  pipeline  for  PD  estimation  that  is  more  lightweight  and  flexible  than  Deep-
LIBRA.  Lastly,  MammoDL  leverages  federated  learning  capabilities  offered  by  the  Open 
Federated Learning (OpenFL) library [30] to increase the accuracy and generalizability of breast 
PD  estimation  with  respect  to  the  ground-truth  labels  produced  by  a  “gold-standard”  Cumulus 
reader. It should be noted that MammoDL is designed to be a hybrid model that combines an ML 
algorithm with a radiologist assessment, which is shown to be more effective than either of the 
two operating independently [31].  

2.  Materials and Methods 

MammoDL was conceived as the solution to three different problems facing the broader 
breast cancer risk assessment community. First, ML-based breast PD estimation tools tended to 
include numerous segmentation steps or classification/regression tasks, leading to overly complex 
models that would either generalize poorly or present significant computational costs that limited 
their usefulness. As a result, the goal was for MammoDL to be a lean ML model with comparable 
accuracy to its predecessor, Deep-LIBRA. Feature pyramids to detect objects at different scales 
[32]  have  already  penetrated  the  medical  imaging  space,  with  examples  like  multi-class 
segmentation  in  chest  radiographs  [33],  which  motivated  MammoDL  to  execute  a  multi-
segmentation task that offered better generalization to new mammogram images (Fig. 1b). Second, 
the  problem  of  small,  single-institution  training  datasets  was  addressed  by  incorporating  a 
federated  learning  pipeline  into  MammoDL,  a  measure  taken  to  further  enhance  model 
generalization  (Fig.  1a).  Finally,  we  have  made  MammoDL  freely  available  on  GitHub  for  all 
those interested in the code and models related to this work. 

2.1.   Study Datasets 

The dataset used for model training and validation (Table 1) consisted of negative FFDM 
screening  exams  obtained  from  the  Hospital  of  the  University  of  Pennsylvania  (HUP), 
Philadelphia, PA, and the Mayo Clinic (MC), Rochester, MN [9]. It included 6.713 bilateral CC-
view images from 1,679 women from the MC and 1,147 bilateral MLO-view images from 575 
women from HUP. The ground-truth PD labels were curated on Cumulus by a “gold-standard” (> 
20  years  of  experience  estimating  PD  with  Cumulus)  human  reader.  The  holdout  test  dataset 

4 

 
 
 
 
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

consisted of similar data from HUP and the MC; it consisted of 278 MLO-view images from 110 
women at HUP and 6463 bi-lateral CC and MLO images from 1628 women at the MC. The HUP 
data is racially diverse, which is an important dataset characteristic to accurately assess breast PD 
[34]. 

In preparation for the multi-segmentation task, all images were labeled with breast masks 
such  that  Class  0  corresponded  to  the  background  and  pectoralis  muscle,  while  Class  1 
corresponded to the breast. What remained was the portion of the mammogram containing only 
the  breast,  which  acted  as  the  input  for  the  second  segmentation  model,  in  which  Class  0 
corresponded to the non-dense tissue, while Class 1 corresponded to the dense tissue.  

2.2.   Data Pre-Processing 

Each original mammogram, stored as DICOM images, was pre-processed by removing the 
metal tag in the image, down-sampling the images to ensure a standardized shape across the cohort 
(i.e., 512x512), rescaling pixel intensities to [0,1] using min-max scaling, and removing the metal 
tag in the mammogram. These images are the inputs to the breast segmentation model during both 
training and inference (Fig. 3a). The inputs to the dense tissue segmentation model are further pre-
processed by removing non-breast pixels (determined by the manual breast mask during training 
and the predicted breast  mask during inference)  and re-normalizing pixel values with min-max 
scaling (Fig. 3c).  

2.3.   Model Architecture 

Due  to  the  wide  use  of  residual  connections  in  many  state-of-the-art  neural  network 
architectures  and their contributions  to  breakthroughs in  computer vision, the ResNet  [35]  was 
selected for this medical imaging task. ResNet architecture not only eases the challenges of training 
deep  neural  networks  (e.g.,  by  avoiding  the  vanishing  gradient  and  degradation  problems)  but 
boasts outstanding generalization to new data points [36]. The ResNet is employed as the encoder 
for a U-Net, [37] a widely used segmentation module that operates by first down-sampling and 
then up-sampling an image, creating the “U”-like shape for which it is named. The specific ResNet 
architecture  selected  was  the  ResNet34,  and  the  weights  were  obtained  from  pre-training  on 
ImageNet data [38]. Modifying the U-Net with a pre-trained ResNet34 encoder (Fig. 2) brings the 
aforementioned ResNet architecture advantages to the segmentation task plus an increase in the 
speed of training due to the ability to obtain pre-trained ResNet encoders [39]. MammoDL consists 
of two such modified U-Nets, the first for identifying the breast from the entire mammogram and 
the second for delineating the dense tissue region from the breast. 

During evaluation and inference, predicted segmentations from each U-Net are resampled 
to the original image size. The number of pixels in each predicted segmentation are calculated as 
a measure of area, with the area of the dense tissue region divided by the area of the breast region 
to return the breast PD (Fig. 3d). 

2.4.   Federated Learning 

The primary motivation for training the MammoDL model in a federated manner was to 
expand  the  size  and  diversity  of  the  medical  training  data  and  facilitate  multi-institutional 
collaborations without having to share the data among institutions [29, 38, 40-42]. In the field of 
medical imaging, the impact of federated learning to a ML-based model cannot be understated, 
with its addition increasing generalization accuracy by up to 35% [42]. In the aggregator-server 
federated learning framework [38], each participant contributing model updates using their own 
data is known as a collaborator, with a single aggegator node combining these model updates into 
a single consensus model. For the purpose of this work, collaborator nodes are hospitals that own 

5 

 
 
 
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

a dataset of FFDM images in which the dense breast tissue is labeled. The DL model is trained 
locally  at  each  collaborator  node  for  a  specified  number  of  epochs.  The  aggregator  node  is 
connected to the collaborator nodes by remote procedure calls (gRPC). OpenFL enables these calls 
to occur via a mutually authenticated transport layer security (TLS) network connection. Through 
this  secure  channel,  the  collaborator  nodes  pass  tasks,  model  and  optimizer  weights,  and  other 
metrics  in  order  for  the  aggregator  node  to  combine  the  model  weights  to  construct  a  global 
consensus  model,  which  is  then  passed  back  to  the  collaborator  nodes.  Each  such  process  is  a 
“federated training round”, and continues for preset number of rounds. 

OpenFL was used to simulate a federated learning scenario, where the MC and HUP were 
collaborator nodes in independent compute processes. To avoid cross-institutional data sharing, 
the model weights were aggregated across all datasets by taking a weighted average of updates 
after each epoch, where the weights were proportional to the dataset size. OpenFL ensured that the 
multi-institutional collaborations were secure, especially for those ML model builders who seek 
to protect their model intellectual property or those data holders who wish to uphold the privacy 
of their information [30].  

2.5.   Model Training and Development 

Both U-Nets were trained with the same hyperparameters. They were trained with a batch 
size of 16, a learning rate of 1e-4, and weight decay of 1e-4. The models trained for 30 epochs 
with the Adam optimizer. Data augmentation (random horizontal and vertical flipping of images) 
was used to improve model generalization. The entire dataset was randomly split into training and 
validation  datasets  using  a  4:1  ratio,  respectively.  Validation  performance  was  used  for 
hyperparameter optimization; thus, the model is evaluated on a holdout test dataset in addition to 
the validation dataset to provide a fair assessment of algorithm performance. 

2.6.   Model Evaluation 

The MammoDL algorithm was evaluated by calculating the mean absolute error (MAE) 
between the true and predicted percent density values across the test data. MAE was chosen over 
mean squared error (MSE), or root mean squared error (RMSE) for its interpretability to clinicians, 
its  percent  density 
as  MAE  directly  estimates 
prediction. However, MAE does not provide a full picture of algorithm performance because it 
does  not  spatially  capture  segmentation  accuracies.  Thus,  the  Dice-Sorensen  coefficient  (DSC) 
[43],  a  widely  used  performance  metric  for  segmentation  tasks  [44],  was  used  to  evaluate  the 
performance of the individual segmentation models. DSC essentially measures the area of overlap 
between  ground-truth  segmentations 𝑋 and  algorithmic  segmentations 𝑌.  It  is  computed  by  the 
. The DSC value ranges from 0 to 1, with 1 perfect overlap of the ground 
equation 𝐷𝑆𝐶 =  

the  average  error  of 

the  model 

in 

2|𝑋∩𝑌|
|𝑋|+|𝑌|

truth and predicted segmentations. Reported DSC values are averaged across images. 

2.7.   Code Availability 

All of the work described here is freely available for those who would like to iterate and 

improve upon this algorithm via our GitHub page https://github.com/ramyamut/MammoDL. 

3.  Results 

3.1.   Evaluation on validation dataset 

The model trained on both datasets using a standard centralized training scheme resulted 
in PD MAEs of 3.7918 and 3.9711, breast segmentation DSCs of 0.9896 and 0.9813, and dense 

6 

 
 
 
 
 
  
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

tissue  segmentation  DSCs  of  0.7755  and  0.7078,  on  the  MC  and  HUP  validation  datasets, 
respectively. The model trained on both datasets using our federated training scheme resulted in 
PD MAEs of 3.9740 and 4.2811, breast segmentation DSCs of 0.9880 and 0.9727, and dense tissue 
segmentation  DSCs  of  0.7634  and  0.6787,  on  the  MC  and  HUP  development  datasets, 
respectively.  The  models  trained  on  single-institution  datasets  had  much  worse  performance 
metrics than the models trained on both datasets (Table 2). 

3.2.   Evaluation on independent test dataset 

The model trained on both datasets using a standard centralized training scheme resulted 
in PD MAEs of 3.8616 and 3.9937, breast segmentation DSCs of 0.9893 and 0.9722, and dense 
tissue segmentation DSCs of 0.7745 and 0.6879, on the MC and HUP test datasets, respectively. 
The model trained on both datasets using our federated training scheme resulted in PD MAEs of 
3.9586  and  4.2409,  breast  segmentation  DSCs  of  0.9881  and  0.9657,  and  dense  tissue 
segmentation DSCs of 0.7637 and 0.6417, on the MC and HUP test datasets, respectively.  The 
models trained on single-institution datasets expectedly performed much worse than the models 
that utilized both datasets for development (Table 3). 

4.  Discussion 

The results suggest that there is a demonstrated need to train on multi-institutional datasets, 
as models trained on both HUP and MC data significantly outperform those trained on either HUP 
or MC data only. Additionally, we have shown that models trained on solely HUP or MC data 
tended  not  to  generalize  well  to  data  from  other  sources.  Furthermore,  the  model  trained  with 
federated  learning  achieved  nearly  the  same  performance  as  the  model  trained  with  a  standard 
centralized training scheme, demonstrating that our federated model is robust, and that federated 
learning can be used to solve the problem of breast PD estimation. 

The generalization performance of the MammoDL model is compared with that of some 
breast PD estimation tools mentioned in the introduction of this work.  Even with an optimized 
cutoff value for risk stratification, Quantra suffers from a sensitivity of 65% and a specificity of 
77% when performing BI-RADS classification, where scores of D3/D4 denote high-risk densities 
and  D1/D2  denote  low-risk  densities  [45].  As  Volpara  requires  the  same  metadata  as  Quantra, 
these software tools tend to behave similarly [9], while the outputs of semi-automated thresholding 
tools,  like  Cumulus,  are  known  to  vary  noticeably  based  on  the  expertise  of  the  individual 
operating  the  tool  [11].  As  a  result,  MammoDL  and  Deep-LIBRA  are  the  standout  performers 
when it comes to generalization to unseen mammogram images, with both models boasting strong 
testing  accuracy  metrics.  The  MAE  for  the  PD  estimates  of  MammoDL  are  lower  than  those 
reported  by  Deep-LIBRA  and  LIBRA  (Fig.  4).  However,  the  crucial  difference  between 
MammoDL and Deep-LIBRA is that MammoDL offers a federated learning pipeline that, when 
trained on more than HUP and MC data, will exhibit even better performance due to learning from 
larger, more diverse training data. Furthermore, MammoDL is a leaner, more flexible model that 
requires fewer segmentation steps and no non-DL classifiers.  

In the future, the goal is to incorporate datasets from more institutions to further validate 
the robustness of this model and its federated training algorithm across diverse populations, as well 
as to integrate the training pipeline into a larger framework, such as the Generally Nuanced Deep 
Learning  Framework  (GaNDLF)  [46],  which  would  allow  for  wider  distribution,  more  robust 
training (by leveraging  a standardized set of pre-processing, augmentation, anonymization, and 
DL architectures), and easier hardware-independent deployment. In addition, we hope to bridge 
this  gap  by  creating  a  user  interface  for  clinicians  to  easily  use  our  tool  for  breast  cancer  risk 

7 

 
 
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

assessment. This user interface is still in development and hence is only discussed in the conclusion 
of this work.  

5.  Conclusion 

A  deep  learning  pipeline  was  implemented  to  calculate  breast  PD  from  input 
mammograms.  The  MammoDL  model  produces  a  validation  MAE  of  less  than  5%  and  an 
interpretable spatial map showing the segmented breast and dense tissue regions that clinicians 
value  over  black-box  predictions.  A  robust  federated  training  capability  was  presented  that 
preserves privacy while maintaining the benefits of training on multi-institutional datasets, enabled 
by  OpenFL.  We  have  made  our  code  publicly  available  on  GitHub  in  hopes  that  our  tool  will 
accelerate breast cancer research and clinical care.  

References and Notes 

1.  Howlader N., Noone A.M., Krapcho M., Miller D., Brest A., Yu M., Ruhl J., Tatalovich Z., 

Mariotto A., Lewis D.R., Chen H.S., Feuer E.J., Cronin K.A. SEER Cancer Statistics 
Review, 1975-2016, National Cancer Institute (2019).  

2.  Lauby-Secretan, B., Scoccianti, C., Loomis, D., Benbrahim-Tallaa, L., Bouvard, V., 

Bianchini, F., Straif, K. Breast-Cancer Screening — Viewpoint of the IARC Working Group. 
New England Journal of Medicine 372 (24) (2015).  

3.  Tabar, L., Vitak, B., Chen, T.H., Yen, A.M., Cohen, A., Tot, T., Chiu, S.Y., Chen, S.L., 

Fann, J.C., Rosell, J., Fohlin, H., Smith, R.A., Duffy, S.W. Swedish two-county trial: impact 
of mammographic screening on breast cancer mortality during 3 decades. Radiology 260, 
658-663 (2011).  

4.  IARC Working Group on the Evaluation of Cancer-Preventive Strategies. Breast Cancer 

Screening 15, IARC Press (2016). 

5.  Njor, S., Nystrom, L., Moss, S., Paci, E., Broeders, M., Segnan, N., Lynge, E., Euroscreen 

Working Group. Breast cancer mortality in mammographic screening in Europe: a review of 
incidence-based mortality studies. Journal of Medical Screening 19, 33-41 (2012). 

6.  Duffy, S.W., Tabár, L., Yen, A.M.-F., Dean, P.B., Smith, R.A., Jonsson, H., Törnberg, S., 

Chen, S.L.-S., Chiu, S.Y.-H., Fann, J.C.-Y., Ku, M.M.-S., Wu, W.Y.-Y., Hsu, C.-Y., Chen, 
Y.-C., Svane, G., Azavedo, E., Grundström, H., Sundén, P., Leifland, K., Frodis, E., Ramos, 
J., Epstein, B., Åkerlund, A., Sundbom, A., Bordás, P., Wallin, H., Starck, L., Björkgren, A., 
Carlson, S., Fredriksson, I., Ahlgren, J., Öhman, D., Holmberg, L., Chen, T.H.-H. 
Mammography screening reduces rates of advanced and fatal breast cancers: Results in 
549,091 women. Cancer 126, 2971-2979 (2020).  

7.  Domingo, L., Hofvind, S., Hubbard, R.A., Roman, M., Benkeser, D., Sala, M., Castells, X. 

Cross-national comparison of screening mammography accuracy measures in U.S., Norway, 
and Spain. European Radiology 26, 2520-2528 (2016). 

8.  Maghsoudi, O.H., Gastounioti, A., Scott, C., Pantalone, L., Wu, F., Cohen, E.A., Winham, 

S., Conant, E.F., Vachon, C., Kontos, D. Deep-LIBRA: An artificial-intelligence method for 
robust quantification of breast density with independent validation in breast cancer risk 
assessment. Medical Image Analysis 73, 102138 (2021). 

9.  Sprague, B.L., Gangnon, R.E., Burt, V., Trentham-Deitz, A., Hampton, J.M., Wellman, R.D., 
Kerlikowske, K., Miglioretti, D.L. Prevalence of mammographically dense breasts in the 
United States. Journal of the National Cancer Institute 106 (10) (2014). 

8 

 
 
 
  
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

10. Sprague, B.L., Conant, E.F., Onega, T., Garcia, M.P., Beaber, E.F., Herschorn, S.D., 

Lehman, C.D., Tosteson, A.N.A., Lacson, R., Schnall, M.D., Kontos, D., Haas, J.S., Weaver, 
D.L., Barlow, W.E., PROSPR Consortium. Variation in mammographic breast density 
assessments among radiologists in clinical practice: a multicenter observational study. Annals 
of Internal Medicine 165 (7), 457-464 (2016).  

11. Irshad, A., Leddy, R., Ackerman, S., Cluver, A., Pavic, D., Abid, A., Lewis, M.C. Effects of 
changes in BI-RADS density assessment guidelines (fourth versus fifth edition) on breast 
density assessment: intra-and interreader agreements and density distribution. American 
Journal of Roentgenology 207 (6), 1366-1371 (2016).  

12. Bitencourt, A., Naranjo, I.D., Lo Gullo, R., Saccarelli, C.R., Pinker, K. AI-enhanced breast 
imaging: Where are we and where are we heading? European Journal of Radiology 142, 
109882 (2021).  

13. Hartman, K., Highnam, R., Warren, R., Jackson, V. Volumetric Assessment of Breast Tissue 
composition from FFDM Images. International Workshop on Digital Mammography 5116, 
33-39 (2008). 

14. Highnam, R., Brady, S.M., Yaffe, M.J., Karssemijer, N., Harvey, J. Robust Breast 

Composition Measurment. International Workshop on Digital Mammography 6136, 378-385 
(2010). 

15. Kontos, D. Bakic, P.R., Acciavatti, R.J., Conant, E.F., Maidment, A.D.A. A Comparative 

Study of Volumetric and Area-Based Breast Density Estimation in Digital Mammography: 
Results from a Screening Population. International Workshop on Digital Mammography 
6136, 378-385 (2010). 

16. Keller, B.M., Nathan, D.L., Wang, Y., Zheng, Y., Gee, J.C., Conant, E.F., Kontos, D. 

Estimation of breast percent density in raw and processed full field digital mammography 
images via adaptive fuzzy c-means clustering and support vector machine segmentation. 
Medical Physics 38 (8), 4903-4917 (2012).  

17. Mustra, M., Grgic, M., Rangayyan, R.M. Review of recent advances in segmentation of the 

breast boundary and the pectoral muscle in mammograms. Medical & Biological 
Engineering & Computing 54, 1003-1024 (2016). 

18. Li, Y., Chen, H., Yang, Y., Yang, N. Pectoral muscle sgmentation in mammograms based on 
homogenous texture and intensity deviation. Pattern Recognition 46 (3), 681-691 (2013). 

19. Shi, P., Zhong, J., Rampun, A., Wang, H. A hierarchical pipeline for breast boundary 
segmentation and calcification detection in mammograms. Computers in Biology and 
Medicine 96, 178-188 (2018). 

20. Anitha, J., Peter, J.D., Pandian, S.I.A. A dual stage adaptive thresholding (DuSAT) for 
automatic mass detection in mammograms. Computer Methods and Programs in 
Biomedicine 138, 93-104 (2017). 

21. Ferrari, R.J., Rangayyan, R.M., Desautels, J.E.L., Borges, R.A., Frere, A.F. Auto- matic 
identification of the pectoral muscle in mammograms. IEEE Transactions on Medical 
Imaging 23 (2), 232-245 (2004).  

22. Kwok, S.M., Chandrasekhar, R., Attikiouzel, Y., Rickard, M.T. Automatic pectoral muscle 
segmentation on mediolateral oblique view mammograms. IEEE Transactions on Medical 
Imaging 23 (9), 1129-1140 (2004).  

23. Mustra, M., Grgic, M. Robust automatic breast and pectoral muscle segmentation from 

scanned mammograms. Signal Processing 93 (10), 2817-2827 (2013).  

24. Nagi, J., Kareem, S.A., Nagi, F., Ahmed, S.K. Automated breast profile segmentation for 
ROI detection using digital mammograms. 2010 IEEE EMBS Conference on Biomedical 
Engineering and Sciences (IECBES). IEEE, 87-92 (2010).  

9 

 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

25. Taghanaki, S.A., Liu, Y., Miles, B., Hamarneh, G. Geometry-based pectoral mus- cle 
segmentation from MLO mammogram views. IEEE Transactions on Biomedical 
Engineering 64 (11), 2662-2671 (2017).  

26. Rampun, A., Morrow, P.J., Scotney, B.W., Winder, J. Fully automated breast boundary and 
pectoral muscle segmentation in mammograms. Artificial Intelligence in Medicine 79, 28-41 
(2017).  

27. Czaplicka, K., Włodarczyk, J. Automatic breast-line and pectoral muscle segmentation. 

Schedae Informaticae 20 (2011).  

28. Dembrower, K., Liu, Y., Azizpour, H., Eklund, M., Smith, K., Lindholm, P., Strand, F. 
Comparison of a deep learning risk score and standard mammographic density score for 
breast cancer risk prediction. Radiology 294 (2), 265-272 (2020).  

29. Sheller, M.J., Edwards, B., Reina, G.A., Martin, J., Pati, S., Kotrotsou, A., Milchenko, M., 

Xu, W., Marcus, D. Colen, R.R., Bakas, S. Federated learning in medicine: facilitating multi-
institutional collaborations without sharing patient data. Scientific Reports 10, 12598 (2020). 
30. Reina, G.A., Gruzdev, A., Foley, P., Perepelkina, O., Sharma, M., Davidyuk, I., Trushkin, I., 
Radionov, M., Mokrov, A., Agapov, D., Martin, J., Edwards, B., Sheller, M.J., Pati, S., 
Moorthy, P.N., Wang, S., Shah, P., Bakas, S. OpenFL: An open-source framework for 
Federated Learning. arXiv (2021).  

31. Rodríguez-Ruiz, A., Krupinski, E., Mordang, J., Schilling, K., Heywang- Kobrunner, S.H., 
Sechopoulos, I., Mann, R.M. Detection of breast cancer with mammography: effect of an 
artificial intelligence support system. Radiology 290, 305–314 (2019).  

32. Lin, T., Dollar, P., Girshick, R., He, K., Hariharan, B., Belongie, S. Feature Pyramid 

Networks for Object Detection. arXiv (2017). 

33. Novikov, A.A., Lenis, D., Major, D., Hladuvka, J., Wimmer, M., Buhler, K. Fully 

Convolutional Architectures for Multiclass Segmentation in Chest Radiographs. IEEE 
Transactions on Medical Imaging 37 (8), 1865-1876 (2018).  

34. McCarthy, A.M., Keller, B.M., Pantalone, L.M., Hsieh, M.-K., Synnestvedt, M., Conant, 
E.F., Armstrong, K., Kontos, D. Racial differences in quantitative measures of area and 
volumetric breast density. Journal of the National Cancer Institute 108 (10), djw104 (2016).  
35. He, K., Zhang, X., Ren, S., Sun, J. Deep residual learning for image recognition. Proceedings 
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770-778 
(2016). 

36. He, F., Liu, T., Tao, D. Why ResNet Works? Residuals Generalize. IEEE Transactions on 

Neural Networks and Learning Systems 31 (12), 5349-5362 (2020).  

37. Ronneberger, O., Fischer, P., Brox, T. U-Net: convolutional networks for biomedical image 
segmentation. International Conference on Medical Image Computing and Computer-
Assisted Intervention, 234-241 (2015). 

38. Reike, N., Hancox, J., Li W., Milletari, F., Roth, H.R., Albarqouni, S., Bakas, S., Galtier, 

M.N., Landman, B.A., Maier-Hein, K., Ourselin, S., Sheller, M., Summers, R.M., Trask, A., 
Xu, D., Baust, M., Cardoso, M.J. The future of digital health with federated learning. npj 
Digital Medicine 3 (119), (2020). 

39. Jalali, Y., Fateh, M., Rezvani, M., Abolghasemi, V., Anisi, M.H. ResBCDU-Net: A Deep 

Learning Framework for Lung CT Image Segmentation. Sensors (Basel) 21 (1), 268 (2021). 

40. Pati, S., Baid, U., Edwards, B., Sheller, M., et al. Federated Learning Enables Big Data for 

Rare Cancer Boundary Detection. arXiV (2022). 

41. Pati, S., Baid, U., Zenk, M., Edwards, B., Sheller, M., Reina, G.A., et al. The Federated 

Tumor Segmentation (FeTS) Challenge. arXiV (2021). 

10 

 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

42. Baid, U., Pati, S., Thakur, S., Edwards, B., Sheller, M., Martin, J., Bakas, S. Federated 

Tumor Segmentation (FeTS) Initiative: The First Large-Scale Real-World Federation. Neuro-
Oncology 23 (6), 135-136 (2021).   

43. Zijdenbos, A.P., Dawant, B.M., Margolin, R.A., Palmer, A.C. Morphometric analysis of 

white matter lesions in MR images: method and validation. IEEE Transactions on Medical 
Imaging 13 (4), 716-725, (1994). 

44. Maier-Hein, L., Reinke, A., Godau, P., Tizabi, M.D., et al. Metrics reloaded: Pitfalls and 

recommendations for image analysis validation. arXiV (2022). 

45. Richard-Davis, G., Whittemore, B., Disher, A., Rice, V.M., Lenin, R.B., Dollins, C., Siegel, 
E.R., Eswaran, H. Evaluation of Quantra Hologic Volumetric Computerized Breast Density 
Software in Comparison With Manual Interpretation in a Diverse Population. Breast Cancer: 
Basic and Clinical Research 12 (2018).  

46. Pati, S., Thakur, S.P., Bhalerao, M., Thermos, S., Baid, U., Gotkowksi, K., Gonzalez, C., 

Guley, O., Hamamci, I.E., Er, S., Grenko, C., Edwards, B., Sheller, M., Agraz, J., Baheti, B., 
Bashyam, V., Sharma, P., Haghighi, B., Gastounioti, A., Bergman, m. Mukhopadhyay, A., 
Tsaftaris, S.A., Menze, B., Kontos, D., Davatzikos, C., Bakas, S. GaNDLF: A Generally 
Nuanced Deep Learning Framework for Scalable End-to-End Clinical Workflows in Medical 
Imaging. arXiv (2021).  

11 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

Acknowledgments:  This  work  was  supported  in  part  by  the  University  of  Pennsylvania 
Department of Electrical and Systems Engineering Senior Design Fund. 

Author Contributions: Ramya Muthukrishnan: Primary code development, data preprocessing, 
performance  metric  calculations,  writing  (final  draft).  Angelina  Heyler:  Federated  learning 
pipeline development, performance metric calculations. Keshava Katti: Writing (original draft), 
initial code development. Sarthak Pati: Support  with OpenFL, writing (review/editing). Walter 
Mankowski: data procurement. Aprupa Alahari: Development of user interface for clinical use. 
Michael Sanborn: Development of user interface for clinical use. Emily F. Conant: Radiologist 
insight,  user  testing.  Pratik  Chaudhari:  Advisor  for  DL  model  selection.  Spyridon  Bakas: 
Conceptualization  of 
resources,  writing 
(review/editing), supervision, project administration. Despina Kontos: Conceptualization of breast 
PD  estimation  model,  computation  resources,  writing  (review/editing),  supervision,  project 
administration.    

learning  components,  computation 

federated 

Competing interests: The authors declare no competing interests.  

Fig. 1. System-level diagram of MammoDL. (a) Three collaborator nodes are shown, with each 
performing local training and then transfering the relevant tasks, model and optimizer weights, and 
other  metrics  to  the  aggregator  node.  (b)  The  aggregator  node  represents  the  two  UNet 
architectures with ResNet34 encodings. Model/metric updates occur in the aggregator node, which 
train the model to execute segmentation of the breast and dense tissue based on information from 
all of the collaborators combined. After training is complete, a user can run inference on a new 
FFDM image to return a breast PD estimation and a spacial mapping that shows the dense tissue 
delineated from the non-dense tissue. 

12 

 
 
 
 
 
 
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

Fig. 2. Diagram of UNet with ResNet34 encoder. The left-hand portion of the UNet architecture 
corresponds to the “down-sampling” or “encoder” section, where the ResNet34 leverages 2 × 2 
adaptive average pooling to progressively reduce the dimensions of the mammogram image while 
increasing the number of feature maps. The ResNet34 operates in five distinct blocks separated by 
pooling  layers,  the  first  corresponding  to  a  single  7 × 7  convolutional  layer  with  batch 
normalization  and  ReLU  activation  and  the  last  four  corresponding  to  32 3 × 3 convolutional 
layers with batch normalization and ReLU activation. The last layer in the ResNet34 is typically a 
fully-connected classifier layer, which is omitted in this UNet architecture. The left-hand portion 
represents  the  “up-sampling”  or  “decoding”  section,  which  involves  mirror-like  2 × 2  up-
convolution operations separating convolutional layers until the mammogram image returns to its 
original size, albeit with only a single channel.  

13 

 
 
 
 
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

Fig. 3. Stages of the segmentation process in the MammoDL training pipeline. (a) This image 
shows an unprocessed FFDM in MLO-view from the HUP dataset. (b) After pre-processing, which 
includes removal of the metal tag, normalization to [0, 255], down-sampling to 512 × 512, and 
adjustment of contrast, the first UNet segments the breast from all other parts of the FFDM image, 
including the background and pectoralis muscle. Some data augmentation does occur at this stage, 
limited to flips across the x- and y-axis. (c) The background and pectoralis muscle are removed 
from the image entirely. Since the initial image was in MLO-view, the absence of the pectoralis 
muscle is noticeable. The area (i.e., number of pixels) corresponding to the breast is returned. (d) 
The second UNet delineates the dense tissue from the non-dense tissue. The area corresponding to 
the dense tissue is returned. The output of the second segmentation task is divided by the output 
of the first segmentation task, representing the proportion of dense tissue detected in the FFDM 
image. This value is returned as a percentage.   

Dataset Type 
Institution 
Number of Images 
Number of Women 
Screening Start Date 
Screening End Date 
White (%) 
Black/Other (%) 

MC 
3,314 
1,662 
2008 
2012 
98 
2 

Training and Validation 

Test 

HUP 
1,147 
575 
2010 
2014 
47 
53 

MC 
6463 
414 
2008 
2014 
94 
6 

HUP 
278 
110 
2011 
2014 
- 
- 

Table 1. General characteristics corresponding to the datasets. Each dataset is accompanied 
by its institution, the number of FFDM images present, the number of women in the screening 
cohort, the year that the screening began, the year that the screening finished, the percent of white 
subjects present in the screening and the percent of black/other subjects present in the screening. 

14 

 
 
 
 
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

Although race information was not available for the test HUP subjects, it is known that the racial 
distribution of the cohort was similar to that of the diverse screening population at HUP [35]. 

Training 
Type 

Training/ 
Validation 
Data 

Performance on MC Validation Data 

Performance on HUP Validation Data 

MAE (%)  Breast 

Dense DSC  MAE (%)  Breast 

HUP + MC  3.9740 
Federated 
13.3893 
Centralized  HUP 
Centralized  MC 
21.4865 
Centralized  HUP + MC  3.7918 

DSC 
0.9880 
0.9545 
0.9865 
0.9896 

0.7634 
0.0037 
0.4162 
0.7755 

4.2811 
9.0138 
13.8530 
3.9711 

DSC 
0.9727 
0.9766 
0.1389 
0.9813 

Dense DSC 

0.6787 
0.2732 
0.1895 
0.7078 

Table 2. Performance on development validation dataset using federated versus centralized 
training. As expected, the models trained on both the HUP and MC data perform better than those 
trained  on  single-institution  datasets,  which  provides  strong  motivation  for  the  need  for  multi-
institutional  collaboration  in  ML-based  medical  imaging.  The  performance  of  the  federated 
training scheme closely approaches that of the centralized one, model But the centralized training 
scheme  presents  numerous  privacy  and  ownership  concerns,  while  maintining  the  privacy  of 
patient mammogram data. Therefore, the federated learning pipeline represents a crucial addition 
to breast PD estimation tools in order to ensure generalization performance is maximized. 

Training 
Type 

Development 
Data 

Performance on MC Test Data 

Performance on HUP Test Data 

MAE (%)  Breast 

MAE (%)  Breast 

HUP + MC 

Federated 
Centralized  HUP 
Centralized  MC 
Centralized  HUP + MC 

3.9586 
12.2529 
27.4804 
3.8616 

DSC 
0.9881 
0.8245 
0.9827 
0.9900 

Dense 
DSC 
0.7637 
0.0060 
0.3815 
0.7745 

4.2409 
6.8729 
11.2794 
3.9937 

DSC 
0.9686 
0.9625 
0.3954 
0.9722 

Dense 
DSC 
0.6417 
0.3523 
0.3210 
0.6879 

Table 3. Performance on test dataset using federated versus centralized training. The trend 
seen in Table 2 is replicated here.  

15 

 
 
 
 
 
 
 
 
 
 
 
 
Unsubmitted Manuscript: Confidential 
Template Revised September 2022 

Fig.  4.  Generalization  performance  of  MammoDL  in  comparison  to  other  breast  PD 
estimation tools.  This  bar plot illustrates the testing accuracy  exhibited  by  MammoDL, Deep-
LIBRA,  and  LIBRA.  The  y-axis  represents  the  MAE  with  respect  to  the  the  “gold-standard” 
Cumulus  PD  values.  The  bars  for  Deep-LIBRA  and  LIBRA  represent  the  3  reported  cross-
validation folds in the Deep-LIBRA paper [9]. MammoDL has two bars, the former representing 
the performance of the federated model and the latter showing the results of the centralized model 
(Table  3).  All  3  models  were  tested  on  data  from  the  Mayo  Clinic,  although  the  test  dataset 
consisted of different images for MammoDL. The generalization behavior of the federated and 
centralized MammoDL models are better than those of both Deep-LIBRA and LIBRA.  

16 

 
 
