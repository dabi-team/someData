Physical LiDAR Simulation in Real-Time Engine

Wouter Jansen∗†‡, Nico Huebel∗†, Jan Steckel∗†
∗FTI Cosys-Lab, University of Antwerp, Antwerp, Belgium
†Flanders Make Strategic Research Centre, Lommel, Belgium
‡wouter.jansen@uantwerpen.be

2
2
0
2

g
u
A
3
2

]

O
R
.
s
c
[

2
v
5
9
2
0
1
.
8
0
2
2
:
v
i
X
r
a

Abstract—Designing and validating sensor applications and
algorithms in simulation is an important step in the modern
development process. Furthermore, modern open-source multi-
sensor simulation frameworks are moving towards the usage of
video-game engines such as the Unreal Engine. Simulation of a
sensor such as a LiDAR can prove to be difﬁcult in such real-time
software. In this paper we present a GPU-accelerated simulation
of LiDAR based on its physical properties and interaction with
the environment. We provide a generation of the depth and
intensity data based on the properties of the sensor as well as the
surface material and incidence angle at which the light beams
hit the surface. It is validated against a real LiDAR sensor and
shown to be accurate and precise although highly depended on
the spectral data used for the material properties.

I. INTRODUCTION

There are many existing simulation frameworks in robotics,
both closed [1]–[7] and open-source [8]–[11]. Techniques such
as real-time rendering, particularly with video-game engines
such as Unity and Unreal have caused accelerated devel-
opment of open-source simulation platforms used by many
organisations and researchers for experimental development,
research, validation and comparison against other systems and
algorithms. One vital aspect of any simulation platform is
to provide accurate sensor models without unrealistic or too
idealistic assumptions while balancing the complexity and
subsequent computational demands of such sensor models
against the simulation performance.
Within this paper we present our implementation of the sim-
ulation of Light Detection And Ranging (LiDAR) sensors in
an open-source framework using the real-time Unreal Engine.
These provide a 3D depth map of the environment (in the
form of a point cloud). However, to our knowledge most
documented and available simulations have one or multiple
shortcomings: (a) they are often build for addressing a speciﬁc
target problem and do not come in a modular framework
together with other sensors [12]–[17]; (b) they provide a
perfect rendition of the perceived environment with simple
noise models [8], [9], [11], or (c) they don’t allow for realistic
environments to use LiDAR sensors [18], [19]. LiDAR is
projecting beams of structured light at a speciﬁc wavelength
with a certain power, which is subject to distortions from
environmental factors such as the properties of the reﬂecting
surfaces. The surface properties are mainly deﬁned by the
angle between the light beam and the surface as well as
the material of the surface, which both affect the reﬂection
intensity perceived by the LiDAR [20], [21].

The major contribution of this research is a realistic real-
time simulation of a high resolution 3D LiDAR sensor in
complex environments by including angle dependant material
reﬂectance properties. Furthermore, other properties of the
LiDAR sensor such as its beam conﬁguration and range capa-
bilities are also modelled and implemented. The implemented
sensor models were based on the existing work of other peer-
reviewed publications. To ensure the real-time performance,
some the computations are done on the GPU. Instance seman-
tic ground-truth labels are also provided for each point. We
validate the simulation by comparing real-world measurements
against a re-created, simulated environment and compare the
accuracy and precision of the resulting reﬂectance and depth
of each point for various material types and different ranges
between the objects and sensor. Finally, we provide an open-
source release [22] of our LiDAR simulation for the AirSim
framework [8] so it can be cross-validated and used by others.

Fig. 1. Overview of the GPU-accelerated LiDAR simulation in Unreal Engine.
Several shader materials are generated on the GPU which are subsequently
read on the CPU from the textures. Their output is used together with a look-
up-table of surface spectral data and the sensor conﬁguration to generate a
3D point cloud with the depth, intensity and instance segmentation label data.

II. SIMULATION IMPLEMENTATION
Our method for simulating LiDAR sensors is implemented
within our version of the AirSim framework [8], [23] as it
provides a wide range of mobile platforms and a stable API for
control and sensor data extraction. As AirSim is built upon the
Unreal Engine, its real-time render pipeline is used to generate
the LiDAR point cloud data. Each subsection will provide a
more detailed explanation on how the data is generated. An
overview of the implementation is shown in Fig. 1.

Unreal EngineGPUCPUDepthImpact AngleSurface MaterialMemory ReadRGB888RGBA8888RGB888A8+Surface MaterialReflectance LUTLidar CapabilityConfigurationDepth + Intensity3D PointcloudGenerationInstance SemanticLabelsGenerationRGB888Shader MaterialsLiDAR SimulationInstance Segmentation 
 
 
 
 
 
A. Depth Point Cloud Generation

B. Instance Semantic Ground-truth

The base of any LiDAR simulation is the depth data
generation. Our method uses a virtual render camera that for
each time step of the simulation will be rotated according to
the elapsed as the simulation is real-time and not ﬁxed step.
This camera has a single post-process material shader that
uses the depth pass of the engine G-Buffer [24]. This serves
as the base of the GPU-accelerated LiDAR simulation method
we use. In order to pertain the highest precision possible, the
depth ﬂoating point value of the G-Buffer is saved by the
shader over the RGB channels of the material to achieve three
bytes unsigned precision. Subsequently, this render camera is
parsed and read out by the CPU and stored in a CPU memory
buffer. This camera has a ﬁxed conﬁgurable square resolution
of width W .
The virtual camera uses the pin-hole model and has a dynamic
Field Of View (FOV) that is calculated before rendering at
each time step by knowing how much of the LiDAR’s hori-
zontal FOV F OVh needs to be parsed based on the sensor’s
spinning frequency ωl and the time delta to the previous time
step. As the resolution is square, the vertical and horizontal
FOV of the virtual camera are equal and deﬁned as F OVc.
With the camera buffer now available on the CPU, the next
step is to sample this image for each of the LiDAR’s beams
based on the horizontal and vertical sample points that are
predeﬁned. In order to do so we need to convert from the
spherical coordinates of the LiDAR beams to the pixel-based
image space coordinates. The virtual render camera’s intrinsic
matrix Kc can be deﬁned as that of an ideal camera without
distortion:

Kc =





fx
0
0

0
fy
0



cu
cv
1



(1)

with the focal lengths fx and fy and principal camera point

(cu, cv) deﬁned as

2 tan

W
2

cu = cv =

fx = fy =

W
F OVc
2
Subsequently, for a single light beam li,j of the LiDAR with
horizontal sample index i and vertical index j and spherical
coordinates (θi, φj) based on the LiDAR properties F OVv and
F OVh for their vertical and horizontal FOVs respectively, we
can calculate the pixel coordinates (ni

(2)

ni

x =

(sin θifx)
cos θi

+ cu nj

y =

y).

x, nj
(sin φj − fy)
cos φj cos θi

+ cy

(3)

The azimuth θi is deﬁned by the horizontal FOV F OVh
of the sensor, which is usually [0°, 360°] for a 360° LiDAR
sensor. Similarly, The elevation φj is deﬁned by the vertical
FOV F OVv and is deﬁned in the sensor data sheets how the
different beams are spread out.
Then the depth value for this light beam can be retrieved from
the CPU buffer at these coordinates. Gaussian noise is also
added within this model as a separate property to deﬁne a
noise scaling factor.

While AirSim already includes ground-truth labels for ob-
jects classes, it is limited to 255 different classes as it relies
on the custom depth buffer (a single byte value) of the Unreal
renderer. UnrealCV [25], [26] provides a solution to uniquely
label millions of object in the virtual world by overwriting
their vertex colours with a unique RGB88 colour that is saved
in a data table. We applied this solution to the LiDAR sensor
within the AirSim framework and upgraded it to the latest
versions of the Unreal Engine. Similar to the depth data
generation, a virtual render camera was used, but with an
additional post processing material shader to only show the
vertex colours. Using the sampling technique described for
the point cloud generation, the RGB888 colour value for each
LiDAR point can be obtained. This value allows to ﬁnd the
object name and class of millions of unique objects from the
automatically generated data table.

C. Surface Material Model

LiDAR is inﬂuenced by material properties of the objects it
observes. The used model is based on the work by Muchken-
huber et al. [20]. We refer to their article for extensive details.
We integrate their model for Lambertian target reﬂectance
Rλ, a value in percentage describing the reﬂectance mea-
sured for wavelength λ at various incidence angles θ. Their
work refers to the ECOSTRESS spectral library [27] which
includes the spectra of thousands of materials measured at
various wavelengths including some that are commonly seen
in LiDAR applications. The samples in this library represent
the reﬂectance in percentage for a incidence angle θ of 0°
Rλ(θ = 0◦). To include the inﬂuence of the incidence angle
of the LiDAR beam in the material model, we can derive the
Lambertian target reﬂectance Rλ for a certain material with a
speciﬁc incidence angle as:

Rλ(θ) = Rλ(θ = 0◦) cos(θ)

(4)

The incidence angle of the LiDAR can be retrieved in
the Unreal Engine similarly to the other implementations
using a custom material shader. In order to calculate the
incidence angle of the LiDAR light beam to each point in
that can be seen from the virtual render
the environment
camera, we make use of the dot product of the surface normal,
which is the unit vector perpendicular to the surface, and the
vector representing the LiDAR light beam. To achieve three
bytes precision, this angular value is stored in the shader in a
RGBA8888 texture.
This leaves the surface material to be acquired for each point.
We chose to use the stencil buffer of Unreal, a custom depth
buffer of a single byte value per pixel. This limits the use
to 255 different materials. We attribute each integer value to
a unique material of the ECOSTRESS spectral library for
the wavelength λ in use by the LiDAR, which is stored at
rendering in the alpha channel of the previously mentioned
RGBA8888 output texture. By parsing this texture on the CPU
we can extract the material and the incidence angle and use
(4) to calculate their inﬂuence on the reﬂectance and intensity.

D. LiDAR Capability Model

The optical performance of a LiDAR sensor is deﬁned in
its data sheets. This includes the reﬂectance limit function
RL(d), which depicts the maximum range d for which a point
would be detected depending on the corresponding Lambertian
target reﬂectance Rλ. For our LiDAR capability model we
implemented a linear function RL(d) with a conﬁgurable
maximum range dmax and the corresponding target reﬂectance
RL,max, which is needed to detect a point at that maximum
range. Then, between a range of 0 m and dmax the function
interpolates Rλ linearly. Any point beyond dmax is dropped.

RL(d) =

RL,max
dmax

× d

(5)

For any point its depth d (including Gaussian noise) and
reﬂectance intensity Rλ from model described in Section II-C
are obtained. Then this linear function is used to decide if the
point is detected (RL(d) ≤ Rλ), or if the point is dropped.

TABLE I
INTENSITY AND DEPTH RESULTS BY DISTANCE TO OBJECTS

Distance
to Objects
33 m
25 m
18 m
12 m
7 m
5 m
4 m
2.5 m

Mean

Std Dev

Depth Error

Intensity Error
Mean
14.65 cm 12.69 cm 10.65
13.54 cm 11.63 cm
8.45
10.69 cm 10.15 cm 10.69
8.65 cm
8.65
6.41 cm
7.11
6.25 cm
8.46
5.88 cm
10.48
5.12 cm
11.35

Std Dev
12.60
12.33
11.88
11.68
10.23
11.95
12.38
12.60

9.77 cm
8.13 cm
6.80 cm
5.46 cm
4.74 cm

TABLE II
INTENSITY AND DEPTH RESULTS BY SURFACE MATERIAL

Depth Error

Surface
Material
Car Paint
Car Light
Reﬂector
Rubber
Concrete
Asphalt
Glass
Wood

Intensity Error
Std Dev
Mean
Mean
6.14 cm
8.49 cm
9.11
10.43 cm 13.41 cm
6.66
13.54 cm 13.45 cm 20.48
8.44 cm 11.98 cm
9.48
7.65 cm
9.00 cm
9.30
3.15 cm
8.68 cm
8.99
0.00 cm
0.00 cm
0.00
4.79 cm
5.45 cm
4.19
Aluminium 18.41 cm 15.12 cm 17.10

Std Dev
11.68
13.90
24.82
13.41
12.16
14.69
0.00
2.68
14.30

These results are shown in Table I and show that the further
away from the objects, the error becomes larger. The accuracy
and precision overall is acceptable and perceived as similar
with that of the recreated objects using the reconstruction
methods. Similarly, in Table II we show the analysis of the
depth and intensity error but now comparing it between the
various surface materials. Here a clear difference can be
observed between the materials. As the ECOSTRESS data is
interpolated for all incidence angles in our model, as well
as being not a perfect match for the materials used in the
experiments, these differences are expected. Furthermore, the
lack of more incidence angle data points in the library is
shown by the lower precision of some of the results. Similar
limitations of using the ECOSTRESS library in this context
were also noted by Muchkenhuber et al. [20]. Finally, the real-
time simulation using GPU-acceleration proved to keep up
with the general simulation rate by the AirSim framework.

IV. CONCLUSIONS & FUTURE WORKS

The simulation of the LiDAR sensor provides the necessary
simulation of depth, intensity and instance segmentation labels
to serve a variety of use-cases. The experimental results show
decent enough accuracy and precision. However, some clear
shortcomings are evident as the ECOSTRESS data is too
limited in scope and detail. The library is too restrictive in the
man-made material section for developing complex and varied
environments that are typical for typical LiDAR applications.
A much broader and larger dataset is needed for more man-
made materials and at various incidence angles for several
common LIDAR wavelengths. Finally, several other LiDAR
physical aspects such as weather inﬂuence, beam divergence
and multi-echo LIDAR can also be considered for future
iterations of the simulation.

Fig. 2. Objects of various materials that were re-created in simulation to
compare measurements of a real Ouster OS0-128 LiDAR sensor and its
simulation counterpart at various distances to the objects.

III. EXPERIMENTAL VALIDATION

For validation of the proposed simulation models, we check
if the simulation model approaches a real sensor to a sufﬁcient
level for various use-cases such as application testing, transfer-
learning and others. We compare real-world recordings with
an Ouster OS0-128 LiDAR (128 beam channels, F OVv of
90°, 1024 samples over a F OVh of 360°) [28] of several
objects made of different materials which can be seen in
Fig. 2. Subsequently, we re-created an accurate analogue of
the real scene in the simulation framework. We used manual
measurements and an Intel Realsense D435i depth camera,
in combination with the Open3D software library [29] for
facilitating scene reconstruction of the more complex objects.
We use the ECOSTRESS data as input for the various surface
materials and their reﬂectance value at the wavelength of the
Ouster OS0-128 which is at 850 nm. From its data sheet
we can also extract the maximum range dmax of 50 m at
a reﬂectance value RL,max of 80%. In a ﬁrst validation we
compare measurements at different ranges to the objects and
calculate the error in both the depth and intensity values for
each beam of the LiDAR sensor between real and simulated
points.

REFERENCES

[1] Siemens, “Lidar simulation and validation for autonomous vehicles.”
[Online]. Available: https://www.plm.automation.siemens.com/global/
en/webinar/lidar-simulation/87062

[2] Ansys, “Ansys AVxcelerate Sensors Test and Validate Sensors for
Self-Driving Cars.” [Online]. Available: https://www.ansys.com/de-de/
products/av-simulation/ansys-avxcelerate-sensors

[3] Vector

and DYNA4,

“Environment Perception for ADAS and
Autonomous Driving.” [Online]. Available: https://www.vector.com/int/
en/products/products-a-z/software/dyna4/sensor-simulation/

[4] Vires and VTD, “Sensor modelling in Autonomous Driving.” [Online].

Available: https://vires.mscsoftware.com/solutions/sensors/

[5] NVIDIA, “DriveWorks SDK.” [Online]. Available: https://developer.

nvidia.com/drive/driveworks

[6] rFpro, “ADAS & Autonomous.” [Online]. Available: https://www.rfpro.

[24] Epic Games,
Overview.”
en-US/RenderingAndGraphics/Overview/

- Rendering
Engine Documentation
[Online]. Available: https://docs.unrealengine.com/4.26/

“Unreal

[25] W. Qiu and A. Yuille, “Unrealcv: Connecting computer vision to unreal
engine,” in Computer Vision – ECCV 2016 Workshops, G. Hua and
H. J´egou, Eds.
Cham: Springer International Publishing, 2016, pp.
909–916.

[26] W. Qiu, F. Zhong, Y. Zhang, S. Qiao, Z. Xiao, T. S. Kim, Y. Wang,
and A. Yuille, “Unrealcv: Virtual worlds for computer vision,” ACM
Multimedia Open Source Software Competition, 2017.

[27] Jet Propulsion Laboratory and California Institute of Technology.,
“ECOSTRESS Spectral Library—Version 1.0.” [Online]. Available:
https://ecostress.jpl.nasa.gov/
View
Ultra-Wide

High-Resolution

[28] “OS0

Imaging

Lidar
https://data.ouster.io/downloads/

Datasheet.”
datasheets/datasheet-rev05-v2p1-os0.pdf

[Online]. Available:

com/virtual-test/adas-and-autonomous/

[29] Q.-Y. Zhou, J. Park, and V. Koltun, “Open3D: A modern library for 3D

data processing,” arXiv:1801.09847, 2018.

[7] dSpace, “AURELION Lidar Model.”

[Online]. Available: https:
//www.dspace.com/fr/fra/home/products/sw/experimentandvisualization/
aurelion sensor-realistic sim/aurelion lidar.cfm

[8] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-ﬁdelity visual
and physical simulation for autonomous vehicles,” in Field and Service
Robotics, M. Hutter and R. Siegwart, Eds. Cham: Springer International
Publishing, 2018, pp. 621–635.

[9] G. Rong, B. H. Shin, H. Tabatabaee, Q. Lu, S. Lemke, M. Moˇzeiko,
E. Boise, G. Uhm, M. Gerow, S. Mehta, E. Agafonov, T. H. Kim,
E. Sterner, K. Ushiroda, M. Reyes, D. Zelenkovsky, and S. Kim,
“Lgsvl simulator: A high ﬁdelity simulator for autonomous driving,” in
2020 IEEE 23rd International Conference on Intelligent Transportation
Systems (ITSC), 2020, pp. 1–6.

[10] Webots, “http://www.cyberbotics.com,” open-source Mobile Robot

Simulation Software. [Online]. Available: http://www.cyberbotics.com

[11] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,
“CARLA: An open urban driving simulator,” in Proceedings of the 1st
Annual Conference on Robot Learning, 2017, pp. 1–16.

[12] G. F. Gusm˜ao, C. R. H. Barbosa, and A. B. Raposo, “Development
and validation of lidar sensor simulators based on parallel raycasting,”
Sensors, vol. 20, no. 24, 2020.

[13] S. Manivasagam, S. Wang, K. Wong, W. Zeng, M. Sazanovich, S. Tan,
B. Yang, W.-C. Ma, and R. Urtasun, “Lidarsim: Realistic lidar simulation
by leveraging the real world,” in 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2020, pp. 11 164–
11 173.

[14] J. Fang, D. Zhou, F. Yan, T. Zhao, F. Zhang, Y. Ma, L. Wang, and
R. Yang, “Augmented lidar simulator for autonomous driving,” IEEE
Robotics and Automation Letters, vol. 5, no. 2, pp. 1931–1938, 2020.

[15] T. Hanke, A. Schaermann, M. Geiger, K. Weiler, N. Hirsenkorn,
A. Rauch, S.-A. Schneider, and E. Biebl, “Generation and validation of
virtual point cloud data for automated driving systems,” in 2017 IEEE
20th International Conference on Intelligent Transportation Systems
(ITSC), 2017, pp. 1–6.

[16] J. Wang and J. Shan, “Segmentation of LiDAR point clouds for building
extraction,” American Society for Photogrammetry and Remote Sensing
Annual Conference 2009, ASPRS 2009, vol. 2, pp. 870–882, 2009.
[17] K. Vasstein, E. F. Brekke, R. Mester, and E. Eide, “Autoferry Gemini: A
real-time simulation platform for electromagnetic radiation sensors on
autonomous ships,” in IOP Conference Series: Materials Science and
Engineering, vol. 929, no. 1.

IOP Publishing Ltd, 11 2020.

[18] N. Koenig and A. Howard, “Design and use paradigms for Gazebo,
an open-source multi-robot simulator,” 2004 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), vol. 3, pp. 2149–
2154, 2004.
[19] MathWorks,

“Automated Driving Toolbox.”

[Online]. Available:

https://nl.mathworks.com/products/automated-driving.html

[20] S. Muckenhuber, H. Holzer, and Z. Bockaj, “Automotive Lidar Mod-
elling Approach Based on Material Properties and Lidar Capabilities,”
Sensors, vol. 20, no. 11, p. 3309, 6 2020.

[21] F. E. Nicodemus, “Directional Reﬂectance and Emissivity of an Opaque

Surface,” Applied Optics, vol. 4, no. 7, pp. 767–775, 7 1965.

[22] Cosys-Lab,

“GPULiDAR-AirSim Github Repository.”

[Online].

Available: https://github.com/Cosys-Lab/GPULiDAR-AirSim

[23] G. Schouten, W. Jansen, and J. Steckel, “Simulation of Pulse-Echo Radar
for Vehicle Control and SLAM,” Sensors 2021, vol. 21, no. 2, p. 523,
1 2021.

