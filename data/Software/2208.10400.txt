DP-Rewrite: Towards Reproducibility and Transparency in Differentially Private Text Rewriting

Timour Igamberdiev, Thomas Arnold and Ivan Habernal

This is a camera-ready version of the article accepted for publication at the 29th International
Conference on Computational Linguistics (COLING 2022). The ﬁnal ofﬁcial version will be published on
the ACL Anthology website later in 2022: https://aclanthology.org/

Please cite this pre-print version as follows.

@InProceedings{Igamberdiev.2022.COLING,

title = {DP-Rewrite: Towards Reproducibility and Transparency
in Differentially Private Text Rewriting},

author = {Igamberdiev, Timour and Arnold, Thomas and

Habernal, Ivan},

publisher = {International Committee on Computational

booktitle = {Proceedings of the 29th International Conference

Linguistics},

on Computational Linguistics},

pages = {(to appear)},
year = {2022},
address = {Gyeongju, Republic of Korea}

}

2
2
0
2

g
u
A
2
2

]
L
C
.
s
c
[

1
v
0
0
4
0
1
.
8
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
DP-Rewrite: Towards Reproducibility and Transparency in Differentially
Private Text Rewriting
Timour Igamberdiev1 and Thomas Arnold2 and Ivan Habernal1
1Trustworthy Human Language Technologies
2Ubiquitous Knowledge Processing Lab
Department of Computer Science, Technical University of Darmstadt
{timour.igamberdiev,ivan.habernal}@tu-darmstadt.de
arnold@ukp.informatik.tu-darmstadt.de

www.trusthlt.org

www.ukp.tu-darmstadt.de

Abstract

Text rewriting with differential privacy (DP)
provides concrete theoretical guarantees for
protecting the privacy of individuals in tex-
tual documents. In practice, existing systems
may lack the means to validate their privacy-
preserving claims,
leading to problems of
transparency and reproducibility. We intro-
duce DP-Rewrite, an open-source frame-
work for differentially private text rewriting
which aims to solve these problems by be-
ing modular, extensible, and highly customiz-
able. Our system incorporates a variety of
downstream datasets, models, pre-training pro-
cedures, and evaluation metrics to provide a
ﬂexible way to lead and validate private text
rewriting research. To demonstrate our soft-
ware in practice, we provide a set of experi-
ments as a case study on the ADePT DP text
rewriting system, detecting a privacy leak in
its pre-training approach. Our system is pub-
licly available, and we hope that it will help
the community to make DP text rewriting re-
search more accessible and transparent.

1

Introduction

Protecting the privacy of individuals has been
gaining attention in NLP. One particular setup is
text rewriting using local differential privacy (DP)
(Dwork and Roth, 2013), which provides proba-
bilistic guarantees of ‘how much’ privacy can be
lost in the worst case if an individual gives us their
piece of text that has been rewritten with DP. For
instance, given a text “I want to ﬂy from Newark
to Cleveland on Friday”, the system may rewrite
it as “Flights from Los Angeles to Houston this
week”. Only a few recent works have touched
on this challenging topic. For example, Krishna
et al. (2021) proposed ADePT: A text rewriting
system based on the Laplace mechanism. However,
it turned out that their DP method was formally
ﬂawed (Habernal, 2021). We also see another
recent approach, DP-VAE (Weggenmann et al.,

2022), which shows results that look surprisingly
good for the level of guaranteed privacy. How-
ever, neither ADePT nor DP-VAE published their
source codes, so the community has no means
to perform any empirical checks to validate the
privacy-preserving claims. Therefore, the lack of
transparency and reproducibility is the main ob-
stacle to the accountability of DP text-rewriting
systems.

We asked whether an open, modular, easily ex-
tensible, and highly customizable framework for
differentially private text rewriting could help the
community gain further insight into the utility and
potential pitfalls of such systems. We hypothesize
that by integrating various downstream datasets,
models, pre-training procedures, and evaluation
metrics into one software package, we improve the
transparency, accountability, and reproducibility of
research in differentially private text rewriting.

Our main contributions are twofold. First, this
demo paper presents DP-Rewrite, an open-
source framework for differentially private text
rewriting experiments. It includes a correct reimple-
mentation of ADePT as a baseline, integrates pre-
training on several datasets, and allows us to easily
perform downstream experiments with varying pri-
vacy guarantees by adjusting the privacy budget ε.
Second, DP-Rewrite allows us to easily detect
another privacy leak in the approach proposed in
ADePT, namely in the pre-training strategy of the
autoencoder, with the system memorizing the input
data. We demonstrate this in detail as a use case of
DP-Rewrite in Section 4.1

2 Related Work

Although the problem of simple data redaction is
a widely researched ﬁeld with several promising
approaches (Hill et al., 2016; Lison et al., 2021),

1Our project is available at https://github.com/

trusthlt/dp-rewrite.

the related problem of private text transformation
is still largely unexplored.

∈ X

We only brieﬂy sketch the main ideas of local
differentially private algorithms in text rewriting.
For a longer introduction to DP see, e.g., Haber-
nal (2022); Senge et al. (2021); Igamberdiev and
Habernal (2022). Let x, x(cid:48)
be two data points
such as texts or vectors, each belonging to a dif-
ferent person. In DP terminology, x and x(cid:48) are
neighboring datasets, as they differ by one person
(Desfontaines and Pejó, 2020). A (local) DP al-
is a function that takes any
gorithm
M
and produces its ‘priva-
single data point x
tized’ version y
which might be an arbitrary
object, such as a text or a vector. Privatization is
. As a
achieved by introducing randomness in
M
result, (ε, 0)-local DP guarantees that for any two
neighboring datasets x, x(cid:48) and any output y

X → Y

∈ X

∈ Y

:

ln

(cid:20) Pr(
Pr(

(x) = y)
(x(cid:48)) = y)

(cid:21)

ε

≤

(1)

M
M

where ε is the privacy budget; the lower, the better
privacy is guaranteed. If a text rewriting algorithm
satisﬁes the local DP, it limits the probability of re-
vealing the true text x after observing the privatized
text y.

Krishna et al. (2021) proposed ADePT, a DP text
rewriting system. It consists of an auto-encoder that
learns a compressed latent representation of text,
and a DP rewriter that uses the trained auto-encoder,
adds Laplace noise to the latent representation vec-
tor, and generates the privatized text. Due to a for-
mal error in the scale of the Laplace noise, ADePT
violated differential privacy (Habernal, 2021).

Bo et al. (2021) proposed a text rewriting ap-
proach that generates words from a latent repre-
sentation while adding DP noise. However, unlike
holistic text rewriting with DP, perturbing text only
at the word level does not protect against privacy
attacks (Mattern et al., 2022).

Even more current, Weggenmann et al.
(2022) proposed an end-to-end approach to text
anonymization using a DP autoencoder, claiming
to produce coherent texts of high privacy standards.
However, several key aspects of the experiments
lack a detailed description, while their results look
surprisingly good. Since the code base is not pub-
lic, we cannot reproduce or reimplement their ap-
proach, and we cannot prove or refute our suspi-
cions.

3 Description of software

The goal of our system is to provide a seamless way
to perform differentially private text rewriting, both
on existing and custom datasets. A user can either
load a dataset that we provide out-of-the-box, or
use a custom one. In addition, we want to make it
fast and convenient to run experiments for existing
methodologies in DP text rewriting (e.g. ADePT),
as well as the ability to integrate novel approaches.
For this, we have a general autoencoder class based
on which out-of-the-box and custom models are
built. In this sense, our software is designed to
be open and modular, where the researcher can
swap out existing components to run a variety of
experiments, as well as use the software for one’s
own privatized text rewriting needs.

The core architecture of our system can be seen
in Figure 1. We divide the experiments into three
distinct modes, pre-training, rewriting, and down-
stream. For all three, the pipeline begins with a dat-
aloader which can either be a dataset provided in
the framework, or a custom dataset speciﬁed by the
user. Additionally, a rewritten dataset can be loaded
for downstream experiments. The loaded dataset
is then preprocessed according to user-speciﬁed
parameters and the user’s selected model, split
into different procedures depending on the model
type (e.g. RNN-based, transformer-based). The
model is then initialized, optionally from an ex-
isting checkpoint. At this point, the main experi-
ment is run based on the speciﬁed mode, either (1)
pre-training the autoencoder, (2) using an existing
checkpoint to rewrite the dataset, or (3) running
a downstream model on an original or rewritten
dataset. For each mode, a variety of evaluations are
available, such as BLEU (Papineni et al., 2002) and
BERTScore (Zhang et al., 2019) for pre-training
and rewriting, and various classiﬁcation metrics
(e.g. F1 score) for downstream experiments. The
differential privacy component is incorporated dur-
ing the rewriting phase for systems such as ADePT,
although our framework also allows to incorporate
it during the pre-training stage.

4 Case study

We present here a case study that demonstrates
the process of using our framework and provides
insights into the ADePT system, for which we
provide an implementation in the software. Our
goal is to investigate the difference in rewritten
texts and downstream evaluations when we pre-

Figure 1: Overall structure of DP-Rewrite. Colors represent groupings of similar components. Blue: Experiment
mode. Grey: Dataset preparation. Green: Datasets (original/rewritten). Orange: Model-related components. Red:
Main experiment loop. Yellow: Additional experiment outputs.

train an autoencoder on one dataset and use this
to rewrite another dataset. If we notice a lot of
tokens from the dataset used for pre-training in the
rewritten dataset, as well as comparatively higher
downstream scores when pre-training and rewrit-
ing on the same dataset, then we can be certain of
another form of privacy leakage in ADePT.

4.1 Datasets

As in Krishna et al. (2021), we use the ATIS
(Dahl et al., 1994) and Snips (Coucke et al., 2018)
datasets to conduct experiments on an intent classi-
ﬁcation task in English. For both datasets, we use
the same train/validation/test split provided by Goo
et al. (2018), with 4,478 training, 500 validation
and 893 test examples for ATIS, and 13,084 train-
ing, 700 validation and 700 test examples for Snips.
There are a total of 26 intent labels in ATIS and 7
in Snips.

4.2

Implementation

We start our experiment pipeline by pre-training
two models, one on ATIS (1) and the other on
Snips (2), in both cases using the training split. For
pre-training, we set the vocabulary to the maxi-
mum number of words from the training set. As
in ADePT, we do not incorporate a differential pri-
vacy component during pre-training, although we
clip encoder hidden representations with a clipping
constant value of 5. We limit sequence lengths to a
maximum of 20 tokens, pre-training for 200 epochs
with a learning rate of 0.003. In contrast to ADePT,
we do not use the (cid:96)2 norm for clipping due to issues
in privacy guarantees outlined by Habernal (2021)
and instead follow the suggested ﬁx for the method
by clipping using the (cid:96)1 norm.

We then use these two models for rewriting,
applying both pre-trained models for rewriting
the training and validation partitions of ATIS and

PretrainDataloaderPreprocessorAutoencoderModelModelPretraining PretrainedModel ExperimentConfigs, Stats,Evaluations Load CheckpointSaveCheckpointSelected Model(e.g. ADePT)Original Dataset(Prepared/Custom)Different preprocessingprocedures depending onmodel type Initialize modelfor experiment DP ModuleRewriteDataloaderPreprocessorAutoencoderModelDatasetRewritingPretrainedModel ExperimentConfigs, Stats,Evaluations Load CheckpointSelected Model(e.g. ADePT)Original Dataset(Prepared/Custom)Different preprocessingprocedures depending onmodel type Initialize modelfor experiment RewrittenDataset Downstr.DataloaderPreprocessorDownstreamModel DownstreamTraining/Eval DownstreamResults, Configs,Stats RewrittenDataset Original Dataset(Prepared/Custom)Different preprocessingprocedures depending onmodel type Selected Model(e.g. BERT-downstream) Initialize modelfor experiment Snips, resulting in four rewriting settings in total.
For each setting, we rewrite using ﬁve ε values,
, 1000, 100, 10, and 1. We use the same clipping

∞
constant value of 5 as in pre-training.

See Appendix B for details of the downstream

experiment setup.

4.3 Results and analysis

Our results can be seen in Figure 2. We observe
the main patterns as follows. First and most im-
portantly, datasets rewritten using a model that was
pre-trained on the same dataset generally show bet-
ter downstream results than datasets rewritten using
a model pre-trained on a different dataset. For in-
stance, at ε = 1, 000, rewritten Snips from a model
pre-trained on Snips has an F1 score of 0.94, while
rewritten Snips from a model pre-trained on ATIS
has only 0.20. In fact, this is true even at ε =
∞
(non-private setting), without any added noise (e.g.
0.94 F1 pre-trained Snips, rewritten Snips vs. 0.18
F1 pre-trained ATIS, rewritten Snips), since for the
latter case the model ends up rewriting the dataset
that was pre-trained on, having memorized many of
its examples. This can be seen in Figure 3, where
the rewritten sentences appear to have no resem-
blance to the original dataset used for rewriting, but
are very similar to the data used for pre-training.

Next, as expected, the results decrease for all
conﬁgurations as the privacy budget ε decreases,
except for rewritten ATIS from a model pre-trained
on Snips, where results are low for all ε values,
probably due to the same reasons as shown in Fig-
ure 3. At the lower ε values of 10 and 1, perfor-
mance is very low for all conﬁgurations. Since
there is so much noise added to the encoder hidden
representations, the utility of ADePT’s rewriting is
severely diminished, for any data inputs.

∞

Finally, compared to running downstream exper-
iments on the original dataset, Snips rewritten with
a model pre-trained on Snips shows about the same
results at high ε values (e.g. 0.94 F1 pre-trained
Snips, rewritten Snips vs. 0.95 F1 original Snips
for ε =
). ATIS rewritten with a model pre-
trained on ATIS shows lower results in this case
(e.g. 0.73 F1 pre-trained ATIS, rewritten ATIS vs.
0.87 F1 original ATIS for ε =
). We speculate
that since ATIS is a smaller dataset, there are less
data points to effectively pre-train ADePT for the
autoencoding task. We additionally report random
and majority baselines in Appendix A on the origi-
nal datasets for comparison.

∞

We have thus shown that, despite ﬁxing the
theoretical privacy guarantees of ADePT, the pre-
training procedure still results in privacy leakage,
with rewritten datasets exposing a lot of informa-
tion from the dataset used for pre-training. As a
result, downstream performance is inﬂated if the
datasets for pre-training and rewriting are the same.

Figure 2: Downstream macro-averaged F1 results for
case study experiments with pre-trained and rewritten
Snips/ATIS datasets, as well as comparisons with re-
sults on the original datasets (“Original Snips” and
“Original ATIS”). Lower ε corresponds to better pri-
vacy.

Figure 3: Sample rewritten texts showing memoriza-
tion by ADePT model when pre-training and rewriting
on different datasets. For a given document in the origi-
nal dataset (“Original Snips/ATIS doc.”), its rewritten
version by the model (“Rewritten Snips/ATIS doc.”)
has no resemblance to it, but is very similar to another
document from the pre-trained dataset (“ATIS/Snips
doc. similar”).

5 Conclusion

We introduced DP-Rewrite, an open-source
framework for differentially private text rewriting
experiments. We have demonstrated a sample use-
case for our framework, which allows us to detect

∞1000100101Privacybudgetε0.20.40.60.81.0Downstr.TestF1OriginalSnipsOriginalATISPr.Snips,Rw.SnipsPr.ATIS,Rw.SnipsPr.Snips,Rw.ATISPr.ATIS,Rw.ATISSnips rewritten from ATISOriginal Snips doc.listen to westbam alumb allergic on google musicRewritten Snips doc.how many people fly on after a turboprop airportATIS doc. similarhow many people fly on a turboprop airportATIS rewritten from SnipsOriginal ATIS doc.what flights leave from phoenixRewritten ATIS doc.start playing my disney springSnips doc. similarstart playing my disney playlistprivacy leakage in the pre-training procedure of
the ADePT system, an example of how the modu-
lar and customizable nature of the software allows
for transparency and reproducibility in DP text-
rewriting research. DP-Rewrite is continuing
to be under active development, and we are incor-
porating new datasets and private text rewriting
methodologies as they are released. We welcome
feedback from the community.

Acknowledgements

The independent research group TrustHLT is sup-
ported by the Hessian Ministry of Higher Educa-
tion, Research, Science and the Arts. This project
was partly supported by the National Research Cen-
ter for Applied Cybersecurity ATHENE. Thanks
to Johannes Gaese and Jonas Rikowski who con-
tributed in the initial implementation phases.

References

Haohan Bo, Steven H. H. Ding, Benjamin C. M. Fung,
and Farkhund Iqbal. 2021. ER-AE: Differentially
private text generation for authorship anonymization.
In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 3997–4007, Online. Association for Compu-
tational Linguistics.

Alice Coucke, Alaa Saade, Adrien Ball, Théodore
Bluche, Alexandre Caulier, David Leroy, Clément
Doumouro, Thibault Gisselbrecht, Francesco Calta-
girone, Thibaut Lavril, et al. 2018. Snips voice plat-
form: an embedded spoken language understanding
system for private-by-design voice interfaces. arXiv
preprint arXiv:1805.10190.

Deborah A Dahl, Madeleine Bates, Michael K Brown,
William M Fisher, Kate Hunicke-Smith, David S Pal-
lett, Christine Pao, Alexander Rudnicky, and Eliza-
beth Shriberg. 1994. Expanding the scope of the atis
task: The atis-3 corpus. In Human Language Tech-
nology: Proceedings of a Workshop held at Plains-
boro, New Jersey, March 8-11, 1994.

Damien Desfontaines and Balázs Pejó. 2020. SoK: Dif-
ferential privacies. Proceedings on Privacy Enhanc-
ing Technologies, 2020(2):288–313.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Cynthia Dwork and Aaron Roth. 2013. The Algorith-
mic Foundations of Differential Privacy. Founda-
tions and Trends® in Theoretical Computer Science,
9(3-4):211–407.

Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li
Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-
Nung Chen. 2018. Slot-gated modeling for joint
slot ﬁlling and intent prediction. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 2 (Short Pa-
pers), pages 753–757.

Ivan Habernal. 2021. When differential privacy meets
NLP: The devil is in the detail. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1522–1528, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.

Ivan Habernal. 2022. How reparametrization trick
broke differentially-private text representation learn-
ing. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 771–777, Dublin, Ire-
land. Association for Computational Linguistics.

Steven Hill, Zhimin Zhou, Lawrence K Saul, and Ho-
vav Shacham. 2016. On the (in) effectiveness of mo-
saicing and blurring as tools for document redaction.
Proc. Priv. Enhancing Technol., 2016(4):403–417.

Timour Igamberdiev and Ivan Habernal. 2022. Privacy-
Preserving Graph Convolutional Networks for Text
Classiﬁcation. In Proceedings of the Language Re-
sources and Evaluation Conference, pages 338–350,
Marseille, France. European Language Resources
Association.

Satyapriya Krishna, Rahul Gupta, and Christophe
Dupuy. 2021. ADePT: Auto-encoder based differen-
tially private text transformation. In Proceedings of
the 16th Conference of the European Chapter of the
Association for Computational Linguistics: Main
Volume, pages 2435–2439, Online. Association for
Computational Linguistics.

Pierre Lison, Ildikó Pilán, David Sanchez, Montserrat
Batet, and Lilja Øvrelid. 2021. Anonymisation mod-
els for text data: State of the art, challenges and fu-
ture directions. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 4188–4203, Online. Association for
Computational Linguistics.

Justus Mattern, Benjamin Weggenmann, and Florian
Kerschbaum. 2022. The Limits of Word Level Dif-
ferential Privacy. arXiv preprint.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting of the Association for Compu-
tational Linguistics, pages 311–318.

Manuel Senge, Timour Igamberdiev, and Ivan Haber-
nal. 2021. One size does not ﬁt all: Investigating

strategies for differentially-private learning across
NLP tasks. arXiv preprint.

Justus Mattern,

Benjamin Weggenmann, Valentin Rublack, Michael
Andrejczuk,
and Florian Ker-
schbaum. 2022. DP-VAE: Human-Readable Text
Anonymization for Online Reviews with Differen-
tially Private Variational Autoencoders. In Proceed-
ings of the ACM Web Conference 2022, pages 721–
731, Virtual Event. ACM.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, and Yoav Artzi. 2019. Bertscore: Eval-
arXiv preprint
uating text generation with bert.
arXiv:1904.09675.

A Detailed results of the case study

Pretr. Dat. Rewr. Dat.

ε Test F1

Snips
Snips
Snips
Snips
Snips
Snips
Snips
Snips
Snips
Snips
ATIS
ATIS
ATIS
ATIS
ATIS
ATIS
ATIS
ATIS
ATIS
ATIS

Snips
Snips
Snips
Snips
Snips
ATIS
ATIS
ATIS
ATIS
ATIS
Snips
Snips
Snips
Snips
Snips
ATIS
ATIS
ATIS
ATIS
ATIS

Snips Orig.
ATIS Orig.
Snips Rand.
ATIS Rand.
Snips Maj.
ATIS Maj.

∞
1,000
100
10
1

∞
1,000
100
10
1

∞
1,000
100
10
1

∞
1,000
100
10
1

0.94 (0.02)
0.94 (0.02)
0.91 (0.02)
0.07 (0.01)
0.06 (0.00)
0.18 (0.07)
0.20 (0.02)
0.19 (0.01)
0.06 (0.01)
0.06 (0.01)
0.51 (0.01)
0.52 (0.03)
0.52 (0.03)
0.50 (0.01)
0.50 (0.01)
0.73 (0.06)
0.68 (0.09)
0.62 (0.03)
0.50 (0.01)
0.50 (0.01)

0.95 (0.01)
0.87 (0.03)
0.14
0.01
0.03
0.13

Table 1: Downstream macro-averaged F1 results for
case study experiments with pre-trained and rewritten
Snips/ATIS datasets. We additionally show results on
the original datasets, as well as random and major-
ity baselines. Test F1 shown as “mean (standard de-
viation)” over ﬁve runs with different random seeds.
Lower ε corresponds to better privacy.

B Downstream experiment setup

For downstream experiments, we use a pre-trained
BERT model (Devlin et al., 2018), with an addi-
tional feedforward layer that takes the mean of the
last hidden states as input and predicts the output
label. We use the rewritten training and validation
sets for each conﬁguration, and the original test
sets for ﬁnal evaluation. We run each conﬁgura-
tion with ﬁve different random seeds and report the
mean and standard deviation.

