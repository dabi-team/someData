2
2
0
2

r
p
A
9
1

]

G
L
.
s
c
[

1
v
6
3
8
8
0
.
4
0
2
2
:
v
i
X
r
a

ML Evaluation Standards Workshop at ICLR 2022

SYSTEM ANALYSIS FOR RESPONSIBLE DESIGN OF
MODERN AI/ML SYSTEMS∗

Virginia H. Goodwin, Rajmonda S. Caceres
MIT Lincoln Laboratory
{virginia.goodwin,rajmonda.caceres}@ll.mit.edu

ABSTRACT

The irresponsible use of ML algorithms in practical settings has received a lot of
deserved attention in the recent years. We posit that the traditional system anal-
ysis perspective is needed when designing and implementing ML algorithms and
systems. Such perspective can provide a formal way for evaluating and enabling
responsible ML practices. In this paper, we review components of the System
Analysis methodology and highlight how they connect and enable responsible
practices of ML design.

1

INTRODUCTION

In recent years the Machine Learning (ML) community has begun to address serious issues with
lack of responsible use of ML algorithms Lazovich et al. (2022); Molnar; Ratner & et al. (2019).
Researchers are starting to address the utility and possible overuse or misuse of standard ML
challenge data sets, such as PASCAL VOC or ImageNet in the computer vision (CV) domain,
and GLUE/SuperGLUE in Natural Language Processing (NLP) Raji et al. (2021). Too frequently,
these common and restricted benchmark data sets are taken as the end-all-be-all of algorithm per-
formance measurement, and yet our experience deploying ML algorithms in ﬁelded systems has
shown time and time again that this method leaves a lot to be desired in the context of real-
world performance on novel data. We observe that even the algorithms that achieve best in class
performance against the benchmark data do not necessarily result in sufﬁcient performance in
novel environments. As Green says, Green (2019) “...
although most people talk about ma-
chine learning’s ability to predict the future, what it really does is predict the past.” Formalism
and mitigation approaches have been offered for speciﬁc problems and algorithms (e.g., classi-
ﬁcation) Brown et al. (2021); Katz et al. (2017); Kleinberg et al. (2016); Lundberg & Lee (2017);
Suresh & Guttag (2019); Suresh et al. (2021). However, these are currently only applicable in very
limited circumstances.

We believe that taking a traditional Systems Analysis approach to ML assessment offers a coherent
pathway out of this problem, which will result in better outcomes for both algorithm development
and performance in real-world systems. Current ML system development too often overlooks a
critical question addressed by Systems Analysis, i.e., what are the real-world costs of false alarms
and missed detections? This includes costs to the operating entity, such as the cost of missing
an important detection and the cost of responding to false alarms, but as we have already seen
too many times, ethical developers and users of ML systems must also consider the costs to the
subjects of the ML system. Already we have seen systems deployed for operational use with no
accounting for how false alarms in particular will impact people’s lives. When the ML system is
making decisions about who deserves an interview Dastin (2018), where police should spend more
∗DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited. This material is
based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force
Contract No. FA8702-15-D-0001. Any opinions, ﬁndings, conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily reﬂect the views of the Under Secretary of Defense for
Research and Engineering. © 2021 Massachusetts Institute of Technology. Delivered to the U.S. Government
with Unlimited Rights, as deﬁned in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any
copyright notice, U.S. Government rights in this work are deﬁned by DFARS 252.227-7013 or DFARS 252.227-
7014 as detailed above. Use of this work other than as speciﬁcally authorized by the U.S. Government may
violate any copyrights that exist in this work.

1

 
 
 
 
 
 
ML Evaluation Standards Workshop at ICLR 2022

time patrolling Green (2019), identifying and even predicting criminal behavior Hao (2019), and
making sentencing recommendations Heaven (2020), the cost to the individuals impacted by false
alarms is profound. “Move fast and break things” is a terrible and immoral operating philosophy
when the “things” being broken are people’s lives.

In this paper, we ﬁrst deﬁne Systems Analysis; we then go on to draw parallels between the main
points of Systems Analysis and how they can apply speciﬁcally to ML systems.

2 WHAT IS SYSTEM ANALYSIS?

Historically, the practice of Systems Analysis is closely linked with that of Systems Engineering.
Systems Engineering began at Bell Labs in the early 1900s and became a signiﬁcant ﬁeld in its own
right during the industrialization driven by WWII Buede (2016). RAND Corporation is also credited
with the initiation of Systems Analysis as a separate ﬁeld in the ﬁfties Buede (2016), although
systems analysis was being done as part of systems engineering earlier.

Systems analysis is often a squishy topic, as it is hard to generalize across a wide array of types of
systems, including hardware, software, human-in-the-loop (HITL) processes, geospatial and tempo-
ral processes. For the purposes of this discussion we break Systems Analysis down into four main
steps:

1. Scoping the Problem Space. This ﬁrst step includes understanding both the technical challenges
and also the full environment and constraints of who is going to be using the ﬁnal system. This
should include understanding the concept of operations (CONOPS), which is, how the operator will
use the system, what rules circumscribe their choices and actions, what response resources they
have, and the cost of mistakes. This is ideally where metrics of performance and success should be
deﬁned. The most common metrics for the system are the false alarm and missed detection rates
that are considered tolerable by the operator. Scoping and properly deﬁning the problem space is
typically the bulk of systems analysis work.

2. Scoping the Solution Space. This second step involves understanding or specifying out what
possible solutions exist, frequently termed “the art of the possible” Delaney (2015). This may in-
clude looking at different hardware and sensor options, as well as understanding what constraints
are placed on your solutions by the operator’s available responses. This may also include looking
at HITL trade-offs, such as whether a user will be available to provide input or if the system must
perform independently.

3. Creating Candidate Solutions. Once we understand the range of possible solutions to a prob-
lem, the next step is to build one or more solutions. There are inevitably challenges that must be
overcome in making something actually work that aren’t addressed in the higher-level scoping of
the solutions space. Furthermore, once a prototype exists, the human element (CONOPS, response,
etc.) inevitably changes when the theorized CONOPS don’t work as well as expected, or new ways
to use the system are discovered.

4. Assessing Performance. Here we circle back to the metrics of performance speciﬁed in Step 1,
and assess how well the candidate solution performs relative to those metrics. These should include
not just technical performance, but how well the system improves operator performance, what are
the cost trade-offs of mistakes (both false alarms and missed detections) and also what unintentional
effects does the system have.

As ML algorithm development is currently practiced, many algorithms are published, and subse-
quently reused in a multitude of different contexts, where the basis of algorithm training, and/or the
metrics of performance, are all built on a published benchmark data set. In these instances, develop-
ers are a priori limiting themselves in both Items 1 and 4 of the above list. They are not adequately
scoping the problem space, because that space may not be properly captured in the pre-collected
and cleaned benchmark data set, and if they simply rely on the traditional benchmark performance
metrics, they are not looking at whole-system performance. We believe that if the above rubric is
used from the beginning of algorithm development, this will result in better outcomes for system
performance in ﬁnal systems.

2

ML Evaluation Standards Workshop at ICLR 2022

3 PARALLELS TO AI/ML SYSTEMS

In this section we take a closer look at each step in Systems Analysis, and make connections to the
machine learning process.

3.1 SCOPING THE PROBLEM SPACE

As noted above, the vast majority of Systems Analysis is performed in Step 1. We can unpack this
into the following pieces: 1) Input data and associated technical challenges, 2) Output results and
associated costs, 3) Constraints on the system.

Input Data and Associated Technical Challenges. Whereas most ML algorithms rely on published
data sets to train and against which to demonstrate performance, deployed systems are ingesting data
“in the wild”, where there is typically lots of noise as well as larger data distribution issues, such
as uneven class distribution, or distributions may shift over time, as the environment changes. A
deep understanding to the technical underpinnings of the data to be analyzed is critical to identify
possible solutions, and possible sources of error. In addition to the details of the collected data, there
are typically technical challenges related to processing power, input data resolution or quality, and
myriad data processing steps that need to be addressed, such as the raw data from a sensor may be
very far from what is expected as input to the ML system. Each of these steps may introduce some
bias or error in the process that needs to be understood.

Output Results and Associated Costs. Here we need to understand the system-level output that
is required, whether it is an alert to an operator, a classiﬁcation result, an image, etc., and most
importantly, how those results are acted on, and what are the costs associated with errors to the
system-level outputs. The cost of false alarms and missed detections is possibly the most critical
piece of the full Systems Analysis. In most systems with an HITL component, operators like to
think about False Alarm Rates and Missed Detection Rates, i.e. how many false alarms and missed
detections can the operator tolerate per hour, per day, etc. Critically, in addition to the costs to the
operator of responding to false alarms and not responding to missed detections, there is the cost to
the subject of the false alarm or missed detection. For example, if the response to a false alarm
is a law enforcement intervention, there is a critical cost to the person who is being erroneously
targeted by the response team. For emerging ML-based systems, these costs are only beginning to
be understood and considered.

Constraints on the System. Closely related to the costs of false alarms and missed detections,
there are typically constraints on the system, both technical and operational. The ML community is
familiar with technical constraints such as processing power, data quality, data transmission rates,
etc. On top of that, for an operational system, we must also consider operational constraints such as
human resources, laws, rules, and social norms, and other “external” system constraints. In practice,
many technically functional systems don’t get used because they break the operational constraints
of the whole system.

It is critical to clearly deﬁne the full scope of the problem space before we embark on proposing a
solution. Details that will emerge in scoping the solution space, such as choosing a speciﬁc algo-
rithm or learning objective, creating and assessing performance metrics, and data preprocessing and
handling, will all have an impact on the performance of the ﬁnal system.

3.2 SCOPING THE SOLUTION SPACE

Scoping the solution space is essentially doing a survey of existing and potential future technologies,
and understanding how various components might be put together to build out a system that will
address the user needs. In the context of ML, given the planned input data per Section 3.1, there
may be various different ML algorithms that would all result in the desired output, or potentially
multiple algorithms may be used in ensemble to get to the ﬁnal desired result. Frequently, the
type or quality of available input data, and other system-level constraints, will restrict the space of
potential solutions. The challenge here in the context of the whole system is understanding how
error or bias may be introduced ﬁrst from the input data, then potentially propagated and magniﬁed
through the processing of the ML algorithms.

3

ML Evaluation Standards Workshop at ICLR 2022

Additionally, as part of scoping out potential solutions, there may be various different options for
exactly how the output results are generated or presented. For example, classiﬁcation system may
output different classes with associated conﬁdence scores, and then there are multiple options at the
system level to assist an operator in making a decision. The system can either present the top N
results if there is an operator available to choose among the results, or the system can assume less
human resources and output the top single classiﬁcation as the deﬁnitive decision. These types of
performance trade-offs are a large part of the the cost-beneﬁt balancing necessary for developing
useful systems.

3.3 CREATE ONE OR MORE CANDIDATE SOLUTIONS

Based on the outcome of Steps 1 and 2, one or more potential solutions may be proposed. For ex-
ample, we may look at a more computationally-intensive solution that gives better performance, vs.
a lighter-weight solution that gives slightly worse performance but at lower computational cost, and
we may seek to mitigate performance shortfalls at another level of the system, such as by presenting
the operator with multiple choices vs. one choice.

The challenge in this stage is to rigorously understand and assess the performance of each system
component, while never losing sight of the goals of the full system.

3.4 ASSESSING SYSTEM PERFORMANCE

Finally, here we circle back to Step 1, where we discussed the overall system performance metrics
and costs. It is critical to assess the full system performance that an operator will experience, and
then trace the performance back to individual component performance. This whole process is usually
iterative, where we get a ﬁrst base-line performance result and then go into the system, perhaps
even re-architecting to include more or different algorithms to achieve the desired overall system
performance. Here we can also make trade-offs of requiring more HITL time (if available) to offset
algorithm performance.

3.5 SYSTEMS ANALYSIS WITHOUT A SYSTEM

While all of the preceding has rested on the presumption that a system exists, that operators of the
system exist, and that developers can understand all relevant aspects of the above, it is also true
that ML systems may be developed in the absence of an existing system from which to glean these
critical details. In the absence of knowing a priori how an algorithm will be deployed, the alternative
solution is documentation. It is imperative that the developers rigorously document the training data
and trained model that they create, including both the intended use cases and inappropriate or out of
scope applications. As noted in Dunnmon et al. (2021), the use of rigorous documentation such as
Data Sheets Gebru et al. (2020) and Model Cards Mitchell et al. (2019) is not only best practice for
documentation, but also beneﬁts the ﬁnal system by eliciting critical questions and issues early in the
design process. In addition, it is critical to create a measurement framework Holstein et al. (2019)
by identifying points within the system at which results can be assessed at different levels (data and
sensing, algorithmic design, human-AI interaction, and system/application level), and create ‘hooks’
in the software to enable continuous or periodic reassessment of performance over time.

4 CONCLUSION

While the additional work required to do systems analysis is non-negligible, we believe that the
beneﬁts of doing this level of analysis at each stage of an AI/ML system development are quite
signiﬁcant to the ﬁnal performance of the system. Many people in the ML community are deeply
concerned about the misuse of ML systems in real-world settings with real costs to individuals and
society. Engaging in systems analysis from the very start of the effort can call out and mitigate
against these harms, in particular by making explicit correct and incorrect use of available data,
as well as external constraints and costs associated with such a system. There is also beneﬁt in
clearly documenting decision-making responsibility and CONOPS between the ML algorithm and
human users of the system to clarify accountability and ownership. In many cases we recognize that
ML algorithm development is divorced from a future real-world system, and therefore full system

4

ML Evaluation Standards Workshop at ICLR 2022

metrics cannot be evaluated. In these cases, we believe that rigorous documentation of the data and
model training can bridge the gap, such that future developers are able to make informed decisions
regarding how appropriate it is to apply the data or model to their speciﬁc problem.

REFERENCES

O. M. Brown, A. B. Curtis,

Principles
uation
2021.
https://arxiv.org/ftp/arxiv/papers/2107/2107.02868.pdf.

J. A. Goodwin.

ai/ml model

performance

robustness.

and

and

of

for

eval-
URL

Dennis Buede. The engineering design of systems: Models and methods. 2016.

Jeffrey

Dastin.
bias

Amazon

scraps

secret

ai
Reuters,

recruiting
2018.

tool

that
URL

showed
https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G.

women.

against

William Delaney. Perspectives on Defense Systems Analysis. MIT Press, 2015.

Jared Dunnmon,

Bryce Goodman,
drea Van Deusen.
Responsible
https://www.diu.mil/responsible-ai-guidelines.

Peter Kirechu,
ai guidelines

Carol
in practice.

Smith,

and Alexan-
URL

2021.

Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,

Hal Daum´e III au2, and Kate Crawford. Datasheets for datasets, 2020.

Ben Green.

The just city: Machine learning’s social and political foundations. MIT
https://doi.org/10.7551/mitpress/11555.001.
URL https://smartenoughcity.mitpress.mit.edu/pub/vmjl8djz.

Press, 2019.
0001.
https://smartenoughcity.mitpress.mit.edu/pub/vmjl8djz.

ISBN 9780262352246.

doi:

Karen Hao. Ai is sending people to jail—and getting it wrong. MIT Technology Review, 2019. URL

https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/.

Will

Douglas Heaven.
be

Predictive
MIT

policing
Technology

algorithms
Review,

are
2020.

racist.

they
URL

need
https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/.

dismantled.

to

Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daum´e, Miro Dudik, and Hanna Wal-
Proc. of CHI Conf. on Hu-
doi: 10.1145/3290605.3300830. URL

lach.
man Factors in Computing Systems, May 2019.
http://dx.doi.org/10.1145/3290605.3300830.

Improving fairness in machine learning systems.

Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: An
efﬁcient SMT solver for verifying deep neural networks. CoRR, abs/1702.01135, 2017. URL
http://arxiv.org/abs/1702.01135.

Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determi-

nation of risk scores. arXiv preprint arXiv:1609.05807, 2016.

Tomo Lazovich, Luca Belli, Aaron Gonzales, Amanda Bower, Uthaipon Tantipongpipat, Kristian
Lum, Ferenc Huszar, and Rumman Chowdhury. Measuring disparate outcomes of content rec-
ommendation algorithms with distributional inequality metrics, 2022.

Scott Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions, 2017.

Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,
Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. Proc.
of the Conf. on Fairness, Accountability, and Transparency, Jan 2019. doi: 10.1145/3287560.
3287596. URL http://dx.doi.org/10.1145/3287560.3287596.

Christoph Molnar.

solving
https://twitter.com/ChristophMolnar/status/1485549716268109824.

A lot of machine learning research has detached itself
own

“benchmark-islands”.

problems,

created

their

real

and

from
URL

5

ML Evaluation Standards Workshop at ICLR 2022

Inioluwa Deborah Raji, Emily Denton, Emily M. Bender, Alex Hanna, and Amandalynne Paullada.
AI and the everything in the whole wide world benchmark. In Neurips, Datasets and Benchmarks
Track (Round 2), 2021. URL https://openreview.net/forum?id=j6NxpQbREA1.

Alexander Ratner and et al. Sysml: The new frontier of machine learning systems. CoRR,

abs/1904.03257, 2019. URL http://arxiv.org/abs/1904.03257.

Harini Suresh and John V Guttag. A framework for understanding unintended consequences of

machine learning. arXiv preprint arXiv:1901.10002, 2019.

Harini Suresh, Steven R Gomez, Kevin K Nam, and Arvind Satyanarayan. Beyond expertise and
roles: A framework to characterize the stakeholders of interpretable machine learning and their
needs. arXiv preprint arXiv:2101.09824, 2021.

6

