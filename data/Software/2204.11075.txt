Smart App Attack: Hacking Deep Learning Models
in Android Apps

Yujin Huang, Chunyang Chen*

1

2
2
0
2

r
p
A
3
2

]

G
L
.
s
c
[

1
v
5
7
0
1
1
.
4
0
2
2
:
v
i
X
r
a

Abstract—On-device deep learning is rapidly gaining popular-
ity in mobile applications. Compared to ofﬂoading deep learning
from smartphones to the cloud, on-device deep learning enables
ofﬂine model inference while preserving user privacy. However,
such mechanisms inevitably store models on users’ smartphones
and may invite adversarial attacks as they are accessible to
attackers. Due to the characteristic of the on-device model, most
existing adversarial attacks cannot be directly applied for on-
device models. In this paper, we introduce a grey-box adversarial
attack framework to hack on-device models by crafting highly
similar binary classiﬁcation models based on identiﬁed transfer
learning approaches and pre-trained models from TensorFlow
Hub. We evaluate the attack effectiveness and generality in
terms of four different settings including pre-trained models,
datasets, transfer learning approaches and adversarial attack
algorithms. The results demonstrate that the proposed attacks
remain effective regardless of different settings, and signiﬁcantly
outperform state-of-the-art baselines. We further conduct an
empirical study on real-world deep learning mobile apps collected
from Google Play. Among 53 apps adopting transfer learning,
we ﬁnd that 71.7% of them can be successfully attacked, which
includes popular ones in medicine, automation, and ﬁnance
categories with critical usage scenarios. The results call for the
awareness and actions of deep learning mobile app developers to
secure the on-device models. The code of this work is available
at https://github.com/Jinxhy/SmartAppAttack.

Index Terms—Deep learning, adversarial attack, mobile appli-

cation, security.

I. INTRODUCTION

D EEP learning has already been proven powerful in sev-

eral ﬁelds, including object detection, natural language
question answering, and speech recognition. Meanwhile, mo-
bile apps are making people’s life much convenient including
reading, chatting, banking, etc. To make mobile apps really
”smart”, many development teams have been adopting deep
learning to equip their apps with artiﬁcial intelligence fea-
tures, such as image classiﬁcation, face recognition, and voice
assistant. There are two ways to host deep learning models for
mobile apps, i.e., on cloud or on device within the app.

Compared to performing deep learning on cloud, on-device
deep learning offers several unique beneﬁts as follows. First,
it avoids sending private users’ data to the cloud, leading
to the bandwidth saving, inference accelerating, and privacy
preserving [1]. Second, apps can run in any situation with no
need for internet connectivity [2]. Also considering the rapidly
increasing computing power of mobile phones, more and

Y. Huang is with the Faculty of Information Technology, Monash Univer-
sity, Melbourne, Victoria 3800, Australia (e-mail: yujin.huang@monash.edu).
C. Chen is with the Faculty of Information Technology, Monash University,
Melbourne, Victoria 3800, Australia (e-mail: chunyang.chen@monash.edu).

*corresponding author

more deep learning models are directly deployed to end-users’
devices [3]. On account of the beneﬁts of embedding deep
learning into mobile apps, mainstream deep learning frame-
works also roll out their corresponding on-device versions such
as TensorFlow Lite (TFLite) [4] and Pytorch Mobile [5].

However, emerging adversarial attacks have exposed great
threats to neural networks [6], which alters input instances
in the form of small perturbations that remain imperceptible
to the human, but fool neural network classiﬁers in making
incorrect predictions. There are many different types of ad-
versarial attacks including white-box (e.g., Fast Gradient Sign
Method [7] and Carlini and Wagner [8] attacks) and black-
box ones (e.g., Zeroth Order Optimization [9] and Bandits
and Priors [10] attacks). White-box attacks require attackers
to have full access to the targeted model, whereas black-
box attacks require only queries to the targeted model that
may return complete or limited information. In general, black-
box attacks are more challenging and expensive compared to
white-box ones. Most adversarial attacks are targeting at on-
cloud deep learning models [11], [12], while very few of them
can be applied for attacking on-device models. In view of
the fact that many mobile apps supported by deep learning
models are used for vital tasks such as ﬁnancial, social or even
life-critical tasks like medical diagnosis, driving assistant, face
recognition, attacking the models inside those apps will post
a severe threat to end-users.

Although there are some works investigating the on-device
model attack or defense including stealing model [2], adding
backdoor [13] or testing model robustness [1], most of them
are empirical studies and merely provide universal methods
to attack on-device models without taking the speciﬁc model
characteristics (i.e., model structure and parameters) into ac-
count. This leads to poor attack performance when on-device
models adopt different model structures and tuning strategies.
In contrast, we are proposing a simple but effective attack
to on-device models by taking their ofﬂine characteristics
into consideration. Different from cloud-based deep learning
models, hackers can obtain the deep learning model with
explicit structural and parametric information. But note that
it
is not fully white-box, as creators of on-device model
frameworks noticed that security concern and take actions to
prevent that leakage. For example, TFLite (the most popular
on-device framework [1], [3]) model is speciﬁed as irreversible
to its trainable format in TensorFlow1. That is why we cannot
directly apply existing white-box adversarial attacks. Although

1Previous ofﬁcial reverse tool https://github.com/tensorﬂow/tensorﬂow/tree/

r1.9/tensorﬂow/contrib/lite has been obsolete since TensorFlow 1.9

 
 
 
 
 
 
we can enumerate a list of rules for manual conversion, it is not
general and easy to be out of date as there may be customized
operations deﬁned by developers and multiple deep learning
frameworks [14]. Hence, the attack to the on-device model
can be regarded as a grey-box attack and we propose a way
to bypass that limitation so that a white-box attack can be
feasible.

In this paper, we propose a grey-box adversarial attack
framework, which is designed speciﬁcally to hack the on-
device models that adopt transfer learning. Given a mobile
app, we ﬁrst extract the deep learning model inside by reverse-
engineering the app. We then check if the model is a ﬁne-
tuned one based on existing pre-trained models on public
repositories (e.g., Tensorﬂow Hub [15], Model Zoo [16]). By
identifying its pre-trained model and corresponding transfer
learning approach, we obtain a trainable and similar white-box
model. For a speciﬁc targeted class, we then ﬁne-tune a binary
adversarial model that shares similar features (i.e., the features
associated with the targeted class) with the victim on-device
model. Finally, we craft adversarial examples based on the
binary adversarial model using existing white-box adversarial
attack approaches to perform attacks.

To evaluate the attack effectiveness and generality of our
framework, we conduct experiments with different settings,
including pre-trained models, datasets, transfer learning ap-
proaches and adversarial attack algorithms. These experiments
independently test the impact of each perspective on attack
performance. The results show that our attacks remain effec-
tive regardless of different settings, with more than 2 times
the success rate compared with the previous attack [1]. To
further examine the feasibility of our framework on real-
world mobile apps, we launch the attack on 53 deep learning
mobile apps that adopt transfer learning. Of these apps, 71.7%
can be successfully attacked, which includes popular ones
in medicine, automation, and ﬁnance categories with critical
usage scenarios such as skin cancer recognition, trafﬁc sign
detection, etc.

We issue this new attack to raise the awareness of the
community for protecting the on-device models. This paper
makes the following research contributions:

• We propose a grey-box adversarial attack framework against
on-device models that adopt transfer learning. It is a com-
plete, practical and scalable pipeline including model extrac-
tion, pre-trained model identiﬁcation and model ﬁne-tuning
with the consideration of the model-speciﬁc characteristics,
followed by an adversarial attack. Such a mechanism en-
ables attackers to construct tailored adversarial examples
for real-world on-device models, which remain effective
regardless of various model structures and transfer learning
strategies.

• We conduct an extensive evaluation on its performance and
generality in different settings. We also carry out a study on
real-world deep learning mobile apps crawled from Google
Play and succeed in 38 of them.

• We provide justiﬁcation for the generality and scalability
of such attacks and discuss potential countermeasures. This
analysis suggests that improving the robustness of ﬁne-tuned

2

models is necessary for developing deep learning mobile
apps, pointing to several promising research directions.

II. BACKGROUND

A. On-device Deep Learning Model

Currently, deep learning powered mobile apps can be
roughly categorized into two ways: cloud-based inference and
on-device inference. The key difference between these two
architectures is the storage location of the deep learning model.
In cloud-based deep learning models, mobile devices need
to send requests to a cloud server and retrieve the inference
results. Ofﬂoading inference execution to the cloud comes
with multiple drawbacks such as data privacy concern, unre-
liable network conditions, and high latency. In comparison,
on-device deep learning models avoid the aforementioned
drawbacks of the cloud-based approach. It performs real-time
inference on smartphones without network connection, and
rarely requires sending user data off the device.

The implementation of on-device deep learning models is
often with the aid of deep learning frameworks, such as Google
TensorFlow (TF) and TFLite [4], Facebook PyTorch [17] and
Pytorch Mobile [5], and Apple Core ML [18]. Among these
frameworks, TFLite is the most popular technology used for
running deep learning models on mobile, embedded, and IoT
devices. It contributes nearly half of the deep learning mobile
apps in the past two years and its usage is growing more
signiﬁcantly than other frameworks [1], [3]. Although existing
deep learning frameworks reduce the engineering efforts of
implementing on-device deep learning models, training a new
deep learning model from scratch is still expensive. Hence,
pre-trained deep learning models and transfer learning tech-
niques are usually utilized in deep learning mobile apps to
reduce training costs [1], [19]. This allows mobile developers
to leverage the representations learned by a pre-trained net-
work and ﬁne-tune them to a speciﬁc task.

B. Transfer Learning

Transfer learning is proposed to transfer the knowledge from
a pre-trained model to a new model (i.e., ﬁne-tuned model)
that performs a different task. This knowledge normally refers
to the model structure and parameters afﬁliated to the layers.
By leveraging existing knowledge, transfer learning enables
researchers or industries to speed up the development of the
new models even when their learning tasks are different from
the pre-trained models.

Fig. 1: Transfer learning.

Pre-trained ModelResulting Fine-tuned ModelFine-tuned Model constructionLayer trained by Fine-tuned ModelLayer of Pre-trained ModelLayer newly added for specific task Frozen Layers (F)Retrained Layers (N-F)OutputOutputOutputInputInputInput3

Fig. 2: The overall workﬂow of the attack framework.

Figure 1 illustrates the process of transfer learning at a high
level. During initialization, a pre-trained model with N layers
is selected as the base model. The ﬁne-tuned model copies
the ﬁrst N − 1 layers (i.e., both the structure and parameters)
of the base model and add a new fully-connected layer that
is tailored to ﬁt the new task. Subsequently, the ﬁne-tuned
model trains the last N − F layers without updating the ﬁrst
F layers (i.e., frozen layers) based on its own dataset. This
allows the ﬁne-tuned model to exploit learned feature maps
of the pre-trained model to extract meaningful features from
new samples and thus lowers the training cost. Depending on
the number of frozen layers (F ) during the training, transfer
learning approaches can be categorized as follows:
• Feature Extraction. The ﬁne-tuned model freezes the ﬁrst
F layers of pre-trained model, where F = N − 1, and only
updates the parameters of the newly-added classiﬁer layer
during the training. This approach is suitable when the task
domains of both the ﬁne-tuned and pre-trained models are
similar, and the new training dataset is small.

• Fine-Tuning. Instead of training only the last layers of the
ﬁne-tuned model, this approach trains the top-level layers of
the pre-trained model alongside the training of the newly-
added classiﬁer. In this case, the frozen layers F is less than
N −1, which allows the parameters to be tuned from generic
feature maps to features associated speciﬁcally with the new
dataset. Fine-Tuning often outperforms Feature Extraction
when the ﬁne-tuned model’s objective is different than the
original task of the pre-trained model, and the new training
dataset is large.

C. Adversarial Attacks in Deep Learning

Adversarial attacks modify the original images with subtle
perturbations, which can mislead the deep learning model to
make incorrect predictions while hardly be distinguished by
human eyes. Based on the attacker’s knowledge of the targeted
classiﬁer, existing attacks for crafting adversarial images fall
into two categories.

White-box Attacks. In the white-box setting, the attacker
is assumed to fully know the targeted model, including both
the structure and parameters [6]. This type of attack [6], [20],
[21] allows the attacker to directly compute the perturbations
that change the model prediction using gradient ascent and
often achieve nearly 100% attack success rate with minimal
perturbations. However, the inaccessibility of models in real-
world applications makes the white-box attack impractical.

Black-box Attacks. In the black-box setting, the attacker
does not know the targeted model (i.e., structure and parame-
ters) and only can perform queries on it [22]. Some black-
box attacks attempt to estimate decision boundaries of the
targeted model using its outputs then construct a substitute
model for generating adversarial
images [23], [24]. Other
attacks execute queries on the targeted model to evaluate the
success of adversarial images and improve them accordingly
[25], [26]. Compared to the white-box scenario, the black-
box attack seems more realistic, however, it typically has the
additional constraint on query budget, which lowers the attack
effectiveness [25].

III. ATTACK TO ON-DEVICE MODELS

In this section, we present a grey-box adversarial attack
framework speciﬁcally for hacking on-device models. The
framework ﬁrst performs model extraction to a mobile app
with a deep learning model and check if it is a ﬁne-tuned
model. By identifying its pre-trained model and corresponding
transfer learning approach, the attacker can build a binary
classiﬁcation model (i.e., adversarial model) against the tar-
geted model (i.e., ﬁne-tuned model) to craft adversarial images
to misclassify a speciﬁcally
that fool
targeted class. The overall workﬂow of our attack framework
is depicted in Figure 2 and the details of training Grey-box
Attack Model are depicted in Figure 4.

the targeted model

A. Extracting On-device Models

including resource ﬁles,

into nearly original form,

Since our attack framework is speciﬁcally for on-device
TFLite models, to obtain targeted models from a mobile app
(i.e., Android app), we ﬁrst examine whether the mobile app
adopts TFLite models, and if so, extract the models from it.
Given an Android APK, we utilize Apktool [27] to decompile
it
.dex
ﬁles, manifest ﬁles, etc. After decompiling, we check whether
the decomposed APK comprises ﬁles with the TFLite model
naming schemes [2], if any such exist, extract model ﬁles from
it. During extraction, each extracted model’s completeness and
operability are checked by loading the model and performing
inference on randomly generated data. This is vital for our
attack as determining whether a targeted model is a ﬁne-tuned
model requires its full information (i.e., model structure and
parameters). In addition, the availability of targeted models is
essential to perform the attack.

TensorFlow HubSet of Pre-trained TFLite ModelsAndroid APKTFLite ModelPre-trained ModelAdversarialexamplesAttackerGrey-box Attack ModelTransfer learning approachExtractCollectIdentifyAttackTrainGenerateComprehended byB. Locating Fine-tuned Models

When locating a ﬁne-tuned model, we consider two metrics:
the structural similarity and the parametric similarity between
a targeted model and a standard pre-trained model from
TensorFlow Hub (TensorHub for short) [15].

layer of the targeted model

Structural Similarity. Given a targeted model extracted
from a deep learning mobile app, we ﬁrst convert it to a
sequence of elements for ease of comparison. Each element
within the sequence corresponds to one layer of the model
and contains the layer’s information,
including identiﬁer,
shape, and data type. As shown in Figure 3, one convo-
lutional
is encoded to ”Mo-
bilenetV1/Conv2d 0/Relu6,[1,112,112,16],ﬂoat32”. To obtain
the structural similarity between a targeted model Mtar and
a pre-trained model Mpre in TensorHub, we compute the
Levenshtein distance [28] between them. The reason is that
various targeted models may ﬁne-tune pre-trained models by
adding one or more new layers, resulting in model sequences
of different lengths between targeted and pre-trained models,
and the Levenshtein distance is able to compute the similarity
for such sequences. The calculation of structural similarity is
formulated as follows:

stru sim(Mtar, Mpre) =

Ltotal − LD(Star, Spre)
Ltotal

(1)

where Star and Spre are respectively the model sequences
of Mtar and Mpre, LD is the Levenshtein distance function,
Ltotal
is the total number of layers of two models, and
stru sim is in the range of 0 to 1. Note that an element is
treated as a character (i.e., a sequence is treated as a string) in
LD, and two elements in a comparable pair are considered as
matched only if all attributes of them are identical. Intuitively,
higher structural similarity indicates that the two models are
more similar in terms of structure.

For the structural similarity, the goal is to determine whether
a targeted model takes advantage of the structure of a pre-
trained model. To ﬁnd out an appropriate similarity threshold
for determining that the two models (i.e., targeted and pre-
trained models) are structurally similar, we conduct a pilot
study on numerous real-world deep learning Android apps that
adopt ﬁne-tuned models to examine the structural similarities
between these ﬁne-tuned models and their corresponding pre-
trained models. Empirically, we ﬁnd that 80% structural simi-
larity is a baseline for locating the pre-trained model in terms
of structure.

Parametric Similarity. In the context of our deﬁnitions in
Section II, transfer learning is categorized into two approaches.
To ensure the effectiveness of our attack, we further adopt
the parametric similarity to determine which transfer learning
approach a ﬁne-tuned model utilizes. This information will be
used to craft a binary adversarial model against the targeted
model, as explained in Section III-C. Given a ﬁne-tuned model
(i.e., targeted model) and its pre-trained model, Mf in, Mpre,
with the structural similarity greater than or equal to 80%, we
still convert two models into sequences, with each layer as one
element, but we adopt the detailed parameters of that layer as
the attribute. Subsequently, we perform subtractions on each
comparable pair, i.e., two elements in the same position. The

4

Fig. 3: Sequence conversion of TFLite model, visualized by
Netron [29].

calculation resulting in 0 represents two elements’ parameters
are the same, then a Boolean value True will be stored in a
sequence with the order. Otherwise, a False will be stored. The
calculation of parametric similarity is formulated as follows:

para sim(Mf in, Mpre) =

Ntrue
Ntotal

(2)

where Ntrue is the number of longest continuous True values,
Ntotal is the total number of Boolean values in the resulting
sequence, and para sim is in the range of 0 to 1. Based
on the parametric similarity between Mf in and Mpre, we
can identify the transfer learning approach that the ﬁne-tuned
model adopts. For instance, we follow the TensorFlow ofﬁcial
transfer learning tutorial [30] to build a ﬁne-tuned model
based on a pre-trained MobileNetV2 that has 156 layers. By
freezing all layers of the base model (i.e., the pre-trained
model without classiﬁcation head has 154 layers) and only
training the parameters of the newly-added classiﬁer layers,
the resulting ﬁne-tuned model and its pre-trained model have
a parametric similarity of approximately 98.72%.

C. Crafting Adversarial Models

In the proposed attack framework, attackers have white-
box access to pre-trained models and partial white-box access
to on-device models that adopt transfer learning (i.e., ﬁne-
tuned models) as they cannot be used to train adversarial
examples. Hence, we consider a given attacker looking to
trigger misclassiﬁcations of a particular class from a ﬁne-tuned
model Mf in, which has been customized through transfer
learning from a pre-trained model Mpre. The adversarial

Pre-trained modelTargeted model Comparable pair  ElementModel comparable sequence  examples are crafted via a binary adversarial model Mbin
retrained from the Mpre.

White-box Pre-trained Model. Mpre is a white-box, mean-
ing the attacker knows its model structure and parameters.
Most popular models, such as MobileNet [31], Inception
[32], and ResNet [33], have been made publicly available
in various repositories (e.g., TensorHub) to increase adoption.
This allows attackers to obtain pre-trained models with min-
imal effort. Even if some pre-trained models are proprietary,
attackers can still access them by pretending to be developers.
Partial White-box Fine-tuned Model. The attacker is able
to obtain the Mf in and examine its structure and parameters
via decompilation of the deep learning mobile app. How-
ever, the training of adversarial examples cannot be directly
performed on Mf in due to the characteristics of the TFLite
model. This causes the Mf in being a partial white-box for the
attacker. Apart from knowing the characteristic of Mf in, the
attacker can also access the corresponding label ﬁle stored in
the decompiled app archive, which offers a chance to ﬁnd the
potential training datasets.

Transfer Learning Parameters. We assume an on-device
model adopts transfer learning. Through the computation of
structural and parametric similarities between it and each pre-
trained model in TensorHub, the attacker is able to know the
type of pre-trained model the on-device model adopted, and
which layers were frozen during the ﬁne-tuning, i.e., which
transfer learning approach is adopted. This information will
be used in the construction of a binary adversarial model.

Binary Adversarial Model. For simplicity, we assume the
attacker attempts to fool an on-device model Mf in that adopts
transfer learning to misclassify a targeted image into any class
other than the true one. As the attacker is unable to perturb
the targeted image on the Mf in but has knowledge of its pre-
trained model Mpre and transfer learning approach Atra, we
thus consider crafting a binary adversarial model Mbin that is
highly similar to the Mf in based on the Mpre and Atra.

To trigger the Mf in to misclassify targeted images x+ into
any class l− different from the targeted class l+, we ﬁrst
collect a set of inputs divided into two parts: targeted images
x+ and non-targeted images x− (i.e., the images correspond
to one of the other classes recognized by Mf in except for the
targeted one). We then feed the collected images (i.e, x+ and
x−) to the Mf in to select correctly classiﬁed inputs and divide
them into the training and test sets (Xtrain and Xtest) for
crafting the Mbin. To obtain the optimal Mbin for constructing
adversarial images, we deﬁne the optimization problem below:

max s(Mbin, Mf in), sstru, spara ∈ s
min (cid:96)(f, Xtrain)
s.t.

f (Xtest) ≥ f (cid:48)(Xtest)

(3)

where s is the similarity function of model structure (for-
mula 1) and parameters (formula 2), (cid:96) is the sparse categorical
cross entropy loss function, and f and f (cid:48) are respectively the
classiﬁer function of Mbin and Mf in. The above optimization
attempts to maximize the structural and parametric similarities
between Mbin and Mf in and minimize the loss of f on Xtrain,
under a constraint that the accuracy of f on Xtest is at least

5

the same as that of f (cid:48). The key insight is that the attacker can
compute perturbations for the x+ based on the Mbin, and use
(cid:48) (i.e., misclassiﬁed as l− by Mbin) to attack
the modiﬁed x+
l+ on the Mf in as the two models are highly similar in terms
of the targeted features.

One critical challenge encountered during the binary model
training is that the training data is insufﬁcient compared to
the original data used for model ﬁne-tuning, which may cause
the Mbin overﬁtting and reduce the power of generalization
(i.e.,
the performance on the targeted class is not similar
to the Mf in). In this case, the adversarial images generated
on Mbin are less powerful to the Mf in. To overcome that
limitation, we adopt
the data augmentation [34] to artiﬁ-
cially augmenting the existing dataset by applying random,
yet realistic, transformations to the training images, such as
rotation and horizontal ﬂipping. This helps expose the Mbin
to different aspects of the training data and thus enhance
the ability of adversarial examples. The complete process of
training a binary adversarial model and crafting corresponding
adversarial images is shown in Figure 4.

Fig. 4: Binary adversarial model training and corresponding
adversarial images crafting.

Label
Classiﬁed count

l+ (no passing)
532

l2
12

l3 (selected as le)
32

...
...

ln
8

TABLE I: Error matrix of the trafﬁc sign recognition model
with respect to the input ”no passing”.

Besides the default setting, we also boost the attack success
rate by ﬁnding the most error-prone class le regarding l+. To
this end, we introduce an error matrix (E) to summarize the
output of Mf in when only passing x+ as model input. Then
the objective is formulated as follows:

le = index

(max Ex+
−j )

i

(4)

where Ex+ ∈ R1×n, n is the total number of classes
recognized by Mf in, j is the index of l+ in Ex+, and i is
the index of le found in Ex+. For instance, given a trafﬁc
sign recognition model and a targeted class ”no passing”, the
resulting error matrix is shown in Table I. It is clear that l3
is the most error-prone among the misclassiﬁed classes as its
classiﬁed count (32) is much higher than other classes and thus
selected as le. Figure 5 shows an example comprising images
taken from the targeted class ”no passing” and its most error-
prone class. Observe that the two classes are highly similar,

Targeted and non-targeted images (x+ & x-)On-device model (Mfin)Correctly classified images (Xtrain & Xtest)Binary adversarial model (Mbin)Adversarial images (x+' )AttackerPre-trained model (Mpre)Transfer learning approach (Atra)Augmented images (Xtrain & Xtest)CollectConfigureInputSelectData augmentationCraftTrainAttackwith only the shape of the red car being slightly different.
Hence, by substituting the non-targeted images x− in the
training set with the most error-prone images xe, the newly
generated adversarial images can be more effective for the
Mf in. The intuition is that the targeted class shares a part
of special features with the most error-prone class, and thus
pushing the targeted image outside its decision boundary (i.e.,
forcing x+ into the decision boundary of le) becomes more
effortless.

(a) Targeted class image.

(b) Most error-prone class image.

Fig. 5: Image examples from the targeted (no passing) and its
most error-prone classes.

IV. EVALUATION
There are four settings that may inﬂuence the effectiveness
of our attack framework against on-device models including
different pre-trained models, datasets, transfer learning ap-
proaches and adversarial attack algorithms. To demonstrate the
attack success rate and generality, we carry out experiments
in all four different settings by speciﬁcally targeting at one
aspect while keeping the other three aspects the same. For
example, when evaluating the performance of our attack in
terms of datasets, the selected transfer learning approach, pre-
trained model, and adversarial attack are ﬁxed. Note that we
adopt the variable control to avoid the explosion of setting
combinations.

A. Experimental Setup

Given one smart app, our approach ﬁrst extracts the deep
learning model. It then identiﬁes the pre-trained model and
how the model ﬁne-tunes the pre-trained model in different
datasets. According to the targeted attack class, we then craft
the binary model and generate the adversarial cases using
different adversarial attacks.

Pre-trained Models. In experiments, we use three dif-
ferent TensorFlow ofﬁcial pre-trained models including Mo-
bileNetV2 [35], InceptionV3 [32] and ResNet50V2 [36] to
build our victim ﬁne-tuned models (i.e., on-device models).
All the pre-trained models are trained on the ImageNet [37]
dataset of 1.3 million images, these models can effectively
serve as generic models of the visual world and are capable
of transfer learning.

Datasets. Since most on-device models are commonly used
in task domains related to the images [3], [1], we follow the
previous works [38], [39] to select three frequently-used image
classiﬁcation datasets to build the victim ﬁne-tuned models
for experiments. The classiﬁcation tasks associated with these
datasets represent typical scenarios developers may face during
transfer learning.
• Object Recognition. It aims to classify an image of an
arbitrary object into its corresponding class. The ﬁne-tuned

6

models are trained using the CIFAR-10 dataset [40] con-
taining 50,000 object images from 10 classes, and comes
with a test dataset of 10,000 images.

• Trafﬁc Sign Recognition. Its objective is to classify vari-
ous trafﬁc signs based on images, which can be used by
autonomous cars to automatically recognize trafﬁc signs.
The ﬁne-tuned models are trained on the GTSRB dataset
[41] containing 39,209 images of 43 different trafﬁc signs.
It also includes a test dataset of 12,630 images.

• Flower Recognition. Its purpose is to classify images
of ﬂowers into different categories, which is a common
example of multi-class classiﬁcation. The ﬁne-tuned models
are trained on the Oxford Flowers dataset [42] containing
6,149 images from 102 classes, and the test dataset contains
1,020 images.
Transfer Learning Approaches. To evaluate the effec-
tiveness of our attack on two transfer learning approaches
(discussed in Section 2), we unfreeze a different number of
the top layers (except for the classiﬁer) of a pre-trained model
(e.g., MobileNetV2) and jointly train both the newly-added
classiﬁer as well as the last unfreezing layers of the base model
to build our victim ﬁne-tuned models. These resulting models
are able to cover most tuning strategies.

Adversarial Attack Algorithms. For the evaluation of our
attack effectiveness against different adversarial attacks, we
focus on untargeted attacks in the white-box setting as our at-
tack fools ﬁne-tuned models to misclassify targeted images by
constructing adversarial examples on known binary adversarial
models. Considering a wide range of white-box untargeted
attack algorithms have been proposed,
is unfeasible to
cover all of them. We thus select three representative attacks
including Fast Gradient Sign Method (FGSM) [6], Carlini and
Wagner (C&W) [20], and Clipping-Aware Noise (CAN) [21]
attacks for experiments as they are either the basis of many
powerful attacks or effective in computer vision tasks [43],
[44].

it

Baselines To attack the victim ﬁne-tuned models, we adopt
three different settings: (1) Default Binary Adversarial Model
Attack (BAM A), which crafts adversarial images based on
a binary model trained on the targeted class (i.e., the class
the attacker intends to force the victim model to misclassify)
and non-targeted class (i.e., an arbitrary class recognized by
the victim model except for the targeted one). (2) Enhanced
Binary Adversarial Model Attack (E−BAM A), it is similar to
the ﬁrst setting but substitutes the non-targeted class with the
most error-prone class (i.e., the class most likely to be misclas-
siﬁed as the targeted one) during binary model training. (3)
Pre-trained Model Attack (P M A), which directly generates
adversarial images solely based on the victim model’s pre-
trained model without taking any other model information into
account, i.e., it ignores the structure and parameter information
of a victim model. Note that the last setting introduced by [1]
is used to perform adversarial attacks for ﬁne-tuned on-device
models, we thus consider comparing it with the proposed
attacks (i.e., BAM A and E − BAM A).

Attack Conﬁguration. We craft adversarial examples using
correctly classiﬁed images collected from the Internet. These
images are not seen by the ﬁne-tuned model during its training

and align with our attack setting, i.e., the attacker has no
access to the training dataset of the on-device model, and
the newly collected images simulate user input. In each set
of experiments, we collect 50 source images for a targeted
class to perform attacks. Success for the untargeted attack
is measured as the percentage of source images that are
incorrectly classiﬁed into any other arbitrary class, so

ASR =

m
t

(5)

where m represents the number of misclassiﬁed images, t is
equal to 50, and ASR represents the attack succes rate.

To evaluate our attack from the above four perspectives,
we control the variable i.e., checking attack performance by
changing one variable while keeping the other three ones ﬁxed.
We implement the attack using TensorFlow Lite [4], lever-
aging the open-source implementation of adversarial attacks
provided by the prior work [45]. In experiments, the ﬁne-
tuning uses the cross-entropy loss and the Adam optimizer
with the default setting as: learning rate = 10−3, β1 = 0.9,
and β2 = 0.999. The results with respect to each perspective
are presented in the following.

B. Effectiveness of the Attack

Performance on different pre-trained models. We build
three different victim ﬁne-tuned models based on pre-trained
MobileNetV2, InceptionV3 and ResNet50V2 by using the
Oxford Flowers dataset and Feature Extraction. The reason for
choosing this dataset is that transfer learning is mainly used
as a solution to the data scarcity problem (i.e., small training
dataset) [38], [46], and the Oxford Flowers dataset contains
less data compared to the other two. After ﬁne-tuning, three
victim models attain 86.08%, 73.73% and 82.45% accuracy,
respectively. To construct adversarial images for a randomly
selected targeted class (i.e., Bougainvillea in our experiments),
we adopt the CAN attack with the epsilon = 20 for P M A,
BAM A and E − BAM A. These values are experimentally
derived by authors to produce unnoticeable image perturba-
tions. For constructing E − BAM A, we use the error matrix
mentioned in Section III-C to ﬁnd out the most error-prone
class Anthurium for Bougainvillea.

Model

Setting

P M A
BAM A
E − BAM A

MobileNetV2

InceptionV3

ResNet50V2

0.14
0.36
0.44

0.04
0.12
0.22

0.06
0.14
0.2

TABLE II: Attack success rate on Flower Recognition against
different pre-trained models.

Table II summarizes the impact of pre-trained model se-
lection on the attack effectiveness. Compared with the de-
fault setting (BAM A), the success rate of our attack is at
least 133% higher than that of P M A for different
types
of ﬁne-tuned models. The poor performance of P M A can
be attributed to the signiﬁcant difference between the ﬁne-
tuned and pre-trained models’ tasks, thus adversarial images
crafted using decision boundary analysis of the pre-trained
model fail on the ﬁne-tuned model. Moreover, when adopting

7

the enhanced setting (E − BAM A), the attack success rate
is further improved by approximately 6-10%. In particular,
we ﬁnd that the ﬁne-tuned model of MobileNetV2 is the
most vulnerable compared to the other two, where the attack
success rate of BAM A and E − BAM A are 36% and 44%,
respectively. This can be explained by the fact that when a
ﬁne-tuned model has higher accuracy, it is more sensitive
to perturbations. Finally, the results suggest that the attack
effectiveness is not correlated with the pre-trained model: our
attack framework is effective for various ﬁne-tuned models
irrespective of the types of their pre-trained models.

Performance on different datasets. We pair the feature
extractor of the pre-trained MobileNetV2 with a newly-added
classiﬁer head and adapt
them to the CIFAR-10, GTSRB
and Oxford Flowers datasets to build three victim ﬁne-tuned
models. Such models achieve 81.79%, 82.70% and 86.08%
accuracy, respectively. For each dataset (i.e., recognition task),
we randomly pick a targeted class and locate its most error-
prone class via the corresponding error matrix. The results are
shown in Table III. To craft adversarial images, we still use
the CAN attack with the same setting (i.e., epsilon =20) as we
only explore the attack effectiveness against different datasets.

Class

Dataset

CIFAR-10
GTSRB
Oxford Flowers

Targeted

Most error-prone

Airplane
Stop
Bougainvillea

Truck
No passing
Anthurium

TABLE III: The selected targeted class and its most error-
prone classs for each dataset.

Table IV shows how the attack effectiveness varies with
respect to different datasets. Across all the recognition tasks,
both BAM A and E − BAM A outperform P M A, with more
than 112% higher attack success rate. Speciﬁcally, we ﬁnd
that the attack success rates of BAM A and E − BAM A are
above 70% for Trafﬁc Sign Recognition. This is because the
classes inside the GTSRB dataset are relatively similar, then
our binary adversarial model can more easily make the targeted
class to be misclassiﬁed into an arbitrary class instead of the
true one. Based on these results, it is clear that our attack
framework remains effective regardless of various datasets
and can lead to higher misclassiﬁcation errors if the targeted
recognition task containing similar classes.

Dataset

Setting

P M A
BAM A
E − BAM A

CIFAR-10

GTSRB

Oxford Flowers

0.06
0.14
0.24

0.34
0.72
0.78

0.14
0.38
0.48

TABLE IV: Attack success rate of MobileNetV2’s ﬁne-tuned
models on various datasets.

Apart from evaluating the attack, we ﬁnd that the targeted
and most error-prone classes are not very similar or even
completely different. This might be because of the visual
perception difference between the human and model. For
instance, in the case of ﬂower recognition, the human perceives
whether two classes are similar based on the concrete object
(e.g., the whole ﬂower with color), while the model perception

8

Fig. 6: Image of original Oxford Flowers and adversarial images generated by FGSM, C&W and CAN attacks.

may be based on the characteristics of the object (e.g., the
texture of the third ﬂower is similar to that of the ﬁrst), as
shown in Figure 7.

(a) Target class image.

(b) Human selection.

(c) Model selection.

Fig. 7: Most error-prone class images selected by the human
and model. The classes of (a), (b) and (c) are Bougainvillea,
Pelargonium and Anthurium, respectively.

Performance on different transfer learning approaches.
We keep the training dataset (Oxford Flowers), the victim ﬁne-
tuned models’ architectures (i.e., MobileNetV2’s base model
plus a fully connected layer) and the attack setting (i.e., CAN
attack with epsilon=20) the same in each set of trials. But the
number of unfreezing layers of the base model is set to a range
of 0 to 60, with an interval of 10, based on the recommended
setting of transfer learning provided by TensorFlow [47]. Note
that 0 unfreezing layer means we adopt Feature Extraction for
transfer learning, while the other number of unfreezing layers
represents different levels of Fine-Tuning.

Layer

Setting

P M A
BAM A
E − BAM A

Feature Extraction
0
0.22
0.36
0.48

Fine-Tuning

10
20
0.16 0.12
0.52 0.54
0.58 0.62

30
0.12
0.50
0.56

40
50
0.20 0.14
0.58 0.50
0.66 0.60

60
0.16
0.56
0.62

TABLE V: Attack success rate of MobileNetV2’s ﬁne-tuned
models against different number of ﬁne-tuning layers.

Table V summarizes the attack effectiveness versus two
transfer learning approaches. Observe that as the number of
ﬁne-tuning layers varies (i.e., the transfer learning approach or
level is changed), our attacks (either BAM A or E − BAM A)
remain higher attack success rates than P M A regardless of
Feature Extraction or Fine-Tuning, which indicates that the
proposed attack framework is universal and robust against
common cases of transfer learning.

Performance on different adversarial attacks. We con-
sider three types of adversarial attack algorithms, FGSM,

C&W and CAN. Their epsilons are set to 0.025, 0.2 and 20,
respectively. These selected epsilon values are experimentally
derived by authors, so as to make the generated adversarial
images preserve high credibility as far as possible. Apart from
epsilons, the rest of the experimental settings (i.e., the pre-
trained model MobileNetV2, the Oxford Flowers dataset and
Feature Extraction) remain the same.

Attack

Setting

P M A
BAM A
E − BAM A

FGSM

C&W

0.16
0.34
0.40

0.04
0.12
0.20

CAN

0.14
0.36
0.46

TABLE VI: Attack success rate of MobileNetV2’s ﬁne-tuned
models under different adversarial attack algorithms.

Table VI summarizes how the setting of attack algorithm
selection inﬂuences the attack effectiveness. For the FGSM
and CAN attacks, BAM A and E − BAM A yield similar
attack performance, with above 2 times attack success rates
compared to P M A. However, when adopting the C&W attack,
the attack success rates of all three settings (i.e., BAM A,
E − BAM A and P M A) are relatively low. To understand the
reason for such results, we examine the adversarial images
generated by three attack algorithms and compare them with
the original images, as shown in Figure 6. Observe that the
images created by the C&W attack look nearly the same
as the original inputs, only with a slight blur around the
ﬂowers, while the images crafted using FGSM and CAN
attacks contain tiny perturbations if examining carefully. These
imperceptible perturbations may be responsible for the higher
attack success rate.

Apart from the above settings, we also evaluate the per-
formance of our attack in terms of different combinations
of the four settings, i.e., pre-trained model, dataset, transfer
learning approach and adversarial attack algorithm are all
taken into account simultaneously. The combinations used
in the experiments are carefully selected to ensure diversity.
The experiment results show that our attack remains effective
and robust regardless of different combinations of the four
settings. Detailed experiment settings and results are available
at https://github.com/Jinxhy/SmartAppAttack.

FGSMC&WClipping-AwareNoiseOriginalTask domain

Image recognition

Object detection

Image segmentation
Pose estimation
Total

Pre-trained model
MobileNetV1 and V2
InceptionV3
SqueezeNet
COCO SSD MobileNet v1
Google Mobile Object Localizer
DeepLabv3
PoseNet

# DL apps adopting transfer learning
23
2
1
9
2
15
1
53

# Fine-tuned model
24
2
1
9
2
17
1
56

TABLE VII: Number of DL apps adopting transfer learning and corresponding ﬁne-tuned TFLite models.

9

V. ATTACKING REAL-WORLD DEEP LEARNING MOBILE
APPS

To estimate the potential damage of the attack framework
to real-world deep learning mobile apps (DL apps for short),
we implement a semiautomatic attack pipeline based on the
methods introduced in Section III and conduct an empirical
study to evaluate the proposed attack on a large set of Android
apps collected from the ofﬁcial Google Play market.

Collecting DL Apps. To ﬁnd the targeted DL apps, we ﬁrst
crawl 63,112 mobile apps from Google Play. These apps cover
a variety of categories (e.g., Medicine, Auto & Vehicles, and
Finance) related to the image domain. Subsequently, we ﬁlter
the apps and extract the corresponding on-device models by
adopting our attack pipeline mentioned in Section III-A. We
obtain 394 apps with 1,345 TFLite models. Note that most
DL apps contain multiple models as they serve more than
one deep-learning-based feature, or require multiple models
to work together to perform speciﬁc tasks.

Identifying and attacking ﬁne-tuned models in DL Apps.
Following the approach mentioned in Section III-B, we locate
DL apps that ﬁne-tune the public pre-trained models from Ten-
sorHub for their own purpose. However, among these extracted
TFLite models, we ﬁnd that 1,120 of them are default Google
Mobile Vision framework [48] without any change, which
concentrates on face, barcode, and text detection. Hence, we
exclude 280 DL apps that only use the Google Mobile Vision
framework and pass the remaining 114 apps to the attack
pipeline. 46.49% (53/114) apps are of ﬁne-tuned models,
which covers various task domains such as object detection,
image segmentation, and pose estimation with details seen in
Table VII. Among these DL apps, nearly half of them perform
tasks related to image recognition, and MobileNetV1 [31]
and V2 [35] are the most commonly used pre-trained models
(24/56).

By knowing pre-trained models and label ﬁles of DL apps
that adopt ﬁne-tune models, our attack framework could suc-
cessfully attack 71.7% (38/53) of them. Here success means
that the generated adversarial images against given targeted
classes can deceive the apps without being perceived by
humans. For the cases of failure, the reasons include (i) the
label ﬁle is missing so we cannot select a targeted class
to perform the attack; (ii) the dataset for training a binary
adversarial model against rare tasks (e.g., prostate cancer
recognition) is insufﬁcient, affecting the attack performance.
Given the fact that the number of DL apps is growing rapidly
[3], we believe the problem should raise concerns in both the
industry and research community.

Real-world Examples. Next, we discuss several real-world

DL apps in more details to illustrate how to attack them
with the proposed framework and present the corresponding
consequences. We select
three DL apps used in security-
critical tasks, including skin cancer recognition, trafﬁc sign
recognition, and cash recognition.

• Skin Cancer Recognition App. Skin cancer recognition can
classify the skin cancer types and help in early detection
[49], which are vital
to ensure a high survival rate in
patients. In this app, a CNN model takes the user’s skin
image captured by a camera as the input and outputs whether
he/she has skin cancer with corresponding suggestions (i.e.,
none for health skin or dermatologist visit for skin cancer).
To launch the attack on the CNN model, we ﬁrst use the
attack pipeline to identify that MobileNetV1 [31] is used as
the pre-trained model. By computing the parametric simi-
larity between the CNN model and its pre-trained model,
we know that a fully unfrozen base model (i.e., a pre-
trained model without classiﬁcation head) is adopted for
Fine-Tuning. Based on the label ﬁle stored in the APK,
we select a targeted class (Melanoma) and collect
the
corresponding data using Google’s Dataset Search2. During
the data collection, we obtain the most error-prone class
(Nevus) to ensure the optimal attack performance based on
the error matrix, which is produced by feeding the targeted
images to the CNN model. Given such information, we build
a binary adversarial model to generate adversarial images.
The subsequent misclassiﬁcation attack achieves a success
rate of 94% (i.e., 47 out of 50 are misclassiﬁed), 56.67%
higher than the previous attack [1].

(a) Normal input with correct output

(b) Adversarial
output

input with wrong

Fig. 8: The behavior of skin cancer recognition app on normal
and adversarial inputs.

2https://datasetsearch.research.google.com/

Figure 8a shows the app correctly classiﬁes the skin lesion
image as melanoma and suggests the user to see a dermatol-
ogist. However, the second image generated by the binary
adversarial model is recognized as healthy skin with a high
conﬁdence level. Due to the increasing use of deep learning
to aid in digital health [50], [51], attacking such apps would
affect medical professionals’ judgement or directly pose
threats to the end-users’ lives.

• Trafﬁc Sign Recognition App. Trafﬁc sign recognition is
able to detect and classify trafﬁc signs for assisting driving.
In this app, a sequence of video frames (images) captured
by a camera are passed to a CNN model as the inputs.
Once an important trafﬁc sign (e.g., stop sign, no entry sign,
and speed limit sign) is detected and recognized, the app
would remind the driver to take a corresponding action (e.g.,
stopping, turning around, and reducing speed).
Again, we are able to launch the attack on the targeted CNN
model via the attack pipeline. It identiﬁes the pre-trained
model used is MobileNetV1 [31] and Feature Extraction is
adopted during transfer learning. In the subsequent training
of the binary adversarial model,
the collected data that
contain the targeted class (Stop sign) and its most error-
prone class (Do not overtake sign) are passed as inputs.
Consequently, the misclassiﬁcation attack achieves a 76%
(i.e., 38 out of 50 are misclassiﬁed) success rate, which is
2.3 times better than the previous attack [1].
Figure 9 shows the app behavior on normal and adversarial
inputs. The app incorrectly reports the stop sign generated
by our binary adversarial model as a speed limit sign. Since
the self-driving vehicle technology is mainly backed up by
deep learning [52], such apps may be directly used for
controlling the vehicle in the future, attacking the models
inside these apps would cause fatal incidents for end-users.

(a) Normal input with correct output

(b) Adversarial
output

input with wrong

Fig. 9: The behavior of trafﬁc sign recognition app on normal
and adversarial inputs.

• Cash Recognition App. Cash recognition can classify
various denominations of the currency (e.g., 20, 500, and
2000 Indian Rupee), which could be used to help visually
impaired people to distinguish different monetary values. In
this app, a user takes a picture of the cash, and the app
reads the currency type and value to the user by using a
customized model.
Again we can launch the attack on the cash recognition
model based on the information identiﬁed by our attack

10

pipeline. Here, the targeted model is built by retraining the
top 28 layers of a MobileNetV1 [31] base model along
with the newly-added classiﬁcation head (i.e., adopting Fine-
Tuning). We then select the targeted (500 Indian Rupee) and
its most-error prone (100 Indian Rupee) classes to build a
binary adversarial model to craft adversarial inputs. As a
result, the misclassiﬁcation attack achieves a success rate of
56% (i.e., 28 out of 50 are misclassiﬁed), 143.48% higher
than the previous attack [1].

(a) Normal input with correct output

(b) Adversarial
output

input with wrong

Fig. 10: The behavior of cash recognition app on normal and
adversarial inputs. | and INR is the symbol and abbreviation
of the Indian Rupee, respectively.

Figure 10 shows the impact of feeding adversarial input
to the app,
in which an Indian 500-rupee banknote is
misclassiﬁed as an Indian 2000-rupee banknote. Since visual
impairment affects information acquisition, it is not hard
to imagine that similar apps can be used in other types of
accessibility services, such as reading newspapers, recogniz-
ing trafﬁc conditions, etc. Attacking deep learning models of
such apps could be a threat to people with visual impairment
that relies on these accessibility services.

A. Attack Generality

VI. DISCUSSION

In this paper, we demonstrate the effectiveness of our attack
for on-device deep learning models related to the ﬁeld of
computer vision. It is also possible to apply our attack to
other ﬁelds, such as natural language processing and speech
recognition. For instance, given a sentiment analysis model
based on a pre-trained neural-net language model, the attacker
can ﬁrst use our framework to identify its pre-trained model
and train a binary adversarial model based on the targeted (e.g.,
positive sentiment) and non-targeted classes, the subsequent
generated adversarial texts can be used to fool the targeted
model (i.e., misclassify the positive text as negative or other
sentiments). Similarly, in speech recognition, the attacker can
also adopt our framework to launch such attacks as long as
the targeted models are based on transfer learning.

Besides the ﬁeld generality, our proposed framework can
also be easily adapted for attacking other edge-support ma-
chine learning frameworks such as PyTorch Mobile [5]. To
demonstrate the adaptability of the proposed framework, we
replace TFLite-related model naming schemes (e.g., ”.tﬂite”),
pre-trained model repository (e.g., TensorHub) and framework
APIs (e.g., ”tf.lite.Interpreter”) in the original attack frame-
work with those (e.g., ”.pt”, Pytorch Hub3 and ”torch.jit”)

3https://pytorch.org/hub/

related to PyTorch Mobile [5]. We then apply our attack to
two randomly-selected real-world DL apps that adopt PyTorch
Mobile [5].: (1) obstacle recognition app that assists visually
impaired people in distinguishing various obstacles in the form
of reading the uploaded picture. It uses a MobileNetV2 [35]
as the pre-trained model and adopts Feature Extraction for
retraining. (2) crop disease recognition app that helps farmers
to recognize the disease of agricultural plants. It adopts Fine-
Tuning that retrains the top 20 layers of an InceptionV3 [32]
base model along with the newly-added classiﬁcation head.
The results are shown in Figure 11 and 12.

(a) Normal input with correct output

(b) Adversarial
output

input with wrong

Fig. 11: The behavior of obstacle recognition app on normal
and adversarial inputs.

(a) Normal input with correct output

(b) Adversarial
output

input with wrong

Fig. 12: The behavior of crop disease recognition app on
normal and adversarial inputs.

B. Attack Scalability

On-device deep learning is not limited to mobile devices,
it can also be applied to embedded and IoT devices, such
as Raspberry Pi and microcontrollers [53], [54]. Attacks on
models inside such devices might be slightly different from
those inside mobile devices, depending on the type of device.
For example, an embedded Linux device can directly run the
locally stored model without installing APK [55]. In this case,
the attacker can effortlessly obtain the model then adopt our
framework to launch the attack. If the model is deployed on

11

a microcontroller, the attacker can recover it from the C byte
array (i.e., the C source ﬁle that contains the model) [56] to
the TFLite model and perform the attack. Apart from this, it
is also possible to extend our framework for attacking other
edge devices that adopt on-device deep learning.

C. Defence of Attack

To defend our attack, we discuss a few possible counter-
measures from the perspective of practitioners. First, deep
learning mobile app developers are responsible for building
the on-device models, so they can take immediate and effective
actions to secure the models, for instance: (1) encrypt the label
ﬁle to prohibit attackers from collecting training data to build
binary adversarial models. (2) obfuscate the model information
like layer names to prevent attackers from gaining insights
into the model i.e., determining whether it is based on the pre-
trained model. (3) adopt Android packing techniques to thwart
attackers from obtaining any information associated with the
model, such as the corresponding model framework API,
model sufﬁx convention and label ﬁle, thereby preventing them
from building binary adversarial models. (4) store and execute
the model in secure hardware. For instance, TF-Trusted [57]
allows developers to run TensorFlow models inside of an Intel
SGX device. (5) train multiple ﬁne-tuned models based on
different types of pre-trained models for a given task, and
use them together to make a prediction (i.e., majority vote).
Thus even if the attacker successfully fools a single ﬁne-tuned
model in the ensemble, the other models may be resistant. This
is because our attack performance cannot remain the same
when the types of pre-trained models changed, as discussed
in Section IV-B. For deep learning framework providers, they
should consider providing better protection mechanisms: (1)
signature-based model loading to ensure the model is utilized
by trusted users. (2) built-in model encryption API assists
developers to encrypt the models, avoiding attackers getting
the model structures and parameters.

VII. RELATED WORK

As this work focuses on the attack to the ﬁne-tuned models
in mobile apps, we introduce related works about the adver-
sarial attacks to transfer learning and security of deep learning
mobile apps in the following text.

A. Adversarial Attacks in Transfer Learning

Transfer learning has been demonstrated its effective to
facilitate the effortless derivation of new models (i.e., ﬁne-
tuned models) through retraining a pre-trained model on
a relevant
task with less data and computation cost [58].
However, pre-tained models are usually publicly available for
sharing and reuse, which inevitably introduces vulnerability to
trigger severe attacks (e.g., adversarial attacks) against transfer
learning applications.

In recent years, researchers have proposed several novel
adversarial attacks against deep neural networks in the context
of transfer learning. For instance, Wang et al. [38] proposed a
transfer-based adversarial attack against Machine Learning as

a service platforms, where the attacker is assumed to have
knowledge about the pre-trained model and an instance of
targeted image. They exploit this knowledge to modify the
internal representation of the source image to make it similar
to the internal representation of the targeted image, which
results in misclassiﬁcation. Similarly, Ji et al. [59] crafted
the malicious primitive model based on the pre-trained feature
extractor, which is utilized to perturb the source image based
on the targeted image such that their internal representations
are close to each other. They exploit the transferability of
adversarial examples to successfully attack corresponding ﬁne-
tuned models incorporated in machine learning systems. Nev-
ertheless, these attacks are only effective when victim ﬁne-
tuned models adopt Feature Extraction, i.e., without changing
any parameter in the pre-trained model’s base model. However,
according to our observation, development teams may adopt
different transfer learning approaches to ﬁne-tune their models
in mobile apps which make these attack invalid.

In addition, Prabhu and Whaley [60] utilized a pre-trained
convolutional neural network model to generate adversarial
examples, which successfully fool a black-box SVM classiﬁer
(i.e., it is not accessible to the attacker) that uses this pre-
trained model as the feature extractor. Following this work,
Rezaei and Liu [61] demonstrated that without any additional
knowledge other than the pre-trained model, attackers can
effectively mislead a feature-extractor-based ﬁne-tuned model
by exploiting the vulnerabilities of Softmax layer. Moreover,
Abdelkader et al. [39] found that using a known feature
extractor (i.e., pre-trained model) exposes a ﬁne-tuned model
to powerful attacks that can be executed without knowledge of
the classiﬁer head at all. Recently, Pal and Tople [62] exploited
unintended features learnt in the pre-trained model to generate
adversarial examples for ﬁne-tuned models, which achieves a
high attack success rate in text prediction task domain.

All these existing attacks assume pre-trained models are
known to the attacker, which may not apply in real-world deep
learning applications. Our attack framework considers a much
more realistic setting in which the attacker does not know the
type of pre-trained model used. We exploit the characteristics
of the on-device model (i.e., it is accessible but untrainable) to
identify the pre-trained model and transfer learning approach
for crafting a self-trained binary adversarial model similar to
the targeted model to generate adversarial images.

12

our work represents a solid step towards the security of deep
learning mobile apps (i.e., attacking on-device models).

Following the previous work [3], Sun et al. [2] analyzed
the model protection problem of on-device machine learning
models in mobile apps, and demonstrated the feasibility of
stealing private models from AI apps as well as the potential
ﬁnancial losses that can occur. Recently, Huang et al. [1]
investigated the vulnerability of deep learning models within
real-world Android apps against adversarial attacks. Their
ﬁndings unveiled that on-device deep learning models, which
adopt or ﬁne-tune pre-trained models, are more vulnerable to
adversarial attacks. However, these approaches either focus on
the model extraction or the exploration of vulnerable models,
they do not provide a practical way of attacking on-device
models. Hence, our work aims to ﬁll this knowledge gap by
proposing an effective model-speciﬁc adversarial attack which
can be directly used for attacking practical deep learning apps.

VIII. CONCLUSION
This paper proposes a simple yet effective adversarial attack
framework against on-device deep learning models. Experi-
ments from four perspectives, including pre-trained models,
datasets, transfer learning approaches and adversarial attack
algorithms, show that
the attack is universal and robust.
Exemplifying three real-world deep learning mobile apps of
skin cancer, trafﬁc sign, and cash recognition demonstrate
the potential damage of the attack. We hope this work can
raise the awareness of the community for protecting on-device
models. A set of avenues for future research include: First,
in this paper, we only concentrate on attacking on-device
models in Android apps. Considering the model accessibility
in Android, it is also possible to transfer such attacks from
Android to iOS or even to counterparts from other platforms
like web, resulting in more powerful and generic attacks.
Second, this paper only considers attacks on deep learning
models in smartphone, it is interesting to explore such attacks
against models in other edge devices like Raspberry Pi. Third,
our study focuses on deep learning models related to image
tasks, other task domains like natural language processing are
also worth studying. Finally, the countermeasures discussed in
Section VI-C may serve as a point of departure for developing
effective defences.

REFERENCES

B. Security of Deep Learning Mobile Apps

The security of mobile apps has become a major research
ﬁeld due to the ubiquity of smartphones [63]. Many studies
[64], [63] perform static or dynamic analysis or both to detect
vulnerabilities in mobile apps. However, there are few studies
that focus on the security of deep learning mobile apps. Wang
et al. [65] and Xu et al. [3] carried out empirical studies on
challenges and current status of pushing deep learning towards
mobile apps and found that most on-device models in mobile
apps are exposed without any protection, which may result
in severe security and privacy issues. Although these studies
reveal the security issues of deep learning mobile apps, they do
not perform practical attacks to on-device models. In contrast,

[1] Y. Huang, H. Hu, and C. Chen, “Robustness of on-device models:
Adversarial attack to deep learning models on android apps,” in
IEEE/ACM 43rd International Conference on Software Engineering:
Software Engineering in Practice (ICSE-SEIP), 2021.

[2] Z. Sun, R. Sun, L. Lu, and A. Mislove, “Mind your weight (s): A large-
scale study on insufﬁcient machine learning model protection in mobile
apps,” in 30th {USENIX} Security Symposium ({USENIX} Security 21),
2021.

[3] M. Xu, J. Liu, Y. Liu, F. X. Lin, Y. Liu, and X. Liu, “A ﬁrst look at
deep learning apps on smartphones,” in The World Wide Web Conference,
2019, pp. 2125–2136.

[4] “Tensorﬂow lite,” https://https://www.tensorﬂow.org/lite, 2021.
[5] “Pytorch mobile,” https://pytorch.org/mobile/home/, 2021.
[6] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.

[7] F. Suya, J. Chi, D. Evans, and Y. Tian, “Hybrid batch attacks: Finding
black-box adversarial examples with limited queries,” in 29th USENIX
Security Symposium USENIX Security 20), 2020, pp. 1327–1344.

[8] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, “Zoo: Zeroth
order optimization based black-box attacks to deep neural networks
without training substitute models,” in Proceedings of the 10th ACM
workshop on artiﬁcial intelligence and security, 2017, pp. 15–26.
[9] J. Chen, M. I. Jordan, and M. J. Wainwright, “Hopskipjumpattack:
A query-efﬁcient decision-based attack,” in 2020 ieee symposium on
security and privacy (sp).

IEEE, 2020, pp. 1277–1294.

[10] K. T. Co, L. Mu˜noz-Gonz´alez, S. de Maupeou, and E. C. Lupu,
“Procedural noise adversarial examples for black-box attacks on deep
convolutional networks,” in Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security, 2019, pp. 275–
289.

[11] X. Li, S. Ji, M. Han, J. Ji, Z. Ren, Y. Liu, and C. Wu, “Adversarial
examples versus cloud-based detectors: A black-box empirical study,”
IEEE Transactions on Dependable and Secure Computing, 2019.
[12] W. Brendel, J. Rauber, and M. Bethge, “Decision-based adversarial
attacks: Reliable attacks against black-box machine learning models,”
arXiv preprint arXiv:1712.04248, 2017.

[13] Y. Li, J. Hua, H. Wang, C. Chen, and Y. Liu, “Deeppayload: Black-box
backdoor attack on deep learning models through neural payload injec-
tion,” in 2021 IEEE/ACM 43rd International Conference on Software
Engineering (ICSE).
IEEE, 2021, pp. 263–274.

[14] X. Wang, Y. Han, V. C. Leung, D. Niyato, X. Yan, and X. Chen, “Edge
computing for artiﬁcial intelligence,” in Edge AI. Springer, 2020, pp.
97–115.

[15] Google, “Tensorﬂow hub,” https://tfhub.dev/, 2021.
[16] J. Y. Koh, “Model zoo,” https://modelzoo.co/, 2021.
[17] “Pytorch,” https://pytorch.org, 2021.
[18] “Core ml,” https://developer.apple.com/documentation/coreml, 2021.
[19] N. C¸ ¨ur¨uko˘glu and B. M. ¨Ozyildirim, “Deep learning on mobile systems,”
in 2018 Innovations in Intelligent Systems and Applications Conference
(ASYU).

IEEE, 2018, pp. 1–4.

[20] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in 2017 ieee symposium on security and privacy (sp).
IEEE,
2017, pp. 39–57.

[21] J. Rauber and M. Bethge, “Fast differentiable clipping-aware normal-

ization and rescaling,” arXiv preprint arXiv:2007.07677, 2020.

[22] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin, “Black-box adversarial at-
tacks with limited queries and information,” in International Conference
on Machine Learning. PMLR, 2018, pp. 2137–2146.

[23] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,” in
Proceedings of the 2017 ACM on Asia conference on computer and
communications security, 2017, pp. 506–519.

[24] N. Papernot, P. McDaniel, and I. Goodfellow, “Transferability in ma-
chine learning: from phenomena to black-box attacks using adversarial
samples,” arXiv preprint arXiv:1605.07277, 2016.

[25] C. Guo, J. Gardner, Y. You, A. G. Wilson, and K. Weinberger, “Simple
black-box adversarial attacks,” in International Conference on Machine
Learning. PMLR, 2019, pp. 2484–2493.

[26] M. Andriushchenko, F. Croce, N. Flammarion, and M. Hein, “Square
attack: a query-efﬁcient black-box adversarial attack via random search,”
in European Conference on Computer Vision. Springer, 2020, pp. 484–
501.

[27] R. Winsniewski, “Apktool: a tool for reverse engineering android apk

ﬁles,” https://ibotpeaches.github.io/Apktool/, 2012.

[28] V. I. Levenshtein, “Binary codes capable of correcting deletions, inser-
tions, and reversals,” in Soviet physics doklady, vol. 10, no. 8. Soviet
Union, 1966, pp. 707–710.

[29] L. Roeder, “Netron,” https://netron.app/, 2021.
[30] “Tensorﬂow transfer

learning,” https://www.tensorﬂow.org/tutorials/

images/transfer learning, 2021.

[31] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861, 2017.

[32] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2016, pp.
2818–2826.

[33] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[34] L. Perez and J. Wang, “The effectiveness of data augmentation in image
classiﬁcation using deep learning,” arXiv preprint arXiv:1712.04621,
2017.

13

[35] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings
of the IEEE conference on computer vision and pattern recognition,
2018, pp. 4510–4520.

[36] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep residual
networks,” in European conference on computer vision. Springer, 2016,
pp. 630–645.

[37] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in 2009 IEEE conference on
computer vision and pattern recognition.

Ieee, 2009, pp. 248–255.

[38] B. Wang, Y. Yao, B. Viswanath, H. Zheng, and B. Y. Zhao, “With
great training comes great vulnerability: Practical attacks against transfer
learning,” in 27th {USENIX} Security Symposium ({USENIX} Security
18), 2018, pp. 1281–1297.

[39] A. Abdelkader, M. J. Curry, L. Fowl, T. Goldstein, A. Schwarzschild,
M. Shu, C. Studer, and C. Zhu, “Headless horseman: Adversarial attacks
on transfer learning models,” in ICASSP 2020-2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2020, pp. 3087–3091.

[40] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features

from tiny images,” 2009.

[41] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, “The German Trafﬁc
Sign Recognition Benchmark: A multi-class classiﬁcation competition,”
in IEEE International Joint Conference on Neural Networks, 2011, pp.
1453–1460.

[42] M.-E. Nilsback and A. Zisserman, “Automated ﬂower classiﬁcation over
a large number of classes,” in Proceedings of the Indian Conference on
Computer Vision, Graphics and Image Processing, Dec 2008.

[43] D. Ye, C. Chen, C. Liu, H. Wang, and S. Jiang, “Detection de-
fense against adversarial attacks with saliency map,” arXiv preprint
arXiv:2009.02738, 2020.

[44] E. Rusak, L. Schott, R. S. Zimmermann, J. Bitterwolf, O. Bringmann,
M. Bethge, and W. Brendel, “A simple way to make neural networks
robust against diverse image corruptions,” in European Conference on
Computer Vision. Springer, 2020, pp. 53–69.

[45] J. Rauber, W. Brendel, and M. Bethge, “Foolbox: A python toolbox to
benchmark the robustness of machine learning models,” arXiv preprint
arXiv:1707.04131, 2017.

[46] Y. Zhang, Y. Song, J. Liang, K. Bai, and Q. Yang, “Two sides of the
same coin: White-box and black-box attacks for transfer learning,” in
Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, 2020, pp. 2989–2997.

[47] “Tensorﬂow: Transfer learning and ﬁne-tuning,” https://www.tensorﬂow.

org/tutorials/images/transfer learning, 2021.

[48] “Google mobile vision,” https://developers.google.com/vision/, 2021.
[49] E. Nasr-Esfahani, S. Samavi, N. Karimi, S. M. R. Soroushmehr, M. H.
Jafari, K. Ward, and K. Najarian, “Melanoma detection by analysis
of clinical images using convolutional neural network,” in 2016 38th
Annual International Conference of the IEEE Engineering in Medicine
and Biology Society (EMBC).

IEEE, 2016, pp. 1373–1376.

[50] T. Y. Tan, L. Zhang, and C. P. Lim, “Intelligent skin cancer diagnosis
using improved particle swarm optimization and deep learning models,”
Applied Soft Computing, vol. 84, p. 105725, 2019.

[51] H. Nahata and S. P. Singh, “Deep learning solutions for skin cancer
detection and diagnosis,” in Machine Learning with Health Care Per-
spective. Springer, 2020, pp. 159–182.

[52] S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu, “A survey
of deep learning techniques for autonomous driving,” Journal of Field
Robotics, vol. 37, no. 3, pp. 362–386, 2020.

[53] S. Vadlamudi, “Driver drowsiness detection on raspberry pi 4.”
[54] P. Warden and D. Situnayake, Tinyml: Machine learning with tensorﬂow
” O’Reilly

lite on arduino and ultra-low-power microcontrollers.
Media, Inc.”, 2019.

[55] “Tensorﬂow lite for embedded linux,” https://www.tensorﬂow.org/lite/

guide/python, 2021.

[56] “Tensorﬂow lite for microcontrollers,” https://www.tensorﬂow.org/lite/

microcontrollers, 2021.

[57] “Tf trusted,” https://github.com/capeprivacy/tf-trusted, 2021.
[58] S. Niu, Y. Liu, J. Wang, and H. Song, “A decade survey of transfer
learning (2010-2020),” IEEE Transactions on Artiﬁcial Intelligence,
2021.

[59] Y. Ji, X. Zhang, S. Ji, X. Luo, and T. Wang, “Model-reuse attacks
on deep learning systems,” in Proceedings of the 2018 ACM SIGSAC
Conference on Computer and Communications Security, 2018, pp. 349–
363.

14

Prabhu

[60] V. U.
online:
ial
transfer
id/wpcontent/uploads/2018/03/greybox attack.pdf, 2018.

J. Whaley,

learning,”

attacks

“On

and

and

grey-box

adversar-
https://unify.

[61] S. Rezaei and X. Liu, “A target-agnostic attack on deep models:
Exploiting security vulnerabilities of transfer learning,” arXiv preprint
arXiv:1904.04334, 2019.

[62] B. Pal and S. Tople, “To transfer or not to transfer: Misclassiﬁca-
transfer learned text classiﬁers,” arXiv preprint

tion attacks against
arXiv:2001.02438, 2020.

[63] A. Amin, A. Eldessouki, M. T. Magdy, N. Abdeen, H. Hindy, and
I. Hegazy, “Androshield: automated android applications vulnerability
detection, a hybrid static and dynamic analysis approach,” Information,
vol. 10, no. 10, p. 326, 2019.

[64] A. Merlo and G. C. Georgiu, “Riskindroid: Machine learning-based risk
analysis on android,” in Iﬁp international conference on ict systems
security and privacy protection. Springer, 2017, pp. 538–552.
[65] J. Wang, B. Cao, P. Yu, L. Sun, W. Bao, and X. Zhu, “Deep learning
towards mobile applications,” in 2018 IEEE 38th International Confer-
ence on Distributed Computing Systems (ICDCS).
IEEE, 2018, pp.
1385–1393.

Yujin Huang is currently pursuing the Ph.D. degree
with the Faculty of Information Technology, Monash
University, Australia. His research concentrates on
causal discovery, natural language processing, and
deep learning security.

Chunyang Chen is a lecturer (Assistant Professor)
in Faculty of Information Technology, Monash Uni-
versity, Australia. His research focuses on software
engineering, deep learning and human-computer in-
teraction. He has published over 40 papers in re-
ferred journals or conferences. He is a member of
IEEE and ACM. He has received ACM SIGSOFT
Distinguished Paper Award in ICSE 2020, Facebook
Research Award in Probability and Programming
2020, etc. https://chunyang-chen.github.io/

