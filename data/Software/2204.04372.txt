2
2
0
2

r
p
A
9

]

G
L
.
s
c
[

1
v
2
7
3
4
0
.
4
0
2
2
:
v
i
X
r
a

ML Evaluation Standards Workshop at ICLR 2022

A SIREN SONG OF OPEN SOURCE REPRODUCIBILITY

Edward Raff
Booz Allen Hamilton
University of Maryland, Baltimore County
raff edward@bah.com

Andrew L. Farris
Booz Allen Hamilton
Farris Drew@bah.com

ABSTRACT

As reproducibility becomes a greater concern, conferences have largely converged
to a strategy of asking reviewers to indicate whether code was attached to a sub-
mission. This is part of a larger trend of taking action based on assumed ideals,
without studying if those actions will yield the desired outcome. Our argument
is that this focus on code for replication is misguided if we want to improve the
state of reproducible research. This focus can be harmful — we should not force
code to be submitted. There is a lack of evidence for effective actions taken by
conferences to encourage and reward reproducibility. We argue that venues must
take more action to advance reproducible machine learning research today.

1

INTRODUCTION

To start, we must be clear that by reproducibility we are referring to the ability of an independent
team to recreate the same qualitative results, and by replication we are referring to the use of code
to re-create the same results. These terms have been used inconsistently across different ﬁelds of
study at various points throughout time (Plesser, 2018). Many major machine learning conferences
have appointed reproducibility chairs, and in doing so have almost uniformly converged on using
check-boxes to indicate that a submission includes code, or asking authors to answer vague ques-
tions about reproducibility. Some venues explicitly ask for code, others do not. Reviewers often
believe that code indicates reproducibility. There appears to be prevailing belief that, if authors
open-source their code and ensure the code reproduces the paper’s results, we can solve the repro-
ducibility crisis (Forde et al., 2018a; Kluyver et al., 2016; Zaharia et al., 2018; Forde et al., 2018b;
Paganini & Forde, 2020; Gardner et al., 2018). Our contention is that open source and associated
replicability aides, are good — but that this idealized notion is not a Pareto optimal improvement
over papers that do not share source code. We argue that there are pros and cons to including source
code with papers when we consider the long-term health of the ﬁeld. The pros are widely known,
and have been explored since the advent of digital communication (Claerbout & Karrenbach, 1992).
In this opinion piece, we argue the cons: current evidence (though more is dearly needed) suggests
open-source code may improve replication, but creates new issues in reproducibility.

Toward our argument, we have a fundamental axiom: if work can be replicated (i.e., using author’s
original code and data) but not reproduced, then the work constitutes, at best, ineffective science
(Drummond, 2009). It is ﬁne for authors to produce such works, but in the long term, we do not
truly understand the mechanism of action or the truth of our methods unless they are reproducible.
Ideally, we desire that the fraction of works that are reproducible increases over time.

We will begin our argument in section 2 by noting prior history of reproducible research in other
ﬁelds, and describing how we are slowly re-learning lessons discovered long ago that show how
having code does not solve reproducibility by our axiom. We provide notable examples on how
code did not beneﬁt, or even delayed, important understanding of machine learning methods in
section 3 with the seminal word2vec and Adam. These are not arguments that these works are
useless or “wrong”, but that code negatively impacted better scientiﬁc understanding in the former,

1

 
 
 
 
 
 
ML Evaluation Standards Workshop at ICLR 2022

and provided no beneﬁt in the latter. Finally we will conclude with the argument that conferences
must create reproducibility tracks that include explicit guidelines for reviewers on how to judge
submissions, so that we can advance the study of reproducibility before blindly stepping toward
ineffective solutions.

2 WE HAVE FORGOTTEN HISTORY, NOW WE ARE REPEATING IT

The machine learning community has only just begun to expend serious effort towards the study of
reproducibility with respect to itself as a domain. The discoveries are unnerving, and have strong
parallels with historical ﬁndings in other domains. Signiﬁcant early work in the study of code repro-
ducibility was done by Hatton (1993; 1997); Hatton & Roberts (1994), performing static analysis
across C and FORTRAN code as well as having multiple implementations of the same algorithm,
and providing the exact same inputs and parameters to each independent implementation. Their
results found a high defect rate, more than 1 issue per 150 lines of code, and that the precision of
independent implementations was only one signiﬁcant ﬁgure. FORTAN and C still form the founda-
tion of scientiﬁc computing, including machine learning packages like NumPy (Harris et al., 2020),
Tensorﬂow (Abadi et al., 2016), and Pytorch (Paszke et al., 2019). These projects are important
components of the computational foundation of our ﬁeld, yet often focus on the pursuit of optimal
performance at the expense of other goals such as maintainability and portability, the need for multi-
disciplinary teams for success (e.g., where we often consider “applications” a secondary track or
area that is often stigmatized), and most importantly, the high difﬁculty of verifying the correctness
of the equations and math implemented (Carver et al., 2007). Indeed as history repeats itself, recent
work has identiﬁed cases where the same models implemented with different packages or hardware
accelerators present reduced precision or accuracy to the order of one signiﬁcant ﬁgure and even
greater variation in run-time consistency (Zhuang et al., 2021). Even within a single set of hardware
and implementation, our most widely used libraries often have non-deterministic implementations
that can cause 10% variances in results (Pham et al., 2020).

A more nuanced version of the above point stems from how we deﬁne replication: does it simply in-
volve the code, or must it also include the data? The latter is how the terminology is most historically
used, and common in other sciences (Plesser, 2018). This is challenging in machine learning due to
the intrinsic “fuzzyness” of what we are working toward: we intrinsically wish to use machine learn-
ing for tasks where thorough speciﬁcation of the data is too difﬁcult to implement in code. We can
again look to other ﬁelds, like software engineering, that attempted to perform reproductions that
included the data process over software repositories (Gonz´alez-Barahona & Robles, 2012). Their
work found that missing or minute details could prevent or signiﬁcantly impede reproduction. In-
deed it becomes unsurprising then that we have only recently discovered considerable labeling is-
sues within foundational datasets like MNIST, CIFAR, and ImageNet (Northcutt et al., 2021). While
data sheets and model cards have been proposed to partially address this issue, (Gebru et al., 2021;
Mitchell et al., 2019) they are proposed without any scientiﬁc study to answer if these interventions
mitigate the underlying problem. It is good for producers and users of datasets to carefully think
about the data in use, but we fear that absent evidence, these approaches may have no direct tangible
impact1. Indeed studies of dataset replication (where no model card exists) have been shockingly
successful in some ways (no evidence of adaptive over-ﬁtting) and identify concerns not addressed
in model cards or data sheets (Engstrom et al., 2020) with similar results over applied domains such
as social media analysis (Geiger et al., 2021).

As such, we argue that there is extensive prior evidence that predicts the current trends in machine
learning reproducible research: having code available means relatively little to the question of re-
producibility, especially in light of inconsistent methods of comparison used through decades of ma-
chine learning literature, leading to invalid conclusions of “improvement” (Bradley, 1997; Alpaydin,
1999; Cawley & Talbot, 2010; Demˇsar, 2006; Benavoli et al., 2016; Bouthillier et al., 2021; 2019;
Dror et al., 2017; 2018; 2019), necessitating that even a system with no bit-rot would not solve the
concerns of our ﬁeld.

1Their ability to change thoughts and focus areas of others, creating positive secondary impacts, is more

likely, but a separate matter beyond our discussion.

2

ML Evaluation Standards Workshop at ICLR 2022

3 HOW CAN OPEN SOURCED CODE HARM US?

Given that we have clear evidence that simply having original source code is not sufﬁcient to enable
reproducibility, we must now ask: can withholding code ever lead to an improvement in repro-
ducibility? It is important to be clear that we are not arguing that no-code is always or even usually
better. We are arguing that a lack of code creates a different kind of forcing function for adoption.
We recognize2 that code sharing is likely to lead to faster adoption of a method that works, but ob-
scures long-term beneﬁts to reproducible work. If a paper’s method must be re-implemented due to
a lack of code, this process organically validates said method. The paper’s method only gets used
and cited3 when others can successfully reproduce it, converging on methods that work and forcing
deeper understanding by a broader population. Further still, this forces the community at large to
be effective communicators and to better understand the details and science required to reproduce
one’s results. The need to enable reproducibility drove Taswell (1998) to develop a proposal to
better specify wavelet transforms, which also enabled better replicability of his methods. We ﬁnd
tangential evidence for this within machine learning where 36% of papers could not be reproduced
from their content, even though many provided source code (Raff, 2019). To further exemplify how
we believe this to be an issue, we will draw from highly successful academics to critique with a bias
to avoid undue harm or stress to early career researchers (in similar spirit to Lipton & Steinhardt
(2019) ).

The seminal word2vec (Mikolov et al., 2013) algorithm is our ﬁrst consideration. A publication
who’s ubiquity and impact in research and application is enormous, and to the best of our knowledge,
has never been replicated. Understanding how and why word2vec worked was studied by many
(Goldberg & Levy, 2014) due to its utility and effectiveness, but was done through the originally
released code (or direct translations into other languages). Yet it took six years for any public
documentation of the fact that the paper and code simply do not perform the same steps Bhat (2019),
making it impossible for anyone to reproduce.

Clearly, word2vec was important and valuable for the community, but there are counterfactual
questions that we argue suggest the long-term health of our research would have been better if
Mikolov et al. (2013) never released their source code. First, there is an unknown amount of person
hours wasted by researchers, students, and others attempting to understand the mechanisms of an
algorithm that was inhibited by faulty foundations4. Second, failure to reproduce by others would
be a forcing function on the original authors to re-examine their code and paper to correctly doc-
ument how and why it works. By releasing the code, this feedback cycle is inhibited. This could
also explain how follow-up work with paragraph2vec (Le & Mikolov, 2014) has similarly evaded
reproduction, even by the paper’s co-authors5.

A different perspective on this matter is seen in the Adam optimizer (Kingma & Ba, 2015), which
has become a widely used default method. This case is interesting in that the simplicity of the ap-
proach has enabled many reproductions, but both the code and the paper lack details on how the
default parameter values were derived. Subtle corrections to the math of Adam in weight decay
(Loshchilov & Hutter, 2019) and the ǫ parameter (Yuan & Gao) can yield large improvements in
the quality of results, as the default values of Adam are not ideal for all cases. While we should,
in general, have no reason to believe in a one-size-ﬁts-all approach, the lack of study around these
details is itself lending to reproducibility challenges in our ﬁeld: the “right” way to set these pa-
rameters (amongst dozens of others in a network) was unstudied, and many sub-ﬁelds began tweak-
ing the defaults for their kinds of networks, creating confusion and slowing reproduction of sub-
sequent research. This kind of issue is not new. Poorly documented accounts of differences in
LBFGS (Liu & Nocedal, 1989) implementation results can be found 6, though we are not aware of
any thorough documentation or study of them. This again suggests an issue with an incomplete
description in the paper, a problem that code can not reduce — but can hide for a period of time.

2Without the same quality of evidence, indeed we are not sure how to design a good experiment for this.

But this is an opinion paper, so we feel some indignant right to be opinionated.

3There are certainly edge cases where a method that does not replicate will be used and cited, we are talking

in the more general broader case of directly building upon or relying on a method.

4Not to mention feelings of inadequacy, anxiety, and stress by students attempting to become researchers in

what is already a needlessly high-stress environment.

5https://groups.google.com/g/word2vec-toolkit/c/Q49FIrNOQRo/m/DoRuBoVNFb0J
6https://discourse.julialang.org/t/optim-jl-vs-scipy-optimize-once-again/61661/5?page=2

3

ML Evaluation Standards Workshop at ICLR 2022

We also argue that relying on open source code creates an academic moral hazard. Distilling the
the essence of the scientiﬁc contribution, and communicating it effectively, is the task of an author.
Although code does not solve reproducibility, it does enable replication and provides short term
beneﬁts in citation rate and adoption (Raff, 2022), thus allows the manuscript to defer “nuscance”
details to the code. Reviewers can run the code to conﬁrm that “it works”, without checking that
the code actually performs the method described precisely or simply be unaware of key confound-
ing design choices. We preemptively rebut an argument that having code does allow checking an
approach. We rebuke this argument by noting that decades of research shows that reading and de-
bugging code does not ensure the same kind of mental processing as reading prose (Ivanova et al.,
2020; Perkins & Simmons, 1988; Letovsky, 1987). This is conﬁrmed by the demonstrated positive
impact of high quality code comments on understanding code (Nurvitadhi et al., 2003). As such
the reading of code is a more challenging mental process than reading well constructed prose in a
paper, and while helpful is not an alternative to effective communication. This fact, combined with
the examples of Adam and word2vec show ways that code, regardless of how easy it is to imple-
ment, can harm reproducibility. We fear that an over-emphasis on code will seed new reproducibility
problems.

4 CONCLUSION

We remind the reader that we are not arguing that open sourcing code is bad. Open source code
is valuable, but is not a panacea for reproducible research. A lack of advances in the science of
reproducible research will lead to negative long-term artifacts that are expensive to remedy. Almost
all current conference efforts focus solely around open-source. We argue there is strong evidence
that this will not improve or solve the problem of reproducibility. In our minds, the primary issue is
that, similar to other ﬁelds of science, the study of better reproducibility practices will have a cost
that is not rewarded (Poldrack, 2019). We call upon the conference chairs at our primary publication
venues create those incentives with two simple changes.

First and foremost, specialized tracks on reproducibility, and rewards for reproducibility, should
become standard at all conferences. The Conference of the Association for Computing Machinery
Special Interest Group in Information Retrieval (SIGIR) is the only conference we are aware of that
has taken a positive step toward the underlying issue by creating a reproducibility track7. However
we argue that SIGIR’s current scope for the track is too small: it only considers papers revisiting
prior techniques and expanding their study (e.g., does this method work on additional problems, or
under altered conditions/constraints?). Reproducibility tracks should encourage research into the
questions of reproducibility itself: user studies, incentive structures, and generally accept a broader
scope of work. This is necessary because the science of quantiﬁed and well-studied reproducibility
receives little attention across all ﬁelds of study. This is also a need as we observe key work re-
visiting fundamental foundations of our ﬁeld yet appearing only on arXiv (Recht et al., 2019; 2018;
Ahn et al., 2022) or outside of our top publishing venues (Barz & Denzler, 2020). By creating re-
producibility tracks with a broader scope, we can immediately create stronger incentives for this
needed work.

Second, and related to the prior concern, is the lack of guidelines given to reviewers for evaluating
reproducibility. If we do not state explicitly what is and is-not acceptable, we only increase the
noise ﬂoor of acceptance and of reproducible work. Of primary concern is that we rely too heavily
on intuitions about what will improve reproducibility because we lack well-deﬁned measurements.
We must instruct reviewers to use a standard that requires quantiﬁcation, even if imperfect, to elevate
the literature from hopes to science. For example, Raff (2021) uses imperfect data with censored
labels to study time to implement an algorithm, but adjusts the model to address data limitations.
Anecdotally, we report as a reviewer in the ﬁrst NeurIPS Datasets and Benchmarks track, multiple
reviewers complain of a lack of novel algorithms, for a track that is explicitly not about algorithms,
and for which no reviewer guidelines about what qualities or desiderata should be included in an
accepted paper for the new track. Similarly we point to Tran et al. (2020) which showed bias in
the acceptance process of OpenReview ICLR papers, with “The main argument for rejection is the
the analysis done in the paper is not typical of ICLR research”8. We consider these cases tragic

7https://sigir.org/sigir2022/call-for-reproducibility-track-papers/
8https://openreview.net/forum?id=Cn706AbJaKW

4

ML Evaluation Standards Workshop at ICLR 2022

in that no explicit instructions appear to exist both around replication, self-study as a ﬁeld, or the
ability to accept work so novel that it does not ﬁt our existing mold. Reproducible research need
not be “novel” in method, require proofs, or advanced math — its criteria should be quantiﬁed
evidence toward any aspect of how (non)reproducible work gets accepted, encouraged, propagated,
discovered, and any other aspect that would reasonably relate to the question of reproducibility. If
we can’t accept quantiﬁed criticism of our ﬁeld and institutions, we are lost as a scientiﬁc discipline.

REFERENCES

Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin
Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale Machine
Learning on Heterogeneous Distributed Systems. arXiv:1603.04467v2, pp. 19, 3 2016. URL
http://download.tensorflow.org/paper/whitepaper2015.pdfhttp://arxiv.org/abs/1603.04467.

Kwangjun Ahn, Prateek Jain, Ziwei Ji, Satyen Kale, Praneeth Netrapalli, and Gil I. Shamir. Re-
pp. 1–51, 2022. URL

producibility in Optimization: Theoretical Framework and Limits.
http://arxiv.org/abs/2202.04598.

Ethem Alpaydin. Combined 5 × 2 cv F Test for Comparing Supervised Classiﬁcation Learning
Algorithms. Neural Comput., 11(9):1885–1892, 11 1999.
ISSN 0899-7667. doi: 10.1162/
089976699300016007. URL http://dx.doi.org/10.1162/089976699300016007.

Bj¨orn Barz and Joachim Denzler. Do We Train on Test Data? Purging CIFAR of Near-Duplicates.
Journal of Imaging, 6(6):41, 6 2020. ISSN 2313-433X. doi: 10.3390/jimaging6060041. URL
http://arxiv.org/abs/1902.00423https://www.mdpi.com/2313-433X/6/6/41.

Alessio Benavoli, Giorgio Corani, and Francesca Mangili. Should We Really Use Post-Hoc Tests
Journal of Machine Learning Research, 17(5):1–10, 2016. URL

Based on Mean-Ranks?
http://jmlr.org/papers/v17/benavoli16a.html.

Siddharth Bhat.

Everything you know about word2vec is wrong,

2019.

URL

https://bollu.github.io/everything-you-know-about-word2vec-is-wrong.html.

In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of

Xavier Bouthillier, C´esar Laurent, and Pascal Vincent. Unreproducible Research is Repro-
the
ducible.
36th International Conference on Machine Learning, volume 97 of Proceedings of Ma-
chine Learning Research, pp. 725–734, Long Beach, California, USA, 2019. PMLR. URL
http://proceedings.mlr.press/v97/bouthillier19a.html.

Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Troﬁmov, Brennan Nichyporuk, Justin
Szeto, Naz Sepah, Edward Raff, Kanika Madan, Vikram Voleti, Samira Ebrahimi Kahou, Vincent
Michalski, Dmitriy Serdyuk, Tal Arbel, Chris Pal, Ga¨el Varoquaux, and Pascal Vincent. Account-
ing for Variance in Machine Learning Benchmarks. In Machine Learning and Systems (MLSys),
2021. URL http://arxiv.org/abs/2103.03098.

Andrew P. Bradley. The use of the area under the ROC curve in the evaluation of machine
ISSN 00313203. doi:

learning algorithms. Pattern Recognition, 30(7):1145–1159, 1997.
10.1016/S0031-3203(96)00142-2.

Jeffrey C. Carver, Richard P. Kendall, Susan E. Squires, and Douglass E. Post.

Soft-
ware Development Environments for Scientiﬁc and Engineering Software: A Series of
In 29th International Conference on Software Engineering (ICSE’07), pp.
Case Studies.
550–559. IEEE, 5 2007.
URL
https://ieeexplore.ieee.org/document/4222616/.

10.1109/ICSE.2007.77.

ISBN 0-7695-2828-7.

doi:

5

ML Evaluation Standards Workshop at ICLR 2022

Gavin C Cawley and Nicola L C Talbot.

and Subsequent Selection Bias
chine Learning Research,
http://dl.acm.org/citation.cfm?id=1756006.1859921.

in Performance Evaluation.
8 2010.

11:2079–2107,

On Over-ﬁtting in Model Selection
of Ma-
URL

Journal
ISSN 1532-4435.

Jon F. Claerbout and Martin Karrenbach.

search a new meaning.
604. Society of Exploration Geophysicists, 1 1992.
http://library.seg.org/doi/abs/10.1190/1.1822162.

Electronic documents give reproducible re-
In SEG Technical Program Expanded Abstracts 1992, pp. 601–
URL
doi:

10.1190/1.1822162.

Janez Demˇsar.

nal of Machine Learning Research, 7:1–30, 12 2006.
http://dl.acm.org/citation.cfm?id=1248547.1248548.

Statistical Comparisons of Classiﬁers over Multiple Data Sets.
ISSN 1532-4435.

Jour-
URL

Rotem Dror, Gili Baumer, Marina Bogomolov, and Roi Reichart. Replicability Analysis for
Natural Language Processing: Testing Signiﬁcance with Multiple Datasets. Transactions of
the Association for Computational Linguistics, 5:471–486, 11 2017.
ISSN 2307-387X. doi:
10.1162/tacl{\ }a{\ }00074. URL https://doi.org/10.1162/tacl_a_00074.

Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. The Hitchhiker’s Guide to Test-
In Proceedings of the 56th An-
ing Statistical Signiﬁcance in Natural Language Processing.
nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.
1383–1392, Melbourne, Australia, 7 2018. Association for Computational Linguistics. doi:
10.18653/v1/P18-1128. URL https://aclanthology.org/P18-1128.

Rotem Dror, Segev Shlomov, and Roi Reichart. Deep Dominance - How to Properly Compare Deep
Neural Models. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pp. 2773–2785, Florence, Italy, 7 2019. Association for Computational Linguistics.
doi: 10.18653/v1/P19-1266. URL https://aclanthology.org/P19-1266.

Chris Drummond. Replicability is not reproducibility: nor is it good science. In Proceedings of the
Evaluation Methods for Machine Learning Workshop at the 26th ICML, Montreal, Canada,2009,
Evaluation Methods for Machine Learning Workshop, the 26th ICML, June 14-18, 2009, Mon-
treal, Canada, 2009.

Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Jacob Steinhardt, and Alek-
sander Madry. Identifying Statistical Bias in Dataset Replication. In Hal Daum´e III and Aarti
Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research, pp. 2922–2932, Virtual, 2020. PMLR. URL
http://proceedings.mlr.press/v119/engstrom20a.html.

Jessica Forde, Tim Head, Chris Holdgraf, Yuvi Panda, Fernando Perez, Gladys Nalvarte, Benjamin
Ragan-kelley, and Erik Sundell. Reproducible Research Environments with repo2docker.
In
Reproducibility in ML Workshop, ICML’18, 2018a.

Jessica Zosa Forde, Matthias Bussonnier, F´elix-Antoine Fortin, Brian E Granger, Timothy Daniel
Head, Chris Holdgraf, Paul Ivanov, Kyle Kelley, Michael D Pacer, Yuvi Panda, Fernando P´erez,
Gladys Nalvarte, Benjamin Ragan-Kelley, Zachary R Sailer, Steven Silvester, Erik Sundell, and
Carol Willing. Reproducing Machine Learning Research on Binder. In Machine Learning Open
Source Software 2018: Sustainable communities, 2018b.

Josh Gardner, Christopher Brooks, and Ryan S Baker. Enabling End-To-End Machine Learning
Replicability : A Case Study in Educational Data Mining. In Reproducibility in ML Workshop,
ICML’18, 2018.

Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,
Hal Daum´e Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64
(12):86–92, 2021. ISSN 15577317. doi: 10.1145/3458723.

R Stuart Geiger, Dominique Cope, Jamie Ip, Marsha Lotosh, Aayush Shah, Jenny Weng,
“Garbage in, garbage out” revisited: What do machine learning ap-
Quantitative Science Stud-
doi: 10.1162/qss{\ }a{\ }00144. URL

and Rebekah Tang.
plication papers report about human-labeled training data?
ies, 2(3):795–827, 11 2021.
https://doi.org/10.1162/qss_a_00144.

ISSN 2641-3337.

6

ML Evaluation Standards Workshop at ICLR 2022

Yoav Goldberg and Omer Levy. word2vec Explained: Deriving Mikolov et al.’s Negative-
URL

arXiv preprint arXiv:1402.3722, 2014.

Sampling Word-Embedding Method.
http://arxiv.org/abs/1402.3722.

Jes´us M Gonz´alez-Barahona and Gregorio Robles. On the Reproducibility of Empirical Software
Engineering Studies Based on Data Retrieved from Development Repositories. Empirical Softw.
Engg., 17(1–2):75–89, 2 2012.
ISSN 1382-3256. doi: 10.1007/s10664-011-9181-9. URL
https://doi.org/10.1007/s10664-011-9181-9.

Charles R Harris, K Jarrod Millman, St´efan J van der Walt, Ralf Gommers, Pauli Virtanen, David
Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, Robert Kern, Matti Pi-
cus, Stephan Hoyer, Marten H van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern´andez del
R´ıo, Mark Wiebe, Pearu Peterson, Pierre G´erard-Marchant, Kevin Sheppard, Tyler Reddy, Warren
Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E Oliphant. Array programming with
NumPy. Nature, 585(7825):357–362, 2020. ISSN 1476-4687. doi: 10.1038/s41586-020-2649-2.
URL https://doi.org/10.1038/s41586-020-2649-2.

L. Hatton. The quality and reliability of scientiﬁc software. Transactions on Information and Com-

munications Technologies, 4, 1993.

L. Hatton. The T experiments: errors in scientiﬁc software.

and Engineering, 4(2):27–38, 1997.
http://ieeexplore.ieee.org/document/609829/.

ISSN 10709924.

IEEE Computational Science
doi: 10.1109/99.609829. URL

L. Hatton and A. Roberts. How accurate is scientiﬁc software?

ware Engineering, 20(10):785–797, 1994.
http://ieeexplore.ieee.org/document/328993/.

IEEE Transactions on Soft-
ISSN 00985589. doi: 10.1109/32.328993. URL

Anna A Ivanova, Shashank Srikant, Yotaro Sueoka, Hope H Kean, Riva Dhamala, Una-May
O’Reilly, Marina U Bers, and Evelina Fedorenko. Comprehension of computer code relies pri-
marily on domain-general executive brain regions. eLife, 9:e58906, 2020. ISSN 2050-084X. doi:
10.7554/eLife.58906. URL https://doi.org/10.7554/eLife.58906.

Diederik P Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimization. In Interna-

tional Conference On Learning Representations, 2015.

Thomas Kluyver, Benjamin Ragan-Kelley, Fernando P´erez, Brian Granger, Matthias Bussonnier,
Jonathan Frederic, Kyle Kelley, Jessica Hamrick, Jason Grout, Sylvain Corlay, Paul Ivanov,
Dami´an Avila, Saﬁa Abdalla, Carol Willing, and Jupyter development team. Jupyter Notebooks -
a publishing format for reproducible computational workﬂows. In Fernando Loizides and Birgit
Scmidt (eds.), Positioning and Power in Academic Publishing: Players, Agents and Agendas, pp.
87–90. IOS Press, 2016. URL https://eprints.soton.ac.uk/403913/.

Quoc Le and Tomas Mikolov. Distributed Representations of Sentences and Documents. In Eric P
Xing and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine Learn-
ing, volume 32 of Proceedings of Machine Learning Research, pp. 1188–1196, Bejing, China,
2014. PMLR. URL https://proceedings.mlr.press/v32/le14.html.

Stanley Letovsky.
of

Cognitive

processes

in

program comprehension.

Systems

nal
1212.
https://www.sciencedirect.com/science/article/pii/016412128790032X.

https://doi.org/10.1016/0164-1212(87)90032-X.

7(4):325–339,

Software,

1987.

ISSN

doi:

and

Jour-
0164-
URL

Zachary C. Lipton and Jacob Steinhardt. Troubling trends in machine-learning scholarship. Queue,

17(1):1–33, 2019. ISSN 15427749. doi: 10.1145/3317287.3328534.

Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization.
Mathematical Programming, 45(1):503–528, 1989. ISSN 1436-4646. doi: 10.1007/BF01589116.
URL https://doi.org/10.1007/BF01589116.

Ilya Loshchilov and Frank Hutter.

Decoupled Weight Decay Regularization.

International Conference
Learning Representations
on
https://github.com/loshchil/AdamW-and-SGDW.

(ICLR),

2019.

In
URL

7

ML Evaluation Standards Workshop at ICLR 2022

Tomas Mikolov, Greg Corrado, Kai Chen, and Jeffrey Dean. Efﬁcient Estimation of Word Repre-
sentations in Vector Space. Proceedings of the International Conference on Learning Representa-
tions (ICLR 2013), pp. 1–12, 2013. ISSN 15324435. doi: 10.1162/153244303322533223. URL
http://arxiv.org/pdf/1301.3781v3.pdf.

Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben
Inioluwa Deborah Raji, and Timnit Gebru. Model Cards
Hutchinson, Elena Spitzer,
the Conference on Fairness, Accountability,
for Model Reporting.
and Transparency, FAT* ’19, pp. 220–229, New York, NY, USA, 2019. Association for
doi: 10.1145/3287560.3287596. URL
Computing Machinery.
https://doi.org/10.1145/3287560.3287596.

ISBN 9781450361255.

In Proceedings of

Curtis G. Northcutt, Anish Athalye, and Jonas Mueller.

Pervasive Label Errors in Test
In 35th Conference on Neural Informa-
Sets Destabilize Machine Learning Benchmarks.
tion Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks, 2021. URL
http://arxiv.org/abs/2103.14749.

E. Nurvitadhi, Wing Wah Leung, and C. Cook.

gram understanding?
ume 1. IEEE, 2003.
http://ieeexplore.ieee.org/document/1263332/.

Do class comments aid java pro-
In 33rd Annual Frontiers in Education, 2003. FIE 2003., vol-
URL
ISBN 0-7803-7961-6.

10.1109/FIE.2003.1263332.

doi:

Michela Paganini and Jessica Zosa Forde.

producible Machine Learning Experiment Orchestration.
http://arxiv.org/abs/2006.07484.

dagger: A Python Framework for Re-
arXiv,
URL

2020.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
PyTorch: An Imperative
Style, High-Performance Deep Learning Library.
In H Wallach, H Larochelle, A Beygelz-
imer, F d\textquotesingle Alch´e-Buc, E Fox, and R Garnett (eds.), Advances in Neural
Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL
http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.

D. N. Perkins and Rebecca Simmons.

for Science, Math, and Programming.

Model
(3):303–326, 9 1988.
http://journals.sagepub.com/doi/10.3102/00346543058003303.

Patterns of Misunderstanding: An Integrative
Review of Educational Research, 58
URL
10.3102/00346543058003303.

ISSN 0034-6543.

doi:

Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan Rosenthal, Lin Tan,
Yaoliang Yu, and Nachiappan Nagappan. Problems and Opportunities in Training Deep Learning
Software Systems: An Analysis of Variance. In Proceedings of the 35th IEEE/ACM International
Conference on Automated Software Engineering, ASE ’20, pp. 771–783, New York, NY, USA,
2020. Association for Computing Machinery.
ISBN 9781450367684. doi: 10.1145/3324884.
3416545. URL https://doi.org/10.1145/3324884.3416545.

Hans E Plesser. Reproducibility vs. Replicability: A Brief History of a Confused Terminology. Fron-
tiers in neuroinformatics, 11:76, 1 2018. ISSN 1662-5196. doi: 10.3389/fninf.2017.00076. URL
https://pubmed.ncbi.nlm.nih.gov/29403370https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5778115/.

Russell A.

Poldrack.

The Costs

1 2019.

14,
https://linkinghub.elsevier.com/retrieve/pii/S0896627318310390.

10.1016/j.neuron.2018.11.030.

ISSN 08966273.

of Reproducibility.
doi:

Neuron,

101(1):11–
URL

Edward Raff. A Step Toward Quantifying Independently Reproducible Machine Learning Research.

In NeurIPS, 2019. URL http://arxiv.org/abs/1909.06674.

Edward Raff. Research Reproducibility as a Survival Analysis. In The Thirty-Fifth AAAI Conference

on Artiﬁcial Intelligence, 2021. URL http://arxiv.org/abs/2012.09932.

Edward Raff. Does the Market of Citations Reward Reproducible Work? In ML Evaluation Stan-

dards Workshop at ICLR 2022, 2022.

8

ML Evaluation Standards Workshop at ICLR 2022

Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt,

CIFAR-10 Classiﬁers Generalize to CIFAR-10?
http://arxiv.org/abs/1806.00451.

and Vaishaal Shankar.
arXiv, pp. 1–25, 2018.

Do
URL

Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classiﬁers
Generalize to ImageNet? arXiv, 2 2019. URL http://arxiv.org/abs/1902.10811.

Carl

Taswell.

Reproducibility
report, UCSD School

Standards
of Medicine, La

for Wavelet

Transform Algorithms.
URL

1998.

Jolla, CA,

Technical
http://www.toolsmiths.com/docs/CT199801.pdf.

David Tran, Alex Valtchanov, Keshav Ganapathy, Raymond Feng, Eric Slud, Micah Goldblum, and
Tom Goldstein. An Open Review of OpenReview: A Critical Analysis of the Machine Learning
Conference Review Process. arXiv, 2020. URL http://arxiv.org/abs/2010.05137.

Wei Yuan and Kai-xin Gao. EAdam Optimizer: How $\epsilon$ Impact Adam. arXiv.

Matei A Zaharia, Andrew Chen, Aaron Davidson, Ali Ghodsi, Sue Ann Hong, Andy Konwinski,
Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Fen Xie, and Corey Zumar.
Accelerating the Machine Learning Lifecycle with MLﬂow. IEEE Data Eng. Bull., 41:39–45,
2018.

Donglin Zhuang, Xingyao Zhang, Shuaiwen Leon Song, and Sara Hooker. Randomness In
arXiv, 2021. URL

Neural Network Training: Characterizing The Impact of Tooling.
http://arxiv.org/abs/2106.11872.

9

