2
2
0
2

g
u
A
8

]
L
M

.
t
a
t
s
[

1
v
2
1
1
4
0
.
8
0
2
2
:
v
i
X
r
a

A review on longitudinal data analysis with
random forest in precision medicine

Jianchang Hu, Silke Szymczak∗
Institute of Medical Biometry and Statistics, University of L¨ubeck, Germany

Abstract

Precision medicine provides customized treatments to patients based
on their characteristics and is a promising approach to improving treat-
ment eﬃciency. Large scale omics data are useful for patient charac-
terization, but often their measurements change over time, leading to
longitudinal data. Random forest is one of the state-of-the-art machine
learning methods for building prediction models, and can play a crucial
role in precision medicine. In this paper, we review extensions of the stan-
dard random forest method for the purpose of longitudinal data analysis.
Extension methods are categorized according to the data structures for
which they are designed. We consider both univariate and multivariate
responses and further categorize the repeated measurements according
to whether the time eﬀect is relevant. Information of available software
implementations of the reviewed extensions is also given. We conclude
with discussions on the limitations of our review and some future research
directions.

Keywords machine learning; repeated measurements; clustered data; mul-

tivariate response; longitudinal data

1

Introduction

The goal of precision medicine is to provide customized treatments to patients
based on their characteristics and thus to improve treatment eﬃciency while
avoiding serious side eﬀects [4, 25, 35]. With recent technological advances,
large scale genetic and other molecular data can now be collected. Along with
demographic and clinical proﬁles they characterize each patient under diﬀerent
aspects. Typical omics data include gene expression, methylation status, protein
or metabolite levels, and microbiome abundances. Many of these measurements,
however, change over time, often depending on disease activity, treatment, co-
morbidities and other environmental factors. Consequently, it is important to

∗Corresponding author. Institute of Medical Biometry and Statistics, University of L¨ubeck,

Ratzeburger Allee 160, 23562 L¨ubeck, Germany. Email: silke.szymczak@uni-luebeck.de

1

 
 
 
 
 
 
measure them for the same patient repeatedly over time, and this leads to longi-
tudinal data, where a single observation captures the measurements at a speciﬁc
time point for a patient.

Depending on the research question, the study design and the outcome
of interest, multiple longitudinal data formats can be envisioned. Predictors
might be available for a single time point only, such as at baseline visit, or are
time-invariant, which is the case for genetic variants. Alternatively, predictors
are measured multiple times during a study; for instance, gene expression or
metabolites are measured from multiple blood samples and microbiome abun-
dance is obtained from stool samples collected during a couple of visits. Simi-
larly, the outcome can be determined at a single time point. Examples include
response to treatment at the end of therapy or after a pre-speciﬁed follow-up
time. But it might also be of interest to predict the outcome over time such
as disease activity or severity. Furthermore, the data format is related to the
study design where the same number of measurements at ﬁxed time points is
taken for each subject or data from a varying number of irregularly spaced time
points are available; the latter is often encountered in observational studies.

In general, a longitudinal data set can be formatted as in Table 1. Here
in total, there are N subjects, for each of them, ni, i = 1, . . . , N observations
are measured, and each observation consists of measurements on m response
variables and p predictors.

Table 1: General structure of longitudinal data

Subject Observation/Time

1
1
·
1
...
N
N
·
N

1
2
·
n1
...
1
2
·
nN

Responses
. . .
. . .
·
. . .
...
. . .
. . .
·
. . .

y111
y121
·
y1n11
...
yN 11
yN 21
·
yN nN 1

y11m
y12m
·
y1n1m
...
yN 1m
yN 2m
·

x111
x121
·
x1n11
...
xN 11
xN 21
·

Predictors
. . .
. . .
·
. . .
...
. . .
. . .
·
. . .

x11p
x12p
·
x1n1p
...
xN 1p
xN 2p
·
xN n1p

yN nN m xN n11

Analyzing longitudinal data is not an easy task. The most distinct feature
of longitudinal data is the repeated measurements from the same subject. This
inevitably leads to clustered and correlated observations. The clustering eﬀect
is due to individual characteristics. For instance, average response to a drug
could vary from patient to patient. In the meantime, if repeated measurements
are collected over a period of time, then there could be serial correlations among
measurements.

Furthermore the observation time for the longitudinal data can be either
equally-spaced or irregularly-spaced, which may aﬀect the approach that can
be used for the analysis. Visits at every other month would lead to equally-

2

spaced observations, while following up at 6 months, 1 year and 2 years after the
treatment provides an example of irregularly-spaced observations. Additionally,
irregular spacing can also occur in observational studies when there are no pre-
speciﬁed follow-up times. One example is the electronic health record data from
patient care. Apart from that, missing values in the repeated measurements are
likely to be present. This loss of observations can occur, for instance, when
patients are not interested in the follow-up investigations. These missing data
pose great challenges to the analysis. They can easily turn an equally-spaced
observation schedule into irregular. More importantly, they may carry vital
information when the missingness could be related to the value of the variable,
which can distort the analysis results if not handled carefully. More discussions
on the characteristics of longitudinal data can be found in the classic textbooks
[15, 21].

Despite the diﬃculties introduced by longitudinal data, they bring rich in-
formation. With longitudinal data, clinicians can better understand disease
progression, especially of chronic diseases, so that patients can be properly strat-
iﬁed and treatment plans can be tailored accordingly [27, 31, 59]. Furthermore,
repeated measurements allow the patients’ treatment responses to be captured
more accurately, so that eﬀective therapies can be implemented and evaluated.
In order to serve the purpose of precision medicine, the development of
prediction models with longitudinal data using statistical or machine learning
approaches is crucial [28]. These models, on the one hand, can be applied to
predict the current status of an individual, i.e. to evaluate if a speciﬁc condition
is present (diagnostic setting). On the other hand, they are useful in forecasting
if a speciﬁc event will occur in the future (prognostic setting). One example
would be to predict the future disease course, including the probability of a
remission or relapse and the need for therapy changes or intensiﬁcation.

One of the state-of-the-art machine learning methods for the development
of prediction models is the random forest (RF) algorithm [5, 9]. It is a non-
parametric approach that can accommodate diﬀerent types of responses such as
categorical or quantitative outcomes and survival times [24]. Moreover, it can
work with predictors of various scales or distributions and is suited for applica-
tions in high-dimensional settings where the number of predictors can be larger
than the number of observations [9, 10]. Thus, it is very suitable for analyzing
omics data which are often high-dimensional, plus metabolite and protein levels
are usually skewed and left censored by limits of detection, and microbiome
abundances often exhibit an excess of zeros. Furthermore, tree-based methods
form data-driven subgroups of samples which can be beneﬁcial for patient strat-
iﬁcation. Via the so-called variable importance measures, the method can also
highlight the relevance of each predictor [5]. This could be especially handy
for pharmacogenomics [37, 43], where potential genetic variants associated with
drug response phenotypes such as drug eﬃcacy and adverse side eﬀects can be
identiﬁed. In fact, Svetnik et al. (2004) demonstrated that the classiﬁcation
and regression tree (CART, [6]) is more powerful in the drug discovery process
compared with conventional methods such as partial least squares and support
vector machine [53].

3

However, as with other machine learning methods, the RF algorithm as-
sumes that observations are independently sampled from a population. This is
unfortunately not the case in longitudinal studies where, as we have pointed
out, multiple measurements for the same subject are often collected at diﬀerent
time points. Conducting statistical analysis on longitudinal data without con-
sidering the dependency among observations could lead to biased inference due
to underestimated standard errors in linear models [41] and spurious subgroup
identiﬁcation and inaccurate variable selection in tree-based methods [16, 47].
Therefore, in this review, we will present a range of extensions of the stan-
dard RF algorithm for the analysis of longitudinal data. We limit our attention
to CART-based RF with a focus on prediction of categorical and quantitative
outcomes. Our review is structured as follows.
In the section 2 we consider
the case where the response variable is univariate. Here we start with a short
review on the standard RF algorithm in subsection 2.1. Following that, in sub-
section 2.2 the case of repeated measurements or clustered data is investigated.
For such data type, the ordering of the observations by time is ignored in the
analysis. Subsection 2.3 then presents methods that incorporate time eﬀect
into modeling. Section 3 focuses on extensions of RF algorithm suitable for
multivariate responses. Several extensions are also able to analyze univariate
longitudinal data because with suitable transformations the latter can be turned
into the multivariate case. After that, we provide information on the currently
available implementations of the reviewed methods in section 4. We conclude
with a discussion in section 5.

2 Univariate response longitudinal data

We start with the simple scenario where the response variable is univariate; that
is m = 1 in Table 1. We ﬁrst brieﬂy review the standard random forest algorithm
as a prediction model, and point out the need for extension in order to better
serve the purpose of longitudinal data analysis. Several RF extension methods
are then presented and discussed. As we mentioned in the previous section,
the most distinct feature of longitudinal data is the repeated measurements,
often collected at diﬀerent time points. Therefore, we categorize these extension
methods by their ways of incorporating the time eﬀects.

2.1 Standard random forest algorithm

In this section, we give a brief description on the standard random forest al-
gorithm based on CART. More detailed descriptions and discussions on CART
and RF can be found in [5, 6, 57].

Random forest is an ensemble of decision trees where each tree is built from
a bootstrapped version of the training data set. Each tree is grown via the
principle of repetitive partition where starting from the root node, the same
node splitting procedure is applied repetitively until certain stopping rules are
met. The main guiding principle for node splitting is to minimize the impurity

4

of response variable in each node of the tree. The impurity of one node is often
measured by the Gini index if the response variable is categorical or by the
variance if it is quantitative. For a binary decision tree such as CART, the node
splitting process consists of selecting a splitting variable and determining the
splitting rule. To choose the splitting variable of a given node, ﬁrstly, a subset
of predictors are randomly selected. For each predictor Xi, splitting rules in
the form of {Xi > s} are investigated for all possible cut-oﬀ values s. The rule
{Xi > s∗} leading to the maximal impurity reduction of the split becomes a
candidate split. Then among all considered predictors, the one with the best
candidate split, in terms of the impurity decrease of the split, is selected as the
splitting variable and the associated candidate split is the splitting rule of the
node. The growth of each decision tree ends if the nodes to split are already
pure (all samples within the node come from the same class or have the same
response value) or other pre-determined stopping rules are met (e.g., minimum
sample size constrain). The nodes in the ﬁnal layer of a tree are called leaves
and are used for prediction of new observations.

To make prediction with RF, an observation goes through every decision
tree in the forest. In each constructed tree, the observation follows the splitting
rules and lands in one leaf which predicts its class membership or response value
depending on whether it is classiﬁcation or regression. The ﬁnal prediction for
the observation is made either by majority voting or averaging, based on results
from all decision trees in the forest.

Because the RF algorithm uses bootstrap samples to grow each decision tree,
some observations are left out in the construction of a given tree. By treating
these out-of-bag (OOB) samples as observations needed to be predicted, it can,
therefore, provide an estimate of prediction error of the constructed forest.

RF algorithm has been used in many ﬁelds including genetic epidemiology,
bioinformatics and precision medicine. Its power in prediction comes from the
aggregation of many weaker learners. The performance is especially good if the
correlations between trees in the forest are low [5]. In addition, the so-called
variable importance measure can be obtained for each predictor, which measures
its relevance to prediction. Thus, for high-dimensional dataset such as omics
data, variable selection procedures based on variable importance measure are
possible (see [13] and the reference therein for a description and comparison of
various variable selection procedures based on variable importance measure).

However, one disadvantage of RF is its lack of interpretability. Unlike a
single decision tree, the result from a forest is hard to interpret. Even though
the variable importance measure can help pinpoint inﬂuential predictors, but
how these important variables work together is unclear.

Although it is possible to directly utilize the standard random forest algo-
rithm for longitudinal data analysis, it may suﬀer from several problems. The
longitudinal data by nature has a clustered structure. When standard RF algo-
rithm is used directly for analysis, as shown in Figure 1, bootstrapped samples
may have a high chance to include observations from every subject. This may
cause correlated or even homogeneous trees to deteriorate the prediction per-
formance. In addition, the estimated prediction error based on OOB samples

5

is often too optimistic due to the high similarity between the observations from
the same subject [26].

Figure 1: Illustration of bootstrap samples used to construct decision trees in
standard RF when it is applied to clustered data.

Moreover, it has also been reported that ignoring such clustered structure
for tree-based methods could result in the detection of spurious subgroups and
inaccurate predictor variable selection [34, 47]. Therefore, there is a need to
build extensions of standard RF for longitudinal data analysis. Because a RF is
an ensemble of systematically constructed decision trees, the extensions below
mainly focus on the modiﬁcations of decision tree construction to better ﬁt the
longitudinal data.

2.2 Clustered data

In some applications, the observations are made repetitively on the same subject
as duplications. For example, gene expression or metabolites are measured from
multiple blood samples and microbiome abundance is obtained from multiple
stool samples in a single visit. This results in clustered data setting. In such
setting, the data still follow the general format shown in Table 1, but there is
hardly any time eﬀect. In other words, the ordering of the observations from
the same subject can be ignored and is not considered in training the prediction
model. Hence, one model for clustered data can be written as follows.

yij = µi + εij,

(1)

where µi reﬂects the mean value of subject i, and εij are random ﬂuctuations
with mean 0 and independent from each other for all j = 1, . . . , ni and across all

6

i = 1, . . . , N . The clustering eﬀect, therefore, is the consequence of the shared
mean value for observations from the same subject.

2.2.1 Averaging

One intuitive approach to deal with the aforementioned clustering eﬀect of re-
peated measurements is to take the average of replicated data for each sub-
ject. This then brings the data structure back to the usual one-subject-one-
observation scenario and retain the needed independence for standard RF algo-
rithm. Vlahou et al. (2004) [54] takes this approach to analyze mass spectrom-
etry data for protein proﬁling in urine.

Despite the simplicity, this approach suﬀers from a loss of information. The
intra-class variation is averaged out. Moreover, this approach also masks the
imbalance design. Diﬀerent subjects could contribute diﬀerent numbers of ob-
servations in the original data set, as in Table 1, ni could be diﬀerent for
i = 1, . . . , N , which may be due to some characteristics of the subjects and
may carry underlying distributional information. However, after averaging out
the repeated measurements, each subject now makes equal contribution to the
training data set. This can have potential eﬀects on the prediction eﬃciency
and variable selection. Karpievitch et al. (2009) [26] showed that this approach,
when compared with the standard RF, is more sensitive to the total number of
subjects N ; reduction in N leads to poorer prediction and variable selection.
The averaging approach may also be diﬃcult to use when classiﬁcation and cat-
egorical predictors are concerned. For a given patient, his/her cholesterol level
based on diﬀerent blood samples may vary which could lead to diﬀerent cate-
gorization; one observation falls into normal level and another belongs to high
level. The averaging approach needs to average all observations of this pati-
tent to end up with a subject-level measurement for analysis. However, when
diﬀerent observation-level measurements from the same subject fall in diﬀerent
categories, this averaging would be impossible for categorical variables.

2.2.2 Subject-level bootstrapping

To overcome the disadvantages that averaging approach have, extensions that
can utilize all observations are needed. But as we pointed out in Section 2.1, the
standard bootstrapping strategy in RF construction can lead to correlated or
even homogeneous trees, which devastatingly hurts the prediction performance
of standard RF. Also the prediction error based on OOB samples is under-
estimated due the similarity between in-bag and out-of-bag data.

To tackle these issues while using all observations, Karpievity et al. (2009)
[26] proposed the subject-level bootstrapping strategy to replace the original
one, and the resulting algorithm is named as RF++. Speciﬁcally, when build-
ing the bootstrap sample to construct a single decision tree in a random forest,
instead of re-sampling at the observation level, as shown in Figure 2, boot-
strap re-sampling at the subject level is performed and all observations from
the selected subjects are included as in-bag observations. Adler et al. (2011a)

7

Figure 2: Illustration of subject-level bootstrap samples used to construct deci-
sion trees in RF++.

and Adler et al. (2011b) [2, 3] further extended this idea to a two-stage boot-
strapping strategy. Firstly, one subject i is chosen randomly and all associated
observations are in bag. Afterwards for each chosen subject the training sam-
ples are chosen by randomly selecting one observation from all ni. Adler et
al. (2011b) [3] showed in their simulation studies that subject-level resampling
based on one observation per subject yields the best prediction results compared
to the standard RF, averaging approach and RF++, although one should also
notice that diﬀerent settings may lead to diﬀerent results and there could be
cases where the other methods are more preferable.

The adoption of subject-level bootstrapping avoids the problem of poten-
tially exposing individual trees to all subjects. The two-stage bootstrapping
strategy could further mitigate the negative eﬀect the intra-cluster correlation
casts on the prediction performance; when only one observation per subject is
selected, even though the same subject might be used in construction of diﬀer-
ent trees, likely diﬀerent observations are selected for the training of diﬀerent
trees, which further reduces the similarity between trees.

Besides the usual observation-level classiﬁcation, Karpievitch et al. (2009)
[26] showed that classiﬁcation at subject level is also possible. A majority vote
can be performed across the observations belonging to the same subject to result
in the subject classiﬁcation. With such results, a subject-level misclassiﬁcation
rate estimate based on OOB samples is also made possible. This information
may be more beneﬁcial and easier to interpret in clinical trials.

However, as pointed out in Hajjem et al. (2014) [19], the subject-level boot-
strapping only adjusts the sampling method for clustering, thus, no random
eﬀects are incorporated in the modeling as well as prediction. Furthermore, for

8

longitudinal studies where time plays a role, this strategy cannot fully utilize
the information contained in the data set.

2.3 Time eﬀects considered

For many research questions, not only values of predictors at the current time
point, but also from the past are helpful, sometimes even crucial, for a good
prediction performance. A large value of a particular biomarker might be rel-
evant if it had rather small values in the past, pointing to an early change on
the molecular level. Therefore, in this section, we would like to review several
RF extensions that take time eﬀects into consideration.

2.3.1 Historical RF

The historical RF is an approach that explicitly considers the history of predic-
tors [49]. Assume that we have training data {yij, tij, Xij}, i = 1, . . . , N and
j = 1, . . . , ni. Here yij denotes the response, Xij the vector of predictors and
tij the time of the j-th observation on the i-th subject. The method estimates
a model for the response yij using both (tij; Xij) (the observations concurrent
with yij) and all preceeding observations of the i-th subject up to (but not in-
cluding) time tij. Thus, for a time-varying predictor, its historical information
along with its current value are both used for modeling. For a time-invariant
predictor, of course, only its current value is used as in the standard RF.

For time-invariant predictors the standard splitting procedure described in
Section 2.1 is adopted when constructing each decision tree. In case of a time-
varying predictor, its historical information, i.e., values within a speciﬁc time
interval before the time concurrent with yij, is ﬁrst represented by a summary
function. One exemplary such function for subject i at time point j counts
the number of past observation values, including both response and predictor
variables, that are measured at a maximum of η1 units of time before the current
time point j and smaller than η2, i.e.,

sc(η; ¯zijk) =

(cid:88)

I(zilk < η2),

(2)

til∈[tij −η1,tij )

where ¯zij = {zil = (yil, xil) : til < tij} denotes the past observations of subject i
prior time j, and ¯zijk is its k-th component. This aggregation results in a single
number per observation and variable for a ﬁxed value of η = (η1, η2). Alternative
summary functions are the relative frequency of values above a certain threshold
or the mean. For each summary function, there is also a windowed versions
where the time interval considered is further limited by an upper bound, i.e.,
til ∈ [tij − η1, tij − η3) in equation (2). The diﬀerent functions usually lead to
similar prediction performance (personal communication). However, it should
be noted that only the frequency based functions are scale invariant which is
one of the properties that make the standard RF algorithm robust. Finally,
the partitioning at a particular node is performed using the predictor with the
smallest Gini impurity or sum-of-squares error for categorical or quantitative

9

response, respectively. However, determination of an optimal cut-oﬀ point for
time-varying predictors includes optimization of the parameters in the summary
function such as η which largely increases the computing expenses especially
when the number of time-varying predictors is large such as in some omics
datasets.

To mitigate the eﬀects of addition optimization of the parameters in the
summary function, Sexton and Laake [49, 48] incorporates an additional level
of randomization where instead of using all observations within the speciﬁed
time interval, only a sub-sample is randomly selected and used for optimizing
the cut-oﬀ point. In addition, subject-level re-sampling strategy is also adopted
in random forest construction. This not only enjoys the advantages mentioned
in Section 2.2.2, but also keeps the complete observation history of a subject.

As variable importance measure, Sexton and Laake [49] consider a delete-
variable approach. Speciﬁcally, to ﬁnd the importance measure of predictor k,
the prediction errors of historical RF models with and without this predictor are
calculated, and their diﬀerence gives the importance measure of the predictor.
This approach is computationally demanding. Furthermore, correlation among
predictor variables could aﬀect the importance measure since masking may make
important variables seem not to be important at all.

2.3.2 Extensions from (generalized) linear mixed eﬀects model

A diﬀerent approach to adjust for the longitudianl structure is to combine (gen-
eralized) linear mixed models ((G)LMMs) with the decision tree or RF algo-
rithm. The (G)LMM is a classic statistical methodology for the analysis of lon-
gitudinal or more general clustered data. As with (G)LMMs the predictors can
be constant or varying over time and diﬀerent time points are possible for each
subject. One advantage of (G)LMM is its explicit modeling of intra-subject
correlation structure as well as subject-level random eﬀects besides the main
ﬁxed eﬀects of interest. After properly adjusting for these random eﬀects and
correlation structure, the longitudinal data becomes conditionally independent,
thus the estimation of the ﬁxed eﬀect component of the model follows exactly
the same way as if independent observations were observed. Moreover, predic-
tion can now be generalized to a wider population. However, the drawbacks of
this approach include its computational complexity to ﬁt mixed eﬀects models
as well as the possibility to misspecify the intra-subject correlation structure.
More detailed descriptions, discussions and applications on classical methods
for longitudinal data analysis can be found in several textbooks, e.g. [15, 21].

The general idea of the RF extension from (G)LMM is to replace the linear
model of the ﬁxed eﬀect component by a tree or RF while keeping the model-
ing of the dependence structure with random eﬀects. Multiple algorithms have
been developed to incorporate the tree or RF into the (G)LMM and are summa-
rized in Table 2. As can be seen, most extensions are based on two approaches,
namely, MERT and RE-EM trees. Also for binary response, a Bayesian ap-
proach called BiMM has been proposed.

We ﬁrst describe approaches for a regression setting based on LMMs, fol-

10

Table 2: Overview of diﬀerent RF extensions from (G)LMM.
Tree

Outcome
Quantitative (Gaussian) MERT

Forest
MERF

Exponential family

Binary

SMERF
SREEMforest

RE-EM tree REEMforest
SMERT
SREEM tree
GMERT
GMET
BiMM tree

GMERF
BiMM forest

lowed by more general methods using GLMMs that can be employed in the
context of classiﬁcation but also for other types of outcomes such as count vari-
ables.

Quantitive (Gaussian) response variable

For a normally distributed quantitative outcome, the classic LMM model can
be written as

yi = Xiβ + Zibi + (cid:15)i,
bi ∼ N (0, D), (cid:15)i ∼ N (0, Ri),

where yi = (yi1, . . . , yini)(cid:48) is the ni × 1 vector of the outcome for the ni ob-
servations of subject i, Xi = [xi1, . . . , xini](cid:48) is the ni × p matrix of predictors
considered as ﬁxed eﬀects, Zi = [zi1, . . . , zini](cid:48) is the ni × q matrix of predictors
modeled as random eﬀects, (cid:15)i = ((cid:15)i1, . . . , (cid:15)ini)(cid:48) is the ni × 1 vector of random
errors, β is the p × 1 unknown vector of parameters of the ﬁxed eﬀects, and bi is
the q × 1 unknown vector of random eﬀects of subject i. Both bi and (cid:15)i are as-
sumed to follow a normal distribution with mean zero and covariance matrix D
and Ri, respectively. It is further assumed that they are independent and that
the observations between subjects are also independent. The parameters can
be estimated by maximum likelihood (ML) or restricted maximum likelihood
(REML) methods.

Two diﬀerent strategies have been proposed in the literature to replace the
linear dependency between the predictors and the outcome. The ﬁrst approach
is the mixed eﬀects regression tree/forest (MERT [18] and MERF [19]) where the
ﬁxed eﬀects are estimated using a standard regression tree or RF. Speciﬁcally,
the modiﬁed model can be written as

yi = f (Xi) + Zibi + (cid:15)i,
bi ∼ N (0, D), (cid:15)i ∼ N (0, Ri),

(3)

where it is further assumed that Ri = σ2Ini where In denotes the identity
matrix with size n, and the function f (Xi) is estimated by the standard tree or
RF. For model ﬁtting an expectation-maximization (EM) algorithm [29] is used

11

which iterates between estimation of the ﬁxed and random eﬀects components.
The general approach can be described as follows (slightly modiﬁed from [18]
and [19]):

1. Initialize with ˆbi = 0, ˆσ2 = 1, and ˆD = Iq;

2. Iterate through the following steps until convergence:

(a) Estimate a regression tree or RF based on the new outcome variable
ˆbi, and predictors Xi. Denote the predictions with ˆf (Xi);

y∗
i = yi −Zi

(b) Fit the linear random eﬀect model yi = ˆf (Xi) + Zibi + (cid:15)i.

The convergence is based on a generalized log-likelihood criterion

GLL(f, bi|y) =

N
(cid:88)

{[yi − f (Xi) − Zibi]T R−1

i

[yi − f (Xi) − Zibi]

i=1
i D−1bi + log |D| + log |Ri|}.
+ bT

This method assumes that the correlation is only due to between subject
variation, i.e. the covariance matrix Ri of the errors (cid:15)i is assumed to be diagonal.
The MERT approach uses a decision tree to estimate f (Xi), while the MERF
method improves prediction performance by considering a standard random
forest.
It can be noticed, that in MERF the bootstrap sample for each tree
is drawn on the observation level and predictions are based on the out-of-bag
sample to reduce the risk of overﬁtting. Note that resampling of individual
observations is possible in this setting since it is assumed that the correlation
between observations can be completely modeled by the random eﬀects. Thus,
using the modiﬁed outcome variable y∗

i results in independent observations.

The second approach was independently proposed in [47] and is called ran-
It still considers the
dom eﬀects expectation-maximization (RE-EM) trees.
model (3), but it does not directly use tree or RF algorithms to estimate the
ﬁxed eﬀects. Instead it considers the partition of samples formed by the regres-
sion tree and estimates local ﬁxed eﬀects within each partition while estimating
the random eﬀects globally. The algorithm is similar to MERT in using the
generalized log-likelihood as convergence criterion.

More speciﬁcally, for model ﬁtting, step 1 is the same as for MERT. Step 2

is modiﬁed as follows.

2. Iterate through the following steps until convergence:

(a) Estimate a regression tree or RF based on the new outcome variable
ˆbi, and predictors Xi. Construct indicator matrix Φi with
y∗
i = yi −Zi
size ni × T where Φi
jt = I(yij ∈ gt), I(·) is the indicator function and
gt is the t-th terminal node of the tree, and T is the total number of
terminal nodes;

12

(b) Fit the linear mixed eﬀects model

yi = Φiµ + Zibi + (cid:15)i,

where µ = (µ1, . . . , µT ) denotes the local ﬁxed eﬀects within each
terminal node.

The tree is thus only used to deﬁne the partition of the sample space and a
system of LMM models is ﬁtted with global random eﬀects and each partition
having its own local ﬁxed eﬀects. The lme function in the R package nlme is
employed for LMM model ﬁtting which allows a general within-subject corre-
lation structure; for instance, an autocorrelation structure within the errors is
possible so that Ri can be a non-diagonal matrix.

An extension of the RE-EM tree is called REEMforest where an ensemble
of RE-EM trees is generated for the ﬁxed eﬀects estimation [8]. The function
ˆf (Xi) is estimated by the mean of the K ﬁtted RE-EM trees:

ˆf (Xi) =

1
K

K
(cid:88)

k=1

Φi,k ˆµk,

where Φi,k is the ni × T indicator matrix based on the tree k and ˆµk is the T × 1
vector of ﬁtted local ﬁxed eﬀects from tree k.

One prominent feature of longitudinal data is its serial correlation within the
observations of the same subject. In order to model such a covariance structure
that varies over time, the MERT and RE-EM tree and their corresponding forest
variants have also been extended to include an additional stochastic component
[8]. The resulting approaches are correspondingly called SMERT, SREEMtree
etc. The model with the additional stochastic component can be written as
follows:

yi = f (Xi) + Zibi + ωi + (cid:15)i,

where ωi = (ωi(t1), . . . , ωi(tni))(cid:48) is a centered Gaussian process with Cov(ωi(s), ωi(t)) =
γ2Γ(s, t). The ωi(t) are independent for diﬀerent subjects i = 1, . . . , N and bi, (cid:15)i
and ωi(t) are mutually independent. Again, a variant of the EM algorithm is
used to estimate the parameters where the deﬁnition of the new variable y∗
i
ˆbi − ˆωi. In
now also includes the additional stochastic component: y∗
their simulation studies, Capitaine et al. (2021) [8] showed that both MERT
and RE-EM based tree and RF algorithms are applicable to high-dimensional
datasets. Furthermore, they demonstrated that tree- and forest-based exten-
sions provide more accurate prediction than LMM and standard RF. For ex-
tensions with stochastic processes, misspeciﬁcation, where true underlying data
generating mechanism uses either no stochastic process or other processes than
the one adopted in the estimation procedure, has only limited impact on predic-
tion performance. Besides, variable selection via variable importance measure is
possible for these methods. These characteristics make the extensions suitable
for omics data analysis. In fact, [8] compared all forest-based extensions, i.e.,

i = yi − Zi

13

MERF, REEMforest, SMERF and SREEMforest on the DALIA vaccine trial
dataset where expression of 32,979 gene transcripts was included in the analysis.
Before we move on to the context of generalized linear models, we would
like to remark that for predicting the outcome of new observations with afore-
mentioned RF extensions, two diﬀerent settings have to be distinguished. The
ﬁrst case is prediction for a new subject i for which no random eﬀects ˆbi are
available. Thus, prediction is solely based on the ﬁxed eﬀect component which
is either given by the prediction of the tree or RF ( ˆf (Xit)) or the predicted eﬀect
associated with the terminal node in which the new observation lands (Φi
jt ˆµt).
Secondly, to predict a new observation for a subject i used in the training pro-
cess, the sum of the ﬁxed component and the corresponding random eﬀect of
subject i can be used.

Generalized response variable

The approaches described so far in this section assume a quantitative outcome
that is normally distributed. Further extensions have been proposed for other
types of outcomes by using generalized linear mixed models (GLMMs) instead
of LMMs.

The GLMM assumes that, conditional on the random eﬀects, the outcome
yi follows a distribution from the exponential family. The GLMM model can be
further speciﬁed as:

g(µi) = ηi = Xiβ + Zibi,

bi ∼ N (0, D),

where µi = E(yi|bi), g(·) is a known link function, and ηi is a ni × 1 vector. The
commonly used link functions include identity link, logit link and log link func-
tions for quantitative, binary and count outcomes, respectively. Parameters of
GLMMs are estimated by ML or REML methods using numerical optimization
algorithms such as penalized quasi-likelihood (PQL) [44], iteratively reweighted
least squares or a Newton-Raphson method [36].

Similar to the quantitative outcome case, the RF extensions from GLMM
replace the linear relationship between outcome and ﬁxed eﬀects predictor vari-
ables by a nonparametric alternative such as a decision tree or RF. The essential
estimation procedure is again using an iterative algorithm inspired by the EM
algorithm [29] to estimate the ﬁxed and random eﬀects separately and itera-
tively. Here, we only provide a brief summary of the approaches and mention
their quantitative counterparts. For more details, we refer the reader to the
original publications.

The MERT approach has been extended to the generalized mixed eﬀects
regression tree (GMERT) [20]. The PQL algorithm of the GLMM is modiﬁed
so that a weighted MERT pseudo-model is used instead of the weighted linear
mixed-eﬀects pseudo-model. The ﬁxed part is again estimated with a standard
regression tree. In this implementation it is necessary to specify initial estimates
In the simulation study with a binary outcome the
of the mean values ˆµi.

14

authors used pre-determined values ˆµij = 0.25 if yij = 0 and ˆµij = 0.75 if yij =
1. Unfortunately, no further discussions on this initialization were presented.

Similarly, the generalized mixed eﬀects tree (GMET) extends the RE-EM
tree [17]. Again, a regression tree is used and the indicator variables for the
terminal nodes are modeled as ﬁxed eﬀects in the mixed eﬀects model. The
modiﬁed outcome variable for the regression tree is y∗
i = ηi − Zibi. However,
ηi needs to be estimated which is usually achieved with a standard generalized
linear model (GLM) using the predictors as ﬁxed eﬀects covariates (in [17, 38]).
Note that this approach is not possible for high dimensional data due to this
need to estimate ηi with GLM since the number of variables then cannot exceed
the number of observations. To the best of our knowledge, no solutions for high
dimensional data have so far been proposed in the literature along this direction.
GMET has further been extended to generalized mixed eﬀects random forest
(GMERF) [38] where instead of growing only a single decision tree, a random
forest is trained.

2.3.3 A Bayesian approach

For binary outcomes, Binary Mixed Model (BiMM) tree [52] considers a Bayesian
implementation of GLMM. The GLMM portion of the BiMM method has the
form

logit(µit) = CART(Xit)β + Zitbit,

where CART(Xit) are indicator variables reﬂecting membership of each longi-
tudinal observation t for subject i in terminal nodes within the decision tree.
Therefore, the use of the tree in this approach is again not to model the ﬁxed
eﬀects directly, but rather to determine similar groups of observations after
random eﬀects have been properly adjusted.

For estimation the BiMM tree method again adopts the EM-like algorithm
and iterates between developing CART models using all predictors and then
using information from the CART model within a Bayesian GLMM to adjust
for the clustered structure of the outcome. Speciﬁcally, the procedure can be
brieﬂy summarized as follows.

(a) CART construction with (yi, Xi) and obtain predicted probability pCART(Xit)

(b) ﬁt Bayesian GLMM with (yit, pCART(Xit), Zit)

(c) update y∗

it by discretization of yit+qit where qit is the predicted probability

from the Bayesian GLMM

(d) repeat (a)-(c) with y∗

it until the change in posterior log-likelihood is less

than a speciﬁed threshold

This tree method is further extended to a forest-based method where all
CART(Xit) are replaced by RF(Xit). More details of the algorithms can be
found in [51, 52].

15

Compared with the previously reviewed frequentist methods, the Bayesian
approach, as pointed out by the authors, can avoid issues with model conver-
gence, especially when data are high dimensional. In addition, when uninfor-
mative priors are used, frequentist GLMM results can be obtained. That is to
say, the Bayesian approach provides a more general framework with frequentist
approaches such as RE-EM tree/forest as special cases.

3 Multivariate response longitudinal data

So far, the reviewed methods are designed for univariate response variables,
however, often an array of health-related symptoms or scores could be of interest
at the same time, which leads to multivariate responses. Moreoever, even with
an univariate response, we can also treat measurements at diﬀerent time points
together as multivariate responses or a discretized response curve. Therefore,
in this section, we would like to shift our attention to extensions of the RF
algorithm that can accomodate multivariate response variables.

Before we start, we would like to note that the algorithms in this section are
directly applicable with time-invariant predictors such as genetic data. If pre-
dictors are also observed at multiple time points, techniques from the previous
sections, such as subject-level bootstrapping, historical RF, and incorporation
of mixed eﬀects etc, need to be used along with the modiﬁcations reviewed in
this section for an adequate analysis.

3.1 Repeatedly measured univariate longitudinal responses

as multivariate response

One distinct feature of longitudinal data is the repeated measurements at diﬀer-
ent time points, which leads to dependence between observations. However, at
the subject level, the usual independency assumption is still reasonable. There-
fore, one strategy to analyze longitudinal data is to consider observations at
diﬀerent time points jointly so that each subject has only one multi-dimensional
response. But within the multi-dimensional response, variables at diﬀerent di-
mensions are not independent as they represent the same measurement taken
at diﬀerent time points. Therefore, when this multivariate response approach
is considered, on the one side, we have the independency between samples, but
on the other side, we still need to take care of the inter-dimensional correlation.
To accomodate multivariate responses, the common strategy to extend the
RF algorithm largely focuses on modifying the split criterion in the construction
of each decision tree, where impurity measures are modiﬁed so that multivariate
responses can be handled properly. In addition the covariance structure needs
to be considered when deﬁning the impurity measure to account for the inter-
dimensional correlation. The modiﬁcations can be roughly categorized into two
classes, using either distance or likelihood based split criteria.

16

3.1.1 Distance based impurity functions

Segal (1992) [46] was among the ﬁrst to extend CART to longitudinal data
by using a distance based measure for node impurity. Speciﬁcally, the author
considered an univariate quantitative outcome but treated measurements at
diﬀerent time points jointly as a multivariate response. It is further assumed
that the observation times for all subjects are the same, so that the dimension
of the multivariate response is ﬁxed and not changing across subjects. For a
given node t, Segal (1992) [46] considers the following generalized sum of square
function:

SS(t) =

(cid:88)

i∈t

(yi − ¯yt)(cid:48)V(θ, t)−1(yi − ¯yt),

(4)

where yi, i = 1 . . . , N is a n × 1 vector, ¯yt is the sample average of yi’s within
node t, V(θ, t) denotes the n × n covariance matrix of the responses within node
t and depends on unknown parameters θ which can be estimated within the
node. Then the splitting rule s of the node t is evaluated via

φ(s, t) = SS(t) − SS(tL) − SS(tR).

In principle, the estimated parameters ˆθ can diﬀer for node t and its daughters
tL and tR, which as the author noticed may lead to negative φ. Hence, the au-
thor further imposes the restriction that for each candidate split the covariance
parameters are determined from the parent node t so that

V(θ, t) = V(θL, tL) = V(θR, tR).

Furthermore, the author provides several candidates for the covariance struc-
ture, namely, independence (i.e., diagonal matrix), ﬁrst-order autoregression
(AR1), compound symmetry (CS), and sample covariance matrix.

The independence structure leads to the sum of square about the mean:

SS(t) =

(cid:88)

n
(cid:88)

i∈t

j=1

(yij − ¯yj)2,

where yij is the outcome for subject i and component j, and all subjects are
assumed to have same number of n components. This is a direct generalization
from the univariate regression tree, and has been used by De’Ath (2002) [11] for
applications in ecology and by Segal and Xiao (2011) [45] in the construction of
the multivariate random forest.

When the sample covariance matrix is adopted in Segal’s approach, the
generalized sum of square function is closely related to the Mahalanobis distance
where the Mahalanobis distance of an observation yi from a set of observations
with mean µi and (nonsingular) covariance matrix S is deﬁned as

D(yi) = (cid:112)(yi − µi)(cid:48)S−1(yi − µi).

17

Larsen and Speckman (2004) [30] directly considered the Mahalanobis distance
as node impurity measurement and split criterion. Instead of updating the co-
variance structure during the tree construction, they estimate the covariance
matrix from the whole data set at the very beginning and use the estimate
throughout the whole process. They still consider the simple average of ob-
servations in each node for µi, but diﬀerent estimators such as trimmed mean
could also be adopted.

Besides the Mahalanobis distance based split criterion, De’Ath (2002) [11]
proposed the distance-based multivariate regression tree (db-MRT), where the
impurity of a given node is measured based on the pair-wise dissimilarities
between observations within the node. Sim et al. (2013) [50] put this approach
into a more formal construction where the dissimilarities between observations
are captured by a distance matrix. Here, a distance matrix D is a symmetric
positive real-valued matrix, where the components Dij denote the distances
between yi and yj and D satisﬁes the three required distance conditions Dii =
0, Dij = Dji, and Dij + Djk ≥ Dik, ∀i, j, k = 1, . . . , N . Then the impurity of
each node t is deﬁned as

Imp(t) =

(cid:88)

D2
ij,

The split criteria s is evaluated via

i,j∈t

φ(s) = −(Imp(t) − Imp(tL) − Imp(tR)),

and the one achieving the maxium gives the optimal splitting criteria.

This approach is more general than the aforementioned extensions in that
the distance matrix does not necessarily depend on the dimension of the original
responses. In fact, it is possible to analyze longitudinal responses at irregular
time points with this approach as long as an appropriate distance measure can be
deﬁned. However, how to make prediction with the resulting RF needs further
consideration because now within a leaf, it is possible to have responses with
diﬀerent dimensions, thus usual sample average would not make sense in such
cases. This approach may also be applicable for analyzing multiple longitudinal
responses. As long as the distance matrix between pairs of responses can be
properly deﬁned, then the construction of the tree and RF does not depend on
the dimension of the responses.

Lastly, when the distance is measured by l1-norm, given the well-known

relationship that

N
(cid:88)

i=1

|yi − ¯y|2 =

1
2N

N
(cid:88)

N
(cid:88)

i=1

j=1

|yi − yj|2,

this distance matrix based approach is connected to Segal’s approach with an
assumed independence covariance structure.

3.1.2 Likelihood based impurity function

Zhang (1998) [56] extended CART to multiple binary response variables. For
responses from an exponential family distribution, the author considered the

18

log-likelihood as the node impurity that depends only on the linear terms and
the sum of the second-order products of the responses. Speciﬁcally, for subject
i, yi is assumed to follow the joint probability distribution:

f (yi; Ψ, θ) = exp(Ψ(cid:48)yi + θwi − A(Ψ, θ)),

where Ψ and θ are arrays of parameters, A(Ψ, θ) is the normalization function
depending only on Ψ and θ, and wi = (cid:80)
j<k yijyik. The node impurity is deﬁned
as the maximum of the log-likelihood derived from this distribution; that is, for
node t,

Imp(t) =

(cid:88)

( ˆΨ(cid:48)yi + ˆθwi − A( ˆΨ, ˆθ)),

i∈t

where ˆΨ and ˆθ are the maximum likelihood estimates of Ψ and θ within the node.
Zhang and Ye (2008) [58] applied the same technique to ordinal responses by
ﬁrst transforming them to binary-valued indicator functions.

When multivariate normally distributed responses are considered, Abdolell
et al. (2002) [1] proposed a likelihood-ratio test statistic as impurity function.
Speciﬁcally, suppose that yi ∼ Nn(µi, Σ), the authors deﬁne the deviance func-
tion for a single observation as

D(µi; yi) = 2[(cid:96)(yi; yi) − (cid:96)(µi; yi)] = (yi − µi)(cid:48)Σ−1(yi − µi),

where (cid:96)(µi; yi) is the log-likelihood function. Assuming that Σ is constant and
given for all i, they further deﬁne the deviance within a node t as

D(ˆµ; y, t) =

(cid:88)

i∈t

D(ˆµ; yi),

where ˆµ is the restricted maximum likelihood estimate (REML) within the node.
The impurity of a node is then measured by the negation of the deviance. As
pointed out by the authors, this deviance function in the context of the multi-
variate normal distribution, is the Mahalanobis distance between µi and yi. In
addition, they also noticed that deviance assessed via the multivariate analy-
sis of variance (MANOVA) approach such as Hotelling’s T 2 is again in a form
of the Mahalanobis distance. These observations connects the likelihood-ratio
test statistics based impurity function with the aforementioned Mahalanobis
distance based one.

The likelihood-ratio test statistics based impurity function is also considered
by Segal (1992) [46] for multivariate normally distributed responses. However,
their splitting rule focuses on the intra-cluster variation structure of subjects
other than the mean structure of responses, which we think may be diﬃcult to
interpret and less of interest in terms of precision medicine.

3.2 Multiple longitudinal responses

If an array of health-related symptoms are monitered at the same time, then this
leads to multiple longitudinal responses. The extensions in the previous section,

19

except the db-MRT, may not be directly applicable because if the repeated
measurements at diﬀerent time points are also considered together, then for
each subject i, its corresponding reponse yi is a matrix (see Table 1).

One possible approach is to combine the extensions in the previous section
with the techniques we reviewed in Section 2 such as subject-level bootstrapping
and mixed eﬀects models. Another approach is given by Yu and Lambert (1999)
[55] which can be considered as an extension from the traditional non-parametric
and semi-parametric regression models for longitudinal data (see Chapter 8-12
of [14] for detailed review and discussions on traditional methods). For longitu-
dinal data observed at many observation times, they treat each response vector
as a random function and ﬁt each trajectory with a spline curve. Then they
use the estimated coeﬃcients of the basis functions as multivariate responses
to ﬁt a regression tree model. When the number of observation times is large,
this approach can eﬀectively reduce the dimensionality of the responses. Fur-
thermore, by considering the same set of expension basis for all trajectories,
the data structure is uniﬁed, so this approach is applicable to irregular-spaced
observations.

4

Implementation

In Table 3, we provide a summary of the software implementations of the RF
extensions reviewed in previous sections. For each method, its implementation
and where to obtain the package or code is listed. Information on the type of
problem the extension method can solve is also given and whether the method
grows a tree or random forest is summarized. If variable importance measure is
supplied by the package or code, such information is also presented.

Brieﬂy, almost all extensions are presented in an R package on CRAN or as
an R program. Programs of MERT and MERF can only be obtained directly
from their authors, no public access is available. The majority of extensions
do not supply variable importance measures, which could be a future research
direction for longitudinal data analysis with random forests. We also remark
that no systematic comparisons and eﬃciency studies have been performed so
far on the listed packages and programs.

5 Discussion

In this paper, we review extensions of the CART-based random forest algo-
rithm for the analysis of longitudinal data. Longitudinal data are common in
areas such clinical trials and precision medicine. Using tree- or forest-based
methods for analysis may help in patients stratiﬁcation, disease progression
prediction, and target biomarker identiﬁcation for drug design. The repeated
measurements of the same subject naturally induce clustering eﬀect in longi-
tudinal data, which negatively aﬀects the variable selection performance and
predictive accuracy of standard RF. To mitigate such eﬀect, subject-level boot-

20

C, R
F
R
T
R
F
T
R
T, F R

Yes (P)
No
No
No
No

[48]
[18]
[19]
[42, 47]
[7, 8]

Table 3: Overview of the implementations of the reviewed RF extensions.
Reference
[26]

Type Response VImp
F

Yes (P)

C, R

Name
RF++

Implementation
Stand-alone software (binary)
(https://sourceforge.net/projects/rfpp)
R package htree (CRAN)
Historical RF
R program (from Dr. Ahlem Hajjem)
MERT
R program (from Dr. Ahlem Hajjem)
MERF
RE-EM
R package REEMtree (CRAN)
(S)MERT, (S)MERF R package LongituRF (CRAN)
(S)REEMforest
GMERT
BiMM forest
Multivariate RF

T
R code (supplement to original paper)
R code (supplement to original paper)
F
R package MultivariateRandomForest (CRAN) F
T
R package mvpart (CRAN)
F
R package randomForestSRC (CRAN)

C
C
R
R
C, R

No
No
No
No
Yes

[20]
[51, 52]
[39, 40, 45]
[11, 12]
[22, 45]

VImp = Variable importance, T = tree, F = forest, R = regression, C = classiﬁcation, P = permutation

strap re-sampling strategy can be considered. This approach works well for
repeated measurements where observation ordering is not important. However,
this approach does not take time eﬀect into consideration, also there is no ran-
dom eﬀects included which limits its application for prediction. Historical RF
is an approach which summarizes the observation histories and uses them along
with concurrent observation to model the conditional mean of response vari-
able. Extensions based on (generalized) linear mixed eﬀects model, from both
frequentist and Bayesian perspectives, provide another solutions to longitudinal
structure, where tree/forest models are used to model the ﬁxed eﬀects compo-
nent in the mixed eﬀects model. Finally, by adjusting the splitting criterion
in the construction of decision tree of RF, multivariate longitudinal response
variables can also be handled.

In this review, we limit our focus on CART-based RF methods, there are
certainly other approaches to be considered. Examples include GUIDE [32] and
conditional inference [23] approaches. Some extensions reviewed here such as
MERT and MERF can easily switch to their approaches for tree/forest construc-
tion, while others may require diﬀerent extensions for longitudinal data. See [33]
for a review on diﬀerent tree construction approaches and related discussions
on extensions for longitudinal data analysis.

As we pointed out in Section 1, missing values represent a major challenge.
Segal (1992) [46] considered the surrogate splitting variable approach, which is
one of the standard solutions for missing values in the literature of tree and
forest methods. However, diﬀerent missing mechanisms may require diﬀerent
approaches to handle missing values. In general, this is still an important re-
search area in the context of developing and applying statistical methods and
machine learning approaches in general.

Variable importance measure is a unique feature that RF can oﬀer to sup-
port variable selection. Some reviewed extensions consider permutation-based
variable importance which is easy to implement but computationally expensive.
Other approaches such as the variable-delete approach may also considered, but
correlation between predictor variables may negatively aﬀect its performance.
How to measure variable importance in a tailored fashion for longitudinal data

21

warrents further study, because this could be beneﬁcial in both understanding
disease progression and searching for target biomarkers for drug design.

Another direction for future methodology development is on the eﬀective
handling of high dimensional longitudinal data. Except for the BiMM and
REEMforest methods, the other reviewed methods have so far not been evalu-
ated on high dimensional data sets. For instance, for the extensions from GLMM
with non-quantitative outcomes, there is a need for an initial GLM ﬁt which
would be very diﬃcult, if not impossible, with the high dimensional data sets.
Omics data are usually high dimensional and may change over time. Having
RF extensions being able to handle such data would provide fruitful insights on
their eﬀects on complex diseases.

Lastly, to the best of our knowlege, systematic benchmark studies to com-
pare these aforementioned RF extensions have not been published so far. For
instance, from the prediction perspective, how would the subject-level boot-
strapping methods and historical RF compare with extensions with mixed ef-
fects model? Such benchmark studies would be informative and important for
practitioners analyzing real data sets. This would be another important study
to conduct.

Funding

This work was supported by the German Federal Ministry of Education and Re-
search (BMBF) funded e:Med Programme on systems medicine [grant 01ZX1510
(ComorbSysMed) to SSzy].

References

[1] M Abdolell, M LeBlanc, D Stephens, and RV Harrison. Binary partition-
ing for continuous longitudinal data: categorizing a prognostic variable.
Statistics in medicine, 21(22):3395–3409, 2002.

[2] Werner Adler, Alexander Brenning, Sergej Potapov, Matthias Schmid, and
Berthold Lausen. Ensemble classiﬁcation of paired data. Computational
Statistics & Data Analysis, 55(5):1933–1941, May 2011.

[3] Werner Adler, Sergej Potapov, and Berthold Lausen. Classiﬁcation of re-
peated measurements data using tree-based ensemble methods. Computa-
tional Statistics, 26(2):355, March 2011.

[4] Euan A Ashley. Towards precision medicine. Nature Reviews Genetics,

17(9):507–522, 2016.

[5] Leo Breiman. Random Forests. Machine Learning, 45(1):5–32, October

2001.

[6] Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen.

Classiﬁcation and regression trees. CRC press, 1984.

22

[7] Louis Capitaine. LongituRF: Random Forests for Longitudinal Data, 2020.

R package version 0.9.

[8] Louis Capitaine, Robin Genuer, and Rodolphe Thi´ebaut. Random forests
for high-dimensional longitudinal data. Statistical Methods in Medical Re-
search, 30(1):166–184, 2021. PMID: 32772626.

[9] Xi Chen and Hemant Ishwaran. Random forests for genomic data analysis.

Genomics, 99(6):323–329, June 2012.

[10] D. Richard Cutler, Thomas C. Edwards, Karen H. Beard, Adele Cutler,
Kyle T. Hess, Jacob Gibson, and Joshua J. Lawler. Random Forests
eprint:
for Classiﬁcation in Ecology. Ecology, 88(11):2783–2792, 2007.
https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1890/07-0539.1.

[11] Glenn De’ath. Multivariate Regression Trees: A New Technique for Mod-
eling Species–Environment Relationships. Ecology, 83(4):1105–1117, 2002.

[12] Glenn De’ath. mvpart: Multivariate partitioning, 2014. R package version

1.6-2.

[13] Frauke Degenhardt, Stephan Seifert, and Silke Szymczak. Evaluation of
variable selection methods for random forests and omics data sets. Brieﬁngs
in bioinformatics, 20(2):492–503, 2019.

[14] Garrett Fitzmaurice, Marie Davidian, Geert Verbeke, and Geert Molen-

berghs. Longitudinal data analysis. CRC press, 2008.

[15] Garrett M Fitzmaurice, Nan M Laird, and James H Ware. Applied longi-

tudinal analysis. John Wiley & Sons, 2012.

[16] Marjolein Fokkema, Niels Smits, Achim Zeileis, Torsten Hothorn, and Henk
Kelderman. Detecting treatment-subgroup interactions in clustered data
with generalized linear mixed-eﬀects model trees. Behavior research meth-
ods, 50(5):2016–2034, 2018.

[17] L Fontana, C Masci, F Ieva, and AM Paganoni. Performing learning analyt-
ics via generalized mixed-eﬀects trees. MOX-Modelling and Scientiﬁc Com-
puting, Department of Mathematics, Politecnico di Milano, via Bonardi,
9:1–17, 2018.

[18] Ahlem Hajjem, Fran¸cois Bellavance, and Denis Larocque. Mixed eﬀects re-
gression trees for clustered data. Statistics & Probability Letters, 81(4):451–
459, April 2011.

[19] Ahlem Hajjem, Fran¸cois Bellavance, and Denis Larocque. Mixed-eﬀects
random forest for clustered data. Journal of Statistical Computation and
Simulation, 84(6):1313–1328, June 2014.

23

[20] Ahlem Hajjem, Denis Larocque, and Fran¸cois Bellavance. Generalized
mixed eﬀects regression trees. Statistics & Probability Letters, 126:114–
118, July 2017.

[21] Donald Hedeker and Robert D Gibbons. Longitudinal data analysis. Wiley-

Interscience, 2006.

[22] Udaya B. Kogalur Hemant Ishwaran. randomForestSRC: Fast Uniﬁed Ran-
dom Forests for Survival, Regression, and Classiﬁcation (RF-SRC), 2022.
R package version 3.1.0.

[23] Torsten Hothorn, Kurt Hornik, and Achim Zeileis. Unbiased recursive
partitioning: A conditional inference framework. Journal of Computational
and Graphical statistics, 15(3):651–674, 2006.

[24] Hemant Ishwaran, Udaya B Kogalur, Eugene H Blackstone, and Michael S
Lauer. Random survival forests. The Annals of applied statistics, 2(3):841–
860, 2008.

[25] J Larry Jameson and Dan L Longo. Precision medicine—personalized,
problematic, and promising. Obstetrical & gynecological survey, 70(10):612–
614, 2015.

[26] Yuliya V. Karpievitch, Elizabeth G. Hill, Anthony P. Leclerc, Alan R.
Dabney, and Jonas S. Almeida. An Introspective Comparison of Random
Forest-Based Classiﬁers for the Analysis of Cluster-Correlated Data by Way
of RF++. PLOS ONE, 4(9):e7087, September 2009.

[27] E Krasniqi, W Schramm, and A Reichenbach. Data-driven stratiﬁcation
of parkinson’s disease patients based on the progression of motor and cog-
nitive disease markers datengetriebene stratiﬁzierung von patienten mit
parkinson-krankheit anhand von verlaufsdaten motorischer und kognitiver
kennzahlen der erkrankung.

[28] Inke R. K¨onig, Oliver Fuchs, Gesine Hansen, Erika von Mutius, and
Matthias V. Kopp. What is precision medicine? The European Respi-
ratory Journal, 50(4), 2017.

[29] Nan M Laird and James H Ware. Random-eﬀects models for longitudinal

data. Biometrics, pages 963–974, 1982.

[30] David R. Larsen and Paul L. Speckman. Multivariate Regression Trees for

Analysis of Abundance Data. Biometrics, 60(2):543–549, 2004.

[31] Jeanne C Latourelle, Michael T Beste, Tiﬀany C Hadzi, Robert E Miller,
Jacob N Oppenheim, Matthew P Valko, Diane M Wuest, Bruce W Church,
Iya G Khalil, Boris Hayete, and Charles S Venuto. Large-scale identiﬁcation
of clinical and genetic predictors of motor progression in patients with newly
diagnosed parkinson’s disease: a longitudinal cohort study and validation.
The Lancet Neurology, 16(11):908–916, 2017.

24

[32] Wei-Yin Loh. Regression trees with unbiased variable selection and interac-
tion detection. Statistica Sinica, 12(2):361–386, 2002. Publisher: Institute
of Statistical Science, Academia Sinica.

[33] Wei-Yin Loh. Fifty years of classiﬁcation and regression trees. International

Statistical Review, 82(3):329–348, 2014.

[34] Daniel P Martin, Timo Von Oertzen, and Sara E Rimm-Kaufman. Eﬃ-
ciently exploring multilevel data with recursive partitioning. Society for
Research on Educational Eﬀectiveness, 2015.

[35] Kyle B Matchett, Niamh Lynam-Lennon, R William Watson, and
James AL Brown. Advances in precision medicine: tailoring individual-
ized therapies. Cancers, 9(11):146, 2017.

[36] Peter McCullagh and John A Nelder. Generalized linear models. Routledge,

2019.

[37] Sean D Mooney. Progress towards the integration of pharmacogenomics in

practice. Human genetics, 134(5):459–465, 2015.

[38] Massimo Pellagatti, Chiara Masci, Francesca Ieva, and Anna M. Paganoni.
Generalized mixed-eﬀects random forest: A ﬂexible approach to pre-
Statistical Analysis and Data Min-
dict university student dropout.
eprint:
ing: The ASA Data Science Journal, 14(3):241–257, 2021.
https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11505.

[39] Raziur Rahman. MultivariateRandomForest: Models Multivariate Cases

Using Random Forests, 2017. R package version 1.1.5.

[40] Raziur Rahman, John Otridge, and Ranadip Pal. IntegratedMRF: random
forest-based framework for integrating prediction from diﬀerent data types.
Bioinformatics, 33(9):1407–1410, 02 2017.

[41] Stephen W Raudenbush and Anthony S Bryk. Hierarchical linear models:

Applications and data analysis methods, volume 1. sage, 2002.

[42] Jeﬀrey Simonoﬀ Rebecca Sela and Wenbo Jing. REEMtree: Regression
Trees with Random Eﬀects for Longitudinal (Panel) Data, 2021. R package
version 0.90.4.

[43] Marylyn D Ritchie. The success of pharmacogenomics in moving genetic as-
sociation studies from bench to bedside: study design and implementation
of precision medicine in the post-gwas era. Human genetics, 131(10):1615–
1626, 2012.

[44] Germ´an Rodr´ıguez. Multilevel generalized linear models. In Handbook of

multilevel analysis, pages 335–376. Springer, 2008.

25

[45] Mark Segal and Yuanyuan Xiao. Multivariate random forests. Wiley Inter-
disciplinary Reviews: Data Mining and Knowledge Discovery, 1(1):80–87,
2011.

[46] Mark Robert Segal. Tree-Structured Methods for Longitudinal Data. Jour-

nal of the American Statistical Association, 87(418):407–418, June 1992.

[47] Rebecca J. Sela and Jeﬀrey S. Simonoﬀ. RE-EM trees: a data mining
approach for longitudinal and clustered data. Machine Learning, 86(2):169–
207, February 2012.

[48] Joe Sexton. htree: Historical Tree Ensembles for Longitudinal Data, 2018.

R package version 2.0.0.

[49] Joseph Sexton and Petter Laake. Historical random forests. Working paper.

[50] Aaron Sim, Dimosthenis Tsagkrasoulis, and Giovanni Montana. Random
forests on distance matrices for imaging genetics studies. Statistical Appli-
cations in Genetics and Molecular Biology, 12(6):757–786, 2013.

[51] Jaime Lynn Speiser, Bethany J. Wolf, Dongjun Chung, Constantine J.
Karvellas, David G. Koch, and Valerie L. Durkalski. BiMM forest: A
random forest method for modeling clustered and longitudinal binary out-
comes. Chemometrics and Intelligent Laboratory Systems, 185:122–134,
February 2019.

[52] Jaime Lynn Speiser, Bethany J. Wolf, Dongjun Chung, Constantine J.
Karvellas, David G. Koch, and Valerie L. Durkalski. BiMM tree: A deci-
sion tree method for modeling clustered and longitudinal binary outcomes.
Communications in statistics: Simulation and computation, 49(4):1004–
1023, 2020.

[53] Vladimir Svetnik, Andy Liaw, Christopher Tong, and Ting Wang. Applica-
tion of breiman’s random forest to modeling structure-activity relationships
of pharmaceutical molecules. In International workshop on multiple Clas-
siﬁer systems, pages 334–343. Springer, 2004.

[54] Antonia Vlahou, Aris Giannopoulos, Betsy W Gregory, Theodoros
Manousakas, Filippos I Kondylis, Lori L Wilson, Paul F Schellhammer,
George L Wright Jr, and O John Semmes. Protein proﬁling in urine for
the diagnosis of bladder cancer. Clinical chemistry, 50(8):1438–1441, 2004.

[55] Yan Yu and Diane Lambert. Fitting trees to functional data, with an
application to time-of-day patterns. Journal of computational and graphical
Statistics, 8(4):749–762, 1999.

[56] Heping Zhang. Classiﬁcation trees for multiple binary responses. Journal

of the American Statistical Association, 93(441):180–193, 1998.

26

[57] Heping Zhang and Burton H Singer. Recursive partitioning and applica-

tions. Springer Science & Business Media, 2010.

[58] Heping Zhang and Yuanqing Ye. A tree-based method for modeling a

multivariate ordinal response. Statistics and its interface, 1(1):169, 2008.

[59] Xi Zhang, Jingyuan Chou, Jian Liang, Cao Xiao, Yize Zhao, Harini Sarva,
Claire Henchcliﬀe, and Fei Wang. Data-driven subtyping of parkinson’s
disease using longitudinal clinical records: a cohort study. Scientiﬁc reports,
9(1):1–12, 2019.

27

