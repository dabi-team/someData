2
2
0
2

n
u
J

2
2

]

C
O
.
h
t
a
m

[

2
v
7
8
7
9
0
.
6
0
2
2
:
v
i
X
r
a

Progress in Mathematical Programming Solvers from
2001 to 2020

Thorsten Koch a,b, Timo Berthold c, Jaap Pedersen b, Charlie Vanaret a

aTechnische Universit¨at Berlin, Chair of Software and Algorithms for Discrete
Optimization, Straße des 17. Juni 135, 10623 Berlin, Germany
bZuse Institute Berlin, Takustraße 7, 14195 Berlin, Germany
cFair Isaac Germany GmbH, Stubenwald-Allee 19, 64625 Bensheim, Germany

Abstract

This study investigates the progress made in lp and milp solver performance

during the last two decades by comparing the solver software from the be-

ginning of the millennium with the codes available today. On average, we

found out that for solving lp/milp, computer hardware got about 20 times

faster, and the algorithms improved by a factor of about nine for lp and

around 50 for milp, which gives a total speed-up of about 180 and 1,000

times, respectively. However, these numbers have a very high variance and

they considerably underestimate the progress made on the algorithmic side:

many problem instances can nowadays be solved within seconds, which the

old codes are not able to solve within any reasonable time.

Keywords: LP solver, MILP solver, Mathematical Programming Software,

Benchmark, Mixed Integer Programming

Email address: koch@zib.de (Thorsten Koch )

Preprint submitted to EURO Journal on Computational Optimization

June 23, 2022

 
 
 
 
 
 
1. How much did the state of the art in (Mixed-Integer) Linear

Programming solvers progress during the last two decades?

The present article aims at providing one possible answer to this question.

We will argue how progress in lp and milp solvers can be measured, how to

evaluate this progress computationally, and how to interpret our results. Our

ﬁndings are summarized in Figures 1 and 2. The main part of this article

provides context in which these ﬁgures can be interpreted.

Figure 1: Comparison of the running times of various lp (left) and milp (right) instances

between the virtual best of cplex , xpress , and mosek , from around 2001 and

the virtual best of cplex ., gurobi ., xpress ., mosek ., and copt . from

2020 running with either 1 or 8 threads on a log scale where mosek , mosek ., and

copt . are only used on the lp instances.

Without doubt, computational methods for solving Linear Programs (lp)

and Mixed Integer Linear Programs (milp) have made tremendous progress

2

 1 10 100 1 10 100 1000Running time Virtual Best of New Codes (1,8 thread) [s]Running time Virtual Best of Old Codes (1 thread) [s]LP instanceLP SGM1 1/9 1 10 100 1000100101102103104105Running time Virtual Best of Old Codes (1 thread) [s]MIPLIB 2017 instanceMIPLIB 2010 instanceMIPLIB 2003 instanceMIP SGM1 1/50Figure 2: Runtime of the virtual best new solver for those 149 instances from the miplib

2017 [1] benchmark set that could not be solved by any of the old solvers within 24 h.

The color for the number of threads indicate which was faster. The left column shows how

many of the 3 solvers solved the instance within 6 h.

3

123Number of solvers that solve instancephysiciansched6-2traininstance6neos-787933neos17neos-3381206-awheadrayage-25-23CMS750_4neos-3046615-murgneos-1445765n2seq36qneos-911970nexp-150-20-8-5nursesched-sprint02pgfiballpiperout-0830n20b8beasleyC3h80x6320droll3000pg5_34mzzv11fhnw-binpack4-48neos-1456979mc11neos-3004026-krkans1208400ex10sct2timtab1supportcase33net12rocI-4-11neos-2978193-indeneos-4413714-turianeos-2746589-doondano3_3uccase12rail507blp-ic98supportcase18glass4graphdraw-domaingmu-35-40n3div36s250r10neos-3216931-puririicir97_tensioncomp07-2idxcsched008traininstance2milo-v12-6-r2-40-1mushroom-bestpeg-solitaire-a3graph20-20-1randsatellites2-60-fsran14x18-disj-8blp-ar98lotsizegen-ip054chromaticindex512-7rd-rplusc-21rail01supportcase6brazil3lectsched-5-objsp98arleo1bppc4-08neos-848589wachplanco-100bnatt400csched007neos-5049753-cuanzans1644855assign1-5-8buildingenergyleo250v-10satellites2-40savsched1var-smallemery-m6j6square41supportcase42ns1760995neos-3555904-turamaneos-631710neos-873061ic97_potentialneos-4532248-waihirail02rocII-5-11sp97aratlanta-ipneos-2657525-crnaneos-4763324-togurumadcost266-UUEmomentum1neos-3627168-kasaigermanrrneos-4338804-snowyuct-subprobreblock115sorrell3radiationm18-12-05proteindesign122trx11p8chromaticindex1024-7physiciansched3-3neos-1354092academictimetablesmallcvs16r128-89rococoB10-011000uccase9thor50ddaysupportcase19bab6gmu-35-50neos-4387871-tavuairish-electricitynursesched-medium-hint03sing44sing326cmflsp50-24-8-8square47neos-3754480-niddaneos-5093327-huahumdws008-01cryptanalysiskb128n5obj16snp-02-004-104gfd-schedulen180f7d50m30k18neos-5195221-niemurneos-3656078-kumeuopm2-z10-s4b1c1s1highschool1-aigioneos-5104907-jaramaneos-5114902-kasavucomp21-2idxseymourneos-4647030-tutakibab2markshare2proteindesign121hz512p9supportcase12radiationm40-10-02neos-4954672-berkelroi5alpha10n8 0200040006000800010000CPU Time [s] 1 thread8 threadsduring the last 40+ years. The question “how much?” naturally arises. And

how much of this progress is due to algorithmic improvement compared to

advances in hardware and compilers?

1.1. Previous studies

This question has been asked before. There are ﬁve studies that focus

solely on the cplex solver and cover the 1990s and 2000s. The ﬁrst two, by

Bixby et al. [2, 3], investigated the progress from 1987 to 2001 regarding the

solution of lps; the latter concluded: Three orders of magnitude in machine

speed and three orders of magnitude in algorithmic speed add up to six orders

of magnitude in solving power: A model that might have taken a year to solve

10 years ago can now solve in less than 30 seconds. For the period from 1997

to 2001, the geometric mean speed-up computed over 677 instances was 2.3.

However, it should be noted that the speed-up for large models with more

than 500,000 rows was over 20.

Bixby et al.

[4] examined milp solving. The study considered 758 in-

stances and compared cplex 5.0 (released in 1997) and cplex 8.0 (2002).

The geometric mean of the speed-up was about 12. The speed-up was con-

siderably higher for the instances that required over 27 hours to solve with

the older code, reaching an average of 528.

Achterberg and Wunderling [5] continued the study up to cplex 12.5 in

2012. The overall geometric mean speed-up on 2,928 milp models turned out

to be 4.71. An average speed-up of up to 78.6 was observed for the instances

that were challenging for version 8.0. This is still an underestimation, as the

old solver hit the time limit of 10,000 seconds for 732 of the instances, while

the new one only had 87 timeouts.

4

Lodi [6] compared cplex 1.2 (1991) with cplex 11.0 (2007) on 1,734

milps and reported a geometric mean speed-up of 67.9. Another revealing

metric shown is the number of instances solved to optimality within the time

limit of 30,000s. On 1,852 milps, cplex 1.2 was able to solve a mere 15.0%,

while version 11.0 on the same hardware could solve 67.1%.

Koch et al. [7, 8] compared the performance of a wide variety of solvers

on the miplib 2010. The progress from 1996 to 2011 was investigated and

the conclusion was unsurprisingly similar. On the one hand, instances that

were already solved “quickly” did not get solved faster. On the other hand,

many instances that used to be “diﬃcult” got solved considerably faster;

these were the ones that contributed the most to the overall speed-up.

Since all of these studies are at least ten years old, it seems about time

to give an update on whether lp and milp development is still going strong.

1.2. Setup of this study

One could argue that all studies, including the present one, have intrinsic

biases. The threshold for discarding problems as ”too easy” inﬂuences the

observed speed-up factors. The higher the threshold, the higher the speed-

up. The same happens on the other end: the lower the time limit given to

the solver, the lower the achievable speed-up.

Another bias comes from the selection of instances.

Instances usually

do not enter a collection because they are quickly solved on the ﬁrst try.

Therefore, there is a tendency to collect “diﬃcult“ instances. On the other

hand, modeling practices rely on the eﬃciency of current solvers, which leads

to a selection that under-represents modeling practices that cannot (at the

time) be satisfyingly solved.

5

Another natural question for our study was which solver to use. When the

initial tests for the miplib 2010 [7] were performed, all three main commercial

solvers achieved roughly the same geometric average running time over the

whole benchmark set. The speed diﬀerence for individual instances, however,

was as large as a factor of 1,000 between the fastest and the slowest solver.

Which solver was the fastest was largely instance-dependent. When miplib

2010 was released, at least one of the three solvers was able to solve each

instance within one hour, but it took years until one single solver was capable

of solving each instance within an hour. To solve a particular instance, why

not use the best solver available? Therefore, it seems natural to us that to

discuss the overall performance gain, we use the virtual best solver available

at the time, unless otherwise stated. The term ”virtual best” refers to a

perfect oracle that would make the right choice among the solvers for a given

instance.

In this article, all running times are given for the two virtual solvers

old and new, where old is the best among the cplex Linear Optimizer

7.0.0 (2000), the Xpress-MP Hyper Integer Barrier Optimizer Release 14.10

(2002), and MOSEK Version 3.2.1.8 (2003) for solving LPs. These codes run

single-threaded, with the exception of the barrier method lp solvers within

xpress and mosek. The best achievable result was systematically kept.

new is the best among the ibm ilog cplex Interactive Optimizer 12.10

(2019), the gurobi Optimizer 9.0 (2020) and the fico xpress Solver 8.11

(2020), as well as mosek Version 8.1 (2017) and copt Version 1.4 (2020) for

solving lps. All solvers were run both sequentially (i.e. single-threaded) and

in parallel, allowing the solver to use up to eight threads.

6

Our study focuses on the developments of the past twenty years, for

three reasons. The ﬁrst, festive reason is to focus on the period during which

EUROPT has been active, following the spirit of this special issue. The

second, apparent reason is that this nicely covers the development of lp and

milp solving in the 21st century (so far). The third, most practical and

constraining reason is that it was very tricky to get old and still running

solver binaries. As we experienced, a 20-year period is borderline and in

some respect already too extensive a time span. There are no contemporary

binaries that run on the old 32-bit computers; the old 32-bit binaries failed

to run on one of our newer systems. Furthermore, as can be seen in Table 1,

the speed diﬀerence between the old code on the old system and the new

code on the new computers is already so enormous that only few instances

can be compared in a meaningful way with reasonable eﬀort.

2. Progress in hardware, software and algorithms

There has been a continuous evolution of the performance of lp and

milp solvers due to two main, intertwined drivers, namely the development

of computers (Section 2.1) and the algorithmic advances (Section 2.2). These

two sources of progress cannot be easily separated. In the following, we will

provide experimental results and discuss which factors inﬂuenced the change

in performance over time and in which direction.

Unless otherwise stated, all computations have been carried out on an

8-core, 8-thread Intel Core i7-9700K CPU @ 3.60 GHz with 64 GB of RAM.

It should be noted that modern CPUs adjust their clock speed to the load.

The used system might speed up to 4.7 GHz when running only a single

7

B&B nodes

Time [s]

old/P-III

new/i7 old/P-III new/i7

Speed-up

Name

nw04

mas76

neos-1122047

mod011

air05

qiu

cap6000

bell5

131

24

467,454

192,719

64

16,671

1,961

36,452

16,768

687,056

1

3,288

2,189

4,434

1

915

54

198

79

926

440

1,393

268

411

neos1171737

28,354

1

116,745

1.62

3.50

1.28

5.77

2.11

1.49

0.10

0.03

2.67

33

57

62

160

209

935

2,680

13,700

43,725

Table 1: Comparison of selected instances: old (870 MHz Pentium-III) vs new (3.6 GHz
i7-9700). Remarks: the slowest of new with one thread needs 45 s to solve mas76 on the

i7. In this case the speed-up is exactly the clock ratio between the computers (the solver used

in old and the one used in new are diﬀerent though). The biggest speed-up happens when

the number of B&B nodes can be reduced. However, whenever there is only one node, no

additional speed-up from parallelization occurs.

8

thread. Unfortunately, there is no easy way to track which speed was ac-

tually used during a particular run. Therefore, 25% and more variation of

computing time in the measurements are not uncommon. In an experiment,

the performance of a single thread halved as we kept the other seven threads

busy. For the eight-core runs, the eﬀect is less pronounced, as the machine

is already under almost full load by the task to be performed.

2.1. Progress in hardware and computational environment

In 2001, two of the latest CPUs were the Intel 32-bit Pentium-III at

around 1 GHz and the Pentium-4 at 1.5 GHz.

IBM oﬀered the 64-bit

POWER7 at over 3 GHz. Although 64-bit systems were available, the ﬁrst

64-bit PC processor was introduced in 2003 and it took quite a few years

until more than four gigabytes of memory became standard. One should

keep in mind that there is a gap of several years between the availability of

a new architecture and the common use of it by developers and companies.

In the following, we list the major developments in hardware and compilers

that came into widespread use during the past twenty years:

Higher clock speed and increased memory bandwidth: both develop-

ments also accelerate old code, even if not recompiled.

More eﬃcient processing of instructions: superscalar processors, out-

of-order execution, branch prediction, and instruction speed-up. As a

consequence, code optimized for an older processor architecture might

not perform optimally on a new one. Recompilation is required to fully

exploit the improvements.

9

New instructions: for example, Fused-Multiply-Add (FMA) and Advanced

Vector Extensions (AVX). To exploit these extensions, the code needs

at least to be recompiled. The use of highly optimized subroutines (e.g.,

ATLAS, OpenBLAS, or IMKL) can provide further speed-up. Barrier

solvers often have speciﬁc subroutines implemented in instruction-set

speciﬁc assembly code.

Parallel cores and simultaneous multi-threading (SMT): both have

increased the maximal computational performance of a single CPU

drastically, however a substantial redesign of the algorithms is required

to exploit them. There is almost no automatic beneﬁt for existing

codes. Additionally, SMT in particular makes it even harder to deter-

mine the best number of parallel threads to use on a given CPU. If

memory accesses are the bottleneck, not using SMT can lead to better

running times. This is aggravated by the power management of mod-

ern CPUs which can decrease clock frequency in case of an increased

number of running threads.

Move from 32-bit to 64-bit addressing/processing: this allows to use

more than 4 GB of RAM and to process 64-bit numbers faster. There

is no beneﬁt for existing 32-bit codes. Since more memory is used per

integer, it possibly can even slow down computations. With some reim-

plementation however, 64-bit addressing can contribute to performance

gains, e.g. by making hash collisions less likely.

Improved optimizing compilers: from, for example, gcc version 2.95

(2001) to gcc version 10 (2020), compilers have improved a lot and

10

generate better performing code. Recompilation is required to beneﬁt

from this.

For an overview of hardware and compiler impact on lp and milp solver

development in the 1980s and 1990s, see [9].

Comparing old and recent architectures is intricate. Old sequential 32-

bit codes using the instruction set available in 2001 will not fully exploit

modern architectures. Conversely, new parallel 64-bit codes based on recent

instructions will not even run on old hardware.

We performed two small tests to estimate the pure hardware speed-up for

solving mathematical optimization problems. First, we ran 33 lp instances

using the single threaded cplex 7 barrier algorithm without crossover on an

old 870 MHZ Pentium-III and the new i7-9700 system, and compared the

running times. The speed-up is 21 on average, although it varies between 16

and nearly 47, depending on the particular instance.

Since the requirements of barrier and simplex algorithms are quite diverse,

we performed a second test: we solved min-cost-ﬂow problems with the net-

work simplex code in cplex. It can be assumed that this code did not change

signiﬁcantly between version 7.0 and 12.10, as the number of iterations on all

instances is identical. We ran 16 publicly available instances in four diﬀerent

settings: cplex 7 on a 870 MHz Pentium-III and on an i7-9700, and we

ran cplex 12 on an otherwise empty i7-9700 and on a fully loaded system.

There is no measurable performance diﬀerence between the two cplex ver-

sions regarding the network simplex running on the same hardware. cplex 7

running on the 870 MHZ Pentium-III and on an empty i7-9700 supposedly

running at 4.7 GHz boost speed diﬀer by a factor of 20 on average. However,

11

if we fully load the system with a multi-core stream benchmark [10], the

performance is halved. One should bear in mind that for each situation, it

is not clear where the bottleneck is. The network simplex is known to be

highly dependent on the performance of the memory subsystem. Overall,

the hardware speed-up that we experienced was not constant. The minimum

factor is around 15 (i7 empty) and seven (i7 loaded), and the maximum was

more than 45. We would like to point out that small diﬀerences in running

times are not signiﬁcant and that the overall impact of the compiler seems

small.

The hardware landscape has been changing even more dramatically in

the last 10 years, during which Graphics Processing Unit (GPU) accelera-

tors have become widely available. However, as of 2020 (to the best of our

knowledge), none of the state-of-the-art lp/milp solvers exploits them. In-

deed, GPUs are tailored much towards dense processing, while solvers rely

heavily on super-sparse linear algebra.

2.2. Progress in algorithms

Two decades of research certainly led to signiﬁcant algorithm improve-

ments. One could now ask how much each new feature contributed to the

speed-up. Unfortunately, there is no easy and meaningful answer to this.

Firstly, we don’t know exactly which features were added to each commer-

cial solver. Secondly, since we compare the virtual best, this would be tricky

to evaluate even if we knew. Thirdly, as other studies showed [11], for MILP

solvers, the sum is more than its parts. In many cases, features support each

other. One preprocessing step removing some variables allows another step

to remove more. But also the opposite is true: Often, if one component

12

is switched oﬀ, part of the eﬀect is provided by the remaining ones. This

complicates a meaningful evaluation of feature impact.

The improvements for milp include many new heuristic methods, such

as RINS [12] and local branching [13], several classes of new or improved

cutting planes, e.g. MCF cuts [14], and a large number of tricks for the bag,

such as conﬂict analysis [15], symmetry detection [16], solution polishing and

dynamic search. Most of them either exploit some special structure in the

instances, or address a shortcoming in the algorithm for a particular class

of problems. Furthermore, codes have been ported to 64-bit addressing and

are therefore able to utilize larger amounts of memory. Moreover, many

algorithms have been parallelized [17], in particular the milp tree search and

barrier methods for lp solving.

In the area of lp solving, theoretical progress has been quite limited.

There were nonetheless numerous improvements to deal with diﬃcult in-

stances. In general, the linear algebra has been sped up by (better) exploit-

ing hyper sparsity and using highly optimized subroutines. Preprocessing

got better. The parallelization of the barrier methods improved, and there

exists nowadays a parallel variant of the simplex algorithm, although its scal-

ability is limited [18]. Nevertheless, with very few exceptions other than due

to sheer size, lps that can be solved nowadays can also be solved with the

old codes, provided one is willing to wait long enough. As Figure 1 shows,

lp solvers have become approximately nine times faster since the beginning

of the millennium. One should note that often, a given algorithm of a given

solver did not become faster, but the fastest choice nowadays is faster than

the fastest choice then. Additionally, the ability to solve very large instances

13

has improved considerably.

In the computational study done in 1999, Bixby et al. [2] used a 400 MHz

P-II with 512 MB of memory. The largest lp that this machine could han-

dle had 1,000,000 rows, 1,685,236 columns, and 3,370,472 non-zeros.

In a

workshop in January 2008 on the Perspectives in Interior Point Methods

for Solving Linear Programs, the instance zib03 with 29,128,799 columns,

19,731,970 rows and 104,422,573 non-zeros was made public. As it turned

out, the simplex algorithm was not suitable to solve it and barrier methods

needed at least about 256 GB of memory, which was not easily available at

that time. The ﬁrst to solve it was Christian Bliek in April 2009, running

cplex out-of-core with eight threads and converging in 12,035,375 seconds

(139 days) to solve the lp without crossover. Each iteration took 56 hours!

Using modern codes on a machine with 2 TB memory and 4 E7-8880v4 CPUs

@ 2.20 GHz with a total of 88 cores, this instance can be solved in 59,432

seconds = 16.5 hours with just 10% of the available memory used. This is a

speed-up of 200 within 10 years. However, when the instance was introduced

in 2008, none of the codes was able to solve it. Therefore there was inﬁnite

progress in the ﬁrst year. Furthermore, 2021 was the ﬁrst time we were able

to compute an optimal basis solution.

This pattern is much more pronounced with milp. First is the step from

unsolvable to solved. This is almost always due to algorithmic improvements.

Then there is a steady progress both due to algorithmic and hardware im-

provements until the instance is considered easy. From then on, if any at

all, speed-ups are mostly due to hardware only. The largest lp in xpress’

instance collection of practically relevant models has more than 200,000,000

14

columns and more than 1,300,000,000 non-zeros. It can be solved in about

one and a half hours. Solving an instance of this size was impossible with

oﬀ-the-shelf hardware and software in 2001.

Before describing our computational experiments in more detail, note

that there are a few caveats to bear in mind:

• Since we are interested in the performance of the overall domain, we

will compare the virtual best solver old from around 2001 (consisting

of xpress, cplex, and mosek) with the virtual best solver new from

2020 (consisting of xpress, cplex, gurobi, mosek, and copt).

• It could be argued that the default parameters are better tuned now

and therefore the old codes would beneﬁt more from hand-tuned pa-

rameters than the new ones. At the same time, the new codes have

more parameters to tune and considerably more sub-algorithms that

can be employed. We decided that it is out of the scope of this study

to try to hand-tune every instance and therefore only the default values

of the solvers will be used.

• Benchmarking got more prominent and ﬁerce during the last decade,

in particular until 2018 (see Mittelmann [19]). There has been consid-

erable tuning on the miplib instances, especially on miplib 2010 and

miplib 2017. It is fair to say that this clearly beneﬁts the newer solvers

and might lead to an overestimation of the progress.

• It should also be noted that instances dano3mip, liu, momentum3,
protfold and t1717 from miplib 2003 still remain unsolved, although

15

substantial eﬀort was put into solving them to proven optimality. Fur-

thermore, there are several old instances that still cannot be solved

within reasonable time without reformulation or special-purpose codes.

3. Computations

As demonstrated numerous times, the test set and experimental setup

have a crucial inﬂuence on the outcome of computational studies. The main

question is how to handle instances that one actor of the comparison cannot

solve. In our case – not too surprisingly – new is able to solve any instance

that old can solve, but not vice versa. When comparing solvers, it is cus-

tomary to set the maximum run time allowed as time for the comparison.

This is reasonable if one compares two solvers on a pre-selected test set.

In our case, the test set and the run time can be chosen; this means that

any speed-up factor can be attained by increasing the number of

instances that the old codes cannot solve and increasing the run

time allowed. Therefore, we decided to split those questions.

3.1. Test set selection

Our lp test set contains the instances used by Hans Mittelmann for his

lp benchmarks [19], which are listed in Table 2.

The following instances were excluded from our tests because either old

solved them in under 10 seconds or new solved them in less than one second:

chrom1024-7, neos3, ns1688926, self, stat96v1.

Additionally, we included the following instances from previous Mittel-

mann lp benchmarks in the test set: dbic1, ns1644855, nug15. We further

added three larger models, named engysys1, engysys2, engysys3, which

16

buildingenergy

cont11

ds-big

datt256

fome13

cont1

ex10

cont4

fhnw-binschedule0

graph40-40

irish-e

L1 sixm250obs

Linf 520c

neos-3025225

neos-5052403-cy

neos-5251015

neos

qap15

s100

scpm1

nug08-3rd

rail02

s250r10

pds-100

rail4284

s82

set-cover-model

shs1023

physiciansched3-3

rmine15

savsched1

square41

stormG2 1000

stp3d

supportcase10

tpl-tub-ws1617

Table 2: lp instances of Hans Mittelmann’s Benchmark of Simplex LP solvers (1-18-2021)

and Benchmark of Barrier LP solvers (12-28-2020)

we currently use for real-world energy systems research. The motivation for

this was to have some hard instances that did not appear in any benchmark

so far.

Finally, the lp relaxations of all miplib-2017 benchmark instances were

added to the lp set. Again, we ignored all instances solved by old in under

10 seconds or solved by new in less than one second. The resulting instances

are listed in Table 3.

bab2

ex9

highschool1-aigio

k1mushroom

neos-2075418-temuka

neos-3402454-bohle

neos-5104907-jarama

ns1760995

opm2-z10-s4

rail01

splice1k1

square47

supportcase19

triptim1

Table 3: miplib 2017 as lp instances

For lps, old eventually (given enough time) solved all the test instances

that new could solve, provided the available memory was suﬃcient. One

17

can argue about this selection. Since we compare virtual best solvers, the

performance (or lack thereof) of a particular solver does not matter. It is safe

to say that these instances were selected because they pose some problems

to existing solvers. Therefore, the speed-up experienced on these instances

is likely in favor of new.

For the milp comparison we used three versions of miplib, namely mi-

plib 2017, miplib 2010, and miplib 2003. It should be noted that we ex-

cluded all instances that could not be solved by new within six hours, since

there is little chance that old would be able to solve them: neos-3024952-loue,
splice1k1, s100 (from miplib 2017) and ds, timtab2, swath, mkc, t1717,
liu, stp3d, dano3mip, momentum3 (from miplib 2003).
Instances mas74
(from miplib 2017) and m100n500k4r1, dfn-gwin-UUM (from miplib 2010)

were omitted for numerical reasons. Furthermore, we excluded any instance

for which a meaningful speed-up could not be computed due to the limited

precision of our timing (whenever old converged within less than ten sec-

onds, and new under one second): modglob, gesa2, set1ch, irp, p2756,

pp08aCUTS, fiber, sp150x300d, neos-827175, drayage-100-23, fixnet6,

p200x1188c, gesa2-o, swath1, pp08a, vpm2, neos-1171448, tanglegram2,
cbs-cta. Note that nw04 was kept in the test set: although old solved it in
just two seconds, new needed more than one second (it took two seconds)

to solve it. Therefore, a meaningful comparison was possible.

3.2. Explanation of Figure 1

Figure 1 aggregates the results for all instances, both lp (left) and milp

(right), that could be solved within 24 hours by both old and new. In total,

these are 56 of 60 lp and 105 of 339 milp instances. Each symbol represents

18

a single instance. The x axis represents the running time of the virtual best

old solver and the y axis represents the running time of the virtual best new

solver. Note that both axes are log-scaled and that we needed one order and

two orders of magnitude more to represent the old running times for lp and

milp instances, respectively. The slowest instance that the old codes could

solve in a day took less than three minutes for the new codes. We clipped

the times to one second, as this is the precision we could measure.

The dotted grey diagonal is the break-even line. Any instance where

old is faster than new would be above this line. This situation does not

occur; however, both virtual solvers took roughly the same time to solve one

milp instance (nw04). Several instances lie on the clipped one-second line,
with running times for old of up to 6,000 seconds; all of them have become

trivial to solve for new. A reason might be the empirical observation that

increasing the allowed running time has a diminishing eﬀect. We will discuss

this further in the conclusions related to Figure 4.

In each plot, a colored diagonal line represents the shifted geometric mean

of the speed-up factor for lp and milp instances, respectively. All instances

(of the corresponding problem type) above (resp. below) the line show a

speed-up factor lower (resp.

larger) than the mean speed-up. There are

some rather extreme cases, in particular for milps. While the lps are more

concentrated around the mean line, there is no signiﬁcant diﬀerence between

miplib 2010 and miplib 2017 instances.

The shifted geometric means are one of the main ﬁndings. For instances

that could be solved by both old and new, the pure algorithmic speed-up

of lp solving was about nine in the last twenty years and the speed-up of

19

milp solving was about 50.

3.3. Explanation of Figure 2

Figure 2 considers the instances that are missing in Figure 1, that is the

149 (out of 240) instances from miplib 2017 that could only be solved by new

codes. The instances are sorted by the running time of the virtual best new

solver. Note that this is a linear scale, not a log scale. Most of the instances

(105 of 149) are solved by all the new solvers, whereas twelve instances could

only be solved by one of the three solvers within the time limit of 6 h.

Except two instances, all instances were solved in less than one hour and

the majority (123 of 149) in less than ten minutes. While one of the eight-

threaded solvers was the fastest for most instances, a single-threaded solver

won in four out of 13 cases for the hardest instances solved in more than half

an hour by new.

This seemingly counter-intuitive behavior is explained by the fact that

these are instances that are not solved by a tree search, but have a root

node that is diﬃcult to solve: the overall solution time is dominated by

a hard initial lp. This explains why they do not beneﬁt from tree-search

parallelization and why they still take considerable time with new.

Of course, modern milp solvers also use parallelization at the root node.

However, this mostly happens in terms of concurrent algorithms: diﬀerent

lp solvers run concurrently to solve the root lp relaxation; two alternative

cut loops run concurrently and the better one carries on at the end of the

root node. Concurrent optimization is excellent for hedging against worst-

case behavior, but it is inherently slower when the default option would have

won in either case. In such a situation, the additional variants (like running

20

primal simplex and barrier for lp solving or an alternative cut loop) compete

for the same resources, and deterministic synchronization might lead to idle

times. In our experiment, the CPU uses turbo-boost for single-thread runs,

even amplifying this situation. Thus, we would expect a single-threaded

solver to win on an instance that can be solved without much branching and

for which dual simplex is the fastest algorithm to solve the initial relaxation.

One main ideal in speeding up the milp solution process is to reduce

the number of branch-and-bound nodes needed. Nearly all modern methods

mentioned above (heuristics, cutting planes, conﬂict analysis, symmetry de-

tection, dynamic search) aim at reducing the number of nodes. The ultimate

success is achieved when an instance can be solved in the root node. The

progress is visible: among our 339 instances, old solved two instances in the

root node, while new solved 25 instances. Unfortunately, the current main

direction in hardware development is to increase the number of available

threads and the main beneﬁt from parallelization is the ability to process

more nodes in parallel. Therefore, hardware and algorithmic development

are to a certain extent now non-synergistic. This was diﬀerent in the past.

3.4. Take the results with a grain of salt

We refrained from aggregating instances that could be solved by both

old and new, and instances that could only be solved by new, into a single

score. While this might be done by using a time limit for unsolved instances

and possibly even a penalty, it can easily skew results.

As a most extreme example, consider using a par score (hence weighing

all timeouts with a factor of ten times the time limit). Due to the high number

of instances that cannot be solved by the old codes, we could obtain almost

21

Figure 3: Speed-up virtual best old vs. virtual best new on miplib 2017 [1] benchmark

set using diﬀerent par values for various time limits.

arbitrarily large speed-up numbers. With a time limit of one hour and no

par score (or rather: par 1), we would get a reasonable speed-up factor of

37 (which is close to the speed-up observed on instances that both versions

could solve). Increasing the time limit to 24 hours would give us a speed-

up factor of 335. Figure 3 demonstrates that a similarly large potential for

exaggerating results hides in the par score. With a time limit of one hour

and a par score, the “speed-up factor” (note that par does not actually

compute speed-ups) would be 226, with a par score, it would be 1374.

Setting the time limit to 24 hours and using par, we would get a factor of

1647 and using par, we would get 8125. We see that by driving the time

limits up and/or using par scores, we can arbitrarily inﬂate the numbers.

It seems much more sound to report the speed-up factor (50) on solved

22

123456789102030405060708090ParX, X=010002000300040005000600070008000Speed-up factor Virtual Best Old Code vs. Virtual Best New CodeTime limit24 h6 h2 h1 hinstances and the impressive 62% of the instances (156/240) that could be

solved by new, but not by old. This also shows where the largest progress

in lp and milp solving lies:

Making new instances and whole problem classes tractable.

3.4.1. Performance Variability

The term performance variability [20], loosely speaking, comprises unex-

pected changes in performance that are triggered by seemingly performance-

neutral changes like changing the order in which constraints of a model are

fed into a MILP solver. Besides others, performance variability is caused by

imperfect tie-breaking. This results in slight numerical diﬀerences caused by

ﬂoating-point arithmetic which may lead to diﬀerent decisions being taken

during the solution process. Even though one can exploit performance vari-

ability in some ways [21], it is mainly considered an undesirable property of

a solver.

We did the following experiment to investigate whether the amount of

performance variability that MILP solvers expose has changed in the past

20 years. Taking only those instances that both old and new can solve, we

generated ten random permutations of rows and columns for each instance.

Mathematically, the instances are equivalent, but diﬀerent permutations can

lead to arbitrarily large diﬀerences in run times and tree size, at least for

some instances, see, e.g., [22]. We ran each permutation of each instance

with each solver with a two hour time limit.

We computed the performance variability for old, for new using one

thread, and for new using eight threads. Therefore, we ﬁrst took the mini-

mum number of nodes needed to solve (or processed within the time limit)

23

a particular instance-permutation combination by either old, by new using

one thread, or by new using eight threads. With these minima, we com-

puted the variability score [7] of each instance over all permutations, again

separately for old, new using one thread, and new using eight threads.

To compensate for seemingly large changes on the lower end, like two nodes

instead of one node, which drive up the score tremendously but have no sig-

niﬁcance in practice, we added a shift of 100 to the node counts. Finally, we

computed the average over the variability scores to get an overall measure

of performance variability for old and the two versions of new. Table 4

summarizes our ﬁndings.

Mean

Median

Std-dev

old

0.1382

0.0926

0.1216

new using 1 thread
new using 8 threads

0.0704

0.0577

0.0627

0.1137

0.0692

0.1431

Table 4: Mean and median variability scores computed by using ten permutations

We observe that the variability is much lower in the new solvers. The one

threaded run of new only exposes half of the variability of the one threaded

run of old. Unsurprisingly, running in parallel increases variability. Note

that the standard-deviation is larger than the mean, even though the score

is bounded by zero from below. This points to a large spread of variability

scores, with a tendency towards the extreme values (including zero vari-

ability). The mean is larger than the median because some outliers have

considerable variability in all cases.

24

4. Conclusion

4.1. LP

For lps, we computed an average speed-up factor of nine. Combining

this with the hardware speed-up, we can conclude that solving lps got about

180 times faster in the last two decades. However, the main diﬀerence comes

from the switch to 64-bit computing, allowing to solve much larger instances,

in particular with parallelized barrier codes. Furthermore, it is fair to say

that the solver implementations became ever more reﬁned, leading to ex-

tremely stable codes. At the same time, little progress has been made on the

theoretical side.

4.2. milp

In this case, the picture gets more diverse. For the instances solved with

the old codes, the average speed-up due to improved algorithms (Figure 1) is

about 50. This means that solving milps got faster by 22% every year during

the last 20 years, purely from the algorithmic side, and that is not taking into

account how many more instances we can solve today as compared to twenty

years ago. Combining this with the hardware speed-up, we ﬁnd an average

total speed-up for solving milps of 1,000: ﬁfteen minutes then become less

than a second now.

The most impressive result is shown in Figure 2: the vastly increased

ability to solve milp instances at all. 149 of 240 instances (62%) from the

miplib 2017 benchmark cannot be solved by any of the old solvers within a

day, even on a modern computer. In contrast, the geometric mean running

time for solving these instances by new is 104 seconds. We argued why

25

deriving estimated speed-up factors like “1 day/104 sec = 830-fold speed-up”

would be misleading and one should distinguish between the precise speed-

up for instances solved by both and the incredible achievements in solving

previously intractable instances.

To summarize, in 2001, one would be pleasantly surprised if one of the

solvers would readily solve an arbitrary milp instance. Nowadays, one is

unpleasantly surprised if none of the solvers can tackle it.

Figure 4 depicts the eﬀect. The number of instances that are solvable

right away is ever increasing, but the shape of the frontier stays identical;

it is simply pushed to the right. However, it is important to note that the

instances on the left are precisely the ones that we wish to solve.

Figure 4: The number of instances that are (quickly) solvable is monotonically increasing

over time and the frontier of “diﬃcult” instances is pushed further to the right (stylized)

If we simply speeded up computations, the curve would become more

L-shaped but would not be shifted. This is the case with, e.g., (better)

26

SolvingtimeNumber of instances∞∞20012020parallelization, or if we use the old codes on a new machine. However, it does

not change much regarding the overall solvability of instances. To really shift

the curve to the right, algorithmic improvements beyond pure computational

speed-ups are needed.

4.3. Outlook

A problem that we foresee in the future is diminishing returns: as can be

deduced from the results, having more and faster cores will not signiﬁcantly

improve the solvability. There are only 14 instances for which old took more

than two hours, but less than 24 hours. As [23] described, there are individ-

ual instances that can be solved by massive amounts of computing power,

however there are few of them. A similar situation is true regarding mem-

ory. There are, without doubt, some extremely large instances. However, the

number of instances that require terabytes of memory are few. And if they

do, scaling to higher numbers of cores does not work particularly well because

of limited memory bandwidth. There are special algorithms for distributed

systems (e.g.

[24]), but these are still far from becoming usable by out-of-

the-box solvers. Given the change in computer architectures, in particular

GPUs, and heterogeneous cores with energy budgets, it becomes increasingly

challenging for the solvers to fully exploit the available hardware. This opens

interesting directions for research.

A similar observation can be made for the algorithmic side. Overall, it is

experienced that every added algorithmic idea aﬀects an increasingly smaller

subset of instances. As always, we hope for breakthrough ideas to appear.

However, so far, solvers still provide signiﬁcant algorithmic improvements

with every release. While additional speed-up by hardware has gone mostly

27

stale, lp and milp solvers are still going strong.

Acknowledgements

The work for this article has been conducted in the Research Campus

MODAL funded by the German Federal Ministry of Education and Research

(BMBF) (fund numbers 05M14ZAM, 05M20ZBM).

This work has been supported by the German Federal Ministry of Eco-

nomic Aﬀairs and Energy (BMWi) through the project UNSEEN (fund no

03EI1004D): Bewertung der Unsicherheit in linear optimierten Energiesystem-

Modellen unter Zuhilfenahme Neuronaler Netze.

We thank Carsten Dresske for providing us with access to a still nicely

running Pentium-III powered computer.

We thank IBM for providing us with cplex Version 7, FICO for providing

us with xpress Version 14, and MOSEK for providing us with MOSEK

Version 3.

References

[1] A. Gleixner, G. Hendel, G. Gamrath, T. Achterberg, M. Bastubbe,

T. Berthold, P. M. Christophel, K. Jarck, T. Koch, J. Linderoth,

M. L¨ubbecke, H. D. Mittelmann, D. Ozyurt, T. K. Ralphs, D. Sal-

vagnin, Y. Shinano, MIPLIB 2017: Data-Driven Compilation of the

6th Mixed-Integer Programming Library, Mathematical Programming

Computation (2021). doi:10.1007/s12532-020-00194-3.

[2] E. R. Bixby, M. Fenelon, Z. Gu, E. Rothberg, R. Wunderling, MIP:

Theory and practice — closing the gap, in: M. J. D. Powell, S. Scholtes

28

(Eds.), System Modelling and Optimization, Springer US, Boston, MA,

2000, pp. 19–49.

[3] R. E. Bixby, Solving real-world linear programs: A decade and more of

progress, Operations Research 50 (2002) 3–15. doi:10.1287/opre.50.

1.3.17780.

[4] R. E. Bixby, M. Fenelon, Z. Gu, E. Rothberg, R. Wunderling, Mixed-

integer programming: a progress report,

in: M. Gr¨otschel (Ed.), The

Sharpest Cut: The Impact of Manfred Padberg and His Work, MPS-

SIAM Series on Optimization, SIAM, 2004, pp. 309––325. doi:10.1137/

1.9780898718805.ch18.

[5] T. Achterberg, R. Wunderling, Mixed integer programming: Analyzing

12 years of progress, in: Facets of Combinatorial Optimization, Springer,

2013, pp. 449–481. doi:10.1007/978-3-642-38189-8_18.

[6] A. Lodi, Mixed integer programming computation,

in: 50 years of

Integer Programming 1958–2008, Springer, 2010, pp. 619–645. doi:10.

1007/978-3-540-68279-0.

[7] T. Koch, T. Achterberg, E. Andersen, O. Bastert, T. Berthold, R. E.

Bixby, E. Danna, G. Gamrath, A. M. Gleixner, S. Heinz, A. Lodi,

H. Mittelmann, T. Ralphs, D. Salvagnin, D. E. Steﬀy, K. Wolter, MI-

PLIB 2010, Mathematical Programming Computation 3 (2011) 103–163.

doi:10.1007/s12532-011-0025-9.

[8] T. Koch, A. Martin, M. E. Pfetsch, Progress in academic computa-

29

tional integer programming, in: Facets of Combinatorial Optimization,

Springer, 2013, pp. 483–506. doi:10.1007/978-3-642-38189-8_18.

[9] R. Ashford, Mixed integer programming: A historical perspective with

Xpress-MP, Annals of Operations Research 149 (2007) 5.

[10] J. D. McCalpin, STREAM: Sustainable memory bandwidth in high per-

formance computers, 2013. https://www.cs.virginia.edu/stream.

[11] T. Achterberg, Constraint Integer Programming, Ph.D. thesis, Technis-

che Universit¨at Berlin, 2009. doi:10.14279/depositonce-1634.

[12] E. Danna, E. Rothberg, C. L. Pape, Exploring relaxation induced neigh-

borhoods to improve MIP solutions, Mathematical Programming 102

(2004) 71–90. doi:10.1007/s10107-004-0518-7.

[13] M. Fischetti, A. Lodi, Local branching, Mathematical Programming 98

(2003) 23–47. doi:10.1007/s10107-003-0395-5.

[14] T. Achterberg, C. Raack, MCF-separator: detecting and exploiting

multi-commodity ﬂow structures in MIPs, Mathematical Programming

Computation 2 (2010) 125–165. doi:10.1007/s12532-010-0015-3.

[15] T. Achterberg, Conﬂict analysis in mixed integer programming, Discrete

Optimization 4 (2007) 4–20. doi:10.1016/j.disopt.2006.10.006.

[16] F. Margot, Exploiting orbits in symmetric ILP, Mathematical Program-

ming 98 (2003) 3–21. doi:10.1007/s10107-003-0394-6.

30

[17] T. Berthold, J. Farmer, S. Heinz, M. Perregaard, Parallelization of the

FICO Xpress-Optimizer, Optimization Methods and Software 33 (2018)

518–529. doi:10.1080/10556788.2017.1333612.

[18] Q. Huangfu, J. A. J. Hall, Parallelizing the dual revised simplex method,

Mathematical Programming Computation 10 (2018) 119–142. doi:10.

1007/s12532-017-0130-5.

[19] H. Mittelmann, Benchmarks for Optimization Software, 2020. http:

//plato.asu.edu/bench.html.

[20] A. Lodi, A. Tramontani, Performance variability in mixed-integer pro-

gramming,

in: Theory Driven by Inﬂuential Applications, INFORMS,

2013, pp. 1–12.

[21] M. Fischetti, A. Lodi, M. Monaci, D. Salvagnin, A. Tramontani, Im-

proving branch-and-cut performance by random sampling, Mathemati-

cal Programming Computation (2015) 1–20.

[22] T. Berthold, A computational study of primal heuristics inside an mi

(nl) p solver, Journal of Global Optimization 70 (2018) 189–206. doi:10.

1007/s10898-017-0600-3.

[23] Y. Shinano, T. Achterberg, T. Berthold, S. Heinz, T. Koch, M. Winkler,

Solving Previously Unsolved MIP Instances with ParaSCIP on Super-

computers by using up to 80,000 Cores, Technical Report 20-16, ZIB,

Takustr. 7, 14195 Berlin, 2020.

[24] D. Rehfeldt, H. Hobbie, D. Sch¨onheit, A. Gleixner, T. Koch, D. M¨ost, A

massively parallel interior-point solver for linear energy system models

31

with block structure, Technical Report 19-41, ZIB, Takustr. 7, 14195

Berlin, 2019.

32

